{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 1095920,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0009124753631651946,
      "grad_norm": 5.147752285003662,
      "learning_rate": 4.9999239603864026e-05,
      "loss": 2.047,
      "step": 100
    },
    {
      "epoch": 0.0018249507263303892,
      "grad_norm": 6.455582141876221,
      "learning_rate": 4.999847920772806e-05,
      "loss": 1.7165,
      "step": 200
    },
    {
      "epoch": 0.0027374260894955835,
      "grad_norm": 6.773009300231934,
      "learning_rate": 4.9997718811592086e-05,
      "loss": 1.5585,
      "step": 300
    },
    {
      "epoch": 0.0036499014526607783,
      "grad_norm": 5.421006202697754,
      "learning_rate": 4.9996958415456116e-05,
      "loss": 1.5489,
      "step": 400
    },
    {
      "epoch": 0.004562376815825973,
      "grad_norm": 5.734410762786865,
      "learning_rate": 4.9996198019320146e-05,
      "loss": 1.4764,
      "step": 500
    },
    {
      "epoch": 0.005474852178991167,
      "grad_norm": 5.093199253082275,
      "learning_rate": 4.9995437623184176e-05,
      "loss": 1.4205,
      "step": 600
    },
    {
      "epoch": 0.006387327542156361,
      "grad_norm": 4.759934902191162,
      "learning_rate": 4.9994677227048206e-05,
      "loss": 1.3341,
      "step": 700
    },
    {
      "epoch": 0.007299802905321557,
      "grad_norm": 4.957815170288086,
      "learning_rate": 4.9993916830912236e-05,
      "loss": 1.4038,
      "step": 800
    },
    {
      "epoch": 0.00821227826848675,
      "grad_norm": 5.099602222442627,
      "learning_rate": 4.999315643477626e-05,
      "loss": 1.3634,
      "step": 900
    },
    {
      "epoch": 0.009124753631651945,
      "grad_norm": 4.585923194885254,
      "learning_rate": 4.9992396038640296e-05,
      "loss": 1.3656,
      "step": 1000
    },
    {
      "epoch": 0.01003722899481714,
      "grad_norm": 6.7032151222229,
      "learning_rate": 4.999163564250432e-05,
      "loss": 1.2793,
      "step": 1100
    },
    {
      "epoch": 0.010949704357982334,
      "grad_norm": 5.388281345367432,
      "learning_rate": 4.999087524636835e-05,
      "loss": 1.3033,
      "step": 1200
    },
    {
      "epoch": 0.01186217972114753,
      "grad_norm": 5.505737781524658,
      "learning_rate": 4.999011485023238e-05,
      "loss": 1.3227,
      "step": 1300
    },
    {
      "epoch": 0.012774655084312723,
      "grad_norm": 5.095860004425049,
      "learning_rate": 4.998935445409641e-05,
      "loss": 1.2672,
      "step": 1400
    },
    {
      "epoch": 0.013687130447477918,
      "grad_norm": 5.0027642250061035,
      "learning_rate": 4.998859405796043e-05,
      "loss": 1.2491,
      "step": 1500
    },
    {
      "epoch": 0.014599605810643113,
      "grad_norm": 6.116654872894287,
      "learning_rate": 4.998783366182447e-05,
      "loss": 1.2494,
      "step": 1600
    },
    {
      "epoch": 0.015512081173808307,
      "grad_norm": 5.332902908325195,
      "learning_rate": 4.998707326568849e-05,
      "loss": 1.2588,
      "step": 1700
    },
    {
      "epoch": 0.0164245565369735,
      "grad_norm": 5.488097190856934,
      "learning_rate": 4.998631286955252e-05,
      "loss": 1.2474,
      "step": 1800
    },
    {
      "epoch": 0.017337031900138695,
      "grad_norm": 4.585949897766113,
      "learning_rate": 4.998555247341655e-05,
      "loss": 1.2708,
      "step": 1900
    },
    {
      "epoch": 0.01824950726330389,
      "grad_norm": 4.305842399597168,
      "learning_rate": 4.9984792077280583e-05,
      "loss": 1.1978,
      "step": 2000
    },
    {
      "epoch": 0.019161982626469086,
      "grad_norm": 5.382846832275391,
      "learning_rate": 4.9984031681144613e-05,
      "loss": 1.2449,
      "step": 2100
    },
    {
      "epoch": 0.02007445798963428,
      "grad_norm": 4.924253463745117,
      "learning_rate": 4.9983271285008644e-05,
      "loss": 1.176,
      "step": 2200
    },
    {
      "epoch": 0.020986933352799473,
      "grad_norm": 6.521198749542236,
      "learning_rate": 4.998251088887267e-05,
      "loss": 1.2217,
      "step": 2300
    },
    {
      "epoch": 0.021899408715964668,
      "grad_norm": 5.8652663230896,
      "learning_rate": 4.9981750492736704e-05,
      "loss": 1.2258,
      "step": 2400
    },
    {
      "epoch": 0.022811884079129863,
      "grad_norm": 5.304313659667969,
      "learning_rate": 4.998099009660073e-05,
      "loss": 1.2162,
      "step": 2500
    },
    {
      "epoch": 0.02372435944229506,
      "grad_norm": 6.493332386016846,
      "learning_rate": 4.998022970046476e-05,
      "loss": 1.218,
      "step": 2600
    },
    {
      "epoch": 0.024636834805460254,
      "grad_norm": 5.013082981109619,
      "learning_rate": 4.997946930432879e-05,
      "loss": 1.1851,
      "step": 2700
    },
    {
      "epoch": 0.025549310168625446,
      "grad_norm": 6.014996528625488,
      "learning_rate": 4.997870890819282e-05,
      "loss": 1.1505,
      "step": 2800
    },
    {
      "epoch": 0.02646178553179064,
      "grad_norm": 4.777201175689697,
      "learning_rate": 4.997794851205684e-05,
      "loss": 1.1823,
      "step": 2900
    },
    {
      "epoch": 0.027374260894955836,
      "grad_norm": 4.827759265899658,
      "learning_rate": 4.997718811592087e-05,
      "loss": 1.131,
      "step": 3000
    },
    {
      "epoch": 0.02828673625812103,
      "grad_norm": 4.82503080368042,
      "learning_rate": 4.99764277197849e-05,
      "loss": 1.1867,
      "step": 3100
    },
    {
      "epoch": 0.029199211621286227,
      "grad_norm": 5.057279109954834,
      "learning_rate": 4.997566732364893e-05,
      "loss": 1.1135,
      "step": 3200
    },
    {
      "epoch": 0.03011168698445142,
      "grad_norm": 4.704108715057373,
      "learning_rate": 4.997490692751296e-05,
      "loss": 1.1804,
      "step": 3300
    },
    {
      "epoch": 0.031024162347616614,
      "grad_norm": 4.007565498352051,
      "learning_rate": 4.9974146531376984e-05,
      "loss": 1.1311,
      "step": 3400
    },
    {
      "epoch": 0.03193663771078181,
      "grad_norm": 4.832082748413086,
      "learning_rate": 4.997338613524102e-05,
      "loss": 1.1455,
      "step": 3500
    },
    {
      "epoch": 0.032849113073947,
      "grad_norm": 6.139422416687012,
      "learning_rate": 4.9972625739105044e-05,
      "loss": 1.1892,
      "step": 3600
    },
    {
      "epoch": 0.033761588437112196,
      "grad_norm": 5.568297863006592,
      "learning_rate": 4.9971865342969074e-05,
      "loss": 1.11,
      "step": 3700
    },
    {
      "epoch": 0.03467406380027739,
      "grad_norm": 6.013718605041504,
      "learning_rate": 4.9971104946833104e-05,
      "loss": 1.1139,
      "step": 3800
    },
    {
      "epoch": 0.035586539163442586,
      "grad_norm": 5.549561977386475,
      "learning_rate": 4.9970344550697134e-05,
      "loss": 1.1549,
      "step": 3900
    },
    {
      "epoch": 0.03649901452660778,
      "grad_norm": 5.1306962966918945,
      "learning_rate": 4.996958415456116e-05,
      "loss": 1.1712,
      "step": 4000
    },
    {
      "epoch": 0.03741148988977298,
      "grad_norm": 5.967419147491455,
      "learning_rate": 4.9968823758425194e-05,
      "loss": 1.1375,
      "step": 4100
    },
    {
      "epoch": 0.03832396525293817,
      "grad_norm": 5.668371677398682,
      "learning_rate": 4.996806336228922e-05,
      "loss": 1.0985,
      "step": 4200
    },
    {
      "epoch": 0.03923644061610337,
      "grad_norm": 4.743771076202393,
      "learning_rate": 4.996730296615325e-05,
      "loss": 1.1469,
      "step": 4300
    },
    {
      "epoch": 0.04014891597926856,
      "grad_norm": 5.017533302307129,
      "learning_rate": 4.996654257001728e-05,
      "loss": 1.0824,
      "step": 4400
    },
    {
      "epoch": 0.04106139134243376,
      "grad_norm": 4.850203990936279,
      "learning_rate": 4.996578217388131e-05,
      "loss": 1.1292,
      "step": 4500
    },
    {
      "epoch": 0.041973866705598946,
      "grad_norm": 5.517262935638428,
      "learning_rate": 4.996502177774534e-05,
      "loss": 1.1289,
      "step": 4600
    },
    {
      "epoch": 0.04288634206876414,
      "grad_norm": 7.167405605316162,
      "learning_rate": 4.996426138160937e-05,
      "loss": 1.1248,
      "step": 4700
    },
    {
      "epoch": 0.043798817431929336,
      "grad_norm": 4.70813512802124,
      "learning_rate": 4.996350098547339e-05,
      "loss": 1.1298,
      "step": 4800
    },
    {
      "epoch": 0.04471129279509453,
      "grad_norm": 5.63304328918457,
      "learning_rate": 4.996274058933743e-05,
      "loss": 1.1251,
      "step": 4900
    },
    {
      "epoch": 0.04562376815825973,
      "grad_norm": 5.758653163909912,
      "learning_rate": 4.996198019320145e-05,
      "loss": 1.0977,
      "step": 5000
    },
    {
      "epoch": 0.04653624352142492,
      "grad_norm": 5.132978916168213,
      "learning_rate": 4.996121979706548e-05,
      "loss": 1.0686,
      "step": 5100
    },
    {
      "epoch": 0.04744871888459012,
      "grad_norm": 4.049198150634766,
      "learning_rate": 4.996045940092951e-05,
      "loss": 1.1011,
      "step": 5200
    },
    {
      "epoch": 0.04836119424775531,
      "grad_norm": 5.664248943328857,
      "learning_rate": 4.995969900479354e-05,
      "loss": 1.0825,
      "step": 5300
    },
    {
      "epoch": 0.04927366961092051,
      "grad_norm": 4.997869968414307,
      "learning_rate": 4.9958938608657565e-05,
      "loss": 1.0886,
      "step": 5400
    },
    {
      "epoch": 0.0501861449740857,
      "grad_norm": 4.508299827575684,
      "learning_rate": 4.99581782125216e-05,
      "loss": 0.9922,
      "step": 5500
    },
    {
      "epoch": 0.05109862033725089,
      "grad_norm": 4.46112585067749,
      "learning_rate": 4.9957417816385625e-05,
      "loss": 1.1265,
      "step": 5600
    },
    {
      "epoch": 0.052011095700416086,
      "grad_norm": 3.937096357345581,
      "learning_rate": 4.9956657420249655e-05,
      "loss": 1.0793,
      "step": 5700
    },
    {
      "epoch": 0.05292357106358128,
      "grad_norm": 5.432738780975342,
      "learning_rate": 4.9955897024113685e-05,
      "loss": 1.0796,
      "step": 5800
    },
    {
      "epoch": 0.05383604642674648,
      "grad_norm": 5.13015604019165,
      "learning_rate": 4.995513662797771e-05,
      "loss": 1.1286,
      "step": 5900
    },
    {
      "epoch": 0.05474852178991167,
      "grad_norm": 4.802987575531006,
      "learning_rate": 4.9954376231841745e-05,
      "loss": 1.1182,
      "step": 6000
    },
    {
      "epoch": 0.05566099715307687,
      "grad_norm": 4.63137674331665,
      "learning_rate": 4.995361583570577e-05,
      "loss": 1.1191,
      "step": 6100
    },
    {
      "epoch": 0.05657347251624206,
      "grad_norm": 5.400720596313477,
      "learning_rate": 4.99528554395698e-05,
      "loss": 1.069,
      "step": 6200
    },
    {
      "epoch": 0.05748594787940726,
      "grad_norm": 4.433725357055664,
      "learning_rate": 4.995209504343383e-05,
      "loss": 1.0804,
      "step": 6300
    },
    {
      "epoch": 0.05839842324257245,
      "grad_norm": 5.5764360427856445,
      "learning_rate": 4.995133464729786e-05,
      "loss": 1.0794,
      "step": 6400
    },
    {
      "epoch": 0.05931089860573765,
      "grad_norm": 4.851415634155273,
      "learning_rate": 4.995057425116189e-05,
      "loss": 1.0642,
      "step": 6500
    },
    {
      "epoch": 0.06022337396890284,
      "grad_norm": 4.8092780113220215,
      "learning_rate": 4.994981385502592e-05,
      "loss": 1.0484,
      "step": 6600
    },
    {
      "epoch": 0.06113584933206803,
      "grad_norm": 5.1137495040893555,
      "learning_rate": 4.994905345888994e-05,
      "loss": 1.0568,
      "step": 6700
    },
    {
      "epoch": 0.06204832469523323,
      "grad_norm": 4.989723205566406,
      "learning_rate": 4.994829306275397e-05,
      "loss": 1.0587,
      "step": 6800
    },
    {
      "epoch": 0.06296080005839842,
      "grad_norm": 5.431092739105225,
      "learning_rate": 4.9947532666618e-05,
      "loss": 1.0431,
      "step": 6900
    },
    {
      "epoch": 0.06387327542156362,
      "grad_norm": 5.896696090698242,
      "learning_rate": 4.994677227048203e-05,
      "loss": 1.0742,
      "step": 7000
    },
    {
      "epoch": 0.06478575078472881,
      "grad_norm": 4.9062018394470215,
      "learning_rate": 4.994601187434606e-05,
      "loss": 1.0246,
      "step": 7100
    },
    {
      "epoch": 0.065698226147894,
      "grad_norm": 5.024612903594971,
      "learning_rate": 4.994525147821009e-05,
      "loss": 1.0823,
      "step": 7200
    },
    {
      "epoch": 0.0666107015110592,
      "grad_norm": 4.10994291305542,
      "learning_rate": 4.9944491082074116e-05,
      "loss": 1.0462,
      "step": 7300
    },
    {
      "epoch": 0.06752317687422439,
      "grad_norm": 5.955977439880371,
      "learning_rate": 4.994373068593815e-05,
      "loss": 1.0274,
      "step": 7400
    },
    {
      "epoch": 0.0684356522373896,
      "grad_norm": 4.850308418273926,
      "learning_rate": 4.9942970289802176e-05,
      "loss": 1.0503,
      "step": 7500
    },
    {
      "epoch": 0.06934812760055478,
      "grad_norm": 6.431196689605713,
      "learning_rate": 4.9942209893666206e-05,
      "loss": 1.0331,
      "step": 7600
    },
    {
      "epoch": 0.07026060296371998,
      "grad_norm": 5.6030988693237305,
      "learning_rate": 4.9941449497530236e-05,
      "loss": 1.0199,
      "step": 7700
    },
    {
      "epoch": 0.07117307832688517,
      "grad_norm": 5.367276191711426,
      "learning_rate": 4.9940689101394266e-05,
      "loss": 1.0476,
      "step": 7800
    },
    {
      "epoch": 0.07208555369005037,
      "grad_norm": 4.659315586090088,
      "learning_rate": 4.9939928705258296e-05,
      "loss": 1.0384,
      "step": 7900
    },
    {
      "epoch": 0.07299802905321556,
      "grad_norm": 4.41477632522583,
      "learning_rate": 4.9939168309122326e-05,
      "loss": 1.0607,
      "step": 8000
    },
    {
      "epoch": 0.07391050441638075,
      "grad_norm": 4.5584635734558105,
      "learning_rate": 4.993840791298635e-05,
      "loss": 1.0282,
      "step": 8100
    },
    {
      "epoch": 0.07482297977954595,
      "grad_norm": 5.057887554168701,
      "learning_rate": 4.9937647516850387e-05,
      "loss": 1.0418,
      "step": 8200
    },
    {
      "epoch": 0.07573545514271114,
      "grad_norm": 4.523952960968018,
      "learning_rate": 4.993688712071441e-05,
      "loss": 1.0542,
      "step": 8300
    },
    {
      "epoch": 0.07664793050587634,
      "grad_norm": 4.021158695220947,
      "learning_rate": 4.993612672457844e-05,
      "loss": 1.0582,
      "step": 8400
    },
    {
      "epoch": 0.07756040586904153,
      "grad_norm": 5.176318168640137,
      "learning_rate": 4.993536632844247e-05,
      "loss": 0.9957,
      "step": 8500
    },
    {
      "epoch": 0.07847288123220673,
      "grad_norm": 4.5128560066223145,
      "learning_rate": 4.993460593230649e-05,
      "loss": 1.0168,
      "step": 8600
    },
    {
      "epoch": 0.07938535659537192,
      "grad_norm": 5.245326519012451,
      "learning_rate": 4.993384553617052e-05,
      "loss": 1.0395,
      "step": 8700
    },
    {
      "epoch": 0.08029783195853712,
      "grad_norm": 6.474645137786865,
      "learning_rate": 4.9933085140034553e-05,
      "loss": 1.051,
      "step": 8800
    },
    {
      "epoch": 0.08121030732170231,
      "grad_norm": 4.271410942077637,
      "learning_rate": 4.9932324743898584e-05,
      "loss": 1.0843,
      "step": 8900
    },
    {
      "epoch": 0.08212278268486752,
      "grad_norm": 4.599697113037109,
      "learning_rate": 4.9931564347762614e-05,
      "loss": 1.0409,
      "step": 9000
    },
    {
      "epoch": 0.0830352580480327,
      "grad_norm": 4.168335437774658,
      "learning_rate": 4.9930803951626644e-05,
      "loss": 1.029,
      "step": 9100
    },
    {
      "epoch": 0.08394773341119789,
      "grad_norm": 5.3780012130737305,
      "learning_rate": 4.993004355549067e-05,
      "loss": 1.0331,
      "step": 9200
    },
    {
      "epoch": 0.0848602087743631,
      "grad_norm": 5.1586174964904785,
      "learning_rate": 4.9929283159354704e-05,
      "loss": 1.0347,
      "step": 9300
    },
    {
      "epoch": 0.08577268413752828,
      "grad_norm": 4.652102947235107,
      "learning_rate": 4.992852276321873e-05,
      "loss": 1.0247,
      "step": 9400
    },
    {
      "epoch": 0.08668515950069348,
      "grad_norm": 5.572147369384766,
      "learning_rate": 4.992776236708276e-05,
      "loss": 1.0644,
      "step": 9500
    },
    {
      "epoch": 0.08759763486385867,
      "grad_norm": 4.166231155395508,
      "learning_rate": 4.992700197094679e-05,
      "loss": 1.0082,
      "step": 9600
    },
    {
      "epoch": 0.08851011022702387,
      "grad_norm": 4.67948579788208,
      "learning_rate": 4.992624157481082e-05,
      "loss": 1.0518,
      "step": 9700
    },
    {
      "epoch": 0.08942258559018906,
      "grad_norm": 4.811708927154541,
      "learning_rate": 4.992548117867484e-05,
      "loss": 1.0716,
      "step": 9800
    },
    {
      "epoch": 0.09033506095335427,
      "grad_norm": 4.517982006072998,
      "learning_rate": 4.992472078253888e-05,
      "loss": 1.0065,
      "step": 9900
    },
    {
      "epoch": 0.09124753631651945,
      "grad_norm": 4.626156330108643,
      "learning_rate": 4.99239603864029e-05,
      "loss": 1.0008,
      "step": 10000
    },
    {
      "epoch": 0.09216001167968464,
      "grad_norm": 5.2847795486450195,
      "learning_rate": 4.992319999026693e-05,
      "loss": 1.0528,
      "step": 10100
    },
    {
      "epoch": 0.09307248704284984,
      "grad_norm": 5.339972496032715,
      "learning_rate": 4.992243959413096e-05,
      "loss": 1.0351,
      "step": 10200
    },
    {
      "epoch": 0.09398496240601503,
      "grad_norm": 5.336594581604004,
      "learning_rate": 4.992167919799499e-05,
      "loss": 1.0033,
      "step": 10300
    },
    {
      "epoch": 0.09489743776918023,
      "grad_norm": 5.415207386016846,
      "learning_rate": 4.992091880185902e-05,
      "loss": 1.0148,
      "step": 10400
    },
    {
      "epoch": 0.09580991313234542,
      "grad_norm": 4.425357341766357,
      "learning_rate": 4.992015840572305e-05,
      "loss": 1.0242,
      "step": 10500
    },
    {
      "epoch": 0.09672238849551063,
      "grad_norm": 5.34060525894165,
      "learning_rate": 4.9919398009587074e-05,
      "loss": 1.0002,
      "step": 10600
    },
    {
      "epoch": 0.09763486385867581,
      "grad_norm": 5.614400863647461,
      "learning_rate": 4.991863761345111e-05,
      "loss": 0.9977,
      "step": 10700
    },
    {
      "epoch": 0.09854733922184102,
      "grad_norm": 4.992709159851074,
      "learning_rate": 4.9917877217315134e-05,
      "loss": 1.0249,
      "step": 10800
    },
    {
      "epoch": 0.0994598145850062,
      "grad_norm": 4.335575103759766,
      "learning_rate": 4.9917116821179165e-05,
      "loss": 0.9782,
      "step": 10900
    },
    {
      "epoch": 0.1003722899481714,
      "grad_norm": 4.879144668579102,
      "learning_rate": 4.9916356425043195e-05,
      "loss": 1.0493,
      "step": 11000
    },
    {
      "epoch": 0.1012847653113366,
      "grad_norm": 4.629830837249756,
      "learning_rate": 4.9915596028907225e-05,
      "loss": 1.0287,
      "step": 11100
    },
    {
      "epoch": 0.10219724067450178,
      "grad_norm": 4.66579008102417,
      "learning_rate": 4.991483563277125e-05,
      "loss": 1.0228,
      "step": 11200
    },
    {
      "epoch": 0.10310971603766698,
      "grad_norm": 5.184679985046387,
      "learning_rate": 4.9914075236635285e-05,
      "loss": 1.0422,
      "step": 11300
    },
    {
      "epoch": 0.10402219140083217,
      "grad_norm": 5.502918720245361,
      "learning_rate": 4.991331484049931e-05,
      "loss": 0.9682,
      "step": 11400
    },
    {
      "epoch": 0.10493466676399738,
      "grad_norm": 5.832265853881836,
      "learning_rate": 4.991255444436334e-05,
      "loss": 1.0422,
      "step": 11500
    },
    {
      "epoch": 0.10584714212716256,
      "grad_norm": 5.033229827880859,
      "learning_rate": 4.991179404822737e-05,
      "loss": 0.9729,
      "step": 11600
    },
    {
      "epoch": 0.10675961749032777,
      "grad_norm": 4.786765098571777,
      "learning_rate": 4.991103365209139e-05,
      "loss": 1.0414,
      "step": 11700
    },
    {
      "epoch": 0.10767209285349295,
      "grad_norm": 5.122344017028809,
      "learning_rate": 4.991027325595543e-05,
      "loss": 1.0109,
      "step": 11800
    },
    {
      "epoch": 0.10858456821665816,
      "grad_norm": 4.933959484100342,
      "learning_rate": 4.990951285981945e-05,
      "loss": 0.993,
      "step": 11900
    },
    {
      "epoch": 0.10949704357982334,
      "grad_norm": 5.143568992614746,
      "learning_rate": 4.990875246368348e-05,
      "loss": 1.0368,
      "step": 12000
    },
    {
      "epoch": 0.11040951894298853,
      "grad_norm": 4.915111541748047,
      "learning_rate": 4.990799206754751e-05,
      "loss": 1.0103,
      "step": 12100
    },
    {
      "epoch": 0.11132199430615373,
      "grad_norm": 4.690226078033447,
      "learning_rate": 4.990723167141154e-05,
      "loss": 1.0325,
      "step": 12200
    },
    {
      "epoch": 0.11223446966931892,
      "grad_norm": 4.9280805587768555,
      "learning_rate": 4.9906471275275565e-05,
      "loss": 0.9983,
      "step": 12300
    },
    {
      "epoch": 0.11314694503248413,
      "grad_norm": 5.5166916847229,
      "learning_rate": 4.99057108791396e-05,
      "loss": 0.9813,
      "step": 12400
    },
    {
      "epoch": 0.11405942039564931,
      "grad_norm": 4.43505859375,
      "learning_rate": 4.9904950483003625e-05,
      "loss": 1.001,
      "step": 12500
    },
    {
      "epoch": 0.11497189575881452,
      "grad_norm": 5.304643154144287,
      "learning_rate": 4.9904190086867655e-05,
      "loss": 1.0188,
      "step": 12600
    },
    {
      "epoch": 0.1158843711219797,
      "grad_norm": 4.515888690948486,
      "learning_rate": 4.9903429690731685e-05,
      "loss": 1.012,
      "step": 12700
    },
    {
      "epoch": 0.1167968464851449,
      "grad_norm": 3.9940059185028076,
      "learning_rate": 4.9902669294595715e-05,
      "loss": 1.0452,
      "step": 12800
    },
    {
      "epoch": 0.1177093218483101,
      "grad_norm": 6.7112579345703125,
      "learning_rate": 4.9901908898459746e-05,
      "loss": 1.004,
      "step": 12900
    },
    {
      "epoch": 0.1186217972114753,
      "grad_norm": 4.408282279968262,
      "learning_rate": 4.9901148502323776e-05,
      "loss": 0.9934,
      "step": 13000
    },
    {
      "epoch": 0.11953427257464048,
      "grad_norm": 4.672858238220215,
      "learning_rate": 4.99003881061878e-05,
      "loss": 1.0078,
      "step": 13100
    },
    {
      "epoch": 0.12044674793780567,
      "grad_norm": 4.908210277557373,
      "learning_rate": 4.9899627710051836e-05,
      "loss": 0.9817,
      "step": 13200
    },
    {
      "epoch": 0.12135922330097088,
      "grad_norm": 3.340280771255493,
      "learning_rate": 4.989886731391586e-05,
      "loss": 0.9717,
      "step": 13300
    },
    {
      "epoch": 0.12227169866413606,
      "grad_norm": 4.678239345550537,
      "learning_rate": 4.989810691777989e-05,
      "loss": 0.9935,
      "step": 13400
    },
    {
      "epoch": 0.12318417402730127,
      "grad_norm": 4.557514667510986,
      "learning_rate": 4.989734652164392e-05,
      "loss": 1.0079,
      "step": 13500
    },
    {
      "epoch": 0.12409664939046645,
      "grad_norm": 4.994625091552734,
      "learning_rate": 4.989658612550795e-05,
      "loss": 0.9925,
      "step": 13600
    },
    {
      "epoch": 0.12500912475363166,
      "grad_norm": 4.440512180328369,
      "learning_rate": 4.989582572937197e-05,
      "loss": 1.023,
      "step": 13700
    },
    {
      "epoch": 0.12592160011679684,
      "grad_norm": 4.469070911407471,
      "learning_rate": 4.989506533323601e-05,
      "loss": 1.0155,
      "step": 13800
    },
    {
      "epoch": 0.12683407547996203,
      "grad_norm": 5.337492942810059,
      "learning_rate": 4.989430493710003e-05,
      "loss": 1.0019,
      "step": 13900
    },
    {
      "epoch": 0.12774655084312725,
      "grad_norm": 6.158061504364014,
      "learning_rate": 4.989354454096406e-05,
      "loss": 1.0251,
      "step": 14000
    },
    {
      "epoch": 0.12865902620629244,
      "grad_norm": 5.3162841796875,
      "learning_rate": 4.989278414482809e-05,
      "loss": 0.9913,
      "step": 14100
    },
    {
      "epoch": 0.12957150156945763,
      "grad_norm": 5.055201053619385,
      "learning_rate": 4.989202374869212e-05,
      "loss": 0.9893,
      "step": 14200
    },
    {
      "epoch": 0.1304839769326228,
      "grad_norm": 5.229305744171143,
      "learning_rate": 4.989126335255615e-05,
      "loss": 0.9818,
      "step": 14300
    },
    {
      "epoch": 0.131396452295788,
      "grad_norm": 3.7757158279418945,
      "learning_rate": 4.9890502956420176e-05,
      "loss": 1.0027,
      "step": 14400
    },
    {
      "epoch": 0.13230892765895322,
      "grad_norm": 4.328711032867432,
      "learning_rate": 4.9889742560284206e-05,
      "loss": 0.9793,
      "step": 14500
    },
    {
      "epoch": 0.1332214030221184,
      "grad_norm": 4.261618614196777,
      "learning_rate": 4.9888982164148236e-05,
      "loss": 1.0276,
      "step": 14600
    },
    {
      "epoch": 0.1341338783852836,
      "grad_norm": 4.917252063751221,
      "learning_rate": 4.9888221768012266e-05,
      "loss": 0.9852,
      "step": 14700
    },
    {
      "epoch": 0.13504635374844878,
      "grad_norm": 4.4042158126831055,
      "learning_rate": 4.988746137187629e-05,
      "loss": 0.9975,
      "step": 14800
    },
    {
      "epoch": 0.135958829111614,
      "grad_norm": 4.1395978927612305,
      "learning_rate": 4.9886700975740327e-05,
      "loss": 0.9687,
      "step": 14900
    },
    {
      "epoch": 0.1368713044747792,
      "grad_norm": 5.148288249969482,
      "learning_rate": 4.988594057960435e-05,
      "loss": 0.9816,
      "step": 15000
    },
    {
      "epoch": 0.13778377983794438,
      "grad_norm": 5.380253314971924,
      "learning_rate": 4.988518018346838e-05,
      "loss": 0.9869,
      "step": 15100
    },
    {
      "epoch": 0.13869625520110956,
      "grad_norm": 5.2568793296813965,
      "learning_rate": 4.988441978733241e-05,
      "loss": 0.9855,
      "step": 15200
    },
    {
      "epoch": 0.13960873056427475,
      "grad_norm": 5.154153823852539,
      "learning_rate": 4.988365939119644e-05,
      "loss": 1.0433,
      "step": 15300
    },
    {
      "epoch": 0.14052120592743997,
      "grad_norm": 4.959994316101074,
      "learning_rate": 4.988289899506047e-05,
      "loss": 0.9531,
      "step": 15400
    },
    {
      "epoch": 0.14143368129060516,
      "grad_norm": 4.81374454498291,
      "learning_rate": 4.98821385989245e-05,
      "loss": 0.9847,
      "step": 15500
    },
    {
      "epoch": 0.14234615665377034,
      "grad_norm": 5.042161464691162,
      "learning_rate": 4.9881378202788523e-05,
      "loss": 0.9733,
      "step": 15600
    },
    {
      "epoch": 0.14325863201693553,
      "grad_norm": 4.790406227111816,
      "learning_rate": 4.988061780665256e-05,
      "loss": 0.9656,
      "step": 15700
    },
    {
      "epoch": 0.14417110738010075,
      "grad_norm": 5.431309700012207,
      "learning_rate": 4.9879857410516584e-05,
      "loss": 0.9684,
      "step": 15800
    },
    {
      "epoch": 0.14508358274326594,
      "grad_norm": 4.307794094085693,
      "learning_rate": 4.9879097014380614e-05,
      "loss": 0.9948,
      "step": 15900
    },
    {
      "epoch": 0.14599605810643113,
      "grad_norm": 4.555723190307617,
      "learning_rate": 4.9878336618244644e-05,
      "loss": 1.0287,
      "step": 16000
    },
    {
      "epoch": 0.14690853346959631,
      "grad_norm": 4.739310264587402,
      "learning_rate": 4.9877576222108674e-05,
      "loss": 1.0039,
      "step": 16100
    },
    {
      "epoch": 0.1478210088327615,
      "grad_norm": 4.440958023071289,
      "learning_rate": 4.98768158259727e-05,
      "loss": 0.9645,
      "step": 16200
    },
    {
      "epoch": 0.14873348419592672,
      "grad_norm": 5.031438827514648,
      "learning_rate": 4.9876055429836734e-05,
      "loss": 0.9226,
      "step": 16300
    },
    {
      "epoch": 0.1496459595590919,
      "grad_norm": 5.225769519805908,
      "learning_rate": 4.987529503370076e-05,
      "loss": 0.9583,
      "step": 16400
    },
    {
      "epoch": 0.1505584349222571,
      "grad_norm": 4.468567371368408,
      "learning_rate": 4.987453463756479e-05,
      "loss": 0.9698,
      "step": 16500
    },
    {
      "epoch": 0.15147091028542228,
      "grad_norm": 4.0949835777282715,
      "learning_rate": 4.987377424142882e-05,
      "loss": 0.9613,
      "step": 16600
    },
    {
      "epoch": 0.1523833856485875,
      "grad_norm": 3.7389895915985107,
      "learning_rate": 4.987301384529285e-05,
      "loss": 0.95,
      "step": 16700
    },
    {
      "epoch": 0.1532958610117527,
      "grad_norm": 4.748741626739502,
      "learning_rate": 4.987225344915688e-05,
      "loss": 0.9964,
      "step": 16800
    },
    {
      "epoch": 0.15420833637491788,
      "grad_norm": 4.41128396987915,
      "learning_rate": 4.987149305302091e-05,
      "loss": 1.0043,
      "step": 16900
    },
    {
      "epoch": 0.15512081173808306,
      "grad_norm": 4.348278522491455,
      "learning_rate": 4.987073265688493e-05,
      "loss": 0.9344,
      "step": 17000
    },
    {
      "epoch": 0.15603328710124825,
      "grad_norm": 5.987673282623291,
      "learning_rate": 4.986997226074896e-05,
      "loss": 0.98,
      "step": 17100
    },
    {
      "epoch": 0.15694576246441347,
      "grad_norm": 4.347245216369629,
      "learning_rate": 4.986921186461299e-05,
      "loss": 0.9777,
      "step": 17200
    },
    {
      "epoch": 0.15785823782757866,
      "grad_norm": 5.230325222015381,
      "learning_rate": 4.9868451468477014e-05,
      "loss": 0.9386,
      "step": 17300
    },
    {
      "epoch": 0.15877071319074385,
      "grad_norm": 4.3565850257873535,
      "learning_rate": 4.986769107234105e-05,
      "loss": 1.0125,
      "step": 17400
    },
    {
      "epoch": 0.15968318855390903,
      "grad_norm": 4.163906097412109,
      "learning_rate": 4.9866930676205074e-05,
      "loss": 1.0012,
      "step": 17500
    },
    {
      "epoch": 0.16059566391707425,
      "grad_norm": 5.310068607330322,
      "learning_rate": 4.9866170280069104e-05,
      "loss": 0.9705,
      "step": 17600
    },
    {
      "epoch": 0.16150813928023944,
      "grad_norm": 4.239343166351318,
      "learning_rate": 4.9865409883933135e-05,
      "loss": 0.9551,
      "step": 17700
    },
    {
      "epoch": 0.16242061464340463,
      "grad_norm": 4.655518531799316,
      "learning_rate": 4.9864649487797165e-05,
      "loss": 0.9708,
      "step": 17800
    },
    {
      "epoch": 0.16333309000656981,
      "grad_norm": 4.728153705596924,
      "learning_rate": 4.9863889091661195e-05,
      "loss": 0.9691,
      "step": 17900
    },
    {
      "epoch": 0.16424556536973503,
      "grad_norm": 4.935769557952881,
      "learning_rate": 4.9863128695525225e-05,
      "loss": 0.9774,
      "step": 18000
    },
    {
      "epoch": 0.16515804073290022,
      "grad_norm": 6.4485297203063965,
      "learning_rate": 4.986236829938925e-05,
      "loss": 0.9821,
      "step": 18100
    },
    {
      "epoch": 0.1660705160960654,
      "grad_norm": 4.827169418334961,
      "learning_rate": 4.9861607903253285e-05,
      "loss": 0.9609,
      "step": 18200
    },
    {
      "epoch": 0.1669829914592306,
      "grad_norm": 4.721896648406982,
      "learning_rate": 4.986084750711731e-05,
      "loss": 0.9618,
      "step": 18300
    },
    {
      "epoch": 0.16789546682239578,
      "grad_norm": 4.433897018432617,
      "learning_rate": 4.986008711098134e-05,
      "loss": 0.9827,
      "step": 18400
    },
    {
      "epoch": 0.168807942185561,
      "grad_norm": 4.125970840454102,
      "learning_rate": 4.985932671484537e-05,
      "loss": 0.9506,
      "step": 18500
    },
    {
      "epoch": 0.1697204175487262,
      "grad_norm": 4.994576930999756,
      "learning_rate": 4.98585663187094e-05,
      "loss": 0.9803,
      "step": 18600
    },
    {
      "epoch": 0.17063289291189138,
      "grad_norm": 4.5279059410095215,
      "learning_rate": 4.985780592257343e-05,
      "loss": 0.9384,
      "step": 18700
    },
    {
      "epoch": 0.17154536827505656,
      "grad_norm": 4.213637351989746,
      "learning_rate": 4.985704552643746e-05,
      "loss": 0.9675,
      "step": 18800
    },
    {
      "epoch": 0.17245784363822178,
      "grad_norm": 4.085555553436279,
      "learning_rate": 4.985628513030148e-05,
      "loss": 1.0069,
      "step": 18900
    },
    {
      "epoch": 0.17337031900138697,
      "grad_norm": 2.7393038272857666,
      "learning_rate": 4.985552473416551e-05,
      "loss": 0.9904,
      "step": 19000
    },
    {
      "epoch": 0.17428279436455216,
      "grad_norm": 4.288165092468262,
      "learning_rate": 4.985476433802954e-05,
      "loss": 1.0161,
      "step": 19100
    },
    {
      "epoch": 0.17519526972771735,
      "grad_norm": 4.287474632263184,
      "learning_rate": 4.985400394189357e-05,
      "loss": 0.9334,
      "step": 19200
    },
    {
      "epoch": 0.17610774509088253,
      "grad_norm": 4.42560338973999,
      "learning_rate": 4.98532435457576e-05,
      "loss": 0.9523,
      "step": 19300
    },
    {
      "epoch": 0.17702022045404775,
      "grad_norm": 4.847213268280029,
      "learning_rate": 4.985248314962163e-05,
      "loss": 0.972,
      "step": 19400
    },
    {
      "epoch": 0.17793269581721294,
      "grad_norm": 4.194736480712891,
      "learning_rate": 4.9851722753485655e-05,
      "loss": 0.9288,
      "step": 19500
    },
    {
      "epoch": 0.17884517118037813,
      "grad_norm": 4.979959011077881,
      "learning_rate": 4.985096235734969e-05,
      "loss": 0.9425,
      "step": 19600
    },
    {
      "epoch": 0.17975764654354331,
      "grad_norm": 4.4122467041015625,
      "learning_rate": 4.9850201961213716e-05,
      "loss": 1.0003,
      "step": 19700
    },
    {
      "epoch": 0.18067012190670853,
      "grad_norm": 4.379156589508057,
      "learning_rate": 4.9849441565077746e-05,
      "loss": 0.9642,
      "step": 19800
    },
    {
      "epoch": 0.18158259726987372,
      "grad_norm": 4.459123611450195,
      "learning_rate": 4.9848681168941776e-05,
      "loss": 0.9549,
      "step": 19900
    },
    {
      "epoch": 0.1824950726330389,
      "grad_norm": 4.236456871032715,
      "learning_rate": 4.98479207728058e-05,
      "loss": 0.9636,
      "step": 20000
    },
    {
      "epoch": 0.1834075479962041,
      "grad_norm": 6.026739120483398,
      "learning_rate": 4.9847160376669836e-05,
      "loss": 0.9472,
      "step": 20100
    },
    {
      "epoch": 0.18432002335936928,
      "grad_norm": 4.636136531829834,
      "learning_rate": 4.984639998053386e-05,
      "loss": 0.968,
      "step": 20200
    },
    {
      "epoch": 0.1852324987225345,
      "grad_norm": 4.744104862213135,
      "learning_rate": 4.984563958439789e-05,
      "loss": 0.9252,
      "step": 20300
    },
    {
      "epoch": 0.1861449740856997,
      "grad_norm": 5.12874174118042,
      "learning_rate": 4.984487918826192e-05,
      "loss": 0.9446,
      "step": 20400
    },
    {
      "epoch": 0.18705744944886488,
      "grad_norm": 4.2995381355285645,
      "learning_rate": 4.984411879212595e-05,
      "loss": 0.9699,
      "step": 20500
    },
    {
      "epoch": 0.18796992481203006,
      "grad_norm": 5.998723030090332,
      "learning_rate": 4.984335839598997e-05,
      "loss": 0.9812,
      "step": 20600
    },
    {
      "epoch": 0.18888240017519528,
      "grad_norm": 4.240713596343994,
      "learning_rate": 4.984259799985401e-05,
      "loss": 0.909,
      "step": 20700
    },
    {
      "epoch": 0.18979487553836047,
      "grad_norm": 4.558784008026123,
      "learning_rate": 4.984183760371803e-05,
      "loss": 0.9189,
      "step": 20800
    },
    {
      "epoch": 0.19070735090152566,
      "grad_norm": 4.780670642852783,
      "learning_rate": 4.984107720758206e-05,
      "loss": 0.9759,
      "step": 20900
    },
    {
      "epoch": 0.19161982626469085,
      "grad_norm": 4.176640033721924,
      "learning_rate": 4.984031681144609e-05,
      "loss": 0.9174,
      "step": 21000
    },
    {
      "epoch": 0.19253230162785606,
      "grad_norm": 5.6177825927734375,
      "learning_rate": 4.983955641531012e-05,
      "loss": 0.9485,
      "step": 21100
    },
    {
      "epoch": 0.19344477699102125,
      "grad_norm": 4.602069854736328,
      "learning_rate": 4.983879601917415e-05,
      "loss": 0.9121,
      "step": 21200
    },
    {
      "epoch": 0.19435725235418644,
      "grad_norm": 4.573707580566406,
      "learning_rate": 4.983803562303818e-05,
      "loss": 0.9538,
      "step": 21300
    },
    {
      "epoch": 0.19526972771735163,
      "grad_norm": 4.385225772857666,
      "learning_rate": 4.9837275226902206e-05,
      "loss": 0.9531,
      "step": 21400
    },
    {
      "epoch": 0.19618220308051681,
      "grad_norm": 3.8044681549072266,
      "learning_rate": 4.983651483076624e-05,
      "loss": 0.9285,
      "step": 21500
    },
    {
      "epoch": 0.19709467844368203,
      "grad_norm": 4.769730091094971,
      "learning_rate": 4.9835754434630267e-05,
      "loss": 0.9473,
      "step": 21600
    },
    {
      "epoch": 0.19800715380684722,
      "grad_norm": 4.86564826965332,
      "learning_rate": 4.9834994038494297e-05,
      "loss": 0.9342,
      "step": 21700
    },
    {
      "epoch": 0.1989196291700124,
      "grad_norm": 4.733358383178711,
      "learning_rate": 4.983423364235833e-05,
      "loss": 0.9429,
      "step": 21800
    },
    {
      "epoch": 0.1998321045331776,
      "grad_norm": 5.444200038909912,
      "learning_rate": 4.983347324622236e-05,
      "loss": 0.9197,
      "step": 21900
    },
    {
      "epoch": 0.2007445798963428,
      "grad_norm": 4.371159076690674,
      "learning_rate": 4.983271285008638e-05,
      "loss": 0.9888,
      "step": 22000
    },
    {
      "epoch": 0.201657055259508,
      "grad_norm": 5.206201553344727,
      "learning_rate": 4.983195245395042e-05,
      "loss": 0.9443,
      "step": 22100
    },
    {
      "epoch": 0.2025695306226732,
      "grad_norm": 4.096121788024902,
      "learning_rate": 4.983119205781444e-05,
      "loss": 0.9245,
      "step": 22200
    },
    {
      "epoch": 0.20348200598583838,
      "grad_norm": 4.145411491394043,
      "learning_rate": 4.983043166167847e-05,
      "loss": 0.962,
      "step": 22300
    },
    {
      "epoch": 0.20439448134900357,
      "grad_norm": 4.476589202880859,
      "learning_rate": 4.98296712655425e-05,
      "loss": 0.9347,
      "step": 22400
    },
    {
      "epoch": 0.20530695671216878,
      "grad_norm": 5.975056171417236,
      "learning_rate": 4.982891086940653e-05,
      "loss": 1.0005,
      "step": 22500
    },
    {
      "epoch": 0.20621943207533397,
      "grad_norm": 4.260934829711914,
      "learning_rate": 4.982815047327056e-05,
      "loss": 0.947,
      "step": 22600
    },
    {
      "epoch": 0.20713190743849916,
      "grad_norm": 4.765782356262207,
      "learning_rate": 4.982739007713459e-05,
      "loss": 0.9337,
      "step": 22700
    },
    {
      "epoch": 0.20804438280166435,
      "grad_norm": 4.586453914642334,
      "learning_rate": 4.9826629680998614e-05,
      "loss": 0.9334,
      "step": 22800
    },
    {
      "epoch": 0.20895685816482956,
      "grad_norm": 4.733198642730713,
      "learning_rate": 4.9825869284862644e-05,
      "loss": 0.9126,
      "step": 22900
    },
    {
      "epoch": 0.20986933352799475,
      "grad_norm": 5.027307510375977,
      "learning_rate": 4.9825108888726674e-05,
      "loss": 0.968,
      "step": 23000
    },
    {
      "epoch": 0.21078180889115994,
      "grad_norm": 4.905813694000244,
      "learning_rate": 4.98243484925907e-05,
      "loss": 0.9306,
      "step": 23100
    },
    {
      "epoch": 0.21169428425432513,
      "grad_norm": 4.60032320022583,
      "learning_rate": 4.9823588096454734e-05,
      "loss": 0.9878,
      "step": 23200
    },
    {
      "epoch": 0.21260675961749032,
      "grad_norm": 4.479092597961426,
      "learning_rate": 4.982282770031876e-05,
      "loss": 0.9409,
      "step": 23300
    },
    {
      "epoch": 0.21351923498065553,
      "grad_norm": 4.68436861038208,
      "learning_rate": 4.982206730418279e-05,
      "loss": 0.9165,
      "step": 23400
    },
    {
      "epoch": 0.21443171034382072,
      "grad_norm": 4.769109725952148,
      "learning_rate": 4.982130690804682e-05,
      "loss": 0.9701,
      "step": 23500
    },
    {
      "epoch": 0.2153441857069859,
      "grad_norm": 5.08104133605957,
      "learning_rate": 4.982054651191085e-05,
      "loss": 0.9659,
      "step": 23600
    },
    {
      "epoch": 0.2162566610701511,
      "grad_norm": 5.292693614959717,
      "learning_rate": 4.981978611577488e-05,
      "loss": 0.9766,
      "step": 23700
    },
    {
      "epoch": 0.2171691364333163,
      "grad_norm": 4.983695030212402,
      "learning_rate": 4.981902571963891e-05,
      "loss": 0.9292,
      "step": 23800
    },
    {
      "epoch": 0.2180816117964815,
      "grad_norm": 5.580954551696777,
      "learning_rate": 4.981826532350293e-05,
      "loss": 0.9238,
      "step": 23900
    },
    {
      "epoch": 0.2189940871596467,
      "grad_norm": 4.009984493255615,
      "learning_rate": 4.981750492736697e-05,
      "loss": 0.9483,
      "step": 24000
    },
    {
      "epoch": 0.21990656252281188,
      "grad_norm": 5.208849906921387,
      "learning_rate": 4.981674453123099e-05,
      "loss": 0.9023,
      "step": 24100
    },
    {
      "epoch": 0.22081903788597707,
      "grad_norm": 5.515771389007568,
      "learning_rate": 4.981598413509502e-05,
      "loss": 0.9394,
      "step": 24200
    },
    {
      "epoch": 0.22173151324914228,
      "grad_norm": 3.8593809604644775,
      "learning_rate": 4.981522373895905e-05,
      "loss": 0.9528,
      "step": 24300
    },
    {
      "epoch": 0.22264398861230747,
      "grad_norm": 3.8547582626342773,
      "learning_rate": 4.981446334282308e-05,
      "loss": 0.9442,
      "step": 24400
    },
    {
      "epoch": 0.22355646397547266,
      "grad_norm": 5.0004191398620605,
      "learning_rate": 4.9813702946687105e-05,
      "loss": 0.9978,
      "step": 24500
    },
    {
      "epoch": 0.22446893933863785,
      "grad_norm": 5.427975177764893,
      "learning_rate": 4.981294255055114e-05,
      "loss": 0.9165,
      "step": 24600
    },
    {
      "epoch": 0.22538141470180306,
      "grad_norm": 5.016482830047607,
      "learning_rate": 4.9812182154415165e-05,
      "loss": 0.964,
      "step": 24700
    },
    {
      "epoch": 0.22629389006496825,
      "grad_norm": 4.590939521789551,
      "learning_rate": 4.9811421758279195e-05,
      "loss": 0.9323,
      "step": 24800
    },
    {
      "epoch": 0.22720636542813344,
      "grad_norm": 3.956953763961792,
      "learning_rate": 4.9810661362143225e-05,
      "loss": 0.9577,
      "step": 24900
    },
    {
      "epoch": 0.22811884079129863,
      "grad_norm": 4.2402167320251465,
      "learning_rate": 4.9809900966007255e-05,
      "loss": 0.9393,
      "step": 25000
    },
    {
      "epoch": 0.22903131615446384,
      "grad_norm": 4.6394147872924805,
      "learning_rate": 4.9809140569871285e-05,
      "loss": 0.9399,
      "step": 25100
    },
    {
      "epoch": 0.22994379151762903,
      "grad_norm": 4.147954940795898,
      "learning_rate": 4.9808380173735315e-05,
      "loss": 0.9316,
      "step": 25200
    },
    {
      "epoch": 0.23085626688079422,
      "grad_norm": 3.613168478012085,
      "learning_rate": 4.980761977759934e-05,
      "loss": 0.9552,
      "step": 25300
    },
    {
      "epoch": 0.2317687422439594,
      "grad_norm": 5.127684593200684,
      "learning_rate": 4.9806859381463375e-05,
      "loss": 0.9156,
      "step": 25400
    },
    {
      "epoch": 0.2326812176071246,
      "grad_norm": 3.879422426223755,
      "learning_rate": 4.98060989853274e-05,
      "loss": 0.9222,
      "step": 25500
    },
    {
      "epoch": 0.2335936929702898,
      "grad_norm": 4.789867877960205,
      "learning_rate": 4.980533858919142e-05,
      "loss": 0.9677,
      "step": 25600
    },
    {
      "epoch": 0.234506168333455,
      "grad_norm": 4.181291580200195,
      "learning_rate": 4.980457819305546e-05,
      "loss": 0.9093,
      "step": 25700
    },
    {
      "epoch": 0.2354186436966202,
      "grad_norm": 4.2762370109558105,
      "learning_rate": 4.980381779691948e-05,
      "loss": 0.8947,
      "step": 25800
    },
    {
      "epoch": 0.23633111905978538,
      "grad_norm": 4.019443988800049,
      "learning_rate": 4.980305740078351e-05,
      "loss": 0.9093,
      "step": 25900
    },
    {
      "epoch": 0.2372435944229506,
      "grad_norm": 4.212311267852783,
      "learning_rate": 4.980229700464754e-05,
      "loss": 0.9365,
      "step": 26000
    },
    {
      "epoch": 0.23815606978611578,
      "grad_norm": 4.375726699829102,
      "learning_rate": 4.980153660851157e-05,
      "loss": 0.9183,
      "step": 26100
    },
    {
      "epoch": 0.23906854514928097,
      "grad_norm": 3.765324592590332,
      "learning_rate": 4.98007762123756e-05,
      "loss": 0.9195,
      "step": 26200
    },
    {
      "epoch": 0.23998102051244616,
      "grad_norm": 5.585471153259277,
      "learning_rate": 4.980001581623963e-05,
      "loss": 0.9002,
      "step": 26300
    },
    {
      "epoch": 0.24089349587561135,
      "grad_norm": 4.4977288246154785,
      "learning_rate": 4.9799255420103656e-05,
      "loss": 0.8802,
      "step": 26400
    },
    {
      "epoch": 0.24180597123877656,
      "grad_norm": 4.4167046546936035,
      "learning_rate": 4.979849502396769e-05,
      "loss": 0.9651,
      "step": 26500
    },
    {
      "epoch": 0.24271844660194175,
      "grad_norm": 4.669652462005615,
      "learning_rate": 4.9797734627831716e-05,
      "loss": 0.8902,
      "step": 26600
    },
    {
      "epoch": 0.24363092196510694,
      "grad_norm": 4.6663923263549805,
      "learning_rate": 4.9796974231695746e-05,
      "loss": 0.921,
      "step": 26700
    },
    {
      "epoch": 0.24454339732827213,
      "grad_norm": 4.178906440734863,
      "learning_rate": 4.9796213835559776e-05,
      "loss": 0.9442,
      "step": 26800
    },
    {
      "epoch": 0.24545587269143734,
      "grad_norm": 4.720544338226318,
      "learning_rate": 4.9795453439423806e-05,
      "loss": 0.9473,
      "step": 26900
    },
    {
      "epoch": 0.24636834805460253,
      "grad_norm": 4.748122215270996,
      "learning_rate": 4.979469304328783e-05,
      "loss": 0.9017,
      "step": 27000
    },
    {
      "epoch": 0.24728082341776772,
      "grad_norm": 5.51088285446167,
      "learning_rate": 4.9793932647151866e-05,
      "loss": 0.8987,
      "step": 27100
    },
    {
      "epoch": 0.2481932987809329,
      "grad_norm": 4.481117248535156,
      "learning_rate": 4.979317225101589e-05,
      "loss": 0.9288,
      "step": 27200
    },
    {
      "epoch": 0.2491057741440981,
      "grad_norm": 4.702326774597168,
      "learning_rate": 4.979241185487992e-05,
      "loss": 0.9532,
      "step": 27300
    },
    {
      "epoch": 0.2500182495072633,
      "grad_norm": 4.560130596160889,
      "learning_rate": 4.979165145874395e-05,
      "loss": 0.9278,
      "step": 27400
    },
    {
      "epoch": 0.2509307248704285,
      "grad_norm": 4.8340911865234375,
      "learning_rate": 4.979089106260798e-05,
      "loss": 0.9159,
      "step": 27500
    },
    {
      "epoch": 0.2518432002335937,
      "grad_norm": 4.6527581214904785,
      "learning_rate": 4.979013066647201e-05,
      "loss": 0.89,
      "step": 27600
    },
    {
      "epoch": 0.2527556755967589,
      "grad_norm": 5.054370880126953,
      "learning_rate": 4.978937027033604e-05,
      "loss": 0.9191,
      "step": 27700
    },
    {
      "epoch": 0.25366815095992407,
      "grad_norm": 4.943380355834961,
      "learning_rate": 4.978860987420006e-05,
      "loss": 0.9107,
      "step": 27800
    },
    {
      "epoch": 0.25458062632308925,
      "grad_norm": 4.376575946807861,
      "learning_rate": 4.97878494780641e-05,
      "loss": 0.9439,
      "step": 27900
    },
    {
      "epoch": 0.2554931016862545,
      "grad_norm": 4.658294677734375,
      "learning_rate": 4.978708908192812e-05,
      "loss": 0.9187,
      "step": 28000
    },
    {
      "epoch": 0.2564055770494197,
      "grad_norm": 4.349798202514648,
      "learning_rate": 4.978632868579215e-05,
      "loss": 0.9335,
      "step": 28100
    },
    {
      "epoch": 0.2573180524125849,
      "grad_norm": 4.690248489379883,
      "learning_rate": 4.978556828965618e-05,
      "loss": 0.8806,
      "step": 28200
    },
    {
      "epoch": 0.25823052777575006,
      "grad_norm": 4.783016681671143,
      "learning_rate": 4.978480789352021e-05,
      "loss": 0.9062,
      "step": 28300
    },
    {
      "epoch": 0.25914300313891525,
      "grad_norm": 3.916212558746338,
      "learning_rate": 4.9784047497384237e-05,
      "loss": 0.9299,
      "step": 28400
    },
    {
      "epoch": 0.26005547850208044,
      "grad_norm": 4.54948616027832,
      "learning_rate": 4.978328710124827e-05,
      "loss": 0.9369,
      "step": 28500
    },
    {
      "epoch": 0.2609679538652456,
      "grad_norm": 4.804208755493164,
      "learning_rate": 4.97825267051123e-05,
      "loss": 0.8957,
      "step": 28600
    },
    {
      "epoch": 0.2618804292284108,
      "grad_norm": 4.384028911590576,
      "learning_rate": 4.978176630897633e-05,
      "loss": 0.9187,
      "step": 28700
    },
    {
      "epoch": 0.262792904591576,
      "grad_norm": 4.76761531829834,
      "learning_rate": 4.978100591284036e-05,
      "loss": 0.9129,
      "step": 28800
    },
    {
      "epoch": 0.26370537995474125,
      "grad_norm": 4.6467108726501465,
      "learning_rate": 4.978024551670438e-05,
      "loss": 0.9465,
      "step": 28900
    },
    {
      "epoch": 0.26461785531790644,
      "grad_norm": 3.8399577140808105,
      "learning_rate": 4.977948512056842e-05,
      "loss": 0.9288,
      "step": 29000
    },
    {
      "epoch": 0.2655303306810716,
      "grad_norm": 4.463375091552734,
      "learning_rate": 4.977872472443244e-05,
      "loss": 0.9056,
      "step": 29100
    },
    {
      "epoch": 0.2664428060442368,
      "grad_norm": 4.988485336303711,
      "learning_rate": 4.977796432829647e-05,
      "loss": 0.899,
      "step": 29200
    },
    {
      "epoch": 0.267355281407402,
      "grad_norm": 4.688909530639648,
      "learning_rate": 4.97772039321605e-05,
      "loss": 0.8864,
      "step": 29300
    },
    {
      "epoch": 0.2682677567705672,
      "grad_norm": 4.3152995109558105,
      "learning_rate": 4.977644353602453e-05,
      "loss": 0.9072,
      "step": 29400
    },
    {
      "epoch": 0.2691802321337324,
      "grad_norm": 4.3116350173950195,
      "learning_rate": 4.9775683139888554e-05,
      "loss": 0.9427,
      "step": 29500
    },
    {
      "epoch": 0.27009270749689757,
      "grad_norm": 4.335596561431885,
      "learning_rate": 4.977492274375259e-05,
      "loss": 0.9206,
      "step": 29600
    },
    {
      "epoch": 0.27100518286006275,
      "grad_norm": 4.467284202575684,
      "learning_rate": 4.9774162347616614e-05,
      "loss": 0.9607,
      "step": 29700
    },
    {
      "epoch": 0.271917658223228,
      "grad_norm": 3.9060189723968506,
      "learning_rate": 4.9773401951480644e-05,
      "loss": 0.8862,
      "step": 29800
    },
    {
      "epoch": 0.2728301335863932,
      "grad_norm": 4.768402576446533,
      "learning_rate": 4.9772641555344674e-05,
      "loss": 0.8857,
      "step": 29900
    },
    {
      "epoch": 0.2737426089495584,
      "grad_norm": 4.459762096405029,
      "learning_rate": 4.9771881159208704e-05,
      "loss": 0.9054,
      "step": 30000
    },
    {
      "epoch": 0.27465508431272356,
      "grad_norm": 4.653633117675781,
      "learning_rate": 4.9771120763072734e-05,
      "loss": 0.9066,
      "step": 30100
    },
    {
      "epoch": 0.27556755967588875,
      "grad_norm": 4.18367338180542,
      "learning_rate": 4.9770360366936764e-05,
      "loss": 0.9193,
      "step": 30200
    },
    {
      "epoch": 0.27648003503905394,
      "grad_norm": 4.825470924377441,
      "learning_rate": 4.976959997080079e-05,
      "loss": 0.9371,
      "step": 30300
    },
    {
      "epoch": 0.27739251040221913,
      "grad_norm": 4.159131050109863,
      "learning_rate": 4.9768839574664824e-05,
      "loss": 0.9191,
      "step": 30400
    },
    {
      "epoch": 0.2783049857653843,
      "grad_norm": 4.357705116271973,
      "learning_rate": 4.976807917852885e-05,
      "loss": 0.9151,
      "step": 30500
    },
    {
      "epoch": 0.2792174611285495,
      "grad_norm": 4.516878128051758,
      "learning_rate": 4.976731878239288e-05,
      "loss": 0.9082,
      "step": 30600
    },
    {
      "epoch": 0.28012993649171475,
      "grad_norm": 4.130741596221924,
      "learning_rate": 4.976655838625691e-05,
      "loss": 0.9348,
      "step": 30700
    },
    {
      "epoch": 0.28104241185487994,
      "grad_norm": 5.64365816116333,
      "learning_rate": 4.976579799012094e-05,
      "loss": 0.9077,
      "step": 30800
    },
    {
      "epoch": 0.2819548872180451,
      "grad_norm": 5.270657539367676,
      "learning_rate": 4.976503759398496e-05,
      "loss": 0.9161,
      "step": 30900
    },
    {
      "epoch": 0.2828673625812103,
      "grad_norm": 4.2988433837890625,
      "learning_rate": 4.9764277197849e-05,
      "loss": 0.9027,
      "step": 31000
    },
    {
      "epoch": 0.2837798379443755,
      "grad_norm": 4.7207159996032715,
      "learning_rate": 4.976351680171302e-05,
      "loss": 0.9297,
      "step": 31100
    },
    {
      "epoch": 0.2846923133075407,
      "grad_norm": 4.913229465484619,
      "learning_rate": 4.976275640557705e-05,
      "loss": 0.9137,
      "step": 31200
    },
    {
      "epoch": 0.2856047886707059,
      "grad_norm": 5.042914867401123,
      "learning_rate": 4.976199600944108e-05,
      "loss": 0.9018,
      "step": 31300
    },
    {
      "epoch": 0.28651726403387107,
      "grad_norm": 4.106817245483398,
      "learning_rate": 4.9761235613305105e-05,
      "loss": 0.8918,
      "step": 31400
    },
    {
      "epoch": 0.28742973939703625,
      "grad_norm": 4.705385208129883,
      "learning_rate": 4.976047521716914e-05,
      "loss": 0.9187,
      "step": 31500
    },
    {
      "epoch": 0.2883422147602015,
      "grad_norm": 4.52363920211792,
      "learning_rate": 4.9759714821033165e-05,
      "loss": 0.9185,
      "step": 31600
    },
    {
      "epoch": 0.2892546901233667,
      "grad_norm": 6.609990119934082,
      "learning_rate": 4.9758954424897195e-05,
      "loss": 0.8964,
      "step": 31700
    },
    {
      "epoch": 0.2901671654865319,
      "grad_norm": 4.282615661621094,
      "learning_rate": 4.9758194028761225e-05,
      "loss": 0.849,
      "step": 31800
    },
    {
      "epoch": 0.29107964084969706,
      "grad_norm": 5.200261116027832,
      "learning_rate": 4.9757433632625255e-05,
      "loss": 0.8818,
      "step": 31900
    },
    {
      "epoch": 0.29199211621286225,
      "grad_norm": 4.476046085357666,
      "learning_rate": 4.9756673236489285e-05,
      "loss": 0.8459,
      "step": 32000
    },
    {
      "epoch": 0.29290459157602744,
      "grad_norm": 4.471263885498047,
      "learning_rate": 4.9755912840353315e-05,
      "loss": 0.8953,
      "step": 32100
    },
    {
      "epoch": 0.29381706693919263,
      "grad_norm": 4.071052074432373,
      "learning_rate": 4.975515244421734e-05,
      "loss": 0.8775,
      "step": 32200
    },
    {
      "epoch": 0.2947295423023578,
      "grad_norm": 4.314451694488525,
      "learning_rate": 4.975439204808137e-05,
      "loss": 0.9398,
      "step": 32300
    },
    {
      "epoch": 0.295642017665523,
      "grad_norm": 4.692590236663818,
      "learning_rate": 4.97536316519454e-05,
      "loss": 0.9109,
      "step": 32400
    },
    {
      "epoch": 0.29655449302868825,
      "grad_norm": 5.666123867034912,
      "learning_rate": 4.975287125580943e-05,
      "loss": 0.9275,
      "step": 32500
    },
    {
      "epoch": 0.29746696839185344,
      "grad_norm": 4.961183071136475,
      "learning_rate": 4.975211085967346e-05,
      "loss": 0.8861,
      "step": 32600
    },
    {
      "epoch": 0.2983794437550186,
      "grad_norm": 4.634074687957764,
      "learning_rate": 4.975135046353749e-05,
      "loss": 0.8925,
      "step": 32700
    },
    {
      "epoch": 0.2992919191181838,
      "grad_norm": 4.925207138061523,
      "learning_rate": 4.975059006740151e-05,
      "loss": 0.9123,
      "step": 32800
    },
    {
      "epoch": 0.300204394481349,
      "grad_norm": 4.109436988830566,
      "learning_rate": 4.974982967126555e-05,
      "loss": 0.9189,
      "step": 32900
    },
    {
      "epoch": 0.3011168698445142,
      "grad_norm": 4.280610084533691,
      "learning_rate": 4.974906927512957e-05,
      "loss": 0.8669,
      "step": 33000
    },
    {
      "epoch": 0.3020293452076794,
      "grad_norm": 4.4800639152526855,
      "learning_rate": 4.97483088789936e-05,
      "loss": 0.9592,
      "step": 33100
    },
    {
      "epoch": 0.30294182057084457,
      "grad_norm": 4.38597297668457,
      "learning_rate": 4.974754848285763e-05,
      "loss": 0.9264,
      "step": 33200
    },
    {
      "epoch": 0.30385429593400975,
      "grad_norm": 4.9118971824646,
      "learning_rate": 4.974678808672166e-05,
      "loss": 0.8853,
      "step": 33300
    },
    {
      "epoch": 0.304766771297175,
      "grad_norm": 4.53200101852417,
      "learning_rate": 4.974602769058569e-05,
      "loss": 0.8636,
      "step": 33400
    },
    {
      "epoch": 0.3056792466603402,
      "grad_norm": 4.4600043296813965,
      "learning_rate": 4.974526729444972e-05,
      "loss": 0.903,
      "step": 33500
    },
    {
      "epoch": 0.3065917220235054,
      "grad_norm": 4.253835678100586,
      "learning_rate": 4.9744506898313746e-05,
      "loss": 0.9222,
      "step": 33600
    },
    {
      "epoch": 0.30750419738667056,
      "grad_norm": 4.862013816833496,
      "learning_rate": 4.974374650217778e-05,
      "loss": 0.9217,
      "step": 33700
    },
    {
      "epoch": 0.30841667274983575,
      "grad_norm": 4.580506801605225,
      "learning_rate": 4.9742986106041806e-05,
      "loss": 0.9116,
      "step": 33800
    },
    {
      "epoch": 0.30932914811300094,
      "grad_norm": 4.423431873321533,
      "learning_rate": 4.9742225709905836e-05,
      "loss": 0.92,
      "step": 33900
    },
    {
      "epoch": 0.31024162347616613,
      "grad_norm": 5.329095363616943,
      "learning_rate": 4.9741465313769866e-05,
      "loss": 0.9626,
      "step": 34000
    },
    {
      "epoch": 0.3111540988393313,
      "grad_norm": 4.764700412750244,
      "learning_rate": 4.974070491763389e-05,
      "loss": 0.8949,
      "step": 34100
    },
    {
      "epoch": 0.3120665742024965,
      "grad_norm": 4.142714977264404,
      "learning_rate": 4.973994452149792e-05,
      "loss": 0.8719,
      "step": 34200
    },
    {
      "epoch": 0.31297904956566175,
      "grad_norm": 3.198824405670166,
      "learning_rate": 4.973918412536195e-05,
      "loss": 0.8895,
      "step": 34300
    },
    {
      "epoch": 0.31389152492882694,
      "grad_norm": 4.480228424072266,
      "learning_rate": 4.973842372922598e-05,
      "loss": 0.9172,
      "step": 34400
    },
    {
      "epoch": 0.3148040002919921,
      "grad_norm": 5.159276962280273,
      "learning_rate": 4.973766333309001e-05,
      "loss": 0.9189,
      "step": 34500
    },
    {
      "epoch": 0.3157164756551573,
      "grad_norm": 5.297529697418213,
      "learning_rate": 4.973690293695404e-05,
      "loss": 0.8928,
      "step": 34600
    },
    {
      "epoch": 0.3166289510183225,
      "grad_norm": 5.341277122497559,
      "learning_rate": 4.973614254081806e-05,
      "loss": 0.8936,
      "step": 34700
    },
    {
      "epoch": 0.3175414263814877,
      "grad_norm": 2.4304521083831787,
      "learning_rate": 4.97353821446821e-05,
      "loss": 0.8626,
      "step": 34800
    },
    {
      "epoch": 0.3184539017446529,
      "grad_norm": 4.8994832038879395,
      "learning_rate": 4.973462174854612e-05,
      "loss": 0.8871,
      "step": 34900
    },
    {
      "epoch": 0.31936637710781807,
      "grad_norm": 4.753241539001465,
      "learning_rate": 4.973386135241015e-05,
      "loss": 0.8844,
      "step": 35000
    },
    {
      "epoch": 0.3202788524709833,
      "grad_norm": 4.54789400100708,
      "learning_rate": 4.973310095627418e-05,
      "loss": 0.867,
      "step": 35100
    },
    {
      "epoch": 0.3211913278341485,
      "grad_norm": 4.878993034362793,
      "learning_rate": 4.973234056013821e-05,
      "loss": 0.9265,
      "step": 35200
    },
    {
      "epoch": 0.3221038031973137,
      "grad_norm": 4.349714279174805,
      "learning_rate": 4.973158016400224e-05,
      "loss": 0.9152,
      "step": 35300
    },
    {
      "epoch": 0.3230162785604789,
      "grad_norm": 5.729046821594238,
      "learning_rate": 4.9730819767866273e-05,
      "loss": 0.9415,
      "step": 35400
    },
    {
      "epoch": 0.32392875392364406,
      "grad_norm": 4.71878719329834,
      "learning_rate": 4.97300593717303e-05,
      "loss": 0.9281,
      "step": 35500
    },
    {
      "epoch": 0.32484122928680925,
      "grad_norm": 3.7945070266723633,
      "learning_rate": 4.972929897559433e-05,
      "loss": 0.8837,
      "step": 35600
    },
    {
      "epoch": 0.32575370464997444,
      "grad_norm": 4.2650370597839355,
      "learning_rate": 4.972853857945836e-05,
      "loss": 0.8979,
      "step": 35700
    },
    {
      "epoch": 0.32666618001313963,
      "grad_norm": 4.932172775268555,
      "learning_rate": 4.972777818332239e-05,
      "loss": 0.8952,
      "step": 35800
    },
    {
      "epoch": 0.3275786553763048,
      "grad_norm": 4.464069843292236,
      "learning_rate": 4.972701778718642e-05,
      "loss": 0.8673,
      "step": 35900
    },
    {
      "epoch": 0.32849113073947006,
      "grad_norm": 4.6190385818481445,
      "learning_rate": 4.972625739105045e-05,
      "loss": 0.9257,
      "step": 36000
    },
    {
      "epoch": 0.32940360610263525,
      "grad_norm": 4.717801570892334,
      "learning_rate": 4.972549699491447e-05,
      "loss": 0.8837,
      "step": 36100
    },
    {
      "epoch": 0.33031608146580044,
      "grad_norm": 3.7096059322357178,
      "learning_rate": 4.972473659877851e-05,
      "loss": 0.9212,
      "step": 36200
    },
    {
      "epoch": 0.3312285568289656,
      "grad_norm": 4.184340000152588,
      "learning_rate": 4.972397620264253e-05,
      "loss": 0.8652,
      "step": 36300
    },
    {
      "epoch": 0.3321410321921308,
      "grad_norm": 3.910094738006592,
      "learning_rate": 4.972321580650656e-05,
      "loss": 0.8887,
      "step": 36400
    },
    {
      "epoch": 0.333053507555296,
      "grad_norm": 4.4064788818359375,
      "learning_rate": 4.972245541037059e-05,
      "loss": 0.8979,
      "step": 36500
    },
    {
      "epoch": 0.3339659829184612,
      "grad_norm": 5.048665523529053,
      "learning_rate": 4.972169501423462e-05,
      "loss": 0.9058,
      "step": 36600
    },
    {
      "epoch": 0.3348784582816264,
      "grad_norm": 4.531785011291504,
      "learning_rate": 4.9720934618098644e-05,
      "loss": 0.9083,
      "step": 36700
    },
    {
      "epoch": 0.33579093364479157,
      "grad_norm": 4.709320068359375,
      "learning_rate": 4.972017422196268e-05,
      "loss": 0.9065,
      "step": 36800
    },
    {
      "epoch": 0.3367034090079568,
      "grad_norm": 4.2103047370910645,
      "learning_rate": 4.9719413825826704e-05,
      "loss": 0.8597,
      "step": 36900
    },
    {
      "epoch": 0.337615884371122,
      "grad_norm": 4.6175408363342285,
      "learning_rate": 4.9718653429690734e-05,
      "loss": 0.9488,
      "step": 37000
    },
    {
      "epoch": 0.3385283597342872,
      "grad_norm": 4.566606044769287,
      "learning_rate": 4.9717893033554764e-05,
      "loss": 0.9019,
      "step": 37100
    },
    {
      "epoch": 0.3394408350974524,
      "grad_norm": 3.789228677749634,
      "learning_rate": 4.971713263741879e-05,
      "loss": 0.9144,
      "step": 37200
    },
    {
      "epoch": 0.34035331046061756,
      "grad_norm": 5.7482452392578125,
      "learning_rate": 4.9716372241282824e-05,
      "loss": 0.9372,
      "step": 37300
    },
    {
      "epoch": 0.34126578582378275,
      "grad_norm": 4.965156078338623,
      "learning_rate": 4.971561184514685e-05,
      "loss": 0.8538,
      "step": 37400
    },
    {
      "epoch": 0.34217826118694794,
      "grad_norm": 5.5746612548828125,
      "learning_rate": 4.971485144901088e-05,
      "loss": 0.8689,
      "step": 37500
    },
    {
      "epoch": 0.34309073655011313,
      "grad_norm": 4.351333141326904,
      "learning_rate": 4.971409105287491e-05,
      "loss": 0.8968,
      "step": 37600
    },
    {
      "epoch": 0.3440032119132783,
      "grad_norm": 4.176421642303467,
      "learning_rate": 4.971333065673894e-05,
      "loss": 0.9027,
      "step": 37700
    },
    {
      "epoch": 0.34491568727644356,
      "grad_norm": 4.266711711883545,
      "learning_rate": 4.971257026060296e-05,
      "loss": 0.8639,
      "step": 37800
    },
    {
      "epoch": 0.34582816263960875,
      "grad_norm": 4.173541069030762,
      "learning_rate": 4.9711809864467e-05,
      "loss": 0.9033,
      "step": 37900
    },
    {
      "epoch": 0.34674063800277394,
      "grad_norm": 3.9787144660949707,
      "learning_rate": 4.971104946833102e-05,
      "loss": 0.9045,
      "step": 38000
    },
    {
      "epoch": 0.3476531133659391,
      "grad_norm": 4.89572286605835,
      "learning_rate": 4.971028907219505e-05,
      "loss": 0.9167,
      "step": 38100
    },
    {
      "epoch": 0.3485655887291043,
      "grad_norm": 3.9807584285736084,
      "learning_rate": 4.970952867605908e-05,
      "loss": 0.9045,
      "step": 38200
    },
    {
      "epoch": 0.3494780640922695,
      "grad_norm": 4.6888651847839355,
      "learning_rate": 4.970876827992311e-05,
      "loss": 0.8575,
      "step": 38300
    },
    {
      "epoch": 0.3503905394554347,
      "grad_norm": 5.020137786865234,
      "learning_rate": 4.970800788378714e-05,
      "loss": 0.899,
      "step": 38400
    },
    {
      "epoch": 0.3513030148185999,
      "grad_norm": 5.1826491355896,
      "learning_rate": 4.970724748765117e-05,
      "loss": 0.8992,
      "step": 38500
    },
    {
      "epoch": 0.35221549018176507,
      "grad_norm": 3.8036251068115234,
      "learning_rate": 4.9706487091515195e-05,
      "loss": 0.9153,
      "step": 38600
    },
    {
      "epoch": 0.3531279655449303,
      "grad_norm": 4.78696870803833,
      "learning_rate": 4.970572669537923e-05,
      "loss": 0.9076,
      "step": 38700
    },
    {
      "epoch": 0.3540404409080955,
      "grad_norm": 4.6873321533203125,
      "learning_rate": 4.9704966299243255e-05,
      "loss": 0.8777,
      "step": 38800
    },
    {
      "epoch": 0.3549529162712607,
      "grad_norm": 4.991820335388184,
      "learning_rate": 4.9704205903107285e-05,
      "loss": 0.8797,
      "step": 38900
    },
    {
      "epoch": 0.3558653916344259,
      "grad_norm": 4.059432506561279,
      "learning_rate": 4.9703445506971315e-05,
      "loss": 0.9207,
      "step": 39000
    },
    {
      "epoch": 0.35677786699759106,
      "grad_norm": 4.309594631195068,
      "learning_rate": 4.9702685110835345e-05,
      "loss": 0.9281,
      "step": 39100
    },
    {
      "epoch": 0.35769034236075625,
      "grad_norm": 4.714588642120361,
      "learning_rate": 4.970192471469937e-05,
      "loss": 0.9001,
      "step": 39200
    },
    {
      "epoch": 0.35860281772392144,
      "grad_norm": 4.999495506286621,
      "learning_rate": 4.9701164318563405e-05,
      "loss": 0.9152,
      "step": 39300
    },
    {
      "epoch": 0.35951529308708663,
      "grad_norm": 5.306675434112549,
      "learning_rate": 4.970040392242743e-05,
      "loss": 0.8891,
      "step": 39400
    },
    {
      "epoch": 0.3604277684502518,
      "grad_norm": 4.625689506530762,
      "learning_rate": 4.969964352629146e-05,
      "loss": 0.8928,
      "step": 39500
    },
    {
      "epoch": 0.36134024381341706,
      "grad_norm": 5.034453868865967,
      "learning_rate": 4.969888313015549e-05,
      "loss": 0.8674,
      "step": 39600
    },
    {
      "epoch": 0.36225271917658225,
      "grad_norm": 5.5609636306762695,
      "learning_rate": 4.969812273401952e-05,
      "loss": 0.8806,
      "step": 39700
    },
    {
      "epoch": 0.36316519453974744,
      "grad_norm": 4.371728897094727,
      "learning_rate": 4.969736233788355e-05,
      "loss": 0.8712,
      "step": 39800
    },
    {
      "epoch": 0.3640776699029126,
      "grad_norm": 3.9664719104766846,
      "learning_rate": 4.969660194174757e-05,
      "loss": 0.8901,
      "step": 39900
    },
    {
      "epoch": 0.3649901452660778,
      "grad_norm": 4.0825676918029785,
      "learning_rate": 4.96958415456116e-05,
      "loss": 0.936,
      "step": 40000
    },
    {
      "epoch": 0.365902620629243,
      "grad_norm": 5.042921543121338,
      "learning_rate": 4.969508114947563e-05,
      "loss": 0.872,
      "step": 40100
    },
    {
      "epoch": 0.3668150959924082,
      "grad_norm": 4.428056240081787,
      "learning_rate": 4.969432075333966e-05,
      "loss": 0.8988,
      "step": 40200
    },
    {
      "epoch": 0.3677275713555734,
      "grad_norm": 5.246150970458984,
      "learning_rate": 4.9693560357203686e-05,
      "loss": 0.8383,
      "step": 40300
    },
    {
      "epoch": 0.36864004671873857,
      "grad_norm": 4.217075347900391,
      "learning_rate": 4.969279996106772e-05,
      "loss": 0.8547,
      "step": 40400
    },
    {
      "epoch": 0.3695525220819038,
      "grad_norm": 5.625909805297852,
      "learning_rate": 4.9692039564931746e-05,
      "loss": 0.9019,
      "step": 40500
    },
    {
      "epoch": 0.370464997445069,
      "grad_norm": 4.623375415802002,
      "learning_rate": 4.9691279168795776e-05,
      "loss": 0.865,
      "step": 40600
    },
    {
      "epoch": 0.3713774728082342,
      "grad_norm": 4.710708141326904,
      "learning_rate": 4.9690518772659806e-05,
      "loss": 0.881,
      "step": 40700
    },
    {
      "epoch": 0.3722899481713994,
      "grad_norm": 4.697977066040039,
      "learning_rate": 4.9689758376523836e-05,
      "loss": 0.8938,
      "step": 40800
    },
    {
      "epoch": 0.37320242353456456,
      "grad_norm": 4.718133926391602,
      "learning_rate": 4.9688997980387866e-05,
      "loss": 0.8936,
      "step": 40900
    },
    {
      "epoch": 0.37411489889772975,
      "grad_norm": 3.9947893619537354,
      "learning_rate": 4.9688237584251896e-05,
      "loss": 0.8304,
      "step": 41000
    },
    {
      "epoch": 0.37502737426089494,
      "grad_norm": 4.798572540283203,
      "learning_rate": 4.968747718811592e-05,
      "loss": 0.8499,
      "step": 41100
    },
    {
      "epoch": 0.37593984962406013,
      "grad_norm": 4.331518650054932,
      "learning_rate": 4.9686716791979956e-05,
      "loss": 0.9083,
      "step": 41200
    },
    {
      "epoch": 0.3768523249872253,
      "grad_norm": 4.585838317871094,
      "learning_rate": 4.968595639584398e-05,
      "loss": 0.897,
      "step": 41300
    },
    {
      "epoch": 0.37776480035039056,
      "grad_norm": 4.866661071777344,
      "learning_rate": 4.968519599970801e-05,
      "loss": 0.8664,
      "step": 41400
    },
    {
      "epoch": 0.37867727571355575,
      "grad_norm": 4.746554851531982,
      "learning_rate": 4.968443560357204e-05,
      "loss": 0.9252,
      "step": 41500
    },
    {
      "epoch": 0.37958975107672094,
      "grad_norm": 4.368016242980957,
      "learning_rate": 4.968367520743607e-05,
      "loss": 0.9125,
      "step": 41600
    },
    {
      "epoch": 0.3805022264398861,
      "grad_norm": 4.517402648925781,
      "learning_rate": 4.968291481130009e-05,
      "loss": 0.9127,
      "step": 41700
    },
    {
      "epoch": 0.3814147018030513,
      "grad_norm": 5.628933906555176,
      "learning_rate": 4.968215441516413e-05,
      "loss": 0.8985,
      "step": 41800
    },
    {
      "epoch": 0.3823271771662165,
      "grad_norm": 4.5241007804870605,
      "learning_rate": 4.968139401902815e-05,
      "loss": 0.8912,
      "step": 41900
    },
    {
      "epoch": 0.3832396525293817,
      "grad_norm": 4.568142890930176,
      "learning_rate": 4.9680633622892183e-05,
      "loss": 0.8903,
      "step": 42000
    },
    {
      "epoch": 0.3841521278925469,
      "grad_norm": 4.476412296295166,
      "learning_rate": 4.9679873226756213e-05,
      "loss": 0.857,
      "step": 42100
    },
    {
      "epoch": 0.3850646032557121,
      "grad_norm": 2.7831857204437256,
      "learning_rate": 4.9679112830620244e-05,
      "loss": 0.8769,
      "step": 42200
    },
    {
      "epoch": 0.3859770786188773,
      "grad_norm": 5.24916934967041,
      "learning_rate": 4.9678352434484274e-05,
      "loss": 0.9196,
      "step": 42300
    },
    {
      "epoch": 0.3868895539820425,
      "grad_norm": 4.86456298828125,
      "learning_rate": 4.9677592038348304e-05,
      "loss": 0.9068,
      "step": 42400
    },
    {
      "epoch": 0.3878020293452077,
      "grad_norm": 4.571994304656982,
      "learning_rate": 4.967683164221233e-05,
      "loss": 0.8774,
      "step": 42500
    },
    {
      "epoch": 0.3887145047083729,
      "grad_norm": 4.412722110748291,
      "learning_rate": 4.9676071246076364e-05,
      "loss": 0.8524,
      "step": 42600
    },
    {
      "epoch": 0.38962698007153806,
      "grad_norm": 4.157369613647461,
      "learning_rate": 4.967531084994039e-05,
      "loss": 0.8435,
      "step": 42700
    },
    {
      "epoch": 0.39053945543470325,
      "grad_norm": 4.4892120361328125,
      "learning_rate": 4.967455045380441e-05,
      "loss": 0.8702,
      "step": 42800
    },
    {
      "epoch": 0.39145193079786844,
      "grad_norm": 4.505863189697266,
      "learning_rate": 4.967379005766845e-05,
      "loss": 0.8508,
      "step": 42900
    },
    {
      "epoch": 0.39236440616103363,
      "grad_norm": 4.1829423904418945,
      "learning_rate": 4.967302966153247e-05,
      "loss": 0.8671,
      "step": 43000
    },
    {
      "epoch": 0.3932768815241989,
      "grad_norm": 4.7080278396606445,
      "learning_rate": 4.96722692653965e-05,
      "loss": 0.8897,
      "step": 43100
    },
    {
      "epoch": 0.39418935688736406,
      "grad_norm": 4.643974304199219,
      "learning_rate": 4.967150886926053e-05,
      "loss": 0.8633,
      "step": 43200
    },
    {
      "epoch": 0.39510183225052925,
      "grad_norm": 4.434110641479492,
      "learning_rate": 4.967074847312456e-05,
      "loss": 0.832,
      "step": 43300
    },
    {
      "epoch": 0.39601430761369444,
      "grad_norm": 4.552793025970459,
      "learning_rate": 4.966998807698859e-05,
      "loss": 0.8829,
      "step": 43400
    },
    {
      "epoch": 0.3969267829768596,
      "grad_norm": 5.2778520584106445,
      "learning_rate": 4.966922768085262e-05,
      "loss": 0.883,
      "step": 43500
    },
    {
      "epoch": 0.3978392583400248,
      "grad_norm": 4.432491779327393,
      "learning_rate": 4.9668467284716644e-05,
      "loss": 0.8835,
      "step": 43600
    },
    {
      "epoch": 0.39875173370319,
      "grad_norm": 4.510258197784424,
      "learning_rate": 4.966770688858068e-05,
      "loss": 0.8836,
      "step": 43700
    },
    {
      "epoch": 0.3996642090663552,
      "grad_norm": 4.633664608001709,
      "learning_rate": 4.9666946492444704e-05,
      "loss": 0.862,
      "step": 43800
    },
    {
      "epoch": 0.4005766844295204,
      "grad_norm": 4.653357982635498,
      "learning_rate": 4.9666186096308734e-05,
      "loss": 0.9026,
      "step": 43900
    },
    {
      "epoch": 0.4014891597926856,
      "grad_norm": 4.031081199645996,
      "learning_rate": 4.9665425700172764e-05,
      "loss": 0.8712,
      "step": 44000
    },
    {
      "epoch": 0.4024016351558508,
      "grad_norm": 4.924448490142822,
      "learning_rate": 4.9664665304036794e-05,
      "loss": 0.8702,
      "step": 44100
    },
    {
      "epoch": 0.403314110519016,
      "grad_norm": 3.8846588134765625,
      "learning_rate": 4.9663904907900825e-05,
      "loss": 0.8729,
      "step": 44200
    },
    {
      "epoch": 0.4042265858821812,
      "grad_norm": 4.277509689331055,
      "learning_rate": 4.9663144511764855e-05,
      "loss": 0.8981,
      "step": 44300
    },
    {
      "epoch": 0.4051390612453464,
      "grad_norm": 4.174125671386719,
      "learning_rate": 4.966238411562888e-05,
      "loss": 0.8432,
      "step": 44400
    },
    {
      "epoch": 0.40605153660851157,
      "grad_norm": 4.5955705642700195,
      "learning_rate": 4.966162371949291e-05,
      "loss": 0.8348,
      "step": 44500
    },
    {
      "epoch": 0.40696401197167675,
      "grad_norm": 4.392089366912842,
      "learning_rate": 4.966086332335694e-05,
      "loss": 0.8936,
      "step": 44600
    },
    {
      "epoch": 0.40787648733484194,
      "grad_norm": 4.425148963928223,
      "learning_rate": 4.966010292722097e-05,
      "loss": 0.8979,
      "step": 44700
    },
    {
      "epoch": 0.40878896269800713,
      "grad_norm": 4.4380340576171875,
      "learning_rate": 4.9659342531085e-05,
      "loss": 0.8273,
      "step": 44800
    },
    {
      "epoch": 0.4097014380611724,
      "grad_norm": 3.6492795944213867,
      "learning_rate": 4.965858213494903e-05,
      "loss": 0.897,
      "step": 44900
    },
    {
      "epoch": 0.41061391342433756,
      "grad_norm": 3.964513063430786,
      "learning_rate": 4.965782173881305e-05,
      "loss": 0.8875,
      "step": 45000
    },
    {
      "epoch": 0.41152638878750275,
      "grad_norm": 4.373297691345215,
      "learning_rate": 4.965706134267709e-05,
      "loss": 0.9014,
      "step": 45100
    },
    {
      "epoch": 0.41243886415066794,
      "grad_norm": 4.890140056610107,
      "learning_rate": 4.965630094654111e-05,
      "loss": 0.8874,
      "step": 45200
    },
    {
      "epoch": 0.4133513395138331,
      "grad_norm": 4.095095157623291,
      "learning_rate": 4.965554055040514e-05,
      "loss": 0.8891,
      "step": 45300
    },
    {
      "epoch": 0.4142638148769983,
      "grad_norm": 4.763930320739746,
      "learning_rate": 4.965478015426917e-05,
      "loss": 0.917,
      "step": 45400
    },
    {
      "epoch": 0.4151762902401635,
      "grad_norm": 5.133937358856201,
      "learning_rate": 4.9654019758133195e-05,
      "loss": 0.8691,
      "step": 45500
    },
    {
      "epoch": 0.4160887656033287,
      "grad_norm": 4.732337951660156,
      "learning_rate": 4.965325936199723e-05,
      "loss": 0.8811,
      "step": 45600
    },
    {
      "epoch": 0.4170012409664939,
      "grad_norm": 4.955063819885254,
      "learning_rate": 4.9652498965861255e-05,
      "loss": 0.9,
      "step": 45700
    },
    {
      "epoch": 0.4179137163296591,
      "grad_norm": 5.280301570892334,
      "learning_rate": 4.9651738569725285e-05,
      "loss": 0.8001,
      "step": 45800
    },
    {
      "epoch": 0.4188261916928243,
      "grad_norm": 4.235603332519531,
      "learning_rate": 4.9650978173589315e-05,
      "loss": 0.8838,
      "step": 45900
    },
    {
      "epoch": 0.4197386670559895,
      "grad_norm": 5.241540908813477,
      "learning_rate": 4.9650217777453345e-05,
      "loss": 0.8626,
      "step": 46000
    },
    {
      "epoch": 0.4206511424191547,
      "grad_norm": 3.946324110031128,
      "learning_rate": 4.964945738131737e-05,
      "loss": 0.891,
      "step": 46100
    },
    {
      "epoch": 0.4215636177823199,
      "grad_norm": 4.390148162841797,
      "learning_rate": 4.9648696985181406e-05,
      "loss": 0.8677,
      "step": 46200
    },
    {
      "epoch": 0.42247609314548507,
      "grad_norm": 4.006017208099365,
      "learning_rate": 4.964793658904543e-05,
      "loss": 0.8826,
      "step": 46300
    },
    {
      "epoch": 0.42338856850865025,
      "grad_norm": 4.614943027496338,
      "learning_rate": 4.964717619290946e-05,
      "loss": 0.8957,
      "step": 46400
    },
    {
      "epoch": 0.42430104387181544,
      "grad_norm": 4.1795125007629395,
      "learning_rate": 4.964641579677349e-05,
      "loss": 0.8799,
      "step": 46500
    },
    {
      "epoch": 0.42521351923498063,
      "grad_norm": 4.405416011810303,
      "learning_rate": 4.964565540063752e-05,
      "loss": 0.8941,
      "step": 46600
    },
    {
      "epoch": 0.4261259945981459,
      "grad_norm": 4.478877067565918,
      "learning_rate": 4.964489500450155e-05,
      "loss": 0.8514,
      "step": 46700
    },
    {
      "epoch": 0.42703846996131106,
      "grad_norm": 5.099846363067627,
      "learning_rate": 4.964413460836558e-05,
      "loss": 0.8637,
      "step": 46800
    },
    {
      "epoch": 0.42795094532447625,
      "grad_norm": 4.584811687469482,
      "learning_rate": 4.96433742122296e-05,
      "loss": 0.8368,
      "step": 46900
    },
    {
      "epoch": 0.42886342068764144,
      "grad_norm": 4.4374260902404785,
      "learning_rate": 4.964261381609364e-05,
      "loss": 0.9167,
      "step": 47000
    },
    {
      "epoch": 0.4297758960508066,
      "grad_norm": 3.5268898010253906,
      "learning_rate": 4.964185341995766e-05,
      "loss": 0.8616,
      "step": 47100
    },
    {
      "epoch": 0.4306883714139718,
      "grad_norm": 6.861501216888428,
      "learning_rate": 4.964109302382169e-05,
      "loss": 0.8854,
      "step": 47200
    },
    {
      "epoch": 0.431600846777137,
      "grad_norm": 4.464598655700684,
      "learning_rate": 4.964033262768572e-05,
      "loss": 0.9265,
      "step": 47300
    },
    {
      "epoch": 0.4325133221403022,
      "grad_norm": 5.6657843589782715,
      "learning_rate": 4.963957223154975e-05,
      "loss": 0.8885,
      "step": 47400
    },
    {
      "epoch": 0.4334257975034674,
      "grad_norm": 4.028448581695557,
      "learning_rate": 4.9638811835413776e-05,
      "loss": 0.8986,
      "step": 47500
    },
    {
      "epoch": 0.4343382728666326,
      "grad_norm": 4.534821033477783,
      "learning_rate": 4.963805143927781e-05,
      "loss": 0.8098,
      "step": 47600
    },
    {
      "epoch": 0.4352507482297978,
      "grad_norm": 4.72796106338501,
      "learning_rate": 4.9637291043141836e-05,
      "loss": 0.8694,
      "step": 47700
    },
    {
      "epoch": 0.436163223592963,
      "grad_norm": 3.8419272899627686,
      "learning_rate": 4.9636530647005866e-05,
      "loss": 0.8765,
      "step": 47800
    },
    {
      "epoch": 0.4370756989561282,
      "grad_norm": 4.474198341369629,
      "learning_rate": 4.9635770250869896e-05,
      "loss": 0.8611,
      "step": 47900
    },
    {
      "epoch": 0.4379881743192934,
      "grad_norm": 4.743485450744629,
      "learning_rate": 4.9635009854733926e-05,
      "loss": 0.8977,
      "step": 48000
    },
    {
      "epoch": 0.43890064968245857,
      "grad_norm": 3.7226877212524414,
      "learning_rate": 4.9634249458597957e-05,
      "loss": 0.8372,
      "step": 48100
    },
    {
      "epoch": 0.43981312504562375,
      "grad_norm": 4.672524929046631,
      "learning_rate": 4.9633489062461987e-05,
      "loss": 0.8812,
      "step": 48200
    },
    {
      "epoch": 0.44072560040878894,
      "grad_norm": 4.201761722564697,
      "learning_rate": 4.963272866632601e-05,
      "loss": 0.8582,
      "step": 48300
    },
    {
      "epoch": 0.44163807577195413,
      "grad_norm": 3.4361767768859863,
      "learning_rate": 4.963196827019004e-05,
      "loss": 0.8902,
      "step": 48400
    },
    {
      "epoch": 0.4425505511351194,
      "grad_norm": 4.349246978759766,
      "learning_rate": 4.963120787405407e-05,
      "loss": 0.8658,
      "step": 48500
    },
    {
      "epoch": 0.44346302649828456,
      "grad_norm": 3.91119122505188,
      "learning_rate": 4.963044747791809e-05,
      "loss": 0.8722,
      "step": 48600
    },
    {
      "epoch": 0.44437550186144975,
      "grad_norm": 4.0265302658081055,
      "learning_rate": 4.962968708178213e-05,
      "loss": 0.8389,
      "step": 48700
    },
    {
      "epoch": 0.44528797722461494,
      "grad_norm": 4.262842178344727,
      "learning_rate": 4.9628926685646153e-05,
      "loss": 0.9005,
      "step": 48800
    },
    {
      "epoch": 0.4462004525877801,
      "grad_norm": 3.947206497192383,
      "learning_rate": 4.9628166289510183e-05,
      "loss": 0.8582,
      "step": 48900
    },
    {
      "epoch": 0.4471129279509453,
      "grad_norm": 5.133264541625977,
      "learning_rate": 4.9627405893374214e-05,
      "loss": 0.8664,
      "step": 49000
    },
    {
      "epoch": 0.4480254033141105,
      "grad_norm": 4.602713108062744,
      "learning_rate": 4.9626645497238244e-05,
      "loss": 0.8407,
      "step": 49100
    },
    {
      "epoch": 0.4489378786772757,
      "grad_norm": 5.097743511199951,
      "learning_rate": 4.9625885101102274e-05,
      "loss": 0.8233,
      "step": 49200
    },
    {
      "epoch": 0.4498503540404409,
      "grad_norm": 4.329585075378418,
      "learning_rate": 4.9625124704966304e-05,
      "loss": 0.8809,
      "step": 49300
    },
    {
      "epoch": 0.4507628294036061,
      "grad_norm": 4.959794998168945,
      "learning_rate": 4.962436430883033e-05,
      "loss": 0.9102,
      "step": 49400
    },
    {
      "epoch": 0.4516753047667713,
      "grad_norm": 4.065136432647705,
      "learning_rate": 4.9623603912694364e-05,
      "loss": 0.9001,
      "step": 49500
    },
    {
      "epoch": 0.4525877801299365,
      "grad_norm": 3.835566282272339,
      "learning_rate": 4.962284351655839e-05,
      "loss": 0.8913,
      "step": 49600
    },
    {
      "epoch": 0.4535002554931017,
      "grad_norm": 4.032618999481201,
      "learning_rate": 4.962208312042242e-05,
      "loss": 0.8797,
      "step": 49700
    },
    {
      "epoch": 0.4544127308562669,
      "grad_norm": 4.375158786773682,
      "learning_rate": 4.962132272428645e-05,
      "loss": 0.8545,
      "step": 49800
    },
    {
      "epoch": 0.45532520621943207,
      "grad_norm": 4.743941307067871,
      "learning_rate": 4.962056232815048e-05,
      "loss": 0.8557,
      "step": 49900
    },
    {
      "epoch": 0.45623768158259725,
      "grad_norm": 4.6911420822143555,
      "learning_rate": 4.96198019320145e-05,
      "loss": 0.8766,
      "step": 50000
    },
    {
      "epoch": 0.45715015694576244,
      "grad_norm": 4.16055154800415,
      "learning_rate": 4.961904153587854e-05,
      "loss": 0.8524,
      "step": 50100
    },
    {
      "epoch": 0.4580626323089277,
      "grad_norm": 5.270699501037598,
      "learning_rate": 4.961828113974256e-05,
      "loss": 0.8752,
      "step": 50200
    },
    {
      "epoch": 0.4589751076720929,
      "grad_norm": 4.305854320526123,
      "learning_rate": 4.961752074360659e-05,
      "loss": 0.8849,
      "step": 50300
    },
    {
      "epoch": 0.45988758303525806,
      "grad_norm": 3.8314645290374756,
      "learning_rate": 4.961676034747062e-05,
      "loss": 0.8844,
      "step": 50400
    },
    {
      "epoch": 0.46080005839842325,
      "grad_norm": 4.599510192871094,
      "learning_rate": 4.961599995133465e-05,
      "loss": 0.8581,
      "step": 50500
    },
    {
      "epoch": 0.46171253376158844,
      "grad_norm": 4.226564407348633,
      "learning_rate": 4.961523955519868e-05,
      "loss": 0.8616,
      "step": 50600
    },
    {
      "epoch": 0.46262500912475363,
      "grad_norm": 4.387575149536133,
      "learning_rate": 4.961447915906271e-05,
      "loss": 0.8248,
      "step": 50700
    },
    {
      "epoch": 0.4635374844879188,
      "grad_norm": 3.9704208374023438,
      "learning_rate": 4.9613718762926734e-05,
      "loss": 0.8291,
      "step": 50800
    },
    {
      "epoch": 0.464449959851084,
      "grad_norm": 4.662217617034912,
      "learning_rate": 4.961295836679077e-05,
      "loss": 0.8865,
      "step": 50900
    },
    {
      "epoch": 0.4653624352142492,
      "grad_norm": 4.586413860321045,
      "learning_rate": 4.9612197970654795e-05,
      "loss": 0.8275,
      "step": 51000
    },
    {
      "epoch": 0.46627491057741444,
      "grad_norm": 5.735574722290039,
      "learning_rate": 4.9611437574518825e-05,
      "loss": 0.8716,
      "step": 51100
    },
    {
      "epoch": 0.4671873859405796,
      "grad_norm": 4.191526889801025,
      "learning_rate": 4.9610677178382855e-05,
      "loss": 0.8569,
      "step": 51200
    },
    {
      "epoch": 0.4680998613037448,
      "grad_norm": 4.667941570281982,
      "learning_rate": 4.960991678224688e-05,
      "loss": 0.8584,
      "step": 51300
    },
    {
      "epoch": 0.46901233666691,
      "grad_norm": 4.074472904205322,
      "learning_rate": 4.960915638611091e-05,
      "loss": 0.847,
      "step": 51400
    },
    {
      "epoch": 0.4699248120300752,
      "grad_norm": 5.005359649658203,
      "learning_rate": 4.960839598997494e-05,
      "loss": 0.884,
      "step": 51500
    },
    {
      "epoch": 0.4708372873932404,
      "grad_norm": 4.093027114868164,
      "learning_rate": 4.960763559383897e-05,
      "loss": 0.8513,
      "step": 51600
    },
    {
      "epoch": 0.47174976275640557,
      "grad_norm": 4.188758373260498,
      "learning_rate": 4.9606875197703e-05,
      "loss": 0.8638,
      "step": 51700
    },
    {
      "epoch": 0.47266223811957075,
      "grad_norm": 5.01624870300293,
      "learning_rate": 4.960611480156703e-05,
      "loss": 0.8834,
      "step": 51800
    },
    {
      "epoch": 0.47357471348273594,
      "grad_norm": 5.136876106262207,
      "learning_rate": 4.960535440543105e-05,
      "loss": 0.8543,
      "step": 51900
    },
    {
      "epoch": 0.4744871888459012,
      "grad_norm": 4.6284003257751465,
      "learning_rate": 4.960459400929509e-05,
      "loss": 0.8592,
      "step": 52000
    },
    {
      "epoch": 0.4753996642090664,
      "grad_norm": 4.87851619720459,
      "learning_rate": 4.960383361315911e-05,
      "loss": 0.9007,
      "step": 52100
    },
    {
      "epoch": 0.47631213957223156,
      "grad_norm": 4.348798751831055,
      "learning_rate": 4.960307321702314e-05,
      "loss": 0.8829,
      "step": 52200
    },
    {
      "epoch": 0.47722461493539675,
      "grad_norm": 3.838481903076172,
      "learning_rate": 4.960231282088717e-05,
      "loss": 0.8817,
      "step": 52300
    },
    {
      "epoch": 0.47813709029856194,
      "grad_norm": 5.301892280578613,
      "learning_rate": 4.96015524247512e-05,
      "loss": 0.8669,
      "step": 52400
    },
    {
      "epoch": 0.47904956566172713,
      "grad_norm": 4.176852226257324,
      "learning_rate": 4.9600792028615225e-05,
      "loss": 0.8615,
      "step": 52500
    },
    {
      "epoch": 0.4799620410248923,
      "grad_norm": 4.115821838378906,
      "learning_rate": 4.960003163247926e-05,
      "loss": 0.8446,
      "step": 52600
    },
    {
      "epoch": 0.4808745163880575,
      "grad_norm": 3.7063019275665283,
      "learning_rate": 4.9599271236343285e-05,
      "loss": 0.8831,
      "step": 52700
    },
    {
      "epoch": 0.4817869917512227,
      "grad_norm": 4.367730617523193,
      "learning_rate": 4.9598510840207315e-05,
      "loss": 0.8723,
      "step": 52800
    },
    {
      "epoch": 0.48269946711438794,
      "grad_norm": 4.724912643432617,
      "learning_rate": 4.9597750444071346e-05,
      "loss": 0.8505,
      "step": 52900
    },
    {
      "epoch": 0.4836119424775531,
      "grad_norm": 4.3315958976745605,
      "learning_rate": 4.9596990047935376e-05,
      "loss": 0.8412,
      "step": 53000
    },
    {
      "epoch": 0.4845244178407183,
      "grad_norm": 5.084322929382324,
      "learning_rate": 4.9596229651799406e-05,
      "loss": 0.8568,
      "step": 53100
    },
    {
      "epoch": 0.4854368932038835,
      "grad_norm": 4.21904182434082,
      "learning_rate": 4.9595469255663436e-05,
      "loss": 0.9187,
      "step": 53200
    },
    {
      "epoch": 0.4863493685670487,
      "grad_norm": 3.706472158432007,
      "learning_rate": 4.959470885952746e-05,
      "loss": 0.8374,
      "step": 53300
    },
    {
      "epoch": 0.4872618439302139,
      "grad_norm": 4.172347068786621,
      "learning_rate": 4.9593948463391496e-05,
      "loss": 0.8188,
      "step": 53400
    },
    {
      "epoch": 0.48817431929337907,
      "grad_norm": 4.471406936645508,
      "learning_rate": 4.959318806725552e-05,
      "loss": 0.8972,
      "step": 53500
    },
    {
      "epoch": 0.48908679465654425,
      "grad_norm": 5.259253978729248,
      "learning_rate": 4.959242767111955e-05,
      "loss": 0.8561,
      "step": 53600
    },
    {
      "epoch": 0.48999927001970944,
      "grad_norm": 4.856886386871338,
      "learning_rate": 4.959166727498358e-05,
      "loss": 0.8669,
      "step": 53700
    },
    {
      "epoch": 0.4909117453828747,
      "grad_norm": 4.594318866729736,
      "learning_rate": 4.959090687884761e-05,
      "loss": 0.9152,
      "step": 53800
    },
    {
      "epoch": 0.4918242207460399,
      "grad_norm": 4.379584789276123,
      "learning_rate": 4.959014648271163e-05,
      "loss": 0.8217,
      "step": 53900
    },
    {
      "epoch": 0.49273669610920506,
      "grad_norm": 4.783738613128662,
      "learning_rate": 4.958938608657566e-05,
      "loss": 0.9214,
      "step": 54000
    },
    {
      "epoch": 0.49364917147237025,
      "grad_norm": 4.4226555824279785,
      "learning_rate": 4.958862569043969e-05,
      "loss": 0.8752,
      "step": 54100
    },
    {
      "epoch": 0.49456164683553544,
      "grad_norm": 4.54957914352417,
      "learning_rate": 4.958786529430372e-05,
      "loss": 0.8518,
      "step": 54200
    },
    {
      "epoch": 0.49547412219870063,
      "grad_norm": 4.330334186553955,
      "learning_rate": 4.958710489816775e-05,
      "loss": 0.8338,
      "step": 54300
    },
    {
      "epoch": 0.4963865975618658,
      "grad_norm": 4.3826904296875,
      "learning_rate": 4.9586344502031776e-05,
      "loss": 0.847,
      "step": 54400
    },
    {
      "epoch": 0.497299072925031,
      "grad_norm": 4.409646987915039,
      "learning_rate": 4.958558410589581e-05,
      "loss": 0.8644,
      "step": 54500
    },
    {
      "epoch": 0.4982115482881962,
      "grad_norm": 4.313534259796143,
      "learning_rate": 4.9584823709759836e-05,
      "loss": 0.9067,
      "step": 54600
    },
    {
      "epoch": 0.49912402365136144,
      "grad_norm": 3.809406042098999,
      "learning_rate": 4.9584063313623866e-05,
      "loss": 0.8321,
      "step": 54700
    },
    {
      "epoch": 0.5000364990145266,
      "grad_norm": 4.157682418823242,
      "learning_rate": 4.9583302917487896e-05,
      "loss": 0.8439,
      "step": 54800
    },
    {
      "epoch": 0.5009489743776918,
      "grad_norm": 4.808996200561523,
      "learning_rate": 4.9582542521351927e-05,
      "loss": 0.8442,
      "step": 54900
    },
    {
      "epoch": 0.501861449740857,
      "grad_norm": 4.356853008270264,
      "learning_rate": 4.958178212521595e-05,
      "loss": 0.8353,
      "step": 55000
    },
    {
      "epoch": 0.5027739251040222,
      "grad_norm": 4.631801128387451,
      "learning_rate": 4.958102172907999e-05,
      "loss": 0.8799,
      "step": 55100
    },
    {
      "epoch": 0.5036864004671874,
      "grad_norm": 5.064004898071289,
      "learning_rate": 4.958026133294401e-05,
      "loss": 0.8746,
      "step": 55200
    },
    {
      "epoch": 0.5045988758303526,
      "grad_norm": 4.485727310180664,
      "learning_rate": 4.957950093680804e-05,
      "loss": 0.8085,
      "step": 55300
    },
    {
      "epoch": 0.5055113511935178,
      "grad_norm": 4.429675579071045,
      "learning_rate": 4.957874054067207e-05,
      "loss": 0.8615,
      "step": 55400
    },
    {
      "epoch": 0.5064238265566829,
      "grad_norm": 4.523533821105957,
      "learning_rate": 4.95779801445361e-05,
      "loss": 0.8774,
      "step": 55500
    },
    {
      "epoch": 0.5073363019198481,
      "grad_norm": 4.26482629776001,
      "learning_rate": 4.957721974840013e-05,
      "loss": 0.8756,
      "step": 55600
    },
    {
      "epoch": 0.5082487772830133,
      "grad_norm": 4.924561500549316,
      "learning_rate": 4.957645935226416e-05,
      "loss": 0.8438,
      "step": 55700
    },
    {
      "epoch": 0.5091612526461785,
      "grad_norm": 4.8236470222473145,
      "learning_rate": 4.9575698956128184e-05,
      "loss": 0.8482,
      "step": 55800
    },
    {
      "epoch": 0.5100737280093437,
      "grad_norm": 4.413031578063965,
      "learning_rate": 4.957493855999222e-05,
      "loss": 0.9126,
      "step": 55900
    },
    {
      "epoch": 0.510986203372509,
      "grad_norm": 3.815059185028076,
      "learning_rate": 4.9574178163856244e-05,
      "loss": 0.832,
      "step": 56000
    },
    {
      "epoch": 0.5118986787356742,
      "grad_norm": 4.703180313110352,
      "learning_rate": 4.9573417767720274e-05,
      "loss": 0.8659,
      "step": 56100
    },
    {
      "epoch": 0.5128111540988394,
      "grad_norm": 3.447298526763916,
      "learning_rate": 4.9572657371584304e-05,
      "loss": 0.8429,
      "step": 56200
    },
    {
      "epoch": 0.5137236294620046,
      "grad_norm": 4.006750583648682,
      "learning_rate": 4.9571896975448334e-05,
      "loss": 0.822,
      "step": 56300
    },
    {
      "epoch": 0.5146361048251697,
      "grad_norm": 4.025365829467773,
      "learning_rate": 4.957113657931236e-05,
      "loss": 0.876,
      "step": 56400
    },
    {
      "epoch": 0.5155485801883349,
      "grad_norm": 4.533187389373779,
      "learning_rate": 4.9570376183176394e-05,
      "loss": 0.856,
      "step": 56500
    },
    {
      "epoch": 0.5164610555515001,
      "grad_norm": 3.7398622035980225,
      "learning_rate": 4.956961578704042e-05,
      "loss": 0.8562,
      "step": 56600
    },
    {
      "epoch": 0.5173735309146653,
      "grad_norm": 5.6135029792785645,
      "learning_rate": 4.956885539090445e-05,
      "loss": 0.8234,
      "step": 56700
    },
    {
      "epoch": 0.5182860062778305,
      "grad_norm": 3.982386350631714,
      "learning_rate": 4.956809499476848e-05,
      "loss": 0.8565,
      "step": 56800
    },
    {
      "epoch": 0.5191984816409957,
      "grad_norm": 5.6519975662231445,
      "learning_rate": 4.95673345986325e-05,
      "loss": 0.8551,
      "step": 56900
    },
    {
      "epoch": 0.5201109570041609,
      "grad_norm": 4.629485130310059,
      "learning_rate": 4.956657420249654e-05,
      "loss": 0.8586,
      "step": 57000
    },
    {
      "epoch": 0.5210234323673261,
      "grad_norm": 4.403879165649414,
      "learning_rate": 4.956581380636056e-05,
      "loss": 0.8591,
      "step": 57100
    },
    {
      "epoch": 0.5219359077304913,
      "grad_norm": 4.165603160858154,
      "learning_rate": 4.956505341022459e-05,
      "loss": 0.8692,
      "step": 57200
    },
    {
      "epoch": 0.5228483830936564,
      "grad_norm": 3.882190704345703,
      "learning_rate": 4.956429301408862e-05,
      "loss": 0.8837,
      "step": 57300
    },
    {
      "epoch": 0.5237608584568216,
      "grad_norm": 4.297092437744141,
      "learning_rate": 4.956353261795265e-05,
      "loss": 0.8666,
      "step": 57400
    },
    {
      "epoch": 0.5246733338199868,
      "grad_norm": 3.209512948989868,
      "learning_rate": 4.956277222181668e-05,
      "loss": 0.8522,
      "step": 57500
    },
    {
      "epoch": 0.525585809183152,
      "grad_norm": 4.320038318634033,
      "learning_rate": 4.956201182568071e-05,
      "loss": 0.8794,
      "step": 57600
    },
    {
      "epoch": 0.5264982845463172,
      "grad_norm": 3.65268874168396,
      "learning_rate": 4.9561251429544735e-05,
      "loss": 0.8809,
      "step": 57700
    },
    {
      "epoch": 0.5274107599094825,
      "grad_norm": 4.016753196716309,
      "learning_rate": 4.9560491033408765e-05,
      "loss": 0.8542,
      "step": 57800
    },
    {
      "epoch": 0.5283232352726477,
      "grad_norm": 5.183125972747803,
      "learning_rate": 4.9559730637272795e-05,
      "loss": 0.8805,
      "step": 57900
    },
    {
      "epoch": 0.5292357106358129,
      "grad_norm": 4.468153476715088,
      "learning_rate": 4.9558970241136825e-05,
      "loss": 0.8573,
      "step": 58000
    },
    {
      "epoch": 0.5301481859989781,
      "grad_norm": 4.622663497924805,
      "learning_rate": 4.9558209845000855e-05,
      "loss": 0.8921,
      "step": 58100
    },
    {
      "epoch": 0.5310606613621432,
      "grad_norm": 5.344147682189941,
      "learning_rate": 4.9557449448864885e-05,
      "loss": 0.8684,
      "step": 58200
    },
    {
      "epoch": 0.5319731367253084,
      "grad_norm": 4.368740081787109,
      "learning_rate": 4.955668905272891e-05,
      "loss": 0.9106,
      "step": 58300
    },
    {
      "epoch": 0.5328856120884736,
      "grad_norm": 4.3609514236450195,
      "learning_rate": 4.9555928656592945e-05,
      "loss": 0.8236,
      "step": 58400
    },
    {
      "epoch": 0.5337980874516388,
      "grad_norm": 4.944219589233398,
      "learning_rate": 4.955516826045697e-05,
      "loss": 0.8759,
      "step": 58500
    },
    {
      "epoch": 0.534710562814804,
      "grad_norm": 4.394794464111328,
      "learning_rate": 4.9554407864321e-05,
      "loss": 0.8241,
      "step": 58600
    },
    {
      "epoch": 0.5356230381779692,
      "grad_norm": 4.155235290527344,
      "learning_rate": 4.955364746818503e-05,
      "loss": 0.8694,
      "step": 58700
    },
    {
      "epoch": 0.5365355135411344,
      "grad_norm": 4.433952331542969,
      "learning_rate": 4.955288707204906e-05,
      "loss": 0.8367,
      "step": 58800
    },
    {
      "epoch": 0.5374479889042996,
      "grad_norm": 4.84781551361084,
      "learning_rate": 4.955212667591309e-05,
      "loss": 0.8504,
      "step": 58900
    },
    {
      "epoch": 0.5383604642674648,
      "grad_norm": 4.672074794769287,
      "learning_rate": 4.955136627977712e-05,
      "loss": 0.8549,
      "step": 59000
    },
    {
      "epoch": 0.5392729396306299,
      "grad_norm": 3.94891095161438,
      "learning_rate": 4.955060588364114e-05,
      "loss": 0.8844,
      "step": 59100
    },
    {
      "epoch": 0.5401854149937951,
      "grad_norm": 4.90338659286499,
      "learning_rate": 4.954984548750518e-05,
      "loss": 0.8198,
      "step": 59200
    },
    {
      "epoch": 0.5410978903569603,
      "grad_norm": 4.662460803985596,
      "learning_rate": 4.95490850913692e-05,
      "loss": 0.861,
      "step": 59300
    },
    {
      "epoch": 0.5420103657201255,
      "grad_norm": 5.359144687652588,
      "learning_rate": 4.954832469523323e-05,
      "loss": 0.8619,
      "step": 59400
    },
    {
      "epoch": 0.5429228410832907,
      "grad_norm": 4.8800950050354,
      "learning_rate": 4.954756429909726e-05,
      "loss": 0.8787,
      "step": 59500
    },
    {
      "epoch": 0.543835316446456,
      "grad_norm": 5.66916036605835,
      "learning_rate": 4.954680390296129e-05,
      "loss": 0.8305,
      "step": 59600
    },
    {
      "epoch": 0.5447477918096212,
      "grad_norm": 3.8752574920654297,
      "learning_rate": 4.9546043506825316e-05,
      "loss": 0.8412,
      "step": 59700
    },
    {
      "epoch": 0.5456602671727864,
      "grad_norm": 3.9005935192108154,
      "learning_rate": 4.9545283110689346e-05,
      "loss": 0.8288,
      "step": 59800
    },
    {
      "epoch": 0.5465727425359516,
      "grad_norm": 4.487043857574463,
      "learning_rate": 4.9544522714553376e-05,
      "loss": 0.8728,
      "step": 59900
    },
    {
      "epoch": 0.5474852178991167,
      "grad_norm": 3.762542963027954,
      "learning_rate": 4.9543762318417406e-05,
      "loss": 0.8233,
      "step": 60000
    },
    {
      "epoch": 0.5483976932622819,
      "grad_norm": 3.8628852367401123,
      "learning_rate": 4.9543001922281436e-05,
      "loss": 0.9083,
      "step": 60100
    },
    {
      "epoch": 0.5493101686254471,
      "grad_norm": 6.241658687591553,
      "learning_rate": 4.954224152614546e-05,
      "loss": 0.8636,
      "step": 60200
    },
    {
      "epoch": 0.5502226439886123,
      "grad_norm": 3.662165641784668,
      "learning_rate": 4.9541481130009496e-05,
      "loss": 0.8647,
      "step": 60300
    },
    {
      "epoch": 0.5511351193517775,
      "grad_norm": 3.8777997493743896,
      "learning_rate": 4.954072073387352e-05,
      "loss": 0.8568,
      "step": 60400
    },
    {
      "epoch": 0.5520475947149427,
      "grad_norm": 3.792858839035034,
      "learning_rate": 4.953996033773755e-05,
      "loss": 0.8483,
      "step": 60500
    },
    {
      "epoch": 0.5529600700781079,
      "grad_norm": 5.1837005615234375,
      "learning_rate": 4.953919994160158e-05,
      "loss": 0.8633,
      "step": 60600
    },
    {
      "epoch": 0.5538725454412731,
      "grad_norm": 3.9799699783325195,
      "learning_rate": 4.953843954546561e-05,
      "loss": 0.8265,
      "step": 60700
    },
    {
      "epoch": 0.5547850208044383,
      "grad_norm": 5.399032115936279,
      "learning_rate": 4.953767914932963e-05,
      "loss": 0.844,
      "step": 60800
    },
    {
      "epoch": 0.5556974961676034,
      "grad_norm": 4.704935073852539,
      "learning_rate": 4.953691875319367e-05,
      "loss": 0.8594,
      "step": 60900
    },
    {
      "epoch": 0.5566099715307686,
      "grad_norm": 5.142425537109375,
      "learning_rate": 4.953615835705769e-05,
      "loss": 0.8849,
      "step": 61000
    },
    {
      "epoch": 0.5575224468939338,
      "grad_norm": 4.402927398681641,
      "learning_rate": 4.953539796092172e-05,
      "loss": 0.8598,
      "step": 61100
    },
    {
      "epoch": 0.558434922257099,
      "grad_norm": 5.250842571258545,
      "learning_rate": 4.953463756478575e-05,
      "loss": 0.8674,
      "step": 61200
    },
    {
      "epoch": 0.5593473976202643,
      "grad_norm": 4.673272609710693,
      "learning_rate": 4.953387716864978e-05,
      "loss": 0.848,
      "step": 61300
    },
    {
      "epoch": 0.5602598729834295,
      "grad_norm": 4.153416633605957,
      "learning_rate": 4.953311677251381e-05,
      "loss": 0.8962,
      "step": 61400
    },
    {
      "epoch": 0.5611723483465947,
      "grad_norm": 3.5645978450775146,
      "learning_rate": 4.953235637637784e-05,
      "loss": 0.8366,
      "step": 61500
    },
    {
      "epoch": 0.5620848237097599,
      "grad_norm": 3.9420642852783203,
      "learning_rate": 4.9531595980241867e-05,
      "loss": 0.8431,
      "step": 61600
    },
    {
      "epoch": 0.5629972990729251,
      "grad_norm": 4.776797294616699,
      "learning_rate": 4.95308355841059e-05,
      "loss": 0.8707,
      "step": 61700
    },
    {
      "epoch": 0.5639097744360902,
      "grad_norm": 4.005620002746582,
      "learning_rate": 4.953007518796993e-05,
      "loss": 0.8532,
      "step": 61800
    },
    {
      "epoch": 0.5648222497992554,
      "grad_norm": 3.7698609828948975,
      "learning_rate": 4.952931479183396e-05,
      "loss": 0.8429,
      "step": 61900
    },
    {
      "epoch": 0.5657347251624206,
      "grad_norm": 4.695664882659912,
      "learning_rate": 4.952855439569799e-05,
      "loss": 0.8443,
      "step": 62000
    },
    {
      "epoch": 0.5666472005255858,
      "grad_norm": 3.8301267623901367,
      "learning_rate": 4.952779399956202e-05,
      "loss": 0.8195,
      "step": 62100
    },
    {
      "epoch": 0.567559675888751,
      "grad_norm": 5.21262788772583,
      "learning_rate": 4.952703360342604e-05,
      "loss": 0.8666,
      "step": 62200
    },
    {
      "epoch": 0.5684721512519162,
      "grad_norm": 3.9048924446105957,
      "learning_rate": 4.952627320729008e-05,
      "loss": 0.807,
      "step": 62300
    },
    {
      "epoch": 0.5693846266150814,
      "grad_norm": 4.381265163421631,
      "learning_rate": 4.95255128111541e-05,
      "loss": 0.8393,
      "step": 62400
    },
    {
      "epoch": 0.5702971019782466,
      "grad_norm": 4.120565891265869,
      "learning_rate": 4.952475241501813e-05,
      "loss": 0.7961,
      "step": 62500
    },
    {
      "epoch": 0.5712095773414118,
      "grad_norm": 5.433224678039551,
      "learning_rate": 4.952399201888216e-05,
      "loss": 0.8399,
      "step": 62600
    },
    {
      "epoch": 0.5721220527045769,
      "grad_norm": 5.0417585372924805,
      "learning_rate": 4.9523231622746184e-05,
      "loss": 0.8126,
      "step": 62700
    },
    {
      "epoch": 0.5730345280677421,
      "grad_norm": 4.7412333488464355,
      "learning_rate": 4.952247122661022e-05,
      "loss": 0.8773,
      "step": 62800
    },
    {
      "epoch": 0.5739470034309073,
      "grad_norm": 5.0166778564453125,
      "learning_rate": 4.9521710830474244e-05,
      "loss": 0.8487,
      "step": 62900
    },
    {
      "epoch": 0.5748594787940725,
      "grad_norm": 4.595765590667725,
      "learning_rate": 4.9520950434338274e-05,
      "loss": 0.865,
      "step": 63000
    },
    {
      "epoch": 0.5757719541572378,
      "grad_norm": 4.813622951507568,
      "learning_rate": 4.9520190038202304e-05,
      "loss": 0.8987,
      "step": 63100
    },
    {
      "epoch": 0.576684429520403,
      "grad_norm": 4.3361406326293945,
      "learning_rate": 4.9519429642066334e-05,
      "loss": 0.8691,
      "step": 63200
    },
    {
      "epoch": 0.5775969048835682,
      "grad_norm": 4.63232946395874,
      "learning_rate": 4.951866924593036e-05,
      "loss": 0.847,
      "step": 63300
    },
    {
      "epoch": 0.5785093802467334,
      "grad_norm": 4.670496940612793,
      "learning_rate": 4.9517908849794394e-05,
      "loss": 0.8658,
      "step": 63400
    },
    {
      "epoch": 0.5794218556098986,
      "grad_norm": 4.201576232910156,
      "learning_rate": 4.951714845365842e-05,
      "loss": 0.8527,
      "step": 63500
    },
    {
      "epoch": 0.5803343309730638,
      "grad_norm": 4.326247692108154,
      "learning_rate": 4.951638805752245e-05,
      "loss": 0.8279,
      "step": 63600
    },
    {
      "epoch": 0.5812468063362289,
      "grad_norm": 3.378110885620117,
      "learning_rate": 4.951562766138648e-05,
      "loss": 0.8414,
      "step": 63700
    },
    {
      "epoch": 0.5821592816993941,
      "grad_norm": 3.6036643981933594,
      "learning_rate": 4.951486726525051e-05,
      "loss": 0.8571,
      "step": 63800
    },
    {
      "epoch": 0.5830717570625593,
      "grad_norm": 4.1000142097473145,
      "learning_rate": 4.951410686911454e-05,
      "loss": 0.8531,
      "step": 63900
    },
    {
      "epoch": 0.5839842324257245,
      "grad_norm": 4.774824142456055,
      "learning_rate": 4.951334647297857e-05,
      "loss": 0.8205,
      "step": 64000
    },
    {
      "epoch": 0.5848967077888897,
      "grad_norm": 4.7783660888671875,
      "learning_rate": 4.951258607684259e-05,
      "loss": 0.8751,
      "step": 64100
    },
    {
      "epoch": 0.5858091831520549,
      "grad_norm": 4.441040992736816,
      "learning_rate": 4.951182568070663e-05,
      "loss": 0.8391,
      "step": 64200
    },
    {
      "epoch": 0.5867216585152201,
      "grad_norm": 4.729088306427002,
      "learning_rate": 4.951106528457065e-05,
      "loss": 0.8654,
      "step": 64300
    },
    {
      "epoch": 0.5876341338783853,
      "grad_norm": 4.5854339599609375,
      "learning_rate": 4.951030488843468e-05,
      "loss": 0.8552,
      "step": 64400
    },
    {
      "epoch": 0.5885466092415504,
      "grad_norm": 4.564760208129883,
      "learning_rate": 4.950954449229871e-05,
      "loss": 0.8286,
      "step": 64500
    },
    {
      "epoch": 0.5894590846047156,
      "grad_norm": 4.409409046173096,
      "learning_rate": 4.950878409616274e-05,
      "loss": 0.8568,
      "step": 64600
    },
    {
      "epoch": 0.5903715599678808,
      "grad_norm": 4.8421478271484375,
      "learning_rate": 4.9508023700026765e-05,
      "loss": 0.835,
      "step": 64700
    },
    {
      "epoch": 0.591284035331046,
      "grad_norm": 4.511265277862549,
      "learning_rate": 4.95072633038908e-05,
      "loss": 0.8377,
      "step": 64800
    },
    {
      "epoch": 0.5921965106942113,
      "grad_norm": 4.921297550201416,
      "learning_rate": 4.9506502907754825e-05,
      "loss": 0.8746,
      "step": 64900
    },
    {
      "epoch": 0.5931089860573765,
      "grad_norm": 5.02126407623291,
      "learning_rate": 4.9505742511618855e-05,
      "loss": 0.862,
      "step": 65000
    },
    {
      "epoch": 0.5940214614205417,
      "grad_norm": 4.413663864135742,
      "learning_rate": 4.9504982115482885e-05,
      "loss": 0.8525,
      "step": 65100
    },
    {
      "epoch": 0.5949339367837069,
      "grad_norm": 5.246824741363525,
      "learning_rate": 4.9504221719346915e-05,
      "loss": 0.8264,
      "step": 65200
    },
    {
      "epoch": 0.5958464121468721,
      "grad_norm": 5.172170162200928,
      "learning_rate": 4.9503461323210945e-05,
      "loss": 0.8019,
      "step": 65300
    },
    {
      "epoch": 0.5967588875100373,
      "grad_norm": 3.7054576873779297,
      "learning_rate": 4.950270092707497e-05,
      "loss": 0.8575,
      "step": 65400
    },
    {
      "epoch": 0.5976713628732024,
      "grad_norm": 4.024428844451904,
      "learning_rate": 4.9501940530939e-05,
      "loss": 0.7833,
      "step": 65500
    },
    {
      "epoch": 0.5985838382363676,
      "grad_norm": 5.492123603820801,
      "learning_rate": 4.950118013480303e-05,
      "loss": 0.8362,
      "step": 65600
    },
    {
      "epoch": 0.5994963135995328,
      "grad_norm": 5.524693965911865,
      "learning_rate": 4.950041973866706e-05,
      "loss": 0.851,
      "step": 65700
    },
    {
      "epoch": 0.600408788962698,
      "grad_norm": 4.54662561416626,
      "learning_rate": 4.949965934253108e-05,
      "loss": 0.8735,
      "step": 65800
    },
    {
      "epoch": 0.6013212643258632,
      "grad_norm": 2.9177236557006836,
      "learning_rate": 4.949889894639512e-05,
      "loss": 0.8775,
      "step": 65900
    },
    {
      "epoch": 0.6022337396890284,
      "grad_norm": 4.250411510467529,
      "learning_rate": 4.949813855025914e-05,
      "loss": 0.8669,
      "step": 66000
    },
    {
      "epoch": 0.6031462150521936,
      "grad_norm": 4.83933162689209,
      "learning_rate": 4.949737815412317e-05,
      "loss": 0.813,
      "step": 66100
    },
    {
      "epoch": 0.6040586904153588,
      "grad_norm": 5.356653690338135,
      "learning_rate": 4.94966177579872e-05,
      "loss": 0.8239,
      "step": 66200
    },
    {
      "epoch": 0.604971165778524,
      "grad_norm": 4.908864498138428,
      "learning_rate": 4.949585736185123e-05,
      "loss": 0.7928,
      "step": 66300
    },
    {
      "epoch": 0.6058836411416891,
      "grad_norm": 4.5835771560668945,
      "learning_rate": 4.949509696571526e-05,
      "loss": 0.8629,
      "step": 66400
    },
    {
      "epoch": 0.6067961165048543,
      "grad_norm": 4.05864143371582,
      "learning_rate": 4.949433656957929e-05,
      "loss": 0.8669,
      "step": 66500
    },
    {
      "epoch": 0.6077085918680195,
      "grad_norm": 5.794661045074463,
      "learning_rate": 4.9493576173443316e-05,
      "loss": 0.8354,
      "step": 66600
    },
    {
      "epoch": 0.6086210672311848,
      "grad_norm": 4.142510890960693,
      "learning_rate": 4.949281577730735e-05,
      "loss": 0.8514,
      "step": 66700
    },
    {
      "epoch": 0.60953354259435,
      "grad_norm": 3.9305617809295654,
      "learning_rate": 4.9492055381171376e-05,
      "loss": 0.8229,
      "step": 66800
    },
    {
      "epoch": 0.6104460179575152,
      "grad_norm": 3.683147668838501,
      "learning_rate": 4.9491294985035406e-05,
      "loss": 0.8285,
      "step": 66900
    },
    {
      "epoch": 0.6113584933206804,
      "grad_norm": 4.041131019592285,
      "learning_rate": 4.9490534588899436e-05,
      "loss": 0.8173,
      "step": 67000
    },
    {
      "epoch": 0.6122709686838456,
      "grad_norm": 3.7250804901123047,
      "learning_rate": 4.9489774192763466e-05,
      "loss": 0.8426,
      "step": 67100
    },
    {
      "epoch": 0.6131834440470108,
      "grad_norm": 4.249514579772949,
      "learning_rate": 4.948901379662749e-05,
      "loss": 0.8526,
      "step": 67200
    },
    {
      "epoch": 0.6140959194101759,
      "grad_norm": 3.675992012023926,
      "learning_rate": 4.9488253400491526e-05,
      "loss": 0.8123,
      "step": 67300
    },
    {
      "epoch": 0.6150083947733411,
      "grad_norm": 4.8282928466796875,
      "learning_rate": 4.948749300435555e-05,
      "loss": 0.7993,
      "step": 67400
    },
    {
      "epoch": 0.6159208701365063,
      "grad_norm": 4.673981666564941,
      "learning_rate": 4.948673260821958e-05,
      "loss": 0.8737,
      "step": 67500
    },
    {
      "epoch": 0.6168333454996715,
      "grad_norm": 3.7605650424957275,
      "learning_rate": 4.948597221208361e-05,
      "loss": 0.8333,
      "step": 67600
    },
    {
      "epoch": 0.6177458208628367,
      "grad_norm": 4.625222682952881,
      "learning_rate": 4.948521181594764e-05,
      "loss": 0.873,
      "step": 67700
    },
    {
      "epoch": 0.6186582962260019,
      "grad_norm": 4.349229335784912,
      "learning_rate": 4.948445141981167e-05,
      "loss": 0.8492,
      "step": 67800
    },
    {
      "epoch": 0.6195707715891671,
      "grad_norm": 4.199069976806641,
      "learning_rate": 4.94836910236757e-05,
      "loss": 0.8213,
      "step": 67900
    },
    {
      "epoch": 0.6204832469523323,
      "grad_norm": 4.990874767303467,
      "learning_rate": 4.948293062753972e-05,
      "loss": 0.8122,
      "step": 68000
    },
    {
      "epoch": 0.6213957223154974,
      "grad_norm": 4.211784362792969,
      "learning_rate": 4.948217023140376e-05,
      "loss": 0.9161,
      "step": 68100
    },
    {
      "epoch": 0.6223081976786626,
      "grad_norm": 4.260753631591797,
      "learning_rate": 4.948140983526778e-05,
      "loss": 0.8896,
      "step": 68200
    },
    {
      "epoch": 0.6232206730418278,
      "grad_norm": 4.3621320724487305,
      "learning_rate": 4.9480649439131806e-05,
      "loss": 0.8773,
      "step": 68300
    },
    {
      "epoch": 0.624133148404993,
      "grad_norm": 4.4237494468688965,
      "learning_rate": 4.947988904299584e-05,
      "loss": 0.8566,
      "step": 68400
    },
    {
      "epoch": 0.6250456237681583,
      "grad_norm": 4.388762950897217,
      "learning_rate": 4.9479128646859867e-05,
      "loss": 0.8284,
      "step": 68500
    },
    {
      "epoch": 0.6259580991313235,
      "grad_norm": 4.31477165222168,
      "learning_rate": 4.94783682507239e-05,
      "loss": 0.8486,
      "step": 68600
    },
    {
      "epoch": 0.6268705744944887,
      "grad_norm": 4.785345077514648,
      "learning_rate": 4.947760785458793e-05,
      "loss": 0.8422,
      "step": 68700
    },
    {
      "epoch": 0.6277830498576539,
      "grad_norm": 4.063412666320801,
      "learning_rate": 4.947684745845196e-05,
      "loss": 0.8382,
      "step": 68800
    },
    {
      "epoch": 0.6286955252208191,
      "grad_norm": 3.8986237049102783,
      "learning_rate": 4.947608706231599e-05,
      "loss": 0.8256,
      "step": 68900
    },
    {
      "epoch": 0.6296080005839843,
      "grad_norm": 4.5089216232299805,
      "learning_rate": 4.947532666618002e-05,
      "loss": 0.7769,
      "step": 69000
    },
    {
      "epoch": 0.6305204759471494,
      "grad_norm": 4.407005310058594,
      "learning_rate": 4.947456627004404e-05,
      "loss": 0.871,
      "step": 69100
    },
    {
      "epoch": 0.6314329513103146,
      "grad_norm": 4.277102947235107,
      "learning_rate": 4.947380587390808e-05,
      "loss": 0.8483,
      "step": 69200
    },
    {
      "epoch": 0.6323454266734798,
      "grad_norm": 4.164785861968994,
      "learning_rate": 4.94730454777721e-05,
      "loss": 0.8274,
      "step": 69300
    },
    {
      "epoch": 0.633257902036645,
      "grad_norm": 4.784895896911621,
      "learning_rate": 4.947228508163613e-05,
      "loss": 0.826,
      "step": 69400
    },
    {
      "epoch": 0.6341703773998102,
      "grad_norm": 5.330996990203857,
      "learning_rate": 4.947152468550016e-05,
      "loss": 0.8299,
      "step": 69500
    },
    {
      "epoch": 0.6350828527629754,
      "grad_norm": 4.131919860839844,
      "learning_rate": 4.947076428936419e-05,
      "loss": 0.8841,
      "step": 69600
    },
    {
      "epoch": 0.6359953281261406,
      "grad_norm": 4.8240861892700195,
      "learning_rate": 4.947000389322822e-05,
      "loss": 0.826,
      "step": 69700
    },
    {
      "epoch": 0.6369078034893058,
      "grad_norm": 5.479900360107422,
      "learning_rate": 4.946924349709225e-05,
      "loss": 0.8468,
      "step": 69800
    },
    {
      "epoch": 0.637820278852471,
      "grad_norm": 5.055140972137451,
      "learning_rate": 4.9468483100956274e-05,
      "loss": 0.8694,
      "step": 69900
    },
    {
      "epoch": 0.6387327542156361,
      "grad_norm": 5.127918243408203,
      "learning_rate": 4.9467722704820304e-05,
      "loss": 0.8552,
      "step": 70000
    },
    {
      "epoch": 0.6396452295788013,
      "grad_norm": 3.982205629348755,
      "learning_rate": 4.9466962308684334e-05,
      "loss": 0.8805,
      "step": 70100
    },
    {
      "epoch": 0.6405577049419666,
      "grad_norm": 4.584067344665527,
      "learning_rate": 4.9466201912548364e-05,
      "loss": 0.8106,
      "step": 70200
    },
    {
      "epoch": 0.6414701803051318,
      "grad_norm": 4.458425045013428,
      "learning_rate": 4.9465441516412394e-05,
      "loss": 0.8513,
      "step": 70300
    },
    {
      "epoch": 0.642382655668297,
      "grad_norm": 3.888054847717285,
      "learning_rate": 4.9464681120276424e-05,
      "loss": 0.8733,
      "step": 70400
    },
    {
      "epoch": 0.6432951310314622,
      "grad_norm": 3.357980489730835,
      "learning_rate": 4.946392072414045e-05,
      "loss": 0.8494,
      "step": 70500
    },
    {
      "epoch": 0.6442076063946274,
      "grad_norm": 4.563622951507568,
      "learning_rate": 4.9463160328004484e-05,
      "loss": 0.8753,
      "step": 70600
    },
    {
      "epoch": 0.6451200817577926,
      "grad_norm": 5.497402191162109,
      "learning_rate": 4.946239993186851e-05,
      "loss": 0.8676,
      "step": 70700
    },
    {
      "epoch": 0.6460325571209578,
      "grad_norm": 4.428991794586182,
      "learning_rate": 4.946163953573254e-05,
      "loss": 0.8567,
      "step": 70800
    },
    {
      "epoch": 0.6469450324841229,
      "grad_norm": 4.8108439445495605,
      "learning_rate": 4.946087913959657e-05,
      "loss": 0.8593,
      "step": 70900
    },
    {
      "epoch": 0.6478575078472881,
      "grad_norm": 4.633126735687256,
      "learning_rate": 4.94601187434606e-05,
      "loss": 0.8178,
      "step": 71000
    },
    {
      "epoch": 0.6487699832104533,
      "grad_norm": 3.711961030960083,
      "learning_rate": 4.945935834732463e-05,
      "loss": 0.8446,
      "step": 71100
    },
    {
      "epoch": 0.6496824585736185,
      "grad_norm": 4.2719879150390625,
      "learning_rate": 4.945859795118865e-05,
      "loss": 0.7942,
      "step": 71200
    },
    {
      "epoch": 0.6505949339367837,
      "grad_norm": 4.306506156921387,
      "learning_rate": 4.945783755505268e-05,
      "loss": 0.8468,
      "step": 71300
    },
    {
      "epoch": 0.6515074092999489,
      "grad_norm": 4.164908409118652,
      "learning_rate": 4.945707715891671e-05,
      "loss": 0.8553,
      "step": 71400
    },
    {
      "epoch": 0.6524198846631141,
      "grad_norm": 4.451453685760498,
      "learning_rate": 4.945631676278074e-05,
      "loss": 0.839,
      "step": 71500
    },
    {
      "epoch": 0.6533323600262793,
      "grad_norm": 4.809549331665039,
      "learning_rate": 4.9455556366644765e-05,
      "loss": 0.8594,
      "step": 71600
    },
    {
      "epoch": 0.6542448353894444,
      "grad_norm": 4.482346057891846,
      "learning_rate": 4.94547959705088e-05,
      "loss": 0.8657,
      "step": 71700
    },
    {
      "epoch": 0.6551573107526096,
      "grad_norm": 4.083438396453857,
      "learning_rate": 4.9454035574372825e-05,
      "loss": 0.8075,
      "step": 71800
    },
    {
      "epoch": 0.6560697861157748,
      "grad_norm": 4.612522125244141,
      "learning_rate": 4.9453275178236855e-05,
      "loss": 0.8745,
      "step": 71900
    },
    {
      "epoch": 0.6569822614789401,
      "grad_norm": 5.434545040130615,
      "learning_rate": 4.9452514782100885e-05,
      "loss": 0.8538,
      "step": 72000
    },
    {
      "epoch": 0.6578947368421053,
      "grad_norm": 4.129838943481445,
      "learning_rate": 4.9451754385964915e-05,
      "loss": 0.8595,
      "step": 72100
    },
    {
      "epoch": 0.6588072122052705,
      "grad_norm": 5.199753284454346,
      "learning_rate": 4.9450993989828945e-05,
      "loss": 0.8165,
      "step": 72200
    },
    {
      "epoch": 0.6597196875684357,
      "grad_norm": 4.441078186035156,
      "learning_rate": 4.9450233593692975e-05,
      "loss": 0.8337,
      "step": 72300
    },
    {
      "epoch": 0.6606321629316009,
      "grad_norm": 4.8697991371154785,
      "learning_rate": 4.9449473197557e-05,
      "loss": 0.8441,
      "step": 72400
    },
    {
      "epoch": 0.6615446382947661,
      "grad_norm": 3.3460795879364014,
      "learning_rate": 4.9448712801421035e-05,
      "loss": 0.8067,
      "step": 72500
    },
    {
      "epoch": 0.6624571136579313,
      "grad_norm": 4.559266567230225,
      "learning_rate": 4.944795240528506e-05,
      "loss": 0.8172,
      "step": 72600
    },
    {
      "epoch": 0.6633695890210964,
      "grad_norm": 3.9192092418670654,
      "learning_rate": 4.944719200914909e-05,
      "loss": 0.8444,
      "step": 72700
    },
    {
      "epoch": 0.6642820643842616,
      "grad_norm": 4.5096116065979,
      "learning_rate": 4.944643161301312e-05,
      "loss": 0.841,
      "step": 72800
    },
    {
      "epoch": 0.6651945397474268,
      "grad_norm": 3.1669437885284424,
      "learning_rate": 4.944567121687715e-05,
      "loss": 0.7747,
      "step": 72900
    },
    {
      "epoch": 0.666107015110592,
      "grad_norm": 2.977501153945923,
      "learning_rate": 4.944491082074117e-05,
      "loss": 0.8142,
      "step": 73000
    },
    {
      "epoch": 0.6670194904737572,
      "grad_norm": 2.9946541786193848,
      "learning_rate": 4.944415042460521e-05,
      "loss": 0.8269,
      "step": 73100
    },
    {
      "epoch": 0.6679319658369224,
      "grad_norm": 4.447492599487305,
      "learning_rate": 4.944339002846923e-05,
      "loss": 0.7976,
      "step": 73200
    },
    {
      "epoch": 0.6688444412000876,
      "grad_norm": 4.259191036224365,
      "learning_rate": 4.944262963233326e-05,
      "loss": 0.8366,
      "step": 73300
    },
    {
      "epoch": 0.6697569165632528,
      "grad_norm": 4.917003154754639,
      "learning_rate": 4.944186923619729e-05,
      "loss": 0.8439,
      "step": 73400
    },
    {
      "epoch": 0.670669391926418,
      "grad_norm": 4.351323127746582,
      "learning_rate": 4.944110884006132e-05,
      "loss": 0.9288,
      "step": 73500
    },
    {
      "epoch": 0.6715818672895831,
      "grad_norm": 5.3559489250183105,
      "learning_rate": 4.944034844392535e-05,
      "loss": 0.8625,
      "step": 73600
    },
    {
      "epoch": 0.6724943426527483,
      "grad_norm": 4.966269016265869,
      "learning_rate": 4.943958804778938e-05,
      "loss": 0.8804,
      "step": 73700
    },
    {
      "epoch": 0.6734068180159136,
      "grad_norm": 5.24700403213501,
      "learning_rate": 4.9438827651653406e-05,
      "loss": 0.8404,
      "step": 73800
    },
    {
      "epoch": 0.6743192933790788,
      "grad_norm": 5.015495300292969,
      "learning_rate": 4.9438067255517436e-05,
      "loss": 0.8366,
      "step": 73900
    },
    {
      "epoch": 0.675231768742244,
      "grad_norm": 3.4284167289733887,
      "learning_rate": 4.9437306859381466e-05,
      "loss": 0.8521,
      "step": 74000
    },
    {
      "epoch": 0.6761442441054092,
      "grad_norm": 4.6031413078308105,
      "learning_rate": 4.943654646324549e-05,
      "loss": 0.8367,
      "step": 74100
    },
    {
      "epoch": 0.6770567194685744,
      "grad_norm": 3.8801307678222656,
      "learning_rate": 4.9435786067109526e-05,
      "loss": 0.821,
      "step": 74200
    },
    {
      "epoch": 0.6779691948317396,
      "grad_norm": 4.244747638702393,
      "learning_rate": 4.943502567097355e-05,
      "loss": 0.8214,
      "step": 74300
    },
    {
      "epoch": 0.6788816701949048,
      "grad_norm": 4.62493896484375,
      "learning_rate": 4.943426527483758e-05,
      "loss": 0.8344,
      "step": 74400
    },
    {
      "epoch": 0.6797941455580699,
      "grad_norm": 4.21189546585083,
      "learning_rate": 4.943350487870161e-05,
      "loss": 0.835,
      "step": 74500
    },
    {
      "epoch": 0.6807066209212351,
      "grad_norm": 4.537622451782227,
      "learning_rate": 4.943274448256564e-05,
      "loss": 0.8715,
      "step": 74600
    },
    {
      "epoch": 0.6816190962844003,
      "grad_norm": 4.150195598602295,
      "learning_rate": 4.943198408642967e-05,
      "loss": 0.8665,
      "step": 74700
    },
    {
      "epoch": 0.6825315716475655,
      "grad_norm": 4.869713306427002,
      "learning_rate": 4.94312236902937e-05,
      "loss": 0.8269,
      "step": 74800
    },
    {
      "epoch": 0.6834440470107307,
      "grad_norm": 3.9774274826049805,
      "learning_rate": 4.943046329415772e-05,
      "loss": 0.8253,
      "step": 74900
    },
    {
      "epoch": 0.6843565223738959,
      "grad_norm": 4.738735198974609,
      "learning_rate": 4.942970289802176e-05,
      "loss": 0.8175,
      "step": 75000
    },
    {
      "epoch": 0.6852689977370611,
      "grad_norm": 4.431366443634033,
      "learning_rate": 4.942894250188578e-05,
      "loss": 0.837,
      "step": 75100
    },
    {
      "epoch": 0.6861814731002263,
      "grad_norm": 5.051496505737305,
      "learning_rate": 4.942818210574981e-05,
      "loss": 0.8371,
      "step": 75200
    },
    {
      "epoch": 0.6870939484633914,
      "grad_norm": 5.32862663269043,
      "learning_rate": 4.9427421709613843e-05,
      "loss": 0.8438,
      "step": 75300
    },
    {
      "epoch": 0.6880064238265566,
      "grad_norm": 4.292612075805664,
      "learning_rate": 4.9426661313477873e-05,
      "loss": 0.8258,
      "step": 75400
    },
    {
      "epoch": 0.6889188991897218,
      "grad_norm": 5.717616558074951,
      "learning_rate": 4.94259009173419e-05,
      "loss": 0.866,
      "step": 75500
    },
    {
      "epoch": 0.6898313745528871,
      "grad_norm": 3.809903144836426,
      "learning_rate": 4.9425140521205934e-05,
      "loss": 0.8385,
      "step": 75600
    },
    {
      "epoch": 0.6907438499160523,
      "grad_norm": 4.629923343658447,
      "learning_rate": 4.942438012506996e-05,
      "loss": 0.8398,
      "step": 75700
    },
    {
      "epoch": 0.6916563252792175,
      "grad_norm": 4.805742263793945,
      "learning_rate": 4.942361972893399e-05,
      "loss": 0.8411,
      "step": 75800
    },
    {
      "epoch": 0.6925688006423827,
      "grad_norm": 4.161559104919434,
      "learning_rate": 4.942285933279802e-05,
      "loss": 0.822,
      "step": 75900
    },
    {
      "epoch": 0.6934812760055479,
      "grad_norm": 4.785707950592041,
      "learning_rate": 4.942209893666205e-05,
      "loss": 0.8577,
      "step": 76000
    },
    {
      "epoch": 0.6943937513687131,
      "grad_norm": 4.735787868499756,
      "learning_rate": 4.942133854052608e-05,
      "loss": 0.8319,
      "step": 76100
    },
    {
      "epoch": 0.6953062267318783,
      "grad_norm": 4.520555019378662,
      "learning_rate": 4.942057814439011e-05,
      "loss": 0.8256,
      "step": 76200
    },
    {
      "epoch": 0.6962187020950434,
      "grad_norm": 3.466339349746704,
      "learning_rate": 4.941981774825413e-05,
      "loss": 0.8461,
      "step": 76300
    },
    {
      "epoch": 0.6971311774582086,
      "grad_norm": 4.562489986419678,
      "learning_rate": 4.941905735211817e-05,
      "loss": 0.833,
      "step": 76400
    },
    {
      "epoch": 0.6980436528213738,
      "grad_norm": 4.7943949699401855,
      "learning_rate": 4.941829695598219e-05,
      "loss": 0.7972,
      "step": 76500
    },
    {
      "epoch": 0.698956128184539,
      "grad_norm": 4.120275497436523,
      "learning_rate": 4.941753655984622e-05,
      "loss": 0.8141,
      "step": 76600
    },
    {
      "epoch": 0.6998686035477042,
      "grad_norm": 4.5894775390625,
      "learning_rate": 4.941677616371025e-05,
      "loss": 0.8593,
      "step": 76700
    },
    {
      "epoch": 0.7007810789108694,
      "grad_norm": 4.56374979019165,
      "learning_rate": 4.9416015767574274e-05,
      "loss": 0.8317,
      "step": 76800
    },
    {
      "epoch": 0.7016935542740346,
      "grad_norm": 3.7669966220855713,
      "learning_rate": 4.9415255371438304e-05,
      "loss": 0.8137,
      "step": 76900
    },
    {
      "epoch": 0.7026060296371998,
      "grad_norm": 4.693946838378906,
      "learning_rate": 4.9414494975302334e-05,
      "loss": 0.8623,
      "step": 77000
    },
    {
      "epoch": 0.703518505000365,
      "grad_norm": 3.0098979473114014,
      "learning_rate": 4.9413734579166364e-05,
      "loss": 0.8473,
      "step": 77100
    },
    {
      "epoch": 0.7044309803635301,
      "grad_norm": 4.986734867095947,
      "learning_rate": 4.9412974183030394e-05,
      "loss": 0.801,
      "step": 77200
    },
    {
      "epoch": 0.7053434557266954,
      "grad_norm": 5.389710426330566,
      "learning_rate": 4.9412213786894424e-05,
      "loss": 0.8474,
      "step": 77300
    },
    {
      "epoch": 0.7062559310898606,
      "grad_norm": 5.18172025680542,
      "learning_rate": 4.941145339075845e-05,
      "loss": 0.8556,
      "step": 77400
    },
    {
      "epoch": 0.7071684064530258,
      "grad_norm": 4.6018967628479,
      "learning_rate": 4.9410692994622485e-05,
      "loss": 0.8558,
      "step": 77500
    },
    {
      "epoch": 0.708080881816191,
      "grad_norm": 3.8747928142547607,
      "learning_rate": 4.940993259848651e-05,
      "loss": 0.807,
      "step": 77600
    },
    {
      "epoch": 0.7089933571793562,
      "grad_norm": 3.8489205837249756,
      "learning_rate": 4.940917220235054e-05,
      "loss": 0.8505,
      "step": 77700
    },
    {
      "epoch": 0.7099058325425214,
      "grad_norm": 4.392721652984619,
      "learning_rate": 4.940841180621457e-05,
      "loss": 0.8585,
      "step": 77800
    },
    {
      "epoch": 0.7108183079056866,
      "grad_norm": 5.071272850036621,
      "learning_rate": 4.94076514100786e-05,
      "loss": 0.8594,
      "step": 77900
    },
    {
      "epoch": 0.7117307832688518,
      "grad_norm": 5.879130840301514,
      "learning_rate": 4.940689101394262e-05,
      "loss": 0.8391,
      "step": 78000
    },
    {
      "epoch": 0.7126432586320169,
      "grad_norm": 4.599280834197998,
      "learning_rate": 4.940613061780666e-05,
      "loss": 0.8129,
      "step": 78100
    },
    {
      "epoch": 0.7135557339951821,
      "grad_norm": 5.115828514099121,
      "learning_rate": 4.940537022167068e-05,
      "loss": 0.8307,
      "step": 78200
    },
    {
      "epoch": 0.7144682093583473,
      "grad_norm": 5.0532145500183105,
      "learning_rate": 4.940460982553471e-05,
      "loss": 0.873,
      "step": 78300
    },
    {
      "epoch": 0.7153806847215125,
      "grad_norm": 4.184568405151367,
      "learning_rate": 4.940384942939874e-05,
      "loss": 0.8305,
      "step": 78400
    },
    {
      "epoch": 0.7162931600846777,
      "grad_norm": 4.607684135437012,
      "learning_rate": 4.940308903326277e-05,
      "loss": 0.861,
      "step": 78500
    },
    {
      "epoch": 0.7172056354478429,
      "grad_norm": 4.45864725112915,
      "learning_rate": 4.94023286371268e-05,
      "loss": 0.8553,
      "step": 78600
    },
    {
      "epoch": 0.7181181108110081,
      "grad_norm": 3.7003118991851807,
      "learning_rate": 4.940156824099083e-05,
      "loss": 0.8124,
      "step": 78700
    },
    {
      "epoch": 0.7190305861741733,
      "grad_norm": 3.5617058277130127,
      "learning_rate": 4.9400807844854855e-05,
      "loss": 0.798,
      "step": 78800
    },
    {
      "epoch": 0.7199430615373384,
      "grad_norm": 4.51419734954834,
      "learning_rate": 4.940004744871889e-05,
      "loss": 0.8886,
      "step": 78900
    },
    {
      "epoch": 0.7208555369005036,
      "grad_norm": 3.588139295578003,
      "learning_rate": 4.9399287052582915e-05,
      "loss": 0.7935,
      "step": 79000
    },
    {
      "epoch": 0.7217680122636689,
      "grad_norm": 4.746662139892578,
      "learning_rate": 4.9398526656446945e-05,
      "loss": 0.8474,
      "step": 79100
    },
    {
      "epoch": 0.7226804876268341,
      "grad_norm": 5.28711462020874,
      "learning_rate": 4.9397766260310975e-05,
      "loss": 0.794,
      "step": 79200
    },
    {
      "epoch": 0.7235929629899993,
      "grad_norm": 5.566308975219727,
      "learning_rate": 4.9397005864175005e-05,
      "loss": 0.8654,
      "step": 79300
    },
    {
      "epoch": 0.7245054383531645,
      "grad_norm": 4.700675964355469,
      "learning_rate": 4.939624546803903e-05,
      "loss": 0.8668,
      "step": 79400
    },
    {
      "epoch": 0.7254179137163297,
      "grad_norm": 4.508908271789551,
      "learning_rate": 4.9395485071903066e-05,
      "loss": 0.8683,
      "step": 79500
    },
    {
      "epoch": 0.7263303890794949,
      "grad_norm": 4.685889720916748,
      "learning_rate": 4.939472467576709e-05,
      "loss": 0.8575,
      "step": 79600
    },
    {
      "epoch": 0.7272428644426601,
      "grad_norm": 5.073739051818848,
      "learning_rate": 4.939396427963112e-05,
      "loss": 0.8575,
      "step": 79700
    },
    {
      "epoch": 0.7281553398058253,
      "grad_norm": 4.6471381187438965,
      "learning_rate": 4.939320388349515e-05,
      "loss": 0.8298,
      "step": 79800
    },
    {
      "epoch": 0.7290678151689904,
      "grad_norm": 4.721318244934082,
      "learning_rate": 4.939244348735917e-05,
      "loss": 0.8083,
      "step": 79900
    },
    {
      "epoch": 0.7299802905321556,
      "grad_norm": 4.105016231536865,
      "learning_rate": 4.939168309122321e-05,
      "loss": 0.8387,
      "step": 80000
    },
    {
      "epoch": 0.7308927658953208,
      "grad_norm": 4.181161880493164,
      "learning_rate": 4.939092269508723e-05,
      "loss": 0.82,
      "step": 80100
    },
    {
      "epoch": 0.731805241258486,
      "grad_norm": 4.324868202209473,
      "learning_rate": 4.939016229895126e-05,
      "loss": 0.842,
      "step": 80200
    },
    {
      "epoch": 0.7327177166216512,
      "grad_norm": 4.650515079498291,
      "learning_rate": 4.938940190281529e-05,
      "loss": 0.8441,
      "step": 80300
    },
    {
      "epoch": 0.7336301919848164,
      "grad_norm": 4.224762916564941,
      "learning_rate": 4.938864150667932e-05,
      "loss": 0.8234,
      "step": 80400
    },
    {
      "epoch": 0.7345426673479816,
      "grad_norm": 4.223389625549316,
      "learning_rate": 4.9387881110543346e-05,
      "loss": 0.8254,
      "step": 80500
    },
    {
      "epoch": 0.7354551427111468,
      "grad_norm": 4.766778945922852,
      "learning_rate": 4.938712071440738e-05,
      "loss": 0.8266,
      "step": 80600
    },
    {
      "epoch": 0.736367618074312,
      "grad_norm": 4.829016208648682,
      "learning_rate": 4.9386360318271406e-05,
      "loss": 0.8251,
      "step": 80700
    },
    {
      "epoch": 0.7372800934374771,
      "grad_norm": 5.915027618408203,
      "learning_rate": 4.9385599922135436e-05,
      "loss": 0.7832,
      "step": 80800
    },
    {
      "epoch": 0.7381925688006424,
      "grad_norm": 4.178036689758301,
      "learning_rate": 4.9384839525999466e-05,
      "loss": 0.8254,
      "step": 80900
    },
    {
      "epoch": 0.7391050441638076,
      "grad_norm": 4.463291168212891,
      "learning_rate": 4.9384079129863496e-05,
      "loss": 0.8593,
      "step": 81000
    },
    {
      "epoch": 0.7400175195269728,
      "grad_norm": 3.762373447418213,
      "learning_rate": 4.9383318733727526e-05,
      "loss": 0.8217,
      "step": 81100
    },
    {
      "epoch": 0.740929994890138,
      "grad_norm": 4.81947135925293,
      "learning_rate": 4.9382558337591556e-05,
      "loss": 0.8384,
      "step": 81200
    },
    {
      "epoch": 0.7418424702533032,
      "grad_norm": 4.852123260498047,
      "learning_rate": 4.938179794145558e-05,
      "loss": 0.8044,
      "step": 81300
    },
    {
      "epoch": 0.7427549456164684,
      "grad_norm": 4.933689594268799,
      "learning_rate": 4.9381037545319617e-05,
      "loss": 0.8489,
      "step": 81400
    },
    {
      "epoch": 0.7436674209796336,
      "grad_norm": 4.180702209472656,
      "learning_rate": 4.938027714918364e-05,
      "loss": 0.8404,
      "step": 81500
    },
    {
      "epoch": 0.7445798963427988,
      "grad_norm": 5.73431396484375,
      "learning_rate": 4.937951675304767e-05,
      "loss": 0.8391,
      "step": 81600
    },
    {
      "epoch": 0.7454923717059639,
      "grad_norm": 4.923954486846924,
      "learning_rate": 4.93787563569117e-05,
      "loss": 0.7941,
      "step": 81700
    },
    {
      "epoch": 0.7464048470691291,
      "grad_norm": 4.900060176849365,
      "learning_rate": 4.937799596077573e-05,
      "loss": 0.8127,
      "step": 81800
    },
    {
      "epoch": 0.7473173224322943,
      "grad_norm": 4.708291530609131,
      "learning_rate": 4.937723556463975e-05,
      "loss": 0.7965,
      "step": 81900
    },
    {
      "epoch": 0.7482297977954595,
      "grad_norm": 4.14299201965332,
      "learning_rate": 4.937647516850379e-05,
      "loss": 0.8238,
      "step": 82000
    },
    {
      "epoch": 0.7491422731586247,
      "grad_norm": 4.260895729064941,
      "learning_rate": 4.9375714772367813e-05,
      "loss": 0.8435,
      "step": 82100
    },
    {
      "epoch": 0.7500547485217899,
      "grad_norm": 5.269938945770264,
      "learning_rate": 4.9374954376231844e-05,
      "loss": 0.8121,
      "step": 82200
    },
    {
      "epoch": 0.7509672238849551,
      "grad_norm": 4.494765758514404,
      "learning_rate": 4.9374193980095874e-05,
      "loss": 0.8699,
      "step": 82300
    },
    {
      "epoch": 0.7518796992481203,
      "grad_norm": 4.834014415740967,
      "learning_rate": 4.93734335839599e-05,
      "loss": 0.8418,
      "step": 82400
    },
    {
      "epoch": 0.7527921746112854,
      "grad_norm": 4.078907489776611,
      "learning_rate": 4.9372673187823934e-05,
      "loss": 0.8391,
      "step": 82500
    },
    {
      "epoch": 0.7537046499744506,
      "grad_norm": 3.4347715377807617,
      "learning_rate": 4.937191279168796e-05,
      "loss": 0.8194,
      "step": 82600
    },
    {
      "epoch": 0.7546171253376159,
      "grad_norm": 5.28909158706665,
      "learning_rate": 4.937115239555199e-05,
      "loss": 0.8375,
      "step": 82700
    },
    {
      "epoch": 0.7555296007007811,
      "grad_norm": 5.127830982208252,
      "learning_rate": 4.937039199941602e-05,
      "loss": 0.8517,
      "step": 82800
    },
    {
      "epoch": 0.7564420760639463,
      "grad_norm": 5.0216240882873535,
      "learning_rate": 4.936963160328005e-05,
      "loss": 0.8662,
      "step": 82900
    },
    {
      "epoch": 0.7573545514271115,
      "grad_norm": 4.077911853790283,
      "learning_rate": 4.936887120714408e-05,
      "loss": 0.8077,
      "step": 83000
    },
    {
      "epoch": 0.7582670267902767,
      "grad_norm": 4.182953834533691,
      "learning_rate": 4.936811081100811e-05,
      "loss": 0.8132,
      "step": 83100
    },
    {
      "epoch": 0.7591795021534419,
      "grad_norm": 5.015237331390381,
      "learning_rate": 4.936735041487213e-05,
      "loss": 0.8239,
      "step": 83200
    },
    {
      "epoch": 0.7600919775166071,
      "grad_norm": 3.561929225921631,
      "learning_rate": 4.936659001873616e-05,
      "loss": 0.8396,
      "step": 83300
    },
    {
      "epoch": 0.7610044528797723,
      "grad_norm": 4.0269246101379395,
      "learning_rate": 4.936582962260019e-05,
      "loss": 0.831,
      "step": 83400
    },
    {
      "epoch": 0.7619169282429374,
      "grad_norm": 5.231485843658447,
      "learning_rate": 4.936506922646422e-05,
      "loss": 0.8288,
      "step": 83500
    },
    {
      "epoch": 0.7628294036061026,
      "grad_norm": 3.9078054428100586,
      "learning_rate": 4.936430883032825e-05,
      "loss": 0.8277,
      "step": 83600
    },
    {
      "epoch": 0.7637418789692678,
      "grad_norm": 4.378294944763184,
      "learning_rate": 4.936354843419228e-05,
      "loss": 0.828,
      "step": 83700
    },
    {
      "epoch": 0.764654354332433,
      "grad_norm": 5.461452960968018,
      "learning_rate": 4.9362788038056304e-05,
      "loss": 0.8497,
      "step": 83800
    },
    {
      "epoch": 0.7655668296955982,
      "grad_norm": 4.476361274719238,
      "learning_rate": 4.936202764192034e-05,
      "loss": 0.8097,
      "step": 83900
    },
    {
      "epoch": 0.7664793050587634,
      "grad_norm": 4.691394805908203,
      "learning_rate": 4.9361267245784364e-05,
      "loss": 0.8477,
      "step": 84000
    },
    {
      "epoch": 0.7673917804219286,
      "grad_norm": 4.5455827713012695,
      "learning_rate": 4.9360506849648394e-05,
      "loss": 0.837,
      "step": 84100
    },
    {
      "epoch": 0.7683042557850938,
      "grad_norm": 4.977505207061768,
      "learning_rate": 4.9359746453512425e-05,
      "loss": 0.865,
      "step": 84200
    },
    {
      "epoch": 0.769216731148259,
      "grad_norm": 3.8330132961273193,
      "learning_rate": 4.9358986057376455e-05,
      "loss": 0.8604,
      "step": 84300
    },
    {
      "epoch": 0.7701292065114242,
      "grad_norm": 4.165061950683594,
      "learning_rate": 4.9358225661240485e-05,
      "loss": 0.8137,
      "step": 84400
    },
    {
      "epoch": 0.7710416818745894,
      "grad_norm": 4.183239936828613,
      "learning_rate": 4.9357465265104515e-05,
      "loss": 0.8418,
      "step": 84500
    },
    {
      "epoch": 0.7719541572377546,
      "grad_norm": 4.276828765869141,
      "learning_rate": 4.935670486896854e-05,
      "loss": 0.819,
      "step": 84600
    },
    {
      "epoch": 0.7728666326009198,
      "grad_norm": 4.8378448486328125,
      "learning_rate": 4.9355944472832575e-05,
      "loss": 0.8538,
      "step": 84700
    },
    {
      "epoch": 0.773779107964085,
      "grad_norm": 4.833832263946533,
      "learning_rate": 4.93551840766966e-05,
      "loss": 0.8515,
      "step": 84800
    },
    {
      "epoch": 0.7746915833272502,
      "grad_norm": 4.399167537689209,
      "learning_rate": 4.935442368056063e-05,
      "loss": 0.8086,
      "step": 84900
    },
    {
      "epoch": 0.7756040586904154,
      "grad_norm": 4.619300842285156,
      "learning_rate": 4.935366328442466e-05,
      "loss": 0.836,
      "step": 85000
    },
    {
      "epoch": 0.7765165340535806,
      "grad_norm": 5.018957138061523,
      "learning_rate": 4.935290288828869e-05,
      "loss": 0.8002,
      "step": 85100
    },
    {
      "epoch": 0.7774290094167458,
      "grad_norm": 4.277730464935303,
      "learning_rate": 4.935214249215271e-05,
      "loss": 0.8244,
      "step": 85200
    },
    {
      "epoch": 0.7783414847799109,
      "grad_norm": 3.882298231124878,
      "learning_rate": 4.935138209601674e-05,
      "loss": 0.8657,
      "step": 85300
    },
    {
      "epoch": 0.7792539601430761,
      "grad_norm": 4.925764560699463,
      "learning_rate": 4.935062169988077e-05,
      "loss": 0.8526,
      "step": 85400
    },
    {
      "epoch": 0.7801664355062413,
      "grad_norm": 4.468506336212158,
      "learning_rate": 4.93498613037448e-05,
      "loss": 0.8009,
      "step": 85500
    },
    {
      "epoch": 0.7810789108694065,
      "grad_norm": 5.089028835296631,
      "learning_rate": 4.934910090760883e-05,
      "loss": 0.8434,
      "step": 85600
    },
    {
      "epoch": 0.7819913862325717,
      "grad_norm": 5.1554388999938965,
      "learning_rate": 4.9348340511472855e-05,
      "loss": 0.8667,
      "step": 85700
    },
    {
      "epoch": 0.7829038615957369,
      "grad_norm": 3.8023324012756348,
      "learning_rate": 4.934758011533689e-05,
      "loss": 0.8161,
      "step": 85800
    },
    {
      "epoch": 0.7838163369589021,
      "grad_norm": 4.654780387878418,
      "learning_rate": 4.9346819719200915e-05,
      "loss": 0.8404,
      "step": 85900
    },
    {
      "epoch": 0.7847288123220673,
      "grad_norm": 2.8039894104003906,
      "learning_rate": 4.9346059323064945e-05,
      "loss": 0.8102,
      "step": 86000
    },
    {
      "epoch": 0.7856412876852324,
      "grad_norm": 4.517291069030762,
      "learning_rate": 4.9345298926928975e-05,
      "loss": 0.8153,
      "step": 86100
    },
    {
      "epoch": 0.7865537630483977,
      "grad_norm": 3.7413532733917236,
      "learning_rate": 4.9344538530793006e-05,
      "loss": 0.7867,
      "step": 86200
    },
    {
      "epoch": 0.7874662384115629,
      "grad_norm": 4.505010604858398,
      "learning_rate": 4.934377813465703e-05,
      "loss": 0.8377,
      "step": 86300
    },
    {
      "epoch": 0.7883787137747281,
      "grad_norm": 5.705231189727783,
      "learning_rate": 4.9343017738521066e-05,
      "loss": 0.8314,
      "step": 86400
    },
    {
      "epoch": 0.7892911891378933,
      "grad_norm": 4.573914527893066,
      "learning_rate": 4.934225734238509e-05,
      "loss": 0.8249,
      "step": 86500
    },
    {
      "epoch": 0.7902036645010585,
      "grad_norm": 3.5666134357452393,
      "learning_rate": 4.934149694624912e-05,
      "loss": 0.8234,
      "step": 86600
    },
    {
      "epoch": 0.7911161398642237,
      "grad_norm": 4.860886096954346,
      "learning_rate": 4.934073655011315e-05,
      "loss": 0.816,
      "step": 86700
    },
    {
      "epoch": 0.7920286152273889,
      "grad_norm": 4.2314863204956055,
      "learning_rate": 4.933997615397718e-05,
      "loss": 0.8162,
      "step": 86800
    },
    {
      "epoch": 0.7929410905905541,
      "grad_norm": 4.378541469573975,
      "learning_rate": 4.933921575784121e-05,
      "loss": 0.8139,
      "step": 86900
    },
    {
      "epoch": 0.7938535659537193,
      "grad_norm": 3.961730480194092,
      "learning_rate": 4.933845536170524e-05,
      "loss": 0.8406,
      "step": 87000
    },
    {
      "epoch": 0.7947660413168844,
      "grad_norm": 4.271843433380127,
      "learning_rate": 4.933769496556926e-05,
      "loss": 0.82,
      "step": 87100
    },
    {
      "epoch": 0.7956785166800496,
      "grad_norm": 4.080942630767822,
      "learning_rate": 4.93369345694333e-05,
      "loss": 0.7915,
      "step": 87200
    },
    {
      "epoch": 0.7965909920432148,
      "grad_norm": 4.274077892303467,
      "learning_rate": 4.933617417329732e-05,
      "loss": 0.8724,
      "step": 87300
    },
    {
      "epoch": 0.79750346740638,
      "grad_norm": 4.241261959075928,
      "learning_rate": 4.933541377716135e-05,
      "loss": 0.8031,
      "step": 87400
    },
    {
      "epoch": 0.7984159427695452,
      "grad_norm": 3.8581318855285645,
      "learning_rate": 4.933465338102538e-05,
      "loss": 0.8278,
      "step": 87500
    },
    {
      "epoch": 0.7993284181327104,
      "grad_norm": 4.490302562713623,
      "learning_rate": 4.933389298488941e-05,
      "loss": 0.8584,
      "step": 87600
    },
    {
      "epoch": 0.8002408934958756,
      "grad_norm": 5.00104284286499,
      "learning_rate": 4.9333132588753436e-05,
      "loss": 0.8606,
      "step": 87700
    },
    {
      "epoch": 0.8011533688590408,
      "grad_norm": 3.3440651893615723,
      "learning_rate": 4.933237219261747e-05,
      "loss": 0.8403,
      "step": 87800
    },
    {
      "epoch": 0.802065844222206,
      "grad_norm": 4.0537919998168945,
      "learning_rate": 4.9331611796481496e-05,
      "loss": 0.8202,
      "step": 87900
    },
    {
      "epoch": 0.8029783195853712,
      "grad_norm": 3.298736572265625,
      "learning_rate": 4.9330851400345526e-05,
      "loss": 0.873,
      "step": 88000
    },
    {
      "epoch": 0.8038907949485364,
      "grad_norm": 3.1193559169769287,
      "learning_rate": 4.9330091004209556e-05,
      "loss": 0.8291,
      "step": 88100
    },
    {
      "epoch": 0.8048032703117016,
      "grad_norm": 4.043341159820557,
      "learning_rate": 4.932933060807358e-05,
      "loss": 0.8402,
      "step": 88200
    },
    {
      "epoch": 0.8057157456748668,
      "grad_norm": 3.840322494506836,
      "learning_rate": 4.932857021193762e-05,
      "loss": 0.8218,
      "step": 88300
    },
    {
      "epoch": 0.806628221038032,
      "grad_norm": 4.854350566864014,
      "learning_rate": 4.932780981580164e-05,
      "loss": 0.8331,
      "step": 88400
    },
    {
      "epoch": 0.8075406964011972,
      "grad_norm": 5.199192047119141,
      "learning_rate": 4.932704941966567e-05,
      "loss": 0.8191,
      "step": 88500
    },
    {
      "epoch": 0.8084531717643624,
      "grad_norm": 4.356496810913086,
      "learning_rate": 4.93262890235297e-05,
      "loss": 0.8443,
      "step": 88600
    },
    {
      "epoch": 0.8093656471275276,
      "grad_norm": 3.9440205097198486,
      "learning_rate": 4.932552862739373e-05,
      "loss": 0.8348,
      "step": 88700
    },
    {
      "epoch": 0.8102781224906928,
      "grad_norm": 4.54090690612793,
      "learning_rate": 4.9324768231257753e-05,
      "loss": 0.8403,
      "step": 88800
    },
    {
      "epoch": 0.8111905978538579,
      "grad_norm": 4.447544097900391,
      "learning_rate": 4.932400783512179e-05,
      "loss": 0.8375,
      "step": 88900
    },
    {
      "epoch": 0.8121030732170231,
      "grad_norm": 4.566890716552734,
      "learning_rate": 4.9323247438985814e-05,
      "loss": 0.8337,
      "step": 89000
    },
    {
      "epoch": 0.8130155485801883,
      "grad_norm": 4.9653167724609375,
      "learning_rate": 4.9322487042849844e-05,
      "loss": 0.8216,
      "step": 89100
    },
    {
      "epoch": 0.8139280239433535,
      "grad_norm": 4.514758110046387,
      "learning_rate": 4.9321726646713874e-05,
      "loss": 0.8033,
      "step": 89200
    },
    {
      "epoch": 0.8148404993065187,
      "grad_norm": 4.57930326461792,
      "learning_rate": 4.9320966250577904e-05,
      "loss": 0.8615,
      "step": 89300
    },
    {
      "epoch": 0.8157529746696839,
      "grad_norm": 4.192154407501221,
      "learning_rate": 4.9320205854441934e-05,
      "loss": 0.7909,
      "step": 89400
    },
    {
      "epoch": 0.8166654500328491,
      "grad_norm": 5.125550746917725,
      "learning_rate": 4.9319445458305964e-05,
      "loss": 0.7466,
      "step": 89500
    },
    {
      "epoch": 0.8175779253960143,
      "grad_norm": 4.338044166564941,
      "learning_rate": 4.931868506216999e-05,
      "loss": 0.8134,
      "step": 89600
    },
    {
      "epoch": 0.8184904007591794,
      "grad_norm": 3.4449684619903564,
      "learning_rate": 4.9317924666034024e-05,
      "loss": 0.8192,
      "step": 89700
    },
    {
      "epoch": 0.8194028761223447,
      "grad_norm": 4.257543563842773,
      "learning_rate": 4.931716426989805e-05,
      "loss": 0.8462,
      "step": 89800
    },
    {
      "epoch": 0.8203153514855099,
      "grad_norm": 5.289962291717529,
      "learning_rate": 4.931640387376208e-05,
      "loss": 0.8226,
      "step": 89900
    },
    {
      "epoch": 0.8212278268486751,
      "grad_norm": 5.156586647033691,
      "learning_rate": 4.931564347762611e-05,
      "loss": 0.8483,
      "step": 90000
    },
    {
      "epoch": 0.8221403022118403,
      "grad_norm": 5.811824798583984,
      "learning_rate": 4.931488308149014e-05,
      "loss": 0.8558,
      "step": 90100
    },
    {
      "epoch": 0.8230527775750055,
      "grad_norm": 4.010898113250732,
      "learning_rate": 4.931412268535416e-05,
      "loss": 0.8298,
      "step": 90200
    },
    {
      "epoch": 0.8239652529381707,
      "grad_norm": 4.552169322967529,
      "learning_rate": 4.93133622892182e-05,
      "loss": 0.8705,
      "step": 90300
    },
    {
      "epoch": 0.8248777283013359,
      "grad_norm": 5.070095062255859,
      "learning_rate": 4.931260189308222e-05,
      "loss": 0.852,
      "step": 90400
    },
    {
      "epoch": 0.8257902036645011,
      "grad_norm": 4.923733711242676,
      "learning_rate": 4.931184149694625e-05,
      "loss": 0.838,
      "step": 90500
    },
    {
      "epoch": 0.8267026790276663,
      "grad_norm": 4.719942569732666,
      "learning_rate": 4.931108110081028e-05,
      "loss": 0.8025,
      "step": 90600
    },
    {
      "epoch": 0.8276151543908314,
      "grad_norm": 3.903637647628784,
      "learning_rate": 4.931032070467431e-05,
      "loss": 0.8122,
      "step": 90700
    },
    {
      "epoch": 0.8285276297539966,
      "grad_norm": 4.348480701446533,
      "learning_rate": 4.930956030853834e-05,
      "loss": 0.8297,
      "step": 90800
    },
    {
      "epoch": 0.8294401051171618,
      "grad_norm": 3.9478089809417725,
      "learning_rate": 4.9308799912402364e-05,
      "loss": 0.8164,
      "step": 90900
    },
    {
      "epoch": 0.830352580480327,
      "grad_norm": 2.9257476329803467,
      "learning_rate": 4.9308039516266395e-05,
      "loss": 0.8259,
      "step": 91000
    },
    {
      "epoch": 0.8312650558434922,
      "grad_norm": 4.219361305236816,
      "learning_rate": 4.9307279120130425e-05,
      "loss": 0.8293,
      "step": 91100
    },
    {
      "epoch": 0.8321775312066574,
      "grad_norm": 3.513094663619995,
      "learning_rate": 4.9306518723994455e-05,
      "loss": 0.8213,
      "step": 91200
    },
    {
      "epoch": 0.8330900065698226,
      "grad_norm": 3.8263747692108154,
      "learning_rate": 4.930575832785848e-05,
      "loss": 0.8503,
      "step": 91300
    },
    {
      "epoch": 0.8340024819329878,
      "grad_norm": 3.4157955646514893,
      "learning_rate": 4.9304997931722515e-05,
      "loss": 0.816,
      "step": 91400
    },
    {
      "epoch": 0.834914957296153,
      "grad_norm": 4.516419410705566,
      "learning_rate": 4.930423753558654e-05,
      "loss": 0.8532,
      "step": 91500
    },
    {
      "epoch": 0.8358274326593182,
      "grad_norm": 4.195001125335693,
      "learning_rate": 4.930347713945057e-05,
      "loss": 0.7935,
      "step": 91600
    },
    {
      "epoch": 0.8367399080224834,
      "grad_norm": 4.643110275268555,
      "learning_rate": 4.93027167433146e-05,
      "loss": 0.8756,
      "step": 91700
    },
    {
      "epoch": 0.8376523833856486,
      "grad_norm": 5.741189479827881,
      "learning_rate": 4.930195634717863e-05,
      "loss": 0.8316,
      "step": 91800
    },
    {
      "epoch": 0.8385648587488138,
      "grad_norm": 3.861846685409546,
      "learning_rate": 4.930119595104266e-05,
      "loss": 0.8106,
      "step": 91900
    },
    {
      "epoch": 0.839477334111979,
      "grad_norm": 4.73490571975708,
      "learning_rate": 4.930043555490669e-05,
      "loss": 0.8087,
      "step": 92000
    },
    {
      "epoch": 0.8403898094751442,
      "grad_norm": 3.5237624645233154,
      "learning_rate": 4.929967515877071e-05,
      "loss": 0.7609,
      "step": 92100
    },
    {
      "epoch": 0.8413022848383094,
      "grad_norm": 3.9503321647644043,
      "learning_rate": 4.929891476263475e-05,
      "loss": 0.7831,
      "step": 92200
    },
    {
      "epoch": 0.8422147602014746,
      "grad_norm": 4.711704730987549,
      "learning_rate": 4.929815436649877e-05,
      "loss": 0.8758,
      "step": 92300
    },
    {
      "epoch": 0.8431272355646398,
      "grad_norm": 4.428909778594971,
      "learning_rate": 4.92973939703628e-05,
      "loss": 0.8116,
      "step": 92400
    },
    {
      "epoch": 0.8440397109278049,
      "grad_norm": 4.171692371368408,
      "learning_rate": 4.929663357422683e-05,
      "loss": 0.7813,
      "step": 92500
    },
    {
      "epoch": 0.8449521862909701,
      "grad_norm": 4.87624979019165,
      "learning_rate": 4.929587317809086e-05,
      "loss": 0.8014,
      "step": 92600
    },
    {
      "epoch": 0.8458646616541353,
      "grad_norm": 5.238212585449219,
      "learning_rate": 4.9295112781954885e-05,
      "loss": 0.8667,
      "step": 92700
    },
    {
      "epoch": 0.8467771370173005,
      "grad_norm": 5.2574896812438965,
      "learning_rate": 4.929435238581892e-05,
      "loss": 0.8323,
      "step": 92800
    },
    {
      "epoch": 0.8476896123804657,
      "grad_norm": 4.23288631439209,
      "learning_rate": 4.9293591989682946e-05,
      "loss": 0.8307,
      "step": 92900
    },
    {
      "epoch": 0.8486020877436309,
      "grad_norm": 4.431314945220947,
      "learning_rate": 4.9292831593546976e-05,
      "loss": 0.8131,
      "step": 93000
    },
    {
      "epoch": 0.8495145631067961,
      "grad_norm": 4.237278461456299,
      "learning_rate": 4.9292071197411006e-05,
      "loss": 0.8881,
      "step": 93100
    },
    {
      "epoch": 0.8504270384699613,
      "grad_norm": 4.318561553955078,
      "learning_rate": 4.9291310801275036e-05,
      "loss": 0.812,
      "step": 93200
    },
    {
      "epoch": 0.8513395138331266,
      "grad_norm": 4.127457618713379,
      "learning_rate": 4.9290550405139066e-05,
      "loss": 0.8081,
      "step": 93300
    },
    {
      "epoch": 0.8522519891962917,
      "grad_norm": 4.378271579742432,
      "learning_rate": 4.9289790009003096e-05,
      "loss": 0.8493,
      "step": 93400
    },
    {
      "epoch": 0.8531644645594569,
      "grad_norm": 4.862797260284424,
      "learning_rate": 4.928902961286712e-05,
      "loss": 0.8203,
      "step": 93500
    },
    {
      "epoch": 0.8540769399226221,
      "grad_norm": 4.263881206512451,
      "learning_rate": 4.9288269216731156e-05,
      "loss": 0.8243,
      "step": 93600
    },
    {
      "epoch": 0.8549894152857873,
      "grad_norm": 4.115057945251465,
      "learning_rate": 4.928750882059518e-05,
      "loss": 0.7897,
      "step": 93700
    },
    {
      "epoch": 0.8559018906489525,
      "grad_norm": 4.02838134765625,
      "learning_rate": 4.92867484244592e-05,
      "loss": 0.7553,
      "step": 93800
    },
    {
      "epoch": 0.8568143660121177,
      "grad_norm": 4.784608364105225,
      "learning_rate": 4.928598802832324e-05,
      "loss": 0.8046,
      "step": 93900
    },
    {
      "epoch": 0.8577268413752829,
      "grad_norm": 4.254231929779053,
      "learning_rate": 4.928522763218726e-05,
      "loss": 0.7455,
      "step": 94000
    },
    {
      "epoch": 0.8586393167384481,
      "grad_norm": 4.496993541717529,
      "learning_rate": 4.928446723605129e-05,
      "loss": 0.841,
      "step": 94100
    },
    {
      "epoch": 0.8595517921016133,
      "grad_norm": 4.228516578674316,
      "learning_rate": 4.928370683991532e-05,
      "loss": 0.7997,
      "step": 94200
    },
    {
      "epoch": 0.8604642674647784,
      "grad_norm": 4.209831714630127,
      "learning_rate": 4.928294644377935e-05,
      "loss": 0.7774,
      "step": 94300
    },
    {
      "epoch": 0.8613767428279436,
      "grad_norm": 4.667964935302734,
      "learning_rate": 4.928218604764338e-05,
      "loss": 0.8281,
      "step": 94400
    },
    {
      "epoch": 0.8622892181911088,
      "grad_norm": 4.248825550079346,
      "learning_rate": 4.928142565150741e-05,
      "loss": 0.8383,
      "step": 94500
    },
    {
      "epoch": 0.863201693554274,
      "grad_norm": 3.569049119949341,
      "learning_rate": 4.9280665255371436e-05,
      "loss": 0.8239,
      "step": 94600
    },
    {
      "epoch": 0.8641141689174392,
      "grad_norm": 4.663922309875488,
      "learning_rate": 4.927990485923547e-05,
      "loss": 0.8306,
      "step": 94700
    },
    {
      "epoch": 0.8650266442806044,
      "grad_norm": 2.4045658111572266,
      "learning_rate": 4.9279144463099496e-05,
      "loss": 0.8582,
      "step": 94800
    },
    {
      "epoch": 0.8659391196437696,
      "grad_norm": 4.205658435821533,
      "learning_rate": 4.9278384066963527e-05,
      "loss": 0.7863,
      "step": 94900
    },
    {
      "epoch": 0.8668515950069348,
      "grad_norm": 4.192543029785156,
      "learning_rate": 4.9277623670827557e-05,
      "loss": 0.7865,
      "step": 95000
    },
    {
      "epoch": 0.8677640703701001,
      "grad_norm": 4.035797595977783,
      "learning_rate": 4.927686327469159e-05,
      "loss": 0.8243,
      "step": 95100
    },
    {
      "epoch": 0.8686765457332652,
      "grad_norm": 4.079620838165283,
      "learning_rate": 4.927610287855562e-05,
      "loss": 0.8309,
      "step": 95200
    },
    {
      "epoch": 0.8695890210964304,
      "grad_norm": 4.9579548835754395,
      "learning_rate": 4.927534248241965e-05,
      "loss": 0.8484,
      "step": 95300
    },
    {
      "epoch": 0.8705014964595956,
      "grad_norm": 4.3537797927856445,
      "learning_rate": 4.927458208628367e-05,
      "loss": 0.7887,
      "step": 95400
    },
    {
      "epoch": 0.8714139718227608,
      "grad_norm": 5.397519588470459,
      "learning_rate": 4.92738216901477e-05,
      "loss": 0.8336,
      "step": 95500
    },
    {
      "epoch": 0.872326447185926,
      "grad_norm": 4.7006659507751465,
      "learning_rate": 4.927306129401173e-05,
      "loss": 0.8252,
      "step": 95600
    },
    {
      "epoch": 0.8732389225490912,
      "grad_norm": 4.504152297973633,
      "learning_rate": 4.927230089787576e-05,
      "loss": 0.8375,
      "step": 95700
    },
    {
      "epoch": 0.8741513979122564,
      "grad_norm": 4.821296691894531,
      "learning_rate": 4.927154050173979e-05,
      "loss": 0.8261,
      "step": 95800
    },
    {
      "epoch": 0.8750638732754216,
      "grad_norm": 4.691882133483887,
      "learning_rate": 4.927078010560382e-05,
      "loss": 0.828,
      "step": 95900
    },
    {
      "epoch": 0.8759763486385868,
      "grad_norm": 4.3082075119018555,
      "learning_rate": 4.9270019709467844e-05,
      "loss": 0.8404,
      "step": 96000
    },
    {
      "epoch": 0.8768888240017519,
      "grad_norm": 4.327220439910889,
      "learning_rate": 4.926925931333188e-05,
      "loss": 0.8019,
      "step": 96100
    },
    {
      "epoch": 0.8778012993649171,
      "grad_norm": 4.894442558288574,
      "learning_rate": 4.9268498917195904e-05,
      "loss": 0.8227,
      "step": 96200
    },
    {
      "epoch": 0.8787137747280823,
      "grad_norm": 4.060361862182617,
      "learning_rate": 4.9267738521059934e-05,
      "loss": 0.815,
      "step": 96300
    },
    {
      "epoch": 0.8796262500912475,
      "grad_norm": 4.348201751708984,
      "learning_rate": 4.9266978124923964e-05,
      "loss": 0.8396,
      "step": 96400
    },
    {
      "epoch": 0.8805387254544127,
      "grad_norm": 4.711004734039307,
      "learning_rate": 4.9266217728787994e-05,
      "loss": 0.8082,
      "step": 96500
    },
    {
      "epoch": 0.8814512008175779,
      "grad_norm": 3.671313524246216,
      "learning_rate": 4.9265457332652024e-05,
      "loss": 0.8072,
      "step": 96600
    },
    {
      "epoch": 0.8823636761807431,
      "grad_norm": 5.566470623016357,
      "learning_rate": 4.926469693651605e-05,
      "loss": 0.8203,
      "step": 96700
    },
    {
      "epoch": 0.8832761515439083,
      "grad_norm": 4.083470821380615,
      "learning_rate": 4.926393654038008e-05,
      "loss": 0.8341,
      "step": 96800
    },
    {
      "epoch": 0.8841886269070736,
      "grad_norm": 4.59853458404541,
      "learning_rate": 4.926317614424411e-05,
      "loss": 0.8069,
      "step": 96900
    },
    {
      "epoch": 0.8851011022702387,
      "grad_norm": 3.283491373062134,
      "learning_rate": 4.926241574810814e-05,
      "loss": 0.7922,
      "step": 97000
    },
    {
      "epoch": 0.8860135776334039,
      "grad_norm": 4.403841018676758,
      "learning_rate": 4.926165535197216e-05,
      "loss": 0.8287,
      "step": 97100
    },
    {
      "epoch": 0.8869260529965691,
      "grad_norm": 4.857583045959473,
      "learning_rate": 4.92608949558362e-05,
      "loss": 0.8477,
      "step": 97200
    },
    {
      "epoch": 0.8878385283597343,
      "grad_norm": 3.9166483879089355,
      "learning_rate": 4.926013455970022e-05,
      "loss": 0.7831,
      "step": 97300
    },
    {
      "epoch": 0.8887510037228995,
      "grad_norm": 4.744034290313721,
      "learning_rate": 4.925937416356425e-05,
      "loss": 0.8498,
      "step": 97400
    },
    {
      "epoch": 0.8896634790860647,
      "grad_norm": 3.9811458587646484,
      "learning_rate": 4.925861376742828e-05,
      "loss": 0.803,
      "step": 97500
    },
    {
      "epoch": 0.8905759544492299,
      "grad_norm": 4.382560729980469,
      "learning_rate": 4.925785337129231e-05,
      "loss": 0.8326,
      "step": 97600
    },
    {
      "epoch": 0.8914884298123951,
      "grad_norm": 3.5730721950531006,
      "learning_rate": 4.925709297515634e-05,
      "loss": 0.805,
      "step": 97700
    },
    {
      "epoch": 0.8924009051755603,
      "grad_norm": 4.5342888832092285,
      "learning_rate": 4.925633257902037e-05,
      "loss": 0.7856,
      "step": 97800
    },
    {
      "epoch": 0.8933133805387254,
      "grad_norm": 4.610008716583252,
      "learning_rate": 4.9255572182884395e-05,
      "loss": 0.8235,
      "step": 97900
    },
    {
      "epoch": 0.8942258559018906,
      "grad_norm": 4.12315034866333,
      "learning_rate": 4.925481178674843e-05,
      "loss": 0.8233,
      "step": 98000
    },
    {
      "epoch": 0.8951383312650558,
      "grad_norm": 4.273352146148682,
      "learning_rate": 4.9254051390612455e-05,
      "loss": 0.8195,
      "step": 98100
    },
    {
      "epoch": 0.896050806628221,
      "grad_norm": 5.011047840118408,
      "learning_rate": 4.9253290994476485e-05,
      "loss": 0.8052,
      "step": 98200
    },
    {
      "epoch": 0.8969632819913862,
      "grad_norm": 4.3210530281066895,
      "learning_rate": 4.9252530598340515e-05,
      "loss": 0.8121,
      "step": 98300
    },
    {
      "epoch": 0.8978757573545514,
      "grad_norm": 4.529335021972656,
      "learning_rate": 4.9251770202204545e-05,
      "loss": 0.8035,
      "step": 98400
    },
    {
      "epoch": 0.8987882327177166,
      "grad_norm": 4.157739162445068,
      "learning_rate": 4.925100980606857e-05,
      "loss": 0.7978,
      "step": 98500
    },
    {
      "epoch": 0.8997007080808818,
      "grad_norm": 4.613167762756348,
      "learning_rate": 4.9250249409932605e-05,
      "loss": 0.8509,
      "step": 98600
    },
    {
      "epoch": 0.9006131834440471,
      "grad_norm": 4.6933698654174805,
      "learning_rate": 4.924948901379663e-05,
      "loss": 0.8044,
      "step": 98700
    },
    {
      "epoch": 0.9015256588072122,
      "grad_norm": 5.267491817474365,
      "learning_rate": 4.924872861766066e-05,
      "loss": 0.8005,
      "step": 98800
    },
    {
      "epoch": 0.9024381341703774,
      "grad_norm": 4.191797733306885,
      "learning_rate": 4.924796822152469e-05,
      "loss": 0.7771,
      "step": 98900
    },
    {
      "epoch": 0.9033506095335426,
      "grad_norm": 5.274658679962158,
      "learning_rate": 4.924720782538872e-05,
      "loss": 0.785,
      "step": 99000
    },
    {
      "epoch": 0.9042630848967078,
      "grad_norm": 3.9826467037200928,
      "learning_rate": 4.924644742925275e-05,
      "loss": 0.8407,
      "step": 99100
    },
    {
      "epoch": 0.905175560259873,
      "grad_norm": 4.56985330581665,
      "learning_rate": 4.924568703311678e-05,
      "loss": 0.814,
      "step": 99200
    },
    {
      "epoch": 0.9060880356230382,
      "grad_norm": 3.3437278270721436,
      "learning_rate": 4.92449266369808e-05,
      "loss": 0.772,
      "step": 99300
    },
    {
      "epoch": 0.9070005109862034,
      "grad_norm": 4.452041149139404,
      "learning_rate": 4.924416624084484e-05,
      "loss": 0.8209,
      "step": 99400
    },
    {
      "epoch": 0.9079129863493686,
      "grad_norm": 4.208791732788086,
      "learning_rate": 4.924340584470886e-05,
      "loss": 0.8426,
      "step": 99500
    },
    {
      "epoch": 0.9088254617125338,
      "grad_norm": 3.6011016368865967,
      "learning_rate": 4.9242645448572885e-05,
      "loss": 0.7987,
      "step": 99600
    },
    {
      "epoch": 0.9097379370756989,
      "grad_norm": 5.137580871582031,
      "learning_rate": 4.924188505243692e-05,
      "loss": 0.8003,
      "step": 99700
    },
    {
      "epoch": 0.9106504124388641,
      "grad_norm": 4.471158027648926,
      "learning_rate": 4.9241124656300946e-05,
      "loss": 0.8023,
      "step": 99800
    },
    {
      "epoch": 0.9115628878020293,
      "grad_norm": 4.66707706451416,
      "learning_rate": 4.9240364260164976e-05,
      "loss": 0.8214,
      "step": 99900
    },
    {
      "epoch": 0.9124753631651945,
      "grad_norm": 4.662070274353027,
      "learning_rate": 4.9239603864029006e-05,
      "loss": 0.8289,
      "step": 100000
    },
    {
      "epoch": 0.9133878385283597,
      "grad_norm": 3.653085708618164,
      "learning_rate": 4.9238843467893036e-05,
      "loss": 0.797,
      "step": 100100
    },
    {
      "epoch": 0.9143003138915249,
      "grad_norm": 3.9109692573547363,
      "learning_rate": 4.9238083071757066e-05,
      "loss": 0.8478,
      "step": 100200
    },
    {
      "epoch": 0.9152127892546901,
      "grad_norm": 4.611108779907227,
      "learning_rate": 4.9237322675621096e-05,
      "loss": 0.7964,
      "step": 100300
    },
    {
      "epoch": 0.9161252646178554,
      "grad_norm": 4.857477188110352,
      "learning_rate": 4.923656227948512e-05,
      "loss": 0.8261,
      "step": 100400
    },
    {
      "epoch": 0.9170377399810206,
      "grad_norm": 4.041773319244385,
      "learning_rate": 4.9235801883349156e-05,
      "loss": 0.8455,
      "step": 100500
    },
    {
      "epoch": 0.9179502153441857,
      "grad_norm": 4.177624702453613,
      "learning_rate": 4.923504148721318e-05,
      "loss": 0.8537,
      "step": 100600
    },
    {
      "epoch": 0.9188626907073509,
      "grad_norm": 5.259537220001221,
      "learning_rate": 4.923428109107721e-05,
      "loss": 0.8289,
      "step": 100700
    },
    {
      "epoch": 0.9197751660705161,
      "grad_norm": 3.7977654933929443,
      "learning_rate": 4.923352069494124e-05,
      "loss": 0.8007,
      "step": 100800
    },
    {
      "epoch": 0.9206876414336813,
      "grad_norm": 3.6080336570739746,
      "learning_rate": 4.923276029880527e-05,
      "loss": 0.794,
      "step": 100900
    },
    {
      "epoch": 0.9216001167968465,
      "grad_norm": 5.056665420532227,
      "learning_rate": 4.923199990266929e-05,
      "loss": 0.7996,
      "step": 101000
    },
    {
      "epoch": 0.9225125921600117,
      "grad_norm": 4.252306938171387,
      "learning_rate": 4.923123950653333e-05,
      "loss": 0.8173,
      "step": 101100
    },
    {
      "epoch": 0.9234250675231769,
      "grad_norm": 4.442465305328369,
      "learning_rate": 4.923047911039735e-05,
      "loss": 0.8323,
      "step": 101200
    },
    {
      "epoch": 0.9243375428863421,
      "grad_norm": 5.036653518676758,
      "learning_rate": 4.922971871426138e-05,
      "loss": 0.8339,
      "step": 101300
    },
    {
      "epoch": 0.9252500182495073,
      "grad_norm": 4.573465824127197,
      "learning_rate": 4.922895831812541e-05,
      "loss": 0.8014,
      "step": 101400
    },
    {
      "epoch": 0.9261624936126724,
      "grad_norm": 5.018781661987305,
      "learning_rate": 4.922819792198944e-05,
      "loss": 0.7749,
      "step": 101500
    },
    {
      "epoch": 0.9270749689758376,
      "grad_norm": 4.6791486740112305,
      "learning_rate": 4.922743752585347e-05,
      "loss": 0.8324,
      "step": 101600
    },
    {
      "epoch": 0.9279874443390028,
      "grad_norm": 3.514674186706543,
      "learning_rate": 4.92266771297175e-05,
      "loss": 0.8155,
      "step": 101700
    },
    {
      "epoch": 0.928899919702168,
      "grad_norm": 4.541314125061035,
      "learning_rate": 4.922591673358153e-05,
      "loss": 0.7881,
      "step": 101800
    },
    {
      "epoch": 0.9298123950653332,
      "grad_norm": 4.924057483673096,
      "learning_rate": 4.9225156337445563e-05,
      "loss": 0.8358,
      "step": 101900
    },
    {
      "epoch": 0.9307248704284984,
      "grad_norm": 4.027756690979004,
      "learning_rate": 4.922439594130959e-05,
      "loss": 0.7946,
      "step": 102000
    },
    {
      "epoch": 0.9316373457916636,
      "grad_norm": 5.166923522949219,
      "learning_rate": 4.922363554517362e-05,
      "loss": 0.814,
      "step": 102100
    },
    {
      "epoch": 0.9325498211548289,
      "grad_norm": 4.484504222869873,
      "learning_rate": 4.922287514903765e-05,
      "loss": 0.8452,
      "step": 102200
    },
    {
      "epoch": 0.9334622965179941,
      "grad_norm": 4.255361557006836,
      "learning_rate": 4.922211475290167e-05,
      "loss": 0.8055,
      "step": 102300
    },
    {
      "epoch": 0.9343747718811592,
      "grad_norm": 4.416584491729736,
      "learning_rate": 4.92213543567657e-05,
      "loss": 0.7858,
      "step": 102400
    },
    {
      "epoch": 0.9352872472443244,
      "grad_norm": 3.4309744834899902,
      "learning_rate": 4.922059396062973e-05,
      "loss": 0.8057,
      "step": 102500
    },
    {
      "epoch": 0.9361997226074896,
      "grad_norm": 4.1648688316345215,
      "learning_rate": 4.921983356449376e-05,
      "loss": 0.8088,
      "step": 102600
    },
    {
      "epoch": 0.9371121979706548,
      "grad_norm": 3.176020383834839,
      "learning_rate": 4.921907316835779e-05,
      "loss": 0.8134,
      "step": 102700
    },
    {
      "epoch": 0.93802467333382,
      "grad_norm": 3.8298935890197754,
      "learning_rate": 4.921831277222182e-05,
      "loss": 0.7952,
      "step": 102800
    },
    {
      "epoch": 0.9389371486969852,
      "grad_norm": 3.313047170639038,
      "learning_rate": 4.9217552376085844e-05,
      "loss": 0.8428,
      "step": 102900
    },
    {
      "epoch": 0.9398496240601504,
      "grad_norm": 4.361220359802246,
      "learning_rate": 4.921679197994988e-05,
      "loss": 0.8201,
      "step": 103000
    },
    {
      "epoch": 0.9407620994233156,
      "grad_norm": 4.489006996154785,
      "learning_rate": 4.9216031583813904e-05,
      "loss": 0.8151,
      "step": 103100
    },
    {
      "epoch": 0.9416745747864808,
      "grad_norm": 4.487113952636719,
      "learning_rate": 4.9215271187677934e-05,
      "loss": 0.8135,
      "step": 103200
    },
    {
      "epoch": 0.9425870501496459,
      "grad_norm": 4.632332801818848,
      "learning_rate": 4.9214510791541964e-05,
      "loss": 0.8117,
      "step": 103300
    },
    {
      "epoch": 0.9434995255128111,
      "grad_norm": 4.682618618011475,
      "learning_rate": 4.9213750395405994e-05,
      "loss": 0.8343,
      "step": 103400
    },
    {
      "epoch": 0.9444120008759763,
      "grad_norm": 4.862354755401611,
      "learning_rate": 4.921298999927002e-05,
      "loss": 0.8249,
      "step": 103500
    },
    {
      "epoch": 0.9453244762391415,
      "grad_norm": 4.24705171585083,
      "learning_rate": 4.9212229603134054e-05,
      "loss": 0.7755,
      "step": 103600
    },
    {
      "epoch": 0.9462369516023067,
      "grad_norm": 4.147958755493164,
      "learning_rate": 4.921146920699808e-05,
      "loss": 0.7951,
      "step": 103700
    },
    {
      "epoch": 0.9471494269654719,
      "grad_norm": 3.7441046237945557,
      "learning_rate": 4.921070881086211e-05,
      "loss": 0.7903,
      "step": 103800
    },
    {
      "epoch": 0.9480619023286371,
      "grad_norm": 4.7192864418029785,
      "learning_rate": 4.920994841472614e-05,
      "loss": 0.7823,
      "step": 103900
    },
    {
      "epoch": 0.9489743776918024,
      "grad_norm": 4.778193473815918,
      "learning_rate": 4.920918801859017e-05,
      "loss": 0.8076,
      "step": 104000
    },
    {
      "epoch": 0.9498868530549676,
      "grad_norm": 4.165223121643066,
      "learning_rate": 4.92084276224542e-05,
      "loss": 0.8885,
      "step": 104100
    },
    {
      "epoch": 0.9507993284181327,
      "grad_norm": 4.219440460205078,
      "learning_rate": 4.920766722631823e-05,
      "loss": 0.8268,
      "step": 104200
    },
    {
      "epoch": 0.9517118037812979,
      "grad_norm": 4.693239688873291,
      "learning_rate": 4.920690683018225e-05,
      "loss": 0.7983,
      "step": 104300
    },
    {
      "epoch": 0.9526242791444631,
      "grad_norm": 4.090469837188721,
      "learning_rate": 4.920614643404629e-05,
      "loss": 0.845,
      "step": 104400
    },
    {
      "epoch": 0.9535367545076283,
      "grad_norm": 3.6865670680999756,
      "learning_rate": 4.920538603791031e-05,
      "loss": 0.8082,
      "step": 104500
    },
    {
      "epoch": 0.9544492298707935,
      "grad_norm": 4.364955425262451,
      "learning_rate": 4.920462564177434e-05,
      "loss": 0.8605,
      "step": 104600
    },
    {
      "epoch": 0.9553617052339587,
      "grad_norm": 4.37839937210083,
      "learning_rate": 4.920386524563837e-05,
      "loss": 0.8032,
      "step": 104700
    },
    {
      "epoch": 0.9562741805971239,
      "grad_norm": 3.8760910034179688,
      "learning_rate": 4.92031048495024e-05,
      "loss": 0.7858,
      "step": 104800
    },
    {
      "epoch": 0.9571866559602891,
      "grad_norm": 2.959866762161255,
      "learning_rate": 4.9202344453366425e-05,
      "loss": 0.8073,
      "step": 104900
    },
    {
      "epoch": 0.9580991313234543,
      "grad_norm": 3.8650312423706055,
      "learning_rate": 4.920158405723046e-05,
      "loss": 0.8171,
      "step": 105000
    },
    {
      "epoch": 0.9590116066866194,
      "grad_norm": 4.417269229888916,
      "learning_rate": 4.9200823661094485e-05,
      "loss": 0.8361,
      "step": 105100
    },
    {
      "epoch": 0.9599240820497846,
      "grad_norm": 3.1146323680877686,
      "learning_rate": 4.9200063264958515e-05,
      "loss": 0.7996,
      "step": 105200
    },
    {
      "epoch": 0.9608365574129498,
      "grad_norm": 4.722936153411865,
      "learning_rate": 4.9199302868822545e-05,
      "loss": 0.8309,
      "step": 105300
    },
    {
      "epoch": 0.961749032776115,
      "grad_norm": 4.323696136474609,
      "learning_rate": 4.919854247268657e-05,
      "loss": 0.8008,
      "step": 105400
    },
    {
      "epoch": 0.9626615081392802,
      "grad_norm": 4.128503799438477,
      "learning_rate": 4.9197782076550605e-05,
      "loss": 0.8008,
      "step": 105500
    },
    {
      "epoch": 0.9635739835024454,
      "grad_norm": 5.203769207000732,
      "learning_rate": 4.919702168041463e-05,
      "loss": 0.8212,
      "step": 105600
    },
    {
      "epoch": 0.9644864588656106,
      "grad_norm": 4.827839374542236,
      "learning_rate": 4.919626128427866e-05,
      "loss": 0.8108,
      "step": 105700
    },
    {
      "epoch": 0.9653989342287759,
      "grad_norm": 4.487204074859619,
      "learning_rate": 4.919550088814269e-05,
      "loss": 0.8317,
      "step": 105800
    },
    {
      "epoch": 0.9663114095919411,
      "grad_norm": 4.2202582359313965,
      "learning_rate": 4.919474049200672e-05,
      "loss": 0.7988,
      "step": 105900
    },
    {
      "epoch": 0.9672238849551063,
      "grad_norm": 4.078925609588623,
      "learning_rate": 4.919398009587074e-05,
      "loss": 0.7979,
      "step": 106000
    },
    {
      "epoch": 0.9681363603182714,
      "grad_norm": 3.008775472640991,
      "learning_rate": 4.919321969973478e-05,
      "loss": 0.7896,
      "step": 106100
    },
    {
      "epoch": 0.9690488356814366,
      "grad_norm": 3.8698740005493164,
      "learning_rate": 4.91924593035988e-05,
      "loss": 0.8233,
      "step": 106200
    },
    {
      "epoch": 0.9699613110446018,
      "grad_norm": 4.3360490798950195,
      "learning_rate": 4.919169890746283e-05,
      "loss": 0.7995,
      "step": 106300
    },
    {
      "epoch": 0.970873786407767,
      "grad_norm": 4.175100803375244,
      "learning_rate": 4.919093851132686e-05,
      "loss": 0.8366,
      "step": 106400
    },
    {
      "epoch": 0.9717862617709322,
      "grad_norm": 3.7064337730407715,
      "learning_rate": 4.919017811519089e-05,
      "loss": 0.8526,
      "step": 106500
    },
    {
      "epoch": 0.9726987371340974,
      "grad_norm": 5.039818286895752,
      "learning_rate": 4.918941771905492e-05,
      "loss": 0.8012,
      "step": 106600
    },
    {
      "epoch": 0.9736112124972626,
      "grad_norm": 3.93877911567688,
      "learning_rate": 4.918865732291895e-05,
      "loss": 0.8147,
      "step": 106700
    },
    {
      "epoch": 0.9745236878604278,
      "grad_norm": 4.428806781768799,
      "learning_rate": 4.9187896926782976e-05,
      "loss": 0.8043,
      "step": 106800
    },
    {
      "epoch": 0.975436163223593,
      "grad_norm": 5.771891117095947,
      "learning_rate": 4.918713653064701e-05,
      "loss": 0.8497,
      "step": 106900
    },
    {
      "epoch": 0.9763486385867581,
      "grad_norm": 3.564302682876587,
      "learning_rate": 4.9186376134511036e-05,
      "loss": 0.8131,
      "step": 107000
    },
    {
      "epoch": 0.9772611139499233,
      "grad_norm": 4.601640701293945,
      "learning_rate": 4.9185615738375066e-05,
      "loss": 0.8199,
      "step": 107100
    },
    {
      "epoch": 0.9781735893130885,
      "grad_norm": 4.803262710571289,
      "learning_rate": 4.9184855342239096e-05,
      "loss": 0.8187,
      "step": 107200
    },
    {
      "epoch": 0.9790860646762537,
      "grad_norm": 4.839353561401367,
      "learning_rate": 4.9184094946103126e-05,
      "loss": 0.7914,
      "step": 107300
    },
    {
      "epoch": 0.9799985400394189,
      "grad_norm": 4.337289810180664,
      "learning_rate": 4.918333454996715e-05,
      "loss": 0.8171,
      "step": 107400
    },
    {
      "epoch": 0.9809110154025842,
      "grad_norm": 4.176076412200928,
      "learning_rate": 4.9182574153831186e-05,
      "loss": 0.8023,
      "step": 107500
    },
    {
      "epoch": 0.9818234907657494,
      "grad_norm": 4.641838073730469,
      "learning_rate": 4.918181375769521e-05,
      "loss": 0.8375,
      "step": 107600
    },
    {
      "epoch": 0.9827359661289146,
      "grad_norm": 4.659456729888916,
      "learning_rate": 4.918105336155924e-05,
      "loss": 0.7842,
      "step": 107700
    },
    {
      "epoch": 0.9836484414920798,
      "grad_norm": 4.786757469177246,
      "learning_rate": 4.918029296542327e-05,
      "loss": 0.8324,
      "step": 107800
    },
    {
      "epoch": 0.9845609168552449,
      "grad_norm": 4.644505977630615,
      "learning_rate": 4.91795325692873e-05,
      "loss": 0.7867,
      "step": 107900
    },
    {
      "epoch": 0.9854733922184101,
      "grad_norm": 5.226531505584717,
      "learning_rate": 4.917877217315133e-05,
      "loss": 0.7756,
      "step": 108000
    },
    {
      "epoch": 0.9863858675815753,
      "grad_norm": 3.1079235076904297,
      "learning_rate": 4.917801177701535e-05,
      "loss": 0.8007,
      "step": 108100
    },
    {
      "epoch": 0.9872983429447405,
      "grad_norm": 2.6500422954559326,
      "learning_rate": 4.917725138087938e-05,
      "loss": 0.7924,
      "step": 108200
    },
    {
      "epoch": 0.9882108183079057,
      "grad_norm": 5.246085166931152,
      "learning_rate": 4.917649098474341e-05,
      "loss": 0.7787,
      "step": 108300
    },
    {
      "epoch": 0.9891232936710709,
      "grad_norm": 4.563582420349121,
      "learning_rate": 4.917573058860744e-05,
      "loss": 0.8008,
      "step": 108400
    },
    {
      "epoch": 0.9900357690342361,
      "grad_norm": 4.103268623352051,
      "learning_rate": 4.917497019247147e-05,
      "loss": 0.8256,
      "step": 108500
    },
    {
      "epoch": 0.9909482443974013,
      "grad_norm": 4.9415602684021,
      "learning_rate": 4.9174209796335503e-05,
      "loss": 0.8421,
      "step": 108600
    },
    {
      "epoch": 0.9918607197605664,
      "grad_norm": 4.5205206871032715,
      "learning_rate": 4.917344940019953e-05,
      "loss": 0.7772,
      "step": 108700
    },
    {
      "epoch": 0.9927731951237316,
      "grad_norm": 4.096977233886719,
      "learning_rate": 4.917268900406356e-05,
      "loss": 0.8122,
      "step": 108800
    },
    {
      "epoch": 0.9936856704868968,
      "grad_norm": 3.992753267288208,
      "learning_rate": 4.917192860792759e-05,
      "loss": 0.8184,
      "step": 108900
    },
    {
      "epoch": 0.994598145850062,
      "grad_norm": 3.8886682987213135,
      "learning_rate": 4.917116821179162e-05,
      "loss": 0.8177,
      "step": 109000
    },
    {
      "epoch": 0.9955106212132272,
      "grad_norm": 2.9450290203094482,
      "learning_rate": 4.917040781565565e-05,
      "loss": 0.7994,
      "step": 109100
    },
    {
      "epoch": 0.9964230965763924,
      "grad_norm": 3.8530452251434326,
      "learning_rate": 4.916964741951968e-05,
      "loss": 0.8257,
      "step": 109200
    },
    {
      "epoch": 0.9973355719395577,
      "grad_norm": 5.633406639099121,
      "learning_rate": 4.91688870233837e-05,
      "loss": 0.7926,
      "step": 109300
    },
    {
      "epoch": 0.9982480473027229,
      "grad_norm": 5.5081658363342285,
      "learning_rate": 4.916812662724774e-05,
      "loss": 0.7843,
      "step": 109400
    },
    {
      "epoch": 0.9991605226658881,
      "grad_norm": 5.067723751068115,
      "learning_rate": 4.916736623111176e-05,
      "loss": 0.8268,
      "step": 109500
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6533700227737427,
      "eval_runtime": 25.3234,
      "eval_samples_per_second": 227.813,
      "eval_steps_per_second": 227.813,
      "step": 109592
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6431974768638611,
      "eval_runtime": 481.7802,
      "eval_samples_per_second": 227.473,
      "eval_steps_per_second": 227.473,
      "step": 109592
    },
    {
      "epoch": 1.0000729980290533,
      "grad_norm": 3.3752288818359375,
      "learning_rate": 4.916660583497579e-05,
      "loss": 0.8276,
      "step": 109600
    },
    {
      "epoch": 1.0009854733922183,
      "grad_norm": 4.573904037475586,
      "learning_rate": 4.916584543883982e-05,
      "loss": 0.7644,
      "step": 109700
    },
    {
      "epoch": 1.0018979487553836,
      "grad_norm": 4.393709659576416,
      "learning_rate": 4.916508504270385e-05,
      "loss": 0.8026,
      "step": 109800
    },
    {
      "epoch": 1.0028104241185487,
      "grad_norm": 4.539994716644287,
      "learning_rate": 4.916432464656788e-05,
      "loss": 0.777,
      "step": 109900
    },
    {
      "epoch": 1.003722899481714,
      "grad_norm": 4.065593719482422,
      "learning_rate": 4.916356425043191e-05,
      "loss": 0.7965,
      "step": 110000
    },
    {
      "epoch": 1.004635374844879,
      "grad_norm": 3.902754068374634,
      "learning_rate": 4.9162803854295934e-05,
      "loss": 0.7879,
      "step": 110100
    },
    {
      "epoch": 1.0055478502080444,
      "grad_norm": 4.583499908447266,
      "learning_rate": 4.916204345815997e-05,
      "loss": 0.7661,
      "step": 110200
    },
    {
      "epoch": 1.0064603255712097,
      "grad_norm": 4.875250816345215,
      "learning_rate": 4.9161283062023994e-05,
      "loss": 0.7871,
      "step": 110300
    },
    {
      "epoch": 1.0073728009343748,
      "grad_norm": 4.905856132507324,
      "learning_rate": 4.9160522665888024e-05,
      "loss": 0.816,
      "step": 110400
    },
    {
      "epoch": 1.00828527629754,
      "grad_norm": 4.831696033477783,
      "learning_rate": 4.9159762269752054e-05,
      "loss": 0.7778,
      "step": 110500
    },
    {
      "epoch": 1.0091977516607051,
      "grad_norm": 4.861575126647949,
      "learning_rate": 4.9159001873616084e-05,
      "loss": 0.7943,
      "step": 110600
    },
    {
      "epoch": 1.0101102270238704,
      "grad_norm": 4.486010551452637,
      "learning_rate": 4.915824147748011e-05,
      "loss": 0.8155,
      "step": 110700
    },
    {
      "epoch": 1.0110227023870355,
      "grad_norm": 4.439225673675537,
      "learning_rate": 4.915748108134414e-05,
      "loss": 0.7651,
      "step": 110800
    },
    {
      "epoch": 1.0119351777502008,
      "grad_norm": 4.106764316558838,
      "learning_rate": 4.915672068520817e-05,
      "loss": 0.7872,
      "step": 110900
    },
    {
      "epoch": 1.0128476531133659,
      "grad_norm": 3.7464308738708496,
      "learning_rate": 4.91559602890722e-05,
      "loss": 0.7734,
      "step": 111000
    },
    {
      "epoch": 1.0137601284765312,
      "grad_norm": 4.506194591522217,
      "learning_rate": 4.915519989293623e-05,
      "loss": 0.8338,
      "step": 111100
    },
    {
      "epoch": 1.0146726038396963,
      "grad_norm": 4.723031520843506,
      "learning_rate": 4.915443949680025e-05,
      "loss": 0.8062,
      "step": 111200
    },
    {
      "epoch": 1.0155850792028616,
      "grad_norm": 5.498967170715332,
      "learning_rate": 4.915367910066429e-05,
      "loss": 0.7791,
      "step": 111300
    },
    {
      "epoch": 1.0164975545660266,
      "grad_norm": 4.513493061065674,
      "learning_rate": 4.915291870452831e-05,
      "loss": 0.8043,
      "step": 111400
    },
    {
      "epoch": 1.017410029929192,
      "grad_norm": 4.211932182312012,
      "learning_rate": 4.915215830839234e-05,
      "loss": 0.8187,
      "step": 111500
    },
    {
      "epoch": 1.018322505292357,
      "grad_norm": 4.933218955993652,
      "learning_rate": 4.915139791225637e-05,
      "loss": 0.8309,
      "step": 111600
    },
    {
      "epoch": 1.0192349806555223,
      "grad_norm": 4.607452392578125,
      "learning_rate": 4.91506375161204e-05,
      "loss": 0.8081,
      "step": 111700
    },
    {
      "epoch": 1.0201474560186874,
      "grad_norm": 5.287213325500488,
      "learning_rate": 4.9149877119984425e-05,
      "loss": 0.7873,
      "step": 111800
    },
    {
      "epoch": 1.0210599313818527,
      "grad_norm": 4.173538684844971,
      "learning_rate": 4.914911672384846e-05,
      "loss": 0.8101,
      "step": 111900
    },
    {
      "epoch": 1.021972406745018,
      "grad_norm": 4.346044540405273,
      "learning_rate": 4.9148356327712485e-05,
      "loss": 0.8172,
      "step": 112000
    },
    {
      "epoch": 1.022884882108183,
      "grad_norm": 3.765488862991333,
      "learning_rate": 4.9147595931576515e-05,
      "loss": 0.7984,
      "step": 112100
    },
    {
      "epoch": 1.0237973574713484,
      "grad_norm": 5.822718143463135,
      "learning_rate": 4.9146835535440545e-05,
      "loss": 0.8322,
      "step": 112200
    },
    {
      "epoch": 1.0247098328345134,
      "grad_norm": 5.25265645980835,
      "learning_rate": 4.9146075139304575e-05,
      "loss": 0.7854,
      "step": 112300
    },
    {
      "epoch": 1.0256223081976787,
      "grad_norm": 3.5450713634490967,
      "learning_rate": 4.9145314743168605e-05,
      "loss": 0.795,
      "step": 112400
    },
    {
      "epoch": 1.0265347835608438,
      "grad_norm": 4.148288249969482,
      "learning_rate": 4.9144554347032635e-05,
      "loss": 0.7477,
      "step": 112500
    },
    {
      "epoch": 1.0274472589240091,
      "grad_norm": 4.38461446762085,
      "learning_rate": 4.914379395089666e-05,
      "loss": 0.781,
      "step": 112600
    },
    {
      "epoch": 1.0283597342871742,
      "grad_norm": 4.803600311279297,
      "learning_rate": 4.9143033554760696e-05,
      "loss": 0.8539,
      "step": 112700
    },
    {
      "epoch": 1.0292722096503395,
      "grad_norm": 3.933455467224121,
      "learning_rate": 4.914227315862472e-05,
      "loss": 0.7881,
      "step": 112800
    },
    {
      "epoch": 1.0301846850135046,
      "grad_norm": 3.8818459510803223,
      "learning_rate": 4.914151276248875e-05,
      "loss": 0.7846,
      "step": 112900
    },
    {
      "epoch": 1.0310971603766699,
      "grad_norm": 4.212377071380615,
      "learning_rate": 4.914075236635278e-05,
      "loss": 0.7694,
      "step": 113000
    },
    {
      "epoch": 1.032009635739835,
      "grad_norm": 4.317943096160889,
      "learning_rate": 4.913999197021681e-05,
      "loss": 0.8406,
      "step": 113100
    },
    {
      "epoch": 1.0329221111030003,
      "grad_norm": 4.412878513336182,
      "learning_rate": 4.913923157408083e-05,
      "loss": 0.8409,
      "step": 113200
    },
    {
      "epoch": 1.0338345864661653,
      "grad_norm": 4.818607330322266,
      "learning_rate": 4.913847117794487e-05,
      "loss": 0.853,
      "step": 113300
    },
    {
      "epoch": 1.0347470618293306,
      "grad_norm": 4.954933166503906,
      "learning_rate": 4.913771078180889e-05,
      "loss": 0.8001,
      "step": 113400
    },
    {
      "epoch": 1.0356595371924957,
      "grad_norm": 5.195662975311279,
      "learning_rate": 4.913695038567292e-05,
      "loss": 0.7784,
      "step": 113500
    },
    {
      "epoch": 1.036572012555661,
      "grad_norm": 4.18207311630249,
      "learning_rate": 4.913618998953695e-05,
      "loss": 0.7849,
      "step": 113600
    },
    {
      "epoch": 1.0374844879188263,
      "grad_norm": 4.666465759277344,
      "learning_rate": 4.9135429593400976e-05,
      "loss": 0.8135,
      "step": 113700
    },
    {
      "epoch": 1.0383969632819914,
      "grad_norm": 4.098796367645264,
      "learning_rate": 4.913466919726501e-05,
      "loss": 0.8183,
      "step": 113800
    },
    {
      "epoch": 1.0393094386451567,
      "grad_norm": 4.186913967132568,
      "learning_rate": 4.9133908801129036e-05,
      "loss": 0.782,
      "step": 113900
    },
    {
      "epoch": 1.0402219140083218,
      "grad_norm": 4.588973045349121,
      "learning_rate": 4.9133148404993066e-05,
      "loss": 0.8425,
      "step": 114000
    },
    {
      "epoch": 1.041134389371487,
      "grad_norm": 4.933404445648193,
      "learning_rate": 4.9132388008857096e-05,
      "loss": 0.7973,
      "step": 114100
    },
    {
      "epoch": 1.0420468647346521,
      "grad_norm": 3.7498772144317627,
      "learning_rate": 4.9131627612721126e-05,
      "loss": 0.7761,
      "step": 114200
    },
    {
      "epoch": 1.0429593400978174,
      "grad_norm": 4.050127029418945,
      "learning_rate": 4.913086721658515e-05,
      "loss": 0.7258,
      "step": 114300
    },
    {
      "epoch": 1.0438718154609825,
      "grad_norm": 4.6534552574157715,
      "learning_rate": 4.9130106820449186e-05,
      "loss": 0.8266,
      "step": 114400
    },
    {
      "epoch": 1.0447842908241478,
      "grad_norm": 5.260849952697754,
      "learning_rate": 4.912934642431321e-05,
      "loss": 0.8221,
      "step": 114500
    },
    {
      "epoch": 1.0456967661873129,
      "grad_norm": 4.420212745666504,
      "learning_rate": 4.912858602817724e-05,
      "loss": 0.7479,
      "step": 114600
    },
    {
      "epoch": 1.0466092415504782,
      "grad_norm": 4.101333141326904,
      "learning_rate": 4.912782563204127e-05,
      "loss": 0.8113,
      "step": 114700
    },
    {
      "epoch": 1.0475217169136433,
      "grad_norm": 4.340179920196533,
      "learning_rate": 4.91270652359053e-05,
      "loss": 0.776,
      "step": 114800
    },
    {
      "epoch": 1.0484341922768086,
      "grad_norm": 4.2434587478637695,
      "learning_rate": 4.912630483976933e-05,
      "loss": 0.7886,
      "step": 114900
    },
    {
      "epoch": 1.0493466676399736,
      "grad_norm": 3.444891929626465,
      "learning_rate": 4.912554444363336e-05,
      "loss": 0.8048,
      "step": 115000
    },
    {
      "epoch": 1.050259143003139,
      "grad_norm": 4.244734287261963,
      "learning_rate": 4.912478404749738e-05,
      "loss": 0.7903,
      "step": 115100
    },
    {
      "epoch": 1.051171618366304,
      "grad_norm": 4.210761070251465,
      "learning_rate": 4.912402365136142e-05,
      "loss": 0.8495,
      "step": 115200
    },
    {
      "epoch": 1.0520840937294693,
      "grad_norm": 4.45902681350708,
      "learning_rate": 4.9123263255225443e-05,
      "loss": 0.8119,
      "step": 115300
    },
    {
      "epoch": 1.0529965690926344,
      "grad_norm": 5.275806427001953,
      "learning_rate": 4.9122502859089473e-05,
      "loss": 0.7545,
      "step": 115400
    },
    {
      "epoch": 1.0539090444557997,
      "grad_norm": 4.009638786315918,
      "learning_rate": 4.9121742462953504e-05,
      "loss": 0.783,
      "step": 115500
    },
    {
      "epoch": 1.054821519818965,
      "grad_norm": 3.3782436847686768,
      "learning_rate": 4.9120982066817534e-05,
      "loss": 0.7741,
      "step": 115600
    },
    {
      "epoch": 1.05573399518213,
      "grad_norm": 4.753432273864746,
      "learning_rate": 4.912022167068156e-05,
      "loss": 0.8076,
      "step": 115700
    },
    {
      "epoch": 1.0566464705452954,
      "grad_norm": 3.6141810417175293,
      "learning_rate": 4.9119461274545594e-05,
      "loss": 0.7369,
      "step": 115800
    },
    {
      "epoch": 1.0575589459084604,
      "grad_norm": 3.934904098510742,
      "learning_rate": 4.911870087840962e-05,
      "loss": 0.8079,
      "step": 115900
    },
    {
      "epoch": 1.0584714212716257,
      "grad_norm": 3.95021390914917,
      "learning_rate": 4.911794048227365e-05,
      "loss": 0.8343,
      "step": 116000
    },
    {
      "epoch": 1.0593838966347908,
      "grad_norm": 4.89616060256958,
      "learning_rate": 4.911718008613768e-05,
      "loss": 0.833,
      "step": 116100
    },
    {
      "epoch": 1.0602963719979561,
      "grad_norm": 4.486929893493652,
      "learning_rate": 4.911641969000171e-05,
      "loss": 0.77,
      "step": 116200
    },
    {
      "epoch": 1.0612088473611212,
      "grad_norm": 5.036891460418701,
      "learning_rate": 4.911565929386574e-05,
      "loss": 0.7815,
      "step": 116300
    },
    {
      "epoch": 1.0621213227242865,
      "grad_norm": 3.845184803009033,
      "learning_rate": 4.911489889772977e-05,
      "loss": 0.7853,
      "step": 116400
    },
    {
      "epoch": 1.0630337980874516,
      "grad_norm": 4.6966776847839355,
      "learning_rate": 4.911413850159379e-05,
      "loss": 0.7781,
      "step": 116500
    },
    {
      "epoch": 1.0639462734506169,
      "grad_norm": 3.831122875213623,
      "learning_rate": 4.911337810545782e-05,
      "loss": 0.824,
      "step": 116600
    },
    {
      "epoch": 1.064858748813782,
      "grad_norm": 3.8223884105682373,
      "learning_rate": 4.911261770932185e-05,
      "loss": 0.8162,
      "step": 116700
    },
    {
      "epoch": 1.0657712241769473,
      "grad_norm": 4.622544288635254,
      "learning_rate": 4.9111857313185874e-05,
      "loss": 0.8173,
      "step": 116800
    },
    {
      "epoch": 1.0666836995401123,
      "grad_norm": 4.838508129119873,
      "learning_rate": 4.911109691704991e-05,
      "loss": 0.7869,
      "step": 116900
    },
    {
      "epoch": 1.0675961749032776,
      "grad_norm": 4.892322063446045,
      "learning_rate": 4.9110336520913934e-05,
      "loss": 0.8474,
      "step": 117000
    },
    {
      "epoch": 1.0685086502664427,
      "grad_norm": 3.5391972064971924,
      "learning_rate": 4.9109576124777964e-05,
      "loss": 0.7759,
      "step": 117100
    },
    {
      "epoch": 1.069421125629608,
      "grad_norm": 5.427028656005859,
      "learning_rate": 4.9108815728641994e-05,
      "loss": 0.795,
      "step": 117200
    },
    {
      "epoch": 1.070333600992773,
      "grad_norm": 3.720475912094116,
      "learning_rate": 4.9108055332506024e-05,
      "loss": 0.8409,
      "step": 117300
    },
    {
      "epoch": 1.0712460763559384,
      "grad_norm": 3.646385431289673,
      "learning_rate": 4.9107294936370054e-05,
      "loss": 0.8067,
      "step": 117400
    },
    {
      "epoch": 1.0721585517191037,
      "grad_norm": 4.564530849456787,
      "learning_rate": 4.9106534540234085e-05,
      "loss": 0.8189,
      "step": 117500
    },
    {
      "epoch": 1.0730710270822688,
      "grad_norm": 5.594854831695557,
      "learning_rate": 4.910577414409811e-05,
      "loss": 0.7686,
      "step": 117600
    },
    {
      "epoch": 1.073983502445434,
      "grad_norm": 3.7473347187042236,
      "learning_rate": 4.9105013747962145e-05,
      "loss": 0.8388,
      "step": 117700
    },
    {
      "epoch": 1.0748959778085991,
      "grad_norm": 5.606793403625488,
      "learning_rate": 4.910425335182617e-05,
      "loss": 0.783,
      "step": 117800
    },
    {
      "epoch": 1.0758084531717644,
      "grad_norm": 3.3371598720550537,
      "learning_rate": 4.91034929556902e-05,
      "loss": 0.7874,
      "step": 117900
    },
    {
      "epoch": 1.0767209285349295,
      "grad_norm": 3.5993638038635254,
      "learning_rate": 4.910273255955423e-05,
      "loss": 0.8176,
      "step": 118000
    },
    {
      "epoch": 1.0776334038980948,
      "grad_norm": 4.08966064453125,
      "learning_rate": 4.910197216341826e-05,
      "loss": 0.7887,
      "step": 118100
    },
    {
      "epoch": 1.0785458792612599,
      "grad_norm": 4.008368968963623,
      "learning_rate": 4.910121176728228e-05,
      "loss": 0.7904,
      "step": 118200
    },
    {
      "epoch": 1.0794583546244252,
      "grad_norm": 3.863945245742798,
      "learning_rate": 4.910045137114632e-05,
      "loss": 0.7992,
      "step": 118300
    },
    {
      "epoch": 1.0803708299875903,
      "grad_norm": 2.972166061401367,
      "learning_rate": 4.909969097501034e-05,
      "loss": 0.7798,
      "step": 118400
    },
    {
      "epoch": 1.0812833053507556,
      "grad_norm": 3.6085376739501953,
      "learning_rate": 4.909893057887437e-05,
      "loss": 0.8146,
      "step": 118500
    },
    {
      "epoch": 1.0821957807139206,
      "grad_norm": 4.765132904052734,
      "learning_rate": 4.90981701827384e-05,
      "loss": 0.7761,
      "step": 118600
    },
    {
      "epoch": 1.083108256077086,
      "grad_norm": 4.8861083984375,
      "learning_rate": 4.909740978660243e-05,
      "loss": 0.8178,
      "step": 118700
    },
    {
      "epoch": 1.084020731440251,
      "grad_norm": 5.075683116912842,
      "learning_rate": 4.909664939046646e-05,
      "loss": 0.7982,
      "step": 118800
    },
    {
      "epoch": 1.0849332068034163,
      "grad_norm": 3.566804885864258,
      "learning_rate": 4.909588899433049e-05,
      "loss": 0.7784,
      "step": 118900
    },
    {
      "epoch": 1.0858456821665814,
      "grad_norm": 3.8119428157806396,
      "learning_rate": 4.9095128598194515e-05,
      "loss": 0.7459,
      "step": 119000
    },
    {
      "epoch": 1.0867581575297467,
      "grad_norm": 5.101635932922363,
      "learning_rate": 4.909436820205855e-05,
      "loss": 0.781,
      "step": 119100
    },
    {
      "epoch": 1.087670632892912,
      "grad_norm": 1.7849262952804565,
      "learning_rate": 4.9093607805922575e-05,
      "loss": 0.7996,
      "step": 119200
    },
    {
      "epoch": 1.088583108256077,
      "grad_norm": 4.357029914855957,
      "learning_rate": 4.90928474097866e-05,
      "loss": 0.8226,
      "step": 119300
    },
    {
      "epoch": 1.0894955836192424,
      "grad_norm": 4.654523849487305,
      "learning_rate": 4.9092087013650635e-05,
      "loss": 0.755,
      "step": 119400
    },
    {
      "epoch": 1.0904080589824074,
      "grad_norm": 4.093306064605713,
      "learning_rate": 4.909132661751466e-05,
      "loss": 0.8417,
      "step": 119500
    },
    {
      "epoch": 1.0913205343455727,
      "grad_norm": 4.145227909088135,
      "learning_rate": 4.909056622137869e-05,
      "loss": 0.7958,
      "step": 119600
    },
    {
      "epoch": 1.0922330097087378,
      "grad_norm": 5.277621269226074,
      "learning_rate": 4.908980582524272e-05,
      "loss": 0.8088,
      "step": 119700
    },
    {
      "epoch": 1.0931454850719031,
      "grad_norm": 3.7081093788146973,
      "learning_rate": 4.908904542910675e-05,
      "loss": 0.742,
      "step": 119800
    },
    {
      "epoch": 1.0940579604350682,
      "grad_norm": 4.9051513671875,
      "learning_rate": 4.908828503297078e-05,
      "loss": 0.7588,
      "step": 119900
    },
    {
      "epoch": 1.0949704357982335,
      "grad_norm": 4.496469974517822,
      "learning_rate": 4.908752463683481e-05,
      "loss": 0.7826,
      "step": 120000
    },
    {
      "epoch": 1.0958829111613986,
      "grad_norm": 3.879674196243286,
      "learning_rate": 4.908676424069883e-05,
      "loss": 0.7665,
      "step": 120100
    },
    {
      "epoch": 1.0967953865245639,
      "grad_norm": 4.845717906951904,
      "learning_rate": 4.908600384456287e-05,
      "loss": 0.7792,
      "step": 120200
    },
    {
      "epoch": 1.097707861887729,
      "grad_norm": 4.501538276672363,
      "learning_rate": 4.908524344842689e-05,
      "loss": 0.8112,
      "step": 120300
    },
    {
      "epoch": 1.0986203372508943,
      "grad_norm": 4.933967590332031,
      "learning_rate": 4.908448305229092e-05,
      "loss": 0.8177,
      "step": 120400
    },
    {
      "epoch": 1.0995328126140593,
      "grad_norm": 4.96112585067749,
      "learning_rate": 4.908372265615495e-05,
      "loss": 0.8078,
      "step": 120500
    },
    {
      "epoch": 1.1004452879772246,
      "grad_norm": 3.906010627746582,
      "learning_rate": 4.908296226001898e-05,
      "loss": 0.7916,
      "step": 120600
    },
    {
      "epoch": 1.1013577633403897,
      "grad_norm": 3.952792167663574,
      "learning_rate": 4.908220186388301e-05,
      "loss": 0.7708,
      "step": 120700
    },
    {
      "epoch": 1.102270238703555,
      "grad_norm": 4.399158954620361,
      "learning_rate": 4.908144146774704e-05,
      "loss": 0.7374,
      "step": 120800
    },
    {
      "epoch": 1.1031827140667203,
      "grad_norm": 3.7380807399749756,
      "learning_rate": 4.9080681071611066e-05,
      "loss": 0.7766,
      "step": 120900
    },
    {
      "epoch": 1.1040951894298854,
      "grad_norm": 4.34990119934082,
      "learning_rate": 4.9079920675475096e-05,
      "loss": 0.8483,
      "step": 121000
    },
    {
      "epoch": 1.1050076647930507,
      "grad_norm": 4.517168045043945,
      "learning_rate": 4.9079160279339126e-05,
      "loss": 0.7963,
      "step": 121100
    },
    {
      "epoch": 1.1059201401562158,
      "grad_norm": 4.760839939117432,
      "learning_rate": 4.9078399883203156e-05,
      "loss": 0.788,
      "step": 121200
    },
    {
      "epoch": 1.106832615519381,
      "grad_norm": 3.4984021186828613,
      "learning_rate": 4.9077639487067186e-05,
      "loss": 0.7695,
      "step": 121300
    },
    {
      "epoch": 1.1077450908825461,
      "grad_norm": 4.350109577178955,
      "learning_rate": 4.9076879090931217e-05,
      "loss": 0.8257,
      "step": 121400
    },
    {
      "epoch": 1.1086575662457114,
      "grad_norm": 4.788187026977539,
      "learning_rate": 4.907611869479524e-05,
      "loss": 0.7527,
      "step": 121500
    },
    {
      "epoch": 1.1095700416088765,
      "grad_norm": 3.674368143081665,
      "learning_rate": 4.907535829865928e-05,
      "loss": 0.8368,
      "step": 121600
    },
    {
      "epoch": 1.1104825169720418,
      "grad_norm": 3.4096717834472656,
      "learning_rate": 4.90745979025233e-05,
      "loss": 0.7947,
      "step": 121700
    },
    {
      "epoch": 1.1113949923352069,
      "grad_norm": 4.622817039489746,
      "learning_rate": 4.907383750638733e-05,
      "loss": 0.7838,
      "step": 121800
    },
    {
      "epoch": 1.1123074676983722,
      "grad_norm": 3.5514094829559326,
      "learning_rate": 4.907307711025136e-05,
      "loss": 0.8057,
      "step": 121900
    },
    {
      "epoch": 1.1132199430615373,
      "grad_norm": 4.791032791137695,
      "learning_rate": 4.907231671411539e-05,
      "loss": 0.782,
      "step": 122000
    },
    {
      "epoch": 1.1141324184247026,
      "grad_norm": 5.052743434906006,
      "learning_rate": 4.907155631797942e-05,
      "loss": 0.832,
      "step": 122100
    },
    {
      "epoch": 1.1150448937878676,
      "grad_norm": 4.7340497970581055,
      "learning_rate": 4.9070795921843443e-05,
      "loss": 0.8038,
      "step": 122200
    },
    {
      "epoch": 1.115957369151033,
      "grad_norm": 3.7251336574554443,
      "learning_rate": 4.9070035525707474e-05,
      "loss": 0.8261,
      "step": 122300
    },
    {
      "epoch": 1.116869844514198,
      "grad_norm": 4.387024402618408,
      "learning_rate": 4.9069275129571504e-05,
      "loss": 0.7972,
      "step": 122400
    },
    {
      "epoch": 1.1177823198773633,
      "grad_norm": 4.51922082901001,
      "learning_rate": 4.9068514733435534e-05,
      "loss": 0.7941,
      "step": 122500
    },
    {
      "epoch": 1.1186947952405286,
      "grad_norm": 3.9721462726593018,
      "learning_rate": 4.906775433729956e-05,
      "loss": 0.7981,
      "step": 122600
    },
    {
      "epoch": 1.1196072706036937,
      "grad_norm": 4.522608280181885,
      "learning_rate": 4.9066993941163594e-05,
      "loss": 0.8127,
      "step": 122700
    },
    {
      "epoch": 1.120519745966859,
      "grad_norm": 4.6119184494018555,
      "learning_rate": 4.906623354502762e-05,
      "loss": 0.7609,
      "step": 122800
    },
    {
      "epoch": 1.121432221330024,
      "grad_norm": 4.629507541656494,
      "learning_rate": 4.906547314889165e-05,
      "loss": 0.7557,
      "step": 122900
    },
    {
      "epoch": 1.1223446966931894,
      "grad_norm": 4.536468982696533,
      "learning_rate": 4.906471275275568e-05,
      "loss": 0.8298,
      "step": 123000
    },
    {
      "epoch": 1.1232571720563544,
      "grad_norm": 3.8183059692382812,
      "learning_rate": 4.906395235661971e-05,
      "loss": 0.7888,
      "step": 123100
    },
    {
      "epoch": 1.1241696474195197,
      "grad_norm": 3.8321969509124756,
      "learning_rate": 4.906319196048374e-05,
      "loss": 0.7857,
      "step": 123200
    },
    {
      "epoch": 1.1250821227826848,
      "grad_norm": 3.684757947921753,
      "learning_rate": 4.906243156434777e-05,
      "loss": 0.7788,
      "step": 123300
    },
    {
      "epoch": 1.1259945981458501,
      "grad_norm": 5.222234725952148,
      "learning_rate": 4.906167116821179e-05,
      "loss": 0.83,
      "step": 123400
    },
    {
      "epoch": 1.1269070735090152,
      "grad_norm": 3.998148202896118,
      "learning_rate": 4.906091077207583e-05,
      "loss": 0.8013,
      "step": 123500
    },
    {
      "epoch": 1.1278195488721805,
      "grad_norm": 3.6052727699279785,
      "learning_rate": 4.906015037593985e-05,
      "loss": 0.7867,
      "step": 123600
    },
    {
      "epoch": 1.1287320242353456,
      "grad_norm": 4.133542060852051,
      "learning_rate": 4.905938997980388e-05,
      "loss": 0.82,
      "step": 123700
    },
    {
      "epoch": 1.1296444995985109,
      "grad_norm": 4.177315711975098,
      "learning_rate": 4.905862958366791e-05,
      "loss": 0.7899,
      "step": 123800
    },
    {
      "epoch": 1.130556974961676,
      "grad_norm": 5.0245256423950195,
      "learning_rate": 4.905786918753194e-05,
      "loss": 0.8046,
      "step": 123900
    },
    {
      "epoch": 1.1314694503248413,
      "grad_norm": 4.277549743652344,
      "learning_rate": 4.9057108791395964e-05,
      "loss": 0.7721,
      "step": 124000
    },
    {
      "epoch": 1.1323819256880063,
      "grad_norm": 4.718342304229736,
      "learning_rate": 4.905634839526e-05,
      "loss": 0.7673,
      "step": 124100
    },
    {
      "epoch": 1.1332944010511716,
      "grad_norm": 4.183311939239502,
      "learning_rate": 4.9055587999124025e-05,
      "loss": 0.7746,
      "step": 124200
    },
    {
      "epoch": 1.134206876414337,
      "grad_norm": 4.287418842315674,
      "learning_rate": 4.9054827602988055e-05,
      "loss": 0.7748,
      "step": 124300
    },
    {
      "epoch": 1.135119351777502,
      "grad_norm": 4.8193159103393555,
      "learning_rate": 4.9054067206852085e-05,
      "loss": 0.7905,
      "step": 124400
    },
    {
      "epoch": 1.136031827140667,
      "grad_norm": 4.032307147979736,
      "learning_rate": 4.9053306810716115e-05,
      "loss": 0.7968,
      "step": 124500
    },
    {
      "epoch": 1.1369443025038324,
      "grad_norm": 3.656615734100342,
      "learning_rate": 4.9052546414580145e-05,
      "loss": 0.8195,
      "step": 124600
    },
    {
      "epoch": 1.1378567778669977,
      "grad_norm": 3.861787796020508,
      "learning_rate": 4.9051786018444175e-05,
      "loss": 0.7837,
      "step": 124700
    },
    {
      "epoch": 1.1387692532301628,
      "grad_norm": 4.429110050201416,
      "learning_rate": 4.90510256223082e-05,
      "loss": 0.8017,
      "step": 124800
    },
    {
      "epoch": 1.139681728593328,
      "grad_norm": 3.1833155155181885,
      "learning_rate": 4.9050265226172235e-05,
      "loss": 0.7585,
      "step": 124900
    },
    {
      "epoch": 1.1405942039564931,
      "grad_norm": 4.430704593658447,
      "learning_rate": 4.904950483003626e-05,
      "loss": 0.7742,
      "step": 125000
    },
    {
      "epoch": 1.1415066793196584,
      "grad_norm": 3.726113796234131,
      "learning_rate": 4.904874443390028e-05,
      "loss": 0.815,
      "step": 125100
    },
    {
      "epoch": 1.1424191546828235,
      "grad_norm": 4.6316118240356445,
      "learning_rate": 4.904798403776432e-05,
      "loss": 0.7694,
      "step": 125200
    },
    {
      "epoch": 1.1433316300459888,
      "grad_norm": 5.689116477966309,
      "learning_rate": 4.904722364162834e-05,
      "loss": 0.7506,
      "step": 125300
    },
    {
      "epoch": 1.1442441054091539,
      "grad_norm": 3.9890387058258057,
      "learning_rate": 4.904646324549237e-05,
      "loss": 0.8017,
      "step": 125400
    },
    {
      "epoch": 1.1451565807723192,
      "grad_norm": 3.862666606903076,
      "learning_rate": 4.90457028493564e-05,
      "loss": 0.7899,
      "step": 125500
    },
    {
      "epoch": 1.1460690561354843,
      "grad_norm": 3.9947988986968994,
      "learning_rate": 4.904494245322043e-05,
      "loss": 0.8398,
      "step": 125600
    },
    {
      "epoch": 1.1469815314986496,
      "grad_norm": 4.296103000640869,
      "learning_rate": 4.904418205708446e-05,
      "loss": 0.7887,
      "step": 125700
    },
    {
      "epoch": 1.1478940068618146,
      "grad_norm": 4.229961395263672,
      "learning_rate": 4.904342166094849e-05,
      "loss": 0.822,
      "step": 125800
    },
    {
      "epoch": 1.14880648222498,
      "grad_norm": 4.91038703918457,
      "learning_rate": 4.9042661264812515e-05,
      "loss": 0.7971,
      "step": 125900
    },
    {
      "epoch": 1.1497189575881452,
      "grad_norm": 4.837188720703125,
      "learning_rate": 4.904190086867655e-05,
      "loss": 0.7984,
      "step": 126000
    },
    {
      "epoch": 1.1506314329513103,
      "grad_norm": 5.502158164978027,
      "learning_rate": 4.9041140472540575e-05,
      "loss": 0.8093,
      "step": 126100
    },
    {
      "epoch": 1.1515439083144754,
      "grad_norm": 4.5554327964782715,
      "learning_rate": 4.9040380076404606e-05,
      "loss": 0.7945,
      "step": 126200
    },
    {
      "epoch": 1.1524563836776407,
      "grad_norm": 4.025181770324707,
      "learning_rate": 4.9039619680268636e-05,
      "loss": 0.8133,
      "step": 126300
    },
    {
      "epoch": 1.153368859040806,
      "grad_norm": 4.376886367797852,
      "learning_rate": 4.9038859284132666e-05,
      "loss": 0.7632,
      "step": 126400
    },
    {
      "epoch": 1.154281334403971,
      "grad_norm": 5.151947498321533,
      "learning_rate": 4.903809888799669e-05,
      "loss": 0.8333,
      "step": 126500
    },
    {
      "epoch": 1.1551938097671364,
      "grad_norm": 4.149950981140137,
      "learning_rate": 4.9037338491860726e-05,
      "loss": 0.7803,
      "step": 126600
    },
    {
      "epoch": 1.1561062851303014,
      "grad_norm": 5.079124927520752,
      "learning_rate": 4.903657809572475e-05,
      "loss": 0.8257,
      "step": 126700
    },
    {
      "epoch": 1.1570187604934667,
      "grad_norm": 4.141164302825928,
      "learning_rate": 4.903581769958878e-05,
      "loss": 0.7815,
      "step": 126800
    },
    {
      "epoch": 1.1579312358566318,
      "grad_norm": 4.183653354644775,
      "learning_rate": 4.903505730345281e-05,
      "loss": 0.7998,
      "step": 126900
    },
    {
      "epoch": 1.1588437112197971,
      "grad_norm": 4.959744453430176,
      "learning_rate": 4.903429690731684e-05,
      "loss": 0.7933,
      "step": 127000
    },
    {
      "epoch": 1.1597561865829622,
      "grad_norm": 3.9417033195495605,
      "learning_rate": 4.903353651118087e-05,
      "loss": 0.7701,
      "step": 127100
    },
    {
      "epoch": 1.1606686619461275,
      "grad_norm": 4.796133995056152,
      "learning_rate": 4.90327761150449e-05,
      "loss": 0.7415,
      "step": 127200
    },
    {
      "epoch": 1.1615811373092926,
      "grad_norm": 4.6867194175720215,
      "learning_rate": 4.903201571890892e-05,
      "loss": 0.7858,
      "step": 127300
    },
    {
      "epoch": 1.1624936126724579,
      "grad_norm": 3.466444730758667,
      "learning_rate": 4.903125532277296e-05,
      "loss": 0.7507,
      "step": 127400
    },
    {
      "epoch": 1.163406088035623,
      "grad_norm": 3.8486368656158447,
      "learning_rate": 4.903049492663698e-05,
      "loss": 0.7735,
      "step": 127500
    },
    {
      "epoch": 1.1643185633987883,
      "grad_norm": 4.202253341674805,
      "learning_rate": 4.902973453050101e-05,
      "loss": 0.7852,
      "step": 127600
    },
    {
      "epoch": 1.1652310387619533,
      "grad_norm": 3.271348714828491,
      "learning_rate": 4.902897413436504e-05,
      "loss": 0.7857,
      "step": 127700
    },
    {
      "epoch": 1.1661435141251186,
      "grad_norm": 4.328294277191162,
      "learning_rate": 4.902821373822907e-05,
      "loss": 0.8,
      "step": 127800
    },
    {
      "epoch": 1.1670559894882837,
      "grad_norm": 3.6418874263763428,
      "learning_rate": 4.9027453342093096e-05,
      "loss": 0.8244,
      "step": 127900
    },
    {
      "epoch": 1.167968464851449,
      "grad_norm": 4.612922191619873,
      "learning_rate": 4.9026692945957126e-05,
      "loss": 0.8325,
      "step": 128000
    },
    {
      "epoch": 1.1688809402146143,
      "grad_norm": 3.9979751110076904,
      "learning_rate": 4.9025932549821156e-05,
      "loss": 0.822,
      "step": 128100
    },
    {
      "epoch": 1.1697934155777794,
      "grad_norm": 4.652318000793457,
      "learning_rate": 4.9025172153685187e-05,
      "loss": 0.7757,
      "step": 128200
    },
    {
      "epoch": 1.1707058909409447,
      "grad_norm": 4.619757175445557,
      "learning_rate": 4.9024411757549217e-05,
      "loss": 0.865,
      "step": 128300
    },
    {
      "epoch": 1.1716183663041098,
      "grad_norm": 4.321486473083496,
      "learning_rate": 4.902365136141324e-05,
      "loss": 0.7939,
      "step": 128400
    },
    {
      "epoch": 1.172530841667275,
      "grad_norm": 5.548765182495117,
      "learning_rate": 4.902289096527728e-05,
      "loss": 0.8181,
      "step": 128500
    },
    {
      "epoch": 1.1734433170304401,
      "grad_norm": 5.2199506759643555,
      "learning_rate": 4.90221305691413e-05,
      "loss": 0.7891,
      "step": 128600
    },
    {
      "epoch": 1.1743557923936054,
      "grad_norm": 3.838097333908081,
      "learning_rate": 4.902137017300533e-05,
      "loss": 0.8235,
      "step": 128700
    },
    {
      "epoch": 1.1752682677567705,
      "grad_norm": 4.56069278717041,
      "learning_rate": 4.902060977686936e-05,
      "loss": 0.768,
      "step": 128800
    },
    {
      "epoch": 1.1761807431199358,
      "grad_norm": 4.450868129730225,
      "learning_rate": 4.901984938073339e-05,
      "loss": 0.7829,
      "step": 128900
    },
    {
      "epoch": 1.177093218483101,
      "grad_norm": 3.9899325370788574,
      "learning_rate": 4.9019088984597414e-05,
      "loss": 0.8058,
      "step": 129000
    },
    {
      "epoch": 1.1780056938462662,
      "grad_norm": 4.1102471351623535,
      "learning_rate": 4.901832858846145e-05,
      "loss": 0.8227,
      "step": 129100
    },
    {
      "epoch": 1.1789181692094313,
      "grad_norm": 4.526034832000732,
      "learning_rate": 4.9017568192325474e-05,
      "loss": 0.792,
      "step": 129200
    },
    {
      "epoch": 1.1798306445725966,
      "grad_norm": 4.357357025146484,
      "learning_rate": 4.9016807796189504e-05,
      "loss": 0.7817,
      "step": 129300
    },
    {
      "epoch": 1.1807431199357616,
      "grad_norm": 3.244356393814087,
      "learning_rate": 4.9016047400053534e-05,
      "loss": 0.781,
      "step": 129400
    },
    {
      "epoch": 1.181655595298927,
      "grad_norm": 3.071680784225464,
      "learning_rate": 4.9015287003917564e-05,
      "loss": 0.7742,
      "step": 129500
    },
    {
      "epoch": 1.182568070662092,
      "grad_norm": 4.1737751960754395,
      "learning_rate": 4.9014526607781594e-05,
      "loss": 0.7785,
      "step": 129600
    },
    {
      "epoch": 1.1834805460252573,
      "grad_norm": 4.111976623535156,
      "learning_rate": 4.9013766211645624e-05,
      "loss": 0.8227,
      "step": 129700
    },
    {
      "epoch": 1.1843930213884226,
      "grad_norm": 3.708075761795044,
      "learning_rate": 4.901300581550965e-05,
      "loss": 0.8038,
      "step": 129800
    },
    {
      "epoch": 1.1853054967515877,
      "grad_norm": 4.765157222747803,
      "learning_rate": 4.9012245419373684e-05,
      "loss": 0.7905,
      "step": 129900
    },
    {
      "epoch": 1.186217972114753,
      "grad_norm": 4.698421001434326,
      "learning_rate": 4.901148502323771e-05,
      "loss": 0.7969,
      "step": 130000
    },
    {
      "epoch": 1.187130447477918,
      "grad_norm": 4.171337127685547,
      "learning_rate": 4.901072462710174e-05,
      "loss": 0.8185,
      "step": 130100
    },
    {
      "epoch": 1.1880429228410834,
      "grad_norm": 5.004813194274902,
      "learning_rate": 4.900996423096577e-05,
      "loss": 0.8359,
      "step": 130200
    },
    {
      "epoch": 1.1889553982042484,
      "grad_norm": 4.7881879806518555,
      "learning_rate": 4.90092038348298e-05,
      "loss": 0.8135,
      "step": 130300
    },
    {
      "epoch": 1.1898678735674137,
      "grad_norm": 4.796361923217773,
      "learning_rate": 4.900844343869382e-05,
      "loss": 0.7601,
      "step": 130400
    },
    {
      "epoch": 1.1907803489305788,
      "grad_norm": 3.900773525238037,
      "learning_rate": 4.900768304255786e-05,
      "loss": 0.7911,
      "step": 130500
    },
    {
      "epoch": 1.1916928242937441,
      "grad_norm": 3.8507940769195557,
      "learning_rate": 4.900692264642188e-05,
      "loss": 0.8015,
      "step": 130600
    },
    {
      "epoch": 1.1926052996569092,
      "grad_norm": 4.4717817306518555,
      "learning_rate": 4.900616225028591e-05,
      "loss": 0.7782,
      "step": 130700
    },
    {
      "epoch": 1.1935177750200745,
      "grad_norm": 4.236399173736572,
      "learning_rate": 4.900540185414994e-05,
      "loss": 0.806,
      "step": 130800
    },
    {
      "epoch": 1.1944302503832396,
      "grad_norm": 3.8699498176574707,
      "learning_rate": 4.9004641458013964e-05,
      "loss": 0.7875,
      "step": 130900
    },
    {
      "epoch": 1.1953427257464049,
      "grad_norm": 3.9290072917938232,
      "learning_rate": 4.9003881061878e-05,
      "loss": 0.7943,
      "step": 131000
    },
    {
      "epoch": 1.19625520110957,
      "grad_norm": 4.282906532287598,
      "learning_rate": 4.9003120665742025e-05,
      "loss": 0.7618,
      "step": 131100
    },
    {
      "epoch": 1.1971676764727353,
      "grad_norm": 4.1331562995910645,
      "learning_rate": 4.9002360269606055e-05,
      "loss": 0.8065,
      "step": 131200
    },
    {
      "epoch": 1.1980801518359003,
      "grad_norm": 5.333472728729248,
      "learning_rate": 4.9001599873470085e-05,
      "loss": 0.7728,
      "step": 131300
    },
    {
      "epoch": 1.1989926271990656,
      "grad_norm": 3.9395740032196045,
      "learning_rate": 4.9000839477334115e-05,
      "loss": 0.775,
      "step": 131400
    },
    {
      "epoch": 1.199905102562231,
      "grad_norm": 4.183406829833984,
      "learning_rate": 4.900007908119814e-05,
      "loss": 0.7316,
      "step": 131500
    },
    {
      "epoch": 1.200817577925396,
      "grad_norm": 3.611016035079956,
      "learning_rate": 4.8999318685062175e-05,
      "loss": 0.8544,
      "step": 131600
    },
    {
      "epoch": 1.201730053288561,
      "grad_norm": 5.228697299957275,
      "learning_rate": 4.89985582889262e-05,
      "loss": 0.792,
      "step": 131700
    },
    {
      "epoch": 1.2026425286517264,
      "grad_norm": 5.085751056671143,
      "learning_rate": 4.899779789279023e-05,
      "loss": 0.818,
      "step": 131800
    },
    {
      "epoch": 1.2035550040148917,
      "grad_norm": 3.989851474761963,
      "learning_rate": 4.899703749665426e-05,
      "loss": 0.7714,
      "step": 131900
    },
    {
      "epoch": 1.2044674793780568,
      "grad_norm": 3.862800121307373,
      "learning_rate": 4.899627710051829e-05,
      "loss": 0.8045,
      "step": 132000
    },
    {
      "epoch": 1.205379954741222,
      "grad_norm": 4.69980525970459,
      "learning_rate": 4.899551670438232e-05,
      "loss": 0.8121,
      "step": 132100
    },
    {
      "epoch": 1.2062924301043871,
      "grad_norm": 4.4799675941467285,
      "learning_rate": 4.899475630824635e-05,
      "loss": 0.8279,
      "step": 132200
    },
    {
      "epoch": 1.2072049054675524,
      "grad_norm": 4.051191329956055,
      "learning_rate": 4.899399591211037e-05,
      "loss": 0.7779,
      "step": 132300
    },
    {
      "epoch": 1.2081173808307175,
      "grad_norm": 3.700695514678955,
      "learning_rate": 4.899323551597441e-05,
      "loss": 0.7999,
      "step": 132400
    },
    {
      "epoch": 1.2090298561938828,
      "grad_norm": 3.4484968185424805,
      "learning_rate": 4.899247511983843e-05,
      "loss": 0.7904,
      "step": 132500
    },
    {
      "epoch": 1.209942331557048,
      "grad_norm": 4.720682621002197,
      "learning_rate": 4.899171472370246e-05,
      "loss": 0.8353,
      "step": 132600
    },
    {
      "epoch": 1.2108548069202132,
      "grad_norm": 4.557180404663086,
      "learning_rate": 4.899095432756649e-05,
      "loss": 0.7882,
      "step": 132700
    },
    {
      "epoch": 1.2117672822833783,
      "grad_norm": 4.622791290283203,
      "learning_rate": 4.899019393143052e-05,
      "loss": 0.7872,
      "step": 132800
    },
    {
      "epoch": 1.2126797576465436,
      "grad_norm": 5.250521183013916,
      "learning_rate": 4.8989433535294545e-05,
      "loss": 0.7604,
      "step": 132900
    },
    {
      "epoch": 1.2135922330097086,
      "grad_norm": 3.3103299140930176,
      "learning_rate": 4.898867313915858e-05,
      "loss": 0.782,
      "step": 133000
    },
    {
      "epoch": 1.214504708372874,
      "grad_norm": 4.298035621643066,
      "learning_rate": 4.8987912743022606e-05,
      "loss": 0.8169,
      "step": 133100
    },
    {
      "epoch": 1.2154171837360392,
      "grad_norm": 3.9822540283203125,
      "learning_rate": 4.8987152346886636e-05,
      "loss": 0.7851,
      "step": 133200
    },
    {
      "epoch": 1.2163296590992043,
      "grad_norm": 4.0763840675354,
      "learning_rate": 4.8986391950750666e-05,
      "loss": 0.7768,
      "step": 133300
    },
    {
      "epoch": 1.2172421344623694,
      "grad_norm": 4.148220062255859,
      "learning_rate": 4.8985631554614696e-05,
      "loss": 0.793,
      "step": 133400
    },
    {
      "epoch": 1.2181546098255347,
      "grad_norm": 2.732801914215088,
      "learning_rate": 4.8984871158478726e-05,
      "loss": 0.7997,
      "step": 133500
    },
    {
      "epoch": 1.2190670851887,
      "grad_norm": 4.0984673500061035,
      "learning_rate": 4.898411076234275e-05,
      "loss": 0.8432,
      "step": 133600
    },
    {
      "epoch": 1.219979560551865,
      "grad_norm": 3.4141268730163574,
      "learning_rate": 4.898335036620678e-05,
      "loss": 0.7957,
      "step": 133700
    },
    {
      "epoch": 1.2208920359150304,
      "grad_norm": 3.153594732284546,
      "learning_rate": 4.898258997007081e-05,
      "loss": 0.7743,
      "step": 133800
    },
    {
      "epoch": 1.2218045112781954,
      "grad_norm": 4.973379135131836,
      "learning_rate": 4.898182957393484e-05,
      "loss": 0.807,
      "step": 133900
    },
    {
      "epoch": 1.2227169866413607,
      "grad_norm": 4.554415702819824,
      "learning_rate": 4.898106917779887e-05,
      "loss": 0.7613,
      "step": 134000
    },
    {
      "epoch": 1.2236294620045258,
      "grad_norm": 3.0750722885131836,
      "learning_rate": 4.89803087816629e-05,
      "loss": 0.8074,
      "step": 134100
    },
    {
      "epoch": 1.2245419373676911,
      "grad_norm": 4.248276710510254,
      "learning_rate": 4.897954838552692e-05,
      "loss": 0.7903,
      "step": 134200
    },
    {
      "epoch": 1.2254544127308562,
      "grad_norm": 4.416791915893555,
      "learning_rate": 4.897878798939095e-05,
      "loss": 0.7787,
      "step": 134300
    },
    {
      "epoch": 1.2263668880940215,
      "grad_norm": 4.238081932067871,
      "learning_rate": 4.897802759325498e-05,
      "loss": 0.768,
      "step": 134400
    },
    {
      "epoch": 1.2272793634571866,
      "grad_norm": 5.495770454406738,
      "learning_rate": 4.897726719711901e-05,
      "loss": 0.7946,
      "step": 134500
    },
    {
      "epoch": 1.2281918388203519,
      "grad_norm": 4.4890947341918945,
      "learning_rate": 4.897650680098304e-05,
      "loss": 0.7535,
      "step": 134600
    },
    {
      "epoch": 1.229104314183517,
      "grad_norm": 4.264195919036865,
      "learning_rate": 4.897574640484707e-05,
      "loss": 0.8185,
      "step": 134700
    },
    {
      "epoch": 1.2300167895466823,
      "grad_norm": 3.9441158771514893,
      "learning_rate": 4.8974986008711096e-05,
      "loss": 0.7665,
      "step": 134800
    },
    {
      "epoch": 1.2309292649098476,
      "grad_norm": 4.03292179107666,
      "learning_rate": 4.897422561257513e-05,
      "loss": 0.786,
      "step": 134900
    },
    {
      "epoch": 1.2318417402730126,
      "grad_norm": 4.161328315734863,
      "learning_rate": 4.8973465216439157e-05,
      "loss": 0.8004,
      "step": 135000
    },
    {
      "epoch": 1.2327542156361777,
      "grad_norm": 6.198822975158691,
      "learning_rate": 4.897270482030319e-05,
      "loss": 0.8113,
      "step": 135100
    },
    {
      "epoch": 1.233666690999343,
      "grad_norm": 4.468789577484131,
      "learning_rate": 4.897194442416722e-05,
      "loss": 0.7765,
      "step": 135200
    },
    {
      "epoch": 1.2345791663625083,
      "grad_norm": 3.9352245330810547,
      "learning_rate": 4.897118402803125e-05,
      "loss": 0.8249,
      "step": 135300
    },
    {
      "epoch": 1.2354916417256734,
      "grad_norm": 4.334795951843262,
      "learning_rate": 4.897042363189528e-05,
      "loss": 0.783,
      "step": 135400
    },
    {
      "epoch": 1.2364041170888387,
      "grad_norm": 3.6239588260650635,
      "learning_rate": 4.896966323575931e-05,
      "loss": 0.7909,
      "step": 135500
    },
    {
      "epoch": 1.2373165924520038,
      "grad_norm": 4.727142333984375,
      "learning_rate": 4.896890283962333e-05,
      "loss": 0.8034,
      "step": 135600
    },
    {
      "epoch": 1.238229067815169,
      "grad_norm": 4.485831260681152,
      "learning_rate": 4.896814244348737e-05,
      "loss": 0.8325,
      "step": 135700
    },
    {
      "epoch": 1.2391415431783341,
      "grad_norm": 3.620913028717041,
      "learning_rate": 4.896738204735139e-05,
      "loss": 0.8101,
      "step": 135800
    },
    {
      "epoch": 1.2400540185414994,
      "grad_norm": 3.8323628902435303,
      "learning_rate": 4.896662165121542e-05,
      "loss": 0.7961,
      "step": 135900
    },
    {
      "epoch": 1.2409664939046645,
      "grad_norm": 4.220150470733643,
      "learning_rate": 4.896586125507945e-05,
      "loss": 0.8385,
      "step": 136000
    },
    {
      "epoch": 1.2418789692678298,
      "grad_norm": 4.050631999969482,
      "learning_rate": 4.896510085894348e-05,
      "loss": 0.8403,
      "step": 136100
    },
    {
      "epoch": 1.242791444630995,
      "grad_norm": 4.12668514251709,
      "learning_rate": 4.8964340462807504e-05,
      "loss": 0.7968,
      "step": 136200
    },
    {
      "epoch": 1.2437039199941602,
      "grad_norm": 4.1275506019592285,
      "learning_rate": 4.896358006667154e-05,
      "loss": 0.7602,
      "step": 136300
    },
    {
      "epoch": 1.2446163953573253,
      "grad_norm": 4.36083984375,
      "learning_rate": 4.8962819670535564e-05,
      "loss": 0.7894,
      "step": 136400
    },
    {
      "epoch": 1.2455288707204906,
      "grad_norm": 3.6618363857269287,
      "learning_rate": 4.8962059274399594e-05,
      "loss": 0.8237,
      "step": 136500
    },
    {
      "epoch": 1.2464413460836559,
      "grad_norm": 5.574678897857666,
      "learning_rate": 4.8961298878263624e-05,
      "loss": 0.8002,
      "step": 136600
    },
    {
      "epoch": 1.247353821446821,
      "grad_norm": 4.178873538970947,
      "learning_rate": 4.896053848212765e-05,
      "loss": 0.7768,
      "step": 136700
    },
    {
      "epoch": 1.248266296809986,
      "grad_norm": 3.8718137741088867,
      "learning_rate": 4.8959778085991684e-05,
      "loss": 0.7942,
      "step": 136800
    },
    {
      "epoch": 1.2491787721731513,
      "grad_norm": 4.367191314697266,
      "learning_rate": 4.895901768985571e-05,
      "loss": 0.8244,
      "step": 136900
    },
    {
      "epoch": 1.2500912475363166,
      "grad_norm": 4.728979110717773,
      "learning_rate": 4.895825729371974e-05,
      "loss": 0.7388,
      "step": 137000
    },
    {
      "epoch": 1.2510037228994817,
      "grad_norm": 3.648041009902954,
      "learning_rate": 4.895749689758377e-05,
      "loss": 0.7674,
      "step": 137100
    },
    {
      "epoch": 1.2519161982626468,
      "grad_norm": 4.688760757446289,
      "learning_rate": 4.89567365014478e-05,
      "loss": 0.7796,
      "step": 137200
    },
    {
      "epoch": 1.252828673625812,
      "grad_norm": 4.709608554840088,
      "learning_rate": 4.895597610531182e-05,
      "loss": 0.7545,
      "step": 137300
    },
    {
      "epoch": 1.2537411489889774,
      "grad_norm": 3.142721176147461,
      "learning_rate": 4.895521570917586e-05,
      "loss": 0.7833,
      "step": 137400
    },
    {
      "epoch": 1.2546536243521424,
      "grad_norm": 4.079244613647461,
      "learning_rate": 4.895445531303988e-05,
      "loss": 0.8132,
      "step": 137500
    },
    {
      "epoch": 1.2555660997153077,
      "grad_norm": 4.428260326385498,
      "learning_rate": 4.895369491690391e-05,
      "loss": 0.7906,
      "step": 137600
    },
    {
      "epoch": 1.2564785750784728,
      "grad_norm": 4.964916706085205,
      "learning_rate": 4.895293452076794e-05,
      "loss": 0.8031,
      "step": 137700
    },
    {
      "epoch": 1.2573910504416381,
      "grad_norm": 5.13106107711792,
      "learning_rate": 4.895217412463197e-05,
      "loss": 0.8039,
      "step": 137800
    },
    {
      "epoch": 1.2583035258048032,
      "grad_norm": 4.721569061279297,
      "learning_rate": 4.8951413728496e-05,
      "loss": 0.7703,
      "step": 137900
    },
    {
      "epoch": 1.2592160011679685,
      "grad_norm": 4.308599472045898,
      "learning_rate": 4.895065333236003e-05,
      "loss": 0.8265,
      "step": 138000
    },
    {
      "epoch": 1.2601284765311336,
      "grad_norm": 4.790018081665039,
      "learning_rate": 4.8949892936224055e-05,
      "loss": 0.832,
      "step": 138100
    },
    {
      "epoch": 1.2610409518942989,
      "grad_norm": 4.244195461273193,
      "learning_rate": 4.894913254008809e-05,
      "loss": 0.7815,
      "step": 138200
    },
    {
      "epoch": 1.2619534272574642,
      "grad_norm": 4.311744689941406,
      "learning_rate": 4.8948372143952115e-05,
      "loss": 0.8365,
      "step": 138300
    },
    {
      "epoch": 1.2628659026206293,
      "grad_norm": 3.212057113647461,
      "learning_rate": 4.8947611747816145e-05,
      "loss": 0.769,
      "step": 138400
    },
    {
      "epoch": 1.2637783779837943,
      "grad_norm": 3.1609456539154053,
      "learning_rate": 4.8946851351680175e-05,
      "loss": 0.7898,
      "step": 138500
    },
    {
      "epoch": 1.2646908533469596,
      "grad_norm": 3.5881502628326416,
      "learning_rate": 4.8946090955544205e-05,
      "loss": 0.7634,
      "step": 138600
    },
    {
      "epoch": 1.265603328710125,
      "grad_norm": 3.796043872833252,
      "learning_rate": 4.894533055940823e-05,
      "loss": 0.8001,
      "step": 138700
    },
    {
      "epoch": 1.26651580407329,
      "grad_norm": 4.550314426422119,
      "learning_rate": 4.8944570163272265e-05,
      "loss": 0.8171,
      "step": 138800
    },
    {
      "epoch": 1.267428279436455,
      "grad_norm": 4.574288368225098,
      "learning_rate": 4.894380976713629e-05,
      "loss": 0.797,
      "step": 138900
    },
    {
      "epoch": 1.2683407547996204,
      "grad_norm": 4.044459342956543,
      "learning_rate": 4.894304937100032e-05,
      "loss": 0.783,
      "step": 139000
    },
    {
      "epoch": 1.2692532301627857,
      "grad_norm": 4.361219882965088,
      "learning_rate": 4.894228897486435e-05,
      "loss": 0.774,
      "step": 139100
    },
    {
      "epoch": 1.2701657055259508,
      "grad_norm": 4.954270839691162,
      "learning_rate": 4.894152857872837e-05,
      "loss": 0.7879,
      "step": 139200
    },
    {
      "epoch": 1.271078180889116,
      "grad_norm": 4.230947494506836,
      "learning_rate": 4.894076818259241e-05,
      "loss": 0.7883,
      "step": 139300
    },
    {
      "epoch": 1.2719906562522811,
      "grad_norm": 4.323333263397217,
      "learning_rate": 4.894000778645643e-05,
      "loss": 0.8117,
      "step": 139400
    },
    {
      "epoch": 1.2729031316154464,
      "grad_norm": 3.870297431945801,
      "learning_rate": 4.893924739032046e-05,
      "loss": 0.8124,
      "step": 139500
    },
    {
      "epoch": 1.2738156069786115,
      "grad_norm": 5.07396125793457,
      "learning_rate": 4.893848699418449e-05,
      "loss": 0.8272,
      "step": 139600
    },
    {
      "epoch": 1.2747280823417768,
      "grad_norm": 4.779957294464111,
      "learning_rate": 4.893772659804852e-05,
      "loss": 0.766,
      "step": 139700
    },
    {
      "epoch": 1.275640557704942,
      "grad_norm": 4.1078619956970215,
      "learning_rate": 4.8936966201912546e-05,
      "loss": 0.8258,
      "step": 139800
    },
    {
      "epoch": 1.2765530330681072,
      "grad_norm": 4.030230522155762,
      "learning_rate": 4.893620580577658e-05,
      "loss": 0.7557,
      "step": 139900
    },
    {
      "epoch": 1.2774655084312725,
      "grad_norm": 4.678022861480713,
      "learning_rate": 4.8935445409640606e-05,
      "loss": 0.756,
      "step": 140000
    },
    {
      "epoch": 1.2783779837944376,
      "grad_norm": 3.858332872390747,
      "learning_rate": 4.8934685013504636e-05,
      "loss": 0.7719,
      "step": 140100
    },
    {
      "epoch": 1.2792904591576026,
      "grad_norm": 3.842310667037964,
      "learning_rate": 4.8933924617368666e-05,
      "loss": 0.7543,
      "step": 140200
    },
    {
      "epoch": 1.280202934520768,
      "grad_norm": 3.400538921356201,
      "learning_rate": 4.8933164221232696e-05,
      "loss": 0.7962,
      "step": 140300
    },
    {
      "epoch": 1.2811154098839332,
      "grad_norm": 3.838630199432373,
      "learning_rate": 4.8932403825096726e-05,
      "loss": 0.8238,
      "step": 140400
    },
    {
      "epoch": 1.2820278852470983,
      "grad_norm": 4.099328517913818,
      "learning_rate": 4.8931643428960756e-05,
      "loss": 0.7595,
      "step": 140500
    },
    {
      "epoch": 1.2829403606102634,
      "grad_norm": 4.035376071929932,
      "learning_rate": 4.893088303282478e-05,
      "loss": 0.7937,
      "step": 140600
    },
    {
      "epoch": 1.2838528359734287,
      "grad_norm": 4.098573684692383,
      "learning_rate": 4.8930122636688816e-05,
      "loss": 0.7902,
      "step": 140700
    },
    {
      "epoch": 1.284765311336594,
      "grad_norm": 3.973428249359131,
      "learning_rate": 4.892936224055284e-05,
      "loss": 0.7853,
      "step": 140800
    },
    {
      "epoch": 1.285677786699759,
      "grad_norm": 4.348409652709961,
      "learning_rate": 4.892860184441687e-05,
      "loss": 0.7922,
      "step": 140900
    },
    {
      "epoch": 1.2865902620629244,
      "grad_norm": 4.054760932922363,
      "learning_rate": 4.89278414482809e-05,
      "loss": 0.8054,
      "step": 141000
    },
    {
      "epoch": 1.2875027374260894,
      "grad_norm": 3.6458542346954346,
      "learning_rate": 4.892708105214493e-05,
      "loss": 0.8159,
      "step": 141100
    },
    {
      "epoch": 1.2884152127892547,
      "grad_norm": 5.402257919311523,
      "learning_rate": 4.892632065600895e-05,
      "loss": 0.7699,
      "step": 141200
    },
    {
      "epoch": 1.2893276881524198,
      "grad_norm": 4.075182914733887,
      "learning_rate": 4.892556025987299e-05,
      "loss": 0.8071,
      "step": 141300
    },
    {
      "epoch": 1.2902401635155851,
      "grad_norm": 5.298579216003418,
      "learning_rate": 4.892479986373701e-05,
      "loss": 0.8115,
      "step": 141400
    },
    {
      "epoch": 1.2911526388787502,
      "grad_norm": 4.832768440246582,
      "learning_rate": 4.892403946760104e-05,
      "loss": 0.8046,
      "step": 141500
    },
    {
      "epoch": 1.2920651142419155,
      "grad_norm": 3.054744243621826,
      "learning_rate": 4.892327907146507e-05,
      "loss": 0.7655,
      "step": 141600
    },
    {
      "epoch": 1.2929775896050808,
      "grad_norm": 4.8365702629089355,
      "learning_rate": 4.89225186753291e-05,
      "loss": 0.8114,
      "step": 141700
    },
    {
      "epoch": 1.2938900649682459,
      "grad_norm": 4.334744453430176,
      "learning_rate": 4.892175827919313e-05,
      "loss": 0.8048,
      "step": 141800
    },
    {
      "epoch": 1.294802540331411,
      "grad_norm": 4.042202949523926,
      "learning_rate": 4.8920997883057163e-05,
      "loss": 0.8017,
      "step": 141900
    },
    {
      "epoch": 1.2957150156945763,
      "grad_norm": 4.493313789367676,
      "learning_rate": 4.892023748692119e-05,
      "loss": 0.7856,
      "step": 142000
    },
    {
      "epoch": 1.2966274910577416,
      "grad_norm": 4.623991012573242,
      "learning_rate": 4.891947709078522e-05,
      "loss": 0.7768,
      "step": 142100
    },
    {
      "epoch": 1.2975399664209066,
      "grad_norm": 4.137228965759277,
      "learning_rate": 4.891871669464925e-05,
      "loss": 0.8207,
      "step": 142200
    },
    {
      "epoch": 1.2984524417840717,
      "grad_norm": 3.418971300125122,
      "learning_rate": 4.891795629851327e-05,
      "loss": 0.7997,
      "step": 142300
    },
    {
      "epoch": 1.299364917147237,
      "grad_norm": 4.283239364624023,
      "learning_rate": 4.891719590237731e-05,
      "loss": 0.8131,
      "step": 142400
    },
    {
      "epoch": 1.3002773925104023,
      "grad_norm": 4.446286201477051,
      "learning_rate": 4.891643550624133e-05,
      "loss": 0.804,
      "step": 142500
    },
    {
      "epoch": 1.3011898678735674,
      "grad_norm": 3.622269868850708,
      "learning_rate": 4.891567511010536e-05,
      "loss": 0.7595,
      "step": 142600
    },
    {
      "epoch": 1.3021023432367327,
      "grad_norm": 4.7857489585876465,
      "learning_rate": 4.891491471396939e-05,
      "loss": 0.7998,
      "step": 142700
    },
    {
      "epoch": 1.3030148185998978,
      "grad_norm": 3.808826208114624,
      "learning_rate": 4.891415431783342e-05,
      "loss": 0.7426,
      "step": 142800
    },
    {
      "epoch": 1.303927293963063,
      "grad_norm": 3.8877944946289062,
      "learning_rate": 4.891339392169745e-05,
      "loss": 0.8024,
      "step": 142900
    },
    {
      "epoch": 1.3048397693262281,
      "grad_norm": 4.66862678527832,
      "learning_rate": 4.891263352556148e-05,
      "loss": 0.77,
      "step": 143000
    },
    {
      "epoch": 1.3057522446893934,
      "grad_norm": 3.615239381790161,
      "learning_rate": 4.8911873129425504e-05,
      "loss": 0.7787,
      "step": 143100
    },
    {
      "epoch": 1.3066647200525585,
      "grad_norm": 4.019284725189209,
      "learning_rate": 4.891111273328954e-05,
      "loss": 0.8168,
      "step": 143200
    },
    {
      "epoch": 1.3075771954157238,
      "grad_norm": 4.385986328125,
      "learning_rate": 4.8910352337153564e-05,
      "loss": 0.7663,
      "step": 143300
    },
    {
      "epoch": 1.308489670778889,
      "grad_norm": 4.7837958335876465,
      "learning_rate": 4.8909591941017594e-05,
      "loss": 0.8097,
      "step": 143400
    },
    {
      "epoch": 1.3094021461420542,
      "grad_norm": 4.473705291748047,
      "learning_rate": 4.8908831544881624e-05,
      "loss": 0.7714,
      "step": 143500
    },
    {
      "epoch": 1.3103146215052193,
      "grad_norm": 4.654783725738525,
      "learning_rate": 4.8908071148745654e-05,
      "loss": 0.7837,
      "step": 143600
    },
    {
      "epoch": 1.3112270968683846,
      "grad_norm": 5.1085004806518555,
      "learning_rate": 4.890731075260968e-05,
      "loss": 0.7984,
      "step": 143700
    },
    {
      "epoch": 1.3121395722315499,
      "grad_norm": 3.9035191535949707,
      "learning_rate": 4.8906550356473714e-05,
      "loss": 0.7598,
      "step": 143800
    },
    {
      "epoch": 1.313052047594715,
      "grad_norm": 3.9345409870147705,
      "learning_rate": 4.890578996033774e-05,
      "loss": 0.7826,
      "step": 143900
    },
    {
      "epoch": 1.31396452295788,
      "grad_norm": 4.093291282653809,
      "learning_rate": 4.890502956420177e-05,
      "loss": 0.8142,
      "step": 144000
    },
    {
      "epoch": 1.3148769983210453,
      "grad_norm": 5.295246124267578,
      "learning_rate": 4.89042691680658e-05,
      "loss": 0.8143,
      "step": 144100
    },
    {
      "epoch": 1.3157894736842106,
      "grad_norm": 5.234483242034912,
      "learning_rate": 4.890350877192983e-05,
      "loss": 0.77,
      "step": 144200
    },
    {
      "epoch": 1.3167019490473757,
      "grad_norm": 4.131352424621582,
      "learning_rate": 4.890274837579386e-05,
      "loss": 0.7976,
      "step": 144300
    },
    {
      "epoch": 1.317614424410541,
      "grad_norm": 4.498581886291504,
      "learning_rate": 4.890198797965789e-05,
      "loss": 0.7952,
      "step": 144400
    },
    {
      "epoch": 1.318526899773706,
      "grad_norm": 3.5406572818756104,
      "learning_rate": 4.890122758352191e-05,
      "loss": 0.8222,
      "step": 144500
    },
    {
      "epoch": 1.3194393751368714,
      "grad_norm": 4.143039226531982,
      "learning_rate": 4.890046718738595e-05,
      "loss": 0.7829,
      "step": 144600
    },
    {
      "epoch": 1.3203518505000365,
      "grad_norm": 4.106692790985107,
      "learning_rate": 4.889970679124997e-05,
      "loss": 0.8048,
      "step": 144700
    },
    {
      "epoch": 1.3212643258632017,
      "grad_norm": 3.6890063285827637,
      "learning_rate": 4.8898946395114e-05,
      "loss": 0.8476,
      "step": 144800
    },
    {
      "epoch": 1.3221768012263668,
      "grad_norm": 4.507870674133301,
      "learning_rate": 4.889818599897803e-05,
      "loss": 0.7869,
      "step": 144900
    },
    {
      "epoch": 1.3230892765895321,
      "grad_norm": 4.877001762390137,
      "learning_rate": 4.8897425602842055e-05,
      "loss": 0.8106,
      "step": 145000
    },
    {
      "epoch": 1.3240017519526972,
      "grad_norm": 4.73610782623291,
      "learning_rate": 4.8896665206706085e-05,
      "loss": 0.7589,
      "step": 145100
    },
    {
      "epoch": 1.3249142273158625,
      "grad_norm": 4.974009037017822,
      "learning_rate": 4.8895904810570115e-05,
      "loss": 0.7825,
      "step": 145200
    },
    {
      "epoch": 1.3258267026790276,
      "grad_norm": 3.659747838973999,
      "learning_rate": 4.8895144414434145e-05,
      "loss": 0.7766,
      "step": 145300
    },
    {
      "epoch": 1.3267391780421929,
      "grad_norm": 4.908456325531006,
      "learning_rate": 4.8894384018298175e-05,
      "loss": 0.7413,
      "step": 145400
    },
    {
      "epoch": 1.3276516534053582,
      "grad_norm": 3.5936508178710938,
      "learning_rate": 4.8893623622162205e-05,
      "loss": 0.793,
      "step": 145500
    },
    {
      "epoch": 1.3285641287685233,
      "grad_norm": 4.196508884429932,
      "learning_rate": 4.889286322602623e-05,
      "loss": 0.7676,
      "step": 145600
    },
    {
      "epoch": 1.3294766041316883,
      "grad_norm": 3.540283203125,
      "learning_rate": 4.8892102829890265e-05,
      "loss": 0.7979,
      "step": 145700
    },
    {
      "epoch": 1.3303890794948536,
      "grad_norm": 4.737167835235596,
      "learning_rate": 4.889134243375429e-05,
      "loss": 0.7882,
      "step": 145800
    },
    {
      "epoch": 1.331301554858019,
      "grad_norm": 4.0540266036987305,
      "learning_rate": 4.889058203761832e-05,
      "loss": 0.8178,
      "step": 145900
    },
    {
      "epoch": 1.332214030221184,
      "grad_norm": 3.526362180709839,
      "learning_rate": 4.888982164148235e-05,
      "loss": 0.8057,
      "step": 146000
    },
    {
      "epoch": 1.333126505584349,
      "grad_norm": 4.405838489532471,
      "learning_rate": 4.888906124534638e-05,
      "loss": 0.7951,
      "step": 146100
    },
    {
      "epoch": 1.3340389809475144,
      "grad_norm": 4.623712062835693,
      "learning_rate": 4.888830084921041e-05,
      "loss": 0.8019,
      "step": 146200
    },
    {
      "epoch": 1.3349514563106797,
      "grad_norm": 4.3376054763793945,
      "learning_rate": 4.888754045307444e-05,
      "loss": 0.8056,
      "step": 146300
    },
    {
      "epoch": 1.3358639316738448,
      "grad_norm": 4.867824077606201,
      "learning_rate": 4.888678005693846e-05,
      "loss": 0.7989,
      "step": 146400
    },
    {
      "epoch": 1.33677640703701,
      "grad_norm": 3.830087423324585,
      "learning_rate": 4.888601966080249e-05,
      "loss": 0.7812,
      "step": 146500
    },
    {
      "epoch": 1.3376888824001751,
      "grad_norm": 4.509480953216553,
      "learning_rate": 4.888525926466652e-05,
      "loss": 0.7733,
      "step": 146600
    },
    {
      "epoch": 1.3386013577633404,
      "grad_norm": 4.425678730010986,
      "learning_rate": 4.888449886853055e-05,
      "loss": 0.754,
      "step": 146700
    },
    {
      "epoch": 1.3395138331265055,
      "grad_norm": 5.506452560424805,
      "learning_rate": 4.888373847239458e-05,
      "loss": 0.8025,
      "step": 146800
    },
    {
      "epoch": 1.3404263084896708,
      "grad_norm": 4.583828926086426,
      "learning_rate": 4.888297807625861e-05,
      "loss": 0.7626,
      "step": 146900
    },
    {
      "epoch": 1.341338783852836,
      "grad_norm": 4.865421772003174,
      "learning_rate": 4.8882217680122636e-05,
      "loss": 0.7912,
      "step": 147000
    },
    {
      "epoch": 1.3422512592160012,
      "grad_norm": 2.9254355430603027,
      "learning_rate": 4.888145728398667e-05,
      "loss": 0.7314,
      "step": 147100
    },
    {
      "epoch": 1.3431637345791665,
      "grad_norm": 4.2121381759643555,
      "learning_rate": 4.8880696887850696e-05,
      "loss": 0.8154,
      "step": 147200
    },
    {
      "epoch": 1.3440762099423316,
      "grad_norm": 4.254022598266602,
      "learning_rate": 4.8879936491714726e-05,
      "loss": 0.7752,
      "step": 147300
    },
    {
      "epoch": 1.3449886853054966,
      "grad_norm": 3.8981430530548096,
      "learning_rate": 4.8879176095578756e-05,
      "loss": 0.83,
      "step": 147400
    },
    {
      "epoch": 1.345901160668662,
      "grad_norm": 5.167783260345459,
      "learning_rate": 4.8878415699442786e-05,
      "loss": 0.8061,
      "step": 147500
    },
    {
      "epoch": 1.3468136360318272,
      "grad_norm": 4.576004505157471,
      "learning_rate": 4.8877655303306816e-05,
      "loss": 0.7997,
      "step": 147600
    },
    {
      "epoch": 1.3477261113949923,
      "grad_norm": 4.439978122711182,
      "learning_rate": 4.887689490717084e-05,
      "loss": 0.7435,
      "step": 147700
    },
    {
      "epoch": 1.3486385867581574,
      "grad_norm": 4.773475646972656,
      "learning_rate": 4.887613451103487e-05,
      "loss": 0.7839,
      "step": 147800
    },
    {
      "epoch": 1.3495510621213227,
      "grad_norm": 4.369948387145996,
      "learning_rate": 4.88753741148989e-05,
      "loss": 0.8176,
      "step": 147900
    },
    {
      "epoch": 1.350463537484488,
      "grad_norm": 4.249583721160889,
      "learning_rate": 4.887461371876293e-05,
      "loss": 0.7846,
      "step": 148000
    },
    {
      "epoch": 1.351376012847653,
      "grad_norm": 4.735424518585205,
      "learning_rate": 4.887385332262695e-05,
      "loss": 0.8224,
      "step": 148100
    },
    {
      "epoch": 1.3522884882108184,
      "grad_norm": 2.6306302547454834,
      "learning_rate": 4.887309292649099e-05,
      "loss": 0.7876,
      "step": 148200
    },
    {
      "epoch": 1.3532009635739835,
      "grad_norm": 3.5169994831085205,
      "learning_rate": 4.887233253035501e-05,
      "loss": 0.7853,
      "step": 148300
    },
    {
      "epoch": 1.3541134389371488,
      "grad_norm": 4.641441345214844,
      "learning_rate": 4.887157213421904e-05,
      "loss": 0.8038,
      "step": 148400
    },
    {
      "epoch": 1.3550259143003138,
      "grad_norm": 4.913157939910889,
      "learning_rate": 4.887081173808307e-05,
      "loss": 0.7703,
      "step": 148500
    },
    {
      "epoch": 1.3559383896634791,
      "grad_norm": 3.7633514404296875,
      "learning_rate": 4.8870051341947103e-05,
      "loss": 0.7589,
      "step": 148600
    },
    {
      "epoch": 1.3568508650266442,
      "grad_norm": 4.556457042694092,
      "learning_rate": 4.8869290945811133e-05,
      "loss": 0.7779,
      "step": 148700
    },
    {
      "epoch": 1.3577633403898095,
      "grad_norm": 4.883115768432617,
      "learning_rate": 4.8868530549675164e-05,
      "loss": 0.7369,
      "step": 148800
    },
    {
      "epoch": 1.3586758157529748,
      "grad_norm": 4.363866806030273,
      "learning_rate": 4.886777015353919e-05,
      "loss": 0.7737,
      "step": 148900
    },
    {
      "epoch": 1.3595882911161399,
      "grad_norm": 5.213743686676025,
      "learning_rate": 4.8867009757403224e-05,
      "loss": 0.809,
      "step": 149000
    },
    {
      "epoch": 1.360500766479305,
      "grad_norm": 5.235286235809326,
      "learning_rate": 4.886624936126725e-05,
      "loss": 0.789,
      "step": 149100
    },
    {
      "epoch": 1.3614132418424703,
      "grad_norm": 4.2480621337890625,
      "learning_rate": 4.886548896513128e-05,
      "loss": 0.7496,
      "step": 149200
    },
    {
      "epoch": 1.3623257172056356,
      "grad_norm": 4.016820430755615,
      "learning_rate": 4.886472856899531e-05,
      "loss": 0.8104,
      "step": 149300
    },
    {
      "epoch": 1.3632381925688006,
      "grad_norm": 4.11636209487915,
      "learning_rate": 4.886396817285934e-05,
      "loss": 0.7792,
      "step": 149400
    },
    {
      "epoch": 1.3641506679319657,
      "grad_norm": 5.130214691162109,
      "learning_rate": 4.886320777672336e-05,
      "loss": 0.7951,
      "step": 149500
    },
    {
      "epoch": 1.365063143295131,
      "grad_norm": 4.154774188995361,
      "learning_rate": 4.88624473805874e-05,
      "loss": 0.795,
      "step": 149600
    },
    {
      "epoch": 1.3659756186582963,
      "grad_norm": 4.398967742919922,
      "learning_rate": 4.886168698445142e-05,
      "loss": 0.798,
      "step": 149700
    },
    {
      "epoch": 1.3668880940214614,
      "grad_norm": 3.728015184402466,
      "learning_rate": 4.886092658831545e-05,
      "loss": 0.8417,
      "step": 149800
    },
    {
      "epoch": 1.3678005693846267,
      "grad_norm": 4.421133995056152,
      "learning_rate": 4.886016619217948e-05,
      "loss": 0.7978,
      "step": 149900
    },
    {
      "epoch": 1.3687130447477918,
      "grad_norm": 4.725980758666992,
      "learning_rate": 4.885940579604351e-05,
      "loss": 0.7654,
      "step": 150000
    },
    {
      "epoch": 1.369625520110957,
      "grad_norm": 4.712015628814697,
      "learning_rate": 4.885864539990754e-05,
      "loss": 0.819,
      "step": 150100
    },
    {
      "epoch": 1.3705379954741221,
      "grad_norm": 4.139769554138184,
      "learning_rate": 4.885788500377157e-05,
      "loss": 0.7761,
      "step": 150200
    },
    {
      "epoch": 1.3714504708372874,
      "grad_norm": 5.185241222381592,
      "learning_rate": 4.8857124607635594e-05,
      "loss": 0.7551,
      "step": 150300
    },
    {
      "epoch": 1.3723629462004525,
      "grad_norm": 3.7040979862213135,
      "learning_rate": 4.885636421149963e-05,
      "loss": 0.7885,
      "step": 150400
    },
    {
      "epoch": 1.3732754215636178,
      "grad_norm": 4.07720947265625,
      "learning_rate": 4.8855603815363654e-05,
      "loss": 0.7674,
      "step": 150500
    },
    {
      "epoch": 1.3741878969267831,
      "grad_norm": 4.92459774017334,
      "learning_rate": 4.885484341922768e-05,
      "loss": 0.8125,
      "step": 150600
    },
    {
      "epoch": 1.3751003722899482,
      "grad_norm": 2.543748378753662,
      "learning_rate": 4.8854083023091714e-05,
      "loss": 0.7644,
      "step": 150700
    },
    {
      "epoch": 1.3760128476531133,
      "grad_norm": 4.484607696533203,
      "learning_rate": 4.885332262695574e-05,
      "loss": 0.7694,
      "step": 150800
    },
    {
      "epoch": 1.3769253230162786,
      "grad_norm": 4.1798248291015625,
      "learning_rate": 4.885256223081977e-05,
      "loss": 0.7505,
      "step": 150900
    },
    {
      "epoch": 1.3778377983794439,
      "grad_norm": 5.654468059539795,
      "learning_rate": 4.88518018346838e-05,
      "loss": 0.7844,
      "step": 151000
    },
    {
      "epoch": 1.378750273742609,
      "grad_norm": 5.1203389167785645,
      "learning_rate": 4.885104143854783e-05,
      "loss": 0.7371,
      "step": 151100
    },
    {
      "epoch": 1.379662749105774,
      "grad_norm": 3.415299415588379,
      "learning_rate": 4.885028104241186e-05,
      "loss": 0.7936,
      "step": 151200
    },
    {
      "epoch": 1.3805752244689393,
      "grad_norm": 4.779162883758545,
      "learning_rate": 4.884952064627589e-05,
      "loss": 0.79,
      "step": 151300
    },
    {
      "epoch": 1.3814876998321046,
      "grad_norm": 4.6649699211120605,
      "learning_rate": 4.884876025013991e-05,
      "loss": 0.7832,
      "step": 151400
    },
    {
      "epoch": 1.3824001751952697,
      "grad_norm": 3.8678090572357178,
      "learning_rate": 4.884799985400395e-05,
      "loss": 0.72,
      "step": 151500
    },
    {
      "epoch": 1.383312650558435,
      "grad_norm": 4.371793746948242,
      "learning_rate": 4.884723945786797e-05,
      "loss": 0.74,
      "step": 151600
    },
    {
      "epoch": 1.3842251259216,
      "grad_norm": 4.37488317489624,
      "learning_rate": 4.8846479061732e-05,
      "loss": 0.7794,
      "step": 151700
    },
    {
      "epoch": 1.3851376012847654,
      "grad_norm": 4.661971569061279,
      "learning_rate": 4.884571866559603e-05,
      "loss": 0.8049,
      "step": 151800
    },
    {
      "epoch": 1.3860500766479305,
      "grad_norm": 4.654907703399658,
      "learning_rate": 4.884495826946006e-05,
      "loss": 0.8169,
      "step": 151900
    },
    {
      "epoch": 1.3869625520110958,
      "grad_norm": 4.482296466827393,
      "learning_rate": 4.8844197873324085e-05,
      "loss": 0.837,
      "step": 152000
    },
    {
      "epoch": 1.3878750273742608,
      "grad_norm": 4.252895355224609,
      "learning_rate": 4.884343747718812e-05,
      "loss": 0.775,
      "step": 152100
    },
    {
      "epoch": 1.3887875027374261,
      "grad_norm": 3.8146650791168213,
      "learning_rate": 4.8842677081052145e-05,
      "loss": 0.8103,
      "step": 152200
    },
    {
      "epoch": 1.3896999781005912,
      "grad_norm": 3.5230770111083984,
      "learning_rate": 4.8841916684916175e-05,
      "loss": 0.7969,
      "step": 152300
    },
    {
      "epoch": 1.3906124534637565,
      "grad_norm": 6.155178546905518,
      "learning_rate": 4.8841156288780205e-05,
      "loss": 0.7439,
      "step": 152400
    },
    {
      "epoch": 1.3915249288269216,
      "grad_norm": 3.9659643173217773,
      "learning_rate": 4.8840395892644235e-05,
      "loss": 0.7855,
      "step": 152500
    },
    {
      "epoch": 1.3924374041900869,
      "grad_norm": 4.546120643615723,
      "learning_rate": 4.8839635496508265e-05,
      "loss": 0.7766,
      "step": 152600
    },
    {
      "epoch": 1.3933498795532522,
      "grad_norm": 4.065113067626953,
      "learning_rate": 4.8838875100372296e-05,
      "loss": 0.7591,
      "step": 152700
    },
    {
      "epoch": 1.3942623549164173,
      "grad_norm": 3.9228873252868652,
      "learning_rate": 4.883811470423632e-05,
      "loss": 0.7491,
      "step": 152800
    },
    {
      "epoch": 1.3951748302795823,
      "grad_norm": 4.182530879974365,
      "learning_rate": 4.8837354308100356e-05,
      "loss": 0.7862,
      "step": 152900
    },
    {
      "epoch": 1.3960873056427476,
      "grad_norm": 4.220347881317139,
      "learning_rate": 4.883659391196438e-05,
      "loss": 0.8079,
      "step": 153000
    },
    {
      "epoch": 1.396999781005913,
      "grad_norm": 4.0926289558410645,
      "learning_rate": 4.883583351582841e-05,
      "loss": 0.8043,
      "step": 153100
    },
    {
      "epoch": 1.397912256369078,
      "grad_norm": 3.8934412002563477,
      "learning_rate": 4.883507311969244e-05,
      "loss": 0.7787,
      "step": 153200
    },
    {
      "epoch": 1.3988247317322433,
      "grad_norm": 4.232353210449219,
      "learning_rate": 4.883431272355647e-05,
      "loss": 0.8187,
      "step": 153300
    },
    {
      "epoch": 1.3997372070954084,
      "grad_norm": 5.165478706359863,
      "learning_rate": 4.883355232742049e-05,
      "loss": 0.8027,
      "step": 153400
    },
    {
      "epoch": 1.4006496824585737,
      "grad_norm": 4.541467189788818,
      "learning_rate": 4.883279193128452e-05,
      "loss": 0.7766,
      "step": 153500
    },
    {
      "epoch": 1.4015621578217388,
      "grad_norm": 3.5163280963897705,
      "learning_rate": 4.883203153514855e-05,
      "loss": 0.7881,
      "step": 153600
    },
    {
      "epoch": 1.402474633184904,
      "grad_norm": 4.457280158996582,
      "learning_rate": 4.883127113901258e-05,
      "loss": 0.7563,
      "step": 153700
    },
    {
      "epoch": 1.4033871085480691,
      "grad_norm": 4.692083358764648,
      "learning_rate": 4.883051074287661e-05,
      "loss": 0.7941,
      "step": 153800
    },
    {
      "epoch": 1.4042995839112344,
      "grad_norm": 3.5561485290527344,
      "learning_rate": 4.8829750346740636e-05,
      "loss": 0.778,
      "step": 153900
    },
    {
      "epoch": 1.4052120592743995,
      "grad_norm": 4.607639312744141,
      "learning_rate": 4.882898995060467e-05,
      "loss": 0.741,
      "step": 154000
    },
    {
      "epoch": 1.4061245346375648,
      "grad_norm": 4.062695503234863,
      "learning_rate": 4.8828229554468696e-05,
      "loss": 0.7697,
      "step": 154100
    },
    {
      "epoch": 1.40703701000073,
      "grad_norm": 4.3309783935546875,
      "learning_rate": 4.8827469158332726e-05,
      "loss": 0.7982,
      "step": 154200
    },
    {
      "epoch": 1.4079494853638952,
      "grad_norm": 5.003017902374268,
      "learning_rate": 4.8826708762196756e-05,
      "loss": 0.7976,
      "step": 154300
    },
    {
      "epoch": 1.4088619607270605,
      "grad_norm": 3.7114651203155518,
      "learning_rate": 4.8825948366060786e-05,
      "loss": 0.8127,
      "step": 154400
    },
    {
      "epoch": 1.4097744360902256,
      "grad_norm": 4.199734210968018,
      "learning_rate": 4.882518796992481e-05,
      "loss": 0.7937,
      "step": 154500
    },
    {
      "epoch": 1.4106869114533906,
      "grad_norm": 3.465270519256592,
      "learning_rate": 4.8824427573788846e-05,
      "loss": 0.7883,
      "step": 154600
    },
    {
      "epoch": 1.411599386816556,
      "grad_norm": 3.9870402812957764,
      "learning_rate": 4.882366717765287e-05,
      "loss": 0.8172,
      "step": 154700
    },
    {
      "epoch": 1.4125118621797212,
      "grad_norm": 5.318056583404541,
      "learning_rate": 4.88229067815169e-05,
      "loss": 0.7839,
      "step": 154800
    },
    {
      "epoch": 1.4134243375428863,
      "grad_norm": 4.140570163726807,
      "learning_rate": 4.882214638538093e-05,
      "loss": 0.8218,
      "step": 154900
    },
    {
      "epoch": 1.4143368129060514,
      "grad_norm": 4.746542930603027,
      "learning_rate": 4.882138598924496e-05,
      "loss": 0.7705,
      "step": 155000
    },
    {
      "epoch": 1.4152492882692167,
      "grad_norm": 4.4167256355285645,
      "learning_rate": 4.882062559310899e-05,
      "loss": 0.7996,
      "step": 155100
    },
    {
      "epoch": 1.416161763632382,
      "grad_norm": 4.126987934112549,
      "learning_rate": 4.881986519697302e-05,
      "loss": 0.811,
      "step": 155200
    },
    {
      "epoch": 1.417074238995547,
      "grad_norm": 3.349886178970337,
      "learning_rate": 4.881910480083704e-05,
      "loss": 0.7886,
      "step": 155300
    },
    {
      "epoch": 1.4179867143587124,
      "grad_norm": 3.640625238418579,
      "learning_rate": 4.881834440470108e-05,
      "loss": 0.7783,
      "step": 155400
    },
    {
      "epoch": 1.4188991897218775,
      "grad_norm": 4.060667037963867,
      "learning_rate": 4.8817584008565104e-05,
      "loss": 0.7954,
      "step": 155500
    },
    {
      "epoch": 1.4198116650850428,
      "grad_norm": 4.035653114318848,
      "learning_rate": 4.8816823612429134e-05,
      "loss": 0.7685,
      "step": 155600
    },
    {
      "epoch": 1.4207241404482078,
      "grad_norm": 4.257676601409912,
      "learning_rate": 4.8816063216293164e-05,
      "loss": 0.7543,
      "step": 155700
    },
    {
      "epoch": 1.4216366158113731,
      "grad_norm": 4.389501571655273,
      "learning_rate": 4.8815302820157194e-05,
      "loss": 0.7917,
      "step": 155800
    },
    {
      "epoch": 1.4225490911745382,
      "grad_norm": 3.8456740379333496,
      "learning_rate": 4.881454242402122e-05,
      "loss": 0.7888,
      "step": 155900
    },
    {
      "epoch": 1.4234615665377035,
      "grad_norm": 4.608313083648682,
      "learning_rate": 4.8813782027885254e-05,
      "loss": 0.7468,
      "step": 156000
    },
    {
      "epoch": 1.4243740419008688,
      "grad_norm": 3.6973557472229004,
      "learning_rate": 4.881302163174928e-05,
      "loss": 0.8097,
      "step": 156100
    },
    {
      "epoch": 1.4252865172640339,
      "grad_norm": 4.49501895904541,
      "learning_rate": 4.881226123561331e-05,
      "loss": 0.8202,
      "step": 156200
    },
    {
      "epoch": 1.426198992627199,
      "grad_norm": 4.587484359741211,
      "learning_rate": 4.881150083947734e-05,
      "loss": 0.8364,
      "step": 156300
    },
    {
      "epoch": 1.4271114679903643,
      "grad_norm": 4.832925319671631,
      "learning_rate": 4.881074044334136e-05,
      "loss": 0.7893,
      "step": 156400
    },
    {
      "epoch": 1.4280239433535296,
      "grad_norm": 4.267385005950928,
      "learning_rate": 4.88099800472054e-05,
      "loss": 0.814,
      "step": 156500
    },
    {
      "epoch": 1.4289364187166946,
      "grad_norm": 4.177628993988037,
      "learning_rate": 4.880921965106942e-05,
      "loss": 0.7696,
      "step": 156600
    },
    {
      "epoch": 1.4298488940798597,
      "grad_norm": 3.8848471641540527,
      "learning_rate": 4.880845925493345e-05,
      "loss": 0.7991,
      "step": 156700
    },
    {
      "epoch": 1.430761369443025,
      "grad_norm": 5.316272735595703,
      "learning_rate": 4.880769885879748e-05,
      "loss": 0.8044,
      "step": 156800
    },
    {
      "epoch": 1.4316738448061903,
      "grad_norm": 4.257831573486328,
      "learning_rate": 4.880693846266151e-05,
      "loss": 0.8052,
      "step": 156900
    },
    {
      "epoch": 1.4325863201693554,
      "grad_norm": 4.578546524047852,
      "learning_rate": 4.8806178066525534e-05,
      "loss": 0.776,
      "step": 157000
    },
    {
      "epoch": 1.4334987955325207,
      "grad_norm": 3.135012626647949,
      "learning_rate": 4.880541767038957e-05,
      "loss": 0.7669,
      "step": 157100
    },
    {
      "epoch": 1.4344112708956858,
      "grad_norm": 4.549189567565918,
      "learning_rate": 4.8804657274253594e-05,
      "loss": 0.7815,
      "step": 157200
    },
    {
      "epoch": 1.435323746258851,
      "grad_norm": 4.881802082061768,
      "learning_rate": 4.8803896878117624e-05,
      "loss": 0.7715,
      "step": 157300
    },
    {
      "epoch": 1.4362362216220161,
      "grad_norm": 4.3244123458862305,
      "learning_rate": 4.8803136481981654e-05,
      "loss": 0.7819,
      "step": 157400
    },
    {
      "epoch": 1.4371486969851814,
      "grad_norm": 4.690623760223389,
      "learning_rate": 4.8802376085845685e-05,
      "loss": 0.747,
      "step": 157500
    },
    {
      "epoch": 1.4380611723483465,
      "grad_norm": 4.15639066696167,
      "learning_rate": 4.8801615689709715e-05,
      "loss": 0.7867,
      "step": 157600
    },
    {
      "epoch": 1.4389736477115118,
      "grad_norm": 4.589691638946533,
      "learning_rate": 4.8800855293573745e-05,
      "loss": 0.8091,
      "step": 157700
    },
    {
      "epoch": 1.4398861230746771,
      "grad_norm": 5.116901397705078,
      "learning_rate": 4.880009489743777e-05,
      "loss": 0.8488,
      "step": 157800
    },
    {
      "epoch": 1.4407985984378422,
      "grad_norm": 4.654663562774658,
      "learning_rate": 4.8799334501301805e-05,
      "loss": 0.7471,
      "step": 157900
    },
    {
      "epoch": 1.4417110738010073,
      "grad_norm": 5.282893657684326,
      "learning_rate": 4.879857410516583e-05,
      "loss": 0.8078,
      "step": 158000
    },
    {
      "epoch": 1.4426235491641726,
      "grad_norm": 4.128176689147949,
      "learning_rate": 4.879781370902986e-05,
      "loss": 0.7373,
      "step": 158100
    },
    {
      "epoch": 1.4435360245273379,
      "grad_norm": 3.2244322299957275,
      "learning_rate": 4.879705331289389e-05,
      "loss": 0.7577,
      "step": 158200
    },
    {
      "epoch": 1.444448499890503,
      "grad_norm": 3.8687288761138916,
      "learning_rate": 4.879629291675792e-05,
      "loss": 0.8018,
      "step": 158300
    },
    {
      "epoch": 1.445360975253668,
      "grad_norm": 5.243131637573242,
      "learning_rate": 4.879553252062194e-05,
      "loss": 0.7998,
      "step": 158400
    },
    {
      "epoch": 1.4462734506168333,
      "grad_norm": 5.473785400390625,
      "learning_rate": 4.879477212448598e-05,
      "loss": 0.8046,
      "step": 158500
    },
    {
      "epoch": 1.4471859259799986,
      "grad_norm": 3.5679259300231934,
      "learning_rate": 4.879401172835e-05,
      "loss": 0.7808,
      "step": 158600
    },
    {
      "epoch": 1.4480984013431637,
      "grad_norm": 4.543566703796387,
      "learning_rate": 4.879325133221403e-05,
      "loss": 0.821,
      "step": 158700
    },
    {
      "epoch": 1.449010876706329,
      "grad_norm": 4.447004318237305,
      "learning_rate": 4.879249093607806e-05,
      "loss": 0.7984,
      "step": 158800
    },
    {
      "epoch": 1.449923352069494,
      "grad_norm": 4.315978527069092,
      "learning_rate": 4.879173053994209e-05,
      "loss": 0.785,
      "step": 158900
    },
    {
      "epoch": 1.4508358274326594,
      "grad_norm": 3.8171443939208984,
      "learning_rate": 4.879097014380612e-05,
      "loss": 0.7871,
      "step": 159000
    },
    {
      "epoch": 1.4517483027958245,
      "grad_norm": 4.569701194763184,
      "learning_rate": 4.8790209747670145e-05,
      "loss": 0.8052,
      "step": 159100
    },
    {
      "epoch": 1.4526607781589898,
      "grad_norm": 3.9485573768615723,
      "learning_rate": 4.8789449351534175e-05,
      "loss": 0.7764,
      "step": 159200
    },
    {
      "epoch": 1.4535732535221548,
      "grad_norm": 4.318072319030762,
      "learning_rate": 4.8788688955398205e-05,
      "loss": 0.7729,
      "step": 159300
    },
    {
      "epoch": 1.4544857288853201,
      "grad_norm": 4.339328765869141,
      "learning_rate": 4.8787928559262235e-05,
      "loss": 0.7947,
      "step": 159400
    },
    {
      "epoch": 1.4553982042484854,
      "grad_norm": 4.655227184295654,
      "learning_rate": 4.8787168163126266e-05,
      "loss": 0.7308,
      "step": 159500
    },
    {
      "epoch": 1.4563106796116505,
      "grad_norm": 4.0664472579956055,
      "learning_rate": 4.8786407766990296e-05,
      "loss": 0.8288,
      "step": 159600
    },
    {
      "epoch": 1.4572231549748156,
      "grad_norm": 4.373852729797363,
      "learning_rate": 4.878564737085432e-05,
      "loss": 0.8179,
      "step": 159700
    },
    {
      "epoch": 1.4581356303379809,
      "grad_norm": 4.998409748077393,
      "learning_rate": 4.878488697471835e-05,
      "loss": 0.81,
      "step": 159800
    },
    {
      "epoch": 1.4590481057011462,
      "grad_norm": 4.263584613800049,
      "learning_rate": 4.878412657858238e-05,
      "loss": 0.7891,
      "step": 159900
    },
    {
      "epoch": 1.4599605810643113,
      "grad_norm": 4.697996616363525,
      "learning_rate": 4.878336618244641e-05,
      "loss": 0.7893,
      "step": 160000
    },
    {
      "epoch": 1.4608730564274763,
      "grad_norm": 3.9987313747406006,
      "learning_rate": 4.878260578631044e-05,
      "loss": 0.8103,
      "step": 160100
    },
    {
      "epoch": 1.4617855317906416,
      "grad_norm": 4.1427001953125,
      "learning_rate": 4.878184539017447e-05,
      "loss": 0.7586,
      "step": 160200
    },
    {
      "epoch": 1.462698007153807,
      "grad_norm": 3.855238676071167,
      "learning_rate": 4.878108499403849e-05,
      "loss": 0.7665,
      "step": 160300
    },
    {
      "epoch": 1.463610482516972,
      "grad_norm": 3.6567916870117188,
      "learning_rate": 4.878032459790253e-05,
      "loss": 0.7604,
      "step": 160400
    },
    {
      "epoch": 1.4645229578801373,
      "grad_norm": 4.153293132781982,
      "learning_rate": 4.877956420176655e-05,
      "loss": 0.7989,
      "step": 160500
    },
    {
      "epoch": 1.4654354332433024,
      "grad_norm": 3.6193976402282715,
      "learning_rate": 4.877880380563058e-05,
      "loss": 0.7356,
      "step": 160600
    },
    {
      "epoch": 1.4663479086064677,
      "grad_norm": 4.739593505859375,
      "learning_rate": 4.877804340949461e-05,
      "loss": 0.7787,
      "step": 160700
    },
    {
      "epoch": 1.4672603839696328,
      "grad_norm": 4.776556968688965,
      "learning_rate": 4.877728301335864e-05,
      "loss": 0.7711,
      "step": 160800
    },
    {
      "epoch": 1.468172859332798,
      "grad_norm": 4.460022449493408,
      "learning_rate": 4.877652261722267e-05,
      "loss": 0.7823,
      "step": 160900
    },
    {
      "epoch": 1.4690853346959631,
      "grad_norm": 4.359416961669922,
      "learning_rate": 4.87757622210867e-05,
      "loss": 0.7655,
      "step": 161000
    },
    {
      "epoch": 1.4699978100591284,
      "grad_norm": 4.9521989822387695,
      "learning_rate": 4.8775001824950726e-05,
      "loss": 0.8189,
      "step": 161100
    },
    {
      "epoch": 1.4709102854222935,
      "grad_norm": 3.874842882156372,
      "learning_rate": 4.877424142881476e-05,
      "loss": 0.7754,
      "step": 161200
    },
    {
      "epoch": 1.4718227607854588,
      "grad_norm": 2.6105728149414062,
      "learning_rate": 4.8773481032678786e-05,
      "loss": 0.7993,
      "step": 161300
    },
    {
      "epoch": 1.472735236148624,
      "grad_norm": 4.2694926261901855,
      "learning_rate": 4.8772720636542816e-05,
      "loss": 0.7339,
      "step": 161400
    },
    {
      "epoch": 1.4736477115117892,
      "grad_norm": 4.504145622253418,
      "learning_rate": 4.8771960240406847e-05,
      "loss": 0.8159,
      "step": 161500
    },
    {
      "epoch": 1.4745601868749545,
      "grad_norm": 4.200126647949219,
      "learning_rate": 4.877119984427088e-05,
      "loss": 0.7869,
      "step": 161600
    },
    {
      "epoch": 1.4754726622381196,
      "grad_norm": 3.905176877975464,
      "learning_rate": 4.87704394481349e-05,
      "loss": 0.7814,
      "step": 161700
    },
    {
      "epoch": 1.4763851376012846,
      "grad_norm": 4.908121109008789,
      "learning_rate": 4.876967905199894e-05,
      "loss": 0.8142,
      "step": 161800
    },
    {
      "epoch": 1.47729761296445,
      "grad_norm": 4.231546401977539,
      "learning_rate": 4.876891865586296e-05,
      "loss": 0.78,
      "step": 161900
    },
    {
      "epoch": 1.4782100883276152,
      "grad_norm": 4.172248363494873,
      "learning_rate": 4.876815825972699e-05,
      "loss": 0.7589,
      "step": 162000
    },
    {
      "epoch": 1.4791225636907803,
      "grad_norm": 4.416748046875,
      "learning_rate": 4.876739786359102e-05,
      "loss": 0.7291,
      "step": 162100
    },
    {
      "epoch": 1.4800350390539456,
      "grad_norm": 3.9764578342437744,
      "learning_rate": 4.8766637467455043e-05,
      "loss": 0.7961,
      "step": 162200
    },
    {
      "epoch": 1.4809475144171107,
      "grad_norm": 4.947453022003174,
      "learning_rate": 4.876587707131908e-05,
      "loss": 0.7453,
      "step": 162300
    },
    {
      "epoch": 1.481859989780276,
      "grad_norm": 3.3750264644622803,
      "learning_rate": 4.8765116675183104e-05,
      "loss": 0.7866,
      "step": 162400
    },
    {
      "epoch": 1.482772465143441,
      "grad_norm": 4.17131233215332,
      "learning_rate": 4.8764356279047134e-05,
      "loss": 0.7511,
      "step": 162500
    },
    {
      "epoch": 1.4836849405066064,
      "grad_norm": 4.8659844398498535,
      "learning_rate": 4.8763595882911164e-05,
      "loss": 0.7893,
      "step": 162600
    },
    {
      "epoch": 1.4845974158697715,
      "grad_norm": 4.2721076011657715,
      "learning_rate": 4.8762835486775194e-05,
      "loss": 0.7567,
      "step": 162700
    },
    {
      "epoch": 1.4855098912329368,
      "grad_norm": 4.120404243469238,
      "learning_rate": 4.876207509063922e-05,
      "loss": 0.7856,
      "step": 162800
    },
    {
      "epoch": 1.4864223665961018,
      "grad_norm": 3.7165164947509766,
      "learning_rate": 4.8761314694503254e-05,
      "loss": 0.777,
      "step": 162900
    },
    {
      "epoch": 1.4873348419592671,
      "grad_norm": 3.937544822692871,
      "learning_rate": 4.876055429836728e-05,
      "loss": 0.7645,
      "step": 163000
    },
    {
      "epoch": 1.4882473173224322,
      "grad_norm": 4.291866302490234,
      "learning_rate": 4.875979390223131e-05,
      "loss": 0.798,
      "step": 163100
    },
    {
      "epoch": 1.4891597926855975,
      "grad_norm": 4.408498287200928,
      "learning_rate": 4.875903350609534e-05,
      "loss": 0.7533,
      "step": 163200
    },
    {
      "epoch": 1.4900722680487628,
      "grad_norm": 3.9166383743286133,
      "learning_rate": 4.875827310995937e-05,
      "loss": 0.7681,
      "step": 163300
    },
    {
      "epoch": 1.4909847434119279,
      "grad_norm": 3.8578531742095947,
      "learning_rate": 4.87575127138234e-05,
      "loss": 0.7834,
      "step": 163400
    },
    {
      "epoch": 1.491897218775093,
      "grad_norm": 4.605504989624023,
      "learning_rate": 4.875675231768743e-05,
      "loss": 0.8251,
      "step": 163500
    },
    {
      "epoch": 1.4928096941382583,
      "grad_norm": 4.831454277038574,
      "learning_rate": 4.875599192155145e-05,
      "loss": 0.7692,
      "step": 163600
    },
    {
      "epoch": 1.4937221695014236,
      "grad_norm": 3.974107503890991,
      "learning_rate": 4.875523152541549e-05,
      "loss": 0.7761,
      "step": 163700
    },
    {
      "epoch": 1.4946346448645886,
      "grad_norm": 5.5999956130981445,
      "learning_rate": 4.875447112927951e-05,
      "loss": 0.7604,
      "step": 163800
    },
    {
      "epoch": 1.4955471202277537,
      "grad_norm": 4.690223217010498,
      "learning_rate": 4.875371073314354e-05,
      "loss": 0.7646,
      "step": 163900
    },
    {
      "epoch": 1.496459595590919,
      "grad_norm": 3.95107364654541,
      "learning_rate": 4.875295033700757e-05,
      "loss": 0.7554,
      "step": 164000
    },
    {
      "epoch": 1.4973720709540843,
      "grad_norm": 4.562201023101807,
      "learning_rate": 4.87521899408716e-05,
      "loss": 0.7652,
      "step": 164100
    },
    {
      "epoch": 1.4982845463172494,
      "grad_norm": 4.590007305145264,
      "learning_rate": 4.8751429544735624e-05,
      "loss": 0.8115,
      "step": 164200
    },
    {
      "epoch": 1.4991970216804147,
      "grad_norm": 4.191768169403076,
      "learning_rate": 4.875066914859966e-05,
      "loss": 0.7738,
      "step": 164300
    },
    {
      "epoch": 1.5001094970435798,
      "grad_norm": 3.8362529277801514,
      "learning_rate": 4.8749908752463685e-05,
      "loss": 0.7963,
      "step": 164400
    },
    {
      "epoch": 1.501021972406745,
      "grad_norm": 3.6391797065734863,
      "learning_rate": 4.8749148356327715e-05,
      "loss": 0.7504,
      "step": 164500
    },
    {
      "epoch": 1.5019344477699104,
      "grad_norm": 4.29632568359375,
      "learning_rate": 4.8748387960191745e-05,
      "loss": 0.7945,
      "step": 164600
    },
    {
      "epoch": 1.5028469231330754,
      "grad_norm": 4.763289451599121,
      "learning_rate": 4.8747627564055775e-05,
      "loss": 0.7921,
      "step": 164700
    },
    {
      "epoch": 1.5037593984962405,
      "grad_norm": 3.670179605484009,
      "learning_rate": 4.8746867167919805e-05,
      "loss": 0.7966,
      "step": 164800
    },
    {
      "epoch": 1.5046718738594058,
      "grad_norm": 5.065375804901123,
      "learning_rate": 4.874610677178383e-05,
      "loss": 0.8253,
      "step": 164900
    },
    {
      "epoch": 1.5055843492225711,
      "grad_norm": 4.909914016723633,
      "learning_rate": 4.874534637564786e-05,
      "loss": 0.7742,
      "step": 165000
    },
    {
      "epoch": 1.5064968245857362,
      "grad_norm": 4.2248382568359375,
      "learning_rate": 4.874458597951189e-05,
      "loss": 0.7757,
      "step": 165100
    },
    {
      "epoch": 1.5074092999489013,
      "grad_norm": 3.639842987060547,
      "learning_rate": 4.874382558337592e-05,
      "loss": 0.7427,
      "step": 165200
    },
    {
      "epoch": 1.5083217753120666,
      "grad_norm": 5.437606334686279,
      "learning_rate": 4.874306518723994e-05,
      "loss": 0.8385,
      "step": 165300
    },
    {
      "epoch": 1.5092342506752319,
      "grad_norm": 3.847785234451294,
      "learning_rate": 4.874230479110398e-05,
      "loss": 0.7934,
      "step": 165400
    },
    {
      "epoch": 1.510146726038397,
      "grad_norm": 4.147586345672607,
      "learning_rate": 4.8741544394968e-05,
      "loss": 0.7804,
      "step": 165500
    },
    {
      "epoch": 1.511059201401562,
      "grad_norm": 4.296707630157471,
      "learning_rate": 4.874078399883203e-05,
      "loss": 0.8001,
      "step": 165600
    },
    {
      "epoch": 1.5119716767647273,
      "grad_norm": 4.192218780517578,
      "learning_rate": 4.874002360269606e-05,
      "loss": 0.7924,
      "step": 165700
    },
    {
      "epoch": 1.5128841521278926,
      "grad_norm": 4.084393501281738,
      "learning_rate": 4.873926320656009e-05,
      "loss": 0.7819,
      "step": 165800
    },
    {
      "epoch": 1.5137966274910577,
      "grad_norm": 3.8163747787475586,
      "learning_rate": 4.873850281042412e-05,
      "loss": 0.7584,
      "step": 165900
    },
    {
      "epoch": 1.5147091028542228,
      "grad_norm": 4.3763885498046875,
      "learning_rate": 4.873774241428815e-05,
      "loss": 0.7809,
      "step": 166000
    },
    {
      "epoch": 1.515621578217388,
      "grad_norm": 4.837245941162109,
      "learning_rate": 4.8736982018152175e-05,
      "loss": 0.7582,
      "step": 166100
    },
    {
      "epoch": 1.5165340535805534,
      "grad_norm": 4.702744483947754,
      "learning_rate": 4.873622162201621e-05,
      "loss": 0.7678,
      "step": 166200
    },
    {
      "epoch": 1.5174465289437187,
      "grad_norm": 4.203624248504639,
      "learning_rate": 4.8735461225880236e-05,
      "loss": 0.7275,
      "step": 166300
    },
    {
      "epoch": 1.5183590043068838,
      "grad_norm": 4.309709072113037,
      "learning_rate": 4.8734700829744266e-05,
      "loss": 0.7596,
      "step": 166400
    },
    {
      "epoch": 1.5192714796700488,
      "grad_norm": 4.669897556304932,
      "learning_rate": 4.8733940433608296e-05,
      "loss": 0.8124,
      "step": 166500
    },
    {
      "epoch": 1.5201839550332141,
      "grad_norm": 4.085377216339111,
      "learning_rate": 4.8733180037472326e-05,
      "loss": 0.7257,
      "step": 166600
    },
    {
      "epoch": 1.5210964303963794,
      "grad_norm": 3.699211359024048,
      "learning_rate": 4.873241964133635e-05,
      "loss": 0.7794,
      "step": 166700
    },
    {
      "epoch": 1.5220089057595445,
      "grad_norm": 4.587733268737793,
      "learning_rate": 4.8731659245200386e-05,
      "loss": 0.7681,
      "step": 166800
    },
    {
      "epoch": 1.5229213811227096,
      "grad_norm": 3.625201463699341,
      "learning_rate": 4.873089884906441e-05,
      "loss": 0.7796,
      "step": 166900
    },
    {
      "epoch": 1.5238338564858749,
      "grad_norm": 4.205513954162598,
      "learning_rate": 4.873013845292844e-05,
      "loss": 0.8083,
      "step": 167000
    },
    {
      "epoch": 1.5247463318490402,
      "grad_norm": 4.374755859375,
      "learning_rate": 4.872937805679247e-05,
      "loss": 0.7893,
      "step": 167100
    },
    {
      "epoch": 1.5256588072122053,
      "grad_norm": 4.925912857055664,
      "learning_rate": 4.87286176606565e-05,
      "loss": 0.7653,
      "step": 167200
    },
    {
      "epoch": 1.5265712825753703,
      "grad_norm": 4.767482280731201,
      "learning_rate": 4.872785726452053e-05,
      "loss": 0.7965,
      "step": 167300
    },
    {
      "epoch": 1.5274837579385356,
      "grad_norm": 4.95378303527832,
      "learning_rate": 4.872709686838456e-05,
      "loss": 0.7659,
      "step": 167400
    },
    {
      "epoch": 1.528396233301701,
      "grad_norm": 4.429091930389404,
      "learning_rate": 4.872633647224858e-05,
      "loss": 0.7906,
      "step": 167500
    },
    {
      "epoch": 1.529308708664866,
      "grad_norm": 3.79144024848938,
      "learning_rate": 4.872557607611261e-05,
      "loss": 0.7692,
      "step": 167600
    },
    {
      "epoch": 1.530221184028031,
      "grad_norm": 4.102790355682373,
      "learning_rate": 4.872481567997664e-05,
      "loss": 0.7868,
      "step": 167700
    },
    {
      "epoch": 1.5311336593911964,
      "grad_norm": 4.143951892852783,
      "learning_rate": 4.8724055283840666e-05,
      "loss": 0.7827,
      "step": 167800
    },
    {
      "epoch": 1.5320461347543617,
      "grad_norm": 5.730300426483154,
      "learning_rate": 4.87232948877047e-05,
      "loss": 0.8251,
      "step": 167900
    },
    {
      "epoch": 1.532958610117527,
      "grad_norm": 6.017107009887695,
      "learning_rate": 4.8722534491568726e-05,
      "loss": 0.7819,
      "step": 168000
    },
    {
      "epoch": 1.533871085480692,
      "grad_norm": 5.039291858673096,
      "learning_rate": 4.8721774095432756e-05,
      "loss": 0.7972,
      "step": 168100
    },
    {
      "epoch": 1.5347835608438571,
      "grad_norm": 4.321450710296631,
      "learning_rate": 4.8721013699296787e-05,
      "loss": 0.7819,
      "step": 168200
    },
    {
      "epoch": 1.5356960362070224,
      "grad_norm": 4.489201068878174,
      "learning_rate": 4.8720253303160817e-05,
      "loss": 0.7898,
      "step": 168300
    },
    {
      "epoch": 1.5366085115701877,
      "grad_norm": 4.42021369934082,
      "learning_rate": 4.871949290702485e-05,
      "loss": 0.7799,
      "step": 168400
    },
    {
      "epoch": 1.5375209869333528,
      "grad_norm": 4.78828239440918,
      "learning_rate": 4.871873251088888e-05,
      "loss": 0.7613,
      "step": 168500
    },
    {
      "epoch": 1.538433462296518,
      "grad_norm": 4.821408271789551,
      "learning_rate": 4.87179721147529e-05,
      "loss": 0.7698,
      "step": 168600
    },
    {
      "epoch": 1.5393459376596832,
      "grad_norm": 4.525170803070068,
      "learning_rate": 4.871721171861694e-05,
      "loss": 0.8031,
      "step": 168700
    },
    {
      "epoch": 1.5402584130228485,
      "grad_norm": 4.573408603668213,
      "learning_rate": 4.871645132248096e-05,
      "loss": 0.7959,
      "step": 168800
    },
    {
      "epoch": 1.5411708883860136,
      "grad_norm": 3.1946566104888916,
      "learning_rate": 4.871569092634499e-05,
      "loss": 0.7329,
      "step": 168900
    },
    {
      "epoch": 1.5420833637491786,
      "grad_norm": 4.360457420349121,
      "learning_rate": 4.871493053020902e-05,
      "loss": 0.8085,
      "step": 169000
    },
    {
      "epoch": 1.542995839112344,
      "grad_norm": 3.3444395065307617,
      "learning_rate": 4.871417013407305e-05,
      "loss": 0.7689,
      "step": 169100
    },
    {
      "epoch": 1.5439083144755092,
      "grad_norm": 4.85498571395874,
      "learning_rate": 4.8713409737937074e-05,
      "loss": 0.8056,
      "step": 169200
    },
    {
      "epoch": 1.5448207898386743,
      "grad_norm": 4.243717670440674,
      "learning_rate": 4.871264934180111e-05,
      "loss": 0.7961,
      "step": 169300
    },
    {
      "epoch": 1.5457332652018394,
      "grad_norm": 4.391078472137451,
      "learning_rate": 4.8711888945665134e-05,
      "loss": 0.83,
      "step": 169400
    },
    {
      "epoch": 1.5466457405650047,
      "grad_norm": 4.327888488769531,
      "learning_rate": 4.8711128549529164e-05,
      "loss": 0.8061,
      "step": 169500
    },
    {
      "epoch": 1.54755821592817,
      "grad_norm": 3.7305490970611572,
      "learning_rate": 4.8710368153393194e-05,
      "loss": 0.7696,
      "step": 169600
    },
    {
      "epoch": 1.5484706912913353,
      "grad_norm": 5.212944030761719,
      "learning_rate": 4.8709607757257224e-05,
      "loss": 0.763,
      "step": 169700
    },
    {
      "epoch": 1.5493831666545004,
      "grad_norm": 5.0500617027282715,
      "learning_rate": 4.8708847361121254e-05,
      "loss": 0.817,
      "step": 169800
    },
    {
      "epoch": 1.5502956420176655,
      "grad_norm": 4.404504299163818,
      "learning_rate": 4.8708086964985284e-05,
      "loss": 0.8273,
      "step": 169900
    },
    {
      "epoch": 1.5512081173808308,
      "grad_norm": 4.302989959716797,
      "learning_rate": 4.870732656884931e-05,
      "loss": 0.7464,
      "step": 170000
    },
    {
      "epoch": 1.552120592743996,
      "grad_norm": 4.157317161560059,
      "learning_rate": 4.8706566172713344e-05,
      "loss": 0.7969,
      "step": 170100
    },
    {
      "epoch": 1.5530330681071611,
      "grad_norm": 4.748043060302734,
      "learning_rate": 4.870580577657737e-05,
      "loss": 0.7604,
      "step": 170200
    },
    {
      "epoch": 1.5539455434703262,
      "grad_norm": 4.344515800476074,
      "learning_rate": 4.87050453804414e-05,
      "loss": 0.7818,
      "step": 170300
    },
    {
      "epoch": 1.5548580188334915,
      "grad_norm": 4.324347972869873,
      "learning_rate": 4.870428498430543e-05,
      "loss": 0.8098,
      "step": 170400
    },
    {
      "epoch": 1.5557704941966568,
      "grad_norm": 4.063113212585449,
      "learning_rate": 4.870352458816945e-05,
      "loss": 0.8103,
      "step": 170500
    },
    {
      "epoch": 1.5566829695598219,
      "grad_norm": 4.949711322784424,
      "learning_rate": 4.870276419203348e-05,
      "loss": 0.7293,
      "step": 170600
    },
    {
      "epoch": 1.557595444922987,
      "grad_norm": 4.281140327453613,
      "learning_rate": 4.870200379589751e-05,
      "loss": 0.7689,
      "step": 170700
    },
    {
      "epoch": 1.5585079202861523,
      "grad_norm": 4.144218921661377,
      "learning_rate": 4.870124339976154e-05,
      "loss": 0.7913,
      "step": 170800
    },
    {
      "epoch": 1.5594203956493176,
      "grad_norm": 4.335687637329102,
      "learning_rate": 4.870048300362557e-05,
      "loss": 0.7846,
      "step": 170900
    },
    {
      "epoch": 1.5603328710124826,
      "grad_norm": 3.819957971572876,
      "learning_rate": 4.86997226074896e-05,
      "loss": 0.7775,
      "step": 171000
    },
    {
      "epoch": 1.5612453463756477,
      "grad_norm": 3.791224956512451,
      "learning_rate": 4.8698962211353625e-05,
      "loss": 0.8113,
      "step": 171100
    },
    {
      "epoch": 1.562157821738813,
      "grad_norm": 2.7187118530273438,
      "learning_rate": 4.869820181521766e-05,
      "loss": 0.769,
      "step": 171200
    },
    {
      "epoch": 1.5630702971019783,
      "grad_norm": 4.8502984046936035,
      "learning_rate": 4.8697441419081685e-05,
      "loss": 0.7953,
      "step": 171300
    },
    {
      "epoch": 1.5639827724651434,
      "grad_norm": 4.957902908325195,
      "learning_rate": 4.8696681022945715e-05,
      "loss": 0.8062,
      "step": 171400
    },
    {
      "epoch": 1.5648952478283087,
      "grad_norm": 3.8339121341705322,
      "learning_rate": 4.8695920626809745e-05,
      "loss": 0.7799,
      "step": 171500
    },
    {
      "epoch": 1.5658077231914738,
      "grad_norm": 4.643214702606201,
      "learning_rate": 4.8695160230673775e-05,
      "loss": 0.757,
      "step": 171600
    },
    {
      "epoch": 1.566720198554639,
      "grad_norm": 4.690615653991699,
      "learning_rate": 4.8694399834537805e-05,
      "loss": 0.7572,
      "step": 171700
    },
    {
      "epoch": 1.5676326739178044,
      "grad_norm": 3.7306923866271973,
      "learning_rate": 4.8693639438401835e-05,
      "loss": 0.7737,
      "step": 171800
    },
    {
      "epoch": 1.5685451492809694,
      "grad_norm": 5.071478366851807,
      "learning_rate": 4.869287904226586e-05,
      "loss": 0.7766,
      "step": 171900
    },
    {
      "epoch": 1.5694576246441345,
      "grad_norm": 4.4041314125061035,
      "learning_rate": 4.869211864612989e-05,
      "loss": 0.7695,
      "step": 172000
    },
    {
      "epoch": 1.5703701000072998,
      "grad_norm": 4.266735076904297,
      "learning_rate": 4.869135824999392e-05,
      "loss": 0.7642,
      "step": 172100
    },
    {
      "epoch": 1.5712825753704651,
      "grad_norm": 4.5303168296813965,
      "learning_rate": 4.869059785385795e-05,
      "loss": 0.8104,
      "step": 172200
    },
    {
      "epoch": 1.5721950507336302,
      "grad_norm": 4.018672943115234,
      "learning_rate": 4.868983745772198e-05,
      "loss": 0.7445,
      "step": 172300
    },
    {
      "epoch": 1.5731075260967953,
      "grad_norm": 4.909472942352295,
      "learning_rate": 4.868907706158601e-05,
      "loss": 0.7937,
      "step": 172400
    },
    {
      "epoch": 1.5740200014599606,
      "grad_norm": 4.179308891296387,
      "learning_rate": 4.868831666545003e-05,
      "loss": 0.7933,
      "step": 172500
    },
    {
      "epoch": 1.5749324768231259,
      "grad_norm": 4.271818161010742,
      "learning_rate": 4.868755626931407e-05,
      "loss": 0.7956,
      "step": 172600
    },
    {
      "epoch": 1.575844952186291,
      "grad_norm": 3.8624932765960693,
      "learning_rate": 4.868679587317809e-05,
      "loss": 0.7476,
      "step": 172700
    },
    {
      "epoch": 1.576757427549456,
      "grad_norm": 3.884460687637329,
      "learning_rate": 4.868603547704212e-05,
      "loss": 0.7682,
      "step": 172800
    },
    {
      "epoch": 1.5776699029126213,
      "grad_norm": 3.290437936782837,
      "learning_rate": 4.868527508090615e-05,
      "loss": 0.8238,
      "step": 172900
    },
    {
      "epoch": 1.5785823782757866,
      "grad_norm": 5.855055809020996,
      "learning_rate": 4.868451468477018e-05,
      "loss": 0.7757,
      "step": 173000
    },
    {
      "epoch": 1.5794948536389517,
      "grad_norm": 4.770255088806152,
      "learning_rate": 4.868375428863421e-05,
      "loss": 0.7875,
      "step": 173100
    },
    {
      "epoch": 1.5804073290021168,
      "grad_norm": 4.870233058929443,
      "learning_rate": 4.868299389249824e-05,
      "loss": 0.813,
      "step": 173200
    },
    {
      "epoch": 1.581319804365282,
      "grad_norm": 4.063090801239014,
      "learning_rate": 4.8682233496362266e-05,
      "loss": 0.7765,
      "step": 173300
    },
    {
      "epoch": 1.5822322797284474,
      "grad_norm": 4.303163528442383,
      "learning_rate": 4.8681473100226296e-05,
      "loss": 0.8091,
      "step": 173400
    },
    {
      "epoch": 1.5831447550916127,
      "grad_norm": 4.111211776733398,
      "learning_rate": 4.8680712704090326e-05,
      "loss": 0.7643,
      "step": 173500
    },
    {
      "epoch": 1.5840572304547778,
      "grad_norm": 4.300267696380615,
      "learning_rate": 4.867995230795435e-05,
      "loss": 0.7919,
      "step": 173600
    },
    {
      "epoch": 1.5849697058179428,
      "grad_norm": 4.014343738555908,
      "learning_rate": 4.8679191911818386e-05,
      "loss": 0.782,
      "step": 173700
    },
    {
      "epoch": 1.5858821811811081,
      "grad_norm": 4.826057434082031,
      "learning_rate": 4.867843151568241e-05,
      "loss": 0.8194,
      "step": 173800
    },
    {
      "epoch": 1.5867946565442734,
      "grad_norm": 3.469198703765869,
      "learning_rate": 4.867767111954644e-05,
      "loss": 0.7809,
      "step": 173900
    },
    {
      "epoch": 1.5877071319074385,
      "grad_norm": 4.044656276702881,
      "learning_rate": 4.867691072341047e-05,
      "loss": 0.7665,
      "step": 174000
    },
    {
      "epoch": 1.5886196072706036,
      "grad_norm": 5.249587535858154,
      "learning_rate": 4.86761503272745e-05,
      "loss": 0.7661,
      "step": 174100
    },
    {
      "epoch": 1.5895320826337689,
      "grad_norm": 4.577809810638428,
      "learning_rate": 4.867538993113853e-05,
      "loss": 0.8044,
      "step": 174200
    },
    {
      "epoch": 1.5904445579969342,
      "grad_norm": 4.020084857940674,
      "learning_rate": 4.867462953500256e-05,
      "loss": 0.7499,
      "step": 174300
    },
    {
      "epoch": 1.5913570333600993,
      "grad_norm": 3.3597524166107178,
      "learning_rate": 4.867386913886658e-05,
      "loss": 0.7694,
      "step": 174400
    },
    {
      "epoch": 1.5922695087232643,
      "grad_norm": 4.278686046600342,
      "learning_rate": 4.867310874273062e-05,
      "loss": 0.7626,
      "step": 174500
    },
    {
      "epoch": 1.5931819840864296,
      "grad_norm": 3.5081186294555664,
      "learning_rate": 4.867234834659464e-05,
      "loss": 0.7901,
      "step": 174600
    },
    {
      "epoch": 1.594094459449595,
      "grad_norm": 4.317670822143555,
      "learning_rate": 4.867158795045867e-05,
      "loss": 0.7536,
      "step": 174700
    },
    {
      "epoch": 1.59500693481276,
      "grad_norm": 4.025084972381592,
      "learning_rate": 4.86708275543227e-05,
      "loss": 0.7839,
      "step": 174800
    },
    {
      "epoch": 1.595919410175925,
      "grad_norm": 4.1290106773376465,
      "learning_rate": 4.867006715818673e-05,
      "loss": 0.7349,
      "step": 174900
    },
    {
      "epoch": 1.5968318855390904,
      "grad_norm": 4.037984848022461,
      "learning_rate": 4.8669306762050757e-05,
      "loss": 0.7874,
      "step": 175000
    },
    {
      "epoch": 1.5977443609022557,
      "grad_norm": 3.785217046737671,
      "learning_rate": 4.8668546365914793e-05,
      "loss": 0.7904,
      "step": 175100
    },
    {
      "epoch": 1.598656836265421,
      "grad_norm": 3.698913097381592,
      "learning_rate": 4.866778596977882e-05,
      "loss": 0.7712,
      "step": 175200
    },
    {
      "epoch": 1.599569311628586,
      "grad_norm": 4.164270401000977,
      "learning_rate": 4.866702557364285e-05,
      "loss": 0.7768,
      "step": 175300
    },
    {
      "epoch": 1.6004817869917511,
      "grad_norm": 4.014329433441162,
      "learning_rate": 4.866626517750688e-05,
      "loss": 0.7613,
      "step": 175400
    },
    {
      "epoch": 1.6013942623549164,
      "grad_norm": 3.627009868621826,
      "learning_rate": 4.866550478137091e-05,
      "loss": 0.7746,
      "step": 175500
    },
    {
      "epoch": 1.6023067377180817,
      "grad_norm": 3.480875015258789,
      "learning_rate": 4.866474438523494e-05,
      "loss": 0.7967,
      "step": 175600
    },
    {
      "epoch": 1.6032192130812468,
      "grad_norm": 4.0238118171691895,
      "learning_rate": 4.866398398909897e-05,
      "loss": 0.7194,
      "step": 175700
    },
    {
      "epoch": 1.604131688444412,
      "grad_norm": 4.351922988891602,
      "learning_rate": 4.866322359296299e-05,
      "loss": 0.7633,
      "step": 175800
    },
    {
      "epoch": 1.6050441638075772,
      "grad_norm": 5.235324382781982,
      "learning_rate": 4.866246319682703e-05,
      "loss": 0.7728,
      "step": 175900
    },
    {
      "epoch": 1.6059566391707425,
      "grad_norm": 4.255066394805908,
      "learning_rate": 4.866170280069105e-05,
      "loss": 0.8206,
      "step": 176000
    },
    {
      "epoch": 1.6068691145339076,
      "grad_norm": 4.4936089515686035,
      "learning_rate": 4.8660942404555074e-05,
      "loss": 0.784,
      "step": 176100
    },
    {
      "epoch": 1.6077815898970726,
      "grad_norm": 4.342044353485107,
      "learning_rate": 4.866018200841911e-05,
      "loss": 0.7802,
      "step": 176200
    },
    {
      "epoch": 1.608694065260238,
      "grad_norm": 3.9527769088745117,
      "learning_rate": 4.8659421612283134e-05,
      "loss": 0.7643,
      "step": 176300
    },
    {
      "epoch": 1.6096065406234032,
      "grad_norm": 3.876397132873535,
      "learning_rate": 4.8658661216147164e-05,
      "loss": 0.7605,
      "step": 176400
    },
    {
      "epoch": 1.6105190159865683,
      "grad_norm": 4.270368576049805,
      "learning_rate": 4.8657900820011194e-05,
      "loss": 0.7667,
      "step": 176500
    },
    {
      "epoch": 1.6114314913497334,
      "grad_norm": 4.872711658477783,
      "learning_rate": 4.8657140423875224e-05,
      "loss": 0.8315,
      "step": 176600
    },
    {
      "epoch": 1.6123439667128987,
      "grad_norm": 4.096817493438721,
      "learning_rate": 4.8656380027739254e-05,
      "loss": 0.8248,
      "step": 176700
    },
    {
      "epoch": 1.613256442076064,
      "grad_norm": 4.468338489532471,
      "learning_rate": 4.8655619631603284e-05,
      "loss": 0.7791,
      "step": 176800
    },
    {
      "epoch": 1.6141689174392293,
      "grad_norm": 5.450893878936768,
      "learning_rate": 4.865485923546731e-05,
      "loss": 0.7818,
      "step": 176900
    },
    {
      "epoch": 1.6150813928023944,
      "grad_norm": 4.7396063804626465,
      "learning_rate": 4.8654098839331344e-05,
      "loss": 0.7745,
      "step": 177000
    },
    {
      "epoch": 1.6159938681655595,
      "grad_norm": 3.637615919113159,
      "learning_rate": 4.865333844319537e-05,
      "loss": 0.722,
      "step": 177100
    },
    {
      "epoch": 1.6169063435287248,
      "grad_norm": 4.542927265167236,
      "learning_rate": 4.86525780470594e-05,
      "loss": 0.7558,
      "step": 177200
    },
    {
      "epoch": 1.61781881889189,
      "grad_norm": 4.680438995361328,
      "learning_rate": 4.865181765092343e-05,
      "loss": 0.8009,
      "step": 177300
    },
    {
      "epoch": 1.6187312942550551,
      "grad_norm": 4.966607570648193,
      "learning_rate": 4.865105725478746e-05,
      "loss": 0.7709,
      "step": 177400
    },
    {
      "epoch": 1.6196437696182202,
      "grad_norm": 4.172004222869873,
      "learning_rate": 4.865029685865148e-05,
      "loss": 0.7882,
      "step": 177500
    },
    {
      "epoch": 1.6205562449813855,
      "grad_norm": 3.4508585929870605,
      "learning_rate": 4.864953646251552e-05,
      "loss": 0.7372,
      "step": 177600
    },
    {
      "epoch": 1.6214687203445508,
      "grad_norm": 3.88975191116333,
      "learning_rate": 4.864877606637954e-05,
      "loss": 0.7742,
      "step": 177700
    },
    {
      "epoch": 1.6223811957077159,
      "grad_norm": 3.707305908203125,
      "learning_rate": 4.864801567024357e-05,
      "loss": 0.7456,
      "step": 177800
    },
    {
      "epoch": 1.623293671070881,
      "grad_norm": 3.3797500133514404,
      "learning_rate": 4.86472552741076e-05,
      "loss": 0.7942,
      "step": 177900
    },
    {
      "epoch": 1.6242061464340463,
      "grad_norm": 4.214262962341309,
      "learning_rate": 4.864649487797163e-05,
      "loss": 0.7769,
      "step": 178000
    },
    {
      "epoch": 1.6251186217972116,
      "grad_norm": 3.0772759914398193,
      "learning_rate": 4.864573448183566e-05,
      "loss": 0.7598,
      "step": 178100
    },
    {
      "epoch": 1.6260310971603766,
      "grad_norm": 4.47250509262085,
      "learning_rate": 4.864497408569969e-05,
      "loss": 0.7383,
      "step": 178200
    },
    {
      "epoch": 1.6269435725235417,
      "grad_norm": 4.640848636627197,
      "learning_rate": 4.8644213689563715e-05,
      "loss": 0.7757,
      "step": 178300
    },
    {
      "epoch": 1.627856047886707,
      "grad_norm": 3.8420190811157227,
      "learning_rate": 4.864345329342775e-05,
      "loss": 0.759,
      "step": 178400
    },
    {
      "epoch": 1.6287685232498723,
      "grad_norm": 4.364875793457031,
      "learning_rate": 4.8642692897291775e-05,
      "loss": 0.7917,
      "step": 178500
    },
    {
      "epoch": 1.6296809986130376,
      "grad_norm": 4.399286270141602,
      "learning_rate": 4.8641932501155805e-05,
      "loss": 0.7962,
      "step": 178600
    },
    {
      "epoch": 1.6305934739762027,
      "grad_norm": 3.106149435043335,
      "learning_rate": 4.8641172105019835e-05,
      "loss": 0.7781,
      "step": 178700
    },
    {
      "epoch": 1.6315059493393678,
      "grad_norm": 3.339730739593506,
      "learning_rate": 4.8640411708883865e-05,
      "loss": 0.7788,
      "step": 178800
    },
    {
      "epoch": 1.632418424702533,
      "grad_norm": 4.518789768218994,
      "learning_rate": 4.863965131274789e-05,
      "loss": 0.7675,
      "step": 178900
    },
    {
      "epoch": 1.6333309000656984,
      "grad_norm": 4.739353179931641,
      "learning_rate": 4.863889091661192e-05,
      "loss": 0.7822,
      "step": 179000
    },
    {
      "epoch": 1.6342433754288634,
      "grad_norm": 3.2505550384521484,
      "learning_rate": 4.863813052047595e-05,
      "loss": 0.7782,
      "step": 179100
    },
    {
      "epoch": 1.6351558507920285,
      "grad_norm": 4.300563812255859,
      "learning_rate": 4.863737012433998e-05,
      "loss": 0.7301,
      "step": 179200
    },
    {
      "epoch": 1.6360683261551938,
      "grad_norm": 4.239462852478027,
      "learning_rate": 4.863660972820401e-05,
      "loss": 0.7762,
      "step": 179300
    },
    {
      "epoch": 1.6369808015183591,
      "grad_norm": 3.900296926498413,
      "learning_rate": 4.863584933206803e-05,
      "loss": 0.8028,
      "step": 179400
    },
    {
      "epoch": 1.6378932768815242,
      "grad_norm": 3.768397331237793,
      "learning_rate": 4.863508893593207e-05,
      "loss": 0.7308,
      "step": 179500
    },
    {
      "epoch": 1.6388057522446893,
      "grad_norm": 4.191596031188965,
      "learning_rate": 4.863432853979609e-05,
      "loss": 0.7345,
      "step": 179600
    },
    {
      "epoch": 1.6397182276078546,
      "grad_norm": 4.57833194732666,
      "learning_rate": 4.863356814366012e-05,
      "loss": 0.7886,
      "step": 179700
    },
    {
      "epoch": 1.6406307029710199,
      "grad_norm": 3.5056517124176025,
      "learning_rate": 4.863280774752415e-05,
      "loss": 0.7952,
      "step": 179800
    },
    {
      "epoch": 1.641543178334185,
      "grad_norm": 3.5463876724243164,
      "learning_rate": 4.863204735138818e-05,
      "loss": 0.7715,
      "step": 179900
    },
    {
      "epoch": 1.64245565369735,
      "grad_norm": 3.954561471939087,
      "learning_rate": 4.8631286955252206e-05,
      "loss": 0.7897,
      "step": 180000
    },
    {
      "epoch": 1.6433681290605153,
      "grad_norm": 4.152578353881836,
      "learning_rate": 4.863052655911624e-05,
      "loss": 0.7513,
      "step": 180100
    },
    {
      "epoch": 1.6442806044236806,
      "grad_norm": 3.904780626296997,
      "learning_rate": 4.8629766162980266e-05,
      "loss": 0.7827,
      "step": 180200
    },
    {
      "epoch": 1.6451930797868457,
      "grad_norm": 3.911745548248291,
      "learning_rate": 4.8629005766844296e-05,
      "loss": 0.7433,
      "step": 180300
    },
    {
      "epoch": 1.646105555150011,
      "grad_norm": 4.27949333190918,
      "learning_rate": 4.8628245370708326e-05,
      "loss": 0.8007,
      "step": 180400
    },
    {
      "epoch": 1.647018030513176,
      "grad_norm": 4.659599781036377,
      "learning_rate": 4.8627484974572356e-05,
      "loss": 0.7982,
      "step": 180500
    },
    {
      "epoch": 1.6479305058763414,
      "grad_norm": 4.510316371917725,
      "learning_rate": 4.8626724578436386e-05,
      "loss": 0.7503,
      "step": 180600
    },
    {
      "epoch": 1.6488429812395067,
      "grad_norm": 4.95881986618042,
      "learning_rate": 4.8625964182300416e-05,
      "loss": 0.7727,
      "step": 180700
    },
    {
      "epoch": 1.6497554566026718,
      "grad_norm": 4.492867469787598,
      "learning_rate": 4.862520378616444e-05,
      "loss": 0.811,
      "step": 180800
    },
    {
      "epoch": 1.6506679319658368,
      "grad_norm": 4.9042768478393555,
      "learning_rate": 4.8624443390028476e-05,
      "loss": 0.782,
      "step": 180900
    },
    {
      "epoch": 1.6515804073290021,
      "grad_norm": 2.9453678131103516,
      "learning_rate": 4.86236829938925e-05,
      "loss": 0.7763,
      "step": 181000
    },
    {
      "epoch": 1.6524928826921674,
      "grad_norm": 4.772642612457275,
      "learning_rate": 4.862292259775653e-05,
      "loss": 0.823,
      "step": 181100
    },
    {
      "epoch": 1.6534053580553325,
      "grad_norm": 3.827670097351074,
      "learning_rate": 4.862216220162056e-05,
      "loss": 0.7468,
      "step": 181200
    },
    {
      "epoch": 1.6543178334184976,
      "grad_norm": 4.197908878326416,
      "learning_rate": 4.862140180548459e-05,
      "loss": 0.7376,
      "step": 181300
    },
    {
      "epoch": 1.6552303087816629,
      "grad_norm": 4.370494842529297,
      "learning_rate": 4.862064140934861e-05,
      "loss": 0.7303,
      "step": 181400
    },
    {
      "epoch": 1.6561427841448282,
      "grad_norm": 4.27587366104126,
      "learning_rate": 4.861988101321265e-05,
      "loss": 0.7748,
      "step": 181500
    },
    {
      "epoch": 1.6570552595079933,
      "grad_norm": 4.345036506652832,
      "learning_rate": 4.861912061707667e-05,
      "loss": 0.7878,
      "step": 181600
    },
    {
      "epoch": 1.6579677348711583,
      "grad_norm": 4.873326301574707,
      "learning_rate": 4.86183602209407e-05,
      "loss": 0.801,
      "step": 181700
    },
    {
      "epoch": 1.6588802102343236,
      "grad_norm": 3.608529567718506,
      "learning_rate": 4.861759982480473e-05,
      "loss": 0.8077,
      "step": 181800
    },
    {
      "epoch": 1.659792685597489,
      "grad_norm": 4.281148433685303,
      "learning_rate": 4.861683942866876e-05,
      "loss": 0.8065,
      "step": 181900
    },
    {
      "epoch": 1.660705160960654,
      "grad_norm": 5.230790138244629,
      "learning_rate": 4.8616079032532793e-05,
      "loss": 0.7668,
      "step": 182000
    },
    {
      "epoch": 1.661617636323819,
      "grad_norm": 5.617367744445801,
      "learning_rate": 4.861531863639682e-05,
      "loss": 0.7618,
      "step": 182100
    },
    {
      "epoch": 1.6625301116869844,
      "grad_norm": 3.364509344100952,
      "learning_rate": 4.861455824026085e-05,
      "loss": 0.7285,
      "step": 182200
    },
    {
      "epoch": 1.6634425870501497,
      "grad_norm": 4.344056129455566,
      "learning_rate": 4.861379784412488e-05,
      "loss": 0.7849,
      "step": 182300
    },
    {
      "epoch": 1.664355062413315,
      "grad_norm": 3.645261287689209,
      "learning_rate": 4.861303744798891e-05,
      "loss": 0.7872,
      "step": 182400
    },
    {
      "epoch": 1.66526753777648,
      "grad_norm": 4.784368515014648,
      "learning_rate": 4.861227705185293e-05,
      "loss": 0.7827,
      "step": 182500
    },
    {
      "epoch": 1.6661800131396451,
      "grad_norm": 4.703847408294678,
      "learning_rate": 4.861151665571697e-05,
      "loss": 0.7513,
      "step": 182600
    },
    {
      "epoch": 1.6670924885028104,
      "grad_norm": 4.828347682952881,
      "learning_rate": 4.861075625958099e-05,
      "loss": 0.7596,
      "step": 182700
    },
    {
      "epoch": 1.6680049638659757,
      "grad_norm": 5.811056137084961,
      "learning_rate": 4.860999586344502e-05,
      "loss": 0.7929,
      "step": 182800
    },
    {
      "epoch": 1.6689174392291408,
      "grad_norm": 4.265167713165283,
      "learning_rate": 4.860923546730905e-05,
      "loss": 0.7827,
      "step": 182900
    },
    {
      "epoch": 1.669829914592306,
      "grad_norm": 4.221785545349121,
      "learning_rate": 4.860847507117308e-05,
      "loss": 0.77,
      "step": 183000
    },
    {
      "epoch": 1.6707423899554712,
      "grad_norm": 3.4674081802368164,
      "learning_rate": 4.860771467503711e-05,
      "loss": 0.7504,
      "step": 183100
    },
    {
      "epoch": 1.6716548653186365,
      "grad_norm": 4.892886638641357,
      "learning_rate": 4.860695427890114e-05,
      "loss": 0.8108,
      "step": 183200
    },
    {
      "epoch": 1.6725673406818016,
      "grad_norm": 4.015299320220947,
      "learning_rate": 4.8606193882765164e-05,
      "loss": 0.8052,
      "step": 183300
    },
    {
      "epoch": 1.6734798160449667,
      "grad_norm": 4.5011396408081055,
      "learning_rate": 4.86054334866292e-05,
      "loss": 0.8062,
      "step": 183400
    },
    {
      "epoch": 1.674392291408132,
      "grad_norm": 4.001413345336914,
      "learning_rate": 4.8604673090493224e-05,
      "loss": 0.7281,
      "step": 183500
    },
    {
      "epoch": 1.6753047667712972,
      "grad_norm": 3.6031339168548584,
      "learning_rate": 4.8603912694357254e-05,
      "loss": 0.7806,
      "step": 183600
    },
    {
      "epoch": 1.6762172421344623,
      "grad_norm": 4.0488362312316895,
      "learning_rate": 4.8603152298221284e-05,
      "loss": 0.7746,
      "step": 183700
    },
    {
      "epoch": 1.6771297174976274,
      "grad_norm": 4.135134220123291,
      "learning_rate": 4.8602391902085314e-05,
      "loss": 0.7155,
      "step": 183800
    },
    {
      "epoch": 1.6780421928607927,
      "grad_norm": 3.724186658859253,
      "learning_rate": 4.860163150594934e-05,
      "loss": 0.7701,
      "step": 183900
    },
    {
      "epoch": 1.678954668223958,
      "grad_norm": 3.5028064250946045,
      "learning_rate": 4.8600871109813375e-05,
      "loss": 0.7514,
      "step": 184000
    },
    {
      "epoch": 1.6798671435871233,
      "grad_norm": 3.4924745559692383,
      "learning_rate": 4.86001107136774e-05,
      "loss": 0.7849,
      "step": 184100
    },
    {
      "epoch": 1.6807796189502884,
      "grad_norm": 5.188884258270264,
      "learning_rate": 4.859935031754143e-05,
      "loss": 0.7444,
      "step": 184200
    },
    {
      "epoch": 1.6816920943134535,
      "grad_norm": 5.228572368621826,
      "learning_rate": 4.859858992140546e-05,
      "loss": 0.8284,
      "step": 184300
    },
    {
      "epoch": 1.6826045696766188,
      "grad_norm": 3.765040636062622,
      "learning_rate": 4.859782952526949e-05,
      "loss": 0.758,
      "step": 184400
    },
    {
      "epoch": 1.683517045039784,
      "grad_norm": 3.5074408054351807,
      "learning_rate": 4.859706912913352e-05,
      "loss": 0.7914,
      "step": 184500
    },
    {
      "epoch": 1.6844295204029491,
      "grad_norm": 4.395068645477295,
      "learning_rate": 4.859630873299755e-05,
      "loss": 0.7631,
      "step": 184600
    },
    {
      "epoch": 1.6853419957661142,
      "grad_norm": 4.525357723236084,
      "learning_rate": 4.859554833686157e-05,
      "loss": 0.7661,
      "step": 184700
    },
    {
      "epoch": 1.6862544711292795,
      "grad_norm": 3.304682970046997,
      "learning_rate": 4.85947879407256e-05,
      "loss": 0.7753,
      "step": 184800
    },
    {
      "epoch": 1.6871669464924448,
      "grad_norm": 4.112278461456299,
      "learning_rate": 4.859402754458963e-05,
      "loss": 0.8116,
      "step": 184900
    },
    {
      "epoch": 1.6880794218556099,
      "grad_norm": 4.753178596496582,
      "learning_rate": 4.859326714845366e-05,
      "loss": 0.771,
      "step": 185000
    },
    {
      "epoch": 1.688991897218775,
      "grad_norm": 4.0662689208984375,
      "learning_rate": 4.859250675231769e-05,
      "loss": 0.8223,
      "step": 185100
    },
    {
      "epoch": 1.6899043725819403,
      "grad_norm": 3.2992875576019287,
      "learning_rate": 4.8591746356181715e-05,
      "loss": 0.8091,
      "step": 185200
    },
    {
      "epoch": 1.6908168479451056,
      "grad_norm": 3.57068133354187,
      "learning_rate": 4.8590985960045745e-05,
      "loss": 0.7854,
      "step": 185300
    },
    {
      "epoch": 1.6917293233082706,
      "grad_norm": 2.699918031692505,
      "learning_rate": 4.8590225563909775e-05,
      "loss": 0.7772,
      "step": 185400
    },
    {
      "epoch": 1.6926417986714357,
      "grad_norm": 3.653090476989746,
      "learning_rate": 4.8589465167773805e-05,
      "loss": 0.7737,
      "step": 185500
    },
    {
      "epoch": 1.693554274034601,
      "grad_norm": 4.239158630371094,
      "learning_rate": 4.8588704771637835e-05,
      "loss": 0.762,
      "step": 185600
    },
    {
      "epoch": 1.6944667493977663,
      "grad_norm": 5.076686382293701,
      "learning_rate": 4.8587944375501865e-05,
      "loss": 0.7366,
      "step": 185700
    },
    {
      "epoch": 1.6953792247609316,
      "grad_norm": 4.236702919006348,
      "learning_rate": 4.858718397936589e-05,
      "loss": 0.7733,
      "step": 185800
    },
    {
      "epoch": 1.6962917001240967,
      "grad_norm": 4.369009017944336,
      "learning_rate": 4.8586423583229925e-05,
      "loss": 0.7542,
      "step": 185900
    },
    {
      "epoch": 1.6972041754872618,
      "grad_norm": 4.5737128257751465,
      "learning_rate": 4.858566318709395e-05,
      "loss": 0.7712,
      "step": 186000
    },
    {
      "epoch": 1.698116650850427,
      "grad_norm": 4.9308180809021,
      "learning_rate": 4.858490279095798e-05,
      "loss": 0.772,
      "step": 186100
    },
    {
      "epoch": 1.6990291262135924,
      "grad_norm": 4.447093486785889,
      "learning_rate": 4.858414239482201e-05,
      "loss": 0.763,
      "step": 186200
    },
    {
      "epoch": 1.6999416015767574,
      "grad_norm": 4.254400253295898,
      "learning_rate": 4.858338199868604e-05,
      "loss": 0.7489,
      "step": 186300
    },
    {
      "epoch": 1.7008540769399225,
      "grad_norm": 3.6729860305786133,
      "learning_rate": 4.858262160255007e-05,
      "loss": 0.7693,
      "step": 186400
    },
    {
      "epoch": 1.7017665523030878,
      "grad_norm": 3.653297185897827,
      "learning_rate": 4.85818612064141e-05,
      "loss": 0.8308,
      "step": 186500
    },
    {
      "epoch": 1.7026790276662531,
      "grad_norm": 3.8551723957061768,
      "learning_rate": 4.858110081027812e-05,
      "loss": 0.8031,
      "step": 186600
    },
    {
      "epoch": 1.7035915030294182,
      "grad_norm": 3.9594192504882812,
      "learning_rate": 4.858034041414216e-05,
      "loss": 0.7551,
      "step": 186700
    },
    {
      "epoch": 1.7045039783925833,
      "grad_norm": 4.676926136016846,
      "learning_rate": 4.857958001800618e-05,
      "loss": 0.799,
      "step": 186800
    },
    {
      "epoch": 1.7054164537557486,
      "grad_norm": 4.889562606811523,
      "learning_rate": 4.857881962187021e-05,
      "loss": 0.7971,
      "step": 186900
    },
    {
      "epoch": 1.7063289291189139,
      "grad_norm": 3.9953665733337402,
      "learning_rate": 4.857805922573424e-05,
      "loss": 0.8043,
      "step": 187000
    },
    {
      "epoch": 1.707241404482079,
      "grad_norm": 4.683414936065674,
      "learning_rate": 4.857729882959827e-05,
      "loss": 0.7858,
      "step": 187100
    },
    {
      "epoch": 1.708153879845244,
      "grad_norm": 4.590930938720703,
      "learning_rate": 4.8576538433462296e-05,
      "loss": 0.7813,
      "step": 187200
    },
    {
      "epoch": 1.7090663552084093,
      "grad_norm": 5.690151214599609,
      "learning_rate": 4.857577803732633e-05,
      "loss": 0.7484,
      "step": 187300
    },
    {
      "epoch": 1.7099788305715746,
      "grad_norm": 4.6652679443359375,
      "learning_rate": 4.8575017641190356e-05,
      "loss": 0.7524,
      "step": 187400
    },
    {
      "epoch": 1.71089130593474,
      "grad_norm": 3.5975682735443115,
      "learning_rate": 4.8574257245054386e-05,
      "loss": 0.7641,
      "step": 187500
    },
    {
      "epoch": 1.711803781297905,
      "grad_norm": 4.000174045562744,
      "learning_rate": 4.8573496848918416e-05,
      "loss": 0.7662,
      "step": 187600
    },
    {
      "epoch": 1.71271625666107,
      "grad_norm": 4.637253761291504,
      "learning_rate": 4.857273645278244e-05,
      "loss": 0.7693,
      "step": 187700
    },
    {
      "epoch": 1.7136287320242354,
      "grad_norm": 4.353042125701904,
      "learning_rate": 4.8571976056646476e-05,
      "loss": 0.7971,
      "step": 187800
    },
    {
      "epoch": 1.7145412073874007,
      "grad_norm": 2.780911445617676,
      "learning_rate": 4.85712156605105e-05,
      "loss": 0.7819,
      "step": 187900
    },
    {
      "epoch": 1.7154536827505658,
      "grad_norm": 4.190706253051758,
      "learning_rate": 4.857045526437453e-05,
      "loss": 0.7711,
      "step": 188000
    },
    {
      "epoch": 1.7163661581137308,
      "grad_norm": 4.372349262237549,
      "learning_rate": 4.856969486823856e-05,
      "loss": 0.7937,
      "step": 188100
    },
    {
      "epoch": 1.7172786334768961,
      "grad_norm": 3.5433733463287354,
      "learning_rate": 4.856893447210259e-05,
      "loss": 0.7882,
      "step": 188200
    },
    {
      "epoch": 1.7181911088400614,
      "grad_norm": 4.001577854156494,
      "learning_rate": 4.856817407596661e-05,
      "loss": 0.7544,
      "step": 188300
    },
    {
      "epoch": 1.7191035842032265,
      "grad_norm": 4.430388927459717,
      "learning_rate": 4.856741367983065e-05,
      "loss": 0.7706,
      "step": 188400
    },
    {
      "epoch": 1.7200160595663916,
      "grad_norm": 3.9581079483032227,
      "learning_rate": 4.856665328369467e-05,
      "loss": 0.7715,
      "step": 188500
    },
    {
      "epoch": 1.7209285349295569,
      "grad_norm": 3.182965040206909,
      "learning_rate": 4.8565892887558703e-05,
      "loss": 0.7323,
      "step": 188600
    },
    {
      "epoch": 1.7218410102927222,
      "grad_norm": 4.404318332672119,
      "learning_rate": 4.8565132491422733e-05,
      "loss": 0.7603,
      "step": 188700
    },
    {
      "epoch": 1.7227534856558873,
      "grad_norm": 3.914802074432373,
      "learning_rate": 4.8564372095286764e-05,
      "loss": 0.7174,
      "step": 188800
    },
    {
      "epoch": 1.7236659610190523,
      "grad_norm": 4.18313455581665,
      "learning_rate": 4.8563611699150794e-05,
      "loss": 0.7712,
      "step": 188900
    },
    {
      "epoch": 1.7245784363822176,
      "grad_norm": 3.8836135864257812,
      "learning_rate": 4.8562851303014824e-05,
      "loss": 0.7386,
      "step": 189000
    },
    {
      "epoch": 1.725490911745383,
      "grad_norm": 4.794348239898682,
      "learning_rate": 4.856209090687885e-05,
      "loss": 0.7768,
      "step": 189100
    },
    {
      "epoch": 1.726403387108548,
      "grad_norm": 3.0948221683502197,
      "learning_rate": 4.8561330510742884e-05,
      "loss": 0.7889,
      "step": 189200
    },
    {
      "epoch": 1.7273158624717133,
      "grad_norm": 4.119332313537598,
      "learning_rate": 4.856057011460691e-05,
      "loss": 0.7635,
      "step": 189300
    },
    {
      "epoch": 1.7282283378348784,
      "grad_norm": 4.057365417480469,
      "learning_rate": 4.855980971847094e-05,
      "loss": 0.8139,
      "step": 189400
    },
    {
      "epoch": 1.7291408131980437,
      "grad_norm": 4.0968475341796875,
      "learning_rate": 4.855904932233497e-05,
      "loss": 0.7848,
      "step": 189500
    },
    {
      "epoch": 1.730053288561209,
      "grad_norm": 4.114497184753418,
      "learning_rate": 4.8558288926199e-05,
      "loss": 0.781,
      "step": 189600
    },
    {
      "epoch": 1.730965763924374,
      "grad_norm": 4.534153938293457,
      "learning_rate": 4.855752853006302e-05,
      "loss": 0.7929,
      "step": 189700
    },
    {
      "epoch": 1.7318782392875391,
      "grad_norm": 4.373645782470703,
      "learning_rate": 4.855676813392706e-05,
      "loss": 0.8326,
      "step": 189800
    },
    {
      "epoch": 1.7327907146507044,
      "grad_norm": 4.015073299407959,
      "learning_rate": 4.855600773779108e-05,
      "loss": 0.7511,
      "step": 189900
    },
    {
      "epoch": 1.7337031900138697,
      "grad_norm": 4.10088586807251,
      "learning_rate": 4.855524734165511e-05,
      "loss": 0.7858,
      "step": 190000
    },
    {
      "epoch": 1.7346156653770348,
      "grad_norm": 3.9724607467651367,
      "learning_rate": 4.855448694551914e-05,
      "loss": 0.7516,
      "step": 190100
    },
    {
      "epoch": 1.7355281407402,
      "grad_norm": 3.5365469455718994,
      "learning_rate": 4.855372654938317e-05,
      "loss": 0.7884,
      "step": 190200
    },
    {
      "epoch": 1.7364406161033652,
      "grad_norm": 4.695392608642578,
      "learning_rate": 4.85529661532472e-05,
      "loss": 0.8034,
      "step": 190300
    },
    {
      "epoch": 1.7373530914665305,
      "grad_norm": 3.658891201019287,
      "learning_rate": 4.8552205757111224e-05,
      "loss": 0.7594,
      "step": 190400
    },
    {
      "epoch": 1.7382655668296956,
      "grad_norm": 4.219161510467529,
      "learning_rate": 4.8551445360975254e-05,
      "loss": 0.7512,
      "step": 190500
    },
    {
      "epoch": 1.7391780421928607,
      "grad_norm": 4.456480026245117,
      "learning_rate": 4.8550684964839284e-05,
      "loss": 0.7527,
      "step": 190600
    },
    {
      "epoch": 1.740090517556026,
      "grad_norm": 4.1608710289001465,
      "learning_rate": 4.8549924568703314e-05,
      "loss": 0.7503,
      "step": 190700
    },
    {
      "epoch": 1.7410029929191913,
      "grad_norm": 3.3603460788726807,
      "learning_rate": 4.854916417256734e-05,
      "loss": 0.7849,
      "step": 190800
    },
    {
      "epoch": 1.7419154682823563,
      "grad_norm": 3.7923591136932373,
      "learning_rate": 4.8548403776431375e-05,
      "loss": 0.7482,
      "step": 190900
    },
    {
      "epoch": 1.7428279436455214,
      "grad_norm": 4.410406589508057,
      "learning_rate": 4.85476433802954e-05,
      "loss": 0.7644,
      "step": 191000
    },
    {
      "epoch": 1.7437404190086867,
      "grad_norm": 3.9388134479522705,
      "learning_rate": 4.854688298415943e-05,
      "loss": 0.7396,
      "step": 191100
    },
    {
      "epoch": 1.744652894371852,
      "grad_norm": 4.430976867675781,
      "learning_rate": 4.854612258802346e-05,
      "loss": 0.8083,
      "step": 191200
    },
    {
      "epoch": 1.7455653697350173,
      "grad_norm": 4.500326156616211,
      "learning_rate": 4.854536219188749e-05,
      "loss": 0.7069,
      "step": 191300
    },
    {
      "epoch": 1.7464778450981824,
      "grad_norm": 4.787961959838867,
      "learning_rate": 4.854460179575152e-05,
      "loss": 0.7788,
      "step": 191400
    },
    {
      "epoch": 1.7473903204613475,
      "grad_norm": 5.616637706756592,
      "learning_rate": 4.854384139961555e-05,
      "loss": 0.8094,
      "step": 191500
    },
    {
      "epoch": 1.7483027958245128,
      "grad_norm": 4.4588727951049805,
      "learning_rate": 4.854308100347957e-05,
      "loss": 0.7702,
      "step": 191600
    },
    {
      "epoch": 1.749215271187678,
      "grad_norm": 3.992840528488159,
      "learning_rate": 4.854232060734361e-05,
      "loss": 0.7529,
      "step": 191700
    },
    {
      "epoch": 1.7501277465508431,
      "grad_norm": 4.490670204162598,
      "learning_rate": 4.854156021120763e-05,
      "loss": 0.8258,
      "step": 191800
    },
    {
      "epoch": 1.7510402219140082,
      "grad_norm": 4.451484203338623,
      "learning_rate": 4.854079981507166e-05,
      "loss": 0.7491,
      "step": 191900
    },
    {
      "epoch": 1.7519526972771735,
      "grad_norm": 3.1273655891418457,
      "learning_rate": 4.854003941893569e-05,
      "loss": 0.7702,
      "step": 192000
    },
    {
      "epoch": 1.7528651726403388,
      "grad_norm": 4.071371078491211,
      "learning_rate": 4.853927902279972e-05,
      "loss": 0.7618,
      "step": 192100
    },
    {
      "epoch": 1.7537776480035039,
      "grad_norm": 4.3319783210754395,
      "learning_rate": 4.8538518626663745e-05,
      "loss": 0.7393,
      "step": 192200
    },
    {
      "epoch": 1.754690123366669,
      "grad_norm": 4.274696350097656,
      "learning_rate": 4.853775823052778e-05,
      "loss": 0.7744,
      "step": 192300
    },
    {
      "epoch": 1.7556025987298343,
      "grad_norm": 4.052804946899414,
      "learning_rate": 4.8536997834391805e-05,
      "loss": 0.7949,
      "step": 192400
    },
    {
      "epoch": 1.7565150740929996,
      "grad_norm": 4.5211005210876465,
      "learning_rate": 4.8536237438255835e-05,
      "loss": 0.7704,
      "step": 192500
    },
    {
      "epoch": 1.7574275494561646,
      "grad_norm": 4.32009744644165,
      "learning_rate": 4.8535477042119865e-05,
      "loss": 0.7227,
      "step": 192600
    },
    {
      "epoch": 1.7583400248193297,
      "grad_norm": 4.185928821563721,
      "learning_rate": 4.8534716645983895e-05,
      "loss": 0.7783,
      "step": 192700
    },
    {
      "epoch": 1.759252500182495,
      "grad_norm": 4.221878528594971,
      "learning_rate": 4.8533956249847926e-05,
      "loss": 0.7964,
      "step": 192800
    },
    {
      "epoch": 1.7601649755456603,
      "grad_norm": 4.500560283660889,
      "learning_rate": 4.8533195853711956e-05,
      "loss": 0.772,
      "step": 192900
    },
    {
      "epoch": 1.7610774509088256,
      "grad_norm": 4.353584289550781,
      "learning_rate": 4.853243545757598e-05,
      "loss": 0.7932,
      "step": 193000
    },
    {
      "epoch": 1.7619899262719907,
      "grad_norm": 4.396876335144043,
      "learning_rate": 4.8531675061440016e-05,
      "loss": 0.7113,
      "step": 193100
    },
    {
      "epoch": 1.7629024016351558,
      "grad_norm": 4.2662882804870605,
      "learning_rate": 4.853091466530404e-05,
      "loss": 0.7215,
      "step": 193200
    },
    {
      "epoch": 1.763814876998321,
      "grad_norm": 4.3764543533325195,
      "learning_rate": 4.853015426916806e-05,
      "loss": 0.8,
      "step": 193300
    },
    {
      "epoch": 1.7647273523614864,
      "grad_norm": 4.28441047668457,
      "learning_rate": 4.85293938730321e-05,
      "loss": 0.7779,
      "step": 193400
    },
    {
      "epoch": 1.7656398277246514,
      "grad_norm": 3.719036817550659,
      "learning_rate": 4.852863347689612e-05,
      "loss": 0.8031,
      "step": 193500
    },
    {
      "epoch": 1.7665523030878165,
      "grad_norm": 3.9463679790496826,
      "learning_rate": 4.852787308076015e-05,
      "loss": 0.7913,
      "step": 193600
    },
    {
      "epoch": 1.7674647784509818,
      "grad_norm": 4.446022987365723,
      "learning_rate": 4.852711268462418e-05,
      "loss": 0.7557,
      "step": 193700
    },
    {
      "epoch": 1.7683772538141471,
      "grad_norm": 3.962153196334839,
      "learning_rate": 4.852635228848821e-05,
      "loss": 0.7743,
      "step": 193800
    },
    {
      "epoch": 1.7692897291773122,
      "grad_norm": 4.742186546325684,
      "learning_rate": 4.852559189235224e-05,
      "loss": 0.7596,
      "step": 193900
    },
    {
      "epoch": 1.7702022045404773,
      "grad_norm": 4.767083644866943,
      "learning_rate": 4.852483149621627e-05,
      "loss": 0.7883,
      "step": 194000
    },
    {
      "epoch": 1.7711146799036426,
      "grad_norm": 4.30496883392334,
      "learning_rate": 4.8524071100080296e-05,
      "loss": 0.7865,
      "step": 194100
    },
    {
      "epoch": 1.7720271552668079,
      "grad_norm": 4.288464069366455,
      "learning_rate": 4.852331070394433e-05,
      "loss": 0.7787,
      "step": 194200
    },
    {
      "epoch": 1.772939630629973,
      "grad_norm": 4.335844039916992,
      "learning_rate": 4.8522550307808356e-05,
      "loss": 0.7746,
      "step": 194300
    },
    {
      "epoch": 1.773852105993138,
      "grad_norm": 4.17941951751709,
      "learning_rate": 4.8521789911672386e-05,
      "loss": 0.7854,
      "step": 194400
    },
    {
      "epoch": 1.7747645813563033,
      "grad_norm": 2.306988477706909,
      "learning_rate": 4.8521029515536416e-05,
      "loss": 0.7376,
      "step": 194500
    },
    {
      "epoch": 1.7756770567194686,
      "grad_norm": 4.52384614944458,
      "learning_rate": 4.8520269119400446e-05,
      "loss": 0.8092,
      "step": 194600
    },
    {
      "epoch": 1.776589532082634,
      "grad_norm": 4.285627365112305,
      "learning_rate": 4.851950872326447e-05,
      "loss": 0.773,
      "step": 194700
    },
    {
      "epoch": 1.777502007445799,
      "grad_norm": 3.77319073677063,
      "learning_rate": 4.8518748327128507e-05,
      "loss": 0.7512,
      "step": 194800
    },
    {
      "epoch": 1.778414482808964,
      "grad_norm": 3.890448808670044,
      "learning_rate": 4.851798793099253e-05,
      "loss": 0.7599,
      "step": 194900
    },
    {
      "epoch": 1.7793269581721294,
      "grad_norm": 4.7139668464660645,
      "learning_rate": 4.851722753485656e-05,
      "loss": 0.7693,
      "step": 195000
    },
    {
      "epoch": 1.7802394335352947,
      "grad_norm": 3.949528932571411,
      "learning_rate": 4.851646713872059e-05,
      "loss": 0.7896,
      "step": 195100
    },
    {
      "epoch": 1.7811519088984598,
      "grad_norm": 4.75702428817749,
      "learning_rate": 4.851570674258462e-05,
      "loss": 0.8098,
      "step": 195200
    },
    {
      "epoch": 1.7820643842616248,
      "grad_norm": 4.496127605438232,
      "learning_rate": 4.851494634644865e-05,
      "loss": 0.8022,
      "step": 195300
    },
    {
      "epoch": 1.7829768596247901,
      "grad_norm": 4.62028694152832,
      "learning_rate": 4.851418595031268e-05,
      "loss": 0.7908,
      "step": 195400
    },
    {
      "epoch": 1.7838893349879554,
      "grad_norm": 4.121727466583252,
      "learning_rate": 4.8513425554176703e-05,
      "loss": 0.7464,
      "step": 195500
    },
    {
      "epoch": 1.7848018103511205,
      "grad_norm": 4.555670738220215,
      "learning_rate": 4.851266515804074e-05,
      "loss": 0.7488,
      "step": 195600
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 4.341109275817871,
      "learning_rate": 4.8511904761904764e-05,
      "loss": 0.7169,
      "step": 195700
    },
    {
      "epoch": 1.7866267610774509,
      "grad_norm": 3.502483606338501,
      "learning_rate": 4.8511144365768794e-05,
      "loss": 0.7248,
      "step": 195800
    },
    {
      "epoch": 1.7875392364406162,
      "grad_norm": 4.0845866203308105,
      "learning_rate": 4.8510383969632824e-05,
      "loss": 0.7454,
      "step": 195900
    },
    {
      "epoch": 1.7884517118037813,
      "grad_norm": 3.7164840698242188,
      "learning_rate": 4.850962357349685e-05,
      "loss": 0.7731,
      "step": 196000
    },
    {
      "epoch": 1.7893641871669463,
      "grad_norm": 4.814009666442871,
      "learning_rate": 4.850886317736088e-05,
      "loss": 0.8292,
      "step": 196100
    },
    {
      "epoch": 1.7902766625301116,
      "grad_norm": 3.9827423095703125,
      "learning_rate": 4.850810278122491e-05,
      "loss": 0.7855,
      "step": 196200
    },
    {
      "epoch": 1.791189137893277,
      "grad_norm": 4.588788986206055,
      "learning_rate": 4.850734238508894e-05,
      "loss": 0.7671,
      "step": 196300
    },
    {
      "epoch": 1.7921016132564422,
      "grad_norm": 4.850348949432373,
      "learning_rate": 4.850658198895297e-05,
      "loss": 0.7975,
      "step": 196400
    },
    {
      "epoch": 1.7930140886196073,
      "grad_norm": 3.127401113510132,
      "learning_rate": 4.8505821592817e-05,
      "loss": 0.7596,
      "step": 196500
    },
    {
      "epoch": 1.7939265639827724,
      "grad_norm": 5.051571369171143,
      "learning_rate": 4.850506119668102e-05,
      "loss": 0.7661,
      "step": 196600
    },
    {
      "epoch": 1.7948390393459377,
      "grad_norm": 4.490049839019775,
      "learning_rate": 4.850430080054506e-05,
      "loss": 0.7689,
      "step": 196700
    },
    {
      "epoch": 1.795751514709103,
      "grad_norm": 2.993772029876709,
      "learning_rate": 4.850354040440908e-05,
      "loss": 0.7504,
      "step": 196800
    },
    {
      "epoch": 1.796663990072268,
      "grad_norm": 4.157618999481201,
      "learning_rate": 4.850278000827311e-05,
      "loss": 0.7601,
      "step": 196900
    },
    {
      "epoch": 1.7975764654354331,
      "grad_norm": 3.284390926361084,
      "learning_rate": 4.850201961213714e-05,
      "loss": 0.7451,
      "step": 197000
    },
    {
      "epoch": 1.7984889407985984,
      "grad_norm": 4.53759765625,
      "learning_rate": 4.850125921600117e-05,
      "loss": 0.7865,
      "step": 197100
    },
    {
      "epoch": 1.7994014161617637,
      "grad_norm": 3.7963576316833496,
      "learning_rate": 4.85004988198652e-05,
      "loss": 0.7558,
      "step": 197200
    },
    {
      "epoch": 1.8003138915249288,
      "grad_norm": 3.843888282775879,
      "learning_rate": 4.849973842372923e-05,
      "loss": 0.79,
      "step": 197300
    },
    {
      "epoch": 1.801226366888094,
      "grad_norm": 4.896747589111328,
      "learning_rate": 4.8498978027593254e-05,
      "loss": 0.7607,
      "step": 197400
    },
    {
      "epoch": 1.8021388422512592,
      "grad_norm": 4.153625011444092,
      "learning_rate": 4.8498217631457285e-05,
      "loss": 0.813,
      "step": 197500
    },
    {
      "epoch": 1.8030513176144245,
      "grad_norm": 5.053310871124268,
      "learning_rate": 4.8497457235321315e-05,
      "loss": 0.7516,
      "step": 197600
    },
    {
      "epoch": 1.8039637929775896,
      "grad_norm": 4.739261627197266,
      "learning_rate": 4.8496696839185345e-05,
      "loss": 0.7693,
      "step": 197700
    },
    {
      "epoch": 1.8048762683407547,
      "grad_norm": 3.8724863529205322,
      "learning_rate": 4.8495936443049375e-05,
      "loss": 0.7736,
      "step": 197800
    },
    {
      "epoch": 1.80578874370392,
      "grad_norm": 4.459965229034424,
      "learning_rate": 4.8495176046913405e-05,
      "loss": 0.7648,
      "step": 197900
    },
    {
      "epoch": 1.8067012190670853,
      "grad_norm": 4.385801792144775,
      "learning_rate": 4.849441565077743e-05,
      "loss": 0.7878,
      "step": 198000
    },
    {
      "epoch": 1.8076136944302503,
      "grad_norm": 4.9018025398254395,
      "learning_rate": 4.8493655254641465e-05,
      "loss": 0.7747,
      "step": 198100
    },
    {
      "epoch": 1.8085261697934156,
      "grad_norm": 3.9980862140655518,
      "learning_rate": 4.849289485850549e-05,
      "loss": 0.7931,
      "step": 198200
    },
    {
      "epoch": 1.8094386451565807,
      "grad_norm": 3.911046028137207,
      "learning_rate": 4.849213446236952e-05,
      "loss": 0.7532,
      "step": 198300
    },
    {
      "epoch": 1.810351120519746,
      "grad_norm": 3.7016549110412598,
      "learning_rate": 4.849137406623355e-05,
      "loss": 0.7944,
      "step": 198400
    },
    {
      "epoch": 1.8112635958829113,
      "grad_norm": 4.7174601554870605,
      "learning_rate": 4.849061367009758e-05,
      "loss": 0.7509,
      "step": 198500
    },
    {
      "epoch": 1.8121760712460764,
      "grad_norm": 4.310943603515625,
      "learning_rate": 4.848985327396161e-05,
      "loss": 0.7938,
      "step": 198600
    },
    {
      "epoch": 1.8130885466092415,
      "grad_norm": 3.550356864929199,
      "learning_rate": 4.848909287782564e-05,
      "loss": 0.7758,
      "step": 198700
    },
    {
      "epoch": 1.8140010219724068,
      "grad_norm": 4.351792812347412,
      "learning_rate": 4.848833248168966e-05,
      "loss": 0.7961,
      "step": 198800
    },
    {
      "epoch": 1.814913497335572,
      "grad_norm": 6.162627220153809,
      "learning_rate": 4.848757208555369e-05,
      "loss": 0.7394,
      "step": 198900
    },
    {
      "epoch": 1.8158259726987371,
      "grad_norm": 4.719892978668213,
      "learning_rate": 4.848681168941772e-05,
      "loss": 0.7594,
      "step": 199000
    },
    {
      "epoch": 1.8167384480619022,
      "grad_norm": 4.561456680297852,
      "learning_rate": 4.8486051293281745e-05,
      "loss": 0.7655,
      "step": 199100
    },
    {
      "epoch": 1.8176509234250675,
      "grad_norm": 3.702491044998169,
      "learning_rate": 4.848529089714578e-05,
      "loss": 0.7875,
      "step": 199200
    },
    {
      "epoch": 1.8185633987882328,
      "grad_norm": 4.5084428787231445,
      "learning_rate": 4.8484530501009805e-05,
      "loss": 0.8021,
      "step": 199300
    },
    {
      "epoch": 1.8194758741513979,
      "grad_norm": 3.400912046432495,
      "learning_rate": 4.8483770104873835e-05,
      "loss": 0.7783,
      "step": 199400
    },
    {
      "epoch": 1.820388349514563,
      "grad_norm": 4.438042640686035,
      "learning_rate": 4.8483009708737866e-05,
      "loss": 0.7685,
      "step": 199500
    },
    {
      "epoch": 1.8213008248777283,
      "grad_norm": 5.069023132324219,
      "learning_rate": 4.8482249312601896e-05,
      "loss": 0.7604,
      "step": 199600
    },
    {
      "epoch": 1.8222133002408936,
      "grad_norm": 3.8963332176208496,
      "learning_rate": 4.8481488916465926e-05,
      "loss": 0.7625,
      "step": 199700
    },
    {
      "epoch": 1.8231257756040586,
      "grad_norm": 4.267152309417725,
      "learning_rate": 4.8480728520329956e-05,
      "loss": 0.7711,
      "step": 199800
    },
    {
      "epoch": 1.824038250967224,
      "grad_norm": 3.666700601577759,
      "learning_rate": 4.847996812419398e-05,
      "loss": 0.7691,
      "step": 199900
    },
    {
      "epoch": 1.824950726330389,
      "grad_norm": 5.513006687164307,
      "learning_rate": 4.8479207728058016e-05,
      "loss": 0.7687,
      "step": 200000
    },
    {
      "epoch": 1.8258632016935543,
      "grad_norm": 4.308884620666504,
      "learning_rate": 4.847844733192204e-05,
      "loss": 0.7807,
      "step": 200100
    },
    {
      "epoch": 1.8267756770567196,
      "grad_norm": 4.460080623626709,
      "learning_rate": 4.847768693578607e-05,
      "loss": 0.7817,
      "step": 200200
    },
    {
      "epoch": 1.8276881524198847,
      "grad_norm": 4.342499256134033,
      "learning_rate": 4.84769265396501e-05,
      "loss": 0.7941,
      "step": 200300
    },
    {
      "epoch": 1.8286006277830498,
      "grad_norm": 3.821305513381958,
      "learning_rate": 4.847616614351413e-05,
      "loss": 0.7869,
      "step": 200400
    },
    {
      "epoch": 1.829513103146215,
      "grad_norm": 4.388462543487549,
      "learning_rate": 4.847540574737815e-05,
      "loss": 0.7931,
      "step": 200500
    },
    {
      "epoch": 1.8304255785093804,
      "grad_norm": 4.2959513664245605,
      "learning_rate": 4.847464535124219e-05,
      "loss": 0.7388,
      "step": 200600
    },
    {
      "epoch": 1.8313380538725454,
      "grad_norm": 4.476956367492676,
      "learning_rate": 4.847388495510621e-05,
      "loss": 0.7352,
      "step": 200700
    },
    {
      "epoch": 1.8322505292357105,
      "grad_norm": 4.059482097625732,
      "learning_rate": 4.847312455897024e-05,
      "loss": 0.7774,
      "step": 200800
    },
    {
      "epoch": 1.8331630045988758,
      "grad_norm": 5.4630632400512695,
      "learning_rate": 4.847236416283427e-05,
      "loss": 0.7668,
      "step": 200900
    },
    {
      "epoch": 1.8340754799620411,
      "grad_norm": 3.3133137226104736,
      "learning_rate": 4.84716037666983e-05,
      "loss": 0.7754,
      "step": 201000
    },
    {
      "epoch": 1.8349879553252062,
      "grad_norm": 4.1131486892700195,
      "learning_rate": 4.847084337056233e-05,
      "loss": 0.761,
      "step": 201100
    },
    {
      "epoch": 1.8359004306883713,
      "grad_norm": 4.118827819824219,
      "learning_rate": 4.847008297442636e-05,
      "loss": 0.7364,
      "step": 201200
    },
    {
      "epoch": 1.8368129060515366,
      "grad_norm": 3.5894973278045654,
      "learning_rate": 4.8469322578290386e-05,
      "loss": 0.7455,
      "step": 201300
    },
    {
      "epoch": 1.8377253814147019,
      "grad_norm": 4.038202285766602,
      "learning_rate": 4.846856218215442e-05,
      "loss": 0.8166,
      "step": 201400
    },
    {
      "epoch": 1.838637856777867,
      "grad_norm": 3.9667465686798096,
      "learning_rate": 4.8467801786018447e-05,
      "loss": 0.7796,
      "step": 201500
    },
    {
      "epoch": 1.839550332141032,
      "grad_norm": 3.6296701431274414,
      "learning_rate": 4.8467041389882477e-05,
      "loss": 0.7742,
      "step": 201600
    },
    {
      "epoch": 1.8404628075041973,
      "grad_norm": 4.789882183074951,
      "learning_rate": 4.846628099374651e-05,
      "loss": 0.7781,
      "step": 201700
    },
    {
      "epoch": 1.8413752828673626,
      "grad_norm": 5.097433567047119,
      "learning_rate": 4.846552059761053e-05,
      "loss": 0.8,
      "step": 201800
    },
    {
      "epoch": 1.842287758230528,
      "grad_norm": 3.6722164154052734,
      "learning_rate": 4.846476020147456e-05,
      "loss": 0.7697,
      "step": 201900
    },
    {
      "epoch": 1.843200233593693,
      "grad_norm": 3.6377384662628174,
      "learning_rate": 4.846399980533859e-05,
      "loss": 0.7547,
      "step": 202000
    },
    {
      "epoch": 1.844112708956858,
      "grad_norm": 4.719507217407227,
      "learning_rate": 4.846323940920262e-05,
      "loss": 0.7916,
      "step": 202100
    },
    {
      "epoch": 1.8450251843200234,
      "grad_norm": 4.473559856414795,
      "learning_rate": 4.846247901306665e-05,
      "loss": 0.7601,
      "step": 202200
    },
    {
      "epoch": 1.8459376596831887,
      "grad_norm": 4.316589832305908,
      "learning_rate": 4.846171861693068e-05,
      "loss": 0.7786,
      "step": 202300
    },
    {
      "epoch": 1.8468501350463538,
      "grad_norm": 1.9613014459609985,
      "learning_rate": 4.8460958220794704e-05,
      "loss": 0.7456,
      "step": 202400
    },
    {
      "epoch": 1.8477626104095188,
      "grad_norm": 4.2635345458984375,
      "learning_rate": 4.846019782465874e-05,
      "loss": 0.7604,
      "step": 202500
    },
    {
      "epoch": 1.8486750857726841,
      "grad_norm": 3.408217430114746,
      "learning_rate": 4.8459437428522764e-05,
      "loss": 0.769,
      "step": 202600
    },
    {
      "epoch": 1.8495875611358494,
      "grad_norm": 4.969542026519775,
      "learning_rate": 4.8458677032386794e-05,
      "loss": 0.7663,
      "step": 202700
    },
    {
      "epoch": 1.8505000364990145,
      "grad_norm": 4.401144981384277,
      "learning_rate": 4.8457916636250824e-05,
      "loss": 0.7795,
      "step": 202800
    },
    {
      "epoch": 1.8514125118621796,
      "grad_norm": 4.447337627410889,
      "learning_rate": 4.8457156240114854e-05,
      "loss": 0.8179,
      "step": 202900
    },
    {
      "epoch": 1.8523249872253449,
      "grad_norm": 3.5918617248535156,
      "learning_rate": 4.845639584397888e-05,
      "loss": 0.775,
      "step": 203000
    },
    {
      "epoch": 1.8532374625885102,
      "grad_norm": 4.3295512199401855,
      "learning_rate": 4.8455635447842914e-05,
      "loss": 0.7547,
      "step": 203100
    },
    {
      "epoch": 1.8541499379516753,
      "grad_norm": 4.177255153656006,
      "learning_rate": 4.845487505170694e-05,
      "loss": 0.764,
      "step": 203200
    },
    {
      "epoch": 1.8550624133148403,
      "grad_norm": 4.826287746429443,
      "learning_rate": 4.845411465557097e-05,
      "loss": 0.7789,
      "step": 203300
    },
    {
      "epoch": 1.8559748886780056,
      "grad_norm": 3.7381813526153564,
      "learning_rate": 4.8453354259435e-05,
      "loss": 0.7571,
      "step": 203400
    },
    {
      "epoch": 1.856887364041171,
      "grad_norm": 3.936962366104126,
      "learning_rate": 4.845259386329903e-05,
      "loss": 0.7464,
      "step": 203500
    },
    {
      "epoch": 1.8577998394043362,
      "grad_norm": 4.420234203338623,
      "learning_rate": 4.845183346716306e-05,
      "loss": 0.7706,
      "step": 203600
    },
    {
      "epoch": 1.8587123147675013,
      "grad_norm": 3.6687915325164795,
      "learning_rate": 4.845107307102709e-05,
      "loss": 0.7447,
      "step": 203700
    },
    {
      "epoch": 1.8596247901306664,
      "grad_norm": 3.706757068634033,
      "learning_rate": 4.845031267489111e-05,
      "loss": 0.8009,
      "step": 203800
    },
    {
      "epoch": 1.8605372654938317,
      "grad_norm": 4.339391708374023,
      "learning_rate": 4.844955227875515e-05,
      "loss": 0.7543,
      "step": 203900
    },
    {
      "epoch": 1.861449740856997,
      "grad_norm": 4.112888336181641,
      "learning_rate": 4.844879188261917e-05,
      "loss": 0.7738,
      "step": 204000
    },
    {
      "epoch": 1.862362216220162,
      "grad_norm": 4.4160261154174805,
      "learning_rate": 4.84480314864832e-05,
      "loss": 0.7742,
      "step": 204100
    },
    {
      "epoch": 1.8632746915833271,
      "grad_norm": 4.439382076263428,
      "learning_rate": 4.844727109034723e-05,
      "loss": 0.7361,
      "step": 204200
    },
    {
      "epoch": 1.8641871669464924,
      "grad_norm": 3.811286687850952,
      "learning_rate": 4.844651069421126e-05,
      "loss": 0.802,
      "step": 204300
    },
    {
      "epoch": 1.8650996423096577,
      "grad_norm": 4.918125152587891,
      "learning_rate": 4.8445750298075285e-05,
      "loss": 0.7522,
      "step": 204400
    },
    {
      "epoch": 1.8660121176728228,
      "grad_norm": 4.5052385330200195,
      "learning_rate": 4.8444989901939315e-05,
      "loss": 0.7449,
      "step": 204500
    },
    {
      "epoch": 1.866924593035988,
      "grad_norm": 3.922494411468506,
      "learning_rate": 4.8444229505803345e-05,
      "loss": 0.7274,
      "step": 204600
    },
    {
      "epoch": 1.8678370683991532,
      "grad_norm": 3.1211013793945312,
      "learning_rate": 4.8443469109667375e-05,
      "loss": 0.7672,
      "step": 204700
    },
    {
      "epoch": 1.8687495437623185,
      "grad_norm": 4.068546772003174,
      "learning_rate": 4.8442708713531405e-05,
      "loss": 0.7944,
      "step": 204800
    },
    {
      "epoch": 1.8696620191254836,
      "grad_norm": 4.661820888519287,
      "learning_rate": 4.844194831739543e-05,
      "loss": 0.7276,
      "step": 204900
    },
    {
      "epoch": 1.8705744944886487,
      "grad_norm": 4.342263698577881,
      "learning_rate": 4.8441187921259465e-05,
      "loss": 0.7931,
      "step": 205000
    },
    {
      "epoch": 1.871486969851814,
      "grad_norm": 4.347567081451416,
      "learning_rate": 4.844042752512349e-05,
      "loss": 0.7912,
      "step": 205100
    },
    {
      "epoch": 1.8723994452149793,
      "grad_norm": 3.625201940536499,
      "learning_rate": 4.843966712898752e-05,
      "loss": 0.7439,
      "step": 205200
    },
    {
      "epoch": 1.8733119205781446,
      "grad_norm": 4.458436965942383,
      "learning_rate": 4.843890673285155e-05,
      "loss": 0.773,
      "step": 205300
    },
    {
      "epoch": 1.8742243959413096,
      "grad_norm": 4.340516090393066,
      "learning_rate": 4.843814633671558e-05,
      "loss": 0.7653,
      "step": 205400
    },
    {
      "epoch": 1.8751368713044747,
      "grad_norm": 4.580911636352539,
      "learning_rate": 4.84373859405796e-05,
      "loss": 0.7364,
      "step": 205500
    },
    {
      "epoch": 1.87604934666764,
      "grad_norm": 3.984591007232666,
      "learning_rate": 4.843662554444364e-05,
      "loss": 0.7809,
      "step": 205600
    },
    {
      "epoch": 1.8769618220308053,
      "grad_norm": 3.8477935791015625,
      "learning_rate": 4.843586514830766e-05,
      "loss": 0.7607,
      "step": 205700
    },
    {
      "epoch": 1.8778742973939704,
      "grad_norm": 4.419501781463623,
      "learning_rate": 4.843510475217169e-05,
      "loss": 0.7866,
      "step": 205800
    },
    {
      "epoch": 1.8787867727571355,
      "grad_norm": 3.517817735671997,
      "learning_rate": 4.843434435603572e-05,
      "loss": 0.7525,
      "step": 205900
    },
    {
      "epoch": 1.8796992481203008,
      "grad_norm": 3.8078906536102295,
      "learning_rate": 4.843358395989975e-05,
      "loss": 0.73,
      "step": 206000
    },
    {
      "epoch": 1.880611723483466,
      "grad_norm": 3.8575165271759033,
      "learning_rate": 4.843282356376378e-05,
      "loss": 0.7434,
      "step": 206100
    },
    {
      "epoch": 1.8815241988466311,
      "grad_norm": 4.170253753662109,
      "learning_rate": 4.843206316762781e-05,
      "loss": 0.7527,
      "step": 206200
    },
    {
      "epoch": 1.8824366742097962,
      "grad_norm": 4.594998359680176,
      "learning_rate": 4.8431302771491836e-05,
      "loss": 0.7196,
      "step": 206300
    },
    {
      "epoch": 1.8833491495729615,
      "grad_norm": 3.265009641647339,
      "learning_rate": 4.843054237535587e-05,
      "loss": 0.8012,
      "step": 206400
    },
    {
      "epoch": 1.8842616249361268,
      "grad_norm": 4.346935749053955,
      "learning_rate": 4.8429781979219896e-05,
      "loss": 0.7321,
      "step": 206500
    },
    {
      "epoch": 1.8851741002992919,
      "grad_norm": 4.34030818939209,
      "learning_rate": 4.8429021583083926e-05,
      "loss": 0.7287,
      "step": 206600
    },
    {
      "epoch": 1.886086575662457,
      "grad_norm": 3.626173257827759,
      "learning_rate": 4.8428261186947956e-05,
      "loss": 0.7598,
      "step": 206700
    },
    {
      "epoch": 1.8869990510256223,
      "grad_norm": 3.914496660232544,
      "learning_rate": 4.8427500790811986e-05,
      "loss": 0.7781,
      "step": 206800
    },
    {
      "epoch": 1.8879115263887876,
      "grad_norm": 3.5614635944366455,
      "learning_rate": 4.842674039467601e-05,
      "loss": 0.7634,
      "step": 206900
    },
    {
      "epoch": 1.8888240017519529,
      "grad_norm": 4.505011558532715,
      "learning_rate": 4.8425979998540046e-05,
      "loss": 0.7606,
      "step": 207000
    },
    {
      "epoch": 1.889736477115118,
      "grad_norm": 3.630859136581421,
      "learning_rate": 4.842521960240407e-05,
      "loss": 0.7358,
      "step": 207100
    },
    {
      "epoch": 1.890648952478283,
      "grad_norm": 4.227797031402588,
      "learning_rate": 4.84244592062681e-05,
      "loss": 0.7956,
      "step": 207200
    },
    {
      "epoch": 1.8915614278414483,
      "grad_norm": 4.1813578605651855,
      "learning_rate": 4.842369881013213e-05,
      "loss": 0.7681,
      "step": 207300
    },
    {
      "epoch": 1.8924739032046136,
      "grad_norm": 4.853649616241455,
      "learning_rate": 4.842293841399615e-05,
      "loss": 0.7842,
      "step": 207400
    },
    {
      "epoch": 1.8933863785677787,
      "grad_norm": 3.8803117275238037,
      "learning_rate": 4.842217801786019e-05,
      "loss": 0.7804,
      "step": 207500
    },
    {
      "epoch": 1.8942988539309438,
      "grad_norm": 4.538166522979736,
      "learning_rate": 4.842141762172421e-05,
      "loss": 0.7721,
      "step": 207600
    },
    {
      "epoch": 1.895211329294109,
      "grad_norm": 4.57363748550415,
      "learning_rate": 4.842065722558824e-05,
      "loss": 0.7844,
      "step": 207700
    },
    {
      "epoch": 1.8961238046572744,
      "grad_norm": 4.311727523803711,
      "learning_rate": 4.841989682945227e-05,
      "loss": 0.7576,
      "step": 207800
    },
    {
      "epoch": 1.8970362800204394,
      "grad_norm": 3.515833616256714,
      "learning_rate": 4.84191364333163e-05,
      "loss": 0.7632,
      "step": 207900
    },
    {
      "epoch": 1.8979487553836045,
      "grad_norm": 4.168056011199951,
      "learning_rate": 4.8418376037180326e-05,
      "loss": 0.7843,
      "step": 208000
    },
    {
      "epoch": 1.8988612307467698,
      "grad_norm": 3.8704450130462646,
      "learning_rate": 4.841761564104436e-05,
      "loss": 0.773,
      "step": 208100
    },
    {
      "epoch": 1.8997737061099351,
      "grad_norm": 4.058967590332031,
      "learning_rate": 4.8416855244908387e-05,
      "loss": 0.7338,
      "step": 208200
    },
    {
      "epoch": 1.9006861814731002,
      "grad_norm": 4.996984481811523,
      "learning_rate": 4.8416094848772417e-05,
      "loss": 0.7833,
      "step": 208300
    },
    {
      "epoch": 1.9015986568362653,
      "grad_norm": 4.421825408935547,
      "learning_rate": 4.841533445263645e-05,
      "loss": 0.7685,
      "step": 208400
    },
    {
      "epoch": 1.9025111321994306,
      "grad_norm": 4.401735305786133,
      "learning_rate": 4.841457405650048e-05,
      "loss": 0.7629,
      "step": 208500
    },
    {
      "epoch": 1.9034236075625959,
      "grad_norm": 4.273854732513428,
      "learning_rate": 4.841381366036451e-05,
      "loss": 0.7792,
      "step": 208600
    },
    {
      "epoch": 1.904336082925761,
      "grad_norm": 4.149355411529541,
      "learning_rate": 4.841305326422854e-05,
      "loss": 0.7601,
      "step": 208700
    },
    {
      "epoch": 1.9052485582889263,
      "grad_norm": 3.767786741256714,
      "learning_rate": 4.841229286809256e-05,
      "loss": 0.7746,
      "step": 208800
    },
    {
      "epoch": 1.9061610336520913,
      "grad_norm": 5.061697959899902,
      "learning_rate": 4.84115324719566e-05,
      "loss": 0.7476,
      "step": 208900
    },
    {
      "epoch": 1.9070735090152566,
      "grad_norm": 3.0965564250946045,
      "learning_rate": 4.841077207582062e-05,
      "loss": 0.7786,
      "step": 209000
    },
    {
      "epoch": 1.907985984378422,
      "grad_norm": 3.7124385833740234,
      "learning_rate": 4.841001167968465e-05,
      "loss": 0.7652,
      "step": 209100
    },
    {
      "epoch": 1.908898459741587,
      "grad_norm": 3.4428293704986572,
      "learning_rate": 4.840925128354868e-05,
      "loss": 0.7585,
      "step": 209200
    },
    {
      "epoch": 1.909810935104752,
      "grad_norm": 2.9928884506225586,
      "learning_rate": 4.840849088741271e-05,
      "loss": 0.7688,
      "step": 209300
    },
    {
      "epoch": 1.9107234104679174,
      "grad_norm": 3.497173309326172,
      "learning_rate": 4.8407730491276734e-05,
      "loss": 0.7574,
      "step": 209400
    },
    {
      "epoch": 1.9116358858310827,
      "grad_norm": 4.6936492919921875,
      "learning_rate": 4.840697009514077e-05,
      "loss": 0.7997,
      "step": 209500
    },
    {
      "epoch": 1.9125483611942478,
      "grad_norm": 4.507292747497559,
      "learning_rate": 4.8406209699004794e-05,
      "loss": 0.7546,
      "step": 209600
    },
    {
      "epoch": 1.9134608365574128,
      "grad_norm": 3.9561076164245605,
      "learning_rate": 4.8405449302868824e-05,
      "loss": 0.7712,
      "step": 209700
    },
    {
      "epoch": 1.9143733119205781,
      "grad_norm": 5.022995948791504,
      "learning_rate": 4.8404688906732854e-05,
      "loss": 0.7767,
      "step": 209800
    },
    {
      "epoch": 1.9152857872837434,
      "grad_norm": 4.014388561248779,
      "learning_rate": 4.8403928510596884e-05,
      "loss": 0.7505,
      "step": 209900
    },
    {
      "epoch": 1.9161982626469085,
      "grad_norm": 4.224432945251465,
      "learning_rate": 4.8403168114460914e-05,
      "loss": 0.7827,
      "step": 210000
    },
    {
      "epoch": 1.9171107380100736,
      "grad_norm": 3.505316972732544,
      "learning_rate": 4.8402407718324944e-05,
      "loss": 0.7826,
      "step": 210100
    },
    {
      "epoch": 1.9180232133732389,
      "grad_norm": 3.824157953262329,
      "learning_rate": 4.840164732218897e-05,
      "loss": 0.769,
      "step": 210200
    },
    {
      "epoch": 1.9189356887364042,
      "grad_norm": 3.58161997795105,
      "learning_rate": 4.8400886926053e-05,
      "loss": 0.7713,
      "step": 210300
    },
    {
      "epoch": 1.9198481640995693,
      "grad_norm": 4.01676607131958,
      "learning_rate": 4.840012652991703e-05,
      "loss": 0.7846,
      "step": 210400
    },
    {
      "epoch": 1.9207606394627343,
      "grad_norm": 4.580050945281982,
      "learning_rate": 4.839936613378106e-05,
      "loss": 0.7749,
      "step": 210500
    },
    {
      "epoch": 1.9216731148258996,
      "grad_norm": 3.99072003364563,
      "learning_rate": 4.839860573764509e-05,
      "loss": 0.7456,
      "step": 210600
    },
    {
      "epoch": 1.922585590189065,
      "grad_norm": 4.543864727020264,
      "learning_rate": 4.839784534150911e-05,
      "loss": 0.7648,
      "step": 210700
    },
    {
      "epoch": 1.9234980655522302,
      "grad_norm": 5.091527462005615,
      "learning_rate": 4.839708494537314e-05,
      "loss": 0.7551,
      "step": 210800
    },
    {
      "epoch": 1.9244105409153953,
      "grad_norm": 4.754980564117432,
      "learning_rate": 4.839632454923717e-05,
      "loss": 0.7632,
      "step": 210900
    },
    {
      "epoch": 1.9253230162785604,
      "grad_norm": 4.005566120147705,
      "learning_rate": 4.83955641531012e-05,
      "loss": 0.7459,
      "step": 211000
    },
    {
      "epoch": 1.9262354916417257,
      "grad_norm": 4.045485973358154,
      "learning_rate": 4.839480375696523e-05,
      "loss": 0.7651,
      "step": 211100
    },
    {
      "epoch": 1.927147967004891,
      "grad_norm": 3.879248857498169,
      "learning_rate": 4.839404336082926e-05,
      "loss": 0.7702,
      "step": 211200
    },
    {
      "epoch": 1.928060442368056,
      "grad_norm": 3.9545340538024902,
      "learning_rate": 4.8393282964693285e-05,
      "loss": 0.7496,
      "step": 211300
    },
    {
      "epoch": 1.9289729177312211,
      "grad_norm": 4.164599895477295,
      "learning_rate": 4.839252256855732e-05,
      "loss": 0.7683,
      "step": 211400
    },
    {
      "epoch": 1.9298853930943864,
      "grad_norm": 4.391934394836426,
      "learning_rate": 4.8391762172421345e-05,
      "loss": 0.7771,
      "step": 211500
    },
    {
      "epoch": 1.9307978684575517,
      "grad_norm": 4.2056121826171875,
      "learning_rate": 4.8391001776285375e-05,
      "loss": 0.7634,
      "step": 211600
    },
    {
      "epoch": 1.9317103438207168,
      "grad_norm": 4.264582633972168,
      "learning_rate": 4.8390241380149405e-05,
      "loss": 0.783,
      "step": 211700
    },
    {
      "epoch": 1.932622819183882,
      "grad_norm": 4.241374492645264,
      "learning_rate": 4.8389480984013435e-05,
      "loss": 0.75,
      "step": 211800
    },
    {
      "epoch": 1.9335352945470472,
      "grad_norm": 4.09393310546875,
      "learning_rate": 4.8388720587877465e-05,
      "loss": 0.7396,
      "step": 211900
    },
    {
      "epoch": 1.9344477699102125,
      "grad_norm": 4.123305320739746,
      "learning_rate": 4.8387960191741495e-05,
      "loss": 0.7845,
      "step": 212000
    },
    {
      "epoch": 1.9353602452733776,
      "grad_norm": 4.657686233520508,
      "learning_rate": 4.838719979560552e-05,
      "loss": 0.7456,
      "step": 212100
    },
    {
      "epoch": 1.9362727206365427,
      "grad_norm": 3.9811370372772217,
      "learning_rate": 4.8386439399469555e-05,
      "loss": 0.769,
      "step": 212200
    },
    {
      "epoch": 1.937185195999708,
      "grad_norm": 4.308603763580322,
      "learning_rate": 4.838567900333358e-05,
      "loss": 0.7591,
      "step": 212300
    },
    {
      "epoch": 1.9380976713628733,
      "grad_norm": 4.649340629577637,
      "learning_rate": 4.838491860719761e-05,
      "loss": 0.7667,
      "step": 212400
    },
    {
      "epoch": 1.9390101467260386,
      "grad_norm": 3.9423742294311523,
      "learning_rate": 4.838415821106164e-05,
      "loss": 0.7514,
      "step": 212500
    },
    {
      "epoch": 1.9399226220892036,
      "grad_norm": 5.0284342765808105,
      "learning_rate": 4.838339781492567e-05,
      "loss": 0.8134,
      "step": 212600
    },
    {
      "epoch": 1.9408350974523687,
      "grad_norm": 4.045130729675293,
      "learning_rate": 4.838263741878969e-05,
      "loss": 0.7722,
      "step": 212700
    },
    {
      "epoch": 1.941747572815534,
      "grad_norm": 3.7998499870300293,
      "learning_rate": 4.838187702265373e-05,
      "loss": 0.7721,
      "step": 212800
    },
    {
      "epoch": 1.9426600481786993,
      "grad_norm": 4.308857440948486,
      "learning_rate": 4.838111662651775e-05,
      "loss": 0.7473,
      "step": 212900
    },
    {
      "epoch": 1.9435725235418644,
      "grad_norm": 3.9688589572906494,
      "learning_rate": 4.838035623038178e-05,
      "loss": 0.7727,
      "step": 213000
    },
    {
      "epoch": 1.9444849989050295,
      "grad_norm": 4.815685272216797,
      "learning_rate": 4.837959583424581e-05,
      "loss": 0.757,
      "step": 213100
    },
    {
      "epoch": 1.9453974742681948,
      "grad_norm": 4.769375801086426,
      "learning_rate": 4.8378835438109836e-05,
      "loss": 0.7677,
      "step": 213200
    },
    {
      "epoch": 1.94630994963136,
      "grad_norm": 3.6761724948883057,
      "learning_rate": 4.837807504197387e-05,
      "loss": 0.7353,
      "step": 213300
    },
    {
      "epoch": 1.9472224249945251,
      "grad_norm": 3.734020233154297,
      "learning_rate": 4.8377314645837896e-05,
      "loss": 0.7765,
      "step": 213400
    },
    {
      "epoch": 1.9481349003576902,
      "grad_norm": 3.951411247253418,
      "learning_rate": 4.8376554249701926e-05,
      "loss": 0.7714,
      "step": 213500
    },
    {
      "epoch": 1.9490473757208555,
      "grad_norm": 6.1570658683776855,
      "learning_rate": 4.8375793853565956e-05,
      "loss": 0.7696,
      "step": 213600
    },
    {
      "epoch": 1.9499598510840208,
      "grad_norm": 3.8618698120117188,
      "learning_rate": 4.8375033457429986e-05,
      "loss": 0.76,
      "step": 213700
    },
    {
      "epoch": 1.950872326447186,
      "grad_norm": 5.639787197113037,
      "learning_rate": 4.837427306129401e-05,
      "loss": 0.7796,
      "step": 213800
    },
    {
      "epoch": 1.951784801810351,
      "grad_norm": 5.221010684967041,
      "learning_rate": 4.8373512665158046e-05,
      "loss": 0.7799,
      "step": 213900
    },
    {
      "epoch": 1.9526972771735163,
      "grad_norm": 5.089977741241455,
      "learning_rate": 4.837275226902207e-05,
      "loss": 0.7992,
      "step": 214000
    },
    {
      "epoch": 1.9536097525366816,
      "grad_norm": 4.303689956665039,
      "learning_rate": 4.83719918728861e-05,
      "loss": 0.7189,
      "step": 214100
    },
    {
      "epoch": 1.9545222278998469,
      "grad_norm": 4.476932048797607,
      "learning_rate": 4.837123147675013e-05,
      "loss": 0.7691,
      "step": 214200
    },
    {
      "epoch": 1.955434703263012,
      "grad_norm": 4.155200958251953,
      "learning_rate": 4.837047108061416e-05,
      "loss": 0.7761,
      "step": 214300
    },
    {
      "epoch": 1.956347178626177,
      "grad_norm": 3.707150459289551,
      "learning_rate": 4.836971068447819e-05,
      "loss": 0.8039,
      "step": 214400
    },
    {
      "epoch": 1.9572596539893423,
      "grad_norm": 4.175279140472412,
      "learning_rate": 4.836895028834222e-05,
      "loss": 0.7581,
      "step": 214500
    },
    {
      "epoch": 1.9581721293525076,
      "grad_norm": 3.8223869800567627,
      "learning_rate": 4.836818989220624e-05,
      "loss": 0.7589,
      "step": 214600
    },
    {
      "epoch": 1.9590846047156727,
      "grad_norm": 4.350831985473633,
      "learning_rate": 4.836742949607028e-05,
      "loss": 0.7709,
      "step": 214700
    },
    {
      "epoch": 1.9599970800788378,
      "grad_norm": 3.9545114040374756,
      "learning_rate": 4.83666690999343e-05,
      "loss": 0.8015,
      "step": 214800
    },
    {
      "epoch": 1.960909555442003,
      "grad_norm": 4.3949294090271,
      "learning_rate": 4.836590870379833e-05,
      "loss": 0.7454,
      "step": 214900
    },
    {
      "epoch": 1.9618220308051684,
      "grad_norm": 3.503653049468994,
      "learning_rate": 4.836514830766236e-05,
      "loss": 0.7821,
      "step": 215000
    },
    {
      "epoch": 1.9627345061683334,
      "grad_norm": 2.8809967041015625,
      "learning_rate": 4.836438791152639e-05,
      "loss": 0.743,
      "step": 215100
    },
    {
      "epoch": 1.9636469815314985,
      "grad_norm": 4.074432849884033,
      "learning_rate": 4.836362751539042e-05,
      "loss": 0.7988,
      "step": 215200
    },
    {
      "epoch": 1.9645594568946638,
      "grad_norm": 3.975098133087158,
      "learning_rate": 4.8362867119254454e-05,
      "loss": 0.7502,
      "step": 215300
    },
    {
      "epoch": 1.9654719322578291,
      "grad_norm": 2.787365198135376,
      "learning_rate": 4.836210672311848e-05,
      "loss": 0.7682,
      "step": 215400
    },
    {
      "epoch": 1.9663844076209942,
      "grad_norm": 4.569724082946777,
      "learning_rate": 4.836134632698251e-05,
      "loss": 0.7299,
      "step": 215500
    },
    {
      "epoch": 1.9672968829841593,
      "grad_norm": 4.007027626037598,
      "learning_rate": 4.836058593084654e-05,
      "loss": 0.7757,
      "step": 215600
    },
    {
      "epoch": 1.9682093583473246,
      "grad_norm": 4.620784759521484,
      "learning_rate": 4.835982553471057e-05,
      "loss": 0.7419,
      "step": 215700
    },
    {
      "epoch": 1.9691218337104899,
      "grad_norm": 4.889620780944824,
      "learning_rate": 4.83590651385746e-05,
      "loss": 0.7777,
      "step": 215800
    },
    {
      "epoch": 1.9700343090736552,
      "grad_norm": 4.535297870635986,
      "learning_rate": 4.835830474243862e-05,
      "loss": 0.7889,
      "step": 215900
    },
    {
      "epoch": 1.9709467844368203,
      "grad_norm": 3.946476697921753,
      "learning_rate": 4.835754434630265e-05,
      "loss": 0.8153,
      "step": 216000
    },
    {
      "epoch": 1.9718592597999853,
      "grad_norm": 3.9288594722747803,
      "learning_rate": 4.835678395016668e-05,
      "loss": 0.7961,
      "step": 216100
    },
    {
      "epoch": 1.9727717351631506,
      "grad_norm": 3.643131732940674,
      "learning_rate": 4.835602355403071e-05,
      "loss": 0.7693,
      "step": 216200
    },
    {
      "epoch": 1.973684210526316,
      "grad_norm": 5.194296360015869,
      "learning_rate": 4.8355263157894734e-05,
      "loss": 0.7787,
      "step": 216300
    },
    {
      "epoch": 1.974596685889481,
      "grad_norm": 3.490907907485962,
      "learning_rate": 4.835450276175877e-05,
      "loss": 0.7529,
      "step": 216400
    },
    {
      "epoch": 1.975509161252646,
      "grad_norm": 4.4233808517456055,
      "learning_rate": 4.8353742365622794e-05,
      "loss": 0.7611,
      "step": 216500
    },
    {
      "epoch": 1.9764216366158114,
      "grad_norm": 4.24946928024292,
      "learning_rate": 4.8352981969486824e-05,
      "loss": 0.7175,
      "step": 216600
    },
    {
      "epoch": 1.9773341119789767,
      "grad_norm": 4.6864471435546875,
      "learning_rate": 4.8352221573350854e-05,
      "loss": 0.7622,
      "step": 216700
    },
    {
      "epoch": 1.9782465873421418,
      "grad_norm": 4.382858753204346,
      "learning_rate": 4.8351461177214884e-05,
      "loss": 0.7482,
      "step": 216800
    },
    {
      "epoch": 1.9791590627053068,
      "grad_norm": 6.085351467132568,
      "learning_rate": 4.8350700781078914e-05,
      "loss": 0.7816,
      "step": 216900
    },
    {
      "epoch": 1.9800715380684721,
      "grad_norm": 4.017479419708252,
      "learning_rate": 4.8349940384942944e-05,
      "loss": 0.7453,
      "step": 217000
    },
    {
      "epoch": 1.9809840134316374,
      "grad_norm": 3.847500801086426,
      "learning_rate": 4.834917998880697e-05,
      "loss": 0.7469,
      "step": 217100
    },
    {
      "epoch": 1.9818964887948025,
      "grad_norm": 4.231930255889893,
      "learning_rate": 4.8348419592671004e-05,
      "loss": 0.7758,
      "step": 217200
    },
    {
      "epoch": 1.9828089641579676,
      "grad_norm": 4.543985843658447,
      "learning_rate": 4.834765919653503e-05,
      "loss": 0.8011,
      "step": 217300
    },
    {
      "epoch": 1.983721439521133,
      "grad_norm": 3.6114234924316406,
      "learning_rate": 4.834689880039906e-05,
      "loss": 0.8242,
      "step": 217400
    },
    {
      "epoch": 1.9846339148842982,
      "grad_norm": 3.452584981918335,
      "learning_rate": 4.834613840426309e-05,
      "loss": 0.7959,
      "step": 217500
    },
    {
      "epoch": 1.9855463902474633,
      "grad_norm": 3.6902782917022705,
      "learning_rate": 4.834537800812712e-05,
      "loss": 0.7744,
      "step": 217600
    },
    {
      "epoch": 1.9864588656106286,
      "grad_norm": 5.131017208099365,
      "learning_rate": 4.834461761199114e-05,
      "loss": 0.7349,
      "step": 217700
    },
    {
      "epoch": 1.9873713409737936,
      "grad_norm": 3.1268961429595947,
      "learning_rate": 4.834385721585518e-05,
      "loss": 0.7382,
      "step": 217800
    },
    {
      "epoch": 1.988283816336959,
      "grad_norm": 4.114807605743408,
      "learning_rate": 4.83430968197192e-05,
      "loss": 0.7474,
      "step": 217900
    },
    {
      "epoch": 1.9891962917001242,
      "grad_norm": 4.660799026489258,
      "learning_rate": 4.834233642358323e-05,
      "loss": 0.7229,
      "step": 218000
    },
    {
      "epoch": 1.9901087670632893,
      "grad_norm": 4.345290660858154,
      "learning_rate": 4.834157602744726e-05,
      "loss": 0.7373,
      "step": 218100
    },
    {
      "epoch": 1.9910212424264544,
      "grad_norm": 4.656675338745117,
      "learning_rate": 4.834081563131129e-05,
      "loss": 0.7805,
      "step": 218200
    },
    {
      "epoch": 1.9919337177896197,
      "grad_norm": 3.389254093170166,
      "learning_rate": 4.834005523517532e-05,
      "loss": 0.8156,
      "step": 218300
    },
    {
      "epoch": 1.992846193152785,
      "grad_norm": 3.811876058578491,
      "learning_rate": 4.833929483903935e-05,
      "loss": 0.7895,
      "step": 218400
    },
    {
      "epoch": 1.99375866851595,
      "grad_norm": 3.83652925491333,
      "learning_rate": 4.8338534442903375e-05,
      "loss": 0.7793,
      "step": 218500
    },
    {
      "epoch": 1.9946711438791151,
      "grad_norm": 4.453648090362549,
      "learning_rate": 4.833777404676741e-05,
      "loss": 0.7749,
      "step": 218600
    },
    {
      "epoch": 1.9955836192422804,
      "grad_norm": 5.90048360824585,
      "learning_rate": 4.8337013650631435e-05,
      "loss": 0.7542,
      "step": 218700
    },
    {
      "epoch": 1.9964960946054457,
      "grad_norm": 3.6060187816619873,
      "learning_rate": 4.833625325449546e-05,
      "loss": 0.7725,
      "step": 218800
    },
    {
      "epoch": 1.9974085699686108,
      "grad_norm": 4.024191856384277,
      "learning_rate": 4.8335492858359495e-05,
      "loss": 0.735,
      "step": 218900
    },
    {
      "epoch": 1.998321045331776,
      "grad_norm": 4.357305526733398,
      "learning_rate": 4.833473246222352e-05,
      "loss": 0.7877,
      "step": 219000
    },
    {
      "epoch": 1.9992335206949412,
      "grad_norm": 3.879009246826172,
      "learning_rate": 4.833397206608755e-05,
      "loss": 0.7733,
      "step": 219100
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.614923894405365,
      "eval_runtime": 25.7945,
      "eval_samples_per_second": 223.652,
      "eval_steps_per_second": 223.652,
      "step": 219184
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.6026489734649658,
      "eval_runtime": 483.2987,
      "eval_samples_per_second": 226.758,
      "eval_steps_per_second": 226.758,
      "step": 219184
    },
    {
      "epoch": 2.0001459960581065,
      "grad_norm": 3.9848127365112305,
      "learning_rate": 4.833321166995158e-05,
      "loss": 0.7377,
      "step": 219200
    },
    {
      "epoch": 2.001058471421272,
      "grad_norm": 4.740444183349609,
      "learning_rate": 4.833245127381561e-05,
      "loss": 0.7729,
      "step": 219300
    },
    {
      "epoch": 2.0019709467844367,
      "grad_norm": 4.986690521240234,
      "learning_rate": 4.833169087767964e-05,
      "loss": 0.7694,
      "step": 219400
    },
    {
      "epoch": 2.002883422147602,
      "grad_norm": 4.404385089874268,
      "learning_rate": 4.833093048154367e-05,
      "loss": 0.7245,
      "step": 219500
    },
    {
      "epoch": 2.0037958975107673,
      "grad_norm": 3.6096160411834717,
      "learning_rate": 4.833017008540769e-05,
      "loss": 0.7708,
      "step": 219600
    },
    {
      "epoch": 2.0047083728739326,
      "grad_norm": 4.174703121185303,
      "learning_rate": 4.832940968927173e-05,
      "loss": 0.7391,
      "step": 219700
    },
    {
      "epoch": 2.0056208482370974,
      "grad_norm": 3.8977468013763428,
      "learning_rate": 4.832864929313575e-05,
      "loss": 0.7811,
      "step": 219800
    },
    {
      "epoch": 2.0065333236002627,
      "grad_norm": 4.233883380889893,
      "learning_rate": 4.832788889699978e-05,
      "loss": 0.8129,
      "step": 219900
    },
    {
      "epoch": 2.007445798963428,
      "grad_norm": 4.054078102111816,
      "learning_rate": 4.832712850086381e-05,
      "loss": 0.7519,
      "step": 220000
    },
    {
      "epoch": 2.0083582743265933,
      "grad_norm": 4.696791172027588,
      "learning_rate": 4.832636810472784e-05,
      "loss": 0.718,
      "step": 220100
    },
    {
      "epoch": 2.009270749689758,
      "grad_norm": 5.321535587310791,
      "learning_rate": 4.8325607708591866e-05,
      "loss": 0.7298,
      "step": 220200
    },
    {
      "epoch": 2.0101832250529235,
      "grad_norm": 4.560953140258789,
      "learning_rate": 4.83248473124559e-05,
      "loss": 0.7674,
      "step": 220300
    },
    {
      "epoch": 2.0110957004160888,
      "grad_norm": 4.513675212860107,
      "learning_rate": 4.8324086916319926e-05,
      "loss": 0.7573,
      "step": 220400
    },
    {
      "epoch": 2.012008175779254,
      "grad_norm": 4.187802314758301,
      "learning_rate": 4.8323326520183956e-05,
      "loss": 0.7649,
      "step": 220500
    },
    {
      "epoch": 2.0129206511424194,
      "grad_norm": 4.343139171600342,
      "learning_rate": 4.8322566124047986e-05,
      "loss": 0.7445,
      "step": 220600
    },
    {
      "epoch": 2.013833126505584,
      "grad_norm": 4.744040012359619,
      "learning_rate": 4.8321805727912016e-05,
      "loss": 0.7336,
      "step": 220700
    },
    {
      "epoch": 2.0147456018687495,
      "grad_norm": 5.105003356933594,
      "learning_rate": 4.8321045331776046e-05,
      "loss": 0.7608,
      "step": 220800
    },
    {
      "epoch": 2.015658077231915,
      "grad_norm": 3.967154026031494,
      "learning_rate": 4.8320284935640076e-05,
      "loss": 0.7517,
      "step": 220900
    },
    {
      "epoch": 2.01657055259508,
      "grad_norm": 4.607671737670898,
      "learning_rate": 4.83195245395041e-05,
      "loss": 0.7433,
      "step": 221000
    },
    {
      "epoch": 2.017483027958245,
      "grad_norm": 5.644398212432861,
      "learning_rate": 4.8318764143368136e-05,
      "loss": 0.762,
      "step": 221100
    },
    {
      "epoch": 2.0183955033214103,
      "grad_norm": 3.1784327030181885,
      "learning_rate": 4.831800374723216e-05,
      "loss": 0.784,
      "step": 221200
    },
    {
      "epoch": 2.0193079786845756,
      "grad_norm": 4.511801242828369,
      "learning_rate": 4.831724335109619e-05,
      "loss": 0.7244,
      "step": 221300
    },
    {
      "epoch": 2.020220454047741,
      "grad_norm": 4.33358097076416,
      "learning_rate": 4.831648295496022e-05,
      "loss": 0.7665,
      "step": 221400
    },
    {
      "epoch": 2.0211329294109057,
      "grad_norm": 4.6959404945373535,
      "learning_rate": 4.831572255882425e-05,
      "loss": 0.7284,
      "step": 221500
    },
    {
      "epoch": 2.022045404774071,
      "grad_norm": 4.255618095397949,
      "learning_rate": 4.831496216268827e-05,
      "loss": 0.7664,
      "step": 221600
    },
    {
      "epoch": 2.0229578801372363,
      "grad_norm": 4.601779460906982,
      "learning_rate": 4.83142017665523e-05,
      "loss": 0.7239,
      "step": 221700
    },
    {
      "epoch": 2.0238703555004016,
      "grad_norm": 2.756661891937256,
      "learning_rate": 4.831344137041633e-05,
      "loss": 0.722,
      "step": 221800
    },
    {
      "epoch": 2.0247828308635665,
      "grad_norm": 3.9832069873809814,
      "learning_rate": 4.8312680974280363e-05,
      "loss": 0.7674,
      "step": 221900
    },
    {
      "epoch": 2.0256953062267318,
      "grad_norm": 4.195547580718994,
      "learning_rate": 4.8311920578144393e-05,
      "loss": 0.7679,
      "step": 222000
    },
    {
      "epoch": 2.026607781589897,
      "grad_norm": 3.017793655395508,
      "learning_rate": 4.831116018200842e-05,
      "loss": 0.783,
      "step": 222100
    },
    {
      "epoch": 2.0275202569530624,
      "grad_norm": 4.391787528991699,
      "learning_rate": 4.8310399785872454e-05,
      "loss": 0.7738,
      "step": 222200
    },
    {
      "epoch": 2.0284327323162277,
      "grad_norm": 4.41478967666626,
      "learning_rate": 4.830963938973648e-05,
      "loss": 0.7602,
      "step": 222300
    },
    {
      "epoch": 2.0293452076793925,
      "grad_norm": 1.7778769731521606,
      "learning_rate": 4.830887899360051e-05,
      "loss": 0.753,
      "step": 222400
    },
    {
      "epoch": 2.030257683042558,
      "grad_norm": 4.722476482391357,
      "learning_rate": 4.830811859746454e-05,
      "loss": 0.774,
      "step": 222500
    },
    {
      "epoch": 2.031170158405723,
      "grad_norm": 3.7610766887664795,
      "learning_rate": 4.830735820132857e-05,
      "loss": 0.77,
      "step": 222600
    },
    {
      "epoch": 2.0320826337688884,
      "grad_norm": 4.656155109405518,
      "learning_rate": 4.83065978051926e-05,
      "loss": 0.783,
      "step": 222700
    },
    {
      "epoch": 2.0329951091320533,
      "grad_norm": 3.6576390266418457,
      "learning_rate": 4.830583740905663e-05,
      "loss": 0.6978,
      "step": 222800
    },
    {
      "epoch": 2.0339075844952186,
      "grad_norm": 4.8795013427734375,
      "learning_rate": 4.830507701292065e-05,
      "loss": 0.7332,
      "step": 222900
    },
    {
      "epoch": 2.034820059858384,
      "grad_norm": 2.884868860244751,
      "learning_rate": 4.830431661678468e-05,
      "loss": 0.8006,
      "step": 223000
    },
    {
      "epoch": 2.035732535221549,
      "grad_norm": 3.8598482608795166,
      "learning_rate": 4.830355622064871e-05,
      "loss": 0.6898,
      "step": 223100
    },
    {
      "epoch": 2.036645010584714,
      "grad_norm": 4.972919940948486,
      "learning_rate": 4.830279582451274e-05,
      "loss": 0.7329,
      "step": 223200
    },
    {
      "epoch": 2.0375574859478793,
      "grad_norm": 4.565117835998535,
      "learning_rate": 4.830203542837677e-05,
      "loss": 0.7568,
      "step": 223300
    },
    {
      "epoch": 2.0384699613110446,
      "grad_norm": 4.401445388793945,
      "learning_rate": 4.83012750322408e-05,
      "loss": 0.7698,
      "step": 223400
    },
    {
      "epoch": 2.03938243667421,
      "grad_norm": 4.1698126792907715,
      "learning_rate": 4.8300514636104824e-05,
      "loss": 0.7404,
      "step": 223500
    },
    {
      "epoch": 2.040294912037375,
      "grad_norm": 4.342547416687012,
      "learning_rate": 4.829975423996886e-05,
      "loss": 0.7674,
      "step": 223600
    },
    {
      "epoch": 2.04120738740054,
      "grad_norm": 3.191192626953125,
      "learning_rate": 4.8298993843832884e-05,
      "loss": 0.7414,
      "step": 223700
    },
    {
      "epoch": 2.0421198627637054,
      "grad_norm": 3.600412368774414,
      "learning_rate": 4.8298233447696914e-05,
      "loss": 0.7513,
      "step": 223800
    },
    {
      "epoch": 2.0430323381268707,
      "grad_norm": 4.197786808013916,
      "learning_rate": 4.8297473051560944e-05,
      "loss": 0.7839,
      "step": 223900
    },
    {
      "epoch": 2.043944813490036,
      "grad_norm": 5.19450569152832,
      "learning_rate": 4.8296712655424974e-05,
      "loss": 0.7944,
      "step": 224000
    },
    {
      "epoch": 2.044857288853201,
      "grad_norm": 4.067723274230957,
      "learning_rate": 4.8295952259289005e-05,
      "loss": 0.7587,
      "step": 224100
    },
    {
      "epoch": 2.045769764216366,
      "grad_norm": 4.068368434906006,
      "learning_rate": 4.8295191863153035e-05,
      "loss": 0.7893,
      "step": 224200
    },
    {
      "epoch": 2.0466822395795314,
      "grad_norm": 3.5190892219543457,
      "learning_rate": 4.829443146701706e-05,
      "loss": 0.7178,
      "step": 224300
    },
    {
      "epoch": 2.0475947149426967,
      "grad_norm": 4.2558913230896,
      "learning_rate": 4.829367107088109e-05,
      "loss": 0.7776,
      "step": 224400
    },
    {
      "epoch": 2.0485071903058616,
      "grad_norm": 3.0543973445892334,
      "learning_rate": 4.829291067474512e-05,
      "loss": 0.7485,
      "step": 224500
    },
    {
      "epoch": 2.049419665669027,
      "grad_norm": 3.28582763671875,
      "learning_rate": 4.829215027860914e-05,
      "loss": 0.7303,
      "step": 224600
    },
    {
      "epoch": 2.050332141032192,
      "grad_norm": 3.419410228729248,
      "learning_rate": 4.829138988247318e-05,
      "loss": 0.7436,
      "step": 224700
    },
    {
      "epoch": 2.0512446163953575,
      "grad_norm": 4.002412796020508,
      "learning_rate": 4.82906294863372e-05,
      "loss": 0.7689,
      "step": 224800
    },
    {
      "epoch": 2.0521570917585223,
      "grad_norm": 3.9553213119506836,
      "learning_rate": 4.828986909020123e-05,
      "loss": 0.7584,
      "step": 224900
    },
    {
      "epoch": 2.0530695671216876,
      "grad_norm": 3.944655418395996,
      "learning_rate": 4.828910869406526e-05,
      "loss": 0.7406,
      "step": 225000
    },
    {
      "epoch": 2.053982042484853,
      "grad_norm": 3.225395917892456,
      "learning_rate": 4.828834829792929e-05,
      "loss": 0.7685,
      "step": 225100
    },
    {
      "epoch": 2.0548945178480182,
      "grad_norm": 4.435255527496338,
      "learning_rate": 4.828758790179332e-05,
      "loss": 0.8068,
      "step": 225200
    },
    {
      "epoch": 2.055806993211183,
      "grad_norm": 3.7752580642700195,
      "learning_rate": 4.828682750565735e-05,
      "loss": 0.7435,
      "step": 225300
    },
    {
      "epoch": 2.0567194685743484,
      "grad_norm": 4.810760021209717,
      "learning_rate": 4.8286067109521375e-05,
      "loss": 0.7453,
      "step": 225400
    },
    {
      "epoch": 2.0576319439375137,
      "grad_norm": 4.185915946960449,
      "learning_rate": 4.828530671338541e-05,
      "loss": 0.7638,
      "step": 225500
    },
    {
      "epoch": 2.058544419300679,
      "grad_norm": 3.623621940612793,
      "learning_rate": 4.8284546317249435e-05,
      "loss": 0.791,
      "step": 225600
    },
    {
      "epoch": 2.0594568946638443,
      "grad_norm": 4.012878894805908,
      "learning_rate": 4.8283785921113465e-05,
      "loss": 0.8043,
      "step": 225700
    },
    {
      "epoch": 2.060369370027009,
      "grad_norm": 3.4101500511169434,
      "learning_rate": 4.8283025524977495e-05,
      "loss": 0.7474,
      "step": 225800
    },
    {
      "epoch": 2.0612818453901744,
      "grad_norm": 3.865739345550537,
      "learning_rate": 4.8282265128841525e-05,
      "loss": 0.7699,
      "step": 225900
    },
    {
      "epoch": 2.0621943207533397,
      "grad_norm": 4.311302661895752,
      "learning_rate": 4.828150473270555e-05,
      "loss": 0.7573,
      "step": 226000
    },
    {
      "epoch": 2.063106796116505,
      "grad_norm": 3.9834420680999756,
      "learning_rate": 4.8280744336569586e-05,
      "loss": 0.7719,
      "step": 226100
    },
    {
      "epoch": 2.06401927147967,
      "grad_norm": 4.419888019561768,
      "learning_rate": 4.827998394043361e-05,
      "loss": 0.7421,
      "step": 226200
    },
    {
      "epoch": 2.064931746842835,
      "grad_norm": 4.99076509475708,
      "learning_rate": 4.827922354429764e-05,
      "loss": 0.7581,
      "step": 226300
    },
    {
      "epoch": 2.0658442222060005,
      "grad_norm": 4.592443466186523,
      "learning_rate": 4.827846314816167e-05,
      "loss": 0.7629,
      "step": 226400
    },
    {
      "epoch": 2.066756697569166,
      "grad_norm": 4.42121696472168,
      "learning_rate": 4.82777027520257e-05,
      "loss": 0.7608,
      "step": 226500
    },
    {
      "epoch": 2.0676691729323307,
      "grad_norm": 3.995309352874756,
      "learning_rate": 4.827694235588973e-05,
      "loss": 0.7647,
      "step": 226600
    },
    {
      "epoch": 2.068581648295496,
      "grad_norm": 3.767775774002075,
      "learning_rate": 4.827618195975376e-05,
      "loss": 0.7949,
      "step": 226700
    },
    {
      "epoch": 2.0694941236586613,
      "grad_norm": 4.555110931396484,
      "learning_rate": 4.827542156361778e-05,
      "loss": 0.6812,
      "step": 226800
    },
    {
      "epoch": 2.0704065990218266,
      "grad_norm": 3.5713818073272705,
      "learning_rate": 4.827466116748182e-05,
      "loss": 0.7746,
      "step": 226900
    },
    {
      "epoch": 2.0713190743849914,
      "grad_norm": 3.5380663871765137,
      "learning_rate": 4.827390077134584e-05,
      "loss": 0.7232,
      "step": 227000
    },
    {
      "epoch": 2.0722315497481567,
      "grad_norm": 5.400002479553223,
      "learning_rate": 4.827314037520987e-05,
      "loss": 0.7487,
      "step": 227100
    },
    {
      "epoch": 2.073144025111322,
      "grad_norm": 3.6710569858551025,
      "learning_rate": 4.82723799790739e-05,
      "loss": 0.7533,
      "step": 227200
    },
    {
      "epoch": 2.0740565004744873,
      "grad_norm": 3.3452606201171875,
      "learning_rate": 4.8271619582937926e-05,
      "loss": 0.7266,
      "step": 227300
    },
    {
      "epoch": 2.0749689758376526,
      "grad_norm": 4.477065563201904,
      "learning_rate": 4.8270859186801956e-05,
      "loss": 0.7592,
      "step": 227400
    },
    {
      "epoch": 2.0758814512008175,
      "grad_norm": 3.404742479324341,
      "learning_rate": 4.8270098790665986e-05,
      "loss": 0.735,
      "step": 227500
    },
    {
      "epoch": 2.0767939265639828,
      "grad_norm": 3.8843116760253906,
      "learning_rate": 4.8269338394530016e-05,
      "loss": 0.7581,
      "step": 227600
    },
    {
      "epoch": 2.077706401927148,
      "grad_norm": 4.648898601531982,
      "learning_rate": 4.8268577998394046e-05,
      "loss": 0.72,
      "step": 227700
    },
    {
      "epoch": 2.0786188772903134,
      "grad_norm": 3.7687125205993652,
      "learning_rate": 4.8267817602258076e-05,
      "loss": 0.7986,
      "step": 227800
    },
    {
      "epoch": 2.079531352653478,
      "grad_norm": 4.711369037628174,
      "learning_rate": 4.82670572061221e-05,
      "loss": 0.7667,
      "step": 227900
    },
    {
      "epoch": 2.0804438280166435,
      "grad_norm": 3.438803195953369,
      "learning_rate": 4.8266296809986137e-05,
      "loss": 0.7602,
      "step": 228000
    },
    {
      "epoch": 2.081356303379809,
      "grad_norm": 4.105189800262451,
      "learning_rate": 4.826553641385016e-05,
      "loss": 0.731,
      "step": 228100
    },
    {
      "epoch": 2.082268778742974,
      "grad_norm": 3.792860507965088,
      "learning_rate": 4.826477601771419e-05,
      "loss": 0.7313,
      "step": 228200
    },
    {
      "epoch": 2.083181254106139,
      "grad_norm": 4.514847755432129,
      "learning_rate": 4.826401562157822e-05,
      "loss": 0.7574,
      "step": 228300
    },
    {
      "epoch": 2.0840937294693043,
      "grad_norm": 4.196302890777588,
      "learning_rate": 4.826325522544225e-05,
      "loss": 0.76,
      "step": 228400
    },
    {
      "epoch": 2.0850062048324696,
      "grad_norm": 3.965599775314331,
      "learning_rate": 4.826249482930627e-05,
      "loss": 0.7126,
      "step": 228500
    },
    {
      "epoch": 2.085918680195635,
      "grad_norm": 3.4669229984283447,
      "learning_rate": 4.826173443317031e-05,
      "loss": 0.7489,
      "step": 228600
    },
    {
      "epoch": 2.0868311555587997,
      "grad_norm": 2.8685991764068604,
      "learning_rate": 4.8260974037034333e-05,
      "loss": 0.7183,
      "step": 228700
    },
    {
      "epoch": 2.087743630921965,
      "grad_norm": 5.672389984130859,
      "learning_rate": 4.8260213640898364e-05,
      "loss": 0.7171,
      "step": 228800
    },
    {
      "epoch": 2.0886561062851303,
      "grad_norm": 5.053860187530518,
      "learning_rate": 4.8259453244762394e-05,
      "loss": 0.8202,
      "step": 228900
    },
    {
      "epoch": 2.0895685816482956,
      "grad_norm": 4.130062103271484,
      "learning_rate": 4.8258692848626424e-05,
      "loss": 0.7724,
      "step": 229000
    },
    {
      "epoch": 2.090481057011461,
      "grad_norm": 3.472567319869995,
      "learning_rate": 4.8257932452490454e-05,
      "loss": 0.7676,
      "step": 229100
    },
    {
      "epoch": 2.0913935323746258,
      "grad_norm": 4.818324089050293,
      "learning_rate": 4.8257172056354484e-05,
      "loss": 0.7469,
      "step": 229200
    },
    {
      "epoch": 2.092306007737791,
      "grad_norm": 4.4438910484313965,
      "learning_rate": 4.825641166021851e-05,
      "loss": 0.776,
      "step": 229300
    },
    {
      "epoch": 2.0932184831009564,
      "grad_norm": 3.155036687850952,
      "learning_rate": 4.8255651264082544e-05,
      "loss": 0.7428,
      "step": 229400
    },
    {
      "epoch": 2.0941309584641217,
      "grad_norm": 4.3562116622924805,
      "learning_rate": 4.825489086794657e-05,
      "loss": 0.7608,
      "step": 229500
    },
    {
      "epoch": 2.0950434338272865,
      "grad_norm": 4.453723430633545,
      "learning_rate": 4.82541304718106e-05,
      "loss": 0.7531,
      "step": 229600
    },
    {
      "epoch": 2.095955909190452,
      "grad_norm": 4.50288724899292,
      "learning_rate": 4.825337007567463e-05,
      "loss": 0.7608,
      "step": 229700
    },
    {
      "epoch": 2.096868384553617,
      "grad_norm": 3.582948684692383,
      "learning_rate": 4.825260967953866e-05,
      "loss": 0.7536,
      "step": 229800
    },
    {
      "epoch": 2.0977808599167824,
      "grad_norm": 3.602450132369995,
      "learning_rate": 4.825184928340268e-05,
      "loss": 0.7464,
      "step": 229900
    },
    {
      "epoch": 2.0986933352799473,
      "grad_norm": 4.907379627227783,
      "learning_rate": 4.825108888726672e-05,
      "loss": 0.8022,
      "step": 230000
    },
    {
      "epoch": 2.0996058106431126,
      "grad_norm": 2.730410575866699,
      "learning_rate": 4.825032849113074e-05,
      "loss": 0.7659,
      "step": 230100
    },
    {
      "epoch": 2.100518286006278,
      "grad_norm": 4.031408786773682,
      "learning_rate": 4.824956809499477e-05,
      "loss": 0.741,
      "step": 230200
    },
    {
      "epoch": 2.101430761369443,
      "grad_norm": 4.504700183868408,
      "learning_rate": 4.82488076988588e-05,
      "loss": 0.7501,
      "step": 230300
    },
    {
      "epoch": 2.102343236732608,
      "grad_norm": 4.703200817108154,
      "learning_rate": 4.8248047302722824e-05,
      "loss": 0.7559,
      "step": 230400
    },
    {
      "epoch": 2.1032557120957733,
      "grad_norm": 4.862987995147705,
      "learning_rate": 4.824728690658686e-05,
      "loss": 0.7664,
      "step": 230500
    },
    {
      "epoch": 2.1041681874589386,
      "grad_norm": 3.5768563747406006,
      "learning_rate": 4.8246526510450884e-05,
      "loss": 0.8058,
      "step": 230600
    },
    {
      "epoch": 2.105080662822104,
      "grad_norm": 3.8511099815368652,
      "learning_rate": 4.8245766114314914e-05,
      "loss": 0.7254,
      "step": 230700
    },
    {
      "epoch": 2.105993138185269,
      "grad_norm": 4.283517837524414,
      "learning_rate": 4.8245005718178945e-05,
      "loss": 0.7586,
      "step": 230800
    },
    {
      "epoch": 2.106905613548434,
      "grad_norm": 4.328595161437988,
      "learning_rate": 4.8244245322042975e-05,
      "loss": 0.7579,
      "step": 230900
    },
    {
      "epoch": 2.1078180889115994,
      "grad_norm": 4.056591033935547,
      "learning_rate": 4.8243484925907e-05,
      "loss": 0.7855,
      "step": 231000
    },
    {
      "epoch": 2.1087305642747647,
      "grad_norm": 4.281001091003418,
      "learning_rate": 4.8242724529771035e-05,
      "loss": 0.7427,
      "step": 231100
    },
    {
      "epoch": 2.10964303963793,
      "grad_norm": 4.4775471687316895,
      "learning_rate": 4.824196413363506e-05,
      "loss": 0.7799,
      "step": 231200
    },
    {
      "epoch": 2.110555515001095,
      "grad_norm": 4.596749782562256,
      "learning_rate": 4.824120373749909e-05,
      "loss": 0.7518,
      "step": 231300
    },
    {
      "epoch": 2.11146799036426,
      "grad_norm": 3.524106979370117,
      "learning_rate": 4.824044334136312e-05,
      "loss": 0.7366,
      "step": 231400
    },
    {
      "epoch": 2.1123804657274254,
      "grad_norm": 3.6373050212860107,
      "learning_rate": 4.823968294522715e-05,
      "loss": 0.7461,
      "step": 231500
    },
    {
      "epoch": 2.1132929410905907,
      "grad_norm": 3.9666171073913574,
      "learning_rate": 4.823892254909118e-05,
      "loss": 0.7713,
      "step": 231600
    },
    {
      "epoch": 2.1142054164537556,
      "grad_norm": 4.534517765045166,
      "learning_rate": 4.823816215295521e-05,
      "loss": 0.7422,
      "step": 231700
    },
    {
      "epoch": 2.115117891816921,
      "grad_norm": 4.039590358734131,
      "learning_rate": 4.823740175681923e-05,
      "loss": 0.7825,
      "step": 231800
    },
    {
      "epoch": 2.116030367180086,
      "grad_norm": 4.054225921630859,
      "learning_rate": 4.823664136068327e-05,
      "loss": 0.7476,
      "step": 231900
    },
    {
      "epoch": 2.1169428425432515,
      "grad_norm": 4.512599468231201,
      "learning_rate": 4.823588096454729e-05,
      "loss": 0.7785,
      "step": 232000
    },
    {
      "epoch": 2.1178553179064163,
      "grad_norm": 4.246995449066162,
      "learning_rate": 4.823512056841132e-05,
      "loss": 0.7719,
      "step": 232100
    },
    {
      "epoch": 2.1187677932695816,
      "grad_norm": 4.325327396392822,
      "learning_rate": 4.823436017227535e-05,
      "loss": 0.7816,
      "step": 232200
    },
    {
      "epoch": 2.119680268632747,
      "grad_norm": 5.47965669631958,
      "learning_rate": 4.823359977613938e-05,
      "loss": 0.7455,
      "step": 232300
    },
    {
      "epoch": 2.1205927439959122,
      "grad_norm": 4.429019451141357,
      "learning_rate": 4.8232839380003405e-05,
      "loss": 0.7895,
      "step": 232400
    },
    {
      "epoch": 2.121505219359077,
      "grad_norm": 3.9920268058776855,
      "learning_rate": 4.823207898386744e-05,
      "loss": 0.7092,
      "step": 232500
    },
    {
      "epoch": 2.1224176947222424,
      "grad_norm": 4.159051895141602,
      "learning_rate": 4.8231318587731465e-05,
      "loss": 0.7936,
      "step": 232600
    },
    {
      "epoch": 2.1233301700854077,
      "grad_norm": 3.795196533203125,
      "learning_rate": 4.8230558191595495e-05,
      "loss": 0.7286,
      "step": 232700
    },
    {
      "epoch": 2.124242645448573,
      "grad_norm": 4.337201118469238,
      "learning_rate": 4.8229797795459526e-05,
      "loss": 0.7426,
      "step": 232800
    },
    {
      "epoch": 2.125155120811738,
      "grad_norm": 4.06099796295166,
      "learning_rate": 4.822903739932355e-05,
      "loss": 0.7445,
      "step": 232900
    },
    {
      "epoch": 2.126067596174903,
      "grad_norm": 3.776902675628662,
      "learning_rate": 4.8228277003187586e-05,
      "loss": 0.7508,
      "step": 233000
    },
    {
      "epoch": 2.1269800715380685,
      "grad_norm": 5.036674499511719,
      "learning_rate": 4.822751660705161e-05,
      "loss": 0.7632,
      "step": 233100
    },
    {
      "epoch": 2.1278925469012338,
      "grad_norm": 4.921738624572754,
      "learning_rate": 4.822675621091564e-05,
      "loss": 0.7163,
      "step": 233200
    },
    {
      "epoch": 2.128805022264399,
      "grad_norm": 4.480890274047852,
      "learning_rate": 4.822599581477967e-05,
      "loss": 0.7533,
      "step": 233300
    },
    {
      "epoch": 2.129717497627564,
      "grad_norm": 4.463461875915527,
      "learning_rate": 4.82252354186437e-05,
      "loss": 0.7498,
      "step": 233400
    },
    {
      "epoch": 2.130629972990729,
      "grad_norm": 2.9914603233337402,
      "learning_rate": 4.822447502250772e-05,
      "loss": 0.6986,
      "step": 233500
    },
    {
      "epoch": 2.1315424483538945,
      "grad_norm": 4.071195602416992,
      "learning_rate": 4.822371462637176e-05,
      "loss": 0.7459,
      "step": 233600
    },
    {
      "epoch": 2.13245492371706,
      "grad_norm": 4.79766321182251,
      "learning_rate": 4.822295423023578e-05,
      "loss": 0.802,
      "step": 233700
    },
    {
      "epoch": 2.1333673990802247,
      "grad_norm": 4.453605651855469,
      "learning_rate": 4.822219383409981e-05,
      "loss": 0.7669,
      "step": 233800
    },
    {
      "epoch": 2.13427987444339,
      "grad_norm": 4.405754566192627,
      "learning_rate": 4.822143343796384e-05,
      "loss": 0.7646,
      "step": 233900
    },
    {
      "epoch": 2.1351923498065553,
      "grad_norm": 4.636214733123779,
      "learning_rate": 4.822067304182787e-05,
      "loss": 0.7426,
      "step": 234000
    },
    {
      "epoch": 2.1361048251697206,
      "grad_norm": 4.104055404663086,
      "learning_rate": 4.82199126456919e-05,
      "loss": 0.8062,
      "step": 234100
    },
    {
      "epoch": 2.1370173005328854,
      "grad_norm": 4.4452009201049805,
      "learning_rate": 4.821915224955593e-05,
      "loss": 0.7413,
      "step": 234200
    },
    {
      "epoch": 2.1379297758960507,
      "grad_norm": 4.048795700073242,
      "learning_rate": 4.8218391853419956e-05,
      "loss": 0.7271,
      "step": 234300
    },
    {
      "epoch": 2.138842251259216,
      "grad_norm": 4.368444919586182,
      "learning_rate": 4.821763145728399e-05,
      "loss": 0.7519,
      "step": 234400
    },
    {
      "epoch": 2.1397547266223813,
      "grad_norm": 3.7129266262054443,
      "learning_rate": 4.8216871061148016e-05,
      "loss": 0.744,
      "step": 234500
    },
    {
      "epoch": 2.140667201985546,
      "grad_norm": 4.5649518966674805,
      "learning_rate": 4.8216110665012046e-05,
      "loss": 0.7985,
      "step": 234600
    },
    {
      "epoch": 2.1415796773487115,
      "grad_norm": 4.258249282836914,
      "learning_rate": 4.8215350268876076e-05,
      "loss": 0.7484,
      "step": 234700
    },
    {
      "epoch": 2.1424921527118768,
      "grad_norm": 4.455859661102295,
      "learning_rate": 4.8214589872740107e-05,
      "loss": 0.7628,
      "step": 234800
    },
    {
      "epoch": 2.143404628075042,
      "grad_norm": 2.900106906890869,
      "learning_rate": 4.821382947660413e-05,
      "loss": 0.7827,
      "step": 234900
    },
    {
      "epoch": 2.1443171034382074,
      "grad_norm": 4.556654930114746,
      "learning_rate": 4.821306908046817e-05,
      "loss": 0.7723,
      "step": 235000
    },
    {
      "epoch": 2.145229578801372,
      "grad_norm": 4.1409711837768555,
      "learning_rate": 4.821230868433219e-05,
      "loss": 0.7784,
      "step": 235100
    },
    {
      "epoch": 2.1461420541645375,
      "grad_norm": 2.8526875972747803,
      "learning_rate": 4.821154828819622e-05,
      "loss": 0.7059,
      "step": 235200
    },
    {
      "epoch": 2.147054529527703,
      "grad_norm": 2.7624213695526123,
      "learning_rate": 4.821078789206025e-05,
      "loss": 0.7801,
      "step": 235300
    },
    {
      "epoch": 2.147967004890868,
      "grad_norm": 4.334150314331055,
      "learning_rate": 4.821002749592428e-05,
      "loss": 0.7889,
      "step": 235400
    },
    {
      "epoch": 2.148879480254033,
      "grad_norm": 4.533090591430664,
      "learning_rate": 4.820926709978831e-05,
      "loss": 0.7319,
      "step": 235500
    },
    {
      "epoch": 2.1497919556171983,
      "grad_norm": 4.5810112953186035,
      "learning_rate": 4.820850670365234e-05,
      "loss": 0.796,
      "step": 235600
    },
    {
      "epoch": 2.1507044309803636,
      "grad_norm": 4.321965217590332,
      "learning_rate": 4.8207746307516364e-05,
      "loss": 0.7198,
      "step": 235700
    },
    {
      "epoch": 2.151616906343529,
      "grad_norm": 5.4777750968933105,
      "learning_rate": 4.8206985911380394e-05,
      "loss": 0.7327,
      "step": 235800
    },
    {
      "epoch": 2.1525293817066937,
      "grad_norm": 4.350769996643066,
      "learning_rate": 4.8206225515244424e-05,
      "loss": 0.7559,
      "step": 235900
    },
    {
      "epoch": 2.153441857069859,
      "grad_norm": 5.0255913734436035,
      "learning_rate": 4.8205465119108454e-05,
      "loss": 0.7214,
      "step": 236000
    },
    {
      "epoch": 2.1543543324330243,
      "grad_norm": 4.568368434906006,
      "learning_rate": 4.8204704722972484e-05,
      "loss": 0.7522,
      "step": 236100
    },
    {
      "epoch": 2.1552668077961896,
      "grad_norm": 5.148994445800781,
      "learning_rate": 4.820394432683651e-05,
      "loss": 0.7707,
      "step": 236200
    },
    {
      "epoch": 2.1561792831593545,
      "grad_norm": 4.56697940826416,
      "learning_rate": 4.820318393070054e-05,
      "loss": 0.7803,
      "step": 236300
    },
    {
      "epoch": 2.1570917585225198,
      "grad_norm": 4.35698127746582,
      "learning_rate": 4.820242353456457e-05,
      "loss": 0.7123,
      "step": 236400
    },
    {
      "epoch": 2.158004233885685,
      "grad_norm": 4.219667911529541,
      "learning_rate": 4.82016631384286e-05,
      "loss": 0.7672,
      "step": 236500
    },
    {
      "epoch": 2.1589167092488504,
      "grad_norm": 4.160414695739746,
      "learning_rate": 4.820090274229263e-05,
      "loss": 0.754,
      "step": 236600
    },
    {
      "epoch": 2.1598291846120157,
      "grad_norm": 4.415648937225342,
      "learning_rate": 4.820014234615666e-05,
      "loss": 0.7769,
      "step": 236700
    },
    {
      "epoch": 2.1607416599751805,
      "grad_norm": 4.882623195648193,
      "learning_rate": 4.819938195002068e-05,
      "loss": 0.7589,
      "step": 236800
    },
    {
      "epoch": 2.161654135338346,
      "grad_norm": 4.59699821472168,
      "learning_rate": 4.819862155388472e-05,
      "loss": 0.7796,
      "step": 236900
    },
    {
      "epoch": 2.162566610701511,
      "grad_norm": 3.9639697074890137,
      "learning_rate": 4.819786115774874e-05,
      "loss": 0.735,
      "step": 237000
    },
    {
      "epoch": 2.1634790860646764,
      "grad_norm": 3.5678114891052246,
      "learning_rate": 4.819710076161277e-05,
      "loss": 0.7345,
      "step": 237100
    },
    {
      "epoch": 2.1643915614278413,
      "grad_norm": 4.175538539886475,
      "learning_rate": 4.81963403654768e-05,
      "loss": 0.7494,
      "step": 237200
    },
    {
      "epoch": 2.1653040367910066,
      "grad_norm": 3.5612292289733887,
      "learning_rate": 4.819557996934083e-05,
      "loss": 0.7608,
      "step": 237300
    },
    {
      "epoch": 2.166216512154172,
      "grad_norm": 3.918041706085205,
      "learning_rate": 4.819481957320486e-05,
      "loss": 0.7329,
      "step": 237400
    },
    {
      "epoch": 2.167128987517337,
      "grad_norm": 3.991248369216919,
      "learning_rate": 4.819405917706889e-05,
      "loss": 0.7342,
      "step": 237500
    },
    {
      "epoch": 2.168041462880502,
      "grad_norm": 4.2018609046936035,
      "learning_rate": 4.8193298780932915e-05,
      "loss": 0.7218,
      "step": 237600
    },
    {
      "epoch": 2.1689539382436673,
      "grad_norm": 3.6165456771850586,
      "learning_rate": 4.819253838479695e-05,
      "loss": 0.7656,
      "step": 237700
    },
    {
      "epoch": 2.1698664136068326,
      "grad_norm": 3.926910877227783,
      "learning_rate": 4.8191777988660975e-05,
      "loss": 0.7214,
      "step": 237800
    },
    {
      "epoch": 2.170778888969998,
      "grad_norm": 3.9072787761688232,
      "learning_rate": 4.8191017592525005e-05,
      "loss": 0.7779,
      "step": 237900
    },
    {
      "epoch": 2.171691364333163,
      "grad_norm": 4.507026195526123,
      "learning_rate": 4.8190257196389035e-05,
      "loss": 0.7221,
      "step": 238000
    },
    {
      "epoch": 2.172603839696328,
      "grad_norm": 4.535642147064209,
      "learning_rate": 4.8189496800253065e-05,
      "loss": 0.7564,
      "step": 238100
    },
    {
      "epoch": 2.1735163150594934,
      "grad_norm": 3.7950563430786133,
      "learning_rate": 4.818873640411709e-05,
      "loss": 0.7845,
      "step": 238200
    },
    {
      "epoch": 2.1744287904226587,
      "grad_norm": 3.761503219604492,
      "learning_rate": 4.8187976007981125e-05,
      "loss": 0.7726,
      "step": 238300
    },
    {
      "epoch": 2.175341265785824,
      "grad_norm": 3.8555309772491455,
      "learning_rate": 4.818721561184515e-05,
      "loss": 0.7664,
      "step": 238400
    },
    {
      "epoch": 2.176253741148989,
      "grad_norm": 4.170435428619385,
      "learning_rate": 4.818645521570918e-05,
      "loss": 0.7352,
      "step": 238500
    },
    {
      "epoch": 2.177166216512154,
      "grad_norm": 4.901576519012451,
      "learning_rate": 4.818569481957321e-05,
      "loss": 0.762,
      "step": 238600
    },
    {
      "epoch": 2.1780786918753194,
      "grad_norm": 4.571393966674805,
      "learning_rate": 4.818493442343723e-05,
      "loss": 0.7439,
      "step": 238700
    },
    {
      "epoch": 2.1789911672384847,
      "grad_norm": 4.2294135093688965,
      "learning_rate": 4.818417402730127e-05,
      "loss": 0.7572,
      "step": 238800
    },
    {
      "epoch": 2.1799036426016496,
      "grad_norm": 4.292881011962891,
      "learning_rate": 4.818341363116529e-05,
      "loss": 0.7847,
      "step": 238900
    },
    {
      "epoch": 2.180816117964815,
      "grad_norm": 3.856762170791626,
      "learning_rate": 4.818265323502932e-05,
      "loss": 0.7373,
      "step": 239000
    },
    {
      "epoch": 2.18172859332798,
      "grad_norm": 4.745737552642822,
      "learning_rate": 4.818189283889335e-05,
      "loss": 0.793,
      "step": 239100
    },
    {
      "epoch": 2.1826410686911455,
      "grad_norm": 4.258452415466309,
      "learning_rate": 4.818113244275738e-05,
      "loss": 0.7564,
      "step": 239200
    },
    {
      "epoch": 2.1835535440543103,
      "grad_norm": 4.851417541503906,
      "learning_rate": 4.8180372046621405e-05,
      "loss": 0.7315,
      "step": 239300
    },
    {
      "epoch": 2.1844660194174756,
      "grad_norm": 4.491654396057129,
      "learning_rate": 4.817961165048544e-05,
      "loss": 0.7269,
      "step": 239400
    },
    {
      "epoch": 2.185378494780641,
      "grad_norm": 2.9144222736358643,
      "learning_rate": 4.8178851254349466e-05,
      "loss": 0.7176,
      "step": 239500
    },
    {
      "epoch": 2.1862909701438062,
      "grad_norm": 4.994063377380371,
      "learning_rate": 4.8178090858213496e-05,
      "loss": 0.7617,
      "step": 239600
    },
    {
      "epoch": 2.187203445506971,
      "grad_norm": 3.876164436340332,
      "learning_rate": 4.8177330462077526e-05,
      "loss": 0.7899,
      "step": 239700
    },
    {
      "epoch": 2.1881159208701364,
      "grad_norm": 3.8771004676818848,
      "learning_rate": 4.8176570065941556e-05,
      "loss": 0.7616,
      "step": 239800
    },
    {
      "epoch": 2.1890283962333017,
      "grad_norm": 3.9436652660369873,
      "learning_rate": 4.8175809669805586e-05,
      "loss": 0.7496,
      "step": 239900
    },
    {
      "epoch": 2.189940871596467,
      "grad_norm": 4.511953830718994,
      "learning_rate": 4.8175049273669616e-05,
      "loss": 0.7216,
      "step": 240000
    },
    {
      "epoch": 2.1908533469596323,
      "grad_norm": 3.9553582668304443,
      "learning_rate": 4.817428887753364e-05,
      "loss": 0.7424,
      "step": 240100
    },
    {
      "epoch": 2.191765822322797,
      "grad_norm": 4.655506610870361,
      "learning_rate": 4.8173528481397676e-05,
      "loss": 0.7952,
      "step": 240200
    },
    {
      "epoch": 2.1926782976859625,
      "grad_norm": 3.753754138946533,
      "learning_rate": 4.81727680852617e-05,
      "loss": 0.7652,
      "step": 240300
    },
    {
      "epoch": 2.1935907730491278,
      "grad_norm": 4.684070587158203,
      "learning_rate": 4.817200768912573e-05,
      "loss": 0.7884,
      "step": 240400
    },
    {
      "epoch": 2.194503248412293,
      "grad_norm": 4.319766521453857,
      "learning_rate": 4.817124729298976e-05,
      "loss": 0.77,
      "step": 240500
    },
    {
      "epoch": 2.195415723775458,
      "grad_norm": 4.3871588706970215,
      "learning_rate": 4.817048689685379e-05,
      "loss": 0.7479,
      "step": 240600
    },
    {
      "epoch": 2.196328199138623,
      "grad_norm": 4.427750110626221,
      "learning_rate": 4.816972650071781e-05,
      "loss": 0.7807,
      "step": 240700
    },
    {
      "epoch": 2.1972406745017885,
      "grad_norm": 3.323963165283203,
      "learning_rate": 4.816896610458185e-05,
      "loss": 0.7964,
      "step": 240800
    },
    {
      "epoch": 2.198153149864954,
      "grad_norm": 3.8786532878875732,
      "learning_rate": 4.816820570844587e-05,
      "loss": 0.7672,
      "step": 240900
    },
    {
      "epoch": 2.1990656252281187,
      "grad_norm": 3.2301483154296875,
      "learning_rate": 4.81674453123099e-05,
      "loss": 0.7374,
      "step": 241000
    },
    {
      "epoch": 2.199978100591284,
      "grad_norm": 3.4716625213623047,
      "learning_rate": 4.816668491617393e-05,
      "loss": 0.7354,
      "step": 241100
    },
    {
      "epoch": 2.2008905759544493,
      "grad_norm": 4.532175540924072,
      "learning_rate": 4.816592452003796e-05,
      "loss": 0.7712,
      "step": 241200
    },
    {
      "epoch": 2.2018030513176146,
      "grad_norm": 4.064497470855713,
      "learning_rate": 4.816516412390199e-05,
      "loss": 0.7492,
      "step": 241300
    },
    {
      "epoch": 2.2027155266807794,
      "grad_norm": 4.2461748123168945,
      "learning_rate": 4.816440372776602e-05,
      "loss": 0.7811,
      "step": 241400
    },
    {
      "epoch": 2.2036280020439447,
      "grad_norm": 4.214197158813477,
      "learning_rate": 4.8163643331630047e-05,
      "loss": 0.7453,
      "step": 241500
    },
    {
      "epoch": 2.20454047740711,
      "grad_norm": 4.078895568847656,
      "learning_rate": 4.8162882935494077e-05,
      "loss": 0.7739,
      "step": 241600
    },
    {
      "epoch": 2.2054529527702753,
      "grad_norm": 5.027469635009766,
      "learning_rate": 4.816212253935811e-05,
      "loss": 0.7397,
      "step": 241700
    },
    {
      "epoch": 2.2063654281334406,
      "grad_norm": 3.6461877822875977,
      "learning_rate": 4.816136214322213e-05,
      "loss": 0.7682,
      "step": 241800
    },
    {
      "epoch": 2.2072779034966055,
      "grad_norm": 3.182164192199707,
      "learning_rate": 4.816060174708617e-05,
      "loss": 0.7574,
      "step": 241900
    },
    {
      "epoch": 2.2081903788597708,
      "grad_norm": 4.8426666259765625,
      "learning_rate": 4.815984135095019e-05,
      "loss": 0.749,
      "step": 242000
    },
    {
      "epoch": 2.209102854222936,
      "grad_norm": 3.9123570919036865,
      "learning_rate": 4.815908095481422e-05,
      "loss": 0.7408,
      "step": 242100
    },
    {
      "epoch": 2.2100153295861014,
      "grad_norm": 4.096908092498779,
      "learning_rate": 4.815832055867825e-05,
      "loss": 0.7442,
      "step": 242200
    },
    {
      "epoch": 2.210927804949266,
      "grad_norm": 4.594240188598633,
      "learning_rate": 4.815756016254228e-05,
      "loss": 0.7859,
      "step": 242300
    },
    {
      "epoch": 2.2118402803124315,
      "grad_norm": 4.2409515380859375,
      "learning_rate": 4.815679976640631e-05,
      "loss": 0.7829,
      "step": 242400
    },
    {
      "epoch": 2.212752755675597,
      "grad_norm": 4.137463092803955,
      "learning_rate": 4.815603937027034e-05,
      "loss": 0.7561,
      "step": 242500
    },
    {
      "epoch": 2.213665231038762,
      "grad_norm": 4.290122985839844,
      "learning_rate": 4.8155278974134364e-05,
      "loss": 0.7392,
      "step": 242600
    },
    {
      "epoch": 2.214577706401927,
      "grad_norm": 3.4522171020507812,
      "learning_rate": 4.81545185779984e-05,
      "loss": 0.76,
      "step": 242700
    },
    {
      "epoch": 2.2154901817650923,
      "grad_norm": 3.4456675052642822,
      "learning_rate": 4.8153758181862424e-05,
      "loss": 0.7662,
      "step": 242800
    },
    {
      "epoch": 2.2164026571282576,
      "grad_norm": 4.571401596069336,
      "learning_rate": 4.8152997785726454e-05,
      "loss": 0.7635,
      "step": 242900
    },
    {
      "epoch": 2.217315132491423,
      "grad_norm": 4.062681198120117,
      "learning_rate": 4.8152237389590484e-05,
      "loss": 0.7506,
      "step": 243000
    },
    {
      "epoch": 2.2182276078545877,
      "grad_norm": 4.1260504722595215,
      "learning_rate": 4.8151476993454514e-05,
      "loss": 0.7624,
      "step": 243100
    },
    {
      "epoch": 2.219140083217753,
      "grad_norm": 3.790408134460449,
      "learning_rate": 4.815071659731854e-05,
      "loss": 0.7864,
      "step": 243200
    },
    {
      "epoch": 2.2200525585809183,
      "grad_norm": 3.9170377254486084,
      "learning_rate": 4.8149956201182574e-05,
      "loss": 0.768,
      "step": 243300
    },
    {
      "epoch": 2.2209650339440836,
      "grad_norm": 4.779740333557129,
      "learning_rate": 4.81491958050466e-05,
      "loss": 0.7896,
      "step": 243400
    },
    {
      "epoch": 2.221877509307249,
      "grad_norm": 3.9985241889953613,
      "learning_rate": 4.814843540891063e-05,
      "loss": 0.7528,
      "step": 243500
    },
    {
      "epoch": 2.2227899846704138,
      "grad_norm": 4.663448810577393,
      "learning_rate": 4.814767501277466e-05,
      "loss": 0.7578,
      "step": 243600
    },
    {
      "epoch": 2.223702460033579,
      "grad_norm": 4.428705215454102,
      "learning_rate": 4.814691461663869e-05,
      "loss": 0.7596,
      "step": 243700
    },
    {
      "epoch": 2.2246149353967444,
      "grad_norm": 3.910759449005127,
      "learning_rate": 4.814615422050272e-05,
      "loss": 0.7627,
      "step": 243800
    },
    {
      "epoch": 2.2255274107599097,
      "grad_norm": 4.138478755950928,
      "learning_rate": 4.814539382436675e-05,
      "loss": 0.7371,
      "step": 243900
    },
    {
      "epoch": 2.2264398861230745,
      "grad_norm": 4.683233737945557,
      "learning_rate": 4.814463342823077e-05,
      "loss": 0.7451,
      "step": 244000
    },
    {
      "epoch": 2.22735236148624,
      "grad_norm": 3.8535876274108887,
      "learning_rate": 4.814387303209481e-05,
      "loss": 0.7906,
      "step": 244100
    },
    {
      "epoch": 2.228264836849405,
      "grad_norm": 4.28766393661499,
      "learning_rate": 4.814311263595883e-05,
      "loss": 0.7759,
      "step": 244200
    },
    {
      "epoch": 2.2291773122125704,
      "grad_norm": 4.505807399749756,
      "learning_rate": 4.8142352239822855e-05,
      "loss": 0.8047,
      "step": 244300
    },
    {
      "epoch": 2.2300897875757353,
      "grad_norm": 4.366114616394043,
      "learning_rate": 4.814159184368689e-05,
      "loss": 0.7502,
      "step": 244400
    },
    {
      "epoch": 2.2310022629389006,
      "grad_norm": 4.452751636505127,
      "learning_rate": 4.8140831447550915e-05,
      "loss": 0.7532,
      "step": 244500
    },
    {
      "epoch": 2.231914738302066,
      "grad_norm": 4.055555820465088,
      "learning_rate": 4.8140071051414945e-05,
      "loss": 0.7919,
      "step": 244600
    },
    {
      "epoch": 2.232827213665231,
      "grad_norm": 4.063518047332764,
      "learning_rate": 4.8139310655278975e-05,
      "loss": 0.7502,
      "step": 244700
    },
    {
      "epoch": 2.233739689028396,
      "grad_norm": 5.854366302490234,
      "learning_rate": 4.8138550259143005e-05,
      "loss": 0.7493,
      "step": 244800
    },
    {
      "epoch": 2.2346521643915613,
      "grad_norm": 4.031323432922363,
      "learning_rate": 4.8137789863007035e-05,
      "loss": 0.7469,
      "step": 244900
    },
    {
      "epoch": 2.2355646397547266,
      "grad_norm": 3.7214574813842773,
      "learning_rate": 4.8137029466871065e-05,
      "loss": 0.753,
      "step": 245000
    },
    {
      "epoch": 2.236477115117892,
      "grad_norm": 4.634091854095459,
      "learning_rate": 4.813626907073509e-05,
      "loss": 0.7609,
      "step": 245100
    },
    {
      "epoch": 2.2373895904810572,
      "grad_norm": 4.483202934265137,
      "learning_rate": 4.8135508674599125e-05,
      "loss": 0.7774,
      "step": 245200
    },
    {
      "epoch": 2.238302065844222,
      "grad_norm": 4.338894367218018,
      "learning_rate": 4.813474827846315e-05,
      "loss": 0.7447,
      "step": 245300
    },
    {
      "epoch": 2.2392145412073874,
      "grad_norm": 3.573911428451538,
      "learning_rate": 4.813398788232718e-05,
      "loss": 0.7374,
      "step": 245400
    },
    {
      "epoch": 2.2401270165705527,
      "grad_norm": 5.562335968017578,
      "learning_rate": 4.813322748619121e-05,
      "loss": 0.7362,
      "step": 245500
    },
    {
      "epoch": 2.241039491933718,
      "grad_norm": 4.48169469833374,
      "learning_rate": 4.813246709005524e-05,
      "loss": 0.7477,
      "step": 245600
    },
    {
      "epoch": 2.241951967296883,
      "grad_norm": 5.471565246582031,
      "learning_rate": 4.813170669391926e-05,
      "loss": 0.7661,
      "step": 245700
    },
    {
      "epoch": 2.242864442660048,
      "grad_norm": 4.018054485321045,
      "learning_rate": 4.81309462977833e-05,
      "loss": 0.7507,
      "step": 245800
    },
    {
      "epoch": 2.2437769180232134,
      "grad_norm": 3.9847285747528076,
      "learning_rate": 4.813018590164732e-05,
      "loss": 0.8029,
      "step": 245900
    },
    {
      "epoch": 2.2446893933863787,
      "grad_norm": 6.193204402923584,
      "learning_rate": 4.812942550551135e-05,
      "loss": 0.8163,
      "step": 246000
    },
    {
      "epoch": 2.2456018687495436,
      "grad_norm": 4.800114154815674,
      "learning_rate": 4.812866510937538e-05,
      "loss": 0.7495,
      "step": 246100
    },
    {
      "epoch": 2.246514344112709,
      "grad_norm": 4.535317897796631,
      "learning_rate": 4.812790471323941e-05,
      "loss": 0.7576,
      "step": 246200
    },
    {
      "epoch": 2.247426819475874,
      "grad_norm": 4.107850074768066,
      "learning_rate": 4.812714431710344e-05,
      "loss": 0.7848,
      "step": 246300
    },
    {
      "epoch": 2.2483392948390395,
      "grad_norm": 5.29035758972168,
      "learning_rate": 4.812638392096747e-05,
      "loss": 0.7489,
      "step": 246400
    },
    {
      "epoch": 2.2492517702022043,
      "grad_norm": 4.343538284301758,
      "learning_rate": 4.8125623524831496e-05,
      "loss": 0.7266,
      "step": 246500
    },
    {
      "epoch": 2.2501642455653696,
      "grad_norm": 5.33819580078125,
      "learning_rate": 4.812486312869553e-05,
      "loss": 0.7556,
      "step": 246600
    },
    {
      "epoch": 2.251076720928535,
      "grad_norm": 4.434078693389893,
      "learning_rate": 4.8124102732559556e-05,
      "loss": 0.7405,
      "step": 246700
    },
    {
      "epoch": 2.2519891962917002,
      "grad_norm": 4.3939080238342285,
      "learning_rate": 4.8123342336423586e-05,
      "loss": 0.7414,
      "step": 246800
    },
    {
      "epoch": 2.2529016716548655,
      "grad_norm": 4.765137195587158,
      "learning_rate": 4.8122581940287616e-05,
      "loss": 0.7623,
      "step": 246900
    },
    {
      "epoch": 2.2538141470180304,
      "grad_norm": 4.156924247741699,
      "learning_rate": 4.8121821544151646e-05,
      "loss": 0.7503,
      "step": 247000
    },
    {
      "epoch": 2.2547266223811957,
      "grad_norm": 4.240808486938477,
      "learning_rate": 4.812106114801567e-05,
      "loss": 0.7662,
      "step": 247100
    },
    {
      "epoch": 2.255639097744361,
      "grad_norm": 4.530921459197998,
      "learning_rate": 4.81203007518797e-05,
      "loss": 0.7327,
      "step": 247200
    },
    {
      "epoch": 2.256551573107526,
      "grad_norm": 4.29808235168457,
      "learning_rate": 4.811954035574373e-05,
      "loss": 0.7082,
      "step": 247300
    },
    {
      "epoch": 2.257464048470691,
      "grad_norm": 4.5872673988342285,
      "learning_rate": 4.811877995960776e-05,
      "loss": 0.7579,
      "step": 247400
    },
    {
      "epoch": 2.2583765238338565,
      "grad_norm": 4.355148792266846,
      "learning_rate": 4.811801956347179e-05,
      "loss": 0.7859,
      "step": 247500
    },
    {
      "epoch": 2.2592889991970218,
      "grad_norm": 4.71502161026001,
      "learning_rate": 4.811725916733581e-05,
      "loss": 0.7699,
      "step": 247600
    },
    {
      "epoch": 2.260201474560187,
      "grad_norm": 3.6959550380706787,
      "learning_rate": 4.811649877119985e-05,
      "loss": 0.7473,
      "step": 247700
    },
    {
      "epoch": 2.261113949923352,
      "grad_norm": 4.095958709716797,
      "learning_rate": 4.811573837506387e-05,
      "loss": 0.7443,
      "step": 247800
    },
    {
      "epoch": 2.262026425286517,
      "grad_norm": 4.1839447021484375,
      "learning_rate": 4.81149779789279e-05,
      "loss": 0.7559,
      "step": 247900
    },
    {
      "epoch": 2.2629389006496825,
      "grad_norm": 4.196472644805908,
      "learning_rate": 4.811421758279193e-05,
      "loss": 0.7933,
      "step": 248000
    },
    {
      "epoch": 2.263851376012848,
      "grad_norm": 4.230500221252441,
      "learning_rate": 4.811345718665596e-05,
      "loss": 0.7412,
      "step": 248100
    },
    {
      "epoch": 2.2647638513760127,
      "grad_norm": 3.5250775814056396,
      "learning_rate": 4.811269679051999e-05,
      "loss": 0.7554,
      "step": 248200
    },
    {
      "epoch": 2.265676326739178,
      "grad_norm": 3.9093003273010254,
      "learning_rate": 4.811193639438402e-05,
      "loss": 0.7543,
      "step": 248300
    },
    {
      "epoch": 2.2665888021023433,
      "grad_norm": 3.9951601028442383,
      "learning_rate": 4.811117599824805e-05,
      "loss": 0.718,
      "step": 248400
    },
    {
      "epoch": 2.2675012774655086,
      "grad_norm": 3.7301688194274902,
      "learning_rate": 4.811041560211208e-05,
      "loss": 0.7612,
      "step": 248500
    },
    {
      "epoch": 2.268413752828674,
      "grad_norm": 4.458370208740234,
      "learning_rate": 4.810965520597611e-05,
      "loss": 0.7592,
      "step": 248600
    },
    {
      "epoch": 2.2693262281918387,
      "grad_norm": 3.3712661266326904,
      "learning_rate": 4.810889480984014e-05,
      "loss": 0.7766,
      "step": 248700
    },
    {
      "epoch": 2.270238703555004,
      "grad_norm": 4.427315711975098,
      "learning_rate": 4.810813441370417e-05,
      "loss": 0.7692,
      "step": 248800
    },
    {
      "epoch": 2.2711511789181693,
      "grad_norm": 4.60746431350708,
      "learning_rate": 4.81073740175682e-05,
      "loss": 0.7856,
      "step": 248900
    },
    {
      "epoch": 2.272063654281334,
      "grad_norm": 4.15381383895874,
      "learning_rate": 4.810661362143222e-05,
      "loss": 0.7779,
      "step": 249000
    },
    {
      "epoch": 2.2729761296444995,
      "grad_norm": 4.143871307373047,
      "learning_rate": 4.810585322529626e-05,
      "loss": 0.722,
      "step": 249100
    },
    {
      "epoch": 2.2738886050076648,
      "grad_norm": 3.4530746936798096,
      "learning_rate": 4.810509282916028e-05,
      "loss": 0.7521,
      "step": 249200
    },
    {
      "epoch": 2.27480108037083,
      "grad_norm": 5.440834999084473,
      "learning_rate": 4.810433243302431e-05,
      "loss": 0.8166,
      "step": 249300
    },
    {
      "epoch": 2.2757135557339954,
      "grad_norm": 4.513769626617432,
      "learning_rate": 4.810357203688834e-05,
      "loss": 0.7524,
      "step": 249400
    },
    {
      "epoch": 2.27662603109716,
      "grad_norm": 4.148751258850098,
      "learning_rate": 4.810281164075237e-05,
      "loss": 0.7669,
      "step": 249500
    },
    {
      "epoch": 2.2775385064603255,
      "grad_norm": 4.277956485748291,
      "learning_rate": 4.81020512446164e-05,
      "loss": 0.7875,
      "step": 249600
    },
    {
      "epoch": 2.278450981823491,
      "grad_norm": 3.4799575805664062,
      "learning_rate": 4.810129084848043e-05,
      "loss": 0.707,
      "step": 249700
    },
    {
      "epoch": 2.279363457186656,
      "grad_norm": 4.2630414962768555,
      "learning_rate": 4.8100530452344454e-05,
      "loss": 0.7157,
      "step": 249800
    },
    {
      "epoch": 2.280275932549821,
      "grad_norm": 3.8566226959228516,
      "learning_rate": 4.8099770056208484e-05,
      "loss": 0.7722,
      "step": 249900
    },
    {
      "epoch": 2.2811884079129863,
      "grad_norm": 4.414796829223633,
      "learning_rate": 4.8099009660072514e-05,
      "loss": 0.7687,
      "step": 250000
    },
    {
      "epoch": 2.2821008832761516,
      "grad_norm": 3.497415542602539,
      "learning_rate": 4.809824926393654e-05,
      "loss": 0.7862,
      "step": 250100
    },
    {
      "epoch": 2.283013358639317,
      "grad_norm": 3.399224042892456,
      "learning_rate": 4.8097488867800574e-05,
      "loss": 0.7575,
      "step": 250200
    },
    {
      "epoch": 2.283925834002482,
      "grad_norm": 4.518787860870361,
      "learning_rate": 4.80967284716646e-05,
      "loss": 0.7581,
      "step": 250300
    },
    {
      "epoch": 2.284838309365647,
      "grad_norm": 3.9282259941101074,
      "learning_rate": 4.809596807552863e-05,
      "loss": 0.7677,
      "step": 250400
    },
    {
      "epoch": 2.2857507847288123,
      "grad_norm": 4.14605712890625,
      "learning_rate": 4.809520767939266e-05,
      "loss": 0.7351,
      "step": 250500
    },
    {
      "epoch": 2.2866632600919776,
      "grad_norm": 4.136803150177002,
      "learning_rate": 4.809444728325669e-05,
      "loss": 0.7484,
      "step": 250600
    },
    {
      "epoch": 2.2875757354551425,
      "grad_norm": 3.7993571758270264,
      "learning_rate": 4.809368688712072e-05,
      "loss": 0.754,
      "step": 250700
    },
    {
      "epoch": 2.2884882108183078,
      "grad_norm": 3.4191198348999023,
      "learning_rate": 4.809292649098475e-05,
      "loss": 0.7239,
      "step": 250800
    },
    {
      "epoch": 2.289400686181473,
      "grad_norm": 4.862163543701172,
      "learning_rate": 4.809216609484877e-05,
      "loss": 0.728,
      "step": 250900
    },
    {
      "epoch": 2.2903131615446384,
      "grad_norm": 4.156049728393555,
      "learning_rate": 4.809140569871281e-05,
      "loss": 0.7132,
      "step": 251000
    },
    {
      "epoch": 2.2912256369078037,
      "grad_norm": 3.6317968368530273,
      "learning_rate": 4.809064530257683e-05,
      "loss": 0.7567,
      "step": 251100
    },
    {
      "epoch": 2.2921381122709685,
      "grad_norm": 3.873749017715454,
      "learning_rate": 4.808988490644086e-05,
      "loss": 0.7308,
      "step": 251200
    },
    {
      "epoch": 2.293050587634134,
      "grad_norm": 3.9993770122528076,
      "learning_rate": 4.808912451030489e-05,
      "loss": 0.775,
      "step": 251300
    },
    {
      "epoch": 2.293963062997299,
      "grad_norm": 4.0276408195495605,
      "learning_rate": 4.808836411416892e-05,
      "loss": 0.7233,
      "step": 251400
    },
    {
      "epoch": 2.2948755383604644,
      "grad_norm": 4.139179229736328,
      "learning_rate": 4.8087603718032945e-05,
      "loss": 0.7485,
      "step": 251500
    },
    {
      "epoch": 2.2957880137236293,
      "grad_norm": 4.44352388381958,
      "learning_rate": 4.808684332189698e-05,
      "loss": 0.7252,
      "step": 251600
    },
    {
      "epoch": 2.2967004890867946,
      "grad_norm": 5.359883785247803,
      "learning_rate": 4.8086082925761005e-05,
      "loss": 0.7585,
      "step": 251700
    },
    {
      "epoch": 2.29761296444996,
      "grad_norm": 4.366308212280273,
      "learning_rate": 4.8085322529625035e-05,
      "loss": 0.7501,
      "step": 251800
    },
    {
      "epoch": 2.298525439813125,
      "grad_norm": 4.289387226104736,
      "learning_rate": 4.8084562133489065e-05,
      "loss": 0.7329,
      "step": 251900
    },
    {
      "epoch": 2.2994379151762905,
      "grad_norm": 4.399377822875977,
      "learning_rate": 4.8083801737353095e-05,
      "loss": 0.7516,
      "step": 252000
    },
    {
      "epoch": 2.3003503905394553,
      "grad_norm": 4.566176891326904,
      "learning_rate": 4.8083041341217125e-05,
      "loss": 0.7438,
      "step": 252100
    },
    {
      "epoch": 2.3012628659026206,
      "grad_norm": 3.4120752811431885,
      "learning_rate": 4.8082280945081155e-05,
      "loss": 0.7416,
      "step": 252200
    },
    {
      "epoch": 2.302175341265786,
      "grad_norm": 5.468228340148926,
      "learning_rate": 4.808152054894518e-05,
      "loss": 0.7416,
      "step": 252300
    },
    {
      "epoch": 2.303087816628951,
      "grad_norm": 4.2433576583862305,
      "learning_rate": 4.8080760152809215e-05,
      "loss": 0.7256,
      "step": 252400
    },
    {
      "epoch": 2.304000291992116,
      "grad_norm": 4.958112716674805,
      "learning_rate": 4.807999975667324e-05,
      "loss": 0.7558,
      "step": 252500
    },
    {
      "epoch": 2.3049127673552814,
      "grad_norm": 4.0313520431518555,
      "learning_rate": 4.807923936053727e-05,
      "loss": 0.7539,
      "step": 252600
    },
    {
      "epoch": 2.3058252427184467,
      "grad_norm": 4.504649639129639,
      "learning_rate": 4.80784789644013e-05,
      "loss": 0.7507,
      "step": 252700
    },
    {
      "epoch": 2.306737718081612,
      "grad_norm": 4.055449962615967,
      "learning_rate": 4.807771856826532e-05,
      "loss": 0.7372,
      "step": 252800
    },
    {
      "epoch": 2.307650193444777,
      "grad_norm": 4.61016321182251,
      "learning_rate": 4.807695817212935e-05,
      "loss": 0.7849,
      "step": 252900
    },
    {
      "epoch": 2.308562668807942,
      "grad_norm": 4.777284622192383,
      "learning_rate": 4.807619777599338e-05,
      "loss": 0.7398,
      "step": 253000
    },
    {
      "epoch": 2.3094751441711074,
      "grad_norm": 4.310007572174072,
      "learning_rate": 4.807543737985741e-05,
      "loss": 0.7467,
      "step": 253100
    },
    {
      "epoch": 2.3103876195342727,
      "grad_norm": 3.1425368785858154,
      "learning_rate": 4.807467698372144e-05,
      "loss": 0.7481,
      "step": 253200
    },
    {
      "epoch": 2.3113000948974376,
      "grad_norm": 3.90972638130188,
      "learning_rate": 4.807391658758547e-05,
      "loss": 0.7456,
      "step": 253300
    },
    {
      "epoch": 2.312212570260603,
      "grad_norm": 3.4132752418518066,
      "learning_rate": 4.8073156191449496e-05,
      "loss": 0.7116,
      "step": 253400
    },
    {
      "epoch": 2.313125045623768,
      "grad_norm": 3.9118809700012207,
      "learning_rate": 4.807239579531353e-05,
      "loss": 0.7357,
      "step": 253500
    },
    {
      "epoch": 2.3140375209869335,
      "grad_norm": 3.52961802482605,
      "learning_rate": 4.8071635399177556e-05,
      "loss": 0.7564,
      "step": 253600
    },
    {
      "epoch": 2.314949996350099,
      "grad_norm": 4.387348175048828,
      "learning_rate": 4.8070875003041586e-05,
      "loss": 0.7697,
      "step": 253700
    },
    {
      "epoch": 2.3158624717132636,
      "grad_norm": 4.647317886352539,
      "learning_rate": 4.8070114606905616e-05,
      "loss": 0.7493,
      "step": 253800
    },
    {
      "epoch": 2.316774947076429,
      "grad_norm": 3.6666088104248047,
      "learning_rate": 4.8069354210769646e-05,
      "loss": 0.7717,
      "step": 253900
    },
    {
      "epoch": 2.3176874224395942,
      "grad_norm": 4.3730950355529785,
      "learning_rate": 4.806859381463367e-05,
      "loss": 0.7701,
      "step": 254000
    },
    {
      "epoch": 2.318599897802759,
      "grad_norm": 4.311970233917236,
      "learning_rate": 4.8067833418497706e-05,
      "loss": 0.7833,
      "step": 254100
    },
    {
      "epoch": 2.3195123731659244,
      "grad_norm": 4.708405494689941,
      "learning_rate": 4.806707302236173e-05,
      "loss": 0.733,
      "step": 254200
    },
    {
      "epoch": 2.3204248485290897,
      "grad_norm": 4.131937503814697,
      "learning_rate": 4.806631262622576e-05,
      "loss": 0.7615,
      "step": 254300
    },
    {
      "epoch": 2.321337323892255,
      "grad_norm": 3.0204622745513916,
      "learning_rate": 4.806555223008979e-05,
      "loss": 0.764,
      "step": 254400
    },
    {
      "epoch": 2.3222497992554203,
      "grad_norm": 2.824643135070801,
      "learning_rate": 4.806479183395382e-05,
      "loss": 0.7311,
      "step": 254500
    },
    {
      "epoch": 2.323162274618585,
      "grad_norm": 4.5384626388549805,
      "learning_rate": 4.806403143781785e-05,
      "loss": 0.748,
      "step": 254600
    },
    {
      "epoch": 2.3240747499817505,
      "grad_norm": 4.294737339019775,
      "learning_rate": 4.806327104168188e-05,
      "loss": 0.7771,
      "step": 254700
    },
    {
      "epoch": 2.3249872253449158,
      "grad_norm": 4.347240924835205,
      "learning_rate": 4.80625106455459e-05,
      "loss": 0.7134,
      "step": 254800
    },
    {
      "epoch": 2.325899700708081,
      "grad_norm": 4.843472957611084,
      "learning_rate": 4.806175024940994e-05,
      "loss": 0.741,
      "step": 254900
    },
    {
      "epoch": 2.326812176071246,
      "grad_norm": 4.97662353515625,
      "learning_rate": 4.806098985327396e-05,
      "loss": 0.728,
      "step": 255000
    },
    {
      "epoch": 2.327724651434411,
      "grad_norm": 5.246923446655273,
      "learning_rate": 4.806022945713799e-05,
      "loss": 0.7776,
      "step": 255100
    },
    {
      "epoch": 2.3286371267975765,
      "grad_norm": 4.297691345214844,
      "learning_rate": 4.8059469061002023e-05,
      "loss": 0.7719,
      "step": 255200
    },
    {
      "epoch": 2.329549602160742,
      "grad_norm": 4.728734016418457,
      "learning_rate": 4.8058708664866053e-05,
      "loss": 0.7666,
      "step": 255300
    },
    {
      "epoch": 2.3304620775239067,
      "grad_norm": 4.029086112976074,
      "learning_rate": 4.805794826873008e-05,
      "loss": 0.7633,
      "step": 255400
    },
    {
      "epoch": 2.331374552887072,
      "grad_norm": 3.5490987300872803,
      "learning_rate": 4.8057187872594114e-05,
      "loss": 0.7205,
      "step": 255500
    },
    {
      "epoch": 2.3322870282502373,
      "grad_norm": 4.473068714141846,
      "learning_rate": 4.805642747645814e-05,
      "loss": 0.7492,
      "step": 255600
    },
    {
      "epoch": 2.3331995036134026,
      "grad_norm": 4.254889488220215,
      "learning_rate": 4.805566708032217e-05,
      "loss": 0.7645,
      "step": 255700
    },
    {
      "epoch": 2.3341119789765674,
      "grad_norm": 4.641648292541504,
      "learning_rate": 4.80549066841862e-05,
      "loss": 0.7719,
      "step": 255800
    },
    {
      "epoch": 2.3350244543397327,
      "grad_norm": 3.964651346206665,
      "learning_rate": 4.805414628805022e-05,
      "loss": 0.7181,
      "step": 255900
    },
    {
      "epoch": 2.335936929702898,
      "grad_norm": 4.869555473327637,
      "learning_rate": 4.805338589191426e-05,
      "loss": 0.7524,
      "step": 256000
    },
    {
      "epoch": 2.3368494050660633,
      "grad_norm": 3.9861626625061035,
      "learning_rate": 4.805262549577828e-05,
      "loss": 0.7538,
      "step": 256100
    },
    {
      "epoch": 2.3377618804292286,
      "grad_norm": 4.348130702972412,
      "learning_rate": 4.805186509964231e-05,
      "loss": 0.7366,
      "step": 256200
    },
    {
      "epoch": 2.3386743557923935,
      "grad_norm": 3.296858310699463,
      "learning_rate": 4.805110470350634e-05,
      "loss": 0.7408,
      "step": 256300
    },
    {
      "epoch": 2.3395868311555588,
      "grad_norm": 4.614160060882568,
      "learning_rate": 4.805034430737037e-05,
      "loss": 0.7566,
      "step": 256400
    },
    {
      "epoch": 2.340499306518724,
      "grad_norm": 4.06968879699707,
      "learning_rate": 4.8049583911234394e-05,
      "loss": 0.7404,
      "step": 256500
    },
    {
      "epoch": 2.3414117818818894,
      "grad_norm": 4.0980658531188965,
      "learning_rate": 4.804882351509843e-05,
      "loss": 0.7796,
      "step": 256600
    },
    {
      "epoch": 2.342324257245054,
      "grad_norm": 3.891533613204956,
      "learning_rate": 4.8048063118962454e-05,
      "loss": 0.7326,
      "step": 256700
    },
    {
      "epoch": 2.3432367326082195,
      "grad_norm": 2.148366928100586,
      "learning_rate": 4.8047302722826484e-05,
      "loss": 0.7704,
      "step": 256800
    },
    {
      "epoch": 2.344149207971385,
      "grad_norm": 4.482457160949707,
      "learning_rate": 4.8046542326690514e-05,
      "loss": 0.6946,
      "step": 256900
    },
    {
      "epoch": 2.34506168333455,
      "grad_norm": 3.907331705093384,
      "learning_rate": 4.8045781930554544e-05,
      "loss": 0.7597,
      "step": 257000
    },
    {
      "epoch": 2.345974158697715,
      "grad_norm": 3.406651020050049,
      "learning_rate": 4.8045021534418574e-05,
      "loss": 0.6969,
      "step": 257100
    },
    {
      "epoch": 2.3468866340608803,
      "grad_norm": 3.9902524948120117,
      "learning_rate": 4.8044261138282604e-05,
      "loss": 0.7591,
      "step": 257200
    },
    {
      "epoch": 2.3477991094240456,
      "grad_norm": 5.671897888183594,
      "learning_rate": 4.804350074214663e-05,
      "loss": 0.7983,
      "step": 257300
    },
    {
      "epoch": 2.348711584787211,
      "grad_norm": 3.060619354248047,
      "learning_rate": 4.8042740346010665e-05,
      "loss": 0.7481,
      "step": 257400
    },
    {
      "epoch": 2.3496240601503757,
      "grad_norm": 3.612231492996216,
      "learning_rate": 4.804197994987469e-05,
      "loss": 0.7905,
      "step": 257500
    },
    {
      "epoch": 2.350536535513541,
      "grad_norm": 4.859724044799805,
      "learning_rate": 4.804121955373872e-05,
      "loss": 0.7246,
      "step": 257600
    },
    {
      "epoch": 2.3514490108767063,
      "grad_norm": 4.6391282081604,
      "learning_rate": 4.804045915760275e-05,
      "loss": 0.7963,
      "step": 257700
    },
    {
      "epoch": 2.3523614862398716,
      "grad_norm": 3.5414254665374756,
      "learning_rate": 4.803969876146678e-05,
      "loss": 0.7393,
      "step": 257800
    },
    {
      "epoch": 2.353273961603037,
      "grad_norm": 4.603810787200928,
      "learning_rate": 4.80389383653308e-05,
      "loss": 0.7565,
      "step": 257900
    },
    {
      "epoch": 2.354186436966202,
      "grad_norm": 4.676778316497803,
      "learning_rate": 4.803817796919484e-05,
      "loss": 0.7841,
      "step": 258000
    },
    {
      "epoch": 2.355098912329367,
      "grad_norm": 4.08136510848999,
      "learning_rate": 4.803741757305886e-05,
      "loss": 0.7453,
      "step": 258100
    },
    {
      "epoch": 2.3560113876925324,
      "grad_norm": 3.730501413345337,
      "learning_rate": 4.803665717692289e-05,
      "loss": 0.7421,
      "step": 258200
    },
    {
      "epoch": 2.3569238630556977,
      "grad_norm": 4.993570327758789,
      "learning_rate": 4.803589678078692e-05,
      "loss": 0.7499,
      "step": 258300
    },
    {
      "epoch": 2.3578363384188625,
      "grad_norm": 4.576367378234863,
      "learning_rate": 4.803513638465095e-05,
      "loss": 0.7011,
      "step": 258400
    },
    {
      "epoch": 2.358748813782028,
      "grad_norm": 4.166915416717529,
      "learning_rate": 4.803437598851498e-05,
      "loss": 0.7793,
      "step": 258500
    },
    {
      "epoch": 2.359661289145193,
      "grad_norm": 5.45366096496582,
      "learning_rate": 4.8033615592379005e-05,
      "loss": 0.7656,
      "step": 258600
    },
    {
      "epoch": 2.3605737645083584,
      "grad_norm": 3.753432273864746,
      "learning_rate": 4.8032855196243035e-05,
      "loss": 0.7453,
      "step": 258700
    },
    {
      "epoch": 2.3614862398715233,
      "grad_norm": 4.5952229499816895,
      "learning_rate": 4.8032094800107065e-05,
      "loss": 0.728,
      "step": 258800
    },
    {
      "epoch": 2.3623987152346886,
      "grad_norm": 4.919516563415527,
      "learning_rate": 4.8031334403971095e-05,
      "loss": 0.768,
      "step": 258900
    },
    {
      "epoch": 2.363311190597854,
      "grad_norm": 4.795294761657715,
      "learning_rate": 4.803057400783512e-05,
      "loss": 0.7285,
      "step": 259000
    },
    {
      "epoch": 2.364223665961019,
      "grad_norm": 4.0202741622924805,
      "learning_rate": 4.8029813611699155e-05,
      "loss": 0.7299,
      "step": 259100
    },
    {
      "epoch": 2.365136141324184,
      "grad_norm": 4.670933723449707,
      "learning_rate": 4.802905321556318e-05,
      "loss": 0.7714,
      "step": 259200
    },
    {
      "epoch": 2.3660486166873493,
      "grad_norm": 3.4826948642730713,
      "learning_rate": 4.802829281942721e-05,
      "loss": 0.753,
      "step": 259300
    },
    {
      "epoch": 2.3669610920505146,
      "grad_norm": 4.64526891708374,
      "learning_rate": 4.802753242329124e-05,
      "loss": 0.7884,
      "step": 259400
    },
    {
      "epoch": 2.36787356741368,
      "grad_norm": 3.872976779937744,
      "learning_rate": 4.802677202715527e-05,
      "loss": 0.7589,
      "step": 259500
    },
    {
      "epoch": 2.3687860427768452,
      "grad_norm": 3.3496017456054688,
      "learning_rate": 4.80260116310193e-05,
      "loss": 0.7196,
      "step": 259600
    },
    {
      "epoch": 2.36969851814001,
      "grad_norm": 4.178680896759033,
      "learning_rate": 4.802525123488333e-05,
      "loss": 0.7756,
      "step": 259700
    },
    {
      "epoch": 2.3706109935031754,
      "grad_norm": 5.0229105949401855,
      "learning_rate": 4.802449083874735e-05,
      "loss": 0.7482,
      "step": 259800
    },
    {
      "epoch": 2.3715234688663407,
      "grad_norm": 4.943207740783691,
      "learning_rate": 4.802373044261139e-05,
      "loss": 0.7633,
      "step": 259900
    },
    {
      "epoch": 2.372435944229506,
      "grad_norm": 4.533847808837891,
      "learning_rate": 4.802297004647541e-05,
      "loss": 0.7493,
      "step": 260000
    },
    {
      "epoch": 2.373348419592671,
      "grad_norm": 4.979477882385254,
      "learning_rate": 4.802220965033944e-05,
      "loss": 0.7659,
      "step": 260100
    },
    {
      "epoch": 2.374260894955836,
      "grad_norm": 4.737014293670654,
      "learning_rate": 4.802144925420347e-05,
      "loss": 0.7854,
      "step": 260200
    },
    {
      "epoch": 2.3751733703190014,
      "grad_norm": 4.097715854644775,
      "learning_rate": 4.80206888580675e-05,
      "loss": 0.7694,
      "step": 260300
    },
    {
      "epoch": 2.3760858456821667,
      "grad_norm": 4.196823596954346,
      "learning_rate": 4.8019928461931526e-05,
      "loss": 0.7676,
      "step": 260400
    },
    {
      "epoch": 2.3769983210453316,
      "grad_norm": 4.436211109161377,
      "learning_rate": 4.801916806579556e-05,
      "loss": 0.7127,
      "step": 260500
    },
    {
      "epoch": 2.377910796408497,
      "grad_norm": 4.414458751678467,
      "learning_rate": 4.8018407669659586e-05,
      "loss": 0.7815,
      "step": 260600
    },
    {
      "epoch": 2.378823271771662,
      "grad_norm": 5.203012943267822,
      "learning_rate": 4.8017647273523616e-05,
      "loss": 0.7703,
      "step": 260700
    },
    {
      "epoch": 2.3797357471348275,
      "grad_norm": 3.6170313358306885,
      "learning_rate": 4.8016886877387646e-05,
      "loss": 0.7649,
      "step": 260800
    },
    {
      "epoch": 2.3806482224979924,
      "grad_norm": 3.5365493297576904,
      "learning_rate": 4.8016126481251676e-05,
      "loss": 0.7705,
      "step": 260900
    },
    {
      "epoch": 2.3815606978611576,
      "grad_norm": 4.332940101623535,
      "learning_rate": 4.8015366085115706e-05,
      "loss": 0.7848,
      "step": 261000
    },
    {
      "epoch": 2.382473173224323,
      "grad_norm": 3.2392711639404297,
      "learning_rate": 4.8014605688979736e-05,
      "loss": 0.7439,
      "step": 261100
    },
    {
      "epoch": 2.3833856485874882,
      "grad_norm": 4.501449108123779,
      "learning_rate": 4.801384529284376e-05,
      "loss": 0.7579,
      "step": 261200
    },
    {
      "epoch": 2.3842981239506535,
      "grad_norm": 3.827681541442871,
      "learning_rate": 4.801308489670779e-05,
      "loss": 0.7882,
      "step": 261300
    },
    {
      "epoch": 2.3852105993138184,
      "grad_norm": 3.4236490726470947,
      "learning_rate": 4.801232450057182e-05,
      "loss": 0.7726,
      "step": 261400
    },
    {
      "epoch": 2.3861230746769837,
      "grad_norm": 4.464744567871094,
      "learning_rate": 4.801156410443585e-05,
      "loss": 0.7664,
      "step": 261500
    },
    {
      "epoch": 2.387035550040149,
      "grad_norm": 4.799960136413574,
      "learning_rate": 4.801080370829988e-05,
      "loss": 0.7702,
      "step": 261600
    },
    {
      "epoch": 2.3879480254033143,
      "grad_norm": 4.20469856262207,
      "learning_rate": 4.80100433121639e-05,
      "loss": 0.7708,
      "step": 261700
    },
    {
      "epoch": 2.388860500766479,
      "grad_norm": 4.777495861053467,
      "learning_rate": 4.800928291602793e-05,
      "loss": 0.7424,
      "step": 261800
    },
    {
      "epoch": 2.3897729761296445,
      "grad_norm": 3.5433871746063232,
      "learning_rate": 4.8008522519891963e-05,
      "loss": 0.7389,
      "step": 261900
    },
    {
      "epoch": 2.3906854514928098,
      "grad_norm": 4.063482284545898,
      "learning_rate": 4.8007762123755993e-05,
      "loss": 0.7817,
      "step": 262000
    },
    {
      "epoch": 2.391597926855975,
      "grad_norm": 4.062030792236328,
      "learning_rate": 4.8007001727620024e-05,
      "loss": 0.7389,
      "step": 262100
    },
    {
      "epoch": 2.39251040221914,
      "grad_norm": 4.136834621429443,
      "learning_rate": 4.8006241331484054e-05,
      "loss": 0.7553,
      "step": 262200
    },
    {
      "epoch": 2.393422877582305,
      "grad_norm": 3.514885187149048,
      "learning_rate": 4.800548093534808e-05,
      "loss": 0.7541,
      "step": 262300
    },
    {
      "epoch": 2.3943353529454705,
      "grad_norm": 4.425102710723877,
      "learning_rate": 4.8004720539212114e-05,
      "loss": 0.7424,
      "step": 262400
    },
    {
      "epoch": 2.395247828308636,
      "grad_norm": 4.202375888824463,
      "learning_rate": 4.800396014307614e-05,
      "loss": 0.7524,
      "step": 262500
    },
    {
      "epoch": 2.3961603036718007,
      "grad_norm": 3.9392740726470947,
      "learning_rate": 4.800319974694017e-05,
      "loss": 0.6808,
      "step": 262600
    },
    {
      "epoch": 2.397072779034966,
      "grad_norm": 4.0762763023376465,
      "learning_rate": 4.80024393508042e-05,
      "loss": 0.7523,
      "step": 262700
    },
    {
      "epoch": 2.3979852543981313,
      "grad_norm": 4.600191116333008,
      "learning_rate": 4.800167895466823e-05,
      "loss": 0.7533,
      "step": 262800
    },
    {
      "epoch": 2.3988977297612966,
      "grad_norm": 3.7704737186431885,
      "learning_rate": 4.800091855853226e-05,
      "loss": 0.7422,
      "step": 262900
    },
    {
      "epoch": 2.399810205124462,
      "grad_norm": 4.560533046722412,
      "learning_rate": 4.800015816239629e-05,
      "loss": 0.7643,
      "step": 263000
    },
    {
      "epoch": 2.4007226804876267,
      "grad_norm": 4.313952922821045,
      "learning_rate": 4.799939776626031e-05,
      "loss": 0.7896,
      "step": 263100
    },
    {
      "epoch": 2.401635155850792,
      "grad_norm": 4.23448371887207,
      "learning_rate": 4.799863737012435e-05,
      "loss": 0.7751,
      "step": 263200
    },
    {
      "epoch": 2.4025476312139573,
      "grad_norm": 3.6205432415008545,
      "learning_rate": 4.799787697398837e-05,
      "loss": 0.7434,
      "step": 263300
    },
    {
      "epoch": 2.403460106577122,
      "grad_norm": 4.549705982208252,
      "learning_rate": 4.79971165778524e-05,
      "loss": 0.7325,
      "step": 263400
    },
    {
      "epoch": 2.4043725819402875,
      "grad_norm": 4.151491641998291,
      "learning_rate": 4.799635618171643e-05,
      "loss": 0.7393,
      "step": 263500
    },
    {
      "epoch": 2.4052850573034528,
      "grad_norm": 4.028878688812256,
      "learning_rate": 4.799559578558046e-05,
      "loss": 0.7912,
      "step": 263600
    },
    {
      "epoch": 2.406197532666618,
      "grad_norm": 4.364522457122803,
      "learning_rate": 4.7994835389444484e-05,
      "loss": 0.7519,
      "step": 263700
    },
    {
      "epoch": 2.4071100080297834,
      "grad_norm": 4.597374439239502,
      "learning_rate": 4.799407499330852e-05,
      "loss": 0.7628,
      "step": 263800
    },
    {
      "epoch": 2.408022483392948,
      "grad_norm": 3.3107099533081055,
      "learning_rate": 4.7993314597172544e-05,
      "loss": 0.7193,
      "step": 263900
    },
    {
      "epoch": 2.4089349587561135,
      "grad_norm": 3.6243531703948975,
      "learning_rate": 4.7992554201036574e-05,
      "loss": 0.7337,
      "step": 264000
    },
    {
      "epoch": 2.409847434119279,
      "grad_norm": 4.493290901184082,
      "learning_rate": 4.7991793804900605e-05,
      "loss": 0.7894,
      "step": 264100
    },
    {
      "epoch": 2.410759909482444,
      "grad_norm": 5.032360553741455,
      "learning_rate": 4.799103340876463e-05,
      "loss": 0.7295,
      "step": 264200
    },
    {
      "epoch": 2.411672384845609,
      "grad_norm": 3.8986892700195312,
      "learning_rate": 4.7990273012628665e-05,
      "loss": 0.763,
      "step": 264300
    },
    {
      "epoch": 2.4125848602087743,
      "grad_norm": 4.313177585601807,
      "learning_rate": 4.798951261649269e-05,
      "loss": 0.7567,
      "step": 264400
    },
    {
      "epoch": 2.4134973355719396,
      "grad_norm": 3.8957061767578125,
      "learning_rate": 4.798875222035672e-05,
      "loss": 0.7824,
      "step": 264500
    },
    {
      "epoch": 2.414409810935105,
      "grad_norm": 3.6166603565216064,
      "learning_rate": 4.798799182422075e-05,
      "loss": 0.762,
      "step": 264600
    },
    {
      "epoch": 2.41532228629827,
      "grad_norm": 4.36676025390625,
      "learning_rate": 4.798723142808478e-05,
      "loss": 0.7641,
      "step": 264700
    },
    {
      "epoch": 2.416234761661435,
      "grad_norm": 4.653953552246094,
      "learning_rate": 4.79864710319488e-05,
      "loss": 0.712,
      "step": 264800
    },
    {
      "epoch": 2.4171472370246003,
      "grad_norm": 5.162287712097168,
      "learning_rate": 4.798571063581284e-05,
      "loss": 0.7232,
      "step": 264900
    },
    {
      "epoch": 2.4180597123877656,
      "grad_norm": 4.554776191711426,
      "learning_rate": 4.798495023967686e-05,
      "loss": 0.7308,
      "step": 265000
    },
    {
      "epoch": 2.4189721877509305,
      "grad_norm": 4.096183776855469,
      "learning_rate": 4.798418984354089e-05,
      "loss": 0.7796,
      "step": 265100
    },
    {
      "epoch": 2.419884663114096,
      "grad_norm": 4.486069679260254,
      "learning_rate": 4.798342944740492e-05,
      "loss": 0.7813,
      "step": 265200
    },
    {
      "epoch": 2.420797138477261,
      "grad_norm": 3.9083831310272217,
      "learning_rate": 4.798266905126895e-05,
      "loss": 0.7617,
      "step": 265300
    },
    {
      "epoch": 2.4217096138404264,
      "grad_norm": 4.420648097991943,
      "learning_rate": 4.798190865513298e-05,
      "loss": 0.7159,
      "step": 265400
    },
    {
      "epoch": 2.4226220892035917,
      "grad_norm": 2.973994255065918,
      "learning_rate": 4.798114825899701e-05,
      "loss": 0.7722,
      "step": 265500
    },
    {
      "epoch": 2.4235345645667565,
      "grad_norm": 3.372541666030884,
      "learning_rate": 4.7980387862861035e-05,
      "loss": 0.7446,
      "step": 265600
    },
    {
      "epoch": 2.424447039929922,
      "grad_norm": 3.1309595108032227,
      "learning_rate": 4.797962746672507e-05,
      "loss": 0.7864,
      "step": 265700
    },
    {
      "epoch": 2.425359515293087,
      "grad_norm": 5.089117050170898,
      "learning_rate": 4.7978867070589095e-05,
      "loss": 0.7544,
      "step": 265800
    },
    {
      "epoch": 2.4262719906562524,
      "grad_norm": 4.135207176208496,
      "learning_rate": 4.7978106674453125e-05,
      "loss": 0.784,
      "step": 265900
    },
    {
      "epoch": 2.4271844660194173,
      "grad_norm": 5.175989627838135,
      "learning_rate": 4.7977346278317155e-05,
      "loss": 0.7789,
      "step": 266000
    },
    {
      "epoch": 2.4280969413825826,
      "grad_norm": 4.09525203704834,
      "learning_rate": 4.7976585882181186e-05,
      "loss": 0.759,
      "step": 266100
    },
    {
      "epoch": 2.429009416745748,
      "grad_norm": 4.454986095428467,
      "learning_rate": 4.797582548604521e-05,
      "loss": 0.7122,
      "step": 266200
    },
    {
      "epoch": 2.429921892108913,
      "grad_norm": 4.420515537261963,
      "learning_rate": 4.7975065089909246e-05,
      "loss": 0.738,
      "step": 266300
    },
    {
      "epoch": 2.4308343674720785,
      "grad_norm": 4.369082450866699,
      "learning_rate": 4.797430469377327e-05,
      "loss": 0.7819,
      "step": 266400
    },
    {
      "epoch": 2.4317468428352433,
      "grad_norm": 5.533791542053223,
      "learning_rate": 4.79735442976373e-05,
      "loss": 0.7754,
      "step": 266500
    },
    {
      "epoch": 2.4326593181984086,
      "grad_norm": 4.898531913757324,
      "learning_rate": 4.797278390150133e-05,
      "loss": 0.7814,
      "step": 266600
    },
    {
      "epoch": 2.433571793561574,
      "grad_norm": 4.620248317718506,
      "learning_rate": 4.797202350536536e-05,
      "loss": 0.7782,
      "step": 266700
    },
    {
      "epoch": 2.434484268924739,
      "grad_norm": 4.023873805999756,
      "learning_rate": 4.797126310922939e-05,
      "loss": 0.7294,
      "step": 266800
    },
    {
      "epoch": 2.435396744287904,
      "grad_norm": 4.359127044677734,
      "learning_rate": 4.797050271309342e-05,
      "loss": 0.7286,
      "step": 266900
    },
    {
      "epoch": 2.4363092196510694,
      "grad_norm": 4.717164039611816,
      "learning_rate": 4.796974231695744e-05,
      "loss": 0.7456,
      "step": 267000
    },
    {
      "epoch": 2.4372216950142347,
      "grad_norm": 4.370820999145508,
      "learning_rate": 4.796898192082147e-05,
      "loss": 0.7568,
      "step": 267100
    },
    {
      "epoch": 2.4381341703774,
      "grad_norm": 3.831090211868286,
      "learning_rate": 4.79682215246855e-05,
      "loss": 0.7188,
      "step": 267200
    },
    {
      "epoch": 2.439046645740565,
      "grad_norm": 4.329850196838379,
      "learning_rate": 4.7967461128549526e-05,
      "loss": 0.755,
      "step": 267300
    },
    {
      "epoch": 2.43995912110373,
      "grad_norm": 4.253810405731201,
      "learning_rate": 4.796670073241356e-05,
      "loss": 0.7374,
      "step": 267400
    },
    {
      "epoch": 2.4408715964668954,
      "grad_norm": 4.871530532836914,
      "learning_rate": 4.7965940336277586e-05,
      "loss": 0.7485,
      "step": 267500
    },
    {
      "epoch": 2.4417840718300607,
      "grad_norm": 3.1309423446655273,
      "learning_rate": 4.7965179940141616e-05,
      "loss": 0.7173,
      "step": 267600
    },
    {
      "epoch": 2.4426965471932256,
      "grad_norm": 4.205932140350342,
      "learning_rate": 4.7964419544005646e-05,
      "loss": 0.7239,
      "step": 267700
    },
    {
      "epoch": 2.443609022556391,
      "grad_norm": 4.588591575622559,
      "learning_rate": 4.7963659147869676e-05,
      "loss": 0.737,
      "step": 267800
    },
    {
      "epoch": 2.444521497919556,
      "grad_norm": 3.5581958293914795,
      "learning_rate": 4.7962898751733706e-05,
      "loss": 0.7357,
      "step": 267900
    },
    {
      "epoch": 2.4454339732827215,
      "grad_norm": 3.38016676902771,
      "learning_rate": 4.7962138355597737e-05,
      "loss": 0.7405,
      "step": 268000
    },
    {
      "epoch": 2.446346448645887,
      "grad_norm": 2.906059741973877,
      "learning_rate": 4.796137795946176e-05,
      "loss": 0.8079,
      "step": 268100
    },
    {
      "epoch": 2.4472589240090517,
      "grad_norm": 4.309026718139648,
      "learning_rate": 4.79606175633258e-05,
      "loss": 0.745,
      "step": 268200
    },
    {
      "epoch": 2.448171399372217,
      "grad_norm": 3.5589308738708496,
      "learning_rate": 4.795985716718982e-05,
      "loss": 0.7638,
      "step": 268300
    },
    {
      "epoch": 2.4490838747353822,
      "grad_norm": 4.033980846405029,
      "learning_rate": 4.795909677105385e-05,
      "loss": 0.7567,
      "step": 268400
    },
    {
      "epoch": 2.449996350098547,
      "grad_norm": 2.610689640045166,
      "learning_rate": 4.795833637491788e-05,
      "loss": 0.7342,
      "step": 268500
    },
    {
      "epoch": 2.4509088254617124,
      "grad_norm": 4.177722930908203,
      "learning_rate": 4.795757597878191e-05,
      "loss": 0.7839,
      "step": 268600
    },
    {
      "epoch": 2.4518213008248777,
      "grad_norm": 4.4632768630981445,
      "learning_rate": 4.7956815582645933e-05,
      "loss": 0.7139,
      "step": 268700
    },
    {
      "epoch": 2.452733776188043,
      "grad_norm": 4.333467960357666,
      "learning_rate": 4.795605518650997e-05,
      "loss": 0.8004,
      "step": 268800
    },
    {
      "epoch": 2.4536462515512083,
      "grad_norm": 4.200747966766357,
      "learning_rate": 4.7955294790373994e-05,
      "loss": 0.7433,
      "step": 268900
    },
    {
      "epoch": 2.454558726914373,
      "grad_norm": 4.339926242828369,
      "learning_rate": 4.7954534394238024e-05,
      "loss": 0.7289,
      "step": 269000
    },
    {
      "epoch": 2.4554712022775385,
      "grad_norm": 4.230561256408691,
      "learning_rate": 4.7953773998102054e-05,
      "loss": 0.7335,
      "step": 269100
    },
    {
      "epoch": 2.4563836776407038,
      "grad_norm": 4.453350067138672,
      "learning_rate": 4.7953013601966084e-05,
      "loss": 0.7206,
      "step": 269200
    },
    {
      "epoch": 2.457296153003869,
      "grad_norm": 3.619267463684082,
      "learning_rate": 4.7952253205830114e-05,
      "loss": 0.7361,
      "step": 269300
    },
    {
      "epoch": 2.458208628367034,
      "grad_norm": 3.3884150981903076,
      "learning_rate": 4.7951492809694144e-05,
      "loss": 0.799,
      "step": 269400
    },
    {
      "epoch": 2.459121103730199,
      "grad_norm": 4.853356838226318,
      "learning_rate": 4.795073241355817e-05,
      "loss": 0.7697,
      "step": 269500
    },
    {
      "epoch": 2.4600335790933645,
      "grad_norm": 4.256330966949463,
      "learning_rate": 4.7949972017422204e-05,
      "loss": 0.7589,
      "step": 269600
    },
    {
      "epoch": 2.46094605445653,
      "grad_norm": 4.461048603057861,
      "learning_rate": 4.794921162128623e-05,
      "loss": 0.7455,
      "step": 269700
    },
    {
      "epoch": 2.461858529819695,
      "grad_norm": 4.197955131530762,
      "learning_rate": 4.794845122515026e-05,
      "loss": 0.7524,
      "step": 269800
    },
    {
      "epoch": 2.46277100518286,
      "grad_norm": 3.8514559268951416,
      "learning_rate": 4.794769082901429e-05,
      "loss": 0.7542,
      "step": 269900
    },
    {
      "epoch": 2.4636834805460253,
      "grad_norm": 3.6424598693847656,
      "learning_rate": 4.794693043287831e-05,
      "loss": 0.7889,
      "step": 270000
    },
    {
      "epoch": 2.4645959559091906,
      "grad_norm": 4.270050525665283,
      "learning_rate": 4.794617003674234e-05,
      "loss": 0.771,
      "step": 270100
    },
    {
      "epoch": 2.4655084312723554,
      "grad_norm": 5.4009809494018555,
      "learning_rate": 4.794540964060637e-05,
      "loss": 0.7408,
      "step": 270200
    },
    {
      "epoch": 2.4664209066355207,
      "grad_norm": 5.278602123260498,
      "learning_rate": 4.79446492444704e-05,
      "loss": 0.7654,
      "step": 270300
    },
    {
      "epoch": 2.467333381998686,
      "grad_norm": 4.89553165435791,
      "learning_rate": 4.794388884833443e-05,
      "loss": 0.7773,
      "step": 270400
    },
    {
      "epoch": 2.4682458573618513,
      "grad_norm": 4.484469413757324,
      "learning_rate": 4.794312845219846e-05,
      "loss": 0.7577,
      "step": 270500
    },
    {
      "epoch": 2.4691583327250166,
      "grad_norm": 4.311034679412842,
      "learning_rate": 4.7942368056062484e-05,
      "loss": 0.7689,
      "step": 270600
    },
    {
      "epoch": 2.4700708080881815,
      "grad_norm": 4.149360656738281,
      "learning_rate": 4.794160765992652e-05,
      "loss": 0.7226,
      "step": 270700
    },
    {
      "epoch": 2.4709832834513468,
      "grad_norm": 4.583990573883057,
      "learning_rate": 4.7940847263790545e-05,
      "loss": 0.7504,
      "step": 270800
    },
    {
      "epoch": 2.471895758814512,
      "grad_norm": 4.644523620605469,
      "learning_rate": 4.7940086867654575e-05,
      "loss": 0.7497,
      "step": 270900
    },
    {
      "epoch": 2.4728082341776774,
      "grad_norm": 4.682635307312012,
      "learning_rate": 4.7939326471518605e-05,
      "loss": 0.7523,
      "step": 271000
    },
    {
      "epoch": 2.473720709540842,
      "grad_norm": 4.057736396789551,
      "learning_rate": 4.7938566075382635e-05,
      "loss": 0.7489,
      "step": 271100
    },
    {
      "epoch": 2.4746331849040075,
      "grad_norm": 4.094883918762207,
      "learning_rate": 4.793780567924666e-05,
      "loss": 0.7405,
      "step": 271200
    },
    {
      "epoch": 2.475545660267173,
      "grad_norm": 4.8242950439453125,
      "learning_rate": 4.7937045283110695e-05,
      "loss": 0.7557,
      "step": 271300
    },
    {
      "epoch": 2.476458135630338,
      "grad_norm": 4.402228832244873,
      "learning_rate": 4.793628488697472e-05,
      "loss": 0.6992,
      "step": 271400
    },
    {
      "epoch": 2.4773706109935034,
      "grad_norm": 5.198949813842773,
      "learning_rate": 4.793552449083875e-05,
      "loss": 0.7355,
      "step": 271500
    },
    {
      "epoch": 2.4782830863566683,
      "grad_norm": 3.911435842514038,
      "learning_rate": 4.793476409470278e-05,
      "loss": 0.7716,
      "step": 271600
    },
    {
      "epoch": 2.4791955617198336,
      "grad_norm": 4.134533882141113,
      "learning_rate": 4.793400369856681e-05,
      "loss": 0.749,
      "step": 271700
    },
    {
      "epoch": 2.480108037082999,
      "grad_norm": 4.262112140655518,
      "learning_rate": 4.793324330243084e-05,
      "loss": 0.7082,
      "step": 271800
    },
    {
      "epoch": 2.4810205124461637,
      "grad_norm": 4.181792736053467,
      "learning_rate": 4.793248290629487e-05,
      "loss": 0.7572,
      "step": 271900
    },
    {
      "epoch": 2.481932987809329,
      "grad_norm": 4.399153709411621,
      "learning_rate": 4.793172251015889e-05,
      "loss": 0.7479,
      "step": 272000
    },
    {
      "epoch": 2.4828454631724943,
      "grad_norm": 3.6383299827575684,
      "learning_rate": 4.793096211402293e-05,
      "loss": 0.7729,
      "step": 272100
    },
    {
      "epoch": 2.4837579385356596,
      "grad_norm": 4.26607084274292,
      "learning_rate": 4.793020171788695e-05,
      "loss": 0.7652,
      "step": 272200
    },
    {
      "epoch": 2.484670413898825,
      "grad_norm": 4.56678581237793,
      "learning_rate": 4.792944132175098e-05,
      "loss": 0.7406,
      "step": 272300
    },
    {
      "epoch": 2.48558288926199,
      "grad_norm": 4.486586093902588,
      "learning_rate": 4.792868092561501e-05,
      "loss": 0.7605,
      "step": 272400
    },
    {
      "epoch": 2.486495364625155,
      "grad_norm": 4.3921589851379395,
      "learning_rate": 4.792792052947904e-05,
      "loss": 0.803,
      "step": 272500
    },
    {
      "epoch": 2.4874078399883204,
      "grad_norm": 4.670212745666504,
      "learning_rate": 4.7927160133343065e-05,
      "loss": 0.7378,
      "step": 272600
    },
    {
      "epoch": 2.4883203153514857,
      "grad_norm": 4.300690174102783,
      "learning_rate": 4.7926399737207095e-05,
      "loss": 0.7624,
      "step": 272700
    },
    {
      "epoch": 2.4892327907146505,
      "grad_norm": 4.28774356842041,
      "learning_rate": 4.7925639341071126e-05,
      "loss": 0.7771,
      "step": 272800
    },
    {
      "epoch": 2.490145266077816,
      "grad_norm": 3.4280688762664795,
      "learning_rate": 4.7924878944935156e-05,
      "loss": 0.7765,
      "step": 272900
    },
    {
      "epoch": 2.491057741440981,
      "grad_norm": 3.2147936820983887,
      "learning_rate": 4.7924118548799186e-05,
      "loss": 0.7879,
      "step": 273000
    },
    {
      "epoch": 2.4919702168041464,
      "grad_norm": 4.3906569480896,
      "learning_rate": 4.792335815266321e-05,
      "loss": 0.7664,
      "step": 273100
    },
    {
      "epoch": 2.4928826921673117,
      "grad_norm": 3.6198384761810303,
      "learning_rate": 4.7922597756527246e-05,
      "loss": 0.7517,
      "step": 273200
    },
    {
      "epoch": 2.4937951675304766,
      "grad_norm": 4.471020698547363,
      "learning_rate": 4.792183736039127e-05,
      "loss": 0.7065,
      "step": 273300
    },
    {
      "epoch": 2.494707642893642,
      "grad_norm": 4.043672561645508,
      "learning_rate": 4.79210769642553e-05,
      "loss": 0.7355,
      "step": 273400
    },
    {
      "epoch": 2.495620118256807,
      "grad_norm": 4.585224151611328,
      "learning_rate": 4.792031656811933e-05,
      "loss": 0.7757,
      "step": 273500
    },
    {
      "epoch": 2.496532593619972,
      "grad_norm": 3.8806118965148926,
      "learning_rate": 4.791955617198336e-05,
      "loss": 0.7775,
      "step": 273600
    },
    {
      "epoch": 2.4974450689831373,
      "grad_norm": 5.415225505828857,
      "learning_rate": 4.791879577584739e-05,
      "loss": 0.7129,
      "step": 273700
    },
    {
      "epoch": 2.4983575443463026,
      "grad_norm": 3.909517765045166,
      "learning_rate": 4.791803537971142e-05,
      "loss": 0.7598,
      "step": 273800
    },
    {
      "epoch": 2.499270019709468,
      "grad_norm": 5.170237064361572,
      "learning_rate": 4.791727498357544e-05,
      "loss": 0.7844,
      "step": 273900
    },
    {
      "epoch": 2.5001824950726332,
      "grad_norm": 4.454148769378662,
      "learning_rate": 4.791651458743947e-05,
      "loss": 0.7257,
      "step": 274000
    },
    {
      "epoch": 2.501094970435798,
      "grad_norm": 4.572015762329102,
      "learning_rate": 4.79157541913035e-05,
      "loss": 0.7997,
      "step": 274100
    },
    {
      "epoch": 2.5020074457989634,
      "grad_norm": 5.056926727294922,
      "learning_rate": 4.791499379516753e-05,
      "loss": 0.703,
      "step": 274200
    },
    {
      "epoch": 2.5029199211621287,
      "grad_norm": 3.694913864135742,
      "learning_rate": 4.791423339903156e-05,
      "loss": 0.7867,
      "step": 274300
    },
    {
      "epoch": 2.5038323965252935,
      "grad_norm": 2.453331470489502,
      "learning_rate": 4.791347300289559e-05,
      "loss": 0.7229,
      "step": 274400
    },
    {
      "epoch": 2.504744871888459,
      "grad_norm": 4.333715438842773,
      "learning_rate": 4.7912712606759616e-05,
      "loss": 0.7714,
      "step": 274500
    },
    {
      "epoch": 2.505657347251624,
      "grad_norm": 3.4057981967926025,
      "learning_rate": 4.791195221062365e-05,
      "loss": 0.7461,
      "step": 274600
    },
    {
      "epoch": 2.5065698226147894,
      "grad_norm": 4.362536430358887,
      "learning_rate": 4.7911191814487676e-05,
      "loss": 0.7128,
      "step": 274700
    },
    {
      "epoch": 2.5074822979779547,
      "grad_norm": 4.073601722717285,
      "learning_rate": 4.7910431418351707e-05,
      "loss": 0.7377,
      "step": 274800
    },
    {
      "epoch": 2.50839477334112,
      "grad_norm": 3.6546249389648438,
      "learning_rate": 4.7909671022215737e-05,
      "loss": 0.7355,
      "step": 274900
    },
    {
      "epoch": 2.509307248704285,
      "grad_norm": 3.9068844318389893,
      "learning_rate": 4.790891062607977e-05,
      "loss": 0.7793,
      "step": 275000
    },
    {
      "epoch": 2.51021972406745,
      "grad_norm": 4.707724094390869,
      "learning_rate": 4.79081502299438e-05,
      "loss": 0.7626,
      "step": 275100
    },
    {
      "epoch": 2.5111321994306155,
      "grad_norm": 4.375301361083984,
      "learning_rate": 4.790738983380783e-05,
      "loss": 0.7625,
      "step": 275200
    },
    {
      "epoch": 2.5120446747937804,
      "grad_norm": 3.756592273712158,
      "learning_rate": 4.790662943767185e-05,
      "loss": 0.7594,
      "step": 275300
    },
    {
      "epoch": 2.5129571501569457,
      "grad_norm": 4.242035388946533,
      "learning_rate": 4.790586904153588e-05,
      "loss": 0.7586,
      "step": 275400
    },
    {
      "epoch": 2.513869625520111,
      "grad_norm": 4.7728962898254395,
      "learning_rate": 4.790510864539991e-05,
      "loss": 0.7692,
      "step": 275500
    },
    {
      "epoch": 2.5147821008832763,
      "grad_norm": 4.065824508666992,
      "learning_rate": 4.7904348249263934e-05,
      "loss": 0.7202,
      "step": 275600
    },
    {
      "epoch": 2.5156945762464415,
      "grad_norm": 4.227772235870361,
      "learning_rate": 4.790358785312797e-05,
      "loss": 0.7204,
      "step": 275700
    },
    {
      "epoch": 2.5166070516096064,
      "grad_norm": 3.9781012535095215,
      "learning_rate": 4.7902827456991994e-05,
      "loss": 0.7753,
      "step": 275800
    },
    {
      "epoch": 2.5175195269727717,
      "grad_norm": 5.133286476135254,
      "learning_rate": 4.7902067060856024e-05,
      "loss": 0.7377,
      "step": 275900
    },
    {
      "epoch": 2.518432002335937,
      "grad_norm": 4.0785136222839355,
      "learning_rate": 4.7901306664720054e-05,
      "loss": 0.718,
      "step": 276000
    },
    {
      "epoch": 2.519344477699102,
      "grad_norm": 4.494305610656738,
      "learning_rate": 4.7900546268584084e-05,
      "loss": 0.746,
      "step": 276100
    },
    {
      "epoch": 2.520256953062267,
      "grad_norm": 4.149068355560303,
      "learning_rate": 4.7899785872448114e-05,
      "loss": 0.776,
      "step": 276200
    },
    {
      "epoch": 2.5211694284254325,
      "grad_norm": 3.9528517723083496,
      "learning_rate": 4.7899025476312144e-05,
      "loss": 0.7562,
      "step": 276300
    },
    {
      "epoch": 2.5220819037885978,
      "grad_norm": 4.362847805023193,
      "learning_rate": 4.789826508017617e-05,
      "loss": 0.7394,
      "step": 276400
    },
    {
      "epoch": 2.522994379151763,
      "grad_norm": 4.690054416656494,
      "learning_rate": 4.7897504684040204e-05,
      "loss": 0.7221,
      "step": 276500
    },
    {
      "epoch": 2.5239068545149284,
      "grad_norm": 4.872218132019043,
      "learning_rate": 4.789674428790423e-05,
      "loss": 0.7374,
      "step": 276600
    },
    {
      "epoch": 2.524819329878093,
      "grad_norm": 3.9604101181030273,
      "learning_rate": 4.789598389176826e-05,
      "loss": 0.7045,
      "step": 276700
    },
    {
      "epoch": 2.5257318052412585,
      "grad_norm": 5.0809197425842285,
      "learning_rate": 4.789522349563229e-05,
      "loss": 0.7362,
      "step": 276800
    },
    {
      "epoch": 2.526644280604424,
      "grad_norm": 4.741335391998291,
      "learning_rate": 4.789446309949632e-05,
      "loss": 0.7354,
      "step": 276900
    },
    {
      "epoch": 2.5275567559675887,
      "grad_norm": 2.8757176399230957,
      "learning_rate": 4.789370270336034e-05,
      "loss": 0.7071,
      "step": 277000
    },
    {
      "epoch": 2.528469231330754,
      "grad_norm": 5.114224910736084,
      "learning_rate": 4.789294230722438e-05,
      "loss": 0.7612,
      "step": 277100
    },
    {
      "epoch": 2.5293817066939193,
      "grad_norm": 2.982736825942993,
      "learning_rate": 4.78921819110884e-05,
      "loss": 0.7506,
      "step": 277200
    },
    {
      "epoch": 2.5302941820570846,
      "grad_norm": 3.9458208084106445,
      "learning_rate": 4.789142151495243e-05,
      "loss": 0.7446,
      "step": 277300
    },
    {
      "epoch": 2.53120665742025,
      "grad_norm": 4.309279918670654,
      "learning_rate": 4.789066111881646e-05,
      "loss": 0.7528,
      "step": 277400
    },
    {
      "epoch": 2.5321191327834147,
      "grad_norm": 4.566804885864258,
      "learning_rate": 4.788990072268049e-05,
      "loss": 0.7424,
      "step": 277500
    },
    {
      "epoch": 2.53303160814658,
      "grad_norm": 3.2585244178771973,
      "learning_rate": 4.788914032654452e-05,
      "loss": 0.722,
      "step": 277600
    },
    {
      "epoch": 2.5339440835097453,
      "grad_norm": 3.7985050678253174,
      "learning_rate": 4.788837993040855e-05,
      "loss": 0.7416,
      "step": 277700
    },
    {
      "epoch": 2.53485655887291,
      "grad_norm": 4.24315071105957,
      "learning_rate": 4.7887619534272575e-05,
      "loss": 0.7853,
      "step": 277800
    },
    {
      "epoch": 2.5357690342360755,
      "grad_norm": 5.091392517089844,
      "learning_rate": 4.788685913813661e-05,
      "loss": 0.7202,
      "step": 277900
    },
    {
      "epoch": 2.5366815095992408,
      "grad_norm": 3.834951639175415,
      "learning_rate": 4.7886098742000635e-05,
      "loss": 0.7383,
      "step": 278000
    },
    {
      "epoch": 2.537593984962406,
      "grad_norm": 4.691978454589844,
      "learning_rate": 4.7885338345864665e-05,
      "loss": 0.7621,
      "step": 278100
    },
    {
      "epoch": 2.5385064603255714,
      "grad_norm": 4.272651672363281,
      "learning_rate": 4.7884577949728695e-05,
      "loss": 0.7517,
      "step": 278200
    },
    {
      "epoch": 2.5394189356887367,
      "grad_norm": 3.974968433380127,
      "learning_rate": 4.7883817553592725e-05,
      "loss": 0.7397,
      "step": 278300
    },
    {
      "epoch": 2.5403314110519015,
      "grad_norm": 3.582533121109009,
      "learning_rate": 4.788305715745675e-05,
      "loss": 0.7474,
      "step": 278400
    },
    {
      "epoch": 2.541243886415067,
      "grad_norm": 4.418085098266602,
      "learning_rate": 4.788229676132078e-05,
      "loss": 0.7514,
      "step": 278500
    },
    {
      "epoch": 2.542156361778232,
      "grad_norm": 3.9760055541992188,
      "learning_rate": 4.788153636518481e-05,
      "loss": 0.7248,
      "step": 278600
    },
    {
      "epoch": 2.543068837141397,
      "grad_norm": 4.94012975692749,
      "learning_rate": 4.788077596904884e-05,
      "loss": 0.7857,
      "step": 278700
    },
    {
      "epoch": 2.5439813125045623,
      "grad_norm": 4.231592178344727,
      "learning_rate": 4.788001557291287e-05,
      "loss": 0.7282,
      "step": 278800
    },
    {
      "epoch": 2.5448937878677276,
      "grad_norm": 3.711662769317627,
      "learning_rate": 4.787925517677689e-05,
      "loss": 0.8066,
      "step": 278900
    },
    {
      "epoch": 2.545806263230893,
      "grad_norm": 4.514216423034668,
      "learning_rate": 4.787849478064093e-05,
      "loss": 0.7677,
      "step": 279000
    },
    {
      "epoch": 2.546718738594058,
      "grad_norm": 4.49628210067749,
      "learning_rate": 4.787773438450495e-05,
      "loss": 0.7409,
      "step": 279100
    },
    {
      "epoch": 2.547631213957223,
      "grad_norm": 5.071300983428955,
      "learning_rate": 4.787697398836898e-05,
      "loss": 0.7161,
      "step": 279200
    },
    {
      "epoch": 2.5485436893203883,
      "grad_norm": 3.8326056003570557,
      "learning_rate": 4.787621359223301e-05,
      "loss": 0.7359,
      "step": 279300
    },
    {
      "epoch": 2.5494561646835536,
      "grad_norm": 3.209555149078369,
      "learning_rate": 4.787545319609704e-05,
      "loss": 0.7657,
      "step": 279400
    },
    {
      "epoch": 2.5503686400467185,
      "grad_norm": 3.9977033138275146,
      "learning_rate": 4.7874692799961065e-05,
      "loss": 0.758,
      "step": 279500
    },
    {
      "epoch": 2.551281115409884,
      "grad_norm": 3.8941805362701416,
      "learning_rate": 4.78739324038251e-05,
      "loss": 0.7924,
      "step": 279600
    },
    {
      "epoch": 2.552193590773049,
      "grad_norm": 4.479129314422607,
      "learning_rate": 4.7873172007689126e-05,
      "loss": 0.7337,
      "step": 279700
    },
    {
      "epoch": 2.5531060661362144,
      "grad_norm": 3.926027297973633,
      "learning_rate": 4.7872411611553156e-05,
      "loss": 0.713,
      "step": 279800
    },
    {
      "epoch": 2.5540185414993797,
      "grad_norm": 3.7855098247528076,
      "learning_rate": 4.7871651215417186e-05,
      "loss": 0.7422,
      "step": 279900
    },
    {
      "epoch": 2.554931016862545,
      "grad_norm": 5.528927326202393,
      "learning_rate": 4.7870890819281216e-05,
      "loss": 0.7918,
      "step": 280000
    },
    {
      "epoch": 2.55584349222571,
      "grad_norm": 4.931251049041748,
      "learning_rate": 4.7870130423145246e-05,
      "loss": 0.7085,
      "step": 280100
    },
    {
      "epoch": 2.556755967588875,
      "grad_norm": 3.9742631912231445,
      "learning_rate": 4.7869370027009276e-05,
      "loss": 0.7463,
      "step": 280200
    },
    {
      "epoch": 2.5576684429520404,
      "grad_norm": 4.416943073272705,
      "learning_rate": 4.78686096308733e-05,
      "loss": 0.7603,
      "step": 280300
    },
    {
      "epoch": 2.5585809183152053,
      "grad_norm": 5.196197986602783,
      "learning_rate": 4.7867849234737336e-05,
      "loss": 0.7198,
      "step": 280400
    },
    {
      "epoch": 2.5594933936783706,
      "grad_norm": 4.023265838623047,
      "learning_rate": 4.786708883860136e-05,
      "loss": 0.7542,
      "step": 280500
    },
    {
      "epoch": 2.560405869041536,
      "grad_norm": 5.33119010925293,
      "learning_rate": 4.786632844246539e-05,
      "loss": 0.725,
      "step": 280600
    },
    {
      "epoch": 2.561318344404701,
      "grad_norm": 4.2814040184021,
      "learning_rate": 4.786556804632942e-05,
      "loss": 0.7425,
      "step": 280700
    },
    {
      "epoch": 2.5622308197678665,
      "grad_norm": 4.37056303024292,
      "learning_rate": 4.786480765019345e-05,
      "loss": 0.7231,
      "step": 280800
    },
    {
      "epoch": 2.5631432951310313,
      "grad_norm": 4.623863220214844,
      "learning_rate": 4.786404725405747e-05,
      "loss": 0.7212,
      "step": 280900
    },
    {
      "epoch": 2.5640557704941966,
      "grad_norm": 4.825305461883545,
      "learning_rate": 4.786328685792151e-05,
      "loss": 0.7445,
      "step": 281000
    },
    {
      "epoch": 2.564968245857362,
      "grad_norm": 4.134830951690674,
      "learning_rate": 4.786252646178553e-05,
      "loss": 0.7004,
      "step": 281100
    },
    {
      "epoch": 2.565880721220527,
      "grad_norm": 4.363703727722168,
      "learning_rate": 4.786176606564956e-05,
      "loss": 0.7524,
      "step": 281200
    },
    {
      "epoch": 2.566793196583692,
      "grad_norm": 4.549022674560547,
      "learning_rate": 4.786100566951359e-05,
      "loss": 0.7626,
      "step": 281300
    },
    {
      "epoch": 2.5677056719468574,
      "grad_norm": 3.462526559829712,
      "learning_rate": 4.7860245273377616e-05,
      "loss": 0.7688,
      "step": 281400
    },
    {
      "epoch": 2.5686181473100227,
      "grad_norm": 4.305879592895508,
      "learning_rate": 4.785948487724165e-05,
      "loss": 0.7853,
      "step": 281500
    },
    {
      "epoch": 2.569530622673188,
      "grad_norm": 3.898777961730957,
      "learning_rate": 4.7858724481105677e-05,
      "loss": 0.7414,
      "step": 281600
    },
    {
      "epoch": 2.5704430980363533,
      "grad_norm": 4.0203962326049805,
      "learning_rate": 4.785796408496971e-05,
      "loss": 0.7588,
      "step": 281700
    },
    {
      "epoch": 2.571355573399518,
      "grad_norm": 4.554797649383545,
      "learning_rate": 4.785720368883374e-05,
      "loss": 0.7172,
      "step": 281800
    },
    {
      "epoch": 2.5722680487626834,
      "grad_norm": 3.9171864986419678,
      "learning_rate": 4.785644329269777e-05,
      "loss": 0.7646,
      "step": 281900
    },
    {
      "epoch": 2.5731805241258487,
      "grad_norm": 3.900747776031494,
      "learning_rate": 4.785568289656179e-05,
      "loss": 0.7418,
      "step": 282000
    },
    {
      "epoch": 2.5740929994890136,
      "grad_norm": 2.5544142723083496,
      "learning_rate": 4.785492250042583e-05,
      "loss": 0.7422,
      "step": 282100
    },
    {
      "epoch": 2.575005474852179,
      "grad_norm": 4.4294257164001465,
      "learning_rate": 4.785416210428985e-05,
      "loss": 0.735,
      "step": 282200
    },
    {
      "epoch": 2.575917950215344,
      "grad_norm": 4.3035173416137695,
      "learning_rate": 4.785340170815388e-05,
      "loss": 0.7835,
      "step": 282300
    },
    {
      "epoch": 2.5768304255785095,
      "grad_norm": 4.2013092041015625,
      "learning_rate": 4.785264131201791e-05,
      "loss": 0.7674,
      "step": 282400
    },
    {
      "epoch": 2.577742900941675,
      "grad_norm": 4.392180442810059,
      "learning_rate": 4.785188091588194e-05,
      "loss": 0.7471,
      "step": 282500
    },
    {
      "epoch": 2.5786553763048397,
      "grad_norm": 4.025987148284912,
      "learning_rate": 4.785112051974597e-05,
      "loss": 0.7247,
      "step": 282600
    },
    {
      "epoch": 2.579567851668005,
      "grad_norm": 4.54083251953125,
      "learning_rate": 4.785036012361e-05,
      "loss": 0.7562,
      "step": 282700
    },
    {
      "epoch": 2.5804803270311703,
      "grad_norm": 4.724671363830566,
      "learning_rate": 4.7849599727474024e-05,
      "loss": 0.744,
      "step": 282800
    },
    {
      "epoch": 2.581392802394335,
      "grad_norm": 3.9653472900390625,
      "learning_rate": 4.784883933133806e-05,
      "loss": 0.7583,
      "step": 282900
    },
    {
      "epoch": 2.5823052777575004,
      "grad_norm": 3.7059497833251953,
      "learning_rate": 4.7848078935202084e-05,
      "loss": 0.7491,
      "step": 283000
    },
    {
      "epoch": 2.5832177531206657,
      "grad_norm": 3.742581605911255,
      "learning_rate": 4.7847318539066114e-05,
      "loss": 0.7649,
      "step": 283100
    },
    {
      "epoch": 2.584130228483831,
      "grad_norm": 4.231660842895508,
      "learning_rate": 4.7846558142930144e-05,
      "loss": 0.7082,
      "step": 283200
    },
    {
      "epoch": 2.5850427038469963,
      "grad_norm": 4.776417255401611,
      "learning_rate": 4.7845797746794174e-05,
      "loss": 0.7268,
      "step": 283300
    },
    {
      "epoch": 2.5859551792101616,
      "grad_norm": 4.148813247680664,
      "learning_rate": 4.78450373506582e-05,
      "loss": 0.7529,
      "step": 283400
    },
    {
      "epoch": 2.5868676545733265,
      "grad_norm": 4.681365966796875,
      "learning_rate": 4.7844276954522234e-05,
      "loss": 0.7416,
      "step": 283500
    },
    {
      "epoch": 2.5877801299364918,
      "grad_norm": 3.856583833694458,
      "learning_rate": 4.784351655838626e-05,
      "loss": 0.7588,
      "step": 283600
    },
    {
      "epoch": 2.588692605299657,
      "grad_norm": 4.6663103103637695,
      "learning_rate": 4.784275616225029e-05,
      "loss": 0.757,
      "step": 283700
    },
    {
      "epoch": 2.589605080662822,
      "grad_norm": 3.8162953853607178,
      "learning_rate": 4.784199576611432e-05,
      "loss": 0.7414,
      "step": 283800
    },
    {
      "epoch": 2.590517556025987,
      "grad_norm": 4.808746337890625,
      "learning_rate": 4.784123536997835e-05,
      "loss": 0.7162,
      "step": 283900
    },
    {
      "epoch": 2.5914300313891525,
      "grad_norm": 4.16181755065918,
      "learning_rate": 4.784047497384238e-05,
      "loss": 0.8117,
      "step": 284000
    },
    {
      "epoch": 2.592342506752318,
      "grad_norm": 4.629458904266357,
      "learning_rate": 4.78397145777064e-05,
      "loss": 0.7641,
      "step": 284100
    },
    {
      "epoch": 2.593254982115483,
      "grad_norm": 3.7545628547668457,
      "learning_rate": 4.783895418157043e-05,
      "loss": 0.7544,
      "step": 284200
    },
    {
      "epoch": 2.594167457478648,
      "grad_norm": 4.05334997177124,
      "learning_rate": 4.783819378543446e-05,
      "loss": 0.7597,
      "step": 284300
    },
    {
      "epoch": 2.5950799328418133,
      "grad_norm": 3.9475674629211426,
      "learning_rate": 4.783743338929849e-05,
      "loss": 0.7468,
      "step": 284400
    },
    {
      "epoch": 2.5959924082049786,
      "grad_norm": 4.356289863586426,
      "learning_rate": 4.7836672993162515e-05,
      "loss": 0.7428,
      "step": 284500
    },
    {
      "epoch": 2.5969048835681434,
      "grad_norm": 4.1399970054626465,
      "learning_rate": 4.783591259702655e-05,
      "loss": 0.7421,
      "step": 284600
    },
    {
      "epoch": 2.5978173589313087,
      "grad_norm": 4.621036529541016,
      "learning_rate": 4.7835152200890575e-05,
      "loss": 0.769,
      "step": 284700
    },
    {
      "epoch": 2.598729834294474,
      "grad_norm": 4.204224586486816,
      "learning_rate": 4.7834391804754605e-05,
      "loss": 0.7486,
      "step": 284800
    },
    {
      "epoch": 2.5996423096576393,
      "grad_norm": 4.043595314025879,
      "learning_rate": 4.7833631408618635e-05,
      "loss": 0.7477,
      "step": 284900
    },
    {
      "epoch": 2.6005547850208046,
      "grad_norm": 4.372663497924805,
      "learning_rate": 4.7832871012482665e-05,
      "loss": 0.7657,
      "step": 285000
    },
    {
      "epoch": 2.6014672603839695,
      "grad_norm": 4.337125301361084,
      "learning_rate": 4.7832110616346695e-05,
      "loss": 0.7744,
      "step": 285100
    },
    {
      "epoch": 2.6023797357471348,
      "grad_norm": 3.1980929374694824,
      "learning_rate": 4.7831350220210725e-05,
      "loss": 0.7316,
      "step": 285200
    },
    {
      "epoch": 2.6032922111103,
      "grad_norm": 4.564276218414307,
      "learning_rate": 4.783058982407475e-05,
      "loss": 0.753,
      "step": 285300
    },
    {
      "epoch": 2.6042046864734654,
      "grad_norm": 4.494230270385742,
      "learning_rate": 4.7829829427938785e-05,
      "loss": 0.7345,
      "step": 285400
    },
    {
      "epoch": 2.6051171618366302,
      "grad_norm": 2.818844795227051,
      "learning_rate": 4.782906903180281e-05,
      "loss": 0.7472,
      "step": 285500
    },
    {
      "epoch": 2.6060296371997955,
      "grad_norm": 4.788093566894531,
      "learning_rate": 4.782830863566684e-05,
      "loss": 0.7185,
      "step": 285600
    },
    {
      "epoch": 2.606942112562961,
      "grad_norm": 3.553546667098999,
      "learning_rate": 4.782754823953087e-05,
      "loss": 0.7253,
      "step": 285700
    },
    {
      "epoch": 2.607854587926126,
      "grad_norm": 3.2920382022857666,
      "learning_rate": 4.78267878433949e-05,
      "loss": 0.7536,
      "step": 285800
    },
    {
      "epoch": 2.6087670632892914,
      "grad_norm": 4.470716953277588,
      "learning_rate": 4.782602744725892e-05,
      "loss": 0.7302,
      "step": 285900
    },
    {
      "epoch": 2.6096795386524563,
      "grad_norm": 4.138998031616211,
      "learning_rate": 4.782526705112296e-05,
      "loss": 0.7663,
      "step": 286000
    },
    {
      "epoch": 2.6105920140156216,
      "grad_norm": 3.930187940597534,
      "learning_rate": 4.782450665498698e-05,
      "loss": 0.7739,
      "step": 286100
    },
    {
      "epoch": 2.611504489378787,
      "grad_norm": 4.4228363037109375,
      "learning_rate": 4.782374625885101e-05,
      "loss": 0.754,
      "step": 286200
    },
    {
      "epoch": 2.6124169647419517,
      "grad_norm": 4.375722408294678,
      "learning_rate": 4.782298586271504e-05,
      "loss": 0.7164,
      "step": 286300
    },
    {
      "epoch": 2.613329440105117,
      "grad_norm": 4.133322715759277,
      "learning_rate": 4.782222546657907e-05,
      "loss": 0.7458,
      "step": 286400
    },
    {
      "epoch": 2.6142419154682823,
      "grad_norm": 4.316246509552002,
      "learning_rate": 4.78214650704431e-05,
      "loss": 0.7578,
      "step": 286500
    },
    {
      "epoch": 2.6151543908314476,
      "grad_norm": 3.402714490890503,
      "learning_rate": 4.782070467430713e-05,
      "loss": 0.7687,
      "step": 286600
    },
    {
      "epoch": 2.616066866194613,
      "grad_norm": 4.153851509094238,
      "learning_rate": 4.7819944278171156e-05,
      "loss": 0.7251,
      "step": 286700
    },
    {
      "epoch": 2.616979341557778,
      "grad_norm": 4.413261413574219,
      "learning_rate": 4.781918388203519e-05,
      "loss": 0.715,
      "step": 286800
    },
    {
      "epoch": 2.617891816920943,
      "grad_norm": 4.299780368804932,
      "learning_rate": 4.7818423485899216e-05,
      "loss": 0.7777,
      "step": 286900
    },
    {
      "epoch": 2.6188042922841084,
      "grad_norm": 3.7241499423980713,
      "learning_rate": 4.7817663089763246e-05,
      "loss": 0.7555,
      "step": 287000
    },
    {
      "epoch": 2.6197167676472737,
      "grad_norm": 4.777988433837891,
      "learning_rate": 4.7816902693627276e-05,
      "loss": 0.7663,
      "step": 287100
    },
    {
      "epoch": 2.6206292430104385,
      "grad_norm": 4.606156349182129,
      "learning_rate": 4.78161422974913e-05,
      "loss": 0.7545,
      "step": 287200
    },
    {
      "epoch": 2.621541718373604,
      "grad_norm": 3.883309841156006,
      "learning_rate": 4.781538190135533e-05,
      "loss": 0.7607,
      "step": 287300
    },
    {
      "epoch": 2.622454193736769,
      "grad_norm": 4.5049638748168945,
      "learning_rate": 4.781462150521936e-05,
      "loss": 0.7485,
      "step": 287400
    },
    {
      "epoch": 2.6233666690999344,
      "grad_norm": 4.651785373687744,
      "learning_rate": 4.781386110908339e-05,
      "loss": 0.7261,
      "step": 287500
    },
    {
      "epoch": 2.6242791444630997,
      "grad_norm": 3.9082045555114746,
      "learning_rate": 4.781310071294742e-05,
      "loss": 0.7223,
      "step": 287600
    },
    {
      "epoch": 2.6251916198262646,
      "grad_norm": 4.780580043792725,
      "learning_rate": 4.781234031681145e-05,
      "loss": 0.7449,
      "step": 287700
    },
    {
      "epoch": 2.62610409518943,
      "grad_norm": 4.998707294464111,
      "learning_rate": 4.781157992067547e-05,
      "loss": 0.7878,
      "step": 287800
    },
    {
      "epoch": 2.627016570552595,
      "grad_norm": 4.726370334625244,
      "learning_rate": 4.781081952453951e-05,
      "loss": 0.7294,
      "step": 287900
    },
    {
      "epoch": 2.62792904591576,
      "grad_norm": 3.3272745609283447,
      "learning_rate": 4.781005912840353e-05,
      "loss": 0.7882,
      "step": 288000
    },
    {
      "epoch": 2.6288415212789253,
      "grad_norm": 4.801259994506836,
      "learning_rate": 4.780929873226756e-05,
      "loss": 0.7524,
      "step": 288100
    },
    {
      "epoch": 2.6297539966420906,
      "grad_norm": 4.667530059814453,
      "learning_rate": 4.780853833613159e-05,
      "loss": 0.7612,
      "step": 288200
    },
    {
      "epoch": 2.630666472005256,
      "grad_norm": 3.4461910724639893,
      "learning_rate": 4.780777793999562e-05,
      "loss": 0.7188,
      "step": 288300
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 5.159572601318359,
      "learning_rate": 4.780701754385965e-05,
      "loss": 0.7218,
      "step": 288400
    },
    {
      "epoch": 2.632491422731586,
      "grad_norm": 4.818763256072998,
      "learning_rate": 4.7806257147723683e-05,
      "loss": 0.7018,
      "step": 288500
    },
    {
      "epoch": 2.6334038980947514,
      "grad_norm": 3.7524795532226562,
      "learning_rate": 4.780549675158771e-05,
      "loss": 0.7587,
      "step": 288600
    },
    {
      "epoch": 2.6343163734579167,
      "grad_norm": 3.7741754055023193,
      "learning_rate": 4.7804736355451744e-05,
      "loss": 0.7603,
      "step": 288700
    },
    {
      "epoch": 2.635228848821082,
      "grad_norm": 3.843740224838257,
      "learning_rate": 4.780397595931577e-05,
      "loss": 0.7192,
      "step": 288800
    },
    {
      "epoch": 2.636141324184247,
      "grad_norm": 2.428179979324341,
      "learning_rate": 4.78032155631798e-05,
      "loss": 0.7297,
      "step": 288900
    },
    {
      "epoch": 2.637053799547412,
      "grad_norm": 3.8244211673736572,
      "learning_rate": 4.780245516704383e-05,
      "loss": 0.7224,
      "step": 289000
    },
    {
      "epoch": 2.6379662749105774,
      "grad_norm": 4.738110065460205,
      "learning_rate": 4.780169477090786e-05,
      "loss": 0.6912,
      "step": 289100
    },
    {
      "epoch": 2.6388787502737427,
      "grad_norm": 5.203180313110352,
      "learning_rate": 4.780093437477188e-05,
      "loss": 0.7302,
      "step": 289200
    },
    {
      "epoch": 2.639791225636908,
      "grad_norm": 4.618938446044922,
      "learning_rate": 4.780017397863592e-05,
      "loss": 0.7689,
      "step": 289300
    },
    {
      "epoch": 2.640703701000073,
      "grad_norm": 4.211232662200928,
      "learning_rate": 4.779941358249994e-05,
      "loss": 0.7716,
      "step": 289400
    },
    {
      "epoch": 2.641616176363238,
      "grad_norm": 5.109865188598633,
      "learning_rate": 4.779865318636397e-05,
      "loss": 0.7603,
      "step": 289500
    },
    {
      "epoch": 2.6425286517264035,
      "grad_norm": 3.786914110183716,
      "learning_rate": 4.7797892790228e-05,
      "loss": 0.7151,
      "step": 289600
    },
    {
      "epoch": 2.6434411270895684,
      "grad_norm": 3.6184728145599365,
      "learning_rate": 4.7797132394092024e-05,
      "loss": 0.7717,
      "step": 289700
    },
    {
      "epoch": 2.6443536024527337,
      "grad_norm": 3.429318428039551,
      "learning_rate": 4.779637199795606e-05,
      "loss": 0.7559,
      "step": 289800
    },
    {
      "epoch": 2.645266077815899,
      "grad_norm": 4.2909111976623535,
      "learning_rate": 4.7795611601820084e-05,
      "loss": 0.7441,
      "step": 289900
    },
    {
      "epoch": 2.6461785531790643,
      "grad_norm": 4.534847259521484,
      "learning_rate": 4.7794851205684114e-05,
      "loss": 0.7372,
      "step": 290000
    },
    {
      "epoch": 2.6470910285422296,
      "grad_norm": 4.544794082641602,
      "learning_rate": 4.7794090809548144e-05,
      "loss": 0.7691,
      "step": 290100
    },
    {
      "epoch": 2.6480035039053944,
      "grad_norm": 3.909726619720459,
      "learning_rate": 4.7793330413412174e-05,
      "loss": 0.7268,
      "step": 290200
    },
    {
      "epoch": 2.6489159792685597,
      "grad_norm": 4.422880172729492,
      "learning_rate": 4.77925700172762e-05,
      "loss": 0.7416,
      "step": 290300
    },
    {
      "epoch": 2.649828454631725,
      "grad_norm": 3.6938819885253906,
      "learning_rate": 4.7791809621140234e-05,
      "loss": 0.7608,
      "step": 290400
    },
    {
      "epoch": 2.65074092999489,
      "grad_norm": 2.6570537090301514,
      "learning_rate": 4.779104922500426e-05,
      "loss": 0.7724,
      "step": 290500
    },
    {
      "epoch": 2.651653405358055,
      "grad_norm": 4.555510520935059,
      "learning_rate": 4.779028882886829e-05,
      "loss": 0.7518,
      "step": 290600
    },
    {
      "epoch": 2.6525658807212205,
      "grad_norm": 4.3610615730285645,
      "learning_rate": 4.778952843273232e-05,
      "loss": 0.7562,
      "step": 290700
    },
    {
      "epoch": 2.6534783560843858,
      "grad_norm": 3.1902222633361816,
      "learning_rate": 4.778876803659635e-05,
      "loss": 0.7128,
      "step": 290800
    },
    {
      "epoch": 2.654390831447551,
      "grad_norm": 3.7394392490386963,
      "learning_rate": 4.778800764046038e-05,
      "loss": 0.7265,
      "step": 290900
    },
    {
      "epoch": 2.6553033068107164,
      "grad_norm": 4.149031162261963,
      "learning_rate": 4.778724724432441e-05,
      "loss": 0.7613,
      "step": 291000
    },
    {
      "epoch": 2.656215782173881,
      "grad_norm": 3.5907647609710693,
      "learning_rate": 4.778648684818843e-05,
      "loss": 0.7926,
      "step": 291100
    },
    {
      "epoch": 2.6571282575370465,
      "grad_norm": 3.685659646987915,
      "learning_rate": 4.778572645205247e-05,
      "loss": 0.7651,
      "step": 291200
    },
    {
      "epoch": 2.658040732900212,
      "grad_norm": 4.865911483764648,
      "learning_rate": 4.778496605591649e-05,
      "loss": 0.7514,
      "step": 291300
    },
    {
      "epoch": 2.6589532082633767,
      "grad_norm": 3.749899387359619,
      "learning_rate": 4.778420565978052e-05,
      "loss": 0.7499,
      "step": 291400
    },
    {
      "epoch": 2.659865683626542,
      "grad_norm": 3.6202502250671387,
      "learning_rate": 4.778344526364455e-05,
      "loss": 0.7535,
      "step": 291500
    },
    {
      "epoch": 2.6607781589897073,
      "grad_norm": 3.479508876800537,
      "learning_rate": 4.778268486750858e-05,
      "loss": 0.7785,
      "step": 291600
    },
    {
      "epoch": 2.6616906343528726,
      "grad_norm": 4.159045219421387,
      "learning_rate": 4.7781924471372605e-05,
      "loss": 0.7282,
      "step": 291700
    },
    {
      "epoch": 2.662603109716038,
      "grad_norm": 5.164236068725586,
      "learning_rate": 4.778116407523664e-05,
      "loss": 0.7377,
      "step": 291800
    },
    {
      "epoch": 2.6635155850792027,
      "grad_norm": 4.921372413635254,
      "learning_rate": 4.7780403679100665e-05,
      "loss": 0.7521,
      "step": 291900
    },
    {
      "epoch": 2.664428060442368,
      "grad_norm": 4.862634658813477,
      "learning_rate": 4.7779643282964695e-05,
      "loss": 0.7249,
      "step": 292000
    },
    {
      "epoch": 2.6653405358055333,
      "grad_norm": 4.2770094871521,
      "learning_rate": 4.7778882886828725e-05,
      "loss": 0.7448,
      "step": 292100
    },
    {
      "epoch": 2.666253011168698,
      "grad_norm": 3.8614015579223633,
      "learning_rate": 4.7778122490692755e-05,
      "loss": 0.7765,
      "step": 292200
    },
    {
      "epoch": 2.6671654865318635,
      "grad_norm": 4.197534084320068,
      "learning_rate": 4.7777362094556785e-05,
      "loss": 0.7514,
      "step": 292300
    },
    {
      "epoch": 2.6680779618950288,
      "grad_norm": 3.910181999206543,
      "learning_rate": 4.7776601698420815e-05,
      "loss": 0.7191,
      "step": 292400
    },
    {
      "epoch": 2.668990437258194,
      "grad_norm": 3.966296434402466,
      "learning_rate": 4.777584130228484e-05,
      "loss": 0.7378,
      "step": 292500
    },
    {
      "epoch": 2.6699029126213594,
      "grad_norm": 4.4865403175354,
      "learning_rate": 4.777508090614887e-05,
      "loss": 0.7083,
      "step": 292600
    },
    {
      "epoch": 2.6708153879845247,
      "grad_norm": 3.9437084197998047,
      "learning_rate": 4.77743205100129e-05,
      "loss": 0.7554,
      "step": 292700
    },
    {
      "epoch": 2.6717278633476895,
      "grad_norm": 4.479506015777588,
      "learning_rate": 4.777356011387692e-05,
      "loss": 0.7565,
      "step": 292800
    },
    {
      "epoch": 2.672640338710855,
      "grad_norm": 3.98154878616333,
      "learning_rate": 4.777279971774096e-05,
      "loss": 0.7604,
      "step": 292900
    },
    {
      "epoch": 2.67355281407402,
      "grad_norm": 4.395720958709717,
      "learning_rate": 4.777203932160498e-05,
      "loss": 0.733,
      "step": 293000
    },
    {
      "epoch": 2.674465289437185,
      "grad_norm": 4.050372123718262,
      "learning_rate": 4.777127892546901e-05,
      "loss": 0.7033,
      "step": 293100
    },
    {
      "epoch": 2.6753777648003503,
      "grad_norm": 3.9869985580444336,
      "learning_rate": 4.777051852933304e-05,
      "loss": 0.7517,
      "step": 293200
    },
    {
      "epoch": 2.6762902401635156,
      "grad_norm": 4.042055606842041,
      "learning_rate": 4.776975813319707e-05,
      "loss": 0.727,
      "step": 293300
    },
    {
      "epoch": 2.677202715526681,
      "grad_norm": 4.069693565368652,
      "learning_rate": 4.77689977370611e-05,
      "loss": 0.7612,
      "step": 293400
    },
    {
      "epoch": 2.678115190889846,
      "grad_norm": 3.2015421390533447,
      "learning_rate": 4.776823734092513e-05,
      "loss": 0.7531,
      "step": 293500
    },
    {
      "epoch": 2.679027666253011,
      "grad_norm": 2.7357630729675293,
      "learning_rate": 4.7767476944789156e-05,
      "loss": 0.7587,
      "step": 293600
    },
    {
      "epoch": 2.6799401416161763,
      "grad_norm": 5.036851406097412,
      "learning_rate": 4.776671654865319e-05,
      "loss": 0.7588,
      "step": 293700
    },
    {
      "epoch": 2.6808526169793416,
      "grad_norm": 5.818563461303711,
      "learning_rate": 4.7765956152517216e-05,
      "loss": 0.736,
      "step": 293800
    },
    {
      "epoch": 2.6817650923425065,
      "grad_norm": 4.294064044952393,
      "learning_rate": 4.7765195756381246e-05,
      "loss": 0.7636,
      "step": 293900
    },
    {
      "epoch": 2.682677567705672,
      "grad_norm": 5.039607524871826,
      "learning_rate": 4.7764435360245276e-05,
      "loss": 0.7265,
      "step": 294000
    },
    {
      "epoch": 2.683590043068837,
      "grad_norm": 3.959088087081909,
      "learning_rate": 4.7763674964109306e-05,
      "loss": 0.7694,
      "step": 294100
    },
    {
      "epoch": 2.6845025184320024,
      "grad_norm": 4.242626667022705,
      "learning_rate": 4.776291456797333e-05,
      "loss": 0.7306,
      "step": 294200
    },
    {
      "epoch": 2.6854149937951677,
      "grad_norm": 4.723871231079102,
      "learning_rate": 4.7762154171837366e-05,
      "loss": 0.7207,
      "step": 294300
    },
    {
      "epoch": 2.686327469158333,
      "grad_norm": 4.045836448669434,
      "learning_rate": 4.776139377570139e-05,
      "loss": 0.733,
      "step": 294400
    },
    {
      "epoch": 2.687239944521498,
      "grad_norm": 4.080225944519043,
      "learning_rate": 4.776063337956542e-05,
      "loss": 0.7182,
      "step": 294500
    },
    {
      "epoch": 2.688152419884663,
      "grad_norm": 4.342936038970947,
      "learning_rate": 4.775987298342945e-05,
      "loss": 0.7385,
      "step": 294600
    },
    {
      "epoch": 2.6890648952478284,
      "grad_norm": 4.165325164794922,
      "learning_rate": 4.775911258729348e-05,
      "loss": 0.7711,
      "step": 294700
    },
    {
      "epoch": 2.6899773706109933,
      "grad_norm": 4.637117385864258,
      "learning_rate": 4.775835219115751e-05,
      "loss": 0.7247,
      "step": 294800
    },
    {
      "epoch": 2.6908898459741586,
      "grad_norm": 3.866770029067993,
      "learning_rate": 4.775759179502154e-05,
      "loss": 0.713,
      "step": 294900
    },
    {
      "epoch": 2.691802321337324,
      "grad_norm": 3.9071507453918457,
      "learning_rate": 4.775683139888556e-05,
      "loss": 0.7527,
      "step": 295000
    },
    {
      "epoch": 2.692714796700489,
      "grad_norm": 4.112789154052734,
      "learning_rate": 4.77560710027496e-05,
      "loss": 0.7513,
      "step": 295100
    },
    {
      "epoch": 2.6936272720636545,
      "grad_norm": 3.3342344760894775,
      "learning_rate": 4.7755310606613623e-05,
      "loss": 0.7511,
      "step": 295200
    },
    {
      "epoch": 2.6945397474268193,
      "grad_norm": 4.225005149841309,
      "learning_rate": 4.7754550210477653e-05,
      "loss": 0.7753,
      "step": 295300
    },
    {
      "epoch": 2.6954522227899846,
      "grad_norm": 4.010132312774658,
      "learning_rate": 4.7753789814341684e-05,
      "loss": 0.7427,
      "step": 295400
    },
    {
      "epoch": 2.69636469815315,
      "grad_norm": 4.209323406219482,
      "learning_rate": 4.775302941820571e-05,
      "loss": 0.7669,
      "step": 295500
    },
    {
      "epoch": 2.697277173516315,
      "grad_norm": 3.693878173828125,
      "learning_rate": 4.775226902206974e-05,
      "loss": 0.7608,
      "step": 295600
    },
    {
      "epoch": 2.69818964887948,
      "grad_norm": 3.697537899017334,
      "learning_rate": 4.775150862593377e-05,
      "loss": 0.7509,
      "step": 295700
    },
    {
      "epoch": 2.6991021242426454,
      "grad_norm": 4.351677894592285,
      "learning_rate": 4.77507482297978e-05,
      "loss": 0.7333,
      "step": 295800
    },
    {
      "epoch": 2.7000145996058107,
      "grad_norm": 4.593519687652588,
      "learning_rate": 4.774998783366183e-05,
      "loss": 0.7139,
      "step": 295900
    },
    {
      "epoch": 2.700927074968976,
      "grad_norm": 4.578489780426025,
      "learning_rate": 4.774922743752586e-05,
      "loss": 0.7523,
      "step": 296000
    },
    {
      "epoch": 2.7018395503321413,
      "grad_norm": 4.717926502227783,
      "learning_rate": 4.774846704138988e-05,
      "loss": 0.7373,
      "step": 296100
    },
    {
      "epoch": 2.702752025695306,
      "grad_norm": 4.473537445068359,
      "learning_rate": 4.774770664525392e-05,
      "loss": 0.7602,
      "step": 296200
    },
    {
      "epoch": 2.7036645010584714,
      "grad_norm": 4.238425254821777,
      "learning_rate": 4.774694624911794e-05,
      "loss": 0.7466,
      "step": 296300
    },
    {
      "epoch": 2.7045769764216367,
      "grad_norm": 3.211223602294922,
      "learning_rate": 4.774618585298197e-05,
      "loss": 0.7437,
      "step": 296400
    },
    {
      "epoch": 2.7054894517848016,
      "grad_norm": 3.155094623565674,
      "learning_rate": 4.7745425456846e-05,
      "loss": 0.7517,
      "step": 296500
    },
    {
      "epoch": 2.706401927147967,
      "grad_norm": 3.442234516143799,
      "learning_rate": 4.774466506071003e-05,
      "loss": 0.7528,
      "step": 296600
    },
    {
      "epoch": 2.707314402511132,
      "grad_norm": 4.3499884605407715,
      "learning_rate": 4.7743904664574054e-05,
      "loss": 0.7327,
      "step": 296700
    },
    {
      "epoch": 2.7082268778742975,
      "grad_norm": 4.8057861328125,
      "learning_rate": 4.774314426843809e-05,
      "loss": 0.7362,
      "step": 296800
    },
    {
      "epoch": 2.709139353237463,
      "grad_norm": 4.3780622482299805,
      "learning_rate": 4.7742383872302114e-05,
      "loss": 0.7409,
      "step": 296900
    },
    {
      "epoch": 2.7100518286006277,
      "grad_norm": 4.203574180603027,
      "learning_rate": 4.7741623476166144e-05,
      "loss": 0.7918,
      "step": 297000
    },
    {
      "epoch": 2.710964303963793,
      "grad_norm": 4.080855369567871,
      "learning_rate": 4.7740863080030174e-05,
      "loss": 0.7516,
      "step": 297100
    },
    {
      "epoch": 2.7118767793269583,
      "grad_norm": 4.7551374435424805,
      "learning_rate": 4.7740102683894204e-05,
      "loss": 0.7327,
      "step": 297200
    },
    {
      "epoch": 2.712789254690123,
      "grad_norm": 4.192632675170898,
      "learning_rate": 4.7739342287758234e-05,
      "loss": 0.7445,
      "step": 297300
    },
    {
      "epoch": 2.7137017300532884,
      "grad_norm": 4.065333843231201,
      "learning_rate": 4.7738581891622265e-05,
      "loss": 0.7405,
      "step": 297400
    },
    {
      "epoch": 2.7146142054164537,
      "grad_norm": 4.040168762207031,
      "learning_rate": 4.773782149548629e-05,
      "loss": 0.7477,
      "step": 297500
    },
    {
      "epoch": 2.715526680779619,
      "grad_norm": 4.771246910095215,
      "learning_rate": 4.7737061099350325e-05,
      "loss": 0.738,
      "step": 297600
    },
    {
      "epoch": 2.7164391561427843,
      "grad_norm": 4.120790958404541,
      "learning_rate": 4.773630070321435e-05,
      "loss": 0.7323,
      "step": 297700
    },
    {
      "epoch": 2.7173516315059496,
      "grad_norm": 4.214909553527832,
      "learning_rate": 4.773554030707838e-05,
      "loss": 0.7829,
      "step": 297800
    },
    {
      "epoch": 2.7182641068691145,
      "grad_norm": 4.135039806365967,
      "learning_rate": 4.773477991094241e-05,
      "loss": 0.723,
      "step": 297900
    },
    {
      "epoch": 2.7191765822322798,
      "grad_norm": 3.754822254180908,
      "learning_rate": 4.773401951480644e-05,
      "loss": 0.7145,
      "step": 298000
    },
    {
      "epoch": 2.720089057595445,
      "grad_norm": 4.081396579742432,
      "learning_rate": 4.773325911867046e-05,
      "loss": 0.7305,
      "step": 298100
    },
    {
      "epoch": 2.72100153295861,
      "grad_norm": 4.1470723152160645,
      "learning_rate": 4.77324987225345e-05,
      "loss": 0.7814,
      "step": 298200
    },
    {
      "epoch": 2.721914008321775,
      "grad_norm": 4.744162082672119,
      "learning_rate": 4.773173832639852e-05,
      "loss": 0.7254,
      "step": 298300
    },
    {
      "epoch": 2.7228264836849405,
      "grad_norm": 5.058165550231934,
      "learning_rate": 4.773097793026255e-05,
      "loss": 0.7925,
      "step": 298400
    },
    {
      "epoch": 2.723738959048106,
      "grad_norm": 3.3713631629943848,
      "learning_rate": 4.773021753412658e-05,
      "loss": 0.7501,
      "step": 298500
    },
    {
      "epoch": 2.724651434411271,
      "grad_norm": 4.837693691253662,
      "learning_rate": 4.7729457137990605e-05,
      "loss": 0.7445,
      "step": 298600
    },
    {
      "epoch": 2.725563909774436,
      "grad_norm": 4.082958698272705,
      "learning_rate": 4.772869674185464e-05,
      "loss": 0.7125,
      "step": 298700
    },
    {
      "epoch": 2.7264763851376013,
      "grad_norm": 4.245000839233398,
      "learning_rate": 4.7727936345718665e-05,
      "loss": 0.7331,
      "step": 298800
    },
    {
      "epoch": 2.7273888605007666,
      "grad_norm": 4.512516021728516,
      "learning_rate": 4.7727175949582695e-05,
      "loss": 0.7647,
      "step": 298900
    },
    {
      "epoch": 2.7283013358639314,
      "grad_norm": 3.8417470455169678,
      "learning_rate": 4.7726415553446725e-05,
      "loss": 0.7007,
      "step": 299000
    },
    {
      "epoch": 2.7292138112270967,
      "grad_norm": 3.5430853366851807,
      "learning_rate": 4.7725655157310755e-05,
      "loss": 0.7511,
      "step": 299100
    },
    {
      "epoch": 2.730126286590262,
      "grad_norm": 3.839170455932617,
      "learning_rate": 4.7724894761174785e-05,
      "loss": 0.7483,
      "step": 299200
    },
    {
      "epoch": 2.7310387619534273,
      "grad_norm": 4.527505874633789,
      "learning_rate": 4.7724134365038816e-05,
      "loss": 0.7769,
      "step": 299300
    },
    {
      "epoch": 2.7319512373165926,
      "grad_norm": 3.1092453002929688,
      "learning_rate": 4.772337396890284e-05,
      "loss": 0.7558,
      "step": 299400
    },
    {
      "epoch": 2.732863712679758,
      "grad_norm": 4.157186985015869,
      "learning_rate": 4.772261357276687e-05,
      "loss": 0.7446,
      "step": 299500
    },
    {
      "epoch": 2.7337761880429228,
      "grad_norm": 4.773948669433594,
      "learning_rate": 4.77218531766309e-05,
      "loss": 0.7376,
      "step": 299600
    },
    {
      "epoch": 2.734688663406088,
      "grad_norm": 4.624686241149902,
      "learning_rate": 4.772109278049493e-05,
      "loss": 0.7667,
      "step": 299700
    },
    {
      "epoch": 2.7356011387692534,
      "grad_norm": 4.332746982574463,
      "learning_rate": 4.772033238435896e-05,
      "loss": 0.7831,
      "step": 299800
    },
    {
      "epoch": 2.7365136141324182,
      "grad_norm": 3.992828607559204,
      "learning_rate": 4.771957198822299e-05,
      "loss": 0.7563,
      "step": 299900
    },
    {
      "epoch": 2.7374260894955835,
      "grad_norm": 4.663727283477783,
      "learning_rate": 4.771881159208701e-05,
      "loss": 0.7307,
      "step": 300000
    },
    {
      "epoch": 2.738338564858749,
      "grad_norm": 3.3838515281677246,
      "learning_rate": 4.771805119595105e-05,
      "loss": 0.7315,
      "step": 300100
    },
    {
      "epoch": 2.739251040221914,
      "grad_norm": 4.311977386474609,
      "learning_rate": 4.771729079981507e-05,
      "loss": 0.7292,
      "step": 300200
    },
    {
      "epoch": 2.7401635155850794,
      "grad_norm": 3.0256595611572266,
      "learning_rate": 4.77165304036791e-05,
      "loss": 0.7267,
      "step": 300300
    },
    {
      "epoch": 2.7410759909482443,
      "grad_norm": 4.352297782897949,
      "learning_rate": 4.771577000754313e-05,
      "loss": 0.7597,
      "step": 300400
    },
    {
      "epoch": 2.7419884663114096,
      "grad_norm": 3.434213161468506,
      "learning_rate": 4.771500961140716e-05,
      "loss": 0.7385,
      "step": 300500
    },
    {
      "epoch": 2.742900941674575,
      "grad_norm": 4.1085076332092285,
      "learning_rate": 4.771424921527119e-05,
      "loss": 0.7555,
      "step": 300600
    },
    {
      "epoch": 2.7438134170377397,
      "grad_norm": 2.708292245864868,
      "learning_rate": 4.771348881913522e-05,
      "loss": 0.7298,
      "step": 300700
    },
    {
      "epoch": 2.744725892400905,
      "grad_norm": 3.820577383041382,
      "learning_rate": 4.7712728422999246e-05,
      "loss": 0.7927,
      "step": 300800
    },
    {
      "epoch": 2.7456383677640703,
      "grad_norm": 3.9249258041381836,
      "learning_rate": 4.7711968026863276e-05,
      "loss": 0.7352,
      "step": 300900
    },
    {
      "epoch": 2.7465508431272356,
      "grad_norm": 4.850990295410156,
      "learning_rate": 4.7711207630727306e-05,
      "loss": 0.7091,
      "step": 301000
    },
    {
      "epoch": 2.747463318490401,
      "grad_norm": 4.5546417236328125,
      "learning_rate": 4.771044723459133e-05,
      "loss": 0.7972,
      "step": 301100
    },
    {
      "epoch": 2.7483757938535662,
      "grad_norm": 3.913883686065674,
      "learning_rate": 4.7709686838455366e-05,
      "loss": 0.7531,
      "step": 301200
    },
    {
      "epoch": 2.749288269216731,
      "grad_norm": 4.271673679351807,
      "learning_rate": 4.770892644231939e-05,
      "loss": 0.7609,
      "step": 301300
    },
    {
      "epoch": 2.7502007445798964,
      "grad_norm": 3.7953639030456543,
      "learning_rate": 4.770816604618342e-05,
      "loss": 0.7888,
      "step": 301400
    },
    {
      "epoch": 2.7511132199430617,
      "grad_norm": 4.163698673248291,
      "learning_rate": 4.770740565004745e-05,
      "loss": 0.757,
      "step": 301500
    },
    {
      "epoch": 2.7520256953062265,
      "grad_norm": 4.0812764167785645,
      "learning_rate": 4.770664525391148e-05,
      "loss": 0.7206,
      "step": 301600
    },
    {
      "epoch": 2.752938170669392,
      "grad_norm": 3.636871576309204,
      "learning_rate": 4.770588485777551e-05,
      "loss": 0.7594,
      "step": 301700
    },
    {
      "epoch": 2.753850646032557,
      "grad_norm": 4.823109149932861,
      "learning_rate": 4.770512446163954e-05,
      "loss": 0.7646,
      "step": 301800
    },
    {
      "epoch": 2.7547631213957224,
      "grad_norm": 4.639152526855469,
      "learning_rate": 4.770436406550356e-05,
      "loss": 0.7395,
      "step": 301900
    },
    {
      "epoch": 2.7556755967588877,
      "grad_norm": 3.814700126647949,
      "learning_rate": 4.77036036693676e-05,
      "loss": 0.7234,
      "step": 302000
    },
    {
      "epoch": 2.7565880721220526,
      "grad_norm": 3.5947577953338623,
      "learning_rate": 4.7702843273231624e-05,
      "loss": 0.6842,
      "step": 302100
    },
    {
      "epoch": 2.757500547485218,
      "grad_norm": 4.534963130950928,
      "learning_rate": 4.7702082877095654e-05,
      "loss": 0.7037,
      "step": 302200
    },
    {
      "epoch": 2.758413022848383,
      "grad_norm": 3.6289844512939453,
      "learning_rate": 4.7701322480959684e-05,
      "loss": 0.7569,
      "step": 302300
    },
    {
      "epoch": 2.759325498211548,
      "grad_norm": 4.37713623046875,
      "learning_rate": 4.7700562084823714e-05,
      "loss": 0.7684,
      "step": 302400
    },
    {
      "epoch": 2.7602379735747133,
      "grad_norm": 4.551497936248779,
      "learning_rate": 4.769980168868774e-05,
      "loss": 0.7712,
      "step": 302500
    },
    {
      "epoch": 2.7611504489378786,
      "grad_norm": 4.726320266723633,
      "learning_rate": 4.7699041292551774e-05,
      "loss": 0.7641,
      "step": 302600
    },
    {
      "epoch": 2.762062924301044,
      "grad_norm": 3.983307361602783,
      "learning_rate": 4.76982808964158e-05,
      "loss": 0.7599,
      "step": 302700
    },
    {
      "epoch": 2.7629753996642092,
      "grad_norm": 3.782399892807007,
      "learning_rate": 4.769752050027983e-05,
      "loss": 0.7788,
      "step": 302800
    },
    {
      "epoch": 2.763887875027374,
      "grad_norm": 3.4031500816345215,
      "learning_rate": 4.769676010414386e-05,
      "loss": 0.7283,
      "step": 302900
    },
    {
      "epoch": 2.7648003503905394,
      "grad_norm": 3.8105757236480713,
      "learning_rate": 4.769599970800789e-05,
      "loss": 0.7565,
      "step": 303000
    },
    {
      "epoch": 2.7657128257537047,
      "grad_norm": 4.2991485595703125,
      "learning_rate": 4.769523931187192e-05,
      "loss": 0.7595,
      "step": 303100
    },
    {
      "epoch": 2.76662530111687,
      "grad_norm": 4.524331092834473,
      "learning_rate": 4.769447891573595e-05,
      "loss": 0.7304,
      "step": 303200
    },
    {
      "epoch": 2.767537776480035,
      "grad_norm": 4.4676032066345215,
      "learning_rate": 4.769371851959997e-05,
      "loss": 0.7617,
      "step": 303300
    },
    {
      "epoch": 2.7684502518432,
      "grad_norm": 4.0756144523620605,
      "learning_rate": 4.769295812346401e-05,
      "loss": 0.7387,
      "step": 303400
    },
    {
      "epoch": 2.7693627272063654,
      "grad_norm": 4.427232265472412,
      "learning_rate": 4.769219772732803e-05,
      "loss": 0.7673,
      "step": 303500
    },
    {
      "epoch": 2.7702752025695307,
      "grad_norm": 4.25983190536499,
      "learning_rate": 4.769143733119206e-05,
      "loss": 0.7553,
      "step": 303600
    },
    {
      "epoch": 2.771187677932696,
      "grad_norm": 4.265072345733643,
      "learning_rate": 4.769067693505609e-05,
      "loss": 0.7308,
      "step": 303700
    },
    {
      "epoch": 2.772100153295861,
      "grad_norm": 4.966302871704102,
      "learning_rate": 4.768991653892012e-05,
      "loss": 0.7755,
      "step": 303800
    },
    {
      "epoch": 2.773012628659026,
      "grad_norm": 4.804720878601074,
      "learning_rate": 4.7689156142784144e-05,
      "loss": 0.7537,
      "step": 303900
    },
    {
      "epoch": 2.7739251040221915,
      "grad_norm": 3.3262321949005127,
      "learning_rate": 4.7688395746648174e-05,
      "loss": 0.731,
      "step": 304000
    },
    {
      "epoch": 2.7748375793853564,
      "grad_norm": 4.49279260635376,
      "learning_rate": 4.7687635350512205e-05,
      "loss": 0.7525,
      "step": 304100
    },
    {
      "epoch": 2.7757500547485217,
      "grad_norm": 4.357911109924316,
      "learning_rate": 4.7686874954376235e-05,
      "loss": 0.695,
      "step": 304200
    },
    {
      "epoch": 2.776662530111687,
      "grad_norm": 4.6520209312438965,
      "learning_rate": 4.7686114558240265e-05,
      "loss": 0.7545,
      "step": 304300
    },
    {
      "epoch": 2.7775750054748523,
      "grad_norm": 3.1896824836730957,
      "learning_rate": 4.768535416210429e-05,
      "loss": 0.7556,
      "step": 304400
    },
    {
      "epoch": 2.7784874808380176,
      "grad_norm": 3.9051835536956787,
      "learning_rate": 4.7684593765968325e-05,
      "loss": 0.7055,
      "step": 304500
    },
    {
      "epoch": 2.7793999562011824,
      "grad_norm": 4.137210369110107,
      "learning_rate": 4.768383336983235e-05,
      "loss": 0.7137,
      "step": 304600
    },
    {
      "epoch": 2.7803124315643477,
      "grad_norm": 3.4745113849639893,
      "learning_rate": 4.768307297369638e-05,
      "loss": 0.76,
      "step": 304700
    },
    {
      "epoch": 2.781224906927513,
      "grad_norm": 3.3531410694122314,
      "learning_rate": 4.768231257756041e-05,
      "loss": 0.7232,
      "step": 304800
    },
    {
      "epoch": 2.7821373822906783,
      "grad_norm": 6.097695827484131,
      "learning_rate": 4.768155218142444e-05,
      "loss": 0.7624,
      "step": 304900
    },
    {
      "epoch": 2.783049857653843,
      "grad_norm": 4.4937424659729,
      "learning_rate": 4.768079178528846e-05,
      "loss": 0.7369,
      "step": 305000
    },
    {
      "epoch": 2.7839623330170085,
      "grad_norm": 4.188962936401367,
      "learning_rate": 4.76800313891525e-05,
      "loss": 0.7303,
      "step": 305100
    },
    {
      "epoch": 2.7848748083801738,
      "grad_norm": 4.431487560272217,
      "learning_rate": 4.767927099301652e-05,
      "loss": 0.7104,
      "step": 305200
    },
    {
      "epoch": 2.785787283743339,
      "grad_norm": 3.905019521713257,
      "learning_rate": 4.767851059688055e-05,
      "loss": 0.7268,
      "step": 305300
    },
    {
      "epoch": 2.7866997591065044,
      "grad_norm": 3.2044003009796143,
      "learning_rate": 4.767775020074458e-05,
      "loss": 0.7107,
      "step": 305400
    },
    {
      "epoch": 2.787612234469669,
      "grad_norm": 4.229112148284912,
      "learning_rate": 4.767698980460861e-05,
      "loss": 0.7424,
      "step": 305500
    },
    {
      "epoch": 2.7885247098328345,
      "grad_norm": 4.2432146072387695,
      "learning_rate": 4.767622940847264e-05,
      "loss": 0.7212,
      "step": 305600
    },
    {
      "epoch": 2.789437185196,
      "grad_norm": 4.02727746963501,
      "learning_rate": 4.767546901233667e-05,
      "loss": 0.7603,
      "step": 305700
    },
    {
      "epoch": 2.7903496605591647,
      "grad_norm": 4.666802406311035,
      "learning_rate": 4.7674708616200695e-05,
      "loss": 0.7289,
      "step": 305800
    },
    {
      "epoch": 2.79126213592233,
      "grad_norm": 4.754619598388672,
      "learning_rate": 4.767394822006473e-05,
      "loss": 0.7391,
      "step": 305900
    },
    {
      "epoch": 2.7921746112854953,
      "grad_norm": 5.3465142250061035,
      "learning_rate": 4.7673187823928755e-05,
      "loss": 0.7723,
      "step": 306000
    },
    {
      "epoch": 2.7930870866486606,
      "grad_norm": 3.3580658435821533,
      "learning_rate": 4.7672427427792786e-05,
      "loss": 0.7609,
      "step": 306100
    },
    {
      "epoch": 2.793999562011826,
      "grad_norm": 4.395479202270508,
      "learning_rate": 4.7671667031656816e-05,
      "loss": 0.6949,
      "step": 306200
    },
    {
      "epoch": 2.7949120373749907,
      "grad_norm": 4.083804130554199,
      "learning_rate": 4.7670906635520846e-05,
      "loss": 0.7402,
      "step": 306300
    },
    {
      "epoch": 2.795824512738156,
      "grad_norm": 4.0578436851501465,
      "learning_rate": 4.767014623938487e-05,
      "loss": 0.7756,
      "step": 306400
    },
    {
      "epoch": 2.7967369881013213,
      "grad_norm": 4.447919845581055,
      "learning_rate": 4.7669385843248906e-05,
      "loss": 0.7747,
      "step": 306500
    },
    {
      "epoch": 2.7976494634644866,
      "grad_norm": 4.390684604644775,
      "learning_rate": 4.766862544711293e-05,
      "loss": 0.7514,
      "step": 306600
    },
    {
      "epoch": 2.7985619388276515,
      "grad_norm": 3.7358899116516113,
      "learning_rate": 4.766786505097696e-05,
      "loss": 0.7556,
      "step": 306700
    },
    {
      "epoch": 2.7994744141908168,
      "grad_norm": 4.162145137786865,
      "learning_rate": 4.766710465484099e-05,
      "loss": 0.7509,
      "step": 306800
    },
    {
      "epoch": 2.800386889553982,
      "grad_norm": 4.642561912536621,
      "learning_rate": 4.766634425870501e-05,
      "loss": 0.7538,
      "step": 306900
    },
    {
      "epoch": 2.8012993649171474,
      "grad_norm": 4.341142654418945,
      "learning_rate": 4.766558386256905e-05,
      "loss": 0.7491,
      "step": 307000
    },
    {
      "epoch": 2.8022118402803127,
      "grad_norm": 4.424136161804199,
      "learning_rate": 4.766482346643307e-05,
      "loss": 0.7825,
      "step": 307100
    },
    {
      "epoch": 2.8031243156434775,
      "grad_norm": 3.891338586807251,
      "learning_rate": 4.76640630702971e-05,
      "loss": 0.7831,
      "step": 307200
    },
    {
      "epoch": 2.804036791006643,
      "grad_norm": 3.9496474266052246,
      "learning_rate": 4.766330267416113e-05,
      "loss": 0.776,
      "step": 307300
    },
    {
      "epoch": 2.804949266369808,
      "grad_norm": 3.6732547283172607,
      "learning_rate": 4.766254227802516e-05,
      "loss": 0.7514,
      "step": 307400
    },
    {
      "epoch": 2.805861741732973,
      "grad_norm": 3.9800522327423096,
      "learning_rate": 4.7661781881889186e-05,
      "loss": 0.7024,
      "step": 307500
    },
    {
      "epoch": 2.8067742170961383,
      "grad_norm": 3.6837780475616455,
      "learning_rate": 4.766102148575322e-05,
      "loss": 0.7315,
      "step": 307600
    },
    {
      "epoch": 2.8076866924593036,
      "grad_norm": 3.7594003677368164,
      "learning_rate": 4.7660261089617246e-05,
      "loss": 0.7442,
      "step": 307700
    },
    {
      "epoch": 2.808599167822469,
      "grad_norm": 3.6754660606384277,
      "learning_rate": 4.7659500693481276e-05,
      "loss": 0.8179,
      "step": 307800
    },
    {
      "epoch": 2.809511643185634,
      "grad_norm": 4.539149761199951,
      "learning_rate": 4.7658740297345306e-05,
      "loss": 0.7336,
      "step": 307900
    },
    {
      "epoch": 2.810424118548799,
      "grad_norm": 4.668184757232666,
      "learning_rate": 4.7657979901209336e-05,
      "loss": 0.7423,
      "step": 308000
    },
    {
      "epoch": 2.8113365939119643,
      "grad_norm": 4.299416542053223,
      "learning_rate": 4.7657219505073367e-05,
      "loss": 0.7526,
      "step": 308100
    },
    {
      "epoch": 2.8122490692751296,
      "grad_norm": 4.242038249969482,
      "learning_rate": 4.76564591089374e-05,
      "loss": 0.7824,
      "step": 308200
    },
    {
      "epoch": 2.813161544638295,
      "grad_norm": 4.412393569946289,
      "learning_rate": 4.765569871280142e-05,
      "loss": 0.7586,
      "step": 308300
    },
    {
      "epoch": 2.81407402000146,
      "grad_norm": 4.247702598571777,
      "learning_rate": 4.765493831666546e-05,
      "loss": 0.745,
      "step": 308400
    },
    {
      "epoch": 2.814986495364625,
      "grad_norm": 3.8871281147003174,
      "learning_rate": 4.765417792052948e-05,
      "loss": 0.7634,
      "step": 308500
    },
    {
      "epoch": 2.8158989707277904,
      "grad_norm": 4.727154731750488,
      "learning_rate": 4.765341752439351e-05,
      "loss": 0.7676,
      "step": 308600
    },
    {
      "epoch": 2.8168114460909557,
      "grad_norm": 3.687052011489868,
      "learning_rate": 4.765265712825754e-05,
      "loss": 0.7517,
      "step": 308700
    },
    {
      "epoch": 2.817723921454121,
      "grad_norm": 4.312893390655518,
      "learning_rate": 4.765189673212157e-05,
      "loss": 0.7331,
      "step": 308800
    },
    {
      "epoch": 2.818636396817286,
      "grad_norm": 4.596399307250977,
      "learning_rate": 4.7651136335985594e-05,
      "loss": 0.7926,
      "step": 308900
    },
    {
      "epoch": 2.819548872180451,
      "grad_norm": 3.924400568008423,
      "learning_rate": 4.765037593984963e-05,
      "loss": 0.741,
      "step": 309000
    },
    {
      "epoch": 2.8204613475436164,
      "grad_norm": 4.138451099395752,
      "learning_rate": 4.7649615543713654e-05,
      "loss": 0.7701,
      "step": 309100
    },
    {
      "epoch": 2.8213738229067813,
      "grad_norm": 3.604227304458618,
      "learning_rate": 4.7648855147577684e-05,
      "loss": 0.7121,
      "step": 309200
    },
    {
      "epoch": 2.8222862982699466,
      "grad_norm": 3.8958828449249268,
      "learning_rate": 4.7648094751441714e-05,
      "loss": 0.7461,
      "step": 309300
    },
    {
      "epoch": 2.823198773633112,
      "grad_norm": 3.9239659309387207,
      "learning_rate": 4.7647334355305744e-05,
      "loss": 0.7358,
      "step": 309400
    },
    {
      "epoch": 2.824111248996277,
      "grad_norm": 3.0126564502716064,
      "learning_rate": 4.7646573959169774e-05,
      "loss": 0.7391,
      "step": 309500
    },
    {
      "epoch": 2.8250237243594425,
      "grad_norm": 4.107829570770264,
      "learning_rate": 4.76458135630338e-05,
      "loss": 0.7129,
      "step": 309600
    },
    {
      "epoch": 2.8259361997226073,
      "grad_norm": 4.4312849044799805,
      "learning_rate": 4.764505316689783e-05,
      "loss": 0.7017,
      "step": 309700
    },
    {
      "epoch": 2.8268486750857726,
      "grad_norm": 3.6389880180358887,
      "learning_rate": 4.764429277076186e-05,
      "loss": 0.7318,
      "step": 309800
    },
    {
      "epoch": 2.827761150448938,
      "grad_norm": 4.193134307861328,
      "learning_rate": 4.764353237462589e-05,
      "loss": 0.7296,
      "step": 309900
    },
    {
      "epoch": 2.828673625812103,
      "grad_norm": 4.685765743255615,
      "learning_rate": 4.764277197848991e-05,
      "loss": 0.7951,
      "step": 310000
    },
    {
      "epoch": 2.829586101175268,
      "grad_norm": 3.785226345062256,
      "learning_rate": 4.764201158235395e-05,
      "loss": 0.7209,
      "step": 310100
    },
    {
      "epoch": 2.8304985765384334,
      "grad_norm": 4.530745983123779,
      "learning_rate": 4.764125118621797e-05,
      "loss": 0.7597,
      "step": 310200
    },
    {
      "epoch": 2.8314110519015987,
      "grad_norm": 4.315216064453125,
      "learning_rate": 4.7640490790082e-05,
      "loss": 0.7499,
      "step": 310300
    },
    {
      "epoch": 2.832323527264764,
      "grad_norm": 4.535353660583496,
      "learning_rate": 4.763973039394603e-05,
      "loss": 0.7204,
      "step": 310400
    },
    {
      "epoch": 2.8332360026279293,
      "grad_norm": 4.242471218109131,
      "learning_rate": 4.763896999781006e-05,
      "loss": 0.7311,
      "step": 310500
    },
    {
      "epoch": 2.834148477991094,
      "grad_norm": 3.805107593536377,
      "learning_rate": 4.763820960167409e-05,
      "loss": 0.7521,
      "step": 310600
    },
    {
      "epoch": 2.8350609533542594,
      "grad_norm": 3.944469690322876,
      "learning_rate": 4.763744920553812e-05,
      "loss": 0.7372,
      "step": 310700
    },
    {
      "epoch": 2.8359734287174247,
      "grad_norm": 4.451483249664307,
      "learning_rate": 4.7636688809402144e-05,
      "loss": 0.7144,
      "step": 310800
    },
    {
      "epoch": 2.8368859040805896,
      "grad_norm": 3.1178500652313232,
      "learning_rate": 4.763592841326618e-05,
      "loss": 0.7451,
      "step": 310900
    },
    {
      "epoch": 2.837798379443755,
      "grad_norm": 4.126510143280029,
      "learning_rate": 4.7635168017130205e-05,
      "loss": 0.7051,
      "step": 311000
    },
    {
      "epoch": 2.83871085480692,
      "grad_norm": 4.668708324432373,
      "learning_rate": 4.7634407620994235e-05,
      "loss": 0.7059,
      "step": 311100
    },
    {
      "epoch": 2.8396233301700855,
      "grad_norm": 4.5374755859375,
      "learning_rate": 4.7633647224858265e-05,
      "loss": 0.7542,
      "step": 311200
    },
    {
      "epoch": 2.840535805533251,
      "grad_norm": 4.449990749359131,
      "learning_rate": 4.7632886828722295e-05,
      "loss": 0.7098,
      "step": 311300
    },
    {
      "epoch": 2.8414482808964157,
      "grad_norm": 3.5085465908050537,
      "learning_rate": 4.763212643258632e-05,
      "loss": 0.7303,
      "step": 311400
    },
    {
      "epoch": 2.842360756259581,
      "grad_norm": 4.058475971221924,
      "learning_rate": 4.7631366036450355e-05,
      "loss": 0.764,
      "step": 311500
    },
    {
      "epoch": 2.8432732316227463,
      "grad_norm": 4.709648132324219,
      "learning_rate": 4.763060564031438e-05,
      "loss": 0.7498,
      "step": 311600
    },
    {
      "epoch": 2.844185706985911,
      "grad_norm": 3.3618416786193848,
      "learning_rate": 4.762984524417841e-05,
      "loss": 0.7212,
      "step": 311700
    },
    {
      "epoch": 2.8450981823490764,
      "grad_norm": 5.279921531677246,
      "learning_rate": 4.762908484804244e-05,
      "loss": 0.7212,
      "step": 311800
    },
    {
      "epoch": 2.8460106577122417,
      "grad_norm": 3.651196002960205,
      "learning_rate": 4.762832445190647e-05,
      "loss": 0.7216,
      "step": 311900
    },
    {
      "epoch": 2.846923133075407,
      "grad_norm": 3.921597719192505,
      "learning_rate": 4.76275640557705e-05,
      "loss": 0.7243,
      "step": 312000
    },
    {
      "epoch": 2.8478356084385723,
      "grad_norm": 4.163989067077637,
      "learning_rate": 4.762680365963453e-05,
      "loss": 0.7448,
      "step": 312100
    },
    {
      "epoch": 2.8487480838017376,
      "grad_norm": 4.773681163787842,
      "learning_rate": 4.762604326349855e-05,
      "loss": 0.7544,
      "step": 312200
    },
    {
      "epoch": 2.8496605591649025,
      "grad_norm": 4.163592338562012,
      "learning_rate": 4.762528286736259e-05,
      "loss": 0.7051,
      "step": 312300
    },
    {
      "epoch": 2.8505730345280678,
      "grad_norm": 4.97133207321167,
      "learning_rate": 4.762452247122661e-05,
      "loss": 0.7926,
      "step": 312400
    },
    {
      "epoch": 2.851485509891233,
      "grad_norm": 3.914813995361328,
      "learning_rate": 4.762376207509064e-05,
      "loss": 0.7697,
      "step": 312500
    },
    {
      "epoch": 2.852397985254398,
      "grad_norm": 4.12174129486084,
      "learning_rate": 4.762300167895467e-05,
      "loss": 0.7592,
      "step": 312600
    },
    {
      "epoch": 2.853310460617563,
      "grad_norm": 4.237317085266113,
      "learning_rate": 4.7622241282818695e-05,
      "loss": 0.755,
      "step": 312700
    },
    {
      "epoch": 2.8542229359807285,
      "grad_norm": 4.400280475616455,
      "learning_rate": 4.7621480886682726e-05,
      "loss": 0.7244,
      "step": 312800
    },
    {
      "epoch": 2.855135411343894,
      "grad_norm": 3.4712705612182617,
      "learning_rate": 4.7620720490546756e-05,
      "loss": 0.7441,
      "step": 312900
    },
    {
      "epoch": 2.856047886707059,
      "grad_norm": 5.468940258026123,
      "learning_rate": 4.7619960094410786e-05,
      "loss": 0.766,
      "step": 313000
    },
    {
      "epoch": 2.856960362070224,
      "grad_norm": 3.696772575378418,
      "learning_rate": 4.7619199698274816e-05,
      "loss": 0.7663,
      "step": 313100
    },
    {
      "epoch": 2.8578728374333893,
      "grad_norm": 3.846517324447632,
      "learning_rate": 4.7618439302138846e-05,
      "loss": 0.7787,
      "step": 313200
    },
    {
      "epoch": 2.8587853127965546,
      "grad_norm": 4.185659885406494,
      "learning_rate": 4.761767890600287e-05,
      "loss": 0.7744,
      "step": 313300
    },
    {
      "epoch": 2.8596977881597194,
      "grad_norm": 3.2796335220336914,
      "learning_rate": 4.7616918509866906e-05,
      "loss": 0.7314,
      "step": 313400
    },
    {
      "epoch": 2.8606102635228847,
      "grad_norm": 4.524516582489014,
      "learning_rate": 4.761615811373093e-05,
      "loss": 0.7208,
      "step": 313500
    },
    {
      "epoch": 2.86152273888605,
      "grad_norm": 4.219945430755615,
      "learning_rate": 4.761539771759496e-05,
      "loss": 0.7348,
      "step": 313600
    },
    {
      "epoch": 2.8624352142492153,
      "grad_norm": 4.273396015167236,
      "learning_rate": 4.761463732145899e-05,
      "loss": 0.7453,
      "step": 313700
    },
    {
      "epoch": 2.8633476896123806,
      "grad_norm": 4.342989921569824,
      "learning_rate": 4.761387692532302e-05,
      "loss": 0.7398,
      "step": 313800
    },
    {
      "epoch": 2.864260164975546,
      "grad_norm": 3.562007188796997,
      "learning_rate": 4.761311652918705e-05,
      "loss": 0.7523,
      "step": 313900
    },
    {
      "epoch": 2.8651726403387108,
      "grad_norm": 4.302432537078857,
      "learning_rate": 4.761235613305108e-05,
      "loss": 0.7428,
      "step": 314000
    },
    {
      "epoch": 2.866085115701876,
      "grad_norm": 4.193124771118164,
      "learning_rate": 4.76115957369151e-05,
      "loss": 0.7335,
      "step": 314100
    },
    {
      "epoch": 2.8669975910650414,
      "grad_norm": 3.913848638534546,
      "learning_rate": 4.761083534077914e-05,
      "loss": 0.7564,
      "step": 314200
    },
    {
      "epoch": 2.8679100664282062,
      "grad_norm": 2.792307138442993,
      "learning_rate": 4.761007494464316e-05,
      "loss": 0.7392,
      "step": 314300
    },
    {
      "epoch": 2.8688225417913715,
      "grad_norm": 3.6929562091827393,
      "learning_rate": 4.760931454850719e-05,
      "loss": 0.7429,
      "step": 314400
    },
    {
      "epoch": 2.869735017154537,
      "grad_norm": 4.311702728271484,
      "learning_rate": 4.760855415237122e-05,
      "loss": 0.7545,
      "step": 314500
    },
    {
      "epoch": 2.870647492517702,
      "grad_norm": 4.504526138305664,
      "learning_rate": 4.760779375623525e-05,
      "loss": 0.7076,
      "step": 314600
    },
    {
      "epoch": 2.8715599678808674,
      "grad_norm": 4.0448126792907715,
      "learning_rate": 4.7607033360099276e-05,
      "loss": 0.6736,
      "step": 314700
    },
    {
      "epoch": 2.8724724432440323,
      "grad_norm": 4.211890697479248,
      "learning_rate": 4.760627296396331e-05,
      "loss": 0.7843,
      "step": 314800
    },
    {
      "epoch": 2.8733849186071976,
      "grad_norm": 4.933600425720215,
      "learning_rate": 4.7605512567827337e-05,
      "loss": 0.7254,
      "step": 314900
    },
    {
      "epoch": 2.874297393970363,
      "grad_norm": 3.9192326068878174,
      "learning_rate": 4.760475217169137e-05,
      "loss": 0.7487,
      "step": 315000
    },
    {
      "epoch": 2.8752098693335277,
      "grad_norm": 4.313394069671631,
      "learning_rate": 4.76039917755554e-05,
      "loss": 0.74,
      "step": 315100
    },
    {
      "epoch": 2.876122344696693,
      "grad_norm": 3.983884334564209,
      "learning_rate": 4.760323137941943e-05,
      "loss": 0.7879,
      "step": 315200
    },
    {
      "epoch": 2.8770348200598583,
      "grad_norm": 3.0730905532836914,
      "learning_rate": 4.760247098328346e-05,
      "loss": 0.7228,
      "step": 315300
    },
    {
      "epoch": 2.8779472954230236,
      "grad_norm": 3.676227331161499,
      "learning_rate": 4.760171058714748e-05,
      "loss": 0.7175,
      "step": 315400
    },
    {
      "epoch": 2.878859770786189,
      "grad_norm": 3.0993006229400635,
      "learning_rate": 4.760095019101151e-05,
      "loss": 0.7612,
      "step": 315500
    },
    {
      "epoch": 2.8797722461493542,
      "grad_norm": 3.577782392501831,
      "learning_rate": 4.760018979487554e-05,
      "loss": 0.7299,
      "step": 315600
    },
    {
      "epoch": 2.880684721512519,
      "grad_norm": 5.16627311706543,
      "learning_rate": 4.759942939873957e-05,
      "loss": 0.7757,
      "step": 315700
    },
    {
      "epoch": 2.8815971968756844,
      "grad_norm": 3.991960048675537,
      "learning_rate": 4.7598669002603594e-05,
      "loss": 0.7681,
      "step": 315800
    },
    {
      "epoch": 2.8825096722388497,
      "grad_norm": 4.3906941413879395,
      "learning_rate": 4.759790860646763e-05,
      "loss": 0.7202,
      "step": 315900
    },
    {
      "epoch": 2.8834221476020145,
      "grad_norm": 3.994208812713623,
      "learning_rate": 4.7597148210331654e-05,
      "loss": 0.7533,
      "step": 316000
    },
    {
      "epoch": 2.88433462296518,
      "grad_norm": 4.276213645935059,
      "learning_rate": 4.7596387814195684e-05,
      "loss": 0.7705,
      "step": 316100
    },
    {
      "epoch": 2.885247098328345,
      "grad_norm": 3.014376163482666,
      "learning_rate": 4.7595627418059714e-05,
      "loss": 0.7375,
      "step": 316200
    },
    {
      "epoch": 2.8861595736915104,
      "grad_norm": 4.027278423309326,
      "learning_rate": 4.7594867021923744e-05,
      "loss": 0.7572,
      "step": 316300
    },
    {
      "epoch": 2.8870720490546757,
      "grad_norm": 3.7967681884765625,
      "learning_rate": 4.7594106625787774e-05,
      "loss": 0.7222,
      "step": 316400
    },
    {
      "epoch": 2.8879845244178406,
      "grad_norm": 4.743557453155518,
      "learning_rate": 4.7593346229651804e-05,
      "loss": 0.7713,
      "step": 316500
    },
    {
      "epoch": 2.888896999781006,
      "grad_norm": 4.53524112701416,
      "learning_rate": 4.759258583351583e-05,
      "loss": 0.7611,
      "step": 316600
    },
    {
      "epoch": 2.889809475144171,
      "grad_norm": 5.257235527038574,
      "learning_rate": 4.7591825437379864e-05,
      "loss": 0.7599,
      "step": 316700
    },
    {
      "epoch": 2.890721950507336,
      "grad_norm": 3.7690694332122803,
      "learning_rate": 4.759106504124389e-05,
      "loss": 0.7617,
      "step": 316800
    },
    {
      "epoch": 2.8916344258705013,
      "grad_norm": 4.375894069671631,
      "learning_rate": 4.759030464510792e-05,
      "loss": 0.7086,
      "step": 316900
    },
    {
      "epoch": 2.8925469012336666,
      "grad_norm": 5.446958065032959,
      "learning_rate": 4.758954424897195e-05,
      "loss": 0.7474,
      "step": 317000
    },
    {
      "epoch": 2.893459376596832,
      "grad_norm": 4.321681499481201,
      "learning_rate": 4.758878385283598e-05,
      "loss": 0.7459,
      "step": 317100
    },
    {
      "epoch": 2.8943718519599972,
      "grad_norm": 4.436869144439697,
      "learning_rate": 4.75880234567e-05,
      "loss": 0.7388,
      "step": 317200
    },
    {
      "epoch": 2.8952843273231625,
      "grad_norm": 4.406415939331055,
      "learning_rate": 4.758726306056404e-05,
      "loss": 0.7881,
      "step": 317300
    },
    {
      "epoch": 2.8961968026863274,
      "grad_norm": 3.936671733856201,
      "learning_rate": 4.758650266442806e-05,
      "loss": 0.7551,
      "step": 317400
    },
    {
      "epoch": 2.8971092780494927,
      "grad_norm": 5.107023239135742,
      "learning_rate": 4.758574226829209e-05,
      "loss": 0.7313,
      "step": 317500
    },
    {
      "epoch": 2.898021753412658,
      "grad_norm": 4.760341167449951,
      "learning_rate": 4.758498187215612e-05,
      "loss": 0.7367,
      "step": 317600
    },
    {
      "epoch": 2.898934228775823,
      "grad_norm": 3.697040319442749,
      "learning_rate": 4.758422147602015e-05,
      "loss": 0.7288,
      "step": 317700
    },
    {
      "epoch": 2.899846704138988,
      "grad_norm": 3.5482962131500244,
      "learning_rate": 4.758346107988418e-05,
      "loss": 0.7494,
      "step": 317800
    },
    {
      "epoch": 2.9007591795021535,
      "grad_norm": 3.724179983139038,
      "learning_rate": 4.758270068374821e-05,
      "loss": 0.7545,
      "step": 317900
    },
    {
      "epoch": 2.9016716548653188,
      "grad_norm": 3.9862570762634277,
      "learning_rate": 4.7581940287612235e-05,
      "loss": 0.7461,
      "step": 318000
    },
    {
      "epoch": 2.902584130228484,
      "grad_norm": 3.674309730529785,
      "learning_rate": 4.7581179891476265e-05,
      "loss": 0.7564,
      "step": 318100
    },
    {
      "epoch": 2.903496605591649,
      "grad_norm": 3.416672706604004,
      "learning_rate": 4.7580419495340295e-05,
      "loss": 0.7597,
      "step": 318200
    },
    {
      "epoch": 2.904409080954814,
      "grad_norm": 4.3306989669799805,
      "learning_rate": 4.757965909920432e-05,
      "loss": 0.7354,
      "step": 318300
    },
    {
      "epoch": 2.9053215563179795,
      "grad_norm": 4.635838985443115,
      "learning_rate": 4.7578898703068355e-05,
      "loss": 0.6979,
      "step": 318400
    },
    {
      "epoch": 2.9062340316811444,
      "grad_norm": 4.624248027801514,
      "learning_rate": 4.757813830693238e-05,
      "loss": 0.7475,
      "step": 318500
    },
    {
      "epoch": 2.9071465070443097,
      "grad_norm": 3.8563408851623535,
      "learning_rate": 4.757737791079641e-05,
      "loss": 0.7361,
      "step": 318600
    },
    {
      "epoch": 2.908058982407475,
      "grad_norm": 4.223636627197266,
      "learning_rate": 4.757661751466044e-05,
      "loss": 0.7411,
      "step": 318700
    },
    {
      "epoch": 2.9089714577706403,
      "grad_norm": 3.3714568614959717,
      "learning_rate": 4.757585711852447e-05,
      "loss": 0.7104,
      "step": 318800
    },
    {
      "epoch": 2.9098839331338056,
      "grad_norm": 3.9874398708343506,
      "learning_rate": 4.75750967223885e-05,
      "loss": 0.7177,
      "step": 318900
    },
    {
      "epoch": 2.910796408496971,
      "grad_norm": 4.549086093902588,
      "learning_rate": 4.757433632625253e-05,
      "loss": 0.7627,
      "step": 319000
    },
    {
      "epoch": 2.9117088838601357,
      "grad_norm": 4.21706485748291,
      "learning_rate": 4.757357593011655e-05,
      "loss": 0.7289,
      "step": 319100
    },
    {
      "epoch": 2.912621359223301,
      "grad_norm": 4.1700663566589355,
      "learning_rate": 4.757281553398059e-05,
      "loss": 0.761,
      "step": 319200
    },
    {
      "epoch": 2.9135338345864663,
      "grad_norm": 3.9366748332977295,
      "learning_rate": 4.757205513784461e-05,
      "loss": 0.6952,
      "step": 319300
    },
    {
      "epoch": 2.914446309949631,
      "grad_norm": 3.265507698059082,
      "learning_rate": 4.757129474170864e-05,
      "loss": 0.7518,
      "step": 319400
    },
    {
      "epoch": 2.9153587853127965,
      "grad_norm": 2.274047613143921,
      "learning_rate": 4.757053434557267e-05,
      "loss": 0.753,
      "step": 319500
    },
    {
      "epoch": 2.9162712606759618,
      "grad_norm": 4.065006256103516,
      "learning_rate": 4.75697739494367e-05,
      "loss": 0.7314,
      "step": 319600
    },
    {
      "epoch": 2.917183736039127,
      "grad_norm": 4.065006732940674,
      "learning_rate": 4.7569013553300726e-05,
      "loss": 0.7379,
      "step": 319700
    },
    {
      "epoch": 2.9180962114022924,
      "grad_norm": 3.732011079788208,
      "learning_rate": 4.756825315716476e-05,
      "loss": 0.7471,
      "step": 319800
    },
    {
      "epoch": 2.919008686765457,
      "grad_norm": 4.283858776092529,
      "learning_rate": 4.7567492761028786e-05,
      "loss": 0.717,
      "step": 319900
    },
    {
      "epoch": 2.9199211621286225,
      "grad_norm": 3.9304730892181396,
      "learning_rate": 4.7566732364892816e-05,
      "loss": 0.7283,
      "step": 320000
    },
    {
      "epoch": 2.920833637491788,
      "grad_norm": 4.579748153686523,
      "learning_rate": 4.7565971968756846e-05,
      "loss": 0.7716,
      "step": 320100
    },
    {
      "epoch": 2.9217461128549527,
      "grad_norm": 3.9924890995025635,
      "learning_rate": 4.7565211572620876e-05,
      "loss": 0.7454,
      "step": 320200
    },
    {
      "epoch": 2.922658588218118,
      "grad_norm": 4.309266090393066,
      "learning_rate": 4.7564451176484906e-05,
      "loss": 0.7311,
      "step": 320300
    },
    {
      "epoch": 2.9235710635812833,
      "grad_norm": 3.811267852783203,
      "learning_rate": 4.7563690780348936e-05,
      "loss": 0.7614,
      "step": 320400
    },
    {
      "epoch": 2.9244835389444486,
      "grad_norm": 4.486725807189941,
      "learning_rate": 4.756293038421296e-05,
      "loss": 0.7383,
      "step": 320500
    },
    {
      "epoch": 2.925396014307614,
      "grad_norm": 3.970198631286621,
      "learning_rate": 4.7562169988076996e-05,
      "loss": 0.738,
      "step": 320600
    },
    {
      "epoch": 2.9263084896707787,
      "grad_norm": 4.293976783752441,
      "learning_rate": 4.756140959194102e-05,
      "loss": 0.7554,
      "step": 320700
    },
    {
      "epoch": 2.927220965033944,
      "grad_norm": 4.325433731079102,
      "learning_rate": 4.756064919580505e-05,
      "loss": 0.7313,
      "step": 320800
    },
    {
      "epoch": 2.9281334403971093,
      "grad_norm": 4.169088840484619,
      "learning_rate": 4.755988879966908e-05,
      "loss": 0.7297,
      "step": 320900
    },
    {
      "epoch": 2.9290459157602746,
      "grad_norm": 4.421458721160889,
      "learning_rate": 4.75591284035331e-05,
      "loss": 0.7211,
      "step": 321000
    },
    {
      "epoch": 2.9299583911234395,
      "grad_norm": 4.049879550933838,
      "learning_rate": 4.755836800739713e-05,
      "loss": 0.7025,
      "step": 321100
    },
    {
      "epoch": 2.9308708664866048,
      "grad_norm": 3.5857057571411133,
      "learning_rate": 4.755760761126116e-05,
      "loss": 0.7312,
      "step": 321200
    },
    {
      "epoch": 2.93178334184977,
      "grad_norm": 3.7289280891418457,
      "learning_rate": 4.755684721512519e-05,
      "loss": 0.7351,
      "step": 321300
    },
    {
      "epoch": 2.9326958172129354,
      "grad_norm": 3.7730650901794434,
      "learning_rate": 4.755608681898922e-05,
      "loss": 0.7539,
      "step": 321400
    },
    {
      "epoch": 2.9336082925761007,
      "grad_norm": 5.170186996459961,
      "learning_rate": 4.755532642285325e-05,
      "loss": 0.7703,
      "step": 321500
    },
    {
      "epoch": 2.9345207679392655,
      "grad_norm": 4.827108860015869,
      "learning_rate": 4.7554566026717277e-05,
      "loss": 0.7602,
      "step": 321600
    },
    {
      "epoch": 2.935433243302431,
      "grad_norm": 4.560858249664307,
      "learning_rate": 4.7553805630581313e-05,
      "loss": 0.7638,
      "step": 321700
    },
    {
      "epoch": 2.936345718665596,
      "grad_norm": 3.689417600631714,
      "learning_rate": 4.755304523444534e-05,
      "loss": 0.7483,
      "step": 321800
    },
    {
      "epoch": 2.937258194028761,
      "grad_norm": 3.8252336978912354,
      "learning_rate": 4.755228483830937e-05,
      "loss": 0.735,
      "step": 321900
    },
    {
      "epoch": 2.9381706693919263,
      "grad_norm": 4.172705173492432,
      "learning_rate": 4.75515244421734e-05,
      "loss": 0.7803,
      "step": 322000
    },
    {
      "epoch": 2.9390831447550916,
      "grad_norm": 4.17579984664917,
      "learning_rate": 4.755076404603743e-05,
      "loss": 0.7259,
      "step": 322100
    },
    {
      "epoch": 2.939995620118257,
      "grad_norm": 4.055509090423584,
      "learning_rate": 4.755000364990145e-05,
      "loss": 0.769,
      "step": 322200
    },
    {
      "epoch": 2.940908095481422,
      "grad_norm": 3.700662136077881,
      "learning_rate": 4.754924325376549e-05,
      "loss": 0.7573,
      "step": 322300
    },
    {
      "epoch": 2.941820570844587,
      "grad_norm": 3.9135334491729736,
      "learning_rate": 4.754848285762951e-05,
      "loss": 0.7671,
      "step": 322400
    },
    {
      "epoch": 2.9427330462077523,
      "grad_norm": 3.13132381439209,
      "learning_rate": 4.754772246149354e-05,
      "loss": 0.77,
      "step": 322500
    },
    {
      "epoch": 2.9436455215709176,
      "grad_norm": 3.6303625106811523,
      "learning_rate": 4.754696206535757e-05,
      "loss": 0.7121,
      "step": 322600
    },
    {
      "epoch": 2.944557996934083,
      "grad_norm": 4.013175010681152,
      "learning_rate": 4.75462016692216e-05,
      "loss": 0.7222,
      "step": 322700
    },
    {
      "epoch": 2.945470472297248,
      "grad_norm": 6.329004287719727,
      "learning_rate": 4.754544127308563e-05,
      "loss": 0.746,
      "step": 322800
    },
    {
      "epoch": 2.946382947660413,
      "grad_norm": 3.6365435123443604,
      "learning_rate": 4.754468087694966e-05,
      "loss": 0.7322,
      "step": 322900
    },
    {
      "epoch": 2.9472954230235784,
      "grad_norm": 4.995657444000244,
      "learning_rate": 4.7543920480813684e-05,
      "loss": 0.7314,
      "step": 323000
    },
    {
      "epoch": 2.9482078983867437,
      "grad_norm": 3.9776761531829834,
      "learning_rate": 4.754316008467772e-05,
      "loss": 0.734,
      "step": 323100
    },
    {
      "epoch": 2.949120373749909,
      "grad_norm": 4.392313480377197,
      "learning_rate": 4.7542399688541744e-05,
      "loss": 0.7292,
      "step": 323200
    },
    {
      "epoch": 2.950032849113074,
      "grad_norm": 4.115069389343262,
      "learning_rate": 4.7541639292405774e-05,
      "loss": 0.7346,
      "step": 323300
    },
    {
      "epoch": 2.950945324476239,
      "grad_norm": 3.9378020763397217,
      "learning_rate": 4.7540878896269804e-05,
      "loss": 0.7489,
      "step": 323400
    },
    {
      "epoch": 2.9518577998394044,
      "grad_norm": 3.926286458969116,
      "learning_rate": 4.7540118500133834e-05,
      "loss": 0.7576,
      "step": 323500
    },
    {
      "epoch": 2.9527702752025693,
      "grad_norm": 3.9952924251556396,
      "learning_rate": 4.753935810399786e-05,
      "loss": 0.7375,
      "step": 323600
    },
    {
      "epoch": 2.9536827505657346,
      "grad_norm": 3.950150489807129,
      "learning_rate": 4.7538597707861894e-05,
      "loss": 0.774,
      "step": 323700
    },
    {
      "epoch": 2.9545952259289,
      "grad_norm": 3.531747817993164,
      "learning_rate": 4.753783731172592e-05,
      "loss": 0.7387,
      "step": 323800
    },
    {
      "epoch": 2.955507701292065,
      "grad_norm": 4.2813873291015625,
      "learning_rate": 4.753707691558995e-05,
      "loss": 0.7625,
      "step": 323900
    },
    {
      "epoch": 2.9564201766552305,
      "grad_norm": 4.145953178405762,
      "learning_rate": 4.753631651945398e-05,
      "loss": 0.7451,
      "step": 324000
    },
    {
      "epoch": 2.9573326520183953,
      "grad_norm": 4.506068706512451,
      "learning_rate": 4.7535556123318e-05,
      "loss": 0.7548,
      "step": 324100
    },
    {
      "epoch": 2.9582451273815606,
      "grad_norm": 3.2896203994750977,
      "learning_rate": 4.753479572718204e-05,
      "loss": 0.7074,
      "step": 324200
    },
    {
      "epoch": 2.959157602744726,
      "grad_norm": 3.9543614387512207,
      "learning_rate": 4.753403533104606e-05,
      "loss": 0.744,
      "step": 324300
    },
    {
      "epoch": 2.9600700781078912,
      "grad_norm": 4.337449073791504,
      "learning_rate": 4.753327493491009e-05,
      "loss": 0.7626,
      "step": 324400
    },
    {
      "epoch": 2.960982553471056,
      "grad_norm": 4.977494716644287,
      "learning_rate": 4.753251453877412e-05,
      "loss": 0.7676,
      "step": 324500
    },
    {
      "epoch": 2.9618950288342214,
      "grad_norm": 4.678647041320801,
      "learning_rate": 4.753175414263815e-05,
      "loss": 0.7661,
      "step": 324600
    },
    {
      "epoch": 2.9628075041973867,
      "grad_norm": 4.463167190551758,
      "learning_rate": 4.753099374650218e-05,
      "loss": 0.7683,
      "step": 324700
    },
    {
      "epoch": 2.963719979560552,
      "grad_norm": 3.62129282951355,
      "learning_rate": 4.753023335036621e-05,
      "loss": 0.774,
      "step": 324800
    },
    {
      "epoch": 2.9646324549237173,
      "grad_norm": 3.793511390686035,
      "learning_rate": 4.7529472954230235e-05,
      "loss": 0.7528,
      "step": 324900
    },
    {
      "epoch": 2.965544930286882,
      "grad_norm": 3.5900979042053223,
      "learning_rate": 4.7528712558094265e-05,
      "loss": 0.7741,
      "step": 325000
    },
    {
      "epoch": 2.9664574056500475,
      "grad_norm": 4.3617634773254395,
      "learning_rate": 4.7527952161958295e-05,
      "loss": 0.7194,
      "step": 325100
    },
    {
      "epoch": 2.9673698810132128,
      "grad_norm": 4.09129524230957,
      "learning_rate": 4.7527191765822325e-05,
      "loss": 0.7499,
      "step": 325200
    },
    {
      "epoch": 2.9682823563763776,
      "grad_norm": 4.355296611785889,
      "learning_rate": 4.7526431369686355e-05,
      "loss": 0.7658,
      "step": 325300
    },
    {
      "epoch": 2.969194831739543,
      "grad_norm": 3.789764165878296,
      "learning_rate": 4.7525670973550385e-05,
      "loss": 0.7498,
      "step": 325400
    },
    {
      "epoch": 2.970107307102708,
      "grad_norm": 4.496196269989014,
      "learning_rate": 4.752491057741441e-05,
      "loss": 0.7163,
      "step": 325500
    },
    {
      "epoch": 2.9710197824658735,
      "grad_norm": 4.194231986999512,
      "learning_rate": 4.7524150181278445e-05,
      "loss": 0.7395,
      "step": 325600
    },
    {
      "epoch": 2.971932257829039,
      "grad_norm": 4.580487251281738,
      "learning_rate": 4.752338978514247e-05,
      "loss": 0.7141,
      "step": 325700
    },
    {
      "epoch": 2.9728447331922037,
      "grad_norm": 3.857272148132324,
      "learning_rate": 4.75226293890065e-05,
      "loss": 0.7426,
      "step": 325800
    },
    {
      "epoch": 2.973757208555369,
      "grad_norm": 4.749979019165039,
      "learning_rate": 4.752186899287053e-05,
      "loss": 0.7587,
      "step": 325900
    },
    {
      "epoch": 2.9746696839185343,
      "grad_norm": 3.9459214210510254,
      "learning_rate": 4.752110859673456e-05,
      "loss": 0.7291,
      "step": 326000
    },
    {
      "epoch": 2.9755821592816996,
      "grad_norm": 4.276742935180664,
      "learning_rate": 4.752034820059859e-05,
      "loss": 0.7717,
      "step": 326100
    },
    {
      "epoch": 2.9764946346448644,
      "grad_norm": 3.839669942855835,
      "learning_rate": 4.751958780446262e-05,
      "loss": 0.7573,
      "step": 326200
    },
    {
      "epoch": 2.9774071100080297,
      "grad_norm": 4.272278785705566,
      "learning_rate": 4.751882740832664e-05,
      "loss": 0.7536,
      "step": 326300
    },
    {
      "epoch": 2.978319585371195,
      "grad_norm": 4.196207523345947,
      "learning_rate": 4.751806701219067e-05,
      "loss": 0.7458,
      "step": 326400
    },
    {
      "epoch": 2.9792320607343603,
      "grad_norm": 3.8383145332336426,
      "learning_rate": 4.75173066160547e-05,
      "loss": 0.7706,
      "step": 326500
    },
    {
      "epoch": 2.9801445360975256,
      "grad_norm": 3.7852187156677246,
      "learning_rate": 4.751654621991873e-05,
      "loss": 0.7425,
      "step": 326600
    },
    {
      "epoch": 2.9810570114606905,
      "grad_norm": 4.456854820251465,
      "learning_rate": 4.751578582378276e-05,
      "loss": 0.8007,
      "step": 326700
    },
    {
      "epoch": 2.9819694868238558,
      "grad_norm": 3.371269941329956,
      "learning_rate": 4.7515025427646786e-05,
      "loss": 0.7385,
      "step": 326800
    },
    {
      "epoch": 2.982881962187021,
      "grad_norm": 3.9002525806427,
      "learning_rate": 4.7514265031510816e-05,
      "loss": 0.7101,
      "step": 326900
    },
    {
      "epoch": 2.983794437550186,
      "grad_norm": 4.739429950714111,
      "learning_rate": 4.7513504635374846e-05,
      "loss": 0.7401,
      "step": 327000
    },
    {
      "epoch": 2.984706912913351,
      "grad_norm": 4.2965898513793945,
      "learning_rate": 4.7512744239238876e-05,
      "loss": 0.7428,
      "step": 327100
    },
    {
      "epoch": 2.9856193882765165,
      "grad_norm": 3.856362819671631,
      "learning_rate": 4.7511983843102906e-05,
      "loss": 0.7722,
      "step": 327200
    },
    {
      "epoch": 2.986531863639682,
      "grad_norm": 4.158272743225098,
      "learning_rate": 4.7511223446966936e-05,
      "loss": 0.7255,
      "step": 327300
    },
    {
      "epoch": 2.987444339002847,
      "grad_norm": 2.7100298404693604,
      "learning_rate": 4.751046305083096e-05,
      "loss": 0.756,
      "step": 327400
    },
    {
      "epoch": 2.988356814366012,
      "grad_norm": 4.124751567840576,
      "learning_rate": 4.7509702654694996e-05,
      "loss": 0.7458,
      "step": 327500
    },
    {
      "epoch": 2.9892692897291773,
      "grad_norm": 4.900174140930176,
      "learning_rate": 4.750894225855902e-05,
      "loss": 0.743,
      "step": 327600
    },
    {
      "epoch": 2.9901817650923426,
      "grad_norm": 3.344113826751709,
      "learning_rate": 4.750818186242305e-05,
      "loss": 0.7908,
      "step": 327700
    },
    {
      "epoch": 2.9910942404555074,
      "grad_norm": 4.022205352783203,
      "learning_rate": 4.750742146628708e-05,
      "loss": 0.7338,
      "step": 327800
    },
    {
      "epoch": 2.9920067158186727,
      "grad_norm": 4.02410888671875,
      "learning_rate": 4.750666107015111e-05,
      "loss": 0.7191,
      "step": 327900
    },
    {
      "epoch": 2.992919191181838,
      "grad_norm": 2.881471633911133,
      "learning_rate": 4.750590067401513e-05,
      "loss": 0.7318,
      "step": 328000
    },
    {
      "epoch": 2.9938316665450033,
      "grad_norm": 3.9767708778381348,
      "learning_rate": 4.750514027787917e-05,
      "loss": 0.7349,
      "step": 328100
    },
    {
      "epoch": 2.9947441419081686,
      "grad_norm": 4.218809127807617,
      "learning_rate": 4.750437988174319e-05,
      "loss": 0.7577,
      "step": 328200
    },
    {
      "epoch": 2.995656617271334,
      "grad_norm": 4.393899917602539,
      "learning_rate": 4.750361948560722e-05,
      "loss": 0.7855,
      "step": 328300
    },
    {
      "epoch": 2.9965690926344988,
      "grad_norm": 4.693893909454346,
      "learning_rate": 4.750285908947125e-05,
      "loss": 0.6918,
      "step": 328400
    },
    {
      "epoch": 2.997481567997664,
      "grad_norm": 4.408359527587891,
      "learning_rate": 4.7502098693335283e-05,
      "loss": 0.7617,
      "step": 328500
    },
    {
      "epoch": 2.9983940433608294,
      "grad_norm": 2.8081071376800537,
      "learning_rate": 4.7501338297199313e-05,
      "loss": 0.7406,
      "step": 328600
    },
    {
      "epoch": 2.9993065187239942,
      "grad_norm": 4.7778706550598145,
      "learning_rate": 4.7500577901063344e-05,
      "loss": 0.79,
      "step": 328700
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.6007510423660278,
      "eval_runtime": 26.021,
      "eval_samples_per_second": 221.706,
      "eval_steps_per_second": 221.706,
      "step": 328776
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.5864646434783936,
      "eval_runtime": 496.7346,
      "eval_samples_per_second": 220.625,
      "eval_steps_per_second": 220.625,
      "step": 328776
    },
    {
      "epoch": 3.0002189940871595,
      "grad_norm": 4.829647064208984,
      "learning_rate": 4.749981750492737e-05,
      "loss": 0.736,
      "step": 328800
    },
    {
      "epoch": 3.001131469450325,
      "grad_norm": 3.4223029613494873,
      "learning_rate": 4.7499057108791404e-05,
      "loss": 0.7419,
      "step": 328900
    },
    {
      "epoch": 3.00204394481349,
      "grad_norm": 4.190162181854248,
      "learning_rate": 4.749829671265543e-05,
      "loss": 0.7353,
      "step": 329000
    },
    {
      "epoch": 3.0029564201766554,
      "grad_norm": 5.000449180603027,
      "learning_rate": 4.749753631651946e-05,
      "loss": 0.6873,
      "step": 329100
    },
    {
      "epoch": 3.0038688955398203,
      "grad_norm": 3.7541754245758057,
      "learning_rate": 4.749677592038349e-05,
      "loss": 0.7567,
      "step": 329200
    },
    {
      "epoch": 3.0047813709029856,
      "grad_norm": 4.628296852111816,
      "learning_rate": 4.749601552424752e-05,
      "loss": 0.7292,
      "step": 329300
    },
    {
      "epoch": 3.005693846266151,
      "grad_norm": 3.9057390689849854,
      "learning_rate": 4.749525512811154e-05,
      "loss": 0.7348,
      "step": 329400
    },
    {
      "epoch": 3.006606321629316,
      "grad_norm": 4.473076820373535,
      "learning_rate": 4.749449473197557e-05,
      "loss": 0.754,
      "step": 329500
    },
    {
      "epoch": 3.007518796992481,
      "grad_norm": 3.5486204624176025,
      "learning_rate": 4.74937343358396e-05,
      "loss": 0.7333,
      "step": 329600
    },
    {
      "epoch": 3.0084312723556463,
      "grad_norm": 4.089474201202393,
      "learning_rate": 4.749297393970363e-05,
      "loss": 0.7773,
      "step": 329700
    },
    {
      "epoch": 3.0093437477188116,
      "grad_norm": 3.8121771812438965,
      "learning_rate": 4.749221354356766e-05,
      "loss": 0.6949,
      "step": 329800
    },
    {
      "epoch": 3.010256223081977,
      "grad_norm": 3.615596294403076,
      "learning_rate": 4.7491453147431684e-05,
      "loss": 0.7125,
      "step": 329900
    },
    {
      "epoch": 3.011168698445142,
      "grad_norm": 3.781252861022949,
      "learning_rate": 4.749069275129572e-05,
      "loss": 0.7283,
      "step": 330000
    },
    {
      "epoch": 3.012081173808307,
      "grad_norm": 3.0981597900390625,
      "learning_rate": 4.7489932355159744e-05,
      "loss": 0.7353,
      "step": 330100
    },
    {
      "epoch": 3.0129936491714724,
      "grad_norm": 4.0826334953308105,
      "learning_rate": 4.7489171959023774e-05,
      "loss": 0.7165,
      "step": 330200
    },
    {
      "epoch": 3.0139061245346377,
      "grad_norm": 4.321416854858398,
      "learning_rate": 4.7488411562887804e-05,
      "loss": 0.7569,
      "step": 330300
    },
    {
      "epoch": 3.0148185998978025,
      "grad_norm": 3.682162046432495,
      "learning_rate": 4.7487651166751834e-05,
      "loss": 0.7428,
      "step": 330400
    },
    {
      "epoch": 3.015731075260968,
      "grad_norm": 4.926322937011719,
      "learning_rate": 4.748689077061586e-05,
      "loss": 0.7671,
      "step": 330500
    },
    {
      "epoch": 3.016643550624133,
      "grad_norm": 4.249837875366211,
      "learning_rate": 4.7486130374479895e-05,
      "loss": 0.7175,
      "step": 330600
    },
    {
      "epoch": 3.0175560259872984,
      "grad_norm": 3.8618640899658203,
      "learning_rate": 4.748536997834392e-05,
      "loss": 0.7046,
      "step": 330700
    },
    {
      "epoch": 3.0184685013504637,
      "grad_norm": 4.241770267486572,
      "learning_rate": 4.748460958220795e-05,
      "loss": 0.6791,
      "step": 330800
    },
    {
      "epoch": 3.0193809767136286,
      "grad_norm": 4.8196516036987305,
      "learning_rate": 4.748384918607198e-05,
      "loss": 0.7323,
      "step": 330900
    },
    {
      "epoch": 3.020293452076794,
      "grad_norm": 3.7052578926086426,
      "learning_rate": 4.748308878993601e-05,
      "loss": 0.7245,
      "step": 331000
    },
    {
      "epoch": 3.021205927439959,
      "grad_norm": 4.021703720092773,
      "learning_rate": 4.748232839380004e-05,
      "loss": 0.7451,
      "step": 331100
    },
    {
      "epoch": 3.0221184028031245,
      "grad_norm": 4.194336891174316,
      "learning_rate": 4.748156799766407e-05,
      "loss": 0.722,
      "step": 331200
    },
    {
      "epoch": 3.0230308781662893,
      "grad_norm": 4.145510673522949,
      "learning_rate": 4.748080760152809e-05,
      "loss": 0.7335,
      "step": 331300
    },
    {
      "epoch": 3.0239433535294546,
      "grad_norm": 3.699402093887329,
      "learning_rate": 4.748004720539213e-05,
      "loss": 0.7468,
      "step": 331400
    },
    {
      "epoch": 3.02485582889262,
      "grad_norm": 4.416788101196289,
      "learning_rate": 4.747928680925615e-05,
      "loss": 0.7722,
      "step": 331500
    },
    {
      "epoch": 3.0257683042557852,
      "grad_norm": 2.986567974090576,
      "learning_rate": 4.747852641312018e-05,
      "loss": 0.693,
      "step": 331600
    },
    {
      "epoch": 3.02668077961895,
      "grad_norm": 4.40974235534668,
      "learning_rate": 4.747776601698421e-05,
      "loss": 0.7363,
      "step": 331700
    },
    {
      "epoch": 3.0275932549821154,
      "grad_norm": 4.045352935791016,
      "learning_rate": 4.747700562084824e-05,
      "loss": 0.7445,
      "step": 331800
    },
    {
      "epoch": 3.0285057303452807,
      "grad_norm": 4.193861961364746,
      "learning_rate": 4.7476245224712265e-05,
      "loss": 0.7362,
      "step": 331900
    },
    {
      "epoch": 3.029418205708446,
      "grad_norm": 4.083004951477051,
      "learning_rate": 4.74754848285763e-05,
      "loss": 0.6976,
      "step": 332000
    },
    {
      "epoch": 3.030330681071611,
      "grad_norm": 4.71462869644165,
      "learning_rate": 4.7474724432440325e-05,
      "loss": 0.7231,
      "step": 332100
    },
    {
      "epoch": 3.031243156434776,
      "grad_norm": 3.844437837600708,
      "learning_rate": 4.7473964036304355e-05,
      "loss": 0.737,
      "step": 332200
    },
    {
      "epoch": 3.0321556317979415,
      "grad_norm": 4.668442726135254,
      "learning_rate": 4.7473203640168385e-05,
      "loss": 0.7404,
      "step": 332300
    },
    {
      "epoch": 3.0330681071611068,
      "grad_norm": 4.819190502166748,
      "learning_rate": 4.747244324403241e-05,
      "loss": 0.7616,
      "step": 332400
    },
    {
      "epoch": 3.033980582524272,
      "grad_norm": 4.233500003814697,
      "learning_rate": 4.7471682847896445e-05,
      "loss": 0.7648,
      "step": 332500
    },
    {
      "epoch": 3.034893057887437,
      "grad_norm": 4.1571736335754395,
      "learning_rate": 4.747092245176047e-05,
      "loss": 0.7139,
      "step": 332600
    },
    {
      "epoch": 3.035805533250602,
      "grad_norm": 3.7233235836029053,
      "learning_rate": 4.74701620556245e-05,
      "loss": 0.7103,
      "step": 332700
    },
    {
      "epoch": 3.0367180086137675,
      "grad_norm": 4.349015712738037,
      "learning_rate": 4.746940165948853e-05,
      "loss": 0.7324,
      "step": 332800
    },
    {
      "epoch": 3.037630483976933,
      "grad_norm": 3.9453182220458984,
      "learning_rate": 4.746864126335256e-05,
      "loss": 0.7359,
      "step": 332900
    },
    {
      "epoch": 3.0385429593400977,
      "grad_norm": 3.6627092361450195,
      "learning_rate": 4.746788086721658e-05,
      "loss": 0.7452,
      "step": 333000
    },
    {
      "epoch": 3.039455434703263,
      "grad_norm": 3.7091097831726074,
      "learning_rate": 4.746712047108062e-05,
      "loss": 0.7458,
      "step": 333100
    },
    {
      "epoch": 3.0403679100664283,
      "grad_norm": 4.060163497924805,
      "learning_rate": 4.746636007494464e-05,
      "loss": 0.7402,
      "step": 333200
    },
    {
      "epoch": 3.0412803854295936,
      "grad_norm": 3.6212270259857178,
      "learning_rate": 4.746559967880867e-05,
      "loss": 0.6792,
      "step": 333300
    },
    {
      "epoch": 3.0421928607927584,
      "grad_norm": 3.9453227519989014,
      "learning_rate": 4.74648392826727e-05,
      "loss": 0.7197,
      "step": 333400
    },
    {
      "epoch": 3.0431053361559237,
      "grad_norm": 4.999670505523682,
      "learning_rate": 4.746407888653673e-05,
      "loss": 0.7524,
      "step": 333500
    },
    {
      "epoch": 3.044017811519089,
      "grad_norm": 3.6593873500823975,
      "learning_rate": 4.746331849040076e-05,
      "loss": 0.7437,
      "step": 333600
    },
    {
      "epoch": 3.0449302868822543,
      "grad_norm": 3.828122615814209,
      "learning_rate": 4.746255809426479e-05,
      "loss": 0.7377,
      "step": 333700
    },
    {
      "epoch": 3.045842762245419,
      "grad_norm": 4.419531345367432,
      "learning_rate": 4.7461797698128816e-05,
      "loss": 0.7592,
      "step": 333800
    },
    {
      "epoch": 3.0467552376085845,
      "grad_norm": 4.916978359222412,
      "learning_rate": 4.746103730199285e-05,
      "loss": 0.7575,
      "step": 333900
    },
    {
      "epoch": 3.0476677129717498,
      "grad_norm": 3.6958541870117188,
      "learning_rate": 4.7460276905856876e-05,
      "loss": 0.799,
      "step": 334000
    },
    {
      "epoch": 3.048580188334915,
      "grad_norm": 4.2128682136535645,
      "learning_rate": 4.7459516509720906e-05,
      "loss": 0.7012,
      "step": 334100
    },
    {
      "epoch": 3.0494926636980804,
      "grad_norm": 24.711877822875977,
      "learning_rate": 4.7458756113584936e-05,
      "loss": 0.7576,
      "step": 334200
    },
    {
      "epoch": 3.050405139061245,
      "grad_norm": 3.5112504959106445,
      "learning_rate": 4.7457995717448966e-05,
      "loss": 0.7502,
      "step": 334300
    },
    {
      "epoch": 3.0513176144244105,
      "grad_norm": 3.850534677505493,
      "learning_rate": 4.745723532131299e-05,
      "loss": 0.716,
      "step": 334400
    },
    {
      "epoch": 3.052230089787576,
      "grad_norm": 4.40253210067749,
      "learning_rate": 4.7456474925177026e-05,
      "loss": 0.7153,
      "step": 334500
    },
    {
      "epoch": 3.053142565150741,
      "grad_norm": 3.724231004714966,
      "learning_rate": 4.745571452904105e-05,
      "loss": 0.788,
      "step": 334600
    },
    {
      "epoch": 3.054055040513906,
      "grad_norm": 4.9337639808654785,
      "learning_rate": 4.745495413290508e-05,
      "loss": 0.7428,
      "step": 334700
    },
    {
      "epoch": 3.0549675158770713,
      "grad_norm": 3.745197296142578,
      "learning_rate": 4.745419373676911e-05,
      "loss": 0.7246,
      "step": 334800
    },
    {
      "epoch": 3.0558799912402366,
      "grad_norm": 3.786308526992798,
      "learning_rate": 4.745343334063314e-05,
      "loss": 0.736,
      "step": 334900
    },
    {
      "epoch": 3.056792466603402,
      "grad_norm": 4.350253105163574,
      "learning_rate": 4.745267294449717e-05,
      "loss": 0.7376,
      "step": 335000
    },
    {
      "epoch": 3.0577049419665667,
      "grad_norm": 4.308244228363037,
      "learning_rate": 4.74519125483612e-05,
      "loss": 0.7678,
      "step": 335100
    },
    {
      "epoch": 3.058617417329732,
      "grad_norm": 3.887155055999756,
      "learning_rate": 4.7451152152225223e-05,
      "loss": 0.7229,
      "step": 335200
    },
    {
      "epoch": 3.0595298926928973,
      "grad_norm": 3.29548716545105,
      "learning_rate": 4.7450391756089253e-05,
      "loss": 0.7095,
      "step": 335300
    },
    {
      "epoch": 3.0604423680560626,
      "grad_norm": 3.6912026405334473,
      "learning_rate": 4.7449631359953284e-05,
      "loss": 0.7199,
      "step": 335400
    },
    {
      "epoch": 3.0613548434192275,
      "grad_norm": 4.6172051429748535,
      "learning_rate": 4.744887096381731e-05,
      "loss": 0.7641,
      "step": 335500
    },
    {
      "epoch": 3.0622673187823928,
      "grad_norm": 4.047191143035889,
      "learning_rate": 4.7448110567681344e-05,
      "loss": 0.6996,
      "step": 335600
    },
    {
      "epoch": 3.063179794145558,
      "grad_norm": 4.789487838745117,
      "learning_rate": 4.744735017154537e-05,
      "loss": 0.7426,
      "step": 335700
    },
    {
      "epoch": 3.0640922695087234,
      "grad_norm": 4.374695777893066,
      "learning_rate": 4.74465897754094e-05,
      "loss": 0.7472,
      "step": 335800
    },
    {
      "epoch": 3.0650047448718887,
      "grad_norm": 4.820422649383545,
      "learning_rate": 4.744582937927343e-05,
      "loss": 0.7421,
      "step": 335900
    },
    {
      "epoch": 3.0659172202350535,
      "grad_norm": 4.058324813842773,
      "learning_rate": 4.744506898313746e-05,
      "loss": 0.7009,
      "step": 336000
    },
    {
      "epoch": 3.066829695598219,
      "grad_norm": 4.2351508140563965,
      "learning_rate": 4.744430858700149e-05,
      "loss": 0.7614,
      "step": 336100
    },
    {
      "epoch": 3.067742170961384,
      "grad_norm": 4.616759777069092,
      "learning_rate": 4.744354819086552e-05,
      "loss": 0.7194,
      "step": 336200
    },
    {
      "epoch": 3.0686546463245494,
      "grad_norm": 4.439854621887207,
      "learning_rate": 4.744278779472954e-05,
      "loss": 0.715,
      "step": 336300
    },
    {
      "epoch": 3.0695671216877143,
      "grad_norm": 3.7628955841064453,
      "learning_rate": 4.744202739859358e-05,
      "loss": 0.7278,
      "step": 336400
    },
    {
      "epoch": 3.0704795970508796,
      "grad_norm": 4.741792678833008,
      "learning_rate": 4.74412670024576e-05,
      "loss": 0.7077,
      "step": 336500
    },
    {
      "epoch": 3.071392072414045,
      "grad_norm": 3.590656042098999,
      "learning_rate": 4.744050660632163e-05,
      "loss": 0.7283,
      "step": 336600
    },
    {
      "epoch": 3.07230454777721,
      "grad_norm": 2.7924790382385254,
      "learning_rate": 4.743974621018566e-05,
      "loss": 0.6855,
      "step": 336700
    },
    {
      "epoch": 3.073217023140375,
      "grad_norm": 3.5856993198394775,
      "learning_rate": 4.743898581404969e-05,
      "loss": 0.6796,
      "step": 336800
    },
    {
      "epoch": 3.0741294985035403,
      "grad_norm": 3.951078414916992,
      "learning_rate": 4.7438225417913714e-05,
      "loss": 0.7326,
      "step": 336900
    },
    {
      "epoch": 3.0750419738667056,
      "grad_norm": 5.001919269561768,
      "learning_rate": 4.743746502177775e-05,
      "loss": 0.7488,
      "step": 337000
    },
    {
      "epoch": 3.075954449229871,
      "grad_norm": 4.407036781311035,
      "learning_rate": 4.7436704625641774e-05,
      "loss": 0.6925,
      "step": 337100
    },
    {
      "epoch": 3.076866924593036,
      "grad_norm": 4.416334629058838,
      "learning_rate": 4.7435944229505804e-05,
      "loss": 0.7124,
      "step": 337200
    },
    {
      "epoch": 3.077779399956201,
      "grad_norm": 3.0188796520233154,
      "learning_rate": 4.7435183833369834e-05,
      "loss": 0.7418,
      "step": 337300
    },
    {
      "epoch": 3.0786918753193664,
      "grad_norm": 4.5791730880737305,
      "learning_rate": 4.7434423437233865e-05,
      "loss": 0.7549,
      "step": 337400
    },
    {
      "epoch": 3.0796043506825317,
      "grad_norm": 3.6830577850341797,
      "learning_rate": 4.7433663041097895e-05,
      "loss": 0.6951,
      "step": 337500
    },
    {
      "epoch": 3.080516826045697,
      "grad_norm": 3.161344528198242,
      "learning_rate": 4.7432902644961925e-05,
      "loss": 0.7345,
      "step": 337600
    },
    {
      "epoch": 3.081429301408862,
      "grad_norm": 4.425090312957764,
      "learning_rate": 4.743214224882595e-05,
      "loss": 0.7381,
      "step": 337700
    },
    {
      "epoch": 3.082341776772027,
      "grad_norm": 4.372733116149902,
      "learning_rate": 4.7431381852689985e-05,
      "loss": 0.7226,
      "step": 337800
    },
    {
      "epoch": 3.0832542521351924,
      "grad_norm": 4.412691116333008,
      "learning_rate": 4.743062145655401e-05,
      "loss": 0.7142,
      "step": 337900
    },
    {
      "epoch": 3.0841667274983577,
      "grad_norm": 3.974215030670166,
      "learning_rate": 4.742986106041804e-05,
      "loss": 0.7702,
      "step": 338000
    },
    {
      "epoch": 3.0850792028615226,
      "grad_norm": 3.8057479858398438,
      "learning_rate": 4.742910066428207e-05,
      "loss": 0.7468,
      "step": 338100
    },
    {
      "epoch": 3.085991678224688,
      "grad_norm": 4.030529022216797,
      "learning_rate": 4.742834026814609e-05,
      "loss": 0.7676,
      "step": 338200
    },
    {
      "epoch": 3.086904153587853,
      "grad_norm": 4.389336109161377,
      "learning_rate": 4.742757987201013e-05,
      "loss": 0.7055,
      "step": 338300
    },
    {
      "epoch": 3.0878166289510185,
      "grad_norm": 5.56527042388916,
      "learning_rate": 4.742681947587415e-05,
      "loss": 0.7137,
      "step": 338400
    },
    {
      "epoch": 3.0887291043141833,
      "grad_norm": 3.6772172451019287,
      "learning_rate": 4.742605907973818e-05,
      "loss": 0.7421,
      "step": 338500
    },
    {
      "epoch": 3.0896415796773486,
      "grad_norm": 2.4828596115112305,
      "learning_rate": 4.742529868360221e-05,
      "loss": 0.7688,
      "step": 338600
    },
    {
      "epoch": 3.090554055040514,
      "grad_norm": 4.723855495452881,
      "learning_rate": 4.742453828746624e-05,
      "loss": 0.7595,
      "step": 338700
    },
    {
      "epoch": 3.0914665304036792,
      "grad_norm": 4.088468074798584,
      "learning_rate": 4.7423777891330265e-05,
      "loss": 0.7464,
      "step": 338800
    },
    {
      "epoch": 3.092379005766844,
      "grad_norm": 3.782299518585205,
      "learning_rate": 4.74230174951943e-05,
      "loss": 0.7221,
      "step": 338900
    },
    {
      "epoch": 3.0932914811300094,
      "grad_norm": 4.39029598236084,
      "learning_rate": 4.7422257099058325e-05,
      "loss": 0.7705,
      "step": 339000
    },
    {
      "epoch": 3.0942039564931747,
      "grad_norm": 3.5769588947296143,
      "learning_rate": 4.7421496702922355e-05,
      "loss": 0.7198,
      "step": 339100
    },
    {
      "epoch": 3.09511643185634,
      "grad_norm": 4.2687578201293945,
      "learning_rate": 4.7420736306786385e-05,
      "loss": 0.7217,
      "step": 339200
    },
    {
      "epoch": 3.0960289072195053,
      "grad_norm": 3.7150838375091553,
      "learning_rate": 4.7419975910650415e-05,
      "loss": 0.7607,
      "step": 339300
    },
    {
      "epoch": 3.09694138258267,
      "grad_norm": 4.1488213539123535,
      "learning_rate": 4.7419215514514446e-05,
      "loss": 0.7329,
      "step": 339400
    },
    {
      "epoch": 3.0978538579458355,
      "grad_norm": 3.0618960857391357,
      "learning_rate": 4.7418455118378476e-05,
      "loss": 0.7434,
      "step": 339500
    },
    {
      "epoch": 3.0987663333090008,
      "grad_norm": 3.3663277626037598,
      "learning_rate": 4.74176947222425e-05,
      "loss": 0.7453,
      "step": 339600
    },
    {
      "epoch": 3.099678808672166,
      "grad_norm": 4.163030624389648,
      "learning_rate": 4.7416934326106536e-05,
      "loss": 0.7489,
      "step": 339700
    },
    {
      "epoch": 3.100591284035331,
      "grad_norm": 5.860739707946777,
      "learning_rate": 4.741617392997056e-05,
      "loss": 0.7375,
      "step": 339800
    },
    {
      "epoch": 3.101503759398496,
      "grad_norm": 3.082676649093628,
      "learning_rate": 4.741541353383459e-05,
      "loss": 0.7004,
      "step": 339900
    },
    {
      "epoch": 3.1024162347616615,
      "grad_norm": 3.8241052627563477,
      "learning_rate": 4.741465313769862e-05,
      "loss": 0.7734,
      "step": 340000
    },
    {
      "epoch": 3.103328710124827,
      "grad_norm": 4.3483076095581055,
      "learning_rate": 4.741389274156265e-05,
      "loss": 0.7373,
      "step": 340100
    },
    {
      "epoch": 3.1042411854879917,
      "grad_norm": 2.8973822593688965,
      "learning_rate": 4.741313234542667e-05,
      "loss": 0.7365,
      "step": 340200
    },
    {
      "epoch": 3.105153660851157,
      "grad_norm": 3.806164503097534,
      "learning_rate": 4.741237194929071e-05,
      "loss": 0.7145,
      "step": 340300
    },
    {
      "epoch": 3.1060661362143223,
      "grad_norm": 4.0867719650268555,
      "learning_rate": 4.741161155315473e-05,
      "loss": 0.7375,
      "step": 340400
    },
    {
      "epoch": 3.1069786115774876,
      "grad_norm": 3.951815605163574,
      "learning_rate": 4.741085115701876e-05,
      "loss": 0.7242,
      "step": 340500
    },
    {
      "epoch": 3.1078910869406524,
      "grad_norm": 6.274041175842285,
      "learning_rate": 4.741009076088279e-05,
      "loss": 0.7224,
      "step": 340600
    },
    {
      "epoch": 3.1088035623038177,
      "grad_norm": 3.727128505706787,
      "learning_rate": 4.740933036474682e-05,
      "loss": 0.7292,
      "step": 340700
    },
    {
      "epoch": 3.109716037666983,
      "grad_norm": 4.463387489318848,
      "learning_rate": 4.740856996861085e-05,
      "loss": 0.7399,
      "step": 340800
    },
    {
      "epoch": 3.1106285130301483,
      "grad_norm": 4.86787223815918,
      "learning_rate": 4.7407809572474876e-05,
      "loss": 0.6935,
      "step": 340900
    },
    {
      "epoch": 3.111540988393313,
      "grad_norm": 3.5722405910491943,
      "learning_rate": 4.7407049176338906e-05,
      "loss": 0.7166,
      "step": 341000
    },
    {
      "epoch": 3.1124534637564785,
      "grad_norm": 4.588637351989746,
      "learning_rate": 4.7406288780202936e-05,
      "loss": 0.7556,
      "step": 341100
    },
    {
      "epoch": 3.1133659391196438,
      "grad_norm": 3.9148781299591064,
      "learning_rate": 4.7405528384066966e-05,
      "loss": 0.7655,
      "step": 341200
    },
    {
      "epoch": 3.114278414482809,
      "grad_norm": 3.8435707092285156,
      "learning_rate": 4.740476798793099e-05,
      "loss": 0.7037,
      "step": 341300
    },
    {
      "epoch": 3.1151908898459744,
      "grad_norm": 3.2705514430999756,
      "learning_rate": 4.7404007591795027e-05,
      "loss": 0.7267,
      "step": 341400
    },
    {
      "epoch": 3.116103365209139,
      "grad_norm": 4.183825969696045,
      "learning_rate": 4.740324719565905e-05,
      "loss": 0.6679,
      "step": 341500
    },
    {
      "epoch": 3.1170158405723045,
      "grad_norm": 4.703831672668457,
      "learning_rate": 4.740248679952308e-05,
      "loss": 0.7476,
      "step": 341600
    },
    {
      "epoch": 3.11792831593547,
      "grad_norm": 2.1743991374969482,
      "learning_rate": 4.740172640338711e-05,
      "loss": 0.6969,
      "step": 341700
    },
    {
      "epoch": 3.118840791298635,
      "grad_norm": 4.085544109344482,
      "learning_rate": 4.740096600725114e-05,
      "loss": 0.7088,
      "step": 341800
    },
    {
      "epoch": 3.1197532666618,
      "grad_norm": 4.687809944152832,
      "learning_rate": 4.740020561111517e-05,
      "loss": 0.795,
      "step": 341900
    },
    {
      "epoch": 3.1206657420249653,
      "grad_norm": 4.099392414093018,
      "learning_rate": 4.73994452149792e-05,
      "loss": 0.7738,
      "step": 342000
    },
    {
      "epoch": 3.1215782173881306,
      "grad_norm": 3.5084011554718018,
      "learning_rate": 4.7398684818843223e-05,
      "loss": 0.7281,
      "step": 342100
    },
    {
      "epoch": 3.122490692751296,
      "grad_norm": 3.9835972785949707,
      "learning_rate": 4.739792442270726e-05,
      "loss": 0.7071,
      "step": 342200
    },
    {
      "epoch": 3.1234031681144607,
      "grad_norm": 4.760087013244629,
      "learning_rate": 4.7397164026571284e-05,
      "loss": 0.7626,
      "step": 342300
    },
    {
      "epoch": 3.124315643477626,
      "grad_norm": 4.911052703857422,
      "learning_rate": 4.7396403630435314e-05,
      "loss": 0.7215,
      "step": 342400
    },
    {
      "epoch": 3.1252281188407913,
      "grad_norm": 4.902522563934326,
      "learning_rate": 4.7395643234299344e-05,
      "loss": 0.7348,
      "step": 342500
    },
    {
      "epoch": 3.1261405942039566,
      "grad_norm": 5.536969184875488,
      "learning_rate": 4.7394882838163374e-05,
      "loss": 0.7335,
      "step": 342600
    },
    {
      "epoch": 3.127053069567122,
      "grad_norm": 4.089111804962158,
      "learning_rate": 4.73941224420274e-05,
      "loss": 0.748,
      "step": 342700
    },
    {
      "epoch": 3.127965544930287,
      "grad_norm": 2.623507022857666,
      "learning_rate": 4.7393362045891434e-05,
      "loss": 0.7277,
      "step": 342800
    },
    {
      "epoch": 3.128878020293452,
      "grad_norm": 4.564382553100586,
      "learning_rate": 4.739260164975546e-05,
      "loss": 0.7215,
      "step": 342900
    },
    {
      "epoch": 3.1297904956566174,
      "grad_norm": 3.3666491508483887,
      "learning_rate": 4.739184125361949e-05,
      "loss": 0.7309,
      "step": 343000
    },
    {
      "epoch": 3.1307029710197822,
      "grad_norm": 4.963802337646484,
      "learning_rate": 4.739108085748352e-05,
      "loss": 0.714,
      "step": 343100
    },
    {
      "epoch": 3.1316154463829475,
      "grad_norm": 4.63443660736084,
      "learning_rate": 4.739032046134755e-05,
      "loss": 0.6999,
      "step": 343200
    },
    {
      "epoch": 3.132527921746113,
      "grad_norm": 3.9302971363067627,
      "learning_rate": 4.738956006521158e-05,
      "loss": 0.7552,
      "step": 343300
    },
    {
      "epoch": 3.133440397109278,
      "grad_norm": 4.296469688415527,
      "learning_rate": 4.738879966907561e-05,
      "loss": 0.7335,
      "step": 343400
    },
    {
      "epoch": 3.1343528724724434,
      "grad_norm": 4.106438636779785,
      "learning_rate": 4.738803927293963e-05,
      "loss": 0.7432,
      "step": 343500
    },
    {
      "epoch": 3.1352653478356083,
      "grad_norm": 3.8689424991607666,
      "learning_rate": 4.738727887680367e-05,
      "loss": 0.7337,
      "step": 343600
    },
    {
      "epoch": 3.1361778231987736,
      "grad_norm": 4.367987632751465,
      "learning_rate": 4.738651848066769e-05,
      "loss": 0.7233,
      "step": 343700
    },
    {
      "epoch": 3.137090298561939,
      "grad_norm": 4.847496032714844,
      "learning_rate": 4.7385758084531714e-05,
      "loss": 0.7544,
      "step": 343800
    },
    {
      "epoch": 3.138002773925104,
      "grad_norm": 3.715731620788574,
      "learning_rate": 4.738499768839575e-05,
      "loss": 0.7604,
      "step": 343900
    },
    {
      "epoch": 3.138915249288269,
      "grad_norm": 4.625882625579834,
      "learning_rate": 4.7384237292259774e-05,
      "loss": 0.7391,
      "step": 344000
    },
    {
      "epoch": 3.1398277246514343,
      "grad_norm": 4.376931667327881,
      "learning_rate": 4.7383476896123805e-05,
      "loss": 0.7585,
      "step": 344100
    },
    {
      "epoch": 3.1407402000145996,
      "grad_norm": 4.050844192504883,
      "learning_rate": 4.7382716499987835e-05,
      "loss": 0.741,
      "step": 344200
    },
    {
      "epoch": 3.141652675377765,
      "grad_norm": 4.693755149841309,
      "learning_rate": 4.7381956103851865e-05,
      "loss": 0.7322,
      "step": 344300
    },
    {
      "epoch": 3.14256515074093,
      "grad_norm": 3.7209432125091553,
      "learning_rate": 4.7381195707715895e-05,
      "loss": 0.7328,
      "step": 344400
    },
    {
      "epoch": 3.143477626104095,
      "grad_norm": 4.33388614654541,
      "learning_rate": 4.7380435311579925e-05,
      "loss": 0.6941,
      "step": 344500
    },
    {
      "epoch": 3.1443901014672604,
      "grad_norm": 3.425295829772949,
      "learning_rate": 4.737967491544395e-05,
      "loss": 0.6965,
      "step": 344600
    },
    {
      "epoch": 3.1453025768304257,
      "grad_norm": 4.341428279876709,
      "learning_rate": 4.7378914519307985e-05,
      "loss": 0.7417,
      "step": 344700
    },
    {
      "epoch": 3.1462150521935905,
      "grad_norm": 3.7263238430023193,
      "learning_rate": 4.737815412317201e-05,
      "loss": 0.7653,
      "step": 344800
    },
    {
      "epoch": 3.147127527556756,
      "grad_norm": 4.154028415679932,
      "learning_rate": 4.737739372703604e-05,
      "loss": 0.7114,
      "step": 344900
    },
    {
      "epoch": 3.148040002919921,
      "grad_norm": 4.406336307525635,
      "learning_rate": 4.737663333090007e-05,
      "loss": 0.7301,
      "step": 345000
    },
    {
      "epoch": 3.1489524782830864,
      "grad_norm": 4.402169704437256,
      "learning_rate": 4.73758729347641e-05,
      "loss": 0.7753,
      "step": 345100
    },
    {
      "epoch": 3.1498649536462517,
      "grad_norm": 3.6829123497009277,
      "learning_rate": 4.737511253862812e-05,
      "loss": 0.729,
      "step": 345200
    },
    {
      "epoch": 3.1507774290094166,
      "grad_norm": 3.4851746559143066,
      "learning_rate": 4.737435214249216e-05,
      "loss": 0.7429,
      "step": 345300
    },
    {
      "epoch": 3.151689904372582,
      "grad_norm": 3.940279722213745,
      "learning_rate": 4.737359174635618e-05,
      "loss": 0.7584,
      "step": 345400
    },
    {
      "epoch": 3.152602379735747,
      "grad_norm": 3.9786806106567383,
      "learning_rate": 4.737283135022021e-05,
      "loss": 0.7503,
      "step": 345500
    },
    {
      "epoch": 3.1535148550989125,
      "grad_norm": 3.271920919418335,
      "learning_rate": 4.737207095408424e-05,
      "loss": 0.741,
      "step": 345600
    },
    {
      "epoch": 3.1544273304620774,
      "grad_norm": 3.7680857181549072,
      "learning_rate": 4.737131055794827e-05,
      "loss": 0.7722,
      "step": 345700
    },
    {
      "epoch": 3.1553398058252426,
      "grad_norm": 3.7396438121795654,
      "learning_rate": 4.73705501618123e-05,
      "loss": 0.7172,
      "step": 345800
    },
    {
      "epoch": 3.156252281188408,
      "grad_norm": 4.086681365966797,
      "learning_rate": 4.736978976567633e-05,
      "loss": 0.7206,
      "step": 345900
    },
    {
      "epoch": 3.1571647565515732,
      "grad_norm": 3.8726003170013428,
      "learning_rate": 4.7369029369540355e-05,
      "loss": 0.7604,
      "step": 346000
    },
    {
      "epoch": 3.158077231914738,
      "grad_norm": 3.666447639465332,
      "learning_rate": 4.736826897340439e-05,
      "loss": 0.727,
      "step": 346100
    },
    {
      "epoch": 3.1589897072779034,
      "grad_norm": 4.260412216186523,
      "learning_rate": 4.7367508577268416e-05,
      "loss": 0.7474,
      "step": 346200
    },
    {
      "epoch": 3.1599021826410687,
      "grad_norm": 3.5994794368743896,
      "learning_rate": 4.7366748181132446e-05,
      "loss": 0.7413,
      "step": 346300
    },
    {
      "epoch": 3.160814658004234,
      "grad_norm": 3.7134933471679688,
      "learning_rate": 4.7365987784996476e-05,
      "loss": 0.7757,
      "step": 346400
    },
    {
      "epoch": 3.161727133367399,
      "grad_norm": 3.9775190353393555,
      "learning_rate": 4.73652273888605e-05,
      "loss": 0.7229,
      "step": 346500
    },
    {
      "epoch": 3.162639608730564,
      "grad_norm": 3.922656774520874,
      "learning_rate": 4.736446699272453e-05,
      "loss": 0.7621,
      "step": 346600
    },
    {
      "epoch": 3.1635520840937295,
      "grad_norm": 4.751176357269287,
      "learning_rate": 4.736370659658856e-05,
      "loss": 0.7289,
      "step": 346700
    },
    {
      "epoch": 3.1644645594568948,
      "grad_norm": 3.973710536956787,
      "learning_rate": 4.736294620045259e-05,
      "loss": 0.7221,
      "step": 346800
    },
    {
      "epoch": 3.16537703482006,
      "grad_norm": 4.632970809936523,
      "learning_rate": 4.736218580431662e-05,
      "loss": 0.7062,
      "step": 346900
    },
    {
      "epoch": 3.166289510183225,
      "grad_norm": 5.796992301940918,
      "learning_rate": 4.736142540818065e-05,
      "loss": 0.783,
      "step": 347000
    },
    {
      "epoch": 3.16720198554639,
      "grad_norm": 4.104768753051758,
      "learning_rate": 4.736066501204467e-05,
      "loss": 0.7324,
      "step": 347100
    },
    {
      "epoch": 3.1681144609095555,
      "grad_norm": 3.5024592876434326,
      "learning_rate": 4.735990461590871e-05,
      "loss": 0.722,
      "step": 347200
    },
    {
      "epoch": 3.169026936272721,
      "grad_norm": 4.26024866104126,
      "learning_rate": 4.735914421977273e-05,
      "loss": 0.7439,
      "step": 347300
    },
    {
      "epoch": 3.1699394116358857,
      "grad_norm": 3.7759640216827393,
      "learning_rate": 4.735838382363676e-05,
      "loss": 0.74,
      "step": 347400
    },
    {
      "epoch": 3.170851886999051,
      "grad_norm": 4.434126377105713,
      "learning_rate": 4.735762342750079e-05,
      "loss": 0.7533,
      "step": 347500
    },
    {
      "epoch": 3.1717643623622163,
      "grad_norm": 4.470386981964111,
      "learning_rate": 4.735686303136482e-05,
      "loss": 0.7469,
      "step": 347600
    },
    {
      "epoch": 3.1726768377253816,
      "grad_norm": 4.848766803741455,
      "learning_rate": 4.7356102635228846e-05,
      "loss": 0.7029,
      "step": 347700
    },
    {
      "epoch": 3.1735893130885464,
      "grad_norm": 4.208463191986084,
      "learning_rate": 4.735534223909288e-05,
      "loss": 0.733,
      "step": 347800
    },
    {
      "epoch": 3.1745017884517117,
      "grad_norm": 3.9383838176727295,
      "learning_rate": 4.7354581842956906e-05,
      "loss": 0.7437,
      "step": 347900
    },
    {
      "epoch": 3.175414263814877,
      "grad_norm": 4.438199043273926,
      "learning_rate": 4.7353821446820936e-05,
      "loss": 0.7243,
      "step": 348000
    },
    {
      "epoch": 3.1763267391780423,
      "grad_norm": 5.116163730621338,
      "learning_rate": 4.7353061050684967e-05,
      "loss": 0.7039,
      "step": 348100
    },
    {
      "epoch": 3.177239214541207,
      "grad_norm": 4.927858829498291,
      "learning_rate": 4.7352300654548997e-05,
      "loss": 0.698,
      "step": 348200
    },
    {
      "epoch": 3.1781516899043725,
      "grad_norm": 4.294830322265625,
      "learning_rate": 4.735154025841303e-05,
      "loss": 0.7851,
      "step": 348300
    },
    {
      "epoch": 3.1790641652675378,
      "grad_norm": 4.1561503410339355,
      "learning_rate": 4.735077986227706e-05,
      "loss": 0.764,
      "step": 348400
    },
    {
      "epoch": 3.179976640630703,
      "grad_norm": 5.016814231872559,
      "learning_rate": 4.735001946614108e-05,
      "loss": 0.7267,
      "step": 348500
    },
    {
      "epoch": 3.1808891159938684,
      "grad_norm": 4.254723072052002,
      "learning_rate": 4.734925907000512e-05,
      "loss": 0.7571,
      "step": 348600
    },
    {
      "epoch": 3.181801591357033,
      "grad_norm": 4.8029704093933105,
      "learning_rate": 4.734849867386914e-05,
      "loss": 0.6738,
      "step": 348700
    },
    {
      "epoch": 3.1827140667201985,
      "grad_norm": 3.70515775680542,
      "learning_rate": 4.734773827773317e-05,
      "loss": 0.775,
      "step": 348800
    },
    {
      "epoch": 3.183626542083364,
      "grad_norm": 4.045658588409424,
      "learning_rate": 4.73469778815972e-05,
      "loss": 0.6944,
      "step": 348900
    },
    {
      "epoch": 3.184539017446529,
      "grad_norm": 5.186099052429199,
      "learning_rate": 4.734621748546123e-05,
      "loss": 0.7413,
      "step": 349000
    },
    {
      "epoch": 3.185451492809694,
      "grad_norm": 3.837202548980713,
      "learning_rate": 4.7345457089325254e-05,
      "loss": 0.7312,
      "step": 349100
    },
    {
      "epoch": 3.1863639681728593,
      "grad_norm": 3.5097739696502686,
      "learning_rate": 4.734469669318929e-05,
      "loss": 0.7804,
      "step": 349200
    },
    {
      "epoch": 3.1872764435360246,
      "grad_norm": 3.7082037925720215,
      "learning_rate": 4.7343936297053314e-05,
      "loss": 0.7488,
      "step": 349300
    },
    {
      "epoch": 3.18818891889919,
      "grad_norm": 3.4510765075683594,
      "learning_rate": 4.7343175900917344e-05,
      "loss": 0.726,
      "step": 349400
    },
    {
      "epoch": 3.1891013942623547,
      "grad_norm": 3.768580913543701,
      "learning_rate": 4.7342415504781374e-05,
      "loss": 0.7541,
      "step": 349500
    },
    {
      "epoch": 3.19001386962552,
      "grad_norm": 3.8975155353546143,
      "learning_rate": 4.73416551086454e-05,
      "loss": 0.7207,
      "step": 349600
    },
    {
      "epoch": 3.1909263449886853,
      "grad_norm": 5.578362464904785,
      "learning_rate": 4.7340894712509434e-05,
      "loss": 0.7062,
      "step": 349700
    },
    {
      "epoch": 3.1918388203518506,
      "grad_norm": 4.617367267608643,
      "learning_rate": 4.734013431637346e-05,
      "loss": 0.7876,
      "step": 349800
    },
    {
      "epoch": 3.1927512957150155,
      "grad_norm": 4.385369300842285,
      "learning_rate": 4.733937392023749e-05,
      "loss": 0.7328,
      "step": 349900
    },
    {
      "epoch": 3.193663771078181,
      "grad_norm": 3.560621500015259,
      "learning_rate": 4.733861352410152e-05,
      "loss": 0.7527,
      "step": 350000
    },
    {
      "epoch": 3.194576246441346,
      "grad_norm": 4.343069076538086,
      "learning_rate": 4.733785312796555e-05,
      "loss": 0.7295,
      "step": 350100
    },
    {
      "epoch": 3.1954887218045114,
      "grad_norm": 4.0662336349487305,
      "learning_rate": 4.733709273182958e-05,
      "loss": 0.73,
      "step": 350200
    },
    {
      "epoch": 3.1964011971676767,
      "grad_norm": 3.9237353801727295,
      "learning_rate": 4.733633233569361e-05,
      "loss": 0.7575,
      "step": 350300
    },
    {
      "epoch": 3.1973136725308415,
      "grad_norm": 3.956166982650757,
      "learning_rate": 4.733557193955763e-05,
      "loss": 0.7425,
      "step": 350400
    },
    {
      "epoch": 3.198226147894007,
      "grad_norm": 3.800799608230591,
      "learning_rate": 4.733481154342166e-05,
      "loss": 0.7488,
      "step": 350500
    },
    {
      "epoch": 3.199138623257172,
      "grad_norm": 4.624366283416748,
      "learning_rate": 4.733405114728569e-05,
      "loss": 0.7109,
      "step": 350600
    },
    {
      "epoch": 3.2000510986203374,
      "grad_norm": 4.464006423950195,
      "learning_rate": 4.733329075114972e-05,
      "loss": 0.7524,
      "step": 350700
    },
    {
      "epoch": 3.2009635739835023,
      "grad_norm": 4.830007076263428,
      "learning_rate": 4.733253035501375e-05,
      "loss": 0.7546,
      "step": 350800
    },
    {
      "epoch": 3.2018760493466676,
      "grad_norm": 4.548877716064453,
      "learning_rate": 4.733176995887778e-05,
      "loss": 0.7404,
      "step": 350900
    },
    {
      "epoch": 3.202788524709833,
      "grad_norm": 3.6263866424560547,
      "learning_rate": 4.7331009562741805e-05,
      "loss": 0.7327,
      "step": 351000
    },
    {
      "epoch": 3.203701000072998,
      "grad_norm": 4.819343566894531,
      "learning_rate": 4.733024916660584e-05,
      "loss": 0.7346,
      "step": 351100
    },
    {
      "epoch": 3.204613475436163,
      "grad_norm": 4.820161819458008,
      "learning_rate": 4.7329488770469865e-05,
      "loss": 0.7589,
      "step": 351200
    },
    {
      "epoch": 3.2055259507993283,
      "grad_norm": 4.03933572769165,
      "learning_rate": 4.7328728374333895e-05,
      "loss": 0.7826,
      "step": 351300
    },
    {
      "epoch": 3.2064384261624936,
      "grad_norm": 4.104106903076172,
      "learning_rate": 4.7327967978197925e-05,
      "loss": 0.7277,
      "step": 351400
    },
    {
      "epoch": 3.207350901525659,
      "grad_norm": 4.2062296867370605,
      "learning_rate": 4.7327207582061955e-05,
      "loss": 0.7352,
      "step": 351500
    },
    {
      "epoch": 3.208263376888824,
      "grad_norm": 3.700218439102173,
      "learning_rate": 4.7326447185925985e-05,
      "loss": 0.7249,
      "step": 351600
    },
    {
      "epoch": 3.209175852251989,
      "grad_norm": 4.222341537475586,
      "learning_rate": 4.7325686789790015e-05,
      "loss": 0.7026,
      "step": 351700
    },
    {
      "epoch": 3.2100883276151544,
      "grad_norm": 3.104199171066284,
      "learning_rate": 4.732492639365404e-05,
      "loss": 0.7616,
      "step": 351800
    },
    {
      "epoch": 3.2110008029783197,
      "grad_norm": 4.949307918548584,
      "learning_rate": 4.732416599751807e-05,
      "loss": 0.7527,
      "step": 351900
    },
    {
      "epoch": 3.211913278341485,
      "grad_norm": 3.703176259994507,
      "learning_rate": 4.73234056013821e-05,
      "loss": 0.6739,
      "step": 352000
    },
    {
      "epoch": 3.21282575370465,
      "grad_norm": 3.853297710418701,
      "learning_rate": 4.732264520524613e-05,
      "loss": 0.7446,
      "step": 352100
    },
    {
      "epoch": 3.213738229067815,
      "grad_norm": 4.73592472076416,
      "learning_rate": 4.732188480911016e-05,
      "loss": 0.7382,
      "step": 352200
    },
    {
      "epoch": 3.2146507044309804,
      "grad_norm": 4.543875217437744,
      "learning_rate": 4.732112441297418e-05,
      "loss": 0.7255,
      "step": 352300
    },
    {
      "epoch": 3.2155631797941457,
      "grad_norm": 3.862865924835205,
      "learning_rate": 4.732036401683821e-05,
      "loss": 0.8014,
      "step": 352400
    },
    {
      "epoch": 3.2164756551573106,
      "grad_norm": 3.703909397125244,
      "learning_rate": 4.731960362070224e-05,
      "loss": 0.7297,
      "step": 352500
    },
    {
      "epoch": 3.217388130520476,
      "grad_norm": 4.4885663986206055,
      "learning_rate": 4.731884322456627e-05,
      "loss": 0.7326,
      "step": 352600
    },
    {
      "epoch": 3.218300605883641,
      "grad_norm": 3.9366137981414795,
      "learning_rate": 4.73180828284303e-05,
      "loss": 0.785,
      "step": 352700
    },
    {
      "epoch": 3.2192130812468065,
      "grad_norm": 4.312831401824951,
      "learning_rate": 4.731732243229433e-05,
      "loss": 0.7478,
      "step": 352800
    },
    {
      "epoch": 3.2201255566099714,
      "grad_norm": 4.002717971801758,
      "learning_rate": 4.7316562036158356e-05,
      "loss": 0.7677,
      "step": 352900
    },
    {
      "epoch": 3.2210380319731367,
      "grad_norm": 4.182300090789795,
      "learning_rate": 4.731580164002239e-05,
      "loss": 0.7658,
      "step": 353000
    },
    {
      "epoch": 3.221950507336302,
      "grad_norm": 4.64077091217041,
      "learning_rate": 4.7315041243886416e-05,
      "loss": 0.7435,
      "step": 353100
    },
    {
      "epoch": 3.2228629826994672,
      "grad_norm": 4.422469139099121,
      "learning_rate": 4.7314280847750446e-05,
      "loss": 0.6918,
      "step": 353200
    },
    {
      "epoch": 3.223775458062632,
      "grad_norm": 4.440311908721924,
      "learning_rate": 4.7313520451614476e-05,
      "loss": 0.7184,
      "step": 353300
    },
    {
      "epoch": 3.2246879334257974,
      "grad_norm": 4.580527305603027,
      "learning_rate": 4.7312760055478506e-05,
      "loss": 0.7559,
      "step": 353400
    },
    {
      "epoch": 3.2256004087889627,
      "grad_norm": 3.5936777591705322,
      "learning_rate": 4.731199965934253e-05,
      "loss": 0.7528,
      "step": 353500
    },
    {
      "epoch": 3.226512884152128,
      "grad_norm": 4.4605793952941895,
      "learning_rate": 4.7311239263206566e-05,
      "loss": 0.7387,
      "step": 353600
    },
    {
      "epoch": 3.2274253595152933,
      "grad_norm": 4.648581504821777,
      "learning_rate": 4.731047886707059e-05,
      "loss": 0.7195,
      "step": 353700
    },
    {
      "epoch": 3.228337834878458,
      "grad_norm": 4.20722770690918,
      "learning_rate": 4.730971847093462e-05,
      "loss": 0.7421,
      "step": 353800
    },
    {
      "epoch": 3.2292503102416235,
      "grad_norm": 3.784273147583008,
      "learning_rate": 4.730895807479865e-05,
      "loss": 0.7535,
      "step": 353900
    },
    {
      "epoch": 3.2301627856047888,
      "grad_norm": 4.477565288543701,
      "learning_rate": 4.730819767866268e-05,
      "loss": 0.7604,
      "step": 354000
    },
    {
      "epoch": 3.231075260967954,
      "grad_norm": 3.8735499382019043,
      "learning_rate": 4.730743728252671e-05,
      "loss": 0.7342,
      "step": 354100
    },
    {
      "epoch": 3.231987736331119,
      "grad_norm": 2.651068925857544,
      "learning_rate": 4.730667688639074e-05,
      "loss": 0.7081,
      "step": 354200
    },
    {
      "epoch": 3.232900211694284,
      "grad_norm": 3.773919105529785,
      "learning_rate": 4.730591649025476e-05,
      "loss": 0.7709,
      "step": 354300
    },
    {
      "epoch": 3.2338126870574495,
      "grad_norm": 3.9416627883911133,
      "learning_rate": 4.73051560941188e-05,
      "loss": 0.704,
      "step": 354400
    },
    {
      "epoch": 3.234725162420615,
      "grad_norm": 3.623955488204956,
      "learning_rate": 4.730439569798282e-05,
      "loss": 0.7288,
      "step": 354500
    },
    {
      "epoch": 3.2356376377837797,
      "grad_norm": 3.5862834453582764,
      "learning_rate": 4.730363530184685e-05,
      "loss": 0.7658,
      "step": 354600
    },
    {
      "epoch": 3.236550113146945,
      "grad_norm": 3.909818410873413,
      "learning_rate": 4.730287490571088e-05,
      "loss": 0.7781,
      "step": 354700
    },
    {
      "epoch": 3.2374625885101103,
      "grad_norm": 4.259603977203369,
      "learning_rate": 4.730211450957491e-05,
      "loss": 0.7394,
      "step": 354800
    },
    {
      "epoch": 3.2383750638732756,
      "grad_norm": 3.852971076965332,
      "learning_rate": 4.7301354113438937e-05,
      "loss": 0.7417,
      "step": 354900
    },
    {
      "epoch": 3.2392875392364404,
      "grad_norm": 3.7706990242004395,
      "learning_rate": 4.7300593717302973e-05,
      "loss": 0.7161,
      "step": 355000
    },
    {
      "epoch": 3.2402000145996057,
      "grad_norm": 2.9450387954711914,
      "learning_rate": 4.7299833321167e-05,
      "loss": 0.7327,
      "step": 355100
    },
    {
      "epoch": 3.241112489962771,
      "grad_norm": 5.373282432556152,
      "learning_rate": 4.729907292503103e-05,
      "loss": 0.8023,
      "step": 355200
    },
    {
      "epoch": 3.2420249653259363,
      "grad_norm": 3.7775378227233887,
      "learning_rate": 4.729831252889506e-05,
      "loss": 0.721,
      "step": 355300
    },
    {
      "epoch": 3.2429374406891016,
      "grad_norm": 4.076517581939697,
      "learning_rate": 4.729755213275908e-05,
      "loss": 0.7363,
      "step": 355400
    },
    {
      "epoch": 3.2438499160522665,
      "grad_norm": 4.0241570472717285,
      "learning_rate": 4.729679173662312e-05,
      "loss": 0.7325,
      "step": 355500
    },
    {
      "epoch": 3.2447623914154318,
      "grad_norm": 2.665332794189453,
      "learning_rate": 4.729603134048714e-05,
      "loss": 0.73,
      "step": 355600
    },
    {
      "epoch": 3.245674866778597,
      "grad_norm": 4.38173770904541,
      "learning_rate": 4.729527094435117e-05,
      "loss": 0.7344,
      "step": 355700
    },
    {
      "epoch": 3.2465873421417624,
      "grad_norm": 4.825966835021973,
      "learning_rate": 4.72945105482152e-05,
      "loss": 0.7188,
      "step": 355800
    },
    {
      "epoch": 3.247499817504927,
      "grad_norm": 4.2694315910339355,
      "learning_rate": 4.729375015207923e-05,
      "loss": 0.7058,
      "step": 355900
    },
    {
      "epoch": 3.2484122928680925,
      "grad_norm": 3.3707547187805176,
      "learning_rate": 4.7292989755943254e-05,
      "loss": 0.7433,
      "step": 356000
    },
    {
      "epoch": 3.249324768231258,
      "grad_norm": 4.078118801116943,
      "learning_rate": 4.729222935980729e-05,
      "loss": 0.7644,
      "step": 356100
    },
    {
      "epoch": 3.250237243594423,
      "grad_norm": 4.490724086761475,
      "learning_rate": 4.7291468963671314e-05,
      "loss": 0.7141,
      "step": 356200
    },
    {
      "epoch": 3.251149718957588,
      "grad_norm": 4.46337366104126,
      "learning_rate": 4.7290708567535344e-05,
      "loss": 0.7327,
      "step": 356300
    },
    {
      "epoch": 3.2520621943207533,
      "grad_norm": 4.3504791259765625,
      "learning_rate": 4.7289948171399374e-05,
      "loss": 0.7598,
      "step": 356400
    },
    {
      "epoch": 3.2529746696839186,
      "grad_norm": 4.286052227020264,
      "learning_rate": 4.7289187775263404e-05,
      "loss": 0.7529,
      "step": 356500
    },
    {
      "epoch": 3.253887145047084,
      "grad_norm": 3.7047247886657715,
      "learning_rate": 4.7288427379127434e-05,
      "loss": 0.7275,
      "step": 356600
    },
    {
      "epoch": 3.2547996204102487,
      "grad_norm": 4.659329414367676,
      "learning_rate": 4.7287666982991464e-05,
      "loss": 0.7545,
      "step": 356700
    },
    {
      "epoch": 3.255712095773414,
      "grad_norm": 4.449203968048096,
      "learning_rate": 4.728690658685549e-05,
      "loss": 0.767,
      "step": 356800
    },
    {
      "epoch": 3.2566245711365793,
      "grad_norm": 4.476222515106201,
      "learning_rate": 4.7286146190719524e-05,
      "loss": 0.7213,
      "step": 356900
    },
    {
      "epoch": 3.2575370464997446,
      "grad_norm": 4.917485237121582,
      "learning_rate": 4.728538579458355e-05,
      "loss": 0.7037,
      "step": 357000
    },
    {
      "epoch": 3.25844952186291,
      "grad_norm": 4.474613666534424,
      "learning_rate": 4.728462539844758e-05,
      "loss": 0.7512,
      "step": 357100
    },
    {
      "epoch": 3.259361997226075,
      "grad_norm": 3.65354061126709,
      "learning_rate": 4.728386500231161e-05,
      "loss": 0.7465,
      "step": 357200
    },
    {
      "epoch": 3.26027447258924,
      "grad_norm": 3.5816283226013184,
      "learning_rate": 4.728310460617564e-05,
      "loss": 0.7739,
      "step": 357300
    },
    {
      "epoch": 3.2611869479524054,
      "grad_norm": 4.228429794311523,
      "learning_rate": 4.728234421003966e-05,
      "loss": 0.733,
      "step": 357400
    },
    {
      "epoch": 3.2620994233155702,
      "grad_norm": 4.323573589324951,
      "learning_rate": 4.72815838139037e-05,
      "loss": 0.7376,
      "step": 357500
    },
    {
      "epoch": 3.2630118986787355,
      "grad_norm": 4.333105564117432,
      "learning_rate": 4.728082341776772e-05,
      "loss": 0.7731,
      "step": 357600
    },
    {
      "epoch": 3.263924374041901,
      "grad_norm": 4.3306121826171875,
      "learning_rate": 4.728006302163175e-05,
      "loss": 0.6986,
      "step": 357700
    },
    {
      "epoch": 3.264836849405066,
      "grad_norm": 2.815873146057129,
      "learning_rate": 4.727930262549578e-05,
      "loss": 0.7721,
      "step": 357800
    },
    {
      "epoch": 3.2657493247682314,
      "grad_norm": 4.835683822631836,
      "learning_rate": 4.7278542229359805e-05,
      "loss": 0.769,
      "step": 357900
    },
    {
      "epoch": 3.2666618001313963,
      "grad_norm": 4.903336524963379,
      "learning_rate": 4.727778183322384e-05,
      "loss": 0.7143,
      "step": 358000
    },
    {
      "epoch": 3.2675742754945616,
      "grad_norm": 3.4079337120056152,
      "learning_rate": 4.7277021437087865e-05,
      "loss": 0.7418,
      "step": 358100
    },
    {
      "epoch": 3.268486750857727,
      "grad_norm": 4.912605285644531,
      "learning_rate": 4.7276261040951895e-05,
      "loss": 0.7168,
      "step": 358200
    },
    {
      "epoch": 3.269399226220892,
      "grad_norm": 3.9866607189178467,
      "learning_rate": 4.7275500644815925e-05,
      "loss": 0.7349,
      "step": 358300
    },
    {
      "epoch": 3.270311701584057,
      "grad_norm": 3.9674057960510254,
      "learning_rate": 4.7274740248679955e-05,
      "loss": 0.715,
      "step": 358400
    },
    {
      "epoch": 3.2712241769472223,
      "grad_norm": 3.9708375930786133,
      "learning_rate": 4.727397985254398e-05,
      "loss": 0.7394,
      "step": 358500
    },
    {
      "epoch": 3.2721366523103876,
      "grad_norm": 3.9409737586975098,
      "learning_rate": 4.7273219456408015e-05,
      "loss": 0.7149,
      "step": 358600
    },
    {
      "epoch": 3.273049127673553,
      "grad_norm": 4.093558311462402,
      "learning_rate": 4.727245906027204e-05,
      "loss": 0.7265,
      "step": 358700
    },
    {
      "epoch": 3.2739616030367182,
      "grad_norm": 3.3591580390930176,
      "learning_rate": 4.727169866413607e-05,
      "loss": 0.6975,
      "step": 358800
    },
    {
      "epoch": 3.274874078399883,
      "grad_norm": 4.772916316986084,
      "learning_rate": 4.72709382680001e-05,
      "loss": 0.7205,
      "step": 358900
    },
    {
      "epoch": 3.2757865537630484,
      "grad_norm": 3.713590145111084,
      "learning_rate": 4.727017787186413e-05,
      "loss": 0.7464,
      "step": 359000
    },
    {
      "epoch": 3.2766990291262137,
      "grad_norm": 4.896561145782471,
      "learning_rate": 4.726941747572816e-05,
      "loss": 0.7405,
      "step": 359100
    },
    {
      "epoch": 3.2776115044893785,
      "grad_norm": 3.169861078262329,
      "learning_rate": 4.726865707959219e-05,
      "loss": 0.7374,
      "step": 359200
    },
    {
      "epoch": 3.278523979852544,
      "grad_norm": 4.759272575378418,
      "learning_rate": 4.726789668345621e-05,
      "loss": 0.7343,
      "step": 359300
    },
    {
      "epoch": 3.279436455215709,
      "grad_norm": 4.4713358879089355,
      "learning_rate": 4.726713628732025e-05,
      "loss": 0.7392,
      "step": 359400
    },
    {
      "epoch": 3.2803489305788744,
      "grad_norm": 5.080846786499023,
      "learning_rate": 4.726637589118427e-05,
      "loss": 0.7023,
      "step": 359500
    },
    {
      "epoch": 3.2812614059420397,
      "grad_norm": 3.4413297176361084,
      "learning_rate": 4.72656154950483e-05,
      "loss": 0.6845,
      "step": 359600
    },
    {
      "epoch": 3.2821738813052046,
      "grad_norm": 4.2492289543151855,
      "learning_rate": 4.726485509891233e-05,
      "loss": 0.7633,
      "step": 359700
    },
    {
      "epoch": 3.28308635666837,
      "grad_norm": 4.017184734344482,
      "learning_rate": 4.726409470277636e-05,
      "loss": 0.7176,
      "step": 359800
    },
    {
      "epoch": 3.283998832031535,
      "grad_norm": 4.449446201324463,
      "learning_rate": 4.7263334306640386e-05,
      "loss": 0.7101,
      "step": 359900
    },
    {
      "epoch": 3.2849113073947005,
      "grad_norm": 4.145537853240967,
      "learning_rate": 4.726257391050442e-05,
      "loss": 0.7121,
      "step": 360000
    },
    {
      "epoch": 3.2858237827578654,
      "grad_norm": 4.296199798583984,
      "learning_rate": 4.7261813514368446e-05,
      "loss": 0.7388,
      "step": 360100
    },
    {
      "epoch": 3.2867362581210307,
      "grad_norm": 4.026522159576416,
      "learning_rate": 4.7261053118232476e-05,
      "loss": 0.7322,
      "step": 360200
    },
    {
      "epoch": 3.287648733484196,
      "grad_norm": 4.837553977966309,
      "learning_rate": 4.7260292722096506e-05,
      "loss": 0.7439,
      "step": 360300
    },
    {
      "epoch": 3.2885612088473613,
      "grad_norm": 4.194300174713135,
      "learning_rate": 4.7259532325960536e-05,
      "loss": 0.7128,
      "step": 360400
    },
    {
      "epoch": 3.2894736842105265,
      "grad_norm": 4.4025678634643555,
      "learning_rate": 4.7258771929824566e-05,
      "loss": 0.7399,
      "step": 360500
    },
    {
      "epoch": 3.2903861595736914,
      "grad_norm": 3.9362597465515137,
      "learning_rate": 4.7258011533688596e-05,
      "loss": 0.7373,
      "step": 360600
    },
    {
      "epoch": 3.2912986349368567,
      "grad_norm": 3.9433135986328125,
      "learning_rate": 4.725725113755262e-05,
      "loss": 0.7522,
      "step": 360700
    },
    {
      "epoch": 3.292211110300022,
      "grad_norm": 5.2779998779296875,
      "learning_rate": 4.725649074141665e-05,
      "loss": 0.7185,
      "step": 360800
    },
    {
      "epoch": 3.293123585663187,
      "grad_norm": 3.31607985496521,
      "learning_rate": 4.725573034528068e-05,
      "loss": 0.7595,
      "step": 360900
    },
    {
      "epoch": 3.294036061026352,
      "grad_norm": 3.575894355773926,
      "learning_rate": 4.72549699491447e-05,
      "loss": 0.7431,
      "step": 361000
    },
    {
      "epoch": 3.2949485363895175,
      "grad_norm": 4.807469844818115,
      "learning_rate": 4.725420955300874e-05,
      "loss": 0.7113,
      "step": 361100
    },
    {
      "epoch": 3.2958610117526828,
      "grad_norm": 5.153182029724121,
      "learning_rate": 4.725344915687276e-05,
      "loss": 0.805,
      "step": 361200
    },
    {
      "epoch": 3.296773487115848,
      "grad_norm": 3.866417646408081,
      "learning_rate": 4.725268876073679e-05,
      "loss": 0.7284,
      "step": 361300
    },
    {
      "epoch": 3.297685962479013,
      "grad_norm": 4.052592754364014,
      "learning_rate": 4.725192836460082e-05,
      "loss": 0.7516,
      "step": 361400
    },
    {
      "epoch": 3.298598437842178,
      "grad_norm": 4.506959438323975,
      "learning_rate": 4.725116796846485e-05,
      "loss": 0.746,
      "step": 361500
    },
    {
      "epoch": 3.2995109132053435,
      "grad_norm": 4.566439151763916,
      "learning_rate": 4.725040757232888e-05,
      "loss": 0.7415,
      "step": 361600
    },
    {
      "epoch": 3.300423388568509,
      "grad_norm": 3.9889464378356934,
      "learning_rate": 4.724964717619291e-05,
      "loss": 0.7475,
      "step": 361700
    },
    {
      "epoch": 3.3013358639316737,
      "grad_norm": 7.089646816253662,
      "learning_rate": 4.724888678005694e-05,
      "loss": 0.727,
      "step": 361800
    },
    {
      "epoch": 3.302248339294839,
      "grad_norm": 4.874438285827637,
      "learning_rate": 4.7248126383920974e-05,
      "loss": 0.7498,
      "step": 361900
    },
    {
      "epoch": 3.3031608146580043,
      "grad_norm": 3.352846622467041,
      "learning_rate": 4.7247365987785e-05,
      "loss": 0.7309,
      "step": 362000
    },
    {
      "epoch": 3.3040732900211696,
      "grad_norm": 3.468832492828369,
      "learning_rate": 4.724660559164903e-05,
      "loss": 0.7362,
      "step": 362100
    },
    {
      "epoch": 3.304985765384335,
      "grad_norm": 4.182566165924072,
      "learning_rate": 4.724584519551306e-05,
      "loss": 0.6814,
      "step": 362200
    },
    {
      "epoch": 3.3058982407474997,
      "grad_norm": 4.029293060302734,
      "learning_rate": 4.724508479937709e-05,
      "loss": 0.7176,
      "step": 362300
    },
    {
      "epoch": 3.306810716110665,
      "grad_norm": 3.752619743347168,
      "learning_rate": 4.724432440324111e-05,
      "loss": 0.7273,
      "step": 362400
    },
    {
      "epoch": 3.3077231914738303,
      "grad_norm": 3.4255526065826416,
      "learning_rate": 4.724356400710515e-05,
      "loss": 0.7152,
      "step": 362500
    },
    {
      "epoch": 3.308635666836995,
      "grad_norm": 4.318052768707275,
      "learning_rate": 4.724280361096917e-05,
      "loss": 0.7312,
      "step": 362600
    },
    {
      "epoch": 3.3095481422001605,
      "grad_norm": 4.146274089813232,
      "learning_rate": 4.72420432148332e-05,
      "loss": 0.7187,
      "step": 362700
    },
    {
      "epoch": 3.3104606175633258,
      "grad_norm": 5.154428005218506,
      "learning_rate": 4.724128281869723e-05,
      "loss": 0.7007,
      "step": 362800
    },
    {
      "epoch": 3.311373092926491,
      "grad_norm": 4.150223255157471,
      "learning_rate": 4.724052242256126e-05,
      "loss": 0.7472,
      "step": 362900
    },
    {
      "epoch": 3.3122855682896564,
      "grad_norm": 4.4604811668396,
      "learning_rate": 4.723976202642529e-05,
      "loss": 0.7365,
      "step": 363000
    },
    {
      "epoch": 3.3131980436528212,
      "grad_norm": 4.428847312927246,
      "learning_rate": 4.723900163028932e-05,
      "loss": 0.6977,
      "step": 363100
    },
    {
      "epoch": 3.3141105190159865,
      "grad_norm": 4.687192916870117,
      "learning_rate": 4.7238241234153344e-05,
      "loss": 0.7558,
      "step": 363200
    },
    {
      "epoch": 3.315022994379152,
      "grad_norm": 4.055146217346191,
      "learning_rate": 4.723748083801738e-05,
      "loss": 0.7275,
      "step": 363300
    },
    {
      "epoch": 3.315935469742317,
      "grad_norm": 4.0029754638671875,
      "learning_rate": 4.7236720441881404e-05,
      "loss": 0.7253,
      "step": 363400
    },
    {
      "epoch": 3.316847945105482,
      "grad_norm": 4.095256328582764,
      "learning_rate": 4.7235960045745434e-05,
      "loss": 0.7162,
      "step": 363500
    },
    {
      "epoch": 3.3177604204686473,
      "grad_norm": 4.553630828857422,
      "learning_rate": 4.7235199649609464e-05,
      "loss": 0.7771,
      "step": 363600
    },
    {
      "epoch": 3.3186728958318126,
      "grad_norm": 3.1385114192962646,
      "learning_rate": 4.723443925347349e-05,
      "loss": 0.7506,
      "step": 363700
    },
    {
      "epoch": 3.319585371194978,
      "grad_norm": 4.546901226043701,
      "learning_rate": 4.7233678857337524e-05,
      "loss": 0.7375,
      "step": 363800
    },
    {
      "epoch": 3.320497846558143,
      "grad_norm": 3.987337112426758,
      "learning_rate": 4.723291846120155e-05,
      "loss": 0.7491,
      "step": 363900
    },
    {
      "epoch": 3.321410321921308,
      "grad_norm": 3.660078763961792,
      "learning_rate": 4.723215806506558e-05,
      "loss": 0.7347,
      "step": 364000
    },
    {
      "epoch": 3.3223227972844733,
      "grad_norm": 3.9080727100372314,
      "learning_rate": 4.723139766892961e-05,
      "loss": 0.7698,
      "step": 364100
    },
    {
      "epoch": 3.3232352726476386,
      "grad_norm": 3.5919349193573,
      "learning_rate": 4.723063727279364e-05,
      "loss": 0.6777,
      "step": 364200
    },
    {
      "epoch": 3.3241477480108035,
      "grad_norm": 4.4137654304504395,
      "learning_rate": 4.722987687665766e-05,
      "loss": 0.7416,
      "step": 364300
    },
    {
      "epoch": 3.325060223373969,
      "grad_norm": 3.9359610080718994,
      "learning_rate": 4.72291164805217e-05,
      "loss": 0.7421,
      "step": 364400
    },
    {
      "epoch": 3.325972698737134,
      "grad_norm": 3.5746092796325684,
      "learning_rate": 4.722835608438572e-05,
      "loss": 0.7259,
      "step": 364500
    },
    {
      "epoch": 3.3268851741002994,
      "grad_norm": 4.664000511169434,
      "learning_rate": 4.722759568824975e-05,
      "loss": 0.73,
      "step": 364600
    },
    {
      "epoch": 3.3277976494634647,
      "grad_norm": 3.9655675888061523,
      "learning_rate": 4.722683529211378e-05,
      "loss": 0.7209,
      "step": 364700
    },
    {
      "epoch": 3.3287101248266295,
      "grad_norm": 3.9267139434814453,
      "learning_rate": 4.722607489597781e-05,
      "loss": 0.724,
      "step": 364800
    },
    {
      "epoch": 3.329622600189795,
      "grad_norm": 2.684523344039917,
      "learning_rate": 4.722531449984184e-05,
      "loss": 0.704,
      "step": 364900
    },
    {
      "epoch": 3.33053507555296,
      "grad_norm": 4.614980697631836,
      "learning_rate": 4.722455410370587e-05,
      "loss": 0.6943,
      "step": 365000
    },
    {
      "epoch": 3.3314475509161254,
      "grad_norm": 3.3952412605285645,
      "learning_rate": 4.7223793707569895e-05,
      "loss": 0.7021,
      "step": 365100
    },
    {
      "epoch": 3.3323600262792903,
      "grad_norm": 4.82344913482666,
      "learning_rate": 4.722303331143393e-05,
      "loss": 0.7786,
      "step": 365200
    },
    {
      "epoch": 3.3332725016424556,
      "grad_norm": 3.819789171218872,
      "learning_rate": 4.7222272915297955e-05,
      "loss": 0.726,
      "step": 365300
    },
    {
      "epoch": 3.334184977005621,
      "grad_norm": 4.057433605194092,
      "learning_rate": 4.7221512519161985e-05,
      "loss": 0.731,
      "step": 365400
    },
    {
      "epoch": 3.335097452368786,
      "grad_norm": 4.953972339630127,
      "learning_rate": 4.7220752123026015e-05,
      "loss": 0.722,
      "step": 365500
    },
    {
      "epoch": 3.3360099277319515,
      "grad_norm": 3.9433388710021973,
      "learning_rate": 4.7219991726890045e-05,
      "loss": 0.7664,
      "step": 365600
    },
    {
      "epoch": 3.3369224030951163,
      "grad_norm": 4.0477375984191895,
      "learning_rate": 4.721923133075407e-05,
      "loss": 0.7727,
      "step": 365700
    },
    {
      "epoch": 3.3378348784582816,
      "grad_norm": 5.134467124938965,
      "learning_rate": 4.7218470934618105e-05,
      "loss": 0.723,
      "step": 365800
    },
    {
      "epoch": 3.338747353821447,
      "grad_norm": 4.142622947692871,
      "learning_rate": 4.721771053848213e-05,
      "loss": 0.7607,
      "step": 365900
    },
    {
      "epoch": 3.339659829184612,
      "grad_norm": 3.6373538970947266,
      "learning_rate": 4.721695014234616e-05,
      "loss": 0.7326,
      "step": 366000
    },
    {
      "epoch": 3.340572304547777,
      "grad_norm": 3.798337936401367,
      "learning_rate": 4.721618974621019e-05,
      "loss": 0.7668,
      "step": 366100
    },
    {
      "epoch": 3.3414847799109424,
      "grad_norm": 3.9893009662628174,
      "learning_rate": 4.721542935007422e-05,
      "loss": 0.7326,
      "step": 366200
    },
    {
      "epoch": 3.3423972552741077,
      "grad_norm": 3.2313199043273926,
      "learning_rate": 4.721466895393825e-05,
      "loss": 0.7863,
      "step": 366300
    },
    {
      "epoch": 3.343309730637273,
      "grad_norm": 4.432122230529785,
      "learning_rate": 4.721390855780227e-05,
      "loss": 0.7597,
      "step": 366400
    },
    {
      "epoch": 3.344222206000438,
      "grad_norm": 4.4990949630737305,
      "learning_rate": 4.72131481616663e-05,
      "loss": 0.7132,
      "step": 366500
    },
    {
      "epoch": 3.345134681363603,
      "grad_norm": 2.989445924758911,
      "learning_rate": 4.721238776553033e-05,
      "loss": 0.7168,
      "step": 366600
    },
    {
      "epoch": 3.3460471567267684,
      "grad_norm": 5.594157695770264,
      "learning_rate": 4.721162736939436e-05,
      "loss": 0.7216,
      "step": 366700
    },
    {
      "epoch": 3.3469596320899337,
      "grad_norm": 3.611997127532959,
      "learning_rate": 4.7210866973258386e-05,
      "loss": 0.7313,
      "step": 366800
    },
    {
      "epoch": 3.3478721074530986,
      "grad_norm": 4.11321496963501,
      "learning_rate": 4.721010657712242e-05,
      "loss": 0.7519,
      "step": 366900
    },
    {
      "epoch": 3.348784582816264,
      "grad_norm": 4.028537273406982,
      "learning_rate": 4.7209346180986446e-05,
      "loss": 0.7569,
      "step": 367000
    },
    {
      "epoch": 3.349697058179429,
      "grad_norm": 4.059695243835449,
      "learning_rate": 4.7208585784850476e-05,
      "loss": 0.7188,
      "step": 367100
    },
    {
      "epoch": 3.3506095335425945,
      "grad_norm": 4.195001602172852,
      "learning_rate": 4.7207825388714506e-05,
      "loss": 0.7442,
      "step": 367200
    },
    {
      "epoch": 3.3515220089057594,
      "grad_norm": 4.015838146209717,
      "learning_rate": 4.7207064992578536e-05,
      "loss": 0.7149,
      "step": 367300
    },
    {
      "epoch": 3.3524344842689247,
      "grad_norm": 3.976783037185669,
      "learning_rate": 4.7206304596442566e-05,
      "loss": 0.7464,
      "step": 367400
    },
    {
      "epoch": 3.35334695963209,
      "grad_norm": 4.181077480316162,
      "learning_rate": 4.7205544200306596e-05,
      "loss": 0.7243,
      "step": 367500
    },
    {
      "epoch": 3.3542594349952553,
      "grad_norm": 3.6365857124328613,
      "learning_rate": 4.720478380417062e-05,
      "loss": 0.7755,
      "step": 367600
    },
    {
      "epoch": 3.35517191035842,
      "grad_norm": 3.258425235748291,
      "learning_rate": 4.7204023408034656e-05,
      "loss": 0.7244,
      "step": 367700
    },
    {
      "epoch": 3.3560843857215854,
      "grad_norm": 2.6900722980499268,
      "learning_rate": 4.720326301189868e-05,
      "loss": 0.7117,
      "step": 367800
    },
    {
      "epoch": 3.3569968610847507,
      "grad_norm": 3.943274736404419,
      "learning_rate": 4.720250261576271e-05,
      "loss": 0.7564,
      "step": 367900
    },
    {
      "epoch": 3.357909336447916,
      "grad_norm": 4.196712970733643,
      "learning_rate": 4.720174221962674e-05,
      "loss": 0.6598,
      "step": 368000
    },
    {
      "epoch": 3.3588218118110813,
      "grad_norm": 3.7970008850097656,
      "learning_rate": 4.720098182349077e-05,
      "loss": 0.7558,
      "step": 368100
    },
    {
      "epoch": 3.359734287174246,
      "grad_norm": 4.393167972564697,
      "learning_rate": 4.720022142735479e-05,
      "loss": 0.722,
      "step": 368200
    },
    {
      "epoch": 3.3606467625374115,
      "grad_norm": 3.9725515842437744,
      "learning_rate": 4.719946103121883e-05,
      "loss": 0.7176,
      "step": 368300
    },
    {
      "epoch": 3.3615592379005768,
      "grad_norm": 4.470964431762695,
      "learning_rate": 4.719870063508285e-05,
      "loss": 0.7736,
      "step": 368400
    },
    {
      "epoch": 3.362471713263742,
      "grad_norm": 4.26746940612793,
      "learning_rate": 4.7197940238946883e-05,
      "loss": 0.7072,
      "step": 368500
    },
    {
      "epoch": 3.363384188626907,
      "grad_norm": 4.154211521148682,
      "learning_rate": 4.7197179842810913e-05,
      "loss": 0.7622,
      "step": 368600
    },
    {
      "epoch": 3.364296663990072,
      "grad_norm": 3.808941602706909,
      "learning_rate": 4.7196419446674944e-05,
      "loss": 0.7329,
      "step": 368700
    },
    {
      "epoch": 3.3652091393532375,
      "grad_norm": 4.723533630371094,
      "learning_rate": 4.7195659050538974e-05,
      "loss": 0.7181,
      "step": 368800
    },
    {
      "epoch": 3.366121614716403,
      "grad_norm": 4.330209732055664,
      "learning_rate": 4.7194898654403004e-05,
      "loss": 0.7146,
      "step": 368900
    },
    {
      "epoch": 3.3670340900795677,
      "grad_norm": 5.269254207611084,
      "learning_rate": 4.719413825826703e-05,
      "loss": 0.7119,
      "step": 369000
    },
    {
      "epoch": 3.367946565442733,
      "grad_norm": 4.551790714263916,
      "learning_rate": 4.7193377862131064e-05,
      "loss": 0.7256,
      "step": 369100
    },
    {
      "epoch": 3.3688590408058983,
      "grad_norm": 6.266209125518799,
      "learning_rate": 4.719261746599509e-05,
      "loss": 0.756,
      "step": 369200
    },
    {
      "epoch": 3.3697715161690636,
      "grad_norm": 4.288627624511719,
      "learning_rate": 4.719185706985911e-05,
      "loss": 0.7298,
      "step": 369300
    },
    {
      "epoch": 3.3706839915322284,
      "grad_norm": 4.096366882324219,
      "learning_rate": 4.719109667372315e-05,
      "loss": 0.7598,
      "step": 369400
    },
    {
      "epoch": 3.3715964668953937,
      "grad_norm": 3.8539271354675293,
      "learning_rate": 4.719033627758717e-05,
      "loss": 0.738,
      "step": 369500
    },
    {
      "epoch": 3.372508942258559,
      "grad_norm": 3.7178151607513428,
      "learning_rate": 4.71895758814512e-05,
      "loss": 0.7336,
      "step": 369600
    },
    {
      "epoch": 3.3734214176217243,
      "grad_norm": 4.111144542694092,
      "learning_rate": 4.718881548531523e-05,
      "loss": 0.7117,
      "step": 369700
    },
    {
      "epoch": 3.3743338929848896,
      "grad_norm": 3.419412851333618,
      "learning_rate": 4.718805508917926e-05,
      "loss": 0.7422,
      "step": 369800
    },
    {
      "epoch": 3.3752463683480545,
      "grad_norm": 3.2009878158569336,
      "learning_rate": 4.718729469304329e-05,
      "loss": 0.7409,
      "step": 369900
    },
    {
      "epoch": 3.3761588437112198,
      "grad_norm": 4.190420627593994,
      "learning_rate": 4.718653429690732e-05,
      "loss": 0.6949,
      "step": 370000
    },
    {
      "epoch": 3.377071319074385,
      "grad_norm": 3.8942928314208984,
      "learning_rate": 4.7185773900771344e-05,
      "loss": 0.7318,
      "step": 370100
    },
    {
      "epoch": 3.3779837944375504,
      "grad_norm": 4.747195243835449,
      "learning_rate": 4.718501350463538e-05,
      "loss": 0.7444,
      "step": 370200
    },
    {
      "epoch": 3.3788962698007152,
      "grad_norm": 3.801800489425659,
      "learning_rate": 4.7184253108499404e-05,
      "loss": 0.704,
      "step": 370300
    },
    {
      "epoch": 3.3798087451638805,
      "grad_norm": 4.522612571716309,
      "learning_rate": 4.7183492712363434e-05,
      "loss": 0.7692,
      "step": 370400
    },
    {
      "epoch": 3.380721220527046,
      "grad_norm": 3.896303415298462,
      "learning_rate": 4.7182732316227464e-05,
      "loss": 0.7412,
      "step": 370500
    },
    {
      "epoch": 3.381633695890211,
      "grad_norm": 4.410833358764648,
      "learning_rate": 4.7181971920091494e-05,
      "loss": 0.7342,
      "step": 370600
    },
    {
      "epoch": 3.382546171253376,
      "grad_norm": 3.5736305713653564,
      "learning_rate": 4.718121152395552e-05,
      "loss": 0.7012,
      "step": 370700
    },
    {
      "epoch": 3.3834586466165413,
      "grad_norm": 4.028936386108398,
      "learning_rate": 4.7180451127819555e-05,
      "loss": 0.7421,
      "step": 370800
    },
    {
      "epoch": 3.3843711219797066,
      "grad_norm": 5.129684925079346,
      "learning_rate": 4.717969073168358e-05,
      "loss": 0.7793,
      "step": 370900
    },
    {
      "epoch": 3.385283597342872,
      "grad_norm": 4.440248966217041,
      "learning_rate": 4.717893033554761e-05,
      "loss": 0.7033,
      "step": 371000
    },
    {
      "epoch": 3.3861960727060367,
      "grad_norm": 4.007363796234131,
      "learning_rate": 4.717816993941164e-05,
      "loss": 0.6979,
      "step": 371100
    },
    {
      "epoch": 3.387108548069202,
      "grad_norm": 4.259214401245117,
      "learning_rate": 4.717740954327567e-05,
      "loss": 0.7719,
      "step": 371200
    },
    {
      "epoch": 3.3880210234323673,
      "grad_norm": 5.102652549743652,
      "learning_rate": 4.71766491471397e-05,
      "loss": 0.7207,
      "step": 371300
    },
    {
      "epoch": 3.3889334987955326,
      "grad_norm": 4.4404296875,
      "learning_rate": 4.717588875100373e-05,
      "loss": 0.7425,
      "step": 371400
    },
    {
      "epoch": 3.389845974158698,
      "grad_norm": 3.561877727508545,
      "learning_rate": 4.717512835486775e-05,
      "loss": 0.7541,
      "step": 371500
    },
    {
      "epoch": 3.390758449521863,
      "grad_norm": 4.427769184112549,
      "learning_rate": 4.717436795873179e-05,
      "loss": 0.7267,
      "step": 371600
    },
    {
      "epoch": 3.391670924885028,
      "grad_norm": 4.826210975646973,
      "learning_rate": 4.717360756259581e-05,
      "loss": 0.7241,
      "step": 371700
    },
    {
      "epoch": 3.3925834002481934,
      "grad_norm": 4.652218341827393,
      "learning_rate": 4.717284716645984e-05,
      "loss": 0.7135,
      "step": 371800
    },
    {
      "epoch": 3.3934958756113587,
      "grad_norm": 3.1571044921875,
      "learning_rate": 4.717208677032387e-05,
      "loss": 0.7453,
      "step": 371900
    },
    {
      "epoch": 3.3944083509745235,
      "grad_norm": 5.3355255126953125,
      "learning_rate": 4.71713263741879e-05,
      "loss": 0.7271,
      "step": 372000
    },
    {
      "epoch": 3.395320826337689,
      "grad_norm": 4.349674701690674,
      "learning_rate": 4.7170565978051925e-05,
      "loss": 0.7144,
      "step": 372100
    },
    {
      "epoch": 3.396233301700854,
      "grad_norm": 4.390585899353027,
      "learning_rate": 4.7169805581915955e-05,
      "loss": 0.7543,
      "step": 372200
    },
    {
      "epoch": 3.3971457770640194,
      "grad_norm": 3.6700282096862793,
      "learning_rate": 4.7169045185779985e-05,
      "loss": 0.7646,
      "step": 372300
    },
    {
      "epoch": 3.3980582524271843,
      "grad_norm": 3.6335861682891846,
      "learning_rate": 4.7168284789644015e-05,
      "loss": 0.7116,
      "step": 372400
    },
    {
      "epoch": 3.3989707277903496,
      "grad_norm": 4.074729919433594,
      "learning_rate": 4.7167524393508045e-05,
      "loss": 0.7013,
      "step": 372500
    },
    {
      "epoch": 3.399883203153515,
      "grad_norm": 4.356374740600586,
      "learning_rate": 4.716676399737207e-05,
      "loss": 0.7124,
      "step": 372600
    },
    {
      "epoch": 3.40079567851668,
      "grad_norm": 3.513235569000244,
      "learning_rate": 4.7166003601236106e-05,
      "loss": 0.7834,
      "step": 372700
    },
    {
      "epoch": 3.401708153879845,
      "grad_norm": 2.655097007751465,
      "learning_rate": 4.716524320510013e-05,
      "loss": 0.7598,
      "step": 372800
    },
    {
      "epoch": 3.4026206292430103,
      "grad_norm": 4.425268173217773,
      "learning_rate": 4.716448280896416e-05,
      "loss": 0.7139,
      "step": 372900
    },
    {
      "epoch": 3.4035331046061756,
      "grad_norm": 2.8519153594970703,
      "learning_rate": 4.716372241282819e-05,
      "loss": 0.7534,
      "step": 373000
    },
    {
      "epoch": 3.404445579969341,
      "grad_norm": 4.044124126434326,
      "learning_rate": 4.716296201669222e-05,
      "loss": 0.7353,
      "step": 373100
    },
    {
      "epoch": 3.4053580553325062,
      "grad_norm": 4.171593189239502,
      "learning_rate": 4.716220162055624e-05,
      "loss": 0.7436,
      "step": 373200
    },
    {
      "epoch": 3.406270530695671,
      "grad_norm": 5.040644645690918,
      "learning_rate": 4.716144122442028e-05,
      "loss": 0.7475,
      "step": 373300
    },
    {
      "epoch": 3.4071830060588364,
      "grad_norm": 3.3636860847473145,
      "learning_rate": 4.71606808282843e-05,
      "loss": 0.6996,
      "step": 373400
    },
    {
      "epoch": 3.4080954814220017,
      "grad_norm": 3.7933969497680664,
      "learning_rate": 4.715992043214833e-05,
      "loss": 0.7476,
      "step": 373500
    },
    {
      "epoch": 3.409007956785167,
      "grad_norm": 5.06417989730835,
      "learning_rate": 4.715916003601236e-05,
      "loss": 0.7211,
      "step": 373600
    },
    {
      "epoch": 3.409920432148332,
      "grad_norm": 3.855746030807495,
      "learning_rate": 4.715839963987639e-05,
      "loss": 0.7503,
      "step": 373700
    },
    {
      "epoch": 3.410832907511497,
      "grad_norm": 3.196737289428711,
      "learning_rate": 4.715763924374042e-05,
      "loss": 0.7218,
      "step": 373800
    },
    {
      "epoch": 3.4117453828746624,
      "grad_norm": 4.786140441894531,
      "learning_rate": 4.715687884760445e-05,
      "loss": 0.732,
      "step": 373900
    },
    {
      "epoch": 3.4126578582378277,
      "grad_norm": 4.197044372558594,
      "learning_rate": 4.7156118451468476e-05,
      "loss": 0.7457,
      "step": 374000
    },
    {
      "epoch": 3.4135703336009926,
      "grad_norm": 4.298619270324707,
      "learning_rate": 4.715535805533251e-05,
      "loss": 0.6949,
      "step": 374100
    },
    {
      "epoch": 3.414482808964158,
      "grad_norm": 3.4744865894317627,
      "learning_rate": 4.7154597659196536e-05,
      "loss": 0.7138,
      "step": 374200
    },
    {
      "epoch": 3.415395284327323,
      "grad_norm": 3.798860549926758,
      "learning_rate": 4.7153837263060566e-05,
      "loss": 0.7488,
      "step": 374300
    },
    {
      "epoch": 3.4163077596904885,
      "grad_norm": 2.8133697509765625,
      "learning_rate": 4.7153076866924596e-05,
      "loss": 0.751,
      "step": 374400
    },
    {
      "epoch": 3.4172202350536534,
      "grad_norm": 4.678199291229248,
      "learning_rate": 4.7152316470788626e-05,
      "loss": 0.6968,
      "step": 374500
    },
    {
      "epoch": 3.4181327104168187,
      "grad_norm": 3.548976421356201,
      "learning_rate": 4.715155607465265e-05,
      "loss": 0.7132,
      "step": 374600
    },
    {
      "epoch": 3.419045185779984,
      "grad_norm": 4.685152053833008,
      "learning_rate": 4.7150795678516687e-05,
      "loss": 0.7174,
      "step": 374700
    },
    {
      "epoch": 3.4199576611431493,
      "grad_norm": 3.274359703063965,
      "learning_rate": 4.715003528238071e-05,
      "loss": 0.6932,
      "step": 374800
    },
    {
      "epoch": 3.4208701365063146,
      "grad_norm": 3.324632406234741,
      "learning_rate": 4.714927488624474e-05,
      "loss": 0.7492,
      "step": 374900
    },
    {
      "epoch": 3.4217826118694794,
      "grad_norm": 4.22348165512085,
      "learning_rate": 4.714851449010877e-05,
      "loss": 0.7176,
      "step": 375000
    },
    {
      "epoch": 3.4226950872326447,
      "grad_norm": 4.197150230407715,
      "learning_rate": 4.714775409397279e-05,
      "loss": 0.7065,
      "step": 375100
    },
    {
      "epoch": 3.42360756259581,
      "grad_norm": 3.6291661262512207,
      "learning_rate": 4.714699369783683e-05,
      "loss": 0.7305,
      "step": 375200
    },
    {
      "epoch": 3.424520037958975,
      "grad_norm": 4.3612565994262695,
      "learning_rate": 4.7146233301700853e-05,
      "loss": 0.7293,
      "step": 375300
    },
    {
      "epoch": 3.42543251332214,
      "grad_norm": 5.565362930297852,
      "learning_rate": 4.7145472905564884e-05,
      "loss": 0.6858,
      "step": 375400
    },
    {
      "epoch": 3.4263449886853055,
      "grad_norm": 4.384678363800049,
      "learning_rate": 4.7144712509428914e-05,
      "loss": 0.7106,
      "step": 375500
    },
    {
      "epoch": 3.4272574640484708,
      "grad_norm": 4.14347505569458,
      "learning_rate": 4.7143952113292944e-05,
      "loss": 0.7558,
      "step": 375600
    },
    {
      "epoch": 3.428169939411636,
      "grad_norm": 4.338446140289307,
      "learning_rate": 4.7143191717156974e-05,
      "loss": 0.7323,
      "step": 375700
    },
    {
      "epoch": 3.429082414774801,
      "grad_norm": 4.201111316680908,
      "learning_rate": 4.7142431321021004e-05,
      "loss": 0.7373,
      "step": 375800
    },
    {
      "epoch": 3.429994890137966,
      "grad_norm": 4.56051778793335,
      "learning_rate": 4.714167092488503e-05,
      "loss": 0.761,
      "step": 375900
    },
    {
      "epoch": 3.4309073655011315,
      "grad_norm": 3.6461691856384277,
      "learning_rate": 4.714091052874906e-05,
      "loss": 0.7386,
      "step": 376000
    },
    {
      "epoch": 3.431819840864297,
      "grad_norm": 3.4417576789855957,
      "learning_rate": 4.714015013261309e-05,
      "loss": 0.7432,
      "step": 376100
    },
    {
      "epoch": 3.4327323162274617,
      "grad_norm": 3.3141069412231445,
      "learning_rate": 4.713938973647712e-05,
      "loss": 0.7663,
      "step": 376200
    },
    {
      "epoch": 3.433644791590627,
      "grad_norm": 4.355069637298584,
      "learning_rate": 4.713862934034115e-05,
      "loss": 0.7316,
      "step": 376300
    },
    {
      "epoch": 3.4345572669537923,
      "grad_norm": 5.028255462646484,
      "learning_rate": 4.713786894420518e-05,
      "loss": 0.7533,
      "step": 376400
    },
    {
      "epoch": 3.4354697423169576,
      "grad_norm": 4.9971137046813965,
      "learning_rate": 4.71371085480692e-05,
      "loss": 0.7411,
      "step": 376500
    },
    {
      "epoch": 3.436382217680123,
      "grad_norm": 4.350061893463135,
      "learning_rate": 4.713634815193324e-05,
      "loss": 0.6903,
      "step": 376600
    },
    {
      "epoch": 3.4372946930432877,
      "grad_norm": 4.04423189163208,
      "learning_rate": 4.713558775579726e-05,
      "loss": 0.7099,
      "step": 376700
    },
    {
      "epoch": 3.438207168406453,
      "grad_norm": 3.3970699310302734,
      "learning_rate": 4.713482735966129e-05,
      "loss": 0.7106,
      "step": 376800
    },
    {
      "epoch": 3.4391196437696183,
      "grad_norm": 4.245410442352295,
      "learning_rate": 4.713406696352532e-05,
      "loss": 0.6895,
      "step": 376900
    },
    {
      "epoch": 3.440032119132783,
      "grad_norm": 3.9024314880371094,
      "learning_rate": 4.713330656738935e-05,
      "loss": 0.7487,
      "step": 377000
    },
    {
      "epoch": 3.4409445944959485,
      "grad_norm": 4.296019077301025,
      "learning_rate": 4.713254617125338e-05,
      "loss": 0.7645,
      "step": 377100
    },
    {
      "epoch": 3.4418570698591138,
      "grad_norm": 3.9502971172332764,
      "learning_rate": 4.713178577511741e-05,
      "loss": 0.7231,
      "step": 377200
    },
    {
      "epoch": 3.442769545222279,
      "grad_norm": 3.5621776580810547,
      "learning_rate": 4.7131025378981434e-05,
      "loss": 0.723,
      "step": 377300
    },
    {
      "epoch": 3.4436820205854444,
      "grad_norm": 4.274910926818848,
      "learning_rate": 4.7130264982845465e-05,
      "loss": 0.7494,
      "step": 377400
    },
    {
      "epoch": 3.4445944959486092,
      "grad_norm": 2.7568092346191406,
      "learning_rate": 4.7129504586709495e-05,
      "loss": 0.7508,
      "step": 377500
    },
    {
      "epoch": 3.4455069713117745,
      "grad_norm": 2.5755839347839355,
      "learning_rate": 4.7128744190573525e-05,
      "loss": 0.7711,
      "step": 377600
    },
    {
      "epoch": 3.44641944667494,
      "grad_norm": 4.13123083114624,
      "learning_rate": 4.7127983794437555e-05,
      "loss": 0.7431,
      "step": 377700
    },
    {
      "epoch": 3.447331922038105,
      "grad_norm": 5.437438488006592,
      "learning_rate": 4.712722339830158e-05,
      "loss": 0.7451,
      "step": 377800
    },
    {
      "epoch": 3.44824439740127,
      "grad_norm": 4.804293155670166,
      "learning_rate": 4.712646300216561e-05,
      "loss": 0.716,
      "step": 377900
    },
    {
      "epoch": 3.4491568727644353,
      "grad_norm": 3.1027374267578125,
      "learning_rate": 4.712570260602964e-05,
      "loss": 0.7484,
      "step": 378000
    },
    {
      "epoch": 3.4500693481276006,
      "grad_norm": 3.5696074962615967,
      "learning_rate": 4.712494220989367e-05,
      "loss": 0.7361,
      "step": 378100
    },
    {
      "epoch": 3.450981823490766,
      "grad_norm": 3.3703739643096924,
      "learning_rate": 4.71241818137577e-05,
      "loss": 0.7307,
      "step": 378200
    },
    {
      "epoch": 3.451894298853931,
      "grad_norm": 4.582193851470947,
      "learning_rate": 4.712342141762173e-05,
      "loss": 0.7198,
      "step": 378300
    },
    {
      "epoch": 3.452806774217096,
      "grad_norm": 4.256913185119629,
      "learning_rate": 4.712266102148575e-05,
      "loss": 0.7201,
      "step": 378400
    },
    {
      "epoch": 3.4537192495802613,
      "grad_norm": 4.739476203918457,
      "learning_rate": 4.712190062534979e-05,
      "loss": 0.7,
      "step": 378500
    },
    {
      "epoch": 3.4546317249434266,
      "grad_norm": 4.537616729736328,
      "learning_rate": 4.712114022921381e-05,
      "loss": 0.6799,
      "step": 378600
    },
    {
      "epoch": 3.4555442003065915,
      "grad_norm": 4.66221809387207,
      "learning_rate": 4.712037983307784e-05,
      "loss": 0.7683,
      "step": 378700
    },
    {
      "epoch": 3.456456675669757,
      "grad_norm": 3.9966211318969727,
      "learning_rate": 4.711961943694187e-05,
      "loss": 0.7116,
      "step": 378800
    },
    {
      "epoch": 3.457369151032922,
      "grad_norm": 4.427234172821045,
      "learning_rate": 4.71188590408059e-05,
      "loss": 0.7655,
      "step": 378900
    },
    {
      "epoch": 3.4582816263960874,
      "grad_norm": 3.959960699081421,
      "learning_rate": 4.7118098644669925e-05,
      "loss": 0.702,
      "step": 379000
    },
    {
      "epoch": 3.4591941017592527,
      "grad_norm": 4.335717678070068,
      "learning_rate": 4.711733824853396e-05,
      "loss": 0.7537,
      "step": 379100
    },
    {
      "epoch": 3.4601065771224175,
      "grad_norm": 3.9631187915802,
      "learning_rate": 4.7116577852397985e-05,
      "loss": 0.7566,
      "step": 379200
    },
    {
      "epoch": 3.461019052485583,
      "grad_norm": 4.3652753829956055,
      "learning_rate": 4.7115817456262015e-05,
      "loss": 0.7644,
      "step": 379300
    },
    {
      "epoch": 3.461931527848748,
      "grad_norm": 4.308724403381348,
      "learning_rate": 4.7115057060126046e-05,
      "loss": 0.756,
      "step": 379400
    },
    {
      "epoch": 3.4628440032119134,
      "grad_norm": 3.8500263690948486,
      "learning_rate": 4.7114296663990076e-05,
      "loss": 0.7464,
      "step": 379500
    },
    {
      "epoch": 3.4637564785750783,
      "grad_norm": 3.4999334812164307,
      "learning_rate": 4.7113536267854106e-05,
      "loss": 0.7409,
      "step": 379600
    },
    {
      "epoch": 3.4646689539382436,
      "grad_norm": 3.705972671508789,
      "learning_rate": 4.7112775871718136e-05,
      "loss": 0.742,
      "step": 379700
    },
    {
      "epoch": 3.465581429301409,
      "grad_norm": 4.357077598571777,
      "learning_rate": 4.711201547558216e-05,
      "loss": 0.7012,
      "step": 379800
    },
    {
      "epoch": 3.466493904664574,
      "grad_norm": 5.236342906951904,
      "learning_rate": 4.7111255079446196e-05,
      "loss": 0.7487,
      "step": 379900
    },
    {
      "epoch": 3.4674063800277395,
      "grad_norm": 3.626518964767456,
      "learning_rate": 4.711049468331022e-05,
      "loss": 0.7348,
      "step": 380000
    },
    {
      "epoch": 3.4683188553909043,
      "grad_norm": 3.777845859527588,
      "learning_rate": 4.710973428717425e-05,
      "loss": 0.7527,
      "step": 380100
    },
    {
      "epoch": 3.4692313307540696,
      "grad_norm": 3.4773497581481934,
      "learning_rate": 4.710897389103828e-05,
      "loss": 0.7358,
      "step": 380200
    },
    {
      "epoch": 3.470143806117235,
      "grad_norm": 4.437536239624023,
      "learning_rate": 4.710821349490231e-05,
      "loss": 0.7339,
      "step": 380300
    },
    {
      "epoch": 3.4710562814804,
      "grad_norm": 4.162502288818359,
      "learning_rate": 4.710745309876633e-05,
      "loss": 0.7119,
      "step": 380400
    },
    {
      "epoch": 3.471968756843565,
      "grad_norm": 4.3291239738464355,
      "learning_rate": 4.710669270263037e-05,
      "loss": 0.7383,
      "step": 380500
    },
    {
      "epoch": 3.4728812322067304,
      "grad_norm": 4.3914265632629395,
      "learning_rate": 4.710593230649439e-05,
      "loss": 0.7553,
      "step": 380600
    },
    {
      "epoch": 3.4737937075698957,
      "grad_norm": 3.256171703338623,
      "learning_rate": 4.710517191035842e-05,
      "loss": 0.7301,
      "step": 380700
    },
    {
      "epoch": 3.474706182933061,
      "grad_norm": 4.542724132537842,
      "learning_rate": 4.710441151422245e-05,
      "loss": 0.7304,
      "step": 380800
    },
    {
      "epoch": 3.475618658296226,
      "grad_norm": 4.580248832702637,
      "learning_rate": 4.7103651118086476e-05,
      "loss": 0.7759,
      "step": 380900
    },
    {
      "epoch": 3.476531133659391,
      "grad_norm": 4.4717326164245605,
      "learning_rate": 4.710289072195051e-05,
      "loss": 0.7351,
      "step": 381000
    },
    {
      "epoch": 3.4774436090225564,
      "grad_norm": 4.648679256439209,
      "learning_rate": 4.7102130325814536e-05,
      "loss": 0.735,
      "step": 381100
    },
    {
      "epoch": 3.4783560843857217,
      "grad_norm": 4.588157653808594,
      "learning_rate": 4.7101369929678566e-05,
      "loss": 0.7289,
      "step": 381200
    },
    {
      "epoch": 3.4792685597488866,
      "grad_norm": 4.121613502502441,
      "learning_rate": 4.7100609533542596e-05,
      "loss": 0.7462,
      "step": 381300
    },
    {
      "epoch": 3.480181035112052,
      "grad_norm": 3.964111804962158,
      "learning_rate": 4.7099849137406627e-05,
      "loss": 0.7271,
      "step": 381400
    },
    {
      "epoch": 3.481093510475217,
      "grad_norm": 4.464456081390381,
      "learning_rate": 4.709908874127065e-05,
      "loss": 0.7429,
      "step": 381500
    },
    {
      "epoch": 3.4820059858383825,
      "grad_norm": 4.133628845214844,
      "learning_rate": 4.709832834513469e-05,
      "loss": 0.7,
      "step": 381600
    },
    {
      "epoch": 3.482918461201548,
      "grad_norm": 4.385806560516357,
      "learning_rate": 4.709756794899871e-05,
      "loss": 0.7206,
      "step": 381700
    },
    {
      "epoch": 3.4838309365647127,
      "grad_norm": 3.5642008781433105,
      "learning_rate": 4.709680755286274e-05,
      "loss": 0.7183,
      "step": 381800
    },
    {
      "epoch": 3.484743411927878,
      "grad_norm": 4.091855525970459,
      "learning_rate": 4.709604715672677e-05,
      "loss": 0.7125,
      "step": 381900
    },
    {
      "epoch": 3.4856558872910433,
      "grad_norm": 4.079415798187256,
      "learning_rate": 4.70952867605908e-05,
      "loss": 0.7162,
      "step": 382000
    },
    {
      "epoch": 3.486568362654208,
      "grad_norm": 4.438429355621338,
      "learning_rate": 4.709452636445483e-05,
      "loss": 0.7508,
      "step": 382100
    },
    {
      "epoch": 3.4874808380173734,
      "grad_norm": 4.479633331298828,
      "learning_rate": 4.709376596831886e-05,
      "loss": 0.7359,
      "step": 382200
    },
    {
      "epoch": 3.4883933133805387,
      "grad_norm": 4.03930139541626,
      "learning_rate": 4.7093005572182884e-05,
      "loss": 0.7853,
      "step": 382300
    },
    {
      "epoch": 3.489305788743704,
      "grad_norm": 4.069916725158691,
      "learning_rate": 4.709224517604692e-05,
      "loss": 0.7165,
      "step": 382400
    },
    {
      "epoch": 3.4902182641068693,
      "grad_norm": 4.855310916900635,
      "learning_rate": 4.7091484779910944e-05,
      "loss": 0.7661,
      "step": 382500
    },
    {
      "epoch": 3.491130739470034,
      "grad_norm": 4.8705902099609375,
      "learning_rate": 4.7090724383774974e-05,
      "loss": 0.7257,
      "step": 382600
    },
    {
      "epoch": 3.4920432148331995,
      "grad_norm": 4.323821544647217,
      "learning_rate": 4.7089963987639004e-05,
      "loss": 0.7437,
      "step": 382700
    },
    {
      "epoch": 3.4929556901963648,
      "grad_norm": 4.496278762817383,
      "learning_rate": 4.7089203591503034e-05,
      "loss": 0.6941,
      "step": 382800
    },
    {
      "epoch": 3.49386816555953,
      "grad_norm": 4.084007263183594,
      "learning_rate": 4.708844319536706e-05,
      "loss": 0.7185,
      "step": 382900
    },
    {
      "epoch": 3.494780640922695,
      "grad_norm": 3.915083169937134,
      "learning_rate": 4.7087682799231094e-05,
      "loss": 0.7111,
      "step": 383000
    },
    {
      "epoch": 3.49569311628586,
      "grad_norm": 4.199038505554199,
      "learning_rate": 4.708692240309512e-05,
      "loss": 0.7017,
      "step": 383100
    },
    {
      "epoch": 3.4966055916490255,
      "grad_norm": 3.6633033752441406,
      "learning_rate": 4.708616200695915e-05,
      "loss": 0.7544,
      "step": 383200
    },
    {
      "epoch": 3.497518067012191,
      "grad_norm": 4.319456577301025,
      "learning_rate": 4.708540161082318e-05,
      "loss": 0.7441,
      "step": 383300
    },
    {
      "epoch": 3.498430542375356,
      "grad_norm": 5.1094069480896,
      "learning_rate": 4.708464121468721e-05,
      "loss": 0.7471,
      "step": 383400
    },
    {
      "epoch": 3.499343017738521,
      "grad_norm": 5.1647114753723145,
      "learning_rate": 4.708388081855124e-05,
      "loss": 0.7239,
      "step": 383500
    },
    {
      "epoch": 3.5002554931016863,
      "grad_norm": 3.508305549621582,
      "learning_rate": 4.708312042241526e-05,
      "loss": 0.7294,
      "step": 383600
    },
    {
      "epoch": 3.5011679684648516,
      "grad_norm": 3.5105645656585693,
      "learning_rate": 4.708236002627929e-05,
      "loss": 0.7255,
      "step": 383700
    },
    {
      "epoch": 3.5020804438280164,
      "grad_norm": 3.9024717807769775,
      "learning_rate": 4.708159963014332e-05,
      "loss": 0.7501,
      "step": 383800
    },
    {
      "epoch": 3.5029929191911817,
      "grad_norm": 4.320307731628418,
      "learning_rate": 4.708083923400735e-05,
      "loss": 0.7661,
      "step": 383900
    },
    {
      "epoch": 3.503905394554347,
      "grad_norm": 4.551271438598633,
      "learning_rate": 4.7080078837871374e-05,
      "loss": 0.7544,
      "step": 384000
    },
    {
      "epoch": 3.5048178699175123,
      "grad_norm": 4.705122470855713,
      "learning_rate": 4.707931844173541e-05,
      "loss": 0.7683,
      "step": 384100
    },
    {
      "epoch": 3.5057303452806776,
      "grad_norm": 4.352100372314453,
      "learning_rate": 4.7078558045599435e-05,
      "loss": 0.7596,
      "step": 384200
    },
    {
      "epoch": 3.5066428206438425,
      "grad_norm": 3.1480278968811035,
      "learning_rate": 4.7077797649463465e-05,
      "loss": 0.7534,
      "step": 384300
    },
    {
      "epoch": 3.5075552960070078,
      "grad_norm": 4.5952653884887695,
      "learning_rate": 4.7077037253327495e-05,
      "loss": 0.7459,
      "step": 384400
    },
    {
      "epoch": 3.508467771370173,
      "grad_norm": 4.38923978805542,
      "learning_rate": 4.7076276857191525e-05,
      "loss": 0.7167,
      "step": 384500
    },
    {
      "epoch": 3.509380246733338,
      "grad_norm": 4.116699695587158,
      "learning_rate": 4.7075516461055555e-05,
      "loss": 0.6993,
      "step": 384600
    },
    {
      "epoch": 3.5102927220965032,
      "grad_norm": 2.6697239875793457,
      "learning_rate": 4.7074756064919585e-05,
      "loss": 0.7318,
      "step": 384700
    },
    {
      "epoch": 3.5112051974596685,
      "grad_norm": 4.341913223266602,
      "learning_rate": 4.707399566878361e-05,
      "loss": 0.7257,
      "step": 384800
    },
    {
      "epoch": 3.512117672822834,
      "grad_norm": 4.267766952514648,
      "learning_rate": 4.7073235272647645e-05,
      "loss": 0.7254,
      "step": 384900
    },
    {
      "epoch": 3.513030148185999,
      "grad_norm": 5.106864929199219,
      "learning_rate": 4.707247487651167e-05,
      "loss": 0.741,
      "step": 385000
    },
    {
      "epoch": 3.5139426235491644,
      "grad_norm": 5.09536600112915,
      "learning_rate": 4.70717144803757e-05,
      "loss": 0.7394,
      "step": 385100
    },
    {
      "epoch": 3.5148550989123293,
      "grad_norm": 4.252213001251221,
      "learning_rate": 4.707095408423973e-05,
      "loss": 0.7536,
      "step": 385200
    },
    {
      "epoch": 3.5157675742754946,
      "grad_norm": 3.274848699569702,
      "learning_rate": 4.707019368810376e-05,
      "loss": 0.7262,
      "step": 385300
    },
    {
      "epoch": 3.51668004963866,
      "grad_norm": 3.8287696838378906,
      "learning_rate": 4.706943329196778e-05,
      "loss": 0.7299,
      "step": 385400
    },
    {
      "epoch": 3.5175925250018247,
      "grad_norm": 4.161135673522949,
      "learning_rate": 4.706867289583182e-05,
      "loss": 0.7179,
      "step": 385500
    },
    {
      "epoch": 3.51850500036499,
      "grad_norm": 3.7112581729888916,
      "learning_rate": 4.706791249969584e-05,
      "loss": 0.7121,
      "step": 385600
    },
    {
      "epoch": 3.5194174757281553,
      "grad_norm": 3.9812300205230713,
      "learning_rate": 4.706715210355987e-05,
      "loss": 0.7112,
      "step": 385700
    },
    {
      "epoch": 3.5203299510913206,
      "grad_norm": 4.101913928985596,
      "learning_rate": 4.70663917074239e-05,
      "loss": 0.7357,
      "step": 385800
    },
    {
      "epoch": 3.521242426454486,
      "grad_norm": 4.664113521575928,
      "learning_rate": 4.706563131128793e-05,
      "loss": 0.7749,
      "step": 385900
    },
    {
      "epoch": 3.522154901817651,
      "grad_norm": 3.4794342517852783,
      "learning_rate": 4.706487091515196e-05,
      "loss": 0.6926,
      "step": 386000
    },
    {
      "epoch": 3.523067377180816,
      "grad_norm": 4.308638095855713,
      "learning_rate": 4.706411051901599e-05,
      "loss": 0.7844,
      "step": 386100
    },
    {
      "epoch": 3.5239798525439814,
      "grad_norm": 4.80692720413208,
      "learning_rate": 4.7063350122880016e-05,
      "loss": 0.6934,
      "step": 386200
    },
    {
      "epoch": 3.5248923279071462,
      "grad_norm": 4.420419216156006,
      "learning_rate": 4.7062589726744046e-05,
      "loss": 0.7335,
      "step": 386300
    },
    {
      "epoch": 3.5258048032703115,
      "grad_norm": 3.880679130554199,
      "learning_rate": 4.7061829330608076e-05,
      "loss": 0.7523,
      "step": 386400
    },
    {
      "epoch": 3.526717278633477,
      "grad_norm": 3.8978872299194336,
      "learning_rate": 4.70610689344721e-05,
      "loss": 0.7478,
      "step": 386500
    },
    {
      "epoch": 3.527629753996642,
      "grad_norm": 4.121887683868408,
      "learning_rate": 4.7060308538336136e-05,
      "loss": 0.7148,
      "step": 386600
    },
    {
      "epoch": 3.5285422293598074,
      "grad_norm": 4.6580939292907715,
      "learning_rate": 4.705954814220016e-05,
      "loss": 0.7443,
      "step": 386700
    },
    {
      "epoch": 3.5294547047229727,
      "grad_norm": 4.153290748596191,
      "learning_rate": 4.705878774606419e-05,
      "loss": 0.7303,
      "step": 386800
    },
    {
      "epoch": 3.5303671800861376,
      "grad_norm": 4.5572333335876465,
      "learning_rate": 4.705802734992822e-05,
      "loss": 0.7537,
      "step": 386900
    },
    {
      "epoch": 3.531279655449303,
      "grad_norm": 4.219689846038818,
      "learning_rate": 4.705726695379225e-05,
      "loss": 0.7384,
      "step": 387000
    },
    {
      "epoch": 3.532192130812468,
      "grad_norm": 4.296765327453613,
      "learning_rate": 4.705650655765628e-05,
      "loss": 0.7139,
      "step": 387100
    },
    {
      "epoch": 3.533104606175633,
      "grad_norm": 4.514101982116699,
      "learning_rate": 4.705574616152031e-05,
      "loss": 0.7102,
      "step": 387200
    },
    {
      "epoch": 3.5340170815387983,
      "grad_norm": 2.7791266441345215,
      "learning_rate": 4.705498576538433e-05,
      "loss": 0.7694,
      "step": 387300
    },
    {
      "epoch": 3.5349295569019636,
      "grad_norm": 4.542418003082275,
      "learning_rate": 4.705422536924837e-05,
      "loss": 0.7261,
      "step": 387400
    },
    {
      "epoch": 3.535842032265129,
      "grad_norm": 3.994535207748413,
      "learning_rate": 4.705346497311239e-05,
      "loss": 0.7246,
      "step": 387500
    },
    {
      "epoch": 3.5367545076282942,
      "grad_norm": 4.6467719078063965,
      "learning_rate": 4.705270457697642e-05,
      "loss": 0.7126,
      "step": 387600
    },
    {
      "epoch": 3.537666982991459,
      "grad_norm": 4.631080150604248,
      "learning_rate": 4.705194418084045e-05,
      "loss": 0.7671,
      "step": 387700
    },
    {
      "epoch": 3.5385794583546244,
      "grad_norm": 4.214254379272461,
      "learning_rate": 4.705118378470448e-05,
      "loss": 0.7424,
      "step": 387800
    },
    {
      "epoch": 3.5394919337177897,
      "grad_norm": 3.7041635513305664,
      "learning_rate": 4.7050423388568506e-05,
      "loss": 0.7538,
      "step": 387900
    },
    {
      "epoch": 3.5404044090809546,
      "grad_norm": 3.6166138648986816,
      "learning_rate": 4.704966299243254e-05,
      "loss": 0.7547,
      "step": 388000
    },
    {
      "epoch": 3.54131688444412,
      "grad_norm": 4.233958721160889,
      "learning_rate": 4.7048902596296567e-05,
      "loss": 0.7293,
      "step": 388100
    },
    {
      "epoch": 3.542229359807285,
      "grad_norm": 3.870820999145508,
      "learning_rate": 4.7048142200160597e-05,
      "loss": 0.7328,
      "step": 388200
    },
    {
      "epoch": 3.5431418351704504,
      "grad_norm": 4.914161205291748,
      "learning_rate": 4.704738180402463e-05,
      "loss": 0.7104,
      "step": 388300
    },
    {
      "epoch": 3.5440543105336157,
      "grad_norm": 4.632920742034912,
      "learning_rate": 4.704662140788866e-05,
      "loss": 0.7515,
      "step": 388400
    },
    {
      "epoch": 3.544966785896781,
      "grad_norm": 3.7064154148101807,
      "learning_rate": 4.704586101175269e-05,
      "loss": 0.7519,
      "step": 388500
    },
    {
      "epoch": 3.545879261259946,
      "grad_norm": 5.0814714431762695,
      "learning_rate": 4.704510061561672e-05,
      "loss": 0.7348,
      "step": 388600
    },
    {
      "epoch": 3.546791736623111,
      "grad_norm": 3.4230120182037354,
      "learning_rate": 4.704434021948074e-05,
      "loss": 0.7081,
      "step": 388700
    },
    {
      "epoch": 3.5477042119862765,
      "grad_norm": 4.456929683685303,
      "learning_rate": 4.704357982334478e-05,
      "loss": 0.696,
      "step": 388800
    },
    {
      "epoch": 3.5486166873494414,
      "grad_norm": 3.6877710819244385,
      "learning_rate": 4.70428194272088e-05,
      "loss": 0.7523,
      "step": 388900
    },
    {
      "epoch": 3.5495291627126067,
      "grad_norm": 4.861880302429199,
      "learning_rate": 4.704205903107283e-05,
      "loss": 0.7554,
      "step": 389000
    },
    {
      "epoch": 3.550441638075772,
      "grad_norm": 4.209542751312256,
      "learning_rate": 4.704129863493686e-05,
      "loss": 0.7295,
      "step": 389100
    },
    {
      "epoch": 3.5513541134389373,
      "grad_norm": 4.373682975769043,
      "learning_rate": 4.7040538238800884e-05,
      "loss": 0.7158,
      "step": 389200
    },
    {
      "epoch": 3.5522665888021026,
      "grad_norm": 4.5250630378723145,
      "learning_rate": 4.703977784266492e-05,
      "loss": 0.7467,
      "step": 389300
    },
    {
      "epoch": 3.5531790641652674,
      "grad_norm": 4.299789905548096,
      "learning_rate": 4.7039017446528944e-05,
      "loss": 0.7266,
      "step": 389400
    },
    {
      "epoch": 3.5540915395284327,
      "grad_norm": 3.6154398918151855,
      "learning_rate": 4.7038257050392974e-05,
      "loss": 0.7618,
      "step": 389500
    },
    {
      "epoch": 3.555004014891598,
      "grad_norm": 3.9639790058135986,
      "learning_rate": 4.7037496654257004e-05,
      "loss": 0.7375,
      "step": 389600
    },
    {
      "epoch": 3.555916490254763,
      "grad_norm": 4.20089054107666,
      "learning_rate": 4.7036736258121034e-05,
      "loss": 0.7294,
      "step": 389700
    },
    {
      "epoch": 3.556828965617928,
      "grad_norm": 4.784577369689941,
      "learning_rate": 4.703597586198506e-05,
      "loss": 0.7026,
      "step": 389800
    },
    {
      "epoch": 3.5577414409810935,
      "grad_norm": 3.589776039123535,
      "learning_rate": 4.7035215465849094e-05,
      "loss": 0.7139,
      "step": 389900
    },
    {
      "epoch": 3.5586539163442588,
      "grad_norm": 4.625105381011963,
      "learning_rate": 4.703445506971312e-05,
      "loss": 0.7862,
      "step": 390000
    },
    {
      "epoch": 3.559566391707424,
      "grad_norm": 4.17859411239624,
      "learning_rate": 4.703369467357715e-05,
      "loss": 0.686,
      "step": 390100
    },
    {
      "epoch": 3.5604788670705894,
      "grad_norm": 4.756293773651123,
      "learning_rate": 4.703293427744118e-05,
      "loss": 0.7406,
      "step": 390200
    },
    {
      "epoch": 3.561391342433754,
      "grad_norm": 5.284499168395996,
      "learning_rate": 4.703217388130521e-05,
      "loss": 0.782,
      "step": 390300
    },
    {
      "epoch": 3.5623038177969195,
      "grad_norm": 4.072105407714844,
      "learning_rate": 4.703141348516924e-05,
      "loss": 0.7293,
      "step": 390400
    },
    {
      "epoch": 3.563216293160085,
      "grad_norm": 4.605783939361572,
      "learning_rate": 4.703065308903327e-05,
      "loss": 0.7577,
      "step": 390500
    },
    {
      "epoch": 3.5641287685232497,
      "grad_norm": 3.9240546226501465,
      "learning_rate": 4.702989269289729e-05,
      "loss": 0.7473,
      "step": 390600
    },
    {
      "epoch": 3.565041243886415,
      "grad_norm": 4.099812984466553,
      "learning_rate": 4.702913229676133e-05,
      "loss": 0.7409,
      "step": 390700
    },
    {
      "epoch": 3.5659537192495803,
      "grad_norm": 4.72015380859375,
      "learning_rate": 4.702837190062535e-05,
      "loss": 0.7146,
      "step": 390800
    },
    {
      "epoch": 3.5668661946127456,
      "grad_norm": 4.520310878753662,
      "learning_rate": 4.702761150448938e-05,
      "loss": 0.7368,
      "step": 390900
    },
    {
      "epoch": 3.567778669975911,
      "grad_norm": 4.654329776763916,
      "learning_rate": 4.702685110835341e-05,
      "loss": 0.7299,
      "step": 391000
    },
    {
      "epoch": 3.5686911453390757,
      "grad_norm": 4.785803318023682,
      "learning_rate": 4.702609071221744e-05,
      "loss": 0.7471,
      "step": 391100
    },
    {
      "epoch": 3.569603620702241,
      "grad_norm": 4.88313627243042,
      "learning_rate": 4.7025330316081465e-05,
      "loss": 0.7146,
      "step": 391200
    },
    {
      "epoch": 3.5705160960654063,
      "grad_norm": 4.609131336212158,
      "learning_rate": 4.70245699199455e-05,
      "loss": 0.732,
      "step": 391300
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 3.971775531768799,
      "learning_rate": 4.7023809523809525e-05,
      "loss": 0.7258,
      "step": 391400
    },
    {
      "epoch": 3.5723410467917365,
      "grad_norm": 3.684692621231079,
      "learning_rate": 4.7023049127673555e-05,
      "loss": 0.6992,
      "step": 391500
    },
    {
      "epoch": 3.5732535221549018,
      "grad_norm": 3.654797077178955,
      "learning_rate": 4.7022288731537585e-05,
      "loss": 0.7622,
      "step": 391600
    },
    {
      "epoch": 3.574165997518067,
      "grad_norm": 4.587968349456787,
      "learning_rate": 4.7021528335401615e-05,
      "loss": 0.728,
      "step": 391700
    },
    {
      "epoch": 3.5750784728812324,
      "grad_norm": 4.265666961669922,
      "learning_rate": 4.7020767939265645e-05,
      "loss": 0.7328,
      "step": 391800
    },
    {
      "epoch": 3.5759909482443977,
      "grad_norm": 4.086557388305664,
      "learning_rate": 4.7020007543129675e-05,
      "loss": 0.7399,
      "step": 391900
    },
    {
      "epoch": 3.5769034236075625,
      "grad_norm": 4.323111534118652,
      "learning_rate": 4.70192471469937e-05,
      "loss": 0.7261,
      "step": 392000
    },
    {
      "epoch": 3.577815898970728,
      "grad_norm": 4.124870777130127,
      "learning_rate": 4.701848675085773e-05,
      "loss": 0.7234,
      "step": 392100
    },
    {
      "epoch": 3.578728374333893,
      "grad_norm": 3.1212918758392334,
      "learning_rate": 4.701772635472176e-05,
      "loss": 0.7468,
      "step": 392200
    },
    {
      "epoch": 3.579640849697058,
      "grad_norm": 3.331052303314209,
      "learning_rate": 4.701696595858578e-05,
      "loss": 0.7184,
      "step": 392300
    },
    {
      "epoch": 3.5805533250602233,
      "grad_norm": 4.67659854888916,
      "learning_rate": 4.701620556244982e-05,
      "loss": 0.7606,
      "step": 392400
    },
    {
      "epoch": 3.5814658004233886,
      "grad_norm": 4.8425822257995605,
      "learning_rate": 4.701544516631384e-05,
      "loss": 0.735,
      "step": 392500
    },
    {
      "epoch": 3.582378275786554,
      "grad_norm": 3.43037748336792,
      "learning_rate": 4.701468477017787e-05,
      "loss": 0.7336,
      "step": 392600
    },
    {
      "epoch": 3.583290751149719,
      "grad_norm": 3.9088683128356934,
      "learning_rate": 4.70139243740419e-05,
      "loss": 0.7246,
      "step": 392700
    },
    {
      "epoch": 3.584203226512884,
      "grad_norm": 3.548752784729004,
      "learning_rate": 4.701316397790593e-05,
      "loss": 0.742,
      "step": 392800
    },
    {
      "epoch": 3.5851157018760493,
      "grad_norm": 3.685309886932373,
      "learning_rate": 4.701240358176996e-05,
      "loss": 0.7237,
      "step": 392900
    },
    {
      "epoch": 3.5860281772392146,
      "grad_norm": 3.6865627765655518,
      "learning_rate": 4.701164318563399e-05,
      "loss": 0.7054,
      "step": 393000
    },
    {
      "epoch": 3.5869406526023795,
      "grad_norm": 4.207443714141846,
      "learning_rate": 4.7010882789498016e-05,
      "loss": 0.7512,
      "step": 393100
    },
    {
      "epoch": 3.587853127965545,
      "grad_norm": 4.42310905456543,
      "learning_rate": 4.701012239336205e-05,
      "loss": 0.6942,
      "step": 393200
    },
    {
      "epoch": 3.58876560332871,
      "grad_norm": 3.5179617404937744,
      "learning_rate": 4.7009361997226076e-05,
      "loss": 0.7282,
      "step": 393300
    },
    {
      "epoch": 3.5896780786918754,
      "grad_norm": 3.965569257736206,
      "learning_rate": 4.7008601601090106e-05,
      "loss": 0.7415,
      "step": 393400
    },
    {
      "epoch": 3.5905905540550407,
      "grad_norm": 3.227510452270508,
      "learning_rate": 4.7007841204954136e-05,
      "loss": 0.7362,
      "step": 393500
    },
    {
      "epoch": 3.591503029418206,
      "grad_norm": 4.167611598968506,
      "learning_rate": 4.7007080808818166e-05,
      "loss": 0.7104,
      "step": 393600
    },
    {
      "epoch": 3.592415504781371,
      "grad_norm": 4.278213024139404,
      "learning_rate": 4.700632041268219e-05,
      "loss": 0.7628,
      "step": 393700
    },
    {
      "epoch": 3.593327980144536,
      "grad_norm": 3.669477939605713,
      "learning_rate": 4.7005560016546226e-05,
      "loss": 0.7239,
      "step": 393800
    },
    {
      "epoch": 3.5942404555077014,
      "grad_norm": 4.124695301055908,
      "learning_rate": 4.700479962041025e-05,
      "loss": 0.7183,
      "step": 393900
    },
    {
      "epoch": 3.5951529308708663,
      "grad_norm": 3.6553375720977783,
      "learning_rate": 4.700403922427428e-05,
      "loss": 0.7305,
      "step": 394000
    },
    {
      "epoch": 3.5960654062340316,
      "grad_norm": 3.9111270904541016,
      "learning_rate": 4.700327882813831e-05,
      "loss": 0.7716,
      "step": 394100
    },
    {
      "epoch": 3.596977881597197,
      "grad_norm": 3.3893654346466064,
      "learning_rate": 4.700251843200234e-05,
      "loss": 0.7732,
      "step": 394200
    },
    {
      "epoch": 3.597890356960362,
      "grad_norm": 4.213991641998291,
      "learning_rate": 4.700175803586637e-05,
      "loss": 0.7583,
      "step": 394300
    },
    {
      "epoch": 3.5988028323235275,
      "grad_norm": 4.370295524597168,
      "learning_rate": 4.70009976397304e-05,
      "loss": 0.7223,
      "step": 394400
    },
    {
      "epoch": 3.5997153076866923,
      "grad_norm": 4.001687049865723,
      "learning_rate": 4.700023724359442e-05,
      "loss": 0.7289,
      "step": 394500
    },
    {
      "epoch": 3.6006277830498576,
      "grad_norm": 3.085576295852661,
      "learning_rate": 4.699947684745846e-05,
      "loss": 0.7373,
      "step": 394600
    },
    {
      "epoch": 3.601540258413023,
      "grad_norm": 4.233882427215576,
      "learning_rate": 4.699871645132248e-05,
      "loss": 0.7215,
      "step": 394700
    },
    {
      "epoch": 3.602452733776188,
      "grad_norm": 3.8944830894470215,
      "learning_rate": 4.6997956055186506e-05,
      "loss": 0.7182,
      "step": 394800
    },
    {
      "epoch": 3.603365209139353,
      "grad_norm": 4.3554534912109375,
      "learning_rate": 4.699719565905054e-05,
      "loss": 0.7611,
      "step": 394900
    },
    {
      "epoch": 3.6042776845025184,
      "grad_norm": 4.083145618438721,
      "learning_rate": 4.699643526291457e-05,
      "loss": 0.7552,
      "step": 395000
    },
    {
      "epoch": 3.6051901598656837,
      "grad_norm": 4.423666477203369,
      "learning_rate": 4.69956748667786e-05,
      "loss": 0.7309,
      "step": 395100
    },
    {
      "epoch": 3.606102635228849,
      "grad_norm": 3.600919723510742,
      "learning_rate": 4.699491447064263e-05,
      "loss": 0.7547,
      "step": 395200
    },
    {
      "epoch": 3.607015110592014,
      "grad_norm": 3.972177267074585,
      "learning_rate": 4.699415407450666e-05,
      "loss": 0.6923,
      "step": 395300
    },
    {
      "epoch": 3.607927585955179,
      "grad_norm": 4.105869770050049,
      "learning_rate": 4.699339367837069e-05,
      "loss": 0.7383,
      "step": 395400
    },
    {
      "epoch": 3.6088400613183444,
      "grad_norm": 3.3853163719177246,
      "learning_rate": 4.699263328223472e-05,
      "loss": 0.7278,
      "step": 395500
    },
    {
      "epoch": 3.6097525366815097,
      "grad_norm": 4.644166469573975,
      "learning_rate": 4.699187288609874e-05,
      "loss": 0.6937,
      "step": 395600
    },
    {
      "epoch": 3.6106650120446746,
      "grad_norm": 3.285562038421631,
      "learning_rate": 4.699111248996278e-05,
      "loss": 0.7218,
      "step": 395700
    },
    {
      "epoch": 3.61157748740784,
      "grad_norm": 4.2214860916137695,
      "learning_rate": 4.69903520938268e-05,
      "loss": 0.7102,
      "step": 395800
    },
    {
      "epoch": 3.612489962771005,
      "grad_norm": 4.777935981750488,
      "learning_rate": 4.698959169769083e-05,
      "loss": 0.7401,
      "step": 395900
    },
    {
      "epoch": 3.6134024381341705,
      "grad_norm": 3.5511348247528076,
      "learning_rate": 4.698883130155486e-05,
      "loss": 0.7609,
      "step": 396000
    },
    {
      "epoch": 3.614314913497336,
      "grad_norm": 3.3377997875213623,
      "learning_rate": 4.698807090541889e-05,
      "loss": 0.6913,
      "step": 396100
    },
    {
      "epoch": 3.6152273888605007,
      "grad_norm": 4.010991096496582,
      "learning_rate": 4.6987310509282914e-05,
      "loss": 0.7155,
      "step": 396200
    },
    {
      "epoch": 3.616139864223666,
      "grad_norm": 4.217403411865234,
      "learning_rate": 4.698655011314695e-05,
      "loss": 0.7419,
      "step": 396300
    },
    {
      "epoch": 3.6170523395868313,
      "grad_norm": 3.675445556640625,
      "learning_rate": 4.6985789717010974e-05,
      "loss": 0.726,
      "step": 396400
    },
    {
      "epoch": 3.617964814949996,
      "grad_norm": 3.675433874130249,
      "learning_rate": 4.6985029320875004e-05,
      "loss": 0.7599,
      "step": 396500
    },
    {
      "epoch": 3.6188772903131614,
      "grad_norm": 4.990625381469727,
      "learning_rate": 4.6984268924739034e-05,
      "loss": 0.7163,
      "step": 396600
    },
    {
      "epoch": 3.6197897656763267,
      "grad_norm": 4.489563941955566,
      "learning_rate": 4.6983508528603064e-05,
      "loss": 0.7474,
      "step": 396700
    },
    {
      "epoch": 3.620702241039492,
      "grad_norm": 2.6821181774139404,
      "learning_rate": 4.6982748132467094e-05,
      "loss": 0.716,
      "step": 396800
    },
    {
      "epoch": 3.6216147164026573,
      "grad_norm": 3.404642105102539,
      "learning_rate": 4.6981987736331124e-05,
      "loss": 0.7115,
      "step": 396900
    },
    {
      "epoch": 3.622527191765822,
      "grad_norm": 4.855235576629639,
      "learning_rate": 4.698122734019515e-05,
      "loss": 0.6964,
      "step": 397000
    },
    {
      "epoch": 3.6234396671289875,
      "grad_norm": 4.301671981811523,
      "learning_rate": 4.6980466944059184e-05,
      "loss": 0.7408,
      "step": 397100
    },
    {
      "epoch": 3.6243521424921528,
      "grad_norm": 4.034816741943359,
      "learning_rate": 4.697970654792321e-05,
      "loss": 0.7034,
      "step": 397200
    },
    {
      "epoch": 3.625264617855318,
      "grad_norm": 3.9431703090667725,
      "learning_rate": 4.697894615178724e-05,
      "loss": 0.7445,
      "step": 397300
    },
    {
      "epoch": 3.626177093218483,
      "grad_norm": 3.8853657245635986,
      "learning_rate": 4.697818575565127e-05,
      "loss": 0.7681,
      "step": 397400
    },
    {
      "epoch": 3.627089568581648,
      "grad_norm": 4.001195430755615,
      "learning_rate": 4.69774253595153e-05,
      "loss": 0.7695,
      "step": 397500
    },
    {
      "epoch": 3.6280020439448135,
      "grad_norm": 4.202527046203613,
      "learning_rate": 4.697666496337932e-05,
      "loss": 0.6952,
      "step": 397600
    },
    {
      "epoch": 3.628914519307979,
      "grad_norm": 5.630483150482178,
      "learning_rate": 4.697590456724335e-05,
      "loss": 0.7563,
      "step": 397700
    },
    {
      "epoch": 3.629826994671144,
      "grad_norm": 3.8677585124969482,
      "learning_rate": 4.697514417110738e-05,
      "loss": 0.7165,
      "step": 397800
    },
    {
      "epoch": 3.630739470034309,
      "grad_norm": 3.119441509246826,
      "learning_rate": 4.697438377497141e-05,
      "loss": 0.7289,
      "step": 397900
    },
    {
      "epoch": 3.6316519453974743,
      "grad_norm": 4.957103252410889,
      "learning_rate": 4.697362337883544e-05,
      "loss": 0.7603,
      "step": 398000
    },
    {
      "epoch": 3.6325644207606396,
      "grad_norm": 4.035820007324219,
      "learning_rate": 4.6972862982699465e-05,
      "loss": 0.7028,
      "step": 398100
    },
    {
      "epoch": 3.6334768961238044,
      "grad_norm": 3.591590404510498,
      "learning_rate": 4.69721025865635e-05,
      "loss": 0.7334,
      "step": 398200
    },
    {
      "epoch": 3.6343893714869697,
      "grad_norm": 4.596391677856445,
      "learning_rate": 4.6971342190427525e-05,
      "loss": 0.7173,
      "step": 398300
    },
    {
      "epoch": 3.635301846850135,
      "grad_norm": 3.928253412246704,
      "learning_rate": 4.6970581794291555e-05,
      "loss": 0.7089,
      "step": 398400
    },
    {
      "epoch": 3.6362143222133003,
      "grad_norm": 5.124921798706055,
      "learning_rate": 4.6969821398155585e-05,
      "loss": 0.717,
      "step": 398500
    },
    {
      "epoch": 3.6371267975764656,
      "grad_norm": 4.445394992828369,
      "learning_rate": 4.6969061002019615e-05,
      "loss": 0.7069,
      "step": 398600
    },
    {
      "epoch": 3.6380392729396305,
      "grad_norm": 3.879671573638916,
      "learning_rate": 4.696830060588364e-05,
      "loss": 0.7163,
      "step": 398700
    },
    {
      "epoch": 3.6389517483027958,
      "grad_norm": 4.278584003448486,
      "learning_rate": 4.6967540209747675e-05,
      "loss": 0.7094,
      "step": 398800
    },
    {
      "epoch": 3.639864223665961,
      "grad_norm": 4.532950401306152,
      "learning_rate": 4.69667798136117e-05,
      "loss": 0.7063,
      "step": 398900
    },
    {
      "epoch": 3.6407766990291264,
      "grad_norm": 4.041388511657715,
      "learning_rate": 4.696601941747573e-05,
      "loss": 0.7338,
      "step": 399000
    },
    {
      "epoch": 3.6416891743922912,
      "grad_norm": 5.109185695648193,
      "learning_rate": 4.696525902133976e-05,
      "loss": 0.7228,
      "step": 399100
    },
    {
      "epoch": 3.6426016497554565,
      "grad_norm": 3.2310128211975098,
      "learning_rate": 4.696449862520379e-05,
      "loss": 0.7864,
      "step": 399200
    },
    {
      "epoch": 3.643514125118622,
      "grad_norm": 3.8082070350646973,
      "learning_rate": 4.696373822906782e-05,
      "loss": 0.7607,
      "step": 399300
    },
    {
      "epoch": 3.644426600481787,
      "grad_norm": 4.446349620819092,
      "learning_rate": 4.696297783293185e-05,
      "loss": 0.7176,
      "step": 399400
    },
    {
      "epoch": 3.6453390758449524,
      "grad_norm": 4.783885955810547,
      "learning_rate": 4.696221743679587e-05,
      "loss": 0.7188,
      "step": 399500
    },
    {
      "epoch": 3.6462515512081173,
      "grad_norm": 4.482202529907227,
      "learning_rate": 4.696145704065991e-05,
      "loss": 0.7465,
      "step": 399600
    },
    {
      "epoch": 3.6471640265712826,
      "grad_norm": 3.8242440223693848,
      "learning_rate": 4.696069664452393e-05,
      "loss": 0.6997,
      "step": 399700
    },
    {
      "epoch": 3.648076501934448,
      "grad_norm": 4.885406017303467,
      "learning_rate": 4.695993624838796e-05,
      "loss": 0.7229,
      "step": 399800
    },
    {
      "epoch": 3.6489889772976127,
      "grad_norm": 5.178364276885986,
      "learning_rate": 4.695917585225199e-05,
      "loss": 0.7367,
      "step": 399900
    },
    {
      "epoch": 3.649901452660778,
      "grad_norm": 2.9231019020080566,
      "learning_rate": 4.695841545611602e-05,
      "loss": 0.7118,
      "step": 400000
    },
    {
      "epoch": 3.6508139280239433,
      "grad_norm": 3.8910326957702637,
      "learning_rate": 4.6957655059980046e-05,
      "loss": 0.725,
      "step": 400100
    },
    {
      "epoch": 3.6517264033871086,
      "grad_norm": 4.1202392578125,
      "learning_rate": 4.695689466384408e-05,
      "loss": 0.7147,
      "step": 400200
    },
    {
      "epoch": 3.652638878750274,
      "grad_norm": 4.764592170715332,
      "learning_rate": 4.6956134267708106e-05,
      "loss": 0.7411,
      "step": 400300
    },
    {
      "epoch": 3.653551354113439,
      "grad_norm": 2.7939138412475586,
      "learning_rate": 4.6955373871572136e-05,
      "loss": 0.7501,
      "step": 400400
    },
    {
      "epoch": 3.654463829476604,
      "grad_norm": 2.602759838104248,
      "learning_rate": 4.6954613475436166e-05,
      "loss": 0.7704,
      "step": 400500
    },
    {
      "epoch": 3.6553763048397694,
      "grad_norm": 3.8708977699279785,
      "learning_rate": 4.695385307930019e-05,
      "loss": 0.747,
      "step": 400600
    },
    {
      "epoch": 3.6562887802029347,
      "grad_norm": 4.2401509284973145,
      "learning_rate": 4.6953092683164226e-05,
      "loss": 0.7143,
      "step": 400700
    },
    {
      "epoch": 3.6572012555660995,
      "grad_norm": 4.394524097442627,
      "learning_rate": 4.695233228702825e-05,
      "loss": 0.6774,
      "step": 400800
    },
    {
      "epoch": 3.658113730929265,
      "grad_norm": 3.9937567710876465,
      "learning_rate": 4.695157189089228e-05,
      "loss": 0.7499,
      "step": 400900
    },
    {
      "epoch": 3.65902620629243,
      "grad_norm": 4.174592971801758,
      "learning_rate": 4.695081149475631e-05,
      "loss": 0.7635,
      "step": 401000
    },
    {
      "epoch": 3.6599386816555954,
      "grad_norm": 4.760500431060791,
      "learning_rate": 4.695005109862034e-05,
      "loss": 0.721,
      "step": 401100
    },
    {
      "epoch": 3.6608511570187607,
      "grad_norm": 3.8649606704711914,
      "learning_rate": 4.694929070248437e-05,
      "loss": 0.7151,
      "step": 401200
    },
    {
      "epoch": 3.6617636323819256,
      "grad_norm": 4.270477771759033,
      "learning_rate": 4.69485303063484e-05,
      "loss": 0.7062,
      "step": 401300
    },
    {
      "epoch": 3.662676107745091,
      "grad_norm": 5.216833591461182,
      "learning_rate": 4.694776991021242e-05,
      "loss": 0.7671,
      "step": 401400
    },
    {
      "epoch": 3.663588583108256,
      "grad_norm": 4.700079917907715,
      "learning_rate": 4.694700951407645e-05,
      "loss": 0.7327,
      "step": 401500
    },
    {
      "epoch": 3.664501058471421,
      "grad_norm": 3.3872745037078857,
      "learning_rate": 4.694624911794048e-05,
      "loss": 0.7161,
      "step": 401600
    },
    {
      "epoch": 3.6654135338345863,
      "grad_norm": 4.16563081741333,
      "learning_rate": 4.694548872180451e-05,
      "loss": 0.7547,
      "step": 401700
    },
    {
      "epoch": 3.6663260091977516,
      "grad_norm": 4.51012659072876,
      "learning_rate": 4.6944728325668543e-05,
      "loss": 0.7071,
      "step": 401800
    },
    {
      "epoch": 3.667238484560917,
      "grad_norm": 4.117525100708008,
      "learning_rate": 4.6943967929532573e-05,
      "loss": 0.6775,
      "step": 401900
    },
    {
      "epoch": 3.6681509599240822,
      "grad_norm": 3.9908406734466553,
      "learning_rate": 4.69432075333966e-05,
      "loss": 0.7431,
      "step": 402000
    },
    {
      "epoch": 3.669063435287247,
      "grad_norm": 4.938055992126465,
      "learning_rate": 4.6942447137260634e-05,
      "loss": 0.7192,
      "step": 402100
    },
    {
      "epoch": 3.6699759106504124,
      "grad_norm": 3.740335464477539,
      "learning_rate": 4.694168674112466e-05,
      "loss": 0.7155,
      "step": 402200
    },
    {
      "epoch": 3.6708883860135777,
      "grad_norm": 4.737590312957764,
      "learning_rate": 4.694092634498869e-05,
      "loss": 0.7317,
      "step": 402300
    },
    {
      "epoch": 3.6718008613767426,
      "grad_norm": 4.243446350097656,
      "learning_rate": 4.694016594885272e-05,
      "loss": 0.7011,
      "step": 402400
    },
    {
      "epoch": 3.672713336739908,
      "grad_norm": 5.541990280151367,
      "learning_rate": 4.693940555271675e-05,
      "loss": 0.7271,
      "step": 402500
    },
    {
      "epoch": 3.673625812103073,
      "grad_norm": 4.3605055809021,
      "learning_rate": 4.693864515658078e-05,
      "loss": 0.728,
      "step": 402600
    },
    {
      "epoch": 3.6745382874662385,
      "grad_norm": 3.1357898712158203,
      "learning_rate": 4.693788476044481e-05,
      "loss": 0.7027,
      "step": 402700
    },
    {
      "epoch": 3.6754507628294038,
      "grad_norm": 4.649283409118652,
      "learning_rate": 4.693712436430883e-05,
      "loss": 0.7331,
      "step": 402800
    },
    {
      "epoch": 3.676363238192569,
      "grad_norm": 3.2287940979003906,
      "learning_rate": 4.693636396817286e-05,
      "loss": 0.7335,
      "step": 402900
    },
    {
      "epoch": 3.677275713555734,
      "grad_norm": 4.06889533996582,
      "learning_rate": 4.693560357203689e-05,
      "loss": 0.6883,
      "step": 403000
    },
    {
      "epoch": 3.678188188918899,
      "grad_norm": 3.7651023864746094,
      "learning_rate": 4.693484317590092e-05,
      "loss": 0.7463,
      "step": 403100
    },
    {
      "epoch": 3.6791006642820645,
      "grad_norm": 4.576114177703857,
      "learning_rate": 4.693408277976495e-05,
      "loss": 0.732,
      "step": 403200
    },
    {
      "epoch": 3.6800131396452294,
      "grad_norm": 4.115462303161621,
      "learning_rate": 4.693332238362898e-05,
      "loss": 0.7342,
      "step": 403300
    },
    {
      "epoch": 3.6809256150083947,
      "grad_norm": 3.7766942977905273,
      "learning_rate": 4.6932561987493004e-05,
      "loss": 0.7439,
      "step": 403400
    },
    {
      "epoch": 3.68183809037156,
      "grad_norm": 4.3301286697387695,
      "learning_rate": 4.6931801591357034e-05,
      "loss": 0.7503,
      "step": 403500
    },
    {
      "epoch": 3.6827505657347253,
      "grad_norm": 3.9166173934936523,
      "learning_rate": 4.6931041195221064e-05,
      "loss": 0.8149,
      "step": 403600
    },
    {
      "epoch": 3.6836630410978906,
      "grad_norm": 4.298655033111572,
      "learning_rate": 4.6930280799085094e-05,
      "loss": 0.7314,
      "step": 403700
    },
    {
      "epoch": 3.6845755164610554,
      "grad_norm": 4.740556240081787,
      "learning_rate": 4.6929520402949124e-05,
      "loss": 0.7411,
      "step": 403800
    },
    {
      "epoch": 3.6854879918242207,
      "grad_norm": 4.141775608062744,
      "learning_rate": 4.692876000681315e-05,
      "loss": 0.7622,
      "step": 403900
    },
    {
      "epoch": 3.686400467187386,
      "grad_norm": 3.9710123538970947,
      "learning_rate": 4.6927999610677185e-05,
      "loss": 0.7513,
      "step": 404000
    },
    {
      "epoch": 3.687312942550551,
      "grad_norm": 4.326180934906006,
      "learning_rate": 4.692723921454121e-05,
      "loss": 0.7307,
      "step": 404100
    },
    {
      "epoch": 3.688225417913716,
      "grad_norm": 3.9501233100891113,
      "learning_rate": 4.692647881840524e-05,
      "loss": 0.7328,
      "step": 404200
    },
    {
      "epoch": 3.6891378932768815,
      "grad_norm": 3.191283702850342,
      "learning_rate": 4.692571842226927e-05,
      "loss": 0.7365,
      "step": 404300
    },
    {
      "epoch": 3.6900503686400468,
      "grad_norm": 4.322779655456543,
      "learning_rate": 4.69249580261333e-05,
      "loss": 0.7182,
      "step": 404400
    },
    {
      "epoch": 3.690962844003212,
      "grad_norm": 4.266713619232178,
      "learning_rate": 4.692419762999732e-05,
      "loss": 0.7494,
      "step": 404500
    },
    {
      "epoch": 3.6918753193663774,
      "grad_norm": 4.305802345275879,
      "learning_rate": 4.692343723386136e-05,
      "loss": 0.7533,
      "step": 404600
    },
    {
      "epoch": 3.692787794729542,
      "grad_norm": 4.277370929718018,
      "learning_rate": 4.692267683772538e-05,
      "loss": 0.7433,
      "step": 404700
    },
    {
      "epoch": 3.6937002700927075,
      "grad_norm": 4.534966468811035,
      "learning_rate": 4.692191644158941e-05,
      "loss": 0.7185,
      "step": 404800
    },
    {
      "epoch": 3.694612745455873,
      "grad_norm": 4.366536617279053,
      "learning_rate": 4.692115604545344e-05,
      "loss": 0.7293,
      "step": 404900
    },
    {
      "epoch": 3.6955252208190377,
      "grad_norm": 5.121294021606445,
      "learning_rate": 4.692039564931747e-05,
      "loss": 0.7397,
      "step": 405000
    },
    {
      "epoch": 3.696437696182203,
      "grad_norm": 4.173304557800293,
      "learning_rate": 4.69196352531815e-05,
      "loss": 0.7152,
      "step": 405100
    },
    {
      "epoch": 3.6973501715453683,
      "grad_norm": 4.280207633972168,
      "learning_rate": 4.691887485704553e-05,
      "loss": 0.7131,
      "step": 405200
    },
    {
      "epoch": 3.6982626469085336,
      "grad_norm": 4.150906085968018,
      "learning_rate": 4.6918114460909555e-05,
      "loss": 0.6801,
      "step": 405300
    },
    {
      "epoch": 3.699175122271699,
      "grad_norm": 3.54752779006958,
      "learning_rate": 4.691735406477359e-05,
      "loss": 0.7287,
      "step": 405400
    },
    {
      "epoch": 3.7000875976348637,
      "grad_norm": 4.6575822830200195,
      "learning_rate": 4.6916593668637615e-05,
      "loss": 0.701,
      "step": 405500
    },
    {
      "epoch": 3.701000072998029,
      "grad_norm": 3.829625129699707,
      "learning_rate": 4.6915833272501645e-05,
      "loss": 0.6966,
      "step": 405600
    },
    {
      "epoch": 3.7019125483611943,
      "grad_norm": 4.263952255249023,
      "learning_rate": 4.6915072876365675e-05,
      "loss": 0.7804,
      "step": 405700
    },
    {
      "epoch": 3.702825023724359,
      "grad_norm": 4.568957805633545,
      "learning_rate": 4.6914312480229705e-05,
      "loss": 0.6955,
      "step": 405800
    },
    {
      "epoch": 3.7037374990875245,
      "grad_norm": 4.375329494476318,
      "learning_rate": 4.691355208409373e-05,
      "loss": 0.7177,
      "step": 405900
    },
    {
      "epoch": 3.7046499744506898,
      "grad_norm": 3.953568458557129,
      "learning_rate": 4.6912791687957766e-05,
      "loss": 0.7299,
      "step": 406000
    },
    {
      "epoch": 3.705562449813855,
      "grad_norm": 4.762763500213623,
      "learning_rate": 4.691203129182179e-05,
      "loss": 0.7205,
      "step": 406100
    },
    {
      "epoch": 3.7064749251770204,
      "grad_norm": 4.047440052032471,
      "learning_rate": 4.691127089568582e-05,
      "loss": 0.7283,
      "step": 406200
    },
    {
      "epoch": 3.7073874005401857,
      "grad_norm": 3.9221413135528564,
      "learning_rate": 4.691051049954985e-05,
      "loss": 0.7373,
      "step": 406300
    },
    {
      "epoch": 3.7082998759033505,
      "grad_norm": 4.766820430755615,
      "learning_rate": 4.690975010341387e-05,
      "loss": 0.7195,
      "step": 406400
    },
    {
      "epoch": 3.709212351266516,
      "grad_norm": 4.0204010009765625,
      "learning_rate": 4.690898970727791e-05,
      "loss": 0.7342,
      "step": 406500
    },
    {
      "epoch": 3.710124826629681,
      "grad_norm": 4.392451763153076,
      "learning_rate": 4.690822931114193e-05,
      "loss": 0.7036,
      "step": 406600
    },
    {
      "epoch": 3.711037301992846,
      "grad_norm": 4.03141450881958,
      "learning_rate": 4.690746891500596e-05,
      "loss": 0.7389,
      "step": 406700
    },
    {
      "epoch": 3.7119497773560113,
      "grad_norm": 2.522848606109619,
      "learning_rate": 4.690670851886999e-05,
      "loss": 0.7169,
      "step": 406800
    },
    {
      "epoch": 3.7128622527191766,
      "grad_norm": 3.6669018268585205,
      "learning_rate": 4.690594812273402e-05,
      "loss": 0.7323,
      "step": 406900
    },
    {
      "epoch": 3.713774728082342,
      "grad_norm": 4.4167304039001465,
      "learning_rate": 4.6905187726598046e-05,
      "loss": 0.7244,
      "step": 407000
    },
    {
      "epoch": 3.714687203445507,
      "grad_norm": 2.9239892959594727,
      "learning_rate": 4.690442733046208e-05,
      "loss": 0.7456,
      "step": 407100
    },
    {
      "epoch": 3.715599678808672,
      "grad_norm": 3.7404255867004395,
      "learning_rate": 4.6903666934326106e-05,
      "loss": 0.7332,
      "step": 407200
    },
    {
      "epoch": 3.7165121541718373,
      "grad_norm": 3.019045352935791,
      "learning_rate": 4.6902906538190136e-05,
      "loss": 0.6795,
      "step": 407300
    },
    {
      "epoch": 3.7174246295350026,
      "grad_norm": 3.512378215789795,
      "learning_rate": 4.6902146142054166e-05,
      "loss": 0.7277,
      "step": 407400
    },
    {
      "epoch": 3.7183371048981675,
      "grad_norm": 4.565727710723877,
      "learning_rate": 4.6901385745918196e-05,
      "loss": 0.7697,
      "step": 407500
    },
    {
      "epoch": 3.719249580261333,
      "grad_norm": 4.012308120727539,
      "learning_rate": 4.6900625349782226e-05,
      "loss": 0.6853,
      "step": 407600
    },
    {
      "epoch": 3.720162055624498,
      "grad_norm": 4.768912315368652,
      "learning_rate": 4.6899864953646256e-05,
      "loss": 0.7602,
      "step": 407700
    },
    {
      "epoch": 3.7210745309876634,
      "grad_norm": 4.485682964324951,
      "learning_rate": 4.689910455751028e-05,
      "loss": 0.8002,
      "step": 407800
    },
    {
      "epoch": 3.7219870063508287,
      "grad_norm": 3.315446138381958,
      "learning_rate": 4.6898344161374317e-05,
      "loss": 0.7155,
      "step": 407900
    },
    {
      "epoch": 3.722899481713994,
      "grad_norm": 5.435439586639404,
      "learning_rate": 4.689758376523834e-05,
      "loss": 0.7639,
      "step": 408000
    },
    {
      "epoch": 3.723811957077159,
      "grad_norm": 4.592061519622803,
      "learning_rate": 4.689682336910237e-05,
      "loss": 0.744,
      "step": 408100
    },
    {
      "epoch": 3.724724432440324,
      "grad_norm": 4.673191547393799,
      "learning_rate": 4.68960629729664e-05,
      "loss": 0.6824,
      "step": 408200
    },
    {
      "epoch": 3.7256369078034894,
      "grad_norm": 3.771062135696411,
      "learning_rate": 4.689530257683043e-05,
      "loss": 0.7172,
      "step": 408300
    },
    {
      "epoch": 3.7265493831666543,
      "grad_norm": 3.3640472888946533,
      "learning_rate": 4.689454218069445e-05,
      "loss": 0.7096,
      "step": 408400
    },
    {
      "epoch": 3.7274618585298196,
      "grad_norm": 3.8883488178253174,
      "learning_rate": 4.689378178455849e-05,
      "loss": 0.7524,
      "step": 408500
    },
    {
      "epoch": 3.728374333892985,
      "grad_norm": 4.429993629455566,
      "learning_rate": 4.6893021388422513e-05,
      "loss": 0.7726,
      "step": 408600
    },
    {
      "epoch": 3.72928680925615,
      "grad_norm": 3.763939380645752,
      "learning_rate": 4.6892260992286544e-05,
      "loss": 0.711,
      "step": 408700
    },
    {
      "epoch": 3.7301992846193155,
      "grad_norm": 3.913940906524658,
      "learning_rate": 4.6891500596150574e-05,
      "loss": 0.7957,
      "step": 408800
    },
    {
      "epoch": 3.7311117599824803,
      "grad_norm": 4.642518520355225,
      "learning_rate": 4.6890740200014604e-05,
      "loss": 0.7323,
      "step": 408900
    },
    {
      "epoch": 3.7320242353456456,
      "grad_norm": 5.013368129730225,
      "learning_rate": 4.6889979803878634e-05,
      "loss": 0.7336,
      "step": 409000
    },
    {
      "epoch": 3.732936710708811,
      "grad_norm": 3.9440016746520996,
      "learning_rate": 4.688921940774266e-05,
      "loss": 0.7294,
      "step": 409100
    },
    {
      "epoch": 3.733849186071976,
      "grad_norm": 3.8278391361236572,
      "learning_rate": 4.688845901160669e-05,
      "loss": 0.7291,
      "step": 409200
    },
    {
      "epoch": 3.734761661435141,
      "grad_norm": 4.628815650939941,
      "learning_rate": 4.688769861547072e-05,
      "loss": 0.7195,
      "step": 409300
    },
    {
      "epoch": 3.7356741367983064,
      "grad_norm": 4.491879463195801,
      "learning_rate": 4.688693821933475e-05,
      "loss": 0.7282,
      "step": 409400
    },
    {
      "epoch": 3.7365866121614717,
      "grad_norm": 3.741939067840576,
      "learning_rate": 4.688617782319877e-05,
      "loss": 0.7098,
      "step": 409500
    },
    {
      "epoch": 3.737499087524637,
      "grad_norm": 4.025262355804443,
      "learning_rate": 4.688541742706281e-05,
      "loss": 0.7324,
      "step": 409600
    },
    {
      "epoch": 3.7384115628878023,
      "grad_norm": 3.1719918251037598,
      "learning_rate": 4.688465703092683e-05,
      "loss": 0.7287,
      "step": 409700
    },
    {
      "epoch": 3.739324038250967,
      "grad_norm": 4.428885459899902,
      "learning_rate": 4.688389663479086e-05,
      "loss": 0.73,
      "step": 409800
    },
    {
      "epoch": 3.7402365136141325,
      "grad_norm": 4.436741352081299,
      "learning_rate": 4.688313623865489e-05,
      "loss": 0.732,
      "step": 409900
    },
    {
      "epoch": 3.7411489889772978,
      "grad_norm": 4.354902267456055,
      "learning_rate": 4.688237584251892e-05,
      "loss": 0.73,
      "step": 410000
    },
    {
      "epoch": 3.7420614643404626,
      "grad_norm": 4.212881088256836,
      "learning_rate": 4.688161544638295e-05,
      "loss": 0.6958,
      "step": 410100
    },
    {
      "epoch": 3.742973939703628,
      "grad_norm": 4.124899387359619,
      "learning_rate": 4.688085505024698e-05,
      "loss": 0.6778,
      "step": 410200
    },
    {
      "epoch": 3.743886415066793,
      "grad_norm": 5.337307453155518,
      "learning_rate": 4.6880094654111004e-05,
      "loss": 0.7177,
      "step": 410300
    },
    {
      "epoch": 3.7447988904299585,
      "grad_norm": 3.842911720275879,
      "learning_rate": 4.687933425797504e-05,
      "loss": 0.7236,
      "step": 410400
    },
    {
      "epoch": 3.745711365793124,
      "grad_norm": 3.8623597621917725,
      "learning_rate": 4.6878573861839064e-05,
      "loss": 0.7666,
      "step": 410500
    },
    {
      "epoch": 3.7466238411562887,
      "grad_norm": 3.567962884902954,
      "learning_rate": 4.6877813465703094e-05,
      "loss": 0.7659,
      "step": 410600
    },
    {
      "epoch": 3.747536316519454,
      "grad_norm": 5.177762031555176,
      "learning_rate": 4.6877053069567125e-05,
      "loss": 0.7136,
      "step": 410700
    },
    {
      "epoch": 3.7484487918826193,
      "grad_norm": 4.00266695022583,
      "learning_rate": 4.6876292673431155e-05,
      "loss": 0.7231,
      "step": 410800
    },
    {
      "epoch": 3.749361267245784,
      "grad_norm": 4.449497699737549,
      "learning_rate": 4.687553227729518e-05,
      "loss": 0.7385,
      "step": 410900
    },
    {
      "epoch": 3.7502737426089494,
      "grad_norm": 4.865801811218262,
      "learning_rate": 4.6874771881159215e-05,
      "loss": 0.692,
      "step": 411000
    },
    {
      "epoch": 3.7511862179721147,
      "grad_norm": 4.736351013183594,
      "learning_rate": 4.687401148502324e-05,
      "loss": 0.7239,
      "step": 411100
    },
    {
      "epoch": 3.75209869333528,
      "grad_norm": 4.471449375152588,
      "learning_rate": 4.687325108888727e-05,
      "loss": 0.7105,
      "step": 411200
    },
    {
      "epoch": 3.7530111686984453,
      "grad_norm": 4.3657546043396,
      "learning_rate": 4.68724906927513e-05,
      "loss": 0.6718,
      "step": 411300
    },
    {
      "epoch": 3.7539236440616106,
      "grad_norm": 4.693267822265625,
      "learning_rate": 4.687173029661533e-05,
      "loss": 0.6909,
      "step": 411400
    },
    {
      "epoch": 3.7548361194247755,
      "grad_norm": 4.75685977935791,
      "learning_rate": 4.687096990047936e-05,
      "loss": 0.7549,
      "step": 411500
    },
    {
      "epoch": 3.7557485947879408,
      "grad_norm": 3.4522595405578613,
      "learning_rate": 4.687020950434339e-05,
      "loss": 0.7288,
      "step": 411600
    },
    {
      "epoch": 3.756661070151106,
      "grad_norm": 4.459201812744141,
      "learning_rate": 4.686944910820741e-05,
      "loss": 0.7198,
      "step": 411700
    },
    {
      "epoch": 3.757573545514271,
      "grad_norm": 2.9051060676574707,
      "learning_rate": 4.686868871207145e-05,
      "loss": 0.7411,
      "step": 411800
    },
    {
      "epoch": 3.758486020877436,
      "grad_norm": 3.9975602626800537,
      "learning_rate": 4.686792831593547e-05,
      "loss": 0.7541,
      "step": 411900
    },
    {
      "epoch": 3.7593984962406015,
      "grad_norm": 4.682058334350586,
      "learning_rate": 4.6867167919799495e-05,
      "loss": 0.7565,
      "step": 412000
    },
    {
      "epoch": 3.760310971603767,
      "grad_norm": 4.750612735748291,
      "learning_rate": 4.686640752366353e-05,
      "loss": 0.7049,
      "step": 412100
    },
    {
      "epoch": 3.761223446966932,
      "grad_norm": 3.556798219680786,
      "learning_rate": 4.6865647127527555e-05,
      "loss": 0.7662,
      "step": 412200
    },
    {
      "epoch": 3.762135922330097,
      "grad_norm": 4.467051029205322,
      "learning_rate": 4.6864886731391585e-05,
      "loss": 0.7621,
      "step": 412300
    },
    {
      "epoch": 3.7630483976932623,
      "grad_norm": 3.3815722465515137,
      "learning_rate": 4.6864126335255615e-05,
      "loss": 0.7088,
      "step": 412400
    },
    {
      "epoch": 3.7639608730564276,
      "grad_norm": 3.584879159927368,
      "learning_rate": 4.6863365939119645e-05,
      "loss": 0.7667,
      "step": 412500
    },
    {
      "epoch": 3.7648733484195924,
      "grad_norm": 3.3691046237945557,
      "learning_rate": 4.6862605542983675e-05,
      "loss": 0.7345,
      "step": 412600
    },
    {
      "epoch": 3.7657858237827577,
      "grad_norm": 4.396666526794434,
      "learning_rate": 4.6861845146847706e-05,
      "loss": 0.7075,
      "step": 412700
    },
    {
      "epoch": 3.766698299145923,
      "grad_norm": 4.540374755859375,
      "learning_rate": 4.686108475071173e-05,
      "loss": 0.717,
      "step": 412800
    },
    {
      "epoch": 3.7676107745090883,
      "grad_norm": 3.721231460571289,
      "learning_rate": 4.6860324354575766e-05,
      "loss": 0.7202,
      "step": 412900
    },
    {
      "epoch": 3.7685232498722536,
      "grad_norm": 3.6482133865356445,
      "learning_rate": 4.685956395843979e-05,
      "loss": 0.7864,
      "step": 413000
    },
    {
      "epoch": 3.7694357252354185,
      "grad_norm": 4.468452453613281,
      "learning_rate": 4.685880356230382e-05,
      "loss": 0.7367,
      "step": 413100
    },
    {
      "epoch": 3.7703482005985838,
      "grad_norm": 4.079746246337891,
      "learning_rate": 4.685804316616785e-05,
      "loss": 0.7348,
      "step": 413200
    },
    {
      "epoch": 3.771260675961749,
      "grad_norm": 4.28073787689209,
      "learning_rate": 4.685728277003188e-05,
      "loss": 0.7159,
      "step": 413300
    },
    {
      "epoch": 3.7721731513249144,
      "grad_norm": 3.950845241546631,
      "learning_rate": 4.68565223738959e-05,
      "loss": 0.704,
      "step": 413400
    },
    {
      "epoch": 3.7730856266880792,
      "grad_norm": 4.535122871398926,
      "learning_rate": 4.685576197775994e-05,
      "loss": 0.6707,
      "step": 413500
    },
    {
      "epoch": 3.7739981020512445,
      "grad_norm": 3.948261260986328,
      "learning_rate": 4.685500158162396e-05,
      "loss": 0.7424,
      "step": 413600
    },
    {
      "epoch": 3.77491057741441,
      "grad_norm": 4.484622001647949,
      "learning_rate": 4.685424118548799e-05,
      "loss": 0.715,
      "step": 413700
    },
    {
      "epoch": 3.775823052777575,
      "grad_norm": 4.021106719970703,
      "learning_rate": 4.685348078935202e-05,
      "loss": 0.7377,
      "step": 413800
    },
    {
      "epoch": 3.7767355281407404,
      "grad_norm": 4.3570170402526855,
      "learning_rate": 4.685272039321605e-05,
      "loss": 0.72,
      "step": 413900
    },
    {
      "epoch": 3.7776480035039053,
      "grad_norm": 4.391208648681641,
      "learning_rate": 4.685195999708008e-05,
      "loss": 0.7067,
      "step": 414000
    },
    {
      "epoch": 3.7785604788670706,
      "grad_norm": 4.157642364501953,
      "learning_rate": 4.685119960094411e-05,
      "loss": 0.7371,
      "step": 414100
    },
    {
      "epoch": 3.779472954230236,
      "grad_norm": 4.925450801849365,
      "learning_rate": 4.6850439204808136e-05,
      "loss": 0.6662,
      "step": 414200
    },
    {
      "epoch": 3.7803854295934007,
      "grad_norm": 4.6176557540893555,
      "learning_rate": 4.684967880867217e-05,
      "loss": 0.7637,
      "step": 414300
    },
    {
      "epoch": 3.781297904956566,
      "grad_norm": 4.204861640930176,
      "learning_rate": 4.6848918412536196e-05,
      "loss": 0.7207,
      "step": 414400
    },
    {
      "epoch": 3.7822103803197313,
      "grad_norm": 4.499551296234131,
      "learning_rate": 4.6848158016400226e-05,
      "loss": 0.7237,
      "step": 414500
    },
    {
      "epoch": 3.7831228556828966,
      "grad_norm": 3.2483091354370117,
      "learning_rate": 4.6847397620264257e-05,
      "loss": 0.7419,
      "step": 414600
    },
    {
      "epoch": 3.784035331046062,
      "grad_norm": 4.136466026306152,
      "learning_rate": 4.684663722412828e-05,
      "loss": 0.7127,
      "step": 414700
    },
    {
      "epoch": 3.784947806409227,
      "grad_norm": 4.493800163269043,
      "learning_rate": 4.684587682799232e-05,
      "loss": 0.7144,
      "step": 414800
    },
    {
      "epoch": 3.785860281772392,
      "grad_norm": 4.317306041717529,
      "learning_rate": 4.684511643185634e-05,
      "loss": 0.733,
      "step": 414900
    },
    {
      "epoch": 3.7867727571355574,
      "grad_norm": 4.553438663482666,
      "learning_rate": 4.684435603572037e-05,
      "loss": 0.7081,
      "step": 415000
    },
    {
      "epoch": 3.7876852324987227,
      "grad_norm": 3.9926247596740723,
      "learning_rate": 4.68435956395844e-05,
      "loss": 0.7316,
      "step": 415100
    },
    {
      "epoch": 3.7885977078618875,
      "grad_norm": 3.0002973079681396,
      "learning_rate": 4.684283524344843e-05,
      "loss": 0.7267,
      "step": 415200
    },
    {
      "epoch": 3.789510183225053,
      "grad_norm": 4.138801574707031,
      "learning_rate": 4.6842074847312453e-05,
      "loss": 0.7051,
      "step": 415300
    },
    {
      "epoch": 3.790422658588218,
      "grad_norm": 3.8082001209259033,
      "learning_rate": 4.684131445117649e-05,
      "loss": 0.7253,
      "step": 415400
    },
    {
      "epoch": 3.7913351339513834,
      "grad_norm": 4.112533092498779,
      "learning_rate": 4.6840554055040514e-05,
      "loss": 0.7518,
      "step": 415500
    },
    {
      "epoch": 3.7922476093145487,
      "grad_norm": 4.338278293609619,
      "learning_rate": 4.6839793658904544e-05,
      "loss": 0.7295,
      "step": 415600
    },
    {
      "epoch": 3.7931600846777136,
      "grad_norm": 3.9437415599823,
      "learning_rate": 4.6839033262768574e-05,
      "loss": 0.7432,
      "step": 415700
    },
    {
      "epoch": 3.794072560040879,
      "grad_norm": 4.40259313583374,
      "learning_rate": 4.6838272866632604e-05,
      "loss": 0.7362,
      "step": 415800
    },
    {
      "epoch": 3.794985035404044,
      "grad_norm": 4.146562099456787,
      "learning_rate": 4.6837512470496634e-05,
      "loss": 0.7103,
      "step": 415900
    },
    {
      "epoch": 3.795897510767209,
      "grad_norm": 4.003937721252441,
      "learning_rate": 4.6836752074360664e-05,
      "loss": 0.7746,
      "step": 416000
    },
    {
      "epoch": 3.7968099861303743,
      "grad_norm": 4.345592975616455,
      "learning_rate": 4.683599167822469e-05,
      "loss": 0.7325,
      "step": 416100
    },
    {
      "epoch": 3.7977224614935396,
      "grad_norm": 4.6925811767578125,
      "learning_rate": 4.6835231282088724e-05,
      "loss": 0.7266,
      "step": 416200
    },
    {
      "epoch": 3.798634936856705,
      "grad_norm": 4.042211055755615,
      "learning_rate": 4.683447088595275e-05,
      "loss": 0.7133,
      "step": 416300
    },
    {
      "epoch": 3.7995474122198702,
      "grad_norm": 4.380801200866699,
      "learning_rate": 4.683371048981678e-05,
      "loss": 0.7283,
      "step": 416400
    },
    {
      "epoch": 3.800459887583035,
      "grad_norm": 4.483060836791992,
      "learning_rate": 4.683295009368081e-05,
      "loss": 0.7061,
      "step": 416500
    },
    {
      "epoch": 3.8013723629462004,
      "grad_norm": 3.849975347518921,
      "learning_rate": 4.683218969754484e-05,
      "loss": 0.7192,
      "step": 416600
    },
    {
      "epoch": 3.8022848383093657,
      "grad_norm": 4.806146621704102,
      "learning_rate": 4.683142930140886e-05,
      "loss": 0.7264,
      "step": 416700
    },
    {
      "epoch": 3.803197313672531,
      "grad_norm": 5.206072807312012,
      "learning_rate": 4.68306689052729e-05,
      "loss": 0.7116,
      "step": 416800
    },
    {
      "epoch": 3.804109789035696,
      "grad_norm": 4.742788314819336,
      "learning_rate": 4.682990850913692e-05,
      "loss": 0.6967,
      "step": 416900
    },
    {
      "epoch": 3.805022264398861,
      "grad_norm": 4.006548881530762,
      "learning_rate": 4.682914811300095e-05,
      "loss": 0.703,
      "step": 417000
    },
    {
      "epoch": 3.8059347397620265,
      "grad_norm": 3.9515912532806396,
      "learning_rate": 4.682838771686498e-05,
      "loss": 0.7205,
      "step": 417100
    },
    {
      "epoch": 3.8068472151251918,
      "grad_norm": 4.602258205413818,
      "learning_rate": 4.682762732072901e-05,
      "loss": 0.7286,
      "step": 417200
    },
    {
      "epoch": 3.807759690488357,
      "grad_norm": 4.126893520355225,
      "learning_rate": 4.682686692459304e-05,
      "loss": 0.744,
      "step": 417300
    },
    {
      "epoch": 3.808672165851522,
      "grad_norm": 4.291680812835693,
      "learning_rate": 4.682610652845707e-05,
      "loss": 0.7275,
      "step": 417400
    },
    {
      "epoch": 3.809584641214687,
      "grad_norm": 3.931988477706909,
      "learning_rate": 4.6825346132321095e-05,
      "loss": 0.7281,
      "step": 417500
    },
    {
      "epoch": 3.8104971165778525,
      "grad_norm": 4.574343204498291,
      "learning_rate": 4.6824585736185125e-05,
      "loss": 0.7491,
      "step": 417600
    },
    {
      "epoch": 3.8114095919410174,
      "grad_norm": 2.7337095737457275,
      "learning_rate": 4.6823825340049155e-05,
      "loss": 0.763,
      "step": 417700
    },
    {
      "epoch": 3.8123220673041827,
      "grad_norm": 3.8994784355163574,
      "learning_rate": 4.682306494391318e-05,
      "loss": 0.7231,
      "step": 417800
    },
    {
      "epoch": 3.813234542667348,
      "grad_norm": 4.843016147613525,
      "learning_rate": 4.6822304547777215e-05,
      "loss": 0.7201,
      "step": 417900
    },
    {
      "epoch": 3.8141470180305133,
      "grad_norm": 4.722092151641846,
      "learning_rate": 4.682154415164124e-05,
      "loss": 0.7451,
      "step": 418000
    },
    {
      "epoch": 3.8150594933936786,
      "grad_norm": 3.956960916519165,
      "learning_rate": 4.682078375550527e-05,
      "loss": 0.7145,
      "step": 418100
    },
    {
      "epoch": 3.8159719687568434,
      "grad_norm": 4.573947429656982,
      "learning_rate": 4.68200233593693e-05,
      "loss": 0.7139,
      "step": 418200
    },
    {
      "epoch": 3.8168844441200087,
      "grad_norm": 4.539123058319092,
      "learning_rate": 4.681926296323333e-05,
      "loss": 0.7145,
      "step": 418300
    },
    {
      "epoch": 3.817796919483174,
      "grad_norm": 2.5424857139587402,
      "learning_rate": 4.681850256709736e-05,
      "loss": 0.7587,
      "step": 418400
    },
    {
      "epoch": 3.8187093948463393,
      "grad_norm": 4.101046085357666,
      "learning_rate": 4.681774217096139e-05,
      "loss": 0.7371,
      "step": 418500
    },
    {
      "epoch": 3.819621870209504,
      "grad_norm": 3.0498008728027344,
      "learning_rate": 4.681698177482541e-05,
      "loss": 0.708,
      "step": 418600
    },
    {
      "epoch": 3.8205343455726695,
      "grad_norm": 3.6773815155029297,
      "learning_rate": 4.681622137868945e-05,
      "loss": 0.7119,
      "step": 418700
    },
    {
      "epoch": 3.8214468209358348,
      "grad_norm": 3.964873790740967,
      "learning_rate": 4.681546098255347e-05,
      "loss": 0.6925,
      "step": 418800
    },
    {
      "epoch": 3.822359296299,
      "grad_norm": 4.172467231750488,
      "learning_rate": 4.68147005864175e-05,
      "loss": 0.7519,
      "step": 418900
    },
    {
      "epoch": 3.8232717716621654,
      "grad_norm": 4.439722537994385,
      "learning_rate": 4.681394019028153e-05,
      "loss": 0.7301,
      "step": 419000
    },
    {
      "epoch": 3.82418424702533,
      "grad_norm": 4.719567775726318,
      "learning_rate": 4.681317979414556e-05,
      "loss": 0.7448,
      "step": 419100
    },
    {
      "epoch": 3.8250967223884955,
      "grad_norm": 4.714568614959717,
      "learning_rate": 4.6812419398009585e-05,
      "loss": 0.7132,
      "step": 419200
    },
    {
      "epoch": 3.826009197751661,
      "grad_norm": 3.6258926391601562,
      "learning_rate": 4.681165900187362e-05,
      "loss": 0.7573,
      "step": 419300
    },
    {
      "epoch": 3.8269216731148257,
      "grad_norm": 3.8651039600372314,
      "learning_rate": 4.6810898605737646e-05,
      "loss": 0.7329,
      "step": 419400
    },
    {
      "epoch": 3.827834148477991,
      "grad_norm": 4.121728897094727,
      "learning_rate": 4.6810138209601676e-05,
      "loss": 0.7202,
      "step": 419500
    },
    {
      "epoch": 3.8287466238411563,
      "grad_norm": 3.9647464752197266,
      "learning_rate": 4.6809377813465706e-05,
      "loss": 0.6924,
      "step": 419600
    },
    {
      "epoch": 3.8296590992043216,
      "grad_norm": 3.467784881591797,
      "learning_rate": 4.6808617417329736e-05,
      "loss": 0.6718,
      "step": 419700
    },
    {
      "epoch": 3.830571574567487,
      "grad_norm": 4.126671314239502,
      "learning_rate": 4.6807857021193766e-05,
      "loss": 0.7281,
      "step": 419800
    },
    {
      "epoch": 3.8314840499306517,
      "grad_norm": 4.7419586181640625,
      "learning_rate": 4.6807096625057796e-05,
      "loss": 0.7647,
      "step": 419900
    },
    {
      "epoch": 3.832396525293817,
      "grad_norm": 3.7579972743988037,
      "learning_rate": 4.680633622892182e-05,
      "loss": 0.7546,
      "step": 420000
    },
    {
      "epoch": 3.8333090006569823,
      "grad_norm": 4.352912902832031,
      "learning_rate": 4.6805575832785856e-05,
      "loss": 0.7209,
      "step": 420100
    },
    {
      "epoch": 3.834221476020147,
      "grad_norm": 3.948391914367676,
      "learning_rate": 4.680481543664988e-05,
      "loss": 0.7215,
      "step": 420200
    },
    {
      "epoch": 3.8351339513833125,
      "grad_norm": 4.025669097900391,
      "learning_rate": 4.680405504051391e-05,
      "loss": 0.7157,
      "step": 420300
    },
    {
      "epoch": 3.8360464267464778,
      "grad_norm": 3.444274663925171,
      "learning_rate": 4.680329464437794e-05,
      "loss": 0.7134,
      "step": 420400
    },
    {
      "epoch": 3.836958902109643,
      "grad_norm": 4.608564853668213,
      "learning_rate": 4.680253424824196e-05,
      "loss": 0.7161,
      "step": 420500
    },
    {
      "epoch": 3.8378713774728084,
      "grad_norm": 4.096133708953857,
      "learning_rate": 4.680177385210599e-05,
      "loss": 0.7382,
      "step": 420600
    },
    {
      "epoch": 3.8387838528359737,
      "grad_norm": 4.4805073738098145,
      "learning_rate": 4.680101345597002e-05,
      "loss": 0.7198,
      "step": 420700
    },
    {
      "epoch": 3.8396963281991385,
      "grad_norm": 4.713702201843262,
      "learning_rate": 4.680025305983405e-05,
      "loss": 0.7535,
      "step": 420800
    },
    {
      "epoch": 3.840608803562304,
      "grad_norm": 4.074568748474121,
      "learning_rate": 4.679949266369808e-05,
      "loss": 0.7355,
      "step": 420900
    },
    {
      "epoch": 3.841521278925469,
      "grad_norm": 4.200648307800293,
      "learning_rate": 4.679873226756211e-05,
      "loss": 0.7839,
      "step": 421000
    },
    {
      "epoch": 3.842433754288634,
      "grad_norm": 5.092453479766846,
      "learning_rate": 4.6797971871426136e-05,
      "loss": 0.749,
      "step": 421100
    },
    {
      "epoch": 3.8433462296517993,
      "grad_norm": 4.3971991539001465,
      "learning_rate": 4.679721147529017e-05,
      "loss": 0.7356,
      "step": 421200
    },
    {
      "epoch": 3.8442587050149646,
      "grad_norm": 4.701125621795654,
      "learning_rate": 4.6796451079154196e-05,
      "loss": 0.7443,
      "step": 421300
    },
    {
      "epoch": 3.84517118037813,
      "grad_norm": 4.459370136260986,
      "learning_rate": 4.6795690683018227e-05,
      "loss": 0.7543,
      "step": 421400
    },
    {
      "epoch": 3.846083655741295,
      "grad_norm": 4.6168928146362305,
      "learning_rate": 4.6794930286882257e-05,
      "loss": 0.7531,
      "step": 421500
    },
    {
      "epoch": 3.84699613110446,
      "grad_norm": 4.654423236846924,
      "learning_rate": 4.679416989074629e-05,
      "loss": 0.7371,
      "step": 421600
    },
    {
      "epoch": 3.8479086064676253,
      "grad_norm": 4.23707914352417,
      "learning_rate": 4.679340949461031e-05,
      "loss": 0.7749,
      "step": 421700
    },
    {
      "epoch": 3.8488210818307906,
      "grad_norm": 3.5811736583709717,
      "learning_rate": 4.679264909847435e-05,
      "loss": 0.7176,
      "step": 421800
    },
    {
      "epoch": 3.8497335571939555,
      "grad_norm": 3.273541212081909,
      "learning_rate": 4.679188870233837e-05,
      "loss": 0.6933,
      "step": 421900
    },
    {
      "epoch": 3.850646032557121,
      "grad_norm": 3.8534910678863525,
      "learning_rate": 4.67911283062024e-05,
      "loss": 0.7379,
      "step": 422000
    },
    {
      "epoch": 3.851558507920286,
      "grad_norm": 4.511319160461426,
      "learning_rate": 4.679036791006643e-05,
      "loss": 0.7192,
      "step": 422100
    },
    {
      "epoch": 3.8524709832834514,
      "grad_norm": 5.189502716064453,
      "learning_rate": 4.678960751393046e-05,
      "loss": 0.7731,
      "step": 422200
    },
    {
      "epoch": 3.8533834586466167,
      "grad_norm": 3.9462575912475586,
      "learning_rate": 4.678884711779449e-05,
      "loss": 0.7399,
      "step": 422300
    },
    {
      "epoch": 3.854295934009782,
      "grad_norm": 3.4737942218780518,
      "learning_rate": 4.678808672165852e-05,
      "loss": 0.77,
      "step": 422400
    },
    {
      "epoch": 3.855208409372947,
      "grad_norm": 3.5022590160369873,
      "learning_rate": 4.6787326325522544e-05,
      "loss": 0.7399,
      "step": 422500
    },
    {
      "epoch": 3.856120884736112,
      "grad_norm": 3.506545305252075,
      "learning_rate": 4.678656592938658e-05,
      "loss": 0.7299,
      "step": 422600
    },
    {
      "epoch": 3.8570333600992774,
      "grad_norm": 3.508518934249878,
      "learning_rate": 4.6785805533250604e-05,
      "loss": 0.7308,
      "step": 422700
    },
    {
      "epoch": 3.8579458354624423,
      "grad_norm": 4.308487415313721,
      "learning_rate": 4.6785045137114634e-05,
      "loss": 0.7364,
      "step": 422800
    },
    {
      "epoch": 3.8588583108256076,
      "grad_norm": 4.856045722961426,
      "learning_rate": 4.6784284740978664e-05,
      "loss": 0.7531,
      "step": 422900
    },
    {
      "epoch": 3.859770786188773,
      "grad_norm": 4.44010066986084,
      "learning_rate": 4.6783524344842694e-05,
      "loss": 0.7518,
      "step": 423000
    },
    {
      "epoch": 3.860683261551938,
      "grad_norm": 3.9993410110473633,
      "learning_rate": 4.678276394870672e-05,
      "loss": 0.7434,
      "step": 423100
    },
    {
      "epoch": 3.8615957369151035,
      "grad_norm": 4.428890705108643,
      "learning_rate": 4.678200355257075e-05,
      "loss": 0.6832,
      "step": 423200
    },
    {
      "epoch": 3.8625082122782683,
      "grad_norm": 4.253194332122803,
      "learning_rate": 4.678124315643478e-05,
      "loss": 0.7447,
      "step": 423300
    },
    {
      "epoch": 3.8634206876414336,
      "grad_norm": 3.8995485305786133,
      "learning_rate": 4.678048276029881e-05,
      "loss": 0.7142,
      "step": 423400
    },
    {
      "epoch": 3.864333163004599,
      "grad_norm": 3.9000678062438965,
      "learning_rate": 4.677972236416284e-05,
      "loss": 0.7049,
      "step": 423500
    },
    {
      "epoch": 3.865245638367764,
      "grad_norm": 5.0789008140563965,
      "learning_rate": 4.677896196802686e-05,
      "loss": 0.7455,
      "step": 423600
    },
    {
      "epoch": 3.866158113730929,
      "grad_norm": 4.176402568817139,
      "learning_rate": 4.67782015718909e-05,
      "loss": 0.7452,
      "step": 423700
    },
    {
      "epoch": 3.8670705890940944,
      "grad_norm": 4.809774875640869,
      "learning_rate": 4.677744117575492e-05,
      "loss": 0.7138,
      "step": 423800
    },
    {
      "epoch": 3.8679830644572597,
      "grad_norm": 4.517361640930176,
      "learning_rate": 4.677668077961895e-05,
      "loss": 0.7159,
      "step": 423900
    },
    {
      "epoch": 3.868895539820425,
      "grad_norm": 4.194600582122803,
      "learning_rate": 4.677592038348298e-05,
      "loss": 0.7398,
      "step": 424000
    },
    {
      "epoch": 3.8698080151835903,
      "grad_norm": 3.528844118118286,
      "learning_rate": 4.677515998734701e-05,
      "loss": 0.7316,
      "step": 424100
    },
    {
      "epoch": 3.870720490546755,
      "grad_norm": 3.261927366256714,
      "learning_rate": 4.6774399591211035e-05,
      "loss": 0.7063,
      "step": 424200
    },
    {
      "epoch": 3.8716329659099205,
      "grad_norm": 3.922795534133911,
      "learning_rate": 4.677363919507507e-05,
      "loss": 0.7041,
      "step": 424300
    },
    {
      "epoch": 3.8725454412730858,
      "grad_norm": 4.101814270019531,
      "learning_rate": 4.6772878798939095e-05,
      "loss": 0.6967,
      "step": 424400
    },
    {
      "epoch": 3.8734579166362506,
      "grad_norm": 4.107807159423828,
      "learning_rate": 4.6772118402803125e-05,
      "loss": 0.727,
      "step": 424500
    },
    {
      "epoch": 3.874370391999416,
      "grad_norm": 4.7787675857543945,
      "learning_rate": 4.6771358006667155e-05,
      "loss": 0.7121,
      "step": 424600
    },
    {
      "epoch": 3.875282867362581,
      "grad_norm": 4.946285724639893,
      "learning_rate": 4.6770597610531185e-05,
      "loss": 0.7386,
      "step": 424700
    },
    {
      "epoch": 3.8761953427257465,
      "grad_norm": 3.644702672958374,
      "learning_rate": 4.6769837214395215e-05,
      "loss": 0.7316,
      "step": 424800
    },
    {
      "epoch": 3.877107818088912,
      "grad_norm": 3.950385570526123,
      "learning_rate": 4.6769076818259245e-05,
      "loss": 0.7575,
      "step": 424900
    },
    {
      "epoch": 3.8780202934520767,
      "grad_norm": 3.897315263748169,
      "learning_rate": 4.676831642212327e-05,
      "loss": 0.7266,
      "step": 425000
    },
    {
      "epoch": 3.878932768815242,
      "grad_norm": 4.162715435028076,
      "learning_rate": 4.6767556025987305e-05,
      "loss": 0.7573,
      "step": 425100
    },
    {
      "epoch": 3.8798452441784073,
      "grad_norm": 2.845947265625,
      "learning_rate": 4.676679562985133e-05,
      "loss": 0.7434,
      "step": 425200
    },
    {
      "epoch": 3.880757719541572,
      "grad_norm": 3.6049742698669434,
      "learning_rate": 4.676603523371536e-05,
      "loss": 0.7319,
      "step": 425300
    },
    {
      "epoch": 3.8816701949047374,
      "grad_norm": 4.130817890167236,
      "learning_rate": 4.676527483757939e-05,
      "loss": 0.7365,
      "step": 425400
    },
    {
      "epoch": 3.8825826702679027,
      "grad_norm": 3.944336414337158,
      "learning_rate": 4.676451444144342e-05,
      "loss": 0.722,
      "step": 425500
    },
    {
      "epoch": 3.883495145631068,
      "grad_norm": 5.3029680252075195,
      "learning_rate": 4.676375404530744e-05,
      "loss": 0.7184,
      "step": 425600
    },
    {
      "epoch": 3.8844076209942333,
      "grad_norm": 3.862999677658081,
      "learning_rate": 4.676299364917148e-05,
      "loss": 0.7215,
      "step": 425700
    },
    {
      "epoch": 3.8853200963573986,
      "grad_norm": 3.7445685863494873,
      "learning_rate": 4.67622332530355e-05,
      "loss": 0.6988,
      "step": 425800
    },
    {
      "epoch": 3.8862325717205635,
      "grad_norm": 3.9586899280548096,
      "learning_rate": 4.676147285689953e-05,
      "loss": 0.7122,
      "step": 425900
    },
    {
      "epoch": 3.8871450470837288,
      "grad_norm": 4.378795623779297,
      "learning_rate": 4.676071246076356e-05,
      "loss": 0.7085,
      "step": 426000
    },
    {
      "epoch": 3.888057522446894,
      "grad_norm": 3.5566792488098145,
      "learning_rate": 4.6759952064627585e-05,
      "loss": 0.6991,
      "step": 426100
    },
    {
      "epoch": 3.888969997810059,
      "grad_norm": 3.502366304397583,
      "learning_rate": 4.675919166849162e-05,
      "loss": 0.7401,
      "step": 426200
    },
    {
      "epoch": 3.889882473173224,
      "grad_norm": 3.3009958267211914,
      "learning_rate": 4.6758431272355646e-05,
      "loss": 0.7302,
      "step": 426300
    },
    {
      "epoch": 3.8907949485363895,
      "grad_norm": 4.391821384429932,
      "learning_rate": 4.6757670876219676e-05,
      "loss": 0.7836,
      "step": 426400
    },
    {
      "epoch": 3.891707423899555,
      "grad_norm": 4.029647350311279,
      "learning_rate": 4.6756910480083706e-05,
      "loss": 0.7485,
      "step": 426500
    },
    {
      "epoch": 3.89261989926272,
      "grad_norm": 3.8162550926208496,
      "learning_rate": 4.6756150083947736e-05,
      "loss": 0.7567,
      "step": 426600
    },
    {
      "epoch": 3.893532374625885,
      "grad_norm": 4.553343296051025,
      "learning_rate": 4.6755389687811766e-05,
      "loss": 0.7607,
      "step": 426700
    },
    {
      "epoch": 3.8944448499890503,
      "grad_norm": 3.6848809719085693,
      "learning_rate": 4.6754629291675796e-05,
      "loss": 0.693,
      "step": 426800
    },
    {
      "epoch": 3.8953573253522156,
      "grad_norm": 3.9013614654541016,
      "learning_rate": 4.675386889553982e-05,
      "loss": 0.747,
      "step": 426900
    },
    {
      "epoch": 3.8962698007153804,
      "grad_norm": 4.564315319061279,
      "learning_rate": 4.675310849940385e-05,
      "loss": 0.7147,
      "step": 427000
    },
    {
      "epoch": 3.8971822760785457,
      "grad_norm": 3.8973658084869385,
      "learning_rate": 4.675234810326788e-05,
      "loss": 0.7187,
      "step": 427100
    },
    {
      "epoch": 3.898094751441711,
      "grad_norm": 4.844481945037842,
      "learning_rate": 4.675158770713191e-05,
      "loss": 0.7363,
      "step": 427200
    },
    {
      "epoch": 3.8990072268048763,
      "grad_norm": 3.764282464981079,
      "learning_rate": 4.675082731099594e-05,
      "loss": 0.6826,
      "step": 427300
    },
    {
      "epoch": 3.8999197021680416,
      "grad_norm": 3.1368868350982666,
      "learning_rate": 4.675006691485997e-05,
      "loss": 0.7487,
      "step": 427400
    },
    {
      "epoch": 3.900832177531207,
      "grad_norm": 2.6279664039611816,
      "learning_rate": 4.674930651872399e-05,
      "loss": 0.7206,
      "step": 427500
    },
    {
      "epoch": 3.901744652894372,
      "grad_norm": 4.453524112701416,
      "learning_rate": 4.674854612258803e-05,
      "loss": 0.7335,
      "step": 427600
    },
    {
      "epoch": 3.902657128257537,
      "grad_norm": 3.8986735343933105,
      "learning_rate": 4.674778572645205e-05,
      "loss": 0.7425,
      "step": 427700
    },
    {
      "epoch": 3.9035696036207024,
      "grad_norm": 2.8265058994293213,
      "learning_rate": 4.674702533031608e-05,
      "loss": 0.7184,
      "step": 427800
    },
    {
      "epoch": 3.9044820789838672,
      "grad_norm": 2.8724238872528076,
      "learning_rate": 4.674626493418011e-05,
      "loss": 0.6823,
      "step": 427900
    },
    {
      "epoch": 3.9053945543470325,
      "grad_norm": 4.988729953765869,
      "learning_rate": 4.674550453804414e-05,
      "loss": 0.7388,
      "step": 428000
    },
    {
      "epoch": 3.906307029710198,
      "grad_norm": 3.875216007232666,
      "learning_rate": 4.674474414190817e-05,
      "loss": 0.7414,
      "step": 428100
    },
    {
      "epoch": 3.907219505073363,
      "grad_norm": 5.122391700744629,
      "learning_rate": 4.67439837457722e-05,
      "loss": 0.7463,
      "step": 428200
    },
    {
      "epoch": 3.9081319804365284,
      "grad_norm": 4.516087532043457,
      "learning_rate": 4.674322334963623e-05,
      "loss": 0.7035,
      "step": 428300
    },
    {
      "epoch": 3.9090444557996933,
      "grad_norm": 3.93717360496521,
      "learning_rate": 4.674246295350026e-05,
      "loss": 0.7201,
      "step": 428400
    },
    {
      "epoch": 3.9099569311628586,
      "grad_norm": 3.5936806201934814,
      "learning_rate": 4.674170255736429e-05,
      "loss": 0.7031,
      "step": 428500
    },
    {
      "epoch": 3.910869406526024,
      "grad_norm": 3.761049509048462,
      "learning_rate": 4.674094216122832e-05,
      "loss": 0.7314,
      "step": 428600
    },
    {
      "epoch": 3.9117818818891887,
      "grad_norm": 3.0191287994384766,
      "learning_rate": 4.674018176509235e-05,
      "loss": 0.7092,
      "step": 428700
    },
    {
      "epoch": 3.912694357252354,
      "grad_norm": 5.288196563720703,
      "learning_rate": 4.673942136895638e-05,
      "loss": 0.7091,
      "step": 428800
    },
    {
      "epoch": 3.9136068326155193,
      "grad_norm": 3.9586546421051025,
      "learning_rate": 4.67386609728204e-05,
      "loss": 0.717,
      "step": 428900
    },
    {
      "epoch": 3.9145193079786846,
      "grad_norm": 4.589771270751953,
      "learning_rate": 4.673790057668443e-05,
      "loss": 0.7258,
      "step": 429000
    },
    {
      "epoch": 3.91543178334185,
      "grad_norm": 3.4769394397735596,
      "learning_rate": 4.673714018054846e-05,
      "loss": 0.726,
      "step": 429100
    },
    {
      "epoch": 3.9163442587050152,
      "grad_norm": 3.7596919536590576,
      "learning_rate": 4.673637978441249e-05,
      "loss": 0.7499,
      "step": 429200
    },
    {
      "epoch": 3.91725673406818,
      "grad_norm": 3.7838172912597656,
      "learning_rate": 4.673561938827652e-05,
      "loss": 0.7636,
      "step": 429300
    },
    {
      "epoch": 3.9181692094313454,
      "grad_norm": 4.13389253616333,
      "learning_rate": 4.6734858992140544e-05,
      "loss": 0.7035,
      "step": 429400
    },
    {
      "epoch": 3.9190816847945107,
      "grad_norm": 4.925431251525879,
      "learning_rate": 4.673409859600458e-05,
      "loss": 0.6941,
      "step": 429500
    },
    {
      "epoch": 3.9199941601576755,
      "grad_norm": 4.046291351318359,
      "learning_rate": 4.6733338199868604e-05,
      "loss": 0.6797,
      "step": 429600
    },
    {
      "epoch": 3.920906635520841,
      "grad_norm": 4.069572448730469,
      "learning_rate": 4.6732577803732634e-05,
      "loss": 0.7276,
      "step": 429700
    },
    {
      "epoch": 3.921819110884006,
      "grad_norm": 4.90880823135376,
      "learning_rate": 4.6731817407596664e-05,
      "loss": 0.7079,
      "step": 429800
    },
    {
      "epoch": 3.9227315862471714,
      "grad_norm": 3.499720811843872,
      "learning_rate": 4.6731057011460694e-05,
      "loss": 0.6816,
      "step": 429900
    },
    {
      "epoch": 3.9236440616103367,
      "grad_norm": 4.070778846740723,
      "learning_rate": 4.673029661532472e-05,
      "loss": 0.7572,
      "step": 430000
    },
    {
      "epoch": 3.9245565369735016,
      "grad_norm": 5.1007513999938965,
      "learning_rate": 4.6729536219188754e-05,
      "loss": 0.7286,
      "step": 430100
    },
    {
      "epoch": 3.925469012336667,
      "grad_norm": 3.8116111755371094,
      "learning_rate": 4.672877582305278e-05,
      "loss": 0.7144,
      "step": 430200
    },
    {
      "epoch": 3.926381487699832,
      "grad_norm": 3.760550022125244,
      "learning_rate": 4.672801542691681e-05,
      "loss": 0.7601,
      "step": 430300
    },
    {
      "epoch": 3.927293963062997,
      "grad_norm": 4.33886194229126,
      "learning_rate": 4.672725503078084e-05,
      "loss": 0.7033,
      "step": 430400
    },
    {
      "epoch": 3.9282064384261624,
      "grad_norm": 3.8456830978393555,
      "learning_rate": 4.672649463464487e-05,
      "loss": 0.7268,
      "step": 430500
    },
    {
      "epoch": 3.9291189137893276,
      "grad_norm": 3.390672445297241,
      "learning_rate": 4.67257342385089e-05,
      "loss": 0.7138,
      "step": 430600
    },
    {
      "epoch": 3.930031389152493,
      "grad_norm": 3.0887491703033447,
      "learning_rate": 4.672497384237293e-05,
      "loss": 0.725,
      "step": 430700
    },
    {
      "epoch": 3.9309438645156582,
      "grad_norm": 4.678948879241943,
      "learning_rate": 4.672421344623695e-05,
      "loss": 0.7302,
      "step": 430800
    },
    {
      "epoch": 3.9318563398788235,
      "grad_norm": 3.790579080581665,
      "learning_rate": 4.672345305010099e-05,
      "loss": 0.7266,
      "step": 430900
    },
    {
      "epoch": 3.9327688152419884,
      "grad_norm": 4.088823318481445,
      "learning_rate": 4.672269265396501e-05,
      "loss": 0.739,
      "step": 431000
    },
    {
      "epoch": 3.9336812906051537,
      "grad_norm": 3.7507035732269287,
      "learning_rate": 4.672193225782904e-05,
      "loss": 0.7436,
      "step": 431100
    },
    {
      "epoch": 3.934593765968319,
      "grad_norm": 3.7010200023651123,
      "learning_rate": 4.672117186169307e-05,
      "loss": 0.6934,
      "step": 431200
    },
    {
      "epoch": 3.935506241331484,
      "grad_norm": 4.35740852355957,
      "learning_rate": 4.67204114655571e-05,
      "loss": 0.7235,
      "step": 431300
    },
    {
      "epoch": 3.936418716694649,
      "grad_norm": 3.239886999130249,
      "learning_rate": 4.6719651069421125e-05,
      "loss": 0.7177,
      "step": 431400
    },
    {
      "epoch": 3.9373311920578145,
      "grad_norm": 4.565549373626709,
      "learning_rate": 4.671889067328516e-05,
      "loss": 0.7416,
      "step": 431500
    },
    {
      "epoch": 3.9382436674209798,
      "grad_norm": 4.769664764404297,
      "learning_rate": 4.6718130277149185e-05,
      "loss": 0.7462,
      "step": 431600
    },
    {
      "epoch": 3.939156142784145,
      "grad_norm": 3.87142276763916,
      "learning_rate": 4.6717369881013215e-05,
      "loss": 0.7474,
      "step": 431700
    },
    {
      "epoch": 3.94006861814731,
      "grad_norm": 3.9326233863830566,
      "learning_rate": 4.6716609484877245e-05,
      "loss": 0.7552,
      "step": 431800
    },
    {
      "epoch": 3.940981093510475,
      "grad_norm": 4.528019428253174,
      "learning_rate": 4.671584908874127e-05,
      "loss": 0.7678,
      "step": 431900
    },
    {
      "epoch": 3.9418935688736405,
      "grad_norm": 3.9742684364318848,
      "learning_rate": 4.6715088692605305e-05,
      "loss": 0.7533,
      "step": 432000
    },
    {
      "epoch": 3.9428060442368054,
      "grad_norm": 4.30088996887207,
      "learning_rate": 4.671432829646933e-05,
      "loss": 0.6913,
      "step": 432100
    },
    {
      "epoch": 3.9437185195999707,
      "grad_norm": 4.800413608551025,
      "learning_rate": 4.671356790033336e-05,
      "loss": 0.7624,
      "step": 432200
    },
    {
      "epoch": 3.944630994963136,
      "grad_norm": 5.007208347320557,
      "learning_rate": 4.671280750419739e-05,
      "loss": 0.7758,
      "step": 432300
    },
    {
      "epoch": 3.9455434703263013,
      "grad_norm": 4.675457000732422,
      "learning_rate": 4.671204710806142e-05,
      "loss": 0.7738,
      "step": 432400
    },
    {
      "epoch": 3.9464559456894666,
      "grad_norm": 3.6544175148010254,
      "learning_rate": 4.671128671192544e-05,
      "loss": 0.7151,
      "step": 432500
    },
    {
      "epoch": 3.9473684210526314,
      "grad_norm": 4.489327430725098,
      "learning_rate": 4.671052631578948e-05,
      "loss": 0.7236,
      "step": 432600
    },
    {
      "epoch": 3.9482808964157967,
      "grad_norm": 4.428761959075928,
      "learning_rate": 4.67097659196535e-05,
      "loss": 0.7262,
      "step": 432700
    },
    {
      "epoch": 3.949193371778962,
      "grad_norm": 4.707226276397705,
      "learning_rate": 4.670900552351753e-05,
      "loss": 0.6802,
      "step": 432800
    },
    {
      "epoch": 3.9501058471421273,
      "grad_norm": 4.308112621307373,
      "learning_rate": 4.670824512738156e-05,
      "loss": 0.6907,
      "step": 432900
    },
    {
      "epoch": 3.951018322505292,
      "grad_norm": 4.279668807983398,
      "learning_rate": 4.670748473124559e-05,
      "loss": 0.717,
      "step": 433000
    },
    {
      "epoch": 3.9519307978684575,
      "grad_norm": 3.5705630779266357,
      "learning_rate": 4.670672433510962e-05,
      "loss": 0.7545,
      "step": 433100
    },
    {
      "epoch": 3.9528432732316228,
      "grad_norm": 5.743168830871582,
      "learning_rate": 4.670596393897365e-05,
      "loss": 0.7161,
      "step": 433200
    },
    {
      "epoch": 3.953755748594788,
      "grad_norm": 4.072877883911133,
      "learning_rate": 4.6705203542837676e-05,
      "loss": 0.7306,
      "step": 433300
    },
    {
      "epoch": 3.9546682239579534,
      "grad_norm": 4.278171062469482,
      "learning_rate": 4.670444314670171e-05,
      "loss": 0.7268,
      "step": 433400
    },
    {
      "epoch": 3.955580699321118,
      "grad_norm": 4.1818647384643555,
      "learning_rate": 4.6703682750565736e-05,
      "loss": 0.7222,
      "step": 433500
    },
    {
      "epoch": 3.9564931746842835,
      "grad_norm": 3.6053128242492676,
      "learning_rate": 4.6702922354429766e-05,
      "loss": 0.7329,
      "step": 433600
    },
    {
      "epoch": 3.957405650047449,
      "grad_norm": 4.110597133636475,
      "learning_rate": 4.6702161958293796e-05,
      "loss": 0.7484,
      "step": 433700
    },
    {
      "epoch": 3.9583181254106137,
      "grad_norm": 3.7387728691101074,
      "learning_rate": 4.6701401562157826e-05,
      "loss": 0.6996,
      "step": 433800
    },
    {
      "epoch": 3.959230600773779,
      "grad_norm": 4.39954948425293,
      "learning_rate": 4.670064116602185e-05,
      "loss": 0.7349,
      "step": 433900
    },
    {
      "epoch": 3.9601430761369443,
      "grad_norm": 4.073185443878174,
      "learning_rate": 4.6699880769885886e-05,
      "loss": 0.7262,
      "step": 434000
    },
    {
      "epoch": 3.9610555515001096,
      "grad_norm": 3.7530665397644043,
      "learning_rate": 4.669912037374991e-05,
      "loss": 0.7124,
      "step": 434100
    },
    {
      "epoch": 3.961968026863275,
      "grad_norm": 4.250348091125488,
      "learning_rate": 4.669835997761394e-05,
      "loss": 0.738,
      "step": 434200
    },
    {
      "epoch": 3.9628805022264397,
      "grad_norm": 3.107875347137451,
      "learning_rate": 4.669759958147797e-05,
      "loss": 0.6929,
      "step": 434300
    },
    {
      "epoch": 3.963792977589605,
      "grad_norm": 4.140047550201416,
      "learning_rate": 4.6696839185342e-05,
      "loss": 0.7312,
      "step": 434400
    },
    {
      "epoch": 3.9647054529527703,
      "grad_norm": 3.340334892272949,
      "learning_rate": 4.669607878920603e-05,
      "loss": 0.739,
      "step": 434500
    },
    {
      "epoch": 3.9656179283159356,
      "grad_norm": 3.902142286300659,
      "learning_rate": 4.669531839307005e-05,
      "loss": 0.7321,
      "step": 434600
    },
    {
      "epoch": 3.9665304036791005,
      "grad_norm": 3.852818012237549,
      "learning_rate": 4.669455799693408e-05,
      "loss": 0.726,
      "step": 434700
    },
    {
      "epoch": 3.967442879042266,
      "grad_norm": 3.203672170639038,
      "learning_rate": 4.669379760079811e-05,
      "loss": 0.7273,
      "step": 434800
    },
    {
      "epoch": 3.968355354405431,
      "grad_norm": 4.398126125335693,
      "learning_rate": 4.669303720466214e-05,
      "loss": 0.7018,
      "step": 434900
    },
    {
      "epoch": 3.9692678297685964,
      "grad_norm": 3.6892387866973877,
      "learning_rate": 4.6692276808526167e-05,
      "loss": 0.7161,
      "step": 435000
    },
    {
      "epoch": 3.9701803051317617,
      "grad_norm": 4.0218186378479,
      "learning_rate": 4.6691516412390203e-05,
      "loss": 0.7197,
      "step": 435100
    },
    {
      "epoch": 3.9710927804949265,
      "grad_norm": 4.412159442901611,
      "learning_rate": 4.669075601625423e-05,
      "loss": 0.7061,
      "step": 435200
    },
    {
      "epoch": 3.972005255858092,
      "grad_norm": 3.8613150119781494,
      "learning_rate": 4.668999562011826e-05,
      "loss": 0.7348,
      "step": 435300
    },
    {
      "epoch": 3.972917731221257,
      "grad_norm": 4.203222274780273,
      "learning_rate": 4.668923522398229e-05,
      "loss": 0.7432,
      "step": 435400
    },
    {
      "epoch": 3.973830206584422,
      "grad_norm": 5.2369513511657715,
      "learning_rate": 4.668847482784632e-05,
      "loss": 0.7639,
      "step": 435500
    },
    {
      "epoch": 3.9747426819475873,
      "grad_norm": 3.7981464862823486,
      "learning_rate": 4.668771443171035e-05,
      "loss": 0.7368,
      "step": 435600
    },
    {
      "epoch": 3.9756551573107526,
      "grad_norm": 3.139664649963379,
      "learning_rate": 4.668695403557438e-05,
      "loss": 0.7263,
      "step": 435700
    },
    {
      "epoch": 3.976567632673918,
      "grad_norm": 4.218707084655762,
      "learning_rate": 4.66861936394384e-05,
      "loss": 0.7267,
      "step": 435800
    },
    {
      "epoch": 3.977480108037083,
      "grad_norm": 2.917421817779541,
      "learning_rate": 4.668543324330244e-05,
      "loss": 0.7131,
      "step": 435900
    },
    {
      "epoch": 3.978392583400248,
      "grad_norm": 3.4951019287109375,
      "learning_rate": 4.668467284716646e-05,
      "loss": 0.748,
      "step": 436000
    },
    {
      "epoch": 3.9793050587634133,
      "grad_norm": 3.83341908454895,
      "learning_rate": 4.668391245103049e-05,
      "loss": 0.7186,
      "step": 436100
    },
    {
      "epoch": 3.9802175341265786,
      "grad_norm": 4.325876235961914,
      "learning_rate": 4.668315205489452e-05,
      "loss": 0.7187,
      "step": 436200
    },
    {
      "epoch": 3.981130009489744,
      "grad_norm": 4.147576332092285,
      "learning_rate": 4.668239165875855e-05,
      "loss": 0.6868,
      "step": 436300
    },
    {
      "epoch": 3.982042484852909,
      "grad_norm": 4.6737189292907715,
      "learning_rate": 4.6681631262622574e-05,
      "loss": 0.7321,
      "step": 436400
    },
    {
      "epoch": 3.982954960216074,
      "grad_norm": 3.7876100540161133,
      "learning_rate": 4.668087086648661e-05,
      "loss": 0.6995,
      "step": 436500
    },
    {
      "epoch": 3.9838674355792394,
      "grad_norm": 4.722206115722656,
      "learning_rate": 4.6680110470350634e-05,
      "loss": 0.7083,
      "step": 436600
    },
    {
      "epoch": 3.9847799109424047,
      "grad_norm": 3.7043583393096924,
      "learning_rate": 4.6679350074214664e-05,
      "loss": 0.6719,
      "step": 436700
    },
    {
      "epoch": 3.98569238630557,
      "grad_norm": 3.70967698097229,
      "learning_rate": 4.6678589678078694e-05,
      "loss": 0.7256,
      "step": 436800
    },
    {
      "epoch": 3.986604861668735,
      "grad_norm": 4.617711067199707,
      "learning_rate": 4.6677829281942724e-05,
      "loss": 0.719,
      "step": 436900
    },
    {
      "epoch": 3.9875173370319,
      "grad_norm": 4.155750274658203,
      "learning_rate": 4.6677068885806754e-05,
      "loss": 0.7578,
      "step": 437000
    },
    {
      "epoch": 3.9884298123950654,
      "grad_norm": 3.8766984939575195,
      "learning_rate": 4.6676308489670784e-05,
      "loss": 0.714,
      "step": 437100
    },
    {
      "epoch": 3.9893422877582303,
      "grad_norm": 3.454226016998291,
      "learning_rate": 4.667554809353481e-05,
      "loss": 0.711,
      "step": 437200
    },
    {
      "epoch": 3.9902547631213956,
      "grad_norm": 4.4558844566345215,
      "learning_rate": 4.6674787697398845e-05,
      "loss": 0.7351,
      "step": 437300
    },
    {
      "epoch": 3.991167238484561,
      "grad_norm": 4.648251533508301,
      "learning_rate": 4.667402730126287e-05,
      "loss": 0.7621,
      "step": 437400
    },
    {
      "epoch": 3.992079713847726,
      "grad_norm": 3.7430057525634766,
      "learning_rate": 4.667326690512689e-05,
      "loss": 0.7301,
      "step": 437500
    },
    {
      "epoch": 3.9929921892108915,
      "grad_norm": 4.747663974761963,
      "learning_rate": 4.667250650899093e-05,
      "loss": 0.7505,
      "step": 437600
    },
    {
      "epoch": 3.9939046645740564,
      "grad_norm": 5.346219062805176,
      "learning_rate": 4.667174611285495e-05,
      "loss": 0.7294,
      "step": 437700
    },
    {
      "epoch": 3.9948171399372217,
      "grad_norm": 5.102816581726074,
      "learning_rate": 4.667098571671898e-05,
      "loss": 0.7195,
      "step": 437800
    },
    {
      "epoch": 3.995729615300387,
      "grad_norm": 3.9042129516601562,
      "learning_rate": 4.667022532058301e-05,
      "loss": 0.7231,
      "step": 437900
    },
    {
      "epoch": 3.9966420906635522,
      "grad_norm": 4.678478240966797,
      "learning_rate": 4.666946492444704e-05,
      "loss": 0.7302,
      "step": 438000
    },
    {
      "epoch": 3.997554566026717,
      "grad_norm": 3.6451733112335205,
      "learning_rate": 4.666870452831107e-05,
      "loss": 0.7362,
      "step": 438100
    },
    {
      "epoch": 3.9984670413898824,
      "grad_norm": 3.891184091567993,
      "learning_rate": 4.66679441321751e-05,
      "loss": 0.7521,
      "step": 438200
    },
    {
      "epoch": 3.9993795167530477,
      "grad_norm": 3.5894312858581543,
      "learning_rate": 4.6667183736039125e-05,
      "loss": 0.6949,
      "step": 438300
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5896905660629272,
      "eval_runtime": 25.4845,
      "eval_samples_per_second": 226.373,
      "eval_steps_per_second": 226.373,
      "step": 438368
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5738134980201721,
      "eval_runtime": 482.5195,
      "eval_samples_per_second": 227.124,
      "eval_steps_per_second": 227.124,
      "step": 438368
    },
    {
      "epoch": 4.000291992116213,
      "grad_norm": 3.74092173576355,
      "learning_rate": 4.666642333990316e-05,
      "loss": 0.7245,
      "step": 438400
    },
    {
      "epoch": 4.001204467479378,
      "grad_norm": 4.659855842590332,
      "learning_rate": 4.6665662943767185e-05,
      "loss": 0.6885,
      "step": 438500
    },
    {
      "epoch": 4.002116942842544,
      "grad_norm": 4.171946048736572,
      "learning_rate": 4.6664902547631215e-05,
      "loss": 0.7453,
      "step": 438600
    },
    {
      "epoch": 4.003029418205708,
      "grad_norm": 4.011196613311768,
      "learning_rate": 4.6664142151495245e-05,
      "loss": 0.7043,
      "step": 438700
    },
    {
      "epoch": 4.003941893568873,
      "grad_norm": 4.822241306304932,
      "learning_rate": 4.6663381755359275e-05,
      "loss": 0.693,
      "step": 438800
    },
    {
      "epoch": 4.004854368932039,
      "grad_norm": 3.4446475505828857,
      "learning_rate": 4.66626213592233e-05,
      "loss": 0.7164,
      "step": 438900
    },
    {
      "epoch": 4.005766844295204,
      "grad_norm": 4.171957969665527,
      "learning_rate": 4.6661860963087335e-05,
      "loss": 0.6826,
      "step": 439000
    },
    {
      "epoch": 4.006679319658369,
      "grad_norm": 4.677346229553223,
      "learning_rate": 4.666110056695136e-05,
      "loss": 0.7187,
      "step": 439100
    },
    {
      "epoch": 4.0075917950215345,
      "grad_norm": 4.043015003204346,
      "learning_rate": 4.666034017081539e-05,
      "loss": 0.7218,
      "step": 439200
    },
    {
      "epoch": 4.0085042703847,
      "grad_norm": 3.6689612865448,
      "learning_rate": 4.665957977467942e-05,
      "loss": 0.7547,
      "step": 439300
    },
    {
      "epoch": 4.009416745747865,
      "grad_norm": 4.128244876861572,
      "learning_rate": 4.665881937854345e-05,
      "loss": 0.7196,
      "step": 439400
    },
    {
      "epoch": 4.01032922111103,
      "grad_norm": 4.3238067626953125,
      "learning_rate": 4.665805898240748e-05,
      "loss": 0.7561,
      "step": 439500
    },
    {
      "epoch": 4.011241696474195,
      "grad_norm": 3.6985981464385986,
      "learning_rate": 4.665729858627151e-05,
      "loss": 0.6901,
      "step": 439600
    },
    {
      "epoch": 4.01215417183736,
      "grad_norm": 4.4140448570251465,
      "learning_rate": 4.665653819013553e-05,
      "loss": 0.7113,
      "step": 439700
    },
    {
      "epoch": 4.013066647200525,
      "grad_norm": 3.883669376373291,
      "learning_rate": 4.665577779399957e-05,
      "loss": 0.7168,
      "step": 439800
    },
    {
      "epoch": 4.013979122563691,
      "grad_norm": 4.404171466827393,
      "learning_rate": 4.665501739786359e-05,
      "loss": 0.7088,
      "step": 439900
    },
    {
      "epoch": 4.014891597926856,
      "grad_norm": 4.288637161254883,
      "learning_rate": 4.665425700172762e-05,
      "loss": 0.7198,
      "step": 440000
    },
    {
      "epoch": 4.015804073290021,
      "grad_norm": 3.2324700355529785,
      "learning_rate": 4.665349660559165e-05,
      "loss": 0.7041,
      "step": 440100
    },
    {
      "epoch": 4.016716548653187,
      "grad_norm": 4.276548385620117,
      "learning_rate": 4.665273620945568e-05,
      "loss": 0.7224,
      "step": 440200
    },
    {
      "epoch": 4.017629024016352,
      "grad_norm": 4.125844955444336,
      "learning_rate": 4.665197581331971e-05,
      "loss": 0.7359,
      "step": 440300
    },
    {
      "epoch": 4.018541499379516,
      "grad_norm": 3.2987823486328125,
      "learning_rate": 4.6651215417183736e-05,
      "loss": 0.6831,
      "step": 440400
    },
    {
      "epoch": 4.019453974742682,
      "grad_norm": 4.8675971031188965,
      "learning_rate": 4.6650455021047766e-05,
      "loss": 0.7112,
      "step": 440500
    },
    {
      "epoch": 4.020366450105847,
      "grad_norm": 4.633395671844482,
      "learning_rate": 4.6649694624911796e-05,
      "loss": 0.7184,
      "step": 440600
    },
    {
      "epoch": 4.021278925469012,
      "grad_norm": 4.57559061050415,
      "learning_rate": 4.6648934228775826e-05,
      "loss": 0.7356,
      "step": 440700
    },
    {
      "epoch": 4.0221914008321775,
      "grad_norm": 4.482644557952881,
      "learning_rate": 4.664817383263985e-05,
      "loss": 0.748,
      "step": 440800
    },
    {
      "epoch": 4.023103876195343,
      "grad_norm": 3.92789888381958,
      "learning_rate": 4.6647413436503886e-05,
      "loss": 0.6899,
      "step": 440900
    },
    {
      "epoch": 4.024016351558508,
      "grad_norm": 3.8159308433532715,
      "learning_rate": 4.664665304036791e-05,
      "loss": 0.6906,
      "step": 441000
    },
    {
      "epoch": 4.024928826921673,
      "grad_norm": 4.6982035636901855,
      "learning_rate": 4.664589264423194e-05,
      "loss": 0.7459,
      "step": 441100
    },
    {
      "epoch": 4.025841302284839,
      "grad_norm": 3.867406129837036,
      "learning_rate": 4.664513224809597e-05,
      "loss": 0.6498,
      "step": 441200
    },
    {
      "epoch": 4.026753777648003,
      "grad_norm": 4.409730911254883,
      "learning_rate": 4.664437185196e-05,
      "loss": 0.7461,
      "step": 441300
    },
    {
      "epoch": 4.027666253011168,
      "grad_norm": 3.209629535675049,
      "learning_rate": 4.664361145582403e-05,
      "loss": 0.6995,
      "step": 441400
    },
    {
      "epoch": 4.028578728374334,
      "grad_norm": 4.171762943267822,
      "learning_rate": 4.664285105968806e-05,
      "loss": 0.7268,
      "step": 441500
    },
    {
      "epoch": 4.029491203737499,
      "grad_norm": 4.254566669464111,
      "learning_rate": 4.664209066355208e-05,
      "loss": 0.7045,
      "step": 441600
    },
    {
      "epoch": 4.030403679100664,
      "grad_norm": 3.821366310119629,
      "learning_rate": 4.664133026741612e-05,
      "loss": 0.7205,
      "step": 441700
    },
    {
      "epoch": 4.03131615446383,
      "grad_norm": 4.377540111541748,
      "learning_rate": 4.6640569871280143e-05,
      "loss": 0.7052,
      "step": 441800
    },
    {
      "epoch": 4.032228629826995,
      "grad_norm": 4.125103950500488,
      "learning_rate": 4.6639809475144173e-05,
      "loss": 0.6997,
      "step": 441900
    },
    {
      "epoch": 4.03314110519016,
      "grad_norm": 4.2199506759643555,
      "learning_rate": 4.6639049079008204e-05,
      "loss": 0.6919,
      "step": 442000
    },
    {
      "epoch": 4.034053580553325,
      "grad_norm": 4.0763773918151855,
      "learning_rate": 4.6638288682872234e-05,
      "loss": 0.7344,
      "step": 442100
    },
    {
      "epoch": 4.03496605591649,
      "grad_norm": 3.6340131759643555,
      "learning_rate": 4.663752828673626e-05,
      "loss": 0.7735,
      "step": 442200
    },
    {
      "epoch": 4.035878531279655,
      "grad_norm": 4.587629795074463,
      "learning_rate": 4.6636767890600294e-05,
      "loss": 0.7284,
      "step": 442300
    },
    {
      "epoch": 4.0367910066428205,
      "grad_norm": 4.981634616851807,
      "learning_rate": 4.663600749446432e-05,
      "loss": 0.7113,
      "step": 442400
    },
    {
      "epoch": 4.037703482005986,
      "grad_norm": 4.164541244506836,
      "learning_rate": 4.663524709832835e-05,
      "loss": 0.7639,
      "step": 442500
    },
    {
      "epoch": 4.038615957369151,
      "grad_norm": 3.721857786178589,
      "learning_rate": 4.663448670219238e-05,
      "loss": 0.757,
      "step": 442600
    },
    {
      "epoch": 4.039528432732316,
      "grad_norm": 4.568319797515869,
      "learning_rate": 4.663372630605641e-05,
      "loss": 0.717,
      "step": 442700
    },
    {
      "epoch": 4.040440908095482,
      "grad_norm": 3.2076475620269775,
      "learning_rate": 4.663296590992044e-05,
      "loss": 0.6912,
      "step": 442800
    },
    {
      "epoch": 4.041353383458647,
      "grad_norm": 4.382143497467041,
      "learning_rate": 4.663220551378447e-05,
      "loss": 0.7497,
      "step": 442900
    },
    {
      "epoch": 4.042265858821811,
      "grad_norm": 4.420133113861084,
      "learning_rate": 4.663144511764849e-05,
      "loss": 0.7517,
      "step": 443000
    },
    {
      "epoch": 4.043178334184977,
      "grad_norm": 4.805071830749512,
      "learning_rate": 4.663068472151252e-05,
      "loss": 0.757,
      "step": 443100
    },
    {
      "epoch": 4.044090809548142,
      "grad_norm": 4.428953170776367,
      "learning_rate": 4.662992432537655e-05,
      "loss": 0.7344,
      "step": 443200
    },
    {
      "epoch": 4.045003284911307,
      "grad_norm": 3.9317939281463623,
      "learning_rate": 4.6629163929240574e-05,
      "loss": 0.7454,
      "step": 443300
    },
    {
      "epoch": 4.045915760274473,
      "grad_norm": 4.251445293426514,
      "learning_rate": 4.662840353310461e-05,
      "loss": 0.6909,
      "step": 443400
    },
    {
      "epoch": 4.046828235637638,
      "grad_norm": 3.6398723125457764,
      "learning_rate": 4.6627643136968634e-05,
      "loss": 0.7046,
      "step": 443500
    },
    {
      "epoch": 4.047740711000803,
      "grad_norm": 4.294349670410156,
      "learning_rate": 4.6626882740832664e-05,
      "loss": 0.6695,
      "step": 443600
    },
    {
      "epoch": 4.0486531863639685,
      "grad_norm": 3.8908941745758057,
      "learning_rate": 4.6626122344696694e-05,
      "loss": 0.7309,
      "step": 443700
    },
    {
      "epoch": 4.049565661727133,
      "grad_norm": 4.3476176261901855,
      "learning_rate": 4.6625361948560724e-05,
      "loss": 0.687,
      "step": 443800
    },
    {
      "epoch": 4.050478137090298,
      "grad_norm": 3.717698097229004,
      "learning_rate": 4.6624601552424754e-05,
      "loss": 0.7012,
      "step": 443900
    },
    {
      "epoch": 4.0513906124534635,
      "grad_norm": 4.436276435852051,
      "learning_rate": 4.6623841156288785e-05,
      "loss": 0.7279,
      "step": 444000
    },
    {
      "epoch": 4.052303087816629,
      "grad_norm": 4.665747165679932,
      "learning_rate": 4.662308076015281e-05,
      "loss": 0.7043,
      "step": 444100
    },
    {
      "epoch": 4.053215563179794,
      "grad_norm": 4.367623329162598,
      "learning_rate": 4.6622320364016845e-05,
      "loss": 0.7034,
      "step": 444200
    },
    {
      "epoch": 4.0541280385429594,
      "grad_norm": 4.082684516906738,
      "learning_rate": 4.662155996788087e-05,
      "loss": 0.736,
      "step": 444300
    },
    {
      "epoch": 4.055040513906125,
      "grad_norm": 3.8461482524871826,
      "learning_rate": 4.66207995717449e-05,
      "loss": 0.7554,
      "step": 444400
    },
    {
      "epoch": 4.05595298926929,
      "grad_norm": 3.03794002532959,
      "learning_rate": 4.662003917560893e-05,
      "loss": 0.6953,
      "step": 444500
    },
    {
      "epoch": 4.056865464632455,
      "grad_norm": 5.283821105957031,
      "learning_rate": 4.661927877947296e-05,
      "loss": 0.7196,
      "step": 444600
    },
    {
      "epoch": 4.05777793999562,
      "grad_norm": 5.594027996063232,
      "learning_rate": 4.661851838333698e-05,
      "loss": 0.7407,
      "step": 444700
    },
    {
      "epoch": 4.058690415358785,
      "grad_norm": 3.1754322052001953,
      "learning_rate": 4.661775798720102e-05,
      "loss": 0.7175,
      "step": 444800
    },
    {
      "epoch": 4.05960289072195,
      "grad_norm": 4.5668134689331055,
      "learning_rate": 4.661699759106504e-05,
      "loss": 0.7357,
      "step": 444900
    },
    {
      "epoch": 4.060515366085116,
      "grad_norm": 3.923750162124634,
      "learning_rate": 4.661623719492907e-05,
      "loss": 0.7518,
      "step": 445000
    },
    {
      "epoch": 4.061427841448281,
      "grad_norm": 4.167392253875732,
      "learning_rate": 4.66154767987931e-05,
      "loss": 0.7106,
      "step": 445100
    },
    {
      "epoch": 4.062340316811446,
      "grad_norm": 4.631115436553955,
      "learning_rate": 4.661471640265713e-05,
      "loss": 0.7238,
      "step": 445200
    },
    {
      "epoch": 4.0632527921746115,
      "grad_norm": 4.832986831665039,
      "learning_rate": 4.661395600652116e-05,
      "loss": 0.7459,
      "step": 445300
    },
    {
      "epoch": 4.064165267537777,
      "grad_norm": 4.211310386657715,
      "learning_rate": 4.661319561038519e-05,
      "loss": 0.7519,
      "step": 445400
    },
    {
      "epoch": 4.065077742900941,
      "grad_norm": 5.621156692504883,
      "learning_rate": 4.6612435214249215e-05,
      "loss": 0.7293,
      "step": 445500
    },
    {
      "epoch": 4.065990218264107,
      "grad_norm": 5.179264068603516,
      "learning_rate": 4.661167481811325e-05,
      "loss": 0.712,
      "step": 445600
    },
    {
      "epoch": 4.066902693627272,
      "grad_norm": 3.606055498123169,
      "learning_rate": 4.6610914421977275e-05,
      "loss": 0.7044,
      "step": 445700
    },
    {
      "epoch": 4.067815168990437,
      "grad_norm": 4.24705696105957,
      "learning_rate": 4.6610154025841305e-05,
      "loss": 0.7385,
      "step": 445800
    },
    {
      "epoch": 4.0687276443536025,
      "grad_norm": 4.00849723815918,
      "learning_rate": 4.6609393629705336e-05,
      "loss": 0.6888,
      "step": 445900
    },
    {
      "epoch": 4.069640119716768,
      "grad_norm": 3.924605131149292,
      "learning_rate": 4.660863323356936e-05,
      "loss": 0.7082,
      "step": 446000
    },
    {
      "epoch": 4.070552595079933,
      "grad_norm": 3.450876474380493,
      "learning_rate": 4.660787283743339e-05,
      "loss": 0.7964,
      "step": 446100
    },
    {
      "epoch": 4.071465070443098,
      "grad_norm": 2.9245662689208984,
      "learning_rate": 4.660711244129742e-05,
      "loss": 0.6889,
      "step": 446200
    },
    {
      "epoch": 4.072377545806264,
      "grad_norm": 3.9542102813720703,
      "learning_rate": 4.660635204516145e-05,
      "loss": 0.7293,
      "step": 446300
    },
    {
      "epoch": 4.073290021169428,
      "grad_norm": 4.331859111785889,
      "learning_rate": 4.660559164902548e-05,
      "loss": 0.7404,
      "step": 446400
    },
    {
      "epoch": 4.074202496532593,
      "grad_norm": 4.619970798492432,
      "learning_rate": 4.660483125288951e-05,
      "loss": 0.7294,
      "step": 446500
    },
    {
      "epoch": 4.075114971895759,
      "grad_norm": 4.638515949249268,
      "learning_rate": 4.660407085675353e-05,
      "loss": 0.7255,
      "step": 446600
    },
    {
      "epoch": 4.076027447258924,
      "grad_norm": 4.58754825592041,
      "learning_rate": 4.660331046061757e-05,
      "loss": 0.6968,
      "step": 446700
    },
    {
      "epoch": 4.076939922622089,
      "grad_norm": 3.5422799587249756,
      "learning_rate": 4.660255006448159e-05,
      "loss": 0.7386,
      "step": 446800
    },
    {
      "epoch": 4.077852397985255,
      "grad_norm": 3.7305092811584473,
      "learning_rate": 4.660178966834562e-05,
      "loss": 0.7092,
      "step": 446900
    },
    {
      "epoch": 4.07876487334842,
      "grad_norm": 4.531289100646973,
      "learning_rate": 4.660102927220965e-05,
      "loss": 0.7597,
      "step": 447000
    },
    {
      "epoch": 4.079677348711585,
      "grad_norm": 3.77016282081604,
      "learning_rate": 4.660026887607368e-05,
      "loss": 0.7251,
      "step": 447100
    },
    {
      "epoch": 4.08058982407475,
      "grad_norm": 5.666352272033691,
      "learning_rate": 4.6599508479937706e-05,
      "loss": 0.7287,
      "step": 447200
    },
    {
      "epoch": 4.081502299437915,
      "grad_norm": 4.288332462310791,
      "learning_rate": 4.659874808380174e-05,
      "loss": 0.745,
      "step": 447300
    },
    {
      "epoch": 4.08241477480108,
      "grad_norm": 3.2884833812713623,
      "learning_rate": 4.6597987687665766e-05,
      "loss": 0.6881,
      "step": 447400
    },
    {
      "epoch": 4.0833272501642455,
      "grad_norm": 4.292118072509766,
      "learning_rate": 4.6597227291529796e-05,
      "loss": 0.7577,
      "step": 447500
    },
    {
      "epoch": 4.084239725527411,
      "grad_norm": 4.474571228027344,
      "learning_rate": 4.6596466895393826e-05,
      "loss": 0.7254,
      "step": 447600
    },
    {
      "epoch": 4.085152200890576,
      "grad_norm": 4.069386005401611,
      "learning_rate": 4.6595706499257856e-05,
      "loss": 0.6655,
      "step": 447700
    },
    {
      "epoch": 4.086064676253741,
      "grad_norm": 4.822988986968994,
      "learning_rate": 4.6594946103121886e-05,
      "loss": 0.7283,
      "step": 447800
    },
    {
      "epoch": 4.086977151616907,
      "grad_norm": 4.347027778625488,
      "learning_rate": 4.6594185706985917e-05,
      "loss": 0.6988,
      "step": 447900
    },
    {
      "epoch": 4.087889626980072,
      "grad_norm": 4.495874881744385,
      "learning_rate": 4.659342531084994e-05,
      "loss": 0.7198,
      "step": 448000
    },
    {
      "epoch": 4.088802102343236,
      "grad_norm": 4.717628479003906,
      "learning_rate": 4.659266491471398e-05,
      "loss": 0.7608,
      "step": 448100
    },
    {
      "epoch": 4.089714577706402,
      "grad_norm": 4.327602863311768,
      "learning_rate": 4.6591904518578e-05,
      "loss": 0.719,
      "step": 448200
    },
    {
      "epoch": 4.090627053069567,
      "grad_norm": 3.9661314487457275,
      "learning_rate": 4.659114412244203e-05,
      "loss": 0.7283,
      "step": 448300
    },
    {
      "epoch": 4.091539528432732,
      "grad_norm": 4.096573829650879,
      "learning_rate": 4.659038372630606e-05,
      "loss": 0.7122,
      "step": 448400
    },
    {
      "epoch": 4.092452003795898,
      "grad_norm": 3.9885780811309814,
      "learning_rate": 4.658962333017009e-05,
      "loss": 0.6942,
      "step": 448500
    },
    {
      "epoch": 4.093364479159063,
      "grad_norm": 4.208357334136963,
      "learning_rate": 4.6588862934034113e-05,
      "loss": 0.7428,
      "step": 448600
    },
    {
      "epoch": 4.094276954522228,
      "grad_norm": 3.5092415809631348,
      "learning_rate": 4.658810253789815e-05,
      "loss": 0.6974,
      "step": 448700
    },
    {
      "epoch": 4.0951894298853935,
      "grad_norm": 4.37342643737793,
      "learning_rate": 4.6587342141762174e-05,
      "loss": 0.7402,
      "step": 448800
    },
    {
      "epoch": 4.096101905248558,
      "grad_norm": 3.6505544185638428,
      "learning_rate": 4.6586581745626204e-05,
      "loss": 0.7373,
      "step": 448900
    },
    {
      "epoch": 4.097014380611723,
      "grad_norm": 3.6952550411224365,
      "learning_rate": 4.6585821349490234e-05,
      "loss": 0.7363,
      "step": 449000
    },
    {
      "epoch": 4.0979268559748885,
      "grad_norm": 4.606664657592773,
      "learning_rate": 4.658506095335426e-05,
      "loss": 0.7136,
      "step": 449100
    },
    {
      "epoch": 4.098839331338054,
      "grad_norm": 4.198788166046143,
      "learning_rate": 4.6584300557218294e-05,
      "loss": 0.7272,
      "step": 449200
    },
    {
      "epoch": 4.099751806701219,
      "grad_norm": 4.189878463745117,
      "learning_rate": 4.658354016108232e-05,
      "loss": 0.7438,
      "step": 449300
    },
    {
      "epoch": 4.100664282064384,
      "grad_norm": 2.8435027599334717,
      "learning_rate": 4.658277976494635e-05,
      "loss": 0.7166,
      "step": 449400
    },
    {
      "epoch": 4.10157675742755,
      "grad_norm": 4.107283115386963,
      "learning_rate": 4.658201936881038e-05,
      "loss": 0.6957,
      "step": 449500
    },
    {
      "epoch": 4.102489232790715,
      "grad_norm": 3.7519702911376953,
      "learning_rate": 4.658125897267441e-05,
      "loss": 0.7017,
      "step": 449600
    },
    {
      "epoch": 4.10340170815388,
      "grad_norm": 3.9356257915496826,
      "learning_rate": 4.658049857653843e-05,
      "loss": 0.712,
      "step": 449700
    },
    {
      "epoch": 4.104314183517045,
      "grad_norm": 3.8730034828186035,
      "learning_rate": 4.657973818040247e-05,
      "loss": 0.7296,
      "step": 449800
    },
    {
      "epoch": 4.10522665888021,
      "grad_norm": 4.476144313812256,
      "learning_rate": 4.657897778426649e-05,
      "loss": 0.7381,
      "step": 449900
    },
    {
      "epoch": 4.106139134243375,
      "grad_norm": 4.492051601409912,
      "learning_rate": 4.657821738813052e-05,
      "loss": 0.7014,
      "step": 450000
    },
    {
      "epoch": 4.107051609606541,
      "grad_norm": 4.0773606300354,
      "learning_rate": 4.657745699199455e-05,
      "loss": 0.7287,
      "step": 450100
    },
    {
      "epoch": 4.107964084969706,
      "grad_norm": 4.2753424644470215,
      "learning_rate": 4.657669659585858e-05,
      "loss": 0.7523,
      "step": 450200
    },
    {
      "epoch": 4.108876560332871,
      "grad_norm": 5.028346538543701,
      "learning_rate": 4.657593619972261e-05,
      "loss": 0.7029,
      "step": 450300
    },
    {
      "epoch": 4.1097890356960365,
      "grad_norm": 4.072327136993408,
      "learning_rate": 4.657517580358664e-05,
      "loss": 0.7573,
      "step": 450400
    },
    {
      "epoch": 4.110701511059202,
      "grad_norm": 4.034938812255859,
      "learning_rate": 4.6574415407450664e-05,
      "loss": 0.7083,
      "step": 450500
    },
    {
      "epoch": 4.111613986422366,
      "grad_norm": 4.92597770690918,
      "learning_rate": 4.65736550113147e-05,
      "loss": 0.6895,
      "step": 450600
    },
    {
      "epoch": 4.1125264617855315,
      "grad_norm": 3.8753485679626465,
      "learning_rate": 4.6572894615178725e-05,
      "loss": 0.7176,
      "step": 450700
    },
    {
      "epoch": 4.113438937148697,
      "grad_norm": 5.356464862823486,
      "learning_rate": 4.6572134219042755e-05,
      "loss": 0.7418,
      "step": 450800
    },
    {
      "epoch": 4.114351412511862,
      "grad_norm": 4.649652004241943,
      "learning_rate": 4.6571373822906785e-05,
      "loss": 0.709,
      "step": 450900
    },
    {
      "epoch": 4.115263887875027,
      "grad_norm": 4.441019058227539,
      "learning_rate": 4.6570613426770815e-05,
      "loss": 0.769,
      "step": 451000
    },
    {
      "epoch": 4.116176363238193,
      "grad_norm": 4.290393829345703,
      "learning_rate": 4.656985303063484e-05,
      "loss": 0.7169,
      "step": 451100
    },
    {
      "epoch": 4.117088838601358,
      "grad_norm": 4.446019649505615,
      "learning_rate": 4.6569092634498875e-05,
      "loss": 0.7161,
      "step": 451200
    },
    {
      "epoch": 4.118001313964523,
      "grad_norm": 3.9044954776763916,
      "learning_rate": 4.65683322383629e-05,
      "loss": 0.7379,
      "step": 451300
    },
    {
      "epoch": 4.118913789327689,
      "grad_norm": 4.037796497344971,
      "learning_rate": 4.656757184222693e-05,
      "loss": 0.7195,
      "step": 451400
    },
    {
      "epoch": 4.119826264690853,
      "grad_norm": 3.4491076469421387,
      "learning_rate": 4.656681144609096e-05,
      "loss": 0.7152,
      "step": 451500
    },
    {
      "epoch": 4.120738740054018,
      "grad_norm": 4.386538505554199,
      "learning_rate": 4.656605104995498e-05,
      "loss": 0.7196,
      "step": 451600
    },
    {
      "epoch": 4.121651215417184,
      "grad_norm": 4.001733303070068,
      "learning_rate": 4.656529065381902e-05,
      "loss": 0.748,
      "step": 451700
    },
    {
      "epoch": 4.122563690780349,
      "grad_norm": 4.050789833068848,
      "learning_rate": 4.656453025768304e-05,
      "loss": 0.7226,
      "step": 451800
    },
    {
      "epoch": 4.123476166143514,
      "grad_norm": 3.6417758464813232,
      "learning_rate": 4.656376986154707e-05,
      "loss": 0.7057,
      "step": 451900
    },
    {
      "epoch": 4.1243886415066795,
      "grad_norm": 3.7444775104522705,
      "learning_rate": 4.65630094654111e-05,
      "loss": 0.7291,
      "step": 452000
    },
    {
      "epoch": 4.125301116869845,
      "grad_norm": 3.7729084491729736,
      "learning_rate": 4.656224906927513e-05,
      "loss": 0.732,
      "step": 452100
    },
    {
      "epoch": 4.12621359223301,
      "grad_norm": 3.850170612335205,
      "learning_rate": 4.656148867313916e-05,
      "loss": 0.7108,
      "step": 452200
    },
    {
      "epoch": 4.1271260675961745,
      "grad_norm": 3.7368886470794678,
      "learning_rate": 4.656072827700319e-05,
      "loss": 0.7122,
      "step": 452300
    },
    {
      "epoch": 4.12803854295934,
      "grad_norm": 5.154019355773926,
      "learning_rate": 4.6559967880867215e-05,
      "loss": 0.7522,
      "step": 452400
    },
    {
      "epoch": 4.128951018322505,
      "grad_norm": 4.546103477478027,
      "learning_rate": 4.6559207484731245e-05,
      "loss": 0.7071,
      "step": 452500
    },
    {
      "epoch": 4.12986349368567,
      "grad_norm": 3.833658218383789,
      "learning_rate": 4.6558447088595275e-05,
      "loss": 0.6708,
      "step": 452600
    },
    {
      "epoch": 4.130775969048836,
      "grad_norm": 3.7088205814361572,
      "learning_rate": 4.6557686692459306e-05,
      "loss": 0.7132,
      "step": 452700
    },
    {
      "epoch": 4.131688444412001,
      "grad_norm": 4.7330732345581055,
      "learning_rate": 4.6556926296323336e-05,
      "loss": 0.7351,
      "step": 452800
    },
    {
      "epoch": 4.132600919775166,
      "grad_norm": 3.7048263549804688,
      "learning_rate": 4.6556165900187366e-05,
      "loss": 0.7237,
      "step": 452900
    },
    {
      "epoch": 4.133513395138332,
      "grad_norm": 5.066310405731201,
      "learning_rate": 4.655540550405139e-05,
      "loss": 0.6947,
      "step": 453000
    },
    {
      "epoch": 4.134425870501497,
      "grad_norm": 3.8342833518981934,
      "learning_rate": 4.6554645107915426e-05,
      "loss": 0.7666,
      "step": 453100
    },
    {
      "epoch": 4.135338345864661,
      "grad_norm": 3.470181465148926,
      "learning_rate": 4.655388471177945e-05,
      "loss": 0.7058,
      "step": 453200
    },
    {
      "epoch": 4.136250821227827,
      "grad_norm": 4.31995964050293,
      "learning_rate": 4.655312431564348e-05,
      "loss": 0.6729,
      "step": 453300
    },
    {
      "epoch": 4.137163296590992,
      "grad_norm": 3.9045627117156982,
      "learning_rate": 4.655236391950751e-05,
      "loss": 0.7234,
      "step": 453400
    },
    {
      "epoch": 4.138075771954157,
      "grad_norm": 4.025768756866455,
      "learning_rate": 4.655160352337154e-05,
      "loss": 0.6951,
      "step": 453500
    },
    {
      "epoch": 4.1389882473173225,
      "grad_norm": 4.061039924621582,
      "learning_rate": 4.655084312723557e-05,
      "loss": 0.7063,
      "step": 453600
    },
    {
      "epoch": 4.139900722680488,
      "grad_norm": 4.674797534942627,
      "learning_rate": 4.65500827310996e-05,
      "loss": 0.7262,
      "step": 453700
    },
    {
      "epoch": 4.140813198043653,
      "grad_norm": 4.242337226867676,
      "learning_rate": 4.654932233496362e-05,
      "loss": 0.6646,
      "step": 453800
    },
    {
      "epoch": 4.141725673406818,
      "grad_norm": 5.044229030609131,
      "learning_rate": 4.654856193882765e-05,
      "loss": 0.7056,
      "step": 453900
    },
    {
      "epoch": 4.142638148769983,
      "grad_norm": 3.6799614429473877,
      "learning_rate": 4.654780154269168e-05,
      "loss": 0.7189,
      "step": 454000
    },
    {
      "epoch": 4.143550624133148,
      "grad_norm": 4.038507461547852,
      "learning_rate": 4.654704114655571e-05,
      "loss": 0.748,
      "step": 454100
    },
    {
      "epoch": 4.144463099496313,
      "grad_norm": 3.5565803050994873,
      "learning_rate": 4.654628075041974e-05,
      "loss": 0.7325,
      "step": 454200
    },
    {
      "epoch": 4.145375574859479,
      "grad_norm": 3.6432838439941406,
      "learning_rate": 4.654552035428377e-05,
      "loss": 0.6896,
      "step": 454300
    },
    {
      "epoch": 4.146288050222644,
      "grad_norm": 4.305571556091309,
      "learning_rate": 4.6544759958147796e-05,
      "loss": 0.7271,
      "step": 454400
    },
    {
      "epoch": 4.147200525585809,
      "grad_norm": 3.980293035507202,
      "learning_rate": 4.6543999562011826e-05,
      "loss": 0.7413,
      "step": 454500
    },
    {
      "epoch": 4.148113000948975,
      "grad_norm": 3.3565685749053955,
      "learning_rate": 4.6543239165875856e-05,
      "loss": 0.7297,
      "step": 454600
    },
    {
      "epoch": 4.14902547631214,
      "grad_norm": 4.890064716339111,
      "learning_rate": 4.6542478769739887e-05,
      "loss": 0.7303,
      "step": 454700
    },
    {
      "epoch": 4.149937951675305,
      "grad_norm": 4.226291656494141,
      "learning_rate": 4.654171837360392e-05,
      "loss": 0.729,
      "step": 454800
    },
    {
      "epoch": 4.15085042703847,
      "grad_norm": 4.316246509552002,
      "learning_rate": 4.654095797746794e-05,
      "loss": 0.6894,
      "step": 454900
    },
    {
      "epoch": 4.151762902401635,
      "grad_norm": 5.93247127532959,
      "learning_rate": 4.654019758133198e-05,
      "loss": 0.7159,
      "step": 455000
    },
    {
      "epoch": 4.1526753777648,
      "grad_norm": 3.675546884536743,
      "learning_rate": 4.6539437185196e-05,
      "loss": 0.7103,
      "step": 455100
    },
    {
      "epoch": 4.1535878531279655,
      "grad_norm": 3.790408134460449,
      "learning_rate": 4.653867678906003e-05,
      "loss": 0.7581,
      "step": 455200
    },
    {
      "epoch": 4.154500328491131,
      "grad_norm": 3.97733998298645,
      "learning_rate": 4.653791639292406e-05,
      "loss": 0.698,
      "step": 455300
    },
    {
      "epoch": 4.155412803854296,
      "grad_norm": 4.170727729797363,
      "learning_rate": 4.653715599678809e-05,
      "loss": 0.7568,
      "step": 455400
    },
    {
      "epoch": 4.156325279217461,
      "grad_norm": 3.952105760574341,
      "learning_rate": 4.6536395600652114e-05,
      "loss": 0.7192,
      "step": 455500
    },
    {
      "epoch": 4.157237754580627,
      "grad_norm": 3.9854722023010254,
      "learning_rate": 4.653563520451615e-05,
      "loss": 0.7267,
      "step": 455600
    },
    {
      "epoch": 4.158150229943791,
      "grad_norm": 3.439004421234131,
      "learning_rate": 4.6534874808380174e-05,
      "loss": 0.6566,
      "step": 455700
    },
    {
      "epoch": 4.159062705306956,
      "grad_norm": 3.4021284580230713,
      "learning_rate": 4.6534114412244204e-05,
      "loss": 0.7496,
      "step": 455800
    },
    {
      "epoch": 4.159975180670122,
      "grad_norm": 3.8835947513580322,
      "learning_rate": 4.6533354016108234e-05,
      "loss": 0.7159,
      "step": 455900
    },
    {
      "epoch": 4.160887656033287,
      "grad_norm": 4.237144947052002,
      "learning_rate": 4.6532593619972264e-05,
      "loss": 0.7208,
      "step": 456000
    },
    {
      "epoch": 4.161800131396452,
      "grad_norm": 3.8433339595794678,
      "learning_rate": 4.6531833223836294e-05,
      "loss": 0.7092,
      "step": 456100
    },
    {
      "epoch": 4.162712606759618,
      "grad_norm": 3.5008187294006348,
      "learning_rate": 4.6531072827700324e-05,
      "loss": 0.7595,
      "step": 456200
    },
    {
      "epoch": 4.163625082122783,
      "grad_norm": 4.9448018074035645,
      "learning_rate": 4.653031243156435e-05,
      "loss": 0.7254,
      "step": 456300
    },
    {
      "epoch": 4.164537557485948,
      "grad_norm": 3.592310667037964,
      "learning_rate": 4.6529552035428384e-05,
      "loss": 0.7226,
      "step": 456400
    },
    {
      "epoch": 4.1654500328491135,
      "grad_norm": 3.6881792545318604,
      "learning_rate": 4.652879163929241e-05,
      "loss": 0.7422,
      "step": 456500
    },
    {
      "epoch": 4.166362508212278,
      "grad_norm": 4.317690372467041,
      "learning_rate": 4.652803124315644e-05,
      "loss": 0.7234,
      "step": 456600
    },
    {
      "epoch": 4.167274983575443,
      "grad_norm": 4.54337215423584,
      "learning_rate": 4.652727084702047e-05,
      "loss": 0.7197,
      "step": 456700
    },
    {
      "epoch": 4.1681874589386085,
      "grad_norm": 4.253677845001221,
      "learning_rate": 4.65265104508845e-05,
      "loss": 0.7421,
      "step": 456800
    },
    {
      "epoch": 4.169099934301774,
      "grad_norm": 4.027900218963623,
      "learning_rate": 4.652575005474852e-05,
      "loss": 0.7036,
      "step": 456900
    },
    {
      "epoch": 4.170012409664939,
      "grad_norm": 4.533304214477539,
      "learning_rate": 4.652498965861256e-05,
      "loss": 0.6761,
      "step": 457000
    },
    {
      "epoch": 4.170924885028104,
      "grad_norm": 3.455029010772705,
      "learning_rate": 4.652422926247658e-05,
      "loss": 0.7183,
      "step": 457100
    },
    {
      "epoch": 4.17183736039127,
      "grad_norm": 3.897984266281128,
      "learning_rate": 4.652346886634061e-05,
      "loss": 0.7184,
      "step": 457200
    },
    {
      "epoch": 4.172749835754435,
      "grad_norm": 4.19802188873291,
      "learning_rate": 4.652270847020464e-05,
      "loss": 0.691,
      "step": 457300
    },
    {
      "epoch": 4.173662311117599,
      "grad_norm": 5.528872013092041,
      "learning_rate": 4.6521948074068664e-05,
      "loss": 0.7377,
      "step": 457400
    },
    {
      "epoch": 4.174574786480765,
      "grad_norm": 4.216035842895508,
      "learning_rate": 4.65211876779327e-05,
      "loss": 0.7267,
      "step": 457500
    },
    {
      "epoch": 4.17548726184393,
      "grad_norm": 4.273606777191162,
      "learning_rate": 4.6520427281796725e-05,
      "loss": 0.6968,
      "step": 457600
    },
    {
      "epoch": 4.176399737207095,
      "grad_norm": 4.077249526977539,
      "learning_rate": 4.6519666885660755e-05,
      "loss": 0.7103,
      "step": 457700
    },
    {
      "epoch": 4.177312212570261,
      "grad_norm": 4.4310736656188965,
      "learning_rate": 4.6518906489524785e-05,
      "loss": 0.7064,
      "step": 457800
    },
    {
      "epoch": 4.178224687933426,
      "grad_norm": 4.231882572174072,
      "learning_rate": 4.6518146093388815e-05,
      "loss": 0.7386,
      "step": 457900
    },
    {
      "epoch": 4.179137163296591,
      "grad_norm": 3.556464433670044,
      "learning_rate": 4.651738569725284e-05,
      "loss": 0.704,
      "step": 458000
    },
    {
      "epoch": 4.1800496386597565,
      "grad_norm": 4.202817440032959,
      "learning_rate": 4.6516625301116875e-05,
      "loss": 0.7253,
      "step": 458100
    },
    {
      "epoch": 4.180962114022922,
      "grad_norm": 3.825798511505127,
      "learning_rate": 4.65158649049809e-05,
      "loss": 0.7355,
      "step": 458200
    },
    {
      "epoch": 4.181874589386086,
      "grad_norm": 4.014496326446533,
      "learning_rate": 4.651510450884493e-05,
      "loss": 0.7409,
      "step": 458300
    },
    {
      "epoch": 4.1827870647492515,
      "grad_norm": 4.918982028961182,
      "learning_rate": 4.651434411270896e-05,
      "loss": 0.7067,
      "step": 458400
    },
    {
      "epoch": 4.183699540112417,
      "grad_norm": 4.120652198791504,
      "learning_rate": 4.651358371657299e-05,
      "loss": 0.7139,
      "step": 458500
    },
    {
      "epoch": 4.184612015475582,
      "grad_norm": 4.259205341339111,
      "learning_rate": 4.651282332043702e-05,
      "loss": 0.6894,
      "step": 458600
    },
    {
      "epoch": 4.1855244908387474,
      "grad_norm": 4.525429725646973,
      "learning_rate": 4.651206292430105e-05,
      "loss": 0.7016,
      "step": 458700
    },
    {
      "epoch": 4.186436966201913,
      "grad_norm": 4.340108394622803,
      "learning_rate": 4.651130252816507e-05,
      "loss": 0.7414,
      "step": 458800
    },
    {
      "epoch": 4.187349441565078,
      "grad_norm": 4.4957356452941895,
      "learning_rate": 4.651054213202911e-05,
      "loss": 0.7346,
      "step": 458900
    },
    {
      "epoch": 4.188261916928243,
      "grad_norm": 4.45215368270874,
      "learning_rate": 4.650978173589313e-05,
      "loss": 0.7561,
      "step": 459000
    },
    {
      "epoch": 4.189174392291408,
      "grad_norm": 4.518923282623291,
      "learning_rate": 4.650902133975716e-05,
      "loss": 0.7137,
      "step": 459100
    },
    {
      "epoch": 4.190086867654573,
      "grad_norm": 4.052689075469971,
      "learning_rate": 4.650826094362119e-05,
      "loss": 0.7407,
      "step": 459200
    },
    {
      "epoch": 4.190999343017738,
      "grad_norm": 4.820469856262207,
      "learning_rate": 4.650750054748522e-05,
      "loss": 0.698,
      "step": 459300
    },
    {
      "epoch": 4.191911818380904,
      "grad_norm": 4.037633419036865,
      "learning_rate": 4.6506740151349245e-05,
      "loss": 0.7517,
      "step": 459400
    },
    {
      "epoch": 4.192824293744069,
      "grad_norm": 3.4755115509033203,
      "learning_rate": 4.650597975521328e-05,
      "loss": 0.7287,
      "step": 459500
    },
    {
      "epoch": 4.193736769107234,
      "grad_norm": 4.2644147872924805,
      "learning_rate": 4.6505219359077306e-05,
      "loss": 0.7651,
      "step": 459600
    },
    {
      "epoch": 4.1946492444703996,
      "grad_norm": 5.5220489501953125,
      "learning_rate": 4.6504458962941336e-05,
      "loss": 0.7272,
      "step": 459700
    },
    {
      "epoch": 4.195561719833565,
      "grad_norm": 3.24365234375,
      "learning_rate": 4.6503698566805366e-05,
      "loss": 0.7229,
      "step": 459800
    },
    {
      "epoch": 4.196474195196729,
      "grad_norm": 3.9069361686706543,
      "learning_rate": 4.6502938170669396e-05,
      "loss": 0.7009,
      "step": 459900
    },
    {
      "epoch": 4.197386670559895,
      "grad_norm": 5.152132034301758,
      "learning_rate": 4.6502177774533426e-05,
      "loss": 0.7335,
      "step": 460000
    },
    {
      "epoch": 4.19829914592306,
      "grad_norm": 3.997020721435547,
      "learning_rate": 4.6501417378397456e-05,
      "loss": 0.714,
      "step": 460100
    },
    {
      "epoch": 4.199211621286225,
      "grad_norm": 3.7196784019470215,
      "learning_rate": 4.650065698226148e-05,
      "loss": 0.7063,
      "step": 460200
    },
    {
      "epoch": 4.2001240966493905,
      "grad_norm": 4.571353435516357,
      "learning_rate": 4.649989658612551e-05,
      "loss": 0.7333,
      "step": 460300
    },
    {
      "epoch": 4.201036572012556,
      "grad_norm": 3.9891135692596436,
      "learning_rate": 4.649913618998954e-05,
      "loss": 0.7291,
      "step": 460400
    },
    {
      "epoch": 4.201949047375721,
      "grad_norm": 3.837181806564331,
      "learning_rate": 4.649837579385356e-05,
      "loss": 0.7431,
      "step": 460500
    },
    {
      "epoch": 4.202861522738886,
      "grad_norm": 3.7140846252441406,
      "learning_rate": 4.64976153977176e-05,
      "loss": 0.7093,
      "step": 460600
    },
    {
      "epoch": 4.203773998102052,
      "grad_norm": 4.0206804275512695,
      "learning_rate": 4.649685500158162e-05,
      "loss": 0.7411,
      "step": 460700
    },
    {
      "epoch": 4.204686473465216,
      "grad_norm": 4.319350242614746,
      "learning_rate": 4.649609460544565e-05,
      "loss": 0.7499,
      "step": 460800
    },
    {
      "epoch": 4.205598948828381,
      "grad_norm": 4.620928764343262,
      "learning_rate": 4.649533420930968e-05,
      "loss": 0.7409,
      "step": 460900
    },
    {
      "epoch": 4.206511424191547,
      "grad_norm": 3.862084150314331,
      "learning_rate": 4.649457381317371e-05,
      "loss": 0.7141,
      "step": 461000
    },
    {
      "epoch": 4.207423899554712,
      "grad_norm": 4.022789001464844,
      "learning_rate": 4.649381341703774e-05,
      "loss": 0.7256,
      "step": 461100
    },
    {
      "epoch": 4.208336374917877,
      "grad_norm": 4.90600061416626,
      "learning_rate": 4.649305302090177e-05,
      "loss": 0.7062,
      "step": 461200
    },
    {
      "epoch": 4.209248850281043,
      "grad_norm": 3.758324384689331,
      "learning_rate": 4.6492292624765796e-05,
      "loss": 0.7456,
      "step": 461300
    },
    {
      "epoch": 4.210161325644208,
      "grad_norm": 5.773505687713623,
      "learning_rate": 4.649153222862983e-05,
      "loss": 0.6867,
      "step": 461400
    },
    {
      "epoch": 4.211073801007373,
      "grad_norm": 5.556274890899658,
      "learning_rate": 4.6490771832493857e-05,
      "loss": 0.7227,
      "step": 461500
    },
    {
      "epoch": 4.211986276370538,
      "grad_norm": 4.051458358764648,
      "learning_rate": 4.649001143635789e-05,
      "loss": 0.7098,
      "step": 461600
    },
    {
      "epoch": 4.212898751733703,
      "grad_norm": 4.505665302276611,
      "learning_rate": 4.648925104022192e-05,
      "loss": 0.6971,
      "step": 461700
    },
    {
      "epoch": 4.213811227096868,
      "grad_norm": 3.7424092292785645,
      "learning_rate": 4.648849064408595e-05,
      "loss": 0.7446,
      "step": 461800
    },
    {
      "epoch": 4.2147237024600335,
      "grad_norm": 3.8896543979644775,
      "learning_rate": 4.648773024794997e-05,
      "loss": 0.6833,
      "step": 461900
    },
    {
      "epoch": 4.215636177823199,
      "grad_norm": 4.329073429107666,
      "learning_rate": 4.648696985181401e-05,
      "loss": 0.756,
      "step": 462000
    },
    {
      "epoch": 4.216548653186364,
      "grad_norm": 3.853903293609619,
      "learning_rate": 4.648620945567803e-05,
      "loss": 0.6951,
      "step": 462100
    },
    {
      "epoch": 4.217461128549529,
      "grad_norm": 4.3834381103515625,
      "learning_rate": 4.648544905954206e-05,
      "loss": 0.7064,
      "step": 462200
    },
    {
      "epoch": 4.218373603912695,
      "grad_norm": 4.208587169647217,
      "learning_rate": 4.648468866340609e-05,
      "loss": 0.7113,
      "step": 462300
    },
    {
      "epoch": 4.21928607927586,
      "grad_norm": 3.811715602874756,
      "learning_rate": 4.648392826727012e-05,
      "loss": 0.6941,
      "step": 462400
    },
    {
      "epoch": 4.220198554639024,
      "grad_norm": 3.800283432006836,
      "learning_rate": 4.648316787113415e-05,
      "loss": 0.731,
      "step": 462500
    },
    {
      "epoch": 4.22111103000219,
      "grad_norm": 4.34367036819458,
      "learning_rate": 4.648240747499818e-05,
      "loss": 0.7618,
      "step": 462600
    },
    {
      "epoch": 4.222023505365355,
      "grad_norm": 4.683267116546631,
      "learning_rate": 4.6481647078862204e-05,
      "loss": 0.7135,
      "step": 462700
    },
    {
      "epoch": 4.22293598072852,
      "grad_norm": 5.160248756408691,
      "learning_rate": 4.648088668272624e-05,
      "loss": 0.7026,
      "step": 462800
    },
    {
      "epoch": 4.223848456091686,
      "grad_norm": 4.330808639526367,
      "learning_rate": 4.6480126286590264e-05,
      "loss": 0.6875,
      "step": 462900
    },
    {
      "epoch": 4.224760931454851,
      "grad_norm": 3.033317804336548,
      "learning_rate": 4.647936589045429e-05,
      "loss": 0.7083,
      "step": 463000
    },
    {
      "epoch": 4.225673406818016,
      "grad_norm": 4.568416118621826,
      "learning_rate": 4.6478605494318324e-05,
      "loss": 0.7394,
      "step": 463100
    },
    {
      "epoch": 4.2265858821811815,
      "grad_norm": 4.542629718780518,
      "learning_rate": 4.647784509818235e-05,
      "loss": 0.7418,
      "step": 463200
    },
    {
      "epoch": 4.227498357544346,
      "grad_norm": 4.04955530166626,
      "learning_rate": 4.647708470204638e-05,
      "loss": 0.7079,
      "step": 463300
    },
    {
      "epoch": 4.228410832907511,
      "grad_norm": 3.771418333053589,
      "learning_rate": 4.647632430591041e-05,
      "loss": 0.7405,
      "step": 463400
    },
    {
      "epoch": 4.2293233082706765,
      "grad_norm": 3.81205153465271,
      "learning_rate": 4.647556390977444e-05,
      "loss": 0.6982,
      "step": 463500
    },
    {
      "epoch": 4.230235783633842,
      "grad_norm": 4.493544101715088,
      "learning_rate": 4.647480351363847e-05,
      "loss": 0.6806,
      "step": 463600
    },
    {
      "epoch": 4.231148258997007,
      "grad_norm": 3.4068777561187744,
      "learning_rate": 4.64740431175025e-05,
      "loss": 0.7246,
      "step": 463700
    },
    {
      "epoch": 4.232060734360172,
      "grad_norm": 3.0035557746887207,
      "learning_rate": 4.647328272136652e-05,
      "loss": 0.7211,
      "step": 463800
    },
    {
      "epoch": 4.232973209723338,
      "grad_norm": 4.4025959968566895,
      "learning_rate": 4.647252232523056e-05,
      "loss": 0.7668,
      "step": 463900
    },
    {
      "epoch": 4.233885685086503,
      "grad_norm": 4.50425910949707,
      "learning_rate": 4.647176192909458e-05,
      "loss": 0.7155,
      "step": 464000
    },
    {
      "epoch": 4.234798160449668,
      "grad_norm": 2.8785312175750732,
      "learning_rate": 4.647100153295861e-05,
      "loss": 0.6947,
      "step": 464100
    },
    {
      "epoch": 4.235710635812833,
      "grad_norm": 4.307662010192871,
      "learning_rate": 4.647024113682264e-05,
      "loss": 0.7247,
      "step": 464200
    },
    {
      "epoch": 4.236623111175998,
      "grad_norm": 3.961496353149414,
      "learning_rate": 4.646948074068667e-05,
      "loss": 0.7142,
      "step": 464300
    },
    {
      "epoch": 4.237535586539163,
      "grad_norm": 4.104386806488037,
      "learning_rate": 4.6468720344550695e-05,
      "loss": 0.7257,
      "step": 464400
    },
    {
      "epoch": 4.238448061902329,
      "grad_norm": 4.405999660491943,
      "learning_rate": 4.646795994841473e-05,
      "loss": 0.7122,
      "step": 464500
    },
    {
      "epoch": 4.239360537265494,
      "grad_norm": 4.055797100067139,
      "learning_rate": 4.6467199552278755e-05,
      "loss": 0.7673,
      "step": 464600
    },
    {
      "epoch": 4.240273012628659,
      "grad_norm": 4.628720760345459,
      "learning_rate": 4.6466439156142785e-05,
      "loss": 0.6964,
      "step": 464700
    },
    {
      "epoch": 4.2411854879918245,
      "grad_norm": 3.951603889465332,
      "learning_rate": 4.6465678760006815e-05,
      "loss": 0.7354,
      "step": 464800
    },
    {
      "epoch": 4.24209796335499,
      "grad_norm": 4.259721279144287,
      "learning_rate": 4.6464918363870845e-05,
      "loss": 0.711,
      "step": 464900
    },
    {
      "epoch": 4.243010438718154,
      "grad_norm": 4.299398899078369,
      "learning_rate": 4.6464157967734875e-05,
      "loss": 0.7375,
      "step": 465000
    },
    {
      "epoch": 4.2439229140813195,
      "grad_norm": 3.9217987060546875,
      "learning_rate": 4.6463397571598905e-05,
      "loss": 0.7299,
      "step": 465100
    },
    {
      "epoch": 4.244835389444485,
      "grad_norm": 3.821410894393921,
      "learning_rate": 4.646263717546293e-05,
      "loss": 0.7003,
      "step": 465200
    },
    {
      "epoch": 4.24574786480765,
      "grad_norm": 3.242408514022827,
      "learning_rate": 4.6461876779326965e-05,
      "loss": 0.7277,
      "step": 465300
    },
    {
      "epoch": 4.246660340170815,
      "grad_norm": 4.448231220245361,
      "learning_rate": 4.646111638319099e-05,
      "loss": 0.7148,
      "step": 465400
    },
    {
      "epoch": 4.247572815533981,
      "grad_norm": 3.938429832458496,
      "learning_rate": 4.646035598705502e-05,
      "loss": 0.7296,
      "step": 465500
    },
    {
      "epoch": 4.248485290897146,
      "grad_norm": 4.402093887329102,
      "learning_rate": 4.645959559091905e-05,
      "loss": 0.6975,
      "step": 465600
    },
    {
      "epoch": 4.249397766260311,
      "grad_norm": 4.700819492340088,
      "learning_rate": 4.645883519478308e-05,
      "loss": 0.7503,
      "step": 465700
    },
    {
      "epoch": 4.250310241623476,
      "grad_norm": 3.5488195419311523,
      "learning_rate": 4.645807479864711e-05,
      "loss": 0.7351,
      "step": 465800
    },
    {
      "epoch": 4.251222716986641,
      "grad_norm": 4.572600841522217,
      "learning_rate": 4.645731440251113e-05,
      "loss": 0.722,
      "step": 465900
    },
    {
      "epoch": 4.252135192349806,
      "grad_norm": 4.282411575317383,
      "learning_rate": 4.645655400637516e-05,
      "loss": 0.7387,
      "step": 466000
    },
    {
      "epoch": 4.253047667712972,
      "grad_norm": 5.905667304992676,
      "learning_rate": 4.645579361023919e-05,
      "loss": 0.7131,
      "step": 466100
    },
    {
      "epoch": 4.253960143076137,
      "grad_norm": 4.142098903656006,
      "learning_rate": 4.645503321410322e-05,
      "loss": 0.7339,
      "step": 466200
    },
    {
      "epoch": 4.254872618439302,
      "grad_norm": 4.208830833435059,
      "learning_rate": 4.6454272817967246e-05,
      "loss": 0.7078,
      "step": 466300
    },
    {
      "epoch": 4.2557850938024675,
      "grad_norm": 4.286264419555664,
      "learning_rate": 4.645351242183128e-05,
      "loss": 0.7101,
      "step": 466400
    },
    {
      "epoch": 4.256697569165633,
      "grad_norm": 4.4613189697265625,
      "learning_rate": 4.6452752025695306e-05,
      "loss": 0.7161,
      "step": 466500
    },
    {
      "epoch": 4.257610044528798,
      "grad_norm": 3.7260916233062744,
      "learning_rate": 4.6451991629559336e-05,
      "loss": 0.6645,
      "step": 466600
    },
    {
      "epoch": 4.2585225198919625,
      "grad_norm": 4.741129398345947,
      "learning_rate": 4.6451231233423366e-05,
      "loss": 0.7497,
      "step": 466700
    },
    {
      "epoch": 4.259434995255128,
      "grad_norm": 3.2985734939575195,
      "learning_rate": 4.6450470837287396e-05,
      "loss": 0.7172,
      "step": 466800
    },
    {
      "epoch": 4.260347470618293,
      "grad_norm": 4.3488922119140625,
      "learning_rate": 4.6449710441151426e-05,
      "loss": 0.6977,
      "step": 466900
    },
    {
      "epoch": 4.261259945981458,
      "grad_norm": 3.8111279010772705,
      "learning_rate": 4.6448950045015456e-05,
      "loss": 0.7257,
      "step": 467000
    },
    {
      "epoch": 4.262172421344624,
      "grad_norm": 3.430642604827881,
      "learning_rate": 4.644818964887948e-05,
      "loss": 0.714,
      "step": 467100
    },
    {
      "epoch": 4.263084896707789,
      "grad_norm": 5.419673919677734,
      "learning_rate": 4.6447429252743516e-05,
      "loss": 0.7484,
      "step": 467200
    },
    {
      "epoch": 4.263997372070954,
      "grad_norm": 3.8754959106445312,
      "learning_rate": 4.644666885660754e-05,
      "loss": 0.744,
      "step": 467300
    },
    {
      "epoch": 4.26490984743412,
      "grad_norm": 5.304737091064453,
      "learning_rate": 4.644590846047157e-05,
      "loss": 0.716,
      "step": 467400
    },
    {
      "epoch": 4.265822322797284,
      "grad_norm": 3.6809942722320557,
      "learning_rate": 4.64451480643356e-05,
      "loss": 0.7003,
      "step": 467500
    },
    {
      "epoch": 4.266734798160449,
      "grad_norm": 3.658895254135132,
      "learning_rate": 4.644438766819963e-05,
      "loss": 0.7026,
      "step": 467600
    },
    {
      "epoch": 4.267647273523615,
      "grad_norm": 4.415676593780518,
      "learning_rate": 4.644362727206365e-05,
      "loss": 0.7292,
      "step": 467700
    },
    {
      "epoch": 4.26855974888678,
      "grad_norm": 4.377502918243408,
      "learning_rate": 4.644286687592769e-05,
      "loss": 0.7067,
      "step": 467800
    },
    {
      "epoch": 4.269472224249945,
      "grad_norm": 4.525350570678711,
      "learning_rate": 4.644210647979171e-05,
      "loss": 0.7102,
      "step": 467900
    },
    {
      "epoch": 4.2703846996131105,
      "grad_norm": 3.449256181716919,
      "learning_rate": 4.644134608365574e-05,
      "loss": 0.7138,
      "step": 468000
    },
    {
      "epoch": 4.271297174976276,
      "grad_norm": 4.331998348236084,
      "learning_rate": 4.644058568751977e-05,
      "loss": 0.7434,
      "step": 468100
    },
    {
      "epoch": 4.272209650339441,
      "grad_norm": 3.772963285446167,
      "learning_rate": 4.64398252913838e-05,
      "loss": 0.7369,
      "step": 468200
    },
    {
      "epoch": 4.273122125702606,
      "grad_norm": 2.8780219554901123,
      "learning_rate": 4.6439064895247833e-05,
      "loss": 0.7279,
      "step": 468300
    },
    {
      "epoch": 4.274034601065771,
      "grad_norm": 3.6575818061828613,
      "learning_rate": 4.6438304499111863e-05,
      "loss": 0.7374,
      "step": 468400
    },
    {
      "epoch": 4.274947076428936,
      "grad_norm": 4.921983242034912,
      "learning_rate": 4.643754410297589e-05,
      "loss": 0.6965,
      "step": 468500
    },
    {
      "epoch": 4.275859551792101,
      "grad_norm": 3.3529305458068848,
      "learning_rate": 4.6436783706839924e-05,
      "loss": 0.6976,
      "step": 468600
    },
    {
      "epoch": 4.276772027155267,
      "grad_norm": 3.4128949642181396,
      "learning_rate": 4.643602331070395e-05,
      "loss": 0.7153,
      "step": 468700
    },
    {
      "epoch": 4.277684502518432,
      "grad_norm": 4.456669807434082,
      "learning_rate": 4.643526291456797e-05,
      "loss": 0.7454,
      "step": 468800
    },
    {
      "epoch": 4.278596977881597,
      "grad_norm": 4.053308010101318,
      "learning_rate": 4.643450251843201e-05,
      "loss": 0.74,
      "step": 468900
    },
    {
      "epoch": 4.279509453244763,
      "grad_norm": 3.19431734085083,
      "learning_rate": 4.643374212229603e-05,
      "loss": 0.7306,
      "step": 469000
    },
    {
      "epoch": 4.280421928607928,
      "grad_norm": 4.179652690887451,
      "learning_rate": 4.643298172616006e-05,
      "loss": 0.7355,
      "step": 469100
    },
    {
      "epoch": 4.281334403971092,
      "grad_norm": 4.017472743988037,
      "learning_rate": 4.643222133002409e-05,
      "loss": 0.6937,
      "step": 469200
    },
    {
      "epoch": 4.282246879334258,
      "grad_norm": 5.048416614532471,
      "learning_rate": 4.643146093388812e-05,
      "loss": 0.6961,
      "step": 469300
    },
    {
      "epoch": 4.283159354697423,
      "grad_norm": 2.2397897243499756,
      "learning_rate": 4.643070053775215e-05,
      "loss": 0.7344,
      "step": 469400
    },
    {
      "epoch": 4.284071830060588,
      "grad_norm": 3.5889596939086914,
      "learning_rate": 4.642994014161618e-05,
      "loss": 0.7245,
      "step": 469500
    },
    {
      "epoch": 4.2849843054237535,
      "grad_norm": 4.5370635986328125,
      "learning_rate": 4.6429179745480204e-05,
      "loss": 0.744,
      "step": 469600
    },
    {
      "epoch": 4.285896780786919,
      "grad_norm": 3.966099739074707,
      "learning_rate": 4.642841934934424e-05,
      "loss": 0.7749,
      "step": 469700
    },
    {
      "epoch": 4.286809256150084,
      "grad_norm": 4.169334411621094,
      "learning_rate": 4.6427658953208264e-05,
      "loss": 0.7346,
      "step": 469800
    },
    {
      "epoch": 4.287721731513249,
      "grad_norm": 3.7896981239318848,
      "learning_rate": 4.6426898557072294e-05,
      "loss": 0.7055,
      "step": 469900
    },
    {
      "epoch": 4.288634206876415,
      "grad_norm": 3.3627007007598877,
      "learning_rate": 4.6426138160936324e-05,
      "loss": 0.7371,
      "step": 470000
    },
    {
      "epoch": 4.289546682239579,
      "grad_norm": 4.209916591644287,
      "learning_rate": 4.6425377764800354e-05,
      "loss": 0.7116,
      "step": 470100
    },
    {
      "epoch": 4.290459157602744,
      "grad_norm": 3.519761562347412,
      "learning_rate": 4.642461736866438e-05,
      "loss": 0.7098,
      "step": 470200
    },
    {
      "epoch": 4.29137163296591,
      "grad_norm": 3.5271878242492676,
      "learning_rate": 4.6423856972528414e-05,
      "loss": 0.7085,
      "step": 470300
    },
    {
      "epoch": 4.292284108329075,
      "grad_norm": 3.030747890472412,
      "learning_rate": 4.642309657639244e-05,
      "loss": 0.7104,
      "step": 470400
    },
    {
      "epoch": 4.29319658369224,
      "grad_norm": 4.44989013671875,
      "learning_rate": 4.642233618025647e-05,
      "loss": 0.7078,
      "step": 470500
    },
    {
      "epoch": 4.294109059055406,
      "grad_norm": 4.708850383758545,
      "learning_rate": 4.64215757841205e-05,
      "loss": 0.7132,
      "step": 470600
    },
    {
      "epoch": 4.295021534418571,
      "grad_norm": 4.773194789886475,
      "learning_rate": 4.642081538798453e-05,
      "loss": 0.7117,
      "step": 470700
    },
    {
      "epoch": 4.295934009781736,
      "grad_norm": 4.017210006713867,
      "learning_rate": 4.642005499184856e-05,
      "loss": 0.7178,
      "step": 470800
    },
    {
      "epoch": 4.296846485144901,
      "grad_norm": 4.581813335418701,
      "learning_rate": 4.641929459571259e-05,
      "loss": 0.7001,
      "step": 470900
    },
    {
      "epoch": 4.297758960508066,
      "grad_norm": 4.089835166931152,
      "learning_rate": 4.641853419957661e-05,
      "loss": 0.6997,
      "step": 471000
    },
    {
      "epoch": 4.298671435871231,
      "grad_norm": 3.7124886512756348,
      "learning_rate": 4.641777380344065e-05,
      "loss": 0.7097,
      "step": 471100
    },
    {
      "epoch": 4.2995839112343965,
      "grad_norm": 4.698883533477783,
      "learning_rate": 4.641701340730467e-05,
      "loss": 0.7542,
      "step": 471200
    },
    {
      "epoch": 4.300496386597562,
      "grad_norm": 4.009435176849365,
      "learning_rate": 4.64162530111687e-05,
      "loss": 0.7226,
      "step": 471300
    },
    {
      "epoch": 4.301408861960727,
      "grad_norm": 3.631989002227783,
      "learning_rate": 4.641549261503273e-05,
      "loss": 0.7498,
      "step": 471400
    },
    {
      "epoch": 4.302321337323892,
      "grad_norm": 3.268404960632324,
      "learning_rate": 4.6414732218896755e-05,
      "loss": 0.7268,
      "step": 471500
    },
    {
      "epoch": 4.303233812687058,
      "grad_norm": 3.8265035152435303,
      "learning_rate": 4.6413971822760785e-05,
      "loss": 0.7036,
      "step": 471600
    },
    {
      "epoch": 4.304146288050223,
      "grad_norm": 3.565538167953491,
      "learning_rate": 4.6413211426624815e-05,
      "loss": 0.7086,
      "step": 471700
    },
    {
      "epoch": 4.3050587634133874,
      "grad_norm": 3.5732359886169434,
      "learning_rate": 4.6412451030488845e-05,
      "loss": 0.7236,
      "step": 471800
    },
    {
      "epoch": 4.305971238776553,
      "grad_norm": 3.7609798908233643,
      "learning_rate": 4.6411690634352875e-05,
      "loss": 0.7382,
      "step": 471900
    },
    {
      "epoch": 4.306883714139718,
      "grad_norm": 4.173635005950928,
      "learning_rate": 4.6410930238216905e-05,
      "loss": 0.7333,
      "step": 472000
    },
    {
      "epoch": 4.307796189502883,
      "grad_norm": 4.290855884552002,
      "learning_rate": 4.641016984208093e-05,
      "loss": 0.7091,
      "step": 472100
    },
    {
      "epoch": 4.308708664866049,
      "grad_norm": 3.6270813941955566,
      "learning_rate": 4.6409409445944965e-05,
      "loss": 0.7267,
      "step": 472200
    },
    {
      "epoch": 4.309621140229214,
      "grad_norm": 4.525612831115723,
      "learning_rate": 4.640864904980899e-05,
      "loss": 0.7519,
      "step": 472300
    },
    {
      "epoch": 4.310533615592379,
      "grad_norm": 3.41174578666687,
      "learning_rate": 4.640788865367302e-05,
      "loss": 0.7067,
      "step": 472400
    },
    {
      "epoch": 4.3114460909555445,
      "grad_norm": 4.070108413696289,
      "learning_rate": 4.640712825753705e-05,
      "loss": 0.7211,
      "step": 472500
    },
    {
      "epoch": 4.312358566318709,
      "grad_norm": 3.5675134658813477,
      "learning_rate": 4.640636786140108e-05,
      "loss": 0.7126,
      "step": 472600
    },
    {
      "epoch": 4.313271041681874,
      "grad_norm": 4.138730049133301,
      "learning_rate": 4.64056074652651e-05,
      "loss": 0.7148,
      "step": 472700
    },
    {
      "epoch": 4.3141835170450396,
      "grad_norm": 3.783017635345459,
      "learning_rate": 4.640484706912914e-05,
      "loss": 0.7346,
      "step": 472800
    },
    {
      "epoch": 4.315095992408205,
      "grad_norm": 4.323934078216553,
      "learning_rate": 4.640408667299316e-05,
      "loss": 0.7637,
      "step": 472900
    },
    {
      "epoch": 4.31600846777137,
      "grad_norm": 4.903764247894287,
      "learning_rate": 4.640332627685719e-05,
      "loss": 0.7209,
      "step": 473000
    },
    {
      "epoch": 4.3169209431345354,
      "grad_norm": 4.568332672119141,
      "learning_rate": 4.640256588072122e-05,
      "loss": 0.7395,
      "step": 473100
    },
    {
      "epoch": 4.317833418497701,
      "grad_norm": 3.6942622661590576,
      "learning_rate": 4.640180548458525e-05,
      "loss": 0.7116,
      "step": 473200
    },
    {
      "epoch": 4.318745893860866,
      "grad_norm": 3.775874137878418,
      "learning_rate": 4.640104508844928e-05,
      "loss": 0.7541,
      "step": 473300
    },
    {
      "epoch": 4.319658369224031,
      "grad_norm": 4.074871063232422,
      "learning_rate": 4.640028469231331e-05,
      "loss": 0.7634,
      "step": 473400
    },
    {
      "epoch": 4.320570844587196,
      "grad_norm": 3.33463716506958,
      "learning_rate": 4.6399524296177336e-05,
      "loss": 0.7282,
      "step": 473500
    },
    {
      "epoch": 4.321483319950361,
      "grad_norm": 4.017728805541992,
      "learning_rate": 4.639876390004137e-05,
      "loss": 0.7589,
      "step": 473600
    },
    {
      "epoch": 4.322395795313526,
      "grad_norm": 3.6960363388061523,
      "learning_rate": 4.6398003503905396e-05,
      "loss": 0.7297,
      "step": 473700
    },
    {
      "epoch": 4.323308270676692,
      "grad_norm": 4.22496223449707,
      "learning_rate": 4.6397243107769426e-05,
      "loss": 0.7688,
      "step": 473800
    },
    {
      "epoch": 4.324220746039857,
      "grad_norm": 3.919149875640869,
      "learning_rate": 4.6396482711633456e-05,
      "loss": 0.7084,
      "step": 473900
    },
    {
      "epoch": 4.325133221403022,
      "grad_norm": 3.978686571121216,
      "learning_rate": 4.6395722315497486e-05,
      "loss": 0.7075,
      "step": 474000
    },
    {
      "epoch": 4.3260456967661876,
      "grad_norm": 4.527106761932373,
      "learning_rate": 4.639496191936151e-05,
      "loss": 0.707,
      "step": 474100
    },
    {
      "epoch": 4.326958172129353,
      "grad_norm": 4.420523166656494,
      "learning_rate": 4.6394201523225546e-05,
      "loss": 0.709,
      "step": 474200
    },
    {
      "epoch": 4.327870647492517,
      "grad_norm": 4.4040398597717285,
      "learning_rate": 4.639344112708957e-05,
      "loss": 0.7718,
      "step": 474300
    },
    {
      "epoch": 4.328783122855683,
      "grad_norm": 5.29049015045166,
      "learning_rate": 4.63926807309536e-05,
      "loss": 0.7531,
      "step": 474400
    },
    {
      "epoch": 4.329695598218848,
      "grad_norm": 3.910494089126587,
      "learning_rate": 4.639192033481763e-05,
      "loss": 0.7358,
      "step": 474500
    },
    {
      "epoch": 4.330608073582013,
      "grad_norm": 4.091604232788086,
      "learning_rate": 4.639115993868165e-05,
      "loss": 0.7005,
      "step": 474600
    },
    {
      "epoch": 4.3315205489451785,
      "grad_norm": 4.49456262588501,
      "learning_rate": 4.639039954254569e-05,
      "loss": 0.7329,
      "step": 474700
    },
    {
      "epoch": 4.332433024308344,
      "grad_norm": 3.5042428970336914,
      "learning_rate": 4.638963914640971e-05,
      "loss": 0.7005,
      "step": 474800
    },
    {
      "epoch": 4.333345499671509,
      "grad_norm": 3.938716411590576,
      "learning_rate": 4.638887875027374e-05,
      "loss": 0.6805,
      "step": 474900
    },
    {
      "epoch": 4.334257975034674,
      "grad_norm": 4.263527870178223,
      "learning_rate": 4.638811835413777e-05,
      "loss": 0.7023,
      "step": 475000
    },
    {
      "epoch": 4.33517045039784,
      "grad_norm": 3.9672977924346924,
      "learning_rate": 4.6387357958001803e-05,
      "loss": 0.7363,
      "step": 475100
    },
    {
      "epoch": 4.336082925761004,
      "grad_norm": 4.0517497062683105,
      "learning_rate": 4.638659756186583e-05,
      "loss": 0.7054,
      "step": 475200
    },
    {
      "epoch": 4.336995401124169,
      "grad_norm": 5.181061744689941,
      "learning_rate": 4.6385837165729864e-05,
      "loss": 0.7214,
      "step": 475300
    },
    {
      "epoch": 4.337907876487335,
      "grad_norm": 4.408751964569092,
      "learning_rate": 4.638507676959389e-05,
      "loss": 0.6909,
      "step": 475400
    },
    {
      "epoch": 4.3388203518505,
      "grad_norm": 4.649965286254883,
      "learning_rate": 4.638431637345792e-05,
      "loss": 0.7394,
      "step": 475500
    },
    {
      "epoch": 4.339732827213665,
      "grad_norm": 3.582986354827881,
      "learning_rate": 4.638355597732195e-05,
      "loss": 0.7172,
      "step": 475600
    },
    {
      "epoch": 4.340645302576831,
      "grad_norm": 3.915212869644165,
      "learning_rate": 4.638279558118598e-05,
      "loss": 0.7122,
      "step": 475700
    },
    {
      "epoch": 4.341557777939996,
      "grad_norm": 4.009119987487793,
      "learning_rate": 4.638203518505001e-05,
      "loss": 0.736,
      "step": 475800
    },
    {
      "epoch": 4.342470253303161,
      "grad_norm": 3.9044976234436035,
      "learning_rate": 4.638127478891404e-05,
      "loss": 0.6977,
      "step": 475900
    },
    {
      "epoch": 4.343382728666326,
      "grad_norm": 3.855288505554199,
      "learning_rate": 4.638051439277806e-05,
      "loss": 0.7432,
      "step": 476000
    },
    {
      "epoch": 4.344295204029491,
      "grad_norm": 4.00529146194458,
      "learning_rate": 4.63797539966421e-05,
      "loss": 0.7557,
      "step": 476100
    },
    {
      "epoch": 4.345207679392656,
      "grad_norm": 3.9033212661743164,
      "learning_rate": 4.637899360050612e-05,
      "loss": 0.7343,
      "step": 476200
    },
    {
      "epoch": 4.3461201547558215,
      "grad_norm": 4.623971462249756,
      "learning_rate": 4.637823320437015e-05,
      "loss": 0.7325,
      "step": 476300
    },
    {
      "epoch": 4.347032630118987,
      "grad_norm": 4.252713203430176,
      "learning_rate": 4.637747280823418e-05,
      "loss": 0.7301,
      "step": 476400
    },
    {
      "epoch": 4.347945105482152,
      "grad_norm": 4.744644641876221,
      "learning_rate": 4.637671241209821e-05,
      "loss": 0.7477,
      "step": 476500
    },
    {
      "epoch": 4.348857580845317,
      "grad_norm": 4.520248889923096,
      "learning_rate": 4.6375952015962234e-05,
      "loss": 0.7092,
      "step": 476600
    },
    {
      "epoch": 4.349770056208483,
      "grad_norm": 5.297287464141846,
      "learning_rate": 4.637519161982627e-05,
      "loss": 0.7143,
      "step": 476700
    },
    {
      "epoch": 4.350682531571648,
      "grad_norm": 2.747622013092041,
      "learning_rate": 4.6374431223690294e-05,
      "loss": 0.733,
      "step": 476800
    },
    {
      "epoch": 4.351595006934812,
      "grad_norm": 4.183982849121094,
      "learning_rate": 4.6373670827554324e-05,
      "loss": 0.727,
      "step": 476900
    },
    {
      "epoch": 4.352507482297978,
      "grad_norm": 4.517886161804199,
      "learning_rate": 4.6372910431418354e-05,
      "loss": 0.7206,
      "step": 477000
    },
    {
      "epoch": 4.353419957661143,
      "grad_norm": 4.212321758270264,
      "learning_rate": 4.6372150035282384e-05,
      "loss": 0.7171,
      "step": 477100
    },
    {
      "epoch": 4.354332433024308,
      "grad_norm": 3.603937864303589,
      "learning_rate": 4.6371389639146415e-05,
      "loss": 0.7088,
      "step": 477200
    },
    {
      "epoch": 4.355244908387474,
      "grad_norm": 4.119752407073975,
      "learning_rate": 4.637062924301044e-05,
      "loss": 0.7221,
      "step": 477300
    },
    {
      "epoch": 4.356157383750639,
      "grad_norm": 3.8095781803131104,
      "learning_rate": 4.636986884687447e-05,
      "loss": 0.7887,
      "step": 477400
    },
    {
      "epoch": 4.357069859113804,
      "grad_norm": 3.52268648147583,
      "learning_rate": 4.63691084507385e-05,
      "loss": 0.7194,
      "step": 477500
    },
    {
      "epoch": 4.3579823344769695,
      "grad_norm": 4.02028226852417,
      "learning_rate": 4.636834805460253e-05,
      "loss": 0.722,
      "step": 477600
    },
    {
      "epoch": 4.358894809840134,
      "grad_norm": 3.583138942718506,
      "learning_rate": 4.636758765846656e-05,
      "loss": 0.7027,
      "step": 477700
    },
    {
      "epoch": 4.359807285203299,
      "grad_norm": 2.795909881591797,
      "learning_rate": 4.636682726233059e-05,
      "loss": 0.7244,
      "step": 477800
    },
    {
      "epoch": 4.3607197605664645,
      "grad_norm": 4.224195957183838,
      "learning_rate": 4.636606686619461e-05,
      "loss": 0.7346,
      "step": 477900
    },
    {
      "epoch": 4.36163223592963,
      "grad_norm": 4.226668357849121,
      "learning_rate": 4.636530647005864e-05,
      "loss": 0.7398,
      "step": 478000
    },
    {
      "epoch": 4.362544711292795,
      "grad_norm": 4.368879318237305,
      "learning_rate": 4.636454607392267e-05,
      "loss": 0.708,
      "step": 478100
    },
    {
      "epoch": 4.36345718665596,
      "grad_norm": 4.605963230133057,
      "learning_rate": 4.63637856777867e-05,
      "loss": 0.7039,
      "step": 478200
    },
    {
      "epoch": 4.364369662019126,
      "grad_norm": 3.007305145263672,
      "learning_rate": 4.636302528165073e-05,
      "loss": 0.7182,
      "step": 478300
    },
    {
      "epoch": 4.365282137382291,
      "grad_norm": 4.329961776733398,
      "learning_rate": 4.636226488551476e-05,
      "loss": 0.6987,
      "step": 478400
    },
    {
      "epoch": 4.366194612745456,
      "grad_norm": 3.920076608657837,
      "learning_rate": 4.6361504489378785e-05,
      "loss": 0.7174,
      "step": 478500
    },
    {
      "epoch": 4.367107088108621,
      "grad_norm": 4.09089994430542,
      "learning_rate": 4.636074409324282e-05,
      "loss": 0.7375,
      "step": 478600
    },
    {
      "epoch": 4.368019563471786,
      "grad_norm": 6.000385761260986,
      "learning_rate": 4.6359983697106845e-05,
      "loss": 0.7114,
      "step": 478700
    },
    {
      "epoch": 4.368932038834951,
      "grad_norm": 3.7034614086151123,
      "learning_rate": 4.6359223300970875e-05,
      "loss": 0.6928,
      "step": 478800
    },
    {
      "epoch": 4.369844514198117,
      "grad_norm": 4.040599346160889,
      "learning_rate": 4.6358462904834905e-05,
      "loss": 0.7369,
      "step": 478900
    },
    {
      "epoch": 4.370756989561282,
      "grad_norm": 2.2647500038146973,
      "learning_rate": 4.6357702508698935e-05,
      "loss": 0.7392,
      "step": 479000
    },
    {
      "epoch": 4.371669464924447,
      "grad_norm": 4.60125732421875,
      "learning_rate": 4.6356942112562965e-05,
      "loss": 0.7415,
      "step": 479100
    },
    {
      "epoch": 4.3725819402876125,
      "grad_norm": 5.33241605758667,
      "learning_rate": 4.6356181716426996e-05,
      "loss": 0.7396,
      "step": 479200
    },
    {
      "epoch": 4.373494415650778,
      "grad_norm": 3.9607861042022705,
      "learning_rate": 4.635542132029102e-05,
      "loss": 0.6804,
      "step": 479300
    },
    {
      "epoch": 4.374406891013942,
      "grad_norm": 4.4680585861206055,
      "learning_rate": 4.635466092415505e-05,
      "loss": 0.7354,
      "step": 479400
    },
    {
      "epoch": 4.3753193663771075,
      "grad_norm": 4.8518476486206055,
      "learning_rate": 4.635390052801908e-05,
      "loss": 0.688,
      "step": 479500
    },
    {
      "epoch": 4.376231841740273,
      "grad_norm": 4.388604164123535,
      "learning_rate": 4.635314013188311e-05,
      "loss": 0.7331,
      "step": 479600
    },
    {
      "epoch": 4.377144317103438,
      "grad_norm": 3.94793701171875,
      "learning_rate": 4.635237973574714e-05,
      "loss": 0.7356,
      "step": 479700
    },
    {
      "epoch": 4.378056792466603,
      "grad_norm": 3.4235639572143555,
      "learning_rate": 4.635161933961117e-05,
      "loss": 0.7161,
      "step": 479800
    },
    {
      "epoch": 4.378969267829769,
      "grad_norm": 3.478804111480713,
      "learning_rate": 4.635085894347519e-05,
      "loss": 0.7401,
      "step": 479900
    },
    {
      "epoch": 4.379881743192934,
      "grad_norm": 3.7234082221984863,
      "learning_rate": 4.635009854733922e-05,
      "loss": 0.7149,
      "step": 480000
    },
    {
      "epoch": 4.380794218556099,
      "grad_norm": 3.560908555984497,
      "learning_rate": 4.634933815120325e-05,
      "loss": 0.71,
      "step": 480100
    },
    {
      "epoch": 4.381706693919265,
      "grad_norm": 2.7686445713043213,
      "learning_rate": 4.634857775506728e-05,
      "loss": 0.7596,
      "step": 480200
    },
    {
      "epoch": 4.382619169282429,
      "grad_norm": 4.319380283355713,
      "learning_rate": 4.634781735893131e-05,
      "loss": 0.6994,
      "step": 480300
    },
    {
      "epoch": 4.383531644645594,
      "grad_norm": 4.900514602661133,
      "learning_rate": 4.6347056962795336e-05,
      "loss": 0.7016,
      "step": 480400
    },
    {
      "epoch": 4.38444412000876,
      "grad_norm": 4.173848628997803,
      "learning_rate": 4.634629656665937e-05,
      "loss": 0.7027,
      "step": 480500
    },
    {
      "epoch": 4.385356595371925,
      "grad_norm": 4.423178195953369,
      "learning_rate": 4.6345536170523396e-05,
      "loss": 0.7177,
      "step": 480600
    },
    {
      "epoch": 4.38626907073509,
      "grad_norm": 3.6370534896850586,
      "learning_rate": 4.6344775774387426e-05,
      "loss": 0.7059,
      "step": 480700
    },
    {
      "epoch": 4.3871815460982555,
      "grad_norm": 4.125079154968262,
      "learning_rate": 4.6344015378251456e-05,
      "loss": 0.7076,
      "step": 480800
    },
    {
      "epoch": 4.388094021461421,
      "grad_norm": 4.088555335998535,
      "learning_rate": 4.6343254982115486e-05,
      "loss": 0.7128,
      "step": 480900
    },
    {
      "epoch": 4.389006496824586,
      "grad_norm": 4.217007637023926,
      "learning_rate": 4.634249458597951e-05,
      "loss": 0.72,
      "step": 481000
    },
    {
      "epoch": 4.3899189721877505,
      "grad_norm": 3.7116825580596924,
      "learning_rate": 4.6341734189843546e-05,
      "loss": 0.6946,
      "step": 481100
    },
    {
      "epoch": 4.390831447550916,
      "grad_norm": 4.020834922790527,
      "learning_rate": 4.634097379370757e-05,
      "loss": 0.7682,
      "step": 481200
    },
    {
      "epoch": 4.391743922914081,
      "grad_norm": 3.2789947986602783,
      "learning_rate": 4.63402133975716e-05,
      "loss": 0.7295,
      "step": 481300
    },
    {
      "epoch": 4.392656398277246,
      "grad_norm": 4.425033092498779,
      "learning_rate": 4.633945300143563e-05,
      "loss": 0.7145,
      "step": 481400
    },
    {
      "epoch": 4.393568873640412,
      "grad_norm": 4.185717582702637,
      "learning_rate": 4.633869260529966e-05,
      "loss": 0.7604,
      "step": 481500
    },
    {
      "epoch": 4.394481349003577,
      "grad_norm": 3.860534191131592,
      "learning_rate": 4.633793220916369e-05,
      "loss": 0.7481,
      "step": 481600
    },
    {
      "epoch": 4.395393824366742,
      "grad_norm": 4.666167259216309,
      "learning_rate": 4.633717181302772e-05,
      "loss": 0.69,
      "step": 481700
    },
    {
      "epoch": 4.396306299729908,
      "grad_norm": 4.997354507446289,
      "learning_rate": 4.633641141689174e-05,
      "loss": 0.7341,
      "step": 481800
    },
    {
      "epoch": 4.397218775093073,
      "grad_norm": 4.241024017333984,
      "learning_rate": 4.633565102075578e-05,
      "loss": 0.6784,
      "step": 481900
    },
    {
      "epoch": 4.398131250456237,
      "grad_norm": 3.5575153827667236,
      "learning_rate": 4.6334890624619804e-05,
      "loss": 0.6805,
      "step": 482000
    },
    {
      "epoch": 4.399043725819403,
      "grad_norm": 4.193514823913574,
      "learning_rate": 4.6334130228483834e-05,
      "loss": 0.7029,
      "step": 482100
    },
    {
      "epoch": 4.399956201182568,
      "grad_norm": 3.766756772994995,
      "learning_rate": 4.6333369832347864e-05,
      "loss": 0.6973,
      "step": 482200
    },
    {
      "epoch": 4.400868676545733,
      "grad_norm": 4.32313346862793,
      "learning_rate": 4.6332609436211894e-05,
      "loss": 0.7554,
      "step": 482300
    },
    {
      "epoch": 4.4017811519088985,
      "grad_norm": 3.7226369380950928,
      "learning_rate": 4.633184904007592e-05,
      "loss": 0.7065,
      "step": 482400
    },
    {
      "epoch": 4.402693627272064,
      "grad_norm": 3.657128095626831,
      "learning_rate": 4.6331088643939954e-05,
      "loss": 0.6906,
      "step": 482500
    },
    {
      "epoch": 4.403606102635229,
      "grad_norm": 4.21638298034668,
      "learning_rate": 4.633032824780398e-05,
      "loss": 0.7276,
      "step": 482600
    },
    {
      "epoch": 4.404518577998394,
      "grad_norm": 3.42168927192688,
      "learning_rate": 4.632956785166801e-05,
      "loss": 0.7009,
      "step": 482700
    },
    {
      "epoch": 4.405431053361559,
      "grad_norm": 3.2386715412139893,
      "learning_rate": 4.632880745553204e-05,
      "loss": 0.7425,
      "step": 482800
    },
    {
      "epoch": 4.406343528724724,
      "grad_norm": 4.441122055053711,
      "learning_rate": 4.632804705939606e-05,
      "loss": 0.7254,
      "step": 482900
    },
    {
      "epoch": 4.407256004087889,
      "grad_norm": 3.632640838623047,
      "learning_rate": 4.63272866632601e-05,
      "loss": 0.7333,
      "step": 483000
    },
    {
      "epoch": 4.408168479451055,
      "grad_norm": 3.5745885372161865,
      "learning_rate": 4.632652626712412e-05,
      "loss": 0.7399,
      "step": 483100
    },
    {
      "epoch": 4.40908095481422,
      "grad_norm": 4.220837593078613,
      "learning_rate": 4.632576587098815e-05,
      "loss": 0.7212,
      "step": 483200
    },
    {
      "epoch": 4.409993430177385,
      "grad_norm": 5.03891134262085,
      "learning_rate": 4.632500547485218e-05,
      "loss": 0.6965,
      "step": 483300
    },
    {
      "epoch": 4.410905905540551,
      "grad_norm": 5.120007038116455,
      "learning_rate": 4.632424507871621e-05,
      "loss": 0.7305,
      "step": 483400
    },
    {
      "epoch": 4.411818380903716,
      "grad_norm": 4.451909065246582,
      "learning_rate": 4.6323484682580234e-05,
      "loss": 0.702,
      "step": 483500
    },
    {
      "epoch": 4.412730856266881,
      "grad_norm": 4.2483811378479,
      "learning_rate": 4.632272428644427e-05,
      "loss": 0.7193,
      "step": 483600
    },
    {
      "epoch": 4.413643331630046,
      "grad_norm": 2.3052780628204346,
      "learning_rate": 4.6321963890308294e-05,
      "loss": 0.7014,
      "step": 483700
    },
    {
      "epoch": 4.414555806993211,
      "grad_norm": 3.8242318630218506,
      "learning_rate": 4.6321203494172324e-05,
      "loss": 0.7199,
      "step": 483800
    },
    {
      "epoch": 4.415468282356376,
      "grad_norm": 3.604576826095581,
      "learning_rate": 4.6320443098036354e-05,
      "loss": 0.7186,
      "step": 483900
    },
    {
      "epoch": 4.4163807577195415,
      "grad_norm": 4.846231460571289,
      "learning_rate": 4.6319682701900385e-05,
      "loss": 0.7444,
      "step": 484000
    },
    {
      "epoch": 4.417293233082707,
      "grad_norm": 3.816805601119995,
      "learning_rate": 4.6318922305764415e-05,
      "loss": 0.7366,
      "step": 484100
    },
    {
      "epoch": 4.418205708445872,
      "grad_norm": 3.679267644882202,
      "learning_rate": 4.6318161909628445e-05,
      "loss": 0.7193,
      "step": 484200
    },
    {
      "epoch": 4.419118183809037,
      "grad_norm": 4.553928852081299,
      "learning_rate": 4.631740151349247e-05,
      "loss": 0.7181,
      "step": 484300
    },
    {
      "epoch": 4.420030659172203,
      "grad_norm": 4.404020309448242,
      "learning_rate": 4.6316641117356505e-05,
      "loss": 0.7128,
      "step": 484400
    },
    {
      "epoch": 4.420943134535367,
      "grad_norm": 4.005002021789551,
      "learning_rate": 4.631588072122053e-05,
      "loss": 0.7757,
      "step": 484500
    },
    {
      "epoch": 4.421855609898532,
      "grad_norm": 3.5017664432525635,
      "learning_rate": 4.631512032508456e-05,
      "loss": 0.7422,
      "step": 484600
    },
    {
      "epoch": 4.422768085261698,
      "grad_norm": 3.350158929824829,
      "learning_rate": 4.631435992894859e-05,
      "loss": 0.7097,
      "step": 484700
    },
    {
      "epoch": 4.423680560624863,
      "grad_norm": 3.4665794372558594,
      "learning_rate": 4.631359953281262e-05,
      "loss": 0.7364,
      "step": 484800
    },
    {
      "epoch": 4.424593035988028,
      "grad_norm": 3.791776418685913,
      "learning_rate": 4.631283913667664e-05,
      "loss": 0.7212,
      "step": 484900
    },
    {
      "epoch": 4.425505511351194,
      "grad_norm": 4.887657165527344,
      "learning_rate": 4.631207874054068e-05,
      "loss": 0.7244,
      "step": 485000
    },
    {
      "epoch": 4.426417986714359,
      "grad_norm": 4.395269870758057,
      "learning_rate": 4.63113183444047e-05,
      "loss": 0.7813,
      "step": 485100
    },
    {
      "epoch": 4.427330462077524,
      "grad_norm": 4.15872049331665,
      "learning_rate": 4.631055794826873e-05,
      "loss": 0.7268,
      "step": 485200
    },
    {
      "epoch": 4.4282429374406895,
      "grad_norm": 3.8539652824401855,
      "learning_rate": 4.630979755213276e-05,
      "loss": 0.7262,
      "step": 485300
    },
    {
      "epoch": 4.429155412803854,
      "grad_norm": 4.2334113121032715,
      "learning_rate": 4.630903715599679e-05,
      "loss": 0.7521,
      "step": 485400
    },
    {
      "epoch": 4.430067888167019,
      "grad_norm": 3.6388111114501953,
      "learning_rate": 4.630827675986082e-05,
      "loss": 0.7146,
      "step": 485500
    },
    {
      "epoch": 4.4309803635301845,
      "grad_norm": 4.545450687408447,
      "learning_rate": 4.630751636372485e-05,
      "loss": 0.7056,
      "step": 485600
    },
    {
      "epoch": 4.43189283889335,
      "grad_norm": 4.656689643859863,
      "learning_rate": 4.6306755967588875e-05,
      "loss": 0.7129,
      "step": 485700
    },
    {
      "epoch": 4.432805314256515,
      "grad_norm": 4.931199550628662,
      "learning_rate": 4.6305995571452905e-05,
      "loss": 0.7304,
      "step": 485800
    },
    {
      "epoch": 4.43371778961968,
      "grad_norm": 4.745253562927246,
      "learning_rate": 4.6305235175316935e-05,
      "loss": 0.7127,
      "step": 485900
    },
    {
      "epoch": 4.434630264982846,
      "grad_norm": 3.462498426437378,
      "learning_rate": 4.630447477918096e-05,
      "loss": 0.7234,
      "step": 486000
    },
    {
      "epoch": 4.435542740346011,
      "grad_norm": 3.6745259761810303,
      "learning_rate": 4.6303714383044996e-05,
      "loss": 0.766,
      "step": 486100
    },
    {
      "epoch": 4.4364552157091754,
      "grad_norm": 4.681110382080078,
      "learning_rate": 4.630295398690902e-05,
      "loss": 0.7195,
      "step": 486200
    },
    {
      "epoch": 4.437367691072341,
      "grad_norm": 3.81838321685791,
      "learning_rate": 4.630219359077305e-05,
      "loss": 0.6774,
      "step": 486300
    },
    {
      "epoch": 4.438280166435506,
      "grad_norm": 4.273909568786621,
      "learning_rate": 4.630143319463708e-05,
      "loss": 0.7191,
      "step": 486400
    },
    {
      "epoch": 4.439192641798671,
      "grad_norm": 4.6818437576293945,
      "learning_rate": 4.630067279850111e-05,
      "loss": 0.6958,
      "step": 486500
    },
    {
      "epoch": 4.440105117161837,
      "grad_norm": 4.387059211730957,
      "learning_rate": 4.629991240236514e-05,
      "loss": 0.7082,
      "step": 486600
    },
    {
      "epoch": 4.441017592525002,
      "grad_norm": 3.945769786834717,
      "learning_rate": 4.629915200622917e-05,
      "loss": 0.7231,
      "step": 486700
    },
    {
      "epoch": 4.441930067888167,
      "grad_norm": 4.442109107971191,
      "learning_rate": 4.629839161009319e-05,
      "loss": 0.6782,
      "step": 486800
    },
    {
      "epoch": 4.4428425432513325,
      "grad_norm": 2.3703629970550537,
      "learning_rate": 4.629763121395723e-05,
      "loss": 0.6953,
      "step": 486900
    },
    {
      "epoch": 4.443755018614498,
      "grad_norm": 3.7769274711608887,
      "learning_rate": 4.629687081782125e-05,
      "loss": 0.739,
      "step": 487000
    },
    {
      "epoch": 4.444667493977662,
      "grad_norm": 4.410767555236816,
      "learning_rate": 4.629611042168528e-05,
      "loss": 0.7025,
      "step": 487100
    },
    {
      "epoch": 4.4455799693408276,
      "grad_norm": 3.491272211074829,
      "learning_rate": 4.629535002554931e-05,
      "loss": 0.7421,
      "step": 487200
    },
    {
      "epoch": 4.446492444703993,
      "grad_norm": 4.178631782531738,
      "learning_rate": 4.629458962941334e-05,
      "loss": 0.7214,
      "step": 487300
    },
    {
      "epoch": 4.447404920067158,
      "grad_norm": 4.67578125,
      "learning_rate": 4.6293829233277366e-05,
      "loss": 0.7101,
      "step": 487400
    },
    {
      "epoch": 4.4483173954303235,
      "grad_norm": 4.433011054992676,
      "learning_rate": 4.62930688371414e-05,
      "loss": 0.6917,
      "step": 487500
    },
    {
      "epoch": 4.449229870793489,
      "grad_norm": 3.7377142906188965,
      "learning_rate": 4.6292308441005426e-05,
      "loss": 0.7306,
      "step": 487600
    },
    {
      "epoch": 4.450142346156654,
      "grad_norm": 3.8939173221588135,
      "learning_rate": 4.6291548044869456e-05,
      "loss": 0.6989,
      "step": 487700
    },
    {
      "epoch": 4.451054821519819,
      "grad_norm": 3.119804620742798,
      "learning_rate": 4.6290787648733486e-05,
      "loss": 0.7421,
      "step": 487800
    },
    {
      "epoch": 4.451967296882984,
      "grad_norm": 3.5807485580444336,
      "learning_rate": 4.6290027252597517e-05,
      "loss": 0.6869,
      "step": 487900
    },
    {
      "epoch": 4.452879772246149,
      "grad_norm": 3.982783079147339,
      "learning_rate": 4.6289266856461547e-05,
      "loss": 0.7218,
      "step": 488000
    },
    {
      "epoch": 4.453792247609314,
      "grad_norm": 5.7115631103515625,
      "learning_rate": 4.628850646032558e-05,
      "loss": 0.7206,
      "step": 488100
    },
    {
      "epoch": 4.45470472297248,
      "grad_norm": 2.3931570053100586,
      "learning_rate": 4.62877460641896e-05,
      "loss": 0.7199,
      "step": 488200
    },
    {
      "epoch": 4.455617198335645,
      "grad_norm": 3.7148184776306152,
      "learning_rate": 4.628698566805364e-05,
      "loss": 0.7102,
      "step": 488300
    },
    {
      "epoch": 4.45652967369881,
      "grad_norm": 3.6199231147766113,
      "learning_rate": 4.628622527191766e-05,
      "loss": 0.6914,
      "step": 488400
    },
    {
      "epoch": 4.457442149061976,
      "grad_norm": 4.656314849853516,
      "learning_rate": 4.628546487578169e-05,
      "loss": 0.7268,
      "step": 488500
    },
    {
      "epoch": 4.458354624425141,
      "grad_norm": 4.444355010986328,
      "learning_rate": 4.628470447964572e-05,
      "loss": 0.7279,
      "step": 488600
    },
    {
      "epoch": 4.459267099788306,
      "grad_norm": 3.573690176010132,
      "learning_rate": 4.6283944083509743e-05,
      "loss": 0.7508,
      "step": 488700
    },
    {
      "epoch": 4.460179575151471,
      "grad_norm": 4.272874355316162,
      "learning_rate": 4.6283183687373774e-05,
      "loss": 0.7338,
      "step": 488800
    },
    {
      "epoch": 4.461092050514636,
      "grad_norm": 4.241148471832275,
      "learning_rate": 4.6282423291237804e-05,
      "loss": 0.743,
      "step": 488900
    },
    {
      "epoch": 4.462004525877801,
      "grad_norm": 4.064316272735596,
      "learning_rate": 4.6281662895101834e-05,
      "loss": 0.7044,
      "step": 489000
    },
    {
      "epoch": 4.4629170012409665,
      "grad_norm": 3.876570224761963,
      "learning_rate": 4.6280902498965864e-05,
      "loss": 0.7849,
      "step": 489100
    },
    {
      "epoch": 4.463829476604132,
      "grad_norm": 4.507420063018799,
      "learning_rate": 4.6280142102829894e-05,
      "loss": 0.6957,
      "step": 489200
    },
    {
      "epoch": 4.464741951967297,
      "grad_norm": 3.6834282875061035,
      "learning_rate": 4.627938170669392e-05,
      "loss": 0.7075,
      "step": 489300
    },
    {
      "epoch": 4.465654427330462,
      "grad_norm": 4.331371784210205,
      "learning_rate": 4.6278621310557954e-05,
      "loss": 0.7507,
      "step": 489400
    },
    {
      "epoch": 4.466566902693628,
      "grad_norm": 4.173430442810059,
      "learning_rate": 4.627786091442198e-05,
      "loss": 0.6884,
      "step": 489500
    },
    {
      "epoch": 4.467479378056792,
      "grad_norm": 4.677781581878662,
      "learning_rate": 4.627710051828601e-05,
      "loss": 0.737,
      "step": 489600
    },
    {
      "epoch": 4.468391853419957,
      "grad_norm": 3.7748262882232666,
      "learning_rate": 4.627634012215004e-05,
      "loss": 0.7319,
      "step": 489700
    },
    {
      "epoch": 4.469304328783123,
      "grad_norm": 3.632902145385742,
      "learning_rate": 4.627557972601407e-05,
      "loss": 0.7066,
      "step": 489800
    },
    {
      "epoch": 4.470216804146288,
      "grad_norm": 4.980330944061279,
      "learning_rate": 4.627481932987809e-05,
      "loss": 0.7093,
      "step": 489900
    },
    {
      "epoch": 4.471129279509453,
      "grad_norm": 3.78434157371521,
      "learning_rate": 4.627405893374213e-05,
      "loss": 0.7282,
      "step": 490000
    },
    {
      "epoch": 4.472041754872619,
      "grad_norm": 4.102535247802734,
      "learning_rate": 4.627329853760615e-05,
      "loss": 0.7189,
      "step": 490100
    },
    {
      "epoch": 4.472954230235784,
      "grad_norm": 3.101747989654541,
      "learning_rate": 4.627253814147018e-05,
      "loss": 0.7299,
      "step": 490200
    },
    {
      "epoch": 4.473866705598949,
      "grad_norm": 3.2046549320220947,
      "learning_rate": 4.627177774533421e-05,
      "loss": 0.722,
      "step": 490300
    },
    {
      "epoch": 4.4747791809621145,
      "grad_norm": 3.9692602157592773,
      "learning_rate": 4.627101734919824e-05,
      "loss": 0.7377,
      "step": 490400
    },
    {
      "epoch": 4.475691656325279,
      "grad_norm": 3.7315633296966553,
      "learning_rate": 4.627025695306227e-05,
      "loss": 0.7439,
      "step": 490500
    },
    {
      "epoch": 4.476604131688444,
      "grad_norm": 4.786585807800293,
      "learning_rate": 4.62694965569263e-05,
      "loss": 0.7206,
      "step": 490600
    },
    {
      "epoch": 4.4775166070516095,
      "grad_norm": 5.510951995849609,
      "learning_rate": 4.6268736160790325e-05,
      "loss": 0.708,
      "step": 490700
    },
    {
      "epoch": 4.478429082414775,
      "grad_norm": 3.8139357566833496,
      "learning_rate": 4.626797576465436e-05,
      "loss": 0.7092,
      "step": 490800
    },
    {
      "epoch": 4.47934155777794,
      "grad_norm": 3.5336616039276123,
      "learning_rate": 4.6267215368518385e-05,
      "loss": 0.6958,
      "step": 490900
    },
    {
      "epoch": 4.480254033141105,
      "grad_norm": 2.6737046241760254,
      "learning_rate": 4.6266454972382415e-05,
      "loss": 0.7515,
      "step": 491000
    },
    {
      "epoch": 4.481166508504271,
      "grad_norm": 4.281325817108154,
      "learning_rate": 4.6265694576246445e-05,
      "loss": 0.7293,
      "step": 491100
    },
    {
      "epoch": 4.482078983867436,
      "grad_norm": 4.169554233551025,
      "learning_rate": 4.6264934180110475e-05,
      "loss": 0.6753,
      "step": 491200
    },
    {
      "epoch": 4.4829914592306,
      "grad_norm": 3.5872037410736084,
      "learning_rate": 4.6264173783974505e-05,
      "loss": 0.7535,
      "step": 491300
    },
    {
      "epoch": 4.483903934593766,
      "grad_norm": 4.62933349609375,
      "learning_rate": 4.626341338783853e-05,
      "loss": 0.7395,
      "step": 491400
    },
    {
      "epoch": 4.484816409956931,
      "grad_norm": 4.454091548919678,
      "learning_rate": 4.626265299170256e-05,
      "loss": 0.7685,
      "step": 491500
    },
    {
      "epoch": 4.485728885320096,
      "grad_norm": 3.358610153198242,
      "learning_rate": 4.626189259556659e-05,
      "loss": 0.6798,
      "step": 491600
    },
    {
      "epoch": 4.486641360683262,
      "grad_norm": 4.723992824554443,
      "learning_rate": 4.626113219943062e-05,
      "loss": 0.7331,
      "step": 491700
    },
    {
      "epoch": 4.487553836046427,
      "grad_norm": 5.316364765167236,
      "learning_rate": 4.626037180329464e-05,
      "loss": 0.6776,
      "step": 491800
    },
    {
      "epoch": 4.488466311409592,
      "grad_norm": 4.166025161743164,
      "learning_rate": 4.625961140715868e-05,
      "loss": 0.7013,
      "step": 491900
    },
    {
      "epoch": 4.4893787867727575,
      "grad_norm": 4.7746405601501465,
      "learning_rate": 4.62588510110227e-05,
      "loss": 0.7049,
      "step": 492000
    },
    {
      "epoch": 4.490291262135923,
      "grad_norm": 2.594114065170288,
      "learning_rate": 4.625809061488673e-05,
      "loss": 0.7349,
      "step": 492100
    },
    {
      "epoch": 4.491203737499087,
      "grad_norm": 4.628129482269287,
      "learning_rate": 4.625733021875076e-05,
      "loss": 0.7113,
      "step": 492200
    },
    {
      "epoch": 4.4921162128622525,
      "grad_norm": 5.28493595123291,
      "learning_rate": 4.625656982261479e-05,
      "loss": 0.7155,
      "step": 492300
    },
    {
      "epoch": 4.493028688225418,
      "grad_norm": 4.4526472091674805,
      "learning_rate": 4.625580942647882e-05,
      "loss": 0.7174,
      "step": 492400
    },
    {
      "epoch": 4.493941163588583,
      "grad_norm": 3.5822348594665527,
      "learning_rate": 4.625504903034285e-05,
      "loss": 0.7058,
      "step": 492500
    },
    {
      "epoch": 4.494853638951748,
      "grad_norm": 4.604287147521973,
      "learning_rate": 4.6254288634206875e-05,
      "loss": 0.736,
      "step": 492600
    },
    {
      "epoch": 4.495766114314914,
      "grad_norm": 4.57342004776001,
      "learning_rate": 4.625352823807091e-05,
      "loss": 0.7195,
      "step": 492700
    },
    {
      "epoch": 4.496678589678079,
      "grad_norm": 4.629848480224609,
      "learning_rate": 4.6252767841934936e-05,
      "loss": 0.7076,
      "step": 492800
    },
    {
      "epoch": 4.497591065041244,
      "grad_norm": 4.497910499572754,
      "learning_rate": 4.6252007445798966e-05,
      "loss": 0.7099,
      "step": 492900
    },
    {
      "epoch": 4.498503540404409,
      "grad_norm": 3.4475884437561035,
      "learning_rate": 4.6251247049662996e-05,
      "loss": 0.7117,
      "step": 493000
    },
    {
      "epoch": 4.499416015767574,
      "grad_norm": 4.002434253692627,
      "learning_rate": 4.6250486653527026e-05,
      "loss": 0.7112,
      "step": 493100
    },
    {
      "epoch": 4.500328491130739,
      "grad_norm": 3.1868128776550293,
      "learning_rate": 4.624972625739105e-05,
      "loss": 0.7067,
      "step": 493200
    },
    {
      "epoch": 4.501240966493905,
      "grad_norm": 3.473120927810669,
      "learning_rate": 4.6248965861255086e-05,
      "loss": 0.731,
      "step": 493300
    },
    {
      "epoch": 4.50215344185707,
      "grad_norm": 4.092675685882568,
      "learning_rate": 4.624820546511911e-05,
      "loss": 0.7339,
      "step": 493400
    },
    {
      "epoch": 4.503065917220235,
      "grad_norm": 4.947376251220703,
      "learning_rate": 4.624744506898314e-05,
      "loss": 0.6668,
      "step": 493500
    },
    {
      "epoch": 4.5039783925834005,
      "grad_norm": 3.729579448699951,
      "learning_rate": 4.624668467284717e-05,
      "loss": 0.6966,
      "step": 493600
    },
    {
      "epoch": 4.504890867946566,
      "grad_norm": 4.70711088180542,
      "learning_rate": 4.62459242767112e-05,
      "loss": 0.7348,
      "step": 493700
    },
    {
      "epoch": 4.505803343309731,
      "grad_norm": 4.568352699279785,
      "learning_rate": 4.624516388057523e-05,
      "loss": 0.7169,
      "step": 493800
    },
    {
      "epoch": 4.5067158186728955,
      "grad_norm": 3.589188814163208,
      "learning_rate": 4.624440348443926e-05,
      "loss": 0.6753,
      "step": 493900
    },
    {
      "epoch": 4.507628294036061,
      "grad_norm": 3.5190014839172363,
      "learning_rate": 4.624364308830328e-05,
      "loss": 0.6896,
      "step": 494000
    },
    {
      "epoch": 4.508540769399226,
      "grad_norm": 3.987668991088867,
      "learning_rate": 4.624288269216732e-05,
      "loss": 0.7097,
      "step": 494100
    },
    {
      "epoch": 4.509453244762391,
      "grad_norm": 3.313992977142334,
      "learning_rate": 4.624212229603134e-05,
      "loss": 0.7305,
      "step": 494200
    },
    {
      "epoch": 4.510365720125557,
      "grad_norm": 5.588449478149414,
      "learning_rate": 4.6241361899895366e-05,
      "loss": 0.7616,
      "step": 494300
    },
    {
      "epoch": 4.511278195488722,
      "grad_norm": 3.8733129501342773,
      "learning_rate": 4.62406015037594e-05,
      "loss": 0.7035,
      "step": 494400
    },
    {
      "epoch": 4.512190670851887,
      "grad_norm": 3.5690605640411377,
      "learning_rate": 4.6239841107623426e-05,
      "loss": 0.6903,
      "step": 494500
    },
    {
      "epoch": 4.513103146215052,
      "grad_norm": 4.766720294952393,
      "learning_rate": 4.6239080711487456e-05,
      "loss": 0.7545,
      "step": 494600
    },
    {
      "epoch": 4.514015621578217,
      "grad_norm": 5.202284812927246,
      "learning_rate": 4.6238320315351487e-05,
      "loss": 0.7083,
      "step": 494700
    },
    {
      "epoch": 4.514928096941382,
      "grad_norm": 2.1883127689361572,
      "learning_rate": 4.6237559919215517e-05,
      "loss": 0.7146,
      "step": 494800
    },
    {
      "epoch": 4.515840572304548,
      "grad_norm": 4.37965726852417,
      "learning_rate": 4.623679952307955e-05,
      "loss": 0.715,
      "step": 494900
    },
    {
      "epoch": 4.516753047667713,
      "grad_norm": 3.9581615924835205,
      "learning_rate": 4.623603912694358e-05,
      "loss": 0.7467,
      "step": 495000
    },
    {
      "epoch": 4.517665523030878,
      "grad_norm": 4.932619571685791,
      "learning_rate": 4.62352787308076e-05,
      "loss": 0.7122,
      "step": 495100
    },
    {
      "epoch": 4.5185779983940435,
      "grad_norm": 5.331919193267822,
      "learning_rate": 4.623451833467164e-05,
      "loss": 0.6667,
      "step": 495200
    },
    {
      "epoch": 4.519490473757209,
      "grad_norm": 3.91249680519104,
      "learning_rate": 4.623375793853566e-05,
      "loss": 0.7171,
      "step": 495300
    },
    {
      "epoch": 4.520402949120374,
      "grad_norm": 4.78994083404541,
      "learning_rate": 4.623299754239969e-05,
      "loss": 0.7068,
      "step": 495400
    },
    {
      "epoch": 4.521315424483539,
      "grad_norm": 5.514134883880615,
      "learning_rate": 4.623223714626372e-05,
      "loss": 0.7159,
      "step": 495500
    },
    {
      "epoch": 4.522227899846704,
      "grad_norm": 4.325395107269287,
      "learning_rate": 4.623147675012775e-05,
      "loss": 0.7584,
      "step": 495600
    },
    {
      "epoch": 4.523140375209869,
      "grad_norm": 4.185732841491699,
      "learning_rate": 4.6230716353991774e-05,
      "loss": 0.7317,
      "step": 495700
    },
    {
      "epoch": 4.524052850573034,
      "grad_norm": 3.5800817012786865,
      "learning_rate": 4.622995595785581e-05,
      "loss": 0.7692,
      "step": 495800
    },
    {
      "epoch": 4.5249653259362,
      "grad_norm": 4.441104412078857,
      "learning_rate": 4.6229195561719834e-05,
      "loss": 0.7194,
      "step": 495900
    },
    {
      "epoch": 4.525877801299365,
      "grad_norm": 4.118045330047607,
      "learning_rate": 4.6228435165583864e-05,
      "loss": 0.7352,
      "step": 496000
    },
    {
      "epoch": 4.52679027666253,
      "grad_norm": 3.158665657043457,
      "learning_rate": 4.6227674769447894e-05,
      "loss": 0.7144,
      "step": 496100
    },
    {
      "epoch": 4.527702752025696,
      "grad_norm": 3.157935619354248,
      "learning_rate": 4.6226914373311924e-05,
      "loss": 0.7581,
      "step": 496200
    },
    {
      "epoch": 4.52861522738886,
      "grad_norm": 3.5534496307373047,
      "learning_rate": 4.6226153977175954e-05,
      "loss": 0.6993,
      "step": 496300
    },
    {
      "epoch": 4.529527702752025,
      "grad_norm": 4.181033611297607,
      "learning_rate": 4.6225393581039984e-05,
      "loss": 0.6947,
      "step": 496400
    },
    {
      "epoch": 4.530440178115191,
      "grad_norm": 4.0189313888549805,
      "learning_rate": 4.622463318490401e-05,
      "loss": 0.7342,
      "step": 496500
    },
    {
      "epoch": 4.531352653478356,
      "grad_norm": 4.408524036407471,
      "learning_rate": 4.6223872788768044e-05,
      "loss": 0.7059,
      "step": 496600
    },
    {
      "epoch": 4.532265128841521,
      "grad_norm": 4.426647186279297,
      "learning_rate": 4.622311239263207e-05,
      "loss": 0.7278,
      "step": 496700
    },
    {
      "epoch": 4.5331776042046865,
      "grad_norm": 4.17750358581543,
      "learning_rate": 4.62223519964961e-05,
      "loss": 0.6707,
      "step": 496800
    },
    {
      "epoch": 4.534090079567852,
      "grad_norm": 2.9887988567352295,
      "learning_rate": 4.622159160036013e-05,
      "loss": 0.7326,
      "step": 496900
    },
    {
      "epoch": 4.535002554931017,
      "grad_norm": 3.7471745014190674,
      "learning_rate": 4.622083120422416e-05,
      "loss": 0.7441,
      "step": 497000
    },
    {
      "epoch": 4.535915030294182,
      "grad_norm": 3.827493190765381,
      "learning_rate": 4.622007080808818e-05,
      "loss": 0.7203,
      "step": 497100
    },
    {
      "epoch": 4.536827505657348,
      "grad_norm": 3.7701656818389893,
      "learning_rate": 4.621931041195221e-05,
      "loss": 0.7306,
      "step": 497200
    },
    {
      "epoch": 4.537739981020512,
      "grad_norm": 4.223287105560303,
      "learning_rate": 4.621855001581624e-05,
      "loss": 0.7126,
      "step": 497300
    },
    {
      "epoch": 4.538652456383677,
      "grad_norm": 4.671576499938965,
      "learning_rate": 4.621778961968027e-05,
      "loss": 0.7222,
      "step": 497400
    },
    {
      "epoch": 4.539564931746843,
      "grad_norm": 3.594428539276123,
      "learning_rate": 4.62170292235443e-05,
      "loss": 0.7394,
      "step": 497500
    },
    {
      "epoch": 4.540477407110008,
      "grad_norm": 4.8457746505737305,
      "learning_rate": 4.6216268827408325e-05,
      "loss": 0.7127,
      "step": 497600
    },
    {
      "epoch": 4.541389882473173,
      "grad_norm": 4.663306713104248,
      "learning_rate": 4.621550843127236e-05,
      "loss": 0.7491,
      "step": 497700
    },
    {
      "epoch": 4.542302357836339,
      "grad_norm": 3.092858076095581,
      "learning_rate": 4.6214748035136385e-05,
      "loss": 0.7254,
      "step": 497800
    },
    {
      "epoch": 4.543214833199504,
      "grad_norm": 3.3953700065612793,
      "learning_rate": 4.6213987639000415e-05,
      "loss": 0.7415,
      "step": 497900
    },
    {
      "epoch": 4.544127308562668,
      "grad_norm": 3.7880351543426514,
      "learning_rate": 4.6213227242864445e-05,
      "loss": 0.7195,
      "step": 498000
    },
    {
      "epoch": 4.545039783925834,
      "grad_norm": 3.700429916381836,
      "learning_rate": 4.6212466846728475e-05,
      "loss": 0.6805,
      "step": 498100
    },
    {
      "epoch": 4.545952259288999,
      "grad_norm": 5.0238471031188965,
      "learning_rate": 4.62117064505925e-05,
      "loss": 0.7621,
      "step": 498200
    },
    {
      "epoch": 4.546864734652164,
      "grad_norm": 4.2721452713012695,
      "learning_rate": 4.6210946054456535e-05,
      "loss": 0.7096,
      "step": 498300
    },
    {
      "epoch": 4.5477772100153295,
      "grad_norm": 4.392626762390137,
      "learning_rate": 4.621018565832056e-05,
      "loss": 0.7115,
      "step": 498400
    },
    {
      "epoch": 4.548689685378495,
      "grad_norm": 3.865877389907837,
      "learning_rate": 4.620942526218459e-05,
      "loss": 0.7672,
      "step": 498500
    },
    {
      "epoch": 4.54960216074166,
      "grad_norm": 3.3730790615081787,
      "learning_rate": 4.620866486604862e-05,
      "loss": 0.7316,
      "step": 498600
    },
    {
      "epoch": 4.550514636104825,
      "grad_norm": 3.814241409301758,
      "learning_rate": 4.620790446991265e-05,
      "loss": 0.7016,
      "step": 498700
    },
    {
      "epoch": 4.551427111467991,
      "grad_norm": 3.107923984527588,
      "learning_rate": 4.620714407377668e-05,
      "loss": 0.7153,
      "step": 498800
    },
    {
      "epoch": 4.552339586831156,
      "grad_norm": 4.024503231048584,
      "learning_rate": 4.620638367764071e-05,
      "loss": 0.7119,
      "step": 498900
    },
    {
      "epoch": 4.55325206219432,
      "grad_norm": 4.221287250518799,
      "learning_rate": 4.620562328150473e-05,
      "loss": 0.7215,
      "step": 499000
    },
    {
      "epoch": 4.554164537557486,
      "grad_norm": 3.9328272342681885,
      "learning_rate": 4.620486288536877e-05,
      "loss": 0.6891,
      "step": 499100
    },
    {
      "epoch": 4.555077012920651,
      "grad_norm": 4.346663951873779,
      "learning_rate": 4.620410248923279e-05,
      "loss": 0.6917,
      "step": 499200
    },
    {
      "epoch": 4.555989488283816,
      "grad_norm": 3.723234176635742,
      "learning_rate": 4.620334209309682e-05,
      "loss": 0.7306,
      "step": 499300
    },
    {
      "epoch": 4.556901963646982,
      "grad_norm": 3.6040561199188232,
      "learning_rate": 4.620258169696085e-05,
      "loss": 0.7188,
      "step": 499400
    },
    {
      "epoch": 4.557814439010147,
      "grad_norm": 4.68319034576416,
      "learning_rate": 4.620182130082488e-05,
      "loss": 0.7323,
      "step": 499500
    },
    {
      "epoch": 4.558726914373312,
      "grad_norm": 3.763336420059204,
      "learning_rate": 4.6201060904688906e-05,
      "loss": 0.7393,
      "step": 499600
    },
    {
      "epoch": 4.559639389736477,
      "grad_norm": 4.656822204589844,
      "learning_rate": 4.620030050855294e-05,
      "loss": 0.7415,
      "step": 499700
    },
    {
      "epoch": 4.560551865099642,
      "grad_norm": 3.489903211593628,
      "learning_rate": 4.6199540112416966e-05,
      "loss": 0.7348,
      "step": 499800
    },
    {
      "epoch": 4.561464340462807,
      "grad_norm": 5.164600849151611,
      "learning_rate": 4.6198779716280996e-05,
      "loss": 0.7586,
      "step": 499900
    },
    {
      "epoch": 4.5623768158259725,
      "grad_norm": 4.142490863800049,
      "learning_rate": 4.6198019320145026e-05,
      "loss": 0.7387,
      "step": 500000
    },
    {
      "epoch": 4.563289291189138,
      "grad_norm": 3.508958339691162,
      "learning_rate": 4.619725892400905e-05,
      "loss": 0.6777,
      "step": 500100
    },
    {
      "epoch": 4.564201766552303,
      "grad_norm": 4.680315971374512,
      "learning_rate": 4.6196498527873086e-05,
      "loss": 0.7087,
      "step": 500200
    },
    {
      "epoch": 4.565114241915468,
      "grad_norm": 4.352699279785156,
      "learning_rate": 4.619573813173711e-05,
      "loss": 0.6832,
      "step": 500300
    },
    {
      "epoch": 4.566026717278634,
      "grad_norm": 3.941730260848999,
      "learning_rate": 4.619497773560114e-05,
      "loss": 0.749,
      "step": 500400
    },
    {
      "epoch": 4.566939192641799,
      "grad_norm": 4.277032852172852,
      "learning_rate": 4.619421733946517e-05,
      "loss": 0.7034,
      "step": 500500
    },
    {
      "epoch": 4.567851668004964,
      "grad_norm": 3.4457757472991943,
      "learning_rate": 4.61934569433292e-05,
      "loss": 0.7061,
      "step": 500600
    },
    {
      "epoch": 4.568764143368129,
      "grad_norm": 3.4947381019592285,
      "learning_rate": 4.619269654719322e-05,
      "loss": 0.6859,
      "step": 500700
    },
    {
      "epoch": 4.569676618731294,
      "grad_norm": 3.9290215969085693,
      "learning_rate": 4.619193615105726e-05,
      "loss": 0.6821,
      "step": 500800
    },
    {
      "epoch": 4.570589094094459,
      "grad_norm": 4.402050495147705,
      "learning_rate": 4.619117575492128e-05,
      "loss": 0.7295,
      "step": 500900
    },
    {
      "epoch": 4.571501569457625,
      "grad_norm": 3.2165815830230713,
      "learning_rate": 4.619041535878531e-05,
      "loss": 0.7336,
      "step": 501000
    },
    {
      "epoch": 4.57241404482079,
      "grad_norm": 3.8179726600646973,
      "learning_rate": 4.618965496264934e-05,
      "loss": 0.6925,
      "step": 501100
    },
    {
      "epoch": 4.573326520183955,
      "grad_norm": 4.240828990936279,
      "learning_rate": 4.618889456651337e-05,
      "loss": 0.746,
      "step": 501200
    },
    {
      "epoch": 4.5742389955471205,
      "grad_norm": 3.81907320022583,
      "learning_rate": 4.61881341703774e-05,
      "loss": 0.708,
      "step": 501300
    },
    {
      "epoch": 4.575151470910285,
      "grad_norm": 3.536604404449463,
      "learning_rate": 4.618737377424143e-05,
      "loss": 0.7036,
      "step": 501400
    },
    {
      "epoch": 4.57606394627345,
      "grad_norm": 4.402661323547363,
      "learning_rate": 4.6186613378105457e-05,
      "loss": 0.7168,
      "step": 501500
    },
    {
      "epoch": 4.5769764216366156,
      "grad_norm": 4.136289596557617,
      "learning_rate": 4.6185852981969493e-05,
      "loss": 0.7472,
      "step": 501600
    },
    {
      "epoch": 4.577888896999781,
      "grad_norm": 4.073991775512695,
      "learning_rate": 4.618509258583352e-05,
      "loss": 0.7567,
      "step": 501700
    },
    {
      "epoch": 4.578801372362946,
      "grad_norm": 4.115298748016357,
      "learning_rate": 4.618433218969755e-05,
      "loss": 0.7418,
      "step": 501800
    },
    {
      "epoch": 4.5797138477261115,
      "grad_norm": 4.662564277648926,
      "learning_rate": 4.618357179356158e-05,
      "loss": 0.7156,
      "step": 501900
    },
    {
      "epoch": 4.580626323089277,
      "grad_norm": 4.252460479736328,
      "learning_rate": 4.618281139742561e-05,
      "loss": 0.7265,
      "step": 502000
    },
    {
      "epoch": 4.581538798452442,
      "grad_norm": 5.090936183929443,
      "learning_rate": 4.618205100128963e-05,
      "loss": 0.71,
      "step": 502100
    },
    {
      "epoch": 4.582451273815607,
      "grad_norm": 4.311941146850586,
      "learning_rate": 4.618129060515367e-05,
      "loss": 0.7129,
      "step": 502200
    },
    {
      "epoch": 4.583363749178773,
      "grad_norm": 3.5374741554260254,
      "learning_rate": 4.618053020901769e-05,
      "loss": 0.6955,
      "step": 502300
    },
    {
      "epoch": 4.584276224541937,
      "grad_norm": 4.343044757843018,
      "learning_rate": 4.617976981288172e-05,
      "loss": 0.7159,
      "step": 502400
    },
    {
      "epoch": 4.585188699905102,
      "grad_norm": 3.950758457183838,
      "learning_rate": 4.617900941674575e-05,
      "loss": 0.6771,
      "step": 502500
    },
    {
      "epoch": 4.586101175268268,
      "grad_norm": 4.739500999450684,
      "learning_rate": 4.617824902060978e-05,
      "loss": 0.7052,
      "step": 502600
    },
    {
      "epoch": 4.587013650631433,
      "grad_norm": 3.8669233322143555,
      "learning_rate": 4.617748862447381e-05,
      "loss": 0.703,
      "step": 502700
    },
    {
      "epoch": 4.587926125994598,
      "grad_norm": 2.6438097953796387,
      "learning_rate": 4.6176728228337834e-05,
      "loss": 0.7055,
      "step": 502800
    },
    {
      "epoch": 4.588838601357764,
      "grad_norm": 4.815332412719727,
      "learning_rate": 4.6175967832201864e-05,
      "loss": 0.7218,
      "step": 502900
    },
    {
      "epoch": 4.589751076720929,
      "grad_norm": 3.9594712257385254,
      "learning_rate": 4.6175207436065894e-05,
      "loss": 0.7386,
      "step": 503000
    },
    {
      "epoch": 4.590663552084093,
      "grad_norm": 4.812444686889648,
      "learning_rate": 4.6174447039929924e-05,
      "loss": 0.6699,
      "step": 503100
    },
    {
      "epoch": 4.591576027447259,
      "grad_norm": 3.5596442222595215,
      "learning_rate": 4.6173686643793954e-05,
      "loss": 0.7096,
      "step": 503200
    },
    {
      "epoch": 4.592488502810424,
      "grad_norm": 3.849217653274536,
      "learning_rate": 4.6172926247657984e-05,
      "loss": 0.7569,
      "step": 503300
    },
    {
      "epoch": 4.593400978173589,
      "grad_norm": 3.4870803356170654,
      "learning_rate": 4.617216585152201e-05,
      "loss": 0.7453,
      "step": 503400
    },
    {
      "epoch": 4.5943134535367545,
      "grad_norm": 3.666433334350586,
      "learning_rate": 4.617140545538604e-05,
      "loss": 0.6734,
      "step": 503500
    },
    {
      "epoch": 4.59522592889992,
      "grad_norm": 3.980403184890747,
      "learning_rate": 4.617064505925007e-05,
      "loss": 0.7434,
      "step": 503600
    },
    {
      "epoch": 4.596138404263085,
      "grad_norm": 4.723818302154541,
      "learning_rate": 4.61698846631141e-05,
      "loss": 0.7146,
      "step": 503700
    },
    {
      "epoch": 4.59705087962625,
      "grad_norm": 2.260962963104248,
      "learning_rate": 4.616912426697813e-05,
      "loss": 0.7158,
      "step": 503800
    },
    {
      "epoch": 4.597963354989416,
      "grad_norm": 4.309844970703125,
      "learning_rate": 4.616836387084216e-05,
      "loss": 0.7298,
      "step": 503900
    },
    {
      "epoch": 4.598875830352581,
      "grad_norm": 4.077917575836182,
      "learning_rate": 4.616760347470618e-05,
      "loss": 0.7312,
      "step": 504000
    },
    {
      "epoch": 4.599788305715745,
      "grad_norm": 4.657540798187256,
      "learning_rate": 4.616684307857022e-05,
      "loss": 0.7786,
      "step": 504100
    },
    {
      "epoch": 4.600700781078911,
      "grad_norm": 5.611563205718994,
      "learning_rate": 4.616608268243424e-05,
      "loss": 0.7525,
      "step": 504200
    },
    {
      "epoch": 4.601613256442076,
      "grad_norm": 4.005708694458008,
      "learning_rate": 4.616532228629827e-05,
      "loss": 0.6895,
      "step": 504300
    },
    {
      "epoch": 4.602525731805241,
      "grad_norm": 4.5242600440979,
      "learning_rate": 4.61645618901623e-05,
      "loss": 0.6945,
      "step": 504400
    },
    {
      "epoch": 4.603438207168407,
      "grad_norm": 4.380107879638672,
      "learning_rate": 4.616380149402633e-05,
      "loss": 0.7224,
      "step": 504500
    },
    {
      "epoch": 4.604350682531572,
      "grad_norm": 4.258439540863037,
      "learning_rate": 4.616304109789036e-05,
      "loss": 0.715,
      "step": 504600
    },
    {
      "epoch": 4.605263157894737,
      "grad_norm": 3.951503276824951,
      "learning_rate": 4.616228070175439e-05,
      "loss": 0.7123,
      "step": 504700
    },
    {
      "epoch": 4.606175633257902,
      "grad_norm": 3.652927875518799,
      "learning_rate": 4.6161520305618415e-05,
      "loss": 0.6947,
      "step": 504800
    },
    {
      "epoch": 4.607088108621067,
      "grad_norm": 3.739908218383789,
      "learning_rate": 4.6160759909482445e-05,
      "loss": 0.7487,
      "step": 504900
    },
    {
      "epoch": 4.608000583984232,
      "grad_norm": 5.148301601409912,
      "learning_rate": 4.6159999513346475e-05,
      "loss": 0.7385,
      "step": 505000
    },
    {
      "epoch": 4.6089130593473975,
      "grad_norm": 3.8440897464752197,
      "learning_rate": 4.6159239117210505e-05,
      "loss": 0.6788,
      "step": 505100
    },
    {
      "epoch": 4.609825534710563,
      "grad_norm": 4.009889125823975,
      "learning_rate": 4.6158478721074535e-05,
      "loss": 0.7405,
      "step": 505200
    },
    {
      "epoch": 4.610738010073728,
      "grad_norm": 5.01583194732666,
      "learning_rate": 4.6157718324938565e-05,
      "loss": 0.7018,
      "step": 505300
    },
    {
      "epoch": 4.611650485436893,
      "grad_norm": 4.016950607299805,
      "learning_rate": 4.615695792880259e-05,
      "loss": 0.7106,
      "step": 505400
    },
    {
      "epoch": 4.612562960800059,
      "grad_norm": 4.266058921813965,
      "learning_rate": 4.6156197532666625e-05,
      "loss": 0.7066,
      "step": 505500
    },
    {
      "epoch": 4.613475436163224,
      "grad_norm": 4.078530311584473,
      "learning_rate": 4.615543713653065e-05,
      "loss": 0.7272,
      "step": 505600
    },
    {
      "epoch": 4.614387911526389,
      "grad_norm": 3.4670462608337402,
      "learning_rate": 4.615467674039468e-05,
      "loss": 0.7322,
      "step": 505700
    },
    {
      "epoch": 4.615300386889554,
      "grad_norm": 4.3314008712768555,
      "learning_rate": 4.615391634425871e-05,
      "loss": 0.6689,
      "step": 505800
    },
    {
      "epoch": 4.616212862252719,
      "grad_norm": 2.6264963150024414,
      "learning_rate": 4.615315594812273e-05,
      "loss": 0.7454,
      "step": 505900
    },
    {
      "epoch": 4.617125337615884,
      "grad_norm": 3.767552614212036,
      "learning_rate": 4.615239555198677e-05,
      "loss": 0.7247,
      "step": 506000
    },
    {
      "epoch": 4.61803781297905,
      "grad_norm": 4.069126605987549,
      "learning_rate": 4.615163515585079e-05,
      "loss": 0.72,
      "step": 506100
    },
    {
      "epoch": 4.618950288342215,
      "grad_norm": 4.146968841552734,
      "learning_rate": 4.615087475971482e-05,
      "loss": 0.6862,
      "step": 506200
    },
    {
      "epoch": 4.61986276370538,
      "grad_norm": 3.644399881362915,
      "learning_rate": 4.615011436357885e-05,
      "loss": 0.7398,
      "step": 506300
    },
    {
      "epoch": 4.6207752390685455,
      "grad_norm": 4.149961948394775,
      "learning_rate": 4.614935396744288e-05,
      "loss": 0.7425,
      "step": 506400
    },
    {
      "epoch": 4.62168771443171,
      "grad_norm": 4.154435157775879,
      "learning_rate": 4.6148593571306906e-05,
      "loss": 0.7001,
      "step": 506500
    },
    {
      "epoch": 4.622600189794875,
      "grad_norm": 4.419651508331299,
      "learning_rate": 4.614783317517094e-05,
      "loss": 0.7538,
      "step": 506600
    },
    {
      "epoch": 4.6235126651580405,
      "grad_norm": 3.297146797180176,
      "learning_rate": 4.6147072779034966e-05,
      "loss": 0.6978,
      "step": 506700
    },
    {
      "epoch": 4.624425140521206,
      "grad_norm": 4.67571496963501,
      "learning_rate": 4.6146312382898996e-05,
      "loss": 0.7234,
      "step": 506800
    },
    {
      "epoch": 4.625337615884371,
      "grad_norm": 3.7542030811309814,
      "learning_rate": 4.6145551986763026e-05,
      "loss": 0.7043,
      "step": 506900
    },
    {
      "epoch": 4.626250091247536,
      "grad_norm": 4.748025894165039,
      "learning_rate": 4.6144791590627056e-05,
      "loss": 0.7453,
      "step": 507000
    },
    {
      "epoch": 4.627162566610702,
      "grad_norm": 3.0865957736968994,
      "learning_rate": 4.6144031194491086e-05,
      "loss": 0.7185,
      "step": 507100
    },
    {
      "epoch": 4.628075041973867,
      "grad_norm": 4.572257041931152,
      "learning_rate": 4.6143270798355116e-05,
      "loss": 0.7023,
      "step": 507200
    },
    {
      "epoch": 4.628987517337032,
      "grad_norm": 4.3534746170043945,
      "learning_rate": 4.614251040221914e-05,
      "loss": 0.7211,
      "step": 507300
    },
    {
      "epoch": 4.629899992700198,
      "grad_norm": 4.373594760894775,
      "learning_rate": 4.6141750006083176e-05,
      "loss": 0.7424,
      "step": 507400
    },
    {
      "epoch": 4.630812468063362,
      "grad_norm": 4.0305657386779785,
      "learning_rate": 4.61409896099472e-05,
      "loss": 0.6974,
      "step": 507500
    },
    {
      "epoch": 4.631724943426527,
      "grad_norm": 4.301596641540527,
      "learning_rate": 4.614022921381123e-05,
      "loss": 0.6971,
      "step": 507600
    },
    {
      "epoch": 4.632637418789693,
      "grad_norm": 4.556138038635254,
      "learning_rate": 4.613946881767526e-05,
      "loss": 0.7497,
      "step": 507700
    },
    {
      "epoch": 4.633549894152858,
      "grad_norm": 4.13594388961792,
      "learning_rate": 4.613870842153929e-05,
      "loss": 0.6982,
      "step": 507800
    },
    {
      "epoch": 4.634462369516023,
      "grad_norm": 5.079357147216797,
      "learning_rate": 4.613794802540331e-05,
      "loss": 0.7367,
      "step": 507900
    },
    {
      "epoch": 4.6353748448791885,
      "grad_norm": 3.7355308532714844,
      "learning_rate": 4.613718762926735e-05,
      "loss": 0.6531,
      "step": 508000
    },
    {
      "epoch": 4.636287320242354,
      "grad_norm": 3.539761543273926,
      "learning_rate": 4.613642723313137e-05,
      "loss": 0.7152,
      "step": 508100
    },
    {
      "epoch": 4.637199795605518,
      "grad_norm": 3.9005370140075684,
      "learning_rate": 4.61356668369954e-05,
      "loss": 0.7329,
      "step": 508200
    },
    {
      "epoch": 4.6381122709686835,
      "grad_norm": 4.033259391784668,
      "learning_rate": 4.613490644085943e-05,
      "loss": 0.7403,
      "step": 508300
    },
    {
      "epoch": 4.639024746331849,
      "grad_norm": 3.647102117538452,
      "learning_rate": 4.613414604472346e-05,
      "loss": 0.7249,
      "step": 508400
    },
    {
      "epoch": 4.639937221695014,
      "grad_norm": 3.4515445232391357,
      "learning_rate": 4.6133385648587494e-05,
      "loss": 0.759,
      "step": 508500
    },
    {
      "epoch": 4.640849697058179,
      "grad_norm": 3.122008800506592,
      "learning_rate": 4.613262525245152e-05,
      "loss": 0.674,
      "step": 508600
    },
    {
      "epoch": 4.641762172421345,
      "grad_norm": 5.0833516120910645,
      "learning_rate": 4.613186485631555e-05,
      "loss": 0.745,
      "step": 508700
    },
    {
      "epoch": 4.64267464778451,
      "grad_norm": 3.6173787117004395,
      "learning_rate": 4.613110446017958e-05,
      "loss": 0.713,
      "step": 508800
    },
    {
      "epoch": 4.643587123147675,
      "grad_norm": 3.895803689956665,
      "learning_rate": 4.613034406404361e-05,
      "loss": 0.7056,
      "step": 508900
    },
    {
      "epoch": 4.644499598510841,
      "grad_norm": 3.94851016998291,
      "learning_rate": 4.612958366790763e-05,
      "loss": 0.7351,
      "step": 509000
    },
    {
      "epoch": 4.645412073874006,
      "grad_norm": 4.092726707458496,
      "learning_rate": 4.612882327177167e-05,
      "loss": 0.7088,
      "step": 509100
    },
    {
      "epoch": 4.64632454923717,
      "grad_norm": 3.7738029956817627,
      "learning_rate": 4.612806287563569e-05,
      "loss": 0.7173,
      "step": 509200
    },
    {
      "epoch": 4.647237024600336,
      "grad_norm": 3.8721346855163574,
      "learning_rate": 4.612730247949972e-05,
      "loss": 0.7631,
      "step": 509300
    },
    {
      "epoch": 4.648149499963501,
      "grad_norm": 4.850299835205078,
      "learning_rate": 4.612654208336375e-05,
      "loss": 0.7484,
      "step": 509400
    },
    {
      "epoch": 4.649061975326666,
      "grad_norm": 3.967766523361206,
      "learning_rate": 4.612578168722778e-05,
      "loss": 0.7176,
      "step": 509500
    },
    {
      "epoch": 4.6499744506898315,
      "grad_norm": 4.15593147277832,
      "learning_rate": 4.612502129109181e-05,
      "loss": 0.7562,
      "step": 509600
    },
    {
      "epoch": 4.650886926052997,
      "grad_norm": 4.3481364250183105,
      "learning_rate": 4.612426089495584e-05,
      "loss": 0.7465,
      "step": 509700
    },
    {
      "epoch": 4.651799401416162,
      "grad_norm": 3.850628137588501,
      "learning_rate": 4.6123500498819864e-05,
      "loss": 0.7465,
      "step": 509800
    },
    {
      "epoch": 4.6527118767793265,
      "grad_norm": 3.2953147888183594,
      "learning_rate": 4.61227401026839e-05,
      "loss": 0.6761,
      "step": 509900
    },
    {
      "epoch": 4.653624352142492,
      "grad_norm": 4.301468849182129,
      "learning_rate": 4.6121979706547924e-05,
      "loss": 0.7171,
      "step": 510000
    },
    {
      "epoch": 4.654536827505657,
      "grad_norm": 3.0047972202301025,
      "learning_rate": 4.6121219310411954e-05,
      "loss": 0.7854,
      "step": 510100
    },
    {
      "epoch": 4.655449302868822,
      "grad_norm": 3.9338908195495605,
      "learning_rate": 4.6120458914275984e-05,
      "loss": 0.7426,
      "step": 510200
    },
    {
      "epoch": 4.656361778231988,
      "grad_norm": 3.8065969944000244,
      "learning_rate": 4.6119698518140014e-05,
      "loss": 0.725,
      "step": 510300
    },
    {
      "epoch": 4.657274253595153,
      "grad_norm": 3.7041146755218506,
      "learning_rate": 4.611893812200404e-05,
      "loss": 0.6927,
      "step": 510400
    },
    {
      "epoch": 4.658186728958318,
      "grad_norm": 3.644251823425293,
      "learning_rate": 4.6118177725868075e-05,
      "loss": 0.7611,
      "step": 510500
    },
    {
      "epoch": 4.659099204321484,
      "grad_norm": 4.014459133148193,
      "learning_rate": 4.61174173297321e-05,
      "loss": 0.6777,
      "step": 510600
    },
    {
      "epoch": 4.660011679684649,
      "grad_norm": 4.124809265136719,
      "learning_rate": 4.611665693359613e-05,
      "loss": 0.7242,
      "step": 510700
    },
    {
      "epoch": 4.660924155047813,
      "grad_norm": 5.401961803436279,
      "learning_rate": 4.611589653746016e-05,
      "loss": 0.7005,
      "step": 510800
    },
    {
      "epoch": 4.661836630410979,
      "grad_norm": 3.741236448287964,
      "learning_rate": 4.611513614132419e-05,
      "loss": 0.7212,
      "step": 510900
    },
    {
      "epoch": 4.662749105774144,
      "grad_norm": 4.438868999481201,
      "learning_rate": 4.611437574518822e-05,
      "loss": 0.7192,
      "step": 511000
    },
    {
      "epoch": 4.663661581137309,
      "grad_norm": 2.575740337371826,
      "learning_rate": 4.611361534905225e-05,
      "loss": 0.7371,
      "step": 511100
    },
    {
      "epoch": 4.6645740565004745,
      "grad_norm": 4.903867721557617,
      "learning_rate": 4.611285495291627e-05,
      "loss": 0.6869,
      "step": 511200
    },
    {
      "epoch": 4.66548653186364,
      "grad_norm": 3.635877847671509,
      "learning_rate": 4.61120945567803e-05,
      "loss": 0.7208,
      "step": 511300
    },
    {
      "epoch": 4.666399007226805,
      "grad_norm": 3.983445882797241,
      "learning_rate": 4.611133416064433e-05,
      "loss": 0.7317,
      "step": 511400
    },
    {
      "epoch": 4.66731148258997,
      "grad_norm": 4.481529235839844,
      "learning_rate": 4.6110573764508355e-05,
      "loss": 0.73,
      "step": 511500
    },
    {
      "epoch": 4.668223957953135,
      "grad_norm": 4.205370903015137,
      "learning_rate": 4.610981336837239e-05,
      "loss": 0.7215,
      "step": 511600
    },
    {
      "epoch": 4.6691364333163,
      "grad_norm": 4.269013404846191,
      "learning_rate": 4.6109052972236415e-05,
      "loss": 0.6859,
      "step": 511700
    },
    {
      "epoch": 4.670048908679465,
      "grad_norm": 3.7031264305114746,
      "learning_rate": 4.6108292576100445e-05,
      "loss": 0.7158,
      "step": 511800
    },
    {
      "epoch": 4.670961384042631,
      "grad_norm": 3.7152552604675293,
      "learning_rate": 4.6107532179964475e-05,
      "loss": 0.6887,
      "step": 511900
    },
    {
      "epoch": 4.671873859405796,
      "grad_norm": 3.846876859664917,
      "learning_rate": 4.6106771783828505e-05,
      "loss": 0.6969,
      "step": 512000
    },
    {
      "epoch": 4.672786334768961,
      "grad_norm": 4.4179205894470215,
      "learning_rate": 4.6106011387692535e-05,
      "loss": 0.7027,
      "step": 512100
    },
    {
      "epoch": 4.673698810132127,
      "grad_norm": 4.347372531890869,
      "learning_rate": 4.6105250991556565e-05,
      "loss": 0.7122,
      "step": 512200
    },
    {
      "epoch": 4.674611285495292,
      "grad_norm": 5.553456783294678,
      "learning_rate": 4.610449059542059e-05,
      "loss": 0.7058,
      "step": 512300
    },
    {
      "epoch": 4.675523760858457,
      "grad_norm": 3.977886199951172,
      "learning_rate": 4.6103730199284625e-05,
      "loss": 0.721,
      "step": 512400
    },
    {
      "epoch": 4.676436236221622,
      "grad_norm": 3.336893081665039,
      "learning_rate": 4.610296980314865e-05,
      "loss": 0.7462,
      "step": 512500
    },
    {
      "epoch": 4.677348711584787,
      "grad_norm": 3.7231545448303223,
      "learning_rate": 4.610220940701268e-05,
      "loss": 0.7075,
      "step": 512600
    },
    {
      "epoch": 4.678261186947952,
      "grad_norm": 4.086382865905762,
      "learning_rate": 4.610144901087671e-05,
      "loss": 0.7163,
      "step": 512700
    },
    {
      "epoch": 4.6791736623111175,
      "grad_norm": 3.3097198009490967,
      "learning_rate": 4.610068861474074e-05,
      "loss": 0.7399,
      "step": 512800
    },
    {
      "epoch": 4.680086137674283,
      "grad_norm": 4.9279255867004395,
      "learning_rate": 4.609992821860476e-05,
      "loss": 0.7361,
      "step": 512900
    },
    {
      "epoch": 4.680998613037448,
      "grad_norm": 4.9522809982299805,
      "learning_rate": 4.60991678224688e-05,
      "loss": 0.7249,
      "step": 513000
    },
    {
      "epoch": 4.681911088400613,
      "grad_norm": 4.355147838592529,
      "learning_rate": 4.609840742633282e-05,
      "loss": 0.7394,
      "step": 513100
    },
    {
      "epoch": 4.682823563763779,
      "grad_norm": 4.50439977645874,
      "learning_rate": 4.609764703019685e-05,
      "loss": 0.7323,
      "step": 513200
    },
    {
      "epoch": 4.683736039126943,
      "grad_norm": 4.085158824920654,
      "learning_rate": 4.609688663406088e-05,
      "loss": 0.7354,
      "step": 513300
    },
    {
      "epoch": 4.684648514490108,
      "grad_norm": 3.968966484069824,
      "learning_rate": 4.609612623792491e-05,
      "loss": 0.7019,
      "step": 513400
    },
    {
      "epoch": 4.685560989853274,
      "grad_norm": 3.7671539783477783,
      "learning_rate": 4.609536584178894e-05,
      "loss": 0.7256,
      "step": 513500
    },
    {
      "epoch": 4.686473465216439,
      "grad_norm": 3.7030420303344727,
      "learning_rate": 4.609460544565297e-05,
      "loss": 0.6913,
      "step": 513600
    },
    {
      "epoch": 4.687385940579604,
      "grad_norm": 5.168842315673828,
      "learning_rate": 4.6093845049516996e-05,
      "loss": 0.6925,
      "step": 513700
    },
    {
      "epoch": 4.68829841594277,
      "grad_norm": 3.155468702316284,
      "learning_rate": 4.609308465338103e-05,
      "loss": 0.7061,
      "step": 513800
    },
    {
      "epoch": 4.689210891305935,
      "grad_norm": 4.1451497077941895,
      "learning_rate": 4.6092324257245056e-05,
      "loss": 0.7201,
      "step": 513900
    },
    {
      "epoch": 4.6901233666691,
      "grad_norm": 4.355569362640381,
      "learning_rate": 4.6091563861109086e-05,
      "loss": 0.7313,
      "step": 514000
    },
    {
      "epoch": 4.6910358420322655,
      "grad_norm": 3.4266371726989746,
      "learning_rate": 4.6090803464973116e-05,
      "loss": 0.7037,
      "step": 514100
    },
    {
      "epoch": 4.69194831739543,
      "grad_norm": 4.048222541809082,
      "learning_rate": 4.609004306883714e-05,
      "loss": 0.7459,
      "step": 514200
    },
    {
      "epoch": 4.692860792758595,
      "grad_norm": 4.777480125427246,
      "learning_rate": 4.608928267270117e-05,
      "loss": 0.719,
      "step": 514300
    },
    {
      "epoch": 4.6937732681217605,
      "grad_norm": 2.912986993789673,
      "learning_rate": 4.60885222765652e-05,
      "loss": 0.6874,
      "step": 514400
    },
    {
      "epoch": 4.694685743484926,
      "grad_norm": 4.731987953186035,
      "learning_rate": 4.608776188042923e-05,
      "loss": 0.7087,
      "step": 514500
    },
    {
      "epoch": 4.695598218848091,
      "grad_norm": 3.7222447395324707,
      "learning_rate": 4.608700148429326e-05,
      "loss": 0.7441,
      "step": 514600
    },
    {
      "epoch": 4.696510694211256,
      "grad_norm": 3.970637559890747,
      "learning_rate": 4.608624108815729e-05,
      "loss": 0.721,
      "step": 514700
    },
    {
      "epoch": 4.697423169574422,
      "grad_norm": 4.1678667068481445,
      "learning_rate": 4.608548069202131e-05,
      "loss": 0.7409,
      "step": 514800
    },
    {
      "epoch": 4.698335644937587,
      "grad_norm": 4.498523712158203,
      "learning_rate": 4.608472029588535e-05,
      "loss": 0.6807,
      "step": 514900
    },
    {
      "epoch": 4.6992481203007515,
      "grad_norm": 3.4794816970825195,
      "learning_rate": 4.608395989974937e-05,
      "loss": 0.742,
      "step": 515000
    },
    {
      "epoch": 4.700160595663917,
      "grad_norm": 4.075324535369873,
      "learning_rate": 4.6083199503613403e-05,
      "loss": 0.7012,
      "step": 515100
    },
    {
      "epoch": 4.701073071027082,
      "grad_norm": 4.349256992340088,
      "learning_rate": 4.6082439107477433e-05,
      "loss": 0.7531,
      "step": 515200
    },
    {
      "epoch": 4.701985546390247,
      "grad_norm": 4.572941303253174,
      "learning_rate": 4.6081678711341464e-05,
      "loss": 0.7309,
      "step": 515300
    },
    {
      "epoch": 4.702898021753413,
      "grad_norm": 4.065393447875977,
      "learning_rate": 4.608091831520549e-05,
      "loss": 0.7614,
      "step": 515400
    },
    {
      "epoch": 4.703810497116578,
      "grad_norm": 3.3852829933166504,
      "learning_rate": 4.6080157919069524e-05,
      "loss": 0.716,
      "step": 515500
    },
    {
      "epoch": 4.704722972479743,
      "grad_norm": 3.9475693702697754,
      "learning_rate": 4.607939752293355e-05,
      "loss": 0.6903,
      "step": 515600
    },
    {
      "epoch": 4.7056354478429085,
      "grad_norm": 3.788588523864746,
      "learning_rate": 4.607863712679758e-05,
      "loss": 0.7128,
      "step": 515700
    },
    {
      "epoch": 4.706547923206074,
      "grad_norm": 4.530508041381836,
      "learning_rate": 4.607787673066161e-05,
      "loss": 0.7598,
      "step": 515800
    },
    {
      "epoch": 4.707460398569238,
      "grad_norm": 4.41411018371582,
      "learning_rate": 4.607711633452564e-05,
      "loss": 0.7114,
      "step": 515900
    },
    {
      "epoch": 4.708372873932404,
      "grad_norm": 3.9536075592041016,
      "learning_rate": 4.607635593838967e-05,
      "loss": 0.7469,
      "step": 516000
    },
    {
      "epoch": 4.709285349295569,
      "grad_norm": 4.661015033721924,
      "learning_rate": 4.60755955422537e-05,
      "loss": 0.7406,
      "step": 516100
    },
    {
      "epoch": 4.710197824658734,
      "grad_norm": 3.8141250610351562,
      "learning_rate": 4.607483514611772e-05,
      "loss": 0.7245,
      "step": 516200
    },
    {
      "epoch": 4.7111103000218995,
      "grad_norm": 4.33654260635376,
      "learning_rate": 4.607407474998176e-05,
      "loss": 0.7308,
      "step": 516300
    },
    {
      "epoch": 4.712022775385065,
      "grad_norm": 4.569074630737305,
      "learning_rate": 4.607331435384578e-05,
      "loss": 0.731,
      "step": 516400
    },
    {
      "epoch": 4.71293525074823,
      "grad_norm": 3.1359498500823975,
      "learning_rate": 4.607255395770981e-05,
      "loss": 0.7763,
      "step": 516500
    },
    {
      "epoch": 4.713847726111395,
      "grad_norm": 3.1633551120758057,
      "learning_rate": 4.607179356157384e-05,
      "loss": 0.7111,
      "step": 516600
    },
    {
      "epoch": 4.71476020147456,
      "grad_norm": 3.9844038486480713,
      "learning_rate": 4.607103316543787e-05,
      "loss": 0.7253,
      "step": 516700
    },
    {
      "epoch": 4.715672676837725,
      "grad_norm": 4.540469646453857,
      "learning_rate": 4.60702727693019e-05,
      "loss": 0.7128,
      "step": 516800
    },
    {
      "epoch": 4.71658515220089,
      "grad_norm": 3.3358826637268066,
      "learning_rate": 4.606951237316593e-05,
      "loss": 0.7357,
      "step": 516900
    },
    {
      "epoch": 4.717497627564056,
      "grad_norm": 3.8342957496643066,
      "learning_rate": 4.6068751977029954e-05,
      "loss": 0.7156,
      "step": 517000
    },
    {
      "epoch": 4.718410102927221,
      "grad_norm": 2.6725051403045654,
      "learning_rate": 4.6067991580893984e-05,
      "loss": 0.7426,
      "step": 517100
    },
    {
      "epoch": 4.719322578290386,
      "grad_norm": 4.744033336639404,
      "learning_rate": 4.6067231184758014e-05,
      "loss": 0.6893,
      "step": 517200
    },
    {
      "epoch": 4.720235053653552,
      "grad_norm": 4.242143154144287,
      "learning_rate": 4.606647078862204e-05,
      "loss": 0.7355,
      "step": 517300
    },
    {
      "epoch": 4.721147529016717,
      "grad_norm": 3.74057936668396,
      "learning_rate": 4.6065710392486075e-05,
      "loss": 0.7086,
      "step": 517400
    },
    {
      "epoch": 4.722060004379882,
      "grad_norm": 5.307298183441162,
      "learning_rate": 4.60649499963501e-05,
      "loss": 0.6963,
      "step": 517500
    },
    {
      "epoch": 4.722972479743047,
      "grad_norm": 4.27975606918335,
      "learning_rate": 4.606418960021413e-05,
      "loss": 0.7731,
      "step": 517600
    },
    {
      "epoch": 4.723884955106212,
      "grad_norm": 5.426167964935303,
      "learning_rate": 4.606342920407816e-05,
      "loss": 0.7352,
      "step": 517700
    },
    {
      "epoch": 4.724797430469377,
      "grad_norm": 4.6832451820373535,
      "learning_rate": 4.606266880794219e-05,
      "loss": 0.7447,
      "step": 517800
    },
    {
      "epoch": 4.7257099058325425,
      "grad_norm": 4.207468509674072,
      "learning_rate": 4.606190841180622e-05,
      "loss": 0.7248,
      "step": 517900
    },
    {
      "epoch": 4.726622381195708,
      "grad_norm": 4.7027435302734375,
      "learning_rate": 4.606114801567025e-05,
      "loss": 0.7208,
      "step": 518000
    },
    {
      "epoch": 4.727534856558873,
      "grad_norm": 3.4098093509674072,
      "learning_rate": 4.606038761953427e-05,
      "loss": 0.7424,
      "step": 518100
    },
    {
      "epoch": 4.728447331922038,
      "grad_norm": 3.5475597381591797,
      "learning_rate": 4.605962722339831e-05,
      "loss": 0.7459,
      "step": 518200
    },
    {
      "epoch": 4.729359807285204,
      "grad_norm": 3.5548319816589355,
      "learning_rate": 4.605886682726233e-05,
      "loss": 0.667,
      "step": 518300
    },
    {
      "epoch": 4.730272282648368,
      "grad_norm": 4.50662088394165,
      "learning_rate": 4.605810643112636e-05,
      "loss": 0.6799,
      "step": 518400
    },
    {
      "epoch": 4.731184758011533,
      "grad_norm": 3.9188194274902344,
      "learning_rate": 4.605734603499039e-05,
      "loss": 0.7286,
      "step": 518500
    },
    {
      "epoch": 4.732097233374699,
      "grad_norm": 3.7193830013275146,
      "learning_rate": 4.605658563885442e-05,
      "loss": 0.6964,
      "step": 518600
    },
    {
      "epoch": 4.733009708737864,
      "grad_norm": 4.412728786468506,
      "learning_rate": 4.6055825242718445e-05,
      "loss": 0.7256,
      "step": 518700
    },
    {
      "epoch": 4.733922184101029,
      "grad_norm": 3.669750928878784,
      "learning_rate": 4.605506484658248e-05,
      "loss": 0.705,
      "step": 518800
    },
    {
      "epoch": 4.734834659464195,
      "grad_norm": 3.324552536010742,
      "learning_rate": 4.6054304450446505e-05,
      "loss": 0.6875,
      "step": 518900
    },
    {
      "epoch": 4.73574713482736,
      "grad_norm": 4.540259838104248,
      "learning_rate": 4.6053544054310535e-05,
      "loss": 0.7441,
      "step": 519000
    },
    {
      "epoch": 4.736659610190525,
      "grad_norm": 4.132631301879883,
      "learning_rate": 4.6052783658174565e-05,
      "loss": 0.6971,
      "step": 519100
    },
    {
      "epoch": 4.7375720855536905,
      "grad_norm": 3.873325824737549,
      "learning_rate": 4.6052023262038596e-05,
      "loss": 0.7344,
      "step": 519200
    },
    {
      "epoch": 4.738484560916855,
      "grad_norm": 4.9890055656433105,
      "learning_rate": 4.6051262865902626e-05,
      "loss": 0.719,
      "step": 519300
    },
    {
      "epoch": 4.73939703628002,
      "grad_norm": 3.2899911403656006,
      "learning_rate": 4.6050502469766656e-05,
      "loss": 0.7428,
      "step": 519400
    },
    {
      "epoch": 4.7403095116431855,
      "grad_norm": 4.780422210693359,
      "learning_rate": 4.604974207363068e-05,
      "loss": 0.6985,
      "step": 519500
    },
    {
      "epoch": 4.741221987006351,
      "grad_norm": 4.289749622344971,
      "learning_rate": 4.6048981677494716e-05,
      "loss": 0.7511,
      "step": 519600
    },
    {
      "epoch": 4.742134462369516,
      "grad_norm": 2.4877524375915527,
      "learning_rate": 4.604822128135874e-05,
      "loss": 0.7415,
      "step": 519700
    },
    {
      "epoch": 4.743046937732681,
      "grad_norm": 3.0803170204162598,
      "learning_rate": 4.604746088522276e-05,
      "loss": 0.7095,
      "step": 519800
    },
    {
      "epoch": 4.743959413095847,
      "grad_norm": 4.066601276397705,
      "learning_rate": 4.60467004890868e-05,
      "loss": 0.7233,
      "step": 519900
    },
    {
      "epoch": 4.744871888459012,
      "grad_norm": 4.218677043914795,
      "learning_rate": 4.604594009295082e-05,
      "loss": 0.7109,
      "step": 520000
    },
    {
      "epoch": 4.745784363822176,
      "grad_norm": 4.779792308807373,
      "learning_rate": 4.604517969681485e-05,
      "loss": 0.6749,
      "step": 520100
    },
    {
      "epoch": 4.746696839185342,
      "grad_norm": 4.365667343139648,
      "learning_rate": 4.604441930067888e-05,
      "loss": 0.7053,
      "step": 520200
    },
    {
      "epoch": 4.747609314548507,
      "grad_norm": 2.7953903675079346,
      "learning_rate": 4.604365890454291e-05,
      "loss": 0.7253,
      "step": 520300
    },
    {
      "epoch": 4.748521789911672,
      "grad_norm": 3.581406354904175,
      "learning_rate": 4.604289850840694e-05,
      "loss": 0.7137,
      "step": 520400
    },
    {
      "epoch": 4.749434265274838,
      "grad_norm": 3.4377615451812744,
      "learning_rate": 4.604213811227097e-05,
      "loss": 0.7212,
      "step": 520500
    },
    {
      "epoch": 4.750346740638003,
      "grad_norm": 4.449584007263184,
      "learning_rate": 4.6041377716134996e-05,
      "loss": 0.7134,
      "step": 520600
    },
    {
      "epoch": 4.751259216001168,
      "grad_norm": 4.2982330322265625,
      "learning_rate": 4.604061731999903e-05,
      "loss": 0.7562,
      "step": 520700
    },
    {
      "epoch": 4.7521716913643335,
      "grad_norm": 4.2383575439453125,
      "learning_rate": 4.6039856923863056e-05,
      "loss": 0.7176,
      "step": 520800
    },
    {
      "epoch": 4.753084166727499,
      "grad_norm": 3.938178062438965,
      "learning_rate": 4.6039096527727086e-05,
      "loss": 0.6703,
      "step": 520900
    },
    {
      "epoch": 4.753996642090663,
      "grad_norm": 3.7202796936035156,
      "learning_rate": 4.6038336131591116e-05,
      "loss": 0.671,
      "step": 521000
    },
    {
      "epoch": 4.7549091174538285,
      "grad_norm": 3.148979902267456,
      "learning_rate": 4.6037575735455146e-05,
      "loss": 0.6857,
      "step": 521100
    },
    {
      "epoch": 4.755821592816994,
      "grad_norm": 3.381483554840088,
      "learning_rate": 4.603681533931917e-05,
      "loss": 0.6938,
      "step": 521200
    },
    {
      "epoch": 4.756734068180159,
      "grad_norm": 3.7265350818634033,
      "learning_rate": 4.6036054943183207e-05,
      "loss": 0.7133,
      "step": 521300
    },
    {
      "epoch": 4.757646543543324,
      "grad_norm": 4.500307559967041,
      "learning_rate": 4.603529454704723e-05,
      "loss": 0.737,
      "step": 521400
    },
    {
      "epoch": 4.75855901890649,
      "grad_norm": 3.0049870014190674,
      "learning_rate": 4.603453415091126e-05,
      "loss": 0.6801,
      "step": 521500
    },
    {
      "epoch": 4.759471494269655,
      "grad_norm": 3.7211246490478516,
      "learning_rate": 4.603377375477529e-05,
      "loss": 0.6999,
      "step": 521600
    },
    {
      "epoch": 4.76038396963282,
      "grad_norm": 2.0614514350891113,
      "learning_rate": 4.603301335863932e-05,
      "loss": 0.7461,
      "step": 521700
    },
    {
      "epoch": 4.761296444995985,
      "grad_norm": 4.302977561950684,
      "learning_rate": 4.603225296250335e-05,
      "loss": 0.7109,
      "step": 521800
    },
    {
      "epoch": 4.76220892035915,
      "grad_norm": 3.4127132892608643,
      "learning_rate": 4.603149256636738e-05,
      "loss": 0.7287,
      "step": 521900
    },
    {
      "epoch": 4.763121395722315,
      "grad_norm": 3.4270429611206055,
      "learning_rate": 4.6030732170231404e-05,
      "loss": 0.7509,
      "step": 522000
    },
    {
      "epoch": 4.764033871085481,
      "grad_norm": 4.02840518951416,
      "learning_rate": 4.602997177409544e-05,
      "loss": 0.6946,
      "step": 522100
    },
    {
      "epoch": 4.764946346448646,
      "grad_norm": 3.6786136627197266,
      "learning_rate": 4.6029211377959464e-05,
      "loss": 0.6883,
      "step": 522200
    },
    {
      "epoch": 4.765858821811811,
      "grad_norm": 4.65305233001709,
      "learning_rate": 4.6028450981823494e-05,
      "loss": 0.6495,
      "step": 522300
    },
    {
      "epoch": 4.7667712971749765,
      "grad_norm": 3.7519497871398926,
      "learning_rate": 4.6027690585687524e-05,
      "loss": 0.7286,
      "step": 522400
    },
    {
      "epoch": 4.767683772538142,
      "grad_norm": 4.489643096923828,
      "learning_rate": 4.6026930189551554e-05,
      "loss": 0.707,
      "step": 522500
    },
    {
      "epoch": 4.768596247901307,
      "grad_norm": 3.2219741344451904,
      "learning_rate": 4.602616979341558e-05,
      "loss": 0.7457,
      "step": 522600
    },
    {
      "epoch": 4.7695087232644715,
      "grad_norm": 4.5459303855896,
      "learning_rate": 4.602540939727961e-05,
      "loss": 0.7205,
      "step": 522700
    },
    {
      "epoch": 4.770421198627637,
      "grad_norm": 4.0647664070129395,
      "learning_rate": 4.602464900114364e-05,
      "loss": 0.6938,
      "step": 522800
    },
    {
      "epoch": 4.771333673990802,
      "grad_norm": 4.736758232116699,
      "learning_rate": 4.602388860500767e-05,
      "loss": 0.7371,
      "step": 522900
    },
    {
      "epoch": 4.772246149353967,
      "grad_norm": 3.5160248279571533,
      "learning_rate": 4.60231282088717e-05,
      "loss": 0.723,
      "step": 523000
    },
    {
      "epoch": 4.773158624717133,
      "grad_norm": 4.288231372833252,
      "learning_rate": 4.602236781273572e-05,
      "loss": 0.7097,
      "step": 523100
    },
    {
      "epoch": 4.774071100080298,
      "grad_norm": 3.596801519393921,
      "learning_rate": 4.602160741659976e-05,
      "loss": 0.7495,
      "step": 523200
    },
    {
      "epoch": 4.774983575443463,
      "grad_norm": 4.165257930755615,
      "learning_rate": 4.602084702046378e-05,
      "loss": 0.7307,
      "step": 523300
    },
    {
      "epoch": 4.775896050806629,
      "grad_norm": 3.6591310501098633,
      "learning_rate": 4.602008662432781e-05,
      "loss": 0.7069,
      "step": 523400
    },
    {
      "epoch": 4.776808526169793,
      "grad_norm": 4.137032508850098,
      "learning_rate": 4.601932622819184e-05,
      "loss": 0.6992,
      "step": 523500
    },
    {
      "epoch": 4.777721001532958,
      "grad_norm": 3.8496580123901367,
      "learning_rate": 4.601856583205587e-05,
      "loss": 0.6902,
      "step": 523600
    },
    {
      "epoch": 4.778633476896124,
      "grad_norm": 3.489091634750366,
      "learning_rate": 4.6017805435919894e-05,
      "loss": 0.6975,
      "step": 523700
    },
    {
      "epoch": 4.779545952259289,
      "grad_norm": 3.9531290531158447,
      "learning_rate": 4.601704503978393e-05,
      "loss": 0.7212,
      "step": 523800
    },
    {
      "epoch": 4.780458427622454,
      "grad_norm": 4.708625316619873,
      "learning_rate": 4.6016284643647954e-05,
      "loss": 0.7285,
      "step": 523900
    },
    {
      "epoch": 4.7813709029856195,
      "grad_norm": 3.3054592609405518,
      "learning_rate": 4.6015524247511985e-05,
      "loss": 0.7192,
      "step": 524000
    },
    {
      "epoch": 4.782283378348785,
      "grad_norm": 3.46624493598938,
      "learning_rate": 4.6014763851376015e-05,
      "loss": 0.6763,
      "step": 524100
    },
    {
      "epoch": 4.78319585371195,
      "grad_norm": 3.119441509246826,
      "learning_rate": 4.6014003455240045e-05,
      "loss": 0.7018,
      "step": 524200
    },
    {
      "epoch": 4.784108329075115,
      "grad_norm": 5.171349048614502,
      "learning_rate": 4.6013243059104075e-05,
      "loss": 0.7092,
      "step": 524300
    },
    {
      "epoch": 4.78502080443828,
      "grad_norm": 3.689997911453247,
      "learning_rate": 4.6012482662968105e-05,
      "loss": 0.7057,
      "step": 524400
    },
    {
      "epoch": 4.785933279801445,
      "grad_norm": 4.658236026763916,
      "learning_rate": 4.601172226683213e-05,
      "loss": 0.7179,
      "step": 524500
    },
    {
      "epoch": 4.78684575516461,
      "grad_norm": 4.082479000091553,
      "learning_rate": 4.6010961870696165e-05,
      "loss": 0.7523,
      "step": 524600
    },
    {
      "epoch": 4.787758230527776,
      "grad_norm": 3.4890713691711426,
      "learning_rate": 4.601020147456019e-05,
      "loss": 0.6851,
      "step": 524700
    },
    {
      "epoch": 4.788670705890941,
      "grad_norm": 3.8034520149230957,
      "learning_rate": 4.600944107842422e-05,
      "loss": 0.7176,
      "step": 524800
    },
    {
      "epoch": 4.789583181254106,
      "grad_norm": 4.069993019104004,
      "learning_rate": 4.600868068228825e-05,
      "loss": 0.7291,
      "step": 524900
    },
    {
      "epoch": 4.790495656617272,
      "grad_norm": 4.94764518737793,
      "learning_rate": 4.600792028615228e-05,
      "loss": 0.6935,
      "step": 525000
    },
    {
      "epoch": 4.791408131980437,
      "grad_norm": 4.567398548126221,
      "learning_rate": 4.60071598900163e-05,
      "loss": 0.7472,
      "step": 525100
    },
    {
      "epoch": 4.792320607343601,
      "grad_norm": 3.422315835952759,
      "learning_rate": 4.600639949388034e-05,
      "loss": 0.6862,
      "step": 525200
    },
    {
      "epoch": 4.793233082706767,
      "grad_norm": 4.032036781311035,
      "learning_rate": 4.600563909774436e-05,
      "loss": 0.6878,
      "step": 525300
    },
    {
      "epoch": 4.794145558069932,
      "grad_norm": 4.1269636154174805,
      "learning_rate": 4.600487870160839e-05,
      "loss": 0.7144,
      "step": 525400
    },
    {
      "epoch": 4.795058033433097,
      "grad_norm": 4.343192100524902,
      "learning_rate": 4.600411830547242e-05,
      "loss": 0.7081,
      "step": 525500
    },
    {
      "epoch": 4.7959705087962625,
      "grad_norm": 3.7882447242736816,
      "learning_rate": 4.6003357909336445e-05,
      "loss": 0.7248,
      "step": 525600
    },
    {
      "epoch": 4.796882984159428,
      "grad_norm": 5.457171440124512,
      "learning_rate": 4.600259751320048e-05,
      "loss": 0.7054,
      "step": 525700
    },
    {
      "epoch": 4.797795459522593,
      "grad_norm": 4.135532855987549,
      "learning_rate": 4.6001837117064505e-05,
      "loss": 0.7127,
      "step": 525800
    },
    {
      "epoch": 4.798707934885758,
      "grad_norm": 3.27457332611084,
      "learning_rate": 4.6001076720928535e-05,
      "loss": 0.71,
      "step": 525900
    },
    {
      "epoch": 4.799620410248924,
      "grad_norm": 4.57355260848999,
      "learning_rate": 4.6000316324792566e-05,
      "loss": 0.7143,
      "step": 526000
    },
    {
      "epoch": 4.800532885612088,
      "grad_norm": 4.4704909324646,
      "learning_rate": 4.5999555928656596e-05,
      "loss": 0.7429,
      "step": 526100
    },
    {
      "epoch": 4.801445360975253,
      "grad_norm": 4.513559341430664,
      "learning_rate": 4.599879553252062e-05,
      "loss": 0.7152,
      "step": 526200
    },
    {
      "epoch": 4.802357836338419,
      "grad_norm": 1.7162790298461914,
      "learning_rate": 4.5998035136384656e-05,
      "loss": 0.6958,
      "step": 526300
    },
    {
      "epoch": 4.803270311701584,
      "grad_norm": 4.286496639251709,
      "learning_rate": 4.599727474024868e-05,
      "loss": 0.7057,
      "step": 526400
    },
    {
      "epoch": 4.804182787064749,
      "grad_norm": 4.7258782386779785,
      "learning_rate": 4.599651434411271e-05,
      "loss": 0.743,
      "step": 526500
    },
    {
      "epoch": 4.805095262427915,
      "grad_norm": 4.6799774169921875,
      "learning_rate": 4.599575394797674e-05,
      "loss": 0.733,
      "step": 526600
    },
    {
      "epoch": 4.80600773779108,
      "grad_norm": 4.824455738067627,
      "learning_rate": 4.599499355184077e-05,
      "loss": 0.7281,
      "step": 526700
    },
    {
      "epoch": 4.806920213154244,
      "grad_norm": 4.287386417388916,
      "learning_rate": 4.59942331557048e-05,
      "loss": 0.7044,
      "step": 526800
    },
    {
      "epoch": 4.80783268851741,
      "grad_norm": 4.122208595275879,
      "learning_rate": 4.599347275956883e-05,
      "loss": 0.693,
      "step": 526900
    },
    {
      "epoch": 4.808745163880575,
      "grad_norm": 3.201770544052124,
      "learning_rate": 4.599271236343285e-05,
      "loss": 0.7076,
      "step": 527000
    },
    {
      "epoch": 4.80965763924374,
      "grad_norm": 3.705120801925659,
      "learning_rate": 4.599195196729689e-05,
      "loss": 0.6865,
      "step": 527100
    },
    {
      "epoch": 4.8105701146069055,
      "grad_norm": 4.787154197692871,
      "learning_rate": 4.599119157116091e-05,
      "loss": 0.7007,
      "step": 527200
    },
    {
      "epoch": 4.811482589970071,
      "grad_norm": 3.4957528114318848,
      "learning_rate": 4.599043117502494e-05,
      "loss": 0.6975,
      "step": 527300
    },
    {
      "epoch": 4.812395065333236,
      "grad_norm": 3.833997964859009,
      "learning_rate": 4.598967077888897e-05,
      "loss": 0.6893,
      "step": 527400
    },
    {
      "epoch": 4.813307540696401,
      "grad_norm": 3.173109531402588,
      "learning_rate": 4.5988910382753e-05,
      "loss": 0.7018,
      "step": 527500
    },
    {
      "epoch": 4.814220016059567,
      "grad_norm": 4.073892593383789,
      "learning_rate": 4.5988149986617026e-05,
      "loss": 0.7068,
      "step": 527600
    },
    {
      "epoch": 4.815132491422732,
      "grad_norm": 4.855428695678711,
      "learning_rate": 4.598738959048106e-05,
      "loss": 0.7291,
      "step": 527700
    },
    {
      "epoch": 4.816044966785896,
      "grad_norm": 3.723571300506592,
      "learning_rate": 4.5986629194345086e-05,
      "loss": 0.7155,
      "step": 527800
    },
    {
      "epoch": 4.816957442149062,
      "grad_norm": 3.8469536304473877,
      "learning_rate": 4.5985868798209116e-05,
      "loss": 0.7259,
      "step": 527900
    },
    {
      "epoch": 4.817869917512227,
      "grad_norm": 3.2889466285705566,
      "learning_rate": 4.5985108402073147e-05,
      "loss": 0.7194,
      "step": 528000
    },
    {
      "epoch": 4.818782392875392,
      "grad_norm": 3.7276463508605957,
      "learning_rate": 4.598434800593718e-05,
      "loss": 0.7082,
      "step": 528100
    },
    {
      "epoch": 4.819694868238558,
      "grad_norm": 4.405858516693115,
      "learning_rate": 4.598358760980121e-05,
      "loss": 0.721,
      "step": 528200
    },
    {
      "epoch": 4.820607343601723,
      "grad_norm": 4.273068904876709,
      "learning_rate": 4.598282721366523e-05,
      "loss": 0.7164,
      "step": 528300
    },
    {
      "epoch": 4.821519818964888,
      "grad_norm": 4.016889572143555,
      "learning_rate": 4.598206681752926e-05,
      "loss": 0.7527,
      "step": 528400
    },
    {
      "epoch": 4.822432294328053,
      "grad_norm": 3.5589723587036133,
      "learning_rate": 4.598130642139329e-05,
      "loss": 0.7276,
      "step": 528500
    },
    {
      "epoch": 4.823344769691218,
      "grad_norm": 3.910355567932129,
      "learning_rate": 4.598054602525732e-05,
      "loss": 0.6851,
      "step": 528600
    },
    {
      "epoch": 4.824257245054383,
      "grad_norm": 4.316831111907959,
      "learning_rate": 4.597978562912135e-05,
      "loss": 0.7219,
      "step": 528700
    },
    {
      "epoch": 4.8251697204175485,
      "grad_norm": 3.073051691055298,
      "learning_rate": 4.597902523298538e-05,
      "loss": 0.678,
      "step": 528800
    },
    {
      "epoch": 4.826082195780714,
      "grad_norm": 3.682408094406128,
      "learning_rate": 4.5978264836849404e-05,
      "loss": 0.7217,
      "step": 528900
    },
    {
      "epoch": 4.826994671143879,
      "grad_norm": 3.8793325424194336,
      "learning_rate": 4.5977504440713434e-05,
      "loss": 0.7421,
      "step": 529000
    },
    {
      "epoch": 4.8279071465070444,
      "grad_norm": 3.9892354011535645,
      "learning_rate": 4.5976744044577464e-05,
      "loss": 0.7218,
      "step": 529100
    },
    {
      "epoch": 4.82881962187021,
      "grad_norm": 3.4949285984039307,
      "learning_rate": 4.5975983648441494e-05,
      "loss": 0.6622,
      "step": 529200
    },
    {
      "epoch": 4.829732097233375,
      "grad_norm": 3.9438109397888184,
      "learning_rate": 4.5975223252305524e-05,
      "loss": 0.7501,
      "step": 529300
    },
    {
      "epoch": 4.83064457259654,
      "grad_norm": 4.6081085205078125,
      "learning_rate": 4.5974462856169554e-05,
      "loss": 0.7479,
      "step": 529400
    },
    {
      "epoch": 4.831557047959705,
      "grad_norm": 3.37274169921875,
      "learning_rate": 4.597370246003358e-05,
      "loss": 0.6876,
      "step": 529500
    },
    {
      "epoch": 4.83246952332287,
      "grad_norm": 4.267991065979004,
      "learning_rate": 4.5972942063897614e-05,
      "loss": 0.7209,
      "step": 529600
    },
    {
      "epoch": 4.833381998686035,
      "grad_norm": 4.753071308135986,
      "learning_rate": 4.597218166776164e-05,
      "loss": 0.7291,
      "step": 529700
    },
    {
      "epoch": 4.834294474049201,
      "grad_norm": 4.118625164031982,
      "learning_rate": 4.597142127162567e-05,
      "loss": 0.7172,
      "step": 529800
    },
    {
      "epoch": 4.835206949412366,
      "grad_norm": 4.65009880065918,
      "learning_rate": 4.59706608754897e-05,
      "loss": 0.7377,
      "step": 529900
    },
    {
      "epoch": 4.836119424775531,
      "grad_norm": 3.6661171913146973,
      "learning_rate": 4.596990047935373e-05,
      "loss": 0.698,
      "step": 530000
    },
    {
      "epoch": 4.8370319001386965,
      "grad_norm": 4.0015740394592285,
      "learning_rate": 4.596914008321776e-05,
      "loss": 0.6993,
      "step": 530100
    },
    {
      "epoch": 4.837944375501861,
      "grad_norm": 4.4947733879089355,
      "learning_rate": 4.596837968708179e-05,
      "loss": 0.7362,
      "step": 530200
    },
    {
      "epoch": 4.838856850865026,
      "grad_norm": 4.794332504272461,
      "learning_rate": 4.596761929094581e-05,
      "loss": 0.7012,
      "step": 530300
    },
    {
      "epoch": 4.839769326228192,
      "grad_norm": 4.419819355010986,
      "learning_rate": 4.596685889480984e-05,
      "loss": 0.7093,
      "step": 530400
    },
    {
      "epoch": 4.840681801591357,
      "grad_norm": 4.60386848449707,
      "learning_rate": 4.596609849867387e-05,
      "loss": 0.7284,
      "step": 530500
    },
    {
      "epoch": 4.841594276954522,
      "grad_norm": 4.615325927734375,
      "learning_rate": 4.59653381025379e-05,
      "loss": 0.7075,
      "step": 530600
    },
    {
      "epoch": 4.8425067523176875,
      "grad_norm": 4.482704162597656,
      "learning_rate": 4.596457770640193e-05,
      "loss": 0.7504,
      "step": 530700
    },
    {
      "epoch": 4.843419227680853,
      "grad_norm": 6.0409369468688965,
      "learning_rate": 4.596381731026596e-05,
      "loss": 0.7258,
      "step": 530800
    },
    {
      "epoch": 4.844331703044018,
      "grad_norm": 4.287993431091309,
      "learning_rate": 4.5963056914129985e-05,
      "loss": 0.726,
      "step": 530900
    },
    {
      "epoch": 4.845244178407183,
      "grad_norm": 4.14185905456543,
      "learning_rate": 4.596229651799402e-05,
      "loss": 0.751,
      "step": 531000
    },
    {
      "epoch": 4.846156653770349,
      "grad_norm": 4.572870254516602,
      "learning_rate": 4.5961536121858045e-05,
      "loss": 0.7763,
      "step": 531100
    },
    {
      "epoch": 4.847069129133513,
      "grad_norm": 3.488861083984375,
      "learning_rate": 4.5960775725722075e-05,
      "loss": 0.7299,
      "step": 531200
    },
    {
      "epoch": 4.847981604496678,
      "grad_norm": 3.9392614364624023,
      "learning_rate": 4.5960015329586105e-05,
      "loss": 0.7366,
      "step": 531300
    },
    {
      "epoch": 4.848894079859844,
      "grad_norm": 4.396456718444824,
      "learning_rate": 4.595925493345013e-05,
      "loss": 0.7106,
      "step": 531400
    },
    {
      "epoch": 4.849806555223009,
      "grad_norm": 4.306144714355469,
      "learning_rate": 4.5958494537314165e-05,
      "loss": 0.6874,
      "step": 531500
    },
    {
      "epoch": 4.850719030586174,
      "grad_norm": 3.3847062587738037,
      "learning_rate": 4.595773414117819e-05,
      "loss": 0.7313,
      "step": 531600
    },
    {
      "epoch": 4.85163150594934,
      "grad_norm": 3.2966818809509277,
      "learning_rate": 4.595697374504222e-05,
      "loss": 0.7229,
      "step": 531700
    },
    {
      "epoch": 4.852543981312505,
      "grad_norm": 4.36367130279541,
      "learning_rate": 4.595621334890625e-05,
      "loss": 0.7322,
      "step": 531800
    },
    {
      "epoch": 4.853456456675669,
      "grad_norm": 4.262944221496582,
      "learning_rate": 4.595545295277028e-05,
      "loss": 0.7131,
      "step": 531900
    },
    {
      "epoch": 4.854368932038835,
      "grad_norm": 4.386026382446289,
      "learning_rate": 4.59546925566343e-05,
      "loss": 0.7003,
      "step": 532000
    },
    {
      "epoch": 4.855281407402,
      "grad_norm": 3.794064521789551,
      "learning_rate": 4.595393216049834e-05,
      "loss": 0.7176,
      "step": 532100
    },
    {
      "epoch": 4.856193882765165,
      "grad_norm": 4.578563690185547,
      "learning_rate": 4.595317176436236e-05,
      "loss": 0.7208,
      "step": 532200
    },
    {
      "epoch": 4.8571063581283305,
      "grad_norm": 4.811943531036377,
      "learning_rate": 4.595241136822639e-05,
      "loss": 0.7616,
      "step": 532300
    },
    {
      "epoch": 4.858018833491496,
      "grad_norm": 4.440401077270508,
      "learning_rate": 4.595165097209042e-05,
      "loss": 0.7162,
      "step": 532400
    },
    {
      "epoch": 4.858931308854661,
      "grad_norm": 3.5461883544921875,
      "learning_rate": 4.595089057595445e-05,
      "loss": 0.74,
      "step": 532500
    },
    {
      "epoch": 4.859843784217826,
      "grad_norm": 3.9919140338897705,
      "learning_rate": 4.595013017981848e-05,
      "loss": 0.721,
      "step": 532600
    },
    {
      "epoch": 4.860756259580992,
      "grad_norm": 4.139779567718506,
      "learning_rate": 4.594936978368251e-05,
      "loss": 0.7288,
      "step": 532700
    },
    {
      "epoch": 4.861668734944157,
      "grad_norm": 4.128092288970947,
      "learning_rate": 4.5948609387546536e-05,
      "loss": 0.7186,
      "step": 532800
    },
    {
      "epoch": 4.862581210307321,
      "grad_norm": 3.155564546585083,
      "learning_rate": 4.594784899141057e-05,
      "loss": 0.7388,
      "step": 532900
    },
    {
      "epoch": 4.863493685670487,
      "grad_norm": 3.980477809906006,
      "learning_rate": 4.5947088595274596e-05,
      "loss": 0.7072,
      "step": 533000
    },
    {
      "epoch": 4.864406161033652,
      "grad_norm": 4.204802989959717,
      "learning_rate": 4.5946328199138626e-05,
      "loss": 0.7395,
      "step": 533100
    },
    {
      "epoch": 4.865318636396817,
      "grad_norm": 4.090383529663086,
      "learning_rate": 4.5945567803002656e-05,
      "loss": 0.7246,
      "step": 533200
    },
    {
      "epoch": 4.866231111759983,
      "grad_norm": 4.143317699432373,
      "learning_rate": 4.5944807406866686e-05,
      "loss": 0.7495,
      "step": 533300
    },
    {
      "epoch": 4.867143587123148,
      "grad_norm": 4.298027038574219,
      "learning_rate": 4.594404701073071e-05,
      "loss": 0.7187,
      "step": 533400
    },
    {
      "epoch": 4.868056062486313,
      "grad_norm": 3.616858959197998,
      "learning_rate": 4.5943286614594746e-05,
      "loss": 0.6895,
      "step": 533500
    },
    {
      "epoch": 4.868968537849478,
      "grad_norm": 4.029165744781494,
      "learning_rate": 4.594252621845877e-05,
      "loss": 0.7398,
      "step": 533600
    },
    {
      "epoch": 4.869881013212643,
      "grad_norm": 4.6915154457092285,
      "learning_rate": 4.59417658223228e-05,
      "loss": 0.7181,
      "step": 533700
    },
    {
      "epoch": 4.870793488575808,
      "grad_norm": 1.914466381072998,
      "learning_rate": 4.594100542618683e-05,
      "loss": 0.7016,
      "step": 533800
    },
    {
      "epoch": 4.8717059639389735,
      "grad_norm": 4.805741786956787,
      "learning_rate": 4.594024503005086e-05,
      "loss": 0.743,
      "step": 533900
    },
    {
      "epoch": 4.872618439302139,
      "grad_norm": 4.7752227783203125,
      "learning_rate": 4.593948463391489e-05,
      "loss": 0.7285,
      "step": 534000
    },
    {
      "epoch": 4.873530914665304,
      "grad_norm": 4.082581043243408,
      "learning_rate": 4.593872423777891e-05,
      "loss": 0.7373,
      "step": 534100
    },
    {
      "epoch": 4.874443390028469,
      "grad_norm": 3.8994462490081787,
      "learning_rate": 4.593796384164294e-05,
      "loss": 0.7085,
      "step": 534200
    },
    {
      "epoch": 4.875355865391635,
      "grad_norm": 3.455911159515381,
      "learning_rate": 4.593720344550697e-05,
      "loss": 0.7399,
      "step": 534300
    },
    {
      "epoch": 4.8762683407548,
      "grad_norm": 3.964561939239502,
      "learning_rate": 4.5936443049371e-05,
      "loss": 0.7106,
      "step": 534400
    },
    {
      "epoch": 4.877180816117965,
      "grad_norm": 3.9203968048095703,
      "learning_rate": 4.5935682653235026e-05,
      "loss": 0.7299,
      "step": 534500
    },
    {
      "epoch": 4.87809329148113,
      "grad_norm": 4.484302520751953,
      "learning_rate": 4.593492225709906e-05,
      "loss": 0.7413,
      "step": 534600
    },
    {
      "epoch": 4.879005766844295,
      "grad_norm": 4.071152687072754,
      "learning_rate": 4.5934161860963087e-05,
      "loss": 0.7105,
      "step": 534700
    },
    {
      "epoch": 4.87991824220746,
      "grad_norm": 5.0931501388549805,
      "learning_rate": 4.5933401464827117e-05,
      "loss": 0.6938,
      "step": 534800
    },
    {
      "epoch": 4.880830717570626,
      "grad_norm": 4.160575866699219,
      "learning_rate": 4.593264106869115e-05,
      "loss": 0.7294,
      "step": 534900
    },
    {
      "epoch": 4.881743192933791,
      "grad_norm": 5.801011562347412,
      "learning_rate": 4.593188067255518e-05,
      "loss": 0.7248,
      "step": 535000
    },
    {
      "epoch": 4.882655668296956,
      "grad_norm": 4.182128429412842,
      "learning_rate": 4.593112027641921e-05,
      "loss": 0.7326,
      "step": 535100
    },
    {
      "epoch": 4.8835681436601215,
      "grad_norm": 3.5813374519348145,
      "learning_rate": 4.593035988028324e-05,
      "loss": 0.7179,
      "step": 535200
    },
    {
      "epoch": 4.884480619023286,
      "grad_norm": 4.627518177032471,
      "learning_rate": 4.592959948414726e-05,
      "loss": 0.678,
      "step": 535300
    },
    {
      "epoch": 4.885393094386451,
      "grad_norm": 4.404438495635986,
      "learning_rate": 4.59288390880113e-05,
      "loss": 0.7079,
      "step": 535400
    },
    {
      "epoch": 4.8863055697496165,
      "grad_norm": 3.754307508468628,
      "learning_rate": 4.592807869187532e-05,
      "loss": 0.6938,
      "step": 535500
    },
    {
      "epoch": 4.887218045112782,
      "grad_norm": 6.979640960693359,
      "learning_rate": 4.592731829573935e-05,
      "loss": 0.7242,
      "step": 535600
    },
    {
      "epoch": 4.888130520475947,
      "grad_norm": 4.580474853515625,
      "learning_rate": 4.592655789960338e-05,
      "loss": 0.7459,
      "step": 535700
    },
    {
      "epoch": 4.889042995839112,
      "grad_norm": 3.8495991230010986,
      "learning_rate": 4.592579750346741e-05,
      "loss": 0.7054,
      "step": 535800
    },
    {
      "epoch": 4.889955471202278,
      "grad_norm": 3.935213088989258,
      "learning_rate": 4.5925037107331434e-05,
      "loss": 0.7474,
      "step": 535900
    },
    {
      "epoch": 4.890867946565443,
      "grad_norm": 3.708354949951172,
      "learning_rate": 4.592427671119547e-05,
      "loss": 0.7308,
      "step": 536000
    },
    {
      "epoch": 4.891780421928608,
      "grad_norm": 3.7595181465148926,
      "learning_rate": 4.5923516315059494e-05,
      "loss": 0.693,
      "step": 536100
    },
    {
      "epoch": 4.892692897291774,
      "grad_norm": 3.7971065044403076,
      "learning_rate": 4.5922755918923524e-05,
      "loss": 0.6888,
      "step": 536200
    },
    {
      "epoch": 4.893605372654938,
      "grad_norm": 4.203096866607666,
      "learning_rate": 4.5921995522787554e-05,
      "loss": 0.6914,
      "step": 536300
    },
    {
      "epoch": 4.894517848018103,
      "grad_norm": 4.995347499847412,
      "learning_rate": 4.5921235126651584e-05,
      "loss": 0.7196,
      "step": 536400
    },
    {
      "epoch": 4.895430323381269,
      "grad_norm": 3.407191753387451,
      "learning_rate": 4.5920474730515614e-05,
      "loss": 0.7225,
      "step": 536500
    },
    {
      "epoch": 4.896342798744434,
      "grad_norm": 5.3021464347839355,
      "learning_rate": 4.5919714334379644e-05,
      "loss": 0.723,
      "step": 536600
    },
    {
      "epoch": 4.897255274107599,
      "grad_norm": 4.152379989624023,
      "learning_rate": 4.591895393824367e-05,
      "loss": 0.6651,
      "step": 536700
    },
    {
      "epoch": 4.8981677494707645,
      "grad_norm": 4.444252967834473,
      "learning_rate": 4.59181935421077e-05,
      "loss": 0.6926,
      "step": 536800
    },
    {
      "epoch": 4.89908022483393,
      "grad_norm": 4.331015586853027,
      "learning_rate": 4.591743314597173e-05,
      "loss": 0.7585,
      "step": 536900
    },
    {
      "epoch": 4.899992700197094,
      "grad_norm": 4.445972919464111,
      "learning_rate": 4.591667274983575e-05,
      "loss": 0.7268,
      "step": 537000
    },
    {
      "epoch": 4.9009051755602595,
      "grad_norm": 3.3963799476623535,
      "learning_rate": 4.591591235369979e-05,
      "loss": 0.7292,
      "step": 537100
    },
    {
      "epoch": 4.901817650923425,
      "grad_norm": 3.0502820014953613,
      "learning_rate": 4.591515195756381e-05,
      "loss": 0.727,
      "step": 537200
    },
    {
      "epoch": 4.90273012628659,
      "grad_norm": 3.728485584259033,
      "learning_rate": 4.591439156142784e-05,
      "loss": 0.7172,
      "step": 537300
    },
    {
      "epoch": 4.903642601649755,
      "grad_norm": 4.186319351196289,
      "learning_rate": 4.591363116529187e-05,
      "loss": 0.7032,
      "step": 537400
    },
    {
      "epoch": 4.904555077012921,
      "grad_norm": 4.076717376708984,
      "learning_rate": 4.59128707691559e-05,
      "loss": 0.7469,
      "step": 537500
    },
    {
      "epoch": 4.905467552376086,
      "grad_norm": 2.560633659362793,
      "learning_rate": 4.591211037301993e-05,
      "loss": 0.7729,
      "step": 537600
    },
    {
      "epoch": 4.906380027739251,
      "grad_norm": 4.1645283699035645,
      "learning_rate": 4.591134997688396e-05,
      "loss": 0.7035,
      "step": 537700
    },
    {
      "epoch": 4.907292503102417,
      "grad_norm": 3.9171321392059326,
      "learning_rate": 4.5910589580747985e-05,
      "loss": 0.7403,
      "step": 537800
    },
    {
      "epoch": 4.908204978465582,
      "grad_norm": 4.518233776092529,
      "learning_rate": 4.590982918461202e-05,
      "loss": 0.7137,
      "step": 537900
    },
    {
      "epoch": 4.909117453828746,
      "grad_norm": 3.9534239768981934,
      "learning_rate": 4.5909068788476045e-05,
      "loss": 0.665,
      "step": 538000
    },
    {
      "epoch": 4.910029929191912,
      "grad_norm": 3.909273862838745,
      "learning_rate": 4.5908308392340075e-05,
      "loss": 0.7462,
      "step": 538100
    },
    {
      "epoch": 4.910942404555077,
      "grad_norm": 4.4456467628479,
      "learning_rate": 4.5907547996204105e-05,
      "loss": 0.7198,
      "step": 538200
    },
    {
      "epoch": 4.911854879918242,
      "grad_norm": 3.0826072692871094,
      "learning_rate": 4.5906787600068135e-05,
      "loss": 0.7757,
      "step": 538300
    },
    {
      "epoch": 4.9127673552814075,
      "grad_norm": 4.181487083435059,
      "learning_rate": 4.590602720393216e-05,
      "loss": 0.7356,
      "step": 538400
    },
    {
      "epoch": 4.913679830644573,
      "grad_norm": 4.781076431274414,
      "learning_rate": 4.5905266807796195e-05,
      "loss": 0.7318,
      "step": 538500
    },
    {
      "epoch": 4.914592306007738,
      "grad_norm": 4.244231224060059,
      "learning_rate": 4.590450641166022e-05,
      "loss": 0.6916,
      "step": 538600
    },
    {
      "epoch": 4.9155047813709025,
      "grad_norm": 4.361503601074219,
      "learning_rate": 4.590374601552425e-05,
      "loss": 0.7291,
      "step": 538700
    },
    {
      "epoch": 4.916417256734068,
      "grad_norm": 5.048709392547607,
      "learning_rate": 4.590298561938828e-05,
      "loss": 0.7267,
      "step": 538800
    },
    {
      "epoch": 4.917329732097233,
      "grad_norm": 1.8603758811950684,
      "learning_rate": 4.590222522325231e-05,
      "loss": 0.7182,
      "step": 538900
    },
    {
      "epoch": 4.918242207460398,
      "grad_norm": 3.6467034816741943,
      "learning_rate": 4.590146482711634e-05,
      "loss": 0.7313,
      "step": 539000
    },
    {
      "epoch": 4.919154682823564,
      "grad_norm": 4.235930919647217,
      "learning_rate": 4.590070443098037e-05,
      "loss": 0.742,
      "step": 539100
    },
    {
      "epoch": 4.920067158186729,
      "grad_norm": 2.6662628650665283,
      "learning_rate": 4.589994403484439e-05,
      "loss": 0.696,
      "step": 539200
    },
    {
      "epoch": 4.920979633549894,
      "grad_norm": 4.4404826164245605,
      "learning_rate": 4.589918363870843e-05,
      "loss": 0.6957,
      "step": 539300
    },
    {
      "epoch": 4.92189210891306,
      "grad_norm": 4.400241374969482,
      "learning_rate": 4.589842324257245e-05,
      "loss": 0.7167,
      "step": 539400
    },
    {
      "epoch": 4.922804584276225,
      "grad_norm": 3.9486451148986816,
      "learning_rate": 4.589766284643648e-05,
      "loss": 0.742,
      "step": 539500
    },
    {
      "epoch": 4.92371705963939,
      "grad_norm": 4.391982078552246,
      "learning_rate": 4.589690245030051e-05,
      "loss": 0.7313,
      "step": 539600
    },
    {
      "epoch": 4.924629535002555,
      "grad_norm": 4.2817487716674805,
      "learning_rate": 4.5896142054164536e-05,
      "loss": 0.6893,
      "step": 539700
    },
    {
      "epoch": 4.92554201036572,
      "grad_norm": 3.7041852474212646,
      "learning_rate": 4.5895381658028566e-05,
      "loss": 0.7411,
      "step": 539800
    },
    {
      "epoch": 4.926454485728885,
      "grad_norm": 4.448864936828613,
      "learning_rate": 4.5894621261892596e-05,
      "loss": 0.7164,
      "step": 539900
    },
    {
      "epoch": 4.9273669610920505,
      "grad_norm": 3.6311867237091064,
      "learning_rate": 4.5893860865756626e-05,
      "loss": 0.7052,
      "step": 540000
    },
    {
      "epoch": 4.928279436455216,
      "grad_norm": 5.067763805389404,
      "learning_rate": 4.5893100469620656e-05,
      "loss": 0.7578,
      "step": 540100
    },
    {
      "epoch": 4.929191911818381,
      "grad_norm": 5.172374725341797,
      "learning_rate": 4.5892340073484686e-05,
      "loss": 0.7019,
      "step": 540200
    },
    {
      "epoch": 4.930104387181546,
      "grad_norm": 3.4215524196624756,
      "learning_rate": 4.589157967734871e-05,
      "loss": 0.7377,
      "step": 540300
    },
    {
      "epoch": 4.931016862544711,
      "grad_norm": 4.210084915161133,
      "learning_rate": 4.5890819281212746e-05,
      "loss": 0.7137,
      "step": 540400
    },
    {
      "epoch": 4.931929337907876,
      "grad_norm": 4.275950908660889,
      "learning_rate": 4.589005888507677e-05,
      "loss": 0.7198,
      "step": 540500
    },
    {
      "epoch": 4.932841813271041,
      "grad_norm": 4.376723766326904,
      "learning_rate": 4.58892984889408e-05,
      "loss": 0.6806,
      "step": 540600
    },
    {
      "epoch": 4.933754288634207,
      "grad_norm": 3.799349784851074,
      "learning_rate": 4.588853809280483e-05,
      "loss": 0.721,
      "step": 540700
    },
    {
      "epoch": 4.934666763997372,
      "grad_norm": 3.0648348331451416,
      "learning_rate": 4.588777769666886e-05,
      "loss": 0.7067,
      "step": 540800
    },
    {
      "epoch": 4.935579239360537,
      "grad_norm": 5.26121187210083,
      "learning_rate": 4.588701730053288e-05,
      "loss": 0.7357,
      "step": 540900
    },
    {
      "epoch": 4.936491714723703,
      "grad_norm": 2.6128628253936768,
      "learning_rate": 4.588625690439692e-05,
      "loss": 0.7437,
      "step": 541000
    },
    {
      "epoch": 4.937404190086868,
      "grad_norm": 3.6473050117492676,
      "learning_rate": 4.588549650826094e-05,
      "loss": 0.7262,
      "step": 541100
    },
    {
      "epoch": 4.938316665450033,
      "grad_norm": 4.309617042541504,
      "learning_rate": 4.588473611212497e-05,
      "loss": 0.7097,
      "step": 541200
    },
    {
      "epoch": 4.9392291408131985,
      "grad_norm": 4.148702144622803,
      "learning_rate": 4.5883975715989e-05,
      "loss": 0.7373,
      "step": 541300
    },
    {
      "epoch": 4.940141616176363,
      "grad_norm": 4.560077667236328,
      "learning_rate": 4.588321531985303e-05,
      "loss": 0.7115,
      "step": 541400
    },
    {
      "epoch": 4.941054091539528,
      "grad_norm": 4.320647239685059,
      "learning_rate": 4.588245492371706e-05,
      "loss": 0.7651,
      "step": 541500
    },
    {
      "epoch": 4.9419665669026935,
      "grad_norm": 4.890129089355469,
      "learning_rate": 4.5881694527581093e-05,
      "loss": 0.7146,
      "step": 541600
    },
    {
      "epoch": 4.942879042265859,
      "grad_norm": 4.254673957824707,
      "learning_rate": 4.588093413144512e-05,
      "loss": 0.6754,
      "step": 541700
    },
    {
      "epoch": 4.943791517629024,
      "grad_norm": 3.546271800994873,
      "learning_rate": 4.5880173735309154e-05,
      "loss": 0.6941,
      "step": 541800
    },
    {
      "epoch": 4.944703992992189,
      "grad_norm": 4.2404022216796875,
      "learning_rate": 4.587941333917318e-05,
      "loss": 0.7379,
      "step": 541900
    },
    {
      "epoch": 4.945616468355355,
      "grad_norm": 3.5319223403930664,
      "learning_rate": 4.587865294303721e-05,
      "loss": 0.7136,
      "step": 542000
    },
    {
      "epoch": 4.946528943718519,
      "grad_norm": 4.262216567993164,
      "learning_rate": 4.587789254690124e-05,
      "loss": 0.7093,
      "step": 542100
    },
    {
      "epoch": 4.947441419081684,
      "grad_norm": 4.694833755493164,
      "learning_rate": 4.587713215076527e-05,
      "loss": 0.6986,
      "step": 542200
    },
    {
      "epoch": 4.94835389444485,
      "grad_norm": 3.5462512969970703,
      "learning_rate": 4.58763717546293e-05,
      "loss": 0.6939,
      "step": 542300
    },
    {
      "epoch": 4.949266369808015,
      "grad_norm": 4.158809661865234,
      "learning_rate": 4.587561135849333e-05,
      "loss": 0.7116,
      "step": 542400
    },
    {
      "epoch": 4.95017884517118,
      "grad_norm": 4.837622165679932,
      "learning_rate": 4.587485096235735e-05,
      "loss": 0.7123,
      "step": 542500
    },
    {
      "epoch": 4.951091320534346,
      "grad_norm": 4.837621688842773,
      "learning_rate": 4.587409056622138e-05,
      "loss": 0.7692,
      "step": 542600
    },
    {
      "epoch": 4.952003795897511,
      "grad_norm": 3.5960867404937744,
      "learning_rate": 4.587333017008541e-05,
      "loss": 0.6824,
      "step": 542700
    },
    {
      "epoch": 4.952916271260676,
      "grad_norm": 3.5705435276031494,
      "learning_rate": 4.5872569773949434e-05,
      "loss": 0.7225,
      "step": 542800
    },
    {
      "epoch": 4.9538287466238415,
      "grad_norm": 4.385049343109131,
      "learning_rate": 4.587180937781347e-05,
      "loss": 0.6862,
      "step": 542900
    },
    {
      "epoch": 4.954741221987007,
      "grad_norm": 3.6552412509918213,
      "learning_rate": 4.5871048981677494e-05,
      "loss": 0.7259,
      "step": 543000
    },
    {
      "epoch": 4.955653697350171,
      "grad_norm": 4.958806991577148,
      "learning_rate": 4.5870288585541524e-05,
      "loss": 0.6911,
      "step": 543100
    },
    {
      "epoch": 4.9565661727133365,
      "grad_norm": 3.101341724395752,
      "learning_rate": 4.5869528189405554e-05,
      "loss": 0.6903,
      "step": 543200
    },
    {
      "epoch": 4.957478648076502,
      "grad_norm": 3.7395997047424316,
      "learning_rate": 4.5868767793269584e-05,
      "loss": 0.6877,
      "step": 543300
    },
    {
      "epoch": 4.958391123439667,
      "grad_norm": 4.0535688400268555,
      "learning_rate": 4.5868007397133614e-05,
      "loss": 0.7148,
      "step": 543400
    },
    {
      "epoch": 4.9593035988028324,
      "grad_norm": 4.356668949127197,
      "learning_rate": 4.5867247000997644e-05,
      "loss": 0.7348,
      "step": 543500
    },
    {
      "epoch": 4.960216074165998,
      "grad_norm": 4.406242847442627,
      "learning_rate": 4.586648660486167e-05,
      "loss": 0.7251,
      "step": 543600
    },
    {
      "epoch": 4.961128549529163,
      "grad_norm": 4.593617916107178,
      "learning_rate": 4.5865726208725704e-05,
      "loss": 0.7429,
      "step": 543700
    },
    {
      "epoch": 4.9620410248923275,
      "grad_norm": 4.8201375007629395,
      "learning_rate": 4.586496581258973e-05,
      "loss": 0.7214,
      "step": 543800
    },
    {
      "epoch": 4.962953500255493,
      "grad_norm": 4.573843479156494,
      "learning_rate": 4.586420541645376e-05,
      "loss": 0.737,
      "step": 543900
    },
    {
      "epoch": 4.963865975618658,
      "grad_norm": 4.384083271026611,
      "learning_rate": 4.586344502031779e-05,
      "loss": 0.7336,
      "step": 544000
    },
    {
      "epoch": 4.964778450981823,
      "grad_norm": 4.533069610595703,
      "learning_rate": 4.586268462418182e-05,
      "loss": 0.6899,
      "step": 544100
    },
    {
      "epoch": 4.965690926344989,
      "grad_norm": 4.145086288452148,
      "learning_rate": 4.586192422804584e-05,
      "loss": 0.7254,
      "step": 544200
    },
    {
      "epoch": 4.966603401708154,
      "grad_norm": 5.733280658721924,
      "learning_rate": 4.586116383190988e-05,
      "loss": 0.6967,
      "step": 544300
    },
    {
      "epoch": 4.967515877071319,
      "grad_norm": 4.319029331207275,
      "learning_rate": 4.58604034357739e-05,
      "loss": 0.7402,
      "step": 544400
    },
    {
      "epoch": 4.9684283524344846,
      "grad_norm": 4.087490081787109,
      "learning_rate": 4.585964303963793e-05,
      "loss": 0.7751,
      "step": 544500
    },
    {
      "epoch": 4.96934082779765,
      "grad_norm": 4.511039733886719,
      "learning_rate": 4.585888264350196e-05,
      "loss": 0.7488,
      "step": 544600
    },
    {
      "epoch": 4.970253303160815,
      "grad_norm": 3.5148138999938965,
      "learning_rate": 4.585812224736599e-05,
      "loss": 0.7188,
      "step": 544700
    },
    {
      "epoch": 4.97116577852398,
      "grad_norm": 5.415441036224365,
      "learning_rate": 4.585736185123002e-05,
      "loss": 0.6874,
      "step": 544800
    },
    {
      "epoch": 4.972078253887145,
      "grad_norm": 4.466864585876465,
      "learning_rate": 4.585660145509405e-05,
      "loss": 0.6803,
      "step": 544900
    },
    {
      "epoch": 4.97299072925031,
      "grad_norm": 4.196568965911865,
      "learning_rate": 4.5855841058958075e-05,
      "loss": 0.7046,
      "step": 545000
    },
    {
      "epoch": 4.9739032046134755,
      "grad_norm": 5.152771472930908,
      "learning_rate": 4.585508066282211e-05,
      "loss": 0.697,
      "step": 545100
    },
    {
      "epoch": 4.974815679976641,
      "grad_norm": 4.423260688781738,
      "learning_rate": 4.5854320266686135e-05,
      "loss": 0.6964,
      "step": 545200
    },
    {
      "epoch": 4.975728155339806,
      "grad_norm": 3.170649290084839,
      "learning_rate": 4.5853559870550165e-05,
      "loss": 0.7294,
      "step": 545300
    },
    {
      "epoch": 4.976640630702971,
      "grad_norm": 3.257094383239746,
      "learning_rate": 4.5852799474414195e-05,
      "loss": 0.7155,
      "step": 545400
    },
    {
      "epoch": 4.977553106066136,
      "grad_norm": 4.595573902130127,
      "learning_rate": 4.585203907827822e-05,
      "loss": 0.7466,
      "step": 545500
    },
    {
      "epoch": 4.978465581429301,
      "grad_norm": 4.114386081695557,
      "learning_rate": 4.585127868214225e-05,
      "loss": 0.7491,
      "step": 545600
    },
    {
      "epoch": 4.979378056792466,
      "grad_norm": 3.8366339206695557,
      "learning_rate": 4.585051828600628e-05,
      "loss": 0.7379,
      "step": 545700
    },
    {
      "epoch": 4.980290532155632,
      "grad_norm": 3.28639817237854,
      "learning_rate": 4.584975788987031e-05,
      "loss": 0.6959,
      "step": 545800
    },
    {
      "epoch": 4.981203007518797,
      "grad_norm": 4.4804158210754395,
      "learning_rate": 4.584899749373434e-05,
      "loss": 0.6922,
      "step": 545900
    },
    {
      "epoch": 4.982115482881962,
      "grad_norm": 3.8258118629455566,
      "learning_rate": 4.584823709759837e-05,
      "loss": 0.6849,
      "step": 546000
    },
    {
      "epoch": 4.983027958245128,
      "grad_norm": 4.855809688568115,
      "learning_rate": 4.584747670146239e-05,
      "loss": 0.7099,
      "step": 546100
    },
    {
      "epoch": 4.983940433608293,
      "grad_norm": 4.527267932891846,
      "learning_rate": 4.584671630532643e-05,
      "loss": 0.7095,
      "step": 546200
    },
    {
      "epoch": 4.984852908971458,
      "grad_norm": 4.178343772888184,
      "learning_rate": 4.584595590919045e-05,
      "loss": 0.7365,
      "step": 546300
    },
    {
      "epoch": 4.9857653843346235,
      "grad_norm": 3.6935532093048096,
      "learning_rate": 4.584519551305448e-05,
      "loss": 0.6819,
      "step": 546400
    },
    {
      "epoch": 4.986677859697788,
      "grad_norm": 3.762619972229004,
      "learning_rate": 4.584443511691851e-05,
      "loss": 0.7298,
      "step": 546500
    },
    {
      "epoch": 4.987590335060953,
      "grad_norm": 4.751269340515137,
      "learning_rate": 4.584367472078254e-05,
      "loss": 0.7247,
      "step": 546600
    },
    {
      "epoch": 4.9885028104241185,
      "grad_norm": 4.030645370483398,
      "learning_rate": 4.5842914324646566e-05,
      "loss": 0.7276,
      "step": 546700
    },
    {
      "epoch": 4.989415285787284,
      "grad_norm": 4.341917991638184,
      "learning_rate": 4.58421539285106e-05,
      "loss": 0.7424,
      "step": 546800
    },
    {
      "epoch": 4.990327761150449,
      "grad_norm": 5.4365410804748535,
      "learning_rate": 4.5841393532374626e-05,
      "loss": 0.7319,
      "step": 546900
    },
    {
      "epoch": 4.991240236513614,
      "grad_norm": 4.280082702636719,
      "learning_rate": 4.5840633136238656e-05,
      "loss": 0.7188,
      "step": 547000
    },
    {
      "epoch": 4.99215271187678,
      "grad_norm": 4.976367950439453,
      "learning_rate": 4.5839872740102686e-05,
      "loss": 0.6653,
      "step": 547100
    },
    {
      "epoch": 4.993065187239944,
      "grad_norm": 4.481325626373291,
      "learning_rate": 4.5839112343966716e-05,
      "loss": 0.7359,
      "step": 547200
    },
    {
      "epoch": 4.993977662603109,
      "grad_norm": 4.715250492095947,
      "learning_rate": 4.5838351947830746e-05,
      "loss": 0.6815,
      "step": 547300
    },
    {
      "epoch": 4.994890137966275,
      "grad_norm": 4.338933944702148,
      "learning_rate": 4.5837591551694776e-05,
      "loss": 0.7134,
      "step": 547400
    },
    {
      "epoch": 4.99580261332944,
      "grad_norm": 2.8483989238739014,
      "learning_rate": 4.58368311555588e-05,
      "loss": 0.7069,
      "step": 547500
    },
    {
      "epoch": 4.996715088692605,
      "grad_norm": 3.411191940307617,
      "learning_rate": 4.5836070759422836e-05,
      "loss": 0.7339,
      "step": 547600
    },
    {
      "epoch": 4.997627564055771,
      "grad_norm": 4.136804580688477,
      "learning_rate": 4.583531036328686e-05,
      "loss": 0.7189,
      "step": 547700
    },
    {
      "epoch": 4.998540039418936,
      "grad_norm": 4.053274631500244,
      "learning_rate": 4.583454996715089e-05,
      "loss": 0.6742,
      "step": 547800
    },
    {
      "epoch": 4.999452514782101,
      "grad_norm": 5.141177654266357,
      "learning_rate": 4.583378957101492e-05,
      "loss": 0.6892,
      "step": 547900
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5806193351745605,
      "eval_runtime": 25.3284,
      "eval_samples_per_second": 227.768,
      "eval_steps_per_second": 227.768,
      "step": 547960
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5641083717346191,
      "eval_runtime": 482.9055,
      "eval_samples_per_second": 226.943,
      "eval_steps_per_second": 226.943,
      "step": 547960
    },
    {
      "epoch": 5.0003649901452665,
      "grad_norm": 4.309816837310791,
      "learning_rate": 4.583302917487895e-05,
      "loss": 0.7186,
      "step": 548000
    },
    {
      "epoch": 5.001277465508431,
      "grad_norm": 4.689461708068848,
      "learning_rate": 4.583226877874297e-05,
      "loss": 0.7106,
      "step": 548100
    },
    {
      "epoch": 5.002189940871596,
      "grad_norm": 3.872129201889038,
      "learning_rate": 4.5831508382607e-05,
      "loss": 0.6963,
      "step": 548200
    },
    {
      "epoch": 5.0031024162347615,
      "grad_norm": 4.090952396392822,
      "learning_rate": 4.583074798647103e-05,
      "loss": 0.7165,
      "step": 548300
    },
    {
      "epoch": 5.004014891597927,
      "grad_norm": 3.998140335083008,
      "learning_rate": 4.5829987590335063e-05,
      "loss": 0.7417,
      "step": 548400
    },
    {
      "epoch": 5.004927366961092,
      "grad_norm": 4.7439866065979,
      "learning_rate": 4.5829227194199093e-05,
      "loss": 0.7213,
      "step": 548500
    },
    {
      "epoch": 5.005839842324257,
      "grad_norm": 3.850745439529419,
      "learning_rate": 4.582846679806312e-05,
      "loss": 0.7265,
      "step": 548600
    },
    {
      "epoch": 5.006752317687423,
      "grad_norm": 4.167159080505371,
      "learning_rate": 4.5827706401927154e-05,
      "loss": 0.7096,
      "step": 548700
    },
    {
      "epoch": 5.007664793050588,
      "grad_norm": 3.6179914474487305,
      "learning_rate": 4.582694600579118e-05,
      "loss": 0.7193,
      "step": 548800
    },
    {
      "epoch": 5.008577268413752,
      "grad_norm": 4.028410911560059,
      "learning_rate": 4.582618560965521e-05,
      "loss": 0.7501,
      "step": 548900
    },
    {
      "epoch": 5.009489743776918,
      "grad_norm": 4.285787105560303,
      "learning_rate": 4.582542521351924e-05,
      "loss": 0.7139,
      "step": 549000
    },
    {
      "epoch": 5.010402219140083,
      "grad_norm": 3.270338296890259,
      "learning_rate": 4.582466481738327e-05,
      "loss": 0.7033,
      "step": 549100
    },
    {
      "epoch": 5.011314694503248,
      "grad_norm": 3.6099770069122314,
      "learning_rate": 4.582390442124729e-05,
      "loss": 0.7341,
      "step": 549200
    },
    {
      "epoch": 5.012227169866414,
      "grad_norm": 2.487919807434082,
      "learning_rate": 4.582314402511133e-05,
      "loss": 0.7232,
      "step": 549300
    },
    {
      "epoch": 5.013139645229579,
      "grad_norm": 4.894607067108154,
      "learning_rate": 4.582238362897535e-05,
      "loss": 0.7197,
      "step": 549400
    },
    {
      "epoch": 5.014052120592744,
      "grad_norm": 3.746939182281494,
      "learning_rate": 4.582162323283938e-05,
      "loss": 0.7211,
      "step": 549500
    },
    {
      "epoch": 5.0149645959559095,
      "grad_norm": 3.598740339279175,
      "learning_rate": 4.582086283670341e-05,
      "loss": 0.6733,
      "step": 549600
    },
    {
      "epoch": 5.015877071319075,
      "grad_norm": 3.4595141410827637,
      "learning_rate": 4.582010244056744e-05,
      "loss": 0.7337,
      "step": 549700
    },
    {
      "epoch": 5.016789546682239,
      "grad_norm": 4.446200847625732,
      "learning_rate": 4.581934204443147e-05,
      "loss": 0.6989,
      "step": 549800
    },
    {
      "epoch": 5.0177020220454045,
      "grad_norm": 4.308772563934326,
      "learning_rate": 4.58185816482955e-05,
      "loss": 0.6949,
      "step": 549900
    },
    {
      "epoch": 5.01861449740857,
      "grad_norm": 4.273824691772461,
      "learning_rate": 4.5817821252159524e-05,
      "loss": 0.7046,
      "step": 550000
    },
    {
      "epoch": 5.019526972771735,
      "grad_norm": 5.36695671081543,
      "learning_rate": 4.581706085602356e-05,
      "loss": 0.7228,
      "step": 550100
    },
    {
      "epoch": 5.0204394481349,
      "grad_norm": 3.405137062072754,
      "learning_rate": 4.5816300459887584e-05,
      "loss": 0.7115,
      "step": 550200
    },
    {
      "epoch": 5.021351923498066,
      "grad_norm": 3.9505457878112793,
      "learning_rate": 4.5815540063751614e-05,
      "loss": 0.7006,
      "step": 550300
    },
    {
      "epoch": 5.022264398861231,
      "grad_norm": 4.475953578948975,
      "learning_rate": 4.5814779667615644e-05,
      "loss": 0.6986,
      "step": 550400
    },
    {
      "epoch": 5.023176874224396,
      "grad_norm": 4.73861837387085,
      "learning_rate": 4.5814019271479675e-05,
      "loss": 0.707,
      "step": 550500
    },
    {
      "epoch": 5.024089349587561,
      "grad_norm": 3.9123895168304443,
      "learning_rate": 4.58132588753437e-05,
      "loss": 0.6941,
      "step": 550600
    },
    {
      "epoch": 5.025001824950726,
      "grad_norm": 3.924591302871704,
      "learning_rate": 4.5812498479207735e-05,
      "loss": 0.6657,
      "step": 550700
    },
    {
      "epoch": 5.025914300313891,
      "grad_norm": 3.738905191421509,
      "learning_rate": 4.581173808307176e-05,
      "loss": 0.7065,
      "step": 550800
    },
    {
      "epoch": 5.026826775677057,
      "grad_norm": 4.45388650894165,
      "learning_rate": 4.581097768693579e-05,
      "loss": 0.6999,
      "step": 550900
    },
    {
      "epoch": 5.027739251040222,
      "grad_norm": 3.286421298980713,
      "learning_rate": 4.581021729079982e-05,
      "loss": 0.7033,
      "step": 551000
    },
    {
      "epoch": 5.028651726403387,
      "grad_norm": 4.014275074005127,
      "learning_rate": 4.580945689466384e-05,
      "loss": 0.7197,
      "step": 551100
    },
    {
      "epoch": 5.0295642017665525,
      "grad_norm": 4.2042951583862305,
      "learning_rate": 4.580869649852788e-05,
      "loss": 0.6753,
      "step": 551200
    },
    {
      "epoch": 5.030476677129718,
      "grad_norm": 2.846524238586426,
      "learning_rate": 4.58079361023919e-05,
      "loss": 0.6996,
      "step": 551300
    },
    {
      "epoch": 5.031389152492883,
      "grad_norm": 4.105154514312744,
      "learning_rate": 4.580717570625593e-05,
      "loss": 0.7379,
      "step": 551400
    },
    {
      "epoch": 5.0323016278560475,
      "grad_norm": 5.0384697914123535,
      "learning_rate": 4.580641531011996e-05,
      "loss": 0.7363,
      "step": 551500
    },
    {
      "epoch": 5.033214103219213,
      "grad_norm": 3.9477810859680176,
      "learning_rate": 4.580565491398399e-05,
      "loss": 0.7474,
      "step": 551600
    },
    {
      "epoch": 5.034126578582378,
      "grad_norm": 3.2526113986968994,
      "learning_rate": 4.5804894517848015e-05,
      "loss": 0.687,
      "step": 551700
    },
    {
      "epoch": 5.035039053945543,
      "grad_norm": 3.9287400245666504,
      "learning_rate": 4.580413412171205e-05,
      "loss": 0.7199,
      "step": 551800
    },
    {
      "epoch": 5.035951529308709,
      "grad_norm": 4.632115840911865,
      "learning_rate": 4.5803373725576075e-05,
      "loss": 0.7428,
      "step": 551900
    },
    {
      "epoch": 5.036864004671874,
      "grad_norm": 4.5745954513549805,
      "learning_rate": 4.5802613329440105e-05,
      "loss": 0.7562,
      "step": 552000
    },
    {
      "epoch": 5.037776480035039,
      "grad_norm": 4.3108134269714355,
      "learning_rate": 4.5801852933304135e-05,
      "loss": 0.6868,
      "step": 552100
    },
    {
      "epoch": 5.038688955398205,
      "grad_norm": 3.88828444480896,
      "learning_rate": 4.5801092537168165e-05,
      "loss": 0.7033,
      "step": 552200
    },
    {
      "epoch": 5.039601430761369,
      "grad_norm": 3.2192602157592773,
      "learning_rate": 4.5800332141032195e-05,
      "loss": 0.6889,
      "step": 552300
    },
    {
      "epoch": 5.040513906124534,
      "grad_norm": 4.198910713195801,
      "learning_rate": 4.5799571744896225e-05,
      "loss": 0.6968,
      "step": 552400
    },
    {
      "epoch": 5.0414263814877,
      "grad_norm": 5.051853656768799,
      "learning_rate": 4.579881134876025e-05,
      "loss": 0.7319,
      "step": 552500
    },
    {
      "epoch": 5.042338856850865,
      "grad_norm": 3.800381660461426,
      "learning_rate": 4.5798050952624286e-05,
      "loss": 0.6852,
      "step": 552600
    },
    {
      "epoch": 5.04325133221403,
      "grad_norm": 4.670203685760498,
      "learning_rate": 4.579729055648831e-05,
      "loss": 0.7153,
      "step": 552700
    },
    {
      "epoch": 5.0441638075771955,
      "grad_norm": 4.616703987121582,
      "learning_rate": 4.579653016035234e-05,
      "loss": 0.7109,
      "step": 552800
    },
    {
      "epoch": 5.045076282940361,
      "grad_norm": 3.90997052192688,
      "learning_rate": 4.579576976421637e-05,
      "loss": 0.7331,
      "step": 552900
    },
    {
      "epoch": 5.045988758303526,
      "grad_norm": 4.57922887802124,
      "learning_rate": 4.57950093680804e-05,
      "loss": 0.6911,
      "step": 553000
    },
    {
      "epoch": 5.046901233666691,
      "grad_norm": 2.9069881439208984,
      "learning_rate": 4.579424897194442e-05,
      "loss": 0.7506,
      "step": 553100
    },
    {
      "epoch": 5.047813709029856,
      "grad_norm": 3.4952030181884766,
      "learning_rate": 4.579348857580846e-05,
      "loss": 0.753,
      "step": 553200
    },
    {
      "epoch": 5.048726184393021,
      "grad_norm": 3.9400134086608887,
      "learning_rate": 4.579272817967248e-05,
      "loss": 0.721,
      "step": 553300
    },
    {
      "epoch": 5.049638659756186,
      "grad_norm": 4.484191417694092,
      "learning_rate": 4.579196778353651e-05,
      "loss": 0.7214,
      "step": 553400
    },
    {
      "epoch": 5.050551135119352,
      "grad_norm": 4.182705879211426,
      "learning_rate": 4.579120738740054e-05,
      "loss": 0.707,
      "step": 553500
    },
    {
      "epoch": 5.051463610482517,
      "grad_norm": 3.3763344287872314,
      "learning_rate": 4.579044699126457e-05,
      "loss": 0.702,
      "step": 553600
    },
    {
      "epoch": 5.052376085845682,
      "grad_norm": 2.7599120140075684,
      "learning_rate": 4.57896865951286e-05,
      "loss": 0.7386,
      "step": 553700
    },
    {
      "epoch": 5.053288561208848,
      "grad_norm": 3.444101333618164,
      "learning_rate": 4.578892619899263e-05,
      "loss": 0.7185,
      "step": 553800
    },
    {
      "epoch": 5.054201036572013,
      "grad_norm": 3.6962106227874756,
      "learning_rate": 4.5788165802856656e-05,
      "loss": 0.727,
      "step": 553900
    },
    {
      "epoch": 5.055113511935177,
      "grad_norm": 4.1862993240356445,
      "learning_rate": 4.5787405406720686e-05,
      "loss": 0.7326,
      "step": 554000
    },
    {
      "epoch": 5.056025987298343,
      "grad_norm": 3.7555015087127686,
      "learning_rate": 4.5786645010584716e-05,
      "loss": 0.7083,
      "step": 554100
    },
    {
      "epoch": 5.056938462661508,
      "grad_norm": 3.6663410663604736,
      "learning_rate": 4.5785884614448746e-05,
      "loss": 0.7419,
      "step": 554200
    },
    {
      "epoch": 5.057850938024673,
      "grad_norm": 4.085482120513916,
      "learning_rate": 4.5785124218312776e-05,
      "loss": 0.7153,
      "step": 554300
    },
    {
      "epoch": 5.0587634133878385,
      "grad_norm": 4.839076042175293,
      "learning_rate": 4.57843638221768e-05,
      "loss": 0.7224,
      "step": 554400
    },
    {
      "epoch": 5.059675888751004,
      "grad_norm": 3.9112629890441895,
      "learning_rate": 4.578360342604083e-05,
      "loss": 0.724,
      "step": 554500
    },
    {
      "epoch": 5.060588364114169,
      "grad_norm": 4.600238800048828,
      "learning_rate": 4.578284302990486e-05,
      "loss": 0.6903,
      "step": 554600
    },
    {
      "epoch": 5.061500839477334,
      "grad_norm": 5.030231952667236,
      "learning_rate": 4.578208263376889e-05,
      "loss": 0.7258,
      "step": 554700
    },
    {
      "epoch": 5.0624133148405,
      "grad_norm": 4.113384246826172,
      "learning_rate": 4.578132223763292e-05,
      "loss": 0.6906,
      "step": 554800
    },
    {
      "epoch": 5.063325790203664,
      "grad_norm": 4.895646572113037,
      "learning_rate": 4.578056184149695e-05,
      "loss": 0.6812,
      "step": 554900
    },
    {
      "epoch": 5.064238265566829,
      "grad_norm": 3.6031386852264404,
      "learning_rate": 4.577980144536097e-05,
      "loss": 0.7381,
      "step": 555000
    },
    {
      "epoch": 5.065150740929995,
      "grad_norm": 3.8469159603118896,
      "learning_rate": 4.577904104922501e-05,
      "loss": 0.7167,
      "step": 555100
    },
    {
      "epoch": 5.06606321629316,
      "grad_norm": 3.960639476776123,
      "learning_rate": 4.5778280653089033e-05,
      "loss": 0.6975,
      "step": 555200
    },
    {
      "epoch": 5.066975691656325,
      "grad_norm": 3.83532452583313,
      "learning_rate": 4.5777520256953064e-05,
      "loss": 0.7336,
      "step": 555300
    },
    {
      "epoch": 5.067888167019491,
      "grad_norm": 5.580438613891602,
      "learning_rate": 4.5776759860817094e-05,
      "loss": 0.6772,
      "step": 555400
    },
    {
      "epoch": 5.068800642382656,
      "grad_norm": 3.979428768157959,
      "learning_rate": 4.5775999464681124e-05,
      "loss": 0.7112,
      "step": 555500
    },
    {
      "epoch": 5.069713117745821,
      "grad_norm": 5.042942047119141,
      "learning_rate": 4.5775239068545154e-05,
      "loss": 0.7405,
      "step": 555600
    },
    {
      "epoch": 5.070625593108986,
      "grad_norm": 4.649831771850586,
      "learning_rate": 4.5774478672409184e-05,
      "loss": 0.686,
      "step": 555700
    },
    {
      "epoch": 5.071538068472151,
      "grad_norm": 4.775732517242432,
      "learning_rate": 4.577371827627321e-05,
      "loss": 0.694,
      "step": 555800
    },
    {
      "epoch": 5.072450543835316,
      "grad_norm": 3.984391927719116,
      "learning_rate": 4.577295788013724e-05,
      "loss": 0.6967,
      "step": 555900
    },
    {
      "epoch": 5.0733630191984815,
      "grad_norm": 3.0218634605407715,
      "learning_rate": 4.577219748400127e-05,
      "loss": 0.6981,
      "step": 556000
    },
    {
      "epoch": 5.074275494561647,
      "grad_norm": 3.4047138690948486,
      "learning_rate": 4.57714370878653e-05,
      "loss": 0.6957,
      "step": 556100
    },
    {
      "epoch": 5.075187969924812,
      "grad_norm": 4.616887092590332,
      "learning_rate": 4.577067669172933e-05,
      "loss": 0.7154,
      "step": 556200
    },
    {
      "epoch": 5.076100445287977,
      "grad_norm": 3.0760955810546875,
      "learning_rate": 4.576991629559336e-05,
      "loss": 0.6775,
      "step": 556300
    },
    {
      "epoch": 5.077012920651143,
      "grad_norm": 4.206048488616943,
      "learning_rate": 4.576915589945738e-05,
      "loss": 0.6945,
      "step": 556400
    },
    {
      "epoch": 5.077925396014308,
      "grad_norm": 4.604321479797363,
      "learning_rate": 4.576839550332142e-05,
      "loss": 0.6968,
      "step": 556500
    },
    {
      "epoch": 5.0788378713774724,
      "grad_norm": 4.780033111572266,
      "learning_rate": 4.576763510718544e-05,
      "loss": 0.7349,
      "step": 556600
    },
    {
      "epoch": 5.079750346740638,
      "grad_norm": 4.622572898864746,
      "learning_rate": 4.576687471104947e-05,
      "loss": 0.6987,
      "step": 556700
    },
    {
      "epoch": 5.080662822103803,
      "grad_norm": 4.054455280303955,
      "learning_rate": 4.57661143149135e-05,
      "loss": 0.7234,
      "step": 556800
    },
    {
      "epoch": 5.081575297466968,
      "grad_norm": 4.613686561584473,
      "learning_rate": 4.5765353918777524e-05,
      "loss": 0.7383,
      "step": 556900
    },
    {
      "epoch": 5.082487772830134,
      "grad_norm": 4.454588413238525,
      "learning_rate": 4.576459352264156e-05,
      "loss": 0.698,
      "step": 557000
    },
    {
      "epoch": 5.083400248193299,
      "grad_norm": 3.659379243850708,
      "learning_rate": 4.5763833126505584e-05,
      "loss": 0.7195,
      "step": 557100
    },
    {
      "epoch": 5.084312723556464,
      "grad_norm": 4.042923450469971,
      "learning_rate": 4.5763072730369614e-05,
      "loss": 0.703,
      "step": 557200
    },
    {
      "epoch": 5.0852251989196295,
      "grad_norm": 2.365414619445801,
      "learning_rate": 4.5762312334233645e-05,
      "loss": 0.7049,
      "step": 557300
    },
    {
      "epoch": 5.086137674282794,
      "grad_norm": 3.8978731632232666,
      "learning_rate": 4.5761551938097675e-05,
      "loss": 0.6796,
      "step": 557400
    },
    {
      "epoch": 5.087050149645959,
      "grad_norm": 4.406797409057617,
      "learning_rate": 4.57607915419617e-05,
      "loss": 0.7146,
      "step": 557500
    },
    {
      "epoch": 5.0879626250091246,
      "grad_norm": 3.2844698429107666,
      "learning_rate": 4.5760031145825735e-05,
      "loss": 0.6888,
      "step": 557600
    },
    {
      "epoch": 5.08887510037229,
      "grad_norm": 3.161632537841797,
      "learning_rate": 4.575927074968976e-05,
      "loss": 0.6997,
      "step": 557700
    },
    {
      "epoch": 5.089787575735455,
      "grad_norm": 4.409976959228516,
      "learning_rate": 4.575851035355379e-05,
      "loss": 0.6987,
      "step": 557800
    },
    {
      "epoch": 5.0907000510986204,
      "grad_norm": 4.590358257293701,
      "learning_rate": 4.575774995741782e-05,
      "loss": 0.7161,
      "step": 557900
    },
    {
      "epoch": 5.091612526461786,
      "grad_norm": 3.6072516441345215,
      "learning_rate": 4.575698956128185e-05,
      "loss": 0.7489,
      "step": 558000
    },
    {
      "epoch": 5.092525001824951,
      "grad_norm": 4.414759635925293,
      "learning_rate": 4.575622916514588e-05,
      "loss": 0.7301,
      "step": 558100
    },
    {
      "epoch": 5.093437477188116,
      "grad_norm": 4.420087814331055,
      "learning_rate": 4.575546876900991e-05,
      "loss": 0.6719,
      "step": 558200
    },
    {
      "epoch": 5.094349952551281,
      "grad_norm": 4.845860004425049,
      "learning_rate": 4.575470837287393e-05,
      "loss": 0.7106,
      "step": 558300
    },
    {
      "epoch": 5.095262427914446,
      "grad_norm": 4.601391315460205,
      "learning_rate": 4.575394797673797e-05,
      "loss": 0.6703,
      "step": 558400
    },
    {
      "epoch": 5.096174903277611,
      "grad_norm": 5.494359493255615,
      "learning_rate": 4.575318758060199e-05,
      "loss": 0.7247,
      "step": 558500
    },
    {
      "epoch": 5.097087378640777,
      "grad_norm": 5.237854480743408,
      "learning_rate": 4.575242718446602e-05,
      "loss": 0.7106,
      "step": 558600
    },
    {
      "epoch": 5.097999854003942,
      "grad_norm": 3.8607075214385986,
      "learning_rate": 4.575166678833005e-05,
      "loss": 0.6876,
      "step": 558700
    },
    {
      "epoch": 5.098912329367107,
      "grad_norm": 3.7031729221343994,
      "learning_rate": 4.575090639219408e-05,
      "loss": 0.7118,
      "step": 558800
    },
    {
      "epoch": 5.0998248047302726,
      "grad_norm": 3.9760751724243164,
      "learning_rate": 4.5750145996058105e-05,
      "loss": 0.7312,
      "step": 558900
    },
    {
      "epoch": 5.100737280093438,
      "grad_norm": 3.9627139568328857,
      "learning_rate": 4.574938559992214e-05,
      "loss": 0.7286,
      "step": 559000
    },
    {
      "epoch": 5.101649755456602,
      "grad_norm": 4.840254306793213,
      "learning_rate": 4.5748625203786165e-05,
      "loss": 0.7303,
      "step": 559100
    },
    {
      "epoch": 5.102562230819768,
      "grad_norm": 4.590498924255371,
      "learning_rate": 4.5747864807650195e-05,
      "loss": 0.6853,
      "step": 559200
    },
    {
      "epoch": 5.103474706182933,
      "grad_norm": 4.116511344909668,
      "learning_rate": 4.5747104411514226e-05,
      "loss": 0.6641,
      "step": 559300
    },
    {
      "epoch": 5.104387181546098,
      "grad_norm": 4.025757789611816,
      "learning_rate": 4.5746344015378256e-05,
      "loss": 0.7069,
      "step": 559400
    },
    {
      "epoch": 5.1052996569092635,
      "grad_norm": 4.261534690856934,
      "learning_rate": 4.5745583619242286e-05,
      "loss": 0.7049,
      "step": 559500
    },
    {
      "epoch": 5.106212132272429,
      "grad_norm": 4.2517924308776855,
      "learning_rate": 4.574482322310631e-05,
      "loss": 0.7309,
      "step": 559600
    },
    {
      "epoch": 5.107124607635594,
      "grad_norm": 3.7593040466308594,
      "learning_rate": 4.574406282697034e-05,
      "loss": 0.7681,
      "step": 559700
    },
    {
      "epoch": 5.108037082998759,
      "grad_norm": 4.639091491699219,
      "learning_rate": 4.574330243083437e-05,
      "loss": 0.7191,
      "step": 559800
    },
    {
      "epoch": 5.108949558361925,
      "grad_norm": 4.09711217880249,
      "learning_rate": 4.57425420346984e-05,
      "loss": 0.7175,
      "step": 559900
    },
    {
      "epoch": 5.109862033725089,
      "grad_norm": 3.7136170864105225,
      "learning_rate": 4.574178163856242e-05,
      "loss": 0.7292,
      "step": 560000
    },
    {
      "epoch": 5.110774509088254,
      "grad_norm": 4.067484378814697,
      "learning_rate": 4.574102124242646e-05,
      "loss": 0.6834,
      "step": 560100
    },
    {
      "epoch": 5.11168698445142,
      "grad_norm": 4.05373477935791,
      "learning_rate": 4.574026084629048e-05,
      "loss": 0.6726,
      "step": 560200
    },
    {
      "epoch": 5.112599459814585,
      "grad_norm": 4.186582088470459,
      "learning_rate": 4.573950045015451e-05,
      "loss": 0.7135,
      "step": 560300
    },
    {
      "epoch": 5.11351193517775,
      "grad_norm": 4.140480041503906,
      "learning_rate": 4.573874005401854e-05,
      "loss": 0.7178,
      "step": 560400
    },
    {
      "epoch": 5.114424410540916,
      "grad_norm": 4.097560405731201,
      "learning_rate": 4.573797965788257e-05,
      "loss": 0.7274,
      "step": 560500
    },
    {
      "epoch": 5.115336885904081,
      "grad_norm": 4.124269962310791,
      "learning_rate": 4.57372192617466e-05,
      "loss": 0.7429,
      "step": 560600
    },
    {
      "epoch": 5.116249361267246,
      "grad_norm": 4.452011585235596,
      "learning_rate": 4.573645886561063e-05,
      "loss": 0.7616,
      "step": 560700
    },
    {
      "epoch": 5.117161836630411,
      "grad_norm": 4.256622314453125,
      "learning_rate": 4.5735698469474656e-05,
      "loss": 0.6779,
      "step": 560800
    },
    {
      "epoch": 5.118074311993576,
      "grad_norm": 3.9790537357330322,
      "learning_rate": 4.573493807333869e-05,
      "loss": 0.7052,
      "step": 560900
    },
    {
      "epoch": 5.118986787356741,
      "grad_norm": 3.3334758281707764,
      "learning_rate": 4.5734177677202716e-05,
      "loss": 0.6767,
      "step": 561000
    },
    {
      "epoch": 5.1198992627199065,
      "grad_norm": 3.6241071224212646,
      "learning_rate": 4.5733417281066746e-05,
      "loss": 0.7242,
      "step": 561100
    },
    {
      "epoch": 5.120811738083072,
      "grad_norm": 3.8606314659118652,
      "learning_rate": 4.5732656884930776e-05,
      "loss": 0.6956,
      "step": 561200
    },
    {
      "epoch": 5.121724213446237,
      "grad_norm": 5.851873874664307,
      "learning_rate": 4.5731896488794807e-05,
      "loss": 0.7013,
      "step": 561300
    },
    {
      "epoch": 5.122636688809402,
      "grad_norm": 4.451363563537598,
      "learning_rate": 4.573113609265883e-05,
      "loss": 0.6829,
      "step": 561400
    },
    {
      "epoch": 5.123549164172568,
      "grad_norm": 3.7628185749053955,
      "learning_rate": 4.573037569652287e-05,
      "loss": 0.7053,
      "step": 561500
    },
    {
      "epoch": 5.124461639535733,
      "grad_norm": 4.89739990234375,
      "learning_rate": 4.572961530038689e-05,
      "loss": 0.6939,
      "step": 561600
    },
    {
      "epoch": 5.125374114898897,
      "grad_norm": 4.668117046356201,
      "learning_rate": 4.572885490425092e-05,
      "loss": 0.6745,
      "step": 561700
    },
    {
      "epoch": 5.126286590262063,
      "grad_norm": 4.204663276672363,
      "learning_rate": 4.572809450811495e-05,
      "loss": 0.7158,
      "step": 561800
    },
    {
      "epoch": 5.127199065625228,
      "grad_norm": 4.97841739654541,
      "learning_rate": 4.572733411197898e-05,
      "loss": 0.7289,
      "step": 561900
    },
    {
      "epoch": 5.128111540988393,
      "grad_norm": 4.088755130767822,
      "learning_rate": 4.572657371584301e-05,
      "loss": 0.7139,
      "step": 562000
    },
    {
      "epoch": 5.129024016351559,
      "grad_norm": 4.8344645500183105,
      "learning_rate": 4.572581331970704e-05,
      "loss": 0.7081,
      "step": 562100
    },
    {
      "epoch": 5.129936491714724,
      "grad_norm": 3.8718340396881104,
      "learning_rate": 4.5725052923571064e-05,
      "loss": 0.6967,
      "step": 562200
    },
    {
      "epoch": 5.130848967077889,
      "grad_norm": 4.535024642944336,
      "learning_rate": 4.57242925274351e-05,
      "loss": 0.698,
      "step": 562300
    },
    {
      "epoch": 5.1317614424410545,
      "grad_norm": 4.212837219238281,
      "learning_rate": 4.5723532131299124e-05,
      "loss": 0.6706,
      "step": 562400
    },
    {
      "epoch": 5.132673917804219,
      "grad_norm": 2.2827837467193604,
      "learning_rate": 4.572277173516315e-05,
      "loss": 0.779,
      "step": 562500
    },
    {
      "epoch": 5.133586393167384,
      "grad_norm": 4.285126209259033,
      "learning_rate": 4.5722011339027184e-05,
      "loss": 0.6797,
      "step": 562600
    },
    {
      "epoch": 5.1344988685305495,
      "grad_norm": 4.376114845275879,
      "learning_rate": 4.572125094289121e-05,
      "loss": 0.6785,
      "step": 562700
    },
    {
      "epoch": 5.135411343893715,
      "grad_norm": 4.938810348510742,
      "learning_rate": 4.572049054675524e-05,
      "loss": 0.7119,
      "step": 562800
    },
    {
      "epoch": 5.13632381925688,
      "grad_norm": 4.358919143676758,
      "learning_rate": 4.571973015061927e-05,
      "loss": 0.7285,
      "step": 562900
    },
    {
      "epoch": 5.137236294620045,
      "grad_norm": 4.049164295196533,
      "learning_rate": 4.57189697544833e-05,
      "loss": 0.7357,
      "step": 563000
    },
    {
      "epoch": 5.138148769983211,
      "grad_norm": 4.214237689971924,
      "learning_rate": 4.571820935834733e-05,
      "loss": 0.6863,
      "step": 563100
    },
    {
      "epoch": 5.139061245346376,
      "grad_norm": 4.570882320404053,
      "learning_rate": 4.571744896221136e-05,
      "loss": 0.7277,
      "step": 563200
    },
    {
      "epoch": 5.139973720709541,
      "grad_norm": 5.023329257965088,
      "learning_rate": 4.571668856607538e-05,
      "loss": 0.7037,
      "step": 563300
    },
    {
      "epoch": 5.140886196072706,
      "grad_norm": 3.6005544662475586,
      "learning_rate": 4.571592816993942e-05,
      "loss": 0.7453,
      "step": 563400
    },
    {
      "epoch": 5.141798671435871,
      "grad_norm": 4.545827388763428,
      "learning_rate": 4.571516777380344e-05,
      "loss": 0.733,
      "step": 563500
    },
    {
      "epoch": 5.142711146799036,
      "grad_norm": 4.648103713989258,
      "learning_rate": 4.571440737766747e-05,
      "loss": 0.7068,
      "step": 563600
    },
    {
      "epoch": 5.143623622162202,
      "grad_norm": 4.38763427734375,
      "learning_rate": 4.57136469815315e-05,
      "loss": 0.6972,
      "step": 563700
    },
    {
      "epoch": 5.144536097525367,
      "grad_norm": 3.987147092819214,
      "learning_rate": 4.571288658539553e-05,
      "loss": 0.6734,
      "step": 563800
    },
    {
      "epoch": 5.145448572888532,
      "grad_norm": 3.7114968299865723,
      "learning_rate": 4.5712126189259554e-05,
      "loss": 0.7144,
      "step": 563900
    },
    {
      "epoch": 5.1463610482516975,
      "grad_norm": 5.115237236022949,
      "learning_rate": 4.571136579312359e-05,
      "loss": 0.7221,
      "step": 564000
    },
    {
      "epoch": 5.147273523614863,
      "grad_norm": 4.372722148895264,
      "learning_rate": 4.5710605396987615e-05,
      "loss": 0.7136,
      "step": 564100
    },
    {
      "epoch": 5.148185998978027,
      "grad_norm": 4.521966934204102,
      "learning_rate": 4.5709845000851645e-05,
      "loss": 0.7359,
      "step": 564200
    },
    {
      "epoch": 5.1490984743411925,
      "grad_norm": 3.7950782775878906,
      "learning_rate": 4.5709084604715675e-05,
      "loss": 0.6804,
      "step": 564300
    },
    {
      "epoch": 5.150010949704358,
      "grad_norm": 2.934058666229248,
      "learning_rate": 4.5708324208579705e-05,
      "loss": 0.719,
      "step": 564400
    },
    {
      "epoch": 5.150923425067523,
      "grad_norm": 3.472367286682129,
      "learning_rate": 4.5707563812443735e-05,
      "loss": 0.7426,
      "step": 564500
    },
    {
      "epoch": 5.151835900430688,
      "grad_norm": 4.050068378448486,
      "learning_rate": 4.5706803416307765e-05,
      "loss": 0.709,
      "step": 564600
    },
    {
      "epoch": 5.152748375793854,
      "grad_norm": 4.363460540771484,
      "learning_rate": 4.570604302017179e-05,
      "loss": 0.7483,
      "step": 564700
    },
    {
      "epoch": 5.153660851157019,
      "grad_norm": 3.7823567390441895,
      "learning_rate": 4.5705282624035825e-05,
      "loss": 0.7165,
      "step": 564800
    },
    {
      "epoch": 5.154573326520184,
      "grad_norm": 4.122178077697754,
      "learning_rate": 4.570452222789985e-05,
      "loss": 0.6839,
      "step": 564900
    },
    {
      "epoch": 5.15548580188335,
      "grad_norm": 3.6750264167785645,
      "learning_rate": 4.570376183176388e-05,
      "loss": 0.6926,
      "step": 565000
    },
    {
      "epoch": 5.156398277246514,
      "grad_norm": 3.9631857872009277,
      "learning_rate": 4.570300143562791e-05,
      "loss": 0.7316,
      "step": 565100
    },
    {
      "epoch": 5.157310752609679,
      "grad_norm": 3.9020681381225586,
      "learning_rate": 4.570224103949193e-05,
      "loss": 0.7312,
      "step": 565200
    },
    {
      "epoch": 5.158223227972845,
      "grad_norm": 3.9974446296691895,
      "learning_rate": 4.570148064335596e-05,
      "loss": 0.7361,
      "step": 565300
    },
    {
      "epoch": 5.15913570333601,
      "grad_norm": 3.9857141971588135,
      "learning_rate": 4.570072024721999e-05,
      "loss": 0.6961,
      "step": 565400
    },
    {
      "epoch": 5.160048178699175,
      "grad_norm": 2.982541561126709,
      "learning_rate": 4.569995985108402e-05,
      "loss": 0.7091,
      "step": 565500
    },
    {
      "epoch": 5.1609606540623405,
      "grad_norm": 3.2911579608917236,
      "learning_rate": 4.569919945494805e-05,
      "loss": 0.7049,
      "step": 565600
    },
    {
      "epoch": 5.161873129425506,
      "grad_norm": 3.935413122177124,
      "learning_rate": 4.569843905881208e-05,
      "loss": 0.7335,
      "step": 565700
    },
    {
      "epoch": 5.162785604788671,
      "grad_norm": 3.6614830493927,
      "learning_rate": 4.5697678662676105e-05,
      "loss": 0.7112,
      "step": 565800
    },
    {
      "epoch": 5.1636980801518355,
      "grad_norm": 3.7179832458496094,
      "learning_rate": 4.569691826654014e-05,
      "loss": 0.6912,
      "step": 565900
    },
    {
      "epoch": 5.164610555515001,
      "grad_norm": 3.7127187252044678,
      "learning_rate": 4.5696157870404166e-05,
      "loss": 0.7448,
      "step": 566000
    },
    {
      "epoch": 5.165523030878166,
      "grad_norm": 4.760695934295654,
      "learning_rate": 4.5695397474268196e-05,
      "loss": 0.6709,
      "step": 566100
    },
    {
      "epoch": 5.166435506241331,
      "grad_norm": 4.034499645233154,
      "learning_rate": 4.5694637078132226e-05,
      "loss": 0.6907,
      "step": 566200
    },
    {
      "epoch": 5.167347981604497,
      "grad_norm": 4.883617401123047,
      "learning_rate": 4.5693876681996256e-05,
      "loss": 0.7195,
      "step": 566300
    },
    {
      "epoch": 5.168260456967662,
      "grad_norm": 4.126011371612549,
      "learning_rate": 4.569311628586028e-05,
      "loss": 0.7079,
      "step": 566400
    },
    {
      "epoch": 5.169172932330827,
      "grad_norm": 3.8430426120758057,
      "learning_rate": 4.5692355889724316e-05,
      "loss": 0.6891,
      "step": 566500
    },
    {
      "epoch": 5.170085407693993,
      "grad_norm": 3.95160174369812,
      "learning_rate": 4.569159549358834e-05,
      "loss": 0.7248,
      "step": 566600
    },
    {
      "epoch": 5.170997883057158,
      "grad_norm": 4.427484512329102,
      "learning_rate": 4.569083509745237e-05,
      "loss": 0.6911,
      "step": 566700
    },
    {
      "epoch": 5.171910358420322,
      "grad_norm": 4.455692768096924,
      "learning_rate": 4.56900747013164e-05,
      "loss": 0.7036,
      "step": 566800
    },
    {
      "epoch": 5.172822833783488,
      "grad_norm": 3.8897533416748047,
      "learning_rate": 4.568931430518043e-05,
      "loss": 0.7372,
      "step": 566900
    },
    {
      "epoch": 5.173735309146653,
      "grad_norm": 3.9429986476898193,
      "learning_rate": 4.568855390904446e-05,
      "loss": 0.719,
      "step": 567000
    },
    {
      "epoch": 5.174647784509818,
      "grad_norm": 4.376254558563232,
      "learning_rate": 4.568779351290849e-05,
      "loss": 0.7415,
      "step": 567100
    },
    {
      "epoch": 5.1755602598729835,
      "grad_norm": 4.438055515289307,
      "learning_rate": 4.568703311677251e-05,
      "loss": 0.6927,
      "step": 567200
    },
    {
      "epoch": 5.176472735236149,
      "grad_norm": 4.558547496795654,
      "learning_rate": 4.568627272063655e-05,
      "loss": 0.6929,
      "step": 567300
    },
    {
      "epoch": 5.177385210599314,
      "grad_norm": 4.110954761505127,
      "learning_rate": 4.568551232450057e-05,
      "loss": 0.6682,
      "step": 567400
    },
    {
      "epoch": 5.178297685962479,
      "grad_norm": 4.3308186531066895,
      "learning_rate": 4.56847519283646e-05,
      "loss": 0.7084,
      "step": 567500
    },
    {
      "epoch": 5.179210161325644,
      "grad_norm": 3.5134990215301514,
      "learning_rate": 4.568399153222863e-05,
      "loss": 0.7249,
      "step": 567600
    },
    {
      "epoch": 5.180122636688809,
      "grad_norm": 3.723200798034668,
      "learning_rate": 4.568323113609266e-05,
      "loss": 0.7062,
      "step": 567700
    },
    {
      "epoch": 5.181035112051974,
      "grad_norm": 4.068431377410889,
      "learning_rate": 4.568247073995669e-05,
      "loss": 0.701,
      "step": 567800
    },
    {
      "epoch": 5.18194758741514,
      "grad_norm": 3.8605289459228516,
      "learning_rate": 4.568171034382072e-05,
      "loss": 0.7127,
      "step": 567900
    },
    {
      "epoch": 5.182860062778305,
      "grad_norm": 4.630443572998047,
      "learning_rate": 4.5680949947684747e-05,
      "loss": 0.7233,
      "step": 568000
    },
    {
      "epoch": 5.18377253814147,
      "grad_norm": 4.016367435455322,
      "learning_rate": 4.5680189551548777e-05,
      "loss": 0.6867,
      "step": 568100
    },
    {
      "epoch": 5.184685013504636,
      "grad_norm": 3.5061488151550293,
      "learning_rate": 4.567942915541281e-05,
      "loss": 0.7334,
      "step": 568200
    },
    {
      "epoch": 5.185597488867801,
      "grad_norm": 3.946305274963379,
      "learning_rate": 4.567866875927683e-05,
      "loss": 0.7607,
      "step": 568300
    },
    {
      "epoch": 5.186509964230966,
      "grad_norm": 4.120181083679199,
      "learning_rate": 4.567790836314087e-05,
      "loss": 0.7908,
      "step": 568400
    },
    {
      "epoch": 5.187422439594131,
      "grad_norm": 4.005480766296387,
      "learning_rate": 4.567714796700489e-05,
      "loss": 0.7506,
      "step": 568500
    },
    {
      "epoch": 5.188334914957296,
      "grad_norm": 4.254665851593018,
      "learning_rate": 4.567638757086892e-05,
      "loss": 0.7232,
      "step": 568600
    },
    {
      "epoch": 5.189247390320461,
      "grad_norm": 3.022580623626709,
      "learning_rate": 4.567562717473295e-05,
      "loss": 0.708,
      "step": 568700
    },
    {
      "epoch": 5.1901598656836265,
      "grad_norm": 2.9768483638763428,
      "learning_rate": 4.567486677859698e-05,
      "loss": 0.6764,
      "step": 568800
    },
    {
      "epoch": 5.191072341046792,
      "grad_norm": 3.4175498485565186,
      "learning_rate": 4.567410638246101e-05,
      "loss": 0.7412,
      "step": 568900
    },
    {
      "epoch": 5.191984816409957,
      "grad_norm": 4.069815158843994,
      "learning_rate": 4.567334598632504e-05,
      "loss": 0.6897,
      "step": 569000
    },
    {
      "epoch": 5.192897291773122,
      "grad_norm": 5.60255241394043,
      "learning_rate": 4.5672585590189064e-05,
      "loss": 0.6942,
      "step": 569100
    },
    {
      "epoch": 5.193809767136288,
      "grad_norm": 3.8452987670898438,
      "learning_rate": 4.56718251940531e-05,
      "loss": 0.7017,
      "step": 569200
    },
    {
      "epoch": 5.194722242499452,
      "grad_norm": 5.292728900909424,
      "learning_rate": 4.5671064797917124e-05,
      "loss": 0.727,
      "step": 569300
    },
    {
      "epoch": 5.195634717862617,
      "grad_norm": 3.9505996704101562,
      "learning_rate": 4.5670304401781154e-05,
      "loss": 0.7108,
      "step": 569400
    },
    {
      "epoch": 5.196547193225783,
      "grad_norm": 3.427896499633789,
      "learning_rate": 4.5669544005645184e-05,
      "loss": 0.7284,
      "step": 569500
    },
    {
      "epoch": 5.197459668588948,
      "grad_norm": 4.660348415374756,
      "learning_rate": 4.5668783609509214e-05,
      "loss": 0.7073,
      "step": 569600
    },
    {
      "epoch": 5.198372143952113,
      "grad_norm": 3.7900099754333496,
      "learning_rate": 4.566802321337324e-05,
      "loss": 0.7335,
      "step": 569700
    },
    {
      "epoch": 5.199284619315279,
      "grad_norm": 4.3330559730529785,
      "learning_rate": 4.5667262817237274e-05,
      "loss": 0.6907,
      "step": 569800
    },
    {
      "epoch": 5.200197094678444,
      "grad_norm": 3.958841562271118,
      "learning_rate": 4.56665024211013e-05,
      "loss": 0.7136,
      "step": 569900
    },
    {
      "epoch": 5.201109570041609,
      "grad_norm": 4.140583515167236,
      "learning_rate": 4.566574202496533e-05,
      "loss": 0.6896,
      "step": 570000
    },
    {
      "epoch": 5.202022045404774,
      "grad_norm": 5.443333625793457,
      "learning_rate": 4.566498162882936e-05,
      "loss": 0.7184,
      "step": 570100
    },
    {
      "epoch": 5.202934520767939,
      "grad_norm": 3.708420991897583,
      "learning_rate": 4.566422123269339e-05,
      "loss": 0.7232,
      "step": 570200
    },
    {
      "epoch": 5.203846996131104,
      "grad_norm": 4.121546268463135,
      "learning_rate": 4.566346083655742e-05,
      "loss": 0.693,
      "step": 570300
    },
    {
      "epoch": 5.2047594714942695,
      "grad_norm": 3.2718451023101807,
      "learning_rate": 4.566270044042145e-05,
      "loss": 0.7248,
      "step": 570400
    },
    {
      "epoch": 5.205671946857435,
      "grad_norm": 4.509308815002441,
      "learning_rate": 4.566194004428547e-05,
      "loss": 0.7073,
      "step": 570500
    },
    {
      "epoch": 5.2065844222206,
      "grad_norm": 4.021223068237305,
      "learning_rate": 4.566117964814951e-05,
      "loss": 0.7128,
      "step": 570600
    },
    {
      "epoch": 5.207496897583765,
      "grad_norm": 4.771207809448242,
      "learning_rate": 4.566041925201353e-05,
      "loss": 0.7112,
      "step": 570700
    },
    {
      "epoch": 5.208409372946931,
      "grad_norm": 3.6292660236358643,
      "learning_rate": 4.565965885587756e-05,
      "loss": 0.7253,
      "step": 570800
    },
    {
      "epoch": 5.209321848310096,
      "grad_norm": 3.805058240890503,
      "learning_rate": 4.565889845974159e-05,
      "loss": 0.7114,
      "step": 570900
    },
    {
      "epoch": 5.2102343236732604,
      "grad_norm": 3.8039324283599854,
      "learning_rate": 4.5658138063605615e-05,
      "loss": 0.7052,
      "step": 571000
    },
    {
      "epoch": 5.211146799036426,
      "grad_norm": 4.417346000671387,
      "learning_rate": 4.5657377667469645e-05,
      "loss": 0.7142,
      "step": 571100
    },
    {
      "epoch": 5.212059274399591,
      "grad_norm": 3.7542684078216553,
      "learning_rate": 4.5656617271333675e-05,
      "loss": 0.6812,
      "step": 571200
    },
    {
      "epoch": 5.212971749762756,
      "grad_norm": 4.714979648590088,
      "learning_rate": 4.5655856875197705e-05,
      "loss": 0.6702,
      "step": 571300
    },
    {
      "epoch": 5.213884225125922,
      "grad_norm": 5.2015180587768555,
      "learning_rate": 4.5655096479061735e-05,
      "loss": 0.7337,
      "step": 571400
    },
    {
      "epoch": 5.214796700489087,
      "grad_norm": 4.912257671356201,
      "learning_rate": 4.5654336082925765e-05,
      "loss": 0.7082,
      "step": 571500
    },
    {
      "epoch": 5.215709175852252,
      "grad_norm": 3.381485939025879,
      "learning_rate": 4.565357568678979e-05,
      "loss": 0.7265,
      "step": 571600
    },
    {
      "epoch": 5.2166216512154175,
      "grad_norm": 3.7010085582733154,
      "learning_rate": 4.5652815290653825e-05,
      "loss": 0.6965,
      "step": 571700
    },
    {
      "epoch": 5.217534126578582,
      "grad_norm": 5.215860843658447,
      "learning_rate": 4.565205489451785e-05,
      "loss": 0.722,
      "step": 571800
    },
    {
      "epoch": 5.218446601941747,
      "grad_norm": 3.266711950302124,
      "learning_rate": 4.565129449838188e-05,
      "loss": 0.6891,
      "step": 571900
    },
    {
      "epoch": 5.2193590773049126,
      "grad_norm": 3.822885513305664,
      "learning_rate": 4.565053410224591e-05,
      "loss": 0.7256,
      "step": 572000
    },
    {
      "epoch": 5.220271552668078,
      "grad_norm": 3.9333155155181885,
      "learning_rate": 4.564977370610994e-05,
      "loss": 0.692,
      "step": 572100
    },
    {
      "epoch": 5.221184028031243,
      "grad_norm": 3.430025339126587,
      "learning_rate": 4.564901330997396e-05,
      "loss": 0.7067,
      "step": 572200
    },
    {
      "epoch": 5.2220965033944085,
      "grad_norm": 4.166595458984375,
      "learning_rate": 4.5648252913838e-05,
      "loss": 0.7504,
      "step": 572300
    },
    {
      "epoch": 5.223008978757574,
      "grad_norm": 4.551184177398682,
      "learning_rate": 4.564749251770202e-05,
      "loss": 0.7468,
      "step": 572400
    },
    {
      "epoch": 5.223921454120739,
      "grad_norm": 4.354397773742676,
      "learning_rate": 4.564673212156605e-05,
      "loss": 0.7505,
      "step": 572500
    },
    {
      "epoch": 5.224833929483904,
      "grad_norm": 3.8413448333740234,
      "learning_rate": 4.564597172543008e-05,
      "loss": 0.7211,
      "step": 572600
    },
    {
      "epoch": 5.225746404847069,
      "grad_norm": 4.189995765686035,
      "learning_rate": 4.564521132929411e-05,
      "loss": 0.7284,
      "step": 572700
    },
    {
      "epoch": 5.226658880210234,
      "grad_norm": 4.569332122802734,
      "learning_rate": 4.564445093315814e-05,
      "loss": 0.704,
      "step": 572800
    },
    {
      "epoch": 5.227571355573399,
      "grad_norm": 4.0912394523620605,
      "learning_rate": 4.564369053702217e-05,
      "loss": 0.7403,
      "step": 572900
    },
    {
      "epoch": 5.228483830936565,
      "grad_norm": 4.271566390991211,
      "learning_rate": 4.5642930140886196e-05,
      "loss": 0.6972,
      "step": 573000
    },
    {
      "epoch": 5.22939630629973,
      "grad_norm": 4.43646764755249,
      "learning_rate": 4.564216974475023e-05,
      "loss": 0.7141,
      "step": 573100
    },
    {
      "epoch": 5.230308781662895,
      "grad_norm": 3.7579498291015625,
      "learning_rate": 4.5641409348614256e-05,
      "loss": 0.7254,
      "step": 573200
    },
    {
      "epoch": 5.231221257026061,
      "grad_norm": 4.334387302398682,
      "learning_rate": 4.5640648952478286e-05,
      "loss": 0.6998,
      "step": 573300
    },
    {
      "epoch": 5.232133732389226,
      "grad_norm": 4.759195804595947,
      "learning_rate": 4.5639888556342316e-05,
      "loss": 0.6948,
      "step": 573400
    },
    {
      "epoch": 5.23304620775239,
      "grad_norm": 3.125824213027954,
      "learning_rate": 4.5639128160206346e-05,
      "loss": 0.7075,
      "step": 573500
    },
    {
      "epoch": 5.233958683115556,
      "grad_norm": 3.7231526374816895,
      "learning_rate": 4.563836776407037e-05,
      "loss": 0.6908,
      "step": 573600
    },
    {
      "epoch": 5.234871158478721,
      "grad_norm": 3.738466501235962,
      "learning_rate": 4.5637607367934406e-05,
      "loss": 0.7387,
      "step": 573700
    },
    {
      "epoch": 5.235783633841886,
      "grad_norm": 4.195945739746094,
      "learning_rate": 4.563684697179843e-05,
      "loss": 0.6769,
      "step": 573800
    },
    {
      "epoch": 5.2366961092050515,
      "grad_norm": 4.905734539031982,
      "learning_rate": 4.563608657566246e-05,
      "loss": 0.7057,
      "step": 573900
    },
    {
      "epoch": 5.237608584568217,
      "grad_norm": 4.278876781463623,
      "learning_rate": 4.563532617952649e-05,
      "loss": 0.7117,
      "step": 574000
    },
    {
      "epoch": 5.238521059931382,
      "grad_norm": 4.591719150543213,
      "learning_rate": 4.563456578339051e-05,
      "loss": 0.7276,
      "step": 574100
    },
    {
      "epoch": 5.239433535294547,
      "grad_norm": 3.4474308490753174,
      "learning_rate": 4.563380538725455e-05,
      "loss": 0.7272,
      "step": 574200
    },
    {
      "epoch": 5.240346010657713,
      "grad_norm": 3.3316471576690674,
      "learning_rate": 4.563304499111857e-05,
      "loss": 0.7018,
      "step": 574300
    },
    {
      "epoch": 5.241258486020877,
      "grad_norm": 3.553666591644287,
      "learning_rate": 4.56322845949826e-05,
      "loss": 0.665,
      "step": 574400
    },
    {
      "epoch": 5.242170961384042,
      "grad_norm": 3.8889806270599365,
      "learning_rate": 4.563152419884663e-05,
      "loss": 0.6908,
      "step": 574500
    },
    {
      "epoch": 5.243083436747208,
      "grad_norm": 3.822035789489746,
      "learning_rate": 4.563076380271066e-05,
      "loss": 0.72,
      "step": 574600
    },
    {
      "epoch": 5.243995912110373,
      "grad_norm": 3.6789259910583496,
      "learning_rate": 4.5630003406574686e-05,
      "loss": 0.6931,
      "step": 574700
    },
    {
      "epoch": 5.244908387473538,
      "grad_norm": 4.358872890472412,
      "learning_rate": 4.562924301043872e-05,
      "loss": 0.6994,
      "step": 574800
    },
    {
      "epoch": 5.245820862836704,
      "grad_norm": 4.239511966705322,
      "learning_rate": 4.562848261430275e-05,
      "loss": 0.6852,
      "step": 574900
    },
    {
      "epoch": 5.246733338199869,
      "grad_norm": 3.894727945327759,
      "learning_rate": 4.562772221816678e-05,
      "loss": 0.7463,
      "step": 575000
    },
    {
      "epoch": 5.247645813563034,
      "grad_norm": 3.6177351474761963,
      "learning_rate": 4.562696182203081e-05,
      "loss": 0.6956,
      "step": 575100
    },
    {
      "epoch": 5.248558288926199,
      "grad_norm": 4.213908672332764,
      "learning_rate": 4.562620142589484e-05,
      "loss": 0.7223,
      "step": 575200
    },
    {
      "epoch": 5.249470764289364,
      "grad_norm": 3.806938648223877,
      "learning_rate": 4.562544102975887e-05,
      "loss": 0.7473,
      "step": 575300
    },
    {
      "epoch": 5.250383239652529,
      "grad_norm": 3.7927169799804688,
      "learning_rate": 4.56246806336229e-05,
      "loss": 0.7039,
      "step": 575400
    },
    {
      "epoch": 5.2512957150156945,
      "grad_norm": 3.936006784439087,
      "learning_rate": 4.562392023748692e-05,
      "loss": 0.7003,
      "step": 575500
    },
    {
      "epoch": 5.25220819037886,
      "grad_norm": 4.127610683441162,
      "learning_rate": 4.562315984135096e-05,
      "loss": 0.7043,
      "step": 575600
    },
    {
      "epoch": 5.253120665742025,
      "grad_norm": 5.633009433746338,
      "learning_rate": 4.562239944521498e-05,
      "loss": 0.7191,
      "step": 575700
    },
    {
      "epoch": 5.25403314110519,
      "grad_norm": 4.357577323913574,
      "learning_rate": 4.562163904907901e-05,
      "loss": 0.6689,
      "step": 575800
    },
    {
      "epoch": 5.254945616468356,
      "grad_norm": 3.9161603450775146,
      "learning_rate": 4.562087865294304e-05,
      "loss": 0.7323,
      "step": 575900
    },
    {
      "epoch": 5.25585809183152,
      "grad_norm": 3.9377589225769043,
      "learning_rate": 4.562011825680707e-05,
      "loss": 0.6851,
      "step": 576000
    },
    {
      "epoch": 5.256770567194685,
      "grad_norm": 5.155007839202881,
      "learning_rate": 4.5619357860671094e-05,
      "loss": 0.6884,
      "step": 576100
    },
    {
      "epoch": 5.257683042557851,
      "grad_norm": 3.8581511974334717,
      "learning_rate": 4.561859746453513e-05,
      "loss": 0.7102,
      "step": 576200
    },
    {
      "epoch": 5.258595517921016,
      "grad_norm": 4.463922023773193,
      "learning_rate": 4.5617837068399154e-05,
      "loss": 0.7106,
      "step": 576300
    },
    {
      "epoch": 5.259507993284181,
      "grad_norm": 3.728774309158325,
      "learning_rate": 4.5617076672263184e-05,
      "loss": 0.7025,
      "step": 576400
    },
    {
      "epoch": 5.260420468647347,
      "grad_norm": 4.2920122146606445,
      "learning_rate": 4.5616316276127214e-05,
      "loss": 0.7242,
      "step": 576500
    },
    {
      "epoch": 5.261332944010512,
      "grad_norm": 4.26301908493042,
      "learning_rate": 4.561555587999124e-05,
      "loss": 0.6966,
      "step": 576600
    },
    {
      "epoch": 5.262245419373677,
      "grad_norm": 4.321219444274902,
      "learning_rate": 4.5614795483855274e-05,
      "loss": 0.7115,
      "step": 576700
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 3.8565104007720947,
      "learning_rate": 4.56140350877193e-05,
      "loss": 0.6577,
      "step": 576800
    },
    {
      "epoch": 5.264070370100007,
      "grad_norm": 4.406994819641113,
      "learning_rate": 4.561327469158333e-05,
      "loss": 0.7563,
      "step": 576900
    },
    {
      "epoch": 5.264982845463172,
      "grad_norm": 4.3472161293029785,
      "learning_rate": 4.561251429544736e-05,
      "loss": 0.7495,
      "step": 577000
    },
    {
      "epoch": 5.2658953208263375,
      "grad_norm": 3.452620506286621,
      "learning_rate": 4.561175389931139e-05,
      "loss": 0.7052,
      "step": 577100
    },
    {
      "epoch": 5.266807796189503,
      "grad_norm": 4.2861785888671875,
      "learning_rate": 4.561099350317541e-05,
      "loss": 0.7087,
      "step": 577200
    },
    {
      "epoch": 5.267720271552668,
      "grad_norm": 4.0711140632629395,
      "learning_rate": 4.561023310703945e-05,
      "loss": 0.7719,
      "step": 577300
    },
    {
      "epoch": 5.268632746915833,
      "grad_norm": 3.4592630863189697,
      "learning_rate": 4.560947271090347e-05,
      "loss": 0.684,
      "step": 577400
    },
    {
      "epoch": 5.269545222278999,
      "grad_norm": 2.8378164768218994,
      "learning_rate": 4.56087123147675e-05,
      "loss": 0.678,
      "step": 577500
    },
    {
      "epoch": 5.270457697642164,
      "grad_norm": 4.274587631225586,
      "learning_rate": 4.560795191863153e-05,
      "loss": 0.7369,
      "step": 577600
    },
    {
      "epoch": 5.271370173005328,
      "grad_norm": 3.9666523933410645,
      "learning_rate": 4.560719152249556e-05,
      "loss": 0.7282,
      "step": 577700
    },
    {
      "epoch": 5.272282648368494,
      "grad_norm": 4.062260150909424,
      "learning_rate": 4.560643112635959e-05,
      "loss": 0.7072,
      "step": 577800
    },
    {
      "epoch": 5.273195123731659,
      "grad_norm": 4.4071478843688965,
      "learning_rate": 4.560567073022362e-05,
      "loss": 0.6946,
      "step": 577900
    },
    {
      "epoch": 5.274107599094824,
      "grad_norm": 3.345099925994873,
      "learning_rate": 4.5604910334087645e-05,
      "loss": 0.7294,
      "step": 578000
    },
    {
      "epoch": 5.27502007445799,
      "grad_norm": 3.9302992820739746,
      "learning_rate": 4.560414993795168e-05,
      "loss": 0.6884,
      "step": 578100
    },
    {
      "epoch": 5.275932549821155,
      "grad_norm": 3.3639612197875977,
      "learning_rate": 4.5603389541815705e-05,
      "loss": 0.7112,
      "step": 578200
    },
    {
      "epoch": 5.27684502518432,
      "grad_norm": 3.690534830093384,
      "learning_rate": 4.5602629145679735e-05,
      "loss": 0.7597,
      "step": 578300
    },
    {
      "epoch": 5.2777575005474855,
      "grad_norm": 3.497965097427368,
      "learning_rate": 4.5601868749543765e-05,
      "loss": 0.6736,
      "step": 578400
    },
    {
      "epoch": 5.278669975910651,
      "grad_norm": 3.2903809547424316,
      "learning_rate": 4.5601108353407795e-05,
      "loss": 0.7137,
      "step": 578500
    },
    {
      "epoch": 5.279582451273815,
      "grad_norm": 3.767732620239258,
      "learning_rate": 4.560034795727182e-05,
      "loss": 0.6918,
      "step": 578600
    },
    {
      "epoch": 5.2804949266369805,
      "grad_norm": 4.593026161193848,
      "learning_rate": 4.5599587561135855e-05,
      "loss": 0.6922,
      "step": 578700
    },
    {
      "epoch": 5.281407402000146,
      "grad_norm": 3.3335893154144287,
      "learning_rate": 4.559882716499988e-05,
      "loss": 0.7382,
      "step": 578800
    },
    {
      "epoch": 5.282319877363311,
      "grad_norm": 4.459598541259766,
      "learning_rate": 4.559806676886391e-05,
      "loss": 0.7209,
      "step": 578900
    },
    {
      "epoch": 5.283232352726476,
      "grad_norm": 4.316257476806641,
      "learning_rate": 4.559730637272794e-05,
      "loss": 0.715,
      "step": 579000
    },
    {
      "epoch": 5.284144828089642,
      "grad_norm": 3.246535301208496,
      "learning_rate": 4.559654597659197e-05,
      "loss": 0.6907,
      "step": 579100
    },
    {
      "epoch": 5.285057303452807,
      "grad_norm": 3.9801275730133057,
      "learning_rate": 4.5595785580456e-05,
      "loss": 0.7277,
      "step": 579200
    },
    {
      "epoch": 5.285969778815972,
      "grad_norm": 3.795923948287964,
      "learning_rate": 4.559502518432003e-05,
      "loss": 0.6785,
      "step": 579300
    },
    {
      "epoch": 5.286882254179137,
      "grad_norm": 4.7646074295043945,
      "learning_rate": 4.559426478818405e-05,
      "loss": 0.6965,
      "step": 579400
    },
    {
      "epoch": 5.287794729542302,
      "grad_norm": 4.660269260406494,
      "learning_rate": 4.559350439204808e-05,
      "loss": 0.7037,
      "step": 579500
    },
    {
      "epoch": 5.288707204905467,
      "grad_norm": 3.964517831802368,
      "learning_rate": 4.559274399591211e-05,
      "loss": 0.7264,
      "step": 579600
    },
    {
      "epoch": 5.289619680268633,
      "grad_norm": 3.572152853012085,
      "learning_rate": 4.559198359977614e-05,
      "loss": 0.7388,
      "step": 579700
    },
    {
      "epoch": 5.290532155631798,
      "grad_norm": 4.446842670440674,
      "learning_rate": 4.559122320364017e-05,
      "loss": 0.6846,
      "step": 579800
    },
    {
      "epoch": 5.291444630994963,
      "grad_norm": 4.0663981437683105,
      "learning_rate": 4.5590462807504196e-05,
      "loss": 0.7118,
      "step": 579900
    },
    {
      "epoch": 5.2923571063581285,
      "grad_norm": 2.667846202850342,
      "learning_rate": 4.5589702411368226e-05,
      "loss": 0.7049,
      "step": 580000
    },
    {
      "epoch": 5.293269581721294,
      "grad_norm": 5.057051181793213,
      "learning_rate": 4.5588942015232256e-05,
      "loss": 0.6963,
      "step": 580100
    },
    {
      "epoch": 5.294182057084459,
      "grad_norm": 4.225252151489258,
      "learning_rate": 4.5588181619096286e-05,
      "loss": 0.6933,
      "step": 580200
    },
    {
      "epoch": 5.2950945324476235,
      "grad_norm": 4.580004692077637,
      "learning_rate": 4.5587421222960316e-05,
      "loss": 0.7094,
      "step": 580300
    },
    {
      "epoch": 5.296007007810789,
      "grad_norm": 4.084142684936523,
      "learning_rate": 4.5586660826824346e-05,
      "loss": 0.7257,
      "step": 580400
    },
    {
      "epoch": 5.296919483173954,
      "grad_norm": 3.350614070892334,
      "learning_rate": 4.558590043068837e-05,
      "loss": 0.7266,
      "step": 580500
    },
    {
      "epoch": 5.297831958537119,
      "grad_norm": 4.212470531463623,
      "learning_rate": 4.5585140034552406e-05,
      "loss": 0.7331,
      "step": 580600
    },
    {
      "epoch": 5.298744433900285,
      "grad_norm": 3.968984365463257,
      "learning_rate": 4.558437963841643e-05,
      "loss": 0.7122,
      "step": 580700
    },
    {
      "epoch": 5.29965690926345,
      "grad_norm": 4.2783522605896,
      "learning_rate": 4.558361924228046e-05,
      "loss": 0.719,
      "step": 580800
    },
    {
      "epoch": 5.300569384626615,
      "grad_norm": 5.180344104766846,
      "learning_rate": 4.558285884614449e-05,
      "loss": 0.7464,
      "step": 580900
    },
    {
      "epoch": 5.301481859989781,
      "grad_norm": 5.074857234954834,
      "learning_rate": 4.558209845000852e-05,
      "loss": 0.6816,
      "step": 581000
    },
    {
      "epoch": 5.302394335352945,
      "grad_norm": 4.788300037384033,
      "learning_rate": 4.558133805387255e-05,
      "loss": 0.7453,
      "step": 581100
    },
    {
      "epoch": 5.30330681071611,
      "grad_norm": 4.200211048126221,
      "learning_rate": 4.558057765773658e-05,
      "loss": 0.7372,
      "step": 581200
    },
    {
      "epoch": 5.304219286079276,
      "grad_norm": 3.1018874645233154,
      "learning_rate": 4.55798172616006e-05,
      "loss": 0.7275,
      "step": 581300
    },
    {
      "epoch": 5.305131761442441,
      "grad_norm": 3.8039655685424805,
      "learning_rate": 4.557905686546463e-05,
      "loss": 0.6775,
      "step": 581400
    },
    {
      "epoch": 5.306044236805606,
      "grad_norm": 3.8118669986724854,
      "learning_rate": 4.557829646932866e-05,
      "loss": 0.7158,
      "step": 581500
    },
    {
      "epoch": 5.3069567121687715,
      "grad_norm": 4.644460201263428,
      "learning_rate": 4.557753607319269e-05,
      "loss": 0.7095,
      "step": 581600
    },
    {
      "epoch": 5.307869187531937,
      "grad_norm": 4.620859622955322,
      "learning_rate": 4.5576775677056723e-05,
      "loss": 0.654,
      "step": 581700
    },
    {
      "epoch": 5.308781662895102,
      "grad_norm": 4.716005325317383,
      "learning_rate": 4.5576015280920754e-05,
      "loss": 0.7713,
      "step": 581800
    },
    {
      "epoch": 5.309694138258267,
      "grad_norm": 4.01810884475708,
      "learning_rate": 4.557525488478478e-05,
      "loss": 0.7409,
      "step": 581900
    },
    {
      "epoch": 5.310606613621432,
      "grad_norm": 4.712665557861328,
      "learning_rate": 4.5574494488648814e-05,
      "loss": 0.6984,
      "step": 582000
    },
    {
      "epoch": 5.311519088984597,
      "grad_norm": 4.393399238586426,
      "learning_rate": 4.557373409251284e-05,
      "loss": 0.7373,
      "step": 582100
    },
    {
      "epoch": 5.312431564347762,
      "grad_norm": 6.508879661560059,
      "learning_rate": 4.557297369637687e-05,
      "loss": 0.7102,
      "step": 582200
    },
    {
      "epoch": 5.313344039710928,
      "grad_norm": 4.069300651550293,
      "learning_rate": 4.55722133002409e-05,
      "loss": 0.7178,
      "step": 582300
    },
    {
      "epoch": 5.314256515074093,
      "grad_norm": 3.4947731494903564,
      "learning_rate": 4.557145290410492e-05,
      "loss": 0.7002,
      "step": 582400
    },
    {
      "epoch": 5.315168990437258,
      "grad_norm": 3.894758939743042,
      "learning_rate": 4.557069250796896e-05,
      "loss": 0.731,
      "step": 582500
    },
    {
      "epoch": 5.316081465800424,
      "grad_norm": 4.520379066467285,
      "learning_rate": 4.556993211183298e-05,
      "loss": 0.7111,
      "step": 582600
    },
    {
      "epoch": 5.316993941163589,
      "grad_norm": 3.6073875427246094,
      "learning_rate": 4.556917171569701e-05,
      "loss": 0.6958,
      "step": 582700
    },
    {
      "epoch": 5.317906416526753,
      "grad_norm": 3.7186310291290283,
      "learning_rate": 4.556841131956104e-05,
      "loss": 0.7019,
      "step": 582800
    },
    {
      "epoch": 5.318818891889919,
      "grad_norm": 3.5217342376708984,
      "learning_rate": 4.556765092342507e-05,
      "loss": 0.6663,
      "step": 582900
    },
    {
      "epoch": 5.319731367253084,
      "grad_norm": 3.9202935695648193,
      "learning_rate": 4.5566890527289094e-05,
      "loss": 0.7381,
      "step": 583000
    },
    {
      "epoch": 5.320643842616249,
      "grad_norm": 4.120860576629639,
      "learning_rate": 4.556613013115313e-05,
      "loss": 0.7342,
      "step": 583100
    },
    {
      "epoch": 5.3215563179794145,
      "grad_norm": 3.316072702407837,
      "learning_rate": 4.5565369735017154e-05,
      "loss": 0.7285,
      "step": 583200
    },
    {
      "epoch": 5.32246879334258,
      "grad_norm": 3.5712475776672363,
      "learning_rate": 4.5564609338881184e-05,
      "loss": 0.727,
      "step": 583300
    },
    {
      "epoch": 5.323381268705745,
      "grad_norm": 3.945929765701294,
      "learning_rate": 4.5563848942745214e-05,
      "loss": 0.7,
      "step": 583400
    },
    {
      "epoch": 5.32429374406891,
      "grad_norm": 3.661452054977417,
      "learning_rate": 4.5563088546609244e-05,
      "loss": 0.6989,
      "step": 583500
    },
    {
      "epoch": 5.325206219432076,
      "grad_norm": 3.7108030319213867,
      "learning_rate": 4.5562328150473274e-05,
      "loss": 0.6881,
      "step": 583600
    },
    {
      "epoch": 5.32611869479524,
      "grad_norm": 4.0432538986206055,
      "learning_rate": 4.5561567754337304e-05,
      "loss": 0.6854,
      "step": 583700
    },
    {
      "epoch": 5.327031170158405,
      "grad_norm": 4.570216655731201,
      "learning_rate": 4.556080735820133e-05,
      "loss": 0.678,
      "step": 583800
    },
    {
      "epoch": 5.327943645521571,
      "grad_norm": 3.7053329944610596,
      "learning_rate": 4.5560046962065365e-05,
      "loss": 0.7659,
      "step": 583900
    },
    {
      "epoch": 5.328856120884736,
      "grad_norm": 3.472687244415283,
      "learning_rate": 4.555928656592939e-05,
      "loss": 0.7157,
      "step": 584000
    },
    {
      "epoch": 5.329768596247901,
      "grad_norm": 4.149516582489014,
      "learning_rate": 4.555852616979342e-05,
      "loss": 0.6893,
      "step": 584100
    },
    {
      "epoch": 5.330681071611067,
      "grad_norm": 4.417212963104248,
      "learning_rate": 4.555776577365745e-05,
      "loss": 0.7379,
      "step": 584200
    },
    {
      "epoch": 5.331593546974232,
      "grad_norm": 5.013735294342041,
      "learning_rate": 4.555700537752148e-05,
      "loss": 0.7085,
      "step": 584300
    },
    {
      "epoch": 5.332506022337397,
      "grad_norm": 4.0964202880859375,
      "learning_rate": 4.55562449813855e-05,
      "loss": 0.6952,
      "step": 584400
    },
    {
      "epoch": 5.333418497700562,
      "grad_norm": 3.9256651401519775,
      "learning_rate": 4.555548458524954e-05,
      "loss": 0.7231,
      "step": 584500
    },
    {
      "epoch": 5.334330973063727,
      "grad_norm": 4.578510761260986,
      "learning_rate": 4.555472418911356e-05,
      "loss": 0.7153,
      "step": 584600
    },
    {
      "epoch": 5.335243448426892,
      "grad_norm": 4.06133508682251,
      "learning_rate": 4.555396379297759e-05,
      "loss": 0.7101,
      "step": 584700
    },
    {
      "epoch": 5.3361559237900575,
      "grad_norm": 4.862227916717529,
      "learning_rate": 4.555320339684162e-05,
      "loss": 0.6985,
      "step": 584800
    },
    {
      "epoch": 5.337068399153223,
      "grad_norm": 3.4960296154022217,
      "learning_rate": 4.555244300070565e-05,
      "loss": 0.7176,
      "step": 584900
    },
    {
      "epoch": 5.337980874516388,
      "grad_norm": 4.5462799072265625,
      "learning_rate": 4.555168260456968e-05,
      "loss": 0.7415,
      "step": 585000
    },
    {
      "epoch": 5.338893349879553,
      "grad_norm": 4.692801475524902,
      "learning_rate": 4.5550922208433705e-05,
      "loss": 0.6967,
      "step": 585100
    },
    {
      "epoch": 5.339805825242719,
      "grad_norm": 3.47585129737854,
      "learning_rate": 4.5550161812297735e-05,
      "loss": 0.6809,
      "step": 585200
    },
    {
      "epoch": 5.340718300605884,
      "grad_norm": 4.5628180503845215,
      "learning_rate": 4.5549401416161765e-05,
      "loss": 0.691,
      "step": 585300
    },
    {
      "epoch": 5.3416307759690485,
      "grad_norm": 4.212802410125732,
      "learning_rate": 4.5548641020025795e-05,
      "loss": 0.7268,
      "step": 585400
    },
    {
      "epoch": 5.342543251332214,
      "grad_norm": 4.9560227394104,
      "learning_rate": 4.554788062388982e-05,
      "loss": 0.7131,
      "step": 585500
    },
    {
      "epoch": 5.343455726695379,
      "grad_norm": 3.517159938812256,
      "learning_rate": 4.5547120227753855e-05,
      "loss": 0.7156,
      "step": 585600
    },
    {
      "epoch": 5.344368202058544,
      "grad_norm": 3.83719801902771,
      "learning_rate": 4.554635983161788e-05,
      "loss": 0.7213,
      "step": 585700
    },
    {
      "epoch": 5.34528067742171,
      "grad_norm": 3.187329053878784,
      "learning_rate": 4.554559943548191e-05,
      "loss": 0.7022,
      "step": 585800
    },
    {
      "epoch": 5.346193152784875,
      "grad_norm": 4.898430347442627,
      "learning_rate": 4.554483903934594e-05,
      "loss": 0.7216,
      "step": 585900
    },
    {
      "epoch": 5.34710562814804,
      "grad_norm": 3.7617743015289307,
      "learning_rate": 4.554407864320997e-05,
      "loss": 0.7383,
      "step": 586000
    },
    {
      "epoch": 5.3480181035112055,
      "grad_norm": 3.530576229095459,
      "learning_rate": 4.5543318247074e-05,
      "loss": 0.7126,
      "step": 586100
    },
    {
      "epoch": 5.34893057887437,
      "grad_norm": 3.5318100452423096,
      "learning_rate": 4.554255785093803e-05,
      "loss": 0.6921,
      "step": 586200
    },
    {
      "epoch": 5.349843054237535,
      "grad_norm": 4.818366050720215,
      "learning_rate": 4.554179745480205e-05,
      "loss": 0.735,
      "step": 586300
    },
    {
      "epoch": 5.3507555296007006,
      "grad_norm": 4.106106758117676,
      "learning_rate": 4.554103705866609e-05,
      "loss": 0.6962,
      "step": 586400
    },
    {
      "epoch": 5.351668004963866,
      "grad_norm": 4.273547172546387,
      "learning_rate": 4.554027666253011e-05,
      "loss": 0.6783,
      "step": 586500
    },
    {
      "epoch": 5.352580480327031,
      "grad_norm": 4.625733375549316,
      "learning_rate": 4.553951626639414e-05,
      "loss": 0.7171,
      "step": 586600
    },
    {
      "epoch": 5.3534929556901965,
      "grad_norm": 3.3849709033966064,
      "learning_rate": 4.553875587025817e-05,
      "loss": 0.7124,
      "step": 586700
    },
    {
      "epoch": 5.354405431053362,
      "grad_norm": 3.918679714202881,
      "learning_rate": 4.55379954741222e-05,
      "loss": 0.6986,
      "step": 586800
    },
    {
      "epoch": 5.355317906416527,
      "grad_norm": 4.203017711639404,
      "learning_rate": 4.5537235077986226e-05,
      "loss": 0.7066,
      "step": 586900
    },
    {
      "epoch": 5.356230381779692,
      "grad_norm": 3.1137921810150146,
      "learning_rate": 4.553647468185026e-05,
      "loss": 0.6922,
      "step": 587000
    },
    {
      "epoch": 5.357142857142857,
      "grad_norm": 3.841928243637085,
      "learning_rate": 4.5535714285714286e-05,
      "loss": 0.7276,
      "step": 587100
    },
    {
      "epoch": 5.358055332506022,
      "grad_norm": 3.8417813777923584,
      "learning_rate": 4.5534953889578316e-05,
      "loss": 0.7089,
      "step": 587200
    },
    {
      "epoch": 5.358967807869187,
      "grad_norm": 3.3892886638641357,
      "learning_rate": 4.5534193493442346e-05,
      "loss": 0.7077,
      "step": 587300
    },
    {
      "epoch": 5.359880283232353,
      "grad_norm": 4.553450107574463,
      "learning_rate": 4.5533433097306376e-05,
      "loss": 0.7447,
      "step": 587400
    },
    {
      "epoch": 5.360792758595518,
      "grad_norm": 4.316464900970459,
      "learning_rate": 4.5532672701170406e-05,
      "loss": 0.7303,
      "step": 587500
    },
    {
      "epoch": 5.361705233958683,
      "grad_norm": 4.038400650024414,
      "learning_rate": 4.5531912305034436e-05,
      "loss": 0.6947,
      "step": 587600
    },
    {
      "epoch": 5.362617709321849,
      "grad_norm": 3.926333427429199,
      "learning_rate": 4.553115190889846e-05,
      "loss": 0.7248,
      "step": 587700
    },
    {
      "epoch": 5.363530184685014,
      "grad_norm": 3.8197338581085205,
      "learning_rate": 4.5530391512762497e-05,
      "loss": 0.7298,
      "step": 587800
    },
    {
      "epoch": 5.364442660048178,
      "grad_norm": 3.3014495372772217,
      "learning_rate": 4.552963111662652e-05,
      "loss": 0.6993,
      "step": 587900
    },
    {
      "epoch": 5.365355135411344,
      "grad_norm": 4.795527458190918,
      "learning_rate": 4.552887072049054e-05,
      "loss": 0.6901,
      "step": 588000
    },
    {
      "epoch": 5.366267610774509,
      "grad_norm": 4.059646129608154,
      "learning_rate": 4.552811032435458e-05,
      "loss": 0.728,
      "step": 588100
    },
    {
      "epoch": 5.367180086137674,
      "grad_norm": 4.519259929656982,
      "learning_rate": 4.55273499282186e-05,
      "loss": 0.7133,
      "step": 588200
    },
    {
      "epoch": 5.3680925615008395,
      "grad_norm": 3.772430419921875,
      "learning_rate": 4.552658953208263e-05,
      "loss": 0.7127,
      "step": 588300
    },
    {
      "epoch": 5.369005036864005,
      "grad_norm": 4.753520965576172,
      "learning_rate": 4.5525829135946663e-05,
      "loss": 0.6781,
      "step": 588400
    },
    {
      "epoch": 5.36991751222717,
      "grad_norm": 3.9254727363586426,
      "learning_rate": 4.5525068739810693e-05,
      "loss": 0.6995,
      "step": 588500
    },
    {
      "epoch": 5.370829987590335,
      "grad_norm": 4.283631801605225,
      "learning_rate": 4.5524308343674724e-05,
      "loss": 0.7429,
      "step": 588600
    },
    {
      "epoch": 5.371742462953501,
      "grad_norm": 3.72385311126709,
      "learning_rate": 4.5523547947538754e-05,
      "loss": 0.6771,
      "step": 588700
    },
    {
      "epoch": 5.372654938316665,
      "grad_norm": 5.366833209991455,
      "learning_rate": 4.552278755140278e-05,
      "loss": 0.7247,
      "step": 588800
    },
    {
      "epoch": 5.37356741367983,
      "grad_norm": 4.54481315612793,
      "learning_rate": 4.5522027155266814e-05,
      "loss": 0.6761,
      "step": 588900
    },
    {
      "epoch": 5.374479889042996,
      "grad_norm": 4.079497814178467,
      "learning_rate": 4.552126675913084e-05,
      "loss": 0.6869,
      "step": 589000
    },
    {
      "epoch": 5.375392364406161,
      "grad_norm": 3.5403990745544434,
      "learning_rate": 4.552050636299487e-05,
      "loss": 0.7399,
      "step": 589100
    },
    {
      "epoch": 5.376304839769326,
      "grad_norm": 3.5981497764587402,
      "learning_rate": 4.55197459668589e-05,
      "loss": 0.6828,
      "step": 589200
    },
    {
      "epoch": 5.377217315132492,
      "grad_norm": 3.31252384185791,
      "learning_rate": 4.551898557072293e-05,
      "loss": 0.66,
      "step": 589300
    },
    {
      "epoch": 5.378129790495657,
      "grad_norm": 4.666115760803223,
      "learning_rate": 4.551822517458695e-05,
      "loss": 0.7241,
      "step": 589400
    },
    {
      "epoch": 5.379042265858822,
      "grad_norm": 4.6326704025268555,
      "learning_rate": 4.551746477845099e-05,
      "loss": 0.7154,
      "step": 589500
    },
    {
      "epoch": 5.379954741221987,
      "grad_norm": 4.217424392700195,
      "learning_rate": 4.551670438231501e-05,
      "loss": 0.724,
      "step": 589600
    },
    {
      "epoch": 5.380867216585152,
      "grad_norm": 3.8662734031677246,
      "learning_rate": 4.551594398617904e-05,
      "loss": 0.7007,
      "step": 589700
    },
    {
      "epoch": 5.381779691948317,
      "grad_norm": 4.280468940734863,
      "learning_rate": 4.551518359004307e-05,
      "loss": 0.7132,
      "step": 589800
    },
    {
      "epoch": 5.3826921673114825,
      "grad_norm": 4.350102424621582,
      "learning_rate": 4.55144231939071e-05,
      "loss": 0.6907,
      "step": 589900
    },
    {
      "epoch": 5.383604642674648,
      "grad_norm": 3.213134288787842,
      "learning_rate": 4.551366279777113e-05,
      "loss": 0.7157,
      "step": 590000
    },
    {
      "epoch": 5.384517118037813,
      "grad_norm": 4.239843845367432,
      "learning_rate": 4.551290240163516e-05,
      "loss": 0.6924,
      "step": 590100
    },
    {
      "epoch": 5.385429593400978,
      "grad_norm": 3.8154706954956055,
      "learning_rate": 4.5512142005499184e-05,
      "loss": 0.7552,
      "step": 590200
    },
    {
      "epoch": 5.386342068764144,
      "grad_norm": 3.5172643661499023,
      "learning_rate": 4.551138160936322e-05,
      "loss": 0.7151,
      "step": 590300
    },
    {
      "epoch": 5.387254544127309,
      "grad_norm": 3.8239028453826904,
      "learning_rate": 4.5510621213227244e-05,
      "loss": 0.7321,
      "step": 590400
    },
    {
      "epoch": 5.388167019490473,
      "grad_norm": 3.9470131397247314,
      "learning_rate": 4.5509860817091274e-05,
      "loss": 0.6959,
      "step": 590500
    },
    {
      "epoch": 5.389079494853639,
      "grad_norm": 3.9783968925476074,
      "learning_rate": 4.5509100420955305e-05,
      "loss": 0.6889,
      "step": 590600
    },
    {
      "epoch": 5.389991970216804,
      "grad_norm": 3.3107354640960693,
      "learning_rate": 4.5508340024819335e-05,
      "loss": 0.7389,
      "step": 590700
    },
    {
      "epoch": 5.390904445579969,
      "grad_norm": 4.567251205444336,
      "learning_rate": 4.550757962868336e-05,
      "loss": 0.6824,
      "step": 590800
    },
    {
      "epoch": 5.391816920943135,
      "grad_norm": 4.581233978271484,
      "learning_rate": 4.550681923254739e-05,
      "loss": 0.6939,
      "step": 590900
    },
    {
      "epoch": 5.3927293963063,
      "grad_norm": 3.4708566665649414,
      "learning_rate": 4.550605883641142e-05,
      "loss": 0.7171,
      "step": 591000
    },
    {
      "epoch": 5.393641871669465,
      "grad_norm": 3.134722948074341,
      "learning_rate": 4.550529844027545e-05,
      "loss": 0.7188,
      "step": 591100
    },
    {
      "epoch": 5.3945543470326305,
      "grad_norm": 4.366696834564209,
      "learning_rate": 4.550453804413948e-05,
      "loss": 0.704,
      "step": 591200
    },
    {
      "epoch": 5.395466822395795,
      "grad_norm": 4.276738166809082,
      "learning_rate": 4.55037776480035e-05,
      "loss": 0.6672,
      "step": 591300
    },
    {
      "epoch": 5.39637929775896,
      "grad_norm": 4.016833305358887,
      "learning_rate": 4.550301725186754e-05,
      "loss": 0.7239,
      "step": 591400
    },
    {
      "epoch": 5.3972917731221255,
      "grad_norm": 4.242523670196533,
      "learning_rate": 4.550225685573156e-05,
      "loss": 0.7392,
      "step": 591500
    },
    {
      "epoch": 5.398204248485291,
      "grad_norm": 5.073846340179443,
      "learning_rate": 4.550149645959559e-05,
      "loss": 0.7144,
      "step": 591600
    },
    {
      "epoch": 5.399116723848456,
      "grad_norm": 4.782216548919678,
      "learning_rate": 4.550073606345962e-05,
      "loss": 0.7109,
      "step": 591700
    },
    {
      "epoch": 5.400029199211621,
      "grad_norm": 4.135695934295654,
      "learning_rate": 4.549997566732365e-05,
      "loss": 0.6877,
      "step": 591800
    },
    {
      "epoch": 5.400941674574787,
      "grad_norm": 3.739473581314087,
      "learning_rate": 4.5499215271187675e-05,
      "loss": 0.7318,
      "step": 591900
    },
    {
      "epoch": 5.401854149937952,
      "grad_norm": 4.72718620300293,
      "learning_rate": 4.549845487505171e-05,
      "loss": 0.694,
      "step": 592000
    },
    {
      "epoch": 5.402766625301117,
      "grad_norm": 3.9459481239318848,
      "learning_rate": 4.5497694478915735e-05,
      "loss": 0.6936,
      "step": 592100
    },
    {
      "epoch": 5.403679100664282,
      "grad_norm": 4.693960189819336,
      "learning_rate": 4.5496934082779765e-05,
      "loss": 0.7099,
      "step": 592200
    },
    {
      "epoch": 5.404591576027447,
      "grad_norm": 4.5120744705200195,
      "learning_rate": 4.5496173686643795e-05,
      "loss": 0.7191,
      "step": 592300
    },
    {
      "epoch": 5.405504051390612,
      "grad_norm": 4.269681453704834,
      "learning_rate": 4.5495413290507825e-05,
      "loss": 0.7281,
      "step": 592400
    },
    {
      "epoch": 5.406416526753778,
      "grad_norm": 3.4821391105651855,
      "learning_rate": 4.5494652894371856e-05,
      "loss": 0.6977,
      "step": 592500
    },
    {
      "epoch": 5.407329002116943,
      "grad_norm": 4.00875186920166,
      "learning_rate": 4.5493892498235886e-05,
      "loss": 0.7562,
      "step": 592600
    },
    {
      "epoch": 5.408241477480108,
      "grad_norm": 5.119884014129639,
      "learning_rate": 4.549313210209991e-05,
      "loss": 0.7116,
      "step": 592700
    },
    {
      "epoch": 5.4091539528432735,
      "grad_norm": 3.9681365489959717,
      "learning_rate": 4.5492371705963946e-05,
      "loss": 0.6931,
      "step": 592800
    },
    {
      "epoch": 5.410066428206439,
      "grad_norm": 3.6237738132476807,
      "learning_rate": 4.549161130982797e-05,
      "loss": 0.6888,
      "step": 592900
    },
    {
      "epoch": 5.410978903569603,
      "grad_norm": 4.2428154945373535,
      "learning_rate": 4.5490850913692e-05,
      "loss": 0.7274,
      "step": 593000
    },
    {
      "epoch": 5.4118913789327685,
      "grad_norm": 3.708591938018799,
      "learning_rate": 4.549009051755603e-05,
      "loss": 0.7466,
      "step": 593100
    },
    {
      "epoch": 5.412803854295934,
      "grad_norm": 3.9311883449554443,
      "learning_rate": 4.548933012142006e-05,
      "loss": 0.7,
      "step": 593200
    },
    {
      "epoch": 5.413716329659099,
      "grad_norm": 3.806899070739746,
      "learning_rate": 4.548856972528409e-05,
      "loss": 0.7376,
      "step": 593300
    },
    {
      "epoch": 5.414628805022264,
      "grad_norm": 3.406595230102539,
      "learning_rate": 4.548780932914812e-05,
      "loss": 0.6965,
      "step": 593400
    },
    {
      "epoch": 5.41554128038543,
      "grad_norm": 3.2015469074249268,
      "learning_rate": 4.548704893301214e-05,
      "loss": 0.7377,
      "step": 593500
    },
    {
      "epoch": 5.416453755748595,
      "grad_norm": 4.0554962158203125,
      "learning_rate": 4.548628853687617e-05,
      "loss": 0.7131,
      "step": 593600
    },
    {
      "epoch": 5.41736623111176,
      "grad_norm": 4.051295757293701,
      "learning_rate": 4.54855281407402e-05,
      "loss": 0.7001,
      "step": 593700
    },
    {
      "epoch": 5.418278706474926,
      "grad_norm": 3.822073459625244,
      "learning_rate": 4.5484767744604226e-05,
      "loss": 0.6861,
      "step": 593800
    },
    {
      "epoch": 5.41919118183809,
      "grad_norm": 3.6653366088867188,
      "learning_rate": 4.548400734846826e-05,
      "loss": 0.7226,
      "step": 593900
    },
    {
      "epoch": 5.420103657201255,
      "grad_norm": 4.698753356933594,
      "learning_rate": 4.5483246952332286e-05,
      "loss": 0.6798,
      "step": 594000
    },
    {
      "epoch": 5.421016132564421,
      "grad_norm": 4.385021209716797,
      "learning_rate": 4.5482486556196316e-05,
      "loss": 0.7594,
      "step": 594100
    },
    {
      "epoch": 5.421928607927586,
      "grad_norm": 4.343277454376221,
      "learning_rate": 4.5481726160060346e-05,
      "loss": 0.74,
      "step": 594200
    },
    {
      "epoch": 5.422841083290751,
      "grad_norm": 4.418269634246826,
      "learning_rate": 4.5480965763924376e-05,
      "loss": 0.701,
      "step": 594300
    },
    {
      "epoch": 5.4237535586539165,
      "grad_norm": 3.928251266479492,
      "learning_rate": 4.5480205367788406e-05,
      "loss": 0.7443,
      "step": 594400
    },
    {
      "epoch": 5.424666034017082,
      "grad_norm": 3.9203860759735107,
      "learning_rate": 4.5479444971652437e-05,
      "loss": 0.7698,
      "step": 594500
    },
    {
      "epoch": 5.425578509380247,
      "grad_norm": 3.926161527633667,
      "learning_rate": 4.547868457551646e-05,
      "loss": 0.7247,
      "step": 594600
    },
    {
      "epoch": 5.4264909847434115,
      "grad_norm": 3.5979113578796387,
      "learning_rate": 4.54779241793805e-05,
      "loss": 0.679,
      "step": 594700
    },
    {
      "epoch": 5.427403460106577,
      "grad_norm": 4.169378757476807,
      "learning_rate": 4.547716378324452e-05,
      "loss": 0.6823,
      "step": 594800
    },
    {
      "epoch": 5.428315935469742,
      "grad_norm": 4.261292457580566,
      "learning_rate": 4.547640338710855e-05,
      "loss": 0.7081,
      "step": 594900
    },
    {
      "epoch": 5.429228410832907,
      "grad_norm": 4.100779056549072,
      "learning_rate": 4.547564299097258e-05,
      "loss": 0.7195,
      "step": 595000
    },
    {
      "epoch": 5.430140886196073,
      "grad_norm": 4.155597686767578,
      "learning_rate": 4.547488259483661e-05,
      "loss": 0.7034,
      "step": 595100
    },
    {
      "epoch": 5.431053361559238,
      "grad_norm": 3.8443922996520996,
      "learning_rate": 4.5474122198700633e-05,
      "loss": 0.7016,
      "step": 595200
    },
    {
      "epoch": 5.431965836922403,
      "grad_norm": 4.49264669418335,
      "learning_rate": 4.547336180256467e-05,
      "loss": 0.743,
      "step": 595300
    },
    {
      "epoch": 5.432878312285569,
      "grad_norm": 3.855642318725586,
      "learning_rate": 4.5472601406428694e-05,
      "loss": 0.712,
      "step": 595400
    },
    {
      "epoch": 5.433790787648734,
      "grad_norm": 4.5690202713012695,
      "learning_rate": 4.5471841010292724e-05,
      "loss": 0.723,
      "step": 595500
    },
    {
      "epoch": 5.434703263011898,
      "grad_norm": 4.279939651489258,
      "learning_rate": 4.5471080614156754e-05,
      "loss": 0.6631,
      "step": 595600
    },
    {
      "epoch": 5.435615738375064,
      "grad_norm": 3.2661261558532715,
      "learning_rate": 4.5470320218020784e-05,
      "loss": 0.7463,
      "step": 595700
    },
    {
      "epoch": 5.436528213738229,
      "grad_norm": 4.281486988067627,
      "learning_rate": 4.5469559821884814e-05,
      "loss": 0.6907,
      "step": 595800
    },
    {
      "epoch": 5.437440689101394,
      "grad_norm": 4.15492582321167,
      "learning_rate": 4.5468799425748844e-05,
      "loss": 0.6735,
      "step": 595900
    },
    {
      "epoch": 5.4383531644645595,
      "grad_norm": 4.059737682342529,
      "learning_rate": 4.546803902961287e-05,
      "loss": 0.7165,
      "step": 596000
    },
    {
      "epoch": 5.439265639827725,
      "grad_norm": 3.574697732925415,
      "learning_rate": 4.5467278633476904e-05,
      "loss": 0.7206,
      "step": 596100
    },
    {
      "epoch": 5.44017811519089,
      "grad_norm": 5.050815582275391,
      "learning_rate": 4.546651823734093e-05,
      "loss": 0.7178,
      "step": 596200
    },
    {
      "epoch": 5.441090590554055,
      "grad_norm": 3.7470321655273438,
      "learning_rate": 4.546575784120496e-05,
      "loss": 0.7156,
      "step": 596300
    },
    {
      "epoch": 5.44200306591722,
      "grad_norm": 5.060332298278809,
      "learning_rate": 4.546499744506899e-05,
      "loss": 0.6855,
      "step": 596400
    },
    {
      "epoch": 5.442915541280385,
      "grad_norm": 3.771744966506958,
      "learning_rate": 4.546423704893301e-05,
      "loss": 0.6957,
      "step": 596500
    },
    {
      "epoch": 5.44382801664355,
      "grad_norm": 3.3055896759033203,
      "learning_rate": 4.546347665279704e-05,
      "loss": 0.717,
      "step": 596600
    },
    {
      "epoch": 5.444740492006716,
      "grad_norm": 4.547671794891357,
      "learning_rate": 4.546271625666107e-05,
      "loss": 0.7531,
      "step": 596700
    },
    {
      "epoch": 5.445652967369881,
      "grad_norm": 4.433072566986084,
      "learning_rate": 4.54619558605251e-05,
      "loss": 0.7184,
      "step": 596800
    },
    {
      "epoch": 5.446565442733046,
      "grad_norm": 4.980215549468994,
      "learning_rate": 4.546119546438913e-05,
      "loss": 0.6972,
      "step": 596900
    },
    {
      "epoch": 5.447477918096212,
      "grad_norm": 3.36242938041687,
      "learning_rate": 4.546043506825316e-05,
      "loss": 0.7226,
      "step": 597000
    },
    {
      "epoch": 5.448390393459377,
      "grad_norm": 2.7472283840179443,
      "learning_rate": 4.5459674672117184e-05,
      "loss": 0.693,
      "step": 597100
    },
    {
      "epoch": 5.449302868822542,
      "grad_norm": 4.773178577423096,
      "learning_rate": 4.545891427598122e-05,
      "loss": 0.7265,
      "step": 597200
    },
    {
      "epoch": 5.450215344185707,
      "grad_norm": 5.110574245452881,
      "learning_rate": 4.5458153879845245e-05,
      "loss": 0.7303,
      "step": 597300
    },
    {
      "epoch": 5.451127819548872,
      "grad_norm": 3.7037124633789062,
      "learning_rate": 4.5457393483709275e-05,
      "loss": 0.7232,
      "step": 597400
    },
    {
      "epoch": 5.452040294912037,
      "grad_norm": 4.725924491882324,
      "learning_rate": 4.5456633087573305e-05,
      "loss": 0.7091,
      "step": 597500
    },
    {
      "epoch": 5.4529527702752025,
      "grad_norm": 3.9202651977539062,
      "learning_rate": 4.5455872691437335e-05,
      "loss": 0.6677,
      "step": 597600
    },
    {
      "epoch": 5.453865245638368,
      "grad_norm": 3.976719856262207,
      "learning_rate": 4.545511229530136e-05,
      "loss": 0.7299,
      "step": 597700
    },
    {
      "epoch": 5.454777721001533,
      "grad_norm": 4.340532302856445,
      "learning_rate": 4.5454351899165395e-05,
      "loss": 0.6799,
      "step": 597800
    },
    {
      "epoch": 5.455690196364698,
      "grad_norm": 4.071364402770996,
      "learning_rate": 4.545359150302942e-05,
      "loss": 0.712,
      "step": 597900
    },
    {
      "epoch": 5.456602671727864,
      "grad_norm": 4.149930953979492,
      "learning_rate": 4.545283110689345e-05,
      "loss": 0.7371,
      "step": 598000
    },
    {
      "epoch": 5.457515147091028,
      "grad_norm": 4.542680263519287,
      "learning_rate": 4.545207071075748e-05,
      "loss": 0.6779,
      "step": 598100
    },
    {
      "epoch": 5.458427622454193,
      "grad_norm": 4.445786476135254,
      "learning_rate": 4.545131031462151e-05,
      "loss": 0.7179,
      "step": 598200
    },
    {
      "epoch": 5.459340097817359,
      "grad_norm": 3.3438053131103516,
      "learning_rate": 4.545054991848554e-05,
      "loss": 0.7418,
      "step": 598300
    },
    {
      "epoch": 5.460252573180524,
      "grad_norm": 4.112296104431152,
      "learning_rate": 4.544978952234957e-05,
      "loss": 0.7111,
      "step": 598400
    },
    {
      "epoch": 5.461165048543689,
      "grad_norm": 3.485776424407959,
      "learning_rate": 4.544902912621359e-05,
      "loss": 0.6965,
      "step": 598500
    },
    {
      "epoch": 5.462077523906855,
      "grad_norm": 4.777273178100586,
      "learning_rate": 4.544826873007763e-05,
      "loss": 0.696,
      "step": 598600
    },
    {
      "epoch": 5.46298999927002,
      "grad_norm": 2.9615659713745117,
      "learning_rate": 4.544750833394165e-05,
      "loss": 0.7017,
      "step": 598700
    },
    {
      "epoch": 5.463902474633185,
      "grad_norm": 4.494898319244385,
      "learning_rate": 4.544674793780568e-05,
      "loss": 0.7093,
      "step": 598800
    },
    {
      "epoch": 5.4648149499963505,
      "grad_norm": 3.946054458618164,
      "learning_rate": 4.544598754166971e-05,
      "loss": 0.7318,
      "step": 598900
    },
    {
      "epoch": 5.465727425359515,
      "grad_norm": 3.923621416091919,
      "learning_rate": 4.544522714553374e-05,
      "loss": 0.7212,
      "step": 599000
    },
    {
      "epoch": 5.46663990072268,
      "grad_norm": 3.410691261291504,
      "learning_rate": 4.5444466749397765e-05,
      "loss": 0.734,
      "step": 599100
    },
    {
      "epoch": 5.4675523760858455,
      "grad_norm": 3.9097495079040527,
      "learning_rate": 4.54437063532618e-05,
      "loss": 0.7111,
      "step": 599200
    },
    {
      "epoch": 5.468464851449011,
      "grad_norm": 3.487046480178833,
      "learning_rate": 4.5442945957125826e-05,
      "loss": 0.6913,
      "step": 599300
    },
    {
      "epoch": 5.469377326812176,
      "grad_norm": 3.808215379714966,
      "learning_rate": 4.5442185560989856e-05,
      "loss": 0.7168,
      "step": 599400
    },
    {
      "epoch": 5.470289802175341,
      "grad_norm": 4.0988335609436035,
      "learning_rate": 4.5441425164853886e-05,
      "loss": 0.7372,
      "step": 599500
    },
    {
      "epoch": 5.471202277538507,
      "grad_norm": 3.49743914604187,
      "learning_rate": 4.544066476871791e-05,
      "loss": 0.6869,
      "step": 599600
    },
    {
      "epoch": 5.472114752901672,
      "grad_norm": 3.4134340286254883,
      "learning_rate": 4.5439904372581946e-05,
      "loss": 0.7062,
      "step": 599700
    },
    {
      "epoch": 5.4730272282648365,
      "grad_norm": 4.431375503540039,
      "learning_rate": 4.543914397644597e-05,
      "loss": 0.7133,
      "step": 599800
    },
    {
      "epoch": 5.473939703628002,
      "grad_norm": 4.558964252471924,
      "learning_rate": 4.543838358031e-05,
      "loss": 0.7284,
      "step": 599900
    },
    {
      "epoch": 5.474852178991167,
      "grad_norm": 3.4924957752227783,
      "learning_rate": 4.543762318417403e-05,
      "loss": 0.723,
      "step": 600000
    },
    {
      "epoch": 5.475764654354332,
      "grad_norm": 3.8397438526153564,
      "learning_rate": 4.543686278803806e-05,
      "loss": 0.7345,
      "step": 600100
    },
    {
      "epoch": 5.476677129717498,
      "grad_norm": 3.508972406387329,
      "learning_rate": 4.543610239190208e-05,
      "loss": 0.7201,
      "step": 600200
    },
    {
      "epoch": 5.477589605080663,
      "grad_norm": 3.8328604698181152,
      "learning_rate": 4.543534199576612e-05,
      "loss": 0.6991,
      "step": 600300
    },
    {
      "epoch": 5.478502080443828,
      "grad_norm": 3.387967586517334,
      "learning_rate": 4.543458159963014e-05,
      "loss": 0.6997,
      "step": 600400
    },
    {
      "epoch": 5.4794145558069935,
      "grad_norm": 4.389172554016113,
      "learning_rate": 4.543382120349417e-05,
      "loss": 0.6923,
      "step": 600500
    },
    {
      "epoch": 5.480327031170159,
      "grad_norm": 4.20940637588501,
      "learning_rate": 4.54330608073582e-05,
      "loss": 0.6946,
      "step": 600600
    },
    {
      "epoch": 5.481239506533323,
      "grad_norm": 4.0144782066345215,
      "learning_rate": 4.543230041122223e-05,
      "loss": 0.7101,
      "step": 600700
    },
    {
      "epoch": 5.482151981896489,
      "grad_norm": 4.286712646484375,
      "learning_rate": 4.543154001508626e-05,
      "loss": 0.6939,
      "step": 600800
    },
    {
      "epoch": 5.483064457259654,
      "grad_norm": 3.484314441680908,
      "learning_rate": 4.543077961895029e-05,
      "loss": 0.7498,
      "step": 600900
    },
    {
      "epoch": 5.483976932622819,
      "grad_norm": 3.6901934146881104,
      "learning_rate": 4.5430019222814316e-05,
      "loss": 0.7119,
      "step": 601000
    },
    {
      "epoch": 5.4848894079859845,
      "grad_norm": 4.133375644683838,
      "learning_rate": 4.542925882667835e-05,
      "loss": 0.7186,
      "step": 601100
    },
    {
      "epoch": 5.48580188334915,
      "grad_norm": 4.721701622009277,
      "learning_rate": 4.5428498430542376e-05,
      "loss": 0.723,
      "step": 601200
    },
    {
      "epoch": 5.486714358712315,
      "grad_norm": 3.1577887535095215,
      "learning_rate": 4.5427738034406407e-05,
      "loss": 0.727,
      "step": 601300
    },
    {
      "epoch": 5.48762683407548,
      "grad_norm": 4.316833972930908,
      "learning_rate": 4.542697763827044e-05,
      "loss": 0.7095,
      "step": 601400
    },
    {
      "epoch": 5.488539309438645,
      "grad_norm": 3.9976580142974854,
      "learning_rate": 4.542621724213447e-05,
      "loss": 0.6731,
      "step": 601500
    },
    {
      "epoch": 5.48945178480181,
      "grad_norm": 3.9599802494049072,
      "learning_rate": 4.542545684599849e-05,
      "loss": 0.6856,
      "step": 601600
    },
    {
      "epoch": 5.490364260164975,
      "grad_norm": 4.377919673919678,
      "learning_rate": 4.542469644986253e-05,
      "loss": 0.6942,
      "step": 601700
    },
    {
      "epoch": 5.491276735528141,
      "grad_norm": 3.9758777618408203,
      "learning_rate": 4.542393605372655e-05,
      "loss": 0.7108,
      "step": 601800
    },
    {
      "epoch": 5.492189210891306,
      "grad_norm": 2.922731876373291,
      "learning_rate": 4.542317565759058e-05,
      "loss": 0.7157,
      "step": 601900
    },
    {
      "epoch": 5.493101686254471,
      "grad_norm": 4.556682586669922,
      "learning_rate": 4.542241526145461e-05,
      "loss": 0.6664,
      "step": 602000
    },
    {
      "epoch": 5.494014161617637,
      "grad_norm": 4.029316425323486,
      "learning_rate": 4.542165486531864e-05,
      "loss": 0.6712,
      "step": 602100
    },
    {
      "epoch": 5.494926636980802,
      "grad_norm": 4.40224552154541,
      "learning_rate": 4.542089446918267e-05,
      "loss": 0.7127,
      "step": 602200
    },
    {
      "epoch": 5.495839112343967,
      "grad_norm": 4.203548908233643,
      "learning_rate": 4.5420134073046694e-05,
      "loss": 0.7164,
      "step": 602300
    },
    {
      "epoch": 5.496751587707132,
      "grad_norm": 3.660499334335327,
      "learning_rate": 4.5419373676910724e-05,
      "loss": 0.6725,
      "step": 602400
    },
    {
      "epoch": 5.497664063070297,
      "grad_norm": 3.5565009117126465,
      "learning_rate": 4.5418613280774754e-05,
      "loss": 0.6832,
      "step": 602500
    },
    {
      "epoch": 5.498576538433462,
      "grad_norm": 4.279128551483154,
      "learning_rate": 4.5417852884638784e-05,
      "loss": 0.7643,
      "step": 602600
    },
    {
      "epoch": 5.4994890137966275,
      "grad_norm": 4.077342510223389,
      "learning_rate": 4.541709248850281e-05,
      "loss": 0.7297,
      "step": 602700
    },
    {
      "epoch": 5.500401489159793,
      "grad_norm": 4.145810604095459,
      "learning_rate": 4.5416332092366844e-05,
      "loss": 0.7261,
      "step": 602800
    },
    {
      "epoch": 5.501313964522958,
      "grad_norm": 3.9640281200408936,
      "learning_rate": 4.541557169623087e-05,
      "loss": 0.7183,
      "step": 602900
    },
    {
      "epoch": 5.502226439886123,
      "grad_norm": 4.734529495239258,
      "learning_rate": 4.54148113000949e-05,
      "loss": 0.7133,
      "step": 603000
    },
    {
      "epoch": 5.503138915249288,
      "grad_norm": 4.009757995605469,
      "learning_rate": 4.541405090395893e-05,
      "loss": 0.6839,
      "step": 603100
    },
    {
      "epoch": 5.504051390612453,
      "grad_norm": 4.647976875305176,
      "learning_rate": 4.541329050782296e-05,
      "loss": 0.7229,
      "step": 603200
    },
    {
      "epoch": 5.504963865975618,
      "grad_norm": 3.963968515396118,
      "learning_rate": 4.541253011168699e-05,
      "loss": 0.7006,
      "step": 603300
    },
    {
      "epoch": 5.505876341338784,
      "grad_norm": 4.085707187652588,
      "learning_rate": 4.541176971555102e-05,
      "loss": 0.7268,
      "step": 603400
    },
    {
      "epoch": 5.506788816701949,
      "grad_norm": 3.2012007236480713,
      "learning_rate": 4.541100931941504e-05,
      "loss": 0.6937,
      "step": 603500
    },
    {
      "epoch": 5.507701292065114,
      "grad_norm": 3.9386606216430664,
      "learning_rate": 4.541024892327908e-05,
      "loss": 0.7276,
      "step": 603600
    },
    {
      "epoch": 5.50861376742828,
      "grad_norm": 4.251444339752197,
      "learning_rate": 4.54094885271431e-05,
      "loss": 0.7449,
      "step": 603700
    },
    {
      "epoch": 5.509526242791445,
      "grad_norm": 4.046072006225586,
      "learning_rate": 4.540872813100713e-05,
      "loss": 0.725,
      "step": 603800
    },
    {
      "epoch": 5.51043871815461,
      "grad_norm": 3.512899160385132,
      "learning_rate": 4.540796773487116e-05,
      "loss": 0.716,
      "step": 603900
    },
    {
      "epoch": 5.5113511935177755,
      "grad_norm": 4.885894298553467,
      "learning_rate": 4.540720733873519e-05,
      "loss": 0.7718,
      "step": 604000
    },
    {
      "epoch": 5.51226366888094,
      "grad_norm": 3.8642008304595947,
      "learning_rate": 4.5406446942599215e-05,
      "loss": 0.7243,
      "step": 604100
    },
    {
      "epoch": 5.513176144244105,
      "grad_norm": 3.9578499794006348,
      "learning_rate": 4.540568654646325e-05,
      "loss": 0.6874,
      "step": 604200
    },
    {
      "epoch": 5.5140886196072705,
      "grad_norm": 4.4783759117126465,
      "learning_rate": 4.5404926150327275e-05,
      "loss": 0.6774,
      "step": 604300
    },
    {
      "epoch": 5.515001094970436,
      "grad_norm": 4.270711898803711,
      "learning_rate": 4.5404165754191305e-05,
      "loss": 0.7326,
      "step": 604400
    },
    {
      "epoch": 5.515913570333601,
      "grad_norm": 3.0127172470092773,
      "learning_rate": 4.5403405358055335e-05,
      "loss": 0.7295,
      "step": 604500
    },
    {
      "epoch": 5.516826045696766,
      "grad_norm": 4.603987216949463,
      "learning_rate": 4.5402644961919365e-05,
      "loss": 0.7215,
      "step": 604600
    },
    {
      "epoch": 5.517738521059932,
      "grad_norm": 4.242974281311035,
      "learning_rate": 4.5401884565783395e-05,
      "loss": 0.6827,
      "step": 604700
    },
    {
      "epoch": 5.518650996423096,
      "grad_norm": 3.861403465270996,
      "learning_rate": 4.5401124169647425e-05,
      "loss": 0.6763,
      "step": 604800
    },
    {
      "epoch": 5.519563471786261,
      "grad_norm": 4.011287689208984,
      "learning_rate": 4.540036377351145e-05,
      "loss": 0.733,
      "step": 604900
    },
    {
      "epoch": 5.520475947149427,
      "grad_norm": 4.427595615386963,
      "learning_rate": 4.539960337737548e-05,
      "loss": 0.7185,
      "step": 605000
    },
    {
      "epoch": 5.521388422512592,
      "grad_norm": 2.9587080478668213,
      "learning_rate": 4.539884298123951e-05,
      "loss": 0.7289,
      "step": 605100
    },
    {
      "epoch": 5.522300897875757,
      "grad_norm": 4.40673303604126,
      "learning_rate": 4.539808258510354e-05,
      "loss": 0.7214,
      "step": 605200
    },
    {
      "epoch": 5.523213373238923,
      "grad_norm": 3.6740520000457764,
      "learning_rate": 4.539732218896757e-05,
      "loss": 0.7264,
      "step": 605300
    },
    {
      "epoch": 5.524125848602088,
      "grad_norm": 4.560242176055908,
      "learning_rate": 4.539656179283159e-05,
      "loss": 0.7055,
      "step": 605400
    },
    {
      "epoch": 5.525038323965253,
      "grad_norm": 4.270146369934082,
      "learning_rate": 4.539580139669562e-05,
      "loss": 0.6935,
      "step": 605500
    },
    {
      "epoch": 5.5259507993284185,
      "grad_norm": 3.63944411277771,
      "learning_rate": 4.539504100055965e-05,
      "loss": 0.6837,
      "step": 605600
    },
    {
      "epoch": 5.526863274691584,
      "grad_norm": 4.397143363952637,
      "learning_rate": 4.539428060442368e-05,
      "loss": 0.6816,
      "step": 605700
    },
    {
      "epoch": 5.527775750054748,
      "grad_norm": 4.268555164337158,
      "learning_rate": 4.539352020828771e-05,
      "loss": 0.6926,
      "step": 605800
    },
    {
      "epoch": 5.5286882254179135,
      "grad_norm": 4.0821003913879395,
      "learning_rate": 4.539275981215174e-05,
      "loss": 0.7209,
      "step": 605900
    },
    {
      "epoch": 5.529600700781079,
      "grad_norm": 4.649663925170898,
      "learning_rate": 4.5391999416015765e-05,
      "loss": 0.7346,
      "step": 606000
    },
    {
      "epoch": 5.530513176144244,
      "grad_norm": 3.3804821968078613,
      "learning_rate": 4.53912390198798e-05,
      "loss": 0.6878,
      "step": 606100
    },
    {
      "epoch": 5.531425651507409,
      "grad_norm": 4.052192211151123,
      "learning_rate": 4.5390478623743826e-05,
      "loss": 0.6925,
      "step": 606200
    },
    {
      "epoch": 5.532338126870575,
      "grad_norm": 3.674246072769165,
      "learning_rate": 4.5389718227607856e-05,
      "loss": 0.7344,
      "step": 606300
    },
    {
      "epoch": 5.53325060223374,
      "grad_norm": 3.9102489948272705,
      "learning_rate": 4.5388957831471886e-05,
      "loss": 0.6799,
      "step": 606400
    },
    {
      "epoch": 5.534163077596904,
      "grad_norm": 2.6968679428100586,
      "learning_rate": 4.5388197435335916e-05,
      "loss": 0.7208,
      "step": 606500
    },
    {
      "epoch": 5.53507555296007,
      "grad_norm": 3.5687332153320312,
      "learning_rate": 4.5387437039199946e-05,
      "loss": 0.712,
      "step": 606600
    },
    {
      "epoch": 5.535988028323235,
      "grad_norm": 4.801436424255371,
      "learning_rate": 4.5386676643063976e-05,
      "loss": 0.6959,
      "step": 606700
    },
    {
      "epoch": 5.5369005036864,
      "grad_norm": 4.686015605926514,
      "learning_rate": 4.5385916246928e-05,
      "loss": 0.7342,
      "step": 606800
    },
    {
      "epoch": 5.537812979049566,
      "grad_norm": 3.69451642036438,
      "learning_rate": 4.538515585079203e-05,
      "loss": 0.7293,
      "step": 606900
    },
    {
      "epoch": 5.538725454412731,
      "grad_norm": 2.3785810470581055,
      "learning_rate": 4.538439545465606e-05,
      "loss": 0.7453,
      "step": 607000
    },
    {
      "epoch": 5.539637929775896,
      "grad_norm": 4.155170440673828,
      "learning_rate": 4.538363505852009e-05,
      "loss": 0.7278,
      "step": 607100
    },
    {
      "epoch": 5.5405504051390615,
      "grad_norm": 3.3234384059906006,
      "learning_rate": 4.538287466238412e-05,
      "loss": 0.6946,
      "step": 607200
    },
    {
      "epoch": 5.541462880502227,
      "grad_norm": 4.208082675933838,
      "learning_rate": 4.538211426624815e-05,
      "loss": 0.6984,
      "step": 607300
    },
    {
      "epoch": 5.542375355865392,
      "grad_norm": 4.212037086486816,
      "learning_rate": 4.538135387011217e-05,
      "loss": 0.702,
      "step": 607400
    },
    {
      "epoch": 5.5432878312285565,
      "grad_norm": 3.3426496982574463,
      "learning_rate": 4.538059347397621e-05,
      "loss": 0.6973,
      "step": 607500
    },
    {
      "epoch": 5.544200306591722,
      "grad_norm": 3.252751588821411,
      "learning_rate": 4.537983307784023e-05,
      "loss": 0.7261,
      "step": 607600
    },
    {
      "epoch": 5.545112781954887,
      "grad_norm": 4.21011209487915,
      "learning_rate": 4.537907268170426e-05,
      "loss": 0.683,
      "step": 607700
    },
    {
      "epoch": 5.546025257318052,
      "grad_norm": 4.182308673858643,
      "learning_rate": 4.537831228556829e-05,
      "loss": 0.6968,
      "step": 607800
    },
    {
      "epoch": 5.546937732681218,
      "grad_norm": 4.104737758636475,
      "learning_rate": 4.5377551889432316e-05,
      "loss": 0.7225,
      "step": 607900
    },
    {
      "epoch": 5.547850208044383,
      "grad_norm": 2.5295698642730713,
      "learning_rate": 4.537679149329635e-05,
      "loss": 0.6912,
      "step": 608000
    },
    {
      "epoch": 5.548762683407548,
      "grad_norm": 4.296083927154541,
      "learning_rate": 4.5376031097160377e-05,
      "loss": 0.706,
      "step": 608100
    },
    {
      "epoch": 5.549675158770713,
      "grad_norm": 5.125720500946045,
      "learning_rate": 4.537527070102441e-05,
      "loss": 0.7172,
      "step": 608200
    },
    {
      "epoch": 5.550587634133878,
      "grad_norm": 3.60664701461792,
      "learning_rate": 4.537451030488844e-05,
      "loss": 0.6833,
      "step": 608300
    },
    {
      "epoch": 5.551500109497043,
      "grad_norm": 3.3569910526275635,
      "learning_rate": 4.537374990875247e-05,
      "loss": 0.7202,
      "step": 608400
    },
    {
      "epoch": 5.552412584860209,
      "grad_norm": 4.206730842590332,
      "learning_rate": 4.537298951261649e-05,
      "loss": 0.6985,
      "step": 608500
    },
    {
      "epoch": 5.553325060223374,
      "grad_norm": 3.7928779125213623,
      "learning_rate": 4.537222911648053e-05,
      "loss": 0.7269,
      "step": 608600
    },
    {
      "epoch": 5.554237535586539,
      "grad_norm": 3.6507580280303955,
      "learning_rate": 4.537146872034455e-05,
      "loss": 0.7094,
      "step": 608700
    },
    {
      "epoch": 5.5551500109497045,
      "grad_norm": 3.257553815841675,
      "learning_rate": 4.537070832420858e-05,
      "loss": 0.6825,
      "step": 608800
    },
    {
      "epoch": 5.55606248631287,
      "grad_norm": 3.6051902770996094,
      "learning_rate": 4.536994792807261e-05,
      "loss": 0.7164,
      "step": 608900
    },
    {
      "epoch": 5.556974961676035,
      "grad_norm": 3.8532798290252686,
      "learning_rate": 4.536918753193664e-05,
      "loss": 0.7209,
      "step": 609000
    },
    {
      "epoch": 5.5578874370392,
      "grad_norm": 3.780914306640625,
      "learning_rate": 4.536842713580067e-05,
      "loss": 0.6991,
      "step": 609100
    },
    {
      "epoch": 5.558799912402365,
      "grad_norm": 3.3615286350250244,
      "learning_rate": 4.53676667396647e-05,
      "loss": 0.7048,
      "step": 609200
    },
    {
      "epoch": 5.55971238776553,
      "grad_norm": 3.7334651947021484,
      "learning_rate": 4.5366906343528724e-05,
      "loss": 0.6721,
      "step": 609300
    },
    {
      "epoch": 5.560624863128695,
      "grad_norm": 1.81600821018219,
      "learning_rate": 4.536614594739276e-05,
      "loss": 0.6954,
      "step": 609400
    },
    {
      "epoch": 5.561537338491861,
      "grad_norm": 3.875664234161377,
      "learning_rate": 4.5365385551256784e-05,
      "loss": 0.6765,
      "step": 609500
    },
    {
      "epoch": 5.562449813855026,
      "grad_norm": 4.700130939483643,
      "learning_rate": 4.5364625155120814e-05,
      "loss": 0.6975,
      "step": 609600
    },
    {
      "epoch": 5.563362289218191,
      "grad_norm": 3.2229535579681396,
      "learning_rate": 4.5363864758984844e-05,
      "loss": 0.7397,
      "step": 609700
    },
    {
      "epoch": 5.564274764581357,
      "grad_norm": 4.623933792114258,
      "learning_rate": 4.5363104362848874e-05,
      "loss": 0.7515,
      "step": 609800
    },
    {
      "epoch": 5.565187239944521,
      "grad_norm": 3.97582745552063,
      "learning_rate": 4.53623439667129e-05,
      "loss": 0.6907,
      "step": 609900
    },
    {
      "epoch": 5.566099715307686,
      "grad_norm": 3.160994291305542,
      "learning_rate": 4.5361583570576934e-05,
      "loss": 0.7028,
      "step": 610000
    },
    {
      "epoch": 5.567012190670852,
      "grad_norm": 4.521780014038086,
      "learning_rate": 4.536082317444096e-05,
      "loss": 0.71,
      "step": 610100
    },
    {
      "epoch": 5.567924666034017,
      "grad_norm": 4.644437789916992,
      "learning_rate": 4.536006277830499e-05,
      "loss": 0.69,
      "step": 610200
    },
    {
      "epoch": 5.568837141397182,
      "grad_norm": 3.0954995155334473,
      "learning_rate": 4.535930238216902e-05,
      "loss": 0.6984,
      "step": 610300
    },
    {
      "epoch": 5.5697496167603475,
      "grad_norm": 3.993699550628662,
      "learning_rate": 4.535854198603305e-05,
      "loss": 0.6677,
      "step": 610400
    },
    {
      "epoch": 5.570662092123513,
      "grad_norm": 4.4451823234558105,
      "learning_rate": 4.535778158989708e-05,
      "loss": 0.6984,
      "step": 610500
    },
    {
      "epoch": 5.571574567486678,
      "grad_norm": 4.7695112228393555,
      "learning_rate": 4.535702119376111e-05,
      "loss": 0.7209,
      "step": 610600
    },
    {
      "epoch": 5.572487042849843,
      "grad_norm": 3.925156593322754,
      "learning_rate": 4.535626079762513e-05,
      "loss": 0.6858,
      "step": 610700
    },
    {
      "epoch": 5.573399518213009,
      "grad_norm": 3.9613020420074463,
      "learning_rate": 4.535550040148916e-05,
      "loss": 0.7058,
      "step": 610800
    },
    {
      "epoch": 5.574311993576173,
      "grad_norm": 3.4257681369781494,
      "learning_rate": 4.535474000535319e-05,
      "loss": 0.7418,
      "step": 610900
    },
    {
      "epoch": 5.575224468939338,
      "grad_norm": 3.7491698265075684,
      "learning_rate": 4.5353979609217215e-05,
      "loss": 0.7317,
      "step": 611000
    },
    {
      "epoch": 5.576136944302504,
      "grad_norm": 4.854706287384033,
      "learning_rate": 4.535321921308125e-05,
      "loss": 0.724,
      "step": 611100
    },
    {
      "epoch": 5.577049419665669,
      "grad_norm": 4.265872955322266,
      "learning_rate": 4.5352458816945275e-05,
      "loss": 0.7123,
      "step": 611200
    },
    {
      "epoch": 5.577961895028834,
      "grad_norm": 4.837655544281006,
      "learning_rate": 4.5351698420809305e-05,
      "loss": 0.6804,
      "step": 611300
    },
    {
      "epoch": 5.578874370392,
      "grad_norm": 4.670650959014893,
      "learning_rate": 4.5350938024673335e-05,
      "loss": 0.7034,
      "step": 611400
    },
    {
      "epoch": 5.579786845755165,
      "grad_norm": 4.529812335968018,
      "learning_rate": 4.5350177628537365e-05,
      "loss": 0.6924,
      "step": 611500
    },
    {
      "epoch": 5.580699321118329,
      "grad_norm": 4.156021595001221,
      "learning_rate": 4.5349417232401395e-05,
      "loss": 0.7293,
      "step": 611600
    },
    {
      "epoch": 5.581611796481495,
      "grad_norm": 4.223960876464844,
      "learning_rate": 4.5348656836265425e-05,
      "loss": 0.7077,
      "step": 611700
    },
    {
      "epoch": 5.58252427184466,
      "grad_norm": 3.354182243347168,
      "learning_rate": 4.534789644012945e-05,
      "loss": 0.72,
      "step": 611800
    },
    {
      "epoch": 5.583436747207825,
      "grad_norm": 3.5179858207702637,
      "learning_rate": 4.5347136043993485e-05,
      "loss": 0.714,
      "step": 611900
    },
    {
      "epoch": 5.5843492225709905,
      "grad_norm": 3.861973762512207,
      "learning_rate": 4.534637564785751e-05,
      "loss": 0.6975,
      "step": 612000
    },
    {
      "epoch": 5.585261697934156,
      "grad_norm": 2.9840307235717773,
      "learning_rate": 4.534561525172154e-05,
      "loss": 0.7413,
      "step": 612100
    },
    {
      "epoch": 5.586174173297321,
      "grad_norm": 3.7552061080932617,
      "learning_rate": 4.534485485558557e-05,
      "loss": 0.6835,
      "step": 612200
    },
    {
      "epoch": 5.587086648660486,
      "grad_norm": 4.1229166984558105,
      "learning_rate": 4.53440944594496e-05,
      "loss": 0.7015,
      "step": 612300
    },
    {
      "epoch": 5.587999124023652,
      "grad_norm": 5.254836082458496,
      "learning_rate": 4.534333406331362e-05,
      "loss": 0.7181,
      "step": 612400
    },
    {
      "epoch": 5.588911599386817,
      "grad_norm": 4.723607063293457,
      "learning_rate": 4.534257366717766e-05,
      "loss": 0.7239,
      "step": 612500
    },
    {
      "epoch": 5.589824074749981,
      "grad_norm": 3.896610736846924,
      "learning_rate": 4.534181327104168e-05,
      "loss": 0.6852,
      "step": 612600
    },
    {
      "epoch": 5.590736550113147,
      "grad_norm": 4.848191261291504,
      "learning_rate": 4.534105287490571e-05,
      "loss": 0.6948,
      "step": 612700
    },
    {
      "epoch": 5.591649025476312,
      "grad_norm": 3.165055990219116,
      "learning_rate": 4.534029247876974e-05,
      "loss": 0.6757,
      "step": 612800
    },
    {
      "epoch": 5.592561500839477,
      "grad_norm": 4.519686698913574,
      "learning_rate": 4.533953208263377e-05,
      "loss": 0.706,
      "step": 612900
    },
    {
      "epoch": 5.593473976202643,
      "grad_norm": 4.543940544128418,
      "learning_rate": 4.53387716864978e-05,
      "loss": 0.7232,
      "step": 613000
    },
    {
      "epoch": 5.594386451565808,
      "grad_norm": 4.542547225952148,
      "learning_rate": 4.533801129036183e-05,
      "loss": 0.6922,
      "step": 613100
    },
    {
      "epoch": 5.595298926928973,
      "grad_norm": 4.120119571685791,
      "learning_rate": 4.5337250894225856e-05,
      "loss": 0.6838,
      "step": 613200
    },
    {
      "epoch": 5.596211402292138,
      "grad_norm": 3.721238851547241,
      "learning_rate": 4.533649049808989e-05,
      "loss": 0.6984,
      "step": 613300
    },
    {
      "epoch": 5.597123877655303,
      "grad_norm": 3.480438232421875,
      "learning_rate": 4.5335730101953916e-05,
      "loss": 0.6786,
      "step": 613400
    },
    {
      "epoch": 5.598036353018468,
      "grad_norm": 3.720130443572998,
      "learning_rate": 4.533496970581794e-05,
      "loss": 0.7041,
      "step": 613500
    },
    {
      "epoch": 5.5989488283816335,
      "grad_norm": 4.080923080444336,
      "learning_rate": 4.5334209309681976e-05,
      "loss": 0.7018,
      "step": 613600
    },
    {
      "epoch": 5.599861303744799,
      "grad_norm": 4.3562235832214355,
      "learning_rate": 4.5333448913546e-05,
      "loss": 0.7055,
      "step": 613700
    },
    {
      "epoch": 5.600773779107964,
      "grad_norm": 3.939404010772705,
      "learning_rate": 4.533268851741003e-05,
      "loss": 0.6973,
      "step": 613800
    },
    {
      "epoch": 5.6016862544711294,
      "grad_norm": 4.422375679016113,
      "learning_rate": 4.533192812127406e-05,
      "loss": 0.7012,
      "step": 613900
    },
    {
      "epoch": 5.602598729834295,
      "grad_norm": 2.7654504776000977,
      "learning_rate": 4.533116772513809e-05,
      "loss": 0.6881,
      "step": 614000
    },
    {
      "epoch": 5.60351120519746,
      "grad_norm": 4.5668840408325195,
      "learning_rate": 4.533040732900212e-05,
      "loss": 0.7165,
      "step": 614100
    },
    {
      "epoch": 5.604423680560625,
      "grad_norm": 2.9434452056884766,
      "learning_rate": 4.532964693286615e-05,
      "loss": 0.7316,
      "step": 614200
    },
    {
      "epoch": 5.60533615592379,
      "grad_norm": 5.270471096038818,
      "learning_rate": 4.532888653673017e-05,
      "loss": 0.7129,
      "step": 614300
    },
    {
      "epoch": 5.606248631286955,
      "grad_norm": 4.5636162757873535,
      "learning_rate": 4.532812614059421e-05,
      "loss": 0.7256,
      "step": 614400
    },
    {
      "epoch": 5.60716110665012,
      "grad_norm": 3.8404548168182373,
      "learning_rate": 4.532736574445823e-05,
      "loss": 0.7018,
      "step": 614500
    },
    {
      "epoch": 5.608073582013286,
      "grad_norm": 4.031583786010742,
      "learning_rate": 4.532660534832226e-05,
      "loss": 0.6953,
      "step": 614600
    },
    {
      "epoch": 5.608986057376451,
      "grad_norm": 3.4020028114318848,
      "learning_rate": 4.532584495218629e-05,
      "loss": 0.6924,
      "step": 614700
    },
    {
      "epoch": 5.609898532739616,
      "grad_norm": 4.299678802490234,
      "learning_rate": 4.532508455605032e-05,
      "loss": 0.7098,
      "step": 614800
    },
    {
      "epoch": 5.6108110081027815,
      "grad_norm": 4.195802688598633,
      "learning_rate": 4.5324324159914347e-05,
      "loss": 0.7093,
      "step": 614900
    },
    {
      "epoch": 5.611723483465946,
      "grad_norm": 3.654203176498413,
      "learning_rate": 4.5323563763778383e-05,
      "loss": 0.7061,
      "step": 615000
    },
    {
      "epoch": 5.612635958829111,
      "grad_norm": 4.071910381317139,
      "learning_rate": 4.532280336764241e-05,
      "loss": 0.7367,
      "step": 615100
    },
    {
      "epoch": 5.613548434192277,
      "grad_norm": 4.347844123840332,
      "learning_rate": 4.532204297150644e-05,
      "loss": 0.7224,
      "step": 615200
    },
    {
      "epoch": 5.614460909555442,
      "grad_norm": 4.165804386138916,
      "learning_rate": 4.532128257537047e-05,
      "loss": 0.6932,
      "step": 615300
    },
    {
      "epoch": 5.615373384918607,
      "grad_norm": 4.505943775177002,
      "learning_rate": 4.53205221792345e-05,
      "loss": 0.7062,
      "step": 615400
    },
    {
      "epoch": 5.6162858602817725,
      "grad_norm": 4.023433208465576,
      "learning_rate": 4.531976178309853e-05,
      "loss": 0.6905,
      "step": 615500
    },
    {
      "epoch": 5.617198335644938,
      "grad_norm": 4.212069034576416,
      "learning_rate": 4.531900138696256e-05,
      "loss": 0.6806,
      "step": 615600
    },
    {
      "epoch": 5.618110811008103,
      "grad_norm": 4.4211812019348145,
      "learning_rate": 4.531824099082658e-05,
      "loss": 0.7278,
      "step": 615700
    },
    {
      "epoch": 5.619023286371268,
      "grad_norm": 4.805001258850098,
      "learning_rate": 4.531748059469062e-05,
      "loss": 0.7354,
      "step": 615800
    },
    {
      "epoch": 5.619935761734434,
      "grad_norm": 3.9927849769592285,
      "learning_rate": 4.531672019855464e-05,
      "loss": 0.6716,
      "step": 615900
    },
    {
      "epoch": 5.620848237097598,
      "grad_norm": 4.401875972747803,
      "learning_rate": 4.531595980241867e-05,
      "loss": 0.7074,
      "step": 616000
    },
    {
      "epoch": 5.621760712460763,
      "grad_norm": 3.638051986694336,
      "learning_rate": 4.53151994062827e-05,
      "loss": 0.661,
      "step": 616100
    },
    {
      "epoch": 5.622673187823929,
      "grad_norm": 3.9113588333129883,
      "learning_rate": 4.531443901014673e-05,
      "loss": 0.7597,
      "step": 616200
    },
    {
      "epoch": 5.623585663187094,
      "grad_norm": 3.721177339553833,
      "learning_rate": 4.5313678614010754e-05,
      "loss": 0.708,
      "step": 616300
    },
    {
      "epoch": 5.624498138550259,
      "grad_norm": 4.121903419494629,
      "learning_rate": 4.5312918217874784e-05,
      "loss": 0.689,
      "step": 616400
    },
    {
      "epoch": 5.625410613913425,
      "grad_norm": 2.724102020263672,
      "learning_rate": 4.5312157821738814e-05,
      "loss": 0.7051,
      "step": 616500
    },
    {
      "epoch": 5.62632308927659,
      "grad_norm": 4.5465850830078125,
      "learning_rate": 4.5311397425602844e-05,
      "loss": 0.7015,
      "step": 616600
    },
    {
      "epoch": 5.627235564639754,
      "grad_norm": 3.9088821411132812,
      "learning_rate": 4.5310637029466874e-05,
      "loss": 0.722,
      "step": 616700
    },
    {
      "epoch": 5.62814804000292,
      "grad_norm": 3.8740859031677246,
      "learning_rate": 4.53098766333309e-05,
      "loss": 0.7143,
      "step": 616800
    },
    {
      "epoch": 5.629060515366085,
      "grad_norm": 4.492135047912598,
      "learning_rate": 4.5309116237194934e-05,
      "loss": 0.713,
      "step": 616900
    },
    {
      "epoch": 5.62997299072925,
      "grad_norm": 4.611722469329834,
      "learning_rate": 4.530835584105896e-05,
      "loss": 0.6995,
      "step": 617000
    },
    {
      "epoch": 5.6308854660924155,
      "grad_norm": 3.801698684692383,
      "learning_rate": 4.530759544492299e-05,
      "loss": 0.7002,
      "step": 617100
    },
    {
      "epoch": 5.631797941455581,
      "grad_norm": 5.042663097381592,
      "learning_rate": 4.530683504878702e-05,
      "loss": 0.718,
      "step": 617200
    },
    {
      "epoch": 5.632710416818746,
      "grad_norm": 3.5849075317382812,
      "learning_rate": 4.530607465265105e-05,
      "loss": 0.7162,
      "step": 617300
    },
    {
      "epoch": 5.633622892181911,
      "grad_norm": 3.8160810470581055,
      "learning_rate": 4.530531425651507e-05,
      "loss": 0.6612,
      "step": 617400
    },
    {
      "epoch": 5.634535367545077,
      "grad_norm": 4.3065338134765625,
      "learning_rate": 4.530455386037911e-05,
      "loss": 0.6966,
      "step": 617500
    },
    {
      "epoch": 5.635447842908242,
      "grad_norm": 4.137662887573242,
      "learning_rate": 4.530379346424313e-05,
      "loss": 0.7383,
      "step": 617600
    },
    {
      "epoch": 5.636360318271406,
      "grad_norm": 3.2645819187164307,
      "learning_rate": 4.530303306810716e-05,
      "loss": 0.7474,
      "step": 617700
    },
    {
      "epoch": 5.637272793634572,
      "grad_norm": 3.858442544937134,
      "learning_rate": 4.530227267197119e-05,
      "loss": 0.7002,
      "step": 617800
    },
    {
      "epoch": 5.638185268997737,
      "grad_norm": 4.8460693359375,
      "learning_rate": 4.530151227583522e-05,
      "loss": 0.7405,
      "step": 617900
    },
    {
      "epoch": 5.639097744360902,
      "grad_norm": 3.7748939990997314,
      "learning_rate": 4.530075187969925e-05,
      "loss": 0.6997,
      "step": 618000
    },
    {
      "epoch": 5.640010219724068,
      "grad_norm": 3.8516104221343994,
      "learning_rate": 4.529999148356328e-05,
      "loss": 0.7151,
      "step": 618100
    },
    {
      "epoch": 5.640922695087233,
      "grad_norm": 3.556410074234009,
      "learning_rate": 4.5299231087427305e-05,
      "loss": 0.7068,
      "step": 618200
    },
    {
      "epoch": 5.641835170450398,
      "grad_norm": 3.9478681087493896,
      "learning_rate": 4.529847069129134e-05,
      "loss": 0.7034,
      "step": 618300
    },
    {
      "epoch": 5.642747645813563,
      "grad_norm": 3.663177490234375,
      "learning_rate": 4.5297710295155365e-05,
      "loss": 0.7122,
      "step": 618400
    },
    {
      "epoch": 5.643660121176728,
      "grad_norm": 4.22908353805542,
      "learning_rate": 4.5296949899019395e-05,
      "loss": 0.6882,
      "step": 618500
    },
    {
      "epoch": 5.644572596539893,
      "grad_norm": 3.910045862197876,
      "learning_rate": 4.5296189502883425e-05,
      "loss": 0.6921,
      "step": 618600
    },
    {
      "epoch": 5.6454850719030585,
      "grad_norm": 4.150722503662109,
      "learning_rate": 4.5295429106747455e-05,
      "loss": 0.7101,
      "step": 618700
    },
    {
      "epoch": 5.646397547266224,
      "grad_norm": 2.211535692214966,
      "learning_rate": 4.5294668710611485e-05,
      "loss": 0.7081,
      "step": 618800
    },
    {
      "epoch": 5.647310022629389,
      "grad_norm": 4.121422290802002,
      "learning_rate": 4.5293908314475515e-05,
      "loss": 0.7195,
      "step": 618900
    },
    {
      "epoch": 5.648222497992554,
      "grad_norm": 4.595743179321289,
      "learning_rate": 4.529314791833954e-05,
      "loss": 0.6617,
      "step": 619000
    },
    {
      "epoch": 5.64913497335572,
      "grad_norm": 2.975532054901123,
      "learning_rate": 4.529238752220357e-05,
      "loss": 0.6797,
      "step": 619100
    },
    {
      "epoch": 5.650047448718885,
      "grad_norm": 4.0652756690979,
      "learning_rate": 4.52916271260676e-05,
      "loss": 0.728,
      "step": 619200
    },
    {
      "epoch": 5.65095992408205,
      "grad_norm": 3.488088846206665,
      "learning_rate": 4.529086672993162e-05,
      "loss": 0.7155,
      "step": 619300
    },
    {
      "epoch": 5.651872399445215,
      "grad_norm": 4.3346662521362305,
      "learning_rate": 4.529010633379566e-05,
      "loss": 0.7211,
      "step": 619400
    },
    {
      "epoch": 5.65278487480838,
      "grad_norm": 4.032890796661377,
      "learning_rate": 4.528934593765968e-05,
      "loss": 0.7188,
      "step": 619500
    },
    {
      "epoch": 5.653697350171545,
      "grad_norm": 3.864480495452881,
      "learning_rate": 4.528858554152371e-05,
      "loss": 0.671,
      "step": 619600
    },
    {
      "epoch": 5.654609825534711,
      "grad_norm": 4.468182563781738,
      "learning_rate": 4.528782514538774e-05,
      "loss": 0.7185,
      "step": 619700
    },
    {
      "epoch": 5.655522300897876,
      "grad_norm": 4.187085151672363,
      "learning_rate": 4.528706474925177e-05,
      "loss": 0.7047,
      "step": 619800
    },
    {
      "epoch": 5.656434776261041,
      "grad_norm": 2.8622097969055176,
      "learning_rate": 4.52863043531158e-05,
      "loss": 0.7099,
      "step": 619900
    },
    {
      "epoch": 5.6573472516242065,
      "grad_norm": 3.4227123260498047,
      "learning_rate": 4.528554395697983e-05,
      "loss": 0.6788,
      "step": 620000
    },
    {
      "epoch": 5.658259726987371,
      "grad_norm": 4.286862373352051,
      "learning_rate": 4.5284783560843856e-05,
      "loss": 0.7175,
      "step": 620100
    },
    {
      "epoch": 5.659172202350536,
      "grad_norm": 4.154837131500244,
      "learning_rate": 4.528402316470789e-05,
      "loss": 0.7084,
      "step": 620200
    },
    {
      "epoch": 5.6600846777137015,
      "grad_norm": 3.5607798099517822,
      "learning_rate": 4.5283262768571916e-05,
      "loss": 0.6914,
      "step": 620300
    },
    {
      "epoch": 5.660997153076867,
      "grad_norm": 3.4867987632751465,
      "learning_rate": 4.5282502372435946e-05,
      "loss": 0.7531,
      "step": 620400
    },
    {
      "epoch": 5.661909628440032,
      "grad_norm": 4.471851825714111,
      "learning_rate": 4.5281741976299976e-05,
      "loss": 0.7231,
      "step": 620500
    },
    {
      "epoch": 5.662822103803197,
      "grad_norm": 4.832911014556885,
      "learning_rate": 4.5280981580164006e-05,
      "loss": 0.699,
      "step": 620600
    },
    {
      "epoch": 5.663734579166363,
      "grad_norm": 4.293267726898193,
      "learning_rate": 4.528022118402803e-05,
      "loss": 0.7243,
      "step": 620700
    },
    {
      "epoch": 5.664647054529528,
      "grad_norm": 4.382997512817383,
      "learning_rate": 4.5279460787892066e-05,
      "loss": 0.7192,
      "step": 620800
    },
    {
      "epoch": 5.665559529892693,
      "grad_norm": 4.080742835998535,
      "learning_rate": 4.527870039175609e-05,
      "loss": 0.7307,
      "step": 620900
    },
    {
      "epoch": 5.666472005255859,
      "grad_norm": 4.359494686126709,
      "learning_rate": 4.527793999562012e-05,
      "loss": 0.7207,
      "step": 621000
    },
    {
      "epoch": 5.667384480619023,
      "grad_norm": 3.7292206287384033,
      "learning_rate": 4.527717959948415e-05,
      "loss": 0.7218,
      "step": 621100
    },
    {
      "epoch": 5.668296955982188,
      "grad_norm": 4.704591274261475,
      "learning_rate": 4.527641920334818e-05,
      "loss": 0.7182,
      "step": 621200
    },
    {
      "epoch": 5.669209431345354,
      "grad_norm": 5.093619346618652,
      "learning_rate": 4.527565880721221e-05,
      "loss": 0.6932,
      "step": 621300
    },
    {
      "epoch": 5.670121906708519,
      "grad_norm": 3.8164658546447754,
      "learning_rate": 4.527489841107624e-05,
      "loss": 0.705,
      "step": 621400
    },
    {
      "epoch": 5.671034382071684,
      "grad_norm": 4.715008735656738,
      "learning_rate": 4.527413801494026e-05,
      "loss": 0.6818,
      "step": 621500
    },
    {
      "epoch": 5.6719468574348495,
      "grad_norm": 4.839920520782471,
      "learning_rate": 4.52733776188043e-05,
      "loss": 0.758,
      "step": 621600
    },
    {
      "epoch": 5.672859332798015,
      "grad_norm": 4.239664554595947,
      "learning_rate": 4.5272617222668323e-05,
      "loss": 0.7259,
      "step": 621700
    },
    {
      "epoch": 5.673771808161179,
      "grad_norm": 3.8106601238250732,
      "learning_rate": 4.5271856826532353e-05,
      "loss": 0.7068,
      "step": 621800
    },
    {
      "epoch": 5.6746842835243445,
      "grad_norm": 4.3178534507751465,
      "learning_rate": 4.5271096430396384e-05,
      "loss": 0.708,
      "step": 621900
    },
    {
      "epoch": 5.67559675888751,
      "grad_norm": 3.9406630992889404,
      "learning_rate": 4.527033603426041e-05,
      "loss": 0.728,
      "step": 622000
    },
    {
      "epoch": 5.676509234250675,
      "grad_norm": 4.7838006019592285,
      "learning_rate": 4.526957563812444e-05,
      "loss": 0.7054,
      "step": 622100
    },
    {
      "epoch": 5.67742170961384,
      "grad_norm": 2.829152822494507,
      "learning_rate": 4.526881524198847e-05,
      "loss": 0.65,
      "step": 622200
    },
    {
      "epoch": 5.678334184977006,
      "grad_norm": 3.5475635528564453,
      "learning_rate": 4.52680548458525e-05,
      "loss": 0.6986,
      "step": 622300
    },
    {
      "epoch": 5.679246660340171,
      "grad_norm": 4.249719619750977,
      "learning_rate": 4.526729444971653e-05,
      "loss": 0.7238,
      "step": 622400
    },
    {
      "epoch": 5.680159135703336,
      "grad_norm": 4.477529048919678,
      "learning_rate": 4.526653405358056e-05,
      "loss": 0.7237,
      "step": 622500
    },
    {
      "epoch": 5.681071611066502,
      "grad_norm": 4.669459342956543,
      "learning_rate": 4.526577365744458e-05,
      "loss": 0.7271,
      "step": 622600
    },
    {
      "epoch": 5.681984086429666,
      "grad_norm": 3.8158376216888428,
      "learning_rate": 4.526501326130862e-05,
      "loss": 0.7155,
      "step": 622700
    },
    {
      "epoch": 5.682896561792831,
      "grad_norm": 4.201997756958008,
      "learning_rate": 4.526425286517264e-05,
      "loss": 0.6817,
      "step": 622800
    },
    {
      "epoch": 5.683809037155997,
      "grad_norm": 4.837583541870117,
      "learning_rate": 4.526349246903667e-05,
      "loss": 0.7419,
      "step": 622900
    },
    {
      "epoch": 5.684721512519162,
      "grad_norm": 4.227145671844482,
      "learning_rate": 4.52627320729007e-05,
      "loss": 0.7431,
      "step": 623000
    },
    {
      "epoch": 5.685633987882327,
      "grad_norm": 3.7822091579437256,
      "learning_rate": 4.526197167676473e-05,
      "loss": 0.7439,
      "step": 623100
    },
    {
      "epoch": 5.6865464632454925,
      "grad_norm": 3.691699266433716,
      "learning_rate": 4.5261211280628754e-05,
      "loss": 0.6577,
      "step": 623200
    },
    {
      "epoch": 5.687458938608658,
      "grad_norm": 3.835225820541382,
      "learning_rate": 4.526045088449279e-05,
      "loss": 0.6833,
      "step": 623300
    },
    {
      "epoch": 5.688371413971823,
      "grad_norm": 3.6950955390930176,
      "learning_rate": 4.5259690488356814e-05,
      "loss": 0.7489,
      "step": 623400
    },
    {
      "epoch": 5.6892838893349875,
      "grad_norm": 4.83299446105957,
      "learning_rate": 4.5258930092220844e-05,
      "loss": 0.6907,
      "step": 623500
    },
    {
      "epoch": 5.690196364698153,
      "grad_norm": 4.508047103881836,
      "learning_rate": 4.5258169696084874e-05,
      "loss": 0.6968,
      "step": 623600
    },
    {
      "epoch": 5.691108840061318,
      "grad_norm": 4.412517547607422,
      "learning_rate": 4.5257409299948904e-05,
      "loss": 0.7336,
      "step": 623700
    },
    {
      "epoch": 5.692021315424483,
      "grad_norm": 4.663590908050537,
      "learning_rate": 4.5256648903812935e-05,
      "loss": 0.7296,
      "step": 623800
    },
    {
      "epoch": 5.692933790787649,
      "grad_norm": 4.8465986251831055,
      "learning_rate": 4.5255888507676965e-05,
      "loss": 0.7041,
      "step": 623900
    },
    {
      "epoch": 5.693846266150814,
      "grad_norm": 3.5110530853271484,
      "learning_rate": 4.525512811154099e-05,
      "loss": 0.6932,
      "step": 624000
    },
    {
      "epoch": 5.694758741513979,
      "grad_norm": 4.495093822479248,
      "learning_rate": 4.5254367715405025e-05,
      "loss": 0.7242,
      "step": 624100
    },
    {
      "epoch": 5.695671216877145,
      "grad_norm": 3.6819112300872803,
      "learning_rate": 4.525360731926905e-05,
      "loss": 0.6998,
      "step": 624200
    },
    {
      "epoch": 5.69658369224031,
      "grad_norm": 4.329512596130371,
      "learning_rate": 4.525284692313308e-05,
      "loss": 0.7118,
      "step": 624300
    },
    {
      "epoch": 5.697496167603474,
      "grad_norm": 3.42266583442688,
      "learning_rate": 4.525208652699711e-05,
      "loss": 0.7408,
      "step": 624400
    },
    {
      "epoch": 5.69840864296664,
      "grad_norm": 3.520447254180908,
      "learning_rate": 4.525132613086114e-05,
      "loss": 0.7233,
      "step": 624500
    },
    {
      "epoch": 5.699321118329805,
      "grad_norm": 4.145502090454102,
      "learning_rate": 4.525056573472516e-05,
      "loss": 0.7146,
      "step": 624600
    },
    {
      "epoch": 5.70023359369297,
      "grad_norm": 3.639826536178589,
      "learning_rate": 4.52498053385892e-05,
      "loss": 0.6932,
      "step": 624700
    },
    {
      "epoch": 5.7011460690561355,
      "grad_norm": 4.801036357879639,
      "learning_rate": 4.524904494245322e-05,
      "loss": 0.6971,
      "step": 624800
    },
    {
      "epoch": 5.702058544419301,
      "grad_norm": 3.986302375793457,
      "learning_rate": 4.524828454631725e-05,
      "loss": 0.7173,
      "step": 624900
    },
    {
      "epoch": 5.702971019782466,
      "grad_norm": 3.933016061782837,
      "learning_rate": 4.524752415018128e-05,
      "loss": 0.6854,
      "step": 625000
    },
    {
      "epoch": 5.703883495145631,
      "grad_norm": 4.238475799560547,
      "learning_rate": 4.5246763754045305e-05,
      "loss": 0.7169,
      "step": 625100
    },
    {
      "epoch": 5.704795970508796,
      "grad_norm": 3.5646724700927734,
      "learning_rate": 4.524600335790934e-05,
      "loss": 0.7156,
      "step": 625200
    },
    {
      "epoch": 5.705708445871961,
      "grad_norm": 3.939121961593628,
      "learning_rate": 4.5245242961773365e-05,
      "loss": 0.6458,
      "step": 625300
    },
    {
      "epoch": 5.706620921235126,
      "grad_norm": 4.710325717926025,
      "learning_rate": 4.5244482565637395e-05,
      "loss": 0.6781,
      "step": 625400
    },
    {
      "epoch": 5.707533396598292,
      "grad_norm": 4.50378942489624,
      "learning_rate": 4.5243722169501425e-05,
      "loss": 0.7367,
      "step": 625500
    },
    {
      "epoch": 5.708445871961457,
      "grad_norm": 4.719475269317627,
      "learning_rate": 4.5242961773365455e-05,
      "loss": 0.7205,
      "step": 625600
    },
    {
      "epoch": 5.709358347324622,
      "grad_norm": 4.397597789764404,
      "learning_rate": 4.524220137722948e-05,
      "loss": 0.7097,
      "step": 625700
    },
    {
      "epoch": 5.710270822687788,
      "grad_norm": 6.071201324462891,
      "learning_rate": 4.5241440981093516e-05,
      "loss": 0.7377,
      "step": 625800
    },
    {
      "epoch": 5.711183298050953,
      "grad_norm": 4.003875732421875,
      "learning_rate": 4.524068058495754e-05,
      "loss": 0.6911,
      "step": 625900
    },
    {
      "epoch": 5.712095773414118,
      "grad_norm": 4.310327529907227,
      "learning_rate": 4.523992018882157e-05,
      "loss": 0.7322,
      "step": 626000
    },
    {
      "epoch": 5.713008248777283,
      "grad_norm": 4.051178455352783,
      "learning_rate": 4.52391597926856e-05,
      "loss": 0.7445,
      "step": 626100
    },
    {
      "epoch": 5.713920724140448,
      "grad_norm": 4.101213455200195,
      "learning_rate": 4.523839939654963e-05,
      "loss": 0.7209,
      "step": 626200
    },
    {
      "epoch": 5.714833199503613,
      "grad_norm": 4.649744510650635,
      "learning_rate": 4.523763900041366e-05,
      "loss": 0.7247,
      "step": 626300
    },
    {
      "epoch": 5.7157456748667785,
      "grad_norm": 4.625921249389648,
      "learning_rate": 4.523687860427769e-05,
      "loss": 0.6904,
      "step": 626400
    },
    {
      "epoch": 5.716658150229944,
      "grad_norm": 3.5706045627593994,
      "learning_rate": 4.523611820814171e-05,
      "loss": 0.7501,
      "step": 626500
    },
    {
      "epoch": 5.717570625593109,
      "grad_norm": 4.068434715270996,
      "learning_rate": 4.523535781200575e-05,
      "loss": 0.7245,
      "step": 626600
    },
    {
      "epoch": 5.718483100956274,
      "grad_norm": 4.198266983032227,
      "learning_rate": 4.523459741586977e-05,
      "loss": 0.7205,
      "step": 626700
    },
    {
      "epoch": 5.71939557631944,
      "grad_norm": 4.707707405090332,
      "learning_rate": 4.52338370197338e-05,
      "loss": 0.6673,
      "step": 626800
    },
    {
      "epoch": 5.720308051682604,
      "grad_norm": 4.397327899932861,
      "learning_rate": 4.523307662359783e-05,
      "loss": 0.7011,
      "step": 626900
    },
    {
      "epoch": 5.721220527045769,
      "grad_norm": 3.99684476852417,
      "learning_rate": 4.523231622746186e-05,
      "loss": 0.7312,
      "step": 627000
    },
    {
      "epoch": 5.722133002408935,
      "grad_norm": 4.047439098358154,
      "learning_rate": 4.5231555831325886e-05,
      "loss": 0.701,
      "step": 627100
    },
    {
      "epoch": 5.7230454777721,
      "grad_norm": 3.8656861782073975,
      "learning_rate": 4.523079543518992e-05,
      "loss": 0.6881,
      "step": 627200
    },
    {
      "epoch": 5.723957953135265,
      "grad_norm": 3.7669665813446045,
      "learning_rate": 4.5230035039053946e-05,
      "loss": 0.6923,
      "step": 627300
    },
    {
      "epoch": 5.724870428498431,
      "grad_norm": 4.2809858322143555,
      "learning_rate": 4.5229274642917976e-05,
      "loss": 0.7412,
      "step": 627400
    },
    {
      "epoch": 5.725782903861596,
      "grad_norm": 4.042063236236572,
      "learning_rate": 4.5228514246782006e-05,
      "loss": 0.7066,
      "step": 627500
    },
    {
      "epoch": 5.726695379224761,
      "grad_norm": 3.7039225101470947,
      "learning_rate": 4.5227753850646036e-05,
      "loss": 0.6793,
      "step": 627600
    },
    {
      "epoch": 5.7276078545879265,
      "grad_norm": 3.728667974472046,
      "learning_rate": 4.5226993454510066e-05,
      "loss": 0.68,
      "step": 627700
    },
    {
      "epoch": 5.728520329951091,
      "grad_norm": 4.316210746765137,
      "learning_rate": 4.522623305837409e-05,
      "loss": 0.6887,
      "step": 627800
    },
    {
      "epoch": 5.729432805314256,
      "grad_norm": 3.359546422958374,
      "learning_rate": 4.522547266223812e-05,
      "loss": 0.7054,
      "step": 627900
    },
    {
      "epoch": 5.7303452806774215,
      "grad_norm": 4.821741104125977,
      "learning_rate": 4.522471226610215e-05,
      "loss": 0.6901,
      "step": 628000
    },
    {
      "epoch": 5.731257756040587,
      "grad_norm": 4.4535603523254395,
      "learning_rate": 4.522395186996618e-05,
      "loss": 0.7277,
      "step": 628100
    },
    {
      "epoch": 5.732170231403752,
      "grad_norm": 3.702260971069336,
      "learning_rate": 4.52231914738302e-05,
      "loss": 0.6987,
      "step": 628200
    },
    {
      "epoch": 5.7330827067669174,
      "grad_norm": 4.958098888397217,
      "learning_rate": 4.522243107769424e-05,
      "loss": 0.7012,
      "step": 628300
    },
    {
      "epoch": 5.733995182130083,
      "grad_norm": 4.066014289855957,
      "learning_rate": 4.522167068155826e-05,
      "loss": 0.7475,
      "step": 628400
    },
    {
      "epoch": 5.734907657493248,
      "grad_norm": 3.7059061527252197,
      "learning_rate": 4.5220910285422293e-05,
      "loss": 0.7486,
      "step": 628500
    },
    {
      "epoch": 5.7358201328564125,
      "grad_norm": 4.454682350158691,
      "learning_rate": 4.5220149889286324e-05,
      "loss": 0.6801,
      "step": 628600
    },
    {
      "epoch": 5.736732608219578,
      "grad_norm": 4.057403564453125,
      "learning_rate": 4.5219389493150354e-05,
      "loss": 0.7023,
      "step": 628700
    },
    {
      "epoch": 5.737645083582743,
      "grad_norm": 3.065054416656494,
      "learning_rate": 4.5218629097014384e-05,
      "loss": 0.7162,
      "step": 628800
    },
    {
      "epoch": 5.738557558945908,
      "grad_norm": 3.795058012008667,
      "learning_rate": 4.5217868700878414e-05,
      "loss": 0.6976,
      "step": 628900
    },
    {
      "epoch": 5.739470034309074,
      "grad_norm": 4.182286262512207,
      "learning_rate": 4.521710830474244e-05,
      "loss": 0.73,
      "step": 629000
    },
    {
      "epoch": 5.740382509672239,
      "grad_norm": 4.948084354400635,
      "learning_rate": 4.5216347908606474e-05,
      "loss": 0.7213,
      "step": 629100
    },
    {
      "epoch": 5.741294985035404,
      "grad_norm": 4.242585182189941,
      "learning_rate": 4.52155875124705e-05,
      "loss": 0.7145,
      "step": 629200
    },
    {
      "epoch": 5.7422074603985696,
      "grad_norm": 4.282761573791504,
      "learning_rate": 4.521482711633453e-05,
      "loss": 0.718,
      "step": 629300
    },
    {
      "epoch": 5.743119935761735,
      "grad_norm": 3.6872754096984863,
      "learning_rate": 4.521406672019856e-05,
      "loss": 0.7206,
      "step": 629400
    },
    {
      "epoch": 5.744032411124899,
      "grad_norm": 3.737082004547119,
      "learning_rate": 4.521330632406259e-05,
      "loss": 0.7226,
      "step": 629500
    },
    {
      "epoch": 5.744944886488065,
      "grad_norm": 3.19468092918396,
      "learning_rate": 4.521254592792661e-05,
      "loss": 0.7199,
      "step": 629600
    },
    {
      "epoch": 5.74585736185123,
      "grad_norm": 4.045685291290283,
      "learning_rate": 4.521178553179065e-05,
      "loss": 0.737,
      "step": 629700
    },
    {
      "epoch": 5.746769837214395,
      "grad_norm": 5.159409999847412,
      "learning_rate": 4.521102513565467e-05,
      "loss": 0.7251,
      "step": 629800
    },
    {
      "epoch": 5.7476823125775605,
      "grad_norm": 3.645781993865967,
      "learning_rate": 4.52102647395187e-05,
      "loss": 0.6645,
      "step": 629900
    },
    {
      "epoch": 5.748594787940726,
      "grad_norm": 4.143540859222412,
      "learning_rate": 4.520950434338273e-05,
      "loss": 0.6843,
      "step": 630000
    },
    {
      "epoch": 5.749507263303891,
      "grad_norm": 3.8903462886810303,
      "learning_rate": 4.520874394724676e-05,
      "loss": 0.7067,
      "step": 630100
    },
    {
      "epoch": 5.750419738667056,
      "grad_norm": 4.1069655418396,
      "learning_rate": 4.520798355111079e-05,
      "loss": 0.7161,
      "step": 630200
    },
    {
      "epoch": 5.751332214030221,
      "grad_norm": 4.572902679443359,
      "learning_rate": 4.520722315497482e-05,
      "loss": 0.6858,
      "step": 630300
    },
    {
      "epoch": 5.752244689393386,
      "grad_norm": 3.9547598361968994,
      "learning_rate": 4.5206462758838844e-05,
      "loss": 0.6975,
      "step": 630400
    },
    {
      "epoch": 5.753157164756551,
      "grad_norm": 4.150738716125488,
      "learning_rate": 4.520570236270288e-05,
      "loss": 0.669,
      "step": 630500
    },
    {
      "epoch": 5.754069640119717,
      "grad_norm": 3.8758833408355713,
      "learning_rate": 4.5204941966566905e-05,
      "loss": 0.7353,
      "step": 630600
    },
    {
      "epoch": 5.754982115482882,
      "grad_norm": 2.5643184185028076,
      "learning_rate": 4.5204181570430935e-05,
      "loss": 0.7091,
      "step": 630700
    },
    {
      "epoch": 5.755894590846047,
      "grad_norm": 4.053369045257568,
      "learning_rate": 4.5203421174294965e-05,
      "loss": 0.6849,
      "step": 630800
    },
    {
      "epoch": 5.756807066209213,
      "grad_norm": 4.5410895347595215,
      "learning_rate": 4.520266077815899e-05,
      "loss": 0.7405,
      "step": 630900
    },
    {
      "epoch": 5.757719541572378,
      "grad_norm": 3.775200605392456,
      "learning_rate": 4.520190038202302e-05,
      "loss": 0.6958,
      "step": 631000
    },
    {
      "epoch": 5.758632016935543,
      "grad_norm": 3.5761802196502686,
      "learning_rate": 4.520113998588705e-05,
      "loss": 0.712,
      "step": 631100
    },
    {
      "epoch": 5.759544492298708,
      "grad_norm": 4.827253818511963,
      "learning_rate": 4.520037958975108e-05,
      "loss": 0.7163,
      "step": 631200
    },
    {
      "epoch": 5.760456967661873,
      "grad_norm": 4.223709583282471,
      "learning_rate": 4.519961919361511e-05,
      "loss": 0.7285,
      "step": 631300
    },
    {
      "epoch": 5.761369443025038,
      "grad_norm": 4.016237735748291,
      "learning_rate": 4.519885879747914e-05,
      "loss": 0.7392,
      "step": 631400
    },
    {
      "epoch": 5.7622819183882035,
      "grad_norm": 4.382522106170654,
      "learning_rate": 4.519809840134316e-05,
      "loss": 0.7192,
      "step": 631500
    },
    {
      "epoch": 5.763194393751369,
      "grad_norm": 3.6976943016052246,
      "learning_rate": 4.51973380052072e-05,
      "loss": 0.7405,
      "step": 631600
    },
    {
      "epoch": 5.764106869114534,
      "grad_norm": 3.923222303390503,
      "learning_rate": 4.519657760907122e-05,
      "loss": 0.7121,
      "step": 631700
    },
    {
      "epoch": 5.765019344477699,
      "grad_norm": 3.5736823081970215,
      "learning_rate": 4.519581721293525e-05,
      "loss": 0.6982,
      "step": 631800
    },
    {
      "epoch": 5.765931819840865,
      "grad_norm": 3.9814014434814453,
      "learning_rate": 4.519505681679928e-05,
      "loss": 0.7075,
      "step": 631900
    },
    {
      "epoch": 5.766844295204029,
      "grad_norm": 4.34462833404541,
      "learning_rate": 4.519429642066331e-05,
      "loss": 0.7129,
      "step": 632000
    },
    {
      "epoch": 5.767756770567194,
      "grad_norm": 5.205185413360596,
      "learning_rate": 4.519353602452734e-05,
      "loss": 0.6811,
      "step": 632100
    },
    {
      "epoch": 5.76866924593036,
      "grad_norm": 4.063653469085693,
      "learning_rate": 4.519277562839137e-05,
      "loss": 0.7292,
      "step": 632200
    },
    {
      "epoch": 5.769581721293525,
      "grad_norm": 3.903305768966675,
      "learning_rate": 4.5192015232255395e-05,
      "loss": 0.7601,
      "step": 632300
    },
    {
      "epoch": 5.77049419665669,
      "grad_norm": 3.9404122829437256,
      "learning_rate": 4.5191254836119425e-05,
      "loss": 0.6842,
      "step": 632400
    },
    {
      "epoch": 5.771406672019856,
      "grad_norm": 3.595114231109619,
      "learning_rate": 4.5190494439983455e-05,
      "loss": 0.7461,
      "step": 632500
    },
    {
      "epoch": 5.772319147383021,
      "grad_norm": 3.9004170894622803,
      "learning_rate": 4.5189734043847486e-05,
      "loss": 0.6826,
      "step": 632600
    },
    {
      "epoch": 5.773231622746186,
      "grad_norm": 2.979531764984131,
      "learning_rate": 4.5188973647711516e-05,
      "loss": 0.7329,
      "step": 632700
    },
    {
      "epoch": 5.7741440981093515,
      "grad_norm": 3.611650228500366,
      "learning_rate": 4.5188213251575546e-05,
      "loss": 0.7011,
      "step": 632800
    },
    {
      "epoch": 5.775056573472516,
      "grad_norm": 4.482967376708984,
      "learning_rate": 4.518745285543957e-05,
      "loss": 0.7213,
      "step": 632900
    },
    {
      "epoch": 5.775969048835681,
      "grad_norm": 4.428642272949219,
      "learning_rate": 4.5186692459303606e-05,
      "loss": 0.7278,
      "step": 633000
    },
    {
      "epoch": 5.7768815241988465,
      "grad_norm": 3.2174155712127686,
      "learning_rate": 4.518593206316763e-05,
      "loss": 0.7233,
      "step": 633100
    },
    {
      "epoch": 5.777793999562012,
      "grad_norm": 5.020467281341553,
      "learning_rate": 4.518517166703166e-05,
      "loss": 0.7099,
      "step": 633200
    },
    {
      "epoch": 5.778706474925177,
      "grad_norm": 3.4945626258850098,
      "learning_rate": 4.518441127089569e-05,
      "loss": 0.7016,
      "step": 633300
    },
    {
      "epoch": 5.779618950288342,
      "grad_norm": 3.9721720218658447,
      "learning_rate": 4.518365087475971e-05,
      "loss": 0.7079,
      "step": 633400
    },
    {
      "epoch": 5.780531425651508,
      "grad_norm": 4.291757583618164,
      "learning_rate": 4.518289047862375e-05,
      "loss": 0.7187,
      "step": 633500
    },
    {
      "epoch": 5.781443901014673,
      "grad_norm": 3.404372453689575,
      "learning_rate": 4.518213008248777e-05,
      "loss": 0.7321,
      "step": 633600
    },
    {
      "epoch": 5.782356376377837,
      "grad_norm": 4.45369291305542,
      "learning_rate": 4.51813696863518e-05,
      "loss": 0.7207,
      "step": 633700
    },
    {
      "epoch": 5.783268851741003,
      "grad_norm": 4.984447956085205,
      "learning_rate": 4.518060929021583e-05,
      "loss": 0.729,
      "step": 633800
    },
    {
      "epoch": 5.784181327104168,
      "grad_norm": 5.107897758483887,
      "learning_rate": 4.517984889407986e-05,
      "loss": 0.6905,
      "step": 633900
    },
    {
      "epoch": 5.785093802467333,
      "grad_norm": 3.497434377670288,
      "learning_rate": 4.5179088497943886e-05,
      "loss": 0.7157,
      "step": 634000
    },
    {
      "epoch": 5.786006277830499,
      "grad_norm": 2.786180257797241,
      "learning_rate": 4.517832810180792e-05,
      "loss": 0.6998,
      "step": 634100
    },
    {
      "epoch": 5.786918753193664,
      "grad_norm": 4.737015724182129,
      "learning_rate": 4.5177567705671946e-05,
      "loss": 0.7576,
      "step": 634200
    },
    {
      "epoch": 5.787831228556829,
      "grad_norm": 3.837176561355591,
      "learning_rate": 4.5176807309535976e-05,
      "loss": 0.7087,
      "step": 634300
    },
    {
      "epoch": 5.7887437039199945,
      "grad_norm": 3.9397664070129395,
      "learning_rate": 4.5176046913400006e-05,
      "loss": 0.6844,
      "step": 634400
    },
    {
      "epoch": 5.78965617928316,
      "grad_norm": 4.431549549102783,
      "learning_rate": 4.5175286517264036e-05,
      "loss": 0.7097,
      "step": 634500
    },
    {
      "epoch": 5.790568654646324,
      "grad_norm": 4.106360912322998,
      "learning_rate": 4.5174526121128067e-05,
      "loss": 0.7069,
      "step": 634600
    },
    {
      "epoch": 5.7914811300094895,
      "grad_norm": 3.6760590076446533,
      "learning_rate": 4.51737657249921e-05,
      "loss": 0.7291,
      "step": 634700
    },
    {
      "epoch": 5.792393605372655,
      "grad_norm": 3.3029651641845703,
      "learning_rate": 4.517300532885612e-05,
      "loss": 0.7127,
      "step": 634800
    },
    {
      "epoch": 5.79330608073582,
      "grad_norm": 4.123733043670654,
      "learning_rate": 4.517224493272016e-05,
      "loss": 0.7216,
      "step": 634900
    },
    {
      "epoch": 5.794218556098985,
      "grad_norm": 3.9727330207824707,
      "learning_rate": 4.517148453658418e-05,
      "loss": 0.7204,
      "step": 635000
    },
    {
      "epoch": 5.795131031462151,
      "grad_norm": 4.451687335968018,
      "learning_rate": 4.517072414044821e-05,
      "loss": 0.6864,
      "step": 635100
    },
    {
      "epoch": 5.796043506825316,
      "grad_norm": 3.5610828399658203,
      "learning_rate": 4.516996374431224e-05,
      "loss": 0.7086,
      "step": 635200
    },
    {
      "epoch": 5.796955982188481,
      "grad_norm": 3.770270586013794,
      "learning_rate": 4.516920334817627e-05,
      "loss": 0.728,
      "step": 635300
    },
    {
      "epoch": 5.797868457551646,
      "grad_norm": 4.4407124519348145,
      "learning_rate": 4.5168442952040294e-05,
      "loss": 0.7128,
      "step": 635400
    },
    {
      "epoch": 5.798780932914811,
      "grad_norm": 4.340844631195068,
      "learning_rate": 4.516768255590433e-05,
      "loss": 0.7115,
      "step": 635500
    },
    {
      "epoch": 5.799693408277976,
      "grad_norm": 3.9150757789611816,
      "learning_rate": 4.5166922159768354e-05,
      "loss": 0.7343,
      "step": 635600
    },
    {
      "epoch": 5.800605883641142,
      "grad_norm": 4.324230194091797,
      "learning_rate": 4.5166161763632384e-05,
      "loss": 0.6885,
      "step": 635700
    },
    {
      "epoch": 5.801518359004307,
      "grad_norm": 3.8186404705047607,
      "learning_rate": 4.5165401367496414e-05,
      "loss": 0.6469,
      "step": 635800
    },
    {
      "epoch": 5.802430834367472,
      "grad_norm": 2.9375877380371094,
      "learning_rate": 4.5164640971360444e-05,
      "loss": 0.705,
      "step": 635900
    },
    {
      "epoch": 5.8033433097306375,
      "grad_norm": 3.431105852127075,
      "learning_rate": 4.5163880575224474e-05,
      "loss": 0.7099,
      "step": 636000
    },
    {
      "epoch": 5.804255785093803,
      "grad_norm": 4.189877986907959,
      "learning_rate": 4.5163120179088504e-05,
      "loss": 0.7059,
      "step": 636100
    },
    {
      "epoch": 5.805168260456968,
      "grad_norm": 3.6476943492889404,
      "learning_rate": 4.516235978295253e-05,
      "loss": 0.662,
      "step": 636200
    },
    {
      "epoch": 5.8060807358201325,
      "grad_norm": 3.6535568237304688,
      "learning_rate": 4.516159938681656e-05,
      "loss": 0.7016,
      "step": 636300
    },
    {
      "epoch": 5.806993211183298,
      "grad_norm": 3.963336229324341,
      "learning_rate": 4.516083899068059e-05,
      "loss": 0.7339,
      "step": 636400
    },
    {
      "epoch": 5.807905686546463,
      "grad_norm": 3.1404995918273926,
      "learning_rate": 4.516007859454461e-05,
      "loss": 0.7195,
      "step": 636500
    },
    {
      "epoch": 5.808818161909628,
      "grad_norm": 4.086606025695801,
      "learning_rate": 4.515931819840865e-05,
      "loss": 0.7074,
      "step": 636600
    },
    {
      "epoch": 5.809730637272794,
      "grad_norm": 4.493069648742676,
      "learning_rate": 4.515855780227267e-05,
      "loss": 0.7183,
      "step": 636700
    },
    {
      "epoch": 5.810643112635959,
      "grad_norm": 3.5181143283843994,
      "learning_rate": 4.51577974061367e-05,
      "loss": 0.6975,
      "step": 636800
    },
    {
      "epoch": 5.811555587999124,
      "grad_norm": 2.817087411880493,
      "learning_rate": 4.515703701000073e-05,
      "loss": 0.689,
      "step": 636900
    },
    {
      "epoch": 5.81246806336229,
      "grad_norm": 3.4661357402801514,
      "learning_rate": 4.515627661386476e-05,
      "loss": 0.7136,
      "step": 637000
    },
    {
      "epoch": 5.813380538725454,
      "grad_norm": 4.872366905212402,
      "learning_rate": 4.515551621772879e-05,
      "loss": 0.717,
      "step": 637100
    },
    {
      "epoch": 5.814293014088619,
      "grad_norm": 4.146517276763916,
      "learning_rate": 4.515475582159282e-05,
      "loss": 0.7786,
      "step": 637200
    },
    {
      "epoch": 5.815205489451785,
      "grad_norm": 6.737903594970703,
      "learning_rate": 4.5153995425456844e-05,
      "loss": 0.6952,
      "step": 637300
    },
    {
      "epoch": 5.81611796481495,
      "grad_norm": 4.050350189208984,
      "learning_rate": 4.515323502932088e-05,
      "loss": 0.7064,
      "step": 637400
    },
    {
      "epoch": 5.817030440178115,
      "grad_norm": 3.595327615737915,
      "learning_rate": 4.5152474633184905e-05,
      "loss": 0.7001,
      "step": 637500
    },
    {
      "epoch": 5.8179429155412805,
      "grad_norm": 4.015720367431641,
      "learning_rate": 4.5151714237048935e-05,
      "loss": 0.7009,
      "step": 637600
    },
    {
      "epoch": 5.818855390904446,
      "grad_norm": 3.5225508213043213,
      "learning_rate": 4.5150953840912965e-05,
      "loss": 0.74,
      "step": 637700
    },
    {
      "epoch": 5.819767866267611,
      "grad_norm": 4.6375908851623535,
      "learning_rate": 4.5150193444776995e-05,
      "loss": 0.7258,
      "step": 637800
    },
    {
      "epoch": 5.820680341630776,
      "grad_norm": 2.222691535949707,
      "learning_rate": 4.514943304864102e-05,
      "loss": 0.7282,
      "step": 637900
    },
    {
      "epoch": 5.821592816993941,
      "grad_norm": 3.8551104068756104,
      "learning_rate": 4.5148672652505055e-05,
      "loss": 0.7184,
      "step": 638000
    },
    {
      "epoch": 5.822505292357106,
      "grad_norm": 3.2167904376983643,
      "learning_rate": 4.514791225636908e-05,
      "loss": 0.6966,
      "step": 638100
    },
    {
      "epoch": 5.823417767720271,
      "grad_norm": 5.1817803382873535,
      "learning_rate": 4.514715186023311e-05,
      "loss": 0.741,
      "step": 638200
    },
    {
      "epoch": 5.824330243083437,
      "grad_norm": 4.594319820404053,
      "learning_rate": 4.514639146409714e-05,
      "loss": 0.6802,
      "step": 638300
    },
    {
      "epoch": 5.825242718446602,
      "grad_norm": 3.9494950771331787,
      "learning_rate": 4.514563106796117e-05,
      "loss": 0.6836,
      "step": 638400
    },
    {
      "epoch": 5.826155193809767,
      "grad_norm": 4.597925186157227,
      "learning_rate": 4.51448706718252e-05,
      "loss": 0.7362,
      "step": 638500
    },
    {
      "epoch": 5.827067669172933,
      "grad_norm": 3.498577833175659,
      "learning_rate": 4.514411027568923e-05,
      "loss": 0.6868,
      "step": 638600
    },
    {
      "epoch": 5.827980144536097,
      "grad_norm": 4.318527698516846,
      "learning_rate": 4.514334987955325e-05,
      "loss": 0.7169,
      "step": 638700
    },
    {
      "epoch": 5.828892619899262,
      "grad_norm": 3.9103736877441406,
      "learning_rate": 4.514258948341729e-05,
      "loss": 0.6921,
      "step": 638800
    },
    {
      "epoch": 5.829805095262428,
      "grad_norm": 4.305480003356934,
      "learning_rate": 4.514182908728131e-05,
      "loss": 0.7129,
      "step": 638900
    },
    {
      "epoch": 5.830717570625593,
      "grad_norm": 3.26253342628479,
      "learning_rate": 4.514106869114534e-05,
      "loss": 0.7034,
      "step": 639000
    },
    {
      "epoch": 5.831630045988758,
      "grad_norm": 3.886066436767578,
      "learning_rate": 4.514030829500937e-05,
      "loss": 0.6963,
      "step": 639100
    },
    {
      "epoch": 5.8325425213519235,
      "grad_norm": 4.0704474449157715,
      "learning_rate": 4.5139547898873395e-05,
      "loss": 0.6697,
      "step": 639200
    },
    {
      "epoch": 5.833454996715089,
      "grad_norm": 3.282294988632202,
      "learning_rate": 4.5138787502737426e-05,
      "loss": 0.7075,
      "step": 639300
    },
    {
      "epoch": 5.834367472078254,
      "grad_norm": 3.9784369468688965,
      "learning_rate": 4.5138027106601456e-05,
      "loss": 0.716,
      "step": 639400
    },
    {
      "epoch": 5.835279947441419,
      "grad_norm": 3.9394290447235107,
      "learning_rate": 4.5137266710465486e-05,
      "loss": 0.7098,
      "step": 639500
    },
    {
      "epoch": 5.836192422804585,
      "grad_norm": 3.0493388175964355,
      "learning_rate": 4.5136506314329516e-05,
      "loss": 0.7045,
      "step": 639600
    },
    {
      "epoch": 5.837104898167749,
      "grad_norm": 4.9796319007873535,
      "learning_rate": 4.5135745918193546e-05,
      "loss": 0.7213,
      "step": 639700
    },
    {
      "epoch": 5.838017373530914,
      "grad_norm": 4.706948280334473,
      "learning_rate": 4.513498552205757e-05,
      "loss": 0.6934,
      "step": 639800
    },
    {
      "epoch": 5.83892984889408,
      "grad_norm": 3.8728740215301514,
      "learning_rate": 4.5134225125921606e-05,
      "loss": 0.6888,
      "step": 639900
    },
    {
      "epoch": 5.839842324257245,
      "grad_norm": 4.4040093421936035,
      "learning_rate": 4.513346472978563e-05,
      "loss": 0.693,
      "step": 640000
    },
    {
      "epoch": 5.84075479962041,
      "grad_norm": 4.350600719451904,
      "learning_rate": 4.513270433364966e-05,
      "loss": 0.6962,
      "step": 640100
    },
    {
      "epoch": 5.841667274983576,
      "grad_norm": 3.656294822692871,
      "learning_rate": 4.513194393751369e-05,
      "loss": 0.721,
      "step": 640200
    },
    {
      "epoch": 5.842579750346741,
      "grad_norm": 4.151819229125977,
      "learning_rate": 4.513118354137772e-05,
      "loss": 0.6899,
      "step": 640300
    },
    {
      "epoch": 5.843492225709905,
      "grad_norm": 3.9801418781280518,
      "learning_rate": 4.513042314524174e-05,
      "loss": 0.7006,
      "step": 640400
    },
    {
      "epoch": 5.844404701073071,
      "grad_norm": 3.786748170852661,
      "learning_rate": 4.512966274910578e-05,
      "loss": 0.7287,
      "step": 640500
    },
    {
      "epoch": 5.845317176436236,
      "grad_norm": 2.8772828578948975,
      "learning_rate": 4.51289023529698e-05,
      "loss": 0.7222,
      "step": 640600
    },
    {
      "epoch": 5.846229651799401,
      "grad_norm": 5.02664852142334,
      "learning_rate": 4.512814195683383e-05,
      "loss": 0.7312,
      "step": 640700
    },
    {
      "epoch": 5.8471421271625665,
      "grad_norm": 3.864832878112793,
      "learning_rate": 4.512738156069786e-05,
      "loss": 0.691,
      "step": 640800
    },
    {
      "epoch": 5.848054602525732,
      "grad_norm": 4.0499267578125,
      "learning_rate": 4.512662116456189e-05,
      "loss": 0.7403,
      "step": 640900
    },
    {
      "epoch": 5.848967077888897,
      "grad_norm": 4.747018337249756,
      "learning_rate": 4.512586076842592e-05,
      "loss": 0.771,
      "step": 641000
    },
    {
      "epoch": 5.849879553252062,
      "grad_norm": 5.068048000335693,
      "learning_rate": 4.512510037228995e-05,
      "loss": 0.6958,
      "step": 641100
    },
    {
      "epoch": 5.850792028615228,
      "grad_norm": 3.451937675476074,
      "learning_rate": 4.5124339976153976e-05,
      "loss": 0.7289,
      "step": 641200
    },
    {
      "epoch": 5.851704503978393,
      "grad_norm": 3.1804299354553223,
      "learning_rate": 4.512357958001801e-05,
      "loss": 0.7055,
      "step": 641300
    },
    {
      "epoch": 5.8526169793415574,
      "grad_norm": 3.80085825920105,
      "learning_rate": 4.5122819183882037e-05,
      "loss": 0.7294,
      "step": 641400
    },
    {
      "epoch": 5.853529454704723,
      "grad_norm": 2.394021511077881,
      "learning_rate": 4.512205878774607e-05,
      "loss": 0.6778,
      "step": 641500
    },
    {
      "epoch": 5.854441930067888,
      "grad_norm": 2.9655544757843018,
      "learning_rate": 4.51212983916101e-05,
      "loss": 0.6738,
      "step": 641600
    },
    {
      "epoch": 5.855354405431053,
      "grad_norm": 4.709043502807617,
      "learning_rate": 4.512053799547413e-05,
      "loss": 0.6778,
      "step": 641700
    },
    {
      "epoch": 5.856266880794219,
      "grad_norm": 3.6440024375915527,
      "learning_rate": 4.511977759933815e-05,
      "loss": 0.6986,
      "step": 641800
    },
    {
      "epoch": 5.857179356157384,
      "grad_norm": 4.1992316246032715,
      "learning_rate": 4.511901720320218e-05,
      "loss": 0.7408,
      "step": 641900
    },
    {
      "epoch": 5.858091831520549,
      "grad_norm": 4.03857946395874,
      "learning_rate": 4.511825680706621e-05,
      "loss": 0.7128,
      "step": 642000
    },
    {
      "epoch": 5.859004306883714,
      "grad_norm": 4.126353740692139,
      "learning_rate": 4.511749641093024e-05,
      "loss": 0.6753,
      "step": 642100
    },
    {
      "epoch": 5.859916782246879,
      "grad_norm": 4.58637809753418,
      "learning_rate": 4.511673601479427e-05,
      "loss": 0.7158,
      "step": 642200
    },
    {
      "epoch": 5.860829257610044,
      "grad_norm": 4.50922966003418,
      "learning_rate": 4.5115975618658294e-05,
      "loss": 0.6798,
      "step": 642300
    },
    {
      "epoch": 5.8617417329732096,
      "grad_norm": 3.831784963607788,
      "learning_rate": 4.511521522252233e-05,
      "loss": 0.7196,
      "step": 642400
    },
    {
      "epoch": 5.862654208336375,
      "grad_norm": 4.82299280166626,
      "learning_rate": 4.5114454826386354e-05,
      "loss": 0.7444,
      "step": 642500
    },
    {
      "epoch": 5.86356668369954,
      "grad_norm": 4.098358154296875,
      "learning_rate": 4.5113694430250384e-05,
      "loss": 0.6984,
      "step": 642600
    },
    {
      "epoch": 5.8644791590627054,
      "grad_norm": 4.118490219116211,
      "learning_rate": 4.5112934034114414e-05,
      "loss": 0.6857,
      "step": 642700
    },
    {
      "epoch": 5.865391634425871,
      "grad_norm": 4.2081298828125,
      "learning_rate": 4.5112173637978444e-05,
      "loss": 0.6976,
      "step": 642800
    },
    {
      "epoch": 5.866304109789036,
      "grad_norm": 4.034109115600586,
      "learning_rate": 4.511141324184247e-05,
      "loss": 0.7043,
      "step": 642900
    },
    {
      "epoch": 5.867216585152201,
      "grad_norm": 4.071435451507568,
      "learning_rate": 4.5110652845706504e-05,
      "loss": 0.7415,
      "step": 643000
    },
    {
      "epoch": 5.868129060515366,
      "grad_norm": 5.408768653869629,
      "learning_rate": 4.510989244957053e-05,
      "loss": 0.702,
      "step": 643100
    },
    {
      "epoch": 5.869041535878531,
      "grad_norm": 3.925532579421997,
      "learning_rate": 4.510913205343456e-05,
      "loss": 0.7045,
      "step": 643200
    },
    {
      "epoch": 5.869954011241696,
      "grad_norm": 4.269707202911377,
      "learning_rate": 4.510837165729859e-05,
      "loss": 0.6887,
      "step": 643300
    },
    {
      "epoch": 5.870866486604862,
      "grad_norm": 4.657344818115234,
      "learning_rate": 4.510761126116262e-05,
      "loss": 0.7078,
      "step": 643400
    },
    {
      "epoch": 5.871778961968027,
      "grad_norm": 3.886918544769287,
      "learning_rate": 4.510685086502665e-05,
      "loss": 0.7026,
      "step": 643500
    },
    {
      "epoch": 5.872691437331192,
      "grad_norm": 4.404752254486084,
      "learning_rate": 4.510609046889068e-05,
      "loss": 0.7039,
      "step": 643600
    },
    {
      "epoch": 5.8736039126943576,
      "grad_norm": 3.6790976524353027,
      "learning_rate": 4.51053300727547e-05,
      "loss": 0.6905,
      "step": 643700
    },
    {
      "epoch": 5.874516388057522,
      "grad_norm": 3.079735517501831,
      "learning_rate": 4.510456967661874e-05,
      "loss": 0.7208,
      "step": 643800
    },
    {
      "epoch": 5.875428863420687,
      "grad_norm": 4.1439619064331055,
      "learning_rate": 4.510380928048276e-05,
      "loss": 0.6453,
      "step": 643900
    },
    {
      "epoch": 5.876341338783853,
      "grad_norm": 3.355517864227295,
      "learning_rate": 4.510304888434679e-05,
      "loss": 0.7104,
      "step": 644000
    },
    {
      "epoch": 5.877253814147018,
      "grad_norm": 3.5205748081207275,
      "learning_rate": 4.510228848821082e-05,
      "loss": 0.6967,
      "step": 644100
    },
    {
      "epoch": 5.878166289510183,
      "grad_norm": 3.9368972778320312,
      "learning_rate": 4.510152809207485e-05,
      "loss": 0.7639,
      "step": 644200
    },
    {
      "epoch": 5.8790787648733485,
      "grad_norm": 4.168262958526611,
      "learning_rate": 4.510076769593888e-05,
      "loss": 0.6715,
      "step": 644300
    },
    {
      "epoch": 5.879991240236514,
      "grad_norm": 3.939626932144165,
      "learning_rate": 4.510000729980291e-05,
      "loss": 0.6608,
      "step": 644400
    },
    {
      "epoch": 5.880903715599679,
      "grad_norm": 4.248400688171387,
      "learning_rate": 4.5099246903666935e-05,
      "loss": 0.7179,
      "step": 644500
    },
    {
      "epoch": 5.881816190962844,
      "grad_norm": 3.8784143924713135,
      "learning_rate": 4.5098486507530965e-05,
      "loss": 0.6897,
      "step": 644600
    },
    {
      "epoch": 5.88272866632601,
      "grad_norm": 4.9606499671936035,
      "learning_rate": 4.5097726111394995e-05,
      "loss": 0.6894,
      "step": 644700
    },
    {
      "epoch": 5.883641141689174,
      "grad_norm": 4.05197811126709,
      "learning_rate": 4.509696571525902e-05,
      "loss": 0.6951,
      "step": 644800
    },
    {
      "epoch": 5.884553617052339,
      "grad_norm": 4.751295566558838,
      "learning_rate": 4.5096205319123055e-05,
      "loss": 0.7047,
      "step": 644900
    },
    {
      "epoch": 5.885466092415505,
      "grad_norm": 4.872359275817871,
      "learning_rate": 4.509544492298708e-05,
      "loss": 0.701,
      "step": 645000
    },
    {
      "epoch": 5.88637856777867,
      "grad_norm": 3.9854345321655273,
      "learning_rate": 4.509468452685111e-05,
      "loss": 0.6933,
      "step": 645100
    },
    {
      "epoch": 5.887291043141835,
      "grad_norm": 4.151093482971191,
      "learning_rate": 4.509392413071514e-05,
      "loss": 0.6688,
      "step": 645200
    },
    {
      "epoch": 5.888203518505001,
      "grad_norm": 3.227677583694458,
      "learning_rate": 4.509316373457917e-05,
      "loss": 0.6976,
      "step": 645300
    },
    {
      "epoch": 5.889115993868166,
      "grad_norm": 3.828230857849121,
      "learning_rate": 4.50924033384432e-05,
      "loss": 0.6935,
      "step": 645400
    },
    {
      "epoch": 5.89002846923133,
      "grad_norm": 3.7528765201568604,
      "learning_rate": 4.509164294230723e-05,
      "loss": 0.7494,
      "step": 645500
    },
    {
      "epoch": 5.890940944594496,
      "grad_norm": 3.5217556953430176,
      "learning_rate": 4.509088254617125e-05,
      "loss": 0.717,
      "step": 645600
    },
    {
      "epoch": 5.891853419957661,
      "grad_norm": 4.602451801300049,
      "learning_rate": 4.509012215003529e-05,
      "loss": 0.6931,
      "step": 645700
    },
    {
      "epoch": 5.892765895320826,
      "grad_norm": 3.0352280139923096,
      "learning_rate": 4.508936175389931e-05,
      "loss": 0.7078,
      "step": 645800
    },
    {
      "epoch": 5.8936783706839915,
      "grad_norm": 4.492842674255371,
      "learning_rate": 4.508860135776334e-05,
      "loss": 0.7179,
      "step": 645900
    },
    {
      "epoch": 5.894590846047157,
      "grad_norm": 4.684051036834717,
      "learning_rate": 4.508784096162737e-05,
      "loss": 0.6946,
      "step": 646000
    },
    {
      "epoch": 5.895503321410322,
      "grad_norm": 4.126565933227539,
      "learning_rate": 4.50870805654914e-05,
      "loss": 0.7215,
      "step": 646100
    },
    {
      "epoch": 5.896415796773487,
      "grad_norm": 3.9941487312316895,
      "learning_rate": 4.5086320169355426e-05,
      "loss": 0.718,
      "step": 646200
    },
    {
      "epoch": 5.897328272136653,
      "grad_norm": 5.586701393127441,
      "learning_rate": 4.508555977321946e-05,
      "loss": 0.7061,
      "step": 646300
    },
    {
      "epoch": 5.898240747499818,
      "grad_norm": 3.9510130882263184,
      "learning_rate": 4.5084799377083486e-05,
      "loss": 0.706,
      "step": 646400
    },
    {
      "epoch": 5.899153222862982,
      "grad_norm": 3.7619411945343018,
      "learning_rate": 4.5084038980947516e-05,
      "loss": 0.7019,
      "step": 646500
    },
    {
      "epoch": 5.900065698226148,
      "grad_norm": 4.290578842163086,
      "learning_rate": 4.5083278584811546e-05,
      "loss": 0.7194,
      "step": 646600
    },
    {
      "epoch": 5.900978173589313,
      "grad_norm": 3.835458278656006,
      "learning_rate": 4.5082518188675576e-05,
      "loss": 0.715,
      "step": 646700
    },
    {
      "epoch": 5.901890648952478,
      "grad_norm": 4.063616752624512,
      "learning_rate": 4.5081757792539606e-05,
      "loss": 0.7012,
      "step": 646800
    },
    {
      "epoch": 5.902803124315644,
      "grad_norm": 3.8508665561676025,
      "learning_rate": 4.5080997396403636e-05,
      "loss": 0.7032,
      "step": 646900
    },
    {
      "epoch": 5.903715599678809,
      "grad_norm": 3.847777843475342,
      "learning_rate": 4.508023700026766e-05,
      "loss": 0.717,
      "step": 647000
    },
    {
      "epoch": 5.904628075041974,
      "grad_norm": 4.718547821044922,
      "learning_rate": 4.5079476604131696e-05,
      "loss": 0.6734,
      "step": 647100
    },
    {
      "epoch": 5.905540550405139,
      "grad_norm": 3.8734490871429443,
      "learning_rate": 4.507871620799572e-05,
      "loss": 0.7274,
      "step": 647200
    },
    {
      "epoch": 5.906453025768304,
      "grad_norm": 3.7980775833129883,
      "learning_rate": 4.507795581185975e-05,
      "loss": 0.7002,
      "step": 647300
    },
    {
      "epoch": 5.907365501131469,
      "grad_norm": 4.205161094665527,
      "learning_rate": 4.507719541572378e-05,
      "loss": 0.7332,
      "step": 647400
    },
    {
      "epoch": 5.9082779764946345,
      "grad_norm": 3.348069429397583,
      "learning_rate": 4.507643501958781e-05,
      "loss": 0.6956,
      "step": 647500
    },
    {
      "epoch": 5.9091904518578,
      "grad_norm": 2.9662439823150635,
      "learning_rate": 4.507567462345183e-05,
      "loss": 0.672,
      "step": 647600
    },
    {
      "epoch": 5.910102927220965,
      "grad_norm": 3.769141674041748,
      "learning_rate": 4.507491422731586e-05,
      "loss": 0.7121,
      "step": 647700
    },
    {
      "epoch": 5.91101540258413,
      "grad_norm": 3.9051103591918945,
      "learning_rate": 4.507415383117989e-05,
      "loss": 0.6853,
      "step": 647800
    },
    {
      "epoch": 5.911927877947296,
      "grad_norm": 4.541888236999512,
      "learning_rate": 4.507339343504392e-05,
      "loss": 0.7486,
      "step": 647900
    },
    {
      "epoch": 5.912840353310461,
      "grad_norm": 5.020726203918457,
      "learning_rate": 4.507263303890795e-05,
      "loss": 0.6983,
      "step": 648000
    },
    {
      "epoch": 5.913752828673626,
      "grad_norm": 4.62569522857666,
      "learning_rate": 4.5071872642771977e-05,
      "loss": 0.7048,
      "step": 648100
    },
    {
      "epoch": 5.914665304036791,
      "grad_norm": 4.225620269775391,
      "learning_rate": 4.5071112246636013e-05,
      "loss": 0.6885,
      "step": 648200
    },
    {
      "epoch": 5.915577779399956,
      "grad_norm": 3.6214990615844727,
      "learning_rate": 4.507035185050004e-05,
      "loss": 0.7014,
      "step": 648300
    },
    {
      "epoch": 5.916490254763121,
      "grad_norm": 4.03452205657959,
      "learning_rate": 4.506959145436407e-05,
      "loss": 0.688,
      "step": 648400
    },
    {
      "epoch": 5.917402730126287,
      "grad_norm": 3.7022664546966553,
      "learning_rate": 4.50688310582281e-05,
      "loss": 0.7323,
      "step": 648500
    },
    {
      "epoch": 5.918315205489452,
      "grad_norm": 4.422335624694824,
      "learning_rate": 4.506807066209213e-05,
      "loss": 0.6882,
      "step": 648600
    },
    {
      "epoch": 5.919227680852617,
      "grad_norm": 3.8599867820739746,
      "learning_rate": 4.506731026595615e-05,
      "loss": 0.679,
      "step": 648700
    },
    {
      "epoch": 5.9201401562157825,
      "grad_norm": 3.6046364307403564,
      "learning_rate": 4.506654986982019e-05,
      "loss": 0.6906,
      "step": 648800
    },
    {
      "epoch": 5.921052631578947,
      "grad_norm": 3.9705111980438232,
      "learning_rate": 4.506578947368421e-05,
      "loss": 0.7354,
      "step": 648900
    },
    {
      "epoch": 5.921965106942112,
      "grad_norm": 3.9013359546661377,
      "learning_rate": 4.506502907754824e-05,
      "loss": 0.7182,
      "step": 649000
    },
    {
      "epoch": 5.9228775823052775,
      "grad_norm": 4.412858009338379,
      "learning_rate": 4.506426868141227e-05,
      "loss": 0.7087,
      "step": 649100
    },
    {
      "epoch": 5.923790057668443,
      "grad_norm": 3.7135133743286133,
      "learning_rate": 4.50635082852763e-05,
      "loss": 0.6834,
      "step": 649200
    },
    {
      "epoch": 5.924702533031608,
      "grad_norm": 2.8973495960235596,
      "learning_rate": 4.506274788914033e-05,
      "loss": 0.7166,
      "step": 649300
    },
    {
      "epoch": 5.925615008394773,
      "grad_norm": 3.387843132019043,
      "learning_rate": 4.506198749300436e-05,
      "loss": 0.7128,
      "step": 649400
    },
    {
      "epoch": 5.926527483757939,
      "grad_norm": 4.510059356689453,
      "learning_rate": 4.5061227096868384e-05,
      "loss": 0.7253,
      "step": 649500
    },
    {
      "epoch": 5.927439959121104,
      "grad_norm": 4.765639305114746,
      "learning_rate": 4.506046670073242e-05,
      "loss": 0.7797,
      "step": 649600
    },
    {
      "epoch": 5.928352434484269,
      "grad_norm": 4.025637626647949,
      "learning_rate": 4.5059706304596444e-05,
      "loss": 0.6804,
      "step": 649700
    },
    {
      "epoch": 5.929264909847435,
      "grad_norm": 4.290301322937012,
      "learning_rate": 4.5058945908460474e-05,
      "loss": 0.7208,
      "step": 649800
    },
    {
      "epoch": 5.930177385210599,
      "grad_norm": 4.345695495605469,
      "learning_rate": 4.5058185512324504e-05,
      "loss": 0.6959,
      "step": 649900
    },
    {
      "epoch": 5.931089860573764,
      "grad_norm": 3.4124417304992676,
      "learning_rate": 4.5057425116188534e-05,
      "loss": 0.7172,
      "step": 650000
    },
    {
      "epoch": 5.93200233593693,
      "grad_norm": 5.405890941619873,
      "learning_rate": 4.505666472005256e-05,
      "loss": 0.6648,
      "step": 650100
    },
    {
      "epoch": 5.932914811300095,
      "grad_norm": 5.420505046844482,
      "learning_rate": 4.5055904323916594e-05,
      "loss": 0.7166,
      "step": 650200
    },
    {
      "epoch": 5.93382728666326,
      "grad_norm": 4.814931869506836,
      "learning_rate": 4.505514392778062e-05,
      "loss": 0.7282,
      "step": 650300
    },
    {
      "epoch": 5.9347397620264255,
      "grad_norm": 4.307559967041016,
      "learning_rate": 4.505438353164465e-05,
      "loss": 0.6885,
      "step": 650400
    },
    {
      "epoch": 5.935652237389591,
      "grad_norm": 3.897784471511841,
      "learning_rate": 4.505362313550868e-05,
      "loss": 0.6713,
      "step": 650500
    },
    {
      "epoch": 5.936564712752755,
      "grad_norm": 4.025582313537598,
      "learning_rate": 4.50528627393727e-05,
      "loss": 0.7302,
      "step": 650600
    },
    {
      "epoch": 5.9374771881159205,
      "grad_norm": 4.224391937255859,
      "learning_rate": 4.505210234323674e-05,
      "loss": 0.7231,
      "step": 650700
    },
    {
      "epoch": 5.938389663479086,
      "grad_norm": 3.9779958724975586,
      "learning_rate": 4.505134194710076e-05,
      "loss": 0.7538,
      "step": 650800
    },
    {
      "epoch": 5.939302138842251,
      "grad_norm": 4.959900856018066,
      "learning_rate": 4.505058155096479e-05,
      "loss": 0.736,
      "step": 650900
    },
    {
      "epoch": 5.940214614205416,
      "grad_norm": 4.3021392822265625,
      "learning_rate": 4.504982115482882e-05,
      "loss": 0.6926,
      "step": 651000
    },
    {
      "epoch": 5.941127089568582,
      "grad_norm": 3.994959831237793,
      "learning_rate": 4.504906075869285e-05,
      "loss": 0.7504,
      "step": 651100
    },
    {
      "epoch": 5.942039564931747,
      "grad_norm": 3.580780506134033,
      "learning_rate": 4.5048300362556875e-05,
      "loss": 0.7113,
      "step": 651200
    },
    {
      "epoch": 5.942952040294912,
      "grad_norm": 3.966596841812134,
      "learning_rate": 4.504753996642091e-05,
      "loss": 0.6882,
      "step": 651300
    },
    {
      "epoch": 5.943864515658078,
      "grad_norm": 4.324662685394287,
      "learning_rate": 4.5046779570284935e-05,
      "loss": 0.6927,
      "step": 651400
    },
    {
      "epoch": 5.944776991021243,
      "grad_norm": 4.481501579284668,
      "learning_rate": 4.5046019174148965e-05,
      "loss": 0.6962,
      "step": 651500
    },
    {
      "epoch": 5.945689466384407,
      "grad_norm": 3.4542088508605957,
      "learning_rate": 4.5045258778012995e-05,
      "loss": 0.7227,
      "step": 651600
    },
    {
      "epoch": 5.946601941747573,
      "grad_norm": 4.41151237487793,
      "learning_rate": 4.5044498381877025e-05,
      "loss": 0.7814,
      "step": 651700
    },
    {
      "epoch": 5.947514417110738,
      "grad_norm": 3.837771415710449,
      "learning_rate": 4.5043737985741055e-05,
      "loss": 0.7128,
      "step": 651800
    },
    {
      "epoch": 5.948426892473903,
      "grad_norm": 4.48085355758667,
      "learning_rate": 4.5042977589605085e-05,
      "loss": 0.7138,
      "step": 651900
    },
    {
      "epoch": 5.9493393678370685,
      "grad_norm": 3.5802054405212402,
      "learning_rate": 4.504221719346911e-05,
      "loss": 0.6921,
      "step": 652000
    },
    {
      "epoch": 5.950251843200234,
      "grad_norm": 3.8830010890960693,
      "learning_rate": 4.5041456797333145e-05,
      "loss": 0.7247,
      "step": 652100
    },
    {
      "epoch": 5.951164318563399,
      "grad_norm": 4.920314311981201,
      "learning_rate": 4.504069640119717e-05,
      "loss": 0.6957,
      "step": 652200
    },
    {
      "epoch": 5.9520767939265635,
      "grad_norm": 4.614048957824707,
      "learning_rate": 4.50399360050612e-05,
      "loss": 0.7024,
      "step": 652300
    },
    {
      "epoch": 5.952989269289729,
      "grad_norm": 4.643592834472656,
      "learning_rate": 4.503917560892523e-05,
      "loss": 0.7636,
      "step": 652400
    },
    {
      "epoch": 5.953901744652894,
      "grad_norm": 4.938957214355469,
      "learning_rate": 4.503841521278926e-05,
      "loss": 0.7489,
      "step": 652500
    },
    {
      "epoch": 5.954814220016059,
      "grad_norm": 5.008144378662109,
      "learning_rate": 4.503765481665328e-05,
      "loss": 0.7261,
      "step": 652600
    },
    {
      "epoch": 5.955726695379225,
      "grad_norm": 4.368013858795166,
      "learning_rate": 4.503689442051732e-05,
      "loss": 0.6882,
      "step": 652700
    },
    {
      "epoch": 5.95663917074239,
      "grad_norm": 4.494246959686279,
      "learning_rate": 4.503613402438134e-05,
      "loss": 0.748,
      "step": 652800
    },
    {
      "epoch": 5.957551646105555,
      "grad_norm": 3.944718837738037,
      "learning_rate": 4.503537362824537e-05,
      "loss": 0.7455,
      "step": 652900
    },
    {
      "epoch": 5.958464121468721,
      "grad_norm": 3.321545362472534,
      "learning_rate": 4.50346132321094e-05,
      "loss": 0.7161,
      "step": 653000
    },
    {
      "epoch": 5.959376596831886,
      "grad_norm": 4.039681434631348,
      "learning_rate": 4.503385283597343e-05,
      "loss": 0.7488,
      "step": 653100
    },
    {
      "epoch": 5.960289072195051,
      "grad_norm": 3.0901033878326416,
      "learning_rate": 4.503309243983746e-05,
      "loss": 0.7246,
      "step": 653200
    },
    {
      "epoch": 5.961201547558216,
      "grad_norm": 3.8929436206817627,
      "learning_rate": 4.5032332043701486e-05,
      "loss": 0.717,
      "step": 653300
    },
    {
      "epoch": 5.962114022921381,
      "grad_norm": 3.8088672161102295,
      "learning_rate": 4.5031571647565516e-05,
      "loss": 0.6919,
      "step": 653400
    },
    {
      "epoch": 5.963026498284546,
      "grad_norm": 4.115588188171387,
      "learning_rate": 4.5030811251429546e-05,
      "loss": 0.6837,
      "step": 653500
    },
    {
      "epoch": 5.9639389736477115,
      "grad_norm": 4.305291175842285,
      "learning_rate": 4.5030050855293576e-05,
      "loss": 0.7217,
      "step": 653600
    },
    {
      "epoch": 5.964851449010877,
      "grad_norm": 3.775848150253296,
      "learning_rate": 4.50292904591576e-05,
      "loss": 0.7252,
      "step": 653700
    },
    {
      "epoch": 5.965763924374042,
      "grad_norm": 4.690791606903076,
      "learning_rate": 4.5028530063021636e-05,
      "loss": 0.6824,
      "step": 653800
    },
    {
      "epoch": 5.966676399737207,
      "grad_norm": 4.126237392425537,
      "learning_rate": 4.502776966688566e-05,
      "loss": 0.6821,
      "step": 653900
    },
    {
      "epoch": 5.967588875100372,
      "grad_norm": 4.62702751159668,
      "learning_rate": 4.502700927074969e-05,
      "loss": 0.714,
      "step": 654000
    },
    {
      "epoch": 5.968501350463537,
      "grad_norm": 3.5706870555877686,
      "learning_rate": 4.502624887461372e-05,
      "loss": 0.6975,
      "step": 654100
    },
    {
      "epoch": 5.969413825826702,
      "grad_norm": 4.209135055541992,
      "learning_rate": 4.502548847847775e-05,
      "loss": 0.7134,
      "step": 654200
    },
    {
      "epoch": 5.970326301189868,
      "grad_norm": 3.395967483520508,
      "learning_rate": 4.502472808234178e-05,
      "loss": 0.7036,
      "step": 654300
    },
    {
      "epoch": 5.971238776553033,
      "grad_norm": 4.901859283447266,
      "learning_rate": 4.502396768620581e-05,
      "loss": 0.7304,
      "step": 654400
    },
    {
      "epoch": 5.972151251916198,
      "grad_norm": 4.528417110443115,
      "learning_rate": 4.502320729006983e-05,
      "loss": 0.7345,
      "step": 654500
    },
    {
      "epoch": 5.973063727279364,
      "grad_norm": 3.4251792430877686,
      "learning_rate": 4.502244689393387e-05,
      "loss": 0.7317,
      "step": 654600
    },
    {
      "epoch": 5.973976202642529,
      "grad_norm": 3.5474424362182617,
      "learning_rate": 4.502168649779789e-05,
      "loss": 0.755,
      "step": 654700
    },
    {
      "epoch": 5.974888678005694,
      "grad_norm": 4.031939506530762,
      "learning_rate": 4.502092610166192e-05,
      "loss": 0.6855,
      "step": 654800
    },
    {
      "epoch": 5.9758011533688595,
      "grad_norm": 3.6332454681396484,
      "learning_rate": 4.502016570552595e-05,
      "loss": 0.7351,
      "step": 654900
    },
    {
      "epoch": 5.976713628732024,
      "grad_norm": 4.314332008361816,
      "learning_rate": 4.5019405309389983e-05,
      "loss": 0.707,
      "step": 655000
    },
    {
      "epoch": 5.977626104095189,
      "grad_norm": 4.362024307250977,
      "learning_rate": 4.501864491325401e-05,
      "loss": 0.6823,
      "step": 655100
    },
    {
      "epoch": 5.9785385794583545,
      "grad_norm": 3.7461531162261963,
      "learning_rate": 4.5017884517118044e-05,
      "loss": 0.7097,
      "step": 655200
    },
    {
      "epoch": 5.97945105482152,
      "grad_norm": 3.7027153968811035,
      "learning_rate": 4.501712412098207e-05,
      "loss": 0.7058,
      "step": 655300
    },
    {
      "epoch": 5.980363530184685,
      "grad_norm": 4.001744747161865,
      "learning_rate": 4.50163637248461e-05,
      "loss": 0.6997,
      "step": 655400
    },
    {
      "epoch": 5.98127600554785,
      "grad_norm": 4.157708168029785,
      "learning_rate": 4.501560332871013e-05,
      "loss": 0.7101,
      "step": 655500
    },
    {
      "epoch": 5.982188480911016,
      "grad_norm": 2.9186205863952637,
      "learning_rate": 4.501484293257416e-05,
      "loss": 0.6539,
      "step": 655600
    },
    {
      "epoch": 5.98310095627418,
      "grad_norm": 2.9100112915039062,
      "learning_rate": 4.501408253643819e-05,
      "loss": 0.7075,
      "step": 655700
    },
    {
      "epoch": 5.9840134316373454,
      "grad_norm": 3.885317087173462,
      "learning_rate": 4.501332214030222e-05,
      "loss": 0.7418,
      "step": 655800
    },
    {
      "epoch": 5.984925907000511,
      "grad_norm": 4.449939250946045,
      "learning_rate": 4.501256174416624e-05,
      "loss": 0.6941,
      "step": 655900
    },
    {
      "epoch": 5.985838382363676,
      "grad_norm": 2.999995470046997,
      "learning_rate": 4.501180134803028e-05,
      "loss": 0.7012,
      "step": 656000
    },
    {
      "epoch": 5.986750857726841,
      "grad_norm": 2.75352144241333,
      "learning_rate": 4.50110409518943e-05,
      "loss": 0.7425,
      "step": 656100
    },
    {
      "epoch": 5.987663333090007,
      "grad_norm": 3.7488832473754883,
      "learning_rate": 4.501028055575833e-05,
      "loss": 0.6905,
      "step": 656200
    },
    {
      "epoch": 5.988575808453172,
      "grad_norm": 4.610507011413574,
      "learning_rate": 4.500952015962236e-05,
      "loss": 0.7275,
      "step": 656300
    },
    {
      "epoch": 5.989488283816337,
      "grad_norm": 4.482577800750732,
      "learning_rate": 4.5008759763486384e-05,
      "loss": 0.6794,
      "step": 656400
    },
    {
      "epoch": 5.9904007591795025,
      "grad_norm": 3.2577223777770996,
      "learning_rate": 4.5007999367350414e-05,
      "loss": 0.7269,
      "step": 656500
    },
    {
      "epoch": 5.991313234542668,
      "grad_norm": 3.16910982131958,
      "learning_rate": 4.5007238971214444e-05,
      "loss": 0.6677,
      "step": 656600
    },
    {
      "epoch": 5.992225709905832,
      "grad_norm": 4.148280620574951,
      "learning_rate": 4.5006478575078474e-05,
      "loss": 0.7243,
      "step": 656700
    },
    {
      "epoch": 5.9931381852689976,
      "grad_norm": 3.7035160064697266,
      "learning_rate": 4.5005718178942504e-05,
      "loss": 0.7016,
      "step": 656800
    },
    {
      "epoch": 5.994050660632163,
      "grad_norm": 2.961906909942627,
      "learning_rate": 4.5004957782806534e-05,
      "loss": 0.7223,
      "step": 656900
    },
    {
      "epoch": 5.994963135995328,
      "grad_norm": 3.6731808185577393,
      "learning_rate": 4.500419738667056e-05,
      "loss": 0.6982,
      "step": 657000
    },
    {
      "epoch": 5.9958756113584935,
      "grad_norm": 4.099311351776123,
      "learning_rate": 4.5003436990534595e-05,
      "loss": 0.6913,
      "step": 657100
    },
    {
      "epoch": 5.996788086721659,
      "grad_norm": 5.077387809753418,
      "learning_rate": 4.500267659439862e-05,
      "loss": 0.7255,
      "step": 657200
    },
    {
      "epoch": 5.997700562084824,
      "grad_norm": 4.378941535949707,
      "learning_rate": 4.500191619826265e-05,
      "loss": 0.7062,
      "step": 657300
    },
    {
      "epoch": 5.9986130374479885,
      "grad_norm": 3.182438373565674,
      "learning_rate": 4.500115580212668e-05,
      "loss": 0.675,
      "step": 657400
    },
    {
      "epoch": 5.999525512811154,
      "grad_norm": 4.703143119812012,
      "learning_rate": 4.500039540599071e-05,
      "loss": 0.694,
      "step": 657500
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5727429389953613,
      "eval_runtime": 25.3522,
      "eval_samples_per_second": 227.554,
      "eval_steps_per_second": 227.554,
      "step": 657552
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5548979640007019,
      "eval_runtime": 481.8366,
      "eval_samples_per_second": 227.446,
      "eval_steps_per_second": 227.446,
      "step": 657552
    },
    {
      "epoch": 6.000437988174319,
      "grad_norm": 4.783563613891602,
      "learning_rate": 4.499963500985474e-05,
      "loss": 0.6991,
      "step": 657600
    },
    {
      "epoch": 6.001350463537484,
      "grad_norm": 3.5358874797821045,
      "learning_rate": 4.499887461371877e-05,
      "loss": 0.6921,
      "step": 657700
    },
    {
      "epoch": 6.00226293890065,
      "grad_norm": 4.66499137878418,
      "learning_rate": 4.499811421758279e-05,
      "loss": 0.7339,
      "step": 657800
    },
    {
      "epoch": 6.003175414263815,
      "grad_norm": 4.518305778503418,
      "learning_rate": 4.499735382144682e-05,
      "loss": 0.7176,
      "step": 657900
    },
    {
      "epoch": 6.00408788962698,
      "grad_norm": 4.6463398933410645,
      "learning_rate": 4.499659342531085e-05,
      "loss": 0.6993,
      "step": 658000
    },
    {
      "epoch": 6.005000364990146,
      "grad_norm": 4.144606113433838,
      "learning_rate": 4.499583302917488e-05,
      "loss": 0.6944,
      "step": 658100
    },
    {
      "epoch": 6.005912840353311,
      "grad_norm": 3.6703386306762695,
      "learning_rate": 4.499507263303891e-05,
      "loss": 0.6986,
      "step": 658200
    },
    {
      "epoch": 6.006825315716475,
      "grad_norm": 4.164997577667236,
      "learning_rate": 4.499431223690294e-05,
      "loss": 0.6927,
      "step": 658300
    },
    {
      "epoch": 6.007737791079641,
      "grad_norm": 3.661802053451538,
      "learning_rate": 4.4993551840766965e-05,
      "loss": 0.718,
      "step": 658400
    },
    {
      "epoch": 6.008650266442806,
      "grad_norm": 2.764716148376465,
      "learning_rate": 4.4992791444631e-05,
      "loss": 0.718,
      "step": 658500
    },
    {
      "epoch": 6.009562741805971,
      "grad_norm": 4.018468856811523,
      "learning_rate": 4.4992031048495025e-05,
      "loss": 0.7331,
      "step": 658600
    },
    {
      "epoch": 6.0104752171691365,
      "grad_norm": 3.6831448078155518,
      "learning_rate": 4.4991270652359055e-05,
      "loss": 0.7152,
      "step": 658700
    },
    {
      "epoch": 6.011387692532302,
      "grad_norm": 4.277090549468994,
      "learning_rate": 4.4990510256223085e-05,
      "loss": 0.7277,
      "step": 658800
    },
    {
      "epoch": 6.012300167895467,
      "grad_norm": 3.6548216342926025,
      "learning_rate": 4.4989749860087115e-05,
      "loss": 0.6957,
      "step": 658900
    },
    {
      "epoch": 6.013212643258632,
      "grad_norm": 4.546633243560791,
      "learning_rate": 4.4988989463951145e-05,
      "loss": 0.7043,
      "step": 659000
    },
    {
      "epoch": 6.014125118621797,
      "grad_norm": 4.153248310089111,
      "learning_rate": 4.498822906781517e-05,
      "loss": 0.687,
      "step": 659100
    },
    {
      "epoch": 6.015037593984962,
      "grad_norm": 3.5150065422058105,
      "learning_rate": 4.49874686716792e-05,
      "loss": 0.6965,
      "step": 659200
    },
    {
      "epoch": 6.015950069348127,
      "grad_norm": 4.066493988037109,
      "learning_rate": 4.498670827554323e-05,
      "loss": 0.6674,
      "step": 659300
    },
    {
      "epoch": 6.016862544711293,
      "grad_norm": 3.4785330295562744,
      "learning_rate": 4.498594787940726e-05,
      "loss": 0.6944,
      "step": 659400
    },
    {
      "epoch": 6.017775020074458,
      "grad_norm": 3.9681379795074463,
      "learning_rate": 4.498518748327128e-05,
      "loss": 0.7331,
      "step": 659500
    },
    {
      "epoch": 6.018687495437623,
      "grad_norm": 3.9086427688598633,
      "learning_rate": 4.498442708713532e-05,
      "loss": 0.6967,
      "step": 659600
    },
    {
      "epoch": 6.019599970800789,
      "grad_norm": 4.246266841888428,
      "learning_rate": 4.498366669099934e-05,
      "loss": 0.6758,
      "step": 659700
    },
    {
      "epoch": 6.020512446163954,
      "grad_norm": 4.139445781707764,
      "learning_rate": 4.498290629486337e-05,
      "loss": 0.7104,
      "step": 659800
    },
    {
      "epoch": 6.021424921527119,
      "grad_norm": 3.4282784461975098,
      "learning_rate": 4.49821458987274e-05,
      "loss": 0.6982,
      "step": 659900
    },
    {
      "epoch": 6.022337396890284,
      "grad_norm": 3.6035754680633545,
      "learning_rate": 4.498138550259143e-05,
      "loss": 0.6919,
      "step": 660000
    },
    {
      "epoch": 6.023249872253449,
      "grad_norm": 4.314492225646973,
      "learning_rate": 4.498062510645546e-05,
      "loss": 0.7064,
      "step": 660100
    },
    {
      "epoch": 6.024162347616614,
      "grad_norm": 6.613515853881836,
      "learning_rate": 4.497986471031949e-05,
      "loss": 0.7355,
      "step": 660200
    },
    {
      "epoch": 6.0250748229797795,
      "grad_norm": 2.380028009414673,
      "learning_rate": 4.4979104314183516e-05,
      "loss": 0.6714,
      "step": 660300
    },
    {
      "epoch": 6.025987298342945,
      "grad_norm": 4.289198875427246,
      "learning_rate": 4.497834391804755e-05,
      "loss": 0.7185,
      "step": 660400
    },
    {
      "epoch": 6.02689977370611,
      "grad_norm": 4.558221817016602,
      "learning_rate": 4.4977583521911576e-05,
      "loss": 0.7125,
      "step": 660500
    },
    {
      "epoch": 6.027812249069275,
      "grad_norm": 3.9230141639709473,
      "learning_rate": 4.4976823125775606e-05,
      "loss": 0.7138,
      "step": 660600
    },
    {
      "epoch": 6.028724724432441,
      "grad_norm": 4.103537082672119,
      "learning_rate": 4.4976062729639636e-05,
      "loss": 0.7185,
      "step": 660700
    },
    {
      "epoch": 6.029637199795605,
      "grad_norm": 3.9496376514434814,
      "learning_rate": 4.4975302333503666e-05,
      "loss": 0.701,
      "step": 660800
    },
    {
      "epoch": 6.03054967515877,
      "grad_norm": 3.8787951469421387,
      "learning_rate": 4.497454193736769e-05,
      "loss": 0.6735,
      "step": 660900
    },
    {
      "epoch": 6.031462150521936,
      "grad_norm": 4.196895122528076,
      "learning_rate": 4.4973781541231726e-05,
      "loss": 0.7208,
      "step": 661000
    },
    {
      "epoch": 6.032374625885101,
      "grad_norm": 5.211982727050781,
      "learning_rate": 4.497302114509575e-05,
      "loss": 0.7276,
      "step": 661100
    },
    {
      "epoch": 6.033287101248266,
      "grad_norm": 3.8485350608825684,
      "learning_rate": 4.497226074895978e-05,
      "loss": 0.6935,
      "step": 661200
    },
    {
      "epoch": 6.034199576611432,
      "grad_norm": 4.1597065925598145,
      "learning_rate": 4.497150035282381e-05,
      "loss": 0.6858,
      "step": 661300
    },
    {
      "epoch": 6.035112051974597,
      "grad_norm": 4.071682453155518,
      "learning_rate": 4.497073995668784e-05,
      "loss": 0.7271,
      "step": 661400
    },
    {
      "epoch": 6.036024527337762,
      "grad_norm": 4.773095607757568,
      "learning_rate": 4.496997956055187e-05,
      "loss": 0.7,
      "step": 661500
    },
    {
      "epoch": 6.0369370027009275,
      "grad_norm": 4.471066474914551,
      "learning_rate": 4.49692191644159e-05,
      "loss": 0.7299,
      "step": 661600
    },
    {
      "epoch": 6.037849478064092,
      "grad_norm": 4.508399963378906,
      "learning_rate": 4.4968458768279923e-05,
      "loss": 0.683,
      "step": 661700
    },
    {
      "epoch": 6.038761953427257,
      "grad_norm": 3.3818976879119873,
      "learning_rate": 4.4967698372143953e-05,
      "loss": 0.7044,
      "step": 661800
    },
    {
      "epoch": 6.0396744287904225,
      "grad_norm": 3.931917667388916,
      "learning_rate": 4.4966937976007984e-05,
      "loss": 0.7229,
      "step": 661900
    },
    {
      "epoch": 6.040586904153588,
      "grad_norm": 3.664498805999756,
      "learning_rate": 4.496617757987201e-05,
      "loss": 0.7166,
      "step": 662000
    },
    {
      "epoch": 6.041499379516753,
      "grad_norm": 4.524367809295654,
      "learning_rate": 4.4965417183736044e-05,
      "loss": 0.6816,
      "step": 662100
    },
    {
      "epoch": 6.042411854879918,
      "grad_norm": 4.35421895980835,
      "learning_rate": 4.496465678760007e-05,
      "loss": 0.6873,
      "step": 662200
    },
    {
      "epoch": 6.043324330243084,
      "grad_norm": 4.378675937652588,
      "learning_rate": 4.49638963914641e-05,
      "loss": 0.7312,
      "step": 662300
    },
    {
      "epoch": 6.044236805606249,
      "grad_norm": 3.0799684524536133,
      "learning_rate": 4.496313599532813e-05,
      "loss": 0.7142,
      "step": 662400
    },
    {
      "epoch": 6.045149280969413,
      "grad_norm": 3.6518898010253906,
      "learning_rate": 4.496237559919216e-05,
      "loss": 0.6972,
      "step": 662500
    },
    {
      "epoch": 6.046061756332579,
      "grad_norm": 3.615407705307007,
      "learning_rate": 4.496161520305619e-05,
      "loss": 0.7232,
      "step": 662600
    },
    {
      "epoch": 6.046974231695744,
      "grad_norm": 4.290450572967529,
      "learning_rate": 4.496085480692022e-05,
      "loss": 0.7183,
      "step": 662700
    },
    {
      "epoch": 6.047886707058909,
      "grad_norm": 3.8253438472747803,
      "learning_rate": 4.496009441078424e-05,
      "loss": 0.693,
      "step": 662800
    },
    {
      "epoch": 6.048799182422075,
      "grad_norm": 4.1921563148498535,
      "learning_rate": 4.495933401464828e-05,
      "loss": 0.7004,
      "step": 662900
    },
    {
      "epoch": 6.04971165778524,
      "grad_norm": 3.8258495330810547,
      "learning_rate": 4.49585736185123e-05,
      "loss": 0.7372,
      "step": 663000
    },
    {
      "epoch": 6.050624133148405,
      "grad_norm": 4.0829315185546875,
      "learning_rate": 4.495781322237633e-05,
      "loss": 0.6953,
      "step": 663100
    },
    {
      "epoch": 6.0515366085115705,
      "grad_norm": 3.842067003250122,
      "learning_rate": 4.495705282624036e-05,
      "loss": 0.7074,
      "step": 663200
    },
    {
      "epoch": 6.052449083874736,
      "grad_norm": 3.6429474353790283,
      "learning_rate": 4.495629243010439e-05,
      "loss": 0.7155,
      "step": 663300
    },
    {
      "epoch": 6.0533615592379,
      "grad_norm": 5.087008476257324,
      "learning_rate": 4.4955532033968414e-05,
      "loss": 0.7062,
      "step": 663400
    },
    {
      "epoch": 6.0542740346010655,
      "grad_norm": 4.26231050491333,
      "learning_rate": 4.495477163783245e-05,
      "loss": 0.7302,
      "step": 663500
    },
    {
      "epoch": 6.055186509964231,
      "grad_norm": 4.090521812438965,
      "learning_rate": 4.4954011241696474e-05,
      "loss": 0.7461,
      "step": 663600
    },
    {
      "epoch": 6.056098985327396,
      "grad_norm": 3.772939920425415,
      "learning_rate": 4.4953250845560504e-05,
      "loss": 0.6708,
      "step": 663700
    },
    {
      "epoch": 6.057011460690561,
      "grad_norm": 4.486741065979004,
      "learning_rate": 4.4952490449424534e-05,
      "loss": 0.7228,
      "step": 663800
    },
    {
      "epoch": 6.057923936053727,
      "grad_norm": 3.910576343536377,
      "learning_rate": 4.4951730053288565e-05,
      "loss": 0.7032,
      "step": 663900
    },
    {
      "epoch": 6.058836411416892,
      "grad_norm": 4.624390602111816,
      "learning_rate": 4.4950969657152595e-05,
      "loss": 0.6887,
      "step": 664000
    },
    {
      "epoch": 6.059748886780057,
      "grad_norm": 3.792123794555664,
      "learning_rate": 4.4950209261016625e-05,
      "loss": 0.7097,
      "step": 664100
    },
    {
      "epoch": 6.060661362143222,
      "grad_norm": 4.456132411956787,
      "learning_rate": 4.494944886488065e-05,
      "loss": 0.7253,
      "step": 664200
    },
    {
      "epoch": 6.061573837506387,
      "grad_norm": 4.185311794281006,
      "learning_rate": 4.4948688468744685e-05,
      "loss": 0.6944,
      "step": 664300
    },
    {
      "epoch": 6.062486312869552,
      "grad_norm": 4.967162609100342,
      "learning_rate": 4.494792807260871e-05,
      "loss": 0.7195,
      "step": 664400
    },
    {
      "epoch": 6.063398788232718,
      "grad_norm": 4.3143696784973145,
      "learning_rate": 4.494716767647274e-05,
      "loss": 0.6933,
      "step": 664500
    },
    {
      "epoch": 6.064311263595883,
      "grad_norm": 4.527093887329102,
      "learning_rate": 4.494640728033677e-05,
      "loss": 0.7252,
      "step": 664600
    },
    {
      "epoch": 6.065223738959048,
      "grad_norm": 4.943764686584473,
      "learning_rate": 4.494564688420079e-05,
      "loss": 0.69,
      "step": 664700
    },
    {
      "epoch": 6.0661362143222135,
      "grad_norm": 3.800891399383545,
      "learning_rate": 4.494488648806482e-05,
      "loss": 0.6837,
      "step": 664800
    },
    {
      "epoch": 6.067048689685379,
      "grad_norm": 2.3539681434631348,
      "learning_rate": 4.494412609192885e-05,
      "loss": 0.6958,
      "step": 664900
    },
    {
      "epoch": 6.067961165048544,
      "grad_norm": 4.198674201965332,
      "learning_rate": 4.494336569579288e-05,
      "loss": 0.7004,
      "step": 665000
    },
    {
      "epoch": 6.0688736404117085,
      "grad_norm": 4.267716407775879,
      "learning_rate": 4.494260529965691e-05,
      "loss": 0.6921,
      "step": 665100
    },
    {
      "epoch": 6.069786115774874,
      "grad_norm": 3.441645622253418,
      "learning_rate": 4.494184490352094e-05,
      "loss": 0.7163,
      "step": 665200
    },
    {
      "epoch": 6.070698591138039,
      "grad_norm": 3.8232948780059814,
      "learning_rate": 4.4941084507384965e-05,
      "loss": 0.7232,
      "step": 665300
    },
    {
      "epoch": 6.071611066501204,
      "grad_norm": 4.500112056732178,
      "learning_rate": 4.4940324111249e-05,
      "loss": 0.6973,
      "step": 665400
    },
    {
      "epoch": 6.07252354186437,
      "grad_norm": 3.188223361968994,
      "learning_rate": 4.4939563715113025e-05,
      "loss": 0.6971,
      "step": 665500
    },
    {
      "epoch": 6.073436017227535,
      "grad_norm": 4.116241931915283,
      "learning_rate": 4.4938803318977055e-05,
      "loss": 0.6945,
      "step": 665600
    },
    {
      "epoch": 6.0743484925907,
      "grad_norm": 4.193192958831787,
      "learning_rate": 4.4938042922841085e-05,
      "loss": 0.6986,
      "step": 665700
    },
    {
      "epoch": 6.075260967953866,
      "grad_norm": 5.12802791595459,
      "learning_rate": 4.4937282526705115e-05,
      "loss": 0.7249,
      "step": 665800
    },
    {
      "epoch": 6.07617344331703,
      "grad_norm": 3.140374183654785,
      "learning_rate": 4.493652213056914e-05,
      "loss": 0.7174,
      "step": 665900
    },
    {
      "epoch": 6.077085918680195,
      "grad_norm": 4.157175064086914,
      "learning_rate": 4.4935761734433176e-05,
      "loss": 0.6946,
      "step": 666000
    },
    {
      "epoch": 6.077998394043361,
      "grad_norm": 3.7353568077087402,
      "learning_rate": 4.49350013382972e-05,
      "loss": 0.6988,
      "step": 666100
    },
    {
      "epoch": 6.078910869406526,
      "grad_norm": 4.670895576477051,
      "learning_rate": 4.493424094216123e-05,
      "loss": 0.7096,
      "step": 666200
    },
    {
      "epoch": 6.079823344769691,
      "grad_norm": 4.302993297576904,
      "learning_rate": 4.493348054602526e-05,
      "loss": 0.7211,
      "step": 666300
    },
    {
      "epoch": 6.0807358201328565,
      "grad_norm": 3.368929386138916,
      "learning_rate": 4.493272014988929e-05,
      "loss": 0.6868,
      "step": 666400
    },
    {
      "epoch": 6.081648295496022,
      "grad_norm": 4.569276809692383,
      "learning_rate": 4.493195975375332e-05,
      "loss": 0.7366,
      "step": 666500
    },
    {
      "epoch": 6.082560770859187,
      "grad_norm": 4.103326797485352,
      "learning_rate": 4.493119935761735e-05,
      "loss": 0.7006,
      "step": 666600
    },
    {
      "epoch": 6.083473246222352,
      "grad_norm": 4.427188396453857,
      "learning_rate": 4.493043896148137e-05,
      "loss": 0.6914,
      "step": 666700
    },
    {
      "epoch": 6.084385721585517,
      "grad_norm": 3.891021251678467,
      "learning_rate": 4.492967856534541e-05,
      "loss": 0.6749,
      "step": 666800
    },
    {
      "epoch": 6.085298196948682,
      "grad_norm": 4.321629524230957,
      "learning_rate": 4.492891816920943e-05,
      "loss": 0.6783,
      "step": 666900
    },
    {
      "epoch": 6.086210672311847,
      "grad_norm": 3.8586976528167725,
      "learning_rate": 4.492815777307346e-05,
      "loss": 0.6805,
      "step": 667000
    },
    {
      "epoch": 6.087123147675013,
      "grad_norm": 4.072860240936279,
      "learning_rate": 4.492739737693749e-05,
      "loss": 0.7262,
      "step": 667100
    },
    {
      "epoch": 6.088035623038178,
      "grad_norm": 5.1797051429748535,
      "learning_rate": 4.492663698080152e-05,
      "loss": 0.682,
      "step": 667200
    },
    {
      "epoch": 6.088948098401343,
      "grad_norm": 3.8488645553588867,
      "learning_rate": 4.4925876584665546e-05,
      "loss": 0.7223,
      "step": 667300
    },
    {
      "epoch": 6.089860573764509,
      "grad_norm": 3.8352904319763184,
      "learning_rate": 4.492511618852958e-05,
      "loss": 0.7185,
      "step": 667400
    },
    {
      "epoch": 6.090773049127674,
      "grad_norm": 4.399896144866943,
      "learning_rate": 4.4924355792393606e-05,
      "loss": 0.6997,
      "step": 667500
    },
    {
      "epoch": 6.091685524490838,
      "grad_norm": 3.4697039127349854,
      "learning_rate": 4.4923595396257636e-05,
      "loss": 0.6933,
      "step": 667600
    },
    {
      "epoch": 6.092597999854004,
      "grad_norm": 3.7955737113952637,
      "learning_rate": 4.4922835000121666e-05,
      "loss": 0.7456,
      "step": 667700
    },
    {
      "epoch": 6.093510475217169,
      "grad_norm": 4.942512512207031,
      "learning_rate": 4.492207460398569e-05,
      "loss": 0.7061,
      "step": 667800
    },
    {
      "epoch": 6.094422950580334,
      "grad_norm": 4.778583526611328,
      "learning_rate": 4.4921314207849727e-05,
      "loss": 0.6941,
      "step": 667900
    },
    {
      "epoch": 6.0953354259434995,
      "grad_norm": 3.9343514442443848,
      "learning_rate": 4.492055381171375e-05,
      "loss": 0.7255,
      "step": 668000
    },
    {
      "epoch": 6.096247901306665,
      "grad_norm": 3.4757912158966064,
      "learning_rate": 4.491979341557778e-05,
      "loss": 0.7091,
      "step": 668100
    },
    {
      "epoch": 6.09716037666983,
      "grad_norm": 3.6782312393188477,
      "learning_rate": 4.491903301944181e-05,
      "loss": 0.7056,
      "step": 668200
    },
    {
      "epoch": 6.098072852032995,
      "grad_norm": 4.697070121765137,
      "learning_rate": 4.491827262330584e-05,
      "loss": 0.6657,
      "step": 668300
    },
    {
      "epoch": 6.098985327396161,
      "grad_norm": 4.083564281463623,
      "learning_rate": 4.491751222716986e-05,
      "loss": 0.6938,
      "step": 668400
    },
    {
      "epoch": 6.099897802759325,
      "grad_norm": 3.9940264225006104,
      "learning_rate": 4.49167518310339e-05,
      "loss": 0.687,
      "step": 668500
    },
    {
      "epoch": 6.10081027812249,
      "grad_norm": 4.367635726928711,
      "learning_rate": 4.4915991434897924e-05,
      "loss": 0.6994,
      "step": 668600
    },
    {
      "epoch": 6.101722753485656,
      "grad_norm": 4.4683613777160645,
      "learning_rate": 4.4915231038761954e-05,
      "loss": 0.671,
      "step": 668700
    },
    {
      "epoch": 6.102635228848821,
      "grad_norm": 5.135393142700195,
      "learning_rate": 4.4914470642625984e-05,
      "loss": 0.71,
      "step": 668800
    },
    {
      "epoch": 6.103547704211986,
      "grad_norm": 3.5945520401000977,
      "learning_rate": 4.4913710246490014e-05,
      "loss": 0.7214,
      "step": 668900
    },
    {
      "epoch": 6.104460179575152,
      "grad_norm": 4.582386016845703,
      "learning_rate": 4.4912949850354044e-05,
      "loss": 0.712,
      "step": 669000
    },
    {
      "epoch": 6.105372654938317,
      "grad_norm": 4.638267993927002,
      "learning_rate": 4.4912189454218074e-05,
      "loss": 0.6973,
      "step": 669100
    },
    {
      "epoch": 6.106285130301482,
      "grad_norm": 3.285783290863037,
      "learning_rate": 4.49114290580821e-05,
      "loss": 0.6788,
      "step": 669200
    },
    {
      "epoch": 6.107197605664647,
      "grad_norm": 4.58412504196167,
      "learning_rate": 4.4910668661946134e-05,
      "loss": 0.6661,
      "step": 669300
    },
    {
      "epoch": 6.108110081027812,
      "grad_norm": 4.020471572875977,
      "learning_rate": 4.490990826581016e-05,
      "loss": 0.7318,
      "step": 669400
    },
    {
      "epoch": 6.109022556390977,
      "grad_norm": 3.299640655517578,
      "learning_rate": 4.490914786967419e-05,
      "loss": 0.6756,
      "step": 669500
    },
    {
      "epoch": 6.1099350317541425,
      "grad_norm": 4.038191318511963,
      "learning_rate": 4.490838747353822e-05,
      "loss": 0.6883,
      "step": 669600
    },
    {
      "epoch": 6.110847507117308,
      "grad_norm": 3.829470634460449,
      "learning_rate": 4.490762707740225e-05,
      "loss": 0.6923,
      "step": 669700
    },
    {
      "epoch": 6.111759982480473,
      "grad_norm": 4.317434787750244,
      "learning_rate": 4.490686668126628e-05,
      "loss": 0.7069,
      "step": 669800
    },
    {
      "epoch": 6.112672457843638,
      "grad_norm": 4.385256290435791,
      "learning_rate": 4.490610628513031e-05,
      "loss": 0.7166,
      "step": 669900
    },
    {
      "epoch": 6.113584933206804,
      "grad_norm": 4.219356536865234,
      "learning_rate": 4.490534588899433e-05,
      "loss": 0.7336,
      "step": 670000
    },
    {
      "epoch": 6.114497408569969,
      "grad_norm": 3.761043071746826,
      "learning_rate": 4.490458549285836e-05,
      "loss": 0.6915,
      "step": 670100
    },
    {
      "epoch": 6.1154098839331335,
      "grad_norm": 3.60140323638916,
      "learning_rate": 4.490382509672239e-05,
      "loss": 0.6638,
      "step": 670200
    },
    {
      "epoch": 6.116322359296299,
      "grad_norm": 4.268237113952637,
      "learning_rate": 4.4903064700586414e-05,
      "loss": 0.719,
      "step": 670300
    },
    {
      "epoch": 6.117234834659464,
      "grad_norm": 6.6377854347229,
      "learning_rate": 4.490230430445045e-05,
      "loss": 0.7036,
      "step": 670400
    },
    {
      "epoch": 6.118147310022629,
      "grad_norm": 4.574065685272217,
      "learning_rate": 4.4901543908314474e-05,
      "loss": 0.7338,
      "step": 670500
    },
    {
      "epoch": 6.119059785385795,
      "grad_norm": 4.556782245635986,
      "learning_rate": 4.4900783512178505e-05,
      "loss": 0.6933,
      "step": 670600
    },
    {
      "epoch": 6.11997226074896,
      "grad_norm": 3.2820208072662354,
      "learning_rate": 4.4900023116042535e-05,
      "loss": 0.7321,
      "step": 670700
    },
    {
      "epoch": 6.120884736112125,
      "grad_norm": 3.9847042560577393,
      "learning_rate": 4.4899262719906565e-05,
      "loss": 0.6724,
      "step": 670800
    },
    {
      "epoch": 6.1217972114752905,
      "grad_norm": 4.106661319732666,
      "learning_rate": 4.4898502323770595e-05,
      "loss": 0.6998,
      "step": 670900
    },
    {
      "epoch": 6.122709686838455,
      "grad_norm": 4.559708118438721,
      "learning_rate": 4.4897741927634625e-05,
      "loss": 0.7027,
      "step": 671000
    },
    {
      "epoch": 6.12362216220162,
      "grad_norm": 3.702141046524048,
      "learning_rate": 4.489698153149865e-05,
      "loss": 0.6869,
      "step": 671100
    },
    {
      "epoch": 6.1245346375647856,
      "grad_norm": 3.8857603073120117,
      "learning_rate": 4.4896221135362685e-05,
      "loss": 0.7021,
      "step": 671200
    },
    {
      "epoch": 6.125447112927951,
      "grad_norm": 4.124631404876709,
      "learning_rate": 4.489546073922671e-05,
      "loss": 0.7292,
      "step": 671300
    },
    {
      "epoch": 6.126359588291116,
      "grad_norm": 4.118966579437256,
      "learning_rate": 4.489470034309074e-05,
      "loss": 0.6987,
      "step": 671400
    },
    {
      "epoch": 6.1272720636542815,
      "grad_norm": 4.260311603546143,
      "learning_rate": 4.489393994695477e-05,
      "loss": 0.662,
      "step": 671500
    },
    {
      "epoch": 6.128184539017447,
      "grad_norm": 4.358447074890137,
      "learning_rate": 4.48931795508188e-05,
      "loss": 0.7166,
      "step": 671600
    },
    {
      "epoch": 6.129097014380612,
      "grad_norm": 4.03070068359375,
      "learning_rate": 4.489241915468282e-05,
      "loss": 0.7102,
      "step": 671700
    },
    {
      "epoch": 6.130009489743777,
      "grad_norm": 3.79685640335083,
      "learning_rate": 4.489165875854686e-05,
      "loss": 0.7186,
      "step": 671800
    },
    {
      "epoch": 6.130921965106942,
      "grad_norm": 3.707993984222412,
      "learning_rate": 4.489089836241088e-05,
      "loss": 0.732,
      "step": 671900
    },
    {
      "epoch": 6.131834440470107,
      "grad_norm": 4.32699728012085,
      "learning_rate": 4.489013796627491e-05,
      "loss": 0.6663,
      "step": 672000
    },
    {
      "epoch": 6.132746915833272,
      "grad_norm": 4.3448944091796875,
      "learning_rate": 4.488937757013894e-05,
      "loss": 0.6973,
      "step": 672100
    },
    {
      "epoch": 6.133659391196438,
      "grad_norm": 4.365200996398926,
      "learning_rate": 4.488861717400297e-05,
      "loss": 0.716,
      "step": 672200
    },
    {
      "epoch": 6.134571866559603,
      "grad_norm": 3.963374137878418,
      "learning_rate": 4.4887856777867e-05,
      "loss": 0.6877,
      "step": 672300
    },
    {
      "epoch": 6.135484341922768,
      "grad_norm": 4.086902618408203,
      "learning_rate": 4.488709638173103e-05,
      "loss": 0.7124,
      "step": 672400
    },
    {
      "epoch": 6.136396817285934,
      "grad_norm": 3.061309337615967,
      "learning_rate": 4.4886335985595055e-05,
      "loss": 0.6787,
      "step": 672500
    },
    {
      "epoch": 6.137309292649099,
      "grad_norm": 3.217332601547241,
      "learning_rate": 4.488557558945909e-05,
      "loss": 0.7066,
      "step": 672600
    },
    {
      "epoch": 6.138221768012263,
      "grad_norm": 3.922881841659546,
      "learning_rate": 4.4884815193323116e-05,
      "loss": 0.6856,
      "step": 672700
    },
    {
      "epoch": 6.139134243375429,
      "grad_norm": 4.443814277648926,
      "learning_rate": 4.4884054797187146e-05,
      "loss": 0.7164,
      "step": 672800
    },
    {
      "epoch": 6.140046718738594,
      "grad_norm": 4.272426605224609,
      "learning_rate": 4.4883294401051176e-05,
      "loss": 0.6562,
      "step": 672900
    },
    {
      "epoch": 6.140959194101759,
      "grad_norm": 3.780320644378662,
      "learning_rate": 4.4882534004915206e-05,
      "loss": 0.7447,
      "step": 673000
    },
    {
      "epoch": 6.1418716694649245,
      "grad_norm": 3.511608600616455,
      "learning_rate": 4.488177360877923e-05,
      "loss": 0.6682,
      "step": 673100
    },
    {
      "epoch": 6.14278414482809,
      "grad_norm": 4.002442836761475,
      "learning_rate": 4.488101321264326e-05,
      "loss": 0.7099,
      "step": 673200
    },
    {
      "epoch": 6.143696620191255,
      "grad_norm": 4.825488090515137,
      "learning_rate": 4.488025281650729e-05,
      "loss": 0.7078,
      "step": 673300
    },
    {
      "epoch": 6.14460909555442,
      "grad_norm": 5.464659214019775,
      "learning_rate": 4.487949242037132e-05,
      "loss": 0.677,
      "step": 673400
    },
    {
      "epoch": 6.145521570917586,
      "grad_norm": 3.9102959632873535,
      "learning_rate": 4.487873202423535e-05,
      "loss": 0.692,
      "step": 673500
    },
    {
      "epoch": 6.14643404628075,
      "grad_norm": 3.754404306411743,
      "learning_rate": 4.487797162809937e-05,
      "loss": 0.7122,
      "step": 673600
    },
    {
      "epoch": 6.147346521643915,
      "grad_norm": 4.625141620635986,
      "learning_rate": 4.487721123196341e-05,
      "loss": 0.692,
      "step": 673700
    },
    {
      "epoch": 6.148258997007081,
      "grad_norm": 3.7901718616485596,
      "learning_rate": 4.487645083582743e-05,
      "loss": 0.7193,
      "step": 673800
    },
    {
      "epoch": 6.149171472370246,
      "grad_norm": 4.171055316925049,
      "learning_rate": 4.487569043969146e-05,
      "loss": 0.7115,
      "step": 673900
    },
    {
      "epoch": 6.150083947733411,
      "grad_norm": 5.145025253295898,
      "learning_rate": 4.487493004355549e-05,
      "loss": 0.7672,
      "step": 674000
    },
    {
      "epoch": 6.150996423096577,
      "grad_norm": 3.2213363647460938,
      "learning_rate": 4.487416964741952e-05,
      "loss": 0.7114,
      "step": 674100
    },
    {
      "epoch": 6.151908898459742,
      "grad_norm": 3.937476873397827,
      "learning_rate": 4.4873409251283546e-05,
      "loss": 0.6849,
      "step": 674200
    },
    {
      "epoch": 6.152821373822907,
      "grad_norm": 3.741544008255005,
      "learning_rate": 4.487264885514758e-05,
      "loss": 0.6783,
      "step": 674300
    },
    {
      "epoch": 6.153733849186072,
      "grad_norm": 3.7926366329193115,
      "learning_rate": 4.4871888459011606e-05,
      "loss": 0.6737,
      "step": 674400
    },
    {
      "epoch": 6.154646324549237,
      "grad_norm": 4.1670756340026855,
      "learning_rate": 4.4871128062875636e-05,
      "loss": 0.689,
      "step": 674500
    },
    {
      "epoch": 6.155558799912402,
      "grad_norm": 3.797177314758301,
      "learning_rate": 4.4870367666739667e-05,
      "loss": 0.725,
      "step": 674600
    },
    {
      "epoch": 6.1564712752755675,
      "grad_norm": 3.15193247795105,
      "learning_rate": 4.48696072706037e-05,
      "loss": 0.6829,
      "step": 674700
    },
    {
      "epoch": 6.157383750638733,
      "grad_norm": 2.5990288257598877,
      "learning_rate": 4.486884687446773e-05,
      "loss": 0.6735,
      "step": 674800
    },
    {
      "epoch": 6.158296226001898,
      "grad_norm": 4.831993103027344,
      "learning_rate": 4.486808647833176e-05,
      "loss": 0.7141,
      "step": 674900
    },
    {
      "epoch": 6.159208701365063,
      "grad_norm": 4.497231483459473,
      "learning_rate": 4.486732608219578e-05,
      "loss": 0.7087,
      "step": 675000
    },
    {
      "epoch": 6.160121176728229,
      "grad_norm": 3.6044137477874756,
      "learning_rate": 4.486656568605982e-05,
      "loss": 0.6863,
      "step": 675100
    },
    {
      "epoch": 6.161033652091394,
      "grad_norm": 4.065199851989746,
      "learning_rate": 4.486580528992384e-05,
      "loss": 0.6992,
      "step": 675200
    },
    {
      "epoch": 6.161946127454558,
      "grad_norm": 4.429364204406738,
      "learning_rate": 4.486504489378787e-05,
      "loss": 0.7313,
      "step": 675300
    },
    {
      "epoch": 6.162858602817724,
      "grad_norm": 3.406137704849243,
      "learning_rate": 4.48642844976519e-05,
      "loss": 0.706,
      "step": 675400
    },
    {
      "epoch": 6.163771078180889,
      "grad_norm": 4.352893352508545,
      "learning_rate": 4.486352410151593e-05,
      "loss": 0.6622,
      "step": 675500
    },
    {
      "epoch": 6.164683553544054,
      "grad_norm": 3.606137275695801,
      "learning_rate": 4.4862763705379954e-05,
      "loss": 0.718,
      "step": 675600
    },
    {
      "epoch": 6.16559602890722,
      "grad_norm": 2.927490711212158,
      "learning_rate": 4.486200330924399e-05,
      "loss": 0.7204,
      "step": 675700
    },
    {
      "epoch": 6.166508504270385,
      "grad_norm": 3.9464287757873535,
      "learning_rate": 4.4861242913108014e-05,
      "loss": 0.6952,
      "step": 675800
    },
    {
      "epoch": 6.16742097963355,
      "grad_norm": 5.318489074707031,
      "learning_rate": 4.4860482516972044e-05,
      "loss": 0.6865,
      "step": 675900
    },
    {
      "epoch": 6.1683334549967155,
      "grad_norm": 2.722031354904175,
      "learning_rate": 4.4859722120836074e-05,
      "loss": 0.6995,
      "step": 676000
    },
    {
      "epoch": 6.16924593035988,
      "grad_norm": 2.417748212814331,
      "learning_rate": 4.48589617247001e-05,
      "loss": 0.7332,
      "step": 676100
    },
    {
      "epoch": 6.170158405723045,
      "grad_norm": 3.6707098484039307,
      "learning_rate": 4.4858201328564134e-05,
      "loss": 0.6674,
      "step": 676200
    },
    {
      "epoch": 6.1710708810862105,
      "grad_norm": 4.0420637130737305,
      "learning_rate": 4.485744093242816e-05,
      "loss": 0.7065,
      "step": 676300
    },
    {
      "epoch": 6.171983356449376,
      "grad_norm": 2.8829824924468994,
      "learning_rate": 4.485668053629219e-05,
      "loss": 0.6946,
      "step": 676400
    },
    {
      "epoch": 6.172895831812541,
      "grad_norm": 4.335200786590576,
      "learning_rate": 4.485592014015622e-05,
      "loss": 0.6855,
      "step": 676500
    },
    {
      "epoch": 6.173808307175706,
      "grad_norm": 4.458539009094238,
      "learning_rate": 4.485515974402025e-05,
      "loss": 0.7043,
      "step": 676600
    },
    {
      "epoch": 6.174720782538872,
      "grad_norm": 4.871562957763672,
      "learning_rate": 4.485439934788427e-05,
      "loss": 0.6613,
      "step": 676700
    },
    {
      "epoch": 6.175633257902037,
      "grad_norm": 4.290918827056885,
      "learning_rate": 4.485363895174831e-05,
      "loss": 0.7362,
      "step": 676800
    },
    {
      "epoch": 6.176545733265202,
      "grad_norm": 4.520313739776611,
      "learning_rate": 4.485287855561233e-05,
      "loss": 0.7213,
      "step": 676900
    },
    {
      "epoch": 6.177458208628367,
      "grad_norm": 4.4802656173706055,
      "learning_rate": 4.485211815947636e-05,
      "loss": 0.7257,
      "step": 677000
    },
    {
      "epoch": 6.178370683991532,
      "grad_norm": 4.517603397369385,
      "learning_rate": 4.485135776334039e-05,
      "loss": 0.7121,
      "step": 677100
    },
    {
      "epoch": 6.179283159354697,
      "grad_norm": 3.7605063915252686,
      "learning_rate": 4.485059736720442e-05,
      "loss": 0.7416,
      "step": 677200
    },
    {
      "epoch": 6.180195634717863,
      "grad_norm": 4.211974620819092,
      "learning_rate": 4.484983697106845e-05,
      "loss": 0.7279,
      "step": 677300
    },
    {
      "epoch": 6.181108110081028,
      "grad_norm": 3.7220404148101807,
      "learning_rate": 4.484907657493248e-05,
      "loss": 0.6891,
      "step": 677400
    },
    {
      "epoch": 6.182020585444193,
      "grad_norm": 3.5340213775634766,
      "learning_rate": 4.4848316178796505e-05,
      "loss": 0.6895,
      "step": 677500
    },
    {
      "epoch": 6.1829330608073585,
      "grad_norm": 3.478844165802002,
      "learning_rate": 4.484755578266054e-05,
      "loss": 0.7034,
      "step": 677600
    },
    {
      "epoch": 6.183845536170524,
      "grad_norm": 3.213836669921875,
      "learning_rate": 4.4846795386524565e-05,
      "loss": 0.7116,
      "step": 677700
    },
    {
      "epoch": 6.184758011533688,
      "grad_norm": 3.5638954639434814,
      "learning_rate": 4.4846034990388595e-05,
      "loss": 0.6755,
      "step": 677800
    },
    {
      "epoch": 6.1856704868968535,
      "grad_norm": 4.410150527954102,
      "learning_rate": 4.4845274594252625e-05,
      "loss": 0.6881,
      "step": 677900
    },
    {
      "epoch": 6.186582962260019,
      "grad_norm": 4.533857822418213,
      "learning_rate": 4.4844514198116655e-05,
      "loss": 0.6916,
      "step": 678000
    },
    {
      "epoch": 6.187495437623184,
      "grad_norm": 4.193152904510498,
      "learning_rate": 4.484375380198068e-05,
      "loss": 0.7265,
      "step": 678100
    },
    {
      "epoch": 6.188407912986349,
      "grad_norm": 2.694835662841797,
      "learning_rate": 4.4842993405844715e-05,
      "loss": 0.7225,
      "step": 678200
    },
    {
      "epoch": 6.189320388349515,
      "grad_norm": 4.0610809326171875,
      "learning_rate": 4.484223300970874e-05,
      "loss": 0.6975,
      "step": 678300
    },
    {
      "epoch": 6.19023286371268,
      "grad_norm": 4.467493534088135,
      "learning_rate": 4.484147261357277e-05,
      "loss": 0.7283,
      "step": 678400
    },
    {
      "epoch": 6.191145339075845,
      "grad_norm": 3.940521001815796,
      "learning_rate": 4.48407122174368e-05,
      "loss": 0.708,
      "step": 678500
    },
    {
      "epoch": 6.192057814439011,
      "grad_norm": 2.5274758338928223,
      "learning_rate": 4.483995182130083e-05,
      "loss": 0.6685,
      "step": 678600
    },
    {
      "epoch": 6.192970289802175,
      "grad_norm": 3.7598049640655518,
      "learning_rate": 4.483919142516486e-05,
      "loss": 0.7353,
      "step": 678700
    },
    {
      "epoch": 6.19388276516534,
      "grad_norm": 4.364467620849609,
      "learning_rate": 4.483843102902888e-05,
      "loss": 0.663,
      "step": 678800
    },
    {
      "epoch": 6.194795240528506,
      "grad_norm": 3.22174334526062,
      "learning_rate": 4.483767063289291e-05,
      "loss": 0.692,
      "step": 678900
    },
    {
      "epoch": 6.195707715891671,
      "grad_norm": 4.458311080932617,
      "learning_rate": 4.483691023675694e-05,
      "loss": 0.7018,
      "step": 679000
    },
    {
      "epoch": 6.196620191254836,
      "grad_norm": 3.959573268890381,
      "learning_rate": 4.483614984062097e-05,
      "loss": 0.7117,
      "step": 679100
    },
    {
      "epoch": 6.1975326666180015,
      "grad_norm": 4.218451976776123,
      "learning_rate": 4.4835389444484995e-05,
      "loss": 0.6612,
      "step": 679200
    },
    {
      "epoch": 6.198445141981167,
      "grad_norm": 4.324655532836914,
      "learning_rate": 4.483462904834903e-05,
      "loss": 0.6606,
      "step": 679300
    },
    {
      "epoch": 6.199357617344332,
      "grad_norm": 4.119804859161377,
      "learning_rate": 4.4833868652213056e-05,
      "loss": 0.7136,
      "step": 679400
    },
    {
      "epoch": 6.2002700927074965,
      "grad_norm": 4.173000812530518,
      "learning_rate": 4.4833108256077086e-05,
      "loss": 0.7316,
      "step": 679500
    },
    {
      "epoch": 6.201182568070662,
      "grad_norm": 3.480100154876709,
      "learning_rate": 4.4832347859941116e-05,
      "loss": 0.709,
      "step": 679600
    },
    {
      "epoch": 6.202095043433827,
      "grad_norm": 4.043144702911377,
      "learning_rate": 4.4831587463805146e-05,
      "loss": 0.7239,
      "step": 679700
    },
    {
      "epoch": 6.203007518796992,
      "grad_norm": 4.042068004608154,
      "learning_rate": 4.4830827067669176e-05,
      "loss": 0.6753,
      "step": 679800
    },
    {
      "epoch": 6.203919994160158,
      "grad_norm": 2.9047462940216064,
      "learning_rate": 4.4830066671533206e-05,
      "loss": 0.6867,
      "step": 679900
    },
    {
      "epoch": 6.204832469523323,
      "grad_norm": 4.4794921875,
      "learning_rate": 4.482930627539723e-05,
      "loss": 0.711,
      "step": 680000
    },
    {
      "epoch": 6.205744944886488,
      "grad_norm": 4.249801158905029,
      "learning_rate": 4.4828545879261266e-05,
      "loss": 0.7403,
      "step": 680100
    },
    {
      "epoch": 6.206657420249654,
      "grad_norm": 4.241300106048584,
      "learning_rate": 4.482778548312529e-05,
      "loss": 0.6767,
      "step": 680200
    },
    {
      "epoch": 6.207569895612819,
      "grad_norm": 4.116776466369629,
      "learning_rate": 4.482702508698932e-05,
      "loss": 0.6713,
      "step": 680300
    },
    {
      "epoch": 6.208482370975983,
      "grad_norm": 3.255354166030884,
      "learning_rate": 4.482626469085335e-05,
      "loss": 0.7195,
      "step": 680400
    },
    {
      "epoch": 6.209394846339149,
      "grad_norm": 4.764317989349365,
      "learning_rate": 4.482550429471738e-05,
      "loss": 0.698,
      "step": 680500
    },
    {
      "epoch": 6.210307321702314,
      "grad_norm": 4.343364238739014,
      "learning_rate": 4.48247438985814e-05,
      "loss": 0.6987,
      "step": 680600
    },
    {
      "epoch": 6.211219797065479,
      "grad_norm": 4.530627727508545,
      "learning_rate": 4.482398350244544e-05,
      "loss": 0.695,
      "step": 680700
    },
    {
      "epoch": 6.2121322724286445,
      "grad_norm": 4.776217937469482,
      "learning_rate": 4.482322310630946e-05,
      "loss": 0.7434,
      "step": 680800
    },
    {
      "epoch": 6.21304474779181,
      "grad_norm": 3.685229778289795,
      "learning_rate": 4.482246271017349e-05,
      "loss": 0.7079,
      "step": 680900
    },
    {
      "epoch": 6.213957223154975,
      "grad_norm": 3.910428047180176,
      "learning_rate": 4.482170231403752e-05,
      "loss": 0.6865,
      "step": 681000
    },
    {
      "epoch": 6.21486969851814,
      "grad_norm": 4.459231376647949,
      "learning_rate": 4.482094191790155e-05,
      "loss": 0.7162,
      "step": 681100
    },
    {
      "epoch": 6.215782173881305,
      "grad_norm": 4.01753044128418,
      "learning_rate": 4.482018152176558e-05,
      "loss": 0.6873,
      "step": 681200
    },
    {
      "epoch": 6.21669464924447,
      "grad_norm": 3.8188533782958984,
      "learning_rate": 4.481942112562961e-05,
      "loss": 0.685,
      "step": 681300
    },
    {
      "epoch": 6.217607124607635,
      "grad_norm": 4.559670448303223,
      "learning_rate": 4.4818660729493637e-05,
      "loss": 0.7399,
      "step": 681400
    },
    {
      "epoch": 6.218519599970801,
      "grad_norm": 4.889669418334961,
      "learning_rate": 4.4817900333357673e-05,
      "loss": 0.7008,
      "step": 681500
    },
    {
      "epoch": 6.219432075333966,
      "grad_norm": 3.8366520404815674,
      "learning_rate": 4.48171399372217e-05,
      "loss": 0.681,
      "step": 681600
    },
    {
      "epoch": 6.220344550697131,
      "grad_norm": 3.8906023502349854,
      "learning_rate": 4.481637954108573e-05,
      "loss": 0.7186,
      "step": 681700
    },
    {
      "epoch": 6.221257026060297,
      "grad_norm": 3.4011175632476807,
      "learning_rate": 4.481561914494976e-05,
      "loss": 0.7222,
      "step": 681800
    },
    {
      "epoch": 6.222169501423462,
      "grad_norm": 5.151028156280518,
      "learning_rate": 4.481485874881378e-05,
      "loss": 0.7096,
      "step": 681900
    },
    {
      "epoch": 6.223081976786626,
      "grad_norm": 3.9700398445129395,
      "learning_rate": 4.481409835267781e-05,
      "loss": 0.6979,
      "step": 682000
    },
    {
      "epoch": 6.223994452149792,
      "grad_norm": 3.654125690460205,
      "learning_rate": 4.481333795654184e-05,
      "loss": 0.7182,
      "step": 682100
    },
    {
      "epoch": 6.224906927512957,
      "grad_norm": 4.409035682678223,
      "learning_rate": 4.481257756040587e-05,
      "loss": 0.7206,
      "step": 682200
    },
    {
      "epoch": 6.225819402876122,
      "grad_norm": 4.139794826507568,
      "learning_rate": 4.48118171642699e-05,
      "loss": 0.6892,
      "step": 682300
    },
    {
      "epoch": 6.2267318782392875,
      "grad_norm": 3.696582555770874,
      "learning_rate": 4.481105676813393e-05,
      "loss": 0.7178,
      "step": 682400
    },
    {
      "epoch": 6.227644353602453,
      "grad_norm": 4.717710494995117,
      "learning_rate": 4.4810296371997954e-05,
      "loss": 0.6952,
      "step": 682500
    },
    {
      "epoch": 6.228556828965618,
      "grad_norm": 4.152105808258057,
      "learning_rate": 4.480953597586199e-05,
      "loss": 0.7182,
      "step": 682600
    },
    {
      "epoch": 6.229469304328783,
      "grad_norm": 3.5222718715667725,
      "learning_rate": 4.4808775579726014e-05,
      "loss": 0.6884,
      "step": 682700
    },
    {
      "epoch": 6.230381779691949,
      "grad_norm": 4.604278087615967,
      "learning_rate": 4.4808015183590044e-05,
      "loss": 0.7107,
      "step": 682800
    },
    {
      "epoch": 6.231294255055113,
      "grad_norm": 2.2207682132720947,
      "learning_rate": 4.4807254787454074e-05,
      "loss": 0.696,
      "step": 682900
    },
    {
      "epoch": 6.232206730418278,
      "grad_norm": 4.517379283905029,
      "learning_rate": 4.4806494391318104e-05,
      "loss": 0.6974,
      "step": 683000
    },
    {
      "epoch": 6.233119205781444,
      "grad_norm": 2.738898277282715,
      "learning_rate": 4.4805733995182134e-05,
      "loss": 0.6988,
      "step": 683100
    },
    {
      "epoch": 6.234031681144609,
      "grad_norm": 3.4690136909484863,
      "learning_rate": 4.4804973599046164e-05,
      "loss": 0.678,
      "step": 683200
    },
    {
      "epoch": 6.234944156507774,
      "grad_norm": 3.797952651977539,
      "learning_rate": 4.480421320291019e-05,
      "loss": 0.7317,
      "step": 683300
    },
    {
      "epoch": 6.23585663187094,
      "grad_norm": 3.9364945888519287,
      "learning_rate": 4.480345280677422e-05,
      "loss": 0.7195,
      "step": 683400
    },
    {
      "epoch": 6.236769107234105,
      "grad_norm": 4.858048439025879,
      "learning_rate": 4.480269241063825e-05,
      "loss": 0.713,
      "step": 683500
    },
    {
      "epoch": 6.23768158259727,
      "grad_norm": 5.0006022453308105,
      "learning_rate": 4.480193201450228e-05,
      "loss": 0.7468,
      "step": 683600
    },
    {
      "epoch": 6.238594057960435,
      "grad_norm": 3.763744592666626,
      "learning_rate": 4.480117161836631e-05,
      "loss": 0.663,
      "step": 683700
    },
    {
      "epoch": 6.2395065333236,
      "grad_norm": 3.4547293186187744,
      "learning_rate": 4.480041122223034e-05,
      "loss": 0.6805,
      "step": 683800
    },
    {
      "epoch": 6.240419008686765,
      "grad_norm": 3.867750644683838,
      "learning_rate": 4.479965082609436e-05,
      "loss": 0.7028,
      "step": 683900
    },
    {
      "epoch": 6.2413314840499305,
      "grad_norm": 4.372367858886719,
      "learning_rate": 4.47988904299584e-05,
      "loss": 0.7105,
      "step": 684000
    },
    {
      "epoch": 6.242243959413096,
      "grad_norm": 3.614494800567627,
      "learning_rate": 4.479813003382242e-05,
      "loss": 0.6888,
      "step": 684100
    },
    {
      "epoch": 6.243156434776261,
      "grad_norm": 4.529193878173828,
      "learning_rate": 4.479736963768645e-05,
      "loss": 0.6672,
      "step": 684200
    },
    {
      "epoch": 6.244068910139426,
      "grad_norm": 3.6282145977020264,
      "learning_rate": 4.479660924155048e-05,
      "loss": 0.6939,
      "step": 684300
    },
    {
      "epoch": 6.244981385502592,
      "grad_norm": 3.792813301086426,
      "learning_rate": 4.479584884541451e-05,
      "loss": 0.7116,
      "step": 684400
    },
    {
      "epoch": 6.245893860865757,
      "grad_norm": 4.263121128082275,
      "learning_rate": 4.479508844927854e-05,
      "loss": 0.7137,
      "step": 684500
    },
    {
      "epoch": 6.2468063362289215,
      "grad_norm": 3.2553234100341797,
      "learning_rate": 4.4794328053142565e-05,
      "loss": 0.6969,
      "step": 684600
    },
    {
      "epoch": 6.247718811592087,
      "grad_norm": 3.8036766052246094,
      "learning_rate": 4.4793567657006595e-05,
      "loss": 0.6957,
      "step": 684700
    },
    {
      "epoch": 6.248631286955252,
      "grad_norm": 3.664477825164795,
      "learning_rate": 4.4792807260870625e-05,
      "loss": 0.6717,
      "step": 684800
    },
    {
      "epoch": 6.249543762318417,
      "grad_norm": 2.89091157913208,
      "learning_rate": 4.4792046864734655e-05,
      "loss": 0.6815,
      "step": 684900
    },
    {
      "epoch": 6.250456237681583,
      "grad_norm": 3.7140140533447266,
      "learning_rate": 4.479128646859868e-05,
      "loss": 0.6736,
      "step": 685000
    },
    {
      "epoch": 6.251368713044748,
      "grad_norm": 3.9840281009674072,
      "learning_rate": 4.4790526072462715e-05,
      "loss": 0.7026,
      "step": 685100
    },
    {
      "epoch": 6.252281188407913,
      "grad_norm": 3.5763652324676514,
      "learning_rate": 4.478976567632674e-05,
      "loss": 0.7089,
      "step": 685200
    },
    {
      "epoch": 6.2531936637710785,
      "grad_norm": 3.092824697494507,
      "learning_rate": 4.478900528019077e-05,
      "loss": 0.6914,
      "step": 685300
    },
    {
      "epoch": 6.254106139134244,
      "grad_norm": 4.4153313636779785,
      "learning_rate": 4.47882448840548e-05,
      "loss": 0.7045,
      "step": 685400
    },
    {
      "epoch": 6.255018614497408,
      "grad_norm": 4.326448440551758,
      "learning_rate": 4.478748448791883e-05,
      "loss": 0.7407,
      "step": 685500
    },
    {
      "epoch": 6.255931089860574,
      "grad_norm": 4.987760066986084,
      "learning_rate": 4.478672409178286e-05,
      "loss": 0.6969,
      "step": 685600
    },
    {
      "epoch": 6.256843565223739,
      "grad_norm": 4.481852054595947,
      "learning_rate": 4.478596369564689e-05,
      "loss": 0.6981,
      "step": 685700
    },
    {
      "epoch": 6.257756040586904,
      "grad_norm": 3.9913620948791504,
      "learning_rate": 4.478520329951091e-05,
      "loss": 0.7168,
      "step": 685800
    },
    {
      "epoch": 6.2586685159500695,
      "grad_norm": 3.660290479660034,
      "learning_rate": 4.478444290337495e-05,
      "loss": 0.7215,
      "step": 685900
    },
    {
      "epoch": 6.259580991313235,
      "grad_norm": 4.193255424499512,
      "learning_rate": 4.478368250723897e-05,
      "loss": 0.6804,
      "step": 686000
    },
    {
      "epoch": 6.2604934666764,
      "grad_norm": 4.287664413452148,
      "learning_rate": 4.4782922111103e-05,
      "loss": 0.7087,
      "step": 686100
    },
    {
      "epoch": 6.2614059420395645,
      "grad_norm": 4.701836585998535,
      "learning_rate": 4.478216171496703e-05,
      "loss": 0.6667,
      "step": 686200
    },
    {
      "epoch": 6.26231841740273,
      "grad_norm": 2.7464466094970703,
      "learning_rate": 4.478140131883106e-05,
      "loss": 0.7109,
      "step": 686300
    },
    {
      "epoch": 6.263230892765895,
      "grad_norm": 3.69285249710083,
      "learning_rate": 4.4780640922695086e-05,
      "loss": 0.7382,
      "step": 686400
    },
    {
      "epoch": 6.26414336812906,
      "grad_norm": 3.2484943866729736,
      "learning_rate": 4.477988052655912e-05,
      "loss": 0.7057,
      "step": 686500
    },
    {
      "epoch": 6.265055843492226,
      "grad_norm": 4.105279445648193,
      "learning_rate": 4.4779120130423146e-05,
      "loss": 0.7154,
      "step": 686600
    },
    {
      "epoch": 6.265968318855391,
      "grad_norm": 4.311297416687012,
      "learning_rate": 4.4778359734287176e-05,
      "loss": 0.6801,
      "step": 686700
    },
    {
      "epoch": 6.266880794218556,
      "grad_norm": 5.010723114013672,
      "learning_rate": 4.4777599338151206e-05,
      "loss": 0.7063,
      "step": 686800
    },
    {
      "epoch": 6.267793269581722,
      "grad_norm": 3.332998037338257,
      "learning_rate": 4.4776838942015236e-05,
      "loss": 0.6798,
      "step": 686900
    },
    {
      "epoch": 6.268705744944887,
      "grad_norm": 3.5556843280792236,
      "learning_rate": 4.4776078545879266e-05,
      "loss": 0.7582,
      "step": 687000
    },
    {
      "epoch": 6.269618220308051,
      "grad_norm": 4.221836090087891,
      "learning_rate": 4.4775318149743296e-05,
      "loss": 0.6818,
      "step": 687100
    },
    {
      "epoch": 6.270530695671217,
      "grad_norm": 3.8253846168518066,
      "learning_rate": 4.477455775360732e-05,
      "loss": 0.7017,
      "step": 687200
    },
    {
      "epoch": 6.271443171034382,
      "grad_norm": 4.304008483886719,
      "learning_rate": 4.4773797357471356e-05,
      "loss": 0.6706,
      "step": 687300
    },
    {
      "epoch": 6.272355646397547,
      "grad_norm": 4.716495037078857,
      "learning_rate": 4.477303696133538e-05,
      "loss": 0.7344,
      "step": 687400
    },
    {
      "epoch": 6.2732681217607125,
      "grad_norm": 4.085420608520508,
      "learning_rate": 4.47722765651994e-05,
      "loss": 0.7013,
      "step": 687500
    },
    {
      "epoch": 6.274180597123878,
      "grad_norm": 4.057220458984375,
      "learning_rate": 4.477151616906344e-05,
      "loss": 0.6912,
      "step": 687600
    },
    {
      "epoch": 6.275093072487043,
      "grad_norm": 2.05401873588562,
      "learning_rate": 4.477075577292746e-05,
      "loss": 0.6794,
      "step": 687700
    },
    {
      "epoch": 6.276005547850208,
      "grad_norm": 4.365731716156006,
      "learning_rate": 4.476999537679149e-05,
      "loss": 0.7196,
      "step": 687800
    },
    {
      "epoch": 6.276918023213373,
      "grad_norm": 3.7508161067962646,
      "learning_rate": 4.476923498065552e-05,
      "loss": 0.7075,
      "step": 687900
    },
    {
      "epoch": 6.277830498576538,
      "grad_norm": 3.664736270904541,
      "learning_rate": 4.476847458451955e-05,
      "loss": 0.6964,
      "step": 688000
    },
    {
      "epoch": 6.278742973939703,
      "grad_norm": 3.7734909057617188,
      "learning_rate": 4.476771418838358e-05,
      "loss": 0.6831,
      "step": 688100
    },
    {
      "epoch": 6.279655449302869,
      "grad_norm": 4.2096476554870605,
      "learning_rate": 4.476695379224761e-05,
      "loss": 0.6889,
      "step": 688200
    },
    {
      "epoch": 6.280567924666034,
      "grad_norm": 4.114940643310547,
      "learning_rate": 4.476619339611164e-05,
      "loss": 0.6878,
      "step": 688300
    },
    {
      "epoch": 6.281480400029199,
      "grad_norm": 4.008427619934082,
      "learning_rate": 4.4765432999975674e-05,
      "loss": 0.6951,
      "step": 688400
    },
    {
      "epoch": 6.282392875392365,
      "grad_norm": 3.901338577270508,
      "learning_rate": 4.47646726038397e-05,
      "loss": 0.7645,
      "step": 688500
    },
    {
      "epoch": 6.28330535075553,
      "grad_norm": 4.607513904571533,
      "learning_rate": 4.476391220770373e-05,
      "loss": 0.6871,
      "step": 688600
    },
    {
      "epoch": 6.284217826118695,
      "grad_norm": 4.559618949890137,
      "learning_rate": 4.476315181156776e-05,
      "loss": 0.7057,
      "step": 688700
    },
    {
      "epoch": 6.28513030148186,
      "grad_norm": 4.2690019607543945,
      "learning_rate": 4.476239141543179e-05,
      "loss": 0.7004,
      "step": 688800
    },
    {
      "epoch": 6.286042776845025,
      "grad_norm": 4.131249904632568,
      "learning_rate": 4.476163101929581e-05,
      "loss": 0.6523,
      "step": 688900
    },
    {
      "epoch": 6.28695525220819,
      "grad_norm": 4.3812255859375,
      "learning_rate": 4.476087062315985e-05,
      "loss": 0.6886,
      "step": 689000
    },
    {
      "epoch": 6.2878677275713555,
      "grad_norm": 4.43854284286499,
      "learning_rate": 4.476011022702387e-05,
      "loss": 0.6709,
      "step": 689100
    },
    {
      "epoch": 6.288780202934521,
      "grad_norm": 4.037818908691406,
      "learning_rate": 4.47593498308879e-05,
      "loss": 0.692,
      "step": 689200
    },
    {
      "epoch": 6.289692678297686,
      "grad_norm": 3.4381651878356934,
      "learning_rate": 4.475858943475193e-05,
      "loss": 0.6701,
      "step": 689300
    },
    {
      "epoch": 6.290605153660851,
      "grad_norm": 4.327359676361084,
      "learning_rate": 4.475782903861596e-05,
      "loss": 0.7013,
      "step": 689400
    },
    {
      "epoch": 6.291517629024017,
      "grad_norm": 3.9035091400146484,
      "learning_rate": 4.475706864247999e-05,
      "loss": 0.7219,
      "step": 689500
    },
    {
      "epoch": 6.292430104387181,
      "grad_norm": 3.9971132278442383,
      "learning_rate": 4.475630824634402e-05,
      "loss": 0.7123,
      "step": 689600
    },
    {
      "epoch": 6.293342579750346,
      "grad_norm": 3.9607608318328857,
      "learning_rate": 4.4755547850208044e-05,
      "loss": 0.7225,
      "step": 689700
    },
    {
      "epoch": 6.294255055113512,
      "grad_norm": 3.2775864601135254,
      "learning_rate": 4.475478745407208e-05,
      "loss": 0.6718,
      "step": 689800
    },
    {
      "epoch": 6.295167530476677,
      "grad_norm": 4.625495910644531,
      "learning_rate": 4.4754027057936104e-05,
      "loss": 0.7343,
      "step": 689900
    },
    {
      "epoch": 6.296080005839842,
      "grad_norm": 4.961603164672852,
      "learning_rate": 4.4753266661800134e-05,
      "loss": 0.7166,
      "step": 690000
    },
    {
      "epoch": 6.296992481203008,
      "grad_norm": 3.2786524295806885,
      "learning_rate": 4.4752506265664164e-05,
      "loss": 0.7001,
      "step": 690100
    },
    {
      "epoch": 6.297904956566173,
      "grad_norm": 4.236326217651367,
      "learning_rate": 4.475174586952819e-05,
      "loss": 0.7005,
      "step": 690200
    },
    {
      "epoch": 6.298817431929338,
      "grad_norm": 2.969337224960327,
      "learning_rate": 4.475098547339222e-05,
      "loss": 0.7213,
      "step": 690300
    },
    {
      "epoch": 6.2997299072925035,
      "grad_norm": 5.126743793487549,
      "learning_rate": 4.475022507725625e-05,
      "loss": 0.7272,
      "step": 690400
    },
    {
      "epoch": 6.300642382655668,
      "grad_norm": 3.9846060276031494,
      "learning_rate": 4.474946468112028e-05,
      "loss": 0.7178,
      "step": 690500
    },
    {
      "epoch": 6.301554858018833,
      "grad_norm": 3.7530550956726074,
      "learning_rate": 4.474870428498431e-05,
      "loss": 0.6778,
      "step": 690600
    },
    {
      "epoch": 6.3024673333819985,
      "grad_norm": 3.2550301551818848,
      "learning_rate": 4.474794388884834e-05,
      "loss": 0.6965,
      "step": 690700
    },
    {
      "epoch": 6.303379808745164,
      "grad_norm": 5.077033996582031,
      "learning_rate": 4.474718349271236e-05,
      "loss": 0.7002,
      "step": 690800
    },
    {
      "epoch": 6.304292284108329,
      "grad_norm": 4.543318271636963,
      "learning_rate": 4.47464230965764e-05,
      "loss": 0.7022,
      "step": 690900
    },
    {
      "epoch": 6.305204759471494,
      "grad_norm": 107.75345611572266,
      "learning_rate": 4.474566270044042e-05,
      "loss": 0.7148,
      "step": 691000
    },
    {
      "epoch": 6.30611723483466,
      "grad_norm": 4.045220375061035,
      "learning_rate": 4.474490230430445e-05,
      "loss": 0.7262,
      "step": 691100
    },
    {
      "epoch": 6.307029710197825,
      "grad_norm": 3.9484450817108154,
      "learning_rate": 4.474414190816848e-05,
      "loss": 0.6723,
      "step": 691200
    },
    {
      "epoch": 6.307942185560989,
      "grad_norm": 4.016618251800537,
      "learning_rate": 4.474338151203251e-05,
      "loss": 0.6867,
      "step": 691300
    },
    {
      "epoch": 6.308854660924155,
      "grad_norm": 4.630461692810059,
      "learning_rate": 4.4742621115896535e-05,
      "loss": 0.6669,
      "step": 691400
    },
    {
      "epoch": 6.30976713628732,
      "grad_norm": 4.176901817321777,
      "learning_rate": 4.474186071976057e-05,
      "loss": 0.7223,
      "step": 691500
    },
    {
      "epoch": 6.310679611650485,
      "grad_norm": 3.7960195541381836,
      "learning_rate": 4.4741100323624595e-05,
      "loss": 0.7099,
      "step": 691600
    },
    {
      "epoch": 6.311592087013651,
      "grad_norm": 3.717036724090576,
      "learning_rate": 4.4740339927488625e-05,
      "loss": 0.7074,
      "step": 691700
    },
    {
      "epoch": 6.312504562376816,
      "grad_norm": 4.456057548522949,
      "learning_rate": 4.4739579531352655e-05,
      "loss": 0.7129,
      "step": 691800
    },
    {
      "epoch": 6.313417037739981,
      "grad_norm": 3.6421706676483154,
      "learning_rate": 4.4738819135216685e-05,
      "loss": 0.7243,
      "step": 691900
    },
    {
      "epoch": 6.3143295131031465,
      "grad_norm": 3.7030856609344482,
      "learning_rate": 4.4738058739080715e-05,
      "loss": 0.6925,
      "step": 692000
    },
    {
      "epoch": 6.315241988466312,
      "grad_norm": 4.182947158813477,
      "learning_rate": 4.4737298342944745e-05,
      "loss": 0.7067,
      "step": 692100
    },
    {
      "epoch": 6.316154463829476,
      "grad_norm": 3.6232457160949707,
      "learning_rate": 4.473653794680877e-05,
      "loss": 0.6942,
      "step": 692200
    },
    {
      "epoch": 6.3170669391926415,
      "grad_norm": 5.877106189727783,
      "learning_rate": 4.4735777550672805e-05,
      "loss": 0.695,
      "step": 692300
    },
    {
      "epoch": 6.317979414555807,
      "grad_norm": 3.3420276641845703,
      "learning_rate": 4.473501715453683e-05,
      "loss": 0.7059,
      "step": 692400
    },
    {
      "epoch": 6.318891889918972,
      "grad_norm": 2.900930881500244,
      "learning_rate": 4.473425675840086e-05,
      "loss": 0.6895,
      "step": 692500
    },
    {
      "epoch": 6.319804365282137,
      "grad_norm": 4.071321964263916,
      "learning_rate": 4.473349636226489e-05,
      "loss": 0.7089,
      "step": 692600
    },
    {
      "epoch": 6.320716840645303,
      "grad_norm": 3.818143606185913,
      "learning_rate": 4.473273596612892e-05,
      "loss": 0.6911,
      "step": 692700
    },
    {
      "epoch": 6.321629316008468,
      "grad_norm": 2.466346263885498,
      "learning_rate": 4.473197556999294e-05,
      "loss": 0.682,
      "step": 692800
    },
    {
      "epoch": 6.322541791371633,
      "grad_norm": 3.56526780128479,
      "learning_rate": 4.473121517385698e-05,
      "loss": 0.7256,
      "step": 692900
    },
    {
      "epoch": 6.323454266734798,
      "grad_norm": 4.257353782653809,
      "learning_rate": 4.4730454777721e-05,
      "loss": 0.7478,
      "step": 693000
    },
    {
      "epoch": 6.324366742097963,
      "grad_norm": 3.669778347015381,
      "learning_rate": 4.472969438158503e-05,
      "loss": 0.733,
      "step": 693100
    },
    {
      "epoch": 6.325279217461128,
      "grad_norm": 3.126323938369751,
      "learning_rate": 4.472893398544906e-05,
      "loss": 0.735,
      "step": 693200
    },
    {
      "epoch": 6.326191692824294,
      "grad_norm": 3.3292510509490967,
      "learning_rate": 4.4728173589313086e-05,
      "loss": 0.6997,
      "step": 693300
    },
    {
      "epoch": 6.327104168187459,
      "grad_norm": 4.698624134063721,
      "learning_rate": 4.472741319317712e-05,
      "loss": 0.7118,
      "step": 693400
    },
    {
      "epoch": 6.328016643550624,
      "grad_norm": 3.909900188446045,
      "learning_rate": 4.4726652797041146e-05,
      "loss": 0.7034,
      "step": 693500
    },
    {
      "epoch": 6.3289291189137895,
      "grad_norm": 4.3546061515808105,
      "learning_rate": 4.4725892400905176e-05,
      "loss": 0.6809,
      "step": 693600
    },
    {
      "epoch": 6.329841594276955,
      "grad_norm": 4.520951747894287,
      "learning_rate": 4.4725132004769206e-05,
      "loss": 0.6728,
      "step": 693700
    },
    {
      "epoch": 6.33075406964012,
      "grad_norm": 3.9306085109710693,
      "learning_rate": 4.4724371608633236e-05,
      "loss": 0.7356,
      "step": 693800
    },
    {
      "epoch": 6.3316665450032845,
      "grad_norm": 4.1754631996154785,
      "learning_rate": 4.472361121249726e-05,
      "loss": 0.7008,
      "step": 693900
    },
    {
      "epoch": 6.33257902036645,
      "grad_norm": 3.8919517993927,
      "learning_rate": 4.4722850816361296e-05,
      "loss": 0.7106,
      "step": 694000
    },
    {
      "epoch": 6.333491495729615,
      "grad_norm": 4.420609474182129,
      "learning_rate": 4.472209042022532e-05,
      "loss": 0.6872,
      "step": 694100
    },
    {
      "epoch": 6.33440397109278,
      "grad_norm": 3.2384567260742188,
      "learning_rate": 4.472133002408935e-05,
      "loss": 0.7025,
      "step": 694200
    },
    {
      "epoch": 6.335316446455946,
      "grad_norm": 3.1641340255737305,
      "learning_rate": 4.472056962795338e-05,
      "loss": 0.7011,
      "step": 694300
    },
    {
      "epoch": 6.336228921819111,
      "grad_norm": 4.882298946380615,
      "learning_rate": 4.471980923181741e-05,
      "loss": 0.7402,
      "step": 694400
    },
    {
      "epoch": 6.337141397182276,
      "grad_norm": 4.924520015716553,
      "learning_rate": 4.471904883568144e-05,
      "loss": 0.6742,
      "step": 694500
    },
    {
      "epoch": 6.338053872545442,
      "grad_norm": 3.2264628410339355,
      "learning_rate": 4.471828843954547e-05,
      "loss": 0.668,
      "step": 694600
    },
    {
      "epoch": 6.338966347908606,
      "grad_norm": 4.11074686050415,
      "learning_rate": 4.471752804340949e-05,
      "loss": 0.7315,
      "step": 694700
    },
    {
      "epoch": 6.339878823271771,
      "grad_norm": 2.8160958290100098,
      "learning_rate": 4.471676764727353e-05,
      "loss": 0.6831,
      "step": 694800
    },
    {
      "epoch": 6.340791298634937,
      "grad_norm": 3.4243783950805664,
      "learning_rate": 4.471600725113755e-05,
      "loss": 0.7038,
      "step": 694900
    },
    {
      "epoch": 6.341703773998102,
      "grad_norm": 4.318795204162598,
      "learning_rate": 4.4715246855001583e-05,
      "loss": 0.6482,
      "step": 695000
    },
    {
      "epoch": 6.342616249361267,
      "grad_norm": 4.3765058517456055,
      "learning_rate": 4.4714486458865613e-05,
      "loss": 0.7378,
      "step": 695100
    },
    {
      "epoch": 6.3435287247244325,
      "grad_norm": 4.888632774353027,
      "learning_rate": 4.4713726062729644e-05,
      "loss": 0.6901,
      "step": 695200
    },
    {
      "epoch": 6.344441200087598,
      "grad_norm": 3.5568912029266357,
      "learning_rate": 4.4712965666593674e-05,
      "loss": 0.7229,
      "step": 695300
    },
    {
      "epoch": 6.345353675450763,
      "grad_norm": 4.204281330108643,
      "learning_rate": 4.4712205270457704e-05,
      "loss": 0.6985,
      "step": 695400
    },
    {
      "epoch": 6.346266150813928,
      "grad_norm": 4.79545259475708,
      "learning_rate": 4.471144487432173e-05,
      "loss": 0.7078,
      "step": 695500
    },
    {
      "epoch": 6.347178626177093,
      "grad_norm": 4.159609794616699,
      "learning_rate": 4.471068447818576e-05,
      "loss": 0.7018,
      "step": 695600
    },
    {
      "epoch": 6.348091101540258,
      "grad_norm": 4.41904354095459,
      "learning_rate": 4.470992408204979e-05,
      "loss": 0.7343,
      "step": 695700
    },
    {
      "epoch": 6.349003576903423,
      "grad_norm": 3.860182762145996,
      "learning_rate": 4.470916368591382e-05,
      "loss": 0.7109,
      "step": 695800
    },
    {
      "epoch": 6.349916052266589,
      "grad_norm": 3.7417871952056885,
      "learning_rate": 4.470840328977785e-05,
      "loss": 0.6982,
      "step": 695900
    },
    {
      "epoch": 6.350828527629754,
      "grad_norm": 5.232280731201172,
      "learning_rate": 4.470764289364187e-05,
      "loss": 0.7242,
      "step": 696000
    },
    {
      "epoch": 6.351741002992919,
      "grad_norm": 4.830164909362793,
      "learning_rate": 4.47068824975059e-05,
      "loss": 0.6914,
      "step": 696100
    },
    {
      "epoch": 6.352653478356085,
      "grad_norm": 3.937892436981201,
      "learning_rate": 4.470612210136993e-05,
      "loss": 0.7263,
      "step": 696200
    },
    {
      "epoch": 6.35356595371925,
      "grad_norm": 4.952809810638428,
      "learning_rate": 4.470536170523396e-05,
      "loss": 0.7126,
      "step": 696300
    },
    {
      "epoch": 6.354478429082414,
      "grad_norm": 3.408900737762451,
      "learning_rate": 4.470460130909799e-05,
      "loss": 0.7022,
      "step": 696400
    },
    {
      "epoch": 6.35539090444558,
      "grad_norm": 4.470297813415527,
      "learning_rate": 4.470384091296202e-05,
      "loss": 0.6509,
      "step": 696500
    },
    {
      "epoch": 6.356303379808745,
      "grad_norm": 4.335177421569824,
      "learning_rate": 4.4703080516826044e-05,
      "loss": 0.6969,
      "step": 696600
    },
    {
      "epoch": 6.35721585517191,
      "grad_norm": 3.9134209156036377,
      "learning_rate": 4.470232012069008e-05,
      "loss": 0.7326,
      "step": 696700
    },
    {
      "epoch": 6.3581283305350755,
      "grad_norm": 3.746108293533325,
      "learning_rate": 4.4701559724554104e-05,
      "loss": 0.7094,
      "step": 696800
    },
    {
      "epoch": 6.359040805898241,
      "grad_norm": 4.282281398773193,
      "learning_rate": 4.4700799328418134e-05,
      "loss": 0.709,
      "step": 696900
    },
    {
      "epoch": 6.359953281261406,
      "grad_norm": 3.9485955238342285,
      "learning_rate": 4.4700038932282164e-05,
      "loss": 0.6693,
      "step": 697000
    },
    {
      "epoch": 6.360865756624571,
      "grad_norm": 3.5442638397216797,
      "learning_rate": 4.4699278536146195e-05,
      "loss": 0.7155,
      "step": 697100
    },
    {
      "epoch": 6.361778231987737,
      "grad_norm": 3.1407370567321777,
      "learning_rate": 4.469851814001022e-05,
      "loss": 0.7214,
      "step": 697200
    },
    {
      "epoch": 6.362690707350901,
      "grad_norm": 4.25857400894165,
      "learning_rate": 4.4697757743874255e-05,
      "loss": 0.6976,
      "step": 697300
    },
    {
      "epoch": 6.363603182714066,
      "grad_norm": 5.162753582000732,
      "learning_rate": 4.469699734773828e-05,
      "loss": 0.7095,
      "step": 697400
    },
    {
      "epoch": 6.364515658077232,
      "grad_norm": 4.406615734100342,
      "learning_rate": 4.469623695160231e-05,
      "loss": 0.7047,
      "step": 697500
    },
    {
      "epoch": 6.365428133440397,
      "grad_norm": 4.244827747344971,
      "learning_rate": 4.469547655546634e-05,
      "loss": 0.7219,
      "step": 697600
    },
    {
      "epoch": 6.366340608803562,
      "grad_norm": 4.456433296203613,
      "learning_rate": 4.469471615933037e-05,
      "loss": 0.6806,
      "step": 697700
    },
    {
      "epoch": 6.367253084166728,
      "grad_norm": 4.144738674163818,
      "learning_rate": 4.46939557631944e-05,
      "loss": 0.7357,
      "step": 697800
    },
    {
      "epoch": 6.368165559529893,
      "grad_norm": 4.599618434906006,
      "learning_rate": 4.469319536705843e-05,
      "loss": 0.7156,
      "step": 697900
    },
    {
      "epoch": 6.369078034893058,
      "grad_norm": 4.319106578826904,
      "learning_rate": 4.469243497092245e-05,
      "loss": 0.7241,
      "step": 698000
    },
    {
      "epoch": 6.369990510256223,
      "grad_norm": 4.246203899383545,
      "learning_rate": 4.469167457478649e-05,
      "loss": 0.7027,
      "step": 698100
    },
    {
      "epoch": 6.370902985619388,
      "grad_norm": 4.522202014923096,
      "learning_rate": 4.469091417865051e-05,
      "loss": 0.6987,
      "step": 698200
    },
    {
      "epoch": 6.371815460982553,
      "grad_norm": 4.162022113800049,
      "learning_rate": 4.469015378251454e-05,
      "loss": 0.6961,
      "step": 698300
    },
    {
      "epoch": 6.3727279363457185,
      "grad_norm": 4.788463115692139,
      "learning_rate": 4.468939338637857e-05,
      "loss": 0.7223,
      "step": 698400
    },
    {
      "epoch": 6.373640411708884,
      "grad_norm": 4.3066630363464355,
      "learning_rate": 4.46886329902426e-05,
      "loss": 0.6997,
      "step": 698500
    },
    {
      "epoch": 6.374552887072049,
      "grad_norm": 4.047451019287109,
      "learning_rate": 4.4687872594106625e-05,
      "loss": 0.7094,
      "step": 698600
    },
    {
      "epoch": 6.3754653624352144,
      "grad_norm": 4.034038066864014,
      "learning_rate": 4.4687112197970655e-05,
      "loss": 0.7053,
      "step": 698700
    },
    {
      "epoch": 6.37637783779838,
      "grad_norm": 3.231065273284912,
      "learning_rate": 4.4686351801834685e-05,
      "loss": 0.7093,
      "step": 698800
    },
    {
      "epoch": 6.377290313161545,
      "grad_norm": 3.718654155731201,
      "learning_rate": 4.4685591405698715e-05,
      "loss": 0.6843,
      "step": 698900
    },
    {
      "epoch": 6.3782027885247095,
      "grad_norm": 3.716557264328003,
      "learning_rate": 4.4684831009562745e-05,
      "loss": 0.7419,
      "step": 699000
    },
    {
      "epoch": 6.379115263887875,
      "grad_norm": 4.035191535949707,
      "learning_rate": 4.468407061342677e-05,
      "loss": 0.7179,
      "step": 699100
    },
    {
      "epoch": 6.38002773925104,
      "grad_norm": 4.0738420486450195,
      "learning_rate": 4.4683310217290806e-05,
      "loss": 0.6972,
      "step": 699200
    },
    {
      "epoch": 6.380940214614205,
      "grad_norm": 4.558052062988281,
      "learning_rate": 4.468254982115483e-05,
      "loss": 0.6857,
      "step": 699300
    },
    {
      "epoch": 6.381852689977371,
      "grad_norm": 3.6057064533233643,
      "learning_rate": 4.468178942501886e-05,
      "loss": 0.6949,
      "step": 699400
    },
    {
      "epoch": 6.382765165340536,
      "grad_norm": 3.573723554611206,
      "learning_rate": 4.468102902888289e-05,
      "loss": 0.7227,
      "step": 699500
    },
    {
      "epoch": 6.383677640703701,
      "grad_norm": 3.278791904449463,
      "learning_rate": 4.468026863274692e-05,
      "loss": 0.7129,
      "step": 699600
    },
    {
      "epoch": 6.3845901160668665,
      "grad_norm": 3.2907752990722656,
      "learning_rate": 4.467950823661094e-05,
      "loss": 0.7334,
      "step": 699700
    },
    {
      "epoch": 6.385502591430031,
      "grad_norm": 4.099874973297119,
      "learning_rate": 4.467874784047498e-05,
      "loss": 0.7059,
      "step": 699800
    },
    {
      "epoch": 6.386415066793196,
      "grad_norm": 3.785590171813965,
      "learning_rate": 4.4677987444339e-05,
      "loss": 0.6619,
      "step": 699900
    },
    {
      "epoch": 6.387327542156362,
      "grad_norm": 4.491004943847656,
      "learning_rate": 4.467722704820303e-05,
      "loss": 0.6646,
      "step": 700000
    },
    {
      "epoch": 6.388240017519527,
      "grad_norm": 4.648149490356445,
      "learning_rate": 4.467646665206706e-05,
      "loss": 0.7198,
      "step": 700100
    },
    {
      "epoch": 6.389152492882692,
      "grad_norm": 4.0936994552612305,
      "learning_rate": 4.467570625593109e-05,
      "loss": 0.6922,
      "step": 700200
    },
    {
      "epoch": 6.3900649682458575,
      "grad_norm": 3.7014830112457275,
      "learning_rate": 4.467494585979512e-05,
      "loss": 0.6489,
      "step": 700300
    },
    {
      "epoch": 6.390977443609023,
      "grad_norm": 3.5355026721954346,
      "learning_rate": 4.467418546365915e-05,
      "loss": 0.7419,
      "step": 700400
    },
    {
      "epoch": 6.391889918972188,
      "grad_norm": 3.2693264484405518,
      "learning_rate": 4.4673425067523176e-05,
      "loss": 0.72,
      "step": 700500
    },
    {
      "epoch": 6.392802394335353,
      "grad_norm": 4.375055313110352,
      "learning_rate": 4.467266467138721e-05,
      "loss": 0.6798,
      "step": 700600
    },
    {
      "epoch": 6.393714869698518,
      "grad_norm": 3.9935219287872314,
      "learning_rate": 4.4671904275251236e-05,
      "loss": 0.7258,
      "step": 700700
    },
    {
      "epoch": 6.394627345061683,
      "grad_norm": 4.321221828460693,
      "learning_rate": 4.4671143879115266e-05,
      "loss": 0.6873,
      "step": 700800
    },
    {
      "epoch": 6.395539820424848,
      "grad_norm": 4.185027122497559,
      "learning_rate": 4.4670383482979296e-05,
      "loss": 0.694,
      "step": 700900
    },
    {
      "epoch": 6.396452295788014,
      "grad_norm": 5.1293044090271,
      "learning_rate": 4.4669623086843326e-05,
      "loss": 0.6978,
      "step": 701000
    },
    {
      "epoch": 6.397364771151179,
      "grad_norm": 4.277797698974609,
      "learning_rate": 4.466886269070735e-05,
      "loss": 0.6961,
      "step": 701100
    },
    {
      "epoch": 6.398277246514344,
      "grad_norm": 3.502347707748413,
      "learning_rate": 4.4668102294571387e-05,
      "loss": 0.7395,
      "step": 701200
    },
    {
      "epoch": 6.39918972187751,
      "grad_norm": 3.7673144340515137,
      "learning_rate": 4.466734189843541e-05,
      "loss": 0.7119,
      "step": 701300
    },
    {
      "epoch": 6.400102197240675,
      "grad_norm": 4.254380226135254,
      "learning_rate": 4.466658150229944e-05,
      "loss": 0.695,
      "step": 701400
    },
    {
      "epoch": 6.401014672603839,
      "grad_norm": 4.1248626708984375,
      "learning_rate": 4.466582110616347e-05,
      "loss": 0.7003,
      "step": 701500
    },
    {
      "epoch": 6.401927147967005,
      "grad_norm": 3.5167407989501953,
      "learning_rate": 4.466506071002749e-05,
      "loss": 0.6925,
      "step": 701600
    },
    {
      "epoch": 6.40283962333017,
      "grad_norm": 4.531363487243652,
      "learning_rate": 4.466430031389153e-05,
      "loss": 0.7,
      "step": 701700
    },
    {
      "epoch": 6.403752098693335,
      "grad_norm": 2.416163444519043,
      "learning_rate": 4.4663539917755553e-05,
      "loss": 0.6959,
      "step": 701800
    },
    {
      "epoch": 6.4046645740565005,
      "grad_norm": 3.4445013999938965,
      "learning_rate": 4.4662779521619584e-05,
      "loss": 0.7375,
      "step": 701900
    },
    {
      "epoch": 6.405577049419666,
      "grad_norm": 4.013191223144531,
      "learning_rate": 4.4662019125483614e-05,
      "loss": 0.691,
      "step": 702000
    },
    {
      "epoch": 6.406489524782831,
      "grad_norm": 3.789790391921997,
      "learning_rate": 4.4661258729347644e-05,
      "loss": 0.7012,
      "step": 702100
    },
    {
      "epoch": 6.407402000145996,
      "grad_norm": 4.513006687164307,
      "learning_rate": 4.466049833321167e-05,
      "loss": 0.7079,
      "step": 702200
    },
    {
      "epoch": 6.408314475509162,
      "grad_norm": 4.636962890625,
      "learning_rate": 4.4659737937075704e-05,
      "loss": 0.7046,
      "step": 702300
    },
    {
      "epoch": 6.409226950872326,
      "grad_norm": 4.280466556549072,
      "learning_rate": 4.465897754093973e-05,
      "loss": 0.7143,
      "step": 702400
    },
    {
      "epoch": 6.410139426235491,
      "grad_norm": 3.1998250484466553,
      "learning_rate": 4.465821714480376e-05,
      "loss": 0.7248,
      "step": 702500
    },
    {
      "epoch": 6.411051901598657,
      "grad_norm": 3.336534261703491,
      "learning_rate": 4.465745674866779e-05,
      "loss": 0.7506,
      "step": 702600
    },
    {
      "epoch": 6.411964376961822,
      "grad_norm": 3.915518283843994,
      "learning_rate": 4.465669635253182e-05,
      "loss": 0.6753,
      "step": 702700
    },
    {
      "epoch": 6.412876852324987,
      "grad_norm": 4.794569492340088,
      "learning_rate": 4.465593595639585e-05,
      "loss": 0.7333,
      "step": 702800
    },
    {
      "epoch": 6.413789327688153,
      "grad_norm": 4.10563325881958,
      "learning_rate": 4.465517556025988e-05,
      "loss": 0.7205,
      "step": 702900
    },
    {
      "epoch": 6.414701803051318,
      "grad_norm": 5.2587456703186035,
      "learning_rate": 4.46544151641239e-05,
      "loss": 0.703,
      "step": 703000
    },
    {
      "epoch": 6.415614278414483,
      "grad_norm": 4.324973106384277,
      "learning_rate": 4.465365476798794e-05,
      "loss": 0.687,
      "step": 703100
    },
    {
      "epoch": 6.416526753777648,
      "grad_norm": 4.139885425567627,
      "learning_rate": 4.465289437185196e-05,
      "loss": 0.6789,
      "step": 703200
    },
    {
      "epoch": 6.417439229140813,
      "grad_norm": 3.921635150909424,
      "learning_rate": 4.465213397571599e-05,
      "loss": 0.7088,
      "step": 703300
    },
    {
      "epoch": 6.418351704503978,
      "grad_norm": 3.455991744995117,
      "learning_rate": 4.465137357958002e-05,
      "loss": 0.7194,
      "step": 703400
    },
    {
      "epoch": 6.4192641798671435,
      "grad_norm": 3.4323253631591797,
      "learning_rate": 4.465061318344405e-05,
      "loss": 0.6734,
      "step": 703500
    },
    {
      "epoch": 6.420176655230309,
      "grad_norm": 4.738193988800049,
      "learning_rate": 4.4649852787308074e-05,
      "loss": 0.6657,
      "step": 703600
    },
    {
      "epoch": 6.421089130593474,
      "grad_norm": 3.491605043411255,
      "learning_rate": 4.464909239117211e-05,
      "loss": 0.6721,
      "step": 703700
    },
    {
      "epoch": 6.422001605956639,
      "grad_norm": 5.177174091339111,
      "learning_rate": 4.4648331995036134e-05,
      "loss": 0.7146,
      "step": 703800
    },
    {
      "epoch": 6.422914081319805,
      "grad_norm": 4.628250598907471,
      "learning_rate": 4.4647571598900165e-05,
      "loss": 0.7284,
      "step": 703900
    },
    {
      "epoch": 6.42382655668297,
      "grad_norm": 4.67479133605957,
      "learning_rate": 4.4646811202764195e-05,
      "loss": 0.6803,
      "step": 704000
    },
    {
      "epoch": 6.424739032046134,
      "grad_norm": 4.41634464263916,
      "learning_rate": 4.4646050806628225e-05,
      "loss": 0.7182,
      "step": 704100
    },
    {
      "epoch": 6.4256515074093,
      "grad_norm": 5.128959655761719,
      "learning_rate": 4.4645290410492255e-05,
      "loss": 0.7288,
      "step": 704200
    },
    {
      "epoch": 6.426563982772465,
      "grad_norm": 3.6928679943084717,
      "learning_rate": 4.4644530014356285e-05,
      "loss": 0.6768,
      "step": 704300
    },
    {
      "epoch": 6.42747645813563,
      "grad_norm": 6.137889385223389,
      "learning_rate": 4.464376961822031e-05,
      "loss": 0.6995,
      "step": 704400
    },
    {
      "epoch": 6.428388933498796,
      "grad_norm": 3.460669994354248,
      "learning_rate": 4.464300922208434e-05,
      "loss": 0.7444,
      "step": 704500
    },
    {
      "epoch": 6.429301408861961,
      "grad_norm": 4.436398029327393,
      "learning_rate": 4.464224882594837e-05,
      "loss": 0.7073,
      "step": 704600
    },
    {
      "epoch": 6.430213884225126,
      "grad_norm": 3.3492722511291504,
      "learning_rate": 4.464148842981239e-05,
      "loss": 0.6946,
      "step": 704700
    },
    {
      "epoch": 6.4311263595882915,
      "grad_norm": 3.972666025161743,
      "learning_rate": 4.464072803367643e-05,
      "loss": 0.7012,
      "step": 704800
    },
    {
      "epoch": 6.432038834951456,
      "grad_norm": 3.8245604038238525,
      "learning_rate": 4.463996763754045e-05,
      "loss": 0.6743,
      "step": 704900
    },
    {
      "epoch": 6.432951310314621,
      "grad_norm": 4.450750350952148,
      "learning_rate": 4.463920724140448e-05,
      "loss": 0.7425,
      "step": 705000
    },
    {
      "epoch": 6.4338637856777865,
      "grad_norm": 4.496067523956299,
      "learning_rate": 4.463844684526851e-05,
      "loss": 0.7184,
      "step": 705100
    },
    {
      "epoch": 6.434776261040952,
      "grad_norm": 3.7107722759246826,
      "learning_rate": 4.463768644913254e-05,
      "loss": 0.7178,
      "step": 705200
    },
    {
      "epoch": 6.435688736404117,
      "grad_norm": 8.249271392822266,
      "learning_rate": 4.463692605299657e-05,
      "loss": 0.6862,
      "step": 705300
    },
    {
      "epoch": 6.436601211767282,
      "grad_norm": 4.394241809844971,
      "learning_rate": 4.46361656568606e-05,
      "loss": 0.7003,
      "step": 705400
    },
    {
      "epoch": 6.437513687130448,
      "grad_norm": 4.176745414733887,
      "learning_rate": 4.4635405260724625e-05,
      "loss": 0.6917,
      "step": 705500
    },
    {
      "epoch": 6.438426162493613,
      "grad_norm": 2.493168830871582,
      "learning_rate": 4.463464486458866e-05,
      "loss": 0.7033,
      "step": 705600
    },
    {
      "epoch": 6.439338637856778,
      "grad_norm": 5.0042314529418945,
      "learning_rate": 4.4633884468452685e-05,
      "loss": 0.7017,
      "step": 705700
    },
    {
      "epoch": 6.440251113219943,
      "grad_norm": 2.857969284057617,
      "learning_rate": 4.4633124072316715e-05,
      "loss": 0.6936,
      "step": 705800
    },
    {
      "epoch": 6.441163588583108,
      "grad_norm": 4.311275959014893,
      "learning_rate": 4.4632363676180746e-05,
      "loss": 0.7182,
      "step": 705900
    },
    {
      "epoch": 6.442076063946273,
      "grad_norm": 3.810927152633667,
      "learning_rate": 4.4631603280044776e-05,
      "loss": 0.6985,
      "step": 706000
    },
    {
      "epoch": 6.442988539309439,
      "grad_norm": 3.2783944606781006,
      "learning_rate": 4.46308428839088e-05,
      "loss": 0.7136,
      "step": 706100
    },
    {
      "epoch": 6.443901014672604,
      "grad_norm": 4.508196830749512,
      "learning_rate": 4.4630082487772836e-05,
      "loss": 0.7085,
      "step": 706200
    },
    {
      "epoch": 6.444813490035769,
      "grad_norm": 3.9224298000335693,
      "learning_rate": 4.462932209163686e-05,
      "loss": 0.6951,
      "step": 706300
    },
    {
      "epoch": 6.4457259653989345,
      "grad_norm": 3.7238152027130127,
      "learning_rate": 4.462856169550089e-05,
      "loss": 0.6824,
      "step": 706400
    },
    {
      "epoch": 6.4466384407621,
      "grad_norm": 3.4653279781341553,
      "learning_rate": 4.462780129936492e-05,
      "loss": 0.6544,
      "step": 706500
    },
    {
      "epoch": 6.447550916125264,
      "grad_norm": 3.9278295040130615,
      "learning_rate": 4.462704090322895e-05,
      "loss": 0.6978,
      "step": 706600
    },
    {
      "epoch": 6.4484633914884295,
      "grad_norm": 3.751295566558838,
      "learning_rate": 4.462628050709298e-05,
      "loss": 0.7232,
      "step": 706700
    },
    {
      "epoch": 6.449375866851595,
      "grad_norm": 4.970921993255615,
      "learning_rate": 4.462552011095701e-05,
      "loss": 0.6917,
      "step": 706800
    },
    {
      "epoch": 6.45028834221476,
      "grad_norm": 2.6971561908721924,
      "learning_rate": 4.462475971482103e-05,
      "loss": 0.6946,
      "step": 706900
    },
    {
      "epoch": 6.451200817577925,
      "grad_norm": 3.5991437435150146,
      "learning_rate": 4.462399931868507e-05,
      "loss": 0.6734,
      "step": 707000
    },
    {
      "epoch": 6.452113292941091,
      "grad_norm": 3.4192957878112793,
      "learning_rate": 4.462323892254909e-05,
      "loss": 0.6763,
      "step": 707100
    },
    {
      "epoch": 6.453025768304256,
      "grad_norm": 3.7984044551849365,
      "learning_rate": 4.462247852641312e-05,
      "loss": 0.6835,
      "step": 707200
    },
    {
      "epoch": 6.453938243667421,
      "grad_norm": 3.8760898113250732,
      "learning_rate": 4.462171813027715e-05,
      "loss": 0.7046,
      "step": 707300
    },
    {
      "epoch": 6.454850719030587,
      "grad_norm": 3.926079511642456,
      "learning_rate": 4.4620957734141176e-05,
      "loss": 0.6856,
      "step": 707400
    },
    {
      "epoch": 6.455763194393751,
      "grad_norm": 3.504823684692383,
      "learning_rate": 4.4620197338005206e-05,
      "loss": 0.6879,
      "step": 707500
    },
    {
      "epoch": 6.456675669756916,
      "grad_norm": 4.8127336502075195,
      "learning_rate": 4.4619436941869236e-05,
      "loss": 0.6918,
      "step": 707600
    },
    {
      "epoch": 6.457588145120082,
      "grad_norm": 3.3806562423706055,
      "learning_rate": 4.4618676545733266e-05,
      "loss": 0.7504,
      "step": 707700
    },
    {
      "epoch": 6.458500620483247,
      "grad_norm": 3.3952138423919678,
      "learning_rate": 4.4617916149597296e-05,
      "loss": 0.7153,
      "step": 707800
    },
    {
      "epoch": 6.459413095846412,
      "grad_norm": 4.345174312591553,
      "learning_rate": 4.4617155753461327e-05,
      "loss": 0.7001,
      "step": 707900
    },
    {
      "epoch": 6.4603255712095775,
      "grad_norm": 4.307764530181885,
      "learning_rate": 4.461639535732535e-05,
      "loss": 0.7107,
      "step": 708000
    },
    {
      "epoch": 6.461238046572743,
      "grad_norm": 4.212578296661377,
      "learning_rate": 4.461563496118939e-05,
      "loss": 0.7181,
      "step": 708100
    },
    {
      "epoch": 6.462150521935908,
      "grad_norm": 3.68623685836792,
      "learning_rate": 4.461487456505341e-05,
      "loss": 0.711,
      "step": 708200
    },
    {
      "epoch": 6.4630629972990725,
      "grad_norm": 4.2654547691345215,
      "learning_rate": 4.461411416891744e-05,
      "loss": 0.6851,
      "step": 708300
    },
    {
      "epoch": 6.463975472662238,
      "grad_norm": 4.048349380493164,
      "learning_rate": 4.461335377278147e-05,
      "loss": 0.6871,
      "step": 708400
    },
    {
      "epoch": 6.464887948025403,
      "grad_norm": 3.489960193634033,
      "learning_rate": 4.46125933766455e-05,
      "loss": 0.7387,
      "step": 708500
    },
    {
      "epoch": 6.465800423388568,
      "grad_norm": 6.12792444229126,
      "learning_rate": 4.461183298050953e-05,
      "loss": 0.7221,
      "step": 708600
    },
    {
      "epoch": 6.466712898751734,
      "grad_norm": 4.351023197174072,
      "learning_rate": 4.461107258437356e-05,
      "loss": 0.6659,
      "step": 708700
    },
    {
      "epoch": 6.467625374114899,
      "grad_norm": 3.979740858078003,
      "learning_rate": 4.4610312188237584e-05,
      "loss": 0.7099,
      "step": 708800
    },
    {
      "epoch": 6.468537849478064,
      "grad_norm": 3.9509341716766357,
      "learning_rate": 4.4609551792101614e-05,
      "loss": 0.7027,
      "step": 708900
    },
    {
      "epoch": 6.46945032484123,
      "grad_norm": 3.408679962158203,
      "learning_rate": 4.4608791395965644e-05,
      "loss": 0.6854,
      "step": 709000
    },
    {
      "epoch": 6.470362800204395,
      "grad_norm": 5.241026878356934,
      "learning_rate": 4.4608030999829674e-05,
      "loss": 0.695,
      "step": 709100
    },
    {
      "epoch": 6.471275275567559,
      "grad_norm": 4.043253421783447,
      "learning_rate": 4.4607270603693704e-05,
      "loss": 0.7323,
      "step": 709200
    },
    {
      "epoch": 6.472187750930725,
      "grad_norm": 4.795631408691406,
      "learning_rate": 4.4606510207557734e-05,
      "loss": 0.7197,
      "step": 709300
    },
    {
      "epoch": 6.47310022629389,
      "grad_norm": 3.1911144256591797,
      "learning_rate": 4.460574981142176e-05,
      "loss": 0.6918,
      "step": 709400
    },
    {
      "epoch": 6.474012701657055,
      "grad_norm": 5.090374946594238,
      "learning_rate": 4.4604989415285794e-05,
      "loss": 0.6971,
      "step": 709500
    },
    {
      "epoch": 6.4749251770202205,
      "grad_norm": 4.042408466339111,
      "learning_rate": 4.460422901914982e-05,
      "loss": 0.7293,
      "step": 709600
    },
    {
      "epoch": 6.475837652383386,
      "grad_norm": 3.5175609588623047,
      "learning_rate": 4.460346862301385e-05,
      "loss": 0.6979,
      "step": 709700
    },
    {
      "epoch": 6.476750127746551,
      "grad_norm": 3.00980544090271,
      "learning_rate": 4.460270822687788e-05,
      "loss": 0.7168,
      "step": 709800
    },
    {
      "epoch": 6.477662603109716,
      "grad_norm": 4.397878646850586,
      "learning_rate": 4.460194783074191e-05,
      "loss": 0.7154,
      "step": 709900
    },
    {
      "epoch": 6.478575078472881,
      "grad_norm": 4.721141338348389,
      "learning_rate": 4.460118743460594e-05,
      "loss": 0.6959,
      "step": 710000
    },
    {
      "epoch": 6.479487553836046,
      "grad_norm": 4.745687961578369,
      "learning_rate": 4.460042703846996e-05,
      "loss": 0.7199,
      "step": 710100
    },
    {
      "epoch": 6.480400029199211,
      "grad_norm": 4.395127773284912,
      "learning_rate": 4.459966664233399e-05,
      "loss": 0.7131,
      "step": 710200
    },
    {
      "epoch": 6.481312504562377,
      "grad_norm": 4.402667999267578,
      "learning_rate": 4.459890624619802e-05,
      "loss": 0.7241,
      "step": 710300
    },
    {
      "epoch": 6.482224979925542,
      "grad_norm": 3.785511016845703,
      "learning_rate": 4.459814585006205e-05,
      "loss": 0.6975,
      "step": 710400
    },
    {
      "epoch": 6.483137455288707,
      "grad_norm": 3.693634033203125,
      "learning_rate": 4.4597385453926074e-05,
      "loss": 0.6717,
      "step": 710500
    },
    {
      "epoch": 6.484049930651873,
      "grad_norm": 4.594852447509766,
      "learning_rate": 4.459662505779011e-05,
      "loss": 0.6969,
      "step": 710600
    },
    {
      "epoch": 6.484962406015038,
      "grad_norm": 4.296628475189209,
      "learning_rate": 4.4595864661654135e-05,
      "loss": 0.7179,
      "step": 710700
    },
    {
      "epoch": 6.485874881378203,
      "grad_norm": 3.5903851985931396,
      "learning_rate": 4.4595104265518165e-05,
      "loss": 0.6629,
      "step": 710800
    },
    {
      "epoch": 6.486787356741368,
      "grad_norm": 3.6911373138427734,
      "learning_rate": 4.4594343869382195e-05,
      "loss": 0.6691,
      "step": 710900
    },
    {
      "epoch": 6.487699832104533,
      "grad_norm": 4.220050811767578,
      "learning_rate": 4.4593583473246225e-05,
      "loss": 0.7153,
      "step": 711000
    },
    {
      "epoch": 6.488612307467698,
      "grad_norm": 5.117668151855469,
      "learning_rate": 4.4592823077110255e-05,
      "loss": 0.6942,
      "step": 711100
    },
    {
      "epoch": 6.4895247828308635,
      "grad_norm": 4.376303672790527,
      "learning_rate": 4.4592062680974285e-05,
      "loss": 0.649,
      "step": 711200
    },
    {
      "epoch": 6.490437258194029,
      "grad_norm": 3.3516407012939453,
      "learning_rate": 4.459130228483831e-05,
      "loss": 0.7546,
      "step": 711300
    },
    {
      "epoch": 6.491349733557194,
      "grad_norm": 3.6307530403137207,
      "learning_rate": 4.4590541888702345e-05,
      "loss": 0.6599,
      "step": 711400
    },
    {
      "epoch": 6.492262208920359,
      "grad_norm": 3.867703676223755,
      "learning_rate": 4.458978149256637e-05,
      "loss": 0.6987,
      "step": 711500
    },
    {
      "epoch": 6.493174684283525,
      "grad_norm": 4.675803184509277,
      "learning_rate": 4.45890210964304e-05,
      "loss": 0.7497,
      "step": 711600
    },
    {
      "epoch": 6.494087159646689,
      "grad_norm": 3.913264751434326,
      "learning_rate": 4.458826070029443e-05,
      "loss": 0.774,
      "step": 711700
    },
    {
      "epoch": 6.494999635009854,
      "grad_norm": 3.9184772968292236,
      "learning_rate": 4.458750030415846e-05,
      "loss": 0.6938,
      "step": 711800
    },
    {
      "epoch": 6.49591211037302,
      "grad_norm": 4.177427291870117,
      "learning_rate": 4.458673990802248e-05,
      "loss": 0.7074,
      "step": 711900
    },
    {
      "epoch": 6.496824585736185,
      "grad_norm": 4.546198844909668,
      "learning_rate": 4.458597951188652e-05,
      "loss": 0.7058,
      "step": 712000
    },
    {
      "epoch": 6.49773706109935,
      "grad_norm": 3.8760476112365723,
      "learning_rate": 4.458521911575054e-05,
      "loss": 0.7034,
      "step": 712100
    },
    {
      "epoch": 6.498649536462516,
      "grad_norm": 4.645260334014893,
      "learning_rate": 4.458445871961457e-05,
      "loss": 0.6956,
      "step": 712200
    },
    {
      "epoch": 6.499562011825681,
      "grad_norm": 3.0677971839904785,
      "learning_rate": 4.45836983234786e-05,
      "loss": 0.683,
      "step": 712300
    },
    {
      "epoch": 6.500474487188846,
      "grad_norm": 4.556290149688721,
      "learning_rate": 4.458293792734263e-05,
      "loss": 0.7171,
      "step": 712400
    },
    {
      "epoch": 6.5013869625520115,
      "grad_norm": 3.960597038269043,
      "learning_rate": 4.458217753120666e-05,
      "loss": 0.6909,
      "step": 712500
    },
    {
      "epoch": 6.502299437915176,
      "grad_norm": 4.121243000030518,
      "learning_rate": 4.458141713507069e-05,
      "loss": 0.7136,
      "step": 712600
    },
    {
      "epoch": 6.503211913278341,
      "grad_norm": 3.448287010192871,
      "learning_rate": 4.4580656738934716e-05,
      "loss": 0.6823,
      "step": 712700
    },
    {
      "epoch": 6.5041243886415065,
      "grad_norm": 4.049189567565918,
      "learning_rate": 4.457989634279875e-05,
      "loss": 0.6746,
      "step": 712800
    },
    {
      "epoch": 6.505036864004672,
      "grad_norm": 4.08564567565918,
      "learning_rate": 4.4579135946662776e-05,
      "loss": 0.7236,
      "step": 712900
    },
    {
      "epoch": 6.505949339367837,
      "grad_norm": 4.219576835632324,
      "learning_rate": 4.45783755505268e-05,
      "loss": 0.7393,
      "step": 713000
    },
    {
      "epoch": 6.5068618147310024,
      "grad_norm": 4.23715877532959,
      "learning_rate": 4.4577615154390836e-05,
      "loss": 0.7,
      "step": 713100
    },
    {
      "epoch": 6.507774290094168,
      "grad_norm": 5.221005439758301,
      "learning_rate": 4.457685475825486e-05,
      "loss": 0.6501,
      "step": 713200
    },
    {
      "epoch": 6.508686765457332,
      "grad_norm": 4.455616474151611,
      "learning_rate": 4.457609436211889e-05,
      "loss": 0.6828,
      "step": 713300
    },
    {
      "epoch": 6.5095992408204975,
      "grad_norm": 2.8996222019195557,
      "learning_rate": 4.457533396598292e-05,
      "loss": 0.703,
      "step": 713400
    },
    {
      "epoch": 6.510511716183663,
      "grad_norm": 3.641951084136963,
      "learning_rate": 4.457457356984695e-05,
      "loss": 0.7178,
      "step": 713500
    },
    {
      "epoch": 6.511424191546828,
      "grad_norm": 4.81974983215332,
      "learning_rate": 4.457381317371098e-05,
      "loss": 0.7166,
      "step": 713600
    },
    {
      "epoch": 6.512336666909993,
      "grad_norm": 3.3156590461730957,
      "learning_rate": 4.457305277757501e-05,
      "loss": 0.7333,
      "step": 713700
    },
    {
      "epoch": 6.513249142273159,
      "grad_norm": 4.742046356201172,
      "learning_rate": 4.457229238143903e-05,
      "loss": 0.6688,
      "step": 713800
    },
    {
      "epoch": 6.514161617636324,
      "grad_norm": 4.331933498382568,
      "learning_rate": 4.457153198530307e-05,
      "loss": 0.7346,
      "step": 713900
    },
    {
      "epoch": 6.515074092999489,
      "grad_norm": 3.8878588676452637,
      "learning_rate": 4.457077158916709e-05,
      "loss": 0.7351,
      "step": 714000
    },
    {
      "epoch": 6.5159865683626546,
      "grad_norm": 4.095713138580322,
      "learning_rate": 4.457001119303112e-05,
      "loss": 0.7213,
      "step": 714100
    },
    {
      "epoch": 6.51689904372582,
      "grad_norm": 4.183657646179199,
      "learning_rate": 4.456925079689515e-05,
      "loss": 0.7156,
      "step": 714200
    },
    {
      "epoch": 6.517811519088984,
      "grad_norm": 4.805415153503418,
      "learning_rate": 4.456849040075918e-05,
      "loss": 0.6754,
      "step": 714300
    },
    {
      "epoch": 6.51872399445215,
      "grad_norm": 4.008872985839844,
      "learning_rate": 4.4567730004623206e-05,
      "loss": 0.757,
      "step": 714400
    },
    {
      "epoch": 6.519636469815315,
      "grad_norm": 5.25453519821167,
      "learning_rate": 4.456696960848724e-05,
      "loss": 0.7326,
      "step": 714500
    },
    {
      "epoch": 6.52054894517848,
      "grad_norm": 4.233522891998291,
      "learning_rate": 4.4566209212351267e-05,
      "loss": 0.672,
      "step": 714600
    },
    {
      "epoch": 6.5214614205416455,
      "grad_norm": 4.072198867797852,
      "learning_rate": 4.4565448816215297e-05,
      "loss": 0.7338,
      "step": 714700
    },
    {
      "epoch": 6.522373895904811,
      "grad_norm": 4.125819206237793,
      "learning_rate": 4.456468842007933e-05,
      "loss": 0.7447,
      "step": 714800
    },
    {
      "epoch": 6.523286371267976,
      "grad_norm": 3.38407564163208,
      "learning_rate": 4.456392802394336e-05,
      "loss": 0.6873,
      "step": 714900
    },
    {
      "epoch": 6.5241988466311405,
      "grad_norm": 3.6195685863494873,
      "learning_rate": 4.456316762780739e-05,
      "loss": 0.7154,
      "step": 715000
    },
    {
      "epoch": 6.525111321994306,
      "grad_norm": 3.4571917057037354,
      "learning_rate": 4.456240723167142e-05,
      "loss": 0.6965,
      "step": 715100
    },
    {
      "epoch": 6.526023797357471,
      "grad_norm": 3.4117352962493896,
      "learning_rate": 4.456164683553544e-05,
      "loss": 0.7076,
      "step": 715200
    },
    {
      "epoch": 6.526936272720636,
      "grad_norm": 3.8629064559936523,
      "learning_rate": 4.456088643939948e-05,
      "loss": 0.6798,
      "step": 715300
    },
    {
      "epoch": 6.527848748083802,
      "grad_norm": 4.465033531188965,
      "learning_rate": 4.45601260432635e-05,
      "loss": 0.6663,
      "step": 715400
    },
    {
      "epoch": 6.528761223446967,
      "grad_norm": 3.4242217540740967,
      "learning_rate": 4.455936564712753e-05,
      "loss": 0.6865,
      "step": 715500
    },
    {
      "epoch": 6.529673698810132,
      "grad_norm": 4.422535419464111,
      "learning_rate": 4.455860525099156e-05,
      "loss": 0.7131,
      "step": 715600
    },
    {
      "epoch": 6.530586174173298,
      "grad_norm": 3.853999376296997,
      "learning_rate": 4.455784485485559e-05,
      "loss": 0.7042,
      "step": 715700
    },
    {
      "epoch": 6.531498649536463,
      "grad_norm": 3.836799144744873,
      "learning_rate": 4.4557084458719614e-05,
      "loss": 0.7118,
      "step": 715800
    },
    {
      "epoch": 6.532411124899628,
      "grad_norm": 4.575750350952148,
      "learning_rate": 4.4556324062583644e-05,
      "loss": 0.6469,
      "step": 715900
    },
    {
      "epoch": 6.533323600262793,
      "grad_norm": 3.6740121841430664,
      "learning_rate": 4.4555563666447674e-05,
      "loss": 0.6737,
      "step": 716000
    },
    {
      "epoch": 6.534236075625958,
      "grad_norm": 4.885090351104736,
      "learning_rate": 4.4554803270311704e-05,
      "loss": 0.6882,
      "step": 716100
    },
    {
      "epoch": 6.535148550989123,
      "grad_norm": 4.055964946746826,
      "learning_rate": 4.4554042874175734e-05,
      "loss": 0.6797,
      "step": 716200
    },
    {
      "epoch": 6.5360610263522885,
      "grad_norm": 4.55148983001709,
      "learning_rate": 4.455328247803976e-05,
      "loss": 0.6491,
      "step": 716300
    },
    {
      "epoch": 6.536973501715454,
      "grad_norm": 3.6171011924743652,
      "learning_rate": 4.4552522081903794e-05,
      "loss": 0.6711,
      "step": 716400
    },
    {
      "epoch": 6.537885977078619,
      "grad_norm": 8.037765502929688,
      "learning_rate": 4.455176168576782e-05,
      "loss": 0.6707,
      "step": 716500
    },
    {
      "epoch": 6.538798452441784,
      "grad_norm": 3.0856127738952637,
      "learning_rate": 4.455100128963185e-05,
      "loss": 0.709,
      "step": 716600
    },
    {
      "epoch": 6.539710927804949,
      "grad_norm": 4.03224515914917,
      "learning_rate": 4.455024089349588e-05,
      "loss": 0.7173,
      "step": 716700
    },
    {
      "epoch": 6.540623403168114,
      "grad_norm": 4.316065311431885,
      "learning_rate": 4.454948049735991e-05,
      "loss": 0.7182,
      "step": 716800
    },
    {
      "epoch": 6.541535878531279,
      "grad_norm": 4.72383451461792,
      "learning_rate": 4.454872010122393e-05,
      "loss": 0.6795,
      "step": 716900
    },
    {
      "epoch": 6.542448353894445,
      "grad_norm": 4.954562187194824,
      "learning_rate": 4.454795970508797e-05,
      "loss": 0.7325,
      "step": 717000
    },
    {
      "epoch": 6.54336082925761,
      "grad_norm": 5.044348239898682,
      "learning_rate": 4.454719930895199e-05,
      "loss": 0.7132,
      "step": 717100
    },
    {
      "epoch": 6.544273304620775,
      "grad_norm": 5.193408012390137,
      "learning_rate": 4.454643891281602e-05,
      "loss": 0.6939,
      "step": 717200
    },
    {
      "epoch": 6.545185779983941,
      "grad_norm": 3.7334678173065186,
      "learning_rate": 4.454567851668005e-05,
      "loss": 0.7166,
      "step": 717300
    },
    {
      "epoch": 6.546098255347106,
      "grad_norm": 3.983313798904419,
      "learning_rate": 4.454491812054408e-05,
      "loss": 0.7092,
      "step": 717400
    },
    {
      "epoch": 6.547010730710271,
      "grad_norm": 3.202629327774048,
      "learning_rate": 4.454415772440811e-05,
      "loss": 0.698,
      "step": 717500
    },
    {
      "epoch": 6.5479232060734365,
      "grad_norm": 3.549325466156006,
      "learning_rate": 4.454339732827214e-05,
      "loss": 0.7331,
      "step": 717600
    },
    {
      "epoch": 6.548835681436601,
      "grad_norm": 3.9059600830078125,
      "learning_rate": 4.4542636932136165e-05,
      "loss": 0.7009,
      "step": 717700
    },
    {
      "epoch": 6.549748156799766,
      "grad_norm": 3.2852306365966797,
      "learning_rate": 4.45418765360002e-05,
      "loss": 0.6769,
      "step": 717800
    },
    {
      "epoch": 6.5506606321629315,
      "grad_norm": 4.055183410644531,
      "learning_rate": 4.4541116139864225e-05,
      "loss": 0.6578,
      "step": 717900
    },
    {
      "epoch": 6.551573107526097,
      "grad_norm": 4.7334489822387695,
      "learning_rate": 4.4540355743728255e-05,
      "loss": 0.7016,
      "step": 718000
    },
    {
      "epoch": 6.552485582889262,
      "grad_norm": 3.9676012992858887,
      "learning_rate": 4.4539595347592285e-05,
      "loss": 0.6694,
      "step": 718100
    },
    {
      "epoch": 6.553398058252427,
      "grad_norm": 4.299698352813721,
      "learning_rate": 4.4538834951456315e-05,
      "loss": 0.7164,
      "step": 718200
    },
    {
      "epoch": 6.554310533615593,
      "grad_norm": 4.35547399520874,
      "learning_rate": 4.453807455532034e-05,
      "loss": 0.7209,
      "step": 718300
    },
    {
      "epoch": 6.555223008978757,
      "grad_norm": 5.994194984436035,
      "learning_rate": 4.4537314159184375e-05,
      "loss": 0.7075,
      "step": 718400
    },
    {
      "epoch": 6.556135484341922,
      "grad_norm": 3.716050863265991,
      "learning_rate": 4.45365537630484e-05,
      "loss": 0.6982,
      "step": 718500
    },
    {
      "epoch": 6.557047959705088,
      "grad_norm": 4.502295970916748,
      "learning_rate": 4.453579336691243e-05,
      "loss": 0.7204,
      "step": 718600
    },
    {
      "epoch": 6.557960435068253,
      "grad_norm": 4.814696788787842,
      "learning_rate": 4.453503297077646e-05,
      "loss": 0.7166,
      "step": 718700
    },
    {
      "epoch": 6.558872910431418,
      "grad_norm": 5.019121170043945,
      "learning_rate": 4.453427257464048e-05,
      "loss": 0.6775,
      "step": 718800
    },
    {
      "epoch": 6.559785385794584,
      "grad_norm": 3.45759916305542,
      "learning_rate": 4.453351217850452e-05,
      "loss": 0.7336,
      "step": 718900
    },
    {
      "epoch": 6.560697861157749,
      "grad_norm": 3.812476634979248,
      "learning_rate": 4.453275178236854e-05,
      "loss": 0.6999,
      "step": 719000
    },
    {
      "epoch": 6.561610336520914,
      "grad_norm": 4.1443305015563965,
      "learning_rate": 4.453199138623257e-05,
      "loss": 0.6544,
      "step": 719100
    },
    {
      "epoch": 6.5625228118840795,
      "grad_norm": 3.9697933197021484,
      "learning_rate": 4.45312309900966e-05,
      "loss": 0.7215,
      "step": 719200
    },
    {
      "epoch": 6.563435287247245,
      "grad_norm": 3.2928266525268555,
      "learning_rate": 4.453047059396063e-05,
      "loss": 0.7248,
      "step": 719300
    },
    {
      "epoch": 6.564347762610409,
      "grad_norm": 4.111701011657715,
      "learning_rate": 4.4529710197824656e-05,
      "loss": 0.6992,
      "step": 719400
    },
    {
      "epoch": 6.5652602379735745,
      "grad_norm": 2.6197049617767334,
      "learning_rate": 4.452894980168869e-05,
      "loss": 0.7335,
      "step": 719500
    },
    {
      "epoch": 6.56617271333674,
      "grad_norm": 3.798903226852417,
      "learning_rate": 4.4528189405552716e-05,
      "loss": 0.7303,
      "step": 719600
    },
    {
      "epoch": 6.567085188699905,
      "grad_norm": 3.518203020095825,
      "learning_rate": 4.4527429009416746e-05,
      "loss": 0.6815,
      "step": 719700
    },
    {
      "epoch": 6.56799766406307,
      "grad_norm": 4.379878997802734,
      "learning_rate": 4.4526668613280776e-05,
      "loss": 0.7018,
      "step": 719800
    },
    {
      "epoch": 6.568910139426236,
      "grad_norm": 2.138880968093872,
      "learning_rate": 4.4525908217144806e-05,
      "loss": 0.6689,
      "step": 719900
    },
    {
      "epoch": 6.569822614789401,
      "grad_norm": 4.036642551422119,
      "learning_rate": 4.4525147821008836e-05,
      "loss": 0.696,
      "step": 720000
    },
    {
      "epoch": 6.570735090152565,
      "grad_norm": 3.8827269077301025,
      "learning_rate": 4.4524387424872866e-05,
      "loss": 0.7092,
      "step": 720100
    },
    {
      "epoch": 6.571647565515731,
      "grad_norm": 4.220145225524902,
      "learning_rate": 4.452362702873689e-05,
      "loss": 0.714,
      "step": 720200
    },
    {
      "epoch": 6.572560040878896,
      "grad_norm": 3.693249464035034,
      "learning_rate": 4.4522866632600926e-05,
      "loss": 0.6868,
      "step": 720300
    },
    {
      "epoch": 6.573472516242061,
      "grad_norm": 3.38566255569458,
      "learning_rate": 4.452210623646495e-05,
      "loss": 0.702,
      "step": 720400
    },
    {
      "epoch": 6.574384991605227,
      "grad_norm": 3.5486221313476562,
      "learning_rate": 4.452134584032898e-05,
      "loss": 0.7056,
      "step": 720500
    },
    {
      "epoch": 6.575297466968392,
      "grad_norm": 4.1944708824157715,
      "learning_rate": 4.452058544419301e-05,
      "loss": 0.6964,
      "step": 720600
    },
    {
      "epoch": 6.576209942331557,
      "grad_norm": 3.8776445388793945,
      "learning_rate": 4.451982504805704e-05,
      "loss": 0.7175,
      "step": 720700
    },
    {
      "epoch": 6.5771224176947225,
      "grad_norm": 4.536561489105225,
      "learning_rate": 4.451906465192107e-05,
      "loss": 0.6927,
      "step": 720800
    },
    {
      "epoch": 6.578034893057888,
      "grad_norm": 3.569322347640991,
      "learning_rate": 4.45183042557851e-05,
      "loss": 0.6821,
      "step": 720900
    },
    {
      "epoch": 6.578947368421053,
      "grad_norm": 3.7804858684539795,
      "learning_rate": 4.451754385964912e-05,
      "loss": 0.7196,
      "step": 721000
    },
    {
      "epoch": 6.5798598437842175,
      "grad_norm": 4.34857702255249,
      "learning_rate": 4.451678346351315e-05,
      "loss": 0.7299,
      "step": 721100
    },
    {
      "epoch": 6.580772319147383,
      "grad_norm": 3.7722973823547363,
      "learning_rate": 4.451602306737718e-05,
      "loss": 0.7328,
      "step": 721200
    },
    {
      "epoch": 6.581684794510548,
      "grad_norm": 4.1584792137146,
      "learning_rate": 4.451526267124121e-05,
      "loss": 0.6698,
      "step": 721300
    },
    {
      "epoch": 6.582597269873713,
      "grad_norm": 3.574951171875,
      "learning_rate": 4.451450227510524e-05,
      "loss": 0.6974,
      "step": 721400
    },
    {
      "epoch": 6.583509745236879,
      "grad_norm": 4.055882453918457,
      "learning_rate": 4.451374187896927e-05,
      "loss": 0.6999,
      "step": 721500
    },
    {
      "epoch": 6.584422220600044,
      "grad_norm": 3.867506980895996,
      "learning_rate": 4.45129814828333e-05,
      "loss": 0.6968,
      "step": 721600
    },
    {
      "epoch": 6.585334695963209,
      "grad_norm": 3.2217588424682617,
      "learning_rate": 4.451222108669733e-05,
      "loss": 0.6609,
      "step": 721700
    },
    {
      "epoch": 6.586247171326374,
      "grad_norm": 4.148773193359375,
      "learning_rate": 4.451146069056136e-05,
      "loss": 0.7025,
      "step": 721800
    },
    {
      "epoch": 6.587159646689539,
      "grad_norm": 4.120987415313721,
      "learning_rate": 4.451070029442539e-05,
      "loss": 0.7151,
      "step": 721900
    },
    {
      "epoch": 6.588072122052704,
      "grad_norm": 4.799786567687988,
      "learning_rate": 4.450993989828942e-05,
      "loss": 0.7034,
      "step": 722000
    },
    {
      "epoch": 6.58898459741587,
      "grad_norm": 4.004027843475342,
      "learning_rate": 4.450917950215344e-05,
      "loss": 0.7116,
      "step": 722100
    },
    {
      "epoch": 6.589897072779035,
      "grad_norm": 3.8396151065826416,
      "learning_rate": 4.450841910601748e-05,
      "loss": 0.6921,
      "step": 722200
    },
    {
      "epoch": 6.5908095481422,
      "grad_norm": 4.145822048187256,
      "learning_rate": 4.45076587098815e-05,
      "loss": 0.708,
      "step": 722300
    },
    {
      "epoch": 6.5917220235053655,
      "grad_norm": 4.029464244842529,
      "learning_rate": 4.450689831374553e-05,
      "loss": 0.6933,
      "step": 722400
    },
    {
      "epoch": 6.592634498868531,
      "grad_norm": 3.9036879539489746,
      "learning_rate": 4.450613791760956e-05,
      "loss": 0.7121,
      "step": 722500
    },
    {
      "epoch": 6.593546974231696,
      "grad_norm": 4.004250526428223,
      "learning_rate": 4.450537752147359e-05,
      "loss": 0.7035,
      "step": 722600
    },
    {
      "epoch": 6.594459449594861,
      "grad_norm": 4.338654041290283,
      "learning_rate": 4.4504617125337614e-05,
      "loss": 0.6774,
      "step": 722700
    },
    {
      "epoch": 6.595371924958026,
      "grad_norm": 3.503462076187134,
      "learning_rate": 4.450385672920165e-05,
      "loss": 0.733,
      "step": 722800
    },
    {
      "epoch": 6.596284400321191,
      "grad_norm": 4.234214782714844,
      "learning_rate": 4.4503096333065674e-05,
      "loss": 0.6952,
      "step": 722900
    },
    {
      "epoch": 6.597196875684356,
      "grad_norm": 4.356234073638916,
      "learning_rate": 4.4502335936929704e-05,
      "loss": 0.7024,
      "step": 723000
    },
    {
      "epoch": 6.598109351047522,
      "grad_norm": 3.390763759613037,
      "learning_rate": 4.4501575540793734e-05,
      "loss": 0.6807,
      "step": 723100
    },
    {
      "epoch": 6.599021826410687,
      "grad_norm": 3.37164568901062,
      "learning_rate": 4.4500815144657764e-05,
      "loss": 0.7086,
      "step": 723200
    },
    {
      "epoch": 6.599934301773852,
      "grad_norm": 4.469120502471924,
      "learning_rate": 4.4500054748521794e-05,
      "loss": 0.697,
      "step": 723300
    },
    {
      "epoch": 6.600846777137018,
      "grad_norm": 5.4676642417907715,
      "learning_rate": 4.4499294352385824e-05,
      "loss": 0.7412,
      "step": 723400
    },
    {
      "epoch": 6.601759252500182,
      "grad_norm": 5.532982349395752,
      "learning_rate": 4.449853395624985e-05,
      "loss": 0.7119,
      "step": 723500
    },
    {
      "epoch": 6.602671727863347,
      "grad_norm": 4.9981536865234375,
      "learning_rate": 4.4497773560113884e-05,
      "loss": 0.6912,
      "step": 723600
    },
    {
      "epoch": 6.603584203226513,
      "grad_norm": 4.230380058288574,
      "learning_rate": 4.449701316397791e-05,
      "loss": 0.7141,
      "step": 723700
    },
    {
      "epoch": 6.604496678589678,
      "grad_norm": 3.702205181121826,
      "learning_rate": 4.449625276784194e-05,
      "loss": 0.7138,
      "step": 723800
    },
    {
      "epoch": 6.605409153952843,
      "grad_norm": 3.768923759460449,
      "learning_rate": 4.449549237170597e-05,
      "loss": 0.6915,
      "step": 723900
    },
    {
      "epoch": 6.6063216293160085,
      "grad_norm": 4.871425628662109,
      "learning_rate": 4.449473197557e-05,
      "loss": 0.6552,
      "step": 724000
    },
    {
      "epoch": 6.607234104679174,
      "grad_norm": 4.533814430236816,
      "learning_rate": 4.449397157943402e-05,
      "loss": 0.7117,
      "step": 724100
    },
    {
      "epoch": 6.608146580042339,
      "grad_norm": 3.656559705734253,
      "learning_rate": 4.449321118329806e-05,
      "loss": 0.724,
      "step": 724200
    },
    {
      "epoch": 6.609059055405504,
      "grad_norm": 4.189270496368408,
      "learning_rate": 4.449245078716208e-05,
      "loss": 0.6778,
      "step": 724300
    },
    {
      "epoch": 6.60997153076867,
      "grad_norm": 4.043576717376709,
      "learning_rate": 4.449169039102611e-05,
      "loss": 0.6853,
      "step": 724400
    },
    {
      "epoch": 6.610884006131834,
      "grad_norm": 4.239159107208252,
      "learning_rate": 4.449092999489014e-05,
      "loss": 0.6973,
      "step": 724500
    },
    {
      "epoch": 6.611796481494999,
      "grad_norm": 4.663426876068115,
      "learning_rate": 4.4490169598754165e-05,
      "loss": 0.6972,
      "step": 724600
    },
    {
      "epoch": 6.612708956858165,
      "grad_norm": 3.55497670173645,
      "learning_rate": 4.44894092026182e-05,
      "loss": 0.6939,
      "step": 724700
    },
    {
      "epoch": 6.61362143222133,
      "grad_norm": 4.421754837036133,
      "learning_rate": 4.4488648806482225e-05,
      "loss": 0.7041,
      "step": 724800
    },
    {
      "epoch": 6.614533907584495,
      "grad_norm": 5.0678887367248535,
      "learning_rate": 4.4487888410346255e-05,
      "loss": 0.6901,
      "step": 724900
    },
    {
      "epoch": 6.615446382947661,
      "grad_norm": 4.436197280883789,
      "learning_rate": 4.4487128014210285e-05,
      "loss": 0.7005,
      "step": 725000
    },
    {
      "epoch": 6.616358858310826,
      "grad_norm": 2.712919235229492,
      "learning_rate": 4.4486367618074315e-05,
      "loss": 0.6914,
      "step": 725100
    },
    {
      "epoch": 6.61727133367399,
      "grad_norm": 4.20569372177124,
      "learning_rate": 4.448560722193834e-05,
      "loss": 0.7026,
      "step": 725200
    },
    {
      "epoch": 6.618183809037156,
      "grad_norm": 3.7540769577026367,
      "learning_rate": 4.4484846825802375e-05,
      "loss": 0.7219,
      "step": 725300
    },
    {
      "epoch": 6.619096284400321,
      "grad_norm": 3.7413291931152344,
      "learning_rate": 4.44840864296664e-05,
      "loss": 0.7199,
      "step": 725400
    },
    {
      "epoch": 6.620008759763486,
      "grad_norm": 3.9223883152008057,
      "learning_rate": 4.448332603353043e-05,
      "loss": 0.7207,
      "step": 725500
    },
    {
      "epoch": 6.6209212351266515,
      "grad_norm": 3.437610149383545,
      "learning_rate": 4.448256563739446e-05,
      "loss": 0.7261,
      "step": 725600
    },
    {
      "epoch": 6.621833710489817,
      "grad_norm": 3.578040838241577,
      "learning_rate": 4.448180524125849e-05,
      "loss": 0.6921,
      "step": 725700
    },
    {
      "epoch": 6.622746185852982,
      "grad_norm": 4.087769031524658,
      "learning_rate": 4.448104484512252e-05,
      "loss": 0.6999,
      "step": 725800
    },
    {
      "epoch": 6.623658661216147,
      "grad_norm": 4.751410961151123,
      "learning_rate": 4.448028444898655e-05,
      "loss": 0.7302,
      "step": 725900
    },
    {
      "epoch": 6.624571136579313,
      "grad_norm": 3.758004665374756,
      "learning_rate": 4.447952405285057e-05,
      "loss": 0.7352,
      "step": 726000
    },
    {
      "epoch": 6.625483611942478,
      "grad_norm": 3.2785520553588867,
      "learning_rate": 4.447876365671461e-05,
      "loss": 0.7121,
      "step": 726100
    },
    {
      "epoch": 6.6263960873056424,
      "grad_norm": 4.188968181610107,
      "learning_rate": 4.447800326057863e-05,
      "loss": 0.7024,
      "step": 726200
    },
    {
      "epoch": 6.627308562668808,
      "grad_norm": 4.367515563964844,
      "learning_rate": 4.447724286444266e-05,
      "loss": 0.6556,
      "step": 726300
    },
    {
      "epoch": 6.628221038031973,
      "grad_norm": 4.3390631675720215,
      "learning_rate": 4.447648246830669e-05,
      "loss": 0.6594,
      "step": 726400
    },
    {
      "epoch": 6.629133513395138,
      "grad_norm": 3.5793325901031494,
      "learning_rate": 4.447572207217072e-05,
      "loss": 0.7108,
      "step": 726500
    },
    {
      "epoch": 6.630045988758304,
      "grad_norm": 3.8902511596679688,
      "learning_rate": 4.4474961676034746e-05,
      "loss": 0.7256,
      "step": 726600
    },
    {
      "epoch": 6.630958464121469,
      "grad_norm": 5.173965930938721,
      "learning_rate": 4.447420127989878e-05,
      "loss": 0.7003,
      "step": 726700
    },
    {
      "epoch": 6.631870939484634,
      "grad_norm": 3.573756217956543,
      "learning_rate": 4.4473440883762806e-05,
      "loss": 0.7117,
      "step": 726800
    },
    {
      "epoch": 6.632783414847799,
      "grad_norm": 4.272743225097656,
      "learning_rate": 4.4472680487626836e-05,
      "loss": 0.7341,
      "step": 726900
    },
    {
      "epoch": 6.633695890210964,
      "grad_norm": 3.9871761798858643,
      "learning_rate": 4.4471920091490866e-05,
      "loss": 0.699,
      "step": 727000
    },
    {
      "epoch": 6.634608365574129,
      "grad_norm": 3.8794915676116943,
      "learning_rate": 4.447115969535489e-05,
      "loss": 0.7048,
      "step": 727100
    },
    {
      "epoch": 6.6355208409372946,
      "grad_norm": 3.37203049659729,
      "learning_rate": 4.4470399299218926e-05,
      "loss": 0.6775,
      "step": 727200
    },
    {
      "epoch": 6.63643331630046,
      "grad_norm": 3.5843281745910645,
      "learning_rate": 4.446963890308295e-05,
      "loss": 0.6988,
      "step": 727300
    },
    {
      "epoch": 6.637345791663625,
      "grad_norm": 4.1563801765441895,
      "learning_rate": 4.446887850694698e-05,
      "loss": 0.7228,
      "step": 727400
    },
    {
      "epoch": 6.6382582670267904,
      "grad_norm": 4.783132553100586,
      "learning_rate": 4.446811811081101e-05,
      "loss": 0.697,
      "step": 727500
    },
    {
      "epoch": 6.639170742389956,
      "grad_norm": 3.7274701595306396,
      "learning_rate": 4.446735771467504e-05,
      "loss": 0.72,
      "step": 727600
    },
    {
      "epoch": 6.640083217753121,
      "grad_norm": 3.9102237224578857,
      "learning_rate": 4.446659731853906e-05,
      "loss": 0.7075,
      "step": 727700
    },
    {
      "epoch": 6.640995693116286,
      "grad_norm": 3.717060089111328,
      "learning_rate": 4.44658369224031e-05,
      "loss": 0.6778,
      "step": 727800
    },
    {
      "epoch": 6.641908168479451,
      "grad_norm": 3.9246695041656494,
      "learning_rate": 4.446507652626712e-05,
      "loss": 0.7208,
      "step": 727900
    },
    {
      "epoch": 6.642820643842616,
      "grad_norm": 4.547123432159424,
      "learning_rate": 4.446431613013115e-05,
      "loss": 0.7089,
      "step": 728000
    },
    {
      "epoch": 6.643733119205781,
      "grad_norm": 4.323455810546875,
      "learning_rate": 4.446355573399518e-05,
      "loss": 0.7117,
      "step": 728100
    },
    {
      "epoch": 6.644645594568947,
      "grad_norm": 4.26030969619751,
      "learning_rate": 4.446279533785921e-05,
      "loss": 0.7239,
      "step": 728200
    },
    {
      "epoch": 6.645558069932112,
      "grad_norm": 3.388444662094116,
      "learning_rate": 4.4462034941723243e-05,
      "loss": 0.6886,
      "step": 728300
    },
    {
      "epoch": 6.646470545295277,
      "grad_norm": 2.849684715270996,
      "learning_rate": 4.4461274545587274e-05,
      "loss": 0.7056,
      "step": 728400
    },
    {
      "epoch": 6.6473830206584426,
      "grad_norm": 3.8320064544677734,
      "learning_rate": 4.44605141494513e-05,
      "loss": 0.7179,
      "step": 728500
    },
    {
      "epoch": 6.648295496021607,
      "grad_norm": 4.119924068450928,
      "learning_rate": 4.4459753753315334e-05,
      "loss": 0.6862,
      "step": 728600
    },
    {
      "epoch": 6.649207971384772,
      "grad_norm": 4.311158180236816,
      "learning_rate": 4.445899335717936e-05,
      "loss": 0.6822,
      "step": 728700
    },
    {
      "epoch": 6.650120446747938,
      "grad_norm": 3.67911696434021,
      "learning_rate": 4.445823296104339e-05,
      "loss": 0.6966,
      "step": 728800
    },
    {
      "epoch": 6.651032922111103,
      "grad_norm": 3.606954336166382,
      "learning_rate": 4.445747256490742e-05,
      "loss": 0.7008,
      "step": 728900
    },
    {
      "epoch": 6.651945397474268,
      "grad_norm": 3.9760751724243164,
      "learning_rate": 4.445671216877145e-05,
      "loss": 0.692,
      "step": 729000
    },
    {
      "epoch": 6.6528578728374335,
      "grad_norm": 3.965667486190796,
      "learning_rate": 4.445595177263547e-05,
      "loss": 0.7124,
      "step": 729100
    },
    {
      "epoch": 6.653770348200599,
      "grad_norm": 4.463765621185303,
      "learning_rate": 4.445519137649951e-05,
      "loss": 0.7418,
      "step": 729200
    },
    {
      "epoch": 6.654682823563764,
      "grad_norm": 4.05556058883667,
      "learning_rate": 4.445443098036353e-05,
      "loss": 0.7017,
      "step": 729300
    },
    {
      "epoch": 6.655595298926929,
      "grad_norm": 4.598114013671875,
      "learning_rate": 4.445367058422756e-05,
      "loss": 0.6863,
      "step": 729400
    },
    {
      "epoch": 6.656507774290095,
      "grad_norm": 4.410822868347168,
      "learning_rate": 4.445291018809159e-05,
      "loss": 0.7074,
      "step": 729500
    },
    {
      "epoch": 6.657420249653259,
      "grad_norm": 5.363226413726807,
      "learning_rate": 4.445214979195562e-05,
      "loss": 0.6954,
      "step": 729600
    },
    {
      "epoch": 6.658332725016424,
      "grad_norm": 4.885540962219238,
      "learning_rate": 4.445138939581965e-05,
      "loss": 0.7114,
      "step": 729700
    },
    {
      "epoch": 6.65924520037959,
      "grad_norm": 4.296926021575928,
      "learning_rate": 4.445062899968368e-05,
      "loss": 0.6611,
      "step": 729800
    },
    {
      "epoch": 6.660157675742755,
      "grad_norm": 4.178254127502441,
      "learning_rate": 4.4449868603547704e-05,
      "loss": 0.6712,
      "step": 729900
    },
    {
      "epoch": 6.66107015110592,
      "grad_norm": 4.052856922149658,
      "learning_rate": 4.4449108207411734e-05,
      "loss": 0.7166,
      "step": 730000
    },
    {
      "epoch": 6.661982626469086,
      "grad_norm": 3.3113608360290527,
      "learning_rate": 4.4448347811275764e-05,
      "loss": 0.7077,
      "step": 730100
    },
    {
      "epoch": 6.662895101832251,
      "grad_norm": 4.300899028778076,
      "learning_rate": 4.444758741513979e-05,
      "loss": 0.6837,
      "step": 730200
    },
    {
      "epoch": 6.663807577195415,
      "grad_norm": 3.702941417694092,
      "learning_rate": 4.4446827019003824e-05,
      "loss": 0.7057,
      "step": 730300
    },
    {
      "epoch": 6.664720052558581,
      "grad_norm": 4.549328327178955,
      "learning_rate": 4.444606662286785e-05,
      "loss": 0.7412,
      "step": 730400
    },
    {
      "epoch": 6.665632527921746,
      "grad_norm": 4.521500110626221,
      "learning_rate": 4.444530622673188e-05,
      "loss": 0.6995,
      "step": 730500
    },
    {
      "epoch": 6.666545003284911,
      "grad_norm": 3.9318525791168213,
      "learning_rate": 4.444454583059591e-05,
      "loss": 0.7104,
      "step": 730600
    },
    {
      "epoch": 6.6674574786480765,
      "grad_norm": 3.5674266815185547,
      "learning_rate": 4.444378543445994e-05,
      "loss": 0.7052,
      "step": 730700
    },
    {
      "epoch": 6.668369954011242,
      "grad_norm": 4.281991004943848,
      "learning_rate": 4.444302503832397e-05,
      "loss": 0.6994,
      "step": 730800
    },
    {
      "epoch": 6.669282429374407,
      "grad_norm": 4.844859600067139,
      "learning_rate": 4.4442264642188e-05,
      "loss": 0.7306,
      "step": 730900
    },
    {
      "epoch": 6.670194904737572,
      "grad_norm": 4.929707050323486,
      "learning_rate": 4.444150424605202e-05,
      "loss": 0.7685,
      "step": 731000
    },
    {
      "epoch": 6.671107380100738,
      "grad_norm": 4.007150173187256,
      "learning_rate": 4.444074384991606e-05,
      "loss": 0.6742,
      "step": 731100
    },
    {
      "epoch": 6.672019855463903,
      "grad_norm": 4.452146053314209,
      "learning_rate": 4.443998345378008e-05,
      "loss": 0.716,
      "step": 731200
    },
    {
      "epoch": 6.672932330827067,
      "grad_norm": 4.951437950134277,
      "learning_rate": 4.443922305764411e-05,
      "loss": 0.7149,
      "step": 731300
    },
    {
      "epoch": 6.673844806190233,
      "grad_norm": 4.029758453369141,
      "learning_rate": 4.443846266150814e-05,
      "loss": 0.7125,
      "step": 731400
    },
    {
      "epoch": 6.674757281553398,
      "grad_norm": 4.557026386260986,
      "learning_rate": 4.443770226537217e-05,
      "loss": 0.722,
      "step": 731500
    },
    {
      "epoch": 6.675669756916563,
      "grad_norm": 3.565103769302368,
      "learning_rate": 4.4436941869236195e-05,
      "loss": 0.7163,
      "step": 731600
    },
    {
      "epoch": 6.676582232279729,
      "grad_norm": 4.099513530731201,
      "learning_rate": 4.443618147310023e-05,
      "loss": 0.7119,
      "step": 731700
    },
    {
      "epoch": 6.677494707642894,
      "grad_norm": 4.46987247467041,
      "learning_rate": 4.4435421076964255e-05,
      "loss": 0.7034,
      "step": 731800
    },
    {
      "epoch": 6.678407183006059,
      "grad_norm": 3.5350115299224854,
      "learning_rate": 4.4434660680828285e-05,
      "loss": 0.7076,
      "step": 731900
    },
    {
      "epoch": 6.679319658369224,
      "grad_norm": 4.960839748382568,
      "learning_rate": 4.4433900284692315e-05,
      "loss": 0.6996,
      "step": 732000
    },
    {
      "epoch": 6.680232133732389,
      "grad_norm": 5.0737223625183105,
      "learning_rate": 4.4433139888556345e-05,
      "loss": 0.7224,
      "step": 732100
    },
    {
      "epoch": 6.681144609095554,
      "grad_norm": 3.8274142742156982,
      "learning_rate": 4.4432379492420375e-05,
      "loss": 0.7033,
      "step": 732200
    },
    {
      "epoch": 6.6820570844587195,
      "grad_norm": 4.395977973937988,
      "learning_rate": 4.4431619096284405e-05,
      "loss": 0.7184,
      "step": 732300
    },
    {
      "epoch": 6.682969559821885,
      "grad_norm": 4.50880765914917,
      "learning_rate": 4.443085870014843e-05,
      "loss": 0.7448,
      "step": 732400
    },
    {
      "epoch": 6.68388203518505,
      "grad_norm": 3.545950174331665,
      "learning_rate": 4.4430098304012466e-05,
      "loss": 0.7032,
      "step": 732500
    },
    {
      "epoch": 6.684794510548215,
      "grad_norm": 3.408844232559204,
      "learning_rate": 4.442933790787649e-05,
      "loss": 0.6682,
      "step": 732600
    },
    {
      "epoch": 6.685706985911381,
      "grad_norm": 2.757239818572998,
      "learning_rate": 4.442857751174052e-05,
      "loss": 0.7152,
      "step": 732700
    },
    {
      "epoch": 6.686619461274546,
      "grad_norm": 3.240858554840088,
      "learning_rate": 4.442781711560455e-05,
      "loss": 0.7142,
      "step": 732800
    },
    {
      "epoch": 6.68753193663771,
      "grad_norm": 4.134974956512451,
      "learning_rate": 4.442705671946857e-05,
      "loss": 0.6951,
      "step": 732900
    },
    {
      "epoch": 6.688444412000876,
      "grad_norm": 4.187920570373535,
      "learning_rate": 4.44262963233326e-05,
      "loss": 0.717,
      "step": 733000
    },
    {
      "epoch": 6.689356887364041,
      "grad_norm": 3.844872236251831,
      "learning_rate": 4.442553592719663e-05,
      "loss": 0.7157,
      "step": 733100
    },
    {
      "epoch": 6.690269362727206,
      "grad_norm": 3.3457143306732178,
      "learning_rate": 4.442477553106066e-05,
      "loss": 0.7016,
      "step": 733200
    },
    {
      "epoch": 6.691181838090372,
      "grad_norm": 3.241088628768921,
      "learning_rate": 4.442401513492469e-05,
      "loss": 0.6849,
      "step": 733300
    },
    {
      "epoch": 6.692094313453537,
      "grad_norm": 4.951421737670898,
      "learning_rate": 4.442325473878872e-05,
      "loss": 0.7064,
      "step": 733400
    },
    {
      "epoch": 6.693006788816702,
      "grad_norm": 4.406996250152588,
      "learning_rate": 4.4422494342652746e-05,
      "loss": 0.723,
      "step": 733500
    },
    {
      "epoch": 6.6939192641798675,
      "grad_norm": 4.3170671463012695,
      "learning_rate": 4.442173394651678e-05,
      "loss": 0.7079,
      "step": 733600
    },
    {
      "epoch": 6.694831739543032,
      "grad_norm": 4.31807804107666,
      "learning_rate": 4.4420973550380806e-05,
      "loss": 0.733,
      "step": 733700
    },
    {
      "epoch": 6.695744214906197,
      "grad_norm": 5.191983222961426,
      "learning_rate": 4.4420213154244836e-05,
      "loss": 0.7367,
      "step": 733800
    },
    {
      "epoch": 6.6966566902693625,
      "grad_norm": 3.8525192737579346,
      "learning_rate": 4.4419452758108866e-05,
      "loss": 0.6804,
      "step": 733900
    },
    {
      "epoch": 6.697569165632528,
      "grad_norm": 5.206203937530518,
      "learning_rate": 4.4418692361972896e-05,
      "loss": 0.6843,
      "step": 734000
    },
    {
      "epoch": 6.698481640995693,
      "grad_norm": 3.888792037963867,
      "learning_rate": 4.4417931965836926e-05,
      "loss": 0.7583,
      "step": 734100
    },
    {
      "epoch": 6.699394116358858,
      "grad_norm": 3.7809338569641113,
      "learning_rate": 4.4417171569700956e-05,
      "loss": 0.6793,
      "step": 734200
    },
    {
      "epoch": 6.700306591722024,
      "grad_norm": 3.568694591522217,
      "learning_rate": 4.441641117356498e-05,
      "loss": 0.7135,
      "step": 734300
    },
    {
      "epoch": 6.701219067085189,
      "grad_norm": 3.4245831966400146,
      "learning_rate": 4.441565077742901e-05,
      "loss": 0.6949,
      "step": 734400
    },
    {
      "epoch": 6.702131542448354,
      "grad_norm": 4.643950939178467,
      "learning_rate": 4.441489038129304e-05,
      "loss": 0.6852,
      "step": 734500
    },
    {
      "epoch": 6.703044017811519,
      "grad_norm": 3.9851582050323486,
      "learning_rate": 4.441412998515707e-05,
      "loss": 0.7024,
      "step": 734600
    },
    {
      "epoch": 6.703956493174684,
      "grad_norm": 3.768235921859741,
      "learning_rate": 4.44133695890211e-05,
      "loss": 0.6959,
      "step": 734700
    },
    {
      "epoch": 6.704868968537849,
      "grad_norm": 4.844738006591797,
      "learning_rate": 4.441260919288513e-05,
      "loss": 0.7478,
      "step": 734800
    },
    {
      "epoch": 6.705781443901015,
      "grad_norm": 3.673473358154297,
      "learning_rate": 4.441184879674915e-05,
      "loss": 0.6951,
      "step": 734900
    },
    {
      "epoch": 6.70669391926418,
      "grad_norm": 3.694164514541626,
      "learning_rate": 4.441108840061319e-05,
      "loss": 0.6805,
      "step": 735000
    },
    {
      "epoch": 6.707606394627345,
      "grad_norm": 3.849614381790161,
      "learning_rate": 4.4410328004477213e-05,
      "loss": 0.691,
      "step": 735100
    },
    {
      "epoch": 6.7085188699905105,
      "grad_norm": 3.9635112285614014,
      "learning_rate": 4.4409567608341244e-05,
      "loss": 0.7028,
      "step": 735200
    },
    {
      "epoch": 6.709431345353676,
      "grad_norm": 4.933084964752197,
      "learning_rate": 4.4408807212205274e-05,
      "loss": 0.6905,
      "step": 735300
    },
    {
      "epoch": 6.71034382071684,
      "grad_norm": 4.8230133056640625,
      "learning_rate": 4.4408046816069304e-05,
      "loss": 0.6962,
      "step": 735400
    },
    {
      "epoch": 6.7112562960800055,
      "grad_norm": 3.792449951171875,
      "learning_rate": 4.4407286419933334e-05,
      "loss": 0.7109,
      "step": 735500
    },
    {
      "epoch": 6.712168771443171,
      "grad_norm": 3.2769789695739746,
      "learning_rate": 4.440652602379736e-05,
      "loss": 0.6595,
      "step": 735600
    },
    {
      "epoch": 6.713081246806336,
      "grad_norm": 2.821986436843872,
      "learning_rate": 4.440576562766139e-05,
      "loss": 0.7142,
      "step": 735700
    },
    {
      "epoch": 6.713993722169501,
      "grad_norm": 3.366753578186035,
      "learning_rate": 4.440500523152542e-05,
      "loss": 0.7018,
      "step": 735800
    },
    {
      "epoch": 6.714906197532667,
      "grad_norm": 2.909071445465088,
      "learning_rate": 4.440424483538945e-05,
      "loss": 0.7084,
      "step": 735900
    },
    {
      "epoch": 6.715818672895832,
      "grad_norm": 4.925312519073486,
      "learning_rate": 4.440348443925347e-05,
      "loss": 0.6846,
      "step": 736000
    },
    {
      "epoch": 6.716731148258997,
      "grad_norm": 4.782595157623291,
      "learning_rate": 4.440272404311751e-05,
      "loss": 0.7569,
      "step": 736100
    },
    {
      "epoch": 6.717643623622163,
      "grad_norm": 4.492761135101318,
      "learning_rate": 4.440196364698153e-05,
      "loss": 0.7226,
      "step": 736200
    },
    {
      "epoch": 6.718556098985327,
      "grad_norm": 3.571323871612549,
      "learning_rate": 4.440120325084556e-05,
      "loss": 0.7132,
      "step": 736300
    },
    {
      "epoch": 6.719468574348492,
      "grad_norm": 3.4130306243896484,
      "learning_rate": 4.440044285470959e-05,
      "loss": 0.693,
      "step": 736400
    },
    {
      "epoch": 6.720381049711658,
      "grad_norm": 4.321424961090088,
      "learning_rate": 4.439968245857362e-05,
      "loss": 0.7217,
      "step": 736500
    },
    {
      "epoch": 6.721293525074823,
      "grad_norm": 4.386263370513916,
      "learning_rate": 4.439892206243765e-05,
      "loss": 0.725,
      "step": 736600
    },
    {
      "epoch": 6.722206000437988,
      "grad_norm": 4.727959156036377,
      "learning_rate": 4.439816166630168e-05,
      "loss": 0.6968,
      "step": 736700
    },
    {
      "epoch": 6.7231184758011535,
      "grad_norm": 3.237840414047241,
      "learning_rate": 4.4397401270165704e-05,
      "loss": 0.7044,
      "step": 736800
    },
    {
      "epoch": 6.724030951164319,
      "grad_norm": 4.079392910003662,
      "learning_rate": 4.439664087402974e-05,
      "loss": 0.7108,
      "step": 736900
    },
    {
      "epoch": 6.724943426527484,
      "grad_norm": 4.3344292640686035,
      "learning_rate": 4.4395880477893764e-05,
      "loss": 0.7084,
      "step": 737000
    },
    {
      "epoch": 6.7258559018906485,
      "grad_norm": 3.78974986076355,
      "learning_rate": 4.4395120081757794e-05,
      "loss": 0.7008,
      "step": 737100
    },
    {
      "epoch": 6.726768377253814,
      "grad_norm": 4.154512405395508,
      "learning_rate": 4.4394359685621825e-05,
      "loss": 0.7144,
      "step": 737200
    },
    {
      "epoch": 6.727680852616979,
      "grad_norm": 4.124074459075928,
      "learning_rate": 4.4393599289485855e-05,
      "loss": 0.6841,
      "step": 737300
    },
    {
      "epoch": 6.728593327980144,
      "grad_norm": 2.7904844284057617,
      "learning_rate": 4.439283889334988e-05,
      "loss": 0.6844,
      "step": 737400
    },
    {
      "epoch": 6.72950580334331,
      "grad_norm": 2.537768602371216,
      "learning_rate": 4.4392078497213915e-05,
      "loss": 0.7446,
      "step": 737500
    },
    {
      "epoch": 6.730418278706475,
      "grad_norm": 4.032204627990723,
      "learning_rate": 4.439131810107794e-05,
      "loss": 0.6534,
      "step": 737600
    },
    {
      "epoch": 6.73133075406964,
      "grad_norm": 2.9418997764587402,
      "learning_rate": 4.439055770494197e-05,
      "loss": 0.7103,
      "step": 737700
    },
    {
      "epoch": 6.732243229432806,
      "grad_norm": 3.139946937561035,
      "learning_rate": 4.4389797308806e-05,
      "loss": 0.6818,
      "step": 737800
    },
    {
      "epoch": 6.733155704795971,
      "grad_norm": 4.624974250793457,
      "learning_rate": 4.438903691267003e-05,
      "loss": 0.7254,
      "step": 737900
    },
    {
      "epoch": 6.734068180159135,
      "grad_norm": 4.481121063232422,
      "learning_rate": 4.438827651653406e-05,
      "loss": 0.7268,
      "step": 738000
    },
    {
      "epoch": 6.734980655522301,
      "grad_norm": 2.023796319961548,
      "learning_rate": 4.438751612039809e-05,
      "loss": 0.7038,
      "step": 738100
    },
    {
      "epoch": 6.735893130885466,
      "grad_norm": 3.899416446685791,
      "learning_rate": 4.438675572426211e-05,
      "loss": 0.677,
      "step": 738200
    },
    {
      "epoch": 6.736805606248631,
      "grad_norm": 4.331249713897705,
      "learning_rate": 4.438599532812615e-05,
      "loss": 0.7047,
      "step": 738300
    },
    {
      "epoch": 6.7377180816117965,
      "grad_norm": 3.664107084274292,
      "learning_rate": 4.438523493199017e-05,
      "loss": 0.7249,
      "step": 738400
    },
    {
      "epoch": 6.738630556974962,
      "grad_norm": 3.2604713439941406,
      "learning_rate": 4.4384474535854195e-05,
      "loss": 0.6893,
      "step": 738500
    },
    {
      "epoch": 6.739543032338127,
      "grad_norm": 8.751185417175293,
      "learning_rate": 4.438371413971823e-05,
      "loss": 0.6592,
      "step": 738600
    },
    {
      "epoch": 6.740455507701292,
      "grad_norm": 3.802650213241577,
      "learning_rate": 4.4382953743582255e-05,
      "loss": 0.7226,
      "step": 738700
    },
    {
      "epoch": 6.741367983064457,
      "grad_norm": 3.954979181289673,
      "learning_rate": 4.4382193347446285e-05,
      "loss": 0.6887,
      "step": 738800
    },
    {
      "epoch": 6.742280458427622,
      "grad_norm": 3.3518459796905518,
      "learning_rate": 4.4381432951310315e-05,
      "loss": 0.7344,
      "step": 738900
    },
    {
      "epoch": 6.743192933790787,
      "grad_norm": 3.5131630897521973,
      "learning_rate": 4.4380672555174345e-05,
      "loss": 0.7566,
      "step": 739000
    },
    {
      "epoch": 6.744105409153953,
      "grad_norm": 3.6592442989349365,
      "learning_rate": 4.4379912159038375e-05,
      "loss": 0.7427,
      "step": 739100
    },
    {
      "epoch": 6.745017884517118,
      "grad_norm": 3.933202028274536,
      "learning_rate": 4.4379151762902406e-05,
      "loss": 0.7144,
      "step": 739200
    },
    {
      "epoch": 6.745930359880283,
      "grad_norm": 3.6201155185699463,
      "learning_rate": 4.437839136676643e-05,
      "loss": 0.7313,
      "step": 739300
    },
    {
      "epoch": 6.746842835243449,
      "grad_norm": 3.941542863845825,
      "learning_rate": 4.4377630970630466e-05,
      "loss": 0.7019,
      "step": 739400
    },
    {
      "epoch": 6.747755310606614,
      "grad_norm": 4.073419570922852,
      "learning_rate": 4.437687057449449e-05,
      "loss": 0.7359,
      "step": 739500
    },
    {
      "epoch": 6.748667785969779,
      "grad_norm": 3.25899338722229,
      "learning_rate": 4.437611017835852e-05,
      "loss": 0.6636,
      "step": 739600
    },
    {
      "epoch": 6.749580261332944,
      "grad_norm": 4.288671970367432,
      "learning_rate": 4.437534978222255e-05,
      "loss": 0.7113,
      "step": 739700
    },
    {
      "epoch": 6.750492736696109,
      "grad_norm": 3.4881269931793213,
      "learning_rate": 4.437458938608658e-05,
      "loss": 0.6652,
      "step": 739800
    },
    {
      "epoch": 6.751405212059274,
      "grad_norm": 4.178130626678467,
      "learning_rate": 4.43738289899506e-05,
      "loss": 0.7201,
      "step": 739900
    },
    {
      "epoch": 6.7523176874224395,
      "grad_norm": 3.3824265003204346,
      "learning_rate": 4.437306859381464e-05,
      "loss": 0.68,
      "step": 740000
    },
    {
      "epoch": 6.753230162785605,
      "grad_norm": 3.495905876159668,
      "learning_rate": 4.437230819767866e-05,
      "loss": 0.6993,
      "step": 740100
    },
    {
      "epoch": 6.75414263814877,
      "grad_norm": 3.7055509090423584,
      "learning_rate": 4.437154780154269e-05,
      "loss": 0.716,
      "step": 740200
    },
    {
      "epoch": 6.755055113511935,
      "grad_norm": 3.7263777256011963,
      "learning_rate": 4.437078740540672e-05,
      "loss": 0.7196,
      "step": 740300
    },
    {
      "epoch": 6.755967588875101,
      "grad_norm": 2.9136881828308105,
      "learning_rate": 4.437002700927075e-05,
      "loss": 0.7284,
      "step": 740400
    },
    {
      "epoch": 6.756880064238265,
      "grad_norm": 4.410844802856445,
      "learning_rate": 4.436926661313478e-05,
      "loss": 0.7341,
      "step": 740500
    },
    {
      "epoch": 6.7577925396014304,
      "grad_norm": 3.6575000286102295,
      "learning_rate": 4.436850621699881e-05,
      "loss": 0.6886,
      "step": 740600
    },
    {
      "epoch": 6.758705014964596,
      "grad_norm": 4.043463230133057,
      "learning_rate": 4.4367745820862836e-05,
      "loss": 0.6907,
      "step": 740700
    },
    {
      "epoch": 6.759617490327761,
      "grad_norm": 3.5906784534454346,
      "learning_rate": 4.436698542472687e-05,
      "loss": 0.6934,
      "step": 740800
    },
    {
      "epoch": 6.760529965690926,
      "grad_norm": 4.209700584411621,
      "learning_rate": 4.4366225028590896e-05,
      "loss": 0.6801,
      "step": 740900
    },
    {
      "epoch": 6.761442441054092,
      "grad_norm": 4.521480560302734,
      "learning_rate": 4.4365464632454926e-05,
      "loss": 0.7013,
      "step": 741000
    },
    {
      "epoch": 6.762354916417257,
      "grad_norm": 3.582505702972412,
      "learning_rate": 4.4364704236318957e-05,
      "loss": 0.7151,
      "step": 741100
    },
    {
      "epoch": 6.763267391780422,
      "grad_norm": 3.7679667472839355,
      "learning_rate": 4.4363943840182987e-05,
      "loss": 0.6926,
      "step": 741200
    },
    {
      "epoch": 6.7641798671435875,
      "grad_norm": 4.328427314758301,
      "learning_rate": 4.436318344404701e-05,
      "loss": 0.6969,
      "step": 741300
    },
    {
      "epoch": 6.765092342506752,
      "grad_norm": 3.562167167663574,
      "learning_rate": 4.436242304791104e-05,
      "loss": 0.6786,
      "step": 741400
    },
    {
      "epoch": 6.766004817869917,
      "grad_norm": 4.23756217956543,
      "learning_rate": 4.436166265177507e-05,
      "loss": 0.7043,
      "step": 741500
    },
    {
      "epoch": 6.7669172932330826,
      "grad_norm": 4.952280044555664,
      "learning_rate": 4.43609022556391e-05,
      "loss": 0.725,
      "step": 741600
    },
    {
      "epoch": 6.767829768596248,
      "grad_norm": 3.479663372039795,
      "learning_rate": 4.436014185950313e-05,
      "loss": 0.7203,
      "step": 741700
    },
    {
      "epoch": 6.768742243959413,
      "grad_norm": 4.160818099975586,
      "learning_rate": 4.4359381463367153e-05,
      "loss": 0.7136,
      "step": 741800
    },
    {
      "epoch": 6.7696547193225785,
      "grad_norm": 3.4889862537384033,
      "learning_rate": 4.435862106723119e-05,
      "loss": 0.751,
      "step": 741900
    },
    {
      "epoch": 6.770567194685744,
      "grad_norm": 3.7035558223724365,
      "learning_rate": 4.4357860671095214e-05,
      "loss": 0.6789,
      "step": 742000
    },
    {
      "epoch": 6.771479670048909,
      "grad_norm": 4.617015838623047,
      "learning_rate": 4.4357100274959244e-05,
      "loss": 0.724,
      "step": 742100
    },
    {
      "epoch": 6.7723921454120735,
      "grad_norm": 3.716423749923706,
      "learning_rate": 4.4356339878823274e-05,
      "loss": 0.7051,
      "step": 742200
    },
    {
      "epoch": 6.773304620775239,
      "grad_norm": 3.5198276042938232,
      "learning_rate": 4.4355579482687304e-05,
      "loss": 0.6773,
      "step": 742300
    },
    {
      "epoch": 6.774217096138404,
      "grad_norm": 3.8777401447296143,
      "learning_rate": 4.435481908655133e-05,
      "loss": 0.6665,
      "step": 742400
    },
    {
      "epoch": 6.775129571501569,
      "grad_norm": 4.546194553375244,
      "learning_rate": 4.4354058690415364e-05,
      "loss": 0.7275,
      "step": 742500
    },
    {
      "epoch": 6.776042046864735,
      "grad_norm": 4.278651714324951,
      "learning_rate": 4.435329829427939e-05,
      "loss": 0.7199,
      "step": 742600
    },
    {
      "epoch": 6.7769545222279,
      "grad_norm": 3.3725273609161377,
      "learning_rate": 4.435253789814342e-05,
      "loss": 0.6777,
      "step": 742700
    },
    {
      "epoch": 6.777866997591065,
      "grad_norm": 5.338140964508057,
      "learning_rate": 4.435177750200745e-05,
      "loss": 0.7,
      "step": 742800
    },
    {
      "epoch": 6.778779472954231,
      "grad_norm": 2.9157395362854004,
      "learning_rate": 4.435101710587148e-05,
      "loss": 0.7019,
      "step": 742900
    },
    {
      "epoch": 6.779691948317396,
      "grad_norm": 4.306320667266846,
      "learning_rate": 4.435025670973551e-05,
      "loss": 0.7245,
      "step": 743000
    },
    {
      "epoch": 6.78060442368056,
      "grad_norm": 2.9928512573242188,
      "learning_rate": 4.434949631359954e-05,
      "loss": 0.7015,
      "step": 743100
    },
    {
      "epoch": 6.781516899043726,
      "grad_norm": 5.256629943847656,
      "learning_rate": 4.434873591746356e-05,
      "loss": 0.7069,
      "step": 743200
    },
    {
      "epoch": 6.782429374406891,
      "grad_norm": 2.70690655708313,
      "learning_rate": 4.43479755213276e-05,
      "loss": 0.7088,
      "step": 743300
    },
    {
      "epoch": 6.783341849770056,
      "grad_norm": 3.591491460800171,
      "learning_rate": 4.434721512519162e-05,
      "loss": 0.7207,
      "step": 743400
    },
    {
      "epoch": 6.7842543251332215,
      "grad_norm": 3.458916664123535,
      "learning_rate": 4.434645472905565e-05,
      "loss": 0.6658,
      "step": 743500
    },
    {
      "epoch": 6.785166800496387,
      "grad_norm": 4.132885456085205,
      "learning_rate": 4.434569433291968e-05,
      "loss": 0.6965,
      "step": 743600
    },
    {
      "epoch": 6.786079275859552,
      "grad_norm": 4.322997093200684,
      "learning_rate": 4.434493393678371e-05,
      "loss": 0.6749,
      "step": 743700
    },
    {
      "epoch": 6.786991751222717,
      "grad_norm": 3.2161431312561035,
      "learning_rate": 4.4344173540647734e-05,
      "loss": 0.7136,
      "step": 743800
    },
    {
      "epoch": 6.787904226585882,
      "grad_norm": 3.7335684299468994,
      "learning_rate": 4.434341314451177e-05,
      "loss": 0.6924,
      "step": 743900
    },
    {
      "epoch": 6.788816701949047,
      "grad_norm": 4.320126056671143,
      "learning_rate": 4.4342652748375795e-05,
      "loss": 0.7302,
      "step": 744000
    },
    {
      "epoch": 6.789729177312212,
      "grad_norm": 4.0258684158325195,
      "learning_rate": 4.4341892352239825e-05,
      "loss": 0.6907,
      "step": 744100
    },
    {
      "epoch": 6.790641652675378,
      "grad_norm": 4.614102840423584,
      "learning_rate": 4.4341131956103855e-05,
      "loss": 0.7091,
      "step": 744200
    },
    {
      "epoch": 6.791554128038543,
      "grad_norm": 4.107690811157227,
      "learning_rate": 4.434037155996788e-05,
      "loss": 0.7251,
      "step": 744300
    },
    {
      "epoch": 6.792466603401708,
      "grad_norm": 3.7915849685668945,
      "learning_rate": 4.4339611163831915e-05,
      "loss": 0.7188,
      "step": 744400
    },
    {
      "epoch": 6.793379078764874,
      "grad_norm": 4.2940874099731445,
      "learning_rate": 4.433885076769594e-05,
      "loss": 0.712,
      "step": 744500
    },
    {
      "epoch": 6.794291554128039,
      "grad_norm": 4.715897560119629,
      "learning_rate": 4.433809037155997e-05,
      "loss": 0.7128,
      "step": 744600
    },
    {
      "epoch": 6.795204029491204,
      "grad_norm": 4.6047043800354,
      "learning_rate": 4.4337329975424e-05,
      "loss": 0.724,
      "step": 744700
    },
    {
      "epoch": 6.796116504854369,
      "grad_norm": 4.7720561027526855,
      "learning_rate": 4.433656957928803e-05,
      "loss": 0.6875,
      "step": 744800
    },
    {
      "epoch": 6.797028980217534,
      "grad_norm": 4.444003582000732,
      "learning_rate": 4.433580918315205e-05,
      "loss": 0.7274,
      "step": 744900
    },
    {
      "epoch": 6.797941455580699,
      "grad_norm": 5.09407901763916,
      "learning_rate": 4.433504878701609e-05,
      "loss": 0.6792,
      "step": 745000
    },
    {
      "epoch": 6.7988539309438645,
      "grad_norm": 3.5822579860687256,
      "learning_rate": 4.433428839088011e-05,
      "loss": 0.704,
      "step": 745100
    },
    {
      "epoch": 6.79976640630703,
      "grad_norm": 4.202481269836426,
      "learning_rate": 4.433352799474414e-05,
      "loss": 0.7614,
      "step": 745200
    },
    {
      "epoch": 6.800678881670195,
      "grad_norm": 4.1388115882873535,
      "learning_rate": 4.433276759860817e-05,
      "loss": 0.7325,
      "step": 745300
    },
    {
      "epoch": 6.80159135703336,
      "grad_norm": 3.911405324935913,
      "learning_rate": 4.43320072024722e-05,
      "loss": 0.6872,
      "step": 745400
    },
    {
      "epoch": 6.802503832396526,
      "grad_norm": 3.6420388221740723,
      "learning_rate": 4.433124680633623e-05,
      "loss": 0.6762,
      "step": 745500
    },
    {
      "epoch": 6.80341630775969,
      "grad_norm": 4.0267486572265625,
      "learning_rate": 4.433048641020026e-05,
      "loss": 0.6696,
      "step": 745600
    },
    {
      "epoch": 6.804328783122855,
      "grad_norm": 4.247797966003418,
      "learning_rate": 4.4329726014064285e-05,
      "loss": 0.6745,
      "step": 745700
    },
    {
      "epoch": 6.805241258486021,
      "grad_norm": 3.6337947845458984,
      "learning_rate": 4.432896561792832e-05,
      "loss": 0.6908,
      "step": 745800
    },
    {
      "epoch": 6.806153733849186,
      "grad_norm": 3.6098008155822754,
      "learning_rate": 4.4328205221792346e-05,
      "loss": 0.6907,
      "step": 745900
    },
    {
      "epoch": 6.807066209212351,
      "grad_norm": 3.7660958766937256,
      "learning_rate": 4.4327444825656376e-05,
      "loss": 0.7001,
      "step": 746000
    },
    {
      "epoch": 6.807978684575517,
      "grad_norm": 4.2920026779174805,
      "learning_rate": 4.4326684429520406e-05,
      "loss": 0.6911,
      "step": 746100
    },
    {
      "epoch": 6.808891159938682,
      "grad_norm": 4.674478054046631,
      "learning_rate": 4.4325924033384436e-05,
      "loss": 0.7047,
      "step": 746200
    },
    {
      "epoch": 6.809803635301847,
      "grad_norm": 4.140860557556152,
      "learning_rate": 4.4325163637248466e-05,
      "loss": 0.7057,
      "step": 746300
    },
    {
      "epoch": 6.8107161106650125,
      "grad_norm": 4.691830158233643,
      "learning_rate": 4.4324403241112496e-05,
      "loss": 0.7057,
      "step": 746400
    },
    {
      "epoch": 6.811628586028177,
      "grad_norm": 5.511941909790039,
      "learning_rate": 4.432364284497652e-05,
      "loss": 0.6973,
      "step": 746500
    },
    {
      "epoch": 6.812541061391342,
      "grad_norm": 4.710936069488525,
      "learning_rate": 4.432288244884055e-05,
      "loss": 0.704,
      "step": 746600
    },
    {
      "epoch": 6.8134535367545075,
      "grad_norm": 3.523682117462158,
      "learning_rate": 4.432212205270458e-05,
      "loss": 0.6781,
      "step": 746700
    },
    {
      "epoch": 6.814366012117673,
      "grad_norm": 5.048026084899902,
      "learning_rate": 4.432136165656861e-05,
      "loss": 0.6829,
      "step": 746800
    },
    {
      "epoch": 6.815278487480838,
      "grad_norm": 3.5604755878448486,
      "learning_rate": 4.432060126043264e-05,
      "loss": 0.7417,
      "step": 746900
    },
    {
      "epoch": 6.816190962844003,
      "grad_norm": 3.810058116912842,
      "learning_rate": 4.431984086429666e-05,
      "loss": 0.7213,
      "step": 747000
    },
    {
      "epoch": 6.817103438207169,
      "grad_norm": 3.9886584281921387,
      "learning_rate": 4.431908046816069e-05,
      "loss": 0.7353,
      "step": 747100
    },
    {
      "epoch": 6.818015913570334,
      "grad_norm": 3.7051286697387695,
      "learning_rate": 4.431832007202472e-05,
      "loss": 0.7044,
      "step": 747200
    },
    {
      "epoch": 6.818928388933498,
      "grad_norm": 3.8836965560913086,
      "learning_rate": 4.431755967588875e-05,
      "loss": 0.6809,
      "step": 747300
    },
    {
      "epoch": 6.819840864296664,
      "grad_norm": 4.175586223602295,
      "learning_rate": 4.431679927975278e-05,
      "loss": 0.7109,
      "step": 747400
    },
    {
      "epoch": 6.820753339659829,
      "grad_norm": 4.588982105255127,
      "learning_rate": 4.431603888361681e-05,
      "loss": 0.6596,
      "step": 747500
    },
    {
      "epoch": 6.821665815022994,
      "grad_norm": 3.472280979156494,
      "learning_rate": 4.4315278487480836e-05,
      "loss": 0.6802,
      "step": 747600
    },
    {
      "epoch": 6.82257829038616,
      "grad_norm": 7.078397274017334,
      "learning_rate": 4.431451809134487e-05,
      "loss": 0.6743,
      "step": 747700
    },
    {
      "epoch": 6.823490765749325,
      "grad_norm": 4.379862308502197,
      "learning_rate": 4.4313757695208896e-05,
      "loss": 0.6905,
      "step": 747800
    },
    {
      "epoch": 6.82440324111249,
      "grad_norm": 4.136855125427246,
      "learning_rate": 4.4312997299072927e-05,
      "loss": 0.72,
      "step": 747900
    },
    {
      "epoch": 6.8253157164756555,
      "grad_norm": 4.189576625823975,
      "learning_rate": 4.431223690293696e-05,
      "loss": 0.7366,
      "step": 748000
    },
    {
      "epoch": 6.826228191838821,
      "grad_norm": 3.8314642906188965,
      "learning_rate": 4.431147650680099e-05,
      "loss": 0.7101,
      "step": 748100
    },
    {
      "epoch": 6.827140667201985,
      "grad_norm": 3.561613082885742,
      "learning_rate": 4.431071611066501e-05,
      "loss": 0.7504,
      "step": 748200
    },
    {
      "epoch": 6.8280531425651505,
      "grad_norm": 4.125001907348633,
      "learning_rate": 4.430995571452905e-05,
      "loss": 0.7132,
      "step": 748300
    },
    {
      "epoch": 6.828965617928316,
      "grad_norm": 5.287569522857666,
      "learning_rate": 4.430919531839307e-05,
      "loss": 0.7093,
      "step": 748400
    },
    {
      "epoch": 6.829878093291481,
      "grad_norm": 4.988315105438232,
      "learning_rate": 4.43084349222571e-05,
      "loss": 0.7094,
      "step": 748500
    },
    {
      "epoch": 6.830790568654646,
      "grad_norm": 3.007150411605835,
      "learning_rate": 4.430767452612113e-05,
      "loss": 0.7131,
      "step": 748600
    },
    {
      "epoch": 6.831703044017812,
      "grad_norm": 3.850097894668579,
      "learning_rate": 4.430691412998516e-05,
      "loss": 0.6871,
      "step": 748700
    },
    {
      "epoch": 6.832615519380977,
      "grad_norm": 4.1678338050842285,
      "learning_rate": 4.430615373384919e-05,
      "loss": 0.6926,
      "step": 748800
    },
    {
      "epoch": 6.833527994744141,
      "grad_norm": 4.1137309074401855,
      "learning_rate": 4.430539333771322e-05,
      "loss": 0.6836,
      "step": 748900
    },
    {
      "epoch": 6.834440470107307,
      "grad_norm": 3.8785274028778076,
      "learning_rate": 4.4304632941577244e-05,
      "loss": 0.7036,
      "step": 749000
    },
    {
      "epoch": 6.835352945470472,
      "grad_norm": 4.301828861236572,
      "learning_rate": 4.430387254544128e-05,
      "loss": 0.6725,
      "step": 749100
    },
    {
      "epoch": 6.836265420833637,
      "grad_norm": 2.450716733932495,
      "learning_rate": 4.4303112149305304e-05,
      "loss": 0.7305,
      "step": 749200
    },
    {
      "epoch": 6.837177896196803,
      "grad_norm": 4.073756694793701,
      "learning_rate": 4.4302351753169334e-05,
      "loss": 0.6801,
      "step": 749300
    },
    {
      "epoch": 6.838090371559968,
      "grad_norm": 5.553459167480469,
      "learning_rate": 4.4301591357033364e-05,
      "loss": 0.7192,
      "step": 749400
    },
    {
      "epoch": 6.839002846923133,
      "grad_norm": 4.040101051330566,
      "learning_rate": 4.4300830960897394e-05,
      "loss": 0.6849,
      "step": 749500
    },
    {
      "epoch": 6.8399153222862985,
      "grad_norm": 4.0107035636901855,
      "learning_rate": 4.430007056476142e-05,
      "loss": 0.7087,
      "step": 749600
    },
    {
      "epoch": 6.840827797649464,
      "grad_norm": 4.207722187042236,
      "learning_rate": 4.4299310168625454e-05,
      "loss": 0.7144,
      "step": 749700
    },
    {
      "epoch": 6.841740273012629,
      "grad_norm": 4.127974987030029,
      "learning_rate": 4.429854977248948e-05,
      "loss": 0.724,
      "step": 749800
    },
    {
      "epoch": 6.8426527483757935,
      "grad_norm": 3.862074136734009,
      "learning_rate": 4.429778937635351e-05,
      "loss": 0.7111,
      "step": 749900
    },
    {
      "epoch": 6.843565223738959,
      "grad_norm": 4.88274621963501,
      "learning_rate": 4.429702898021754e-05,
      "loss": 0.6754,
      "step": 750000
    },
    {
      "epoch": 6.844477699102124,
      "grad_norm": 4.459415435791016,
      "learning_rate": 4.429626858408156e-05,
      "loss": 0.6892,
      "step": 750100
    },
    {
      "epoch": 6.845390174465289,
      "grad_norm": 3.56453275680542,
      "learning_rate": 4.42955081879456e-05,
      "loss": 0.6817,
      "step": 750200
    },
    {
      "epoch": 6.846302649828455,
      "grad_norm": 4.7116289138793945,
      "learning_rate": 4.429474779180962e-05,
      "loss": 0.6836,
      "step": 750300
    },
    {
      "epoch": 6.84721512519162,
      "grad_norm": 4.424437046051025,
      "learning_rate": 4.429398739567365e-05,
      "loss": 0.7156,
      "step": 750400
    },
    {
      "epoch": 6.848127600554785,
      "grad_norm": 4.192497730255127,
      "learning_rate": 4.429322699953768e-05,
      "loss": 0.6746,
      "step": 750500
    },
    {
      "epoch": 6.84904007591795,
      "grad_norm": 4.400852203369141,
      "learning_rate": 4.429246660340171e-05,
      "loss": 0.7282,
      "step": 750600
    },
    {
      "epoch": 6.849952551281115,
      "grad_norm": 4.271945476531982,
      "learning_rate": 4.4291706207265735e-05,
      "loss": 0.6784,
      "step": 750700
    },
    {
      "epoch": 6.85086502664428,
      "grad_norm": 3.768873691558838,
      "learning_rate": 4.429094581112977e-05,
      "loss": 0.6765,
      "step": 750800
    },
    {
      "epoch": 6.851777502007446,
      "grad_norm": 2.6098036766052246,
      "learning_rate": 4.4290185414993795e-05,
      "loss": 0.7126,
      "step": 750900
    },
    {
      "epoch": 6.852689977370611,
      "grad_norm": 4.004312992095947,
      "learning_rate": 4.4289425018857825e-05,
      "loss": 0.6729,
      "step": 751000
    },
    {
      "epoch": 6.853602452733776,
      "grad_norm": 3.9126482009887695,
      "learning_rate": 4.4288664622721855e-05,
      "loss": 0.6664,
      "step": 751100
    },
    {
      "epoch": 6.8545149280969415,
      "grad_norm": 4.046774864196777,
      "learning_rate": 4.4287904226585885e-05,
      "loss": 0.6987,
      "step": 751200
    },
    {
      "epoch": 6.855427403460107,
      "grad_norm": 3.930245876312256,
      "learning_rate": 4.4287143830449915e-05,
      "loss": 0.6883,
      "step": 751300
    },
    {
      "epoch": 6.856339878823272,
      "grad_norm": 3.631669759750366,
      "learning_rate": 4.4286383434313945e-05,
      "loss": 0.6744,
      "step": 751400
    },
    {
      "epoch": 6.857252354186437,
      "grad_norm": 3.838818073272705,
      "learning_rate": 4.428562303817797e-05,
      "loss": 0.7041,
      "step": 751500
    },
    {
      "epoch": 6.858164829549602,
      "grad_norm": 4.802136421203613,
      "learning_rate": 4.4284862642042005e-05,
      "loss": 0.7401,
      "step": 751600
    },
    {
      "epoch": 6.859077304912767,
      "grad_norm": 3.828219413757324,
      "learning_rate": 4.428410224590603e-05,
      "loss": 0.7035,
      "step": 751700
    },
    {
      "epoch": 6.859989780275932,
      "grad_norm": 3.774829149246216,
      "learning_rate": 4.428334184977006e-05,
      "loss": 0.7078,
      "step": 751800
    },
    {
      "epoch": 6.860902255639098,
      "grad_norm": 4.1249003410339355,
      "learning_rate": 4.428258145363409e-05,
      "loss": 0.6837,
      "step": 751900
    },
    {
      "epoch": 6.861814731002263,
      "grad_norm": 4.04304838180542,
      "learning_rate": 4.428182105749812e-05,
      "loss": 0.7191,
      "step": 752000
    },
    {
      "epoch": 6.862727206365428,
      "grad_norm": 4.7974371910095215,
      "learning_rate": 4.428106066136214e-05,
      "loss": 0.7193,
      "step": 752100
    },
    {
      "epoch": 6.863639681728594,
      "grad_norm": 4.0334858894348145,
      "learning_rate": 4.428030026522618e-05,
      "loss": 0.684,
      "step": 752200
    },
    {
      "epoch": 6.864552157091758,
      "grad_norm": 3.3113505840301514,
      "learning_rate": 4.42795398690902e-05,
      "loss": 0.729,
      "step": 752300
    },
    {
      "epoch": 6.865464632454923,
      "grad_norm": 3.9025087356567383,
      "learning_rate": 4.427877947295423e-05,
      "loss": 0.6869,
      "step": 752400
    },
    {
      "epoch": 6.866377107818089,
      "grad_norm": 2.8964457511901855,
      "learning_rate": 4.427801907681826e-05,
      "loss": 0.7365,
      "step": 752500
    },
    {
      "epoch": 6.867289583181254,
      "grad_norm": 4.27467679977417,
      "learning_rate": 4.427725868068229e-05,
      "loss": 0.691,
      "step": 752600
    },
    {
      "epoch": 6.868202058544419,
      "grad_norm": 3.9670941829681396,
      "learning_rate": 4.427649828454632e-05,
      "loss": 0.6932,
      "step": 752700
    },
    {
      "epoch": 6.8691145339075845,
      "grad_norm": 3.6182785034179688,
      "learning_rate": 4.4275737888410346e-05,
      "loss": 0.6905,
      "step": 752800
    },
    {
      "epoch": 6.87002700927075,
      "grad_norm": 4.242321968078613,
      "learning_rate": 4.4274977492274376e-05,
      "loss": 0.7371,
      "step": 752900
    },
    {
      "epoch": 6.870939484633915,
      "grad_norm": 3.959416627883911,
      "learning_rate": 4.4274217096138406e-05,
      "loss": 0.7026,
      "step": 753000
    },
    {
      "epoch": 6.87185195999708,
      "grad_norm": 2.844425678253174,
      "learning_rate": 4.4273456700002436e-05,
      "loss": 0.698,
      "step": 753100
    },
    {
      "epoch": 6.872764435360246,
      "grad_norm": 4.3412628173828125,
      "learning_rate": 4.427269630386646e-05,
      "loss": 0.7222,
      "step": 753200
    },
    {
      "epoch": 6.87367691072341,
      "grad_norm": 4.443614482879639,
      "learning_rate": 4.4271935907730496e-05,
      "loss": 0.7211,
      "step": 753300
    },
    {
      "epoch": 6.874589386086575,
      "grad_norm": 4.441691875457764,
      "learning_rate": 4.427117551159452e-05,
      "loss": 0.7125,
      "step": 753400
    },
    {
      "epoch": 6.875501861449741,
      "grad_norm": 5.149247169494629,
      "learning_rate": 4.427041511545855e-05,
      "loss": 0.7246,
      "step": 753500
    },
    {
      "epoch": 6.876414336812906,
      "grad_norm": 4.487112522125244,
      "learning_rate": 4.426965471932258e-05,
      "loss": 0.6479,
      "step": 753600
    },
    {
      "epoch": 6.877326812176071,
      "grad_norm": 4.363757610321045,
      "learning_rate": 4.426889432318661e-05,
      "loss": 0.7019,
      "step": 753700
    },
    {
      "epoch": 6.878239287539237,
      "grad_norm": 3.9420247077941895,
      "learning_rate": 4.426813392705064e-05,
      "loss": 0.7153,
      "step": 753800
    },
    {
      "epoch": 6.879151762902402,
      "grad_norm": 4.4397759437561035,
      "learning_rate": 4.426737353091467e-05,
      "loss": 0.67,
      "step": 753900
    },
    {
      "epoch": 6.880064238265566,
      "grad_norm": 3.0035791397094727,
      "learning_rate": 4.426661313477869e-05,
      "loss": 0.6651,
      "step": 754000
    },
    {
      "epoch": 6.880976713628732,
      "grad_norm": 4.516870498657227,
      "learning_rate": 4.426585273864273e-05,
      "loss": 0.6701,
      "step": 754100
    },
    {
      "epoch": 6.881889188991897,
      "grad_norm": 3.8281314373016357,
      "learning_rate": 4.426509234250675e-05,
      "loss": 0.6903,
      "step": 754200
    },
    {
      "epoch": 6.882801664355062,
      "grad_norm": 4.918989181518555,
      "learning_rate": 4.426433194637078e-05,
      "loss": 0.6626,
      "step": 754300
    },
    {
      "epoch": 6.8837141397182275,
      "grad_norm": 3.9032037258148193,
      "learning_rate": 4.426357155023481e-05,
      "loss": 0.6942,
      "step": 754400
    },
    {
      "epoch": 6.884626615081393,
      "grad_norm": 4.305855751037598,
      "learning_rate": 4.426281115409884e-05,
      "loss": 0.6985,
      "step": 754500
    },
    {
      "epoch": 6.885539090444558,
      "grad_norm": 3.2492048740386963,
      "learning_rate": 4.4262050757962867e-05,
      "loss": 0.7075,
      "step": 754600
    },
    {
      "epoch": 6.886451565807723,
      "grad_norm": 3.8114523887634277,
      "learning_rate": 4.42612903618269e-05,
      "loss": 0.6999,
      "step": 754700
    },
    {
      "epoch": 6.887364041170889,
      "grad_norm": 4.0102949142456055,
      "learning_rate": 4.426052996569093e-05,
      "loss": 0.6846,
      "step": 754800
    },
    {
      "epoch": 6.888276516534054,
      "grad_norm": 3.8480043411254883,
      "learning_rate": 4.425976956955496e-05,
      "loss": 0.7264,
      "step": 754900
    },
    {
      "epoch": 6.8891889918972185,
      "grad_norm": 3.6795809268951416,
      "learning_rate": 4.425900917341899e-05,
      "loss": 0.7036,
      "step": 755000
    },
    {
      "epoch": 6.890101467260384,
      "grad_norm": 3.1680853366851807,
      "learning_rate": 4.425824877728302e-05,
      "loss": 0.7009,
      "step": 755100
    },
    {
      "epoch": 6.891013942623549,
      "grad_norm": 5.037182807922363,
      "learning_rate": 4.425748838114705e-05,
      "loss": 0.6906,
      "step": 755200
    },
    {
      "epoch": 6.891926417986714,
      "grad_norm": 3.838935375213623,
      "learning_rate": 4.425672798501108e-05,
      "loss": 0.731,
      "step": 755300
    },
    {
      "epoch": 6.89283889334988,
      "grad_norm": 4.170586585998535,
      "learning_rate": 4.42559675888751e-05,
      "loss": 0.691,
      "step": 755400
    },
    {
      "epoch": 6.893751368713045,
      "grad_norm": 4.504696369171143,
      "learning_rate": 4.425520719273913e-05,
      "loss": 0.6802,
      "step": 755500
    },
    {
      "epoch": 6.89466384407621,
      "grad_norm": 4.1281328201293945,
      "learning_rate": 4.425444679660316e-05,
      "loss": 0.7052,
      "step": 755600
    },
    {
      "epoch": 6.895576319439375,
      "grad_norm": 4.347054958343506,
      "learning_rate": 4.4253686400467184e-05,
      "loss": 0.707,
      "step": 755700
    },
    {
      "epoch": 6.89648879480254,
      "grad_norm": 4.457136631011963,
      "learning_rate": 4.425292600433122e-05,
      "loss": 0.6929,
      "step": 755800
    },
    {
      "epoch": 6.897401270165705,
      "grad_norm": 3.753711700439453,
      "learning_rate": 4.4252165608195244e-05,
      "loss": 0.6808,
      "step": 755900
    },
    {
      "epoch": 6.8983137455288706,
      "grad_norm": 2.46919846534729,
      "learning_rate": 4.4251405212059274e-05,
      "loss": 0.7323,
      "step": 756000
    },
    {
      "epoch": 6.899226220892036,
      "grad_norm": 4.140567779541016,
      "learning_rate": 4.4250644815923304e-05,
      "loss": 0.7017,
      "step": 756100
    },
    {
      "epoch": 6.900138696255201,
      "grad_norm": 4.375730514526367,
      "learning_rate": 4.4249884419787334e-05,
      "loss": 0.6927,
      "step": 756200
    },
    {
      "epoch": 6.9010511716183665,
      "grad_norm": 4.159852027893066,
      "learning_rate": 4.4249124023651364e-05,
      "loss": 0.7502,
      "step": 756300
    },
    {
      "epoch": 6.901963646981532,
      "grad_norm": 3.6553709506988525,
      "learning_rate": 4.4248363627515394e-05,
      "loss": 0.6982,
      "step": 756400
    },
    {
      "epoch": 6.902876122344697,
      "grad_norm": 4.199901103973389,
      "learning_rate": 4.424760323137942e-05,
      "loss": 0.7087,
      "step": 756500
    },
    {
      "epoch": 6.903788597707862,
      "grad_norm": 3.5866944789886475,
      "learning_rate": 4.4246842835243454e-05,
      "loss": 0.6856,
      "step": 756600
    },
    {
      "epoch": 6.904701073071027,
      "grad_norm": 3.663930654525757,
      "learning_rate": 4.424608243910748e-05,
      "loss": 0.6605,
      "step": 756700
    },
    {
      "epoch": 6.905613548434192,
      "grad_norm": 3.428816795349121,
      "learning_rate": 4.424532204297151e-05,
      "loss": 0.7138,
      "step": 756800
    },
    {
      "epoch": 6.906526023797357,
      "grad_norm": 2.8503713607788086,
      "learning_rate": 4.424456164683554e-05,
      "loss": 0.7063,
      "step": 756900
    },
    {
      "epoch": 6.907438499160523,
      "grad_norm": 3.542673110961914,
      "learning_rate": 4.424380125069957e-05,
      "loss": 0.6601,
      "step": 757000
    },
    {
      "epoch": 6.908350974523688,
      "grad_norm": 4.400412559509277,
      "learning_rate": 4.424304085456359e-05,
      "loss": 0.709,
      "step": 757100
    },
    {
      "epoch": 6.909263449886853,
      "grad_norm": 3.6823577880859375,
      "learning_rate": 4.424228045842763e-05,
      "loss": 0.6604,
      "step": 757200
    },
    {
      "epoch": 6.910175925250019,
      "grad_norm": 3.779933452606201,
      "learning_rate": 4.424152006229165e-05,
      "loss": 0.6933,
      "step": 757300
    },
    {
      "epoch": 6.911088400613183,
      "grad_norm": 4.390562057495117,
      "learning_rate": 4.424075966615568e-05,
      "loss": 0.7051,
      "step": 757400
    },
    {
      "epoch": 6.912000875976348,
      "grad_norm": 3.8317856788635254,
      "learning_rate": 4.423999927001971e-05,
      "loss": 0.7034,
      "step": 757500
    },
    {
      "epoch": 6.912913351339514,
      "grad_norm": 4.930249214172363,
      "learning_rate": 4.423923887388374e-05,
      "loss": 0.7095,
      "step": 757600
    },
    {
      "epoch": 6.913825826702679,
      "grad_norm": 3.6359102725982666,
      "learning_rate": 4.423847847774777e-05,
      "loss": 0.6814,
      "step": 757700
    },
    {
      "epoch": 6.914738302065844,
      "grad_norm": 5.011292457580566,
      "learning_rate": 4.42377180816118e-05,
      "loss": 0.7084,
      "step": 757800
    },
    {
      "epoch": 6.9156507774290095,
      "grad_norm": 4.0985798835754395,
      "learning_rate": 4.4236957685475825e-05,
      "loss": 0.7143,
      "step": 757900
    },
    {
      "epoch": 6.916563252792175,
      "grad_norm": 3.7709717750549316,
      "learning_rate": 4.423619728933986e-05,
      "loss": 0.6935,
      "step": 758000
    },
    {
      "epoch": 6.91747572815534,
      "grad_norm": 4.816951751708984,
      "learning_rate": 4.4235436893203885e-05,
      "loss": 0.7093,
      "step": 758100
    },
    {
      "epoch": 6.918388203518505,
      "grad_norm": 3.431016683578491,
      "learning_rate": 4.4234676497067915e-05,
      "loss": 0.715,
      "step": 758200
    },
    {
      "epoch": 6.919300678881671,
      "grad_norm": 4.647478103637695,
      "learning_rate": 4.4233916100931945e-05,
      "loss": 0.7233,
      "step": 758300
    },
    {
      "epoch": 6.920213154244835,
      "grad_norm": 4.694158554077148,
      "learning_rate": 4.423315570479597e-05,
      "loss": 0.7398,
      "step": 758400
    },
    {
      "epoch": 6.921125629608,
      "grad_norm": 3.518735885620117,
      "learning_rate": 4.423239530866e-05,
      "loss": 0.7086,
      "step": 758500
    },
    {
      "epoch": 6.922038104971166,
      "grad_norm": 4.500468730926514,
      "learning_rate": 4.423163491252403e-05,
      "loss": 0.6725,
      "step": 758600
    },
    {
      "epoch": 6.922950580334331,
      "grad_norm": 3.3827261924743652,
      "learning_rate": 4.423087451638806e-05,
      "loss": 0.7206,
      "step": 758700
    },
    {
      "epoch": 6.923863055697496,
      "grad_norm": 4.481821060180664,
      "learning_rate": 4.423011412025209e-05,
      "loss": 0.6962,
      "step": 758800
    },
    {
      "epoch": 6.924775531060662,
      "grad_norm": 4.394136905670166,
      "learning_rate": 4.422935372411612e-05,
      "loss": 0.6772,
      "step": 758900
    },
    {
      "epoch": 6.925688006423827,
      "grad_norm": 2.4604053497314453,
      "learning_rate": 4.422859332798014e-05,
      "loss": 0.6886,
      "step": 759000
    },
    {
      "epoch": 6.926600481786991,
      "grad_norm": 4.551462173461914,
      "learning_rate": 4.422783293184418e-05,
      "loss": 0.7217,
      "step": 759100
    },
    {
      "epoch": 6.927512957150157,
      "grad_norm": 3.953071117401123,
      "learning_rate": 4.42270725357082e-05,
      "loss": 0.7256,
      "step": 759200
    },
    {
      "epoch": 6.928425432513322,
      "grad_norm": 4.26982307434082,
      "learning_rate": 4.422631213957223e-05,
      "loss": 0.7075,
      "step": 759300
    },
    {
      "epoch": 6.929337907876487,
      "grad_norm": 2.892392158508301,
      "learning_rate": 4.422555174343626e-05,
      "loss": 0.7166,
      "step": 759400
    },
    {
      "epoch": 6.9302503832396525,
      "grad_norm": 3.398183822631836,
      "learning_rate": 4.422479134730029e-05,
      "loss": 0.7028,
      "step": 759500
    },
    {
      "epoch": 6.931162858602818,
      "grad_norm": 3.6208581924438477,
      "learning_rate": 4.422403095116432e-05,
      "loss": 0.7185,
      "step": 759600
    },
    {
      "epoch": 6.932075333965983,
      "grad_norm": 3.7753610610961914,
      "learning_rate": 4.422327055502835e-05,
      "loss": 0.7173,
      "step": 759700
    },
    {
      "epoch": 6.932987809329148,
      "grad_norm": 4.279267311096191,
      "learning_rate": 4.4222510158892376e-05,
      "loss": 0.6525,
      "step": 759800
    },
    {
      "epoch": 6.933900284692314,
      "grad_norm": 4.319032192230225,
      "learning_rate": 4.4221749762756406e-05,
      "loss": 0.7316,
      "step": 759900
    },
    {
      "epoch": 6.934812760055479,
      "grad_norm": 3.8253238201141357,
      "learning_rate": 4.4220989366620436e-05,
      "loss": 0.6905,
      "step": 760000
    },
    {
      "epoch": 6.935725235418643,
      "grad_norm": 3.610482692718506,
      "learning_rate": 4.4220228970484466e-05,
      "loss": 0.7243,
      "step": 760100
    },
    {
      "epoch": 6.936637710781809,
      "grad_norm": 3.54036283493042,
      "learning_rate": 4.4219468574348496e-05,
      "loss": 0.7282,
      "step": 760200
    },
    {
      "epoch": 6.937550186144974,
      "grad_norm": 3.2633981704711914,
      "learning_rate": 4.4218708178212526e-05,
      "loss": 0.7074,
      "step": 760300
    },
    {
      "epoch": 6.938462661508139,
      "grad_norm": 5.031848907470703,
      "learning_rate": 4.421794778207655e-05,
      "loss": 0.6814,
      "step": 760400
    },
    {
      "epoch": 6.939375136871305,
      "grad_norm": 3.7186851501464844,
      "learning_rate": 4.4217187385940586e-05,
      "loss": 0.691,
      "step": 760500
    },
    {
      "epoch": 6.94028761223447,
      "grad_norm": 2.110105037689209,
      "learning_rate": 4.421642698980461e-05,
      "loss": 0.7392,
      "step": 760600
    },
    {
      "epoch": 6.941200087597635,
      "grad_norm": 4.393390655517578,
      "learning_rate": 4.421566659366864e-05,
      "loss": 0.7137,
      "step": 760700
    },
    {
      "epoch": 6.9421125629608,
      "grad_norm": 4.045477390289307,
      "learning_rate": 4.421490619753267e-05,
      "loss": 0.7104,
      "step": 760800
    },
    {
      "epoch": 6.943025038323965,
      "grad_norm": 4.001308441162109,
      "learning_rate": 4.42141458013967e-05,
      "loss": 0.7118,
      "step": 760900
    },
    {
      "epoch": 6.94393751368713,
      "grad_norm": 3.7559447288513184,
      "learning_rate": 4.421338540526073e-05,
      "loss": 0.6697,
      "step": 761000
    },
    {
      "epoch": 6.9448499890502955,
      "grad_norm": 4.6031413078308105,
      "learning_rate": 4.421262500912476e-05,
      "loss": 0.713,
      "step": 761100
    },
    {
      "epoch": 6.945762464413461,
      "grad_norm": 3.3177239894866943,
      "learning_rate": 4.421186461298878e-05,
      "loss": 0.6877,
      "step": 761200
    },
    {
      "epoch": 6.946674939776626,
      "grad_norm": 4.318973541259766,
      "learning_rate": 4.421110421685281e-05,
      "loss": 0.6749,
      "step": 761300
    },
    {
      "epoch": 6.947587415139791,
      "grad_norm": 4.603702545166016,
      "learning_rate": 4.421034382071684e-05,
      "loss": 0.6986,
      "step": 761400
    },
    {
      "epoch": 6.948499890502957,
      "grad_norm": 3.91684889793396,
      "learning_rate": 4.4209583424580867e-05,
      "loss": 0.6908,
      "step": 761500
    },
    {
      "epoch": 6.949412365866122,
      "grad_norm": 3.7648966312408447,
      "learning_rate": 4.4208823028444903e-05,
      "loss": 0.6848,
      "step": 761600
    },
    {
      "epoch": 6.950324841229287,
      "grad_norm": 3.9700818061828613,
      "learning_rate": 4.420806263230893e-05,
      "loss": 0.7423,
      "step": 761700
    },
    {
      "epoch": 6.951237316592452,
      "grad_norm": 4.539159774780273,
      "learning_rate": 4.420730223617296e-05,
      "loss": 0.6875,
      "step": 761800
    },
    {
      "epoch": 6.952149791955617,
      "grad_norm": 4.463672637939453,
      "learning_rate": 4.420654184003699e-05,
      "loss": 0.665,
      "step": 761900
    },
    {
      "epoch": 6.953062267318782,
      "grad_norm": 3.9229891300201416,
      "learning_rate": 4.420578144390102e-05,
      "loss": 0.7272,
      "step": 762000
    },
    {
      "epoch": 6.953974742681948,
      "grad_norm": 4.347483158111572,
      "learning_rate": 4.420502104776505e-05,
      "loss": 0.6878,
      "step": 762100
    },
    {
      "epoch": 6.954887218045113,
      "grad_norm": 2.973703622817993,
      "learning_rate": 4.420426065162908e-05,
      "loss": 0.702,
      "step": 762200
    },
    {
      "epoch": 6.955799693408278,
      "grad_norm": 3.539077043533325,
      "learning_rate": 4.42035002554931e-05,
      "loss": 0.6786,
      "step": 762300
    },
    {
      "epoch": 6.9567121687714435,
      "grad_norm": 3.812999725341797,
      "learning_rate": 4.420273985935714e-05,
      "loss": 0.7249,
      "step": 762400
    },
    {
      "epoch": 6.957624644134608,
      "grad_norm": 3.742833375930786,
      "learning_rate": 4.420197946322116e-05,
      "loss": 0.7137,
      "step": 762500
    },
    {
      "epoch": 6.958537119497773,
      "grad_norm": 3.8551244735717773,
      "learning_rate": 4.420121906708519e-05,
      "loss": 0.7364,
      "step": 762600
    },
    {
      "epoch": 6.9594495948609385,
      "grad_norm": 4.318471908569336,
      "learning_rate": 4.420045867094922e-05,
      "loss": 0.7026,
      "step": 762700
    },
    {
      "epoch": 6.960362070224104,
      "grad_norm": 7.096879005432129,
      "learning_rate": 4.419969827481325e-05,
      "loss": 0.7363,
      "step": 762800
    },
    {
      "epoch": 6.961274545587269,
      "grad_norm": 5.680782318115234,
      "learning_rate": 4.4198937878677274e-05,
      "loss": 0.6985,
      "step": 762900
    },
    {
      "epoch": 6.962187020950434,
      "grad_norm": 4.726323127746582,
      "learning_rate": 4.419817748254131e-05,
      "loss": 0.6641,
      "step": 763000
    },
    {
      "epoch": 6.9630994963136,
      "grad_norm": 3.5827507972717285,
      "learning_rate": 4.4197417086405334e-05,
      "loss": 0.6809,
      "step": 763100
    },
    {
      "epoch": 6.964011971676765,
      "grad_norm": 3.7593541145324707,
      "learning_rate": 4.4196656690269364e-05,
      "loss": 0.7024,
      "step": 763200
    },
    {
      "epoch": 6.96492444703993,
      "grad_norm": 4.56695556640625,
      "learning_rate": 4.4195896294133394e-05,
      "loss": 0.6971,
      "step": 763300
    },
    {
      "epoch": 6.965836922403096,
      "grad_norm": 3.3397209644317627,
      "learning_rate": 4.4195135897997424e-05,
      "loss": 0.7076,
      "step": 763400
    },
    {
      "epoch": 6.96674939776626,
      "grad_norm": 3.995863437652588,
      "learning_rate": 4.4194375501861454e-05,
      "loss": 0.6621,
      "step": 763500
    },
    {
      "epoch": 6.967661873129425,
      "grad_norm": 4.006987571716309,
      "learning_rate": 4.4193615105725484e-05,
      "loss": 0.7119,
      "step": 763600
    },
    {
      "epoch": 6.968574348492591,
      "grad_norm": 4.722498416900635,
      "learning_rate": 4.419285470958951e-05,
      "loss": 0.6984,
      "step": 763700
    },
    {
      "epoch": 6.969486823855756,
      "grad_norm": 4.61163854598999,
      "learning_rate": 4.4192094313453545e-05,
      "loss": 0.7487,
      "step": 763800
    },
    {
      "epoch": 6.970399299218921,
      "grad_norm": 4.096593379974365,
      "learning_rate": 4.419133391731757e-05,
      "loss": 0.6897,
      "step": 763900
    },
    {
      "epoch": 6.9713117745820865,
      "grad_norm": 4.654801845550537,
      "learning_rate": 4.41905735211816e-05,
      "loss": 0.6532,
      "step": 764000
    },
    {
      "epoch": 6.972224249945252,
      "grad_norm": 3.6400182247161865,
      "learning_rate": 4.418981312504563e-05,
      "loss": 0.6644,
      "step": 764100
    },
    {
      "epoch": 6.973136725308416,
      "grad_norm": 4.058648109436035,
      "learning_rate": 4.418905272890965e-05,
      "loss": 0.6708,
      "step": 764200
    },
    {
      "epoch": 6.9740492006715815,
      "grad_norm": 4.034899711608887,
      "learning_rate": 4.418829233277368e-05,
      "loss": 0.6878,
      "step": 764300
    },
    {
      "epoch": 6.974961676034747,
      "grad_norm": 3.5743041038513184,
      "learning_rate": 4.418753193663771e-05,
      "loss": 0.7154,
      "step": 764400
    },
    {
      "epoch": 6.975874151397912,
      "grad_norm": 3.2817540168762207,
      "learning_rate": 4.418677154050174e-05,
      "loss": 0.7144,
      "step": 764500
    },
    {
      "epoch": 6.976786626761077,
      "grad_norm": 4.046992778778076,
      "learning_rate": 4.418601114436577e-05,
      "loss": 0.7254,
      "step": 764600
    },
    {
      "epoch": 6.977699102124243,
      "grad_norm": 4.105567932128906,
      "learning_rate": 4.41852507482298e-05,
      "loss": 0.7036,
      "step": 764700
    },
    {
      "epoch": 6.978611577487408,
      "grad_norm": 3.691520929336548,
      "learning_rate": 4.4184490352093825e-05,
      "loss": 0.655,
      "step": 764800
    },
    {
      "epoch": 6.979524052850573,
      "grad_norm": 3.6415297985076904,
      "learning_rate": 4.418372995595786e-05,
      "loss": 0.7065,
      "step": 764900
    },
    {
      "epoch": 6.980436528213739,
      "grad_norm": 2.810953140258789,
      "learning_rate": 4.4182969559821885e-05,
      "loss": 0.6813,
      "step": 765000
    },
    {
      "epoch": 6.981349003576904,
      "grad_norm": 3.784106731414795,
      "learning_rate": 4.4182209163685915e-05,
      "loss": 0.6774,
      "step": 765100
    },
    {
      "epoch": 6.982261478940068,
      "grad_norm": 3.663634777069092,
      "learning_rate": 4.4181448767549945e-05,
      "loss": 0.6986,
      "step": 765200
    },
    {
      "epoch": 6.983173954303234,
      "grad_norm": 4.353752613067627,
      "learning_rate": 4.4180688371413975e-05,
      "loss": 0.7016,
      "step": 765300
    },
    {
      "epoch": 6.984086429666399,
      "grad_norm": 5.173269271850586,
      "learning_rate": 4.4179927975278e-05,
      "loss": 0.6991,
      "step": 765400
    },
    {
      "epoch": 6.984998905029564,
      "grad_norm": 4.022024631500244,
      "learning_rate": 4.4179167579142035e-05,
      "loss": 0.716,
      "step": 765500
    },
    {
      "epoch": 6.9859113803927295,
      "grad_norm": 4.737987041473389,
      "learning_rate": 4.417840718300606e-05,
      "loss": 0.7459,
      "step": 765600
    },
    {
      "epoch": 6.986823855755895,
      "grad_norm": 4.492669582366943,
      "learning_rate": 4.417764678687009e-05,
      "loss": 0.6885,
      "step": 765700
    },
    {
      "epoch": 6.98773633111906,
      "grad_norm": 4.1784281730651855,
      "learning_rate": 4.417688639073412e-05,
      "loss": 0.7099,
      "step": 765800
    },
    {
      "epoch": 6.9886488064822245,
      "grad_norm": 3.259091854095459,
      "learning_rate": 4.417612599459815e-05,
      "loss": 0.6931,
      "step": 765900
    },
    {
      "epoch": 6.98956128184539,
      "grad_norm": 4.339446544647217,
      "learning_rate": 4.417536559846218e-05,
      "loss": 0.7201,
      "step": 766000
    },
    {
      "epoch": 6.990473757208555,
      "grad_norm": 4.2547502517700195,
      "learning_rate": 4.417460520232621e-05,
      "loss": 0.7022,
      "step": 766100
    },
    {
      "epoch": 6.99138623257172,
      "grad_norm": 4.454148769378662,
      "learning_rate": 4.417384480619023e-05,
      "loss": 0.7241,
      "step": 766200
    },
    {
      "epoch": 6.992298707934886,
      "grad_norm": 3.1653010845184326,
      "learning_rate": 4.417308441005427e-05,
      "loss": 0.6755,
      "step": 766300
    },
    {
      "epoch": 6.993211183298051,
      "grad_norm": 3.728367567062378,
      "learning_rate": 4.417232401391829e-05,
      "loss": 0.7025,
      "step": 766400
    },
    {
      "epoch": 6.994123658661216,
      "grad_norm": 4.274419784545898,
      "learning_rate": 4.417156361778232e-05,
      "loss": 0.7444,
      "step": 766500
    },
    {
      "epoch": 6.995036134024382,
      "grad_norm": 3.5104007720947266,
      "learning_rate": 4.417080322164635e-05,
      "loss": 0.6976,
      "step": 766600
    },
    {
      "epoch": 6.995948609387547,
      "grad_norm": 3.6631722450256348,
      "learning_rate": 4.417004282551038e-05,
      "loss": 0.7238,
      "step": 766700
    },
    {
      "epoch": 6.996861084750712,
      "grad_norm": 3.7651145458221436,
      "learning_rate": 4.4169282429374406e-05,
      "loss": 0.6943,
      "step": 766800
    },
    {
      "epoch": 6.997773560113877,
      "grad_norm": 3.650191068649292,
      "learning_rate": 4.4168522033238436e-05,
      "loss": 0.6837,
      "step": 766900
    },
    {
      "epoch": 6.998686035477042,
      "grad_norm": 3.2611844539642334,
      "learning_rate": 4.4167761637102466e-05,
      "loss": 0.698,
      "step": 767000
    },
    {
      "epoch": 6.999598510840207,
      "grad_norm": 4.520684242248535,
      "learning_rate": 4.4167001240966496e-05,
      "loss": 0.6872,
      "step": 767100
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.570025622844696,
      "eval_runtime": 25.4066,
      "eval_samples_per_second": 227.067,
      "eval_steps_per_second": 227.067,
      "step": 767144
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.5517760515213013,
      "eval_runtime": 493.924,
      "eval_samples_per_second": 221.88,
      "eval_steps_per_second": 221.88,
      "step": 767144
    },
    {
      "epoch": 7.0005109862033725,
      "grad_norm": 3.9517359733581543,
      "learning_rate": 4.4166240844830526e-05,
      "loss": 0.669,
      "step": 767200
    },
    {
      "epoch": 7.001423461566538,
      "grad_norm": 4.4877400398254395,
      "learning_rate": 4.416548044869455e-05,
      "loss": 0.7426,
      "step": 767300
    },
    {
      "epoch": 7.002335936929703,
      "grad_norm": 3.4234349727630615,
      "learning_rate": 4.4164720052558586e-05,
      "loss": 0.6763,
      "step": 767400
    },
    {
      "epoch": 7.003248412292868,
      "grad_norm": 4.420263290405273,
      "learning_rate": 4.416395965642261e-05,
      "loss": 0.7127,
      "step": 767500
    },
    {
      "epoch": 7.004160887656034,
      "grad_norm": 4.555130958557129,
      "learning_rate": 4.416319926028664e-05,
      "loss": 0.6981,
      "step": 767600
    },
    {
      "epoch": 7.005073363019198,
      "grad_norm": 4.342155456542969,
      "learning_rate": 4.416243886415067e-05,
      "loss": 0.6831,
      "step": 767700
    },
    {
      "epoch": 7.005985838382363,
      "grad_norm": 4.541335582733154,
      "learning_rate": 4.41616784680147e-05,
      "loss": 0.7185,
      "step": 767800
    },
    {
      "epoch": 7.006898313745529,
      "grad_norm": 4.1691131591796875,
      "learning_rate": 4.416091807187872e-05,
      "loss": 0.7393,
      "step": 767900
    },
    {
      "epoch": 7.007810789108694,
      "grad_norm": 4.419147491455078,
      "learning_rate": 4.416015767574276e-05,
      "loss": 0.7114,
      "step": 768000
    },
    {
      "epoch": 7.008723264471859,
      "grad_norm": 3.522830009460449,
      "learning_rate": 4.415939727960678e-05,
      "loss": 0.6779,
      "step": 768100
    },
    {
      "epoch": 7.009635739835025,
      "grad_norm": 4.483643054962158,
      "learning_rate": 4.415863688347081e-05,
      "loss": 0.6857,
      "step": 768200
    },
    {
      "epoch": 7.01054821519819,
      "grad_norm": 3.8431434631347656,
      "learning_rate": 4.4157876487334843e-05,
      "loss": 0.6938,
      "step": 768300
    },
    {
      "epoch": 7.011460690561355,
      "grad_norm": 4.57380485534668,
      "learning_rate": 4.4157116091198873e-05,
      "loss": 0.7083,
      "step": 768400
    },
    {
      "epoch": 7.01237316592452,
      "grad_norm": 4.095864295959473,
      "learning_rate": 4.4156355695062904e-05,
      "loss": 0.7083,
      "step": 768500
    },
    {
      "epoch": 7.013285641287685,
      "grad_norm": 2.077208995819092,
      "learning_rate": 4.4155595298926934e-05,
      "loss": 0.7257,
      "step": 768600
    },
    {
      "epoch": 7.01419811665085,
      "grad_norm": 3.946009397506714,
      "learning_rate": 4.415483490279096e-05,
      "loss": 0.7098,
      "step": 768700
    },
    {
      "epoch": 7.0151105920140155,
      "grad_norm": 3.056142568588257,
      "learning_rate": 4.4154074506654994e-05,
      "loss": 0.6577,
      "step": 768800
    },
    {
      "epoch": 7.016023067377181,
      "grad_norm": 4.911185264587402,
      "learning_rate": 4.415331411051902e-05,
      "loss": 0.7061,
      "step": 768900
    },
    {
      "epoch": 7.016935542740346,
      "grad_norm": 4.219922065734863,
      "learning_rate": 4.415255371438305e-05,
      "loss": 0.7036,
      "step": 769000
    },
    {
      "epoch": 7.017848018103511,
      "grad_norm": 3.5912539958953857,
      "learning_rate": 4.415179331824708e-05,
      "loss": 0.6445,
      "step": 769100
    },
    {
      "epoch": 7.018760493466677,
      "grad_norm": 3.1248652935028076,
      "learning_rate": 4.415103292211111e-05,
      "loss": 0.7079,
      "step": 769200
    },
    {
      "epoch": 7.019672968829841,
      "grad_norm": 4.9219183921813965,
      "learning_rate": 4.415027252597513e-05,
      "loss": 0.7188,
      "step": 769300
    },
    {
      "epoch": 7.0205854441930065,
      "grad_norm": 4.050947666168213,
      "learning_rate": 4.414951212983917e-05,
      "loss": 0.6881,
      "step": 769400
    },
    {
      "epoch": 7.021497919556172,
      "grad_norm": 4.457200527191162,
      "learning_rate": 4.414875173370319e-05,
      "loss": 0.6961,
      "step": 769500
    },
    {
      "epoch": 7.022410394919337,
      "grad_norm": 4.140766620635986,
      "learning_rate": 4.414799133756722e-05,
      "loss": 0.7104,
      "step": 769600
    },
    {
      "epoch": 7.023322870282502,
      "grad_norm": 4.855465412139893,
      "learning_rate": 4.414723094143125e-05,
      "loss": 0.7081,
      "step": 769700
    },
    {
      "epoch": 7.024235345645668,
      "grad_norm": 4.283823490142822,
      "learning_rate": 4.4146470545295274e-05,
      "loss": 0.7168,
      "step": 769800
    },
    {
      "epoch": 7.025147821008833,
      "grad_norm": 3.953240156173706,
      "learning_rate": 4.414571014915931e-05,
      "loss": 0.6573,
      "step": 769900
    },
    {
      "epoch": 7.026060296371998,
      "grad_norm": 4.991389274597168,
      "learning_rate": 4.4144949753023334e-05,
      "loss": 0.7495,
      "step": 770000
    },
    {
      "epoch": 7.0269727717351635,
      "grad_norm": 3.5842947959899902,
      "learning_rate": 4.4144189356887364e-05,
      "loss": 0.675,
      "step": 770100
    },
    {
      "epoch": 7.027885247098328,
      "grad_norm": 3.6074118614196777,
      "learning_rate": 4.4143428960751394e-05,
      "loss": 0.6615,
      "step": 770200
    },
    {
      "epoch": 7.028797722461493,
      "grad_norm": 4.386507034301758,
      "learning_rate": 4.4142668564615424e-05,
      "loss": 0.6639,
      "step": 770300
    },
    {
      "epoch": 7.029710197824659,
      "grad_norm": 4.081402778625488,
      "learning_rate": 4.414190816847945e-05,
      "loss": 0.6939,
      "step": 770400
    },
    {
      "epoch": 7.030622673187824,
      "grad_norm": 3.934051752090454,
      "learning_rate": 4.4141147772343485e-05,
      "loss": 0.6891,
      "step": 770500
    },
    {
      "epoch": 7.031535148550989,
      "grad_norm": 4.9994215965271,
      "learning_rate": 4.414038737620751e-05,
      "loss": 0.6729,
      "step": 770600
    },
    {
      "epoch": 7.0324476239141545,
      "grad_norm": 3.09846830368042,
      "learning_rate": 4.413962698007154e-05,
      "loss": 0.704,
      "step": 770700
    },
    {
      "epoch": 7.03336009927732,
      "grad_norm": 3.0120859146118164,
      "learning_rate": 4.413886658393557e-05,
      "loss": 0.6658,
      "step": 770800
    },
    {
      "epoch": 7.034272574640485,
      "grad_norm": 3.984382390975952,
      "learning_rate": 4.41381061877996e-05,
      "loss": 0.6991,
      "step": 770900
    },
    {
      "epoch": 7.0351850500036495,
      "grad_norm": 3.0680432319641113,
      "learning_rate": 4.413734579166363e-05,
      "loss": 0.6787,
      "step": 771000
    },
    {
      "epoch": 7.036097525366815,
      "grad_norm": 3.8654849529266357,
      "learning_rate": 4.413658539552766e-05,
      "loss": 0.6803,
      "step": 771100
    },
    {
      "epoch": 7.03701000072998,
      "grad_norm": 4.454318523406982,
      "learning_rate": 4.413582499939168e-05,
      "loss": 0.707,
      "step": 771200
    },
    {
      "epoch": 7.037922476093145,
      "grad_norm": 4.5684709548950195,
      "learning_rate": 4.413506460325572e-05,
      "loss": 0.7247,
      "step": 771300
    },
    {
      "epoch": 7.038834951456311,
      "grad_norm": 3.667092800140381,
      "learning_rate": 4.413430420711974e-05,
      "loss": 0.7042,
      "step": 771400
    },
    {
      "epoch": 7.039747426819476,
      "grad_norm": 3.718829870223999,
      "learning_rate": 4.413354381098377e-05,
      "loss": 0.6784,
      "step": 771500
    },
    {
      "epoch": 7.040659902182641,
      "grad_norm": 4.579007148742676,
      "learning_rate": 4.41327834148478e-05,
      "loss": 0.6914,
      "step": 771600
    },
    {
      "epoch": 7.041572377545807,
      "grad_norm": 4.352647304534912,
      "learning_rate": 4.413202301871183e-05,
      "loss": 0.6615,
      "step": 771700
    },
    {
      "epoch": 7.042484852908972,
      "grad_norm": 4.604331016540527,
      "learning_rate": 4.413126262257586e-05,
      "loss": 0.6812,
      "step": 771800
    },
    {
      "epoch": 7.043397328272136,
      "grad_norm": 3.5000100135803223,
      "learning_rate": 4.413050222643989e-05,
      "loss": 0.6865,
      "step": 771900
    },
    {
      "epoch": 7.044309803635302,
      "grad_norm": 2.1465187072753906,
      "learning_rate": 4.4129741830303915e-05,
      "loss": 0.7145,
      "step": 772000
    },
    {
      "epoch": 7.045222278998467,
      "grad_norm": 3.4353952407836914,
      "learning_rate": 4.4128981434167945e-05,
      "loss": 0.6998,
      "step": 772100
    },
    {
      "epoch": 7.046134754361632,
      "grad_norm": 4.592819690704346,
      "learning_rate": 4.4128221038031975e-05,
      "loss": 0.6586,
      "step": 772200
    },
    {
      "epoch": 7.0470472297247975,
      "grad_norm": 3.571037530899048,
      "learning_rate": 4.4127460641896005e-05,
      "loss": 0.6642,
      "step": 772300
    },
    {
      "epoch": 7.047959705087963,
      "grad_norm": 4.041794776916504,
      "learning_rate": 4.4126700245760036e-05,
      "loss": 0.6927,
      "step": 772400
    },
    {
      "epoch": 7.048872180451128,
      "grad_norm": 3.3081374168395996,
      "learning_rate": 4.4125939849624066e-05,
      "loss": 0.7348,
      "step": 772500
    },
    {
      "epoch": 7.049784655814293,
      "grad_norm": 4.181865215301514,
      "learning_rate": 4.412517945348809e-05,
      "loss": 0.7196,
      "step": 772600
    },
    {
      "epoch": 7.050697131177458,
      "grad_norm": 3.9900882244110107,
      "learning_rate": 4.412441905735212e-05,
      "loss": 0.7,
      "step": 772700
    },
    {
      "epoch": 7.051609606540623,
      "grad_norm": 4.091845512390137,
      "learning_rate": 4.412365866121615e-05,
      "loss": 0.7025,
      "step": 772800
    },
    {
      "epoch": 7.052522081903788,
      "grad_norm": 4.37125301361084,
      "learning_rate": 4.412289826508018e-05,
      "loss": 0.7032,
      "step": 772900
    },
    {
      "epoch": 7.053434557266954,
      "grad_norm": 4.168264389038086,
      "learning_rate": 4.412213786894421e-05,
      "loss": 0.677,
      "step": 773000
    },
    {
      "epoch": 7.054347032630119,
      "grad_norm": 4.3244099617004395,
      "learning_rate": 4.412137747280823e-05,
      "loss": 0.6685,
      "step": 773100
    },
    {
      "epoch": 7.055259507993284,
      "grad_norm": 4.175966739654541,
      "learning_rate": 4.412061707667227e-05,
      "loss": 0.6743,
      "step": 773200
    },
    {
      "epoch": 7.05617198335645,
      "grad_norm": 4.697596073150635,
      "learning_rate": 4.411985668053629e-05,
      "loss": 0.7219,
      "step": 773300
    },
    {
      "epoch": 7.057084458719615,
      "grad_norm": 4.63954496383667,
      "learning_rate": 4.411909628440032e-05,
      "loss": 0.6925,
      "step": 773400
    },
    {
      "epoch": 7.05799693408278,
      "grad_norm": 4.5432868003845215,
      "learning_rate": 4.411833588826435e-05,
      "loss": 0.7012,
      "step": 773500
    },
    {
      "epoch": 7.058909409445945,
      "grad_norm": 3.8770925998687744,
      "learning_rate": 4.411757549212838e-05,
      "loss": 0.7378,
      "step": 773600
    },
    {
      "epoch": 7.05982188480911,
      "grad_norm": 4.31780481338501,
      "learning_rate": 4.4116815095992406e-05,
      "loss": 0.7115,
      "step": 773700
    },
    {
      "epoch": 7.060734360172275,
      "grad_norm": 4.37352991104126,
      "learning_rate": 4.411605469985644e-05,
      "loss": 0.6711,
      "step": 773800
    },
    {
      "epoch": 7.0616468355354405,
      "grad_norm": 3.513216972351074,
      "learning_rate": 4.4115294303720466e-05,
      "loss": 0.6882,
      "step": 773900
    },
    {
      "epoch": 7.062559310898606,
      "grad_norm": 4.421019554138184,
      "learning_rate": 4.4114533907584496e-05,
      "loss": 0.7045,
      "step": 774000
    },
    {
      "epoch": 7.063471786261771,
      "grad_norm": 4.778985977172852,
      "learning_rate": 4.4113773511448526e-05,
      "loss": 0.707,
      "step": 774100
    },
    {
      "epoch": 7.064384261624936,
      "grad_norm": 3.177471160888672,
      "learning_rate": 4.4113013115312556e-05,
      "loss": 0.6838,
      "step": 774200
    },
    {
      "epoch": 7.065296736988102,
      "grad_norm": 2.9858851432800293,
      "learning_rate": 4.4112252719176586e-05,
      "loss": 0.6792,
      "step": 774300
    },
    {
      "epoch": 7.066209212351266,
      "grad_norm": 4.231459140777588,
      "learning_rate": 4.4111492323040617e-05,
      "loss": 0.7009,
      "step": 774400
    },
    {
      "epoch": 7.067121687714431,
      "grad_norm": 5.2311882972717285,
      "learning_rate": 4.411073192690464e-05,
      "loss": 0.6861,
      "step": 774500
    },
    {
      "epoch": 7.068034163077597,
      "grad_norm": 3.4155473709106445,
      "learning_rate": 4.410997153076868e-05,
      "loss": 0.7209,
      "step": 774600
    },
    {
      "epoch": 7.068946638440762,
      "grad_norm": 3.7196755409240723,
      "learning_rate": 4.41092111346327e-05,
      "loss": 0.7421,
      "step": 774700
    },
    {
      "epoch": 7.069859113803927,
      "grad_norm": 3.7938413619995117,
      "learning_rate": 4.410845073849673e-05,
      "loss": 0.7061,
      "step": 774800
    },
    {
      "epoch": 7.070771589167093,
      "grad_norm": 4.099930286407471,
      "learning_rate": 4.410769034236076e-05,
      "loss": 0.6889,
      "step": 774900
    },
    {
      "epoch": 7.071684064530258,
      "grad_norm": 3.3296778202056885,
      "learning_rate": 4.410692994622479e-05,
      "loss": 0.6761,
      "step": 775000
    },
    {
      "epoch": 7.072596539893423,
      "grad_norm": 3.1117072105407715,
      "learning_rate": 4.4106169550088813e-05,
      "loss": 0.7063,
      "step": 775100
    },
    {
      "epoch": 7.0735090152565885,
      "grad_norm": 4.352723598480225,
      "learning_rate": 4.410540915395285e-05,
      "loss": 0.6962,
      "step": 775200
    },
    {
      "epoch": 7.074421490619753,
      "grad_norm": 4.5133957862854,
      "learning_rate": 4.4104648757816874e-05,
      "loss": 0.6772,
      "step": 775300
    },
    {
      "epoch": 7.075333965982918,
      "grad_norm": 3.3202426433563232,
      "learning_rate": 4.4103888361680904e-05,
      "loss": 0.6732,
      "step": 775400
    },
    {
      "epoch": 7.0762464413460835,
      "grad_norm": 4.378335952758789,
      "learning_rate": 4.4103127965544934e-05,
      "loss": 0.6888,
      "step": 775500
    },
    {
      "epoch": 7.077158916709249,
      "grad_norm": 4.3316755294799805,
      "learning_rate": 4.410236756940896e-05,
      "loss": 0.7258,
      "step": 775600
    },
    {
      "epoch": 7.078071392072414,
      "grad_norm": 4.385164260864258,
      "learning_rate": 4.4101607173272994e-05,
      "loss": 0.7145,
      "step": 775700
    },
    {
      "epoch": 7.078983867435579,
      "grad_norm": 4.469864368438721,
      "learning_rate": 4.410084677713702e-05,
      "loss": 0.7129,
      "step": 775800
    },
    {
      "epoch": 7.079896342798745,
      "grad_norm": 3.3645570278167725,
      "learning_rate": 4.410008638100105e-05,
      "loss": 0.7171,
      "step": 775900
    },
    {
      "epoch": 7.08080881816191,
      "grad_norm": 4.704232215881348,
      "learning_rate": 4.409932598486508e-05,
      "loss": 0.72,
      "step": 776000
    },
    {
      "epoch": 7.081721293525074,
      "grad_norm": 4.124395370483398,
      "learning_rate": 4.409856558872911e-05,
      "loss": 0.7078,
      "step": 776100
    },
    {
      "epoch": 7.08263376888824,
      "grad_norm": 3.584989547729492,
      "learning_rate": 4.409780519259313e-05,
      "loss": 0.7071,
      "step": 776200
    },
    {
      "epoch": 7.083546244251405,
      "grad_norm": 4.7015700340271,
      "learning_rate": 4.409704479645717e-05,
      "loss": 0.7234,
      "step": 776300
    },
    {
      "epoch": 7.08445871961457,
      "grad_norm": 3.7001290321350098,
      "learning_rate": 4.409628440032119e-05,
      "loss": 0.6852,
      "step": 776400
    },
    {
      "epoch": 7.085371194977736,
      "grad_norm": 3.7896039485931396,
      "learning_rate": 4.409552400418522e-05,
      "loss": 0.7157,
      "step": 776500
    },
    {
      "epoch": 7.086283670340901,
      "grad_norm": 4.471704483032227,
      "learning_rate": 4.409476360804925e-05,
      "loss": 0.6956,
      "step": 776600
    },
    {
      "epoch": 7.087196145704066,
      "grad_norm": 3.1246337890625,
      "learning_rate": 4.409400321191328e-05,
      "loss": 0.7038,
      "step": 776700
    },
    {
      "epoch": 7.0881086210672315,
      "grad_norm": 4.0954790115356445,
      "learning_rate": 4.409324281577731e-05,
      "loss": 0.7069,
      "step": 776800
    },
    {
      "epoch": 7.089021096430397,
      "grad_norm": 3.0751028060913086,
      "learning_rate": 4.409248241964134e-05,
      "loss": 0.6995,
      "step": 776900
    },
    {
      "epoch": 7.089933571793561,
      "grad_norm": 4.7882490158081055,
      "learning_rate": 4.4091722023505364e-05,
      "loss": 0.6806,
      "step": 777000
    },
    {
      "epoch": 7.0908460471567265,
      "grad_norm": 4.0457916259765625,
      "learning_rate": 4.40909616273694e-05,
      "loss": 0.6963,
      "step": 777100
    },
    {
      "epoch": 7.091758522519892,
      "grad_norm": 4.120274543762207,
      "learning_rate": 4.4090201231233425e-05,
      "loss": 0.6954,
      "step": 777200
    },
    {
      "epoch": 7.092670997883057,
      "grad_norm": 3.835704803466797,
      "learning_rate": 4.4089440835097455e-05,
      "loss": 0.6703,
      "step": 777300
    },
    {
      "epoch": 7.093583473246222,
      "grad_norm": 3.64601993560791,
      "learning_rate": 4.4088680438961485e-05,
      "loss": 0.7167,
      "step": 777400
    },
    {
      "epoch": 7.094495948609388,
      "grad_norm": 3.7930238246917725,
      "learning_rate": 4.4087920042825515e-05,
      "loss": 0.7382,
      "step": 777500
    },
    {
      "epoch": 7.095408423972553,
      "grad_norm": 3.836552381515503,
      "learning_rate": 4.408715964668954e-05,
      "loss": 0.677,
      "step": 777600
    },
    {
      "epoch": 7.096320899335718,
      "grad_norm": 3.7270028591156006,
      "learning_rate": 4.4086399250553575e-05,
      "loss": 0.6716,
      "step": 777700
    },
    {
      "epoch": 7.097233374698883,
      "grad_norm": 3.857178211212158,
      "learning_rate": 4.40856388544176e-05,
      "loss": 0.7059,
      "step": 777800
    },
    {
      "epoch": 7.098145850062048,
      "grad_norm": 4.10733699798584,
      "learning_rate": 4.408487845828163e-05,
      "loss": 0.6954,
      "step": 777900
    },
    {
      "epoch": 7.099058325425213,
      "grad_norm": 3.9670028686523438,
      "learning_rate": 4.408411806214566e-05,
      "loss": 0.6852,
      "step": 778000
    },
    {
      "epoch": 7.099970800788379,
      "grad_norm": 4.340911388397217,
      "learning_rate": 4.408335766600969e-05,
      "loss": 0.6972,
      "step": 778100
    },
    {
      "epoch": 7.100883276151544,
      "grad_norm": 4.4387311935424805,
      "learning_rate": 4.408259726987372e-05,
      "loss": 0.716,
      "step": 778200
    },
    {
      "epoch": 7.101795751514709,
      "grad_norm": 4.1552886962890625,
      "learning_rate": 4.408183687373774e-05,
      "loss": 0.6842,
      "step": 778300
    },
    {
      "epoch": 7.1027082268778745,
      "grad_norm": 3.9086766242980957,
      "learning_rate": 4.408107647760177e-05,
      "loss": 0.6858,
      "step": 778400
    },
    {
      "epoch": 7.10362070224104,
      "grad_norm": 4.627938747406006,
      "learning_rate": 4.40803160814658e-05,
      "loss": 0.6986,
      "step": 778500
    },
    {
      "epoch": 7.104533177604205,
      "grad_norm": 4.518060684204102,
      "learning_rate": 4.407955568532983e-05,
      "loss": 0.6727,
      "step": 778600
    },
    {
      "epoch": 7.1054456529673695,
      "grad_norm": 3.028123378753662,
      "learning_rate": 4.4078795289193855e-05,
      "loss": 0.6727,
      "step": 778700
    },
    {
      "epoch": 7.106358128330535,
      "grad_norm": 4.486236572265625,
      "learning_rate": 4.407803489305789e-05,
      "loss": 0.6961,
      "step": 778800
    },
    {
      "epoch": 7.1072706036937,
      "grad_norm": 3.4053220748901367,
      "learning_rate": 4.4077274496921915e-05,
      "loss": 0.6776,
      "step": 778900
    },
    {
      "epoch": 7.108183079056865,
      "grad_norm": 4.665427207946777,
      "learning_rate": 4.4076514100785945e-05,
      "loss": 0.7053,
      "step": 779000
    },
    {
      "epoch": 7.109095554420031,
      "grad_norm": 3.8778109550476074,
      "learning_rate": 4.4075753704649975e-05,
      "loss": 0.6831,
      "step": 779100
    },
    {
      "epoch": 7.110008029783196,
      "grad_norm": 4.285707473754883,
      "learning_rate": 4.4074993308514006e-05,
      "loss": 0.7205,
      "step": 779200
    },
    {
      "epoch": 7.110920505146361,
      "grad_norm": 4.650745391845703,
      "learning_rate": 4.4074232912378036e-05,
      "loss": 0.7179,
      "step": 779300
    },
    {
      "epoch": 7.111832980509527,
      "grad_norm": 4.143398284912109,
      "learning_rate": 4.4073472516242066e-05,
      "loss": 0.7127,
      "step": 779400
    },
    {
      "epoch": 7.112745455872691,
      "grad_norm": 3.359959363937378,
      "learning_rate": 4.407271212010609e-05,
      "loss": 0.6804,
      "step": 779500
    },
    {
      "epoch": 7.113657931235856,
      "grad_norm": 4.413231372833252,
      "learning_rate": 4.4071951723970126e-05,
      "loss": 0.6621,
      "step": 779600
    },
    {
      "epoch": 7.114570406599022,
      "grad_norm": 4.2279767990112305,
      "learning_rate": 4.407119132783415e-05,
      "loss": 0.7031,
      "step": 779700
    },
    {
      "epoch": 7.115482881962187,
      "grad_norm": 4.666309833526611,
      "learning_rate": 4.407043093169818e-05,
      "loss": 0.7268,
      "step": 779800
    },
    {
      "epoch": 7.116395357325352,
      "grad_norm": 3.8327720165252686,
      "learning_rate": 4.406967053556221e-05,
      "loss": 0.7072,
      "step": 779900
    },
    {
      "epoch": 7.1173078326885175,
      "grad_norm": 4.043359279632568,
      "learning_rate": 4.406891013942624e-05,
      "loss": 0.7049,
      "step": 780000
    },
    {
      "epoch": 7.118220308051683,
      "grad_norm": 3.0163071155548096,
      "learning_rate": 4.406814974329026e-05,
      "loss": 0.682,
      "step": 780100
    },
    {
      "epoch": 7.119132783414848,
      "grad_norm": 4.300875186920166,
      "learning_rate": 4.40673893471543e-05,
      "loss": 0.6951,
      "step": 780200
    },
    {
      "epoch": 7.120045258778013,
      "grad_norm": 4.390305042266846,
      "learning_rate": 4.406662895101832e-05,
      "loss": 0.6994,
      "step": 780300
    },
    {
      "epoch": 7.120957734141178,
      "grad_norm": 3.4746928215026855,
      "learning_rate": 4.406586855488235e-05,
      "loss": 0.7012,
      "step": 780400
    },
    {
      "epoch": 7.121870209504343,
      "grad_norm": 3.878338575363159,
      "learning_rate": 4.406510815874638e-05,
      "loss": 0.718,
      "step": 780500
    },
    {
      "epoch": 7.122782684867508,
      "grad_norm": 3.185511589050293,
      "learning_rate": 4.406434776261041e-05,
      "loss": 0.7005,
      "step": 780600
    },
    {
      "epoch": 7.123695160230674,
      "grad_norm": 6.239773273468018,
      "learning_rate": 4.406358736647444e-05,
      "loss": 0.6971,
      "step": 780700
    },
    {
      "epoch": 7.124607635593839,
      "grad_norm": 3.3896846771240234,
      "learning_rate": 4.406282697033847e-05,
      "loss": 0.6574,
      "step": 780800
    },
    {
      "epoch": 7.125520110957004,
      "grad_norm": 4.404430866241455,
      "learning_rate": 4.4062066574202496e-05,
      "loss": 0.6962,
      "step": 780900
    },
    {
      "epoch": 7.12643258632017,
      "grad_norm": 3.457149028778076,
      "learning_rate": 4.406130617806653e-05,
      "loss": 0.7233,
      "step": 781000
    },
    {
      "epoch": 7.127345061683335,
      "grad_norm": 4.359443187713623,
      "learning_rate": 4.4060545781930556e-05,
      "loss": 0.6723,
      "step": 781100
    },
    {
      "epoch": 7.128257537046499,
      "grad_norm": 4.025453090667725,
      "learning_rate": 4.405978538579458e-05,
      "loss": 0.6993,
      "step": 781200
    },
    {
      "epoch": 7.129170012409665,
      "grad_norm": 3.9268343448638916,
      "learning_rate": 4.405902498965862e-05,
      "loss": 0.7144,
      "step": 781300
    },
    {
      "epoch": 7.13008248777283,
      "grad_norm": 3.9238245487213135,
      "learning_rate": 4.405826459352264e-05,
      "loss": 0.6896,
      "step": 781400
    },
    {
      "epoch": 7.130994963135995,
      "grad_norm": 3.597262144088745,
      "learning_rate": 4.405750419738667e-05,
      "loss": 0.6876,
      "step": 781500
    },
    {
      "epoch": 7.1319074384991605,
      "grad_norm": 2.877572536468506,
      "learning_rate": 4.40567438012507e-05,
      "loss": 0.7093,
      "step": 781600
    },
    {
      "epoch": 7.132819913862326,
      "grad_norm": 3.581268548965454,
      "learning_rate": 4.405598340511473e-05,
      "loss": 0.7095,
      "step": 781700
    },
    {
      "epoch": 7.133732389225491,
      "grad_norm": 4.0464887619018555,
      "learning_rate": 4.405522300897876e-05,
      "loss": 0.7348,
      "step": 781800
    },
    {
      "epoch": 7.134644864588656,
      "grad_norm": 3.8381459712982178,
      "learning_rate": 4.405446261284279e-05,
      "loss": 0.703,
      "step": 781900
    },
    {
      "epoch": 7.135557339951822,
      "grad_norm": 4.060784339904785,
      "learning_rate": 4.4053702216706814e-05,
      "loss": 0.7062,
      "step": 782000
    },
    {
      "epoch": 7.136469815314986,
      "grad_norm": 4.143705368041992,
      "learning_rate": 4.405294182057085e-05,
      "loss": 0.7156,
      "step": 782100
    },
    {
      "epoch": 7.137382290678151,
      "grad_norm": 4.465351104736328,
      "learning_rate": 4.4052181424434874e-05,
      "loss": 0.7192,
      "step": 782200
    },
    {
      "epoch": 7.138294766041317,
      "grad_norm": 3.36891508102417,
      "learning_rate": 4.4051421028298904e-05,
      "loss": 0.6972,
      "step": 782300
    },
    {
      "epoch": 7.139207241404482,
      "grad_norm": 4.356187343597412,
      "learning_rate": 4.4050660632162934e-05,
      "loss": 0.6777,
      "step": 782400
    },
    {
      "epoch": 7.140119716767647,
      "grad_norm": 4.02525520324707,
      "learning_rate": 4.4049900236026964e-05,
      "loss": 0.7034,
      "step": 782500
    },
    {
      "epoch": 7.141032192130813,
      "grad_norm": 3.415879726409912,
      "learning_rate": 4.404913983989099e-05,
      "loss": 0.7079,
      "step": 782600
    },
    {
      "epoch": 7.141944667493978,
      "grad_norm": 3.701392412185669,
      "learning_rate": 4.4048379443755024e-05,
      "loss": 0.6754,
      "step": 782700
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 4.267332553863525,
      "learning_rate": 4.404761904761905e-05,
      "loss": 0.6901,
      "step": 782800
    },
    {
      "epoch": 7.143769618220308,
      "grad_norm": 4.617811679840088,
      "learning_rate": 4.404685865148308e-05,
      "loss": 0.7222,
      "step": 782900
    },
    {
      "epoch": 7.144682093583473,
      "grad_norm": 3.6442606449127197,
      "learning_rate": 4.404609825534711e-05,
      "loss": 0.7399,
      "step": 783000
    },
    {
      "epoch": 7.145594568946638,
      "grad_norm": 4.219976425170898,
      "learning_rate": 4.404533785921114e-05,
      "loss": 0.7055,
      "step": 783100
    },
    {
      "epoch": 7.1465070443098035,
      "grad_norm": 3.261258125305176,
      "learning_rate": 4.404457746307517e-05,
      "loss": 0.6618,
      "step": 783200
    },
    {
      "epoch": 7.147419519672969,
      "grad_norm": 2.031641960144043,
      "learning_rate": 4.40438170669392e-05,
      "loss": 0.7041,
      "step": 783300
    },
    {
      "epoch": 7.148331995036134,
      "grad_norm": 3.5924694538116455,
      "learning_rate": 4.404305667080322e-05,
      "loss": 0.705,
      "step": 783400
    },
    {
      "epoch": 7.1492444703992994,
      "grad_norm": 3.892404556274414,
      "learning_rate": 4.404229627466726e-05,
      "loss": 0.6805,
      "step": 783500
    },
    {
      "epoch": 7.150156945762465,
      "grad_norm": 3.868241548538208,
      "learning_rate": 4.404153587853128e-05,
      "loss": 0.7079,
      "step": 783600
    },
    {
      "epoch": 7.15106942112563,
      "grad_norm": 6.169383525848389,
      "learning_rate": 4.404077548239531e-05,
      "loss": 0.7111,
      "step": 783700
    },
    {
      "epoch": 7.1519818964887945,
      "grad_norm": 3.9259891510009766,
      "learning_rate": 4.404001508625934e-05,
      "loss": 0.7365,
      "step": 783800
    },
    {
      "epoch": 7.15289437185196,
      "grad_norm": 3.673689126968384,
      "learning_rate": 4.4039254690123364e-05,
      "loss": 0.6853,
      "step": 783900
    },
    {
      "epoch": 7.153806847215125,
      "grad_norm": 3.887834072113037,
      "learning_rate": 4.4038494293987395e-05,
      "loss": 0.7054,
      "step": 784000
    },
    {
      "epoch": 7.15471932257829,
      "grad_norm": 4.456930160522461,
      "learning_rate": 4.4037733897851425e-05,
      "loss": 0.6987,
      "step": 784100
    },
    {
      "epoch": 7.155631797941456,
      "grad_norm": 3.2096781730651855,
      "learning_rate": 4.4036973501715455e-05,
      "loss": 0.6971,
      "step": 784200
    },
    {
      "epoch": 7.156544273304621,
      "grad_norm": 4.447254657745361,
      "learning_rate": 4.4036213105579485e-05,
      "loss": 0.6972,
      "step": 784300
    },
    {
      "epoch": 7.157456748667786,
      "grad_norm": 4.687038898468018,
      "learning_rate": 4.4035452709443515e-05,
      "loss": 0.6921,
      "step": 784400
    },
    {
      "epoch": 7.1583692240309515,
      "grad_norm": 3.6996545791625977,
      "learning_rate": 4.403469231330754e-05,
      "loss": 0.6816,
      "step": 784500
    },
    {
      "epoch": 7.159281699394116,
      "grad_norm": 4.551315784454346,
      "learning_rate": 4.4033931917171575e-05,
      "loss": 0.7238,
      "step": 784600
    },
    {
      "epoch": 7.160194174757281,
      "grad_norm": 5.23851203918457,
      "learning_rate": 4.40331715210356e-05,
      "loss": 0.7295,
      "step": 784700
    },
    {
      "epoch": 7.161106650120447,
      "grad_norm": 4.350287437438965,
      "learning_rate": 4.403241112489963e-05,
      "loss": 0.6615,
      "step": 784800
    },
    {
      "epoch": 7.162019125483612,
      "grad_norm": 4.022897243499756,
      "learning_rate": 4.403165072876366e-05,
      "loss": 0.7264,
      "step": 784900
    },
    {
      "epoch": 7.162931600846777,
      "grad_norm": 4.230446815490723,
      "learning_rate": 4.403089033262769e-05,
      "loss": 0.7107,
      "step": 785000
    },
    {
      "epoch": 7.1638440762099425,
      "grad_norm": 4.445788860321045,
      "learning_rate": 4.403012993649172e-05,
      "loss": 0.6682,
      "step": 785100
    },
    {
      "epoch": 7.164756551573108,
      "grad_norm": 4.296337604522705,
      "learning_rate": 4.402936954035575e-05,
      "loss": 0.7032,
      "step": 785200
    },
    {
      "epoch": 7.165669026936273,
      "grad_norm": 4.191080570220947,
      "learning_rate": 4.402860914421977e-05,
      "loss": 0.6987,
      "step": 785300
    },
    {
      "epoch": 7.166581502299438,
      "grad_norm": 3.7397360801696777,
      "learning_rate": 4.40278487480838e-05,
      "loss": 0.6832,
      "step": 785400
    },
    {
      "epoch": 7.167493977662603,
      "grad_norm": 2.68438720703125,
      "learning_rate": 4.402708835194783e-05,
      "loss": 0.7022,
      "step": 785500
    },
    {
      "epoch": 7.168406453025768,
      "grad_norm": 2.2503299713134766,
      "learning_rate": 4.402632795581186e-05,
      "loss": 0.6665,
      "step": 785600
    },
    {
      "epoch": 7.169318928388933,
      "grad_norm": 3.7389490604400635,
      "learning_rate": 4.402556755967589e-05,
      "loss": 0.7187,
      "step": 785700
    },
    {
      "epoch": 7.170231403752099,
      "grad_norm": 4.339995861053467,
      "learning_rate": 4.402480716353992e-05,
      "loss": 0.7127,
      "step": 785800
    },
    {
      "epoch": 7.171143879115264,
      "grad_norm": 4.4081950187683105,
      "learning_rate": 4.4024046767403946e-05,
      "loss": 0.6618,
      "step": 785900
    },
    {
      "epoch": 7.172056354478429,
      "grad_norm": 7.339504241943359,
      "learning_rate": 4.402328637126798e-05,
      "loss": 0.7154,
      "step": 786000
    },
    {
      "epoch": 7.172968829841595,
      "grad_norm": 3.4693443775177,
      "learning_rate": 4.4022525975132006e-05,
      "loss": 0.6939,
      "step": 786100
    },
    {
      "epoch": 7.17388130520476,
      "grad_norm": 3.6940183639526367,
      "learning_rate": 4.4021765578996036e-05,
      "loss": 0.6858,
      "step": 786200
    },
    {
      "epoch": 7.174793780567924,
      "grad_norm": 3.785064935684204,
      "learning_rate": 4.4021005182860066e-05,
      "loss": 0.6756,
      "step": 786300
    },
    {
      "epoch": 7.17570625593109,
      "grad_norm": 3.8336689472198486,
      "learning_rate": 4.4020244786724096e-05,
      "loss": 0.695,
      "step": 786400
    },
    {
      "epoch": 7.176618731294255,
      "grad_norm": 4.6137919425964355,
      "learning_rate": 4.4019484390588126e-05,
      "loss": 0.7173,
      "step": 786500
    },
    {
      "epoch": 7.17753120665742,
      "grad_norm": 4.432587623596191,
      "learning_rate": 4.4018723994452156e-05,
      "loss": 0.7224,
      "step": 786600
    },
    {
      "epoch": 7.1784436820205855,
      "grad_norm": 4.083642959594727,
      "learning_rate": 4.401796359831618e-05,
      "loss": 0.6698,
      "step": 786700
    },
    {
      "epoch": 7.179356157383751,
      "grad_norm": 3.9152376651763916,
      "learning_rate": 4.401720320218021e-05,
      "loss": 0.6939,
      "step": 786800
    },
    {
      "epoch": 7.180268632746916,
      "grad_norm": 3.68656063079834,
      "learning_rate": 4.401644280604424e-05,
      "loss": 0.701,
      "step": 786900
    },
    {
      "epoch": 7.181181108110081,
      "grad_norm": 4.399816036224365,
      "learning_rate": 4.401568240990826e-05,
      "loss": 0.7086,
      "step": 787000
    },
    {
      "epoch": 7.182093583473247,
      "grad_norm": 3.1940324306488037,
      "learning_rate": 4.40149220137723e-05,
      "loss": 0.7202,
      "step": 787100
    },
    {
      "epoch": 7.183006058836411,
      "grad_norm": 4.242788791656494,
      "learning_rate": 4.401416161763632e-05,
      "loss": 0.716,
      "step": 787200
    },
    {
      "epoch": 7.183918534199576,
      "grad_norm": 5.326297283172607,
      "learning_rate": 4.401340122150035e-05,
      "loss": 0.7165,
      "step": 787300
    },
    {
      "epoch": 7.184831009562742,
      "grad_norm": 4.142185688018799,
      "learning_rate": 4.401264082536438e-05,
      "loss": 0.6905,
      "step": 787400
    },
    {
      "epoch": 7.185743484925907,
      "grad_norm": 4.7417683601379395,
      "learning_rate": 4.401188042922841e-05,
      "loss": 0.6838,
      "step": 787500
    },
    {
      "epoch": 7.186655960289072,
      "grad_norm": 4.144077301025391,
      "learning_rate": 4.401112003309244e-05,
      "loss": 0.7086,
      "step": 787600
    },
    {
      "epoch": 7.187568435652238,
      "grad_norm": 3.742180585861206,
      "learning_rate": 4.401035963695647e-05,
      "loss": 0.6739,
      "step": 787700
    },
    {
      "epoch": 7.188480911015403,
      "grad_norm": 5.190042495727539,
      "learning_rate": 4.4009599240820496e-05,
      "loss": 0.719,
      "step": 787800
    },
    {
      "epoch": 7.189393386378568,
      "grad_norm": 4.519218444824219,
      "learning_rate": 4.400883884468453e-05,
      "loss": 0.7055,
      "step": 787900
    },
    {
      "epoch": 7.190305861741733,
      "grad_norm": 3.654409408569336,
      "learning_rate": 4.4008078448548557e-05,
      "loss": 0.6656,
      "step": 788000
    },
    {
      "epoch": 7.191218337104898,
      "grad_norm": 4.215460777282715,
      "learning_rate": 4.400731805241259e-05,
      "loss": 0.7151,
      "step": 788100
    },
    {
      "epoch": 7.192130812468063,
      "grad_norm": 3.5147616863250732,
      "learning_rate": 4.400655765627662e-05,
      "loss": 0.6869,
      "step": 788200
    },
    {
      "epoch": 7.1930432878312285,
      "grad_norm": 4.534344673156738,
      "learning_rate": 4.400579726014065e-05,
      "loss": 0.7105,
      "step": 788300
    },
    {
      "epoch": 7.193955763194394,
      "grad_norm": 5.463032245635986,
      "learning_rate": 4.400503686400467e-05,
      "loss": 0.7118,
      "step": 788400
    },
    {
      "epoch": 7.194868238557559,
      "grad_norm": 4.187001705169678,
      "learning_rate": 4.400427646786871e-05,
      "loss": 0.6874,
      "step": 788500
    },
    {
      "epoch": 7.195780713920724,
      "grad_norm": 3.3070220947265625,
      "learning_rate": 4.400351607173273e-05,
      "loss": 0.6502,
      "step": 788600
    },
    {
      "epoch": 7.19669318928389,
      "grad_norm": 3.6522090435028076,
      "learning_rate": 4.400275567559676e-05,
      "loss": 0.6981,
      "step": 788700
    },
    {
      "epoch": 7.197605664647055,
      "grad_norm": 6.017216205596924,
      "learning_rate": 4.400199527946079e-05,
      "loss": 0.7312,
      "step": 788800
    },
    {
      "epoch": 7.198518140010219,
      "grad_norm": 2.8778305053710938,
      "learning_rate": 4.400123488332482e-05,
      "loss": 0.6864,
      "step": 788900
    },
    {
      "epoch": 7.199430615373385,
      "grad_norm": 4.753449440002441,
      "learning_rate": 4.400047448718885e-05,
      "loss": 0.7065,
      "step": 789000
    },
    {
      "epoch": 7.20034309073655,
      "grad_norm": 3.5276591777801514,
      "learning_rate": 4.399971409105288e-05,
      "loss": 0.6886,
      "step": 789100
    },
    {
      "epoch": 7.201255566099715,
      "grad_norm": 4.075018405914307,
      "learning_rate": 4.3998953694916904e-05,
      "loss": 0.6907,
      "step": 789200
    },
    {
      "epoch": 7.202168041462881,
      "grad_norm": 4.363877296447754,
      "learning_rate": 4.399819329878094e-05,
      "loss": 0.7437,
      "step": 789300
    },
    {
      "epoch": 7.203080516826046,
      "grad_norm": 3.8531534671783447,
      "learning_rate": 4.3997432902644964e-05,
      "loss": 0.6903,
      "step": 789400
    },
    {
      "epoch": 7.203992992189211,
      "grad_norm": 3.6541733741760254,
      "learning_rate": 4.3996672506508994e-05,
      "loss": 0.7119,
      "step": 789500
    },
    {
      "epoch": 7.2049054675523765,
      "grad_norm": 4.211763381958008,
      "learning_rate": 4.3995912110373024e-05,
      "loss": 0.6431,
      "step": 789600
    },
    {
      "epoch": 7.205817942915541,
      "grad_norm": 2.786571979522705,
      "learning_rate": 4.399515171423705e-05,
      "loss": 0.6615,
      "step": 789700
    },
    {
      "epoch": 7.206730418278706,
      "grad_norm": 5.102870941162109,
      "learning_rate": 4.399439131810108e-05,
      "loss": 0.7191,
      "step": 789800
    },
    {
      "epoch": 7.2076428936418715,
      "grad_norm": 4.136616230010986,
      "learning_rate": 4.399363092196511e-05,
      "loss": 0.7105,
      "step": 789900
    },
    {
      "epoch": 7.208555369005037,
      "grad_norm": 4.2045722007751465,
      "learning_rate": 4.399287052582914e-05,
      "loss": 0.7469,
      "step": 790000
    },
    {
      "epoch": 7.209467844368202,
      "grad_norm": 3.7943735122680664,
      "learning_rate": 4.399211012969317e-05,
      "loss": 0.6883,
      "step": 790100
    },
    {
      "epoch": 7.210380319731367,
      "grad_norm": 4.271440029144287,
      "learning_rate": 4.39913497335572e-05,
      "loss": 0.7206,
      "step": 790200
    },
    {
      "epoch": 7.211292795094533,
      "grad_norm": 1.492713212966919,
      "learning_rate": 4.399058933742122e-05,
      "loss": 0.6676,
      "step": 790300
    },
    {
      "epoch": 7.212205270457698,
      "grad_norm": 3.6480681896209717,
      "learning_rate": 4.398982894128526e-05,
      "loss": 0.6601,
      "step": 790400
    },
    {
      "epoch": 7.213117745820863,
      "grad_norm": 4.574777603149414,
      "learning_rate": 4.398906854514928e-05,
      "loss": 0.669,
      "step": 790500
    },
    {
      "epoch": 7.214030221184028,
      "grad_norm": 3.6681923866271973,
      "learning_rate": 4.398830814901331e-05,
      "loss": 0.6792,
      "step": 790600
    },
    {
      "epoch": 7.214942696547193,
      "grad_norm": 3.4911065101623535,
      "learning_rate": 4.398754775287734e-05,
      "loss": 0.7046,
      "step": 790700
    },
    {
      "epoch": 7.215855171910358,
      "grad_norm": 3.8792388439178467,
      "learning_rate": 4.398678735674137e-05,
      "loss": 0.7021,
      "step": 790800
    },
    {
      "epoch": 7.216767647273524,
      "grad_norm": 8.869109153747559,
      "learning_rate": 4.3986026960605395e-05,
      "loss": 0.6781,
      "step": 790900
    },
    {
      "epoch": 7.217680122636689,
      "grad_norm": 4.009599208831787,
      "learning_rate": 4.398526656446943e-05,
      "loss": 0.6841,
      "step": 791000
    },
    {
      "epoch": 7.218592597999854,
      "grad_norm": 4.213083267211914,
      "learning_rate": 4.3984506168333455e-05,
      "loss": 0.6522,
      "step": 791100
    },
    {
      "epoch": 7.2195050733630195,
      "grad_norm": 4.133686065673828,
      "learning_rate": 4.3983745772197485e-05,
      "loss": 0.7118,
      "step": 791200
    },
    {
      "epoch": 7.220417548726185,
      "grad_norm": 4.441986560821533,
      "learning_rate": 4.3982985376061515e-05,
      "loss": 0.669,
      "step": 791300
    },
    {
      "epoch": 7.221330024089349,
      "grad_norm": 4.694604396820068,
      "learning_rate": 4.3982224979925545e-05,
      "loss": 0.7182,
      "step": 791400
    },
    {
      "epoch": 7.2222424994525145,
      "grad_norm": 3.6673660278320312,
      "learning_rate": 4.3981464583789575e-05,
      "loss": 0.661,
      "step": 791500
    },
    {
      "epoch": 7.22315497481568,
      "grad_norm": 4.090642929077148,
      "learning_rate": 4.3980704187653605e-05,
      "loss": 0.6741,
      "step": 791600
    },
    {
      "epoch": 7.224067450178845,
      "grad_norm": 4.163355827331543,
      "learning_rate": 4.397994379151763e-05,
      "loss": 0.7596,
      "step": 791700
    },
    {
      "epoch": 7.22497992554201,
      "grad_norm": 3.397186279296875,
      "learning_rate": 4.3979183395381665e-05,
      "loss": 0.6809,
      "step": 791800
    },
    {
      "epoch": 7.225892400905176,
      "grad_norm": 3.676542282104492,
      "learning_rate": 4.397842299924569e-05,
      "loss": 0.7446,
      "step": 791900
    },
    {
      "epoch": 7.226804876268341,
      "grad_norm": 4.11844539642334,
      "learning_rate": 4.397766260310972e-05,
      "loss": 0.6923,
      "step": 792000
    },
    {
      "epoch": 7.227717351631506,
      "grad_norm": 3.9368488788604736,
      "learning_rate": 4.397690220697375e-05,
      "loss": 0.6794,
      "step": 792100
    },
    {
      "epoch": 7.228629826994671,
      "grad_norm": 3.507559299468994,
      "learning_rate": 4.397614181083778e-05,
      "loss": 0.6746,
      "step": 792200
    },
    {
      "epoch": 7.229542302357836,
      "grad_norm": 4.128397464752197,
      "learning_rate": 4.39753814147018e-05,
      "loss": 0.6655,
      "step": 792300
    },
    {
      "epoch": 7.230454777721001,
      "grad_norm": 3.694988965988159,
      "learning_rate": 4.397462101856584e-05,
      "loss": 0.7058,
      "step": 792400
    },
    {
      "epoch": 7.231367253084167,
      "grad_norm": 4.406230926513672,
      "learning_rate": 4.397386062242986e-05,
      "loss": 0.7144,
      "step": 792500
    },
    {
      "epoch": 7.232279728447332,
      "grad_norm": 4.178304672241211,
      "learning_rate": 4.397310022629389e-05,
      "loss": 0.7157,
      "step": 792600
    },
    {
      "epoch": 7.233192203810497,
      "grad_norm": 4.194164276123047,
      "learning_rate": 4.397233983015792e-05,
      "loss": 0.709,
      "step": 792700
    },
    {
      "epoch": 7.2341046791736625,
      "grad_norm": 3.9983949661254883,
      "learning_rate": 4.3971579434021946e-05,
      "loss": 0.6876,
      "step": 792800
    },
    {
      "epoch": 7.235017154536828,
      "grad_norm": 3.5883896350860596,
      "learning_rate": 4.397081903788598e-05,
      "loss": 0.7151,
      "step": 792900
    },
    {
      "epoch": 7.235929629899993,
      "grad_norm": 3.2340521812438965,
      "learning_rate": 4.3970058641750006e-05,
      "loss": 0.7013,
      "step": 793000
    },
    {
      "epoch": 7.2368421052631575,
      "grad_norm": 3.7482547760009766,
      "learning_rate": 4.3969298245614036e-05,
      "loss": 0.6944,
      "step": 793100
    },
    {
      "epoch": 7.237754580626323,
      "grad_norm": 3.91239595413208,
      "learning_rate": 4.3968537849478066e-05,
      "loss": 0.6821,
      "step": 793200
    },
    {
      "epoch": 7.238667055989488,
      "grad_norm": 3.048926591873169,
      "learning_rate": 4.3967777453342096e-05,
      "loss": 0.7039,
      "step": 793300
    },
    {
      "epoch": 7.239579531352653,
      "grad_norm": 4.603440761566162,
      "learning_rate": 4.396701705720612e-05,
      "loss": 0.676,
      "step": 793400
    },
    {
      "epoch": 7.240492006715819,
      "grad_norm": 3.904399871826172,
      "learning_rate": 4.3966256661070156e-05,
      "loss": 0.6917,
      "step": 793500
    },
    {
      "epoch": 7.241404482078984,
      "grad_norm": 3.7981605529785156,
      "learning_rate": 4.396549626493418e-05,
      "loss": 0.6846,
      "step": 793600
    },
    {
      "epoch": 7.242316957442149,
      "grad_norm": 3.3273465633392334,
      "learning_rate": 4.396473586879821e-05,
      "loss": 0.6741,
      "step": 793700
    },
    {
      "epoch": 7.243229432805315,
      "grad_norm": 3.833369493484497,
      "learning_rate": 4.396397547266224e-05,
      "loss": 0.7205,
      "step": 793800
    },
    {
      "epoch": 7.244141908168479,
      "grad_norm": 4.009827613830566,
      "learning_rate": 4.396321507652627e-05,
      "loss": 0.7058,
      "step": 793900
    },
    {
      "epoch": 7.245054383531644,
      "grad_norm": 4.112338066101074,
      "learning_rate": 4.39624546803903e-05,
      "loss": 0.7376,
      "step": 794000
    },
    {
      "epoch": 7.24596685889481,
      "grad_norm": 4.449377536773682,
      "learning_rate": 4.396169428425433e-05,
      "loss": 0.6988,
      "step": 794100
    },
    {
      "epoch": 7.246879334257975,
      "grad_norm": 4.137576580047607,
      "learning_rate": 4.396093388811835e-05,
      "loss": 0.709,
      "step": 794200
    },
    {
      "epoch": 7.24779180962114,
      "grad_norm": 4.206430912017822,
      "learning_rate": 4.396017349198239e-05,
      "loss": 0.7412,
      "step": 794300
    },
    {
      "epoch": 7.2487042849843055,
      "grad_norm": 3.6203839778900146,
      "learning_rate": 4.395941309584641e-05,
      "loss": 0.7028,
      "step": 794400
    },
    {
      "epoch": 7.249616760347471,
      "grad_norm": 4.5650434494018555,
      "learning_rate": 4.395865269971044e-05,
      "loss": 0.7028,
      "step": 794500
    },
    {
      "epoch": 7.250529235710636,
      "grad_norm": 4.354389190673828,
      "learning_rate": 4.395789230357447e-05,
      "loss": 0.6855,
      "step": 794600
    },
    {
      "epoch": 7.251441711073801,
      "grad_norm": 4.212982654571533,
      "learning_rate": 4.39571319074385e-05,
      "loss": 0.6946,
      "step": 794700
    },
    {
      "epoch": 7.252354186436966,
      "grad_norm": 3.815408229827881,
      "learning_rate": 4.395637151130253e-05,
      "loss": 0.7594,
      "step": 794800
    },
    {
      "epoch": 7.253266661800131,
      "grad_norm": 3.2473275661468506,
      "learning_rate": 4.3955611115166563e-05,
      "loss": 0.7108,
      "step": 794900
    },
    {
      "epoch": 7.254179137163296,
      "grad_norm": 3.5937154293060303,
      "learning_rate": 4.395485071903059e-05,
      "loss": 0.7082,
      "step": 795000
    },
    {
      "epoch": 7.255091612526462,
      "grad_norm": 4.004167079925537,
      "learning_rate": 4.395409032289462e-05,
      "loss": 0.6847,
      "step": 795100
    },
    {
      "epoch": 7.256004087889627,
      "grad_norm": 4.165943145751953,
      "learning_rate": 4.395332992675865e-05,
      "loss": 0.722,
      "step": 795200
    },
    {
      "epoch": 7.256916563252792,
      "grad_norm": 4.215304851531982,
      "learning_rate": 4.395256953062267e-05,
      "loss": 0.7218,
      "step": 795300
    },
    {
      "epoch": 7.257829038615958,
      "grad_norm": 3.794142246246338,
      "learning_rate": 4.395180913448671e-05,
      "loss": 0.6995,
      "step": 795400
    },
    {
      "epoch": 7.258741513979123,
      "grad_norm": 4.432075500488281,
      "learning_rate": 4.395104873835073e-05,
      "loss": 0.6883,
      "step": 795500
    },
    {
      "epoch": 7.259653989342288,
      "grad_norm": 3.0985536575317383,
      "learning_rate": 4.395028834221476e-05,
      "loss": 0.68,
      "step": 795600
    },
    {
      "epoch": 7.260566464705453,
      "grad_norm": 4.100605487823486,
      "learning_rate": 4.394952794607879e-05,
      "loss": 0.6959,
      "step": 795700
    },
    {
      "epoch": 7.261478940068618,
      "grad_norm": 3.6588876247406006,
      "learning_rate": 4.394876754994282e-05,
      "loss": 0.6945,
      "step": 795800
    },
    {
      "epoch": 7.262391415431783,
      "grad_norm": 2.914518117904663,
      "learning_rate": 4.3948007153806844e-05,
      "loss": 0.6931,
      "step": 795900
    },
    {
      "epoch": 7.2633038907949485,
      "grad_norm": 4.087057113647461,
      "learning_rate": 4.394724675767088e-05,
      "loss": 0.7126,
      "step": 796000
    },
    {
      "epoch": 7.264216366158114,
      "grad_norm": 4.350010395050049,
      "learning_rate": 4.3946486361534904e-05,
      "loss": 0.7157,
      "step": 796100
    },
    {
      "epoch": 7.265128841521279,
      "grad_norm": 5.0188374519348145,
      "learning_rate": 4.3945725965398934e-05,
      "loss": 0.6886,
      "step": 796200
    },
    {
      "epoch": 7.266041316884444,
      "grad_norm": 5.20460319519043,
      "learning_rate": 4.3944965569262964e-05,
      "loss": 0.712,
      "step": 796300
    },
    {
      "epoch": 7.266953792247609,
      "grad_norm": 4.267230033874512,
      "learning_rate": 4.3944205173126994e-05,
      "loss": 0.6644,
      "step": 796400
    },
    {
      "epoch": 7.267866267610774,
      "grad_norm": 3.1255345344543457,
      "learning_rate": 4.3943444776991024e-05,
      "loss": 0.6819,
      "step": 796500
    },
    {
      "epoch": 7.268778742973939,
      "grad_norm": 3.3273065090179443,
      "learning_rate": 4.3942684380855054e-05,
      "loss": 0.7054,
      "step": 796600
    },
    {
      "epoch": 7.269691218337105,
      "grad_norm": 3.79487943649292,
      "learning_rate": 4.394192398471908e-05,
      "loss": 0.7194,
      "step": 796700
    },
    {
      "epoch": 7.27060369370027,
      "grad_norm": 4.1118998527526855,
      "learning_rate": 4.3941163588583114e-05,
      "loss": 0.6704,
      "step": 796800
    },
    {
      "epoch": 7.271516169063435,
      "grad_norm": 4.353735446929932,
      "learning_rate": 4.394040319244714e-05,
      "loss": 0.7257,
      "step": 796900
    },
    {
      "epoch": 7.272428644426601,
      "grad_norm": 3.8319826126098633,
      "learning_rate": 4.393964279631117e-05,
      "loss": 0.7163,
      "step": 797000
    },
    {
      "epoch": 7.273341119789766,
      "grad_norm": 3.902933359146118,
      "learning_rate": 4.39388824001752e-05,
      "loss": 0.705,
      "step": 797100
    },
    {
      "epoch": 7.274253595152931,
      "grad_norm": 3.8039469718933105,
      "learning_rate": 4.393812200403923e-05,
      "loss": 0.7219,
      "step": 797200
    },
    {
      "epoch": 7.275166070516096,
      "grad_norm": 4.2596516609191895,
      "learning_rate": 4.393736160790326e-05,
      "loss": 0.66,
      "step": 797300
    },
    {
      "epoch": 7.276078545879261,
      "grad_norm": 3.9035942554473877,
      "learning_rate": 4.393660121176729e-05,
      "loss": 0.7155,
      "step": 797400
    },
    {
      "epoch": 7.276991021242426,
      "grad_norm": 4.07306432723999,
      "learning_rate": 4.393584081563131e-05,
      "loss": 0.6783,
      "step": 797500
    },
    {
      "epoch": 7.2779034966055915,
      "grad_norm": 4.2867021560668945,
      "learning_rate": 4.393508041949534e-05,
      "loss": 0.7064,
      "step": 797600
    },
    {
      "epoch": 7.278815971968757,
      "grad_norm": 4.100190162658691,
      "learning_rate": 4.393432002335937e-05,
      "loss": 0.6913,
      "step": 797700
    },
    {
      "epoch": 7.279728447331922,
      "grad_norm": 3.724581718444824,
      "learning_rate": 4.39335596272234e-05,
      "loss": 0.7119,
      "step": 797800
    },
    {
      "epoch": 7.2806409226950874,
      "grad_norm": 3.20990252494812,
      "learning_rate": 4.393279923108743e-05,
      "loss": 0.6675,
      "step": 797900
    },
    {
      "epoch": 7.281553398058253,
      "grad_norm": 3.786679267883301,
      "learning_rate": 4.393203883495146e-05,
      "loss": 0.7206,
      "step": 798000
    },
    {
      "epoch": 7.282465873421417,
      "grad_norm": 4.249395370483398,
      "learning_rate": 4.3931278438815485e-05,
      "loss": 0.667,
      "step": 798100
    },
    {
      "epoch": 7.2833783487845825,
      "grad_norm": 3.4726221561431885,
      "learning_rate": 4.3930518042679515e-05,
      "loss": 0.6772,
      "step": 798200
    },
    {
      "epoch": 7.284290824147748,
      "grad_norm": 3.352907657623291,
      "learning_rate": 4.3929757646543545e-05,
      "loss": 0.6967,
      "step": 798300
    },
    {
      "epoch": 7.285203299510913,
      "grad_norm": 4.048654079437256,
      "learning_rate": 4.3928997250407575e-05,
      "loss": 0.6902,
      "step": 798400
    },
    {
      "epoch": 7.286115774874078,
      "grad_norm": 3.3658323287963867,
      "learning_rate": 4.3928236854271605e-05,
      "loss": 0.7096,
      "step": 798500
    },
    {
      "epoch": 7.287028250237244,
      "grad_norm": 4.1788411140441895,
      "learning_rate": 4.392747645813563e-05,
      "loss": 0.6839,
      "step": 798600
    },
    {
      "epoch": 7.287940725600409,
      "grad_norm": 4.447180271148682,
      "learning_rate": 4.3926716061999665e-05,
      "loss": 0.7027,
      "step": 798700
    },
    {
      "epoch": 7.288853200963574,
      "grad_norm": 4.155704021453857,
      "learning_rate": 4.392595566586369e-05,
      "loss": 0.6561,
      "step": 798800
    },
    {
      "epoch": 7.2897656763267396,
      "grad_norm": 4.3874664306640625,
      "learning_rate": 4.392519526972772e-05,
      "loss": 0.7072,
      "step": 798900
    },
    {
      "epoch": 7.290678151689904,
      "grad_norm": 4.505002975463867,
      "learning_rate": 4.392443487359175e-05,
      "loss": 0.7007,
      "step": 799000
    },
    {
      "epoch": 7.291590627053069,
      "grad_norm": 4.366691589355469,
      "learning_rate": 4.392367447745578e-05,
      "loss": 0.7181,
      "step": 799100
    },
    {
      "epoch": 7.292503102416235,
      "grad_norm": 4.0178446769714355,
      "learning_rate": 4.39229140813198e-05,
      "loss": 0.7058,
      "step": 799200
    },
    {
      "epoch": 7.2934155777794,
      "grad_norm": 4.643970966339111,
      "learning_rate": 4.392215368518384e-05,
      "loss": 0.66,
      "step": 799300
    },
    {
      "epoch": 7.294328053142565,
      "grad_norm": 2.8731555938720703,
      "learning_rate": 4.392139328904786e-05,
      "loss": 0.6806,
      "step": 799400
    },
    {
      "epoch": 7.2952405285057305,
      "grad_norm": 4.570672035217285,
      "learning_rate": 4.392063289291189e-05,
      "loss": 0.6958,
      "step": 799500
    },
    {
      "epoch": 7.296153003868896,
      "grad_norm": 4.078376770019531,
      "learning_rate": 4.391987249677592e-05,
      "loss": 0.6958,
      "step": 799600
    },
    {
      "epoch": 7.297065479232061,
      "grad_norm": 3.8705122470855713,
      "learning_rate": 4.391911210063995e-05,
      "loss": 0.7076,
      "step": 799700
    },
    {
      "epoch": 7.2979779545952255,
      "grad_norm": 3.4344398975372314,
      "learning_rate": 4.391835170450398e-05,
      "loss": 0.7121,
      "step": 799800
    },
    {
      "epoch": 7.298890429958391,
      "grad_norm": 3.7644190788269043,
      "learning_rate": 4.391759130836801e-05,
      "loss": 0.6836,
      "step": 799900
    },
    {
      "epoch": 7.299802905321556,
      "grad_norm": 4.407524108886719,
      "learning_rate": 4.3916830912232036e-05,
      "loss": 0.714,
      "step": 800000
    },
    {
      "epoch": 7.300715380684721,
      "grad_norm": 3.4343960285186768,
      "learning_rate": 4.391607051609607e-05,
      "loss": 0.6845,
      "step": 800100
    },
    {
      "epoch": 7.301627856047887,
      "grad_norm": 3.75887393951416,
      "learning_rate": 4.3915310119960096e-05,
      "loss": 0.6886,
      "step": 800200
    },
    {
      "epoch": 7.302540331411052,
      "grad_norm": 3.588996648788452,
      "learning_rate": 4.3914549723824126e-05,
      "loss": 0.7448,
      "step": 800300
    },
    {
      "epoch": 7.303452806774217,
      "grad_norm": 3.3995625972747803,
      "learning_rate": 4.3913789327688156e-05,
      "loss": 0.6842,
      "step": 800400
    },
    {
      "epoch": 7.304365282137383,
      "grad_norm": 4.327515602111816,
      "learning_rate": 4.3913028931552186e-05,
      "loss": 0.6754,
      "step": 800500
    },
    {
      "epoch": 7.305277757500548,
      "grad_norm": 3.465836763381958,
      "learning_rate": 4.391226853541621e-05,
      "loss": 0.6697,
      "step": 800600
    },
    {
      "epoch": 7.306190232863712,
      "grad_norm": 4.164401054382324,
      "learning_rate": 4.3911508139280246e-05,
      "loss": 0.7382,
      "step": 800700
    },
    {
      "epoch": 7.307102708226878,
      "grad_norm": 3.6707468032836914,
      "learning_rate": 4.391074774314427e-05,
      "loss": 0.6861,
      "step": 800800
    },
    {
      "epoch": 7.308015183590043,
      "grad_norm": 3.8111624717712402,
      "learning_rate": 4.39099873470083e-05,
      "loss": 0.7191,
      "step": 800900
    },
    {
      "epoch": 7.308927658953208,
      "grad_norm": 4.394351005554199,
      "learning_rate": 4.390922695087233e-05,
      "loss": 0.7095,
      "step": 801000
    },
    {
      "epoch": 7.3098401343163735,
      "grad_norm": 4.501474857330322,
      "learning_rate": 4.390846655473635e-05,
      "loss": 0.7043,
      "step": 801100
    },
    {
      "epoch": 7.310752609679539,
      "grad_norm": 2.770991802215576,
      "learning_rate": 4.390770615860039e-05,
      "loss": 0.7044,
      "step": 801200
    },
    {
      "epoch": 7.311665085042704,
      "grad_norm": 4.198536396026611,
      "learning_rate": 4.390694576246441e-05,
      "loss": 0.7039,
      "step": 801300
    },
    {
      "epoch": 7.312577560405869,
      "grad_norm": 3.564713478088379,
      "learning_rate": 4.390618536632844e-05,
      "loss": 0.6638,
      "step": 801400
    },
    {
      "epoch": 7.313490035769034,
      "grad_norm": 4.661861419677734,
      "learning_rate": 4.390542497019247e-05,
      "loss": 0.6749,
      "step": 801500
    },
    {
      "epoch": 7.314402511132199,
      "grad_norm": 4.337643146514893,
      "learning_rate": 4.3904664574056503e-05,
      "loss": 0.7184,
      "step": 801600
    },
    {
      "epoch": 7.315314986495364,
      "grad_norm": 4.438243389129639,
      "learning_rate": 4.390390417792053e-05,
      "loss": 0.6827,
      "step": 801700
    },
    {
      "epoch": 7.31622746185853,
      "grad_norm": 4.981945037841797,
      "learning_rate": 4.3903143781784564e-05,
      "loss": 0.7214,
      "step": 801800
    },
    {
      "epoch": 7.317139937221695,
      "grad_norm": 3.5267512798309326,
      "learning_rate": 4.390238338564859e-05,
      "loss": 0.6523,
      "step": 801900
    },
    {
      "epoch": 7.31805241258486,
      "grad_norm": 4.563197612762451,
      "learning_rate": 4.390162298951262e-05,
      "loss": 0.6862,
      "step": 802000
    },
    {
      "epoch": 7.318964887948026,
      "grad_norm": 3.7198734283447266,
      "learning_rate": 4.390086259337665e-05,
      "loss": 0.7337,
      "step": 802100
    },
    {
      "epoch": 7.319877363311191,
      "grad_norm": 4.870389461517334,
      "learning_rate": 4.390010219724068e-05,
      "loss": 0.737,
      "step": 802200
    },
    {
      "epoch": 7.320789838674356,
      "grad_norm": 4.11421012878418,
      "learning_rate": 4.389934180110471e-05,
      "loss": 0.7245,
      "step": 802300
    },
    {
      "epoch": 7.321702314037521,
      "grad_norm": 3.2064483165740967,
      "learning_rate": 4.389858140496874e-05,
      "loss": 0.6708,
      "step": 802400
    },
    {
      "epoch": 7.322614789400686,
      "grad_norm": 4.453713893890381,
      "learning_rate": 4.389782100883276e-05,
      "loss": 0.6956,
      "step": 802500
    },
    {
      "epoch": 7.323527264763851,
      "grad_norm": 4.280559539794922,
      "learning_rate": 4.38970606126968e-05,
      "loss": 0.6733,
      "step": 802600
    },
    {
      "epoch": 7.3244397401270165,
      "grad_norm": 4.467256546020508,
      "learning_rate": 4.389630021656082e-05,
      "loss": 0.7536,
      "step": 802700
    },
    {
      "epoch": 7.325352215490182,
      "grad_norm": 3.936249256134033,
      "learning_rate": 4.389553982042485e-05,
      "loss": 0.6628,
      "step": 802800
    },
    {
      "epoch": 7.326264690853347,
      "grad_norm": 3.4597887992858887,
      "learning_rate": 4.389477942428888e-05,
      "loss": 0.6928,
      "step": 802900
    },
    {
      "epoch": 7.327177166216512,
      "grad_norm": 3.911545991897583,
      "learning_rate": 4.389401902815291e-05,
      "loss": 0.6922,
      "step": 803000
    },
    {
      "epoch": 7.328089641579678,
      "grad_norm": 4.193163871765137,
      "learning_rate": 4.3893258632016934e-05,
      "loss": 0.7453,
      "step": 803100
    },
    {
      "epoch": 7.329002116942842,
      "grad_norm": 3.3723180294036865,
      "learning_rate": 4.389249823588097e-05,
      "loss": 0.6999,
      "step": 803200
    },
    {
      "epoch": 7.329914592306007,
      "grad_norm": 4.490355014801025,
      "learning_rate": 4.3891737839744994e-05,
      "loss": 0.6577,
      "step": 803300
    },
    {
      "epoch": 7.330827067669173,
      "grad_norm": 3.9124362468719482,
      "learning_rate": 4.3890977443609024e-05,
      "loss": 0.6521,
      "step": 803400
    },
    {
      "epoch": 7.331739543032338,
      "grad_norm": 4.717910289764404,
      "learning_rate": 4.3890217047473054e-05,
      "loss": 0.6996,
      "step": 803500
    },
    {
      "epoch": 7.332652018395503,
      "grad_norm": 4.611574172973633,
      "learning_rate": 4.3889456651337084e-05,
      "loss": 0.6883,
      "step": 803600
    },
    {
      "epoch": 7.333564493758669,
      "grad_norm": 4.375025272369385,
      "learning_rate": 4.3888696255201115e-05,
      "loss": 0.7085,
      "step": 803700
    },
    {
      "epoch": 7.334476969121834,
      "grad_norm": 4.705760478973389,
      "learning_rate": 4.388793585906514e-05,
      "loss": 0.7514,
      "step": 803800
    },
    {
      "epoch": 7.335389444484999,
      "grad_norm": 4.789991855621338,
      "learning_rate": 4.388717546292917e-05,
      "loss": 0.6858,
      "step": 803900
    },
    {
      "epoch": 7.3363019198481645,
      "grad_norm": 3.910628318786621,
      "learning_rate": 4.38864150667932e-05,
      "loss": 0.6928,
      "step": 804000
    },
    {
      "epoch": 7.337214395211329,
      "grad_norm": 3.5077285766601562,
      "learning_rate": 4.388565467065723e-05,
      "loss": 0.7196,
      "step": 804100
    },
    {
      "epoch": 7.338126870574494,
      "grad_norm": 4.0957794189453125,
      "learning_rate": 4.388489427452125e-05,
      "loss": 0.7336,
      "step": 804200
    },
    {
      "epoch": 7.3390393459376595,
      "grad_norm": 4.425766944885254,
      "learning_rate": 4.388413387838529e-05,
      "loss": 0.6859,
      "step": 804300
    },
    {
      "epoch": 7.339951821300825,
      "grad_norm": 3.155609607696533,
      "learning_rate": 4.388337348224931e-05,
      "loss": 0.7197,
      "step": 804400
    },
    {
      "epoch": 7.34086429666399,
      "grad_norm": 3.9991185665130615,
      "learning_rate": 4.388261308611334e-05,
      "loss": 0.7182,
      "step": 804500
    },
    {
      "epoch": 7.341776772027155,
      "grad_norm": 3.770934820175171,
      "learning_rate": 4.388185268997737e-05,
      "loss": 0.7195,
      "step": 804600
    },
    {
      "epoch": 7.342689247390321,
      "grad_norm": 2.8729400634765625,
      "learning_rate": 4.38810922938414e-05,
      "loss": 0.6568,
      "step": 804700
    },
    {
      "epoch": 7.343601722753486,
      "grad_norm": 4.561777591705322,
      "learning_rate": 4.388033189770543e-05,
      "loss": 0.697,
      "step": 804800
    },
    {
      "epoch": 7.34451419811665,
      "grad_norm": 4.247983455657959,
      "learning_rate": 4.387957150156946e-05,
      "loss": 0.7057,
      "step": 804900
    },
    {
      "epoch": 7.345426673479816,
      "grad_norm": 3.5122711658477783,
      "learning_rate": 4.3878811105433485e-05,
      "loss": 0.6712,
      "step": 805000
    },
    {
      "epoch": 7.346339148842981,
      "grad_norm": 3.7248153686523438,
      "learning_rate": 4.387805070929752e-05,
      "loss": 0.7168,
      "step": 805100
    },
    {
      "epoch": 7.347251624206146,
      "grad_norm": 4.209258079528809,
      "learning_rate": 4.3877290313161545e-05,
      "loss": 0.6895,
      "step": 805200
    },
    {
      "epoch": 7.348164099569312,
      "grad_norm": 4.267983436584473,
      "learning_rate": 4.3876529917025575e-05,
      "loss": 0.7626,
      "step": 805300
    },
    {
      "epoch": 7.349076574932477,
      "grad_norm": 4.060825824737549,
      "learning_rate": 4.3875769520889605e-05,
      "loss": 0.7022,
      "step": 805400
    },
    {
      "epoch": 7.349989050295642,
      "grad_norm": 4.244182109832764,
      "learning_rate": 4.3875009124753635e-05,
      "loss": 0.7088,
      "step": 805500
    },
    {
      "epoch": 7.3509015256588075,
      "grad_norm": 3.959535598754883,
      "learning_rate": 4.387424872861766e-05,
      "loss": 0.6776,
      "step": 805600
    },
    {
      "epoch": 7.351814001021973,
      "grad_norm": 4.514089584350586,
      "learning_rate": 4.3873488332481696e-05,
      "loss": 0.6942,
      "step": 805700
    },
    {
      "epoch": 7.352726476385137,
      "grad_norm": 3.4181108474731445,
      "learning_rate": 4.387272793634572e-05,
      "loss": 0.7224,
      "step": 805800
    },
    {
      "epoch": 7.3536389517483025,
      "grad_norm": 4.672802925109863,
      "learning_rate": 4.387196754020975e-05,
      "loss": 0.6538,
      "step": 805900
    },
    {
      "epoch": 7.354551427111468,
      "grad_norm": 4.857397556304932,
      "learning_rate": 4.387120714407378e-05,
      "loss": 0.7359,
      "step": 806000
    },
    {
      "epoch": 7.355463902474633,
      "grad_norm": 3.8194074630737305,
      "learning_rate": 4.387044674793781e-05,
      "loss": 0.6701,
      "step": 806100
    },
    {
      "epoch": 7.356376377837798,
      "grad_norm": 5.153172492980957,
      "learning_rate": 4.386968635180184e-05,
      "loss": 0.6889,
      "step": 806200
    },
    {
      "epoch": 7.357288853200964,
      "grad_norm": 3.8318819999694824,
      "learning_rate": 4.386892595566587e-05,
      "loss": 0.7283,
      "step": 806300
    },
    {
      "epoch": 7.358201328564129,
      "grad_norm": 4.073362350463867,
      "learning_rate": 4.386816555952989e-05,
      "loss": 0.6605,
      "step": 806400
    },
    {
      "epoch": 7.359113803927294,
      "grad_norm": 4.003269672393799,
      "learning_rate": 4.386740516339393e-05,
      "loss": 0.6849,
      "step": 806500
    },
    {
      "epoch": 7.360026279290459,
      "grad_norm": 3.6159043312072754,
      "learning_rate": 4.386664476725795e-05,
      "loss": 0.6974,
      "step": 806600
    },
    {
      "epoch": 7.360938754653624,
      "grad_norm": 4.13291072845459,
      "learning_rate": 4.3865884371121976e-05,
      "loss": 0.7177,
      "step": 806700
    },
    {
      "epoch": 7.361851230016789,
      "grad_norm": 4.700050354003906,
      "learning_rate": 4.386512397498601e-05,
      "loss": 0.7033,
      "step": 806800
    },
    {
      "epoch": 7.362763705379955,
      "grad_norm": 3.536982297897339,
      "learning_rate": 4.3864363578850036e-05,
      "loss": 0.7195,
      "step": 806900
    },
    {
      "epoch": 7.36367618074312,
      "grad_norm": 3.542942523956299,
      "learning_rate": 4.3863603182714066e-05,
      "loss": 0.706,
      "step": 807000
    },
    {
      "epoch": 7.364588656106285,
      "grad_norm": 4.860027313232422,
      "learning_rate": 4.3862842786578096e-05,
      "loss": 0.7166,
      "step": 807100
    },
    {
      "epoch": 7.3655011314694505,
      "grad_norm": 4.04811429977417,
      "learning_rate": 4.3862082390442126e-05,
      "loss": 0.6522,
      "step": 807200
    },
    {
      "epoch": 7.366413606832616,
      "grad_norm": 4.662905216217041,
      "learning_rate": 4.3861321994306156e-05,
      "loss": 0.7027,
      "step": 807300
    },
    {
      "epoch": 7.367326082195781,
      "grad_norm": 5.10421895980835,
      "learning_rate": 4.3860561598170186e-05,
      "loss": 0.6948,
      "step": 807400
    },
    {
      "epoch": 7.3682385575589455,
      "grad_norm": 3.9471211433410645,
      "learning_rate": 4.385980120203421e-05,
      "loss": 0.6926,
      "step": 807500
    },
    {
      "epoch": 7.369151032922111,
      "grad_norm": 4.242860794067383,
      "learning_rate": 4.3859040805898246e-05,
      "loss": 0.6965,
      "step": 807600
    },
    {
      "epoch": 7.370063508285276,
      "grad_norm": 4.47153902053833,
      "learning_rate": 4.385828040976227e-05,
      "loss": 0.723,
      "step": 807700
    },
    {
      "epoch": 7.370975983648441,
      "grad_norm": 4.07893705368042,
      "learning_rate": 4.38575200136263e-05,
      "loss": 0.6643,
      "step": 807800
    },
    {
      "epoch": 7.371888459011607,
      "grad_norm": 3.196371555328369,
      "learning_rate": 4.385675961749033e-05,
      "loss": 0.7115,
      "step": 807900
    },
    {
      "epoch": 7.372800934374772,
      "grad_norm": 4.494369029998779,
      "learning_rate": 4.385599922135436e-05,
      "loss": 0.724,
      "step": 808000
    },
    {
      "epoch": 7.373713409737937,
      "grad_norm": 4.750156402587891,
      "learning_rate": 4.385523882521838e-05,
      "loss": 0.6783,
      "step": 808100
    },
    {
      "epoch": 7.374625885101103,
      "grad_norm": 2.6975347995758057,
      "learning_rate": 4.385447842908242e-05,
      "loss": 0.7234,
      "step": 808200
    },
    {
      "epoch": 7.375538360464267,
      "grad_norm": 4.759696960449219,
      "learning_rate": 4.3853718032946443e-05,
      "loss": 0.6944,
      "step": 808300
    },
    {
      "epoch": 7.376450835827432,
      "grad_norm": 4.466505527496338,
      "learning_rate": 4.3852957636810473e-05,
      "loss": 0.693,
      "step": 808400
    },
    {
      "epoch": 7.377363311190598,
      "grad_norm": 3.9122018814086914,
      "learning_rate": 4.3852197240674504e-05,
      "loss": 0.6947,
      "step": 808500
    },
    {
      "epoch": 7.378275786553763,
      "grad_norm": 3.7569854259490967,
      "learning_rate": 4.3851436844538534e-05,
      "loss": 0.715,
      "step": 808600
    },
    {
      "epoch": 7.379188261916928,
      "grad_norm": 4.6999945640563965,
      "learning_rate": 4.3850676448402564e-05,
      "loss": 0.7031,
      "step": 808700
    },
    {
      "epoch": 7.3801007372800935,
      "grad_norm": 3.7958860397338867,
      "learning_rate": 4.3849916052266594e-05,
      "loss": 0.7124,
      "step": 808800
    },
    {
      "epoch": 7.381013212643259,
      "grad_norm": 4.303598880767822,
      "learning_rate": 4.384915565613062e-05,
      "loss": 0.6689,
      "step": 808900
    },
    {
      "epoch": 7.381925688006424,
      "grad_norm": 5.749253749847412,
      "learning_rate": 4.3848395259994654e-05,
      "loss": 0.7131,
      "step": 809000
    },
    {
      "epoch": 7.382838163369589,
      "grad_norm": 4.002306938171387,
      "learning_rate": 4.384763486385868e-05,
      "loss": 0.6996,
      "step": 809100
    },
    {
      "epoch": 7.383750638732754,
      "grad_norm": 20.26498794555664,
      "learning_rate": 4.384687446772271e-05,
      "loss": 0.7197,
      "step": 809200
    },
    {
      "epoch": 7.384663114095919,
      "grad_norm": 3.6949236392974854,
      "learning_rate": 4.384611407158674e-05,
      "loss": 0.6994,
      "step": 809300
    },
    {
      "epoch": 7.385575589459084,
      "grad_norm": 4.884005546569824,
      "learning_rate": 4.384535367545077e-05,
      "loss": 0.7065,
      "step": 809400
    },
    {
      "epoch": 7.38648806482225,
      "grad_norm": 3.53572154045105,
      "learning_rate": 4.384459327931479e-05,
      "loss": 0.67,
      "step": 809500
    },
    {
      "epoch": 7.387400540185415,
      "grad_norm": 4.671032905578613,
      "learning_rate": 4.384383288317882e-05,
      "loss": 0.7248,
      "step": 809600
    },
    {
      "epoch": 7.38831301554858,
      "grad_norm": 3.867337703704834,
      "learning_rate": 4.384307248704285e-05,
      "loss": 0.7121,
      "step": 809700
    },
    {
      "epoch": 7.389225490911746,
      "grad_norm": 4.192674160003662,
      "learning_rate": 4.384231209090688e-05,
      "loss": 0.6902,
      "step": 809800
    },
    {
      "epoch": 7.390137966274911,
      "grad_norm": 4.857807636260986,
      "learning_rate": 4.384155169477091e-05,
      "loss": 0.6875,
      "step": 809900
    },
    {
      "epoch": 7.391050441638075,
      "grad_norm": 4.064435958862305,
      "learning_rate": 4.3840791298634934e-05,
      "loss": 0.677,
      "step": 810000
    },
    {
      "epoch": 7.391962917001241,
      "grad_norm": 4.384884357452393,
      "learning_rate": 4.384003090249897e-05,
      "loss": 0.6925,
      "step": 810100
    },
    {
      "epoch": 7.392875392364406,
      "grad_norm": 4.195639610290527,
      "learning_rate": 4.3839270506362994e-05,
      "loss": 0.6861,
      "step": 810200
    },
    {
      "epoch": 7.393787867727571,
      "grad_norm": 3.7641241550445557,
      "learning_rate": 4.3838510110227024e-05,
      "loss": 0.6859,
      "step": 810300
    },
    {
      "epoch": 7.3947003430907365,
      "grad_norm": 3.755934476852417,
      "learning_rate": 4.3837749714091054e-05,
      "loss": 0.6585,
      "step": 810400
    },
    {
      "epoch": 7.395612818453902,
      "grad_norm": 4.554218292236328,
      "learning_rate": 4.3836989317955085e-05,
      "loss": 0.7045,
      "step": 810500
    },
    {
      "epoch": 7.396525293817067,
      "grad_norm": 4.531383037567139,
      "learning_rate": 4.3836228921819115e-05,
      "loss": 0.7121,
      "step": 810600
    },
    {
      "epoch": 7.397437769180232,
      "grad_norm": 4.265566349029541,
      "learning_rate": 4.3835468525683145e-05,
      "loss": 0.7221,
      "step": 810700
    },
    {
      "epoch": 7.398350244543398,
      "grad_norm": 4.317162990570068,
      "learning_rate": 4.383470812954717e-05,
      "loss": 0.7093,
      "step": 810800
    },
    {
      "epoch": 7.399262719906562,
      "grad_norm": 3.804091215133667,
      "learning_rate": 4.38339477334112e-05,
      "loss": 0.6666,
      "step": 810900
    },
    {
      "epoch": 7.4001751952697274,
      "grad_norm": 3.574925184249878,
      "learning_rate": 4.383318733727523e-05,
      "loss": 0.6754,
      "step": 811000
    },
    {
      "epoch": 7.401087670632893,
      "grad_norm": 3.855776071548462,
      "learning_rate": 4.383242694113926e-05,
      "loss": 0.6571,
      "step": 811100
    },
    {
      "epoch": 7.402000145996058,
      "grad_norm": 4.040219306945801,
      "learning_rate": 4.383166654500329e-05,
      "loss": 0.6997,
      "step": 811200
    },
    {
      "epoch": 7.402912621359223,
      "grad_norm": 3.7689504623413086,
      "learning_rate": 4.383090614886732e-05,
      "loss": 0.6951,
      "step": 811300
    },
    {
      "epoch": 7.403825096722389,
      "grad_norm": 5.051730632781982,
      "learning_rate": 4.383014575273134e-05,
      "loss": 0.7321,
      "step": 811400
    },
    {
      "epoch": 7.404737572085554,
      "grad_norm": 3.7448947429656982,
      "learning_rate": 4.382938535659538e-05,
      "loss": 0.6624,
      "step": 811500
    },
    {
      "epoch": 7.405650047448719,
      "grad_norm": 3.725383758544922,
      "learning_rate": 4.38286249604594e-05,
      "loss": 0.7128,
      "step": 811600
    },
    {
      "epoch": 7.406562522811884,
      "grad_norm": 4.444459438323975,
      "learning_rate": 4.382786456432343e-05,
      "loss": 0.7213,
      "step": 811700
    },
    {
      "epoch": 7.407474998175049,
      "grad_norm": 3.242053270339966,
      "learning_rate": 4.382710416818746e-05,
      "loss": 0.6807,
      "step": 811800
    },
    {
      "epoch": 7.408387473538214,
      "grad_norm": 4.055274486541748,
      "learning_rate": 4.382634377205149e-05,
      "loss": 0.7178,
      "step": 811900
    },
    {
      "epoch": 7.4092999489013796,
      "grad_norm": 3.721242904663086,
      "learning_rate": 4.382558337591552e-05,
      "loss": 0.7222,
      "step": 812000
    },
    {
      "epoch": 7.410212424264545,
      "grad_norm": 3.7304654121398926,
      "learning_rate": 4.382482297977955e-05,
      "loss": 0.7084,
      "step": 812100
    },
    {
      "epoch": 7.41112489962771,
      "grad_norm": 3.2400362491607666,
      "learning_rate": 4.3824062583643575e-05,
      "loss": 0.7105,
      "step": 812200
    },
    {
      "epoch": 7.4120373749908754,
      "grad_norm": 1.9988242387771606,
      "learning_rate": 4.3823302187507605e-05,
      "loss": 0.6636,
      "step": 812300
    },
    {
      "epoch": 7.412949850354041,
      "grad_norm": 4.724806785583496,
      "learning_rate": 4.3822541791371635e-05,
      "loss": 0.6801,
      "step": 812400
    },
    {
      "epoch": 7.413862325717206,
      "grad_norm": 5.283994197845459,
      "learning_rate": 4.382178139523566e-05,
      "loss": 0.7219,
      "step": 812500
    },
    {
      "epoch": 7.4147748010803705,
      "grad_norm": 3.6981236934661865,
      "learning_rate": 4.3821020999099696e-05,
      "loss": 0.6873,
      "step": 812600
    },
    {
      "epoch": 7.415687276443536,
      "grad_norm": 4.981631755828857,
      "learning_rate": 4.382026060296372e-05,
      "loss": 0.7019,
      "step": 812700
    },
    {
      "epoch": 7.416599751806701,
      "grad_norm": 4.458011150360107,
      "learning_rate": 4.381950020682775e-05,
      "loss": 0.6946,
      "step": 812800
    },
    {
      "epoch": 7.417512227169866,
      "grad_norm": 4.185325622558594,
      "learning_rate": 4.381873981069178e-05,
      "loss": 0.7015,
      "step": 812900
    },
    {
      "epoch": 7.418424702533032,
      "grad_norm": 4.218400955200195,
      "learning_rate": 4.381797941455581e-05,
      "loss": 0.6875,
      "step": 813000
    },
    {
      "epoch": 7.419337177896197,
      "grad_norm": 3.621615409851074,
      "learning_rate": 4.381721901841984e-05,
      "loss": 0.7114,
      "step": 813100
    },
    {
      "epoch": 7.420249653259362,
      "grad_norm": 4.011375427246094,
      "learning_rate": 4.381645862228387e-05,
      "loss": 0.693,
      "step": 813200
    },
    {
      "epoch": 7.4211621286225276,
      "grad_norm": 3.7475931644439697,
      "learning_rate": 4.381569822614789e-05,
      "loss": 0.6827,
      "step": 813300
    },
    {
      "epoch": 7.422074603985692,
      "grad_norm": 4.077504634857178,
      "learning_rate": 4.381493783001193e-05,
      "loss": 0.7078,
      "step": 813400
    },
    {
      "epoch": 7.422987079348857,
      "grad_norm": 3.665560007095337,
      "learning_rate": 4.381417743387595e-05,
      "loss": 0.6651,
      "step": 813500
    },
    {
      "epoch": 7.423899554712023,
      "grad_norm": 4.735321044921875,
      "learning_rate": 4.381341703773998e-05,
      "loss": 0.6684,
      "step": 813600
    },
    {
      "epoch": 7.424812030075188,
      "grad_norm": 3.5916223526000977,
      "learning_rate": 4.381265664160401e-05,
      "loss": 0.6453,
      "step": 813700
    },
    {
      "epoch": 7.425724505438353,
      "grad_norm": 4.051627159118652,
      "learning_rate": 4.381189624546804e-05,
      "loss": 0.7274,
      "step": 813800
    },
    {
      "epoch": 7.4266369808015185,
      "grad_norm": 5.178016185760498,
      "learning_rate": 4.3811135849332066e-05,
      "loss": 0.7165,
      "step": 813900
    },
    {
      "epoch": 7.427549456164684,
      "grad_norm": 4.38910436630249,
      "learning_rate": 4.38103754531961e-05,
      "loss": 0.6707,
      "step": 814000
    },
    {
      "epoch": 7.428461931527849,
      "grad_norm": 4.468398094177246,
      "learning_rate": 4.3809615057060126e-05,
      "loss": 0.7092,
      "step": 814100
    },
    {
      "epoch": 7.429374406891014,
      "grad_norm": 3.461245536804199,
      "learning_rate": 4.3808854660924156e-05,
      "loss": 0.6793,
      "step": 814200
    },
    {
      "epoch": 7.430286882254179,
      "grad_norm": 3.868239641189575,
      "learning_rate": 4.3808094264788186e-05,
      "loss": 0.6821,
      "step": 814300
    },
    {
      "epoch": 7.431199357617344,
      "grad_norm": 4.25483512878418,
      "learning_rate": 4.3807333868652217e-05,
      "loss": 0.6673,
      "step": 814400
    },
    {
      "epoch": 7.432111832980509,
      "grad_norm": 4.042174339294434,
      "learning_rate": 4.3806573472516247e-05,
      "loss": 0.6838,
      "step": 814500
    },
    {
      "epoch": 7.433024308343675,
      "grad_norm": 3.359243392944336,
      "learning_rate": 4.380581307638028e-05,
      "loss": 0.7219,
      "step": 814600
    },
    {
      "epoch": 7.43393678370684,
      "grad_norm": 3.8243355751037598,
      "learning_rate": 4.38050526802443e-05,
      "loss": 0.7119,
      "step": 814700
    },
    {
      "epoch": 7.434849259070005,
      "grad_norm": 3.892812728881836,
      "learning_rate": 4.380429228410834e-05,
      "loss": 0.6645,
      "step": 814800
    },
    {
      "epoch": 7.435761734433171,
      "grad_norm": 4.426499843597412,
      "learning_rate": 4.380353188797236e-05,
      "loss": 0.6923,
      "step": 814900
    },
    {
      "epoch": 7.436674209796336,
      "grad_norm": 3.78376841545105,
      "learning_rate": 4.380277149183639e-05,
      "loss": 0.731,
      "step": 815000
    },
    {
      "epoch": 7.4375866851595,
      "grad_norm": 4.355355262756348,
      "learning_rate": 4.380201109570042e-05,
      "loss": 0.7135,
      "step": 815100
    },
    {
      "epoch": 7.438499160522666,
      "grad_norm": 4.117298126220703,
      "learning_rate": 4.3801250699564443e-05,
      "loss": 0.6825,
      "step": 815200
    },
    {
      "epoch": 7.439411635885831,
      "grad_norm": 4.037682056427002,
      "learning_rate": 4.3800490303428474e-05,
      "loss": 0.7047,
      "step": 815300
    },
    {
      "epoch": 7.440324111248996,
      "grad_norm": 3.444333076477051,
      "learning_rate": 4.3799729907292504e-05,
      "loss": 0.6771,
      "step": 815400
    },
    {
      "epoch": 7.4412365866121615,
      "grad_norm": 3.288004159927368,
      "learning_rate": 4.3798969511156534e-05,
      "loss": 0.6703,
      "step": 815500
    },
    {
      "epoch": 7.442149061975327,
      "grad_norm": 3.3673527240753174,
      "learning_rate": 4.3798209115020564e-05,
      "loss": 0.6746,
      "step": 815600
    },
    {
      "epoch": 7.443061537338492,
      "grad_norm": 3.740640878677368,
      "learning_rate": 4.3797448718884594e-05,
      "loss": 0.7263,
      "step": 815700
    },
    {
      "epoch": 7.443974012701657,
      "grad_norm": 4.454596042633057,
      "learning_rate": 4.379668832274862e-05,
      "loss": 0.6759,
      "step": 815800
    },
    {
      "epoch": 7.444886488064823,
      "grad_norm": 4.093662261962891,
      "learning_rate": 4.3795927926612654e-05,
      "loss": 0.6987,
      "step": 815900
    },
    {
      "epoch": 7.445798963427987,
      "grad_norm": 4.220093727111816,
      "learning_rate": 4.379516753047668e-05,
      "loss": 0.6495,
      "step": 816000
    },
    {
      "epoch": 7.446711438791152,
      "grad_norm": 3.901853084564209,
      "learning_rate": 4.379440713434071e-05,
      "loss": 0.6693,
      "step": 816100
    },
    {
      "epoch": 7.447623914154318,
      "grad_norm": 3.668553352355957,
      "learning_rate": 4.379364673820474e-05,
      "loss": 0.7124,
      "step": 816200
    },
    {
      "epoch": 7.448536389517483,
      "grad_norm": 2.8004956245422363,
      "learning_rate": 4.379288634206877e-05,
      "loss": 0.6887,
      "step": 816300
    },
    {
      "epoch": 7.449448864880648,
      "grad_norm": 4.830202102661133,
      "learning_rate": 4.379212594593279e-05,
      "loss": 0.7209,
      "step": 816400
    },
    {
      "epoch": 7.450361340243814,
      "grad_norm": 3.9181456565856934,
      "learning_rate": 4.379136554979683e-05,
      "loss": 0.7282,
      "step": 816500
    },
    {
      "epoch": 7.451273815606979,
      "grad_norm": 3.2210447788238525,
      "learning_rate": 4.379060515366085e-05,
      "loss": 0.6766,
      "step": 816600
    },
    {
      "epoch": 7.452186290970144,
      "grad_norm": 4.047525405883789,
      "learning_rate": 4.378984475752488e-05,
      "loss": 0.7363,
      "step": 816700
    },
    {
      "epoch": 7.453098766333309,
      "grad_norm": 4.4684343338012695,
      "learning_rate": 4.378908436138891e-05,
      "loss": 0.6804,
      "step": 816800
    },
    {
      "epoch": 7.454011241696474,
      "grad_norm": 3.62174654006958,
      "learning_rate": 4.378832396525294e-05,
      "loss": 0.6704,
      "step": 816900
    },
    {
      "epoch": 7.454923717059639,
      "grad_norm": 3.2828948497772217,
      "learning_rate": 4.378756356911697e-05,
      "loss": 0.6925,
      "step": 817000
    },
    {
      "epoch": 7.4558361924228045,
      "grad_norm": 4.351017951965332,
      "learning_rate": 4.3786803172981e-05,
      "loss": 0.683,
      "step": 817100
    },
    {
      "epoch": 7.45674866778597,
      "grad_norm": 3.6233348846435547,
      "learning_rate": 4.3786042776845025e-05,
      "loss": 0.711,
      "step": 817200
    },
    {
      "epoch": 7.457661143149135,
      "grad_norm": 3.482828140258789,
      "learning_rate": 4.378528238070906e-05,
      "loss": 0.7022,
      "step": 817300
    },
    {
      "epoch": 7.4585736185123,
      "grad_norm": 3.889434814453125,
      "learning_rate": 4.3784521984573085e-05,
      "loss": 0.732,
      "step": 817400
    },
    {
      "epoch": 7.459486093875466,
      "grad_norm": 3.991011381149292,
      "learning_rate": 4.3783761588437115e-05,
      "loss": 0.6884,
      "step": 817500
    },
    {
      "epoch": 7.460398569238631,
      "grad_norm": 4.093149185180664,
      "learning_rate": 4.3783001192301145e-05,
      "loss": 0.7423,
      "step": 817600
    },
    {
      "epoch": 7.461311044601795,
      "grad_norm": 3.9885151386260986,
      "learning_rate": 4.3782240796165175e-05,
      "loss": 0.6881,
      "step": 817700
    },
    {
      "epoch": 7.462223519964961,
      "grad_norm": 3.846951484680176,
      "learning_rate": 4.37814804000292e-05,
      "loss": 0.7031,
      "step": 817800
    },
    {
      "epoch": 7.463135995328126,
      "grad_norm": 4.265954971313477,
      "learning_rate": 4.3780720003893235e-05,
      "loss": 0.6979,
      "step": 817900
    },
    {
      "epoch": 7.464048470691291,
      "grad_norm": 3.693903923034668,
      "learning_rate": 4.377995960775726e-05,
      "loss": 0.6798,
      "step": 818000
    },
    {
      "epoch": 7.464960946054457,
      "grad_norm": 4.511372089385986,
      "learning_rate": 4.377919921162129e-05,
      "loss": 0.6861,
      "step": 818100
    },
    {
      "epoch": 7.465873421417622,
      "grad_norm": 3.984600305557251,
      "learning_rate": 4.377843881548532e-05,
      "loss": 0.7479,
      "step": 818200
    },
    {
      "epoch": 7.466785896780787,
      "grad_norm": 4.445286750793457,
      "learning_rate": 4.377767841934934e-05,
      "loss": 0.7141,
      "step": 818300
    },
    {
      "epoch": 7.4676983721439525,
      "grad_norm": 4.180960178375244,
      "learning_rate": 4.377691802321338e-05,
      "loss": 0.6918,
      "step": 818400
    },
    {
      "epoch": 7.468610847507117,
      "grad_norm": 3.499938488006592,
      "learning_rate": 4.37761576270774e-05,
      "loss": 0.6924,
      "step": 818500
    },
    {
      "epoch": 7.469523322870282,
      "grad_norm": 4.78202486038208,
      "learning_rate": 4.377539723094143e-05,
      "loss": 0.7129,
      "step": 818600
    },
    {
      "epoch": 7.4704357982334475,
      "grad_norm": 3.6795654296875,
      "learning_rate": 4.377463683480546e-05,
      "loss": 0.7176,
      "step": 818700
    },
    {
      "epoch": 7.471348273596613,
      "grad_norm": 3.9754703044891357,
      "learning_rate": 4.377387643866949e-05,
      "loss": 0.6596,
      "step": 818800
    },
    {
      "epoch": 7.472260748959778,
      "grad_norm": 3.841280698776245,
      "learning_rate": 4.3773116042533515e-05,
      "loss": 0.6648,
      "step": 818900
    },
    {
      "epoch": 7.473173224322943,
      "grad_norm": 4.145411491394043,
      "learning_rate": 4.377235564639755e-05,
      "loss": 0.6963,
      "step": 819000
    },
    {
      "epoch": 7.474085699686109,
      "grad_norm": 4.632399082183838,
      "learning_rate": 4.3771595250261575e-05,
      "loss": 0.7132,
      "step": 819100
    },
    {
      "epoch": 7.474998175049274,
      "grad_norm": 3.1326963901519775,
      "learning_rate": 4.3770834854125606e-05,
      "loss": 0.7012,
      "step": 819200
    },
    {
      "epoch": 7.475910650412439,
      "grad_norm": 4.382837772369385,
      "learning_rate": 4.3770074457989636e-05,
      "loss": 0.7247,
      "step": 819300
    },
    {
      "epoch": 7.476823125775604,
      "grad_norm": 4.352077484130859,
      "learning_rate": 4.3769314061853666e-05,
      "loss": 0.6723,
      "step": 819400
    },
    {
      "epoch": 7.477735601138769,
      "grad_norm": 4.566636562347412,
      "learning_rate": 4.3768553665717696e-05,
      "loss": 0.6732,
      "step": 819500
    },
    {
      "epoch": 7.478648076501934,
      "grad_norm": 4.684793949127197,
      "learning_rate": 4.3767793269581726e-05,
      "loss": 0.7023,
      "step": 819600
    },
    {
      "epoch": 7.4795605518651,
      "grad_norm": 4.580609321594238,
      "learning_rate": 4.376703287344575e-05,
      "loss": 0.6944,
      "step": 819700
    },
    {
      "epoch": 7.480473027228265,
      "grad_norm": 4.413619518280029,
      "learning_rate": 4.3766272477309786e-05,
      "loss": 0.704,
      "step": 819800
    },
    {
      "epoch": 7.48138550259143,
      "grad_norm": 4.022915363311768,
      "learning_rate": 4.376551208117381e-05,
      "loss": 0.6946,
      "step": 819900
    },
    {
      "epoch": 7.4822979779545955,
      "grad_norm": 3.7413928508758545,
      "learning_rate": 4.376475168503784e-05,
      "loss": 0.6963,
      "step": 820000
    },
    {
      "epoch": 7.483210453317761,
      "grad_norm": 4.0756001472473145,
      "learning_rate": 4.376399128890187e-05,
      "loss": 0.6959,
      "step": 820100
    },
    {
      "epoch": 7.484122928680925,
      "grad_norm": 3.146135091781616,
      "learning_rate": 4.37632308927659e-05,
      "loss": 0.6973,
      "step": 820200
    },
    {
      "epoch": 7.4850354040440905,
      "grad_norm": 3.9242255687713623,
      "learning_rate": 4.376247049662992e-05,
      "loss": 0.6915,
      "step": 820300
    },
    {
      "epoch": 7.485947879407256,
      "grad_norm": 4.6294026374816895,
      "learning_rate": 4.376171010049396e-05,
      "loss": 0.7124,
      "step": 820400
    },
    {
      "epoch": 7.486860354770421,
      "grad_norm": 4.138352394104004,
      "learning_rate": 4.376094970435798e-05,
      "loss": 0.6658,
      "step": 820500
    },
    {
      "epoch": 7.487772830133586,
      "grad_norm": 4.147820949554443,
      "learning_rate": 4.376018930822201e-05,
      "loss": 0.7221,
      "step": 820600
    },
    {
      "epoch": 7.488685305496752,
      "grad_norm": 2.987393379211426,
      "learning_rate": 4.375942891208604e-05,
      "loss": 0.6644,
      "step": 820700
    },
    {
      "epoch": 7.489597780859917,
      "grad_norm": 3.866220712661743,
      "learning_rate": 4.375866851595007e-05,
      "loss": 0.688,
      "step": 820800
    },
    {
      "epoch": 7.490510256223082,
      "grad_norm": 5.262596607208252,
      "learning_rate": 4.37579081198141e-05,
      "loss": 0.6853,
      "step": 820900
    },
    {
      "epoch": 7.491422731586248,
      "grad_norm": 3.8340086936950684,
      "learning_rate": 4.3757147723678126e-05,
      "loss": 0.7173,
      "step": 821000
    },
    {
      "epoch": 7.492335206949412,
      "grad_norm": 4.083425998687744,
      "learning_rate": 4.3756387327542156e-05,
      "loss": 0.6746,
      "step": 821100
    },
    {
      "epoch": 7.493247682312577,
      "grad_norm": 4.152157306671143,
      "learning_rate": 4.3755626931406187e-05,
      "loss": 0.6949,
      "step": 821200
    },
    {
      "epoch": 7.494160157675743,
      "grad_norm": 4.682239532470703,
      "learning_rate": 4.375486653527022e-05,
      "loss": 0.7333,
      "step": 821300
    },
    {
      "epoch": 7.495072633038908,
      "grad_norm": 5.178226470947266,
      "learning_rate": 4.375410613913424e-05,
      "loss": 0.6783,
      "step": 821400
    },
    {
      "epoch": 7.495985108402073,
      "grad_norm": 3.9742045402526855,
      "learning_rate": 4.375334574299828e-05,
      "loss": 0.6888,
      "step": 821500
    },
    {
      "epoch": 7.4968975837652385,
      "grad_norm": 3.544782876968384,
      "learning_rate": 4.37525853468623e-05,
      "loss": 0.7022,
      "step": 821600
    },
    {
      "epoch": 7.497810059128404,
      "grad_norm": 3.898430824279785,
      "learning_rate": 4.375182495072633e-05,
      "loss": 0.6726,
      "step": 821700
    },
    {
      "epoch": 7.498722534491569,
      "grad_norm": 5.01003360748291,
      "learning_rate": 4.375106455459036e-05,
      "loss": 0.6781,
      "step": 821800
    },
    {
      "epoch": 7.4996350098547335,
      "grad_norm": 3.831519842147827,
      "learning_rate": 4.375030415845439e-05,
      "loss": 0.6797,
      "step": 821900
    },
    {
      "epoch": 7.500547485217899,
      "grad_norm": 3.982713460922241,
      "learning_rate": 4.374954376231842e-05,
      "loss": 0.6915,
      "step": 822000
    },
    {
      "epoch": 7.501459960581064,
      "grad_norm": 4.075774669647217,
      "learning_rate": 4.374878336618245e-05,
      "loss": 0.6997,
      "step": 822100
    },
    {
      "epoch": 7.502372435944229,
      "grad_norm": 3.966477870941162,
      "learning_rate": 4.3748022970046474e-05,
      "loss": 0.6915,
      "step": 822200
    },
    {
      "epoch": 7.503284911307395,
      "grad_norm": 3.678098201751709,
      "learning_rate": 4.374726257391051e-05,
      "loss": 0.7291,
      "step": 822300
    },
    {
      "epoch": 7.50419738667056,
      "grad_norm": 3.4443130493164062,
      "learning_rate": 4.3746502177774534e-05,
      "loss": 0.7232,
      "step": 822400
    },
    {
      "epoch": 7.505109862033725,
      "grad_norm": 4.751734733581543,
      "learning_rate": 4.3745741781638564e-05,
      "loss": 0.719,
      "step": 822500
    },
    {
      "epoch": 7.506022337396891,
      "grad_norm": 3.869203567504883,
      "learning_rate": 4.3744981385502594e-05,
      "loss": 0.6921,
      "step": 822600
    },
    {
      "epoch": 7.506934812760056,
      "grad_norm": 3.9284300804138184,
      "learning_rate": 4.3744220989366624e-05,
      "loss": 0.6965,
      "step": 822700
    },
    {
      "epoch": 7.50784728812322,
      "grad_norm": 4.540218830108643,
      "learning_rate": 4.3743460593230654e-05,
      "loss": 0.7054,
      "step": 822800
    },
    {
      "epoch": 7.508759763486386,
      "grad_norm": 4.2862348556518555,
      "learning_rate": 4.3742700197094684e-05,
      "loss": 0.6747,
      "step": 822900
    },
    {
      "epoch": 7.509672238849551,
      "grad_norm": 3.23042368888855,
      "learning_rate": 4.374193980095871e-05,
      "loss": 0.7064,
      "step": 823000
    },
    {
      "epoch": 7.510584714212716,
      "grad_norm": 4.514774322509766,
      "learning_rate": 4.374117940482274e-05,
      "loss": 0.686,
      "step": 823100
    },
    {
      "epoch": 7.5114971895758815,
      "grad_norm": 3.4577839374542236,
      "learning_rate": 4.374041900868677e-05,
      "loss": 0.7064,
      "step": 823200
    },
    {
      "epoch": 7.512409664939047,
      "grad_norm": 4.218812942504883,
      "learning_rate": 4.37396586125508e-05,
      "loss": 0.7092,
      "step": 823300
    },
    {
      "epoch": 7.513322140302212,
      "grad_norm": 3.7847704887390137,
      "learning_rate": 4.373889821641483e-05,
      "loss": 0.7317,
      "step": 823400
    },
    {
      "epoch": 7.5142346156653765,
      "grad_norm": 3.544926643371582,
      "learning_rate": 4.373813782027886e-05,
      "loss": 0.68,
      "step": 823500
    },
    {
      "epoch": 7.515147091028542,
      "grad_norm": 4.131680488586426,
      "learning_rate": 4.373737742414288e-05,
      "loss": 0.6986,
      "step": 823600
    },
    {
      "epoch": 7.516059566391707,
      "grad_norm": 4.0436530113220215,
      "learning_rate": 4.373661702800691e-05,
      "loss": 0.7045,
      "step": 823700
    },
    {
      "epoch": 7.516972041754872,
      "grad_norm": 4.9372172355651855,
      "learning_rate": 4.373585663187094e-05,
      "loss": 0.731,
      "step": 823800
    },
    {
      "epoch": 7.517884517118038,
      "grad_norm": 3.664670467376709,
      "learning_rate": 4.373509623573497e-05,
      "loss": 0.7064,
      "step": 823900
    },
    {
      "epoch": 7.518796992481203,
      "grad_norm": 3.2372419834136963,
      "learning_rate": 4.3734335839599e-05,
      "loss": 0.6899,
      "step": 824000
    },
    {
      "epoch": 7.519709467844368,
      "grad_norm": 4.012200355529785,
      "learning_rate": 4.3733575443463025e-05,
      "loss": 0.7317,
      "step": 824100
    },
    {
      "epoch": 7.520621943207534,
      "grad_norm": 3.728733539581299,
      "learning_rate": 4.373281504732706e-05,
      "loss": 0.7342,
      "step": 824200
    },
    {
      "epoch": 7.521534418570699,
      "grad_norm": 4.813868045806885,
      "learning_rate": 4.3732054651191085e-05,
      "loss": 0.6619,
      "step": 824300
    },
    {
      "epoch": 7.522446893933864,
      "grad_norm": 4.167680740356445,
      "learning_rate": 4.3731294255055115e-05,
      "loss": 0.7056,
      "step": 824400
    },
    {
      "epoch": 7.523359369297029,
      "grad_norm": 4.077917575836182,
      "learning_rate": 4.3730533858919145e-05,
      "loss": 0.6828,
      "step": 824500
    },
    {
      "epoch": 7.524271844660194,
      "grad_norm": 3.61303448677063,
      "learning_rate": 4.3729773462783175e-05,
      "loss": 0.6567,
      "step": 824600
    },
    {
      "epoch": 7.525184320023359,
      "grad_norm": 3.6328482627868652,
      "learning_rate": 4.37290130666472e-05,
      "loss": 0.6868,
      "step": 824700
    },
    {
      "epoch": 7.5260967953865245,
      "grad_norm": 4.165770053863525,
      "learning_rate": 4.3728252670511235e-05,
      "loss": 0.6874,
      "step": 824800
    },
    {
      "epoch": 7.52700927074969,
      "grad_norm": 3.6820948123931885,
      "learning_rate": 4.372749227437526e-05,
      "loss": 0.6806,
      "step": 824900
    },
    {
      "epoch": 7.527921746112855,
      "grad_norm": 4.288150310516357,
      "learning_rate": 4.372673187823929e-05,
      "loss": 0.665,
      "step": 825000
    },
    {
      "epoch": 7.52883422147602,
      "grad_norm": 3.9250566959381104,
      "learning_rate": 4.372597148210332e-05,
      "loss": 0.6919,
      "step": 825100
    },
    {
      "epoch": 7.529746696839185,
      "grad_norm": 4.196986198425293,
      "learning_rate": 4.372521108596735e-05,
      "loss": 0.7387,
      "step": 825200
    },
    {
      "epoch": 7.53065917220235,
      "grad_norm": 5.190121650695801,
      "learning_rate": 4.372445068983138e-05,
      "loss": 0.7083,
      "step": 825300
    },
    {
      "epoch": 7.5315716475655154,
      "grad_norm": 3.8521411418914795,
      "learning_rate": 4.372369029369541e-05,
      "loss": 0.6944,
      "step": 825400
    },
    {
      "epoch": 7.532484122928681,
      "grad_norm": 4.236936569213867,
      "learning_rate": 4.372292989755943e-05,
      "loss": 0.7054,
      "step": 825500
    },
    {
      "epoch": 7.533396598291846,
      "grad_norm": 3.6554908752441406,
      "learning_rate": 4.372216950142347e-05,
      "loss": 0.7059,
      "step": 825600
    },
    {
      "epoch": 7.534309073655011,
      "grad_norm": 4.488113880157471,
      "learning_rate": 4.372140910528749e-05,
      "loss": 0.6965,
      "step": 825700
    },
    {
      "epoch": 7.535221549018177,
      "grad_norm": 4.021990776062012,
      "learning_rate": 4.372064870915152e-05,
      "loss": 0.7225,
      "step": 825800
    },
    {
      "epoch": 7.536134024381342,
      "grad_norm": 3.830493688583374,
      "learning_rate": 4.371988831301555e-05,
      "loss": 0.6831,
      "step": 825900
    },
    {
      "epoch": 7.537046499744507,
      "grad_norm": 4.194352149963379,
      "learning_rate": 4.371912791687958e-05,
      "loss": 0.7511,
      "step": 826000
    },
    {
      "epoch": 7.5379589751076725,
      "grad_norm": 4.081770896911621,
      "learning_rate": 4.3718367520743606e-05,
      "loss": 0.6848,
      "step": 826100
    },
    {
      "epoch": 7.538871450470837,
      "grad_norm": 4.357117652893066,
      "learning_rate": 4.371760712460764e-05,
      "loss": 0.6933,
      "step": 826200
    },
    {
      "epoch": 7.539783925834002,
      "grad_norm": 3.9806480407714844,
      "learning_rate": 4.3716846728471666e-05,
      "loss": 0.684,
      "step": 826300
    },
    {
      "epoch": 7.5406964011971676,
      "grad_norm": 4.1675801277160645,
      "learning_rate": 4.3716086332335696e-05,
      "loss": 0.6813,
      "step": 826400
    },
    {
      "epoch": 7.541608876560333,
      "grad_norm": 3.881354808807373,
      "learning_rate": 4.3715325936199726e-05,
      "loss": 0.6934,
      "step": 826500
    },
    {
      "epoch": 7.542521351923498,
      "grad_norm": 4.137505531311035,
      "learning_rate": 4.371456554006375e-05,
      "loss": 0.6659,
      "step": 826600
    },
    {
      "epoch": 7.5434338272866635,
      "grad_norm": 3.822427272796631,
      "learning_rate": 4.3713805143927786e-05,
      "loss": 0.6968,
      "step": 826700
    },
    {
      "epoch": 7.544346302649829,
      "grad_norm": 3.3752360343933105,
      "learning_rate": 4.371304474779181e-05,
      "loss": 0.6834,
      "step": 826800
    },
    {
      "epoch": 7.545258778012993,
      "grad_norm": 4.44993782043457,
      "learning_rate": 4.371228435165584e-05,
      "loss": 0.7153,
      "step": 826900
    },
    {
      "epoch": 7.5461712533761585,
      "grad_norm": 4.299799919128418,
      "learning_rate": 4.371152395551987e-05,
      "loss": 0.6703,
      "step": 827000
    },
    {
      "epoch": 7.547083728739324,
      "grad_norm": 3.3332912921905518,
      "learning_rate": 4.37107635593839e-05,
      "loss": 0.7072,
      "step": 827100
    },
    {
      "epoch": 7.547996204102489,
      "grad_norm": 4.06351900100708,
      "learning_rate": 4.371000316324792e-05,
      "loss": 0.677,
      "step": 827200
    },
    {
      "epoch": 7.548908679465654,
      "grad_norm": 3.370919704437256,
      "learning_rate": 4.370924276711196e-05,
      "loss": 0.6849,
      "step": 827300
    },
    {
      "epoch": 7.54982115482882,
      "grad_norm": 4.032772541046143,
      "learning_rate": 4.370848237097598e-05,
      "loss": 0.7476,
      "step": 827400
    },
    {
      "epoch": 7.550733630191985,
      "grad_norm": 3.965735673904419,
      "learning_rate": 4.370772197484001e-05,
      "loss": 0.6733,
      "step": 827500
    },
    {
      "epoch": 7.55164610555515,
      "grad_norm": 4.115852355957031,
      "learning_rate": 4.370696157870404e-05,
      "loss": 0.6881,
      "step": 827600
    },
    {
      "epoch": 7.552558580918316,
      "grad_norm": 3.7652809619903564,
      "learning_rate": 4.370620118256807e-05,
      "loss": 0.7074,
      "step": 827700
    },
    {
      "epoch": 7.553471056281481,
      "grad_norm": 3.3622002601623535,
      "learning_rate": 4.37054407864321e-05,
      "loss": 0.69,
      "step": 827800
    },
    {
      "epoch": 7.554383531644645,
      "grad_norm": 3.596393346786499,
      "learning_rate": 4.370468039029613e-05,
      "loss": 0.6809,
      "step": 827900
    },
    {
      "epoch": 7.555296007007811,
      "grad_norm": 4.028471946716309,
      "learning_rate": 4.3703919994160157e-05,
      "loss": 0.6695,
      "step": 828000
    },
    {
      "epoch": 7.556208482370976,
      "grad_norm": 3.8204493522644043,
      "learning_rate": 4.3703159598024193e-05,
      "loss": 0.6757,
      "step": 828100
    },
    {
      "epoch": 7.557120957734141,
      "grad_norm": 3.868447780609131,
      "learning_rate": 4.370239920188822e-05,
      "loss": 0.6815,
      "step": 828200
    },
    {
      "epoch": 7.5580334330973065,
      "grad_norm": 3.7369766235351562,
      "learning_rate": 4.370163880575225e-05,
      "loss": 0.6941,
      "step": 828300
    },
    {
      "epoch": 7.558945908460472,
      "grad_norm": 4.191002368927002,
      "learning_rate": 4.370087840961628e-05,
      "loss": 0.6894,
      "step": 828400
    },
    {
      "epoch": 7.559858383823637,
      "grad_norm": 3.2027854919433594,
      "learning_rate": 4.370011801348031e-05,
      "loss": 0.6825,
      "step": 828500
    },
    {
      "epoch": 7.5607708591868015,
      "grad_norm": 4.6186137199401855,
      "learning_rate": 4.369935761734433e-05,
      "loss": 0.7036,
      "step": 828600
    },
    {
      "epoch": 7.561683334549967,
      "grad_norm": 3.4578468799591064,
      "learning_rate": 4.369859722120837e-05,
      "loss": 0.7118,
      "step": 828700
    },
    {
      "epoch": 7.562595809913132,
      "grad_norm": 4.046095848083496,
      "learning_rate": 4.369783682507239e-05,
      "loss": 0.6942,
      "step": 828800
    },
    {
      "epoch": 7.563508285276297,
      "grad_norm": 5.1434760093688965,
      "learning_rate": 4.369707642893642e-05,
      "loss": 0.6925,
      "step": 828900
    },
    {
      "epoch": 7.564420760639463,
      "grad_norm": 4.134810447692871,
      "learning_rate": 4.369631603280045e-05,
      "loss": 0.6588,
      "step": 829000
    },
    {
      "epoch": 7.565333236002628,
      "grad_norm": 4.2955498695373535,
      "learning_rate": 4.369555563666448e-05,
      "loss": 0.718,
      "step": 829100
    },
    {
      "epoch": 7.566245711365793,
      "grad_norm": 3.9031059741973877,
      "learning_rate": 4.369479524052851e-05,
      "loss": 0.7005,
      "step": 829200
    },
    {
      "epoch": 7.567158186728959,
      "grad_norm": 3.520287275314331,
      "learning_rate": 4.369403484439254e-05,
      "loss": 0.6707,
      "step": 829300
    },
    {
      "epoch": 7.568070662092124,
      "grad_norm": 3.6776628494262695,
      "learning_rate": 4.3693274448256564e-05,
      "loss": 0.698,
      "step": 829400
    },
    {
      "epoch": 7.568983137455289,
      "grad_norm": 4.141726970672607,
      "learning_rate": 4.3692514052120594e-05,
      "loss": 0.6794,
      "step": 829500
    },
    {
      "epoch": 7.569895612818454,
      "grad_norm": 2.781644105911255,
      "learning_rate": 4.3691753655984624e-05,
      "loss": 0.6582,
      "step": 829600
    },
    {
      "epoch": 7.570808088181619,
      "grad_norm": 3.8696815967559814,
      "learning_rate": 4.369099325984865e-05,
      "loss": 0.7447,
      "step": 829700
    },
    {
      "epoch": 7.571720563544784,
      "grad_norm": 4.1930108070373535,
      "learning_rate": 4.3690232863712684e-05,
      "loss": 0.6778,
      "step": 829800
    },
    {
      "epoch": 7.5726330389079495,
      "grad_norm": 3.7408690452575684,
      "learning_rate": 4.368947246757671e-05,
      "loss": 0.7076,
      "step": 829900
    },
    {
      "epoch": 7.573545514271115,
      "grad_norm": 4.45097017288208,
      "learning_rate": 4.368871207144074e-05,
      "loss": 0.6884,
      "step": 830000
    },
    {
      "epoch": 7.57445798963428,
      "grad_norm": 3.9984583854675293,
      "learning_rate": 4.368795167530477e-05,
      "loss": 0.7023,
      "step": 830100
    },
    {
      "epoch": 7.575370464997445,
      "grad_norm": 3.3389997482299805,
      "learning_rate": 4.36871912791688e-05,
      "loss": 0.688,
      "step": 830200
    },
    {
      "epoch": 7.57628294036061,
      "grad_norm": 4.781709671020508,
      "learning_rate": 4.368643088303283e-05,
      "loss": 0.6723,
      "step": 830300
    },
    {
      "epoch": 7.577195415723775,
      "grad_norm": 3.201582908630371,
      "learning_rate": 4.368567048689686e-05,
      "loss": 0.6884,
      "step": 830400
    },
    {
      "epoch": 7.57810789108694,
      "grad_norm": 4.436415195465088,
      "learning_rate": 4.368491009076088e-05,
      "loss": 0.6942,
      "step": 830500
    },
    {
      "epoch": 7.579020366450106,
      "grad_norm": 4.650563716888428,
      "learning_rate": 4.368414969462492e-05,
      "loss": 0.7031,
      "step": 830600
    },
    {
      "epoch": 7.579932841813271,
      "grad_norm": 2.9957218170166016,
      "learning_rate": 4.368338929848894e-05,
      "loss": 0.7175,
      "step": 830700
    },
    {
      "epoch": 7.580845317176436,
      "grad_norm": 3.1953506469726562,
      "learning_rate": 4.368262890235297e-05,
      "loss": 0.6876,
      "step": 830800
    },
    {
      "epoch": 7.581757792539602,
      "grad_norm": 4.726193428039551,
      "learning_rate": 4.3681868506217e-05,
      "loss": 0.6914,
      "step": 830900
    },
    {
      "epoch": 7.582670267902767,
      "grad_norm": 4.671447277069092,
      "learning_rate": 4.368110811008103e-05,
      "loss": 0.7047,
      "step": 831000
    },
    {
      "epoch": 7.583582743265932,
      "grad_norm": 4.393194675445557,
      "learning_rate": 4.3680347713945055e-05,
      "loss": 0.7143,
      "step": 831100
    },
    {
      "epoch": 7.5844952186290975,
      "grad_norm": 4.5350775718688965,
      "learning_rate": 4.367958731780909e-05,
      "loss": 0.7236,
      "step": 831200
    },
    {
      "epoch": 7.585407693992262,
      "grad_norm": 5.109282493591309,
      "learning_rate": 4.3678826921673115e-05,
      "loss": 0.6969,
      "step": 831300
    },
    {
      "epoch": 7.586320169355427,
      "grad_norm": 5.462926864624023,
      "learning_rate": 4.3678066525537145e-05,
      "loss": 0.6541,
      "step": 831400
    },
    {
      "epoch": 7.5872326447185925,
      "grad_norm": 3.6163134574890137,
      "learning_rate": 4.3677306129401175e-05,
      "loss": 0.7051,
      "step": 831500
    },
    {
      "epoch": 7.588145120081758,
      "grad_norm": 4.295520305633545,
      "learning_rate": 4.3676545733265205e-05,
      "loss": 0.6809,
      "step": 831600
    },
    {
      "epoch": 7.589057595444923,
      "grad_norm": 4.211669921875,
      "learning_rate": 4.3675785337129235e-05,
      "loss": 0.6854,
      "step": 831700
    },
    {
      "epoch": 7.589970070808088,
      "grad_norm": 3.9282498359680176,
      "learning_rate": 4.3675024940993265e-05,
      "loss": 0.73,
      "step": 831800
    },
    {
      "epoch": 7.590882546171254,
      "grad_norm": 4.13392972946167,
      "learning_rate": 4.367426454485729e-05,
      "loss": 0.6637,
      "step": 831900
    },
    {
      "epoch": 7.591795021534418,
      "grad_norm": 4.7091240882873535,
      "learning_rate": 4.3673504148721325e-05,
      "loss": 0.7007,
      "step": 832000
    },
    {
      "epoch": 7.592707496897583,
      "grad_norm": 3.454125165939331,
      "learning_rate": 4.367274375258535e-05,
      "loss": 0.6896,
      "step": 832100
    },
    {
      "epoch": 7.593619972260749,
      "grad_norm": 5.05800724029541,
      "learning_rate": 4.367198335644937e-05,
      "loss": 0.7016,
      "step": 832200
    },
    {
      "epoch": 7.594532447623914,
      "grad_norm": 4.15768575668335,
      "learning_rate": 4.367122296031341e-05,
      "loss": 0.7095,
      "step": 832300
    },
    {
      "epoch": 7.595444922987079,
      "grad_norm": 4.145569801330566,
      "learning_rate": 4.367046256417743e-05,
      "loss": 0.6925,
      "step": 832400
    },
    {
      "epoch": 7.596357398350245,
      "grad_norm": 3.3839271068573,
      "learning_rate": 4.366970216804146e-05,
      "loss": 0.7449,
      "step": 832500
    },
    {
      "epoch": 7.59726987371341,
      "grad_norm": 3.603264331817627,
      "learning_rate": 4.366894177190549e-05,
      "loss": 0.7099,
      "step": 832600
    },
    {
      "epoch": 7.598182349076575,
      "grad_norm": 4.35944128036499,
      "learning_rate": 4.366818137576952e-05,
      "loss": 0.7157,
      "step": 832700
    },
    {
      "epoch": 7.5990948244397405,
      "grad_norm": 3.939987897872925,
      "learning_rate": 4.366742097963355e-05,
      "loss": 0.7225,
      "step": 832800
    },
    {
      "epoch": 7.600007299802906,
      "grad_norm": 4.445362091064453,
      "learning_rate": 4.366666058349758e-05,
      "loss": 0.6873,
      "step": 832900
    },
    {
      "epoch": 7.60091977516607,
      "grad_norm": 3.7213358879089355,
      "learning_rate": 4.3665900187361606e-05,
      "loss": 0.6948,
      "step": 833000
    },
    {
      "epoch": 7.6018322505292355,
      "grad_norm": 3.507244348526001,
      "learning_rate": 4.366513979122564e-05,
      "loss": 0.6993,
      "step": 833100
    },
    {
      "epoch": 7.602744725892401,
      "grad_norm": 4.056572437286377,
      "learning_rate": 4.3664379395089666e-05,
      "loss": 0.7003,
      "step": 833200
    },
    {
      "epoch": 7.603657201255566,
      "grad_norm": 4.233511447906494,
      "learning_rate": 4.3663618998953696e-05,
      "loss": 0.732,
      "step": 833300
    },
    {
      "epoch": 7.604569676618731,
      "grad_norm": 4.228979587554932,
      "learning_rate": 4.3662858602817726e-05,
      "loss": 0.6432,
      "step": 833400
    },
    {
      "epoch": 7.605482151981897,
      "grad_norm": 4.127023696899414,
      "learning_rate": 4.3662098206681756e-05,
      "loss": 0.6993,
      "step": 833500
    },
    {
      "epoch": 7.606394627345062,
      "grad_norm": 3.986886978149414,
      "learning_rate": 4.366133781054578e-05,
      "loss": 0.7072,
      "step": 833600
    },
    {
      "epoch": 7.607307102708226,
      "grad_norm": 4.117677688598633,
      "learning_rate": 4.3660577414409816e-05,
      "loss": 0.7067,
      "step": 833700
    },
    {
      "epoch": 7.608219578071392,
      "grad_norm": 4.759803771972656,
      "learning_rate": 4.365981701827384e-05,
      "loss": 0.6815,
      "step": 833800
    },
    {
      "epoch": 7.609132053434557,
      "grad_norm": 3.9599320888519287,
      "learning_rate": 4.365905662213787e-05,
      "loss": 0.6645,
      "step": 833900
    },
    {
      "epoch": 7.610044528797722,
      "grad_norm": 3.7746634483337402,
      "learning_rate": 4.36582962260019e-05,
      "loss": 0.6642,
      "step": 834000
    },
    {
      "epoch": 7.610957004160888,
      "grad_norm": 3.984276056289673,
      "learning_rate": 4.365753582986593e-05,
      "loss": 0.7032,
      "step": 834100
    },
    {
      "epoch": 7.611869479524053,
      "grad_norm": 3.7212278842926025,
      "learning_rate": 4.365677543372996e-05,
      "loss": 0.7008,
      "step": 834200
    },
    {
      "epoch": 7.612781954887218,
      "grad_norm": 4.124815940856934,
      "learning_rate": 4.365601503759399e-05,
      "loss": 0.7031,
      "step": 834300
    },
    {
      "epoch": 7.6136944302503835,
      "grad_norm": 3.649230718612671,
      "learning_rate": 4.365525464145801e-05,
      "loss": 0.7079,
      "step": 834400
    },
    {
      "epoch": 7.614606905613549,
      "grad_norm": 3.7245044708251953,
      "learning_rate": 4.365449424532205e-05,
      "loss": 0.721,
      "step": 834500
    },
    {
      "epoch": 7.615519380976714,
      "grad_norm": 4.341365337371826,
      "learning_rate": 4.365373384918607e-05,
      "loss": 0.7013,
      "step": 834600
    },
    {
      "epoch": 7.6164318563398785,
      "grad_norm": 4.095317840576172,
      "learning_rate": 4.36529734530501e-05,
      "loss": 0.7308,
      "step": 834700
    },
    {
      "epoch": 7.617344331703044,
      "grad_norm": 4.403505802154541,
      "learning_rate": 4.365221305691413e-05,
      "loss": 0.7171,
      "step": 834800
    },
    {
      "epoch": 7.618256807066209,
      "grad_norm": 4.377169609069824,
      "learning_rate": 4.3651452660778163e-05,
      "loss": 0.6975,
      "step": 834900
    },
    {
      "epoch": 7.619169282429374,
      "grad_norm": 4.772034168243408,
      "learning_rate": 4.365069226464219e-05,
      "loss": 0.6813,
      "step": 835000
    },
    {
      "epoch": 7.62008175779254,
      "grad_norm": 4.755535125732422,
      "learning_rate": 4.364993186850622e-05,
      "loss": 0.6323,
      "step": 835100
    },
    {
      "epoch": 7.620994233155705,
      "grad_norm": 3.8567821979522705,
      "learning_rate": 4.364917147237025e-05,
      "loss": 0.7049,
      "step": 835200
    },
    {
      "epoch": 7.62190670851887,
      "grad_norm": 3.736551523208618,
      "learning_rate": 4.364841107623428e-05,
      "loss": 0.7097,
      "step": 835300
    },
    {
      "epoch": 7.622819183882035,
      "grad_norm": 3.2088937759399414,
      "learning_rate": 4.364765068009831e-05,
      "loss": 0.6934,
      "step": 835400
    },
    {
      "epoch": 7.6237316592452,
      "grad_norm": 4.253748893737793,
      "learning_rate": 4.364689028396233e-05,
      "loss": 0.658,
      "step": 835500
    },
    {
      "epoch": 7.624644134608365,
      "grad_norm": 4.3632307052612305,
      "learning_rate": 4.364612988782637e-05,
      "loss": 0.7188,
      "step": 835600
    },
    {
      "epoch": 7.625556609971531,
      "grad_norm": 4.807528972625732,
      "learning_rate": 4.364536949169039e-05,
      "loss": 0.6804,
      "step": 835700
    },
    {
      "epoch": 7.626469085334696,
      "grad_norm": 4.111430644989014,
      "learning_rate": 4.364460909555442e-05,
      "loss": 0.722,
      "step": 835800
    },
    {
      "epoch": 7.627381560697861,
      "grad_norm": 4.732369422912598,
      "learning_rate": 4.364384869941845e-05,
      "loss": 0.6838,
      "step": 835900
    },
    {
      "epoch": 7.6282940360610265,
      "grad_norm": 4.2442402839660645,
      "learning_rate": 4.364308830328248e-05,
      "loss": 0.6692,
      "step": 836000
    },
    {
      "epoch": 7.629206511424192,
      "grad_norm": 4.170345783233643,
      "learning_rate": 4.364232790714651e-05,
      "loss": 0.6976,
      "step": 836100
    },
    {
      "epoch": 7.630118986787357,
      "grad_norm": 3.8851583003997803,
      "learning_rate": 4.364156751101054e-05,
      "loss": 0.6714,
      "step": 836200
    },
    {
      "epoch": 7.631031462150522,
      "grad_norm": 3.6845622062683105,
      "learning_rate": 4.3640807114874564e-05,
      "loss": 0.6751,
      "step": 836300
    },
    {
      "epoch": 7.631943937513687,
      "grad_norm": 5.060897350311279,
      "learning_rate": 4.3640046718738594e-05,
      "loss": 0.6954,
      "step": 836400
    },
    {
      "epoch": 7.632856412876852,
      "grad_norm": 4.863738059997559,
      "learning_rate": 4.3639286322602624e-05,
      "loss": 0.7116,
      "step": 836500
    },
    {
      "epoch": 7.633768888240017,
      "grad_norm": 4.520031929016113,
      "learning_rate": 4.3638525926466654e-05,
      "loss": 0.6704,
      "step": 836600
    },
    {
      "epoch": 7.634681363603183,
      "grad_norm": 3.8468334674835205,
      "learning_rate": 4.3637765530330684e-05,
      "loss": 0.7076,
      "step": 836700
    },
    {
      "epoch": 7.635593838966348,
      "grad_norm": 2.3554232120513916,
      "learning_rate": 4.3637005134194714e-05,
      "loss": 0.7061,
      "step": 836800
    },
    {
      "epoch": 7.636506314329513,
      "grad_norm": 3.812086343765259,
      "learning_rate": 4.363624473805874e-05,
      "loss": 0.6587,
      "step": 836900
    },
    {
      "epoch": 7.637418789692679,
      "grad_norm": 4.588507175445557,
      "learning_rate": 4.3635484341922775e-05,
      "loss": 0.699,
      "step": 837000
    },
    {
      "epoch": 7.638331265055843,
      "grad_norm": 3.4698574542999268,
      "learning_rate": 4.36347239457868e-05,
      "loss": 0.7145,
      "step": 837100
    },
    {
      "epoch": 7.639243740419008,
      "grad_norm": 4.979612827301025,
      "learning_rate": 4.363396354965083e-05,
      "loss": 0.6764,
      "step": 837200
    },
    {
      "epoch": 7.640156215782174,
      "grad_norm": 3.8250253200531006,
      "learning_rate": 4.363320315351486e-05,
      "loss": 0.7028,
      "step": 837300
    },
    {
      "epoch": 7.641068691145339,
      "grad_norm": 3.755314588546753,
      "learning_rate": 4.363244275737889e-05,
      "loss": 0.7046,
      "step": 837400
    },
    {
      "epoch": 7.641981166508504,
      "grad_norm": 4.318455696105957,
      "learning_rate": 4.363168236124292e-05,
      "loss": 0.6998,
      "step": 837500
    },
    {
      "epoch": 7.6428936418716695,
      "grad_norm": 4.198925495147705,
      "learning_rate": 4.363092196510695e-05,
      "loss": 0.6671,
      "step": 837600
    },
    {
      "epoch": 7.643806117234835,
      "grad_norm": 4.466520309448242,
      "learning_rate": 4.363016156897097e-05,
      "loss": 0.6975,
      "step": 837700
    },
    {
      "epoch": 7.644718592598,
      "grad_norm": 4.865048408508301,
      "learning_rate": 4.362940117283501e-05,
      "loss": 0.7048,
      "step": 837800
    },
    {
      "epoch": 7.645631067961165,
      "grad_norm": 4.866092205047607,
      "learning_rate": 4.362864077669903e-05,
      "loss": 0.6817,
      "step": 837900
    },
    {
      "epoch": 7.646543543324331,
      "grad_norm": 3.67604923248291,
      "learning_rate": 4.3627880380563055e-05,
      "loss": 0.671,
      "step": 838000
    },
    {
      "epoch": 7.647456018687495,
      "grad_norm": 4.6847076416015625,
      "learning_rate": 4.362711998442709e-05,
      "loss": 0.6948,
      "step": 838100
    },
    {
      "epoch": 7.64836849405066,
      "grad_norm": 2.3243672847747803,
      "learning_rate": 4.3626359588291115e-05,
      "loss": 0.7183,
      "step": 838200
    },
    {
      "epoch": 7.649280969413826,
      "grad_norm": 5.2218098640441895,
      "learning_rate": 4.3625599192155145e-05,
      "loss": 0.7228,
      "step": 838300
    },
    {
      "epoch": 7.650193444776991,
      "grad_norm": 3.4436283111572266,
      "learning_rate": 4.3624838796019175e-05,
      "loss": 0.6922,
      "step": 838400
    },
    {
      "epoch": 7.651105920140156,
      "grad_norm": 4.251325607299805,
      "learning_rate": 4.3624078399883205e-05,
      "loss": 0.7018,
      "step": 838500
    },
    {
      "epoch": 7.652018395503322,
      "grad_norm": 2.7395238876342773,
      "learning_rate": 4.3623318003747235e-05,
      "loss": 0.6942,
      "step": 838600
    },
    {
      "epoch": 7.652930870866487,
      "grad_norm": 3.7291224002838135,
      "learning_rate": 4.3622557607611265e-05,
      "loss": 0.6955,
      "step": 838700
    },
    {
      "epoch": 7.653843346229651,
      "grad_norm": 4.349228382110596,
      "learning_rate": 4.362179721147529e-05,
      "loss": 0.671,
      "step": 838800
    },
    {
      "epoch": 7.654755821592817,
      "grad_norm": 3.74143385887146,
      "learning_rate": 4.3621036815339325e-05,
      "loss": 0.6896,
      "step": 838900
    },
    {
      "epoch": 7.655668296955982,
      "grad_norm": 4.236349105834961,
      "learning_rate": 4.362027641920335e-05,
      "loss": 0.731,
      "step": 839000
    },
    {
      "epoch": 7.656580772319147,
      "grad_norm": 3.8991634845733643,
      "learning_rate": 4.361951602306738e-05,
      "loss": 0.7122,
      "step": 839100
    },
    {
      "epoch": 7.6574932476823125,
      "grad_norm": 3.6338789463043213,
      "learning_rate": 4.361875562693141e-05,
      "loss": 0.65,
      "step": 839200
    },
    {
      "epoch": 7.658405723045478,
      "grad_norm": 3.7549610137939453,
      "learning_rate": 4.361799523079544e-05,
      "loss": 0.7064,
      "step": 839300
    },
    {
      "epoch": 7.659318198408643,
      "grad_norm": 4.374834060668945,
      "learning_rate": 4.361723483465946e-05,
      "loss": 0.6816,
      "step": 839400
    },
    {
      "epoch": 7.660230673771808,
      "grad_norm": 2.9358136653900146,
      "learning_rate": 4.36164744385235e-05,
      "loss": 0.6953,
      "step": 839500
    },
    {
      "epoch": 7.661143149134974,
      "grad_norm": 3.9897820949554443,
      "learning_rate": 4.361571404238752e-05,
      "loss": 0.7123,
      "step": 839600
    },
    {
      "epoch": 7.662055624498139,
      "grad_norm": 4.111044883728027,
      "learning_rate": 4.361495364625155e-05,
      "loss": 0.6825,
      "step": 839700
    },
    {
      "epoch": 7.6629680998613035,
      "grad_norm": 4.161956787109375,
      "learning_rate": 4.361419325011558e-05,
      "loss": 0.7049,
      "step": 839800
    },
    {
      "epoch": 7.663880575224469,
      "grad_norm": 5.062530517578125,
      "learning_rate": 4.361343285397961e-05,
      "loss": 0.7132,
      "step": 839900
    },
    {
      "epoch": 7.664793050587634,
      "grad_norm": 3.59728741645813,
      "learning_rate": 4.361267245784364e-05,
      "loss": 0.6895,
      "step": 840000
    },
    {
      "epoch": 7.665705525950799,
      "grad_norm": 3.978452205657959,
      "learning_rate": 4.361191206170767e-05,
      "loss": 0.6724,
      "step": 840100
    },
    {
      "epoch": 7.666618001313965,
      "grad_norm": 3.3608036041259766,
      "learning_rate": 4.3611151665571696e-05,
      "loss": 0.6854,
      "step": 840200
    },
    {
      "epoch": 7.66753047667713,
      "grad_norm": 3.4444196224212646,
      "learning_rate": 4.361039126943573e-05,
      "loss": 0.6767,
      "step": 840300
    },
    {
      "epoch": 7.668442952040295,
      "grad_norm": 5.2795891761779785,
      "learning_rate": 4.3609630873299756e-05,
      "loss": 0.7178,
      "step": 840400
    },
    {
      "epoch": 7.66935542740346,
      "grad_norm": 4.020692348480225,
      "learning_rate": 4.3608870477163786e-05,
      "loss": 0.7118,
      "step": 840500
    },
    {
      "epoch": 7.670267902766625,
      "grad_norm": 4.14314079284668,
      "learning_rate": 4.3608110081027816e-05,
      "loss": 0.6675,
      "step": 840600
    },
    {
      "epoch": 7.67118037812979,
      "grad_norm": 4.264001369476318,
      "learning_rate": 4.360734968489184e-05,
      "loss": 0.7016,
      "step": 840700
    },
    {
      "epoch": 7.6720928534929556,
      "grad_norm": 3.927081346511841,
      "learning_rate": 4.360658928875587e-05,
      "loss": 0.6783,
      "step": 840800
    },
    {
      "epoch": 7.673005328856121,
      "grad_norm": 4.4424052238464355,
      "learning_rate": 4.36058288926199e-05,
      "loss": 0.7319,
      "step": 840900
    },
    {
      "epoch": 7.673917804219286,
      "grad_norm": 4.575034141540527,
      "learning_rate": 4.360506849648393e-05,
      "loss": 0.6691,
      "step": 841000
    },
    {
      "epoch": 7.6748302795824515,
      "grad_norm": 4.278447151184082,
      "learning_rate": 4.360430810034796e-05,
      "loss": 0.7179,
      "step": 841100
    },
    {
      "epoch": 7.675742754945617,
      "grad_norm": 4.774869441986084,
      "learning_rate": 4.360354770421199e-05,
      "loss": 0.6771,
      "step": 841200
    },
    {
      "epoch": 7.676655230308782,
      "grad_norm": 4.533443927764893,
      "learning_rate": 4.360278730807601e-05,
      "loss": 0.7408,
      "step": 841300
    },
    {
      "epoch": 7.677567705671947,
      "grad_norm": 4.064189910888672,
      "learning_rate": 4.360202691194005e-05,
      "loss": 0.7074,
      "step": 841400
    },
    {
      "epoch": 7.678480181035112,
      "grad_norm": 3.637604236602783,
      "learning_rate": 4.360126651580407e-05,
      "loss": 0.6992,
      "step": 841500
    },
    {
      "epoch": 7.679392656398277,
      "grad_norm": 3.6784133911132812,
      "learning_rate": 4.3600506119668103e-05,
      "loss": 0.6863,
      "step": 841600
    },
    {
      "epoch": 7.680305131761442,
      "grad_norm": 4.0239176750183105,
      "learning_rate": 4.3599745723532133e-05,
      "loss": 0.6964,
      "step": 841700
    },
    {
      "epoch": 7.681217607124608,
      "grad_norm": 3.8291873931884766,
      "learning_rate": 4.3598985327396164e-05,
      "loss": 0.6945,
      "step": 841800
    },
    {
      "epoch": 7.682130082487773,
      "grad_norm": 2.624408483505249,
      "learning_rate": 4.359822493126019e-05,
      "loss": 0.6929,
      "step": 841900
    },
    {
      "epoch": 7.683042557850938,
      "grad_norm": 3.675644874572754,
      "learning_rate": 4.3597464535124224e-05,
      "loss": 0.7214,
      "step": 842000
    },
    {
      "epoch": 7.683955033214104,
      "grad_norm": 4.690632343292236,
      "learning_rate": 4.359670413898825e-05,
      "loss": 0.7091,
      "step": 842100
    },
    {
      "epoch": 7.684867508577268,
      "grad_norm": 3.606598138809204,
      "learning_rate": 4.359594374285228e-05,
      "loss": 0.7059,
      "step": 842200
    },
    {
      "epoch": 7.685779983940433,
      "grad_norm": 3.8486430644989014,
      "learning_rate": 4.359518334671631e-05,
      "loss": 0.7219,
      "step": 842300
    },
    {
      "epoch": 7.686692459303599,
      "grad_norm": 4.222850322723389,
      "learning_rate": 4.359442295058034e-05,
      "loss": 0.6983,
      "step": 842400
    },
    {
      "epoch": 7.687604934666764,
      "grad_norm": 3.250622034072876,
      "learning_rate": 4.359366255444437e-05,
      "loss": 0.7271,
      "step": 842500
    },
    {
      "epoch": 7.688517410029929,
      "grad_norm": 4.217115879058838,
      "learning_rate": 4.35929021583084e-05,
      "loss": 0.6295,
      "step": 842600
    },
    {
      "epoch": 7.6894298853930945,
      "grad_norm": 3.284071207046509,
      "learning_rate": 4.359214176217242e-05,
      "loss": 0.7037,
      "step": 842700
    },
    {
      "epoch": 7.69034236075626,
      "grad_norm": 3.577777862548828,
      "learning_rate": 4.359138136603646e-05,
      "loss": 0.6558,
      "step": 842800
    },
    {
      "epoch": 7.691254836119425,
      "grad_norm": 4.888125419616699,
      "learning_rate": 4.359062096990048e-05,
      "loss": 0.6969,
      "step": 842900
    },
    {
      "epoch": 7.69216731148259,
      "grad_norm": 4.785491466522217,
      "learning_rate": 4.358986057376451e-05,
      "loss": 0.712,
      "step": 843000
    },
    {
      "epoch": 7.693079786845756,
      "grad_norm": 4.207540035247803,
      "learning_rate": 4.358910017762854e-05,
      "loss": 0.7271,
      "step": 843100
    },
    {
      "epoch": 7.69399226220892,
      "grad_norm": 4.492232799530029,
      "learning_rate": 4.358833978149257e-05,
      "loss": 0.6836,
      "step": 843200
    },
    {
      "epoch": 7.694904737572085,
      "grad_norm": 4.419890880584717,
      "learning_rate": 4.3587579385356594e-05,
      "loss": 0.7303,
      "step": 843300
    },
    {
      "epoch": 7.695817212935251,
      "grad_norm": 3.895454168319702,
      "learning_rate": 4.358681898922063e-05,
      "loss": 0.6938,
      "step": 843400
    },
    {
      "epoch": 7.696729688298416,
      "grad_norm": 4.173753261566162,
      "learning_rate": 4.3586058593084654e-05,
      "loss": 0.6674,
      "step": 843500
    },
    {
      "epoch": 7.697642163661581,
      "grad_norm": 4.391902923583984,
      "learning_rate": 4.3585298196948684e-05,
      "loss": 0.6827,
      "step": 843600
    },
    {
      "epoch": 7.698554639024747,
      "grad_norm": 3.6962802410125732,
      "learning_rate": 4.3584537800812714e-05,
      "loss": 0.72,
      "step": 843700
    },
    {
      "epoch": 7.699467114387912,
      "grad_norm": 3.8635013103485107,
      "learning_rate": 4.358377740467674e-05,
      "loss": 0.6961,
      "step": 843800
    },
    {
      "epoch": 7.700379589751076,
      "grad_norm": 4.633449554443359,
      "learning_rate": 4.3583017008540775e-05,
      "loss": 0.6661,
      "step": 843900
    },
    {
      "epoch": 7.701292065114242,
      "grad_norm": 3.518231153488159,
      "learning_rate": 4.35822566124048e-05,
      "loss": 0.6983,
      "step": 844000
    },
    {
      "epoch": 7.702204540477407,
      "grad_norm": 3.4609429836273193,
      "learning_rate": 4.358149621626883e-05,
      "loss": 0.7057,
      "step": 844100
    },
    {
      "epoch": 7.703117015840572,
      "grad_norm": 3.6925413608551025,
      "learning_rate": 4.358073582013286e-05,
      "loss": 0.7188,
      "step": 844200
    },
    {
      "epoch": 7.7040294912037375,
      "grad_norm": 4.307399749755859,
      "learning_rate": 4.357997542399689e-05,
      "loss": 0.7083,
      "step": 844300
    },
    {
      "epoch": 7.704941966566903,
      "grad_norm": 3.943211793899536,
      "learning_rate": 4.357921502786091e-05,
      "loss": 0.6827,
      "step": 844400
    },
    {
      "epoch": 7.705854441930068,
      "grad_norm": 4.41214656829834,
      "learning_rate": 4.357845463172495e-05,
      "loss": 0.7033,
      "step": 844500
    },
    {
      "epoch": 7.706766917293233,
      "grad_norm": 3.682692766189575,
      "learning_rate": 4.357769423558897e-05,
      "loss": 0.7114,
      "step": 844600
    },
    {
      "epoch": 7.707679392656399,
      "grad_norm": 4.187567234039307,
      "learning_rate": 4.3576933839453e-05,
      "loss": 0.7263,
      "step": 844700
    },
    {
      "epoch": 7.708591868019563,
      "grad_norm": 5.490072727203369,
      "learning_rate": 4.357617344331703e-05,
      "loss": 0.6885,
      "step": 844800
    },
    {
      "epoch": 7.709504343382728,
      "grad_norm": 2.2237164974212646,
      "learning_rate": 4.357541304718106e-05,
      "loss": 0.6941,
      "step": 844900
    },
    {
      "epoch": 7.710416818745894,
      "grad_norm": 3.4317753314971924,
      "learning_rate": 4.357465265104509e-05,
      "loss": 0.7249,
      "step": 845000
    },
    {
      "epoch": 7.711329294109059,
      "grad_norm": 3.756511926651001,
      "learning_rate": 4.357389225490912e-05,
      "loss": 0.7027,
      "step": 845100
    },
    {
      "epoch": 7.712241769472224,
      "grad_norm": 4.627763271331787,
      "learning_rate": 4.3573131858773145e-05,
      "loss": 0.6861,
      "step": 845200
    },
    {
      "epoch": 7.71315424483539,
      "grad_norm": 3.837630033493042,
      "learning_rate": 4.357237146263718e-05,
      "loss": 0.7084,
      "step": 845300
    },
    {
      "epoch": 7.714066720198555,
      "grad_norm": 4.1799397468566895,
      "learning_rate": 4.3571611066501205e-05,
      "loss": 0.7,
      "step": 845400
    },
    {
      "epoch": 7.71497919556172,
      "grad_norm": 3.559089183807373,
      "learning_rate": 4.3570850670365235e-05,
      "loss": 0.6702,
      "step": 845500
    },
    {
      "epoch": 7.715891670924885,
      "grad_norm": 2.5370094776153564,
      "learning_rate": 4.3570090274229265e-05,
      "loss": 0.686,
      "step": 845600
    },
    {
      "epoch": 7.71680414628805,
      "grad_norm": 4.033316135406494,
      "learning_rate": 4.3569329878093296e-05,
      "loss": 0.6929,
      "step": 845700
    },
    {
      "epoch": 7.717716621651215,
      "grad_norm": 3.554568290710449,
      "learning_rate": 4.356856948195732e-05,
      "loss": 0.6947,
      "step": 845800
    },
    {
      "epoch": 7.7186290970143805,
      "grad_norm": 4.2861433029174805,
      "learning_rate": 4.3567809085821356e-05,
      "loss": 0.6995,
      "step": 845900
    },
    {
      "epoch": 7.719541572377546,
      "grad_norm": 3.651843309402466,
      "learning_rate": 4.356704868968538e-05,
      "loss": 0.6705,
      "step": 846000
    },
    {
      "epoch": 7.720454047740711,
      "grad_norm": 4.00740909576416,
      "learning_rate": 4.356628829354941e-05,
      "loss": 0.7244,
      "step": 846100
    },
    {
      "epoch": 7.721366523103876,
      "grad_norm": 3.5091049671173096,
      "learning_rate": 4.356552789741344e-05,
      "loss": 0.7139,
      "step": 846200
    },
    {
      "epoch": 7.722278998467042,
      "grad_norm": 3.953394651412964,
      "learning_rate": 4.356476750127747e-05,
      "loss": 0.7113,
      "step": 846300
    },
    {
      "epoch": 7.723191473830207,
      "grad_norm": 4.236630916595459,
      "learning_rate": 4.35640071051415e-05,
      "loss": 0.722,
      "step": 846400
    },
    {
      "epoch": 7.724103949193371,
      "grad_norm": 3.784419059753418,
      "learning_rate": 4.356324670900552e-05,
      "loss": 0.6808,
      "step": 846500
    },
    {
      "epoch": 7.725016424556537,
      "grad_norm": 4.630743980407715,
      "learning_rate": 4.356248631286955e-05,
      "loss": 0.7252,
      "step": 846600
    },
    {
      "epoch": 7.725928899919702,
      "grad_norm": 3.6180715560913086,
      "learning_rate": 4.356172591673358e-05,
      "loss": 0.6948,
      "step": 846700
    },
    {
      "epoch": 7.726841375282867,
      "grad_norm": 4.504891872406006,
      "learning_rate": 4.356096552059761e-05,
      "loss": 0.6781,
      "step": 846800
    },
    {
      "epoch": 7.727753850646033,
      "grad_norm": 3.0685486793518066,
      "learning_rate": 4.3560205124461636e-05,
      "loss": 0.7037,
      "step": 846900
    },
    {
      "epoch": 7.728666326009198,
      "grad_norm": 3.643460273742676,
      "learning_rate": 4.355944472832567e-05,
      "loss": 0.6709,
      "step": 847000
    },
    {
      "epoch": 7.729578801372363,
      "grad_norm": 3.782557725906372,
      "learning_rate": 4.3558684332189696e-05,
      "loss": 0.7357,
      "step": 847100
    },
    {
      "epoch": 7.7304912767355285,
      "grad_norm": 3.7952468395233154,
      "learning_rate": 4.3557923936053726e-05,
      "loss": 0.6892,
      "step": 847200
    },
    {
      "epoch": 7.731403752098693,
      "grad_norm": 4.267347812652588,
      "learning_rate": 4.3557163539917756e-05,
      "loss": 0.7167,
      "step": 847300
    },
    {
      "epoch": 7.732316227461858,
      "grad_norm": 4.264255046844482,
      "learning_rate": 4.3556403143781786e-05,
      "loss": 0.6742,
      "step": 847400
    },
    {
      "epoch": 7.7332287028250235,
      "grad_norm": 3.9189951419830322,
      "learning_rate": 4.3555642747645816e-05,
      "loss": 0.7173,
      "step": 847500
    },
    {
      "epoch": 7.734141178188189,
      "grad_norm": 4.061638832092285,
      "learning_rate": 4.3554882351509846e-05,
      "loss": 0.7102,
      "step": 847600
    },
    {
      "epoch": 7.735053653551354,
      "grad_norm": 3.955868721008301,
      "learning_rate": 4.355412195537387e-05,
      "loss": 0.7066,
      "step": 847700
    },
    {
      "epoch": 7.735966128914519,
      "grad_norm": 4.513481616973877,
      "learning_rate": 4.3553361559237907e-05,
      "loss": 0.6887,
      "step": 847800
    },
    {
      "epoch": 7.736878604277685,
      "grad_norm": 3.760317087173462,
      "learning_rate": 4.355260116310193e-05,
      "loss": 0.7048,
      "step": 847900
    },
    {
      "epoch": 7.73779107964085,
      "grad_norm": 4.517033576965332,
      "learning_rate": 4.355184076696596e-05,
      "loss": 0.7293,
      "step": 848000
    },
    {
      "epoch": 7.738703555004015,
      "grad_norm": 4.764754295349121,
      "learning_rate": 4.355108037082999e-05,
      "loss": 0.6944,
      "step": 848100
    },
    {
      "epoch": 7.73961603036718,
      "grad_norm": 4.046374797821045,
      "learning_rate": 4.355031997469402e-05,
      "loss": 0.7324,
      "step": 848200
    },
    {
      "epoch": 7.740528505730345,
      "grad_norm": 3.9796879291534424,
      "learning_rate": 4.354955957855805e-05,
      "loss": 0.6772,
      "step": 848300
    },
    {
      "epoch": 7.74144098109351,
      "grad_norm": 4.79267692565918,
      "learning_rate": 4.354879918242208e-05,
      "loss": 0.6745,
      "step": 848400
    },
    {
      "epoch": 7.742353456456676,
      "grad_norm": 4.795390605926514,
      "learning_rate": 4.3548038786286104e-05,
      "loss": 0.6649,
      "step": 848500
    },
    {
      "epoch": 7.743265931819841,
      "grad_norm": 2.938749074935913,
      "learning_rate": 4.3547278390150134e-05,
      "loss": 0.7015,
      "step": 848600
    },
    {
      "epoch": 7.744178407183006,
      "grad_norm": 4.849437713623047,
      "learning_rate": 4.3546517994014164e-05,
      "loss": 0.6911,
      "step": 848700
    },
    {
      "epoch": 7.7450908825461715,
      "grad_norm": 3.484349012374878,
      "learning_rate": 4.3545757597878194e-05,
      "loss": 0.6891,
      "step": 848800
    },
    {
      "epoch": 7.746003357909337,
      "grad_norm": 3.8640384674072266,
      "learning_rate": 4.3544997201742224e-05,
      "loss": 0.7264,
      "step": 848900
    },
    {
      "epoch": 7.746915833272501,
      "grad_norm": 4.102295875549316,
      "learning_rate": 4.3544236805606254e-05,
      "loss": 0.696,
      "step": 849000
    },
    {
      "epoch": 7.7478283086356665,
      "grad_norm": 3.2990448474884033,
      "learning_rate": 4.354347640947028e-05,
      "loss": 0.7275,
      "step": 849100
    },
    {
      "epoch": 7.748740783998832,
      "grad_norm": 4.39278507232666,
      "learning_rate": 4.3542716013334314e-05,
      "loss": 0.7189,
      "step": 849200
    },
    {
      "epoch": 7.749653259361997,
      "grad_norm": 4.6378936767578125,
      "learning_rate": 4.354195561719834e-05,
      "loss": 0.6848,
      "step": 849300
    },
    {
      "epoch": 7.750565734725162,
      "grad_norm": 3.1701884269714355,
      "learning_rate": 4.354119522106237e-05,
      "loss": 0.6682,
      "step": 849400
    },
    {
      "epoch": 7.751478210088328,
      "grad_norm": 4.965383529663086,
      "learning_rate": 4.35404348249264e-05,
      "loss": 0.6879,
      "step": 849500
    },
    {
      "epoch": 7.752390685451493,
      "grad_norm": 4.020119667053223,
      "learning_rate": 4.353967442879042e-05,
      "loss": 0.7211,
      "step": 849600
    },
    {
      "epoch": 7.753303160814658,
      "grad_norm": 4.8893818855285645,
      "learning_rate": 4.353891403265446e-05,
      "loss": 0.6763,
      "step": 849700
    },
    {
      "epoch": 7.754215636177824,
      "grad_norm": 4.293438911437988,
      "learning_rate": 4.353815363651848e-05,
      "loss": 0.6801,
      "step": 849800
    },
    {
      "epoch": 7.755128111540988,
      "grad_norm": 3.4041574001312256,
      "learning_rate": 4.353739324038251e-05,
      "loss": 0.703,
      "step": 849900
    },
    {
      "epoch": 7.756040586904153,
      "grad_norm": 4.238494873046875,
      "learning_rate": 4.353663284424654e-05,
      "loss": 0.695,
      "step": 850000
    },
    {
      "epoch": 7.756953062267319,
      "grad_norm": 3.689260959625244,
      "learning_rate": 4.353587244811057e-05,
      "loss": 0.674,
      "step": 850100
    },
    {
      "epoch": 7.757865537630484,
      "grad_norm": 3.1402108669281006,
      "learning_rate": 4.3535112051974594e-05,
      "loss": 0.6909,
      "step": 850200
    },
    {
      "epoch": 7.758778012993649,
      "grad_norm": 3.619098663330078,
      "learning_rate": 4.353435165583863e-05,
      "loss": 0.7281,
      "step": 850300
    },
    {
      "epoch": 7.7596904883568145,
      "grad_norm": 3.7524330615997314,
      "learning_rate": 4.3533591259702654e-05,
      "loss": 0.6757,
      "step": 850400
    },
    {
      "epoch": 7.76060296371998,
      "grad_norm": 3.647907018661499,
      "learning_rate": 4.3532830863566685e-05,
      "loss": 0.7016,
      "step": 850500
    },
    {
      "epoch": 7.761515439083145,
      "grad_norm": 3.474026679992676,
      "learning_rate": 4.3532070467430715e-05,
      "loss": 0.6968,
      "step": 850600
    },
    {
      "epoch": 7.7624279144463095,
      "grad_norm": 4.941494464874268,
      "learning_rate": 4.3531310071294745e-05,
      "loss": 0.6893,
      "step": 850700
    },
    {
      "epoch": 7.763340389809475,
      "grad_norm": 3.5476479530334473,
      "learning_rate": 4.3530549675158775e-05,
      "loss": 0.674,
      "step": 850800
    },
    {
      "epoch": 7.76425286517264,
      "grad_norm": 4.700377941131592,
      "learning_rate": 4.3529789279022805e-05,
      "loss": 0.6775,
      "step": 850900
    },
    {
      "epoch": 7.765165340535805,
      "grad_norm": 4.093170166015625,
      "learning_rate": 4.352902888288683e-05,
      "loss": 0.6694,
      "step": 851000
    },
    {
      "epoch": 7.766077815898971,
      "grad_norm": 4.099690914154053,
      "learning_rate": 4.3528268486750865e-05,
      "loss": 0.6632,
      "step": 851100
    },
    {
      "epoch": 7.766990291262136,
      "grad_norm": 4.423161506652832,
      "learning_rate": 4.352750809061489e-05,
      "loss": 0.7184,
      "step": 851200
    },
    {
      "epoch": 7.767902766625301,
      "grad_norm": 4.986924648284912,
      "learning_rate": 4.352674769447892e-05,
      "loss": 0.7262,
      "step": 851300
    },
    {
      "epoch": 7.768815241988467,
      "grad_norm": 3.8795673847198486,
      "learning_rate": 4.352598729834295e-05,
      "loss": 0.7059,
      "step": 851400
    },
    {
      "epoch": 7.769727717351632,
      "grad_norm": 4.20343017578125,
      "learning_rate": 4.352522690220698e-05,
      "loss": 0.6854,
      "step": 851500
    },
    {
      "epoch": 7.770640192714796,
      "grad_norm": 3.2198307514190674,
      "learning_rate": 4.3524466506071e-05,
      "loss": 0.6554,
      "step": 851600
    },
    {
      "epoch": 7.771552668077962,
      "grad_norm": 4.697622299194336,
      "learning_rate": 4.352370610993504e-05,
      "loss": 0.6759,
      "step": 851700
    },
    {
      "epoch": 7.772465143441127,
      "grad_norm": 3.74253249168396,
      "learning_rate": 4.352294571379906e-05,
      "loss": 0.7413,
      "step": 851800
    },
    {
      "epoch": 7.773377618804292,
      "grad_norm": 3.812645673751831,
      "learning_rate": 4.352218531766309e-05,
      "loss": 0.6436,
      "step": 851900
    },
    {
      "epoch": 7.7742900941674575,
      "grad_norm": 2.989866256713867,
      "learning_rate": 4.352142492152712e-05,
      "loss": 0.6454,
      "step": 852000
    },
    {
      "epoch": 7.775202569530623,
      "grad_norm": 3.9683871269226074,
      "learning_rate": 4.3520664525391145e-05,
      "loss": 0.6626,
      "step": 852100
    },
    {
      "epoch": 7.776115044893788,
      "grad_norm": 3.813403367996216,
      "learning_rate": 4.351990412925518e-05,
      "loss": 0.6871,
      "step": 852200
    },
    {
      "epoch": 7.777027520256953,
      "grad_norm": 3.9279637336730957,
      "learning_rate": 4.3519143733119205e-05,
      "loss": 0.6828,
      "step": 852300
    },
    {
      "epoch": 7.777939995620118,
      "grad_norm": 4.633453845977783,
      "learning_rate": 4.3518383336983235e-05,
      "loss": 0.6601,
      "step": 852400
    },
    {
      "epoch": 7.778852470983283,
      "grad_norm": 3.6971592903137207,
      "learning_rate": 4.3517622940847266e-05,
      "loss": 0.6724,
      "step": 852500
    },
    {
      "epoch": 7.779764946346448,
      "grad_norm": 5.621857643127441,
      "learning_rate": 4.3516862544711296e-05,
      "loss": 0.6994,
      "step": 852600
    },
    {
      "epoch": 7.780677421709614,
      "grad_norm": 2.9947192668914795,
      "learning_rate": 4.351610214857532e-05,
      "loss": 0.6753,
      "step": 852700
    },
    {
      "epoch": 7.781589897072779,
      "grad_norm": 3.830216407775879,
      "learning_rate": 4.3515341752439356e-05,
      "loss": 0.7128,
      "step": 852800
    },
    {
      "epoch": 7.782502372435944,
      "grad_norm": 3.4657552242279053,
      "learning_rate": 4.351458135630338e-05,
      "loss": 0.6683,
      "step": 852900
    },
    {
      "epoch": 7.78341484779911,
      "grad_norm": 4.220477104187012,
      "learning_rate": 4.351382096016741e-05,
      "loss": 0.6936,
      "step": 853000
    },
    {
      "epoch": 7.784327323162275,
      "grad_norm": 3.562744617462158,
      "learning_rate": 4.351306056403144e-05,
      "loss": 0.6942,
      "step": 853100
    },
    {
      "epoch": 7.78523979852544,
      "grad_norm": 4.11710262298584,
      "learning_rate": 4.351230016789547e-05,
      "loss": 0.6973,
      "step": 853200
    },
    {
      "epoch": 7.786152273888605,
      "grad_norm": 4.757798671722412,
      "learning_rate": 4.35115397717595e-05,
      "loss": 0.7171,
      "step": 853300
    },
    {
      "epoch": 7.78706474925177,
      "grad_norm": 4.33920431137085,
      "learning_rate": 4.351077937562353e-05,
      "loss": 0.6603,
      "step": 853400
    },
    {
      "epoch": 7.787977224614935,
      "grad_norm": 3.700490713119507,
      "learning_rate": 4.351001897948755e-05,
      "loss": 0.6755,
      "step": 853500
    },
    {
      "epoch": 7.7888896999781005,
      "grad_norm": 4.079649925231934,
      "learning_rate": 4.350925858335159e-05,
      "loss": 0.7121,
      "step": 853600
    },
    {
      "epoch": 7.789802175341266,
      "grad_norm": 4.18595552444458,
      "learning_rate": 4.350849818721561e-05,
      "loss": 0.6939,
      "step": 853700
    },
    {
      "epoch": 7.790714650704431,
      "grad_norm": 3.3777828216552734,
      "learning_rate": 4.350773779107964e-05,
      "loss": 0.6985,
      "step": 853800
    },
    {
      "epoch": 7.791627126067596,
      "grad_norm": 3.978780508041382,
      "learning_rate": 4.350697739494367e-05,
      "loss": 0.7098,
      "step": 853900
    },
    {
      "epoch": 7.792539601430762,
      "grad_norm": 3.8084378242492676,
      "learning_rate": 4.35062169988077e-05,
      "loss": 0.6841,
      "step": 854000
    },
    {
      "epoch": 7.793452076793926,
      "grad_norm": 3.3772614002227783,
      "learning_rate": 4.3505456602671726e-05,
      "loss": 0.7282,
      "step": 854100
    },
    {
      "epoch": 7.7943645521570915,
      "grad_norm": 4.279480934143066,
      "learning_rate": 4.350469620653576e-05,
      "loss": 0.7368,
      "step": 854200
    },
    {
      "epoch": 7.795277027520257,
      "grad_norm": 3.7492430210113525,
      "learning_rate": 4.3503935810399786e-05,
      "loss": 0.7409,
      "step": 854300
    },
    {
      "epoch": 7.796189502883422,
      "grad_norm": 3.560288906097412,
      "learning_rate": 4.3503175414263816e-05,
      "loss": 0.6965,
      "step": 854400
    },
    {
      "epoch": 7.797101978246587,
      "grad_norm": 3.824108839035034,
      "learning_rate": 4.3502415018127847e-05,
      "loss": 0.7136,
      "step": 854500
    },
    {
      "epoch": 7.798014453609753,
      "grad_norm": 5.2309675216674805,
      "learning_rate": 4.350165462199188e-05,
      "loss": 0.6914,
      "step": 854600
    },
    {
      "epoch": 7.798926928972918,
      "grad_norm": 3.4307005405426025,
      "learning_rate": 4.350089422585591e-05,
      "loss": 0.7254,
      "step": 854700
    },
    {
      "epoch": 7.799839404336083,
      "grad_norm": 3.8422632217407227,
      "learning_rate": 4.350013382971994e-05,
      "loss": 0.7163,
      "step": 854800
    },
    {
      "epoch": 7.8007518796992485,
      "grad_norm": 3.775348663330078,
      "learning_rate": 4.349937343358396e-05,
      "loss": 0.7012,
      "step": 854900
    },
    {
      "epoch": 7.801664355062413,
      "grad_norm": 4.294026851654053,
      "learning_rate": 4.349861303744799e-05,
      "loss": 0.6757,
      "step": 855000
    },
    {
      "epoch": 7.802576830425578,
      "grad_norm": 4.719831943511963,
      "learning_rate": 4.349785264131202e-05,
      "loss": 0.7175,
      "step": 855100
    },
    {
      "epoch": 7.803489305788744,
      "grad_norm": 3.2588913440704346,
      "learning_rate": 4.3497092245176043e-05,
      "loss": 0.6818,
      "step": 855200
    },
    {
      "epoch": 7.804401781151909,
      "grad_norm": 2.9756851196289062,
      "learning_rate": 4.349633184904008e-05,
      "loss": 0.6773,
      "step": 855300
    },
    {
      "epoch": 7.805314256515074,
      "grad_norm": 4.28321647644043,
      "learning_rate": 4.3495571452904104e-05,
      "loss": 0.7178,
      "step": 855400
    },
    {
      "epoch": 7.8062267318782395,
      "grad_norm": 4.409846305847168,
      "learning_rate": 4.3494811056768134e-05,
      "loss": 0.7079,
      "step": 855500
    },
    {
      "epoch": 7.807139207241405,
      "grad_norm": 3.8418867588043213,
      "learning_rate": 4.3494050660632164e-05,
      "loss": 0.6811,
      "step": 855600
    },
    {
      "epoch": 7.80805168260457,
      "grad_norm": 4.145766258239746,
      "learning_rate": 4.3493290264496194e-05,
      "loss": 0.73,
      "step": 855700
    },
    {
      "epoch": 7.8089641579677345,
      "grad_norm": 3.4753365516662598,
      "learning_rate": 4.3492529868360224e-05,
      "loss": 0.6831,
      "step": 855800
    },
    {
      "epoch": 7.8098766333309,
      "grad_norm": 3.9146058559417725,
      "learning_rate": 4.3491769472224254e-05,
      "loss": 0.726,
      "step": 855900
    },
    {
      "epoch": 7.810789108694065,
      "grad_norm": 3.5522420406341553,
      "learning_rate": 4.349100907608828e-05,
      "loss": 0.6669,
      "step": 856000
    },
    {
      "epoch": 7.81170158405723,
      "grad_norm": 4.227845191955566,
      "learning_rate": 4.3490248679952314e-05,
      "loss": 0.6859,
      "step": 856100
    },
    {
      "epoch": 7.812614059420396,
      "grad_norm": 4.496219158172607,
      "learning_rate": 4.348948828381634e-05,
      "loss": 0.708,
      "step": 856200
    },
    {
      "epoch": 7.813526534783561,
      "grad_norm": 3.531346082687378,
      "learning_rate": 4.348872788768037e-05,
      "loss": 0.649,
      "step": 856300
    },
    {
      "epoch": 7.814439010146726,
      "grad_norm": 4.049829006195068,
      "learning_rate": 4.34879674915444e-05,
      "loss": 0.676,
      "step": 856400
    },
    {
      "epoch": 7.815351485509892,
      "grad_norm": 3.181466579437256,
      "learning_rate": 4.348720709540843e-05,
      "loss": 0.7072,
      "step": 856500
    },
    {
      "epoch": 7.816263960873057,
      "grad_norm": 3.828235387802124,
      "learning_rate": 4.348644669927245e-05,
      "loss": 0.6876,
      "step": 856600
    },
    {
      "epoch": 7.817176436236221,
      "grad_norm": 4.111601829528809,
      "learning_rate": 4.348568630313649e-05,
      "loss": 0.7239,
      "step": 856700
    },
    {
      "epoch": 7.818088911599387,
      "grad_norm": 3.5744972229003906,
      "learning_rate": 4.348492590700051e-05,
      "loss": 0.6822,
      "step": 856800
    },
    {
      "epoch": 7.819001386962552,
      "grad_norm": 4.131752014160156,
      "learning_rate": 4.348416551086454e-05,
      "loss": 0.6805,
      "step": 856900
    },
    {
      "epoch": 7.819913862325717,
      "grad_norm": 2.7949585914611816,
      "learning_rate": 4.348340511472857e-05,
      "loss": 0.6551,
      "step": 857000
    },
    {
      "epoch": 7.8208263376888825,
      "grad_norm": 3.758662700653076,
      "learning_rate": 4.34826447185926e-05,
      "loss": 0.6591,
      "step": 857100
    },
    {
      "epoch": 7.821738813052048,
      "grad_norm": 3.196770191192627,
      "learning_rate": 4.348188432245663e-05,
      "loss": 0.7056,
      "step": 857200
    },
    {
      "epoch": 7.822651288415213,
      "grad_norm": 4.924016952514648,
      "learning_rate": 4.348112392632066e-05,
      "loss": 0.6417,
      "step": 857300
    },
    {
      "epoch": 7.823563763778378,
      "grad_norm": 3.5616772174835205,
      "learning_rate": 4.3480363530184685e-05,
      "loss": 0.726,
      "step": 857400
    },
    {
      "epoch": 7.824476239141543,
      "grad_norm": 3.8229825496673584,
      "learning_rate": 4.347960313404872e-05,
      "loss": 0.6513,
      "step": 857500
    },
    {
      "epoch": 7.825388714504708,
      "grad_norm": 3.6475656032562256,
      "learning_rate": 4.3478842737912745e-05,
      "loss": 0.703,
      "step": 857600
    },
    {
      "epoch": 7.826301189867873,
      "grad_norm": 4.817680835723877,
      "learning_rate": 4.3478082341776775e-05,
      "loss": 0.7228,
      "step": 857700
    },
    {
      "epoch": 7.827213665231039,
      "grad_norm": 3.7174758911132812,
      "learning_rate": 4.3477321945640805e-05,
      "loss": 0.6928,
      "step": 857800
    },
    {
      "epoch": 7.828126140594204,
      "grad_norm": 4.83469295501709,
      "learning_rate": 4.347656154950483e-05,
      "loss": 0.7315,
      "step": 857900
    },
    {
      "epoch": 7.829038615957369,
      "grad_norm": 3.4599924087524414,
      "learning_rate": 4.347580115336886e-05,
      "loss": 0.6918,
      "step": 858000
    },
    {
      "epoch": 7.829951091320535,
      "grad_norm": 4.0931243896484375,
      "learning_rate": 4.347504075723289e-05,
      "loss": 0.7148,
      "step": 858100
    },
    {
      "epoch": 7.8308635666837,
      "grad_norm": 4.361287593841553,
      "learning_rate": 4.347428036109692e-05,
      "loss": 0.7081,
      "step": 858200
    },
    {
      "epoch": 7.831776042046865,
      "grad_norm": 4.244421005249023,
      "learning_rate": 4.347351996496095e-05,
      "loss": 0.746,
      "step": 858300
    },
    {
      "epoch": 7.83268851741003,
      "grad_norm": 4.782170295715332,
      "learning_rate": 4.347275956882498e-05,
      "loss": 0.6756,
      "step": 858400
    },
    {
      "epoch": 7.833600992773195,
      "grad_norm": 3.6980743408203125,
      "learning_rate": 4.3471999172689e-05,
      "loss": 0.6932,
      "step": 858500
    },
    {
      "epoch": 7.83451346813636,
      "grad_norm": 4.378568649291992,
      "learning_rate": 4.347123877655304e-05,
      "loss": 0.6944,
      "step": 858600
    },
    {
      "epoch": 7.8354259434995255,
      "grad_norm": 3.852241039276123,
      "learning_rate": 4.347047838041706e-05,
      "loss": 0.7179,
      "step": 858700
    },
    {
      "epoch": 7.836338418862691,
      "grad_norm": 4.023504734039307,
      "learning_rate": 4.346971798428109e-05,
      "loss": 0.6623,
      "step": 858800
    },
    {
      "epoch": 7.837250894225856,
      "grad_norm": 4.339041233062744,
      "learning_rate": 4.346895758814512e-05,
      "loss": 0.7437,
      "step": 858900
    },
    {
      "epoch": 7.838163369589021,
      "grad_norm": 4.250088691711426,
      "learning_rate": 4.346819719200915e-05,
      "loss": 0.6915,
      "step": 859000
    },
    {
      "epoch": 7.839075844952187,
      "grad_norm": 3.784695863723755,
      "learning_rate": 4.3467436795873175e-05,
      "loss": 0.7242,
      "step": 859100
    },
    {
      "epoch": 7.839988320315351,
      "grad_norm": 3.3948240280151367,
      "learning_rate": 4.346667639973721e-05,
      "loss": 0.7079,
      "step": 859200
    },
    {
      "epoch": 7.840900795678516,
      "grad_norm": 3.016991138458252,
      "learning_rate": 4.3465916003601236e-05,
      "loss": 0.733,
      "step": 859300
    },
    {
      "epoch": 7.841813271041682,
      "grad_norm": 3.6754753589630127,
      "learning_rate": 4.3465155607465266e-05,
      "loss": 0.7132,
      "step": 859400
    },
    {
      "epoch": 7.842725746404847,
      "grad_norm": 3.4645180702209473,
      "learning_rate": 4.3464395211329296e-05,
      "loss": 0.7172,
      "step": 859500
    },
    {
      "epoch": 7.843638221768012,
      "grad_norm": 3.0976171493530273,
      "learning_rate": 4.3463634815193326e-05,
      "loss": 0.6852,
      "step": 859600
    },
    {
      "epoch": 7.844550697131178,
      "grad_norm": 4.506588459014893,
      "learning_rate": 4.3462874419057356e-05,
      "loss": 0.7137,
      "step": 859700
    },
    {
      "epoch": 7.845463172494343,
      "grad_norm": 5.38510799407959,
      "learning_rate": 4.3462114022921386e-05,
      "loss": 0.6972,
      "step": 859800
    },
    {
      "epoch": 7.846375647857508,
      "grad_norm": 3.7222747802734375,
      "learning_rate": 4.346135362678541e-05,
      "loss": 0.6568,
      "step": 859900
    },
    {
      "epoch": 7.8472881232206735,
      "grad_norm": 4.7902374267578125,
      "learning_rate": 4.3460593230649446e-05,
      "loss": 0.6809,
      "step": 860000
    },
    {
      "epoch": 7.848200598583838,
      "grad_norm": 3.957688093185425,
      "learning_rate": 4.345983283451347e-05,
      "loss": 0.7191,
      "step": 860100
    },
    {
      "epoch": 7.849113073947003,
      "grad_norm": 4.420200347900391,
      "learning_rate": 4.34590724383775e-05,
      "loss": 0.6874,
      "step": 860200
    },
    {
      "epoch": 7.8500255493101685,
      "grad_norm": 3.609529733657837,
      "learning_rate": 4.345831204224153e-05,
      "loss": 0.7001,
      "step": 860300
    },
    {
      "epoch": 7.850938024673334,
      "grad_norm": 4.293417453765869,
      "learning_rate": 4.345755164610556e-05,
      "loss": 0.6932,
      "step": 860400
    },
    {
      "epoch": 7.851850500036499,
      "grad_norm": 2.8831512928009033,
      "learning_rate": 4.345679124996958e-05,
      "loss": 0.7113,
      "step": 860500
    },
    {
      "epoch": 7.852762975399664,
      "grad_norm": 4.7625651359558105,
      "learning_rate": 4.345603085383361e-05,
      "loss": 0.676,
      "step": 860600
    },
    {
      "epoch": 7.85367545076283,
      "grad_norm": 4.027315616607666,
      "learning_rate": 4.345527045769764e-05,
      "loss": 0.6756,
      "step": 860700
    },
    {
      "epoch": 7.854587926125994,
      "grad_norm": 3.8554649353027344,
      "learning_rate": 4.345451006156167e-05,
      "loss": 0.7103,
      "step": 860800
    },
    {
      "epoch": 7.855500401489159,
      "grad_norm": 4.872337818145752,
      "learning_rate": 4.34537496654257e-05,
      "loss": 0.6858,
      "step": 860900
    },
    {
      "epoch": 7.856412876852325,
      "grad_norm": 3.257814645767212,
      "learning_rate": 4.3452989269289726e-05,
      "loss": 0.6884,
      "step": 861000
    },
    {
      "epoch": 7.85732535221549,
      "grad_norm": 2.739409923553467,
      "learning_rate": 4.345222887315376e-05,
      "loss": 0.6812,
      "step": 861100
    },
    {
      "epoch": 7.858237827578655,
      "grad_norm": 4.0925750732421875,
      "learning_rate": 4.3451468477017787e-05,
      "loss": 0.6955,
      "step": 861200
    },
    {
      "epoch": 7.859150302941821,
      "grad_norm": 3.9577221870422363,
      "learning_rate": 4.3450708080881817e-05,
      "loss": 0.7279,
      "step": 861300
    },
    {
      "epoch": 7.860062778304986,
      "grad_norm": 3.0375912189483643,
      "learning_rate": 4.344994768474585e-05,
      "loss": 0.7177,
      "step": 861400
    },
    {
      "epoch": 7.860975253668151,
      "grad_norm": 4.1426191329956055,
      "learning_rate": 4.344918728860988e-05,
      "loss": 0.6688,
      "step": 861500
    },
    {
      "epoch": 7.8618877290313165,
      "grad_norm": 4.250489234924316,
      "learning_rate": 4.344842689247391e-05,
      "loss": 0.6727,
      "step": 861600
    },
    {
      "epoch": 7.862800204394482,
      "grad_norm": 3.601443290710449,
      "learning_rate": 4.344766649633794e-05,
      "loss": 0.7214,
      "step": 861700
    },
    {
      "epoch": 7.863712679757646,
      "grad_norm": 3.070139169692993,
      "learning_rate": 4.344690610020196e-05,
      "loss": 0.6943,
      "step": 861800
    },
    {
      "epoch": 7.8646251551208115,
      "grad_norm": 3.5192322731018066,
      "learning_rate": 4.344614570406599e-05,
      "loss": 0.6894,
      "step": 861900
    },
    {
      "epoch": 7.865537630483977,
      "grad_norm": 4.196908473968506,
      "learning_rate": 4.344538530793002e-05,
      "loss": 0.7639,
      "step": 862000
    },
    {
      "epoch": 7.866450105847142,
      "grad_norm": 4.027012348175049,
      "learning_rate": 4.344462491179405e-05,
      "loss": 0.6691,
      "step": 862100
    },
    {
      "epoch": 7.867362581210307,
      "grad_norm": 3.634298324584961,
      "learning_rate": 4.344386451565808e-05,
      "loss": 0.6818,
      "step": 862200
    },
    {
      "epoch": 7.868275056573473,
      "grad_norm": 3.933680534362793,
      "learning_rate": 4.344310411952211e-05,
      "loss": 0.7113,
      "step": 862300
    },
    {
      "epoch": 7.869187531936638,
      "grad_norm": 4.065258979797363,
      "learning_rate": 4.3442343723386134e-05,
      "loss": 0.714,
      "step": 862400
    },
    {
      "epoch": 7.870100007299802,
      "grad_norm": 5.180586338043213,
      "learning_rate": 4.344158332725017e-05,
      "loss": 0.6675,
      "step": 862500
    },
    {
      "epoch": 7.871012482662968,
      "grad_norm": 4.336562633514404,
      "learning_rate": 4.3440822931114194e-05,
      "loss": 0.684,
      "step": 862600
    },
    {
      "epoch": 7.871924958026133,
      "grad_norm": 4.204392910003662,
      "learning_rate": 4.3440062534978224e-05,
      "loss": 0.7055,
      "step": 862700
    },
    {
      "epoch": 7.872837433389298,
      "grad_norm": 4.382106304168701,
      "learning_rate": 4.3439302138842254e-05,
      "loss": 0.6925,
      "step": 862800
    },
    {
      "epoch": 7.873749908752464,
      "grad_norm": 3.4005658626556396,
      "learning_rate": 4.3438541742706284e-05,
      "loss": 0.6788,
      "step": 862900
    },
    {
      "epoch": 7.874662384115629,
      "grad_norm": 3.4492101669311523,
      "learning_rate": 4.3437781346570314e-05,
      "loss": 0.6834,
      "step": 863000
    },
    {
      "epoch": 7.875574859478794,
      "grad_norm": 4.628288745880127,
      "learning_rate": 4.3437020950434344e-05,
      "loss": 0.6472,
      "step": 863100
    },
    {
      "epoch": 7.8764873348419595,
      "grad_norm": 3.0348703861236572,
      "learning_rate": 4.343626055429837e-05,
      "loss": 0.7072,
      "step": 863200
    },
    {
      "epoch": 7.877399810205125,
      "grad_norm": 3.5966954231262207,
      "learning_rate": 4.3435500158162404e-05,
      "loss": 0.6883,
      "step": 863300
    },
    {
      "epoch": 7.87831228556829,
      "grad_norm": 3.992166042327881,
      "learning_rate": 4.343473976202643e-05,
      "loss": 0.675,
      "step": 863400
    },
    {
      "epoch": 7.8792247609314545,
      "grad_norm": 3.226351022720337,
      "learning_rate": 4.343397936589045e-05,
      "loss": 0.7288,
      "step": 863500
    },
    {
      "epoch": 7.88013723629462,
      "grad_norm": 3.369819164276123,
      "learning_rate": 4.343321896975449e-05,
      "loss": 0.6866,
      "step": 863600
    },
    {
      "epoch": 7.881049711657785,
      "grad_norm": 3.2516705989837646,
      "learning_rate": 4.343245857361851e-05,
      "loss": 0.6823,
      "step": 863700
    },
    {
      "epoch": 7.88196218702095,
      "grad_norm": 3.0044312477111816,
      "learning_rate": 4.343169817748254e-05,
      "loss": 0.7001,
      "step": 863800
    },
    {
      "epoch": 7.882874662384116,
      "grad_norm": 4.9921698570251465,
      "learning_rate": 4.343093778134657e-05,
      "loss": 0.696,
      "step": 863900
    },
    {
      "epoch": 7.883787137747281,
      "grad_norm": 3.587230682373047,
      "learning_rate": 4.34301773852106e-05,
      "loss": 0.6511,
      "step": 864000
    },
    {
      "epoch": 7.884699613110446,
      "grad_norm": 4.576704025268555,
      "learning_rate": 4.342941698907463e-05,
      "loss": 0.7092,
      "step": 864100
    },
    {
      "epoch": 7.885612088473611,
      "grad_norm": 4.047885894775391,
      "learning_rate": 4.342865659293866e-05,
      "loss": 0.7391,
      "step": 864200
    },
    {
      "epoch": 7.886524563836776,
      "grad_norm": 3.9760327339172363,
      "learning_rate": 4.3427896196802685e-05,
      "loss": 0.6921,
      "step": 864300
    },
    {
      "epoch": 7.887437039199941,
      "grad_norm": 3.689317226409912,
      "learning_rate": 4.342713580066672e-05,
      "loss": 0.6992,
      "step": 864400
    },
    {
      "epoch": 7.888349514563107,
      "grad_norm": 4.039359092712402,
      "learning_rate": 4.3426375404530745e-05,
      "loss": 0.6688,
      "step": 864500
    },
    {
      "epoch": 7.889261989926272,
      "grad_norm": 3.9917993545532227,
      "learning_rate": 4.3425615008394775e-05,
      "loss": 0.6736,
      "step": 864600
    },
    {
      "epoch": 7.890174465289437,
      "grad_norm": 4.382320880889893,
      "learning_rate": 4.3424854612258805e-05,
      "loss": 0.7167,
      "step": 864700
    },
    {
      "epoch": 7.8910869406526025,
      "grad_norm": 4.02768087387085,
      "learning_rate": 4.3424094216122835e-05,
      "loss": 0.7049,
      "step": 864800
    },
    {
      "epoch": 7.891999416015768,
      "grad_norm": 4.3116278648376465,
      "learning_rate": 4.342333381998686e-05,
      "loss": 0.6966,
      "step": 864900
    },
    {
      "epoch": 7.892911891378933,
      "grad_norm": 3.7664082050323486,
      "learning_rate": 4.3422573423850895e-05,
      "loss": 0.6907,
      "step": 865000
    },
    {
      "epoch": 7.893824366742098,
      "grad_norm": 4.306000709533691,
      "learning_rate": 4.342181302771492e-05,
      "loss": 0.7156,
      "step": 865100
    },
    {
      "epoch": 7.894736842105263,
      "grad_norm": 3.5938572883605957,
      "learning_rate": 4.342105263157895e-05,
      "loss": 0.6947,
      "step": 865200
    },
    {
      "epoch": 7.895649317468428,
      "grad_norm": 4.055775165557861,
      "learning_rate": 4.342029223544298e-05,
      "loss": 0.6632,
      "step": 865300
    },
    {
      "epoch": 7.896561792831593,
      "grad_norm": 2.7179293632507324,
      "learning_rate": 4.341953183930701e-05,
      "loss": 0.6645,
      "step": 865400
    },
    {
      "epoch": 7.897474268194759,
      "grad_norm": 4.585500240325928,
      "learning_rate": 4.341877144317104e-05,
      "loss": 0.6669,
      "step": 865500
    },
    {
      "epoch": 7.898386743557924,
      "grad_norm": 4.93878173828125,
      "learning_rate": 4.341801104703507e-05,
      "loss": 0.6957,
      "step": 865600
    },
    {
      "epoch": 7.899299218921089,
      "grad_norm": 4.576260089874268,
      "learning_rate": 4.341725065089909e-05,
      "loss": 0.7007,
      "step": 865700
    },
    {
      "epoch": 7.900211694284255,
      "grad_norm": 4.035399436950684,
      "learning_rate": 4.341649025476313e-05,
      "loss": 0.7176,
      "step": 865800
    },
    {
      "epoch": 7.901124169647419,
      "grad_norm": 3.2370986938476562,
      "learning_rate": 4.341572985862715e-05,
      "loss": 0.6682,
      "step": 865900
    },
    {
      "epoch": 7.902036645010584,
      "grad_norm": 3.6984243392944336,
      "learning_rate": 4.341496946249118e-05,
      "loss": 0.6809,
      "step": 866000
    },
    {
      "epoch": 7.90294912037375,
      "grad_norm": 3.4616644382476807,
      "learning_rate": 4.341420906635521e-05,
      "loss": 0.7523,
      "step": 866100
    },
    {
      "epoch": 7.903861595736915,
      "grad_norm": 3.734602451324463,
      "learning_rate": 4.341344867021924e-05,
      "loss": 0.6885,
      "step": 866200
    },
    {
      "epoch": 7.90477407110008,
      "grad_norm": 3.831331253051758,
      "learning_rate": 4.3412688274083266e-05,
      "loss": 0.6976,
      "step": 866300
    },
    {
      "epoch": 7.9056865464632455,
      "grad_norm": 3.0695319175720215,
      "learning_rate": 4.3411927877947296e-05,
      "loss": 0.6908,
      "step": 866400
    },
    {
      "epoch": 7.906599021826411,
      "grad_norm": 3.692765474319458,
      "learning_rate": 4.3411167481811326e-05,
      "loss": 0.6907,
      "step": 866500
    },
    {
      "epoch": 7.907511497189576,
      "grad_norm": 4.401780128479004,
      "learning_rate": 4.3410407085675356e-05,
      "loss": 0.6443,
      "step": 866600
    },
    {
      "epoch": 7.908423972552741,
      "grad_norm": 4.643414497375488,
      "learning_rate": 4.3409646689539386e-05,
      "loss": 0.7066,
      "step": 866700
    },
    {
      "epoch": 7.909336447915907,
      "grad_norm": 2.655587673187256,
      "learning_rate": 4.340888629340341e-05,
      "loss": 0.6771,
      "step": 866800
    },
    {
      "epoch": 7.910248923279071,
      "grad_norm": 3.9567294120788574,
      "learning_rate": 4.3408125897267446e-05,
      "loss": 0.6749,
      "step": 866900
    },
    {
      "epoch": 7.911161398642236,
      "grad_norm": 4.482800006866455,
      "learning_rate": 4.340736550113147e-05,
      "loss": 0.7117,
      "step": 867000
    },
    {
      "epoch": 7.912073874005402,
      "grad_norm": 3.3415071964263916,
      "learning_rate": 4.34066051049955e-05,
      "loss": 0.6804,
      "step": 867100
    },
    {
      "epoch": 7.912986349368567,
      "grad_norm": 3.9441092014312744,
      "learning_rate": 4.340584470885953e-05,
      "loss": 0.6975,
      "step": 867200
    },
    {
      "epoch": 7.913898824731732,
      "grad_norm": 3.7106707096099854,
      "learning_rate": 4.340508431272356e-05,
      "loss": 0.6958,
      "step": 867300
    },
    {
      "epoch": 7.914811300094898,
      "grad_norm": 4.398614406585693,
      "learning_rate": 4.340432391658758e-05,
      "loss": 0.6793,
      "step": 867400
    },
    {
      "epoch": 7.915723775458063,
      "grad_norm": 3.641056776046753,
      "learning_rate": 4.340356352045162e-05,
      "loss": 0.6719,
      "step": 867500
    },
    {
      "epoch": 7.916636250821227,
      "grad_norm": 3.4827537536621094,
      "learning_rate": 4.340280312431564e-05,
      "loss": 0.6367,
      "step": 867600
    },
    {
      "epoch": 7.917548726184393,
      "grad_norm": 4.0364089012146,
      "learning_rate": 4.340204272817967e-05,
      "loss": 0.6943,
      "step": 867700
    },
    {
      "epoch": 7.918461201547558,
      "grad_norm": 3.2435288429260254,
      "learning_rate": 4.34012823320437e-05,
      "loss": 0.6796,
      "step": 867800
    },
    {
      "epoch": 7.919373676910723,
      "grad_norm": 5.067698001861572,
      "learning_rate": 4.340052193590773e-05,
      "loss": 0.7071,
      "step": 867900
    },
    {
      "epoch": 7.9202861522738885,
      "grad_norm": 4.308900356292725,
      "learning_rate": 4.339976153977176e-05,
      "loss": 0.7003,
      "step": 868000
    },
    {
      "epoch": 7.921198627637054,
      "grad_norm": 3.991194009780884,
      "learning_rate": 4.3399001143635793e-05,
      "loss": 0.7041,
      "step": 868100
    },
    {
      "epoch": 7.922111103000219,
      "grad_norm": 4.091464042663574,
      "learning_rate": 4.339824074749982e-05,
      "loss": 0.6972,
      "step": 868200
    },
    {
      "epoch": 7.9230235783633844,
      "grad_norm": 4.2221832275390625,
      "learning_rate": 4.3397480351363854e-05,
      "loss": 0.6834,
      "step": 868300
    },
    {
      "epoch": 7.92393605372655,
      "grad_norm": 3.9187965393066406,
      "learning_rate": 4.339671995522788e-05,
      "loss": 0.7141,
      "step": 868400
    },
    {
      "epoch": 7.924848529089715,
      "grad_norm": 4.159482479095459,
      "learning_rate": 4.339595955909191e-05,
      "loss": 0.7027,
      "step": 868500
    },
    {
      "epoch": 7.9257610044528795,
      "grad_norm": 4.4235334396362305,
      "learning_rate": 4.339519916295594e-05,
      "loss": 0.694,
      "step": 868600
    },
    {
      "epoch": 7.926673479816045,
      "grad_norm": 2.959911584854126,
      "learning_rate": 4.339443876681997e-05,
      "loss": 0.7289,
      "step": 868700
    },
    {
      "epoch": 7.92758595517921,
      "grad_norm": 3.785498857498169,
      "learning_rate": 4.339367837068399e-05,
      "loss": 0.7128,
      "step": 868800
    },
    {
      "epoch": 7.928498430542375,
      "grad_norm": 4.038228511810303,
      "learning_rate": 4.339291797454803e-05,
      "loss": 0.6871,
      "step": 868900
    },
    {
      "epoch": 7.929410905905541,
      "grad_norm": 3.2943334579467773,
      "learning_rate": 4.339215757841205e-05,
      "loss": 0.7266,
      "step": 869000
    },
    {
      "epoch": 7.930323381268706,
      "grad_norm": 3.400674819946289,
      "learning_rate": 4.339139718227608e-05,
      "loss": 0.6827,
      "step": 869100
    },
    {
      "epoch": 7.931235856631871,
      "grad_norm": 4.088144302368164,
      "learning_rate": 4.339063678614011e-05,
      "loss": 0.6539,
      "step": 869200
    },
    {
      "epoch": 7.932148331995036,
      "grad_norm": 3.713918447494507,
      "learning_rate": 4.3389876390004134e-05,
      "loss": 0.6623,
      "step": 869300
    },
    {
      "epoch": 7.933060807358201,
      "grad_norm": 4.874606609344482,
      "learning_rate": 4.338911599386817e-05,
      "loss": 0.7086,
      "step": 869400
    },
    {
      "epoch": 7.933973282721366,
      "grad_norm": 4.581846714019775,
      "learning_rate": 4.3388355597732194e-05,
      "loss": 0.7116,
      "step": 869500
    },
    {
      "epoch": 7.934885758084532,
      "grad_norm": 5.607998371124268,
      "learning_rate": 4.3387595201596224e-05,
      "loss": 0.7122,
      "step": 869600
    },
    {
      "epoch": 7.935798233447697,
      "grad_norm": 4.214158535003662,
      "learning_rate": 4.3386834805460254e-05,
      "loss": 0.6869,
      "step": 869700
    },
    {
      "epoch": 7.936710708810862,
      "grad_norm": 4.154017925262451,
      "learning_rate": 4.3386074409324284e-05,
      "loss": 0.6849,
      "step": 869800
    },
    {
      "epoch": 7.9376231841740275,
      "grad_norm": 3.2581844329833984,
      "learning_rate": 4.338531401318831e-05,
      "loss": 0.6951,
      "step": 869900
    },
    {
      "epoch": 7.938535659537193,
      "grad_norm": 2.6630325317382812,
      "learning_rate": 4.3384553617052344e-05,
      "loss": 0.7192,
      "step": 870000
    },
    {
      "epoch": 7.939448134900358,
      "grad_norm": 4.273288249969482,
      "learning_rate": 4.338379322091637e-05,
      "loss": 0.7,
      "step": 870100
    },
    {
      "epoch": 7.940360610263523,
      "grad_norm": 3.617541790008545,
      "learning_rate": 4.33830328247804e-05,
      "loss": 0.7087,
      "step": 870200
    },
    {
      "epoch": 7.941273085626688,
      "grad_norm": 4.746089458465576,
      "learning_rate": 4.338227242864443e-05,
      "loss": 0.6706,
      "step": 870300
    },
    {
      "epoch": 7.942185560989853,
      "grad_norm": 2.5388424396514893,
      "learning_rate": 4.338151203250846e-05,
      "loss": 0.7048,
      "step": 870400
    },
    {
      "epoch": 7.943098036353018,
      "grad_norm": 3.64139723777771,
      "learning_rate": 4.338075163637249e-05,
      "loss": 0.6685,
      "step": 870500
    },
    {
      "epoch": 7.944010511716184,
      "grad_norm": 4.362249374389648,
      "learning_rate": 4.337999124023652e-05,
      "loss": 0.7148,
      "step": 870600
    },
    {
      "epoch": 7.944922987079349,
      "grad_norm": 3.746586322784424,
      "learning_rate": 4.337923084410054e-05,
      "loss": 0.6977,
      "step": 870700
    },
    {
      "epoch": 7.945835462442514,
      "grad_norm": 4.129938125610352,
      "learning_rate": 4.337847044796458e-05,
      "loss": 0.6583,
      "step": 870800
    },
    {
      "epoch": 7.94674793780568,
      "grad_norm": 4.183487415313721,
      "learning_rate": 4.33777100518286e-05,
      "loss": 0.6761,
      "step": 870900
    },
    {
      "epoch": 7.947660413168844,
      "grad_norm": 3.749455690383911,
      "learning_rate": 4.337694965569263e-05,
      "loss": 0.6565,
      "step": 871000
    },
    {
      "epoch": 7.948572888532009,
      "grad_norm": 4.651016712188721,
      "learning_rate": 4.337618925955666e-05,
      "loss": 0.6904,
      "step": 871100
    },
    {
      "epoch": 7.949485363895175,
      "grad_norm": 3.860997438430786,
      "learning_rate": 4.337542886342069e-05,
      "loss": 0.722,
      "step": 871200
    },
    {
      "epoch": 7.95039783925834,
      "grad_norm": 4.566565036773682,
      "learning_rate": 4.3374668467284715e-05,
      "loss": 0.6779,
      "step": 871300
    },
    {
      "epoch": 7.951310314621505,
      "grad_norm": 4.7542595863342285,
      "learning_rate": 4.337390807114875e-05,
      "loss": 0.716,
      "step": 871400
    },
    {
      "epoch": 7.9522227899846705,
      "grad_norm": 4.718204498291016,
      "learning_rate": 4.3373147675012775e-05,
      "loss": 0.6624,
      "step": 871500
    },
    {
      "epoch": 7.953135265347836,
      "grad_norm": 3.6437020301818848,
      "learning_rate": 4.3372387278876805e-05,
      "loss": 0.6882,
      "step": 871600
    },
    {
      "epoch": 7.954047740711001,
      "grad_norm": 4.534060955047607,
      "learning_rate": 4.3371626882740835e-05,
      "loss": 0.7311,
      "step": 871700
    },
    {
      "epoch": 7.954960216074166,
      "grad_norm": 3.5692615509033203,
      "learning_rate": 4.3370866486604865e-05,
      "loss": 0.6898,
      "step": 871800
    },
    {
      "epoch": 7.955872691437332,
      "grad_norm": 2.25280499458313,
      "learning_rate": 4.3370106090468895e-05,
      "loss": 0.6655,
      "step": 871900
    },
    {
      "epoch": 7.956785166800496,
      "grad_norm": 4.0848188400268555,
      "learning_rate": 4.336934569433292e-05,
      "loss": 0.6703,
      "step": 872000
    },
    {
      "epoch": 7.957697642163661,
      "grad_norm": 3.2952089309692383,
      "learning_rate": 4.336858529819695e-05,
      "loss": 0.7255,
      "step": 872100
    },
    {
      "epoch": 7.958610117526827,
      "grad_norm": 3.7512404918670654,
      "learning_rate": 4.336782490206098e-05,
      "loss": 0.6989,
      "step": 872200
    },
    {
      "epoch": 7.959522592889992,
      "grad_norm": 3.2835581302642822,
      "learning_rate": 4.336706450592501e-05,
      "loss": 0.6937,
      "step": 872300
    },
    {
      "epoch": 7.960435068253157,
      "grad_norm": 3.0631871223449707,
      "learning_rate": 4.336630410978903e-05,
      "loss": 0.6474,
      "step": 872400
    },
    {
      "epoch": 7.961347543616323,
      "grad_norm": 5.513552665710449,
      "learning_rate": 4.336554371365307e-05,
      "loss": 0.6979,
      "step": 872500
    },
    {
      "epoch": 7.962260018979488,
      "grad_norm": 3.8734700679779053,
      "learning_rate": 4.336478331751709e-05,
      "loss": 0.6806,
      "step": 872600
    },
    {
      "epoch": 7.963172494342652,
      "grad_norm": 5.11903190612793,
      "learning_rate": 4.336402292138112e-05,
      "loss": 0.6866,
      "step": 872700
    },
    {
      "epoch": 7.964084969705818,
      "grad_norm": 3.9259393215179443,
      "learning_rate": 4.336326252524515e-05,
      "loss": 0.7094,
      "step": 872800
    },
    {
      "epoch": 7.964997445068983,
      "grad_norm": 3.6486432552337646,
      "learning_rate": 4.336250212910918e-05,
      "loss": 0.6916,
      "step": 872900
    },
    {
      "epoch": 7.965909920432148,
      "grad_norm": 4.771013259887695,
      "learning_rate": 4.336174173297321e-05,
      "loss": 0.7016,
      "step": 873000
    },
    {
      "epoch": 7.9668223957953135,
      "grad_norm": 3.9559688568115234,
      "learning_rate": 4.336098133683724e-05,
      "loss": 0.6726,
      "step": 873100
    },
    {
      "epoch": 7.967734871158479,
      "grad_norm": 4.46288537979126,
      "learning_rate": 4.3360220940701266e-05,
      "loss": 0.7353,
      "step": 873200
    },
    {
      "epoch": 7.968647346521644,
      "grad_norm": 3.907543182373047,
      "learning_rate": 4.33594605445653e-05,
      "loss": 0.7117,
      "step": 873300
    },
    {
      "epoch": 7.969559821884809,
      "grad_norm": 3.411651372909546,
      "learning_rate": 4.3358700148429326e-05,
      "loss": 0.6705,
      "step": 873400
    },
    {
      "epoch": 7.970472297247975,
      "grad_norm": 3.3620312213897705,
      "learning_rate": 4.3357939752293356e-05,
      "loss": 0.7276,
      "step": 873500
    },
    {
      "epoch": 7.97138477261114,
      "grad_norm": 3.843477249145508,
      "learning_rate": 4.3357179356157386e-05,
      "loss": 0.6758,
      "step": 873600
    },
    {
      "epoch": 7.972297247974304,
      "grad_norm": 3.8519983291625977,
      "learning_rate": 4.3356418960021416e-05,
      "loss": 0.6876,
      "step": 873700
    },
    {
      "epoch": 7.97320972333747,
      "grad_norm": 3.9106101989746094,
      "learning_rate": 4.3355658563885446e-05,
      "loss": 0.6564,
      "step": 873800
    },
    {
      "epoch": 7.974122198700635,
      "grad_norm": 3.9118096828460693,
      "learning_rate": 4.3354898167749476e-05,
      "loss": 0.7271,
      "step": 873900
    },
    {
      "epoch": 7.9750346740638,
      "grad_norm": 4.199712753295898,
      "learning_rate": 4.33541377716135e-05,
      "loss": 0.6984,
      "step": 874000
    },
    {
      "epoch": 7.975947149426966,
      "grad_norm": 4.224099159240723,
      "learning_rate": 4.335337737547753e-05,
      "loss": 0.7406,
      "step": 874100
    },
    {
      "epoch": 7.976859624790131,
      "grad_norm": 3.848785877227783,
      "learning_rate": 4.335261697934156e-05,
      "loss": 0.7133,
      "step": 874200
    },
    {
      "epoch": 7.977772100153296,
      "grad_norm": 4.614713191986084,
      "learning_rate": 4.335185658320559e-05,
      "loss": 0.6909,
      "step": 874300
    },
    {
      "epoch": 7.978684575516461,
      "grad_norm": 4.045093536376953,
      "learning_rate": 4.335109618706962e-05,
      "loss": 0.7021,
      "step": 874400
    },
    {
      "epoch": 7.979597050879626,
      "grad_norm": 2.941575765609741,
      "learning_rate": 4.335033579093365e-05,
      "loss": 0.7211,
      "step": 874500
    },
    {
      "epoch": 7.980509526242791,
      "grad_norm": 3.814073324203491,
      "learning_rate": 4.334957539479767e-05,
      "loss": 0.6889,
      "step": 874600
    },
    {
      "epoch": 7.9814220016059565,
      "grad_norm": 3.505443572998047,
      "learning_rate": 4.334881499866171e-05,
      "loss": 0.6989,
      "step": 874700
    },
    {
      "epoch": 7.982334476969122,
      "grad_norm": 4.517552852630615,
      "learning_rate": 4.334805460252573e-05,
      "loss": 0.7187,
      "step": 874800
    },
    {
      "epoch": 7.983246952332287,
      "grad_norm": 3.3017594814300537,
      "learning_rate": 4.3347294206389763e-05,
      "loss": 0.7083,
      "step": 874900
    },
    {
      "epoch": 7.984159427695452,
      "grad_norm": 4.8640875816345215,
      "learning_rate": 4.3346533810253794e-05,
      "loss": 0.6663,
      "step": 875000
    },
    {
      "epoch": 7.985071903058618,
      "grad_norm": 3.7033047676086426,
      "learning_rate": 4.334577341411782e-05,
      "loss": 0.6767,
      "step": 875100
    },
    {
      "epoch": 7.985984378421783,
      "grad_norm": 4.218296527862549,
      "learning_rate": 4.3345013017981854e-05,
      "loss": 0.7189,
      "step": 875200
    },
    {
      "epoch": 7.986896853784948,
      "grad_norm": 3.854884147644043,
      "learning_rate": 4.334425262184588e-05,
      "loss": 0.6972,
      "step": 875300
    },
    {
      "epoch": 7.987809329148113,
      "grad_norm": 4.805549144744873,
      "learning_rate": 4.334349222570991e-05,
      "loss": 0.6414,
      "step": 875400
    },
    {
      "epoch": 7.988721804511278,
      "grad_norm": 4.794344425201416,
      "learning_rate": 4.334273182957394e-05,
      "loss": 0.7169,
      "step": 875500
    },
    {
      "epoch": 7.989634279874443,
      "grad_norm": 4.80329704284668,
      "learning_rate": 4.334197143343797e-05,
      "loss": 0.688,
      "step": 875600
    },
    {
      "epoch": 7.990546755237609,
      "grad_norm": 3.9132447242736816,
      "learning_rate": 4.334121103730199e-05,
      "loss": 0.7395,
      "step": 875700
    },
    {
      "epoch": 7.991459230600774,
      "grad_norm": 3.6492092609405518,
      "learning_rate": 4.334045064116603e-05,
      "loss": 0.7276,
      "step": 875800
    },
    {
      "epoch": 7.992371705963939,
      "grad_norm": 2.9623749256134033,
      "learning_rate": 4.333969024503005e-05,
      "loss": 0.6874,
      "step": 875900
    },
    {
      "epoch": 7.9932841813271045,
      "grad_norm": 4.29524040222168,
      "learning_rate": 4.333892984889408e-05,
      "loss": 0.6567,
      "step": 876000
    },
    {
      "epoch": 7.994196656690269,
      "grad_norm": 4.22204065322876,
      "learning_rate": 4.333816945275811e-05,
      "loss": 0.6686,
      "step": 876100
    },
    {
      "epoch": 7.995109132053434,
      "grad_norm": 4.438220977783203,
      "learning_rate": 4.333740905662214e-05,
      "loss": 0.6481,
      "step": 876200
    },
    {
      "epoch": 7.9960216074165995,
      "grad_norm": 3.780810594558716,
      "learning_rate": 4.333664866048617e-05,
      "loss": 0.6757,
      "step": 876300
    },
    {
      "epoch": 7.996934082779765,
      "grad_norm": 3.3005223274230957,
      "learning_rate": 4.33358882643502e-05,
      "loss": 0.7208,
      "step": 876400
    },
    {
      "epoch": 7.99784655814293,
      "grad_norm": 4.103063583374023,
      "learning_rate": 4.3335127868214224e-05,
      "loss": 0.6872,
      "step": 876500
    },
    {
      "epoch": 7.998759033506095,
      "grad_norm": 4.763699531555176,
      "learning_rate": 4.333436747207826e-05,
      "loss": 0.6879,
      "step": 876600
    },
    {
      "epoch": 7.999671508869261,
      "grad_norm": 4.98311710357666,
      "learning_rate": 4.3333607075942284e-05,
      "loss": 0.6903,
      "step": 876700
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.5649169087409973,
      "eval_runtime": 25.823,
      "eval_samples_per_second": 223.406,
      "eval_steps_per_second": 223.406,
      "step": 876736
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.5456418991088867,
      "eval_runtime": 511.1512,
      "eval_samples_per_second": 214.402,
      "eval_steps_per_second": 214.402,
      "step": 876736
    },
    {
      "epoch": 8.000583984232426,
      "grad_norm": 3.9775190353393555,
      "learning_rate": 4.3332846679806314e-05,
      "loss": 0.7011,
      "step": 876800
    },
    {
      "epoch": 8.001496459595591,
      "grad_norm": 4.314237117767334,
      "learning_rate": 4.3332086283670344e-05,
      "loss": 0.7021,
      "step": 876900
    },
    {
      "epoch": 8.002408934958757,
      "grad_norm": 3.072526693344116,
      "learning_rate": 4.3331325887534375e-05,
      "loss": 0.6578,
      "step": 877000
    },
    {
      "epoch": 8.003321410321922,
      "grad_norm": 3.515641212463379,
      "learning_rate": 4.33305654913984e-05,
      "loss": 0.6822,
      "step": 877100
    },
    {
      "epoch": 8.004233885685087,
      "grad_norm": 4.26830530166626,
      "learning_rate": 4.3329805095262435e-05,
      "loss": 0.6969,
      "step": 877200
    },
    {
      "epoch": 8.005146361048253,
      "grad_norm": 4.9181718826293945,
      "learning_rate": 4.332904469912646e-05,
      "loss": 0.6963,
      "step": 877300
    },
    {
      "epoch": 8.006058836411416,
      "grad_norm": 3.2730917930603027,
      "learning_rate": 4.332828430299049e-05,
      "loss": 0.6925,
      "step": 877400
    },
    {
      "epoch": 8.006971311774581,
      "grad_norm": 4.1408491134643555,
      "learning_rate": 4.332752390685452e-05,
      "loss": 0.6831,
      "step": 877500
    },
    {
      "epoch": 8.007883787137747,
      "grad_norm": 4.319575786590576,
      "learning_rate": 4.332676351071855e-05,
      "loss": 0.7176,
      "step": 877600
    },
    {
      "epoch": 8.008796262500912,
      "grad_norm": 3.6790523529052734,
      "learning_rate": 4.332600311458258e-05,
      "loss": 0.6549,
      "step": 877700
    },
    {
      "epoch": 8.009708737864077,
      "grad_norm": 4.936703681945801,
      "learning_rate": 4.33252427184466e-05,
      "loss": 0.6804,
      "step": 877800
    },
    {
      "epoch": 8.010621213227243,
      "grad_norm": 3.0759313106536865,
      "learning_rate": 4.332448232231063e-05,
      "loss": 0.702,
      "step": 877900
    },
    {
      "epoch": 8.011533688590408,
      "grad_norm": 3.6406705379486084,
      "learning_rate": 4.332372192617466e-05,
      "loss": 0.6134,
      "step": 878000
    },
    {
      "epoch": 8.012446163953573,
      "grad_norm": 4.199434280395508,
      "learning_rate": 4.332296153003869e-05,
      "loss": 0.6729,
      "step": 878100
    },
    {
      "epoch": 8.013358639316738,
      "grad_norm": 3.1209604740142822,
      "learning_rate": 4.3322201133902715e-05,
      "loss": 0.68,
      "step": 878200
    },
    {
      "epoch": 8.014271114679904,
      "grad_norm": 4.334529876708984,
      "learning_rate": 4.332144073776675e-05,
      "loss": 0.7087,
      "step": 878300
    },
    {
      "epoch": 8.015183590043069,
      "grad_norm": 3.974224805831909,
      "learning_rate": 4.3320680341630775e-05,
      "loss": 0.7176,
      "step": 878400
    },
    {
      "epoch": 8.016096065406234,
      "grad_norm": 3.5769965648651123,
      "learning_rate": 4.3319919945494805e-05,
      "loss": 0.7314,
      "step": 878500
    },
    {
      "epoch": 8.0170085407694,
      "grad_norm": 3.680960178375244,
      "learning_rate": 4.3319159549358835e-05,
      "loss": 0.685,
      "step": 878600
    },
    {
      "epoch": 8.017921016132565,
      "grad_norm": 4.218897342681885,
      "learning_rate": 4.3318399153222865e-05,
      "loss": 0.6587,
      "step": 878700
    },
    {
      "epoch": 8.01883349149573,
      "grad_norm": 4.122633934020996,
      "learning_rate": 4.3317638757086895e-05,
      "loss": 0.6994,
      "step": 878800
    },
    {
      "epoch": 8.019745966858896,
      "grad_norm": 3.2704017162323,
      "learning_rate": 4.3316878360950925e-05,
      "loss": 0.685,
      "step": 878900
    },
    {
      "epoch": 8.02065844222206,
      "grad_norm": 4.322717189788818,
      "learning_rate": 4.331611796481495e-05,
      "loss": 0.6938,
      "step": 879000
    },
    {
      "epoch": 8.021570917585224,
      "grad_norm": 3.7835934162139893,
      "learning_rate": 4.3315357568678986e-05,
      "loss": 0.6685,
      "step": 879100
    },
    {
      "epoch": 8.02248339294839,
      "grad_norm": 4.263713836669922,
      "learning_rate": 4.331459717254301e-05,
      "loss": 0.6983,
      "step": 879200
    },
    {
      "epoch": 8.023395868311555,
      "grad_norm": 3.3174548149108887,
      "learning_rate": 4.331383677640704e-05,
      "loss": 0.702,
      "step": 879300
    },
    {
      "epoch": 8.02430834367472,
      "grad_norm": 4.73351526260376,
      "learning_rate": 4.331307638027107e-05,
      "loss": 0.6861,
      "step": 879400
    },
    {
      "epoch": 8.025220819037886,
      "grad_norm": 3.7541821002960205,
      "learning_rate": 4.33123159841351e-05,
      "loss": 0.7092,
      "step": 879500
    },
    {
      "epoch": 8.02613329440105,
      "grad_norm": 3.277181625366211,
      "learning_rate": 4.331155558799912e-05,
      "loss": 0.6914,
      "step": 879600
    },
    {
      "epoch": 8.027045769764216,
      "grad_norm": 4.35828161239624,
      "learning_rate": 4.331079519186316e-05,
      "loss": 0.6748,
      "step": 879700
    },
    {
      "epoch": 8.027958245127381,
      "grad_norm": 3.4904863834381104,
      "learning_rate": 4.331003479572718e-05,
      "loss": 0.669,
      "step": 879800
    },
    {
      "epoch": 8.028870720490547,
      "grad_norm": 3.6204140186309814,
      "learning_rate": 4.330927439959121e-05,
      "loss": 0.6883,
      "step": 879900
    },
    {
      "epoch": 8.029783195853712,
      "grad_norm": 3.6118271350860596,
      "learning_rate": 4.330851400345524e-05,
      "loss": 0.6606,
      "step": 880000
    },
    {
      "epoch": 8.030695671216877,
      "grad_norm": 4.161611080169678,
      "learning_rate": 4.330775360731927e-05,
      "loss": 0.6811,
      "step": 880100
    },
    {
      "epoch": 8.031608146580043,
      "grad_norm": 1.8115230798721313,
      "learning_rate": 4.33069932111833e-05,
      "loss": 0.6973,
      "step": 880200
    },
    {
      "epoch": 8.032520621943208,
      "grad_norm": 5.066248893737793,
      "learning_rate": 4.330623281504733e-05,
      "loss": 0.7,
      "step": 880300
    },
    {
      "epoch": 8.033433097306373,
      "grad_norm": 4.063070774078369,
      "learning_rate": 4.3305472418911356e-05,
      "loss": 0.6982,
      "step": 880400
    },
    {
      "epoch": 8.034345572669539,
      "grad_norm": 3.0885229110717773,
      "learning_rate": 4.3304712022775386e-05,
      "loss": 0.7021,
      "step": 880500
    },
    {
      "epoch": 8.035258048032704,
      "grad_norm": 4.498169422149658,
      "learning_rate": 4.3303951626639416e-05,
      "loss": 0.6821,
      "step": 880600
    },
    {
      "epoch": 8.03617052339587,
      "grad_norm": 3.6428446769714355,
      "learning_rate": 4.330319123050344e-05,
      "loss": 0.6783,
      "step": 880700
    },
    {
      "epoch": 8.037082998759033,
      "grad_norm": 3.5136828422546387,
      "learning_rate": 4.3302430834367476e-05,
      "loss": 0.706,
      "step": 880800
    },
    {
      "epoch": 8.037995474122198,
      "grad_norm": 3.883906364440918,
      "learning_rate": 4.33016704382315e-05,
      "loss": 0.6826,
      "step": 880900
    },
    {
      "epoch": 8.038907949485363,
      "grad_norm": 4.2811479568481445,
      "learning_rate": 4.330091004209553e-05,
      "loss": 0.7134,
      "step": 881000
    },
    {
      "epoch": 8.039820424848529,
      "grad_norm": 3.843883991241455,
      "learning_rate": 4.330014964595956e-05,
      "loss": 0.6646,
      "step": 881100
    },
    {
      "epoch": 8.040732900211694,
      "grad_norm": 4.627439022064209,
      "learning_rate": 4.329938924982359e-05,
      "loss": 0.69,
      "step": 881200
    },
    {
      "epoch": 8.04164537557486,
      "grad_norm": 3.4633865356445312,
      "learning_rate": 4.329862885368762e-05,
      "loss": 0.7066,
      "step": 881300
    },
    {
      "epoch": 8.042557850938024,
      "grad_norm": 4.908775806427002,
      "learning_rate": 4.329786845755165e-05,
      "loss": 0.6695,
      "step": 881400
    },
    {
      "epoch": 8.04347032630119,
      "grad_norm": 4.381628513336182,
      "learning_rate": 4.329710806141567e-05,
      "loss": 0.6713,
      "step": 881500
    },
    {
      "epoch": 8.044382801664355,
      "grad_norm": 4.025003433227539,
      "learning_rate": 4.329634766527971e-05,
      "loss": 0.6729,
      "step": 881600
    },
    {
      "epoch": 8.04529527702752,
      "grad_norm": 4.5593647956848145,
      "learning_rate": 4.3295587269143733e-05,
      "loss": 0.7548,
      "step": 881700
    },
    {
      "epoch": 8.046207752390686,
      "grad_norm": 3.548614263534546,
      "learning_rate": 4.3294826873007764e-05,
      "loss": 0.6832,
      "step": 881800
    },
    {
      "epoch": 8.047120227753851,
      "grad_norm": 3.378525733947754,
      "learning_rate": 4.3294066476871794e-05,
      "loss": 0.6786,
      "step": 881900
    },
    {
      "epoch": 8.048032703117016,
      "grad_norm": 3.593996524810791,
      "learning_rate": 4.3293306080735824e-05,
      "loss": 0.6802,
      "step": 882000
    },
    {
      "epoch": 8.048945178480182,
      "grad_norm": 4.1383562088012695,
      "learning_rate": 4.329254568459985e-05,
      "loss": 0.6783,
      "step": 882100
    },
    {
      "epoch": 8.049857653843347,
      "grad_norm": 3.9265568256378174,
      "learning_rate": 4.3291785288463884e-05,
      "loss": 0.7154,
      "step": 882200
    },
    {
      "epoch": 8.050770129206512,
      "grad_norm": 4.454307556152344,
      "learning_rate": 4.329102489232791e-05,
      "loss": 0.7061,
      "step": 882300
    },
    {
      "epoch": 8.051682604569677,
      "grad_norm": 5.734065532684326,
      "learning_rate": 4.329026449619194e-05,
      "loss": 0.691,
      "step": 882400
    },
    {
      "epoch": 8.052595079932841,
      "grad_norm": 3.2836966514587402,
      "learning_rate": 4.328950410005597e-05,
      "loss": 0.6835,
      "step": 882500
    },
    {
      "epoch": 8.053507555296006,
      "grad_norm": 3.9546146392822266,
      "learning_rate": 4.328874370392e-05,
      "loss": 0.7059,
      "step": 882600
    },
    {
      "epoch": 8.054420030659172,
      "grad_norm": 5.426226615905762,
      "learning_rate": 4.328798330778403e-05,
      "loss": 0.7214,
      "step": 882700
    },
    {
      "epoch": 8.055332506022337,
      "grad_norm": 4.201070785522461,
      "learning_rate": 4.328722291164806e-05,
      "loss": 0.6876,
      "step": 882800
    },
    {
      "epoch": 8.056244981385502,
      "grad_norm": 3.8020520210266113,
      "learning_rate": 4.328646251551208e-05,
      "loss": 0.7028,
      "step": 882900
    },
    {
      "epoch": 8.057157456748667,
      "grad_norm": 3.570013999938965,
      "learning_rate": 4.328570211937612e-05,
      "loss": 0.6859,
      "step": 883000
    },
    {
      "epoch": 8.058069932111833,
      "grad_norm": 3.2235732078552246,
      "learning_rate": 4.328494172324014e-05,
      "loss": 0.7057,
      "step": 883100
    },
    {
      "epoch": 8.058982407474998,
      "grad_norm": 4.220862865447998,
      "learning_rate": 4.328418132710417e-05,
      "loss": 0.6829,
      "step": 883200
    },
    {
      "epoch": 8.059894882838163,
      "grad_norm": 4.733626842498779,
      "learning_rate": 4.32834209309682e-05,
      "loss": 0.6786,
      "step": 883300
    },
    {
      "epoch": 8.060807358201329,
      "grad_norm": 4.351741313934326,
      "learning_rate": 4.3282660534832224e-05,
      "loss": 0.7054,
      "step": 883400
    },
    {
      "epoch": 8.061719833564494,
      "grad_norm": 4.87039041519165,
      "learning_rate": 4.3281900138696254e-05,
      "loss": 0.7044,
      "step": 883500
    },
    {
      "epoch": 8.06263230892766,
      "grad_norm": 3.508094072341919,
      "learning_rate": 4.3281139742560284e-05,
      "loss": 0.7375,
      "step": 883600
    },
    {
      "epoch": 8.063544784290825,
      "grad_norm": 4.266427993774414,
      "learning_rate": 4.3280379346424314e-05,
      "loss": 0.6861,
      "step": 883700
    },
    {
      "epoch": 8.06445725965399,
      "grad_norm": 3.726860523223877,
      "learning_rate": 4.3279618950288345e-05,
      "loss": 0.669,
      "step": 883800
    },
    {
      "epoch": 8.065369735017155,
      "grad_norm": 4.1524763107299805,
      "learning_rate": 4.3278858554152375e-05,
      "loss": 0.717,
      "step": 883900
    },
    {
      "epoch": 8.06628221038032,
      "grad_norm": 5.067074775695801,
      "learning_rate": 4.32780981580164e-05,
      "loss": 0.6462,
      "step": 884000
    },
    {
      "epoch": 8.067194685743486,
      "grad_norm": 4.332582473754883,
      "learning_rate": 4.3277337761880435e-05,
      "loss": 0.7073,
      "step": 884100
    },
    {
      "epoch": 8.06810716110665,
      "grad_norm": 4.388003826141357,
      "learning_rate": 4.327657736574446e-05,
      "loss": 0.6933,
      "step": 884200
    },
    {
      "epoch": 8.069019636469815,
      "grad_norm": 3.962632656097412,
      "learning_rate": 4.327581696960849e-05,
      "loss": 0.7015,
      "step": 884300
    },
    {
      "epoch": 8.06993211183298,
      "grad_norm": 3.549198865890503,
      "learning_rate": 4.327505657347252e-05,
      "loss": 0.7319,
      "step": 884400
    },
    {
      "epoch": 8.070844587196145,
      "grad_norm": 3.860858678817749,
      "learning_rate": 4.327429617733655e-05,
      "loss": 0.6937,
      "step": 884500
    },
    {
      "epoch": 8.07175706255931,
      "grad_norm": 3.6518054008483887,
      "learning_rate": 4.327353578120057e-05,
      "loss": 0.7212,
      "step": 884600
    },
    {
      "epoch": 8.072669537922476,
      "grad_norm": 4.59030294418335,
      "learning_rate": 4.327277538506461e-05,
      "loss": 0.7159,
      "step": 884700
    },
    {
      "epoch": 8.073582013285641,
      "grad_norm": 3.8838250637054443,
      "learning_rate": 4.327201498892863e-05,
      "loss": 0.6678,
      "step": 884800
    },
    {
      "epoch": 8.074494488648806,
      "grad_norm": 3.7880687713623047,
      "learning_rate": 4.327125459279266e-05,
      "loss": 0.6875,
      "step": 884900
    },
    {
      "epoch": 8.075406964011972,
      "grad_norm": 2.67380952835083,
      "learning_rate": 4.327049419665669e-05,
      "loss": 0.6853,
      "step": 885000
    },
    {
      "epoch": 8.076319439375137,
      "grad_norm": 4.69459342956543,
      "learning_rate": 4.326973380052072e-05,
      "loss": 0.7099,
      "step": 885100
    },
    {
      "epoch": 8.077231914738302,
      "grad_norm": 3.4387547969818115,
      "learning_rate": 4.326897340438475e-05,
      "loss": 0.684,
      "step": 885200
    },
    {
      "epoch": 8.078144390101468,
      "grad_norm": 3.283226728439331,
      "learning_rate": 4.326821300824878e-05,
      "loss": 0.6559,
      "step": 885300
    },
    {
      "epoch": 8.079056865464633,
      "grad_norm": 3.7468910217285156,
      "learning_rate": 4.3267452612112805e-05,
      "loss": 0.6738,
      "step": 885400
    },
    {
      "epoch": 8.079969340827798,
      "grad_norm": 3.8332009315490723,
      "learning_rate": 4.326669221597684e-05,
      "loss": 0.6902,
      "step": 885500
    },
    {
      "epoch": 8.080881816190963,
      "grad_norm": 3.3816845417022705,
      "learning_rate": 4.3265931819840865e-05,
      "loss": 0.678,
      "step": 885600
    },
    {
      "epoch": 8.081794291554129,
      "grad_norm": 3.414186954498291,
      "learning_rate": 4.3265171423704895e-05,
      "loss": 0.6502,
      "step": 885700
    },
    {
      "epoch": 8.082706766917294,
      "grad_norm": 3.8059346675872803,
      "learning_rate": 4.3264411027568926e-05,
      "loss": 0.6794,
      "step": 885800
    },
    {
      "epoch": 8.083619242280458,
      "grad_norm": 4.209977626800537,
      "learning_rate": 4.3263650631432956e-05,
      "loss": 0.722,
      "step": 885900
    },
    {
      "epoch": 8.084531717643623,
      "grad_norm": 4.510006904602051,
      "learning_rate": 4.326289023529698e-05,
      "loss": 0.7106,
      "step": 886000
    },
    {
      "epoch": 8.085444193006788,
      "grad_norm": 3.892432689666748,
      "learning_rate": 4.3262129839161016e-05,
      "loss": 0.6939,
      "step": 886100
    },
    {
      "epoch": 8.086356668369953,
      "grad_norm": 4.363701820373535,
      "learning_rate": 4.326136944302504e-05,
      "loss": 0.7137,
      "step": 886200
    },
    {
      "epoch": 8.087269143733119,
      "grad_norm": 4.436156272888184,
      "learning_rate": 4.326060904688907e-05,
      "loss": 0.6995,
      "step": 886300
    },
    {
      "epoch": 8.088181619096284,
      "grad_norm": 3.824453353881836,
      "learning_rate": 4.32598486507531e-05,
      "loss": 0.686,
      "step": 886400
    },
    {
      "epoch": 8.08909409445945,
      "grad_norm": 5.568856239318848,
      "learning_rate": 4.325908825461712e-05,
      "loss": 0.7112,
      "step": 886500
    },
    {
      "epoch": 8.090006569822615,
      "grad_norm": 3.8490447998046875,
      "learning_rate": 4.325832785848116e-05,
      "loss": 0.718,
      "step": 886600
    },
    {
      "epoch": 8.09091904518578,
      "grad_norm": 4.063446044921875,
      "learning_rate": 4.325756746234518e-05,
      "loss": 0.7117,
      "step": 886700
    },
    {
      "epoch": 8.091831520548945,
      "grad_norm": 3.9858174324035645,
      "learning_rate": 4.325680706620921e-05,
      "loss": 0.6653,
      "step": 886800
    },
    {
      "epoch": 8.09274399591211,
      "grad_norm": 3.9542622566223145,
      "learning_rate": 4.325604667007324e-05,
      "loss": 0.6706,
      "step": 886900
    },
    {
      "epoch": 8.093656471275276,
      "grad_norm": 3.5861127376556396,
      "learning_rate": 4.325528627393727e-05,
      "loss": 0.6971,
      "step": 887000
    },
    {
      "epoch": 8.094568946638441,
      "grad_norm": 3.9649581909179688,
      "learning_rate": 4.32545258778013e-05,
      "loss": 0.7008,
      "step": 887100
    },
    {
      "epoch": 8.095481422001606,
      "grad_norm": 3.390678644180298,
      "learning_rate": 4.325376548166533e-05,
      "loss": 0.6838,
      "step": 887200
    },
    {
      "epoch": 8.096393897364772,
      "grad_norm": 3.786205291748047,
      "learning_rate": 4.3253005085529356e-05,
      "loss": 0.6828,
      "step": 887300
    },
    {
      "epoch": 8.097306372727937,
      "grad_norm": 4.405398368835449,
      "learning_rate": 4.3252244689393386e-05,
      "loss": 0.6766,
      "step": 887400
    },
    {
      "epoch": 8.098218848091102,
      "grad_norm": 3.915923833847046,
      "learning_rate": 4.3251484293257416e-05,
      "loss": 0.6926,
      "step": 887500
    },
    {
      "epoch": 8.099131323454266,
      "grad_norm": 3.8060507774353027,
      "learning_rate": 4.3250723897121446e-05,
      "loss": 0.7144,
      "step": 887600
    },
    {
      "epoch": 8.100043798817431,
      "grad_norm": 3.388855457305908,
      "learning_rate": 4.3249963500985477e-05,
      "loss": 0.704,
      "step": 887700
    },
    {
      "epoch": 8.100956274180596,
      "grad_norm": 4.152966022491455,
      "learning_rate": 4.3249203104849507e-05,
      "loss": 0.6826,
      "step": 887800
    },
    {
      "epoch": 8.101868749543762,
      "grad_norm": 4.141097545623779,
      "learning_rate": 4.324844270871353e-05,
      "loss": 0.6557,
      "step": 887900
    },
    {
      "epoch": 8.102781224906927,
      "grad_norm": 3.2579894065856934,
      "learning_rate": 4.324768231257757e-05,
      "loss": 0.6724,
      "step": 888000
    },
    {
      "epoch": 8.103693700270092,
      "grad_norm": 2.7759830951690674,
      "learning_rate": 4.324692191644159e-05,
      "loss": 0.7055,
      "step": 888100
    },
    {
      "epoch": 8.104606175633258,
      "grad_norm": 4.737297058105469,
      "learning_rate": 4.324616152030562e-05,
      "loss": 0.7056,
      "step": 888200
    },
    {
      "epoch": 8.105518650996423,
      "grad_norm": 3.9737796783447266,
      "learning_rate": 4.324540112416965e-05,
      "loss": 0.6761,
      "step": 888300
    },
    {
      "epoch": 8.106431126359588,
      "grad_norm": 4.245744228363037,
      "learning_rate": 4.324464072803368e-05,
      "loss": 0.7134,
      "step": 888400
    },
    {
      "epoch": 8.107343601722754,
      "grad_norm": 4.064609527587891,
      "learning_rate": 4.324388033189771e-05,
      "loss": 0.7023,
      "step": 888500
    },
    {
      "epoch": 8.108256077085919,
      "grad_norm": 3.8811569213867188,
      "learning_rate": 4.324311993576174e-05,
      "loss": 0.6744,
      "step": 888600
    },
    {
      "epoch": 8.109168552449084,
      "grad_norm": 3.0728249549865723,
      "learning_rate": 4.3242359539625764e-05,
      "loss": 0.6631,
      "step": 888700
    },
    {
      "epoch": 8.11008102781225,
      "grad_norm": 3.3049979209899902,
      "learning_rate": 4.32415991434898e-05,
      "loss": 0.7191,
      "step": 888800
    },
    {
      "epoch": 8.110993503175415,
      "grad_norm": 4.509120464324951,
      "learning_rate": 4.3240838747353824e-05,
      "loss": 0.6888,
      "step": 888900
    },
    {
      "epoch": 8.11190597853858,
      "grad_norm": 3.950230598449707,
      "learning_rate": 4.324007835121785e-05,
      "loss": 0.7269,
      "step": 889000
    },
    {
      "epoch": 8.112818453901745,
      "grad_norm": 4.241959095001221,
      "learning_rate": 4.3239317955081884e-05,
      "loss": 0.678,
      "step": 889100
    },
    {
      "epoch": 8.11373092926491,
      "grad_norm": 2.6514034271240234,
      "learning_rate": 4.323855755894591e-05,
      "loss": 0.7164,
      "step": 889200
    },
    {
      "epoch": 8.114643404628074,
      "grad_norm": 3.881984233856201,
      "learning_rate": 4.323779716280994e-05,
      "loss": 0.6723,
      "step": 889300
    },
    {
      "epoch": 8.11555587999124,
      "grad_norm": 3.4704999923706055,
      "learning_rate": 4.323703676667397e-05,
      "loss": 0.6931,
      "step": 889400
    },
    {
      "epoch": 8.116468355354405,
      "grad_norm": 4.569070339202881,
      "learning_rate": 4.3236276370538e-05,
      "loss": 0.6792,
      "step": 889500
    },
    {
      "epoch": 8.11738083071757,
      "grad_norm": 3.811114549636841,
      "learning_rate": 4.323551597440203e-05,
      "loss": 0.6873,
      "step": 889600
    },
    {
      "epoch": 8.118293306080735,
      "grad_norm": 3.425370931625366,
      "learning_rate": 4.323475557826606e-05,
      "loss": 0.6951,
      "step": 889700
    },
    {
      "epoch": 8.1192057814439,
      "grad_norm": 4.192529201507568,
      "learning_rate": 4.323399518213008e-05,
      "loss": 0.7028,
      "step": 889800
    },
    {
      "epoch": 8.120118256807066,
      "grad_norm": 3.9600298404693604,
      "learning_rate": 4.323323478599412e-05,
      "loss": 0.671,
      "step": 889900
    },
    {
      "epoch": 8.121030732170231,
      "grad_norm": 3.4883618354797363,
      "learning_rate": 4.323247438985814e-05,
      "loss": 0.7116,
      "step": 890000
    },
    {
      "epoch": 8.121943207533397,
      "grad_norm": 4.53610897064209,
      "learning_rate": 4.323171399372217e-05,
      "loss": 0.7073,
      "step": 890100
    },
    {
      "epoch": 8.122855682896562,
      "grad_norm": 4.171738147735596,
      "learning_rate": 4.32309535975862e-05,
      "loss": 0.6882,
      "step": 890200
    },
    {
      "epoch": 8.123768158259727,
      "grad_norm": 3.852323293685913,
      "learning_rate": 4.323019320145023e-05,
      "loss": 0.7276,
      "step": 890300
    },
    {
      "epoch": 8.124680633622893,
      "grad_norm": 4.766654968261719,
      "learning_rate": 4.3229432805314254e-05,
      "loss": 0.7145,
      "step": 890400
    },
    {
      "epoch": 8.125593108986058,
      "grad_norm": 3.3362784385681152,
      "learning_rate": 4.322867240917829e-05,
      "loss": 0.6828,
      "step": 890500
    },
    {
      "epoch": 8.126505584349223,
      "grad_norm": 3.1535286903381348,
      "learning_rate": 4.3227912013042315e-05,
      "loss": 0.696,
      "step": 890600
    },
    {
      "epoch": 8.127418059712388,
      "grad_norm": 4.336650371551514,
      "learning_rate": 4.3227151616906345e-05,
      "loss": 0.6826,
      "step": 890700
    },
    {
      "epoch": 8.128330535075554,
      "grad_norm": 3.5767645835876465,
      "learning_rate": 4.3226391220770375e-05,
      "loss": 0.6774,
      "step": 890800
    },
    {
      "epoch": 8.129243010438719,
      "grad_norm": 3.994896173477173,
      "learning_rate": 4.3225630824634405e-05,
      "loss": 0.6823,
      "step": 890900
    },
    {
      "epoch": 8.130155485801883,
      "grad_norm": 4.353017330169678,
      "learning_rate": 4.3224870428498435e-05,
      "loss": 0.6969,
      "step": 891000
    },
    {
      "epoch": 8.131067961165048,
      "grad_norm": 3.9876174926757812,
      "learning_rate": 4.3224110032362465e-05,
      "loss": 0.7012,
      "step": 891100
    },
    {
      "epoch": 8.131980436528213,
      "grad_norm": 5.942585468292236,
      "learning_rate": 4.322334963622649e-05,
      "loss": 0.6785,
      "step": 891200
    },
    {
      "epoch": 8.132892911891378,
      "grad_norm": 3.38728404045105,
      "learning_rate": 4.3222589240090525e-05,
      "loss": 0.7018,
      "step": 891300
    },
    {
      "epoch": 8.133805387254544,
      "grad_norm": 4.759131908416748,
      "learning_rate": 4.322182884395455e-05,
      "loss": 0.6754,
      "step": 891400
    },
    {
      "epoch": 8.134717862617709,
      "grad_norm": 3.94195818901062,
      "learning_rate": 4.322106844781858e-05,
      "loss": 0.6863,
      "step": 891500
    },
    {
      "epoch": 8.135630337980874,
      "grad_norm": 4.699409008026123,
      "learning_rate": 4.322030805168261e-05,
      "loss": 0.6867,
      "step": 891600
    },
    {
      "epoch": 8.13654281334404,
      "grad_norm": 4.307948112487793,
      "learning_rate": 4.321954765554664e-05,
      "loss": 0.7325,
      "step": 891700
    },
    {
      "epoch": 8.137455288707205,
      "grad_norm": 2.9956929683685303,
      "learning_rate": 4.321878725941066e-05,
      "loss": 0.7021,
      "step": 891800
    },
    {
      "epoch": 8.13836776407037,
      "grad_norm": 2.9199106693267822,
      "learning_rate": 4.321802686327469e-05,
      "loss": 0.6924,
      "step": 891900
    },
    {
      "epoch": 8.139280239433536,
      "grad_norm": 4.293725490570068,
      "learning_rate": 4.321726646713872e-05,
      "loss": 0.6653,
      "step": 892000
    },
    {
      "epoch": 8.1401927147967,
      "grad_norm": 4.087550163269043,
      "learning_rate": 4.321650607100275e-05,
      "loss": 0.6684,
      "step": 892100
    },
    {
      "epoch": 8.141105190159866,
      "grad_norm": 2.6629273891448975,
      "learning_rate": 4.321574567486678e-05,
      "loss": 0.7231,
      "step": 892200
    },
    {
      "epoch": 8.142017665523031,
      "grad_norm": 4.754263401031494,
      "learning_rate": 4.3214985278730805e-05,
      "loss": 0.6584,
      "step": 892300
    },
    {
      "epoch": 8.142930140886197,
      "grad_norm": 4.072902202606201,
      "learning_rate": 4.321422488259484e-05,
      "loss": 0.6523,
      "step": 892400
    },
    {
      "epoch": 8.143842616249362,
      "grad_norm": 4.543206214904785,
      "learning_rate": 4.3213464486458866e-05,
      "loss": 0.7009,
      "step": 892500
    },
    {
      "epoch": 8.144755091612527,
      "grad_norm": 3.9852404594421387,
      "learning_rate": 4.3212704090322896e-05,
      "loss": 0.7024,
      "step": 892600
    },
    {
      "epoch": 8.14566756697569,
      "grad_norm": 4.111549377441406,
      "learning_rate": 4.3211943694186926e-05,
      "loss": 0.7305,
      "step": 892700
    },
    {
      "epoch": 8.146580042338856,
      "grad_norm": 4.9625043869018555,
      "learning_rate": 4.3211183298050956e-05,
      "loss": 0.7062,
      "step": 892800
    },
    {
      "epoch": 8.147492517702021,
      "grad_norm": 4.17948055267334,
      "learning_rate": 4.321042290191498e-05,
      "loss": 0.6895,
      "step": 892900
    },
    {
      "epoch": 8.148404993065187,
      "grad_norm": 4.158456325531006,
      "learning_rate": 4.3209662505779016e-05,
      "loss": 0.6942,
      "step": 893000
    },
    {
      "epoch": 8.149317468428352,
      "grad_norm": 4.5270304679870605,
      "learning_rate": 4.320890210964304e-05,
      "loss": 0.6801,
      "step": 893100
    },
    {
      "epoch": 8.150229943791517,
      "grad_norm": 4.2289910316467285,
      "learning_rate": 4.320814171350707e-05,
      "loss": 0.6591,
      "step": 893200
    },
    {
      "epoch": 8.151142419154683,
      "grad_norm": 4.521142482757568,
      "learning_rate": 4.32073813173711e-05,
      "loss": 0.7001,
      "step": 893300
    },
    {
      "epoch": 8.152054894517848,
      "grad_norm": 3.555860757827759,
      "learning_rate": 4.320662092123513e-05,
      "loss": 0.7174,
      "step": 893400
    },
    {
      "epoch": 8.152967369881013,
      "grad_norm": 3.7363944053649902,
      "learning_rate": 4.320586052509916e-05,
      "loss": 0.6709,
      "step": 893500
    },
    {
      "epoch": 8.153879845244179,
      "grad_norm": 3.627835512161255,
      "learning_rate": 4.320510012896319e-05,
      "loss": 0.6836,
      "step": 893600
    },
    {
      "epoch": 8.154792320607344,
      "grad_norm": 3.834182024002075,
      "learning_rate": 4.320433973282721e-05,
      "loss": 0.6413,
      "step": 893700
    },
    {
      "epoch": 8.15570479597051,
      "grad_norm": 4.6267266273498535,
      "learning_rate": 4.320357933669125e-05,
      "loss": 0.6909,
      "step": 893800
    },
    {
      "epoch": 8.156617271333674,
      "grad_norm": 4.86889123916626,
      "learning_rate": 4.320281894055527e-05,
      "loss": 0.6922,
      "step": 893900
    },
    {
      "epoch": 8.15752974669684,
      "grad_norm": 3.9707159996032715,
      "learning_rate": 4.32020585444193e-05,
      "loss": 0.6693,
      "step": 894000
    },
    {
      "epoch": 8.158442222060005,
      "grad_norm": 4.868534564971924,
      "learning_rate": 4.320129814828333e-05,
      "loss": 0.6961,
      "step": 894100
    },
    {
      "epoch": 8.15935469742317,
      "grad_norm": 3.6890676021575928,
      "learning_rate": 4.320053775214736e-05,
      "loss": 0.6733,
      "step": 894200
    },
    {
      "epoch": 8.160267172786336,
      "grad_norm": 3.3500993251800537,
      "learning_rate": 4.3199777356011386e-05,
      "loss": 0.6453,
      "step": 894300
    },
    {
      "epoch": 8.1611796481495,
      "grad_norm": 3.971871852874756,
      "learning_rate": 4.319901695987542e-05,
      "loss": 0.6747,
      "step": 894400
    },
    {
      "epoch": 8.162092123512664,
      "grad_norm": 4.054221153259277,
      "learning_rate": 4.3198256563739447e-05,
      "loss": 0.7,
      "step": 894500
    },
    {
      "epoch": 8.16300459887583,
      "grad_norm": 3.780073642730713,
      "learning_rate": 4.3197496167603477e-05,
      "loss": 0.7031,
      "step": 894600
    },
    {
      "epoch": 8.163917074238995,
      "grad_norm": 3.907409191131592,
      "learning_rate": 4.319673577146751e-05,
      "loss": 0.7303,
      "step": 894700
    },
    {
      "epoch": 8.16482954960216,
      "grad_norm": 3.3079910278320312,
      "learning_rate": 4.319597537533153e-05,
      "loss": 0.7021,
      "step": 894800
    },
    {
      "epoch": 8.165742024965326,
      "grad_norm": 4.061936855316162,
      "learning_rate": 4.319521497919557e-05,
      "loss": 0.7142,
      "step": 894900
    },
    {
      "epoch": 8.166654500328491,
      "grad_norm": 4.532439708709717,
      "learning_rate": 4.319445458305959e-05,
      "loss": 0.6592,
      "step": 895000
    },
    {
      "epoch": 8.167566975691656,
      "grad_norm": 4.360556602478027,
      "learning_rate": 4.319369418692362e-05,
      "loss": 0.707,
      "step": 895100
    },
    {
      "epoch": 8.168479451054822,
      "grad_norm": 4.226576805114746,
      "learning_rate": 4.319293379078765e-05,
      "loss": 0.6521,
      "step": 895200
    },
    {
      "epoch": 8.169391926417987,
      "grad_norm": 4.040319919586182,
      "learning_rate": 4.319217339465168e-05,
      "loss": 0.674,
      "step": 895300
    },
    {
      "epoch": 8.170304401781152,
      "grad_norm": 3.609889507293701,
      "learning_rate": 4.3191412998515704e-05,
      "loss": 0.6565,
      "step": 895400
    },
    {
      "epoch": 8.171216877144317,
      "grad_norm": 3.0858211517333984,
      "learning_rate": 4.319065260237974e-05,
      "loss": 0.7041,
      "step": 895500
    },
    {
      "epoch": 8.172129352507483,
      "grad_norm": 4.998082160949707,
      "learning_rate": 4.3189892206243764e-05,
      "loss": 0.6918,
      "step": 895600
    },
    {
      "epoch": 8.173041827870648,
      "grad_norm": 3.58944034576416,
      "learning_rate": 4.3189131810107794e-05,
      "loss": 0.6844,
      "step": 895700
    },
    {
      "epoch": 8.173954303233813,
      "grad_norm": 3.4295198917388916,
      "learning_rate": 4.3188371413971824e-05,
      "loss": 0.6759,
      "step": 895800
    },
    {
      "epoch": 8.174866778596979,
      "grad_norm": 4.035761833190918,
      "learning_rate": 4.3187611017835854e-05,
      "loss": 0.7055,
      "step": 895900
    },
    {
      "epoch": 8.175779253960144,
      "grad_norm": 4.170413494110107,
      "learning_rate": 4.3186850621699884e-05,
      "loss": 0.7076,
      "step": 896000
    },
    {
      "epoch": 8.176691729323307,
      "grad_norm": 3.7264254093170166,
      "learning_rate": 4.3186090225563914e-05,
      "loss": 0.6655,
      "step": 896100
    },
    {
      "epoch": 8.177604204686473,
      "grad_norm": 5.043753147125244,
      "learning_rate": 4.318532982942794e-05,
      "loss": 0.6916,
      "step": 896200
    },
    {
      "epoch": 8.178516680049638,
      "grad_norm": 4.126008987426758,
      "learning_rate": 4.3184569433291974e-05,
      "loss": 0.682,
      "step": 896300
    },
    {
      "epoch": 8.179429155412803,
      "grad_norm": 3.8131465911865234,
      "learning_rate": 4.3183809037156e-05,
      "loss": 0.7158,
      "step": 896400
    },
    {
      "epoch": 8.180341630775969,
      "grad_norm": 4.767441272735596,
      "learning_rate": 4.318304864102003e-05,
      "loss": 0.6786,
      "step": 896500
    },
    {
      "epoch": 8.181254106139134,
      "grad_norm": 4.189358234405518,
      "learning_rate": 4.318228824488406e-05,
      "loss": 0.6681,
      "step": 896600
    },
    {
      "epoch": 8.1821665815023,
      "grad_norm": 3.9355356693267822,
      "learning_rate": 4.318152784874809e-05,
      "loss": 0.6774,
      "step": 896700
    },
    {
      "epoch": 8.183079056865465,
      "grad_norm": 4.683169841766357,
      "learning_rate": 4.318076745261211e-05,
      "loss": 0.6702,
      "step": 896800
    },
    {
      "epoch": 8.18399153222863,
      "grad_norm": 4.143485069274902,
      "learning_rate": 4.318000705647615e-05,
      "loss": 0.7037,
      "step": 896900
    },
    {
      "epoch": 8.184904007591795,
      "grad_norm": 4.185540676116943,
      "learning_rate": 4.317924666034017e-05,
      "loss": 0.6403,
      "step": 897000
    },
    {
      "epoch": 8.18581648295496,
      "grad_norm": 4.6523823738098145,
      "learning_rate": 4.31784862642042e-05,
      "loss": 0.6714,
      "step": 897100
    },
    {
      "epoch": 8.186728958318126,
      "grad_norm": 3.9036600589752197,
      "learning_rate": 4.317772586806823e-05,
      "loss": 0.6847,
      "step": 897200
    },
    {
      "epoch": 8.187641433681291,
      "grad_norm": 3.7624239921569824,
      "learning_rate": 4.317696547193226e-05,
      "loss": 0.6603,
      "step": 897300
    },
    {
      "epoch": 8.188553909044456,
      "grad_norm": 3.4879581928253174,
      "learning_rate": 4.317620507579629e-05,
      "loss": 0.6763,
      "step": 897400
    },
    {
      "epoch": 8.189466384407622,
      "grad_norm": 3.981558084487915,
      "learning_rate": 4.3175444679660315e-05,
      "loss": 0.667,
      "step": 897500
    },
    {
      "epoch": 8.190378859770787,
      "grad_norm": 4.154876708984375,
      "learning_rate": 4.3174684283524345e-05,
      "loss": 0.7427,
      "step": 897600
    },
    {
      "epoch": 8.191291335133952,
      "grad_norm": 3.880878448486328,
      "learning_rate": 4.3173923887388375e-05,
      "loss": 0.7335,
      "step": 897700
    },
    {
      "epoch": 8.192203810497116,
      "grad_norm": 4.001242637634277,
      "learning_rate": 4.3173163491252405e-05,
      "loss": 0.6914,
      "step": 897800
    },
    {
      "epoch": 8.193116285860281,
      "grad_norm": 4.385862350463867,
      "learning_rate": 4.317240309511643e-05,
      "loss": 0.707,
      "step": 897900
    },
    {
      "epoch": 8.194028761223446,
      "grad_norm": 3.708311080932617,
      "learning_rate": 4.3171642698980465e-05,
      "loss": 0.7053,
      "step": 898000
    },
    {
      "epoch": 8.194941236586612,
      "grad_norm": 4.745175361633301,
      "learning_rate": 4.317088230284449e-05,
      "loss": 0.7162,
      "step": 898100
    },
    {
      "epoch": 8.195853711949777,
      "grad_norm": 4.40576171875,
      "learning_rate": 4.317012190670852e-05,
      "loss": 0.7089,
      "step": 898200
    },
    {
      "epoch": 8.196766187312942,
      "grad_norm": 4.508560657501221,
      "learning_rate": 4.316936151057255e-05,
      "loss": 0.7089,
      "step": 898300
    },
    {
      "epoch": 8.197678662676108,
      "grad_norm": 3.155931234359741,
      "learning_rate": 4.316860111443658e-05,
      "loss": 0.6764,
      "step": 898400
    },
    {
      "epoch": 8.198591138039273,
      "grad_norm": 4.994153022766113,
      "learning_rate": 4.316784071830061e-05,
      "loss": 0.7207,
      "step": 898500
    },
    {
      "epoch": 8.199503613402438,
      "grad_norm": 4.705711841583252,
      "learning_rate": 4.316708032216464e-05,
      "loss": 0.7031,
      "step": 898600
    },
    {
      "epoch": 8.200416088765603,
      "grad_norm": 3.54752254486084,
      "learning_rate": 4.316631992602866e-05,
      "loss": 0.6703,
      "step": 898700
    },
    {
      "epoch": 8.201328564128769,
      "grad_norm": 3.7556774616241455,
      "learning_rate": 4.31655595298927e-05,
      "loss": 0.6883,
      "step": 898800
    },
    {
      "epoch": 8.202241039491934,
      "grad_norm": 4.7294182777404785,
      "learning_rate": 4.316479913375672e-05,
      "loss": 0.6724,
      "step": 898900
    },
    {
      "epoch": 8.2031535148551,
      "grad_norm": 3.9801583290100098,
      "learning_rate": 4.316403873762075e-05,
      "loss": 0.6579,
      "step": 899000
    },
    {
      "epoch": 8.204065990218265,
      "grad_norm": 3.748716354370117,
      "learning_rate": 4.316327834148478e-05,
      "loss": 0.6575,
      "step": 899100
    },
    {
      "epoch": 8.20497846558143,
      "grad_norm": 4.499078750610352,
      "learning_rate": 4.316251794534881e-05,
      "loss": 0.7095,
      "step": 899200
    },
    {
      "epoch": 8.205890940944595,
      "grad_norm": 3.9980530738830566,
      "learning_rate": 4.316175754921284e-05,
      "loss": 0.6843,
      "step": 899300
    },
    {
      "epoch": 8.20680341630776,
      "grad_norm": 3.932596445083618,
      "learning_rate": 4.316099715307687e-05,
      "loss": 0.6526,
      "step": 899400
    },
    {
      "epoch": 8.207715891670924,
      "grad_norm": 4.721630573272705,
      "learning_rate": 4.3160236756940896e-05,
      "loss": 0.6749,
      "step": 899500
    },
    {
      "epoch": 8.20862836703409,
      "grad_norm": 3.0601072311401367,
      "learning_rate": 4.3159476360804926e-05,
      "loss": 0.6847,
      "step": 899600
    },
    {
      "epoch": 8.209540842397255,
      "grad_norm": 4.6258111000061035,
      "learning_rate": 4.3158715964668956e-05,
      "loss": 0.7221,
      "step": 899700
    },
    {
      "epoch": 8.21045331776042,
      "grad_norm": 3.8271570205688477,
      "learning_rate": 4.3157955568532986e-05,
      "loss": 0.6776,
      "step": 899800
    },
    {
      "epoch": 8.211365793123585,
      "grad_norm": 4.012001991271973,
      "learning_rate": 4.3157195172397016e-05,
      "loss": 0.6527,
      "step": 899900
    },
    {
      "epoch": 8.21227826848675,
      "grad_norm": 4.902737140655518,
      "learning_rate": 4.3156434776261046e-05,
      "loss": 0.6879,
      "step": 900000
    },
    {
      "epoch": 8.213190743849916,
      "grad_norm": 4.7796430587768555,
      "learning_rate": 4.315567438012507e-05,
      "loss": 0.679,
      "step": 900100
    },
    {
      "epoch": 8.214103219213081,
      "grad_norm": 4.558629989624023,
      "learning_rate": 4.3154913983989106e-05,
      "loss": 0.718,
      "step": 900200
    },
    {
      "epoch": 8.215015694576246,
      "grad_norm": 3.8366947174072266,
      "learning_rate": 4.315415358785313e-05,
      "loss": 0.6955,
      "step": 900300
    },
    {
      "epoch": 8.215928169939412,
      "grad_norm": 3.3010809421539307,
      "learning_rate": 4.315339319171716e-05,
      "loss": 0.6775,
      "step": 900400
    },
    {
      "epoch": 8.216840645302577,
      "grad_norm": 4.0294928550720215,
      "learning_rate": 4.315263279558119e-05,
      "loss": 0.6956,
      "step": 900500
    },
    {
      "epoch": 8.217753120665742,
      "grad_norm": 5.227572441101074,
      "learning_rate": 4.315187239944521e-05,
      "loss": 0.6888,
      "step": 900600
    },
    {
      "epoch": 8.218665596028908,
      "grad_norm": 3.6663079261779785,
      "learning_rate": 4.315111200330925e-05,
      "loss": 0.695,
      "step": 900700
    },
    {
      "epoch": 8.219578071392073,
      "grad_norm": 4.534142017364502,
      "learning_rate": 4.315035160717327e-05,
      "loss": 0.7106,
      "step": 900800
    },
    {
      "epoch": 8.220490546755238,
      "grad_norm": 4.330626964569092,
      "learning_rate": 4.31495912110373e-05,
      "loss": 0.7227,
      "step": 900900
    },
    {
      "epoch": 8.221403022118404,
      "grad_norm": 4.45059871673584,
      "learning_rate": 4.314883081490133e-05,
      "loss": 0.6731,
      "step": 901000
    },
    {
      "epoch": 8.222315497481569,
      "grad_norm": 3.3555421829223633,
      "learning_rate": 4.314807041876536e-05,
      "loss": 0.6915,
      "step": 901100
    },
    {
      "epoch": 8.223227972844732,
      "grad_norm": 4.643892765045166,
      "learning_rate": 4.3147310022629387e-05,
      "loss": 0.6749,
      "step": 901200
    },
    {
      "epoch": 8.224140448207898,
      "grad_norm": 3.6018500328063965,
      "learning_rate": 4.314654962649342e-05,
      "loss": 0.7052,
      "step": 901300
    },
    {
      "epoch": 8.225052923571063,
      "grad_norm": 4.171970367431641,
      "learning_rate": 4.314578923035745e-05,
      "loss": 0.6807,
      "step": 901400
    },
    {
      "epoch": 8.225965398934228,
      "grad_norm": 4.230401515960693,
      "learning_rate": 4.314502883422148e-05,
      "loss": 0.7308,
      "step": 901500
    },
    {
      "epoch": 8.226877874297394,
      "grad_norm": 3.8691318035125732,
      "learning_rate": 4.314426843808551e-05,
      "loss": 0.6757,
      "step": 901600
    },
    {
      "epoch": 8.227790349660559,
      "grad_norm": 3.7881553173065186,
      "learning_rate": 4.314350804194954e-05,
      "loss": 0.6717,
      "step": 901700
    },
    {
      "epoch": 8.228702825023724,
      "grad_norm": 4.49279260635376,
      "learning_rate": 4.314274764581357e-05,
      "loss": 0.7028,
      "step": 901800
    },
    {
      "epoch": 8.22961530038689,
      "grad_norm": 4.617364883422852,
      "learning_rate": 4.31419872496776e-05,
      "loss": 0.6963,
      "step": 901900
    },
    {
      "epoch": 8.230527775750055,
      "grad_norm": 3.781522274017334,
      "learning_rate": 4.314122685354162e-05,
      "loss": 0.7121,
      "step": 902000
    },
    {
      "epoch": 8.23144025111322,
      "grad_norm": 4.476654529571533,
      "learning_rate": 4.314046645740566e-05,
      "loss": 0.7144,
      "step": 902100
    },
    {
      "epoch": 8.232352726476385,
      "grad_norm": 3.893932580947876,
      "learning_rate": 4.313970606126968e-05,
      "loss": 0.7155,
      "step": 902200
    },
    {
      "epoch": 8.23326520183955,
      "grad_norm": 3.0618443489074707,
      "learning_rate": 4.313894566513371e-05,
      "loss": 0.6948,
      "step": 902300
    },
    {
      "epoch": 8.234177677202716,
      "grad_norm": 2.505251407623291,
      "learning_rate": 4.313818526899774e-05,
      "loss": 0.6944,
      "step": 902400
    },
    {
      "epoch": 8.235090152565881,
      "grad_norm": 4.250978469848633,
      "learning_rate": 4.313742487286177e-05,
      "loss": 0.6913,
      "step": 902500
    },
    {
      "epoch": 8.236002627929047,
      "grad_norm": 4.038027286529541,
      "learning_rate": 4.3136664476725794e-05,
      "loss": 0.7197,
      "step": 902600
    },
    {
      "epoch": 8.236915103292212,
      "grad_norm": 5.196727752685547,
      "learning_rate": 4.313590408058983e-05,
      "loss": 0.6755,
      "step": 902700
    },
    {
      "epoch": 8.237827578655377,
      "grad_norm": 4.059708118438721,
      "learning_rate": 4.3135143684453854e-05,
      "loss": 0.6745,
      "step": 902800
    },
    {
      "epoch": 8.23874005401854,
      "grad_norm": 3.5756325721740723,
      "learning_rate": 4.3134383288317884e-05,
      "loss": 0.679,
      "step": 902900
    },
    {
      "epoch": 8.239652529381706,
      "grad_norm": 3.366243839263916,
      "learning_rate": 4.3133622892181914e-05,
      "loss": 0.7075,
      "step": 903000
    },
    {
      "epoch": 8.240565004744871,
      "grad_norm": 4.45531702041626,
      "learning_rate": 4.3132862496045944e-05,
      "loss": 0.7536,
      "step": 903100
    },
    {
      "epoch": 8.241477480108037,
      "grad_norm": 4.610413074493408,
      "learning_rate": 4.3132102099909974e-05,
      "loss": 0.6866,
      "step": 903200
    },
    {
      "epoch": 8.242389955471202,
      "grad_norm": 3.7621357440948486,
      "learning_rate": 4.3131341703774e-05,
      "loss": 0.6615,
      "step": 903300
    },
    {
      "epoch": 8.243302430834367,
      "grad_norm": 3.4255666732788086,
      "learning_rate": 4.313058130763803e-05,
      "loss": 0.6664,
      "step": 903400
    },
    {
      "epoch": 8.244214906197532,
      "grad_norm": 4.340534687042236,
      "learning_rate": 4.312982091150206e-05,
      "loss": 0.7176,
      "step": 903500
    },
    {
      "epoch": 8.245127381560698,
      "grad_norm": 4.347183704376221,
      "learning_rate": 4.312906051536609e-05,
      "loss": 0.6861,
      "step": 903600
    },
    {
      "epoch": 8.246039856923863,
      "grad_norm": 4.301657199859619,
      "learning_rate": 4.312830011923011e-05,
      "loss": 0.6843,
      "step": 903700
    },
    {
      "epoch": 8.246952332287028,
      "grad_norm": 4.106287479400635,
      "learning_rate": 4.312753972309415e-05,
      "loss": 0.6806,
      "step": 903800
    },
    {
      "epoch": 8.247864807650194,
      "grad_norm": 4.724213600158691,
      "learning_rate": 4.312677932695817e-05,
      "loss": 0.6597,
      "step": 903900
    },
    {
      "epoch": 8.248777283013359,
      "grad_norm": 3.779496908187866,
      "learning_rate": 4.31260189308222e-05,
      "loss": 0.6786,
      "step": 904000
    },
    {
      "epoch": 8.249689758376524,
      "grad_norm": 3.773543119430542,
      "learning_rate": 4.312525853468623e-05,
      "loss": 0.7159,
      "step": 904100
    },
    {
      "epoch": 8.25060223373969,
      "grad_norm": 4.424266338348389,
      "learning_rate": 4.312449813855026e-05,
      "loss": 0.6877,
      "step": 904200
    },
    {
      "epoch": 8.251514709102855,
      "grad_norm": 3.420099973678589,
      "learning_rate": 4.312373774241429e-05,
      "loss": 0.6806,
      "step": 904300
    },
    {
      "epoch": 8.25242718446602,
      "grad_norm": 4.212875843048096,
      "learning_rate": 4.312297734627832e-05,
      "loss": 0.6968,
      "step": 904400
    },
    {
      "epoch": 8.253339659829184,
      "grad_norm": 4.4270477294921875,
      "learning_rate": 4.3122216950142345e-05,
      "loss": 0.6904,
      "step": 904500
    },
    {
      "epoch": 8.254252135192349,
      "grad_norm": 4.423628807067871,
      "learning_rate": 4.312145655400638e-05,
      "loss": 0.6984,
      "step": 904600
    },
    {
      "epoch": 8.255164610555514,
      "grad_norm": 3.6957085132598877,
      "learning_rate": 4.3120696157870405e-05,
      "loss": 0.6611,
      "step": 904700
    },
    {
      "epoch": 8.25607708591868,
      "grad_norm": 4.1090922355651855,
      "learning_rate": 4.3119935761734435e-05,
      "loss": 0.6876,
      "step": 904800
    },
    {
      "epoch": 8.256989561281845,
      "grad_norm": 3.475411891937256,
      "learning_rate": 4.3119175365598465e-05,
      "loss": 0.7032,
      "step": 904900
    },
    {
      "epoch": 8.25790203664501,
      "grad_norm": 4.165278911590576,
      "learning_rate": 4.3118414969462495e-05,
      "loss": 0.7083,
      "step": 905000
    },
    {
      "epoch": 8.258814512008176,
      "grad_norm": 3.9837112426757812,
      "learning_rate": 4.311765457332652e-05,
      "loss": 0.752,
      "step": 905100
    },
    {
      "epoch": 8.25972698737134,
      "grad_norm": 4.09998893737793,
      "learning_rate": 4.3116894177190555e-05,
      "loss": 0.6292,
      "step": 905200
    },
    {
      "epoch": 8.260639462734506,
      "grad_norm": 4.435786247253418,
      "learning_rate": 4.311613378105458e-05,
      "loss": 0.7121,
      "step": 905300
    },
    {
      "epoch": 8.261551938097671,
      "grad_norm": 4.343197345733643,
      "learning_rate": 4.311537338491861e-05,
      "loss": 0.6529,
      "step": 905400
    },
    {
      "epoch": 8.262464413460837,
      "grad_norm": 4.317851543426514,
      "learning_rate": 4.311461298878264e-05,
      "loss": 0.6806,
      "step": 905500
    },
    {
      "epoch": 8.263376888824002,
      "grad_norm": 4.024224758148193,
      "learning_rate": 4.311385259264667e-05,
      "loss": 0.7071,
      "step": 905600
    },
    {
      "epoch": 8.264289364187167,
      "grad_norm": 4.498983860015869,
      "learning_rate": 4.31130921965107e-05,
      "loss": 0.7055,
      "step": 905700
    },
    {
      "epoch": 8.265201839550333,
      "grad_norm": 5.214164733886719,
      "learning_rate": 4.311233180037473e-05,
      "loss": 0.6934,
      "step": 905800
    },
    {
      "epoch": 8.266114314913498,
      "grad_norm": 3.593114137649536,
      "learning_rate": 4.311157140423875e-05,
      "loss": 0.6807,
      "step": 905900
    },
    {
      "epoch": 8.267026790276663,
      "grad_norm": 4.346960067749023,
      "learning_rate": 4.311081100810279e-05,
      "loss": 0.6933,
      "step": 906000
    },
    {
      "epoch": 8.267939265639829,
      "grad_norm": 2.850217342376709,
      "learning_rate": 4.311005061196681e-05,
      "loss": 0.7099,
      "step": 906100
    },
    {
      "epoch": 8.268851741002994,
      "grad_norm": 4.4721784591674805,
      "learning_rate": 4.3109290215830836e-05,
      "loss": 0.6911,
      "step": 906200
    },
    {
      "epoch": 8.269764216366157,
      "grad_norm": 4.527761459350586,
      "learning_rate": 4.310852981969487e-05,
      "loss": 0.6891,
      "step": 906300
    },
    {
      "epoch": 8.270676691729323,
      "grad_norm": 3.664778470993042,
      "learning_rate": 4.3107769423558896e-05,
      "loss": 0.69,
      "step": 906400
    },
    {
      "epoch": 8.271589167092488,
      "grad_norm": 4.178751468658447,
      "learning_rate": 4.3107009027422926e-05,
      "loss": 0.7141,
      "step": 906500
    },
    {
      "epoch": 8.272501642455653,
      "grad_norm": 3.8218038082122803,
      "learning_rate": 4.3106248631286956e-05,
      "loss": 0.6813,
      "step": 906600
    },
    {
      "epoch": 8.273414117818819,
      "grad_norm": 4.5780487060546875,
      "learning_rate": 4.3105488235150986e-05,
      "loss": 0.6551,
      "step": 906700
    },
    {
      "epoch": 8.274326593181984,
      "grad_norm": 3.825646162033081,
      "learning_rate": 4.3104727839015016e-05,
      "loss": 0.701,
      "step": 906800
    },
    {
      "epoch": 8.27523906854515,
      "grad_norm": 4.826428413391113,
      "learning_rate": 4.3103967442879046e-05,
      "loss": 0.6803,
      "step": 906900
    },
    {
      "epoch": 8.276151543908314,
      "grad_norm": 4.169136047363281,
      "learning_rate": 4.310320704674307e-05,
      "loss": 0.6624,
      "step": 907000
    },
    {
      "epoch": 8.27706401927148,
      "grad_norm": 4.2183356285095215,
      "learning_rate": 4.3102446650607106e-05,
      "loss": 0.7101,
      "step": 907100
    },
    {
      "epoch": 8.277976494634645,
      "grad_norm": 3.4117443561553955,
      "learning_rate": 4.310168625447113e-05,
      "loss": 0.6822,
      "step": 907200
    },
    {
      "epoch": 8.27888896999781,
      "grad_norm": 3.9635074138641357,
      "learning_rate": 4.310092585833516e-05,
      "loss": 0.6802,
      "step": 907300
    },
    {
      "epoch": 8.279801445360976,
      "grad_norm": 4.059930324554443,
      "learning_rate": 4.310016546219919e-05,
      "loss": 0.709,
      "step": 907400
    },
    {
      "epoch": 8.280713920724141,
      "grad_norm": 3.4787979125976562,
      "learning_rate": 4.309940506606322e-05,
      "loss": 0.6986,
      "step": 907500
    },
    {
      "epoch": 8.281626396087306,
      "grad_norm": 4.356464385986328,
      "learning_rate": 4.309864466992724e-05,
      "loss": 0.6862,
      "step": 907600
    },
    {
      "epoch": 8.282538871450472,
      "grad_norm": 4.17765474319458,
      "learning_rate": 4.309788427379128e-05,
      "loss": 0.7074,
      "step": 907700
    },
    {
      "epoch": 8.283451346813637,
      "grad_norm": 3.659579277038574,
      "learning_rate": 4.30971238776553e-05,
      "loss": 0.6701,
      "step": 907800
    },
    {
      "epoch": 8.2843638221768,
      "grad_norm": 4.417840003967285,
      "learning_rate": 4.309636348151933e-05,
      "loss": 0.6916,
      "step": 907900
    },
    {
      "epoch": 8.285276297539966,
      "grad_norm": 3.980186939239502,
      "learning_rate": 4.309560308538336e-05,
      "loss": 0.6979,
      "step": 908000
    },
    {
      "epoch": 8.286188772903131,
      "grad_norm": 3.924341917037964,
      "learning_rate": 4.309484268924739e-05,
      "loss": 0.6783,
      "step": 908100
    },
    {
      "epoch": 8.287101248266296,
      "grad_norm": 3.7140631675720215,
      "learning_rate": 4.3094082293111423e-05,
      "loss": 0.6978,
      "step": 908200
    },
    {
      "epoch": 8.288013723629462,
      "grad_norm": 3.7031219005584717,
      "learning_rate": 4.3093321896975454e-05,
      "loss": 0.7019,
      "step": 908300
    },
    {
      "epoch": 8.288926198992627,
      "grad_norm": 4.412001132965088,
      "learning_rate": 4.309256150083948e-05,
      "loss": 0.7172,
      "step": 908400
    },
    {
      "epoch": 8.289838674355792,
      "grad_norm": 3.45268177986145,
      "learning_rate": 4.3091801104703514e-05,
      "loss": 0.6667,
      "step": 908500
    },
    {
      "epoch": 8.290751149718957,
      "grad_norm": 4.222031116485596,
      "learning_rate": 4.309104070856754e-05,
      "loss": 0.6519,
      "step": 908600
    },
    {
      "epoch": 8.291663625082123,
      "grad_norm": 4.7961554527282715,
      "learning_rate": 4.309028031243157e-05,
      "loss": 0.7047,
      "step": 908700
    },
    {
      "epoch": 8.292576100445288,
      "grad_norm": 4.562835693359375,
      "learning_rate": 4.30895199162956e-05,
      "loss": 0.6977,
      "step": 908800
    },
    {
      "epoch": 8.293488575808453,
      "grad_norm": 3.532726287841797,
      "learning_rate": 4.308875952015962e-05,
      "loss": 0.6916,
      "step": 908900
    },
    {
      "epoch": 8.294401051171619,
      "grad_norm": 4.449898719787598,
      "learning_rate": 4.308799912402365e-05,
      "loss": 0.701,
      "step": 909000
    },
    {
      "epoch": 8.295313526534784,
      "grad_norm": 4.098489284515381,
      "learning_rate": 4.308723872788768e-05,
      "loss": 0.7316,
      "step": 909100
    },
    {
      "epoch": 8.29622600189795,
      "grad_norm": 3.2459867000579834,
      "learning_rate": 4.308647833175171e-05,
      "loss": 0.6767,
      "step": 909200
    },
    {
      "epoch": 8.297138477261115,
      "grad_norm": 3.9901134967803955,
      "learning_rate": 4.308571793561574e-05,
      "loss": 0.7017,
      "step": 909300
    },
    {
      "epoch": 8.29805095262428,
      "grad_norm": 2.8254435062408447,
      "learning_rate": 4.308495753947977e-05,
      "loss": 0.7075,
      "step": 909400
    },
    {
      "epoch": 8.298963427987445,
      "grad_norm": 4.570394992828369,
      "learning_rate": 4.3084197143343794e-05,
      "loss": 0.6841,
      "step": 909500
    },
    {
      "epoch": 8.29987590335061,
      "grad_norm": 4.292635440826416,
      "learning_rate": 4.308343674720783e-05,
      "loss": 0.6732,
      "step": 909600
    },
    {
      "epoch": 8.300788378713774,
      "grad_norm": 4.236603260040283,
      "learning_rate": 4.3082676351071854e-05,
      "loss": 0.7002,
      "step": 909700
    },
    {
      "epoch": 8.30170085407694,
      "grad_norm": 4.393643856048584,
      "learning_rate": 4.3081915954935884e-05,
      "loss": 0.6848,
      "step": 909800
    },
    {
      "epoch": 8.302613329440105,
      "grad_norm": 3.546294927597046,
      "learning_rate": 4.3081155558799914e-05,
      "loss": 0.6914,
      "step": 909900
    },
    {
      "epoch": 8.30352580480327,
      "grad_norm": 4.545020580291748,
      "learning_rate": 4.3080395162663944e-05,
      "loss": 0.6675,
      "step": 910000
    },
    {
      "epoch": 8.304438280166435,
      "grad_norm": 3.8138036727905273,
      "learning_rate": 4.307963476652797e-05,
      "loss": 0.7348,
      "step": 910100
    },
    {
      "epoch": 8.3053507555296,
      "grad_norm": 2.790085792541504,
      "learning_rate": 4.3078874370392004e-05,
      "loss": 0.6403,
      "step": 910200
    },
    {
      "epoch": 8.306263230892766,
      "grad_norm": 3.7121002674102783,
      "learning_rate": 4.307811397425603e-05,
      "loss": 0.6961,
      "step": 910300
    },
    {
      "epoch": 8.307175706255931,
      "grad_norm": 4.110426902770996,
      "learning_rate": 4.307735357812006e-05,
      "loss": 0.7279,
      "step": 910400
    },
    {
      "epoch": 8.308088181619096,
      "grad_norm": 4.23179817199707,
      "learning_rate": 4.307659318198409e-05,
      "loss": 0.6888,
      "step": 910500
    },
    {
      "epoch": 8.309000656982262,
      "grad_norm": 4.183701992034912,
      "learning_rate": 4.307583278584812e-05,
      "loss": 0.6587,
      "step": 910600
    },
    {
      "epoch": 8.309913132345427,
      "grad_norm": 4.499713897705078,
      "learning_rate": 4.307507238971215e-05,
      "loss": 0.7107,
      "step": 910700
    },
    {
      "epoch": 8.310825607708592,
      "grad_norm": 3.822786569595337,
      "learning_rate": 4.307431199357618e-05,
      "loss": 0.663,
      "step": 910800
    },
    {
      "epoch": 8.311738083071758,
      "grad_norm": 4.923058032989502,
      "learning_rate": 4.30735515974402e-05,
      "loss": 0.6933,
      "step": 910900
    },
    {
      "epoch": 8.312650558434923,
      "grad_norm": 4.95471715927124,
      "learning_rate": 4.307279120130424e-05,
      "loss": 0.6509,
      "step": 911000
    },
    {
      "epoch": 8.313563033798088,
      "grad_norm": 4.137294769287109,
      "learning_rate": 4.307203080516826e-05,
      "loss": 0.7122,
      "step": 911100
    },
    {
      "epoch": 8.314475509161253,
      "grad_norm": 3.432532787322998,
      "learning_rate": 4.307127040903229e-05,
      "loss": 0.6649,
      "step": 911200
    },
    {
      "epoch": 8.315387984524417,
      "grad_norm": 3.1268980503082275,
      "learning_rate": 4.307051001289632e-05,
      "loss": 0.718,
      "step": 911300
    },
    {
      "epoch": 8.316300459887582,
      "grad_norm": 3.6780731678009033,
      "learning_rate": 4.306974961676035e-05,
      "loss": 0.7096,
      "step": 911400
    },
    {
      "epoch": 8.317212935250748,
      "grad_norm": 3.6390068531036377,
      "learning_rate": 4.3068989220624375e-05,
      "loss": 0.6939,
      "step": 911500
    },
    {
      "epoch": 8.318125410613913,
      "grad_norm": 4.536137580871582,
      "learning_rate": 4.306822882448841e-05,
      "loss": 0.7072,
      "step": 911600
    },
    {
      "epoch": 8.319037885977078,
      "grad_norm": 4.0996174812316895,
      "learning_rate": 4.3067468428352435e-05,
      "loss": 0.6375,
      "step": 911700
    },
    {
      "epoch": 8.319950361340243,
      "grad_norm": 3.586677312850952,
      "learning_rate": 4.3066708032216465e-05,
      "loss": 0.7024,
      "step": 911800
    },
    {
      "epoch": 8.320862836703409,
      "grad_norm": 4.077868461608887,
      "learning_rate": 4.3065947636080495e-05,
      "loss": 0.7014,
      "step": 911900
    },
    {
      "epoch": 8.321775312066574,
      "grad_norm": 3.2625229358673096,
      "learning_rate": 4.306518723994452e-05,
      "loss": 0.7158,
      "step": 912000
    },
    {
      "epoch": 8.32268778742974,
      "grad_norm": 4.1829447746276855,
      "learning_rate": 4.3064426843808555e-05,
      "loss": 0.7195,
      "step": 912100
    },
    {
      "epoch": 8.323600262792905,
      "grad_norm": 3.64870285987854,
      "learning_rate": 4.306366644767258e-05,
      "loss": 0.7299,
      "step": 912200
    },
    {
      "epoch": 8.32451273815607,
      "grad_norm": 4.8645453453063965,
      "learning_rate": 4.306290605153661e-05,
      "loss": 0.709,
      "step": 912300
    },
    {
      "epoch": 8.325425213519235,
      "grad_norm": 3.6317102909088135,
      "learning_rate": 4.306214565540064e-05,
      "loss": 0.6695,
      "step": 912400
    },
    {
      "epoch": 8.3263376888824,
      "grad_norm": 4.797938823699951,
      "learning_rate": 4.306138525926467e-05,
      "loss": 0.7204,
      "step": 912500
    },
    {
      "epoch": 8.327250164245566,
      "grad_norm": 5.368269443511963,
      "learning_rate": 4.30606248631287e-05,
      "loss": 0.6377,
      "step": 912600
    },
    {
      "epoch": 8.328162639608731,
      "grad_norm": 3.143399715423584,
      "learning_rate": 4.305986446699273e-05,
      "loss": 0.6955,
      "step": 912700
    },
    {
      "epoch": 8.329075114971896,
      "grad_norm": 3.8928098678588867,
      "learning_rate": 4.305910407085675e-05,
      "loss": 0.7129,
      "step": 912800
    },
    {
      "epoch": 8.329987590335062,
      "grad_norm": 3.7717413902282715,
      "learning_rate": 4.305834367472078e-05,
      "loss": 0.688,
      "step": 912900
    },
    {
      "epoch": 8.330900065698227,
      "grad_norm": 3.6500236988067627,
      "learning_rate": 4.305758327858481e-05,
      "loss": 0.6548,
      "step": 913000
    },
    {
      "epoch": 8.33181254106139,
      "grad_norm": 4.384334087371826,
      "learning_rate": 4.305682288244884e-05,
      "loss": 0.6642,
      "step": 913100
    },
    {
      "epoch": 8.332725016424556,
      "grad_norm": 4.168957710266113,
      "learning_rate": 4.305606248631287e-05,
      "loss": 0.7243,
      "step": 913200
    },
    {
      "epoch": 8.333637491787721,
      "grad_norm": 3.944460391998291,
      "learning_rate": 4.30553020901769e-05,
      "loss": 0.7305,
      "step": 913300
    },
    {
      "epoch": 8.334549967150886,
      "grad_norm": 3.875765562057495,
      "learning_rate": 4.3054541694040926e-05,
      "loss": 0.6629,
      "step": 913400
    },
    {
      "epoch": 8.335462442514052,
      "grad_norm": 3.605272054672241,
      "learning_rate": 4.305378129790496e-05,
      "loss": 0.6857,
      "step": 913500
    },
    {
      "epoch": 8.336374917877217,
      "grad_norm": 3.4019198417663574,
      "learning_rate": 4.3053020901768986e-05,
      "loss": 0.6506,
      "step": 913600
    },
    {
      "epoch": 8.337287393240382,
      "grad_norm": 4.167999267578125,
      "learning_rate": 4.3052260505633016e-05,
      "loss": 0.6704,
      "step": 913700
    },
    {
      "epoch": 8.338199868603548,
      "grad_norm": 3.8956069946289062,
      "learning_rate": 4.3051500109497046e-05,
      "loss": 0.6738,
      "step": 913800
    },
    {
      "epoch": 8.339112343966713,
      "grad_norm": 3.7286765575408936,
      "learning_rate": 4.3050739713361076e-05,
      "loss": 0.6795,
      "step": 913900
    },
    {
      "epoch": 8.340024819329878,
      "grad_norm": 4.028064727783203,
      "learning_rate": 4.3049979317225106e-05,
      "loss": 0.7287,
      "step": 914000
    },
    {
      "epoch": 8.340937294693044,
      "grad_norm": 4.394911289215088,
      "learning_rate": 4.3049218921089136e-05,
      "loss": 0.6192,
      "step": 914100
    },
    {
      "epoch": 8.341849770056209,
      "grad_norm": 4.031632423400879,
      "learning_rate": 4.304845852495316e-05,
      "loss": 0.7197,
      "step": 914200
    },
    {
      "epoch": 8.342762245419374,
      "grad_norm": 3.659471273422241,
      "learning_rate": 4.3047698128817197e-05,
      "loss": 0.6736,
      "step": 914300
    },
    {
      "epoch": 8.34367472078254,
      "grad_norm": 5.047649383544922,
      "learning_rate": 4.304693773268122e-05,
      "loss": 0.6925,
      "step": 914400
    },
    {
      "epoch": 8.344587196145705,
      "grad_norm": 4.884827613830566,
      "learning_rate": 4.304617733654525e-05,
      "loss": 0.7003,
      "step": 914500
    },
    {
      "epoch": 8.34549967150887,
      "grad_norm": 3.7175087928771973,
      "learning_rate": 4.304541694040928e-05,
      "loss": 0.6825,
      "step": 914600
    },
    {
      "epoch": 8.346412146872034,
      "grad_norm": 4.137422561645508,
      "learning_rate": 4.30446565442733e-05,
      "loss": 0.6576,
      "step": 914700
    },
    {
      "epoch": 8.347324622235199,
      "grad_norm": 3.9531753063201904,
      "learning_rate": 4.304389614813733e-05,
      "loss": 0.679,
      "step": 914800
    },
    {
      "epoch": 8.348237097598364,
      "grad_norm": 3.6645960807800293,
      "learning_rate": 4.3043135752001363e-05,
      "loss": 0.7025,
      "step": 914900
    },
    {
      "epoch": 8.34914957296153,
      "grad_norm": 4.361098289489746,
      "learning_rate": 4.3042375355865393e-05,
      "loss": 0.7227,
      "step": 915000
    },
    {
      "epoch": 8.350062048324695,
      "grad_norm": 4.282525062561035,
      "learning_rate": 4.3041614959729424e-05,
      "loss": 0.6859,
      "step": 915100
    },
    {
      "epoch": 8.35097452368786,
      "grad_norm": 3.9958231449127197,
      "learning_rate": 4.3040854563593454e-05,
      "loss": 0.6738,
      "step": 915200
    },
    {
      "epoch": 8.351886999051025,
      "grad_norm": 4.034171104431152,
      "learning_rate": 4.304009416745748e-05,
      "loss": 0.7017,
      "step": 915300
    },
    {
      "epoch": 8.35279947441419,
      "grad_norm": 3.753279685974121,
      "learning_rate": 4.3039333771321514e-05,
      "loss": 0.6557,
      "step": 915400
    },
    {
      "epoch": 8.353711949777356,
      "grad_norm": 3.8640644550323486,
      "learning_rate": 4.303857337518554e-05,
      "loss": 0.6765,
      "step": 915500
    },
    {
      "epoch": 8.354624425140521,
      "grad_norm": 4.093048572540283,
      "learning_rate": 4.303781297904957e-05,
      "loss": 0.7517,
      "step": 915600
    },
    {
      "epoch": 8.355536900503687,
      "grad_norm": 3.8590645790100098,
      "learning_rate": 4.30370525829136e-05,
      "loss": 0.6938,
      "step": 915700
    },
    {
      "epoch": 8.356449375866852,
      "grad_norm": 3.9182865619659424,
      "learning_rate": 4.303629218677763e-05,
      "loss": 0.6717,
      "step": 915800
    },
    {
      "epoch": 8.357361851230017,
      "grad_norm": 3.3352575302124023,
      "learning_rate": 4.303553179064165e-05,
      "loss": 0.6669,
      "step": 915900
    },
    {
      "epoch": 8.358274326593182,
      "grad_norm": 4.507414817810059,
      "learning_rate": 4.303477139450569e-05,
      "loss": 0.6734,
      "step": 916000
    },
    {
      "epoch": 8.359186801956348,
      "grad_norm": 4.116888999938965,
      "learning_rate": 4.303401099836971e-05,
      "loss": 0.693,
      "step": 916100
    },
    {
      "epoch": 8.360099277319513,
      "grad_norm": 4.320826053619385,
      "learning_rate": 4.303325060223374e-05,
      "loss": 0.7426,
      "step": 916200
    },
    {
      "epoch": 8.361011752682678,
      "grad_norm": 4.220562934875488,
      "learning_rate": 4.303249020609777e-05,
      "loss": 0.697,
      "step": 916300
    },
    {
      "epoch": 8.361924228045844,
      "grad_norm": 4.370825290679932,
      "learning_rate": 4.30317298099618e-05,
      "loss": 0.7019,
      "step": 916400
    },
    {
      "epoch": 8.362836703409007,
      "grad_norm": 3.9544689655303955,
      "learning_rate": 4.303096941382583e-05,
      "loss": 0.6976,
      "step": 916500
    },
    {
      "epoch": 8.363749178772172,
      "grad_norm": 3.0649454593658447,
      "learning_rate": 4.303020901768986e-05,
      "loss": 0.7035,
      "step": 916600
    },
    {
      "epoch": 8.364661654135338,
      "grad_norm": 3.9191386699676514,
      "learning_rate": 4.3029448621553884e-05,
      "loss": 0.6865,
      "step": 916700
    },
    {
      "epoch": 8.365574129498503,
      "grad_norm": 4.834989070892334,
      "learning_rate": 4.302868822541792e-05,
      "loss": 0.6594,
      "step": 916800
    },
    {
      "epoch": 8.366486604861668,
      "grad_norm": 4.671690940856934,
      "learning_rate": 4.3027927829281944e-05,
      "loss": 0.7027,
      "step": 916900
    },
    {
      "epoch": 8.367399080224834,
      "grad_norm": 3.6265268325805664,
      "learning_rate": 4.3027167433145974e-05,
      "loss": 0.6747,
      "step": 917000
    },
    {
      "epoch": 8.368311555587999,
      "grad_norm": 3.197741746902466,
      "learning_rate": 4.3026407037010005e-05,
      "loss": 0.6946,
      "step": 917100
    },
    {
      "epoch": 8.369224030951164,
      "grad_norm": 4.67659330368042,
      "learning_rate": 4.3025646640874035e-05,
      "loss": 0.6734,
      "step": 917200
    },
    {
      "epoch": 8.37013650631433,
      "grad_norm": 4.04337215423584,
      "learning_rate": 4.302488624473806e-05,
      "loss": 0.6876,
      "step": 917300
    },
    {
      "epoch": 8.371048981677495,
      "grad_norm": 4.087386608123779,
      "learning_rate": 4.302412584860209e-05,
      "loss": 0.6895,
      "step": 917400
    },
    {
      "epoch": 8.37196145704066,
      "grad_norm": 4.4540839195251465,
      "learning_rate": 4.302336545246612e-05,
      "loss": 0.6428,
      "step": 917500
    },
    {
      "epoch": 8.372873932403825,
      "grad_norm": 3.6259706020355225,
      "learning_rate": 4.302260505633015e-05,
      "loss": 0.6937,
      "step": 917600
    },
    {
      "epoch": 8.37378640776699,
      "grad_norm": 4.359135627746582,
      "learning_rate": 4.302184466019418e-05,
      "loss": 0.7014,
      "step": 917700
    },
    {
      "epoch": 8.374698883130156,
      "grad_norm": 4.209835052490234,
      "learning_rate": 4.30210842640582e-05,
      "loss": 0.6771,
      "step": 917800
    },
    {
      "epoch": 8.375611358493321,
      "grad_norm": 4.714927673339844,
      "learning_rate": 4.302032386792224e-05,
      "loss": 0.6648,
      "step": 917900
    },
    {
      "epoch": 8.376523833856487,
      "grad_norm": 4.23422384262085,
      "learning_rate": 4.301956347178626e-05,
      "loss": 0.7159,
      "step": 918000
    },
    {
      "epoch": 8.37743630921965,
      "grad_norm": 4.5980448722839355,
      "learning_rate": 4.301880307565029e-05,
      "loss": 0.6755,
      "step": 918100
    },
    {
      "epoch": 8.378348784582816,
      "grad_norm": 3.7861366271972656,
      "learning_rate": 4.301804267951432e-05,
      "loss": 0.7299,
      "step": 918200
    },
    {
      "epoch": 8.37926125994598,
      "grad_norm": 4.417375087738037,
      "learning_rate": 4.301728228337835e-05,
      "loss": 0.6774,
      "step": 918300
    },
    {
      "epoch": 8.380173735309146,
      "grad_norm": 3.8312079906463623,
      "learning_rate": 4.3016521887242375e-05,
      "loss": 0.6842,
      "step": 918400
    },
    {
      "epoch": 8.381086210672311,
      "grad_norm": 3.8712775707244873,
      "learning_rate": 4.301576149110641e-05,
      "loss": 0.6804,
      "step": 918500
    },
    {
      "epoch": 8.381998686035477,
      "grad_norm": 3.7621490955352783,
      "learning_rate": 4.3015001094970435e-05,
      "loss": 0.7059,
      "step": 918600
    },
    {
      "epoch": 8.382911161398642,
      "grad_norm": 3.887162923812866,
      "learning_rate": 4.3014240698834465e-05,
      "loss": 0.6502,
      "step": 918700
    },
    {
      "epoch": 8.383823636761807,
      "grad_norm": 3.5102264881134033,
      "learning_rate": 4.3013480302698495e-05,
      "loss": 0.6768,
      "step": 918800
    },
    {
      "epoch": 8.384736112124973,
      "grad_norm": 3.0960044860839844,
      "learning_rate": 4.3012719906562525e-05,
      "loss": 0.7043,
      "step": 918900
    },
    {
      "epoch": 8.385648587488138,
      "grad_norm": 3.2137951850891113,
      "learning_rate": 4.3011959510426556e-05,
      "loss": 0.6535,
      "step": 919000
    },
    {
      "epoch": 8.386561062851303,
      "grad_norm": 3.097700357437134,
      "learning_rate": 4.3011199114290586e-05,
      "loss": 0.701,
      "step": 919100
    },
    {
      "epoch": 8.387473538214469,
      "grad_norm": 4.191669940948486,
      "learning_rate": 4.301043871815461e-05,
      "loss": 0.6809,
      "step": 919200
    },
    {
      "epoch": 8.388386013577634,
      "grad_norm": 4.257297992706299,
      "learning_rate": 4.3009678322018646e-05,
      "loss": 0.6857,
      "step": 919300
    },
    {
      "epoch": 8.389298488940799,
      "grad_norm": 4.010186195373535,
      "learning_rate": 4.300891792588267e-05,
      "loss": 0.6982,
      "step": 919400
    },
    {
      "epoch": 8.390210964303964,
      "grad_norm": 4.996772289276123,
      "learning_rate": 4.30081575297467e-05,
      "loss": 0.6883,
      "step": 919500
    },
    {
      "epoch": 8.39112343966713,
      "grad_norm": 3.5900766849517822,
      "learning_rate": 4.300739713361073e-05,
      "loss": 0.6721,
      "step": 919600
    },
    {
      "epoch": 8.392035915030295,
      "grad_norm": 4.723376274108887,
      "learning_rate": 4.300663673747476e-05,
      "loss": 0.6746,
      "step": 919700
    },
    {
      "epoch": 8.392948390393459,
      "grad_norm": 3.945683479309082,
      "learning_rate": 4.300587634133878e-05,
      "loss": 0.6645,
      "step": 919800
    },
    {
      "epoch": 8.393860865756624,
      "grad_norm": 3.9775657653808594,
      "learning_rate": 4.300511594520282e-05,
      "loss": 0.6932,
      "step": 919900
    },
    {
      "epoch": 8.39477334111979,
      "grad_norm": 4.408447742462158,
      "learning_rate": 4.300435554906684e-05,
      "loss": 0.6741,
      "step": 920000
    },
    {
      "epoch": 8.395685816482954,
      "grad_norm": 3.390110969543457,
      "learning_rate": 4.300359515293087e-05,
      "loss": 0.6671,
      "step": 920100
    },
    {
      "epoch": 8.39659829184612,
      "grad_norm": 4.378751754760742,
      "learning_rate": 4.30028347567949e-05,
      "loss": 0.6917,
      "step": 920200
    },
    {
      "epoch": 8.397510767209285,
      "grad_norm": 4.6700968742370605,
      "learning_rate": 4.3002074360658926e-05,
      "loss": 0.7233,
      "step": 920300
    },
    {
      "epoch": 8.39842324257245,
      "grad_norm": 4.1554999351501465,
      "learning_rate": 4.300131396452296e-05,
      "loss": 0.726,
      "step": 920400
    },
    {
      "epoch": 8.399335717935616,
      "grad_norm": 3.734680652618408,
      "learning_rate": 4.3000553568386986e-05,
      "loss": 0.67,
      "step": 920500
    },
    {
      "epoch": 8.400248193298781,
      "grad_norm": 3.0936365127563477,
      "learning_rate": 4.2999793172251016e-05,
      "loss": 0.705,
      "step": 920600
    },
    {
      "epoch": 8.401160668661946,
      "grad_norm": 3.8574023246765137,
      "learning_rate": 4.2999032776115046e-05,
      "loss": 0.706,
      "step": 920700
    },
    {
      "epoch": 8.402073144025112,
      "grad_norm": 4.2245965003967285,
      "learning_rate": 4.2998272379979076e-05,
      "loss": 0.6874,
      "step": 920800
    },
    {
      "epoch": 8.402985619388277,
      "grad_norm": 4.676945686340332,
      "learning_rate": 4.29975119838431e-05,
      "loss": 0.706,
      "step": 920900
    },
    {
      "epoch": 8.403898094751442,
      "grad_norm": 4.029869556427002,
      "learning_rate": 4.2996751587707137e-05,
      "loss": 0.6947,
      "step": 921000
    },
    {
      "epoch": 8.404810570114607,
      "grad_norm": 4.104787826538086,
      "learning_rate": 4.299599119157116e-05,
      "loss": 0.6799,
      "step": 921100
    },
    {
      "epoch": 8.405723045477773,
      "grad_norm": 4.070987224578857,
      "learning_rate": 4.299523079543519e-05,
      "loss": 0.6753,
      "step": 921200
    },
    {
      "epoch": 8.406635520840938,
      "grad_norm": 3.4864654541015625,
      "learning_rate": 4.299447039929922e-05,
      "loss": 0.6935,
      "step": 921300
    },
    {
      "epoch": 8.407547996204103,
      "grad_norm": 4.106379508972168,
      "learning_rate": 4.299371000316325e-05,
      "loss": 0.6869,
      "step": 921400
    },
    {
      "epoch": 8.408460471567267,
      "grad_norm": 3.673173666000366,
      "learning_rate": 4.299294960702728e-05,
      "loss": 0.7057,
      "step": 921500
    },
    {
      "epoch": 8.409372946930432,
      "grad_norm": 3.478827714920044,
      "learning_rate": 4.299218921089131e-05,
      "loss": 0.6828,
      "step": 921600
    },
    {
      "epoch": 8.410285422293597,
      "grad_norm": 5.402975559234619,
      "learning_rate": 4.2991428814755333e-05,
      "loss": 0.7155,
      "step": 921700
    },
    {
      "epoch": 8.411197897656763,
      "grad_norm": 2.9330787658691406,
      "learning_rate": 4.299066841861937e-05,
      "loss": 0.6768,
      "step": 921800
    },
    {
      "epoch": 8.412110373019928,
      "grad_norm": 2.6334116458892822,
      "learning_rate": 4.2989908022483394e-05,
      "loss": 0.6874,
      "step": 921900
    },
    {
      "epoch": 8.413022848383093,
      "grad_norm": 3.8905439376831055,
      "learning_rate": 4.2989147626347424e-05,
      "loss": 0.6765,
      "step": 922000
    },
    {
      "epoch": 8.413935323746259,
      "grad_norm": 4.473351955413818,
      "learning_rate": 4.2988387230211454e-05,
      "loss": 0.7131,
      "step": 922100
    },
    {
      "epoch": 8.414847799109424,
      "grad_norm": 4.230125427246094,
      "learning_rate": 4.2987626834075484e-05,
      "loss": 0.6428,
      "step": 922200
    },
    {
      "epoch": 8.41576027447259,
      "grad_norm": 4.623242378234863,
      "learning_rate": 4.298686643793951e-05,
      "loss": 0.7429,
      "step": 922300
    },
    {
      "epoch": 8.416672749835755,
      "grad_norm": 4.806014060974121,
      "learning_rate": 4.2986106041803544e-05,
      "loss": 0.7237,
      "step": 922400
    },
    {
      "epoch": 8.41758522519892,
      "grad_norm": 4.563775539398193,
      "learning_rate": 4.298534564566757e-05,
      "loss": 0.7347,
      "step": 922500
    },
    {
      "epoch": 8.418497700562085,
      "grad_norm": 4.240817070007324,
      "learning_rate": 4.29845852495316e-05,
      "loss": 0.6927,
      "step": 922600
    },
    {
      "epoch": 8.41941017592525,
      "grad_norm": 5.562720775604248,
      "learning_rate": 4.298382485339563e-05,
      "loss": 0.6956,
      "step": 922700
    },
    {
      "epoch": 8.420322651288416,
      "grad_norm": 3.5669736862182617,
      "learning_rate": 4.298306445725966e-05,
      "loss": 0.6612,
      "step": 922800
    },
    {
      "epoch": 8.421235126651581,
      "grad_norm": 3.902909994125366,
      "learning_rate": 4.298230406112369e-05,
      "loss": 0.695,
      "step": 922900
    },
    {
      "epoch": 8.422147602014746,
      "grad_norm": 4.012169361114502,
      "learning_rate": 4.298154366498772e-05,
      "loss": 0.7103,
      "step": 923000
    },
    {
      "epoch": 8.423060077377912,
      "grad_norm": 4.462925434112549,
      "learning_rate": 4.298078326885174e-05,
      "loss": 0.6698,
      "step": 923100
    },
    {
      "epoch": 8.423972552741075,
      "grad_norm": 3.3578896522521973,
      "learning_rate": 4.298002287271577e-05,
      "loss": 0.7297,
      "step": 923200
    },
    {
      "epoch": 8.42488502810424,
      "grad_norm": 4.193613529205322,
      "learning_rate": 4.29792624765798e-05,
      "loss": 0.7099,
      "step": 923300
    },
    {
      "epoch": 8.425797503467406,
      "grad_norm": 4.707058906555176,
      "learning_rate": 4.2978502080443824e-05,
      "loss": 0.688,
      "step": 923400
    },
    {
      "epoch": 8.426709978830571,
      "grad_norm": 3.751340866088867,
      "learning_rate": 4.297774168430786e-05,
      "loss": 0.7085,
      "step": 923500
    },
    {
      "epoch": 8.427622454193736,
      "grad_norm": 4.215314865112305,
      "learning_rate": 4.2976981288171884e-05,
      "loss": 0.7161,
      "step": 923600
    },
    {
      "epoch": 8.428534929556902,
      "grad_norm": 4.332005023956299,
      "learning_rate": 4.2976220892035914e-05,
      "loss": 0.6772,
      "step": 923700
    },
    {
      "epoch": 8.429447404920067,
      "grad_norm": 3.397000312805176,
      "learning_rate": 4.2975460495899945e-05,
      "loss": 0.705,
      "step": 923800
    },
    {
      "epoch": 8.430359880283232,
      "grad_norm": 4.273595333099365,
      "learning_rate": 4.2974700099763975e-05,
      "loss": 0.689,
      "step": 923900
    },
    {
      "epoch": 8.431272355646398,
      "grad_norm": 3.7249178886413574,
      "learning_rate": 4.2973939703628005e-05,
      "loss": 0.7084,
      "step": 924000
    },
    {
      "epoch": 8.432184831009563,
      "grad_norm": 4.2104973793029785,
      "learning_rate": 4.2973179307492035e-05,
      "loss": 0.6746,
      "step": 924100
    },
    {
      "epoch": 8.433097306372728,
      "grad_norm": 3.523674488067627,
      "learning_rate": 4.297241891135606e-05,
      "loss": 0.7282,
      "step": 924200
    },
    {
      "epoch": 8.434009781735893,
      "grad_norm": 3.3104758262634277,
      "learning_rate": 4.2971658515220095e-05,
      "loss": 0.6756,
      "step": 924300
    },
    {
      "epoch": 8.434922257099059,
      "grad_norm": 3.598187208175659,
      "learning_rate": 4.297089811908412e-05,
      "loss": 0.6982,
      "step": 924400
    },
    {
      "epoch": 8.435834732462224,
      "grad_norm": 4.198276996612549,
      "learning_rate": 4.297013772294815e-05,
      "loss": 0.705,
      "step": 924500
    },
    {
      "epoch": 8.43674720782539,
      "grad_norm": 4.026358604431152,
      "learning_rate": 4.296937732681218e-05,
      "loss": 0.6651,
      "step": 924600
    },
    {
      "epoch": 8.437659683188555,
      "grad_norm": 4.416149139404297,
      "learning_rate": 4.296861693067621e-05,
      "loss": 0.706,
      "step": 924700
    },
    {
      "epoch": 8.43857215855172,
      "grad_norm": 4.332330703735352,
      "learning_rate": 4.296785653454024e-05,
      "loss": 0.6863,
      "step": 924800
    },
    {
      "epoch": 8.439484633914883,
      "grad_norm": 4.548913478851318,
      "learning_rate": 4.296709613840427e-05,
      "loss": 0.6798,
      "step": 924900
    },
    {
      "epoch": 8.440397109278049,
      "grad_norm": 2.585111141204834,
      "learning_rate": 4.296633574226829e-05,
      "loss": 0.6446,
      "step": 925000
    },
    {
      "epoch": 8.441309584641214,
      "grad_norm": 4.209070205688477,
      "learning_rate": 4.296557534613232e-05,
      "loss": 0.7084,
      "step": 925100
    },
    {
      "epoch": 8.44222206000438,
      "grad_norm": 3.613788366317749,
      "learning_rate": 4.296481494999635e-05,
      "loss": 0.6777,
      "step": 925200
    },
    {
      "epoch": 8.443134535367545,
      "grad_norm": 4.34600305557251,
      "learning_rate": 4.296405455386038e-05,
      "loss": 0.7342,
      "step": 925300
    },
    {
      "epoch": 8.44404701073071,
      "grad_norm": 3.8210103511810303,
      "learning_rate": 4.296329415772441e-05,
      "loss": 0.7179,
      "step": 925400
    },
    {
      "epoch": 8.444959486093875,
      "grad_norm": 3.6017961502075195,
      "learning_rate": 4.296253376158844e-05,
      "loss": 0.6901,
      "step": 925500
    },
    {
      "epoch": 8.44587196145704,
      "grad_norm": 3.413083076477051,
      "learning_rate": 4.2961773365452465e-05,
      "loss": 0.6704,
      "step": 925600
    },
    {
      "epoch": 8.446784436820206,
      "grad_norm": 2.939394474029541,
      "learning_rate": 4.29610129693165e-05,
      "loss": 0.6842,
      "step": 925700
    },
    {
      "epoch": 8.447696912183371,
      "grad_norm": 4.304088592529297,
      "learning_rate": 4.2960252573180526e-05,
      "loss": 0.7179,
      "step": 925800
    },
    {
      "epoch": 8.448609387546536,
      "grad_norm": 3.677213191986084,
      "learning_rate": 4.2959492177044556e-05,
      "loss": 0.711,
      "step": 925900
    },
    {
      "epoch": 8.449521862909702,
      "grad_norm": 3.438796281814575,
      "learning_rate": 4.2958731780908586e-05,
      "loss": 0.7023,
      "step": 926000
    },
    {
      "epoch": 8.450434338272867,
      "grad_norm": 3.4581105709075928,
      "learning_rate": 4.295797138477261e-05,
      "loss": 0.6774,
      "step": 926100
    },
    {
      "epoch": 8.451346813636032,
      "grad_norm": 3.6093826293945312,
      "learning_rate": 4.2957210988636646e-05,
      "loss": 0.717,
      "step": 926200
    },
    {
      "epoch": 8.452259288999198,
      "grad_norm": 3.3679358959198,
      "learning_rate": 4.295645059250067e-05,
      "loss": 0.6905,
      "step": 926300
    },
    {
      "epoch": 8.453171764362363,
      "grad_norm": 4.481640338897705,
      "learning_rate": 4.29556901963647e-05,
      "loss": 0.7099,
      "step": 926400
    },
    {
      "epoch": 8.454084239725528,
      "grad_norm": 4.854067325592041,
      "learning_rate": 4.295492980022873e-05,
      "loss": 0.6794,
      "step": 926500
    },
    {
      "epoch": 8.454996715088692,
      "grad_norm": 3.6919541358947754,
      "learning_rate": 4.295416940409276e-05,
      "loss": 0.7254,
      "step": 926600
    },
    {
      "epoch": 8.455909190451857,
      "grad_norm": 4.612982749938965,
      "learning_rate": 4.295340900795678e-05,
      "loss": 0.6822,
      "step": 926700
    },
    {
      "epoch": 8.456821665815022,
      "grad_norm": 5.219628810882568,
      "learning_rate": 4.295264861182082e-05,
      "loss": 0.7354,
      "step": 926800
    },
    {
      "epoch": 8.457734141178188,
      "grad_norm": 4.474000930786133,
      "learning_rate": 4.295188821568484e-05,
      "loss": 0.655,
      "step": 926900
    },
    {
      "epoch": 8.458646616541353,
      "grad_norm": 4.765678405761719,
      "learning_rate": 4.295112781954887e-05,
      "loss": 0.7356,
      "step": 927000
    },
    {
      "epoch": 8.459559091904518,
      "grad_norm": 4.523629188537598,
      "learning_rate": 4.29503674234129e-05,
      "loss": 0.6878,
      "step": 927100
    },
    {
      "epoch": 8.460471567267684,
      "grad_norm": 4.226830959320068,
      "learning_rate": 4.294960702727693e-05,
      "loss": 0.711,
      "step": 927200
    },
    {
      "epoch": 8.461384042630849,
      "grad_norm": 3.9884538650512695,
      "learning_rate": 4.294884663114096e-05,
      "loss": 0.6869,
      "step": 927300
    },
    {
      "epoch": 8.462296517994014,
      "grad_norm": 2.628687620162964,
      "learning_rate": 4.294808623500499e-05,
      "loss": 0.7413,
      "step": 927400
    },
    {
      "epoch": 8.46320899335718,
      "grad_norm": 3.4836654663085938,
      "learning_rate": 4.2947325838869016e-05,
      "loss": 0.6996,
      "step": 927500
    },
    {
      "epoch": 8.464121468720345,
      "grad_norm": 4.267566204071045,
      "learning_rate": 4.294656544273305e-05,
      "loss": 0.6856,
      "step": 927600
    },
    {
      "epoch": 8.46503394408351,
      "grad_norm": 3.647613525390625,
      "learning_rate": 4.2945805046597076e-05,
      "loss": 0.6782,
      "step": 927700
    },
    {
      "epoch": 8.465946419446675,
      "grad_norm": 3.7128446102142334,
      "learning_rate": 4.2945044650461107e-05,
      "loss": 0.7082,
      "step": 927800
    },
    {
      "epoch": 8.46685889480984,
      "grad_norm": 4.408089637756348,
      "learning_rate": 4.294428425432514e-05,
      "loss": 0.7319,
      "step": 927900
    },
    {
      "epoch": 8.467771370173006,
      "grad_norm": 4.069081783294678,
      "learning_rate": 4.294352385818917e-05,
      "loss": 0.6529,
      "step": 928000
    },
    {
      "epoch": 8.468683845536171,
      "grad_norm": 3.608491897583008,
      "learning_rate": 4.294276346205319e-05,
      "loss": 0.6759,
      "step": 928100
    },
    {
      "epoch": 8.469596320899337,
      "grad_norm": 3.7213845252990723,
      "learning_rate": 4.294200306591723e-05,
      "loss": 0.7076,
      "step": 928200
    },
    {
      "epoch": 8.4705087962625,
      "grad_norm": 4.346243381500244,
      "learning_rate": 4.294124266978125e-05,
      "loss": 0.7046,
      "step": 928300
    },
    {
      "epoch": 8.471421271625665,
      "grad_norm": 4.360228061676025,
      "learning_rate": 4.294048227364528e-05,
      "loss": 0.7307,
      "step": 928400
    },
    {
      "epoch": 8.47233374698883,
      "grad_norm": 3.0334978103637695,
      "learning_rate": 4.293972187750931e-05,
      "loss": 0.6981,
      "step": 928500
    },
    {
      "epoch": 8.473246222351996,
      "grad_norm": 4.282708168029785,
      "learning_rate": 4.293896148137334e-05,
      "loss": 0.636,
      "step": 928600
    },
    {
      "epoch": 8.474158697715161,
      "grad_norm": 3.076488733291626,
      "learning_rate": 4.293820108523737e-05,
      "loss": 0.6848,
      "step": 928700
    },
    {
      "epoch": 8.475071173078327,
      "grad_norm": 4.70486307144165,
      "learning_rate": 4.2937440689101394e-05,
      "loss": 0.7203,
      "step": 928800
    },
    {
      "epoch": 8.475983648441492,
      "grad_norm": 2.994544744491577,
      "learning_rate": 4.2936680292965424e-05,
      "loss": 0.6824,
      "step": 928900
    },
    {
      "epoch": 8.476896123804657,
      "grad_norm": 3.184018611907959,
      "learning_rate": 4.2935919896829454e-05,
      "loss": 0.6973,
      "step": 929000
    },
    {
      "epoch": 8.477808599167822,
      "grad_norm": 3.924750566482544,
      "learning_rate": 4.2935159500693484e-05,
      "loss": 0.6508,
      "step": 929100
    },
    {
      "epoch": 8.478721074530988,
      "grad_norm": 4.318126201629639,
      "learning_rate": 4.293439910455751e-05,
      "loss": 0.6967,
      "step": 929200
    },
    {
      "epoch": 8.479633549894153,
      "grad_norm": 3.914992332458496,
      "learning_rate": 4.2933638708421544e-05,
      "loss": 0.6753,
      "step": 929300
    },
    {
      "epoch": 8.480546025257318,
      "grad_norm": 3.507265567779541,
      "learning_rate": 4.293287831228557e-05,
      "loss": 0.7108,
      "step": 929400
    },
    {
      "epoch": 8.481458500620484,
      "grad_norm": 4.357921123504639,
      "learning_rate": 4.29321179161496e-05,
      "loss": 0.6855,
      "step": 929500
    },
    {
      "epoch": 8.482370975983649,
      "grad_norm": 3.7227354049682617,
      "learning_rate": 4.293135752001363e-05,
      "loss": 0.7208,
      "step": 929600
    },
    {
      "epoch": 8.483283451346814,
      "grad_norm": 5.706446170806885,
      "learning_rate": 4.293059712387766e-05,
      "loss": 0.6991,
      "step": 929700
    },
    {
      "epoch": 8.48419592670998,
      "grad_norm": 4.307857513427734,
      "learning_rate": 4.292983672774169e-05,
      "loss": 0.7243,
      "step": 929800
    },
    {
      "epoch": 8.485108402073145,
      "grad_norm": 4.351646423339844,
      "learning_rate": 4.292907633160572e-05,
      "loss": 0.6959,
      "step": 929900
    },
    {
      "epoch": 8.486020877436308,
      "grad_norm": 2.8590245246887207,
      "learning_rate": 4.292831593546974e-05,
      "loss": 0.719,
      "step": 930000
    },
    {
      "epoch": 8.486933352799474,
      "grad_norm": 4.281290054321289,
      "learning_rate": 4.292755553933378e-05,
      "loss": 0.6829,
      "step": 930100
    },
    {
      "epoch": 8.487845828162639,
      "grad_norm": 3.9819271564483643,
      "learning_rate": 4.29267951431978e-05,
      "loss": 0.6819,
      "step": 930200
    },
    {
      "epoch": 8.488758303525804,
      "grad_norm": 4.524006366729736,
      "learning_rate": 4.292603474706183e-05,
      "loss": 0.659,
      "step": 930300
    },
    {
      "epoch": 8.48967077888897,
      "grad_norm": 4.850165843963623,
      "learning_rate": 4.292527435092586e-05,
      "loss": 0.6846,
      "step": 930400
    },
    {
      "epoch": 8.490583254252135,
      "grad_norm": 4.241346836090088,
      "learning_rate": 4.292451395478989e-05,
      "loss": 0.6959,
      "step": 930500
    },
    {
      "epoch": 8.4914957296153,
      "grad_norm": 4.62457275390625,
      "learning_rate": 4.2923753558653915e-05,
      "loss": 0.6859,
      "step": 930600
    },
    {
      "epoch": 8.492408204978465,
      "grad_norm": 3.6817970275878906,
      "learning_rate": 4.292299316251795e-05,
      "loss": 0.6777,
      "step": 930700
    },
    {
      "epoch": 8.49332068034163,
      "grad_norm": 4.114397048950195,
      "learning_rate": 4.2922232766381975e-05,
      "loss": 0.6945,
      "step": 930800
    },
    {
      "epoch": 8.494233155704796,
      "grad_norm": 4.381923675537109,
      "learning_rate": 4.2921472370246005e-05,
      "loss": 0.682,
      "step": 930900
    },
    {
      "epoch": 8.495145631067961,
      "grad_norm": 4.2336249351501465,
      "learning_rate": 4.2920711974110035e-05,
      "loss": 0.7354,
      "step": 931000
    },
    {
      "epoch": 8.496058106431127,
      "grad_norm": 3.6348021030426025,
      "learning_rate": 4.2919951577974065e-05,
      "loss": 0.7248,
      "step": 931100
    },
    {
      "epoch": 8.496970581794292,
      "grad_norm": 3.9921913146972656,
      "learning_rate": 4.2919191181838095e-05,
      "loss": 0.6875,
      "step": 931200
    },
    {
      "epoch": 8.497883057157457,
      "grad_norm": 3.4525582790374756,
      "learning_rate": 4.2918430785702125e-05,
      "loss": 0.721,
      "step": 931300
    },
    {
      "epoch": 8.498795532520623,
      "grad_norm": 2.828700542449951,
      "learning_rate": 4.291767038956615e-05,
      "loss": 0.7077,
      "step": 931400
    },
    {
      "epoch": 8.499708007883788,
      "grad_norm": 5.164009094238281,
      "learning_rate": 4.2916909993430185e-05,
      "loss": 0.6619,
      "step": 931500
    },
    {
      "epoch": 8.500620483246951,
      "grad_norm": 4.779816627502441,
      "learning_rate": 4.291614959729421e-05,
      "loss": 0.719,
      "step": 931600
    },
    {
      "epoch": 8.501532958610117,
      "grad_norm": 2.5595078468322754,
      "learning_rate": 4.291538920115823e-05,
      "loss": 0.6988,
      "step": 931700
    },
    {
      "epoch": 8.502445433973282,
      "grad_norm": 3.7351009845733643,
      "learning_rate": 4.291462880502227e-05,
      "loss": 0.6864,
      "step": 931800
    },
    {
      "epoch": 8.503357909336447,
      "grad_norm": 3.7129979133605957,
      "learning_rate": 4.291386840888629e-05,
      "loss": 0.6594,
      "step": 931900
    },
    {
      "epoch": 8.504270384699613,
      "grad_norm": 4.723987579345703,
      "learning_rate": 4.291310801275032e-05,
      "loss": 0.6707,
      "step": 932000
    },
    {
      "epoch": 8.505182860062778,
      "grad_norm": 4.564173221588135,
      "learning_rate": 4.291234761661435e-05,
      "loss": 0.695,
      "step": 932100
    },
    {
      "epoch": 8.506095335425943,
      "grad_norm": 3.9447848796844482,
      "learning_rate": 4.291158722047838e-05,
      "loss": 0.6988,
      "step": 932200
    },
    {
      "epoch": 8.507007810789109,
      "grad_norm": 4.316539287567139,
      "learning_rate": 4.291082682434241e-05,
      "loss": 0.6915,
      "step": 932300
    },
    {
      "epoch": 8.507920286152274,
      "grad_norm": 4.80819034576416,
      "learning_rate": 4.291006642820644e-05,
      "loss": 0.7133,
      "step": 932400
    },
    {
      "epoch": 8.508832761515439,
      "grad_norm": 3.713784694671631,
      "learning_rate": 4.2909306032070466e-05,
      "loss": 0.7056,
      "step": 932500
    },
    {
      "epoch": 8.509745236878604,
      "grad_norm": 3.2157106399536133,
      "learning_rate": 4.29085456359345e-05,
      "loss": 0.6935,
      "step": 932600
    },
    {
      "epoch": 8.51065771224177,
      "grad_norm": 4.04204797744751,
      "learning_rate": 4.2907785239798526e-05,
      "loss": 0.6447,
      "step": 932700
    },
    {
      "epoch": 8.511570187604935,
      "grad_norm": 3.763472318649292,
      "learning_rate": 4.2907024843662556e-05,
      "loss": 0.7393,
      "step": 932800
    },
    {
      "epoch": 8.5124826629681,
      "grad_norm": 4.609529495239258,
      "learning_rate": 4.2906264447526586e-05,
      "loss": 0.6944,
      "step": 932900
    },
    {
      "epoch": 8.513395138331266,
      "grad_norm": 4.254741668701172,
      "learning_rate": 4.2905504051390616e-05,
      "loss": 0.6972,
      "step": 933000
    },
    {
      "epoch": 8.514307613694431,
      "grad_norm": 3.924105644226074,
      "learning_rate": 4.290474365525464e-05,
      "loss": 0.6616,
      "step": 933100
    },
    {
      "epoch": 8.515220089057596,
      "grad_norm": 3.558948516845703,
      "learning_rate": 4.2903983259118676e-05,
      "loss": 0.6985,
      "step": 933200
    },
    {
      "epoch": 8.516132564420761,
      "grad_norm": 4.414175510406494,
      "learning_rate": 4.29032228629827e-05,
      "loss": 0.7091,
      "step": 933300
    },
    {
      "epoch": 8.517045039783925,
      "grad_norm": 4.591301918029785,
      "learning_rate": 4.290246246684673e-05,
      "loss": 0.7072,
      "step": 933400
    },
    {
      "epoch": 8.51795751514709,
      "grad_norm": 4.617011547088623,
      "learning_rate": 4.290170207071076e-05,
      "loss": 0.6891,
      "step": 933500
    },
    {
      "epoch": 8.518869990510256,
      "grad_norm": 3.3121681213378906,
      "learning_rate": 4.290094167457479e-05,
      "loss": 0.694,
      "step": 933600
    },
    {
      "epoch": 8.519782465873421,
      "grad_norm": 3.2786638736724854,
      "learning_rate": 4.290018127843882e-05,
      "loss": 0.7168,
      "step": 933700
    },
    {
      "epoch": 8.520694941236586,
      "grad_norm": 4.590315341949463,
      "learning_rate": 4.289942088230285e-05,
      "loss": 0.7033,
      "step": 933800
    },
    {
      "epoch": 8.521607416599752,
      "grad_norm": 4.554507255554199,
      "learning_rate": 4.289866048616687e-05,
      "loss": 0.6998,
      "step": 933900
    },
    {
      "epoch": 8.522519891962917,
      "grad_norm": 4.041472911834717,
      "learning_rate": 4.289790009003091e-05,
      "loss": 0.6794,
      "step": 934000
    },
    {
      "epoch": 8.523432367326082,
      "grad_norm": 3.964993953704834,
      "learning_rate": 4.289713969389493e-05,
      "loss": 0.6646,
      "step": 934100
    },
    {
      "epoch": 8.524344842689247,
      "grad_norm": 4.22160005569458,
      "learning_rate": 4.289637929775896e-05,
      "loss": 0.6876,
      "step": 934200
    },
    {
      "epoch": 8.525257318052413,
      "grad_norm": 3.7093753814697266,
      "learning_rate": 4.289561890162299e-05,
      "loss": 0.7073,
      "step": 934300
    },
    {
      "epoch": 8.526169793415578,
      "grad_norm": 4.52933931350708,
      "learning_rate": 4.289485850548702e-05,
      "loss": 0.6714,
      "step": 934400
    },
    {
      "epoch": 8.527082268778743,
      "grad_norm": 3.3797290325164795,
      "learning_rate": 4.2894098109351047e-05,
      "loss": 0.6592,
      "step": 934500
    },
    {
      "epoch": 8.527994744141909,
      "grad_norm": 3.5388522148132324,
      "learning_rate": 4.2893337713215077e-05,
      "loss": 0.6578,
      "step": 934600
    },
    {
      "epoch": 8.528907219505074,
      "grad_norm": 3.158256769180298,
      "learning_rate": 4.289257731707911e-05,
      "loss": 0.7078,
      "step": 934700
    },
    {
      "epoch": 8.52981969486824,
      "grad_norm": 5.015801429748535,
      "learning_rate": 4.289181692094314e-05,
      "loss": 0.6925,
      "step": 934800
    },
    {
      "epoch": 8.530732170231405,
      "grad_norm": 4.841044902801514,
      "learning_rate": 4.289105652480717e-05,
      "loss": 0.6887,
      "step": 934900
    },
    {
      "epoch": 8.531644645594568,
      "grad_norm": 4.485329627990723,
      "learning_rate": 4.289029612867119e-05,
      "loss": 0.6621,
      "step": 935000
    },
    {
      "epoch": 8.532557120957733,
      "grad_norm": 3.1783039569854736,
      "learning_rate": 4.288953573253523e-05,
      "loss": 0.682,
      "step": 935100
    },
    {
      "epoch": 8.533469596320899,
      "grad_norm": 3.822097063064575,
      "learning_rate": 4.288877533639925e-05,
      "loss": 0.685,
      "step": 935200
    },
    {
      "epoch": 8.534382071684064,
      "grad_norm": 4.128735065460205,
      "learning_rate": 4.288801494026328e-05,
      "loss": 0.6977,
      "step": 935300
    },
    {
      "epoch": 8.53529454704723,
      "grad_norm": 4.7984113693237305,
      "learning_rate": 4.288725454412731e-05,
      "loss": 0.6895,
      "step": 935400
    },
    {
      "epoch": 8.536207022410395,
      "grad_norm": 4.1758341789245605,
      "learning_rate": 4.288649414799134e-05,
      "loss": 0.678,
      "step": 935500
    },
    {
      "epoch": 8.53711949777356,
      "grad_norm": 3.2647829055786133,
      "learning_rate": 4.2885733751855364e-05,
      "loss": 0.6219,
      "step": 935600
    },
    {
      "epoch": 8.538031973136725,
      "grad_norm": 3.612480640411377,
      "learning_rate": 4.28849733557194e-05,
      "loss": 0.658,
      "step": 935700
    },
    {
      "epoch": 8.53894444849989,
      "grad_norm": 3.4187300205230713,
      "learning_rate": 4.2884212959583424e-05,
      "loss": 0.6866,
      "step": 935800
    },
    {
      "epoch": 8.539856923863056,
      "grad_norm": 4.633886337280273,
      "learning_rate": 4.2883452563447454e-05,
      "loss": 0.7208,
      "step": 935900
    },
    {
      "epoch": 8.540769399226221,
      "grad_norm": 4.349327087402344,
      "learning_rate": 4.2882692167311484e-05,
      "loss": 0.6721,
      "step": 936000
    },
    {
      "epoch": 8.541681874589386,
      "grad_norm": 4.011903762817383,
      "learning_rate": 4.2881931771175514e-05,
      "loss": 0.6804,
      "step": 936100
    },
    {
      "epoch": 8.542594349952552,
      "grad_norm": 4.529149532318115,
      "learning_rate": 4.2881171375039544e-05,
      "loss": 0.6578,
      "step": 936200
    },
    {
      "epoch": 8.543506825315717,
      "grad_norm": 3.2186732292175293,
      "learning_rate": 4.2880410978903574e-05,
      "loss": 0.7149,
      "step": 936300
    },
    {
      "epoch": 8.544419300678882,
      "grad_norm": 3.9136898517608643,
      "learning_rate": 4.28796505827676e-05,
      "loss": 0.6861,
      "step": 936400
    },
    {
      "epoch": 8.545331776042048,
      "grad_norm": 4.987484931945801,
      "learning_rate": 4.2878890186631634e-05,
      "loss": 0.7151,
      "step": 936500
    },
    {
      "epoch": 8.546244251405213,
      "grad_norm": 4.216484546661377,
      "learning_rate": 4.287812979049566e-05,
      "loss": 0.683,
      "step": 936600
    },
    {
      "epoch": 8.547156726768378,
      "grad_norm": 4.45297384262085,
      "learning_rate": 4.287736939435969e-05,
      "loss": 0.6641,
      "step": 936700
    },
    {
      "epoch": 8.548069202131542,
      "grad_norm": 4.263337135314941,
      "learning_rate": 4.287660899822372e-05,
      "loss": 0.6833,
      "step": 936800
    },
    {
      "epoch": 8.548981677494707,
      "grad_norm": 4.081191062927246,
      "learning_rate": 4.287584860208775e-05,
      "loss": 0.709,
      "step": 936900
    },
    {
      "epoch": 8.549894152857872,
      "grad_norm": 3.360619068145752,
      "learning_rate": 4.287508820595177e-05,
      "loss": 0.7083,
      "step": 937000
    },
    {
      "epoch": 8.550806628221038,
      "grad_norm": 2.9746217727661133,
      "learning_rate": 4.287432780981581e-05,
      "loss": 0.6761,
      "step": 937100
    },
    {
      "epoch": 8.551719103584203,
      "grad_norm": 3.7696592807769775,
      "learning_rate": 4.287356741367983e-05,
      "loss": 0.6752,
      "step": 937200
    },
    {
      "epoch": 8.552631578947368,
      "grad_norm": 5.242902755737305,
      "learning_rate": 4.287280701754386e-05,
      "loss": 0.7218,
      "step": 937300
    },
    {
      "epoch": 8.553544054310533,
      "grad_norm": 4.019429683685303,
      "learning_rate": 4.287204662140789e-05,
      "loss": 0.6985,
      "step": 937400
    },
    {
      "epoch": 8.554456529673699,
      "grad_norm": 3.8114452362060547,
      "learning_rate": 4.2871286225271915e-05,
      "loss": 0.719,
      "step": 937500
    },
    {
      "epoch": 8.555369005036864,
      "grad_norm": 4.25385856628418,
      "learning_rate": 4.287052582913595e-05,
      "loss": 0.7012,
      "step": 937600
    },
    {
      "epoch": 8.55628148040003,
      "grad_norm": 4.582767963409424,
      "learning_rate": 4.2869765432999975e-05,
      "loss": 0.6721,
      "step": 937700
    },
    {
      "epoch": 8.557193955763195,
      "grad_norm": 3.8929951190948486,
      "learning_rate": 4.2869005036864005e-05,
      "loss": 0.6733,
      "step": 937800
    },
    {
      "epoch": 8.55810643112636,
      "grad_norm": 4.210599899291992,
      "learning_rate": 4.2868244640728035e-05,
      "loss": 0.705,
      "step": 937900
    },
    {
      "epoch": 8.559018906489525,
      "grad_norm": 2.6469385623931885,
      "learning_rate": 4.2867484244592065e-05,
      "loss": 0.671,
      "step": 938000
    },
    {
      "epoch": 8.55993138185269,
      "grad_norm": 3.8719482421875,
      "learning_rate": 4.2866723848456095e-05,
      "loss": 0.6749,
      "step": 938100
    },
    {
      "epoch": 8.560843857215856,
      "grad_norm": 3.97019624710083,
      "learning_rate": 4.2865963452320125e-05,
      "loss": 0.6768,
      "step": 938200
    },
    {
      "epoch": 8.561756332579021,
      "grad_norm": 4.549047946929932,
      "learning_rate": 4.286520305618415e-05,
      "loss": 0.7054,
      "step": 938300
    },
    {
      "epoch": 8.562668807942185,
      "grad_norm": 4.263016700744629,
      "learning_rate": 4.286444266004818e-05,
      "loss": 0.7079,
      "step": 938400
    },
    {
      "epoch": 8.56358128330535,
      "grad_norm": 3.3143537044525146,
      "learning_rate": 4.286368226391221e-05,
      "loss": 0.7084,
      "step": 938500
    },
    {
      "epoch": 8.564493758668515,
      "grad_norm": 4.320733070373535,
      "learning_rate": 4.286292186777624e-05,
      "loss": 0.7118,
      "step": 938600
    },
    {
      "epoch": 8.56540623403168,
      "grad_norm": 3.7243640422821045,
      "learning_rate": 4.286216147164027e-05,
      "loss": 0.6444,
      "step": 938700
    },
    {
      "epoch": 8.566318709394846,
      "grad_norm": 4.2086615562438965,
      "learning_rate": 4.28614010755043e-05,
      "loss": 0.7038,
      "step": 938800
    },
    {
      "epoch": 8.567231184758011,
      "grad_norm": 4.079308986663818,
      "learning_rate": 4.286064067936832e-05,
      "loss": 0.7135,
      "step": 938900
    },
    {
      "epoch": 8.568143660121176,
      "grad_norm": 4.054991245269775,
      "learning_rate": 4.285988028323236e-05,
      "loss": 0.6778,
      "step": 939000
    },
    {
      "epoch": 8.569056135484342,
      "grad_norm": 5.201022148132324,
      "learning_rate": 4.285911988709638e-05,
      "loss": 0.692,
      "step": 939100
    },
    {
      "epoch": 8.569968610847507,
      "grad_norm": 4.209438323974609,
      "learning_rate": 4.285835949096041e-05,
      "loss": 0.6988,
      "step": 939200
    },
    {
      "epoch": 8.570881086210672,
      "grad_norm": 2.8098833560943604,
      "learning_rate": 4.285759909482444e-05,
      "loss": 0.7057,
      "step": 939300
    },
    {
      "epoch": 8.571793561573838,
      "grad_norm": 4.715164661407471,
      "learning_rate": 4.285683869868847e-05,
      "loss": 0.7008,
      "step": 939400
    },
    {
      "epoch": 8.572706036937003,
      "grad_norm": 3.374596118927002,
      "learning_rate": 4.28560783025525e-05,
      "loss": 0.7117,
      "step": 939500
    },
    {
      "epoch": 8.573618512300168,
      "grad_norm": 4.367314338684082,
      "learning_rate": 4.285531790641653e-05,
      "loss": 0.6954,
      "step": 939600
    },
    {
      "epoch": 8.574530987663334,
      "grad_norm": 4.279378414154053,
      "learning_rate": 4.2854557510280556e-05,
      "loss": 0.6708,
      "step": 939700
    },
    {
      "epoch": 8.575443463026499,
      "grad_norm": 4.493437767028809,
      "learning_rate": 4.285379711414459e-05,
      "loss": 0.6974,
      "step": 939800
    },
    {
      "epoch": 8.576355938389664,
      "grad_norm": 3.8691189289093018,
      "learning_rate": 4.2853036718008616e-05,
      "loss": 0.693,
      "step": 939900
    },
    {
      "epoch": 8.57726841375283,
      "grad_norm": 3.7298593521118164,
      "learning_rate": 4.2852276321872646e-05,
      "loss": 0.6809,
      "step": 940000
    },
    {
      "epoch": 8.578180889115995,
      "grad_norm": 3.9047088623046875,
      "learning_rate": 4.2851515925736676e-05,
      "loss": 0.6677,
      "step": 940100
    },
    {
      "epoch": 8.579093364479158,
      "grad_norm": 4.0348615646362305,
      "learning_rate": 4.28507555296007e-05,
      "loss": 0.6567,
      "step": 940200
    },
    {
      "epoch": 8.580005839842324,
      "grad_norm": 3.639444351196289,
      "learning_rate": 4.284999513346473e-05,
      "loss": 0.707,
      "step": 940300
    },
    {
      "epoch": 8.580918315205489,
      "grad_norm": 4.170766353607178,
      "learning_rate": 4.284923473732876e-05,
      "loss": 0.6887,
      "step": 940400
    },
    {
      "epoch": 8.581830790568654,
      "grad_norm": 3.91451358795166,
      "learning_rate": 4.284847434119279e-05,
      "loss": 0.6804,
      "step": 940500
    },
    {
      "epoch": 8.58274326593182,
      "grad_norm": 3.5978353023529053,
      "learning_rate": 4.284771394505682e-05,
      "loss": 0.6871,
      "step": 940600
    },
    {
      "epoch": 8.583655741294985,
      "grad_norm": 4.3831377029418945,
      "learning_rate": 4.284695354892085e-05,
      "loss": 0.7131,
      "step": 940700
    },
    {
      "epoch": 8.58456821665815,
      "grad_norm": 4.880579471588135,
      "learning_rate": 4.284619315278487e-05,
      "loss": 0.6757,
      "step": 940800
    },
    {
      "epoch": 8.585480692021315,
      "grad_norm": 4.934750080108643,
      "learning_rate": 4.284543275664891e-05,
      "loss": 0.6366,
      "step": 940900
    },
    {
      "epoch": 8.58639316738448,
      "grad_norm": 4.129445552825928,
      "learning_rate": 4.284467236051293e-05,
      "loss": 0.7088,
      "step": 941000
    },
    {
      "epoch": 8.587305642747646,
      "grad_norm": 5.523000717163086,
      "learning_rate": 4.284391196437696e-05,
      "loss": 0.6723,
      "step": 941100
    },
    {
      "epoch": 8.588218118110811,
      "grad_norm": 3.2534589767456055,
      "learning_rate": 4.284315156824099e-05,
      "loss": 0.6736,
      "step": 941200
    },
    {
      "epoch": 8.589130593473977,
      "grad_norm": 4.072915077209473,
      "learning_rate": 4.284239117210502e-05,
      "loss": 0.6739,
      "step": 941300
    },
    {
      "epoch": 8.590043068837142,
      "grad_norm": 2.7079646587371826,
      "learning_rate": 4.284163077596905e-05,
      "loss": 0.6843,
      "step": 941400
    },
    {
      "epoch": 8.590955544200307,
      "grad_norm": 3.9166109561920166,
      "learning_rate": 4.2840870379833083e-05,
      "loss": 0.7249,
      "step": 941500
    },
    {
      "epoch": 8.591868019563472,
      "grad_norm": 4.4056549072265625,
      "learning_rate": 4.284010998369711e-05,
      "loss": 0.7104,
      "step": 941600
    },
    {
      "epoch": 8.592780494926638,
      "grad_norm": 3.9633278846740723,
      "learning_rate": 4.283934958756114e-05,
      "loss": 0.6889,
      "step": 941700
    },
    {
      "epoch": 8.593692970289801,
      "grad_norm": 4.455895900726318,
      "learning_rate": 4.283858919142517e-05,
      "loss": 0.6797,
      "step": 941800
    },
    {
      "epoch": 8.594605445652967,
      "grad_norm": 4.649496555328369,
      "learning_rate": 4.28378287952892e-05,
      "loss": 0.6884,
      "step": 941900
    },
    {
      "epoch": 8.595517921016132,
      "grad_norm": 3.24750018119812,
      "learning_rate": 4.283706839915323e-05,
      "loss": 0.6724,
      "step": 942000
    },
    {
      "epoch": 8.596430396379297,
      "grad_norm": 4.171895503997803,
      "learning_rate": 4.283630800301726e-05,
      "loss": 0.7577,
      "step": 942100
    },
    {
      "epoch": 8.597342871742462,
      "grad_norm": 4.207581043243408,
      "learning_rate": 4.283554760688128e-05,
      "loss": 0.6939,
      "step": 942200
    },
    {
      "epoch": 8.598255347105628,
      "grad_norm": 3.5732157230377197,
      "learning_rate": 4.283478721074532e-05,
      "loss": 0.6736,
      "step": 942300
    },
    {
      "epoch": 8.599167822468793,
      "grad_norm": 4.149338245391846,
      "learning_rate": 4.283402681460934e-05,
      "loss": 0.6944,
      "step": 942400
    },
    {
      "epoch": 8.600080297831958,
      "grad_norm": 3.779040813446045,
      "learning_rate": 4.283326641847337e-05,
      "loss": 0.7179,
      "step": 942500
    },
    {
      "epoch": 8.600992773195124,
      "grad_norm": 4.431803226470947,
      "learning_rate": 4.28325060223374e-05,
      "loss": 0.6804,
      "step": 942600
    },
    {
      "epoch": 8.601905248558289,
      "grad_norm": 3.5689868927001953,
      "learning_rate": 4.283174562620143e-05,
      "loss": 0.6979,
      "step": 942700
    },
    {
      "epoch": 8.602817723921454,
      "grad_norm": 3.8213696479797363,
      "learning_rate": 4.2830985230065454e-05,
      "loss": 0.6632,
      "step": 942800
    },
    {
      "epoch": 8.60373019928462,
      "grad_norm": 3.4232378005981445,
      "learning_rate": 4.283022483392949e-05,
      "loss": 0.7016,
      "step": 942900
    },
    {
      "epoch": 8.604642674647785,
      "grad_norm": 3.352644205093384,
      "learning_rate": 4.2829464437793514e-05,
      "loss": 0.6716,
      "step": 943000
    },
    {
      "epoch": 8.60555515001095,
      "grad_norm": 3.443665027618408,
      "learning_rate": 4.2828704041657544e-05,
      "loss": 0.6634,
      "step": 943100
    },
    {
      "epoch": 8.606467625374115,
      "grad_norm": 3.6235392093658447,
      "learning_rate": 4.2827943645521574e-05,
      "loss": 0.7239,
      "step": 943200
    },
    {
      "epoch": 8.60738010073728,
      "grad_norm": 4.51863956451416,
      "learning_rate": 4.28271832493856e-05,
      "loss": 0.7039,
      "step": 943300
    },
    {
      "epoch": 8.608292576100446,
      "grad_norm": 3.762866735458374,
      "learning_rate": 4.2826422853249634e-05,
      "loss": 0.6957,
      "step": 943400
    },
    {
      "epoch": 8.609205051463611,
      "grad_norm": 4.02010440826416,
      "learning_rate": 4.282566245711366e-05,
      "loss": 0.6799,
      "step": 943500
    },
    {
      "epoch": 8.610117526826775,
      "grad_norm": 4.564213275909424,
      "learning_rate": 4.282490206097769e-05,
      "loss": 0.6637,
      "step": 943600
    },
    {
      "epoch": 8.61103000218994,
      "grad_norm": 5.372439861297607,
      "learning_rate": 4.282414166484172e-05,
      "loss": 0.6818,
      "step": 943700
    },
    {
      "epoch": 8.611942477553105,
      "grad_norm": 4.964064598083496,
      "learning_rate": 4.282338126870575e-05,
      "loss": 0.7355,
      "step": 943800
    },
    {
      "epoch": 8.61285495291627,
      "grad_norm": 4.6709885597229,
      "learning_rate": 4.282262087256977e-05,
      "loss": 0.7044,
      "step": 943900
    },
    {
      "epoch": 8.613767428279436,
      "grad_norm": 3.752581834793091,
      "learning_rate": 4.282186047643381e-05,
      "loss": 0.6757,
      "step": 944000
    },
    {
      "epoch": 8.614679903642601,
      "grad_norm": 3.921313762664795,
      "learning_rate": 4.282110008029783e-05,
      "loss": 0.6607,
      "step": 944100
    },
    {
      "epoch": 8.615592379005767,
      "grad_norm": 3.400583505630493,
      "learning_rate": 4.282033968416186e-05,
      "loss": 0.7098,
      "step": 944200
    },
    {
      "epoch": 8.616504854368932,
      "grad_norm": 3.4105567932128906,
      "learning_rate": 4.281957928802589e-05,
      "loss": 0.6967,
      "step": 944300
    },
    {
      "epoch": 8.617417329732097,
      "grad_norm": 3.6789562702178955,
      "learning_rate": 4.281881889188992e-05,
      "loss": 0.6902,
      "step": 944400
    },
    {
      "epoch": 8.618329805095263,
      "grad_norm": 4.854008197784424,
      "learning_rate": 4.281805849575395e-05,
      "loss": 0.6785,
      "step": 944500
    },
    {
      "epoch": 8.619242280458428,
      "grad_norm": 4.339860916137695,
      "learning_rate": 4.281729809961798e-05,
      "loss": 0.707,
      "step": 944600
    },
    {
      "epoch": 8.620154755821593,
      "grad_norm": 4.234857559204102,
      "learning_rate": 4.2816537703482005e-05,
      "loss": 0.6924,
      "step": 944700
    },
    {
      "epoch": 8.621067231184758,
      "grad_norm": 3.3139452934265137,
      "learning_rate": 4.281577730734604e-05,
      "loss": 0.6772,
      "step": 944800
    },
    {
      "epoch": 8.621979706547924,
      "grad_norm": 4.443836688995361,
      "learning_rate": 4.2815016911210065e-05,
      "loss": 0.6559,
      "step": 944900
    },
    {
      "epoch": 8.622892181911089,
      "grad_norm": 4.097362041473389,
      "learning_rate": 4.2814256515074095e-05,
      "loss": 0.6982,
      "step": 945000
    },
    {
      "epoch": 8.623804657274254,
      "grad_norm": 4.113375186920166,
      "learning_rate": 4.2813496118938125e-05,
      "loss": 0.7105,
      "step": 945100
    },
    {
      "epoch": 8.624717132637418,
      "grad_norm": 4.085689067840576,
      "learning_rate": 4.2812735722802155e-05,
      "loss": 0.6646,
      "step": 945200
    },
    {
      "epoch": 8.625629608000583,
      "grad_norm": 4.477710247039795,
      "learning_rate": 4.281197532666618e-05,
      "loss": 0.7008,
      "step": 945300
    },
    {
      "epoch": 8.626542083363749,
      "grad_norm": 3.752272844314575,
      "learning_rate": 4.2811214930530215e-05,
      "loss": 0.6776,
      "step": 945400
    },
    {
      "epoch": 8.627454558726914,
      "grad_norm": 2.892942190170288,
      "learning_rate": 4.281045453439424e-05,
      "loss": 0.6691,
      "step": 945500
    },
    {
      "epoch": 8.628367034090079,
      "grad_norm": 3.641221523284912,
      "learning_rate": 4.280969413825827e-05,
      "loss": 0.6823,
      "step": 945600
    },
    {
      "epoch": 8.629279509453244,
      "grad_norm": 4.327759742736816,
      "learning_rate": 4.28089337421223e-05,
      "loss": 0.6909,
      "step": 945700
    },
    {
      "epoch": 8.63019198481641,
      "grad_norm": 3.9379308223724365,
      "learning_rate": 4.280817334598632e-05,
      "loss": 0.7102,
      "step": 945800
    },
    {
      "epoch": 8.631104460179575,
      "grad_norm": 3.7915306091308594,
      "learning_rate": 4.280741294985036e-05,
      "loss": 0.7136,
      "step": 945900
    },
    {
      "epoch": 8.63201693554274,
      "grad_norm": 4.176239967346191,
      "learning_rate": 4.280665255371438e-05,
      "loss": 0.675,
      "step": 946000
    },
    {
      "epoch": 8.632929410905906,
      "grad_norm": 4.507342338562012,
      "learning_rate": 4.280589215757841e-05,
      "loss": 0.7074,
      "step": 946100
    },
    {
      "epoch": 8.633841886269071,
      "grad_norm": 2.961669445037842,
      "learning_rate": 4.280513176144244e-05,
      "loss": 0.6835,
      "step": 946200
    },
    {
      "epoch": 8.634754361632236,
      "grad_norm": 3.9516522884368896,
      "learning_rate": 4.280437136530647e-05,
      "loss": 0.7025,
      "step": 946300
    },
    {
      "epoch": 8.635666836995401,
      "grad_norm": 3.005063533782959,
      "learning_rate": 4.2803610969170496e-05,
      "loss": 0.6688,
      "step": 946400
    },
    {
      "epoch": 8.636579312358567,
      "grad_norm": 3.5722198486328125,
      "learning_rate": 4.280285057303453e-05,
      "loss": 0.6595,
      "step": 946500
    },
    {
      "epoch": 8.637491787721732,
      "grad_norm": 3.6967077255249023,
      "learning_rate": 4.2802090176898556e-05,
      "loss": 0.6948,
      "step": 946600
    },
    {
      "epoch": 8.638404263084897,
      "grad_norm": 4.265610694885254,
      "learning_rate": 4.2801329780762586e-05,
      "loss": 0.7047,
      "step": 946700
    },
    {
      "epoch": 8.639316738448063,
      "grad_norm": 3.0744972229003906,
      "learning_rate": 4.2800569384626616e-05,
      "loss": 0.6772,
      "step": 946800
    },
    {
      "epoch": 8.640229213811228,
      "grad_norm": 3.619595766067505,
      "learning_rate": 4.2799808988490646e-05,
      "loss": 0.7171,
      "step": 946900
    },
    {
      "epoch": 8.641141689174392,
      "grad_norm": 4.156959533691406,
      "learning_rate": 4.2799048592354676e-05,
      "loss": 0.7018,
      "step": 947000
    },
    {
      "epoch": 8.642054164537557,
      "grad_norm": 3.885859251022339,
      "learning_rate": 4.2798288196218706e-05,
      "loss": 0.6981,
      "step": 947100
    },
    {
      "epoch": 8.642966639900722,
      "grad_norm": 4.54072904586792,
      "learning_rate": 4.279752780008273e-05,
      "loss": 0.7314,
      "step": 947200
    },
    {
      "epoch": 8.643879115263887,
      "grad_norm": 4.696082592010498,
      "learning_rate": 4.2796767403946766e-05,
      "loss": 0.698,
      "step": 947300
    },
    {
      "epoch": 8.644791590627053,
      "grad_norm": 3.039865493774414,
      "learning_rate": 4.279600700781079e-05,
      "loss": 0.7231,
      "step": 947400
    },
    {
      "epoch": 8.645704065990218,
      "grad_norm": 2.2725789546966553,
      "learning_rate": 4.279524661167482e-05,
      "loss": 0.6661,
      "step": 947500
    },
    {
      "epoch": 8.646616541353383,
      "grad_norm": 3.013836622238159,
      "learning_rate": 4.279448621553885e-05,
      "loss": 0.6917,
      "step": 947600
    },
    {
      "epoch": 8.647529016716549,
      "grad_norm": 3.8821909427642822,
      "learning_rate": 4.279372581940288e-05,
      "loss": 0.6826,
      "step": 947700
    },
    {
      "epoch": 8.648441492079714,
      "grad_norm": 4.648346424102783,
      "learning_rate": 4.27929654232669e-05,
      "loss": 0.6642,
      "step": 947800
    },
    {
      "epoch": 8.64935396744288,
      "grad_norm": 3.5333406925201416,
      "learning_rate": 4.279220502713094e-05,
      "loss": 0.6895,
      "step": 947900
    },
    {
      "epoch": 8.650266442806045,
      "grad_norm": 4.730583667755127,
      "learning_rate": 4.279144463099496e-05,
      "loss": 0.6892,
      "step": 948000
    },
    {
      "epoch": 8.65117891816921,
      "grad_norm": 5.138101577758789,
      "learning_rate": 4.279068423485899e-05,
      "loss": 0.663,
      "step": 948100
    },
    {
      "epoch": 8.652091393532375,
      "grad_norm": 4.761519908905029,
      "learning_rate": 4.2789923838723023e-05,
      "loss": 0.6757,
      "step": 948200
    },
    {
      "epoch": 8.65300386889554,
      "grad_norm": 3.8793928623199463,
      "learning_rate": 4.2789163442587053e-05,
      "loss": 0.6721,
      "step": 948300
    },
    {
      "epoch": 8.653916344258706,
      "grad_norm": 3.624521255493164,
      "learning_rate": 4.2788403046451084e-05,
      "loss": 0.6941,
      "step": 948400
    },
    {
      "epoch": 8.654828819621871,
      "grad_norm": 3.8578908443450928,
      "learning_rate": 4.2787642650315114e-05,
      "loss": 0.6935,
      "step": 948500
    },
    {
      "epoch": 8.655741294985035,
      "grad_norm": 2.941619396209717,
      "learning_rate": 4.278688225417914e-05,
      "loss": 0.6953,
      "step": 948600
    },
    {
      "epoch": 8.6566537703482,
      "grad_norm": 3.2808732986450195,
      "learning_rate": 4.278612185804317e-05,
      "loss": 0.701,
      "step": 948700
    },
    {
      "epoch": 8.657566245711365,
      "grad_norm": 3.167703628540039,
      "learning_rate": 4.27853614619072e-05,
      "loss": 0.7374,
      "step": 948800
    },
    {
      "epoch": 8.65847872107453,
      "grad_norm": 4.042928695678711,
      "learning_rate": 4.278460106577122e-05,
      "loss": 0.7114,
      "step": 948900
    },
    {
      "epoch": 8.659391196437696,
      "grad_norm": 3.8759658336639404,
      "learning_rate": 4.278384066963526e-05,
      "loss": 0.7052,
      "step": 949000
    },
    {
      "epoch": 8.660303671800861,
      "grad_norm": 4.585443019866943,
      "learning_rate": 4.278308027349928e-05,
      "loss": 0.6955,
      "step": 949100
    },
    {
      "epoch": 8.661216147164026,
      "grad_norm": 3.7933921813964844,
      "learning_rate": 4.278231987736331e-05,
      "loss": 0.6861,
      "step": 949200
    },
    {
      "epoch": 8.662128622527192,
      "grad_norm": 4.2288618087768555,
      "learning_rate": 4.278155948122734e-05,
      "loss": 0.6729,
      "step": 949300
    },
    {
      "epoch": 8.663041097890357,
      "grad_norm": 4.257462501525879,
      "learning_rate": 4.278079908509137e-05,
      "loss": 0.6596,
      "step": 949400
    },
    {
      "epoch": 8.663953573253522,
      "grad_norm": 2.739384174346924,
      "learning_rate": 4.27800386889554e-05,
      "loss": 0.6749,
      "step": 949500
    },
    {
      "epoch": 8.664866048616688,
      "grad_norm": 3.9148802757263184,
      "learning_rate": 4.277927829281943e-05,
      "loss": 0.6588,
      "step": 949600
    },
    {
      "epoch": 8.665778523979853,
      "grad_norm": 4.026828765869141,
      "learning_rate": 4.2778517896683454e-05,
      "loss": 0.7073,
      "step": 949700
    },
    {
      "epoch": 8.666690999343018,
      "grad_norm": 4.190201759338379,
      "learning_rate": 4.277775750054749e-05,
      "loss": 0.7157,
      "step": 949800
    },
    {
      "epoch": 8.667603474706183,
      "grad_norm": 2.6689484119415283,
      "learning_rate": 4.2776997104411514e-05,
      "loss": 0.7074,
      "step": 949900
    },
    {
      "epoch": 8.668515950069349,
      "grad_norm": 4.347658634185791,
      "learning_rate": 4.2776236708275544e-05,
      "loss": 0.702,
      "step": 950000
    },
    {
      "epoch": 8.669428425432514,
      "grad_norm": 1.980560302734375,
      "learning_rate": 4.2775476312139574e-05,
      "loss": 0.7038,
      "step": 950100
    },
    {
      "epoch": 8.67034090079568,
      "grad_norm": 4.223177433013916,
      "learning_rate": 4.2774715916003604e-05,
      "loss": 0.6752,
      "step": 950200
    },
    {
      "epoch": 8.671253376158845,
      "grad_norm": 4.948660373687744,
      "learning_rate": 4.2773955519867635e-05,
      "loss": 0.6797,
      "step": 950300
    },
    {
      "epoch": 8.672165851522008,
      "grad_norm": 5.884774208068848,
      "learning_rate": 4.2773195123731665e-05,
      "loss": 0.6671,
      "step": 950400
    },
    {
      "epoch": 8.673078326885173,
      "grad_norm": 5.040980815887451,
      "learning_rate": 4.277243472759569e-05,
      "loss": 0.6925,
      "step": 950500
    },
    {
      "epoch": 8.673990802248339,
      "grad_norm": 4.172145366668701,
      "learning_rate": 4.277167433145972e-05,
      "loss": 0.6966,
      "step": 950600
    },
    {
      "epoch": 8.674903277611504,
      "grad_norm": 4.762953281402588,
      "learning_rate": 4.277091393532375e-05,
      "loss": 0.7424,
      "step": 950700
    },
    {
      "epoch": 8.67581575297467,
      "grad_norm": 4.508694171905518,
      "learning_rate": 4.277015353918778e-05,
      "loss": 0.6704,
      "step": 950800
    },
    {
      "epoch": 8.676728228337835,
      "grad_norm": 4.532323837280273,
      "learning_rate": 4.276939314305181e-05,
      "loss": 0.6839,
      "step": 950900
    },
    {
      "epoch": 8.677640703701,
      "grad_norm": 4.2119622230529785,
      "learning_rate": 4.276863274691584e-05,
      "loss": 0.7176,
      "step": 951000
    },
    {
      "epoch": 8.678553179064165,
      "grad_norm": 2.7438385486602783,
      "learning_rate": 4.276787235077986e-05,
      "loss": 0.6731,
      "step": 951100
    },
    {
      "epoch": 8.67946565442733,
      "grad_norm": 4.579495906829834,
      "learning_rate": 4.27671119546439e-05,
      "loss": 0.7147,
      "step": 951200
    },
    {
      "epoch": 8.680378129790496,
      "grad_norm": 4.0099897384643555,
      "learning_rate": 4.276635155850792e-05,
      "loss": 0.7054,
      "step": 951300
    },
    {
      "epoch": 8.681290605153661,
      "grad_norm": 3.1744561195373535,
      "learning_rate": 4.276559116237195e-05,
      "loss": 0.6728,
      "step": 951400
    },
    {
      "epoch": 8.682203080516826,
      "grad_norm": 3.983186960220337,
      "learning_rate": 4.276483076623598e-05,
      "loss": 0.6564,
      "step": 951500
    },
    {
      "epoch": 8.683115555879992,
      "grad_norm": 4.157851219177246,
      "learning_rate": 4.2764070370100005e-05,
      "loss": 0.6705,
      "step": 951600
    },
    {
      "epoch": 8.684028031243157,
      "grad_norm": 3.6804990768432617,
      "learning_rate": 4.276330997396404e-05,
      "loss": 0.649,
      "step": 951700
    },
    {
      "epoch": 8.684940506606322,
      "grad_norm": 3.7482614517211914,
      "learning_rate": 4.2762549577828065e-05,
      "loss": 0.7043,
      "step": 951800
    },
    {
      "epoch": 8.685852981969488,
      "grad_norm": 3.3422369956970215,
      "learning_rate": 4.2761789181692095e-05,
      "loss": 0.678,
      "step": 951900
    },
    {
      "epoch": 8.686765457332651,
      "grad_norm": 4.761162757873535,
      "learning_rate": 4.2761028785556125e-05,
      "loss": 0.6958,
      "step": 952000
    },
    {
      "epoch": 8.687677932695816,
      "grad_norm": 4.869395732879639,
      "learning_rate": 4.2760268389420155e-05,
      "loss": 0.7119,
      "step": 952100
    },
    {
      "epoch": 8.688590408058982,
      "grad_norm": 3.6613521575927734,
      "learning_rate": 4.275950799328418e-05,
      "loss": 0.6634,
      "step": 952200
    },
    {
      "epoch": 8.689502883422147,
      "grad_norm": 3.854647636413574,
      "learning_rate": 4.2758747597148216e-05,
      "loss": 0.6786,
      "step": 952300
    },
    {
      "epoch": 8.690415358785312,
      "grad_norm": 5.423867702484131,
      "learning_rate": 4.275798720101224e-05,
      "loss": 0.7284,
      "step": 952400
    },
    {
      "epoch": 8.691327834148478,
      "grad_norm": 4.318268299102783,
      "learning_rate": 4.275722680487627e-05,
      "loss": 0.6819,
      "step": 952500
    },
    {
      "epoch": 8.692240309511643,
      "grad_norm": 4.132924556732178,
      "learning_rate": 4.27564664087403e-05,
      "loss": 0.6812,
      "step": 952600
    },
    {
      "epoch": 8.693152784874808,
      "grad_norm": 4.392977237701416,
      "learning_rate": 4.275570601260433e-05,
      "loss": 0.7046,
      "step": 952700
    },
    {
      "epoch": 8.694065260237974,
      "grad_norm": 2.962832450866699,
      "learning_rate": 4.275494561646836e-05,
      "loss": 0.6743,
      "step": 952800
    },
    {
      "epoch": 8.694977735601139,
      "grad_norm": 4.429337024688721,
      "learning_rate": 4.275418522033239e-05,
      "loss": 0.6721,
      "step": 952900
    },
    {
      "epoch": 8.695890210964304,
      "grad_norm": 3.2297210693359375,
      "learning_rate": 4.275342482419641e-05,
      "loss": 0.6717,
      "step": 953000
    },
    {
      "epoch": 8.69680268632747,
      "grad_norm": 2.763733386993408,
      "learning_rate": 4.275266442806045e-05,
      "loss": 0.6995,
      "step": 953100
    },
    {
      "epoch": 8.697715161690635,
      "grad_norm": 3.297626256942749,
      "learning_rate": 4.275190403192447e-05,
      "loss": 0.687,
      "step": 953200
    },
    {
      "epoch": 8.6986276370538,
      "grad_norm": 3.9642717838287354,
      "learning_rate": 4.27511436357885e-05,
      "loss": 0.6729,
      "step": 953300
    },
    {
      "epoch": 8.699540112416965,
      "grad_norm": 3.7408149242401123,
      "learning_rate": 4.275038323965253e-05,
      "loss": 0.6795,
      "step": 953400
    },
    {
      "epoch": 8.70045258778013,
      "grad_norm": 3.837465763092041,
      "learning_rate": 4.274962284351656e-05,
      "loss": 0.7189,
      "step": 953500
    },
    {
      "epoch": 8.701365063143296,
      "grad_norm": 4.521764278411865,
      "learning_rate": 4.2748862447380586e-05,
      "loss": 0.6981,
      "step": 953600
    },
    {
      "epoch": 8.702277538506461,
      "grad_norm": 3.45348858833313,
      "learning_rate": 4.274810205124462e-05,
      "loss": 0.705,
      "step": 953700
    },
    {
      "epoch": 8.703190013869625,
      "grad_norm": 3.824486494064331,
      "learning_rate": 4.2747341655108646e-05,
      "loss": 0.7351,
      "step": 953800
    },
    {
      "epoch": 8.70410248923279,
      "grad_norm": 3.805464029312134,
      "learning_rate": 4.2746581258972676e-05,
      "loss": 0.6866,
      "step": 953900
    },
    {
      "epoch": 8.705014964595955,
      "grad_norm": 5.104446887969971,
      "learning_rate": 4.2745820862836706e-05,
      "loss": 0.6916,
      "step": 954000
    },
    {
      "epoch": 8.70592743995912,
      "grad_norm": 4.048432350158691,
      "learning_rate": 4.2745060466700736e-05,
      "loss": 0.688,
      "step": 954100
    },
    {
      "epoch": 8.706839915322286,
      "grad_norm": 4.880617141723633,
      "learning_rate": 4.2744300070564766e-05,
      "loss": 0.6697,
      "step": 954200
    },
    {
      "epoch": 8.707752390685451,
      "grad_norm": 4.509122848510742,
      "learning_rate": 4.274353967442879e-05,
      "loss": 0.6836,
      "step": 954300
    },
    {
      "epoch": 8.708664866048617,
      "grad_norm": 3.939649820327759,
      "learning_rate": 4.274277927829282e-05,
      "loss": 0.6725,
      "step": 954400
    },
    {
      "epoch": 8.709577341411782,
      "grad_norm": 4.62452507019043,
      "learning_rate": 4.274201888215685e-05,
      "loss": 0.704,
      "step": 954500
    },
    {
      "epoch": 8.710489816774947,
      "grad_norm": 3.9115660190582275,
      "learning_rate": 4.274125848602088e-05,
      "loss": 0.7177,
      "step": 954600
    },
    {
      "epoch": 8.711402292138112,
      "grad_norm": 3.7388222217559814,
      "learning_rate": 4.27404980898849e-05,
      "loss": 0.7275,
      "step": 954700
    },
    {
      "epoch": 8.712314767501278,
      "grad_norm": 4.057292461395264,
      "learning_rate": 4.273973769374894e-05,
      "loss": 0.6792,
      "step": 954800
    },
    {
      "epoch": 8.713227242864443,
      "grad_norm": 3.566645860671997,
      "learning_rate": 4.2738977297612963e-05,
      "loss": 0.6944,
      "step": 954900
    },
    {
      "epoch": 8.714139718227608,
      "grad_norm": 3.984506607055664,
      "learning_rate": 4.2738216901476993e-05,
      "loss": 0.6384,
      "step": 955000
    },
    {
      "epoch": 8.715052193590774,
      "grad_norm": 3.8415582180023193,
      "learning_rate": 4.2737456505341024e-05,
      "loss": 0.7013,
      "step": 955100
    },
    {
      "epoch": 8.715964668953939,
      "grad_norm": 3.3115391731262207,
      "learning_rate": 4.2736696109205054e-05,
      "loss": 0.6813,
      "step": 955200
    },
    {
      "epoch": 8.716877144317104,
      "grad_norm": 3.7125182151794434,
      "learning_rate": 4.2735935713069084e-05,
      "loss": 0.6966,
      "step": 955300
    },
    {
      "epoch": 8.717789619680268,
      "grad_norm": 3.7641897201538086,
      "learning_rate": 4.2735175316933114e-05,
      "loss": 0.6847,
      "step": 955400
    },
    {
      "epoch": 8.718702095043433,
      "grad_norm": 4.101402759552002,
      "learning_rate": 4.273441492079714e-05,
      "loss": 0.6817,
      "step": 955500
    },
    {
      "epoch": 8.719614570406598,
      "grad_norm": 3.7674951553344727,
      "learning_rate": 4.2733654524661174e-05,
      "loss": 0.6608,
      "step": 955600
    },
    {
      "epoch": 8.720527045769764,
      "grad_norm": 3.654539108276367,
      "learning_rate": 4.27328941285252e-05,
      "loss": 0.7064,
      "step": 955700
    },
    {
      "epoch": 8.721439521132929,
      "grad_norm": 4.317281246185303,
      "learning_rate": 4.273213373238923e-05,
      "loss": 0.6998,
      "step": 955800
    },
    {
      "epoch": 8.722351996496094,
      "grad_norm": 2.915250778198242,
      "learning_rate": 4.273137333625326e-05,
      "loss": 0.6757,
      "step": 955900
    },
    {
      "epoch": 8.72326447185926,
      "grad_norm": 3.9804320335388184,
      "learning_rate": 4.273061294011729e-05,
      "loss": 0.6877,
      "step": 956000
    },
    {
      "epoch": 8.724176947222425,
      "grad_norm": 3.468193292617798,
      "learning_rate": 4.272985254398131e-05,
      "loss": 0.6976,
      "step": 956100
    },
    {
      "epoch": 8.72508942258559,
      "grad_norm": 3.7970714569091797,
      "learning_rate": 4.272909214784535e-05,
      "loss": 0.6539,
      "step": 956200
    },
    {
      "epoch": 8.726001897948755,
      "grad_norm": 3.5710537433624268,
      "learning_rate": 4.272833175170937e-05,
      "loss": 0.6996,
      "step": 956300
    },
    {
      "epoch": 8.72691437331192,
      "grad_norm": 3.9886796474456787,
      "learning_rate": 4.27275713555734e-05,
      "loss": 0.688,
      "step": 956400
    },
    {
      "epoch": 8.727826848675086,
      "grad_norm": 4.154107093811035,
      "learning_rate": 4.272681095943743e-05,
      "loss": 0.6774,
      "step": 956500
    },
    {
      "epoch": 8.728739324038251,
      "grad_norm": 4.095993518829346,
      "learning_rate": 4.272605056330146e-05,
      "loss": 0.7495,
      "step": 956600
    },
    {
      "epoch": 8.729651799401417,
      "grad_norm": 5.001583099365234,
      "learning_rate": 4.272529016716549e-05,
      "loss": 0.6977,
      "step": 956700
    },
    {
      "epoch": 8.730564274764582,
      "grad_norm": 4.825366973876953,
      "learning_rate": 4.272452977102952e-05,
      "loss": 0.6762,
      "step": 956800
    },
    {
      "epoch": 8.731476750127747,
      "grad_norm": 3.509634494781494,
      "learning_rate": 4.2723769374893544e-05,
      "loss": 0.6866,
      "step": 956900
    },
    {
      "epoch": 8.732389225490913,
      "grad_norm": 4.132373809814453,
      "learning_rate": 4.272300897875758e-05,
      "loss": 0.7187,
      "step": 957000
    },
    {
      "epoch": 8.733301700854078,
      "grad_norm": 3.3202242851257324,
      "learning_rate": 4.2722248582621605e-05,
      "loss": 0.7034,
      "step": 957100
    },
    {
      "epoch": 8.734214176217241,
      "grad_norm": 3.4364876747131348,
      "learning_rate": 4.272148818648563e-05,
      "loss": 0.7088,
      "step": 957200
    },
    {
      "epoch": 8.735126651580407,
      "grad_norm": 4.039809226989746,
      "learning_rate": 4.2720727790349665e-05,
      "loss": 0.698,
      "step": 957300
    },
    {
      "epoch": 8.736039126943572,
      "grad_norm": 4.539731502532959,
      "learning_rate": 4.271996739421369e-05,
      "loss": 0.7046,
      "step": 957400
    },
    {
      "epoch": 8.736951602306737,
      "grad_norm": 3.9276185035705566,
      "learning_rate": 4.271920699807772e-05,
      "loss": 0.7239,
      "step": 957500
    },
    {
      "epoch": 8.737864077669903,
      "grad_norm": 3.3778367042541504,
      "learning_rate": 4.271844660194175e-05,
      "loss": 0.654,
      "step": 957600
    },
    {
      "epoch": 8.738776553033068,
      "grad_norm": 4.037869930267334,
      "learning_rate": 4.271768620580578e-05,
      "loss": 0.7039,
      "step": 957700
    },
    {
      "epoch": 8.739689028396233,
      "grad_norm": 5.022063255310059,
      "learning_rate": 4.271692580966981e-05,
      "loss": 0.7085,
      "step": 957800
    },
    {
      "epoch": 8.740601503759398,
      "grad_norm": 3.910778522491455,
      "learning_rate": 4.271616541353384e-05,
      "loss": 0.7121,
      "step": 957900
    },
    {
      "epoch": 8.741513979122564,
      "grad_norm": 4.541445255279541,
      "learning_rate": 4.271540501739786e-05,
      "loss": 0.691,
      "step": 958000
    },
    {
      "epoch": 8.742426454485729,
      "grad_norm": 4.222489833831787,
      "learning_rate": 4.27146446212619e-05,
      "loss": 0.708,
      "step": 958100
    },
    {
      "epoch": 8.743338929848894,
      "grad_norm": 4.41886043548584,
      "learning_rate": 4.271388422512592e-05,
      "loss": 0.6877,
      "step": 958200
    },
    {
      "epoch": 8.74425140521206,
      "grad_norm": 4.544247627258301,
      "learning_rate": 4.271312382898995e-05,
      "loss": 0.6952,
      "step": 958300
    },
    {
      "epoch": 8.745163880575225,
      "grad_norm": 4.44713020324707,
      "learning_rate": 4.271236343285398e-05,
      "loss": 0.7146,
      "step": 958400
    },
    {
      "epoch": 8.74607635593839,
      "grad_norm": 4.292306423187256,
      "learning_rate": 4.271160303671801e-05,
      "loss": 0.6906,
      "step": 958500
    },
    {
      "epoch": 8.746988831301556,
      "grad_norm": 4.077670097351074,
      "learning_rate": 4.2710842640582035e-05,
      "loss": 0.7044,
      "step": 958600
    },
    {
      "epoch": 8.74790130666472,
      "grad_norm": 3.9459550380706787,
      "learning_rate": 4.271008224444607e-05,
      "loss": 0.6496,
      "step": 958700
    },
    {
      "epoch": 8.748813782027884,
      "grad_norm": 5.114006996154785,
      "learning_rate": 4.2709321848310095e-05,
      "loss": 0.6636,
      "step": 958800
    },
    {
      "epoch": 8.74972625739105,
      "grad_norm": 3.845803737640381,
      "learning_rate": 4.2708561452174125e-05,
      "loss": 0.7236,
      "step": 958900
    },
    {
      "epoch": 8.750638732754215,
      "grad_norm": 4.630256175994873,
      "learning_rate": 4.2707801056038155e-05,
      "loss": 0.6918,
      "step": 959000
    },
    {
      "epoch": 8.75155120811738,
      "grad_norm": 4.044834613800049,
      "learning_rate": 4.2707040659902186e-05,
      "loss": 0.7078,
      "step": 959100
    },
    {
      "epoch": 8.752463683480546,
      "grad_norm": 3.942305088043213,
      "learning_rate": 4.2706280263766216e-05,
      "loss": 0.7005,
      "step": 959200
    },
    {
      "epoch": 8.753376158843711,
      "grad_norm": 3.8921058177948,
      "learning_rate": 4.2705519867630246e-05,
      "loss": 0.6977,
      "step": 959300
    },
    {
      "epoch": 8.754288634206876,
      "grad_norm": 4.075397491455078,
      "learning_rate": 4.270475947149427e-05,
      "loss": 0.7147,
      "step": 959400
    },
    {
      "epoch": 8.755201109570041,
      "grad_norm": 4.687642574310303,
      "learning_rate": 4.2703999075358306e-05,
      "loss": 0.6356,
      "step": 959500
    },
    {
      "epoch": 8.756113584933207,
      "grad_norm": 3.9999165534973145,
      "learning_rate": 4.270323867922233e-05,
      "loss": 0.6726,
      "step": 959600
    },
    {
      "epoch": 8.757026060296372,
      "grad_norm": 5.095276832580566,
      "learning_rate": 4.270247828308636e-05,
      "loss": 0.7083,
      "step": 959700
    },
    {
      "epoch": 8.757938535659537,
      "grad_norm": 3.346327304840088,
      "learning_rate": 4.270171788695039e-05,
      "loss": 0.7098,
      "step": 959800
    },
    {
      "epoch": 8.758851011022703,
      "grad_norm": 3.0817532539367676,
      "learning_rate": 4.270095749081442e-05,
      "loss": 0.6687,
      "step": 959900
    },
    {
      "epoch": 8.759763486385868,
      "grad_norm": 3.3186137676239014,
      "learning_rate": 4.270019709467844e-05,
      "loss": 0.7133,
      "step": 960000
    },
    {
      "epoch": 8.760675961749033,
      "grad_norm": 4.323690414428711,
      "learning_rate": 4.269943669854247e-05,
      "loss": 0.7062,
      "step": 960100
    },
    {
      "epoch": 8.761588437112199,
      "grad_norm": 4.136162281036377,
      "learning_rate": 4.26986763024065e-05,
      "loss": 0.719,
      "step": 960200
    },
    {
      "epoch": 8.762500912475364,
      "grad_norm": 3.745687961578369,
      "learning_rate": 4.269791590627053e-05,
      "loss": 0.6741,
      "step": 960300
    },
    {
      "epoch": 8.76341338783853,
      "grad_norm": 3.7787938117980957,
      "learning_rate": 4.269715551013456e-05,
      "loss": 0.6914,
      "step": 960400
    },
    {
      "epoch": 8.764325863201694,
      "grad_norm": 3.592756748199463,
      "learning_rate": 4.2696395113998586e-05,
      "loss": 0.7169,
      "step": 960500
    },
    {
      "epoch": 8.765238338564858,
      "grad_norm": 2.9527242183685303,
      "learning_rate": 4.269563471786262e-05,
      "loss": 0.6647,
      "step": 960600
    },
    {
      "epoch": 8.766150813928023,
      "grad_norm": 3.627807855606079,
      "learning_rate": 4.2694874321726646e-05,
      "loss": 0.6745,
      "step": 960700
    },
    {
      "epoch": 8.767063289291189,
      "grad_norm": 4.824066162109375,
      "learning_rate": 4.2694113925590676e-05,
      "loss": 0.6737,
      "step": 960800
    },
    {
      "epoch": 8.767975764654354,
      "grad_norm": 3.9421231746673584,
      "learning_rate": 4.2693353529454706e-05,
      "loss": 0.6798,
      "step": 960900
    },
    {
      "epoch": 8.76888824001752,
      "grad_norm": 4.154472827911377,
      "learning_rate": 4.2692593133318737e-05,
      "loss": 0.701,
      "step": 961000
    },
    {
      "epoch": 8.769800715380685,
      "grad_norm": 3.992810010910034,
      "learning_rate": 4.269183273718276e-05,
      "loss": 0.6527,
      "step": 961100
    },
    {
      "epoch": 8.77071319074385,
      "grad_norm": 4.664453506469727,
      "learning_rate": 4.26910723410468e-05,
      "loss": 0.6971,
      "step": 961200
    },
    {
      "epoch": 8.771625666107015,
      "grad_norm": 2.5699820518493652,
      "learning_rate": 4.269031194491082e-05,
      "loss": 0.6942,
      "step": 961300
    },
    {
      "epoch": 8.77253814147018,
      "grad_norm": 4.167699337005615,
      "learning_rate": 4.268955154877485e-05,
      "loss": 0.6859,
      "step": 961400
    },
    {
      "epoch": 8.773450616833346,
      "grad_norm": 3.6339850425720215,
      "learning_rate": 4.268879115263888e-05,
      "loss": 0.6647,
      "step": 961500
    },
    {
      "epoch": 8.774363092196511,
      "grad_norm": 4.597803115844727,
      "learning_rate": 4.268803075650291e-05,
      "loss": 0.7017,
      "step": 961600
    },
    {
      "epoch": 8.775275567559676,
      "grad_norm": 4.96338415145874,
      "learning_rate": 4.268727036036694e-05,
      "loss": 0.6417,
      "step": 961700
    },
    {
      "epoch": 8.776188042922842,
      "grad_norm": 3.4920401573181152,
      "learning_rate": 4.268650996423097e-05,
      "loss": 0.6955,
      "step": 961800
    },
    {
      "epoch": 8.777100518286007,
      "grad_norm": 3.7826225757598877,
      "learning_rate": 4.2685749568094994e-05,
      "loss": 0.6715,
      "step": 961900
    },
    {
      "epoch": 8.778012993649172,
      "grad_norm": 4.014893531799316,
      "learning_rate": 4.268498917195903e-05,
      "loss": 0.6776,
      "step": 962000
    },
    {
      "epoch": 8.778925469012336,
      "grad_norm": 4.656087398529053,
      "learning_rate": 4.2684228775823054e-05,
      "loss": 0.7203,
      "step": 962100
    },
    {
      "epoch": 8.779837944375501,
      "grad_norm": 3.6185648441314697,
      "learning_rate": 4.2683468379687084e-05,
      "loss": 0.701,
      "step": 962200
    },
    {
      "epoch": 8.780750419738666,
      "grad_norm": 4.658039093017578,
      "learning_rate": 4.2682707983551114e-05,
      "loss": 0.6771,
      "step": 962300
    },
    {
      "epoch": 8.781662895101832,
      "grad_norm": 4.172976970672607,
      "learning_rate": 4.2681947587415144e-05,
      "loss": 0.6842,
      "step": 962400
    },
    {
      "epoch": 8.782575370464997,
      "grad_norm": 3.732041835784912,
      "learning_rate": 4.268118719127917e-05,
      "loss": 0.6408,
      "step": 962500
    },
    {
      "epoch": 8.783487845828162,
      "grad_norm": 3.8778975009918213,
      "learning_rate": 4.2680426795143204e-05,
      "loss": 0.6851,
      "step": 962600
    },
    {
      "epoch": 8.784400321191328,
      "grad_norm": 4.842998027801514,
      "learning_rate": 4.267966639900723e-05,
      "loss": 0.6881,
      "step": 962700
    },
    {
      "epoch": 8.785312796554493,
      "grad_norm": 4.5958251953125,
      "learning_rate": 4.267890600287126e-05,
      "loss": 0.6707,
      "step": 962800
    },
    {
      "epoch": 8.786225271917658,
      "grad_norm": 3.8122715950012207,
      "learning_rate": 4.267814560673529e-05,
      "loss": 0.7118,
      "step": 962900
    },
    {
      "epoch": 8.787137747280823,
      "grad_norm": 4.111536026000977,
      "learning_rate": 4.267738521059931e-05,
      "loss": 0.6943,
      "step": 963000
    },
    {
      "epoch": 8.788050222643989,
      "grad_norm": 3.567204475402832,
      "learning_rate": 4.267662481446335e-05,
      "loss": 0.7047,
      "step": 963100
    },
    {
      "epoch": 8.788962698007154,
      "grad_norm": 4.304591655731201,
      "learning_rate": 4.267586441832737e-05,
      "loss": 0.6926,
      "step": 963200
    },
    {
      "epoch": 8.78987517337032,
      "grad_norm": 3.614922046661377,
      "learning_rate": 4.26751040221914e-05,
      "loss": 0.6903,
      "step": 963300
    },
    {
      "epoch": 8.790787648733485,
      "grad_norm": 4.5252604484558105,
      "learning_rate": 4.267434362605543e-05,
      "loss": 0.716,
      "step": 963400
    },
    {
      "epoch": 8.79170012409665,
      "grad_norm": 3.0766637325286865,
      "learning_rate": 4.267358322991946e-05,
      "loss": 0.6629,
      "step": 963500
    },
    {
      "epoch": 8.792612599459815,
      "grad_norm": 4.344666481018066,
      "learning_rate": 4.267282283378349e-05,
      "loss": 0.6789,
      "step": 963600
    },
    {
      "epoch": 8.79352507482298,
      "grad_norm": 3.475626230239868,
      "learning_rate": 4.267206243764752e-05,
      "loss": 0.7017,
      "step": 963700
    },
    {
      "epoch": 8.794437550186146,
      "grad_norm": 3.91457462310791,
      "learning_rate": 4.2671302041511545e-05,
      "loss": 0.6989,
      "step": 963800
    },
    {
      "epoch": 8.79535002554931,
      "grad_norm": 4.608712673187256,
      "learning_rate": 4.267054164537558e-05,
      "loss": 0.6997,
      "step": 963900
    },
    {
      "epoch": 8.796262500912475,
      "grad_norm": 4.0716233253479,
      "learning_rate": 4.2669781249239605e-05,
      "loss": 0.6947,
      "step": 964000
    },
    {
      "epoch": 8.79717497627564,
      "grad_norm": 3.842085361480713,
      "learning_rate": 4.2669020853103635e-05,
      "loss": 0.7221,
      "step": 964100
    },
    {
      "epoch": 8.798087451638805,
      "grad_norm": 3.113224506378174,
      "learning_rate": 4.2668260456967665e-05,
      "loss": 0.6536,
      "step": 964200
    },
    {
      "epoch": 8.79899992700197,
      "grad_norm": 3.90539813041687,
      "learning_rate": 4.2667500060831695e-05,
      "loss": 0.6982,
      "step": 964300
    },
    {
      "epoch": 8.799912402365136,
      "grad_norm": 4.669756889343262,
      "learning_rate": 4.266673966469572e-05,
      "loss": 0.717,
      "step": 964400
    },
    {
      "epoch": 8.800824877728301,
      "grad_norm": 3.6964244842529297,
      "learning_rate": 4.2665979268559755e-05,
      "loss": 0.6695,
      "step": 964500
    },
    {
      "epoch": 8.801737353091466,
      "grad_norm": 4.326542377471924,
      "learning_rate": 4.266521887242378e-05,
      "loss": 0.7306,
      "step": 964600
    },
    {
      "epoch": 8.802649828454632,
      "grad_norm": 3.681429386138916,
      "learning_rate": 4.266445847628781e-05,
      "loss": 0.7008,
      "step": 964700
    },
    {
      "epoch": 8.803562303817797,
      "grad_norm": 4.100143909454346,
      "learning_rate": 4.266369808015184e-05,
      "loss": 0.6783,
      "step": 964800
    },
    {
      "epoch": 8.804474779180962,
      "grad_norm": 4.836963176727295,
      "learning_rate": 4.266293768401587e-05,
      "loss": 0.6867,
      "step": 964900
    },
    {
      "epoch": 8.805387254544128,
      "grad_norm": 3.574833631515503,
      "learning_rate": 4.26621772878799e-05,
      "loss": 0.6767,
      "step": 965000
    },
    {
      "epoch": 8.806299729907293,
      "grad_norm": 3.314471483230591,
      "learning_rate": 4.266141689174393e-05,
      "loss": 0.7058,
      "step": 965100
    },
    {
      "epoch": 8.807212205270458,
      "grad_norm": 3.8267693519592285,
      "learning_rate": 4.266065649560795e-05,
      "loss": 0.6744,
      "step": 965200
    },
    {
      "epoch": 8.808124680633624,
      "grad_norm": 4.0914306640625,
      "learning_rate": 4.265989609947199e-05,
      "loss": 0.7022,
      "step": 965300
    },
    {
      "epoch": 8.809037155996789,
      "grad_norm": 4.448312282562256,
      "learning_rate": 4.265913570333601e-05,
      "loss": 0.6846,
      "step": 965400
    },
    {
      "epoch": 8.809949631359952,
      "grad_norm": 2.603837490081787,
      "learning_rate": 4.265837530720004e-05,
      "loss": 0.682,
      "step": 965500
    },
    {
      "epoch": 8.810862106723118,
      "grad_norm": 5.772803783416748,
      "learning_rate": 4.265761491106407e-05,
      "loss": 0.6989,
      "step": 965600
    },
    {
      "epoch": 8.811774582086283,
      "grad_norm": 3.6974542140960693,
      "learning_rate": 4.2656854514928095e-05,
      "loss": 0.6887,
      "step": 965700
    },
    {
      "epoch": 8.812687057449448,
      "grad_norm": 3.339946746826172,
      "learning_rate": 4.2656094118792126e-05,
      "loss": 0.6667,
      "step": 965800
    },
    {
      "epoch": 8.813599532812614,
      "grad_norm": 3.5575671195983887,
      "learning_rate": 4.2655333722656156e-05,
      "loss": 0.6919,
      "step": 965900
    },
    {
      "epoch": 8.814512008175779,
      "grad_norm": 3.403733015060425,
      "learning_rate": 4.2654573326520186e-05,
      "loss": 0.661,
      "step": 966000
    },
    {
      "epoch": 8.815424483538944,
      "grad_norm": 3.4373631477355957,
      "learning_rate": 4.2653812930384216e-05,
      "loss": 0.7332,
      "step": 966100
    },
    {
      "epoch": 8.81633695890211,
      "grad_norm": 3.8577263355255127,
      "learning_rate": 4.2653052534248246e-05,
      "loss": 0.6787,
      "step": 966200
    },
    {
      "epoch": 8.817249434265275,
      "grad_norm": 4.075719356536865,
      "learning_rate": 4.265229213811227e-05,
      "loss": 0.6748,
      "step": 966300
    },
    {
      "epoch": 8.81816190962844,
      "grad_norm": 4.192840099334717,
      "learning_rate": 4.2651531741976306e-05,
      "loss": 0.7084,
      "step": 966400
    },
    {
      "epoch": 8.819074384991605,
      "grad_norm": 3.5924532413482666,
      "learning_rate": 4.265077134584033e-05,
      "loss": 0.6797,
      "step": 966500
    },
    {
      "epoch": 8.81998686035477,
      "grad_norm": 4.660105228424072,
      "learning_rate": 4.265001094970436e-05,
      "loss": 0.6794,
      "step": 966600
    },
    {
      "epoch": 8.820899335717936,
      "grad_norm": 2.9914777278900146,
      "learning_rate": 4.264925055356839e-05,
      "loss": 0.6729,
      "step": 966700
    },
    {
      "epoch": 8.821811811081101,
      "grad_norm": 3.015983819961548,
      "learning_rate": 4.264849015743242e-05,
      "loss": 0.7003,
      "step": 966800
    },
    {
      "epoch": 8.822724286444267,
      "grad_norm": 3.585306167602539,
      "learning_rate": 4.264772976129644e-05,
      "loss": 0.7243,
      "step": 966900
    },
    {
      "epoch": 8.823636761807432,
      "grad_norm": 4.3318772315979,
      "learning_rate": 4.264696936516048e-05,
      "loss": 0.709,
      "step": 967000
    },
    {
      "epoch": 8.824549237170597,
      "grad_norm": 3.6665332317352295,
      "learning_rate": 4.26462089690245e-05,
      "loss": 0.7393,
      "step": 967100
    },
    {
      "epoch": 8.825461712533762,
      "grad_norm": 6.201006889343262,
      "learning_rate": 4.264544857288853e-05,
      "loss": 0.6575,
      "step": 967200
    },
    {
      "epoch": 8.826374187896926,
      "grad_norm": 3.8110511302948,
      "learning_rate": 4.264468817675256e-05,
      "loss": 0.6798,
      "step": 967300
    },
    {
      "epoch": 8.827286663260091,
      "grad_norm": 4.3300557136535645,
      "learning_rate": 4.264392778061659e-05,
      "loss": 0.701,
      "step": 967400
    },
    {
      "epoch": 8.828199138623257,
      "grad_norm": 3.091665506362915,
      "learning_rate": 4.264316738448062e-05,
      "loss": 0.6797,
      "step": 967500
    },
    {
      "epoch": 8.829111613986422,
      "grad_norm": 4.791927814483643,
      "learning_rate": 4.264240698834465e-05,
      "loss": 0.6531,
      "step": 967600
    },
    {
      "epoch": 8.830024089349587,
      "grad_norm": 4.264462471008301,
      "learning_rate": 4.2641646592208676e-05,
      "loss": 0.6555,
      "step": 967700
    },
    {
      "epoch": 8.830936564712752,
      "grad_norm": 4.081413745880127,
      "learning_rate": 4.264088619607271e-05,
      "loss": 0.6693,
      "step": 967800
    },
    {
      "epoch": 8.831849040075918,
      "grad_norm": 3.4459822177886963,
      "learning_rate": 4.2640125799936737e-05,
      "loss": 0.6777,
      "step": 967900
    },
    {
      "epoch": 8.832761515439083,
      "grad_norm": 3.683292865753174,
      "learning_rate": 4.263936540380077e-05,
      "loss": 0.6975,
      "step": 968000
    },
    {
      "epoch": 8.833673990802248,
      "grad_norm": 3.649143695831299,
      "learning_rate": 4.26386050076648e-05,
      "loss": 0.6794,
      "step": 968100
    },
    {
      "epoch": 8.834586466165414,
      "grad_norm": 3.312363624572754,
      "learning_rate": 4.263784461152883e-05,
      "loss": 0.6821,
      "step": 968200
    },
    {
      "epoch": 8.835498941528579,
      "grad_norm": 4.887383460998535,
      "learning_rate": 4.263708421539285e-05,
      "loss": 0.6634,
      "step": 968300
    },
    {
      "epoch": 8.836411416891744,
      "grad_norm": 4.086462020874023,
      "learning_rate": 4.263632381925689e-05,
      "loss": 0.6691,
      "step": 968400
    },
    {
      "epoch": 8.83732389225491,
      "grad_norm": 3.653653860092163,
      "learning_rate": 4.263556342312091e-05,
      "loss": 0.7063,
      "step": 968500
    },
    {
      "epoch": 8.838236367618075,
      "grad_norm": 5.171545028686523,
      "learning_rate": 4.263480302698494e-05,
      "loss": 0.6781,
      "step": 968600
    },
    {
      "epoch": 8.83914884298124,
      "grad_norm": 3.8796565532684326,
      "learning_rate": 4.263404263084897e-05,
      "loss": 0.6633,
      "step": 968700
    },
    {
      "epoch": 8.840061318344405,
      "grad_norm": 4.339902877807617,
      "learning_rate": 4.2633282234712994e-05,
      "loss": 0.6587,
      "step": 968800
    },
    {
      "epoch": 8.840973793707569,
      "grad_norm": 3.77559494972229,
      "learning_rate": 4.263252183857703e-05,
      "loss": 0.6825,
      "step": 968900
    },
    {
      "epoch": 8.841886269070734,
      "grad_norm": 3.9587137699127197,
      "learning_rate": 4.2631761442441054e-05,
      "loss": 0.7283,
      "step": 969000
    },
    {
      "epoch": 8.8427987444339,
      "grad_norm": 3.916862964630127,
      "learning_rate": 4.2631001046305084e-05,
      "loss": 0.684,
      "step": 969100
    },
    {
      "epoch": 8.843711219797065,
      "grad_norm": 4.690819263458252,
      "learning_rate": 4.2630240650169114e-05,
      "loss": 0.7159,
      "step": 969200
    },
    {
      "epoch": 8.84462369516023,
      "grad_norm": 4.0259904861450195,
      "learning_rate": 4.2629480254033144e-05,
      "loss": 0.7072,
      "step": 969300
    },
    {
      "epoch": 8.845536170523395,
      "grad_norm": 4.327505588531494,
      "learning_rate": 4.262871985789717e-05,
      "loss": 0.6554,
      "step": 969400
    },
    {
      "epoch": 8.84644864588656,
      "grad_norm": 2.819697380065918,
      "learning_rate": 4.2627959461761204e-05,
      "loss": 0.6812,
      "step": 969500
    },
    {
      "epoch": 8.847361121249726,
      "grad_norm": 4.191427230834961,
      "learning_rate": 4.262719906562523e-05,
      "loss": 0.7111,
      "step": 969600
    },
    {
      "epoch": 8.848273596612891,
      "grad_norm": 3.7609612941741943,
      "learning_rate": 4.262643866948926e-05,
      "loss": 0.6785,
      "step": 969700
    },
    {
      "epoch": 8.849186071976057,
      "grad_norm": 3.8935389518737793,
      "learning_rate": 4.262567827335329e-05,
      "loss": 0.6807,
      "step": 969800
    },
    {
      "epoch": 8.850098547339222,
      "grad_norm": 3.9283199310302734,
      "learning_rate": 4.262491787721732e-05,
      "loss": 0.6592,
      "step": 969900
    },
    {
      "epoch": 8.851011022702387,
      "grad_norm": 4.766818523406982,
      "learning_rate": 4.262415748108135e-05,
      "loss": 0.6523,
      "step": 970000
    },
    {
      "epoch": 8.851923498065553,
      "grad_norm": 4.118963718414307,
      "learning_rate": 4.262339708494538e-05,
      "loss": 0.6734,
      "step": 970100
    },
    {
      "epoch": 8.852835973428718,
      "grad_norm": 3.959655284881592,
      "learning_rate": 4.26226366888094e-05,
      "loss": 0.7026,
      "step": 970200
    },
    {
      "epoch": 8.853748448791883,
      "grad_norm": 3.698868989944458,
      "learning_rate": 4.262187629267344e-05,
      "loss": 0.6897,
      "step": 970300
    },
    {
      "epoch": 8.854660924155048,
      "grad_norm": 5.0061354637146,
      "learning_rate": 4.262111589653746e-05,
      "loss": 0.6968,
      "step": 970400
    },
    {
      "epoch": 8.855573399518214,
      "grad_norm": 3.5642948150634766,
      "learning_rate": 4.262035550040149e-05,
      "loss": 0.673,
      "step": 970500
    },
    {
      "epoch": 8.856485874881379,
      "grad_norm": 4.247872352600098,
      "learning_rate": 4.261959510426552e-05,
      "loss": 0.7118,
      "step": 970600
    },
    {
      "epoch": 8.857398350244543,
      "grad_norm": 4.247005462646484,
      "learning_rate": 4.261883470812955e-05,
      "loss": 0.6569,
      "step": 970700
    },
    {
      "epoch": 8.858310825607708,
      "grad_norm": 3.723088502883911,
      "learning_rate": 4.2618074311993575e-05,
      "loss": 0.6859,
      "step": 970800
    },
    {
      "epoch": 8.859223300970873,
      "grad_norm": 3.3036091327667236,
      "learning_rate": 4.261731391585761e-05,
      "loss": 0.7018,
      "step": 970900
    },
    {
      "epoch": 8.860135776334038,
      "grad_norm": 3.803433656692505,
      "learning_rate": 4.2616553519721635e-05,
      "loss": 0.695,
      "step": 971000
    },
    {
      "epoch": 8.861048251697204,
      "grad_norm": 4.069474220275879,
      "learning_rate": 4.2615793123585665e-05,
      "loss": 0.6779,
      "step": 971100
    },
    {
      "epoch": 8.861960727060369,
      "grad_norm": 4.46933650970459,
      "learning_rate": 4.2615032727449695e-05,
      "loss": 0.6679,
      "step": 971200
    },
    {
      "epoch": 8.862873202423534,
      "grad_norm": 3.8928825855255127,
      "learning_rate": 4.2614272331313725e-05,
      "loss": 0.6859,
      "step": 971300
    },
    {
      "epoch": 8.8637856777867,
      "grad_norm": 3.8821773529052734,
      "learning_rate": 4.2613511935177755e-05,
      "loss": 0.7055,
      "step": 971400
    },
    {
      "epoch": 8.864698153149865,
      "grad_norm": 4.296620845794678,
      "learning_rate": 4.261275153904178e-05,
      "loss": 0.7005,
      "step": 971500
    },
    {
      "epoch": 8.86561062851303,
      "grad_norm": 3.9460201263427734,
      "learning_rate": 4.261199114290581e-05,
      "loss": 0.6918,
      "step": 971600
    },
    {
      "epoch": 8.866523103876196,
      "grad_norm": 4.794704914093018,
      "learning_rate": 4.261123074676984e-05,
      "loss": 0.7275,
      "step": 971700
    },
    {
      "epoch": 8.86743557923936,
      "grad_norm": 3.435999870300293,
      "learning_rate": 4.261047035063387e-05,
      "loss": 0.7297,
      "step": 971800
    },
    {
      "epoch": 8.868348054602526,
      "grad_norm": 3.7739779949188232,
      "learning_rate": 4.260970995449789e-05,
      "loss": 0.6776,
      "step": 971900
    },
    {
      "epoch": 8.869260529965691,
      "grad_norm": 4.737090587615967,
      "learning_rate": 4.260894955836193e-05,
      "loss": 0.7312,
      "step": 972000
    },
    {
      "epoch": 8.870173005328857,
      "grad_norm": 3.9797022342681885,
      "learning_rate": 4.260818916222595e-05,
      "loss": 0.697,
      "step": 972100
    },
    {
      "epoch": 8.871085480692022,
      "grad_norm": 4.36033821105957,
      "learning_rate": 4.260742876608998e-05,
      "loss": 0.728,
      "step": 972200
    },
    {
      "epoch": 8.871997956055186,
      "grad_norm": 4.606149673461914,
      "learning_rate": 4.260666836995401e-05,
      "loss": 0.6962,
      "step": 972300
    },
    {
      "epoch": 8.872910431418351,
      "grad_norm": 4.357407569885254,
      "learning_rate": 4.260590797381804e-05,
      "loss": 0.7092,
      "step": 972400
    },
    {
      "epoch": 8.873822906781516,
      "grad_norm": 3.7970051765441895,
      "learning_rate": 4.260514757768207e-05,
      "loss": 0.6806,
      "step": 972500
    },
    {
      "epoch": 8.874735382144681,
      "grad_norm": 4.218258380889893,
      "learning_rate": 4.26043871815461e-05,
      "loss": 0.6971,
      "step": 972600
    },
    {
      "epoch": 8.875647857507847,
      "grad_norm": 3.048454999923706,
      "learning_rate": 4.2603626785410126e-05,
      "loss": 0.6905,
      "step": 972700
    },
    {
      "epoch": 8.876560332871012,
      "grad_norm": 3.9828670024871826,
      "learning_rate": 4.260286638927416e-05,
      "loss": 0.6997,
      "step": 972800
    },
    {
      "epoch": 8.877472808234177,
      "grad_norm": 4.71586275100708,
      "learning_rate": 4.2602105993138186e-05,
      "loss": 0.7028,
      "step": 972900
    },
    {
      "epoch": 8.878385283597343,
      "grad_norm": 4.024412631988525,
      "learning_rate": 4.2601345597002216e-05,
      "loss": 0.6773,
      "step": 973000
    },
    {
      "epoch": 8.879297758960508,
      "grad_norm": 4.549737930297852,
      "learning_rate": 4.2600585200866246e-05,
      "loss": 0.7335,
      "step": 973100
    },
    {
      "epoch": 8.880210234323673,
      "grad_norm": 3.5103609561920166,
      "learning_rate": 4.2599824804730276e-05,
      "loss": 0.6873,
      "step": 973200
    },
    {
      "epoch": 8.881122709686839,
      "grad_norm": 3.6854872703552246,
      "learning_rate": 4.25990644085943e-05,
      "loss": 0.6961,
      "step": 973300
    },
    {
      "epoch": 8.882035185050004,
      "grad_norm": 3.814143657684326,
      "learning_rate": 4.2598304012458336e-05,
      "loss": 0.7137,
      "step": 973400
    },
    {
      "epoch": 8.88294766041317,
      "grad_norm": 4.454617977142334,
      "learning_rate": 4.259754361632236e-05,
      "loss": 0.7006,
      "step": 973500
    },
    {
      "epoch": 8.883860135776334,
      "grad_norm": 3.2619056701660156,
      "learning_rate": 4.259678322018639e-05,
      "loss": 0.7079,
      "step": 973600
    },
    {
      "epoch": 8.8847726111395,
      "grad_norm": 4.446093559265137,
      "learning_rate": 4.259602282405042e-05,
      "loss": 0.6987,
      "step": 973700
    },
    {
      "epoch": 8.885685086502665,
      "grad_norm": 4.026805400848389,
      "learning_rate": 4.259526242791445e-05,
      "loss": 0.6427,
      "step": 973800
    },
    {
      "epoch": 8.88659756186583,
      "grad_norm": 4.254736423492432,
      "learning_rate": 4.259450203177848e-05,
      "loss": 0.7177,
      "step": 973900
    },
    {
      "epoch": 8.887510037228996,
      "grad_norm": 4.985766887664795,
      "learning_rate": 4.259374163564251e-05,
      "loss": 0.7144,
      "step": 974000
    },
    {
      "epoch": 8.88842251259216,
      "grad_norm": 4.3003339767456055,
      "learning_rate": 4.259298123950653e-05,
      "loss": 0.7008,
      "step": 974100
    },
    {
      "epoch": 8.889334987955325,
      "grad_norm": 4.049459934234619,
      "learning_rate": 4.259222084337056e-05,
      "loss": 0.6636,
      "step": 974200
    },
    {
      "epoch": 8.89024746331849,
      "grad_norm": 5.611355781555176,
      "learning_rate": 4.259146044723459e-05,
      "loss": 0.6768,
      "step": 974300
    },
    {
      "epoch": 8.891159938681655,
      "grad_norm": 5.105888843536377,
      "learning_rate": 4.2590700051098616e-05,
      "loss": 0.6824,
      "step": 974400
    },
    {
      "epoch": 8.89207241404482,
      "grad_norm": 4.28756856918335,
      "learning_rate": 4.258993965496265e-05,
      "loss": 0.7135,
      "step": 974500
    },
    {
      "epoch": 8.892984889407986,
      "grad_norm": 4.5187296867370605,
      "learning_rate": 4.2589179258826677e-05,
      "loss": 0.6991,
      "step": 974600
    },
    {
      "epoch": 8.893897364771151,
      "grad_norm": 4.045691967010498,
      "learning_rate": 4.258841886269071e-05,
      "loss": 0.6764,
      "step": 974700
    },
    {
      "epoch": 8.894809840134316,
      "grad_norm": 4.906615734100342,
      "learning_rate": 4.258765846655474e-05,
      "loss": 0.696,
      "step": 974800
    },
    {
      "epoch": 8.895722315497482,
      "grad_norm": 3.753084897994995,
      "learning_rate": 4.258689807041877e-05,
      "loss": 0.6966,
      "step": 974900
    },
    {
      "epoch": 8.896634790860647,
      "grad_norm": 3.6894943714141846,
      "learning_rate": 4.25861376742828e-05,
      "loss": 0.6953,
      "step": 975000
    },
    {
      "epoch": 8.897547266223812,
      "grad_norm": 3.8191959857940674,
      "learning_rate": 4.258537727814683e-05,
      "loss": 0.689,
      "step": 975100
    },
    {
      "epoch": 8.898459741586978,
      "grad_norm": 3.564814805984497,
      "learning_rate": 4.258461688201085e-05,
      "loss": 0.6958,
      "step": 975200
    },
    {
      "epoch": 8.899372216950143,
      "grad_norm": 3.555805206298828,
      "learning_rate": 4.258385648587489e-05,
      "loss": 0.6664,
      "step": 975300
    },
    {
      "epoch": 8.900284692313308,
      "grad_norm": 4.072283744812012,
      "learning_rate": 4.258309608973891e-05,
      "loss": 0.6906,
      "step": 975400
    },
    {
      "epoch": 8.901197167676473,
      "grad_norm": 3.8328068256378174,
      "learning_rate": 4.258233569360294e-05,
      "loss": 0.7026,
      "step": 975500
    },
    {
      "epoch": 8.902109643039639,
      "grad_norm": 4.885941028594971,
      "learning_rate": 4.258157529746697e-05,
      "loss": 0.6872,
      "step": 975600
    },
    {
      "epoch": 8.903022118402802,
      "grad_norm": 3.3379263877868652,
      "learning_rate": 4.2580814901331e-05,
      "loss": 0.7082,
      "step": 975700
    },
    {
      "epoch": 8.903934593765968,
      "grad_norm": 4.393401145935059,
      "learning_rate": 4.258005450519503e-05,
      "loss": 0.6994,
      "step": 975800
    },
    {
      "epoch": 8.904847069129133,
      "grad_norm": 3.479818105697632,
      "learning_rate": 4.257929410905906e-05,
      "loss": 0.7322,
      "step": 975900
    },
    {
      "epoch": 8.905759544492298,
      "grad_norm": 5.140160083770752,
      "learning_rate": 4.2578533712923084e-05,
      "loss": 0.6886,
      "step": 976000
    },
    {
      "epoch": 8.906672019855463,
      "grad_norm": 4.014913082122803,
      "learning_rate": 4.2577773316787114e-05,
      "loss": 0.6801,
      "step": 976100
    },
    {
      "epoch": 8.907584495218629,
      "grad_norm": 4.239692687988281,
      "learning_rate": 4.2577012920651144e-05,
      "loss": 0.6843,
      "step": 976200
    },
    {
      "epoch": 8.908496970581794,
      "grad_norm": 4.503061294555664,
      "learning_rate": 4.2576252524515174e-05,
      "loss": 0.667,
      "step": 976300
    },
    {
      "epoch": 8.90940944594496,
      "grad_norm": 4.31625509262085,
      "learning_rate": 4.2575492128379204e-05,
      "loss": 0.6942,
      "step": 976400
    },
    {
      "epoch": 8.910321921308125,
      "grad_norm": 3.8174664974212646,
      "learning_rate": 4.2574731732243234e-05,
      "loss": 0.6931,
      "step": 976500
    },
    {
      "epoch": 8.91123439667129,
      "grad_norm": 3.0811965465545654,
      "learning_rate": 4.257397133610726e-05,
      "loss": 0.682,
      "step": 976600
    },
    {
      "epoch": 8.912146872034455,
      "grad_norm": 3.901123285293579,
      "learning_rate": 4.2573210939971294e-05,
      "loss": 0.7309,
      "step": 976700
    },
    {
      "epoch": 8.91305934739762,
      "grad_norm": 3.3033480644226074,
      "learning_rate": 4.257245054383532e-05,
      "loss": 0.6565,
      "step": 976800
    },
    {
      "epoch": 8.913971822760786,
      "grad_norm": 4.025848865509033,
      "learning_rate": 4.257169014769935e-05,
      "loss": 0.6799,
      "step": 976900
    },
    {
      "epoch": 8.914884298123951,
      "grad_norm": 4.060513496398926,
      "learning_rate": 4.257092975156338e-05,
      "loss": 0.6992,
      "step": 977000
    },
    {
      "epoch": 8.915796773487116,
      "grad_norm": 4.488919734954834,
      "learning_rate": 4.25701693554274e-05,
      "loss": 0.724,
      "step": 977100
    },
    {
      "epoch": 8.916709248850282,
      "grad_norm": 3.847275972366333,
      "learning_rate": 4.256940895929144e-05,
      "loss": 0.7274,
      "step": 977200
    },
    {
      "epoch": 8.917621724213447,
      "grad_norm": 3.472229242324829,
      "learning_rate": 4.256864856315546e-05,
      "loss": 0.6786,
      "step": 977300
    },
    {
      "epoch": 8.918534199576612,
      "grad_norm": 4.949853420257568,
      "learning_rate": 4.256788816701949e-05,
      "loss": 0.7336,
      "step": 977400
    },
    {
      "epoch": 8.919446674939776,
      "grad_norm": 5.516632080078125,
      "learning_rate": 4.256712777088352e-05,
      "loss": 0.6893,
      "step": 977500
    },
    {
      "epoch": 8.920359150302941,
      "grad_norm": 4.097377300262451,
      "learning_rate": 4.256636737474755e-05,
      "loss": 0.7164,
      "step": 977600
    },
    {
      "epoch": 8.921271625666106,
      "grad_norm": 4.62138557434082,
      "learning_rate": 4.2565606978611575e-05,
      "loss": 0.6645,
      "step": 977700
    },
    {
      "epoch": 8.922184101029272,
      "grad_norm": 3.952422618865967,
      "learning_rate": 4.256484658247561e-05,
      "loss": 0.6754,
      "step": 977800
    },
    {
      "epoch": 8.923096576392437,
      "grad_norm": 4.331683158874512,
      "learning_rate": 4.2564086186339635e-05,
      "loss": 0.7406,
      "step": 977900
    },
    {
      "epoch": 8.924009051755602,
      "grad_norm": 3.618962287902832,
      "learning_rate": 4.2563325790203665e-05,
      "loss": 0.6982,
      "step": 978000
    },
    {
      "epoch": 8.924921527118768,
      "grad_norm": 3.6842124462127686,
      "learning_rate": 4.2562565394067695e-05,
      "loss": 0.6877,
      "step": 978100
    },
    {
      "epoch": 8.925834002481933,
      "grad_norm": 5.848505020141602,
      "learning_rate": 4.2561804997931725e-05,
      "loss": 0.6947,
      "step": 978200
    },
    {
      "epoch": 8.926746477845098,
      "grad_norm": 4.233847618103027,
      "learning_rate": 4.2561044601795755e-05,
      "loss": 0.6552,
      "step": 978300
    },
    {
      "epoch": 8.927658953208264,
      "grad_norm": 3.6912741661071777,
      "learning_rate": 4.2560284205659785e-05,
      "loss": 0.686,
      "step": 978400
    },
    {
      "epoch": 8.928571428571429,
      "grad_norm": 3.864701271057129,
      "learning_rate": 4.255952380952381e-05,
      "loss": 0.6591,
      "step": 978500
    },
    {
      "epoch": 8.929483903934594,
      "grad_norm": 2.9554660320281982,
      "learning_rate": 4.2558763413387845e-05,
      "loss": 0.6692,
      "step": 978600
    },
    {
      "epoch": 8.93039637929776,
      "grad_norm": 3.4007043838500977,
      "learning_rate": 4.255800301725187e-05,
      "loss": 0.6767,
      "step": 978700
    },
    {
      "epoch": 8.931308854660925,
      "grad_norm": 4.415915012359619,
      "learning_rate": 4.25572426211159e-05,
      "loss": 0.6861,
      "step": 978800
    },
    {
      "epoch": 8.93222133002409,
      "grad_norm": 4.235389709472656,
      "learning_rate": 4.255648222497993e-05,
      "loss": 0.7167,
      "step": 978900
    },
    {
      "epoch": 8.933133805387255,
      "grad_norm": 3.5334911346435547,
      "learning_rate": 4.255572182884396e-05,
      "loss": 0.6995,
      "step": 979000
    },
    {
      "epoch": 8.934046280750419,
      "grad_norm": 4.265587329864502,
      "learning_rate": 4.255496143270798e-05,
      "loss": 0.6723,
      "step": 979100
    },
    {
      "epoch": 8.934958756113584,
      "grad_norm": 4.17864990234375,
      "learning_rate": 4.255420103657202e-05,
      "loss": 0.7324,
      "step": 979200
    },
    {
      "epoch": 8.93587123147675,
      "grad_norm": 3.16629695892334,
      "learning_rate": 4.255344064043604e-05,
      "loss": 0.696,
      "step": 979300
    },
    {
      "epoch": 8.936783706839915,
      "grad_norm": 4.435820579528809,
      "learning_rate": 4.255268024430007e-05,
      "loss": 0.6671,
      "step": 979400
    },
    {
      "epoch": 8.93769618220308,
      "grad_norm": 5.17104434967041,
      "learning_rate": 4.25519198481641e-05,
      "loss": 0.695,
      "step": 979500
    },
    {
      "epoch": 8.938608657566245,
      "grad_norm": 4.29407262802124,
      "learning_rate": 4.255115945202813e-05,
      "loss": 0.6805,
      "step": 979600
    },
    {
      "epoch": 8.93952113292941,
      "grad_norm": 3.3571269512176514,
      "learning_rate": 4.255039905589216e-05,
      "loss": 0.665,
      "step": 979700
    },
    {
      "epoch": 8.940433608292576,
      "grad_norm": 4.182502746582031,
      "learning_rate": 4.254963865975619e-05,
      "loss": 0.6835,
      "step": 979800
    },
    {
      "epoch": 8.941346083655741,
      "grad_norm": 3.2455050945281982,
      "learning_rate": 4.2548878263620216e-05,
      "loss": 0.7288,
      "step": 979900
    },
    {
      "epoch": 8.942258559018907,
      "grad_norm": 3.602226972579956,
      "learning_rate": 4.2548117867484246e-05,
      "loss": 0.6937,
      "step": 980000
    },
    {
      "epoch": 8.943171034382072,
      "grad_norm": 2.4655003547668457,
      "learning_rate": 4.2547357471348276e-05,
      "loss": 0.6854,
      "step": 980100
    },
    {
      "epoch": 8.944083509745237,
      "grad_norm": 5.516621112823486,
      "learning_rate": 4.25465970752123e-05,
      "loss": 0.6804,
      "step": 980200
    },
    {
      "epoch": 8.944995985108402,
      "grad_norm": 3.9137539863586426,
      "learning_rate": 4.2545836679076336e-05,
      "loss": 0.6711,
      "step": 980300
    },
    {
      "epoch": 8.945908460471568,
      "grad_norm": 3.7866687774658203,
      "learning_rate": 4.254507628294036e-05,
      "loss": 0.7042,
      "step": 980400
    },
    {
      "epoch": 8.946820935834733,
      "grad_norm": 4.188610076904297,
      "learning_rate": 4.254431588680439e-05,
      "loss": 0.6845,
      "step": 980500
    },
    {
      "epoch": 8.947733411197898,
      "grad_norm": 4.185877323150635,
      "learning_rate": 4.254355549066842e-05,
      "loss": 0.6984,
      "step": 980600
    },
    {
      "epoch": 8.948645886561064,
      "grad_norm": 3.699636220932007,
      "learning_rate": 4.254279509453245e-05,
      "loss": 0.6722,
      "step": 980700
    },
    {
      "epoch": 8.949558361924229,
      "grad_norm": 3.8984737396240234,
      "learning_rate": 4.254203469839648e-05,
      "loss": 0.6837,
      "step": 980800
    },
    {
      "epoch": 8.950470837287392,
      "grad_norm": 4.510892868041992,
      "learning_rate": 4.254127430226051e-05,
      "loss": 0.7059,
      "step": 980900
    },
    {
      "epoch": 8.951383312650558,
      "grad_norm": 3.184582471847534,
      "learning_rate": 4.254051390612453e-05,
      "loss": 0.6824,
      "step": 981000
    },
    {
      "epoch": 8.952295788013723,
      "grad_norm": 3.5234577655792236,
      "learning_rate": 4.253975350998857e-05,
      "loss": 0.7056,
      "step": 981100
    },
    {
      "epoch": 8.953208263376888,
      "grad_norm": 3.968106746673584,
      "learning_rate": 4.253899311385259e-05,
      "loss": 0.715,
      "step": 981200
    },
    {
      "epoch": 8.954120738740054,
      "grad_norm": 4.266195774078369,
      "learning_rate": 4.253823271771662e-05,
      "loss": 0.714,
      "step": 981300
    },
    {
      "epoch": 8.955033214103219,
      "grad_norm": 4.088198184967041,
      "learning_rate": 4.253747232158065e-05,
      "loss": 0.7357,
      "step": 981400
    },
    {
      "epoch": 8.955945689466384,
      "grad_norm": 3.28603458404541,
      "learning_rate": 4.2536711925444683e-05,
      "loss": 0.677,
      "step": 981500
    },
    {
      "epoch": 8.95685816482955,
      "grad_norm": 3.368438482284546,
      "learning_rate": 4.253595152930871e-05,
      "loss": 0.6905,
      "step": 981600
    },
    {
      "epoch": 8.957770640192715,
      "grad_norm": 4.090303421020508,
      "learning_rate": 4.2535191133172744e-05,
      "loss": 0.676,
      "step": 981700
    },
    {
      "epoch": 8.95868311555588,
      "grad_norm": 4.308651924133301,
      "learning_rate": 4.253443073703677e-05,
      "loss": 0.6757,
      "step": 981800
    },
    {
      "epoch": 8.959595590919045,
      "grad_norm": 3.2010107040405273,
      "learning_rate": 4.25336703409008e-05,
      "loss": 0.7036,
      "step": 981900
    },
    {
      "epoch": 8.96050806628221,
      "grad_norm": 3.9413185119628906,
      "learning_rate": 4.253290994476483e-05,
      "loss": 0.6629,
      "step": 982000
    },
    {
      "epoch": 8.961420541645376,
      "grad_norm": 3.803697347640991,
      "learning_rate": 4.253214954862886e-05,
      "loss": 0.709,
      "step": 982100
    },
    {
      "epoch": 8.962333017008541,
      "grad_norm": 3.721827983856201,
      "learning_rate": 4.253138915249289e-05,
      "loss": 0.6967,
      "step": 982200
    },
    {
      "epoch": 8.963245492371707,
      "grad_norm": 3.523149251937866,
      "learning_rate": 4.253062875635692e-05,
      "loss": 0.6604,
      "step": 982300
    },
    {
      "epoch": 8.964157967734872,
      "grad_norm": 4.1168389320373535,
      "learning_rate": 4.252986836022094e-05,
      "loss": 0.6811,
      "step": 982400
    },
    {
      "epoch": 8.965070443098035,
      "grad_norm": 3.890875816345215,
      "learning_rate": 4.252910796408498e-05,
      "loss": 0.7014,
      "step": 982500
    },
    {
      "epoch": 8.9659829184612,
      "grad_norm": 3.9635698795318604,
      "learning_rate": 4.2528347567949e-05,
      "loss": 0.7823,
      "step": 982600
    },
    {
      "epoch": 8.966895393824366,
      "grad_norm": 4.073853969573975,
      "learning_rate": 4.2527587171813024e-05,
      "loss": 0.6875,
      "step": 982700
    },
    {
      "epoch": 8.967807869187531,
      "grad_norm": 4.25563383102417,
      "learning_rate": 4.252682677567706e-05,
      "loss": 0.6796,
      "step": 982800
    },
    {
      "epoch": 8.968720344550697,
      "grad_norm": 3.7431252002716064,
      "learning_rate": 4.2526066379541084e-05,
      "loss": 0.6868,
      "step": 982900
    },
    {
      "epoch": 8.969632819913862,
      "grad_norm": 4.568156719207764,
      "learning_rate": 4.2525305983405114e-05,
      "loss": 0.7127,
      "step": 983000
    },
    {
      "epoch": 8.970545295277027,
      "grad_norm": 3.7569611072540283,
      "learning_rate": 4.2524545587269144e-05,
      "loss": 0.6839,
      "step": 983100
    },
    {
      "epoch": 8.971457770640193,
      "grad_norm": 4.601203918457031,
      "learning_rate": 4.2523785191133174e-05,
      "loss": 0.6729,
      "step": 983200
    },
    {
      "epoch": 8.972370246003358,
      "grad_norm": 3.9025678634643555,
      "learning_rate": 4.2523024794997204e-05,
      "loss": 0.7092,
      "step": 983300
    },
    {
      "epoch": 8.973282721366523,
      "grad_norm": 4.672538757324219,
      "learning_rate": 4.2522264398861234e-05,
      "loss": 0.6758,
      "step": 983400
    },
    {
      "epoch": 8.974195196729688,
      "grad_norm": 3.9124279022216797,
      "learning_rate": 4.252150400272526e-05,
      "loss": 0.7321,
      "step": 983500
    },
    {
      "epoch": 8.975107672092854,
      "grad_norm": 3.6747264862060547,
      "learning_rate": 4.2520743606589295e-05,
      "loss": 0.6712,
      "step": 983600
    },
    {
      "epoch": 8.976020147456019,
      "grad_norm": 4.220804691314697,
      "learning_rate": 4.251998321045332e-05,
      "loss": 0.6817,
      "step": 983700
    },
    {
      "epoch": 8.976932622819184,
      "grad_norm": 4.119688034057617,
      "learning_rate": 4.251922281431735e-05,
      "loss": 0.7039,
      "step": 983800
    },
    {
      "epoch": 8.97784509818235,
      "grad_norm": 2.876044988632202,
      "learning_rate": 4.251846241818138e-05,
      "loss": 0.6898,
      "step": 983900
    },
    {
      "epoch": 8.978757573545515,
      "grad_norm": 4.317413330078125,
      "learning_rate": 4.251770202204541e-05,
      "loss": 0.6524,
      "step": 984000
    },
    {
      "epoch": 8.97967004890868,
      "grad_norm": 3.7559638023376465,
      "learning_rate": 4.251694162590943e-05,
      "loss": 0.7131,
      "step": 984100
    },
    {
      "epoch": 8.980582524271846,
      "grad_norm": 3.848132610321045,
      "learning_rate": 4.251618122977347e-05,
      "loss": 0.671,
      "step": 984200
    },
    {
      "epoch": 8.981494999635009,
      "grad_norm": 4.528705596923828,
      "learning_rate": 4.251542083363749e-05,
      "loss": 0.7117,
      "step": 984300
    },
    {
      "epoch": 8.982407474998174,
      "grad_norm": 4.07546329498291,
      "learning_rate": 4.251466043750152e-05,
      "loss": 0.6446,
      "step": 984400
    },
    {
      "epoch": 8.98331995036134,
      "grad_norm": 4.483000755310059,
      "learning_rate": 4.251390004136555e-05,
      "loss": 0.6967,
      "step": 984500
    },
    {
      "epoch": 8.984232425724505,
      "grad_norm": 3.7132668495178223,
      "learning_rate": 4.251313964522958e-05,
      "loss": 0.6924,
      "step": 984600
    },
    {
      "epoch": 8.98514490108767,
      "grad_norm": 4.557685375213623,
      "learning_rate": 4.251237924909361e-05,
      "loss": 0.6687,
      "step": 984700
    },
    {
      "epoch": 8.986057376450836,
      "grad_norm": 4.4680256843566895,
      "learning_rate": 4.251161885295764e-05,
      "loss": 0.7391,
      "step": 984800
    },
    {
      "epoch": 8.986969851814,
      "grad_norm": 4.282927513122559,
      "learning_rate": 4.2510858456821665e-05,
      "loss": 0.7178,
      "step": 984900
    },
    {
      "epoch": 8.987882327177166,
      "grad_norm": 4.972204208374023,
      "learning_rate": 4.25100980606857e-05,
      "loss": 0.7111,
      "step": 985000
    },
    {
      "epoch": 8.988794802540331,
      "grad_norm": 5.274710655212402,
      "learning_rate": 4.2509337664549725e-05,
      "loss": 0.7196,
      "step": 985100
    },
    {
      "epoch": 8.989707277903497,
      "grad_norm": 4.070643901824951,
      "learning_rate": 4.2508577268413755e-05,
      "loss": 0.6942,
      "step": 985200
    },
    {
      "epoch": 8.990619753266662,
      "grad_norm": 3.8642005920410156,
      "learning_rate": 4.2507816872277785e-05,
      "loss": 0.6815,
      "step": 985300
    },
    {
      "epoch": 8.991532228629827,
      "grad_norm": 4.58231782913208,
      "learning_rate": 4.2507056476141815e-05,
      "loss": 0.7196,
      "step": 985400
    },
    {
      "epoch": 8.992444703992993,
      "grad_norm": 3.6722021102905273,
      "learning_rate": 4.250629608000584e-05,
      "loss": 0.6524,
      "step": 985500
    },
    {
      "epoch": 8.993357179356158,
      "grad_norm": 3.3232955932617188,
      "learning_rate": 4.250553568386987e-05,
      "loss": 0.7483,
      "step": 985600
    },
    {
      "epoch": 8.994269654719323,
      "grad_norm": 4.083379745483398,
      "learning_rate": 4.25047752877339e-05,
      "loss": 0.7379,
      "step": 985700
    },
    {
      "epoch": 8.995182130082489,
      "grad_norm": 3.5726840496063232,
      "learning_rate": 4.250401489159793e-05,
      "loss": 0.7035,
      "step": 985800
    },
    {
      "epoch": 8.996094605445652,
      "grad_norm": 4.1744561195373535,
      "learning_rate": 4.250325449546196e-05,
      "loss": 0.7134,
      "step": 985900
    },
    {
      "epoch": 8.997007080808817,
      "grad_norm": 3.070491313934326,
      "learning_rate": 4.250249409932598e-05,
      "loss": 0.6884,
      "step": 986000
    },
    {
      "epoch": 8.997919556171983,
      "grad_norm": 3.77071213722229,
      "learning_rate": 4.250173370319002e-05,
      "loss": 0.6986,
      "step": 986100
    },
    {
      "epoch": 8.998832031535148,
      "grad_norm": 5.052147388458252,
      "learning_rate": 4.250097330705404e-05,
      "loss": 0.6916,
      "step": 986200
    },
    {
      "epoch": 8.999744506898313,
      "grad_norm": 4.31129789352417,
      "learning_rate": 4.250021291091807e-05,
      "loss": 0.6874,
      "step": 986300
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.5616425275802612,
      "eval_runtime": 25.7659,
      "eval_samples_per_second": 223.9,
      "eval_steps_per_second": 223.9,
      "step": 986328
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.5423380136489868,
      "eval_runtime": 484.209,
      "eval_samples_per_second": 226.332,
      "eval_steps_per_second": 226.332,
      "step": 986328
    },
    {
      "epoch": 9.000656982261479,
      "grad_norm": 4.276936054229736,
      "learning_rate": 4.24994525147821e-05,
      "loss": 0.692,
      "step": 986400
    },
    {
      "epoch": 9.001569457624644,
      "grad_norm": 5.083064079284668,
      "learning_rate": 4.249869211864613e-05,
      "loss": 0.6901,
      "step": 986500
    },
    {
      "epoch": 9.00248193298781,
      "grad_norm": 3.212932586669922,
      "learning_rate": 4.2497931722510156e-05,
      "loss": 0.6954,
      "step": 986600
    },
    {
      "epoch": 9.003394408350974,
      "grad_norm": 4.656891822814941,
      "learning_rate": 4.249717132637419e-05,
      "loss": 0.7181,
      "step": 986700
    },
    {
      "epoch": 9.00430688371414,
      "grad_norm": 4.440963268280029,
      "learning_rate": 4.2496410930238216e-05,
      "loss": 0.6686,
      "step": 986800
    },
    {
      "epoch": 9.005219359077305,
      "grad_norm": 4.48715877532959,
      "learning_rate": 4.2495650534102246e-05,
      "loss": 0.6826,
      "step": 986900
    },
    {
      "epoch": 9.00613183444047,
      "grad_norm": 3.9602630138397217,
      "learning_rate": 4.2494890137966276e-05,
      "loss": 0.6994,
      "step": 987000
    },
    {
      "epoch": 9.007044309803636,
      "grad_norm": 4.42113733291626,
      "learning_rate": 4.2494129741830306e-05,
      "loss": 0.6771,
      "step": 987100
    },
    {
      "epoch": 9.007956785166801,
      "grad_norm": 4.0755696296691895,
      "learning_rate": 4.2493369345694336e-05,
      "loss": 0.6684,
      "step": 987200
    },
    {
      "epoch": 9.008869260529966,
      "grad_norm": 3.9511191844940186,
      "learning_rate": 4.2492608949558366e-05,
      "loss": 0.6584,
      "step": 987300
    },
    {
      "epoch": 9.009781735893132,
      "grad_norm": 4.7913665771484375,
      "learning_rate": 4.249184855342239e-05,
      "loss": 0.6663,
      "step": 987400
    },
    {
      "epoch": 9.010694211256297,
      "grad_norm": 3.9952473640441895,
      "learning_rate": 4.2491088157286426e-05,
      "loss": 0.7074,
      "step": 987500
    },
    {
      "epoch": 9.01160668661946,
      "grad_norm": 3.9225335121154785,
      "learning_rate": 4.249032776115045e-05,
      "loss": 0.7303,
      "step": 987600
    },
    {
      "epoch": 9.012519161982626,
      "grad_norm": 3.5807600021362305,
      "learning_rate": 4.248956736501448e-05,
      "loss": 0.6462,
      "step": 987700
    },
    {
      "epoch": 9.013431637345791,
      "grad_norm": 4.3838276863098145,
      "learning_rate": 4.248880696887851e-05,
      "loss": 0.6596,
      "step": 987800
    },
    {
      "epoch": 9.014344112708956,
      "grad_norm": 4.198085784912109,
      "learning_rate": 4.248804657274254e-05,
      "loss": 0.665,
      "step": 987900
    },
    {
      "epoch": 9.015256588072122,
      "grad_norm": 3.187870502471924,
      "learning_rate": 4.248728617660656e-05,
      "loss": 0.6802,
      "step": 988000
    },
    {
      "epoch": 9.016169063435287,
      "grad_norm": 3.492354393005371,
      "learning_rate": 4.24865257804706e-05,
      "loss": 0.7021,
      "step": 988100
    },
    {
      "epoch": 9.017081538798452,
      "grad_norm": 4.008871078491211,
      "learning_rate": 4.2485765384334623e-05,
      "loss": 0.7094,
      "step": 988200
    },
    {
      "epoch": 9.017994014161617,
      "grad_norm": 3.934966564178467,
      "learning_rate": 4.2485004988198653e-05,
      "loss": 0.678,
      "step": 988300
    },
    {
      "epoch": 9.018906489524783,
      "grad_norm": 4.461606502532959,
      "learning_rate": 4.2484244592062684e-05,
      "loss": 0.7047,
      "step": 988400
    },
    {
      "epoch": 9.019818964887948,
      "grad_norm": 3.586822032928467,
      "learning_rate": 4.248348419592671e-05,
      "loss": 0.6826,
      "step": 988500
    },
    {
      "epoch": 9.020731440251113,
      "grad_norm": 4.383121490478516,
      "learning_rate": 4.2482723799790744e-05,
      "loss": 0.7089,
      "step": 988600
    },
    {
      "epoch": 9.021643915614279,
      "grad_norm": 2.364818811416626,
      "learning_rate": 4.248196340365477e-05,
      "loss": 0.6355,
      "step": 988700
    },
    {
      "epoch": 9.022556390977444,
      "grad_norm": 4.50720739364624,
      "learning_rate": 4.24812030075188e-05,
      "loss": 0.6659,
      "step": 988800
    },
    {
      "epoch": 9.02346886634061,
      "grad_norm": 4.1467156410217285,
      "learning_rate": 4.248044261138283e-05,
      "loss": 0.6426,
      "step": 988900
    },
    {
      "epoch": 9.024381341703775,
      "grad_norm": 3.5688953399658203,
      "learning_rate": 4.247968221524686e-05,
      "loss": 0.6863,
      "step": 989000
    },
    {
      "epoch": 9.02529381706694,
      "grad_norm": 4.689392566680908,
      "learning_rate": 4.247892181911089e-05,
      "loss": 0.6774,
      "step": 989100
    },
    {
      "epoch": 9.026206292430105,
      "grad_norm": 4.0082783699035645,
      "learning_rate": 4.247816142297492e-05,
      "loss": 0.6438,
      "step": 989200
    },
    {
      "epoch": 9.027118767793269,
      "grad_norm": 3.4840993881225586,
      "learning_rate": 4.247740102683894e-05,
      "loss": 0.6693,
      "step": 989300
    },
    {
      "epoch": 9.028031243156434,
      "grad_norm": 4.507316589355469,
      "learning_rate": 4.247664063070298e-05,
      "loss": 0.6855,
      "step": 989400
    },
    {
      "epoch": 9.0289437185196,
      "grad_norm": 3.783522367477417,
      "learning_rate": 4.2475880234567e-05,
      "loss": 0.6618,
      "step": 989500
    },
    {
      "epoch": 9.029856193882765,
      "grad_norm": 3.4614219665527344,
      "learning_rate": 4.247511983843103e-05,
      "loss": 0.6974,
      "step": 989600
    },
    {
      "epoch": 9.03076866924593,
      "grad_norm": 3.1566855907440186,
      "learning_rate": 4.247435944229506e-05,
      "loss": 0.6739,
      "step": 989700
    },
    {
      "epoch": 9.031681144609095,
      "grad_norm": 4.143608570098877,
      "learning_rate": 4.247359904615909e-05,
      "loss": 0.6758,
      "step": 989800
    },
    {
      "epoch": 9.03259361997226,
      "grad_norm": 4.534526824951172,
      "learning_rate": 4.2472838650023114e-05,
      "loss": 0.667,
      "step": 989900
    },
    {
      "epoch": 9.033506095335426,
      "grad_norm": 3.8564565181732178,
      "learning_rate": 4.247207825388715e-05,
      "loss": 0.6624,
      "step": 990000
    },
    {
      "epoch": 9.034418570698591,
      "grad_norm": 3.769296407699585,
      "learning_rate": 4.2471317857751174e-05,
      "loss": 0.6614,
      "step": 990100
    },
    {
      "epoch": 9.035331046061756,
      "grad_norm": 4.421175003051758,
      "learning_rate": 4.2470557461615204e-05,
      "loss": 0.6834,
      "step": 990200
    },
    {
      "epoch": 9.036243521424922,
      "grad_norm": 2.7274229526519775,
      "learning_rate": 4.2469797065479234e-05,
      "loss": 0.6746,
      "step": 990300
    },
    {
      "epoch": 9.037155996788087,
      "grad_norm": 4.410780906677246,
      "learning_rate": 4.2469036669343265e-05,
      "loss": 0.6668,
      "step": 990400
    },
    {
      "epoch": 9.038068472151252,
      "grad_norm": 5.216732025146484,
      "learning_rate": 4.2468276273207295e-05,
      "loss": 0.6508,
      "step": 990500
    },
    {
      "epoch": 9.038980947514418,
      "grad_norm": 3.5199804306030273,
      "learning_rate": 4.2467515877071325e-05,
      "loss": 0.698,
      "step": 990600
    },
    {
      "epoch": 9.039893422877583,
      "grad_norm": 3.8263485431671143,
      "learning_rate": 4.246675548093535e-05,
      "loss": 0.7003,
      "step": 990700
    },
    {
      "epoch": 9.040805898240748,
      "grad_norm": 4.263550758361816,
      "learning_rate": 4.2465995084799385e-05,
      "loss": 0.646,
      "step": 990800
    },
    {
      "epoch": 9.041718373603914,
      "grad_norm": 4.296667098999023,
      "learning_rate": 4.246523468866341e-05,
      "loss": 0.696,
      "step": 990900
    },
    {
      "epoch": 9.042630848967077,
      "grad_norm": 4.087649822235107,
      "learning_rate": 4.246447429252744e-05,
      "loss": 0.7112,
      "step": 991000
    },
    {
      "epoch": 9.043543324330242,
      "grad_norm": 4.016242980957031,
      "learning_rate": 4.246371389639147e-05,
      "loss": 0.6371,
      "step": 991100
    },
    {
      "epoch": 9.044455799693408,
      "grad_norm": 3.720762014389038,
      "learning_rate": 4.24629535002555e-05,
      "loss": 0.6434,
      "step": 991200
    },
    {
      "epoch": 9.045368275056573,
      "grad_norm": 3.59704327583313,
      "learning_rate": 4.246219310411952e-05,
      "loss": 0.708,
      "step": 991300
    },
    {
      "epoch": 9.046280750419738,
      "grad_norm": 3.0873520374298096,
      "learning_rate": 4.246143270798355e-05,
      "loss": 0.6978,
      "step": 991400
    },
    {
      "epoch": 9.047193225782904,
      "grad_norm": 4.333673000335693,
      "learning_rate": 4.246067231184758e-05,
      "loss": 0.6914,
      "step": 991500
    },
    {
      "epoch": 9.048105701146069,
      "grad_norm": 3.8438847064971924,
      "learning_rate": 4.245991191571161e-05,
      "loss": 0.7219,
      "step": 991600
    },
    {
      "epoch": 9.049018176509234,
      "grad_norm": 4.174044132232666,
      "learning_rate": 4.245915151957564e-05,
      "loss": 0.7033,
      "step": 991700
    },
    {
      "epoch": 9.0499306518724,
      "grad_norm": 4.430037498474121,
      "learning_rate": 4.2458391123439665e-05,
      "loss": 0.6862,
      "step": 991800
    },
    {
      "epoch": 9.050843127235565,
      "grad_norm": 3.934051275253296,
      "learning_rate": 4.24576307273037e-05,
      "loss": 0.6888,
      "step": 991900
    },
    {
      "epoch": 9.05175560259873,
      "grad_norm": 3.5119009017944336,
      "learning_rate": 4.2456870331167725e-05,
      "loss": 0.7237,
      "step": 992000
    },
    {
      "epoch": 9.052668077961895,
      "grad_norm": 3.859905242919922,
      "learning_rate": 4.2456109935031755e-05,
      "loss": 0.7109,
      "step": 992100
    },
    {
      "epoch": 9.05358055332506,
      "grad_norm": 3.428394079208374,
      "learning_rate": 4.2455349538895785e-05,
      "loss": 0.6739,
      "step": 992200
    },
    {
      "epoch": 9.054493028688226,
      "grad_norm": 3.930764675140381,
      "learning_rate": 4.2454589142759816e-05,
      "loss": 0.6464,
      "step": 992300
    },
    {
      "epoch": 9.055405504051391,
      "grad_norm": 4.273039817810059,
      "learning_rate": 4.245382874662384e-05,
      "loss": 0.7086,
      "step": 992400
    },
    {
      "epoch": 9.056317979414557,
      "grad_norm": 3.897806406021118,
      "learning_rate": 4.2453068350487876e-05,
      "loss": 0.7067,
      "step": 992500
    },
    {
      "epoch": 9.057230454777722,
      "grad_norm": 4.1897969245910645,
      "learning_rate": 4.24523079543519e-05,
      "loss": 0.6602,
      "step": 992600
    },
    {
      "epoch": 9.058142930140885,
      "grad_norm": 3.3308255672454834,
      "learning_rate": 4.245154755821593e-05,
      "loss": 0.6776,
      "step": 992700
    },
    {
      "epoch": 9.05905540550405,
      "grad_norm": 3.704439163208008,
      "learning_rate": 4.245078716207996e-05,
      "loss": 0.7052,
      "step": 992800
    },
    {
      "epoch": 9.059967880867216,
      "grad_norm": 4.208245277404785,
      "learning_rate": 4.245002676594399e-05,
      "loss": 0.6295,
      "step": 992900
    },
    {
      "epoch": 9.060880356230381,
      "grad_norm": 4.165113925933838,
      "learning_rate": 4.244926636980802e-05,
      "loss": 0.6216,
      "step": 993000
    },
    {
      "epoch": 9.061792831593547,
      "grad_norm": 4.137325763702393,
      "learning_rate": 4.244850597367205e-05,
      "loss": 0.7053,
      "step": 993100
    },
    {
      "epoch": 9.062705306956712,
      "grad_norm": 3.1956424713134766,
      "learning_rate": 4.244774557753607e-05,
      "loss": 0.6734,
      "step": 993200
    },
    {
      "epoch": 9.063617782319877,
      "grad_norm": 4.168454647064209,
      "learning_rate": 4.244698518140011e-05,
      "loss": 0.6325,
      "step": 993300
    },
    {
      "epoch": 9.064530257683042,
      "grad_norm": 3.9402401447296143,
      "learning_rate": 4.244622478526413e-05,
      "loss": 0.6728,
      "step": 993400
    },
    {
      "epoch": 9.065442733046208,
      "grad_norm": 3.6701130867004395,
      "learning_rate": 4.244546438912816e-05,
      "loss": 0.6946,
      "step": 993500
    },
    {
      "epoch": 9.066355208409373,
      "grad_norm": 4.076098442077637,
      "learning_rate": 4.244470399299219e-05,
      "loss": 0.6736,
      "step": 993600
    },
    {
      "epoch": 9.067267683772538,
      "grad_norm": 4.072411060333252,
      "learning_rate": 4.244394359685622e-05,
      "loss": 0.7003,
      "step": 993700
    },
    {
      "epoch": 9.068180159135704,
      "grad_norm": 4.173089504241943,
      "learning_rate": 4.2443183200720246e-05,
      "loss": 0.6624,
      "step": 993800
    },
    {
      "epoch": 9.069092634498869,
      "grad_norm": 3.0148279666900635,
      "learning_rate": 4.244242280458428e-05,
      "loss": 0.6733,
      "step": 993900
    },
    {
      "epoch": 9.070005109862034,
      "grad_norm": 4.165518760681152,
      "learning_rate": 4.2441662408448306e-05,
      "loss": 0.6577,
      "step": 994000
    },
    {
      "epoch": 9.0709175852252,
      "grad_norm": 3.6839239597320557,
      "learning_rate": 4.2440902012312336e-05,
      "loss": 0.7542,
      "step": 994100
    },
    {
      "epoch": 9.071830060588365,
      "grad_norm": 3.983856439590454,
      "learning_rate": 4.2440141616176366e-05,
      "loss": 0.6949,
      "step": 994200
    },
    {
      "epoch": 9.07274253595153,
      "grad_norm": 4.021657943725586,
      "learning_rate": 4.243938122004039e-05,
      "loss": 0.6666,
      "step": 994300
    },
    {
      "epoch": 9.073655011314694,
      "grad_norm": 4.538328170776367,
      "learning_rate": 4.2438620823904427e-05,
      "loss": 0.6901,
      "step": 994400
    },
    {
      "epoch": 9.074567486677859,
      "grad_norm": 4.030138969421387,
      "learning_rate": 4.243786042776845e-05,
      "loss": 0.7105,
      "step": 994500
    },
    {
      "epoch": 9.075479962041024,
      "grad_norm": 4.086736679077148,
      "learning_rate": 4.243710003163248e-05,
      "loss": 0.7133,
      "step": 994600
    },
    {
      "epoch": 9.07639243740419,
      "grad_norm": 4.63868522644043,
      "learning_rate": 4.243633963549651e-05,
      "loss": 0.7152,
      "step": 994700
    },
    {
      "epoch": 9.077304912767355,
      "grad_norm": 4.214611530303955,
      "learning_rate": 4.243557923936054e-05,
      "loss": 0.6962,
      "step": 994800
    },
    {
      "epoch": 9.07821738813052,
      "grad_norm": 3.221776247024536,
      "learning_rate": 4.243481884322456e-05,
      "loss": 0.6652,
      "step": 994900
    },
    {
      "epoch": 9.079129863493685,
      "grad_norm": 4.792639255523682,
      "learning_rate": 4.24340584470886e-05,
      "loss": 0.6466,
      "step": 995000
    },
    {
      "epoch": 9.08004233885685,
      "grad_norm": 4.947932720184326,
      "learning_rate": 4.2433298050952624e-05,
      "loss": 0.687,
      "step": 995100
    },
    {
      "epoch": 9.080954814220016,
      "grad_norm": 4.241794586181641,
      "learning_rate": 4.2432537654816654e-05,
      "loss": 0.6634,
      "step": 995200
    },
    {
      "epoch": 9.081867289583181,
      "grad_norm": 3.9629063606262207,
      "learning_rate": 4.2431777258680684e-05,
      "loss": 0.6616,
      "step": 995300
    },
    {
      "epoch": 9.082779764946347,
      "grad_norm": 3.3251664638519287,
      "learning_rate": 4.2431016862544714e-05,
      "loss": 0.6905,
      "step": 995400
    },
    {
      "epoch": 9.083692240309512,
      "grad_norm": 4.233701705932617,
      "learning_rate": 4.2430256466408744e-05,
      "loss": 0.6868,
      "step": 995500
    },
    {
      "epoch": 9.084604715672677,
      "grad_norm": 3.152334690093994,
      "learning_rate": 4.2429496070272774e-05,
      "loss": 0.6581,
      "step": 995600
    },
    {
      "epoch": 9.085517191035843,
      "grad_norm": 3.5622880458831787,
      "learning_rate": 4.24287356741368e-05,
      "loss": 0.7001,
      "step": 995700
    },
    {
      "epoch": 9.086429666399008,
      "grad_norm": 2.8820688724517822,
      "learning_rate": 4.2427975278000834e-05,
      "loss": 0.6901,
      "step": 995800
    },
    {
      "epoch": 9.087342141762173,
      "grad_norm": 4.606955051422119,
      "learning_rate": 4.242721488186486e-05,
      "loss": 0.6678,
      "step": 995900
    },
    {
      "epoch": 9.088254617125338,
      "grad_norm": 3.866661310195923,
      "learning_rate": 4.242645448572889e-05,
      "loss": 0.6879,
      "step": 996000
    },
    {
      "epoch": 9.089167092488502,
      "grad_norm": 3.651728630065918,
      "learning_rate": 4.242569408959292e-05,
      "loss": 0.6887,
      "step": 996100
    },
    {
      "epoch": 9.090079567851667,
      "grad_norm": 3.9135489463806152,
      "learning_rate": 4.242493369345695e-05,
      "loss": 0.6478,
      "step": 996200
    },
    {
      "epoch": 9.090992043214833,
      "grad_norm": 3.5546422004699707,
      "learning_rate": 4.242417329732097e-05,
      "loss": 0.676,
      "step": 996300
    },
    {
      "epoch": 9.091904518577998,
      "grad_norm": 3.7210657596588135,
      "learning_rate": 4.242341290118501e-05,
      "loss": 0.7003,
      "step": 996400
    },
    {
      "epoch": 9.092816993941163,
      "grad_norm": 4.014716148376465,
      "learning_rate": 4.242265250504903e-05,
      "loss": 0.6483,
      "step": 996500
    },
    {
      "epoch": 9.093729469304328,
      "grad_norm": 3.373776912689209,
      "learning_rate": 4.242189210891306e-05,
      "loss": 0.6766,
      "step": 996600
    },
    {
      "epoch": 9.094641944667494,
      "grad_norm": 4.519649028778076,
      "learning_rate": 4.242113171277709e-05,
      "loss": 0.6992,
      "step": 996700
    },
    {
      "epoch": 9.095554420030659,
      "grad_norm": 3.713190793991089,
      "learning_rate": 4.242037131664112e-05,
      "loss": 0.6763,
      "step": 996800
    },
    {
      "epoch": 9.096466895393824,
      "grad_norm": 3.755406618118286,
      "learning_rate": 4.241961092050515e-05,
      "loss": 0.671,
      "step": 996900
    },
    {
      "epoch": 9.09737937075699,
      "grad_norm": 5.0221991539001465,
      "learning_rate": 4.2418850524369174e-05,
      "loss": 0.6962,
      "step": 997000
    },
    {
      "epoch": 9.098291846120155,
      "grad_norm": 4.401358127593994,
      "learning_rate": 4.2418090128233205e-05,
      "loss": 0.6869,
      "step": 997100
    },
    {
      "epoch": 9.09920432148332,
      "grad_norm": 4.001951694488525,
      "learning_rate": 4.2417329732097235e-05,
      "loss": 0.7031,
      "step": 997200
    },
    {
      "epoch": 9.100116796846486,
      "grad_norm": 6.085352897644043,
      "learning_rate": 4.2416569335961265e-05,
      "loss": 0.6724,
      "step": 997300
    },
    {
      "epoch": 9.10102927220965,
      "grad_norm": 3.0352272987365723,
      "learning_rate": 4.241580893982529e-05,
      "loss": 0.6622,
      "step": 997400
    },
    {
      "epoch": 9.101941747572816,
      "grad_norm": 3.335618495941162,
      "learning_rate": 4.2415048543689325e-05,
      "loss": 0.6978,
      "step": 997500
    },
    {
      "epoch": 9.102854222935981,
      "grad_norm": 4.732894420623779,
      "learning_rate": 4.241428814755335e-05,
      "loss": 0.7009,
      "step": 997600
    },
    {
      "epoch": 9.103766698299147,
      "grad_norm": 4.223591327667236,
      "learning_rate": 4.241352775141738e-05,
      "loss": 0.6973,
      "step": 997700
    },
    {
      "epoch": 9.10467917366231,
      "grad_norm": 5.024528980255127,
      "learning_rate": 4.241276735528141e-05,
      "loss": 0.7259,
      "step": 997800
    },
    {
      "epoch": 9.105591649025476,
      "grad_norm": 4.221793174743652,
      "learning_rate": 4.241200695914544e-05,
      "loss": 0.6943,
      "step": 997900
    },
    {
      "epoch": 9.10650412438864,
      "grad_norm": 4.731501579284668,
      "learning_rate": 4.241124656300947e-05,
      "loss": 0.7033,
      "step": 998000
    },
    {
      "epoch": 9.107416599751806,
      "grad_norm": 3.83807635307312,
      "learning_rate": 4.24104861668735e-05,
      "loss": 0.6307,
      "step": 998100
    },
    {
      "epoch": 9.108329075114971,
      "grad_norm": 3.430365800857544,
      "learning_rate": 4.240972577073752e-05,
      "loss": 0.6919,
      "step": 998200
    },
    {
      "epoch": 9.109241550478137,
      "grad_norm": 3.7149484157562256,
      "learning_rate": 4.240896537460156e-05,
      "loss": 0.6883,
      "step": 998300
    },
    {
      "epoch": 9.110154025841302,
      "grad_norm": 3.157634973526001,
      "learning_rate": 4.240820497846558e-05,
      "loss": 0.6787,
      "step": 998400
    },
    {
      "epoch": 9.111066501204467,
      "grad_norm": 4.0279221534729,
      "learning_rate": 4.240744458232961e-05,
      "loss": 0.6814,
      "step": 998500
    },
    {
      "epoch": 9.111978976567633,
      "grad_norm": 3.6775453090667725,
      "learning_rate": 4.240668418619364e-05,
      "loss": 0.6885,
      "step": 998600
    },
    {
      "epoch": 9.112891451930798,
      "grad_norm": 4.467973709106445,
      "learning_rate": 4.240592379005767e-05,
      "loss": 0.6578,
      "step": 998700
    },
    {
      "epoch": 9.113803927293963,
      "grad_norm": 4.173099517822266,
      "learning_rate": 4.2405163393921695e-05,
      "loss": 0.6794,
      "step": 998800
    },
    {
      "epoch": 9.114716402657129,
      "grad_norm": 4.3006768226623535,
      "learning_rate": 4.240440299778573e-05,
      "loss": 0.6794,
      "step": 998900
    },
    {
      "epoch": 9.115628878020294,
      "grad_norm": 3.8174400329589844,
      "learning_rate": 4.2403642601649755e-05,
      "loss": 0.7206,
      "step": 999000
    },
    {
      "epoch": 9.11654135338346,
      "grad_norm": 3.9872238636016846,
      "learning_rate": 4.2402882205513786e-05,
      "loss": 0.7011,
      "step": 999100
    },
    {
      "epoch": 9.117453828746624,
      "grad_norm": 4.151017189025879,
      "learning_rate": 4.2402121809377816e-05,
      "loss": 0.7038,
      "step": 999200
    },
    {
      "epoch": 9.11836630410979,
      "grad_norm": 3.9730658531188965,
      "learning_rate": 4.2401361413241846e-05,
      "loss": 0.6977,
      "step": 999300
    },
    {
      "epoch": 9.119278779472955,
      "grad_norm": 3.1285080909729004,
      "learning_rate": 4.2400601017105876e-05,
      "loss": 0.7046,
      "step": 999400
    },
    {
      "epoch": 9.120191254836119,
      "grad_norm": 3.809692144393921,
      "learning_rate": 4.2399840620969906e-05,
      "loss": 0.6728,
      "step": 999500
    },
    {
      "epoch": 9.121103730199284,
      "grad_norm": 4.480796813964844,
      "learning_rate": 4.239908022483393e-05,
      "loss": 0.6784,
      "step": 999600
    },
    {
      "epoch": 9.12201620556245,
      "grad_norm": 3.5998494625091553,
      "learning_rate": 4.2398319828697966e-05,
      "loss": 0.6925,
      "step": 999700
    },
    {
      "epoch": 9.122928680925614,
      "grad_norm": 4.162919998168945,
      "learning_rate": 4.239755943256199e-05,
      "loss": 0.701,
      "step": 999800
    },
    {
      "epoch": 9.12384115628878,
      "grad_norm": 3.7808918952941895,
      "learning_rate": 4.239679903642601e-05,
      "loss": 0.7056,
      "step": 999900
    },
    {
      "epoch": 9.124753631651945,
      "grad_norm": 4.181920051574707,
      "learning_rate": 4.239603864029005e-05,
      "loss": 0.7042,
      "step": 1000000
    },
    {
      "epoch": 9.12566610701511,
      "grad_norm": 3.8921587467193604,
      "learning_rate": 4.239527824415407e-05,
      "loss": 0.6899,
      "step": 1000100
    },
    {
      "epoch": 9.126578582378276,
      "grad_norm": 3.771798610687256,
      "learning_rate": 4.23945178480181e-05,
      "loss": 0.6907,
      "step": 1000200
    },
    {
      "epoch": 9.127491057741441,
      "grad_norm": 4.5093994140625,
      "learning_rate": 4.239375745188213e-05,
      "loss": 0.6631,
      "step": 1000300
    },
    {
      "epoch": 9.128403533104606,
      "grad_norm": 4.18259334564209,
      "learning_rate": 4.239299705574616e-05,
      "loss": 0.7153,
      "step": 1000400
    },
    {
      "epoch": 9.129316008467772,
      "grad_norm": 4.210383415222168,
      "learning_rate": 4.239223665961019e-05,
      "loss": 0.6731,
      "step": 1000500
    },
    {
      "epoch": 9.130228483830937,
      "grad_norm": 3.9406986236572266,
      "learning_rate": 4.239147626347422e-05,
      "loss": 0.6795,
      "step": 1000600
    },
    {
      "epoch": 9.131140959194102,
      "grad_norm": 3.81520676612854,
      "learning_rate": 4.2390715867338246e-05,
      "loss": 0.7038,
      "step": 1000700
    },
    {
      "epoch": 9.132053434557267,
      "grad_norm": 3.586968421936035,
      "learning_rate": 4.238995547120228e-05,
      "loss": 0.6656,
      "step": 1000800
    },
    {
      "epoch": 9.132965909920433,
      "grad_norm": 3.7744219303131104,
      "learning_rate": 4.2389195075066306e-05,
      "loss": 0.7101,
      "step": 1000900
    },
    {
      "epoch": 9.133878385283598,
      "grad_norm": 3.7650699615478516,
      "learning_rate": 4.2388434678930336e-05,
      "loss": 0.6643,
      "step": 1001000
    },
    {
      "epoch": 9.134790860646763,
      "grad_norm": 4.137081146240234,
      "learning_rate": 4.2387674282794367e-05,
      "loss": 0.6492,
      "step": 1001100
    },
    {
      "epoch": 9.135703336009927,
      "grad_norm": 4.845391273498535,
      "learning_rate": 4.23869138866584e-05,
      "loss": 0.7107,
      "step": 1001200
    },
    {
      "epoch": 9.136615811373092,
      "grad_norm": 3.627626657485962,
      "learning_rate": 4.238615349052243e-05,
      "loss": 0.6901,
      "step": 1001300
    },
    {
      "epoch": 9.137528286736257,
      "grad_norm": 3.3040833473205566,
      "learning_rate": 4.238539309438646e-05,
      "loss": 0.6823,
      "step": 1001400
    },
    {
      "epoch": 9.138440762099423,
      "grad_norm": 3.970717191696167,
      "learning_rate": 4.238463269825048e-05,
      "loss": 0.6864,
      "step": 1001500
    },
    {
      "epoch": 9.139353237462588,
      "grad_norm": 3.045030355453491,
      "learning_rate": 4.238387230211451e-05,
      "loss": 0.6857,
      "step": 1001600
    },
    {
      "epoch": 9.140265712825753,
      "grad_norm": 4.852859973907471,
      "learning_rate": 4.238311190597854e-05,
      "loss": 0.6678,
      "step": 1001700
    },
    {
      "epoch": 9.141178188188919,
      "grad_norm": 4.2646894454956055,
      "learning_rate": 4.238235150984257e-05,
      "loss": 0.6639,
      "step": 1001800
    },
    {
      "epoch": 9.142090663552084,
      "grad_norm": 3.6171088218688965,
      "learning_rate": 4.23815911137066e-05,
      "loss": 0.647,
      "step": 1001900
    },
    {
      "epoch": 9.14300313891525,
      "grad_norm": 3.789923667907715,
      "learning_rate": 4.238083071757063e-05,
      "loss": 0.6963,
      "step": 1002000
    },
    {
      "epoch": 9.143915614278415,
      "grad_norm": 3.3128576278686523,
      "learning_rate": 4.2380070321434654e-05,
      "loss": 0.7205,
      "step": 1002100
    },
    {
      "epoch": 9.14482808964158,
      "grad_norm": 2.945993423461914,
      "learning_rate": 4.237930992529869e-05,
      "loss": 0.7138,
      "step": 1002200
    },
    {
      "epoch": 9.145740565004745,
      "grad_norm": 3.9419608116149902,
      "learning_rate": 4.2378549529162714e-05,
      "loss": 0.7193,
      "step": 1002300
    },
    {
      "epoch": 9.14665304036791,
      "grad_norm": 4.185678958892822,
      "learning_rate": 4.2377789133026744e-05,
      "loss": 0.6758,
      "step": 1002400
    },
    {
      "epoch": 9.147565515731076,
      "grad_norm": 4.248563766479492,
      "learning_rate": 4.2377028736890774e-05,
      "loss": 0.6843,
      "step": 1002500
    },
    {
      "epoch": 9.148477991094241,
      "grad_norm": 4.288710117340088,
      "learning_rate": 4.23762683407548e-05,
      "loss": 0.7013,
      "step": 1002600
    },
    {
      "epoch": 9.149390466457406,
      "grad_norm": 3.185872793197632,
      "learning_rate": 4.2375507944618834e-05,
      "loss": 0.7392,
      "step": 1002700
    },
    {
      "epoch": 9.150302941820572,
      "grad_norm": 4.106564998626709,
      "learning_rate": 4.237474754848286e-05,
      "loss": 0.6846,
      "step": 1002800
    },
    {
      "epoch": 9.151215417183735,
      "grad_norm": 4.609188079833984,
      "learning_rate": 4.237398715234689e-05,
      "loss": 0.6607,
      "step": 1002900
    },
    {
      "epoch": 9.1521278925469,
      "grad_norm": 3.255047559738159,
      "learning_rate": 4.237322675621092e-05,
      "loss": 0.715,
      "step": 1003000
    },
    {
      "epoch": 9.153040367910066,
      "grad_norm": 3.7732810974121094,
      "learning_rate": 4.237246636007495e-05,
      "loss": 0.7017,
      "step": 1003100
    },
    {
      "epoch": 9.153952843273231,
      "grad_norm": 4.469143867492676,
      "learning_rate": 4.237170596393897e-05,
      "loss": 0.6606,
      "step": 1003200
    },
    {
      "epoch": 9.154865318636396,
      "grad_norm": 3.9850594997406006,
      "learning_rate": 4.237094556780301e-05,
      "loss": 0.6865,
      "step": 1003300
    },
    {
      "epoch": 9.155777793999562,
      "grad_norm": 3.56988525390625,
      "learning_rate": 4.237018517166703e-05,
      "loss": 0.7033,
      "step": 1003400
    },
    {
      "epoch": 9.156690269362727,
      "grad_norm": 3.3262221813201904,
      "learning_rate": 4.236942477553106e-05,
      "loss": 0.696,
      "step": 1003500
    },
    {
      "epoch": 9.157602744725892,
      "grad_norm": 3.8416996002197266,
      "learning_rate": 4.236866437939509e-05,
      "loss": 0.6616,
      "step": 1003600
    },
    {
      "epoch": 9.158515220089058,
      "grad_norm": 3.5708489418029785,
      "learning_rate": 4.236790398325912e-05,
      "loss": 0.6669,
      "step": 1003700
    },
    {
      "epoch": 9.159427695452223,
      "grad_norm": 3.516906261444092,
      "learning_rate": 4.236714358712315e-05,
      "loss": 0.6914,
      "step": 1003800
    },
    {
      "epoch": 9.160340170815388,
      "grad_norm": 4.010988235473633,
      "learning_rate": 4.236638319098718e-05,
      "loss": 0.675,
      "step": 1003900
    },
    {
      "epoch": 9.161252646178554,
      "grad_norm": 4.592622756958008,
      "learning_rate": 4.2365622794851205e-05,
      "loss": 0.704,
      "step": 1004000
    },
    {
      "epoch": 9.162165121541719,
      "grad_norm": 4.720539093017578,
      "learning_rate": 4.236486239871524e-05,
      "loss": 0.6893,
      "step": 1004100
    },
    {
      "epoch": 9.163077596904884,
      "grad_norm": 4.3479084968566895,
      "learning_rate": 4.2364102002579265e-05,
      "loss": 0.7065,
      "step": 1004200
    },
    {
      "epoch": 9.16399007226805,
      "grad_norm": 4.332796096801758,
      "learning_rate": 4.2363341606443295e-05,
      "loss": 0.7484,
      "step": 1004300
    },
    {
      "epoch": 9.164902547631215,
      "grad_norm": 4.325990200042725,
      "learning_rate": 4.2362581210307325e-05,
      "loss": 0.6604,
      "step": 1004400
    },
    {
      "epoch": 9.16581502299438,
      "grad_norm": 4.431541919708252,
      "learning_rate": 4.2361820814171355e-05,
      "loss": 0.6884,
      "step": 1004500
    },
    {
      "epoch": 9.166727498357544,
      "grad_norm": 4.895318031311035,
      "learning_rate": 4.236106041803538e-05,
      "loss": 0.6667,
      "step": 1004600
    },
    {
      "epoch": 9.167639973720709,
      "grad_norm": 4.164173126220703,
      "learning_rate": 4.2360300021899415e-05,
      "loss": 0.661,
      "step": 1004700
    },
    {
      "epoch": 9.168552449083874,
      "grad_norm": 4.608367919921875,
      "learning_rate": 4.235953962576344e-05,
      "loss": 0.6789,
      "step": 1004800
    },
    {
      "epoch": 9.16946492444704,
      "grad_norm": 5.364264488220215,
      "learning_rate": 4.235877922962747e-05,
      "loss": 0.7002,
      "step": 1004900
    },
    {
      "epoch": 9.170377399810205,
      "grad_norm": 3.937777519226074,
      "learning_rate": 4.23580188334915e-05,
      "loss": 0.7063,
      "step": 1005000
    },
    {
      "epoch": 9.17128987517337,
      "grad_norm": 4.057193756103516,
      "learning_rate": 4.235725843735553e-05,
      "loss": 0.7001,
      "step": 1005100
    },
    {
      "epoch": 9.172202350536535,
      "grad_norm": 4.082249164581299,
      "learning_rate": 4.235649804121956e-05,
      "loss": 0.7114,
      "step": 1005200
    },
    {
      "epoch": 9.1731148258997,
      "grad_norm": 2.518195390701294,
      "learning_rate": 4.235573764508359e-05,
      "loss": 0.6648,
      "step": 1005300
    },
    {
      "epoch": 9.174027301262866,
      "grad_norm": 3.8938252925872803,
      "learning_rate": 4.235497724894761e-05,
      "loss": 0.6767,
      "step": 1005400
    },
    {
      "epoch": 9.174939776626031,
      "grad_norm": 4.098297595977783,
      "learning_rate": 4.235421685281164e-05,
      "loss": 0.6836,
      "step": 1005500
    },
    {
      "epoch": 9.175852251989197,
      "grad_norm": 3.6391074657440186,
      "learning_rate": 4.235345645667567e-05,
      "loss": 0.6689,
      "step": 1005600
    },
    {
      "epoch": 9.176764727352362,
      "grad_norm": 3.4406306743621826,
      "learning_rate": 4.2352696060539695e-05,
      "loss": 0.6881,
      "step": 1005700
    },
    {
      "epoch": 9.177677202715527,
      "grad_norm": 4.644636154174805,
      "learning_rate": 4.235193566440373e-05,
      "loss": 0.6764,
      "step": 1005800
    },
    {
      "epoch": 9.178589678078692,
      "grad_norm": 3.629600763320923,
      "learning_rate": 4.2351175268267756e-05,
      "loss": 0.6882,
      "step": 1005900
    },
    {
      "epoch": 9.179502153441858,
      "grad_norm": 3.802757501602173,
      "learning_rate": 4.2350414872131786e-05,
      "loss": 0.6703,
      "step": 1006000
    },
    {
      "epoch": 9.180414628805023,
      "grad_norm": 3.07163143157959,
      "learning_rate": 4.2349654475995816e-05,
      "loss": 0.6973,
      "step": 1006100
    },
    {
      "epoch": 9.181327104168188,
      "grad_norm": 4.426400184631348,
      "learning_rate": 4.2348894079859846e-05,
      "loss": 0.7035,
      "step": 1006200
    },
    {
      "epoch": 9.182239579531352,
      "grad_norm": 5.614645004272461,
      "learning_rate": 4.2348133683723876e-05,
      "loss": 0.6814,
      "step": 1006300
    },
    {
      "epoch": 9.183152054894517,
      "grad_norm": 4.170994281768799,
      "learning_rate": 4.2347373287587906e-05,
      "loss": 0.6926,
      "step": 1006400
    },
    {
      "epoch": 9.184064530257682,
      "grad_norm": 4.175393104553223,
      "learning_rate": 4.234661289145193e-05,
      "loss": 0.6878,
      "step": 1006500
    },
    {
      "epoch": 9.184977005620848,
      "grad_norm": 3.450143575668335,
      "learning_rate": 4.2345852495315966e-05,
      "loss": 0.692,
      "step": 1006600
    },
    {
      "epoch": 9.185889480984013,
      "grad_norm": 3.1755027770996094,
      "learning_rate": 4.234509209917999e-05,
      "loss": 0.6837,
      "step": 1006700
    },
    {
      "epoch": 9.186801956347178,
      "grad_norm": 4.06076192855835,
      "learning_rate": 4.234433170304402e-05,
      "loss": 0.6835,
      "step": 1006800
    },
    {
      "epoch": 9.187714431710344,
      "grad_norm": 3.4100589752197266,
      "learning_rate": 4.234357130690805e-05,
      "loss": 0.7032,
      "step": 1006900
    },
    {
      "epoch": 9.188626907073509,
      "grad_norm": 4.261612892150879,
      "learning_rate": 4.234281091077208e-05,
      "loss": 0.7164,
      "step": 1007000
    },
    {
      "epoch": 9.189539382436674,
      "grad_norm": 3.8950917720794678,
      "learning_rate": 4.23420505146361e-05,
      "loss": 0.6754,
      "step": 1007100
    },
    {
      "epoch": 9.19045185779984,
      "grad_norm": 3.179936647415161,
      "learning_rate": 4.234129011850014e-05,
      "loss": 0.6921,
      "step": 1007200
    },
    {
      "epoch": 9.191364333163005,
      "grad_norm": 3.08827805519104,
      "learning_rate": 4.234052972236416e-05,
      "loss": 0.6633,
      "step": 1007300
    },
    {
      "epoch": 9.19227680852617,
      "grad_norm": 4.570652961730957,
      "learning_rate": 4.233976932622819e-05,
      "loss": 0.7,
      "step": 1007400
    },
    {
      "epoch": 9.193189283889335,
      "grad_norm": 4.040396213531494,
      "learning_rate": 4.233900893009222e-05,
      "loss": 0.6867,
      "step": 1007500
    },
    {
      "epoch": 9.1941017592525,
      "grad_norm": 3.4311628341674805,
      "learning_rate": 4.233824853395625e-05,
      "loss": 0.7108,
      "step": 1007600
    },
    {
      "epoch": 9.195014234615666,
      "grad_norm": 4.841002941131592,
      "learning_rate": 4.233748813782028e-05,
      "loss": 0.6417,
      "step": 1007700
    },
    {
      "epoch": 9.195926709978831,
      "grad_norm": 2.9645678997039795,
      "learning_rate": 4.233672774168431e-05,
      "loss": 0.6965,
      "step": 1007800
    },
    {
      "epoch": 9.196839185341997,
      "grad_norm": 3.486936092376709,
      "learning_rate": 4.2335967345548337e-05,
      "loss": 0.6834,
      "step": 1007900
    },
    {
      "epoch": 9.19775166070516,
      "grad_norm": 3.477900743484497,
      "learning_rate": 4.2335206949412373e-05,
      "loss": 0.678,
      "step": 1008000
    },
    {
      "epoch": 9.198664136068325,
      "grad_norm": 4.0541605949401855,
      "learning_rate": 4.23344465532764e-05,
      "loss": 0.6641,
      "step": 1008100
    },
    {
      "epoch": 9.19957661143149,
      "grad_norm": 3.9712073802948,
      "learning_rate": 4.233368615714043e-05,
      "loss": 0.6776,
      "step": 1008200
    },
    {
      "epoch": 9.200489086794656,
      "grad_norm": 4.718518257141113,
      "learning_rate": 4.233292576100446e-05,
      "loss": 0.6823,
      "step": 1008300
    },
    {
      "epoch": 9.201401562157821,
      "grad_norm": 4.989162921905518,
      "learning_rate": 4.233216536486848e-05,
      "loss": 0.6806,
      "step": 1008400
    },
    {
      "epoch": 9.202314037520987,
      "grad_norm": 4.022651195526123,
      "learning_rate": 4.233140496873251e-05,
      "loss": 0.6841,
      "step": 1008500
    },
    {
      "epoch": 9.203226512884152,
      "grad_norm": 3.273362398147583,
      "learning_rate": 4.233064457259654e-05,
      "loss": 0.7221,
      "step": 1008600
    },
    {
      "epoch": 9.204138988247317,
      "grad_norm": 3.0761396884918213,
      "learning_rate": 4.232988417646057e-05,
      "loss": 0.7094,
      "step": 1008700
    },
    {
      "epoch": 9.205051463610483,
      "grad_norm": 3.5660104751586914,
      "learning_rate": 4.23291237803246e-05,
      "loss": 0.6798,
      "step": 1008800
    },
    {
      "epoch": 9.205963938973648,
      "grad_norm": 4.331326007843018,
      "learning_rate": 4.232836338418863e-05,
      "loss": 0.696,
      "step": 1008900
    },
    {
      "epoch": 9.206876414336813,
      "grad_norm": 3.1985578536987305,
      "learning_rate": 4.2327602988052654e-05,
      "loss": 0.7166,
      "step": 1009000
    },
    {
      "epoch": 9.207788889699978,
      "grad_norm": 4.516120910644531,
      "learning_rate": 4.232684259191669e-05,
      "loss": 0.694,
      "step": 1009100
    },
    {
      "epoch": 9.208701365063144,
      "grad_norm": 4.021616458892822,
      "learning_rate": 4.2326082195780714e-05,
      "loss": 0.6463,
      "step": 1009200
    },
    {
      "epoch": 9.209613840426309,
      "grad_norm": 3.9598968029022217,
      "learning_rate": 4.2325321799644744e-05,
      "loss": 0.6674,
      "step": 1009300
    },
    {
      "epoch": 9.210526315789474,
      "grad_norm": 3.989555835723877,
      "learning_rate": 4.2324561403508774e-05,
      "loss": 0.6669,
      "step": 1009400
    },
    {
      "epoch": 9.21143879115264,
      "grad_norm": 4.239436149597168,
      "learning_rate": 4.2323801007372804e-05,
      "loss": 0.6964,
      "step": 1009500
    },
    {
      "epoch": 9.212351266515805,
      "grad_norm": 4.134191036224365,
      "learning_rate": 4.232304061123683e-05,
      "loss": 0.6551,
      "step": 1009600
    },
    {
      "epoch": 9.213263741878968,
      "grad_norm": 4.292231559753418,
      "learning_rate": 4.2322280215100864e-05,
      "loss": 0.7104,
      "step": 1009700
    },
    {
      "epoch": 9.214176217242134,
      "grad_norm": 4.343762397766113,
      "learning_rate": 4.232151981896489e-05,
      "loss": 0.6656,
      "step": 1009800
    },
    {
      "epoch": 9.215088692605299,
      "grad_norm": 4.013726234436035,
      "learning_rate": 4.232075942282892e-05,
      "loss": 0.705,
      "step": 1009900
    },
    {
      "epoch": 9.216001167968464,
      "grad_norm": 3.3336939811706543,
      "learning_rate": 4.231999902669295e-05,
      "loss": 0.6989,
      "step": 1010000
    },
    {
      "epoch": 9.21691364333163,
      "grad_norm": 3.6037824153900146,
      "learning_rate": 4.231923863055698e-05,
      "loss": 0.6936,
      "step": 1010100
    },
    {
      "epoch": 9.217826118694795,
      "grad_norm": 4.106587886810303,
      "learning_rate": 4.231847823442101e-05,
      "loss": 0.6726,
      "step": 1010200
    },
    {
      "epoch": 9.21873859405796,
      "grad_norm": 3.5812506675720215,
      "learning_rate": 4.231771783828504e-05,
      "loss": 0.6919,
      "step": 1010300
    },
    {
      "epoch": 9.219651069421126,
      "grad_norm": 5.154552936553955,
      "learning_rate": 4.231695744214906e-05,
      "loss": 0.6825,
      "step": 1010400
    },
    {
      "epoch": 9.22056354478429,
      "grad_norm": 4.373902797698975,
      "learning_rate": 4.23161970460131e-05,
      "loss": 0.7127,
      "step": 1010500
    },
    {
      "epoch": 9.221476020147456,
      "grad_norm": 4.004782199859619,
      "learning_rate": 4.231543664987712e-05,
      "loss": 0.6588,
      "step": 1010600
    },
    {
      "epoch": 9.222388495510621,
      "grad_norm": 3.5797154903411865,
      "learning_rate": 4.231467625374115e-05,
      "loss": 0.6461,
      "step": 1010700
    },
    {
      "epoch": 9.223300970873787,
      "grad_norm": 5.386041164398193,
      "learning_rate": 4.231391585760518e-05,
      "loss": 0.657,
      "step": 1010800
    },
    {
      "epoch": 9.224213446236952,
      "grad_norm": 3.914717674255371,
      "learning_rate": 4.231315546146921e-05,
      "loss": 0.6744,
      "step": 1010900
    },
    {
      "epoch": 9.225125921600117,
      "grad_norm": 4.762959003448486,
      "learning_rate": 4.2312395065333235e-05,
      "loss": 0.6946,
      "step": 1011000
    },
    {
      "epoch": 9.226038396963283,
      "grad_norm": 4.522224426269531,
      "learning_rate": 4.2311634669197265e-05,
      "loss": 0.6864,
      "step": 1011100
    },
    {
      "epoch": 9.226950872326448,
      "grad_norm": 4.159684181213379,
      "learning_rate": 4.2310874273061295e-05,
      "loss": 0.6302,
      "step": 1011200
    },
    {
      "epoch": 9.227863347689613,
      "grad_norm": 2.8297996520996094,
      "learning_rate": 4.2310113876925325e-05,
      "loss": 0.7037,
      "step": 1011300
    },
    {
      "epoch": 9.228775823052777,
      "grad_norm": 3.6565046310424805,
      "learning_rate": 4.2309353480789355e-05,
      "loss": 0.6923,
      "step": 1011400
    },
    {
      "epoch": 9.229688298415942,
      "grad_norm": 5.061120510101318,
      "learning_rate": 4.230859308465338e-05,
      "loss": 0.698,
      "step": 1011500
    },
    {
      "epoch": 9.230600773779107,
      "grad_norm": 3.881319999694824,
      "learning_rate": 4.2307832688517415e-05,
      "loss": 0.6952,
      "step": 1011600
    },
    {
      "epoch": 9.231513249142273,
      "grad_norm": 3.6889965534210205,
      "learning_rate": 4.230707229238144e-05,
      "loss": 0.7111,
      "step": 1011700
    },
    {
      "epoch": 9.232425724505438,
      "grad_norm": 3.960726737976074,
      "learning_rate": 4.230631189624547e-05,
      "loss": 0.6849,
      "step": 1011800
    },
    {
      "epoch": 9.233338199868603,
      "grad_norm": 4.267967224121094,
      "learning_rate": 4.23055515001095e-05,
      "loss": 0.6691,
      "step": 1011900
    },
    {
      "epoch": 9.234250675231769,
      "grad_norm": 4.831088542938232,
      "learning_rate": 4.230479110397353e-05,
      "loss": 0.6941,
      "step": 1012000
    },
    {
      "epoch": 9.235163150594934,
      "grad_norm": 3.9191765785217285,
      "learning_rate": 4.230403070783755e-05,
      "loss": 0.6845,
      "step": 1012100
    },
    {
      "epoch": 9.2360756259581,
      "grad_norm": 2.54862904548645,
      "learning_rate": 4.230327031170159e-05,
      "loss": 0.6553,
      "step": 1012200
    },
    {
      "epoch": 9.236988101321264,
      "grad_norm": 4.1474223136901855,
      "learning_rate": 4.230250991556561e-05,
      "loss": 0.695,
      "step": 1012300
    },
    {
      "epoch": 9.23790057668443,
      "grad_norm": 3.6521244049072266,
      "learning_rate": 4.230174951942964e-05,
      "loss": 0.6729,
      "step": 1012400
    },
    {
      "epoch": 9.238813052047595,
      "grad_norm": 3.705174207687378,
      "learning_rate": 4.230098912329367e-05,
      "loss": 0.7103,
      "step": 1012500
    },
    {
      "epoch": 9.23972552741076,
      "grad_norm": 3.4640910625457764,
      "learning_rate": 4.23002287271577e-05,
      "loss": 0.6527,
      "step": 1012600
    },
    {
      "epoch": 9.240638002773926,
      "grad_norm": 4.0576252937316895,
      "learning_rate": 4.229946833102173e-05,
      "loss": 0.6689,
      "step": 1012700
    },
    {
      "epoch": 9.241550478137091,
      "grad_norm": 3.9828617572784424,
      "learning_rate": 4.229870793488576e-05,
      "loss": 0.7153,
      "step": 1012800
    },
    {
      "epoch": 9.242462953500256,
      "grad_norm": 4.265980243682861,
      "learning_rate": 4.2297947538749786e-05,
      "loss": 0.6857,
      "step": 1012900
    },
    {
      "epoch": 9.243375428863422,
      "grad_norm": 5.168938636779785,
      "learning_rate": 4.229718714261382e-05,
      "loss": 0.6814,
      "step": 1013000
    },
    {
      "epoch": 9.244287904226585,
      "grad_norm": 4.3421311378479,
      "learning_rate": 4.2296426746477846e-05,
      "loss": 0.6584,
      "step": 1013100
    },
    {
      "epoch": 9.24520037958975,
      "grad_norm": 3.791320562362671,
      "learning_rate": 4.2295666350341876e-05,
      "loss": 0.6543,
      "step": 1013200
    },
    {
      "epoch": 9.246112854952916,
      "grad_norm": 4.398874282836914,
      "learning_rate": 4.2294905954205906e-05,
      "loss": 0.7006,
      "step": 1013300
    },
    {
      "epoch": 9.247025330316081,
      "grad_norm": 5.315958023071289,
      "learning_rate": 4.2294145558069936e-05,
      "loss": 0.6976,
      "step": 1013400
    },
    {
      "epoch": 9.247937805679246,
      "grad_norm": 3.5351247787475586,
      "learning_rate": 4.229338516193396e-05,
      "loss": 0.6835,
      "step": 1013500
    },
    {
      "epoch": 9.248850281042412,
      "grad_norm": 3.931973695755005,
      "learning_rate": 4.2292624765797996e-05,
      "loss": 0.7401,
      "step": 1013600
    },
    {
      "epoch": 9.249762756405577,
      "grad_norm": 4.228905200958252,
      "learning_rate": 4.229186436966202e-05,
      "loss": 0.6801,
      "step": 1013700
    },
    {
      "epoch": 9.250675231768742,
      "grad_norm": 4.39605712890625,
      "learning_rate": 4.229110397352605e-05,
      "loss": 0.73,
      "step": 1013800
    },
    {
      "epoch": 9.251587707131907,
      "grad_norm": 2.8522212505340576,
      "learning_rate": 4.229034357739008e-05,
      "loss": 0.7078,
      "step": 1013900
    },
    {
      "epoch": 9.252500182495073,
      "grad_norm": 3.7691872119903564,
      "learning_rate": 4.22895831812541e-05,
      "loss": 0.7145,
      "step": 1014000
    },
    {
      "epoch": 9.253412657858238,
      "grad_norm": 4.155939102172852,
      "learning_rate": 4.228882278511814e-05,
      "loss": 0.6987,
      "step": 1014100
    },
    {
      "epoch": 9.254325133221403,
      "grad_norm": 3.6233208179473877,
      "learning_rate": 4.228806238898216e-05,
      "loss": 0.7325,
      "step": 1014200
    },
    {
      "epoch": 9.255237608584569,
      "grad_norm": 3.9881649017333984,
      "learning_rate": 4.228730199284619e-05,
      "loss": 0.6989,
      "step": 1014300
    },
    {
      "epoch": 9.256150083947734,
      "grad_norm": 4.387468338012695,
      "learning_rate": 4.228654159671022e-05,
      "loss": 0.67,
      "step": 1014400
    },
    {
      "epoch": 9.2570625593109,
      "grad_norm": 4.053160190582275,
      "learning_rate": 4.228578120057425e-05,
      "loss": 0.6961,
      "step": 1014500
    },
    {
      "epoch": 9.257975034674065,
      "grad_norm": 4.239496231079102,
      "learning_rate": 4.228502080443828e-05,
      "loss": 0.6848,
      "step": 1014600
    },
    {
      "epoch": 9.25888751003723,
      "grad_norm": 3.8580050468444824,
      "learning_rate": 4.2284260408302313e-05,
      "loss": 0.673,
      "step": 1014700
    },
    {
      "epoch": 9.259799985400393,
      "grad_norm": 4.1181488037109375,
      "learning_rate": 4.228350001216634e-05,
      "loss": 0.7052,
      "step": 1014800
    },
    {
      "epoch": 9.260712460763559,
      "grad_norm": 4.0159406661987305,
      "learning_rate": 4.2282739616030374e-05,
      "loss": 0.7026,
      "step": 1014900
    },
    {
      "epoch": 9.261624936126724,
      "grad_norm": 3.1607868671417236,
      "learning_rate": 4.22819792198944e-05,
      "loss": 0.6661,
      "step": 1015000
    },
    {
      "epoch": 9.26253741148989,
      "grad_norm": 3.965627908706665,
      "learning_rate": 4.228121882375843e-05,
      "loss": 0.6684,
      "step": 1015100
    },
    {
      "epoch": 9.263449886853055,
      "grad_norm": 3.9322593212127686,
      "learning_rate": 4.228045842762246e-05,
      "loss": 0.7445,
      "step": 1015200
    },
    {
      "epoch": 9.26436236221622,
      "grad_norm": 3.829540252685547,
      "learning_rate": 4.227969803148649e-05,
      "loss": 0.6862,
      "step": 1015300
    },
    {
      "epoch": 9.265274837579385,
      "grad_norm": 3.6224098205566406,
      "learning_rate": 4.227893763535051e-05,
      "loss": 0.6889,
      "step": 1015400
    },
    {
      "epoch": 9.26618731294255,
      "grad_norm": 5.514328956604004,
      "learning_rate": 4.227817723921455e-05,
      "loss": 0.7159,
      "step": 1015500
    },
    {
      "epoch": 9.267099788305716,
      "grad_norm": 2.558812379837036,
      "learning_rate": 4.227741684307857e-05,
      "loss": 0.7097,
      "step": 1015600
    },
    {
      "epoch": 9.268012263668881,
      "grad_norm": 4.051952838897705,
      "learning_rate": 4.22766564469426e-05,
      "loss": 0.6882,
      "step": 1015700
    },
    {
      "epoch": 9.268924739032046,
      "grad_norm": 3.9301822185516357,
      "learning_rate": 4.227589605080663e-05,
      "loss": 0.6652,
      "step": 1015800
    },
    {
      "epoch": 9.269837214395212,
      "grad_norm": 3.9218590259552,
      "learning_rate": 4.227513565467066e-05,
      "loss": 0.6877,
      "step": 1015900
    },
    {
      "epoch": 9.270749689758377,
      "grad_norm": 3.7582788467407227,
      "learning_rate": 4.227437525853469e-05,
      "loss": 0.689,
      "step": 1016000
    },
    {
      "epoch": 9.271662165121542,
      "grad_norm": 4.520484447479248,
      "learning_rate": 4.227361486239872e-05,
      "loss": 0.6961,
      "step": 1016100
    },
    {
      "epoch": 9.272574640484708,
      "grad_norm": 3.2525827884674072,
      "learning_rate": 4.2272854466262744e-05,
      "loss": 0.6392,
      "step": 1016200
    },
    {
      "epoch": 9.273487115847873,
      "grad_norm": 4.711410045623779,
      "learning_rate": 4.227209407012678e-05,
      "loss": 0.6889,
      "step": 1016300
    },
    {
      "epoch": 9.274399591211036,
      "grad_norm": 4.287329196929932,
      "learning_rate": 4.2271333673990804e-05,
      "loss": 0.714,
      "step": 1016400
    },
    {
      "epoch": 9.275312066574202,
      "grad_norm": 3.231738328933716,
      "learning_rate": 4.2270573277854834e-05,
      "loss": 0.6499,
      "step": 1016500
    },
    {
      "epoch": 9.276224541937367,
      "grad_norm": 6.4894914627075195,
      "learning_rate": 4.2269812881718864e-05,
      "loss": 0.6889,
      "step": 1016600
    },
    {
      "epoch": 9.277137017300532,
      "grad_norm": 4.37999963760376,
      "learning_rate": 4.2269052485582894e-05,
      "loss": 0.6896,
      "step": 1016700
    },
    {
      "epoch": 9.278049492663698,
      "grad_norm": 4.245206356048584,
      "learning_rate": 4.226829208944692e-05,
      "loss": 0.6912,
      "step": 1016800
    },
    {
      "epoch": 9.278961968026863,
      "grad_norm": 4.129788398742676,
      "learning_rate": 4.226753169331095e-05,
      "loss": 0.6612,
      "step": 1016900
    },
    {
      "epoch": 9.279874443390028,
      "grad_norm": 4.833460807800293,
      "learning_rate": 4.226677129717498e-05,
      "loss": 0.6362,
      "step": 1017000
    },
    {
      "epoch": 9.280786918753194,
      "grad_norm": 3.9440596103668213,
      "learning_rate": 4.226601090103901e-05,
      "loss": 0.6664,
      "step": 1017100
    },
    {
      "epoch": 9.281699394116359,
      "grad_norm": 3.274395227432251,
      "learning_rate": 4.226525050490304e-05,
      "loss": 0.6995,
      "step": 1017200
    },
    {
      "epoch": 9.282611869479524,
      "grad_norm": 4.039734840393066,
      "learning_rate": 4.226449010876706e-05,
      "loss": 0.676,
      "step": 1017300
    },
    {
      "epoch": 9.28352434484269,
      "grad_norm": 4.390150547027588,
      "learning_rate": 4.22637297126311e-05,
      "loss": 0.6856,
      "step": 1017400
    },
    {
      "epoch": 9.284436820205855,
      "grad_norm": 3.6353976726531982,
      "learning_rate": 4.226296931649512e-05,
      "loss": 0.6537,
      "step": 1017500
    },
    {
      "epoch": 9.28534929556902,
      "grad_norm": 4.269376754760742,
      "learning_rate": 4.226220892035915e-05,
      "loss": 0.7018,
      "step": 1017600
    },
    {
      "epoch": 9.286261770932185,
      "grad_norm": 4.020158767700195,
      "learning_rate": 4.226144852422318e-05,
      "loss": 0.6917,
      "step": 1017700
    },
    {
      "epoch": 9.28717424629535,
      "grad_norm": 3.552337169647217,
      "learning_rate": 4.226068812808721e-05,
      "loss": 0.7003,
      "step": 1017800
    },
    {
      "epoch": 9.288086721658516,
      "grad_norm": 4.80539083480835,
      "learning_rate": 4.2259927731951235e-05,
      "loss": 0.6817,
      "step": 1017900
    },
    {
      "epoch": 9.288999197021681,
      "grad_norm": 3.7126855850219727,
      "learning_rate": 4.225916733581527e-05,
      "loss": 0.6901,
      "step": 1018000
    },
    {
      "epoch": 9.289911672384846,
      "grad_norm": 3.564615249633789,
      "learning_rate": 4.2258406939679295e-05,
      "loss": 0.6722,
      "step": 1018100
    },
    {
      "epoch": 9.29082414774801,
      "grad_norm": 3.475800037384033,
      "learning_rate": 4.2257646543543325e-05,
      "loss": 0.6456,
      "step": 1018200
    },
    {
      "epoch": 9.291736623111175,
      "grad_norm": 4.61053466796875,
      "learning_rate": 4.2256886147407355e-05,
      "loss": 0.6818,
      "step": 1018300
    },
    {
      "epoch": 9.29264909847434,
      "grad_norm": 4.546444416046143,
      "learning_rate": 4.2256125751271385e-05,
      "loss": 0.6927,
      "step": 1018400
    },
    {
      "epoch": 9.293561573837506,
      "grad_norm": 3.8891592025756836,
      "learning_rate": 4.2255365355135415e-05,
      "loss": 0.6872,
      "step": 1018500
    },
    {
      "epoch": 9.294474049200671,
      "grad_norm": 4.825060844421387,
      "learning_rate": 4.2254604958999445e-05,
      "loss": 0.6995,
      "step": 1018600
    },
    {
      "epoch": 9.295386524563837,
      "grad_norm": 3.5443131923675537,
      "learning_rate": 4.225384456286347e-05,
      "loss": 0.6921,
      "step": 1018700
    },
    {
      "epoch": 9.296298999927002,
      "grad_norm": 3.7513833045959473,
      "learning_rate": 4.2253084166727505e-05,
      "loss": 0.6888,
      "step": 1018800
    },
    {
      "epoch": 9.297211475290167,
      "grad_norm": 3.952662467956543,
      "learning_rate": 4.225232377059153e-05,
      "loss": 0.6909,
      "step": 1018900
    },
    {
      "epoch": 9.298123950653332,
      "grad_norm": 4.286088943481445,
      "learning_rate": 4.225156337445556e-05,
      "loss": 0.6722,
      "step": 1019000
    },
    {
      "epoch": 9.299036426016498,
      "grad_norm": 3.9760892391204834,
      "learning_rate": 4.225080297831959e-05,
      "loss": 0.7006,
      "step": 1019100
    },
    {
      "epoch": 9.299948901379663,
      "grad_norm": 4.208019733428955,
      "learning_rate": 4.225004258218362e-05,
      "loss": 0.7083,
      "step": 1019200
    },
    {
      "epoch": 9.300861376742828,
      "grad_norm": 3.7811970710754395,
      "learning_rate": 4.224928218604764e-05,
      "loss": 0.6995,
      "step": 1019300
    },
    {
      "epoch": 9.301773852105994,
      "grad_norm": 4.674227714538574,
      "learning_rate": 4.224852178991168e-05,
      "loss": 0.6656,
      "step": 1019400
    },
    {
      "epoch": 9.302686327469159,
      "grad_norm": 4.22041130065918,
      "learning_rate": 4.22477613937757e-05,
      "loss": 0.6891,
      "step": 1019500
    },
    {
      "epoch": 9.303598802832324,
      "grad_norm": 4.474923133850098,
      "learning_rate": 4.224700099763973e-05,
      "loss": 0.7189,
      "step": 1019600
    },
    {
      "epoch": 9.30451127819549,
      "grad_norm": 4.010810852050781,
      "learning_rate": 4.224624060150376e-05,
      "loss": 0.6887,
      "step": 1019700
    },
    {
      "epoch": 9.305423753558653,
      "grad_norm": 4.436617851257324,
      "learning_rate": 4.2245480205367786e-05,
      "loss": 0.708,
      "step": 1019800
    },
    {
      "epoch": 9.306336228921818,
      "grad_norm": 4.429542541503906,
      "learning_rate": 4.224471980923182e-05,
      "loss": 0.6857,
      "step": 1019900
    },
    {
      "epoch": 9.307248704284984,
      "grad_norm": 3.8179898262023926,
      "learning_rate": 4.2243959413095846e-05,
      "loss": 0.6768,
      "step": 1020000
    },
    {
      "epoch": 9.308161179648149,
      "grad_norm": 3.9111714363098145,
      "learning_rate": 4.2243199016959876e-05,
      "loss": 0.7085,
      "step": 1020100
    },
    {
      "epoch": 9.309073655011314,
      "grad_norm": 4.15036678314209,
      "learning_rate": 4.2242438620823906e-05,
      "loss": 0.7078,
      "step": 1020200
    },
    {
      "epoch": 9.30998613037448,
      "grad_norm": 3.772925615310669,
      "learning_rate": 4.2241678224687936e-05,
      "loss": 0.7185,
      "step": 1020300
    },
    {
      "epoch": 9.310898605737645,
      "grad_norm": 4.021751403808594,
      "learning_rate": 4.224091782855196e-05,
      "loss": 0.6623,
      "step": 1020400
    },
    {
      "epoch": 9.31181108110081,
      "grad_norm": 1.6022757291793823,
      "learning_rate": 4.2240157432415996e-05,
      "loss": 0.7244,
      "step": 1020500
    },
    {
      "epoch": 9.312723556463975,
      "grad_norm": 3.8309433460235596,
      "learning_rate": 4.223939703628002e-05,
      "loss": 0.6819,
      "step": 1020600
    },
    {
      "epoch": 9.31363603182714,
      "grad_norm": 3.2187881469726562,
      "learning_rate": 4.223863664014405e-05,
      "loss": 0.6433,
      "step": 1020700
    },
    {
      "epoch": 9.314548507190306,
      "grad_norm": 4.343905925750732,
      "learning_rate": 4.223787624400808e-05,
      "loss": 0.6633,
      "step": 1020800
    },
    {
      "epoch": 9.315460982553471,
      "grad_norm": 2.7611773014068604,
      "learning_rate": 4.223711584787211e-05,
      "loss": 0.6921,
      "step": 1020900
    },
    {
      "epoch": 9.316373457916637,
      "grad_norm": 3.793440341949463,
      "learning_rate": 4.223635545173614e-05,
      "loss": 0.6561,
      "step": 1021000
    },
    {
      "epoch": 9.317285933279802,
      "grad_norm": 4.58931827545166,
      "learning_rate": 4.223559505560017e-05,
      "loss": 0.6809,
      "step": 1021100
    },
    {
      "epoch": 9.318198408642967,
      "grad_norm": 3.9630322456359863,
      "learning_rate": 4.223483465946419e-05,
      "loss": 0.6747,
      "step": 1021200
    },
    {
      "epoch": 9.319110884006133,
      "grad_norm": 4.758917808532715,
      "learning_rate": 4.223407426332823e-05,
      "loss": 0.6808,
      "step": 1021300
    },
    {
      "epoch": 9.320023359369298,
      "grad_norm": 3.2800397872924805,
      "learning_rate": 4.223331386719225e-05,
      "loss": 0.6832,
      "step": 1021400
    },
    {
      "epoch": 9.320935834732463,
      "grad_norm": 4.199540138244629,
      "learning_rate": 4.2232553471056283e-05,
      "loss": 0.6776,
      "step": 1021500
    },
    {
      "epoch": 9.321848310095627,
      "grad_norm": 3.8151707649230957,
      "learning_rate": 4.2231793074920313e-05,
      "loss": 0.6671,
      "step": 1021600
    },
    {
      "epoch": 9.322760785458792,
      "grad_norm": 3.8260858058929443,
      "learning_rate": 4.2231032678784344e-05,
      "loss": 0.7061,
      "step": 1021700
    },
    {
      "epoch": 9.323673260821957,
      "grad_norm": 3.62874698638916,
      "learning_rate": 4.223027228264837e-05,
      "loss": 0.6931,
      "step": 1021800
    },
    {
      "epoch": 9.324585736185123,
      "grad_norm": 4.170671463012695,
      "learning_rate": 4.2229511886512404e-05,
      "loss": 0.7058,
      "step": 1021900
    },
    {
      "epoch": 9.325498211548288,
      "grad_norm": 4.644150257110596,
      "learning_rate": 4.222875149037643e-05,
      "loss": 0.6988,
      "step": 1022000
    },
    {
      "epoch": 9.326410686911453,
      "grad_norm": 4.101475238800049,
      "learning_rate": 4.222799109424046e-05,
      "loss": 0.664,
      "step": 1022100
    },
    {
      "epoch": 9.327323162274618,
      "grad_norm": 4.035585880279541,
      "learning_rate": 4.222723069810449e-05,
      "loss": 0.6541,
      "step": 1022200
    },
    {
      "epoch": 9.328235637637784,
      "grad_norm": 4.317383289337158,
      "learning_rate": 4.222647030196852e-05,
      "loss": 0.6994,
      "step": 1022300
    },
    {
      "epoch": 9.329148113000949,
      "grad_norm": 2.189788818359375,
      "learning_rate": 4.222570990583255e-05,
      "loss": 0.705,
      "step": 1022400
    },
    {
      "epoch": 9.330060588364114,
      "grad_norm": 4.471169471740723,
      "learning_rate": 4.222494950969657e-05,
      "loss": 0.6862,
      "step": 1022500
    },
    {
      "epoch": 9.33097306372728,
      "grad_norm": 3.884019374847412,
      "learning_rate": 4.22241891135606e-05,
      "loss": 0.6907,
      "step": 1022600
    },
    {
      "epoch": 9.331885539090445,
      "grad_norm": 4.912066459655762,
      "learning_rate": 4.222342871742463e-05,
      "loss": 0.681,
      "step": 1022700
    },
    {
      "epoch": 9.33279801445361,
      "grad_norm": 4.144530296325684,
      "learning_rate": 4.222266832128866e-05,
      "loss": 0.704,
      "step": 1022800
    },
    {
      "epoch": 9.333710489816776,
      "grad_norm": 4.401227951049805,
      "learning_rate": 4.2221907925152684e-05,
      "loss": 0.65,
      "step": 1022900
    },
    {
      "epoch": 9.33462296517994,
      "grad_norm": 3.0993239879608154,
      "learning_rate": 4.222114752901672e-05,
      "loss": 0.7153,
      "step": 1023000
    },
    {
      "epoch": 9.335535440543106,
      "grad_norm": 3.876152515411377,
      "learning_rate": 4.2220387132880744e-05,
      "loss": 0.745,
      "step": 1023100
    },
    {
      "epoch": 9.33644791590627,
      "grad_norm": 4.030261993408203,
      "learning_rate": 4.2219626736744774e-05,
      "loss": 0.7046,
      "step": 1023200
    },
    {
      "epoch": 9.337360391269435,
      "grad_norm": 4.547581672668457,
      "learning_rate": 4.2218866340608804e-05,
      "loss": 0.6727,
      "step": 1023300
    },
    {
      "epoch": 9.3382728666326,
      "grad_norm": 3.5449070930480957,
      "learning_rate": 4.2218105944472834e-05,
      "loss": 0.711,
      "step": 1023400
    },
    {
      "epoch": 9.339185341995766,
      "grad_norm": 3.7230823040008545,
      "learning_rate": 4.2217345548336864e-05,
      "loss": 0.6717,
      "step": 1023500
    },
    {
      "epoch": 9.34009781735893,
      "grad_norm": 3.7901601791381836,
      "learning_rate": 4.2216585152200895e-05,
      "loss": 0.696,
      "step": 1023600
    },
    {
      "epoch": 9.341010292722096,
      "grad_norm": 3.189918279647827,
      "learning_rate": 4.221582475606492e-05,
      "loss": 0.7038,
      "step": 1023700
    },
    {
      "epoch": 9.341922768085261,
      "grad_norm": 3.8802945613861084,
      "learning_rate": 4.2215064359928955e-05,
      "loss": 0.6498,
      "step": 1023800
    },
    {
      "epoch": 9.342835243448427,
      "grad_norm": 4.451180934906006,
      "learning_rate": 4.221430396379298e-05,
      "loss": 0.6603,
      "step": 1023900
    },
    {
      "epoch": 9.343747718811592,
      "grad_norm": 3.2270004749298096,
      "learning_rate": 4.221354356765701e-05,
      "loss": 0.674,
      "step": 1024000
    },
    {
      "epoch": 9.344660194174757,
      "grad_norm": 4.3284010887146,
      "learning_rate": 4.221278317152104e-05,
      "loss": 0.6838,
      "step": 1024100
    },
    {
      "epoch": 9.345572669537923,
      "grad_norm": 3.5581772327423096,
      "learning_rate": 4.221202277538507e-05,
      "loss": 0.7421,
      "step": 1024200
    },
    {
      "epoch": 9.346485144901088,
      "grad_norm": 4.1267499923706055,
      "learning_rate": 4.221126237924909e-05,
      "loss": 0.6551,
      "step": 1024300
    },
    {
      "epoch": 9.347397620264253,
      "grad_norm": 2.890957832336426,
      "learning_rate": 4.221050198311313e-05,
      "loss": 0.7348,
      "step": 1024400
    },
    {
      "epoch": 9.348310095627419,
      "grad_norm": 4.641096591949463,
      "learning_rate": 4.220974158697715e-05,
      "loss": 0.7141,
      "step": 1024500
    },
    {
      "epoch": 9.349222570990584,
      "grad_norm": 4.120054721832275,
      "learning_rate": 4.220898119084118e-05,
      "loss": 0.7042,
      "step": 1024600
    },
    {
      "epoch": 9.35013504635375,
      "grad_norm": 4.058304309844971,
      "learning_rate": 4.220822079470521e-05,
      "loss": 0.661,
      "step": 1024700
    },
    {
      "epoch": 9.351047521716914,
      "grad_norm": 3.3362340927124023,
      "learning_rate": 4.220746039856924e-05,
      "loss": 0.6515,
      "step": 1024800
    },
    {
      "epoch": 9.35195999708008,
      "grad_norm": 4.1841020584106445,
      "learning_rate": 4.220670000243327e-05,
      "loss": 0.6884,
      "step": 1024900
    },
    {
      "epoch": 9.352872472443243,
      "grad_norm": 3.045286178588867,
      "learning_rate": 4.22059396062973e-05,
      "loss": 0.6843,
      "step": 1025000
    },
    {
      "epoch": 9.353784947806409,
      "grad_norm": 4.516927242279053,
      "learning_rate": 4.2205179210161325e-05,
      "loss": 0.6692,
      "step": 1025100
    },
    {
      "epoch": 9.354697423169574,
      "grad_norm": 3.623349666595459,
      "learning_rate": 4.220441881402536e-05,
      "loss": 0.6926,
      "step": 1025200
    },
    {
      "epoch": 9.35560989853274,
      "grad_norm": 4.340774059295654,
      "learning_rate": 4.2203658417889385e-05,
      "loss": 0.6815,
      "step": 1025300
    },
    {
      "epoch": 9.356522373895904,
      "grad_norm": 3.7029523849487305,
      "learning_rate": 4.2202898021753415e-05,
      "loss": 0.7011,
      "step": 1025400
    },
    {
      "epoch": 9.35743484925907,
      "grad_norm": 4.562255382537842,
      "learning_rate": 4.2202137625617445e-05,
      "loss": 0.6786,
      "step": 1025500
    },
    {
      "epoch": 9.358347324622235,
      "grad_norm": 4.227823257446289,
      "learning_rate": 4.220137722948147e-05,
      "loss": 0.6734,
      "step": 1025600
    },
    {
      "epoch": 9.3592597999854,
      "grad_norm": 3.775744676589966,
      "learning_rate": 4.22006168333455e-05,
      "loss": 0.722,
      "step": 1025700
    },
    {
      "epoch": 9.360172275348566,
      "grad_norm": 4.218776226043701,
      "learning_rate": 4.219985643720953e-05,
      "loss": 0.6839,
      "step": 1025800
    },
    {
      "epoch": 9.361084750711731,
      "grad_norm": 4.53493595123291,
      "learning_rate": 4.219909604107356e-05,
      "loss": 0.7132,
      "step": 1025900
    },
    {
      "epoch": 9.361997226074896,
      "grad_norm": 3.9366002082824707,
      "learning_rate": 4.219833564493759e-05,
      "loss": 0.6819,
      "step": 1026000
    },
    {
      "epoch": 9.362909701438062,
      "grad_norm": 3.505176305770874,
      "learning_rate": 4.219757524880162e-05,
      "loss": 0.6775,
      "step": 1026100
    },
    {
      "epoch": 9.363822176801227,
      "grad_norm": 4.581604957580566,
      "learning_rate": 4.219681485266564e-05,
      "loss": 0.6779,
      "step": 1026200
    },
    {
      "epoch": 9.364734652164392,
      "grad_norm": 3.737510919570923,
      "learning_rate": 4.219605445652968e-05,
      "loss": 0.6864,
      "step": 1026300
    },
    {
      "epoch": 9.365647127527557,
      "grad_norm": 3.1642637252807617,
      "learning_rate": 4.21952940603937e-05,
      "loss": 0.6776,
      "step": 1026400
    },
    {
      "epoch": 9.366559602890723,
      "grad_norm": 4.4910359382629395,
      "learning_rate": 4.219453366425773e-05,
      "loss": 0.6879,
      "step": 1026500
    },
    {
      "epoch": 9.367472078253886,
      "grad_norm": 3.2121098041534424,
      "learning_rate": 4.219377326812176e-05,
      "loss": 0.7223,
      "step": 1026600
    },
    {
      "epoch": 9.368384553617052,
      "grad_norm": 3.8942413330078125,
      "learning_rate": 4.219301287198579e-05,
      "loss": 0.6805,
      "step": 1026700
    },
    {
      "epoch": 9.369297028980217,
      "grad_norm": 3.969207286834717,
      "learning_rate": 4.219225247584982e-05,
      "loss": 0.6882,
      "step": 1026800
    },
    {
      "epoch": 9.370209504343382,
      "grad_norm": 3.6383981704711914,
      "learning_rate": 4.219149207971385e-05,
      "loss": 0.678,
      "step": 1026900
    },
    {
      "epoch": 9.371121979706547,
      "grad_norm": 3.969862937927246,
      "learning_rate": 4.2190731683577876e-05,
      "loss": 0.656,
      "step": 1027000
    },
    {
      "epoch": 9.372034455069713,
      "grad_norm": 3.0815067291259766,
      "learning_rate": 4.2189971287441906e-05,
      "loss": 0.7051,
      "step": 1027100
    },
    {
      "epoch": 9.372946930432878,
      "grad_norm": 4.727599143981934,
      "learning_rate": 4.2189210891305936e-05,
      "loss": 0.6994,
      "step": 1027200
    },
    {
      "epoch": 9.373859405796043,
      "grad_norm": 3.633721113204956,
      "learning_rate": 4.2188450495169966e-05,
      "loss": 0.6947,
      "step": 1027300
    },
    {
      "epoch": 9.374771881159209,
      "grad_norm": 5.670085430145264,
      "learning_rate": 4.2187690099033996e-05,
      "loss": 0.672,
      "step": 1027400
    },
    {
      "epoch": 9.375684356522374,
      "grad_norm": 4.109684467315674,
      "learning_rate": 4.2186929702898026e-05,
      "loss": 0.7012,
      "step": 1027500
    },
    {
      "epoch": 9.37659683188554,
      "grad_norm": 4.187488555908203,
      "learning_rate": 4.218616930676205e-05,
      "loss": 0.7111,
      "step": 1027600
    },
    {
      "epoch": 9.377509307248705,
      "grad_norm": 4.396571636199951,
      "learning_rate": 4.218540891062609e-05,
      "loss": 0.7067,
      "step": 1027700
    },
    {
      "epoch": 9.37842178261187,
      "grad_norm": 4.36080265045166,
      "learning_rate": 4.218464851449011e-05,
      "loss": 0.664,
      "step": 1027800
    },
    {
      "epoch": 9.379334257975035,
      "grad_norm": 3.936532735824585,
      "learning_rate": 4.218388811835414e-05,
      "loss": 0.668,
      "step": 1027900
    },
    {
      "epoch": 9.3802467333382,
      "grad_norm": 3.9691689014434814,
      "learning_rate": 4.218312772221817e-05,
      "loss": 0.6864,
      "step": 1028000
    },
    {
      "epoch": 9.381159208701366,
      "grad_norm": 3.563363790512085,
      "learning_rate": 4.21823673260822e-05,
      "loss": 0.6853,
      "step": 1028100
    },
    {
      "epoch": 9.382071684064531,
      "grad_norm": 3.337952136993408,
      "learning_rate": 4.218160692994623e-05,
      "loss": 0.6751,
      "step": 1028200
    },
    {
      "epoch": 9.382984159427695,
      "grad_norm": 3.7961535453796387,
      "learning_rate": 4.2180846533810253e-05,
      "loss": 0.7042,
      "step": 1028300
    },
    {
      "epoch": 9.38389663479086,
      "grad_norm": 3.434385299682617,
      "learning_rate": 4.2180086137674284e-05,
      "loss": 0.7172,
      "step": 1028400
    },
    {
      "epoch": 9.384809110154025,
      "grad_norm": 2.9598658084869385,
      "learning_rate": 4.2179325741538314e-05,
      "loss": 0.6887,
      "step": 1028500
    },
    {
      "epoch": 9.38572158551719,
      "grad_norm": 3.767016649246216,
      "learning_rate": 4.2178565345402344e-05,
      "loss": 0.6579,
      "step": 1028600
    },
    {
      "epoch": 9.386634060880356,
      "grad_norm": 4.5087127685546875,
      "learning_rate": 4.217780494926637e-05,
      "loss": 0.6848,
      "step": 1028700
    },
    {
      "epoch": 9.387546536243521,
      "grad_norm": 4.655892372131348,
      "learning_rate": 4.2177044553130404e-05,
      "loss": 0.6778,
      "step": 1028800
    },
    {
      "epoch": 9.388459011606686,
      "grad_norm": 4.468744277954102,
      "learning_rate": 4.217628415699443e-05,
      "loss": 0.6812,
      "step": 1028900
    },
    {
      "epoch": 9.389371486969852,
      "grad_norm": 3.947286605834961,
      "learning_rate": 4.217552376085846e-05,
      "loss": 0.6779,
      "step": 1029000
    },
    {
      "epoch": 9.390283962333017,
      "grad_norm": 5.115045070648193,
      "learning_rate": 4.217476336472249e-05,
      "loss": 0.7053,
      "step": 1029100
    },
    {
      "epoch": 9.391196437696182,
      "grad_norm": 5.429532527923584,
      "learning_rate": 4.217400296858652e-05,
      "loss": 0.693,
      "step": 1029200
    },
    {
      "epoch": 9.392108913059348,
      "grad_norm": 3.999176502227783,
      "learning_rate": 4.217324257245055e-05,
      "loss": 0.7067,
      "step": 1029300
    },
    {
      "epoch": 9.393021388422513,
      "grad_norm": 3.5333006381988525,
      "learning_rate": 4.217248217631458e-05,
      "loss": 0.6554,
      "step": 1029400
    },
    {
      "epoch": 9.393933863785678,
      "grad_norm": 4.387144088745117,
      "learning_rate": 4.21717217801786e-05,
      "loss": 0.6912,
      "step": 1029500
    },
    {
      "epoch": 9.394846339148843,
      "grad_norm": 4.144079685211182,
      "learning_rate": 4.217096138404264e-05,
      "loss": 0.7165,
      "step": 1029600
    },
    {
      "epoch": 9.395758814512009,
      "grad_norm": 4.352203845977783,
      "learning_rate": 4.217020098790666e-05,
      "loss": 0.71,
      "step": 1029700
    },
    {
      "epoch": 9.396671289875174,
      "grad_norm": 4.380428314208984,
      "learning_rate": 4.216944059177069e-05,
      "loss": 0.6347,
      "step": 1029800
    },
    {
      "epoch": 9.39758376523834,
      "grad_norm": 5.298384189605713,
      "learning_rate": 4.216868019563472e-05,
      "loss": 0.686,
      "step": 1029900
    },
    {
      "epoch": 9.398496240601503,
      "grad_norm": 3.334346055984497,
      "learning_rate": 4.216791979949875e-05,
      "loss": 0.6631,
      "step": 1030000
    },
    {
      "epoch": 9.399408715964668,
      "grad_norm": 3.9900102615356445,
      "learning_rate": 4.2167159403362774e-05,
      "loss": 0.7124,
      "step": 1030100
    },
    {
      "epoch": 9.400321191327834,
      "grad_norm": 4.688041687011719,
      "learning_rate": 4.216639900722681e-05,
      "loss": 0.7027,
      "step": 1030200
    },
    {
      "epoch": 9.401233666690999,
      "grad_norm": 3.9362540245056152,
      "learning_rate": 4.2165638611090834e-05,
      "loss": 0.6971,
      "step": 1030300
    },
    {
      "epoch": 9.402146142054164,
      "grad_norm": 3.387500047683716,
      "learning_rate": 4.2164878214954865e-05,
      "loss": 0.6748,
      "step": 1030400
    },
    {
      "epoch": 9.40305861741733,
      "grad_norm": 3.3872947692871094,
      "learning_rate": 4.2164117818818895e-05,
      "loss": 0.7142,
      "step": 1030500
    },
    {
      "epoch": 9.403971092780495,
      "grad_norm": 4.196599006652832,
      "learning_rate": 4.2163357422682925e-05,
      "loss": 0.652,
      "step": 1030600
    },
    {
      "epoch": 9.40488356814366,
      "grad_norm": 3.8548102378845215,
      "learning_rate": 4.2162597026546955e-05,
      "loss": 0.6756,
      "step": 1030700
    },
    {
      "epoch": 9.405796043506825,
      "grad_norm": 3.7686855792999268,
      "learning_rate": 4.2161836630410985e-05,
      "loss": 0.7112,
      "step": 1030800
    },
    {
      "epoch": 9.40670851886999,
      "grad_norm": 4.275586128234863,
      "learning_rate": 4.216107623427501e-05,
      "loss": 0.6381,
      "step": 1030900
    },
    {
      "epoch": 9.407620994233156,
      "grad_norm": 4.166195392608643,
      "learning_rate": 4.216031583813904e-05,
      "loss": 0.7134,
      "step": 1031000
    },
    {
      "epoch": 9.408533469596321,
      "grad_norm": 3.7396562099456787,
      "learning_rate": 4.215955544200307e-05,
      "loss": 0.7099,
      "step": 1031100
    },
    {
      "epoch": 9.409445944959486,
      "grad_norm": 3.7397916316986084,
      "learning_rate": 4.215879504586709e-05,
      "loss": 0.6622,
      "step": 1031200
    },
    {
      "epoch": 9.410358420322652,
      "grad_norm": 4.1400465965271,
      "learning_rate": 4.215803464973113e-05,
      "loss": 0.6799,
      "step": 1031300
    },
    {
      "epoch": 9.411270895685817,
      "grad_norm": 4.387306213378906,
      "learning_rate": 4.215727425359515e-05,
      "loss": 0.6862,
      "step": 1031400
    },
    {
      "epoch": 9.412183371048982,
      "grad_norm": 2.917870283126831,
      "learning_rate": 4.215651385745918e-05,
      "loss": 0.6977,
      "step": 1031500
    },
    {
      "epoch": 9.413095846412148,
      "grad_norm": 4.672044277191162,
      "learning_rate": 4.215575346132321e-05,
      "loss": 0.7276,
      "step": 1031600
    },
    {
      "epoch": 9.414008321775311,
      "grad_norm": 3.342397689819336,
      "learning_rate": 4.215499306518724e-05,
      "loss": 0.6886,
      "step": 1031700
    },
    {
      "epoch": 9.414920797138477,
      "grad_norm": 3.654146432876587,
      "learning_rate": 4.215423266905127e-05,
      "loss": 0.7382,
      "step": 1031800
    },
    {
      "epoch": 9.415833272501642,
      "grad_norm": 4.2851057052612305,
      "learning_rate": 4.21534722729153e-05,
      "loss": 0.7302,
      "step": 1031900
    },
    {
      "epoch": 9.416745747864807,
      "grad_norm": 3.5406501293182373,
      "learning_rate": 4.2152711876779325e-05,
      "loss": 0.7203,
      "step": 1032000
    },
    {
      "epoch": 9.417658223227972,
      "grad_norm": 3.538540840148926,
      "learning_rate": 4.215195148064336e-05,
      "loss": 0.6657,
      "step": 1032100
    },
    {
      "epoch": 9.418570698591138,
      "grad_norm": 4.5822672843933105,
      "learning_rate": 4.2151191084507385e-05,
      "loss": 0.6771,
      "step": 1032200
    },
    {
      "epoch": 9.419483173954303,
      "grad_norm": 3.3681259155273438,
      "learning_rate": 4.2150430688371415e-05,
      "loss": 0.6735,
      "step": 1032300
    },
    {
      "epoch": 9.420395649317468,
      "grad_norm": 4.465587139129639,
      "learning_rate": 4.2149670292235446e-05,
      "loss": 0.7025,
      "step": 1032400
    },
    {
      "epoch": 9.421308124680634,
      "grad_norm": 3.9485673904418945,
      "learning_rate": 4.2148909896099476e-05,
      "loss": 0.6778,
      "step": 1032500
    },
    {
      "epoch": 9.422220600043799,
      "grad_norm": 4.185150623321533,
      "learning_rate": 4.21481494999635e-05,
      "loss": 0.6952,
      "step": 1032600
    },
    {
      "epoch": 9.423133075406964,
      "grad_norm": 3.4424149990081787,
      "learning_rate": 4.2147389103827536e-05,
      "loss": 0.659,
      "step": 1032700
    },
    {
      "epoch": 9.42404555077013,
      "grad_norm": 4.3244709968566895,
      "learning_rate": 4.214662870769156e-05,
      "loss": 0.703,
      "step": 1032800
    },
    {
      "epoch": 9.424958026133295,
      "grad_norm": 3.868161916732788,
      "learning_rate": 4.214586831155559e-05,
      "loss": 0.6987,
      "step": 1032900
    },
    {
      "epoch": 9.42587050149646,
      "grad_norm": 4.465963840484619,
      "learning_rate": 4.214510791541962e-05,
      "loss": 0.6754,
      "step": 1033000
    },
    {
      "epoch": 9.426782976859625,
      "grad_norm": 4.142523288726807,
      "learning_rate": 4.214434751928365e-05,
      "loss": 0.6757,
      "step": 1033100
    },
    {
      "epoch": 9.42769545222279,
      "grad_norm": 4.112814903259277,
      "learning_rate": 4.214358712314768e-05,
      "loss": 0.6993,
      "step": 1033200
    },
    {
      "epoch": 9.428607927585956,
      "grad_norm": 3.32192325592041,
      "learning_rate": 4.214282672701171e-05,
      "loss": 0.7077,
      "step": 1033300
    },
    {
      "epoch": 9.42952040294912,
      "grad_norm": 4.048878192901611,
      "learning_rate": 4.214206633087573e-05,
      "loss": 0.7109,
      "step": 1033400
    },
    {
      "epoch": 9.430432878312285,
      "grad_norm": 3.98360538482666,
      "learning_rate": 4.214130593473977e-05,
      "loss": 0.6685,
      "step": 1033500
    },
    {
      "epoch": 9.43134535367545,
      "grad_norm": 3.5738611221313477,
      "learning_rate": 4.214054553860379e-05,
      "loss": 0.6631,
      "step": 1033600
    },
    {
      "epoch": 9.432257829038615,
      "grad_norm": 4.442104816436768,
      "learning_rate": 4.213978514246782e-05,
      "loss": 0.6797,
      "step": 1033700
    },
    {
      "epoch": 9.43317030440178,
      "grad_norm": 3.6942780017852783,
      "learning_rate": 4.213902474633185e-05,
      "loss": 0.7051,
      "step": 1033800
    },
    {
      "epoch": 9.434082779764946,
      "grad_norm": 3.77108097076416,
      "learning_rate": 4.2138264350195876e-05,
      "loss": 0.6818,
      "step": 1033900
    },
    {
      "epoch": 9.434995255128111,
      "grad_norm": 3.515017509460449,
      "learning_rate": 4.2137503954059906e-05,
      "loss": 0.6999,
      "step": 1034000
    },
    {
      "epoch": 9.435907730491277,
      "grad_norm": 3.8200902938842773,
      "learning_rate": 4.2136743557923936e-05,
      "loss": 0.6878,
      "step": 1034100
    },
    {
      "epoch": 9.436820205854442,
      "grad_norm": 5.337068557739258,
      "learning_rate": 4.2135983161787966e-05,
      "loss": 0.698,
      "step": 1034200
    },
    {
      "epoch": 9.437732681217607,
      "grad_norm": 3.662679433822632,
      "learning_rate": 4.2135222765651997e-05,
      "loss": 0.6915,
      "step": 1034300
    },
    {
      "epoch": 9.438645156580773,
      "grad_norm": 4.556603908538818,
      "learning_rate": 4.2134462369516027e-05,
      "loss": 0.6658,
      "step": 1034400
    },
    {
      "epoch": 9.439557631943938,
      "grad_norm": 4.178386211395264,
      "learning_rate": 4.213370197338005e-05,
      "loss": 0.671,
      "step": 1034500
    },
    {
      "epoch": 9.440470107307103,
      "grad_norm": 3.7823872566223145,
      "learning_rate": 4.213294157724409e-05,
      "loss": 0.7022,
      "step": 1034600
    },
    {
      "epoch": 9.441382582670268,
      "grad_norm": 3.698256254196167,
      "learning_rate": 4.213218118110811e-05,
      "loss": 0.699,
      "step": 1034700
    },
    {
      "epoch": 9.442295058033434,
      "grad_norm": 3.384831666946411,
      "learning_rate": 4.213142078497214e-05,
      "loss": 0.7006,
      "step": 1034800
    },
    {
      "epoch": 9.443207533396599,
      "grad_norm": 4.678988456726074,
      "learning_rate": 4.213066038883617e-05,
      "loss": 0.6883,
      "step": 1034900
    },
    {
      "epoch": 9.444120008759764,
      "grad_norm": 4.905159950256348,
      "learning_rate": 4.21298999927002e-05,
      "loss": 0.6498,
      "step": 1035000
    },
    {
      "epoch": 9.445032484122928,
      "grad_norm": 4.5042619705200195,
      "learning_rate": 4.2129139596564223e-05,
      "loss": 0.6774,
      "step": 1035100
    },
    {
      "epoch": 9.445944959486093,
      "grad_norm": 3.8539535999298096,
      "learning_rate": 4.212837920042826e-05,
      "loss": 0.6703,
      "step": 1035200
    },
    {
      "epoch": 9.446857434849258,
      "grad_norm": 3.6338253021240234,
      "learning_rate": 4.2127618804292284e-05,
      "loss": 0.69,
      "step": 1035300
    },
    {
      "epoch": 9.447769910212424,
      "grad_norm": 3.355351448059082,
      "learning_rate": 4.2126858408156314e-05,
      "loss": 0.6602,
      "step": 1035400
    },
    {
      "epoch": 9.448682385575589,
      "grad_norm": 4.224224090576172,
      "learning_rate": 4.2126098012020344e-05,
      "loss": 0.7233,
      "step": 1035500
    },
    {
      "epoch": 9.449594860938754,
      "grad_norm": 2.8754894733428955,
      "learning_rate": 4.2125337615884374e-05,
      "loss": 0.6707,
      "step": 1035600
    },
    {
      "epoch": 9.45050733630192,
      "grad_norm": 4.261956214904785,
      "learning_rate": 4.2124577219748404e-05,
      "loss": 0.7036,
      "step": 1035700
    },
    {
      "epoch": 9.451419811665085,
      "grad_norm": 3.678805351257324,
      "learning_rate": 4.2123816823612434e-05,
      "loss": 0.6802,
      "step": 1035800
    },
    {
      "epoch": 9.45233228702825,
      "grad_norm": 3.9402546882629395,
      "learning_rate": 4.212305642747646e-05,
      "loss": 0.6732,
      "step": 1035900
    },
    {
      "epoch": 9.453244762391416,
      "grad_norm": 4.613270282745361,
      "learning_rate": 4.2122296031340494e-05,
      "loss": 0.6584,
      "step": 1036000
    },
    {
      "epoch": 9.45415723775458,
      "grad_norm": 4.25130558013916,
      "learning_rate": 4.212153563520452e-05,
      "loss": 0.671,
      "step": 1036100
    },
    {
      "epoch": 9.455069713117746,
      "grad_norm": 3.9758834838867188,
      "learning_rate": 4.212077523906855e-05,
      "loss": 0.6864,
      "step": 1036200
    },
    {
      "epoch": 9.455982188480911,
      "grad_norm": 3.9804608821868896,
      "learning_rate": 4.212001484293258e-05,
      "loss": 0.6892,
      "step": 1036300
    },
    {
      "epoch": 9.456894663844077,
      "grad_norm": 4.855099201202393,
      "learning_rate": 4.211925444679661e-05,
      "loss": 0.6933,
      "step": 1036400
    },
    {
      "epoch": 9.457807139207242,
      "grad_norm": 1.7767120599746704,
      "learning_rate": 4.211849405066063e-05,
      "loss": 0.67,
      "step": 1036500
    },
    {
      "epoch": 9.458719614570407,
      "grad_norm": 3.8884294033050537,
      "learning_rate": 4.211773365452467e-05,
      "loss": 0.6938,
      "step": 1036600
    },
    {
      "epoch": 9.459632089933573,
      "grad_norm": 3.6226065158843994,
      "learning_rate": 4.211697325838869e-05,
      "loss": 0.6866,
      "step": 1036700
    },
    {
      "epoch": 9.460544565296736,
      "grad_norm": 4.411003112792969,
      "learning_rate": 4.211621286225272e-05,
      "loss": 0.6853,
      "step": 1036800
    },
    {
      "epoch": 9.461457040659901,
      "grad_norm": 3.52571964263916,
      "learning_rate": 4.211545246611675e-05,
      "loss": 0.6902,
      "step": 1036900
    },
    {
      "epoch": 9.462369516023067,
      "grad_norm": 3.4550225734710693,
      "learning_rate": 4.2114692069980774e-05,
      "loss": 0.6863,
      "step": 1037000
    },
    {
      "epoch": 9.463281991386232,
      "grad_norm": 4.23763370513916,
      "learning_rate": 4.211393167384481e-05,
      "loss": 0.6667,
      "step": 1037100
    },
    {
      "epoch": 9.464194466749397,
      "grad_norm": 4.281533718109131,
      "learning_rate": 4.2113171277708835e-05,
      "loss": 0.7041,
      "step": 1037200
    },
    {
      "epoch": 9.465106942112563,
      "grad_norm": 4.536357402801514,
      "learning_rate": 4.2112410881572865e-05,
      "loss": 0.7498,
      "step": 1037300
    },
    {
      "epoch": 9.466019417475728,
      "grad_norm": 5.004851818084717,
      "learning_rate": 4.2111650485436895e-05,
      "loss": 0.6842,
      "step": 1037400
    },
    {
      "epoch": 9.466931892838893,
      "grad_norm": 3.698444366455078,
      "learning_rate": 4.2110890089300925e-05,
      "loss": 0.6768,
      "step": 1037500
    },
    {
      "epoch": 9.467844368202059,
      "grad_norm": 4.564634323120117,
      "learning_rate": 4.211012969316495e-05,
      "loss": 0.6976,
      "step": 1037600
    },
    {
      "epoch": 9.468756843565224,
      "grad_norm": 3.567201614379883,
      "learning_rate": 4.2109369297028985e-05,
      "loss": 0.6463,
      "step": 1037700
    },
    {
      "epoch": 9.46966931892839,
      "grad_norm": 4.494640827178955,
      "learning_rate": 4.210860890089301e-05,
      "loss": 0.6866,
      "step": 1037800
    },
    {
      "epoch": 9.470581794291554,
      "grad_norm": 3.211843729019165,
      "learning_rate": 4.210784850475704e-05,
      "loss": 0.6411,
      "step": 1037900
    },
    {
      "epoch": 9.47149426965472,
      "grad_norm": 4.310295581817627,
      "learning_rate": 4.210708810862107e-05,
      "loss": 0.692,
      "step": 1038000
    },
    {
      "epoch": 9.472406745017885,
      "grad_norm": 4.61641263961792,
      "learning_rate": 4.21063277124851e-05,
      "loss": 0.6789,
      "step": 1038100
    },
    {
      "epoch": 9.47331922038105,
      "grad_norm": 4.029472351074219,
      "learning_rate": 4.210556731634913e-05,
      "loss": 0.69,
      "step": 1038200
    },
    {
      "epoch": 9.474231695744216,
      "grad_norm": 4.19501256942749,
      "learning_rate": 4.210480692021316e-05,
      "loss": 0.6687,
      "step": 1038300
    },
    {
      "epoch": 9.475144171107381,
      "grad_norm": 4.1268720626831055,
      "learning_rate": 4.210404652407718e-05,
      "loss": 0.7034,
      "step": 1038400
    },
    {
      "epoch": 9.476056646470544,
      "grad_norm": 4.213670253753662,
      "learning_rate": 4.210328612794122e-05,
      "loss": 0.7072,
      "step": 1038500
    },
    {
      "epoch": 9.47696912183371,
      "grad_norm": 4.534756660461426,
      "learning_rate": 4.210252573180524e-05,
      "loss": 0.6943,
      "step": 1038600
    },
    {
      "epoch": 9.477881597196875,
      "grad_norm": 3.8916821479797363,
      "learning_rate": 4.210176533566927e-05,
      "loss": 0.7583,
      "step": 1038700
    },
    {
      "epoch": 9.47879407256004,
      "grad_norm": 3.6414594650268555,
      "learning_rate": 4.21010049395333e-05,
      "loss": 0.6654,
      "step": 1038800
    },
    {
      "epoch": 9.479706547923206,
      "grad_norm": 3.777022123336792,
      "learning_rate": 4.210024454339733e-05,
      "loss": 0.6971,
      "step": 1038900
    },
    {
      "epoch": 9.480619023286371,
      "grad_norm": 3.8468356132507324,
      "learning_rate": 4.2099484147261355e-05,
      "loss": 0.7489,
      "step": 1039000
    },
    {
      "epoch": 9.481531498649536,
      "grad_norm": 4.0302557945251465,
      "learning_rate": 4.209872375112539e-05,
      "loss": 0.6913,
      "step": 1039100
    },
    {
      "epoch": 9.482443974012702,
      "grad_norm": 4.298581123352051,
      "learning_rate": 4.2097963354989416e-05,
      "loss": 0.6966,
      "step": 1039200
    },
    {
      "epoch": 9.483356449375867,
      "grad_norm": 4.752712726593018,
      "learning_rate": 4.2097202958853446e-05,
      "loss": 0.6718,
      "step": 1039300
    },
    {
      "epoch": 9.484268924739032,
      "grad_norm": 3.8899855613708496,
      "learning_rate": 4.2096442562717476e-05,
      "loss": 0.7096,
      "step": 1039400
    },
    {
      "epoch": 9.485181400102197,
      "grad_norm": 3.785313606262207,
      "learning_rate": 4.20956821665815e-05,
      "loss": 0.6622,
      "step": 1039500
    },
    {
      "epoch": 9.486093875465363,
      "grad_norm": 4.2725043296813965,
      "learning_rate": 4.2094921770445536e-05,
      "loss": 0.6799,
      "step": 1039600
    },
    {
      "epoch": 9.487006350828528,
      "grad_norm": 4.050200939178467,
      "learning_rate": 4.209416137430956e-05,
      "loss": 0.6879,
      "step": 1039700
    },
    {
      "epoch": 9.487918826191693,
      "grad_norm": 3.804880142211914,
      "learning_rate": 4.209340097817359e-05,
      "loss": 0.6732,
      "step": 1039800
    },
    {
      "epoch": 9.488831301554859,
      "grad_norm": 4.241050720214844,
      "learning_rate": 4.209264058203762e-05,
      "loss": 0.6805,
      "step": 1039900
    },
    {
      "epoch": 9.489743776918024,
      "grad_norm": 3.9039697647094727,
      "learning_rate": 4.209188018590165e-05,
      "loss": 0.6587,
      "step": 1040000
    },
    {
      "epoch": 9.49065625228119,
      "grad_norm": 4.264087200164795,
      "learning_rate": 4.209111978976568e-05,
      "loss": 0.6926,
      "step": 1040100
    },
    {
      "epoch": 9.491568727644353,
      "grad_norm": 4.023120880126953,
      "learning_rate": 4.209035939362971e-05,
      "loss": 0.6635,
      "step": 1040200
    },
    {
      "epoch": 9.492481203007518,
      "grad_norm": 5.010732650756836,
      "learning_rate": 4.208959899749373e-05,
      "loss": 0.6695,
      "step": 1040300
    },
    {
      "epoch": 9.493393678370683,
      "grad_norm": 3.896918535232544,
      "learning_rate": 4.208883860135777e-05,
      "loss": 0.6768,
      "step": 1040400
    },
    {
      "epoch": 9.494306153733849,
      "grad_norm": 3.8482110500335693,
      "learning_rate": 4.208807820522179e-05,
      "loss": 0.6808,
      "step": 1040500
    },
    {
      "epoch": 9.495218629097014,
      "grad_norm": 4.969793796539307,
      "learning_rate": 4.208731780908582e-05,
      "loss": 0.6846,
      "step": 1040600
    },
    {
      "epoch": 9.49613110446018,
      "grad_norm": 3.9417967796325684,
      "learning_rate": 4.208655741294985e-05,
      "loss": 0.6656,
      "step": 1040700
    },
    {
      "epoch": 9.497043579823345,
      "grad_norm": 4.068247318267822,
      "learning_rate": 4.208579701681388e-05,
      "loss": 0.679,
      "step": 1040800
    },
    {
      "epoch": 9.49795605518651,
      "grad_norm": 4.062982559204102,
      "learning_rate": 4.2085036620677906e-05,
      "loss": 0.6969,
      "step": 1040900
    },
    {
      "epoch": 9.498868530549675,
      "grad_norm": 3.024477481842041,
      "learning_rate": 4.208427622454194e-05,
      "loss": 0.6786,
      "step": 1041000
    },
    {
      "epoch": 9.49978100591284,
      "grad_norm": 4.142333507537842,
      "learning_rate": 4.2083515828405967e-05,
      "loss": 0.7044,
      "step": 1041100
    },
    {
      "epoch": 9.500693481276006,
      "grad_norm": 3.750549793243408,
      "learning_rate": 4.2082755432269997e-05,
      "loss": 0.7023,
      "step": 1041200
    },
    {
      "epoch": 9.501605956639171,
      "grad_norm": 3.6089847087860107,
      "learning_rate": 4.208199503613403e-05,
      "loss": 0.7263,
      "step": 1041300
    },
    {
      "epoch": 9.502518432002336,
      "grad_norm": 3.7693068981170654,
      "learning_rate": 4.208123463999806e-05,
      "loss": 0.6903,
      "step": 1041400
    },
    {
      "epoch": 9.503430907365502,
      "grad_norm": 4.3945488929748535,
      "learning_rate": 4.208047424386209e-05,
      "loss": 0.6552,
      "step": 1041500
    },
    {
      "epoch": 9.504343382728667,
      "grad_norm": 4.256235599517822,
      "learning_rate": 4.207971384772612e-05,
      "loss": 0.7061,
      "step": 1041600
    },
    {
      "epoch": 9.505255858091832,
      "grad_norm": 4.021578788757324,
      "learning_rate": 4.207895345159014e-05,
      "loss": 0.6741,
      "step": 1041700
    },
    {
      "epoch": 9.506168333454998,
      "grad_norm": 4.930951118469238,
      "learning_rate": 4.207819305545418e-05,
      "loss": 0.6945,
      "step": 1041800
    },
    {
      "epoch": 9.507080808818161,
      "grad_norm": 4.447865009307861,
      "learning_rate": 4.20774326593182e-05,
      "loss": 0.6951,
      "step": 1041900
    },
    {
      "epoch": 9.507993284181326,
      "grad_norm": 4.038752555847168,
      "learning_rate": 4.207667226318223e-05,
      "loss": 0.7137,
      "step": 1042000
    },
    {
      "epoch": 9.508905759544492,
      "grad_norm": 3.456648111343384,
      "learning_rate": 4.207591186704626e-05,
      "loss": 0.6471,
      "step": 1042100
    },
    {
      "epoch": 9.509818234907657,
      "grad_norm": 4.671725749969482,
      "learning_rate": 4.207515147091029e-05,
      "loss": 0.7138,
      "step": 1042200
    },
    {
      "epoch": 9.510730710270822,
      "grad_norm": 4.347525596618652,
      "learning_rate": 4.2074391074774314e-05,
      "loss": 0.7143,
      "step": 1042300
    },
    {
      "epoch": 9.511643185633988,
      "grad_norm": 4.800418853759766,
      "learning_rate": 4.2073630678638344e-05,
      "loss": 0.6924,
      "step": 1042400
    },
    {
      "epoch": 9.512555660997153,
      "grad_norm": 3.5134122371673584,
      "learning_rate": 4.2072870282502374e-05,
      "loss": 0.6804,
      "step": 1042500
    },
    {
      "epoch": 9.513468136360318,
      "grad_norm": 3.979417085647583,
      "learning_rate": 4.2072109886366404e-05,
      "loss": 0.6599,
      "step": 1042600
    },
    {
      "epoch": 9.514380611723483,
      "grad_norm": 2.9027223587036133,
      "learning_rate": 4.2071349490230434e-05,
      "loss": 0.6655,
      "step": 1042700
    },
    {
      "epoch": 9.515293087086649,
      "grad_norm": 4.3939433097839355,
      "learning_rate": 4.207058909409446e-05,
      "loss": 0.7077,
      "step": 1042800
    },
    {
      "epoch": 9.516205562449814,
      "grad_norm": 4.298759460449219,
      "learning_rate": 4.2069828697958494e-05,
      "loss": 0.6904,
      "step": 1042900
    },
    {
      "epoch": 9.51711803781298,
      "grad_norm": 3.4967455863952637,
      "learning_rate": 4.206906830182252e-05,
      "loss": 0.6719,
      "step": 1043000
    },
    {
      "epoch": 9.518030513176145,
      "grad_norm": 3.2115955352783203,
      "learning_rate": 4.206830790568655e-05,
      "loss": 0.7158,
      "step": 1043100
    },
    {
      "epoch": 9.51894298853931,
      "grad_norm": 4.071151256561279,
      "learning_rate": 4.206754750955058e-05,
      "loss": 0.6939,
      "step": 1043200
    },
    {
      "epoch": 9.519855463902475,
      "grad_norm": 5.4955153465271,
      "learning_rate": 4.206678711341461e-05,
      "loss": 0.7202,
      "step": 1043300
    },
    {
      "epoch": 9.52076793926564,
      "grad_norm": 3.6601269245147705,
      "learning_rate": 4.206602671727863e-05,
      "loss": 0.6618,
      "step": 1043400
    },
    {
      "epoch": 9.521680414628804,
      "grad_norm": 4.346809387207031,
      "learning_rate": 4.206526632114267e-05,
      "loss": 0.682,
      "step": 1043500
    },
    {
      "epoch": 9.52259288999197,
      "grad_norm": 3.981787919998169,
      "learning_rate": 4.206450592500669e-05,
      "loss": 0.717,
      "step": 1043600
    },
    {
      "epoch": 9.523505365355135,
      "grad_norm": 4.160004615783691,
      "learning_rate": 4.206374552887072e-05,
      "loss": 0.7011,
      "step": 1043700
    },
    {
      "epoch": 9.5244178407183,
      "grad_norm": 4.251143932342529,
      "learning_rate": 4.206298513273475e-05,
      "loss": 0.6518,
      "step": 1043800
    },
    {
      "epoch": 9.525330316081465,
      "grad_norm": 3.5131258964538574,
      "learning_rate": 4.206222473659878e-05,
      "loss": 0.7541,
      "step": 1043900
    },
    {
      "epoch": 9.52624279144463,
      "grad_norm": 4.312569618225098,
      "learning_rate": 4.206146434046281e-05,
      "loss": 0.6988,
      "step": 1044000
    },
    {
      "epoch": 9.527155266807796,
      "grad_norm": 4.609282970428467,
      "learning_rate": 4.206070394432684e-05,
      "loss": 0.7047,
      "step": 1044100
    },
    {
      "epoch": 9.528067742170961,
      "grad_norm": 4.220204830169678,
      "learning_rate": 4.2059943548190865e-05,
      "loss": 0.6644,
      "step": 1044200
    },
    {
      "epoch": 9.528980217534126,
      "grad_norm": 4.0241475105285645,
      "learning_rate": 4.20591831520549e-05,
      "loss": 0.6967,
      "step": 1044300
    },
    {
      "epoch": 9.529892692897292,
      "grad_norm": 3.689612627029419,
      "learning_rate": 4.2058422755918925e-05,
      "loss": 0.6634,
      "step": 1044400
    },
    {
      "epoch": 9.530805168260457,
      "grad_norm": 3.9827778339385986,
      "learning_rate": 4.2057662359782955e-05,
      "loss": 0.6628,
      "step": 1044500
    },
    {
      "epoch": 9.531717643623622,
      "grad_norm": 3.682453155517578,
      "learning_rate": 4.2056901963646985e-05,
      "loss": 0.6959,
      "step": 1044600
    },
    {
      "epoch": 9.532630118986788,
      "grad_norm": 4.6917243003845215,
      "learning_rate": 4.2056141567511015e-05,
      "loss": 0.6731,
      "step": 1044700
    },
    {
      "epoch": 9.533542594349953,
      "grad_norm": 4.0317792892456055,
      "learning_rate": 4.205538117137504e-05,
      "loss": 0.6561,
      "step": 1044800
    },
    {
      "epoch": 9.534455069713118,
      "grad_norm": 3.3319289684295654,
      "learning_rate": 4.2054620775239075e-05,
      "loss": 0.7069,
      "step": 1044900
    },
    {
      "epoch": 9.535367545076284,
      "grad_norm": 4.421604633331299,
      "learning_rate": 4.20538603791031e-05,
      "loss": 0.6843,
      "step": 1045000
    },
    {
      "epoch": 9.536280020439449,
      "grad_norm": 4.572640419006348,
      "learning_rate": 4.205309998296713e-05,
      "loss": 0.6757,
      "step": 1045100
    },
    {
      "epoch": 9.537192495802614,
      "grad_norm": 3.5042576789855957,
      "learning_rate": 4.205233958683116e-05,
      "loss": 0.7098,
      "step": 1045200
    },
    {
      "epoch": 9.538104971165778,
      "grad_norm": 3.265349864959717,
      "learning_rate": 4.205157919069518e-05,
      "loss": 0.7082,
      "step": 1045300
    },
    {
      "epoch": 9.539017446528943,
      "grad_norm": 3.222318172454834,
      "learning_rate": 4.205081879455922e-05,
      "loss": 0.6898,
      "step": 1045400
    },
    {
      "epoch": 9.539929921892108,
      "grad_norm": 3.116952419281006,
      "learning_rate": 4.205005839842324e-05,
      "loss": 0.6957,
      "step": 1045500
    },
    {
      "epoch": 9.540842397255274,
      "grad_norm": 4.492216110229492,
      "learning_rate": 4.204929800228727e-05,
      "loss": 0.6863,
      "step": 1045600
    },
    {
      "epoch": 9.541754872618439,
      "grad_norm": 3.4077975749969482,
      "learning_rate": 4.20485376061513e-05,
      "loss": 0.6581,
      "step": 1045700
    },
    {
      "epoch": 9.542667347981604,
      "grad_norm": 3.827552080154419,
      "learning_rate": 4.204777721001533e-05,
      "loss": 0.6742,
      "step": 1045800
    },
    {
      "epoch": 9.54357982334477,
      "grad_norm": 3.8205513954162598,
      "learning_rate": 4.2047016813879356e-05,
      "loss": 0.7325,
      "step": 1045900
    },
    {
      "epoch": 9.544492298707935,
      "grad_norm": 4.461954116821289,
      "learning_rate": 4.204625641774339e-05,
      "loss": 0.7183,
      "step": 1046000
    },
    {
      "epoch": 9.5454047740711,
      "grad_norm": 3.6666338443756104,
      "learning_rate": 4.2045496021607416e-05,
      "loss": 0.7224,
      "step": 1046100
    },
    {
      "epoch": 9.546317249434265,
      "grad_norm": 3.404446840286255,
      "learning_rate": 4.2044735625471446e-05,
      "loss": 0.7108,
      "step": 1046200
    },
    {
      "epoch": 9.54722972479743,
      "grad_norm": 3.2446210384368896,
      "learning_rate": 4.2043975229335476e-05,
      "loss": 0.719,
      "step": 1046300
    },
    {
      "epoch": 9.548142200160596,
      "grad_norm": 2.3229868412017822,
      "learning_rate": 4.2043214833199506e-05,
      "loss": 0.6844,
      "step": 1046400
    },
    {
      "epoch": 9.549054675523761,
      "grad_norm": 3.542642593383789,
      "learning_rate": 4.2042454437063536e-05,
      "loss": 0.7059,
      "step": 1046500
    },
    {
      "epoch": 9.549967150886927,
      "grad_norm": 3.2048752307891846,
      "learning_rate": 4.2041694040927566e-05,
      "loss": 0.6879,
      "step": 1046600
    },
    {
      "epoch": 9.550879626250092,
      "grad_norm": 4.606025695800781,
      "learning_rate": 4.204093364479159e-05,
      "loss": 0.6798,
      "step": 1046700
    },
    {
      "epoch": 9.551792101613257,
      "grad_norm": 4.266276836395264,
      "learning_rate": 4.2040173248655626e-05,
      "loss": 0.7089,
      "step": 1046800
    },
    {
      "epoch": 9.55270457697642,
      "grad_norm": 3.7432758808135986,
      "learning_rate": 4.203941285251965e-05,
      "loss": 0.6924,
      "step": 1046900
    },
    {
      "epoch": 9.553617052339586,
      "grad_norm": 4.585308074951172,
      "learning_rate": 4.203865245638368e-05,
      "loss": 0.6319,
      "step": 1047000
    },
    {
      "epoch": 9.554529527702751,
      "grad_norm": 4.167628765106201,
      "learning_rate": 4.203789206024771e-05,
      "loss": 0.7387,
      "step": 1047100
    },
    {
      "epoch": 9.555442003065917,
      "grad_norm": 3.699842929840088,
      "learning_rate": 4.203713166411174e-05,
      "loss": 0.6737,
      "step": 1047200
    },
    {
      "epoch": 9.556354478429082,
      "grad_norm": 3.5696611404418945,
      "learning_rate": 4.203637126797576e-05,
      "loss": 0.6757,
      "step": 1047300
    },
    {
      "epoch": 9.557266953792247,
      "grad_norm": 4.542766571044922,
      "learning_rate": 4.20356108718398e-05,
      "loss": 0.65,
      "step": 1047400
    },
    {
      "epoch": 9.558179429155413,
      "grad_norm": 4.582049369812012,
      "learning_rate": 4.203485047570382e-05,
      "loss": 0.6586,
      "step": 1047500
    },
    {
      "epoch": 9.559091904518578,
      "grad_norm": 4.131943702697754,
      "learning_rate": 4.203409007956785e-05,
      "loss": 0.7194,
      "step": 1047600
    },
    {
      "epoch": 9.560004379881743,
      "grad_norm": 4.021688461303711,
      "learning_rate": 4.203332968343188e-05,
      "loss": 0.696,
      "step": 1047700
    },
    {
      "epoch": 9.560916855244908,
      "grad_norm": 4.867702960968018,
      "learning_rate": 4.203256928729591e-05,
      "loss": 0.7026,
      "step": 1047800
    },
    {
      "epoch": 9.561829330608074,
      "grad_norm": 4.774940490722656,
      "learning_rate": 4.203180889115994e-05,
      "loss": 0.65,
      "step": 1047900
    },
    {
      "epoch": 9.562741805971239,
      "grad_norm": 4.314733505249023,
      "learning_rate": 4.2031048495023973e-05,
      "loss": 0.6883,
      "step": 1048000
    },
    {
      "epoch": 9.563654281334404,
      "grad_norm": 4.074741840362549,
      "learning_rate": 4.2030288098888e-05,
      "loss": 0.6882,
      "step": 1048100
    },
    {
      "epoch": 9.56456675669757,
      "grad_norm": 4.547235488891602,
      "learning_rate": 4.202952770275203e-05,
      "loss": 0.6874,
      "step": 1048200
    },
    {
      "epoch": 9.565479232060735,
      "grad_norm": 3.8238556385040283,
      "learning_rate": 4.202876730661606e-05,
      "loss": 0.6914,
      "step": 1048300
    },
    {
      "epoch": 9.5663917074239,
      "grad_norm": 3.6361663341522217,
      "learning_rate": 4.202800691048008e-05,
      "loss": 0.7013,
      "step": 1048400
    },
    {
      "epoch": 9.567304182787066,
      "grad_norm": 4.329410076141357,
      "learning_rate": 4.202724651434412e-05,
      "loss": 0.645,
      "step": 1048500
    },
    {
      "epoch": 9.56821665815023,
      "grad_norm": 3.9513325691223145,
      "learning_rate": 4.202648611820814e-05,
      "loss": 0.7034,
      "step": 1048600
    },
    {
      "epoch": 9.569129133513394,
      "grad_norm": 5.021946907043457,
      "learning_rate": 4.202572572207217e-05,
      "loss": 0.7073,
      "step": 1048700
    },
    {
      "epoch": 9.57004160887656,
      "grad_norm": 4.9767069816589355,
      "learning_rate": 4.20249653259362e-05,
      "loss": 0.6845,
      "step": 1048800
    },
    {
      "epoch": 9.570954084239725,
      "grad_norm": 3.8401670455932617,
      "learning_rate": 4.202420492980023e-05,
      "loss": 0.7397,
      "step": 1048900
    },
    {
      "epoch": 9.57186655960289,
      "grad_norm": 3.826986074447632,
      "learning_rate": 4.202344453366426e-05,
      "loss": 0.7141,
      "step": 1049000
    },
    {
      "epoch": 9.572779034966056,
      "grad_norm": 4.110471725463867,
      "learning_rate": 4.202268413752829e-05,
      "loss": 0.6596,
      "step": 1049100
    },
    {
      "epoch": 9.57369151032922,
      "grad_norm": 4.412177562713623,
      "learning_rate": 4.2021923741392314e-05,
      "loss": 0.6902,
      "step": 1049200
    },
    {
      "epoch": 9.574603985692386,
      "grad_norm": 4.547984600067139,
      "learning_rate": 4.202116334525635e-05,
      "loss": 0.6953,
      "step": 1049300
    },
    {
      "epoch": 9.575516461055551,
      "grad_norm": 4.561061859130859,
      "learning_rate": 4.2020402949120374e-05,
      "loss": 0.7099,
      "step": 1049400
    },
    {
      "epoch": 9.576428936418717,
      "grad_norm": 3.9724531173706055,
      "learning_rate": 4.2019642552984404e-05,
      "loss": 0.6899,
      "step": 1049500
    },
    {
      "epoch": 9.577341411781882,
      "grad_norm": 4.019392967224121,
      "learning_rate": 4.2018882156848434e-05,
      "loss": 0.694,
      "step": 1049600
    },
    {
      "epoch": 9.578253887145047,
      "grad_norm": 3.525061845779419,
      "learning_rate": 4.2018121760712464e-05,
      "loss": 0.7093,
      "step": 1049700
    },
    {
      "epoch": 9.579166362508213,
      "grad_norm": 4.418355941772461,
      "learning_rate": 4.201736136457649e-05,
      "loss": 0.6838,
      "step": 1049800
    },
    {
      "epoch": 9.580078837871378,
      "grad_norm": 4.414090633392334,
      "learning_rate": 4.2016600968440524e-05,
      "loss": 0.6574,
      "step": 1049900
    },
    {
      "epoch": 9.580991313234543,
      "grad_norm": 4.145714282989502,
      "learning_rate": 4.201584057230455e-05,
      "loss": 0.6918,
      "step": 1050000
    },
    {
      "epoch": 9.581903788597709,
      "grad_norm": 5.502116680145264,
      "learning_rate": 4.201508017616858e-05,
      "loss": 0.7197,
      "step": 1050100
    },
    {
      "epoch": 9.582816263960874,
      "grad_norm": 3.6928348541259766,
      "learning_rate": 4.201431978003261e-05,
      "loss": 0.7345,
      "step": 1050200
    },
    {
      "epoch": 9.583728739324037,
      "grad_norm": 4.231928825378418,
      "learning_rate": 4.201355938389664e-05,
      "loss": 0.684,
      "step": 1050300
    },
    {
      "epoch": 9.584641214687203,
      "grad_norm": 3.6863291263580322,
      "learning_rate": 4.201279898776067e-05,
      "loss": 0.6645,
      "step": 1050400
    },
    {
      "epoch": 9.585553690050368,
      "grad_norm": 3.365999937057495,
      "learning_rate": 4.20120385916247e-05,
      "loss": 0.6732,
      "step": 1050500
    },
    {
      "epoch": 9.586466165413533,
      "grad_norm": 4.652895450592041,
      "learning_rate": 4.201127819548872e-05,
      "loss": 0.7222,
      "step": 1050600
    },
    {
      "epoch": 9.587378640776699,
      "grad_norm": 4.278105735778809,
      "learning_rate": 4.201051779935276e-05,
      "loss": 0.6844,
      "step": 1050700
    },
    {
      "epoch": 9.588291116139864,
      "grad_norm": 3.9010274410247803,
      "learning_rate": 4.200975740321678e-05,
      "loss": 0.6942,
      "step": 1050800
    },
    {
      "epoch": 9.58920359150303,
      "grad_norm": 3.470146894454956,
      "learning_rate": 4.200899700708081e-05,
      "loss": 0.7054,
      "step": 1050900
    },
    {
      "epoch": 9.590116066866194,
      "grad_norm": 3.0673882961273193,
      "learning_rate": 4.200823661094484e-05,
      "loss": 0.7045,
      "step": 1051000
    },
    {
      "epoch": 9.59102854222936,
      "grad_norm": 3.7533535957336426,
      "learning_rate": 4.2007476214808865e-05,
      "loss": 0.6738,
      "step": 1051100
    },
    {
      "epoch": 9.591941017592525,
      "grad_norm": 2.9501259326934814,
      "learning_rate": 4.2006715818672895e-05,
      "loss": 0.6642,
      "step": 1051200
    },
    {
      "epoch": 9.59285349295569,
      "grad_norm": 5.285445213317871,
      "learning_rate": 4.2005955422536925e-05,
      "loss": 0.66,
      "step": 1051300
    },
    {
      "epoch": 9.593765968318856,
      "grad_norm": 4.100728988647461,
      "learning_rate": 4.2005195026400955e-05,
      "loss": 0.6684,
      "step": 1051400
    },
    {
      "epoch": 9.594678443682021,
      "grad_norm": 4.721038341522217,
      "learning_rate": 4.2004434630264985e-05,
      "loss": 0.6736,
      "step": 1051500
    },
    {
      "epoch": 9.595590919045186,
      "grad_norm": 3.9387316703796387,
      "learning_rate": 4.2003674234129015e-05,
      "loss": 0.688,
      "step": 1051600
    },
    {
      "epoch": 9.596503394408352,
      "grad_norm": 4.484405040740967,
      "learning_rate": 4.200291383799304e-05,
      "loss": 0.7252,
      "step": 1051700
    },
    {
      "epoch": 9.597415869771517,
      "grad_norm": 3.919447660446167,
      "learning_rate": 4.2002153441857075e-05,
      "loss": 0.6997,
      "step": 1051800
    },
    {
      "epoch": 9.598328345134682,
      "grad_norm": 4.433726787567139,
      "learning_rate": 4.20013930457211e-05,
      "loss": 0.6772,
      "step": 1051900
    },
    {
      "epoch": 9.599240820497847,
      "grad_norm": 4.244019031524658,
      "learning_rate": 4.200063264958513e-05,
      "loss": 0.6848,
      "step": 1052000
    },
    {
      "epoch": 9.600153295861011,
      "grad_norm": 4.038148880004883,
      "learning_rate": 4.199987225344916e-05,
      "loss": 0.6818,
      "step": 1052100
    },
    {
      "epoch": 9.601065771224176,
      "grad_norm": 3.646914005279541,
      "learning_rate": 4.199911185731319e-05,
      "loss": 0.7062,
      "step": 1052200
    },
    {
      "epoch": 9.601978246587342,
      "grad_norm": 3.974599599838257,
      "learning_rate": 4.199835146117722e-05,
      "loss": 0.6915,
      "step": 1052300
    },
    {
      "epoch": 9.602890721950507,
      "grad_norm": 4.956359386444092,
      "learning_rate": 4.199759106504125e-05,
      "loss": 0.6464,
      "step": 1052400
    },
    {
      "epoch": 9.603803197313672,
      "grad_norm": 4.081851959228516,
      "learning_rate": 4.199683066890527e-05,
      "loss": 0.6666,
      "step": 1052500
    },
    {
      "epoch": 9.604715672676837,
      "grad_norm": 3.049833059310913,
      "learning_rate": 4.19960702727693e-05,
      "loss": 0.6563,
      "step": 1052600
    },
    {
      "epoch": 9.605628148040003,
      "grad_norm": 4.397568702697754,
      "learning_rate": 4.199530987663333e-05,
      "loss": 0.6653,
      "step": 1052700
    },
    {
      "epoch": 9.606540623403168,
      "grad_norm": 3.842686414718628,
      "learning_rate": 4.199454948049736e-05,
      "loss": 0.6806,
      "step": 1052800
    },
    {
      "epoch": 9.607453098766333,
      "grad_norm": 3.242382764816284,
      "learning_rate": 4.199378908436139e-05,
      "loss": 0.6885,
      "step": 1052900
    },
    {
      "epoch": 9.608365574129499,
      "grad_norm": 4.145986557006836,
      "learning_rate": 4.199302868822542e-05,
      "loss": 0.7169,
      "step": 1053000
    },
    {
      "epoch": 9.609278049492664,
      "grad_norm": 3.9116647243499756,
      "learning_rate": 4.1992268292089446e-05,
      "loss": 0.6903,
      "step": 1053100
    },
    {
      "epoch": 9.61019052485583,
      "grad_norm": 3.1816465854644775,
      "learning_rate": 4.199150789595348e-05,
      "loss": 0.6914,
      "step": 1053200
    },
    {
      "epoch": 9.611103000218995,
      "grad_norm": 4.358950614929199,
      "learning_rate": 4.1990747499817506e-05,
      "loss": 0.6611,
      "step": 1053300
    },
    {
      "epoch": 9.61201547558216,
      "grad_norm": 5.058081150054932,
      "learning_rate": 4.1989987103681536e-05,
      "loss": 0.7038,
      "step": 1053400
    },
    {
      "epoch": 9.612927950945325,
      "grad_norm": 5.519130706787109,
      "learning_rate": 4.1989226707545566e-05,
      "loss": 0.7224,
      "step": 1053500
    },
    {
      "epoch": 9.61384042630849,
      "grad_norm": 3.9541704654693604,
      "learning_rate": 4.1988466311409596e-05,
      "loss": 0.7077,
      "step": 1053600
    },
    {
      "epoch": 9.614752901671654,
      "grad_norm": 3.971968173980713,
      "learning_rate": 4.1987705915273626e-05,
      "loss": 0.7062,
      "step": 1053700
    },
    {
      "epoch": 9.61566537703482,
      "grad_norm": 4.095557689666748,
      "learning_rate": 4.198694551913765e-05,
      "loss": 0.6951,
      "step": 1053800
    },
    {
      "epoch": 9.616577852397985,
      "grad_norm": 3.8572230339050293,
      "learning_rate": 4.198618512300168e-05,
      "loss": 0.6927,
      "step": 1053900
    },
    {
      "epoch": 9.61749032776115,
      "grad_norm": 4.456506729125977,
      "learning_rate": 4.198542472686571e-05,
      "loss": 0.7032,
      "step": 1054000
    },
    {
      "epoch": 9.618402803124315,
      "grad_norm": 5.575626850128174,
      "learning_rate": 4.198466433072974e-05,
      "loss": 0.6699,
      "step": 1054100
    },
    {
      "epoch": 9.61931527848748,
      "grad_norm": 4.365490913391113,
      "learning_rate": 4.198390393459376e-05,
      "loss": 0.6288,
      "step": 1054200
    },
    {
      "epoch": 9.620227753850646,
      "grad_norm": 3.705909013748169,
      "learning_rate": 4.19831435384578e-05,
      "loss": 0.6654,
      "step": 1054300
    },
    {
      "epoch": 9.621140229213811,
      "grad_norm": 3.9123282432556152,
      "learning_rate": 4.198238314232182e-05,
      "loss": 0.6403,
      "step": 1054400
    },
    {
      "epoch": 9.622052704576976,
      "grad_norm": 4.580163955688477,
      "learning_rate": 4.198162274618585e-05,
      "loss": 0.6704,
      "step": 1054500
    },
    {
      "epoch": 9.622965179940142,
      "grad_norm": 4.069721698760986,
      "learning_rate": 4.198086235004988e-05,
      "loss": 0.7338,
      "step": 1054600
    },
    {
      "epoch": 9.623877655303307,
      "grad_norm": 3.482910633087158,
      "learning_rate": 4.198010195391391e-05,
      "loss": 0.71,
      "step": 1054700
    },
    {
      "epoch": 9.624790130666472,
      "grad_norm": 3.7731246948242188,
      "learning_rate": 4.1979341557777943e-05,
      "loss": 0.7013,
      "step": 1054800
    },
    {
      "epoch": 9.625702606029638,
      "grad_norm": 4.067139625549316,
      "learning_rate": 4.1978581161641974e-05,
      "loss": 0.686,
      "step": 1054900
    },
    {
      "epoch": 9.626615081392803,
      "grad_norm": 4.635762691497803,
      "learning_rate": 4.1977820765506e-05,
      "loss": 0.7038,
      "step": 1055000
    },
    {
      "epoch": 9.627527556755968,
      "grad_norm": 4.121212959289551,
      "learning_rate": 4.1977060369370034e-05,
      "loss": 0.7065,
      "step": 1055100
    },
    {
      "epoch": 9.628440032119133,
      "grad_norm": 4.379951000213623,
      "learning_rate": 4.197629997323406e-05,
      "loss": 0.6658,
      "step": 1055200
    },
    {
      "epoch": 9.629352507482299,
      "grad_norm": 5.830300331115723,
      "learning_rate": 4.197553957709809e-05,
      "loss": 0.7217,
      "step": 1055300
    },
    {
      "epoch": 9.630264982845464,
      "grad_norm": 4.555906772613525,
      "learning_rate": 4.197477918096212e-05,
      "loss": 0.7072,
      "step": 1055400
    },
    {
      "epoch": 9.631177458208628,
      "grad_norm": 4.320178031921387,
      "learning_rate": 4.197401878482615e-05,
      "loss": 0.6746,
      "step": 1055500
    },
    {
      "epoch": 9.632089933571793,
      "grad_norm": 4.969639778137207,
      "learning_rate": 4.197325838869017e-05,
      "loss": 0.6826,
      "step": 1055600
    },
    {
      "epoch": 9.633002408934958,
      "grad_norm": 3.8673505783081055,
      "learning_rate": 4.197249799255421e-05,
      "loss": 0.681,
      "step": 1055700
    },
    {
      "epoch": 9.633914884298123,
      "grad_norm": 5.921220302581787,
      "learning_rate": 4.197173759641823e-05,
      "loss": 0.7009,
      "step": 1055800
    },
    {
      "epoch": 9.634827359661289,
      "grad_norm": 3.0905895233154297,
      "learning_rate": 4.197097720028226e-05,
      "loss": 0.6538,
      "step": 1055900
    },
    {
      "epoch": 9.635739835024454,
      "grad_norm": 4.552901744842529,
      "learning_rate": 4.197021680414629e-05,
      "loss": 0.6961,
      "step": 1056000
    },
    {
      "epoch": 9.63665231038762,
      "grad_norm": 4.227990627288818,
      "learning_rate": 4.196945640801032e-05,
      "loss": 0.6723,
      "step": 1056100
    },
    {
      "epoch": 9.637564785750785,
      "grad_norm": 3.6011509895324707,
      "learning_rate": 4.196869601187435e-05,
      "loss": 0.7155,
      "step": 1056200
    },
    {
      "epoch": 9.63847726111395,
      "grad_norm": 4.78896951675415,
      "learning_rate": 4.196793561573838e-05,
      "loss": 0.6965,
      "step": 1056300
    },
    {
      "epoch": 9.639389736477115,
      "grad_norm": 3.8631222248077393,
      "learning_rate": 4.1967175219602404e-05,
      "loss": 0.6893,
      "step": 1056400
    },
    {
      "epoch": 9.64030221184028,
      "grad_norm": 2.9405694007873535,
      "learning_rate": 4.196641482346644e-05,
      "loss": 0.6949,
      "step": 1056500
    },
    {
      "epoch": 9.641214687203446,
      "grad_norm": 4.150258541107178,
      "learning_rate": 4.1965654427330464e-05,
      "loss": 0.683,
      "step": 1056600
    },
    {
      "epoch": 9.642127162566611,
      "grad_norm": 4.678225994110107,
      "learning_rate": 4.196489403119449e-05,
      "loss": 0.6978,
      "step": 1056700
    },
    {
      "epoch": 9.643039637929776,
      "grad_norm": 4.05911111831665,
      "learning_rate": 4.1964133635058524e-05,
      "loss": 0.6759,
      "step": 1056800
    },
    {
      "epoch": 9.643952113292942,
      "grad_norm": 4.617972373962402,
      "learning_rate": 4.196337323892255e-05,
      "loss": 0.6722,
      "step": 1056900
    },
    {
      "epoch": 9.644864588656107,
      "grad_norm": 4.3874640464782715,
      "learning_rate": 4.196261284278658e-05,
      "loss": 0.7372,
      "step": 1057000
    },
    {
      "epoch": 9.64577706401927,
      "grad_norm": 3.5969693660736084,
      "learning_rate": 4.196185244665061e-05,
      "loss": 0.7188,
      "step": 1057100
    },
    {
      "epoch": 9.646689539382436,
      "grad_norm": 4.597809314727783,
      "learning_rate": 4.196109205051464e-05,
      "loss": 0.7033,
      "step": 1057200
    },
    {
      "epoch": 9.647602014745601,
      "grad_norm": 4.524618625640869,
      "learning_rate": 4.196033165437867e-05,
      "loss": 0.7119,
      "step": 1057300
    },
    {
      "epoch": 9.648514490108766,
      "grad_norm": 4.104069232940674,
      "learning_rate": 4.19595712582427e-05,
      "loss": 0.6918,
      "step": 1057400
    },
    {
      "epoch": 9.649426965471932,
      "grad_norm": 3.1397199630737305,
      "learning_rate": 4.195881086210672e-05,
      "loss": 0.6943,
      "step": 1057500
    },
    {
      "epoch": 9.650339440835097,
      "grad_norm": 4.9059247970581055,
      "learning_rate": 4.195805046597076e-05,
      "loss": 0.6877,
      "step": 1057600
    },
    {
      "epoch": 9.651251916198262,
      "grad_norm": 4.1606245040893555,
      "learning_rate": 4.195729006983478e-05,
      "loss": 0.6731,
      "step": 1057700
    },
    {
      "epoch": 9.652164391561428,
      "grad_norm": 4.631048202514648,
      "learning_rate": 4.195652967369881e-05,
      "loss": 0.6976,
      "step": 1057800
    },
    {
      "epoch": 9.653076866924593,
      "grad_norm": 3.482191801071167,
      "learning_rate": 4.195576927756284e-05,
      "loss": 0.7046,
      "step": 1057900
    },
    {
      "epoch": 9.653989342287758,
      "grad_norm": 3.760122060775757,
      "learning_rate": 4.195500888142687e-05,
      "loss": 0.7111,
      "step": 1058000
    },
    {
      "epoch": 9.654901817650924,
      "grad_norm": 3.3817927837371826,
      "learning_rate": 4.1954248485290895e-05,
      "loss": 0.694,
      "step": 1058100
    },
    {
      "epoch": 9.655814293014089,
      "grad_norm": 3.7892026901245117,
      "learning_rate": 4.195348808915493e-05,
      "loss": 0.6778,
      "step": 1058200
    },
    {
      "epoch": 9.656726768377254,
      "grad_norm": 5.264703273773193,
      "learning_rate": 4.1952727693018955e-05,
      "loss": 0.6721,
      "step": 1058300
    },
    {
      "epoch": 9.65763924374042,
      "grad_norm": 4.024043083190918,
      "learning_rate": 4.1951967296882985e-05,
      "loss": 0.7071,
      "step": 1058400
    },
    {
      "epoch": 9.658551719103585,
      "grad_norm": 4.027953147888184,
      "learning_rate": 4.1951206900747015e-05,
      "loss": 0.6987,
      "step": 1058500
    },
    {
      "epoch": 9.65946419446675,
      "grad_norm": 4.438676357269287,
      "learning_rate": 4.1950446504611045e-05,
      "loss": 0.7106,
      "step": 1058600
    },
    {
      "epoch": 9.660376669829915,
      "grad_norm": 3.8508079051971436,
      "learning_rate": 4.1949686108475075e-05,
      "loss": 0.6874,
      "step": 1058700
    },
    {
      "epoch": 9.66128914519308,
      "grad_norm": 3.891989231109619,
      "learning_rate": 4.1948925712339105e-05,
      "loss": 0.6733,
      "step": 1058800
    },
    {
      "epoch": 9.662201620556244,
      "grad_norm": 4.765913486480713,
      "learning_rate": 4.194816531620313e-05,
      "loss": 0.7123,
      "step": 1058900
    },
    {
      "epoch": 9.66311409591941,
      "grad_norm": 3.556466817855835,
      "learning_rate": 4.1947404920067166e-05,
      "loss": 0.721,
      "step": 1059000
    },
    {
      "epoch": 9.664026571282575,
      "grad_norm": 5.408646583557129,
      "learning_rate": 4.194664452393119e-05,
      "loss": 0.6824,
      "step": 1059100
    },
    {
      "epoch": 9.66493904664574,
      "grad_norm": 3.458488702774048,
      "learning_rate": 4.194588412779522e-05,
      "loss": 0.7151,
      "step": 1059200
    },
    {
      "epoch": 9.665851522008905,
      "grad_norm": 3.5938150882720947,
      "learning_rate": 4.194512373165925e-05,
      "loss": 0.6679,
      "step": 1059300
    },
    {
      "epoch": 9.66676399737207,
      "grad_norm": 4.304788589477539,
      "learning_rate": 4.194436333552327e-05,
      "loss": 0.6932,
      "step": 1059400
    },
    {
      "epoch": 9.667676472735236,
      "grad_norm": 4.056553840637207,
      "learning_rate": 4.19436029393873e-05,
      "loss": 0.7166,
      "step": 1059500
    },
    {
      "epoch": 9.668588948098401,
      "grad_norm": 3.9915618896484375,
      "learning_rate": 4.194284254325133e-05,
      "loss": 0.6716,
      "step": 1059600
    },
    {
      "epoch": 9.669501423461567,
      "grad_norm": 3.6864147186279297,
      "learning_rate": 4.194208214711536e-05,
      "loss": 0.6893,
      "step": 1059700
    },
    {
      "epoch": 9.670413898824732,
      "grad_norm": 3.8682379722595215,
      "learning_rate": 4.194132175097939e-05,
      "loss": 0.6975,
      "step": 1059800
    },
    {
      "epoch": 9.671326374187897,
      "grad_norm": 3.8015639781951904,
      "learning_rate": 4.194056135484342e-05,
      "loss": 0.6828,
      "step": 1059900
    },
    {
      "epoch": 9.672238849551063,
      "grad_norm": 3.200275421142578,
      "learning_rate": 4.1939800958707446e-05,
      "loss": 0.6536,
      "step": 1060000
    },
    {
      "epoch": 9.673151324914228,
      "grad_norm": 4.007441997528076,
      "learning_rate": 4.193904056257148e-05,
      "loss": 0.6906,
      "step": 1060100
    },
    {
      "epoch": 9.674063800277393,
      "grad_norm": 4.314164638519287,
      "learning_rate": 4.1938280166435506e-05,
      "loss": 0.7011,
      "step": 1060200
    },
    {
      "epoch": 9.674976275640558,
      "grad_norm": 4.423404216766357,
      "learning_rate": 4.1937519770299536e-05,
      "loss": 0.6584,
      "step": 1060300
    },
    {
      "epoch": 9.675888751003724,
      "grad_norm": 4.633143901824951,
      "learning_rate": 4.1936759374163566e-05,
      "loss": 0.6914,
      "step": 1060400
    },
    {
      "epoch": 9.676801226366887,
      "grad_norm": 3.8118209838867188,
      "learning_rate": 4.1935998978027596e-05,
      "loss": 0.6855,
      "step": 1060500
    },
    {
      "epoch": 9.677713701730053,
      "grad_norm": 2.8885581493377686,
      "learning_rate": 4.193523858189162e-05,
      "loss": 0.6795,
      "step": 1060600
    },
    {
      "epoch": 9.678626177093218,
      "grad_norm": 4.312844753265381,
      "learning_rate": 4.1934478185755656e-05,
      "loss": 0.6933,
      "step": 1060700
    },
    {
      "epoch": 9.679538652456383,
      "grad_norm": 4.259014129638672,
      "learning_rate": 4.193371778961968e-05,
      "loss": 0.6736,
      "step": 1060800
    },
    {
      "epoch": 9.680451127819548,
      "grad_norm": 3.685227155685425,
      "learning_rate": 4.193295739348371e-05,
      "loss": 0.6832,
      "step": 1060900
    },
    {
      "epoch": 9.681363603182714,
      "grad_norm": 5.086840629577637,
      "learning_rate": 4.193219699734774e-05,
      "loss": 0.6866,
      "step": 1061000
    },
    {
      "epoch": 9.682276078545879,
      "grad_norm": 3.655785083770752,
      "learning_rate": 4.193143660121177e-05,
      "loss": 0.7066,
      "step": 1061100
    },
    {
      "epoch": 9.683188553909044,
      "grad_norm": 4.160398006439209,
      "learning_rate": 4.19306762050758e-05,
      "loss": 0.6708,
      "step": 1061200
    },
    {
      "epoch": 9.68410102927221,
      "grad_norm": 3.393552303314209,
      "learning_rate": 4.192991580893983e-05,
      "loss": 0.6899,
      "step": 1061300
    },
    {
      "epoch": 9.685013504635375,
      "grad_norm": 3.623995542526245,
      "learning_rate": 4.192915541280385e-05,
      "loss": 0.6674,
      "step": 1061400
    },
    {
      "epoch": 9.68592597999854,
      "grad_norm": 3.689476728439331,
      "learning_rate": 4.192839501666789e-05,
      "loss": 0.6632,
      "step": 1061500
    },
    {
      "epoch": 9.686838455361706,
      "grad_norm": 4.1099677085876465,
      "learning_rate": 4.1927634620531913e-05,
      "loss": 0.665,
      "step": 1061600
    },
    {
      "epoch": 9.68775093072487,
      "grad_norm": 5.07739782333374,
      "learning_rate": 4.1926874224395944e-05,
      "loss": 0.6784,
      "step": 1061700
    },
    {
      "epoch": 9.688663406088036,
      "grad_norm": 4.6538872718811035,
      "learning_rate": 4.1926113828259974e-05,
      "loss": 0.7072,
      "step": 1061800
    },
    {
      "epoch": 9.689575881451201,
      "grad_norm": 4.093661785125732,
      "learning_rate": 4.1925353432124004e-05,
      "loss": 0.6775,
      "step": 1061900
    },
    {
      "epoch": 9.690488356814367,
      "grad_norm": 4.046497344970703,
      "learning_rate": 4.192459303598803e-05,
      "loss": 0.6582,
      "step": 1062000
    },
    {
      "epoch": 9.691400832177532,
      "grad_norm": 3.0489304065704346,
      "learning_rate": 4.1923832639852064e-05,
      "loss": 0.7119,
      "step": 1062100
    },
    {
      "epoch": 9.692313307540697,
      "grad_norm": 4.07534646987915,
      "learning_rate": 4.192307224371609e-05,
      "loss": 0.6971,
      "step": 1062200
    },
    {
      "epoch": 9.69322578290386,
      "grad_norm": 3.1935081481933594,
      "learning_rate": 4.192231184758012e-05,
      "loss": 0.6575,
      "step": 1062300
    },
    {
      "epoch": 9.694138258267026,
      "grad_norm": 4.939996242523193,
      "learning_rate": 4.192155145144415e-05,
      "loss": 0.6553,
      "step": 1062400
    },
    {
      "epoch": 9.695050733630191,
      "grad_norm": 3.616520404815674,
      "learning_rate": 4.192079105530817e-05,
      "loss": 0.672,
      "step": 1062500
    },
    {
      "epoch": 9.695963208993357,
      "grad_norm": 4.151928901672363,
      "learning_rate": 4.192003065917221e-05,
      "loss": 0.6431,
      "step": 1062600
    },
    {
      "epoch": 9.696875684356522,
      "grad_norm": 3.998534679412842,
      "learning_rate": 4.191927026303623e-05,
      "loss": 0.6887,
      "step": 1062700
    },
    {
      "epoch": 9.697788159719687,
      "grad_norm": 4.566169738769531,
      "learning_rate": 4.191850986690026e-05,
      "loss": 0.6615,
      "step": 1062800
    },
    {
      "epoch": 9.698700635082853,
      "grad_norm": 4.438015937805176,
      "learning_rate": 4.191774947076429e-05,
      "loss": 0.7018,
      "step": 1062900
    },
    {
      "epoch": 9.699613110446018,
      "grad_norm": 4.431830406188965,
      "learning_rate": 4.191698907462832e-05,
      "loss": 0.662,
      "step": 1063000
    },
    {
      "epoch": 9.700525585809183,
      "grad_norm": 3.532010316848755,
      "learning_rate": 4.1916228678492344e-05,
      "loss": 0.6636,
      "step": 1063100
    },
    {
      "epoch": 9.701438061172349,
      "grad_norm": 3.482034683227539,
      "learning_rate": 4.191546828235638e-05,
      "loss": 0.7323,
      "step": 1063200
    },
    {
      "epoch": 9.702350536535514,
      "grad_norm": 3.5129106044769287,
      "learning_rate": 4.1914707886220404e-05,
      "loss": 0.715,
      "step": 1063300
    },
    {
      "epoch": 9.70326301189868,
      "grad_norm": 3.1632468700408936,
      "learning_rate": 4.1913947490084434e-05,
      "loss": 0.6814,
      "step": 1063400
    },
    {
      "epoch": 9.704175487261844,
      "grad_norm": 4.158954620361328,
      "learning_rate": 4.1913187093948464e-05,
      "loss": 0.689,
      "step": 1063500
    },
    {
      "epoch": 9.70508796262501,
      "grad_norm": 4.166699409484863,
      "learning_rate": 4.1912426697812494e-05,
      "loss": 0.6583,
      "step": 1063600
    },
    {
      "epoch": 9.706000437988175,
      "grad_norm": 3.932544231414795,
      "learning_rate": 4.1911666301676525e-05,
      "loss": 0.6536,
      "step": 1063700
    },
    {
      "epoch": 9.70691291335134,
      "grad_norm": 4.355273246765137,
      "learning_rate": 4.1910905905540555e-05,
      "loss": 0.6304,
      "step": 1063800
    },
    {
      "epoch": 9.707825388714504,
      "grad_norm": 3.9441826343536377,
      "learning_rate": 4.191014550940458e-05,
      "loss": 0.7128,
      "step": 1063900
    },
    {
      "epoch": 9.70873786407767,
      "grad_norm": 3.798729658126831,
      "learning_rate": 4.1909385113268615e-05,
      "loss": 0.7166,
      "step": 1064000
    },
    {
      "epoch": 9.709650339440834,
      "grad_norm": 3.9510438442230225,
      "learning_rate": 4.190862471713264e-05,
      "loss": 0.6818,
      "step": 1064100
    },
    {
      "epoch": 9.710562814804,
      "grad_norm": 3.837585926055908,
      "learning_rate": 4.190786432099667e-05,
      "loss": 0.6784,
      "step": 1064200
    },
    {
      "epoch": 9.711475290167165,
      "grad_norm": 3.9772307872772217,
      "learning_rate": 4.19071039248607e-05,
      "loss": 0.6852,
      "step": 1064300
    },
    {
      "epoch": 9.71238776553033,
      "grad_norm": 3.737553834915161,
      "learning_rate": 4.190634352872473e-05,
      "loss": 0.6622,
      "step": 1064400
    },
    {
      "epoch": 9.713300240893496,
      "grad_norm": 4.534098148345947,
      "learning_rate": 4.190558313258875e-05,
      "loss": 0.6669,
      "step": 1064500
    },
    {
      "epoch": 9.714212716256661,
      "grad_norm": 3.83590030670166,
      "learning_rate": 4.190482273645279e-05,
      "loss": 0.7134,
      "step": 1064600
    },
    {
      "epoch": 9.715125191619826,
      "grad_norm": 3.872997522354126,
      "learning_rate": 4.190406234031681e-05,
      "loss": 0.6506,
      "step": 1064700
    },
    {
      "epoch": 9.716037666982992,
      "grad_norm": 4.036196231842041,
      "learning_rate": 4.190330194418084e-05,
      "loss": 0.6839,
      "step": 1064800
    },
    {
      "epoch": 9.716950142346157,
      "grad_norm": 4.841969966888428,
      "learning_rate": 4.190254154804487e-05,
      "loss": 0.6767,
      "step": 1064900
    },
    {
      "epoch": 9.717862617709322,
      "grad_norm": 4.003310203552246,
      "learning_rate": 4.19017811519089e-05,
      "loss": 0.681,
      "step": 1065000
    },
    {
      "epoch": 9.718775093072487,
      "grad_norm": 3.335698127746582,
      "learning_rate": 4.190102075577293e-05,
      "loss": 0.6643,
      "step": 1065100
    },
    {
      "epoch": 9.719687568435653,
      "grad_norm": 3.7285609245300293,
      "learning_rate": 4.1900260359636955e-05,
      "loss": 0.6376,
      "step": 1065200
    },
    {
      "epoch": 9.720600043798818,
      "grad_norm": 2.729830503463745,
      "learning_rate": 4.1899499963500985e-05,
      "loss": 0.7109,
      "step": 1065300
    },
    {
      "epoch": 9.721512519161983,
      "grad_norm": 3.879340171813965,
      "learning_rate": 4.1898739567365015e-05,
      "loss": 0.6809,
      "step": 1065400
    },
    {
      "epoch": 9.722424994525149,
      "grad_norm": 3.8698570728302,
      "learning_rate": 4.1897979171229045e-05,
      "loss": 0.6839,
      "step": 1065500
    },
    {
      "epoch": 9.723337469888314,
      "grad_norm": 4.166755676269531,
      "learning_rate": 4.1897218775093076e-05,
      "loss": 0.6986,
      "step": 1065600
    },
    {
      "epoch": 9.724249945251477,
      "grad_norm": 4.644587516784668,
      "learning_rate": 4.1896458378957106e-05,
      "loss": 0.6648,
      "step": 1065700
    },
    {
      "epoch": 9.725162420614643,
      "grad_norm": 3.9673092365264893,
      "learning_rate": 4.189569798282113e-05,
      "loss": 0.6952,
      "step": 1065800
    },
    {
      "epoch": 9.726074895977808,
      "grad_norm": 4.034038543701172,
      "learning_rate": 4.1894937586685166e-05,
      "loss": 0.6756,
      "step": 1065900
    },
    {
      "epoch": 9.726987371340973,
      "grad_norm": 3.3599419593811035,
      "learning_rate": 4.189417719054919e-05,
      "loss": 0.658,
      "step": 1066000
    },
    {
      "epoch": 9.727899846704139,
      "grad_norm": 4.23164176940918,
      "learning_rate": 4.189341679441322e-05,
      "loss": 0.653,
      "step": 1066100
    },
    {
      "epoch": 9.728812322067304,
      "grad_norm": 4.437072277069092,
      "learning_rate": 4.189265639827725e-05,
      "loss": 0.7058,
      "step": 1066200
    },
    {
      "epoch": 9.72972479743047,
      "grad_norm": 4.788966655731201,
      "learning_rate": 4.189189600214128e-05,
      "loss": 0.6934,
      "step": 1066300
    },
    {
      "epoch": 9.730637272793635,
      "grad_norm": 3.7474687099456787,
      "learning_rate": 4.18911356060053e-05,
      "loss": 0.7153,
      "step": 1066400
    },
    {
      "epoch": 9.7315497481568,
      "grad_norm": 4.278707981109619,
      "learning_rate": 4.189037520986934e-05,
      "loss": 0.71,
      "step": 1066500
    },
    {
      "epoch": 9.732462223519965,
      "grad_norm": 3.660294771194458,
      "learning_rate": 4.188961481373336e-05,
      "loss": 0.6475,
      "step": 1066600
    },
    {
      "epoch": 9.73337469888313,
      "grad_norm": 3.8177194595336914,
      "learning_rate": 4.188885441759739e-05,
      "loss": 0.6814,
      "step": 1066700
    },
    {
      "epoch": 9.734287174246296,
      "grad_norm": 4.334066867828369,
      "learning_rate": 4.188809402146142e-05,
      "loss": 0.6907,
      "step": 1066800
    },
    {
      "epoch": 9.735199649609461,
      "grad_norm": 3.9861886501312256,
      "learning_rate": 4.188733362532545e-05,
      "loss": 0.6839,
      "step": 1066900
    },
    {
      "epoch": 9.736112124972626,
      "grad_norm": 3.6913440227508545,
      "learning_rate": 4.188657322918948e-05,
      "loss": 0.6951,
      "step": 1067000
    },
    {
      "epoch": 9.737024600335792,
      "grad_norm": 4.310192108154297,
      "learning_rate": 4.188581283305351e-05,
      "loss": 0.6779,
      "step": 1067100
    },
    {
      "epoch": 9.737937075698957,
      "grad_norm": 4.041154384613037,
      "learning_rate": 4.1885052436917536e-05,
      "loss": 0.7051,
      "step": 1067200
    },
    {
      "epoch": 9.73884955106212,
      "grad_norm": 4.028541088104248,
      "learning_rate": 4.188429204078157e-05,
      "loss": 0.6363,
      "step": 1067300
    },
    {
      "epoch": 9.739762026425286,
      "grad_norm": 4.498058319091797,
      "learning_rate": 4.1883531644645596e-05,
      "loss": 0.6753,
      "step": 1067400
    },
    {
      "epoch": 9.740674501788451,
      "grad_norm": 4.1611528396606445,
      "learning_rate": 4.1882771248509626e-05,
      "loss": 0.6797,
      "step": 1067500
    },
    {
      "epoch": 9.741586977151616,
      "grad_norm": 3.9858245849609375,
      "learning_rate": 4.1882010852373657e-05,
      "loss": 0.6844,
      "step": 1067600
    },
    {
      "epoch": 9.742499452514782,
      "grad_norm": 4.6148600578308105,
      "learning_rate": 4.1881250456237687e-05,
      "loss": 0.6602,
      "step": 1067700
    },
    {
      "epoch": 9.743411927877947,
      "grad_norm": 3.8077430725097656,
      "learning_rate": 4.188049006010171e-05,
      "loss": 0.6797,
      "step": 1067800
    },
    {
      "epoch": 9.744324403241112,
      "grad_norm": 4.297841548919678,
      "learning_rate": 4.187972966396574e-05,
      "loss": 0.6845,
      "step": 1067900
    },
    {
      "epoch": 9.745236878604278,
      "grad_norm": 4.838282585144043,
      "learning_rate": 4.187896926782977e-05,
      "loss": 0.689,
      "step": 1068000
    },
    {
      "epoch": 9.746149353967443,
      "grad_norm": 4.280093669891357,
      "learning_rate": 4.18782088716938e-05,
      "loss": 0.6834,
      "step": 1068100
    },
    {
      "epoch": 9.747061829330608,
      "grad_norm": 4.675840377807617,
      "learning_rate": 4.187744847555783e-05,
      "loss": 0.6872,
      "step": 1068200
    },
    {
      "epoch": 9.747974304693773,
      "grad_norm": 3.7948310375213623,
      "learning_rate": 4.1876688079421853e-05,
      "loss": 0.6445,
      "step": 1068300
    },
    {
      "epoch": 9.748886780056939,
      "grad_norm": 3.879864454269409,
      "learning_rate": 4.187592768328589e-05,
      "loss": 0.6389,
      "step": 1068400
    },
    {
      "epoch": 9.749799255420104,
      "grad_norm": 4.317088603973389,
      "learning_rate": 4.1875167287149914e-05,
      "loss": 0.7085,
      "step": 1068500
    },
    {
      "epoch": 9.75071173078327,
      "grad_norm": 4.540278434753418,
      "learning_rate": 4.1874406891013944e-05,
      "loss": 0.6744,
      "step": 1068600
    },
    {
      "epoch": 9.751624206146435,
      "grad_norm": 3.069707155227661,
      "learning_rate": 4.1873646494877974e-05,
      "loss": 0.6883,
      "step": 1068700
    },
    {
      "epoch": 9.7525366815096,
      "grad_norm": 4.139070987701416,
      "learning_rate": 4.1872886098742004e-05,
      "loss": 0.6875,
      "step": 1068800
    },
    {
      "epoch": 9.753449156872765,
      "grad_norm": 3.364851474761963,
      "learning_rate": 4.187212570260603e-05,
      "loss": 0.6656,
      "step": 1068900
    },
    {
      "epoch": 9.75436163223593,
      "grad_norm": 4.868878364562988,
      "learning_rate": 4.1871365306470064e-05,
      "loss": 0.6525,
      "step": 1069000
    },
    {
      "epoch": 9.755274107599094,
      "grad_norm": 4.236691474914551,
      "learning_rate": 4.187060491033409e-05,
      "loss": 0.7091,
      "step": 1069100
    },
    {
      "epoch": 9.75618658296226,
      "grad_norm": 3.5051474571228027,
      "learning_rate": 4.186984451419812e-05,
      "loss": 0.7154,
      "step": 1069200
    },
    {
      "epoch": 9.757099058325425,
      "grad_norm": 3.14882493019104,
      "learning_rate": 4.186908411806215e-05,
      "loss": 0.6773,
      "step": 1069300
    },
    {
      "epoch": 9.75801153368859,
      "grad_norm": 4.103671073913574,
      "learning_rate": 4.186832372192618e-05,
      "loss": 0.6826,
      "step": 1069400
    },
    {
      "epoch": 9.758924009051755,
      "grad_norm": 4.309864044189453,
      "learning_rate": 4.186756332579021e-05,
      "loss": 0.7132,
      "step": 1069500
    },
    {
      "epoch": 9.75983648441492,
      "grad_norm": 3.999403238296509,
      "learning_rate": 4.186680292965424e-05,
      "loss": 0.6705,
      "step": 1069600
    },
    {
      "epoch": 9.760748959778086,
      "grad_norm": 3.775251626968384,
      "learning_rate": 4.186604253351826e-05,
      "loss": 0.6348,
      "step": 1069700
    },
    {
      "epoch": 9.761661435141251,
      "grad_norm": 4.288086414337158,
      "learning_rate": 4.18652821373823e-05,
      "loss": 0.6966,
      "step": 1069800
    },
    {
      "epoch": 9.762573910504416,
      "grad_norm": 4.032319068908691,
      "learning_rate": 4.186452174124632e-05,
      "loss": 0.7402,
      "step": 1069900
    },
    {
      "epoch": 9.763486385867582,
      "grad_norm": 4.648684501647949,
      "learning_rate": 4.186376134511035e-05,
      "loss": 0.708,
      "step": 1070000
    },
    {
      "epoch": 9.764398861230747,
      "grad_norm": 2.190603256225586,
      "learning_rate": 4.186300094897438e-05,
      "loss": 0.703,
      "step": 1070100
    },
    {
      "epoch": 9.765311336593912,
      "grad_norm": 4.0288825035095215,
      "learning_rate": 4.186224055283841e-05,
      "loss": 0.6915,
      "step": 1070200
    },
    {
      "epoch": 9.766223811957078,
      "grad_norm": 4.220302104949951,
      "learning_rate": 4.1861480156702434e-05,
      "loss": 0.6636,
      "step": 1070300
    },
    {
      "epoch": 9.767136287320243,
      "grad_norm": 3.6990349292755127,
      "learning_rate": 4.186071976056647e-05,
      "loss": 0.6618,
      "step": 1070400
    },
    {
      "epoch": 9.768048762683408,
      "grad_norm": 3.287961721420288,
      "learning_rate": 4.1859959364430495e-05,
      "loss": 0.6944,
      "step": 1070500
    },
    {
      "epoch": 9.768961238046574,
      "grad_norm": 4.083730220794678,
      "learning_rate": 4.1859198968294525e-05,
      "loss": 0.6739,
      "step": 1070600
    },
    {
      "epoch": 9.769873713409737,
      "grad_norm": 2.6838536262512207,
      "learning_rate": 4.1858438572158555e-05,
      "loss": 0.6908,
      "step": 1070700
    },
    {
      "epoch": 9.770786188772902,
      "grad_norm": 4.730576038360596,
      "learning_rate": 4.185767817602258e-05,
      "loss": 0.7103,
      "step": 1070800
    },
    {
      "epoch": 9.771698664136068,
      "grad_norm": 3.830930233001709,
      "learning_rate": 4.1856917779886615e-05,
      "loss": 0.6831,
      "step": 1070900
    },
    {
      "epoch": 9.772611139499233,
      "grad_norm": 2.629645586013794,
      "learning_rate": 4.185615738375064e-05,
      "loss": 0.6832,
      "step": 1071000
    },
    {
      "epoch": 9.773523614862398,
      "grad_norm": 4.555361270904541,
      "learning_rate": 4.185539698761467e-05,
      "loss": 0.7151,
      "step": 1071100
    },
    {
      "epoch": 9.774436090225564,
      "grad_norm": 3.3087306022644043,
      "learning_rate": 4.18546365914787e-05,
      "loss": 0.6873,
      "step": 1071200
    },
    {
      "epoch": 9.775348565588729,
      "grad_norm": 4.264929294586182,
      "learning_rate": 4.185387619534273e-05,
      "loss": 0.6781,
      "step": 1071300
    },
    {
      "epoch": 9.776261040951894,
      "grad_norm": 4.27105188369751,
      "learning_rate": 4.185311579920675e-05,
      "loss": 0.6921,
      "step": 1071400
    },
    {
      "epoch": 9.77717351631506,
      "grad_norm": 3.347308397293091,
      "learning_rate": 4.185235540307079e-05,
      "loss": 0.6809,
      "step": 1071500
    },
    {
      "epoch": 9.778085991678225,
      "grad_norm": 4.46964693069458,
      "learning_rate": 4.185159500693481e-05,
      "loss": 0.713,
      "step": 1071600
    },
    {
      "epoch": 9.77899846704139,
      "grad_norm": 4.4622111320495605,
      "learning_rate": 4.185083461079884e-05,
      "loss": 0.7255,
      "step": 1071700
    },
    {
      "epoch": 9.779910942404555,
      "grad_norm": 3.097506046295166,
      "learning_rate": 4.185007421466287e-05,
      "loss": 0.7227,
      "step": 1071800
    },
    {
      "epoch": 9.78082341776772,
      "grad_norm": 4.088296890258789,
      "learning_rate": 4.18493138185269e-05,
      "loss": 0.7057,
      "step": 1071900
    },
    {
      "epoch": 9.781735893130886,
      "grad_norm": 4.35189151763916,
      "learning_rate": 4.184855342239093e-05,
      "loss": 0.6975,
      "step": 1072000
    },
    {
      "epoch": 9.782648368494051,
      "grad_norm": 4.714570045471191,
      "learning_rate": 4.184779302625496e-05,
      "loss": 0.6775,
      "step": 1072100
    },
    {
      "epoch": 9.783560843857217,
      "grad_norm": 3.7683706283569336,
      "learning_rate": 4.1847032630118985e-05,
      "loss": 0.707,
      "step": 1072200
    },
    {
      "epoch": 9.784473319220382,
      "grad_norm": 3.5355465412139893,
      "learning_rate": 4.184627223398302e-05,
      "loss": 0.6956,
      "step": 1072300
    },
    {
      "epoch": 9.785385794583547,
      "grad_norm": 4.836661338806152,
      "learning_rate": 4.1845511837847046e-05,
      "loss": 0.7016,
      "step": 1072400
    },
    {
      "epoch": 9.78629826994671,
      "grad_norm": 3.774085521697998,
      "learning_rate": 4.1844751441711076e-05,
      "loss": 0.715,
      "step": 1072500
    },
    {
      "epoch": 9.787210745309876,
      "grad_norm": 3.9787843227386475,
      "learning_rate": 4.1843991045575106e-05,
      "loss": 0.6766,
      "step": 1072600
    },
    {
      "epoch": 9.788123220673041,
      "grad_norm": 5.333104610443115,
      "learning_rate": 4.1843230649439136e-05,
      "loss": 0.6874,
      "step": 1072700
    },
    {
      "epoch": 9.789035696036207,
      "grad_norm": 4.477747917175293,
      "learning_rate": 4.184247025330316e-05,
      "loss": 0.6723,
      "step": 1072800
    },
    {
      "epoch": 9.789948171399372,
      "grad_norm": 3.032205581665039,
      "learning_rate": 4.1841709857167196e-05,
      "loss": 0.683,
      "step": 1072900
    },
    {
      "epoch": 9.790860646762537,
      "grad_norm": 4.473459243774414,
      "learning_rate": 4.184094946103122e-05,
      "loss": 0.7047,
      "step": 1073000
    },
    {
      "epoch": 9.791773122125702,
      "grad_norm": 3.8464105129241943,
      "learning_rate": 4.184018906489525e-05,
      "loss": 0.6659,
      "step": 1073100
    },
    {
      "epoch": 9.792685597488868,
      "grad_norm": 4.495197772979736,
      "learning_rate": 4.183942866875928e-05,
      "loss": 0.6858,
      "step": 1073200
    },
    {
      "epoch": 9.793598072852033,
      "grad_norm": 4.659499645233154,
      "learning_rate": 4.183866827262331e-05,
      "loss": 0.6832,
      "step": 1073300
    },
    {
      "epoch": 9.794510548215198,
      "grad_norm": 4.937007904052734,
      "learning_rate": 4.183790787648734e-05,
      "loss": 0.7056,
      "step": 1073400
    },
    {
      "epoch": 9.795423023578364,
      "grad_norm": 5.356139659881592,
      "learning_rate": 4.183714748035137e-05,
      "loss": 0.705,
      "step": 1073500
    },
    {
      "epoch": 9.796335498941529,
      "grad_norm": 4.171779632568359,
      "learning_rate": 4.183638708421539e-05,
      "loss": 0.733,
      "step": 1073600
    },
    {
      "epoch": 9.797247974304694,
      "grad_norm": 3.81846284866333,
      "learning_rate": 4.183562668807942e-05,
      "loss": 0.6929,
      "step": 1073700
    },
    {
      "epoch": 9.79816044966786,
      "grad_norm": 3.701054334640503,
      "learning_rate": 4.183486629194345e-05,
      "loss": 0.671,
      "step": 1073800
    },
    {
      "epoch": 9.799072925031025,
      "grad_norm": 3.5856521129608154,
      "learning_rate": 4.1834105895807476e-05,
      "loss": 0.676,
      "step": 1073900
    },
    {
      "epoch": 9.799985400394188,
      "grad_norm": 4.248843669891357,
      "learning_rate": 4.183334549967151e-05,
      "loss": 0.7206,
      "step": 1074000
    },
    {
      "epoch": 9.800897875757354,
      "grad_norm": 4.307608604431152,
      "learning_rate": 4.1832585103535536e-05,
      "loss": 0.6774,
      "step": 1074100
    },
    {
      "epoch": 9.801810351120519,
      "grad_norm": 2.4320456981658936,
      "learning_rate": 4.1831824707399566e-05,
      "loss": 0.6914,
      "step": 1074200
    },
    {
      "epoch": 9.802722826483684,
      "grad_norm": 3.8462095260620117,
      "learning_rate": 4.1831064311263596e-05,
      "loss": 0.7282,
      "step": 1074300
    },
    {
      "epoch": 9.80363530184685,
      "grad_norm": 4.869100093841553,
      "learning_rate": 4.1830303915127627e-05,
      "loss": 0.706,
      "step": 1074400
    },
    {
      "epoch": 9.804547777210015,
      "grad_norm": 3.9670910835266113,
      "learning_rate": 4.182954351899166e-05,
      "loss": 0.6863,
      "step": 1074500
    },
    {
      "epoch": 9.80546025257318,
      "grad_norm": 3.047347068786621,
      "learning_rate": 4.182878312285569e-05,
      "loss": 0.7064,
      "step": 1074600
    },
    {
      "epoch": 9.806372727936346,
      "grad_norm": 3.7152090072631836,
      "learning_rate": 4.182802272671971e-05,
      "loss": 0.7149,
      "step": 1074700
    },
    {
      "epoch": 9.80728520329951,
      "grad_norm": 4.340177536010742,
      "learning_rate": 4.182726233058375e-05,
      "loss": 0.6938,
      "step": 1074800
    },
    {
      "epoch": 9.808197678662676,
      "grad_norm": 4.365852355957031,
      "learning_rate": 4.182650193444777e-05,
      "loss": 0.7038,
      "step": 1074900
    },
    {
      "epoch": 9.809110154025841,
      "grad_norm": 3.845046043395996,
      "learning_rate": 4.18257415383118e-05,
      "loss": 0.6976,
      "step": 1075000
    },
    {
      "epoch": 9.810022629389007,
      "grad_norm": 2.691045045852661,
      "learning_rate": 4.182498114217583e-05,
      "loss": 0.6581,
      "step": 1075100
    },
    {
      "epoch": 9.810935104752172,
      "grad_norm": 3.557662010192871,
      "learning_rate": 4.182422074603986e-05,
      "loss": 0.6656,
      "step": 1075200
    },
    {
      "epoch": 9.811847580115337,
      "grad_norm": 4.238551616668701,
      "learning_rate": 4.1823460349903884e-05,
      "loss": 0.6733,
      "step": 1075300
    },
    {
      "epoch": 9.812760055478503,
      "grad_norm": 4.070290565490723,
      "learning_rate": 4.182269995376792e-05,
      "loss": 0.7182,
      "step": 1075400
    },
    {
      "epoch": 9.813672530841668,
      "grad_norm": 4.24527645111084,
      "learning_rate": 4.1821939557631944e-05,
      "loss": 0.6706,
      "step": 1075500
    },
    {
      "epoch": 9.814585006204833,
      "grad_norm": 3.646772861480713,
      "learning_rate": 4.1821179161495974e-05,
      "loss": 0.6809,
      "step": 1075600
    },
    {
      "epoch": 9.815497481567999,
      "grad_norm": 4.201425552368164,
      "learning_rate": 4.1820418765360004e-05,
      "loss": 0.7017,
      "step": 1075700
    },
    {
      "epoch": 9.816409956931162,
      "grad_norm": 4.008927822113037,
      "learning_rate": 4.1819658369224034e-05,
      "loss": 0.7108,
      "step": 1075800
    },
    {
      "epoch": 9.817322432294327,
      "grad_norm": 4.349188804626465,
      "learning_rate": 4.1818897973088064e-05,
      "loss": 0.6843,
      "step": 1075900
    },
    {
      "epoch": 9.818234907657493,
      "grad_norm": 3.7651360034942627,
      "learning_rate": 4.1818137576952094e-05,
      "loss": 0.6583,
      "step": 1076000
    },
    {
      "epoch": 9.819147383020658,
      "grad_norm": 4.020369529724121,
      "learning_rate": 4.181737718081612e-05,
      "loss": 0.668,
      "step": 1076100
    },
    {
      "epoch": 9.820059858383823,
      "grad_norm": 3.600757360458374,
      "learning_rate": 4.1816616784680154e-05,
      "loss": 0.6597,
      "step": 1076200
    },
    {
      "epoch": 9.820972333746989,
      "grad_norm": 4.261402606964111,
      "learning_rate": 4.181585638854418e-05,
      "loss": 0.6679,
      "step": 1076300
    },
    {
      "epoch": 9.821884809110154,
      "grad_norm": 4.175405979156494,
      "learning_rate": 4.181509599240821e-05,
      "loss": 0.6676,
      "step": 1076400
    },
    {
      "epoch": 9.82279728447332,
      "grad_norm": 3.7964932918548584,
      "learning_rate": 4.181433559627224e-05,
      "loss": 0.6812,
      "step": 1076500
    },
    {
      "epoch": 9.823709759836484,
      "grad_norm": 3.5396568775177,
      "learning_rate": 4.181357520013626e-05,
      "loss": 0.6965,
      "step": 1076600
    },
    {
      "epoch": 9.82462223519965,
      "grad_norm": 2.9596166610717773,
      "learning_rate": 4.181281480400029e-05,
      "loss": 0.6637,
      "step": 1076700
    },
    {
      "epoch": 9.825534710562815,
      "grad_norm": 4.009225845336914,
      "learning_rate": 4.181205440786432e-05,
      "loss": 0.6754,
      "step": 1076800
    },
    {
      "epoch": 9.82644718592598,
      "grad_norm": 3.543701410293579,
      "learning_rate": 4.181129401172835e-05,
      "loss": 0.7389,
      "step": 1076900
    },
    {
      "epoch": 9.827359661289146,
      "grad_norm": 3.7933387756347656,
      "learning_rate": 4.181053361559238e-05,
      "loss": 0.6789,
      "step": 1077000
    },
    {
      "epoch": 9.828272136652311,
      "grad_norm": 4.033106327056885,
      "learning_rate": 4.180977321945641e-05,
      "loss": 0.7032,
      "step": 1077100
    },
    {
      "epoch": 9.829184612015476,
      "grad_norm": 4.440486907958984,
      "learning_rate": 4.1809012823320435e-05,
      "loss": 0.7009,
      "step": 1077200
    },
    {
      "epoch": 9.830097087378642,
      "grad_norm": 4.483726978302002,
      "learning_rate": 4.180825242718447e-05,
      "loss": 0.6957,
      "step": 1077300
    },
    {
      "epoch": 9.831009562741805,
      "grad_norm": 4.532613277435303,
      "learning_rate": 4.1807492031048495e-05,
      "loss": 0.6637,
      "step": 1077400
    },
    {
      "epoch": 9.83192203810497,
      "grad_norm": 4.516092777252197,
      "learning_rate": 4.1806731634912525e-05,
      "loss": 0.6774,
      "step": 1077500
    },
    {
      "epoch": 9.832834513468136,
      "grad_norm": 3.674896240234375,
      "learning_rate": 4.1805971238776555e-05,
      "loss": 0.6804,
      "step": 1077600
    },
    {
      "epoch": 9.833746988831301,
      "grad_norm": 4.666006565093994,
      "learning_rate": 4.1805210842640585e-05,
      "loss": 0.6697,
      "step": 1077700
    },
    {
      "epoch": 9.834659464194466,
      "grad_norm": 3.386629581451416,
      "learning_rate": 4.1804450446504615e-05,
      "loss": 0.6669,
      "step": 1077800
    },
    {
      "epoch": 9.835571939557632,
      "grad_norm": 3.922884941101074,
      "learning_rate": 4.1803690050368645e-05,
      "loss": 0.6762,
      "step": 1077900
    },
    {
      "epoch": 9.836484414920797,
      "grad_norm": 4.057193279266357,
      "learning_rate": 4.180292965423267e-05,
      "loss": 0.664,
      "step": 1078000
    },
    {
      "epoch": 9.837396890283962,
      "grad_norm": 4.614736557006836,
      "learning_rate": 4.18021692580967e-05,
      "loss": 0.6504,
      "step": 1078100
    },
    {
      "epoch": 9.838309365647127,
      "grad_norm": 3.8318333625793457,
      "learning_rate": 4.180140886196073e-05,
      "loss": 0.6869,
      "step": 1078200
    },
    {
      "epoch": 9.839221841010293,
      "grad_norm": 4.725569725036621,
      "learning_rate": 4.180064846582476e-05,
      "loss": 0.7064,
      "step": 1078300
    },
    {
      "epoch": 9.840134316373458,
      "grad_norm": 4.434678077697754,
      "learning_rate": 4.179988806968879e-05,
      "loss": 0.6982,
      "step": 1078400
    },
    {
      "epoch": 9.841046791736623,
      "grad_norm": 3.9052178859710693,
      "learning_rate": 4.179912767355282e-05,
      "loss": 0.6926,
      "step": 1078500
    },
    {
      "epoch": 9.841959267099789,
      "grad_norm": 3.0446274280548096,
      "learning_rate": 4.179836727741684e-05,
      "loss": 0.6929,
      "step": 1078600
    },
    {
      "epoch": 9.842871742462954,
      "grad_norm": 4.953341960906982,
      "learning_rate": 4.179760688128088e-05,
      "loss": 0.6842,
      "step": 1078700
    },
    {
      "epoch": 9.84378421782612,
      "grad_norm": 4.305928707122803,
      "learning_rate": 4.17968464851449e-05,
      "loss": 0.7085,
      "step": 1078800
    },
    {
      "epoch": 9.844696693189285,
      "grad_norm": 2.816901206970215,
      "learning_rate": 4.179608608900893e-05,
      "loss": 0.7047,
      "step": 1078900
    },
    {
      "epoch": 9.84560916855245,
      "grad_norm": 3.7608821392059326,
      "learning_rate": 4.179532569287296e-05,
      "loss": 0.6572,
      "step": 1079000
    },
    {
      "epoch": 9.846521643915615,
      "grad_norm": 3.5631749629974365,
      "learning_rate": 4.179456529673699e-05,
      "loss": 0.662,
      "step": 1079100
    },
    {
      "epoch": 9.847434119278779,
      "grad_norm": 3.7217915058135986,
      "learning_rate": 4.179380490060102e-05,
      "loss": 0.6984,
      "step": 1079200
    },
    {
      "epoch": 9.848346594641944,
      "grad_norm": 3.0720512866973877,
      "learning_rate": 4.1793044504465046e-05,
      "loss": 0.7199,
      "step": 1079300
    },
    {
      "epoch": 9.84925907000511,
      "grad_norm": 3.695255756378174,
      "learning_rate": 4.1792284108329076e-05,
      "loss": 0.6918,
      "step": 1079400
    },
    {
      "epoch": 9.850171545368275,
      "grad_norm": 3.5363595485687256,
      "learning_rate": 4.1791523712193106e-05,
      "loss": 0.6692,
      "step": 1079500
    },
    {
      "epoch": 9.85108402073144,
      "grad_norm": 3.6338582038879395,
      "learning_rate": 4.1790763316057136e-05,
      "loss": 0.6936,
      "step": 1079600
    },
    {
      "epoch": 9.851996496094605,
      "grad_norm": 3.4482288360595703,
      "learning_rate": 4.179000291992116e-05,
      "loss": 0.7136,
      "step": 1079700
    },
    {
      "epoch": 9.85290897145777,
      "grad_norm": 4.164356708526611,
      "learning_rate": 4.1789242523785196e-05,
      "loss": 0.6827,
      "step": 1079800
    },
    {
      "epoch": 9.853821446820936,
      "grad_norm": 4.853909492492676,
      "learning_rate": 4.178848212764922e-05,
      "loss": 0.6826,
      "step": 1079900
    },
    {
      "epoch": 9.854733922184101,
      "grad_norm": 3.7776477336883545,
      "learning_rate": 4.178772173151325e-05,
      "loss": 0.6926,
      "step": 1080000
    },
    {
      "epoch": 9.855646397547266,
      "grad_norm": 4.476935863494873,
      "learning_rate": 4.178696133537728e-05,
      "loss": 0.6797,
      "step": 1080100
    },
    {
      "epoch": 9.856558872910432,
      "grad_norm": 4.593636989593506,
      "learning_rate": 4.178620093924131e-05,
      "loss": 0.6837,
      "step": 1080200
    },
    {
      "epoch": 9.857471348273597,
      "grad_norm": 4.043949604034424,
      "learning_rate": 4.178544054310534e-05,
      "loss": 0.6592,
      "step": 1080300
    },
    {
      "epoch": 9.858383823636762,
      "grad_norm": 4.319063663482666,
      "learning_rate": 4.178468014696937e-05,
      "loss": 0.6895,
      "step": 1080400
    },
    {
      "epoch": 9.859296298999928,
      "grad_norm": 3.3835883140563965,
      "learning_rate": 4.178391975083339e-05,
      "loss": 0.6756,
      "step": 1080500
    },
    {
      "epoch": 9.860208774363093,
      "grad_norm": 3.8298180103302,
      "learning_rate": 4.178315935469743e-05,
      "loss": 0.6906,
      "step": 1080600
    },
    {
      "epoch": 9.861121249726258,
      "grad_norm": 3.656163454055786,
      "learning_rate": 4.178239895856145e-05,
      "loss": 0.6753,
      "step": 1080700
    },
    {
      "epoch": 9.862033725089422,
      "grad_norm": 4.362341403961182,
      "learning_rate": 4.178163856242548e-05,
      "loss": 0.6907,
      "step": 1080800
    },
    {
      "epoch": 9.862946200452587,
      "grad_norm": 4.376436710357666,
      "learning_rate": 4.178087816628951e-05,
      "loss": 0.6649,
      "step": 1080900
    },
    {
      "epoch": 9.863858675815752,
      "grad_norm": 3.990239143371582,
      "learning_rate": 4.178011777015354e-05,
      "loss": 0.7042,
      "step": 1081000
    },
    {
      "epoch": 9.864771151178918,
      "grad_norm": 4.455503463745117,
      "learning_rate": 4.1779357374017567e-05,
      "loss": 0.6881,
      "step": 1081100
    },
    {
      "epoch": 9.865683626542083,
      "grad_norm": 3.691786289215088,
      "learning_rate": 4.17785969778816e-05,
      "loss": 0.6766,
      "step": 1081200
    },
    {
      "epoch": 9.866596101905248,
      "grad_norm": 4.2812180519104,
      "learning_rate": 4.177783658174563e-05,
      "loss": 0.7374,
      "step": 1081300
    },
    {
      "epoch": 9.867508577268413,
      "grad_norm": 3.307929277420044,
      "learning_rate": 4.177707618560966e-05,
      "loss": 0.6988,
      "step": 1081400
    },
    {
      "epoch": 9.868421052631579,
      "grad_norm": 4.2818145751953125,
      "learning_rate": 4.177631578947369e-05,
      "loss": 0.7123,
      "step": 1081500
    },
    {
      "epoch": 9.869333527994744,
      "grad_norm": 4.513278961181641,
      "learning_rate": 4.177555539333772e-05,
      "loss": 0.6732,
      "step": 1081600
    },
    {
      "epoch": 9.87024600335791,
      "grad_norm": 4.131185054779053,
      "learning_rate": 4.177479499720175e-05,
      "loss": 0.7223,
      "step": 1081700
    },
    {
      "epoch": 9.871158478721075,
      "grad_norm": 2.6593916416168213,
      "learning_rate": 4.177403460106578e-05,
      "loss": 0.6598,
      "step": 1081800
    },
    {
      "epoch": 9.87207095408424,
      "grad_norm": 3.519040107727051,
      "learning_rate": 4.17732742049298e-05,
      "loss": 0.6558,
      "step": 1081900
    },
    {
      "epoch": 9.872983429447405,
      "grad_norm": 3.612779140472412,
      "learning_rate": 4.177251380879384e-05,
      "loss": 0.6889,
      "step": 1082000
    },
    {
      "epoch": 9.87389590481057,
      "grad_norm": 4.206757545471191,
      "learning_rate": 4.177175341265786e-05,
      "loss": 0.6782,
      "step": 1082100
    },
    {
      "epoch": 9.874808380173736,
      "grad_norm": 4.057595729827881,
      "learning_rate": 4.1770993016521884e-05,
      "loss": 0.6701,
      "step": 1082200
    },
    {
      "epoch": 9.875720855536901,
      "grad_norm": 3.8856046199798584,
      "learning_rate": 4.177023262038592e-05,
      "loss": 0.6872,
      "step": 1082300
    },
    {
      "epoch": 9.876633330900066,
      "grad_norm": 3.956207513809204,
      "learning_rate": 4.1769472224249944e-05,
      "loss": 0.6627,
      "step": 1082400
    },
    {
      "epoch": 9.877545806263232,
      "grad_norm": 3.5118513107299805,
      "learning_rate": 4.1768711828113974e-05,
      "loss": 0.686,
      "step": 1082500
    },
    {
      "epoch": 9.878458281626395,
      "grad_norm": 3.9782936573028564,
      "learning_rate": 4.1767951431978004e-05,
      "loss": 0.6974,
      "step": 1082600
    },
    {
      "epoch": 9.87937075698956,
      "grad_norm": 3.7906932830810547,
      "learning_rate": 4.1767191035842034e-05,
      "loss": 0.6982,
      "step": 1082700
    },
    {
      "epoch": 9.880283232352726,
      "grad_norm": 4.817989826202393,
      "learning_rate": 4.1766430639706064e-05,
      "loss": 0.7062,
      "step": 1082800
    },
    {
      "epoch": 9.881195707715891,
      "grad_norm": 4.199226379394531,
      "learning_rate": 4.1765670243570094e-05,
      "loss": 0.7089,
      "step": 1082900
    },
    {
      "epoch": 9.882108183079056,
      "grad_norm": 3.7058656215667725,
      "learning_rate": 4.176490984743412e-05,
      "loss": 0.6629,
      "step": 1083000
    },
    {
      "epoch": 9.883020658442222,
      "grad_norm": 4.20803689956665,
      "learning_rate": 4.1764149451298154e-05,
      "loss": 0.6719,
      "step": 1083100
    },
    {
      "epoch": 9.883933133805387,
      "grad_norm": 4.193877220153809,
      "learning_rate": 4.176338905516218e-05,
      "loss": 0.6997,
      "step": 1083200
    },
    {
      "epoch": 9.884845609168552,
      "grad_norm": 3.838634490966797,
      "learning_rate": 4.176262865902621e-05,
      "loss": 0.6876,
      "step": 1083300
    },
    {
      "epoch": 9.885758084531718,
      "grad_norm": 4.541000843048096,
      "learning_rate": 4.176186826289024e-05,
      "loss": 0.7148,
      "step": 1083400
    },
    {
      "epoch": 9.886670559894883,
      "grad_norm": 4.327784061431885,
      "learning_rate": 4.176110786675427e-05,
      "loss": 0.6965,
      "step": 1083500
    },
    {
      "epoch": 9.887583035258048,
      "grad_norm": 4.037759304046631,
      "learning_rate": 4.176034747061829e-05,
      "loss": 0.6996,
      "step": 1083600
    },
    {
      "epoch": 9.888495510621214,
      "grad_norm": 3.5808568000793457,
      "learning_rate": 4.175958707448233e-05,
      "loss": 0.7122,
      "step": 1083700
    },
    {
      "epoch": 9.889407985984379,
      "grad_norm": 4.112850189208984,
      "learning_rate": 4.175882667834635e-05,
      "loss": 0.6854,
      "step": 1083800
    },
    {
      "epoch": 9.890320461347544,
      "grad_norm": 4.533609390258789,
      "learning_rate": 4.175806628221038e-05,
      "loss": 0.7147,
      "step": 1083900
    },
    {
      "epoch": 9.89123293671071,
      "grad_norm": 4.154815673828125,
      "learning_rate": 4.175730588607441e-05,
      "loss": 0.6983,
      "step": 1084000
    },
    {
      "epoch": 9.892145412073875,
      "grad_norm": 4.079634189605713,
      "learning_rate": 4.175654548993844e-05,
      "loss": 0.6791,
      "step": 1084100
    },
    {
      "epoch": 9.893057887437038,
      "grad_norm": 4.862916469573975,
      "learning_rate": 4.175578509380247e-05,
      "loss": 0.702,
      "step": 1084200
    },
    {
      "epoch": 9.893970362800204,
      "grad_norm": 3.1713552474975586,
      "learning_rate": 4.17550246976665e-05,
      "loss": 0.7043,
      "step": 1084300
    },
    {
      "epoch": 9.894882838163369,
      "grad_norm": 4.301878929138184,
      "learning_rate": 4.1754264301530525e-05,
      "loss": 0.6478,
      "step": 1084400
    },
    {
      "epoch": 9.895795313526534,
      "grad_norm": 3.9492499828338623,
      "learning_rate": 4.175350390539456e-05,
      "loss": 0.7329,
      "step": 1084500
    },
    {
      "epoch": 9.8967077888897,
      "grad_norm": 4.099228858947754,
      "learning_rate": 4.1752743509258585e-05,
      "loss": 0.7095,
      "step": 1084600
    },
    {
      "epoch": 9.897620264252865,
      "grad_norm": 3.5460424423217773,
      "learning_rate": 4.1751983113122615e-05,
      "loss": 0.681,
      "step": 1084700
    },
    {
      "epoch": 9.89853273961603,
      "grad_norm": 3.620833158493042,
      "learning_rate": 4.1751222716986645e-05,
      "loss": 0.6676,
      "step": 1084800
    },
    {
      "epoch": 9.899445214979195,
      "grad_norm": 2.871201992034912,
      "learning_rate": 4.1750462320850675e-05,
      "loss": 0.654,
      "step": 1084900
    },
    {
      "epoch": 9.90035769034236,
      "grad_norm": 4.50564432144165,
      "learning_rate": 4.17497019247147e-05,
      "loss": 0.7132,
      "step": 1085000
    },
    {
      "epoch": 9.901270165705526,
      "grad_norm": 3.315309762954712,
      "learning_rate": 4.174894152857873e-05,
      "loss": 0.6691,
      "step": 1085100
    },
    {
      "epoch": 9.902182641068691,
      "grad_norm": 4.223397254943848,
      "learning_rate": 4.174818113244276e-05,
      "loss": 0.6996,
      "step": 1085200
    },
    {
      "epoch": 9.903095116431857,
      "grad_norm": 2.8709983825683594,
      "learning_rate": 4.174742073630679e-05,
      "loss": 0.6881,
      "step": 1085300
    },
    {
      "epoch": 9.904007591795022,
      "grad_norm": 4.607710361480713,
      "learning_rate": 4.174666034017082e-05,
      "loss": 0.6484,
      "step": 1085400
    },
    {
      "epoch": 9.904920067158187,
      "grad_norm": 4.76191520690918,
      "learning_rate": 4.174589994403484e-05,
      "loss": 0.6932,
      "step": 1085500
    },
    {
      "epoch": 9.905832542521352,
      "grad_norm": 4.637314796447754,
      "learning_rate": 4.174513954789888e-05,
      "loss": 0.6615,
      "step": 1085600
    },
    {
      "epoch": 9.906745017884518,
      "grad_norm": 3.862023115158081,
      "learning_rate": 4.17443791517629e-05,
      "loss": 0.6496,
      "step": 1085700
    },
    {
      "epoch": 9.907657493247683,
      "grad_norm": 2.8924710750579834,
      "learning_rate": 4.174361875562693e-05,
      "loss": 0.6914,
      "step": 1085800
    },
    {
      "epoch": 9.908569968610848,
      "grad_norm": 3.519364833831787,
      "learning_rate": 4.174285835949096e-05,
      "loss": 0.7093,
      "step": 1085900
    },
    {
      "epoch": 9.909482443974012,
      "grad_norm": 4.1272993087768555,
      "learning_rate": 4.174209796335499e-05,
      "loss": 0.6953,
      "step": 1086000
    },
    {
      "epoch": 9.910394919337177,
      "grad_norm": 4.56439733505249,
      "learning_rate": 4.1741337567219016e-05,
      "loss": 0.7124,
      "step": 1086100
    },
    {
      "epoch": 9.911307394700342,
      "grad_norm": 5.008925914764404,
      "learning_rate": 4.174057717108305e-05,
      "loss": 0.6996,
      "step": 1086200
    },
    {
      "epoch": 9.912219870063508,
      "grad_norm": 4.011565685272217,
      "learning_rate": 4.1739816774947076e-05,
      "loss": 0.6792,
      "step": 1086300
    },
    {
      "epoch": 9.913132345426673,
      "grad_norm": 3.9535489082336426,
      "learning_rate": 4.1739056378811106e-05,
      "loss": 0.6729,
      "step": 1086400
    },
    {
      "epoch": 9.914044820789838,
      "grad_norm": 18.768686294555664,
      "learning_rate": 4.1738295982675136e-05,
      "loss": 0.7149,
      "step": 1086500
    },
    {
      "epoch": 9.914957296153004,
      "grad_norm": 4.185816287994385,
      "learning_rate": 4.1737535586539166e-05,
      "loss": 0.668,
      "step": 1086600
    },
    {
      "epoch": 9.915869771516169,
      "grad_norm": 4.570326328277588,
      "learning_rate": 4.1736775190403196e-05,
      "loss": 0.7276,
      "step": 1086700
    },
    {
      "epoch": 9.916782246879334,
      "grad_norm": 3.9911105632781982,
      "learning_rate": 4.1736014794267226e-05,
      "loss": 0.7111,
      "step": 1086800
    },
    {
      "epoch": 9.9176947222425,
      "grad_norm": 3.8183393478393555,
      "learning_rate": 4.173525439813125e-05,
      "loss": 0.6862,
      "step": 1086900
    },
    {
      "epoch": 9.918607197605665,
      "grad_norm": 3.939756155014038,
      "learning_rate": 4.1734494001995286e-05,
      "loss": 0.6405,
      "step": 1087000
    },
    {
      "epoch": 9.91951967296883,
      "grad_norm": 4.637406826019287,
      "learning_rate": 4.173373360585931e-05,
      "loss": 0.64,
      "step": 1087100
    },
    {
      "epoch": 9.920432148331995,
      "grad_norm": 3.049759864807129,
      "learning_rate": 4.173297320972334e-05,
      "loss": 0.6502,
      "step": 1087200
    },
    {
      "epoch": 9.92134462369516,
      "grad_norm": 3.9197216033935547,
      "learning_rate": 4.173221281358737e-05,
      "loss": 0.65,
      "step": 1087300
    },
    {
      "epoch": 9.922257099058326,
      "grad_norm": 3.5783610343933105,
      "learning_rate": 4.17314524174514e-05,
      "loss": 0.6567,
      "step": 1087400
    },
    {
      "epoch": 9.923169574421491,
      "grad_norm": 4.015076160430908,
      "learning_rate": 4.173069202131542e-05,
      "loss": 0.6942,
      "step": 1087500
    },
    {
      "epoch": 9.924082049784655,
      "grad_norm": 3.6587953567504883,
      "learning_rate": 4.172993162517946e-05,
      "loss": 0.689,
      "step": 1087600
    },
    {
      "epoch": 9.92499452514782,
      "grad_norm": 3.2867300510406494,
      "learning_rate": 4.172917122904348e-05,
      "loss": 0.6836,
      "step": 1087700
    },
    {
      "epoch": 9.925907000510986,
      "grad_norm": 4.322750091552734,
      "learning_rate": 4.172841083290751e-05,
      "loss": 0.6775,
      "step": 1087800
    },
    {
      "epoch": 9.92681947587415,
      "grad_norm": 3.8682947158813477,
      "learning_rate": 4.172765043677154e-05,
      "loss": 0.666,
      "step": 1087900
    },
    {
      "epoch": 9.927731951237316,
      "grad_norm": 2.643592119216919,
      "learning_rate": 4.172689004063557e-05,
      "loss": 0.6843,
      "step": 1088000
    },
    {
      "epoch": 9.928644426600481,
      "grad_norm": 3.6793460845947266,
      "learning_rate": 4.1726129644499603e-05,
      "loss": 0.7151,
      "step": 1088100
    },
    {
      "epoch": 9.929556901963647,
      "grad_norm": 4.723134994506836,
      "learning_rate": 4.172536924836363e-05,
      "loss": 0.6437,
      "step": 1088200
    },
    {
      "epoch": 9.930469377326812,
      "grad_norm": 5.10726261138916,
      "learning_rate": 4.172460885222766e-05,
      "loss": 0.6736,
      "step": 1088300
    },
    {
      "epoch": 9.931381852689977,
      "grad_norm": 4.632194519042969,
      "learning_rate": 4.172384845609169e-05,
      "loss": 0.741,
      "step": 1088400
    },
    {
      "epoch": 9.932294328053143,
      "grad_norm": 3.5483593940734863,
      "learning_rate": 4.172308805995572e-05,
      "loss": 0.7019,
      "step": 1088500
    },
    {
      "epoch": 9.933206803416308,
      "grad_norm": 3.3958306312561035,
      "learning_rate": 4.172232766381974e-05,
      "loss": 0.721,
      "step": 1088600
    },
    {
      "epoch": 9.934119278779473,
      "grad_norm": 4.384274005889893,
      "learning_rate": 4.172156726768378e-05,
      "loss": 0.6785,
      "step": 1088700
    },
    {
      "epoch": 9.935031754142639,
      "grad_norm": 3.9057810306549072,
      "learning_rate": 4.17208068715478e-05,
      "loss": 0.713,
      "step": 1088800
    },
    {
      "epoch": 9.935944229505804,
      "grad_norm": 4.1857404708862305,
      "learning_rate": 4.172004647541183e-05,
      "loss": 0.6823,
      "step": 1088900
    },
    {
      "epoch": 9.936856704868969,
      "grad_norm": 4.099109172821045,
      "learning_rate": 4.171928607927586e-05,
      "loss": 0.6828,
      "step": 1089000
    },
    {
      "epoch": 9.937769180232134,
      "grad_norm": 4.265744209289551,
      "learning_rate": 4.171852568313989e-05,
      "loss": 0.6976,
      "step": 1089100
    },
    {
      "epoch": 9.9386816555953,
      "grad_norm": 4.495561122894287,
      "learning_rate": 4.171776528700392e-05,
      "loss": 0.6609,
      "step": 1089200
    },
    {
      "epoch": 9.939594130958465,
      "grad_norm": 4.029120922088623,
      "learning_rate": 4.171700489086795e-05,
      "loss": 0.6658,
      "step": 1089300
    },
    {
      "epoch": 9.940506606321629,
      "grad_norm": 4.594613075256348,
      "learning_rate": 4.1716244494731974e-05,
      "loss": 0.6623,
      "step": 1089400
    },
    {
      "epoch": 9.941419081684794,
      "grad_norm": 3.842564344406128,
      "learning_rate": 4.171548409859601e-05,
      "loss": 0.6643,
      "step": 1089500
    },
    {
      "epoch": 9.94233155704796,
      "grad_norm": 3.773674964904785,
      "learning_rate": 4.1714723702460034e-05,
      "loss": 0.6552,
      "step": 1089600
    },
    {
      "epoch": 9.943244032411124,
      "grad_norm": 4.446870803833008,
      "learning_rate": 4.1713963306324064e-05,
      "loss": 0.7191,
      "step": 1089700
    },
    {
      "epoch": 9.94415650777429,
      "grad_norm": 4.8916544914245605,
      "learning_rate": 4.1713202910188094e-05,
      "loss": 0.6875,
      "step": 1089800
    },
    {
      "epoch": 9.945068983137455,
      "grad_norm": 4.054086685180664,
      "learning_rate": 4.1712442514052124e-05,
      "loss": 0.6866,
      "step": 1089900
    },
    {
      "epoch": 9.94598145850062,
      "grad_norm": 3.9155263900756836,
      "learning_rate": 4.171168211791615e-05,
      "loss": 0.6744,
      "step": 1090000
    },
    {
      "epoch": 9.946893933863786,
      "grad_norm": 4.255630970001221,
      "learning_rate": 4.1710921721780184e-05,
      "loss": 0.7142,
      "step": 1090100
    },
    {
      "epoch": 9.947806409226951,
      "grad_norm": 3.4772098064422607,
      "learning_rate": 4.171016132564421e-05,
      "loss": 0.6705,
      "step": 1090200
    },
    {
      "epoch": 9.948718884590116,
      "grad_norm": 4.614444255828857,
      "learning_rate": 4.170940092950824e-05,
      "loss": 0.6567,
      "step": 1090300
    },
    {
      "epoch": 9.949631359953282,
      "grad_norm": 3.699232339859009,
      "learning_rate": 4.170864053337227e-05,
      "loss": 0.6482,
      "step": 1090400
    },
    {
      "epoch": 9.950543835316447,
      "grad_norm": 3.447265148162842,
      "learning_rate": 4.17078801372363e-05,
      "loss": 0.6625,
      "step": 1090500
    },
    {
      "epoch": 9.951456310679612,
      "grad_norm": 3.383255958557129,
      "learning_rate": 4.170711974110033e-05,
      "loss": 0.6479,
      "step": 1090600
    },
    {
      "epoch": 9.952368786042777,
      "grad_norm": 3.4827160835266113,
      "learning_rate": 4.170635934496435e-05,
      "loss": 0.6758,
      "step": 1090700
    },
    {
      "epoch": 9.953281261405943,
      "grad_norm": 3.7139720916748047,
      "learning_rate": 4.170559894882838e-05,
      "loss": 0.6893,
      "step": 1090800
    },
    {
      "epoch": 9.954193736769108,
      "grad_norm": 3.779874086380005,
      "learning_rate": 4.170483855269241e-05,
      "loss": 0.6996,
      "step": 1090900
    },
    {
      "epoch": 9.955106212132272,
      "grad_norm": 4.6512298583984375,
      "learning_rate": 4.170407815655644e-05,
      "loss": 0.6747,
      "step": 1091000
    },
    {
      "epoch": 9.956018687495437,
      "grad_norm": 4.583681583404541,
      "learning_rate": 4.170331776042047e-05,
      "loss": 0.6862,
      "step": 1091100
    },
    {
      "epoch": 9.956931162858602,
      "grad_norm": 3.8051950931549072,
      "learning_rate": 4.17025573642845e-05,
      "loss": 0.7007,
      "step": 1091200
    },
    {
      "epoch": 9.957843638221767,
      "grad_norm": 4.0058112144470215,
      "learning_rate": 4.1701796968148525e-05,
      "loss": 0.7083,
      "step": 1091300
    },
    {
      "epoch": 9.958756113584933,
      "grad_norm": 3.725111484527588,
      "learning_rate": 4.170103657201256e-05,
      "loss": 0.7007,
      "step": 1091400
    },
    {
      "epoch": 9.959668588948098,
      "grad_norm": 4.4567437171936035,
      "learning_rate": 4.1700276175876585e-05,
      "loss": 0.6805,
      "step": 1091500
    },
    {
      "epoch": 9.960581064311263,
      "grad_norm": 4.189384460449219,
      "learning_rate": 4.1699515779740615e-05,
      "loss": 0.6998,
      "step": 1091600
    },
    {
      "epoch": 9.961493539674429,
      "grad_norm": 3.6324872970581055,
      "learning_rate": 4.1698755383604645e-05,
      "loss": 0.6559,
      "step": 1091700
    },
    {
      "epoch": 9.962406015037594,
      "grad_norm": 4.571900367736816,
      "learning_rate": 4.1697994987468675e-05,
      "loss": 0.6714,
      "step": 1091800
    },
    {
      "epoch": 9.96331849040076,
      "grad_norm": 4.621914386749268,
      "learning_rate": 4.16972345913327e-05,
      "loss": 0.7121,
      "step": 1091900
    },
    {
      "epoch": 9.964230965763925,
      "grad_norm": 4.500577449798584,
      "learning_rate": 4.1696474195196735e-05,
      "loss": 0.6578,
      "step": 1092000
    },
    {
      "epoch": 9.96514344112709,
      "grad_norm": 4.2316179275512695,
      "learning_rate": 4.169571379906076e-05,
      "loss": 0.6405,
      "step": 1092100
    },
    {
      "epoch": 9.966055916490255,
      "grad_norm": 4.110091209411621,
      "learning_rate": 4.169495340292479e-05,
      "loss": 0.6971,
      "step": 1092200
    },
    {
      "epoch": 9.96696839185342,
      "grad_norm": 3.3607702255249023,
      "learning_rate": 4.169419300678882e-05,
      "loss": 0.6868,
      "step": 1092300
    },
    {
      "epoch": 9.967880867216586,
      "grad_norm": 2.9718070030212402,
      "learning_rate": 4.169343261065285e-05,
      "loss": 0.6577,
      "step": 1092400
    },
    {
      "epoch": 9.968793342579751,
      "grad_norm": 3.679885149002075,
      "learning_rate": 4.169267221451688e-05,
      "loss": 0.6919,
      "step": 1092500
    },
    {
      "epoch": 9.969705817942916,
      "grad_norm": 3.9954614639282227,
      "learning_rate": 4.169191181838091e-05,
      "loss": 0.6549,
      "step": 1092600
    },
    {
      "epoch": 9.970618293306082,
      "grad_norm": 3.9992666244506836,
      "learning_rate": 4.169115142224493e-05,
      "loss": 0.691,
      "step": 1092700
    },
    {
      "epoch": 9.971530768669245,
      "grad_norm": 3.162036895751953,
      "learning_rate": 4.169039102610897e-05,
      "loss": 0.682,
      "step": 1092800
    },
    {
      "epoch": 9.97244324403241,
      "grad_norm": 3.902311086654663,
      "learning_rate": 4.168963062997299e-05,
      "loss": 0.7052,
      "step": 1092900
    },
    {
      "epoch": 9.973355719395576,
      "grad_norm": 3.9927899837493896,
      "learning_rate": 4.168887023383702e-05,
      "loss": 0.7184,
      "step": 1093000
    },
    {
      "epoch": 9.974268194758741,
      "grad_norm": 3.5331716537475586,
      "learning_rate": 4.168810983770105e-05,
      "loss": 0.6622,
      "step": 1093100
    },
    {
      "epoch": 9.975180670121906,
      "grad_norm": 4.205077171325684,
      "learning_rate": 4.168734944156508e-05,
      "loss": 0.6801,
      "step": 1093200
    },
    {
      "epoch": 9.976093145485072,
      "grad_norm": 3.86080265045166,
      "learning_rate": 4.1686589045429106e-05,
      "loss": 0.7156,
      "step": 1093300
    },
    {
      "epoch": 9.977005620848237,
      "grad_norm": 3.912292242050171,
      "learning_rate": 4.168582864929314e-05,
      "loss": 0.7256,
      "step": 1093400
    },
    {
      "epoch": 9.977918096211402,
      "grad_norm": 4.467443466186523,
      "learning_rate": 4.1685068253157166e-05,
      "loss": 0.6958,
      "step": 1093500
    },
    {
      "epoch": 9.978830571574568,
      "grad_norm": 3.5675339698791504,
      "learning_rate": 4.1684307857021196e-05,
      "loss": 0.6321,
      "step": 1093600
    },
    {
      "epoch": 9.979743046937733,
      "grad_norm": 3.9915008544921875,
      "learning_rate": 4.1683547460885226e-05,
      "loss": 0.6465,
      "step": 1093700
    },
    {
      "epoch": 9.980655522300898,
      "grad_norm": 4.274028301239014,
      "learning_rate": 4.168278706474925e-05,
      "loss": 0.6822,
      "step": 1093800
    },
    {
      "epoch": 9.981567997664063,
      "grad_norm": 3.5114493370056152,
      "learning_rate": 4.1682026668613286e-05,
      "loss": 0.7036,
      "step": 1093900
    },
    {
      "epoch": 9.982480473027229,
      "grad_norm": 3.940976619720459,
      "learning_rate": 4.168126627247731e-05,
      "loss": 0.7007,
      "step": 1094000
    },
    {
      "epoch": 9.983392948390394,
      "grad_norm": 4.491188049316406,
      "learning_rate": 4.168050587634134e-05,
      "loss": 0.6725,
      "step": 1094100
    },
    {
      "epoch": 9.98430542375356,
      "grad_norm": 4.586746692657471,
      "learning_rate": 4.167974548020537e-05,
      "loss": 0.6939,
      "step": 1094200
    },
    {
      "epoch": 9.985217899116725,
      "grad_norm": 4.403792381286621,
      "learning_rate": 4.16789850840694e-05,
      "loss": 0.6927,
      "step": 1094300
    },
    {
      "epoch": 9.986130374479888,
      "grad_norm": 4.30879545211792,
      "learning_rate": 4.167822468793342e-05,
      "loss": 0.7065,
      "step": 1094400
    },
    {
      "epoch": 9.987042849843053,
      "grad_norm": 3.86307954788208,
      "learning_rate": 4.167746429179746e-05,
      "loss": 0.6941,
      "step": 1094500
    },
    {
      "epoch": 9.987955325206219,
      "grad_norm": 4.788852214813232,
      "learning_rate": 4.167670389566148e-05,
      "loss": 0.6841,
      "step": 1094600
    },
    {
      "epoch": 9.988867800569384,
      "grad_norm": 3.802431583404541,
      "learning_rate": 4.167594349952551e-05,
      "loss": 0.7241,
      "step": 1094700
    },
    {
      "epoch": 9.98978027593255,
      "grad_norm": 3.497995138168335,
      "learning_rate": 4.1675183103389543e-05,
      "loss": 0.676,
      "step": 1094800
    },
    {
      "epoch": 9.990692751295715,
      "grad_norm": 3.505768060684204,
      "learning_rate": 4.1674422707253573e-05,
      "loss": 0.7157,
      "step": 1094900
    },
    {
      "epoch": 9.99160522665888,
      "grad_norm": 4.475107669830322,
      "learning_rate": 4.1673662311117604e-05,
      "loss": 0.6968,
      "step": 1095000
    },
    {
      "epoch": 9.992517702022045,
      "grad_norm": 4.231032848358154,
      "learning_rate": 4.1672901914981634e-05,
      "loss": 0.6908,
      "step": 1095100
    },
    {
      "epoch": 9.99343017738521,
      "grad_norm": 4.625504493713379,
      "learning_rate": 4.167214151884566e-05,
      "loss": 0.7024,
      "step": 1095200
    },
    {
      "epoch": 9.994342652748376,
      "grad_norm": 4.1114654541015625,
      "learning_rate": 4.1671381122709694e-05,
      "loss": 0.6969,
      "step": 1095300
    },
    {
      "epoch": 9.995255128111541,
      "grad_norm": 4.365017414093018,
      "learning_rate": 4.167062072657372e-05,
      "loss": 0.6748,
      "step": 1095400
    },
    {
      "epoch": 9.996167603474706,
      "grad_norm": 4.00681734085083,
      "learning_rate": 4.166986033043775e-05,
      "loss": 0.6721,
      "step": 1095500
    },
    {
      "epoch": 9.997080078837872,
      "grad_norm": 3.854417324066162,
      "learning_rate": 4.166909993430178e-05,
      "loss": 0.6849,
      "step": 1095600
    },
    {
      "epoch": 9.997992554201037,
      "grad_norm": 3.249636650085449,
      "learning_rate": 4.166833953816581e-05,
      "loss": 0.6933,
      "step": 1095700
    },
    {
      "epoch": 9.998905029564202,
      "grad_norm": 4.197507381439209,
      "learning_rate": 4.166757914202983e-05,
      "loss": 0.7009,
      "step": 1095800
    },
    {
      "epoch": 9.999817504927368,
      "grad_norm": 4.249133586883545,
      "learning_rate": 4.166681874589387e-05,
      "loss": 0.6875,
      "step": 1095900
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.5576725602149963,
      "eval_runtime": 25.5242,
      "eval_samples_per_second": 226.021,
      "eval_steps_per_second": 226.021,
      "step": 1095920
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.5375607013702393,
      "eval_runtime": 487.2938,
      "eval_samples_per_second": 224.899,
      "eval_steps_per_second": 224.899,
      "step": 1095920
    }
  ],
  "logging_steps": 100,
  "max_steps": 6575520,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 678164414054400.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
