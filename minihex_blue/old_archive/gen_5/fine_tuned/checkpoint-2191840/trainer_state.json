{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 2191840,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0009124753631651946,
      "grad_norm": 5.147752285003662,
      "learning_rate": 4.9999239603864026e-05,
      "loss": 2.047,
      "step": 100
    },
    {
      "epoch": 0.0018249507263303892,
      "grad_norm": 6.455582141876221,
      "learning_rate": 4.999847920772806e-05,
      "loss": 1.7165,
      "step": 200
    },
    {
      "epoch": 0.0027374260894955835,
      "grad_norm": 6.773009300231934,
      "learning_rate": 4.9997718811592086e-05,
      "loss": 1.5585,
      "step": 300
    },
    {
      "epoch": 0.0036499014526607783,
      "grad_norm": 5.421006202697754,
      "learning_rate": 4.9996958415456116e-05,
      "loss": 1.5489,
      "step": 400
    },
    {
      "epoch": 0.004562376815825973,
      "grad_norm": 5.734410762786865,
      "learning_rate": 4.9996198019320146e-05,
      "loss": 1.4764,
      "step": 500
    },
    {
      "epoch": 0.005474852178991167,
      "grad_norm": 5.093199253082275,
      "learning_rate": 4.9995437623184176e-05,
      "loss": 1.4205,
      "step": 600
    },
    {
      "epoch": 0.006387327542156361,
      "grad_norm": 4.759934902191162,
      "learning_rate": 4.9994677227048206e-05,
      "loss": 1.3341,
      "step": 700
    },
    {
      "epoch": 0.007299802905321557,
      "grad_norm": 4.957815170288086,
      "learning_rate": 4.9993916830912236e-05,
      "loss": 1.4038,
      "step": 800
    },
    {
      "epoch": 0.00821227826848675,
      "grad_norm": 5.099602222442627,
      "learning_rate": 4.999315643477626e-05,
      "loss": 1.3634,
      "step": 900
    },
    {
      "epoch": 0.009124753631651945,
      "grad_norm": 4.585923194885254,
      "learning_rate": 4.9992396038640296e-05,
      "loss": 1.3656,
      "step": 1000
    },
    {
      "epoch": 0.01003722899481714,
      "grad_norm": 6.7032151222229,
      "learning_rate": 4.999163564250432e-05,
      "loss": 1.2793,
      "step": 1100
    },
    {
      "epoch": 0.010949704357982334,
      "grad_norm": 5.388281345367432,
      "learning_rate": 4.999087524636835e-05,
      "loss": 1.3033,
      "step": 1200
    },
    {
      "epoch": 0.01186217972114753,
      "grad_norm": 5.505737781524658,
      "learning_rate": 4.999011485023238e-05,
      "loss": 1.3227,
      "step": 1300
    },
    {
      "epoch": 0.012774655084312723,
      "grad_norm": 5.095860004425049,
      "learning_rate": 4.998935445409641e-05,
      "loss": 1.2672,
      "step": 1400
    },
    {
      "epoch": 0.013687130447477918,
      "grad_norm": 5.0027642250061035,
      "learning_rate": 4.998859405796043e-05,
      "loss": 1.2491,
      "step": 1500
    },
    {
      "epoch": 0.014599605810643113,
      "grad_norm": 6.116654872894287,
      "learning_rate": 4.998783366182447e-05,
      "loss": 1.2494,
      "step": 1600
    },
    {
      "epoch": 0.015512081173808307,
      "grad_norm": 5.332902908325195,
      "learning_rate": 4.998707326568849e-05,
      "loss": 1.2588,
      "step": 1700
    },
    {
      "epoch": 0.0164245565369735,
      "grad_norm": 5.488097190856934,
      "learning_rate": 4.998631286955252e-05,
      "loss": 1.2474,
      "step": 1800
    },
    {
      "epoch": 0.017337031900138695,
      "grad_norm": 4.585949897766113,
      "learning_rate": 4.998555247341655e-05,
      "loss": 1.2708,
      "step": 1900
    },
    {
      "epoch": 0.01824950726330389,
      "grad_norm": 4.305842399597168,
      "learning_rate": 4.9984792077280583e-05,
      "loss": 1.1978,
      "step": 2000
    },
    {
      "epoch": 0.019161982626469086,
      "grad_norm": 5.382846832275391,
      "learning_rate": 4.9984031681144613e-05,
      "loss": 1.2449,
      "step": 2100
    },
    {
      "epoch": 0.02007445798963428,
      "grad_norm": 4.924253463745117,
      "learning_rate": 4.9983271285008644e-05,
      "loss": 1.176,
      "step": 2200
    },
    {
      "epoch": 0.020986933352799473,
      "grad_norm": 6.521198749542236,
      "learning_rate": 4.998251088887267e-05,
      "loss": 1.2217,
      "step": 2300
    },
    {
      "epoch": 0.021899408715964668,
      "grad_norm": 5.8652663230896,
      "learning_rate": 4.9981750492736704e-05,
      "loss": 1.2258,
      "step": 2400
    },
    {
      "epoch": 0.022811884079129863,
      "grad_norm": 5.304313659667969,
      "learning_rate": 4.998099009660073e-05,
      "loss": 1.2162,
      "step": 2500
    },
    {
      "epoch": 0.02372435944229506,
      "grad_norm": 6.493332386016846,
      "learning_rate": 4.998022970046476e-05,
      "loss": 1.218,
      "step": 2600
    },
    {
      "epoch": 0.024636834805460254,
      "grad_norm": 5.013082981109619,
      "learning_rate": 4.997946930432879e-05,
      "loss": 1.1851,
      "step": 2700
    },
    {
      "epoch": 0.025549310168625446,
      "grad_norm": 6.014996528625488,
      "learning_rate": 4.997870890819282e-05,
      "loss": 1.1505,
      "step": 2800
    },
    {
      "epoch": 0.02646178553179064,
      "grad_norm": 4.777201175689697,
      "learning_rate": 4.997794851205684e-05,
      "loss": 1.1823,
      "step": 2900
    },
    {
      "epoch": 0.027374260894955836,
      "grad_norm": 4.827759265899658,
      "learning_rate": 4.997718811592087e-05,
      "loss": 1.131,
      "step": 3000
    },
    {
      "epoch": 0.02828673625812103,
      "grad_norm": 4.82503080368042,
      "learning_rate": 4.99764277197849e-05,
      "loss": 1.1867,
      "step": 3100
    },
    {
      "epoch": 0.029199211621286227,
      "grad_norm": 5.057279109954834,
      "learning_rate": 4.997566732364893e-05,
      "loss": 1.1135,
      "step": 3200
    },
    {
      "epoch": 0.03011168698445142,
      "grad_norm": 4.704108715057373,
      "learning_rate": 4.997490692751296e-05,
      "loss": 1.1804,
      "step": 3300
    },
    {
      "epoch": 0.031024162347616614,
      "grad_norm": 4.007565498352051,
      "learning_rate": 4.9974146531376984e-05,
      "loss": 1.1311,
      "step": 3400
    },
    {
      "epoch": 0.03193663771078181,
      "grad_norm": 4.832082748413086,
      "learning_rate": 4.997338613524102e-05,
      "loss": 1.1455,
      "step": 3500
    },
    {
      "epoch": 0.032849113073947,
      "grad_norm": 6.139422416687012,
      "learning_rate": 4.9972625739105044e-05,
      "loss": 1.1892,
      "step": 3600
    },
    {
      "epoch": 0.033761588437112196,
      "grad_norm": 5.568297863006592,
      "learning_rate": 4.9971865342969074e-05,
      "loss": 1.11,
      "step": 3700
    },
    {
      "epoch": 0.03467406380027739,
      "grad_norm": 6.013718605041504,
      "learning_rate": 4.9971104946833104e-05,
      "loss": 1.1139,
      "step": 3800
    },
    {
      "epoch": 0.035586539163442586,
      "grad_norm": 5.549561977386475,
      "learning_rate": 4.9970344550697134e-05,
      "loss": 1.1549,
      "step": 3900
    },
    {
      "epoch": 0.03649901452660778,
      "grad_norm": 5.1306962966918945,
      "learning_rate": 4.996958415456116e-05,
      "loss": 1.1712,
      "step": 4000
    },
    {
      "epoch": 0.03741148988977298,
      "grad_norm": 5.967419147491455,
      "learning_rate": 4.9968823758425194e-05,
      "loss": 1.1375,
      "step": 4100
    },
    {
      "epoch": 0.03832396525293817,
      "grad_norm": 5.668371677398682,
      "learning_rate": 4.996806336228922e-05,
      "loss": 1.0985,
      "step": 4200
    },
    {
      "epoch": 0.03923644061610337,
      "grad_norm": 4.743771076202393,
      "learning_rate": 4.996730296615325e-05,
      "loss": 1.1469,
      "step": 4300
    },
    {
      "epoch": 0.04014891597926856,
      "grad_norm": 5.017533302307129,
      "learning_rate": 4.996654257001728e-05,
      "loss": 1.0824,
      "step": 4400
    },
    {
      "epoch": 0.04106139134243376,
      "grad_norm": 4.850203990936279,
      "learning_rate": 4.996578217388131e-05,
      "loss": 1.1292,
      "step": 4500
    },
    {
      "epoch": 0.041973866705598946,
      "grad_norm": 5.517262935638428,
      "learning_rate": 4.996502177774534e-05,
      "loss": 1.1289,
      "step": 4600
    },
    {
      "epoch": 0.04288634206876414,
      "grad_norm": 7.167405605316162,
      "learning_rate": 4.996426138160937e-05,
      "loss": 1.1248,
      "step": 4700
    },
    {
      "epoch": 0.043798817431929336,
      "grad_norm": 4.70813512802124,
      "learning_rate": 4.996350098547339e-05,
      "loss": 1.1298,
      "step": 4800
    },
    {
      "epoch": 0.04471129279509453,
      "grad_norm": 5.63304328918457,
      "learning_rate": 4.996274058933743e-05,
      "loss": 1.1251,
      "step": 4900
    },
    {
      "epoch": 0.04562376815825973,
      "grad_norm": 5.758653163909912,
      "learning_rate": 4.996198019320145e-05,
      "loss": 1.0977,
      "step": 5000
    },
    {
      "epoch": 0.04653624352142492,
      "grad_norm": 5.132978916168213,
      "learning_rate": 4.996121979706548e-05,
      "loss": 1.0686,
      "step": 5100
    },
    {
      "epoch": 0.04744871888459012,
      "grad_norm": 4.049198150634766,
      "learning_rate": 4.996045940092951e-05,
      "loss": 1.1011,
      "step": 5200
    },
    {
      "epoch": 0.04836119424775531,
      "grad_norm": 5.664248943328857,
      "learning_rate": 4.995969900479354e-05,
      "loss": 1.0825,
      "step": 5300
    },
    {
      "epoch": 0.04927366961092051,
      "grad_norm": 4.997869968414307,
      "learning_rate": 4.9958938608657565e-05,
      "loss": 1.0886,
      "step": 5400
    },
    {
      "epoch": 0.0501861449740857,
      "grad_norm": 4.508299827575684,
      "learning_rate": 4.99581782125216e-05,
      "loss": 0.9922,
      "step": 5500
    },
    {
      "epoch": 0.05109862033725089,
      "grad_norm": 4.46112585067749,
      "learning_rate": 4.9957417816385625e-05,
      "loss": 1.1265,
      "step": 5600
    },
    {
      "epoch": 0.052011095700416086,
      "grad_norm": 3.937096357345581,
      "learning_rate": 4.9956657420249655e-05,
      "loss": 1.0793,
      "step": 5700
    },
    {
      "epoch": 0.05292357106358128,
      "grad_norm": 5.432738780975342,
      "learning_rate": 4.9955897024113685e-05,
      "loss": 1.0796,
      "step": 5800
    },
    {
      "epoch": 0.05383604642674648,
      "grad_norm": 5.13015604019165,
      "learning_rate": 4.995513662797771e-05,
      "loss": 1.1286,
      "step": 5900
    },
    {
      "epoch": 0.05474852178991167,
      "grad_norm": 4.802987575531006,
      "learning_rate": 4.9954376231841745e-05,
      "loss": 1.1182,
      "step": 6000
    },
    {
      "epoch": 0.05566099715307687,
      "grad_norm": 4.63137674331665,
      "learning_rate": 4.995361583570577e-05,
      "loss": 1.1191,
      "step": 6100
    },
    {
      "epoch": 0.05657347251624206,
      "grad_norm": 5.400720596313477,
      "learning_rate": 4.99528554395698e-05,
      "loss": 1.069,
      "step": 6200
    },
    {
      "epoch": 0.05748594787940726,
      "grad_norm": 4.433725357055664,
      "learning_rate": 4.995209504343383e-05,
      "loss": 1.0804,
      "step": 6300
    },
    {
      "epoch": 0.05839842324257245,
      "grad_norm": 5.5764360427856445,
      "learning_rate": 4.995133464729786e-05,
      "loss": 1.0794,
      "step": 6400
    },
    {
      "epoch": 0.05931089860573765,
      "grad_norm": 4.851415634155273,
      "learning_rate": 4.995057425116189e-05,
      "loss": 1.0642,
      "step": 6500
    },
    {
      "epoch": 0.06022337396890284,
      "grad_norm": 4.8092780113220215,
      "learning_rate": 4.994981385502592e-05,
      "loss": 1.0484,
      "step": 6600
    },
    {
      "epoch": 0.06113584933206803,
      "grad_norm": 5.1137495040893555,
      "learning_rate": 4.994905345888994e-05,
      "loss": 1.0568,
      "step": 6700
    },
    {
      "epoch": 0.06204832469523323,
      "grad_norm": 4.989723205566406,
      "learning_rate": 4.994829306275397e-05,
      "loss": 1.0587,
      "step": 6800
    },
    {
      "epoch": 0.06296080005839842,
      "grad_norm": 5.431092739105225,
      "learning_rate": 4.9947532666618e-05,
      "loss": 1.0431,
      "step": 6900
    },
    {
      "epoch": 0.06387327542156362,
      "grad_norm": 5.896696090698242,
      "learning_rate": 4.994677227048203e-05,
      "loss": 1.0742,
      "step": 7000
    },
    {
      "epoch": 0.06478575078472881,
      "grad_norm": 4.9062018394470215,
      "learning_rate": 4.994601187434606e-05,
      "loss": 1.0246,
      "step": 7100
    },
    {
      "epoch": 0.065698226147894,
      "grad_norm": 5.024612903594971,
      "learning_rate": 4.994525147821009e-05,
      "loss": 1.0823,
      "step": 7200
    },
    {
      "epoch": 0.0666107015110592,
      "grad_norm": 4.10994291305542,
      "learning_rate": 4.9944491082074116e-05,
      "loss": 1.0462,
      "step": 7300
    },
    {
      "epoch": 0.06752317687422439,
      "grad_norm": 5.955977439880371,
      "learning_rate": 4.994373068593815e-05,
      "loss": 1.0274,
      "step": 7400
    },
    {
      "epoch": 0.0684356522373896,
      "grad_norm": 4.850308418273926,
      "learning_rate": 4.9942970289802176e-05,
      "loss": 1.0503,
      "step": 7500
    },
    {
      "epoch": 0.06934812760055478,
      "grad_norm": 6.431196689605713,
      "learning_rate": 4.9942209893666206e-05,
      "loss": 1.0331,
      "step": 7600
    },
    {
      "epoch": 0.07026060296371998,
      "grad_norm": 5.6030988693237305,
      "learning_rate": 4.9941449497530236e-05,
      "loss": 1.0199,
      "step": 7700
    },
    {
      "epoch": 0.07117307832688517,
      "grad_norm": 5.367276191711426,
      "learning_rate": 4.9940689101394266e-05,
      "loss": 1.0476,
      "step": 7800
    },
    {
      "epoch": 0.07208555369005037,
      "grad_norm": 4.659315586090088,
      "learning_rate": 4.9939928705258296e-05,
      "loss": 1.0384,
      "step": 7900
    },
    {
      "epoch": 0.07299802905321556,
      "grad_norm": 4.41477632522583,
      "learning_rate": 4.9939168309122326e-05,
      "loss": 1.0607,
      "step": 8000
    },
    {
      "epoch": 0.07391050441638075,
      "grad_norm": 4.5584635734558105,
      "learning_rate": 4.993840791298635e-05,
      "loss": 1.0282,
      "step": 8100
    },
    {
      "epoch": 0.07482297977954595,
      "grad_norm": 5.057887554168701,
      "learning_rate": 4.9937647516850387e-05,
      "loss": 1.0418,
      "step": 8200
    },
    {
      "epoch": 0.07573545514271114,
      "grad_norm": 4.523952960968018,
      "learning_rate": 4.993688712071441e-05,
      "loss": 1.0542,
      "step": 8300
    },
    {
      "epoch": 0.07664793050587634,
      "grad_norm": 4.021158695220947,
      "learning_rate": 4.993612672457844e-05,
      "loss": 1.0582,
      "step": 8400
    },
    {
      "epoch": 0.07756040586904153,
      "grad_norm": 5.176318168640137,
      "learning_rate": 4.993536632844247e-05,
      "loss": 0.9957,
      "step": 8500
    },
    {
      "epoch": 0.07847288123220673,
      "grad_norm": 4.5128560066223145,
      "learning_rate": 4.993460593230649e-05,
      "loss": 1.0168,
      "step": 8600
    },
    {
      "epoch": 0.07938535659537192,
      "grad_norm": 5.245326519012451,
      "learning_rate": 4.993384553617052e-05,
      "loss": 1.0395,
      "step": 8700
    },
    {
      "epoch": 0.08029783195853712,
      "grad_norm": 6.474645137786865,
      "learning_rate": 4.9933085140034553e-05,
      "loss": 1.051,
      "step": 8800
    },
    {
      "epoch": 0.08121030732170231,
      "grad_norm": 4.271410942077637,
      "learning_rate": 4.9932324743898584e-05,
      "loss": 1.0843,
      "step": 8900
    },
    {
      "epoch": 0.08212278268486752,
      "grad_norm": 4.599697113037109,
      "learning_rate": 4.9931564347762614e-05,
      "loss": 1.0409,
      "step": 9000
    },
    {
      "epoch": 0.0830352580480327,
      "grad_norm": 4.168335437774658,
      "learning_rate": 4.9930803951626644e-05,
      "loss": 1.029,
      "step": 9100
    },
    {
      "epoch": 0.08394773341119789,
      "grad_norm": 5.3780012130737305,
      "learning_rate": 4.993004355549067e-05,
      "loss": 1.0331,
      "step": 9200
    },
    {
      "epoch": 0.0848602087743631,
      "grad_norm": 5.1586174964904785,
      "learning_rate": 4.9929283159354704e-05,
      "loss": 1.0347,
      "step": 9300
    },
    {
      "epoch": 0.08577268413752828,
      "grad_norm": 4.652102947235107,
      "learning_rate": 4.992852276321873e-05,
      "loss": 1.0247,
      "step": 9400
    },
    {
      "epoch": 0.08668515950069348,
      "grad_norm": 5.572147369384766,
      "learning_rate": 4.992776236708276e-05,
      "loss": 1.0644,
      "step": 9500
    },
    {
      "epoch": 0.08759763486385867,
      "grad_norm": 4.166231155395508,
      "learning_rate": 4.992700197094679e-05,
      "loss": 1.0082,
      "step": 9600
    },
    {
      "epoch": 0.08851011022702387,
      "grad_norm": 4.67948579788208,
      "learning_rate": 4.992624157481082e-05,
      "loss": 1.0518,
      "step": 9700
    },
    {
      "epoch": 0.08942258559018906,
      "grad_norm": 4.811708927154541,
      "learning_rate": 4.992548117867484e-05,
      "loss": 1.0716,
      "step": 9800
    },
    {
      "epoch": 0.09033506095335427,
      "grad_norm": 4.517982006072998,
      "learning_rate": 4.992472078253888e-05,
      "loss": 1.0065,
      "step": 9900
    },
    {
      "epoch": 0.09124753631651945,
      "grad_norm": 4.626156330108643,
      "learning_rate": 4.99239603864029e-05,
      "loss": 1.0008,
      "step": 10000
    },
    {
      "epoch": 0.09216001167968464,
      "grad_norm": 5.2847795486450195,
      "learning_rate": 4.992319999026693e-05,
      "loss": 1.0528,
      "step": 10100
    },
    {
      "epoch": 0.09307248704284984,
      "grad_norm": 5.339972496032715,
      "learning_rate": 4.992243959413096e-05,
      "loss": 1.0351,
      "step": 10200
    },
    {
      "epoch": 0.09398496240601503,
      "grad_norm": 5.336594581604004,
      "learning_rate": 4.992167919799499e-05,
      "loss": 1.0033,
      "step": 10300
    },
    {
      "epoch": 0.09489743776918023,
      "grad_norm": 5.415207386016846,
      "learning_rate": 4.992091880185902e-05,
      "loss": 1.0148,
      "step": 10400
    },
    {
      "epoch": 0.09580991313234542,
      "grad_norm": 4.425357341766357,
      "learning_rate": 4.992015840572305e-05,
      "loss": 1.0242,
      "step": 10500
    },
    {
      "epoch": 0.09672238849551063,
      "grad_norm": 5.34060525894165,
      "learning_rate": 4.9919398009587074e-05,
      "loss": 1.0002,
      "step": 10600
    },
    {
      "epoch": 0.09763486385867581,
      "grad_norm": 5.614400863647461,
      "learning_rate": 4.991863761345111e-05,
      "loss": 0.9977,
      "step": 10700
    },
    {
      "epoch": 0.09854733922184102,
      "grad_norm": 4.992709159851074,
      "learning_rate": 4.9917877217315134e-05,
      "loss": 1.0249,
      "step": 10800
    },
    {
      "epoch": 0.0994598145850062,
      "grad_norm": 4.335575103759766,
      "learning_rate": 4.9917116821179165e-05,
      "loss": 0.9782,
      "step": 10900
    },
    {
      "epoch": 0.1003722899481714,
      "grad_norm": 4.879144668579102,
      "learning_rate": 4.9916356425043195e-05,
      "loss": 1.0493,
      "step": 11000
    },
    {
      "epoch": 0.1012847653113366,
      "grad_norm": 4.629830837249756,
      "learning_rate": 4.9915596028907225e-05,
      "loss": 1.0287,
      "step": 11100
    },
    {
      "epoch": 0.10219724067450178,
      "grad_norm": 4.66579008102417,
      "learning_rate": 4.991483563277125e-05,
      "loss": 1.0228,
      "step": 11200
    },
    {
      "epoch": 0.10310971603766698,
      "grad_norm": 5.184679985046387,
      "learning_rate": 4.9914075236635285e-05,
      "loss": 1.0422,
      "step": 11300
    },
    {
      "epoch": 0.10402219140083217,
      "grad_norm": 5.502918720245361,
      "learning_rate": 4.991331484049931e-05,
      "loss": 0.9682,
      "step": 11400
    },
    {
      "epoch": 0.10493466676399738,
      "grad_norm": 5.832265853881836,
      "learning_rate": 4.991255444436334e-05,
      "loss": 1.0422,
      "step": 11500
    },
    {
      "epoch": 0.10584714212716256,
      "grad_norm": 5.033229827880859,
      "learning_rate": 4.991179404822737e-05,
      "loss": 0.9729,
      "step": 11600
    },
    {
      "epoch": 0.10675961749032777,
      "grad_norm": 4.786765098571777,
      "learning_rate": 4.991103365209139e-05,
      "loss": 1.0414,
      "step": 11700
    },
    {
      "epoch": 0.10767209285349295,
      "grad_norm": 5.122344017028809,
      "learning_rate": 4.991027325595543e-05,
      "loss": 1.0109,
      "step": 11800
    },
    {
      "epoch": 0.10858456821665816,
      "grad_norm": 4.933959484100342,
      "learning_rate": 4.990951285981945e-05,
      "loss": 0.993,
      "step": 11900
    },
    {
      "epoch": 0.10949704357982334,
      "grad_norm": 5.143568992614746,
      "learning_rate": 4.990875246368348e-05,
      "loss": 1.0368,
      "step": 12000
    },
    {
      "epoch": 0.11040951894298853,
      "grad_norm": 4.915111541748047,
      "learning_rate": 4.990799206754751e-05,
      "loss": 1.0103,
      "step": 12100
    },
    {
      "epoch": 0.11132199430615373,
      "grad_norm": 4.690226078033447,
      "learning_rate": 4.990723167141154e-05,
      "loss": 1.0325,
      "step": 12200
    },
    {
      "epoch": 0.11223446966931892,
      "grad_norm": 4.9280805587768555,
      "learning_rate": 4.9906471275275565e-05,
      "loss": 0.9983,
      "step": 12300
    },
    {
      "epoch": 0.11314694503248413,
      "grad_norm": 5.5166916847229,
      "learning_rate": 4.99057108791396e-05,
      "loss": 0.9813,
      "step": 12400
    },
    {
      "epoch": 0.11405942039564931,
      "grad_norm": 4.43505859375,
      "learning_rate": 4.9904950483003625e-05,
      "loss": 1.001,
      "step": 12500
    },
    {
      "epoch": 0.11497189575881452,
      "grad_norm": 5.304643154144287,
      "learning_rate": 4.9904190086867655e-05,
      "loss": 1.0188,
      "step": 12600
    },
    {
      "epoch": 0.1158843711219797,
      "grad_norm": 4.515888690948486,
      "learning_rate": 4.9903429690731685e-05,
      "loss": 1.012,
      "step": 12700
    },
    {
      "epoch": 0.1167968464851449,
      "grad_norm": 3.9940059185028076,
      "learning_rate": 4.9902669294595715e-05,
      "loss": 1.0452,
      "step": 12800
    },
    {
      "epoch": 0.1177093218483101,
      "grad_norm": 6.7112579345703125,
      "learning_rate": 4.9901908898459746e-05,
      "loss": 1.004,
      "step": 12900
    },
    {
      "epoch": 0.1186217972114753,
      "grad_norm": 4.408282279968262,
      "learning_rate": 4.9901148502323776e-05,
      "loss": 0.9934,
      "step": 13000
    },
    {
      "epoch": 0.11953427257464048,
      "grad_norm": 4.672858238220215,
      "learning_rate": 4.99003881061878e-05,
      "loss": 1.0078,
      "step": 13100
    },
    {
      "epoch": 0.12044674793780567,
      "grad_norm": 4.908210277557373,
      "learning_rate": 4.9899627710051836e-05,
      "loss": 0.9817,
      "step": 13200
    },
    {
      "epoch": 0.12135922330097088,
      "grad_norm": 3.340280771255493,
      "learning_rate": 4.989886731391586e-05,
      "loss": 0.9717,
      "step": 13300
    },
    {
      "epoch": 0.12227169866413606,
      "grad_norm": 4.678239345550537,
      "learning_rate": 4.989810691777989e-05,
      "loss": 0.9935,
      "step": 13400
    },
    {
      "epoch": 0.12318417402730127,
      "grad_norm": 4.557514667510986,
      "learning_rate": 4.989734652164392e-05,
      "loss": 1.0079,
      "step": 13500
    },
    {
      "epoch": 0.12409664939046645,
      "grad_norm": 4.994625091552734,
      "learning_rate": 4.989658612550795e-05,
      "loss": 0.9925,
      "step": 13600
    },
    {
      "epoch": 0.12500912475363166,
      "grad_norm": 4.440512180328369,
      "learning_rate": 4.989582572937197e-05,
      "loss": 1.023,
      "step": 13700
    },
    {
      "epoch": 0.12592160011679684,
      "grad_norm": 4.469070911407471,
      "learning_rate": 4.989506533323601e-05,
      "loss": 1.0155,
      "step": 13800
    },
    {
      "epoch": 0.12683407547996203,
      "grad_norm": 5.337492942810059,
      "learning_rate": 4.989430493710003e-05,
      "loss": 1.0019,
      "step": 13900
    },
    {
      "epoch": 0.12774655084312725,
      "grad_norm": 6.158061504364014,
      "learning_rate": 4.989354454096406e-05,
      "loss": 1.0251,
      "step": 14000
    },
    {
      "epoch": 0.12865902620629244,
      "grad_norm": 5.3162841796875,
      "learning_rate": 4.989278414482809e-05,
      "loss": 0.9913,
      "step": 14100
    },
    {
      "epoch": 0.12957150156945763,
      "grad_norm": 5.055201053619385,
      "learning_rate": 4.989202374869212e-05,
      "loss": 0.9893,
      "step": 14200
    },
    {
      "epoch": 0.1304839769326228,
      "grad_norm": 5.229305744171143,
      "learning_rate": 4.989126335255615e-05,
      "loss": 0.9818,
      "step": 14300
    },
    {
      "epoch": 0.131396452295788,
      "grad_norm": 3.7757158279418945,
      "learning_rate": 4.9890502956420176e-05,
      "loss": 1.0027,
      "step": 14400
    },
    {
      "epoch": 0.13230892765895322,
      "grad_norm": 4.328711032867432,
      "learning_rate": 4.9889742560284206e-05,
      "loss": 0.9793,
      "step": 14500
    },
    {
      "epoch": 0.1332214030221184,
      "grad_norm": 4.261618614196777,
      "learning_rate": 4.9888982164148236e-05,
      "loss": 1.0276,
      "step": 14600
    },
    {
      "epoch": 0.1341338783852836,
      "grad_norm": 4.917252063751221,
      "learning_rate": 4.9888221768012266e-05,
      "loss": 0.9852,
      "step": 14700
    },
    {
      "epoch": 0.13504635374844878,
      "grad_norm": 4.4042158126831055,
      "learning_rate": 4.988746137187629e-05,
      "loss": 0.9975,
      "step": 14800
    },
    {
      "epoch": 0.135958829111614,
      "grad_norm": 4.1395978927612305,
      "learning_rate": 4.9886700975740327e-05,
      "loss": 0.9687,
      "step": 14900
    },
    {
      "epoch": 0.1368713044747792,
      "grad_norm": 5.148288249969482,
      "learning_rate": 4.988594057960435e-05,
      "loss": 0.9816,
      "step": 15000
    },
    {
      "epoch": 0.13778377983794438,
      "grad_norm": 5.380253314971924,
      "learning_rate": 4.988518018346838e-05,
      "loss": 0.9869,
      "step": 15100
    },
    {
      "epoch": 0.13869625520110956,
      "grad_norm": 5.2568793296813965,
      "learning_rate": 4.988441978733241e-05,
      "loss": 0.9855,
      "step": 15200
    },
    {
      "epoch": 0.13960873056427475,
      "grad_norm": 5.154153823852539,
      "learning_rate": 4.988365939119644e-05,
      "loss": 1.0433,
      "step": 15300
    },
    {
      "epoch": 0.14052120592743997,
      "grad_norm": 4.959994316101074,
      "learning_rate": 4.988289899506047e-05,
      "loss": 0.9531,
      "step": 15400
    },
    {
      "epoch": 0.14143368129060516,
      "grad_norm": 4.81374454498291,
      "learning_rate": 4.98821385989245e-05,
      "loss": 0.9847,
      "step": 15500
    },
    {
      "epoch": 0.14234615665377034,
      "grad_norm": 5.042161464691162,
      "learning_rate": 4.9881378202788523e-05,
      "loss": 0.9733,
      "step": 15600
    },
    {
      "epoch": 0.14325863201693553,
      "grad_norm": 4.790406227111816,
      "learning_rate": 4.988061780665256e-05,
      "loss": 0.9656,
      "step": 15700
    },
    {
      "epoch": 0.14417110738010075,
      "grad_norm": 5.431309700012207,
      "learning_rate": 4.9879857410516584e-05,
      "loss": 0.9684,
      "step": 15800
    },
    {
      "epoch": 0.14508358274326594,
      "grad_norm": 4.307794094085693,
      "learning_rate": 4.9879097014380614e-05,
      "loss": 0.9948,
      "step": 15900
    },
    {
      "epoch": 0.14599605810643113,
      "grad_norm": 4.555723190307617,
      "learning_rate": 4.9878336618244644e-05,
      "loss": 1.0287,
      "step": 16000
    },
    {
      "epoch": 0.14690853346959631,
      "grad_norm": 4.739310264587402,
      "learning_rate": 4.9877576222108674e-05,
      "loss": 1.0039,
      "step": 16100
    },
    {
      "epoch": 0.1478210088327615,
      "grad_norm": 4.440958023071289,
      "learning_rate": 4.98768158259727e-05,
      "loss": 0.9645,
      "step": 16200
    },
    {
      "epoch": 0.14873348419592672,
      "grad_norm": 5.031438827514648,
      "learning_rate": 4.9876055429836734e-05,
      "loss": 0.9226,
      "step": 16300
    },
    {
      "epoch": 0.1496459595590919,
      "grad_norm": 5.225769519805908,
      "learning_rate": 4.987529503370076e-05,
      "loss": 0.9583,
      "step": 16400
    },
    {
      "epoch": 0.1505584349222571,
      "grad_norm": 4.468567371368408,
      "learning_rate": 4.987453463756479e-05,
      "loss": 0.9698,
      "step": 16500
    },
    {
      "epoch": 0.15147091028542228,
      "grad_norm": 4.0949835777282715,
      "learning_rate": 4.987377424142882e-05,
      "loss": 0.9613,
      "step": 16600
    },
    {
      "epoch": 0.1523833856485875,
      "grad_norm": 3.7389895915985107,
      "learning_rate": 4.987301384529285e-05,
      "loss": 0.95,
      "step": 16700
    },
    {
      "epoch": 0.1532958610117527,
      "grad_norm": 4.748741626739502,
      "learning_rate": 4.987225344915688e-05,
      "loss": 0.9964,
      "step": 16800
    },
    {
      "epoch": 0.15420833637491788,
      "grad_norm": 4.41128396987915,
      "learning_rate": 4.987149305302091e-05,
      "loss": 1.0043,
      "step": 16900
    },
    {
      "epoch": 0.15512081173808306,
      "grad_norm": 4.348278522491455,
      "learning_rate": 4.987073265688493e-05,
      "loss": 0.9344,
      "step": 17000
    },
    {
      "epoch": 0.15603328710124825,
      "grad_norm": 5.987673282623291,
      "learning_rate": 4.986997226074896e-05,
      "loss": 0.98,
      "step": 17100
    },
    {
      "epoch": 0.15694576246441347,
      "grad_norm": 4.347245216369629,
      "learning_rate": 4.986921186461299e-05,
      "loss": 0.9777,
      "step": 17200
    },
    {
      "epoch": 0.15785823782757866,
      "grad_norm": 5.230325222015381,
      "learning_rate": 4.9868451468477014e-05,
      "loss": 0.9386,
      "step": 17300
    },
    {
      "epoch": 0.15877071319074385,
      "grad_norm": 4.3565850257873535,
      "learning_rate": 4.986769107234105e-05,
      "loss": 1.0125,
      "step": 17400
    },
    {
      "epoch": 0.15968318855390903,
      "grad_norm": 4.163906097412109,
      "learning_rate": 4.9866930676205074e-05,
      "loss": 1.0012,
      "step": 17500
    },
    {
      "epoch": 0.16059566391707425,
      "grad_norm": 5.310068607330322,
      "learning_rate": 4.9866170280069104e-05,
      "loss": 0.9705,
      "step": 17600
    },
    {
      "epoch": 0.16150813928023944,
      "grad_norm": 4.239343166351318,
      "learning_rate": 4.9865409883933135e-05,
      "loss": 0.9551,
      "step": 17700
    },
    {
      "epoch": 0.16242061464340463,
      "grad_norm": 4.655518531799316,
      "learning_rate": 4.9864649487797165e-05,
      "loss": 0.9708,
      "step": 17800
    },
    {
      "epoch": 0.16333309000656981,
      "grad_norm": 4.728153705596924,
      "learning_rate": 4.9863889091661195e-05,
      "loss": 0.9691,
      "step": 17900
    },
    {
      "epoch": 0.16424556536973503,
      "grad_norm": 4.935769557952881,
      "learning_rate": 4.9863128695525225e-05,
      "loss": 0.9774,
      "step": 18000
    },
    {
      "epoch": 0.16515804073290022,
      "grad_norm": 6.4485297203063965,
      "learning_rate": 4.986236829938925e-05,
      "loss": 0.9821,
      "step": 18100
    },
    {
      "epoch": 0.1660705160960654,
      "grad_norm": 4.827169418334961,
      "learning_rate": 4.9861607903253285e-05,
      "loss": 0.9609,
      "step": 18200
    },
    {
      "epoch": 0.1669829914592306,
      "grad_norm": 4.721896648406982,
      "learning_rate": 4.986084750711731e-05,
      "loss": 0.9618,
      "step": 18300
    },
    {
      "epoch": 0.16789546682239578,
      "grad_norm": 4.433897018432617,
      "learning_rate": 4.986008711098134e-05,
      "loss": 0.9827,
      "step": 18400
    },
    {
      "epoch": 0.168807942185561,
      "grad_norm": 4.125970840454102,
      "learning_rate": 4.985932671484537e-05,
      "loss": 0.9506,
      "step": 18500
    },
    {
      "epoch": 0.1697204175487262,
      "grad_norm": 4.994576930999756,
      "learning_rate": 4.98585663187094e-05,
      "loss": 0.9803,
      "step": 18600
    },
    {
      "epoch": 0.17063289291189138,
      "grad_norm": 4.5279059410095215,
      "learning_rate": 4.985780592257343e-05,
      "loss": 0.9384,
      "step": 18700
    },
    {
      "epoch": 0.17154536827505656,
      "grad_norm": 4.213637351989746,
      "learning_rate": 4.985704552643746e-05,
      "loss": 0.9675,
      "step": 18800
    },
    {
      "epoch": 0.17245784363822178,
      "grad_norm": 4.085555553436279,
      "learning_rate": 4.985628513030148e-05,
      "loss": 1.0069,
      "step": 18900
    },
    {
      "epoch": 0.17337031900138697,
      "grad_norm": 2.7393038272857666,
      "learning_rate": 4.985552473416551e-05,
      "loss": 0.9904,
      "step": 19000
    },
    {
      "epoch": 0.17428279436455216,
      "grad_norm": 4.288165092468262,
      "learning_rate": 4.985476433802954e-05,
      "loss": 1.0161,
      "step": 19100
    },
    {
      "epoch": 0.17519526972771735,
      "grad_norm": 4.287474632263184,
      "learning_rate": 4.985400394189357e-05,
      "loss": 0.9334,
      "step": 19200
    },
    {
      "epoch": 0.17610774509088253,
      "grad_norm": 4.42560338973999,
      "learning_rate": 4.98532435457576e-05,
      "loss": 0.9523,
      "step": 19300
    },
    {
      "epoch": 0.17702022045404775,
      "grad_norm": 4.847213268280029,
      "learning_rate": 4.985248314962163e-05,
      "loss": 0.972,
      "step": 19400
    },
    {
      "epoch": 0.17793269581721294,
      "grad_norm": 4.194736480712891,
      "learning_rate": 4.9851722753485655e-05,
      "loss": 0.9288,
      "step": 19500
    },
    {
      "epoch": 0.17884517118037813,
      "grad_norm": 4.979959011077881,
      "learning_rate": 4.985096235734969e-05,
      "loss": 0.9425,
      "step": 19600
    },
    {
      "epoch": 0.17975764654354331,
      "grad_norm": 4.4122467041015625,
      "learning_rate": 4.9850201961213716e-05,
      "loss": 1.0003,
      "step": 19700
    },
    {
      "epoch": 0.18067012190670853,
      "grad_norm": 4.379156589508057,
      "learning_rate": 4.9849441565077746e-05,
      "loss": 0.9642,
      "step": 19800
    },
    {
      "epoch": 0.18158259726987372,
      "grad_norm": 4.459123611450195,
      "learning_rate": 4.9848681168941776e-05,
      "loss": 0.9549,
      "step": 19900
    },
    {
      "epoch": 0.1824950726330389,
      "grad_norm": 4.236456871032715,
      "learning_rate": 4.98479207728058e-05,
      "loss": 0.9636,
      "step": 20000
    },
    {
      "epoch": 0.1834075479962041,
      "grad_norm": 6.026739120483398,
      "learning_rate": 4.9847160376669836e-05,
      "loss": 0.9472,
      "step": 20100
    },
    {
      "epoch": 0.18432002335936928,
      "grad_norm": 4.636136531829834,
      "learning_rate": 4.984639998053386e-05,
      "loss": 0.968,
      "step": 20200
    },
    {
      "epoch": 0.1852324987225345,
      "grad_norm": 4.744104862213135,
      "learning_rate": 4.984563958439789e-05,
      "loss": 0.9252,
      "step": 20300
    },
    {
      "epoch": 0.1861449740856997,
      "grad_norm": 5.12874174118042,
      "learning_rate": 4.984487918826192e-05,
      "loss": 0.9446,
      "step": 20400
    },
    {
      "epoch": 0.18705744944886488,
      "grad_norm": 4.2995381355285645,
      "learning_rate": 4.984411879212595e-05,
      "loss": 0.9699,
      "step": 20500
    },
    {
      "epoch": 0.18796992481203006,
      "grad_norm": 5.998723030090332,
      "learning_rate": 4.984335839598997e-05,
      "loss": 0.9812,
      "step": 20600
    },
    {
      "epoch": 0.18888240017519528,
      "grad_norm": 4.240713596343994,
      "learning_rate": 4.984259799985401e-05,
      "loss": 0.909,
      "step": 20700
    },
    {
      "epoch": 0.18979487553836047,
      "grad_norm": 4.558784008026123,
      "learning_rate": 4.984183760371803e-05,
      "loss": 0.9189,
      "step": 20800
    },
    {
      "epoch": 0.19070735090152566,
      "grad_norm": 4.780670642852783,
      "learning_rate": 4.984107720758206e-05,
      "loss": 0.9759,
      "step": 20900
    },
    {
      "epoch": 0.19161982626469085,
      "grad_norm": 4.176640033721924,
      "learning_rate": 4.984031681144609e-05,
      "loss": 0.9174,
      "step": 21000
    },
    {
      "epoch": 0.19253230162785606,
      "grad_norm": 5.6177825927734375,
      "learning_rate": 4.983955641531012e-05,
      "loss": 0.9485,
      "step": 21100
    },
    {
      "epoch": 0.19344477699102125,
      "grad_norm": 4.602069854736328,
      "learning_rate": 4.983879601917415e-05,
      "loss": 0.9121,
      "step": 21200
    },
    {
      "epoch": 0.19435725235418644,
      "grad_norm": 4.573707580566406,
      "learning_rate": 4.983803562303818e-05,
      "loss": 0.9538,
      "step": 21300
    },
    {
      "epoch": 0.19526972771735163,
      "grad_norm": 4.385225772857666,
      "learning_rate": 4.9837275226902206e-05,
      "loss": 0.9531,
      "step": 21400
    },
    {
      "epoch": 0.19618220308051681,
      "grad_norm": 3.8044681549072266,
      "learning_rate": 4.983651483076624e-05,
      "loss": 0.9285,
      "step": 21500
    },
    {
      "epoch": 0.19709467844368203,
      "grad_norm": 4.769730091094971,
      "learning_rate": 4.9835754434630267e-05,
      "loss": 0.9473,
      "step": 21600
    },
    {
      "epoch": 0.19800715380684722,
      "grad_norm": 4.86564826965332,
      "learning_rate": 4.9834994038494297e-05,
      "loss": 0.9342,
      "step": 21700
    },
    {
      "epoch": 0.1989196291700124,
      "grad_norm": 4.733358383178711,
      "learning_rate": 4.983423364235833e-05,
      "loss": 0.9429,
      "step": 21800
    },
    {
      "epoch": 0.1998321045331776,
      "grad_norm": 5.444200038909912,
      "learning_rate": 4.983347324622236e-05,
      "loss": 0.9197,
      "step": 21900
    },
    {
      "epoch": 0.2007445798963428,
      "grad_norm": 4.371159076690674,
      "learning_rate": 4.983271285008638e-05,
      "loss": 0.9888,
      "step": 22000
    },
    {
      "epoch": 0.201657055259508,
      "grad_norm": 5.206201553344727,
      "learning_rate": 4.983195245395042e-05,
      "loss": 0.9443,
      "step": 22100
    },
    {
      "epoch": 0.2025695306226732,
      "grad_norm": 4.096121788024902,
      "learning_rate": 4.983119205781444e-05,
      "loss": 0.9245,
      "step": 22200
    },
    {
      "epoch": 0.20348200598583838,
      "grad_norm": 4.145411491394043,
      "learning_rate": 4.983043166167847e-05,
      "loss": 0.962,
      "step": 22300
    },
    {
      "epoch": 0.20439448134900357,
      "grad_norm": 4.476589202880859,
      "learning_rate": 4.98296712655425e-05,
      "loss": 0.9347,
      "step": 22400
    },
    {
      "epoch": 0.20530695671216878,
      "grad_norm": 5.975056171417236,
      "learning_rate": 4.982891086940653e-05,
      "loss": 1.0005,
      "step": 22500
    },
    {
      "epoch": 0.20621943207533397,
      "grad_norm": 4.260934829711914,
      "learning_rate": 4.982815047327056e-05,
      "loss": 0.947,
      "step": 22600
    },
    {
      "epoch": 0.20713190743849916,
      "grad_norm": 4.765782356262207,
      "learning_rate": 4.982739007713459e-05,
      "loss": 0.9337,
      "step": 22700
    },
    {
      "epoch": 0.20804438280166435,
      "grad_norm": 4.586453914642334,
      "learning_rate": 4.9826629680998614e-05,
      "loss": 0.9334,
      "step": 22800
    },
    {
      "epoch": 0.20895685816482956,
      "grad_norm": 4.733198642730713,
      "learning_rate": 4.9825869284862644e-05,
      "loss": 0.9126,
      "step": 22900
    },
    {
      "epoch": 0.20986933352799475,
      "grad_norm": 5.027307510375977,
      "learning_rate": 4.9825108888726674e-05,
      "loss": 0.968,
      "step": 23000
    },
    {
      "epoch": 0.21078180889115994,
      "grad_norm": 4.905813694000244,
      "learning_rate": 4.98243484925907e-05,
      "loss": 0.9306,
      "step": 23100
    },
    {
      "epoch": 0.21169428425432513,
      "grad_norm": 4.60032320022583,
      "learning_rate": 4.9823588096454734e-05,
      "loss": 0.9878,
      "step": 23200
    },
    {
      "epoch": 0.21260675961749032,
      "grad_norm": 4.479092597961426,
      "learning_rate": 4.982282770031876e-05,
      "loss": 0.9409,
      "step": 23300
    },
    {
      "epoch": 0.21351923498065553,
      "grad_norm": 4.68436861038208,
      "learning_rate": 4.982206730418279e-05,
      "loss": 0.9165,
      "step": 23400
    },
    {
      "epoch": 0.21443171034382072,
      "grad_norm": 4.769109725952148,
      "learning_rate": 4.982130690804682e-05,
      "loss": 0.9701,
      "step": 23500
    },
    {
      "epoch": 0.2153441857069859,
      "grad_norm": 5.08104133605957,
      "learning_rate": 4.982054651191085e-05,
      "loss": 0.9659,
      "step": 23600
    },
    {
      "epoch": 0.2162566610701511,
      "grad_norm": 5.292693614959717,
      "learning_rate": 4.981978611577488e-05,
      "loss": 0.9766,
      "step": 23700
    },
    {
      "epoch": 0.2171691364333163,
      "grad_norm": 4.983695030212402,
      "learning_rate": 4.981902571963891e-05,
      "loss": 0.9292,
      "step": 23800
    },
    {
      "epoch": 0.2180816117964815,
      "grad_norm": 5.580954551696777,
      "learning_rate": 4.981826532350293e-05,
      "loss": 0.9238,
      "step": 23900
    },
    {
      "epoch": 0.2189940871596467,
      "grad_norm": 4.009984493255615,
      "learning_rate": 4.981750492736697e-05,
      "loss": 0.9483,
      "step": 24000
    },
    {
      "epoch": 0.21990656252281188,
      "grad_norm": 5.208849906921387,
      "learning_rate": 4.981674453123099e-05,
      "loss": 0.9023,
      "step": 24100
    },
    {
      "epoch": 0.22081903788597707,
      "grad_norm": 5.515771389007568,
      "learning_rate": 4.981598413509502e-05,
      "loss": 0.9394,
      "step": 24200
    },
    {
      "epoch": 0.22173151324914228,
      "grad_norm": 3.8593809604644775,
      "learning_rate": 4.981522373895905e-05,
      "loss": 0.9528,
      "step": 24300
    },
    {
      "epoch": 0.22264398861230747,
      "grad_norm": 3.8547582626342773,
      "learning_rate": 4.981446334282308e-05,
      "loss": 0.9442,
      "step": 24400
    },
    {
      "epoch": 0.22355646397547266,
      "grad_norm": 5.0004191398620605,
      "learning_rate": 4.9813702946687105e-05,
      "loss": 0.9978,
      "step": 24500
    },
    {
      "epoch": 0.22446893933863785,
      "grad_norm": 5.427975177764893,
      "learning_rate": 4.981294255055114e-05,
      "loss": 0.9165,
      "step": 24600
    },
    {
      "epoch": 0.22538141470180306,
      "grad_norm": 5.016482830047607,
      "learning_rate": 4.9812182154415165e-05,
      "loss": 0.964,
      "step": 24700
    },
    {
      "epoch": 0.22629389006496825,
      "grad_norm": 4.590939521789551,
      "learning_rate": 4.9811421758279195e-05,
      "loss": 0.9323,
      "step": 24800
    },
    {
      "epoch": 0.22720636542813344,
      "grad_norm": 3.956953763961792,
      "learning_rate": 4.9810661362143225e-05,
      "loss": 0.9577,
      "step": 24900
    },
    {
      "epoch": 0.22811884079129863,
      "grad_norm": 4.2402167320251465,
      "learning_rate": 4.9809900966007255e-05,
      "loss": 0.9393,
      "step": 25000
    },
    {
      "epoch": 0.22903131615446384,
      "grad_norm": 4.6394147872924805,
      "learning_rate": 4.9809140569871285e-05,
      "loss": 0.9399,
      "step": 25100
    },
    {
      "epoch": 0.22994379151762903,
      "grad_norm": 4.147954940795898,
      "learning_rate": 4.9808380173735315e-05,
      "loss": 0.9316,
      "step": 25200
    },
    {
      "epoch": 0.23085626688079422,
      "grad_norm": 3.613168478012085,
      "learning_rate": 4.980761977759934e-05,
      "loss": 0.9552,
      "step": 25300
    },
    {
      "epoch": 0.2317687422439594,
      "grad_norm": 5.127684593200684,
      "learning_rate": 4.9806859381463375e-05,
      "loss": 0.9156,
      "step": 25400
    },
    {
      "epoch": 0.2326812176071246,
      "grad_norm": 3.879422426223755,
      "learning_rate": 4.98060989853274e-05,
      "loss": 0.9222,
      "step": 25500
    },
    {
      "epoch": 0.2335936929702898,
      "grad_norm": 4.789867877960205,
      "learning_rate": 4.980533858919142e-05,
      "loss": 0.9677,
      "step": 25600
    },
    {
      "epoch": 0.234506168333455,
      "grad_norm": 4.181291580200195,
      "learning_rate": 4.980457819305546e-05,
      "loss": 0.9093,
      "step": 25700
    },
    {
      "epoch": 0.2354186436966202,
      "grad_norm": 4.2762370109558105,
      "learning_rate": 4.980381779691948e-05,
      "loss": 0.8947,
      "step": 25800
    },
    {
      "epoch": 0.23633111905978538,
      "grad_norm": 4.019443988800049,
      "learning_rate": 4.980305740078351e-05,
      "loss": 0.9093,
      "step": 25900
    },
    {
      "epoch": 0.2372435944229506,
      "grad_norm": 4.212311267852783,
      "learning_rate": 4.980229700464754e-05,
      "loss": 0.9365,
      "step": 26000
    },
    {
      "epoch": 0.23815606978611578,
      "grad_norm": 4.375726699829102,
      "learning_rate": 4.980153660851157e-05,
      "loss": 0.9183,
      "step": 26100
    },
    {
      "epoch": 0.23906854514928097,
      "grad_norm": 3.765324592590332,
      "learning_rate": 4.98007762123756e-05,
      "loss": 0.9195,
      "step": 26200
    },
    {
      "epoch": 0.23998102051244616,
      "grad_norm": 5.585471153259277,
      "learning_rate": 4.980001581623963e-05,
      "loss": 0.9002,
      "step": 26300
    },
    {
      "epoch": 0.24089349587561135,
      "grad_norm": 4.4977288246154785,
      "learning_rate": 4.9799255420103656e-05,
      "loss": 0.8802,
      "step": 26400
    },
    {
      "epoch": 0.24180597123877656,
      "grad_norm": 4.4167046546936035,
      "learning_rate": 4.979849502396769e-05,
      "loss": 0.9651,
      "step": 26500
    },
    {
      "epoch": 0.24271844660194175,
      "grad_norm": 4.669652462005615,
      "learning_rate": 4.9797734627831716e-05,
      "loss": 0.8902,
      "step": 26600
    },
    {
      "epoch": 0.24363092196510694,
      "grad_norm": 4.6663923263549805,
      "learning_rate": 4.9796974231695746e-05,
      "loss": 0.921,
      "step": 26700
    },
    {
      "epoch": 0.24454339732827213,
      "grad_norm": 4.178906440734863,
      "learning_rate": 4.9796213835559776e-05,
      "loss": 0.9442,
      "step": 26800
    },
    {
      "epoch": 0.24545587269143734,
      "grad_norm": 4.720544338226318,
      "learning_rate": 4.9795453439423806e-05,
      "loss": 0.9473,
      "step": 26900
    },
    {
      "epoch": 0.24636834805460253,
      "grad_norm": 4.748122215270996,
      "learning_rate": 4.979469304328783e-05,
      "loss": 0.9017,
      "step": 27000
    },
    {
      "epoch": 0.24728082341776772,
      "grad_norm": 5.51088285446167,
      "learning_rate": 4.9793932647151866e-05,
      "loss": 0.8987,
      "step": 27100
    },
    {
      "epoch": 0.2481932987809329,
      "grad_norm": 4.481117248535156,
      "learning_rate": 4.979317225101589e-05,
      "loss": 0.9288,
      "step": 27200
    },
    {
      "epoch": 0.2491057741440981,
      "grad_norm": 4.702326774597168,
      "learning_rate": 4.979241185487992e-05,
      "loss": 0.9532,
      "step": 27300
    },
    {
      "epoch": 0.2500182495072633,
      "grad_norm": 4.560130596160889,
      "learning_rate": 4.979165145874395e-05,
      "loss": 0.9278,
      "step": 27400
    },
    {
      "epoch": 0.2509307248704285,
      "grad_norm": 4.8340911865234375,
      "learning_rate": 4.979089106260798e-05,
      "loss": 0.9159,
      "step": 27500
    },
    {
      "epoch": 0.2518432002335937,
      "grad_norm": 4.6527581214904785,
      "learning_rate": 4.979013066647201e-05,
      "loss": 0.89,
      "step": 27600
    },
    {
      "epoch": 0.2527556755967589,
      "grad_norm": 5.054370880126953,
      "learning_rate": 4.978937027033604e-05,
      "loss": 0.9191,
      "step": 27700
    },
    {
      "epoch": 0.25366815095992407,
      "grad_norm": 4.943380355834961,
      "learning_rate": 4.978860987420006e-05,
      "loss": 0.9107,
      "step": 27800
    },
    {
      "epoch": 0.25458062632308925,
      "grad_norm": 4.376575946807861,
      "learning_rate": 4.97878494780641e-05,
      "loss": 0.9439,
      "step": 27900
    },
    {
      "epoch": 0.2554931016862545,
      "grad_norm": 4.658294677734375,
      "learning_rate": 4.978708908192812e-05,
      "loss": 0.9187,
      "step": 28000
    },
    {
      "epoch": 0.2564055770494197,
      "grad_norm": 4.349798202514648,
      "learning_rate": 4.978632868579215e-05,
      "loss": 0.9335,
      "step": 28100
    },
    {
      "epoch": 0.2573180524125849,
      "grad_norm": 4.690248489379883,
      "learning_rate": 4.978556828965618e-05,
      "loss": 0.8806,
      "step": 28200
    },
    {
      "epoch": 0.25823052777575006,
      "grad_norm": 4.783016681671143,
      "learning_rate": 4.978480789352021e-05,
      "loss": 0.9062,
      "step": 28300
    },
    {
      "epoch": 0.25914300313891525,
      "grad_norm": 3.916212558746338,
      "learning_rate": 4.9784047497384237e-05,
      "loss": 0.9299,
      "step": 28400
    },
    {
      "epoch": 0.26005547850208044,
      "grad_norm": 4.54948616027832,
      "learning_rate": 4.978328710124827e-05,
      "loss": 0.9369,
      "step": 28500
    },
    {
      "epoch": 0.2609679538652456,
      "grad_norm": 4.804208755493164,
      "learning_rate": 4.97825267051123e-05,
      "loss": 0.8957,
      "step": 28600
    },
    {
      "epoch": 0.2618804292284108,
      "grad_norm": 4.384028911590576,
      "learning_rate": 4.978176630897633e-05,
      "loss": 0.9187,
      "step": 28700
    },
    {
      "epoch": 0.262792904591576,
      "grad_norm": 4.76761531829834,
      "learning_rate": 4.978100591284036e-05,
      "loss": 0.9129,
      "step": 28800
    },
    {
      "epoch": 0.26370537995474125,
      "grad_norm": 4.6467108726501465,
      "learning_rate": 4.978024551670438e-05,
      "loss": 0.9465,
      "step": 28900
    },
    {
      "epoch": 0.26461785531790644,
      "grad_norm": 3.8399577140808105,
      "learning_rate": 4.977948512056842e-05,
      "loss": 0.9288,
      "step": 29000
    },
    {
      "epoch": 0.2655303306810716,
      "grad_norm": 4.463375091552734,
      "learning_rate": 4.977872472443244e-05,
      "loss": 0.9056,
      "step": 29100
    },
    {
      "epoch": 0.2664428060442368,
      "grad_norm": 4.988485336303711,
      "learning_rate": 4.977796432829647e-05,
      "loss": 0.899,
      "step": 29200
    },
    {
      "epoch": 0.267355281407402,
      "grad_norm": 4.688909530639648,
      "learning_rate": 4.97772039321605e-05,
      "loss": 0.8864,
      "step": 29300
    },
    {
      "epoch": 0.2682677567705672,
      "grad_norm": 4.3152995109558105,
      "learning_rate": 4.977644353602453e-05,
      "loss": 0.9072,
      "step": 29400
    },
    {
      "epoch": 0.2691802321337324,
      "grad_norm": 4.3116350173950195,
      "learning_rate": 4.9775683139888554e-05,
      "loss": 0.9427,
      "step": 29500
    },
    {
      "epoch": 0.27009270749689757,
      "grad_norm": 4.335596561431885,
      "learning_rate": 4.977492274375259e-05,
      "loss": 0.9206,
      "step": 29600
    },
    {
      "epoch": 0.27100518286006275,
      "grad_norm": 4.467284202575684,
      "learning_rate": 4.9774162347616614e-05,
      "loss": 0.9607,
      "step": 29700
    },
    {
      "epoch": 0.271917658223228,
      "grad_norm": 3.9060189723968506,
      "learning_rate": 4.9773401951480644e-05,
      "loss": 0.8862,
      "step": 29800
    },
    {
      "epoch": 0.2728301335863932,
      "grad_norm": 4.768402576446533,
      "learning_rate": 4.9772641555344674e-05,
      "loss": 0.8857,
      "step": 29900
    },
    {
      "epoch": 0.2737426089495584,
      "grad_norm": 4.459762096405029,
      "learning_rate": 4.9771881159208704e-05,
      "loss": 0.9054,
      "step": 30000
    },
    {
      "epoch": 0.27465508431272356,
      "grad_norm": 4.653633117675781,
      "learning_rate": 4.9771120763072734e-05,
      "loss": 0.9066,
      "step": 30100
    },
    {
      "epoch": 0.27556755967588875,
      "grad_norm": 4.18367338180542,
      "learning_rate": 4.9770360366936764e-05,
      "loss": 0.9193,
      "step": 30200
    },
    {
      "epoch": 0.27648003503905394,
      "grad_norm": 4.825470924377441,
      "learning_rate": 4.976959997080079e-05,
      "loss": 0.9371,
      "step": 30300
    },
    {
      "epoch": 0.27739251040221913,
      "grad_norm": 4.159131050109863,
      "learning_rate": 4.9768839574664824e-05,
      "loss": 0.9191,
      "step": 30400
    },
    {
      "epoch": 0.2783049857653843,
      "grad_norm": 4.357705116271973,
      "learning_rate": 4.976807917852885e-05,
      "loss": 0.9151,
      "step": 30500
    },
    {
      "epoch": 0.2792174611285495,
      "grad_norm": 4.516878128051758,
      "learning_rate": 4.976731878239288e-05,
      "loss": 0.9082,
      "step": 30600
    },
    {
      "epoch": 0.28012993649171475,
      "grad_norm": 4.130741596221924,
      "learning_rate": 4.976655838625691e-05,
      "loss": 0.9348,
      "step": 30700
    },
    {
      "epoch": 0.28104241185487994,
      "grad_norm": 5.64365816116333,
      "learning_rate": 4.976579799012094e-05,
      "loss": 0.9077,
      "step": 30800
    },
    {
      "epoch": 0.2819548872180451,
      "grad_norm": 5.270657539367676,
      "learning_rate": 4.976503759398496e-05,
      "loss": 0.9161,
      "step": 30900
    },
    {
      "epoch": 0.2828673625812103,
      "grad_norm": 4.2988433837890625,
      "learning_rate": 4.9764277197849e-05,
      "loss": 0.9027,
      "step": 31000
    },
    {
      "epoch": 0.2837798379443755,
      "grad_norm": 4.7207159996032715,
      "learning_rate": 4.976351680171302e-05,
      "loss": 0.9297,
      "step": 31100
    },
    {
      "epoch": 0.2846923133075407,
      "grad_norm": 4.913229465484619,
      "learning_rate": 4.976275640557705e-05,
      "loss": 0.9137,
      "step": 31200
    },
    {
      "epoch": 0.2856047886707059,
      "grad_norm": 5.042914867401123,
      "learning_rate": 4.976199600944108e-05,
      "loss": 0.9018,
      "step": 31300
    },
    {
      "epoch": 0.28651726403387107,
      "grad_norm": 4.106817245483398,
      "learning_rate": 4.9761235613305105e-05,
      "loss": 0.8918,
      "step": 31400
    },
    {
      "epoch": 0.28742973939703625,
      "grad_norm": 4.705385208129883,
      "learning_rate": 4.976047521716914e-05,
      "loss": 0.9187,
      "step": 31500
    },
    {
      "epoch": 0.2883422147602015,
      "grad_norm": 4.52363920211792,
      "learning_rate": 4.9759714821033165e-05,
      "loss": 0.9185,
      "step": 31600
    },
    {
      "epoch": 0.2892546901233667,
      "grad_norm": 6.609990119934082,
      "learning_rate": 4.9758954424897195e-05,
      "loss": 0.8964,
      "step": 31700
    },
    {
      "epoch": 0.2901671654865319,
      "grad_norm": 4.282615661621094,
      "learning_rate": 4.9758194028761225e-05,
      "loss": 0.849,
      "step": 31800
    },
    {
      "epoch": 0.29107964084969706,
      "grad_norm": 5.200261116027832,
      "learning_rate": 4.9757433632625255e-05,
      "loss": 0.8818,
      "step": 31900
    },
    {
      "epoch": 0.29199211621286225,
      "grad_norm": 4.476046085357666,
      "learning_rate": 4.9756673236489285e-05,
      "loss": 0.8459,
      "step": 32000
    },
    {
      "epoch": 0.29290459157602744,
      "grad_norm": 4.471263885498047,
      "learning_rate": 4.9755912840353315e-05,
      "loss": 0.8953,
      "step": 32100
    },
    {
      "epoch": 0.29381706693919263,
      "grad_norm": 4.071052074432373,
      "learning_rate": 4.975515244421734e-05,
      "loss": 0.8775,
      "step": 32200
    },
    {
      "epoch": 0.2947295423023578,
      "grad_norm": 4.314451694488525,
      "learning_rate": 4.975439204808137e-05,
      "loss": 0.9398,
      "step": 32300
    },
    {
      "epoch": 0.295642017665523,
      "grad_norm": 4.692590236663818,
      "learning_rate": 4.97536316519454e-05,
      "loss": 0.9109,
      "step": 32400
    },
    {
      "epoch": 0.29655449302868825,
      "grad_norm": 5.666123867034912,
      "learning_rate": 4.975287125580943e-05,
      "loss": 0.9275,
      "step": 32500
    },
    {
      "epoch": 0.29746696839185344,
      "grad_norm": 4.961183071136475,
      "learning_rate": 4.975211085967346e-05,
      "loss": 0.8861,
      "step": 32600
    },
    {
      "epoch": 0.2983794437550186,
      "grad_norm": 4.634074687957764,
      "learning_rate": 4.975135046353749e-05,
      "loss": 0.8925,
      "step": 32700
    },
    {
      "epoch": 0.2992919191181838,
      "grad_norm": 4.925207138061523,
      "learning_rate": 4.975059006740151e-05,
      "loss": 0.9123,
      "step": 32800
    },
    {
      "epoch": 0.300204394481349,
      "grad_norm": 4.109436988830566,
      "learning_rate": 4.974982967126555e-05,
      "loss": 0.9189,
      "step": 32900
    },
    {
      "epoch": 0.3011168698445142,
      "grad_norm": 4.280610084533691,
      "learning_rate": 4.974906927512957e-05,
      "loss": 0.8669,
      "step": 33000
    },
    {
      "epoch": 0.3020293452076794,
      "grad_norm": 4.4800639152526855,
      "learning_rate": 4.97483088789936e-05,
      "loss": 0.9592,
      "step": 33100
    },
    {
      "epoch": 0.30294182057084457,
      "grad_norm": 4.38597297668457,
      "learning_rate": 4.974754848285763e-05,
      "loss": 0.9264,
      "step": 33200
    },
    {
      "epoch": 0.30385429593400975,
      "grad_norm": 4.9118971824646,
      "learning_rate": 4.974678808672166e-05,
      "loss": 0.8853,
      "step": 33300
    },
    {
      "epoch": 0.304766771297175,
      "grad_norm": 4.53200101852417,
      "learning_rate": 4.974602769058569e-05,
      "loss": 0.8636,
      "step": 33400
    },
    {
      "epoch": 0.3056792466603402,
      "grad_norm": 4.4600043296813965,
      "learning_rate": 4.974526729444972e-05,
      "loss": 0.903,
      "step": 33500
    },
    {
      "epoch": 0.3065917220235054,
      "grad_norm": 4.253835678100586,
      "learning_rate": 4.9744506898313746e-05,
      "loss": 0.9222,
      "step": 33600
    },
    {
      "epoch": 0.30750419738667056,
      "grad_norm": 4.862013816833496,
      "learning_rate": 4.974374650217778e-05,
      "loss": 0.9217,
      "step": 33700
    },
    {
      "epoch": 0.30841667274983575,
      "grad_norm": 4.580506801605225,
      "learning_rate": 4.9742986106041806e-05,
      "loss": 0.9116,
      "step": 33800
    },
    {
      "epoch": 0.30932914811300094,
      "grad_norm": 4.423431873321533,
      "learning_rate": 4.9742225709905836e-05,
      "loss": 0.92,
      "step": 33900
    },
    {
      "epoch": 0.31024162347616613,
      "grad_norm": 5.329095363616943,
      "learning_rate": 4.9741465313769866e-05,
      "loss": 0.9626,
      "step": 34000
    },
    {
      "epoch": 0.3111540988393313,
      "grad_norm": 4.764700412750244,
      "learning_rate": 4.974070491763389e-05,
      "loss": 0.8949,
      "step": 34100
    },
    {
      "epoch": 0.3120665742024965,
      "grad_norm": 4.142714977264404,
      "learning_rate": 4.973994452149792e-05,
      "loss": 0.8719,
      "step": 34200
    },
    {
      "epoch": 0.31297904956566175,
      "grad_norm": 3.198824405670166,
      "learning_rate": 4.973918412536195e-05,
      "loss": 0.8895,
      "step": 34300
    },
    {
      "epoch": 0.31389152492882694,
      "grad_norm": 4.480228424072266,
      "learning_rate": 4.973842372922598e-05,
      "loss": 0.9172,
      "step": 34400
    },
    {
      "epoch": 0.3148040002919921,
      "grad_norm": 5.159276962280273,
      "learning_rate": 4.973766333309001e-05,
      "loss": 0.9189,
      "step": 34500
    },
    {
      "epoch": 0.3157164756551573,
      "grad_norm": 5.297529697418213,
      "learning_rate": 4.973690293695404e-05,
      "loss": 0.8928,
      "step": 34600
    },
    {
      "epoch": 0.3166289510183225,
      "grad_norm": 5.341277122497559,
      "learning_rate": 4.973614254081806e-05,
      "loss": 0.8936,
      "step": 34700
    },
    {
      "epoch": 0.3175414263814877,
      "grad_norm": 2.4304521083831787,
      "learning_rate": 4.97353821446821e-05,
      "loss": 0.8626,
      "step": 34800
    },
    {
      "epoch": 0.3184539017446529,
      "grad_norm": 4.8994832038879395,
      "learning_rate": 4.973462174854612e-05,
      "loss": 0.8871,
      "step": 34900
    },
    {
      "epoch": 0.31936637710781807,
      "grad_norm": 4.753241539001465,
      "learning_rate": 4.973386135241015e-05,
      "loss": 0.8844,
      "step": 35000
    },
    {
      "epoch": 0.3202788524709833,
      "grad_norm": 4.54789400100708,
      "learning_rate": 4.973310095627418e-05,
      "loss": 0.867,
      "step": 35100
    },
    {
      "epoch": 0.3211913278341485,
      "grad_norm": 4.878993034362793,
      "learning_rate": 4.973234056013821e-05,
      "loss": 0.9265,
      "step": 35200
    },
    {
      "epoch": 0.3221038031973137,
      "grad_norm": 4.349714279174805,
      "learning_rate": 4.973158016400224e-05,
      "loss": 0.9152,
      "step": 35300
    },
    {
      "epoch": 0.3230162785604789,
      "grad_norm": 5.729046821594238,
      "learning_rate": 4.9730819767866273e-05,
      "loss": 0.9415,
      "step": 35400
    },
    {
      "epoch": 0.32392875392364406,
      "grad_norm": 4.71878719329834,
      "learning_rate": 4.97300593717303e-05,
      "loss": 0.9281,
      "step": 35500
    },
    {
      "epoch": 0.32484122928680925,
      "grad_norm": 3.7945070266723633,
      "learning_rate": 4.972929897559433e-05,
      "loss": 0.8837,
      "step": 35600
    },
    {
      "epoch": 0.32575370464997444,
      "grad_norm": 4.2650370597839355,
      "learning_rate": 4.972853857945836e-05,
      "loss": 0.8979,
      "step": 35700
    },
    {
      "epoch": 0.32666618001313963,
      "grad_norm": 4.932172775268555,
      "learning_rate": 4.972777818332239e-05,
      "loss": 0.8952,
      "step": 35800
    },
    {
      "epoch": 0.3275786553763048,
      "grad_norm": 4.464069843292236,
      "learning_rate": 4.972701778718642e-05,
      "loss": 0.8673,
      "step": 35900
    },
    {
      "epoch": 0.32849113073947006,
      "grad_norm": 4.6190385818481445,
      "learning_rate": 4.972625739105045e-05,
      "loss": 0.9257,
      "step": 36000
    },
    {
      "epoch": 0.32940360610263525,
      "grad_norm": 4.717801570892334,
      "learning_rate": 4.972549699491447e-05,
      "loss": 0.8837,
      "step": 36100
    },
    {
      "epoch": 0.33031608146580044,
      "grad_norm": 3.7096059322357178,
      "learning_rate": 4.972473659877851e-05,
      "loss": 0.9212,
      "step": 36200
    },
    {
      "epoch": 0.3312285568289656,
      "grad_norm": 4.184340000152588,
      "learning_rate": 4.972397620264253e-05,
      "loss": 0.8652,
      "step": 36300
    },
    {
      "epoch": 0.3321410321921308,
      "grad_norm": 3.910094738006592,
      "learning_rate": 4.972321580650656e-05,
      "loss": 0.8887,
      "step": 36400
    },
    {
      "epoch": 0.333053507555296,
      "grad_norm": 4.4064788818359375,
      "learning_rate": 4.972245541037059e-05,
      "loss": 0.8979,
      "step": 36500
    },
    {
      "epoch": 0.3339659829184612,
      "grad_norm": 5.048665523529053,
      "learning_rate": 4.972169501423462e-05,
      "loss": 0.9058,
      "step": 36600
    },
    {
      "epoch": 0.3348784582816264,
      "grad_norm": 4.531785011291504,
      "learning_rate": 4.9720934618098644e-05,
      "loss": 0.9083,
      "step": 36700
    },
    {
      "epoch": 0.33579093364479157,
      "grad_norm": 4.709320068359375,
      "learning_rate": 4.972017422196268e-05,
      "loss": 0.9065,
      "step": 36800
    },
    {
      "epoch": 0.3367034090079568,
      "grad_norm": 4.2103047370910645,
      "learning_rate": 4.9719413825826704e-05,
      "loss": 0.8597,
      "step": 36900
    },
    {
      "epoch": 0.337615884371122,
      "grad_norm": 4.6175408363342285,
      "learning_rate": 4.9718653429690734e-05,
      "loss": 0.9488,
      "step": 37000
    },
    {
      "epoch": 0.3385283597342872,
      "grad_norm": 4.566606044769287,
      "learning_rate": 4.9717893033554764e-05,
      "loss": 0.9019,
      "step": 37100
    },
    {
      "epoch": 0.3394408350974524,
      "grad_norm": 3.789228677749634,
      "learning_rate": 4.971713263741879e-05,
      "loss": 0.9144,
      "step": 37200
    },
    {
      "epoch": 0.34035331046061756,
      "grad_norm": 5.7482452392578125,
      "learning_rate": 4.9716372241282824e-05,
      "loss": 0.9372,
      "step": 37300
    },
    {
      "epoch": 0.34126578582378275,
      "grad_norm": 4.965156078338623,
      "learning_rate": 4.971561184514685e-05,
      "loss": 0.8538,
      "step": 37400
    },
    {
      "epoch": 0.34217826118694794,
      "grad_norm": 5.5746612548828125,
      "learning_rate": 4.971485144901088e-05,
      "loss": 0.8689,
      "step": 37500
    },
    {
      "epoch": 0.34309073655011313,
      "grad_norm": 4.351333141326904,
      "learning_rate": 4.971409105287491e-05,
      "loss": 0.8968,
      "step": 37600
    },
    {
      "epoch": 0.3440032119132783,
      "grad_norm": 4.176421642303467,
      "learning_rate": 4.971333065673894e-05,
      "loss": 0.9027,
      "step": 37700
    },
    {
      "epoch": 0.34491568727644356,
      "grad_norm": 4.266711711883545,
      "learning_rate": 4.971257026060296e-05,
      "loss": 0.8639,
      "step": 37800
    },
    {
      "epoch": 0.34582816263960875,
      "grad_norm": 4.173541069030762,
      "learning_rate": 4.9711809864467e-05,
      "loss": 0.9033,
      "step": 37900
    },
    {
      "epoch": 0.34674063800277394,
      "grad_norm": 3.9787144660949707,
      "learning_rate": 4.971104946833102e-05,
      "loss": 0.9045,
      "step": 38000
    },
    {
      "epoch": 0.3476531133659391,
      "grad_norm": 4.89572286605835,
      "learning_rate": 4.971028907219505e-05,
      "loss": 0.9167,
      "step": 38100
    },
    {
      "epoch": 0.3485655887291043,
      "grad_norm": 3.9807584285736084,
      "learning_rate": 4.970952867605908e-05,
      "loss": 0.9045,
      "step": 38200
    },
    {
      "epoch": 0.3494780640922695,
      "grad_norm": 4.6888651847839355,
      "learning_rate": 4.970876827992311e-05,
      "loss": 0.8575,
      "step": 38300
    },
    {
      "epoch": 0.3503905394554347,
      "grad_norm": 5.020137786865234,
      "learning_rate": 4.970800788378714e-05,
      "loss": 0.899,
      "step": 38400
    },
    {
      "epoch": 0.3513030148185999,
      "grad_norm": 5.1826491355896,
      "learning_rate": 4.970724748765117e-05,
      "loss": 0.8992,
      "step": 38500
    },
    {
      "epoch": 0.35221549018176507,
      "grad_norm": 3.8036251068115234,
      "learning_rate": 4.9706487091515195e-05,
      "loss": 0.9153,
      "step": 38600
    },
    {
      "epoch": 0.3531279655449303,
      "grad_norm": 4.78696870803833,
      "learning_rate": 4.970572669537923e-05,
      "loss": 0.9076,
      "step": 38700
    },
    {
      "epoch": 0.3540404409080955,
      "grad_norm": 4.6873321533203125,
      "learning_rate": 4.9704966299243255e-05,
      "loss": 0.8777,
      "step": 38800
    },
    {
      "epoch": 0.3549529162712607,
      "grad_norm": 4.991820335388184,
      "learning_rate": 4.9704205903107285e-05,
      "loss": 0.8797,
      "step": 38900
    },
    {
      "epoch": 0.3558653916344259,
      "grad_norm": 4.059432506561279,
      "learning_rate": 4.9703445506971315e-05,
      "loss": 0.9207,
      "step": 39000
    },
    {
      "epoch": 0.35677786699759106,
      "grad_norm": 4.309594631195068,
      "learning_rate": 4.9702685110835345e-05,
      "loss": 0.9281,
      "step": 39100
    },
    {
      "epoch": 0.35769034236075625,
      "grad_norm": 4.714588642120361,
      "learning_rate": 4.970192471469937e-05,
      "loss": 0.9001,
      "step": 39200
    },
    {
      "epoch": 0.35860281772392144,
      "grad_norm": 4.999495506286621,
      "learning_rate": 4.9701164318563405e-05,
      "loss": 0.9152,
      "step": 39300
    },
    {
      "epoch": 0.35951529308708663,
      "grad_norm": 5.306675434112549,
      "learning_rate": 4.970040392242743e-05,
      "loss": 0.8891,
      "step": 39400
    },
    {
      "epoch": 0.3604277684502518,
      "grad_norm": 4.625689506530762,
      "learning_rate": 4.969964352629146e-05,
      "loss": 0.8928,
      "step": 39500
    },
    {
      "epoch": 0.36134024381341706,
      "grad_norm": 5.034453868865967,
      "learning_rate": 4.969888313015549e-05,
      "loss": 0.8674,
      "step": 39600
    },
    {
      "epoch": 0.36225271917658225,
      "grad_norm": 5.5609636306762695,
      "learning_rate": 4.969812273401952e-05,
      "loss": 0.8806,
      "step": 39700
    },
    {
      "epoch": 0.36316519453974744,
      "grad_norm": 4.371728897094727,
      "learning_rate": 4.969736233788355e-05,
      "loss": 0.8712,
      "step": 39800
    },
    {
      "epoch": 0.3640776699029126,
      "grad_norm": 3.9664719104766846,
      "learning_rate": 4.969660194174757e-05,
      "loss": 0.8901,
      "step": 39900
    },
    {
      "epoch": 0.3649901452660778,
      "grad_norm": 4.0825676918029785,
      "learning_rate": 4.96958415456116e-05,
      "loss": 0.936,
      "step": 40000
    },
    {
      "epoch": 0.365902620629243,
      "grad_norm": 5.042921543121338,
      "learning_rate": 4.969508114947563e-05,
      "loss": 0.872,
      "step": 40100
    },
    {
      "epoch": 0.3668150959924082,
      "grad_norm": 4.428056240081787,
      "learning_rate": 4.969432075333966e-05,
      "loss": 0.8988,
      "step": 40200
    },
    {
      "epoch": 0.3677275713555734,
      "grad_norm": 5.246150970458984,
      "learning_rate": 4.9693560357203686e-05,
      "loss": 0.8383,
      "step": 40300
    },
    {
      "epoch": 0.36864004671873857,
      "grad_norm": 4.217075347900391,
      "learning_rate": 4.969279996106772e-05,
      "loss": 0.8547,
      "step": 40400
    },
    {
      "epoch": 0.3695525220819038,
      "grad_norm": 5.625909805297852,
      "learning_rate": 4.9692039564931746e-05,
      "loss": 0.9019,
      "step": 40500
    },
    {
      "epoch": 0.370464997445069,
      "grad_norm": 4.623375415802002,
      "learning_rate": 4.9691279168795776e-05,
      "loss": 0.865,
      "step": 40600
    },
    {
      "epoch": 0.3713774728082342,
      "grad_norm": 4.710708141326904,
      "learning_rate": 4.9690518772659806e-05,
      "loss": 0.881,
      "step": 40700
    },
    {
      "epoch": 0.3722899481713994,
      "grad_norm": 4.697977066040039,
      "learning_rate": 4.9689758376523836e-05,
      "loss": 0.8938,
      "step": 40800
    },
    {
      "epoch": 0.37320242353456456,
      "grad_norm": 4.718133926391602,
      "learning_rate": 4.9688997980387866e-05,
      "loss": 0.8936,
      "step": 40900
    },
    {
      "epoch": 0.37411489889772975,
      "grad_norm": 3.9947893619537354,
      "learning_rate": 4.9688237584251896e-05,
      "loss": 0.8304,
      "step": 41000
    },
    {
      "epoch": 0.37502737426089494,
      "grad_norm": 4.798572540283203,
      "learning_rate": 4.968747718811592e-05,
      "loss": 0.8499,
      "step": 41100
    },
    {
      "epoch": 0.37593984962406013,
      "grad_norm": 4.331518650054932,
      "learning_rate": 4.9686716791979956e-05,
      "loss": 0.9083,
      "step": 41200
    },
    {
      "epoch": 0.3768523249872253,
      "grad_norm": 4.585838317871094,
      "learning_rate": 4.968595639584398e-05,
      "loss": 0.897,
      "step": 41300
    },
    {
      "epoch": 0.37776480035039056,
      "grad_norm": 4.866661071777344,
      "learning_rate": 4.968519599970801e-05,
      "loss": 0.8664,
      "step": 41400
    },
    {
      "epoch": 0.37867727571355575,
      "grad_norm": 4.746554851531982,
      "learning_rate": 4.968443560357204e-05,
      "loss": 0.9252,
      "step": 41500
    },
    {
      "epoch": 0.37958975107672094,
      "grad_norm": 4.368016242980957,
      "learning_rate": 4.968367520743607e-05,
      "loss": 0.9125,
      "step": 41600
    },
    {
      "epoch": 0.3805022264398861,
      "grad_norm": 4.517402648925781,
      "learning_rate": 4.968291481130009e-05,
      "loss": 0.9127,
      "step": 41700
    },
    {
      "epoch": 0.3814147018030513,
      "grad_norm": 5.628933906555176,
      "learning_rate": 4.968215441516413e-05,
      "loss": 0.8985,
      "step": 41800
    },
    {
      "epoch": 0.3823271771662165,
      "grad_norm": 4.5241007804870605,
      "learning_rate": 4.968139401902815e-05,
      "loss": 0.8912,
      "step": 41900
    },
    {
      "epoch": 0.3832396525293817,
      "grad_norm": 4.568142890930176,
      "learning_rate": 4.9680633622892183e-05,
      "loss": 0.8903,
      "step": 42000
    },
    {
      "epoch": 0.3841521278925469,
      "grad_norm": 4.476412296295166,
      "learning_rate": 4.9679873226756213e-05,
      "loss": 0.857,
      "step": 42100
    },
    {
      "epoch": 0.3850646032557121,
      "grad_norm": 2.7831857204437256,
      "learning_rate": 4.9679112830620244e-05,
      "loss": 0.8769,
      "step": 42200
    },
    {
      "epoch": 0.3859770786188773,
      "grad_norm": 5.24916934967041,
      "learning_rate": 4.9678352434484274e-05,
      "loss": 0.9196,
      "step": 42300
    },
    {
      "epoch": 0.3868895539820425,
      "grad_norm": 4.86456298828125,
      "learning_rate": 4.9677592038348304e-05,
      "loss": 0.9068,
      "step": 42400
    },
    {
      "epoch": 0.3878020293452077,
      "grad_norm": 4.571994304656982,
      "learning_rate": 4.967683164221233e-05,
      "loss": 0.8774,
      "step": 42500
    },
    {
      "epoch": 0.3887145047083729,
      "grad_norm": 4.412722110748291,
      "learning_rate": 4.9676071246076364e-05,
      "loss": 0.8524,
      "step": 42600
    },
    {
      "epoch": 0.38962698007153806,
      "grad_norm": 4.157369613647461,
      "learning_rate": 4.967531084994039e-05,
      "loss": 0.8435,
      "step": 42700
    },
    {
      "epoch": 0.39053945543470325,
      "grad_norm": 4.4892120361328125,
      "learning_rate": 4.967455045380441e-05,
      "loss": 0.8702,
      "step": 42800
    },
    {
      "epoch": 0.39145193079786844,
      "grad_norm": 4.505863189697266,
      "learning_rate": 4.967379005766845e-05,
      "loss": 0.8508,
      "step": 42900
    },
    {
      "epoch": 0.39236440616103363,
      "grad_norm": 4.1829423904418945,
      "learning_rate": 4.967302966153247e-05,
      "loss": 0.8671,
      "step": 43000
    },
    {
      "epoch": 0.3932768815241989,
      "grad_norm": 4.7080278396606445,
      "learning_rate": 4.96722692653965e-05,
      "loss": 0.8897,
      "step": 43100
    },
    {
      "epoch": 0.39418935688736406,
      "grad_norm": 4.643974304199219,
      "learning_rate": 4.967150886926053e-05,
      "loss": 0.8633,
      "step": 43200
    },
    {
      "epoch": 0.39510183225052925,
      "grad_norm": 4.434110641479492,
      "learning_rate": 4.967074847312456e-05,
      "loss": 0.832,
      "step": 43300
    },
    {
      "epoch": 0.39601430761369444,
      "grad_norm": 4.552793025970459,
      "learning_rate": 4.966998807698859e-05,
      "loss": 0.8829,
      "step": 43400
    },
    {
      "epoch": 0.3969267829768596,
      "grad_norm": 5.2778520584106445,
      "learning_rate": 4.966922768085262e-05,
      "loss": 0.883,
      "step": 43500
    },
    {
      "epoch": 0.3978392583400248,
      "grad_norm": 4.432491779327393,
      "learning_rate": 4.9668467284716644e-05,
      "loss": 0.8835,
      "step": 43600
    },
    {
      "epoch": 0.39875173370319,
      "grad_norm": 4.510258197784424,
      "learning_rate": 4.966770688858068e-05,
      "loss": 0.8836,
      "step": 43700
    },
    {
      "epoch": 0.3996642090663552,
      "grad_norm": 4.633664608001709,
      "learning_rate": 4.9666946492444704e-05,
      "loss": 0.862,
      "step": 43800
    },
    {
      "epoch": 0.4005766844295204,
      "grad_norm": 4.653357982635498,
      "learning_rate": 4.9666186096308734e-05,
      "loss": 0.9026,
      "step": 43900
    },
    {
      "epoch": 0.4014891597926856,
      "grad_norm": 4.031081199645996,
      "learning_rate": 4.9665425700172764e-05,
      "loss": 0.8712,
      "step": 44000
    },
    {
      "epoch": 0.4024016351558508,
      "grad_norm": 4.924448490142822,
      "learning_rate": 4.9664665304036794e-05,
      "loss": 0.8702,
      "step": 44100
    },
    {
      "epoch": 0.403314110519016,
      "grad_norm": 3.8846588134765625,
      "learning_rate": 4.9663904907900825e-05,
      "loss": 0.8729,
      "step": 44200
    },
    {
      "epoch": 0.4042265858821812,
      "grad_norm": 4.277509689331055,
      "learning_rate": 4.9663144511764855e-05,
      "loss": 0.8981,
      "step": 44300
    },
    {
      "epoch": 0.4051390612453464,
      "grad_norm": 4.174125671386719,
      "learning_rate": 4.966238411562888e-05,
      "loss": 0.8432,
      "step": 44400
    },
    {
      "epoch": 0.40605153660851157,
      "grad_norm": 4.5955705642700195,
      "learning_rate": 4.966162371949291e-05,
      "loss": 0.8348,
      "step": 44500
    },
    {
      "epoch": 0.40696401197167675,
      "grad_norm": 4.392089366912842,
      "learning_rate": 4.966086332335694e-05,
      "loss": 0.8936,
      "step": 44600
    },
    {
      "epoch": 0.40787648733484194,
      "grad_norm": 4.425148963928223,
      "learning_rate": 4.966010292722097e-05,
      "loss": 0.8979,
      "step": 44700
    },
    {
      "epoch": 0.40878896269800713,
      "grad_norm": 4.4380340576171875,
      "learning_rate": 4.9659342531085e-05,
      "loss": 0.8273,
      "step": 44800
    },
    {
      "epoch": 0.4097014380611724,
      "grad_norm": 3.6492795944213867,
      "learning_rate": 4.965858213494903e-05,
      "loss": 0.897,
      "step": 44900
    },
    {
      "epoch": 0.41061391342433756,
      "grad_norm": 3.964513063430786,
      "learning_rate": 4.965782173881305e-05,
      "loss": 0.8875,
      "step": 45000
    },
    {
      "epoch": 0.41152638878750275,
      "grad_norm": 4.373297691345215,
      "learning_rate": 4.965706134267709e-05,
      "loss": 0.9014,
      "step": 45100
    },
    {
      "epoch": 0.41243886415066794,
      "grad_norm": 4.890140056610107,
      "learning_rate": 4.965630094654111e-05,
      "loss": 0.8874,
      "step": 45200
    },
    {
      "epoch": 0.4133513395138331,
      "grad_norm": 4.095095157623291,
      "learning_rate": 4.965554055040514e-05,
      "loss": 0.8891,
      "step": 45300
    },
    {
      "epoch": 0.4142638148769983,
      "grad_norm": 4.763930320739746,
      "learning_rate": 4.965478015426917e-05,
      "loss": 0.917,
      "step": 45400
    },
    {
      "epoch": 0.4151762902401635,
      "grad_norm": 5.133937358856201,
      "learning_rate": 4.9654019758133195e-05,
      "loss": 0.8691,
      "step": 45500
    },
    {
      "epoch": 0.4160887656033287,
      "grad_norm": 4.732337951660156,
      "learning_rate": 4.965325936199723e-05,
      "loss": 0.8811,
      "step": 45600
    },
    {
      "epoch": 0.4170012409664939,
      "grad_norm": 4.955063819885254,
      "learning_rate": 4.9652498965861255e-05,
      "loss": 0.9,
      "step": 45700
    },
    {
      "epoch": 0.4179137163296591,
      "grad_norm": 5.280301570892334,
      "learning_rate": 4.9651738569725285e-05,
      "loss": 0.8001,
      "step": 45800
    },
    {
      "epoch": 0.4188261916928243,
      "grad_norm": 4.235603332519531,
      "learning_rate": 4.9650978173589315e-05,
      "loss": 0.8838,
      "step": 45900
    },
    {
      "epoch": 0.4197386670559895,
      "grad_norm": 5.241540908813477,
      "learning_rate": 4.9650217777453345e-05,
      "loss": 0.8626,
      "step": 46000
    },
    {
      "epoch": 0.4206511424191547,
      "grad_norm": 3.946324110031128,
      "learning_rate": 4.964945738131737e-05,
      "loss": 0.891,
      "step": 46100
    },
    {
      "epoch": 0.4215636177823199,
      "grad_norm": 4.390148162841797,
      "learning_rate": 4.9648696985181406e-05,
      "loss": 0.8677,
      "step": 46200
    },
    {
      "epoch": 0.42247609314548507,
      "grad_norm": 4.006017208099365,
      "learning_rate": 4.964793658904543e-05,
      "loss": 0.8826,
      "step": 46300
    },
    {
      "epoch": 0.42338856850865025,
      "grad_norm": 4.614943027496338,
      "learning_rate": 4.964717619290946e-05,
      "loss": 0.8957,
      "step": 46400
    },
    {
      "epoch": 0.42430104387181544,
      "grad_norm": 4.1795125007629395,
      "learning_rate": 4.964641579677349e-05,
      "loss": 0.8799,
      "step": 46500
    },
    {
      "epoch": 0.42521351923498063,
      "grad_norm": 4.405416011810303,
      "learning_rate": 4.964565540063752e-05,
      "loss": 0.8941,
      "step": 46600
    },
    {
      "epoch": 0.4261259945981459,
      "grad_norm": 4.478877067565918,
      "learning_rate": 4.964489500450155e-05,
      "loss": 0.8514,
      "step": 46700
    },
    {
      "epoch": 0.42703846996131106,
      "grad_norm": 5.099846363067627,
      "learning_rate": 4.964413460836558e-05,
      "loss": 0.8637,
      "step": 46800
    },
    {
      "epoch": 0.42795094532447625,
      "grad_norm": 4.584811687469482,
      "learning_rate": 4.96433742122296e-05,
      "loss": 0.8368,
      "step": 46900
    },
    {
      "epoch": 0.42886342068764144,
      "grad_norm": 4.4374260902404785,
      "learning_rate": 4.964261381609364e-05,
      "loss": 0.9167,
      "step": 47000
    },
    {
      "epoch": 0.4297758960508066,
      "grad_norm": 3.5268898010253906,
      "learning_rate": 4.964185341995766e-05,
      "loss": 0.8616,
      "step": 47100
    },
    {
      "epoch": 0.4306883714139718,
      "grad_norm": 6.861501216888428,
      "learning_rate": 4.964109302382169e-05,
      "loss": 0.8854,
      "step": 47200
    },
    {
      "epoch": 0.431600846777137,
      "grad_norm": 4.464598655700684,
      "learning_rate": 4.964033262768572e-05,
      "loss": 0.9265,
      "step": 47300
    },
    {
      "epoch": 0.4325133221403022,
      "grad_norm": 5.6657843589782715,
      "learning_rate": 4.963957223154975e-05,
      "loss": 0.8885,
      "step": 47400
    },
    {
      "epoch": 0.4334257975034674,
      "grad_norm": 4.028448581695557,
      "learning_rate": 4.9638811835413776e-05,
      "loss": 0.8986,
      "step": 47500
    },
    {
      "epoch": 0.4343382728666326,
      "grad_norm": 4.534821033477783,
      "learning_rate": 4.963805143927781e-05,
      "loss": 0.8098,
      "step": 47600
    },
    {
      "epoch": 0.4352507482297978,
      "grad_norm": 4.72796106338501,
      "learning_rate": 4.9637291043141836e-05,
      "loss": 0.8694,
      "step": 47700
    },
    {
      "epoch": 0.436163223592963,
      "grad_norm": 3.8419272899627686,
      "learning_rate": 4.9636530647005866e-05,
      "loss": 0.8765,
      "step": 47800
    },
    {
      "epoch": 0.4370756989561282,
      "grad_norm": 4.474198341369629,
      "learning_rate": 4.9635770250869896e-05,
      "loss": 0.8611,
      "step": 47900
    },
    {
      "epoch": 0.4379881743192934,
      "grad_norm": 4.743485450744629,
      "learning_rate": 4.9635009854733926e-05,
      "loss": 0.8977,
      "step": 48000
    },
    {
      "epoch": 0.43890064968245857,
      "grad_norm": 3.7226877212524414,
      "learning_rate": 4.9634249458597957e-05,
      "loss": 0.8372,
      "step": 48100
    },
    {
      "epoch": 0.43981312504562375,
      "grad_norm": 4.672524929046631,
      "learning_rate": 4.9633489062461987e-05,
      "loss": 0.8812,
      "step": 48200
    },
    {
      "epoch": 0.44072560040878894,
      "grad_norm": 4.201761722564697,
      "learning_rate": 4.963272866632601e-05,
      "loss": 0.8582,
      "step": 48300
    },
    {
      "epoch": 0.44163807577195413,
      "grad_norm": 3.4361767768859863,
      "learning_rate": 4.963196827019004e-05,
      "loss": 0.8902,
      "step": 48400
    },
    {
      "epoch": 0.4425505511351194,
      "grad_norm": 4.349246978759766,
      "learning_rate": 4.963120787405407e-05,
      "loss": 0.8658,
      "step": 48500
    },
    {
      "epoch": 0.44346302649828456,
      "grad_norm": 3.91119122505188,
      "learning_rate": 4.963044747791809e-05,
      "loss": 0.8722,
      "step": 48600
    },
    {
      "epoch": 0.44437550186144975,
      "grad_norm": 4.0265302658081055,
      "learning_rate": 4.962968708178213e-05,
      "loss": 0.8389,
      "step": 48700
    },
    {
      "epoch": 0.44528797722461494,
      "grad_norm": 4.262842178344727,
      "learning_rate": 4.9628926685646153e-05,
      "loss": 0.9005,
      "step": 48800
    },
    {
      "epoch": 0.4462004525877801,
      "grad_norm": 3.947206497192383,
      "learning_rate": 4.9628166289510183e-05,
      "loss": 0.8582,
      "step": 48900
    },
    {
      "epoch": 0.4471129279509453,
      "grad_norm": 5.133264541625977,
      "learning_rate": 4.9627405893374214e-05,
      "loss": 0.8664,
      "step": 49000
    },
    {
      "epoch": 0.4480254033141105,
      "grad_norm": 4.602713108062744,
      "learning_rate": 4.9626645497238244e-05,
      "loss": 0.8407,
      "step": 49100
    },
    {
      "epoch": 0.4489378786772757,
      "grad_norm": 5.097743511199951,
      "learning_rate": 4.9625885101102274e-05,
      "loss": 0.8233,
      "step": 49200
    },
    {
      "epoch": 0.4498503540404409,
      "grad_norm": 4.329585075378418,
      "learning_rate": 4.9625124704966304e-05,
      "loss": 0.8809,
      "step": 49300
    },
    {
      "epoch": 0.4507628294036061,
      "grad_norm": 4.959794998168945,
      "learning_rate": 4.962436430883033e-05,
      "loss": 0.9102,
      "step": 49400
    },
    {
      "epoch": 0.4516753047667713,
      "grad_norm": 4.065136432647705,
      "learning_rate": 4.9623603912694364e-05,
      "loss": 0.9001,
      "step": 49500
    },
    {
      "epoch": 0.4525877801299365,
      "grad_norm": 3.835566282272339,
      "learning_rate": 4.962284351655839e-05,
      "loss": 0.8913,
      "step": 49600
    },
    {
      "epoch": 0.4535002554931017,
      "grad_norm": 4.032618999481201,
      "learning_rate": 4.962208312042242e-05,
      "loss": 0.8797,
      "step": 49700
    },
    {
      "epoch": 0.4544127308562669,
      "grad_norm": 4.375158786773682,
      "learning_rate": 4.962132272428645e-05,
      "loss": 0.8545,
      "step": 49800
    },
    {
      "epoch": 0.45532520621943207,
      "grad_norm": 4.743941307067871,
      "learning_rate": 4.962056232815048e-05,
      "loss": 0.8557,
      "step": 49900
    },
    {
      "epoch": 0.45623768158259725,
      "grad_norm": 4.6911420822143555,
      "learning_rate": 4.96198019320145e-05,
      "loss": 0.8766,
      "step": 50000
    },
    {
      "epoch": 0.45715015694576244,
      "grad_norm": 4.16055154800415,
      "learning_rate": 4.961904153587854e-05,
      "loss": 0.8524,
      "step": 50100
    },
    {
      "epoch": 0.4580626323089277,
      "grad_norm": 5.270699501037598,
      "learning_rate": 4.961828113974256e-05,
      "loss": 0.8752,
      "step": 50200
    },
    {
      "epoch": 0.4589751076720929,
      "grad_norm": 4.305854320526123,
      "learning_rate": 4.961752074360659e-05,
      "loss": 0.8849,
      "step": 50300
    },
    {
      "epoch": 0.45988758303525806,
      "grad_norm": 3.8314645290374756,
      "learning_rate": 4.961676034747062e-05,
      "loss": 0.8844,
      "step": 50400
    },
    {
      "epoch": 0.46080005839842325,
      "grad_norm": 4.599510192871094,
      "learning_rate": 4.961599995133465e-05,
      "loss": 0.8581,
      "step": 50500
    },
    {
      "epoch": 0.46171253376158844,
      "grad_norm": 4.226564407348633,
      "learning_rate": 4.961523955519868e-05,
      "loss": 0.8616,
      "step": 50600
    },
    {
      "epoch": 0.46262500912475363,
      "grad_norm": 4.387575149536133,
      "learning_rate": 4.961447915906271e-05,
      "loss": 0.8248,
      "step": 50700
    },
    {
      "epoch": 0.4635374844879188,
      "grad_norm": 3.9704208374023438,
      "learning_rate": 4.9613718762926734e-05,
      "loss": 0.8291,
      "step": 50800
    },
    {
      "epoch": 0.464449959851084,
      "grad_norm": 4.662217617034912,
      "learning_rate": 4.961295836679077e-05,
      "loss": 0.8865,
      "step": 50900
    },
    {
      "epoch": 0.4653624352142492,
      "grad_norm": 4.586413860321045,
      "learning_rate": 4.9612197970654795e-05,
      "loss": 0.8275,
      "step": 51000
    },
    {
      "epoch": 0.46627491057741444,
      "grad_norm": 5.735574722290039,
      "learning_rate": 4.9611437574518825e-05,
      "loss": 0.8716,
      "step": 51100
    },
    {
      "epoch": 0.4671873859405796,
      "grad_norm": 4.191526889801025,
      "learning_rate": 4.9610677178382855e-05,
      "loss": 0.8569,
      "step": 51200
    },
    {
      "epoch": 0.4680998613037448,
      "grad_norm": 4.667941570281982,
      "learning_rate": 4.960991678224688e-05,
      "loss": 0.8584,
      "step": 51300
    },
    {
      "epoch": 0.46901233666691,
      "grad_norm": 4.074472904205322,
      "learning_rate": 4.960915638611091e-05,
      "loss": 0.847,
      "step": 51400
    },
    {
      "epoch": 0.4699248120300752,
      "grad_norm": 5.005359649658203,
      "learning_rate": 4.960839598997494e-05,
      "loss": 0.884,
      "step": 51500
    },
    {
      "epoch": 0.4708372873932404,
      "grad_norm": 4.093027114868164,
      "learning_rate": 4.960763559383897e-05,
      "loss": 0.8513,
      "step": 51600
    },
    {
      "epoch": 0.47174976275640557,
      "grad_norm": 4.188758373260498,
      "learning_rate": 4.9606875197703e-05,
      "loss": 0.8638,
      "step": 51700
    },
    {
      "epoch": 0.47266223811957075,
      "grad_norm": 5.01624870300293,
      "learning_rate": 4.960611480156703e-05,
      "loss": 0.8834,
      "step": 51800
    },
    {
      "epoch": 0.47357471348273594,
      "grad_norm": 5.136876106262207,
      "learning_rate": 4.960535440543105e-05,
      "loss": 0.8543,
      "step": 51900
    },
    {
      "epoch": 0.4744871888459012,
      "grad_norm": 4.6284003257751465,
      "learning_rate": 4.960459400929509e-05,
      "loss": 0.8592,
      "step": 52000
    },
    {
      "epoch": 0.4753996642090664,
      "grad_norm": 4.87851619720459,
      "learning_rate": 4.960383361315911e-05,
      "loss": 0.9007,
      "step": 52100
    },
    {
      "epoch": 0.47631213957223156,
      "grad_norm": 4.348798751831055,
      "learning_rate": 4.960307321702314e-05,
      "loss": 0.8829,
      "step": 52200
    },
    {
      "epoch": 0.47722461493539675,
      "grad_norm": 3.838481903076172,
      "learning_rate": 4.960231282088717e-05,
      "loss": 0.8817,
      "step": 52300
    },
    {
      "epoch": 0.47813709029856194,
      "grad_norm": 5.301892280578613,
      "learning_rate": 4.96015524247512e-05,
      "loss": 0.8669,
      "step": 52400
    },
    {
      "epoch": 0.47904956566172713,
      "grad_norm": 4.176852226257324,
      "learning_rate": 4.9600792028615225e-05,
      "loss": 0.8615,
      "step": 52500
    },
    {
      "epoch": 0.4799620410248923,
      "grad_norm": 4.115821838378906,
      "learning_rate": 4.960003163247926e-05,
      "loss": 0.8446,
      "step": 52600
    },
    {
      "epoch": 0.4808745163880575,
      "grad_norm": 3.7063019275665283,
      "learning_rate": 4.9599271236343285e-05,
      "loss": 0.8831,
      "step": 52700
    },
    {
      "epoch": 0.4817869917512227,
      "grad_norm": 4.367730617523193,
      "learning_rate": 4.9598510840207315e-05,
      "loss": 0.8723,
      "step": 52800
    },
    {
      "epoch": 0.48269946711438794,
      "grad_norm": 4.724912643432617,
      "learning_rate": 4.9597750444071346e-05,
      "loss": 0.8505,
      "step": 52900
    },
    {
      "epoch": 0.4836119424775531,
      "grad_norm": 4.3315958976745605,
      "learning_rate": 4.9596990047935376e-05,
      "loss": 0.8412,
      "step": 53000
    },
    {
      "epoch": 0.4845244178407183,
      "grad_norm": 5.084322929382324,
      "learning_rate": 4.9596229651799406e-05,
      "loss": 0.8568,
      "step": 53100
    },
    {
      "epoch": 0.4854368932038835,
      "grad_norm": 4.21904182434082,
      "learning_rate": 4.9595469255663436e-05,
      "loss": 0.9187,
      "step": 53200
    },
    {
      "epoch": 0.4863493685670487,
      "grad_norm": 3.706472158432007,
      "learning_rate": 4.959470885952746e-05,
      "loss": 0.8374,
      "step": 53300
    },
    {
      "epoch": 0.4872618439302139,
      "grad_norm": 4.172347068786621,
      "learning_rate": 4.9593948463391496e-05,
      "loss": 0.8188,
      "step": 53400
    },
    {
      "epoch": 0.48817431929337907,
      "grad_norm": 4.471406936645508,
      "learning_rate": 4.959318806725552e-05,
      "loss": 0.8972,
      "step": 53500
    },
    {
      "epoch": 0.48908679465654425,
      "grad_norm": 5.259253978729248,
      "learning_rate": 4.959242767111955e-05,
      "loss": 0.8561,
      "step": 53600
    },
    {
      "epoch": 0.48999927001970944,
      "grad_norm": 4.856886386871338,
      "learning_rate": 4.959166727498358e-05,
      "loss": 0.8669,
      "step": 53700
    },
    {
      "epoch": 0.4909117453828747,
      "grad_norm": 4.594318866729736,
      "learning_rate": 4.959090687884761e-05,
      "loss": 0.9152,
      "step": 53800
    },
    {
      "epoch": 0.4918242207460399,
      "grad_norm": 4.379584789276123,
      "learning_rate": 4.959014648271163e-05,
      "loss": 0.8217,
      "step": 53900
    },
    {
      "epoch": 0.49273669610920506,
      "grad_norm": 4.783738613128662,
      "learning_rate": 4.958938608657566e-05,
      "loss": 0.9214,
      "step": 54000
    },
    {
      "epoch": 0.49364917147237025,
      "grad_norm": 4.4226555824279785,
      "learning_rate": 4.958862569043969e-05,
      "loss": 0.8752,
      "step": 54100
    },
    {
      "epoch": 0.49456164683553544,
      "grad_norm": 4.54957914352417,
      "learning_rate": 4.958786529430372e-05,
      "loss": 0.8518,
      "step": 54200
    },
    {
      "epoch": 0.49547412219870063,
      "grad_norm": 4.330334186553955,
      "learning_rate": 4.958710489816775e-05,
      "loss": 0.8338,
      "step": 54300
    },
    {
      "epoch": 0.4963865975618658,
      "grad_norm": 4.3826904296875,
      "learning_rate": 4.9586344502031776e-05,
      "loss": 0.847,
      "step": 54400
    },
    {
      "epoch": 0.497299072925031,
      "grad_norm": 4.409646987915039,
      "learning_rate": 4.958558410589581e-05,
      "loss": 0.8644,
      "step": 54500
    },
    {
      "epoch": 0.4982115482881962,
      "grad_norm": 4.313534259796143,
      "learning_rate": 4.9584823709759836e-05,
      "loss": 0.9067,
      "step": 54600
    },
    {
      "epoch": 0.49912402365136144,
      "grad_norm": 3.809406042098999,
      "learning_rate": 4.9584063313623866e-05,
      "loss": 0.8321,
      "step": 54700
    },
    {
      "epoch": 0.5000364990145266,
      "grad_norm": 4.157682418823242,
      "learning_rate": 4.9583302917487896e-05,
      "loss": 0.8439,
      "step": 54800
    },
    {
      "epoch": 0.5009489743776918,
      "grad_norm": 4.808996200561523,
      "learning_rate": 4.9582542521351927e-05,
      "loss": 0.8442,
      "step": 54900
    },
    {
      "epoch": 0.501861449740857,
      "grad_norm": 4.356853008270264,
      "learning_rate": 4.958178212521595e-05,
      "loss": 0.8353,
      "step": 55000
    },
    {
      "epoch": 0.5027739251040222,
      "grad_norm": 4.631801128387451,
      "learning_rate": 4.958102172907999e-05,
      "loss": 0.8799,
      "step": 55100
    },
    {
      "epoch": 0.5036864004671874,
      "grad_norm": 5.064004898071289,
      "learning_rate": 4.958026133294401e-05,
      "loss": 0.8746,
      "step": 55200
    },
    {
      "epoch": 0.5045988758303526,
      "grad_norm": 4.485727310180664,
      "learning_rate": 4.957950093680804e-05,
      "loss": 0.8085,
      "step": 55300
    },
    {
      "epoch": 0.5055113511935178,
      "grad_norm": 4.429675579071045,
      "learning_rate": 4.957874054067207e-05,
      "loss": 0.8615,
      "step": 55400
    },
    {
      "epoch": 0.5064238265566829,
      "grad_norm": 4.523533821105957,
      "learning_rate": 4.95779801445361e-05,
      "loss": 0.8774,
      "step": 55500
    },
    {
      "epoch": 0.5073363019198481,
      "grad_norm": 4.26482629776001,
      "learning_rate": 4.957721974840013e-05,
      "loss": 0.8756,
      "step": 55600
    },
    {
      "epoch": 0.5082487772830133,
      "grad_norm": 4.924561500549316,
      "learning_rate": 4.957645935226416e-05,
      "loss": 0.8438,
      "step": 55700
    },
    {
      "epoch": 0.5091612526461785,
      "grad_norm": 4.8236470222473145,
      "learning_rate": 4.9575698956128184e-05,
      "loss": 0.8482,
      "step": 55800
    },
    {
      "epoch": 0.5100737280093437,
      "grad_norm": 4.413031578063965,
      "learning_rate": 4.957493855999222e-05,
      "loss": 0.9126,
      "step": 55900
    },
    {
      "epoch": 0.510986203372509,
      "grad_norm": 3.815059185028076,
      "learning_rate": 4.9574178163856244e-05,
      "loss": 0.832,
      "step": 56000
    },
    {
      "epoch": 0.5118986787356742,
      "grad_norm": 4.703180313110352,
      "learning_rate": 4.9573417767720274e-05,
      "loss": 0.8659,
      "step": 56100
    },
    {
      "epoch": 0.5128111540988394,
      "grad_norm": 3.447298526763916,
      "learning_rate": 4.9572657371584304e-05,
      "loss": 0.8429,
      "step": 56200
    },
    {
      "epoch": 0.5137236294620046,
      "grad_norm": 4.006750583648682,
      "learning_rate": 4.9571896975448334e-05,
      "loss": 0.822,
      "step": 56300
    },
    {
      "epoch": 0.5146361048251697,
      "grad_norm": 4.025365829467773,
      "learning_rate": 4.957113657931236e-05,
      "loss": 0.876,
      "step": 56400
    },
    {
      "epoch": 0.5155485801883349,
      "grad_norm": 4.533187389373779,
      "learning_rate": 4.9570376183176394e-05,
      "loss": 0.856,
      "step": 56500
    },
    {
      "epoch": 0.5164610555515001,
      "grad_norm": 3.7398622035980225,
      "learning_rate": 4.956961578704042e-05,
      "loss": 0.8562,
      "step": 56600
    },
    {
      "epoch": 0.5173735309146653,
      "grad_norm": 5.6135029792785645,
      "learning_rate": 4.956885539090445e-05,
      "loss": 0.8234,
      "step": 56700
    },
    {
      "epoch": 0.5182860062778305,
      "grad_norm": 3.982386350631714,
      "learning_rate": 4.956809499476848e-05,
      "loss": 0.8565,
      "step": 56800
    },
    {
      "epoch": 0.5191984816409957,
      "grad_norm": 5.6519975662231445,
      "learning_rate": 4.95673345986325e-05,
      "loss": 0.8551,
      "step": 56900
    },
    {
      "epoch": 0.5201109570041609,
      "grad_norm": 4.629485130310059,
      "learning_rate": 4.956657420249654e-05,
      "loss": 0.8586,
      "step": 57000
    },
    {
      "epoch": 0.5210234323673261,
      "grad_norm": 4.403879165649414,
      "learning_rate": 4.956581380636056e-05,
      "loss": 0.8591,
      "step": 57100
    },
    {
      "epoch": 0.5219359077304913,
      "grad_norm": 4.165603160858154,
      "learning_rate": 4.956505341022459e-05,
      "loss": 0.8692,
      "step": 57200
    },
    {
      "epoch": 0.5228483830936564,
      "grad_norm": 3.882190704345703,
      "learning_rate": 4.956429301408862e-05,
      "loss": 0.8837,
      "step": 57300
    },
    {
      "epoch": 0.5237608584568216,
      "grad_norm": 4.297092437744141,
      "learning_rate": 4.956353261795265e-05,
      "loss": 0.8666,
      "step": 57400
    },
    {
      "epoch": 0.5246733338199868,
      "grad_norm": 3.209512948989868,
      "learning_rate": 4.956277222181668e-05,
      "loss": 0.8522,
      "step": 57500
    },
    {
      "epoch": 0.525585809183152,
      "grad_norm": 4.320038318634033,
      "learning_rate": 4.956201182568071e-05,
      "loss": 0.8794,
      "step": 57600
    },
    {
      "epoch": 0.5264982845463172,
      "grad_norm": 3.65268874168396,
      "learning_rate": 4.9561251429544735e-05,
      "loss": 0.8809,
      "step": 57700
    },
    {
      "epoch": 0.5274107599094825,
      "grad_norm": 4.016753196716309,
      "learning_rate": 4.9560491033408765e-05,
      "loss": 0.8542,
      "step": 57800
    },
    {
      "epoch": 0.5283232352726477,
      "grad_norm": 5.183125972747803,
      "learning_rate": 4.9559730637272795e-05,
      "loss": 0.8805,
      "step": 57900
    },
    {
      "epoch": 0.5292357106358129,
      "grad_norm": 4.468153476715088,
      "learning_rate": 4.9558970241136825e-05,
      "loss": 0.8573,
      "step": 58000
    },
    {
      "epoch": 0.5301481859989781,
      "grad_norm": 4.622663497924805,
      "learning_rate": 4.9558209845000855e-05,
      "loss": 0.8921,
      "step": 58100
    },
    {
      "epoch": 0.5310606613621432,
      "grad_norm": 5.344147682189941,
      "learning_rate": 4.9557449448864885e-05,
      "loss": 0.8684,
      "step": 58200
    },
    {
      "epoch": 0.5319731367253084,
      "grad_norm": 4.368740081787109,
      "learning_rate": 4.955668905272891e-05,
      "loss": 0.9106,
      "step": 58300
    },
    {
      "epoch": 0.5328856120884736,
      "grad_norm": 4.3609514236450195,
      "learning_rate": 4.9555928656592945e-05,
      "loss": 0.8236,
      "step": 58400
    },
    {
      "epoch": 0.5337980874516388,
      "grad_norm": 4.944219589233398,
      "learning_rate": 4.955516826045697e-05,
      "loss": 0.8759,
      "step": 58500
    },
    {
      "epoch": 0.534710562814804,
      "grad_norm": 4.394794464111328,
      "learning_rate": 4.9554407864321e-05,
      "loss": 0.8241,
      "step": 58600
    },
    {
      "epoch": 0.5356230381779692,
      "grad_norm": 4.155235290527344,
      "learning_rate": 4.955364746818503e-05,
      "loss": 0.8694,
      "step": 58700
    },
    {
      "epoch": 0.5365355135411344,
      "grad_norm": 4.433952331542969,
      "learning_rate": 4.955288707204906e-05,
      "loss": 0.8367,
      "step": 58800
    },
    {
      "epoch": 0.5374479889042996,
      "grad_norm": 4.84781551361084,
      "learning_rate": 4.955212667591309e-05,
      "loss": 0.8504,
      "step": 58900
    },
    {
      "epoch": 0.5383604642674648,
      "grad_norm": 4.672074794769287,
      "learning_rate": 4.955136627977712e-05,
      "loss": 0.8549,
      "step": 59000
    },
    {
      "epoch": 0.5392729396306299,
      "grad_norm": 3.94891095161438,
      "learning_rate": 4.955060588364114e-05,
      "loss": 0.8844,
      "step": 59100
    },
    {
      "epoch": 0.5401854149937951,
      "grad_norm": 4.90338659286499,
      "learning_rate": 4.954984548750518e-05,
      "loss": 0.8198,
      "step": 59200
    },
    {
      "epoch": 0.5410978903569603,
      "grad_norm": 4.662460803985596,
      "learning_rate": 4.95490850913692e-05,
      "loss": 0.861,
      "step": 59300
    },
    {
      "epoch": 0.5420103657201255,
      "grad_norm": 5.359144687652588,
      "learning_rate": 4.954832469523323e-05,
      "loss": 0.8619,
      "step": 59400
    },
    {
      "epoch": 0.5429228410832907,
      "grad_norm": 4.8800950050354,
      "learning_rate": 4.954756429909726e-05,
      "loss": 0.8787,
      "step": 59500
    },
    {
      "epoch": 0.543835316446456,
      "grad_norm": 5.66916036605835,
      "learning_rate": 4.954680390296129e-05,
      "loss": 0.8305,
      "step": 59600
    },
    {
      "epoch": 0.5447477918096212,
      "grad_norm": 3.8752574920654297,
      "learning_rate": 4.9546043506825316e-05,
      "loss": 0.8412,
      "step": 59700
    },
    {
      "epoch": 0.5456602671727864,
      "grad_norm": 3.9005935192108154,
      "learning_rate": 4.9545283110689346e-05,
      "loss": 0.8288,
      "step": 59800
    },
    {
      "epoch": 0.5465727425359516,
      "grad_norm": 4.487043857574463,
      "learning_rate": 4.9544522714553376e-05,
      "loss": 0.8728,
      "step": 59900
    },
    {
      "epoch": 0.5474852178991167,
      "grad_norm": 3.762542963027954,
      "learning_rate": 4.9543762318417406e-05,
      "loss": 0.8233,
      "step": 60000
    },
    {
      "epoch": 0.5483976932622819,
      "grad_norm": 3.8628852367401123,
      "learning_rate": 4.9543001922281436e-05,
      "loss": 0.9083,
      "step": 60100
    },
    {
      "epoch": 0.5493101686254471,
      "grad_norm": 6.241658687591553,
      "learning_rate": 4.954224152614546e-05,
      "loss": 0.8636,
      "step": 60200
    },
    {
      "epoch": 0.5502226439886123,
      "grad_norm": 3.662165641784668,
      "learning_rate": 4.9541481130009496e-05,
      "loss": 0.8647,
      "step": 60300
    },
    {
      "epoch": 0.5511351193517775,
      "grad_norm": 3.8777997493743896,
      "learning_rate": 4.954072073387352e-05,
      "loss": 0.8568,
      "step": 60400
    },
    {
      "epoch": 0.5520475947149427,
      "grad_norm": 3.792858839035034,
      "learning_rate": 4.953996033773755e-05,
      "loss": 0.8483,
      "step": 60500
    },
    {
      "epoch": 0.5529600700781079,
      "grad_norm": 5.1837005615234375,
      "learning_rate": 4.953919994160158e-05,
      "loss": 0.8633,
      "step": 60600
    },
    {
      "epoch": 0.5538725454412731,
      "grad_norm": 3.9799699783325195,
      "learning_rate": 4.953843954546561e-05,
      "loss": 0.8265,
      "step": 60700
    },
    {
      "epoch": 0.5547850208044383,
      "grad_norm": 5.399032115936279,
      "learning_rate": 4.953767914932963e-05,
      "loss": 0.844,
      "step": 60800
    },
    {
      "epoch": 0.5556974961676034,
      "grad_norm": 4.704935073852539,
      "learning_rate": 4.953691875319367e-05,
      "loss": 0.8594,
      "step": 60900
    },
    {
      "epoch": 0.5566099715307686,
      "grad_norm": 5.142425537109375,
      "learning_rate": 4.953615835705769e-05,
      "loss": 0.8849,
      "step": 61000
    },
    {
      "epoch": 0.5575224468939338,
      "grad_norm": 4.402927398681641,
      "learning_rate": 4.953539796092172e-05,
      "loss": 0.8598,
      "step": 61100
    },
    {
      "epoch": 0.558434922257099,
      "grad_norm": 5.250842571258545,
      "learning_rate": 4.953463756478575e-05,
      "loss": 0.8674,
      "step": 61200
    },
    {
      "epoch": 0.5593473976202643,
      "grad_norm": 4.673272609710693,
      "learning_rate": 4.953387716864978e-05,
      "loss": 0.848,
      "step": 61300
    },
    {
      "epoch": 0.5602598729834295,
      "grad_norm": 4.153416633605957,
      "learning_rate": 4.953311677251381e-05,
      "loss": 0.8962,
      "step": 61400
    },
    {
      "epoch": 0.5611723483465947,
      "grad_norm": 3.5645978450775146,
      "learning_rate": 4.953235637637784e-05,
      "loss": 0.8366,
      "step": 61500
    },
    {
      "epoch": 0.5620848237097599,
      "grad_norm": 3.9420642852783203,
      "learning_rate": 4.9531595980241867e-05,
      "loss": 0.8431,
      "step": 61600
    },
    {
      "epoch": 0.5629972990729251,
      "grad_norm": 4.776797294616699,
      "learning_rate": 4.95308355841059e-05,
      "loss": 0.8707,
      "step": 61700
    },
    {
      "epoch": 0.5639097744360902,
      "grad_norm": 4.005620002746582,
      "learning_rate": 4.953007518796993e-05,
      "loss": 0.8532,
      "step": 61800
    },
    {
      "epoch": 0.5648222497992554,
      "grad_norm": 3.7698609828948975,
      "learning_rate": 4.952931479183396e-05,
      "loss": 0.8429,
      "step": 61900
    },
    {
      "epoch": 0.5657347251624206,
      "grad_norm": 4.695664882659912,
      "learning_rate": 4.952855439569799e-05,
      "loss": 0.8443,
      "step": 62000
    },
    {
      "epoch": 0.5666472005255858,
      "grad_norm": 3.8301267623901367,
      "learning_rate": 4.952779399956202e-05,
      "loss": 0.8195,
      "step": 62100
    },
    {
      "epoch": 0.567559675888751,
      "grad_norm": 5.21262788772583,
      "learning_rate": 4.952703360342604e-05,
      "loss": 0.8666,
      "step": 62200
    },
    {
      "epoch": 0.5684721512519162,
      "grad_norm": 3.9048924446105957,
      "learning_rate": 4.952627320729008e-05,
      "loss": 0.807,
      "step": 62300
    },
    {
      "epoch": 0.5693846266150814,
      "grad_norm": 4.381265163421631,
      "learning_rate": 4.95255128111541e-05,
      "loss": 0.8393,
      "step": 62400
    },
    {
      "epoch": 0.5702971019782466,
      "grad_norm": 4.120565891265869,
      "learning_rate": 4.952475241501813e-05,
      "loss": 0.7961,
      "step": 62500
    },
    {
      "epoch": 0.5712095773414118,
      "grad_norm": 5.433224678039551,
      "learning_rate": 4.952399201888216e-05,
      "loss": 0.8399,
      "step": 62600
    },
    {
      "epoch": 0.5721220527045769,
      "grad_norm": 5.0417585372924805,
      "learning_rate": 4.9523231622746184e-05,
      "loss": 0.8126,
      "step": 62700
    },
    {
      "epoch": 0.5730345280677421,
      "grad_norm": 4.7412333488464355,
      "learning_rate": 4.952247122661022e-05,
      "loss": 0.8773,
      "step": 62800
    },
    {
      "epoch": 0.5739470034309073,
      "grad_norm": 5.0166778564453125,
      "learning_rate": 4.9521710830474244e-05,
      "loss": 0.8487,
      "step": 62900
    },
    {
      "epoch": 0.5748594787940725,
      "grad_norm": 4.595765590667725,
      "learning_rate": 4.9520950434338274e-05,
      "loss": 0.865,
      "step": 63000
    },
    {
      "epoch": 0.5757719541572378,
      "grad_norm": 4.813622951507568,
      "learning_rate": 4.9520190038202304e-05,
      "loss": 0.8987,
      "step": 63100
    },
    {
      "epoch": 0.576684429520403,
      "grad_norm": 4.3361406326293945,
      "learning_rate": 4.9519429642066334e-05,
      "loss": 0.8691,
      "step": 63200
    },
    {
      "epoch": 0.5775969048835682,
      "grad_norm": 4.63232946395874,
      "learning_rate": 4.951866924593036e-05,
      "loss": 0.847,
      "step": 63300
    },
    {
      "epoch": 0.5785093802467334,
      "grad_norm": 4.670496940612793,
      "learning_rate": 4.9517908849794394e-05,
      "loss": 0.8658,
      "step": 63400
    },
    {
      "epoch": 0.5794218556098986,
      "grad_norm": 4.201576232910156,
      "learning_rate": 4.951714845365842e-05,
      "loss": 0.8527,
      "step": 63500
    },
    {
      "epoch": 0.5803343309730638,
      "grad_norm": 4.326247692108154,
      "learning_rate": 4.951638805752245e-05,
      "loss": 0.8279,
      "step": 63600
    },
    {
      "epoch": 0.5812468063362289,
      "grad_norm": 3.378110885620117,
      "learning_rate": 4.951562766138648e-05,
      "loss": 0.8414,
      "step": 63700
    },
    {
      "epoch": 0.5821592816993941,
      "grad_norm": 3.6036643981933594,
      "learning_rate": 4.951486726525051e-05,
      "loss": 0.8571,
      "step": 63800
    },
    {
      "epoch": 0.5830717570625593,
      "grad_norm": 4.1000142097473145,
      "learning_rate": 4.951410686911454e-05,
      "loss": 0.8531,
      "step": 63900
    },
    {
      "epoch": 0.5839842324257245,
      "grad_norm": 4.774824142456055,
      "learning_rate": 4.951334647297857e-05,
      "loss": 0.8205,
      "step": 64000
    },
    {
      "epoch": 0.5848967077888897,
      "grad_norm": 4.7783660888671875,
      "learning_rate": 4.951258607684259e-05,
      "loss": 0.8751,
      "step": 64100
    },
    {
      "epoch": 0.5858091831520549,
      "grad_norm": 4.441040992736816,
      "learning_rate": 4.951182568070663e-05,
      "loss": 0.8391,
      "step": 64200
    },
    {
      "epoch": 0.5867216585152201,
      "grad_norm": 4.729088306427002,
      "learning_rate": 4.951106528457065e-05,
      "loss": 0.8654,
      "step": 64300
    },
    {
      "epoch": 0.5876341338783853,
      "grad_norm": 4.5854339599609375,
      "learning_rate": 4.951030488843468e-05,
      "loss": 0.8552,
      "step": 64400
    },
    {
      "epoch": 0.5885466092415504,
      "grad_norm": 4.564760208129883,
      "learning_rate": 4.950954449229871e-05,
      "loss": 0.8286,
      "step": 64500
    },
    {
      "epoch": 0.5894590846047156,
      "grad_norm": 4.409409046173096,
      "learning_rate": 4.950878409616274e-05,
      "loss": 0.8568,
      "step": 64600
    },
    {
      "epoch": 0.5903715599678808,
      "grad_norm": 4.8421478271484375,
      "learning_rate": 4.9508023700026765e-05,
      "loss": 0.835,
      "step": 64700
    },
    {
      "epoch": 0.591284035331046,
      "grad_norm": 4.511265277862549,
      "learning_rate": 4.95072633038908e-05,
      "loss": 0.8377,
      "step": 64800
    },
    {
      "epoch": 0.5921965106942113,
      "grad_norm": 4.921297550201416,
      "learning_rate": 4.9506502907754825e-05,
      "loss": 0.8746,
      "step": 64900
    },
    {
      "epoch": 0.5931089860573765,
      "grad_norm": 5.02126407623291,
      "learning_rate": 4.9505742511618855e-05,
      "loss": 0.862,
      "step": 65000
    },
    {
      "epoch": 0.5940214614205417,
      "grad_norm": 4.413663864135742,
      "learning_rate": 4.9504982115482885e-05,
      "loss": 0.8525,
      "step": 65100
    },
    {
      "epoch": 0.5949339367837069,
      "grad_norm": 5.246824741363525,
      "learning_rate": 4.9504221719346915e-05,
      "loss": 0.8264,
      "step": 65200
    },
    {
      "epoch": 0.5958464121468721,
      "grad_norm": 5.172170162200928,
      "learning_rate": 4.9503461323210945e-05,
      "loss": 0.8019,
      "step": 65300
    },
    {
      "epoch": 0.5967588875100373,
      "grad_norm": 3.7054576873779297,
      "learning_rate": 4.950270092707497e-05,
      "loss": 0.8575,
      "step": 65400
    },
    {
      "epoch": 0.5976713628732024,
      "grad_norm": 4.024428844451904,
      "learning_rate": 4.9501940530939e-05,
      "loss": 0.7833,
      "step": 65500
    },
    {
      "epoch": 0.5985838382363676,
      "grad_norm": 5.492123603820801,
      "learning_rate": 4.950118013480303e-05,
      "loss": 0.8362,
      "step": 65600
    },
    {
      "epoch": 0.5994963135995328,
      "grad_norm": 5.524693965911865,
      "learning_rate": 4.950041973866706e-05,
      "loss": 0.851,
      "step": 65700
    },
    {
      "epoch": 0.600408788962698,
      "grad_norm": 4.54662561416626,
      "learning_rate": 4.949965934253108e-05,
      "loss": 0.8735,
      "step": 65800
    },
    {
      "epoch": 0.6013212643258632,
      "grad_norm": 2.9177236557006836,
      "learning_rate": 4.949889894639512e-05,
      "loss": 0.8775,
      "step": 65900
    },
    {
      "epoch": 0.6022337396890284,
      "grad_norm": 4.250411510467529,
      "learning_rate": 4.949813855025914e-05,
      "loss": 0.8669,
      "step": 66000
    },
    {
      "epoch": 0.6031462150521936,
      "grad_norm": 4.83933162689209,
      "learning_rate": 4.949737815412317e-05,
      "loss": 0.813,
      "step": 66100
    },
    {
      "epoch": 0.6040586904153588,
      "grad_norm": 5.356653690338135,
      "learning_rate": 4.94966177579872e-05,
      "loss": 0.8239,
      "step": 66200
    },
    {
      "epoch": 0.604971165778524,
      "grad_norm": 4.908864498138428,
      "learning_rate": 4.949585736185123e-05,
      "loss": 0.7928,
      "step": 66300
    },
    {
      "epoch": 0.6058836411416891,
      "grad_norm": 4.5835771560668945,
      "learning_rate": 4.949509696571526e-05,
      "loss": 0.8629,
      "step": 66400
    },
    {
      "epoch": 0.6067961165048543,
      "grad_norm": 4.05864143371582,
      "learning_rate": 4.949433656957929e-05,
      "loss": 0.8669,
      "step": 66500
    },
    {
      "epoch": 0.6077085918680195,
      "grad_norm": 5.794661045074463,
      "learning_rate": 4.9493576173443316e-05,
      "loss": 0.8354,
      "step": 66600
    },
    {
      "epoch": 0.6086210672311848,
      "grad_norm": 4.142510890960693,
      "learning_rate": 4.949281577730735e-05,
      "loss": 0.8514,
      "step": 66700
    },
    {
      "epoch": 0.60953354259435,
      "grad_norm": 3.9305617809295654,
      "learning_rate": 4.9492055381171376e-05,
      "loss": 0.8229,
      "step": 66800
    },
    {
      "epoch": 0.6104460179575152,
      "grad_norm": 3.683147668838501,
      "learning_rate": 4.9491294985035406e-05,
      "loss": 0.8285,
      "step": 66900
    },
    {
      "epoch": 0.6113584933206804,
      "grad_norm": 4.041131019592285,
      "learning_rate": 4.9490534588899436e-05,
      "loss": 0.8173,
      "step": 67000
    },
    {
      "epoch": 0.6122709686838456,
      "grad_norm": 3.7250804901123047,
      "learning_rate": 4.9489774192763466e-05,
      "loss": 0.8426,
      "step": 67100
    },
    {
      "epoch": 0.6131834440470108,
      "grad_norm": 4.249514579772949,
      "learning_rate": 4.948901379662749e-05,
      "loss": 0.8526,
      "step": 67200
    },
    {
      "epoch": 0.6140959194101759,
      "grad_norm": 3.675992012023926,
      "learning_rate": 4.9488253400491526e-05,
      "loss": 0.8123,
      "step": 67300
    },
    {
      "epoch": 0.6150083947733411,
      "grad_norm": 4.8282928466796875,
      "learning_rate": 4.948749300435555e-05,
      "loss": 0.7993,
      "step": 67400
    },
    {
      "epoch": 0.6159208701365063,
      "grad_norm": 4.673981666564941,
      "learning_rate": 4.948673260821958e-05,
      "loss": 0.8737,
      "step": 67500
    },
    {
      "epoch": 0.6168333454996715,
      "grad_norm": 3.7605650424957275,
      "learning_rate": 4.948597221208361e-05,
      "loss": 0.8333,
      "step": 67600
    },
    {
      "epoch": 0.6177458208628367,
      "grad_norm": 4.625222682952881,
      "learning_rate": 4.948521181594764e-05,
      "loss": 0.873,
      "step": 67700
    },
    {
      "epoch": 0.6186582962260019,
      "grad_norm": 4.349229335784912,
      "learning_rate": 4.948445141981167e-05,
      "loss": 0.8492,
      "step": 67800
    },
    {
      "epoch": 0.6195707715891671,
      "grad_norm": 4.199069976806641,
      "learning_rate": 4.94836910236757e-05,
      "loss": 0.8213,
      "step": 67900
    },
    {
      "epoch": 0.6204832469523323,
      "grad_norm": 4.990874767303467,
      "learning_rate": 4.948293062753972e-05,
      "loss": 0.8122,
      "step": 68000
    },
    {
      "epoch": 0.6213957223154974,
      "grad_norm": 4.211784362792969,
      "learning_rate": 4.948217023140376e-05,
      "loss": 0.9161,
      "step": 68100
    },
    {
      "epoch": 0.6223081976786626,
      "grad_norm": 4.260753631591797,
      "learning_rate": 4.948140983526778e-05,
      "loss": 0.8896,
      "step": 68200
    },
    {
      "epoch": 0.6232206730418278,
      "grad_norm": 4.3621320724487305,
      "learning_rate": 4.9480649439131806e-05,
      "loss": 0.8773,
      "step": 68300
    },
    {
      "epoch": 0.624133148404993,
      "grad_norm": 4.4237494468688965,
      "learning_rate": 4.947988904299584e-05,
      "loss": 0.8566,
      "step": 68400
    },
    {
      "epoch": 0.6250456237681583,
      "grad_norm": 4.388762950897217,
      "learning_rate": 4.9479128646859867e-05,
      "loss": 0.8284,
      "step": 68500
    },
    {
      "epoch": 0.6259580991313235,
      "grad_norm": 4.31477165222168,
      "learning_rate": 4.94783682507239e-05,
      "loss": 0.8486,
      "step": 68600
    },
    {
      "epoch": 0.6268705744944887,
      "grad_norm": 4.785345077514648,
      "learning_rate": 4.947760785458793e-05,
      "loss": 0.8422,
      "step": 68700
    },
    {
      "epoch": 0.6277830498576539,
      "grad_norm": 4.063412666320801,
      "learning_rate": 4.947684745845196e-05,
      "loss": 0.8382,
      "step": 68800
    },
    {
      "epoch": 0.6286955252208191,
      "grad_norm": 3.8986237049102783,
      "learning_rate": 4.947608706231599e-05,
      "loss": 0.8256,
      "step": 68900
    },
    {
      "epoch": 0.6296080005839843,
      "grad_norm": 4.5089216232299805,
      "learning_rate": 4.947532666618002e-05,
      "loss": 0.7769,
      "step": 69000
    },
    {
      "epoch": 0.6305204759471494,
      "grad_norm": 4.407005310058594,
      "learning_rate": 4.947456627004404e-05,
      "loss": 0.871,
      "step": 69100
    },
    {
      "epoch": 0.6314329513103146,
      "grad_norm": 4.277102947235107,
      "learning_rate": 4.947380587390808e-05,
      "loss": 0.8483,
      "step": 69200
    },
    {
      "epoch": 0.6323454266734798,
      "grad_norm": 4.164785861968994,
      "learning_rate": 4.94730454777721e-05,
      "loss": 0.8274,
      "step": 69300
    },
    {
      "epoch": 0.633257902036645,
      "grad_norm": 4.784895896911621,
      "learning_rate": 4.947228508163613e-05,
      "loss": 0.826,
      "step": 69400
    },
    {
      "epoch": 0.6341703773998102,
      "grad_norm": 5.330996990203857,
      "learning_rate": 4.947152468550016e-05,
      "loss": 0.8299,
      "step": 69500
    },
    {
      "epoch": 0.6350828527629754,
      "grad_norm": 4.131919860839844,
      "learning_rate": 4.947076428936419e-05,
      "loss": 0.8841,
      "step": 69600
    },
    {
      "epoch": 0.6359953281261406,
      "grad_norm": 4.8240861892700195,
      "learning_rate": 4.947000389322822e-05,
      "loss": 0.826,
      "step": 69700
    },
    {
      "epoch": 0.6369078034893058,
      "grad_norm": 5.479900360107422,
      "learning_rate": 4.946924349709225e-05,
      "loss": 0.8468,
      "step": 69800
    },
    {
      "epoch": 0.637820278852471,
      "grad_norm": 5.055140972137451,
      "learning_rate": 4.9468483100956274e-05,
      "loss": 0.8694,
      "step": 69900
    },
    {
      "epoch": 0.6387327542156361,
      "grad_norm": 5.127918243408203,
      "learning_rate": 4.9467722704820304e-05,
      "loss": 0.8552,
      "step": 70000
    },
    {
      "epoch": 0.6396452295788013,
      "grad_norm": 3.982205629348755,
      "learning_rate": 4.9466962308684334e-05,
      "loss": 0.8805,
      "step": 70100
    },
    {
      "epoch": 0.6405577049419666,
      "grad_norm": 4.584067344665527,
      "learning_rate": 4.9466201912548364e-05,
      "loss": 0.8106,
      "step": 70200
    },
    {
      "epoch": 0.6414701803051318,
      "grad_norm": 4.458425045013428,
      "learning_rate": 4.9465441516412394e-05,
      "loss": 0.8513,
      "step": 70300
    },
    {
      "epoch": 0.642382655668297,
      "grad_norm": 3.888054847717285,
      "learning_rate": 4.9464681120276424e-05,
      "loss": 0.8733,
      "step": 70400
    },
    {
      "epoch": 0.6432951310314622,
      "grad_norm": 3.357980489730835,
      "learning_rate": 4.946392072414045e-05,
      "loss": 0.8494,
      "step": 70500
    },
    {
      "epoch": 0.6442076063946274,
      "grad_norm": 4.563622951507568,
      "learning_rate": 4.9463160328004484e-05,
      "loss": 0.8753,
      "step": 70600
    },
    {
      "epoch": 0.6451200817577926,
      "grad_norm": 5.497402191162109,
      "learning_rate": 4.946239993186851e-05,
      "loss": 0.8676,
      "step": 70700
    },
    {
      "epoch": 0.6460325571209578,
      "grad_norm": 4.428991794586182,
      "learning_rate": 4.946163953573254e-05,
      "loss": 0.8567,
      "step": 70800
    },
    {
      "epoch": 0.6469450324841229,
      "grad_norm": 4.8108439445495605,
      "learning_rate": 4.946087913959657e-05,
      "loss": 0.8593,
      "step": 70900
    },
    {
      "epoch": 0.6478575078472881,
      "grad_norm": 4.633126735687256,
      "learning_rate": 4.94601187434606e-05,
      "loss": 0.8178,
      "step": 71000
    },
    {
      "epoch": 0.6487699832104533,
      "grad_norm": 3.711961030960083,
      "learning_rate": 4.945935834732463e-05,
      "loss": 0.8446,
      "step": 71100
    },
    {
      "epoch": 0.6496824585736185,
      "grad_norm": 4.2719879150390625,
      "learning_rate": 4.945859795118865e-05,
      "loss": 0.7942,
      "step": 71200
    },
    {
      "epoch": 0.6505949339367837,
      "grad_norm": 4.306506156921387,
      "learning_rate": 4.945783755505268e-05,
      "loss": 0.8468,
      "step": 71300
    },
    {
      "epoch": 0.6515074092999489,
      "grad_norm": 4.164908409118652,
      "learning_rate": 4.945707715891671e-05,
      "loss": 0.8553,
      "step": 71400
    },
    {
      "epoch": 0.6524198846631141,
      "grad_norm": 4.451453685760498,
      "learning_rate": 4.945631676278074e-05,
      "loss": 0.839,
      "step": 71500
    },
    {
      "epoch": 0.6533323600262793,
      "grad_norm": 4.809549331665039,
      "learning_rate": 4.9455556366644765e-05,
      "loss": 0.8594,
      "step": 71600
    },
    {
      "epoch": 0.6542448353894444,
      "grad_norm": 4.482346057891846,
      "learning_rate": 4.94547959705088e-05,
      "loss": 0.8657,
      "step": 71700
    },
    {
      "epoch": 0.6551573107526096,
      "grad_norm": 4.083438396453857,
      "learning_rate": 4.9454035574372825e-05,
      "loss": 0.8075,
      "step": 71800
    },
    {
      "epoch": 0.6560697861157748,
      "grad_norm": 4.612522125244141,
      "learning_rate": 4.9453275178236855e-05,
      "loss": 0.8745,
      "step": 71900
    },
    {
      "epoch": 0.6569822614789401,
      "grad_norm": 5.434545040130615,
      "learning_rate": 4.9452514782100885e-05,
      "loss": 0.8538,
      "step": 72000
    },
    {
      "epoch": 0.6578947368421053,
      "grad_norm": 4.129838943481445,
      "learning_rate": 4.9451754385964915e-05,
      "loss": 0.8595,
      "step": 72100
    },
    {
      "epoch": 0.6588072122052705,
      "grad_norm": 5.199753284454346,
      "learning_rate": 4.9450993989828945e-05,
      "loss": 0.8165,
      "step": 72200
    },
    {
      "epoch": 0.6597196875684357,
      "grad_norm": 4.441078186035156,
      "learning_rate": 4.9450233593692975e-05,
      "loss": 0.8337,
      "step": 72300
    },
    {
      "epoch": 0.6606321629316009,
      "grad_norm": 4.8697991371154785,
      "learning_rate": 4.9449473197557e-05,
      "loss": 0.8441,
      "step": 72400
    },
    {
      "epoch": 0.6615446382947661,
      "grad_norm": 3.3460795879364014,
      "learning_rate": 4.9448712801421035e-05,
      "loss": 0.8067,
      "step": 72500
    },
    {
      "epoch": 0.6624571136579313,
      "grad_norm": 4.559266567230225,
      "learning_rate": 4.944795240528506e-05,
      "loss": 0.8172,
      "step": 72600
    },
    {
      "epoch": 0.6633695890210964,
      "grad_norm": 3.9192092418670654,
      "learning_rate": 4.944719200914909e-05,
      "loss": 0.8444,
      "step": 72700
    },
    {
      "epoch": 0.6642820643842616,
      "grad_norm": 4.5096116065979,
      "learning_rate": 4.944643161301312e-05,
      "loss": 0.841,
      "step": 72800
    },
    {
      "epoch": 0.6651945397474268,
      "grad_norm": 3.1669437885284424,
      "learning_rate": 4.944567121687715e-05,
      "loss": 0.7747,
      "step": 72900
    },
    {
      "epoch": 0.666107015110592,
      "grad_norm": 2.977501153945923,
      "learning_rate": 4.944491082074117e-05,
      "loss": 0.8142,
      "step": 73000
    },
    {
      "epoch": 0.6670194904737572,
      "grad_norm": 2.9946541786193848,
      "learning_rate": 4.944415042460521e-05,
      "loss": 0.8269,
      "step": 73100
    },
    {
      "epoch": 0.6679319658369224,
      "grad_norm": 4.447492599487305,
      "learning_rate": 4.944339002846923e-05,
      "loss": 0.7976,
      "step": 73200
    },
    {
      "epoch": 0.6688444412000876,
      "grad_norm": 4.259191036224365,
      "learning_rate": 4.944262963233326e-05,
      "loss": 0.8366,
      "step": 73300
    },
    {
      "epoch": 0.6697569165632528,
      "grad_norm": 4.917003154754639,
      "learning_rate": 4.944186923619729e-05,
      "loss": 0.8439,
      "step": 73400
    },
    {
      "epoch": 0.670669391926418,
      "grad_norm": 4.351323127746582,
      "learning_rate": 4.944110884006132e-05,
      "loss": 0.9288,
      "step": 73500
    },
    {
      "epoch": 0.6715818672895831,
      "grad_norm": 5.3559489250183105,
      "learning_rate": 4.944034844392535e-05,
      "loss": 0.8625,
      "step": 73600
    },
    {
      "epoch": 0.6724943426527483,
      "grad_norm": 4.966269016265869,
      "learning_rate": 4.943958804778938e-05,
      "loss": 0.8804,
      "step": 73700
    },
    {
      "epoch": 0.6734068180159136,
      "grad_norm": 5.24700403213501,
      "learning_rate": 4.9438827651653406e-05,
      "loss": 0.8404,
      "step": 73800
    },
    {
      "epoch": 0.6743192933790788,
      "grad_norm": 5.015495300292969,
      "learning_rate": 4.9438067255517436e-05,
      "loss": 0.8366,
      "step": 73900
    },
    {
      "epoch": 0.675231768742244,
      "grad_norm": 3.4284167289733887,
      "learning_rate": 4.9437306859381466e-05,
      "loss": 0.8521,
      "step": 74000
    },
    {
      "epoch": 0.6761442441054092,
      "grad_norm": 4.6031413078308105,
      "learning_rate": 4.943654646324549e-05,
      "loss": 0.8367,
      "step": 74100
    },
    {
      "epoch": 0.6770567194685744,
      "grad_norm": 3.8801307678222656,
      "learning_rate": 4.9435786067109526e-05,
      "loss": 0.821,
      "step": 74200
    },
    {
      "epoch": 0.6779691948317396,
      "grad_norm": 4.244747638702393,
      "learning_rate": 4.943502567097355e-05,
      "loss": 0.8214,
      "step": 74300
    },
    {
      "epoch": 0.6788816701949048,
      "grad_norm": 4.62493896484375,
      "learning_rate": 4.943426527483758e-05,
      "loss": 0.8344,
      "step": 74400
    },
    {
      "epoch": 0.6797941455580699,
      "grad_norm": 4.21189546585083,
      "learning_rate": 4.943350487870161e-05,
      "loss": 0.835,
      "step": 74500
    },
    {
      "epoch": 0.6807066209212351,
      "grad_norm": 4.537622451782227,
      "learning_rate": 4.943274448256564e-05,
      "loss": 0.8715,
      "step": 74600
    },
    {
      "epoch": 0.6816190962844003,
      "grad_norm": 4.150195598602295,
      "learning_rate": 4.943198408642967e-05,
      "loss": 0.8665,
      "step": 74700
    },
    {
      "epoch": 0.6825315716475655,
      "grad_norm": 4.869713306427002,
      "learning_rate": 4.94312236902937e-05,
      "loss": 0.8269,
      "step": 74800
    },
    {
      "epoch": 0.6834440470107307,
      "grad_norm": 3.9774274826049805,
      "learning_rate": 4.943046329415772e-05,
      "loss": 0.8253,
      "step": 74900
    },
    {
      "epoch": 0.6843565223738959,
      "grad_norm": 4.738735198974609,
      "learning_rate": 4.942970289802176e-05,
      "loss": 0.8175,
      "step": 75000
    },
    {
      "epoch": 0.6852689977370611,
      "grad_norm": 4.431366443634033,
      "learning_rate": 4.942894250188578e-05,
      "loss": 0.837,
      "step": 75100
    },
    {
      "epoch": 0.6861814731002263,
      "grad_norm": 5.051496505737305,
      "learning_rate": 4.942818210574981e-05,
      "loss": 0.8371,
      "step": 75200
    },
    {
      "epoch": 0.6870939484633914,
      "grad_norm": 5.32862663269043,
      "learning_rate": 4.9427421709613843e-05,
      "loss": 0.8438,
      "step": 75300
    },
    {
      "epoch": 0.6880064238265566,
      "grad_norm": 4.292612075805664,
      "learning_rate": 4.9426661313477873e-05,
      "loss": 0.8258,
      "step": 75400
    },
    {
      "epoch": 0.6889188991897218,
      "grad_norm": 5.717616558074951,
      "learning_rate": 4.94259009173419e-05,
      "loss": 0.866,
      "step": 75500
    },
    {
      "epoch": 0.6898313745528871,
      "grad_norm": 3.809903144836426,
      "learning_rate": 4.9425140521205934e-05,
      "loss": 0.8385,
      "step": 75600
    },
    {
      "epoch": 0.6907438499160523,
      "grad_norm": 4.629923343658447,
      "learning_rate": 4.942438012506996e-05,
      "loss": 0.8398,
      "step": 75700
    },
    {
      "epoch": 0.6916563252792175,
      "grad_norm": 4.805742263793945,
      "learning_rate": 4.942361972893399e-05,
      "loss": 0.8411,
      "step": 75800
    },
    {
      "epoch": 0.6925688006423827,
      "grad_norm": 4.161559104919434,
      "learning_rate": 4.942285933279802e-05,
      "loss": 0.822,
      "step": 75900
    },
    {
      "epoch": 0.6934812760055479,
      "grad_norm": 4.785707950592041,
      "learning_rate": 4.942209893666205e-05,
      "loss": 0.8577,
      "step": 76000
    },
    {
      "epoch": 0.6943937513687131,
      "grad_norm": 4.735787868499756,
      "learning_rate": 4.942133854052608e-05,
      "loss": 0.8319,
      "step": 76100
    },
    {
      "epoch": 0.6953062267318783,
      "grad_norm": 4.520555019378662,
      "learning_rate": 4.942057814439011e-05,
      "loss": 0.8256,
      "step": 76200
    },
    {
      "epoch": 0.6962187020950434,
      "grad_norm": 3.466339349746704,
      "learning_rate": 4.941981774825413e-05,
      "loss": 0.8461,
      "step": 76300
    },
    {
      "epoch": 0.6971311774582086,
      "grad_norm": 4.562489986419678,
      "learning_rate": 4.941905735211817e-05,
      "loss": 0.833,
      "step": 76400
    },
    {
      "epoch": 0.6980436528213738,
      "grad_norm": 4.7943949699401855,
      "learning_rate": 4.941829695598219e-05,
      "loss": 0.7972,
      "step": 76500
    },
    {
      "epoch": 0.698956128184539,
      "grad_norm": 4.120275497436523,
      "learning_rate": 4.941753655984622e-05,
      "loss": 0.8141,
      "step": 76600
    },
    {
      "epoch": 0.6998686035477042,
      "grad_norm": 4.5894775390625,
      "learning_rate": 4.941677616371025e-05,
      "loss": 0.8593,
      "step": 76700
    },
    {
      "epoch": 0.7007810789108694,
      "grad_norm": 4.56374979019165,
      "learning_rate": 4.9416015767574274e-05,
      "loss": 0.8317,
      "step": 76800
    },
    {
      "epoch": 0.7016935542740346,
      "grad_norm": 3.7669966220855713,
      "learning_rate": 4.9415255371438304e-05,
      "loss": 0.8137,
      "step": 76900
    },
    {
      "epoch": 0.7026060296371998,
      "grad_norm": 4.693946838378906,
      "learning_rate": 4.9414494975302334e-05,
      "loss": 0.8623,
      "step": 77000
    },
    {
      "epoch": 0.703518505000365,
      "grad_norm": 3.0098979473114014,
      "learning_rate": 4.9413734579166364e-05,
      "loss": 0.8473,
      "step": 77100
    },
    {
      "epoch": 0.7044309803635301,
      "grad_norm": 4.986734867095947,
      "learning_rate": 4.9412974183030394e-05,
      "loss": 0.801,
      "step": 77200
    },
    {
      "epoch": 0.7053434557266954,
      "grad_norm": 5.389710426330566,
      "learning_rate": 4.9412213786894424e-05,
      "loss": 0.8474,
      "step": 77300
    },
    {
      "epoch": 0.7062559310898606,
      "grad_norm": 5.18172025680542,
      "learning_rate": 4.941145339075845e-05,
      "loss": 0.8556,
      "step": 77400
    },
    {
      "epoch": 0.7071684064530258,
      "grad_norm": 4.6018967628479,
      "learning_rate": 4.9410692994622485e-05,
      "loss": 0.8558,
      "step": 77500
    },
    {
      "epoch": 0.708080881816191,
      "grad_norm": 3.8747928142547607,
      "learning_rate": 4.940993259848651e-05,
      "loss": 0.807,
      "step": 77600
    },
    {
      "epoch": 0.7089933571793562,
      "grad_norm": 3.8489205837249756,
      "learning_rate": 4.940917220235054e-05,
      "loss": 0.8505,
      "step": 77700
    },
    {
      "epoch": 0.7099058325425214,
      "grad_norm": 4.392721652984619,
      "learning_rate": 4.940841180621457e-05,
      "loss": 0.8585,
      "step": 77800
    },
    {
      "epoch": 0.7108183079056866,
      "grad_norm": 5.071272850036621,
      "learning_rate": 4.94076514100786e-05,
      "loss": 0.8594,
      "step": 77900
    },
    {
      "epoch": 0.7117307832688518,
      "grad_norm": 5.879130840301514,
      "learning_rate": 4.940689101394262e-05,
      "loss": 0.8391,
      "step": 78000
    },
    {
      "epoch": 0.7126432586320169,
      "grad_norm": 4.599280834197998,
      "learning_rate": 4.940613061780666e-05,
      "loss": 0.8129,
      "step": 78100
    },
    {
      "epoch": 0.7135557339951821,
      "grad_norm": 5.115828514099121,
      "learning_rate": 4.940537022167068e-05,
      "loss": 0.8307,
      "step": 78200
    },
    {
      "epoch": 0.7144682093583473,
      "grad_norm": 5.0532145500183105,
      "learning_rate": 4.940460982553471e-05,
      "loss": 0.873,
      "step": 78300
    },
    {
      "epoch": 0.7153806847215125,
      "grad_norm": 4.184568405151367,
      "learning_rate": 4.940384942939874e-05,
      "loss": 0.8305,
      "step": 78400
    },
    {
      "epoch": 0.7162931600846777,
      "grad_norm": 4.607684135437012,
      "learning_rate": 4.940308903326277e-05,
      "loss": 0.861,
      "step": 78500
    },
    {
      "epoch": 0.7172056354478429,
      "grad_norm": 4.45864725112915,
      "learning_rate": 4.94023286371268e-05,
      "loss": 0.8553,
      "step": 78600
    },
    {
      "epoch": 0.7181181108110081,
      "grad_norm": 3.7003118991851807,
      "learning_rate": 4.940156824099083e-05,
      "loss": 0.8124,
      "step": 78700
    },
    {
      "epoch": 0.7190305861741733,
      "grad_norm": 3.5617058277130127,
      "learning_rate": 4.9400807844854855e-05,
      "loss": 0.798,
      "step": 78800
    },
    {
      "epoch": 0.7199430615373384,
      "grad_norm": 4.51419734954834,
      "learning_rate": 4.940004744871889e-05,
      "loss": 0.8886,
      "step": 78900
    },
    {
      "epoch": 0.7208555369005036,
      "grad_norm": 3.588139295578003,
      "learning_rate": 4.9399287052582915e-05,
      "loss": 0.7935,
      "step": 79000
    },
    {
      "epoch": 0.7217680122636689,
      "grad_norm": 4.746662139892578,
      "learning_rate": 4.9398526656446945e-05,
      "loss": 0.8474,
      "step": 79100
    },
    {
      "epoch": 0.7226804876268341,
      "grad_norm": 5.28711462020874,
      "learning_rate": 4.9397766260310975e-05,
      "loss": 0.794,
      "step": 79200
    },
    {
      "epoch": 0.7235929629899993,
      "grad_norm": 5.566308975219727,
      "learning_rate": 4.9397005864175005e-05,
      "loss": 0.8654,
      "step": 79300
    },
    {
      "epoch": 0.7245054383531645,
      "grad_norm": 4.700675964355469,
      "learning_rate": 4.939624546803903e-05,
      "loss": 0.8668,
      "step": 79400
    },
    {
      "epoch": 0.7254179137163297,
      "grad_norm": 4.508908271789551,
      "learning_rate": 4.9395485071903066e-05,
      "loss": 0.8683,
      "step": 79500
    },
    {
      "epoch": 0.7263303890794949,
      "grad_norm": 4.685889720916748,
      "learning_rate": 4.939472467576709e-05,
      "loss": 0.8575,
      "step": 79600
    },
    {
      "epoch": 0.7272428644426601,
      "grad_norm": 5.073739051818848,
      "learning_rate": 4.939396427963112e-05,
      "loss": 0.8575,
      "step": 79700
    },
    {
      "epoch": 0.7281553398058253,
      "grad_norm": 4.6471381187438965,
      "learning_rate": 4.939320388349515e-05,
      "loss": 0.8298,
      "step": 79800
    },
    {
      "epoch": 0.7290678151689904,
      "grad_norm": 4.721318244934082,
      "learning_rate": 4.939244348735917e-05,
      "loss": 0.8083,
      "step": 79900
    },
    {
      "epoch": 0.7299802905321556,
      "grad_norm": 4.105016231536865,
      "learning_rate": 4.939168309122321e-05,
      "loss": 0.8387,
      "step": 80000
    },
    {
      "epoch": 0.7308927658953208,
      "grad_norm": 4.181161880493164,
      "learning_rate": 4.939092269508723e-05,
      "loss": 0.82,
      "step": 80100
    },
    {
      "epoch": 0.731805241258486,
      "grad_norm": 4.324868202209473,
      "learning_rate": 4.939016229895126e-05,
      "loss": 0.842,
      "step": 80200
    },
    {
      "epoch": 0.7327177166216512,
      "grad_norm": 4.650515079498291,
      "learning_rate": 4.938940190281529e-05,
      "loss": 0.8441,
      "step": 80300
    },
    {
      "epoch": 0.7336301919848164,
      "grad_norm": 4.224762916564941,
      "learning_rate": 4.938864150667932e-05,
      "loss": 0.8234,
      "step": 80400
    },
    {
      "epoch": 0.7345426673479816,
      "grad_norm": 4.223389625549316,
      "learning_rate": 4.9387881110543346e-05,
      "loss": 0.8254,
      "step": 80500
    },
    {
      "epoch": 0.7354551427111468,
      "grad_norm": 4.766778945922852,
      "learning_rate": 4.938712071440738e-05,
      "loss": 0.8266,
      "step": 80600
    },
    {
      "epoch": 0.736367618074312,
      "grad_norm": 4.829016208648682,
      "learning_rate": 4.9386360318271406e-05,
      "loss": 0.8251,
      "step": 80700
    },
    {
      "epoch": 0.7372800934374771,
      "grad_norm": 5.915027618408203,
      "learning_rate": 4.9385599922135436e-05,
      "loss": 0.7832,
      "step": 80800
    },
    {
      "epoch": 0.7381925688006424,
      "grad_norm": 4.178036689758301,
      "learning_rate": 4.9384839525999466e-05,
      "loss": 0.8254,
      "step": 80900
    },
    {
      "epoch": 0.7391050441638076,
      "grad_norm": 4.463291168212891,
      "learning_rate": 4.9384079129863496e-05,
      "loss": 0.8593,
      "step": 81000
    },
    {
      "epoch": 0.7400175195269728,
      "grad_norm": 3.762373447418213,
      "learning_rate": 4.9383318733727526e-05,
      "loss": 0.8217,
      "step": 81100
    },
    {
      "epoch": 0.740929994890138,
      "grad_norm": 4.81947135925293,
      "learning_rate": 4.9382558337591556e-05,
      "loss": 0.8384,
      "step": 81200
    },
    {
      "epoch": 0.7418424702533032,
      "grad_norm": 4.852123260498047,
      "learning_rate": 4.938179794145558e-05,
      "loss": 0.8044,
      "step": 81300
    },
    {
      "epoch": 0.7427549456164684,
      "grad_norm": 4.933689594268799,
      "learning_rate": 4.9381037545319617e-05,
      "loss": 0.8489,
      "step": 81400
    },
    {
      "epoch": 0.7436674209796336,
      "grad_norm": 4.180702209472656,
      "learning_rate": 4.938027714918364e-05,
      "loss": 0.8404,
      "step": 81500
    },
    {
      "epoch": 0.7445798963427988,
      "grad_norm": 5.73431396484375,
      "learning_rate": 4.937951675304767e-05,
      "loss": 0.8391,
      "step": 81600
    },
    {
      "epoch": 0.7454923717059639,
      "grad_norm": 4.923954486846924,
      "learning_rate": 4.93787563569117e-05,
      "loss": 0.7941,
      "step": 81700
    },
    {
      "epoch": 0.7464048470691291,
      "grad_norm": 4.900060176849365,
      "learning_rate": 4.937799596077573e-05,
      "loss": 0.8127,
      "step": 81800
    },
    {
      "epoch": 0.7473173224322943,
      "grad_norm": 4.708291530609131,
      "learning_rate": 4.937723556463975e-05,
      "loss": 0.7965,
      "step": 81900
    },
    {
      "epoch": 0.7482297977954595,
      "grad_norm": 4.14299201965332,
      "learning_rate": 4.937647516850379e-05,
      "loss": 0.8238,
      "step": 82000
    },
    {
      "epoch": 0.7491422731586247,
      "grad_norm": 4.260895729064941,
      "learning_rate": 4.9375714772367813e-05,
      "loss": 0.8435,
      "step": 82100
    },
    {
      "epoch": 0.7500547485217899,
      "grad_norm": 5.269938945770264,
      "learning_rate": 4.9374954376231844e-05,
      "loss": 0.8121,
      "step": 82200
    },
    {
      "epoch": 0.7509672238849551,
      "grad_norm": 4.494765758514404,
      "learning_rate": 4.9374193980095874e-05,
      "loss": 0.8699,
      "step": 82300
    },
    {
      "epoch": 0.7518796992481203,
      "grad_norm": 4.834014415740967,
      "learning_rate": 4.93734335839599e-05,
      "loss": 0.8418,
      "step": 82400
    },
    {
      "epoch": 0.7527921746112854,
      "grad_norm": 4.078907489776611,
      "learning_rate": 4.9372673187823934e-05,
      "loss": 0.8391,
      "step": 82500
    },
    {
      "epoch": 0.7537046499744506,
      "grad_norm": 3.4347715377807617,
      "learning_rate": 4.937191279168796e-05,
      "loss": 0.8194,
      "step": 82600
    },
    {
      "epoch": 0.7546171253376159,
      "grad_norm": 5.28909158706665,
      "learning_rate": 4.937115239555199e-05,
      "loss": 0.8375,
      "step": 82700
    },
    {
      "epoch": 0.7555296007007811,
      "grad_norm": 5.127830982208252,
      "learning_rate": 4.937039199941602e-05,
      "loss": 0.8517,
      "step": 82800
    },
    {
      "epoch": 0.7564420760639463,
      "grad_norm": 5.0216240882873535,
      "learning_rate": 4.936963160328005e-05,
      "loss": 0.8662,
      "step": 82900
    },
    {
      "epoch": 0.7573545514271115,
      "grad_norm": 4.077911853790283,
      "learning_rate": 4.936887120714408e-05,
      "loss": 0.8077,
      "step": 83000
    },
    {
      "epoch": 0.7582670267902767,
      "grad_norm": 4.182953834533691,
      "learning_rate": 4.936811081100811e-05,
      "loss": 0.8132,
      "step": 83100
    },
    {
      "epoch": 0.7591795021534419,
      "grad_norm": 5.015237331390381,
      "learning_rate": 4.936735041487213e-05,
      "loss": 0.8239,
      "step": 83200
    },
    {
      "epoch": 0.7600919775166071,
      "grad_norm": 3.561929225921631,
      "learning_rate": 4.936659001873616e-05,
      "loss": 0.8396,
      "step": 83300
    },
    {
      "epoch": 0.7610044528797723,
      "grad_norm": 4.0269246101379395,
      "learning_rate": 4.936582962260019e-05,
      "loss": 0.831,
      "step": 83400
    },
    {
      "epoch": 0.7619169282429374,
      "grad_norm": 5.231485843658447,
      "learning_rate": 4.936506922646422e-05,
      "loss": 0.8288,
      "step": 83500
    },
    {
      "epoch": 0.7628294036061026,
      "grad_norm": 3.9078054428100586,
      "learning_rate": 4.936430883032825e-05,
      "loss": 0.8277,
      "step": 83600
    },
    {
      "epoch": 0.7637418789692678,
      "grad_norm": 4.378294944763184,
      "learning_rate": 4.936354843419228e-05,
      "loss": 0.828,
      "step": 83700
    },
    {
      "epoch": 0.764654354332433,
      "grad_norm": 5.461452960968018,
      "learning_rate": 4.9362788038056304e-05,
      "loss": 0.8497,
      "step": 83800
    },
    {
      "epoch": 0.7655668296955982,
      "grad_norm": 4.476361274719238,
      "learning_rate": 4.936202764192034e-05,
      "loss": 0.8097,
      "step": 83900
    },
    {
      "epoch": 0.7664793050587634,
      "grad_norm": 4.691394805908203,
      "learning_rate": 4.9361267245784364e-05,
      "loss": 0.8477,
      "step": 84000
    },
    {
      "epoch": 0.7673917804219286,
      "grad_norm": 4.5455827713012695,
      "learning_rate": 4.9360506849648394e-05,
      "loss": 0.837,
      "step": 84100
    },
    {
      "epoch": 0.7683042557850938,
      "grad_norm": 4.977505207061768,
      "learning_rate": 4.9359746453512425e-05,
      "loss": 0.865,
      "step": 84200
    },
    {
      "epoch": 0.769216731148259,
      "grad_norm": 3.8330132961273193,
      "learning_rate": 4.9358986057376455e-05,
      "loss": 0.8604,
      "step": 84300
    },
    {
      "epoch": 0.7701292065114242,
      "grad_norm": 4.165061950683594,
      "learning_rate": 4.9358225661240485e-05,
      "loss": 0.8137,
      "step": 84400
    },
    {
      "epoch": 0.7710416818745894,
      "grad_norm": 4.183239936828613,
      "learning_rate": 4.9357465265104515e-05,
      "loss": 0.8418,
      "step": 84500
    },
    {
      "epoch": 0.7719541572377546,
      "grad_norm": 4.276828765869141,
      "learning_rate": 4.935670486896854e-05,
      "loss": 0.819,
      "step": 84600
    },
    {
      "epoch": 0.7728666326009198,
      "grad_norm": 4.8378448486328125,
      "learning_rate": 4.9355944472832575e-05,
      "loss": 0.8538,
      "step": 84700
    },
    {
      "epoch": 0.773779107964085,
      "grad_norm": 4.833832263946533,
      "learning_rate": 4.93551840766966e-05,
      "loss": 0.8515,
      "step": 84800
    },
    {
      "epoch": 0.7746915833272502,
      "grad_norm": 4.399167537689209,
      "learning_rate": 4.935442368056063e-05,
      "loss": 0.8086,
      "step": 84900
    },
    {
      "epoch": 0.7756040586904154,
      "grad_norm": 4.619300842285156,
      "learning_rate": 4.935366328442466e-05,
      "loss": 0.836,
      "step": 85000
    },
    {
      "epoch": 0.7765165340535806,
      "grad_norm": 5.018957138061523,
      "learning_rate": 4.935290288828869e-05,
      "loss": 0.8002,
      "step": 85100
    },
    {
      "epoch": 0.7774290094167458,
      "grad_norm": 4.277730464935303,
      "learning_rate": 4.935214249215271e-05,
      "loss": 0.8244,
      "step": 85200
    },
    {
      "epoch": 0.7783414847799109,
      "grad_norm": 3.882298231124878,
      "learning_rate": 4.935138209601674e-05,
      "loss": 0.8657,
      "step": 85300
    },
    {
      "epoch": 0.7792539601430761,
      "grad_norm": 4.925764560699463,
      "learning_rate": 4.935062169988077e-05,
      "loss": 0.8526,
      "step": 85400
    },
    {
      "epoch": 0.7801664355062413,
      "grad_norm": 4.468506336212158,
      "learning_rate": 4.93498613037448e-05,
      "loss": 0.8009,
      "step": 85500
    },
    {
      "epoch": 0.7810789108694065,
      "grad_norm": 5.089028835296631,
      "learning_rate": 4.934910090760883e-05,
      "loss": 0.8434,
      "step": 85600
    },
    {
      "epoch": 0.7819913862325717,
      "grad_norm": 5.1554388999938965,
      "learning_rate": 4.9348340511472855e-05,
      "loss": 0.8667,
      "step": 85700
    },
    {
      "epoch": 0.7829038615957369,
      "grad_norm": 3.8023324012756348,
      "learning_rate": 4.934758011533689e-05,
      "loss": 0.8161,
      "step": 85800
    },
    {
      "epoch": 0.7838163369589021,
      "grad_norm": 4.654780387878418,
      "learning_rate": 4.9346819719200915e-05,
      "loss": 0.8404,
      "step": 85900
    },
    {
      "epoch": 0.7847288123220673,
      "grad_norm": 2.8039894104003906,
      "learning_rate": 4.9346059323064945e-05,
      "loss": 0.8102,
      "step": 86000
    },
    {
      "epoch": 0.7856412876852324,
      "grad_norm": 4.517291069030762,
      "learning_rate": 4.9345298926928975e-05,
      "loss": 0.8153,
      "step": 86100
    },
    {
      "epoch": 0.7865537630483977,
      "grad_norm": 3.7413532733917236,
      "learning_rate": 4.9344538530793006e-05,
      "loss": 0.7867,
      "step": 86200
    },
    {
      "epoch": 0.7874662384115629,
      "grad_norm": 4.505010604858398,
      "learning_rate": 4.934377813465703e-05,
      "loss": 0.8377,
      "step": 86300
    },
    {
      "epoch": 0.7883787137747281,
      "grad_norm": 5.705231189727783,
      "learning_rate": 4.9343017738521066e-05,
      "loss": 0.8314,
      "step": 86400
    },
    {
      "epoch": 0.7892911891378933,
      "grad_norm": 4.573914527893066,
      "learning_rate": 4.934225734238509e-05,
      "loss": 0.8249,
      "step": 86500
    },
    {
      "epoch": 0.7902036645010585,
      "grad_norm": 3.5666134357452393,
      "learning_rate": 4.934149694624912e-05,
      "loss": 0.8234,
      "step": 86600
    },
    {
      "epoch": 0.7911161398642237,
      "grad_norm": 4.860886096954346,
      "learning_rate": 4.934073655011315e-05,
      "loss": 0.816,
      "step": 86700
    },
    {
      "epoch": 0.7920286152273889,
      "grad_norm": 4.2314863204956055,
      "learning_rate": 4.933997615397718e-05,
      "loss": 0.8162,
      "step": 86800
    },
    {
      "epoch": 0.7929410905905541,
      "grad_norm": 4.378541469573975,
      "learning_rate": 4.933921575784121e-05,
      "loss": 0.8139,
      "step": 86900
    },
    {
      "epoch": 0.7938535659537193,
      "grad_norm": 3.961730480194092,
      "learning_rate": 4.933845536170524e-05,
      "loss": 0.8406,
      "step": 87000
    },
    {
      "epoch": 0.7947660413168844,
      "grad_norm": 4.271843433380127,
      "learning_rate": 4.933769496556926e-05,
      "loss": 0.82,
      "step": 87100
    },
    {
      "epoch": 0.7956785166800496,
      "grad_norm": 4.080942630767822,
      "learning_rate": 4.93369345694333e-05,
      "loss": 0.7915,
      "step": 87200
    },
    {
      "epoch": 0.7965909920432148,
      "grad_norm": 4.274077892303467,
      "learning_rate": 4.933617417329732e-05,
      "loss": 0.8724,
      "step": 87300
    },
    {
      "epoch": 0.79750346740638,
      "grad_norm": 4.241261959075928,
      "learning_rate": 4.933541377716135e-05,
      "loss": 0.8031,
      "step": 87400
    },
    {
      "epoch": 0.7984159427695452,
      "grad_norm": 3.8581318855285645,
      "learning_rate": 4.933465338102538e-05,
      "loss": 0.8278,
      "step": 87500
    },
    {
      "epoch": 0.7993284181327104,
      "grad_norm": 4.490302562713623,
      "learning_rate": 4.933389298488941e-05,
      "loss": 0.8584,
      "step": 87600
    },
    {
      "epoch": 0.8002408934958756,
      "grad_norm": 5.00104284286499,
      "learning_rate": 4.9333132588753436e-05,
      "loss": 0.8606,
      "step": 87700
    },
    {
      "epoch": 0.8011533688590408,
      "grad_norm": 3.3440651893615723,
      "learning_rate": 4.933237219261747e-05,
      "loss": 0.8403,
      "step": 87800
    },
    {
      "epoch": 0.802065844222206,
      "grad_norm": 4.0537919998168945,
      "learning_rate": 4.9331611796481496e-05,
      "loss": 0.8202,
      "step": 87900
    },
    {
      "epoch": 0.8029783195853712,
      "grad_norm": 3.298736572265625,
      "learning_rate": 4.9330851400345526e-05,
      "loss": 0.873,
      "step": 88000
    },
    {
      "epoch": 0.8038907949485364,
      "grad_norm": 3.1193559169769287,
      "learning_rate": 4.9330091004209556e-05,
      "loss": 0.8291,
      "step": 88100
    },
    {
      "epoch": 0.8048032703117016,
      "grad_norm": 4.043341159820557,
      "learning_rate": 4.932933060807358e-05,
      "loss": 0.8402,
      "step": 88200
    },
    {
      "epoch": 0.8057157456748668,
      "grad_norm": 3.840322494506836,
      "learning_rate": 4.932857021193762e-05,
      "loss": 0.8218,
      "step": 88300
    },
    {
      "epoch": 0.806628221038032,
      "grad_norm": 4.854350566864014,
      "learning_rate": 4.932780981580164e-05,
      "loss": 0.8331,
      "step": 88400
    },
    {
      "epoch": 0.8075406964011972,
      "grad_norm": 5.199192047119141,
      "learning_rate": 4.932704941966567e-05,
      "loss": 0.8191,
      "step": 88500
    },
    {
      "epoch": 0.8084531717643624,
      "grad_norm": 4.356496810913086,
      "learning_rate": 4.93262890235297e-05,
      "loss": 0.8443,
      "step": 88600
    },
    {
      "epoch": 0.8093656471275276,
      "grad_norm": 3.9440205097198486,
      "learning_rate": 4.932552862739373e-05,
      "loss": 0.8348,
      "step": 88700
    },
    {
      "epoch": 0.8102781224906928,
      "grad_norm": 4.54090690612793,
      "learning_rate": 4.9324768231257753e-05,
      "loss": 0.8403,
      "step": 88800
    },
    {
      "epoch": 0.8111905978538579,
      "grad_norm": 4.447544097900391,
      "learning_rate": 4.932400783512179e-05,
      "loss": 0.8375,
      "step": 88900
    },
    {
      "epoch": 0.8121030732170231,
      "grad_norm": 4.566890716552734,
      "learning_rate": 4.9323247438985814e-05,
      "loss": 0.8337,
      "step": 89000
    },
    {
      "epoch": 0.8130155485801883,
      "grad_norm": 4.9653167724609375,
      "learning_rate": 4.9322487042849844e-05,
      "loss": 0.8216,
      "step": 89100
    },
    {
      "epoch": 0.8139280239433535,
      "grad_norm": 4.514758110046387,
      "learning_rate": 4.9321726646713874e-05,
      "loss": 0.8033,
      "step": 89200
    },
    {
      "epoch": 0.8148404993065187,
      "grad_norm": 4.57930326461792,
      "learning_rate": 4.9320966250577904e-05,
      "loss": 0.8615,
      "step": 89300
    },
    {
      "epoch": 0.8157529746696839,
      "grad_norm": 4.192154407501221,
      "learning_rate": 4.9320205854441934e-05,
      "loss": 0.7909,
      "step": 89400
    },
    {
      "epoch": 0.8166654500328491,
      "grad_norm": 5.125550746917725,
      "learning_rate": 4.9319445458305964e-05,
      "loss": 0.7466,
      "step": 89500
    },
    {
      "epoch": 0.8175779253960143,
      "grad_norm": 4.338044166564941,
      "learning_rate": 4.931868506216999e-05,
      "loss": 0.8134,
      "step": 89600
    },
    {
      "epoch": 0.8184904007591794,
      "grad_norm": 3.4449684619903564,
      "learning_rate": 4.9317924666034024e-05,
      "loss": 0.8192,
      "step": 89700
    },
    {
      "epoch": 0.8194028761223447,
      "grad_norm": 4.257543563842773,
      "learning_rate": 4.931716426989805e-05,
      "loss": 0.8462,
      "step": 89800
    },
    {
      "epoch": 0.8203153514855099,
      "grad_norm": 5.289962291717529,
      "learning_rate": 4.931640387376208e-05,
      "loss": 0.8226,
      "step": 89900
    },
    {
      "epoch": 0.8212278268486751,
      "grad_norm": 5.156586647033691,
      "learning_rate": 4.931564347762611e-05,
      "loss": 0.8483,
      "step": 90000
    },
    {
      "epoch": 0.8221403022118403,
      "grad_norm": 5.811824798583984,
      "learning_rate": 4.931488308149014e-05,
      "loss": 0.8558,
      "step": 90100
    },
    {
      "epoch": 0.8230527775750055,
      "grad_norm": 4.010898113250732,
      "learning_rate": 4.931412268535416e-05,
      "loss": 0.8298,
      "step": 90200
    },
    {
      "epoch": 0.8239652529381707,
      "grad_norm": 4.552169322967529,
      "learning_rate": 4.93133622892182e-05,
      "loss": 0.8705,
      "step": 90300
    },
    {
      "epoch": 0.8248777283013359,
      "grad_norm": 5.070095062255859,
      "learning_rate": 4.931260189308222e-05,
      "loss": 0.852,
      "step": 90400
    },
    {
      "epoch": 0.8257902036645011,
      "grad_norm": 4.923733711242676,
      "learning_rate": 4.931184149694625e-05,
      "loss": 0.838,
      "step": 90500
    },
    {
      "epoch": 0.8267026790276663,
      "grad_norm": 4.719942569732666,
      "learning_rate": 4.931108110081028e-05,
      "loss": 0.8025,
      "step": 90600
    },
    {
      "epoch": 0.8276151543908314,
      "grad_norm": 3.903637647628784,
      "learning_rate": 4.931032070467431e-05,
      "loss": 0.8122,
      "step": 90700
    },
    {
      "epoch": 0.8285276297539966,
      "grad_norm": 4.348480701446533,
      "learning_rate": 4.930956030853834e-05,
      "loss": 0.8297,
      "step": 90800
    },
    {
      "epoch": 0.8294401051171618,
      "grad_norm": 3.9478089809417725,
      "learning_rate": 4.9308799912402364e-05,
      "loss": 0.8164,
      "step": 90900
    },
    {
      "epoch": 0.830352580480327,
      "grad_norm": 2.9257476329803467,
      "learning_rate": 4.9308039516266395e-05,
      "loss": 0.8259,
      "step": 91000
    },
    {
      "epoch": 0.8312650558434922,
      "grad_norm": 4.219361305236816,
      "learning_rate": 4.9307279120130425e-05,
      "loss": 0.8293,
      "step": 91100
    },
    {
      "epoch": 0.8321775312066574,
      "grad_norm": 3.513094663619995,
      "learning_rate": 4.9306518723994455e-05,
      "loss": 0.8213,
      "step": 91200
    },
    {
      "epoch": 0.8330900065698226,
      "grad_norm": 3.8263747692108154,
      "learning_rate": 4.930575832785848e-05,
      "loss": 0.8503,
      "step": 91300
    },
    {
      "epoch": 0.8340024819329878,
      "grad_norm": 3.4157955646514893,
      "learning_rate": 4.9304997931722515e-05,
      "loss": 0.816,
      "step": 91400
    },
    {
      "epoch": 0.834914957296153,
      "grad_norm": 4.516419410705566,
      "learning_rate": 4.930423753558654e-05,
      "loss": 0.8532,
      "step": 91500
    },
    {
      "epoch": 0.8358274326593182,
      "grad_norm": 4.195001125335693,
      "learning_rate": 4.930347713945057e-05,
      "loss": 0.7935,
      "step": 91600
    },
    {
      "epoch": 0.8367399080224834,
      "grad_norm": 4.643110275268555,
      "learning_rate": 4.93027167433146e-05,
      "loss": 0.8756,
      "step": 91700
    },
    {
      "epoch": 0.8376523833856486,
      "grad_norm": 5.741189479827881,
      "learning_rate": 4.930195634717863e-05,
      "loss": 0.8316,
      "step": 91800
    },
    {
      "epoch": 0.8385648587488138,
      "grad_norm": 3.861846685409546,
      "learning_rate": 4.930119595104266e-05,
      "loss": 0.8106,
      "step": 91900
    },
    {
      "epoch": 0.839477334111979,
      "grad_norm": 4.73490571975708,
      "learning_rate": 4.930043555490669e-05,
      "loss": 0.8087,
      "step": 92000
    },
    {
      "epoch": 0.8403898094751442,
      "grad_norm": 3.5237624645233154,
      "learning_rate": 4.929967515877071e-05,
      "loss": 0.7609,
      "step": 92100
    },
    {
      "epoch": 0.8413022848383094,
      "grad_norm": 3.9503321647644043,
      "learning_rate": 4.929891476263475e-05,
      "loss": 0.7831,
      "step": 92200
    },
    {
      "epoch": 0.8422147602014746,
      "grad_norm": 4.711704730987549,
      "learning_rate": 4.929815436649877e-05,
      "loss": 0.8758,
      "step": 92300
    },
    {
      "epoch": 0.8431272355646398,
      "grad_norm": 4.428909778594971,
      "learning_rate": 4.92973939703628e-05,
      "loss": 0.8116,
      "step": 92400
    },
    {
      "epoch": 0.8440397109278049,
      "grad_norm": 4.171692371368408,
      "learning_rate": 4.929663357422683e-05,
      "loss": 0.7813,
      "step": 92500
    },
    {
      "epoch": 0.8449521862909701,
      "grad_norm": 4.87624979019165,
      "learning_rate": 4.929587317809086e-05,
      "loss": 0.8014,
      "step": 92600
    },
    {
      "epoch": 0.8458646616541353,
      "grad_norm": 5.238212585449219,
      "learning_rate": 4.9295112781954885e-05,
      "loss": 0.8667,
      "step": 92700
    },
    {
      "epoch": 0.8467771370173005,
      "grad_norm": 5.2574896812438965,
      "learning_rate": 4.929435238581892e-05,
      "loss": 0.8323,
      "step": 92800
    },
    {
      "epoch": 0.8476896123804657,
      "grad_norm": 4.23288631439209,
      "learning_rate": 4.9293591989682946e-05,
      "loss": 0.8307,
      "step": 92900
    },
    {
      "epoch": 0.8486020877436309,
      "grad_norm": 4.431314945220947,
      "learning_rate": 4.9292831593546976e-05,
      "loss": 0.8131,
      "step": 93000
    },
    {
      "epoch": 0.8495145631067961,
      "grad_norm": 4.237278461456299,
      "learning_rate": 4.9292071197411006e-05,
      "loss": 0.8881,
      "step": 93100
    },
    {
      "epoch": 0.8504270384699613,
      "grad_norm": 4.318561553955078,
      "learning_rate": 4.9291310801275036e-05,
      "loss": 0.812,
      "step": 93200
    },
    {
      "epoch": 0.8513395138331266,
      "grad_norm": 4.127457618713379,
      "learning_rate": 4.9290550405139066e-05,
      "loss": 0.8081,
      "step": 93300
    },
    {
      "epoch": 0.8522519891962917,
      "grad_norm": 4.378271579742432,
      "learning_rate": 4.9289790009003096e-05,
      "loss": 0.8493,
      "step": 93400
    },
    {
      "epoch": 0.8531644645594569,
      "grad_norm": 4.862797260284424,
      "learning_rate": 4.928902961286712e-05,
      "loss": 0.8203,
      "step": 93500
    },
    {
      "epoch": 0.8540769399226221,
      "grad_norm": 4.263881206512451,
      "learning_rate": 4.9288269216731156e-05,
      "loss": 0.8243,
      "step": 93600
    },
    {
      "epoch": 0.8549894152857873,
      "grad_norm": 4.115057945251465,
      "learning_rate": 4.928750882059518e-05,
      "loss": 0.7897,
      "step": 93700
    },
    {
      "epoch": 0.8559018906489525,
      "grad_norm": 4.02838134765625,
      "learning_rate": 4.92867484244592e-05,
      "loss": 0.7553,
      "step": 93800
    },
    {
      "epoch": 0.8568143660121177,
      "grad_norm": 4.784608364105225,
      "learning_rate": 4.928598802832324e-05,
      "loss": 0.8046,
      "step": 93900
    },
    {
      "epoch": 0.8577268413752829,
      "grad_norm": 4.254231929779053,
      "learning_rate": 4.928522763218726e-05,
      "loss": 0.7455,
      "step": 94000
    },
    {
      "epoch": 0.8586393167384481,
      "grad_norm": 4.496993541717529,
      "learning_rate": 4.928446723605129e-05,
      "loss": 0.841,
      "step": 94100
    },
    {
      "epoch": 0.8595517921016133,
      "grad_norm": 4.228516578674316,
      "learning_rate": 4.928370683991532e-05,
      "loss": 0.7997,
      "step": 94200
    },
    {
      "epoch": 0.8604642674647784,
      "grad_norm": 4.209831714630127,
      "learning_rate": 4.928294644377935e-05,
      "loss": 0.7774,
      "step": 94300
    },
    {
      "epoch": 0.8613767428279436,
      "grad_norm": 4.667964935302734,
      "learning_rate": 4.928218604764338e-05,
      "loss": 0.8281,
      "step": 94400
    },
    {
      "epoch": 0.8622892181911088,
      "grad_norm": 4.248825550079346,
      "learning_rate": 4.928142565150741e-05,
      "loss": 0.8383,
      "step": 94500
    },
    {
      "epoch": 0.863201693554274,
      "grad_norm": 3.569049119949341,
      "learning_rate": 4.9280665255371436e-05,
      "loss": 0.8239,
      "step": 94600
    },
    {
      "epoch": 0.8641141689174392,
      "grad_norm": 4.663922309875488,
      "learning_rate": 4.927990485923547e-05,
      "loss": 0.8306,
      "step": 94700
    },
    {
      "epoch": 0.8650266442806044,
      "grad_norm": 2.4045658111572266,
      "learning_rate": 4.9279144463099496e-05,
      "loss": 0.8582,
      "step": 94800
    },
    {
      "epoch": 0.8659391196437696,
      "grad_norm": 4.205658435821533,
      "learning_rate": 4.9278384066963527e-05,
      "loss": 0.7863,
      "step": 94900
    },
    {
      "epoch": 0.8668515950069348,
      "grad_norm": 4.192543029785156,
      "learning_rate": 4.9277623670827557e-05,
      "loss": 0.7865,
      "step": 95000
    },
    {
      "epoch": 0.8677640703701001,
      "grad_norm": 4.035797595977783,
      "learning_rate": 4.927686327469159e-05,
      "loss": 0.8243,
      "step": 95100
    },
    {
      "epoch": 0.8686765457332652,
      "grad_norm": 4.079620838165283,
      "learning_rate": 4.927610287855562e-05,
      "loss": 0.8309,
      "step": 95200
    },
    {
      "epoch": 0.8695890210964304,
      "grad_norm": 4.9579548835754395,
      "learning_rate": 4.927534248241965e-05,
      "loss": 0.8484,
      "step": 95300
    },
    {
      "epoch": 0.8705014964595956,
      "grad_norm": 4.3537797927856445,
      "learning_rate": 4.927458208628367e-05,
      "loss": 0.7887,
      "step": 95400
    },
    {
      "epoch": 0.8714139718227608,
      "grad_norm": 5.397519588470459,
      "learning_rate": 4.92738216901477e-05,
      "loss": 0.8336,
      "step": 95500
    },
    {
      "epoch": 0.872326447185926,
      "grad_norm": 4.7006659507751465,
      "learning_rate": 4.927306129401173e-05,
      "loss": 0.8252,
      "step": 95600
    },
    {
      "epoch": 0.8732389225490912,
      "grad_norm": 4.504152297973633,
      "learning_rate": 4.927230089787576e-05,
      "loss": 0.8375,
      "step": 95700
    },
    {
      "epoch": 0.8741513979122564,
      "grad_norm": 4.821296691894531,
      "learning_rate": 4.927154050173979e-05,
      "loss": 0.8261,
      "step": 95800
    },
    {
      "epoch": 0.8750638732754216,
      "grad_norm": 4.691882133483887,
      "learning_rate": 4.927078010560382e-05,
      "loss": 0.828,
      "step": 95900
    },
    {
      "epoch": 0.8759763486385868,
      "grad_norm": 4.3082075119018555,
      "learning_rate": 4.9270019709467844e-05,
      "loss": 0.8404,
      "step": 96000
    },
    {
      "epoch": 0.8768888240017519,
      "grad_norm": 4.327220439910889,
      "learning_rate": 4.926925931333188e-05,
      "loss": 0.8019,
      "step": 96100
    },
    {
      "epoch": 0.8778012993649171,
      "grad_norm": 4.894442558288574,
      "learning_rate": 4.9268498917195904e-05,
      "loss": 0.8227,
      "step": 96200
    },
    {
      "epoch": 0.8787137747280823,
      "grad_norm": 4.060361862182617,
      "learning_rate": 4.9267738521059934e-05,
      "loss": 0.815,
      "step": 96300
    },
    {
      "epoch": 0.8796262500912475,
      "grad_norm": 4.348201751708984,
      "learning_rate": 4.9266978124923964e-05,
      "loss": 0.8396,
      "step": 96400
    },
    {
      "epoch": 0.8805387254544127,
      "grad_norm": 4.711004734039307,
      "learning_rate": 4.9266217728787994e-05,
      "loss": 0.8082,
      "step": 96500
    },
    {
      "epoch": 0.8814512008175779,
      "grad_norm": 3.671313524246216,
      "learning_rate": 4.9265457332652024e-05,
      "loss": 0.8072,
      "step": 96600
    },
    {
      "epoch": 0.8823636761807431,
      "grad_norm": 5.566470623016357,
      "learning_rate": 4.926469693651605e-05,
      "loss": 0.8203,
      "step": 96700
    },
    {
      "epoch": 0.8832761515439083,
      "grad_norm": 4.083470821380615,
      "learning_rate": 4.926393654038008e-05,
      "loss": 0.8341,
      "step": 96800
    },
    {
      "epoch": 0.8841886269070736,
      "grad_norm": 4.59853458404541,
      "learning_rate": 4.926317614424411e-05,
      "loss": 0.8069,
      "step": 96900
    },
    {
      "epoch": 0.8851011022702387,
      "grad_norm": 3.283491373062134,
      "learning_rate": 4.926241574810814e-05,
      "loss": 0.7922,
      "step": 97000
    },
    {
      "epoch": 0.8860135776334039,
      "grad_norm": 4.403841018676758,
      "learning_rate": 4.926165535197216e-05,
      "loss": 0.8287,
      "step": 97100
    },
    {
      "epoch": 0.8869260529965691,
      "grad_norm": 4.857583045959473,
      "learning_rate": 4.92608949558362e-05,
      "loss": 0.8477,
      "step": 97200
    },
    {
      "epoch": 0.8878385283597343,
      "grad_norm": 3.9166483879089355,
      "learning_rate": 4.926013455970022e-05,
      "loss": 0.7831,
      "step": 97300
    },
    {
      "epoch": 0.8887510037228995,
      "grad_norm": 4.744034290313721,
      "learning_rate": 4.925937416356425e-05,
      "loss": 0.8498,
      "step": 97400
    },
    {
      "epoch": 0.8896634790860647,
      "grad_norm": 3.9811458587646484,
      "learning_rate": 4.925861376742828e-05,
      "loss": 0.803,
      "step": 97500
    },
    {
      "epoch": 0.8905759544492299,
      "grad_norm": 4.382560729980469,
      "learning_rate": 4.925785337129231e-05,
      "loss": 0.8326,
      "step": 97600
    },
    {
      "epoch": 0.8914884298123951,
      "grad_norm": 3.5730721950531006,
      "learning_rate": 4.925709297515634e-05,
      "loss": 0.805,
      "step": 97700
    },
    {
      "epoch": 0.8924009051755603,
      "grad_norm": 4.5342888832092285,
      "learning_rate": 4.925633257902037e-05,
      "loss": 0.7856,
      "step": 97800
    },
    {
      "epoch": 0.8933133805387254,
      "grad_norm": 4.610008716583252,
      "learning_rate": 4.9255572182884395e-05,
      "loss": 0.8235,
      "step": 97900
    },
    {
      "epoch": 0.8942258559018906,
      "grad_norm": 4.12315034866333,
      "learning_rate": 4.925481178674843e-05,
      "loss": 0.8233,
      "step": 98000
    },
    {
      "epoch": 0.8951383312650558,
      "grad_norm": 4.273352146148682,
      "learning_rate": 4.9254051390612455e-05,
      "loss": 0.8195,
      "step": 98100
    },
    {
      "epoch": 0.896050806628221,
      "grad_norm": 5.011047840118408,
      "learning_rate": 4.9253290994476485e-05,
      "loss": 0.8052,
      "step": 98200
    },
    {
      "epoch": 0.8969632819913862,
      "grad_norm": 4.3210530281066895,
      "learning_rate": 4.9252530598340515e-05,
      "loss": 0.8121,
      "step": 98300
    },
    {
      "epoch": 0.8978757573545514,
      "grad_norm": 4.529335021972656,
      "learning_rate": 4.9251770202204545e-05,
      "loss": 0.8035,
      "step": 98400
    },
    {
      "epoch": 0.8987882327177166,
      "grad_norm": 4.157739162445068,
      "learning_rate": 4.925100980606857e-05,
      "loss": 0.7978,
      "step": 98500
    },
    {
      "epoch": 0.8997007080808818,
      "grad_norm": 4.613167762756348,
      "learning_rate": 4.9250249409932605e-05,
      "loss": 0.8509,
      "step": 98600
    },
    {
      "epoch": 0.9006131834440471,
      "grad_norm": 4.6933698654174805,
      "learning_rate": 4.924948901379663e-05,
      "loss": 0.8044,
      "step": 98700
    },
    {
      "epoch": 0.9015256588072122,
      "grad_norm": 5.267491817474365,
      "learning_rate": 4.924872861766066e-05,
      "loss": 0.8005,
      "step": 98800
    },
    {
      "epoch": 0.9024381341703774,
      "grad_norm": 4.191797733306885,
      "learning_rate": 4.924796822152469e-05,
      "loss": 0.7771,
      "step": 98900
    },
    {
      "epoch": 0.9033506095335426,
      "grad_norm": 5.274658679962158,
      "learning_rate": 4.924720782538872e-05,
      "loss": 0.785,
      "step": 99000
    },
    {
      "epoch": 0.9042630848967078,
      "grad_norm": 3.9826467037200928,
      "learning_rate": 4.924644742925275e-05,
      "loss": 0.8407,
      "step": 99100
    },
    {
      "epoch": 0.905175560259873,
      "grad_norm": 4.56985330581665,
      "learning_rate": 4.924568703311678e-05,
      "loss": 0.814,
      "step": 99200
    },
    {
      "epoch": 0.9060880356230382,
      "grad_norm": 3.3437278270721436,
      "learning_rate": 4.92449266369808e-05,
      "loss": 0.772,
      "step": 99300
    },
    {
      "epoch": 0.9070005109862034,
      "grad_norm": 4.452041149139404,
      "learning_rate": 4.924416624084484e-05,
      "loss": 0.8209,
      "step": 99400
    },
    {
      "epoch": 0.9079129863493686,
      "grad_norm": 4.208791732788086,
      "learning_rate": 4.924340584470886e-05,
      "loss": 0.8426,
      "step": 99500
    },
    {
      "epoch": 0.9088254617125338,
      "grad_norm": 3.6011016368865967,
      "learning_rate": 4.9242645448572885e-05,
      "loss": 0.7987,
      "step": 99600
    },
    {
      "epoch": 0.9097379370756989,
      "grad_norm": 5.137580871582031,
      "learning_rate": 4.924188505243692e-05,
      "loss": 0.8003,
      "step": 99700
    },
    {
      "epoch": 0.9106504124388641,
      "grad_norm": 4.471158027648926,
      "learning_rate": 4.9241124656300946e-05,
      "loss": 0.8023,
      "step": 99800
    },
    {
      "epoch": 0.9115628878020293,
      "grad_norm": 4.66707706451416,
      "learning_rate": 4.9240364260164976e-05,
      "loss": 0.8214,
      "step": 99900
    },
    {
      "epoch": 0.9124753631651945,
      "grad_norm": 4.662070274353027,
      "learning_rate": 4.9239603864029006e-05,
      "loss": 0.8289,
      "step": 100000
    },
    {
      "epoch": 0.9133878385283597,
      "grad_norm": 3.653085708618164,
      "learning_rate": 4.9238843467893036e-05,
      "loss": 0.797,
      "step": 100100
    },
    {
      "epoch": 0.9143003138915249,
      "grad_norm": 3.9109692573547363,
      "learning_rate": 4.9238083071757066e-05,
      "loss": 0.8478,
      "step": 100200
    },
    {
      "epoch": 0.9152127892546901,
      "grad_norm": 4.611108779907227,
      "learning_rate": 4.9237322675621096e-05,
      "loss": 0.7964,
      "step": 100300
    },
    {
      "epoch": 0.9161252646178554,
      "grad_norm": 4.857477188110352,
      "learning_rate": 4.923656227948512e-05,
      "loss": 0.8261,
      "step": 100400
    },
    {
      "epoch": 0.9170377399810206,
      "grad_norm": 4.041773319244385,
      "learning_rate": 4.9235801883349156e-05,
      "loss": 0.8455,
      "step": 100500
    },
    {
      "epoch": 0.9179502153441857,
      "grad_norm": 4.177624702453613,
      "learning_rate": 4.923504148721318e-05,
      "loss": 0.8537,
      "step": 100600
    },
    {
      "epoch": 0.9188626907073509,
      "grad_norm": 5.259537220001221,
      "learning_rate": 4.923428109107721e-05,
      "loss": 0.8289,
      "step": 100700
    },
    {
      "epoch": 0.9197751660705161,
      "grad_norm": 3.7977654933929443,
      "learning_rate": 4.923352069494124e-05,
      "loss": 0.8007,
      "step": 100800
    },
    {
      "epoch": 0.9206876414336813,
      "grad_norm": 3.6080336570739746,
      "learning_rate": 4.923276029880527e-05,
      "loss": 0.794,
      "step": 100900
    },
    {
      "epoch": 0.9216001167968465,
      "grad_norm": 5.056665420532227,
      "learning_rate": 4.923199990266929e-05,
      "loss": 0.7996,
      "step": 101000
    },
    {
      "epoch": 0.9225125921600117,
      "grad_norm": 4.252306938171387,
      "learning_rate": 4.923123950653333e-05,
      "loss": 0.8173,
      "step": 101100
    },
    {
      "epoch": 0.9234250675231769,
      "grad_norm": 4.442465305328369,
      "learning_rate": 4.923047911039735e-05,
      "loss": 0.8323,
      "step": 101200
    },
    {
      "epoch": 0.9243375428863421,
      "grad_norm": 5.036653518676758,
      "learning_rate": 4.922971871426138e-05,
      "loss": 0.8339,
      "step": 101300
    },
    {
      "epoch": 0.9252500182495073,
      "grad_norm": 4.573465824127197,
      "learning_rate": 4.922895831812541e-05,
      "loss": 0.8014,
      "step": 101400
    },
    {
      "epoch": 0.9261624936126724,
      "grad_norm": 5.018781661987305,
      "learning_rate": 4.922819792198944e-05,
      "loss": 0.7749,
      "step": 101500
    },
    {
      "epoch": 0.9270749689758376,
      "grad_norm": 4.6791486740112305,
      "learning_rate": 4.922743752585347e-05,
      "loss": 0.8324,
      "step": 101600
    },
    {
      "epoch": 0.9279874443390028,
      "grad_norm": 3.514674186706543,
      "learning_rate": 4.92266771297175e-05,
      "loss": 0.8155,
      "step": 101700
    },
    {
      "epoch": 0.928899919702168,
      "grad_norm": 4.541314125061035,
      "learning_rate": 4.922591673358153e-05,
      "loss": 0.7881,
      "step": 101800
    },
    {
      "epoch": 0.9298123950653332,
      "grad_norm": 4.924057483673096,
      "learning_rate": 4.9225156337445563e-05,
      "loss": 0.8358,
      "step": 101900
    },
    {
      "epoch": 0.9307248704284984,
      "grad_norm": 4.027756690979004,
      "learning_rate": 4.922439594130959e-05,
      "loss": 0.7946,
      "step": 102000
    },
    {
      "epoch": 0.9316373457916636,
      "grad_norm": 5.166923522949219,
      "learning_rate": 4.922363554517362e-05,
      "loss": 0.814,
      "step": 102100
    },
    {
      "epoch": 0.9325498211548289,
      "grad_norm": 4.484504222869873,
      "learning_rate": 4.922287514903765e-05,
      "loss": 0.8452,
      "step": 102200
    },
    {
      "epoch": 0.9334622965179941,
      "grad_norm": 4.255361557006836,
      "learning_rate": 4.922211475290167e-05,
      "loss": 0.8055,
      "step": 102300
    },
    {
      "epoch": 0.9343747718811592,
      "grad_norm": 4.416584491729736,
      "learning_rate": 4.92213543567657e-05,
      "loss": 0.7858,
      "step": 102400
    },
    {
      "epoch": 0.9352872472443244,
      "grad_norm": 3.4309744834899902,
      "learning_rate": 4.922059396062973e-05,
      "loss": 0.8057,
      "step": 102500
    },
    {
      "epoch": 0.9361997226074896,
      "grad_norm": 4.1648688316345215,
      "learning_rate": 4.921983356449376e-05,
      "loss": 0.8088,
      "step": 102600
    },
    {
      "epoch": 0.9371121979706548,
      "grad_norm": 3.176020383834839,
      "learning_rate": 4.921907316835779e-05,
      "loss": 0.8134,
      "step": 102700
    },
    {
      "epoch": 0.93802467333382,
      "grad_norm": 3.8298935890197754,
      "learning_rate": 4.921831277222182e-05,
      "loss": 0.7952,
      "step": 102800
    },
    {
      "epoch": 0.9389371486969852,
      "grad_norm": 3.313047170639038,
      "learning_rate": 4.9217552376085844e-05,
      "loss": 0.8428,
      "step": 102900
    },
    {
      "epoch": 0.9398496240601504,
      "grad_norm": 4.361220359802246,
      "learning_rate": 4.921679197994988e-05,
      "loss": 0.8201,
      "step": 103000
    },
    {
      "epoch": 0.9407620994233156,
      "grad_norm": 4.489006996154785,
      "learning_rate": 4.9216031583813904e-05,
      "loss": 0.8151,
      "step": 103100
    },
    {
      "epoch": 0.9416745747864808,
      "grad_norm": 4.487113952636719,
      "learning_rate": 4.9215271187677934e-05,
      "loss": 0.8135,
      "step": 103200
    },
    {
      "epoch": 0.9425870501496459,
      "grad_norm": 4.632332801818848,
      "learning_rate": 4.9214510791541964e-05,
      "loss": 0.8117,
      "step": 103300
    },
    {
      "epoch": 0.9434995255128111,
      "grad_norm": 4.682618618011475,
      "learning_rate": 4.9213750395405994e-05,
      "loss": 0.8343,
      "step": 103400
    },
    {
      "epoch": 0.9444120008759763,
      "grad_norm": 4.862354755401611,
      "learning_rate": 4.921298999927002e-05,
      "loss": 0.8249,
      "step": 103500
    },
    {
      "epoch": 0.9453244762391415,
      "grad_norm": 4.24705171585083,
      "learning_rate": 4.9212229603134054e-05,
      "loss": 0.7755,
      "step": 103600
    },
    {
      "epoch": 0.9462369516023067,
      "grad_norm": 4.147958755493164,
      "learning_rate": 4.921146920699808e-05,
      "loss": 0.7951,
      "step": 103700
    },
    {
      "epoch": 0.9471494269654719,
      "grad_norm": 3.7441046237945557,
      "learning_rate": 4.921070881086211e-05,
      "loss": 0.7903,
      "step": 103800
    },
    {
      "epoch": 0.9480619023286371,
      "grad_norm": 4.7192864418029785,
      "learning_rate": 4.920994841472614e-05,
      "loss": 0.7823,
      "step": 103900
    },
    {
      "epoch": 0.9489743776918024,
      "grad_norm": 4.778193473815918,
      "learning_rate": 4.920918801859017e-05,
      "loss": 0.8076,
      "step": 104000
    },
    {
      "epoch": 0.9498868530549676,
      "grad_norm": 4.165223121643066,
      "learning_rate": 4.92084276224542e-05,
      "loss": 0.8885,
      "step": 104100
    },
    {
      "epoch": 0.9507993284181327,
      "grad_norm": 4.219440460205078,
      "learning_rate": 4.920766722631823e-05,
      "loss": 0.8268,
      "step": 104200
    },
    {
      "epoch": 0.9517118037812979,
      "grad_norm": 4.693239688873291,
      "learning_rate": 4.920690683018225e-05,
      "loss": 0.7983,
      "step": 104300
    },
    {
      "epoch": 0.9526242791444631,
      "grad_norm": 4.090469837188721,
      "learning_rate": 4.920614643404629e-05,
      "loss": 0.845,
      "step": 104400
    },
    {
      "epoch": 0.9535367545076283,
      "grad_norm": 3.6865670680999756,
      "learning_rate": 4.920538603791031e-05,
      "loss": 0.8082,
      "step": 104500
    },
    {
      "epoch": 0.9544492298707935,
      "grad_norm": 4.364955425262451,
      "learning_rate": 4.920462564177434e-05,
      "loss": 0.8605,
      "step": 104600
    },
    {
      "epoch": 0.9553617052339587,
      "grad_norm": 4.37839937210083,
      "learning_rate": 4.920386524563837e-05,
      "loss": 0.8032,
      "step": 104700
    },
    {
      "epoch": 0.9562741805971239,
      "grad_norm": 3.8760910034179688,
      "learning_rate": 4.92031048495024e-05,
      "loss": 0.7858,
      "step": 104800
    },
    {
      "epoch": 0.9571866559602891,
      "grad_norm": 2.959866762161255,
      "learning_rate": 4.9202344453366425e-05,
      "loss": 0.8073,
      "step": 104900
    },
    {
      "epoch": 0.9580991313234543,
      "grad_norm": 3.8650312423706055,
      "learning_rate": 4.920158405723046e-05,
      "loss": 0.8171,
      "step": 105000
    },
    {
      "epoch": 0.9590116066866194,
      "grad_norm": 4.417269229888916,
      "learning_rate": 4.9200823661094485e-05,
      "loss": 0.8361,
      "step": 105100
    },
    {
      "epoch": 0.9599240820497846,
      "grad_norm": 3.1146323680877686,
      "learning_rate": 4.9200063264958515e-05,
      "loss": 0.7996,
      "step": 105200
    },
    {
      "epoch": 0.9608365574129498,
      "grad_norm": 4.722936153411865,
      "learning_rate": 4.9199302868822545e-05,
      "loss": 0.8309,
      "step": 105300
    },
    {
      "epoch": 0.961749032776115,
      "grad_norm": 4.323696136474609,
      "learning_rate": 4.919854247268657e-05,
      "loss": 0.8008,
      "step": 105400
    },
    {
      "epoch": 0.9626615081392802,
      "grad_norm": 4.128503799438477,
      "learning_rate": 4.9197782076550605e-05,
      "loss": 0.8008,
      "step": 105500
    },
    {
      "epoch": 0.9635739835024454,
      "grad_norm": 5.203769207000732,
      "learning_rate": 4.919702168041463e-05,
      "loss": 0.8212,
      "step": 105600
    },
    {
      "epoch": 0.9644864588656106,
      "grad_norm": 4.827839374542236,
      "learning_rate": 4.919626128427866e-05,
      "loss": 0.8108,
      "step": 105700
    },
    {
      "epoch": 0.9653989342287759,
      "grad_norm": 4.487204074859619,
      "learning_rate": 4.919550088814269e-05,
      "loss": 0.8317,
      "step": 105800
    },
    {
      "epoch": 0.9663114095919411,
      "grad_norm": 4.2202582359313965,
      "learning_rate": 4.919474049200672e-05,
      "loss": 0.7988,
      "step": 105900
    },
    {
      "epoch": 0.9672238849551063,
      "grad_norm": 4.078925609588623,
      "learning_rate": 4.919398009587074e-05,
      "loss": 0.7979,
      "step": 106000
    },
    {
      "epoch": 0.9681363603182714,
      "grad_norm": 3.008775472640991,
      "learning_rate": 4.919321969973478e-05,
      "loss": 0.7896,
      "step": 106100
    },
    {
      "epoch": 0.9690488356814366,
      "grad_norm": 3.8698740005493164,
      "learning_rate": 4.91924593035988e-05,
      "loss": 0.8233,
      "step": 106200
    },
    {
      "epoch": 0.9699613110446018,
      "grad_norm": 4.3360490798950195,
      "learning_rate": 4.919169890746283e-05,
      "loss": 0.7995,
      "step": 106300
    },
    {
      "epoch": 0.970873786407767,
      "grad_norm": 4.175100803375244,
      "learning_rate": 4.919093851132686e-05,
      "loss": 0.8366,
      "step": 106400
    },
    {
      "epoch": 0.9717862617709322,
      "grad_norm": 3.7064337730407715,
      "learning_rate": 4.919017811519089e-05,
      "loss": 0.8526,
      "step": 106500
    },
    {
      "epoch": 0.9726987371340974,
      "grad_norm": 5.039818286895752,
      "learning_rate": 4.918941771905492e-05,
      "loss": 0.8012,
      "step": 106600
    },
    {
      "epoch": 0.9736112124972626,
      "grad_norm": 3.93877911567688,
      "learning_rate": 4.918865732291895e-05,
      "loss": 0.8147,
      "step": 106700
    },
    {
      "epoch": 0.9745236878604278,
      "grad_norm": 4.428806781768799,
      "learning_rate": 4.9187896926782976e-05,
      "loss": 0.8043,
      "step": 106800
    },
    {
      "epoch": 0.975436163223593,
      "grad_norm": 5.771891117095947,
      "learning_rate": 4.918713653064701e-05,
      "loss": 0.8497,
      "step": 106900
    },
    {
      "epoch": 0.9763486385867581,
      "grad_norm": 3.564302682876587,
      "learning_rate": 4.9186376134511036e-05,
      "loss": 0.8131,
      "step": 107000
    },
    {
      "epoch": 0.9772611139499233,
      "grad_norm": 4.601640701293945,
      "learning_rate": 4.9185615738375066e-05,
      "loss": 0.8199,
      "step": 107100
    },
    {
      "epoch": 0.9781735893130885,
      "grad_norm": 4.803262710571289,
      "learning_rate": 4.9184855342239096e-05,
      "loss": 0.8187,
      "step": 107200
    },
    {
      "epoch": 0.9790860646762537,
      "grad_norm": 4.839353561401367,
      "learning_rate": 4.9184094946103126e-05,
      "loss": 0.7914,
      "step": 107300
    },
    {
      "epoch": 0.9799985400394189,
      "grad_norm": 4.337289810180664,
      "learning_rate": 4.918333454996715e-05,
      "loss": 0.8171,
      "step": 107400
    },
    {
      "epoch": 0.9809110154025842,
      "grad_norm": 4.176076412200928,
      "learning_rate": 4.9182574153831186e-05,
      "loss": 0.8023,
      "step": 107500
    },
    {
      "epoch": 0.9818234907657494,
      "grad_norm": 4.641838073730469,
      "learning_rate": 4.918181375769521e-05,
      "loss": 0.8375,
      "step": 107600
    },
    {
      "epoch": 0.9827359661289146,
      "grad_norm": 4.659456729888916,
      "learning_rate": 4.918105336155924e-05,
      "loss": 0.7842,
      "step": 107700
    },
    {
      "epoch": 0.9836484414920798,
      "grad_norm": 4.786757469177246,
      "learning_rate": 4.918029296542327e-05,
      "loss": 0.8324,
      "step": 107800
    },
    {
      "epoch": 0.9845609168552449,
      "grad_norm": 4.644505977630615,
      "learning_rate": 4.91795325692873e-05,
      "loss": 0.7867,
      "step": 107900
    },
    {
      "epoch": 0.9854733922184101,
      "grad_norm": 5.226531505584717,
      "learning_rate": 4.917877217315133e-05,
      "loss": 0.7756,
      "step": 108000
    },
    {
      "epoch": 0.9863858675815753,
      "grad_norm": 3.1079235076904297,
      "learning_rate": 4.917801177701535e-05,
      "loss": 0.8007,
      "step": 108100
    },
    {
      "epoch": 0.9872983429447405,
      "grad_norm": 2.6500422954559326,
      "learning_rate": 4.917725138087938e-05,
      "loss": 0.7924,
      "step": 108200
    },
    {
      "epoch": 0.9882108183079057,
      "grad_norm": 5.246085166931152,
      "learning_rate": 4.917649098474341e-05,
      "loss": 0.7787,
      "step": 108300
    },
    {
      "epoch": 0.9891232936710709,
      "grad_norm": 4.563582420349121,
      "learning_rate": 4.917573058860744e-05,
      "loss": 0.8008,
      "step": 108400
    },
    {
      "epoch": 0.9900357690342361,
      "grad_norm": 4.103268623352051,
      "learning_rate": 4.917497019247147e-05,
      "loss": 0.8256,
      "step": 108500
    },
    {
      "epoch": 0.9909482443974013,
      "grad_norm": 4.9415602684021,
      "learning_rate": 4.9174209796335503e-05,
      "loss": 0.8421,
      "step": 108600
    },
    {
      "epoch": 0.9918607197605664,
      "grad_norm": 4.5205206871032715,
      "learning_rate": 4.917344940019953e-05,
      "loss": 0.7772,
      "step": 108700
    },
    {
      "epoch": 0.9927731951237316,
      "grad_norm": 4.096977233886719,
      "learning_rate": 4.917268900406356e-05,
      "loss": 0.8122,
      "step": 108800
    },
    {
      "epoch": 0.9936856704868968,
      "grad_norm": 3.992753267288208,
      "learning_rate": 4.917192860792759e-05,
      "loss": 0.8184,
      "step": 108900
    },
    {
      "epoch": 0.994598145850062,
      "grad_norm": 3.8886682987213135,
      "learning_rate": 4.917116821179162e-05,
      "loss": 0.8177,
      "step": 109000
    },
    {
      "epoch": 0.9955106212132272,
      "grad_norm": 2.9450290203094482,
      "learning_rate": 4.917040781565565e-05,
      "loss": 0.7994,
      "step": 109100
    },
    {
      "epoch": 0.9964230965763924,
      "grad_norm": 3.8530452251434326,
      "learning_rate": 4.916964741951968e-05,
      "loss": 0.8257,
      "step": 109200
    },
    {
      "epoch": 0.9973355719395577,
      "grad_norm": 5.633406639099121,
      "learning_rate": 4.91688870233837e-05,
      "loss": 0.7926,
      "step": 109300
    },
    {
      "epoch": 0.9982480473027229,
      "grad_norm": 5.5081658363342285,
      "learning_rate": 4.916812662724774e-05,
      "loss": 0.7843,
      "step": 109400
    },
    {
      "epoch": 0.9991605226658881,
      "grad_norm": 5.067723751068115,
      "learning_rate": 4.916736623111176e-05,
      "loss": 0.8268,
      "step": 109500
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6533700227737427,
      "eval_runtime": 25.3234,
      "eval_samples_per_second": 227.813,
      "eval_steps_per_second": 227.813,
      "step": 109592
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6431974768638611,
      "eval_runtime": 481.7802,
      "eval_samples_per_second": 227.473,
      "eval_steps_per_second": 227.473,
      "step": 109592
    },
    {
      "epoch": 1.0000729980290533,
      "grad_norm": 3.3752288818359375,
      "learning_rate": 4.916660583497579e-05,
      "loss": 0.8276,
      "step": 109600
    },
    {
      "epoch": 1.0009854733922183,
      "grad_norm": 4.573904037475586,
      "learning_rate": 4.916584543883982e-05,
      "loss": 0.7644,
      "step": 109700
    },
    {
      "epoch": 1.0018979487553836,
      "grad_norm": 4.393709659576416,
      "learning_rate": 4.916508504270385e-05,
      "loss": 0.8026,
      "step": 109800
    },
    {
      "epoch": 1.0028104241185487,
      "grad_norm": 4.539994716644287,
      "learning_rate": 4.916432464656788e-05,
      "loss": 0.777,
      "step": 109900
    },
    {
      "epoch": 1.003722899481714,
      "grad_norm": 4.065593719482422,
      "learning_rate": 4.916356425043191e-05,
      "loss": 0.7965,
      "step": 110000
    },
    {
      "epoch": 1.004635374844879,
      "grad_norm": 3.902754068374634,
      "learning_rate": 4.9162803854295934e-05,
      "loss": 0.7879,
      "step": 110100
    },
    {
      "epoch": 1.0055478502080444,
      "grad_norm": 4.583499908447266,
      "learning_rate": 4.916204345815997e-05,
      "loss": 0.7661,
      "step": 110200
    },
    {
      "epoch": 1.0064603255712097,
      "grad_norm": 4.875250816345215,
      "learning_rate": 4.9161283062023994e-05,
      "loss": 0.7871,
      "step": 110300
    },
    {
      "epoch": 1.0073728009343748,
      "grad_norm": 4.905856132507324,
      "learning_rate": 4.9160522665888024e-05,
      "loss": 0.816,
      "step": 110400
    },
    {
      "epoch": 1.00828527629754,
      "grad_norm": 4.831696033477783,
      "learning_rate": 4.9159762269752054e-05,
      "loss": 0.7778,
      "step": 110500
    },
    {
      "epoch": 1.0091977516607051,
      "grad_norm": 4.861575126647949,
      "learning_rate": 4.9159001873616084e-05,
      "loss": 0.7943,
      "step": 110600
    },
    {
      "epoch": 1.0101102270238704,
      "grad_norm": 4.486010551452637,
      "learning_rate": 4.915824147748011e-05,
      "loss": 0.8155,
      "step": 110700
    },
    {
      "epoch": 1.0110227023870355,
      "grad_norm": 4.439225673675537,
      "learning_rate": 4.915748108134414e-05,
      "loss": 0.7651,
      "step": 110800
    },
    {
      "epoch": 1.0119351777502008,
      "grad_norm": 4.106764316558838,
      "learning_rate": 4.915672068520817e-05,
      "loss": 0.7872,
      "step": 110900
    },
    {
      "epoch": 1.0128476531133659,
      "grad_norm": 3.7464308738708496,
      "learning_rate": 4.91559602890722e-05,
      "loss": 0.7734,
      "step": 111000
    },
    {
      "epoch": 1.0137601284765312,
      "grad_norm": 4.506194591522217,
      "learning_rate": 4.915519989293623e-05,
      "loss": 0.8338,
      "step": 111100
    },
    {
      "epoch": 1.0146726038396963,
      "grad_norm": 4.723031520843506,
      "learning_rate": 4.915443949680025e-05,
      "loss": 0.8062,
      "step": 111200
    },
    {
      "epoch": 1.0155850792028616,
      "grad_norm": 5.498967170715332,
      "learning_rate": 4.915367910066429e-05,
      "loss": 0.7791,
      "step": 111300
    },
    {
      "epoch": 1.0164975545660266,
      "grad_norm": 4.513493061065674,
      "learning_rate": 4.915291870452831e-05,
      "loss": 0.8043,
      "step": 111400
    },
    {
      "epoch": 1.017410029929192,
      "grad_norm": 4.211932182312012,
      "learning_rate": 4.915215830839234e-05,
      "loss": 0.8187,
      "step": 111500
    },
    {
      "epoch": 1.018322505292357,
      "grad_norm": 4.933218955993652,
      "learning_rate": 4.915139791225637e-05,
      "loss": 0.8309,
      "step": 111600
    },
    {
      "epoch": 1.0192349806555223,
      "grad_norm": 4.607452392578125,
      "learning_rate": 4.91506375161204e-05,
      "loss": 0.8081,
      "step": 111700
    },
    {
      "epoch": 1.0201474560186874,
      "grad_norm": 5.287213325500488,
      "learning_rate": 4.9149877119984425e-05,
      "loss": 0.7873,
      "step": 111800
    },
    {
      "epoch": 1.0210599313818527,
      "grad_norm": 4.173538684844971,
      "learning_rate": 4.914911672384846e-05,
      "loss": 0.8101,
      "step": 111900
    },
    {
      "epoch": 1.021972406745018,
      "grad_norm": 4.346044540405273,
      "learning_rate": 4.9148356327712485e-05,
      "loss": 0.8172,
      "step": 112000
    },
    {
      "epoch": 1.022884882108183,
      "grad_norm": 3.765488862991333,
      "learning_rate": 4.9147595931576515e-05,
      "loss": 0.7984,
      "step": 112100
    },
    {
      "epoch": 1.0237973574713484,
      "grad_norm": 5.822718143463135,
      "learning_rate": 4.9146835535440545e-05,
      "loss": 0.8322,
      "step": 112200
    },
    {
      "epoch": 1.0247098328345134,
      "grad_norm": 5.25265645980835,
      "learning_rate": 4.9146075139304575e-05,
      "loss": 0.7854,
      "step": 112300
    },
    {
      "epoch": 1.0256223081976787,
      "grad_norm": 3.5450713634490967,
      "learning_rate": 4.9145314743168605e-05,
      "loss": 0.795,
      "step": 112400
    },
    {
      "epoch": 1.0265347835608438,
      "grad_norm": 4.148288249969482,
      "learning_rate": 4.9144554347032635e-05,
      "loss": 0.7477,
      "step": 112500
    },
    {
      "epoch": 1.0274472589240091,
      "grad_norm": 4.38461446762085,
      "learning_rate": 4.914379395089666e-05,
      "loss": 0.781,
      "step": 112600
    },
    {
      "epoch": 1.0283597342871742,
      "grad_norm": 4.803600311279297,
      "learning_rate": 4.9143033554760696e-05,
      "loss": 0.8539,
      "step": 112700
    },
    {
      "epoch": 1.0292722096503395,
      "grad_norm": 3.933455467224121,
      "learning_rate": 4.914227315862472e-05,
      "loss": 0.7881,
      "step": 112800
    },
    {
      "epoch": 1.0301846850135046,
      "grad_norm": 3.8818459510803223,
      "learning_rate": 4.914151276248875e-05,
      "loss": 0.7846,
      "step": 112900
    },
    {
      "epoch": 1.0310971603766699,
      "grad_norm": 4.212377071380615,
      "learning_rate": 4.914075236635278e-05,
      "loss": 0.7694,
      "step": 113000
    },
    {
      "epoch": 1.032009635739835,
      "grad_norm": 4.317943096160889,
      "learning_rate": 4.913999197021681e-05,
      "loss": 0.8406,
      "step": 113100
    },
    {
      "epoch": 1.0329221111030003,
      "grad_norm": 4.412878513336182,
      "learning_rate": 4.913923157408083e-05,
      "loss": 0.8409,
      "step": 113200
    },
    {
      "epoch": 1.0338345864661653,
      "grad_norm": 4.818607330322266,
      "learning_rate": 4.913847117794487e-05,
      "loss": 0.853,
      "step": 113300
    },
    {
      "epoch": 1.0347470618293306,
      "grad_norm": 4.954933166503906,
      "learning_rate": 4.913771078180889e-05,
      "loss": 0.8001,
      "step": 113400
    },
    {
      "epoch": 1.0356595371924957,
      "grad_norm": 5.195662975311279,
      "learning_rate": 4.913695038567292e-05,
      "loss": 0.7784,
      "step": 113500
    },
    {
      "epoch": 1.036572012555661,
      "grad_norm": 4.18207311630249,
      "learning_rate": 4.913618998953695e-05,
      "loss": 0.7849,
      "step": 113600
    },
    {
      "epoch": 1.0374844879188263,
      "grad_norm": 4.666465759277344,
      "learning_rate": 4.9135429593400976e-05,
      "loss": 0.8135,
      "step": 113700
    },
    {
      "epoch": 1.0383969632819914,
      "grad_norm": 4.098796367645264,
      "learning_rate": 4.913466919726501e-05,
      "loss": 0.8183,
      "step": 113800
    },
    {
      "epoch": 1.0393094386451567,
      "grad_norm": 4.186913967132568,
      "learning_rate": 4.9133908801129036e-05,
      "loss": 0.782,
      "step": 113900
    },
    {
      "epoch": 1.0402219140083218,
      "grad_norm": 4.588973045349121,
      "learning_rate": 4.9133148404993066e-05,
      "loss": 0.8425,
      "step": 114000
    },
    {
      "epoch": 1.041134389371487,
      "grad_norm": 4.933404445648193,
      "learning_rate": 4.9132388008857096e-05,
      "loss": 0.7973,
      "step": 114100
    },
    {
      "epoch": 1.0420468647346521,
      "grad_norm": 3.7498772144317627,
      "learning_rate": 4.9131627612721126e-05,
      "loss": 0.7761,
      "step": 114200
    },
    {
      "epoch": 1.0429593400978174,
      "grad_norm": 4.050127029418945,
      "learning_rate": 4.913086721658515e-05,
      "loss": 0.7258,
      "step": 114300
    },
    {
      "epoch": 1.0438718154609825,
      "grad_norm": 4.6534552574157715,
      "learning_rate": 4.9130106820449186e-05,
      "loss": 0.8266,
      "step": 114400
    },
    {
      "epoch": 1.0447842908241478,
      "grad_norm": 5.260849952697754,
      "learning_rate": 4.912934642431321e-05,
      "loss": 0.8221,
      "step": 114500
    },
    {
      "epoch": 1.0456967661873129,
      "grad_norm": 4.420212745666504,
      "learning_rate": 4.912858602817724e-05,
      "loss": 0.7479,
      "step": 114600
    },
    {
      "epoch": 1.0466092415504782,
      "grad_norm": 4.101333141326904,
      "learning_rate": 4.912782563204127e-05,
      "loss": 0.8113,
      "step": 114700
    },
    {
      "epoch": 1.0475217169136433,
      "grad_norm": 4.340179920196533,
      "learning_rate": 4.91270652359053e-05,
      "loss": 0.776,
      "step": 114800
    },
    {
      "epoch": 1.0484341922768086,
      "grad_norm": 4.2434587478637695,
      "learning_rate": 4.912630483976933e-05,
      "loss": 0.7886,
      "step": 114900
    },
    {
      "epoch": 1.0493466676399736,
      "grad_norm": 3.444891929626465,
      "learning_rate": 4.912554444363336e-05,
      "loss": 0.8048,
      "step": 115000
    },
    {
      "epoch": 1.050259143003139,
      "grad_norm": 4.244734287261963,
      "learning_rate": 4.912478404749738e-05,
      "loss": 0.7903,
      "step": 115100
    },
    {
      "epoch": 1.051171618366304,
      "grad_norm": 4.210761070251465,
      "learning_rate": 4.912402365136142e-05,
      "loss": 0.8495,
      "step": 115200
    },
    {
      "epoch": 1.0520840937294693,
      "grad_norm": 4.45902681350708,
      "learning_rate": 4.9123263255225443e-05,
      "loss": 0.8119,
      "step": 115300
    },
    {
      "epoch": 1.0529965690926344,
      "grad_norm": 5.275806427001953,
      "learning_rate": 4.9122502859089473e-05,
      "loss": 0.7545,
      "step": 115400
    },
    {
      "epoch": 1.0539090444557997,
      "grad_norm": 4.009638786315918,
      "learning_rate": 4.9121742462953504e-05,
      "loss": 0.783,
      "step": 115500
    },
    {
      "epoch": 1.054821519818965,
      "grad_norm": 3.3782436847686768,
      "learning_rate": 4.9120982066817534e-05,
      "loss": 0.7741,
      "step": 115600
    },
    {
      "epoch": 1.05573399518213,
      "grad_norm": 4.753432273864746,
      "learning_rate": 4.912022167068156e-05,
      "loss": 0.8076,
      "step": 115700
    },
    {
      "epoch": 1.0566464705452954,
      "grad_norm": 3.6141810417175293,
      "learning_rate": 4.9119461274545594e-05,
      "loss": 0.7369,
      "step": 115800
    },
    {
      "epoch": 1.0575589459084604,
      "grad_norm": 3.934904098510742,
      "learning_rate": 4.911870087840962e-05,
      "loss": 0.8079,
      "step": 115900
    },
    {
      "epoch": 1.0584714212716257,
      "grad_norm": 3.95021390914917,
      "learning_rate": 4.911794048227365e-05,
      "loss": 0.8343,
      "step": 116000
    },
    {
      "epoch": 1.0593838966347908,
      "grad_norm": 4.89616060256958,
      "learning_rate": 4.911718008613768e-05,
      "loss": 0.833,
      "step": 116100
    },
    {
      "epoch": 1.0602963719979561,
      "grad_norm": 4.486929893493652,
      "learning_rate": 4.911641969000171e-05,
      "loss": 0.77,
      "step": 116200
    },
    {
      "epoch": 1.0612088473611212,
      "grad_norm": 5.036891460418701,
      "learning_rate": 4.911565929386574e-05,
      "loss": 0.7815,
      "step": 116300
    },
    {
      "epoch": 1.0621213227242865,
      "grad_norm": 3.845184803009033,
      "learning_rate": 4.911489889772977e-05,
      "loss": 0.7853,
      "step": 116400
    },
    {
      "epoch": 1.0630337980874516,
      "grad_norm": 4.6966776847839355,
      "learning_rate": 4.911413850159379e-05,
      "loss": 0.7781,
      "step": 116500
    },
    {
      "epoch": 1.0639462734506169,
      "grad_norm": 3.831122875213623,
      "learning_rate": 4.911337810545782e-05,
      "loss": 0.824,
      "step": 116600
    },
    {
      "epoch": 1.064858748813782,
      "grad_norm": 3.8223884105682373,
      "learning_rate": 4.911261770932185e-05,
      "loss": 0.8162,
      "step": 116700
    },
    {
      "epoch": 1.0657712241769473,
      "grad_norm": 4.622544288635254,
      "learning_rate": 4.9111857313185874e-05,
      "loss": 0.8173,
      "step": 116800
    },
    {
      "epoch": 1.0666836995401123,
      "grad_norm": 4.838508129119873,
      "learning_rate": 4.911109691704991e-05,
      "loss": 0.7869,
      "step": 116900
    },
    {
      "epoch": 1.0675961749032776,
      "grad_norm": 4.892322063446045,
      "learning_rate": 4.9110336520913934e-05,
      "loss": 0.8474,
      "step": 117000
    },
    {
      "epoch": 1.0685086502664427,
      "grad_norm": 3.5391972064971924,
      "learning_rate": 4.9109576124777964e-05,
      "loss": 0.7759,
      "step": 117100
    },
    {
      "epoch": 1.069421125629608,
      "grad_norm": 5.427028656005859,
      "learning_rate": 4.9108815728641994e-05,
      "loss": 0.795,
      "step": 117200
    },
    {
      "epoch": 1.070333600992773,
      "grad_norm": 3.720475912094116,
      "learning_rate": 4.9108055332506024e-05,
      "loss": 0.8409,
      "step": 117300
    },
    {
      "epoch": 1.0712460763559384,
      "grad_norm": 3.646385431289673,
      "learning_rate": 4.9107294936370054e-05,
      "loss": 0.8067,
      "step": 117400
    },
    {
      "epoch": 1.0721585517191037,
      "grad_norm": 4.564530849456787,
      "learning_rate": 4.9106534540234085e-05,
      "loss": 0.8189,
      "step": 117500
    },
    {
      "epoch": 1.0730710270822688,
      "grad_norm": 5.594854831695557,
      "learning_rate": 4.910577414409811e-05,
      "loss": 0.7686,
      "step": 117600
    },
    {
      "epoch": 1.073983502445434,
      "grad_norm": 3.7473347187042236,
      "learning_rate": 4.9105013747962145e-05,
      "loss": 0.8388,
      "step": 117700
    },
    {
      "epoch": 1.0748959778085991,
      "grad_norm": 5.606793403625488,
      "learning_rate": 4.910425335182617e-05,
      "loss": 0.783,
      "step": 117800
    },
    {
      "epoch": 1.0758084531717644,
      "grad_norm": 3.3371598720550537,
      "learning_rate": 4.91034929556902e-05,
      "loss": 0.7874,
      "step": 117900
    },
    {
      "epoch": 1.0767209285349295,
      "grad_norm": 3.5993638038635254,
      "learning_rate": 4.910273255955423e-05,
      "loss": 0.8176,
      "step": 118000
    },
    {
      "epoch": 1.0776334038980948,
      "grad_norm": 4.08966064453125,
      "learning_rate": 4.910197216341826e-05,
      "loss": 0.7887,
      "step": 118100
    },
    {
      "epoch": 1.0785458792612599,
      "grad_norm": 4.008368968963623,
      "learning_rate": 4.910121176728228e-05,
      "loss": 0.7904,
      "step": 118200
    },
    {
      "epoch": 1.0794583546244252,
      "grad_norm": 3.863945245742798,
      "learning_rate": 4.910045137114632e-05,
      "loss": 0.7992,
      "step": 118300
    },
    {
      "epoch": 1.0803708299875903,
      "grad_norm": 2.972166061401367,
      "learning_rate": 4.909969097501034e-05,
      "loss": 0.7798,
      "step": 118400
    },
    {
      "epoch": 1.0812833053507556,
      "grad_norm": 3.6085376739501953,
      "learning_rate": 4.909893057887437e-05,
      "loss": 0.8146,
      "step": 118500
    },
    {
      "epoch": 1.0821957807139206,
      "grad_norm": 4.765132904052734,
      "learning_rate": 4.90981701827384e-05,
      "loss": 0.7761,
      "step": 118600
    },
    {
      "epoch": 1.083108256077086,
      "grad_norm": 4.8861083984375,
      "learning_rate": 4.909740978660243e-05,
      "loss": 0.8178,
      "step": 118700
    },
    {
      "epoch": 1.084020731440251,
      "grad_norm": 5.075683116912842,
      "learning_rate": 4.909664939046646e-05,
      "loss": 0.7982,
      "step": 118800
    },
    {
      "epoch": 1.0849332068034163,
      "grad_norm": 3.566804885864258,
      "learning_rate": 4.909588899433049e-05,
      "loss": 0.7784,
      "step": 118900
    },
    {
      "epoch": 1.0858456821665814,
      "grad_norm": 3.8119428157806396,
      "learning_rate": 4.9095128598194515e-05,
      "loss": 0.7459,
      "step": 119000
    },
    {
      "epoch": 1.0867581575297467,
      "grad_norm": 5.101635932922363,
      "learning_rate": 4.909436820205855e-05,
      "loss": 0.781,
      "step": 119100
    },
    {
      "epoch": 1.087670632892912,
      "grad_norm": 1.7849262952804565,
      "learning_rate": 4.9093607805922575e-05,
      "loss": 0.7996,
      "step": 119200
    },
    {
      "epoch": 1.088583108256077,
      "grad_norm": 4.357029914855957,
      "learning_rate": 4.90928474097866e-05,
      "loss": 0.8226,
      "step": 119300
    },
    {
      "epoch": 1.0894955836192424,
      "grad_norm": 4.654523849487305,
      "learning_rate": 4.9092087013650635e-05,
      "loss": 0.755,
      "step": 119400
    },
    {
      "epoch": 1.0904080589824074,
      "grad_norm": 4.093306064605713,
      "learning_rate": 4.909132661751466e-05,
      "loss": 0.8417,
      "step": 119500
    },
    {
      "epoch": 1.0913205343455727,
      "grad_norm": 4.145227909088135,
      "learning_rate": 4.909056622137869e-05,
      "loss": 0.7958,
      "step": 119600
    },
    {
      "epoch": 1.0922330097087378,
      "grad_norm": 5.277621269226074,
      "learning_rate": 4.908980582524272e-05,
      "loss": 0.8088,
      "step": 119700
    },
    {
      "epoch": 1.0931454850719031,
      "grad_norm": 3.7081093788146973,
      "learning_rate": 4.908904542910675e-05,
      "loss": 0.742,
      "step": 119800
    },
    {
      "epoch": 1.0940579604350682,
      "grad_norm": 4.9051513671875,
      "learning_rate": 4.908828503297078e-05,
      "loss": 0.7588,
      "step": 119900
    },
    {
      "epoch": 1.0949704357982335,
      "grad_norm": 4.496469974517822,
      "learning_rate": 4.908752463683481e-05,
      "loss": 0.7826,
      "step": 120000
    },
    {
      "epoch": 1.0958829111613986,
      "grad_norm": 3.879674196243286,
      "learning_rate": 4.908676424069883e-05,
      "loss": 0.7665,
      "step": 120100
    },
    {
      "epoch": 1.0967953865245639,
      "grad_norm": 4.845717906951904,
      "learning_rate": 4.908600384456287e-05,
      "loss": 0.7792,
      "step": 120200
    },
    {
      "epoch": 1.097707861887729,
      "grad_norm": 4.501538276672363,
      "learning_rate": 4.908524344842689e-05,
      "loss": 0.8112,
      "step": 120300
    },
    {
      "epoch": 1.0986203372508943,
      "grad_norm": 4.933967590332031,
      "learning_rate": 4.908448305229092e-05,
      "loss": 0.8177,
      "step": 120400
    },
    {
      "epoch": 1.0995328126140593,
      "grad_norm": 4.96112585067749,
      "learning_rate": 4.908372265615495e-05,
      "loss": 0.8078,
      "step": 120500
    },
    {
      "epoch": 1.1004452879772246,
      "grad_norm": 3.906010627746582,
      "learning_rate": 4.908296226001898e-05,
      "loss": 0.7916,
      "step": 120600
    },
    {
      "epoch": 1.1013577633403897,
      "grad_norm": 3.952792167663574,
      "learning_rate": 4.908220186388301e-05,
      "loss": 0.7708,
      "step": 120700
    },
    {
      "epoch": 1.102270238703555,
      "grad_norm": 4.399158954620361,
      "learning_rate": 4.908144146774704e-05,
      "loss": 0.7374,
      "step": 120800
    },
    {
      "epoch": 1.1031827140667203,
      "grad_norm": 3.7380807399749756,
      "learning_rate": 4.9080681071611066e-05,
      "loss": 0.7766,
      "step": 120900
    },
    {
      "epoch": 1.1040951894298854,
      "grad_norm": 4.34990119934082,
      "learning_rate": 4.9079920675475096e-05,
      "loss": 0.8483,
      "step": 121000
    },
    {
      "epoch": 1.1050076647930507,
      "grad_norm": 4.517168045043945,
      "learning_rate": 4.9079160279339126e-05,
      "loss": 0.7963,
      "step": 121100
    },
    {
      "epoch": 1.1059201401562158,
      "grad_norm": 4.760839939117432,
      "learning_rate": 4.9078399883203156e-05,
      "loss": 0.788,
      "step": 121200
    },
    {
      "epoch": 1.106832615519381,
      "grad_norm": 3.4984021186828613,
      "learning_rate": 4.9077639487067186e-05,
      "loss": 0.7695,
      "step": 121300
    },
    {
      "epoch": 1.1077450908825461,
      "grad_norm": 4.350109577178955,
      "learning_rate": 4.9076879090931217e-05,
      "loss": 0.8257,
      "step": 121400
    },
    {
      "epoch": 1.1086575662457114,
      "grad_norm": 4.788187026977539,
      "learning_rate": 4.907611869479524e-05,
      "loss": 0.7527,
      "step": 121500
    },
    {
      "epoch": 1.1095700416088765,
      "grad_norm": 3.674368143081665,
      "learning_rate": 4.907535829865928e-05,
      "loss": 0.8368,
      "step": 121600
    },
    {
      "epoch": 1.1104825169720418,
      "grad_norm": 3.4096717834472656,
      "learning_rate": 4.90745979025233e-05,
      "loss": 0.7947,
      "step": 121700
    },
    {
      "epoch": 1.1113949923352069,
      "grad_norm": 4.622817039489746,
      "learning_rate": 4.907383750638733e-05,
      "loss": 0.7838,
      "step": 121800
    },
    {
      "epoch": 1.1123074676983722,
      "grad_norm": 3.5514094829559326,
      "learning_rate": 4.907307711025136e-05,
      "loss": 0.8057,
      "step": 121900
    },
    {
      "epoch": 1.1132199430615373,
      "grad_norm": 4.791032791137695,
      "learning_rate": 4.907231671411539e-05,
      "loss": 0.782,
      "step": 122000
    },
    {
      "epoch": 1.1141324184247026,
      "grad_norm": 5.052743434906006,
      "learning_rate": 4.907155631797942e-05,
      "loss": 0.832,
      "step": 122100
    },
    {
      "epoch": 1.1150448937878676,
      "grad_norm": 4.7340497970581055,
      "learning_rate": 4.9070795921843443e-05,
      "loss": 0.8038,
      "step": 122200
    },
    {
      "epoch": 1.115957369151033,
      "grad_norm": 3.7251336574554443,
      "learning_rate": 4.9070035525707474e-05,
      "loss": 0.8261,
      "step": 122300
    },
    {
      "epoch": 1.116869844514198,
      "grad_norm": 4.387024402618408,
      "learning_rate": 4.9069275129571504e-05,
      "loss": 0.7972,
      "step": 122400
    },
    {
      "epoch": 1.1177823198773633,
      "grad_norm": 4.51922082901001,
      "learning_rate": 4.9068514733435534e-05,
      "loss": 0.7941,
      "step": 122500
    },
    {
      "epoch": 1.1186947952405286,
      "grad_norm": 3.9721462726593018,
      "learning_rate": 4.906775433729956e-05,
      "loss": 0.7981,
      "step": 122600
    },
    {
      "epoch": 1.1196072706036937,
      "grad_norm": 4.522608280181885,
      "learning_rate": 4.9066993941163594e-05,
      "loss": 0.8127,
      "step": 122700
    },
    {
      "epoch": 1.120519745966859,
      "grad_norm": 4.6119184494018555,
      "learning_rate": 4.906623354502762e-05,
      "loss": 0.7609,
      "step": 122800
    },
    {
      "epoch": 1.121432221330024,
      "grad_norm": 4.629507541656494,
      "learning_rate": 4.906547314889165e-05,
      "loss": 0.7557,
      "step": 122900
    },
    {
      "epoch": 1.1223446966931894,
      "grad_norm": 4.536468982696533,
      "learning_rate": 4.906471275275568e-05,
      "loss": 0.8298,
      "step": 123000
    },
    {
      "epoch": 1.1232571720563544,
      "grad_norm": 3.8183059692382812,
      "learning_rate": 4.906395235661971e-05,
      "loss": 0.7888,
      "step": 123100
    },
    {
      "epoch": 1.1241696474195197,
      "grad_norm": 3.8321969509124756,
      "learning_rate": 4.906319196048374e-05,
      "loss": 0.7857,
      "step": 123200
    },
    {
      "epoch": 1.1250821227826848,
      "grad_norm": 3.684757947921753,
      "learning_rate": 4.906243156434777e-05,
      "loss": 0.7788,
      "step": 123300
    },
    {
      "epoch": 1.1259945981458501,
      "grad_norm": 5.222234725952148,
      "learning_rate": 4.906167116821179e-05,
      "loss": 0.83,
      "step": 123400
    },
    {
      "epoch": 1.1269070735090152,
      "grad_norm": 3.998148202896118,
      "learning_rate": 4.906091077207583e-05,
      "loss": 0.8013,
      "step": 123500
    },
    {
      "epoch": 1.1278195488721805,
      "grad_norm": 3.6052727699279785,
      "learning_rate": 4.906015037593985e-05,
      "loss": 0.7867,
      "step": 123600
    },
    {
      "epoch": 1.1287320242353456,
      "grad_norm": 4.133542060852051,
      "learning_rate": 4.905938997980388e-05,
      "loss": 0.82,
      "step": 123700
    },
    {
      "epoch": 1.1296444995985109,
      "grad_norm": 4.177315711975098,
      "learning_rate": 4.905862958366791e-05,
      "loss": 0.7899,
      "step": 123800
    },
    {
      "epoch": 1.130556974961676,
      "grad_norm": 5.0245256423950195,
      "learning_rate": 4.905786918753194e-05,
      "loss": 0.8046,
      "step": 123900
    },
    {
      "epoch": 1.1314694503248413,
      "grad_norm": 4.277549743652344,
      "learning_rate": 4.9057108791395964e-05,
      "loss": 0.7721,
      "step": 124000
    },
    {
      "epoch": 1.1323819256880063,
      "grad_norm": 4.718342304229736,
      "learning_rate": 4.905634839526e-05,
      "loss": 0.7673,
      "step": 124100
    },
    {
      "epoch": 1.1332944010511716,
      "grad_norm": 4.183311939239502,
      "learning_rate": 4.9055587999124025e-05,
      "loss": 0.7746,
      "step": 124200
    },
    {
      "epoch": 1.134206876414337,
      "grad_norm": 4.287418842315674,
      "learning_rate": 4.9054827602988055e-05,
      "loss": 0.7748,
      "step": 124300
    },
    {
      "epoch": 1.135119351777502,
      "grad_norm": 4.8193159103393555,
      "learning_rate": 4.9054067206852085e-05,
      "loss": 0.7905,
      "step": 124400
    },
    {
      "epoch": 1.136031827140667,
      "grad_norm": 4.032307147979736,
      "learning_rate": 4.9053306810716115e-05,
      "loss": 0.7968,
      "step": 124500
    },
    {
      "epoch": 1.1369443025038324,
      "grad_norm": 3.656615734100342,
      "learning_rate": 4.9052546414580145e-05,
      "loss": 0.8195,
      "step": 124600
    },
    {
      "epoch": 1.1378567778669977,
      "grad_norm": 3.861787796020508,
      "learning_rate": 4.9051786018444175e-05,
      "loss": 0.7837,
      "step": 124700
    },
    {
      "epoch": 1.1387692532301628,
      "grad_norm": 4.429110050201416,
      "learning_rate": 4.90510256223082e-05,
      "loss": 0.8017,
      "step": 124800
    },
    {
      "epoch": 1.139681728593328,
      "grad_norm": 3.1833155155181885,
      "learning_rate": 4.9050265226172235e-05,
      "loss": 0.7585,
      "step": 124900
    },
    {
      "epoch": 1.1405942039564931,
      "grad_norm": 4.430704593658447,
      "learning_rate": 4.904950483003626e-05,
      "loss": 0.7742,
      "step": 125000
    },
    {
      "epoch": 1.1415066793196584,
      "grad_norm": 3.726113796234131,
      "learning_rate": 4.904874443390028e-05,
      "loss": 0.815,
      "step": 125100
    },
    {
      "epoch": 1.1424191546828235,
      "grad_norm": 4.6316118240356445,
      "learning_rate": 4.904798403776432e-05,
      "loss": 0.7694,
      "step": 125200
    },
    {
      "epoch": 1.1433316300459888,
      "grad_norm": 5.689116477966309,
      "learning_rate": 4.904722364162834e-05,
      "loss": 0.7506,
      "step": 125300
    },
    {
      "epoch": 1.1442441054091539,
      "grad_norm": 3.9890387058258057,
      "learning_rate": 4.904646324549237e-05,
      "loss": 0.8017,
      "step": 125400
    },
    {
      "epoch": 1.1451565807723192,
      "grad_norm": 3.862666606903076,
      "learning_rate": 4.90457028493564e-05,
      "loss": 0.7899,
      "step": 125500
    },
    {
      "epoch": 1.1460690561354843,
      "grad_norm": 3.9947988986968994,
      "learning_rate": 4.904494245322043e-05,
      "loss": 0.8398,
      "step": 125600
    },
    {
      "epoch": 1.1469815314986496,
      "grad_norm": 4.296103000640869,
      "learning_rate": 4.904418205708446e-05,
      "loss": 0.7887,
      "step": 125700
    },
    {
      "epoch": 1.1478940068618146,
      "grad_norm": 4.229961395263672,
      "learning_rate": 4.904342166094849e-05,
      "loss": 0.822,
      "step": 125800
    },
    {
      "epoch": 1.14880648222498,
      "grad_norm": 4.91038703918457,
      "learning_rate": 4.9042661264812515e-05,
      "loss": 0.7971,
      "step": 125900
    },
    {
      "epoch": 1.1497189575881452,
      "grad_norm": 4.837188720703125,
      "learning_rate": 4.904190086867655e-05,
      "loss": 0.7984,
      "step": 126000
    },
    {
      "epoch": 1.1506314329513103,
      "grad_norm": 5.502158164978027,
      "learning_rate": 4.9041140472540575e-05,
      "loss": 0.8093,
      "step": 126100
    },
    {
      "epoch": 1.1515439083144754,
      "grad_norm": 4.5554327964782715,
      "learning_rate": 4.9040380076404606e-05,
      "loss": 0.7945,
      "step": 126200
    },
    {
      "epoch": 1.1524563836776407,
      "grad_norm": 4.025181770324707,
      "learning_rate": 4.9039619680268636e-05,
      "loss": 0.8133,
      "step": 126300
    },
    {
      "epoch": 1.153368859040806,
      "grad_norm": 4.376886367797852,
      "learning_rate": 4.9038859284132666e-05,
      "loss": 0.7632,
      "step": 126400
    },
    {
      "epoch": 1.154281334403971,
      "grad_norm": 5.151947498321533,
      "learning_rate": 4.903809888799669e-05,
      "loss": 0.8333,
      "step": 126500
    },
    {
      "epoch": 1.1551938097671364,
      "grad_norm": 4.149950981140137,
      "learning_rate": 4.9037338491860726e-05,
      "loss": 0.7803,
      "step": 126600
    },
    {
      "epoch": 1.1561062851303014,
      "grad_norm": 5.079124927520752,
      "learning_rate": 4.903657809572475e-05,
      "loss": 0.8257,
      "step": 126700
    },
    {
      "epoch": 1.1570187604934667,
      "grad_norm": 4.141164302825928,
      "learning_rate": 4.903581769958878e-05,
      "loss": 0.7815,
      "step": 126800
    },
    {
      "epoch": 1.1579312358566318,
      "grad_norm": 4.183653354644775,
      "learning_rate": 4.903505730345281e-05,
      "loss": 0.7998,
      "step": 126900
    },
    {
      "epoch": 1.1588437112197971,
      "grad_norm": 4.959744453430176,
      "learning_rate": 4.903429690731684e-05,
      "loss": 0.7933,
      "step": 127000
    },
    {
      "epoch": 1.1597561865829622,
      "grad_norm": 3.9417033195495605,
      "learning_rate": 4.903353651118087e-05,
      "loss": 0.7701,
      "step": 127100
    },
    {
      "epoch": 1.1606686619461275,
      "grad_norm": 4.796133995056152,
      "learning_rate": 4.90327761150449e-05,
      "loss": 0.7415,
      "step": 127200
    },
    {
      "epoch": 1.1615811373092926,
      "grad_norm": 4.6867194175720215,
      "learning_rate": 4.903201571890892e-05,
      "loss": 0.7858,
      "step": 127300
    },
    {
      "epoch": 1.1624936126724579,
      "grad_norm": 3.466444730758667,
      "learning_rate": 4.903125532277296e-05,
      "loss": 0.7507,
      "step": 127400
    },
    {
      "epoch": 1.163406088035623,
      "grad_norm": 3.8486368656158447,
      "learning_rate": 4.903049492663698e-05,
      "loss": 0.7735,
      "step": 127500
    },
    {
      "epoch": 1.1643185633987883,
      "grad_norm": 4.202253341674805,
      "learning_rate": 4.902973453050101e-05,
      "loss": 0.7852,
      "step": 127600
    },
    {
      "epoch": 1.1652310387619533,
      "grad_norm": 3.271348714828491,
      "learning_rate": 4.902897413436504e-05,
      "loss": 0.7857,
      "step": 127700
    },
    {
      "epoch": 1.1661435141251186,
      "grad_norm": 4.328294277191162,
      "learning_rate": 4.902821373822907e-05,
      "loss": 0.8,
      "step": 127800
    },
    {
      "epoch": 1.1670559894882837,
      "grad_norm": 3.6418874263763428,
      "learning_rate": 4.9027453342093096e-05,
      "loss": 0.8244,
      "step": 127900
    },
    {
      "epoch": 1.167968464851449,
      "grad_norm": 4.612922191619873,
      "learning_rate": 4.9026692945957126e-05,
      "loss": 0.8325,
      "step": 128000
    },
    {
      "epoch": 1.1688809402146143,
      "grad_norm": 3.9979751110076904,
      "learning_rate": 4.9025932549821156e-05,
      "loss": 0.822,
      "step": 128100
    },
    {
      "epoch": 1.1697934155777794,
      "grad_norm": 4.652318000793457,
      "learning_rate": 4.9025172153685187e-05,
      "loss": 0.7757,
      "step": 128200
    },
    {
      "epoch": 1.1707058909409447,
      "grad_norm": 4.619757175445557,
      "learning_rate": 4.9024411757549217e-05,
      "loss": 0.865,
      "step": 128300
    },
    {
      "epoch": 1.1716183663041098,
      "grad_norm": 4.321486473083496,
      "learning_rate": 4.902365136141324e-05,
      "loss": 0.7939,
      "step": 128400
    },
    {
      "epoch": 1.172530841667275,
      "grad_norm": 5.548765182495117,
      "learning_rate": 4.902289096527728e-05,
      "loss": 0.8181,
      "step": 128500
    },
    {
      "epoch": 1.1734433170304401,
      "grad_norm": 5.2199506759643555,
      "learning_rate": 4.90221305691413e-05,
      "loss": 0.7891,
      "step": 128600
    },
    {
      "epoch": 1.1743557923936054,
      "grad_norm": 3.838097333908081,
      "learning_rate": 4.902137017300533e-05,
      "loss": 0.8235,
      "step": 128700
    },
    {
      "epoch": 1.1752682677567705,
      "grad_norm": 4.56069278717041,
      "learning_rate": 4.902060977686936e-05,
      "loss": 0.768,
      "step": 128800
    },
    {
      "epoch": 1.1761807431199358,
      "grad_norm": 4.450868129730225,
      "learning_rate": 4.901984938073339e-05,
      "loss": 0.7829,
      "step": 128900
    },
    {
      "epoch": 1.177093218483101,
      "grad_norm": 3.9899325370788574,
      "learning_rate": 4.9019088984597414e-05,
      "loss": 0.8058,
      "step": 129000
    },
    {
      "epoch": 1.1780056938462662,
      "grad_norm": 4.1102471351623535,
      "learning_rate": 4.901832858846145e-05,
      "loss": 0.8227,
      "step": 129100
    },
    {
      "epoch": 1.1789181692094313,
      "grad_norm": 4.526034832000732,
      "learning_rate": 4.9017568192325474e-05,
      "loss": 0.792,
      "step": 129200
    },
    {
      "epoch": 1.1798306445725966,
      "grad_norm": 4.357357025146484,
      "learning_rate": 4.9016807796189504e-05,
      "loss": 0.7817,
      "step": 129300
    },
    {
      "epoch": 1.1807431199357616,
      "grad_norm": 3.244356393814087,
      "learning_rate": 4.9016047400053534e-05,
      "loss": 0.781,
      "step": 129400
    },
    {
      "epoch": 1.181655595298927,
      "grad_norm": 3.071680784225464,
      "learning_rate": 4.9015287003917564e-05,
      "loss": 0.7742,
      "step": 129500
    },
    {
      "epoch": 1.182568070662092,
      "grad_norm": 4.1737751960754395,
      "learning_rate": 4.9014526607781594e-05,
      "loss": 0.7785,
      "step": 129600
    },
    {
      "epoch": 1.1834805460252573,
      "grad_norm": 4.111976623535156,
      "learning_rate": 4.9013766211645624e-05,
      "loss": 0.8227,
      "step": 129700
    },
    {
      "epoch": 1.1843930213884226,
      "grad_norm": 3.708075761795044,
      "learning_rate": 4.901300581550965e-05,
      "loss": 0.8038,
      "step": 129800
    },
    {
      "epoch": 1.1853054967515877,
      "grad_norm": 4.765157222747803,
      "learning_rate": 4.9012245419373684e-05,
      "loss": 0.7905,
      "step": 129900
    },
    {
      "epoch": 1.186217972114753,
      "grad_norm": 4.698421001434326,
      "learning_rate": 4.901148502323771e-05,
      "loss": 0.7969,
      "step": 130000
    },
    {
      "epoch": 1.187130447477918,
      "grad_norm": 4.171337127685547,
      "learning_rate": 4.901072462710174e-05,
      "loss": 0.8185,
      "step": 130100
    },
    {
      "epoch": 1.1880429228410834,
      "grad_norm": 5.004813194274902,
      "learning_rate": 4.900996423096577e-05,
      "loss": 0.8359,
      "step": 130200
    },
    {
      "epoch": 1.1889553982042484,
      "grad_norm": 4.7881879806518555,
      "learning_rate": 4.90092038348298e-05,
      "loss": 0.8135,
      "step": 130300
    },
    {
      "epoch": 1.1898678735674137,
      "grad_norm": 4.796361923217773,
      "learning_rate": 4.900844343869382e-05,
      "loss": 0.7601,
      "step": 130400
    },
    {
      "epoch": 1.1907803489305788,
      "grad_norm": 3.900773525238037,
      "learning_rate": 4.900768304255786e-05,
      "loss": 0.7911,
      "step": 130500
    },
    {
      "epoch": 1.1916928242937441,
      "grad_norm": 3.8507940769195557,
      "learning_rate": 4.900692264642188e-05,
      "loss": 0.8015,
      "step": 130600
    },
    {
      "epoch": 1.1926052996569092,
      "grad_norm": 4.4717817306518555,
      "learning_rate": 4.900616225028591e-05,
      "loss": 0.7782,
      "step": 130700
    },
    {
      "epoch": 1.1935177750200745,
      "grad_norm": 4.236399173736572,
      "learning_rate": 4.900540185414994e-05,
      "loss": 0.806,
      "step": 130800
    },
    {
      "epoch": 1.1944302503832396,
      "grad_norm": 3.8699498176574707,
      "learning_rate": 4.9004641458013964e-05,
      "loss": 0.7875,
      "step": 130900
    },
    {
      "epoch": 1.1953427257464049,
      "grad_norm": 3.9290072917938232,
      "learning_rate": 4.9003881061878e-05,
      "loss": 0.7943,
      "step": 131000
    },
    {
      "epoch": 1.19625520110957,
      "grad_norm": 4.282906532287598,
      "learning_rate": 4.9003120665742025e-05,
      "loss": 0.7618,
      "step": 131100
    },
    {
      "epoch": 1.1971676764727353,
      "grad_norm": 4.1331562995910645,
      "learning_rate": 4.9002360269606055e-05,
      "loss": 0.8065,
      "step": 131200
    },
    {
      "epoch": 1.1980801518359003,
      "grad_norm": 5.333472728729248,
      "learning_rate": 4.9001599873470085e-05,
      "loss": 0.7728,
      "step": 131300
    },
    {
      "epoch": 1.1989926271990656,
      "grad_norm": 3.9395740032196045,
      "learning_rate": 4.9000839477334115e-05,
      "loss": 0.775,
      "step": 131400
    },
    {
      "epoch": 1.199905102562231,
      "grad_norm": 4.183406829833984,
      "learning_rate": 4.900007908119814e-05,
      "loss": 0.7316,
      "step": 131500
    },
    {
      "epoch": 1.200817577925396,
      "grad_norm": 3.611016035079956,
      "learning_rate": 4.8999318685062175e-05,
      "loss": 0.8544,
      "step": 131600
    },
    {
      "epoch": 1.201730053288561,
      "grad_norm": 5.228697299957275,
      "learning_rate": 4.89985582889262e-05,
      "loss": 0.792,
      "step": 131700
    },
    {
      "epoch": 1.2026425286517264,
      "grad_norm": 5.085751056671143,
      "learning_rate": 4.899779789279023e-05,
      "loss": 0.818,
      "step": 131800
    },
    {
      "epoch": 1.2035550040148917,
      "grad_norm": 3.989851474761963,
      "learning_rate": 4.899703749665426e-05,
      "loss": 0.7714,
      "step": 131900
    },
    {
      "epoch": 1.2044674793780568,
      "grad_norm": 3.862800121307373,
      "learning_rate": 4.899627710051829e-05,
      "loss": 0.8045,
      "step": 132000
    },
    {
      "epoch": 1.205379954741222,
      "grad_norm": 4.69980525970459,
      "learning_rate": 4.899551670438232e-05,
      "loss": 0.8121,
      "step": 132100
    },
    {
      "epoch": 1.2062924301043871,
      "grad_norm": 4.4799675941467285,
      "learning_rate": 4.899475630824635e-05,
      "loss": 0.8279,
      "step": 132200
    },
    {
      "epoch": 1.2072049054675524,
      "grad_norm": 4.051191329956055,
      "learning_rate": 4.899399591211037e-05,
      "loss": 0.7779,
      "step": 132300
    },
    {
      "epoch": 1.2081173808307175,
      "grad_norm": 3.700695514678955,
      "learning_rate": 4.899323551597441e-05,
      "loss": 0.7999,
      "step": 132400
    },
    {
      "epoch": 1.2090298561938828,
      "grad_norm": 3.4484968185424805,
      "learning_rate": 4.899247511983843e-05,
      "loss": 0.7904,
      "step": 132500
    },
    {
      "epoch": 1.209942331557048,
      "grad_norm": 4.720682621002197,
      "learning_rate": 4.899171472370246e-05,
      "loss": 0.8353,
      "step": 132600
    },
    {
      "epoch": 1.2108548069202132,
      "grad_norm": 4.557180404663086,
      "learning_rate": 4.899095432756649e-05,
      "loss": 0.7882,
      "step": 132700
    },
    {
      "epoch": 1.2117672822833783,
      "grad_norm": 4.622791290283203,
      "learning_rate": 4.899019393143052e-05,
      "loss": 0.7872,
      "step": 132800
    },
    {
      "epoch": 1.2126797576465436,
      "grad_norm": 5.250521183013916,
      "learning_rate": 4.8989433535294545e-05,
      "loss": 0.7604,
      "step": 132900
    },
    {
      "epoch": 1.2135922330097086,
      "grad_norm": 3.3103299140930176,
      "learning_rate": 4.898867313915858e-05,
      "loss": 0.782,
      "step": 133000
    },
    {
      "epoch": 1.214504708372874,
      "grad_norm": 4.298035621643066,
      "learning_rate": 4.8987912743022606e-05,
      "loss": 0.8169,
      "step": 133100
    },
    {
      "epoch": 1.2154171837360392,
      "grad_norm": 3.9822540283203125,
      "learning_rate": 4.8987152346886636e-05,
      "loss": 0.7851,
      "step": 133200
    },
    {
      "epoch": 1.2163296590992043,
      "grad_norm": 4.0763840675354,
      "learning_rate": 4.8986391950750666e-05,
      "loss": 0.7768,
      "step": 133300
    },
    {
      "epoch": 1.2172421344623694,
      "grad_norm": 4.148220062255859,
      "learning_rate": 4.8985631554614696e-05,
      "loss": 0.793,
      "step": 133400
    },
    {
      "epoch": 1.2181546098255347,
      "grad_norm": 2.732801914215088,
      "learning_rate": 4.8984871158478726e-05,
      "loss": 0.7997,
      "step": 133500
    },
    {
      "epoch": 1.2190670851887,
      "grad_norm": 4.0984673500061035,
      "learning_rate": 4.898411076234275e-05,
      "loss": 0.8432,
      "step": 133600
    },
    {
      "epoch": 1.219979560551865,
      "grad_norm": 3.4141268730163574,
      "learning_rate": 4.898335036620678e-05,
      "loss": 0.7957,
      "step": 133700
    },
    {
      "epoch": 1.2208920359150304,
      "grad_norm": 3.153594732284546,
      "learning_rate": 4.898258997007081e-05,
      "loss": 0.7743,
      "step": 133800
    },
    {
      "epoch": 1.2218045112781954,
      "grad_norm": 4.973379135131836,
      "learning_rate": 4.898182957393484e-05,
      "loss": 0.807,
      "step": 133900
    },
    {
      "epoch": 1.2227169866413607,
      "grad_norm": 4.554415702819824,
      "learning_rate": 4.898106917779887e-05,
      "loss": 0.7613,
      "step": 134000
    },
    {
      "epoch": 1.2236294620045258,
      "grad_norm": 3.0750722885131836,
      "learning_rate": 4.89803087816629e-05,
      "loss": 0.8074,
      "step": 134100
    },
    {
      "epoch": 1.2245419373676911,
      "grad_norm": 4.248276710510254,
      "learning_rate": 4.897954838552692e-05,
      "loss": 0.7903,
      "step": 134200
    },
    {
      "epoch": 1.2254544127308562,
      "grad_norm": 4.416791915893555,
      "learning_rate": 4.897878798939095e-05,
      "loss": 0.7787,
      "step": 134300
    },
    {
      "epoch": 1.2263668880940215,
      "grad_norm": 4.238081932067871,
      "learning_rate": 4.897802759325498e-05,
      "loss": 0.768,
      "step": 134400
    },
    {
      "epoch": 1.2272793634571866,
      "grad_norm": 5.495770454406738,
      "learning_rate": 4.897726719711901e-05,
      "loss": 0.7946,
      "step": 134500
    },
    {
      "epoch": 1.2281918388203519,
      "grad_norm": 4.4890947341918945,
      "learning_rate": 4.897650680098304e-05,
      "loss": 0.7535,
      "step": 134600
    },
    {
      "epoch": 1.229104314183517,
      "grad_norm": 4.264195919036865,
      "learning_rate": 4.897574640484707e-05,
      "loss": 0.8185,
      "step": 134700
    },
    {
      "epoch": 1.2300167895466823,
      "grad_norm": 3.9441158771514893,
      "learning_rate": 4.8974986008711096e-05,
      "loss": 0.7665,
      "step": 134800
    },
    {
      "epoch": 1.2309292649098476,
      "grad_norm": 4.03292179107666,
      "learning_rate": 4.897422561257513e-05,
      "loss": 0.786,
      "step": 134900
    },
    {
      "epoch": 1.2318417402730126,
      "grad_norm": 4.161328315734863,
      "learning_rate": 4.8973465216439157e-05,
      "loss": 0.8004,
      "step": 135000
    },
    {
      "epoch": 1.2327542156361777,
      "grad_norm": 6.198822975158691,
      "learning_rate": 4.897270482030319e-05,
      "loss": 0.8113,
      "step": 135100
    },
    {
      "epoch": 1.233666690999343,
      "grad_norm": 4.468789577484131,
      "learning_rate": 4.897194442416722e-05,
      "loss": 0.7765,
      "step": 135200
    },
    {
      "epoch": 1.2345791663625083,
      "grad_norm": 3.9352245330810547,
      "learning_rate": 4.897118402803125e-05,
      "loss": 0.8249,
      "step": 135300
    },
    {
      "epoch": 1.2354916417256734,
      "grad_norm": 4.334795951843262,
      "learning_rate": 4.897042363189528e-05,
      "loss": 0.783,
      "step": 135400
    },
    {
      "epoch": 1.2364041170888387,
      "grad_norm": 3.6239588260650635,
      "learning_rate": 4.896966323575931e-05,
      "loss": 0.7909,
      "step": 135500
    },
    {
      "epoch": 1.2373165924520038,
      "grad_norm": 4.727142333984375,
      "learning_rate": 4.896890283962333e-05,
      "loss": 0.8034,
      "step": 135600
    },
    {
      "epoch": 1.238229067815169,
      "grad_norm": 4.485831260681152,
      "learning_rate": 4.896814244348737e-05,
      "loss": 0.8325,
      "step": 135700
    },
    {
      "epoch": 1.2391415431783341,
      "grad_norm": 3.620913028717041,
      "learning_rate": 4.896738204735139e-05,
      "loss": 0.8101,
      "step": 135800
    },
    {
      "epoch": 1.2400540185414994,
      "grad_norm": 3.8323628902435303,
      "learning_rate": 4.896662165121542e-05,
      "loss": 0.7961,
      "step": 135900
    },
    {
      "epoch": 1.2409664939046645,
      "grad_norm": 4.220150470733643,
      "learning_rate": 4.896586125507945e-05,
      "loss": 0.8385,
      "step": 136000
    },
    {
      "epoch": 1.2418789692678298,
      "grad_norm": 4.050631999969482,
      "learning_rate": 4.896510085894348e-05,
      "loss": 0.8403,
      "step": 136100
    },
    {
      "epoch": 1.242791444630995,
      "grad_norm": 4.12668514251709,
      "learning_rate": 4.8964340462807504e-05,
      "loss": 0.7968,
      "step": 136200
    },
    {
      "epoch": 1.2437039199941602,
      "grad_norm": 4.1275506019592285,
      "learning_rate": 4.896358006667154e-05,
      "loss": 0.7602,
      "step": 136300
    },
    {
      "epoch": 1.2446163953573253,
      "grad_norm": 4.36083984375,
      "learning_rate": 4.8962819670535564e-05,
      "loss": 0.7894,
      "step": 136400
    },
    {
      "epoch": 1.2455288707204906,
      "grad_norm": 3.6618363857269287,
      "learning_rate": 4.8962059274399594e-05,
      "loss": 0.8237,
      "step": 136500
    },
    {
      "epoch": 1.2464413460836559,
      "grad_norm": 5.574678897857666,
      "learning_rate": 4.8961298878263624e-05,
      "loss": 0.8002,
      "step": 136600
    },
    {
      "epoch": 1.247353821446821,
      "grad_norm": 4.178873538970947,
      "learning_rate": 4.896053848212765e-05,
      "loss": 0.7768,
      "step": 136700
    },
    {
      "epoch": 1.248266296809986,
      "grad_norm": 3.8718137741088867,
      "learning_rate": 4.8959778085991684e-05,
      "loss": 0.7942,
      "step": 136800
    },
    {
      "epoch": 1.2491787721731513,
      "grad_norm": 4.367191314697266,
      "learning_rate": 4.895901768985571e-05,
      "loss": 0.8244,
      "step": 136900
    },
    {
      "epoch": 1.2500912475363166,
      "grad_norm": 4.728979110717773,
      "learning_rate": 4.895825729371974e-05,
      "loss": 0.7388,
      "step": 137000
    },
    {
      "epoch": 1.2510037228994817,
      "grad_norm": 3.648041009902954,
      "learning_rate": 4.895749689758377e-05,
      "loss": 0.7674,
      "step": 137100
    },
    {
      "epoch": 1.2519161982626468,
      "grad_norm": 4.688760757446289,
      "learning_rate": 4.89567365014478e-05,
      "loss": 0.7796,
      "step": 137200
    },
    {
      "epoch": 1.252828673625812,
      "grad_norm": 4.709608554840088,
      "learning_rate": 4.895597610531182e-05,
      "loss": 0.7545,
      "step": 137300
    },
    {
      "epoch": 1.2537411489889774,
      "grad_norm": 3.142721176147461,
      "learning_rate": 4.895521570917586e-05,
      "loss": 0.7833,
      "step": 137400
    },
    {
      "epoch": 1.2546536243521424,
      "grad_norm": 4.079244613647461,
      "learning_rate": 4.895445531303988e-05,
      "loss": 0.8132,
      "step": 137500
    },
    {
      "epoch": 1.2555660997153077,
      "grad_norm": 4.428260326385498,
      "learning_rate": 4.895369491690391e-05,
      "loss": 0.7906,
      "step": 137600
    },
    {
      "epoch": 1.2564785750784728,
      "grad_norm": 4.964916706085205,
      "learning_rate": 4.895293452076794e-05,
      "loss": 0.8031,
      "step": 137700
    },
    {
      "epoch": 1.2573910504416381,
      "grad_norm": 5.13106107711792,
      "learning_rate": 4.895217412463197e-05,
      "loss": 0.8039,
      "step": 137800
    },
    {
      "epoch": 1.2583035258048032,
      "grad_norm": 4.721569061279297,
      "learning_rate": 4.8951413728496e-05,
      "loss": 0.7703,
      "step": 137900
    },
    {
      "epoch": 1.2592160011679685,
      "grad_norm": 4.308599472045898,
      "learning_rate": 4.895065333236003e-05,
      "loss": 0.8265,
      "step": 138000
    },
    {
      "epoch": 1.2601284765311336,
      "grad_norm": 4.790018081665039,
      "learning_rate": 4.8949892936224055e-05,
      "loss": 0.832,
      "step": 138100
    },
    {
      "epoch": 1.2610409518942989,
      "grad_norm": 4.244195461273193,
      "learning_rate": 4.894913254008809e-05,
      "loss": 0.7815,
      "step": 138200
    },
    {
      "epoch": 1.2619534272574642,
      "grad_norm": 4.311744689941406,
      "learning_rate": 4.8948372143952115e-05,
      "loss": 0.8365,
      "step": 138300
    },
    {
      "epoch": 1.2628659026206293,
      "grad_norm": 3.212057113647461,
      "learning_rate": 4.8947611747816145e-05,
      "loss": 0.769,
      "step": 138400
    },
    {
      "epoch": 1.2637783779837943,
      "grad_norm": 3.1609456539154053,
      "learning_rate": 4.8946851351680175e-05,
      "loss": 0.7898,
      "step": 138500
    },
    {
      "epoch": 1.2646908533469596,
      "grad_norm": 3.5881502628326416,
      "learning_rate": 4.8946090955544205e-05,
      "loss": 0.7634,
      "step": 138600
    },
    {
      "epoch": 1.265603328710125,
      "grad_norm": 3.796043872833252,
      "learning_rate": 4.894533055940823e-05,
      "loss": 0.8001,
      "step": 138700
    },
    {
      "epoch": 1.26651580407329,
      "grad_norm": 4.550314426422119,
      "learning_rate": 4.8944570163272265e-05,
      "loss": 0.8171,
      "step": 138800
    },
    {
      "epoch": 1.267428279436455,
      "grad_norm": 4.574288368225098,
      "learning_rate": 4.894380976713629e-05,
      "loss": 0.797,
      "step": 138900
    },
    {
      "epoch": 1.2683407547996204,
      "grad_norm": 4.044459342956543,
      "learning_rate": 4.894304937100032e-05,
      "loss": 0.783,
      "step": 139000
    },
    {
      "epoch": 1.2692532301627857,
      "grad_norm": 4.361219882965088,
      "learning_rate": 4.894228897486435e-05,
      "loss": 0.774,
      "step": 139100
    },
    {
      "epoch": 1.2701657055259508,
      "grad_norm": 4.954270839691162,
      "learning_rate": 4.894152857872837e-05,
      "loss": 0.7879,
      "step": 139200
    },
    {
      "epoch": 1.271078180889116,
      "grad_norm": 4.230947494506836,
      "learning_rate": 4.894076818259241e-05,
      "loss": 0.7883,
      "step": 139300
    },
    {
      "epoch": 1.2719906562522811,
      "grad_norm": 4.323333263397217,
      "learning_rate": 4.894000778645643e-05,
      "loss": 0.8117,
      "step": 139400
    },
    {
      "epoch": 1.2729031316154464,
      "grad_norm": 3.870297431945801,
      "learning_rate": 4.893924739032046e-05,
      "loss": 0.8124,
      "step": 139500
    },
    {
      "epoch": 1.2738156069786115,
      "grad_norm": 5.07396125793457,
      "learning_rate": 4.893848699418449e-05,
      "loss": 0.8272,
      "step": 139600
    },
    {
      "epoch": 1.2747280823417768,
      "grad_norm": 4.779957294464111,
      "learning_rate": 4.893772659804852e-05,
      "loss": 0.766,
      "step": 139700
    },
    {
      "epoch": 1.275640557704942,
      "grad_norm": 4.1078619956970215,
      "learning_rate": 4.8936966201912546e-05,
      "loss": 0.8258,
      "step": 139800
    },
    {
      "epoch": 1.2765530330681072,
      "grad_norm": 4.030230522155762,
      "learning_rate": 4.893620580577658e-05,
      "loss": 0.7557,
      "step": 139900
    },
    {
      "epoch": 1.2774655084312725,
      "grad_norm": 4.678022861480713,
      "learning_rate": 4.8935445409640606e-05,
      "loss": 0.756,
      "step": 140000
    },
    {
      "epoch": 1.2783779837944376,
      "grad_norm": 3.858332872390747,
      "learning_rate": 4.8934685013504636e-05,
      "loss": 0.7719,
      "step": 140100
    },
    {
      "epoch": 1.2792904591576026,
      "grad_norm": 3.842310667037964,
      "learning_rate": 4.8933924617368666e-05,
      "loss": 0.7543,
      "step": 140200
    },
    {
      "epoch": 1.280202934520768,
      "grad_norm": 3.400538921356201,
      "learning_rate": 4.8933164221232696e-05,
      "loss": 0.7962,
      "step": 140300
    },
    {
      "epoch": 1.2811154098839332,
      "grad_norm": 3.838630199432373,
      "learning_rate": 4.8932403825096726e-05,
      "loss": 0.8238,
      "step": 140400
    },
    {
      "epoch": 1.2820278852470983,
      "grad_norm": 4.099328517913818,
      "learning_rate": 4.8931643428960756e-05,
      "loss": 0.7595,
      "step": 140500
    },
    {
      "epoch": 1.2829403606102634,
      "grad_norm": 4.035376071929932,
      "learning_rate": 4.893088303282478e-05,
      "loss": 0.7937,
      "step": 140600
    },
    {
      "epoch": 1.2838528359734287,
      "grad_norm": 4.098573684692383,
      "learning_rate": 4.8930122636688816e-05,
      "loss": 0.7902,
      "step": 140700
    },
    {
      "epoch": 1.284765311336594,
      "grad_norm": 3.973428249359131,
      "learning_rate": 4.892936224055284e-05,
      "loss": 0.7853,
      "step": 140800
    },
    {
      "epoch": 1.285677786699759,
      "grad_norm": 4.348409652709961,
      "learning_rate": 4.892860184441687e-05,
      "loss": 0.7922,
      "step": 140900
    },
    {
      "epoch": 1.2865902620629244,
      "grad_norm": 4.054760932922363,
      "learning_rate": 4.89278414482809e-05,
      "loss": 0.8054,
      "step": 141000
    },
    {
      "epoch": 1.2875027374260894,
      "grad_norm": 3.6458542346954346,
      "learning_rate": 4.892708105214493e-05,
      "loss": 0.8159,
      "step": 141100
    },
    {
      "epoch": 1.2884152127892547,
      "grad_norm": 5.402257919311523,
      "learning_rate": 4.892632065600895e-05,
      "loss": 0.7699,
      "step": 141200
    },
    {
      "epoch": 1.2893276881524198,
      "grad_norm": 4.075182914733887,
      "learning_rate": 4.892556025987299e-05,
      "loss": 0.8071,
      "step": 141300
    },
    {
      "epoch": 1.2902401635155851,
      "grad_norm": 5.298579216003418,
      "learning_rate": 4.892479986373701e-05,
      "loss": 0.8115,
      "step": 141400
    },
    {
      "epoch": 1.2911526388787502,
      "grad_norm": 4.832768440246582,
      "learning_rate": 4.892403946760104e-05,
      "loss": 0.8046,
      "step": 141500
    },
    {
      "epoch": 1.2920651142419155,
      "grad_norm": 3.054744243621826,
      "learning_rate": 4.892327907146507e-05,
      "loss": 0.7655,
      "step": 141600
    },
    {
      "epoch": 1.2929775896050808,
      "grad_norm": 4.8365702629089355,
      "learning_rate": 4.89225186753291e-05,
      "loss": 0.8114,
      "step": 141700
    },
    {
      "epoch": 1.2938900649682459,
      "grad_norm": 4.334744453430176,
      "learning_rate": 4.892175827919313e-05,
      "loss": 0.8048,
      "step": 141800
    },
    {
      "epoch": 1.294802540331411,
      "grad_norm": 4.042202949523926,
      "learning_rate": 4.8920997883057163e-05,
      "loss": 0.8017,
      "step": 141900
    },
    {
      "epoch": 1.2957150156945763,
      "grad_norm": 4.493313789367676,
      "learning_rate": 4.892023748692119e-05,
      "loss": 0.7856,
      "step": 142000
    },
    {
      "epoch": 1.2966274910577416,
      "grad_norm": 4.623991012573242,
      "learning_rate": 4.891947709078522e-05,
      "loss": 0.7768,
      "step": 142100
    },
    {
      "epoch": 1.2975399664209066,
      "grad_norm": 4.137228965759277,
      "learning_rate": 4.891871669464925e-05,
      "loss": 0.8207,
      "step": 142200
    },
    {
      "epoch": 1.2984524417840717,
      "grad_norm": 3.418971300125122,
      "learning_rate": 4.891795629851327e-05,
      "loss": 0.7997,
      "step": 142300
    },
    {
      "epoch": 1.299364917147237,
      "grad_norm": 4.283239364624023,
      "learning_rate": 4.891719590237731e-05,
      "loss": 0.8131,
      "step": 142400
    },
    {
      "epoch": 1.3002773925104023,
      "grad_norm": 4.446286201477051,
      "learning_rate": 4.891643550624133e-05,
      "loss": 0.804,
      "step": 142500
    },
    {
      "epoch": 1.3011898678735674,
      "grad_norm": 3.622269868850708,
      "learning_rate": 4.891567511010536e-05,
      "loss": 0.7595,
      "step": 142600
    },
    {
      "epoch": 1.3021023432367327,
      "grad_norm": 4.7857489585876465,
      "learning_rate": 4.891491471396939e-05,
      "loss": 0.7998,
      "step": 142700
    },
    {
      "epoch": 1.3030148185998978,
      "grad_norm": 3.808826208114624,
      "learning_rate": 4.891415431783342e-05,
      "loss": 0.7426,
      "step": 142800
    },
    {
      "epoch": 1.303927293963063,
      "grad_norm": 3.8877944946289062,
      "learning_rate": 4.891339392169745e-05,
      "loss": 0.8024,
      "step": 142900
    },
    {
      "epoch": 1.3048397693262281,
      "grad_norm": 4.66862678527832,
      "learning_rate": 4.891263352556148e-05,
      "loss": 0.77,
      "step": 143000
    },
    {
      "epoch": 1.3057522446893934,
      "grad_norm": 3.615239381790161,
      "learning_rate": 4.8911873129425504e-05,
      "loss": 0.7787,
      "step": 143100
    },
    {
      "epoch": 1.3066647200525585,
      "grad_norm": 4.019284725189209,
      "learning_rate": 4.891111273328954e-05,
      "loss": 0.8168,
      "step": 143200
    },
    {
      "epoch": 1.3075771954157238,
      "grad_norm": 4.385986328125,
      "learning_rate": 4.8910352337153564e-05,
      "loss": 0.7663,
      "step": 143300
    },
    {
      "epoch": 1.308489670778889,
      "grad_norm": 4.7837958335876465,
      "learning_rate": 4.8909591941017594e-05,
      "loss": 0.8097,
      "step": 143400
    },
    {
      "epoch": 1.3094021461420542,
      "grad_norm": 4.473705291748047,
      "learning_rate": 4.8908831544881624e-05,
      "loss": 0.7714,
      "step": 143500
    },
    {
      "epoch": 1.3103146215052193,
      "grad_norm": 4.654783725738525,
      "learning_rate": 4.8908071148745654e-05,
      "loss": 0.7837,
      "step": 143600
    },
    {
      "epoch": 1.3112270968683846,
      "grad_norm": 5.1085004806518555,
      "learning_rate": 4.890731075260968e-05,
      "loss": 0.7984,
      "step": 143700
    },
    {
      "epoch": 1.3121395722315499,
      "grad_norm": 3.9035191535949707,
      "learning_rate": 4.8906550356473714e-05,
      "loss": 0.7598,
      "step": 143800
    },
    {
      "epoch": 1.313052047594715,
      "grad_norm": 3.9345409870147705,
      "learning_rate": 4.890578996033774e-05,
      "loss": 0.7826,
      "step": 143900
    },
    {
      "epoch": 1.31396452295788,
      "grad_norm": 4.093291282653809,
      "learning_rate": 4.890502956420177e-05,
      "loss": 0.8142,
      "step": 144000
    },
    {
      "epoch": 1.3148769983210453,
      "grad_norm": 5.295246124267578,
      "learning_rate": 4.89042691680658e-05,
      "loss": 0.8143,
      "step": 144100
    },
    {
      "epoch": 1.3157894736842106,
      "grad_norm": 5.234483242034912,
      "learning_rate": 4.890350877192983e-05,
      "loss": 0.77,
      "step": 144200
    },
    {
      "epoch": 1.3167019490473757,
      "grad_norm": 4.131352424621582,
      "learning_rate": 4.890274837579386e-05,
      "loss": 0.7976,
      "step": 144300
    },
    {
      "epoch": 1.317614424410541,
      "grad_norm": 4.498581886291504,
      "learning_rate": 4.890198797965789e-05,
      "loss": 0.7952,
      "step": 144400
    },
    {
      "epoch": 1.318526899773706,
      "grad_norm": 3.5406572818756104,
      "learning_rate": 4.890122758352191e-05,
      "loss": 0.8222,
      "step": 144500
    },
    {
      "epoch": 1.3194393751368714,
      "grad_norm": 4.143039226531982,
      "learning_rate": 4.890046718738595e-05,
      "loss": 0.7829,
      "step": 144600
    },
    {
      "epoch": 1.3203518505000365,
      "grad_norm": 4.106692790985107,
      "learning_rate": 4.889970679124997e-05,
      "loss": 0.8048,
      "step": 144700
    },
    {
      "epoch": 1.3212643258632017,
      "grad_norm": 3.6890063285827637,
      "learning_rate": 4.8898946395114e-05,
      "loss": 0.8476,
      "step": 144800
    },
    {
      "epoch": 1.3221768012263668,
      "grad_norm": 4.507870674133301,
      "learning_rate": 4.889818599897803e-05,
      "loss": 0.7869,
      "step": 144900
    },
    {
      "epoch": 1.3230892765895321,
      "grad_norm": 4.877001762390137,
      "learning_rate": 4.8897425602842055e-05,
      "loss": 0.8106,
      "step": 145000
    },
    {
      "epoch": 1.3240017519526972,
      "grad_norm": 4.73610782623291,
      "learning_rate": 4.8896665206706085e-05,
      "loss": 0.7589,
      "step": 145100
    },
    {
      "epoch": 1.3249142273158625,
      "grad_norm": 4.974009037017822,
      "learning_rate": 4.8895904810570115e-05,
      "loss": 0.7825,
      "step": 145200
    },
    {
      "epoch": 1.3258267026790276,
      "grad_norm": 3.659747838973999,
      "learning_rate": 4.8895144414434145e-05,
      "loss": 0.7766,
      "step": 145300
    },
    {
      "epoch": 1.3267391780421929,
      "grad_norm": 4.908456325531006,
      "learning_rate": 4.8894384018298175e-05,
      "loss": 0.7413,
      "step": 145400
    },
    {
      "epoch": 1.3276516534053582,
      "grad_norm": 3.5936508178710938,
      "learning_rate": 4.8893623622162205e-05,
      "loss": 0.793,
      "step": 145500
    },
    {
      "epoch": 1.3285641287685233,
      "grad_norm": 4.196508884429932,
      "learning_rate": 4.889286322602623e-05,
      "loss": 0.7676,
      "step": 145600
    },
    {
      "epoch": 1.3294766041316883,
      "grad_norm": 3.540283203125,
      "learning_rate": 4.8892102829890265e-05,
      "loss": 0.7979,
      "step": 145700
    },
    {
      "epoch": 1.3303890794948536,
      "grad_norm": 4.737167835235596,
      "learning_rate": 4.889134243375429e-05,
      "loss": 0.7882,
      "step": 145800
    },
    {
      "epoch": 1.331301554858019,
      "grad_norm": 4.0540266036987305,
      "learning_rate": 4.889058203761832e-05,
      "loss": 0.8178,
      "step": 145900
    },
    {
      "epoch": 1.332214030221184,
      "grad_norm": 3.526362180709839,
      "learning_rate": 4.888982164148235e-05,
      "loss": 0.8057,
      "step": 146000
    },
    {
      "epoch": 1.333126505584349,
      "grad_norm": 4.405838489532471,
      "learning_rate": 4.888906124534638e-05,
      "loss": 0.7951,
      "step": 146100
    },
    {
      "epoch": 1.3340389809475144,
      "grad_norm": 4.623712062835693,
      "learning_rate": 4.888830084921041e-05,
      "loss": 0.8019,
      "step": 146200
    },
    {
      "epoch": 1.3349514563106797,
      "grad_norm": 4.3376054763793945,
      "learning_rate": 4.888754045307444e-05,
      "loss": 0.8056,
      "step": 146300
    },
    {
      "epoch": 1.3358639316738448,
      "grad_norm": 4.867824077606201,
      "learning_rate": 4.888678005693846e-05,
      "loss": 0.7989,
      "step": 146400
    },
    {
      "epoch": 1.33677640703701,
      "grad_norm": 3.830087423324585,
      "learning_rate": 4.888601966080249e-05,
      "loss": 0.7812,
      "step": 146500
    },
    {
      "epoch": 1.3376888824001751,
      "grad_norm": 4.509480953216553,
      "learning_rate": 4.888525926466652e-05,
      "loss": 0.7733,
      "step": 146600
    },
    {
      "epoch": 1.3386013577633404,
      "grad_norm": 4.425678730010986,
      "learning_rate": 4.888449886853055e-05,
      "loss": 0.754,
      "step": 146700
    },
    {
      "epoch": 1.3395138331265055,
      "grad_norm": 5.506452560424805,
      "learning_rate": 4.888373847239458e-05,
      "loss": 0.8025,
      "step": 146800
    },
    {
      "epoch": 1.3404263084896708,
      "grad_norm": 4.583828926086426,
      "learning_rate": 4.888297807625861e-05,
      "loss": 0.7626,
      "step": 146900
    },
    {
      "epoch": 1.341338783852836,
      "grad_norm": 4.865421772003174,
      "learning_rate": 4.8882217680122636e-05,
      "loss": 0.7912,
      "step": 147000
    },
    {
      "epoch": 1.3422512592160012,
      "grad_norm": 2.9254355430603027,
      "learning_rate": 4.888145728398667e-05,
      "loss": 0.7314,
      "step": 147100
    },
    {
      "epoch": 1.3431637345791665,
      "grad_norm": 4.2121381759643555,
      "learning_rate": 4.8880696887850696e-05,
      "loss": 0.8154,
      "step": 147200
    },
    {
      "epoch": 1.3440762099423316,
      "grad_norm": 4.254022598266602,
      "learning_rate": 4.8879936491714726e-05,
      "loss": 0.7752,
      "step": 147300
    },
    {
      "epoch": 1.3449886853054966,
      "grad_norm": 3.8981430530548096,
      "learning_rate": 4.8879176095578756e-05,
      "loss": 0.83,
      "step": 147400
    },
    {
      "epoch": 1.345901160668662,
      "grad_norm": 5.167783260345459,
      "learning_rate": 4.8878415699442786e-05,
      "loss": 0.8061,
      "step": 147500
    },
    {
      "epoch": 1.3468136360318272,
      "grad_norm": 4.576004505157471,
      "learning_rate": 4.8877655303306816e-05,
      "loss": 0.7997,
      "step": 147600
    },
    {
      "epoch": 1.3477261113949923,
      "grad_norm": 4.439978122711182,
      "learning_rate": 4.887689490717084e-05,
      "loss": 0.7435,
      "step": 147700
    },
    {
      "epoch": 1.3486385867581574,
      "grad_norm": 4.773475646972656,
      "learning_rate": 4.887613451103487e-05,
      "loss": 0.7839,
      "step": 147800
    },
    {
      "epoch": 1.3495510621213227,
      "grad_norm": 4.369948387145996,
      "learning_rate": 4.88753741148989e-05,
      "loss": 0.8176,
      "step": 147900
    },
    {
      "epoch": 1.350463537484488,
      "grad_norm": 4.249583721160889,
      "learning_rate": 4.887461371876293e-05,
      "loss": 0.7846,
      "step": 148000
    },
    {
      "epoch": 1.351376012847653,
      "grad_norm": 4.735424518585205,
      "learning_rate": 4.887385332262695e-05,
      "loss": 0.8224,
      "step": 148100
    },
    {
      "epoch": 1.3522884882108184,
      "grad_norm": 2.6306302547454834,
      "learning_rate": 4.887309292649099e-05,
      "loss": 0.7876,
      "step": 148200
    },
    {
      "epoch": 1.3532009635739835,
      "grad_norm": 3.5169994831085205,
      "learning_rate": 4.887233253035501e-05,
      "loss": 0.7853,
      "step": 148300
    },
    {
      "epoch": 1.3541134389371488,
      "grad_norm": 4.641441345214844,
      "learning_rate": 4.887157213421904e-05,
      "loss": 0.8038,
      "step": 148400
    },
    {
      "epoch": 1.3550259143003138,
      "grad_norm": 4.913157939910889,
      "learning_rate": 4.887081173808307e-05,
      "loss": 0.7703,
      "step": 148500
    },
    {
      "epoch": 1.3559383896634791,
      "grad_norm": 3.7633514404296875,
      "learning_rate": 4.8870051341947103e-05,
      "loss": 0.7589,
      "step": 148600
    },
    {
      "epoch": 1.3568508650266442,
      "grad_norm": 4.556457042694092,
      "learning_rate": 4.8869290945811133e-05,
      "loss": 0.7779,
      "step": 148700
    },
    {
      "epoch": 1.3577633403898095,
      "grad_norm": 4.883115768432617,
      "learning_rate": 4.8868530549675164e-05,
      "loss": 0.7369,
      "step": 148800
    },
    {
      "epoch": 1.3586758157529748,
      "grad_norm": 4.363866806030273,
      "learning_rate": 4.886777015353919e-05,
      "loss": 0.7737,
      "step": 148900
    },
    {
      "epoch": 1.3595882911161399,
      "grad_norm": 5.213743686676025,
      "learning_rate": 4.8867009757403224e-05,
      "loss": 0.809,
      "step": 149000
    },
    {
      "epoch": 1.360500766479305,
      "grad_norm": 5.235286235809326,
      "learning_rate": 4.886624936126725e-05,
      "loss": 0.789,
      "step": 149100
    },
    {
      "epoch": 1.3614132418424703,
      "grad_norm": 4.2480621337890625,
      "learning_rate": 4.886548896513128e-05,
      "loss": 0.7496,
      "step": 149200
    },
    {
      "epoch": 1.3623257172056356,
      "grad_norm": 4.016820430755615,
      "learning_rate": 4.886472856899531e-05,
      "loss": 0.8104,
      "step": 149300
    },
    {
      "epoch": 1.3632381925688006,
      "grad_norm": 4.11636209487915,
      "learning_rate": 4.886396817285934e-05,
      "loss": 0.7792,
      "step": 149400
    },
    {
      "epoch": 1.3641506679319657,
      "grad_norm": 5.130214691162109,
      "learning_rate": 4.886320777672336e-05,
      "loss": 0.7951,
      "step": 149500
    },
    {
      "epoch": 1.365063143295131,
      "grad_norm": 4.154774188995361,
      "learning_rate": 4.88624473805874e-05,
      "loss": 0.795,
      "step": 149600
    },
    {
      "epoch": 1.3659756186582963,
      "grad_norm": 4.398967742919922,
      "learning_rate": 4.886168698445142e-05,
      "loss": 0.798,
      "step": 149700
    },
    {
      "epoch": 1.3668880940214614,
      "grad_norm": 3.728015184402466,
      "learning_rate": 4.886092658831545e-05,
      "loss": 0.8417,
      "step": 149800
    },
    {
      "epoch": 1.3678005693846267,
      "grad_norm": 4.421133995056152,
      "learning_rate": 4.886016619217948e-05,
      "loss": 0.7978,
      "step": 149900
    },
    {
      "epoch": 1.3687130447477918,
      "grad_norm": 4.725980758666992,
      "learning_rate": 4.885940579604351e-05,
      "loss": 0.7654,
      "step": 150000
    },
    {
      "epoch": 1.369625520110957,
      "grad_norm": 4.712015628814697,
      "learning_rate": 4.885864539990754e-05,
      "loss": 0.819,
      "step": 150100
    },
    {
      "epoch": 1.3705379954741221,
      "grad_norm": 4.139769554138184,
      "learning_rate": 4.885788500377157e-05,
      "loss": 0.7761,
      "step": 150200
    },
    {
      "epoch": 1.3714504708372874,
      "grad_norm": 5.185241222381592,
      "learning_rate": 4.8857124607635594e-05,
      "loss": 0.7551,
      "step": 150300
    },
    {
      "epoch": 1.3723629462004525,
      "grad_norm": 3.7040979862213135,
      "learning_rate": 4.885636421149963e-05,
      "loss": 0.7885,
      "step": 150400
    },
    {
      "epoch": 1.3732754215636178,
      "grad_norm": 4.07720947265625,
      "learning_rate": 4.8855603815363654e-05,
      "loss": 0.7674,
      "step": 150500
    },
    {
      "epoch": 1.3741878969267831,
      "grad_norm": 4.92459774017334,
      "learning_rate": 4.885484341922768e-05,
      "loss": 0.8125,
      "step": 150600
    },
    {
      "epoch": 1.3751003722899482,
      "grad_norm": 2.543748378753662,
      "learning_rate": 4.8854083023091714e-05,
      "loss": 0.7644,
      "step": 150700
    },
    {
      "epoch": 1.3760128476531133,
      "grad_norm": 4.484607696533203,
      "learning_rate": 4.885332262695574e-05,
      "loss": 0.7694,
      "step": 150800
    },
    {
      "epoch": 1.3769253230162786,
      "grad_norm": 4.1798248291015625,
      "learning_rate": 4.885256223081977e-05,
      "loss": 0.7505,
      "step": 150900
    },
    {
      "epoch": 1.3778377983794439,
      "grad_norm": 5.654468059539795,
      "learning_rate": 4.88518018346838e-05,
      "loss": 0.7844,
      "step": 151000
    },
    {
      "epoch": 1.378750273742609,
      "grad_norm": 5.1203389167785645,
      "learning_rate": 4.885104143854783e-05,
      "loss": 0.7371,
      "step": 151100
    },
    {
      "epoch": 1.379662749105774,
      "grad_norm": 3.415299415588379,
      "learning_rate": 4.885028104241186e-05,
      "loss": 0.7936,
      "step": 151200
    },
    {
      "epoch": 1.3805752244689393,
      "grad_norm": 4.779162883758545,
      "learning_rate": 4.884952064627589e-05,
      "loss": 0.79,
      "step": 151300
    },
    {
      "epoch": 1.3814876998321046,
      "grad_norm": 4.6649699211120605,
      "learning_rate": 4.884876025013991e-05,
      "loss": 0.7832,
      "step": 151400
    },
    {
      "epoch": 1.3824001751952697,
      "grad_norm": 3.8678090572357178,
      "learning_rate": 4.884799985400395e-05,
      "loss": 0.72,
      "step": 151500
    },
    {
      "epoch": 1.383312650558435,
      "grad_norm": 4.371793746948242,
      "learning_rate": 4.884723945786797e-05,
      "loss": 0.74,
      "step": 151600
    },
    {
      "epoch": 1.3842251259216,
      "grad_norm": 4.37488317489624,
      "learning_rate": 4.8846479061732e-05,
      "loss": 0.7794,
      "step": 151700
    },
    {
      "epoch": 1.3851376012847654,
      "grad_norm": 4.661971569061279,
      "learning_rate": 4.884571866559603e-05,
      "loss": 0.8049,
      "step": 151800
    },
    {
      "epoch": 1.3860500766479305,
      "grad_norm": 4.654907703399658,
      "learning_rate": 4.884495826946006e-05,
      "loss": 0.8169,
      "step": 151900
    },
    {
      "epoch": 1.3869625520110958,
      "grad_norm": 4.482296466827393,
      "learning_rate": 4.8844197873324085e-05,
      "loss": 0.837,
      "step": 152000
    },
    {
      "epoch": 1.3878750273742608,
      "grad_norm": 4.252895355224609,
      "learning_rate": 4.884343747718812e-05,
      "loss": 0.775,
      "step": 152100
    },
    {
      "epoch": 1.3887875027374261,
      "grad_norm": 3.8146650791168213,
      "learning_rate": 4.8842677081052145e-05,
      "loss": 0.8103,
      "step": 152200
    },
    {
      "epoch": 1.3896999781005912,
      "grad_norm": 3.5230770111083984,
      "learning_rate": 4.8841916684916175e-05,
      "loss": 0.7969,
      "step": 152300
    },
    {
      "epoch": 1.3906124534637565,
      "grad_norm": 6.155178546905518,
      "learning_rate": 4.8841156288780205e-05,
      "loss": 0.7439,
      "step": 152400
    },
    {
      "epoch": 1.3915249288269216,
      "grad_norm": 3.9659643173217773,
      "learning_rate": 4.8840395892644235e-05,
      "loss": 0.7855,
      "step": 152500
    },
    {
      "epoch": 1.3924374041900869,
      "grad_norm": 4.546120643615723,
      "learning_rate": 4.8839635496508265e-05,
      "loss": 0.7766,
      "step": 152600
    },
    {
      "epoch": 1.3933498795532522,
      "grad_norm": 4.065113067626953,
      "learning_rate": 4.8838875100372296e-05,
      "loss": 0.7591,
      "step": 152700
    },
    {
      "epoch": 1.3942623549164173,
      "grad_norm": 3.9228873252868652,
      "learning_rate": 4.883811470423632e-05,
      "loss": 0.7491,
      "step": 152800
    },
    {
      "epoch": 1.3951748302795823,
      "grad_norm": 4.182530879974365,
      "learning_rate": 4.8837354308100356e-05,
      "loss": 0.7862,
      "step": 152900
    },
    {
      "epoch": 1.3960873056427476,
      "grad_norm": 4.220347881317139,
      "learning_rate": 4.883659391196438e-05,
      "loss": 0.8079,
      "step": 153000
    },
    {
      "epoch": 1.396999781005913,
      "grad_norm": 4.0926289558410645,
      "learning_rate": 4.883583351582841e-05,
      "loss": 0.8043,
      "step": 153100
    },
    {
      "epoch": 1.397912256369078,
      "grad_norm": 3.8934412002563477,
      "learning_rate": 4.883507311969244e-05,
      "loss": 0.7787,
      "step": 153200
    },
    {
      "epoch": 1.3988247317322433,
      "grad_norm": 4.232353210449219,
      "learning_rate": 4.883431272355647e-05,
      "loss": 0.8187,
      "step": 153300
    },
    {
      "epoch": 1.3997372070954084,
      "grad_norm": 5.165478706359863,
      "learning_rate": 4.883355232742049e-05,
      "loss": 0.8027,
      "step": 153400
    },
    {
      "epoch": 1.4006496824585737,
      "grad_norm": 4.541467189788818,
      "learning_rate": 4.883279193128452e-05,
      "loss": 0.7766,
      "step": 153500
    },
    {
      "epoch": 1.4015621578217388,
      "grad_norm": 3.5163280963897705,
      "learning_rate": 4.883203153514855e-05,
      "loss": 0.7881,
      "step": 153600
    },
    {
      "epoch": 1.402474633184904,
      "grad_norm": 4.457280158996582,
      "learning_rate": 4.883127113901258e-05,
      "loss": 0.7563,
      "step": 153700
    },
    {
      "epoch": 1.4033871085480691,
      "grad_norm": 4.692083358764648,
      "learning_rate": 4.883051074287661e-05,
      "loss": 0.7941,
      "step": 153800
    },
    {
      "epoch": 1.4042995839112344,
      "grad_norm": 3.5561485290527344,
      "learning_rate": 4.8829750346740636e-05,
      "loss": 0.778,
      "step": 153900
    },
    {
      "epoch": 1.4052120592743995,
      "grad_norm": 4.607639312744141,
      "learning_rate": 4.882898995060467e-05,
      "loss": 0.741,
      "step": 154000
    },
    {
      "epoch": 1.4061245346375648,
      "grad_norm": 4.062695503234863,
      "learning_rate": 4.8828229554468696e-05,
      "loss": 0.7697,
      "step": 154100
    },
    {
      "epoch": 1.40703701000073,
      "grad_norm": 4.3309783935546875,
      "learning_rate": 4.8827469158332726e-05,
      "loss": 0.7982,
      "step": 154200
    },
    {
      "epoch": 1.4079494853638952,
      "grad_norm": 5.003017902374268,
      "learning_rate": 4.8826708762196756e-05,
      "loss": 0.7976,
      "step": 154300
    },
    {
      "epoch": 1.4088619607270605,
      "grad_norm": 3.7114651203155518,
      "learning_rate": 4.8825948366060786e-05,
      "loss": 0.8127,
      "step": 154400
    },
    {
      "epoch": 1.4097744360902256,
      "grad_norm": 4.199734210968018,
      "learning_rate": 4.882518796992481e-05,
      "loss": 0.7937,
      "step": 154500
    },
    {
      "epoch": 1.4106869114533906,
      "grad_norm": 3.465270519256592,
      "learning_rate": 4.8824427573788846e-05,
      "loss": 0.7883,
      "step": 154600
    },
    {
      "epoch": 1.411599386816556,
      "grad_norm": 3.9870402812957764,
      "learning_rate": 4.882366717765287e-05,
      "loss": 0.8172,
      "step": 154700
    },
    {
      "epoch": 1.4125118621797212,
      "grad_norm": 5.318056583404541,
      "learning_rate": 4.88229067815169e-05,
      "loss": 0.7839,
      "step": 154800
    },
    {
      "epoch": 1.4134243375428863,
      "grad_norm": 4.140570163726807,
      "learning_rate": 4.882214638538093e-05,
      "loss": 0.8218,
      "step": 154900
    },
    {
      "epoch": 1.4143368129060514,
      "grad_norm": 4.746542930603027,
      "learning_rate": 4.882138598924496e-05,
      "loss": 0.7705,
      "step": 155000
    },
    {
      "epoch": 1.4152492882692167,
      "grad_norm": 4.4167256355285645,
      "learning_rate": 4.882062559310899e-05,
      "loss": 0.7996,
      "step": 155100
    },
    {
      "epoch": 1.416161763632382,
      "grad_norm": 4.126987934112549,
      "learning_rate": 4.881986519697302e-05,
      "loss": 0.811,
      "step": 155200
    },
    {
      "epoch": 1.417074238995547,
      "grad_norm": 3.349886178970337,
      "learning_rate": 4.881910480083704e-05,
      "loss": 0.7886,
      "step": 155300
    },
    {
      "epoch": 1.4179867143587124,
      "grad_norm": 3.640625238418579,
      "learning_rate": 4.881834440470108e-05,
      "loss": 0.7783,
      "step": 155400
    },
    {
      "epoch": 1.4188991897218775,
      "grad_norm": 4.060667037963867,
      "learning_rate": 4.8817584008565104e-05,
      "loss": 0.7954,
      "step": 155500
    },
    {
      "epoch": 1.4198116650850428,
      "grad_norm": 4.035653114318848,
      "learning_rate": 4.8816823612429134e-05,
      "loss": 0.7685,
      "step": 155600
    },
    {
      "epoch": 1.4207241404482078,
      "grad_norm": 4.257676601409912,
      "learning_rate": 4.8816063216293164e-05,
      "loss": 0.7543,
      "step": 155700
    },
    {
      "epoch": 1.4216366158113731,
      "grad_norm": 4.389501571655273,
      "learning_rate": 4.8815302820157194e-05,
      "loss": 0.7917,
      "step": 155800
    },
    {
      "epoch": 1.4225490911745382,
      "grad_norm": 3.8456740379333496,
      "learning_rate": 4.881454242402122e-05,
      "loss": 0.7888,
      "step": 155900
    },
    {
      "epoch": 1.4234615665377035,
      "grad_norm": 4.608313083648682,
      "learning_rate": 4.8813782027885254e-05,
      "loss": 0.7468,
      "step": 156000
    },
    {
      "epoch": 1.4243740419008688,
      "grad_norm": 3.6973557472229004,
      "learning_rate": 4.881302163174928e-05,
      "loss": 0.8097,
      "step": 156100
    },
    {
      "epoch": 1.4252865172640339,
      "grad_norm": 4.49501895904541,
      "learning_rate": 4.881226123561331e-05,
      "loss": 0.8202,
      "step": 156200
    },
    {
      "epoch": 1.426198992627199,
      "grad_norm": 4.587484359741211,
      "learning_rate": 4.881150083947734e-05,
      "loss": 0.8364,
      "step": 156300
    },
    {
      "epoch": 1.4271114679903643,
      "grad_norm": 4.832925319671631,
      "learning_rate": 4.881074044334136e-05,
      "loss": 0.7893,
      "step": 156400
    },
    {
      "epoch": 1.4280239433535296,
      "grad_norm": 4.267385005950928,
      "learning_rate": 4.88099800472054e-05,
      "loss": 0.814,
      "step": 156500
    },
    {
      "epoch": 1.4289364187166946,
      "grad_norm": 4.177628993988037,
      "learning_rate": 4.880921965106942e-05,
      "loss": 0.7696,
      "step": 156600
    },
    {
      "epoch": 1.4298488940798597,
      "grad_norm": 3.8848471641540527,
      "learning_rate": 4.880845925493345e-05,
      "loss": 0.7991,
      "step": 156700
    },
    {
      "epoch": 1.430761369443025,
      "grad_norm": 5.316272735595703,
      "learning_rate": 4.880769885879748e-05,
      "loss": 0.8044,
      "step": 156800
    },
    {
      "epoch": 1.4316738448061903,
      "grad_norm": 4.257831573486328,
      "learning_rate": 4.880693846266151e-05,
      "loss": 0.8052,
      "step": 156900
    },
    {
      "epoch": 1.4325863201693554,
      "grad_norm": 4.578546524047852,
      "learning_rate": 4.8806178066525534e-05,
      "loss": 0.776,
      "step": 157000
    },
    {
      "epoch": 1.4334987955325207,
      "grad_norm": 3.135012626647949,
      "learning_rate": 4.880541767038957e-05,
      "loss": 0.7669,
      "step": 157100
    },
    {
      "epoch": 1.4344112708956858,
      "grad_norm": 4.549189567565918,
      "learning_rate": 4.8804657274253594e-05,
      "loss": 0.7815,
      "step": 157200
    },
    {
      "epoch": 1.435323746258851,
      "grad_norm": 4.881802082061768,
      "learning_rate": 4.8803896878117624e-05,
      "loss": 0.7715,
      "step": 157300
    },
    {
      "epoch": 1.4362362216220161,
      "grad_norm": 4.3244123458862305,
      "learning_rate": 4.8803136481981654e-05,
      "loss": 0.7819,
      "step": 157400
    },
    {
      "epoch": 1.4371486969851814,
      "grad_norm": 4.690623760223389,
      "learning_rate": 4.8802376085845685e-05,
      "loss": 0.747,
      "step": 157500
    },
    {
      "epoch": 1.4380611723483465,
      "grad_norm": 4.15639066696167,
      "learning_rate": 4.8801615689709715e-05,
      "loss": 0.7867,
      "step": 157600
    },
    {
      "epoch": 1.4389736477115118,
      "grad_norm": 4.589691638946533,
      "learning_rate": 4.8800855293573745e-05,
      "loss": 0.8091,
      "step": 157700
    },
    {
      "epoch": 1.4398861230746771,
      "grad_norm": 5.116901397705078,
      "learning_rate": 4.880009489743777e-05,
      "loss": 0.8488,
      "step": 157800
    },
    {
      "epoch": 1.4407985984378422,
      "grad_norm": 4.654663562774658,
      "learning_rate": 4.8799334501301805e-05,
      "loss": 0.7471,
      "step": 157900
    },
    {
      "epoch": 1.4417110738010073,
      "grad_norm": 5.282893657684326,
      "learning_rate": 4.879857410516583e-05,
      "loss": 0.8078,
      "step": 158000
    },
    {
      "epoch": 1.4426235491641726,
      "grad_norm": 4.128176689147949,
      "learning_rate": 4.879781370902986e-05,
      "loss": 0.7373,
      "step": 158100
    },
    {
      "epoch": 1.4435360245273379,
      "grad_norm": 3.2244322299957275,
      "learning_rate": 4.879705331289389e-05,
      "loss": 0.7577,
      "step": 158200
    },
    {
      "epoch": 1.444448499890503,
      "grad_norm": 3.8687288761138916,
      "learning_rate": 4.879629291675792e-05,
      "loss": 0.8018,
      "step": 158300
    },
    {
      "epoch": 1.445360975253668,
      "grad_norm": 5.243131637573242,
      "learning_rate": 4.879553252062194e-05,
      "loss": 0.7998,
      "step": 158400
    },
    {
      "epoch": 1.4462734506168333,
      "grad_norm": 5.473785400390625,
      "learning_rate": 4.879477212448598e-05,
      "loss": 0.8046,
      "step": 158500
    },
    {
      "epoch": 1.4471859259799986,
      "grad_norm": 3.5679259300231934,
      "learning_rate": 4.879401172835e-05,
      "loss": 0.7808,
      "step": 158600
    },
    {
      "epoch": 1.4480984013431637,
      "grad_norm": 4.543566703796387,
      "learning_rate": 4.879325133221403e-05,
      "loss": 0.821,
      "step": 158700
    },
    {
      "epoch": 1.449010876706329,
      "grad_norm": 4.447004318237305,
      "learning_rate": 4.879249093607806e-05,
      "loss": 0.7984,
      "step": 158800
    },
    {
      "epoch": 1.449923352069494,
      "grad_norm": 4.315978527069092,
      "learning_rate": 4.879173053994209e-05,
      "loss": 0.785,
      "step": 158900
    },
    {
      "epoch": 1.4508358274326594,
      "grad_norm": 3.8171443939208984,
      "learning_rate": 4.879097014380612e-05,
      "loss": 0.7871,
      "step": 159000
    },
    {
      "epoch": 1.4517483027958245,
      "grad_norm": 4.569701194763184,
      "learning_rate": 4.8790209747670145e-05,
      "loss": 0.8052,
      "step": 159100
    },
    {
      "epoch": 1.4526607781589898,
      "grad_norm": 3.9485573768615723,
      "learning_rate": 4.8789449351534175e-05,
      "loss": 0.7764,
      "step": 159200
    },
    {
      "epoch": 1.4535732535221548,
      "grad_norm": 4.318072319030762,
      "learning_rate": 4.8788688955398205e-05,
      "loss": 0.7729,
      "step": 159300
    },
    {
      "epoch": 1.4544857288853201,
      "grad_norm": 4.339328765869141,
      "learning_rate": 4.8787928559262235e-05,
      "loss": 0.7947,
      "step": 159400
    },
    {
      "epoch": 1.4553982042484854,
      "grad_norm": 4.655227184295654,
      "learning_rate": 4.8787168163126266e-05,
      "loss": 0.7308,
      "step": 159500
    },
    {
      "epoch": 1.4563106796116505,
      "grad_norm": 4.0664472579956055,
      "learning_rate": 4.8786407766990296e-05,
      "loss": 0.8288,
      "step": 159600
    },
    {
      "epoch": 1.4572231549748156,
      "grad_norm": 4.373852729797363,
      "learning_rate": 4.878564737085432e-05,
      "loss": 0.8179,
      "step": 159700
    },
    {
      "epoch": 1.4581356303379809,
      "grad_norm": 4.998409748077393,
      "learning_rate": 4.878488697471835e-05,
      "loss": 0.81,
      "step": 159800
    },
    {
      "epoch": 1.4590481057011462,
      "grad_norm": 4.263584613800049,
      "learning_rate": 4.878412657858238e-05,
      "loss": 0.7891,
      "step": 159900
    },
    {
      "epoch": 1.4599605810643113,
      "grad_norm": 4.697996616363525,
      "learning_rate": 4.878336618244641e-05,
      "loss": 0.7893,
      "step": 160000
    },
    {
      "epoch": 1.4608730564274763,
      "grad_norm": 3.9987313747406006,
      "learning_rate": 4.878260578631044e-05,
      "loss": 0.8103,
      "step": 160100
    },
    {
      "epoch": 1.4617855317906416,
      "grad_norm": 4.1427001953125,
      "learning_rate": 4.878184539017447e-05,
      "loss": 0.7586,
      "step": 160200
    },
    {
      "epoch": 1.462698007153807,
      "grad_norm": 3.855238676071167,
      "learning_rate": 4.878108499403849e-05,
      "loss": 0.7665,
      "step": 160300
    },
    {
      "epoch": 1.463610482516972,
      "grad_norm": 3.6567916870117188,
      "learning_rate": 4.878032459790253e-05,
      "loss": 0.7604,
      "step": 160400
    },
    {
      "epoch": 1.4645229578801373,
      "grad_norm": 4.153293132781982,
      "learning_rate": 4.877956420176655e-05,
      "loss": 0.7989,
      "step": 160500
    },
    {
      "epoch": 1.4654354332433024,
      "grad_norm": 3.6193976402282715,
      "learning_rate": 4.877880380563058e-05,
      "loss": 0.7356,
      "step": 160600
    },
    {
      "epoch": 1.4663479086064677,
      "grad_norm": 4.739593505859375,
      "learning_rate": 4.877804340949461e-05,
      "loss": 0.7787,
      "step": 160700
    },
    {
      "epoch": 1.4672603839696328,
      "grad_norm": 4.776556968688965,
      "learning_rate": 4.877728301335864e-05,
      "loss": 0.7711,
      "step": 160800
    },
    {
      "epoch": 1.468172859332798,
      "grad_norm": 4.460022449493408,
      "learning_rate": 4.877652261722267e-05,
      "loss": 0.7823,
      "step": 160900
    },
    {
      "epoch": 1.4690853346959631,
      "grad_norm": 4.359416961669922,
      "learning_rate": 4.87757622210867e-05,
      "loss": 0.7655,
      "step": 161000
    },
    {
      "epoch": 1.4699978100591284,
      "grad_norm": 4.9521989822387695,
      "learning_rate": 4.8775001824950726e-05,
      "loss": 0.8189,
      "step": 161100
    },
    {
      "epoch": 1.4709102854222935,
      "grad_norm": 3.874842882156372,
      "learning_rate": 4.877424142881476e-05,
      "loss": 0.7754,
      "step": 161200
    },
    {
      "epoch": 1.4718227607854588,
      "grad_norm": 2.6105728149414062,
      "learning_rate": 4.8773481032678786e-05,
      "loss": 0.7993,
      "step": 161300
    },
    {
      "epoch": 1.472735236148624,
      "grad_norm": 4.2694926261901855,
      "learning_rate": 4.8772720636542816e-05,
      "loss": 0.7339,
      "step": 161400
    },
    {
      "epoch": 1.4736477115117892,
      "grad_norm": 4.504145622253418,
      "learning_rate": 4.8771960240406847e-05,
      "loss": 0.8159,
      "step": 161500
    },
    {
      "epoch": 1.4745601868749545,
      "grad_norm": 4.200126647949219,
      "learning_rate": 4.877119984427088e-05,
      "loss": 0.7869,
      "step": 161600
    },
    {
      "epoch": 1.4754726622381196,
      "grad_norm": 3.905176877975464,
      "learning_rate": 4.87704394481349e-05,
      "loss": 0.7814,
      "step": 161700
    },
    {
      "epoch": 1.4763851376012846,
      "grad_norm": 4.908121109008789,
      "learning_rate": 4.876967905199894e-05,
      "loss": 0.8142,
      "step": 161800
    },
    {
      "epoch": 1.47729761296445,
      "grad_norm": 4.231546401977539,
      "learning_rate": 4.876891865586296e-05,
      "loss": 0.78,
      "step": 161900
    },
    {
      "epoch": 1.4782100883276152,
      "grad_norm": 4.172248363494873,
      "learning_rate": 4.876815825972699e-05,
      "loss": 0.7589,
      "step": 162000
    },
    {
      "epoch": 1.4791225636907803,
      "grad_norm": 4.416748046875,
      "learning_rate": 4.876739786359102e-05,
      "loss": 0.7291,
      "step": 162100
    },
    {
      "epoch": 1.4800350390539456,
      "grad_norm": 3.9764578342437744,
      "learning_rate": 4.8766637467455043e-05,
      "loss": 0.7961,
      "step": 162200
    },
    {
      "epoch": 1.4809475144171107,
      "grad_norm": 4.947453022003174,
      "learning_rate": 4.876587707131908e-05,
      "loss": 0.7453,
      "step": 162300
    },
    {
      "epoch": 1.481859989780276,
      "grad_norm": 3.3750264644622803,
      "learning_rate": 4.8765116675183104e-05,
      "loss": 0.7866,
      "step": 162400
    },
    {
      "epoch": 1.482772465143441,
      "grad_norm": 4.17131233215332,
      "learning_rate": 4.8764356279047134e-05,
      "loss": 0.7511,
      "step": 162500
    },
    {
      "epoch": 1.4836849405066064,
      "grad_norm": 4.8659844398498535,
      "learning_rate": 4.8763595882911164e-05,
      "loss": 0.7893,
      "step": 162600
    },
    {
      "epoch": 1.4845974158697715,
      "grad_norm": 4.2721076011657715,
      "learning_rate": 4.8762835486775194e-05,
      "loss": 0.7567,
      "step": 162700
    },
    {
      "epoch": 1.4855098912329368,
      "grad_norm": 4.120404243469238,
      "learning_rate": 4.876207509063922e-05,
      "loss": 0.7856,
      "step": 162800
    },
    {
      "epoch": 1.4864223665961018,
      "grad_norm": 3.7165164947509766,
      "learning_rate": 4.8761314694503254e-05,
      "loss": 0.777,
      "step": 162900
    },
    {
      "epoch": 1.4873348419592671,
      "grad_norm": 3.937544822692871,
      "learning_rate": 4.876055429836728e-05,
      "loss": 0.7645,
      "step": 163000
    },
    {
      "epoch": 1.4882473173224322,
      "grad_norm": 4.291866302490234,
      "learning_rate": 4.875979390223131e-05,
      "loss": 0.798,
      "step": 163100
    },
    {
      "epoch": 1.4891597926855975,
      "grad_norm": 4.408498287200928,
      "learning_rate": 4.875903350609534e-05,
      "loss": 0.7533,
      "step": 163200
    },
    {
      "epoch": 1.4900722680487628,
      "grad_norm": 3.9166383743286133,
      "learning_rate": 4.875827310995937e-05,
      "loss": 0.7681,
      "step": 163300
    },
    {
      "epoch": 1.4909847434119279,
      "grad_norm": 3.8578531742095947,
      "learning_rate": 4.87575127138234e-05,
      "loss": 0.7834,
      "step": 163400
    },
    {
      "epoch": 1.491897218775093,
      "grad_norm": 4.605504989624023,
      "learning_rate": 4.875675231768743e-05,
      "loss": 0.8251,
      "step": 163500
    },
    {
      "epoch": 1.4928096941382583,
      "grad_norm": 4.831454277038574,
      "learning_rate": 4.875599192155145e-05,
      "loss": 0.7692,
      "step": 163600
    },
    {
      "epoch": 1.4937221695014236,
      "grad_norm": 3.974107503890991,
      "learning_rate": 4.875523152541549e-05,
      "loss": 0.7761,
      "step": 163700
    },
    {
      "epoch": 1.4946346448645886,
      "grad_norm": 5.5999956130981445,
      "learning_rate": 4.875447112927951e-05,
      "loss": 0.7604,
      "step": 163800
    },
    {
      "epoch": 1.4955471202277537,
      "grad_norm": 4.690223217010498,
      "learning_rate": 4.875371073314354e-05,
      "loss": 0.7646,
      "step": 163900
    },
    {
      "epoch": 1.496459595590919,
      "grad_norm": 3.95107364654541,
      "learning_rate": 4.875295033700757e-05,
      "loss": 0.7554,
      "step": 164000
    },
    {
      "epoch": 1.4973720709540843,
      "grad_norm": 4.562201023101807,
      "learning_rate": 4.87521899408716e-05,
      "loss": 0.7652,
      "step": 164100
    },
    {
      "epoch": 1.4982845463172494,
      "grad_norm": 4.590007305145264,
      "learning_rate": 4.8751429544735624e-05,
      "loss": 0.8115,
      "step": 164200
    },
    {
      "epoch": 1.4991970216804147,
      "grad_norm": 4.191768169403076,
      "learning_rate": 4.875066914859966e-05,
      "loss": 0.7738,
      "step": 164300
    },
    {
      "epoch": 1.5001094970435798,
      "grad_norm": 3.8362529277801514,
      "learning_rate": 4.8749908752463685e-05,
      "loss": 0.7963,
      "step": 164400
    },
    {
      "epoch": 1.501021972406745,
      "grad_norm": 3.6391797065734863,
      "learning_rate": 4.8749148356327715e-05,
      "loss": 0.7504,
      "step": 164500
    },
    {
      "epoch": 1.5019344477699104,
      "grad_norm": 4.29632568359375,
      "learning_rate": 4.8748387960191745e-05,
      "loss": 0.7945,
      "step": 164600
    },
    {
      "epoch": 1.5028469231330754,
      "grad_norm": 4.763289451599121,
      "learning_rate": 4.8747627564055775e-05,
      "loss": 0.7921,
      "step": 164700
    },
    {
      "epoch": 1.5037593984962405,
      "grad_norm": 3.670179605484009,
      "learning_rate": 4.8746867167919805e-05,
      "loss": 0.7966,
      "step": 164800
    },
    {
      "epoch": 1.5046718738594058,
      "grad_norm": 5.065375804901123,
      "learning_rate": 4.874610677178383e-05,
      "loss": 0.8253,
      "step": 164900
    },
    {
      "epoch": 1.5055843492225711,
      "grad_norm": 4.909914016723633,
      "learning_rate": 4.874534637564786e-05,
      "loss": 0.7742,
      "step": 165000
    },
    {
      "epoch": 1.5064968245857362,
      "grad_norm": 4.2248382568359375,
      "learning_rate": 4.874458597951189e-05,
      "loss": 0.7757,
      "step": 165100
    },
    {
      "epoch": 1.5074092999489013,
      "grad_norm": 3.639842987060547,
      "learning_rate": 4.874382558337592e-05,
      "loss": 0.7427,
      "step": 165200
    },
    {
      "epoch": 1.5083217753120666,
      "grad_norm": 5.437606334686279,
      "learning_rate": 4.874306518723994e-05,
      "loss": 0.8385,
      "step": 165300
    },
    {
      "epoch": 1.5092342506752319,
      "grad_norm": 3.847785234451294,
      "learning_rate": 4.874230479110398e-05,
      "loss": 0.7934,
      "step": 165400
    },
    {
      "epoch": 1.510146726038397,
      "grad_norm": 4.147586345672607,
      "learning_rate": 4.8741544394968e-05,
      "loss": 0.7804,
      "step": 165500
    },
    {
      "epoch": 1.511059201401562,
      "grad_norm": 4.296707630157471,
      "learning_rate": 4.874078399883203e-05,
      "loss": 0.8001,
      "step": 165600
    },
    {
      "epoch": 1.5119716767647273,
      "grad_norm": 4.192218780517578,
      "learning_rate": 4.874002360269606e-05,
      "loss": 0.7924,
      "step": 165700
    },
    {
      "epoch": 1.5128841521278926,
      "grad_norm": 4.084393501281738,
      "learning_rate": 4.873926320656009e-05,
      "loss": 0.7819,
      "step": 165800
    },
    {
      "epoch": 1.5137966274910577,
      "grad_norm": 3.8163747787475586,
      "learning_rate": 4.873850281042412e-05,
      "loss": 0.7584,
      "step": 165900
    },
    {
      "epoch": 1.5147091028542228,
      "grad_norm": 4.3763885498046875,
      "learning_rate": 4.873774241428815e-05,
      "loss": 0.7809,
      "step": 166000
    },
    {
      "epoch": 1.515621578217388,
      "grad_norm": 4.837245941162109,
      "learning_rate": 4.8736982018152175e-05,
      "loss": 0.7582,
      "step": 166100
    },
    {
      "epoch": 1.5165340535805534,
      "grad_norm": 4.702744483947754,
      "learning_rate": 4.873622162201621e-05,
      "loss": 0.7678,
      "step": 166200
    },
    {
      "epoch": 1.5174465289437187,
      "grad_norm": 4.203624248504639,
      "learning_rate": 4.8735461225880236e-05,
      "loss": 0.7275,
      "step": 166300
    },
    {
      "epoch": 1.5183590043068838,
      "grad_norm": 4.309709072113037,
      "learning_rate": 4.8734700829744266e-05,
      "loss": 0.7596,
      "step": 166400
    },
    {
      "epoch": 1.5192714796700488,
      "grad_norm": 4.669897556304932,
      "learning_rate": 4.8733940433608296e-05,
      "loss": 0.8124,
      "step": 166500
    },
    {
      "epoch": 1.5201839550332141,
      "grad_norm": 4.085377216339111,
      "learning_rate": 4.8733180037472326e-05,
      "loss": 0.7257,
      "step": 166600
    },
    {
      "epoch": 1.5210964303963794,
      "grad_norm": 3.699211359024048,
      "learning_rate": 4.873241964133635e-05,
      "loss": 0.7794,
      "step": 166700
    },
    {
      "epoch": 1.5220089057595445,
      "grad_norm": 4.587733268737793,
      "learning_rate": 4.8731659245200386e-05,
      "loss": 0.7681,
      "step": 166800
    },
    {
      "epoch": 1.5229213811227096,
      "grad_norm": 3.625201463699341,
      "learning_rate": 4.873089884906441e-05,
      "loss": 0.7796,
      "step": 166900
    },
    {
      "epoch": 1.5238338564858749,
      "grad_norm": 4.205513954162598,
      "learning_rate": 4.873013845292844e-05,
      "loss": 0.8083,
      "step": 167000
    },
    {
      "epoch": 1.5247463318490402,
      "grad_norm": 4.374755859375,
      "learning_rate": 4.872937805679247e-05,
      "loss": 0.7893,
      "step": 167100
    },
    {
      "epoch": 1.5256588072122053,
      "grad_norm": 4.925912857055664,
      "learning_rate": 4.87286176606565e-05,
      "loss": 0.7653,
      "step": 167200
    },
    {
      "epoch": 1.5265712825753703,
      "grad_norm": 4.767482280731201,
      "learning_rate": 4.872785726452053e-05,
      "loss": 0.7965,
      "step": 167300
    },
    {
      "epoch": 1.5274837579385356,
      "grad_norm": 4.95378303527832,
      "learning_rate": 4.872709686838456e-05,
      "loss": 0.7659,
      "step": 167400
    },
    {
      "epoch": 1.528396233301701,
      "grad_norm": 4.429091930389404,
      "learning_rate": 4.872633647224858e-05,
      "loss": 0.7906,
      "step": 167500
    },
    {
      "epoch": 1.529308708664866,
      "grad_norm": 3.79144024848938,
      "learning_rate": 4.872557607611261e-05,
      "loss": 0.7692,
      "step": 167600
    },
    {
      "epoch": 1.530221184028031,
      "grad_norm": 4.102790355682373,
      "learning_rate": 4.872481567997664e-05,
      "loss": 0.7868,
      "step": 167700
    },
    {
      "epoch": 1.5311336593911964,
      "grad_norm": 4.143951892852783,
      "learning_rate": 4.8724055283840666e-05,
      "loss": 0.7827,
      "step": 167800
    },
    {
      "epoch": 1.5320461347543617,
      "grad_norm": 5.730300426483154,
      "learning_rate": 4.87232948877047e-05,
      "loss": 0.8251,
      "step": 167900
    },
    {
      "epoch": 1.532958610117527,
      "grad_norm": 6.017107009887695,
      "learning_rate": 4.8722534491568726e-05,
      "loss": 0.7819,
      "step": 168000
    },
    {
      "epoch": 1.533871085480692,
      "grad_norm": 5.039291858673096,
      "learning_rate": 4.8721774095432756e-05,
      "loss": 0.7972,
      "step": 168100
    },
    {
      "epoch": 1.5347835608438571,
      "grad_norm": 4.321450710296631,
      "learning_rate": 4.8721013699296787e-05,
      "loss": 0.7819,
      "step": 168200
    },
    {
      "epoch": 1.5356960362070224,
      "grad_norm": 4.489201068878174,
      "learning_rate": 4.8720253303160817e-05,
      "loss": 0.7898,
      "step": 168300
    },
    {
      "epoch": 1.5366085115701877,
      "grad_norm": 4.42021369934082,
      "learning_rate": 4.871949290702485e-05,
      "loss": 0.7799,
      "step": 168400
    },
    {
      "epoch": 1.5375209869333528,
      "grad_norm": 4.78828239440918,
      "learning_rate": 4.871873251088888e-05,
      "loss": 0.7613,
      "step": 168500
    },
    {
      "epoch": 1.538433462296518,
      "grad_norm": 4.821408271789551,
      "learning_rate": 4.87179721147529e-05,
      "loss": 0.7698,
      "step": 168600
    },
    {
      "epoch": 1.5393459376596832,
      "grad_norm": 4.525170803070068,
      "learning_rate": 4.871721171861694e-05,
      "loss": 0.8031,
      "step": 168700
    },
    {
      "epoch": 1.5402584130228485,
      "grad_norm": 4.573408603668213,
      "learning_rate": 4.871645132248096e-05,
      "loss": 0.7959,
      "step": 168800
    },
    {
      "epoch": 1.5411708883860136,
      "grad_norm": 3.1946566104888916,
      "learning_rate": 4.871569092634499e-05,
      "loss": 0.7329,
      "step": 168900
    },
    {
      "epoch": 1.5420833637491786,
      "grad_norm": 4.360457420349121,
      "learning_rate": 4.871493053020902e-05,
      "loss": 0.8085,
      "step": 169000
    },
    {
      "epoch": 1.542995839112344,
      "grad_norm": 3.3444395065307617,
      "learning_rate": 4.871417013407305e-05,
      "loss": 0.7689,
      "step": 169100
    },
    {
      "epoch": 1.5439083144755092,
      "grad_norm": 4.85498571395874,
      "learning_rate": 4.8713409737937074e-05,
      "loss": 0.8056,
      "step": 169200
    },
    {
      "epoch": 1.5448207898386743,
      "grad_norm": 4.243717670440674,
      "learning_rate": 4.871264934180111e-05,
      "loss": 0.7961,
      "step": 169300
    },
    {
      "epoch": 1.5457332652018394,
      "grad_norm": 4.391078472137451,
      "learning_rate": 4.8711888945665134e-05,
      "loss": 0.83,
      "step": 169400
    },
    {
      "epoch": 1.5466457405650047,
      "grad_norm": 4.327888488769531,
      "learning_rate": 4.8711128549529164e-05,
      "loss": 0.8061,
      "step": 169500
    },
    {
      "epoch": 1.54755821592817,
      "grad_norm": 3.7305490970611572,
      "learning_rate": 4.8710368153393194e-05,
      "loss": 0.7696,
      "step": 169600
    },
    {
      "epoch": 1.5484706912913353,
      "grad_norm": 5.212944030761719,
      "learning_rate": 4.8709607757257224e-05,
      "loss": 0.763,
      "step": 169700
    },
    {
      "epoch": 1.5493831666545004,
      "grad_norm": 5.0500617027282715,
      "learning_rate": 4.8708847361121254e-05,
      "loss": 0.817,
      "step": 169800
    },
    {
      "epoch": 1.5502956420176655,
      "grad_norm": 4.404504299163818,
      "learning_rate": 4.8708086964985284e-05,
      "loss": 0.8273,
      "step": 169900
    },
    {
      "epoch": 1.5512081173808308,
      "grad_norm": 4.302989959716797,
      "learning_rate": 4.870732656884931e-05,
      "loss": 0.7464,
      "step": 170000
    },
    {
      "epoch": 1.552120592743996,
      "grad_norm": 4.157317161560059,
      "learning_rate": 4.8706566172713344e-05,
      "loss": 0.7969,
      "step": 170100
    },
    {
      "epoch": 1.5530330681071611,
      "grad_norm": 4.748043060302734,
      "learning_rate": 4.870580577657737e-05,
      "loss": 0.7604,
      "step": 170200
    },
    {
      "epoch": 1.5539455434703262,
      "grad_norm": 4.344515800476074,
      "learning_rate": 4.87050453804414e-05,
      "loss": 0.7818,
      "step": 170300
    },
    {
      "epoch": 1.5548580188334915,
      "grad_norm": 4.324347972869873,
      "learning_rate": 4.870428498430543e-05,
      "loss": 0.8098,
      "step": 170400
    },
    {
      "epoch": 1.5557704941966568,
      "grad_norm": 4.063113212585449,
      "learning_rate": 4.870352458816945e-05,
      "loss": 0.8103,
      "step": 170500
    },
    {
      "epoch": 1.5566829695598219,
      "grad_norm": 4.949711322784424,
      "learning_rate": 4.870276419203348e-05,
      "loss": 0.7293,
      "step": 170600
    },
    {
      "epoch": 1.557595444922987,
      "grad_norm": 4.281140327453613,
      "learning_rate": 4.870200379589751e-05,
      "loss": 0.7689,
      "step": 170700
    },
    {
      "epoch": 1.5585079202861523,
      "grad_norm": 4.144218921661377,
      "learning_rate": 4.870124339976154e-05,
      "loss": 0.7913,
      "step": 170800
    },
    {
      "epoch": 1.5594203956493176,
      "grad_norm": 4.335687637329102,
      "learning_rate": 4.870048300362557e-05,
      "loss": 0.7846,
      "step": 170900
    },
    {
      "epoch": 1.5603328710124826,
      "grad_norm": 3.819957971572876,
      "learning_rate": 4.86997226074896e-05,
      "loss": 0.7775,
      "step": 171000
    },
    {
      "epoch": 1.5612453463756477,
      "grad_norm": 3.791224956512451,
      "learning_rate": 4.8698962211353625e-05,
      "loss": 0.8113,
      "step": 171100
    },
    {
      "epoch": 1.562157821738813,
      "grad_norm": 2.7187118530273438,
      "learning_rate": 4.869820181521766e-05,
      "loss": 0.769,
      "step": 171200
    },
    {
      "epoch": 1.5630702971019783,
      "grad_norm": 4.8502984046936035,
      "learning_rate": 4.8697441419081685e-05,
      "loss": 0.7953,
      "step": 171300
    },
    {
      "epoch": 1.5639827724651434,
      "grad_norm": 4.957902908325195,
      "learning_rate": 4.8696681022945715e-05,
      "loss": 0.8062,
      "step": 171400
    },
    {
      "epoch": 1.5648952478283087,
      "grad_norm": 3.8339121341705322,
      "learning_rate": 4.8695920626809745e-05,
      "loss": 0.7799,
      "step": 171500
    },
    {
      "epoch": 1.5658077231914738,
      "grad_norm": 4.643214702606201,
      "learning_rate": 4.8695160230673775e-05,
      "loss": 0.757,
      "step": 171600
    },
    {
      "epoch": 1.566720198554639,
      "grad_norm": 4.690615653991699,
      "learning_rate": 4.8694399834537805e-05,
      "loss": 0.7572,
      "step": 171700
    },
    {
      "epoch": 1.5676326739178044,
      "grad_norm": 3.7306923866271973,
      "learning_rate": 4.8693639438401835e-05,
      "loss": 0.7737,
      "step": 171800
    },
    {
      "epoch": 1.5685451492809694,
      "grad_norm": 5.071478366851807,
      "learning_rate": 4.869287904226586e-05,
      "loss": 0.7766,
      "step": 171900
    },
    {
      "epoch": 1.5694576246441345,
      "grad_norm": 4.4041314125061035,
      "learning_rate": 4.869211864612989e-05,
      "loss": 0.7695,
      "step": 172000
    },
    {
      "epoch": 1.5703701000072998,
      "grad_norm": 4.266735076904297,
      "learning_rate": 4.869135824999392e-05,
      "loss": 0.7642,
      "step": 172100
    },
    {
      "epoch": 1.5712825753704651,
      "grad_norm": 4.5303168296813965,
      "learning_rate": 4.869059785385795e-05,
      "loss": 0.8104,
      "step": 172200
    },
    {
      "epoch": 1.5721950507336302,
      "grad_norm": 4.018672943115234,
      "learning_rate": 4.868983745772198e-05,
      "loss": 0.7445,
      "step": 172300
    },
    {
      "epoch": 1.5731075260967953,
      "grad_norm": 4.909472942352295,
      "learning_rate": 4.868907706158601e-05,
      "loss": 0.7937,
      "step": 172400
    },
    {
      "epoch": 1.5740200014599606,
      "grad_norm": 4.179308891296387,
      "learning_rate": 4.868831666545003e-05,
      "loss": 0.7933,
      "step": 172500
    },
    {
      "epoch": 1.5749324768231259,
      "grad_norm": 4.271818161010742,
      "learning_rate": 4.868755626931407e-05,
      "loss": 0.7956,
      "step": 172600
    },
    {
      "epoch": 1.575844952186291,
      "grad_norm": 3.8624932765960693,
      "learning_rate": 4.868679587317809e-05,
      "loss": 0.7476,
      "step": 172700
    },
    {
      "epoch": 1.576757427549456,
      "grad_norm": 3.884460687637329,
      "learning_rate": 4.868603547704212e-05,
      "loss": 0.7682,
      "step": 172800
    },
    {
      "epoch": 1.5776699029126213,
      "grad_norm": 3.290437936782837,
      "learning_rate": 4.868527508090615e-05,
      "loss": 0.8238,
      "step": 172900
    },
    {
      "epoch": 1.5785823782757866,
      "grad_norm": 5.855055809020996,
      "learning_rate": 4.868451468477018e-05,
      "loss": 0.7757,
      "step": 173000
    },
    {
      "epoch": 1.5794948536389517,
      "grad_norm": 4.770255088806152,
      "learning_rate": 4.868375428863421e-05,
      "loss": 0.7875,
      "step": 173100
    },
    {
      "epoch": 1.5804073290021168,
      "grad_norm": 4.870233058929443,
      "learning_rate": 4.868299389249824e-05,
      "loss": 0.813,
      "step": 173200
    },
    {
      "epoch": 1.581319804365282,
      "grad_norm": 4.063090801239014,
      "learning_rate": 4.8682233496362266e-05,
      "loss": 0.7765,
      "step": 173300
    },
    {
      "epoch": 1.5822322797284474,
      "grad_norm": 4.303163528442383,
      "learning_rate": 4.8681473100226296e-05,
      "loss": 0.8091,
      "step": 173400
    },
    {
      "epoch": 1.5831447550916127,
      "grad_norm": 4.111211776733398,
      "learning_rate": 4.8680712704090326e-05,
      "loss": 0.7643,
      "step": 173500
    },
    {
      "epoch": 1.5840572304547778,
      "grad_norm": 4.300267696380615,
      "learning_rate": 4.867995230795435e-05,
      "loss": 0.7919,
      "step": 173600
    },
    {
      "epoch": 1.5849697058179428,
      "grad_norm": 4.014343738555908,
      "learning_rate": 4.8679191911818386e-05,
      "loss": 0.782,
      "step": 173700
    },
    {
      "epoch": 1.5858821811811081,
      "grad_norm": 4.826057434082031,
      "learning_rate": 4.867843151568241e-05,
      "loss": 0.8194,
      "step": 173800
    },
    {
      "epoch": 1.5867946565442734,
      "grad_norm": 3.469198703765869,
      "learning_rate": 4.867767111954644e-05,
      "loss": 0.7809,
      "step": 173900
    },
    {
      "epoch": 1.5877071319074385,
      "grad_norm": 4.044656276702881,
      "learning_rate": 4.867691072341047e-05,
      "loss": 0.7665,
      "step": 174000
    },
    {
      "epoch": 1.5886196072706036,
      "grad_norm": 5.249587535858154,
      "learning_rate": 4.86761503272745e-05,
      "loss": 0.7661,
      "step": 174100
    },
    {
      "epoch": 1.5895320826337689,
      "grad_norm": 4.577809810638428,
      "learning_rate": 4.867538993113853e-05,
      "loss": 0.8044,
      "step": 174200
    },
    {
      "epoch": 1.5904445579969342,
      "grad_norm": 4.020084857940674,
      "learning_rate": 4.867462953500256e-05,
      "loss": 0.7499,
      "step": 174300
    },
    {
      "epoch": 1.5913570333600993,
      "grad_norm": 3.3597524166107178,
      "learning_rate": 4.867386913886658e-05,
      "loss": 0.7694,
      "step": 174400
    },
    {
      "epoch": 1.5922695087232643,
      "grad_norm": 4.278686046600342,
      "learning_rate": 4.867310874273062e-05,
      "loss": 0.7626,
      "step": 174500
    },
    {
      "epoch": 1.5931819840864296,
      "grad_norm": 3.5081186294555664,
      "learning_rate": 4.867234834659464e-05,
      "loss": 0.7901,
      "step": 174600
    },
    {
      "epoch": 1.594094459449595,
      "grad_norm": 4.317670822143555,
      "learning_rate": 4.867158795045867e-05,
      "loss": 0.7536,
      "step": 174700
    },
    {
      "epoch": 1.59500693481276,
      "grad_norm": 4.025084972381592,
      "learning_rate": 4.86708275543227e-05,
      "loss": 0.7839,
      "step": 174800
    },
    {
      "epoch": 1.595919410175925,
      "grad_norm": 4.1290106773376465,
      "learning_rate": 4.867006715818673e-05,
      "loss": 0.7349,
      "step": 174900
    },
    {
      "epoch": 1.5968318855390904,
      "grad_norm": 4.037984848022461,
      "learning_rate": 4.8669306762050757e-05,
      "loss": 0.7874,
      "step": 175000
    },
    {
      "epoch": 1.5977443609022557,
      "grad_norm": 3.785217046737671,
      "learning_rate": 4.8668546365914793e-05,
      "loss": 0.7904,
      "step": 175100
    },
    {
      "epoch": 1.598656836265421,
      "grad_norm": 3.698913097381592,
      "learning_rate": 4.866778596977882e-05,
      "loss": 0.7712,
      "step": 175200
    },
    {
      "epoch": 1.599569311628586,
      "grad_norm": 4.164270401000977,
      "learning_rate": 4.866702557364285e-05,
      "loss": 0.7768,
      "step": 175300
    },
    {
      "epoch": 1.6004817869917511,
      "grad_norm": 4.014329433441162,
      "learning_rate": 4.866626517750688e-05,
      "loss": 0.7613,
      "step": 175400
    },
    {
      "epoch": 1.6013942623549164,
      "grad_norm": 3.627009868621826,
      "learning_rate": 4.866550478137091e-05,
      "loss": 0.7746,
      "step": 175500
    },
    {
      "epoch": 1.6023067377180817,
      "grad_norm": 3.480875015258789,
      "learning_rate": 4.866474438523494e-05,
      "loss": 0.7967,
      "step": 175600
    },
    {
      "epoch": 1.6032192130812468,
      "grad_norm": 4.0238118171691895,
      "learning_rate": 4.866398398909897e-05,
      "loss": 0.7194,
      "step": 175700
    },
    {
      "epoch": 1.604131688444412,
      "grad_norm": 4.351922988891602,
      "learning_rate": 4.866322359296299e-05,
      "loss": 0.7633,
      "step": 175800
    },
    {
      "epoch": 1.6050441638075772,
      "grad_norm": 5.235324382781982,
      "learning_rate": 4.866246319682703e-05,
      "loss": 0.7728,
      "step": 175900
    },
    {
      "epoch": 1.6059566391707425,
      "grad_norm": 4.255066394805908,
      "learning_rate": 4.866170280069105e-05,
      "loss": 0.8206,
      "step": 176000
    },
    {
      "epoch": 1.6068691145339076,
      "grad_norm": 4.4936089515686035,
      "learning_rate": 4.8660942404555074e-05,
      "loss": 0.784,
      "step": 176100
    },
    {
      "epoch": 1.6077815898970726,
      "grad_norm": 4.342044353485107,
      "learning_rate": 4.866018200841911e-05,
      "loss": 0.7802,
      "step": 176200
    },
    {
      "epoch": 1.608694065260238,
      "grad_norm": 3.9527769088745117,
      "learning_rate": 4.8659421612283134e-05,
      "loss": 0.7643,
      "step": 176300
    },
    {
      "epoch": 1.6096065406234032,
      "grad_norm": 3.876397132873535,
      "learning_rate": 4.8658661216147164e-05,
      "loss": 0.7605,
      "step": 176400
    },
    {
      "epoch": 1.6105190159865683,
      "grad_norm": 4.270368576049805,
      "learning_rate": 4.8657900820011194e-05,
      "loss": 0.7667,
      "step": 176500
    },
    {
      "epoch": 1.6114314913497334,
      "grad_norm": 4.872711658477783,
      "learning_rate": 4.8657140423875224e-05,
      "loss": 0.8315,
      "step": 176600
    },
    {
      "epoch": 1.6123439667128987,
      "grad_norm": 4.096817493438721,
      "learning_rate": 4.8656380027739254e-05,
      "loss": 0.8248,
      "step": 176700
    },
    {
      "epoch": 1.613256442076064,
      "grad_norm": 4.468338489532471,
      "learning_rate": 4.8655619631603284e-05,
      "loss": 0.7791,
      "step": 176800
    },
    {
      "epoch": 1.6141689174392293,
      "grad_norm": 5.450893878936768,
      "learning_rate": 4.865485923546731e-05,
      "loss": 0.7818,
      "step": 176900
    },
    {
      "epoch": 1.6150813928023944,
      "grad_norm": 4.7396063804626465,
      "learning_rate": 4.8654098839331344e-05,
      "loss": 0.7745,
      "step": 177000
    },
    {
      "epoch": 1.6159938681655595,
      "grad_norm": 3.637615919113159,
      "learning_rate": 4.865333844319537e-05,
      "loss": 0.722,
      "step": 177100
    },
    {
      "epoch": 1.6169063435287248,
      "grad_norm": 4.542927265167236,
      "learning_rate": 4.86525780470594e-05,
      "loss": 0.7558,
      "step": 177200
    },
    {
      "epoch": 1.61781881889189,
      "grad_norm": 4.680438995361328,
      "learning_rate": 4.865181765092343e-05,
      "loss": 0.8009,
      "step": 177300
    },
    {
      "epoch": 1.6187312942550551,
      "grad_norm": 4.966607570648193,
      "learning_rate": 4.865105725478746e-05,
      "loss": 0.7709,
      "step": 177400
    },
    {
      "epoch": 1.6196437696182202,
      "grad_norm": 4.172004222869873,
      "learning_rate": 4.865029685865148e-05,
      "loss": 0.7882,
      "step": 177500
    },
    {
      "epoch": 1.6205562449813855,
      "grad_norm": 3.4508585929870605,
      "learning_rate": 4.864953646251552e-05,
      "loss": 0.7372,
      "step": 177600
    },
    {
      "epoch": 1.6214687203445508,
      "grad_norm": 3.88975191116333,
      "learning_rate": 4.864877606637954e-05,
      "loss": 0.7742,
      "step": 177700
    },
    {
      "epoch": 1.6223811957077159,
      "grad_norm": 3.707305908203125,
      "learning_rate": 4.864801567024357e-05,
      "loss": 0.7456,
      "step": 177800
    },
    {
      "epoch": 1.623293671070881,
      "grad_norm": 3.3797500133514404,
      "learning_rate": 4.86472552741076e-05,
      "loss": 0.7942,
      "step": 177900
    },
    {
      "epoch": 1.6242061464340463,
      "grad_norm": 4.214262962341309,
      "learning_rate": 4.864649487797163e-05,
      "loss": 0.7769,
      "step": 178000
    },
    {
      "epoch": 1.6251186217972116,
      "grad_norm": 3.0772759914398193,
      "learning_rate": 4.864573448183566e-05,
      "loss": 0.7598,
      "step": 178100
    },
    {
      "epoch": 1.6260310971603766,
      "grad_norm": 4.47250509262085,
      "learning_rate": 4.864497408569969e-05,
      "loss": 0.7383,
      "step": 178200
    },
    {
      "epoch": 1.6269435725235417,
      "grad_norm": 4.640848636627197,
      "learning_rate": 4.8644213689563715e-05,
      "loss": 0.7757,
      "step": 178300
    },
    {
      "epoch": 1.627856047886707,
      "grad_norm": 3.8420190811157227,
      "learning_rate": 4.864345329342775e-05,
      "loss": 0.759,
      "step": 178400
    },
    {
      "epoch": 1.6287685232498723,
      "grad_norm": 4.364875793457031,
      "learning_rate": 4.8642692897291775e-05,
      "loss": 0.7917,
      "step": 178500
    },
    {
      "epoch": 1.6296809986130376,
      "grad_norm": 4.399286270141602,
      "learning_rate": 4.8641932501155805e-05,
      "loss": 0.7962,
      "step": 178600
    },
    {
      "epoch": 1.6305934739762027,
      "grad_norm": 3.106149435043335,
      "learning_rate": 4.8641172105019835e-05,
      "loss": 0.7781,
      "step": 178700
    },
    {
      "epoch": 1.6315059493393678,
      "grad_norm": 3.339730739593506,
      "learning_rate": 4.8640411708883865e-05,
      "loss": 0.7788,
      "step": 178800
    },
    {
      "epoch": 1.632418424702533,
      "grad_norm": 4.518789768218994,
      "learning_rate": 4.863965131274789e-05,
      "loss": 0.7675,
      "step": 178900
    },
    {
      "epoch": 1.6333309000656984,
      "grad_norm": 4.739353179931641,
      "learning_rate": 4.863889091661192e-05,
      "loss": 0.7822,
      "step": 179000
    },
    {
      "epoch": 1.6342433754288634,
      "grad_norm": 3.2505550384521484,
      "learning_rate": 4.863813052047595e-05,
      "loss": 0.7782,
      "step": 179100
    },
    {
      "epoch": 1.6351558507920285,
      "grad_norm": 4.300563812255859,
      "learning_rate": 4.863737012433998e-05,
      "loss": 0.7301,
      "step": 179200
    },
    {
      "epoch": 1.6360683261551938,
      "grad_norm": 4.239462852478027,
      "learning_rate": 4.863660972820401e-05,
      "loss": 0.7762,
      "step": 179300
    },
    {
      "epoch": 1.6369808015183591,
      "grad_norm": 3.900296926498413,
      "learning_rate": 4.863584933206803e-05,
      "loss": 0.8028,
      "step": 179400
    },
    {
      "epoch": 1.6378932768815242,
      "grad_norm": 3.768397331237793,
      "learning_rate": 4.863508893593207e-05,
      "loss": 0.7308,
      "step": 179500
    },
    {
      "epoch": 1.6388057522446893,
      "grad_norm": 4.191596031188965,
      "learning_rate": 4.863432853979609e-05,
      "loss": 0.7345,
      "step": 179600
    },
    {
      "epoch": 1.6397182276078546,
      "grad_norm": 4.57833194732666,
      "learning_rate": 4.863356814366012e-05,
      "loss": 0.7886,
      "step": 179700
    },
    {
      "epoch": 1.6406307029710199,
      "grad_norm": 3.5056517124176025,
      "learning_rate": 4.863280774752415e-05,
      "loss": 0.7952,
      "step": 179800
    },
    {
      "epoch": 1.641543178334185,
      "grad_norm": 3.5463876724243164,
      "learning_rate": 4.863204735138818e-05,
      "loss": 0.7715,
      "step": 179900
    },
    {
      "epoch": 1.64245565369735,
      "grad_norm": 3.954561471939087,
      "learning_rate": 4.8631286955252206e-05,
      "loss": 0.7897,
      "step": 180000
    },
    {
      "epoch": 1.6433681290605153,
      "grad_norm": 4.152578353881836,
      "learning_rate": 4.863052655911624e-05,
      "loss": 0.7513,
      "step": 180100
    },
    {
      "epoch": 1.6442806044236806,
      "grad_norm": 3.904780626296997,
      "learning_rate": 4.8629766162980266e-05,
      "loss": 0.7827,
      "step": 180200
    },
    {
      "epoch": 1.6451930797868457,
      "grad_norm": 3.911745548248291,
      "learning_rate": 4.8629005766844296e-05,
      "loss": 0.7433,
      "step": 180300
    },
    {
      "epoch": 1.646105555150011,
      "grad_norm": 4.27949333190918,
      "learning_rate": 4.8628245370708326e-05,
      "loss": 0.8007,
      "step": 180400
    },
    {
      "epoch": 1.647018030513176,
      "grad_norm": 4.659599781036377,
      "learning_rate": 4.8627484974572356e-05,
      "loss": 0.7982,
      "step": 180500
    },
    {
      "epoch": 1.6479305058763414,
      "grad_norm": 4.510316371917725,
      "learning_rate": 4.8626724578436386e-05,
      "loss": 0.7503,
      "step": 180600
    },
    {
      "epoch": 1.6488429812395067,
      "grad_norm": 4.95881986618042,
      "learning_rate": 4.8625964182300416e-05,
      "loss": 0.7727,
      "step": 180700
    },
    {
      "epoch": 1.6497554566026718,
      "grad_norm": 4.492867469787598,
      "learning_rate": 4.862520378616444e-05,
      "loss": 0.811,
      "step": 180800
    },
    {
      "epoch": 1.6506679319658368,
      "grad_norm": 4.9042768478393555,
      "learning_rate": 4.8624443390028476e-05,
      "loss": 0.782,
      "step": 180900
    },
    {
      "epoch": 1.6515804073290021,
      "grad_norm": 2.9453678131103516,
      "learning_rate": 4.86236829938925e-05,
      "loss": 0.7763,
      "step": 181000
    },
    {
      "epoch": 1.6524928826921674,
      "grad_norm": 4.772642612457275,
      "learning_rate": 4.862292259775653e-05,
      "loss": 0.823,
      "step": 181100
    },
    {
      "epoch": 1.6534053580553325,
      "grad_norm": 3.827670097351074,
      "learning_rate": 4.862216220162056e-05,
      "loss": 0.7468,
      "step": 181200
    },
    {
      "epoch": 1.6543178334184976,
      "grad_norm": 4.197908878326416,
      "learning_rate": 4.862140180548459e-05,
      "loss": 0.7376,
      "step": 181300
    },
    {
      "epoch": 1.6552303087816629,
      "grad_norm": 4.370494842529297,
      "learning_rate": 4.862064140934861e-05,
      "loss": 0.7303,
      "step": 181400
    },
    {
      "epoch": 1.6561427841448282,
      "grad_norm": 4.27587366104126,
      "learning_rate": 4.861988101321265e-05,
      "loss": 0.7748,
      "step": 181500
    },
    {
      "epoch": 1.6570552595079933,
      "grad_norm": 4.345036506652832,
      "learning_rate": 4.861912061707667e-05,
      "loss": 0.7878,
      "step": 181600
    },
    {
      "epoch": 1.6579677348711583,
      "grad_norm": 4.873326301574707,
      "learning_rate": 4.86183602209407e-05,
      "loss": 0.801,
      "step": 181700
    },
    {
      "epoch": 1.6588802102343236,
      "grad_norm": 3.608529567718506,
      "learning_rate": 4.861759982480473e-05,
      "loss": 0.8077,
      "step": 181800
    },
    {
      "epoch": 1.659792685597489,
      "grad_norm": 4.281148433685303,
      "learning_rate": 4.861683942866876e-05,
      "loss": 0.8065,
      "step": 181900
    },
    {
      "epoch": 1.660705160960654,
      "grad_norm": 5.230790138244629,
      "learning_rate": 4.8616079032532793e-05,
      "loss": 0.7668,
      "step": 182000
    },
    {
      "epoch": 1.661617636323819,
      "grad_norm": 5.617367744445801,
      "learning_rate": 4.861531863639682e-05,
      "loss": 0.7618,
      "step": 182100
    },
    {
      "epoch": 1.6625301116869844,
      "grad_norm": 3.364509344100952,
      "learning_rate": 4.861455824026085e-05,
      "loss": 0.7285,
      "step": 182200
    },
    {
      "epoch": 1.6634425870501497,
      "grad_norm": 4.344056129455566,
      "learning_rate": 4.861379784412488e-05,
      "loss": 0.7849,
      "step": 182300
    },
    {
      "epoch": 1.664355062413315,
      "grad_norm": 3.645261287689209,
      "learning_rate": 4.861303744798891e-05,
      "loss": 0.7872,
      "step": 182400
    },
    {
      "epoch": 1.66526753777648,
      "grad_norm": 4.784368515014648,
      "learning_rate": 4.861227705185293e-05,
      "loss": 0.7827,
      "step": 182500
    },
    {
      "epoch": 1.6661800131396451,
      "grad_norm": 4.703847408294678,
      "learning_rate": 4.861151665571697e-05,
      "loss": 0.7513,
      "step": 182600
    },
    {
      "epoch": 1.6670924885028104,
      "grad_norm": 4.828347682952881,
      "learning_rate": 4.861075625958099e-05,
      "loss": 0.7596,
      "step": 182700
    },
    {
      "epoch": 1.6680049638659757,
      "grad_norm": 5.811056137084961,
      "learning_rate": 4.860999586344502e-05,
      "loss": 0.7929,
      "step": 182800
    },
    {
      "epoch": 1.6689174392291408,
      "grad_norm": 4.265167713165283,
      "learning_rate": 4.860923546730905e-05,
      "loss": 0.7827,
      "step": 182900
    },
    {
      "epoch": 1.669829914592306,
      "grad_norm": 4.221785545349121,
      "learning_rate": 4.860847507117308e-05,
      "loss": 0.77,
      "step": 183000
    },
    {
      "epoch": 1.6707423899554712,
      "grad_norm": 3.4674081802368164,
      "learning_rate": 4.860771467503711e-05,
      "loss": 0.7504,
      "step": 183100
    },
    {
      "epoch": 1.6716548653186365,
      "grad_norm": 4.892886638641357,
      "learning_rate": 4.860695427890114e-05,
      "loss": 0.8108,
      "step": 183200
    },
    {
      "epoch": 1.6725673406818016,
      "grad_norm": 4.015299320220947,
      "learning_rate": 4.8606193882765164e-05,
      "loss": 0.8052,
      "step": 183300
    },
    {
      "epoch": 1.6734798160449667,
      "grad_norm": 4.5011396408081055,
      "learning_rate": 4.86054334866292e-05,
      "loss": 0.8062,
      "step": 183400
    },
    {
      "epoch": 1.674392291408132,
      "grad_norm": 4.001413345336914,
      "learning_rate": 4.8604673090493224e-05,
      "loss": 0.7281,
      "step": 183500
    },
    {
      "epoch": 1.6753047667712972,
      "grad_norm": 3.6031339168548584,
      "learning_rate": 4.8603912694357254e-05,
      "loss": 0.7806,
      "step": 183600
    },
    {
      "epoch": 1.6762172421344623,
      "grad_norm": 4.0488362312316895,
      "learning_rate": 4.8603152298221284e-05,
      "loss": 0.7746,
      "step": 183700
    },
    {
      "epoch": 1.6771297174976274,
      "grad_norm": 4.135134220123291,
      "learning_rate": 4.8602391902085314e-05,
      "loss": 0.7155,
      "step": 183800
    },
    {
      "epoch": 1.6780421928607927,
      "grad_norm": 3.724186658859253,
      "learning_rate": 4.860163150594934e-05,
      "loss": 0.7701,
      "step": 183900
    },
    {
      "epoch": 1.678954668223958,
      "grad_norm": 3.5028064250946045,
      "learning_rate": 4.8600871109813375e-05,
      "loss": 0.7514,
      "step": 184000
    },
    {
      "epoch": 1.6798671435871233,
      "grad_norm": 3.4924745559692383,
      "learning_rate": 4.86001107136774e-05,
      "loss": 0.7849,
      "step": 184100
    },
    {
      "epoch": 1.6807796189502884,
      "grad_norm": 5.188884258270264,
      "learning_rate": 4.859935031754143e-05,
      "loss": 0.7444,
      "step": 184200
    },
    {
      "epoch": 1.6816920943134535,
      "grad_norm": 5.228572368621826,
      "learning_rate": 4.859858992140546e-05,
      "loss": 0.8284,
      "step": 184300
    },
    {
      "epoch": 1.6826045696766188,
      "grad_norm": 3.765040636062622,
      "learning_rate": 4.859782952526949e-05,
      "loss": 0.758,
      "step": 184400
    },
    {
      "epoch": 1.683517045039784,
      "grad_norm": 3.5074408054351807,
      "learning_rate": 4.859706912913352e-05,
      "loss": 0.7914,
      "step": 184500
    },
    {
      "epoch": 1.6844295204029491,
      "grad_norm": 4.395068645477295,
      "learning_rate": 4.859630873299755e-05,
      "loss": 0.7631,
      "step": 184600
    },
    {
      "epoch": 1.6853419957661142,
      "grad_norm": 4.525357723236084,
      "learning_rate": 4.859554833686157e-05,
      "loss": 0.7661,
      "step": 184700
    },
    {
      "epoch": 1.6862544711292795,
      "grad_norm": 3.304682970046997,
      "learning_rate": 4.85947879407256e-05,
      "loss": 0.7753,
      "step": 184800
    },
    {
      "epoch": 1.6871669464924448,
      "grad_norm": 4.112278461456299,
      "learning_rate": 4.859402754458963e-05,
      "loss": 0.8116,
      "step": 184900
    },
    {
      "epoch": 1.6880794218556099,
      "grad_norm": 4.753178596496582,
      "learning_rate": 4.859326714845366e-05,
      "loss": 0.771,
      "step": 185000
    },
    {
      "epoch": 1.688991897218775,
      "grad_norm": 4.0662689208984375,
      "learning_rate": 4.859250675231769e-05,
      "loss": 0.8223,
      "step": 185100
    },
    {
      "epoch": 1.6899043725819403,
      "grad_norm": 3.2992875576019287,
      "learning_rate": 4.8591746356181715e-05,
      "loss": 0.8091,
      "step": 185200
    },
    {
      "epoch": 1.6908168479451056,
      "grad_norm": 3.57068133354187,
      "learning_rate": 4.8590985960045745e-05,
      "loss": 0.7854,
      "step": 185300
    },
    {
      "epoch": 1.6917293233082706,
      "grad_norm": 2.699918031692505,
      "learning_rate": 4.8590225563909775e-05,
      "loss": 0.7772,
      "step": 185400
    },
    {
      "epoch": 1.6926417986714357,
      "grad_norm": 3.653090476989746,
      "learning_rate": 4.8589465167773805e-05,
      "loss": 0.7737,
      "step": 185500
    },
    {
      "epoch": 1.693554274034601,
      "grad_norm": 4.239158630371094,
      "learning_rate": 4.8588704771637835e-05,
      "loss": 0.762,
      "step": 185600
    },
    {
      "epoch": 1.6944667493977663,
      "grad_norm": 5.076686382293701,
      "learning_rate": 4.8587944375501865e-05,
      "loss": 0.7366,
      "step": 185700
    },
    {
      "epoch": 1.6953792247609316,
      "grad_norm": 4.236702919006348,
      "learning_rate": 4.858718397936589e-05,
      "loss": 0.7733,
      "step": 185800
    },
    {
      "epoch": 1.6962917001240967,
      "grad_norm": 4.369009017944336,
      "learning_rate": 4.8586423583229925e-05,
      "loss": 0.7542,
      "step": 185900
    },
    {
      "epoch": 1.6972041754872618,
      "grad_norm": 4.5737128257751465,
      "learning_rate": 4.858566318709395e-05,
      "loss": 0.7712,
      "step": 186000
    },
    {
      "epoch": 1.698116650850427,
      "grad_norm": 4.9308180809021,
      "learning_rate": 4.858490279095798e-05,
      "loss": 0.772,
      "step": 186100
    },
    {
      "epoch": 1.6990291262135924,
      "grad_norm": 4.447093486785889,
      "learning_rate": 4.858414239482201e-05,
      "loss": 0.763,
      "step": 186200
    },
    {
      "epoch": 1.6999416015767574,
      "grad_norm": 4.254400253295898,
      "learning_rate": 4.858338199868604e-05,
      "loss": 0.7489,
      "step": 186300
    },
    {
      "epoch": 1.7008540769399225,
      "grad_norm": 3.6729860305786133,
      "learning_rate": 4.858262160255007e-05,
      "loss": 0.7693,
      "step": 186400
    },
    {
      "epoch": 1.7017665523030878,
      "grad_norm": 3.653297185897827,
      "learning_rate": 4.85818612064141e-05,
      "loss": 0.8308,
      "step": 186500
    },
    {
      "epoch": 1.7026790276662531,
      "grad_norm": 3.8551723957061768,
      "learning_rate": 4.858110081027812e-05,
      "loss": 0.8031,
      "step": 186600
    },
    {
      "epoch": 1.7035915030294182,
      "grad_norm": 3.9594192504882812,
      "learning_rate": 4.858034041414216e-05,
      "loss": 0.7551,
      "step": 186700
    },
    {
      "epoch": 1.7045039783925833,
      "grad_norm": 4.676926136016846,
      "learning_rate": 4.857958001800618e-05,
      "loss": 0.799,
      "step": 186800
    },
    {
      "epoch": 1.7054164537557486,
      "grad_norm": 4.889562606811523,
      "learning_rate": 4.857881962187021e-05,
      "loss": 0.7971,
      "step": 186900
    },
    {
      "epoch": 1.7063289291189139,
      "grad_norm": 3.9953665733337402,
      "learning_rate": 4.857805922573424e-05,
      "loss": 0.8043,
      "step": 187000
    },
    {
      "epoch": 1.707241404482079,
      "grad_norm": 4.683414936065674,
      "learning_rate": 4.857729882959827e-05,
      "loss": 0.7858,
      "step": 187100
    },
    {
      "epoch": 1.708153879845244,
      "grad_norm": 4.590930938720703,
      "learning_rate": 4.8576538433462296e-05,
      "loss": 0.7813,
      "step": 187200
    },
    {
      "epoch": 1.7090663552084093,
      "grad_norm": 5.690151214599609,
      "learning_rate": 4.857577803732633e-05,
      "loss": 0.7484,
      "step": 187300
    },
    {
      "epoch": 1.7099788305715746,
      "grad_norm": 4.6652679443359375,
      "learning_rate": 4.8575017641190356e-05,
      "loss": 0.7524,
      "step": 187400
    },
    {
      "epoch": 1.71089130593474,
      "grad_norm": 3.5975682735443115,
      "learning_rate": 4.8574257245054386e-05,
      "loss": 0.7641,
      "step": 187500
    },
    {
      "epoch": 1.711803781297905,
      "grad_norm": 4.000174045562744,
      "learning_rate": 4.8573496848918416e-05,
      "loss": 0.7662,
      "step": 187600
    },
    {
      "epoch": 1.71271625666107,
      "grad_norm": 4.637253761291504,
      "learning_rate": 4.857273645278244e-05,
      "loss": 0.7693,
      "step": 187700
    },
    {
      "epoch": 1.7136287320242354,
      "grad_norm": 4.353042125701904,
      "learning_rate": 4.8571976056646476e-05,
      "loss": 0.7971,
      "step": 187800
    },
    {
      "epoch": 1.7145412073874007,
      "grad_norm": 2.780911445617676,
      "learning_rate": 4.85712156605105e-05,
      "loss": 0.7819,
      "step": 187900
    },
    {
      "epoch": 1.7154536827505658,
      "grad_norm": 4.190706253051758,
      "learning_rate": 4.857045526437453e-05,
      "loss": 0.7711,
      "step": 188000
    },
    {
      "epoch": 1.7163661581137308,
      "grad_norm": 4.372349262237549,
      "learning_rate": 4.856969486823856e-05,
      "loss": 0.7937,
      "step": 188100
    },
    {
      "epoch": 1.7172786334768961,
      "grad_norm": 3.5433733463287354,
      "learning_rate": 4.856893447210259e-05,
      "loss": 0.7882,
      "step": 188200
    },
    {
      "epoch": 1.7181911088400614,
      "grad_norm": 4.001577854156494,
      "learning_rate": 4.856817407596661e-05,
      "loss": 0.7544,
      "step": 188300
    },
    {
      "epoch": 1.7191035842032265,
      "grad_norm": 4.430388927459717,
      "learning_rate": 4.856741367983065e-05,
      "loss": 0.7706,
      "step": 188400
    },
    {
      "epoch": 1.7200160595663916,
      "grad_norm": 3.9581079483032227,
      "learning_rate": 4.856665328369467e-05,
      "loss": 0.7715,
      "step": 188500
    },
    {
      "epoch": 1.7209285349295569,
      "grad_norm": 3.182965040206909,
      "learning_rate": 4.8565892887558703e-05,
      "loss": 0.7323,
      "step": 188600
    },
    {
      "epoch": 1.7218410102927222,
      "grad_norm": 4.404318332672119,
      "learning_rate": 4.8565132491422733e-05,
      "loss": 0.7603,
      "step": 188700
    },
    {
      "epoch": 1.7227534856558873,
      "grad_norm": 3.914802074432373,
      "learning_rate": 4.8564372095286764e-05,
      "loss": 0.7174,
      "step": 188800
    },
    {
      "epoch": 1.7236659610190523,
      "grad_norm": 4.18313455581665,
      "learning_rate": 4.8563611699150794e-05,
      "loss": 0.7712,
      "step": 188900
    },
    {
      "epoch": 1.7245784363822176,
      "grad_norm": 3.8836135864257812,
      "learning_rate": 4.8562851303014824e-05,
      "loss": 0.7386,
      "step": 189000
    },
    {
      "epoch": 1.725490911745383,
      "grad_norm": 4.794348239898682,
      "learning_rate": 4.856209090687885e-05,
      "loss": 0.7768,
      "step": 189100
    },
    {
      "epoch": 1.726403387108548,
      "grad_norm": 3.0948221683502197,
      "learning_rate": 4.8561330510742884e-05,
      "loss": 0.7889,
      "step": 189200
    },
    {
      "epoch": 1.7273158624717133,
      "grad_norm": 4.119332313537598,
      "learning_rate": 4.856057011460691e-05,
      "loss": 0.7635,
      "step": 189300
    },
    {
      "epoch": 1.7282283378348784,
      "grad_norm": 4.057365417480469,
      "learning_rate": 4.855980971847094e-05,
      "loss": 0.8139,
      "step": 189400
    },
    {
      "epoch": 1.7291408131980437,
      "grad_norm": 4.0968475341796875,
      "learning_rate": 4.855904932233497e-05,
      "loss": 0.7848,
      "step": 189500
    },
    {
      "epoch": 1.730053288561209,
      "grad_norm": 4.114497184753418,
      "learning_rate": 4.8558288926199e-05,
      "loss": 0.781,
      "step": 189600
    },
    {
      "epoch": 1.730965763924374,
      "grad_norm": 4.534153938293457,
      "learning_rate": 4.855752853006302e-05,
      "loss": 0.7929,
      "step": 189700
    },
    {
      "epoch": 1.7318782392875391,
      "grad_norm": 4.373645782470703,
      "learning_rate": 4.855676813392706e-05,
      "loss": 0.8326,
      "step": 189800
    },
    {
      "epoch": 1.7327907146507044,
      "grad_norm": 4.015073299407959,
      "learning_rate": 4.855600773779108e-05,
      "loss": 0.7511,
      "step": 189900
    },
    {
      "epoch": 1.7337031900138697,
      "grad_norm": 4.10088586807251,
      "learning_rate": 4.855524734165511e-05,
      "loss": 0.7858,
      "step": 190000
    },
    {
      "epoch": 1.7346156653770348,
      "grad_norm": 3.9724607467651367,
      "learning_rate": 4.855448694551914e-05,
      "loss": 0.7516,
      "step": 190100
    },
    {
      "epoch": 1.7355281407402,
      "grad_norm": 3.5365469455718994,
      "learning_rate": 4.855372654938317e-05,
      "loss": 0.7884,
      "step": 190200
    },
    {
      "epoch": 1.7364406161033652,
      "grad_norm": 4.695392608642578,
      "learning_rate": 4.85529661532472e-05,
      "loss": 0.8034,
      "step": 190300
    },
    {
      "epoch": 1.7373530914665305,
      "grad_norm": 3.658891201019287,
      "learning_rate": 4.8552205757111224e-05,
      "loss": 0.7594,
      "step": 190400
    },
    {
      "epoch": 1.7382655668296956,
      "grad_norm": 4.219161510467529,
      "learning_rate": 4.8551445360975254e-05,
      "loss": 0.7512,
      "step": 190500
    },
    {
      "epoch": 1.7391780421928607,
      "grad_norm": 4.456480026245117,
      "learning_rate": 4.8550684964839284e-05,
      "loss": 0.7527,
      "step": 190600
    },
    {
      "epoch": 1.740090517556026,
      "grad_norm": 4.1608710289001465,
      "learning_rate": 4.8549924568703314e-05,
      "loss": 0.7503,
      "step": 190700
    },
    {
      "epoch": 1.7410029929191913,
      "grad_norm": 3.3603460788726807,
      "learning_rate": 4.854916417256734e-05,
      "loss": 0.7849,
      "step": 190800
    },
    {
      "epoch": 1.7419154682823563,
      "grad_norm": 3.7923591136932373,
      "learning_rate": 4.8548403776431375e-05,
      "loss": 0.7482,
      "step": 190900
    },
    {
      "epoch": 1.7428279436455214,
      "grad_norm": 4.410406589508057,
      "learning_rate": 4.85476433802954e-05,
      "loss": 0.7644,
      "step": 191000
    },
    {
      "epoch": 1.7437404190086867,
      "grad_norm": 3.9388134479522705,
      "learning_rate": 4.854688298415943e-05,
      "loss": 0.7396,
      "step": 191100
    },
    {
      "epoch": 1.744652894371852,
      "grad_norm": 4.430976867675781,
      "learning_rate": 4.854612258802346e-05,
      "loss": 0.8083,
      "step": 191200
    },
    {
      "epoch": 1.7455653697350173,
      "grad_norm": 4.500326156616211,
      "learning_rate": 4.854536219188749e-05,
      "loss": 0.7069,
      "step": 191300
    },
    {
      "epoch": 1.7464778450981824,
      "grad_norm": 4.787961959838867,
      "learning_rate": 4.854460179575152e-05,
      "loss": 0.7788,
      "step": 191400
    },
    {
      "epoch": 1.7473903204613475,
      "grad_norm": 5.616637706756592,
      "learning_rate": 4.854384139961555e-05,
      "loss": 0.8094,
      "step": 191500
    },
    {
      "epoch": 1.7483027958245128,
      "grad_norm": 4.4588727951049805,
      "learning_rate": 4.854308100347957e-05,
      "loss": 0.7702,
      "step": 191600
    },
    {
      "epoch": 1.749215271187678,
      "grad_norm": 3.992840528488159,
      "learning_rate": 4.854232060734361e-05,
      "loss": 0.7529,
      "step": 191700
    },
    {
      "epoch": 1.7501277465508431,
      "grad_norm": 4.490670204162598,
      "learning_rate": 4.854156021120763e-05,
      "loss": 0.8258,
      "step": 191800
    },
    {
      "epoch": 1.7510402219140082,
      "grad_norm": 4.451484203338623,
      "learning_rate": 4.854079981507166e-05,
      "loss": 0.7491,
      "step": 191900
    },
    {
      "epoch": 1.7519526972771735,
      "grad_norm": 3.1273655891418457,
      "learning_rate": 4.854003941893569e-05,
      "loss": 0.7702,
      "step": 192000
    },
    {
      "epoch": 1.7528651726403388,
      "grad_norm": 4.071371078491211,
      "learning_rate": 4.853927902279972e-05,
      "loss": 0.7618,
      "step": 192100
    },
    {
      "epoch": 1.7537776480035039,
      "grad_norm": 4.3319783210754395,
      "learning_rate": 4.8538518626663745e-05,
      "loss": 0.7393,
      "step": 192200
    },
    {
      "epoch": 1.754690123366669,
      "grad_norm": 4.274696350097656,
      "learning_rate": 4.853775823052778e-05,
      "loss": 0.7744,
      "step": 192300
    },
    {
      "epoch": 1.7556025987298343,
      "grad_norm": 4.052804946899414,
      "learning_rate": 4.8536997834391805e-05,
      "loss": 0.7949,
      "step": 192400
    },
    {
      "epoch": 1.7565150740929996,
      "grad_norm": 4.5211005210876465,
      "learning_rate": 4.8536237438255835e-05,
      "loss": 0.7704,
      "step": 192500
    },
    {
      "epoch": 1.7574275494561646,
      "grad_norm": 4.32009744644165,
      "learning_rate": 4.8535477042119865e-05,
      "loss": 0.7227,
      "step": 192600
    },
    {
      "epoch": 1.7583400248193297,
      "grad_norm": 4.185928821563721,
      "learning_rate": 4.8534716645983895e-05,
      "loss": 0.7783,
      "step": 192700
    },
    {
      "epoch": 1.759252500182495,
      "grad_norm": 4.221878528594971,
      "learning_rate": 4.8533956249847926e-05,
      "loss": 0.7964,
      "step": 192800
    },
    {
      "epoch": 1.7601649755456603,
      "grad_norm": 4.500560283660889,
      "learning_rate": 4.8533195853711956e-05,
      "loss": 0.772,
      "step": 192900
    },
    {
      "epoch": 1.7610774509088256,
      "grad_norm": 4.353584289550781,
      "learning_rate": 4.853243545757598e-05,
      "loss": 0.7932,
      "step": 193000
    },
    {
      "epoch": 1.7619899262719907,
      "grad_norm": 4.396876335144043,
      "learning_rate": 4.8531675061440016e-05,
      "loss": 0.7113,
      "step": 193100
    },
    {
      "epoch": 1.7629024016351558,
      "grad_norm": 4.2662882804870605,
      "learning_rate": 4.853091466530404e-05,
      "loss": 0.7215,
      "step": 193200
    },
    {
      "epoch": 1.763814876998321,
      "grad_norm": 4.3764543533325195,
      "learning_rate": 4.853015426916806e-05,
      "loss": 0.8,
      "step": 193300
    },
    {
      "epoch": 1.7647273523614864,
      "grad_norm": 4.28441047668457,
      "learning_rate": 4.85293938730321e-05,
      "loss": 0.7779,
      "step": 193400
    },
    {
      "epoch": 1.7656398277246514,
      "grad_norm": 3.719036817550659,
      "learning_rate": 4.852863347689612e-05,
      "loss": 0.8031,
      "step": 193500
    },
    {
      "epoch": 1.7665523030878165,
      "grad_norm": 3.9463679790496826,
      "learning_rate": 4.852787308076015e-05,
      "loss": 0.7913,
      "step": 193600
    },
    {
      "epoch": 1.7674647784509818,
      "grad_norm": 4.446022987365723,
      "learning_rate": 4.852711268462418e-05,
      "loss": 0.7557,
      "step": 193700
    },
    {
      "epoch": 1.7683772538141471,
      "grad_norm": 3.962153196334839,
      "learning_rate": 4.852635228848821e-05,
      "loss": 0.7743,
      "step": 193800
    },
    {
      "epoch": 1.7692897291773122,
      "grad_norm": 4.742186546325684,
      "learning_rate": 4.852559189235224e-05,
      "loss": 0.7596,
      "step": 193900
    },
    {
      "epoch": 1.7702022045404773,
      "grad_norm": 4.767083644866943,
      "learning_rate": 4.852483149621627e-05,
      "loss": 0.7883,
      "step": 194000
    },
    {
      "epoch": 1.7711146799036426,
      "grad_norm": 4.30496883392334,
      "learning_rate": 4.8524071100080296e-05,
      "loss": 0.7865,
      "step": 194100
    },
    {
      "epoch": 1.7720271552668079,
      "grad_norm": 4.288464069366455,
      "learning_rate": 4.852331070394433e-05,
      "loss": 0.7787,
      "step": 194200
    },
    {
      "epoch": 1.772939630629973,
      "grad_norm": 4.335844039916992,
      "learning_rate": 4.8522550307808356e-05,
      "loss": 0.7746,
      "step": 194300
    },
    {
      "epoch": 1.773852105993138,
      "grad_norm": 4.17941951751709,
      "learning_rate": 4.8521789911672386e-05,
      "loss": 0.7854,
      "step": 194400
    },
    {
      "epoch": 1.7747645813563033,
      "grad_norm": 2.306988477706909,
      "learning_rate": 4.8521029515536416e-05,
      "loss": 0.7376,
      "step": 194500
    },
    {
      "epoch": 1.7756770567194686,
      "grad_norm": 4.52384614944458,
      "learning_rate": 4.8520269119400446e-05,
      "loss": 0.8092,
      "step": 194600
    },
    {
      "epoch": 1.776589532082634,
      "grad_norm": 4.285627365112305,
      "learning_rate": 4.851950872326447e-05,
      "loss": 0.773,
      "step": 194700
    },
    {
      "epoch": 1.777502007445799,
      "grad_norm": 3.77319073677063,
      "learning_rate": 4.8518748327128507e-05,
      "loss": 0.7512,
      "step": 194800
    },
    {
      "epoch": 1.778414482808964,
      "grad_norm": 3.890448808670044,
      "learning_rate": 4.851798793099253e-05,
      "loss": 0.7599,
      "step": 194900
    },
    {
      "epoch": 1.7793269581721294,
      "grad_norm": 4.7139668464660645,
      "learning_rate": 4.851722753485656e-05,
      "loss": 0.7693,
      "step": 195000
    },
    {
      "epoch": 1.7802394335352947,
      "grad_norm": 3.949528932571411,
      "learning_rate": 4.851646713872059e-05,
      "loss": 0.7896,
      "step": 195100
    },
    {
      "epoch": 1.7811519088984598,
      "grad_norm": 4.75702428817749,
      "learning_rate": 4.851570674258462e-05,
      "loss": 0.8098,
      "step": 195200
    },
    {
      "epoch": 1.7820643842616248,
      "grad_norm": 4.496127605438232,
      "learning_rate": 4.851494634644865e-05,
      "loss": 0.8022,
      "step": 195300
    },
    {
      "epoch": 1.7829768596247901,
      "grad_norm": 4.62028694152832,
      "learning_rate": 4.851418595031268e-05,
      "loss": 0.7908,
      "step": 195400
    },
    {
      "epoch": 1.7838893349879554,
      "grad_norm": 4.121727466583252,
      "learning_rate": 4.8513425554176703e-05,
      "loss": 0.7464,
      "step": 195500
    },
    {
      "epoch": 1.7848018103511205,
      "grad_norm": 4.555670738220215,
      "learning_rate": 4.851266515804074e-05,
      "loss": 0.7488,
      "step": 195600
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 4.341109275817871,
      "learning_rate": 4.8511904761904764e-05,
      "loss": 0.7169,
      "step": 195700
    },
    {
      "epoch": 1.7866267610774509,
      "grad_norm": 3.502483606338501,
      "learning_rate": 4.8511144365768794e-05,
      "loss": 0.7248,
      "step": 195800
    },
    {
      "epoch": 1.7875392364406162,
      "grad_norm": 4.0845866203308105,
      "learning_rate": 4.8510383969632824e-05,
      "loss": 0.7454,
      "step": 195900
    },
    {
      "epoch": 1.7884517118037813,
      "grad_norm": 3.7164840698242188,
      "learning_rate": 4.850962357349685e-05,
      "loss": 0.7731,
      "step": 196000
    },
    {
      "epoch": 1.7893641871669463,
      "grad_norm": 4.814009666442871,
      "learning_rate": 4.850886317736088e-05,
      "loss": 0.8292,
      "step": 196100
    },
    {
      "epoch": 1.7902766625301116,
      "grad_norm": 3.9827423095703125,
      "learning_rate": 4.850810278122491e-05,
      "loss": 0.7855,
      "step": 196200
    },
    {
      "epoch": 1.791189137893277,
      "grad_norm": 4.588788986206055,
      "learning_rate": 4.850734238508894e-05,
      "loss": 0.7671,
      "step": 196300
    },
    {
      "epoch": 1.7921016132564422,
      "grad_norm": 4.850348949432373,
      "learning_rate": 4.850658198895297e-05,
      "loss": 0.7975,
      "step": 196400
    },
    {
      "epoch": 1.7930140886196073,
      "grad_norm": 3.127401113510132,
      "learning_rate": 4.8505821592817e-05,
      "loss": 0.7596,
      "step": 196500
    },
    {
      "epoch": 1.7939265639827724,
      "grad_norm": 5.051571369171143,
      "learning_rate": 4.850506119668102e-05,
      "loss": 0.7661,
      "step": 196600
    },
    {
      "epoch": 1.7948390393459377,
      "grad_norm": 4.490049839019775,
      "learning_rate": 4.850430080054506e-05,
      "loss": 0.7689,
      "step": 196700
    },
    {
      "epoch": 1.795751514709103,
      "grad_norm": 2.993772029876709,
      "learning_rate": 4.850354040440908e-05,
      "loss": 0.7504,
      "step": 196800
    },
    {
      "epoch": 1.796663990072268,
      "grad_norm": 4.157618999481201,
      "learning_rate": 4.850278000827311e-05,
      "loss": 0.7601,
      "step": 196900
    },
    {
      "epoch": 1.7975764654354331,
      "grad_norm": 3.284390926361084,
      "learning_rate": 4.850201961213714e-05,
      "loss": 0.7451,
      "step": 197000
    },
    {
      "epoch": 1.7984889407985984,
      "grad_norm": 4.53759765625,
      "learning_rate": 4.850125921600117e-05,
      "loss": 0.7865,
      "step": 197100
    },
    {
      "epoch": 1.7994014161617637,
      "grad_norm": 3.7963576316833496,
      "learning_rate": 4.85004988198652e-05,
      "loss": 0.7558,
      "step": 197200
    },
    {
      "epoch": 1.8003138915249288,
      "grad_norm": 3.843888282775879,
      "learning_rate": 4.849973842372923e-05,
      "loss": 0.79,
      "step": 197300
    },
    {
      "epoch": 1.801226366888094,
      "grad_norm": 4.896747589111328,
      "learning_rate": 4.8498978027593254e-05,
      "loss": 0.7607,
      "step": 197400
    },
    {
      "epoch": 1.8021388422512592,
      "grad_norm": 4.153625011444092,
      "learning_rate": 4.8498217631457285e-05,
      "loss": 0.813,
      "step": 197500
    },
    {
      "epoch": 1.8030513176144245,
      "grad_norm": 5.053310871124268,
      "learning_rate": 4.8497457235321315e-05,
      "loss": 0.7516,
      "step": 197600
    },
    {
      "epoch": 1.8039637929775896,
      "grad_norm": 4.739261627197266,
      "learning_rate": 4.8496696839185345e-05,
      "loss": 0.7693,
      "step": 197700
    },
    {
      "epoch": 1.8048762683407547,
      "grad_norm": 3.8724863529205322,
      "learning_rate": 4.8495936443049375e-05,
      "loss": 0.7736,
      "step": 197800
    },
    {
      "epoch": 1.80578874370392,
      "grad_norm": 4.459965229034424,
      "learning_rate": 4.8495176046913405e-05,
      "loss": 0.7648,
      "step": 197900
    },
    {
      "epoch": 1.8067012190670853,
      "grad_norm": 4.385801792144775,
      "learning_rate": 4.849441565077743e-05,
      "loss": 0.7878,
      "step": 198000
    },
    {
      "epoch": 1.8076136944302503,
      "grad_norm": 4.9018025398254395,
      "learning_rate": 4.8493655254641465e-05,
      "loss": 0.7747,
      "step": 198100
    },
    {
      "epoch": 1.8085261697934156,
      "grad_norm": 3.9980862140655518,
      "learning_rate": 4.849289485850549e-05,
      "loss": 0.7931,
      "step": 198200
    },
    {
      "epoch": 1.8094386451565807,
      "grad_norm": 3.911046028137207,
      "learning_rate": 4.849213446236952e-05,
      "loss": 0.7532,
      "step": 198300
    },
    {
      "epoch": 1.810351120519746,
      "grad_norm": 3.7016549110412598,
      "learning_rate": 4.849137406623355e-05,
      "loss": 0.7944,
      "step": 198400
    },
    {
      "epoch": 1.8112635958829113,
      "grad_norm": 4.7174601554870605,
      "learning_rate": 4.849061367009758e-05,
      "loss": 0.7509,
      "step": 198500
    },
    {
      "epoch": 1.8121760712460764,
      "grad_norm": 4.310943603515625,
      "learning_rate": 4.848985327396161e-05,
      "loss": 0.7938,
      "step": 198600
    },
    {
      "epoch": 1.8130885466092415,
      "grad_norm": 3.550356864929199,
      "learning_rate": 4.848909287782564e-05,
      "loss": 0.7758,
      "step": 198700
    },
    {
      "epoch": 1.8140010219724068,
      "grad_norm": 4.351792812347412,
      "learning_rate": 4.848833248168966e-05,
      "loss": 0.7961,
      "step": 198800
    },
    {
      "epoch": 1.814913497335572,
      "grad_norm": 6.162627220153809,
      "learning_rate": 4.848757208555369e-05,
      "loss": 0.7394,
      "step": 198900
    },
    {
      "epoch": 1.8158259726987371,
      "grad_norm": 4.719892978668213,
      "learning_rate": 4.848681168941772e-05,
      "loss": 0.7594,
      "step": 199000
    },
    {
      "epoch": 1.8167384480619022,
      "grad_norm": 4.561456680297852,
      "learning_rate": 4.8486051293281745e-05,
      "loss": 0.7655,
      "step": 199100
    },
    {
      "epoch": 1.8176509234250675,
      "grad_norm": 3.702491044998169,
      "learning_rate": 4.848529089714578e-05,
      "loss": 0.7875,
      "step": 199200
    },
    {
      "epoch": 1.8185633987882328,
      "grad_norm": 4.5084428787231445,
      "learning_rate": 4.8484530501009805e-05,
      "loss": 0.8021,
      "step": 199300
    },
    {
      "epoch": 1.8194758741513979,
      "grad_norm": 3.400912046432495,
      "learning_rate": 4.8483770104873835e-05,
      "loss": 0.7783,
      "step": 199400
    },
    {
      "epoch": 1.820388349514563,
      "grad_norm": 4.438042640686035,
      "learning_rate": 4.8483009708737866e-05,
      "loss": 0.7685,
      "step": 199500
    },
    {
      "epoch": 1.8213008248777283,
      "grad_norm": 5.069023132324219,
      "learning_rate": 4.8482249312601896e-05,
      "loss": 0.7604,
      "step": 199600
    },
    {
      "epoch": 1.8222133002408936,
      "grad_norm": 3.8963332176208496,
      "learning_rate": 4.8481488916465926e-05,
      "loss": 0.7625,
      "step": 199700
    },
    {
      "epoch": 1.8231257756040586,
      "grad_norm": 4.267152309417725,
      "learning_rate": 4.8480728520329956e-05,
      "loss": 0.7711,
      "step": 199800
    },
    {
      "epoch": 1.824038250967224,
      "grad_norm": 3.666700601577759,
      "learning_rate": 4.847996812419398e-05,
      "loss": 0.7691,
      "step": 199900
    },
    {
      "epoch": 1.824950726330389,
      "grad_norm": 5.513006687164307,
      "learning_rate": 4.8479207728058016e-05,
      "loss": 0.7687,
      "step": 200000
    },
    {
      "epoch": 1.8258632016935543,
      "grad_norm": 4.308884620666504,
      "learning_rate": 4.847844733192204e-05,
      "loss": 0.7807,
      "step": 200100
    },
    {
      "epoch": 1.8267756770567196,
      "grad_norm": 4.460080623626709,
      "learning_rate": 4.847768693578607e-05,
      "loss": 0.7817,
      "step": 200200
    },
    {
      "epoch": 1.8276881524198847,
      "grad_norm": 4.342499256134033,
      "learning_rate": 4.84769265396501e-05,
      "loss": 0.7941,
      "step": 200300
    },
    {
      "epoch": 1.8286006277830498,
      "grad_norm": 3.821305513381958,
      "learning_rate": 4.847616614351413e-05,
      "loss": 0.7869,
      "step": 200400
    },
    {
      "epoch": 1.829513103146215,
      "grad_norm": 4.388462543487549,
      "learning_rate": 4.847540574737815e-05,
      "loss": 0.7931,
      "step": 200500
    },
    {
      "epoch": 1.8304255785093804,
      "grad_norm": 4.2959513664245605,
      "learning_rate": 4.847464535124219e-05,
      "loss": 0.7388,
      "step": 200600
    },
    {
      "epoch": 1.8313380538725454,
      "grad_norm": 4.476956367492676,
      "learning_rate": 4.847388495510621e-05,
      "loss": 0.7352,
      "step": 200700
    },
    {
      "epoch": 1.8322505292357105,
      "grad_norm": 4.059482097625732,
      "learning_rate": 4.847312455897024e-05,
      "loss": 0.7774,
      "step": 200800
    },
    {
      "epoch": 1.8331630045988758,
      "grad_norm": 5.4630632400512695,
      "learning_rate": 4.847236416283427e-05,
      "loss": 0.7668,
      "step": 200900
    },
    {
      "epoch": 1.8340754799620411,
      "grad_norm": 3.3133137226104736,
      "learning_rate": 4.84716037666983e-05,
      "loss": 0.7754,
      "step": 201000
    },
    {
      "epoch": 1.8349879553252062,
      "grad_norm": 4.1131486892700195,
      "learning_rate": 4.847084337056233e-05,
      "loss": 0.761,
      "step": 201100
    },
    {
      "epoch": 1.8359004306883713,
      "grad_norm": 4.118827819824219,
      "learning_rate": 4.847008297442636e-05,
      "loss": 0.7364,
      "step": 201200
    },
    {
      "epoch": 1.8368129060515366,
      "grad_norm": 3.5894973278045654,
      "learning_rate": 4.8469322578290386e-05,
      "loss": 0.7455,
      "step": 201300
    },
    {
      "epoch": 1.8377253814147019,
      "grad_norm": 4.038202285766602,
      "learning_rate": 4.846856218215442e-05,
      "loss": 0.8166,
      "step": 201400
    },
    {
      "epoch": 1.838637856777867,
      "grad_norm": 3.9667465686798096,
      "learning_rate": 4.8467801786018447e-05,
      "loss": 0.7796,
      "step": 201500
    },
    {
      "epoch": 1.839550332141032,
      "grad_norm": 3.6296701431274414,
      "learning_rate": 4.8467041389882477e-05,
      "loss": 0.7742,
      "step": 201600
    },
    {
      "epoch": 1.8404628075041973,
      "grad_norm": 4.789882183074951,
      "learning_rate": 4.846628099374651e-05,
      "loss": 0.7781,
      "step": 201700
    },
    {
      "epoch": 1.8413752828673626,
      "grad_norm": 5.097433567047119,
      "learning_rate": 4.846552059761053e-05,
      "loss": 0.8,
      "step": 201800
    },
    {
      "epoch": 1.842287758230528,
      "grad_norm": 3.6722164154052734,
      "learning_rate": 4.846476020147456e-05,
      "loss": 0.7697,
      "step": 201900
    },
    {
      "epoch": 1.843200233593693,
      "grad_norm": 3.6377384662628174,
      "learning_rate": 4.846399980533859e-05,
      "loss": 0.7547,
      "step": 202000
    },
    {
      "epoch": 1.844112708956858,
      "grad_norm": 4.719507217407227,
      "learning_rate": 4.846323940920262e-05,
      "loss": 0.7916,
      "step": 202100
    },
    {
      "epoch": 1.8450251843200234,
      "grad_norm": 4.473559856414795,
      "learning_rate": 4.846247901306665e-05,
      "loss": 0.7601,
      "step": 202200
    },
    {
      "epoch": 1.8459376596831887,
      "grad_norm": 4.316589832305908,
      "learning_rate": 4.846171861693068e-05,
      "loss": 0.7786,
      "step": 202300
    },
    {
      "epoch": 1.8468501350463538,
      "grad_norm": 1.9613014459609985,
      "learning_rate": 4.8460958220794704e-05,
      "loss": 0.7456,
      "step": 202400
    },
    {
      "epoch": 1.8477626104095188,
      "grad_norm": 4.2635345458984375,
      "learning_rate": 4.846019782465874e-05,
      "loss": 0.7604,
      "step": 202500
    },
    {
      "epoch": 1.8486750857726841,
      "grad_norm": 3.408217430114746,
      "learning_rate": 4.8459437428522764e-05,
      "loss": 0.769,
      "step": 202600
    },
    {
      "epoch": 1.8495875611358494,
      "grad_norm": 4.969542026519775,
      "learning_rate": 4.8458677032386794e-05,
      "loss": 0.7663,
      "step": 202700
    },
    {
      "epoch": 1.8505000364990145,
      "grad_norm": 4.401144981384277,
      "learning_rate": 4.8457916636250824e-05,
      "loss": 0.7795,
      "step": 202800
    },
    {
      "epoch": 1.8514125118621796,
      "grad_norm": 4.447337627410889,
      "learning_rate": 4.8457156240114854e-05,
      "loss": 0.8179,
      "step": 202900
    },
    {
      "epoch": 1.8523249872253449,
      "grad_norm": 3.5918617248535156,
      "learning_rate": 4.845639584397888e-05,
      "loss": 0.775,
      "step": 203000
    },
    {
      "epoch": 1.8532374625885102,
      "grad_norm": 4.3295512199401855,
      "learning_rate": 4.8455635447842914e-05,
      "loss": 0.7547,
      "step": 203100
    },
    {
      "epoch": 1.8541499379516753,
      "grad_norm": 4.177255153656006,
      "learning_rate": 4.845487505170694e-05,
      "loss": 0.764,
      "step": 203200
    },
    {
      "epoch": 1.8550624133148403,
      "grad_norm": 4.826287746429443,
      "learning_rate": 4.845411465557097e-05,
      "loss": 0.7789,
      "step": 203300
    },
    {
      "epoch": 1.8559748886780056,
      "grad_norm": 3.7381813526153564,
      "learning_rate": 4.8453354259435e-05,
      "loss": 0.7571,
      "step": 203400
    },
    {
      "epoch": 1.856887364041171,
      "grad_norm": 3.936962366104126,
      "learning_rate": 4.845259386329903e-05,
      "loss": 0.7464,
      "step": 203500
    },
    {
      "epoch": 1.8577998394043362,
      "grad_norm": 4.420234203338623,
      "learning_rate": 4.845183346716306e-05,
      "loss": 0.7706,
      "step": 203600
    },
    {
      "epoch": 1.8587123147675013,
      "grad_norm": 3.6687915325164795,
      "learning_rate": 4.845107307102709e-05,
      "loss": 0.7447,
      "step": 203700
    },
    {
      "epoch": 1.8596247901306664,
      "grad_norm": 3.706757068634033,
      "learning_rate": 4.845031267489111e-05,
      "loss": 0.8009,
      "step": 203800
    },
    {
      "epoch": 1.8605372654938317,
      "grad_norm": 4.339391708374023,
      "learning_rate": 4.844955227875515e-05,
      "loss": 0.7543,
      "step": 203900
    },
    {
      "epoch": 1.861449740856997,
      "grad_norm": 4.112888336181641,
      "learning_rate": 4.844879188261917e-05,
      "loss": 0.7738,
      "step": 204000
    },
    {
      "epoch": 1.862362216220162,
      "grad_norm": 4.4160261154174805,
      "learning_rate": 4.84480314864832e-05,
      "loss": 0.7742,
      "step": 204100
    },
    {
      "epoch": 1.8632746915833271,
      "grad_norm": 4.439382076263428,
      "learning_rate": 4.844727109034723e-05,
      "loss": 0.7361,
      "step": 204200
    },
    {
      "epoch": 1.8641871669464924,
      "grad_norm": 3.811286687850952,
      "learning_rate": 4.844651069421126e-05,
      "loss": 0.802,
      "step": 204300
    },
    {
      "epoch": 1.8650996423096577,
      "grad_norm": 4.918125152587891,
      "learning_rate": 4.8445750298075285e-05,
      "loss": 0.7522,
      "step": 204400
    },
    {
      "epoch": 1.8660121176728228,
      "grad_norm": 4.5052385330200195,
      "learning_rate": 4.8444989901939315e-05,
      "loss": 0.7449,
      "step": 204500
    },
    {
      "epoch": 1.866924593035988,
      "grad_norm": 3.922494411468506,
      "learning_rate": 4.8444229505803345e-05,
      "loss": 0.7274,
      "step": 204600
    },
    {
      "epoch": 1.8678370683991532,
      "grad_norm": 3.1211013793945312,
      "learning_rate": 4.8443469109667375e-05,
      "loss": 0.7672,
      "step": 204700
    },
    {
      "epoch": 1.8687495437623185,
      "grad_norm": 4.068546772003174,
      "learning_rate": 4.8442708713531405e-05,
      "loss": 0.7944,
      "step": 204800
    },
    {
      "epoch": 1.8696620191254836,
      "grad_norm": 4.661820888519287,
      "learning_rate": 4.844194831739543e-05,
      "loss": 0.7276,
      "step": 204900
    },
    {
      "epoch": 1.8705744944886487,
      "grad_norm": 4.342263698577881,
      "learning_rate": 4.8441187921259465e-05,
      "loss": 0.7931,
      "step": 205000
    },
    {
      "epoch": 1.871486969851814,
      "grad_norm": 4.347567081451416,
      "learning_rate": 4.844042752512349e-05,
      "loss": 0.7912,
      "step": 205100
    },
    {
      "epoch": 1.8723994452149793,
      "grad_norm": 3.625201940536499,
      "learning_rate": 4.843966712898752e-05,
      "loss": 0.7439,
      "step": 205200
    },
    {
      "epoch": 1.8733119205781446,
      "grad_norm": 4.458436965942383,
      "learning_rate": 4.843890673285155e-05,
      "loss": 0.773,
      "step": 205300
    },
    {
      "epoch": 1.8742243959413096,
      "grad_norm": 4.340516090393066,
      "learning_rate": 4.843814633671558e-05,
      "loss": 0.7653,
      "step": 205400
    },
    {
      "epoch": 1.8751368713044747,
      "grad_norm": 4.580911636352539,
      "learning_rate": 4.84373859405796e-05,
      "loss": 0.7364,
      "step": 205500
    },
    {
      "epoch": 1.87604934666764,
      "grad_norm": 3.984591007232666,
      "learning_rate": 4.843662554444364e-05,
      "loss": 0.7809,
      "step": 205600
    },
    {
      "epoch": 1.8769618220308053,
      "grad_norm": 3.8477935791015625,
      "learning_rate": 4.843586514830766e-05,
      "loss": 0.7607,
      "step": 205700
    },
    {
      "epoch": 1.8778742973939704,
      "grad_norm": 4.419501781463623,
      "learning_rate": 4.843510475217169e-05,
      "loss": 0.7866,
      "step": 205800
    },
    {
      "epoch": 1.8787867727571355,
      "grad_norm": 3.517817735671997,
      "learning_rate": 4.843434435603572e-05,
      "loss": 0.7525,
      "step": 205900
    },
    {
      "epoch": 1.8796992481203008,
      "grad_norm": 3.8078906536102295,
      "learning_rate": 4.843358395989975e-05,
      "loss": 0.73,
      "step": 206000
    },
    {
      "epoch": 1.880611723483466,
      "grad_norm": 3.8575165271759033,
      "learning_rate": 4.843282356376378e-05,
      "loss": 0.7434,
      "step": 206100
    },
    {
      "epoch": 1.8815241988466311,
      "grad_norm": 4.170253753662109,
      "learning_rate": 4.843206316762781e-05,
      "loss": 0.7527,
      "step": 206200
    },
    {
      "epoch": 1.8824366742097962,
      "grad_norm": 4.594998359680176,
      "learning_rate": 4.8431302771491836e-05,
      "loss": 0.7196,
      "step": 206300
    },
    {
      "epoch": 1.8833491495729615,
      "grad_norm": 3.265009641647339,
      "learning_rate": 4.843054237535587e-05,
      "loss": 0.8012,
      "step": 206400
    },
    {
      "epoch": 1.8842616249361268,
      "grad_norm": 4.346935749053955,
      "learning_rate": 4.8429781979219896e-05,
      "loss": 0.7321,
      "step": 206500
    },
    {
      "epoch": 1.8851741002992919,
      "grad_norm": 4.34030818939209,
      "learning_rate": 4.8429021583083926e-05,
      "loss": 0.7287,
      "step": 206600
    },
    {
      "epoch": 1.886086575662457,
      "grad_norm": 3.626173257827759,
      "learning_rate": 4.8428261186947956e-05,
      "loss": 0.7598,
      "step": 206700
    },
    {
      "epoch": 1.8869990510256223,
      "grad_norm": 3.914496660232544,
      "learning_rate": 4.8427500790811986e-05,
      "loss": 0.7781,
      "step": 206800
    },
    {
      "epoch": 1.8879115263887876,
      "grad_norm": 3.5614635944366455,
      "learning_rate": 4.842674039467601e-05,
      "loss": 0.7634,
      "step": 206900
    },
    {
      "epoch": 1.8888240017519529,
      "grad_norm": 4.505011558532715,
      "learning_rate": 4.8425979998540046e-05,
      "loss": 0.7606,
      "step": 207000
    },
    {
      "epoch": 1.889736477115118,
      "grad_norm": 3.630859136581421,
      "learning_rate": 4.842521960240407e-05,
      "loss": 0.7358,
      "step": 207100
    },
    {
      "epoch": 1.890648952478283,
      "grad_norm": 4.227797031402588,
      "learning_rate": 4.84244592062681e-05,
      "loss": 0.7956,
      "step": 207200
    },
    {
      "epoch": 1.8915614278414483,
      "grad_norm": 4.1813578605651855,
      "learning_rate": 4.842369881013213e-05,
      "loss": 0.7681,
      "step": 207300
    },
    {
      "epoch": 1.8924739032046136,
      "grad_norm": 4.853649616241455,
      "learning_rate": 4.842293841399615e-05,
      "loss": 0.7842,
      "step": 207400
    },
    {
      "epoch": 1.8933863785677787,
      "grad_norm": 3.8803117275238037,
      "learning_rate": 4.842217801786019e-05,
      "loss": 0.7804,
      "step": 207500
    },
    {
      "epoch": 1.8942988539309438,
      "grad_norm": 4.538166522979736,
      "learning_rate": 4.842141762172421e-05,
      "loss": 0.7721,
      "step": 207600
    },
    {
      "epoch": 1.895211329294109,
      "grad_norm": 4.57363748550415,
      "learning_rate": 4.842065722558824e-05,
      "loss": 0.7844,
      "step": 207700
    },
    {
      "epoch": 1.8961238046572744,
      "grad_norm": 4.311727523803711,
      "learning_rate": 4.841989682945227e-05,
      "loss": 0.7576,
      "step": 207800
    },
    {
      "epoch": 1.8970362800204394,
      "grad_norm": 3.515833616256714,
      "learning_rate": 4.84191364333163e-05,
      "loss": 0.7632,
      "step": 207900
    },
    {
      "epoch": 1.8979487553836045,
      "grad_norm": 4.168056011199951,
      "learning_rate": 4.8418376037180326e-05,
      "loss": 0.7843,
      "step": 208000
    },
    {
      "epoch": 1.8988612307467698,
      "grad_norm": 3.8704450130462646,
      "learning_rate": 4.841761564104436e-05,
      "loss": 0.773,
      "step": 208100
    },
    {
      "epoch": 1.8997737061099351,
      "grad_norm": 4.058967590332031,
      "learning_rate": 4.8416855244908387e-05,
      "loss": 0.7338,
      "step": 208200
    },
    {
      "epoch": 1.9006861814731002,
      "grad_norm": 4.996984481811523,
      "learning_rate": 4.8416094848772417e-05,
      "loss": 0.7833,
      "step": 208300
    },
    {
      "epoch": 1.9015986568362653,
      "grad_norm": 4.421825408935547,
      "learning_rate": 4.841533445263645e-05,
      "loss": 0.7685,
      "step": 208400
    },
    {
      "epoch": 1.9025111321994306,
      "grad_norm": 4.401735305786133,
      "learning_rate": 4.841457405650048e-05,
      "loss": 0.7629,
      "step": 208500
    },
    {
      "epoch": 1.9034236075625959,
      "grad_norm": 4.273854732513428,
      "learning_rate": 4.841381366036451e-05,
      "loss": 0.7792,
      "step": 208600
    },
    {
      "epoch": 1.904336082925761,
      "grad_norm": 4.149355411529541,
      "learning_rate": 4.841305326422854e-05,
      "loss": 0.7601,
      "step": 208700
    },
    {
      "epoch": 1.9052485582889263,
      "grad_norm": 3.767786741256714,
      "learning_rate": 4.841229286809256e-05,
      "loss": 0.7746,
      "step": 208800
    },
    {
      "epoch": 1.9061610336520913,
      "grad_norm": 5.061697959899902,
      "learning_rate": 4.84115324719566e-05,
      "loss": 0.7476,
      "step": 208900
    },
    {
      "epoch": 1.9070735090152566,
      "grad_norm": 3.0965564250946045,
      "learning_rate": 4.841077207582062e-05,
      "loss": 0.7786,
      "step": 209000
    },
    {
      "epoch": 1.907985984378422,
      "grad_norm": 3.7124385833740234,
      "learning_rate": 4.841001167968465e-05,
      "loss": 0.7652,
      "step": 209100
    },
    {
      "epoch": 1.908898459741587,
      "grad_norm": 3.4428293704986572,
      "learning_rate": 4.840925128354868e-05,
      "loss": 0.7585,
      "step": 209200
    },
    {
      "epoch": 1.909810935104752,
      "grad_norm": 2.9928884506225586,
      "learning_rate": 4.840849088741271e-05,
      "loss": 0.7688,
      "step": 209300
    },
    {
      "epoch": 1.9107234104679174,
      "grad_norm": 3.497173309326172,
      "learning_rate": 4.8407730491276734e-05,
      "loss": 0.7574,
      "step": 209400
    },
    {
      "epoch": 1.9116358858310827,
      "grad_norm": 4.6936492919921875,
      "learning_rate": 4.840697009514077e-05,
      "loss": 0.7997,
      "step": 209500
    },
    {
      "epoch": 1.9125483611942478,
      "grad_norm": 4.507292747497559,
      "learning_rate": 4.8406209699004794e-05,
      "loss": 0.7546,
      "step": 209600
    },
    {
      "epoch": 1.9134608365574128,
      "grad_norm": 3.9561076164245605,
      "learning_rate": 4.8405449302868824e-05,
      "loss": 0.7712,
      "step": 209700
    },
    {
      "epoch": 1.9143733119205781,
      "grad_norm": 5.022995948791504,
      "learning_rate": 4.8404688906732854e-05,
      "loss": 0.7767,
      "step": 209800
    },
    {
      "epoch": 1.9152857872837434,
      "grad_norm": 4.014388561248779,
      "learning_rate": 4.8403928510596884e-05,
      "loss": 0.7505,
      "step": 209900
    },
    {
      "epoch": 1.9161982626469085,
      "grad_norm": 4.224432945251465,
      "learning_rate": 4.8403168114460914e-05,
      "loss": 0.7827,
      "step": 210000
    },
    {
      "epoch": 1.9171107380100736,
      "grad_norm": 3.505316972732544,
      "learning_rate": 4.8402407718324944e-05,
      "loss": 0.7826,
      "step": 210100
    },
    {
      "epoch": 1.9180232133732389,
      "grad_norm": 3.824157953262329,
      "learning_rate": 4.840164732218897e-05,
      "loss": 0.769,
      "step": 210200
    },
    {
      "epoch": 1.9189356887364042,
      "grad_norm": 3.58161997795105,
      "learning_rate": 4.8400886926053e-05,
      "loss": 0.7713,
      "step": 210300
    },
    {
      "epoch": 1.9198481640995693,
      "grad_norm": 4.01676607131958,
      "learning_rate": 4.840012652991703e-05,
      "loss": 0.7846,
      "step": 210400
    },
    {
      "epoch": 1.9207606394627343,
      "grad_norm": 4.580050945281982,
      "learning_rate": 4.839936613378106e-05,
      "loss": 0.7749,
      "step": 210500
    },
    {
      "epoch": 1.9216731148258996,
      "grad_norm": 3.99072003364563,
      "learning_rate": 4.839860573764509e-05,
      "loss": 0.7456,
      "step": 210600
    },
    {
      "epoch": 1.922585590189065,
      "grad_norm": 4.543864727020264,
      "learning_rate": 4.839784534150911e-05,
      "loss": 0.7648,
      "step": 210700
    },
    {
      "epoch": 1.9234980655522302,
      "grad_norm": 5.091527462005615,
      "learning_rate": 4.839708494537314e-05,
      "loss": 0.7551,
      "step": 210800
    },
    {
      "epoch": 1.9244105409153953,
      "grad_norm": 4.754980564117432,
      "learning_rate": 4.839632454923717e-05,
      "loss": 0.7632,
      "step": 210900
    },
    {
      "epoch": 1.9253230162785604,
      "grad_norm": 4.005566120147705,
      "learning_rate": 4.83955641531012e-05,
      "loss": 0.7459,
      "step": 211000
    },
    {
      "epoch": 1.9262354916417257,
      "grad_norm": 4.045485973358154,
      "learning_rate": 4.839480375696523e-05,
      "loss": 0.7651,
      "step": 211100
    },
    {
      "epoch": 1.927147967004891,
      "grad_norm": 3.879248857498169,
      "learning_rate": 4.839404336082926e-05,
      "loss": 0.7702,
      "step": 211200
    },
    {
      "epoch": 1.928060442368056,
      "grad_norm": 3.9545340538024902,
      "learning_rate": 4.8393282964693285e-05,
      "loss": 0.7496,
      "step": 211300
    },
    {
      "epoch": 1.9289729177312211,
      "grad_norm": 4.164599895477295,
      "learning_rate": 4.839252256855732e-05,
      "loss": 0.7683,
      "step": 211400
    },
    {
      "epoch": 1.9298853930943864,
      "grad_norm": 4.391934394836426,
      "learning_rate": 4.8391762172421345e-05,
      "loss": 0.7771,
      "step": 211500
    },
    {
      "epoch": 1.9307978684575517,
      "grad_norm": 4.2056121826171875,
      "learning_rate": 4.8391001776285375e-05,
      "loss": 0.7634,
      "step": 211600
    },
    {
      "epoch": 1.9317103438207168,
      "grad_norm": 4.264582633972168,
      "learning_rate": 4.8390241380149405e-05,
      "loss": 0.783,
      "step": 211700
    },
    {
      "epoch": 1.932622819183882,
      "grad_norm": 4.241374492645264,
      "learning_rate": 4.8389480984013435e-05,
      "loss": 0.75,
      "step": 211800
    },
    {
      "epoch": 1.9335352945470472,
      "grad_norm": 4.09393310546875,
      "learning_rate": 4.8388720587877465e-05,
      "loss": 0.7396,
      "step": 211900
    },
    {
      "epoch": 1.9344477699102125,
      "grad_norm": 4.123305320739746,
      "learning_rate": 4.8387960191741495e-05,
      "loss": 0.7845,
      "step": 212000
    },
    {
      "epoch": 1.9353602452733776,
      "grad_norm": 4.657686233520508,
      "learning_rate": 4.838719979560552e-05,
      "loss": 0.7456,
      "step": 212100
    },
    {
      "epoch": 1.9362727206365427,
      "grad_norm": 3.9811370372772217,
      "learning_rate": 4.8386439399469555e-05,
      "loss": 0.769,
      "step": 212200
    },
    {
      "epoch": 1.937185195999708,
      "grad_norm": 4.308603763580322,
      "learning_rate": 4.838567900333358e-05,
      "loss": 0.7591,
      "step": 212300
    },
    {
      "epoch": 1.9380976713628733,
      "grad_norm": 4.649340629577637,
      "learning_rate": 4.838491860719761e-05,
      "loss": 0.7667,
      "step": 212400
    },
    {
      "epoch": 1.9390101467260386,
      "grad_norm": 3.9423742294311523,
      "learning_rate": 4.838415821106164e-05,
      "loss": 0.7514,
      "step": 212500
    },
    {
      "epoch": 1.9399226220892036,
      "grad_norm": 5.0284342765808105,
      "learning_rate": 4.838339781492567e-05,
      "loss": 0.8134,
      "step": 212600
    },
    {
      "epoch": 1.9408350974523687,
      "grad_norm": 4.045130729675293,
      "learning_rate": 4.838263741878969e-05,
      "loss": 0.7722,
      "step": 212700
    },
    {
      "epoch": 1.941747572815534,
      "grad_norm": 3.7998499870300293,
      "learning_rate": 4.838187702265373e-05,
      "loss": 0.7721,
      "step": 212800
    },
    {
      "epoch": 1.9426600481786993,
      "grad_norm": 4.308857440948486,
      "learning_rate": 4.838111662651775e-05,
      "loss": 0.7473,
      "step": 212900
    },
    {
      "epoch": 1.9435725235418644,
      "grad_norm": 3.9688589572906494,
      "learning_rate": 4.838035623038178e-05,
      "loss": 0.7727,
      "step": 213000
    },
    {
      "epoch": 1.9444849989050295,
      "grad_norm": 4.815685272216797,
      "learning_rate": 4.837959583424581e-05,
      "loss": 0.757,
      "step": 213100
    },
    {
      "epoch": 1.9453974742681948,
      "grad_norm": 4.769375801086426,
      "learning_rate": 4.8378835438109836e-05,
      "loss": 0.7677,
      "step": 213200
    },
    {
      "epoch": 1.94630994963136,
      "grad_norm": 3.6761724948883057,
      "learning_rate": 4.837807504197387e-05,
      "loss": 0.7353,
      "step": 213300
    },
    {
      "epoch": 1.9472224249945251,
      "grad_norm": 3.734020233154297,
      "learning_rate": 4.8377314645837896e-05,
      "loss": 0.7765,
      "step": 213400
    },
    {
      "epoch": 1.9481349003576902,
      "grad_norm": 3.951411247253418,
      "learning_rate": 4.8376554249701926e-05,
      "loss": 0.7714,
      "step": 213500
    },
    {
      "epoch": 1.9490473757208555,
      "grad_norm": 6.1570658683776855,
      "learning_rate": 4.8375793853565956e-05,
      "loss": 0.7696,
      "step": 213600
    },
    {
      "epoch": 1.9499598510840208,
      "grad_norm": 3.8618698120117188,
      "learning_rate": 4.8375033457429986e-05,
      "loss": 0.76,
      "step": 213700
    },
    {
      "epoch": 1.950872326447186,
      "grad_norm": 5.639787197113037,
      "learning_rate": 4.837427306129401e-05,
      "loss": 0.7796,
      "step": 213800
    },
    {
      "epoch": 1.951784801810351,
      "grad_norm": 5.221010684967041,
      "learning_rate": 4.8373512665158046e-05,
      "loss": 0.7799,
      "step": 213900
    },
    {
      "epoch": 1.9526972771735163,
      "grad_norm": 5.089977741241455,
      "learning_rate": 4.837275226902207e-05,
      "loss": 0.7992,
      "step": 214000
    },
    {
      "epoch": 1.9536097525366816,
      "grad_norm": 4.303689956665039,
      "learning_rate": 4.83719918728861e-05,
      "loss": 0.7189,
      "step": 214100
    },
    {
      "epoch": 1.9545222278998469,
      "grad_norm": 4.476932048797607,
      "learning_rate": 4.837123147675013e-05,
      "loss": 0.7691,
      "step": 214200
    },
    {
      "epoch": 1.955434703263012,
      "grad_norm": 4.155200958251953,
      "learning_rate": 4.837047108061416e-05,
      "loss": 0.7761,
      "step": 214300
    },
    {
      "epoch": 1.956347178626177,
      "grad_norm": 3.707150459289551,
      "learning_rate": 4.836971068447819e-05,
      "loss": 0.8039,
      "step": 214400
    },
    {
      "epoch": 1.9572596539893423,
      "grad_norm": 4.175279140472412,
      "learning_rate": 4.836895028834222e-05,
      "loss": 0.7581,
      "step": 214500
    },
    {
      "epoch": 1.9581721293525076,
      "grad_norm": 3.8223869800567627,
      "learning_rate": 4.836818989220624e-05,
      "loss": 0.7589,
      "step": 214600
    },
    {
      "epoch": 1.9590846047156727,
      "grad_norm": 4.350831985473633,
      "learning_rate": 4.836742949607028e-05,
      "loss": 0.7709,
      "step": 214700
    },
    {
      "epoch": 1.9599970800788378,
      "grad_norm": 3.9545114040374756,
      "learning_rate": 4.83666690999343e-05,
      "loss": 0.8015,
      "step": 214800
    },
    {
      "epoch": 1.960909555442003,
      "grad_norm": 4.3949294090271,
      "learning_rate": 4.836590870379833e-05,
      "loss": 0.7454,
      "step": 214900
    },
    {
      "epoch": 1.9618220308051684,
      "grad_norm": 3.503653049468994,
      "learning_rate": 4.836514830766236e-05,
      "loss": 0.7821,
      "step": 215000
    },
    {
      "epoch": 1.9627345061683334,
      "grad_norm": 2.8809967041015625,
      "learning_rate": 4.836438791152639e-05,
      "loss": 0.743,
      "step": 215100
    },
    {
      "epoch": 1.9636469815314985,
      "grad_norm": 4.074432849884033,
      "learning_rate": 4.836362751539042e-05,
      "loss": 0.7988,
      "step": 215200
    },
    {
      "epoch": 1.9645594568946638,
      "grad_norm": 3.975098133087158,
      "learning_rate": 4.8362867119254454e-05,
      "loss": 0.7502,
      "step": 215300
    },
    {
      "epoch": 1.9654719322578291,
      "grad_norm": 2.787365198135376,
      "learning_rate": 4.836210672311848e-05,
      "loss": 0.7682,
      "step": 215400
    },
    {
      "epoch": 1.9663844076209942,
      "grad_norm": 4.569724082946777,
      "learning_rate": 4.836134632698251e-05,
      "loss": 0.7299,
      "step": 215500
    },
    {
      "epoch": 1.9672968829841593,
      "grad_norm": 4.007027626037598,
      "learning_rate": 4.836058593084654e-05,
      "loss": 0.7757,
      "step": 215600
    },
    {
      "epoch": 1.9682093583473246,
      "grad_norm": 4.620784759521484,
      "learning_rate": 4.835982553471057e-05,
      "loss": 0.7419,
      "step": 215700
    },
    {
      "epoch": 1.9691218337104899,
      "grad_norm": 4.889620780944824,
      "learning_rate": 4.83590651385746e-05,
      "loss": 0.7777,
      "step": 215800
    },
    {
      "epoch": 1.9700343090736552,
      "grad_norm": 4.535297870635986,
      "learning_rate": 4.835830474243862e-05,
      "loss": 0.7889,
      "step": 215900
    },
    {
      "epoch": 1.9709467844368203,
      "grad_norm": 3.946476697921753,
      "learning_rate": 4.835754434630265e-05,
      "loss": 0.8153,
      "step": 216000
    },
    {
      "epoch": 1.9718592597999853,
      "grad_norm": 3.9288594722747803,
      "learning_rate": 4.835678395016668e-05,
      "loss": 0.7961,
      "step": 216100
    },
    {
      "epoch": 1.9727717351631506,
      "grad_norm": 3.643131732940674,
      "learning_rate": 4.835602355403071e-05,
      "loss": 0.7693,
      "step": 216200
    },
    {
      "epoch": 1.973684210526316,
      "grad_norm": 5.194296360015869,
      "learning_rate": 4.8355263157894734e-05,
      "loss": 0.7787,
      "step": 216300
    },
    {
      "epoch": 1.974596685889481,
      "grad_norm": 3.490907907485962,
      "learning_rate": 4.835450276175877e-05,
      "loss": 0.7529,
      "step": 216400
    },
    {
      "epoch": 1.975509161252646,
      "grad_norm": 4.4233808517456055,
      "learning_rate": 4.8353742365622794e-05,
      "loss": 0.7611,
      "step": 216500
    },
    {
      "epoch": 1.9764216366158114,
      "grad_norm": 4.24946928024292,
      "learning_rate": 4.8352981969486824e-05,
      "loss": 0.7175,
      "step": 216600
    },
    {
      "epoch": 1.9773341119789767,
      "grad_norm": 4.6864471435546875,
      "learning_rate": 4.8352221573350854e-05,
      "loss": 0.7622,
      "step": 216700
    },
    {
      "epoch": 1.9782465873421418,
      "grad_norm": 4.382858753204346,
      "learning_rate": 4.8351461177214884e-05,
      "loss": 0.7482,
      "step": 216800
    },
    {
      "epoch": 1.9791590627053068,
      "grad_norm": 6.085351467132568,
      "learning_rate": 4.8350700781078914e-05,
      "loss": 0.7816,
      "step": 216900
    },
    {
      "epoch": 1.9800715380684721,
      "grad_norm": 4.017479419708252,
      "learning_rate": 4.8349940384942944e-05,
      "loss": 0.7453,
      "step": 217000
    },
    {
      "epoch": 1.9809840134316374,
      "grad_norm": 3.847500801086426,
      "learning_rate": 4.834917998880697e-05,
      "loss": 0.7469,
      "step": 217100
    },
    {
      "epoch": 1.9818964887948025,
      "grad_norm": 4.231930255889893,
      "learning_rate": 4.8348419592671004e-05,
      "loss": 0.7758,
      "step": 217200
    },
    {
      "epoch": 1.9828089641579676,
      "grad_norm": 4.543985843658447,
      "learning_rate": 4.834765919653503e-05,
      "loss": 0.8011,
      "step": 217300
    },
    {
      "epoch": 1.983721439521133,
      "grad_norm": 3.6114234924316406,
      "learning_rate": 4.834689880039906e-05,
      "loss": 0.8242,
      "step": 217400
    },
    {
      "epoch": 1.9846339148842982,
      "grad_norm": 3.452584981918335,
      "learning_rate": 4.834613840426309e-05,
      "loss": 0.7959,
      "step": 217500
    },
    {
      "epoch": 1.9855463902474633,
      "grad_norm": 3.6902782917022705,
      "learning_rate": 4.834537800812712e-05,
      "loss": 0.7744,
      "step": 217600
    },
    {
      "epoch": 1.9864588656106286,
      "grad_norm": 5.131017208099365,
      "learning_rate": 4.834461761199114e-05,
      "loss": 0.7349,
      "step": 217700
    },
    {
      "epoch": 1.9873713409737936,
      "grad_norm": 3.1268961429595947,
      "learning_rate": 4.834385721585518e-05,
      "loss": 0.7382,
      "step": 217800
    },
    {
      "epoch": 1.988283816336959,
      "grad_norm": 4.114807605743408,
      "learning_rate": 4.83430968197192e-05,
      "loss": 0.7474,
      "step": 217900
    },
    {
      "epoch": 1.9891962917001242,
      "grad_norm": 4.660799026489258,
      "learning_rate": 4.834233642358323e-05,
      "loss": 0.7229,
      "step": 218000
    },
    {
      "epoch": 1.9901087670632893,
      "grad_norm": 4.345290660858154,
      "learning_rate": 4.834157602744726e-05,
      "loss": 0.7373,
      "step": 218100
    },
    {
      "epoch": 1.9910212424264544,
      "grad_norm": 4.656675338745117,
      "learning_rate": 4.834081563131129e-05,
      "loss": 0.7805,
      "step": 218200
    },
    {
      "epoch": 1.9919337177896197,
      "grad_norm": 3.389254093170166,
      "learning_rate": 4.834005523517532e-05,
      "loss": 0.8156,
      "step": 218300
    },
    {
      "epoch": 1.992846193152785,
      "grad_norm": 3.811876058578491,
      "learning_rate": 4.833929483903935e-05,
      "loss": 0.7895,
      "step": 218400
    },
    {
      "epoch": 1.99375866851595,
      "grad_norm": 3.83652925491333,
      "learning_rate": 4.8338534442903375e-05,
      "loss": 0.7793,
      "step": 218500
    },
    {
      "epoch": 1.9946711438791151,
      "grad_norm": 4.453648090362549,
      "learning_rate": 4.833777404676741e-05,
      "loss": 0.7749,
      "step": 218600
    },
    {
      "epoch": 1.9955836192422804,
      "grad_norm": 5.90048360824585,
      "learning_rate": 4.8337013650631435e-05,
      "loss": 0.7542,
      "step": 218700
    },
    {
      "epoch": 1.9964960946054457,
      "grad_norm": 3.6060187816619873,
      "learning_rate": 4.833625325449546e-05,
      "loss": 0.7725,
      "step": 218800
    },
    {
      "epoch": 1.9974085699686108,
      "grad_norm": 4.024191856384277,
      "learning_rate": 4.8335492858359495e-05,
      "loss": 0.735,
      "step": 218900
    },
    {
      "epoch": 1.998321045331776,
      "grad_norm": 4.357305526733398,
      "learning_rate": 4.833473246222352e-05,
      "loss": 0.7877,
      "step": 219000
    },
    {
      "epoch": 1.9992335206949412,
      "grad_norm": 3.879009246826172,
      "learning_rate": 4.833397206608755e-05,
      "loss": 0.7733,
      "step": 219100
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.614923894405365,
      "eval_runtime": 25.7945,
      "eval_samples_per_second": 223.652,
      "eval_steps_per_second": 223.652,
      "step": 219184
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.6026489734649658,
      "eval_runtime": 483.2987,
      "eval_samples_per_second": 226.758,
      "eval_steps_per_second": 226.758,
      "step": 219184
    },
    {
      "epoch": 2.0001459960581065,
      "grad_norm": 3.9848127365112305,
      "learning_rate": 4.833321166995158e-05,
      "loss": 0.7377,
      "step": 219200
    },
    {
      "epoch": 2.001058471421272,
      "grad_norm": 4.740444183349609,
      "learning_rate": 4.833245127381561e-05,
      "loss": 0.7729,
      "step": 219300
    },
    {
      "epoch": 2.0019709467844367,
      "grad_norm": 4.986690521240234,
      "learning_rate": 4.833169087767964e-05,
      "loss": 0.7694,
      "step": 219400
    },
    {
      "epoch": 2.002883422147602,
      "grad_norm": 4.404385089874268,
      "learning_rate": 4.833093048154367e-05,
      "loss": 0.7245,
      "step": 219500
    },
    {
      "epoch": 2.0037958975107673,
      "grad_norm": 3.6096160411834717,
      "learning_rate": 4.833017008540769e-05,
      "loss": 0.7708,
      "step": 219600
    },
    {
      "epoch": 2.0047083728739326,
      "grad_norm": 4.174703121185303,
      "learning_rate": 4.832940968927173e-05,
      "loss": 0.7391,
      "step": 219700
    },
    {
      "epoch": 2.0056208482370974,
      "grad_norm": 3.8977468013763428,
      "learning_rate": 4.832864929313575e-05,
      "loss": 0.7811,
      "step": 219800
    },
    {
      "epoch": 2.0065333236002627,
      "grad_norm": 4.233883380889893,
      "learning_rate": 4.832788889699978e-05,
      "loss": 0.8129,
      "step": 219900
    },
    {
      "epoch": 2.007445798963428,
      "grad_norm": 4.054078102111816,
      "learning_rate": 4.832712850086381e-05,
      "loss": 0.7519,
      "step": 220000
    },
    {
      "epoch": 2.0083582743265933,
      "grad_norm": 4.696791172027588,
      "learning_rate": 4.832636810472784e-05,
      "loss": 0.718,
      "step": 220100
    },
    {
      "epoch": 2.009270749689758,
      "grad_norm": 5.321535587310791,
      "learning_rate": 4.8325607708591866e-05,
      "loss": 0.7298,
      "step": 220200
    },
    {
      "epoch": 2.0101832250529235,
      "grad_norm": 4.560953140258789,
      "learning_rate": 4.83248473124559e-05,
      "loss": 0.7674,
      "step": 220300
    },
    {
      "epoch": 2.0110957004160888,
      "grad_norm": 4.513675212860107,
      "learning_rate": 4.8324086916319926e-05,
      "loss": 0.7573,
      "step": 220400
    },
    {
      "epoch": 2.012008175779254,
      "grad_norm": 4.187802314758301,
      "learning_rate": 4.8323326520183956e-05,
      "loss": 0.7649,
      "step": 220500
    },
    {
      "epoch": 2.0129206511424194,
      "grad_norm": 4.343139171600342,
      "learning_rate": 4.8322566124047986e-05,
      "loss": 0.7445,
      "step": 220600
    },
    {
      "epoch": 2.013833126505584,
      "grad_norm": 4.744040012359619,
      "learning_rate": 4.8321805727912016e-05,
      "loss": 0.7336,
      "step": 220700
    },
    {
      "epoch": 2.0147456018687495,
      "grad_norm": 5.105003356933594,
      "learning_rate": 4.8321045331776046e-05,
      "loss": 0.7608,
      "step": 220800
    },
    {
      "epoch": 2.015658077231915,
      "grad_norm": 3.967154026031494,
      "learning_rate": 4.8320284935640076e-05,
      "loss": 0.7517,
      "step": 220900
    },
    {
      "epoch": 2.01657055259508,
      "grad_norm": 4.607671737670898,
      "learning_rate": 4.83195245395041e-05,
      "loss": 0.7433,
      "step": 221000
    },
    {
      "epoch": 2.017483027958245,
      "grad_norm": 5.644398212432861,
      "learning_rate": 4.8318764143368136e-05,
      "loss": 0.762,
      "step": 221100
    },
    {
      "epoch": 2.0183955033214103,
      "grad_norm": 3.1784327030181885,
      "learning_rate": 4.831800374723216e-05,
      "loss": 0.784,
      "step": 221200
    },
    {
      "epoch": 2.0193079786845756,
      "grad_norm": 4.511801242828369,
      "learning_rate": 4.831724335109619e-05,
      "loss": 0.7244,
      "step": 221300
    },
    {
      "epoch": 2.020220454047741,
      "grad_norm": 4.33358097076416,
      "learning_rate": 4.831648295496022e-05,
      "loss": 0.7665,
      "step": 221400
    },
    {
      "epoch": 2.0211329294109057,
      "grad_norm": 4.6959404945373535,
      "learning_rate": 4.831572255882425e-05,
      "loss": 0.7284,
      "step": 221500
    },
    {
      "epoch": 2.022045404774071,
      "grad_norm": 4.255618095397949,
      "learning_rate": 4.831496216268827e-05,
      "loss": 0.7664,
      "step": 221600
    },
    {
      "epoch": 2.0229578801372363,
      "grad_norm": 4.601779460906982,
      "learning_rate": 4.83142017665523e-05,
      "loss": 0.7239,
      "step": 221700
    },
    {
      "epoch": 2.0238703555004016,
      "grad_norm": 2.756661891937256,
      "learning_rate": 4.831344137041633e-05,
      "loss": 0.722,
      "step": 221800
    },
    {
      "epoch": 2.0247828308635665,
      "grad_norm": 3.9832069873809814,
      "learning_rate": 4.8312680974280363e-05,
      "loss": 0.7674,
      "step": 221900
    },
    {
      "epoch": 2.0256953062267318,
      "grad_norm": 4.195547580718994,
      "learning_rate": 4.8311920578144393e-05,
      "loss": 0.7679,
      "step": 222000
    },
    {
      "epoch": 2.026607781589897,
      "grad_norm": 3.017793655395508,
      "learning_rate": 4.831116018200842e-05,
      "loss": 0.783,
      "step": 222100
    },
    {
      "epoch": 2.0275202569530624,
      "grad_norm": 4.391787528991699,
      "learning_rate": 4.8310399785872454e-05,
      "loss": 0.7738,
      "step": 222200
    },
    {
      "epoch": 2.0284327323162277,
      "grad_norm": 4.41478967666626,
      "learning_rate": 4.830963938973648e-05,
      "loss": 0.7602,
      "step": 222300
    },
    {
      "epoch": 2.0293452076793925,
      "grad_norm": 1.7778769731521606,
      "learning_rate": 4.830887899360051e-05,
      "loss": 0.753,
      "step": 222400
    },
    {
      "epoch": 2.030257683042558,
      "grad_norm": 4.722476482391357,
      "learning_rate": 4.830811859746454e-05,
      "loss": 0.774,
      "step": 222500
    },
    {
      "epoch": 2.031170158405723,
      "grad_norm": 3.7610766887664795,
      "learning_rate": 4.830735820132857e-05,
      "loss": 0.77,
      "step": 222600
    },
    {
      "epoch": 2.0320826337688884,
      "grad_norm": 4.656155109405518,
      "learning_rate": 4.83065978051926e-05,
      "loss": 0.783,
      "step": 222700
    },
    {
      "epoch": 2.0329951091320533,
      "grad_norm": 3.6576390266418457,
      "learning_rate": 4.830583740905663e-05,
      "loss": 0.6978,
      "step": 222800
    },
    {
      "epoch": 2.0339075844952186,
      "grad_norm": 4.8795013427734375,
      "learning_rate": 4.830507701292065e-05,
      "loss": 0.7332,
      "step": 222900
    },
    {
      "epoch": 2.034820059858384,
      "grad_norm": 2.884868860244751,
      "learning_rate": 4.830431661678468e-05,
      "loss": 0.8006,
      "step": 223000
    },
    {
      "epoch": 2.035732535221549,
      "grad_norm": 3.8598482608795166,
      "learning_rate": 4.830355622064871e-05,
      "loss": 0.6898,
      "step": 223100
    },
    {
      "epoch": 2.036645010584714,
      "grad_norm": 4.972919940948486,
      "learning_rate": 4.830279582451274e-05,
      "loss": 0.7329,
      "step": 223200
    },
    {
      "epoch": 2.0375574859478793,
      "grad_norm": 4.565117835998535,
      "learning_rate": 4.830203542837677e-05,
      "loss": 0.7568,
      "step": 223300
    },
    {
      "epoch": 2.0384699613110446,
      "grad_norm": 4.401445388793945,
      "learning_rate": 4.83012750322408e-05,
      "loss": 0.7698,
      "step": 223400
    },
    {
      "epoch": 2.03938243667421,
      "grad_norm": 4.1698126792907715,
      "learning_rate": 4.8300514636104824e-05,
      "loss": 0.7404,
      "step": 223500
    },
    {
      "epoch": 2.040294912037375,
      "grad_norm": 4.342547416687012,
      "learning_rate": 4.829975423996886e-05,
      "loss": 0.7674,
      "step": 223600
    },
    {
      "epoch": 2.04120738740054,
      "grad_norm": 3.191192626953125,
      "learning_rate": 4.8298993843832884e-05,
      "loss": 0.7414,
      "step": 223700
    },
    {
      "epoch": 2.0421198627637054,
      "grad_norm": 3.600412368774414,
      "learning_rate": 4.8298233447696914e-05,
      "loss": 0.7513,
      "step": 223800
    },
    {
      "epoch": 2.0430323381268707,
      "grad_norm": 4.197786808013916,
      "learning_rate": 4.8297473051560944e-05,
      "loss": 0.7839,
      "step": 223900
    },
    {
      "epoch": 2.043944813490036,
      "grad_norm": 5.19450569152832,
      "learning_rate": 4.8296712655424974e-05,
      "loss": 0.7944,
      "step": 224000
    },
    {
      "epoch": 2.044857288853201,
      "grad_norm": 4.067723274230957,
      "learning_rate": 4.8295952259289005e-05,
      "loss": 0.7587,
      "step": 224100
    },
    {
      "epoch": 2.045769764216366,
      "grad_norm": 4.068368434906006,
      "learning_rate": 4.8295191863153035e-05,
      "loss": 0.7893,
      "step": 224200
    },
    {
      "epoch": 2.0466822395795314,
      "grad_norm": 3.5190892219543457,
      "learning_rate": 4.829443146701706e-05,
      "loss": 0.7178,
      "step": 224300
    },
    {
      "epoch": 2.0475947149426967,
      "grad_norm": 4.2558913230896,
      "learning_rate": 4.829367107088109e-05,
      "loss": 0.7776,
      "step": 224400
    },
    {
      "epoch": 2.0485071903058616,
      "grad_norm": 3.0543973445892334,
      "learning_rate": 4.829291067474512e-05,
      "loss": 0.7485,
      "step": 224500
    },
    {
      "epoch": 2.049419665669027,
      "grad_norm": 3.28582763671875,
      "learning_rate": 4.829215027860914e-05,
      "loss": 0.7303,
      "step": 224600
    },
    {
      "epoch": 2.050332141032192,
      "grad_norm": 3.419410228729248,
      "learning_rate": 4.829138988247318e-05,
      "loss": 0.7436,
      "step": 224700
    },
    {
      "epoch": 2.0512446163953575,
      "grad_norm": 4.002412796020508,
      "learning_rate": 4.82906294863372e-05,
      "loss": 0.7689,
      "step": 224800
    },
    {
      "epoch": 2.0521570917585223,
      "grad_norm": 3.9553213119506836,
      "learning_rate": 4.828986909020123e-05,
      "loss": 0.7584,
      "step": 224900
    },
    {
      "epoch": 2.0530695671216876,
      "grad_norm": 3.944655418395996,
      "learning_rate": 4.828910869406526e-05,
      "loss": 0.7406,
      "step": 225000
    },
    {
      "epoch": 2.053982042484853,
      "grad_norm": 3.225395917892456,
      "learning_rate": 4.828834829792929e-05,
      "loss": 0.7685,
      "step": 225100
    },
    {
      "epoch": 2.0548945178480182,
      "grad_norm": 4.435255527496338,
      "learning_rate": 4.828758790179332e-05,
      "loss": 0.8068,
      "step": 225200
    },
    {
      "epoch": 2.055806993211183,
      "grad_norm": 3.7752580642700195,
      "learning_rate": 4.828682750565735e-05,
      "loss": 0.7435,
      "step": 225300
    },
    {
      "epoch": 2.0567194685743484,
      "grad_norm": 4.810760021209717,
      "learning_rate": 4.8286067109521375e-05,
      "loss": 0.7453,
      "step": 225400
    },
    {
      "epoch": 2.0576319439375137,
      "grad_norm": 4.185915946960449,
      "learning_rate": 4.828530671338541e-05,
      "loss": 0.7638,
      "step": 225500
    },
    {
      "epoch": 2.058544419300679,
      "grad_norm": 3.623621940612793,
      "learning_rate": 4.8284546317249435e-05,
      "loss": 0.791,
      "step": 225600
    },
    {
      "epoch": 2.0594568946638443,
      "grad_norm": 4.012878894805908,
      "learning_rate": 4.8283785921113465e-05,
      "loss": 0.8043,
      "step": 225700
    },
    {
      "epoch": 2.060369370027009,
      "grad_norm": 3.4101500511169434,
      "learning_rate": 4.8283025524977495e-05,
      "loss": 0.7474,
      "step": 225800
    },
    {
      "epoch": 2.0612818453901744,
      "grad_norm": 3.865739345550537,
      "learning_rate": 4.8282265128841525e-05,
      "loss": 0.7699,
      "step": 225900
    },
    {
      "epoch": 2.0621943207533397,
      "grad_norm": 4.311302661895752,
      "learning_rate": 4.828150473270555e-05,
      "loss": 0.7573,
      "step": 226000
    },
    {
      "epoch": 2.063106796116505,
      "grad_norm": 3.9834420680999756,
      "learning_rate": 4.8280744336569586e-05,
      "loss": 0.7719,
      "step": 226100
    },
    {
      "epoch": 2.06401927147967,
      "grad_norm": 4.419888019561768,
      "learning_rate": 4.827998394043361e-05,
      "loss": 0.7421,
      "step": 226200
    },
    {
      "epoch": 2.064931746842835,
      "grad_norm": 4.99076509475708,
      "learning_rate": 4.827922354429764e-05,
      "loss": 0.7581,
      "step": 226300
    },
    {
      "epoch": 2.0658442222060005,
      "grad_norm": 4.592443466186523,
      "learning_rate": 4.827846314816167e-05,
      "loss": 0.7629,
      "step": 226400
    },
    {
      "epoch": 2.066756697569166,
      "grad_norm": 4.42121696472168,
      "learning_rate": 4.82777027520257e-05,
      "loss": 0.7608,
      "step": 226500
    },
    {
      "epoch": 2.0676691729323307,
      "grad_norm": 3.995309352874756,
      "learning_rate": 4.827694235588973e-05,
      "loss": 0.7647,
      "step": 226600
    },
    {
      "epoch": 2.068581648295496,
      "grad_norm": 3.767775774002075,
      "learning_rate": 4.827618195975376e-05,
      "loss": 0.7949,
      "step": 226700
    },
    {
      "epoch": 2.0694941236586613,
      "grad_norm": 4.555110931396484,
      "learning_rate": 4.827542156361778e-05,
      "loss": 0.6812,
      "step": 226800
    },
    {
      "epoch": 2.0704065990218266,
      "grad_norm": 3.5713818073272705,
      "learning_rate": 4.827466116748182e-05,
      "loss": 0.7746,
      "step": 226900
    },
    {
      "epoch": 2.0713190743849914,
      "grad_norm": 3.5380663871765137,
      "learning_rate": 4.827390077134584e-05,
      "loss": 0.7232,
      "step": 227000
    },
    {
      "epoch": 2.0722315497481567,
      "grad_norm": 5.400002479553223,
      "learning_rate": 4.827314037520987e-05,
      "loss": 0.7487,
      "step": 227100
    },
    {
      "epoch": 2.073144025111322,
      "grad_norm": 3.6710569858551025,
      "learning_rate": 4.82723799790739e-05,
      "loss": 0.7533,
      "step": 227200
    },
    {
      "epoch": 2.0740565004744873,
      "grad_norm": 3.3452606201171875,
      "learning_rate": 4.8271619582937926e-05,
      "loss": 0.7266,
      "step": 227300
    },
    {
      "epoch": 2.0749689758376526,
      "grad_norm": 4.477065563201904,
      "learning_rate": 4.8270859186801956e-05,
      "loss": 0.7592,
      "step": 227400
    },
    {
      "epoch": 2.0758814512008175,
      "grad_norm": 3.404742479324341,
      "learning_rate": 4.8270098790665986e-05,
      "loss": 0.735,
      "step": 227500
    },
    {
      "epoch": 2.0767939265639828,
      "grad_norm": 3.8843116760253906,
      "learning_rate": 4.8269338394530016e-05,
      "loss": 0.7581,
      "step": 227600
    },
    {
      "epoch": 2.077706401927148,
      "grad_norm": 4.648898601531982,
      "learning_rate": 4.8268577998394046e-05,
      "loss": 0.72,
      "step": 227700
    },
    {
      "epoch": 2.0786188772903134,
      "grad_norm": 3.7687125205993652,
      "learning_rate": 4.8267817602258076e-05,
      "loss": 0.7986,
      "step": 227800
    },
    {
      "epoch": 2.079531352653478,
      "grad_norm": 4.711369037628174,
      "learning_rate": 4.82670572061221e-05,
      "loss": 0.7667,
      "step": 227900
    },
    {
      "epoch": 2.0804438280166435,
      "grad_norm": 3.438803195953369,
      "learning_rate": 4.8266296809986137e-05,
      "loss": 0.7602,
      "step": 228000
    },
    {
      "epoch": 2.081356303379809,
      "grad_norm": 4.105189800262451,
      "learning_rate": 4.826553641385016e-05,
      "loss": 0.731,
      "step": 228100
    },
    {
      "epoch": 2.082268778742974,
      "grad_norm": 3.792860507965088,
      "learning_rate": 4.826477601771419e-05,
      "loss": 0.7313,
      "step": 228200
    },
    {
      "epoch": 2.083181254106139,
      "grad_norm": 4.514847755432129,
      "learning_rate": 4.826401562157822e-05,
      "loss": 0.7574,
      "step": 228300
    },
    {
      "epoch": 2.0840937294693043,
      "grad_norm": 4.196302890777588,
      "learning_rate": 4.826325522544225e-05,
      "loss": 0.76,
      "step": 228400
    },
    {
      "epoch": 2.0850062048324696,
      "grad_norm": 3.965599775314331,
      "learning_rate": 4.826249482930627e-05,
      "loss": 0.7126,
      "step": 228500
    },
    {
      "epoch": 2.085918680195635,
      "grad_norm": 3.4669229984283447,
      "learning_rate": 4.826173443317031e-05,
      "loss": 0.7489,
      "step": 228600
    },
    {
      "epoch": 2.0868311555587997,
      "grad_norm": 2.8685991764068604,
      "learning_rate": 4.8260974037034333e-05,
      "loss": 0.7183,
      "step": 228700
    },
    {
      "epoch": 2.087743630921965,
      "grad_norm": 5.672389984130859,
      "learning_rate": 4.8260213640898364e-05,
      "loss": 0.7171,
      "step": 228800
    },
    {
      "epoch": 2.0886561062851303,
      "grad_norm": 5.053860187530518,
      "learning_rate": 4.8259453244762394e-05,
      "loss": 0.8202,
      "step": 228900
    },
    {
      "epoch": 2.0895685816482956,
      "grad_norm": 4.130062103271484,
      "learning_rate": 4.8258692848626424e-05,
      "loss": 0.7724,
      "step": 229000
    },
    {
      "epoch": 2.090481057011461,
      "grad_norm": 3.472567319869995,
      "learning_rate": 4.8257932452490454e-05,
      "loss": 0.7676,
      "step": 229100
    },
    {
      "epoch": 2.0913935323746258,
      "grad_norm": 4.818324089050293,
      "learning_rate": 4.8257172056354484e-05,
      "loss": 0.7469,
      "step": 229200
    },
    {
      "epoch": 2.092306007737791,
      "grad_norm": 4.4438910484313965,
      "learning_rate": 4.825641166021851e-05,
      "loss": 0.776,
      "step": 229300
    },
    {
      "epoch": 2.0932184831009564,
      "grad_norm": 3.155036687850952,
      "learning_rate": 4.8255651264082544e-05,
      "loss": 0.7428,
      "step": 229400
    },
    {
      "epoch": 2.0941309584641217,
      "grad_norm": 4.3562116622924805,
      "learning_rate": 4.825489086794657e-05,
      "loss": 0.7608,
      "step": 229500
    },
    {
      "epoch": 2.0950434338272865,
      "grad_norm": 4.453723430633545,
      "learning_rate": 4.82541304718106e-05,
      "loss": 0.7531,
      "step": 229600
    },
    {
      "epoch": 2.095955909190452,
      "grad_norm": 4.50288724899292,
      "learning_rate": 4.825337007567463e-05,
      "loss": 0.7608,
      "step": 229700
    },
    {
      "epoch": 2.096868384553617,
      "grad_norm": 3.582948684692383,
      "learning_rate": 4.825260967953866e-05,
      "loss": 0.7536,
      "step": 229800
    },
    {
      "epoch": 2.0977808599167824,
      "grad_norm": 3.602450132369995,
      "learning_rate": 4.825184928340268e-05,
      "loss": 0.7464,
      "step": 229900
    },
    {
      "epoch": 2.0986933352799473,
      "grad_norm": 4.907379627227783,
      "learning_rate": 4.825108888726672e-05,
      "loss": 0.8022,
      "step": 230000
    },
    {
      "epoch": 2.0996058106431126,
      "grad_norm": 2.730410575866699,
      "learning_rate": 4.825032849113074e-05,
      "loss": 0.7659,
      "step": 230100
    },
    {
      "epoch": 2.100518286006278,
      "grad_norm": 4.031408786773682,
      "learning_rate": 4.824956809499477e-05,
      "loss": 0.741,
      "step": 230200
    },
    {
      "epoch": 2.101430761369443,
      "grad_norm": 4.504700183868408,
      "learning_rate": 4.82488076988588e-05,
      "loss": 0.7501,
      "step": 230300
    },
    {
      "epoch": 2.102343236732608,
      "grad_norm": 4.703200817108154,
      "learning_rate": 4.8248047302722824e-05,
      "loss": 0.7559,
      "step": 230400
    },
    {
      "epoch": 2.1032557120957733,
      "grad_norm": 4.862987995147705,
      "learning_rate": 4.824728690658686e-05,
      "loss": 0.7664,
      "step": 230500
    },
    {
      "epoch": 2.1041681874589386,
      "grad_norm": 3.5768563747406006,
      "learning_rate": 4.8246526510450884e-05,
      "loss": 0.8058,
      "step": 230600
    },
    {
      "epoch": 2.105080662822104,
      "grad_norm": 3.8511099815368652,
      "learning_rate": 4.8245766114314914e-05,
      "loss": 0.7254,
      "step": 230700
    },
    {
      "epoch": 2.105993138185269,
      "grad_norm": 4.283517837524414,
      "learning_rate": 4.8245005718178945e-05,
      "loss": 0.7586,
      "step": 230800
    },
    {
      "epoch": 2.106905613548434,
      "grad_norm": 4.328595161437988,
      "learning_rate": 4.8244245322042975e-05,
      "loss": 0.7579,
      "step": 230900
    },
    {
      "epoch": 2.1078180889115994,
      "grad_norm": 4.056591033935547,
      "learning_rate": 4.8243484925907e-05,
      "loss": 0.7855,
      "step": 231000
    },
    {
      "epoch": 2.1087305642747647,
      "grad_norm": 4.281001091003418,
      "learning_rate": 4.8242724529771035e-05,
      "loss": 0.7427,
      "step": 231100
    },
    {
      "epoch": 2.10964303963793,
      "grad_norm": 4.4775471687316895,
      "learning_rate": 4.824196413363506e-05,
      "loss": 0.7799,
      "step": 231200
    },
    {
      "epoch": 2.110555515001095,
      "grad_norm": 4.596749782562256,
      "learning_rate": 4.824120373749909e-05,
      "loss": 0.7518,
      "step": 231300
    },
    {
      "epoch": 2.11146799036426,
      "grad_norm": 3.524106979370117,
      "learning_rate": 4.824044334136312e-05,
      "loss": 0.7366,
      "step": 231400
    },
    {
      "epoch": 2.1123804657274254,
      "grad_norm": 3.6373050212860107,
      "learning_rate": 4.823968294522715e-05,
      "loss": 0.7461,
      "step": 231500
    },
    {
      "epoch": 2.1132929410905907,
      "grad_norm": 3.9666171073913574,
      "learning_rate": 4.823892254909118e-05,
      "loss": 0.7713,
      "step": 231600
    },
    {
      "epoch": 2.1142054164537556,
      "grad_norm": 4.534517765045166,
      "learning_rate": 4.823816215295521e-05,
      "loss": 0.7422,
      "step": 231700
    },
    {
      "epoch": 2.115117891816921,
      "grad_norm": 4.039590358734131,
      "learning_rate": 4.823740175681923e-05,
      "loss": 0.7825,
      "step": 231800
    },
    {
      "epoch": 2.116030367180086,
      "grad_norm": 4.054225921630859,
      "learning_rate": 4.823664136068327e-05,
      "loss": 0.7476,
      "step": 231900
    },
    {
      "epoch": 2.1169428425432515,
      "grad_norm": 4.512599468231201,
      "learning_rate": 4.823588096454729e-05,
      "loss": 0.7785,
      "step": 232000
    },
    {
      "epoch": 2.1178553179064163,
      "grad_norm": 4.246995449066162,
      "learning_rate": 4.823512056841132e-05,
      "loss": 0.7719,
      "step": 232100
    },
    {
      "epoch": 2.1187677932695816,
      "grad_norm": 4.325327396392822,
      "learning_rate": 4.823436017227535e-05,
      "loss": 0.7816,
      "step": 232200
    },
    {
      "epoch": 2.119680268632747,
      "grad_norm": 5.47965669631958,
      "learning_rate": 4.823359977613938e-05,
      "loss": 0.7455,
      "step": 232300
    },
    {
      "epoch": 2.1205927439959122,
      "grad_norm": 4.429019451141357,
      "learning_rate": 4.8232839380003405e-05,
      "loss": 0.7895,
      "step": 232400
    },
    {
      "epoch": 2.121505219359077,
      "grad_norm": 3.9920268058776855,
      "learning_rate": 4.823207898386744e-05,
      "loss": 0.7092,
      "step": 232500
    },
    {
      "epoch": 2.1224176947222424,
      "grad_norm": 4.159051895141602,
      "learning_rate": 4.8231318587731465e-05,
      "loss": 0.7936,
      "step": 232600
    },
    {
      "epoch": 2.1233301700854077,
      "grad_norm": 3.795196533203125,
      "learning_rate": 4.8230558191595495e-05,
      "loss": 0.7286,
      "step": 232700
    },
    {
      "epoch": 2.124242645448573,
      "grad_norm": 4.337201118469238,
      "learning_rate": 4.8229797795459526e-05,
      "loss": 0.7426,
      "step": 232800
    },
    {
      "epoch": 2.125155120811738,
      "grad_norm": 4.06099796295166,
      "learning_rate": 4.822903739932355e-05,
      "loss": 0.7445,
      "step": 232900
    },
    {
      "epoch": 2.126067596174903,
      "grad_norm": 3.776902675628662,
      "learning_rate": 4.8228277003187586e-05,
      "loss": 0.7508,
      "step": 233000
    },
    {
      "epoch": 2.1269800715380685,
      "grad_norm": 5.036674499511719,
      "learning_rate": 4.822751660705161e-05,
      "loss": 0.7632,
      "step": 233100
    },
    {
      "epoch": 2.1278925469012338,
      "grad_norm": 4.921738624572754,
      "learning_rate": 4.822675621091564e-05,
      "loss": 0.7163,
      "step": 233200
    },
    {
      "epoch": 2.128805022264399,
      "grad_norm": 4.480890274047852,
      "learning_rate": 4.822599581477967e-05,
      "loss": 0.7533,
      "step": 233300
    },
    {
      "epoch": 2.129717497627564,
      "grad_norm": 4.463461875915527,
      "learning_rate": 4.82252354186437e-05,
      "loss": 0.7498,
      "step": 233400
    },
    {
      "epoch": 2.130629972990729,
      "grad_norm": 2.9914603233337402,
      "learning_rate": 4.822447502250772e-05,
      "loss": 0.6986,
      "step": 233500
    },
    {
      "epoch": 2.1315424483538945,
      "grad_norm": 4.071195602416992,
      "learning_rate": 4.822371462637176e-05,
      "loss": 0.7459,
      "step": 233600
    },
    {
      "epoch": 2.13245492371706,
      "grad_norm": 4.79766321182251,
      "learning_rate": 4.822295423023578e-05,
      "loss": 0.802,
      "step": 233700
    },
    {
      "epoch": 2.1333673990802247,
      "grad_norm": 4.453605651855469,
      "learning_rate": 4.822219383409981e-05,
      "loss": 0.7669,
      "step": 233800
    },
    {
      "epoch": 2.13427987444339,
      "grad_norm": 4.405754566192627,
      "learning_rate": 4.822143343796384e-05,
      "loss": 0.7646,
      "step": 233900
    },
    {
      "epoch": 2.1351923498065553,
      "grad_norm": 4.636214733123779,
      "learning_rate": 4.822067304182787e-05,
      "loss": 0.7426,
      "step": 234000
    },
    {
      "epoch": 2.1361048251697206,
      "grad_norm": 4.104055404663086,
      "learning_rate": 4.82199126456919e-05,
      "loss": 0.8062,
      "step": 234100
    },
    {
      "epoch": 2.1370173005328854,
      "grad_norm": 4.4452009201049805,
      "learning_rate": 4.821915224955593e-05,
      "loss": 0.7413,
      "step": 234200
    },
    {
      "epoch": 2.1379297758960507,
      "grad_norm": 4.048795700073242,
      "learning_rate": 4.8218391853419956e-05,
      "loss": 0.7271,
      "step": 234300
    },
    {
      "epoch": 2.138842251259216,
      "grad_norm": 4.368444919586182,
      "learning_rate": 4.821763145728399e-05,
      "loss": 0.7519,
      "step": 234400
    },
    {
      "epoch": 2.1397547266223813,
      "grad_norm": 3.7129266262054443,
      "learning_rate": 4.8216871061148016e-05,
      "loss": 0.744,
      "step": 234500
    },
    {
      "epoch": 2.140667201985546,
      "grad_norm": 4.5649518966674805,
      "learning_rate": 4.8216110665012046e-05,
      "loss": 0.7985,
      "step": 234600
    },
    {
      "epoch": 2.1415796773487115,
      "grad_norm": 4.258249282836914,
      "learning_rate": 4.8215350268876076e-05,
      "loss": 0.7484,
      "step": 234700
    },
    {
      "epoch": 2.1424921527118768,
      "grad_norm": 4.455859661102295,
      "learning_rate": 4.8214589872740107e-05,
      "loss": 0.7628,
      "step": 234800
    },
    {
      "epoch": 2.143404628075042,
      "grad_norm": 2.900106906890869,
      "learning_rate": 4.821382947660413e-05,
      "loss": 0.7827,
      "step": 234900
    },
    {
      "epoch": 2.1443171034382074,
      "grad_norm": 4.556654930114746,
      "learning_rate": 4.821306908046817e-05,
      "loss": 0.7723,
      "step": 235000
    },
    {
      "epoch": 2.145229578801372,
      "grad_norm": 4.1409711837768555,
      "learning_rate": 4.821230868433219e-05,
      "loss": 0.7784,
      "step": 235100
    },
    {
      "epoch": 2.1461420541645375,
      "grad_norm": 2.8526875972747803,
      "learning_rate": 4.821154828819622e-05,
      "loss": 0.7059,
      "step": 235200
    },
    {
      "epoch": 2.147054529527703,
      "grad_norm": 2.7624213695526123,
      "learning_rate": 4.821078789206025e-05,
      "loss": 0.7801,
      "step": 235300
    },
    {
      "epoch": 2.147967004890868,
      "grad_norm": 4.334150314331055,
      "learning_rate": 4.821002749592428e-05,
      "loss": 0.7889,
      "step": 235400
    },
    {
      "epoch": 2.148879480254033,
      "grad_norm": 4.533090591430664,
      "learning_rate": 4.820926709978831e-05,
      "loss": 0.7319,
      "step": 235500
    },
    {
      "epoch": 2.1497919556171983,
      "grad_norm": 4.5810112953186035,
      "learning_rate": 4.820850670365234e-05,
      "loss": 0.796,
      "step": 235600
    },
    {
      "epoch": 2.1507044309803636,
      "grad_norm": 4.321965217590332,
      "learning_rate": 4.8207746307516364e-05,
      "loss": 0.7198,
      "step": 235700
    },
    {
      "epoch": 2.151616906343529,
      "grad_norm": 5.4777750968933105,
      "learning_rate": 4.8206985911380394e-05,
      "loss": 0.7327,
      "step": 235800
    },
    {
      "epoch": 2.1525293817066937,
      "grad_norm": 4.350769996643066,
      "learning_rate": 4.8206225515244424e-05,
      "loss": 0.7559,
      "step": 235900
    },
    {
      "epoch": 2.153441857069859,
      "grad_norm": 5.0255913734436035,
      "learning_rate": 4.8205465119108454e-05,
      "loss": 0.7214,
      "step": 236000
    },
    {
      "epoch": 2.1543543324330243,
      "grad_norm": 4.568368434906006,
      "learning_rate": 4.8204704722972484e-05,
      "loss": 0.7522,
      "step": 236100
    },
    {
      "epoch": 2.1552668077961896,
      "grad_norm": 5.148994445800781,
      "learning_rate": 4.820394432683651e-05,
      "loss": 0.7707,
      "step": 236200
    },
    {
      "epoch": 2.1561792831593545,
      "grad_norm": 4.56697940826416,
      "learning_rate": 4.820318393070054e-05,
      "loss": 0.7803,
      "step": 236300
    },
    {
      "epoch": 2.1570917585225198,
      "grad_norm": 4.35698127746582,
      "learning_rate": 4.820242353456457e-05,
      "loss": 0.7123,
      "step": 236400
    },
    {
      "epoch": 2.158004233885685,
      "grad_norm": 4.219667911529541,
      "learning_rate": 4.82016631384286e-05,
      "loss": 0.7672,
      "step": 236500
    },
    {
      "epoch": 2.1589167092488504,
      "grad_norm": 4.160414695739746,
      "learning_rate": 4.820090274229263e-05,
      "loss": 0.754,
      "step": 236600
    },
    {
      "epoch": 2.1598291846120157,
      "grad_norm": 4.415648937225342,
      "learning_rate": 4.820014234615666e-05,
      "loss": 0.7769,
      "step": 236700
    },
    {
      "epoch": 2.1607416599751805,
      "grad_norm": 4.882623195648193,
      "learning_rate": 4.819938195002068e-05,
      "loss": 0.7589,
      "step": 236800
    },
    {
      "epoch": 2.161654135338346,
      "grad_norm": 4.59699821472168,
      "learning_rate": 4.819862155388472e-05,
      "loss": 0.7796,
      "step": 236900
    },
    {
      "epoch": 2.162566610701511,
      "grad_norm": 3.9639697074890137,
      "learning_rate": 4.819786115774874e-05,
      "loss": 0.735,
      "step": 237000
    },
    {
      "epoch": 2.1634790860646764,
      "grad_norm": 3.5678114891052246,
      "learning_rate": 4.819710076161277e-05,
      "loss": 0.7345,
      "step": 237100
    },
    {
      "epoch": 2.1643915614278413,
      "grad_norm": 4.175538539886475,
      "learning_rate": 4.81963403654768e-05,
      "loss": 0.7494,
      "step": 237200
    },
    {
      "epoch": 2.1653040367910066,
      "grad_norm": 3.5612292289733887,
      "learning_rate": 4.819557996934083e-05,
      "loss": 0.7608,
      "step": 237300
    },
    {
      "epoch": 2.166216512154172,
      "grad_norm": 3.918041706085205,
      "learning_rate": 4.819481957320486e-05,
      "loss": 0.7329,
      "step": 237400
    },
    {
      "epoch": 2.167128987517337,
      "grad_norm": 3.991248369216919,
      "learning_rate": 4.819405917706889e-05,
      "loss": 0.7342,
      "step": 237500
    },
    {
      "epoch": 2.168041462880502,
      "grad_norm": 4.2018609046936035,
      "learning_rate": 4.8193298780932915e-05,
      "loss": 0.7218,
      "step": 237600
    },
    {
      "epoch": 2.1689539382436673,
      "grad_norm": 3.6165456771850586,
      "learning_rate": 4.819253838479695e-05,
      "loss": 0.7656,
      "step": 237700
    },
    {
      "epoch": 2.1698664136068326,
      "grad_norm": 3.926910877227783,
      "learning_rate": 4.8191777988660975e-05,
      "loss": 0.7214,
      "step": 237800
    },
    {
      "epoch": 2.170778888969998,
      "grad_norm": 3.9072787761688232,
      "learning_rate": 4.8191017592525005e-05,
      "loss": 0.7779,
      "step": 237900
    },
    {
      "epoch": 2.171691364333163,
      "grad_norm": 4.507026195526123,
      "learning_rate": 4.8190257196389035e-05,
      "loss": 0.7221,
      "step": 238000
    },
    {
      "epoch": 2.172603839696328,
      "grad_norm": 4.535642147064209,
      "learning_rate": 4.8189496800253065e-05,
      "loss": 0.7564,
      "step": 238100
    },
    {
      "epoch": 2.1735163150594934,
      "grad_norm": 3.7950563430786133,
      "learning_rate": 4.818873640411709e-05,
      "loss": 0.7845,
      "step": 238200
    },
    {
      "epoch": 2.1744287904226587,
      "grad_norm": 3.761503219604492,
      "learning_rate": 4.8187976007981125e-05,
      "loss": 0.7726,
      "step": 238300
    },
    {
      "epoch": 2.175341265785824,
      "grad_norm": 3.8555309772491455,
      "learning_rate": 4.818721561184515e-05,
      "loss": 0.7664,
      "step": 238400
    },
    {
      "epoch": 2.176253741148989,
      "grad_norm": 4.170435428619385,
      "learning_rate": 4.818645521570918e-05,
      "loss": 0.7352,
      "step": 238500
    },
    {
      "epoch": 2.177166216512154,
      "grad_norm": 4.901576519012451,
      "learning_rate": 4.818569481957321e-05,
      "loss": 0.762,
      "step": 238600
    },
    {
      "epoch": 2.1780786918753194,
      "grad_norm": 4.571393966674805,
      "learning_rate": 4.818493442343723e-05,
      "loss": 0.7439,
      "step": 238700
    },
    {
      "epoch": 2.1789911672384847,
      "grad_norm": 4.2294135093688965,
      "learning_rate": 4.818417402730127e-05,
      "loss": 0.7572,
      "step": 238800
    },
    {
      "epoch": 2.1799036426016496,
      "grad_norm": 4.292881011962891,
      "learning_rate": 4.818341363116529e-05,
      "loss": 0.7847,
      "step": 238900
    },
    {
      "epoch": 2.180816117964815,
      "grad_norm": 3.856762170791626,
      "learning_rate": 4.818265323502932e-05,
      "loss": 0.7373,
      "step": 239000
    },
    {
      "epoch": 2.18172859332798,
      "grad_norm": 4.745737552642822,
      "learning_rate": 4.818189283889335e-05,
      "loss": 0.793,
      "step": 239100
    },
    {
      "epoch": 2.1826410686911455,
      "grad_norm": 4.258452415466309,
      "learning_rate": 4.818113244275738e-05,
      "loss": 0.7564,
      "step": 239200
    },
    {
      "epoch": 2.1835535440543103,
      "grad_norm": 4.851417541503906,
      "learning_rate": 4.8180372046621405e-05,
      "loss": 0.7315,
      "step": 239300
    },
    {
      "epoch": 2.1844660194174756,
      "grad_norm": 4.491654396057129,
      "learning_rate": 4.817961165048544e-05,
      "loss": 0.7269,
      "step": 239400
    },
    {
      "epoch": 2.185378494780641,
      "grad_norm": 2.9144222736358643,
      "learning_rate": 4.8178851254349466e-05,
      "loss": 0.7176,
      "step": 239500
    },
    {
      "epoch": 2.1862909701438062,
      "grad_norm": 4.994063377380371,
      "learning_rate": 4.8178090858213496e-05,
      "loss": 0.7617,
      "step": 239600
    },
    {
      "epoch": 2.187203445506971,
      "grad_norm": 3.876164436340332,
      "learning_rate": 4.8177330462077526e-05,
      "loss": 0.7899,
      "step": 239700
    },
    {
      "epoch": 2.1881159208701364,
      "grad_norm": 3.8771004676818848,
      "learning_rate": 4.8176570065941556e-05,
      "loss": 0.7616,
      "step": 239800
    },
    {
      "epoch": 2.1890283962333017,
      "grad_norm": 3.9436652660369873,
      "learning_rate": 4.8175809669805586e-05,
      "loss": 0.7496,
      "step": 239900
    },
    {
      "epoch": 2.189940871596467,
      "grad_norm": 4.511953830718994,
      "learning_rate": 4.8175049273669616e-05,
      "loss": 0.7216,
      "step": 240000
    },
    {
      "epoch": 2.1908533469596323,
      "grad_norm": 3.9553582668304443,
      "learning_rate": 4.817428887753364e-05,
      "loss": 0.7424,
      "step": 240100
    },
    {
      "epoch": 2.191765822322797,
      "grad_norm": 4.655506610870361,
      "learning_rate": 4.8173528481397676e-05,
      "loss": 0.7952,
      "step": 240200
    },
    {
      "epoch": 2.1926782976859625,
      "grad_norm": 3.753754138946533,
      "learning_rate": 4.81727680852617e-05,
      "loss": 0.7652,
      "step": 240300
    },
    {
      "epoch": 2.1935907730491278,
      "grad_norm": 4.684070587158203,
      "learning_rate": 4.817200768912573e-05,
      "loss": 0.7884,
      "step": 240400
    },
    {
      "epoch": 2.194503248412293,
      "grad_norm": 4.319766521453857,
      "learning_rate": 4.817124729298976e-05,
      "loss": 0.77,
      "step": 240500
    },
    {
      "epoch": 2.195415723775458,
      "grad_norm": 4.3871588706970215,
      "learning_rate": 4.817048689685379e-05,
      "loss": 0.7479,
      "step": 240600
    },
    {
      "epoch": 2.196328199138623,
      "grad_norm": 4.427750110626221,
      "learning_rate": 4.816972650071781e-05,
      "loss": 0.7807,
      "step": 240700
    },
    {
      "epoch": 2.1972406745017885,
      "grad_norm": 3.323963165283203,
      "learning_rate": 4.816896610458185e-05,
      "loss": 0.7964,
      "step": 240800
    },
    {
      "epoch": 2.198153149864954,
      "grad_norm": 3.8786532878875732,
      "learning_rate": 4.816820570844587e-05,
      "loss": 0.7672,
      "step": 240900
    },
    {
      "epoch": 2.1990656252281187,
      "grad_norm": 3.2301483154296875,
      "learning_rate": 4.81674453123099e-05,
      "loss": 0.7374,
      "step": 241000
    },
    {
      "epoch": 2.199978100591284,
      "grad_norm": 3.4716625213623047,
      "learning_rate": 4.816668491617393e-05,
      "loss": 0.7354,
      "step": 241100
    },
    {
      "epoch": 2.2008905759544493,
      "grad_norm": 4.532175540924072,
      "learning_rate": 4.816592452003796e-05,
      "loss": 0.7712,
      "step": 241200
    },
    {
      "epoch": 2.2018030513176146,
      "grad_norm": 4.064497470855713,
      "learning_rate": 4.816516412390199e-05,
      "loss": 0.7492,
      "step": 241300
    },
    {
      "epoch": 2.2027155266807794,
      "grad_norm": 4.2461748123168945,
      "learning_rate": 4.816440372776602e-05,
      "loss": 0.7811,
      "step": 241400
    },
    {
      "epoch": 2.2036280020439447,
      "grad_norm": 4.214197158813477,
      "learning_rate": 4.8163643331630047e-05,
      "loss": 0.7453,
      "step": 241500
    },
    {
      "epoch": 2.20454047740711,
      "grad_norm": 4.078895568847656,
      "learning_rate": 4.8162882935494077e-05,
      "loss": 0.7739,
      "step": 241600
    },
    {
      "epoch": 2.2054529527702753,
      "grad_norm": 5.027469635009766,
      "learning_rate": 4.816212253935811e-05,
      "loss": 0.7397,
      "step": 241700
    },
    {
      "epoch": 2.2063654281334406,
      "grad_norm": 3.6461877822875977,
      "learning_rate": 4.816136214322213e-05,
      "loss": 0.7682,
      "step": 241800
    },
    {
      "epoch": 2.2072779034966055,
      "grad_norm": 3.182164192199707,
      "learning_rate": 4.816060174708617e-05,
      "loss": 0.7574,
      "step": 241900
    },
    {
      "epoch": 2.2081903788597708,
      "grad_norm": 4.8426666259765625,
      "learning_rate": 4.815984135095019e-05,
      "loss": 0.749,
      "step": 242000
    },
    {
      "epoch": 2.209102854222936,
      "grad_norm": 3.9123570919036865,
      "learning_rate": 4.815908095481422e-05,
      "loss": 0.7408,
      "step": 242100
    },
    {
      "epoch": 2.2100153295861014,
      "grad_norm": 4.096908092498779,
      "learning_rate": 4.815832055867825e-05,
      "loss": 0.7442,
      "step": 242200
    },
    {
      "epoch": 2.210927804949266,
      "grad_norm": 4.594240188598633,
      "learning_rate": 4.815756016254228e-05,
      "loss": 0.7859,
      "step": 242300
    },
    {
      "epoch": 2.2118402803124315,
      "grad_norm": 4.2409515380859375,
      "learning_rate": 4.815679976640631e-05,
      "loss": 0.7829,
      "step": 242400
    },
    {
      "epoch": 2.212752755675597,
      "grad_norm": 4.137463092803955,
      "learning_rate": 4.815603937027034e-05,
      "loss": 0.7561,
      "step": 242500
    },
    {
      "epoch": 2.213665231038762,
      "grad_norm": 4.290122985839844,
      "learning_rate": 4.8155278974134364e-05,
      "loss": 0.7392,
      "step": 242600
    },
    {
      "epoch": 2.214577706401927,
      "grad_norm": 3.4522171020507812,
      "learning_rate": 4.81545185779984e-05,
      "loss": 0.76,
      "step": 242700
    },
    {
      "epoch": 2.2154901817650923,
      "grad_norm": 3.4456675052642822,
      "learning_rate": 4.8153758181862424e-05,
      "loss": 0.7662,
      "step": 242800
    },
    {
      "epoch": 2.2164026571282576,
      "grad_norm": 4.571401596069336,
      "learning_rate": 4.8152997785726454e-05,
      "loss": 0.7635,
      "step": 242900
    },
    {
      "epoch": 2.217315132491423,
      "grad_norm": 4.062681198120117,
      "learning_rate": 4.8152237389590484e-05,
      "loss": 0.7506,
      "step": 243000
    },
    {
      "epoch": 2.2182276078545877,
      "grad_norm": 4.1260504722595215,
      "learning_rate": 4.8151476993454514e-05,
      "loss": 0.7624,
      "step": 243100
    },
    {
      "epoch": 2.219140083217753,
      "grad_norm": 3.790408134460449,
      "learning_rate": 4.815071659731854e-05,
      "loss": 0.7864,
      "step": 243200
    },
    {
      "epoch": 2.2200525585809183,
      "grad_norm": 3.9170377254486084,
      "learning_rate": 4.8149956201182574e-05,
      "loss": 0.768,
      "step": 243300
    },
    {
      "epoch": 2.2209650339440836,
      "grad_norm": 4.779740333557129,
      "learning_rate": 4.81491958050466e-05,
      "loss": 0.7896,
      "step": 243400
    },
    {
      "epoch": 2.221877509307249,
      "grad_norm": 3.9985241889953613,
      "learning_rate": 4.814843540891063e-05,
      "loss": 0.7528,
      "step": 243500
    },
    {
      "epoch": 2.2227899846704138,
      "grad_norm": 4.663448810577393,
      "learning_rate": 4.814767501277466e-05,
      "loss": 0.7578,
      "step": 243600
    },
    {
      "epoch": 2.223702460033579,
      "grad_norm": 4.428705215454102,
      "learning_rate": 4.814691461663869e-05,
      "loss": 0.7596,
      "step": 243700
    },
    {
      "epoch": 2.2246149353967444,
      "grad_norm": 3.910759449005127,
      "learning_rate": 4.814615422050272e-05,
      "loss": 0.7627,
      "step": 243800
    },
    {
      "epoch": 2.2255274107599097,
      "grad_norm": 4.138478755950928,
      "learning_rate": 4.814539382436675e-05,
      "loss": 0.7371,
      "step": 243900
    },
    {
      "epoch": 2.2264398861230745,
      "grad_norm": 4.683233737945557,
      "learning_rate": 4.814463342823077e-05,
      "loss": 0.7451,
      "step": 244000
    },
    {
      "epoch": 2.22735236148624,
      "grad_norm": 3.8535876274108887,
      "learning_rate": 4.814387303209481e-05,
      "loss": 0.7906,
      "step": 244100
    },
    {
      "epoch": 2.228264836849405,
      "grad_norm": 4.28766393661499,
      "learning_rate": 4.814311263595883e-05,
      "loss": 0.7759,
      "step": 244200
    },
    {
      "epoch": 2.2291773122125704,
      "grad_norm": 4.505807399749756,
      "learning_rate": 4.8142352239822855e-05,
      "loss": 0.8047,
      "step": 244300
    },
    {
      "epoch": 2.2300897875757353,
      "grad_norm": 4.366114616394043,
      "learning_rate": 4.814159184368689e-05,
      "loss": 0.7502,
      "step": 244400
    },
    {
      "epoch": 2.2310022629389006,
      "grad_norm": 4.452751636505127,
      "learning_rate": 4.8140831447550915e-05,
      "loss": 0.7532,
      "step": 244500
    },
    {
      "epoch": 2.231914738302066,
      "grad_norm": 4.055555820465088,
      "learning_rate": 4.8140071051414945e-05,
      "loss": 0.7919,
      "step": 244600
    },
    {
      "epoch": 2.232827213665231,
      "grad_norm": 4.063518047332764,
      "learning_rate": 4.8139310655278975e-05,
      "loss": 0.7502,
      "step": 244700
    },
    {
      "epoch": 2.233739689028396,
      "grad_norm": 5.854366302490234,
      "learning_rate": 4.8138550259143005e-05,
      "loss": 0.7493,
      "step": 244800
    },
    {
      "epoch": 2.2346521643915613,
      "grad_norm": 4.031323432922363,
      "learning_rate": 4.8137789863007035e-05,
      "loss": 0.7469,
      "step": 244900
    },
    {
      "epoch": 2.2355646397547266,
      "grad_norm": 3.7214574813842773,
      "learning_rate": 4.8137029466871065e-05,
      "loss": 0.753,
      "step": 245000
    },
    {
      "epoch": 2.236477115117892,
      "grad_norm": 4.634091854095459,
      "learning_rate": 4.813626907073509e-05,
      "loss": 0.7609,
      "step": 245100
    },
    {
      "epoch": 2.2373895904810572,
      "grad_norm": 4.483202934265137,
      "learning_rate": 4.8135508674599125e-05,
      "loss": 0.7774,
      "step": 245200
    },
    {
      "epoch": 2.238302065844222,
      "grad_norm": 4.338894367218018,
      "learning_rate": 4.813474827846315e-05,
      "loss": 0.7447,
      "step": 245300
    },
    {
      "epoch": 2.2392145412073874,
      "grad_norm": 3.573911428451538,
      "learning_rate": 4.813398788232718e-05,
      "loss": 0.7374,
      "step": 245400
    },
    {
      "epoch": 2.2401270165705527,
      "grad_norm": 5.562335968017578,
      "learning_rate": 4.813322748619121e-05,
      "loss": 0.7362,
      "step": 245500
    },
    {
      "epoch": 2.241039491933718,
      "grad_norm": 4.48169469833374,
      "learning_rate": 4.813246709005524e-05,
      "loss": 0.7477,
      "step": 245600
    },
    {
      "epoch": 2.241951967296883,
      "grad_norm": 5.471565246582031,
      "learning_rate": 4.813170669391926e-05,
      "loss": 0.7661,
      "step": 245700
    },
    {
      "epoch": 2.242864442660048,
      "grad_norm": 4.018054485321045,
      "learning_rate": 4.81309462977833e-05,
      "loss": 0.7507,
      "step": 245800
    },
    {
      "epoch": 2.2437769180232134,
      "grad_norm": 3.9847285747528076,
      "learning_rate": 4.813018590164732e-05,
      "loss": 0.8029,
      "step": 245900
    },
    {
      "epoch": 2.2446893933863787,
      "grad_norm": 6.193204402923584,
      "learning_rate": 4.812942550551135e-05,
      "loss": 0.8163,
      "step": 246000
    },
    {
      "epoch": 2.2456018687495436,
      "grad_norm": 4.800114154815674,
      "learning_rate": 4.812866510937538e-05,
      "loss": 0.7495,
      "step": 246100
    },
    {
      "epoch": 2.246514344112709,
      "grad_norm": 4.535317897796631,
      "learning_rate": 4.812790471323941e-05,
      "loss": 0.7576,
      "step": 246200
    },
    {
      "epoch": 2.247426819475874,
      "grad_norm": 4.107850074768066,
      "learning_rate": 4.812714431710344e-05,
      "loss": 0.7848,
      "step": 246300
    },
    {
      "epoch": 2.2483392948390395,
      "grad_norm": 5.29035758972168,
      "learning_rate": 4.812638392096747e-05,
      "loss": 0.7489,
      "step": 246400
    },
    {
      "epoch": 2.2492517702022043,
      "grad_norm": 4.343538284301758,
      "learning_rate": 4.8125623524831496e-05,
      "loss": 0.7266,
      "step": 246500
    },
    {
      "epoch": 2.2501642455653696,
      "grad_norm": 5.33819580078125,
      "learning_rate": 4.812486312869553e-05,
      "loss": 0.7556,
      "step": 246600
    },
    {
      "epoch": 2.251076720928535,
      "grad_norm": 4.434078693389893,
      "learning_rate": 4.8124102732559556e-05,
      "loss": 0.7405,
      "step": 246700
    },
    {
      "epoch": 2.2519891962917002,
      "grad_norm": 4.3939080238342285,
      "learning_rate": 4.8123342336423586e-05,
      "loss": 0.7414,
      "step": 246800
    },
    {
      "epoch": 2.2529016716548655,
      "grad_norm": 4.765137195587158,
      "learning_rate": 4.8122581940287616e-05,
      "loss": 0.7623,
      "step": 246900
    },
    {
      "epoch": 2.2538141470180304,
      "grad_norm": 4.156924247741699,
      "learning_rate": 4.8121821544151646e-05,
      "loss": 0.7503,
      "step": 247000
    },
    {
      "epoch": 2.2547266223811957,
      "grad_norm": 4.240808486938477,
      "learning_rate": 4.812106114801567e-05,
      "loss": 0.7662,
      "step": 247100
    },
    {
      "epoch": 2.255639097744361,
      "grad_norm": 4.530921459197998,
      "learning_rate": 4.81203007518797e-05,
      "loss": 0.7327,
      "step": 247200
    },
    {
      "epoch": 2.256551573107526,
      "grad_norm": 4.29808235168457,
      "learning_rate": 4.811954035574373e-05,
      "loss": 0.7082,
      "step": 247300
    },
    {
      "epoch": 2.257464048470691,
      "grad_norm": 4.5872673988342285,
      "learning_rate": 4.811877995960776e-05,
      "loss": 0.7579,
      "step": 247400
    },
    {
      "epoch": 2.2583765238338565,
      "grad_norm": 4.355148792266846,
      "learning_rate": 4.811801956347179e-05,
      "loss": 0.7859,
      "step": 247500
    },
    {
      "epoch": 2.2592889991970218,
      "grad_norm": 4.71502161026001,
      "learning_rate": 4.811725916733581e-05,
      "loss": 0.7699,
      "step": 247600
    },
    {
      "epoch": 2.260201474560187,
      "grad_norm": 3.6959550380706787,
      "learning_rate": 4.811649877119985e-05,
      "loss": 0.7473,
      "step": 247700
    },
    {
      "epoch": 2.261113949923352,
      "grad_norm": 4.095958709716797,
      "learning_rate": 4.811573837506387e-05,
      "loss": 0.7443,
      "step": 247800
    },
    {
      "epoch": 2.262026425286517,
      "grad_norm": 4.1839447021484375,
      "learning_rate": 4.81149779789279e-05,
      "loss": 0.7559,
      "step": 247900
    },
    {
      "epoch": 2.2629389006496825,
      "grad_norm": 4.196472644805908,
      "learning_rate": 4.811421758279193e-05,
      "loss": 0.7933,
      "step": 248000
    },
    {
      "epoch": 2.263851376012848,
      "grad_norm": 4.230500221252441,
      "learning_rate": 4.811345718665596e-05,
      "loss": 0.7412,
      "step": 248100
    },
    {
      "epoch": 2.2647638513760127,
      "grad_norm": 3.5250775814056396,
      "learning_rate": 4.811269679051999e-05,
      "loss": 0.7554,
      "step": 248200
    },
    {
      "epoch": 2.265676326739178,
      "grad_norm": 3.9093003273010254,
      "learning_rate": 4.811193639438402e-05,
      "loss": 0.7543,
      "step": 248300
    },
    {
      "epoch": 2.2665888021023433,
      "grad_norm": 3.9951601028442383,
      "learning_rate": 4.811117599824805e-05,
      "loss": 0.718,
      "step": 248400
    },
    {
      "epoch": 2.2675012774655086,
      "grad_norm": 3.7301688194274902,
      "learning_rate": 4.811041560211208e-05,
      "loss": 0.7612,
      "step": 248500
    },
    {
      "epoch": 2.268413752828674,
      "grad_norm": 4.458370208740234,
      "learning_rate": 4.810965520597611e-05,
      "loss": 0.7592,
      "step": 248600
    },
    {
      "epoch": 2.2693262281918387,
      "grad_norm": 3.3712661266326904,
      "learning_rate": 4.810889480984014e-05,
      "loss": 0.7766,
      "step": 248700
    },
    {
      "epoch": 2.270238703555004,
      "grad_norm": 4.427315711975098,
      "learning_rate": 4.810813441370417e-05,
      "loss": 0.7692,
      "step": 248800
    },
    {
      "epoch": 2.2711511789181693,
      "grad_norm": 4.60746431350708,
      "learning_rate": 4.81073740175682e-05,
      "loss": 0.7856,
      "step": 248900
    },
    {
      "epoch": 2.272063654281334,
      "grad_norm": 4.15381383895874,
      "learning_rate": 4.810661362143222e-05,
      "loss": 0.7779,
      "step": 249000
    },
    {
      "epoch": 2.2729761296444995,
      "grad_norm": 4.143871307373047,
      "learning_rate": 4.810585322529626e-05,
      "loss": 0.722,
      "step": 249100
    },
    {
      "epoch": 2.2738886050076648,
      "grad_norm": 3.4530746936798096,
      "learning_rate": 4.810509282916028e-05,
      "loss": 0.7521,
      "step": 249200
    },
    {
      "epoch": 2.27480108037083,
      "grad_norm": 5.440834999084473,
      "learning_rate": 4.810433243302431e-05,
      "loss": 0.8166,
      "step": 249300
    },
    {
      "epoch": 2.2757135557339954,
      "grad_norm": 4.513769626617432,
      "learning_rate": 4.810357203688834e-05,
      "loss": 0.7524,
      "step": 249400
    },
    {
      "epoch": 2.27662603109716,
      "grad_norm": 4.148751258850098,
      "learning_rate": 4.810281164075237e-05,
      "loss": 0.7669,
      "step": 249500
    },
    {
      "epoch": 2.2775385064603255,
      "grad_norm": 4.277956485748291,
      "learning_rate": 4.81020512446164e-05,
      "loss": 0.7875,
      "step": 249600
    },
    {
      "epoch": 2.278450981823491,
      "grad_norm": 3.4799575805664062,
      "learning_rate": 4.810129084848043e-05,
      "loss": 0.707,
      "step": 249700
    },
    {
      "epoch": 2.279363457186656,
      "grad_norm": 4.2630414962768555,
      "learning_rate": 4.8100530452344454e-05,
      "loss": 0.7157,
      "step": 249800
    },
    {
      "epoch": 2.280275932549821,
      "grad_norm": 3.8566226959228516,
      "learning_rate": 4.8099770056208484e-05,
      "loss": 0.7722,
      "step": 249900
    },
    {
      "epoch": 2.2811884079129863,
      "grad_norm": 4.414796829223633,
      "learning_rate": 4.8099009660072514e-05,
      "loss": 0.7687,
      "step": 250000
    },
    {
      "epoch": 2.2821008832761516,
      "grad_norm": 3.497415542602539,
      "learning_rate": 4.809824926393654e-05,
      "loss": 0.7862,
      "step": 250100
    },
    {
      "epoch": 2.283013358639317,
      "grad_norm": 3.399224042892456,
      "learning_rate": 4.8097488867800574e-05,
      "loss": 0.7575,
      "step": 250200
    },
    {
      "epoch": 2.283925834002482,
      "grad_norm": 4.518787860870361,
      "learning_rate": 4.80967284716646e-05,
      "loss": 0.7581,
      "step": 250300
    },
    {
      "epoch": 2.284838309365647,
      "grad_norm": 3.9282259941101074,
      "learning_rate": 4.809596807552863e-05,
      "loss": 0.7677,
      "step": 250400
    },
    {
      "epoch": 2.2857507847288123,
      "grad_norm": 4.14605712890625,
      "learning_rate": 4.809520767939266e-05,
      "loss": 0.7351,
      "step": 250500
    },
    {
      "epoch": 2.2866632600919776,
      "grad_norm": 4.136803150177002,
      "learning_rate": 4.809444728325669e-05,
      "loss": 0.7484,
      "step": 250600
    },
    {
      "epoch": 2.2875757354551425,
      "grad_norm": 3.7993571758270264,
      "learning_rate": 4.809368688712072e-05,
      "loss": 0.754,
      "step": 250700
    },
    {
      "epoch": 2.2884882108183078,
      "grad_norm": 3.4191198348999023,
      "learning_rate": 4.809292649098475e-05,
      "loss": 0.7239,
      "step": 250800
    },
    {
      "epoch": 2.289400686181473,
      "grad_norm": 4.862163543701172,
      "learning_rate": 4.809216609484877e-05,
      "loss": 0.728,
      "step": 250900
    },
    {
      "epoch": 2.2903131615446384,
      "grad_norm": 4.156049728393555,
      "learning_rate": 4.809140569871281e-05,
      "loss": 0.7132,
      "step": 251000
    },
    {
      "epoch": 2.2912256369078037,
      "grad_norm": 3.6317968368530273,
      "learning_rate": 4.809064530257683e-05,
      "loss": 0.7567,
      "step": 251100
    },
    {
      "epoch": 2.2921381122709685,
      "grad_norm": 3.873749017715454,
      "learning_rate": 4.808988490644086e-05,
      "loss": 0.7308,
      "step": 251200
    },
    {
      "epoch": 2.293050587634134,
      "grad_norm": 3.9993770122528076,
      "learning_rate": 4.808912451030489e-05,
      "loss": 0.775,
      "step": 251300
    },
    {
      "epoch": 2.293963062997299,
      "grad_norm": 4.0276408195495605,
      "learning_rate": 4.808836411416892e-05,
      "loss": 0.7233,
      "step": 251400
    },
    {
      "epoch": 2.2948755383604644,
      "grad_norm": 4.139179229736328,
      "learning_rate": 4.8087603718032945e-05,
      "loss": 0.7485,
      "step": 251500
    },
    {
      "epoch": 2.2957880137236293,
      "grad_norm": 4.44352388381958,
      "learning_rate": 4.808684332189698e-05,
      "loss": 0.7252,
      "step": 251600
    },
    {
      "epoch": 2.2967004890867946,
      "grad_norm": 5.359883785247803,
      "learning_rate": 4.8086082925761005e-05,
      "loss": 0.7585,
      "step": 251700
    },
    {
      "epoch": 2.29761296444996,
      "grad_norm": 4.366308212280273,
      "learning_rate": 4.8085322529625035e-05,
      "loss": 0.7501,
      "step": 251800
    },
    {
      "epoch": 2.298525439813125,
      "grad_norm": 4.289387226104736,
      "learning_rate": 4.8084562133489065e-05,
      "loss": 0.7329,
      "step": 251900
    },
    {
      "epoch": 2.2994379151762905,
      "grad_norm": 4.399377822875977,
      "learning_rate": 4.8083801737353095e-05,
      "loss": 0.7516,
      "step": 252000
    },
    {
      "epoch": 2.3003503905394553,
      "grad_norm": 4.566176891326904,
      "learning_rate": 4.8083041341217125e-05,
      "loss": 0.7438,
      "step": 252100
    },
    {
      "epoch": 2.3012628659026206,
      "grad_norm": 3.4120752811431885,
      "learning_rate": 4.8082280945081155e-05,
      "loss": 0.7416,
      "step": 252200
    },
    {
      "epoch": 2.302175341265786,
      "grad_norm": 5.468228340148926,
      "learning_rate": 4.808152054894518e-05,
      "loss": 0.7416,
      "step": 252300
    },
    {
      "epoch": 2.303087816628951,
      "grad_norm": 4.2433576583862305,
      "learning_rate": 4.8080760152809215e-05,
      "loss": 0.7256,
      "step": 252400
    },
    {
      "epoch": 2.304000291992116,
      "grad_norm": 4.958112716674805,
      "learning_rate": 4.807999975667324e-05,
      "loss": 0.7558,
      "step": 252500
    },
    {
      "epoch": 2.3049127673552814,
      "grad_norm": 4.0313520431518555,
      "learning_rate": 4.807923936053727e-05,
      "loss": 0.7539,
      "step": 252600
    },
    {
      "epoch": 2.3058252427184467,
      "grad_norm": 4.504649639129639,
      "learning_rate": 4.80784789644013e-05,
      "loss": 0.7507,
      "step": 252700
    },
    {
      "epoch": 2.306737718081612,
      "grad_norm": 4.055449962615967,
      "learning_rate": 4.807771856826532e-05,
      "loss": 0.7372,
      "step": 252800
    },
    {
      "epoch": 2.307650193444777,
      "grad_norm": 4.61016321182251,
      "learning_rate": 4.807695817212935e-05,
      "loss": 0.7849,
      "step": 252900
    },
    {
      "epoch": 2.308562668807942,
      "grad_norm": 4.777284622192383,
      "learning_rate": 4.807619777599338e-05,
      "loss": 0.7398,
      "step": 253000
    },
    {
      "epoch": 2.3094751441711074,
      "grad_norm": 4.310007572174072,
      "learning_rate": 4.807543737985741e-05,
      "loss": 0.7467,
      "step": 253100
    },
    {
      "epoch": 2.3103876195342727,
      "grad_norm": 3.1425368785858154,
      "learning_rate": 4.807467698372144e-05,
      "loss": 0.7481,
      "step": 253200
    },
    {
      "epoch": 2.3113000948974376,
      "grad_norm": 3.90972638130188,
      "learning_rate": 4.807391658758547e-05,
      "loss": 0.7456,
      "step": 253300
    },
    {
      "epoch": 2.312212570260603,
      "grad_norm": 3.4132752418518066,
      "learning_rate": 4.8073156191449496e-05,
      "loss": 0.7116,
      "step": 253400
    },
    {
      "epoch": 2.313125045623768,
      "grad_norm": 3.9118809700012207,
      "learning_rate": 4.807239579531353e-05,
      "loss": 0.7357,
      "step": 253500
    },
    {
      "epoch": 2.3140375209869335,
      "grad_norm": 3.52961802482605,
      "learning_rate": 4.8071635399177556e-05,
      "loss": 0.7564,
      "step": 253600
    },
    {
      "epoch": 2.314949996350099,
      "grad_norm": 4.387348175048828,
      "learning_rate": 4.8070875003041586e-05,
      "loss": 0.7697,
      "step": 253700
    },
    {
      "epoch": 2.3158624717132636,
      "grad_norm": 4.647317886352539,
      "learning_rate": 4.8070114606905616e-05,
      "loss": 0.7493,
      "step": 253800
    },
    {
      "epoch": 2.316774947076429,
      "grad_norm": 3.6666088104248047,
      "learning_rate": 4.8069354210769646e-05,
      "loss": 0.7717,
      "step": 253900
    },
    {
      "epoch": 2.3176874224395942,
      "grad_norm": 4.3730950355529785,
      "learning_rate": 4.806859381463367e-05,
      "loss": 0.7701,
      "step": 254000
    },
    {
      "epoch": 2.318599897802759,
      "grad_norm": 4.311970233917236,
      "learning_rate": 4.8067833418497706e-05,
      "loss": 0.7833,
      "step": 254100
    },
    {
      "epoch": 2.3195123731659244,
      "grad_norm": 4.708405494689941,
      "learning_rate": 4.806707302236173e-05,
      "loss": 0.733,
      "step": 254200
    },
    {
      "epoch": 2.3204248485290897,
      "grad_norm": 4.131937503814697,
      "learning_rate": 4.806631262622576e-05,
      "loss": 0.7615,
      "step": 254300
    },
    {
      "epoch": 2.321337323892255,
      "grad_norm": 3.0204622745513916,
      "learning_rate": 4.806555223008979e-05,
      "loss": 0.764,
      "step": 254400
    },
    {
      "epoch": 2.3222497992554203,
      "grad_norm": 2.824643135070801,
      "learning_rate": 4.806479183395382e-05,
      "loss": 0.7311,
      "step": 254500
    },
    {
      "epoch": 2.323162274618585,
      "grad_norm": 4.5384626388549805,
      "learning_rate": 4.806403143781785e-05,
      "loss": 0.748,
      "step": 254600
    },
    {
      "epoch": 2.3240747499817505,
      "grad_norm": 4.294737339019775,
      "learning_rate": 4.806327104168188e-05,
      "loss": 0.7771,
      "step": 254700
    },
    {
      "epoch": 2.3249872253449158,
      "grad_norm": 4.347240924835205,
      "learning_rate": 4.80625106455459e-05,
      "loss": 0.7134,
      "step": 254800
    },
    {
      "epoch": 2.325899700708081,
      "grad_norm": 4.843472957611084,
      "learning_rate": 4.806175024940994e-05,
      "loss": 0.741,
      "step": 254900
    },
    {
      "epoch": 2.326812176071246,
      "grad_norm": 4.97662353515625,
      "learning_rate": 4.806098985327396e-05,
      "loss": 0.728,
      "step": 255000
    },
    {
      "epoch": 2.327724651434411,
      "grad_norm": 5.246923446655273,
      "learning_rate": 4.806022945713799e-05,
      "loss": 0.7776,
      "step": 255100
    },
    {
      "epoch": 2.3286371267975765,
      "grad_norm": 4.297691345214844,
      "learning_rate": 4.8059469061002023e-05,
      "loss": 0.7719,
      "step": 255200
    },
    {
      "epoch": 2.329549602160742,
      "grad_norm": 4.728734016418457,
      "learning_rate": 4.8058708664866053e-05,
      "loss": 0.7666,
      "step": 255300
    },
    {
      "epoch": 2.3304620775239067,
      "grad_norm": 4.029086112976074,
      "learning_rate": 4.805794826873008e-05,
      "loss": 0.7633,
      "step": 255400
    },
    {
      "epoch": 2.331374552887072,
      "grad_norm": 3.5490987300872803,
      "learning_rate": 4.8057187872594114e-05,
      "loss": 0.7205,
      "step": 255500
    },
    {
      "epoch": 2.3322870282502373,
      "grad_norm": 4.473068714141846,
      "learning_rate": 4.805642747645814e-05,
      "loss": 0.7492,
      "step": 255600
    },
    {
      "epoch": 2.3331995036134026,
      "grad_norm": 4.254889488220215,
      "learning_rate": 4.805566708032217e-05,
      "loss": 0.7645,
      "step": 255700
    },
    {
      "epoch": 2.3341119789765674,
      "grad_norm": 4.641648292541504,
      "learning_rate": 4.80549066841862e-05,
      "loss": 0.7719,
      "step": 255800
    },
    {
      "epoch": 2.3350244543397327,
      "grad_norm": 3.964651346206665,
      "learning_rate": 4.805414628805022e-05,
      "loss": 0.7181,
      "step": 255900
    },
    {
      "epoch": 2.335936929702898,
      "grad_norm": 4.869555473327637,
      "learning_rate": 4.805338589191426e-05,
      "loss": 0.7524,
      "step": 256000
    },
    {
      "epoch": 2.3368494050660633,
      "grad_norm": 3.9861626625061035,
      "learning_rate": 4.805262549577828e-05,
      "loss": 0.7538,
      "step": 256100
    },
    {
      "epoch": 2.3377618804292286,
      "grad_norm": 4.348130702972412,
      "learning_rate": 4.805186509964231e-05,
      "loss": 0.7366,
      "step": 256200
    },
    {
      "epoch": 2.3386743557923935,
      "grad_norm": 3.296858310699463,
      "learning_rate": 4.805110470350634e-05,
      "loss": 0.7408,
      "step": 256300
    },
    {
      "epoch": 2.3395868311555588,
      "grad_norm": 4.614160060882568,
      "learning_rate": 4.805034430737037e-05,
      "loss": 0.7566,
      "step": 256400
    },
    {
      "epoch": 2.340499306518724,
      "grad_norm": 4.06968879699707,
      "learning_rate": 4.8049583911234394e-05,
      "loss": 0.7404,
      "step": 256500
    },
    {
      "epoch": 2.3414117818818894,
      "grad_norm": 4.0980658531188965,
      "learning_rate": 4.804882351509843e-05,
      "loss": 0.7796,
      "step": 256600
    },
    {
      "epoch": 2.342324257245054,
      "grad_norm": 3.891533613204956,
      "learning_rate": 4.8048063118962454e-05,
      "loss": 0.7326,
      "step": 256700
    },
    {
      "epoch": 2.3432367326082195,
      "grad_norm": 2.148366928100586,
      "learning_rate": 4.8047302722826484e-05,
      "loss": 0.7704,
      "step": 256800
    },
    {
      "epoch": 2.344149207971385,
      "grad_norm": 4.482457160949707,
      "learning_rate": 4.8046542326690514e-05,
      "loss": 0.6946,
      "step": 256900
    },
    {
      "epoch": 2.34506168333455,
      "grad_norm": 3.907331705093384,
      "learning_rate": 4.8045781930554544e-05,
      "loss": 0.7597,
      "step": 257000
    },
    {
      "epoch": 2.345974158697715,
      "grad_norm": 3.406651020050049,
      "learning_rate": 4.8045021534418574e-05,
      "loss": 0.6969,
      "step": 257100
    },
    {
      "epoch": 2.3468866340608803,
      "grad_norm": 3.9902524948120117,
      "learning_rate": 4.8044261138282604e-05,
      "loss": 0.7591,
      "step": 257200
    },
    {
      "epoch": 2.3477991094240456,
      "grad_norm": 5.671897888183594,
      "learning_rate": 4.804350074214663e-05,
      "loss": 0.7983,
      "step": 257300
    },
    {
      "epoch": 2.348711584787211,
      "grad_norm": 3.060619354248047,
      "learning_rate": 4.8042740346010665e-05,
      "loss": 0.7481,
      "step": 257400
    },
    {
      "epoch": 2.3496240601503757,
      "grad_norm": 3.612231492996216,
      "learning_rate": 4.804197994987469e-05,
      "loss": 0.7905,
      "step": 257500
    },
    {
      "epoch": 2.350536535513541,
      "grad_norm": 4.859724044799805,
      "learning_rate": 4.804121955373872e-05,
      "loss": 0.7246,
      "step": 257600
    },
    {
      "epoch": 2.3514490108767063,
      "grad_norm": 4.6391282081604,
      "learning_rate": 4.804045915760275e-05,
      "loss": 0.7963,
      "step": 257700
    },
    {
      "epoch": 2.3523614862398716,
      "grad_norm": 3.5414254665374756,
      "learning_rate": 4.803969876146678e-05,
      "loss": 0.7393,
      "step": 257800
    },
    {
      "epoch": 2.353273961603037,
      "grad_norm": 4.603810787200928,
      "learning_rate": 4.80389383653308e-05,
      "loss": 0.7565,
      "step": 257900
    },
    {
      "epoch": 2.354186436966202,
      "grad_norm": 4.676778316497803,
      "learning_rate": 4.803817796919484e-05,
      "loss": 0.7841,
      "step": 258000
    },
    {
      "epoch": 2.355098912329367,
      "grad_norm": 4.08136510848999,
      "learning_rate": 4.803741757305886e-05,
      "loss": 0.7453,
      "step": 258100
    },
    {
      "epoch": 2.3560113876925324,
      "grad_norm": 3.730501413345337,
      "learning_rate": 4.803665717692289e-05,
      "loss": 0.7421,
      "step": 258200
    },
    {
      "epoch": 2.3569238630556977,
      "grad_norm": 4.993570327758789,
      "learning_rate": 4.803589678078692e-05,
      "loss": 0.7499,
      "step": 258300
    },
    {
      "epoch": 2.3578363384188625,
      "grad_norm": 4.576367378234863,
      "learning_rate": 4.803513638465095e-05,
      "loss": 0.7011,
      "step": 258400
    },
    {
      "epoch": 2.358748813782028,
      "grad_norm": 4.166915416717529,
      "learning_rate": 4.803437598851498e-05,
      "loss": 0.7793,
      "step": 258500
    },
    {
      "epoch": 2.359661289145193,
      "grad_norm": 5.45366096496582,
      "learning_rate": 4.8033615592379005e-05,
      "loss": 0.7656,
      "step": 258600
    },
    {
      "epoch": 2.3605737645083584,
      "grad_norm": 3.753432273864746,
      "learning_rate": 4.8032855196243035e-05,
      "loss": 0.7453,
      "step": 258700
    },
    {
      "epoch": 2.3614862398715233,
      "grad_norm": 4.5952229499816895,
      "learning_rate": 4.8032094800107065e-05,
      "loss": 0.728,
      "step": 258800
    },
    {
      "epoch": 2.3623987152346886,
      "grad_norm": 4.919516563415527,
      "learning_rate": 4.8031334403971095e-05,
      "loss": 0.768,
      "step": 258900
    },
    {
      "epoch": 2.363311190597854,
      "grad_norm": 4.795294761657715,
      "learning_rate": 4.803057400783512e-05,
      "loss": 0.7285,
      "step": 259000
    },
    {
      "epoch": 2.364223665961019,
      "grad_norm": 4.0202741622924805,
      "learning_rate": 4.8029813611699155e-05,
      "loss": 0.7299,
      "step": 259100
    },
    {
      "epoch": 2.365136141324184,
      "grad_norm": 4.670933723449707,
      "learning_rate": 4.802905321556318e-05,
      "loss": 0.7714,
      "step": 259200
    },
    {
      "epoch": 2.3660486166873493,
      "grad_norm": 3.4826948642730713,
      "learning_rate": 4.802829281942721e-05,
      "loss": 0.753,
      "step": 259300
    },
    {
      "epoch": 2.3669610920505146,
      "grad_norm": 4.64526891708374,
      "learning_rate": 4.802753242329124e-05,
      "loss": 0.7884,
      "step": 259400
    },
    {
      "epoch": 2.36787356741368,
      "grad_norm": 3.872976779937744,
      "learning_rate": 4.802677202715527e-05,
      "loss": 0.7589,
      "step": 259500
    },
    {
      "epoch": 2.3687860427768452,
      "grad_norm": 3.3496017456054688,
      "learning_rate": 4.80260116310193e-05,
      "loss": 0.7196,
      "step": 259600
    },
    {
      "epoch": 2.36969851814001,
      "grad_norm": 4.178680896759033,
      "learning_rate": 4.802525123488333e-05,
      "loss": 0.7756,
      "step": 259700
    },
    {
      "epoch": 2.3706109935031754,
      "grad_norm": 5.0229105949401855,
      "learning_rate": 4.802449083874735e-05,
      "loss": 0.7482,
      "step": 259800
    },
    {
      "epoch": 2.3715234688663407,
      "grad_norm": 4.943207740783691,
      "learning_rate": 4.802373044261139e-05,
      "loss": 0.7633,
      "step": 259900
    },
    {
      "epoch": 2.372435944229506,
      "grad_norm": 4.533847808837891,
      "learning_rate": 4.802297004647541e-05,
      "loss": 0.7493,
      "step": 260000
    },
    {
      "epoch": 2.373348419592671,
      "grad_norm": 4.979477882385254,
      "learning_rate": 4.802220965033944e-05,
      "loss": 0.7659,
      "step": 260100
    },
    {
      "epoch": 2.374260894955836,
      "grad_norm": 4.737014293670654,
      "learning_rate": 4.802144925420347e-05,
      "loss": 0.7854,
      "step": 260200
    },
    {
      "epoch": 2.3751733703190014,
      "grad_norm": 4.097715854644775,
      "learning_rate": 4.80206888580675e-05,
      "loss": 0.7694,
      "step": 260300
    },
    {
      "epoch": 2.3760858456821667,
      "grad_norm": 4.196823596954346,
      "learning_rate": 4.8019928461931526e-05,
      "loss": 0.7676,
      "step": 260400
    },
    {
      "epoch": 2.3769983210453316,
      "grad_norm": 4.436211109161377,
      "learning_rate": 4.801916806579556e-05,
      "loss": 0.7127,
      "step": 260500
    },
    {
      "epoch": 2.377910796408497,
      "grad_norm": 4.414458751678467,
      "learning_rate": 4.8018407669659586e-05,
      "loss": 0.7815,
      "step": 260600
    },
    {
      "epoch": 2.378823271771662,
      "grad_norm": 5.203012943267822,
      "learning_rate": 4.8017647273523616e-05,
      "loss": 0.7703,
      "step": 260700
    },
    {
      "epoch": 2.3797357471348275,
      "grad_norm": 3.6170313358306885,
      "learning_rate": 4.8016886877387646e-05,
      "loss": 0.7649,
      "step": 260800
    },
    {
      "epoch": 2.3806482224979924,
      "grad_norm": 3.5365493297576904,
      "learning_rate": 4.8016126481251676e-05,
      "loss": 0.7705,
      "step": 260900
    },
    {
      "epoch": 2.3815606978611576,
      "grad_norm": 4.332940101623535,
      "learning_rate": 4.8015366085115706e-05,
      "loss": 0.7848,
      "step": 261000
    },
    {
      "epoch": 2.382473173224323,
      "grad_norm": 3.2392711639404297,
      "learning_rate": 4.8014605688979736e-05,
      "loss": 0.7439,
      "step": 261100
    },
    {
      "epoch": 2.3833856485874882,
      "grad_norm": 4.501449108123779,
      "learning_rate": 4.801384529284376e-05,
      "loss": 0.7579,
      "step": 261200
    },
    {
      "epoch": 2.3842981239506535,
      "grad_norm": 3.827681541442871,
      "learning_rate": 4.801308489670779e-05,
      "loss": 0.7882,
      "step": 261300
    },
    {
      "epoch": 2.3852105993138184,
      "grad_norm": 3.4236490726470947,
      "learning_rate": 4.801232450057182e-05,
      "loss": 0.7726,
      "step": 261400
    },
    {
      "epoch": 2.3861230746769837,
      "grad_norm": 4.464744567871094,
      "learning_rate": 4.801156410443585e-05,
      "loss": 0.7664,
      "step": 261500
    },
    {
      "epoch": 2.387035550040149,
      "grad_norm": 4.799960136413574,
      "learning_rate": 4.801080370829988e-05,
      "loss": 0.7702,
      "step": 261600
    },
    {
      "epoch": 2.3879480254033143,
      "grad_norm": 4.20469856262207,
      "learning_rate": 4.80100433121639e-05,
      "loss": 0.7708,
      "step": 261700
    },
    {
      "epoch": 2.388860500766479,
      "grad_norm": 4.777495861053467,
      "learning_rate": 4.800928291602793e-05,
      "loss": 0.7424,
      "step": 261800
    },
    {
      "epoch": 2.3897729761296445,
      "grad_norm": 3.5433871746063232,
      "learning_rate": 4.8008522519891963e-05,
      "loss": 0.7389,
      "step": 261900
    },
    {
      "epoch": 2.3906854514928098,
      "grad_norm": 4.063482284545898,
      "learning_rate": 4.8007762123755993e-05,
      "loss": 0.7817,
      "step": 262000
    },
    {
      "epoch": 2.391597926855975,
      "grad_norm": 4.062030792236328,
      "learning_rate": 4.8007001727620024e-05,
      "loss": 0.7389,
      "step": 262100
    },
    {
      "epoch": 2.39251040221914,
      "grad_norm": 4.136834621429443,
      "learning_rate": 4.8006241331484054e-05,
      "loss": 0.7553,
      "step": 262200
    },
    {
      "epoch": 2.393422877582305,
      "grad_norm": 3.514885187149048,
      "learning_rate": 4.800548093534808e-05,
      "loss": 0.7541,
      "step": 262300
    },
    {
      "epoch": 2.3943353529454705,
      "grad_norm": 4.425102710723877,
      "learning_rate": 4.8004720539212114e-05,
      "loss": 0.7424,
      "step": 262400
    },
    {
      "epoch": 2.395247828308636,
      "grad_norm": 4.202375888824463,
      "learning_rate": 4.800396014307614e-05,
      "loss": 0.7524,
      "step": 262500
    },
    {
      "epoch": 2.3961603036718007,
      "grad_norm": 3.9392740726470947,
      "learning_rate": 4.800319974694017e-05,
      "loss": 0.6808,
      "step": 262600
    },
    {
      "epoch": 2.397072779034966,
      "grad_norm": 4.0762763023376465,
      "learning_rate": 4.80024393508042e-05,
      "loss": 0.7523,
      "step": 262700
    },
    {
      "epoch": 2.3979852543981313,
      "grad_norm": 4.600191116333008,
      "learning_rate": 4.800167895466823e-05,
      "loss": 0.7533,
      "step": 262800
    },
    {
      "epoch": 2.3988977297612966,
      "grad_norm": 3.7704737186431885,
      "learning_rate": 4.800091855853226e-05,
      "loss": 0.7422,
      "step": 262900
    },
    {
      "epoch": 2.399810205124462,
      "grad_norm": 4.560533046722412,
      "learning_rate": 4.800015816239629e-05,
      "loss": 0.7643,
      "step": 263000
    },
    {
      "epoch": 2.4007226804876267,
      "grad_norm": 4.313952922821045,
      "learning_rate": 4.799939776626031e-05,
      "loss": 0.7896,
      "step": 263100
    },
    {
      "epoch": 2.401635155850792,
      "grad_norm": 4.23448371887207,
      "learning_rate": 4.799863737012435e-05,
      "loss": 0.7751,
      "step": 263200
    },
    {
      "epoch": 2.4025476312139573,
      "grad_norm": 3.6205432415008545,
      "learning_rate": 4.799787697398837e-05,
      "loss": 0.7434,
      "step": 263300
    },
    {
      "epoch": 2.403460106577122,
      "grad_norm": 4.549705982208252,
      "learning_rate": 4.79971165778524e-05,
      "loss": 0.7325,
      "step": 263400
    },
    {
      "epoch": 2.4043725819402875,
      "grad_norm": 4.151491641998291,
      "learning_rate": 4.799635618171643e-05,
      "loss": 0.7393,
      "step": 263500
    },
    {
      "epoch": 2.4052850573034528,
      "grad_norm": 4.028878688812256,
      "learning_rate": 4.799559578558046e-05,
      "loss": 0.7912,
      "step": 263600
    },
    {
      "epoch": 2.406197532666618,
      "grad_norm": 4.364522457122803,
      "learning_rate": 4.7994835389444484e-05,
      "loss": 0.7519,
      "step": 263700
    },
    {
      "epoch": 2.4071100080297834,
      "grad_norm": 4.597374439239502,
      "learning_rate": 4.799407499330852e-05,
      "loss": 0.7628,
      "step": 263800
    },
    {
      "epoch": 2.408022483392948,
      "grad_norm": 3.3107099533081055,
      "learning_rate": 4.7993314597172544e-05,
      "loss": 0.7193,
      "step": 263900
    },
    {
      "epoch": 2.4089349587561135,
      "grad_norm": 3.6243531703948975,
      "learning_rate": 4.7992554201036574e-05,
      "loss": 0.7337,
      "step": 264000
    },
    {
      "epoch": 2.409847434119279,
      "grad_norm": 4.493290901184082,
      "learning_rate": 4.7991793804900605e-05,
      "loss": 0.7894,
      "step": 264100
    },
    {
      "epoch": 2.410759909482444,
      "grad_norm": 5.032360553741455,
      "learning_rate": 4.799103340876463e-05,
      "loss": 0.7295,
      "step": 264200
    },
    {
      "epoch": 2.411672384845609,
      "grad_norm": 3.8986892700195312,
      "learning_rate": 4.7990273012628665e-05,
      "loss": 0.763,
      "step": 264300
    },
    {
      "epoch": 2.4125848602087743,
      "grad_norm": 4.313177585601807,
      "learning_rate": 4.798951261649269e-05,
      "loss": 0.7567,
      "step": 264400
    },
    {
      "epoch": 2.4134973355719396,
      "grad_norm": 3.8957061767578125,
      "learning_rate": 4.798875222035672e-05,
      "loss": 0.7824,
      "step": 264500
    },
    {
      "epoch": 2.414409810935105,
      "grad_norm": 3.6166603565216064,
      "learning_rate": 4.798799182422075e-05,
      "loss": 0.762,
      "step": 264600
    },
    {
      "epoch": 2.41532228629827,
      "grad_norm": 4.36676025390625,
      "learning_rate": 4.798723142808478e-05,
      "loss": 0.7641,
      "step": 264700
    },
    {
      "epoch": 2.416234761661435,
      "grad_norm": 4.653953552246094,
      "learning_rate": 4.79864710319488e-05,
      "loss": 0.712,
      "step": 264800
    },
    {
      "epoch": 2.4171472370246003,
      "grad_norm": 5.162287712097168,
      "learning_rate": 4.798571063581284e-05,
      "loss": 0.7232,
      "step": 264900
    },
    {
      "epoch": 2.4180597123877656,
      "grad_norm": 4.554776191711426,
      "learning_rate": 4.798495023967686e-05,
      "loss": 0.7308,
      "step": 265000
    },
    {
      "epoch": 2.4189721877509305,
      "grad_norm": 4.096183776855469,
      "learning_rate": 4.798418984354089e-05,
      "loss": 0.7796,
      "step": 265100
    },
    {
      "epoch": 2.419884663114096,
      "grad_norm": 4.486069679260254,
      "learning_rate": 4.798342944740492e-05,
      "loss": 0.7813,
      "step": 265200
    },
    {
      "epoch": 2.420797138477261,
      "grad_norm": 3.9083831310272217,
      "learning_rate": 4.798266905126895e-05,
      "loss": 0.7617,
      "step": 265300
    },
    {
      "epoch": 2.4217096138404264,
      "grad_norm": 4.420648097991943,
      "learning_rate": 4.798190865513298e-05,
      "loss": 0.7159,
      "step": 265400
    },
    {
      "epoch": 2.4226220892035917,
      "grad_norm": 2.973994255065918,
      "learning_rate": 4.798114825899701e-05,
      "loss": 0.7722,
      "step": 265500
    },
    {
      "epoch": 2.4235345645667565,
      "grad_norm": 3.372541666030884,
      "learning_rate": 4.7980387862861035e-05,
      "loss": 0.7446,
      "step": 265600
    },
    {
      "epoch": 2.424447039929922,
      "grad_norm": 3.1309595108032227,
      "learning_rate": 4.797962746672507e-05,
      "loss": 0.7864,
      "step": 265700
    },
    {
      "epoch": 2.425359515293087,
      "grad_norm": 5.089117050170898,
      "learning_rate": 4.7978867070589095e-05,
      "loss": 0.7544,
      "step": 265800
    },
    {
      "epoch": 2.4262719906562524,
      "grad_norm": 4.135207176208496,
      "learning_rate": 4.7978106674453125e-05,
      "loss": 0.784,
      "step": 265900
    },
    {
      "epoch": 2.4271844660194173,
      "grad_norm": 5.175989627838135,
      "learning_rate": 4.7977346278317155e-05,
      "loss": 0.7789,
      "step": 266000
    },
    {
      "epoch": 2.4280969413825826,
      "grad_norm": 4.09525203704834,
      "learning_rate": 4.7976585882181186e-05,
      "loss": 0.759,
      "step": 266100
    },
    {
      "epoch": 2.429009416745748,
      "grad_norm": 4.454986095428467,
      "learning_rate": 4.797582548604521e-05,
      "loss": 0.7122,
      "step": 266200
    },
    {
      "epoch": 2.429921892108913,
      "grad_norm": 4.420515537261963,
      "learning_rate": 4.7975065089909246e-05,
      "loss": 0.738,
      "step": 266300
    },
    {
      "epoch": 2.4308343674720785,
      "grad_norm": 4.369082450866699,
      "learning_rate": 4.797430469377327e-05,
      "loss": 0.7819,
      "step": 266400
    },
    {
      "epoch": 2.4317468428352433,
      "grad_norm": 5.533791542053223,
      "learning_rate": 4.79735442976373e-05,
      "loss": 0.7754,
      "step": 266500
    },
    {
      "epoch": 2.4326593181984086,
      "grad_norm": 4.898531913757324,
      "learning_rate": 4.797278390150133e-05,
      "loss": 0.7814,
      "step": 266600
    },
    {
      "epoch": 2.433571793561574,
      "grad_norm": 4.620248317718506,
      "learning_rate": 4.797202350536536e-05,
      "loss": 0.7782,
      "step": 266700
    },
    {
      "epoch": 2.434484268924739,
      "grad_norm": 4.023873805999756,
      "learning_rate": 4.797126310922939e-05,
      "loss": 0.7294,
      "step": 266800
    },
    {
      "epoch": 2.435396744287904,
      "grad_norm": 4.359127044677734,
      "learning_rate": 4.797050271309342e-05,
      "loss": 0.7286,
      "step": 266900
    },
    {
      "epoch": 2.4363092196510694,
      "grad_norm": 4.717164039611816,
      "learning_rate": 4.796974231695744e-05,
      "loss": 0.7456,
      "step": 267000
    },
    {
      "epoch": 2.4372216950142347,
      "grad_norm": 4.370820999145508,
      "learning_rate": 4.796898192082147e-05,
      "loss": 0.7568,
      "step": 267100
    },
    {
      "epoch": 2.4381341703774,
      "grad_norm": 3.831090211868286,
      "learning_rate": 4.79682215246855e-05,
      "loss": 0.7188,
      "step": 267200
    },
    {
      "epoch": 2.439046645740565,
      "grad_norm": 4.329850196838379,
      "learning_rate": 4.7967461128549526e-05,
      "loss": 0.755,
      "step": 267300
    },
    {
      "epoch": 2.43995912110373,
      "grad_norm": 4.253810405731201,
      "learning_rate": 4.796670073241356e-05,
      "loss": 0.7374,
      "step": 267400
    },
    {
      "epoch": 2.4408715964668954,
      "grad_norm": 4.871530532836914,
      "learning_rate": 4.7965940336277586e-05,
      "loss": 0.7485,
      "step": 267500
    },
    {
      "epoch": 2.4417840718300607,
      "grad_norm": 3.1309423446655273,
      "learning_rate": 4.7965179940141616e-05,
      "loss": 0.7173,
      "step": 267600
    },
    {
      "epoch": 2.4426965471932256,
      "grad_norm": 4.205932140350342,
      "learning_rate": 4.7964419544005646e-05,
      "loss": 0.7239,
      "step": 267700
    },
    {
      "epoch": 2.443609022556391,
      "grad_norm": 4.588591575622559,
      "learning_rate": 4.7963659147869676e-05,
      "loss": 0.737,
      "step": 267800
    },
    {
      "epoch": 2.444521497919556,
      "grad_norm": 3.5581958293914795,
      "learning_rate": 4.7962898751733706e-05,
      "loss": 0.7357,
      "step": 267900
    },
    {
      "epoch": 2.4454339732827215,
      "grad_norm": 3.38016676902771,
      "learning_rate": 4.7962138355597737e-05,
      "loss": 0.7405,
      "step": 268000
    },
    {
      "epoch": 2.446346448645887,
      "grad_norm": 2.906059741973877,
      "learning_rate": 4.796137795946176e-05,
      "loss": 0.8079,
      "step": 268100
    },
    {
      "epoch": 2.4472589240090517,
      "grad_norm": 4.309026718139648,
      "learning_rate": 4.79606175633258e-05,
      "loss": 0.745,
      "step": 268200
    },
    {
      "epoch": 2.448171399372217,
      "grad_norm": 3.5589308738708496,
      "learning_rate": 4.795985716718982e-05,
      "loss": 0.7638,
      "step": 268300
    },
    {
      "epoch": 2.4490838747353822,
      "grad_norm": 4.033980846405029,
      "learning_rate": 4.795909677105385e-05,
      "loss": 0.7567,
      "step": 268400
    },
    {
      "epoch": 2.449996350098547,
      "grad_norm": 2.610689640045166,
      "learning_rate": 4.795833637491788e-05,
      "loss": 0.7342,
      "step": 268500
    },
    {
      "epoch": 2.4509088254617124,
      "grad_norm": 4.177722930908203,
      "learning_rate": 4.795757597878191e-05,
      "loss": 0.7839,
      "step": 268600
    },
    {
      "epoch": 2.4518213008248777,
      "grad_norm": 4.4632768630981445,
      "learning_rate": 4.7956815582645933e-05,
      "loss": 0.7139,
      "step": 268700
    },
    {
      "epoch": 2.452733776188043,
      "grad_norm": 4.333467960357666,
      "learning_rate": 4.795605518650997e-05,
      "loss": 0.8004,
      "step": 268800
    },
    {
      "epoch": 2.4536462515512083,
      "grad_norm": 4.200747966766357,
      "learning_rate": 4.7955294790373994e-05,
      "loss": 0.7433,
      "step": 268900
    },
    {
      "epoch": 2.454558726914373,
      "grad_norm": 4.339926242828369,
      "learning_rate": 4.7954534394238024e-05,
      "loss": 0.7289,
      "step": 269000
    },
    {
      "epoch": 2.4554712022775385,
      "grad_norm": 4.230561256408691,
      "learning_rate": 4.7953773998102054e-05,
      "loss": 0.7335,
      "step": 269100
    },
    {
      "epoch": 2.4563836776407038,
      "grad_norm": 4.453350067138672,
      "learning_rate": 4.7953013601966084e-05,
      "loss": 0.7206,
      "step": 269200
    },
    {
      "epoch": 2.457296153003869,
      "grad_norm": 3.619267463684082,
      "learning_rate": 4.7952253205830114e-05,
      "loss": 0.7361,
      "step": 269300
    },
    {
      "epoch": 2.458208628367034,
      "grad_norm": 3.3884150981903076,
      "learning_rate": 4.7951492809694144e-05,
      "loss": 0.799,
      "step": 269400
    },
    {
      "epoch": 2.459121103730199,
      "grad_norm": 4.853356838226318,
      "learning_rate": 4.795073241355817e-05,
      "loss": 0.7697,
      "step": 269500
    },
    {
      "epoch": 2.4600335790933645,
      "grad_norm": 4.256330966949463,
      "learning_rate": 4.7949972017422204e-05,
      "loss": 0.7589,
      "step": 269600
    },
    {
      "epoch": 2.46094605445653,
      "grad_norm": 4.461048603057861,
      "learning_rate": 4.794921162128623e-05,
      "loss": 0.7455,
      "step": 269700
    },
    {
      "epoch": 2.461858529819695,
      "grad_norm": 4.197955131530762,
      "learning_rate": 4.794845122515026e-05,
      "loss": 0.7524,
      "step": 269800
    },
    {
      "epoch": 2.46277100518286,
      "grad_norm": 3.8514559268951416,
      "learning_rate": 4.794769082901429e-05,
      "loss": 0.7542,
      "step": 269900
    },
    {
      "epoch": 2.4636834805460253,
      "grad_norm": 3.6424598693847656,
      "learning_rate": 4.794693043287831e-05,
      "loss": 0.7889,
      "step": 270000
    },
    {
      "epoch": 2.4645959559091906,
      "grad_norm": 4.270050525665283,
      "learning_rate": 4.794617003674234e-05,
      "loss": 0.771,
      "step": 270100
    },
    {
      "epoch": 2.4655084312723554,
      "grad_norm": 5.4009809494018555,
      "learning_rate": 4.794540964060637e-05,
      "loss": 0.7408,
      "step": 270200
    },
    {
      "epoch": 2.4664209066355207,
      "grad_norm": 5.278602123260498,
      "learning_rate": 4.79446492444704e-05,
      "loss": 0.7654,
      "step": 270300
    },
    {
      "epoch": 2.467333381998686,
      "grad_norm": 4.89553165435791,
      "learning_rate": 4.794388884833443e-05,
      "loss": 0.7773,
      "step": 270400
    },
    {
      "epoch": 2.4682458573618513,
      "grad_norm": 4.484469413757324,
      "learning_rate": 4.794312845219846e-05,
      "loss": 0.7577,
      "step": 270500
    },
    {
      "epoch": 2.4691583327250166,
      "grad_norm": 4.311034679412842,
      "learning_rate": 4.7942368056062484e-05,
      "loss": 0.7689,
      "step": 270600
    },
    {
      "epoch": 2.4700708080881815,
      "grad_norm": 4.149360656738281,
      "learning_rate": 4.794160765992652e-05,
      "loss": 0.7226,
      "step": 270700
    },
    {
      "epoch": 2.4709832834513468,
      "grad_norm": 4.583990573883057,
      "learning_rate": 4.7940847263790545e-05,
      "loss": 0.7504,
      "step": 270800
    },
    {
      "epoch": 2.471895758814512,
      "grad_norm": 4.644523620605469,
      "learning_rate": 4.7940086867654575e-05,
      "loss": 0.7497,
      "step": 270900
    },
    {
      "epoch": 2.4728082341776774,
      "grad_norm": 4.682635307312012,
      "learning_rate": 4.7939326471518605e-05,
      "loss": 0.7523,
      "step": 271000
    },
    {
      "epoch": 2.473720709540842,
      "grad_norm": 4.057736396789551,
      "learning_rate": 4.7938566075382635e-05,
      "loss": 0.7489,
      "step": 271100
    },
    {
      "epoch": 2.4746331849040075,
      "grad_norm": 4.094883918762207,
      "learning_rate": 4.793780567924666e-05,
      "loss": 0.7405,
      "step": 271200
    },
    {
      "epoch": 2.475545660267173,
      "grad_norm": 4.8242950439453125,
      "learning_rate": 4.7937045283110695e-05,
      "loss": 0.7557,
      "step": 271300
    },
    {
      "epoch": 2.476458135630338,
      "grad_norm": 4.402228832244873,
      "learning_rate": 4.793628488697472e-05,
      "loss": 0.6992,
      "step": 271400
    },
    {
      "epoch": 2.4773706109935034,
      "grad_norm": 5.198949813842773,
      "learning_rate": 4.793552449083875e-05,
      "loss": 0.7355,
      "step": 271500
    },
    {
      "epoch": 2.4782830863566683,
      "grad_norm": 3.911435842514038,
      "learning_rate": 4.793476409470278e-05,
      "loss": 0.7716,
      "step": 271600
    },
    {
      "epoch": 2.4791955617198336,
      "grad_norm": 4.134533882141113,
      "learning_rate": 4.793400369856681e-05,
      "loss": 0.749,
      "step": 271700
    },
    {
      "epoch": 2.480108037082999,
      "grad_norm": 4.262112140655518,
      "learning_rate": 4.793324330243084e-05,
      "loss": 0.7082,
      "step": 271800
    },
    {
      "epoch": 2.4810205124461637,
      "grad_norm": 4.181792736053467,
      "learning_rate": 4.793248290629487e-05,
      "loss": 0.7572,
      "step": 271900
    },
    {
      "epoch": 2.481932987809329,
      "grad_norm": 4.399153709411621,
      "learning_rate": 4.793172251015889e-05,
      "loss": 0.7479,
      "step": 272000
    },
    {
      "epoch": 2.4828454631724943,
      "grad_norm": 3.6383299827575684,
      "learning_rate": 4.793096211402293e-05,
      "loss": 0.7729,
      "step": 272100
    },
    {
      "epoch": 2.4837579385356596,
      "grad_norm": 4.26607084274292,
      "learning_rate": 4.793020171788695e-05,
      "loss": 0.7652,
      "step": 272200
    },
    {
      "epoch": 2.484670413898825,
      "grad_norm": 4.56678581237793,
      "learning_rate": 4.792944132175098e-05,
      "loss": 0.7406,
      "step": 272300
    },
    {
      "epoch": 2.48558288926199,
      "grad_norm": 4.486586093902588,
      "learning_rate": 4.792868092561501e-05,
      "loss": 0.7605,
      "step": 272400
    },
    {
      "epoch": 2.486495364625155,
      "grad_norm": 4.3921589851379395,
      "learning_rate": 4.792792052947904e-05,
      "loss": 0.803,
      "step": 272500
    },
    {
      "epoch": 2.4874078399883204,
      "grad_norm": 4.670212745666504,
      "learning_rate": 4.7927160133343065e-05,
      "loss": 0.7378,
      "step": 272600
    },
    {
      "epoch": 2.4883203153514857,
      "grad_norm": 4.300690174102783,
      "learning_rate": 4.7926399737207095e-05,
      "loss": 0.7624,
      "step": 272700
    },
    {
      "epoch": 2.4892327907146505,
      "grad_norm": 4.28774356842041,
      "learning_rate": 4.7925639341071126e-05,
      "loss": 0.7771,
      "step": 272800
    },
    {
      "epoch": 2.490145266077816,
      "grad_norm": 3.4280688762664795,
      "learning_rate": 4.7924878944935156e-05,
      "loss": 0.7765,
      "step": 272900
    },
    {
      "epoch": 2.491057741440981,
      "grad_norm": 3.2147936820983887,
      "learning_rate": 4.7924118548799186e-05,
      "loss": 0.7879,
      "step": 273000
    },
    {
      "epoch": 2.4919702168041464,
      "grad_norm": 4.3906569480896,
      "learning_rate": 4.792335815266321e-05,
      "loss": 0.7664,
      "step": 273100
    },
    {
      "epoch": 2.4928826921673117,
      "grad_norm": 3.6198384761810303,
      "learning_rate": 4.7922597756527246e-05,
      "loss": 0.7517,
      "step": 273200
    },
    {
      "epoch": 2.4937951675304766,
      "grad_norm": 4.471020698547363,
      "learning_rate": 4.792183736039127e-05,
      "loss": 0.7065,
      "step": 273300
    },
    {
      "epoch": 2.494707642893642,
      "grad_norm": 4.043672561645508,
      "learning_rate": 4.79210769642553e-05,
      "loss": 0.7355,
      "step": 273400
    },
    {
      "epoch": 2.495620118256807,
      "grad_norm": 4.585224151611328,
      "learning_rate": 4.792031656811933e-05,
      "loss": 0.7757,
      "step": 273500
    },
    {
      "epoch": 2.496532593619972,
      "grad_norm": 3.8806118965148926,
      "learning_rate": 4.791955617198336e-05,
      "loss": 0.7775,
      "step": 273600
    },
    {
      "epoch": 2.4974450689831373,
      "grad_norm": 5.415225505828857,
      "learning_rate": 4.791879577584739e-05,
      "loss": 0.7129,
      "step": 273700
    },
    {
      "epoch": 2.4983575443463026,
      "grad_norm": 3.909517765045166,
      "learning_rate": 4.791803537971142e-05,
      "loss": 0.7598,
      "step": 273800
    },
    {
      "epoch": 2.499270019709468,
      "grad_norm": 5.170237064361572,
      "learning_rate": 4.791727498357544e-05,
      "loss": 0.7844,
      "step": 273900
    },
    {
      "epoch": 2.5001824950726332,
      "grad_norm": 4.454148769378662,
      "learning_rate": 4.791651458743947e-05,
      "loss": 0.7257,
      "step": 274000
    },
    {
      "epoch": 2.501094970435798,
      "grad_norm": 4.572015762329102,
      "learning_rate": 4.79157541913035e-05,
      "loss": 0.7997,
      "step": 274100
    },
    {
      "epoch": 2.5020074457989634,
      "grad_norm": 5.056926727294922,
      "learning_rate": 4.791499379516753e-05,
      "loss": 0.703,
      "step": 274200
    },
    {
      "epoch": 2.5029199211621287,
      "grad_norm": 3.694913864135742,
      "learning_rate": 4.791423339903156e-05,
      "loss": 0.7867,
      "step": 274300
    },
    {
      "epoch": 2.5038323965252935,
      "grad_norm": 2.453331470489502,
      "learning_rate": 4.791347300289559e-05,
      "loss": 0.7229,
      "step": 274400
    },
    {
      "epoch": 2.504744871888459,
      "grad_norm": 4.333715438842773,
      "learning_rate": 4.7912712606759616e-05,
      "loss": 0.7714,
      "step": 274500
    },
    {
      "epoch": 2.505657347251624,
      "grad_norm": 3.4057981967926025,
      "learning_rate": 4.791195221062365e-05,
      "loss": 0.7461,
      "step": 274600
    },
    {
      "epoch": 2.5065698226147894,
      "grad_norm": 4.362536430358887,
      "learning_rate": 4.7911191814487676e-05,
      "loss": 0.7128,
      "step": 274700
    },
    {
      "epoch": 2.5074822979779547,
      "grad_norm": 4.073601722717285,
      "learning_rate": 4.7910431418351707e-05,
      "loss": 0.7377,
      "step": 274800
    },
    {
      "epoch": 2.50839477334112,
      "grad_norm": 3.6546249389648438,
      "learning_rate": 4.7909671022215737e-05,
      "loss": 0.7355,
      "step": 274900
    },
    {
      "epoch": 2.509307248704285,
      "grad_norm": 3.9068844318389893,
      "learning_rate": 4.790891062607977e-05,
      "loss": 0.7793,
      "step": 275000
    },
    {
      "epoch": 2.51021972406745,
      "grad_norm": 4.707724094390869,
      "learning_rate": 4.79081502299438e-05,
      "loss": 0.7626,
      "step": 275100
    },
    {
      "epoch": 2.5111321994306155,
      "grad_norm": 4.375301361083984,
      "learning_rate": 4.790738983380783e-05,
      "loss": 0.7625,
      "step": 275200
    },
    {
      "epoch": 2.5120446747937804,
      "grad_norm": 3.756592273712158,
      "learning_rate": 4.790662943767185e-05,
      "loss": 0.7594,
      "step": 275300
    },
    {
      "epoch": 2.5129571501569457,
      "grad_norm": 4.242035388946533,
      "learning_rate": 4.790586904153588e-05,
      "loss": 0.7586,
      "step": 275400
    },
    {
      "epoch": 2.513869625520111,
      "grad_norm": 4.7728962898254395,
      "learning_rate": 4.790510864539991e-05,
      "loss": 0.7692,
      "step": 275500
    },
    {
      "epoch": 2.5147821008832763,
      "grad_norm": 4.065824508666992,
      "learning_rate": 4.7904348249263934e-05,
      "loss": 0.7202,
      "step": 275600
    },
    {
      "epoch": 2.5156945762464415,
      "grad_norm": 4.227772235870361,
      "learning_rate": 4.790358785312797e-05,
      "loss": 0.7204,
      "step": 275700
    },
    {
      "epoch": 2.5166070516096064,
      "grad_norm": 3.9781012535095215,
      "learning_rate": 4.7902827456991994e-05,
      "loss": 0.7753,
      "step": 275800
    },
    {
      "epoch": 2.5175195269727717,
      "grad_norm": 5.133286476135254,
      "learning_rate": 4.7902067060856024e-05,
      "loss": 0.7377,
      "step": 275900
    },
    {
      "epoch": 2.518432002335937,
      "grad_norm": 4.0785136222839355,
      "learning_rate": 4.7901306664720054e-05,
      "loss": 0.718,
      "step": 276000
    },
    {
      "epoch": 2.519344477699102,
      "grad_norm": 4.494305610656738,
      "learning_rate": 4.7900546268584084e-05,
      "loss": 0.746,
      "step": 276100
    },
    {
      "epoch": 2.520256953062267,
      "grad_norm": 4.149068355560303,
      "learning_rate": 4.7899785872448114e-05,
      "loss": 0.776,
      "step": 276200
    },
    {
      "epoch": 2.5211694284254325,
      "grad_norm": 3.9528517723083496,
      "learning_rate": 4.7899025476312144e-05,
      "loss": 0.7562,
      "step": 276300
    },
    {
      "epoch": 2.5220819037885978,
      "grad_norm": 4.362847805023193,
      "learning_rate": 4.789826508017617e-05,
      "loss": 0.7394,
      "step": 276400
    },
    {
      "epoch": 2.522994379151763,
      "grad_norm": 4.690054416656494,
      "learning_rate": 4.7897504684040204e-05,
      "loss": 0.7221,
      "step": 276500
    },
    {
      "epoch": 2.5239068545149284,
      "grad_norm": 4.872218132019043,
      "learning_rate": 4.789674428790423e-05,
      "loss": 0.7374,
      "step": 276600
    },
    {
      "epoch": 2.524819329878093,
      "grad_norm": 3.9604101181030273,
      "learning_rate": 4.789598389176826e-05,
      "loss": 0.7045,
      "step": 276700
    },
    {
      "epoch": 2.5257318052412585,
      "grad_norm": 5.0809197425842285,
      "learning_rate": 4.789522349563229e-05,
      "loss": 0.7362,
      "step": 276800
    },
    {
      "epoch": 2.526644280604424,
      "grad_norm": 4.741335391998291,
      "learning_rate": 4.789446309949632e-05,
      "loss": 0.7354,
      "step": 276900
    },
    {
      "epoch": 2.5275567559675887,
      "grad_norm": 2.8757176399230957,
      "learning_rate": 4.789370270336034e-05,
      "loss": 0.7071,
      "step": 277000
    },
    {
      "epoch": 2.528469231330754,
      "grad_norm": 5.114224910736084,
      "learning_rate": 4.789294230722438e-05,
      "loss": 0.7612,
      "step": 277100
    },
    {
      "epoch": 2.5293817066939193,
      "grad_norm": 2.982736825942993,
      "learning_rate": 4.78921819110884e-05,
      "loss": 0.7506,
      "step": 277200
    },
    {
      "epoch": 2.5302941820570846,
      "grad_norm": 3.9458208084106445,
      "learning_rate": 4.789142151495243e-05,
      "loss": 0.7446,
      "step": 277300
    },
    {
      "epoch": 2.53120665742025,
      "grad_norm": 4.309279918670654,
      "learning_rate": 4.789066111881646e-05,
      "loss": 0.7528,
      "step": 277400
    },
    {
      "epoch": 2.5321191327834147,
      "grad_norm": 4.566804885864258,
      "learning_rate": 4.788990072268049e-05,
      "loss": 0.7424,
      "step": 277500
    },
    {
      "epoch": 2.53303160814658,
      "grad_norm": 3.2585244178771973,
      "learning_rate": 4.788914032654452e-05,
      "loss": 0.722,
      "step": 277600
    },
    {
      "epoch": 2.5339440835097453,
      "grad_norm": 3.7985050678253174,
      "learning_rate": 4.788837993040855e-05,
      "loss": 0.7416,
      "step": 277700
    },
    {
      "epoch": 2.53485655887291,
      "grad_norm": 4.24315071105957,
      "learning_rate": 4.7887619534272575e-05,
      "loss": 0.7853,
      "step": 277800
    },
    {
      "epoch": 2.5357690342360755,
      "grad_norm": 5.091392517089844,
      "learning_rate": 4.788685913813661e-05,
      "loss": 0.7202,
      "step": 277900
    },
    {
      "epoch": 2.5366815095992408,
      "grad_norm": 3.834951639175415,
      "learning_rate": 4.7886098742000635e-05,
      "loss": 0.7383,
      "step": 278000
    },
    {
      "epoch": 2.537593984962406,
      "grad_norm": 4.691978454589844,
      "learning_rate": 4.7885338345864665e-05,
      "loss": 0.7621,
      "step": 278100
    },
    {
      "epoch": 2.5385064603255714,
      "grad_norm": 4.272651672363281,
      "learning_rate": 4.7884577949728695e-05,
      "loss": 0.7517,
      "step": 278200
    },
    {
      "epoch": 2.5394189356887367,
      "grad_norm": 3.974968433380127,
      "learning_rate": 4.7883817553592725e-05,
      "loss": 0.7397,
      "step": 278300
    },
    {
      "epoch": 2.5403314110519015,
      "grad_norm": 3.582533121109009,
      "learning_rate": 4.788305715745675e-05,
      "loss": 0.7474,
      "step": 278400
    },
    {
      "epoch": 2.541243886415067,
      "grad_norm": 4.418085098266602,
      "learning_rate": 4.788229676132078e-05,
      "loss": 0.7514,
      "step": 278500
    },
    {
      "epoch": 2.542156361778232,
      "grad_norm": 3.9760055541992188,
      "learning_rate": 4.788153636518481e-05,
      "loss": 0.7248,
      "step": 278600
    },
    {
      "epoch": 2.543068837141397,
      "grad_norm": 4.94012975692749,
      "learning_rate": 4.788077596904884e-05,
      "loss": 0.7857,
      "step": 278700
    },
    {
      "epoch": 2.5439813125045623,
      "grad_norm": 4.231592178344727,
      "learning_rate": 4.788001557291287e-05,
      "loss": 0.7282,
      "step": 278800
    },
    {
      "epoch": 2.5448937878677276,
      "grad_norm": 3.711662769317627,
      "learning_rate": 4.787925517677689e-05,
      "loss": 0.8066,
      "step": 278900
    },
    {
      "epoch": 2.545806263230893,
      "grad_norm": 4.514216423034668,
      "learning_rate": 4.787849478064093e-05,
      "loss": 0.7677,
      "step": 279000
    },
    {
      "epoch": 2.546718738594058,
      "grad_norm": 4.49628210067749,
      "learning_rate": 4.787773438450495e-05,
      "loss": 0.7409,
      "step": 279100
    },
    {
      "epoch": 2.547631213957223,
      "grad_norm": 5.071300983428955,
      "learning_rate": 4.787697398836898e-05,
      "loss": 0.7161,
      "step": 279200
    },
    {
      "epoch": 2.5485436893203883,
      "grad_norm": 3.8326056003570557,
      "learning_rate": 4.787621359223301e-05,
      "loss": 0.7359,
      "step": 279300
    },
    {
      "epoch": 2.5494561646835536,
      "grad_norm": 3.209555149078369,
      "learning_rate": 4.787545319609704e-05,
      "loss": 0.7657,
      "step": 279400
    },
    {
      "epoch": 2.5503686400467185,
      "grad_norm": 3.9977033138275146,
      "learning_rate": 4.7874692799961065e-05,
      "loss": 0.758,
      "step": 279500
    },
    {
      "epoch": 2.551281115409884,
      "grad_norm": 3.8941805362701416,
      "learning_rate": 4.78739324038251e-05,
      "loss": 0.7924,
      "step": 279600
    },
    {
      "epoch": 2.552193590773049,
      "grad_norm": 4.479129314422607,
      "learning_rate": 4.7873172007689126e-05,
      "loss": 0.7337,
      "step": 279700
    },
    {
      "epoch": 2.5531060661362144,
      "grad_norm": 3.926027297973633,
      "learning_rate": 4.7872411611553156e-05,
      "loss": 0.713,
      "step": 279800
    },
    {
      "epoch": 2.5540185414993797,
      "grad_norm": 3.7855098247528076,
      "learning_rate": 4.7871651215417186e-05,
      "loss": 0.7422,
      "step": 279900
    },
    {
      "epoch": 2.554931016862545,
      "grad_norm": 5.528927326202393,
      "learning_rate": 4.7870890819281216e-05,
      "loss": 0.7918,
      "step": 280000
    },
    {
      "epoch": 2.55584349222571,
      "grad_norm": 4.931251049041748,
      "learning_rate": 4.7870130423145246e-05,
      "loss": 0.7085,
      "step": 280100
    },
    {
      "epoch": 2.556755967588875,
      "grad_norm": 3.9742631912231445,
      "learning_rate": 4.7869370027009276e-05,
      "loss": 0.7463,
      "step": 280200
    },
    {
      "epoch": 2.5576684429520404,
      "grad_norm": 4.416943073272705,
      "learning_rate": 4.78686096308733e-05,
      "loss": 0.7603,
      "step": 280300
    },
    {
      "epoch": 2.5585809183152053,
      "grad_norm": 5.196197986602783,
      "learning_rate": 4.7867849234737336e-05,
      "loss": 0.7198,
      "step": 280400
    },
    {
      "epoch": 2.5594933936783706,
      "grad_norm": 4.023265838623047,
      "learning_rate": 4.786708883860136e-05,
      "loss": 0.7542,
      "step": 280500
    },
    {
      "epoch": 2.560405869041536,
      "grad_norm": 5.33119010925293,
      "learning_rate": 4.786632844246539e-05,
      "loss": 0.725,
      "step": 280600
    },
    {
      "epoch": 2.561318344404701,
      "grad_norm": 4.2814040184021,
      "learning_rate": 4.786556804632942e-05,
      "loss": 0.7425,
      "step": 280700
    },
    {
      "epoch": 2.5622308197678665,
      "grad_norm": 4.37056303024292,
      "learning_rate": 4.786480765019345e-05,
      "loss": 0.7231,
      "step": 280800
    },
    {
      "epoch": 2.5631432951310313,
      "grad_norm": 4.623863220214844,
      "learning_rate": 4.786404725405747e-05,
      "loss": 0.7212,
      "step": 280900
    },
    {
      "epoch": 2.5640557704941966,
      "grad_norm": 4.825305461883545,
      "learning_rate": 4.786328685792151e-05,
      "loss": 0.7445,
      "step": 281000
    },
    {
      "epoch": 2.564968245857362,
      "grad_norm": 4.134830951690674,
      "learning_rate": 4.786252646178553e-05,
      "loss": 0.7004,
      "step": 281100
    },
    {
      "epoch": 2.565880721220527,
      "grad_norm": 4.363703727722168,
      "learning_rate": 4.786176606564956e-05,
      "loss": 0.7524,
      "step": 281200
    },
    {
      "epoch": 2.566793196583692,
      "grad_norm": 4.549022674560547,
      "learning_rate": 4.786100566951359e-05,
      "loss": 0.7626,
      "step": 281300
    },
    {
      "epoch": 2.5677056719468574,
      "grad_norm": 3.462526559829712,
      "learning_rate": 4.7860245273377616e-05,
      "loss": 0.7688,
      "step": 281400
    },
    {
      "epoch": 2.5686181473100227,
      "grad_norm": 4.305879592895508,
      "learning_rate": 4.785948487724165e-05,
      "loss": 0.7853,
      "step": 281500
    },
    {
      "epoch": 2.569530622673188,
      "grad_norm": 3.898777961730957,
      "learning_rate": 4.7858724481105677e-05,
      "loss": 0.7414,
      "step": 281600
    },
    {
      "epoch": 2.5704430980363533,
      "grad_norm": 4.0203962326049805,
      "learning_rate": 4.785796408496971e-05,
      "loss": 0.7588,
      "step": 281700
    },
    {
      "epoch": 2.571355573399518,
      "grad_norm": 4.554797649383545,
      "learning_rate": 4.785720368883374e-05,
      "loss": 0.7172,
      "step": 281800
    },
    {
      "epoch": 2.5722680487626834,
      "grad_norm": 3.9171864986419678,
      "learning_rate": 4.785644329269777e-05,
      "loss": 0.7646,
      "step": 281900
    },
    {
      "epoch": 2.5731805241258487,
      "grad_norm": 3.900747776031494,
      "learning_rate": 4.785568289656179e-05,
      "loss": 0.7418,
      "step": 282000
    },
    {
      "epoch": 2.5740929994890136,
      "grad_norm": 2.5544142723083496,
      "learning_rate": 4.785492250042583e-05,
      "loss": 0.7422,
      "step": 282100
    },
    {
      "epoch": 2.575005474852179,
      "grad_norm": 4.4294257164001465,
      "learning_rate": 4.785416210428985e-05,
      "loss": 0.735,
      "step": 282200
    },
    {
      "epoch": 2.575917950215344,
      "grad_norm": 4.3035173416137695,
      "learning_rate": 4.785340170815388e-05,
      "loss": 0.7835,
      "step": 282300
    },
    {
      "epoch": 2.5768304255785095,
      "grad_norm": 4.2013092041015625,
      "learning_rate": 4.785264131201791e-05,
      "loss": 0.7674,
      "step": 282400
    },
    {
      "epoch": 2.577742900941675,
      "grad_norm": 4.392180442810059,
      "learning_rate": 4.785188091588194e-05,
      "loss": 0.7471,
      "step": 282500
    },
    {
      "epoch": 2.5786553763048397,
      "grad_norm": 4.025987148284912,
      "learning_rate": 4.785112051974597e-05,
      "loss": 0.7247,
      "step": 282600
    },
    {
      "epoch": 2.579567851668005,
      "grad_norm": 4.54083251953125,
      "learning_rate": 4.785036012361e-05,
      "loss": 0.7562,
      "step": 282700
    },
    {
      "epoch": 2.5804803270311703,
      "grad_norm": 4.724671363830566,
      "learning_rate": 4.7849599727474024e-05,
      "loss": 0.744,
      "step": 282800
    },
    {
      "epoch": 2.581392802394335,
      "grad_norm": 3.9653472900390625,
      "learning_rate": 4.784883933133806e-05,
      "loss": 0.7583,
      "step": 282900
    },
    {
      "epoch": 2.5823052777575004,
      "grad_norm": 3.7059497833251953,
      "learning_rate": 4.7848078935202084e-05,
      "loss": 0.7491,
      "step": 283000
    },
    {
      "epoch": 2.5832177531206657,
      "grad_norm": 3.742581605911255,
      "learning_rate": 4.7847318539066114e-05,
      "loss": 0.7649,
      "step": 283100
    },
    {
      "epoch": 2.584130228483831,
      "grad_norm": 4.231660842895508,
      "learning_rate": 4.7846558142930144e-05,
      "loss": 0.7082,
      "step": 283200
    },
    {
      "epoch": 2.5850427038469963,
      "grad_norm": 4.776417255401611,
      "learning_rate": 4.7845797746794174e-05,
      "loss": 0.7268,
      "step": 283300
    },
    {
      "epoch": 2.5859551792101616,
      "grad_norm": 4.148813247680664,
      "learning_rate": 4.78450373506582e-05,
      "loss": 0.7529,
      "step": 283400
    },
    {
      "epoch": 2.5868676545733265,
      "grad_norm": 4.681365966796875,
      "learning_rate": 4.7844276954522234e-05,
      "loss": 0.7416,
      "step": 283500
    },
    {
      "epoch": 2.5877801299364918,
      "grad_norm": 3.856583833694458,
      "learning_rate": 4.784351655838626e-05,
      "loss": 0.7588,
      "step": 283600
    },
    {
      "epoch": 2.588692605299657,
      "grad_norm": 4.6663103103637695,
      "learning_rate": 4.784275616225029e-05,
      "loss": 0.757,
      "step": 283700
    },
    {
      "epoch": 2.589605080662822,
      "grad_norm": 3.8162953853607178,
      "learning_rate": 4.784199576611432e-05,
      "loss": 0.7414,
      "step": 283800
    },
    {
      "epoch": 2.590517556025987,
      "grad_norm": 4.808746337890625,
      "learning_rate": 4.784123536997835e-05,
      "loss": 0.7162,
      "step": 283900
    },
    {
      "epoch": 2.5914300313891525,
      "grad_norm": 4.16181755065918,
      "learning_rate": 4.784047497384238e-05,
      "loss": 0.8117,
      "step": 284000
    },
    {
      "epoch": 2.592342506752318,
      "grad_norm": 4.629458904266357,
      "learning_rate": 4.78397145777064e-05,
      "loss": 0.7641,
      "step": 284100
    },
    {
      "epoch": 2.593254982115483,
      "grad_norm": 3.7545628547668457,
      "learning_rate": 4.783895418157043e-05,
      "loss": 0.7544,
      "step": 284200
    },
    {
      "epoch": 2.594167457478648,
      "grad_norm": 4.05334997177124,
      "learning_rate": 4.783819378543446e-05,
      "loss": 0.7597,
      "step": 284300
    },
    {
      "epoch": 2.5950799328418133,
      "grad_norm": 3.9475674629211426,
      "learning_rate": 4.783743338929849e-05,
      "loss": 0.7468,
      "step": 284400
    },
    {
      "epoch": 2.5959924082049786,
      "grad_norm": 4.356289863586426,
      "learning_rate": 4.7836672993162515e-05,
      "loss": 0.7428,
      "step": 284500
    },
    {
      "epoch": 2.5969048835681434,
      "grad_norm": 4.1399970054626465,
      "learning_rate": 4.783591259702655e-05,
      "loss": 0.7421,
      "step": 284600
    },
    {
      "epoch": 2.5978173589313087,
      "grad_norm": 4.621036529541016,
      "learning_rate": 4.7835152200890575e-05,
      "loss": 0.769,
      "step": 284700
    },
    {
      "epoch": 2.598729834294474,
      "grad_norm": 4.204224586486816,
      "learning_rate": 4.7834391804754605e-05,
      "loss": 0.7486,
      "step": 284800
    },
    {
      "epoch": 2.5996423096576393,
      "grad_norm": 4.043595314025879,
      "learning_rate": 4.7833631408618635e-05,
      "loss": 0.7477,
      "step": 284900
    },
    {
      "epoch": 2.6005547850208046,
      "grad_norm": 4.372663497924805,
      "learning_rate": 4.7832871012482665e-05,
      "loss": 0.7657,
      "step": 285000
    },
    {
      "epoch": 2.6014672603839695,
      "grad_norm": 4.337125301361084,
      "learning_rate": 4.7832110616346695e-05,
      "loss": 0.7744,
      "step": 285100
    },
    {
      "epoch": 2.6023797357471348,
      "grad_norm": 3.1980929374694824,
      "learning_rate": 4.7831350220210725e-05,
      "loss": 0.7316,
      "step": 285200
    },
    {
      "epoch": 2.6032922111103,
      "grad_norm": 4.564276218414307,
      "learning_rate": 4.783058982407475e-05,
      "loss": 0.753,
      "step": 285300
    },
    {
      "epoch": 2.6042046864734654,
      "grad_norm": 4.494230270385742,
      "learning_rate": 4.7829829427938785e-05,
      "loss": 0.7345,
      "step": 285400
    },
    {
      "epoch": 2.6051171618366302,
      "grad_norm": 2.818844795227051,
      "learning_rate": 4.782906903180281e-05,
      "loss": 0.7472,
      "step": 285500
    },
    {
      "epoch": 2.6060296371997955,
      "grad_norm": 4.788093566894531,
      "learning_rate": 4.782830863566684e-05,
      "loss": 0.7185,
      "step": 285600
    },
    {
      "epoch": 2.606942112562961,
      "grad_norm": 3.553546667098999,
      "learning_rate": 4.782754823953087e-05,
      "loss": 0.7253,
      "step": 285700
    },
    {
      "epoch": 2.607854587926126,
      "grad_norm": 3.2920382022857666,
      "learning_rate": 4.78267878433949e-05,
      "loss": 0.7536,
      "step": 285800
    },
    {
      "epoch": 2.6087670632892914,
      "grad_norm": 4.470716953277588,
      "learning_rate": 4.782602744725892e-05,
      "loss": 0.7302,
      "step": 285900
    },
    {
      "epoch": 2.6096795386524563,
      "grad_norm": 4.138998031616211,
      "learning_rate": 4.782526705112296e-05,
      "loss": 0.7663,
      "step": 286000
    },
    {
      "epoch": 2.6105920140156216,
      "grad_norm": 3.930187940597534,
      "learning_rate": 4.782450665498698e-05,
      "loss": 0.7739,
      "step": 286100
    },
    {
      "epoch": 2.611504489378787,
      "grad_norm": 4.4228363037109375,
      "learning_rate": 4.782374625885101e-05,
      "loss": 0.754,
      "step": 286200
    },
    {
      "epoch": 2.6124169647419517,
      "grad_norm": 4.375722408294678,
      "learning_rate": 4.782298586271504e-05,
      "loss": 0.7164,
      "step": 286300
    },
    {
      "epoch": 2.613329440105117,
      "grad_norm": 4.133322715759277,
      "learning_rate": 4.782222546657907e-05,
      "loss": 0.7458,
      "step": 286400
    },
    {
      "epoch": 2.6142419154682823,
      "grad_norm": 4.316246509552002,
      "learning_rate": 4.78214650704431e-05,
      "loss": 0.7578,
      "step": 286500
    },
    {
      "epoch": 2.6151543908314476,
      "grad_norm": 3.402714490890503,
      "learning_rate": 4.782070467430713e-05,
      "loss": 0.7687,
      "step": 286600
    },
    {
      "epoch": 2.616066866194613,
      "grad_norm": 4.153851509094238,
      "learning_rate": 4.7819944278171156e-05,
      "loss": 0.7251,
      "step": 286700
    },
    {
      "epoch": 2.616979341557778,
      "grad_norm": 4.413261413574219,
      "learning_rate": 4.781918388203519e-05,
      "loss": 0.715,
      "step": 286800
    },
    {
      "epoch": 2.617891816920943,
      "grad_norm": 4.299780368804932,
      "learning_rate": 4.7818423485899216e-05,
      "loss": 0.7777,
      "step": 286900
    },
    {
      "epoch": 2.6188042922841084,
      "grad_norm": 3.7241499423980713,
      "learning_rate": 4.7817663089763246e-05,
      "loss": 0.7555,
      "step": 287000
    },
    {
      "epoch": 2.6197167676472737,
      "grad_norm": 4.777988433837891,
      "learning_rate": 4.7816902693627276e-05,
      "loss": 0.7663,
      "step": 287100
    },
    {
      "epoch": 2.6206292430104385,
      "grad_norm": 4.606156349182129,
      "learning_rate": 4.78161422974913e-05,
      "loss": 0.7545,
      "step": 287200
    },
    {
      "epoch": 2.621541718373604,
      "grad_norm": 3.883309841156006,
      "learning_rate": 4.781538190135533e-05,
      "loss": 0.7607,
      "step": 287300
    },
    {
      "epoch": 2.622454193736769,
      "grad_norm": 4.5049638748168945,
      "learning_rate": 4.781462150521936e-05,
      "loss": 0.7485,
      "step": 287400
    },
    {
      "epoch": 2.6233666690999344,
      "grad_norm": 4.651785373687744,
      "learning_rate": 4.781386110908339e-05,
      "loss": 0.7261,
      "step": 287500
    },
    {
      "epoch": 2.6242791444630997,
      "grad_norm": 3.9082045555114746,
      "learning_rate": 4.781310071294742e-05,
      "loss": 0.7223,
      "step": 287600
    },
    {
      "epoch": 2.6251916198262646,
      "grad_norm": 4.780580043792725,
      "learning_rate": 4.781234031681145e-05,
      "loss": 0.7449,
      "step": 287700
    },
    {
      "epoch": 2.62610409518943,
      "grad_norm": 4.998707294464111,
      "learning_rate": 4.781157992067547e-05,
      "loss": 0.7878,
      "step": 287800
    },
    {
      "epoch": 2.627016570552595,
      "grad_norm": 4.726370334625244,
      "learning_rate": 4.781081952453951e-05,
      "loss": 0.7294,
      "step": 287900
    },
    {
      "epoch": 2.62792904591576,
      "grad_norm": 3.3272745609283447,
      "learning_rate": 4.781005912840353e-05,
      "loss": 0.7882,
      "step": 288000
    },
    {
      "epoch": 2.6288415212789253,
      "grad_norm": 4.801259994506836,
      "learning_rate": 4.780929873226756e-05,
      "loss": 0.7524,
      "step": 288100
    },
    {
      "epoch": 2.6297539966420906,
      "grad_norm": 4.667530059814453,
      "learning_rate": 4.780853833613159e-05,
      "loss": 0.7612,
      "step": 288200
    },
    {
      "epoch": 2.630666472005256,
      "grad_norm": 3.4461910724639893,
      "learning_rate": 4.780777793999562e-05,
      "loss": 0.7188,
      "step": 288300
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 5.159572601318359,
      "learning_rate": 4.780701754385965e-05,
      "loss": 0.7218,
      "step": 288400
    },
    {
      "epoch": 2.632491422731586,
      "grad_norm": 4.818763256072998,
      "learning_rate": 4.7806257147723683e-05,
      "loss": 0.7018,
      "step": 288500
    },
    {
      "epoch": 2.6334038980947514,
      "grad_norm": 3.7524795532226562,
      "learning_rate": 4.780549675158771e-05,
      "loss": 0.7587,
      "step": 288600
    },
    {
      "epoch": 2.6343163734579167,
      "grad_norm": 3.7741754055023193,
      "learning_rate": 4.7804736355451744e-05,
      "loss": 0.7603,
      "step": 288700
    },
    {
      "epoch": 2.635228848821082,
      "grad_norm": 3.843740224838257,
      "learning_rate": 4.780397595931577e-05,
      "loss": 0.7192,
      "step": 288800
    },
    {
      "epoch": 2.636141324184247,
      "grad_norm": 2.428179979324341,
      "learning_rate": 4.78032155631798e-05,
      "loss": 0.7297,
      "step": 288900
    },
    {
      "epoch": 2.637053799547412,
      "grad_norm": 3.8244211673736572,
      "learning_rate": 4.780245516704383e-05,
      "loss": 0.7224,
      "step": 289000
    },
    {
      "epoch": 2.6379662749105774,
      "grad_norm": 4.738110065460205,
      "learning_rate": 4.780169477090786e-05,
      "loss": 0.6912,
      "step": 289100
    },
    {
      "epoch": 2.6388787502737427,
      "grad_norm": 5.203180313110352,
      "learning_rate": 4.780093437477188e-05,
      "loss": 0.7302,
      "step": 289200
    },
    {
      "epoch": 2.639791225636908,
      "grad_norm": 4.618938446044922,
      "learning_rate": 4.780017397863592e-05,
      "loss": 0.7689,
      "step": 289300
    },
    {
      "epoch": 2.640703701000073,
      "grad_norm": 4.211232662200928,
      "learning_rate": 4.779941358249994e-05,
      "loss": 0.7716,
      "step": 289400
    },
    {
      "epoch": 2.641616176363238,
      "grad_norm": 5.109865188598633,
      "learning_rate": 4.779865318636397e-05,
      "loss": 0.7603,
      "step": 289500
    },
    {
      "epoch": 2.6425286517264035,
      "grad_norm": 3.786914110183716,
      "learning_rate": 4.7797892790228e-05,
      "loss": 0.7151,
      "step": 289600
    },
    {
      "epoch": 2.6434411270895684,
      "grad_norm": 3.6184728145599365,
      "learning_rate": 4.7797132394092024e-05,
      "loss": 0.7717,
      "step": 289700
    },
    {
      "epoch": 2.6443536024527337,
      "grad_norm": 3.429318428039551,
      "learning_rate": 4.779637199795606e-05,
      "loss": 0.7559,
      "step": 289800
    },
    {
      "epoch": 2.645266077815899,
      "grad_norm": 4.2909111976623535,
      "learning_rate": 4.7795611601820084e-05,
      "loss": 0.7441,
      "step": 289900
    },
    {
      "epoch": 2.6461785531790643,
      "grad_norm": 4.534847259521484,
      "learning_rate": 4.7794851205684114e-05,
      "loss": 0.7372,
      "step": 290000
    },
    {
      "epoch": 2.6470910285422296,
      "grad_norm": 4.544794082641602,
      "learning_rate": 4.7794090809548144e-05,
      "loss": 0.7691,
      "step": 290100
    },
    {
      "epoch": 2.6480035039053944,
      "grad_norm": 3.909726619720459,
      "learning_rate": 4.7793330413412174e-05,
      "loss": 0.7268,
      "step": 290200
    },
    {
      "epoch": 2.6489159792685597,
      "grad_norm": 4.422880172729492,
      "learning_rate": 4.77925700172762e-05,
      "loss": 0.7416,
      "step": 290300
    },
    {
      "epoch": 2.649828454631725,
      "grad_norm": 3.6938819885253906,
      "learning_rate": 4.7791809621140234e-05,
      "loss": 0.7608,
      "step": 290400
    },
    {
      "epoch": 2.65074092999489,
      "grad_norm": 2.6570537090301514,
      "learning_rate": 4.779104922500426e-05,
      "loss": 0.7724,
      "step": 290500
    },
    {
      "epoch": 2.651653405358055,
      "grad_norm": 4.555510520935059,
      "learning_rate": 4.779028882886829e-05,
      "loss": 0.7518,
      "step": 290600
    },
    {
      "epoch": 2.6525658807212205,
      "grad_norm": 4.3610615730285645,
      "learning_rate": 4.778952843273232e-05,
      "loss": 0.7562,
      "step": 290700
    },
    {
      "epoch": 2.6534783560843858,
      "grad_norm": 3.1902222633361816,
      "learning_rate": 4.778876803659635e-05,
      "loss": 0.7128,
      "step": 290800
    },
    {
      "epoch": 2.654390831447551,
      "grad_norm": 3.7394392490386963,
      "learning_rate": 4.778800764046038e-05,
      "loss": 0.7265,
      "step": 290900
    },
    {
      "epoch": 2.6553033068107164,
      "grad_norm": 4.149031162261963,
      "learning_rate": 4.778724724432441e-05,
      "loss": 0.7613,
      "step": 291000
    },
    {
      "epoch": 2.656215782173881,
      "grad_norm": 3.5907647609710693,
      "learning_rate": 4.778648684818843e-05,
      "loss": 0.7926,
      "step": 291100
    },
    {
      "epoch": 2.6571282575370465,
      "grad_norm": 3.685659646987915,
      "learning_rate": 4.778572645205247e-05,
      "loss": 0.7651,
      "step": 291200
    },
    {
      "epoch": 2.658040732900212,
      "grad_norm": 4.865911483764648,
      "learning_rate": 4.778496605591649e-05,
      "loss": 0.7514,
      "step": 291300
    },
    {
      "epoch": 2.6589532082633767,
      "grad_norm": 3.749899387359619,
      "learning_rate": 4.778420565978052e-05,
      "loss": 0.7499,
      "step": 291400
    },
    {
      "epoch": 2.659865683626542,
      "grad_norm": 3.6202502250671387,
      "learning_rate": 4.778344526364455e-05,
      "loss": 0.7535,
      "step": 291500
    },
    {
      "epoch": 2.6607781589897073,
      "grad_norm": 3.479508876800537,
      "learning_rate": 4.778268486750858e-05,
      "loss": 0.7785,
      "step": 291600
    },
    {
      "epoch": 2.6616906343528726,
      "grad_norm": 4.159045219421387,
      "learning_rate": 4.7781924471372605e-05,
      "loss": 0.7282,
      "step": 291700
    },
    {
      "epoch": 2.662603109716038,
      "grad_norm": 5.164236068725586,
      "learning_rate": 4.778116407523664e-05,
      "loss": 0.7377,
      "step": 291800
    },
    {
      "epoch": 2.6635155850792027,
      "grad_norm": 4.921372413635254,
      "learning_rate": 4.7780403679100665e-05,
      "loss": 0.7521,
      "step": 291900
    },
    {
      "epoch": 2.664428060442368,
      "grad_norm": 4.862634658813477,
      "learning_rate": 4.7779643282964695e-05,
      "loss": 0.7249,
      "step": 292000
    },
    {
      "epoch": 2.6653405358055333,
      "grad_norm": 4.2770094871521,
      "learning_rate": 4.7778882886828725e-05,
      "loss": 0.7448,
      "step": 292100
    },
    {
      "epoch": 2.666253011168698,
      "grad_norm": 3.8614015579223633,
      "learning_rate": 4.7778122490692755e-05,
      "loss": 0.7765,
      "step": 292200
    },
    {
      "epoch": 2.6671654865318635,
      "grad_norm": 4.197534084320068,
      "learning_rate": 4.7777362094556785e-05,
      "loss": 0.7514,
      "step": 292300
    },
    {
      "epoch": 2.6680779618950288,
      "grad_norm": 3.910181999206543,
      "learning_rate": 4.7776601698420815e-05,
      "loss": 0.7191,
      "step": 292400
    },
    {
      "epoch": 2.668990437258194,
      "grad_norm": 3.966296434402466,
      "learning_rate": 4.777584130228484e-05,
      "loss": 0.7378,
      "step": 292500
    },
    {
      "epoch": 2.6699029126213594,
      "grad_norm": 4.4865403175354,
      "learning_rate": 4.777508090614887e-05,
      "loss": 0.7083,
      "step": 292600
    },
    {
      "epoch": 2.6708153879845247,
      "grad_norm": 3.9437084197998047,
      "learning_rate": 4.77743205100129e-05,
      "loss": 0.7554,
      "step": 292700
    },
    {
      "epoch": 2.6717278633476895,
      "grad_norm": 4.479506015777588,
      "learning_rate": 4.777356011387692e-05,
      "loss": 0.7565,
      "step": 292800
    },
    {
      "epoch": 2.672640338710855,
      "grad_norm": 3.98154878616333,
      "learning_rate": 4.777279971774096e-05,
      "loss": 0.7604,
      "step": 292900
    },
    {
      "epoch": 2.67355281407402,
      "grad_norm": 4.395720958709717,
      "learning_rate": 4.777203932160498e-05,
      "loss": 0.733,
      "step": 293000
    },
    {
      "epoch": 2.674465289437185,
      "grad_norm": 4.050372123718262,
      "learning_rate": 4.777127892546901e-05,
      "loss": 0.7033,
      "step": 293100
    },
    {
      "epoch": 2.6753777648003503,
      "grad_norm": 3.9869985580444336,
      "learning_rate": 4.777051852933304e-05,
      "loss": 0.7517,
      "step": 293200
    },
    {
      "epoch": 2.6762902401635156,
      "grad_norm": 4.042055606842041,
      "learning_rate": 4.776975813319707e-05,
      "loss": 0.727,
      "step": 293300
    },
    {
      "epoch": 2.677202715526681,
      "grad_norm": 4.069693565368652,
      "learning_rate": 4.77689977370611e-05,
      "loss": 0.7612,
      "step": 293400
    },
    {
      "epoch": 2.678115190889846,
      "grad_norm": 3.2015421390533447,
      "learning_rate": 4.776823734092513e-05,
      "loss": 0.7531,
      "step": 293500
    },
    {
      "epoch": 2.679027666253011,
      "grad_norm": 2.7357630729675293,
      "learning_rate": 4.7767476944789156e-05,
      "loss": 0.7587,
      "step": 293600
    },
    {
      "epoch": 2.6799401416161763,
      "grad_norm": 5.036851406097412,
      "learning_rate": 4.776671654865319e-05,
      "loss": 0.7588,
      "step": 293700
    },
    {
      "epoch": 2.6808526169793416,
      "grad_norm": 5.818563461303711,
      "learning_rate": 4.7765956152517216e-05,
      "loss": 0.736,
      "step": 293800
    },
    {
      "epoch": 2.6817650923425065,
      "grad_norm": 4.294064044952393,
      "learning_rate": 4.7765195756381246e-05,
      "loss": 0.7636,
      "step": 293900
    },
    {
      "epoch": 2.682677567705672,
      "grad_norm": 5.039607524871826,
      "learning_rate": 4.7764435360245276e-05,
      "loss": 0.7265,
      "step": 294000
    },
    {
      "epoch": 2.683590043068837,
      "grad_norm": 3.959088087081909,
      "learning_rate": 4.7763674964109306e-05,
      "loss": 0.7694,
      "step": 294100
    },
    {
      "epoch": 2.6845025184320024,
      "grad_norm": 4.242626667022705,
      "learning_rate": 4.776291456797333e-05,
      "loss": 0.7306,
      "step": 294200
    },
    {
      "epoch": 2.6854149937951677,
      "grad_norm": 4.723871231079102,
      "learning_rate": 4.7762154171837366e-05,
      "loss": 0.7207,
      "step": 294300
    },
    {
      "epoch": 2.686327469158333,
      "grad_norm": 4.045836448669434,
      "learning_rate": 4.776139377570139e-05,
      "loss": 0.733,
      "step": 294400
    },
    {
      "epoch": 2.687239944521498,
      "grad_norm": 4.080225944519043,
      "learning_rate": 4.776063337956542e-05,
      "loss": 0.7182,
      "step": 294500
    },
    {
      "epoch": 2.688152419884663,
      "grad_norm": 4.342936038970947,
      "learning_rate": 4.775987298342945e-05,
      "loss": 0.7385,
      "step": 294600
    },
    {
      "epoch": 2.6890648952478284,
      "grad_norm": 4.165325164794922,
      "learning_rate": 4.775911258729348e-05,
      "loss": 0.7711,
      "step": 294700
    },
    {
      "epoch": 2.6899773706109933,
      "grad_norm": 4.637117385864258,
      "learning_rate": 4.775835219115751e-05,
      "loss": 0.7247,
      "step": 294800
    },
    {
      "epoch": 2.6908898459741586,
      "grad_norm": 3.866770029067993,
      "learning_rate": 4.775759179502154e-05,
      "loss": 0.713,
      "step": 294900
    },
    {
      "epoch": 2.691802321337324,
      "grad_norm": 3.9071507453918457,
      "learning_rate": 4.775683139888556e-05,
      "loss": 0.7527,
      "step": 295000
    },
    {
      "epoch": 2.692714796700489,
      "grad_norm": 4.112789154052734,
      "learning_rate": 4.77560710027496e-05,
      "loss": 0.7513,
      "step": 295100
    },
    {
      "epoch": 2.6936272720636545,
      "grad_norm": 3.3342344760894775,
      "learning_rate": 4.7755310606613623e-05,
      "loss": 0.7511,
      "step": 295200
    },
    {
      "epoch": 2.6945397474268193,
      "grad_norm": 4.225005149841309,
      "learning_rate": 4.7754550210477653e-05,
      "loss": 0.7753,
      "step": 295300
    },
    {
      "epoch": 2.6954522227899846,
      "grad_norm": 4.010132312774658,
      "learning_rate": 4.7753789814341684e-05,
      "loss": 0.7427,
      "step": 295400
    },
    {
      "epoch": 2.69636469815315,
      "grad_norm": 4.209323406219482,
      "learning_rate": 4.775302941820571e-05,
      "loss": 0.7669,
      "step": 295500
    },
    {
      "epoch": 2.697277173516315,
      "grad_norm": 3.693878173828125,
      "learning_rate": 4.775226902206974e-05,
      "loss": 0.7608,
      "step": 295600
    },
    {
      "epoch": 2.69818964887948,
      "grad_norm": 3.697537899017334,
      "learning_rate": 4.775150862593377e-05,
      "loss": 0.7509,
      "step": 295700
    },
    {
      "epoch": 2.6991021242426454,
      "grad_norm": 4.351677894592285,
      "learning_rate": 4.77507482297978e-05,
      "loss": 0.7333,
      "step": 295800
    },
    {
      "epoch": 2.7000145996058107,
      "grad_norm": 4.593519687652588,
      "learning_rate": 4.774998783366183e-05,
      "loss": 0.7139,
      "step": 295900
    },
    {
      "epoch": 2.700927074968976,
      "grad_norm": 4.578489780426025,
      "learning_rate": 4.774922743752586e-05,
      "loss": 0.7523,
      "step": 296000
    },
    {
      "epoch": 2.7018395503321413,
      "grad_norm": 4.717926502227783,
      "learning_rate": 4.774846704138988e-05,
      "loss": 0.7373,
      "step": 296100
    },
    {
      "epoch": 2.702752025695306,
      "grad_norm": 4.473537445068359,
      "learning_rate": 4.774770664525392e-05,
      "loss": 0.7602,
      "step": 296200
    },
    {
      "epoch": 2.7036645010584714,
      "grad_norm": 4.238425254821777,
      "learning_rate": 4.774694624911794e-05,
      "loss": 0.7466,
      "step": 296300
    },
    {
      "epoch": 2.7045769764216367,
      "grad_norm": 3.211223602294922,
      "learning_rate": 4.774618585298197e-05,
      "loss": 0.7437,
      "step": 296400
    },
    {
      "epoch": 2.7054894517848016,
      "grad_norm": 3.155094623565674,
      "learning_rate": 4.7745425456846e-05,
      "loss": 0.7517,
      "step": 296500
    },
    {
      "epoch": 2.706401927147967,
      "grad_norm": 3.442234516143799,
      "learning_rate": 4.774466506071003e-05,
      "loss": 0.7528,
      "step": 296600
    },
    {
      "epoch": 2.707314402511132,
      "grad_norm": 4.3499884605407715,
      "learning_rate": 4.7743904664574054e-05,
      "loss": 0.7327,
      "step": 296700
    },
    {
      "epoch": 2.7082268778742975,
      "grad_norm": 4.8057861328125,
      "learning_rate": 4.774314426843809e-05,
      "loss": 0.7362,
      "step": 296800
    },
    {
      "epoch": 2.709139353237463,
      "grad_norm": 4.3780622482299805,
      "learning_rate": 4.7742383872302114e-05,
      "loss": 0.7409,
      "step": 296900
    },
    {
      "epoch": 2.7100518286006277,
      "grad_norm": 4.203574180603027,
      "learning_rate": 4.7741623476166144e-05,
      "loss": 0.7918,
      "step": 297000
    },
    {
      "epoch": 2.710964303963793,
      "grad_norm": 4.080855369567871,
      "learning_rate": 4.7740863080030174e-05,
      "loss": 0.7516,
      "step": 297100
    },
    {
      "epoch": 2.7118767793269583,
      "grad_norm": 4.7551374435424805,
      "learning_rate": 4.7740102683894204e-05,
      "loss": 0.7327,
      "step": 297200
    },
    {
      "epoch": 2.712789254690123,
      "grad_norm": 4.192632675170898,
      "learning_rate": 4.7739342287758234e-05,
      "loss": 0.7445,
      "step": 297300
    },
    {
      "epoch": 2.7137017300532884,
      "grad_norm": 4.065333843231201,
      "learning_rate": 4.7738581891622265e-05,
      "loss": 0.7405,
      "step": 297400
    },
    {
      "epoch": 2.7146142054164537,
      "grad_norm": 4.040168762207031,
      "learning_rate": 4.773782149548629e-05,
      "loss": 0.7477,
      "step": 297500
    },
    {
      "epoch": 2.715526680779619,
      "grad_norm": 4.771246910095215,
      "learning_rate": 4.7737061099350325e-05,
      "loss": 0.738,
      "step": 297600
    },
    {
      "epoch": 2.7164391561427843,
      "grad_norm": 4.120790958404541,
      "learning_rate": 4.773630070321435e-05,
      "loss": 0.7323,
      "step": 297700
    },
    {
      "epoch": 2.7173516315059496,
      "grad_norm": 4.214909553527832,
      "learning_rate": 4.773554030707838e-05,
      "loss": 0.7829,
      "step": 297800
    },
    {
      "epoch": 2.7182641068691145,
      "grad_norm": 4.135039806365967,
      "learning_rate": 4.773477991094241e-05,
      "loss": 0.723,
      "step": 297900
    },
    {
      "epoch": 2.7191765822322798,
      "grad_norm": 3.754822254180908,
      "learning_rate": 4.773401951480644e-05,
      "loss": 0.7145,
      "step": 298000
    },
    {
      "epoch": 2.720089057595445,
      "grad_norm": 4.081396579742432,
      "learning_rate": 4.773325911867046e-05,
      "loss": 0.7305,
      "step": 298100
    },
    {
      "epoch": 2.72100153295861,
      "grad_norm": 4.1470723152160645,
      "learning_rate": 4.77324987225345e-05,
      "loss": 0.7814,
      "step": 298200
    },
    {
      "epoch": 2.721914008321775,
      "grad_norm": 4.744162082672119,
      "learning_rate": 4.773173832639852e-05,
      "loss": 0.7254,
      "step": 298300
    },
    {
      "epoch": 2.7228264836849405,
      "grad_norm": 5.058165550231934,
      "learning_rate": 4.773097793026255e-05,
      "loss": 0.7925,
      "step": 298400
    },
    {
      "epoch": 2.723738959048106,
      "grad_norm": 3.3713631629943848,
      "learning_rate": 4.773021753412658e-05,
      "loss": 0.7501,
      "step": 298500
    },
    {
      "epoch": 2.724651434411271,
      "grad_norm": 4.837693691253662,
      "learning_rate": 4.7729457137990605e-05,
      "loss": 0.7445,
      "step": 298600
    },
    {
      "epoch": 2.725563909774436,
      "grad_norm": 4.082958698272705,
      "learning_rate": 4.772869674185464e-05,
      "loss": 0.7125,
      "step": 298700
    },
    {
      "epoch": 2.7264763851376013,
      "grad_norm": 4.245000839233398,
      "learning_rate": 4.7727936345718665e-05,
      "loss": 0.7331,
      "step": 298800
    },
    {
      "epoch": 2.7273888605007666,
      "grad_norm": 4.512516021728516,
      "learning_rate": 4.7727175949582695e-05,
      "loss": 0.7647,
      "step": 298900
    },
    {
      "epoch": 2.7283013358639314,
      "grad_norm": 3.8417470455169678,
      "learning_rate": 4.7726415553446725e-05,
      "loss": 0.7007,
      "step": 299000
    },
    {
      "epoch": 2.7292138112270967,
      "grad_norm": 3.5430853366851807,
      "learning_rate": 4.7725655157310755e-05,
      "loss": 0.7511,
      "step": 299100
    },
    {
      "epoch": 2.730126286590262,
      "grad_norm": 3.839170455932617,
      "learning_rate": 4.7724894761174785e-05,
      "loss": 0.7483,
      "step": 299200
    },
    {
      "epoch": 2.7310387619534273,
      "grad_norm": 4.527505874633789,
      "learning_rate": 4.7724134365038816e-05,
      "loss": 0.7769,
      "step": 299300
    },
    {
      "epoch": 2.7319512373165926,
      "grad_norm": 3.1092453002929688,
      "learning_rate": 4.772337396890284e-05,
      "loss": 0.7558,
      "step": 299400
    },
    {
      "epoch": 2.732863712679758,
      "grad_norm": 4.157186985015869,
      "learning_rate": 4.772261357276687e-05,
      "loss": 0.7446,
      "step": 299500
    },
    {
      "epoch": 2.7337761880429228,
      "grad_norm": 4.773948669433594,
      "learning_rate": 4.77218531766309e-05,
      "loss": 0.7376,
      "step": 299600
    },
    {
      "epoch": 2.734688663406088,
      "grad_norm": 4.624686241149902,
      "learning_rate": 4.772109278049493e-05,
      "loss": 0.7667,
      "step": 299700
    },
    {
      "epoch": 2.7356011387692534,
      "grad_norm": 4.332746982574463,
      "learning_rate": 4.772033238435896e-05,
      "loss": 0.7831,
      "step": 299800
    },
    {
      "epoch": 2.7365136141324182,
      "grad_norm": 3.992828607559204,
      "learning_rate": 4.771957198822299e-05,
      "loss": 0.7563,
      "step": 299900
    },
    {
      "epoch": 2.7374260894955835,
      "grad_norm": 4.663727283477783,
      "learning_rate": 4.771881159208701e-05,
      "loss": 0.7307,
      "step": 300000
    },
    {
      "epoch": 2.738338564858749,
      "grad_norm": 3.3838515281677246,
      "learning_rate": 4.771805119595105e-05,
      "loss": 0.7315,
      "step": 300100
    },
    {
      "epoch": 2.739251040221914,
      "grad_norm": 4.311977386474609,
      "learning_rate": 4.771729079981507e-05,
      "loss": 0.7292,
      "step": 300200
    },
    {
      "epoch": 2.7401635155850794,
      "grad_norm": 3.0256595611572266,
      "learning_rate": 4.77165304036791e-05,
      "loss": 0.7267,
      "step": 300300
    },
    {
      "epoch": 2.7410759909482443,
      "grad_norm": 4.352297782897949,
      "learning_rate": 4.771577000754313e-05,
      "loss": 0.7597,
      "step": 300400
    },
    {
      "epoch": 2.7419884663114096,
      "grad_norm": 3.434213161468506,
      "learning_rate": 4.771500961140716e-05,
      "loss": 0.7385,
      "step": 300500
    },
    {
      "epoch": 2.742900941674575,
      "grad_norm": 4.1085076332092285,
      "learning_rate": 4.771424921527119e-05,
      "loss": 0.7555,
      "step": 300600
    },
    {
      "epoch": 2.7438134170377397,
      "grad_norm": 2.708292245864868,
      "learning_rate": 4.771348881913522e-05,
      "loss": 0.7298,
      "step": 300700
    },
    {
      "epoch": 2.744725892400905,
      "grad_norm": 3.820577383041382,
      "learning_rate": 4.7712728422999246e-05,
      "loss": 0.7927,
      "step": 300800
    },
    {
      "epoch": 2.7456383677640703,
      "grad_norm": 3.9249258041381836,
      "learning_rate": 4.7711968026863276e-05,
      "loss": 0.7352,
      "step": 300900
    },
    {
      "epoch": 2.7465508431272356,
      "grad_norm": 4.850990295410156,
      "learning_rate": 4.7711207630727306e-05,
      "loss": 0.7091,
      "step": 301000
    },
    {
      "epoch": 2.747463318490401,
      "grad_norm": 4.5546417236328125,
      "learning_rate": 4.771044723459133e-05,
      "loss": 0.7972,
      "step": 301100
    },
    {
      "epoch": 2.7483757938535662,
      "grad_norm": 3.913883686065674,
      "learning_rate": 4.7709686838455366e-05,
      "loss": 0.7531,
      "step": 301200
    },
    {
      "epoch": 2.749288269216731,
      "grad_norm": 4.271673679351807,
      "learning_rate": 4.770892644231939e-05,
      "loss": 0.7609,
      "step": 301300
    },
    {
      "epoch": 2.7502007445798964,
      "grad_norm": 3.7953639030456543,
      "learning_rate": 4.770816604618342e-05,
      "loss": 0.7888,
      "step": 301400
    },
    {
      "epoch": 2.7511132199430617,
      "grad_norm": 4.163698673248291,
      "learning_rate": 4.770740565004745e-05,
      "loss": 0.757,
      "step": 301500
    },
    {
      "epoch": 2.7520256953062265,
      "grad_norm": 4.0812764167785645,
      "learning_rate": 4.770664525391148e-05,
      "loss": 0.7206,
      "step": 301600
    },
    {
      "epoch": 2.752938170669392,
      "grad_norm": 3.636871576309204,
      "learning_rate": 4.770588485777551e-05,
      "loss": 0.7594,
      "step": 301700
    },
    {
      "epoch": 2.753850646032557,
      "grad_norm": 4.823109149932861,
      "learning_rate": 4.770512446163954e-05,
      "loss": 0.7646,
      "step": 301800
    },
    {
      "epoch": 2.7547631213957224,
      "grad_norm": 4.639152526855469,
      "learning_rate": 4.770436406550356e-05,
      "loss": 0.7395,
      "step": 301900
    },
    {
      "epoch": 2.7556755967588877,
      "grad_norm": 3.814700126647949,
      "learning_rate": 4.77036036693676e-05,
      "loss": 0.7234,
      "step": 302000
    },
    {
      "epoch": 2.7565880721220526,
      "grad_norm": 3.5947577953338623,
      "learning_rate": 4.7702843273231624e-05,
      "loss": 0.6842,
      "step": 302100
    },
    {
      "epoch": 2.757500547485218,
      "grad_norm": 4.534963130950928,
      "learning_rate": 4.7702082877095654e-05,
      "loss": 0.7037,
      "step": 302200
    },
    {
      "epoch": 2.758413022848383,
      "grad_norm": 3.6289844512939453,
      "learning_rate": 4.7701322480959684e-05,
      "loss": 0.7569,
      "step": 302300
    },
    {
      "epoch": 2.759325498211548,
      "grad_norm": 4.37713623046875,
      "learning_rate": 4.7700562084823714e-05,
      "loss": 0.7684,
      "step": 302400
    },
    {
      "epoch": 2.7602379735747133,
      "grad_norm": 4.551497936248779,
      "learning_rate": 4.769980168868774e-05,
      "loss": 0.7712,
      "step": 302500
    },
    {
      "epoch": 2.7611504489378786,
      "grad_norm": 4.726320266723633,
      "learning_rate": 4.7699041292551774e-05,
      "loss": 0.7641,
      "step": 302600
    },
    {
      "epoch": 2.762062924301044,
      "grad_norm": 3.983307361602783,
      "learning_rate": 4.76982808964158e-05,
      "loss": 0.7599,
      "step": 302700
    },
    {
      "epoch": 2.7629753996642092,
      "grad_norm": 3.782399892807007,
      "learning_rate": 4.769752050027983e-05,
      "loss": 0.7788,
      "step": 302800
    },
    {
      "epoch": 2.763887875027374,
      "grad_norm": 3.4031500816345215,
      "learning_rate": 4.769676010414386e-05,
      "loss": 0.7283,
      "step": 302900
    },
    {
      "epoch": 2.7648003503905394,
      "grad_norm": 3.8105757236480713,
      "learning_rate": 4.769599970800789e-05,
      "loss": 0.7565,
      "step": 303000
    },
    {
      "epoch": 2.7657128257537047,
      "grad_norm": 4.2991485595703125,
      "learning_rate": 4.769523931187192e-05,
      "loss": 0.7595,
      "step": 303100
    },
    {
      "epoch": 2.76662530111687,
      "grad_norm": 4.524331092834473,
      "learning_rate": 4.769447891573595e-05,
      "loss": 0.7304,
      "step": 303200
    },
    {
      "epoch": 2.767537776480035,
      "grad_norm": 4.4676032066345215,
      "learning_rate": 4.769371851959997e-05,
      "loss": 0.7617,
      "step": 303300
    },
    {
      "epoch": 2.7684502518432,
      "grad_norm": 4.0756144523620605,
      "learning_rate": 4.769295812346401e-05,
      "loss": 0.7387,
      "step": 303400
    },
    {
      "epoch": 2.7693627272063654,
      "grad_norm": 4.427232265472412,
      "learning_rate": 4.769219772732803e-05,
      "loss": 0.7673,
      "step": 303500
    },
    {
      "epoch": 2.7702752025695307,
      "grad_norm": 4.25983190536499,
      "learning_rate": 4.769143733119206e-05,
      "loss": 0.7553,
      "step": 303600
    },
    {
      "epoch": 2.771187677932696,
      "grad_norm": 4.265072345733643,
      "learning_rate": 4.769067693505609e-05,
      "loss": 0.7308,
      "step": 303700
    },
    {
      "epoch": 2.772100153295861,
      "grad_norm": 4.966302871704102,
      "learning_rate": 4.768991653892012e-05,
      "loss": 0.7755,
      "step": 303800
    },
    {
      "epoch": 2.773012628659026,
      "grad_norm": 4.804720878601074,
      "learning_rate": 4.7689156142784144e-05,
      "loss": 0.7537,
      "step": 303900
    },
    {
      "epoch": 2.7739251040221915,
      "grad_norm": 3.3262321949005127,
      "learning_rate": 4.7688395746648174e-05,
      "loss": 0.731,
      "step": 304000
    },
    {
      "epoch": 2.7748375793853564,
      "grad_norm": 4.49279260635376,
      "learning_rate": 4.7687635350512205e-05,
      "loss": 0.7525,
      "step": 304100
    },
    {
      "epoch": 2.7757500547485217,
      "grad_norm": 4.357911109924316,
      "learning_rate": 4.7686874954376235e-05,
      "loss": 0.695,
      "step": 304200
    },
    {
      "epoch": 2.776662530111687,
      "grad_norm": 4.6520209312438965,
      "learning_rate": 4.7686114558240265e-05,
      "loss": 0.7545,
      "step": 304300
    },
    {
      "epoch": 2.7775750054748523,
      "grad_norm": 3.1896824836730957,
      "learning_rate": 4.768535416210429e-05,
      "loss": 0.7556,
      "step": 304400
    },
    {
      "epoch": 2.7784874808380176,
      "grad_norm": 3.9051835536956787,
      "learning_rate": 4.7684593765968325e-05,
      "loss": 0.7055,
      "step": 304500
    },
    {
      "epoch": 2.7793999562011824,
      "grad_norm": 4.137210369110107,
      "learning_rate": 4.768383336983235e-05,
      "loss": 0.7137,
      "step": 304600
    },
    {
      "epoch": 2.7803124315643477,
      "grad_norm": 3.4745113849639893,
      "learning_rate": 4.768307297369638e-05,
      "loss": 0.76,
      "step": 304700
    },
    {
      "epoch": 2.781224906927513,
      "grad_norm": 3.3531410694122314,
      "learning_rate": 4.768231257756041e-05,
      "loss": 0.7232,
      "step": 304800
    },
    {
      "epoch": 2.7821373822906783,
      "grad_norm": 6.097695827484131,
      "learning_rate": 4.768155218142444e-05,
      "loss": 0.7624,
      "step": 304900
    },
    {
      "epoch": 2.783049857653843,
      "grad_norm": 4.4937424659729,
      "learning_rate": 4.768079178528846e-05,
      "loss": 0.7369,
      "step": 305000
    },
    {
      "epoch": 2.7839623330170085,
      "grad_norm": 4.188962936401367,
      "learning_rate": 4.76800313891525e-05,
      "loss": 0.7303,
      "step": 305100
    },
    {
      "epoch": 2.7848748083801738,
      "grad_norm": 4.431487560272217,
      "learning_rate": 4.767927099301652e-05,
      "loss": 0.7104,
      "step": 305200
    },
    {
      "epoch": 2.785787283743339,
      "grad_norm": 3.905019521713257,
      "learning_rate": 4.767851059688055e-05,
      "loss": 0.7268,
      "step": 305300
    },
    {
      "epoch": 2.7866997591065044,
      "grad_norm": 3.2044003009796143,
      "learning_rate": 4.767775020074458e-05,
      "loss": 0.7107,
      "step": 305400
    },
    {
      "epoch": 2.787612234469669,
      "grad_norm": 4.229112148284912,
      "learning_rate": 4.767698980460861e-05,
      "loss": 0.7424,
      "step": 305500
    },
    {
      "epoch": 2.7885247098328345,
      "grad_norm": 4.2432146072387695,
      "learning_rate": 4.767622940847264e-05,
      "loss": 0.7212,
      "step": 305600
    },
    {
      "epoch": 2.789437185196,
      "grad_norm": 4.02727746963501,
      "learning_rate": 4.767546901233667e-05,
      "loss": 0.7603,
      "step": 305700
    },
    {
      "epoch": 2.7903496605591647,
      "grad_norm": 4.666802406311035,
      "learning_rate": 4.7674708616200695e-05,
      "loss": 0.7289,
      "step": 305800
    },
    {
      "epoch": 2.79126213592233,
      "grad_norm": 4.754619598388672,
      "learning_rate": 4.767394822006473e-05,
      "loss": 0.7391,
      "step": 305900
    },
    {
      "epoch": 2.7921746112854953,
      "grad_norm": 5.3465142250061035,
      "learning_rate": 4.7673187823928755e-05,
      "loss": 0.7723,
      "step": 306000
    },
    {
      "epoch": 2.7930870866486606,
      "grad_norm": 3.3580658435821533,
      "learning_rate": 4.7672427427792786e-05,
      "loss": 0.7609,
      "step": 306100
    },
    {
      "epoch": 2.793999562011826,
      "grad_norm": 4.395479202270508,
      "learning_rate": 4.7671667031656816e-05,
      "loss": 0.6949,
      "step": 306200
    },
    {
      "epoch": 2.7949120373749907,
      "grad_norm": 4.083804130554199,
      "learning_rate": 4.7670906635520846e-05,
      "loss": 0.7402,
      "step": 306300
    },
    {
      "epoch": 2.795824512738156,
      "grad_norm": 4.0578436851501465,
      "learning_rate": 4.767014623938487e-05,
      "loss": 0.7756,
      "step": 306400
    },
    {
      "epoch": 2.7967369881013213,
      "grad_norm": 4.447919845581055,
      "learning_rate": 4.7669385843248906e-05,
      "loss": 0.7747,
      "step": 306500
    },
    {
      "epoch": 2.7976494634644866,
      "grad_norm": 4.390684604644775,
      "learning_rate": 4.766862544711293e-05,
      "loss": 0.7514,
      "step": 306600
    },
    {
      "epoch": 2.7985619388276515,
      "grad_norm": 3.7358899116516113,
      "learning_rate": 4.766786505097696e-05,
      "loss": 0.7556,
      "step": 306700
    },
    {
      "epoch": 2.7994744141908168,
      "grad_norm": 4.162145137786865,
      "learning_rate": 4.766710465484099e-05,
      "loss": 0.7509,
      "step": 306800
    },
    {
      "epoch": 2.800386889553982,
      "grad_norm": 4.642561912536621,
      "learning_rate": 4.766634425870501e-05,
      "loss": 0.7538,
      "step": 306900
    },
    {
      "epoch": 2.8012993649171474,
      "grad_norm": 4.341142654418945,
      "learning_rate": 4.766558386256905e-05,
      "loss": 0.7491,
      "step": 307000
    },
    {
      "epoch": 2.8022118402803127,
      "grad_norm": 4.424136161804199,
      "learning_rate": 4.766482346643307e-05,
      "loss": 0.7825,
      "step": 307100
    },
    {
      "epoch": 2.8031243156434775,
      "grad_norm": 3.891338586807251,
      "learning_rate": 4.76640630702971e-05,
      "loss": 0.7831,
      "step": 307200
    },
    {
      "epoch": 2.804036791006643,
      "grad_norm": 3.9496474266052246,
      "learning_rate": 4.766330267416113e-05,
      "loss": 0.776,
      "step": 307300
    },
    {
      "epoch": 2.804949266369808,
      "grad_norm": 3.6732547283172607,
      "learning_rate": 4.766254227802516e-05,
      "loss": 0.7514,
      "step": 307400
    },
    {
      "epoch": 2.805861741732973,
      "grad_norm": 3.9800522327423096,
      "learning_rate": 4.7661781881889186e-05,
      "loss": 0.7024,
      "step": 307500
    },
    {
      "epoch": 2.8067742170961383,
      "grad_norm": 3.6837780475616455,
      "learning_rate": 4.766102148575322e-05,
      "loss": 0.7315,
      "step": 307600
    },
    {
      "epoch": 2.8076866924593036,
      "grad_norm": 3.7594003677368164,
      "learning_rate": 4.7660261089617246e-05,
      "loss": 0.7442,
      "step": 307700
    },
    {
      "epoch": 2.808599167822469,
      "grad_norm": 3.6754660606384277,
      "learning_rate": 4.7659500693481276e-05,
      "loss": 0.8179,
      "step": 307800
    },
    {
      "epoch": 2.809511643185634,
      "grad_norm": 4.539149761199951,
      "learning_rate": 4.7658740297345306e-05,
      "loss": 0.7336,
      "step": 307900
    },
    {
      "epoch": 2.810424118548799,
      "grad_norm": 4.668184757232666,
      "learning_rate": 4.7657979901209336e-05,
      "loss": 0.7423,
      "step": 308000
    },
    {
      "epoch": 2.8113365939119643,
      "grad_norm": 4.299416542053223,
      "learning_rate": 4.7657219505073367e-05,
      "loss": 0.7526,
      "step": 308100
    },
    {
      "epoch": 2.8122490692751296,
      "grad_norm": 4.242038249969482,
      "learning_rate": 4.76564591089374e-05,
      "loss": 0.7824,
      "step": 308200
    },
    {
      "epoch": 2.813161544638295,
      "grad_norm": 4.412393569946289,
      "learning_rate": 4.765569871280142e-05,
      "loss": 0.7586,
      "step": 308300
    },
    {
      "epoch": 2.81407402000146,
      "grad_norm": 4.247702598571777,
      "learning_rate": 4.765493831666546e-05,
      "loss": 0.745,
      "step": 308400
    },
    {
      "epoch": 2.814986495364625,
      "grad_norm": 3.8871281147003174,
      "learning_rate": 4.765417792052948e-05,
      "loss": 0.7634,
      "step": 308500
    },
    {
      "epoch": 2.8158989707277904,
      "grad_norm": 4.727154731750488,
      "learning_rate": 4.765341752439351e-05,
      "loss": 0.7676,
      "step": 308600
    },
    {
      "epoch": 2.8168114460909557,
      "grad_norm": 3.687052011489868,
      "learning_rate": 4.765265712825754e-05,
      "loss": 0.7517,
      "step": 308700
    },
    {
      "epoch": 2.817723921454121,
      "grad_norm": 4.312893390655518,
      "learning_rate": 4.765189673212157e-05,
      "loss": 0.7331,
      "step": 308800
    },
    {
      "epoch": 2.818636396817286,
      "grad_norm": 4.596399307250977,
      "learning_rate": 4.7651136335985594e-05,
      "loss": 0.7926,
      "step": 308900
    },
    {
      "epoch": 2.819548872180451,
      "grad_norm": 3.924400568008423,
      "learning_rate": 4.765037593984963e-05,
      "loss": 0.741,
      "step": 309000
    },
    {
      "epoch": 2.8204613475436164,
      "grad_norm": 4.138451099395752,
      "learning_rate": 4.7649615543713654e-05,
      "loss": 0.7701,
      "step": 309100
    },
    {
      "epoch": 2.8213738229067813,
      "grad_norm": 3.604227304458618,
      "learning_rate": 4.7648855147577684e-05,
      "loss": 0.7121,
      "step": 309200
    },
    {
      "epoch": 2.8222862982699466,
      "grad_norm": 3.8958828449249268,
      "learning_rate": 4.7648094751441714e-05,
      "loss": 0.7461,
      "step": 309300
    },
    {
      "epoch": 2.823198773633112,
      "grad_norm": 3.9239659309387207,
      "learning_rate": 4.7647334355305744e-05,
      "loss": 0.7358,
      "step": 309400
    },
    {
      "epoch": 2.824111248996277,
      "grad_norm": 3.0126564502716064,
      "learning_rate": 4.7646573959169774e-05,
      "loss": 0.7391,
      "step": 309500
    },
    {
      "epoch": 2.8250237243594425,
      "grad_norm": 4.107829570770264,
      "learning_rate": 4.76458135630338e-05,
      "loss": 0.7129,
      "step": 309600
    },
    {
      "epoch": 2.8259361997226073,
      "grad_norm": 4.4312849044799805,
      "learning_rate": 4.764505316689783e-05,
      "loss": 0.7017,
      "step": 309700
    },
    {
      "epoch": 2.8268486750857726,
      "grad_norm": 3.6389880180358887,
      "learning_rate": 4.764429277076186e-05,
      "loss": 0.7318,
      "step": 309800
    },
    {
      "epoch": 2.827761150448938,
      "grad_norm": 4.193134307861328,
      "learning_rate": 4.764353237462589e-05,
      "loss": 0.7296,
      "step": 309900
    },
    {
      "epoch": 2.828673625812103,
      "grad_norm": 4.685765743255615,
      "learning_rate": 4.764277197848991e-05,
      "loss": 0.7951,
      "step": 310000
    },
    {
      "epoch": 2.829586101175268,
      "grad_norm": 3.785226345062256,
      "learning_rate": 4.764201158235395e-05,
      "loss": 0.7209,
      "step": 310100
    },
    {
      "epoch": 2.8304985765384334,
      "grad_norm": 4.530745983123779,
      "learning_rate": 4.764125118621797e-05,
      "loss": 0.7597,
      "step": 310200
    },
    {
      "epoch": 2.8314110519015987,
      "grad_norm": 4.315216064453125,
      "learning_rate": 4.7640490790082e-05,
      "loss": 0.7499,
      "step": 310300
    },
    {
      "epoch": 2.832323527264764,
      "grad_norm": 4.535353660583496,
      "learning_rate": 4.763973039394603e-05,
      "loss": 0.7204,
      "step": 310400
    },
    {
      "epoch": 2.8332360026279293,
      "grad_norm": 4.242471218109131,
      "learning_rate": 4.763896999781006e-05,
      "loss": 0.7311,
      "step": 310500
    },
    {
      "epoch": 2.834148477991094,
      "grad_norm": 3.805107593536377,
      "learning_rate": 4.763820960167409e-05,
      "loss": 0.7521,
      "step": 310600
    },
    {
      "epoch": 2.8350609533542594,
      "grad_norm": 3.944469690322876,
      "learning_rate": 4.763744920553812e-05,
      "loss": 0.7372,
      "step": 310700
    },
    {
      "epoch": 2.8359734287174247,
      "grad_norm": 4.451483249664307,
      "learning_rate": 4.7636688809402144e-05,
      "loss": 0.7144,
      "step": 310800
    },
    {
      "epoch": 2.8368859040805896,
      "grad_norm": 3.1178500652313232,
      "learning_rate": 4.763592841326618e-05,
      "loss": 0.7451,
      "step": 310900
    },
    {
      "epoch": 2.837798379443755,
      "grad_norm": 4.126510143280029,
      "learning_rate": 4.7635168017130205e-05,
      "loss": 0.7051,
      "step": 311000
    },
    {
      "epoch": 2.83871085480692,
      "grad_norm": 4.668708324432373,
      "learning_rate": 4.7634407620994235e-05,
      "loss": 0.7059,
      "step": 311100
    },
    {
      "epoch": 2.8396233301700855,
      "grad_norm": 4.5374755859375,
      "learning_rate": 4.7633647224858265e-05,
      "loss": 0.7542,
      "step": 311200
    },
    {
      "epoch": 2.840535805533251,
      "grad_norm": 4.449990749359131,
      "learning_rate": 4.7632886828722295e-05,
      "loss": 0.7098,
      "step": 311300
    },
    {
      "epoch": 2.8414482808964157,
      "grad_norm": 3.5085465908050537,
      "learning_rate": 4.763212643258632e-05,
      "loss": 0.7303,
      "step": 311400
    },
    {
      "epoch": 2.842360756259581,
      "grad_norm": 4.058475971221924,
      "learning_rate": 4.7631366036450355e-05,
      "loss": 0.764,
      "step": 311500
    },
    {
      "epoch": 2.8432732316227463,
      "grad_norm": 4.709648132324219,
      "learning_rate": 4.763060564031438e-05,
      "loss": 0.7498,
      "step": 311600
    },
    {
      "epoch": 2.844185706985911,
      "grad_norm": 3.3618416786193848,
      "learning_rate": 4.762984524417841e-05,
      "loss": 0.7212,
      "step": 311700
    },
    {
      "epoch": 2.8450981823490764,
      "grad_norm": 5.279921531677246,
      "learning_rate": 4.762908484804244e-05,
      "loss": 0.7212,
      "step": 311800
    },
    {
      "epoch": 2.8460106577122417,
      "grad_norm": 3.651196002960205,
      "learning_rate": 4.762832445190647e-05,
      "loss": 0.7216,
      "step": 311900
    },
    {
      "epoch": 2.846923133075407,
      "grad_norm": 3.921597719192505,
      "learning_rate": 4.76275640557705e-05,
      "loss": 0.7243,
      "step": 312000
    },
    {
      "epoch": 2.8478356084385723,
      "grad_norm": 4.163989067077637,
      "learning_rate": 4.762680365963453e-05,
      "loss": 0.7448,
      "step": 312100
    },
    {
      "epoch": 2.8487480838017376,
      "grad_norm": 4.773681163787842,
      "learning_rate": 4.762604326349855e-05,
      "loss": 0.7544,
      "step": 312200
    },
    {
      "epoch": 2.8496605591649025,
      "grad_norm": 4.163592338562012,
      "learning_rate": 4.762528286736259e-05,
      "loss": 0.7051,
      "step": 312300
    },
    {
      "epoch": 2.8505730345280678,
      "grad_norm": 4.97133207321167,
      "learning_rate": 4.762452247122661e-05,
      "loss": 0.7926,
      "step": 312400
    },
    {
      "epoch": 2.851485509891233,
      "grad_norm": 3.914813995361328,
      "learning_rate": 4.762376207509064e-05,
      "loss": 0.7697,
      "step": 312500
    },
    {
      "epoch": 2.852397985254398,
      "grad_norm": 4.12174129486084,
      "learning_rate": 4.762300167895467e-05,
      "loss": 0.7592,
      "step": 312600
    },
    {
      "epoch": 2.853310460617563,
      "grad_norm": 4.237317085266113,
      "learning_rate": 4.7622241282818695e-05,
      "loss": 0.755,
      "step": 312700
    },
    {
      "epoch": 2.8542229359807285,
      "grad_norm": 4.400280475616455,
      "learning_rate": 4.7621480886682726e-05,
      "loss": 0.7244,
      "step": 312800
    },
    {
      "epoch": 2.855135411343894,
      "grad_norm": 3.4712705612182617,
      "learning_rate": 4.7620720490546756e-05,
      "loss": 0.7441,
      "step": 312900
    },
    {
      "epoch": 2.856047886707059,
      "grad_norm": 5.468940258026123,
      "learning_rate": 4.7619960094410786e-05,
      "loss": 0.766,
      "step": 313000
    },
    {
      "epoch": 2.856960362070224,
      "grad_norm": 3.696772575378418,
      "learning_rate": 4.7619199698274816e-05,
      "loss": 0.7663,
      "step": 313100
    },
    {
      "epoch": 2.8578728374333893,
      "grad_norm": 3.846517324447632,
      "learning_rate": 4.7618439302138846e-05,
      "loss": 0.7787,
      "step": 313200
    },
    {
      "epoch": 2.8587853127965546,
      "grad_norm": 4.185659885406494,
      "learning_rate": 4.761767890600287e-05,
      "loss": 0.7744,
      "step": 313300
    },
    {
      "epoch": 2.8596977881597194,
      "grad_norm": 3.2796335220336914,
      "learning_rate": 4.7616918509866906e-05,
      "loss": 0.7314,
      "step": 313400
    },
    {
      "epoch": 2.8606102635228847,
      "grad_norm": 4.524516582489014,
      "learning_rate": 4.761615811373093e-05,
      "loss": 0.7208,
      "step": 313500
    },
    {
      "epoch": 2.86152273888605,
      "grad_norm": 4.219945430755615,
      "learning_rate": 4.761539771759496e-05,
      "loss": 0.7348,
      "step": 313600
    },
    {
      "epoch": 2.8624352142492153,
      "grad_norm": 4.273396015167236,
      "learning_rate": 4.761463732145899e-05,
      "loss": 0.7453,
      "step": 313700
    },
    {
      "epoch": 2.8633476896123806,
      "grad_norm": 4.342989921569824,
      "learning_rate": 4.761387692532302e-05,
      "loss": 0.7398,
      "step": 313800
    },
    {
      "epoch": 2.864260164975546,
      "grad_norm": 3.562007188796997,
      "learning_rate": 4.761311652918705e-05,
      "loss": 0.7523,
      "step": 313900
    },
    {
      "epoch": 2.8651726403387108,
      "grad_norm": 4.302432537078857,
      "learning_rate": 4.761235613305108e-05,
      "loss": 0.7428,
      "step": 314000
    },
    {
      "epoch": 2.866085115701876,
      "grad_norm": 4.193124771118164,
      "learning_rate": 4.76115957369151e-05,
      "loss": 0.7335,
      "step": 314100
    },
    {
      "epoch": 2.8669975910650414,
      "grad_norm": 3.913848638534546,
      "learning_rate": 4.761083534077914e-05,
      "loss": 0.7564,
      "step": 314200
    },
    {
      "epoch": 2.8679100664282062,
      "grad_norm": 2.792307138442993,
      "learning_rate": 4.761007494464316e-05,
      "loss": 0.7392,
      "step": 314300
    },
    {
      "epoch": 2.8688225417913715,
      "grad_norm": 3.6929562091827393,
      "learning_rate": 4.760931454850719e-05,
      "loss": 0.7429,
      "step": 314400
    },
    {
      "epoch": 2.869735017154537,
      "grad_norm": 4.311702728271484,
      "learning_rate": 4.760855415237122e-05,
      "loss": 0.7545,
      "step": 314500
    },
    {
      "epoch": 2.870647492517702,
      "grad_norm": 4.504526138305664,
      "learning_rate": 4.760779375623525e-05,
      "loss": 0.7076,
      "step": 314600
    },
    {
      "epoch": 2.8715599678808674,
      "grad_norm": 4.0448126792907715,
      "learning_rate": 4.7607033360099276e-05,
      "loss": 0.6736,
      "step": 314700
    },
    {
      "epoch": 2.8724724432440323,
      "grad_norm": 4.211890697479248,
      "learning_rate": 4.760627296396331e-05,
      "loss": 0.7843,
      "step": 314800
    },
    {
      "epoch": 2.8733849186071976,
      "grad_norm": 4.933600425720215,
      "learning_rate": 4.7605512567827337e-05,
      "loss": 0.7254,
      "step": 314900
    },
    {
      "epoch": 2.874297393970363,
      "grad_norm": 3.9192326068878174,
      "learning_rate": 4.760475217169137e-05,
      "loss": 0.7487,
      "step": 315000
    },
    {
      "epoch": 2.8752098693335277,
      "grad_norm": 4.313394069671631,
      "learning_rate": 4.76039917755554e-05,
      "loss": 0.74,
      "step": 315100
    },
    {
      "epoch": 2.876122344696693,
      "grad_norm": 3.983884334564209,
      "learning_rate": 4.760323137941943e-05,
      "loss": 0.7879,
      "step": 315200
    },
    {
      "epoch": 2.8770348200598583,
      "grad_norm": 3.0730905532836914,
      "learning_rate": 4.760247098328346e-05,
      "loss": 0.7228,
      "step": 315300
    },
    {
      "epoch": 2.8779472954230236,
      "grad_norm": 3.676227331161499,
      "learning_rate": 4.760171058714748e-05,
      "loss": 0.7175,
      "step": 315400
    },
    {
      "epoch": 2.878859770786189,
      "grad_norm": 3.0993006229400635,
      "learning_rate": 4.760095019101151e-05,
      "loss": 0.7612,
      "step": 315500
    },
    {
      "epoch": 2.8797722461493542,
      "grad_norm": 3.577782392501831,
      "learning_rate": 4.760018979487554e-05,
      "loss": 0.7299,
      "step": 315600
    },
    {
      "epoch": 2.880684721512519,
      "grad_norm": 5.16627311706543,
      "learning_rate": 4.759942939873957e-05,
      "loss": 0.7757,
      "step": 315700
    },
    {
      "epoch": 2.8815971968756844,
      "grad_norm": 3.991960048675537,
      "learning_rate": 4.7598669002603594e-05,
      "loss": 0.7681,
      "step": 315800
    },
    {
      "epoch": 2.8825096722388497,
      "grad_norm": 4.3906941413879395,
      "learning_rate": 4.759790860646763e-05,
      "loss": 0.7202,
      "step": 315900
    },
    {
      "epoch": 2.8834221476020145,
      "grad_norm": 3.994208812713623,
      "learning_rate": 4.7597148210331654e-05,
      "loss": 0.7533,
      "step": 316000
    },
    {
      "epoch": 2.88433462296518,
      "grad_norm": 4.276213645935059,
      "learning_rate": 4.7596387814195684e-05,
      "loss": 0.7705,
      "step": 316100
    },
    {
      "epoch": 2.885247098328345,
      "grad_norm": 3.014376163482666,
      "learning_rate": 4.7595627418059714e-05,
      "loss": 0.7375,
      "step": 316200
    },
    {
      "epoch": 2.8861595736915104,
      "grad_norm": 4.027278423309326,
      "learning_rate": 4.7594867021923744e-05,
      "loss": 0.7572,
      "step": 316300
    },
    {
      "epoch": 2.8870720490546757,
      "grad_norm": 3.7967681884765625,
      "learning_rate": 4.7594106625787774e-05,
      "loss": 0.7222,
      "step": 316400
    },
    {
      "epoch": 2.8879845244178406,
      "grad_norm": 4.743557453155518,
      "learning_rate": 4.7593346229651804e-05,
      "loss": 0.7713,
      "step": 316500
    },
    {
      "epoch": 2.888896999781006,
      "grad_norm": 4.53524112701416,
      "learning_rate": 4.759258583351583e-05,
      "loss": 0.7611,
      "step": 316600
    },
    {
      "epoch": 2.889809475144171,
      "grad_norm": 5.257235527038574,
      "learning_rate": 4.7591825437379864e-05,
      "loss": 0.7599,
      "step": 316700
    },
    {
      "epoch": 2.890721950507336,
      "grad_norm": 3.7690694332122803,
      "learning_rate": 4.759106504124389e-05,
      "loss": 0.7617,
      "step": 316800
    },
    {
      "epoch": 2.8916344258705013,
      "grad_norm": 4.375894069671631,
      "learning_rate": 4.759030464510792e-05,
      "loss": 0.7086,
      "step": 316900
    },
    {
      "epoch": 2.8925469012336666,
      "grad_norm": 5.446958065032959,
      "learning_rate": 4.758954424897195e-05,
      "loss": 0.7474,
      "step": 317000
    },
    {
      "epoch": 2.893459376596832,
      "grad_norm": 4.321681499481201,
      "learning_rate": 4.758878385283598e-05,
      "loss": 0.7459,
      "step": 317100
    },
    {
      "epoch": 2.8943718519599972,
      "grad_norm": 4.436869144439697,
      "learning_rate": 4.75880234567e-05,
      "loss": 0.7388,
      "step": 317200
    },
    {
      "epoch": 2.8952843273231625,
      "grad_norm": 4.406415939331055,
      "learning_rate": 4.758726306056404e-05,
      "loss": 0.7881,
      "step": 317300
    },
    {
      "epoch": 2.8961968026863274,
      "grad_norm": 3.936671733856201,
      "learning_rate": 4.758650266442806e-05,
      "loss": 0.7551,
      "step": 317400
    },
    {
      "epoch": 2.8971092780494927,
      "grad_norm": 5.107023239135742,
      "learning_rate": 4.758574226829209e-05,
      "loss": 0.7313,
      "step": 317500
    },
    {
      "epoch": 2.898021753412658,
      "grad_norm": 4.760341167449951,
      "learning_rate": 4.758498187215612e-05,
      "loss": 0.7367,
      "step": 317600
    },
    {
      "epoch": 2.898934228775823,
      "grad_norm": 3.697040319442749,
      "learning_rate": 4.758422147602015e-05,
      "loss": 0.7288,
      "step": 317700
    },
    {
      "epoch": 2.899846704138988,
      "grad_norm": 3.5482962131500244,
      "learning_rate": 4.758346107988418e-05,
      "loss": 0.7494,
      "step": 317800
    },
    {
      "epoch": 2.9007591795021535,
      "grad_norm": 3.724179983139038,
      "learning_rate": 4.758270068374821e-05,
      "loss": 0.7545,
      "step": 317900
    },
    {
      "epoch": 2.9016716548653188,
      "grad_norm": 3.9862570762634277,
      "learning_rate": 4.7581940287612235e-05,
      "loss": 0.7461,
      "step": 318000
    },
    {
      "epoch": 2.902584130228484,
      "grad_norm": 3.674309730529785,
      "learning_rate": 4.7581179891476265e-05,
      "loss": 0.7564,
      "step": 318100
    },
    {
      "epoch": 2.903496605591649,
      "grad_norm": 3.416672706604004,
      "learning_rate": 4.7580419495340295e-05,
      "loss": 0.7597,
      "step": 318200
    },
    {
      "epoch": 2.904409080954814,
      "grad_norm": 4.3306989669799805,
      "learning_rate": 4.757965909920432e-05,
      "loss": 0.7354,
      "step": 318300
    },
    {
      "epoch": 2.9053215563179795,
      "grad_norm": 4.635838985443115,
      "learning_rate": 4.7578898703068355e-05,
      "loss": 0.6979,
      "step": 318400
    },
    {
      "epoch": 2.9062340316811444,
      "grad_norm": 4.624248027801514,
      "learning_rate": 4.757813830693238e-05,
      "loss": 0.7475,
      "step": 318500
    },
    {
      "epoch": 2.9071465070443097,
      "grad_norm": 3.8563408851623535,
      "learning_rate": 4.757737791079641e-05,
      "loss": 0.7361,
      "step": 318600
    },
    {
      "epoch": 2.908058982407475,
      "grad_norm": 4.223636627197266,
      "learning_rate": 4.757661751466044e-05,
      "loss": 0.7411,
      "step": 318700
    },
    {
      "epoch": 2.9089714577706403,
      "grad_norm": 3.3714568614959717,
      "learning_rate": 4.757585711852447e-05,
      "loss": 0.7104,
      "step": 318800
    },
    {
      "epoch": 2.9098839331338056,
      "grad_norm": 3.9874398708343506,
      "learning_rate": 4.75750967223885e-05,
      "loss": 0.7177,
      "step": 318900
    },
    {
      "epoch": 2.910796408496971,
      "grad_norm": 4.549086093902588,
      "learning_rate": 4.757433632625253e-05,
      "loss": 0.7627,
      "step": 319000
    },
    {
      "epoch": 2.9117088838601357,
      "grad_norm": 4.21706485748291,
      "learning_rate": 4.757357593011655e-05,
      "loss": 0.7289,
      "step": 319100
    },
    {
      "epoch": 2.912621359223301,
      "grad_norm": 4.1700663566589355,
      "learning_rate": 4.757281553398059e-05,
      "loss": 0.761,
      "step": 319200
    },
    {
      "epoch": 2.9135338345864663,
      "grad_norm": 3.9366748332977295,
      "learning_rate": 4.757205513784461e-05,
      "loss": 0.6952,
      "step": 319300
    },
    {
      "epoch": 2.914446309949631,
      "grad_norm": 3.265507698059082,
      "learning_rate": 4.757129474170864e-05,
      "loss": 0.7518,
      "step": 319400
    },
    {
      "epoch": 2.9153587853127965,
      "grad_norm": 2.274047613143921,
      "learning_rate": 4.757053434557267e-05,
      "loss": 0.753,
      "step": 319500
    },
    {
      "epoch": 2.9162712606759618,
      "grad_norm": 4.065006256103516,
      "learning_rate": 4.75697739494367e-05,
      "loss": 0.7314,
      "step": 319600
    },
    {
      "epoch": 2.917183736039127,
      "grad_norm": 4.065006732940674,
      "learning_rate": 4.7569013553300726e-05,
      "loss": 0.7379,
      "step": 319700
    },
    {
      "epoch": 2.9180962114022924,
      "grad_norm": 3.732011079788208,
      "learning_rate": 4.756825315716476e-05,
      "loss": 0.7471,
      "step": 319800
    },
    {
      "epoch": 2.919008686765457,
      "grad_norm": 4.283858776092529,
      "learning_rate": 4.7567492761028786e-05,
      "loss": 0.717,
      "step": 319900
    },
    {
      "epoch": 2.9199211621286225,
      "grad_norm": 3.9304730892181396,
      "learning_rate": 4.7566732364892816e-05,
      "loss": 0.7283,
      "step": 320000
    },
    {
      "epoch": 2.920833637491788,
      "grad_norm": 4.579748153686523,
      "learning_rate": 4.7565971968756846e-05,
      "loss": 0.7716,
      "step": 320100
    },
    {
      "epoch": 2.9217461128549527,
      "grad_norm": 3.9924890995025635,
      "learning_rate": 4.7565211572620876e-05,
      "loss": 0.7454,
      "step": 320200
    },
    {
      "epoch": 2.922658588218118,
      "grad_norm": 4.309266090393066,
      "learning_rate": 4.7564451176484906e-05,
      "loss": 0.7311,
      "step": 320300
    },
    {
      "epoch": 2.9235710635812833,
      "grad_norm": 3.811267852783203,
      "learning_rate": 4.7563690780348936e-05,
      "loss": 0.7614,
      "step": 320400
    },
    {
      "epoch": 2.9244835389444486,
      "grad_norm": 4.486725807189941,
      "learning_rate": 4.756293038421296e-05,
      "loss": 0.7383,
      "step": 320500
    },
    {
      "epoch": 2.925396014307614,
      "grad_norm": 3.970198631286621,
      "learning_rate": 4.7562169988076996e-05,
      "loss": 0.738,
      "step": 320600
    },
    {
      "epoch": 2.9263084896707787,
      "grad_norm": 4.293976783752441,
      "learning_rate": 4.756140959194102e-05,
      "loss": 0.7554,
      "step": 320700
    },
    {
      "epoch": 2.927220965033944,
      "grad_norm": 4.325433731079102,
      "learning_rate": 4.756064919580505e-05,
      "loss": 0.7313,
      "step": 320800
    },
    {
      "epoch": 2.9281334403971093,
      "grad_norm": 4.169088840484619,
      "learning_rate": 4.755988879966908e-05,
      "loss": 0.7297,
      "step": 320900
    },
    {
      "epoch": 2.9290459157602746,
      "grad_norm": 4.421458721160889,
      "learning_rate": 4.75591284035331e-05,
      "loss": 0.7211,
      "step": 321000
    },
    {
      "epoch": 2.9299583911234395,
      "grad_norm": 4.049879550933838,
      "learning_rate": 4.755836800739713e-05,
      "loss": 0.7025,
      "step": 321100
    },
    {
      "epoch": 2.9308708664866048,
      "grad_norm": 3.5857057571411133,
      "learning_rate": 4.755760761126116e-05,
      "loss": 0.7312,
      "step": 321200
    },
    {
      "epoch": 2.93178334184977,
      "grad_norm": 3.7289280891418457,
      "learning_rate": 4.755684721512519e-05,
      "loss": 0.7351,
      "step": 321300
    },
    {
      "epoch": 2.9326958172129354,
      "grad_norm": 3.7730650901794434,
      "learning_rate": 4.755608681898922e-05,
      "loss": 0.7539,
      "step": 321400
    },
    {
      "epoch": 2.9336082925761007,
      "grad_norm": 5.170186996459961,
      "learning_rate": 4.755532642285325e-05,
      "loss": 0.7703,
      "step": 321500
    },
    {
      "epoch": 2.9345207679392655,
      "grad_norm": 4.827108860015869,
      "learning_rate": 4.7554566026717277e-05,
      "loss": 0.7602,
      "step": 321600
    },
    {
      "epoch": 2.935433243302431,
      "grad_norm": 4.560858249664307,
      "learning_rate": 4.7553805630581313e-05,
      "loss": 0.7638,
      "step": 321700
    },
    {
      "epoch": 2.936345718665596,
      "grad_norm": 3.689417600631714,
      "learning_rate": 4.755304523444534e-05,
      "loss": 0.7483,
      "step": 321800
    },
    {
      "epoch": 2.937258194028761,
      "grad_norm": 3.8252336978912354,
      "learning_rate": 4.755228483830937e-05,
      "loss": 0.735,
      "step": 321900
    },
    {
      "epoch": 2.9381706693919263,
      "grad_norm": 4.172705173492432,
      "learning_rate": 4.75515244421734e-05,
      "loss": 0.7803,
      "step": 322000
    },
    {
      "epoch": 2.9390831447550916,
      "grad_norm": 4.17579984664917,
      "learning_rate": 4.755076404603743e-05,
      "loss": 0.7259,
      "step": 322100
    },
    {
      "epoch": 2.939995620118257,
      "grad_norm": 4.055509090423584,
      "learning_rate": 4.755000364990145e-05,
      "loss": 0.769,
      "step": 322200
    },
    {
      "epoch": 2.940908095481422,
      "grad_norm": 3.700662136077881,
      "learning_rate": 4.754924325376549e-05,
      "loss": 0.7573,
      "step": 322300
    },
    {
      "epoch": 2.941820570844587,
      "grad_norm": 3.9135334491729736,
      "learning_rate": 4.754848285762951e-05,
      "loss": 0.7671,
      "step": 322400
    },
    {
      "epoch": 2.9427330462077523,
      "grad_norm": 3.13132381439209,
      "learning_rate": 4.754772246149354e-05,
      "loss": 0.77,
      "step": 322500
    },
    {
      "epoch": 2.9436455215709176,
      "grad_norm": 3.6303625106811523,
      "learning_rate": 4.754696206535757e-05,
      "loss": 0.7121,
      "step": 322600
    },
    {
      "epoch": 2.944557996934083,
      "grad_norm": 4.013175010681152,
      "learning_rate": 4.75462016692216e-05,
      "loss": 0.7222,
      "step": 322700
    },
    {
      "epoch": 2.945470472297248,
      "grad_norm": 6.329004287719727,
      "learning_rate": 4.754544127308563e-05,
      "loss": 0.746,
      "step": 322800
    },
    {
      "epoch": 2.946382947660413,
      "grad_norm": 3.6365435123443604,
      "learning_rate": 4.754468087694966e-05,
      "loss": 0.7322,
      "step": 322900
    },
    {
      "epoch": 2.9472954230235784,
      "grad_norm": 4.995657444000244,
      "learning_rate": 4.7543920480813684e-05,
      "loss": 0.7314,
      "step": 323000
    },
    {
      "epoch": 2.9482078983867437,
      "grad_norm": 3.9776761531829834,
      "learning_rate": 4.754316008467772e-05,
      "loss": 0.734,
      "step": 323100
    },
    {
      "epoch": 2.949120373749909,
      "grad_norm": 4.392313480377197,
      "learning_rate": 4.7542399688541744e-05,
      "loss": 0.7292,
      "step": 323200
    },
    {
      "epoch": 2.950032849113074,
      "grad_norm": 4.115069389343262,
      "learning_rate": 4.7541639292405774e-05,
      "loss": 0.7346,
      "step": 323300
    },
    {
      "epoch": 2.950945324476239,
      "grad_norm": 3.9378020763397217,
      "learning_rate": 4.7540878896269804e-05,
      "loss": 0.7489,
      "step": 323400
    },
    {
      "epoch": 2.9518577998394044,
      "grad_norm": 3.926286458969116,
      "learning_rate": 4.7540118500133834e-05,
      "loss": 0.7576,
      "step": 323500
    },
    {
      "epoch": 2.9527702752025693,
      "grad_norm": 3.9952924251556396,
      "learning_rate": 4.753935810399786e-05,
      "loss": 0.7375,
      "step": 323600
    },
    {
      "epoch": 2.9536827505657346,
      "grad_norm": 3.950150489807129,
      "learning_rate": 4.7538597707861894e-05,
      "loss": 0.774,
      "step": 323700
    },
    {
      "epoch": 2.9545952259289,
      "grad_norm": 3.531747817993164,
      "learning_rate": 4.753783731172592e-05,
      "loss": 0.7387,
      "step": 323800
    },
    {
      "epoch": 2.955507701292065,
      "grad_norm": 4.2813873291015625,
      "learning_rate": 4.753707691558995e-05,
      "loss": 0.7625,
      "step": 323900
    },
    {
      "epoch": 2.9564201766552305,
      "grad_norm": 4.145953178405762,
      "learning_rate": 4.753631651945398e-05,
      "loss": 0.7451,
      "step": 324000
    },
    {
      "epoch": 2.9573326520183953,
      "grad_norm": 4.506068706512451,
      "learning_rate": 4.7535556123318e-05,
      "loss": 0.7548,
      "step": 324100
    },
    {
      "epoch": 2.9582451273815606,
      "grad_norm": 3.2896203994750977,
      "learning_rate": 4.753479572718204e-05,
      "loss": 0.7074,
      "step": 324200
    },
    {
      "epoch": 2.959157602744726,
      "grad_norm": 3.9543614387512207,
      "learning_rate": 4.753403533104606e-05,
      "loss": 0.744,
      "step": 324300
    },
    {
      "epoch": 2.9600700781078912,
      "grad_norm": 4.337449073791504,
      "learning_rate": 4.753327493491009e-05,
      "loss": 0.7626,
      "step": 324400
    },
    {
      "epoch": 2.960982553471056,
      "grad_norm": 4.977494716644287,
      "learning_rate": 4.753251453877412e-05,
      "loss": 0.7676,
      "step": 324500
    },
    {
      "epoch": 2.9618950288342214,
      "grad_norm": 4.678647041320801,
      "learning_rate": 4.753175414263815e-05,
      "loss": 0.7661,
      "step": 324600
    },
    {
      "epoch": 2.9628075041973867,
      "grad_norm": 4.463167190551758,
      "learning_rate": 4.753099374650218e-05,
      "loss": 0.7683,
      "step": 324700
    },
    {
      "epoch": 2.963719979560552,
      "grad_norm": 3.62129282951355,
      "learning_rate": 4.753023335036621e-05,
      "loss": 0.774,
      "step": 324800
    },
    {
      "epoch": 2.9646324549237173,
      "grad_norm": 3.793511390686035,
      "learning_rate": 4.7529472954230235e-05,
      "loss": 0.7528,
      "step": 324900
    },
    {
      "epoch": 2.965544930286882,
      "grad_norm": 3.5900979042053223,
      "learning_rate": 4.7528712558094265e-05,
      "loss": 0.7741,
      "step": 325000
    },
    {
      "epoch": 2.9664574056500475,
      "grad_norm": 4.3617634773254395,
      "learning_rate": 4.7527952161958295e-05,
      "loss": 0.7194,
      "step": 325100
    },
    {
      "epoch": 2.9673698810132128,
      "grad_norm": 4.09129524230957,
      "learning_rate": 4.7527191765822325e-05,
      "loss": 0.7499,
      "step": 325200
    },
    {
      "epoch": 2.9682823563763776,
      "grad_norm": 4.355296611785889,
      "learning_rate": 4.7526431369686355e-05,
      "loss": 0.7658,
      "step": 325300
    },
    {
      "epoch": 2.969194831739543,
      "grad_norm": 3.789764165878296,
      "learning_rate": 4.7525670973550385e-05,
      "loss": 0.7498,
      "step": 325400
    },
    {
      "epoch": 2.970107307102708,
      "grad_norm": 4.496196269989014,
      "learning_rate": 4.752491057741441e-05,
      "loss": 0.7163,
      "step": 325500
    },
    {
      "epoch": 2.9710197824658735,
      "grad_norm": 4.194231986999512,
      "learning_rate": 4.7524150181278445e-05,
      "loss": 0.7395,
      "step": 325600
    },
    {
      "epoch": 2.971932257829039,
      "grad_norm": 4.580487251281738,
      "learning_rate": 4.752338978514247e-05,
      "loss": 0.7141,
      "step": 325700
    },
    {
      "epoch": 2.9728447331922037,
      "grad_norm": 3.857272148132324,
      "learning_rate": 4.75226293890065e-05,
      "loss": 0.7426,
      "step": 325800
    },
    {
      "epoch": 2.973757208555369,
      "grad_norm": 4.749979019165039,
      "learning_rate": 4.752186899287053e-05,
      "loss": 0.7587,
      "step": 325900
    },
    {
      "epoch": 2.9746696839185343,
      "grad_norm": 3.9459214210510254,
      "learning_rate": 4.752110859673456e-05,
      "loss": 0.7291,
      "step": 326000
    },
    {
      "epoch": 2.9755821592816996,
      "grad_norm": 4.276742935180664,
      "learning_rate": 4.752034820059859e-05,
      "loss": 0.7717,
      "step": 326100
    },
    {
      "epoch": 2.9764946346448644,
      "grad_norm": 3.839669942855835,
      "learning_rate": 4.751958780446262e-05,
      "loss": 0.7573,
      "step": 326200
    },
    {
      "epoch": 2.9774071100080297,
      "grad_norm": 4.272278785705566,
      "learning_rate": 4.751882740832664e-05,
      "loss": 0.7536,
      "step": 326300
    },
    {
      "epoch": 2.978319585371195,
      "grad_norm": 4.196207523345947,
      "learning_rate": 4.751806701219067e-05,
      "loss": 0.7458,
      "step": 326400
    },
    {
      "epoch": 2.9792320607343603,
      "grad_norm": 3.8383145332336426,
      "learning_rate": 4.75173066160547e-05,
      "loss": 0.7706,
      "step": 326500
    },
    {
      "epoch": 2.9801445360975256,
      "grad_norm": 3.7852187156677246,
      "learning_rate": 4.751654621991873e-05,
      "loss": 0.7425,
      "step": 326600
    },
    {
      "epoch": 2.9810570114606905,
      "grad_norm": 4.456854820251465,
      "learning_rate": 4.751578582378276e-05,
      "loss": 0.8007,
      "step": 326700
    },
    {
      "epoch": 2.9819694868238558,
      "grad_norm": 3.371269941329956,
      "learning_rate": 4.7515025427646786e-05,
      "loss": 0.7385,
      "step": 326800
    },
    {
      "epoch": 2.982881962187021,
      "grad_norm": 3.9002525806427,
      "learning_rate": 4.7514265031510816e-05,
      "loss": 0.7101,
      "step": 326900
    },
    {
      "epoch": 2.983794437550186,
      "grad_norm": 4.739429950714111,
      "learning_rate": 4.7513504635374846e-05,
      "loss": 0.7401,
      "step": 327000
    },
    {
      "epoch": 2.984706912913351,
      "grad_norm": 4.2965898513793945,
      "learning_rate": 4.7512744239238876e-05,
      "loss": 0.7428,
      "step": 327100
    },
    {
      "epoch": 2.9856193882765165,
      "grad_norm": 3.856362819671631,
      "learning_rate": 4.7511983843102906e-05,
      "loss": 0.7722,
      "step": 327200
    },
    {
      "epoch": 2.986531863639682,
      "grad_norm": 4.158272743225098,
      "learning_rate": 4.7511223446966936e-05,
      "loss": 0.7255,
      "step": 327300
    },
    {
      "epoch": 2.987444339002847,
      "grad_norm": 2.7100298404693604,
      "learning_rate": 4.751046305083096e-05,
      "loss": 0.756,
      "step": 327400
    },
    {
      "epoch": 2.988356814366012,
      "grad_norm": 4.124751567840576,
      "learning_rate": 4.7509702654694996e-05,
      "loss": 0.7458,
      "step": 327500
    },
    {
      "epoch": 2.9892692897291773,
      "grad_norm": 4.900174140930176,
      "learning_rate": 4.750894225855902e-05,
      "loss": 0.743,
      "step": 327600
    },
    {
      "epoch": 2.9901817650923426,
      "grad_norm": 3.344113826751709,
      "learning_rate": 4.750818186242305e-05,
      "loss": 0.7908,
      "step": 327700
    },
    {
      "epoch": 2.9910942404555074,
      "grad_norm": 4.022205352783203,
      "learning_rate": 4.750742146628708e-05,
      "loss": 0.7338,
      "step": 327800
    },
    {
      "epoch": 2.9920067158186727,
      "grad_norm": 4.02410888671875,
      "learning_rate": 4.750666107015111e-05,
      "loss": 0.7191,
      "step": 327900
    },
    {
      "epoch": 2.992919191181838,
      "grad_norm": 2.881471633911133,
      "learning_rate": 4.750590067401513e-05,
      "loss": 0.7318,
      "step": 328000
    },
    {
      "epoch": 2.9938316665450033,
      "grad_norm": 3.9767708778381348,
      "learning_rate": 4.750514027787917e-05,
      "loss": 0.7349,
      "step": 328100
    },
    {
      "epoch": 2.9947441419081686,
      "grad_norm": 4.218809127807617,
      "learning_rate": 4.750437988174319e-05,
      "loss": 0.7577,
      "step": 328200
    },
    {
      "epoch": 2.995656617271334,
      "grad_norm": 4.393899917602539,
      "learning_rate": 4.750361948560722e-05,
      "loss": 0.7855,
      "step": 328300
    },
    {
      "epoch": 2.9965690926344988,
      "grad_norm": 4.693893909454346,
      "learning_rate": 4.750285908947125e-05,
      "loss": 0.6918,
      "step": 328400
    },
    {
      "epoch": 2.997481567997664,
      "grad_norm": 4.408359527587891,
      "learning_rate": 4.7502098693335283e-05,
      "loss": 0.7617,
      "step": 328500
    },
    {
      "epoch": 2.9983940433608294,
      "grad_norm": 2.8081071376800537,
      "learning_rate": 4.7501338297199313e-05,
      "loss": 0.7406,
      "step": 328600
    },
    {
      "epoch": 2.9993065187239942,
      "grad_norm": 4.7778706550598145,
      "learning_rate": 4.7500577901063344e-05,
      "loss": 0.79,
      "step": 328700
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.6007510423660278,
      "eval_runtime": 26.021,
      "eval_samples_per_second": 221.706,
      "eval_steps_per_second": 221.706,
      "step": 328776
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.5864646434783936,
      "eval_runtime": 496.7346,
      "eval_samples_per_second": 220.625,
      "eval_steps_per_second": 220.625,
      "step": 328776
    },
    {
      "epoch": 3.0002189940871595,
      "grad_norm": 4.829647064208984,
      "learning_rate": 4.749981750492737e-05,
      "loss": 0.736,
      "step": 328800
    },
    {
      "epoch": 3.001131469450325,
      "grad_norm": 3.4223029613494873,
      "learning_rate": 4.7499057108791404e-05,
      "loss": 0.7419,
      "step": 328900
    },
    {
      "epoch": 3.00204394481349,
      "grad_norm": 4.190162181854248,
      "learning_rate": 4.749829671265543e-05,
      "loss": 0.7353,
      "step": 329000
    },
    {
      "epoch": 3.0029564201766554,
      "grad_norm": 5.000449180603027,
      "learning_rate": 4.749753631651946e-05,
      "loss": 0.6873,
      "step": 329100
    },
    {
      "epoch": 3.0038688955398203,
      "grad_norm": 3.7541754245758057,
      "learning_rate": 4.749677592038349e-05,
      "loss": 0.7567,
      "step": 329200
    },
    {
      "epoch": 3.0047813709029856,
      "grad_norm": 4.628296852111816,
      "learning_rate": 4.749601552424752e-05,
      "loss": 0.7292,
      "step": 329300
    },
    {
      "epoch": 3.005693846266151,
      "grad_norm": 3.9057390689849854,
      "learning_rate": 4.749525512811154e-05,
      "loss": 0.7348,
      "step": 329400
    },
    {
      "epoch": 3.006606321629316,
      "grad_norm": 4.473076820373535,
      "learning_rate": 4.749449473197557e-05,
      "loss": 0.754,
      "step": 329500
    },
    {
      "epoch": 3.007518796992481,
      "grad_norm": 3.5486204624176025,
      "learning_rate": 4.74937343358396e-05,
      "loss": 0.7333,
      "step": 329600
    },
    {
      "epoch": 3.0084312723556463,
      "grad_norm": 4.089474201202393,
      "learning_rate": 4.749297393970363e-05,
      "loss": 0.7773,
      "step": 329700
    },
    {
      "epoch": 3.0093437477188116,
      "grad_norm": 3.8121771812438965,
      "learning_rate": 4.749221354356766e-05,
      "loss": 0.6949,
      "step": 329800
    },
    {
      "epoch": 3.010256223081977,
      "grad_norm": 3.615596294403076,
      "learning_rate": 4.7491453147431684e-05,
      "loss": 0.7125,
      "step": 329900
    },
    {
      "epoch": 3.011168698445142,
      "grad_norm": 3.781252861022949,
      "learning_rate": 4.749069275129572e-05,
      "loss": 0.7283,
      "step": 330000
    },
    {
      "epoch": 3.012081173808307,
      "grad_norm": 3.0981597900390625,
      "learning_rate": 4.7489932355159744e-05,
      "loss": 0.7353,
      "step": 330100
    },
    {
      "epoch": 3.0129936491714724,
      "grad_norm": 4.0826334953308105,
      "learning_rate": 4.7489171959023774e-05,
      "loss": 0.7165,
      "step": 330200
    },
    {
      "epoch": 3.0139061245346377,
      "grad_norm": 4.321416854858398,
      "learning_rate": 4.7488411562887804e-05,
      "loss": 0.7569,
      "step": 330300
    },
    {
      "epoch": 3.0148185998978025,
      "grad_norm": 3.682162046432495,
      "learning_rate": 4.7487651166751834e-05,
      "loss": 0.7428,
      "step": 330400
    },
    {
      "epoch": 3.015731075260968,
      "grad_norm": 4.926322937011719,
      "learning_rate": 4.748689077061586e-05,
      "loss": 0.7671,
      "step": 330500
    },
    {
      "epoch": 3.016643550624133,
      "grad_norm": 4.249837875366211,
      "learning_rate": 4.7486130374479895e-05,
      "loss": 0.7175,
      "step": 330600
    },
    {
      "epoch": 3.0175560259872984,
      "grad_norm": 3.8618640899658203,
      "learning_rate": 4.748536997834392e-05,
      "loss": 0.7046,
      "step": 330700
    },
    {
      "epoch": 3.0184685013504637,
      "grad_norm": 4.241770267486572,
      "learning_rate": 4.748460958220795e-05,
      "loss": 0.6791,
      "step": 330800
    },
    {
      "epoch": 3.0193809767136286,
      "grad_norm": 4.8196516036987305,
      "learning_rate": 4.748384918607198e-05,
      "loss": 0.7323,
      "step": 330900
    },
    {
      "epoch": 3.020293452076794,
      "grad_norm": 3.7052578926086426,
      "learning_rate": 4.748308878993601e-05,
      "loss": 0.7245,
      "step": 331000
    },
    {
      "epoch": 3.021205927439959,
      "grad_norm": 4.021703720092773,
      "learning_rate": 4.748232839380004e-05,
      "loss": 0.7451,
      "step": 331100
    },
    {
      "epoch": 3.0221184028031245,
      "grad_norm": 4.194336891174316,
      "learning_rate": 4.748156799766407e-05,
      "loss": 0.722,
      "step": 331200
    },
    {
      "epoch": 3.0230308781662893,
      "grad_norm": 4.145510673522949,
      "learning_rate": 4.748080760152809e-05,
      "loss": 0.7335,
      "step": 331300
    },
    {
      "epoch": 3.0239433535294546,
      "grad_norm": 3.699402093887329,
      "learning_rate": 4.748004720539213e-05,
      "loss": 0.7468,
      "step": 331400
    },
    {
      "epoch": 3.02485582889262,
      "grad_norm": 4.416788101196289,
      "learning_rate": 4.747928680925615e-05,
      "loss": 0.7722,
      "step": 331500
    },
    {
      "epoch": 3.0257683042557852,
      "grad_norm": 2.986567974090576,
      "learning_rate": 4.747852641312018e-05,
      "loss": 0.693,
      "step": 331600
    },
    {
      "epoch": 3.02668077961895,
      "grad_norm": 4.40974235534668,
      "learning_rate": 4.747776601698421e-05,
      "loss": 0.7363,
      "step": 331700
    },
    {
      "epoch": 3.0275932549821154,
      "grad_norm": 4.045352935791016,
      "learning_rate": 4.747700562084824e-05,
      "loss": 0.7445,
      "step": 331800
    },
    {
      "epoch": 3.0285057303452807,
      "grad_norm": 4.193861961364746,
      "learning_rate": 4.7476245224712265e-05,
      "loss": 0.7362,
      "step": 331900
    },
    {
      "epoch": 3.029418205708446,
      "grad_norm": 4.083004951477051,
      "learning_rate": 4.74754848285763e-05,
      "loss": 0.6976,
      "step": 332000
    },
    {
      "epoch": 3.030330681071611,
      "grad_norm": 4.71462869644165,
      "learning_rate": 4.7474724432440325e-05,
      "loss": 0.7231,
      "step": 332100
    },
    {
      "epoch": 3.031243156434776,
      "grad_norm": 3.844437837600708,
      "learning_rate": 4.7473964036304355e-05,
      "loss": 0.737,
      "step": 332200
    },
    {
      "epoch": 3.0321556317979415,
      "grad_norm": 4.668442726135254,
      "learning_rate": 4.7473203640168385e-05,
      "loss": 0.7404,
      "step": 332300
    },
    {
      "epoch": 3.0330681071611068,
      "grad_norm": 4.819190502166748,
      "learning_rate": 4.747244324403241e-05,
      "loss": 0.7616,
      "step": 332400
    },
    {
      "epoch": 3.033980582524272,
      "grad_norm": 4.233500003814697,
      "learning_rate": 4.7471682847896445e-05,
      "loss": 0.7648,
      "step": 332500
    },
    {
      "epoch": 3.034893057887437,
      "grad_norm": 4.1571736335754395,
      "learning_rate": 4.747092245176047e-05,
      "loss": 0.7139,
      "step": 332600
    },
    {
      "epoch": 3.035805533250602,
      "grad_norm": 3.7233235836029053,
      "learning_rate": 4.74701620556245e-05,
      "loss": 0.7103,
      "step": 332700
    },
    {
      "epoch": 3.0367180086137675,
      "grad_norm": 4.349015712738037,
      "learning_rate": 4.746940165948853e-05,
      "loss": 0.7324,
      "step": 332800
    },
    {
      "epoch": 3.037630483976933,
      "grad_norm": 3.9453182220458984,
      "learning_rate": 4.746864126335256e-05,
      "loss": 0.7359,
      "step": 332900
    },
    {
      "epoch": 3.0385429593400977,
      "grad_norm": 3.6627092361450195,
      "learning_rate": 4.746788086721658e-05,
      "loss": 0.7452,
      "step": 333000
    },
    {
      "epoch": 3.039455434703263,
      "grad_norm": 3.7091097831726074,
      "learning_rate": 4.746712047108062e-05,
      "loss": 0.7458,
      "step": 333100
    },
    {
      "epoch": 3.0403679100664283,
      "grad_norm": 4.060163497924805,
      "learning_rate": 4.746636007494464e-05,
      "loss": 0.7402,
      "step": 333200
    },
    {
      "epoch": 3.0412803854295936,
      "grad_norm": 3.6212270259857178,
      "learning_rate": 4.746559967880867e-05,
      "loss": 0.6792,
      "step": 333300
    },
    {
      "epoch": 3.0421928607927584,
      "grad_norm": 3.9453227519989014,
      "learning_rate": 4.74648392826727e-05,
      "loss": 0.7197,
      "step": 333400
    },
    {
      "epoch": 3.0431053361559237,
      "grad_norm": 4.999670505523682,
      "learning_rate": 4.746407888653673e-05,
      "loss": 0.7524,
      "step": 333500
    },
    {
      "epoch": 3.044017811519089,
      "grad_norm": 3.6593873500823975,
      "learning_rate": 4.746331849040076e-05,
      "loss": 0.7437,
      "step": 333600
    },
    {
      "epoch": 3.0449302868822543,
      "grad_norm": 3.828122615814209,
      "learning_rate": 4.746255809426479e-05,
      "loss": 0.7377,
      "step": 333700
    },
    {
      "epoch": 3.045842762245419,
      "grad_norm": 4.419531345367432,
      "learning_rate": 4.7461797698128816e-05,
      "loss": 0.7592,
      "step": 333800
    },
    {
      "epoch": 3.0467552376085845,
      "grad_norm": 4.916978359222412,
      "learning_rate": 4.746103730199285e-05,
      "loss": 0.7575,
      "step": 333900
    },
    {
      "epoch": 3.0476677129717498,
      "grad_norm": 3.6958541870117188,
      "learning_rate": 4.7460276905856876e-05,
      "loss": 0.799,
      "step": 334000
    },
    {
      "epoch": 3.048580188334915,
      "grad_norm": 4.2128682136535645,
      "learning_rate": 4.7459516509720906e-05,
      "loss": 0.7012,
      "step": 334100
    },
    {
      "epoch": 3.0494926636980804,
      "grad_norm": 24.711877822875977,
      "learning_rate": 4.7458756113584936e-05,
      "loss": 0.7576,
      "step": 334200
    },
    {
      "epoch": 3.050405139061245,
      "grad_norm": 3.5112504959106445,
      "learning_rate": 4.7457995717448966e-05,
      "loss": 0.7502,
      "step": 334300
    },
    {
      "epoch": 3.0513176144244105,
      "grad_norm": 3.850534677505493,
      "learning_rate": 4.745723532131299e-05,
      "loss": 0.716,
      "step": 334400
    },
    {
      "epoch": 3.052230089787576,
      "grad_norm": 4.40253210067749,
      "learning_rate": 4.7456474925177026e-05,
      "loss": 0.7153,
      "step": 334500
    },
    {
      "epoch": 3.053142565150741,
      "grad_norm": 3.724231004714966,
      "learning_rate": 4.745571452904105e-05,
      "loss": 0.788,
      "step": 334600
    },
    {
      "epoch": 3.054055040513906,
      "grad_norm": 4.9337639808654785,
      "learning_rate": 4.745495413290508e-05,
      "loss": 0.7428,
      "step": 334700
    },
    {
      "epoch": 3.0549675158770713,
      "grad_norm": 3.745197296142578,
      "learning_rate": 4.745419373676911e-05,
      "loss": 0.7246,
      "step": 334800
    },
    {
      "epoch": 3.0558799912402366,
      "grad_norm": 3.786308526992798,
      "learning_rate": 4.745343334063314e-05,
      "loss": 0.736,
      "step": 334900
    },
    {
      "epoch": 3.056792466603402,
      "grad_norm": 4.350253105163574,
      "learning_rate": 4.745267294449717e-05,
      "loss": 0.7376,
      "step": 335000
    },
    {
      "epoch": 3.0577049419665667,
      "grad_norm": 4.308244228363037,
      "learning_rate": 4.74519125483612e-05,
      "loss": 0.7678,
      "step": 335100
    },
    {
      "epoch": 3.058617417329732,
      "grad_norm": 3.887155055999756,
      "learning_rate": 4.7451152152225223e-05,
      "loss": 0.7229,
      "step": 335200
    },
    {
      "epoch": 3.0595298926928973,
      "grad_norm": 3.29548716545105,
      "learning_rate": 4.7450391756089253e-05,
      "loss": 0.7095,
      "step": 335300
    },
    {
      "epoch": 3.0604423680560626,
      "grad_norm": 3.6912026405334473,
      "learning_rate": 4.7449631359953284e-05,
      "loss": 0.7199,
      "step": 335400
    },
    {
      "epoch": 3.0613548434192275,
      "grad_norm": 4.6172051429748535,
      "learning_rate": 4.744887096381731e-05,
      "loss": 0.7641,
      "step": 335500
    },
    {
      "epoch": 3.0622673187823928,
      "grad_norm": 4.047191143035889,
      "learning_rate": 4.7448110567681344e-05,
      "loss": 0.6996,
      "step": 335600
    },
    {
      "epoch": 3.063179794145558,
      "grad_norm": 4.789487838745117,
      "learning_rate": 4.744735017154537e-05,
      "loss": 0.7426,
      "step": 335700
    },
    {
      "epoch": 3.0640922695087234,
      "grad_norm": 4.374695777893066,
      "learning_rate": 4.74465897754094e-05,
      "loss": 0.7472,
      "step": 335800
    },
    {
      "epoch": 3.0650047448718887,
      "grad_norm": 4.820422649383545,
      "learning_rate": 4.744582937927343e-05,
      "loss": 0.7421,
      "step": 335900
    },
    {
      "epoch": 3.0659172202350535,
      "grad_norm": 4.058324813842773,
      "learning_rate": 4.744506898313746e-05,
      "loss": 0.7009,
      "step": 336000
    },
    {
      "epoch": 3.066829695598219,
      "grad_norm": 4.2351508140563965,
      "learning_rate": 4.744430858700149e-05,
      "loss": 0.7614,
      "step": 336100
    },
    {
      "epoch": 3.067742170961384,
      "grad_norm": 4.616759777069092,
      "learning_rate": 4.744354819086552e-05,
      "loss": 0.7194,
      "step": 336200
    },
    {
      "epoch": 3.0686546463245494,
      "grad_norm": 4.439854621887207,
      "learning_rate": 4.744278779472954e-05,
      "loss": 0.715,
      "step": 336300
    },
    {
      "epoch": 3.0695671216877143,
      "grad_norm": 3.7628955841064453,
      "learning_rate": 4.744202739859358e-05,
      "loss": 0.7278,
      "step": 336400
    },
    {
      "epoch": 3.0704795970508796,
      "grad_norm": 4.741792678833008,
      "learning_rate": 4.74412670024576e-05,
      "loss": 0.7077,
      "step": 336500
    },
    {
      "epoch": 3.071392072414045,
      "grad_norm": 3.590656042098999,
      "learning_rate": 4.744050660632163e-05,
      "loss": 0.7283,
      "step": 336600
    },
    {
      "epoch": 3.07230454777721,
      "grad_norm": 2.7924790382385254,
      "learning_rate": 4.743974621018566e-05,
      "loss": 0.6855,
      "step": 336700
    },
    {
      "epoch": 3.073217023140375,
      "grad_norm": 3.5856993198394775,
      "learning_rate": 4.743898581404969e-05,
      "loss": 0.6796,
      "step": 336800
    },
    {
      "epoch": 3.0741294985035403,
      "grad_norm": 3.951078414916992,
      "learning_rate": 4.7438225417913714e-05,
      "loss": 0.7326,
      "step": 336900
    },
    {
      "epoch": 3.0750419738667056,
      "grad_norm": 5.001919269561768,
      "learning_rate": 4.743746502177775e-05,
      "loss": 0.7488,
      "step": 337000
    },
    {
      "epoch": 3.075954449229871,
      "grad_norm": 4.407036781311035,
      "learning_rate": 4.7436704625641774e-05,
      "loss": 0.6925,
      "step": 337100
    },
    {
      "epoch": 3.076866924593036,
      "grad_norm": 4.416334629058838,
      "learning_rate": 4.7435944229505804e-05,
      "loss": 0.7124,
      "step": 337200
    },
    {
      "epoch": 3.077779399956201,
      "grad_norm": 3.0188796520233154,
      "learning_rate": 4.7435183833369834e-05,
      "loss": 0.7418,
      "step": 337300
    },
    {
      "epoch": 3.0786918753193664,
      "grad_norm": 4.5791730880737305,
      "learning_rate": 4.7434423437233865e-05,
      "loss": 0.7549,
      "step": 337400
    },
    {
      "epoch": 3.0796043506825317,
      "grad_norm": 3.6830577850341797,
      "learning_rate": 4.7433663041097895e-05,
      "loss": 0.6951,
      "step": 337500
    },
    {
      "epoch": 3.080516826045697,
      "grad_norm": 3.161344528198242,
      "learning_rate": 4.7432902644961925e-05,
      "loss": 0.7345,
      "step": 337600
    },
    {
      "epoch": 3.081429301408862,
      "grad_norm": 4.425090312957764,
      "learning_rate": 4.743214224882595e-05,
      "loss": 0.7381,
      "step": 337700
    },
    {
      "epoch": 3.082341776772027,
      "grad_norm": 4.372733116149902,
      "learning_rate": 4.7431381852689985e-05,
      "loss": 0.7226,
      "step": 337800
    },
    {
      "epoch": 3.0832542521351924,
      "grad_norm": 4.412691116333008,
      "learning_rate": 4.743062145655401e-05,
      "loss": 0.7142,
      "step": 337900
    },
    {
      "epoch": 3.0841667274983577,
      "grad_norm": 3.974215030670166,
      "learning_rate": 4.742986106041804e-05,
      "loss": 0.7702,
      "step": 338000
    },
    {
      "epoch": 3.0850792028615226,
      "grad_norm": 3.8057479858398438,
      "learning_rate": 4.742910066428207e-05,
      "loss": 0.7468,
      "step": 338100
    },
    {
      "epoch": 3.085991678224688,
      "grad_norm": 4.030529022216797,
      "learning_rate": 4.742834026814609e-05,
      "loss": 0.7676,
      "step": 338200
    },
    {
      "epoch": 3.086904153587853,
      "grad_norm": 4.389336109161377,
      "learning_rate": 4.742757987201013e-05,
      "loss": 0.7055,
      "step": 338300
    },
    {
      "epoch": 3.0878166289510185,
      "grad_norm": 5.56527042388916,
      "learning_rate": 4.742681947587415e-05,
      "loss": 0.7137,
      "step": 338400
    },
    {
      "epoch": 3.0887291043141833,
      "grad_norm": 3.6772172451019287,
      "learning_rate": 4.742605907973818e-05,
      "loss": 0.7421,
      "step": 338500
    },
    {
      "epoch": 3.0896415796773486,
      "grad_norm": 2.4828596115112305,
      "learning_rate": 4.742529868360221e-05,
      "loss": 0.7688,
      "step": 338600
    },
    {
      "epoch": 3.090554055040514,
      "grad_norm": 4.723855495452881,
      "learning_rate": 4.742453828746624e-05,
      "loss": 0.7595,
      "step": 338700
    },
    {
      "epoch": 3.0914665304036792,
      "grad_norm": 4.088468074798584,
      "learning_rate": 4.7423777891330265e-05,
      "loss": 0.7464,
      "step": 338800
    },
    {
      "epoch": 3.092379005766844,
      "grad_norm": 3.782299518585205,
      "learning_rate": 4.74230174951943e-05,
      "loss": 0.7221,
      "step": 338900
    },
    {
      "epoch": 3.0932914811300094,
      "grad_norm": 4.39029598236084,
      "learning_rate": 4.7422257099058325e-05,
      "loss": 0.7705,
      "step": 339000
    },
    {
      "epoch": 3.0942039564931747,
      "grad_norm": 3.5769588947296143,
      "learning_rate": 4.7421496702922355e-05,
      "loss": 0.7198,
      "step": 339100
    },
    {
      "epoch": 3.09511643185634,
      "grad_norm": 4.2687578201293945,
      "learning_rate": 4.7420736306786385e-05,
      "loss": 0.7217,
      "step": 339200
    },
    {
      "epoch": 3.0960289072195053,
      "grad_norm": 3.7150838375091553,
      "learning_rate": 4.7419975910650415e-05,
      "loss": 0.7607,
      "step": 339300
    },
    {
      "epoch": 3.09694138258267,
      "grad_norm": 4.1488213539123535,
      "learning_rate": 4.7419215514514446e-05,
      "loss": 0.7329,
      "step": 339400
    },
    {
      "epoch": 3.0978538579458355,
      "grad_norm": 3.0618960857391357,
      "learning_rate": 4.7418455118378476e-05,
      "loss": 0.7434,
      "step": 339500
    },
    {
      "epoch": 3.0987663333090008,
      "grad_norm": 3.3663277626037598,
      "learning_rate": 4.74176947222425e-05,
      "loss": 0.7453,
      "step": 339600
    },
    {
      "epoch": 3.099678808672166,
      "grad_norm": 4.163030624389648,
      "learning_rate": 4.7416934326106536e-05,
      "loss": 0.7489,
      "step": 339700
    },
    {
      "epoch": 3.100591284035331,
      "grad_norm": 5.860739707946777,
      "learning_rate": 4.741617392997056e-05,
      "loss": 0.7375,
      "step": 339800
    },
    {
      "epoch": 3.101503759398496,
      "grad_norm": 3.082676649093628,
      "learning_rate": 4.741541353383459e-05,
      "loss": 0.7004,
      "step": 339900
    },
    {
      "epoch": 3.1024162347616615,
      "grad_norm": 3.8241052627563477,
      "learning_rate": 4.741465313769862e-05,
      "loss": 0.7734,
      "step": 340000
    },
    {
      "epoch": 3.103328710124827,
      "grad_norm": 4.3483076095581055,
      "learning_rate": 4.741389274156265e-05,
      "loss": 0.7373,
      "step": 340100
    },
    {
      "epoch": 3.1042411854879917,
      "grad_norm": 2.8973822593688965,
      "learning_rate": 4.741313234542667e-05,
      "loss": 0.7365,
      "step": 340200
    },
    {
      "epoch": 3.105153660851157,
      "grad_norm": 3.806164503097534,
      "learning_rate": 4.741237194929071e-05,
      "loss": 0.7145,
      "step": 340300
    },
    {
      "epoch": 3.1060661362143223,
      "grad_norm": 4.0867719650268555,
      "learning_rate": 4.741161155315473e-05,
      "loss": 0.7375,
      "step": 340400
    },
    {
      "epoch": 3.1069786115774876,
      "grad_norm": 3.951815605163574,
      "learning_rate": 4.741085115701876e-05,
      "loss": 0.7242,
      "step": 340500
    },
    {
      "epoch": 3.1078910869406524,
      "grad_norm": 6.274041175842285,
      "learning_rate": 4.741009076088279e-05,
      "loss": 0.7224,
      "step": 340600
    },
    {
      "epoch": 3.1088035623038177,
      "grad_norm": 3.727128505706787,
      "learning_rate": 4.740933036474682e-05,
      "loss": 0.7292,
      "step": 340700
    },
    {
      "epoch": 3.109716037666983,
      "grad_norm": 4.463387489318848,
      "learning_rate": 4.740856996861085e-05,
      "loss": 0.7399,
      "step": 340800
    },
    {
      "epoch": 3.1106285130301483,
      "grad_norm": 4.86787223815918,
      "learning_rate": 4.7407809572474876e-05,
      "loss": 0.6935,
      "step": 340900
    },
    {
      "epoch": 3.111540988393313,
      "grad_norm": 3.5722405910491943,
      "learning_rate": 4.7407049176338906e-05,
      "loss": 0.7166,
      "step": 341000
    },
    {
      "epoch": 3.1124534637564785,
      "grad_norm": 4.588637351989746,
      "learning_rate": 4.7406288780202936e-05,
      "loss": 0.7556,
      "step": 341100
    },
    {
      "epoch": 3.1133659391196438,
      "grad_norm": 3.9148781299591064,
      "learning_rate": 4.7405528384066966e-05,
      "loss": 0.7655,
      "step": 341200
    },
    {
      "epoch": 3.114278414482809,
      "grad_norm": 3.8435707092285156,
      "learning_rate": 4.740476798793099e-05,
      "loss": 0.7037,
      "step": 341300
    },
    {
      "epoch": 3.1151908898459744,
      "grad_norm": 3.2705514430999756,
      "learning_rate": 4.7404007591795027e-05,
      "loss": 0.7267,
      "step": 341400
    },
    {
      "epoch": 3.116103365209139,
      "grad_norm": 4.183825969696045,
      "learning_rate": 4.740324719565905e-05,
      "loss": 0.6679,
      "step": 341500
    },
    {
      "epoch": 3.1170158405723045,
      "grad_norm": 4.703831672668457,
      "learning_rate": 4.740248679952308e-05,
      "loss": 0.7476,
      "step": 341600
    },
    {
      "epoch": 3.11792831593547,
      "grad_norm": 2.1743991374969482,
      "learning_rate": 4.740172640338711e-05,
      "loss": 0.6969,
      "step": 341700
    },
    {
      "epoch": 3.118840791298635,
      "grad_norm": 4.085544109344482,
      "learning_rate": 4.740096600725114e-05,
      "loss": 0.7088,
      "step": 341800
    },
    {
      "epoch": 3.1197532666618,
      "grad_norm": 4.687809944152832,
      "learning_rate": 4.740020561111517e-05,
      "loss": 0.795,
      "step": 341900
    },
    {
      "epoch": 3.1206657420249653,
      "grad_norm": 4.099392414093018,
      "learning_rate": 4.73994452149792e-05,
      "loss": 0.7738,
      "step": 342000
    },
    {
      "epoch": 3.1215782173881306,
      "grad_norm": 3.5084011554718018,
      "learning_rate": 4.7398684818843223e-05,
      "loss": 0.7281,
      "step": 342100
    },
    {
      "epoch": 3.122490692751296,
      "grad_norm": 3.9835972785949707,
      "learning_rate": 4.739792442270726e-05,
      "loss": 0.7071,
      "step": 342200
    },
    {
      "epoch": 3.1234031681144607,
      "grad_norm": 4.760087013244629,
      "learning_rate": 4.7397164026571284e-05,
      "loss": 0.7626,
      "step": 342300
    },
    {
      "epoch": 3.124315643477626,
      "grad_norm": 4.911052703857422,
      "learning_rate": 4.7396403630435314e-05,
      "loss": 0.7215,
      "step": 342400
    },
    {
      "epoch": 3.1252281188407913,
      "grad_norm": 4.902522563934326,
      "learning_rate": 4.7395643234299344e-05,
      "loss": 0.7348,
      "step": 342500
    },
    {
      "epoch": 3.1261405942039566,
      "grad_norm": 5.536969184875488,
      "learning_rate": 4.7394882838163374e-05,
      "loss": 0.7335,
      "step": 342600
    },
    {
      "epoch": 3.127053069567122,
      "grad_norm": 4.089111804962158,
      "learning_rate": 4.73941224420274e-05,
      "loss": 0.748,
      "step": 342700
    },
    {
      "epoch": 3.127965544930287,
      "grad_norm": 2.623507022857666,
      "learning_rate": 4.7393362045891434e-05,
      "loss": 0.7277,
      "step": 342800
    },
    {
      "epoch": 3.128878020293452,
      "grad_norm": 4.564382553100586,
      "learning_rate": 4.739260164975546e-05,
      "loss": 0.7215,
      "step": 342900
    },
    {
      "epoch": 3.1297904956566174,
      "grad_norm": 3.3666491508483887,
      "learning_rate": 4.739184125361949e-05,
      "loss": 0.7309,
      "step": 343000
    },
    {
      "epoch": 3.1307029710197822,
      "grad_norm": 4.963802337646484,
      "learning_rate": 4.739108085748352e-05,
      "loss": 0.714,
      "step": 343100
    },
    {
      "epoch": 3.1316154463829475,
      "grad_norm": 4.63443660736084,
      "learning_rate": 4.739032046134755e-05,
      "loss": 0.6999,
      "step": 343200
    },
    {
      "epoch": 3.132527921746113,
      "grad_norm": 3.9302971363067627,
      "learning_rate": 4.738956006521158e-05,
      "loss": 0.7552,
      "step": 343300
    },
    {
      "epoch": 3.133440397109278,
      "grad_norm": 4.296469688415527,
      "learning_rate": 4.738879966907561e-05,
      "loss": 0.7335,
      "step": 343400
    },
    {
      "epoch": 3.1343528724724434,
      "grad_norm": 4.106438636779785,
      "learning_rate": 4.738803927293963e-05,
      "loss": 0.7432,
      "step": 343500
    },
    {
      "epoch": 3.1352653478356083,
      "grad_norm": 3.8689424991607666,
      "learning_rate": 4.738727887680367e-05,
      "loss": 0.7337,
      "step": 343600
    },
    {
      "epoch": 3.1361778231987736,
      "grad_norm": 4.367987632751465,
      "learning_rate": 4.738651848066769e-05,
      "loss": 0.7233,
      "step": 343700
    },
    {
      "epoch": 3.137090298561939,
      "grad_norm": 4.847496032714844,
      "learning_rate": 4.7385758084531714e-05,
      "loss": 0.7544,
      "step": 343800
    },
    {
      "epoch": 3.138002773925104,
      "grad_norm": 3.715731620788574,
      "learning_rate": 4.738499768839575e-05,
      "loss": 0.7604,
      "step": 343900
    },
    {
      "epoch": 3.138915249288269,
      "grad_norm": 4.625882625579834,
      "learning_rate": 4.7384237292259774e-05,
      "loss": 0.7391,
      "step": 344000
    },
    {
      "epoch": 3.1398277246514343,
      "grad_norm": 4.376931667327881,
      "learning_rate": 4.7383476896123805e-05,
      "loss": 0.7585,
      "step": 344100
    },
    {
      "epoch": 3.1407402000145996,
      "grad_norm": 4.050844192504883,
      "learning_rate": 4.7382716499987835e-05,
      "loss": 0.741,
      "step": 344200
    },
    {
      "epoch": 3.141652675377765,
      "grad_norm": 4.693755149841309,
      "learning_rate": 4.7381956103851865e-05,
      "loss": 0.7322,
      "step": 344300
    },
    {
      "epoch": 3.14256515074093,
      "grad_norm": 3.7209432125091553,
      "learning_rate": 4.7381195707715895e-05,
      "loss": 0.7328,
      "step": 344400
    },
    {
      "epoch": 3.143477626104095,
      "grad_norm": 4.33388614654541,
      "learning_rate": 4.7380435311579925e-05,
      "loss": 0.6941,
      "step": 344500
    },
    {
      "epoch": 3.1443901014672604,
      "grad_norm": 3.425295829772949,
      "learning_rate": 4.737967491544395e-05,
      "loss": 0.6965,
      "step": 344600
    },
    {
      "epoch": 3.1453025768304257,
      "grad_norm": 4.341428279876709,
      "learning_rate": 4.7378914519307985e-05,
      "loss": 0.7417,
      "step": 344700
    },
    {
      "epoch": 3.1462150521935905,
      "grad_norm": 3.7263238430023193,
      "learning_rate": 4.737815412317201e-05,
      "loss": 0.7653,
      "step": 344800
    },
    {
      "epoch": 3.147127527556756,
      "grad_norm": 4.154028415679932,
      "learning_rate": 4.737739372703604e-05,
      "loss": 0.7114,
      "step": 344900
    },
    {
      "epoch": 3.148040002919921,
      "grad_norm": 4.406336307525635,
      "learning_rate": 4.737663333090007e-05,
      "loss": 0.7301,
      "step": 345000
    },
    {
      "epoch": 3.1489524782830864,
      "grad_norm": 4.402169704437256,
      "learning_rate": 4.73758729347641e-05,
      "loss": 0.7753,
      "step": 345100
    },
    {
      "epoch": 3.1498649536462517,
      "grad_norm": 3.6829123497009277,
      "learning_rate": 4.737511253862812e-05,
      "loss": 0.729,
      "step": 345200
    },
    {
      "epoch": 3.1507774290094166,
      "grad_norm": 3.4851746559143066,
      "learning_rate": 4.737435214249216e-05,
      "loss": 0.7429,
      "step": 345300
    },
    {
      "epoch": 3.151689904372582,
      "grad_norm": 3.940279722213745,
      "learning_rate": 4.737359174635618e-05,
      "loss": 0.7584,
      "step": 345400
    },
    {
      "epoch": 3.152602379735747,
      "grad_norm": 3.9786806106567383,
      "learning_rate": 4.737283135022021e-05,
      "loss": 0.7503,
      "step": 345500
    },
    {
      "epoch": 3.1535148550989125,
      "grad_norm": 3.271920919418335,
      "learning_rate": 4.737207095408424e-05,
      "loss": 0.741,
      "step": 345600
    },
    {
      "epoch": 3.1544273304620774,
      "grad_norm": 3.7680857181549072,
      "learning_rate": 4.737131055794827e-05,
      "loss": 0.7722,
      "step": 345700
    },
    {
      "epoch": 3.1553398058252426,
      "grad_norm": 3.7396438121795654,
      "learning_rate": 4.73705501618123e-05,
      "loss": 0.7172,
      "step": 345800
    },
    {
      "epoch": 3.156252281188408,
      "grad_norm": 4.086681365966797,
      "learning_rate": 4.736978976567633e-05,
      "loss": 0.7206,
      "step": 345900
    },
    {
      "epoch": 3.1571647565515732,
      "grad_norm": 3.8726003170013428,
      "learning_rate": 4.7369029369540355e-05,
      "loss": 0.7604,
      "step": 346000
    },
    {
      "epoch": 3.158077231914738,
      "grad_norm": 3.666447639465332,
      "learning_rate": 4.736826897340439e-05,
      "loss": 0.727,
      "step": 346100
    },
    {
      "epoch": 3.1589897072779034,
      "grad_norm": 4.260412216186523,
      "learning_rate": 4.7367508577268416e-05,
      "loss": 0.7474,
      "step": 346200
    },
    {
      "epoch": 3.1599021826410687,
      "grad_norm": 3.5994794368743896,
      "learning_rate": 4.7366748181132446e-05,
      "loss": 0.7413,
      "step": 346300
    },
    {
      "epoch": 3.160814658004234,
      "grad_norm": 3.7134933471679688,
      "learning_rate": 4.7365987784996476e-05,
      "loss": 0.7757,
      "step": 346400
    },
    {
      "epoch": 3.161727133367399,
      "grad_norm": 3.9775190353393555,
      "learning_rate": 4.73652273888605e-05,
      "loss": 0.7229,
      "step": 346500
    },
    {
      "epoch": 3.162639608730564,
      "grad_norm": 3.922656774520874,
      "learning_rate": 4.736446699272453e-05,
      "loss": 0.7621,
      "step": 346600
    },
    {
      "epoch": 3.1635520840937295,
      "grad_norm": 4.751176357269287,
      "learning_rate": 4.736370659658856e-05,
      "loss": 0.7289,
      "step": 346700
    },
    {
      "epoch": 3.1644645594568948,
      "grad_norm": 3.973710536956787,
      "learning_rate": 4.736294620045259e-05,
      "loss": 0.7221,
      "step": 346800
    },
    {
      "epoch": 3.16537703482006,
      "grad_norm": 4.632970809936523,
      "learning_rate": 4.736218580431662e-05,
      "loss": 0.7062,
      "step": 346900
    },
    {
      "epoch": 3.166289510183225,
      "grad_norm": 5.796992301940918,
      "learning_rate": 4.736142540818065e-05,
      "loss": 0.783,
      "step": 347000
    },
    {
      "epoch": 3.16720198554639,
      "grad_norm": 4.104768753051758,
      "learning_rate": 4.736066501204467e-05,
      "loss": 0.7324,
      "step": 347100
    },
    {
      "epoch": 3.1681144609095555,
      "grad_norm": 3.5024592876434326,
      "learning_rate": 4.735990461590871e-05,
      "loss": 0.722,
      "step": 347200
    },
    {
      "epoch": 3.169026936272721,
      "grad_norm": 4.26024866104126,
      "learning_rate": 4.735914421977273e-05,
      "loss": 0.7439,
      "step": 347300
    },
    {
      "epoch": 3.1699394116358857,
      "grad_norm": 3.7759640216827393,
      "learning_rate": 4.735838382363676e-05,
      "loss": 0.74,
      "step": 347400
    },
    {
      "epoch": 3.170851886999051,
      "grad_norm": 4.434126377105713,
      "learning_rate": 4.735762342750079e-05,
      "loss": 0.7533,
      "step": 347500
    },
    {
      "epoch": 3.1717643623622163,
      "grad_norm": 4.470386981964111,
      "learning_rate": 4.735686303136482e-05,
      "loss": 0.7469,
      "step": 347600
    },
    {
      "epoch": 3.1726768377253816,
      "grad_norm": 4.848766803741455,
      "learning_rate": 4.7356102635228846e-05,
      "loss": 0.7029,
      "step": 347700
    },
    {
      "epoch": 3.1735893130885464,
      "grad_norm": 4.208463191986084,
      "learning_rate": 4.735534223909288e-05,
      "loss": 0.733,
      "step": 347800
    },
    {
      "epoch": 3.1745017884517117,
      "grad_norm": 3.9383838176727295,
      "learning_rate": 4.7354581842956906e-05,
      "loss": 0.7437,
      "step": 347900
    },
    {
      "epoch": 3.175414263814877,
      "grad_norm": 4.438199043273926,
      "learning_rate": 4.7353821446820936e-05,
      "loss": 0.7243,
      "step": 348000
    },
    {
      "epoch": 3.1763267391780423,
      "grad_norm": 5.116163730621338,
      "learning_rate": 4.7353061050684967e-05,
      "loss": 0.7039,
      "step": 348100
    },
    {
      "epoch": 3.177239214541207,
      "grad_norm": 4.927858829498291,
      "learning_rate": 4.7352300654548997e-05,
      "loss": 0.698,
      "step": 348200
    },
    {
      "epoch": 3.1781516899043725,
      "grad_norm": 4.294830322265625,
      "learning_rate": 4.735154025841303e-05,
      "loss": 0.7851,
      "step": 348300
    },
    {
      "epoch": 3.1790641652675378,
      "grad_norm": 4.1561503410339355,
      "learning_rate": 4.735077986227706e-05,
      "loss": 0.764,
      "step": 348400
    },
    {
      "epoch": 3.179976640630703,
      "grad_norm": 5.016814231872559,
      "learning_rate": 4.735001946614108e-05,
      "loss": 0.7267,
      "step": 348500
    },
    {
      "epoch": 3.1808891159938684,
      "grad_norm": 4.254723072052002,
      "learning_rate": 4.734925907000512e-05,
      "loss": 0.7571,
      "step": 348600
    },
    {
      "epoch": 3.181801591357033,
      "grad_norm": 4.8029704093933105,
      "learning_rate": 4.734849867386914e-05,
      "loss": 0.6738,
      "step": 348700
    },
    {
      "epoch": 3.1827140667201985,
      "grad_norm": 3.70515775680542,
      "learning_rate": 4.734773827773317e-05,
      "loss": 0.775,
      "step": 348800
    },
    {
      "epoch": 3.183626542083364,
      "grad_norm": 4.045658588409424,
      "learning_rate": 4.73469778815972e-05,
      "loss": 0.6944,
      "step": 348900
    },
    {
      "epoch": 3.184539017446529,
      "grad_norm": 5.186099052429199,
      "learning_rate": 4.734621748546123e-05,
      "loss": 0.7413,
      "step": 349000
    },
    {
      "epoch": 3.185451492809694,
      "grad_norm": 3.837202548980713,
      "learning_rate": 4.7345457089325254e-05,
      "loss": 0.7312,
      "step": 349100
    },
    {
      "epoch": 3.1863639681728593,
      "grad_norm": 3.5097739696502686,
      "learning_rate": 4.734469669318929e-05,
      "loss": 0.7804,
      "step": 349200
    },
    {
      "epoch": 3.1872764435360246,
      "grad_norm": 3.7082037925720215,
      "learning_rate": 4.7343936297053314e-05,
      "loss": 0.7488,
      "step": 349300
    },
    {
      "epoch": 3.18818891889919,
      "grad_norm": 3.4510765075683594,
      "learning_rate": 4.7343175900917344e-05,
      "loss": 0.726,
      "step": 349400
    },
    {
      "epoch": 3.1891013942623547,
      "grad_norm": 3.768580913543701,
      "learning_rate": 4.7342415504781374e-05,
      "loss": 0.7541,
      "step": 349500
    },
    {
      "epoch": 3.19001386962552,
      "grad_norm": 3.8975155353546143,
      "learning_rate": 4.73416551086454e-05,
      "loss": 0.7207,
      "step": 349600
    },
    {
      "epoch": 3.1909263449886853,
      "grad_norm": 5.578362464904785,
      "learning_rate": 4.7340894712509434e-05,
      "loss": 0.7062,
      "step": 349700
    },
    {
      "epoch": 3.1918388203518506,
      "grad_norm": 4.617367267608643,
      "learning_rate": 4.734013431637346e-05,
      "loss": 0.7876,
      "step": 349800
    },
    {
      "epoch": 3.1927512957150155,
      "grad_norm": 4.385369300842285,
      "learning_rate": 4.733937392023749e-05,
      "loss": 0.7328,
      "step": 349900
    },
    {
      "epoch": 3.193663771078181,
      "grad_norm": 3.560621500015259,
      "learning_rate": 4.733861352410152e-05,
      "loss": 0.7527,
      "step": 350000
    },
    {
      "epoch": 3.194576246441346,
      "grad_norm": 4.343069076538086,
      "learning_rate": 4.733785312796555e-05,
      "loss": 0.7295,
      "step": 350100
    },
    {
      "epoch": 3.1954887218045114,
      "grad_norm": 4.0662336349487305,
      "learning_rate": 4.733709273182958e-05,
      "loss": 0.73,
      "step": 350200
    },
    {
      "epoch": 3.1964011971676767,
      "grad_norm": 3.9237353801727295,
      "learning_rate": 4.733633233569361e-05,
      "loss": 0.7575,
      "step": 350300
    },
    {
      "epoch": 3.1973136725308415,
      "grad_norm": 3.956166982650757,
      "learning_rate": 4.733557193955763e-05,
      "loss": 0.7425,
      "step": 350400
    },
    {
      "epoch": 3.198226147894007,
      "grad_norm": 3.800799608230591,
      "learning_rate": 4.733481154342166e-05,
      "loss": 0.7488,
      "step": 350500
    },
    {
      "epoch": 3.199138623257172,
      "grad_norm": 4.624366283416748,
      "learning_rate": 4.733405114728569e-05,
      "loss": 0.7109,
      "step": 350600
    },
    {
      "epoch": 3.2000510986203374,
      "grad_norm": 4.464006423950195,
      "learning_rate": 4.733329075114972e-05,
      "loss": 0.7524,
      "step": 350700
    },
    {
      "epoch": 3.2009635739835023,
      "grad_norm": 4.830007076263428,
      "learning_rate": 4.733253035501375e-05,
      "loss": 0.7546,
      "step": 350800
    },
    {
      "epoch": 3.2018760493466676,
      "grad_norm": 4.548877716064453,
      "learning_rate": 4.733176995887778e-05,
      "loss": 0.7404,
      "step": 350900
    },
    {
      "epoch": 3.202788524709833,
      "grad_norm": 3.6263866424560547,
      "learning_rate": 4.7331009562741805e-05,
      "loss": 0.7327,
      "step": 351000
    },
    {
      "epoch": 3.203701000072998,
      "grad_norm": 4.819343566894531,
      "learning_rate": 4.733024916660584e-05,
      "loss": 0.7346,
      "step": 351100
    },
    {
      "epoch": 3.204613475436163,
      "grad_norm": 4.820161819458008,
      "learning_rate": 4.7329488770469865e-05,
      "loss": 0.7589,
      "step": 351200
    },
    {
      "epoch": 3.2055259507993283,
      "grad_norm": 4.03933572769165,
      "learning_rate": 4.7328728374333895e-05,
      "loss": 0.7826,
      "step": 351300
    },
    {
      "epoch": 3.2064384261624936,
      "grad_norm": 4.104106903076172,
      "learning_rate": 4.7327967978197925e-05,
      "loss": 0.7277,
      "step": 351400
    },
    {
      "epoch": 3.207350901525659,
      "grad_norm": 4.2062296867370605,
      "learning_rate": 4.7327207582061955e-05,
      "loss": 0.7352,
      "step": 351500
    },
    {
      "epoch": 3.208263376888824,
      "grad_norm": 3.700218439102173,
      "learning_rate": 4.7326447185925985e-05,
      "loss": 0.7249,
      "step": 351600
    },
    {
      "epoch": 3.209175852251989,
      "grad_norm": 4.222341537475586,
      "learning_rate": 4.7325686789790015e-05,
      "loss": 0.7026,
      "step": 351700
    },
    {
      "epoch": 3.2100883276151544,
      "grad_norm": 3.104199171066284,
      "learning_rate": 4.732492639365404e-05,
      "loss": 0.7616,
      "step": 351800
    },
    {
      "epoch": 3.2110008029783197,
      "grad_norm": 4.949307918548584,
      "learning_rate": 4.732416599751807e-05,
      "loss": 0.7527,
      "step": 351900
    },
    {
      "epoch": 3.211913278341485,
      "grad_norm": 3.703176259994507,
      "learning_rate": 4.73234056013821e-05,
      "loss": 0.6739,
      "step": 352000
    },
    {
      "epoch": 3.21282575370465,
      "grad_norm": 3.853297710418701,
      "learning_rate": 4.732264520524613e-05,
      "loss": 0.7446,
      "step": 352100
    },
    {
      "epoch": 3.213738229067815,
      "grad_norm": 4.73592472076416,
      "learning_rate": 4.732188480911016e-05,
      "loss": 0.7382,
      "step": 352200
    },
    {
      "epoch": 3.2146507044309804,
      "grad_norm": 4.543875217437744,
      "learning_rate": 4.732112441297418e-05,
      "loss": 0.7255,
      "step": 352300
    },
    {
      "epoch": 3.2155631797941457,
      "grad_norm": 3.862865924835205,
      "learning_rate": 4.732036401683821e-05,
      "loss": 0.8014,
      "step": 352400
    },
    {
      "epoch": 3.2164756551573106,
      "grad_norm": 3.703909397125244,
      "learning_rate": 4.731960362070224e-05,
      "loss": 0.7297,
      "step": 352500
    },
    {
      "epoch": 3.217388130520476,
      "grad_norm": 4.4885663986206055,
      "learning_rate": 4.731884322456627e-05,
      "loss": 0.7326,
      "step": 352600
    },
    {
      "epoch": 3.218300605883641,
      "grad_norm": 3.9366137981414795,
      "learning_rate": 4.73180828284303e-05,
      "loss": 0.785,
      "step": 352700
    },
    {
      "epoch": 3.2192130812468065,
      "grad_norm": 4.312831401824951,
      "learning_rate": 4.731732243229433e-05,
      "loss": 0.7478,
      "step": 352800
    },
    {
      "epoch": 3.2201255566099714,
      "grad_norm": 4.002717971801758,
      "learning_rate": 4.7316562036158356e-05,
      "loss": 0.7677,
      "step": 352900
    },
    {
      "epoch": 3.2210380319731367,
      "grad_norm": 4.182300090789795,
      "learning_rate": 4.731580164002239e-05,
      "loss": 0.7658,
      "step": 353000
    },
    {
      "epoch": 3.221950507336302,
      "grad_norm": 4.64077091217041,
      "learning_rate": 4.7315041243886416e-05,
      "loss": 0.7435,
      "step": 353100
    },
    {
      "epoch": 3.2228629826994672,
      "grad_norm": 4.422469139099121,
      "learning_rate": 4.7314280847750446e-05,
      "loss": 0.6918,
      "step": 353200
    },
    {
      "epoch": 3.223775458062632,
      "grad_norm": 4.440311908721924,
      "learning_rate": 4.7313520451614476e-05,
      "loss": 0.7184,
      "step": 353300
    },
    {
      "epoch": 3.2246879334257974,
      "grad_norm": 4.580527305603027,
      "learning_rate": 4.7312760055478506e-05,
      "loss": 0.7559,
      "step": 353400
    },
    {
      "epoch": 3.2256004087889627,
      "grad_norm": 3.5936777591705322,
      "learning_rate": 4.731199965934253e-05,
      "loss": 0.7528,
      "step": 353500
    },
    {
      "epoch": 3.226512884152128,
      "grad_norm": 4.4605793952941895,
      "learning_rate": 4.7311239263206566e-05,
      "loss": 0.7387,
      "step": 353600
    },
    {
      "epoch": 3.2274253595152933,
      "grad_norm": 4.648581504821777,
      "learning_rate": 4.731047886707059e-05,
      "loss": 0.7195,
      "step": 353700
    },
    {
      "epoch": 3.228337834878458,
      "grad_norm": 4.20722770690918,
      "learning_rate": 4.730971847093462e-05,
      "loss": 0.7421,
      "step": 353800
    },
    {
      "epoch": 3.2292503102416235,
      "grad_norm": 3.784273147583008,
      "learning_rate": 4.730895807479865e-05,
      "loss": 0.7535,
      "step": 353900
    },
    {
      "epoch": 3.2301627856047888,
      "grad_norm": 4.477565288543701,
      "learning_rate": 4.730819767866268e-05,
      "loss": 0.7604,
      "step": 354000
    },
    {
      "epoch": 3.231075260967954,
      "grad_norm": 3.8735499382019043,
      "learning_rate": 4.730743728252671e-05,
      "loss": 0.7342,
      "step": 354100
    },
    {
      "epoch": 3.231987736331119,
      "grad_norm": 2.651068925857544,
      "learning_rate": 4.730667688639074e-05,
      "loss": 0.7081,
      "step": 354200
    },
    {
      "epoch": 3.232900211694284,
      "grad_norm": 3.773919105529785,
      "learning_rate": 4.730591649025476e-05,
      "loss": 0.7709,
      "step": 354300
    },
    {
      "epoch": 3.2338126870574495,
      "grad_norm": 3.9416627883911133,
      "learning_rate": 4.73051560941188e-05,
      "loss": 0.704,
      "step": 354400
    },
    {
      "epoch": 3.234725162420615,
      "grad_norm": 3.623955488204956,
      "learning_rate": 4.730439569798282e-05,
      "loss": 0.7288,
      "step": 354500
    },
    {
      "epoch": 3.2356376377837797,
      "grad_norm": 3.5862834453582764,
      "learning_rate": 4.730363530184685e-05,
      "loss": 0.7658,
      "step": 354600
    },
    {
      "epoch": 3.236550113146945,
      "grad_norm": 3.909818410873413,
      "learning_rate": 4.730287490571088e-05,
      "loss": 0.7781,
      "step": 354700
    },
    {
      "epoch": 3.2374625885101103,
      "grad_norm": 4.259603977203369,
      "learning_rate": 4.730211450957491e-05,
      "loss": 0.7394,
      "step": 354800
    },
    {
      "epoch": 3.2383750638732756,
      "grad_norm": 3.852971076965332,
      "learning_rate": 4.7301354113438937e-05,
      "loss": 0.7417,
      "step": 354900
    },
    {
      "epoch": 3.2392875392364404,
      "grad_norm": 3.7706990242004395,
      "learning_rate": 4.7300593717302973e-05,
      "loss": 0.7161,
      "step": 355000
    },
    {
      "epoch": 3.2402000145996057,
      "grad_norm": 2.9450387954711914,
      "learning_rate": 4.7299833321167e-05,
      "loss": 0.7327,
      "step": 355100
    },
    {
      "epoch": 3.241112489962771,
      "grad_norm": 5.373282432556152,
      "learning_rate": 4.729907292503103e-05,
      "loss": 0.8023,
      "step": 355200
    },
    {
      "epoch": 3.2420249653259363,
      "grad_norm": 3.7775378227233887,
      "learning_rate": 4.729831252889506e-05,
      "loss": 0.721,
      "step": 355300
    },
    {
      "epoch": 3.2429374406891016,
      "grad_norm": 4.076517581939697,
      "learning_rate": 4.729755213275908e-05,
      "loss": 0.7363,
      "step": 355400
    },
    {
      "epoch": 3.2438499160522665,
      "grad_norm": 4.0241570472717285,
      "learning_rate": 4.729679173662312e-05,
      "loss": 0.7325,
      "step": 355500
    },
    {
      "epoch": 3.2447623914154318,
      "grad_norm": 2.665332794189453,
      "learning_rate": 4.729603134048714e-05,
      "loss": 0.73,
      "step": 355600
    },
    {
      "epoch": 3.245674866778597,
      "grad_norm": 4.38173770904541,
      "learning_rate": 4.729527094435117e-05,
      "loss": 0.7344,
      "step": 355700
    },
    {
      "epoch": 3.2465873421417624,
      "grad_norm": 4.825966835021973,
      "learning_rate": 4.72945105482152e-05,
      "loss": 0.7188,
      "step": 355800
    },
    {
      "epoch": 3.247499817504927,
      "grad_norm": 4.2694315910339355,
      "learning_rate": 4.729375015207923e-05,
      "loss": 0.7058,
      "step": 355900
    },
    {
      "epoch": 3.2484122928680925,
      "grad_norm": 3.3707547187805176,
      "learning_rate": 4.7292989755943254e-05,
      "loss": 0.7433,
      "step": 356000
    },
    {
      "epoch": 3.249324768231258,
      "grad_norm": 4.078118801116943,
      "learning_rate": 4.729222935980729e-05,
      "loss": 0.7644,
      "step": 356100
    },
    {
      "epoch": 3.250237243594423,
      "grad_norm": 4.490724086761475,
      "learning_rate": 4.7291468963671314e-05,
      "loss": 0.7141,
      "step": 356200
    },
    {
      "epoch": 3.251149718957588,
      "grad_norm": 4.46337366104126,
      "learning_rate": 4.7290708567535344e-05,
      "loss": 0.7327,
      "step": 356300
    },
    {
      "epoch": 3.2520621943207533,
      "grad_norm": 4.3504791259765625,
      "learning_rate": 4.7289948171399374e-05,
      "loss": 0.7598,
      "step": 356400
    },
    {
      "epoch": 3.2529746696839186,
      "grad_norm": 4.286052227020264,
      "learning_rate": 4.7289187775263404e-05,
      "loss": 0.7529,
      "step": 356500
    },
    {
      "epoch": 3.253887145047084,
      "grad_norm": 3.7047247886657715,
      "learning_rate": 4.7288427379127434e-05,
      "loss": 0.7275,
      "step": 356600
    },
    {
      "epoch": 3.2547996204102487,
      "grad_norm": 4.659329414367676,
      "learning_rate": 4.7287666982991464e-05,
      "loss": 0.7545,
      "step": 356700
    },
    {
      "epoch": 3.255712095773414,
      "grad_norm": 4.449203968048096,
      "learning_rate": 4.728690658685549e-05,
      "loss": 0.767,
      "step": 356800
    },
    {
      "epoch": 3.2566245711365793,
      "grad_norm": 4.476222515106201,
      "learning_rate": 4.7286146190719524e-05,
      "loss": 0.7213,
      "step": 356900
    },
    {
      "epoch": 3.2575370464997446,
      "grad_norm": 4.917485237121582,
      "learning_rate": 4.728538579458355e-05,
      "loss": 0.7037,
      "step": 357000
    },
    {
      "epoch": 3.25844952186291,
      "grad_norm": 4.474613666534424,
      "learning_rate": 4.728462539844758e-05,
      "loss": 0.7512,
      "step": 357100
    },
    {
      "epoch": 3.259361997226075,
      "grad_norm": 3.65354061126709,
      "learning_rate": 4.728386500231161e-05,
      "loss": 0.7465,
      "step": 357200
    },
    {
      "epoch": 3.26027447258924,
      "grad_norm": 3.5816283226013184,
      "learning_rate": 4.728310460617564e-05,
      "loss": 0.7739,
      "step": 357300
    },
    {
      "epoch": 3.2611869479524054,
      "grad_norm": 4.228429794311523,
      "learning_rate": 4.728234421003966e-05,
      "loss": 0.733,
      "step": 357400
    },
    {
      "epoch": 3.2620994233155702,
      "grad_norm": 4.323573589324951,
      "learning_rate": 4.72815838139037e-05,
      "loss": 0.7376,
      "step": 357500
    },
    {
      "epoch": 3.2630118986787355,
      "grad_norm": 4.333105564117432,
      "learning_rate": 4.728082341776772e-05,
      "loss": 0.7731,
      "step": 357600
    },
    {
      "epoch": 3.263924374041901,
      "grad_norm": 4.3306121826171875,
      "learning_rate": 4.728006302163175e-05,
      "loss": 0.6986,
      "step": 357700
    },
    {
      "epoch": 3.264836849405066,
      "grad_norm": 2.815873146057129,
      "learning_rate": 4.727930262549578e-05,
      "loss": 0.7721,
      "step": 357800
    },
    {
      "epoch": 3.2657493247682314,
      "grad_norm": 4.835683822631836,
      "learning_rate": 4.7278542229359805e-05,
      "loss": 0.769,
      "step": 357900
    },
    {
      "epoch": 3.2666618001313963,
      "grad_norm": 4.903336524963379,
      "learning_rate": 4.727778183322384e-05,
      "loss": 0.7143,
      "step": 358000
    },
    {
      "epoch": 3.2675742754945616,
      "grad_norm": 3.4079337120056152,
      "learning_rate": 4.7277021437087865e-05,
      "loss": 0.7418,
      "step": 358100
    },
    {
      "epoch": 3.268486750857727,
      "grad_norm": 4.912605285644531,
      "learning_rate": 4.7276261040951895e-05,
      "loss": 0.7168,
      "step": 358200
    },
    {
      "epoch": 3.269399226220892,
      "grad_norm": 3.9866607189178467,
      "learning_rate": 4.7275500644815925e-05,
      "loss": 0.7349,
      "step": 358300
    },
    {
      "epoch": 3.270311701584057,
      "grad_norm": 3.9674057960510254,
      "learning_rate": 4.7274740248679955e-05,
      "loss": 0.715,
      "step": 358400
    },
    {
      "epoch": 3.2712241769472223,
      "grad_norm": 3.9708375930786133,
      "learning_rate": 4.727397985254398e-05,
      "loss": 0.7394,
      "step": 358500
    },
    {
      "epoch": 3.2721366523103876,
      "grad_norm": 3.9409737586975098,
      "learning_rate": 4.7273219456408015e-05,
      "loss": 0.7149,
      "step": 358600
    },
    {
      "epoch": 3.273049127673553,
      "grad_norm": 4.093558311462402,
      "learning_rate": 4.727245906027204e-05,
      "loss": 0.7265,
      "step": 358700
    },
    {
      "epoch": 3.2739616030367182,
      "grad_norm": 3.3591580390930176,
      "learning_rate": 4.727169866413607e-05,
      "loss": 0.6975,
      "step": 358800
    },
    {
      "epoch": 3.274874078399883,
      "grad_norm": 4.772916316986084,
      "learning_rate": 4.72709382680001e-05,
      "loss": 0.7205,
      "step": 358900
    },
    {
      "epoch": 3.2757865537630484,
      "grad_norm": 3.713590145111084,
      "learning_rate": 4.727017787186413e-05,
      "loss": 0.7464,
      "step": 359000
    },
    {
      "epoch": 3.2766990291262137,
      "grad_norm": 4.896561145782471,
      "learning_rate": 4.726941747572816e-05,
      "loss": 0.7405,
      "step": 359100
    },
    {
      "epoch": 3.2776115044893785,
      "grad_norm": 3.169861078262329,
      "learning_rate": 4.726865707959219e-05,
      "loss": 0.7374,
      "step": 359200
    },
    {
      "epoch": 3.278523979852544,
      "grad_norm": 4.759272575378418,
      "learning_rate": 4.726789668345621e-05,
      "loss": 0.7343,
      "step": 359300
    },
    {
      "epoch": 3.279436455215709,
      "grad_norm": 4.4713358879089355,
      "learning_rate": 4.726713628732025e-05,
      "loss": 0.7392,
      "step": 359400
    },
    {
      "epoch": 3.2803489305788744,
      "grad_norm": 5.080846786499023,
      "learning_rate": 4.726637589118427e-05,
      "loss": 0.7023,
      "step": 359500
    },
    {
      "epoch": 3.2812614059420397,
      "grad_norm": 3.4413297176361084,
      "learning_rate": 4.72656154950483e-05,
      "loss": 0.6845,
      "step": 359600
    },
    {
      "epoch": 3.2821738813052046,
      "grad_norm": 4.2492289543151855,
      "learning_rate": 4.726485509891233e-05,
      "loss": 0.7633,
      "step": 359700
    },
    {
      "epoch": 3.28308635666837,
      "grad_norm": 4.017184734344482,
      "learning_rate": 4.726409470277636e-05,
      "loss": 0.7176,
      "step": 359800
    },
    {
      "epoch": 3.283998832031535,
      "grad_norm": 4.449446201324463,
      "learning_rate": 4.7263334306640386e-05,
      "loss": 0.7101,
      "step": 359900
    },
    {
      "epoch": 3.2849113073947005,
      "grad_norm": 4.145537853240967,
      "learning_rate": 4.726257391050442e-05,
      "loss": 0.7121,
      "step": 360000
    },
    {
      "epoch": 3.2858237827578654,
      "grad_norm": 4.296199798583984,
      "learning_rate": 4.7261813514368446e-05,
      "loss": 0.7388,
      "step": 360100
    },
    {
      "epoch": 3.2867362581210307,
      "grad_norm": 4.026522159576416,
      "learning_rate": 4.7261053118232476e-05,
      "loss": 0.7322,
      "step": 360200
    },
    {
      "epoch": 3.287648733484196,
      "grad_norm": 4.837553977966309,
      "learning_rate": 4.7260292722096506e-05,
      "loss": 0.7439,
      "step": 360300
    },
    {
      "epoch": 3.2885612088473613,
      "grad_norm": 4.194300174713135,
      "learning_rate": 4.7259532325960536e-05,
      "loss": 0.7128,
      "step": 360400
    },
    {
      "epoch": 3.2894736842105265,
      "grad_norm": 4.4025678634643555,
      "learning_rate": 4.7258771929824566e-05,
      "loss": 0.7399,
      "step": 360500
    },
    {
      "epoch": 3.2903861595736914,
      "grad_norm": 3.9362597465515137,
      "learning_rate": 4.7258011533688596e-05,
      "loss": 0.7373,
      "step": 360600
    },
    {
      "epoch": 3.2912986349368567,
      "grad_norm": 3.9433135986328125,
      "learning_rate": 4.725725113755262e-05,
      "loss": 0.7522,
      "step": 360700
    },
    {
      "epoch": 3.292211110300022,
      "grad_norm": 5.2779998779296875,
      "learning_rate": 4.725649074141665e-05,
      "loss": 0.7185,
      "step": 360800
    },
    {
      "epoch": 3.293123585663187,
      "grad_norm": 3.31607985496521,
      "learning_rate": 4.725573034528068e-05,
      "loss": 0.7595,
      "step": 360900
    },
    {
      "epoch": 3.294036061026352,
      "grad_norm": 3.575894355773926,
      "learning_rate": 4.72549699491447e-05,
      "loss": 0.7431,
      "step": 361000
    },
    {
      "epoch": 3.2949485363895175,
      "grad_norm": 4.807469844818115,
      "learning_rate": 4.725420955300874e-05,
      "loss": 0.7113,
      "step": 361100
    },
    {
      "epoch": 3.2958610117526828,
      "grad_norm": 5.153182029724121,
      "learning_rate": 4.725344915687276e-05,
      "loss": 0.805,
      "step": 361200
    },
    {
      "epoch": 3.296773487115848,
      "grad_norm": 3.866417646408081,
      "learning_rate": 4.725268876073679e-05,
      "loss": 0.7284,
      "step": 361300
    },
    {
      "epoch": 3.297685962479013,
      "grad_norm": 4.052592754364014,
      "learning_rate": 4.725192836460082e-05,
      "loss": 0.7516,
      "step": 361400
    },
    {
      "epoch": 3.298598437842178,
      "grad_norm": 4.506959438323975,
      "learning_rate": 4.725116796846485e-05,
      "loss": 0.746,
      "step": 361500
    },
    {
      "epoch": 3.2995109132053435,
      "grad_norm": 4.566439151763916,
      "learning_rate": 4.725040757232888e-05,
      "loss": 0.7415,
      "step": 361600
    },
    {
      "epoch": 3.300423388568509,
      "grad_norm": 3.9889464378356934,
      "learning_rate": 4.724964717619291e-05,
      "loss": 0.7475,
      "step": 361700
    },
    {
      "epoch": 3.3013358639316737,
      "grad_norm": 7.089646816253662,
      "learning_rate": 4.724888678005694e-05,
      "loss": 0.727,
      "step": 361800
    },
    {
      "epoch": 3.302248339294839,
      "grad_norm": 4.874438285827637,
      "learning_rate": 4.7248126383920974e-05,
      "loss": 0.7498,
      "step": 361900
    },
    {
      "epoch": 3.3031608146580043,
      "grad_norm": 3.352846622467041,
      "learning_rate": 4.7247365987785e-05,
      "loss": 0.7309,
      "step": 362000
    },
    {
      "epoch": 3.3040732900211696,
      "grad_norm": 3.468832492828369,
      "learning_rate": 4.724660559164903e-05,
      "loss": 0.7362,
      "step": 362100
    },
    {
      "epoch": 3.304985765384335,
      "grad_norm": 4.182566165924072,
      "learning_rate": 4.724584519551306e-05,
      "loss": 0.6814,
      "step": 362200
    },
    {
      "epoch": 3.3058982407474997,
      "grad_norm": 4.029293060302734,
      "learning_rate": 4.724508479937709e-05,
      "loss": 0.7176,
      "step": 362300
    },
    {
      "epoch": 3.306810716110665,
      "grad_norm": 3.752619743347168,
      "learning_rate": 4.724432440324111e-05,
      "loss": 0.7273,
      "step": 362400
    },
    {
      "epoch": 3.3077231914738303,
      "grad_norm": 3.4255526065826416,
      "learning_rate": 4.724356400710515e-05,
      "loss": 0.7152,
      "step": 362500
    },
    {
      "epoch": 3.308635666836995,
      "grad_norm": 4.318052768707275,
      "learning_rate": 4.724280361096917e-05,
      "loss": 0.7312,
      "step": 362600
    },
    {
      "epoch": 3.3095481422001605,
      "grad_norm": 4.146274089813232,
      "learning_rate": 4.72420432148332e-05,
      "loss": 0.7187,
      "step": 362700
    },
    {
      "epoch": 3.3104606175633258,
      "grad_norm": 5.154428005218506,
      "learning_rate": 4.724128281869723e-05,
      "loss": 0.7007,
      "step": 362800
    },
    {
      "epoch": 3.311373092926491,
      "grad_norm": 4.150223255157471,
      "learning_rate": 4.724052242256126e-05,
      "loss": 0.7472,
      "step": 362900
    },
    {
      "epoch": 3.3122855682896564,
      "grad_norm": 4.4604811668396,
      "learning_rate": 4.723976202642529e-05,
      "loss": 0.7365,
      "step": 363000
    },
    {
      "epoch": 3.3131980436528212,
      "grad_norm": 4.428847312927246,
      "learning_rate": 4.723900163028932e-05,
      "loss": 0.6977,
      "step": 363100
    },
    {
      "epoch": 3.3141105190159865,
      "grad_norm": 4.687192916870117,
      "learning_rate": 4.7238241234153344e-05,
      "loss": 0.7558,
      "step": 363200
    },
    {
      "epoch": 3.315022994379152,
      "grad_norm": 4.055146217346191,
      "learning_rate": 4.723748083801738e-05,
      "loss": 0.7275,
      "step": 363300
    },
    {
      "epoch": 3.315935469742317,
      "grad_norm": 4.0029754638671875,
      "learning_rate": 4.7236720441881404e-05,
      "loss": 0.7253,
      "step": 363400
    },
    {
      "epoch": 3.316847945105482,
      "grad_norm": 4.095256328582764,
      "learning_rate": 4.7235960045745434e-05,
      "loss": 0.7162,
      "step": 363500
    },
    {
      "epoch": 3.3177604204686473,
      "grad_norm": 4.553630828857422,
      "learning_rate": 4.7235199649609464e-05,
      "loss": 0.7771,
      "step": 363600
    },
    {
      "epoch": 3.3186728958318126,
      "grad_norm": 3.1385114192962646,
      "learning_rate": 4.723443925347349e-05,
      "loss": 0.7506,
      "step": 363700
    },
    {
      "epoch": 3.319585371194978,
      "grad_norm": 4.546901226043701,
      "learning_rate": 4.7233678857337524e-05,
      "loss": 0.7375,
      "step": 363800
    },
    {
      "epoch": 3.320497846558143,
      "grad_norm": 3.987337112426758,
      "learning_rate": 4.723291846120155e-05,
      "loss": 0.7491,
      "step": 363900
    },
    {
      "epoch": 3.321410321921308,
      "grad_norm": 3.660078763961792,
      "learning_rate": 4.723215806506558e-05,
      "loss": 0.7347,
      "step": 364000
    },
    {
      "epoch": 3.3223227972844733,
      "grad_norm": 3.9080727100372314,
      "learning_rate": 4.723139766892961e-05,
      "loss": 0.7698,
      "step": 364100
    },
    {
      "epoch": 3.3232352726476386,
      "grad_norm": 3.5919349193573,
      "learning_rate": 4.723063727279364e-05,
      "loss": 0.6777,
      "step": 364200
    },
    {
      "epoch": 3.3241477480108035,
      "grad_norm": 4.4137654304504395,
      "learning_rate": 4.722987687665766e-05,
      "loss": 0.7416,
      "step": 364300
    },
    {
      "epoch": 3.325060223373969,
      "grad_norm": 3.9359610080718994,
      "learning_rate": 4.72291164805217e-05,
      "loss": 0.7421,
      "step": 364400
    },
    {
      "epoch": 3.325972698737134,
      "grad_norm": 3.5746092796325684,
      "learning_rate": 4.722835608438572e-05,
      "loss": 0.7259,
      "step": 364500
    },
    {
      "epoch": 3.3268851741002994,
      "grad_norm": 4.664000511169434,
      "learning_rate": 4.722759568824975e-05,
      "loss": 0.73,
      "step": 364600
    },
    {
      "epoch": 3.3277976494634647,
      "grad_norm": 3.9655675888061523,
      "learning_rate": 4.722683529211378e-05,
      "loss": 0.7209,
      "step": 364700
    },
    {
      "epoch": 3.3287101248266295,
      "grad_norm": 3.9267139434814453,
      "learning_rate": 4.722607489597781e-05,
      "loss": 0.724,
      "step": 364800
    },
    {
      "epoch": 3.329622600189795,
      "grad_norm": 2.684523344039917,
      "learning_rate": 4.722531449984184e-05,
      "loss": 0.704,
      "step": 364900
    },
    {
      "epoch": 3.33053507555296,
      "grad_norm": 4.614980697631836,
      "learning_rate": 4.722455410370587e-05,
      "loss": 0.6943,
      "step": 365000
    },
    {
      "epoch": 3.3314475509161254,
      "grad_norm": 3.3952412605285645,
      "learning_rate": 4.7223793707569895e-05,
      "loss": 0.7021,
      "step": 365100
    },
    {
      "epoch": 3.3323600262792903,
      "grad_norm": 4.82344913482666,
      "learning_rate": 4.722303331143393e-05,
      "loss": 0.7786,
      "step": 365200
    },
    {
      "epoch": 3.3332725016424556,
      "grad_norm": 3.819789171218872,
      "learning_rate": 4.7222272915297955e-05,
      "loss": 0.726,
      "step": 365300
    },
    {
      "epoch": 3.334184977005621,
      "grad_norm": 4.057433605194092,
      "learning_rate": 4.7221512519161985e-05,
      "loss": 0.731,
      "step": 365400
    },
    {
      "epoch": 3.335097452368786,
      "grad_norm": 4.953972339630127,
      "learning_rate": 4.7220752123026015e-05,
      "loss": 0.722,
      "step": 365500
    },
    {
      "epoch": 3.3360099277319515,
      "grad_norm": 3.9433388710021973,
      "learning_rate": 4.7219991726890045e-05,
      "loss": 0.7664,
      "step": 365600
    },
    {
      "epoch": 3.3369224030951163,
      "grad_norm": 4.0477375984191895,
      "learning_rate": 4.721923133075407e-05,
      "loss": 0.7727,
      "step": 365700
    },
    {
      "epoch": 3.3378348784582816,
      "grad_norm": 5.134467124938965,
      "learning_rate": 4.7218470934618105e-05,
      "loss": 0.723,
      "step": 365800
    },
    {
      "epoch": 3.338747353821447,
      "grad_norm": 4.142622947692871,
      "learning_rate": 4.721771053848213e-05,
      "loss": 0.7607,
      "step": 365900
    },
    {
      "epoch": 3.339659829184612,
      "grad_norm": 3.6373538970947266,
      "learning_rate": 4.721695014234616e-05,
      "loss": 0.7326,
      "step": 366000
    },
    {
      "epoch": 3.340572304547777,
      "grad_norm": 3.798337936401367,
      "learning_rate": 4.721618974621019e-05,
      "loss": 0.7668,
      "step": 366100
    },
    {
      "epoch": 3.3414847799109424,
      "grad_norm": 3.9893009662628174,
      "learning_rate": 4.721542935007422e-05,
      "loss": 0.7326,
      "step": 366200
    },
    {
      "epoch": 3.3423972552741077,
      "grad_norm": 3.2313199043273926,
      "learning_rate": 4.721466895393825e-05,
      "loss": 0.7863,
      "step": 366300
    },
    {
      "epoch": 3.343309730637273,
      "grad_norm": 4.432122230529785,
      "learning_rate": 4.721390855780227e-05,
      "loss": 0.7597,
      "step": 366400
    },
    {
      "epoch": 3.344222206000438,
      "grad_norm": 4.4990949630737305,
      "learning_rate": 4.72131481616663e-05,
      "loss": 0.7132,
      "step": 366500
    },
    {
      "epoch": 3.345134681363603,
      "grad_norm": 2.989445924758911,
      "learning_rate": 4.721238776553033e-05,
      "loss": 0.7168,
      "step": 366600
    },
    {
      "epoch": 3.3460471567267684,
      "grad_norm": 5.594157695770264,
      "learning_rate": 4.721162736939436e-05,
      "loss": 0.7216,
      "step": 366700
    },
    {
      "epoch": 3.3469596320899337,
      "grad_norm": 3.611997127532959,
      "learning_rate": 4.7210866973258386e-05,
      "loss": 0.7313,
      "step": 366800
    },
    {
      "epoch": 3.3478721074530986,
      "grad_norm": 4.11321496963501,
      "learning_rate": 4.721010657712242e-05,
      "loss": 0.7519,
      "step": 366900
    },
    {
      "epoch": 3.348784582816264,
      "grad_norm": 4.028537273406982,
      "learning_rate": 4.7209346180986446e-05,
      "loss": 0.7569,
      "step": 367000
    },
    {
      "epoch": 3.349697058179429,
      "grad_norm": 4.059695243835449,
      "learning_rate": 4.7208585784850476e-05,
      "loss": 0.7188,
      "step": 367100
    },
    {
      "epoch": 3.3506095335425945,
      "grad_norm": 4.195001602172852,
      "learning_rate": 4.7207825388714506e-05,
      "loss": 0.7442,
      "step": 367200
    },
    {
      "epoch": 3.3515220089057594,
      "grad_norm": 4.015838146209717,
      "learning_rate": 4.7207064992578536e-05,
      "loss": 0.7149,
      "step": 367300
    },
    {
      "epoch": 3.3524344842689247,
      "grad_norm": 3.976783037185669,
      "learning_rate": 4.7206304596442566e-05,
      "loss": 0.7464,
      "step": 367400
    },
    {
      "epoch": 3.35334695963209,
      "grad_norm": 4.181077480316162,
      "learning_rate": 4.7205544200306596e-05,
      "loss": 0.7243,
      "step": 367500
    },
    {
      "epoch": 3.3542594349952553,
      "grad_norm": 3.6365857124328613,
      "learning_rate": 4.720478380417062e-05,
      "loss": 0.7755,
      "step": 367600
    },
    {
      "epoch": 3.35517191035842,
      "grad_norm": 3.258425235748291,
      "learning_rate": 4.7204023408034656e-05,
      "loss": 0.7244,
      "step": 367700
    },
    {
      "epoch": 3.3560843857215854,
      "grad_norm": 2.6900722980499268,
      "learning_rate": 4.720326301189868e-05,
      "loss": 0.7117,
      "step": 367800
    },
    {
      "epoch": 3.3569968610847507,
      "grad_norm": 3.943274736404419,
      "learning_rate": 4.720250261576271e-05,
      "loss": 0.7564,
      "step": 367900
    },
    {
      "epoch": 3.357909336447916,
      "grad_norm": 4.196712970733643,
      "learning_rate": 4.720174221962674e-05,
      "loss": 0.6598,
      "step": 368000
    },
    {
      "epoch": 3.3588218118110813,
      "grad_norm": 3.7970008850097656,
      "learning_rate": 4.720098182349077e-05,
      "loss": 0.7558,
      "step": 368100
    },
    {
      "epoch": 3.359734287174246,
      "grad_norm": 4.393167972564697,
      "learning_rate": 4.720022142735479e-05,
      "loss": 0.722,
      "step": 368200
    },
    {
      "epoch": 3.3606467625374115,
      "grad_norm": 3.9725515842437744,
      "learning_rate": 4.719946103121883e-05,
      "loss": 0.7176,
      "step": 368300
    },
    {
      "epoch": 3.3615592379005768,
      "grad_norm": 4.470964431762695,
      "learning_rate": 4.719870063508285e-05,
      "loss": 0.7736,
      "step": 368400
    },
    {
      "epoch": 3.362471713263742,
      "grad_norm": 4.26746940612793,
      "learning_rate": 4.7197940238946883e-05,
      "loss": 0.7072,
      "step": 368500
    },
    {
      "epoch": 3.363384188626907,
      "grad_norm": 4.154211521148682,
      "learning_rate": 4.7197179842810913e-05,
      "loss": 0.7622,
      "step": 368600
    },
    {
      "epoch": 3.364296663990072,
      "grad_norm": 3.808941602706909,
      "learning_rate": 4.7196419446674944e-05,
      "loss": 0.7329,
      "step": 368700
    },
    {
      "epoch": 3.3652091393532375,
      "grad_norm": 4.723533630371094,
      "learning_rate": 4.7195659050538974e-05,
      "loss": 0.7181,
      "step": 368800
    },
    {
      "epoch": 3.366121614716403,
      "grad_norm": 4.330209732055664,
      "learning_rate": 4.7194898654403004e-05,
      "loss": 0.7146,
      "step": 368900
    },
    {
      "epoch": 3.3670340900795677,
      "grad_norm": 5.269254207611084,
      "learning_rate": 4.719413825826703e-05,
      "loss": 0.7119,
      "step": 369000
    },
    {
      "epoch": 3.367946565442733,
      "grad_norm": 4.551790714263916,
      "learning_rate": 4.7193377862131064e-05,
      "loss": 0.7256,
      "step": 369100
    },
    {
      "epoch": 3.3688590408058983,
      "grad_norm": 6.266209125518799,
      "learning_rate": 4.719261746599509e-05,
      "loss": 0.756,
      "step": 369200
    },
    {
      "epoch": 3.3697715161690636,
      "grad_norm": 4.288627624511719,
      "learning_rate": 4.719185706985911e-05,
      "loss": 0.7298,
      "step": 369300
    },
    {
      "epoch": 3.3706839915322284,
      "grad_norm": 4.096366882324219,
      "learning_rate": 4.719109667372315e-05,
      "loss": 0.7598,
      "step": 369400
    },
    {
      "epoch": 3.3715964668953937,
      "grad_norm": 3.8539271354675293,
      "learning_rate": 4.719033627758717e-05,
      "loss": 0.738,
      "step": 369500
    },
    {
      "epoch": 3.372508942258559,
      "grad_norm": 3.7178151607513428,
      "learning_rate": 4.71895758814512e-05,
      "loss": 0.7336,
      "step": 369600
    },
    {
      "epoch": 3.3734214176217243,
      "grad_norm": 4.111144542694092,
      "learning_rate": 4.718881548531523e-05,
      "loss": 0.7117,
      "step": 369700
    },
    {
      "epoch": 3.3743338929848896,
      "grad_norm": 3.419412851333618,
      "learning_rate": 4.718805508917926e-05,
      "loss": 0.7422,
      "step": 369800
    },
    {
      "epoch": 3.3752463683480545,
      "grad_norm": 3.2009878158569336,
      "learning_rate": 4.718729469304329e-05,
      "loss": 0.7409,
      "step": 369900
    },
    {
      "epoch": 3.3761588437112198,
      "grad_norm": 4.190420627593994,
      "learning_rate": 4.718653429690732e-05,
      "loss": 0.6949,
      "step": 370000
    },
    {
      "epoch": 3.377071319074385,
      "grad_norm": 3.8942928314208984,
      "learning_rate": 4.7185773900771344e-05,
      "loss": 0.7318,
      "step": 370100
    },
    {
      "epoch": 3.3779837944375504,
      "grad_norm": 4.747195243835449,
      "learning_rate": 4.718501350463538e-05,
      "loss": 0.7444,
      "step": 370200
    },
    {
      "epoch": 3.3788962698007152,
      "grad_norm": 3.801800489425659,
      "learning_rate": 4.7184253108499404e-05,
      "loss": 0.704,
      "step": 370300
    },
    {
      "epoch": 3.3798087451638805,
      "grad_norm": 4.522612571716309,
      "learning_rate": 4.7183492712363434e-05,
      "loss": 0.7692,
      "step": 370400
    },
    {
      "epoch": 3.380721220527046,
      "grad_norm": 3.896303415298462,
      "learning_rate": 4.7182732316227464e-05,
      "loss": 0.7412,
      "step": 370500
    },
    {
      "epoch": 3.381633695890211,
      "grad_norm": 4.410833358764648,
      "learning_rate": 4.7181971920091494e-05,
      "loss": 0.7342,
      "step": 370600
    },
    {
      "epoch": 3.382546171253376,
      "grad_norm": 3.5736305713653564,
      "learning_rate": 4.718121152395552e-05,
      "loss": 0.7012,
      "step": 370700
    },
    {
      "epoch": 3.3834586466165413,
      "grad_norm": 4.028936386108398,
      "learning_rate": 4.7180451127819555e-05,
      "loss": 0.7421,
      "step": 370800
    },
    {
      "epoch": 3.3843711219797066,
      "grad_norm": 5.129684925079346,
      "learning_rate": 4.717969073168358e-05,
      "loss": 0.7793,
      "step": 370900
    },
    {
      "epoch": 3.385283597342872,
      "grad_norm": 4.440248966217041,
      "learning_rate": 4.717893033554761e-05,
      "loss": 0.7033,
      "step": 371000
    },
    {
      "epoch": 3.3861960727060367,
      "grad_norm": 4.007363796234131,
      "learning_rate": 4.717816993941164e-05,
      "loss": 0.6979,
      "step": 371100
    },
    {
      "epoch": 3.387108548069202,
      "grad_norm": 4.259214401245117,
      "learning_rate": 4.717740954327567e-05,
      "loss": 0.7719,
      "step": 371200
    },
    {
      "epoch": 3.3880210234323673,
      "grad_norm": 5.102652549743652,
      "learning_rate": 4.71766491471397e-05,
      "loss": 0.7207,
      "step": 371300
    },
    {
      "epoch": 3.3889334987955326,
      "grad_norm": 4.4404296875,
      "learning_rate": 4.717588875100373e-05,
      "loss": 0.7425,
      "step": 371400
    },
    {
      "epoch": 3.389845974158698,
      "grad_norm": 3.561877727508545,
      "learning_rate": 4.717512835486775e-05,
      "loss": 0.7541,
      "step": 371500
    },
    {
      "epoch": 3.390758449521863,
      "grad_norm": 4.427769184112549,
      "learning_rate": 4.717436795873179e-05,
      "loss": 0.7267,
      "step": 371600
    },
    {
      "epoch": 3.391670924885028,
      "grad_norm": 4.826210975646973,
      "learning_rate": 4.717360756259581e-05,
      "loss": 0.7241,
      "step": 371700
    },
    {
      "epoch": 3.3925834002481934,
      "grad_norm": 4.652218341827393,
      "learning_rate": 4.717284716645984e-05,
      "loss": 0.7135,
      "step": 371800
    },
    {
      "epoch": 3.3934958756113587,
      "grad_norm": 3.1571044921875,
      "learning_rate": 4.717208677032387e-05,
      "loss": 0.7453,
      "step": 371900
    },
    {
      "epoch": 3.3944083509745235,
      "grad_norm": 5.3355255126953125,
      "learning_rate": 4.71713263741879e-05,
      "loss": 0.7271,
      "step": 372000
    },
    {
      "epoch": 3.395320826337689,
      "grad_norm": 4.349674701690674,
      "learning_rate": 4.7170565978051925e-05,
      "loss": 0.7144,
      "step": 372100
    },
    {
      "epoch": 3.396233301700854,
      "grad_norm": 4.390585899353027,
      "learning_rate": 4.7169805581915955e-05,
      "loss": 0.7543,
      "step": 372200
    },
    {
      "epoch": 3.3971457770640194,
      "grad_norm": 3.6700282096862793,
      "learning_rate": 4.7169045185779985e-05,
      "loss": 0.7646,
      "step": 372300
    },
    {
      "epoch": 3.3980582524271843,
      "grad_norm": 3.6335861682891846,
      "learning_rate": 4.7168284789644015e-05,
      "loss": 0.7116,
      "step": 372400
    },
    {
      "epoch": 3.3989707277903496,
      "grad_norm": 4.074729919433594,
      "learning_rate": 4.7167524393508045e-05,
      "loss": 0.7013,
      "step": 372500
    },
    {
      "epoch": 3.399883203153515,
      "grad_norm": 4.356374740600586,
      "learning_rate": 4.716676399737207e-05,
      "loss": 0.7124,
      "step": 372600
    },
    {
      "epoch": 3.40079567851668,
      "grad_norm": 3.513235569000244,
      "learning_rate": 4.7166003601236106e-05,
      "loss": 0.7834,
      "step": 372700
    },
    {
      "epoch": 3.401708153879845,
      "grad_norm": 2.655097007751465,
      "learning_rate": 4.716524320510013e-05,
      "loss": 0.7598,
      "step": 372800
    },
    {
      "epoch": 3.4026206292430103,
      "grad_norm": 4.425268173217773,
      "learning_rate": 4.716448280896416e-05,
      "loss": 0.7139,
      "step": 372900
    },
    {
      "epoch": 3.4035331046061756,
      "grad_norm": 2.8519153594970703,
      "learning_rate": 4.716372241282819e-05,
      "loss": 0.7534,
      "step": 373000
    },
    {
      "epoch": 3.404445579969341,
      "grad_norm": 4.044124126434326,
      "learning_rate": 4.716296201669222e-05,
      "loss": 0.7353,
      "step": 373100
    },
    {
      "epoch": 3.4053580553325062,
      "grad_norm": 4.171593189239502,
      "learning_rate": 4.716220162055624e-05,
      "loss": 0.7436,
      "step": 373200
    },
    {
      "epoch": 3.406270530695671,
      "grad_norm": 5.040644645690918,
      "learning_rate": 4.716144122442028e-05,
      "loss": 0.7475,
      "step": 373300
    },
    {
      "epoch": 3.4071830060588364,
      "grad_norm": 3.3636860847473145,
      "learning_rate": 4.71606808282843e-05,
      "loss": 0.6996,
      "step": 373400
    },
    {
      "epoch": 3.4080954814220017,
      "grad_norm": 3.7933969497680664,
      "learning_rate": 4.715992043214833e-05,
      "loss": 0.7476,
      "step": 373500
    },
    {
      "epoch": 3.409007956785167,
      "grad_norm": 5.06417989730835,
      "learning_rate": 4.715916003601236e-05,
      "loss": 0.7211,
      "step": 373600
    },
    {
      "epoch": 3.409920432148332,
      "grad_norm": 3.855746030807495,
      "learning_rate": 4.715839963987639e-05,
      "loss": 0.7503,
      "step": 373700
    },
    {
      "epoch": 3.410832907511497,
      "grad_norm": 3.196737289428711,
      "learning_rate": 4.715763924374042e-05,
      "loss": 0.7218,
      "step": 373800
    },
    {
      "epoch": 3.4117453828746624,
      "grad_norm": 4.786140441894531,
      "learning_rate": 4.715687884760445e-05,
      "loss": 0.732,
      "step": 373900
    },
    {
      "epoch": 3.4126578582378277,
      "grad_norm": 4.197044372558594,
      "learning_rate": 4.7156118451468476e-05,
      "loss": 0.7457,
      "step": 374000
    },
    {
      "epoch": 3.4135703336009926,
      "grad_norm": 4.298619270324707,
      "learning_rate": 4.715535805533251e-05,
      "loss": 0.6949,
      "step": 374100
    },
    {
      "epoch": 3.414482808964158,
      "grad_norm": 3.4744865894317627,
      "learning_rate": 4.7154597659196536e-05,
      "loss": 0.7138,
      "step": 374200
    },
    {
      "epoch": 3.415395284327323,
      "grad_norm": 3.798860549926758,
      "learning_rate": 4.7153837263060566e-05,
      "loss": 0.7488,
      "step": 374300
    },
    {
      "epoch": 3.4163077596904885,
      "grad_norm": 2.8133697509765625,
      "learning_rate": 4.7153076866924596e-05,
      "loss": 0.751,
      "step": 374400
    },
    {
      "epoch": 3.4172202350536534,
      "grad_norm": 4.678199291229248,
      "learning_rate": 4.7152316470788626e-05,
      "loss": 0.6968,
      "step": 374500
    },
    {
      "epoch": 3.4181327104168187,
      "grad_norm": 3.548976421356201,
      "learning_rate": 4.715155607465265e-05,
      "loss": 0.7132,
      "step": 374600
    },
    {
      "epoch": 3.419045185779984,
      "grad_norm": 4.685152053833008,
      "learning_rate": 4.7150795678516687e-05,
      "loss": 0.7174,
      "step": 374700
    },
    {
      "epoch": 3.4199576611431493,
      "grad_norm": 3.274359703063965,
      "learning_rate": 4.715003528238071e-05,
      "loss": 0.6932,
      "step": 374800
    },
    {
      "epoch": 3.4208701365063146,
      "grad_norm": 3.324632406234741,
      "learning_rate": 4.714927488624474e-05,
      "loss": 0.7492,
      "step": 374900
    },
    {
      "epoch": 3.4217826118694794,
      "grad_norm": 4.22348165512085,
      "learning_rate": 4.714851449010877e-05,
      "loss": 0.7176,
      "step": 375000
    },
    {
      "epoch": 3.4226950872326447,
      "grad_norm": 4.197150230407715,
      "learning_rate": 4.714775409397279e-05,
      "loss": 0.7065,
      "step": 375100
    },
    {
      "epoch": 3.42360756259581,
      "grad_norm": 3.6291661262512207,
      "learning_rate": 4.714699369783683e-05,
      "loss": 0.7305,
      "step": 375200
    },
    {
      "epoch": 3.424520037958975,
      "grad_norm": 4.3612565994262695,
      "learning_rate": 4.7146233301700853e-05,
      "loss": 0.7293,
      "step": 375300
    },
    {
      "epoch": 3.42543251332214,
      "grad_norm": 5.565362930297852,
      "learning_rate": 4.7145472905564884e-05,
      "loss": 0.6858,
      "step": 375400
    },
    {
      "epoch": 3.4263449886853055,
      "grad_norm": 4.384678363800049,
      "learning_rate": 4.7144712509428914e-05,
      "loss": 0.7106,
      "step": 375500
    },
    {
      "epoch": 3.4272574640484708,
      "grad_norm": 4.14347505569458,
      "learning_rate": 4.7143952113292944e-05,
      "loss": 0.7558,
      "step": 375600
    },
    {
      "epoch": 3.428169939411636,
      "grad_norm": 4.338446140289307,
      "learning_rate": 4.7143191717156974e-05,
      "loss": 0.7323,
      "step": 375700
    },
    {
      "epoch": 3.429082414774801,
      "grad_norm": 4.201111316680908,
      "learning_rate": 4.7142431321021004e-05,
      "loss": 0.7373,
      "step": 375800
    },
    {
      "epoch": 3.429994890137966,
      "grad_norm": 4.56051778793335,
      "learning_rate": 4.714167092488503e-05,
      "loss": 0.761,
      "step": 375900
    },
    {
      "epoch": 3.4309073655011315,
      "grad_norm": 3.6461691856384277,
      "learning_rate": 4.714091052874906e-05,
      "loss": 0.7386,
      "step": 376000
    },
    {
      "epoch": 3.431819840864297,
      "grad_norm": 3.4417576789855957,
      "learning_rate": 4.714015013261309e-05,
      "loss": 0.7432,
      "step": 376100
    },
    {
      "epoch": 3.4327323162274617,
      "grad_norm": 3.3141069412231445,
      "learning_rate": 4.713938973647712e-05,
      "loss": 0.7663,
      "step": 376200
    },
    {
      "epoch": 3.433644791590627,
      "grad_norm": 4.355069637298584,
      "learning_rate": 4.713862934034115e-05,
      "loss": 0.7316,
      "step": 376300
    },
    {
      "epoch": 3.4345572669537923,
      "grad_norm": 5.028255462646484,
      "learning_rate": 4.713786894420518e-05,
      "loss": 0.7533,
      "step": 376400
    },
    {
      "epoch": 3.4354697423169576,
      "grad_norm": 4.9971137046813965,
      "learning_rate": 4.71371085480692e-05,
      "loss": 0.7411,
      "step": 376500
    },
    {
      "epoch": 3.436382217680123,
      "grad_norm": 4.350061893463135,
      "learning_rate": 4.713634815193324e-05,
      "loss": 0.6903,
      "step": 376600
    },
    {
      "epoch": 3.4372946930432877,
      "grad_norm": 4.04423189163208,
      "learning_rate": 4.713558775579726e-05,
      "loss": 0.7099,
      "step": 376700
    },
    {
      "epoch": 3.438207168406453,
      "grad_norm": 3.3970699310302734,
      "learning_rate": 4.713482735966129e-05,
      "loss": 0.7106,
      "step": 376800
    },
    {
      "epoch": 3.4391196437696183,
      "grad_norm": 4.245410442352295,
      "learning_rate": 4.713406696352532e-05,
      "loss": 0.6895,
      "step": 376900
    },
    {
      "epoch": 3.440032119132783,
      "grad_norm": 3.9024314880371094,
      "learning_rate": 4.713330656738935e-05,
      "loss": 0.7487,
      "step": 377000
    },
    {
      "epoch": 3.4409445944959485,
      "grad_norm": 4.296019077301025,
      "learning_rate": 4.713254617125338e-05,
      "loss": 0.7645,
      "step": 377100
    },
    {
      "epoch": 3.4418570698591138,
      "grad_norm": 3.9502971172332764,
      "learning_rate": 4.713178577511741e-05,
      "loss": 0.7231,
      "step": 377200
    },
    {
      "epoch": 3.442769545222279,
      "grad_norm": 3.5621776580810547,
      "learning_rate": 4.7131025378981434e-05,
      "loss": 0.723,
      "step": 377300
    },
    {
      "epoch": 3.4436820205854444,
      "grad_norm": 4.274910926818848,
      "learning_rate": 4.7130264982845465e-05,
      "loss": 0.7494,
      "step": 377400
    },
    {
      "epoch": 3.4445944959486092,
      "grad_norm": 2.7568092346191406,
      "learning_rate": 4.7129504586709495e-05,
      "loss": 0.7508,
      "step": 377500
    },
    {
      "epoch": 3.4455069713117745,
      "grad_norm": 2.5755839347839355,
      "learning_rate": 4.7128744190573525e-05,
      "loss": 0.7711,
      "step": 377600
    },
    {
      "epoch": 3.44641944667494,
      "grad_norm": 4.13123083114624,
      "learning_rate": 4.7127983794437555e-05,
      "loss": 0.7431,
      "step": 377700
    },
    {
      "epoch": 3.447331922038105,
      "grad_norm": 5.437438488006592,
      "learning_rate": 4.712722339830158e-05,
      "loss": 0.7451,
      "step": 377800
    },
    {
      "epoch": 3.44824439740127,
      "grad_norm": 4.804293155670166,
      "learning_rate": 4.712646300216561e-05,
      "loss": 0.716,
      "step": 377900
    },
    {
      "epoch": 3.4491568727644353,
      "grad_norm": 3.1027374267578125,
      "learning_rate": 4.712570260602964e-05,
      "loss": 0.7484,
      "step": 378000
    },
    {
      "epoch": 3.4500693481276006,
      "grad_norm": 3.5696074962615967,
      "learning_rate": 4.712494220989367e-05,
      "loss": 0.7361,
      "step": 378100
    },
    {
      "epoch": 3.450981823490766,
      "grad_norm": 3.3703739643096924,
      "learning_rate": 4.71241818137577e-05,
      "loss": 0.7307,
      "step": 378200
    },
    {
      "epoch": 3.451894298853931,
      "grad_norm": 4.582193851470947,
      "learning_rate": 4.712342141762173e-05,
      "loss": 0.7198,
      "step": 378300
    },
    {
      "epoch": 3.452806774217096,
      "grad_norm": 4.256913185119629,
      "learning_rate": 4.712266102148575e-05,
      "loss": 0.7201,
      "step": 378400
    },
    {
      "epoch": 3.4537192495802613,
      "grad_norm": 4.739476203918457,
      "learning_rate": 4.712190062534979e-05,
      "loss": 0.7,
      "step": 378500
    },
    {
      "epoch": 3.4546317249434266,
      "grad_norm": 4.537616729736328,
      "learning_rate": 4.712114022921381e-05,
      "loss": 0.6799,
      "step": 378600
    },
    {
      "epoch": 3.4555442003065915,
      "grad_norm": 4.66221809387207,
      "learning_rate": 4.712037983307784e-05,
      "loss": 0.7683,
      "step": 378700
    },
    {
      "epoch": 3.456456675669757,
      "grad_norm": 3.9966211318969727,
      "learning_rate": 4.711961943694187e-05,
      "loss": 0.7116,
      "step": 378800
    },
    {
      "epoch": 3.457369151032922,
      "grad_norm": 4.427234172821045,
      "learning_rate": 4.71188590408059e-05,
      "loss": 0.7655,
      "step": 378900
    },
    {
      "epoch": 3.4582816263960874,
      "grad_norm": 3.959960699081421,
      "learning_rate": 4.7118098644669925e-05,
      "loss": 0.702,
      "step": 379000
    },
    {
      "epoch": 3.4591941017592527,
      "grad_norm": 4.335717678070068,
      "learning_rate": 4.711733824853396e-05,
      "loss": 0.7537,
      "step": 379100
    },
    {
      "epoch": 3.4601065771224175,
      "grad_norm": 3.9631187915802,
      "learning_rate": 4.7116577852397985e-05,
      "loss": 0.7566,
      "step": 379200
    },
    {
      "epoch": 3.461019052485583,
      "grad_norm": 4.3652753829956055,
      "learning_rate": 4.7115817456262015e-05,
      "loss": 0.7644,
      "step": 379300
    },
    {
      "epoch": 3.461931527848748,
      "grad_norm": 4.308724403381348,
      "learning_rate": 4.7115057060126046e-05,
      "loss": 0.756,
      "step": 379400
    },
    {
      "epoch": 3.4628440032119134,
      "grad_norm": 3.8500263690948486,
      "learning_rate": 4.7114296663990076e-05,
      "loss": 0.7464,
      "step": 379500
    },
    {
      "epoch": 3.4637564785750783,
      "grad_norm": 3.4999334812164307,
      "learning_rate": 4.7113536267854106e-05,
      "loss": 0.7409,
      "step": 379600
    },
    {
      "epoch": 3.4646689539382436,
      "grad_norm": 3.705972671508789,
      "learning_rate": 4.7112775871718136e-05,
      "loss": 0.742,
      "step": 379700
    },
    {
      "epoch": 3.465581429301409,
      "grad_norm": 4.357077598571777,
      "learning_rate": 4.711201547558216e-05,
      "loss": 0.7012,
      "step": 379800
    },
    {
      "epoch": 3.466493904664574,
      "grad_norm": 5.236342906951904,
      "learning_rate": 4.7111255079446196e-05,
      "loss": 0.7487,
      "step": 379900
    },
    {
      "epoch": 3.4674063800277395,
      "grad_norm": 3.626518964767456,
      "learning_rate": 4.711049468331022e-05,
      "loss": 0.7348,
      "step": 380000
    },
    {
      "epoch": 3.4683188553909043,
      "grad_norm": 3.777845859527588,
      "learning_rate": 4.710973428717425e-05,
      "loss": 0.7527,
      "step": 380100
    },
    {
      "epoch": 3.4692313307540696,
      "grad_norm": 3.4773497581481934,
      "learning_rate": 4.710897389103828e-05,
      "loss": 0.7358,
      "step": 380200
    },
    {
      "epoch": 3.470143806117235,
      "grad_norm": 4.437536239624023,
      "learning_rate": 4.710821349490231e-05,
      "loss": 0.7339,
      "step": 380300
    },
    {
      "epoch": 3.4710562814804,
      "grad_norm": 4.162502288818359,
      "learning_rate": 4.710745309876633e-05,
      "loss": 0.7119,
      "step": 380400
    },
    {
      "epoch": 3.471968756843565,
      "grad_norm": 4.3291239738464355,
      "learning_rate": 4.710669270263037e-05,
      "loss": 0.7383,
      "step": 380500
    },
    {
      "epoch": 3.4728812322067304,
      "grad_norm": 4.3914265632629395,
      "learning_rate": 4.710593230649439e-05,
      "loss": 0.7553,
      "step": 380600
    },
    {
      "epoch": 3.4737937075698957,
      "grad_norm": 3.256171703338623,
      "learning_rate": 4.710517191035842e-05,
      "loss": 0.7301,
      "step": 380700
    },
    {
      "epoch": 3.474706182933061,
      "grad_norm": 4.542724132537842,
      "learning_rate": 4.710441151422245e-05,
      "loss": 0.7304,
      "step": 380800
    },
    {
      "epoch": 3.475618658296226,
      "grad_norm": 4.580248832702637,
      "learning_rate": 4.7103651118086476e-05,
      "loss": 0.7759,
      "step": 380900
    },
    {
      "epoch": 3.476531133659391,
      "grad_norm": 4.4717326164245605,
      "learning_rate": 4.710289072195051e-05,
      "loss": 0.7351,
      "step": 381000
    },
    {
      "epoch": 3.4774436090225564,
      "grad_norm": 4.648679256439209,
      "learning_rate": 4.7102130325814536e-05,
      "loss": 0.735,
      "step": 381100
    },
    {
      "epoch": 3.4783560843857217,
      "grad_norm": 4.588157653808594,
      "learning_rate": 4.7101369929678566e-05,
      "loss": 0.7289,
      "step": 381200
    },
    {
      "epoch": 3.4792685597488866,
      "grad_norm": 4.121613502502441,
      "learning_rate": 4.7100609533542596e-05,
      "loss": 0.7462,
      "step": 381300
    },
    {
      "epoch": 3.480181035112052,
      "grad_norm": 3.964111804962158,
      "learning_rate": 4.7099849137406627e-05,
      "loss": 0.7271,
      "step": 381400
    },
    {
      "epoch": 3.481093510475217,
      "grad_norm": 4.464456081390381,
      "learning_rate": 4.709908874127065e-05,
      "loss": 0.7429,
      "step": 381500
    },
    {
      "epoch": 3.4820059858383825,
      "grad_norm": 4.133628845214844,
      "learning_rate": 4.709832834513469e-05,
      "loss": 0.7,
      "step": 381600
    },
    {
      "epoch": 3.482918461201548,
      "grad_norm": 4.385806560516357,
      "learning_rate": 4.709756794899871e-05,
      "loss": 0.7206,
      "step": 381700
    },
    {
      "epoch": 3.4838309365647127,
      "grad_norm": 3.5642008781433105,
      "learning_rate": 4.709680755286274e-05,
      "loss": 0.7183,
      "step": 381800
    },
    {
      "epoch": 3.484743411927878,
      "grad_norm": 4.091855525970459,
      "learning_rate": 4.709604715672677e-05,
      "loss": 0.7125,
      "step": 381900
    },
    {
      "epoch": 3.4856558872910433,
      "grad_norm": 4.079415798187256,
      "learning_rate": 4.70952867605908e-05,
      "loss": 0.7162,
      "step": 382000
    },
    {
      "epoch": 3.486568362654208,
      "grad_norm": 4.438429355621338,
      "learning_rate": 4.709452636445483e-05,
      "loss": 0.7508,
      "step": 382100
    },
    {
      "epoch": 3.4874808380173734,
      "grad_norm": 4.479633331298828,
      "learning_rate": 4.709376596831886e-05,
      "loss": 0.7359,
      "step": 382200
    },
    {
      "epoch": 3.4883933133805387,
      "grad_norm": 4.03930139541626,
      "learning_rate": 4.7093005572182884e-05,
      "loss": 0.7853,
      "step": 382300
    },
    {
      "epoch": 3.489305788743704,
      "grad_norm": 4.069916725158691,
      "learning_rate": 4.709224517604692e-05,
      "loss": 0.7165,
      "step": 382400
    },
    {
      "epoch": 3.4902182641068693,
      "grad_norm": 4.855310916900635,
      "learning_rate": 4.7091484779910944e-05,
      "loss": 0.7661,
      "step": 382500
    },
    {
      "epoch": 3.491130739470034,
      "grad_norm": 4.8705902099609375,
      "learning_rate": 4.7090724383774974e-05,
      "loss": 0.7257,
      "step": 382600
    },
    {
      "epoch": 3.4920432148331995,
      "grad_norm": 4.323821544647217,
      "learning_rate": 4.7089963987639004e-05,
      "loss": 0.7437,
      "step": 382700
    },
    {
      "epoch": 3.4929556901963648,
      "grad_norm": 4.496278762817383,
      "learning_rate": 4.7089203591503034e-05,
      "loss": 0.6941,
      "step": 382800
    },
    {
      "epoch": 3.49386816555953,
      "grad_norm": 4.084007263183594,
      "learning_rate": 4.708844319536706e-05,
      "loss": 0.7185,
      "step": 382900
    },
    {
      "epoch": 3.494780640922695,
      "grad_norm": 3.915083169937134,
      "learning_rate": 4.7087682799231094e-05,
      "loss": 0.7111,
      "step": 383000
    },
    {
      "epoch": 3.49569311628586,
      "grad_norm": 4.199038505554199,
      "learning_rate": 4.708692240309512e-05,
      "loss": 0.7017,
      "step": 383100
    },
    {
      "epoch": 3.4966055916490255,
      "grad_norm": 3.6633033752441406,
      "learning_rate": 4.708616200695915e-05,
      "loss": 0.7544,
      "step": 383200
    },
    {
      "epoch": 3.497518067012191,
      "grad_norm": 4.319456577301025,
      "learning_rate": 4.708540161082318e-05,
      "loss": 0.7441,
      "step": 383300
    },
    {
      "epoch": 3.498430542375356,
      "grad_norm": 5.1094069480896,
      "learning_rate": 4.708464121468721e-05,
      "loss": 0.7471,
      "step": 383400
    },
    {
      "epoch": 3.499343017738521,
      "grad_norm": 5.1647114753723145,
      "learning_rate": 4.708388081855124e-05,
      "loss": 0.7239,
      "step": 383500
    },
    {
      "epoch": 3.5002554931016863,
      "grad_norm": 3.508305549621582,
      "learning_rate": 4.708312042241526e-05,
      "loss": 0.7294,
      "step": 383600
    },
    {
      "epoch": 3.5011679684648516,
      "grad_norm": 3.5105645656585693,
      "learning_rate": 4.708236002627929e-05,
      "loss": 0.7255,
      "step": 383700
    },
    {
      "epoch": 3.5020804438280164,
      "grad_norm": 3.9024717807769775,
      "learning_rate": 4.708159963014332e-05,
      "loss": 0.7501,
      "step": 383800
    },
    {
      "epoch": 3.5029929191911817,
      "grad_norm": 4.320307731628418,
      "learning_rate": 4.708083923400735e-05,
      "loss": 0.7661,
      "step": 383900
    },
    {
      "epoch": 3.503905394554347,
      "grad_norm": 4.551271438598633,
      "learning_rate": 4.7080078837871374e-05,
      "loss": 0.7544,
      "step": 384000
    },
    {
      "epoch": 3.5048178699175123,
      "grad_norm": 4.705122470855713,
      "learning_rate": 4.707931844173541e-05,
      "loss": 0.7683,
      "step": 384100
    },
    {
      "epoch": 3.5057303452806776,
      "grad_norm": 4.352100372314453,
      "learning_rate": 4.7078558045599435e-05,
      "loss": 0.7596,
      "step": 384200
    },
    {
      "epoch": 3.5066428206438425,
      "grad_norm": 3.1480278968811035,
      "learning_rate": 4.7077797649463465e-05,
      "loss": 0.7534,
      "step": 384300
    },
    {
      "epoch": 3.5075552960070078,
      "grad_norm": 4.5952653884887695,
      "learning_rate": 4.7077037253327495e-05,
      "loss": 0.7459,
      "step": 384400
    },
    {
      "epoch": 3.508467771370173,
      "grad_norm": 4.38923978805542,
      "learning_rate": 4.7076276857191525e-05,
      "loss": 0.7167,
      "step": 384500
    },
    {
      "epoch": 3.509380246733338,
      "grad_norm": 4.116699695587158,
      "learning_rate": 4.7075516461055555e-05,
      "loss": 0.6993,
      "step": 384600
    },
    {
      "epoch": 3.5102927220965032,
      "grad_norm": 2.6697239875793457,
      "learning_rate": 4.7074756064919585e-05,
      "loss": 0.7318,
      "step": 384700
    },
    {
      "epoch": 3.5112051974596685,
      "grad_norm": 4.341913223266602,
      "learning_rate": 4.707399566878361e-05,
      "loss": 0.7257,
      "step": 384800
    },
    {
      "epoch": 3.512117672822834,
      "grad_norm": 4.267766952514648,
      "learning_rate": 4.7073235272647645e-05,
      "loss": 0.7254,
      "step": 384900
    },
    {
      "epoch": 3.513030148185999,
      "grad_norm": 5.106864929199219,
      "learning_rate": 4.707247487651167e-05,
      "loss": 0.741,
      "step": 385000
    },
    {
      "epoch": 3.5139426235491644,
      "grad_norm": 5.09536600112915,
      "learning_rate": 4.70717144803757e-05,
      "loss": 0.7394,
      "step": 385100
    },
    {
      "epoch": 3.5148550989123293,
      "grad_norm": 4.252213001251221,
      "learning_rate": 4.707095408423973e-05,
      "loss": 0.7536,
      "step": 385200
    },
    {
      "epoch": 3.5157675742754946,
      "grad_norm": 3.274848699569702,
      "learning_rate": 4.707019368810376e-05,
      "loss": 0.7262,
      "step": 385300
    },
    {
      "epoch": 3.51668004963866,
      "grad_norm": 3.8287696838378906,
      "learning_rate": 4.706943329196778e-05,
      "loss": 0.7299,
      "step": 385400
    },
    {
      "epoch": 3.5175925250018247,
      "grad_norm": 4.161135673522949,
      "learning_rate": 4.706867289583182e-05,
      "loss": 0.7179,
      "step": 385500
    },
    {
      "epoch": 3.51850500036499,
      "grad_norm": 3.7112581729888916,
      "learning_rate": 4.706791249969584e-05,
      "loss": 0.7121,
      "step": 385600
    },
    {
      "epoch": 3.5194174757281553,
      "grad_norm": 3.9812300205230713,
      "learning_rate": 4.706715210355987e-05,
      "loss": 0.7112,
      "step": 385700
    },
    {
      "epoch": 3.5203299510913206,
      "grad_norm": 4.101913928985596,
      "learning_rate": 4.70663917074239e-05,
      "loss": 0.7357,
      "step": 385800
    },
    {
      "epoch": 3.521242426454486,
      "grad_norm": 4.664113521575928,
      "learning_rate": 4.706563131128793e-05,
      "loss": 0.7749,
      "step": 385900
    },
    {
      "epoch": 3.522154901817651,
      "grad_norm": 3.4794342517852783,
      "learning_rate": 4.706487091515196e-05,
      "loss": 0.6926,
      "step": 386000
    },
    {
      "epoch": 3.523067377180816,
      "grad_norm": 4.308638095855713,
      "learning_rate": 4.706411051901599e-05,
      "loss": 0.7844,
      "step": 386100
    },
    {
      "epoch": 3.5239798525439814,
      "grad_norm": 4.80692720413208,
      "learning_rate": 4.7063350122880016e-05,
      "loss": 0.6934,
      "step": 386200
    },
    {
      "epoch": 3.5248923279071462,
      "grad_norm": 4.420419216156006,
      "learning_rate": 4.7062589726744046e-05,
      "loss": 0.7335,
      "step": 386300
    },
    {
      "epoch": 3.5258048032703115,
      "grad_norm": 3.880679130554199,
      "learning_rate": 4.7061829330608076e-05,
      "loss": 0.7523,
      "step": 386400
    },
    {
      "epoch": 3.526717278633477,
      "grad_norm": 3.8978872299194336,
      "learning_rate": 4.70610689344721e-05,
      "loss": 0.7478,
      "step": 386500
    },
    {
      "epoch": 3.527629753996642,
      "grad_norm": 4.121887683868408,
      "learning_rate": 4.7060308538336136e-05,
      "loss": 0.7148,
      "step": 386600
    },
    {
      "epoch": 3.5285422293598074,
      "grad_norm": 4.6580939292907715,
      "learning_rate": 4.705954814220016e-05,
      "loss": 0.7443,
      "step": 386700
    },
    {
      "epoch": 3.5294547047229727,
      "grad_norm": 4.153290748596191,
      "learning_rate": 4.705878774606419e-05,
      "loss": 0.7303,
      "step": 386800
    },
    {
      "epoch": 3.5303671800861376,
      "grad_norm": 4.5572333335876465,
      "learning_rate": 4.705802734992822e-05,
      "loss": 0.7537,
      "step": 386900
    },
    {
      "epoch": 3.531279655449303,
      "grad_norm": 4.219689846038818,
      "learning_rate": 4.705726695379225e-05,
      "loss": 0.7384,
      "step": 387000
    },
    {
      "epoch": 3.532192130812468,
      "grad_norm": 4.296765327453613,
      "learning_rate": 4.705650655765628e-05,
      "loss": 0.7139,
      "step": 387100
    },
    {
      "epoch": 3.533104606175633,
      "grad_norm": 4.514101982116699,
      "learning_rate": 4.705574616152031e-05,
      "loss": 0.7102,
      "step": 387200
    },
    {
      "epoch": 3.5340170815387983,
      "grad_norm": 2.7791266441345215,
      "learning_rate": 4.705498576538433e-05,
      "loss": 0.7694,
      "step": 387300
    },
    {
      "epoch": 3.5349295569019636,
      "grad_norm": 4.542418003082275,
      "learning_rate": 4.705422536924837e-05,
      "loss": 0.7261,
      "step": 387400
    },
    {
      "epoch": 3.535842032265129,
      "grad_norm": 3.994535207748413,
      "learning_rate": 4.705346497311239e-05,
      "loss": 0.7246,
      "step": 387500
    },
    {
      "epoch": 3.5367545076282942,
      "grad_norm": 4.6467719078063965,
      "learning_rate": 4.705270457697642e-05,
      "loss": 0.7126,
      "step": 387600
    },
    {
      "epoch": 3.537666982991459,
      "grad_norm": 4.631080150604248,
      "learning_rate": 4.705194418084045e-05,
      "loss": 0.7671,
      "step": 387700
    },
    {
      "epoch": 3.5385794583546244,
      "grad_norm": 4.214254379272461,
      "learning_rate": 4.705118378470448e-05,
      "loss": 0.7424,
      "step": 387800
    },
    {
      "epoch": 3.5394919337177897,
      "grad_norm": 3.7041635513305664,
      "learning_rate": 4.7050423388568506e-05,
      "loss": 0.7538,
      "step": 387900
    },
    {
      "epoch": 3.5404044090809546,
      "grad_norm": 3.6166138648986816,
      "learning_rate": 4.704966299243254e-05,
      "loss": 0.7547,
      "step": 388000
    },
    {
      "epoch": 3.54131688444412,
      "grad_norm": 4.233958721160889,
      "learning_rate": 4.7048902596296567e-05,
      "loss": 0.7293,
      "step": 388100
    },
    {
      "epoch": 3.542229359807285,
      "grad_norm": 3.870820999145508,
      "learning_rate": 4.7048142200160597e-05,
      "loss": 0.7328,
      "step": 388200
    },
    {
      "epoch": 3.5431418351704504,
      "grad_norm": 4.914161205291748,
      "learning_rate": 4.704738180402463e-05,
      "loss": 0.7104,
      "step": 388300
    },
    {
      "epoch": 3.5440543105336157,
      "grad_norm": 4.632920742034912,
      "learning_rate": 4.704662140788866e-05,
      "loss": 0.7515,
      "step": 388400
    },
    {
      "epoch": 3.544966785896781,
      "grad_norm": 3.7064154148101807,
      "learning_rate": 4.704586101175269e-05,
      "loss": 0.7519,
      "step": 388500
    },
    {
      "epoch": 3.545879261259946,
      "grad_norm": 5.0814714431762695,
      "learning_rate": 4.704510061561672e-05,
      "loss": 0.7348,
      "step": 388600
    },
    {
      "epoch": 3.546791736623111,
      "grad_norm": 3.4230120182037354,
      "learning_rate": 4.704434021948074e-05,
      "loss": 0.7081,
      "step": 388700
    },
    {
      "epoch": 3.5477042119862765,
      "grad_norm": 4.456929683685303,
      "learning_rate": 4.704357982334478e-05,
      "loss": 0.696,
      "step": 388800
    },
    {
      "epoch": 3.5486166873494414,
      "grad_norm": 3.6877710819244385,
      "learning_rate": 4.70428194272088e-05,
      "loss": 0.7523,
      "step": 388900
    },
    {
      "epoch": 3.5495291627126067,
      "grad_norm": 4.861880302429199,
      "learning_rate": 4.704205903107283e-05,
      "loss": 0.7554,
      "step": 389000
    },
    {
      "epoch": 3.550441638075772,
      "grad_norm": 4.209542751312256,
      "learning_rate": 4.704129863493686e-05,
      "loss": 0.7295,
      "step": 389100
    },
    {
      "epoch": 3.5513541134389373,
      "grad_norm": 4.373682975769043,
      "learning_rate": 4.7040538238800884e-05,
      "loss": 0.7158,
      "step": 389200
    },
    {
      "epoch": 3.5522665888021026,
      "grad_norm": 4.5250630378723145,
      "learning_rate": 4.703977784266492e-05,
      "loss": 0.7467,
      "step": 389300
    },
    {
      "epoch": 3.5531790641652674,
      "grad_norm": 4.299789905548096,
      "learning_rate": 4.7039017446528944e-05,
      "loss": 0.7266,
      "step": 389400
    },
    {
      "epoch": 3.5540915395284327,
      "grad_norm": 3.6154398918151855,
      "learning_rate": 4.7038257050392974e-05,
      "loss": 0.7618,
      "step": 389500
    },
    {
      "epoch": 3.555004014891598,
      "grad_norm": 3.9639790058135986,
      "learning_rate": 4.7037496654257004e-05,
      "loss": 0.7375,
      "step": 389600
    },
    {
      "epoch": 3.555916490254763,
      "grad_norm": 4.20089054107666,
      "learning_rate": 4.7036736258121034e-05,
      "loss": 0.7294,
      "step": 389700
    },
    {
      "epoch": 3.556828965617928,
      "grad_norm": 4.784577369689941,
      "learning_rate": 4.703597586198506e-05,
      "loss": 0.7026,
      "step": 389800
    },
    {
      "epoch": 3.5577414409810935,
      "grad_norm": 3.589776039123535,
      "learning_rate": 4.7035215465849094e-05,
      "loss": 0.7139,
      "step": 389900
    },
    {
      "epoch": 3.5586539163442588,
      "grad_norm": 4.625105381011963,
      "learning_rate": 4.703445506971312e-05,
      "loss": 0.7862,
      "step": 390000
    },
    {
      "epoch": 3.559566391707424,
      "grad_norm": 4.17859411239624,
      "learning_rate": 4.703369467357715e-05,
      "loss": 0.686,
      "step": 390100
    },
    {
      "epoch": 3.5604788670705894,
      "grad_norm": 4.756293773651123,
      "learning_rate": 4.703293427744118e-05,
      "loss": 0.7406,
      "step": 390200
    },
    {
      "epoch": 3.561391342433754,
      "grad_norm": 5.284499168395996,
      "learning_rate": 4.703217388130521e-05,
      "loss": 0.782,
      "step": 390300
    },
    {
      "epoch": 3.5623038177969195,
      "grad_norm": 4.072105407714844,
      "learning_rate": 4.703141348516924e-05,
      "loss": 0.7293,
      "step": 390400
    },
    {
      "epoch": 3.563216293160085,
      "grad_norm": 4.605783939361572,
      "learning_rate": 4.703065308903327e-05,
      "loss": 0.7577,
      "step": 390500
    },
    {
      "epoch": 3.5641287685232497,
      "grad_norm": 3.9240546226501465,
      "learning_rate": 4.702989269289729e-05,
      "loss": 0.7473,
      "step": 390600
    },
    {
      "epoch": 3.565041243886415,
      "grad_norm": 4.099812984466553,
      "learning_rate": 4.702913229676133e-05,
      "loss": 0.7409,
      "step": 390700
    },
    {
      "epoch": 3.5659537192495803,
      "grad_norm": 4.72015380859375,
      "learning_rate": 4.702837190062535e-05,
      "loss": 0.7146,
      "step": 390800
    },
    {
      "epoch": 3.5668661946127456,
      "grad_norm": 4.520310878753662,
      "learning_rate": 4.702761150448938e-05,
      "loss": 0.7368,
      "step": 390900
    },
    {
      "epoch": 3.567778669975911,
      "grad_norm": 4.654329776763916,
      "learning_rate": 4.702685110835341e-05,
      "loss": 0.7299,
      "step": 391000
    },
    {
      "epoch": 3.5686911453390757,
      "grad_norm": 4.785803318023682,
      "learning_rate": 4.702609071221744e-05,
      "loss": 0.7471,
      "step": 391100
    },
    {
      "epoch": 3.569603620702241,
      "grad_norm": 4.88313627243042,
      "learning_rate": 4.7025330316081465e-05,
      "loss": 0.7146,
      "step": 391200
    },
    {
      "epoch": 3.5705160960654063,
      "grad_norm": 4.609131336212158,
      "learning_rate": 4.70245699199455e-05,
      "loss": 0.732,
      "step": 391300
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 3.971775531768799,
      "learning_rate": 4.7023809523809525e-05,
      "loss": 0.7258,
      "step": 391400
    },
    {
      "epoch": 3.5723410467917365,
      "grad_norm": 3.684692621231079,
      "learning_rate": 4.7023049127673555e-05,
      "loss": 0.6992,
      "step": 391500
    },
    {
      "epoch": 3.5732535221549018,
      "grad_norm": 3.654797077178955,
      "learning_rate": 4.7022288731537585e-05,
      "loss": 0.7622,
      "step": 391600
    },
    {
      "epoch": 3.574165997518067,
      "grad_norm": 4.587968349456787,
      "learning_rate": 4.7021528335401615e-05,
      "loss": 0.728,
      "step": 391700
    },
    {
      "epoch": 3.5750784728812324,
      "grad_norm": 4.265666961669922,
      "learning_rate": 4.7020767939265645e-05,
      "loss": 0.7328,
      "step": 391800
    },
    {
      "epoch": 3.5759909482443977,
      "grad_norm": 4.086557388305664,
      "learning_rate": 4.7020007543129675e-05,
      "loss": 0.7399,
      "step": 391900
    },
    {
      "epoch": 3.5769034236075625,
      "grad_norm": 4.323111534118652,
      "learning_rate": 4.70192471469937e-05,
      "loss": 0.7261,
      "step": 392000
    },
    {
      "epoch": 3.577815898970728,
      "grad_norm": 4.124870777130127,
      "learning_rate": 4.701848675085773e-05,
      "loss": 0.7234,
      "step": 392100
    },
    {
      "epoch": 3.578728374333893,
      "grad_norm": 3.1212918758392334,
      "learning_rate": 4.701772635472176e-05,
      "loss": 0.7468,
      "step": 392200
    },
    {
      "epoch": 3.579640849697058,
      "grad_norm": 3.331052303314209,
      "learning_rate": 4.701696595858578e-05,
      "loss": 0.7184,
      "step": 392300
    },
    {
      "epoch": 3.5805533250602233,
      "grad_norm": 4.67659854888916,
      "learning_rate": 4.701620556244982e-05,
      "loss": 0.7606,
      "step": 392400
    },
    {
      "epoch": 3.5814658004233886,
      "grad_norm": 4.8425822257995605,
      "learning_rate": 4.701544516631384e-05,
      "loss": 0.735,
      "step": 392500
    },
    {
      "epoch": 3.582378275786554,
      "grad_norm": 3.43037748336792,
      "learning_rate": 4.701468477017787e-05,
      "loss": 0.7336,
      "step": 392600
    },
    {
      "epoch": 3.583290751149719,
      "grad_norm": 3.9088683128356934,
      "learning_rate": 4.70139243740419e-05,
      "loss": 0.7246,
      "step": 392700
    },
    {
      "epoch": 3.584203226512884,
      "grad_norm": 3.548752784729004,
      "learning_rate": 4.701316397790593e-05,
      "loss": 0.742,
      "step": 392800
    },
    {
      "epoch": 3.5851157018760493,
      "grad_norm": 3.685309886932373,
      "learning_rate": 4.701240358176996e-05,
      "loss": 0.7237,
      "step": 392900
    },
    {
      "epoch": 3.5860281772392146,
      "grad_norm": 3.6865627765655518,
      "learning_rate": 4.701164318563399e-05,
      "loss": 0.7054,
      "step": 393000
    },
    {
      "epoch": 3.5869406526023795,
      "grad_norm": 4.207443714141846,
      "learning_rate": 4.7010882789498016e-05,
      "loss": 0.7512,
      "step": 393100
    },
    {
      "epoch": 3.587853127965545,
      "grad_norm": 4.42310905456543,
      "learning_rate": 4.701012239336205e-05,
      "loss": 0.6942,
      "step": 393200
    },
    {
      "epoch": 3.58876560332871,
      "grad_norm": 3.5179617404937744,
      "learning_rate": 4.7009361997226076e-05,
      "loss": 0.7282,
      "step": 393300
    },
    {
      "epoch": 3.5896780786918754,
      "grad_norm": 3.965569257736206,
      "learning_rate": 4.7008601601090106e-05,
      "loss": 0.7415,
      "step": 393400
    },
    {
      "epoch": 3.5905905540550407,
      "grad_norm": 3.227510452270508,
      "learning_rate": 4.7007841204954136e-05,
      "loss": 0.7362,
      "step": 393500
    },
    {
      "epoch": 3.591503029418206,
      "grad_norm": 4.167611598968506,
      "learning_rate": 4.7007080808818166e-05,
      "loss": 0.7104,
      "step": 393600
    },
    {
      "epoch": 3.592415504781371,
      "grad_norm": 4.278213024139404,
      "learning_rate": 4.700632041268219e-05,
      "loss": 0.7628,
      "step": 393700
    },
    {
      "epoch": 3.593327980144536,
      "grad_norm": 3.669477939605713,
      "learning_rate": 4.7005560016546226e-05,
      "loss": 0.7239,
      "step": 393800
    },
    {
      "epoch": 3.5942404555077014,
      "grad_norm": 4.124695301055908,
      "learning_rate": 4.700479962041025e-05,
      "loss": 0.7183,
      "step": 393900
    },
    {
      "epoch": 3.5951529308708663,
      "grad_norm": 3.6553375720977783,
      "learning_rate": 4.700403922427428e-05,
      "loss": 0.7305,
      "step": 394000
    },
    {
      "epoch": 3.5960654062340316,
      "grad_norm": 3.9111270904541016,
      "learning_rate": 4.700327882813831e-05,
      "loss": 0.7716,
      "step": 394100
    },
    {
      "epoch": 3.596977881597197,
      "grad_norm": 3.3893654346466064,
      "learning_rate": 4.700251843200234e-05,
      "loss": 0.7732,
      "step": 394200
    },
    {
      "epoch": 3.597890356960362,
      "grad_norm": 4.213991641998291,
      "learning_rate": 4.700175803586637e-05,
      "loss": 0.7583,
      "step": 394300
    },
    {
      "epoch": 3.5988028323235275,
      "grad_norm": 4.370295524597168,
      "learning_rate": 4.70009976397304e-05,
      "loss": 0.7223,
      "step": 394400
    },
    {
      "epoch": 3.5997153076866923,
      "grad_norm": 4.001687049865723,
      "learning_rate": 4.700023724359442e-05,
      "loss": 0.7289,
      "step": 394500
    },
    {
      "epoch": 3.6006277830498576,
      "grad_norm": 3.085576295852661,
      "learning_rate": 4.699947684745846e-05,
      "loss": 0.7373,
      "step": 394600
    },
    {
      "epoch": 3.601540258413023,
      "grad_norm": 4.233882427215576,
      "learning_rate": 4.699871645132248e-05,
      "loss": 0.7215,
      "step": 394700
    },
    {
      "epoch": 3.602452733776188,
      "grad_norm": 3.8944830894470215,
      "learning_rate": 4.6997956055186506e-05,
      "loss": 0.7182,
      "step": 394800
    },
    {
      "epoch": 3.603365209139353,
      "grad_norm": 4.3554534912109375,
      "learning_rate": 4.699719565905054e-05,
      "loss": 0.7611,
      "step": 394900
    },
    {
      "epoch": 3.6042776845025184,
      "grad_norm": 4.083145618438721,
      "learning_rate": 4.699643526291457e-05,
      "loss": 0.7552,
      "step": 395000
    },
    {
      "epoch": 3.6051901598656837,
      "grad_norm": 4.423666477203369,
      "learning_rate": 4.69956748667786e-05,
      "loss": 0.7309,
      "step": 395100
    },
    {
      "epoch": 3.606102635228849,
      "grad_norm": 3.600919723510742,
      "learning_rate": 4.699491447064263e-05,
      "loss": 0.7547,
      "step": 395200
    },
    {
      "epoch": 3.607015110592014,
      "grad_norm": 3.972177267074585,
      "learning_rate": 4.699415407450666e-05,
      "loss": 0.6923,
      "step": 395300
    },
    {
      "epoch": 3.607927585955179,
      "grad_norm": 4.105869770050049,
      "learning_rate": 4.699339367837069e-05,
      "loss": 0.7383,
      "step": 395400
    },
    {
      "epoch": 3.6088400613183444,
      "grad_norm": 3.3853163719177246,
      "learning_rate": 4.699263328223472e-05,
      "loss": 0.7278,
      "step": 395500
    },
    {
      "epoch": 3.6097525366815097,
      "grad_norm": 4.644166469573975,
      "learning_rate": 4.699187288609874e-05,
      "loss": 0.6937,
      "step": 395600
    },
    {
      "epoch": 3.6106650120446746,
      "grad_norm": 3.285562038421631,
      "learning_rate": 4.699111248996278e-05,
      "loss": 0.7218,
      "step": 395700
    },
    {
      "epoch": 3.61157748740784,
      "grad_norm": 4.2214860916137695,
      "learning_rate": 4.69903520938268e-05,
      "loss": 0.7102,
      "step": 395800
    },
    {
      "epoch": 3.612489962771005,
      "grad_norm": 4.777935981750488,
      "learning_rate": 4.698959169769083e-05,
      "loss": 0.7401,
      "step": 395900
    },
    {
      "epoch": 3.6134024381341705,
      "grad_norm": 3.5511348247528076,
      "learning_rate": 4.698883130155486e-05,
      "loss": 0.7609,
      "step": 396000
    },
    {
      "epoch": 3.614314913497336,
      "grad_norm": 3.3377997875213623,
      "learning_rate": 4.698807090541889e-05,
      "loss": 0.6913,
      "step": 396100
    },
    {
      "epoch": 3.6152273888605007,
      "grad_norm": 4.010991096496582,
      "learning_rate": 4.6987310509282914e-05,
      "loss": 0.7155,
      "step": 396200
    },
    {
      "epoch": 3.616139864223666,
      "grad_norm": 4.217403411865234,
      "learning_rate": 4.698655011314695e-05,
      "loss": 0.7419,
      "step": 396300
    },
    {
      "epoch": 3.6170523395868313,
      "grad_norm": 3.675445556640625,
      "learning_rate": 4.6985789717010974e-05,
      "loss": 0.726,
      "step": 396400
    },
    {
      "epoch": 3.617964814949996,
      "grad_norm": 3.675433874130249,
      "learning_rate": 4.6985029320875004e-05,
      "loss": 0.7599,
      "step": 396500
    },
    {
      "epoch": 3.6188772903131614,
      "grad_norm": 4.990625381469727,
      "learning_rate": 4.6984268924739034e-05,
      "loss": 0.7163,
      "step": 396600
    },
    {
      "epoch": 3.6197897656763267,
      "grad_norm": 4.489563941955566,
      "learning_rate": 4.6983508528603064e-05,
      "loss": 0.7474,
      "step": 396700
    },
    {
      "epoch": 3.620702241039492,
      "grad_norm": 2.6821181774139404,
      "learning_rate": 4.6982748132467094e-05,
      "loss": 0.716,
      "step": 396800
    },
    {
      "epoch": 3.6216147164026573,
      "grad_norm": 3.404642105102539,
      "learning_rate": 4.6981987736331124e-05,
      "loss": 0.7115,
      "step": 396900
    },
    {
      "epoch": 3.622527191765822,
      "grad_norm": 4.855235576629639,
      "learning_rate": 4.698122734019515e-05,
      "loss": 0.6964,
      "step": 397000
    },
    {
      "epoch": 3.6234396671289875,
      "grad_norm": 4.301671981811523,
      "learning_rate": 4.6980466944059184e-05,
      "loss": 0.7408,
      "step": 397100
    },
    {
      "epoch": 3.6243521424921528,
      "grad_norm": 4.034816741943359,
      "learning_rate": 4.697970654792321e-05,
      "loss": 0.7034,
      "step": 397200
    },
    {
      "epoch": 3.625264617855318,
      "grad_norm": 3.9431703090667725,
      "learning_rate": 4.697894615178724e-05,
      "loss": 0.7445,
      "step": 397300
    },
    {
      "epoch": 3.626177093218483,
      "grad_norm": 3.8853657245635986,
      "learning_rate": 4.697818575565127e-05,
      "loss": 0.7681,
      "step": 397400
    },
    {
      "epoch": 3.627089568581648,
      "grad_norm": 4.001195430755615,
      "learning_rate": 4.69774253595153e-05,
      "loss": 0.7695,
      "step": 397500
    },
    {
      "epoch": 3.6280020439448135,
      "grad_norm": 4.202527046203613,
      "learning_rate": 4.697666496337932e-05,
      "loss": 0.6952,
      "step": 397600
    },
    {
      "epoch": 3.628914519307979,
      "grad_norm": 5.630483150482178,
      "learning_rate": 4.697590456724335e-05,
      "loss": 0.7563,
      "step": 397700
    },
    {
      "epoch": 3.629826994671144,
      "grad_norm": 3.8677585124969482,
      "learning_rate": 4.697514417110738e-05,
      "loss": 0.7165,
      "step": 397800
    },
    {
      "epoch": 3.630739470034309,
      "grad_norm": 3.119441509246826,
      "learning_rate": 4.697438377497141e-05,
      "loss": 0.7289,
      "step": 397900
    },
    {
      "epoch": 3.6316519453974743,
      "grad_norm": 4.957103252410889,
      "learning_rate": 4.697362337883544e-05,
      "loss": 0.7603,
      "step": 398000
    },
    {
      "epoch": 3.6325644207606396,
      "grad_norm": 4.035820007324219,
      "learning_rate": 4.6972862982699465e-05,
      "loss": 0.7028,
      "step": 398100
    },
    {
      "epoch": 3.6334768961238044,
      "grad_norm": 3.591590404510498,
      "learning_rate": 4.69721025865635e-05,
      "loss": 0.7334,
      "step": 398200
    },
    {
      "epoch": 3.6343893714869697,
      "grad_norm": 4.596391677856445,
      "learning_rate": 4.6971342190427525e-05,
      "loss": 0.7173,
      "step": 398300
    },
    {
      "epoch": 3.635301846850135,
      "grad_norm": 3.928253412246704,
      "learning_rate": 4.6970581794291555e-05,
      "loss": 0.7089,
      "step": 398400
    },
    {
      "epoch": 3.6362143222133003,
      "grad_norm": 5.124921798706055,
      "learning_rate": 4.6969821398155585e-05,
      "loss": 0.717,
      "step": 398500
    },
    {
      "epoch": 3.6371267975764656,
      "grad_norm": 4.445394992828369,
      "learning_rate": 4.6969061002019615e-05,
      "loss": 0.7069,
      "step": 398600
    },
    {
      "epoch": 3.6380392729396305,
      "grad_norm": 3.879671573638916,
      "learning_rate": 4.696830060588364e-05,
      "loss": 0.7163,
      "step": 398700
    },
    {
      "epoch": 3.6389517483027958,
      "grad_norm": 4.278584003448486,
      "learning_rate": 4.6967540209747675e-05,
      "loss": 0.7094,
      "step": 398800
    },
    {
      "epoch": 3.639864223665961,
      "grad_norm": 4.532950401306152,
      "learning_rate": 4.69667798136117e-05,
      "loss": 0.7063,
      "step": 398900
    },
    {
      "epoch": 3.6407766990291264,
      "grad_norm": 4.041388511657715,
      "learning_rate": 4.696601941747573e-05,
      "loss": 0.7338,
      "step": 399000
    },
    {
      "epoch": 3.6416891743922912,
      "grad_norm": 5.109185695648193,
      "learning_rate": 4.696525902133976e-05,
      "loss": 0.7228,
      "step": 399100
    },
    {
      "epoch": 3.6426016497554565,
      "grad_norm": 3.2310128211975098,
      "learning_rate": 4.696449862520379e-05,
      "loss": 0.7864,
      "step": 399200
    },
    {
      "epoch": 3.643514125118622,
      "grad_norm": 3.8082070350646973,
      "learning_rate": 4.696373822906782e-05,
      "loss": 0.7607,
      "step": 399300
    },
    {
      "epoch": 3.644426600481787,
      "grad_norm": 4.446349620819092,
      "learning_rate": 4.696297783293185e-05,
      "loss": 0.7176,
      "step": 399400
    },
    {
      "epoch": 3.6453390758449524,
      "grad_norm": 4.783885955810547,
      "learning_rate": 4.696221743679587e-05,
      "loss": 0.7188,
      "step": 399500
    },
    {
      "epoch": 3.6462515512081173,
      "grad_norm": 4.482202529907227,
      "learning_rate": 4.696145704065991e-05,
      "loss": 0.7465,
      "step": 399600
    },
    {
      "epoch": 3.6471640265712826,
      "grad_norm": 3.8242440223693848,
      "learning_rate": 4.696069664452393e-05,
      "loss": 0.6997,
      "step": 399700
    },
    {
      "epoch": 3.648076501934448,
      "grad_norm": 4.885406017303467,
      "learning_rate": 4.695993624838796e-05,
      "loss": 0.7229,
      "step": 399800
    },
    {
      "epoch": 3.6489889772976127,
      "grad_norm": 5.178364276885986,
      "learning_rate": 4.695917585225199e-05,
      "loss": 0.7367,
      "step": 399900
    },
    {
      "epoch": 3.649901452660778,
      "grad_norm": 2.9231019020080566,
      "learning_rate": 4.695841545611602e-05,
      "loss": 0.7118,
      "step": 400000
    },
    {
      "epoch": 3.6508139280239433,
      "grad_norm": 3.8910326957702637,
      "learning_rate": 4.6957655059980046e-05,
      "loss": 0.725,
      "step": 400100
    },
    {
      "epoch": 3.6517264033871086,
      "grad_norm": 4.1202392578125,
      "learning_rate": 4.695689466384408e-05,
      "loss": 0.7147,
      "step": 400200
    },
    {
      "epoch": 3.652638878750274,
      "grad_norm": 4.764592170715332,
      "learning_rate": 4.6956134267708106e-05,
      "loss": 0.7411,
      "step": 400300
    },
    {
      "epoch": 3.653551354113439,
      "grad_norm": 2.7939138412475586,
      "learning_rate": 4.6955373871572136e-05,
      "loss": 0.7501,
      "step": 400400
    },
    {
      "epoch": 3.654463829476604,
      "grad_norm": 2.602759838104248,
      "learning_rate": 4.6954613475436166e-05,
      "loss": 0.7704,
      "step": 400500
    },
    {
      "epoch": 3.6553763048397694,
      "grad_norm": 3.8708977699279785,
      "learning_rate": 4.695385307930019e-05,
      "loss": 0.747,
      "step": 400600
    },
    {
      "epoch": 3.6562887802029347,
      "grad_norm": 4.2401509284973145,
      "learning_rate": 4.6953092683164226e-05,
      "loss": 0.7143,
      "step": 400700
    },
    {
      "epoch": 3.6572012555660995,
      "grad_norm": 4.394524097442627,
      "learning_rate": 4.695233228702825e-05,
      "loss": 0.6774,
      "step": 400800
    },
    {
      "epoch": 3.658113730929265,
      "grad_norm": 3.9937567710876465,
      "learning_rate": 4.695157189089228e-05,
      "loss": 0.7499,
      "step": 400900
    },
    {
      "epoch": 3.65902620629243,
      "grad_norm": 4.174592971801758,
      "learning_rate": 4.695081149475631e-05,
      "loss": 0.7635,
      "step": 401000
    },
    {
      "epoch": 3.6599386816555954,
      "grad_norm": 4.760500431060791,
      "learning_rate": 4.695005109862034e-05,
      "loss": 0.721,
      "step": 401100
    },
    {
      "epoch": 3.6608511570187607,
      "grad_norm": 3.8649606704711914,
      "learning_rate": 4.694929070248437e-05,
      "loss": 0.7151,
      "step": 401200
    },
    {
      "epoch": 3.6617636323819256,
      "grad_norm": 4.270477771759033,
      "learning_rate": 4.69485303063484e-05,
      "loss": 0.7062,
      "step": 401300
    },
    {
      "epoch": 3.662676107745091,
      "grad_norm": 5.216833591461182,
      "learning_rate": 4.694776991021242e-05,
      "loss": 0.7671,
      "step": 401400
    },
    {
      "epoch": 3.663588583108256,
      "grad_norm": 4.700079917907715,
      "learning_rate": 4.694700951407645e-05,
      "loss": 0.7327,
      "step": 401500
    },
    {
      "epoch": 3.664501058471421,
      "grad_norm": 3.3872745037078857,
      "learning_rate": 4.694624911794048e-05,
      "loss": 0.7161,
      "step": 401600
    },
    {
      "epoch": 3.6654135338345863,
      "grad_norm": 4.16563081741333,
      "learning_rate": 4.694548872180451e-05,
      "loss": 0.7547,
      "step": 401700
    },
    {
      "epoch": 3.6663260091977516,
      "grad_norm": 4.51012659072876,
      "learning_rate": 4.6944728325668543e-05,
      "loss": 0.7071,
      "step": 401800
    },
    {
      "epoch": 3.667238484560917,
      "grad_norm": 4.117525100708008,
      "learning_rate": 4.6943967929532573e-05,
      "loss": 0.6775,
      "step": 401900
    },
    {
      "epoch": 3.6681509599240822,
      "grad_norm": 3.9908406734466553,
      "learning_rate": 4.69432075333966e-05,
      "loss": 0.7431,
      "step": 402000
    },
    {
      "epoch": 3.669063435287247,
      "grad_norm": 4.938055992126465,
      "learning_rate": 4.6942447137260634e-05,
      "loss": 0.7192,
      "step": 402100
    },
    {
      "epoch": 3.6699759106504124,
      "grad_norm": 3.740335464477539,
      "learning_rate": 4.694168674112466e-05,
      "loss": 0.7155,
      "step": 402200
    },
    {
      "epoch": 3.6708883860135777,
      "grad_norm": 4.737590312957764,
      "learning_rate": 4.694092634498869e-05,
      "loss": 0.7317,
      "step": 402300
    },
    {
      "epoch": 3.6718008613767426,
      "grad_norm": 4.243446350097656,
      "learning_rate": 4.694016594885272e-05,
      "loss": 0.7011,
      "step": 402400
    },
    {
      "epoch": 3.672713336739908,
      "grad_norm": 5.541990280151367,
      "learning_rate": 4.693940555271675e-05,
      "loss": 0.7271,
      "step": 402500
    },
    {
      "epoch": 3.673625812103073,
      "grad_norm": 4.3605055809021,
      "learning_rate": 4.693864515658078e-05,
      "loss": 0.728,
      "step": 402600
    },
    {
      "epoch": 3.6745382874662385,
      "grad_norm": 3.1357898712158203,
      "learning_rate": 4.693788476044481e-05,
      "loss": 0.7027,
      "step": 402700
    },
    {
      "epoch": 3.6754507628294038,
      "grad_norm": 4.649283409118652,
      "learning_rate": 4.693712436430883e-05,
      "loss": 0.7331,
      "step": 402800
    },
    {
      "epoch": 3.676363238192569,
      "grad_norm": 3.2287940979003906,
      "learning_rate": 4.693636396817286e-05,
      "loss": 0.7335,
      "step": 402900
    },
    {
      "epoch": 3.677275713555734,
      "grad_norm": 4.06889533996582,
      "learning_rate": 4.693560357203689e-05,
      "loss": 0.6883,
      "step": 403000
    },
    {
      "epoch": 3.678188188918899,
      "grad_norm": 3.7651023864746094,
      "learning_rate": 4.693484317590092e-05,
      "loss": 0.7463,
      "step": 403100
    },
    {
      "epoch": 3.6791006642820645,
      "grad_norm": 4.576114177703857,
      "learning_rate": 4.693408277976495e-05,
      "loss": 0.732,
      "step": 403200
    },
    {
      "epoch": 3.6800131396452294,
      "grad_norm": 4.115462303161621,
      "learning_rate": 4.693332238362898e-05,
      "loss": 0.7342,
      "step": 403300
    },
    {
      "epoch": 3.6809256150083947,
      "grad_norm": 3.7766942977905273,
      "learning_rate": 4.6932561987493004e-05,
      "loss": 0.7439,
      "step": 403400
    },
    {
      "epoch": 3.68183809037156,
      "grad_norm": 4.3301286697387695,
      "learning_rate": 4.6931801591357034e-05,
      "loss": 0.7503,
      "step": 403500
    },
    {
      "epoch": 3.6827505657347253,
      "grad_norm": 3.9166173934936523,
      "learning_rate": 4.6931041195221064e-05,
      "loss": 0.8149,
      "step": 403600
    },
    {
      "epoch": 3.6836630410978906,
      "grad_norm": 4.298655033111572,
      "learning_rate": 4.6930280799085094e-05,
      "loss": 0.7314,
      "step": 403700
    },
    {
      "epoch": 3.6845755164610554,
      "grad_norm": 4.740556240081787,
      "learning_rate": 4.6929520402949124e-05,
      "loss": 0.7411,
      "step": 403800
    },
    {
      "epoch": 3.6854879918242207,
      "grad_norm": 4.141775608062744,
      "learning_rate": 4.692876000681315e-05,
      "loss": 0.7622,
      "step": 403900
    },
    {
      "epoch": 3.686400467187386,
      "grad_norm": 3.9710123538970947,
      "learning_rate": 4.6927999610677185e-05,
      "loss": 0.7513,
      "step": 404000
    },
    {
      "epoch": 3.687312942550551,
      "grad_norm": 4.326180934906006,
      "learning_rate": 4.692723921454121e-05,
      "loss": 0.7307,
      "step": 404100
    },
    {
      "epoch": 3.688225417913716,
      "grad_norm": 3.9501233100891113,
      "learning_rate": 4.692647881840524e-05,
      "loss": 0.7328,
      "step": 404200
    },
    {
      "epoch": 3.6891378932768815,
      "grad_norm": 3.191283702850342,
      "learning_rate": 4.692571842226927e-05,
      "loss": 0.7365,
      "step": 404300
    },
    {
      "epoch": 3.6900503686400468,
      "grad_norm": 4.322779655456543,
      "learning_rate": 4.69249580261333e-05,
      "loss": 0.7182,
      "step": 404400
    },
    {
      "epoch": 3.690962844003212,
      "grad_norm": 4.266713619232178,
      "learning_rate": 4.692419762999732e-05,
      "loss": 0.7494,
      "step": 404500
    },
    {
      "epoch": 3.6918753193663774,
      "grad_norm": 4.305802345275879,
      "learning_rate": 4.692343723386136e-05,
      "loss": 0.7533,
      "step": 404600
    },
    {
      "epoch": 3.692787794729542,
      "grad_norm": 4.277370929718018,
      "learning_rate": 4.692267683772538e-05,
      "loss": 0.7433,
      "step": 404700
    },
    {
      "epoch": 3.6937002700927075,
      "grad_norm": 4.534966468811035,
      "learning_rate": 4.692191644158941e-05,
      "loss": 0.7185,
      "step": 404800
    },
    {
      "epoch": 3.694612745455873,
      "grad_norm": 4.366536617279053,
      "learning_rate": 4.692115604545344e-05,
      "loss": 0.7293,
      "step": 404900
    },
    {
      "epoch": 3.6955252208190377,
      "grad_norm": 5.121294021606445,
      "learning_rate": 4.692039564931747e-05,
      "loss": 0.7397,
      "step": 405000
    },
    {
      "epoch": 3.696437696182203,
      "grad_norm": 4.173304557800293,
      "learning_rate": 4.69196352531815e-05,
      "loss": 0.7152,
      "step": 405100
    },
    {
      "epoch": 3.6973501715453683,
      "grad_norm": 4.280207633972168,
      "learning_rate": 4.691887485704553e-05,
      "loss": 0.7131,
      "step": 405200
    },
    {
      "epoch": 3.6982626469085336,
      "grad_norm": 4.150906085968018,
      "learning_rate": 4.6918114460909555e-05,
      "loss": 0.6801,
      "step": 405300
    },
    {
      "epoch": 3.699175122271699,
      "grad_norm": 3.54752779006958,
      "learning_rate": 4.691735406477359e-05,
      "loss": 0.7287,
      "step": 405400
    },
    {
      "epoch": 3.7000875976348637,
      "grad_norm": 4.6575822830200195,
      "learning_rate": 4.6916593668637615e-05,
      "loss": 0.701,
      "step": 405500
    },
    {
      "epoch": 3.701000072998029,
      "grad_norm": 3.829625129699707,
      "learning_rate": 4.6915833272501645e-05,
      "loss": 0.6966,
      "step": 405600
    },
    {
      "epoch": 3.7019125483611943,
      "grad_norm": 4.263952255249023,
      "learning_rate": 4.6915072876365675e-05,
      "loss": 0.7804,
      "step": 405700
    },
    {
      "epoch": 3.702825023724359,
      "grad_norm": 4.568957805633545,
      "learning_rate": 4.6914312480229705e-05,
      "loss": 0.6955,
      "step": 405800
    },
    {
      "epoch": 3.7037374990875245,
      "grad_norm": 4.375329494476318,
      "learning_rate": 4.691355208409373e-05,
      "loss": 0.7177,
      "step": 405900
    },
    {
      "epoch": 3.7046499744506898,
      "grad_norm": 3.953568458557129,
      "learning_rate": 4.6912791687957766e-05,
      "loss": 0.7299,
      "step": 406000
    },
    {
      "epoch": 3.705562449813855,
      "grad_norm": 4.762763500213623,
      "learning_rate": 4.691203129182179e-05,
      "loss": 0.7205,
      "step": 406100
    },
    {
      "epoch": 3.7064749251770204,
      "grad_norm": 4.047440052032471,
      "learning_rate": 4.691127089568582e-05,
      "loss": 0.7283,
      "step": 406200
    },
    {
      "epoch": 3.7073874005401857,
      "grad_norm": 3.9221413135528564,
      "learning_rate": 4.691051049954985e-05,
      "loss": 0.7373,
      "step": 406300
    },
    {
      "epoch": 3.7082998759033505,
      "grad_norm": 4.766820430755615,
      "learning_rate": 4.690975010341387e-05,
      "loss": 0.7195,
      "step": 406400
    },
    {
      "epoch": 3.709212351266516,
      "grad_norm": 4.0204010009765625,
      "learning_rate": 4.690898970727791e-05,
      "loss": 0.7342,
      "step": 406500
    },
    {
      "epoch": 3.710124826629681,
      "grad_norm": 4.392451763153076,
      "learning_rate": 4.690822931114193e-05,
      "loss": 0.7036,
      "step": 406600
    },
    {
      "epoch": 3.711037301992846,
      "grad_norm": 4.03141450881958,
      "learning_rate": 4.690746891500596e-05,
      "loss": 0.7389,
      "step": 406700
    },
    {
      "epoch": 3.7119497773560113,
      "grad_norm": 2.522848606109619,
      "learning_rate": 4.690670851886999e-05,
      "loss": 0.7169,
      "step": 406800
    },
    {
      "epoch": 3.7128622527191766,
      "grad_norm": 3.6669018268585205,
      "learning_rate": 4.690594812273402e-05,
      "loss": 0.7323,
      "step": 406900
    },
    {
      "epoch": 3.713774728082342,
      "grad_norm": 4.4167304039001465,
      "learning_rate": 4.6905187726598046e-05,
      "loss": 0.7244,
      "step": 407000
    },
    {
      "epoch": 3.714687203445507,
      "grad_norm": 2.9239892959594727,
      "learning_rate": 4.690442733046208e-05,
      "loss": 0.7456,
      "step": 407100
    },
    {
      "epoch": 3.715599678808672,
      "grad_norm": 3.7404255867004395,
      "learning_rate": 4.6903666934326106e-05,
      "loss": 0.7332,
      "step": 407200
    },
    {
      "epoch": 3.7165121541718373,
      "grad_norm": 3.019045352935791,
      "learning_rate": 4.6902906538190136e-05,
      "loss": 0.6795,
      "step": 407300
    },
    {
      "epoch": 3.7174246295350026,
      "grad_norm": 3.512378215789795,
      "learning_rate": 4.6902146142054166e-05,
      "loss": 0.7277,
      "step": 407400
    },
    {
      "epoch": 3.7183371048981675,
      "grad_norm": 4.565727710723877,
      "learning_rate": 4.6901385745918196e-05,
      "loss": 0.7697,
      "step": 407500
    },
    {
      "epoch": 3.719249580261333,
      "grad_norm": 4.012308120727539,
      "learning_rate": 4.6900625349782226e-05,
      "loss": 0.6853,
      "step": 407600
    },
    {
      "epoch": 3.720162055624498,
      "grad_norm": 4.768912315368652,
      "learning_rate": 4.6899864953646256e-05,
      "loss": 0.7602,
      "step": 407700
    },
    {
      "epoch": 3.7210745309876634,
      "grad_norm": 4.485682964324951,
      "learning_rate": 4.689910455751028e-05,
      "loss": 0.8002,
      "step": 407800
    },
    {
      "epoch": 3.7219870063508287,
      "grad_norm": 3.315446138381958,
      "learning_rate": 4.6898344161374317e-05,
      "loss": 0.7155,
      "step": 407900
    },
    {
      "epoch": 3.722899481713994,
      "grad_norm": 5.435439586639404,
      "learning_rate": 4.689758376523834e-05,
      "loss": 0.7639,
      "step": 408000
    },
    {
      "epoch": 3.723811957077159,
      "grad_norm": 4.592061519622803,
      "learning_rate": 4.689682336910237e-05,
      "loss": 0.744,
      "step": 408100
    },
    {
      "epoch": 3.724724432440324,
      "grad_norm": 4.673191547393799,
      "learning_rate": 4.68960629729664e-05,
      "loss": 0.6824,
      "step": 408200
    },
    {
      "epoch": 3.7256369078034894,
      "grad_norm": 3.771062135696411,
      "learning_rate": 4.689530257683043e-05,
      "loss": 0.7172,
      "step": 408300
    },
    {
      "epoch": 3.7265493831666543,
      "grad_norm": 3.3640472888946533,
      "learning_rate": 4.689454218069445e-05,
      "loss": 0.7096,
      "step": 408400
    },
    {
      "epoch": 3.7274618585298196,
      "grad_norm": 3.8883488178253174,
      "learning_rate": 4.689378178455849e-05,
      "loss": 0.7524,
      "step": 408500
    },
    {
      "epoch": 3.728374333892985,
      "grad_norm": 4.429993629455566,
      "learning_rate": 4.6893021388422513e-05,
      "loss": 0.7726,
      "step": 408600
    },
    {
      "epoch": 3.72928680925615,
      "grad_norm": 3.763939380645752,
      "learning_rate": 4.6892260992286544e-05,
      "loss": 0.711,
      "step": 408700
    },
    {
      "epoch": 3.7301992846193155,
      "grad_norm": 3.913940906524658,
      "learning_rate": 4.6891500596150574e-05,
      "loss": 0.7957,
      "step": 408800
    },
    {
      "epoch": 3.7311117599824803,
      "grad_norm": 4.642518520355225,
      "learning_rate": 4.6890740200014604e-05,
      "loss": 0.7323,
      "step": 408900
    },
    {
      "epoch": 3.7320242353456456,
      "grad_norm": 5.013368129730225,
      "learning_rate": 4.6889979803878634e-05,
      "loss": 0.7336,
      "step": 409000
    },
    {
      "epoch": 3.732936710708811,
      "grad_norm": 3.9440016746520996,
      "learning_rate": 4.688921940774266e-05,
      "loss": 0.7294,
      "step": 409100
    },
    {
      "epoch": 3.733849186071976,
      "grad_norm": 3.8278391361236572,
      "learning_rate": 4.688845901160669e-05,
      "loss": 0.7291,
      "step": 409200
    },
    {
      "epoch": 3.734761661435141,
      "grad_norm": 4.628815650939941,
      "learning_rate": 4.688769861547072e-05,
      "loss": 0.7195,
      "step": 409300
    },
    {
      "epoch": 3.7356741367983064,
      "grad_norm": 4.491879463195801,
      "learning_rate": 4.688693821933475e-05,
      "loss": 0.7282,
      "step": 409400
    },
    {
      "epoch": 3.7365866121614717,
      "grad_norm": 3.741939067840576,
      "learning_rate": 4.688617782319877e-05,
      "loss": 0.7098,
      "step": 409500
    },
    {
      "epoch": 3.737499087524637,
      "grad_norm": 4.025262355804443,
      "learning_rate": 4.688541742706281e-05,
      "loss": 0.7324,
      "step": 409600
    },
    {
      "epoch": 3.7384115628878023,
      "grad_norm": 3.1719918251037598,
      "learning_rate": 4.688465703092683e-05,
      "loss": 0.7287,
      "step": 409700
    },
    {
      "epoch": 3.739324038250967,
      "grad_norm": 4.428885459899902,
      "learning_rate": 4.688389663479086e-05,
      "loss": 0.73,
      "step": 409800
    },
    {
      "epoch": 3.7402365136141325,
      "grad_norm": 4.436741352081299,
      "learning_rate": 4.688313623865489e-05,
      "loss": 0.732,
      "step": 409900
    },
    {
      "epoch": 3.7411489889772978,
      "grad_norm": 4.354902267456055,
      "learning_rate": 4.688237584251892e-05,
      "loss": 0.73,
      "step": 410000
    },
    {
      "epoch": 3.7420614643404626,
      "grad_norm": 4.212881088256836,
      "learning_rate": 4.688161544638295e-05,
      "loss": 0.6958,
      "step": 410100
    },
    {
      "epoch": 3.742973939703628,
      "grad_norm": 4.124899387359619,
      "learning_rate": 4.688085505024698e-05,
      "loss": 0.6778,
      "step": 410200
    },
    {
      "epoch": 3.743886415066793,
      "grad_norm": 5.337307453155518,
      "learning_rate": 4.6880094654111004e-05,
      "loss": 0.7177,
      "step": 410300
    },
    {
      "epoch": 3.7447988904299585,
      "grad_norm": 3.842911720275879,
      "learning_rate": 4.687933425797504e-05,
      "loss": 0.7236,
      "step": 410400
    },
    {
      "epoch": 3.745711365793124,
      "grad_norm": 3.8623597621917725,
      "learning_rate": 4.6878573861839064e-05,
      "loss": 0.7666,
      "step": 410500
    },
    {
      "epoch": 3.7466238411562887,
      "grad_norm": 3.567962884902954,
      "learning_rate": 4.6877813465703094e-05,
      "loss": 0.7659,
      "step": 410600
    },
    {
      "epoch": 3.747536316519454,
      "grad_norm": 5.177762031555176,
      "learning_rate": 4.6877053069567125e-05,
      "loss": 0.7136,
      "step": 410700
    },
    {
      "epoch": 3.7484487918826193,
      "grad_norm": 4.00266695022583,
      "learning_rate": 4.6876292673431155e-05,
      "loss": 0.7231,
      "step": 410800
    },
    {
      "epoch": 3.749361267245784,
      "grad_norm": 4.449497699737549,
      "learning_rate": 4.687553227729518e-05,
      "loss": 0.7385,
      "step": 410900
    },
    {
      "epoch": 3.7502737426089494,
      "grad_norm": 4.865801811218262,
      "learning_rate": 4.6874771881159215e-05,
      "loss": 0.692,
      "step": 411000
    },
    {
      "epoch": 3.7511862179721147,
      "grad_norm": 4.736351013183594,
      "learning_rate": 4.687401148502324e-05,
      "loss": 0.7239,
      "step": 411100
    },
    {
      "epoch": 3.75209869333528,
      "grad_norm": 4.471449375152588,
      "learning_rate": 4.687325108888727e-05,
      "loss": 0.7105,
      "step": 411200
    },
    {
      "epoch": 3.7530111686984453,
      "grad_norm": 4.3657546043396,
      "learning_rate": 4.68724906927513e-05,
      "loss": 0.6718,
      "step": 411300
    },
    {
      "epoch": 3.7539236440616106,
      "grad_norm": 4.693267822265625,
      "learning_rate": 4.687173029661533e-05,
      "loss": 0.6909,
      "step": 411400
    },
    {
      "epoch": 3.7548361194247755,
      "grad_norm": 4.75685977935791,
      "learning_rate": 4.687096990047936e-05,
      "loss": 0.7549,
      "step": 411500
    },
    {
      "epoch": 3.7557485947879408,
      "grad_norm": 3.4522595405578613,
      "learning_rate": 4.687020950434339e-05,
      "loss": 0.7288,
      "step": 411600
    },
    {
      "epoch": 3.756661070151106,
      "grad_norm": 4.459201812744141,
      "learning_rate": 4.686944910820741e-05,
      "loss": 0.7198,
      "step": 411700
    },
    {
      "epoch": 3.757573545514271,
      "grad_norm": 2.9051060676574707,
      "learning_rate": 4.686868871207145e-05,
      "loss": 0.7411,
      "step": 411800
    },
    {
      "epoch": 3.758486020877436,
      "grad_norm": 3.9975602626800537,
      "learning_rate": 4.686792831593547e-05,
      "loss": 0.7541,
      "step": 411900
    },
    {
      "epoch": 3.7593984962406015,
      "grad_norm": 4.682058334350586,
      "learning_rate": 4.6867167919799495e-05,
      "loss": 0.7565,
      "step": 412000
    },
    {
      "epoch": 3.760310971603767,
      "grad_norm": 4.750612735748291,
      "learning_rate": 4.686640752366353e-05,
      "loss": 0.7049,
      "step": 412100
    },
    {
      "epoch": 3.761223446966932,
      "grad_norm": 3.556798219680786,
      "learning_rate": 4.6865647127527555e-05,
      "loss": 0.7662,
      "step": 412200
    },
    {
      "epoch": 3.762135922330097,
      "grad_norm": 4.467051029205322,
      "learning_rate": 4.6864886731391585e-05,
      "loss": 0.7621,
      "step": 412300
    },
    {
      "epoch": 3.7630483976932623,
      "grad_norm": 3.3815722465515137,
      "learning_rate": 4.6864126335255615e-05,
      "loss": 0.7088,
      "step": 412400
    },
    {
      "epoch": 3.7639608730564276,
      "grad_norm": 3.584879159927368,
      "learning_rate": 4.6863365939119645e-05,
      "loss": 0.7667,
      "step": 412500
    },
    {
      "epoch": 3.7648733484195924,
      "grad_norm": 3.3691046237945557,
      "learning_rate": 4.6862605542983675e-05,
      "loss": 0.7345,
      "step": 412600
    },
    {
      "epoch": 3.7657858237827577,
      "grad_norm": 4.396666526794434,
      "learning_rate": 4.6861845146847706e-05,
      "loss": 0.7075,
      "step": 412700
    },
    {
      "epoch": 3.766698299145923,
      "grad_norm": 4.540374755859375,
      "learning_rate": 4.686108475071173e-05,
      "loss": 0.717,
      "step": 412800
    },
    {
      "epoch": 3.7676107745090883,
      "grad_norm": 3.721231460571289,
      "learning_rate": 4.6860324354575766e-05,
      "loss": 0.7202,
      "step": 412900
    },
    {
      "epoch": 3.7685232498722536,
      "grad_norm": 3.6482133865356445,
      "learning_rate": 4.685956395843979e-05,
      "loss": 0.7864,
      "step": 413000
    },
    {
      "epoch": 3.7694357252354185,
      "grad_norm": 4.468452453613281,
      "learning_rate": 4.685880356230382e-05,
      "loss": 0.7367,
      "step": 413100
    },
    {
      "epoch": 3.7703482005985838,
      "grad_norm": 4.079746246337891,
      "learning_rate": 4.685804316616785e-05,
      "loss": 0.7348,
      "step": 413200
    },
    {
      "epoch": 3.771260675961749,
      "grad_norm": 4.28073787689209,
      "learning_rate": 4.685728277003188e-05,
      "loss": 0.7159,
      "step": 413300
    },
    {
      "epoch": 3.7721731513249144,
      "grad_norm": 3.950845241546631,
      "learning_rate": 4.68565223738959e-05,
      "loss": 0.704,
      "step": 413400
    },
    {
      "epoch": 3.7730856266880792,
      "grad_norm": 4.535122871398926,
      "learning_rate": 4.685576197775994e-05,
      "loss": 0.6707,
      "step": 413500
    },
    {
      "epoch": 3.7739981020512445,
      "grad_norm": 3.948261260986328,
      "learning_rate": 4.685500158162396e-05,
      "loss": 0.7424,
      "step": 413600
    },
    {
      "epoch": 3.77491057741441,
      "grad_norm": 4.484622001647949,
      "learning_rate": 4.685424118548799e-05,
      "loss": 0.715,
      "step": 413700
    },
    {
      "epoch": 3.775823052777575,
      "grad_norm": 4.021106719970703,
      "learning_rate": 4.685348078935202e-05,
      "loss": 0.7377,
      "step": 413800
    },
    {
      "epoch": 3.7767355281407404,
      "grad_norm": 4.3570170402526855,
      "learning_rate": 4.685272039321605e-05,
      "loss": 0.72,
      "step": 413900
    },
    {
      "epoch": 3.7776480035039053,
      "grad_norm": 4.391208648681641,
      "learning_rate": 4.685195999708008e-05,
      "loss": 0.7067,
      "step": 414000
    },
    {
      "epoch": 3.7785604788670706,
      "grad_norm": 4.157642364501953,
      "learning_rate": 4.685119960094411e-05,
      "loss": 0.7371,
      "step": 414100
    },
    {
      "epoch": 3.779472954230236,
      "grad_norm": 4.925450801849365,
      "learning_rate": 4.6850439204808136e-05,
      "loss": 0.6662,
      "step": 414200
    },
    {
      "epoch": 3.7803854295934007,
      "grad_norm": 4.6176557540893555,
      "learning_rate": 4.684967880867217e-05,
      "loss": 0.7637,
      "step": 414300
    },
    {
      "epoch": 3.781297904956566,
      "grad_norm": 4.204861640930176,
      "learning_rate": 4.6848918412536196e-05,
      "loss": 0.7207,
      "step": 414400
    },
    {
      "epoch": 3.7822103803197313,
      "grad_norm": 4.499551296234131,
      "learning_rate": 4.6848158016400226e-05,
      "loss": 0.7237,
      "step": 414500
    },
    {
      "epoch": 3.7831228556828966,
      "grad_norm": 3.2483091354370117,
      "learning_rate": 4.6847397620264257e-05,
      "loss": 0.7419,
      "step": 414600
    },
    {
      "epoch": 3.784035331046062,
      "grad_norm": 4.136466026306152,
      "learning_rate": 4.684663722412828e-05,
      "loss": 0.7127,
      "step": 414700
    },
    {
      "epoch": 3.784947806409227,
      "grad_norm": 4.493800163269043,
      "learning_rate": 4.684587682799232e-05,
      "loss": 0.7144,
      "step": 414800
    },
    {
      "epoch": 3.785860281772392,
      "grad_norm": 4.317306041717529,
      "learning_rate": 4.684511643185634e-05,
      "loss": 0.733,
      "step": 414900
    },
    {
      "epoch": 3.7867727571355574,
      "grad_norm": 4.553438663482666,
      "learning_rate": 4.684435603572037e-05,
      "loss": 0.7081,
      "step": 415000
    },
    {
      "epoch": 3.7876852324987227,
      "grad_norm": 3.9926247596740723,
      "learning_rate": 4.68435956395844e-05,
      "loss": 0.7316,
      "step": 415100
    },
    {
      "epoch": 3.7885977078618875,
      "grad_norm": 3.0002973079681396,
      "learning_rate": 4.684283524344843e-05,
      "loss": 0.7267,
      "step": 415200
    },
    {
      "epoch": 3.789510183225053,
      "grad_norm": 4.138801574707031,
      "learning_rate": 4.6842074847312453e-05,
      "loss": 0.7051,
      "step": 415300
    },
    {
      "epoch": 3.790422658588218,
      "grad_norm": 3.8082001209259033,
      "learning_rate": 4.684131445117649e-05,
      "loss": 0.7253,
      "step": 415400
    },
    {
      "epoch": 3.7913351339513834,
      "grad_norm": 4.112533092498779,
      "learning_rate": 4.6840554055040514e-05,
      "loss": 0.7518,
      "step": 415500
    },
    {
      "epoch": 3.7922476093145487,
      "grad_norm": 4.338278293609619,
      "learning_rate": 4.6839793658904544e-05,
      "loss": 0.7295,
      "step": 415600
    },
    {
      "epoch": 3.7931600846777136,
      "grad_norm": 3.9437415599823,
      "learning_rate": 4.6839033262768574e-05,
      "loss": 0.7432,
      "step": 415700
    },
    {
      "epoch": 3.794072560040879,
      "grad_norm": 4.40259313583374,
      "learning_rate": 4.6838272866632604e-05,
      "loss": 0.7362,
      "step": 415800
    },
    {
      "epoch": 3.794985035404044,
      "grad_norm": 4.146562099456787,
      "learning_rate": 4.6837512470496634e-05,
      "loss": 0.7103,
      "step": 415900
    },
    {
      "epoch": 3.795897510767209,
      "grad_norm": 4.003937721252441,
      "learning_rate": 4.6836752074360664e-05,
      "loss": 0.7746,
      "step": 416000
    },
    {
      "epoch": 3.7968099861303743,
      "grad_norm": 4.345592975616455,
      "learning_rate": 4.683599167822469e-05,
      "loss": 0.7325,
      "step": 416100
    },
    {
      "epoch": 3.7977224614935396,
      "grad_norm": 4.6925811767578125,
      "learning_rate": 4.6835231282088724e-05,
      "loss": 0.7266,
      "step": 416200
    },
    {
      "epoch": 3.798634936856705,
      "grad_norm": 4.042211055755615,
      "learning_rate": 4.683447088595275e-05,
      "loss": 0.7133,
      "step": 416300
    },
    {
      "epoch": 3.7995474122198702,
      "grad_norm": 4.380801200866699,
      "learning_rate": 4.683371048981678e-05,
      "loss": 0.7283,
      "step": 416400
    },
    {
      "epoch": 3.800459887583035,
      "grad_norm": 4.483060836791992,
      "learning_rate": 4.683295009368081e-05,
      "loss": 0.7061,
      "step": 416500
    },
    {
      "epoch": 3.8013723629462004,
      "grad_norm": 3.849975347518921,
      "learning_rate": 4.683218969754484e-05,
      "loss": 0.7192,
      "step": 416600
    },
    {
      "epoch": 3.8022848383093657,
      "grad_norm": 4.806146621704102,
      "learning_rate": 4.683142930140886e-05,
      "loss": 0.7264,
      "step": 416700
    },
    {
      "epoch": 3.803197313672531,
      "grad_norm": 5.206072807312012,
      "learning_rate": 4.68306689052729e-05,
      "loss": 0.7116,
      "step": 416800
    },
    {
      "epoch": 3.804109789035696,
      "grad_norm": 4.742788314819336,
      "learning_rate": 4.682990850913692e-05,
      "loss": 0.6967,
      "step": 416900
    },
    {
      "epoch": 3.805022264398861,
      "grad_norm": 4.006548881530762,
      "learning_rate": 4.682914811300095e-05,
      "loss": 0.703,
      "step": 417000
    },
    {
      "epoch": 3.8059347397620265,
      "grad_norm": 3.9515912532806396,
      "learning_rate": 4.682838771686498e-05,
      "loss": 0.7205,
      "step": 417100
    },
    {
      "epoch": 3.8068472151251918,
      "grad_norm": 4.602258205413818,
      "learning_rate": 4.682762732072901e-05,
      "loss": 0.7286,
      "step": 417200
    },
    {
      "epoch": 3.807759690488357,
      "grad_norm": 4.126893520355225,
      "learning_rate": 4.682686692459304e-05,
      "loss": 0.744,
      "step": 417300
    },
    {
      "epoch": 3.808672165851522,
      "grad_norm": 4.291680812835693,
      "learning_rate": 4.682610652845707e-05,
      "loss": 0.7275,
      "step": 417400
    },
    {
      "epoch": 3.809584641214687,
      "grad_norm": 3.931988477706909,
      "learning_rate": 4.6825346132321095e-05,
      "loss": 0.7281,
      "step": 417500
    },
    {
      "epoch": 3.8104971165778525,
      "grad_norm": 4.574343204498291,
      "learning_rate": 4.6824585736185125e-05,
      "loss": 0.7491,
      "step": 417600
    },
    {
      "epoch": 3.8114095919410174,
      "grad_norm": 2.7337095737457275,
      "learning_rate": 4.6823825340049155e-05,
      "loss": 0.763,
      "step": 417700
    },
    {
      "epoch": 3.8123220673041827,
      "grad_norm": 3.8994784355163574,
      "learning_rate": 4.682306494391318e-05,
      "loss": 0.7231,
      "step": 417800
    },
    {
      "epoch": 3.813234542667348,
      "grad_norm": 4.843016147613525,
      "learning_rate": 4.6822304547777215e-05,
      "loss": 0.7201,
      "step": 417900
    },
    {
      "epoch": 3.8141470180305133,
      "grad_norm": 4.722092151641846,
      "learning_rate": 4.682154415164124e-05,
      "loss": 0.7451,
      "step": 418000
    },
    {
      "epoch": 3.8150594933936786,
      "grad_norm": 3.956960916519165,
      "learning_rate": 4.682078375550527e-05,
      "loss": 0.7145,
      "step": 418100
    },
    {
      "epoch": 3.8159719687568434,
      "grad_norm": 4.573947429656982,
      "learning_rate": 4.68200233593693e-05,
      "loss": 0.7139,
      "step": 418200
    },
    {
      "epoch": 3.8168844441200087,
      "grad_norm": 4.539123058319092,
      "learning_rate": 4.681926296323333e-05,
      "loss": 0.7145,
      "step": 418300
    },
    {
      "epoch": 3.817796919483174,
      "grad_norm": 2.5424857139587402,
      "learning_rate": 4.681850256709736e-05,
      "loss": 0.7587,
      "step": 418400
    },
    {
      "epoch": 3.8187093948463393,
      "grad_norm": 4.101046085357666,
      "learning_rate": 4.681774217096139e-05,
      "loss": 0.7371,
      "step": 418500
    },
    {
      "epoch": 3.819621870209504,
      "grad_norm": 3.0498008728027344,
      "learning_rate": 4.681698177482541e-05,
      "loss": 0.708,
      "step": 418600
    },
    {
      "epoch": 3.8205343455726695,
      "grad_norm": 3.6773815155029297,
      "learning_rate": 4.681622137868945e-05,
      "loss": 0.7119,
      "step": 418700
    },
    {
      "epoch": 3.8214468209358348,
      "grad_norm": 3.964873790740967,
      "learning_rate": 4.681546098255347e-05,
      "loss": 0.6925,
      "step": 418800
    },
    {
      "epoch": 3.822359296299,
      "grad_norm": 4.172467231750488,
      "learning_rate": 4.68147005864175e-05,
      "loss": 0.7519,
      "step": 418900
    },
    {
      "epoch": 3.8232717716621654,
      "grad_norm": 4.439722537994385,
      "learning_rate": 4.681394019028153e-05,
      "loss": 0.7301,
      "step": 419000
    },
    {
      "epoch": 3.82418424702533,
      "grad_norm": 4.719567775726318,
      "learning_rate": 4.681317979414556e-05,
      "loss": 0.7448,
      "step": 419100
    },
    {
      "epoch": 3.8250967223884955,
      "grad_norm": 4.714568614959717,
      "learning_rate": 4.6812419398009585e-05,
      "loss": 0.7132,
      "step": 419200
    },
    {
      "epoch": 3.826009197751661,
      "grad_norm": 3.6258926391601562,
      "learning_rate": 4.681165900187362e-05,
      "loss": 0.7573,
      "step": 419300
    },
    {
      "epoch": 3.8269216731148257,
      "grad_norm": 3.8651039600372314,
      "learning_rate": 4.6810898605737646e-05,
      "loss": 0.7329,
      "step": 419400
    },
    {
      "epoch": 3.827834148477991,
      "grad_norm": 4.121728897094727,
      "learning_rate": 4.6810138209601676e-05,
      "loss": 0.7202,
      "step": 419500
    },
    {
      "epoch": 3.8287466238411563,
      "grad_norm": 3.9647464752197266,
      "learning_rate": 4.6809377813465706e-05,
      "loss": 0.6924,
      "step": 419600
    },
    {
      "epoch": 3.8296590992043216,
      "grad_norm": 3.467784881591797,
      "learning_rate": 4.6808617417329736e-05,
      "loss": 0.6718,
      "step": 419700
    },
    {
      "epoch": 3.830571574567487,
      "grad_norm": 4.126671314239502,
      "learning_rate": 4.6807857021193766e-05,
      "loss": 0.7281,
      "step": 419800
    },
    {
      "epoch": 3.8314840499306517,
      "grad_norm": 4.7419586181640625,
      "learning_rate": 4.6807096625057796e-05,
      "loss": 0.7647,
      "step": 419900
    },
    {
      "epoch": 3.832396525293817,
      "grad_norm": 3.7579972743988037,
      "learning_rate": 4.680633622892182e-05,
      "loss": 0.7546,
      "step": 420000
    },
    {
      "epoch": 3.8333090006569823,
      "grad_norm": 4.352912902832031,
      "learning_rate": 4.6805575832785856e-05,
      "loss": 0.7209,
      "step": 420100
    },
    {
      "epoch": 3.834221476020147,
      "grad_norm": 3.948391914367676,
      "learning_rate": 4.680481543664988e-05,
      "loss": 0.7215,
      "step": 420200
    },
    {
      "epoch": 3.8351339513833125,
      "grad_norm": 4.025669097900391,
      "learning_rate": 4.680405504051391e-05,
      "loss": 0.7157,
      "step": 420300
    },
    {
      "epoch": 3.8360464267464778,
      "grad_norm": 3.444274663925171,
      "learning_rate": 4.680329464437794e-05,
      "loss": 0.7134,
      "step": 420400
    },
    {
      "epoch": 3.836958902109643,
      "grad_norm": 4.608564853668213,
      "learning_rate": 4.680253424824196e-05,
      "loss": 0.7161,
      "step": 420500
    },
    {
      "epoch": 3.8378713774728084,
      "grad_norm": 4.096133708953857,
      "learning_rate": 4.680177385210599e-05,
      "loss": 0.7382,
      "step": 420600
    },
    {
      "epoch": 3.8387838528359737,
      "grad_norm": 4.4805073738098145,
      "learning_rate": 4.680101345597002e-05,
      "loss": 0.7198,
      "step": 420700
    },
    {
      "epoch": 3.8396963281991385,
      "grad_norm": 4.713702201843262,
      "learning_rate": 4.680025305983405e-05,
      "loss": 0.7535,
      "step": 420800
    },
    {
      "epoch": 3.840608803562304,
      "grad_norm": 4.074568748474121,
      "learning_rate": 4.679949266369808e-05,
      "loss": 0.7355,
      "step": 420900
    },
    {
      "epoch": 3.841521278925469,
      "grad_norm": 4.200648307800293,
      "learning_rate": 4.679873226756211e-05,
      "loss": 0.7839,
      "step": 421000
    },
    {
      "epoch": 3.842433754288634,
      "grad_norm": 5.092453479766846,
      "learning_rate": 4.6797971871426136e-05,
      "loss": 0.749,
      "step": 421100
    },
    {
      "epoch": 3.8433462296517993,
      "grad_norm": 4.3971991539001465,
      "learning_rate": 4.679721147529017e-05,
      "loss": 0.7356,
      "step": 421200
    },
    {
      "epoch": 3.8442587050149646,
      "grad_norm": 4.701125621795654,
      "learning_rate": 4.6796451079154196e-05,
      "loss": 0.7443,
      "step": 421300
    },
    {
      "epoch": 3.84517118037813,
      "grad_norm": 4.459370136260986,
      "learning_rate": 4.6795690683018227e-05,
      "loss": 0.7543,
      "step": 421400
    },
    {
      "epoch": 3.846083655741295,
      "grad_norm": 4.6168928146362305,
      "learning_rate": 4.6794930286882257e-05,
      "loss": 0.7531,
      "step": 421500
    },
    {
      "epoch": 3.84699613110446,
      "grad_norm": 4.654423236846924,
      "learning_rate": 4.679416989074629e-05,
      "loss": 0.7371,
      "step": 421600
    },
    {
      "epoch": 3.8479086064676253,
      "grad_norm": 4.23707914352417,
      "learning_rate": 4.679340949461031e-05,
      "loss": 0.7749,
      "step": 421700
    },
    {
      "epoch": 3.8488210818307906,
      "grad_norm": 3.5811736583709717,
      "learning_rate": 4.679264909847435e-05,
      "loss": 0.7176,
      "step": 421800
    },
    {
      "epoch": 3.8497335571939555,
      "grad_norm": 3.273541212081909,
      "learning_rate": 4.679188870233837e-05,
      "loss": 0.6933,
      "step": 421900
    },
    {
      "epoch": 3.850646032557121,
      "grad_norm": 3.8534910678863525,
      "learning_rate": 4.67911283062024e-05,
      "loss": 0.7379,
      "step": 422000
    },
    {
      "epoch": 3.851558507920286,
      "grad_norm": 4.511319160461426,
      "learning_rate": 4.679036791006643e-05,
      "loss": 0.7192,
      "step": 422100
    },
    {
      "epoch": 3.8524709832834514,
      "grad_norm": 5.189502716064453,
      "learning_rate": 4.678960751393046e-05,
      "loss": 0.7731,
      "step": 422200
    },
    {
      "epoch": 3.8533834586466167,
      "grad_norm": 3.9462575912475586,
      "learning_rate": 4.678884711779449e-05,
      "loss": 0.7399,
      "step": 422300
    },
    {
      "epoch": 3.854295934009782,
      "grad_norm": 3.4737942218780518,
      "learning_rate": 4.678808672165852e-05,
      "loss": 0.77,
      "step": 422400
    },
    {
      "epoch": 3.855208409372947,
      "grad_norm": 3.5022590160369873,
      "learning_rate": 4.6787326325522544e-05,
      "loss": 0.7399,
      "step": 422500
    },
    {
      "epoch": 3.856120884736112,
      "grad_norm": 3.506545305252075,
      "learning_rate": 4.678656592938658e-05,
      "loss": 0.7299,
      "step": 422600
    },
    {
      "epoch": 3.8570333600992774,
      "grad_norm": 3.508518934249878,
      "learning_rate": 4.6785805533250604e-05,
      "loss": 0.7308,
      "step": 422700
    },
    {
      "epoch": 3.8579458354624423,
      "grad_norm": 4.308487415313721,
      "learning_rate": 4.6785045137114634e-05,
      "loss": 0.7364,
      "step": 422800
    },
    {
      "epoch": 3.8588583108256076,
      "grad_norm": 4.856045722961426,
      "learning_rate": 4.6784284740978664e-05,
      "loss": 0.7531,
      "step": 422900
    },
    {
      "epoch": 3.859770786188773,
      "grad_norm": 4.44010066986084,
      "learning_rate": 4.6783524344842694e-05,
      "loss": 0.7518,
      "step": 423000
    },
    {
      "epoch": 3.860683261551938,
      "grad_norm": 3.9993410110473633,
      "learning_rate": 4.678276394870672e-05,
      "loss": 0.7434,
      "step": 423100
    },
    {
      "epoch": 3.8615957369151035,
      "grad_norm": 4.428890705108643,
      "learning_rate": 4.678200355257075e-05,
      "loss": 0.6832,
      "step": 423200
    },
    {
      "epoch": 3.8625082122782683,
      "grad_norm": 4.253194332122803,
      "learning_rate": 4.678124315643478e-05,
      "loss": 0.7447,
      "step": 423300
    },
    {
      "epoch": 3.8634206876414336,
      "grad_norm": 3.8995485305786133,
      "learning_rate": 4.678048276029881e-05,
      "loss": 0.7142,
      "step": 423400
    },
    {
      "epoch": 3.864333163004599,
      "grad_norm": 3.9000678062438965,
      "learning_rate": 4.677972236416284e-05,
      "loss": 0.7049,
      "step": 423500
    },
    {
      "epoch": 3.865245638367764,
      "grad_norm": 5.0789008140563965,
      "learning_rate": 4.677896196802686e-05,
      "loss": 0.7455,
      "step": 423600
    },
    {
      "epoch": 3.866158113730929,
      "grad_norm": 4.176402568817139,
      "learning_rate": 4.67782015718909e-05,
      "loss": 0.7452,
      "step": 423700
    },
    {
      "epoch": 3.8670705890940944,
      "grad_norm": 4.809774875640869,
      "learning_rate": 4.677744117575492e-05,
      "loss": 0.7138,
      "step": 423800
    },
    {
      "epoch": 3.8679830644572597,
      "grad_norm": 4.517361640930176,
      "learning_rate": 4.677668077961895e-05,
      "loss": 0.7159,
      "step": 423900
    },
    {
      "epoch": 3.868895539820425,
      "grad_norm": 4.194600582122803,
      "learning_rate": 4.677592038348298e-05,
      "loss": 0.7398,
      "step": 424000
    },
    {
      "epoch": 3.8698080151835903,
      "grad_norm": 3.528844118118286,
      "learning_rate": 4.677515998734701e-05,
      "loss": 0.7316,
      "step": 424100
    },
    {
      "epoch": 3.870720490546755,
      "grad_norm": 3.261927366256714,
      "learning_rate": 4.6774399591211035e-05,
      "loss": 0.7063,
      "step": 424200
    },
    {
      "epoch": 3.8716329659099205,
      "grad_norm": 3.922795534133911,
      "learning_rate": 4.677363919507507e-05,
      "loss": 0.7041,
      "step": 424300
    },
    {
      "epoch": 3.8725454412730858,
      "grad_norm": 4.101814270019531,
      "learning_rate": 4.6772878798939095e-05,
      "loss": 0.6967,
      "step": 424400
    },
    {
      "epoch": 3.8734579166362506,
      "grad_norm": 4.107807159423828,
      "learning_rate": 4.6772118402803125e-05,
      "loss": 0.727,
      "step": 424500
    },
    {
      "epoch": 3.874370391999416,
      "grad_norm": 4.7787675857543945,
      "learning_rate": 4.6771358006667155e-05,
      "loss": 0.7121,
      "step": 424600
    },
    {
      "epoch": 3.875282867362581,
      "grad_norm": 4.946285724639893,
      "learning_rate": 4.6770597610531185e-05,
      "loss": 0.7386,
      "step": 424700
    },
    {
      "epoch": 3.8761953427257465,
      "grad_norm": 3.644702672958374,
      "learning_rate": 4.6769837214395215e-05,
      "loss": 0.7316,
      "step": 424800
    },
    {
      "epoch": 3.877107818088912,
      "grad_norm": 3.950385570526123,
      "learning_rate": 4.6769076818259245e-05,
      "loss": 0.7575,
      "step": 424900
    },
    {
      "epoch": 3.8780202934520767,
      "grad_norm": 3.897315263748169,
      "learning_rate": 4.676831642212327e-05,
      "loss": 0.7266,
      "step": 425000
    },
    {
      "epoch": 3.878932768815242,
      "grad_norm": 4.162715435028076,
      "learning_rate": 4.6767556025987305e-05,
      "loss": 0.7573,
      "step": 425100
    },
    {
      "epoch": 3.8798452441784073,
      "grad_norm": 2.845947265625,
      "learning_rate": 4.676679562985133e-05,
      "loss": 0.7434,
      "step": 425200
    },
    {
      "epoch": 3.880757719541572,
      "grad_norm": 3.6049742698669434,
      "learning_rate": 4.676603523371536e-05,
      "loss": 0.7319,
      "step": 425300
    },
    {
      "epoch": 3.8816701949047374,
      "grad_norm": 4.130817890167236,
      "learning_rate": 4.676527483757939e-05,
      "loss": 0.7365,
      "step": 425400
    },
    {
      "epoch": 3.8825826702679027,
      "grad_norm": 3.944336414337158,
      "learning_rate": 4.676451444144342e-05,
      "loss": 0.722,
      "step": 425500
    },
    {
      "epoch": 3.883495145631068,
      "grad_norm": 5.3029680252075195,
      "learning_rate": 4.676375404530744e-05,
      "loss": 0.7184,
      "step": 425600
    },
    {
      "epoch": 3.8844076209942333,
      "grad_norm": 3.862999677658081,
      "learning_rate": 4.676299364917148e-05,
      "loss": 0.7215,
      "step": 425700
    },
    {
      "epoch": 3.8853200963573986,
      "grad_norm": 3.7445685863494873,
      "learning_rate": 4.67622332530355e-05,
      "loss": 0.6988,
      "step": 425800
    },
    {
      "epoch": 3.8862325717205635,
      "grad_norm": 3.9586899280548096,
      "learning_rate": 4.676147285689953e-05,
      "loss": 0.7122,
      "step": 425900
    },
    {
      "epoch": 3.8871450470837288,
      "grad_norm": 4.378795623779297,
      "learning_rate": 4.676071246076356e-05,
      "loss": 0.7085,
      "step": 426000
    },
    {
      "epoch": 3.888057522446894,
      "grad_norm": 3.5566792488098145,
      "learning_rate": 4.6759952064627585e-05,
      "loss": 0.6991,
      "step": 426100
    },
    {
      "epoch": 3.888969997810059,
      "grad_norm": 3.502366304397583,
      "learning_rate": 4.675919166849162e-05,
      "loss": 0.7401,
      "step": 426200
    },
    {
      "epoch": 3.889882473173224,
      "grad_norm": 3.3009958267211914,
      "learning_rate": 4.6758431272355646e-05,
      "loss": 0.7302,
      "step": 426300
    },
    {
      "epoch": 3.8907949485363895,
      "grad_norm": 4.391821384429932,
      "learning_rate": 4.6757670876219676e-05,
      "loss": 0.7836,
      "step": 426400
    },
    {
      "epoch": 3.891707423899555,
      "grad_norm": 4.029647350311279,
      "learning_rate": 4.6756910480083706e-05,
      "loss": 0.7485,
      "step": 426500
    },
    {
      "epoch": 3.89261989926272,
      "grad_norm": 3.8162550926208496,
      "learning_rate": 4.6756150083947736e-05,
      "loss": 0.7567,
      "step": 426600
    },
    {
      "epoch": 3.893532374625885,
      "grad_norm": 4.553343296051025,
      "learning_rate": 4.6755389687811766e-05,
      "loss": 0.7607,
      "step": 426700
    },
    {
      "epoch": 3.8944448499890503,
      "grad_norm": 3.6848809719085693,
      "learning_rate": 4.6754629291675796e-05,
      "loss": 0.693,
      "step": 426800
    },
    {
      "epoch": 3.8953573253522156,
      "grad_norm": 3.9013614654541016,
      "learning_rate": 4.675386889553982e-05,
      "loss": 0.747,
      "step": 426900
    },
    {
      "epoch": 3.8962698007153804,
      "grad_norm": 4.564315319061279,
      "learning_rate": 4.675310849940385e-05,
      "loss": 0.7147,
      "step": 427000
    },
    {
      "epoch": 3.8971822760785457,
      "grad_norm": 3.8973658084869385,
      "learning_rate": 4.675234810326788e-05,
      "loss": 0.7187,
      "step": 427100
    },
    {
      "epoch": 3.898094751441711,
      "grad_norm": 4.844481945037842,
      "learning_rate": 4.675158770713191e-05,
      "loss": 0.7363,
      "step": 427200
    },
    {
      "epoch": 3.8990072268048763,
      "grad_norm": 3.764282464981079,
      "learning_rate": 4.675082731099594e-05,
      "loss": 0.6826,
      "step": 427300
    },
    {
      "epoch": 3.8999197021680416,
      "grad_norm": 3.1368868350982666,
      "learning_rate": 4.675006691485997e-05,
      "loss": 0.7487,
      "step": 427400
    },
    {
      "epoch": 3.900832177531207,
      "grad_norm": 2.6279664039611816,
      "learning_rate": 4.674930651872399e-05,
      "loss": 0.7206,
      "step": 427500
    },
    {
      "epoch": 3.901744652894372,
      "grad_norm": 4.453524112701416,
      "learning_rate": 4.674854612258803e-05,
      "loss": 0.7335,
      "step": 427600
    },
    {
      "epoch": 3.902657128257537,
      "grad_norm": 3.8986735343933105,
      "learning_rate": 4.674778572645205e-05,
      "loss": 0.7425,
      "step": 427700
    },
    {
      "epoch": 3.9035696036207024,
      "grad_norm": 2.8265058994293213,
      "learning_rate": 4.674702533031608e-05,
      "loss": 0.7184,
      "step": 427800
    },
    {
      "epoch": 3.9044820789838672,
      "grad_norm": 2.8724238872528076,
      "learning_rate": 4.674626493418011e-05,
      "loss": 0.6823,
      "step": 427900
    },
    {
      "epoch": 3.9053945543470325,
      "grad_norm": 4.988729953765869,
      "learning_rate": 4.674550453804414e-05,
      "loss": 0.7388,
      "step": 428000
    },
    {
      "epoch": 3.906307029710198,
      "grad_norm": 3.875216007232666,
      "learning_rate": 4.674474414190817e-05,
      "loss": 0.7414,
      "step": 428100
    },
    {
      "epoch": 3.907219505073363,
      "grad_norm": 5.122391700744629,
      "learning_rate": 4.67439837457722e-05,
      "loss": 0.7463,
      "step": 428200
    },
    {
      "epoch": 3.9081319804365284,
      "grad_norm": 4.516087532043457,
      "learning_rate": 4.674322334963623e-05,
      "loss": 0.7035,
      "step": 428300
    },
    {
      "epoch": 3.9090444557996933,
      "grad_norm": 3.93717360496521,
      "learning_rate": 4.674246295350026e-05,
      "loss": 0.7201,
      "step": 428400
    },
    {
      "epoch": 3.9099569311628586,
      "grad_norm": 3.5936806201934814,
      "learning_rate": 4.674170255736429e-05,
      "loss": 0.7031,
      "step": 428500
    },
    {
      "epoch": 3.910869406526024,
      "grad_norm": 3.761049509048462,
      "learning_rate": 4.674094216122832e-05,
      "loss": 0.7314,
      "step": 428600
    },
    {
      "epoch": 3.9117818818891887,
      "grad_norm": 3.0191287994384766,
      "learning_rate": 4.674018176509235e-05,
      "loss": 0.7092,
      "step": 428700
    },
    {
      "epoch": 3.912694357252354,
      "grad_norm": 5.288196563720703,
      "learning_rate": 4.673942136895638e-05,
      "loss": 0.7091,
      "step": 428800
    },
    {
      "epoch": 3.9136068326155193,
      "grad_norm": 3.9586546421051025,
      "learning_rate": 4.67386609728204e-05,
      "loss": 0.717,
      "step": 428900
    },
    {
      "epoch": 3.9145193079786846,
      "grad_norm": 4.589771270751953,
      "learning_rate": 4.673790057668443e-05,
      "loss": 0.7258,
      "step": 429000
    },
    {
      "epoch": 3.91543178334185,
      "grad_norm": 3.4769394397735596,
      "learning_rate": 4.673714018054846e-05,
      "loss": 0.726,
      "step": 429100
    },
    {
      "epoch": 3.9163442587050152,
      "grad_norm": 3.7596919536590576,
      "learning_rate": 4.673637978441249e-05,
      "loss": 0.7499,
      "step": 429200
    },
    {
      "epoch": 3.91725673406818,
      "grad_norm": 3.7838172912597656,
      "learning_rate": 4.673561938827652e-05,
      "loss": 0.7636,
      "step": 429300
    },
    {
      "epoch": 3.9181692094313454,
      "grad_norm": 4.13389253616333,
      "learning_rate": 4.6734858992140544e-05,
      "loss": 0.7035,
      "step": 429400
    },
    {
      "epoch": 3.9190816847945107,
      "grad_norm": 4.925431251525879,
      "learning_rate": 4.673409859600458e-05,
      "loss": 0.6941,
      "step": 429500
    },
    {
      "epoch": 3.9199941601576755,
      "grad_norm": 4.046291351318359,
      "learning_rate": 4.6733338199868604e-05,
      "loss": 0.6797,
      "step": 429600
    },
    {
      "epoch": 3.920906635520841,
      "grad_norm": 4.069572448730469,
      "learning_rate": 4.6732577803732634e-05,
      "loss": 0.7276,
      "step": 429700
    },
    {
      "epoch": 3.921819110884006,
      "grad_norm": 4.90880823135376,
      "learning_rate": 4.6731817407596664e-05,
      "loss": 0.7079,
      "step": 429800
    },
    {
      "epoch": 3.9227315862471714,
      "grad_norm": 3.499720811843872,
      "learning_rate": 4.6731057011460694e-05,
      "loss": 0.6816,
      "step": 429900
    },
    {
      "epoch": 3.9236440616103367,
      "grad_norm": 4.070778846740723,
      "learning_rate": 4.673029661532472e-05,
      "loss": 0.7572,
      "step": 430000
    },
    {
      "epoch": 3.9245565369735016,
      "grad_norm": 5.1007513999938965,
      "learning_rate": 4.6729536219188754e-05,
      "loss": 0.7286,
      "step": 430100
    },
    {
      "epoch": 3.925469012336667,
      "grad_norm": 3.8116111755371094,
      "learning_rate": 4.672877582305278e-05,
      "loss": 0.7144,
      "step": 430200
    },
    {
      "epoch": 3.926381487699832,
      "grad_norm": 3.760550022125244,
      "learning_rate": 4.672801542691681e-05,
      "loss": 0.7601,
      "step": 430300
    },
    {
      "epoch": 3.927293963062997,
      "grad_norm": 4.33886194229126,
      "learning_rate": 4.672725503078084e-05,
      "loss": 0.7033,
      "step": 430400
    },
    {
      "epoch": 3.9282064384261624,
      "grad_norm": 3.8456830978393555,
      "learning_rate": 4.672649463464487e-05,
      "loss": 0.7268,
      "step": 430500
    },
    {
      "epoch": 3.9291189137893276,
      "grad_norm": 3.390672445297241,
      "learning_rate": 4.67257342385089e-05,
      "loss": 0.7138,
      "step": 430600
    },
    {
      "epoch": 3.930031389152493,
      "grad_norm": 3.0887491703033447,
      "learning_rate": 4.672497384237293e-05,
      "loss": 0.725,
      "step": 430700
    },
    {
      "epoch": 3.9309438645156582,
      "grad_norm": 4.678948879241943,
      "learning_rate": 4.672421344623695e-05,
      "loss": 0.7302,
      "step": 430800
    },
    {
      "epoch": 3.9318563398788235,
      "grad_norm": 3.790579080581665,
      "learning_rate": 4.672345305010099e-05,
      "loss": 0.7266,
      "step": 430900
    },
    {
      "epoch": 3.9327688152419884,
      "grad_norm": 4.088823318481445,
      "learning_rate": 4.672269265396501e-05,
      "loss": 0.739,
      "step": 431000
    },
    {
      "epoch": 3.9336812906051537,
      "grad_norm": 3.7507035732269287,
      "learning_rate": 4.672193225782904e-05,
      "loss": 0.7436,
      "step": 431100
    },
    {
      "epoch": 3.934593765968319,
      "grad_norm": 3.7010200023651123,
      "learning_rate": 4.672117186169307e-05,
      "loss": 0.6934,
      "step": 431200
    },
    {
      "epoch": 3.935506241331484,
      "grad_norm": 4.35740852355957,
      "learning_rate": 4.67204114655571e-05,
      "loss": 0.7235,
      "step": 431300
    },
    {
      "epoch": 3.936418716694649,
      "grad_norm": 3.239886999130249,
      "learning_rate": 4.6719651069421125e-05,
      "loss": 0.7177,
      "step": 431400
    },
    {
      "epoch": 3.9373311920578145,
      "grad_norm": 4.565549373626709,
      "learning_rate": 4.671889067328516e-05,
      "loss": 0.7416,
      "step": 431500
    },
    {
      "epoch": 3.9382436674209798,
      "grad_norm": 4.769664764404297,
      "learning_rate": 4.6718130277149185e-05,
      "loss": 0.7462,
      "step": 431600
    },
    {
      "epoch": 3.939156142784145,
      "grad_norm": 3.87142276763916,
      "learning_rate": 4.6717369881013215e-05,
      "loss": 0.7474,
      "step": 431700
    },
    {
      "epoch": 3.94006861814731,
      "grad_norm": 3.9326233863830566,
      "learning_rate": 4.6716609484877245e-05,
      "loss": 0.7552,
      "step": 431800
    },
    {
      "epoch": 3.940981093510475,
      "grad_norm": 4.528019428253174,
      "learning_rate": 4.671584908874127e-05,
      "loss": 0.7678,
      "step": 431900
    },
    {
      "epoch": 3.9418935688736405,
      "grad_norm": 3.9742684364318848,
      "learning_rate": 4.6715088692605305e-05,
      "loss": 0.7533,
      "step": 432000
    },
    {
      "epoch": 3.9428060442368054,
      "grad_norm": 4.30088996887207,
      "learning_rate": 4.671432829646933e-05,
      "loss": 0.6913,
      "step": 432100
    },
    {
      "epoch": 3.9437185195999707,
      "grad_norm": 4.800413608551025,
      "learning_rate": 4.671356790033336e-05,
      "loss": 0.7624,
      "step": 432200
    },
    {
      "epoch": 3.944630994963136,
      "grad_norm": 5.007208347320557,
      "learning_rate": 4.671280750419739e-05,
      "loss": 0.7758,
      "step": 432300
    },
    {
      "epoch": 3.9455434703263013,
      "grad_norm": 4.675457000732422,
      "learning_rate": 4.671204710806142e-05,
      "loss": 0.7738,
      "step": 432400
    },
    {
      "epoch": 3.9464559456894666,
      "grad_norm": 3.6544175148010254,
      "learning_rate": 4.671128671192544e-05,
      "loss": 0.7151,
      "step": 432500
    },
    {
      "epoch": 3.9473684210526314,
      "grad_norm": 4.489327430725098,
      "learning_rate": 4.671052631578948e-05,
      "loss": 0.7236,
      "step": 432600
    },
    {
      "epoch": 3.9482808964157967,
      "grad_norm": 4.428761959075928,
      "learning_rate": 4.67097659196535e-05,
      "loss": 0.7262,
      "step": 432700
    },
    {
      "epoch": 3.949193371778962,
      "grad_norm": 4.707226276397705,
      "learning_rate": 4.670900552351753e-05,
      "loss": 0.6802,
      "step": 432800
    },
    {
      "epoch": 3.9501058471421273,
      "grad_norm": 4.308112621307373,
      "learning_rate": 4.670824512738156e-05,
      "loss": 0.6907,
      "step": 432900
    },
    {
      "epoch": 3.951018322505292,
      "grad_norm": 4.279668807983398,
      "learning_rate": 4.670748473124559e-05,
      "loss": 0.717,
      "step": 433000
    },
    {
      "epoch": 3.9519307978684575,
      "grad_norm": 3.5705630779266357,
      "learning_rate": 4.670672433510962e-05,
      "loss": 0.7545,
      "step": 433100
    },
    {
      "epoch": 3.9528432732316228,
      "grad_norm": 5.743168830871582,
      "learning_rate": 4.670596393897365e-05,
      "loss": 0.7161,
      "step": 433200
    },
    {
      "epoch": 3.953755748594788,
      "grad_norm": 4.072877883911133,
      "learning_rate": 4.6705203542837676e-05,
      "loss": 0.7306,
      "step": 433300
    },
    {
      "epoch": 3.9546682239579534,
      "grad_norm": 4.278171062469482,
      "learning_rate": 4.670444314670171e-05,
      "loss": 0.7268,
      "step": 433400
    },
    {
      "epoch": 3.955580699321118,
      "grad_norm": 4.1818647384643555,
      "learning_rate": 4.6703682750565736e-05,
      "loss": 0.7222,
      "step": 433500
    },
    {
      "epoch": 3.9564931746842835,
      "grad_norm": 3.6053128242492676,
      "learning_rate": 4.6702922354429766e-05,
      "loss": 0.7329,
      "step": 433600
    },
    {
      "epoch": 3.957405650047449,
      "grad_norm": 4.110597133636475,
      "learning_rate": 4.6702161958293796e-05,
      "loss": 0.7484,
      "step": 433700
    },
    {
      "epoch": 3.9583181254106137,
      "grad_norm": 3.7387728691101074,
      "learning_rate": 4.6701401562157826e-05,
      "loss": 0.6996,
      "step": 433800
    },
    {
      "epoch": 3.959230600773779,
      "grad_norm": 4.39954948425293,
      "learning_rate": 4.670064116602185e-05,
      "loss": 0.7349,
      "step": 433900
    },
    {
      "epoch": 3.9601430761369443,
      "grad_norm": 4.073185443878174,
      "learning_rate": 4.6699880769885886e-05,
      "loss": 0.7262,
      "step": 434000
    },
    {
      "epoch": 3.9610555515001096,
      "grad_norm": 3.7530665397644043,
      "learning_rate": 4.669912037374991e-05,
      "loss": 0.7124,
      "step": 434100
    },
    {
      "epoch": 3.961968026863275,
      "grad_norm": 4.250348091125488,
      "learning_rate": 4.669835997761394e-05,
      "loss": 0.738,
      "step": 434200
    },
    {
      "epoch": 3.9628805022264397,
      "grad_norm": 3.107875347137451,
      "learning_rate": 4.669759958147797e-05,
      "loss": 0.6929,
      "step": 434300
    },
    {
      "epoch": 3.963792977589605,
      "grad_norm": 4.140047550201416,
      "learning_rate": 4.6696839185342e-05,
      "loss": 0.7312,
      "step": 434400
    },
    {
      "epoch": 3.9647054529527703,
      "grad_norm": 3.340334892272949,
      "learning_rate": 4.669607878920603e-05,
      "loss": 0.739,
      "step": 434500
    },
    {
      "epoch": 3.9656179283159356,
      "grad_norm": 3.902142286300659,
      "learning_rate": 4.669531839307005e-05,
      "loss": 0.7321,
      "step": 434600
    },
    {
      "epoch": 3.9665304036791005,
      "grad_norm": 3.852818012237549,
      "learning_rate": 4.669455799693408e-05,
      "loss": 0.726,
      "step": 434700
    },
    {
      "epoch": 3.967442879042266,
      "grad_norm": 3.203672170639038,
      "learning_rate": 4.669379760079811e-05,
      "loss": 0.7273,
      "step": 434800
    },
    {
      "epoch": 3.968355354405431,
      "grad_norm": 4.398126125335693,
      "learning_rate": 4.669303720466214e-05,
      "loss": 0.7018,
      "step": 434900
    },
    {
      "epoch": 3.9692678297685964,
      "grad_norm": 3.6892387866973877,
      "learning_rate": 4.6692276808526167e-05,
      "loss": 0.7161,
      "step": 435000
    },
    {
      "epoch": 3.9701803051317617,
      "grad_norm": 4.0218186378479,
      "learning_rate": 4.6691516412390203e-05,
      "loss": 0.7197,
      "step": 435100
    },
    {
      "epoch": 3.9710927804949265,
      "grad_norm": 4.412159442901611,
      "learning_rate": 4.669075601625423e-05,
      "loss": 0.7061,
      "step": 435200
    },
    {
      "epoch": 3.972005255858092,
      "grad_norm": 3.8613150119781494,
      "learning_rate": 4.668999562011826e-05,
      "loss": 0.7348,
      "step": 435300
    },
    {
      "epoch": 3.972917731221257,
      "grad_norm": 4.203222274780273,
      "learning_rate": 4.668923522398229e-05,
      "loss": 0.7432,
      "step": 435400
    },
    {
      "epoch": 3.973830206584422,
      "grad_norm": 5.2369513511657715,
      "learning_rate": 4.668847482784632e-05,
      "loss": 0.7639,
      "step": 435500
    },
    {
      "epoch": 3.9747426819475873,
      "grad_norm": 3.7981464862823486,
      "learning_rate": 4.668771443171035e-05,
      "loss": 0.7368,
      "step": 435600
    },
    {
      "epoch": 3.9756551573107526,
      "grad_norm": 3.139664649963379,
      "learning_rate": 4.668695403557438e-05,
      "loss": 0.7263,
      "step": 435700
    },
    {
      "epoch": 3.976567632673918,
      "grad_norm": 4.218707084655762,
      "learning_rate": 4.66861936394384e-05,
      "loss": 0.7267,
      "step": 435800
    },
    {
      "epoch": 3.977480108037083,
      "grad_norm": 2.917421817779541,
      "learning_rate": 4.668543324330244e-05,
      "loss": 0.7131,
      "step": 435900
    },
    {
      "epoch": 3.978392583400248,
      "grad_norm": 3.4951019287109375,
      "learning_rate": 4.668467284716646e-05,
      "loss": 0.748,
      "step": 436000
    },
    {
      "epoch": 3.9793050587634133,
      "grad_norm": 3.83341908454895,
      "learning_rate": 4.668391245103049e-05,
      "loss": 0.7186,
      "step": 436100
    },
    {
      "epoch": 3.9802175341265786,
      "grad_norm": 4.325876235961914,
      "learning_rate": 4.668315205489452e-05,
      "loss": 0.7187,
      "step": 436200
    },
    {
      "epoch": 3.981130009489744,
      "grad_norm": 4.147576332092285,
      "learning_rate": 4.668239165875855e-05,
      "loss": 0.6868,
      "step": 436300
    },
    {
      "epoch": 3.982042484852909,
      "grad_norm": 4.6737189292907715,
      "learning_rate": 4.6681631262622574e-05,
      "loss": 0.7321,
      "step": 436400
    },
    {
      "epoch": 3.982954960216074,
      "grad_norm": 3.7876100540161133,
      "learning_rate": 4.668087086648661e-05,
      "loss": 0.6995,
      "step": 436500
    },
    {
      "epoch": 3.9838674355792394,
      "grad_norm": 4.722206115722656,
      "learning_rate": 4.6680110470350634e-05,
      "loss": 0.7083,
      "step": 436600
    },
    {
      "epoch": 3.9847799109424047,
      "grad_norm": 3.7043583393096924,
      "learning_rate": 4.6679350074214664e-05,
      "loss": 0.6719,
      "step": 436700
    },
    {
      "epoch": 3.98569238630557,
      "grad_norm": 3.70967698097229,
      "learning_rate": 4.6678589678078694e-05,
      "loss": 0.7256,
      "step": 436800
    },
    {
      "epoch": 3.986604861668735,
      "grad_norm": 4.617711067199707,
      "learning_rate": 4.6677829281942724e-05,
      "loss": 0.719,
      "step": 436900
    },
    {
      "epoch": 3.9875173370319,
      "grad_norm": 4.155750274658203,
      "learning_rate": 4.6677068885806754e-05,
      "loss": 0.7578,
      "step": 437000
    },
    {
      "epoch": 3.9884298123950654,
      "grad_norm": 3.8766984939575195,
      "learning_rate": 4.6676308489670784e-05,
      "loss": 0.714,
      "step": 437100
    },
    {
      "epoch": 3.9893422877582303,
      "grad_norm": 3.454226016998291,
      "learning_rate": 4.667554809353481e-05,
      "loss": 0.711,
      "step": 437200
    },
    {
      "epoch": 3.9902547631213956,
      "grad_norm": 4.4558844566345215,
      "learning_rate": 4.6674787697398845e-05,
      "loss": 0.7351,
      "step": 437300
    },
    {
      "epoch": 3.991167238484561,
      "grad_norm": 4.648251533508301,
      "learning_rate": 4.667402730126287e-05,
      "loss": 0.7621,
      "step": 437400
    },
    {
      "epoch": 3.992079713847726,
      "grad_norm": 3.7430057525634766,
      "learning_rate": 4.667326690512689e-05,
      "loss": 0.7301,
      "step": 437500
    },
    {
      "epoch": 3.9929921892108915,
      "grad_norm": 4.747663974761963,
      "learning_rate": 4.667250650899093e-05,
      "loss": 0.7505,
      "step": 437600
    },
    {
      "epoch": 3.9939046645740564,
      "grad_norm": 5.346219062805176,
      "learning_rate": 4.667174611285495e-05,
      "loss": 0.7294,
      "step": 437700
    },
    {
      "epoch": 3.9948171399372217,
      "grad_norm": 5.102816581726074,
      "learning_rate": 4.667098571671898e-05,
      "loss": 0.7195,
      "step": 437800
    },
    {
      "epoch": 3.995729615300387,
      "grad_norm": 3.9042129516601562,
      "learning_rate": 4.667022532058301e-05,
      "loss": 0.7231,
      "step": 437900
    },
    {
      "epoch": 3.9966420906635522,
      "grad_norm": 4.678478240966797,
      "learning_rate": 4.666946492444704e-05,
      "loss": 0.7302,
      "step": 438000
    },
    {
      "epoch": 3.997554566026717,
      "grad_norm": 3.6451733112335205,
      "learning_rate": 4.666870452831107e-05,
      "loss": 0.7362,
      "step": 438100
    },
    {
      "epoch": 3.9984670413898824,
      "grad_norm": 3.891184091567993,
      "learning_rate": 4.66679441321751e-05,
      "loss": 0.7521,
      "step": 438200
    },
    {
      "epoch": 3.9993795167530477,
      "grad_norm": 3.5894312858581543,
      "learning_rate": 4.6667183736039125e-05,
      "loss": 0.6949,
      "step": 438300
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5896905660629272,
      "eval_runtime": 25.4845,
      "eval_samples_per_second": 226.373,
      "eval_steps_per_second": 226.373,
      "step": 438368
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5738134980201721,
      "eval_runtime": 482.5195,
      "eval_samples_per_second": 227.124,
      "eval_steps_per_second": 227.124,
      "step": 438368
    },
    {
      "epoch": 4.000291992116213,
      "grad_norm": 3.74092173576355,
      "learning_rate": 4.666642333990316e-05,
      "loss": 0.7245,
      "step": 438400
    },
    {
      "epoch": 4.001204467479378,
      "grad_norm": 4.659855842590332,
      "learning_rate": 4.6665662943767185e-05,
      "loss": 0.6885,
      "step": 438500
    },
    {
      "epoch": 4.002116942842544,
      "grad_norm": 4.171946048736572,
      "learning_rate": 4.6664902547631215e-05,
      "loss": 0.7453,
      "step": 438600
    },
    {
      "epoch": 4.003029418205708,
      "grad_norm": 4.011196613311768,
      "learning_rate": 4.6664142151495245e-05,
      "loss": 0.7043,
      "step": 438700
    },
    {
      "epoch": 4.003941893568873,
      "grad_norm": 4.822241306304932,
      "learning_rate": 4.6663381755359275e-05,
      "loss": 0.693,
      "step": 438800
    },
    {
      "epoch": 4.004854368932039,
      "grad_norm": 3.4446475505828857,
      "learning_rate": 4.66626213592233e-05,
      "loss": 0.7164,
      "step": 438900
    },
    {
      "epoch": 4.005766844295204,
      "grad_norm": 4.171957969665527,
      "learning_rate": 4.6661860963087335e-05,
      "loss": 0.6826,
      "step": 439000
    },
    {
      "epoch": 4.006679319658369,
      "grad_norm": 4.677346229553223,
      "learning_rate": 4.666110056695136e-05,
      "loss": 0.7187,
      "step": 439100
    },
    {
      "epoch": 4.0075917950215345,
      "grad_norm": 4.043015003204346,
      "learning_rate": 4.666034017081539e-05,
      "loss": 0.7218,
      "step": 439200
    },
    {
      "epoch": 4.0085042703847,
      "grad_norm": 3.6689612865448,
      "learning_rate": 4.665957977467942e-05,
      "loss": 0.7547,
      "step": 439300
    },
    {
      "epoch": 4.009416745747865,
      "grad_norm": 4.128244876861572,
      "learning_rate": 4.665881937854345e-05,
      "loss": 0.7196,
      "step": 439400
    },
    {
      "epoch": 4.01032922111103,
      "grad_norm": 4.3238067626953125,
      "learning_rate": 4.665805898240748e-05,
      "loss": 0.7561,
      "step": 439500
    },
    {
      "epoch": 4.011241696474195,
      "grad_norm": 3.6985981464385986,
      "learning_rate": 4.665729858627151e-05,
      "loss": 0.6901,
      "step": 439600
    },
    {
      "epoch": 4.01215417183736,
      "grad_norm": 4.4140448570251465,
      "learning_rate": 4.665653819013553e-05,
      "loss": 0.7113,
      "step": 439700
    },
    {
      "epoch": 4.013066647200525,
      "grad_norm": 3.883669376373291,
      "learning_rate": 4.665577779399957e-05,
      "loss": 0.7168,
      "step": 439800
    },
    {
      "epoch": 4.013979122563691,
      "grad_norm": 4.404171466827393,
      "learning_rate": 4.665501739786359e-05,
      "loss": 0.7088,
      "step": 439900
    },
    {
      "epoch": 4.014891597926856,
      "grad_norm": 4.288637161254883,
      "learning_rate": 4.665425700172762e-05,
      "loss": 0.7198,
      "step": 440000
    },
    {
      "epoch": 4.015804073290021,
      "grad_norm": 3.2324700355529785,
      "learning_rate": 4.665349660559165e-05,
      "loss": 0.7041,
      "step": 440100
    },
    {
      "epoch": 4.016716548653187,
      "grad_norm": 4.276548385620117,
      "learning_rate": 4.665273620945568e-05,
      "loss": 0.7224,
      "step": 440200
    },
    {
      "epoch": 4.017629024016352,
      "grad_norm": 4.125844955444336,
      "learning_rate": 4.665197581331971e-05,
      "loss": 0.7359,
      "step": 440300
    },
    {
      "epoch": 4.018541499379516,
      "grad_norm": 3.2987823486328125,
      "learning_rate": 4.6651215417183736e-05,
      "loss": 0.6831,
      "step": 440400
    },
    {
      "epoch": 4.019453974742682,
      "grad_norm": 4.8675971031188965,
      "learning_rate": 4.6650455021047766e-05,
      "loss": 0.7112,
      "step": 440500
    },
    {
      "epoch": 4.020366450105847,
      "grad_norm": 4.633395671844482,
      "learning_rate": 4.6649694624911796e-05,
      "loss": 0.7184,
      "step": 440600
    },
    {
      "epoch": 4.021278925469012,
      "grad_norm": 4.57559061050415,
      "learning_rate": 4.6648934228775826e-05,
      "loss": 0.7356,
      "step": 440700
    },
    {
      "epoch": 4.0221914008321775,
      "grad_norm": 4.482644557952881,
      "learning_rate": 4.664817383263985e-05,
      "loss": 0.748,
      "step": 440800
    },
    {
      "epoch": 4.023103876195343,
      "grad_norm": 3.92789888381958,
      "learning_rate": 4.6647413436503886e-05,
      "loss": 0.6899,
      "step": 440900
    },
    {
      "epoch": 4.024016351558508,
      "grad_norm": 3.8159308433532715,
      "learning_rate": 4.664665304036791e-05,
      "loss": 0.6906,
      "step": 441000
    },
    {
      "epoch": 4.024928826921673,
      "grad_norm": 4.6982035636901855,
      "learning_rate": 4.664589264423194e-05,
      "loss": 0.7459,
      "step": 441100
    },
    {
      "epoch": 4.025841302284839,
      "grad_norm": 3.867406129837036,
      "learning_rate": 4.664513224809597e-05,
      "loss": 0.6498,
      "step": 441200
    },
    {
      "epoch": 4.026753777648003,
      "grad_norm": 4.409730911254883,
      "learning_rate": 4.664437185196e-05,
      "loss": 0.7461,
      "step": 441300
    },
    {
      "epoch": 4.027666253011168,
      "grad_norm": 3.209629535675049,
      "learning_rate": 4.664361145582403e-05,
      "loss": 0.6995,
      "step": 441400
    },
    {
      "epoch": 4.028578728374334,
      "grad_norm": 4.171762943267822,
      "learning_rate": 4.664285105968806e-05,
      "loss": 0.7268,
      "step": 441500
    },
    {
      "epoch": 4.029491203737499,
      "grad_norm": 4.254566669464111,
      "learning_rate": 4.664209066355208e-05,
      "loss": 0.7045,
      "step": 441600
    },
    {
      "epoch": 4.030403679100664,
      "grad_norm": 3.821366310119629,
      "learning_rate": 4.664133026741612e-05,
      "loss": 0.7205,
      "step": 441700
    },
    {
      "epoch": 4.03131615446383,
      "grad_norm": 4.377540111541748,
      "learning_rate": 4.6640569871280143e-05,
      "loss": 0.7052,
      "step": 441800
    },
    {
      "epoch": 4.032228629826995,
      "grad_norm": 4.125103950500488,
      "learning_rate": 4.6639809475144173e-05,
      "loss": 0.6997,
      "step": 441900
    },
    {
      "epoch": 4.03314110519016,
      "grad_norm": 4.2199506759643555,
      "learning_rate": 4.6639049079008204e-05,
      "loss": 0.6919,
      "step": 442000
    },
    {
      "epoch": 4.034053580553325,
      "grad_norm": 4.0763773918151855,
      "learning_rate": 4.6638288682872234e-05,
      "loss": 0.7344,
      "step": 442100
    },
    {
      "epoch": 4.03496605591649,
      "grad_norm": 3.6340131759643555,
      "learning_rate": 4.663752828673626e-05,
      "loss": 0.7735,
      "step": 442200
    },
    {
      "epoch": 4.035878531279655,
      "grad_norm": 4.587629795074463,
      "learning_rate": 4.6636767890600294e-05,
      "loss": 0.7284,
      "step": 442300
    },
    {
      "epoch": 4.0367910066428205,
      "grad_norm": 4.981634616851807,
      "learning_rate": 4.663600749446432e-05,
      "loss": 0.7113,
      "step": 442400
    },
    {
      "epoch": 4.037703482005986,
      "grad_norm": 4.164541244506836,
      "learning_rate": 4.663524709832835e-05,
      "loss": 0.7639,
      "step": 442500
    },
    {
      "epoch": 4.038615957369151,
      "grad_norm": 3.721857786178589,
      "learning_rate": 4.663448670219238e-05,
      "loss": 0.757,
      "step": 442600
    },
    {
      "epoch": 4.039528432732316,
      "grad_norm": 4.568319797515869,
      "learning_rate": 4.663372630605641e-05,
      "loss": 0.717,
      "step": 442700
    },
    {
      "epoch": 4.040440908095482,
      "grad_norm": 3.2076475620269775,
      "learning_rate": 4.663296590992044e-05,
      "loss": 0.6912,
      "step": 442800
    },
    {
      "epoch": 4.041353383458647,
      "grad_norm": 4.382143497467041,
      "learning_rate": 4.663220551378447e-05,
      "loss": 0.7497,
      "step": 442900
    },
    {
      "epoch": 4.042265858821811,
      "grad_norm": 4.420133113861084,
      "learning_rate": 4.663144511764849e-05,
      "loss": 0.7517,
      "step": 443000
    },
    {
      "epoch": 4.043178334184977,
      "grad_norm": 4.805071830749512,
      "learning_rate": 4.663068472151252e-05,
      "loss": 0.757,
      "step": 443100
    },
    {
      "epoch": 4.044090809548142,
      "grad_norm": 4.428953170776367,
      "learning_rate": 4.662992432537655e-05,
      "loss": 0.7344,
      "step": 443200
    },
    {
      "epoch": 4.045003284911307,
      "grad_norm": 3.9317939281463623,
      "learning_rate": 4.6629163929240574e-05,
      "loss": 0.7454,
      "step": 443300
    },
    {
      "epoch": 4.045915760274473,
      "grad_norm": 4.251445293426514,
      "learning_rate": 4.662840353310461e-05,
      "loss": 0.6909,
      "step": 443400
    },
    {
      "epoch": 4.046828235637638,
      "grad_norm": 3.6398723125457764,
      "learning_rate": 4.6627643136968634e-05,
      "loss": 0.7046,
      "step": 443500
    },
    {
      "epoch": 4.047740711000803,
      "grad_norm": 4.294349670410156,
      "learning_rate": 4.6626882740832664e-05,
      "loss": 0.6695,
      "step": 443600
    },
    {
      "epoch": 4.0486531863639685,
      "grad_norm": 3.8908941745758057,
      "learning_rate": 4.6626122344696694e-05,
      "loss": 0.7309,
      "step": 443700
    },
    {
      "epoch": 4.049565661727133,
      "grad_norm": 4.3476176261901855,
      "learning_rate": 4.6625361948560724e-05,
      "loss": 0.687,
      "step": 443800
    },
    {
      "epoch": 4.050478137090298,
      "grad_norm": 3.717698097229004,
      "learning_rate": 4.6624601552424754e-05,
      "loss": 0.7012,
      "step": 443900
    },
    {
      "epoch": 4.0513906124534635,
      "grad_norm": 4.436276435852051,
      "learning_rate": 4.6623841156288785e-05,
      "loss": 0.7279,
      "step": 444000
    },
    {
      "epoch": 4.052303087816629,
      "grad_norm": 4.665747165679932,
      "learning_rate": 4.662308076015281e-05,
      "loss": 0.7043,
      "step": 444100
    },
    {
      "epoch": 4.053215563179794,
      "grad_norm": 4.367623329162598,
      "learning_rate": 4.6622320364016845e-05,
      "loss": 0.7034,
      "step": 444200
    },
    {
      "epoch": 4.0541280385429594,
      "grad_norm": 4.082684516906738,
      "learning_rate": 4.662155996788087e-05,
      "loss": 0.736,
      "step": 444300
    },
    {
      "epoch": 4.055040513906125,
      "grad_norm": 3.8461482524871826,
      "learning_rate": 4.66207995717449e-05,
      "loss": 0.7554,
      "step": 444400
    },
    {
      "epoch": 4.05595298926929,
      "grad_norm": 3.03794002532959,
      "learning_rate": 4.662003917560893e-05,
      "loss": 0.6953,
      "step": 444500
    },
    {
      "epoch": 4.056865464632455,
      "grad_norm": 5.283821105957031,
      "learning_rate": 4.661927877947296e-05,
      "loss": 0.7196,
      "step": 444600
    },
    {
      "epoch": 4.05777793999562,
      "grad_norm": 5.594027996063232,
      "learning_rate": 4.661851838333698e-05,
      "loss": 0.7407,
      "step": 444700
    },
    {
      "epoch": 4.058690415358785,
      "grad_norm": 3.1754322052001953,
      "learning_rate": 4.661775798720102e-05,
      "loss": 0.7175,
      "step": 444800
    },
    {
      "epoch": 4.05960289072195,
      "grad_norm": 4.5668134689331055,
      "learning_rate": 4.661699759106504e-05,
      "loss": 0.7357,
      "step": 444900
    },
    {
      "epoch": 4.060515366085116,
      "grad_norm": 3.923750162124634,
      "learning_rate": 4.661623719492907e-05,
      "loss": 0.7518,
      "step": 445000
    },
    {
      "epoch": 4.061427841448281,
      "grad_norm": 4.167392253875732,
      "learning_rate": 4.66154767987931e-05,
      "loss": 0.7106,
      "step": 445100
    },
    {
      "epoch": 4.062340316811446,
      "grad_norm": 4.631115436553955,
      "learning_rate": 4.661471640265713e-05,
      "loss": 0.7238,
      "step": 445200
    },
    {
      "epoch": 4.0632527921746115,
      "grad_norm": 4.832986831665039,
      "learning_rate": 4.661395600652116e-05,
      "loss": 0.7459,
      "step": 445300
    },
    {
      "epoch": 4.064165267537777,
      "grad_norm": 4.211310386657715,
      "learning_rate": 4.661319561038519e-05,
      "loss": 0.7519,
      "step": 445400
    },
    {
      "epoch": 4.065077742900941,
      "grad_norm": 5.621156692504883,
      "learning_rate": 4.6612435214249215e-05,
      "loss": 0.7293,
      "step": 445500
    },
    {
      "epoch": 4.065990218264107,
      "grad_norm": 5.179264068603516,
      "learning_rate": 4.661167481811325e-05,
      "loss": 0.712,
      "step": 445600
    },
    {
      "epoch": 4.066902693627272,
      "grad_norm": 3.606055498123169,
      "learning_rate": 4.6610914421977275e-05,
      "loss": 0.7044,
      "step": 445700
    },
    {
      "epoch": 4.067815168990437,
      "grad_norm": 4.24705696105957,
      "learning_rate": 4.6610154025841305e-05,
      "loss": 0.7385,
      "step": 445800
    },
    {
      "epoch": 4.0687276443536025,
      "grad_norm": 4.00849723815918,
      "learning_rate": 4.6609393629705336e-05,
      "loss": 0.6888,
      "step": 445900
    },
    {
      "epoch": 4.069640119716768,
      "grad_norm": 3.924605131149292,
      "learning_rate": 4.660863323356936e-05,
      "loss": 0.7082,
      "step": 446000
    },
    {
      "epoch": 4.070552595079933,
      "grad_norm": 3.450876474380493,
      "learning_rate": 4.660787283743339e-05,
      "loss": 0.7964,
      "step": 446100
    },
    {
      "epoch": 4.071465070443098,
      "grad_norm": 2.9245662689208984,
      "learning_rate": 4.660711244129742e-05,
      "loss": 0.6889,
      "step": 446200
    },
    {
      "epoch": 4.072377545806264,
      "grad_norm": 3.9542102813720703,
      "learning_rate": 4.660635204516145e-05,
      "loss": 0.7293,
      "step": 446300
    },
    {
      "epoch": 4.073290021169428,
      "grad_norm": 4.331859111785889,
      "learning_rate": 4.660559164902548e-05,
      "loss": 0.7404,
      "step": 446400
    },
    {
      "epoch": 4.074202496532593,
      "grad_norm": 4.619970798492432,
      "learning_rate": 4.660483125288951e-05,
      "loss": 0.7294,
      "step": 446500
    },
    {
      "epoch": 4.075114971895759,
      "grad_norm": 4.638515949249268,
      "learning_rate": 4.660407085675353e-05,
      "loss": 0.7255,
      "step": 446600
    },
    {
      "epoch": 4.076027447258924,
      "grad_norm": 4.58754825592041,
      "learning_rate": 4.660331046061757e-05,
      "loss": 0.6968,
      "step": 446700
    },
    {
      "epoch": 4.076939922622089,
      "grad_norm": 3.5422799587249756,
      "learning_rate": 4.660255006448159e-05,
      "loss": 0.7386,
      "step": 446800
    },
    {
      "epoch": 4.077852397985255,
      "grad_norm": 3.7305092811584473,
      "learning_rate": 4.660178966834562e-05,
      "loss": 0.7092,
      "step": 446900
    },
    {
      "epoch": 4.07876487334842,
      "grad_norm": 4.531289100646973,
      "learning_rate": 4.660102927220965e-05,
      "loss": 0.7597,
      "step": 447000
    },
    {
      "epoch": 4.079677348711585,
      "grad_norm": 3.77016282081604,
      "learning_rate": 4.660026887607368e-05,
      "loss": 0.7251,
      "step": 447100
    },
    {
      "epoch": 4.08058982407475,
      "grad_norm": 5.666352272033691,
      "learning_rate": 4.6599508479937706e-05,
      "loss": 0.7287,
      "step": 447200
    },
    {
      "epoch": 4.081502299437915,
      "grad_norm": 4.288332462310791,
      "learning_rate": 4.659874808380174e-05,
      "loss": 0.745,
      "step": 447300
    },
    {
      "epoch": 4.08241477480108,
      "grad_norm": 3.2884833812713623,
      "learning_rate": 4.6597987687665766e-05,
      "loss": 0.6881,
      "step": 447400
    },
    {
      "epoch": 4.0833272501642455,
      "grad_norm": 4.292118072509766,
      "learning_rate": 4.6597227291529796e-05,
      "loss": 0.7577,
      "step": 447500
    },
    {
      "epoch": 4.084239725527411,
      "grad_norm": 4.474571228027344,
      "learning_rate": 4.6596466895393826e-05,
      "loss": 0.7254,
      "step": 447600
    },
    {
      "epoch": 4.085152200890576,
      "grad_norm": 4.069386005401611,
      "learning_rate": 4.6595706499257856e-05,
      "loss": 0.6655,
      "step": 447700
    },
    {
      "epoch": 4.086064676253741,
      "grad_norm": 4.822988986968994,
      "learning_rate": 4.6594946103121886e-05,
      "loss": 0.7283,
      "step": 447800
    },
    {
      "epoch": 4.086977151616907,
      "grad_norm": 4.347027778625488,
      "learning_rate": 4.6594185706985917e-05,
      "loss": 0.6988,
      "step": 447900
    },
    {
      "epoch": 4.087889626980072,
      "grad_norm": 4.495874881744385,
      "learning_rate": 4.659342531084994e-05,
      "loss": 0.7198,
      "step": 448000
    },
    {
      "epoch": 4.088802102343236,
      "grad_norm": 4.717628479003906,
      "learning_rate": 4.659266491471398e-05,
      "loss": 0.7608,
      "step": 448100
    },
    {
      "epoch": 4.089714577706402,
      "grad_norm": 4.327602863311768,
      "learning_rate": 4.6591904518578e-05,
      "loss": 0.719,
      "step": 448200
    },
    {
      "epoch": 4.090627053069567,
      "grad_norm": 3.9661314487457275,
      "learning_rate": 4.659114412244203e-05,
      "loss": 0.7283,
      "step": 448300
    },
    {
      "epoch": 4.091539528432732,
      "grad_norm": 4.096573829650879,
      "learning_rate": 4.659038372630606e-05,
      "loss": 0.7122,
      "step": 448400
    },
    {
      "epoch": 4.092452003795898,
      "grad_norm": 3.9885780811309814,
      "learning_rate": 4.658962333017009e-05,
      "loss": 0.6942,
      "step": 448500
    },
    {
      "epoch": 4.093364479159063,
      "grad_norm": 4.208357334136963,
      "learning_rate": 4.6588862934034113e-05,
      "loss": 0.7428,
      "step": 448600
    },
    {
      "epoch": 4.094276954522228,
      "grad_norm": 3.5092415809631348,
      "learning_rate": 4.658810253789815e-05,
      "loss": 0.6974,
      "step": 448700
    },
    {
      "epoch": 4.0951894298853935,
      "grad_norm": 4.37342643737793,
      "learning_rate": 4.6587342141762174e-05,
      "loss": 0.7402,
      "step": 448800
    },
    {
      "epoch": 4.096101905248558,
      "grad_norm": 3.6505544185638428,
      "learning_rate": 4.6586581745626204e-05,
      "loss": 0.7373,
      "step": 448900
    },
    {
      "epoch": 4.097014380611723,
      "grad_norm": 3.6952550411224365,
      "learning_rate": 4.6585821349490234e-05,
      "loss": 0.7363,
      "step": 449000
    },
    {
      "epoch": 4.0979268559748885,
      "grad_norm": 4.606664657592773,
      "learning_rate": 4.658506095335426e-05,
      "loss": 0.7136,
      "step": 449100
    },
    {
      "epoch": 4.098839331338054,
      "grad_norm": 4.198788166046143,
      "learning_rate": 4.6584300557218294e-05,
      "loss": 0.7272,
      "step": 449200
    },
    {
      "epoch": 4.099751806701219,
      "grad_norm": 4.189878463745117,
      "learning_rate": 4.658354016108232e-05,
      "loss": 0.7438,
      "step": 449300
    },
    {
      "epoch": 4.100664282064384,
      "grad_norm": 2.8435027599334717,
      "learning_rate": 4.658277976494635e-05,
      "loss": 0.7166,
      "step": 449400
    },
    {
      "epoch": 4.10157675742755,
      "grad_norm": 4.107283115386963,
      "learning_rate": 4.658201936881038e-05,
      "loss": 0.6957,
      "step": 449500
    },
    {
      "epoch": 4.102489232790715,
      "grad_norm": 3.7519702911376953,
      "learning_rate": 4.658125897267441e-05,
      "loss": 0.7017,
      "step": 449600
    },
    {
      "epoch": 4.10340170815388,
      "grad_norm": 3.9356257915496826,
      "learning_rate": 4.658049857653843e-05,
      "loss": 0.712,
      "step": 449700
    },
    {
      "epoch": 4.104314183517045,
      "grad_norm": 3.8730034828186035,
      "learning_rate": 4.657973818040247e-05,
      "loss": 0.7296,
      "step": 449800
    },
    {
      "epoch": 4.10522665888021,
      "grad_norm": 4.476144313812256,
      "learning_rate": 4.657897778426649e-05,
      "loss": 0.7381,
      "step": 449900
    },
    {
      "epoch": 4.106139134243375,
      "grad_norm": 4.492051601409912,
      "learning_rate": 4.657821738813052e-05,
      "loss": 0.7014,
      "step": 450000
    },
    {
      "epoch": 4.107051609606541,
      "grad_norm": 4.0773606300354,
      "learning_rate": 4.657745699199455e-05,
      "loss": 0.7287,
      "step": 450100
    },
    {
      "epoch": 4.107964084969706,
      "grad_norm": 4.2753424644470215,
      "learning_rate": 4.657669659585858e-05,
      "loss": 0.7523,
      "step": 450200
    },
    {
      "epoch": 4.108876560332871,
      "grad_norm": 5.028346538543701,
      "learning_rate": 4.657593619972261e-05,
      "loss": 0.7029,
      "step": 450300
    },
    {
      "epoch": 4.1097890356960365,
      "grad_norm": 4.072327136993408,
      "learning_rate": 4.657517580358664e-05,
      "loss": 0.7573,
      "step": 450400
    },
    {
      "epoch": 4.110701511059202,
      "grad_norm": 4.034938812255859,
      "learning_rate": 4.6574415407450664e-05,
      "loss": 0.7083,
      "step": 450500
    },
    {
      "epoch": 4.111613986422366,
      "grad_norm": 4.92597770690918,
      "learning_rate": 4.65736550113147e-05,
      "loss": 0.6895,
      "step": 450600
    },
    {
      "epoch": 4.1125264617855315,
      "grad_norm": 3.8753485679626465,
      "learning_rate": 4.6572894615178725e-05,
      "loss": 0.7176,
      "step": 450700
    },
    {
      "epoch": 4.113438937148697,
      "grad_norm": 5.356464862823486,
      "learning_rate": 4.6572134219042755e-05,
      "loss": 0.7418,
      "step": 450800
    },
    {
      "epoch": 4.114351412511862,
      "grad_norm": 4.649652004241943,
      "learning_rate": 4.6571373822906785e-05,
      "loss": 0.709,
      "step": 450900
    },
    {
      "epoch": 4.115263887875027,
      "grad_norm": 4.441019058227539,
      "learning_rate": 4.6570613426770815e-05,
      "loss": 0.769,
      "step": 451000
    },
    {
      "epoch": 4.116176363238193,
      "grad_norm": 4.290393829345703,
      "learning_rate": 4.656985303063484e-05,
      "loss": 0.7169,
      "step": 451100
    },
    {
      "epoch": 4.117088838601358,
      "grad_norm": 4.446019649505615,
      "learning_rate": 4.6569092634498875e-05,
      "loss": 0.7161,
      "step": 451200
    },
    {
      "epoch": 4.118001313964523,
      "grad_norm": 3.9044954776763916,
      "learning_rate": 4.65683322383629e-05,
      "loss": 0.7379,
      "step": 451300
    },
    {
      "epoch": 4.118913789327689,
      "grad_norm": 4.037796497344971,
      "learning_rate": 4.656757184222693e-05,
      "loss": 0.7195,
      "step": 451400
    },
    {
      "epoch": 4.119826264690853,
      "grad_norm": 3.4491076469421387,
      "learning_rate": 4.656681144609096e-05,
      "loss": 0.7152,
      "step": 451500
    },
    {
      "epoch": 4.120738740054018,
      "grad_norm": 4.386538505554199,
      "learning_rate": 4.656605104995498e-05,
      "loss": 0.7196,
      "step": 451600
    },
    {
      "epoch": 4.121651215417184,
      "grad_norm": 4.001733303070068,
      "learning_rate": 4.656529065381902e-05,
      "loss": 0.748,
      "step": 451700
    },
    {
      "epoch": 4.122563690780349,
      "grad_norm": 4.050789833068848,
      "learning_rate": 4.656453025768304e-05,
      "loss": 0.7226,
      "step": 451800
    },
    {
      "epoch": 4.123476166143514,
      "grad_norm": 3.6417758464813232,
      "learning_rate": 4.656376986154707e-05,
      "loss": 0.7057,
      "step": 451900
    },
    {
      "epoch": 4.1243886415066795,
      "grad_norm": 3.7444775104522705,
      "learning_rate": 4.65630094654111e-05,
      "loss": 0.7291,
      "step": 452000
    },
    {
      "epoch": 4.125301116869845,
      "grad_norm": 3.7729084491729736,
      "learning_rate": 4.656224906927513e-05,
      "loss": 0.732,
      "step": 452100
    },
    {
      "epoch": 4.12621359223301,
      "grad_norm": 3.850170612335205,
      "learning_rate": 4.656148867313916e-05,
      "loss": 0.7108,
      "step": 452200
    },
    {
      "epoch": 4.1271260675961745,
      "grad_norm": 3.7368886470794678,
      "learning_rate": 4.656072827700319e-05,
      "loss": 0.7122,
      "step": 452300
    },
    {
      "epoch": 4.12803854295934,
      "grad_norm": 5.154019355773926,
      "learning_rate": 4.6559967880867215e-05,
      "loss": 0.7522,
      "step": 452400
    },
    {
      "epoch": 4.128951018322505,
      "grad_norm": 4.546103477478027,
      "learning_rate": 4.6559207484731245e-05,
      "loss": 0.7071,
      "step": 452500
    },
    {
      "epoch": 4.12986349368567,
      "grad_norm": 3.833658218383789,
      "learning_rate": 4.6558447088595275e-05,
      "loss": 0.6708,
      "step": 452600
    },
    {
      "epoch": 4.130775969048836,
      "grad_norm": 3.7088205814361572,
      "learning_rate": 4.6557686692459306e-05,
      "loss": 0.7132,
      "step": 452700
    },
    {
      "epoch": 4.131688444412001,
      "grad_norm": 4.7330732345581055,
      "learning_rate": 4.6556926296323336e-05,
      "loss": 0.7351,
      "step": 452800
    },
    {
      "epoch": 4.132600919775166,
      "grad_norm": 3.7048263549804688,
      "learning_rate": 4.6556165900187366e-05,
      "loss": 0.7237,
      "step": 452900
    },
    {
      "epoch": 4.133513395138332,
      "grad_norm": 5.066310405731201,
      "learning_rate": 4.655540550405139e-05,
      "loss": 0.6947,
      "step": 453000
    },
    {
      "epoch": 4.134425870501497,
      "grad_norm": 3.8342833518981934,
      "learning_rate": 4.6554645107915426e-05,
      "loss": 0.7666,
      "step": 453100
    },
    {
      "epoch": 4.135338345864661,
      "grad_norm": 3.470181465148926,
      "learning_rate": 4.655388471177945e-05,
      "loss": 0.7058,
      "step": 453200
    },
    {
      "epoch": 4.136250821227827,
      "grad_norm": 4.31995964050293,
      "learning_rate": 4.655312431564348e-05,
      "loss": 0.6729,
      "step": 453300
    },
    {
      "epoch": 4.137163296590992,
      "grad_norm": 3.9045627117156982,
      "learning_rate": 4.655236391950751e-05,
      "loss": 0.7234,
      "step": 453400
    },
    {
      "epoch": 4.138075771954157,
      "grad_norm": 4.025768756866455,
      "learning_rate": 4.655160352337154e-05,
      "loss": 0.6951,
      "step": 453500
    },
    {
      "epoch": 4.1389882473173225,
      "grad_norm": 4.061039924621582,
      "learning_rate": 4.655084312723557e-05,
      "loss": 0.7063,
      "step": 453600
    },
    {
      "epoch": 4.139900722680488,
      "grad_norm": 4.674797534942627,
      "learning_rate": 4.65500827310996e-05,
      "loss": 0.7262,
      "step": 453700
    },
    {
      "epoch": 4.140813198043653,
      "grad_norm": 4.242337226867676,
      "learning_rate": 4.654932233496362e-05,
      "loss": 0.6646,
      "step": 453800
    },
    {
      "epoch": 4.141725673406818,
      "grad_norm": 5.044229030609131,
      "learning_rate": 4.654856193882765e-05,
      "loss": 0.7056,
      "step": 453900
    },
    {
      "epoch": 4.142638148769983,
      "grad_norm": 3.6799614429473877,
      "learning_rate": 4.654780154269168e-05,
      "loss": 0.7189,
      "step": 454000
    },
    {
      "epoch": 4.143550624133148,
      "grad_norm": 4.038507461547852,
      "learning_rate": 4.654704114655571e-05,
      "loss": 0.748,
      "step": 454100
    },
    {
      "epoch": 4.144463099496313,
      "grad_norm": 3.5565803050994873,
      "learning_rate": 4.654628075041974e-05,
      "loss": 0.7325,
      "step": 454200
    },
    {
      "epoch": 4.145375574859479,
      "grad_norm": 3.6432838439941406,
      "learning_rate": 4.654552035428377e-05,
      "loss": 0.6896,
      "step": 454300
    },
    {
      "epoch": 4.146288050222644,
      "grad_norm": 4.305571556091309,
      "learning_rate": 4.6544759958147796e-05,
      "loss": 0.7271,
      "step": 454400
    },
    {
      "epoch": 4.147200525585809,
      "grad_norm": 3.980293035507202,
      "learning_rate": 4.6543999562011826e-05,
      "loss": 0.7413,
      "step": 454500
    },
    {
      "epoch": 4.148113000948975,
      "grad_norm": 3.3565685749053955,
      "learning_rate": 4.6543239165875856e-05,
      "loss": 0.7297,
      "step": 454600
    },
    {
      "epoch": 4.14902547631214,
      "grad_norm": 4.890064716339111,
      "learning_rate": 4.6542478769739887e-05,
      "loss": 0.7303,
      "step": 454700
    },
    {
      "epoch": 4.149937951675305,
      "grad_norm": 4.226291656494141,
      "learning_rate": 4.654171837360392e-05,
      "loss": 0.729,
      "step": 454800
    },
    {
      "epoch": 4.15085042703847,
      "grad_norm": 4.316246509552002,
      "learning_rate": 4.654095797746794e-05,
      "loss": 0.6894,
      "step": 454900
    },
    {
      "epoch": 4.151762902401635,
      "grad_norm": 5.93247127532959,
      "learning_rate": 4.654019758133198e-05,
      "loss": 0.7159,
      "step": 455000
    },
    {
      "epoch": 4.1526753777648,
      "grad_norm": 3.675546884536743,
      "learning_rate": 4.6539437185196e-05,
      "loss": 0.7103,
      "step": 455100
    },
    {
      "epoch": 4.1535878531279655,
      "grad_norm": 3.790408134460449,
      "learning_rate": 4.653867678906003e-05,
      "loss": 0.7581,
      "step": 455200
    },
    {
      "epoch": 4.154500328491131,
      "grad_norm": 3.97733998298645,
      "learning_rate": 4.653791639292406e-05,
      "loss": 0.698,
      "step": 455300
    },
    {
      "epoch": 4.155412803854296,
      "grad_norm": 4.170727729797363,
      "learning_rate": 4.653715599678809e-05,
      "loss": 0.7568,
      "step": 455400
    },
    {
      "epoch": 4.156325279217461,
      "grad_norm": 3.952105760574341,
      "learning_rate": 4.6536395600652114e-05,
      "loss": 0.7192,
      "step": 455500
    },
    {
      "epoch": 4.157237754580627,
      "grad_norm": 3.9854722023010254,
      "learning_rate": 4.653563520451615e-05,
      "loss": 0.7267,
      "step": 455600
    },
    {
      "epoch": 4.158150229943791,
      "grad_norm": 3.439004421234131,
      "learning_rate": 4.6534874808380174e-05,
      "loss": 0.6566,
      "step": 455700
    },
    {
      "epoch": 4.159062705306956,
      "grad_norm": 3.4021284580230713,
      "learning_rate": 4.6534114412244204e-05,
      "loss": 0.7496,
      "step": 455800
    },
    {
      "epoch": 4.159975180670122,
      "grad_norm": 3.8835947513580322,
      "learning_rate": 4.6533354016108234e-05,
      "loss": 0.7159,
      "step": 455900
    },
    {
      "epoch": 4.160887656033287,
      "grad_norm": 4.237144947052002,
      "learning_rate": 4.6532593619972264e-05,
      "loss": 0.7208,
      "step": 456000
    },
    {
      "epoch": 4.161800131396452,
      "grad_norm": 3.8433339595794678,
      "learning_rate": 4.6531833223836294e-05,
      "loss": 0.7092,
      "step": 456100
    },
    {
      "epoch": 4.162712606759618,
      "grad_norm": 3.5008187294006348,
      "learning_rate": 4.6531072827700324e-05,
      "loss": 0.7595,
      "step": 456200
    },
    {
      "epoch": 4.163625082122783,
      "grad_norm": 4.9448018074035645,
      "learning_rate": 4.653031243156435e-05,
      "loss": 0.7254,
      "step": 456300
    },
    {
      "epoch": 4.164537557485948,
      "grad_norm": 3.592310667037964,
      "learning_rate": 4.6529552035428384e-05,
      "loss": 0.7226,
      "step": 456400
    },
    {
      "epoch": 4.1654500328491135,
      "grad_norm": 3.6881792545318604,
      "learning_rate": 4.652879163929241e-05,
      "loss": 0.7422,
      "step": 456500
    },
    {
      "epoch": 4.166362508212278,
      "grad_norm": 4.317690372467041,
      "learning_rate": 4.652803124315644e-05,
      "loss": 0.7234,
      "step": 456600
    },
    {
      "epoch": 4.167274983575443,
      "grad_norm": 4.54337215423584,
      "learning_rate": 4.652727084702047e-05,
      "loss": 0.7197,
      "step": 456700
    },
    {
      "epoch": 4.1681874589386085,
      "grad_norm": 4.253677845001221,
      "learning_rate": 4.65265104508845e-05,
      "loss": 0.7421,
      "step": 456800
    },
    {
      "epoch": 4.169099934301774,
      "grad_norm": 4.027900218963623,
      "learning_rate": 4.652575005474852e-05,
      "loss": 0.7036,
      "step": 456900
    },
    {
      "epoch": 4.170012409664939,
      "grad_norm": 4.533304214477539,
      "learning_rate": 4.652498965861256e-05,
      "loss": 0.6761,
      "step": 457000
    },
    {
      "epoch": 4.170924885028104,
      "grad_norm": 3.455029010772705,
      "learning_rate": 4.652422926247658e-05,
      "loss": 0.7183,
      "step": 457100
    },
    {
      "epoch": 4.17183736039127,
      "grad_norm": 3.897984266281128,
      "learning_rate": 4.652346886634061e-05,
      "loss": 0.7184,
      "step": 457200
    },
    {
      "epoch": 4.172749835754435,
      "grad_norm": 4.19802188873291,
      "learning_rate": 4.652270847020464e-05,
      "loss": 0.691,
      "step": 457300
    },
    {
      "epoch": 4.173662311117599,
      "grad_norm": 5.528872013092041,
      "learning_rate": 4.6521948074068664e-05,
      "loss": 0.7377,
      "step": 457400
    },
    {
      "epoch": 4.174574786480765,
      "grad_norm": 4.216035842895508,
      "learning_rate": 4.65211876779327e-05,
      "loss": 0.7267,
      "step": 457500
    },
    {
      "epoch": 4.17548726184393,
      "grad_norm": 4.273606777191162,
      "learning_rate": 4.6520427281796725e-05,
      "loss": 0.6968,
      "step": 457600
    },
    {
      "epoch": 4.176399737207095,
      "grad_norm": 4.077249526977539,
      "learning_rate": 4.6519666885660755e-05,
      "loss": 0.7103,
      "step": 457700
    },
    {
      "epoch": 4.177312212570261,
      "grad_norm": 4.4310736656188965,
      "learning_rate": 4.6518906489524785e-05,
      "loss": 0.7064,
      "step": 457800
    },
    {
      "epoch": 4.178224687933426,
      "grad_norm": 4.231882572174072,
      "learning_rate": 4.6518146093388815e-05,
      "loss": 0.7386,
      "step": 457900
    },
    {
      "epoch": 4.179137163296591,
      "grad_norm": 3.556464433670044,
      "learning_rate": 4.651738569725284e-05,
      "loss": 0.704,
      "step": 458000
    },
    {
      "epoch": 4.1800496386597565,
      "grad_norm": 4.202817440032959,
      "learning_rate": 4.6516625301116875e-05,
      "loss": 0.7253,
      "step": 458100
    },
    {
      "epoch": 4.180962114022922,
      "grad_norm": 3.825798511505127,
      "learning_rate": 4.65158649049809e-05,
      "loss": 0.7355,
      "step": 458200
    },
    {
      "epoch": 4.181874589386086,
      "grad_norm": 4.014496326446533,
      "learning_rate": 4.651510450884493e-05,
      "loss": 0.7409,
      "step": 458300
    },
    {
      "epoch": 4.1827870647492515,
      "grad_norm": 4.918982028961182,
      "learning_rate": 4.651434411270896e-05,
      "loss": 0.7067,
      "step": 458400
    },
    {
      "epoch": 4.183699540112417,
      "grad_norm": 4.120652198791504,
      "learning_rate": 4.651358371657299e-05,
      "loss": 0.7139,
      "step": 458500
    },
    {
      "epoch": 4.184612015475582,
      "grad_norm": 4.259205341339111,
      "learning_rate": 4.651282332043702e-05,
      "loss": 0.6894,
      "step": 458600
    },
    {
      "epoch": 4.1855244908387474,
      "grad_norm": 4.525429725646973,
      "learning_rate": 4.651206292430105e-05,
      "loss": 0.7016,
      "step": 458700
    },
    {
      "epoch": 4.186436966201913,
      "grad_norm": 4.340108394622803,
      "learning_rate": 4.651130252816507e-05,
      "loss": 0.7414,
      "step": 458800
    },
    {
      "epoch": 4.187349441565078,
      "grad_norm": 4.4957356452941895,
      "learning_rate": 4.651054213202911e-05,
      "loss": 0.7346,
      "step": 458900
    },
    {
      "epoch": 4.188261916928243,
      "grad_norm": 4.45215368270874,
      "learning_rate": 4.650978173589313e-05,
      "loss": 0.7561,
      "step": 459000
    },
    {
      "epoch": 4.189174392291408,
      "grad_norm": 4.518923282623291,
      "learning_rate": 4.650902133975716e-05,
      "loss": 0.7137,
      "step": 459100
    },
    {
      "epoch": 4.190086867654573,
      "grad_norm": 4.052689075469971,
      "learning_rate": 4.650826094362119e-05,
      "loss": 0.7407,
      "step": 459200
    },
    {
      "epoch": 4.190999343017738,
      "grad_norm": 4.820469856262207,
      "learning_rate": 4.650750054748522e-05,
      "loss": 0.698,
      "step": 459300
    },
    {
      "epoch": 4.191911818380904,
      "grad_norm": 4.037633419036865,
      "learning_rate": 4.6506740151349245e-05,
      "loss": 0.7517,
      "step": 459400
    },
    {
      "epoch": 4.192824293744069,
      "grad_norm": 3.4755115509033203,
      "learning_rate": 4.650597975521328e-05,
      "loss": 0.7287,
      "step": 459500
    },
    {
      "epoch": 4.193736769107234,
      "grad_norm": 4.2644147872924805,
      "learning_rate": 4.6505219359077306e-05,
      "loss": 0.7651,
      "step": 459600
    },
    {
      "epoch": 4.1946492444703996,
      "grad_norm": 5.5220489501953125,
      "learning_rate": 4.6504458962941336e-05,
      "loss": 0.7272,
      "step": 459700
    },
    {
      "epoch": 4.195561719833565,
      "grad_norm": 3.24365234375,
      "learning_rate": 4.6503698566805366e-05,
      "loss": 0.7229,
      "step": 459800
    },
    {
      "epoch": 4.196474195196729,
      "grad_norm": 3.9069361686706543,
      "learning_rate": 4.6502938170669396e-05,
      "loss": 0.7009,
      "step": 459900
    },
    {
      "epoch": 4.197386670559895,
      "grad_norm": 5.152132034301758,
      "learning_rate": 4.6502177774533426e-05,
      "loss": 0.7335,
      "step": 460000
    },
    {
      "epoch": 4.19829914592306,
      "grad_norm": 3.997020721435547,
      "learning_rate": 4.6501417378397456e-05,
      "loss": 0.714,
      "step": 460100
    },
    {
      "epoch": 4.199211621286225,
      "grad_norm": 3.7196784019470215,
      "learning_rate": 4.650065698226148e-05,
      "loss": 0.7063,
      "step": 460200
    },
    {
      "epoch": 4.2001240966493905,
      "grad_norm": 4.571353435516357,
      "learning_rate": 4.649989658612551e-05,
      "loss": 0.7333,
      "step": 460300
    },
    {
      "epoch": 4.201036572012556,
      "grad_norm": 3.9891135692596436,
      "learning_rate": 4.649913618998954e-05,
      "loss": 0.7291,
      "step": 460400
    },
    {
      "epoch": 4.201949047375721,
      "grad_norm": 3.837181806564331,
      "learning_rate": 4.649837579385356e-05,
      "loss": 0.7431,
      "step": 460500
    },
    {
      "epoch": 4.202861522738886,
      "grad_norm": 3.7140846252441406,
      "learning_rate": 4.64976153977176e-05,
      "loss": 0.7093,
      "step": 460600
    },
    {
      "epoch": 4.203773998102052,
      "grad_norm": 4.0206804275512695,
      "learning_rate": 4.649685500158162e-05,
      "loss": 0.7411,
      "step": 460700
    },
    {
      "epoch": 4.204686473465216,
      "grad_norm": 4.319350242614746,
      "learning_rate": 4.649609460544565e-05,
      "loss": 0.7499,
      "step": 460800
    },
    {
      "epoch": 4.205598948828381,
      "grad_norm": 4.620928764343262,
      "learning_rate": 4.649533420930968e-05,
      "loss": 0.7409,
      "step": 460900
    },
    {
      "epoch": 4.206511424191547,
      "grad_norm": 3.862084150314331,
      "learning_rate": 4.649457381317371e-05,
      "loss": 0.7141,
      "step": 461000
    },
    {
      "epoch": 4.207423899554712,
      "grad_norm": 4.022789001464844,
      "learning_rate": 4.649381341703774e-05,
      "loss": 0.7256,
      "step": 461100
    },
    {
      "epoch": 4.208336374917877,
      "grad_norm": 4.90600061416626,
      "learning_rate": 4.649305302090177e-05,
      "loss": 0.7062,
      "step": 461200
    },
    {
      "epoch": 4.209248850281043,
      "grad_norm": 3.758324384689331,
      "learning_rate": 4.6492292624765796e-05,
      "loss": 0.7456,
      "step": 461300
    },
    {
      "epoch": 4.210161325644208,
      "grad_norm": 5.773505687713623,
      "learning_rate": 4.649153222862983e-05,
      "loss": 0.6867,
      "step": 461400
    },
    {
      "epoch": 4.211073801007373,
      "grad_norm": 5.556274890899658,
      "learning_rate": 4.6490771832493857e-05,
      "loss": 0.7227,
      "step": 461500
    },
    {
      "epoch": 4.211986276370538,
      "grad_norm": 4.051458358764648,
      "learning_rate": 4.649001143635789e-05,
      "loss": 0.7098,
      "step": 461600
    },
    {
      "epoch": 4.212898751733703,
      "grad_norm": 4.505665302276611,
      "learning_rate": 4.648925104022192e-05,
      "loss": 0.6971,
      "step": 461700
    },
    {
      "epoch": 4.213811227096868,
      "grad_norm": 3.7424092292785645,
      "learning_rate": 4.648849064408595e-05,
      "loss": 0.7446,
      "step": 461800
    },
    {
      "epoch": 4.2147237024600335,
      "grad_norm": 3.8896543979644775,
      "learning_rate": 4.648773024794997e-05,
      "loss": 0.6833,
      "step": 461900
    },
    {
      "epoch": 4.215636177823199,
      "grad_norm": 4.329073429107666,
      "learning_rate": 4.648696985181401e-05,
      "loss": 0.756,
      "step": 462000
    },
    {
      "epoch": 4.216548653186364,
      "grad_norm": 3.853903293609619,
      "learning_rate": 4.648620945567803e-05,
      "loss": 0.6951,
      "step": 462100
    },
    {
      "epoch": 4.217461128549529,
      "grad_norm": 4.3834381103515625,
      "learning_rate": 4.648544905954206e-05,
      "loss": 0.7064,
      "step": 462200
    },
    {
      "epoch": 4.218373603912695,
      "grad_norm": 4.208587169647217,
      "learning_rate": 4.648468866340609e-05,
      "loss": 0.7113,
      "step": 462300
    },
    {
      "epoch": 4.21928607927586,
      "grad_norm": 3.811715602874756,
      "learning_rate": 4.648392826727012e-05,
      "loss": 0.6941,
      "step": 462400
    },
    {
      "epoch": 4.220198554639024,
      "grad_norm": 3.800283432006836,
      "learning_rate": 4.648316787113415e-05,
      "loss": 0.731,
      "step": 462500
    },
    {
      "epoch": 4.22111103000219,
      "grad_norm": 4.34367036819458,
      "learning_rate": 4.648240747499818e-05,
      "loss": 0.7618,
      "step": 462600
    },
    {
      "epoch": 4.222023505365355,
      "grad_norm": 4.683267116546631,
      "learning_rate": 4.6481647078862204e-05,
      "loss": 0.7135,
      "step": 462700
    },
    {
      "epoch": 4.22293598072852,
      "grad_norm": 5.160248756408691,
      "learning_rate": 4.648088668272624e-05,
      "loss": 0.7026,
      "step": 462800
    },
    {
      "epoch": 4.223848456091686,
      "grad_norm": 4.330808639526367,
      "learning_rate": 4.6480126286590264e-05,
      "loss": 0.6875,
      "step": 462900
    },
    {
      "epoch": 4.224760931454851,
      "grad_norm": 3.033317804336548,
      "learning_rate": 4.647936589045429e-05,
      "loss": 0.7083,
      "step": 463000
    },
    {
      "epoch": 4.225673406818016,
      "grad_norm": 4.568416118621826,
      "learning_rate": 4.6478605494318324e-05,
      "loss": 0.7394,
      "step": 463100
    },
    {
      "epoch": 4.2265858821811815,
      "grad_norm": 4.542629718780518,
      "learning_rate": 4.647784509818235e-05,
      "loss": 0.7418,
      "step": 463200
    },
    {
      "epoch": 4.227498357544346,
      "grad_norm": 4.04955530166626,
      "learning_rate": 4.647708470204638e-05,
      "loss": 0.7079,
      "step": 463300
    },
    {
      "epoch": 4.228410832907511,
      "grad_norm": 3.771418333053589,
      "learning_rate": 4.647632430591041e-05,
      "loss": 0.7405,
      "step": 463400
    },
    {
      "epoch": 4.2293233082706765,
      "grad_norm": 3.81205153465271,
      "learning_rate": 4.647556390977444e-05,
      "loss": 0.6982,
      "step": 463500
    },
    {
      "epoch": 4.230235783633842,
      "grad_norm": 4.493544101715088,
      "learning_rate": 4.647480351363847e-05,
      "loss": 0.6806,
      "step": 463600
    },
    {
      "epoch": 4.231148258997007,
      "grad_norm": 3.4068777561187744,
      "learning_rate": 4.64740431175025e-05,
      "loss": 0.7246,
      "step": 463700
    },
    {
      "epoch": 4.232060734360172,
      "grad_norm": 3.0035557746887207,
      "learning_rate": 4.647328272136652e-05,
      "loss": 0.7211,
      "step": 463800
    },
    {
      "epoch": 4.232973209723338,
      "grad_norm": 4.4025959968566895,
      "learning_rate": 4.647252232523056e-05,
      "loss": 0.7668,
      "step": 463900
    },
    {
      "epoch": 4.233885685086503,
      "grad_norm": 4.50425910949707,
      "learning_rate": 4.647176192909458e-05,
      "loss": 0.7155,
      "step": 464000
    },
    {
      "epoch": 4.234798160449668,
      "grad_norm": 2.8785312175750732,
      "learning_rate": 4.647100153295861e-05,
      "loss": 0.6947,
      "step": 464100
    },
    {
      "epoch": 4.235710635812833,
      "grad_norm": 4.307662010192871,
      "learning_rate": 4.647024113682264e-05,
      "loss": 0.7247,
      "step": 464200
    },
    {
      "epoch": 4.236623111175998,
      "grad_norm": 3.961496353149414,
      "learning_rate": 4.646948074068667e-05,
      "loss": 0.7142,
      "step": 464300
    },
    {
      "epoch": 4.237535586539163,
      "grad_norm": 4.104386806488037,
      "learning_rate": 4.6468720344550695e-05,
      "loss": 0.7257,
      "step": 464400
    },
    {
      "epoch": 4.238448061902329,
      "grad_norm": 4.405999660491943,
      "learning_rate": 4.646795994841473e-05,
      "loss": 0.7122,
      "step": 464500
    },
    {
      "epoch": 4.239360537265494,
      "grad_norm": 4.055797100067139,
      "learning_rate": 4.6467199552278755e-05,
      "loss": 0.7673,
      "step": 464600
    },
    {
      "epoch": 4.240273012628659,
      "grad_norm": 4.628720760345459,
      "learning_rate": 4.6466439156142785e-05,
      "loss": 0.6964,
      "step": 464700
    },
    {
      "epoch": 4.2411854879918245,
      "grad_norm": 3.951603889465332,
      "learning_rate": 4.6465678760006815e-05,
      "loss": 0.7354,
      "step": 464800
    },
    {
      "epoch": 4.24209796335499,
      "grad_norm": 4.259721279144287,
      "learning_rate": 4.6464918363870845e-05,
      "loss": 0.711,
      "step": 464900
    },
    {
      "epoch": 4.243010438718154,
      "grad_norm": 4.299398899078369,
      "learning_rate": 4.6464157967734875e-05,
      "loss": 0.7375,
      "step": 465000
    },
    {
      "epoch": 4.2439229140813195,
      "grad_norm": 3.9217987060546875,
      "learning_rate": 4.6463397571598905e-05,
      "loss": 0.7299,
      "step": 465100
    },
    {
      "epoch": 4.244835389444485,
      "grad_norm": 3.821410894393921,
      "learning_rate": 4.646263717546293e-05,
      "loss": 0.7003,
      "step": 465200
    },
    {
      "epoch": 4.24574786480765,
      "grad_norm": 3.242408514022827,
      "learning_rate": 4.6461876779326965e-05,
      "loss": 0.7277,
      "step": 465300
    },
    {
      "epoch": 4.246660340170815,
      "grad_norm": 4.448231220245361,
      "learning_rate": 4.646111638319099e-05,
      "loss": 0.7148,
      "step": 465400
    },
    {
      "epoch": 4.247572815533981,
      "grad_norm": 3.938429832458496,
      "learning_rate": 4.646035598705502e-05,
      "loss": 0.7296,
      "step": 465500
    },
    {
      "epoch": 4.248485290897146,
      "grad_norm": 4.402093887329102,
      "learning_rate": 4.645959559091905e-05,
      "loss": 0.6975,
      "step": 465600
    },
    {
      "epoch": 4.249397766260311,
      "grad_norm": 4.700819492340088,
      "learning_rate": 4.645883519478308e-05,
      "loss": 0.7503,
      "step": 465700
    },
    {
      "epoch": 4.250310241623476,
      "grad_norm": 3.5488195419311523,
      "learning_rate": 4.645807479864711e-05,
      "loss": 0.7351,
      "step": 465800
    },
    {
      "epoch": 4.251222716986641,
      "grad_norm": 4.572600841522217,
      "learning_rate": 4.645731440251113e-05,
      "loss": 0.722,
      "step": 465900
    },
    {
      "epoch": 4.252135192349806,
      "grad_norm": 4.282411575317383,
      "learning_rate": 4.645655400637516e-05,
      "loss": 0.7387,
      "step": 466000
    },
    {
      "epoch": 4.253047667712972,
      "grad_norm": 5.905667304992676,
      "learning_rate": 4.645579361023919e-05,
      "loss": 0.7131,
      "step": 466100
    },
    {
      "epoch": 4.253960143076137,
      "grad_norm": 4.142098903656006,
      "learning_rate": 4.645503321410322e-05,
      "loss": 0.7339,
      "step": 466200
    },
    {
      "epoch": 4.254872618439302,
      "grad_norm": 4.208830833435059,
      "learning_rate": 4.6454272817967246e-05,
      "loss": 0.7078,
      "step": 466300
    },
    {
      "epoch": 4.2557850938024675,
      "grad_norm": 4.286264419555664,
      "learning_rate": 4.645351242183128e-05,
      "loss": 0.7101,
      "step": 466400
    },
    {
      "epoch": 4.256697569165633,
      "grad_norm": 4.4613189697265625,
      "learning_rate": 4.6452752025695306e-05,
      "loss": 0.7161,
      "step": 466500
    },
    {
      "epoch": 4.257610044528798,
      "grad_norm": 3.7260916233062744,
      "learning_rate": 4.6451991629559336e-05,
      "loss": 0.6645,
      "step": 466600
    },
    {
      "epoch": 4.2585225198919625,
      "grad_norm": 4.741129398345947,
      "learning_rate": 4.6451231233423366e-05,
      "loss": 0.7497,
      "step": 466700
    },
    {
      "epoch": 4.259434995255128,
      "grad_norm": 3.2985734939575195,
      "learning_rate": 4.6450470837287396e-05,
      "loss": 0.7172,
      "step": 466800
    },
    {
      "epoch": 4.260347470618293,
      "grad_norm": 4.3488922119140625,
      "learning_rate": 4.6449710441151426e-05,
      "loss": 0.6977,
      "step": 466900
    },
    {
      "epoch": 4.261259945981458,
      "grad_norm": 3.8111279010772705,
      "learning_rate": 4.6448950045015456e-05,
      "loss": 0.7257,
      "step": 467000
    },
    {
      "epoch": 4.262172421344624,
      "grad_norm": 3.430642604827881,
      "learning_rate": 4.644818964887948e-05,
      "loss": 0.714,
      "step": 467100
    },
    {
      "epoch": 4.263084896707789,
      "grad_norm": 5.419673919677734,
      "learning_rate": 4.6447429252743516e-05,
      "loss": 0.7484,
      "step": 467200
    },
    {
      "epoch": 4.263997372070954,
      "grad_norm": 3.8754959106445312,
      "learning_rate": 4.644666885660754e-05,
      "loss": 0.744,
      "step": 467300
    },
    {
      "epoch": 4.26490984743412,
      "grad_norm": 5.304737091064453,
      "learning_rate": 4.644590846047157e-05,
      "loss": 0.716,
      "step": 467400
    },
    {
      "epoch": 4.265822322797284,
      "grad_norm": 3.6809942722320557,
      "learning_rate": 4.64451480643356e-05,
      "loss": 0.7003,
      "step": 467500
    },
    {
      "epoch": 4.266734798160449,
      "grad_norm": 3.658895254135132,
      "learning_rate": 4.644438766819963e-05,
      "loss": 0.7026,
      "step": 467600
    },
    {
      "epoch": 4.267647273523615,
      "grad_norm": 4.415676593780518,
      "learning_rate": 4.644362727206365e-05,
      "loss": 0.7292,
      "step": 467700
    },
    {
      "epoch": 4.26855974888678,
      "grad_norm": 4.377502918243408,
      "learning_rate": 4.644286687592769e-05,
      "loss": 0.7067,
      "step": 467800
    },
    {
      "epoch": 4.269472224249945,
      "grad_norm": 4.525350570678711,
      "learning_rate": 4.644210647979171e-05,
      "loss": 0.7102,
      "step": 467900
    },
    {
      "epoch": 4.2703846996131105,
      "grad_norm": 3.449256181716919,
      "learning_rate": 4.644134608365574e-05,
      "loss": 0.7138,
      "step": 468000
    },
    {
      "epoch": 4.271297174976276,
      "grad_norm": 4.331998348236084,
      "learning_rate": 4.644058568751977e-05,
      "loss": 0.7434,
      "step": 468100
    },
    {
      "epoch": 4.272209650339441,
      "grad_norm": 3.772963285446167,
      "learning_rate": 4.64398252913838e-05,
      "loss": 0.7369,
      "step": 468200
    },
    {
      "epoch": 4.273122125702606,
      "grad_norm": 2.8780219554901123,
      "learning_rate": 4.6439064895247833e-05,
      "loss": 0.7279,
      "step": 468300
    },
    {
      "epoch": 4.274034601065771,
      "grad_norm": 3.6575818061828613,
      "learning_rate": 4.6438304499111863e-05,
      "loss": 0.7374,
      "step": 468400
    },
    {
      "epoch": 4.274947076428936,
      "grad_norm": 4.921983242034912,
      "learning_rate": 4.643754410297589e-05,
      "loss": 0.6965,
      "step": 468500
    },
    {
      "epoch": 4.275859551792101,
      "grad_norm": 3.3529305458068848,
      "learning_rate": 4.6436783706839924e-05,
      "loss": 0.6976,
      "step": 468600
    },
    {
      "epoch": 4.276772027155267,
      "grad_norm": 3.4128949642181396,
      "learning_rate": 4.643602331070395e-05,
      "loss": 0.7153,
      "step": 468700
    },
    {
      "epoch": 4.277684502518432,
      "grad_norm": 4.456669807434082,
      "learning_rate": 4.643526291456797e-05,
      "loss": 0.7454,
      "step": 468800
    },
    {
      "epoch": 4.278596977881597,
      "grad_norm": 4.053308010101318,
      "learning_rate": 4.643450251843201e-05,
      "loss": 0.74,
      "step": 468900
    },
    {
      "epoch": 4.279509453244763,
      "grad_norm": 3.19431734085083,
      "learning_rate": 4.643374212229603e-05,
      "loss": 0.7306,
      "step": 469000
    },
    {
      "epoch": 4.280421928607928,
      "grad_norm": 4.179652690887451,
      "learning_rate": 4.643298172616006e-05,
      "loss": 0.7355,
      "step": 469100
    },
    {
      "epoch": 4.281334403971092,
      "grad_norm": 4.017472743988037,
      "learning_rate": 4.643222133002409e-05,
      "loss": 0.6937,
      "step": 469200
    },
    {
      "epoch": 4.282246879334258,
      "grad_norm": 5.048416614532471,
      "learning_rate": 4.643146093388812e-05,
      "loss": 0.6961,
      "step": 469300
    },
    {
      "epoch": 4.283159354697423,
      "grad_norm": 2.2397897243499756,
      "learning_rate": 4.643070053775215e-05,
      "loss": 0.7344,
      "step": 469400
    },
    {
      "epoch": 4.284071830060588,
      "grad_norm": 3.5889596939086914,
      "learning_rate": 4.642994014161618e-05,
      "loss": 0.7245,
      "step": 469500
    },
    {
      "epoch": 4.2849843054237535,
      "grad_norm": 4.5370635986328125,
      "learning_rate": 4.6429179745480204e-05,
      "loss": 0.744,
      "step": 469600
    },
    {
      "epoch": 4.285896780786919,
      "grad_norm": 3.966099739074707,
      "learning_rate": 4.642841934934424e-05,
      "loss": 0.7749,
      "step": 469700
    },
    {
      "epoch": 4.286809256150084,
      "grad_norm": 4.169334411621094,
      "learning_rate": 4.6427658953208264e-05,
      "loss": 0.7346,
      "step": 469800
    },
    {
      "epoch": 4.287721731513249,
      "grad_norm": 3.7896981239318848,
      "learning_rate": 4.6426898557072294e-05,
      "loss": 0.7055,
      "step": 469900
    },
    {
      "epoch": 4.288634206876415,
      "grad_norm": 3.3627007007598877,
      "learning_rate": 4.6426138160936324e-05,
      "loss": 0.7371,
      "step": 470000
    },
    {
      "epoch": 4.289546682239579,
      "grad_norm": 4.209916591644287,
      "learning_rate": 4.6425377764800354e-05,
      "loss": 0.7116,
      "step": 470100
    },
    {
      "epoch": 4.290459157602744,
      "grad_norm": 3.519761562347412,
      "learning_rate": 4.642461736866438e-05,
      "loss": 0.7098,
      "step": 470200
    },
    {
      "epoch": 4.29137163296591,
      "grad_norm": 3.5271878242492676,
      "learning_rate": 4.6423856972528414e-05,
      "loss": 0.7085,
      "step": 470300
    },
    {
      "epoch": 4.292284108329075,
      "grad_norm": 3.030747890472412,
      "learning_rate": 4.642309657639244e-05,
      "loss": 0.7104,
      "step": 470400
    },
    {
      "epoch": 4.29319658369224,
      "grad_norm": 4.44989013671875,
      "learning_rate": 4.642233618025647e-05,
      "loss": 0.7078,
      "step": 470500
    },
    {
      "epoch": 4.294109059055406,
      "grad_norm": 4.708850383758545,
      "learning_rate": 4.64215757841205e-05,
      "loss": 0.7132,
      "step": 470600
    },
    {
      "epoch": 4.295021534418571,
      "grad_norm": 4.773194789886475,
      "learning_rate": 4.642081538798453e-05,
      "loss": 0.7117,
      "step": 470700
    },
    {
      "epoch": 4.295934009781736,
      "grad_norm": 4.017210006713867,
      "learning_rate": 4.642005499184856e-05,
      "loss": 0.7178,
      "step": 470800
    },
    {
      "epoch": 4.296846485144901,
      "grad_norm": 4.581813335418701,
      "learning_rate": 4.641929459571259e-05,
      "loss": 0.7001,
      "step": 470900
    },
    {
      "epoch": 4.297758960508066,
      "grad_norm": 4.089835166931152,
      "learning_rate": 4.641853419957661e-05,
      "loss": 0.6997,
      "step": 471000
    },
    {
      "epoch": 4.298671435871231,
      "grad_norm": 3.7124886512756348,
      "learning_rate": 4.641777380344065e-05,
      "loss": 0.7097,
      "step": 471100
    },
    {
      "epoch": 4.2995839112343965,
      "grad_norm": 4.698883533477783,
      "learning_rate": 4.641701340730467e-05,
      "loss": 0.7542,
      "step": 471200
    },
    {
      "epoch": 4.300496386597562,
      "grad_norm": 4.009435176849365,
      "learning_rate": 4.64162530111687e-05,
      "loss": 0.7226,
      "step": 471300
    },
    {
      "epoch": 4.301408861960727,
      "grad_norm": 3.631989002227783,
      "learning_rate": 4.641549261503273e-05,
      "loss": 0.7498,
      "step": 471400
    },
    {
      "epoch": 4.302321337323892,
      "grad_norm": 3.268404960632324,
      "learning_rate": 4.6414732218896755e-05,
      "loss": 0.7268,
      "step": 471500
    },
    {
      "epoch": 4.303233812687058,
      "grad_norm": 3.8265035152435303,
      "learning_rate": 4.6413971822760785e-05,
      "loss": 0.7036,
      "step": 471600
    },
    {
      "epoch": 4.304146288050223,
      "grad_norm": 3.565538167953491,
      "learning_rate": 4.6413211426624815e-05,
      "loss": 0.7086,
      "step": 471700
    },
    {
      "epoch": 4.3050587634133874,
      "grad_norm": 3.5732359886169434,
      "learning_rate": 4.6412451030488845e-05,
      "loss": 0.7236,
      "step": 471800
    },
    {
      "epoch": 4.305971238776553,
      "grad_norm": 3.7609798908233643,
      "learning_rate": 4.6411690634352875e-05,
      "loss": 0.7382,
      "step": 471900
    },
    {
      "epoch": 4.306883714139718,
      "grad_norm": 4.173635005950928,
      "learning_rate": 4.6410930238216905e-05,
      "loss": 0.7333,
      "step": 472000
    },
    {
      "epoch": 4.307796189502883,
      "grad_norm": 4.290855884552002,
      "learning_rate": 4.641016984208093e-05,
      "loss": 0.7091,
      "step": 472100
    },
    {
      "epoch": 4.308708664866049,
      "grad_norm": 3.6270813941955566,
      "learning_rate": 4.6409409445944965e-05,
      "loss": 0.7267,
      "step": 472200
    },
    {
      "epoch": 4.309621140229214,
      "grad_norm": 4.525612831115723,
      "learning_rate": 4.640864904980899e-05,
      "loss": 0.7519,
      "step": 472300
    },
    {
      "epoch": 4.310533615592379,
      "grad_norm": 3.41174578666687,
      "learning_rate": 4.640788865367302e-05,
      "loss": 0.7067,
      "step": 472400
    },
    {
      "epoch": 4.3114460909555445,
      "grad_norm": 4.070108413696289,
      "learning_rate": 4.640712825753705e-05,
      "loss": 0.7211,
      "step": 472500
    },
    {
      "epoch": 4.312358566318709,
      "grad_norm": 3.5675134658813477,
      "learning_rate": 4.640636786140108e-05,
      "loss": 0.7126,
      "step": 472600
    },
    {
      "epoch": 4.313271041681874,
      "grad_norm": 4.138730049133301,
      "learning_rate": 4.64056074652651e-05,
      "loss": 0.7148,
      "step": 472700
    },
    {
      "epoch": 4.3141835170450396,
      "grad_norm": 3.783017635345459,
      "learning_rate": 4.640484706912914e-05,
      "loss": 0.7346,
      "step": 472800
    },
    {
      "epoch": 4.315095992408205,
      "grad_norm": 4.323934078216553,
      "learning_rate": 4.640408667299316e-05,
      "loss": 0.7637,
      "step": 472900
    },
    {
      "epoch": 4.31600846777137,
      "grad_norm": 4.903764247894287,
      "learning_rate": 4.640332627685719e-05,
      "loss": 0.7209,
      "step": 473000
    },
    {
      "epoch": 4.3169209431345354,
      "grad_norm": 4.568332672119141,
      "learning_rate": 4.640256588072122e-05,
      "loss": 0.7395,
      "step": 473100
    },
    {
      "epoch": 4.317833418497701,
      "grad_norm": 3.6942622661590576,
      "learning_rate": 4.640180548458525e-05,
      "loss": 0.7116,
      "step": 473200
    },
    {
      "epoch": 4.318745893860866,
      "grad_norm": 3.775874137878418,
      "learning_rate": 4.640104508844928e-05,
      "loss": 0.7541,
      "step": 473300
    },
    {
      "epoch": 4.319658369224031,
      "grad_norm": 4.074871063232422,
      "learning_rate": 4.640028469231331e-05,
      "loss": 0.7634,
      "step": 473400
    },
    {
      "epoch": 4.320570844587196,
      "grad_norm": 3.33463716506958,
      "learning_rate": 4.6399524296177336e-05,
      "loss": 0.7282,
      "step": 473500
    },
    {
      "epoch": 4.321483319950361,
      "grad_norm": 4.017728805541992,
      "learning_rate": 4.639876390004137e-05,
      "loss": 0.7589,
      "step": 473600
    },
    {
      "epoch": 4.322395795313526,
      "grad_norm": 3.6960363388061523,
      "learning_rate": 4.6398003503905396e-05,
      "loss": 0.7297,
      "step": 473700
    },
    {
      "epoch": 4.323308270676692,
      "grad_norm": 4.22496223449707,
      "learning_rate": 4.6397243107769426e-05,
      "loss": 0.7688,
      "step": 473800
    },
    {
      "epoch": 4.324220746039857,
      "grad_norm": 3.919149875640869,
      "learning_rate": 4.6396482711633456e-05,
      "loss": 0.7084,
      "step": 473900
    },
    {
      "epoch": 4.325133221403022,
      "grad_norm": 3.978686571121216,
      "learning_rate": 4.6395722315497486e-05,
      "loss": 0.7075,
      "step": 474000
    },
    {
      "epoch": 4.3260456967661876,
      "grad_norm": 4.527106761932373,
      "learning_rate": 4.639496191936151e-05,
      "loss": 0.707,
      "step": 474100
    },
    {
      "epoch": 4.326958172129353,
      "grad_norm": 4.420523166656494,
      "learning_rate": 4.6394201523225546e-05,
      "loss": 0.709,
      "step": 474200
    },
    {
      "epoch": 4.327870647492517,
      "grad_norm": 4.4040398597717285,
      "learning_rate": 4.639344112708957e-05,
      "loss": 0.7718,
      "step": 474300
    },
    {
      "epoch": 4.328783122855683,
      "grad_norm": 5.29049015045166,
      "learning_rate": 4.63926807309536e-05,
      "loss": 0.7531,
      "step": 474400
    },
    {
      "epoch": 4.329695598218848,
      "grad_norm": 3.910494089126587,
      "learning_rate": 4.639192033481763e-05,
      "loss": 0.7358,
      "step": 474500
    },
    {
      "epoch": 4.330608073582013,
      "grad_norm": 4.091604232788086,
      "learning_rate": 4.639115993868165e-05,
      "loss": 0.7005,
      "step": 474600
    },
    {
      "epoch": 4.3315205489451785,
      "grad_norm": 4.49456262588501,
      "learning_rate": 4.639039954254569e-05,
      "loss": 0.7329,
      "step": 474700
    },
    {
      "epoch": 4.332433024308344,
      "grad_norm": 3.5042428970336914,
      "learning_rate": 4.638963914640971e-05,
      "loss": 0.7005,
      "step": 474800
    },
    {
      "epoch": 4.333345499671509,
      "grad_norm": 3.938716411590576,
      "learning_rate": 4.638887875027374e-05,
      "loss": 0.6805,
      "step": 474900
    },
    {
      "epoch": 4.334257975034674,
      "grad_norm": 4.263527870178223,
      "learning_rate": 4.638811835413777e-05,
      "loss": 0.7023,
      "step": 475000
    },
    {
      "epoch": 4.33517045039784,
      "grad_norm": 3.9672977924346924,
      "learning_rate": 4.6387357958001803e-05,
      "loss": 0.7363,
      "step": 475100
    },
    {
      "epoch": 4.336082925761004,
      "grad_norm": 4.0517497062683105,
      "learning_rate": 4.638659756186583e-05,
      "loss": 0.7054,
      "step": 475200
    },
    {
      "epoch": 4.336995401124169,
      "grad_norm": 5.181061744689941,
      "learning_rate": 4.6385837165729864e-05,
      "loss": 0.7214,
      "step": 475300
    },
    {
      "epoch": 4.337907876487335,
      "grad_norm": 4.408751964569092,
      "learning_rate": 4.638507676959389e-05,
      "loss": 0.6909,
      "step": 475400
    },
    {
      "epoch": 4.3388203518505,
      "grad_norm": 4.649965286254883,
      "learning_rate": 4.638431637345792e-05,
      "loss": 0.7394,
      "step": 475500
    },
    {
      "epoch": 4.339732827213665,
      "grad_norm": 3.582986354827881,
      "learning_rate": 4.638355597732195e-05,
      "loss": 0.7172,
      "step": 475600
    },
    {
      "epoch": 4.340645302576831,
      "grad_norm": 3.915212869644165,
      "learning_rate": 4.638279558118598e-05,
      "loss": 0.7122,
      "step": 475700
    },
    {
      "epoch": 4.341557777939996,
      "grad_norm": 4.009119987487793,
      "learning_rate": 4.638203518505001e-05,
      "loss": 0.736,
      "step": 475800
    },
    {
      "epoch": 4.342470253303161,
      "grad_norm": 3.9044976234436035,
      "learning_rate": 4.638127478891404e-05,
      "loss": 0.6977,
      "step": 475900
    },
    {
      "epoch": 4.343382728666326,
      "grad_norm": 3.855288505554199,
      "learning_rate": 4.638051439277806e-05,
      "loss": 0.7432,
      "step": 476000
    },
    {
      "epoch": 4.344295204029491,
      "grad_norm": 4.00529146194458,
      "learning_rate": 4.63797539966421e-05,
      "loss": 0.7557,
      "step": 476100
    },
    {
      "epoch": 4.345207679392656,
      "grad_norm": 3.9033212661743164,
      "learning_rate": 4.637899360050612e-05,
      "loss": 0.7343,
      "step": 476200
    },
    {
      "epoch": 4.3461201547558215,
      "grad_norm": 4.623971462249756,
      "learning_rate": 4.637823320437015e-05,
      "loss": 0.7325,
      "step": 476300
    },
    {
      "epoch": 4.347032630118987,
      "grad_norm": 4.252713203430176,
      "learning_rate": 4.637747280823418e-05,
      "loss": 0.7301,
      "step": 476400
    },
    {
      "epoch": 4.347945105482152,
      "grad_norm": 4.744644641876221,
      "learning_rate": 4.637671241209821e-05,
      "loss": 0.7477,
      "step": 476500
    },
    {
      "epoch": 4.348857580845317,
      "grad_norm": 4.520248889923096,
      "learning_rate": 4.6375952015962234e-05,
      "loss": 0.7092,
      "step": 476600
    },
    {
      "epoch": 4.349770056208483,
      "grad_norm": 5.297287464141846,
      "learning_rate": 4.637519161982627e-05,
      "loss": 0.7143,
      "step": 476700
    },
    {
      "epoch": 4.350682531571648,
      "grad_norm": 2.747622013092041,
      "learning_rate": 4.6374431223690294e-05,
      "loss": 0.733,
      "step": 476800
    },
    {
      "epoch": 4.351595006934812,
      "grad_norm": 4.183982849121094,
      "learning_rate": 4.6373670827554324e-05,
      "loss": 0.727,
      "step": 476900
    },
    {
      "epoch": 4.352507482297978,
      "grad_norm": 4.517886161804199,
      "learning_rate": 4.6372910431418354e-05,
      "loss": 0.7206,
      "step": 477000
    },
    {
      "epoch": 4.353419957661143,
      "grad_norm": 4.212321758270264,
      "learning_rate": 4.6372150035282384e-05,
      "loss": 0.7171,
      "step": 477100
    },
    {
      "epoch": 4.354332433024308,
      "grad_norm": 3.603937864303589,
      "learning_rate": 4.6371389639146415e-05,
      "loss": 0.7088,
      "step": 477200
    },
    {
      "epoch": 4.355244908387474,
      "grad_norm": 4.119752407073975,
      "learning_rate": 4.637062924301044e-05,
      "loss": 0.7221,
      "step": 477300
    },
    {
      "epoch": 4.356157383750639,
      "grad_norm": 3.8095781803131104,
      "learning_rate": 4.636986884687447e-05,
      "loss": 0.7887,
      "step": 477400
    },
    {
      "epoch": 4.357069859113804,
      "grad_norm": 3.52268648147583,
      "learning_rate": 4.63691084507385e-05,
      "loss": 0.7194,
      "step": 477500
    },
    {
      "epoch": 4.3579823344769695,
      "grad_norm": 4.02028226852417,
      "learning_rate": 4.636834805460253e-05,
      "loss": 0.722,
      "step": 477600
    },
    {
      "epoch": 4.358894809840134,
      "grad_norm": 3.583138942718506,
      "learning_rate": 4.636758765846656e-05,
      "loss": 0.7027,
      "step": 477700
    },
    {
      "epoch": 4.359807285203299,
      "grad_norm": 2.795909881591797,
      "learning_rate": 4.636682726233059e-05,
      "loss": 0.7244,
      "step": 477800
    },
    {
      "epoch": 4.3607197605664645,
      "grad_norm": 4.224195957183838,
      "learning_rate": 4.636606686619461e-05,
      "loss": 0.7346,
      "step": 477900
    },
    {
      "epoch": 4.36163223592963,
      "grad_norm": 4.226668357849121,
      "learning_rate": 4.636530647005864e-05,
      "loss": 0.7398,
      "step": 478000
    },
    {
      "epoch": 4.362544711292795,
      "grad_norm": 4.368879318237305,
      "learning_rate": 4.636454607392267e-05,
      "loss": 0.708,
      "step": 478100
    },
    {
      "epoch": 4.36345718665596,
      "grad_norm": 4.605963230133057,
      "learning_rate": 4.63637856777867e-05,
      "loss": 0.7039,
      "step": 478200
    },
    {
      "epoch": 4.364369662019126,
      "grad_norm": 3.007305145263672,
      "learning_rate": 4.636302528165073e-05,
      "loss": 0.7182,
      "step": 478300
    },
    {
      "epoch": 4.365282137382291,
      "grad_norm": 4.329961776733398,
      "learning_rate": 4.636226488551476e-05,
      "loss": 0.6987,
      "step": 478400
    },
    {
      "epoch": 4.366194612745456,
      "grad_norm": 3.920076608657837,
      "learning_rate": 4.6361504489378785e-05,
      "loss": 0.7174,
      "step": 478500
    },
    {
      "epoch": 4.367107088108621,
      "grad_norm": 4.09089994430542,
      "learning_rate": 4.636074409324282e-05,
      "loss": 0.7375,
      "step": 478600
    },
    {
      "epoch": 4.368019563471786,
      "grad_norm": 6.000385761260986,
      "learning_rate": 4.6359983697106845e-05,
      "loss": 0.7114,
      "step": 478700
    },
    {
      "epoch": 4.368932038834951,
      "grad_norm": 3.7034614086151123,
      "learning_rate": 4.6359223300970875e-05,
      "loss": 0.6928,
      "step": 478800
    },
    {
      "epoch": 4.369844514198117,
      "grad_norm": 4.040599346160889,
      "learning_rate": 4.6358462904834905e-05,
      "loss": 0.7369,
      "step": 478900
    },
    {
      "epoch": 4.370756989561282,
      "grad_norm": 2.2647500038146973,
      "learning_rate": 4.6357702508698935e-05,
      "loss": 0.7392,
      "step": 479000
    },
    {
      "epoch": 4.371669464924447,
      "grad_norm": 4.60125732421875,
      "learning_rate": 4.6356942112562965e-05,
      "loss": 0.7415,
      "step": 479100
    },
    {
      "epoch": 4.3725819402876125,
      "grad_norm": 5.33241605758667,
      "learning_rate": 4.6356181716426996e-05,
      "loss": 0.7396,
      "step": 479200
    },
    {
      "epoch": 4.373494415650778,
      "grad_norm": 3.9607861042022705,
      "learning_rate": 4.635542132029102e-05,
      "loss": 0.6804,
      "step": 479300
    },
    {
      "epoch": 4.374406891013942,
      "grad_norm": 4.4680585861206055,
      "learning_rate": 4.635466092415505e-05,
      "loss": 0.7354,
      "step": 479400
    },
    {
      "epoch": 4.3753193663771075,
      "grad_norm": 4.8518476486206055,
      "learning_rate": 4.635390052801908e-05,
      "loss": 0.688,
      "step": 479500
    },
    {
      "epoch": 4.376231841740273,
      "grad_norm": 4.388604164123535,
      "learning_rate": 4.635314013188311e-05,
      "loss": 0.7331,
      "step": 479600
    },
    {
      "epoch": 4.377144317103438,
      "grad_norm": 3.94793701171875,
      "learning_rate": 4.635237973574714e-05,
      "loss": 0.7356,
      "step": 479700
    },
    {
      "epoch": 4.378056792466603,
      "grad_norm": 3.4235639572143555,
      "learning_rate": 4.635161933961117e-05,
      "loss": 0.7161,
      "step": 479800
    },
    {
      "epoch": 4.378969267829769,
      "grad_norm": 3.478804111480713,
      "learning_rate": 4.635085894347519e-05,
      "loss": 0.7401,
      "step": 479900
    },
    {
      "epoch": 4.379881743192934,
      "grad_norm": 3.7234082221984863,
      "learning_rate": 4.635009854733922e-05,
      "loss": 0.7149,
      "step": 480000
    },
    {
      "epoch": 4.380794218556099,
      "grad_norm": 3.560908555984497,
      "learning_rate": 4.634933815120325e-05,
      "loss": 0.71,
      "step": 480100
    },
    {
      "epoch": 4.381706693919265,
      "grad_norm": 2.7686445713043213,
      "learning_rate": 4.634857775506728e-05,
      "loss": 0.7596,
      "step": 480200
    },
    {
      "epoch": 4.382619169282429,
      "grad_norm": 4.319380283355713,
      "learning_rate": 4.634781735893131e-05,
      "loss": 0.6994,
      "step": 480300
    },
    {
      "epoch": 4.383531644645594,
      "grad_norm": 4.900514602661133,
      "learning_rate": 4.6347056962795336e-05,
      "loss": 0.7016,
      "step": 480400
    },
    {
      "epoch": 4.38444412000876,
      "grad_norm": 4.173848628997803,
      "learning_rate": 4.634629656665937e-05,
      "loss": 0.7027,
      "step": 480500
    },
    {
      "epoch": 4.385356595371925,
      "grad_norm": 4.423178195953369,
      "learning_rate": 4.6345536170523396e-05,
      "loss": 0.7177,
      "step": 480600
    },
    {
      "epoch": 4.38626907073509,
      "grad_norm": 3.6370534896850586,
      "learning_rate": 4.6344775774387426e-05,
      "loss": 0.7059,
      "step": 480700
    },
    {
      "epoch": 4.3871815460982555,
      "grad_norm": 4.125079154968262,
      "learning_rate": 4.6344015378251456e-05,
      "loss": 0.7076,
      "step": 480800
    },
    {
      "epoch": 4.388094021461421,
      "grad_norm": 4.088555335998535,
      "learning_rate": 4.6343254982115486e-05,
      "loss": 0.7128,
      "step": 480900
    },
    {
      "epoch": 4.389006496824586,
      "grad_norm": 4.217007637023926,
      "learning_rate": 4.634249458597951e-05,
      "loss": 0.72,
      "step": 481000
    },
    {
      "epoch": 4.3899189721877505,
      "grad_norm": 3.7116825580596924,
      "learning_rate": 4.6341734189843546e-05,
      "loss": 0.6946,
      "step": 481100
    },
    {
      "epoch": 4.390831447550916,
      "grad_norm": 4.020834922790527,
      "learning_rate": 4.634097379370757e-05,
      "loss": 0.7682,
      "step": 481200
    },
    {
      "epoch": 4.391743922914081,
      "grad_norm": 3.2789947986602783,
      "learning_rate": 4.63402133975716e-05,
      "loss": 0.7295,
      "step": 481300
    },
    {
      "epoch": 4.392656398277246,
      "grad_norm": 4.425033092498779,
      "learning_rate": 4.633945300143563e-05,
      "loss": 0.7145,
      "step": 481400
    },
    {
      "epoch": 4.393568873640412,
      "grad_norm": 4.185717582702637,
      "learning_rate": 4.633869260529966e-05,
      "loss": 0.7604,
      "step": 481500
    },
    {
      "epoch": 4.394481349003577,
      "grad_norm": 3.860534191131592,
      "learning_rate": 4.633793220916369e-05,
      "loss": 0.7481,
      "step": 481600
    },
    {
      "epoch": 4.395393824366742,
      "grad_norm": 4.666167259216309,
      "learning_rate": 4.633717181302772e-05,
      "loss": 0.69,
      "step": 481700
    },
    {
      "epoch": 4.396306299729908,
      "grad_norm": 4.997354507446289,
      "learning_rate": 4.633641141689174e-05,
      "loss": 0.7341,
      "step": 481800
    },
    {
      "epoch": 4.397218775093073,
      "grad_norm": 4.241024017333984,
      "learning_rate": 4.633565102075578e-05,
      "loss": 0.6784,
      "step": 481900
    },
    {
      "epoch": 4.398131250456237,
      "grad_norm": 3.5575153827667236,
      "learning_rate": 4.6334890624619804e-05,
      "loss": 0.6805,
      "step": 482000
    },
    {
      "epoch": 4.399043725819403,
      "grad_norm": 4.193514823913574,
      "learning_rate": 4.6334130228483834e-05,
      "loss": 0.7029,
      "step": 482100
    },
    {
      "epoch": 4.399956201182568,
      "grad_norm": 3.766756772994995,
      "learning_rate": 4.6333369832347864e-05,
      "loss": 0.6973,
      "step": 482200
    },
    {
      "epoch": 4.400868676545733,
      "grad_norm": 4.32313346862793,
      "learning_rate": 4.6332609436211894e-05,
      "loss": 0.7554,
      "step": 482300
    },
    {
      "epoch": 4.4017811519088985,
      "grad_norm": 3.7226369380950928,
      "learning_rate": 4.633184904007592e-05,
      "loss": 0.7065,
      "step": 482400
    },
    {
      "epoch": 4.402693627272064,
      "grad_norm": 3.657128095626831,
      "learning_rate": 4.6331088643939954e-05,
      "loss": 0.6906,
      "step": 482500
    },
    {
      "epoch": 4.403606102635229,
      "grad_norm": 4.21638298034668,
      "learning_rate": 4.633032824780398e-05,
      "loss": 0.7276,
      "step": 482600
    },
    {
      "epoch": 4.404518577998394,
      "grad_norm": 3.42168927192688,
      "learning_rate": 4.632956785166801e-05,
      "loss": 0.7009,
      "step": 482700
    },
    {
      "epoch": 4.405431053361559,
      "grad_norm": 3.2386715412139893,
      "learning_rate": 4.632880745553204e-05,
      "loss": 0.7425,
      "step": 482800
    },
    {
      "epoch": 4.406343528724724,
      "grad_norm": 4.441122055053711,
      "learning_rate": 4.632804705939606e-05,
      "loss": 0.7254,
      "step": 482900
    },
    {
      "epoch": 4.407256004087889,
      "grad_norm": 3.632640838623047,
      "learning_rate": 4.63272866632601e-05,
      "loss": 0.7333,
      "step": 483000
    },
    {
      "epoch": 4.408168479451055,
      "grad_norm": 3.5745885372161865,
      "learning_rate": 4.632652626712412e-05,
      "loss": 0.7399,
      "step": 483100
    },
    {
      "epoch": 4.40908095481422,
      "grad_norm": 4.220837593078613,
      "learning_rate": 4.632576587098815e-05,
      "loss": 0.7212,
      "step": 483200
    },
    {
      "epoch": 4.409993430177385,
      "grad_norm": 5.03891134262085,
      "learning_rate": 4.632500547485218e-05,
      "loss": 0.6965,
      "step": 483300
    },
    {
      "epoch": 4.410905905540551,
      "grad_norm": 5.120007038116455,
      "learning_rate": 4.632424507871621e-05,
      "loss": 0.7305,
      "step": 483400
    },
    {
      "epoch": 4.411818380903716,
      "grad_norm": 4.451909065246582,
      "learning_rate": 4.6323484682580234e-05,
      "loss": 0.702,
      "step": 483500
    },
    {
      "epoch": 4.412730856266881,
      "grad_norm": 4.2483811378479,
      "learning_rate": 4.632272428644427e-05,
      "loss": 0.7193,
      "step": 483600
    },
    {
      "epoch": 4.413643331630046,
      "grad_norm": 2.3052780628204346,
      "learning_rate": 4.6321963890308294e-05,
      "loss": 0.7014,
      "step": 483700
    },
    {
      "epoch": 4.414555806993211,
      "grad_norm": 3.8242318630218506,
      "learning_rate": 4.6321203494172324e-05,
      "loss": 0.7199,
      "step": 483800
    },
    {
      "epoch": 4.415468282356376,
      "grad_norm": 3.604576826095581,
      "learning_rate": 4.6320443098036354e-05,
      "loss": 0.7186,
      "step": 483900
    },
    {
      "epoch": 4.4163807577195415,
      "grad_norm": 4.846231460571289,
      "learning_rate": 4.6319682701900385e-05,
      "loss": 0.7444,
      "step": 484000
    },
    {
      "epoch": 4.417293233082707,
      "grad_norm": 3.816805601119995,
      "learning_rate": 4.6318922305764415e-05,
      "loss": 0.7366,
      "step": 484100
    },
    {
      "epoch": 4.418205708445872,
      "grad_norm": 3.679267644882202,
      "learning_rate": 4.6318161909628445e-05,
      "loss": 0.7193,
      "step": 484200
    },
    {
      "epoch": 4.419118183809037,
      "grad_norm": 4.553928852081299,
      "learning_rate": 4.631740151349247e-05,
      "loss": 0.7181,
      "step": 484300
    },
    {
      "epoch": 4.420030659172203,
      "grad_norm": 4.404020309448242,
      "learning_rate": 4.6316641117356505e-05,
      "loss": 0.7128,
      "step": 484400
    },
    {
      "epoch": 4.420943134535367,
      "grad_norm": 4.005002021789551,
      "learning_rate": 4.631588072122053e-05,
      "loss": 0.7757,
      "step": 484500
    },
    {
      "epoch": 4.421855609898532,
      "grad_norm": 3.5017664432525635,
      "learning_rate": 4.631512032508456e-05,
      "loss": 0.7422,
      "step": 484600
    },
    {
      "epoch": 4.422768085261698,
      "grad_norm": 3.350158929824829,
      "learning_rate": 4.631435992894859e-05,
      "loss": 0.7097,
      "step": 484700
    },
    {
      "epoch": 4.423680560624863,
      "grad_norm": 3.4665794372558594,
      "learning_rate": 4.631359953281262e-05,
      "loss": 0.7364,
      "step": 484800
    },
    {
      "epoch": 4.424593035988028,
      "grad_norm": 3.791776418685913,
      "learning_rate": 4.631283913667664e-05,
      "loss": 0.7212,
      "step": 484900
    },
    {
      "epoch": 4.425505511351194,
      "grad_norm": 4.887657165527344,
      "learning_rate": 4.631207874054068e-05,
      "loss": 0.7244,
      "step": 485000
    },
    {
      "epoch": 4.426417986714359,
      "grad_norm": 4.395269870758057,
      "learning_rate": 4.63113183444047e-05,
      "loss": 0.7813,
      "step": 485100
    },
    {
      "epoch": 4.427330462077524,
      "grad_norm": 4.15872049331665,
      "learning_rate": 4.631055794826873e-05,
      "loss": 0.7268,
      "step": 485200
    },
    {
      "epoch": 4.4282429374406895,
      "grad_norm": 3.8539652824401855,
      "learning_rate": 4.630979755213276e-05,
      "loss": 0.7262,
      "step": 485300
    },
    {
      "epoch": 4.429155412803854,
      "grad_norm": 4.2334113121032715,
      "learning_rate": 4.630903715599679e-05,
      "loss": 0.7521,
      "step": 485400
    },
    {
      "epoch": 4.430067888167019,
      "grad_norm": 3.6388111114501953,
      "learning_rate": 4.630827675986082e-05,
      "loss": 0.7146,
      "step": 485500
    },
    {
      "epoch": 4.4309803635301845,
      "grad_norm": 4.545450687408447,
      "learning_rate": 4.630751636372485e-05,
      "loss": 0.7056,
      "step": 485600
    },
    {
      "epoch": 4.43189283889335,
      "grad_norm": 4.656689643859863,
      "learning_rate": 4.6306755967588875e-05,
      "loss": 0.7129,
      "step": 485700
    },
    {
      "epoch": 4.432805314256515,
      "grad_norm": 4.931199550628662,
      "learning_rate": 4.6305995571452905e-05,
      "loss": 0.7304,
      "step": 485800
    },
    {
      "epoch": 4.43371778961968,
      "grad_norm": 4.745253562927246,
      "learning_rate": 4.6305235175316935e-05,
      "loss": 0.7127,
      "step": 485900
    },
    {
      "epoch": 4.434630264982846,
      "grad_norm": 3.462498426437378,
      "learning_rate": 4.630447477918096e-05,
      "loss": 0.7234,
      "step": 486000
    },
    {
      "epoch": 4.435542740346011,
      "grad_norm": 3.6745259761810303,
      "learning_rate": 4.6303714383044996e-05,
      "loss": 0.766,
      "step": 486100
    },
    {
      "epoch": 4.4364552157091754,
      "grad_norm": 4.681110382080078,
      "learning_rate": 4.630295398690902e-05,
      "loss": 0.7195,
      "step": 486200
    },
    {
      "epoch": 4.437367691072341,
      "grad_norm": 3.81838321685791,
      "learning_rate": 4.630219359077305e-05,
      "loss": 0.6774,
      "step": 486300
    },
    {
      "epoch": 4.438280166435506,
      "grad_norm": 4.273909568786621,
      "learning_rate": 4.630143319463708e-05,
      "loss": 0.7191,
      "step": 486400
    },
    {
      "epoch": 4.439192641798671,
      "grad_norm": 4.6818437576293945,
      "learning_rate": 4.630067279850111e-05,
      "loss": 0.6958,
      "step": 486500
    },
    {
      "epoch": 4.440105117161837,
      "grad_norm": 4.387059211730957,
      "learning_rate": 4.629991240236514e-05,
      "loss": 0.7082,
      "step": 486600
    },
    {
      "epoch": 4.441017592525002,
      "grad_norm": 3.945769786834717,
      "learning_rate": 4.629915200622917e-05,
      "loss": 0.7231,
      "step": 486700
    },
    {
      "epoch": 4.441930067888167,
      "grad_norm": 4.442109107971191,
      "learning_rate": 4.629839161009319e-05,
      "loss": 0.6782,
      "step": 486800
    },
    {
      "epoch": 4.4428425432513325,
      "grad_norm": 2.3703629970550537,
      "learning_rate": 4.629763121395723e-05,
      "loss": 0.6953,
      "step": 486900
    },
    {
      "epoch": 4.443755018614498,
      "grad_norm": 3.7769274711608887,
      "learning_rate": 4.629687081782125e-05,
      "loss": 0.739,
      "step": 487000
    },
    {
      "epoch": 4.444667493977662,
      "grad_norm": 4.410767555236816,
      "learning_rate": 4.629611042168528e-05,
      "loss": 0.7025,
      "step": 487100
    },
    {
      "epoch": 4.4455799693408276,
      "grad_norm": 3.491272211074829,
      "learning_rate": 4.629535002554931e-05,
      "loss": 0.7421,
      "step": 487200
    },
    {
      "epoch": 4.446492444703993,
      "grad_norm": 4.178631782531738,
      "learning_rate": 4.629458962941334e-05,
      "loss": 0.7214,
      "step": 487300
    },
    {
      "epoch": 4.447404920067158,
      "grad_norm": 4.67578125,
      "learning_rate": 4.6293829233277366e-05,
      "loss": 0.7101,
      "step": 487400
    },
    {
      "epoch": 4.4483173954303235,
      "grad_norm": 4.433011054992676,
      "learning_rate": 4.62930688371414e-05,
      "loss": 0.6917,
      "step": 487500
    },
    {
      "epoch": 4.449229870793489,
      "grad_norm": 3.7377142906188965,
      "learning_rate": 4.6292308441005426e-05,
      "loss": 0.7306,
      "step": 487600
    },
    {
      "epoch": 4.450142346156654,
      "grad_norm": 3.8939173221588135,
      "learning_rate": 4.6291548044869456e-05,
      "loss": 0.6989,
      "step": 487700
    },
    {
      "epoch": 4.451054821519819,
      "grad_norm": 3.119804620742798,
      "learning_rate": 4.6290787648733486e-05,
      "loss": 0.7421,
      "step": 487800
    },
    {
      "epoch": 4.451967296882984,
      "grad_norm": 3.5807485580444336,
      "learning_rate": 4.6290027252597517e-05,
      "loss": 0.6869,
      "step": 487900
    },
    {
      "epoch": 4.452879772246149,
      "grad_norm": 3.982783079147339,
      "learning_rate": 4.6289266856461547e-05,
      "loss": 0.7218,
      "step": 488000
    },
    {
      "epoch": 4.453792247609314,
      "grad_norm": 5.7115631103515625,
      "learning_rate": 4.628850646032558e-05,
      "loss": 0.7206,
      "step": 488100
    },
    {
      "epoch": 4.45470472297248,
      "grad_norm": 2.3931570053100586,
      "learning_rate": 4.62877460641896e-05,
      "loss": 0.7199,
      "step": 488200
    },
    {
      "epoch": 4.455617198335645,
      "grad_norm": 3.7148184776306152,
      "learning_rate": 4.628698566805364e-05,
      "loss": 0.7102,
      "step": 488300
    },
    {
      "epoch": 4.45652967369881,
      "grad_norm": 3.6199231147766113,
      "learning_rate": 4.628622527191766e-05,
      "loss": 0.6914,
      "step": 488400
    },
    {
      "epoch": 4.457442149061976,
      "grad_norm": 4.656314849853516,
      "learning_rate": 4.628546487578169e-05,
      "loss": 0.7268,
      "step": 488500
    },
    {
      "epoch": 4.458354624425141,
      "grad_norm": 4.444355010986328,
      "learning_rate": 4.628470447964572e-05,
      "loss": 0.7279,
      "step": 488600
    },
    {
      "epoch": 4.459267099788306,
      "grad_norm": 3.573690176010132,
      "learning_rate": 4.6283944083509743e-05,
      "loss": 0.7508,
      "step": 488700
    },
    {
      "epoch": 4.460179575151471,
      "grad_norm": 4.272874355316162,
      "learning_rate": 4.6283183687373774e-05,
      "loss": 0.7338,
      "step": 488800
    },
    {
      "epoch": 4.461092050514636,
      "grad_norm": 4.241148471832275,
      "learning_rate": 4.6282423291237804e-05,
      "loss": 0.743,
      "step": 488900
    },
    {
      "epoch": 4.462004525877801,
      "grad_norm": 4.064316272735596,
      "learning_rate": 4.6281662895101834e-05,
      "loss": 0.7044,
      "step": 489000
    },
    {
      "epoch": 4.4629170012409665,
      "grad_norm": 3.876570224761963,
      "learning_rate": 4.6280902498965864e-05,
      "loss": 0.7849,
      "step": 489100
    },
    {
      "epoch": 4.463829476604132,
      "grad_norm": 4.507420063018799,
      "learning_rate": 4.6280142102829894e-05,
      "loss": 0.6957,
      "step": 489200
    },
    {
      "epoch": 4.464741951967297,
      "grad_norm": 3.6834282875061035,
      "learning_rate": 4.627938170669392e-05,
      "loss": 0.7075,
      "step": 489300
    },
    {
      "epoch": 4.465654427330462,
      "grad_norm": 4.331371784210205,
      "learning_rate": 4.6278621310557954e-05,
      "loss": 0.7507,
      "step": 489400
    },
    {
      "epoch": 4.466566902693628,
      "grad_norm": 4.173430442810059,
      "learning_rate": 4.627786091442198e-05,
      "loss": 0.6884,
      "step": 489500
    },
    {
      "epoch": 4.467479378056792,
      "grad_norm": 4.677781581878662,
      "learning_rate": 4.627710051828601e-05,
      "loss": 0.737,
      "step": 489600
    },
    {
      "epoch": 4.468391853419957,
      "grad_norm": 3.7748262882232666,
      "learning_rate": 4.627634012215004e-05,
      "loss": 0.7319,
      "step": 489700
    },
    {
      "epoch": 4.469304328783123,
      "grad_norm": 3.632902145385742,
      "learning_rate": 4.627557972601407e-05,
      "loss": 0.7066,
      "step": 489800
    },
    {
      "epoch": 4.470216804146288,
      "grad_norm": 4.980330944061279,
      "learning_rate": 4.627481932987809e-05,
      "loss": 0.7093,
      "step": 489900
    },
    {
      "epoch": 4.471129279509453,
      "grad_norm": 3.78434157371521,
      "learning_rate": 4.627405893374213e-05,
      "loss": 0.7282,
      "step": 490000
    },
    {
      "epoch": 4.472041754872619,
      "grad_norm": 4.102535247802734,
      "learning_rate": 4.627329853760615e-05,
      "loss": 0.7189,
      "step": 490100
    },
    {
      "epoch": 4.472954230235784,
      "grad_norm": 3.101747989654541,
      "learning_rate": 4.627253814147018e-05,
      "loss": 0.7299,
      "step": 490200
    },
    {
      "epoch": 4.473866705598949,
      "grad_norm": 3.2046549320220947,
      "learning_rate": 4.627177774533421e-05,
      "loss": 0.722,
      "step": 490300
    },
    {
      "epoch": 4.4747791809621145,
      "grad_norm": 3.9692602157592773,
      "learning_rate": 4.627101734919824e-05,
      "loss": 0.7377,
      "step": 490400
    },
    {
      "epoch": 4.475691656325279,
      "grad_norm": 3.7315633296966553,
      "learning_rate": 4.627025695306227e-05,
      "loss": 0.7439,
      "step": 490500
    },
    {
      "epoch": 4.476604131688444,
      "grad_norm": 4.786585807800293,
      "learning_rate": 4.62694965569263e-05,
      "loss": 0.7206,
      "step": 490600
    },
    {
      "epoch": 4.4775166070516095,
      "grad_norm": 5.510951995849609,
      "learning_rate": 4.6268736160790325e-05,
      "loss": 0.708,
      "step": 490700
    },
    {
      "epoch": 4.478429082414775,
      "grad_norm": 3.8139357566833496,
      "learning_rate": 4.626797576465436e-05,
      "loss": 0.7092,
      "step": 490800
    },
    {
      "epoch": 4.47934155777794,
      "grad_norm": 3.5336616039276123,
      "learning_rate": 4.6267215368518385e-05,
      "loss": 0.6958,
      "step": 490900
    },
    {
      "epoch": 4.480254033141105,
      "grad_norm": 2.6737046241760254,
      "learning_rate": 4.6266454972382415e-05,
      "loss": 0.7515,
      "step": 491000
    },
    {
      "epoch": 4.481166508504271,
      "grad_norm": 4.281325817108154,
      "learning_rate": 4.6265694576246445e-05,
      "loss": 0.7293,
      "step": 491100
    },
    {
      "epoch": 4.482078983867436,
      "grad_norm": 4.169554233551025,
      "learning_rate": 4.6264934180110475e-05,
      "loss": 0.6753,
      "step": 491200
    },
    {
      "epoch": 4.4829914592306,
      "grad_norm": 3.5872037410736084,
      "learning_rate": 4.6264173783974505e-05,
      "loss": 0.7535,
      "step": 491300
    },
    {
      "epoch": 4.483903934593766,
      "grad_norm": 4.62933349609375,
      "learning_rate": 4.626341338783853e-05,
      "loss": 0.7395,
      "step": 491400
    },
    {
      "epoch": 4.484816409956931,
      "grad_norm": 4.454091548919678,
      "learning_rate": 4.626265299170256e-05,
      "loss": 0.7685,
      "step": 491500
    },
    {
      "epoch": 4.485728885320096,
      "grad_norm": 3.358610153198242,
      "learning_rate": 4.626189259556659e-05,
      "loss": 0.6798,
      "step": 491600
    },
    {
      "epoch": 4.486641360683262,
      "grad_norm": 4.723992824554443,
      "learning_rate": 4.626113219943062e-05,
      "loss": 0.7331,
      "step": 491700
    },
    {
      "epoch": 4.487553836046427,
      "grad_norm": 5.316364765167236,
      "learning_rate": 4.626037180329464e-05,
      "loss": 0.6776,
      "step": 491800
    },
    {
      "epoch": 4.488466311409592,
      "grad_norm": 4.166025161743164,
      "learning_rate": 4.625961140715868e-05,
      "loss": 0.7013,
      "step": 491900
    },
    {
      "epoch": 4.4893787867727575,
      "grad_norm": 4.7746405601501465,
      "learning_rate": 4.62588510110227e-05,
      "loss": 0.7049,
      "step": 492000
    },
    {
      "epoch": 4.490291262135923,
      "grad_norm": 2.594114065170288,
      "learning_rate": 4.625809061488673e-05,
      "loss": 0.7349,
      "step": 492100
    },
    {
      "epoch": 4.491203737499087,
      "grad_norm": 4.628129482269287,
      "learning_rate": 4.625733021875076e-05,
      "loss": 0.7113,
      "step": 492200
    },
    {
      "epoch": 4.4921162128622525,
      "grad_norm": 5.28493595123291,
      "learning_rate": 4.625656982261479e-05,
      "loss": 0.7155,
      "step": 492300
    },
    {
      "epoch": 4.493028688225418,
      "grad_norm": 4.4526472091674805,
      "learning_rate": 4.625580942647882e-05,
      "loss": 0.7174,
      "step": 492400
    },
    {
      "epoch": 4.493941163588583,
      "grad_norm": 3.5822348594665527,
      "learning_rate": 4.625504903034285e-05,
      "loss": 0.7058,
      "step": 492500
    },
    {
      "epoch": 4.494853638951748,
      "grad_norm": 4.604287147521973,
      "learning_rate": 4.6254288634206875e-05,
      "loss": 0.736,
      "step": 492600
    },
    {
      "epoch": 4.495766114314914,
      "grad_norm": 4.57342004776001,
      "learning_rate": 4.625352823807091e-05,
      "loss": 0.7195,
      "step": 492700
    },
    {
      "epoch": 4.496678589678079,
      "grad_norm": 4.629848480224609,
      "learning_rate": 4.6252767841934936e-05,
      "loss": 0.7076,
      "step": 492800
    },
    {
      "epoch": 4.497591065041244,
      "grad_norm": 4.497910499572754,
      "learning_rate": 4.6252007445798966e-05,
      "loss": 0.7099,
      "step": 492900
    },
    {
      "epoch": 4.498503540404409,
      "grad_norm": 3.4475884437561035,
      "learning_rate": 4.6251247049662996e-05,
      "loss": 0.7117,
      "step": 493000
    },
    {
      "epoch": 4.499416015767574,
      "grad_norm": 4.002434253692627,
      "learning_rate": 4.6250486653527026e-05,
      "loss": 0.7112,
      "step": 493100
    },
    {
      "epoch": 4.500328491130739,
      "grad_norm": 3.1868128776550293,
      "learning_rate": 4.624972625739105e-05,
      "loss": 0.7067,
      "step": 493200
    },
    {
      "epoch": 4.501240966493905,
      "grad_norm": 3.473120927810669,
      "learning_rate": 4.6248965861255086e-05,
      "loss": 0.731,
      "step": 493300
    },
    {
      "epoch": 4.50215344185707,
      "grad_norm": 4.092675685882568,
      "learning_rate": 4.624820546511911e-05,
      "loss": 0.7339,
      "step": 493400
    },
    {
      "epoch": 4.503065917220235,
      "grad_norm": 4.947376251220703,
      "learning_rate": 4.624744506898314e-05,
      "loss": 0.6668,
      "step": 493500
    },
    {
      "epoch": 4.5039783925834005,
      "grad_norm": 3.729579448699951,
      "learning_rate": 4.624668467284717e-05,
      "loss": 0.6966,
      "step": 493600
    },
    {
      "epoch": 4.504890867946566,
      "grad_norm": 4.70711088180542,
      "learning_rate": 4.62459242767112e-05,
      "loss": 0.7348,
      "step": 493700
    },
    {
      "epoch": 4.505803343309731,
      "grad_norm": 4.568352699279785,
      "learning_rate": 4.624516388057523e-05,
      "loss": 0.7169,
      "step": 493800
    },
    {
      "epoch": 4.5067158186728955,
      "grad_norm": 3.589188814163208,
      "learning_rate": 4.624440348443926e-05,
      "loss": 0.6753,
      "step": 493900
    },
    {
      "epoch": 4.507628294036061,
      "grad_norm": 3.5190014839172363,
      "learning_rate": 4.624364308830328e-05,
      "loss": 0.6896,
      "step": 494000
    },
    {
      "epoch": 4.508540769399226,
      "grad_norm": 3.987668991088867,
      "learning_rate": 4.624288269216732e-05,
      "loss": 0.7097,
      "step": 494100
    },
    {
      "epoch": 4.509453244762391,
      "grad_norm": 3.313992977142334,
      "learning_rate": 4.624212229603134e-05,
      "loss": 0.7305,
      "step": 494200
    },
    {
      "epoch": 4.510365720125557,
      "grad_norm": 5.588449478149414,
      "learning_rate": 4.6241361899895366e-05,
      "loss": 0.7616,
      "step": 494300
    },
    {
      "epoch": 4.511278195488722,
      "grad_norm": 3.8733129501342773,
      "learning_rate": 4.62406015037594e-05,
      "loss": 0.7035,
      "step": 494400
    },
    {
      "epoch": 4.512190670851887,
      "grad_norm": 3.5690605640411377,
      "learning_rate": 4.6239841107623426e-05,
      "loss": 0.6903,
      "step": 494500
    },
    {
      "epoch": 4.513103146215052,
      "grad_norm": 4.766720294952393,
      "learning_rate": 4.6239080711487456e-05,
      "loss": 0.7545,
      "step": 494600
    },
    {
      "epoch": 4.514015621578217,
      "grad_norm": 5.202284812927246,
      "learning_rate": 4.6238320315351487e-05,
      "loss": 0.7083,
      "step": 494700
    },
    {
      "epoch": 4.514928096941382,
      "grad_norm": 2.1883127689361572,
      "learning_rate": 4.6237559919215517e-05,
      "loss": 0.7146,
      "step": 494800
    },
    {
      "epoch": 4.515840572304548,
      "grad_norm": 4.37965726852417,
      "learning_rate": 4.623679952307955e-05,
      "loss": 0.715,
      "step": 494900
    },
    {
      "epoch": 4.516753047667713,
      "grad_norm": 3.9581615924835205,
      "learning_rate": 4.623603912694358e-05,
      "loss": 0.7467,
      "step": 495000
    },
    {
      "epoch": 4.517665523030878,
      "grad_norm": 4.932619571685791,
      "learning_rate": 4.62352787308076e-05,
      "loss": 0.7122,
      "step": 495100
    },
    {
      "epoch": 4.5185779983940435,
      "grad_norm": 5.331919193267822,
      "learning_rate": 4.623451833467164e-05,
      "loss": 0.6667,
      "step": 495200
    },
    {
      "epoch": 4.519490473757209,
      "grad_norm": 3.91249680519104,
      "learning_rate": 4.623375793853566e-05,
      "loss": 0.7171,
      "step": 495300
    },
    {
      "epoch": 4.520402949120374,
      "grad_norm": 4.78994083404541,
      "learning_rate": 4.623299754239969e-05,
      "loss": 0.7068,
      "step": 495400
    },
    {
      "epoch": 4.521315424483539,
      "grad_norm": 5.514134883880615,
      "learning_rate": 4.623223714626372e-05,
      "loss": 0.7159,
      "step": 495500
    },
    {
      "epoch": 4.522227899846704,
      "grad_norm": 4.325395107269287,
      "learning_rate": 4.623147675012775e-05,
      "loss": 0.7584,
      "step": 495600
    },
    {
      "epoch": 4.523140375209869,
      "grad_norm": 4.185732841491699,
      "learning_rate": 4.6230716353991774e-05,
      "loss": 0.7317,
      "step": 495700
    },
    {
      "epoch": 4.524052850573034,
      "grad_norm": 3.5800817012786865,
      "learning_rate": 4.622995595785581e-05,
      "loss": 0.7692,
      "step": 495800
    },
    {
      "epoch": 4.5249653259362,
      "grad_norm": 4.441104412078857,
      "learning_rate": 4.6229195561719834e-05,
      "loss": 0.7194,
      "step": 495900
    },
    {
      "epoch": 4.525877801299365,
      "grad_norm": 4.118045330047607,
      "learning_rate": 4.6228435165583864e-05,
      "loss": 0.7352,
      "step": 496000
    },
    {
      "epoch": 4.52679027666253,
      "grad_norm": 3.158665657043457,
      "learning_rate": 4.6227674769447894e-05,
      "loss": 0.7144,
      "step": 496100
    },
    {
      "epoch": 4.527702752025696,
      "grad_norm": 3.157935619354248,
      "learning_rate": 4.6226914373311924e-05,
      "loss": 0.7581,
      "step": 496200
    },
    {
      "epoch": 4.52861522738886,
      "grad_norm": 3.5534496307373047,
      "learning_rate": 4.6226153977175954e-05,
      "loss": 0.6993,
      "step": 496300
    },
    {
      "epoch": 4.529527702752025,
      "grad_norm": 4.181033611297607,
      "learning_rate": 4.6225393581039984e-05,
      "loss": 0.6947,
      "step": 496400
    },
    {
      "epoch": 4.530440178115191,
      "grad_norm": 4.0189313888549805,
      "learning_rate": 4.622463318490401e-05,
      "loss": 0.7342,
      "step": 496500
    },
    {
      "epoch": 4.531352653478356,
      "grad_norm": 4.408524036407471,
      "learning_rate": 4.6223872788768044e-05,
      "loss": 0.7059,
      "step": 496600
    },
    {
      "epoch": 4.532265128841521,
      "grad_norm": 4.426647186279297,
      "learning_rate": 4.622311239263207e-05,
      "loss": 0.7278,
      "step": 496700
    },
    {
      "epoch": 4.5331776042046865,
      "grad_norm": 4.17750358581543,
      "learning_rate": 4.62223519964961e-05,
      "loss": 0.6707,
      "step": 496800
    },
    {
      "epoch": 4.534090079567852,
      "grad_norm": 2.9887988567352295,
      "learning_rate": 4.622159160036013e-05,
      "loss": 0.7326,
      "step": 496900
    },
    {
      "epoch": 4.535002554931017,
      "grad_norm": 3.7471745014190674,
      "learning_rate": 4.622083120422416e-05,
      "loss": 0.7441,
      "step": 497000
    },
    {
      "epoch": 4.535915030294182,
      "grad_norm": 3.827493190765381,
      "learning_rate": 4.622007080808818e-05,
      "loss": 0.7203,
      "step": 497100
    },
    {
      "epoch": 4.536827505657348,
      "grad_norm": 3.7701656818389893,
      "learning_rate": 4.621931041195221e-05,
      "loss": 0.7306,
      "step": 497200
    },
    {
      "epoch": 4.537739981020512,
      "grad_norm": 4.223287105560303,
      "learning_rate": 4.621855001581624e-05,
      "loss": 0.7126,
      "step": 497300
    },
    {
      "epoch": 4.538652456383677,
      "grad_norm": 4.671576499938965,
      "learning_rate": 4.621778961968027e-05,
      "loss": 0.7222,
      "step": 497400
    },
    {
      "epoch": 4.539564931746843,
      "grad_norm": 3.594428539276123,
      "learning_rate": 4.62170292235443e-05,
      "loss": 0.7394,
      "step": 497500
    },
    {
      "epoch": 4.540477407110008,
      "grad_norm": 4.8457746505737305,
      "learning_rate": 4.6216268827408325e-05,
      "loss": 0.7127,
      "step": 497600
    },
    {
      "epoch": 4.541389882473173,
      "grad_norm": 4.663306713104248,
      "learning_rate": 4.621550843127236e-05,
      "loss": 0.7491,
      "step": 497700
    },
    {
      "epoch": 4.542302357836339,
      "grad_norm": 3.092858076095581,
      "learning_rate": 4.6214748035136385e-05,
      "loss": 0.7254,
      "step": 497800
    },
    {
      "epoch": 4.543214833199504,
      "grad_norm": 3.3953700065612793,
      "learning_rate": 4.6213987639000415e-05,
      "loss": 0.7415,
      "step": 497900
    },
    {
      "epoch": 4.544127308562668,
      "grad_norm": 3.7880351543426514,
      "learning_rate": 4.6213227242864445e-05,
      "loss": 0.7195,
      "step": 498000
    },
    {
      "epoch": 4.545039783925834,
      "grad_norm": 3.700429916381836,
      "learning_rate": 4.6212466846728475e-05,
      "loss": 0.6805,
      "step": 498100
    },
    {
      "epoch": 4.545952259288999,
      "grad_norm": 5.0238471031188965,
      "learning_rate": 4.62117064505925e-05,
      "loss": 0.7621,
      "step": 498200
    },
    {
      "epoch": 4.546864734652164,
      "grad_norm": 4.2721452713012695,
      "learning_rate": 4.6210946054456535e-05,
      "loss": 0.7096,
      "step": 498300
    },
    {
      "epoch": 4.5477772100153295,
      "grad_norm": 4.392626762390137,
      "learning_rate": 4.621018565832056e-05,
      "loss": 0.7115,
      "step": 498400
    },
    {
      "epoch": 4.548689685378495,
      "grad_norm": 3.865877389907837,
      "learning_rate": 4.620942526218459e-05,
      "loss": 0.7672,
      "step": 498500
    },
    {
      "epoch": 4.54960216074166,
      "grad_norm": 3.3730790615081787,
      "learning_rate": 4.620866486604862e-05,
      "loss": 0.7316,
      "step": 498600
    },
    {
      "epoch": 4.550514636104825,
      "grad_norm": 3.814241409301758,
      "learning_rate": 4.620790446991265e-05,
      "loss": 0.7016,
      "step": 498700
    },
    {
      "epoch": 4.551427111467991,
      "grad_norm": 3.107923984527588,
      "learning_rate": 4.620714407377668e-05,
      "loss": 0.7153,
      "step": 498800
    },
    {
      "epoch": 4.552339586831156,
      "grad_norm": 4.024503231048584,
      "learning_rate": 4.620638367764071e-05,
      "loss": 0.7119,
      "step": 498900
    },
    {
      "epoch": 4.55325206219432,
      "grad_norm": 4.221287250518799,
      "learning_rate": 4.620562328150473e-05,
      "loss": 0.7215,
      "step": 499000
    },
    {
      "epoch": 4.554164537557486,
      "grad_norm": 3.9328272342681885,
      "learning_rate": 4.620486288536877e-05,
      "loss": 0.6891,
      "step": 499100
    },
    {
      "epoch": 4.555077012920651,
      "grad_norm": 4.346663951873779,
      "learning_rate": 4.620410248923279e-05,
      "loss": 0.6917,
      "step": 499200
    },
    {
      "epoch": 4.555989488283816,
      "grad_norm": 3.723234176635742,
      "learning_rate": 4.620334209309682e-05,
      "loss": 0.7306,
      "step": 499300
    },
    {
      "epoch": 4.556901963646982,
      "grad_norm": 3.6040561199188232,
      "learning_rate": 4.620258169696085e-05,
      "loss": 0.7188,
      "step": 499400
    },
    {
      "epoch": 4.557814439010147,
      "grad_norm": 4.68319034576416,
      "learning_rate": 4.620182130082488e-05,
      "loss": 0.7323,
      "step": 499500
    },
    {
      "epoch": 4.558726914373312,
      "grad_norm": 3.763336420059204,
      "learning_rate": 4.6201060904688906e-05,
      "loss": 0.7393,
      "step": 499600
    },
    {
      "epoch": 4.559639389736477,
      "grad_norm": 4.656822204589844,
      "learning_rate": 4.620030050855294e-05,
      "loss": 0.7415,
      "step": 499700
    },
    {
      "epoch": 4.560551865099642,
      "grad_norm": 3.489903211593628,
      "learning_rate": 4.6199540112416966e-05,
      "loss": 0.7348,
      "step": 499800
    },
    {
      "epoch": 4.561464340462807,
      "grad_norm": 5.164600849151611,
      "learning_rate": 4.6198779716280996e-05,
      "loss": 0.7586,
      "step": 499900
    },
    {
      "epoch": 4.5623768158259725,
      "grad_norm": 4.142490863800049,
      "learning_rate": 4.6198019320145026e-05,
      "loss": 0.7387,
      "step": 500000
    },
    {
      "epoch": 4.563289291189138,
      "grad_norm": 3.508958339691162,
      "learning_rate": 4.619725892400905e-05,
      "loss": 0.6777,
      "step": 500100
    },
    {
      "epoch": 4.564201766552303,
      "grad_norm": 4.680315971374512,
      "learning_rate": 4.6196498527873086e-05,
      "loss": 0.7087,
      "step": 500200
    },
    {
      "epoch": 4.565114241915468,
      "grad_norm": 4.352699279785156,
      "learning_rate": 4.619573813173711e-05,
      "loss": 0.6832,
      "step": 500300
    },
    {
      "epoch": 4.566026717278634,
      "grad_norm": 3.941730260848999,
      "learning_rate": 4.619497773560114e-05,
      "loss": 0.749,
      "step": 500400
    },
    {
      "epoch": 4.566939192641799,
      "grad_norm": 4.277032852172852,
      "learning_rate": 4.619421733946517e-05,
      "loss": 0.7034,
      "step": 500500
    },
    {
      "epoch": 4.567851668004964,
      "grad_norm": 3.4457757472991943,
      "learning_rate": 4.61934569433292e-05,
      "loss": 0.7061,
      "step": 500600
    },
    {
      "epoch": 4.568764143368129,
      "grad_norm": 3.4947381019592285,
      "learning_rate": 4.619269654719322e-05,
      "loss": 0.6859,
      "step": 500700
    },
    {
      "epoch": 4.569676618731294,
      "grad_norm": 3.9290215969085693,
      "learning_rate": 4.619193615105726e-05,
      "loss": 0.6821,
      "step": 500800
    },
    {
      "epoch": 4.570589094094459,
      "grad_norm": 4.402050495147705,
      "learning_rate": 4.619117575492128e-05,
      "loss": 0.7295,
      "step": 500900
    },
    {
      "epoch": 4.571501569457625,
      "grad_norm": 3.2165815830230713,
      "learning_rate": 4.619041535878531e-05,
      "loss": 0.7336,
      "step": 501000
    },
    {
      "epoch": 4.57241404482079,
      "grad_norm": 3.8179726600646973,
      "learning_rate": 4.618965496264934e-05,
      "loss": 0.6925,
      "step": 501100
    },
    {
      "epoch": 4.573326520183955,
      "grad_norm": 4.240828990936279,
      "learning_rate": 4.618889456651337e-05,
      "loss": 0.746,
      "step": 501200
    },
    {
      "epoch": 4.5742389955471205,
      "grad_norm": 3.81907320022583,
      "learning_rate": 4.61881341703774e-05,
      "loss": 0.708,
      "step": 501300
    },
    {
      "epoch": 4.575151470910285,
      "grad_norm": 3.536604404449463,
      "learning_rate": 4.618737377424143e-05,
      "loss": 0.7036,
      "step": 501400
    },
    {
      "epoch": 4.57606394627345,
      "grad_norm": 4.402661323547363,
      "learning_rate": 4.6186613378105457e-05,
      "loss": 0.7168,
      "step": 501500
    },
    {
      "epoch": 4.5769764216366156,
      "grad_norm": 4.136289596557617,
      "learning_rate": 4.6185852981969493e-05,
      "loss": 0.7472,
      "step": 501600
    },
    {
      "epoch": 4.577888896999781,
      "grad_norm": 4.073991775512695,
      "learning_rate": 4.618509258583352e-05,
      "loss": 0.7567,
      "step": 501700
    },
    {
      "epoch": 4.578801372362946,
      "grad_norm": 4.115298748016357,
      "learning_rate": 4.618433218969755e-05,
      "loss": 0.7418,
      "step": 501800
    },
    {
      "epoch": 4.5797138477261115,
      "grad_norm": 4.662564277648926,
      "learning_rate": 4.618357179356158e-05,
      "loss": 0.7156,
      "step": 501900
    },
    {
      "epoch": 4.580626323089277,
      "grad_norm": 4.252460479736328,
      "learning_rate": 4.618281139742561e-05,
      "loss": 0.7265,
      "step": 502000
    },
    {
      "epoch": 4.581538798452442,
      "grad_norm": 5.090936183929443,
      "learning_rate": 4.618205100128963e-05,
      "loss": 0.71,
      "step": 502100
    },
    {
      "epoch": 4.582451273815607,
      "grad_norm": 4.311941146850586,
      "learning_rate": 4.618129060515367e-05,
      "loss": 0.7129,
      "step": 502200
    },
    {
      "epoch": 4.583363749178773,
      "grad_norm": 3.5374741554260254,
      "learning_rate": 4.618053020901769e-05,
      "loss": 0.6955,
      "step": 502300
    },
    {
      "epoch": 4.584276224541937,
      "grad_norm": 4.343044757843018,
      "learning_rate": 4.617976981288172e-05,
      "loss": 0.7159,
      "step": 502400
    },
    {
      "epoch": 4.585188699905102,
      "grad_norm": 3.950758457183838,
      "learning_rate": 4.617900941674575e-05,
      "loss": 0.6771,
      "step": 502500
    },
    {
      "epoch": 4.586101175268268,
      "grad_norm": 4.739500999450684,
      "learning_rate": 4.617824902060978e-05,
      "loss": 0.7052,
      "step": 502600
    },
    {
      "epoch": 4.587013650631433,
      "grad_norm": 3.8669233322143555,
      "learning_rate": 4.617748862447381e-05,
      "loss": 0.703,
      "step": 502700
    },
    {
      "epoch": 4.587926125994598,
      "grad_norm": 2.6438097953796387,
      "learning_rate": 4.6176728228337834e-05,
      "loss": 0.7055,
      "step": 502800
    },
    {
      "epoch": 4.588838601357764,
      "grad_norm": 4.815332412719727,
      "learning_rate": 4.6175967832201864e-05,
      "loss": 0.7218,
      "step": 502900
    },
    {
      "epoch": 4.589751076720929,
      "grad_norm": 3.9594712257385254,
      "learning_rate": 4.6175207436065894e-05,
      "loss": 0.7386,
      "step": 503000
    },
    {
      "epoch": 4.590663552084093,
      "grad_norm": 4.812444686889648,
      "learning_rate": 4.6174447039929924e-05,
      "loss": 0.6699,
      "step": 503100
    },
    {
      "epoch": 4.591576027447259,
      "grad_norm": 3.5596442222595215,
      "learning_rate": 4.6173686643793954e-05,
      "loss": 0.7096,
      "step": 503200
    },
    {
      "epoch": 4.592488502810424,
      "grad_norm": 3.849217653274536,
      "learning_rate": 4.6172926247657984e-05,
      "loss": 0.7569,
      "step": 503300
    },
    {
      "epoch": 4.593400978173589,
      "grad_norm": 3.4870803356170654,
      "learning_rate": 4.617216585152201e-05,
      "loss": 0.7453,
      "step": 503400
    },
    {
      "epoch": 4.5943134535367545,
      "grad_norm": 3.666433334350586,
      "learning_rate": 4.617140545538604e-05,
      "loss": 0.6734,
      "step": 503500
    },
    {
      "epoch": 4.59522592889992,
      "grad_norm": 3.980403184890747,
      "learning_rate": 4.617064505925007e-05,
      "loss": 0.7434,
      "step": 503600
    },
    {
      "epoch": 4.596138404263085,
      "grad_norm": 4.723818302154541,
      "learning_rate": 4.61698846631141e-05,
      "loss": 0.7146,
      "step": 503700
    },
    {
      "epoch": 4.59705087962625,
      "grad_norm": 2.260962963104248,
      "learning_rate": 4.616912426697813e-05,
      "loss": 0.7158,
      "step": 503800
    },
    {
      "epoch": 4.597963354989416,
      "grad_norm": 4.309844970703125,
      "learning_rate": 4.616836387084216e-05,
      "loss": 0.7298,
      "step": 503900
    },
    {
      "epoch": 4.598875830352581,
      "grad_norm": 4.077917575836182,
      "learning_rate": 4.616760347470618e-05,
      "loss": 0.7312,
      "step": 504000
    },
    {
      "epoch": 4.599788305715745,
      "grad_norm": 4.657540798187256,
      "learning_rate": 4.616684307857022e-05,
      "loss": 0.7786,
      "step": 504100
    },
    {
      "epoch": 4.600700781078911,
      "grad_norm": 5.611563205718994,
      "learning_rate": 4.616608268243424e-05,
      "loss": 0.7525,
      "step": 504200
    },
    {
      "epoch": 4.601613256442076,
      "grad_norm": 4.005708694458008,
      "learning_rate": 4.616532228629827e-05,
      "loss": 0.6895,
      "step": 504300
    },
    {
      "epoch": 4.602525731805241,
      "grad_norm": 4.5242600440979,
      "learning_rate": 4.61645618901623e-05,
      "loss": 0.6945,
      "step": 504400
    },
    {
      "epoch": 4.603438207168407,
      "grad_norm": 4.380107879638672,
      "learning_rate": 4.616380149402633e-05,
      "loss": 0.7224,
      "step": 504500
    },
    {
      "epoch": 4.604350682531572,
      "grad_norm": 4.258439540863037,
      "learning_rate": 4.616304109789036e-05,
      "loss": 0.715,
      "step": 504600
    },
    {
      "epoch": 4.605263157894737,
      "grad_norm": 3.951503276824951,
      "learning_rate": 4.616228070175439e-05,
      "loss": 0.7123,
      "step": 504700
    },
    {
      "epoch": 4.606175633257902,
      "grad_norm": 3.652927875518799,
      "learning_rate": 4.6161520305618415e-05,
      "loss": 0.6947,
      "step": 504800
    },
    {
      "epoch": 4.607088108621067,
      "grad_norm": 3.739908218383789,
      "learning_rate": 4.6160759909482445e-05,
      "loss": 0.7487,
      "step": 504900
    },
    {
      "epoch": 4.608000583984232,
      "grad_norm": 5.148301601409912,
      "learning_rate": 4.6159999513346475e-05,
      "loss": 0.7385,
      "step": 505000
    },
    {
      "epoch": 4.6089130593473975,
      "grad_norm": 3.8440897464752197,
      "learning_rate": 4.6159239117210505e-05,
      "loss": 0.6788,
      "step": 505100
    },
    {
      "epoch": 4.609825534710563,
      "grad_norm": 4.009889125823975,
      "learning_rate": 4.6158478721074535e-05,
      "loss": 0.7405,
      "step": 505200
    },
    {
      "epoch": 4.610738010073728,
      "grad_norm": 5.01583194732666,
      "learning_rate": 4.6157718324938565e-05,
      "loss": 0.7018,
      "step": 505300
    },
    {
      "epoch": 4.611650485436893,
      "grad_norm": 4.016950607299805,
      "learning_rate": 4.615695792880259e-05,
      "loss": 0.7106,
      "step": 505400
    },
    {
      "epoch": 4.612562960800059,
      "grad_norm": 4.266058921813965,
      "learning_rate": 4.6156197532666625e-05,
      "loss": 0.7066,
      "step": 505500
    },
    {
      "epoch": 4.613475436163224,
      "grad_norm": 4.078530311584473,
      "learning_rate": 4.615543713653065e-05,
      "loss": 0.7272,
      "step": 505600
    },
    {
      "epoch": 4.614387911526389,
      "grad_norm": 3.4670462608337402,
      "learning_rate": 4.615467674039468e-05,
      "loss": 0.7322,
      "step": 505700
    },
    {
      "epoch": 4.615300386889554,
      "grad_norm": 4.3314008712768555,
      "learning_rate": 4.615391634425871e-05,
      "loss": 0.6689,
      "step": 505800
    },
    {
      "epoch": 4.616212862252719,
      "grad_norm": 2.6264963150024414,
      "learning_rate": 4.615315594812273e-05,
      "loss": 0.7454,
      "step": 505900
    },
    {
      "epoch": 4.617125337615884,
      "grad_norm": 3.767552614212036,
      "learning_rate": 4.615239555198677e-05,
      "loss": 0.7247,
      "step": 506000
    },
    {
      "epoch": 4.61803781297905,
      "grad_norm": 4.069126605987549,
      "learning_rate": 4.615163515585079e-05,
      "loss": 0.72,
      "step": 506100
    },
    {
      "epoch": 4.618950288342215,
      "grad_norm": 4.146968841552734,
      "learning_rate": 4.615087475971482e-05,
      "loss": 0.6862,
      "step": 506200
    },
    {
      "epoch": 4.61986276370538,
      "grad_norm": 3.644399881362915,
      "learning_rate": 4.615011436357885e-05,
      "loss": 0.7398,
      "step": 506300
    },
    {
      "epoch": 4.6207752390685455,
      "grad_norm": 4.149961948394775,
      "learning_rate": 4.614935396744288e-05,
      "loss": 0.7425,
      "step": 506400
    },
    {
      "epoch": 4.62168771443171,
      "grad_norm": 4.154435157775879,
      "learning_rate": 4.6148593571306906e-05,
      "loss": 0.7001,
      "step": 506500
    },
    {
      "epoch": 4.622600189794875,
      "grad_norm": 4.419651508331299,
      "learning_rate": 4.614783317517094e-05,
      "loss": 0.7538,
      "step": 506600
    },
    {
      "epoch": 4.6235126651580405,
      "grad_norm": 3.297146797180176,
      "learning_rate": 4.6147072779034966e-05,
      "loss": 0.6978,
      "step": 506700
    },
    {
      "epoch": 4.624425140521206,
      "grad_norm": 4.67571496963501,
      "learning_rate": 4.6146312382898996e-05,
      "loss": 0.7234,
      "step": 506800
    },
    {
      "epoch": 4.625337615884371,
      "grad_norm": 3.7542030811309814,
      "learning_rate": 4.6145551986763026e-05,
      "loss": 0.7043,
      "step": 506900
    },
    {
      "epoch": 4.626250091247536,
      "grad_norm": 4.748025894165039,
      "learning_rate": 4.6144791590627056e-05,
      "loss": 0.7453,
      "step": 507000
    },
    {
      "epoch": 4.627162566610702,
      "grad_norm": 3.0865957736968994,
      "learning_rate": 4.6144031194491086e-05,
      "loss": 0.7185,
      "step": 507100
    },
    {
      "epoch": 4.628075041973867,
      "grad_norm": 4.572257041931152,
      "learning_rate": 4.6143270798355116e-05,
      "loss": 0.7023,
      "step": 507200
    },
    {
      "epoch": 4.628987517337032,
      "grad_norm": 4.3534746170043945,
      "learning_rate": 4.614251040221914e-05,
      "loss": 0.7211,
      "step": 507300
    },
    {
      "epoch": 4.629899992700198,
      "grad_norm": 4.373594760894775,
      "learning_rate": 4.6141750006083176e-05,
      "loss": 0.7424,
      "step": 507400
    },
    {
      "epoch": 4.630812468063362,
      "grad_norm": 4.0305657386779785,
      "learning_rate": 4.61409896099472e-05,
      "loss": 0.6974,
      "step": 507500
    },
    {
      "epoch": 4.631724943426527,
      "grad_norm": 4.301596641540527,
      "learning_rate": 4.614022921381123e-05,
      "loss": 0.6971,
      "step": 507600
    },
    {
      "epoch": 4.632637418789693,
      "grad_norm": 4.556138038635254,
      "learning_rate": 4.613946881767526e-05,
      "loss": 0.7497,
      "step": 507700
    },
    {
      "epoch": 4.633549894152858,
      "grad_norm": 4.13594388961792,
      "learning_rate": 4.613870842153929e-05,
      "loss": 0.6982,
      "step": 507800
    },
    {
      "epoch": 4.634462369516023,
      "grad_norm": 5.079357147216797,
      "learning_rate": 4.613794802540331e-05,
      "loss": 0.7367,
      "step": 507900
    },
    {
      "epoch": 4.6353748448791885,
      "grad_norm": 3.7355308532714844,
      "learning_rate": 4.613718762926735e-05,
      "loss": 0.6531,
      "step": 508000
    },
    {
      "epoch": 4.636287320242354,
      "grad_norm": 3.539761543273926,
      "learning_rate": 4.613642723313137e-05,
      "loss": 0.7152,
      "step": 508100
    },
    {
      "epoch": 4.637199795605518,
      "grad_norm": 3.9005370140075684,
      "learning_rate": 4.61356668369954e-05,
      "loss": 0.7329,
      "step": 508200
    },
    {
      "epoch": 4.6381122709686835,
      "grad_norm": 4.033259391784668,
      "learning_rate": 4.613490644085943e-05,
      "loss": 0.7403,
      "step": 508300
    },
    {
      "epoch": 4.639024746331849,
      "grad_norm": 3.647102117538452,
      "learning_rate": 4.613414604472346e-05,
      "loss": 0.7249,
      "step": 508400
    },
    {
      "epoch": 4.639937221695014,
      "grad_norm": 3.4515445232391357,
      "learning_rate": 4.6133385648587494e-05,
      "loss": 0.759,
      "step": 508500
    },
    {
      "epoch": 4.640849697058179,
      "grad_norm": 3.122008800506592,
      "learning_rate": 4.613262525245152e-05,
      "loss": 0.674,
      "step": 508600
    },
    {
      "epoch": 4.641762172421345,
      "grad_norm": 5.0833516120910645,
      "learning_rate": 4.613186485631555e-05,
      "loss": 0.745,
      "step": 508700
    },
    {
      "epoch": 4.64267464778451,
      "grad_norm": 3.6173787117004395,
      "learning_rate": 4.613110446017958e-05,
      "loss": 0.713,
      "step": 508800
    },
    {
      "epoch": 4.643587123147675,
      "grad_norm": 3.895803689956665,
      "learning_rate": 4.613034406404361e-05,
      "loss": 0.7056,
      "step": 508900
    },
    {
      "epoch": 4.644499598510841,
      "grad_norm": 3.94851016998291,
      "learning_rate": 4.612958366790763e-05,
      "loss": 0.7351,
      "step": 509000
    },
    {
      "epoch": 4.645412073874006,
      "grad_norm": 4.092726707458496,
      "learning_rate": 4.612882327177167e-05,
      "loss": 0.7088,
      "step": 509100
    },
    {
      "epoch": 4.64632454923717,
      "grad_norm": 3.7738029956817627,
      "learning_rate": 4.612806287563569e-05,
      "loss": 0.7173,
      "step": 509200
    },
    {
      "epoch": 4.647237024600336,
      "grad_norm": 3.8721346855163574,
      "learning_rate": 4.612730247949972e-05,
      "loss": 0.7631,
      "step": 509300
    },
    {
      "epoch": 4.648149499963501,
      "grad_norm": 4.850299835205078,
      "learning_rate": 4.612654208336375e-05,
      "loss": 0.7484,
      "step": 509400
    },
    {
      "epoch": 4.649061975326666,
      "grad_norm": 3.967766523361206,
      "learning_rate": 4.612578168722778e-05,
      "loss": 0.7176,
      "step": 509500
    },
    {
      "epoch": 4.6499744506898315,
      "grad_norm": 4.15593147277832,
      "learning_rate": 4.612502129109181e-05,
      "loss": 0.7562,
      "step": 509600
    },
    {
      "epoch": 4.650886926052997,
      "grad_norm": 4.3481364250183105,
      "learning_rate": 4.612426089495584e-05,
      "loss": 0.7465,
      "step": 509700
    },
    {
      "epoch": 4.651799401416162,
      "grad_norm": 3.850628137588501,
      "learning_rate": 4.6123500498819864e-05,
      "loss": 0.7465,
      "step": 509800
    },
    {
      "epoch": 4.6527118767793265,
      "grad_norm": 3.2953147888183594,
      "learning_rate": 4.61227401026839e-05,
      "loss": 0.6761,
      "step": 509900
    },
    {
      "epoch": 4.653624352142492,
      "grad_norm": 4.301468849182129,
      "learning_rate": 4.6121979706547924e-05,
      "loss": 0.7171,
      "step": 510000
    },
    {
      "epoch": 4.654536827505657,
      "grad_norm": 3.0047972202301025,
      "learning_rate": 4.6121219310411954e-05,
      "loss": 0.7854,
      "step": 510100
    },
    {
      "epoch": 4.655449302868822,
      "grad_norm": 3.9338908195495605,
      "learning_rate": 4.6120458914275984e-05,
      "loss": 0.7426,
      "step": 510200
    },
    {
      "epoch": 4.656361778231988,
      "grad_norm": 3.8065969944000244,
      "learning_rate": 4.6119698518140014e-05,
      "loss": 0.725,
      "step": 510300
    },
    {
      "epoch": 4.657274253595153,
      "grad_norm": 3.7041146755218506,
      "learning_rate": 4.611893812200404e-05,
      "loss": 0.6927,
      "step": 510400
    },
    {
      "epoch": 4.658186728958318,
      "grad_norm": 3.644251823425293,
      "learning_rate": 4.6118177725868075e-05,
      "loss": 0.7611,
      "step": 510500
    },
    {
      "epoch": 4.659099204321484,
      "grad_norm": 4.014459133148193,
      "learning_rate": 4.61174173297321e-05,
      "loss": 0.6777,
      "step": 510600
    },
    {
      "epoch": 4.660011679684649,
      "grad_norm": 4.124809265136719,
      "learning_rate": 4.611665693359613e-05,
      "loss": 0.7242,
      "step": 510700
    },
    {
      "epoch": 4.660924155047813,
      "grad_norm": 5.401961803436279,
      "learning_rate": 4.611589653746016e-05,
      "loss": 0.7005,
      "step": 510800
    },
    {
      "epoch": 4.661836630410979,
      "grad_norm": 3.741236448287964,
      "learning_rate": 4.611513614132419e-05,
      "loss": 0.7212,
      "step": 510900
    },
    {
      "epoch": 4.662749105774144,
      "grad_norm": 4.438868999481201,
      "learning_rate": 4.611437574518822e-05,
      "loss": 0.7192,
      "step": 511000
    },
    {
      "epoch": 4.663661581137309,
      "grad_norm": 2.575740337371826,
      "learning_rate": 4.611361534905225e-05,
      "loss": 0.7371,
      "step": 511100
    },
    {
      "epoch": 4.6645740565004745,
      "grad_norm": 4.903867721557617,
      "learning_rate": 4.611285495291627e-05,
      "loss": 0.6869,
      "step": 511200
    },
    {
      "epoch": 4.66548653186364,
      "grad_norm": 3.635877847671509,
      "learning_rate": 4.61120945567803e-05,
      "loss": 0.7208,
      "step": 511300
    },
    {
      "epoch": 4.666399007226805,
      "grad_norm": 3.983445882797241,
      "learning_rate": 4.611133416064433e-05,
      "loss": 0.7317,
      "step": 511400
    },
    {
      "epoch": 4.66731148258997,
      "grad_norm": 4.481529235839844,
      "learning_rate": 4.6110573764508355e-05,
      "loss": 0.73,
      "step": 511500
    },
    {
      "epoch": 4.668223957953135,
      "grad_norm": 4.205370903015137,
      "learning_rate": 4.610981336837239e-05,
      "loss": 0.7215,
      "step": 511600
    },
    {
      "epoch": 4.6691364333163,
      "grad_norm": 4.269013404846191,
      "learning_rate": 4.6109052972236415e-05,
      "loss": 0.6859,
      "step": 511700
    },
    {
      "epoch": 4.670048908679465,
      "grad_norm": 3.7031264305114746,
      "learning_rate": 4.6108292576100445e-05,
      "loss": 0.7158,
      "step": 511800
    },
    {
      "epoch": 4.670961384042631,
      "grad_norm": 3.7152552604675293,
      "learning_rate": 4.6107532179964475e-05,
      "loss": 0.6887,
      "step": 511900
    },
    {
      "epoch": 4.671873859405796,
      "grad_norm": 3.846876859664917,
      "learning_rate": 4.6106771783828505e-05,
      "loss": 0.6969,
      "step": 512000
    },
    {
      "epoch": 4.672786334768961,
      "grad_norm": 4.4179205894470215,
      "learning_rate": 4.6106011387692535e-05,
      "loss": 0.7027,
      "step": 512100
    },
    {
      "epoch": 4.673698810132127,
      "grad_norm": 4.347372531890869,
      "learning_rate": 4.6105250991556565e-05,
      "loss": 0.7122,
      "step": 512200
    },
    {
      "epoch": 4.674611285495292,
      "grad_norm": 5.553456783294678,
      "learning_rate": 4.610449059542059e-05,
      "loss": 0.7058,
      "step": 512300
    },
    {
      "epoch": 4.675523760858457,
      "grad_norm": 3.977886199951172,
      "learning_rate": 4.6103730199284625e-05,
      "loss": 0.721,
      "step": 512400
    },
    {
      "epoch": 4.676436236221622,
      "grad_norm": 3.336893081665039,
      "learning_rate": 4.610296980314865e-05,
      "loss": 0.7462,
      "step": 512500
    },
    {
      "epoch": 4.677348711584787,
      "grad_norm": 3.7231545448303223,
      "learning_rate": 4.610220940701268e-05,
      "loss": 0.7075,
      "step": 512600
    },
    {
      "epoch": 4.678261186947952,
      "grad_norm": 4.086382865905762,
      "learning_rate": 4.610144901087671e-05,
      "loss": 0.7163,
      "step": 512700
    },
    {
      "epoch": 4.6791736623111175,
      "grad_norm": 3.3097198009490967,
      "learning_rate": 4.610068861474074e-05,
      "loss": 0.7399,
      "step": 512800
    },
    {
      "epoch": 4.680086137674283,
      "grad_norm": 4.9279255867004395,
      "learning_rate": 4.609992821860476e-05,
      "loss": 0.7361,
      "step": 512900
    },
    {
      "epoch": 4.680998613037448,
      "grad_norm": 4.9522809982299805,
      "learning_rate": 4.60991678224688e-05,
      "loss": 0.7249,
      "step": 513000
    },
    {
      "epoch": 4.681911088400613,
      "grad_norm": 4.355147838592529,
      "learning_rate": 4.609840742633282e-05,
      "loss": 0.7394,
      "step": 513100
    },
    {
      "epoch": 4.682823563763779,
      "grad_norm": 4.50439977645874,
      "learning_rate": 4.609764703019685e-05,
      "loss": 0.7323,
      "step": 513200
    },
    {
      "epoch": 4.683736039126943,
      "grad_norm": 4.085158824920654,
      "learning_rate": 4.609688663406088e-05,
      "loss": 0.7354,
      "step": 513300
    },
    {
      "epoch": 4.684648514490108,
      "grad_norm": 3.968966484069824,
      "learning_rate": 4.609612623792491e-05,
      "loss": 0.7019,
      "step": 513400
    },
    {
      "epoch": 4.685560989853274,
      "grad_norm": 3.7671539783477783,
      "learning_rate": 4.609536584178894e-05,
      "loss": 0.7256,
      "step": 513500
    },
    {
      "epoch": 4.686473465216439,
      "grad_norm": 3.7030420303344727,
      "learning_rate": 4.609460544565297e-05,
      "loss": 0.6913,
      "step": 513600
    },
    {
      "epoch": 4.687385940579604,
      "grad_norm": 5.168842315673828,
      "learning_rate": 4.6093845049516996e-05,
      "loss": 0.6925,
      "step": 513700
    },
    {
      "epoch": 4.68829841594277,
      "grad_norm": 3.155468702316284,
      "learning_rate": 4.609308465338103e-05,
      "loss": 0.7061,
      "step": 513800
    },
    {
      "epoch": 4.689210891305935,
      "grad_norm": 4.1451497077941895,
      "learning_rate": 4.6092324257245056e-05,
      "loss": 0.7201,
      "step": 513900
    },
    {
      "epoch": 4.6901233666691,
      "grad_norm": 4.355569362640381,
      "learning_rate": 4.6091563861109086e-05,
      "loss": 0.7313,
      "step": 514000
    },
    {
      "epoch": 4.6910358420322655,
      "grad_norm": 3.4266371726989746,
      "learning_rate": 4.6090803464973116e-05,
      "loss": 0.7037,
      "step": 514100
    },
    {
      "epoch": 4.69194831739543,
      "grad_norm": 4.048222541809082,
      "learning_rate": 4.609004306883714e-05,
      "loss": 0.7459,
      "step": 514200
    },
    {
      "epoch": 4.692860792758595,
      "grad_norm": 4.777480125427246,
      "learning_rate": 4.608928267270117e-05,
      "loss": 0.719,
      "step": 514300
    },
    {
      "epoch": 4.6937732681217605,
      "grad_norm": 2.912986993789673,
      "learning_rate": 4.60885222765652e-05,
      "loss": 0.6874,
      "step": 514400
    },
    {
      "epoch": 4.694685743484926,
      "grad_norm": 4.731987953186035,
      "learning_rate": 4.608776188042923e-05,
      "loss": 0.7087,
      "step": 514500
    },
    {
      "epoch": 4.695598218848091,
      "grad_norm": 3.7222447395324707,
      "learning_rate": 4.608700148429326e-05,
      "loss": 0.7441,
      "step": 514600
    },
    {
      "epoch": 4.696510694211256,
      "grad_norm": 3.970637559890747,
      "learning_rate": 4.608624108815729e-05,
      "loss": 0.721,
      "step": 514700
    },
    {
      "epoch": 4.697423169574422,
      "grad_norm": 4.1678667068481445,
      "learning_rate": 4.608548069202131e-05,
      "loss": 0.7409,
      "step": 514800
    },
    {
      "epoch": 4.698335644937587,
      "grad_norm": 4.498523712158203,
      "learning_rate": 4.608472029588535e-05,
      "loss": 0.6807,
      "step": 514900
    },
    {
      "epoch": 4.6992481203007515,
      "grad_norm": 3.4794816970825195,
      "learning_rate": 4.608395989974937e-05,
      "loss": 0.742,
      "step": 515000
    },
    {
      "epoch": 4.700160595663917,
      "grad_norm": 4.075324535369873,
      "learning_rate": 4.6083199503613403e-05,
      "loss": 0.7012,
      "step": 515100
    },
    {
      "epoch": 4.701073071027082,
      "grad_norm": 4.349256992340088,
      "learning_rate": 4.6082439107477433e-05,
      "loss": 0.7531,
      "step": 515200
    },
    {
      "epoch": 4.701985546390247,
      "grad_norm": 4.572941303253174,
      "learning_rate": 4.6081678711341464e-05,
      "loss": 0.7309,
      "step": 515300
    },
    {
      "epoch": 4.702898021753413,
      "grad_norm": 4.065393447875977,
      "learning_rate": 4.608091831520549e-05,
      "loss": 0.7614,
      "step": 515400
    },
    {
      "epoch": 4.703810497116578,
      "grad_norm": 3.3852829933166504,
      "learning_rate": 4.6080157919069524e-05,
      "loss": 0.716,
      "step": 515500
    },
    {
      "epoch": 4.704722972479743,
      "grad_norm": 3.9475693702697754,
      "learning_rate": 4.607939752293355e-05,
      "loss": 0.6903,
      "step": 515600
    },
    {
      "epoch": 4.7056354478429085,
      "grad_norm": 3.788588523864746,
      "learning_rate": 4.607863712679758e-05,
      "loss": 0.7128,
      "step": 515700
    },
    {
      "epoch": 4.706547923206074,
      "grad_norm": 4.530508041381836,
      "learning_rate": 4.607787673066161e-05,
      "loss": 0.7598,
      "step": 515800
    },
    {
      "epoch": 4.707460398569238,
      "grad_norm": 4.41411018371582,
      "learning_rate": 4.607711633452564e-05,
      "loss": 0.7114,
      "step": 515900
    },
    {
      "epoch": 4.708372873932404,
      "grad_norm": 3.9536075592041016,
      "learning_rate": 4.607635593838967e-05,
      "loss": 0.7469,
      "step": 516000
    },
    {
      "epoch": 4.709285349295569,
      "grad_norm": 4.661015033721924,
      "learning_rate": 4.60755955422537e-05,
      "loss": 0.7406,
      "step": 516100
    },
    {
      "epoch": 4.710197824658734,
      "grad_norm": 3.8141250610351562,
      "learning_rate": 4.607483514611772e-05,
      "loss": 0.7245,
      "step": 516200
    },
    {
      "epoch": 4.7111103000218995,
      "grad_norm": 4.33654260635376,
      "learning_rate": 4.607407474998176e-05,
      "loss": 0.7308,
      "step": 516300
    },
    {
      "epoch": 4.712022775385065,
      "grad_norm": 4.569074630737305,
      "learning_rate": 4.607331435384578e-05,
      "loss": 0.731,
      "step": 516400
    },
    {
      "epoch": 4.71293525074823,
      "grad_norm": 3.1359498500823975,
      "learning_rate": 4.607255395770981e-05,
      "loss": 0.7763,
      "step": 516500
    },
    {
      "epoch": 4.713847726111395,
      "grad_norm": 3.1633551120758057,
      "learning_rate": 4.607179356157384e-05,
      "loss": 0.7111,
      "step": 516600
    },
    {
      "epoch": 4.71476020147456,
      "grad_norm": 3.9844038486480713,
      "learning_rate": 4.607103316543787e-05,
      "loss": 0.7253,
      "step": 516700
    },
    {
      "epoch": 4.715672676837725,
      "grad_norm": 4.540469646453857,
      "learning_rate": 4.60702727693019e-05,
      "loss": 0.7128,
      "step": 516800
    },
    {
      "epoch": 4.71658515220089,
      "grad_norm": 3.3358826637268066,
      "learning_rate": 4.606951237316593e-05,
      "loss": 0.7357,
      "step": 516900
    },
    {
      "epoch": 4.717497627564056,
      "grad_norm": 3.8342957496643066,
      "learning_rate": 4.6068751977029954e-05,
      "loss": 0.7156,
      "step": 517000
    },
    {
      "epoch": 4.718410102927221,
      "grad_norm": 2.6725051403045654,
      "learning_rate": 4.6067991580893984e-05,
      "loss": 0.7426,
      "step": 517100
    },
    {
      "epoch": 4.719322578290386,
      "grad_norm": 4.744033336639404,
      "learning_rate": 4.6067231184758014e-05,
      "loss": 0.6893,
      "step": 517200
    },
    {
      "epoch": 4.720235053653552,
      "grad_norm": 4.242143154144287,
      "learning_rate": 4.606647078862204e-05,
      "loss": 0.7355,
      "step": 517300
    },
    {
      "epoch": 4.721147529016717,
      "grad_norm": 3.74057936668396,
      "learning_rate": 4.6065710392486075e-05,
      "loss": 0.7086,
      "step": 517400
    },
    {
      "epoch": 4.722060004379882,
      "grad_norm": 5.307298183441162,
      "learning_rate": 4.60649499963501e-05,
      "loss": 0.6963,
      "step": 517500
    },
    {
      "epoch": 4.722972479743047,
      "grad_norm": 4.27975606918335,
      "learning_rate": 4.606418960021413e-05,
      "loss": 0.7731,
      "step": 517600
    },
    {
      "epoch": 4.723884955106212,
      "grad_norm": 5.426167964935303,
      "learning_rate": 4.606342920407816e-05,
      "loss": 0.7352,
      "step": 517700
    },
    {
      "epoch": 4.724797430469377,
      "grad_norm": 4.6832451820373535,
      "learning_rate": 4.606266880794219e-05,
      "loss": 0.7447,
      "step": 517800
    },
    {
      "epoch": 4.7257099058325425,
      "grad_norm": 4.207468509674072,
      "learning_rate": 4.606190841180622e-05,
      "loss": 0.7248,
      "step": 517900
    },
    {
      "epoch": 4.726622381195708,
      "grad_norm": 4.7027435302734375,
      "learning_rate": 4.606114801567025e-05,
      "loss": 0.7208,
      "step": 518000
    },
    {
      "epoch": 4.727534856558873,
      "grad_norm": 3.4098093509674072,
      "learning_rate": 4.606038761953427e-05,
      "loss": 0.7424,
      "step": 518100
    },
    {
      "epoch": 4.728447331922038,
      "grad_norm": 3.5475597381591797,
      "learning_rate": 4.605962722339831e-05,
      "loss": 0.7459,
      "step": 518200
    },
    {
      "epoch": 4.729359807285204,
      "grad_norm": 3.5548319816589355,
      "learning_rate": 4.605886682726233e-05,
      "loss": 0.667,
      "step": 518300
    },
    {
      "epoch": 4.730272282648368,
      "grad_norm": 4.50662088394165,
      "learning_rate": 4.605810643112636e-05,
      "loss": 0.6799,
      "step": 518400
    },
    {
      "epoch": 4.731184758011533,
      "grad_norm": 3.9188194274902344,
      "learning_rate": 4.605734603499039e-05,
      "loss": 0.7286,
      "step": 518500
    },
    {
      "epoch": 4.732097233374699,
      "grad_norm": 3.7193830013275146,
      "learning_rate": 4.605658563885442e-05,
      "loss": 0.6964,
      "step": 518600
    },
    {
      "epoch": 4.733009708737864,
      "grad_norm": 4.412728786468506,
      "learning_rate": 4.6055825242718445e-05,
      "loss": 0.7256,
      "step": 518700
    },
    {
      "epoch": 4.733922184101029,
      "grad_norm": 3.669750928878784,
      "learning_rate": 4.605506484658248e-05,
      "loss": 0.705,
      "step": 518800
    },
    {
      "epoch": 4.734834659464195,
      "grad_norm": 3.324552536010742,
      "learning_rate": 4.6054304450446505e-05,
      "loss": 0.6875,
      "step": 518900
    },
    {
      "epoch": 4.73574713482736,
      "grad_norm": 4.540259838104248,
      "learning_rate": 4.6053544054310535e-05,
      "loss": 0.7441,
      "step": 519000
    },
    {
      "epoch": 4.736659610190525,
      "grad_norm": 4.132631301879883,
      "learning_rate": 4.6052783658174565e-05,
      "loss": 0.6971,
      "step": 519100
    },
    {
      "epoch": 4.7375720855536905,
      "grad_norm": 3.873325824737549,
      "learning_rate": 4.6052023262038596e-05,
      "loss": 0.7344,
      "step": 519200
    },
    {
      "epoch": 4.738484560916855,
      "grad_norm": 4.9890055656433105,
      "learning_rate": 4.6051262865902626e-05,
      "loss": 0.719,
      "step": 519300
    },
    {
      "epoch": 4.73939703628002,
      "grad_norm": 3.2899911403656006,
      "learning_rate": 4.6050502469766656e-05,
      "loss": 0.7428,
      "step": 519400
    },
    {
      "epoch": 4.7403095116431855,
      "grad_norm": 4.780422210693359,
      "learning_rate": 4.604974207363068e-05,
      "loss": 0.6985,
      "step": 519500
    },
    {
      "epoch": 4.741221987006351,
      "grad_norm": 4.289749622344971,
      "learning_rate": 4.6048981677494716e-05,
      "loss": 0.7511,
      "step": 519600
    },
    {
      "epoch": 4.742134462369516,
      "grad_norm": 2.4877524375915527,
      "learning_rate": 4.604822128135874e-05,
      "loss": 0.7415,
      "step": 519700
    },
    {
      "epoch": 4.743046937732681,
      "grad_norm": 3.0803170204162598,
      "learning_rate": 4.604746088522276e-05,
      "loss": 0.7095,
      "step": 519800
    },
    {
      "epoch": 4.743959413095847,
      "grad_norm": 4.066601276397705,
      "learning_rate": 4.60467004890868e-05,
      "loss": 0.7233,
      "step": 519900
    },
    {
      "epoch": 4.744871888459012,
      "grad_norm": 4.218677043914795,
      "learning_rate": 4.604594009295082e-05,
      "loss": 0.7109,
      "step": 520000
    },
    {
      "epoch": 4.745784363822176,
      "grad_norm": 4.779792308807373,
      "learning_rate": 4.604517969681485e-05,
      "loss": 0.6749,
      "step": 520100
    },
    {
      "epoch": 4.746696839185342,
      "grad_norm": 4.365667343139648,
      "learning_rate": 4.604441930067888e-05,
      "loss": 0.7053,
      "step": 520200
    },
    {
      "epoch": 4.747609314548507,
      "grad_norm": 2.7953903675079346,
      "learning_rate": 4.604365890454291e-05,
      "loss": 0.7253,
      "step": 520300
    },
    {
      "epoch": 4.748521789911672,
      "grad_norm": 3.581406354904175,
      "learning_rate": 4.604289850840694e-05,
      "loss": 0.7137,
      "step": 520400
    },
    {
      "epoch": 4.749434265274838,
      "grad_norm": 3.4377615451812744,
      "learning_rate": 4.604213811227097e-05,
      "loss": 0.7212,
      "step": 520500
    },
    {
      "epoch": 4.750346740638003,
      "grad_norm": 4.449584007263184,
      "learning_rate": 4.6041377716134996e-05,
      "loss": 0.7134,
      "step": 520600
    },
    {
      "epoch": 4.751259216001168,
      "grad_norm": 4.2982330322265625,
      "learning_rate": 4.604061731999903e-05,
      "loss": 0.7562,
      "step": 520700
    },
    {
      "epoch": 4.7521716913643335,
      "grad_norm": 4.2383575439453125,
      "learning_rate": 4.6039856923863056e-05,
      "loss": 0.7176,
      "step": 520800
    },
    {
      "epoch": 4.753084166727499,
      "grad_norm": 3.938178062438965,
      "learning_rate": 4.6039096527727086e-05,
      "loss": 0.6703,
      "step": 520900
    },
    {
      "epoch": 4.753996642090663,
      "grad_norm": 3.7202796936035156,
      "learning_rate": 4.6038336131591116e-05,
      "loss": 0.671,
      "step": 521000
    },
    {
      "epoch": 4.7549091174538285,
      "grad_norm": 3.148979902267456,
      "learning_rate": 4.6037575735455146e-05,
      "loss": 0.6857,
      "step": 521100
    },
    {
      "epoch": 4.755821592816994,
      "grad_norm": 3.381483554840088,
      "learning_rate": 4.603681533931917e-05,
      "loss": 0.6938,
      "step": 521200
    },
    {
      "epoch": 4.756734068180159,
      "grad_norm": 3.7265350818634033,
      "learning_rate": 4.6036054943183207e-05,
      "loss": 0.7133,
      "step": 521300
    },
    {
      "epoch": 4.757646543543324,
      "grad_norm": 4.500307559967041,
      "learning_rate": 4.603529454704723e-05,
      "loss": 0.737,
      "step": 521400
    },
    {
      "epoch": 4.75855901890649,
      "grad_norm": 3.0049870014190674,
      "learning_rate": 4.603453415091126e-05,
      "loss": 0.6801,
      "step": 521500
    },
    {
      "epoch": 4.759471494269655,
      "grad_norm": 3.7211246490478516,
      "learning_rate": 4.603377375477529e-05,
      "loss": 0.6999,
      "step": 521600
    },
    {
      "epoch": 4.76038396963282,
      "grad_norm": 2.0614514350891113,
      "learning_rate": 4.603301335863932e-05,
      "loss": 0.7461,
      "step": 521700
    },
    {
      "epoch": 4.761296444995985,
      "grad_norm": 4.302977561950684,
      "learning_rate": 4.603225296250335e-05,
      "loss": 0.7109,
      "step": 521800
    },
    {
      "epoch": 4.76220892035915,
      "grad_norm": 3.4127132892608643,
      "learning_rate": 4.603149256636738e-05,
      "loss": 0.7287,
      "step": 521900
    },
    {
      "epoch": 4.763121395722315,
      "grad_norm": 3.4270429611206055,
      "learning_rate": 4.6030732170231404e-05,
      "loss": 0.7509,
      "step": 522000
    },
    {
      "epoch": 4.764033871085481,
      "grad_norm": 4.02840518951416,
      "learning_rate": 4.602997177409544e-05,
      "loss": 0.6946,
      "step": 522100
    },
    {
      "epoch": 4.764946346448646,
      "grad_norm": 3.6786136627197266,
      "learning_rate": 4.6029211377959464e-05,
      "loss": 0.6883,
      "step": 522200
    },
    {
      "epoch": 4.765858821811811,
      "grad_norm": 4.65305233001709,
      "learning_rate": 4.6028450981823494e-05,
      "loss": 0.6495,
      "step": 522300
    },
    {
      "epoch": 4.7667712971749765,
      "grad_norm": 3.7519497871398926,
      "learning_rate": 4.6027690585687524e-05,
      "loss": 0.7286,
      "step": 522400
    },
    {
      "epoch": 4.767683772538142,
      "grad_norm": 4.489643096923828,
      "learning_rate": 4.6026930189551554e-05,
      "loss": 0.707,
      "step": 522500
    },
    {
      "epoch": 4.768596247901307,
      "grad_norm": 3.2219741344451904,
      "learning_rate": 4.602616979341558e-05,
      "loss": 0.7457,
      "step": 522600
    },
    {
      "epoch": 4.7695087232644715,
      "grad_norm": 4.5459303855896,
      "learning_rate": 4.602540939727961e-05,
      "loss": 0.7205,
      "step": 522700
    },
    {
      "epoch": 4.770421198627637,
      "grad_norm": 4.0647664070129395,
      "learning_rate": 4.602464900114364e-05,
      "loss": 0.6938,
      "step": 522800
    },
    {
      "epoch": 4.771333673990802,
      "grad_norm": 4.736758232116699,
      "learning_rate": 4.602388860500767e-05,
      "loss": 0.7371,
      "step": 522900
    },
    {
      "epoch": 4.772246149353967,
      "grad_norm": 3.5160248279571533,
      "learning_rate": 4.60231282088717e-05,
      "loss": 0.723,
      "step": 523000
    },
    {
      "epoch": 4.773158624717133,
      "grad_norm": 4.288231372833252,
      "learning_rate": 4.602236781273572e-05,
      "loss": 0.7097,
      "step": 523100
    },
    {
      "epoch": 4.774071100080298,
      "grad_norm": 3.596801519393921,
      "learning_rate": 4.602160741659976e-05,
      "loss": 0.7495,
      "step": 523200
    },
    {
      "epoch": 4.774983575443463,
      "grad_norm": 4.165257930755615,
      "learning_rate": 4.602084702046378e-05,
      "loss": 0.7307,
      "step": 523300
    },
    {
      "epoch": 4.775896050806629,
      "grad_norm": 3.6591310501098633,
      "learning_rate": 4.602008662432781e-05,
      "loss": 0.7069,
      "step": 523400
    },
    {
      "epoch": 4.776808526169793,
      "grad_norm": 4.137032508850098,
      "learning_rate": 4.601932622819184e-05,
      "loss": 0.6992,
      "step": 523500
    },
    {
      "epoch": 4.777721001532958,
      "grad_norm": 3.8496580123901367,
      "learning_rate": 4.601856583205587e-05,
      "loss": 0.6902,
      "step": 523600
    },
    {
      "epoch": 4.778633476896124,
      "grad_norm": 3.489091634750366,
      "learning_rate": 4.6017805435919894e-05,
      "loss": 0.6975,
      "step": 523700
    },
    {
      "epoch": 4.779545952259289,
      "grad_norm": 3.9531290531158447,
      "learning_rate": 4.601704503978393e-05,
      "loss": 0.7212,
      "step": 523800
    },
    {
      "epoch": 4.780458427622454,
      "grad_norm": 4.708625316619873,
      "learning_rate": 4.6016284643647954e-05,
      "loss": 0.7285,
      "step": 523900
    },
    {
      "epoch": 4.7813709029856195,
      "grad_norm": 3.3054592609405518,
      "learning_rate": 4.6015524247511985e-05,
      "loss": 0.7192,
      "step": 524000
    },
    {
      "epoch": 4.782283378348785,
      "grad_norm": 3.46624493598938,
      "learning_rate": 4.6014763851376015e-05,
      "loss": 0.6763,
      "step": 524100
    },
    {
      "epoch": 4.78319585371195,
      "grad_norm": 3.119441509246826,
      "learning_rate": 4.6014003455240045e-05,
      "loss": 0.7018,
      "step": 524200
    },
    {
      "epoch": 4.784108329075115,
      "grad_norm": 5.171349048614502,
      "learning_rate": 4.6013243059104075e-05,
      "loss": 0.7092,
      "step": 524300
    },
    {
      "epoch": 4.78502080443828,
      "grad_norm": 3.689997911453247,
      "learning_rate": 4.6012482662968105e-05,
      "loss": 0.7057,
      "step": 524400
    },
    {
      "epoch": 4.785933279801445,
      "grad_norm": 4.658236026763916,
      "learning_rate": 4.601172226683213e-05,
      "loss": 0.7179,
      "step": 524500
    },
    {
      "epoch": 4.78684575516461,
      "grad_norm": 4.082479000091553,
      "learning_rate": 4.6010961870696165e-05,
      "loss": 0.7523,
      "step": 524600
    },
    {
      "epoch": 4.787758230527776,
      "grad_norm": 3.4890713691711426,
      "learning_rate": 4.601020147456019e-05,
      "loss": 0.6851,
      "step": 524700
    },
    {
      "epoch": 4.788670705890941,
      "grad_norm": 3.8034520149230957,
      "learning_rate": 4.600944107842422e-05,
      "loss": 0.7176,
      "step": 524800
    },
    {
      "epoch": 4.789583181254106,
      "grad_norm": 4.069993019104004,
      "learning_rate": 4.600868068228825e-05,
      "loss": 0.7291,
      "step": 524900
    },
    {
      "epoch": 4.790495656617272,
      "grad_norm": 4.94764518737793,
      "learning_rate": 4.600792028615228e-05,
      "loss": 0.6935,
      "step": 525000
    },
    {
      "epoch": 4.791408131980437,
      "grad_norm": 4.567398548126221,
      "learning_rate": 4.60071598900163e-05,
      "loss": 0.7472,
      "step": 525100
    },
    {
      "epoch": 4.792320607343601,
      "grad_norm": 3.422315835952759,
      "learning_rate": 4.600639949388034e-05,
      "loss": 0.6862,
      "step": 525200
    },
    {
      "epoch": 4.793233082706767,
      "grad_norm": 4.032036781311035,
      "learning_rate": 4.600563909774436e-05,
      "loss": 0.6878,
      "step": 525300
    },
    {
      "epoch": 4.794145558069932,
      "grad_norm": 4.1269636154174805,
      "learning_rate": 4.600487870160839e-05,
      "loss": 0.7144,
      "step": 525400
    },
    {
      "epoch": 4.795058033433097,
      "grad_norm": 4.343192100524902,
      "learning_rate": 4.600411830547242e-05,
      "loss": 0.7081,
      "step": 525500
    },
    {
      "epoch": 4.7959705087962625,
      "grad_norm": 3.7882447242736816,
      "learning_rate": 4.6003357909336445e-05,
      "loss": 0.7248,
      "step": 525600
    },
    {
      "epoch": 4.796882984159428,
      "grad_norm": 5.457171440124512,
      "learning_rate": 4.600259751320048e-05,
      "loss": 0.7054,
      "step": 525700
    },
    {
      "epoch": 4.797795459522593,
      "grad_norm": 4.135532855987549,
      "learning_rate": 4.6001837117064505e-05,
      "loss": 0.7127,
      "step": 525800
    },
    {
      "epoch": 4.798707934885758,
      "grad_norm": 3.27457332611084,
      "learning_rate": 4.6001076720928535e-05,
      "loss": 0.71,
      "step": 525900
    },
    {
      "epoch": 4.799620410248924,
      "grad_norm": 4.57355260848999,
      "learning_rate": 4.6000316324792566e-05,
      "loss": 0.7143,
      "step": 526000
    },
    {
      "epoch": 4.800532885612088,
      "grad_norm": 4.4704909324646,
      "learning_rate": 4.5999555928656596e-05,
      "loss": 0.7429,
      "step": 526100
    },
    {
      "epoch": 4.801445360975253,
      "grad_norm": 4.513559341430664,
      "learning_rate": 4.599879553252062e-05,
      "loss": 0.7152,
      "step": 526200
    },
    {
      "epoch": 4.802357836338419,
      "grad_norm": 1.7162790298461914,
      "learning_rate": 4.5998035136384656e-05,
      "loss": 0.6958,
      "step": 526300
    },
    {
      "epoch": 4.803270311701584,
      "grad_norm": 4.286496639251709,
      "learning_rate": 4.599727474024868e-05,
      "loss": 0.7057,
      "step": 526400
    },
    {
      "epoch": 4.804182787064749,
      "grad_norm": 4.7258782386779785,
      "learning_rate": 4.599651434411271e-05,
      "loss": 0.743,
      "step": 526500
    },
    {
      "epoch": 4.805095262427915,
      "grad_norm": 4.6799774169921875,
      "learning_rate": 4.599575394797674e-05,
      "loss": 0.733,
      "step": 526600
    },
    {
      "epoch": 4.80600773779108,
      "grad_norm": 4.824455738067627,
      "learning_rate": 4.599499355184077e-05,
      "loss": 0.7281,
      "step": 526700
    },
    {
      "epoch": 4.806920213154244,
      "grad_norm": 4.287386417388916,
      "learning_rate": 4.59942331557048e-05,
      "loss": 0.7044,
      "step": 526800
    },
    {
      "epoch": 4.80783268851741,
      "grad_norm": 4.122208595275879,
      "learning_rate": 4.599347275956883e-05,
      "loss": 0.693,
      "step": 526900
    },
    {
      "epoch": 4.808745163880575,
      "grad_norm": 3.201770544052124,
      "learning_rate": 4.599271236343285e-05,
      "loss": 0.7076,
      "step": 527000
    },
    {
      "epoch": 4.80965763924374,
      "grad_norm": 3.705120801925659,
      "learning_rate": 4.599195196729689e-05,
      "loss": 0.6865,
      "step": 527100
    },
    {
      "epoch": 4.8105701146069055,
      "grad_norm": 4.787154197692871,
      "learning_rate": 4.599119157116091e-05,
      "loss": 0.7007,
      "step": 527200
    },
    {
      "epoch": 4.811482589970071,
      "grad_norm": 3.4957528114318848,
      "learning_rate": 4.599043117502494e-05,
      "loss": 0.6975,
      "step": 527300
    },
    {
      "epoch": 4.812395065333236,
      "grad_norm": 3.833997964859009,
      "learning_rate": 4.598967077888897e-05,
      "loss": 0.6893,
      "step": 527400
    },
    {
      "epoch": 4.813307540696401,
      "grad_norm": 3.173109531402588,
      "learning_rate": 4.5988910382753e-05,
      "loss": 0.7018,
      "step": 527500
    },
    {
      "epoch": 4.814220016059567,
      "grad_norm": 4.073892593383789,
      "learning_rate": 4.5988149986617026e-05,
      "loss": 0.7068,
      "step": 527600
    },
    {
      "epoch": 4.815132491422732,
      "grad_norm": 4.855428695678711,
      "learning_rate": 4.598738959048106e-05,
      "loss": 0.7291,
      "step": 527700
    },
    {
      "epoch": 4.816044966785896,
      "grad_norm": 3.723571300506592,
      "learning_rate": 4.5986629194345086e-05,
      "loss": 0.7155,
      "step": 527800
    },
    {
      "epoch": 4.816957442149062,
      "grad_norm": 3.8469536304473877,
      "learning_rate": 4.5985868798209116e-05,
      "loss": 0.7259,
      "step": 527900
    },
    {
      "epoch": 4.817869917512227,
      "grad_norm": 3.2889466285705566,
      "learning_rate": 4.5985108402073147e-05,
      "loss": 0.7194,
      "step": 528000
    },
    {
      "epoch": 4.818782392875392,
      "grad_norm": 3.7276463508605957,
      "learning_rate": 4.598434800593718e-05,
      "loss": 0.7082,
      "step": 528100
    },
    {
      "epoch": 4.819694868238558,
      "grad_norm": 4.405858516693115,
      "learning_rate": 4.598358760980121e-05,
      "loss": 0.721,
      "step": 528200
    },
    {
      "epoch": 4.820607343601723,
      "grad_norm": 4.273068904876709,
      "learning_rate": 4.598282721366523e-05,
      "loss": 0.7164,
      "step": 528300
    },
    {
      "epoch": 4.821519818964888,
      "grad_norm": 4.016889572143555,
      "learning_rate": 4.598206681752926e-05,
      "loss": 0.7527,
      "step": 528400
    },
    {
      "epoch": 4.822432294328053,
      "grad_norm": 3.5589723587036133,
      "learning_rate": 4.598130642139329e-05,
      "loss": 0.7276,
      "step": 528500
    },
    {
      "epoch": 4.823344769691218,
      "grad_norm": 3.910355567932129,
      "learning_rate": 4.598054602525732e-05,
      "loss": 0.6851,
      "step": 528600
    },
    {
      "epoch": 4.824257245054383,
      "grad_norm": 4.316831111907959,
      "learning_rate": 4.597978562912135e-05,
      "loss": 0.7219,
      "step": 528700
    },
    {
      "epoch": 4.8251697204175485,
      "grad_norm": 3.073051691055298,
      "learning_rate": 4.597902523298538e-05,
      "loss": 0.678,
      "step": 528800
    },
    {
      "epoch": 4.826082195780714,
      "grad_norm": 3.682408094406128,
      "learning_rate": 4.5978264836849404e-05,
      "loss": 0.7217,
      "step": 528900
    },
    {
      "epoch": 4.826994671143879,
      "grad_norm": 3.8793325424194336,
      "learning_rate": 4.5977504440713434e-05,
      "loss": 0.7421,
      "step": 529000
    },
    {
      "epoch": 4.8279071465070444,
      "grad_norm": 3.9892354011535645,
      "learning_rate": 4.5976744044577464e-05,
      "loss": 0.7218,
      "step": 529100
    },
    {
      "epoch": 4.82881962187021,
      "grad_norm": 3.4949285984039307,
      "learning_rate": 4.5975983648441494e-05,
      "loss": 0.6622,
      "step": 529200
    },
    {
      "epoch": 4.829732097233375,
      "grad_norm": 3.9438109397888184,
      "learning_rate": 4.5975223252305524e-05,
      "loss": 0.7501,
      "step": 529300
    },
    {
      "epoch": 4.83064457259654,
      "grad_norm": 4.6081085205078125,
      "learning_rate": 4.5974462856169554e-05,
      "loss": 0.7479,
      "step": 529400
    },
    {
      "epoch": 4.831557047959705,
      "grad_norm": 3.37274169921875,
      "learning_rate": 4.597370246003358e-05,
      "loss": 0.6876,
      "step": 529500
    },
    {
      "epoch": 4.83246952332287,
      "grad_norm": 4.267991065979004,
      "learning_rate": 4.5972942063897614e-05,
      "loss": 0.7209,
      "step": 529600
    },
    {
      "epoch": 4.833381998686035,
      "grad_norm": 4.753071308135986,
      "learning_rate": 4.597218166776164e-05,
      "loss": 0.7291,
      "step": 529700
    },
    {
      "epoch": 4.834294474049201,
      "grad_norm": 4.118625164031982,
      "learning_rate": 4.597142127162567e-05,
      "loss": 0.7172,
      "step": 529800
    },
    {
      "epoch": 4.835206949412366,
      "grad_norm": 4.65009880065918,
      "learning_rate": 4.59706608754897e-05,
      "loss": 0.7377,
      "step": 529900
    },
    {
      "epoch": 4.836119424775531,
      "grad_norm": 3.6661171913146973,
      "learning_rate": 4.596990047935373e-05,
      "loss": 0.698,
      "step": 530000
    },
    {
      "epoch": 4.8370319001386965,
      "grad_norm": 4.0015740394592285,
      "learning_rate": 4.596914008321776e-05,
      "loss": 0.6993,
      "step": 530100
    },
    {
      "epoch": 4.837944375501861,
      "grad_norm": 4.4947733879089355,
      "learning_rate": 4.596837968708179e-05,
      "loss": 0.7362,
      "step": 530200
    },
    {
      "epoch": 4.838856850865026,
      "grad_norm": 4.794332504272461,
      "learning_rate": 4.596761929094581e-05,
      "loss": 0.7012,
      "step": 530300
    },
    {
      "epoch": 4.839769326228192,
      "grad_norm": 4.419819355010986,
      "learning_rate": 4.596685889480984e-05,
      "loss": 0.7093,
      "step": 530400
    },
    {
      "epoch": 4.840681801591357,
      "grad_norm": 4.60386848449707,
      "learning_rate": 4.596609849867387e-05,
      "loss": 0.7284,
      "step": 530500
    },
    {
      "epoch": 4.841594276954522,
      "grad_norm": 4.615325927734375,
      "learning_rate": 4.59653381025379e-05,
      "loss": 0.7075,
      "step": 530600
    },
    {
      "epoch": 4.8425067523176875,
      "grad_norm": 4.482704162597656,
      "learning_rate": 4.596457770640193e-05,
      "loss": 0.7504,
      "step": 530700
    },
    {
      "epoch": 4.843419227680853,
      "grad_norm": 6.0409369468688965,
      "learning_rate": 4.596381731026596e-05,
      "loss": 0.7258,
      "step": 530800
    },
    {
      "epoch": 4.844331703044018,
      "grad_norm": 4.287993431091309,
      "learning_rate": 4.5963056914129985e-05,
      "loss": 0.726,
      "step": 530900
    },
    {
      "epoch": 4.845244178407183,
      "grad_norm": 4.14185905456543,
      "learning_rate": 4.596229651799402e-05,
      "loss": 0.751,
      "step": 531000
    },
    {
      "epoch": 4.846156653770349,
      "grad_norm": 4.572870254516602,
      "learning_rate": 4.5961536121858045e-05,
      "loss": 0.7763,
      "step": 531100
    },
    {
      "epoch": 4.847069129133513,
      "grad_norm": 3.488861083984375,
      "learning_rate": 4.5960775725722075e-05,
      "loss": 0.7299,
      "step": 531200
    },
    {
      "epoch": 4.847981604496678,
      "grad_norm": 3.9392614364624023,
      "learning_rate": 4.5960015329586105e-05,
      "loss": 0.7366,
      "step": 531300
    },
    {
      "epoch": 4.848894079859844,
      "grad_norm": 4.396456718444824,
      "learning_rate": 4.595925493345013e-05,
      "loss": 0.7106,
      "step": 531400
    },
    {
      "epoch": 4.849806555223009,
      "grad_norm": 4.306144714355469,
      "learning_rate": 4.5958494537314165e-05,
      "loss": 0.6874,
      "step": 531500
    },
    {
      "epoch": 4.850719030586174,
      "grad_norm": 3.3847062587738037,
      "learning_rate": 4.595773414117819e-05,
      "loss": 0.7313,
      "step": 531600
    },
    {
      "epoch": 4.85163150594934,
      "grad_norm": 3.2966818809509277,
      "learning_rate": 4.595697374504222e-05,
      "loss": 0.7229,
      "step": 531700
    },
    {
      "epoch": 4.852543981312505,
      "grad_norm": 4.36367130279541,
      "learning_rate": 4.595621334890625e-05,
      "loss": 0.7322,
      "step": 531800
    },
    {
      "epoch": 4.853456456675669,
      "grad_norm": 4.262944221496582,
      "learning_rate": 4.595545295277028e-05,
      "loss": 0.7131,
      "step": 531900
    },
    {
      "epoch": 4.854368932038835,
      "grad_norm": 4.386026382446289,
      "learning_rate": 4.59546925566343e-05,
      "loss": 0.7003,
      "step": 532000
    },
    {
      "epoch": 4.855281407402,
      "grad_norm": 3.794064521789551,
      "learning_rate": 4.595393216049834e-05,
      "loss": 0.7176,
      "step": 532100
    },
    {
      "epoch": 4.856193882765165,
      "grad_norm": 4.578563690185547,
      "learning_rate": 4.595317176436236e-05,
      "loss": 0.7208,
      "step": 532200
    },
    {
      "epoch": 4.8571063581283305,
      "grad_norm": 4.811943531036377,
      "learning_rate": 4.595241136822639e-05,
      "loss": 0.7616,
      "step": 532300
    },
    {
      "epoch": 4.858018833491496,
      "grad_norm": 4.440401077270508,
      "learning_rate": 4.595165097209042e-05,
      "loss": 0.7162,
      "step": 532400
    },
    {
      "epoch": 4.858931308854661,
      "grad_norm": 3.5461883544921875,
      "learning_rate": 4.595089057595445e-05,
      "loss": 0.74,
      "step": 532500
    },
    {
      "epoch": 4.859843784217826,
      "grad_norm": 3.9919140338897705,
      "learning_rate": 4.595013017981848e-05,
      "loss": 0.721,
      "step": 532600
    },
    {
      "epoch": 4.860756259580992,
      "grad_norm": 4.139779567718506,
      "learning_rate": 4.594936978368251e-05,
      "loss": 0.7288,
      "step": 532700
    },
    {
      "epoch": 4.861668734944157,
      "grad_norm": 4.128092288970947,
      "learning_rate": 4.5948609387546536e-05,
      "loss": 0.7186,
      "step": 532800
    },
    {
      "epoch": 4.862581210307321,
      "grad_norm": 3.155564546585083,
      "learning_rate": 4.594784899141057e-05,
      "loss": 0.7388,
      "step": 532900
    },
    {
      "epoch": 4.863493685670487,
      "grad_norm": 3.980477809906006,
      "learning_rate": 4.5947088595274596e-05,
      "loss": 0.7072,
      "step": 533000
    },
    {
      "epoch": 4.864406161033652,
      "grad_norm": 4.204802989959717,
      "learning_rate": 4.5946328199138626e-05,
      "loss": 0.7395,
      "step": 533100
    },
    {
      "epoch": 4.865318636396817,
      "grad_norm": 4.090383529663086,
      "learning_rate": 4.5945567803002656e-05,
      "loss": 0.7246,
      "step": 533200
    },
    {
      "epoch": 4.866231111759983,
      "grad_norm": 4.143317699432373,
      "learning_rate": 4.5944807406866686e-05,
      "loss": 0.7495,
      "step": 533300
    },
    {
      "epoch": 4.867143587123148,
      "grad_norm": 4.298027038574219,
      "learning_rate": 4.594404701073071e-05,
      "loss": 0.7187,
      "step": 533400
    },
    {
      "epoch": 4.868056062486313,
      "grad_norm": 3.616858959197998,
      "learning_rate": 4.5943286614594746e-05,
      "loss": 0.6895,
      "step": 533500
    },
    {
      "epoch": 4.868968537849478,
      "grad_norm": 4.029165744781494,
      "learning_rate": 4.594252621845877e-05,
      "loss": 0.7398,
      "step": 533600
    },
    {
      "epoch": 4.869881013212643,
      "grad_norm": 4.6915154457092285,
      "learning_rate": 4.59417658223228e-05,
      "loss": 0.7181,
      "step": 533700
    },
    {
      "epoch": 4.870793488575808,
      "grad_norm": 1.914466381072998,
      "learning_rate": 4.594100542618683e-05,
      "loss": 0.7016,
      "step": 533800
    },
    {
      "epoch": 4.8717059639389735,
      "grad_norm": 4.805741786956787,
      "learning_rate": 4.594024503005086e-05,
      "loss": 0.743,
      "step": 533900
    },
    {
      "epoch": 4.872618439302139,
      "grad_norm": 4.7752227783203125,
      "learning_rate": 4.593948463391489e-05,
      "loss": 0.7285,
      "step": 534000
    },
    {
      "epoch": 4.873530914665304,
      "grad_norm": 4.082581043243408,
      "learning_rate": 4.593872423777891e-05,
      "loss": 0.7373,
      "step": 534100
    },
    {
      "epoch": 4.874443390028469,
      "grad_norm": 3.8994462490081787,
      "learning_rate": 4.593796384164294e-05,
      "loss": 0.7085,
      "step": 534200
    },
    {
      "epoch": 4.875355865391635,
      "grad_norm": 3.455911159515381,
      "learning_rate": 4.593720344550697e-05,
      "loss": 0.7399,
      "step": 534300
    },
    {
      "epoch": 4.8762683407548,
      "grad_norm": 3.964561939239502,
      "learning_rate": 4.5936443049371e-05,
      "loss": 0.7106,
      "step": 534400
    },
    {
      "epoch": 4.877180816117965,
      "grad_norm": 3.9203968048095703,
      "learning_rate": 4.5935682653235026e-05,
      "loss": 0.7299,
      "step": 534500
    },
    {
      "epoch": 4.87809329148113,
      "grad_norm": 4.484302520751953,
      "learning_rate": 4.593492225709906e-05,
      "loss": 0.7413,
      "step": 534600
    },
    {
      "epoch": 4.879005766844295,
      "grad_norm": 4.071152687072754,
      "learning_rate": 4.5934161860963087e-05,
      "loss": 0.7105,
      "step": 534700
    },
    {
      "epoch": 4.87991824220746,
      "grad_norm": 5.0931501388549805,
      "learning_rate": 4.5933401464827117e-05,
      "loss": 0.6938,
      "step": 534800
    },
    {
      "epoch": 4.880830717570626,
      "grad_norm": 4.160575866699219,
      "learning_rate": 4.593264106869115e-05,
      "loss": 0.7294,
      "step": 534900
    },
    {
      "epoch": 4.881743192933791,
      "grad_norm": 5.801011562347412,
      "learning_rate": 4.593188067255518e-05,
      "loss": 0.7248,
      "step": 535000
    },
    {
      "epoch": 4.882655668296956,
      "grad_norm": 4.182128429412842,
      "learning_rate": 4.593112027641921e-05,
      "loss": 0.7326,
      "step": 535100
    },
    {
      "epoch": 4.8835681436601215,
      "grad_norm": 3.5813374519348145,
      "learning_rate": 4.593035988028324e-05,
      "loss": 0.7179,
      "step": 535200
    },
    {
      "epoch": 4.884480619023286,
      "grad_norm": 4.627518177032471,
      "learning_rate": 4.592959948414726e-05,
      "loss": 0.678,
      "step": 535300
    },
    {
      "epoch": 4.885393094386451,
      "grad_norm": 4.404438495635986,
      "learning_rate": 4.59288390880113e-05,
      "loss": 0.7079,
      "step": 535400
    },
    {
      "epoch": 4.8863055697496165,
      "grad_norm": 3.754307508468628,
      "learning_rate": 4.592807869187532e-05,
      "loss": 0.6938,
      "step": 535500
    },
    {
      "epoch": 4.887218045112782,
      "grad_norm": 6.979640960693359,
      "learning_rate": 4.592731829573935e-05,
      "loss": 0.7242,
      "step": 535600
    },
    {
      "epoch": 4.888130520475947,
      "grad_norm": 4.580474853515625,
      "learning_rate": 4.592655789960338e-05,
      "loss": 0.7459,
      "step": 535700
    },
    {
      "epoch": 4.889042995839112,
      "grad_norm": 3.8495991230010986,
      "learning_rate": 4.592579750346741e-05,
      "loss": 0.7054,
      "step": 535800
    },
    {
      "epoch": 4.889955471202278,
      "grad_norm": 3.935213088989258,
      "learning_rate": 4.5925037107331434e-05,
      "loss": 0.7474,
      "step": 535900
    },
    {
      "epoch": 4.890867946565443,
      "grad_norm": 3.708354949951172,
      "learning_rate": 4.592427671119547e-05,
      "loss": 0.7308,
      "step": 536000
    },
    {
      "epoch": 4.891780421928608,
      "grad_norm": 3.7595181465148926,
      "learning_rate": 4.5923516315059494e-05,
      "loss": 0.693,
      "step": 536100
    },
    {
      "epoch": 4.892692897291774,
      "grad_norm": 3.7971065044403076,
      "learning_rate": 4.5922755918923524e-05,
      "loss": 0.6888,
      "step": 536200
    },
    {
      "epoch": 4.893605372654938,
      "grad_norm": 4.203096866607666,
      "learning_rate": 4.5921995522787554e-05,
      "loss": 0.6914,
      "step": 536300
    },
    {
      "epoch": 4.894517848018103,
      "grad_norm": 4.995347499847412,
      "learning_rate": 4.5921235126651584e-05,
      "loss": 0.7196,
      "step": 536400
    },
    {
      "epoch": 4.895430323381269,
      "grad_norm": 3.407191753387451,
      "learning_rate": 4.5920474730515614e-05,
      "loss": 0.7225,
      "step": 536500
    },
    {
      "epoch": 4.896342798744434,
      "grad_norm": 5.3021464347839355,
      "learning_rate": 4.5919714334379644e-05,
      "loss": 0.723,
      "step": 536600
    },
    {
      "epoch": 4.897255274107599,
      "grad_norm": 4.152379989624023,
      "learning_rate": 4.591895393824367e-05,
      "loss": 0.6651,
      "step": 536700
    },
    {
      "epoch": 4.8981677494707645,
      "grad_norm": 4.444252967834473,
      "learning_rate": 4.59181935421077e-05,
      "loss": 0.6926,
      "step": 536800
    },
    {
      "epoch": 4.89908022483393,
      "grad_norm": 4.331015586853027,
      "learning_rate": 4.591743314597173e-05,
      "loss": 0.7585,
      "step": 536900
    },
    {
      "epoch": 4.899992700197094,
      "grad_norm": 4.445972919464111,
      "learning_rate": 4.591667274983575e-05,
      "loss": 0.7268,
      "step": 537000
    },
    {
      "epoch": 4.9009051755602595,
      "grad_norm": 3.3963799476623535,
      "learning_rate": 4.591591235369979e-05,
      "loss": 0.7292,
      "step": 537100
    },
    {
      "epoch": 4.901817650923425,
      "grad_norm": 3.0502820014953613,
      "learning_rate": 4.591515195756381e-05,
      "loss": 0.727,
      "step": 537200
    },
    {
      "epoch": 4.90273012628659,
      "grad_norm": 3.728485584259033,
      "learning_rate": 4.591439156142784e-05,
      "loss": 0.7172,
      "step": 537300
    },
    {
      "epoch": 4.903642601649755,
      "grad_norm": 4.186319351196289,
      "learning_rate": 4.591363116529187e-05,
      "loss": 0.7032,
      "step": 537400
    },
    {
      "epoch": 4.904555077012921,
      "grad_norm": 4.076717376708984,
      "learning_rate": 4.59128707691559e-05,
      "loss": 0.7469,
      "step": 537500
    },
    {
      "epoch": 4.905467552376086,
      "grad_norm": 2.560633659362793,
      "learning_rate": 4.591211037301993e-05,
      "loss": 0.7729,
      "step": 537600
    },
    {
      "epoch": 4.906380027739251,
      "grad_norm": 4.1645283699035645,
      "learning_rate": 4.591134997688396e-05,
      "loss": 0.7035,
      "step": 537700
    },
    {
      "epoch": 4.907292503102417,
      "grad_norm": 3.9171321392059326,
      "learning_rate": 4.5910589580747985e-05,
      "loss": 0.7403,
      "step": 537800
    },
    {
      "epoch": 4.908204978465582,
      "grad_norm": 4.518233776092529,
      "learning_rate": 4.590982918461202e-05,
      "loss": 0.7137,
      "step": 537900
    },
    {
      "epoch": 4.909117453828746,
      "grad_norm": 3.9534239768981934,
      "learning_rate": 4.5909068788476045e-05,
      "loss": 0.665,
      "step": 538000
    },
    {
      "epoch": 4.910029929191912,
      "grad_norm": 3.909273862838745,
      "learning_rate": 4.5908308392340075e-05,
      "loss": 0.7462,
      "step": 538100
    },
    {
      "epoch": 4.910942404555077,
      "grad_norm": 4.4456467628479,
      "learning_rate": 4.5907547996204105e-05,
      "loss": 0.7198,
      "step": 538200
    },
    {
      "epoch": 4.911854879918242,
      "grad_norm": 3.0826072692871094,
      "learning_rate": 4.5906787600068135e-05,
      "loss": 0.7757,
      "step": 538300
    },
    {
      "epoch": 4.9127673552814075,
      "grad_norm": 4.181487083435059,
      "learning_rate": 4.590602720393216e-05,
      "loss": 0.7356,
      "step": 538400
    },
    {
      "epoch": 4.913679830644573,
      "grad_norm": 4.781076431274414,
      "learning_rate": 4.5905266807796195e-05,
      "loss": 0.7318,
      "step": 538500
    },
    {
      "epoch": 4.914592306007738,
      "grad_norm": 4.244231224060059,
      "learning_rate": 4.590450641166022e-05,
      "loss": 0.6916,
      "step": 538600
    },
    {
      "epoch": 4.9155047813709025,
      "grad_norm": 4.361503601074219,
      "learning_rate": 4.590374601552425e-05,
      "loss": 0.7291,
      "step": 538700
    },
    {
      "epoch": 4.916417256734068,
      "grad_norm": 5.048709392547607,
      "learning_rate": 4.590298561938828e-05,
      "loss": 0.7267,
      "step": 538800
    },
    {
      "epoch": 4.917329732097233,
      "grad_norm": 1.8603758811950684,
      "learning_rate": 4.590222522325231e-05,
      "loss": 0.7182,
      "step": 538900
    },
    {
      "epoch": 4.918242207460398,
      "grad_norm": 3.6467034816741943,
      "learning_rate": 4.590146482711634e-05,
      "loss": 0.7313,
      "step": 539000
    },
    {
      "epoch": 4.919154682823564,
      "grad_norm": 4.235930919647217,
      "learning_rate": 4.590070443098037e-05,
      "loss": 0.742,
      "step": 539100
    },
    {
      "epoch": 4.920067158186729,
      "grad_norm": 2.6662628650665283,
      "learning_rate": 4.589994403484439e-05,
      "loss": 0.696,
      "step": 539200
    },
    {
      "epoch": 4.920979633549894,
      "grad_norm": 4.4404826164245605,
      "learning_rate": 4.589918363870843e-05,
      "loss": 0.6957,
      "step": 539300
    },
    {
      "epoch": 4.92189210891306,
      "grad_norm": 4.400241374969482,
      "learning_rate": 4.589842324257245e-05,
      "loss": 0.7167,
      "step": 539400
    },
    {
      "epoch": 4.922804584276225,
      "grad_norm": 3.9486451148986816,
      "learning_rate": 4.589766284643648e-05,
      "loss": 0.742,
      "step": 539500
    },
    {
      "epoch": 4.92371705963939,
      "grad_norm": 4.391982078552246,
      "learning_rate": 4.589690245030051e-05,
      "loss": 0.7313,
      "step": 539600
    },
    {
      "epoch": 4.924629535002555,
      "grad_norm": 4.2817487716674805,
      "learning_rate": 4.5896142054164536e-05,
      "loss": 0.6893,
      "step": 539700
    },
    {
      "epoch": 4.92554201036572,
      "grad_norm": 3.7041852474212646,
      "learning_rate": 4.5895381658028566e-05,
      "loss": 0.7411,
      "step": 539800
    },
    {
      "epoch": 4.926454485728885,
      "grad_norm": 4.448864936828613,
      "learning_rate": 4.5894621261892596e-05,
      "loss": 0.7164,
      "step": 539900
    },
    {
      "epoch": 4.9273669610920505,
      "grad_norm": 3.6311867237091064,
      "learning_rate": 4.5893860865756626e-05,
      "loss": 0.7052,
      "step": 540000
    },
    {
      "epoch": 4.928279436455216,
      "grad_norm": 5.067763805389404,
      "learning_rate": 4.5893100469620656e-05,
      "loss": 0.7578,
      "step": 540100
    },
    {
      "epoch": 4.929191911818381,
      "grad_norm": 5.172374725341797,
      "learning_rate": 4.5892340073484686e-05,
      "loss": 0.7019,
      "step": 540200
    },
    {
      "epoch": 4.930104387181546,
      "grad_norm": 3.4215524196624756,
      "learning_rate": 4.589157967734871e-05,
      "loss": 0.7377,
      "step": 540300
    },
    {
      "epoch": 4.931016862544711,
      "grad_norm": 4.210084915161133,
      "learning_rate": 4.5890819281212746e-05,
      "loss": 0.7137,
      "step": 540400
    },
    {
      "epoch": 4.931929337907876,
      "grad_norm": 4.275950908660889,
      "learning_rate": 4.589005888507677e-05,
      "loss": 0.7198,
      "step": 540500
    },
    {
      "epoch": 4.932841813271041,
      "grad_norm": 4.376723766326904,
      "learning_rate": 4.58892984889408e-05,
      "loss": 0.6806,
      "step": 540600
    },
    {
      "epoch": 4.933754288634207,
      "grad_norm": 3.799349784851074,
      "learning_rate": 4.588853809280483e-05,
      "loss": 0.721,
      "step": 540700
    },
    {
      "epoch": 4.934666763997372,
      "grad_norm": 3.0648348331451416,
      "learning_rate": 4.588777769666886e-05,
      "loss": 0.7067,
      "step": 540800
    },
    {
      "epoch": 4.935579239360537,
      "grad_norm": 5.26121187210083,
      "learning_rate": 4.588701730053288e-05,
      "loss": 0.7357,
      "step": 540900
    },
    {
      "epoch": 4.936491714723703,
      "grad_norm": 2.6128628253936768,
      "learning_rate": 4.588625690439692e-05,
      "loss": 0.7437,
      "step": 541000
    },
    {
      "epoch": 4.937404190086868,
      "grad_norm": 3.6473050117492676,
      "learning_rate": 4.588549650826094e-05,
      "loss": 0.7262,
      "step": 541100
    },
    {
      "epoch": 4.938316665450033,
      "grad_norm": 4.309617042541504,
      "learning_rate": 4.588473611212497e-05,
      "loss": 0.7097,
      "step": 541200
    },
    {
      "epoch": 4.9392291408131985,
      "grad_norm": 4.148702144622803,
      "learning_rate": 4.5883975715989e-05,
      "loss": 0.7373,
      "step": 541300
    },
    {
      "epoch": 4.940141616176363,
      "grad_norm": 4.560077667236328,
      "learning_rate": 4.588321531985303e-05,
      "loss": 0.7115,
      "step": 541400
    },
    {
      "epoch": 4.941054091539528,
      "grad_norm": 4.320647239685059,
      "learning_rate": 4.588245492371706e-05,
      "loss": 0.7651,
      "step": 541500
    },
    {
      "epoch": 4.9419665669026935,
      "grad_norm": 4.890129089355469,
      "learning_rate": 4.5881694527581093e-05,
      "loss": 0.7146,
      "step": 541600
    },
    {
      "epoch": 4.942879042265859,
      "grad_norm": 4.254673957824707,
      "learning_rate": 4.588093413144512e-05,
      "loss": 0.6754,
      "step": 541700
    },
    {
      "epoch": 4.943791517629024,
      "grad_norm": 3.546271800994873,
      "learning_rate": 4.5880173735309154e-05,
      "loss": 0.6941,
      "step": 541800
    },
    {
      "epoch": 4.944703992992189,
      "grad_norm": 4.2404022216796875,
      "learning_rate": 4.587941333917318e-05,
      "loss": 0.7379,
      "step": 541900
    },
    {
      "epoch": 4.945616468355355,
      "grad_norm": 3.5319223403930664,
      "learning_rate": 4.587865294303721e-05,
      "loss": 0.7136,
      "step": 542000
    },
    {
      "epoch": 4.946528943718519,
      "grad_norm": 4.262216567993164,
      "learning_rate": 4.587789254690124e-05,
      "loss": 0.7093,
      "step": 542100
    },
    {
      "epoch": 4.947441419081684,
      "grad_norm": 4.694833755493164,
      "learning_rate": 4.587713215076527e-05,
      "loss": 0.6986,
      "step": 542200
    },
    {
      "epoch": 4.94835389444485,
      "grad_norm": 3.5462512969970703,
      "learning_rate": 4.58763717546293e-05,
      "loss": 0.6939,
      "step": 542300
    },
    {
      "epoch": 4.949266369808015,
      "grad_norm": 4.158809661865234,
      "learning_rate": 4.587561135849333e-05,
      "loss": 0.7116,
      "step": 542400
    },
    {
      "epoch": 4.95017884517118,
      "grad_norm": 4.837622165679932,
      "learning_rate": 4.587485096235735e-05,
      "loss": 0.7123,
      "step": 542500
    },
    {
      "epoch": 4.951091320534346,
      "grad_norm": 4.837621688842773,
      "learning_rate": 4.587409056622138e-05,
      "loss": 0.7692,
      "step": 542600
    },
    {
      "epoch": 4.952003795897511,
      "grad_norm": 3.5960867404937744,
      "learning_rate": 4.587333017008541e-05,
      "loss": 0.6824,
      "step": 542700
    },
    {
      "epoch": 4.952916271260676,
      "grad_norm": 3.5705435276031494,
      "learning_rate": 4.5872569773949434e-05,
      "loss": 0.7225,
      "step": 542800
    },
    {
      "epoch": 4.9538287466238415,
      "grad_norm": 4.385049343109131,
      "learning_rate": 4.587180937781347e-05,
      "loss": 0.6862,
      "step": 542900
    },
    {
      "epoch": 4.954741221987007,
      "grad_norm": 3.6552412509918213,
      "learning_rate": 4.5871048981677494e-05,
      "loss": 0.7259,
      "step": 543000
    },
    {
      "epoch": 4.955653697350171,
      "grad_norm": 4.958806991577148,
      "learning_rate": 4.5870288585541524e-05,
      "loss": 0.6911,
      "step": 543100
    },
    {
      "epoch": 4.9565661727133365,
      "grad_norm": 3.101341724395752,
      "learning_rate": 4.5869528189405554e-05,
      "loss": 0.6903,
      "step": 543200
    },
    {
      "epoch": 4.957478648076502,
      "grad_norm": 3.7395997047424316,
      "learning_rate": 4.5868767793269584e-05,
      "loss": 0.6877,
      "step": 543300
    },
    {
      "epoch": 4.958391123439667,
      "grad_norm": 4.0535688400268555,
      "learning_rate": 4.5868007397133614e-05,
      "loss": 0.7148,
      "step": 543400
    },
    {
      "epoch": 4.9593035988028324,
      "grad_norm": 4.356668949127197,
      "learning_rate": 4.5867247000997644e-05,
      "loss": 0.7348,
      "step": 543500
    },
    {
      "epoch": 4.960216074165998,
      "grad_norm": 4.406242847442627,
      "learning_rate": 4.586648660486167e-05,
      "loss": 0.7251,
      "step": 543600
    },
    {
      "epoch": 4.961128549529163,
      "grad_norm": 4.593617916107178,
      "learning_rate": 4.5865726208725704e-05,
      "loss": 0.7429,
      "step": 543700
    },
    {
      "epoch": 4.9620410248923275,
      "grad_norm": 4.8201375007629395,
      "learning_rate": 4.586496581258973e-05,
      "loss": 0.7214,
      "step": 543800
    },
    {
      "epoch": 4.962953500255493,
      "grad_norm": 4.573843479156494,
      "learning_rate": 4.586420541645376e-05,
      "loss": 0.737,
      "step": 543900
    },
    {
      "epoch": 4.963865975618658,
      "grad_norm": 4.384083271026611,
      "learning_rate": 4.586344502031779e-05,
      "loss": 0.7336,
      "step": 544000
    },
    {
      "epoch": 4.964778450981823,
      "grad_norm": 4.533069610595703,
      "learning_rate": 4.586268462418182e-05,
      "loss": 0.6899,
      "step": 544100
    },
    {
      "epoch": 4.965690926344989,
      "grad_norm": 4.145086288452148,
      "learning_rate": 4.586192422804584e-05,
      "loss": 0.7254,
      "step": 544200
    },
    {
      "epoch": 4.966603401708154,
      "grad_norm": 5.733280658721924,
      "learning_rate": 4.586116383190988e-05,
      "loss": 0.6967,
      "step": 544300
    },
    {
      "epoch": 4.967515877071319,
      "grad_norm": 4.319029331207275,
      "learning_rate": 4.58604034357739e-05,
      "loss": 0.7402,
      "step": 544400
    },
    {
      "epoch": 4.9684283524344846,
      "grad_norm": 4.087490081787109,
      "learning_rate": 4.585964303963793e-05,
      "loss": 0.7751,
      "step": 544500
    },
    {
      "epoch": 4.96934082779765,
      "grad_norm": 4.511039733886719,
      "learning_rate": 4.585888264350196e-05,
      "loss": 0.7488,
      "step": 544600
    },
    {
      "epoch": 4.970253303160815,
      "grad_norm": 3.5148138999938965,
      "learning_rate": 4.585812224736599e-05,
      "loss": 0.7188,
      "step": 544700
    },
    {
      "epoch": 4.97116577852398,
      "grad_norm": 5.415441036224365,
      "learning_rate": 4.585736185123002e-05,
      "loss": 0.6874,
      "step": 544800
    },
    {
      "epoch": 4.972078253887145,
      "grad_norm": 4.466864585876465,
      "learning_rate": 4.585660145509405e-05,
      "loss": 0.6803,
      "step": 544900
    },
    {
      "epoch": 4.97299072925031,
      "grad_norm": 4.196568965911865,
      "learning_rate": 4.5855841058958075e-05,
      "loss": 0.7046,
      "step": 545000
    },
    {
      "epoch": 4.9739032046134755,
      "grad_norm": 5.152771472930908,
      "learning_rate": 4.585508066282211e-05,
      "loss": 0.697,
      "step": 545100
    },
    {
      "epoch": 4.974815679976641,
      "grad_norm": 4.423260688781738,
      "learning_rate": 4.5854320266686135e-05,
      "loss": 0.6964,
      "step": 545200
    },
    {
      "epoch": 4.975728155339806,
      "grad_norm": 3.170649290084839,
      "learning_rate": 4.5853559870550165e-05,
      "loss": 0.7294,
      "step": 545300
    },
    {
      "epoch": 4.976640630702971,
      "grad_norm": 3.257094383239746,
      "learning_rate": 4.5852799474414195e-05,
      "loss": 0.7155,
      "step": 545400
    },
    {
      "epoch": 4.977553106066136,
      "grad_norm": 4.595573902130127,
      "learning_rate": 4.585203907827822e-05,
      "loss": 0.7466,
      "step": 545500
    },
    {
      "epoch": 4.978465581429301,
      "grad_norm": 4.114386081695557,
      "learning_rate": 4.585127868214225e-05,
      "loss": 0.7491,
      "step": 545600
    },
    {
      "epoch": 4.979378056792466,
      "grad_norm": 3.8366339206695557,
      "learning_rate": 4.585051828600628e-05,
      "loss": 0.7379,
      "step": 545700
    },
    {
      "epoch": 4.980290532155632,
      "grad_norm": 3.28639817237854,
      "learning_rate": 4.584975788987031e-05,
      "loss": 0.6959,
      "step": 545800
    },
    {
      "epoch": 4.981203007518797,
      "grad_norm": 4.4804158210754395,
      "learning_rate": 4.584899749373434e-05,
      "loss": 0.6922,
      "step": 545900
    },
    {
      "epoch": 4.982115482881962,
      "grad_norm": 3.8258118629455566,
      "learning_rate": 4.584823709759837e-05,
      "loss": 0.6849,
      "step": 546000
    },
    {
      "epoch": 4.983027958245128,
      "grad_norm": 4.855809688568115,
      "learning_rate": 4.584747670146239e-05,
      "loss": 0.7099,
      "step": 546100
    },
    {
      "epoch": 4.983940433608293,
      "grad_norm": 4.527267932891846,
      "learning_rate": 4.584671630532643e-05,
      "loss": 0.7095,
      "step": 546200
    },
    {
      "epoch": 4.984852908971458,
      "grad_norm": 4.178343772888184,
      "learning_rate": 4.584595590919045e-05,
      "loss": 0.7365,
      "step": 546300
    },
    {
      "epoch": 4.9857653843346235,
      "grad_norm": 3.6935532093048096,
      "learning_rate": 4.584519551305448e-05,
      "loss": 0.6819,
      "step": 546400
    },
    {
      "epoch": 4.986677859697788,
      "grad_norm": 3.762619972229004,
      "learning_rate": 4.584443511691851e-05,
      "loss": 0.7298,
      "step": 546500
    },
    {
      "epoch": 4.987590335060953,
      "grad_norm": 4.751269340515137,
      "learning_rate": 4.584367472078254e-05,
      "loss": 0.7247,
      "step": 546600
    },
    {
      "epoch": 4.9885028104241185,
      "grad_norm": 4.030645370483398,
      "learning_rate": 4.5842914324646566e-05,
      "loss": 0.7276,
      "step": 546700
    },
    {
      "epoch": 4.989415285787284,
      "grad_norm": 4.341917991638184,
      "learning_rate": 4.58421539285106e-05,
      "loss": 0.7424,
      "step": 546800
    },
    {
      "epoch": 4.990327761150449,
      "grad_norm": 5.4365410804748535,
      "learning_rate": 4.5841393532374626e-05,
      "loss": 0.7319,
      "step": 546900
    },
    {
      "epoch": 4.991240236513614,
      "grad_norm": 4.280082702636719,
      "learning_rate": 4.5840633136238656e-05,
      "loss": 0.7188,
      "step": 547000
    },
    {
      "epoch": 4.99215271187678,
      "grad_norm": 4.976367950439453,
      "learning_rate": 4.5839872740102686e-05,
      "loss": 0.6653,
      "step": 547100
    },
    {
      "epoch": 4.993065187239944,
      "grad_norm": 4.481325626373291,
      "learning_rate": 4.5839112343966716e-05,
      "loss": 0.7359,
      "step": 547200
    },
    {
      "epoch": 4.993977662603109,
      "grad_norm": 4.715250492095947,
      "learning_rate": 4.5838351947830746e-05,
      "loss": 0.6815,
      "step": 547300
    },
    {
      "epoch": 4.994890137966275,
      "grad_norm": 4.338933944702148,
      "learning_rate": 4.5837591551694776e-05,
      "loss": 0.7134,
      "step": 547400
    },
    {
      "epoch": 4.99580261332944,
      "grad_norm": 2.8483989238739014,
      "learning_rate": 4.58368311555588e-05,
      "loss": 0.7069,
      "step": 547500
    },
    {
      "epoch": 4.996715088692605,
      "grad_norm": 3.411191940307617,
      "learning_rate": 4.5836070759422836e-05,
      "loss": 0.7339,
      "step": 547600
    },
    {
      "epoch": 4.997627564055771,
      "grad_norm": 4.136804580688477,
      "learning_rate": 4.583531036328686e-05,
      "loss": 0.7189,
      "step": 547700
    },
    {
      "epoch": 4.998540039418936,
      "grad_norm": 4.053274631500244,
      "learning_rate": 4.583454996715089e-05,
      "loss": 0.6742,
      "step": 547800
    },
    {
      "epoch": 4.999452514782101,
      "grad_norm": 5.141177654266357,
      "learning_rate": 4.583378957101492e-05,
      "loss": 0.6892,
      "step": 547900
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5806193351745605,
      "eval_runtime": 25.3284,
      "eval_samples_per_second": 227.768,
      "eval_steps_per_second": 227.768,
      "step": 547960
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5641083717346191,
      "eval_runtime": 482.9055,
      "eval_samples_per_second": 226.943,
      "eval_steps_per_second": 226.943,
      "step": 547960
    },
    {
      "epoch": 5.0003649901452665,
      "grad_norm": 4.309816837310791,
      "learning_rate": 4.583302917487895e-05,
      "loss": 0.7186,
      "step": 548000
    },
    {
      "epoch": 5.001277465508431,
      "grad_norm": 4.689461708068848,
      "learning_rate": 4.583226877874297e-05,
      "loss": 0.7106,
      "step": 548100
    },
    {
      "epoch": 5.002189940871596,
      "grad_norm": 3.872129201889038,
      "learning_rate": 4.5831508382607e-05,
      "loss": 0.6963,
      "step": 548200
    },
    {
      "epoch": 5.0031024162347615,
      "grad_norm": 4.090952396392822,
      "learning_rate": 4.583074798647103e-05,
      "loss": 0.7165,
      "step": 548300
    },
    {
      "epoch": 5.004014891597927,
      "grad_norm": 3.998140335083008,
      "learning_rate": 4.5829987590335063e-05,
      "loss": 0.7417,
      "step": 548400
    },
    {
      "epoch": 5.004927366961092,
      "grad_norm": 4.7439866065979,
      "learning_rate": 4.5829227194199093e-05,
      "loss": 0.7213,
      "step": 548500
    },
    {
      "epoch": 5.005839842324257,
      "grad_norm": 3.850745439529419,
      "learning_rate": 4.582846679806312e-05,
      "loss": 0.7265,
      "step": 548600
    },
    {
      "epoch": 5.006752317687423,
      "grad_norm": 4.167159080505371,
      "learning_rate": 4.5827706401927154e-05,
      "loss": 0.7096,
      "step": 548700
    },
    {
      "epoch": 5.007664793050588,
      "grad_norm": 3.6179914474487305,
      "learning_rate": 4.582694600579118e-05,
      "loss": 0.7193,
      "step": 548800
    },
    {
      "epoch": 5.008577268413752,
      "grad_norm": 4.028410911560059,
      "learning_rate": 4.582618560965521e-05,
      "loss": 0.7501,
      "step": 548900
    },
    {
      "epoch": 5.009489743776918,
      "grad_norm": 4.285787105560303,
      "learning_rate": 4.582542521351924e-05,
      "loss": 0.7139,
      "step": 549000
    },
    {
      "epoch": 5.010402219140083,
      "grad_norm": 3.270338296890259,
      "learning_rate": 4.582466481738327e-05,
      "loss": 0.7033,
      "step": 549100
    },
    {
      "epoch": 5.011314694503248,
      "grad_norm": 3.6099770069122314,
      "learning_rate": 4.582390442124729e-05,
      "loss": 0.7341,
      "step": 549200
    },
    {
      "epoch": 5.012227169866414,
      "grad_norm": 2.487919807434082,
      "learning_rate": 4.582314402511133e-05,
      "loss": 0.7232,
      "step": 549300
    },
    {
      "epoch": 5.013139645229579,
      "grad_norm": 4.894607067108154,
      "learning_rate": 4.582238362897535e-05,
      "loss": 0.7197,
      "step": 549400
    },
    {
      "epoch": 5.014052120592744,
      "grad_norm": 3.746939182281494,
      "learning_rate": 4.582162323283938e-05,
      "loss": 0.7211,
      "step": 549500
    },
    {
      "epoch": 5.0149645959559095,
      "grad_norm": 3.598740339279175,
      "learning_rate": 4.582086283670341e-05,
      "loss": 0.6733,
      "step": 549600
    },
    {
      "epoch": 5.015877071319075,
      "grad_norm": 3.4595141410827637,
      "learning_rate": 4.582010244056744e-05,
      "loss": 0.7337,
      "step": 549700
    },
    {
      "epoch": 5.016789546682239,
      "grad_norm": 4.446200847625732,
      "learning_rate": 4.581934204443147e-05,
      "loss": 0.6989,
      "step": 549800
    },
    {
      "epoch": 5.0177020220454045,
      "grad_norm": 4.308772563934326,
      "learning_rate": 4.58185816482955e-05,
      "loss": 0.6949,
      "step": 549900
    },
    {
      "epoch": 5.01861449740857,
      "grad_norm": 4.273824691772461,
      "learning_rate": 4.5817821252159524e-05,
      "loss": 0.7046,
      "step": 550000
    },
    {
      "epoch": 5.019526972771735,
      "grad_norm": 5.36695671081543,
      "learning_rate": 4.581706085602356e-05,
      "loss": 0.7228,
      "step": 550100
    },
    {
      "epoch": 5.0204394481349,
      "grad_norm": 3.405137062072754,
      "learning_rate": 4.5816300459887584e-05,
      "loss": 0.7115,
      "step": 550200
    },
    {
      "epoch": 5.021351923498066,
      "grad_norm": 3.9505457878112793,
      "learning_rate": 4.5815540063751614e-05,
      "loss": 0.7006,
      "step": 550300
    },
    {
      "epoch": 5.022264398861231,
      "grad_norm": 4.475953578948975,
      "learning_rate": 4.5814779667615644e-05,
      "loss": 0.6986,
      "step": 550400
    },
    {
      "epoch": 5.023176874224396,
      "grad_norm": 4.73861837387085,
      "learning_rate": 4.5814019271479675e-05,
      "loss": 0.707,
      "step": 550500
    },
    {
      "epoch": 5.024089349587561,
      "grad_norm": 3.9123895168304443,
      "learning_rate": 4.58132588753437e-05,
      "loss": 0.6941,
      "step": 550600
    },
    {
      "epoch": 5.025001824950726,
      "grad_norm": 3.924591302871704,
      "learning_rate": 4.5812498479207735e-05,
      "loss": 0.6657,
      "step": 550700
    },
    {
      "epoch": 5.025914300313891,
      "grad_norm": 3.738905191421509,
      "learning_rate": 4.581173808307176e-05,
      "loss": 0.7065,
      "step": 550800
    },
    {
      "epoch": 5.026826775677057,
      "grad_norm": 4.45388650894165,
      "learning_rate": 4.581097768693579e-05,
      "loss": 0.6999,
      "step": 550900
    },
    {
      "epoch": 5.027739251040222,
      "grad_norm": 3.286421298980713,
      "learning_rate": 4.581021729079982e-05,
      "loss": 0.7033,
      "step": 551000
    },
    {
      "epoch": 5.028651726403387,
      "grad_norm": 4.014275074005127,
      "learning_rate": 4.580945689466384e-05,
      "loss": 0.7197,
      "step": 551100
    },
    {
      "epoch": 5.0295642017665525,
      "grad_norm": 4.2042951583862305,
      "learning_rate": 4.580869649852788e-05,
      "loss": 0.6753,
      "step": 551200
    },
    {
      "epoch": 5.030476677129718,
      "grad_norm": 2.846524238586426,
      "learning_rate": 4.58079361023919e-05,
      "loss": 0.6996,
      "step": 551300
    },
    {
      "epoch": 5.031389152492883,
      "grad_norm": 4.105154514312744,
      "learning_rate": 4.580717570625593e-05,
      "loss": 0.7379,
      "step": 551400
    },
    {
      "epoch": 5.0323016278560475,
      "grad_norm": 5.0384697914123535,
      "learning_rate": 4.580641531011996e-05,
      "loss": 0.7363,
      "step": 551500
    },
    {
      "epoch": 5.033214103219213,
      "grad_norm": 3.9477810859680176,
      "learning_rate": 4.580565491398399e-05,
      "loss": 0.7474,
      "step": 551600
    },
    {
      "epoch": 5.034126578582378,
      "grad_norm": 3.2526113986968994,
      "learning_rate": 4.5804894517848015e-05,
      "loss": 0.687,
      "step": 551700
    },
    {
      "epoch": 5.035039053945543,
      "grad_norm": 3.9287400245666504,
      "learning_rate": 4.580413412171205e-05,
      "loss": 0.7199,
      "step": 551800
    },
    {
      "epoch": 5.035951529308709,
      "grad_norm": 4.632115840911865,
      "learning_rate": 4.5803373725576075e-05,
      "loss": 0.7428,
      "step": 551900
    },
    {
      "epoch": 5.036864004671874,
      "grad_norm": 4.5745954513549805,
      "learning_rate": 4.5802613329440105e-05,
      "loss": 0.7562,
      "step": 552000
    },
    {
      "epoch": 5.037776480035039,
      "grad_norm": 4.3108134269714355,
      "learning_rate": 4.5801852933304135e-05,
      "loss": 0.6868,
      "step": 552100
    },
    {
      "epoch": 5.038688955398205,
      "grad_norm": 3.88828444480896,
      "learning_rate": 4.5801092537168165e-05,
      "loss": 0.7033,
      "step": 552200
    },
    {
      "epoch": 5.039601430761369,
      "grad_norm": 3.2192602157592773,
      "learning_rate": 4.5800332141032195e-05,
      "loss": 0.6889,
      "step": 552300
    },
    {
      "epoch": 5.040513906124534,
      "grad_norm": 4.198910713195801,
      "learning_rate": 4.5799571744896225e-05,
      "loss": 0.6968,
      "step": 552400
    },
    {
      "epoch": 5.0414263814877,
      "grad_norm": 5.051853656768799,
      "learning_rate": 4.579881134876025e-05,
      "loss": 0.7319,
      "step": 552500
    },
    {
      "epoch": 5.042338856850865,
      "grad_norm": 3.800381660461426,
      "learning_rate": 4.5798050952624286e-05,
      "loss": 0.6852,
      "step": 552600
    },
    {
      "epoch": 5.04325133221403,
      "grad_norm": 4.670203685760498,
      "learning_rate": 4.579729055648831e-05,
      "loss": 0.7153,
      "step": 552700
    },
    {
      "epoch": 5.0441638075771955,
      "grad_norm": 4.616703987121582,
      "learning_rate": 4.579653016035234e-05,
      "loss": 0.7109,
      "step": 552800
    },
    {
      "epoch": 5.045076282940361,
      "grad_norm": 3.90997052192688,
      "learning_rate": 4.579576976421637e-05,
      "loss": 0.7331,
      "step": 552900
    },
    {
      "epoch": 5.045988758303526,
      "grad_norm": 4.57922887802124,
      "learning_rate": 4.57950093680804e-05,
      "loss": 0.6911,
      "step": 553000
    },
    {
      "epoch": 5.046901233666691,
      "grad_norm": 2.9069881439208984,
      "learning_rate": 4.579424897194442e-05,
      "loss": 0.7506,
      "step": 553100
    },
    {
      "epoch": 5.047813709029856,
      "grad_norm": 3.4952030181884766,
      "learning_rate": 4.579348857580846e-05,
      "loss": 0.753,
      "step": 553200
    },
    {
      "epoch": 5.048726184393021,
      "grad_norm": 3.9400134086608887,
      "learning_rate": 4.579272817967248e-05,
      "loss": 0.721,
      "step": 553300
    },
    {
      "epoch": 5.049638659756186,
      "grad_norm": 4.484191417694092,
      "learning_rate": 4.579196778353651e-05,
      "loss": 0.7214,
      "step": 553400
    },
    {
      "epoch": 5.050551135119352,
      "grad_norm": 4.182705879211426,
      "learning_rate": 4.579120738740054e-05,
      "loss": 0.707,
      "step": 553500
    },
    {
      "epoch": 5.051463610482517,
      "grad_norm": 3.3763344287872314,
      "learning_rate": 4.579044699126457e-05,
      "loss": 0.702,
      "step": 553600
    },
    {
      "epoch": 5.052376085845682,
      "grad_norm": 2.7599120140075684,
      "learning_rate": 4.57896865951286e-05,
      "loss": 0.7386,
      "step": 553700
    },
    {
      "epoch": 5.053288561208848,
      "grad_norm": 3.444101333618164,
      "learning_rate": 4.578892619899263e-05,
      "loss": 0.7185,
      "step": 553800
    },
    {
      "epoch": 5.054201036572013,
      "grad_norm": 3.6962106227874756,
      "learning_rate": 4.5788165802856656e-05,
      "loss": 0.727,
      "step": 553900
    },
    {
      "epoch": 5.055113511935177,
      "grad_norm": 4.1862993240356445,
      "learning_rate": 4.5787405406720686e-05,
      "loss": 0.7326,
      "step": 554000
    },
    {
      "epoch": 5.056025987298343,
      "grad_norm": 3.7555015087127686,
      "learning_rate": 4.5786645010584716e-05,
      "loss": 0.7083,
      "step": 554100
    },
    {
      "epoch": 5.056938462661508,
      "grad_norm": 3.6663410663604736,
      "learning_rate": 4.5785884614448746e-05,
      "loss": 0.7419,
      "step": 554200
    },
    {
      "epoch": 5.057850938024673,
      "grad_norm": 4.085482120513916,
      "learning_rate": 4.5785124218312776e-05,
      "loss": 0.7153,
      "step": 554300
    },
    {
      "epoch": 5.0587634133878385,
      "grad_norm": 4.839076042175293,
      "learning_rate": 4.57843638221768e-05,
      "loss": 0.7224,
      "step": 554400
    },
    {
      "epoch": 5.059675888751004,
      "grad_norm": 3.9112629890441895,
      "learning_rate": 4.578360342604083e-05,
      "loss": 0.724,
      "step": 554500
    },
    {
      "epoch": 5.060588364114169,
      "grad_norm": 4.600238800048828,
      "learning_rate": 4.578284302990486e-05,
      "loss": 0.6903,
      "step": 554600
    },
    {
      "epoch": 5.061500839477334,
      "grad_norm": 5.030231952667236,
      "learning_rate": 4.578208263376889e-05,
      "loss": 0.7258,
      "step": 554700
    },
    {
      "epoch": 5.0624133148405,
      "grad_norm": 4.113384246826172,
      "learning_rate": 4.578132223763292e-05,
      "loss": 0.6906,
      "step": 554800
    },
    {
      "epoch": 5.063325790203664,
      "grad_norm": 4.895646572113037,
      "learning_rate": 4.578056184149695e-05,
      "loss": 0.6812,
      "step": 554900
    },
    {
      "epoch": 5.064238265566829,
      "grad_norm": 3.6031386852264404,
      "learning_rate": 4.577980144536097e-05,
      "loss": 0.7381,
      "step": 555000
    },
    {
      "epoch": 5.065150740929995,
      "grad_norm": 3.8469159603118896,
      "learning_rate": 4.577904104922501e-05,
      "loss": 0.7167,
      "step": 555100
    },
    {
      "epoch": 5.06606321629316,
      "grad_norm": 3.960639476776123,
      "learning_rate": 4.5778280653089033e-05,
      "loss": 0.6975,
      "step": 555200
    },
    {
      "epoch": 5.066975691656325,
      "grad_norm": 3.83532452583313,
      "learning_rate": 4.5777520256953064e-05,
      "loss": 0.7336,
      "step": 555300
    },
    {
      "epoch": 5.067888167019491,
      "grad_norm": 5.580438613891602,
      "learning_rate": 4.5776759860817094e-05,
      "loss": 0.6772,
      "step": 555400
    },
    {
      "epoch": 5.068800642382656,
      "grad_norm": 3.979428768157959,
      "learning_rate": 4.5775999464681124e-05,
      "loss": 0.7112,
      "step": 555500
    },
    {
      "epoch": 5.069713117745821,
      "grad_norm": 5.042942047119141,
      "learning_rate": 4.5775239068545154e-05,
      "loss": 0.7405,
      "step": 555600
    },
    {
      "epoch": 5.070625593108986,
      "grad_norm": 4.649831771850586,
      "learning_rate": 4.5774478672409184e-05,
      "loss": 0.686,
      "step": 555700
    },
    {
      "epoch": 5.071538068472151,
      "grad_norm": 4.775732517242432,
      "learning_rate": 4.577371827627321e-05,
      "loss": 0.694,
      "step": 555800
    },
    {
      "epoch": 5.072450543835316,
      "grad_norm": 3.984391927719116,
      "learning_rate": 4.577295788013724e-05,
      "loss": 0.6967,
      "step": 555900
    },
    {
      "epoch": 5.0733630191984815,
      "grad_norm": 3.0218634605407715,
      "learning_rate": 4.577219748400127e-05,
      "loss": 0.6981,
      "step": 556000
    },
    {
      "epoch": 5.074275494561647,
      "grad_norm": 3.4047138690948486,
      "learning_rate": 4.57714370878653e-05,
      "loss": 0.6957,
      "step": 556100
    },
    {
      "epoch": 5.075187969924812,
      "grad_norm": 4.616887092590332,
      "learning_rate": 4.577067669172933e-05,
      "loss": 0.7154,
      "step": 556200
    },
    {
      "epoch": 5.076100445287977,
      "grad_norm": 3.0760955810546875,
      "learning_rate": 4.576991629559336e-05,
      "loss": 0.6775,
      "step": 556300
    },
    {
      "epoch": 5.077012920651143,
      "grad_norm": 4.206048488616943,
      "learning_rate": 4.576915589945738e-05,
      "loss": 0.6945,
      "step": 556400
    },
    {
      "epoch": 5.077925396014308,
      "grad_norm": 4.604321479797363,
      "learning_rate": 4.576839550332142e-05,
      "loss": 0.6968,
      "step": 556500
    },
    {
      "epoch": 5.0788378713774724,
      "grad_norm": 4.780033111572266,
      "learning_rate": 4.576763510718544e-05,
      "loss": 0.7349,
      "step": 556600
    },
    {
      "epoch": 5.079750346740638,
      "grad_norm": 4.622572898864746,
      "learning_rate": 4.576687471104947e-05,
      "loss": 0.6987,
      "step": 556700
    },
    {
      "epoch": 5.080662822103803,
      "grad_norm": 4.054455280303955,
      "learning_rate": 4.57661143149135e-05,
      "loss": 0.7234,
      "step": 556800
    },
    {
      "epoch": 5.081575297466968,
      "grad_norm": 4.613686561584473,
      "learning_rate": 4.5765353918777524e-05,
      "loss": 0.7383,
      "step": 556900
    },
    {
      "epoch": 5.082487772830134,
      "grad_norm": 4.454588413238525,
      "learning_rate": 4.576459352264156e-05,
      "loss": 0.698,
      "step": 557000
    },
    {
      "epoch": 5.083400248193299,
      "grad_norm": 3.659379243850708,
      "learning_rate": 4.5763833126505584e-05,
      "loss": 0.7195,
      "step": 557100
    },
    {
      "epoch": 5.084312723556464,
      "grad_norm": 4.042923450469971,
      "learning_rate": 4.5763072730369614e-05,
      "loss": 0.703,
      "step": 557200
    },
    {
      "epoch": 5.0852251989196295,
      "grad_norm": 2.365414619445801,
      "learning_rate": 4.5762312334233645e-05,
      "loss": 0.7049,
      "step": 557300
    },
    {
      "epoch": 5.086137674282794,
      "grad_norm": 3.8978731632232666,
      "learning_rate": 4.5761551938097675e-05,
      "loss": 0.6796,
      "step": 557400
    },
    {
      "epoch": 5.087050149645959,
      "grad_norm": 4.406797409057617,
      "learning_rate": 4.57607915419617e-05,
      "loss": 0.7146,
      "step": 557500
    },
    {
      "epoch": 5.0879626250091246,
      "grad_norm": 3.2844698429107666,
      "learning_rate": 4.5760031145825735e-05,
      "loss": 0.6888,
      "step": 557600
    },
    {
      "epoch": 5.08887510037229,
      "grad_norm": 3.161632537841797,
      "learning_rate": 4.575927074968976e-05,
      "loss": 0.6997,
      "step": 557700
    },
    {
      "epoch": 5.089787575735455,
      "grad_norm": 4.409976959228516,
      "learning_rate": 4.575851035355379e-05,
      "loss": 0.6987,
      "step": 557800
    },
    {
      "epoch": 5.0907000510986204,
      "grad_norm": 4.590358257293701,
      "learning_rate": 4.575774995741782e-05,
      "loss": 0.7161,
      "step": 557900
    },
    {
      "epoch": 5.091612526461786,
      "grad_norm": 3.6072516441345215,
      "learning_rate": 4.575698956128185e-05,
      "loss": 0.7489,
      "step": 558000
    },
    {
      "epoch": 5.092525001824951,
      "grad_norm": 4.414759635925293,
      "learning_rate": 4.575622916514588e-05,
      "loss": 0.7301,
      "step": 558100
    },
    {
      "epoch": 5.093437477188116,
      "grad_norm": 4.420087814331055,
      "learning_rate": 4.575546876900991e-05,
      "loss": 0.6719,
      "step": 558200
    },
    {
      "epoch": 5.094349952551281,
      "grad_norm": 4.845860004425049,
      "learning_rate": 4.575470837287393e-05,
      "loss": 0.7106,
      "step": 558300
    },
    {
      "epoch": 5.095262427914446,
      "grad_norm": 4.601391315460205,
      "learning_rate": 4.575394797673797e-05,
      "loss": 0.6703,
      "step": 558400
    },
    {
      "epoch": 5.096174903277611,
      "grad_norm": 5.494359493255615,
      "learning_rate": 4.575318758060199e-05,
      "loss": 0.7247,
      "step": 558500
    },
    {
      "epoch": 5.097087378640777,
      "grad_norm": 5.237854480743408,
      "learning_rate": 4.575242718446602e-05,
      "loss": 0.7106,
      "step": 558600
    },
    {
      "epoch": 5.097999854003942,
      "grad_norm": 3.8607075214385986,
      "learning_rate": 4.575166678833005e-05,
      "loss": 0.6876,
      "step": 558700
    },
    {
      "epoch": 5.098912329367107,
      "grad_norm": 3.7031729221343994,
      "learning_rate": 4.575090639219408e-05,
      "loss": 0.7118,
      "step": 558800
    },
    {
      "epoch": 5.0998248047302726,
      "grad_norm": 3.9760751724243164,
      "learning_rate": 4.5750145996058105e-05,
      "loss": 0.7312,
      "step": 558900
    },
    {
      "epoch": 5.100737280093438,
      "grad_norm": 3.9627139568328857,
      "learning_rate": 4.574938559992214e-05,
      "loss": 0.7286,
      "step": 559000
    },
    {
      "epoch": 5.101649755456602,
      "grad_norm": 4.840254306793213,
      "learning_rate": 4.5748625203786165e-05,
      "loss": 0.7303,
      "step": 559100
    },
    {
      "epoch": 5.102562230819768,
      "grad_norm": 4.590498924255371,
      "learning_rate": 4.5747864807650195e-05,
      "loss": 0.6853,
      "step": 559200
    },
    {
      "epoch": 5.103474706182933,
      "grad_norm": 4.116511344909668,
      "learning_rate": 4.5747104411514226e-05,
      "loss": 0.6641,
      "step": 559300
    },
    {
      "epoch": 5.104387181546098,
      "grad_norm": 4.025757789611816,
      "learning_rate": 4.5746344015378256e-05,
      "loss": 0.7069,
      "step": 559400
    },
    {
      "epoch": 5.1052996569092635,
      "grad_norm": 4.261534690856934,
      "learning_rate": 4.5745583619242286e-05,
      "loss": 0.7049,
      "step": 559500
    },
    {
      "epoch": 5.106212132272429,
      "grad_norm": 4.2517924308776855,
      "learning_rate": 4.574482322310631e-05,
      "loss": 0.7309,
      "step": 559600
    },
    {
      "epoch": 5.107124607635594,
      "grad_norm": 3.7593040466308594,
      "learning_rate": 4.574406282697034e-05,
      "loss": 0.7681,
      "step": 559700
    },
    {
      "epoch": 5.108037082998759,
      "grad_norm": 4.639091491699219,
      "learning_rate": 4.574330243083437e-05,
      "loss": 0.7191,
      "step": 559800
    },
    {
      "epoch": 5.108949558361925,
      "grad_norm": 4.09711217880249,
      "learning_rate": 4.57425420346984e-05,
      "loss": 0.7175,
      "step": 559900
    },
    {
      "epoch": 5.109862033725089,
      "grad_norm": 3.7136170864105225,
      "learning_rate": 4.574178163856242e-05,
      "loss": 0.7292,
      "step": 560000
    },
    {
      "epoch": 5.110774509088254,
      "grad_norm": 4.067484378814697,
      "learning_rate": 4.574102124242646e-05,
      "loss": 0.6834,
      "step": 560100
    },
    {
      "epoch": 5.11168698445142,
      "grad_norm": 4.05373477935791,
      "learning_rate": 4.574026084629048e-05,
      "loss": 0.6726,
      "step": 560200
    },
    {
      "epoch": 5.112599459814585,
      "grad_norm": 4.186582088470459,
      "learning_rate": 4.573950045015451e-05,
      "loss": 0.7135,
      "step": 560300
    },
    {
      "epoch": 5.11351193517775,
      "grad_norm": 4.140480041503906,
      "learning_rate": 4.573874005401854e-05,
      "loss": 0.7178,
      "step": 560400
    },
    {
      "epoch": 5.114424410540916,
      "grad_norm": 4.097560405731201,
      "learning_rate": 4.573797965788257e-05,
      "loss": 0.7274,
      "step": 560500
    },
    {
      "epoch": 5.115336885904081,
      "grad_norm": 4.124269962310791,
      "learning_rate": 4.57372192617466e-05,
      "loss": 0.7429,
      "step": 560600
    },
    {
      "epoch": 5.116249361267246,
      "grad_norm": 4.452011585235596,
      "learning_rate": 4.573645886561063e-05,
      "loss": 0.7616,
      "step": 560700
    },
    {
      "epoch": 5.117161836630411,
      "grad_norm": 4.256622314453125,
      "learning_rate": 4.5735698469474656e-05,
      "loss": 0.6779,
      "step": 560800
    },
    {
      "epoch": 5.118074311993576,
      "grad_norm": 3.9790537357330322,
      "learning_rate": 4.573493807333869e-05,
      "loss": 0.7052,
      "step": 560900
    },
    {
      "epoch": 5.118986787356741,
      "grad_norm": 3.3334758281707764,
      "learning_rate": 4.5734177677202716e-05,
      "loss": 0.6767,
      "step": 561000
    },
    {
      "epoch": 5.1198992627199065,
      "grad_norm": 3.6241071224212646,
      "learning_rate": 4.5733417281066746e-05,
      "loss": 0.7242,
      "step": 561100
    },
    {
      "epoch": 5.120811738083072,
      "grad_norm": 3.8606314659118652,
      "learning_rate": 4.5732656884930776e-05,
      "loss": 0.6956,
      "step": 561200
    },
    {
      "epoch": 5.121724213446237,
      "grad_norm": 5.851873874664307,
      "learning_rate": 4.5731896488794807e-05,
      "loss": 0.7013,
      "step": 561300
    },
    {
      "epoch": 5.122636688809402,
      "grad_norm": 4.451363563537598,
      "learning_rate": 4.573113609265883e-05,
      "loss": 0.6829,
      "step": 561400
    },
    {
      "epoch": 5.123549164172568,
      "grad_norm": 3.7628185749053955,
      "learning_rate": 4.573037569652287e-05,
      "loss": 0.7053,
      "step": 561500
    },
    {
      "epoch": 5.124461639535733,
      "grad_norm": 4.89739990234375,
      "learning_rate": 4.572961530038689e-05,
      "loss": 0.6939,
      "step": 561600
    },
    {
      "epoch": 5.125374114898897,
      "grad_norm": 4.668117046356201,
      "learning_rate": 4.572885490425092e-05,
      "loss": 0.6745,
      "step": 561700
    },
    {
      "epoch": 5.126286590262063,
      "grad_norm": 4.204663276672363,
      "learning_rate": 4.572809450811495e-05,
      "loss": 0.7158,
      "step": 561800
    },
    {
      "epoch": 5.127199065625228,
      "grad_norm": 4.97841739654541,
      "learning_rate": 4.572733411197898e-05,
      "loss": 0.7289,
      "step": 561900
    },
    {
      "epoch": 5.128111540988393,
      "grad_norm": 4.088755130767822,
      "learning_rate": 4.572657371584301e-05,
      "loss": 0.7139,
      "step": 562000
    },
    {
      "epoch": 5.129024016351559,
      "grad_norm": 4.8344645500183105,
      "learning_rate": 4.572581331970704e-05,
      "loss": 0.7081,
      "step": 562100
    },
    {
      "epoch": 5.129936491714724,
      "grad_norm": 3.8718340396881104,
      "learning_rate": 4.5725052923571064e-05,
      "loss": 0.6967,
      "step": 562200
    },
    {
      "epoch": 5.130848967077889,
      "grad_norm": 4.535024642944336,
      "learning_rate": 4.57242925274351e-05,
      "loss": 0.698,
      "step": 562300
    },
    {
      "epoch": 5.1317614424410545,
      "grad_norm": 4.212837219238281,
      "learning_rate": 4.5723532131299124e-05,
      "loss": 0.6706,
      "step": 562400
    },
    {
      "epoch": 5.132673917804219,
      "grad_norm": 2.2827837467193604,
      "learning_rate": 4.572277173516315e-05,
      "loss": 0.779,
      "step": 562500
    },
    {
      "epoch": 5.133586393167384,
      "grad_norm": 4.285126209259033,
      "learning_rate": 4.5722011339027184e-05,
      "loss": 0.6797,
      "step": 562600
    },
    {
      "epoch": 5.1344988685305495,
      "grad_norm": 4.376114845275879,
      "learning_rate": 4.572125094289121e-05,
      "loss": 0.6785,
      "step": 562700
    },
    {
      "epoch": 5.135411343893715,
      "grad_norm": 4.938810348510742,
      "learning_rate": 4.572049054675524e-05,
      "loss": 0.7119,
      "step": 562800
    },
    {
      "epoch": 5.13632381925688,
      "grad_norm": 4.358919143676758,
      "learning_rate": 4.571973015061927e-05,
      "loss": 0.7285,
      "step": 562900
    },
    {
      "epoch": 5.137236294620045,
      "grad_norm": 4.049164295196533,
      "learning_rate": 4.57189697544833e-05,
      "loss": 0.7357,
      "step": 563000
    },
    {
      "epoch": 5.138148769983211,
      "grad_norm": 4.214237689971924,
      "learning_rate": 4.571820935834733e-05,
      "loss": 0.6863,
      "step": 563100
    },
    {
      "epoch": 5.139061245346376,
      "grad_norm": 4.570882320404053,
      "learning_rate": 4.571744896221136e-05,
      "loss": 0.7277,
      "step": 563200
    },
    {
      "epoch": 5.139973720709541,
      "grad_norm": 5.023329257965088,
      "learning_rate": 4.571668856607538e-05,
      "loss": 0.7037,
      "step": 563300
    },
    {
      "epoch": 5.140886196072706,
      "grad_norm": 3.6005544662475586,
      "learning_rate": 4.571592816993942e-05,
      "loss": 0.7453,
      "step": 563400
    },
    {
      "epoch": 5.141798671435871,
      "grad_norm": 4.545827388763428,
      "learning_rate": 4.571516777380344e-05,
      "loss": 0.733,
      "step": 563500
    },
    {
      "epoch": 5.142711146799036,
      "grad_norm": 4.648103713989258,
      "learning_rate": 4.571440737766747e-05,
      "loss": 0.7068,
      "step": 563600
    },
    {
      "epoch": 5.143623622162202,
      "grad_norm": 4.38763427734375,
      "learning_rate": 4.57136469815315e-05,
      "loss": 0.6972,
      "step": 563700
    },
    {
      "epoch": 5.144536097525367,
      "grad_norm": 3.987147092819214,
      "learning_rate": 4.571288658539553e-05,
      "loss": 0.6734,
      "step": 563800
    },
    {
      "epoch": 5.145448572888532,
      "grad_norm": 3.7114968299865723,
      "learning_rate": 4.5712126189259554e-05,
      "loss": 0.7144,
      "step": 563900
    },
    {
      "epoch": 5.1463610482516975,
      "grad_norm": 5.115237236022949,
      "learning_rate": 4.571136579312359e-05,
      "loss": 0.7221,
      "step": 564000
    },
    {
      "epoch": 5.147273523614863,
      "grad_norm": 4.372722148895264,
      "learning_rate": 4.5710605396987615e-05,
      "loss": 0.7136,
      "step": 564100
    },
    {
      "epoch": 5.148185998978027,
      "grad_norm": 4.521966934204102,
      "learning_rate": 4.5709845000851645e-05,
      "loss": 0.7359,
      "step": 564200
    },
    {
      "epoch": 5.1490984743411925,
      "grad_norm": 3.7950782775878906,
      "learning_rate": 4.5709084604715675e-05,
      "loss": 0.6804,
      "step": 564300
    },
    {
      "epoch": 5.150010949704358,
      "grad_norm": 2.934058666229248,
      "learning_rate": 4.5708324208579705e-05,
      "loss": 0.719,
      "step": 564400
    },
    {
      "epoch": 5.150923425067523,
      "grad_norm": 3.472367286682129,
      "learning_rate": 4.5707563812443735e-05,
      "loss": 0.7426,
      "step": 564500
    },
    {
      "epoch": 5.151835900430688,
      "grad_norm": 4.050068378448486,
      "learning_rate": 4.5706803416307765e-05,
      "loss": 0.709,
      "step": 564600
    },
    {
      "epoch": 5.152748375793854,
      "grad_norm": 4.363460540771484,
      "learning_rate": 4.570604302017179e-05,
      "loss": 0.7483,
      "step": 564700
    },
    {
      "epoch": 5.153660851157019,
      "grad_norm": 3.7823567390441895,
      "learning_rate": 4.5705282624035825e-05,
      "loss": 0.7165,
      "step": 564800
    },
    {
      "epoch": 5.154573326520184,
      "grad_norm": 4.122178077697754,
      "learning_rate": 4.570452222789985e-05,
      "loss": 0.6839,
      "step": 564900
    },
    {
      "epoch": 5.15548580188335,
      "grad_norm": 3.6750264167785645,
      "learning_rate": 4.570376183176388e-05,
      "loss": 0.6926,
      "step": 565000
    },
    {
      "epoch": 5.156398277246514,
      "grad_norm": 3.9631857872009277,
      "learning_rate": 4.570300143562791e-05,
      "loss": 0.7316,
      "step": 565100
    },
    {
      "epoch": 5.157310752609679,
      "grad_norm": 3.9020681381225586,
      "learning_rate": 4.570224103949193e-05,
      "loss": 0.7312,
      "step": 565200
    },
    {
      "epoch": 5.158223227972845,
      "grad_norm": 3.9974446296691895,
      "learning_rate": 4.570148064335596e-05,
      "loss": 0.7361,
      "step": 565300
    },
    {
      "epoch": 5.15913570333601,
      "grad_norm": 3.9857141971588135,
      "learning_rate": 4.570072024721999e-05,
      "loss": 0.6961,
      "step": 565400
    },
    {
      "epoch": 5.160048178699175,
      "grad_norm": 2.982541561126709,
      "learning_rate": 4.569995985108402e-05,
      "loss": 0.7091,
      "step": 565500
    },
    {
      "epoch": 5.1609606540623405,
      "grad_norm": 3.2911579608917236,
      "learning_rate": 4.569919945494805e-05,
      "loss": 0.7049,
      "step": 565600
    },
    {
      "epoch": 5.161873129425506,
      "grad_norm": 3.935413122177124,
      "learning_rate": 4.569843905881208e-05,
      "loss": 0.7335,
      "step": 565700
    },
    {
      "epoch": 5.162785604788671,
      "grad_norm": 3.6614830493927,
      "learning_rate": 4.5697678662676105e-05,
      "loss": 0.7112,
      "step": 565800
    },
    {
      "epoch": 5.1636980801518355,
      "grad_norm": 3.7179832458496094,
      "learning_rate": 4.569691826654014e-05,
      "loss": 0.6912,
      "step": 565900
    },
    {
      "epoch": 5.164610555515001,
      "grad_norm": 3.7127187252044678,
      "learning_rate": 4.5696157870404166e-05,
      "loss": 0.7448,
      "step": 566000
    },
    {
      "epoch": 5.165523030878166,
      "grad_norm": 4.760695934295654,
      "learning_rate": 4.5695397474268196e-05,
      "loss": 0.6709,
      "step": 566100
    },
    {
      "epoch": 5.166435506241331,
      "grad_norm": 4.034499645233154,
      "learning_rate": 4.5694637078132226e-05,
      "loss": 0.6907,
      "step": 566200
    },
    {
      "epoch": 5.167347981604497,
      "grad_norm": 4.883617401123047,
      "learning_rate": 4.5693876681996256e-05,
      "loss": 0.7195,
      "step": 566300
    },
    {
      "epoch": 5.168260456967662,
      "grad_norm": 4.126011371612549,
      "learning_rate": 4.569311628586028e-05,
      "loss": 0.7079,
      "step": 566400
    },
    {
      "epoch": 5.169172932330827,
      "grad_norm": 3.8430426120758057,
      "learning_rate": 4.5692355889724316e-05,
      "loss": 0.6891,
      "step": 566500
    },
    {
      "epoch": 5.170085407693993,
      "grad_norm": 3.95160174369812,
      "learning_rate": 4.569159549358834e-05,
      "loss": 0.7248,
      "step": 566600
    },
    {
      "epoch": 5.170997883057158,
      "grad_norm": 4.427484512329102,
      "learning_rate": 4.569083509745237e-05,
      "loss": 0.6911,
      "step": 566700
    },
    {
      "epoch": 5.171910358420322,
      "grad_norm": 4.455692768096924,
      "learning_rate": 4.56900747013164e-05,
      "loss": 0.7036,
      "step": 566800
    },
    {
      "epoch": 5.172822833783488,
      "grad_norm": 3.8897533416748047,
      "learning_rate": 4.568931430518043e-05,
      "loss": 0.7372,
      "step": 566900
    },
    {
      "epoch": 5.173735309146653,
      "grad_norm": 3.9429986476898193,
      "learning_rate": 4.568855390904446e-05,
      "loss": 0.719,
      "step": 567000
    },
    {
      "epoch": 5.174647784509818,
      "grad_norm": 4.376254558563232,
      "learning_rate": 4.568779351290849e-05,
      "loss": 0.7415,
      "step": 567100
    },
    {
      "epoch": 5.1755602598729835,
      "grad_norm": 4.438055515289307,
      "learning_rate": 4.568703311677251e-05,
      "loss": 0.6927,
      "step": 567200
    },
    {
      "epoch": 5.176472735236149,
      "grad_norm": 4.558547496795654,
      "learning_rate": 4.568627272063655e-05,
      "loss": 0.6929,
      "step": 567300
    },
    {
      "epoch": 5.177385210599314,
      "grad_norm": 4.110954761505127,
      "learning_rate": 4.568551232450057e-05,
      "loss": 0.6682,
      "step": 567400
    },
    {
      "epoch": 5.178297685962479,
      "grad_norm": 4.3308186531066895,
      "learning_rate": 4.56847519283646e-05,
      "loss": 0.7084,
      "step": 567500
    },
    {
      "epoch": 5.179210161325644,
      "grad_norm": 3.5134990215301514,
      "learning_rate": 4.568399153222863e-05,
      "loss": 0.7249,
      "step": 567600
    },
    {
      "epoch": 5.180122636688809,
      "grad_norm": 3.723200798034668,
      "learning_rate": 4.568323113609266e-05,
      "loss": 0.7062,
      "step": 567700
    },
    {
      "epoch": 5.181035112051974,
      "grad_norm": 4.068431377410889,
      "learning_rate": 4.568247073995669e-05,
      "loss": 0.701,
      "step": 567800
    },
    {
      "epoch": 5.18194758741514,
      "grad_norm": 3.8605289459228516,
      "learning_rate": 4.568171034382072e-05,
      "loss": 0.7127,
      "step": 567900
    },
    {
      "epoch": 5.182860062778305,
      "grad_norm": 4.630443572998047,
      "learning_rate": 4.5680949947684747e-05,
      "loss": 0.7233,
      "step": 568000
    },
    {
      "epoch": 5.18377253814147,
      "grad_norm": 4.016367435455322,
      "learning_rate": 4.5680189551548777e-05,
      "loss": 0.6867,
      "step": 568100
    },
    {
      "epoch": 5.184685013504636,
      "grad_norm": 3.5061488151550293,
      "learning_rate": 4.567942915541281e-05,
      "loss": 0.7334,
      "step": 568200
    },
    {
      "epoch": 5.185597488867801,
      "grad_norm": 3.946305274963379,
      "learning_rate": 4.567866875927683e-05,
      "loss": 0.7607,
      "step": 568300
    },
    {
      "epoch": 5.186509964230966,
      "grad_norm": 4.120181083679199,
      "learning_rate": 4.567790836314087e-05,
      "loss": 0.7908,
      "step": 568400
    },
    {
      "epoch": 5.187422439594131,
      "grad_norm": 4.005480766296387,
      "learning_rate": 4.567714796700489e-05,
      "loss": 0.7506,
      "step": 568500
    },
    {
      "epoch": 5.188334914957296,
      "grad_norm": 4.254665851593018,
      "learning_rate": 4.567638757086892e-05,
      "loss": 0.7232,
      "step": 568600
    },
    {
      "epoch": 5.189247390320461,
      "grad_norm": 3.022580623626709,
      "learning_rate": 4.567562717473295e-05,
      "loss": 0.708,
      "step": 568700
    },
    {
      "epoch": 5.1901598656836265,
      "grad_norm": 2.9768483638763428,
      "learning_rate": 4.567486677859698e-05,
      "loss": 0.6764,
      "step": 568800
    },
    {
      "epoch": 5.191072341046792,
      "grad_norm": 3.4175498485565186,
      "learning_rate": 4.567410638246101e-05,
      "loss": 0.7412,
      "step": 568900
    },
    {
      "epoch": 5.191984816409957,
      "grad_norm": 4.069815158843994,
      "learning_rate": 4.567334598632504e-05,
      "loss": 0.6897,
      "step": 569000
    },
    {
      "epoch": 5.192897291773122,
      "grad_norm": 5.60255241394043,
      "learning_rate": 4.5672585590189064e-05,
      "loss": 0.6942,
      "step": 569100
    },
    {
      "epoch": 5.193809767136288,
      "grad_norm": 3.8452987670898438,
      "learning_rate": 4.56718251940531e-05,
      "loss": 0.7017,
      "step": 569200
    },
    {
      "epoch": 5.194722242499452,
      "grad_norm": 5.292728900909424,
      "learning_rate": 4.5671064797917124e-05,
      "loss": 0.727,
      "step": 569300
    },
    {
      "epoch": 5.195634717862617,
      "grad_norm": 3.9505996704101562,
      "learning_rate": 4.5670304401781154e-05,
      "loss": 0.7108,
      "step": 569400
    },
    {
      "epoch": 5.196547193225783,
      "grad_norm": 3.427896499633789,
      "learning_rate": 4.5669544005645184e-05,
      "loss": 0.7284,
      "step": 569500
    },
    {
      "epoch": 5.197459668588948,
      "grad_norm": 4.660348415374756,
      "learning_rate": 4.5668783609509214e-05,
      "loss": 0.7073,
      "step": 569600
    },
    {
      "epoch": 5.198372143952113,
      "grad_norm": 3.7900099754333496,
      "learning_rate": 4.566802321337324e-05,
      "loss": 0.7335,
      "step": 569700
    },
    {
      "epoch": 5.199284619315279,
      "grad_norm": 4.3330559730529785,
      "learning_rate": 4.5667262817237274e-05,
      "loss": 0.6907,
      "step": 569800
    },
    {
      "epoch": 5.200197094678444,
      "grad_norm": 3.958841562271118,
      "learning_rate": 4.56665024211013e-05,
      "loss": 0.7136,
      "step": 569900
    },
    {
      "epoch": 5.201109570041609,
      "grad_norm": 4.140583515167236,
      "learning_rate": 4.566574202496533e-05,
      "loss": 0.6896,
      "step": 570000
    },
    {
      "epoch": 5.202022045404774,
      "grad_norm": 5.443333625793457,
      "learning_rate": 4.566498162882936e-05,
      "loss": 0.7184,
      "step": 570100
    },
    {
      "epoch": 5.202934520767939,
      "grad_norm": 3.708420991897583,
      "learning_rate": 4.566422123269339e-05,
      "loss": 0.7232,
      "step": 570200
    },
    {
      "epoch": 5.203846996131104,
      "grad_norm": 4.121546268463135,
      "learning_rate": 4.566346083655742e-05,
      "loss": 0.693,
      "step": 570300
    },
    {
      "epoch": 5.2047594714942695,
      "grad_norm": 3.2718451023101807,
      "learning_rate": 4.566270044042145e-05,
      "loss": 0.7248,
      "step": 570400
    },
    {
      "epoch": 5.205671946857435,
      "grad_norm": 4.509308815002441,
      "learning_rate": 4.566194004428547e-05,
      "loss": 0.7073,
      "step": 570500
    },
    {
      "epoch": 5.2065844222206,
      "grad_norm": 4.021223068237305,
      "learning_rate": 4.566117964814951e-05,
      "loss": 0.7128,
      "step": 570600
    },
    {
      "epoch": 5.207496897583765,
      "grad_norm": 4.771207809448242,
      "learning_rate": 4.566041925201353e-05,
      "loss": 0.7112,
      "step": 570700
    },
    {
      "epoch": 5.208409372946931,
      "grad_norm": 3.6292660236358643,
      "learning_rate": 4.565965885587756e-05,
      "loss": 0.7253,
      "step": 570800
    },
    {
      "epoch": 5.209321848310096,
      "grad_norm": 3.805058240890503,
      "learning_rate": 4.565889845974159e-05,
      "loss": 0.7114,
      "step": 570900
    },
    {
      "epoch": 5.2102343236732604,
      "grad_norm": 3.8039324283599854,
      "learning_rate": 4.5658138063605615e-05,
      "loss": 0.7052,
      "step": 571000
    },
    {
      "epoch": 5.211146799036426,
      "grad_norm": 4.417346000671387,
      "learning_rate": 4.5657377667469645e-05,
      "loss": 0.7142,
      "step": 571100
    },
    {
      "epoch": 5.212059274399591,
      "grad_norm": 3.7542684078216553,
      "learning_rate": 4.5656617271333675e-05,
      "loss": 0.6812,
      "step": 571200
    },
    {
      "epoch": 5.212971749762756,
      "grad_norm": 4.714979648590088,
      "learning_rate": 4.5655856875197705e-05,
      "loss": 0.6702,
      "step": 571300
    },
    {
      "epoch": 5.213884225125922,
      "grad_norm": 5.2015180587768555,
      "learning_rate": 4.5655096479061735e-05,
      "loss": 0.7337,
      "step": 571400
    },
    {
      "epoch": 5.214796700489087,
      "grad_norm": 4.912257671356201,
      "learning_rate": 4.5654336082925765e-05,
      "loss": 0.7082,
      "step": 571500
    },
    {
      "epoch": 5.215709175852252,
      "grad_norm": 3.381485939025879,
      "learning_rate": 4.565357568678979e-05,
      "loss": 0.7265,
      "step": 571600
    },
    {
      "epoch": 5.2166216512154175,
      "grad_norm": 3.7010085582733154,
      "learning_rate": 4.5652815290653825e-05,
      "loss": 0.6965,
      "step": 571700
    },
    {
      "epoch": 5.217534126578582,
      "grad_norm": 5.215860843658447,
      "learning_rate": 4.565205489451785e-05,
      "loss": 0.722,
      "step": 571800
    },
    {
      "epoch": 5.218446601941747,
      "grad_norm": 3.266711950302124,
      "learning_rate": 4.565129449838188e-05,
      "loss": 0.6891,
      "step": 571900
    },
    {
      "epoch": 5.2193590773049126,
      "grad_norm": 3.822885513305664,
      "learning_rate": 4.565053410224591e-05,
      "loss": 0.7256,
      "step": 572000
    },
    {
      "epoch": 5.220271552668078,
      "grad_norm": 3.9333155155181885,
      "learning_rate": 4.564977370610994e-05,
      "loss": 0.692,
      "step": 572100
    },
    {
      "epoch": 5.221184028031243,
      "grad_norm": 3.430025339126587,
      "learning_rate": 4.564901330997396e-05,
      "loss": 0.7067,
      "step": 572200
    },
    {
      "epoch": 5.2220965033944085,
      "grad_norm": 4.166595458984375,
      "learning_rate": 4.5648252913838e-05,
      "loss": 0.7504,
      "step": 572300
    },
    {
      "epoch": 5.223008978757574,
      "grad_norm": 4.551184177398682,
      "learning_rate": 4.564749251770202e-05,
      "loss": 0.7468,
      "step": 572400
    },
    {
      "epoch": 5.223921454120739,
      "grad_norm": 4.354397773742676,
      "learning_rate": 4.564673212156605e-05,
      "loss": 0.7505,
      "step": 572500
    },
    {
      "epoch": 5.224833929483904,
      "grad_norm": 3.8413448333740234,
      "learning_rate": 4.564597172543008e-05,
      "loss": 0.7211,
      "step": 572600
    },
    {
      "epoch": 5.225746404847069,
      "grad_norm": 4.189995765686035,
      "learning_rate": 4.564521132929411e-05,
      "loss": 0.7284,
      "step": 572700
    },
    {
      "epoch": 5.226658880210234,
      "grad_norm": 4.569332122802734,
      "learning_rate": 4.564445093315814e-05,
      "loss": 0.704,
      "step": 572800
    },
    {
      "epoch": 5.227571355573399,
      "grad_norm": 4.0912394523620605,
      "learning_rate": 4.564369053702217e-05,
      "loss": 0.7403,
      "step": 572900
    },
    {
      "epoch": 5.228483830936565,
      "grad_norm": 4.271566390991211,
      "learning_rate": 4.5642930140886196e-05,
      "loss": 0.6972,
      "step": 573000
    },
    {
      "epoch": 5.22939630629973,
      "grad_norm": 4.43646764755249,
      "learning_rate": 4.564216974475023e-05,
      "loss": 0.7141,
      "step": 573100
    },
    {
      "epoch": 5.230308781662895,
      "grad_norm": 3.7579498291015625,
      "learning_rate": 4.5641409348614256e-05,
      "loss": 0.7254,
      "step": 573200
    },
    {
      "epoch": 5.231221257026061,
      "grad_norm": 4.334387302398682,
      "learning_rate": 4.5640648952478286e-05,
      "loss": 0.6998,
      "step": 573300
    },
    {
      "epoch": 5.232133732389226,
      "grad_norm": 4.759195804595947,
      "learning_rate": 4.5639888556342316e-05,
      "loss": 0.6948,
      "step": 573400
    },
    {
      "epoch": 5.23304620775239,
      "grad_norm": 3.125824213027954,
      "learning_rate": 4.5639128160206346e-05,
      "loss": 0.7075,
      "step": 573500
    },
    {
      "epoch": 5.233958683115556,
      "grad_norm": 3.7231526374816895,
      "learning_rate": 4.563836776407037e-05,
      "loss": 0.6908,
      "step": 573600
    },
    {
      "epoch": 5.234871158478721,
      "grad_norm": 3.738466501235962,
      "learning_rate": 4.5637607367934406e-05,
      "loss": 0.7387,
      "step": 573700
    },
    {
      "epoch": 5.235783633841886,
      "grad_norm": 4.195945739746094,
      "learning_rate": 4.563684697179843e-05,
      "loss": 0.6769,
      "step": 573800
    },
    {
      "epoch": 5.2366961092050515,
      "grad_norm": 4.905734539031982,
      "learning_rate": 4.563608657566246e-05,
      "loss": 0.7057,
      "step": 573900
    },
    {
      "epoch": 5.237608584568217,
      "grad_norm": 4.278876781463623,
      "learning_rate": 4.563532617952649e-05,
      "loss": 0.7117,
      "step": 574000
    },
    {
      "epoch": 5.238521059931382,
      "grad_norm": 4.591719150543213,
      "learning_rate": 4.563456578339051e-05,
      "loss": 0.7276,
      "step": 574100
    },
    {
      "epoch": 5.239433535294547,
      "grad_norm": 3.4474308490753174,
      "learning_rate": 4.563380538725455e-05,
      "loss": 0.7272,
      "step": 574200
    },
    {
      "epoch": 5.240346010657713,
      "grad_norm": 3.3316471576690674,
      "learning_rate": 4.563304499111857e-05,
      "loss": 0.7018,
      "step": 574300
    },
    {
      "epoch": 5.241258486020877,
      "grad_norm": 3.553666591644287,
      "learning_rate": 4.56322845949826e-05,
      "loss": 0.665,
      "step": 574400
    },
    {
      "epoch": 5.242170961384042,
      "grad_norm": 3.8889806270599365,
      "learning_rate": 4.563152419884663e-05,
      "loss": 0.6908,
      "step": 574500
    },
    {
      "epoch": 5.243083436747208,
      "grad_norm": 3.822035789489746,
      "learning_rate": 4.563076380271066e-05,
      "loss": 0.72,
      "step": 574600
    },
    {
      "epoch": 5.243995912110373,
      "grad_norm": 3.6789259910583496,
      "learning_rate": 4.5630003406574686e-05,
      "loss": 0.6931,
      "step": 574700
    },
    {
      "epoch": 5.244908387473538,
      "grad_norm": 4.358872890472412,
      "learning_rate": 4.562924301043872e-05,
      "loss": 0.6994,
      "step": 574800
    },
    {
      "epoch": 5.245820862836704,
      "grad_norm": 4.239511966705322,
      "learning_rate": 4.562848261430275e-05,
      "loss": 0.6852,
      "step": 574900
    },
    {
      "epoch": 5.246733338199869,
      "grad_norm": 3.894727945327759,
      "learning_rate": 4.562772221816678e-05,
      "loss": 0.7463,
      "step": 575000
    },
    {
      "epoch": 5.247645813563034,
      "grad_norm": 3.6177351474761963,
      "learning_rate": 4.562696182203081e-05,
      "loss": 0.6956,
      "step": 575100
    },
    {
      "epoch": 5.248558288926199,
      "grad_norm": 4.213908672332764,
      "learning_rate": 4.562620142589484e-05,
      "loss": 0.7223,
      "step": 575200
    },
    {
      "epoch": 5.249470764289364,
      "grad_norm": 3.806938648223877,
      "learning_rate": 4.562544102975887e-05,
      "loss": 0.7473,
      "step": 575300
    },
    {
      "epoch": 5.250383239652529,
      "grad_norm": 3.7927169799804688,
      "learning_rate": 4.56246806336229e-05,
      "loss": 0.7039,
      "step": 575400
    },
    {
      "epoch": 5.2512957150156945,
      "grad_norm": 3.936006784439087,
      "learning_rate": 4.562392023748692e-05,
      "loss": 0.7003,
      "step": 575500
    },
    {
      "epoch": 5.25220819037886,
      "grad_norm": 4.127610683441162,
      "learning_rate": 4.562315984135096e-05,
      "loss": 0.7043,
      "step": 575600
    },
    {
      "epoch": 5.253120665742025,
      "grad_norm": 5.633009433746338,
      "learning_rate": 4.562239944521498e-05,
      "loss": 0.7191,
      "step": 575700
    },
    {
      "epoch": 5.25403314110519,
      "grad_norm": 4.357577323913574,
      "learning_rate": 4.562163904907901e-05,
      "loss": 0.6689,
      "step": 575800
    },
    {
      "epoch": 5.254945616468356,
      "grad_norm": 3.9161603450775146,
      "learning_rate": 4.562087865294304e-05,
      "loss": 0.7323,
      "step": 575900
    },
    {
      "epoch": 5.25585809183152,
      "grad_norm": 3.9377589225769043,
      "learning_rate": 4.562011825680707e-05,
      "loss": 0.6851,
      "step": 576000
    },
    {
      "epoch": 5.256770567194685,
      "grad_norm": 5.155007839202881,
      "learning_rate": 4.5619357860671094e-05,
      "loss": 0.6884,
      "step": 576100
    },
    {
      "epoch": 5.257683042557851,
      "grad_norm": 3.8581511974334717,
      "learning_rate": 4.561859746453513e-05,
      "loss": 0.7102,
      "step": 576200
    },
    {
      "epoch": 5.258595517921016,
      "grad_norm": 4.463922023773193,
      "learning_rate": 4.5617837068399154e-05,
      "loss": 0.7106,
      "step": 576300
    },
    {
      "epoch": 5.259507993284181,
      "grad_norm": 3.728774309158325,
      "learning_rate": 4.5617076672263184e-05,
      "loss": 0.7025,
      "step": 576400
    },
    {
      "epoch": 5.260420468647347,
      "grad_norm": 4.2920122146606445,
      "learning_rate": 4.5616316276127214e-05,
      "loss": 0.7242,
      "step": 576500
    },
    {
      "epoch": 5.261332944010512,
      "grad_norm": 4.26301908493042,
      "learning_rate": 4.561555587999124e-05,
      "loss": 0.6966,
      "step": 576600
    },
    {
      "epoch": 5.262245419373677,
      "grad_norm": 4.321219444274902,
      "learning_rate": 4.5614795483855274e-05,
      "loss": 0.7115,
      "step": 576700
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 3.8565104007720947,
      "learning_rate": 4.56140350877193e-05,
      "loss": 0.6577,
      "step": 576800
    },
    {
      "epoch": 5.264070370100007,
      "grad_norm": 4.406994819641113,
      "learning_rate": 4.561327469158333e-05,
      "loss": 0.7563,
      "step": 576900
    },
    {
      "epoch": 5.264982845463172,
      "grad_norm": 4.3472161293029785,
      "learning_rate": 4.561251429544736e-05,
      "loss": 0.7495,
      "step": 577000
    },
    {
      "epoch": 5.2658953208263375,
      "grad_norm": 3.452620506286621,
      "learning_rate": 4.561175389931139e-05,
      "loss": 0.7052,
      "step": 577100
    },
    {
      "epoch": 5.266807796189503,
      "grad_norm": 4.2861785888671875,
      "learning_rate": 4.561099350317541e-05,
      "loss": 0.7087,
      "step": 577200
    },
    {
      "epoch": 5.267720271552668,
      "grad_norm": 4.0711140632629395,
      "learning_rate": 4.561023310703945e-05,
      "loss": 0.7719,
      "step": 577300
    },
    {
      "epoch": 5.268632746915833,
      "grad_norm": 3.4592630863189697,
      "learning_rate": 4.560947271090347e-05,
      "loss": 0.684,
      "step": 577400
    },
    {
      "epoch": 5.269545222278999,
      "grad_norm": 2.8378164768218994,
      "learning_rate": 4.56087123147675e-05,
      "loss": 0.678,
      "step": 577500
    },
    {
      "epoch": 5.270457697642164,
      "grad_norm": 4.274587631225586,
      "learning_rate": 4.560795191863153e-05,
      "loss": 0.7369,
      "step": 577600
    },
    {
      "epoch": 5.271370173005328,
      "grad_norm": 3.9666523933410645,
      "learning_rate": 4.560719152249556e-05,
      "loss": 0.7282,
      "step": 577700
    },
    {
      "epoch": 5.272282648368494,
      "grad_norm": 4.062260150909424,
      "learning_rate": 4.560643112635959e-05,
      "loss": 0.7072,
      "step": 577800
    },
    {
      "epoch": 5.273195123731659,
      "grad_norm": 4.4071478843688965,
      "learning_rate": 4.560567073022362e-05,
      "loss": 0.6946,
      "step": 577900
    },
    {
      "epoch": 5.274107599094824,
      "grad_norm": 3.345099925994873,
      "learning_rate": 4.5604910334087645e-05,
      "loss": 0.7294,
      "step": 578000
    },
    {
      "epoch": 5.27502007445799,
      "grad_norm": 3.9302992820739746,
      "learning_rate": 4.560414993795168e-05,
      "loss": 0.6884,
      "step": 578100
    },
    {
      "epoch": 5.275932549821155,
      "grad_norm": 3.3639612197875977,
      "learning_rate": 4.5603389541815705e-05,
      "loss": 0.7112,
      "step": 578200
    },
    {
      "epoch": 5.27684502518432,
      "grad_norm": 3.690534830093384,
      "learning_rate": 4.5602629145679735e-05,
      "loss": 0.7597,
      "step": 578300
    },
    {
      "epoch": 5.2777575005474855,
      "grad_norm": 3.497965097427368,
      "learning_rate": 4.5601868749543765e-05,
      "loss": 0.6736,
      "step": 578400
    },
    {
      "epoch": 5.278669975910651,
      "grad_norm": 3.2903809547424316,
      "learning_rate": 4.5601108353407795e-05,
      "loss": 0.7137,
      "step": 578500
    },
    {
      "epoch": 5.279582451273815,
      "grad_norm": 3.767732620239258,
      "learning_rate": 4.560034795727182e-05,
      "loss": 0.6918,
      "step": 578600
    },
    {
      "epoch": 5.2804949266369805,
      "grad_norm": 4.593026161193848,
      "learning_rate": 4.5599587561135855e-05,
      "loss": 0.6922,
      "step": 578700
    },
    {
      "epoch": 5.281407402000146,
      "grad_norm": 3.3335893154144287,
      "learning_rate": 4.559882716499988e-05,
      "loss": 0.7382,
      "step": 578800
    },
    {
      "epoch": 5.282319877363311,
      "grad_norm": 4.459598541259766,
      "learning_rate": 4.559806676886391e-05,
      "loss": 0.7209,
      "step": 578900
    },
    {
      "epoch": 5.283232352726476,
      "grad_norm": 4.316257476806641,
      "learning_rate": 4.559730637272794e-05,
      "loss": 0.715,
      "step": 579000
    },
    {
      "epoch": 5.284144828089642,
      "grad_norm": 3.246535301208496,
      "learning_rate": 4.559654597659197e-05,
      "loss": 0.6907,
      "step": 579100
    },
    {
      "epoch": 5.285057303452807,
      "grad_norm": 3.9801275730133057,
      "learning_rate": 4.5595785580456e-05,
      "loss": 0.7277,
      "step": 579200
    },
    {
      "epoch": 5.285969778815972,
      "grad_norm": 3.795923948287964,
      "learning_rate": 4.559502518432003e-05,
      "loss": 0.6785,
      "step": 579300
    },
    {
      "epoch": 5.286882254179137,
      "grad_norm": 4.7646074295043945,
      "learning_rate": 4.559426478818405e-05,
      "loss": 0.6965,
      "step": 579400
    },
    {
      "epoch": 5.287794729542302,
      "grad_norm": 4.660269260406494,
      "learning_rate": 4.559350439204808e-05,
      "loss": 0.7037,
      "step": 579500
    },
    {
      "epoch": 5.288707204905467,
      "grad_norm": 3.964517831802368,
      "learning_rate": 4.559274399591211e-05,
      "loss": 0.7264,
      "step": 579600
    },
    {
      "epoch": 5.289619680268633,
      "grad_norm": 3.572152853012085,
      "learning_rate": 4.559198359977614e-05,
      "loss": 0.7388,
      "step": 579700
    },
    {
      "epoch": 5.290532155631798,
      "grad_norm": 4.446842670440674,
      "learning_rate": 4.559122320364017e-05,
      "loss": 0.6846,
      "step": 579800
    },
    {
      "epoch": 5.291444630994963,
      "grad_norm": 4.0663981437683105,
      "learning_rate": 4.5590462807504196e-05,
      "loss": 0.7118,
      "step": 579900
    },
    {
      "epoch": 5.2923571063581285,
      "grad_norm": 2.667846202850342,
      "learning_rate": 4.5589702411368226e-05,
      "loss": 0.7049,
      "step": 580000
    },
    {
      "epoch": 5.293269581721294,
      "grad_norm": 5.057051181793213,
      "learning_rate": 4.5588942015232256e-05,
      "loss": 0.6963,
      "step": 580100
    },
    {
      "epoch": 5.294182057084459,
      "grad_norm": 4.225252151489258,
      "learning_rate": 4.5588181619096286e-05,
      "loss": 0.6933,
      "step": 580200
    },
    {
      "epoch": 5.2950945324476235,
      "grad_norm": 4.580004692077637,
      "learning_rate": 4.5587421222960316e-05,
      "loss": 0.7094,
      "step": 580300
    },
    {
      "epoch": 5.296007007810789,
      "grad_norm": 4.084142684936523,
      "learning_rate": 4.5586660826824346e-05,
      "loss": 0.7257,
      "step": 580400
    },
    {
      "epoch": 5.296919483173954,
      "grad_norm": 3.350614070892334,
      "learning_rate": 4.558590043068837e-05,
      "loss": 0.7266,
      "step": 580500
    },
    {
      "epoch": 5.297831958537119,
      "grad_norm": 4.212470531463623,
      "learning_rate": 4.5585140034552406e-05,
      "loss": 0.7331,
      "step": 580600
    },
    {
      "epoch": 5.298744433900285,
      "grad_norm": 3.968984365463257,
      "learning_rate": 4.558437963841643e-05,
      "loss": 0.7122,
      "step": 580700
    },
    {
      "epoch": 5.29965690926345,
      "grad_norm": 4.2783522605896,
      "learning_rate": 4.558361924228046e-05,
      "loss": 0.719,
      "step": 580800
    },
    {
      "epoch": 5.300569384626615,
      "grad_norm": 5.180344104766846,
      "learning_rate": 4.558285884614449e-05,
      "loss": 0.7464,
      "step": 580900
    },
    {
      "epoch": 5.301481859989781,
      "grad_norm": 5.074857234954834,
      "learning_rate": 4.558209845000852e-05,
      "loss": 0.6816,
      "step": 581000
    },
    {
      "epoch": 5.302394335352945,
      "grad_norm": 4.788300037384033,
      "learning_rate": 4.558133805387255e-05,
      "loss": 0.7453,
      "step": 581100
    },
    {
      "epoch": 5.30330681071611,
      "grad_norm": 4.200211048126221,
      "learning_rate": 4.558057765773658e-05,
      "loss": 0.7372,
      "step": 581200
    },
    {
      "epoch": 5.304219286079276,
      "grad_norm": 3.1018874645233154,
      "learning_rate": 4.55798172616006e-05,
      "loss": 0.7275,
      "step": 581300
    },
    {
      "epoch": 5.305131761442441,
      "grad_norm": 3.8039655685424805,
      "learning_rate": 4.557905686546463e-05,
      "loss": 0.6775,
      "step": 581400
    },
    {
      "epoch": 5.306044236805606,
      "grad_norm": 3.8118669986724854,
      "learning_rate": 4.557829646932866e-05,
      "loss": 0.7158,
      "step": 581500
    },
    {
      "epoch": 5.3069567121687715,
      "grad_norm": 4.644460201263428,
      "learning_rate": 4.557753607319269e-05,
      "loss": 0.7095,
      "step": 581600
    },
    {
      "epoch": 5.307869187531937,
      "grad_norm": 4.620859622955322,
      "learning_rate": 4.5576775677056723e-05,
      "loss": 0.654,
      "step": 581700
    },
    {
      "epoch": 5.308781662895102,
      "grad_norm": 4.716005325317383,
      "learning_rate": 4.5576015280920754e-05,
      "loss": 0.7713,
      "step": 581800
    },
    {
      "epoch": 5.309694138258267,
      "grad_norm": 4.01810884475708,
      "learning_rate": 4.557525488478478e-05,
      "loss": 0.7409,
      "step": 581900
    },
    {
      "epoch": 5.310606613621432,
      "grad_norm": 4.712665557861328,
      "learning_rate": 4.5574494488648814e-05,
      "loss": 0.6984,
      "step": 582000
    },
    {
      "epoch": 5.311519088984597,
      "grad_norm": 4.393399238586426,
      "learning_rate": 4.557373409251284e-05,
      "loss": 0.7373,
      "step": 582100
    },
    {
      "epoch": 5.312431564347762,
      "grad_norm": 6.508879661560059,
      "learning_rate": 4.557297369637687e-05,
      "loss": 0.7102,
      "step": 582200
    },
    {
      "epoch": 5.313344039710928,
      "grad_norm": 4.069300651550293,
      "learning_rate": 4.55722133002409e-05,
      "loss": 0.7178,
      "step": 582300
    },
    {
      "epoch": 5.314256515074093,
      "grad_norm": 3.4947731494903564,
      "learning_rate": 4.557145290410492e-05,
      "loss": 0.7002,
      "step": 582400
    },
    {
      "epoch": 5.315168990437258,
      "grad_norm": 3.894758939743042,
      "learning_rate": 4.557069250796896e-05,
      "loss": 0.731,
      "step": 582500
    },
    {
      "epoch": 5.316081465800424,
      "grad_norm": 4.520379066467285,
      "learning_rate": 4.556993211183298e-05,
      "loss": 0.7111,
      "step": 582600
    },
    {
      "epoch": 5.316993941163589,
      "grad_norm": 3.6073875427246094,
      "learning_rate": 4.556917171569701e-05,
      "loss": 0.6958,
      "step": 582700
    },
    {
      "epoch": 5.317906416526753,
      "grad_norm": 3.7186310291290283,
      "learning_rate": 4.556841131956104e-05,
      "loss": 0.7019,
      "step": 582800
    },
    {
      "epoch": 5.318818891889919,
      "grad_norm": 3.5217342376708984,
      "learning_rate": 4.556765092342507e-05,
      "loss": 0.6663,
      "step": 582900
    },
    {
      "epoch": 5.319731367253084,
      "grad_norm": 3.9202935695648193,
      "learning_rate": 4.5566890527289094e-05,
      "loss": 0.7381,
      "step": 583000
    },
    {
      "epoch": 5.320643842616249,
      "grad_norm": 4.120860576629639,
      "learning_rate": 4.556613013115313e-05,
      "loss": 0.7342,
      "step": 583100
    },
    {
      "epoch": 5.3215563179794145,
      "grad_norm": 3.316072702407837,
      "learning_rate": 4.5565369735017154e-05,
      "loss": 0.7285,
      "step": 583200
    },
    {
      "epoch": 5.32246879334258,
      "grad_norm": 3.5712475776672363,
      "learning_rate": 4.5564609338881184e-05,
      "loss": 0.727,
      "step": 583300
    },
    {
      "epoch": 5.323381268705745,
      "grad_norm": 3.945929765701294,
      "learning_rate": 4.5563848942745214e-05,
      "loss": 0.7,
      "step": 583400
    },
    {
      "epoch": 5.32429374406891,
      "grad_norm": 3.661452054977417,
      "learning_rate": 4.5563088546609244e-05,
      "loss": 0.6989,
      "step": 583500
    },
    {
      "epoch": 5.325206219432076,
      "grad_norm": 3.7108030319213867,
      "learning_rate": 4.5562328150473274e-05,
      "loss": 0.6881,
      "step": 583600
    },
    {
      "epoch": 5.32611869479524,
      "grad_norm": 4.0432538986206055,
      "learning_rate": 4.5561567754337304e-05,
      "loss": 0.6854,
      "step": 583700
    },
    {
      "epoch": 5.327031170158405,
      "grad_norm": 4.570216655731201,
      "learning_rate": 4.556080735820133e-05,
      "loss": 0.678,
      "step": 583800
    },
    {
      "epoch": 5.327943645521571,
      "grad_norm": 3.7053329944610596,
      "learning_rate": 4.5560046962065365e-05,
      "loss": 0.7659,
      "step": 583900
    },
    {
      "epoch": 5.328856120884736,
      "grad_norm": 3.472687244415283,
      "learning_rate": 4.555928656592939e-05,
      "loss": 0.7157,
      "step": 584000
    },
    {
      "epoch": 5.329768596247901,
      "grad_norm": 4.149516582489014,
      "learning_rate": 4.555852616979342e-05,
      "loss": 0.6893,
      "step": 584100
    },
    {
      "epoch": 5.330681071611067,
      "grad_norm": 4.417212963104248,
      "learning_rate": 4.555776577365745e-05,
      "loss": 0.7379,
      "step": 584200
    },
    {
      "epoch": 5.331593546974232,
      "grad_norm": 5.013735294342041,
      "learning_rate": 4.555700537752148e-05,
      "loss": 0.7085,
      "step": 584300
    },
    {
      "epoch": 5.332506022337397,
      "grad_norm": 4.0964202880859375,
      "learning_rate": 4.55562449813855e-05,
      "loss": 0.6952,
      "step": 584400
    },
    {
      "epoch": 5.333418497700562,
      "grad_norm": 3.9256651401519775,
      "learning_rate": 4.555548458524954e-05,
      "loss": 0.7231,
      "step": 584500
    },
    {
      "epoch": 5.334330973063727,
      "grad_norm": 4.578510761260986,
      "learning_rate": 4.555472418911356e-05,
      "loss": 0.7153,
      "step": 584600
    },
    {
      "epoch": 5.335243448426892,
      "grad_norm": 4.06133508682251,
      "learning_rate": 4.555396379297759e-05,
      "loss": 0.7101,
      "step": 584700
    },
    {
      "epoch": 5.3361559237900575,
      "grad_norm": 4.862227916717529,
      "learning_rate": 4.555320339684162e-05,
      "loss": 0.6985,
      "step": 584800
    },
    {
      "epoch": 5.337068399153223,
      "grad_norm": 3.4960296154022217,
      "learning_rate": 4.555244300070565e-05,
      "loss": 0.7176,
      "step": 584900
    },
    {
      "epoch": 5.337980874516388,
      "grad_norm": 4.5462799072265625,
      "learning_rate": 4.555168260456968e-05,
      "loss": 0.7415,
      "step": 585000
    },
    {
      "epoch": 5.338893349879553,
      "grad_norm": 4.692801475524902,
      "learning_rate": 4.5550922208433705e-05,
      "loss": 0.6967,
      "step": 585100
    },
    {
      "epoch": 5.339805825242719,
      "grad_norm": 3.47585129737854,
      "learning_rate": 4.5550161812297735e-05,
      "loss": 0.6809,
      "step": 585200
    },
    {
      "epoch": 5.340718300605884,
      "grad_norm": 4.5628180503845215,
      "learning_rate": 4.5549401416161765e-05,
      "loss": 0.691,
      "step": 585300
    },
    {
      "epoch": 5.3416307759690485,
      "grad_norm": 4.212802410125732,
      "learning_rate": 4.5548641020025795e-05,
      "loss": 0.7268,
      "step": 585400
    },
    {
      "epoch": 5.342543251332214,
      "grad_norm": 4.9560227394104,
      "learning_rate": 4.554788062388982e-05,
      "loss": 0.7131,
      "step": 585500
    },
    {
      "epoch": 5.343455726695379,
      "grad_norm": 3.517159938812256,
      "learning_rate": 4.5547120227753855e-05,
      "loss": 0.7156,
      "step": 585600
    },
    {
      "epoch": 5.344368202058544,
      "grad_norm": 3.83719801902771,
      "learning_rate": 4.554635983161788e-05,
      "loss": 0.7213,
      "step": 585700
    },
    {
      "epoch": 5.34528067742171,
      "grad_norm": 3.187329053878784,
      "learning_rate": 4.554559943548191e-05,
      "loss": 0.7022,
      "step": 585800
    },
    {
      "epoch": 5.346193152784875,
      "grad_norm": 4.898430347442627,
      "learning_rate": 4.554483903934594e-05,
      "loss": 0.7216,
      "step": 585900
    },
    {
      "epoch": 5.34710562814804,
      "grad_norm": 3.7617743015289307,
      "learning_rate": 4.554407864320997e-05,
      "loss": 0.7383,
      "step": 586000
    },
    {
      "epoch": 5.3480181035112055,
      "grad_norm": 3.530576229095459,
      "learning_rate": 4.5543318247074e-05,
      "loss": 0.7126,
      "step": 586100
    },
    {
      "epoch": 5.34893057887437,
      "grad_norm": 3.5318100452423096,
      "learning_rate": 4.554255785093803e-05,
      "loss": 0.6921,
      "step": 586200
    },
    {
      "epoch": 5.349843054237535,
      "grad_norm": 4.818366050720215,
      "learning_rate": 4.554179745480205e-05,
      "loss": 0.735,
      "step": 586300
    },
    {
      "epoch": 5.3507555296007006,
      "grad_norm": 4.106106758117676,
      "learning_rate": 4.554103705866609e-05,
      "loss": 0.6962,
      "step": 586400
    },
    {
      "epoch": 5.351668004963866,
      "grad_norm": 4.273547172546387,
      "learning_rate": 4.554027666253011e-05,
      "loss": 0.6783,
      "step": 586500
    },
    {
      "epoch": 5.352580480327031,
      "grad_norm": 4.625733375549316,
      "learning_rate": 4.553951626639414e-05,
      "loss": 0.7171,
      "step": 586600
    },
    {
      "epoch": 5.3534929556901965,
      "grad_norm": 3.3849709033966064,
      "learning_rate": 4.553875587025817e-05,
      "loss": 0.7124,
      "step": 586700
    },
    {
      "epoch": 5.354405431053362,
      "grad_norm": 3.918679714202881,
      "learning_rate": 4.55379954741222e-05,
      "loss": 0.6986,
      "step": 586800
    },
    {
      "epoch": 5.355317906416527,
      "grad_norm": 4.203017711639404,
      "learning_rate": 4.5537235077986226e-05,
      "loss": 0.7066,
      "step": 586900
    },
    {
      "epoch": 5.356230381779692,
      "grad_norm": 3.1137921810150146,
      "learning_rate": 4.553647468185026e-05,
      "loss": 0.6922,
      "step": 587000
    },
    {
      "epoch": 5.357142857142857,
      "grad_norm": 3.841928243637085,
      "learning_rate": 4.5535714285714286e-05,
      "loss": 0.7276,
      "step": 587100
    },
    {
      "epoch": 5.358055332506022,
      "grad_norm": 3.8417813777923584,
      "learning_rate": 4.5534953889578316e-05,
      "loss": 0.7089,
      "step": 587200
    },
    {
      "epoch": 5.358967807869187,
      "grad_norm": 3.3892886638641357,
      "learning_rate": 4.5534193493442346e-05,
      "loss": 0.7077,
      "step": 587300
    },
    {
      "epoch": 5.359880283232353,
      "grad_norm": 4.553450107574463,
      "learning_rate": 4.5533433097306376e-05,
      "loss": 0.7447,
      "step": 587400
    },
    {
      "epoch": 5.360792758595518,
      "grad_norm": 4.316464900970459,
      "learning_rate": 4.5532672701170406e-05,
      "loss": 0.7303,
      "step": 587500
    },
    {
      "epoch": 5.361705233958683,
      "grad_norm": 4.038400650024414,
      "learning_rate": 4.5531912305034436e-05,
      "loss": 0.6947,
      "step": 587600
    },
    {
      "epoch": 5.362617709321849,
      "grad_norm": 3.926333427429199,
      "learning_rate": 4.553115190889846e-05,
      "loss": 0.7248,
      "step": 587700
    },
    {
      "epoch": 5.363530184685014,
      "grad_norm": 3.8197338581085205,
      "learning_rate": 4.5530391512762497e-05,
      "loss": 0.7298,
      "step": 587800
    },
    {
      "epoch": 5.364442660048178,
      "grad_norm": 3.3014495372772217,
      "learning_rate": 4.552963111662652e-05,
      "loss": 0.6993,
      "step": 587900
    },
    {
      "epoch": 5.365355135411344,
      "grad_norm": 4.795527458190918,
      "learning_rate": 4.552887072049054e-05,
      "loss": 0.6901,
      "step": 588000
    },
    {
      "epoch": 5.366267610774509,
      "grad_norm": 4.059646129608154,
      "learning_rate": 4.552811032435458e-05,
      "loss": 0.728,
      "step": 588100
    },
    {
      "epoch": 5.367180086137674,
      "grad_norm": 4.519259929656982,
      "learning_rate": 4.55273499282186e-05,
      "loss": 0.7133,
      "step": 588200
    },
    {
      "epoch": 5.3680925615008395,
      "grad_norm": 3.772430419921875,
      "learning_rate": 4.552658953208263e-05,
      "loss": 0.7127,
      "step": 588300
    },
    {
      "epoch": 5.369005036864005,
      "grad_norm": 4.753520965576172,
      "learning_rate": 4.5525829135946663e-05,
      "loss": 0.6781,
      "step": 588400
    },
    {
      "epoch": 5.36991751222717,
      "grad_norm": 3.9254727363586426,
      "learning_rate": 4.5525068739810693e-05,
      "loss": 0.6995,
      "step": 588500
    },
    {
      "epoch": 5.370829987590335,
      "grad_norm": 4.283631801605225,
      "learning_rate": 4.5524308343674724e-05,
      "loss": 0.7429,
      "step": 588600
    },
    {
      "epoch": 5.371742462953501,
      "grad_norm": 3.72385311126709,
      "learning_rate": 4.5523547947538754e-05,
      "loss": 0.6771,
      "step": 588700
    },
    {
      "epoch": 5.372654938316665,
      "grad_norm": 5.366833209991455,
      "learning_rate": 4.552278755140278e-05,
      "loss": 0.7247,
      "step": 588800
    },
    {
      "epoch": 5.37356741367983,
      "grad_norm": 4.54481315612793,
      "learning_rate": 4.5522027155266814e-05,
      "loss": 0.6761,
      "step": 588900
    },
    {
      "epoch": 5.374479889042996,
      "grad_norm": 4.079497814178467,
      "learning_rate": 4.552126675913084e-05,
      "loss": 0.6869,
      "step": 589000
    },
    {
      "epoch": 5.375392364406161,
      "grad_norm": 3.5403990745544434,
      "learning_rate": 4.552050636299487e-05,
      "loss": 0.7399,
      "step": 589100
    },
    {
      "epoch": 5.376304839769326,
      "grad_norm": 3.5981497764587402,
      "learning_rate": 4.55197459668589e-05,
      "loss": 0.6828,
      "step": 589200
    },
    {
      "epoch": 5.377217315132492,
      "grad_norm": 3.31252384185791,
      "learning_rate": 4.551898557072293e-05,
      "loss": 0.66,
      "step": 589300
    },
    {
      "epoch": 5.378129790495657,
      "grad_norm": 4.666115760803223,
      "learning_rate": 4.551822517458695e-05,
      "loss": 0.7241,
      "step": 589400
    },
    {
      "epoch": 5.379042265858822,
      "grad_norm": 4.6326704025268555,
      "learning_rate": 4.551746477845099e-05,
      "loss": 0.7154,
      "step": 589500
    },
    {
      "epoch": 5.379954741221987,
      "grad_norm": 4.217424392700195,
      "learning_rate": 4.551670438231501e-05,
      "loss": 0.724,
      "step": 589600
    },
    {
      "epoch": 5.380867216585152,
      "grad_norm": 3.8662734031677246,
      "learning_rate": 4.551594398617904e-05,
      "loss": 0.7007,
      "step": 589700
    },
    {
      "epoch": 5.381779691948317,
      "grad_norm": 4.280468940734863,
      "learning_rate": 4.551518359004307e-05,
      "loss": 0.7132,
      "step": 589800
    },
    {
      "epoch": 5.3826921673114825,
      "grad_norm": 4.350102424621582,
      "learning_rate": 4.55144231939071e-05,
      "loss": 0.6907,
      "step": 589900
    },
    {
      "epoch": 5.383604642674648,
      "grad_norm": 3.213134288787842,
      "learning_rate": 4.551366279777113e-05,
      "loss": 0.7157,
      "step": 590000
    },
    {
      "epoch": 5.384517118037813,
      "grad_norm": 4.239843845367432,
      "learning_rate": 4.551290240163516e-05,
      "loss": 0.6924,
      "step": 590100
    },
    {
      "epoch": 5.385429593400978,
      "grad_norm": 3.8154706954956055,
      "learning_rate": 4.5512142005499184e-05,
      "loss": 0.7552,
      "step": 590200
    },
    {
      "epoch": 5.386342068764144,
      "grad_norm": 3.5172643661499023,
      "learning_rate": 4.551138160936322e-05,
      "loss": 0.7151,
      "step": 590300
    },
    {
      "epoch": 5.387254544127309,
      "grad_norm": 3.8239028453826904,
      "learning_rate": 4.5510621213227244e-05,
      "loss": 0.7321,
      "step": 590400
    },
    {
      "epoch": 5.388167019490473,
      "grad_norm": 3.9470131397247314,
      "learning_rate": 4.5509860817091274e-05,
      "loss": 0.6959,
      "step": 590500
    },
    {
      "epoch": 5.389079494853639,
      "grad_norm": 3.9783968925476074,
      "learning_rate": 4.5509100420955305e-05,
      "loss": 0.6889,
      "step": 590600
    },
    {
      "epoch": 5.389991970216804,
      "grad_norm": 3.3107354640960693,
      "learning_rate": 4.5508340024819335e-05,
      "loss": 0.7389,
      "step": 590700
    },
    {
      "epoch": 5.390904445579969,
      "grad_norm": 4.567251205444336,
      "learning_rate": 4.550757962868336e-05,
      "loss": 0.6824,
      "step": 590800
    },
    {
      "epoch": 5.391816920943135,
      "grad_norm": 4.581233978271484,
      "learning_rate": 4.550681923254739e-05,
      "loss": 0.6939,
      "step": 590900
    },
    {
      "epoch": 5.3927293963063,
      "grad_norm": 3.4708566665649414,
      "learning_rate": 4.550605883641142e-05,
      "loss": 0.7171,
      "step": 591000
    },
    {
      "epoch": 5.393641871669465,
      "grad_norm": 3.134722948074341,
      "learning_rate": 4.550529844027545e-05,
      "loss": 0.7188,
      "step": 591100
    },
    {
      "epoch": 5.3945543470326305,
      "grad_norm": 4.366696834564209,
      "learning_rate": 4.550453804413948e-05,
      "loss": 0.704,
      "step": 591200
    },
    {
      "epoch": 5.395466822395795,
      "grad_norm": 4.276738166809082,
      "learning_rate": 4.55037776480035e-05,
      "loss": 0.6672,
      "step": 591300
    },
    {
      "epoch": 5.39637929775896,
      "grad_norm": 4.016833305358887,
      "learning_rate": 4.550301725186754e-05,
      "loss": 0.7239,
      "step": 591400
    },
    {
      "epoch": 5.3972917731221255,
      "grad_norm": 4.242523670196533,
      "learning_rate": 4.550225685573156e-05,
      "loss": 0.7392,
      "step": 591500
    },
    {
      "epoch": 5.398204248485291,
      "grad_norm": 5.073846340179443,
      "learning_rate": 4.550149645959559e-05,
      "loss": 0.7144,
      "step": 591600
    },
    {
      "epoch": 5.399116723848456,
      "grad_norm": 4.782216548919678,
      "learning_rate": 4.550073606345962e-05,
      "loss": 0.7109,
      "step": 591700
    },
    {
      "epoch": 5.400029199211621,
      "grad_norm": 4.135695934295654,
      "learning_rate": 4.549997566732365e-05,
      "loss": 0.6877,
      "step": 591800
    },
    {
      "epoch": 5.400941674574787,
      "grad_norm": 3.739473581314087,
      "learning_rate": 4.5499215271187675e-05,
      "loss": 0.7318,
      "step": 591900
    },
    {
      "epoch": 5.401854149937952,
      "grad_norm": 4.72718620300293,
      "learning_rate": 4.549845487505171e-05,
      "loss": 0.694,
      "step": 592000
    },
    {
      "epoch": 5.402766625301117,
      "grad_norm": 3.9459481239318848,
      "learning_rate": 4.5497694478915735e-05,
      "loss": 0.6936,
      "step": 592100
    },
    {
      "epoch": 5.403679100664282,
      "grad_norm": 4.693960189819336,
      "learning_rate": 4.5496934082779765e-05,
      "loss": 0.7099,
      "step": 592200
    },
    {
      "epoch": 5.404591576027447,
      "grad_norm": 4.5120744705200195,
      "learning_rate": 4.5496173686643795e-05,
      "loss": 0.7191,
      "step": 592300
    },
    {
      "epoch": 5.405504051390612,
      "grad_norm": 4.269681453704834,
      "learning_rate": 4.5495413290507825e-05,
      "loss": 0.7281,
      "step": 592400
    },
    {
      "epoch": 5.406416526753778,
      "grad_norm": 3.4821391105651855,
      "learning_rate": 4.5494652894371856e-05,
      "loss": 0.6977,
      "step": 592500
    },
    {
      "epoch": 5.407329002116943,
      "grad_norm": 4.00875186920166,
      "learning_rate": 4.5493892498235886e-05,
      "loss": 0.7562,
      "step": 592600
    },
    {
      "epoch": 5.408241477480108,
      "grad_norm": 5.119884014129639,
      "learning_rate": 4.549313210209991e-05,
      "loss": 0.7116,
      "step": 592700
    },
    {
      "epoch": 5.4091539528432735,
      "grad_norm": 3.9681365489959717,
      "learning_rate": 4.5492371705963946e-05,
      "loss": 0.6931,
      "step": 592800
    },
    {
      "epoch": 5.410066428206439,
      "grad_norm": 3.6237738132476807,
      "learning_rate": 4.549161130982797e-05,
      "loss": 0.6888,
      "step": 592900
    },
    {
      "epoch": 5.410978903569603,
      "grad_norm": 4.2428154945373535,
      "learning_rate": 4.5490850913692e-05,
      "loss": 0.7274,
      "step": 593000
    },
    {
      "epoch": 5.4118913789327685,
      "grad_norm": 3.708591938018799,
      "learning_rate": 4.549009051755603e-05,
      "loss": 0.7466,
      "step": 593100
    },
    {
      "epoch": 5.412803854295934,
      "grad_norm": 3.9311883449554443,
      "learning_rate": 4.548933012142006e-05,
      "loss": 0.7,
      "step": 593200
    },
    {
      "epoch": 5.413716329659099,
      "grad_norm": 3.806899070739746,
      "learning_rate": 4.548856972528409e-05,
      "loss": 0.7376,
      "step": 593300
    },
    {
      "epoch": 5.414628805022264,
      "grad_norm": 3.406595230102539,
      "learning_rate": 4.548780932914812e-05,
      "loss": 0.6965,
      "step": 593400
    },
    {
      "epoch": 5.41554128038543,
      "grad_norm": 3.2015469074249268,
      "learning_rate": 4.548704893301214e-05,
      "loss": 0.7377,
      "step": 593500
    },
    {
      "epoch": 5.416453755748595,
      "grad_norm": 4.0554962158203125,
      "learning_rate": 4.548628853687617e-05,
      "loss": 0.7131,
      "step": 593600
    },
    {
      "epoch": 5.41736623111176,
      "grad_norm": 4.051295757293701,
      "learning_rate": 4.54855281407402e-05,
      "loss": 0.7001,
      "step": 593700
    },
    {
      "epoch": 5.418278706474926,
      "grad_norm": 3.822073459625244,
      "learning_rate": 4.5484767744604226e-05,
      "loss": 0.6861,
      "step": 593800
    },
    {
      "epoch": 5.41919118183809,
      "grad_norm": 3.6653366088867188,
      "learning_rate": 4.548400734846826e-05,
      "loss": 0.7226,
      "step": 593900
    },
    {
      "epoch": 5.420103657201255,
      "grad_norm": 4.698753356933594,
      "learning_rate": 4.5483246952332286e-05,
      "loss": 0.6798,
      "step": 594000
    },
    {
      "epoch": 5.421016132564421,
      "grad_norm": 4.385021209716797,
      "learning_rate": 4.5482486556196316e-05,
      "loss": 0.7594,
      "step": 594100
    },
    {
      "epoch": 5.421928607927586,
      "grad_norm": 4.343277454376221,
      "learning_rate": 4.5481726160060346e-05,
      "loss": 0.74,
      "step": 594200
    },
    {
      "epoch": 5.422841083290751,
      "grad_norm": 4.418269634246826,
      "learning_rate": 4.5480965763924376e-05,
      "loss": 0.701,
      "step": 594300
    },
    {
      "epoch": 5.4237535586539165,
      "grad_norm": 3.928251266479492,
      "learning_rate": 4.5480205367788406e-05,
      "loss": 0.7443,
      "step": 594400
    },
    {
      "epoch": 5.424666034017082,
      "grad_norm": 3.9203860759735107,
      "learning_rate": 4.5479444971652437e-05,
      "loss": 0.7698,
      "step": 594500
    },
    {
      "epoch": 5.425578509380247,
      "grad_norm": 3.926161527633667,
      "learning_rate": 4.547868457551646e-05,
      "loss": 0.7247,
      "step": 594600
    },
    {
      "epoch": 5.4264909847434115,
      "grad_norm": 3.5979113578796387,
      "learning_rate": 4.54779241793805e-05,
      "loss": 0.679,
      "step": 594700
    },
    {
      "epoch": 5.427403460106577,
      "grad_norm": 4.169378757476807,
      "learning_rate": 4.547716378324452e-05,
      "loss": 0.6823,
      "step": 594800
    },
    {
      "epoch": 5.428315935469742,
      "grad_norm": 4.261292457580566,
      "learning_rate": 4.547640338710855e-05,
      "loss": 0.7081,
      "step": 594900
    },
    {
      "epoch": 5.429228410832907,
      "grad_norm": 4.100779056549072,
      "learning_rate": 4.547564299097258e-05,
      "loss": 0.7195,
      "step": 595000
    },
    {
      "epoch": 5.430140886196073,
      "grad_norm": 4.155597686767578,
      "learning_rate": 4.547488259483661e-05,
      "loss": 0.7034,
      "step": 595100
    },
    {
      "epoch": 5.431053361559238,
      "grad_norm": 3.8443922996520996,
      "learning_rate": 4.5474122198700633e-05,
      "loss": 0.7016,
      "step": 595200
    },
    {
      "epoch": 5.431965836922403,
      "grad_norm": 4.49264669418335,
      "learning_rate": 4.547336180256467e-05,
      "loss": 0.743,
      "step": 595300
    },
    {
      "epoch": 5.432878312285569,
      "grad_norm": 3.855642318725586,
      "learning_rate": 4.5472601406428694e-05,
      "loss": 0.712,
      "step": 595400
    },
    {
      "epoch": 5.433790787648734,
      "grad_norm": 4.5690202713012695,
      "learning_rate": 4.5471841010292724e-05,
      "loss": 0.723,
      "step": 595500
    },
    {
      "epoch": 5.434703263011898,
      "grad_norm": 4.279939651489258,
      "learning_rate": 4.5471080614156754e-05,
      "loss": 0.6631,
      "step": 595600
    },
    {
      "epoch": 5.435615738375064,
      "grad_norm": 3.2661261558532715,
      "learning_rate": 4.5470320218020784e-05,
      "loss": 0.7463,
      "step": 595700
    },
    {
      "epoch": 5.436528213738229,
      "grad_norm": 4.281486988067627,
      "learning_rate": 4.5469559821884814e-05,
      "loss": 0.6907,
      "step": 595800
    },
    {
      "epoch": 5.437440689101394,
      "grad_norm": 4.15492582321167,
      "learning_rate": 4.5468799425748844e-05,
      "loss": 0.6735,
      "step": 595900
    },
    {
      "epoch": 5.4383531644645595,
      "grad_norm": 4.059737682342529,
      "learning_rate": 4.546803902961287e-05,
      "loss": 0.7165,
      "step": 596000
    },
    {
      "epoch": 5.439265639827725,
      "grad_norm": 3.574697732925415,
      "learning_rate": 4.5467278633476904e-05,
      "loss": 0.7206,
      "step": 596100
    },
    {
      "epoch": 5.44017811519089,
      "grad_norm": 5.050815582275391,
      "learning_rate": 4.546651823734093e-05,
      "loss": 0.7178,
      "step": 596200
    },
    {
      "epoch": 5.441090590554055,
      "grad_norm": 3.7470321655273438,
      "learning_rate": 4.546575784120496e-05,
      "loss": 0.7156,
      "step": 596300
    },
    {
      "epoch": 5.44200306591722,
      "grad_norm": 5.060332298278809,
      "learning_rate": 4.546499744506899e-05,
      "loss": 0.6855,
      "step": 596400
    },
    {
      "epoch": 5.442915541280385,
      "grad_norm": 3.771744966506958,
      "learning_rate": 4.546423704893301e-05,
      "loss": 0.6957,
      "step": 596500
    },
    {
      "epoch": 5.44382801664355,
      "grad_norm": 3.3055896759033203,
      "learning_rate": 4.546347665279704e-05,
      "loss": 0.717,
      "step": 596600
    },
    {
      "epoch": 5.444740492006716,
      "grad_norm": 4.547671794891357,
      "learning_rate": 4.546271625666107e-05,
      "loss": 0.7531,
      "step": 596700
    },
    {
      "epoch": 5.445652967369881,
      "grad_norm": 4.433072566986084,
      "learning_rate": 4.54619558605251e-05,
      "loss": 0.7184,
      "step": 596800
    },
    {
      "epoch": 5.446565442733046,
      "grad_norm": 4.980215549468994,
      "learning_rate": 4.546119546438913e-05,
      "loss": 0.6972,
      "step": 596900
    },
    {
      "epoch": 5.447477918096212,
      "grad_norm": 3.36242938041687,
      "learning_rate": 4.546043506825316e-05,
      "loss": 0.7226,
      "step": 597000
    },
    {
      "epoch": 5.448390393459377,
      "grad_norm": 2.7472283840179443,
      "learning_rate": 4.5459674672117184e-05,
      "loss": 0.693,
      "step": 597100
    },
    {
      "epoch": 5.449302868822542,
      "grad_norm": 4.773178577423096,
      "learning_rate": 4.545891427598122e-05,
      "loss": 0.7265,
      "step": 597200
    },
    {
      "epoch": 5.450215344185707,
      "grad_norm": 5.110574245452881,
      "learning_rate": 4.5458153879845245e-05,
      "loss": 0.7303,
      "step": 597300
    },
    {
      "epoch": 5.451127819548872,
      "grad_norm": 3.7037124633789062,
      "learning_rate": 4.5457393483709275e-05,
      "loss": 0.7232,
      "step": 597400
    },
    {
      "epoch": 5.452040294912037,
      "grad_norm": 4.725924491882324,
      "learning_rate": 4.5456633087573305e-05,
      "loss": 0.7091,
      "step": 597500
    },
    {
      "epoch": 5.4529527702752025,
      "grad_norm": 3.9202651977539062,
      "learning_rate": 4.5455872691437335e-05,
      "loss": 0.6677,
      "step": 597600
    },
    {
      "epoch": 5.453865245638368,
      "grad_norm": 3.976719856262207,
      "learning_rate": 4.545511229530136e-05,
      "loss": 0.7299,
      "step": 597700
    },
    {
      "epoch": 5.454777721001533,
      "grad_norm": 4.340532302856445,
      "learning_rate": 4.5454351899165395e-05,
      "loss": 0.6799,
      "step": 597800
    },
    {
      "epoch": 5.455690196364698,
      "grad_norm": 4.071364402770996,
      "learning_rate": 4.545359150302942e-05,
      "loss": 0.712,
      "step": 597900
    },
    {
      "epoch": 5.456602671727864,
      "grad_norm": 4.149930953979492,
      "learning_rate": 4.545283110689345e-05,
      "loss": 0.7371,
      "step": 598000
    },
    {
      "epoch": 5.457515147091028,
      "grad_norm": 4.542680263519287,
      "learning_rate": 4.545207071075748e-05,
      "loss": 0.6779,
      "step": 598100
    },
    {
      "epoch": 5.458427622454193,
      "grad_norm": 4.445786476135254,
      "learning_rate": 4.545131031462151e-05,
      "loss": 0.7179,
      "step": 598200
    },
    {
      "epoch": 5.459340097817359,
      "grad_norm": 3.3438053131103516,
      "learning_rate": 4.545054991848554e-05,
      "loss": 0.7418,
      "step": 598300
    },
    {
      "epoch": 5.460252573180524,
      "grad_norm": 4.112296104431152,
      "learning_rate": 4.544978952234957e-05,
      "loss": 0.7111,
      "step": 598400
    },
    {
      "epoch": 5.461165048543689,
      "grad_norm": 3.485776424407959,
      "learning_rate": 4.544902912621359e-05,
      "loss": 0.6965,
      "step": 598500
    },
    {
      "epoch": 5.462077523906855,
      "grad_norm": 4.777273178100586,
      "learning_rate": 4.544826873007763e-05,
      "loss": 0.696,
      "step": 598600
    },
    {
      "epoch": 5.46298999927002,
      "grad_norm": 2.9615659713745117,
      "learning_rate": 4.544750833394165e-05,
      "loss": 0.7017,
      "step": 598700
    },
    {
      "epoch": 5.463902474633185,
      "grad_norm": 4.494898319244385,
      "learning_rate": 4.544674793780568e-05,
      "loss": 0.7093,
      "step": 598800
    },
    {
      "epoch": 5.4648149499963505,
      "grad_norm": 3.946054458618164,
      "learning_rate": 4.544598754166971e-05,
      "loss": 0.7318,
      "step": 598900
    },
    {
      "epoch": 5.465727425359515,
      "grad_norm": 3.923621416091919,
      "learning_rate": 4.544522714553374e-05,
      "loss": 0.7212,
      "step": 599000
    },
    {
      "epoch": 5.46663990072268,
      "grad_norm": 3.410691261291504,
      "learning_rate": 4.5444466749397765e-05,
      "loss": 0.734,
      "step": 599100
    },
    {
      "epoch": 5.4675523760858455,
      "grad_norm": 3.9097495079040527,
      "learning_rate": 4.54437063532618e-05,
      "loss": 0.7111,
      "step": 599200
    },
    {
      "epoch": 5.468464851449011,
      "grad_norm": 3.487046480178833,
      "learning_rate": 4.5442945957125826e-05,
      "loss": 0.6913,
      "step": 599300
    },
    {
      "epoch": 5.469377326812176,
      "grad_norm": 3.808215379714966,
      "learning_rate": 4.5442185560989856e-05,
      "loss": 0.7168,
      "step": 599400
    },
    {
      "epoch": 5.470289802175341,
      "grad_norm": 4.0988335609436035,
      "learning_rate": 4.5441425164853886e-05,
      "loss": 0.7372,
      "step": 599500
    },
    {
      "epoch": 5.471202277538507,
      "grad_norm": 3.49743914604187,
      "learning_rate": 4.544066476871791e-05,
      "loss": 0.6869,
      "step": 599600
    },
    {
      "epoch": 5.472114752901672,
      "grad_norm": 3.4134340286254883,
      "learning_rate": 4.5439904372581946e-05,
      "loss": 0.7062,
      "step": 599700
    },
    {
      "epoch": 5.4730272282648365,
      "grad_norm": 4.431375503540039,
      "learning_rate": 4.543914397644597e-05,
      "loss": 0.7133,
      "step": 599800
    },
    {
      "epoch": 5.473939703628002,
      "grad_norm": 4.558964252471924,
      "learning_rate": 4.543838358031e-05,
      "loss": 0.7284,
      "step": 599900
    },
    {
      "epoch": 5.474852178991167,
      "grad_norm": 3.4924957752227783,
      "learning_rate": 4.543762318417403e-05,
      "loss": 0.723,
      "step": 600000
    },
    {
      "epoch": 5.475764654354332,
      "grad_norm": 3.8397438526153564,
      "learning_rate": 4.543686278803806e-05,
      "loss": 0.7345,
      "step": 600100
    },
    {
      "epoch": 5.476677129717498,
      "grad_norm": 3.508972406387329,
      "learning_rate": 4.543610239190208e-05,
      "loss": 0.7201,
      "step": 600200
    },
    {
      "epoch": 5.477589605080663,
      "grad_norm": 3.8328604698181152,
      "learning_rate": 4.543534199576612e-05,
      "loss": 0.6991,
      "step": 600300
    },
    {
      "epoch": 5.478502080443828,
      "grad_norm": 3.387967586517334,
      "learning_rate": 4.543458159963014e-05,
      "loss": 0.6997,
      "step": 600400
    },
    {
      "epoch": 5.4794145558069935,
      "grad_norm": 4.389172554016113,
      "learning_rate": 4.543382120349417e-05,
      "loss": 0.6923,
      "step": 600500
    },
    {
      "epoch": 5.480327031170159,
      "grad_norm": 4.20940637588501,
      "learning_rate": 4.54330608073582e-05,
      "loss": 0.6946,
      "step": 600600
    },
    {
      "epoch": 5.481239506533323,
      "grad_norm": 4.0144782066345215,
      "learning_rate": 4.543230041122223e-05,
      "loss": 0.7101,
      "step": 600700
    },
    {
      "epoch": 5.482151981896489,
      "grad_norm": 4.286712646484375,
      "learning_rate": 4.543154001508626e-05,
      "loss": 0.6939,
      "step": 600800
    },
    {
      "epoch": 5.483064457259654,
      "grad_norm": 3.484314441680908,
      "learning_rate": 4.543077961895029e-05,
      "loss": 0.7498,
      "step": 600900
    },
    {
      "epoch": 5.483976932622819,
      "grad_norm": 3.6901934146881104,
      "learning_rate": 4.5430019222814316e-05,
      "loss": 0.7119,
      "step": 601000
    },
    {
      "epoch": 5.4848894079859845,
      "grad_norm": 4.133375644683838,
      "learning_rate": 4.542925882667835e-05,
      "loss": 0.7186,
      "step": 601100
    },
    {
      "epoch": 5.48580188334915,
      "grad_norm": 4.721701622009277,
      "learning_rate": 4.5428498430542376e-05,
      "loss": 0.723,
      "step": 601200
    },
    {
      "epoch": 5.486714358712315,
      "grad_norm": 3.1577887535095215,
      "learning_rate": 4.5427738034406407e-05,
      "loss": 0.727,
      "step": 601300
    },
    {
      "epoch": 5.48762683407548,
      "grad_norm": 4.316833972930908,
      "learning_rate": 4.542697763827044e-05,
      "loss": 0.7095,
      "step": 601400
    },
    {
      "epoch": 5.488539309438645,
      "grad_norm": 3.9976580142974854,
      "learning_rate": 4.542621724213447e-05,
      "loss": 0.6731,
      "step": 601500
    },
    {
      "epoch": 5.48945178480181,
      "grad_norm": 3.9599802494049072,
      "learning_rate": 4.542545684599849e-05,
      "loss": 0.6856,
      "step": 601600
    },
    {
      "epoch": 5.490364260164975,
      "grad_norm": 4.377919673919678,
      "learning_rate": 4.542469644986253e-05,
      "loss": 0.6942,
      "step": 601700
    },
    {
      "epoch": 5.491276735528141,
      "grad_norm": 3.9758777618408203,
      "learning_rate": 4.542393605372655e-05,
      "loss": 0.7108,
      "step": 601800
    },
    {
      "epoch": 5.492189210891306,
      "grad_norm": 2.922731876373291,
      "learning_rate": 4.542317565759058e-05,
      "loss": 0.7157,
      "step": 601900
    },
    {
      "epoch": 5.493101686254471,
      "grad_norm": 4.556682586669922,
      "learning_rate": 4.542241526145461e-05,
      "loss": 0.6664,
      "step": 602000
    },
    {
      "epoch": 5.494014161617637,
      "grad_norm": 4.029316425323486,
      "learning_rate": 4.542165486531864e-05,
      "loss": 0.6712,
      "step": 602100
    },
    {
      "epoch": 5.494926636980802,
      "grad_norm": 4.40224552154541,
      "learning_rate": 4.542089446918267e-05,
      "loss": 0.7127,
      "step": 602200
    },
    {
      "epoch": 5.495839112343967,
      "grad_norm": 4.203548908233643,
      "learning_rate": 4.5420134073046694e-05,
      "loss": 0.7164,
      "step": 602300
    },
    {
      "epoch": 5.496751587707132,
      "grad_norm": 3.660499334335327,
      "learning_rate": 4.5419373676910724e-05,
      "loss": 0.6725,
      "step": 602400
    },
    {
      "epoch": 5.497664063070297,
      "grad_norm": 3.5565009117126465,
      "learning_rate": 4.5418613280774754e-05,
      "loss": 0.6832,
      "step": 602500
    },
    {
      "epoch": 5.498576538433462,
      "grad_norm": 4.279128551483154,
      "learning_rate": 4.5417852884638784e-05,
      "loss": 0.7643,
      "step": 602600
    },
    {
      "epoch": 5.4994890137966275,
      "grad_norm": 4.077342510223389,
      "learning_rate": 4.541709248850281e-05,
      "loss": 0.7297,
      "step": 602700
    },
    {
      "epoch": 5.500401489159793,
      "grad_norm": 4.145810604095459,
      "learning_rate": 4.5416332092366844e-05,
      "loss": 0.7261,
      "step": 602800
    },
    {
      "epoch": 5.501313964522958,
      "grad_norm": 3.9640281200408936,
      "learning_rate": 4.541557169623087e-05,
      "loss": 0.7183,
      "step": 602900
    },
    {
      "epoch": 5.502226439886123,
      "grad_norm": 4.734529495239258,
      "learning_rate": 4.54148113000949e-05,
      "loss": 0.7133,
      "step": 603000
    },
    {
      "epoch": 5.503138915249288,
      "grad_norm": 4.009757995605469,
      "learning_rate": 4.541405090395893e-05,
      "loss": 0.6839,
      "step": 603100
    },
    {
      "epoch": 5.504051390612453,
      "grad_norm": 4.647976875305176,
      "learning_rate": 4.541329050782296e-05,
      "loss": 0.7229,
      "step": 603200
    },
    {
      "epoch": 5.504963865975618,
      "grad_norm": 3.963968515396118,
      "learning_rate": 4.541253011168699e-05,
      "loss": 0.7006,
      "step": 603300
    },
    {
      "epoch": 5.505876341338784,
      "grad_norm": 4.085707187652588,
      "learning_rate": 4.541176971555102e-05,
      "loss": 0.7268,
      "step": 603400
    },
    {
      "epoch": 5.506788816701949,
      "grad_norm": 3.2012007236480713,
      "learning_rate": 4.541100931941504e-05,
      "loss": 0.6937,
      "step": 603500
    },
    {
      "epoch": 5.507701292065114,
      "grad_norm": 3.9386606216430664,
      "learning_rate": 4.541024892327908e-05,
      "loss": 0.7276,
      "step": 603600
    },
    {
      "epoch": 5.50861376742828,
      "grad_norm": 4.251444339752197,
      "learning_rate": 4.54094885271431e-05,
      "loss": 0.7449,
      "step": 603700
    },
    {
      "epoch": 5.509526242791445,
      "grad_norm": 4.046072006225586,
      "learning_rate": 4.540872813100713e-05,
      "loss": 0.725,
      "step": 603800
    },
    {
      "epoch": 5.51043871815461,
      "grad_norm": 3.512899160385132,
      "learning_rate": 4.540796773487116e-05,
      "loss": 0.716,
      "step": 603900
    },
    {
      "epoch": 5.5113511935177755,
      "grad_norm": 4.885894298553467,
      "learning_rate": 4.540720733873519e-05,
      "loss": 0.7718,
      "step": 604000
    },
    {
      "epoch": 5.51226366888094,
      "grad_norm": 3.8642008304595947,
      "learning_rate": 4.5406446942599215e-05,
      "loss": 0.7243,
      "step": 604100
    },
    {
      "epoch": 5.513176144244105,
      "grad_norm": 3.9578499794006348,
      "learning_rate": 4.540568654646325e-05,
      "loss": 0.6874,
      "step": 604200
    },
    {
      "epoch": 5.5140886196072705,
      "grad_norm": 4.4783759117126465,
      "learning_rate": 4.5404926150327275e-05,
      "loss": 0.6774,
      "step": 604300
    },
    {
      "epoch": 5.515001094970436,
      "grad_norm": 4.270711898803711,
      "learning_rate": 4.5404165754191305e-05,
      "loss": 0.7326,
      "step": 604400
    },
    {
      "epoch": 5.515913570333601,
      "grad_norm": 3.0127172470092773,
      "learning_rate": 4.5403405358055335e-05,
      "loss": 0.7295,
      "step": 604500
    },
    {
      "epoch": 5.516826045696766,
      "grad_norm": 4.603987216949463,
      "learning_rate": 4.5402644961919365e-05,
      "loss": 0.7215,
      "step": 604600
    },
    {
      "epoch": 5.517738521059932,
      "grad_norm": 4.242974281311035,
      "learning_rate": 4.5401884565783395e-05,
      "loss": 0.6827,
      "step": 604700
    },
    {
      "epoch": 5.518650996423096,
      "grad_norm": 3.861403465270996,
      "learning_rate": 4.5401124169647425e-05,
      "loss": 0.6763,
      "step": 604800
    },
    {
      "epoch": 5.519563471786261,
      "grad_norm": 4.011287689208984,
      "learning_rate": 4.540036377351145e-05,
      "loss": 0.733,
      "step": 604900
    },
    {
      "epoch": 5.520475947149427,
      "grad_norm": 4.427595615386963,
      "learning_rate": 4.539960337737548e-05,
      "loss": 0.7185,
      "step": 605000
    },
    {
      "epoch": 5.521388422512592,
      "grad_norm": 2.9587080478668213,
      "learning_rate": 4.539884298123951e-05,
      "loss": 0.7289,
      "step": 605100
    },
    {
      "epoch": 5.522300897875757,
      "grad_norm": 4.40673303604126,
      "learning_rate": 4.539808258510354e-05,
      "loss": 0.7214,
      "step": 605200
    },
    {
      "epoch": 5.523213373238923,
      "grad_norm": 3.6740520000457764,
      "learning_rate": 4.539732218896757e-05,
      "loss": 0.7264,
      "step": 605300
    },
    {
      "epoch": 5.524125848602088,
      "grad_norm": 4.560242176055908,
      "learning_rate": 4.539656179283159e-05,
      "loss": 0.7055,
      "step": 605400
    },
    {
      "epoch": 5.525038323965253,
      "grad_norm": 4.270146369934082,
      "learning_rate": 4.539580139669562e-05,
      "loss": 0.6935,
      "step": 605500
    },
    {
      "epoch": 5.5259507993284185,
      "grad_norm": 3.63944411277771,
      "learning_rate": 4.539504100055965e-05,
      "loss": 0.6837,
      "step": 605600
    },
    {
      "epoch": 5.526863274691584,
      "grad_norm": 4.397143363952637,
      "learning_rate": 4.539428060442368e-05,
      "loss": 0.6816,
      "step": 605700
    },
    {
      "epoch": 5.527775750054748,
      "grad_norm": 4.268555164337158,
      "learning_rate": 4.539352020828771e-05,
      "loss": 0.6926,
      "step": 605800
    },
    {
      "epoch": 5.5286882254179135,
      "grad_norm": 4.0821003913879395,
      "learning_rate": 4.539275981215174e-05,
      "loss": 0.7209,
      "step": 605900
    },
    {
      "epoch": 5.529600700781079,
      "grad_norm": 4.649663925170898,
      "learning_rate": 4.5391999416015765e-05,
      "loss": 0.7346,
      "step": 606000
    },
    {
      "epoch": 5.530513176144244,
      "grad_norm": 3.3804821968078613,
      "learning_rate": 4.53912390198798e-05,
      "loss": 0.6878,
      "step": 606100
    },
    {
      "epoch": 5.531425651507409,
      "grad_norm": 4.052192211151123,
      "learning_rate": 4.5390478623743826e-05,
      "loss": 0.6925,
      "step": 606200
    },
    {
      "epoch": 5.532338126870575,
      "grad_norm": 3.674246072769165,
      "learning_rate": 4.5389718227607856e-05,
      "loss": 0.7344,
      "step": 606300
    },
    {
      "epoch": 5.53325060223374,
      "grad_norm": 3.9102489948272705,
      "learning_rate": 4.5388957831471886e-05,
      "loss": 0.6799,
      "step": 606400
    },
    {
      "epoch": 5.534163077596904,
      "grad_norm": 2.6968679428100586,
      "learning_rate": 4.5388197435335916e-05,
      "loss": 0.7208,
      "step": 606500
    },
    {
      "epoch": 5.53507555296007,
      "grad_norm": 3.5687332153320312,
      "learning_rate": 4.5387437039199946e-05,
      "loss": 0.712,
      "step": 606600
    },
    {
      "epoch": 5.535988028323235,
      "grad_norm": 4.801436424255371,
      "learning_rate": 4.5386676643063976e-05,
      "loss": 0.6959,
      "step": 606700
    },
    {
      "epoch": 5.5369005036864,
      "grad_norm": 4.686015605926514,
      "learning_rate": 4.5385916246928e-05,
      "loss": 0.7342,
      "step": 606800
    },
    {
      "epoch": 5.537812979049566,
      "grad_norm": 3.69451642036438,
      "learning_rate": 4.538515585079203e-05,
      "loss": 0.7293,
      "step": 606900
    },
    {
      "epoch": 5.538725454412731,
      "grad_norm": 2.3785810470581055,
      "learning_rate": 4.538439545465606e-05,
      "loss": 0.7453,
      "step": 607000
    },
    {
      "epoch": 5.539637929775896,
      "grad_norm": 4.155170440673828,
      "learning_rate": 4.538363505852009e-05,
      "loss": 0.7278,
      "step": 607100
    },
    {
      "epoch": 5.5405504051390615,
      "grad_norm": 3.3234384059906006,
      "learning_rate": 4.538287466238412e-05,
      "loss": 0.6946,
      "step": 607200
    },
    {
      "epoch": 5.541462880502227,
      "grad_norm": 4.208082675933838,
      "learning_rate": 4.538211426624815e-05,
      "loss": 0.6984,
      "step": 607300
    },
    {
      "epoch": 5.542375355865392,
      "grad_norm": 4.212037086486816,
      "learning_rate": 4.538135387011217e-05,
      "loss": 0.702,
      "step": 607400
    },
    {
      "epoch": 5.5432878312285565,
      "grad_norm": 3.3426496982574463,
      "learning_rate": 4.538059347397621e-05,
      "loss": 0.6973,
      "step": 607500
    },
    {
      "epoch": 5.544200306591722,
      "grad_norm": 3.252751588821411,
      "learning_rate": 4.537983307784023e-05,
      "loss": 0.7261,
      "step": 607600
    },
    {
      "epoch": 5.545112781954887,
      "grad_norm": 4.21011209487915,
      "learning_rate": 4.537907268170426e-05,
      "loss": 0.683,
      "step": 607700
    },
    {
      "epoch": 5.546025257318052,
      "grad_norm": 4.182308673858643,
      "learning_rate": 4.537831228556829e-05,
      "loss": 0.6968,
      "step": 607800
    },
    {
      "epoch": 5.546937732681218,
      "grad_norm": 4.104737758636475,
      "learning_rate": 4.5377551889432316e-05,
      "loss": 0.7225,
      "step": 607900
    },
    {
      "epoch": 5.547850208044383,
      "grad_norm": 2.5295698642730713,
      "learning_rate": 4.537679149329635e-05,
      "loss": 0.6912,
      "step": 608000
    },
    {
      "epoch": 5.548762683407548,
      "grad_norm": 4.296083927154541,
      "learning_rate": 4.5376031097160377e-05,
      "loss": 0.706,
      "step": 608100
    },
    {
      "epoch": 5.549675158770713,
      "grad_norm": 5.125720500946045,
      "learning_rate": 4.537527070102441e-05,
      "loss": 0.7172,
      "step": 608200
    },
    {
      "epoch": 5.550587634133878,
      "grad_norm": 3.60664701461792,
      "learning_rate": 4.537451030488844e-05,
      "loss": 0.6833,
      "step": 608300
    },
    {
      "epoch": 5.551500109497043,
      "grad_norm": 3.3569910526275635,
      "learning_rate": 4.537374990875247e-05,
      "loss": 0.7202,
      "step": 608400
    },
    {
      "epoch": 5.552412584860209,
      "grad_norm": 4.206730842590332,
      "learning_rate": 4.537298951261649e-05,
      "loss": 0.6985,
      "step": 608500
    },
    {
      "epoch": 5.553325060223374,
      "grad_norm": 3.7928779125213623,
      "learning_rate": 4.537222911648053e-05,
      "loss": 0.7269,
      "step": 608600
    },
    {
      "epoch": 5.554237535586539,
      "grad_norm": 3.6507580280303955,
      "learning_rate": 4.537146872034455e-05,
      "loss": 0.7094,
      "step": 608700
    },
    {
      "epoch": 5.5551500109497045,
      "grad_norm": 3.257553815841675,
      "learning_rate": 4.537070832420858e-05,
      "loss": 0.6825,
      "step": 608800
    },
    {
      "epoch": 5.55606248631287,
      "grad_norm": 3.6051902770996094,
      "learning_rate": 4.536994792807261e-05,
      "loss": 0.7164,
      "step": 608900
    },
    {
      "epoch": 5.556974961676035,
      "grad_norm": 3.8532798290252686,
      "learning_rate": 4.536918753193664e-05,
      "loss": 0.7209,
      "step": 609000
    },
    {
      "epoch": 5.5578874370392,
      "grad_norm": 3.780914306640625,
      "learning_rate": 4.536842713580067e-05,
      "loss": 0.6991,
      "step": 609100
    },
    {
      "epoch": 5.558799912402365,
      "grad_norm": 3.3615286350250244,
      "learning_rate": 4.53676667396647e-05,
      "loss": 0.7048,
      "step": 609200
    },
    {
      "epoch": 5.55971238776553,
      "grad_norm": 3.7334651947021484,
      "learning_rate": 4.5366906343528724e-05,
      "loss": 0.6721,
      "step": 609300
    },
    {
      "epoch": 5.560624863128695,
      "grad_norm": 1.81600821018219,
      "learning_rate": 4.536614594739276e-05,
      "loss": 0.6954,
      "step": 609400
    },
    {
      "epoch": 5.561537338491861,
      "grad_norm": 3.875664234161377,
      "learning_rate": 4.5365385551256784e-05,
      "loss": 0.6765,
      "step": 609500
    },
    {
      "epoch": 5.562449813855026,
      "grad_norm": 4.700130939483643,
      "learning_rate": 4.5364625155120814e-05,
      "loss": 0.6975,
      "step": 609600
    },
    {
      "epoch": 5.563362289218191,
      "grad_norm": 3.2229535579681396,
      "learning_rate": 4.5363864758984844e-05,
      "loss": 0.7397,
      "step": 609700
    },
    {
      "epoch": 5.564274764581357,
      "grad_norm": 4.623933792114258,
      "learning_rate": 4.5363104362848874e-05,
      "loss": 0.7515,
      "step": 609800
    },
    {
      "epoch": 5.565187239944521,
      "grad_norm": 3.97582745552063,
      "learning_rate": 4.53623439667129e-05,
      "loss": 0.6907,
      "step": 609900
    },
    {
      "epoch": 5.566099715307686,
      "grad_norm": 3.160994291305542,
      "learning_rate": 4.5361583570576934e-05,
      "loss": 0.7028,
      "step": 610000
    },
    {
      "epoch": 5.567012190670852,
      "grad_norm": 4.521780014038086,
      "learning_rate": 4.536082317444096e-05,
      "loss": 0.71,
      "step": 610100
    },
    {
      "epoch": 5.567924666034017,
      "grad_norm": 4.644437789916992,
      "learning_rate": 4.536006277830499e-05,
      "loss": 0.69,
      "step": 610200
    },
    {
      "epoch": 5.568837141397182,
      "grad_norm": 3.0954995155334473,
      "learning_rate": 4.535930238216902e-05,
      "loss": 0.6984,
      "step": 610300
    },
    {
      "epoch": 5.5697496167603475,
      "grad_norm": 3.993699550628662,
      "learning_rate": 4.535854198603305e-05,
      "loss": 0.6677,
      "step": 610400
    },
    {
      "epoch": 5.570662092123513,
      "grad_norm": 4.4451823234558105,
      "learning_rate": 4.535778158989708e-05,
      "loss": 0.6984,
      "step": 610500
    },
    {
      "epoch": 5.571574567486678,
      "grad_norm": 4.7695112228393555,
      "learning_rate": 4.535702119376111e-05,
      "loss": 0.7209,
      "step": 610600
    },
    {
      "epoch": 5.572487042849843,
      "grad_norm": 3.925156593322754,
      "learning_rate": 4.535626079762513e-05,
      "loss": 0.6858,
      "step": 610700
    },
    {
      "epoch": 5.573399518213009,
      "grad_norm": 3.9613020420074463,
      "learning_rate": 4.535550040148916e-05,
      "loss": 0.7058,
      "step": 610800
    },
    {
      "epoch": 5.574311993576173,
      "grad_norm": 3.4257681369781494,
      "learning_rate": 4.535474000535319e-05,
      "loss": 0.7418,
      "step": 610900
    },
    {
      "epoch": 5.575224468939338,
      "grad_norm": 3.7491698265075684,
      "learning_rate": 4.5353979609217215e-05,
      "loss": 0.7317,
      "step": 611000
    },
    {
      "epoch": 5.576136944302504,
      "grad_norm": 4.854706287384033,
      "learning_rate": 4.535321921308125e-05,
      "loss": 0.724,
      "step": 611100
    },
    {
      "epoch": 5.577049419665669,
      "grad_norm": 4.265872955322266,
      "learning_rate": 4.5352458816945275e-05,
      "loss": 0.7123,
      "step": 611200
    },
    {
      "epoch": 5.577961895028834,
      "grad_norm": 4.837655544281006,
      "learning_rate": 4.5351698420809305e-05,
      "loss": 0.6804,
      "step": 611300
    },
    {
      "epoch": 5.578874370392,
      "grad_norm": 4.670650959014893,
      "learning_rate": 4.5350938024673335e-05,
      "loss": 0.7034,
      "step": 611400
    },
    {
      "epoch": 5.579786845755165,
      "grad_norm": 4.529812335968018,
      "learning_rate": 4.5350177628537365e-05,
      "loss": 0.6924,
      "step": 611500
    },
    {
      "epoch": 5.580699321118329,
      "grad_norm": 4.156021595001221,
      "learning_rate": 4.5349417232401395e-05,
      "loss": 0.7293,
      "step": 611600
    },
    {
      "epoch": 5.581611796481495,
      "grad_norm": 4.223960876464844,
      "learning_rate": 4.5348656836265425e-05,
      "loss": 0.7077,
      "step": 611700
    },
    {
      "epoch": 5.58252427184466,
      "grad_norm": 3.354182243347168,
      "learning_rate": 4.534789644012945e-05,
      "loss": 0.72,
      "step": 611800
    },
    {
      "epoch": 5.583436747207825,
      "grad_norm": 3.5179858207702637,
      "learning_rate": 4.5347136043993485e-05,
      "loss": 0.714,
      "step": 611900
    },
    {
      "epoch": 5.5843492225709905,
      "grad_norm": 3.861973762512207,
      "learning_rate": 4.534637564785751e-05,
      "loss": 0.6975,
      "step": 612000
    },
    {
      "epoch": 5.585261697934156,
      "grad_norm": 2.9840307235717773,
      "learning_rate": 4.534561525172154e-05,
      "loss": 0.7413,
      "step": 612100
    },
    {
      "epoch": 5.586174173297321,
      "grad_norm": 3.7552061080932617,
      "learning_rate": 4.534485485558557e-05,
      "loss": 0.6835,
      "step": 612200
    },
    {
      "epoch": 5.587086648660486,
      "grad_norm": 4.1229166984558105,
      "learning_rate": 4.53440944594496e-05,
      "loss": 0.7015,
      "step": 612300
    },
    {
      "epoch": 5.587999124023652,
      "grad_norm": 5.254836082458496,
      "learning_rate": 4.534333406331362e-05,
      "loss": 0.7181,
      "step": 612400
    },
    {
      "epoch": 5.588911599386817,
      "grad_norm": 4.723607063293457,
      "learning_rate": 4.534257366717766e-05,
      "loss": 0.7239,
      "step": 612500
    },
    {
      "epoch": 5.589824074749981,
      "grad_norm": 3.896610736846924,
      "learning_rate": 4.534181327104168e-05,
      "loss": 0.6852,
      "step": 612600
    },
    {
      "epoch": 5.590736550113147,
      "grad_norm": 4.848191261291504,
      "learning_rate": 4.534105287490571e-05,
      "loss": 0.6948,
      "step": 612700
    },
    {
      "epoch": 5.591649025476312,
      "grad_norm": 3.165055990219116,
      "learning_rate": 4.534029247876974e-05,
      "loss": 0.6757,
      "step": 612800
    },
    {
      "epoch": 5.592561500839477,
      "grad_norm": 4.519686698913574,
      "learning_rate": 4.533953208263377e-05,
      "loss": 0.706,
      "step": 612900
    },
    {
      "epoch": 5.593473976202643,
      "grad_norm": 4.543940544128418,
      "learning_rate": 4.53387716864978e-05,
      "loss": 0.7232,
      "step": 613000
    },
    {
      "epoch": 5.594386451565808,
      "grad_norm": 4.542547225952148,
      "learning_rate": 4.533801129036183e-05,
      "loss": 0.6922,
      "step": 613100
    },
    {
      "epoch": 5.595298926928973,
      "grad_norm": 4.120119571685791,
      "learning_rate": 4.5337250894225856e-05,
      "loss": 0.6838,
      "step": 613200
    },
    {
      "epoch": 5.596211402292138,
      "grad_norm": 3.721238851547241,
      "learning_rate": 4.533649049808989e-05,
      "loss": 0.6984,
      "step": 613300
    },
    {
      "epoch": 5.597123877655303,
      "grad_norm": 3.480438232421875,
      "learning_rate": 4.5335730101953916e-05,
      "loss": 0.6786,
      "step": 613400
    },
    {
      "epoch": 5.598036353018468,
      "grad_norm": 3.720130443572998,
      "learning_rate": 4.533496970581794e-05,
      "loss": 0.7041,
      "step": 613500
    },
    {
      "epoch": 5.5989488283816335,
      "grad_norm": 4.080923080444336,
      "learning_rate": 4.5334209309681976e-05,
      "loss": 0.7018,
      "step": 613600
    },
    {
      "epoch": 5.599861303744799,
      "grad_norm": 4.3562235832214355,
      "learning_rate": 4.5333448913546e-05,
      "loss": 0.7055,
      "step": 613700
    },
    {
      "epoch": 5.600773779107964,
      "grad_norm": 3.939404010772705,
      "learning_rate": 4.533268851741003e-05,
      "loss": 0.6973,
      "step": 613800
    },
    {
      "epoch": 5.6016862544711294,
      "grad_norm": 4.422375679016113,
      "learning_rate": 4.533192812127406e-05,
      "loss": 0.7012,
      "step": 613900
    },
    {
      "epoch": 5.602598729834295,
      "grad_norm": 2.7654504776000977,
      "learning_rate": 4.533116772513809e-05,
      "loss": 0.6881,
      "step": 614000
    },
    {
      "epoch": 5.60351120519746,
      "grad_norm": 4.5668840408325195,
      "learning_rate": 4.533040732900212e-05,
      "loss": 0.7165,
      "step": 614100
    },
    {
      "epoch": 5.604423680560625,
      "grad_norm": 2.9434452056884766,
      "learning_rate": 4.532964693286615e-05,
      "loss": 0.7316,
      "step": 614200
    },
    {
      "epoch": 5.60533615592379,
      "grad_norm": 5.270471096038818,
      "learning_rate": 4.532888653673017e-05,
      "loss": 0.7129,
      "step": 614300
    },
    {
      "epoch": 5.606248631286955,
      "grad_norm": 4.5636162757873535,
      "learning_rate": 4.532812614059421e-05,
      "loss": 0.7256,
      "step": 614400
    },
    {
      "epoch": 5.60716110665012,
      "grad_norm": 3.8404548168182373,
      "learning_rate": 4.532736574445823e-05,
      "loss": 0.7018,
      "step": 614500
    },
    {
      "epoch": 5.608073582013286,
      "grad_norm": 4.031583786010742,
      "learning_rate": 4.532660534832226e-05,
      "loss": 0.6953,
      "step": 614600
    },
    {
      "epoch": 5.608986057376451,
      "grad_norm": 3.4020028114318848,
      "learning_rate": 4.532584495218629e-05,
      "loss": 0.6924,
      "step": 614700
    },
    {
      "epoch": 5.609898532739616,
      "grad_norm": 4.299678802490234,
      "learning_rate": 4.532508455605032e-05,
      "loss": 0.7098,
      "step": 614800
    },
    {
      "epoch": 5.6108110081027815,
      "grad_norm": 4.195802688598633,
      "learning_rate": 4.5324324159914347e-05,
      "loss": 0.7093,
      "step": 614900
    },
    {
      "epoch": 5.611723483465946,
      "grad_norm": 3.654203176498413,
      "learning_rate": 4.5323563763778383e-05,
      "loss": 0.7061,
      "step": 615000
    },
    {
      "epoch": 5.612635958829111,
      "grad_norm": 4.071910381317139,
      "learning_rate": 4.532280336764241e-05,
      "loss": 0.7367,
      "step": 615100
    },
    {
      "epoch": 5.613548434192277,
      "grad_norm": 4.347844123840332,
      "learning_rate": 4.532204297150644e-05,
      "loss": 0.7224,
      "step": 615200
    },
    {
      "epoch": 5.614460909555442,
      "grad_norm": 4.165804386138916,
      "learning_rate": 4.532128257537047e-05,
      "loss": 0.6932,
      "step": 615300
    },
    {
      "epoch": 5.615373384918607,
      "grad_norm": 4.505943775177002,
      "learning_rate": 4.53205221792345e-05,
      "loss": 0.7062,
      "step": 615400
    },
    {
      "epoch": 5.6162858602817725,
      "grad_norm": 4.023433208465576,
      "learning_rate": 4.531976178309853e-05,
      "loss": 0.6905,
      "step": 615500
    },
    {
      "epoch": 5.617198335644938,
      "grad_norm": 4.212069034576416,
      "learning_rate": 4.531900138696256e-05,
      "loss": 0.6806,
      "step": 615600
    },
    {
      "epoch": 5.618110811008103,
      "grad_norm": 4.4211812019348145,
      "learning_rate": 4.531824099082658e-05,
      "loss": 0.7278,
      "step": 615700
    },
    {
      "epoch": 5.619023286371268,
      "grad_norm": 4.805001258850098,
      "learning_rate": 4.531748059469062e-05,
      "loss": 0.7354,
      "step": 615800
    },
    {
      "epoch": 5.619935761734434,
      "grad_norm": 3.9927849769592285,
      "learning_rate": 4.531672019855464e-05,
      "loss": 0.6716,
      "step": 615900
    },
    {
      "epoch": 5.620848237097598,
      "grad_norm": 4.401875972747803,
      "learning_rate": 4.531595980241867e-05,
      "loss": 0.7074,
      "step": 616000
    },
    {
      "epoch": 5.621760712460763,
      "grad_norm": 3.638051986694336,
      "learning_rate": 4.53151994062827e-05,
      "loss": 0.661,
      "step": 616100
    },
    {
      "epoch": 5.622673187823929,
      "grad_norm": 3.9113588333129883,
      "learning_rate": 4.531443901014673e-05,
      "loss": 0.7597,
      "step": 616200
    },
    {
      "epoch": 5.623585663187094,
      "grad_norm": 3.721177339553833,
      "learning_rate": 4.5313678614010754e-05,
      "loss": 0.708,
      "step": 616300
    },
    {
      "epoch": 5.624498138550259,
      "grad_norm": 4.121903419494629,
      "learning_rate": 4.5312918217874784e-05,
      "loss": 0.689,
      "step": 616400
    },
    {
      "epoch": 5.625410613913425,
      "grad_norm": 2.724102020263672,
      "learning_rate": 4.5312157821738814e-05,
      "loss": 0.7051,
      "step": 616500
    },
    {
      "epoch": 5.62632308927659,
      "grad_norm": 4.5465850830078125,
      "learning_rate": 4.5311397425602844e-05,
      "loss": 0.7015,
      "step": 616600
    },
    {
      "epoch": 5.627235564639754,
      "grad_norm": 3.9088821411132812,
      "learning_rate": 4.5310637029466874e-05,
      "loss": 0.722,
      "step": 616700
    },
    {
      "epoch": 5.62814804000292,
      "grad_norm": 3.8740859031677246,
      "learning_rate": 4.53098766333309e-05,
      "loss": 0.7143,
      "step": 616800
    },
    {
      "epoch": 5.629060515366085,
      "grad_norm": 4.492135047912598,
      "learning_rate": 4.5309116237194934e-05,
      "loss": 0.713,
      "step": 616900
    },
    {
      "epoch": 5.62997299072925,
      "grad_norm": 4.611722469329834,
      "learning_rate": 4.530835584105896e-05,
      "loss": 0.6995,
      "step": 617000
    },
    {
      "epoch": 5.6308854660924155,
      "grad_norm": 3.801698684692383,
      "learning_rate": 4.530759544492299e-05,
      "loss": 0.7002,
      "step": 617100
    },
    {
      "epoch": 5.631797941455581,
      "grad_norm": 5.042663097381592,
      "learning_rate": 4.530683504878702e-05,
      "loss": 0.718,
      "step": 617200
    },
    {
      "epoch": 5.632710416818746,
      "grad_norm": 3.5849075317382812,
      "learning_rate": 4.530607465265105e-05,
      "loss": 0.7162,
      "step": 617300
    },
    {
      "epoch": 5.633622892181911,
      "grad_norm": 3.8160810470581055,
      "learning_rate": 4.530531425651507e-05,
      "loss": 0.6612,
      "step": 617400
    },
    {
      "epoch": 5.634535367545077,
      "grad_norm": 4.3065338134765625,
      "learning_rate": 4.530455386037911e-05,
      "loss": 0.6966,
      "step": 617500
    },
    {
      "epoch": 5.635447842908242,
      "grad_norm": 4.137662887573242,
      "learning_rate": 4.530379346424313e-05,
      "loss": 0.7383,
      "step": 617600
    },
    {
      "epoch": 5.636360318271406,
      "grad_norm": 3.2645819187164307,
      "learning_rate": 4.530303306810716e-05,
      "loss": 0.7474,
      "step": 617700
    },
    {
      "epoch": 5.637272793634572,
      "grad_norm": 3.858442544937134,
      "learning_rate": 4.530227267197119e-05,
      "loss": 0.7002,
      "step": 617800
    },
    {
      "epoch": 5.638185268997737,
      "grad_norm": 4.8460693359375,
      "learning_rate": 4.530151227583522e-05,
      "loss": 0.7405,
      "step": 617900
    },
    {
      "epoch": 5.639097744360902,
      "grad_norm": 3.7748939990997314,
      "learning_rate": 4.530075187969925e-05,
      "loss": 0.6997,
      "step": 618000
    },
    {
      "epoch": 5.640010219724068,
      "grad_norm": 3.8516104221343994,
      "learning_rate": 4.529999148356328e-05,
      "loss": 0.7151,
      "step": 618100
    },
    {
      "epoch": 5.640922695087233,
      "grad_norm": 3.556410074234009,
      "learning_rate": 4.5299231087427305e-05,
      "loss": 0.7068,
      "step": 618200
    },
    {
      "epoch": 5.641835170450398,
      "grad_norm": 3.9478681087493896,
      "learning_rate": 4.529847069129134e-05,
      "loss": 0.7034,
      "step": 618300
    },
    {
      "epoch": 5.642747645813563,
      "grad_norm": 3.663177490234375,
      "learning_rate": 4.5297710295155365e-05,
      "loss": 0.7122,
      "step": 618400
    },
    {
      "epoch": 5.643660121176728,
      "grad_norm": 4.22908353805542,
      "learning_rate": 4.5296949899019395e-05,
      "loss": 0.6882,
      "step": 618500
    },
    {
      "epoch": 5.644572596539893,
      "grad_norm": 3.910045862197876,
      "learning_rate": 4.5296189502883425e-05,
      "loss": 0.6921,
      "step": 618600
    },
    {
      "epoch": 5.6454850719030585,
      "grad_norm": 4.150722503662109,
      "learning_rate": 4.5295429106747455e-05,
      "loss": 0.7101,
      "step": 618700
    },
    {
      "epoch": 5.646397547266224,
      "grad_norm": 2.211535692214966,
      "learning_rate": 4.5294668710611485e-05,
      "loss": 0.7081,
      "step": 618800
    },
    {
      "epoch": 5.647310022629389,
      "grad_norm": 4.121422290802002,
      "learning_rate": 4.5293908314475515e-05,
      "loss": 0.7195,
      "step": 618900
    },
    {
      "epoch": 5.648222497992554,
      "grad_norm": 4.595743179321289,
      "learning_rate": 4.529314791833954e-05,
      "loss": 0.6617,
      "step": 619000
    },
    {
      "epoch": 5.64913497335572,
      "grad_norm": 2.975532054901123,
      "learning_rate": 4.529238752220357e-05,
      "loss": 0.6797,
      "step": 619100
    },
    {
      "epoch": 5.650047448718885,
      "grad_norm": 4.0652756690979,
      "learning_rate": 4.52916271260676e-05,
      "loss": 0.728,
      "step": 619200
    },
    {
      "epoch": 5.65095992408205,
      "grad_norm": 3.488088846206665,
      "learning_rate": 4.529086672993162e-05,
      "loss": 0.7155,
      "step": 619300
    },
    {
      "epoch": 5.651872399445215,
      "grad_norm": 4.3346662521362305,
      "learning_rate": 4.529010633379566e-05,
      "loss": 0.7211,
      "step": 619400
    },
    {
      "epoch": 5.65278487480838,
      "grad_norm": 4.032890796661377,
      "learning_rate": 4.528934593765968e-05,
      "loss": 0.7188,
      "step": 619500
    },
    {
      "epoch": 5.653697350171545,
      "grad_norm": 3.864480495452881,
      "learning_rate": 4.528858554152371e-05,
      "loss": 0.671,
      "step": 619600
    },
    {
      "epoch": 5.654609825534711,
      "grad_norm": 4.468182563781738,
      "learning_rate": 4.528782514538774e-05,
      "loss": 0.7185,
      "step": 619700
    },
    {
      "epoch": 5.655522300897876,
      "grad_norm": 4.187085151672363,
      "learning_rate": 4.528706474925177e-05,
      "loss": 0.7047,
      "step": 619800
    },
    {
      "epoch": 5.656434776261041,
      "grad_norm": 2.8622097969055176,
      "learning_rate": 4.52863043531158e-05,
      "loss": 0.7099,
      "step": 619900
    },
    {
      "epoch": 5.6573472516242065,
      "grad_norm": 3.4227123260498047,
      "learning_rate": 4.528554395697983e-05,
      "loss": 0.6788,
      "step": 620000
    },
    {
      "epoch": 5.658259726987371,
      "grad_norm": 4.286862373352051,
      "learning_rate": 4.5284783560843856e-05,
      "loss": 0.7175,
      "step": 620100
    },
    {
      "epoch": 5.659172202350536,
      "grad_norm": 4.154837131500244,
      "learning_rate": 4.528402316470789e-05,
      "loss": 0.7084,
      "step": 620200
    },
    {
      "epoch": 5.6600846777137015,
      "grad_norm": 3.5607798099517822,
      "learning_rate": 4.5283262768571916e-05,
      "loss": 0.6914,
      "step": 620300
    },
    {
      "epoch": 5.660997153076867,
      "grad_norm": 3.4867987632751465,
      "learning_rate": 4.5282502372435946e-05,
      "loss": 0.7531,
      "step": 620400
    },
    {
      "epoch": 5.661909628440032,
      "grad_norm": 4.471851825714111,
      "learning_rate": 4.5281741976299976e-05,
      "loss": 0.7231,
      "step": 620500
    },
    {
      "epoch": 5.662822103803197,
      "grad_norm": 4.832911014556885,
      "learning_rate": 4.5280981580164006e-05,
      "loss": 0.699,
      "step": 620600
    },
    {
      "epoch": 5.663734579166363,
      "grad_norm": 4.293267726898193,
      "learning_rate": 4.528022118402803e-05,
      "loss": 0.7243,
      "step": 620700
    },
    {
      "epoch": 5.664647054529528,
      "grad_norm": 4.382997512817383,
      "learning_rate": 4.5279460787892066e-05,
      "loss": 0.7192,
      "step": 620800
    },
    {
      "epoch": 5.665559529892693,
      "grad_norm": 4.080742835998535,
      "learning_rate": 4.527870039175609e-05,
      "loss": 0.7307,
      "step": 620900
    },
    {
      "epoch": 5.666472005255859,
      "grad_norm": 4.359494686126709,
      "learning_rate": 4.527793999562012e-05,
      "loss": 0.7207,
      "step": 621000
    },
    {
      "epoch": 5.667384480619023,
      "grad_norm": 3.7292206287384033,
      "learning_rate": 4.527717959948415e-05,
      "loss": 0.7218,
      "step": 621100
    },
    {
      "epoch": 5.668296955982188,
      "grad_norm": 4.704591274261475,
      "learning_rate": 4.527641920334818e-05,
      "loss": 0.7182,
      "step": 621200
    },
    {
      "epoch": 5.669209431345354,
      "grad_norm": 5.093619346618652,
      "learning_rate": 4.527565880721221e-05,
      "loss": 0.6932,
      "step": 621300
    },
    {
      "epoch": 5.670121906708519,
      "grad_norm": 3.8164658546447754,
      "learning_rate": 4.527489841107624e-05,
      "loss": 0.705,
      "step": 621400
    },
    {
      "epoch": 5.671034382071684,
      "grad_norm": 4.715008735656738,
      "learning_rate": 4.527413801494026e-05,
      "loss": 0.6818,
      "step": 621500
    },
    {
      "epoch": 5.6719468574348495,
      "grad_norm": 4.839920520782471,
      "learning_rate": 4.52733776188043e-05,
      "loss": 0.758,
      "step": 621600
    },
    {
      "epoch": 5.672859332798015,
      "grad_norm": 4.239664554595947,
      "learning_rate": 4.5272617222668323e-05,
      "loss": 0.7259,
      "step": 621700
    },
    {
      "epoch": 5.673771808161179,
      "grad_norm": 3.8106601238250732,
      "learning_rate": 4.5271856826532353e-05,
      "loss": 0.7068,
      "step": 621800
    },
    {
      "epoch": 5.6746842835243445,
      "grad_norm": 4.3178534507751465,
      "learning_rate": 4.5271096430396384e-05,
      "loss": 0.708,
      "step": 621900
    },
    {
      "epoch": 5.67559675888751,
      "grad_norm": 3.9406630992889404,
      "learning_rate": 4.527033603426041e-05,
      "loss": 0.728,
      "step": 622000
    },
    {
      "epoch": 5.676509234250675,
      "grad_norm": 4.7838006019592285,
      "learning_rate": 4.526957563812444e-05,
      "loss": 0.7054,
      "step": 622100
    },
    {
      "epoch": 5.67742170961384,
      "grad_norm": 2.829152822494507,
      "learning_rate": 4.526881524198847e-05,
      "loss": 0.65,
      "step": 622200
    },
    {
      "epoch": 5.678334184977006,
      "grad_norm": 3.5475635528564453,
      "learning_rate": 4.52680548458525e-05,
      "loss": 0.6986,
      "step": 622300
    },
    {
      "epoch": 5.679246660340171,
      "grad_norm": 4.249719619750977,
      "learning_rate": 4.526729444971653e-05,
      "loss": 0.7238,
      "step": 622400
    },
    {
      "epoch": 5.680159135703336,
      "grad_norm": 4.477529048919678,
      "learning_rate": 4.526653405358056e-05,
      "loss": 0.7237,
      "step": 622500
    },
    {
      "epoch": 5.681071611066502,
      "grad_norm": 4.669459342956543,
      "learning_rate": 4.526577365744458e-05,
      "loss": 0.7271,
      "step": 622600
    },
    {
      "epoch": 5.681984086429666,
      "grad_norm": 3.8158376216888428,
      "learning_rate": 4.526501326130862e-05,
      "loss": 0.7155,
      "step": 622700
    },
    {
      "epoch": 5.682896561792831,
      "grad_norm": 4.201997756958008,
      "learning_rate": 4.526425286517264e-05,
      "loss": 0.6817,
      "step": 622800
    },
    {
      "epoch": 5.683809037155997,
      "grad_norm": 4.837583541870117,
      "learning_rate": 4.526349246903667e-05,
      "loss": 0.7419,
      "step": 622900
    },
    {
      "epoch": 5.684721512519162,
      "grad_norm": 4.227145671844482,
      "learning_rate": 4.52627320729007e-05,
      "loss": 0.7431,
      "step": 623000
    },
    {
      "epoch": 5.685633987882327,
      "grad_norm": 3.7822091579437256,
      "learning_rate": 4.526197167676473e-05,
      "loss": 0.7439,
      "step": 623100
    },
    {
      "epoch": 5.6865464632454925,
      "grad_norm": 3.691699266433716,
      "learning_rate": 4.5261211280628754e-05,
      "loss": 0.6577,
      "step": 623200
    },
    {
      "epoch": 5.687458938608658,
      "grad_norm": 3.835225820541382,
      "learning_rate": 4.526045088449279e-05,
      "loss": 0.6833,
      "step": 623300
    },
    {
      "epoch": 5.688371413971823,
      "grad_norm": 3.6950955390930176,
      "learning_rate": 4.5259690488356814e-05,
      "loss": 0.7489,
      "step": 623400
    },
    {
      "epoch": 5.6892838893349875,
      "grad_norm": 4.83299446105957,
      "learning_rate": 4.5258930092220844e-05,
      "loss": 0.6907,
      "step": 623500
    },
    {
      "epoch": 5.690196364698153,
      "grad_norm": 4.508047103881836,
      "learning_rate": 4.5258169696084874e-05,
      "loss": 0.6968,
      "step": 623600
    },
    {
      "epoch": 5.691108840061318,
      "grad_norm": 4.412517547607422,
      "learning_rate": 4.5257409299948904e-05,
      "loss": 0.7336,
      "step": 623700
    },
    {
      "epoch": 5.692021315424483,
      "grad_norm": 4.663590908050537,
      "learning_rate": 4.5256648903812935e-05,
      "loss": 0.7296,
      "step": 623800
    },
    {
      "epoch": 5.692933790787649,
      "grad_norm": 4.8465986251831055,
      "learning_rate": 4.5255888507676965e-05,
      "loss": 0.7041,
      "step": 623900
    },
    {
      "epoch": 5.693846266150814,
      "grad_norm": 3.5110530853271484,
      "learning_rate": 4.525512811154099e-05,
      "loss": 0.6932,
      "step": 624000
    },
    {
      "epoch": 5.694758741513979,
      "grad_norm": 4.495093822479248,
      "learning_rate": 4.5254367715405025e-05,
      "loss": 0.7242,
      "step": 624100
    },
    {
      "epoch": 5.695671216877145,
      "grad_norm": 3.6819112300872803,
      "learning_rate": 4.525360731926905e-05,
      "loss": 0.6998,
      "step": 624200
    },
    {
      "epoch": 5.69658369224031,
      "grad_norm": 4.329512596130371,
      "learning_rate": 4.525284692313308e-05,
      "loss": 0.7118,
      "step": 624300
    },
    {
      "epoch": 5.697496167603474,
      "grad_norm": 3.42266583442688,
      "learning_rate": 4.525208652699711e-05,
      "loss": 0.7408,
      "step": 624400
    },
    {
      "epoch": 5.69840864296664,
      "grad_norm": 3.520447254180908,
      "learning_rate": 4.525132613086114e-05,
      "loss": 0.7233,
      "step": 624500
    },
    {
      "epoch": 5.699321118329805,
      "grad_norm": 4.145502090454102,
      "learning_rate": 4.525056573472516e-05,
      "loss": 0.7146,
      "step": 624600
    },
    {
      "epoch": 5.70023359369297,
      "grad_norm": 3.639826536178589,
      "learning_rate": 4.52498053385892e-05,
      "loss": 0.6932,
      "step": 624700
    },
    {
      "epoch": 5.7011460690561355,
      "grad_norm": 4.801036357879639,
      "learning_rate": 4.524904494245322e-05,
      "loss": 0.6971,
      "step": 624800
    },
    {
      "epoch": 5.702058544419301,
      "grad_norm": 3.986302375793457,
      "learning_rate": 4.524828454631725e-05,
      "loss": 0.7173,
      "step": 624900
    },
    {
      "epoch": 5.702971019782466,
      "grad_norm": 3.933016061782837,
      "learning_rate": 4.524752415018128e-05,
      "loss": 0.6854,
      "step": 625000
    },
    {
      "epoch": 5.703883495145631,
      "grad_norm": 4.238475799560547,
      "learning_rate": 4.5246763754045305e-05,
      "loss": 0.7169,
      "step": 625100
    },
    {
      "epoch": 5.704795970508796,
      "grad_norm": 3.5646724700927734,
      "learning_rate": 4.524600335790934e-05,
      "loss": 0.7156,
      "step": 625200
    },
    {
      "epoch": 5.705708445871961,
      "grad_norm": 3.939121961593628,
      "learning_rate": 4.5245242961773365e-05,
      "loss": 0.6458,
      "step": 625300
    },
    {
      "epoch": 5.706620921235126,
      "grad_norm": 4.710325717926025,
      "learning_rate": 4.5244482565637395e-05,
      "loss": 0.6781,
      "step": 625400
    },
    {
      "epoch": 5.707533396598292,
      "grad_norm": 4.50378942489624,
      "learning_rate": 4.5243722169501425e-05,
      "loss": 0.7367,
      "step": 625500
    },
    {
      "epoch": 5.708445871961457,
      "grad_norm": 4.719475269317627,
      "learning_rate": 4.5242961773365455e-05,
      "loss": 0.7205,
      "step": 625600
    },
    {
      "epoch": 5.709358347324622,
      "grad_norm": 4.397597789764404,
      "learning_rate": 4.524220137722948e-05,
      "loss": 0.7097,
      "step": 625700
    },
    {
      "epoch": 5.710270822687788,
      "grad_norm": 6.071201324462891,
      "learning_rate": 4.5241440981093516e-05,
      "loss": 0.7377,
      "step": 625800
    },
    {
      "epoch": 5.711183298050953,
      "grad_norm": 4.003875732421875,
      "learning_rate": 4.524068058495754e-05,
      "loss": 0.6911,
      "step": 625900
    },
    {
      "epoch": 5.712095773414118,
      "grad_norm": 4.310327529907227,
      "learning_rate": 4.523992018882157e-05,
      "loss": 0.7322,
      "step": 626000
    },
    {
      "epoch": 5.713008248777283,
      "grad_norm": 4.051178455352783,
      "learning_rate": 4.52391597926856e-05,
      "loss": 0.7445,
      "step": 626100
    },
    {
      "epoch": 5.713920724140448,
      "grad_norm": 4.101213455200195,
      "learning_rate": 4.523839939654963e-05,
      "loss": 0.7209,
      "step": 626200
    },
    {
      "epoch": 5.714833199503613,
      "grad_norm": 4.649744510650635,
      "learning_rate": 4.523763900041366e-05,
      "loss": 0.7247,
      "step": 626300
    },
    {
      "epoch": 5.7157456748667785,
      "grad_norm": 4.625921249389648,
      "learning_rate": 4.523687860427769e-05,
      "loss": 0.6904,
      "step": 626400
    },
    {
      "epoch": 5.716658150229944,
      "grad_norm": 3.5706045627593994,
      "learning_rate": 4.523611820814171e-05,
      "loss": 0.7501,
      "step": 626500
    },
    {
      "epoch": 5.717570625593109,
      "grad_norm": 4.068434715270996,
      "learning_rate": 4.523535781200575e-05,
      "loss": 0.7245,
      "step": 626600
    },
    {
      "epoch": 5.718483100956274,
      "grad_norm": 4.198266983032227,
      "learning_rate": 4.523459741586977e-05,
      "loss": 0.7205,
      "step": 626700
    },
    {
      "epoch": 5.71939557631944,
      "grad_norm": 4.707707405090332,
      "learning_rate": 4.52338370197338e-05,
      "loss": 0.6673,
      "step": 626800
    },
    {
      "epoch": 5.720308051682604,
      "grad_norm": 4.397327899932861,
      "learning_rate": 4.523307662359783e-05,
      "loss": 0.7011,
      "step": 626900
    },
    {
      "epoch": 5.721220527045769,
      "grad_norm": 3.99684476852417,
      "learning_rate": 4.523231622746186e-05,
      "loss": 0.7312,
      "step": 627000
    },
    {
      "epoch": 5.722133002408935,
      "grad_norm": 4.047439098358154,
      "learning_rate": 4.5231555831325886e-05,
      "loss": 0.701,
      "step": 627100
    },
    {
      "epoch": 5.7230454777721,
      "grad_norm": 3.8656861782073975,
      "learning_rate": 4.523079543518992e-05,
      "loss": 0.6881,
      "step": 627200
    },
    {
      "epoch": 5.723957953135265,
      "grad_norm": 3.7669665813446045,
      "learning_rate": 4.5230035039053946e-05,
      "loss": 0.6923,
      "step": 627300
    },
    {
      "epoch": 5.724870428498431,
      "grad_norm": 4.2809858322143555,
      "learning_rate": 4.5229274642917976e-05,
      "loss": 0.7412,
      "step": 627400
    },
    {
      "epoch": 5.725782903861596,
      "grad_norm": 4.042063236236572,
      "learning_rate": 4.5228514246782006e-05,
      "loss": 0.7066,
      "step": 627500
    },
    {
      "epoch": 5.726695379224761,
      "grad_norm": 3.7039225101470947,
      "learning_rate": 4.5227753850646036e-05,
      "loss": 0.6793,
      "step": 627600
    },
    {
      "epoch": 5.7276078545879265,
      "grad_norm": 3.728667974472046,
      "learning_rate": 4.5226993454510066e-05,
      "loss": 0.68,
      "step": 627700
    },
    {
      "epoch": 5.728520329951091,
      "grad_norm": 4.316210746765137,
      "learning_rate": 4.522623305837409e-05,
      "loss": 0.6887,
      "step": 627800
    },
    {
      "epoch": 5.729432805314256,
      "grad_norm": 3.359546422958374,
      "learning_rate": 4.522547266223812e-05,
      "loss": 0.7054,
      "step": 627900
    },
    {
      "epoch": 5.7303452806774215,
      "grad_norm": 4.821741104125977,
      "learning_rate": 4.522471226610215e-05,
      "loss": 0.6901,
      "step": 628000
    },
    {
      "epoch": 5.731257756040587,
      "grad_norm": 4.4535603523254395,
      "learning_rate": 4.522395186996618e-05,
      "loss": 0.7277,
      "step": 628100
    },
    {
      "epoch": 5.732170231403752,
      "grad_norm": 3.702260971069336,
      "learning_rate": 4.52231914738302e-05,
      "loss": 0.6987,
      "step": 628200
    },
    {
      "epoch": 5.7330827067669174,
      "grad_norm": 4.958098888397217,
      "learning_rate": 4.522243107769424e-05,
      "loss": 0.7012,
      "step": 628300
    },
    {
      "epoch": 5.733995182130083,
      "grad_norm": 4.066014289855957,
      "learning_rate": 4.522167068155826e-05,
      "loss": 0.7475,
      "step": 628400
    },
    {
      "epoch": 5.734907657493248,
      "grad_norm": 3.7059061527252197,
      "learning_rate": 4.5220910285422293e-05,
      "loss": 0.7486,
      "step": 628500
    },
    {
      "epoch": 5.7358201328564125,
      "grad_norm": 4.454682350158691,
      "learning_rate": 4.5220149889286324e-05,
      "loss": 0.6801,
      "step": 628600
    },
    {
      "epoch": 5.736732608219578,
      "grad_norm": 4.057403564453125,
      "learning_rate": 4.5219389493150354e-05,
      "loss": 0.7023,
      "step": 628700
    },
    {
      "epoch": 5.737645083582743,
      "grad_norm": 3.065054416656494,
      "learning_rate": 4.5218629097014384e-05,
      "loss": 0.7162,
      "step": 628800
    },
    {
      "epoch": 5.738557558945908,
      "grad_norm": 3.795058012008667,
      "learning_rate": 4.5217868700878414e-05,
      "loss": 0.6976,
      "step": 628900
    },
    {
      "epoch": 5.739470034309074,
      "grad_norm": 4.182286262512207,
      "learning_rate": 4.521710830474244e-05,
      "loss": 0.73,
      "step": 629000
    },
    {
      "epoch": 5.740382509672239,
      "grad_norm": 4.948084354400635,
      "learning_rate": 4.5216347908606474e-05,
      "loss": 0.7213,
      "step": 629100
    },
    {
      "epoch": 5.741294985035404,
      "grad_norm": 4.242585182189941,
      "learning_rate": 4.52155875124705e-05,
      "loss": 0.7145,
      "step": 629200
    },
    {
      "epoch": 5.7422074603985696,
      "grad_norm": 4.282761573791504,
      "learning_rate": 4.521482711633453e-05,
      "loss": 0.718,
      "step": 629300
    },
    {
      "epoch": 5.743119935761735,
      "grad_norm": 3.6872754096984863,
      "learning_rate": 4.521406672019856e-05,
      "loss": 0.7206,
      "step": 629400
    },
    {
      "epoch": 5.744032411124899,
      "grad_norm": 3.737082004547119,
      "learning_rate": 4.521330632406259e-05,
      "loss": 0.7226,
      "step": 629500
    },
    {
      "epoch": 5.744944886488065,
      "grad_norm": 3.19468092918396,
      "learning_rate": 4.521254592792661e-05,
      "loss": 0.7199,
      "step": 629600
    },
    {
      "epoch": 5.74585736185123,
      "grad_norm": 4.045685291290283,
      "learning_rate": 4.521178553179065e-05,
      "loss": 0.737,
      "step": 629700
    },
    {
      "epoch": 5.746769837214395,
      "grad_norm": 5.159409999847412,
      "learning_rate": 4.521102513565467e-05,
      "loss": 0.7251,
      "step": 629800
    },
    {
      "epoch": 5.7476823125775605,
      "grad_norm": 3.645781993865967,
      "learning_rate": 4.52102647395187e-05,
      "loss": 0.6645,
      "step": 629900
    },
    {
      "epoch": 5.748594787940726,
      "grad_norm": 4.143540859222412,
      "learning_rate": 4.520950434338273e-05,
      "loss": 0.6843,
      "step": 630000
    },
    {
      "epoch": 5.749507263303891,
      "grad_norm": 3.8903462886810303,
      "learning_rate": 4.520874394724676e-05,
      "loss": 0.7067,
      "step": 630100
    },
    {
      "epoch": 5.750419738667056,
      "grad_norm": 4.1069655418396,
      "learning_rate": 4.520798355111079e-05,
      "loss": 0.7161,
      "step": 630200
    },
    {
      "epoch": 5.751332214030221,
      "grad_norm": 4.572902679443359,
      "learning_rate": 4.520722315497482e-05,
      "loss": 0.6858,
      "step": 630300
    },
    {
      "epoch": 5.752244689393386,
      "grad_norm": 3.9547598361968994,
      "learning_rate": 4.5206462758838844e-05,
      "loss": 0.6975,
      "step": 630400
    },
    {
      "epoch": 5.753157164756551,
      "grad_norm": 4.150738716125488,
      "learning_rate": 4.520570236270288e-05,
      "loss": 0.669,
      "step": 630500
    },
    {
      "epoch": 5.754069640119717,
      "grad_norm": 3.8758833408355713,
      "learning_rate": 4.5204941966566905e-05,
      "loss": 0.7353,
      "step": 630600
    },
    {
      "epoch": 5.754982115482882,
      "grad_norm": 2.5643184185028076,
      "learning_rate": 4.5204181570430935e-05,
      "loss": 0.7091,
      "step": 630700
    },
    {
      "epoch": 5.755894590846047,
      "grad_norm": 4.053369045257568,
      "learning_rate": 4.5203421174294965e-05,
      "loss": 0.6849,
      "step": 630800
    },
    {
      "epoch": 5.756807066209213,
      "grad_norm": 4.5410895347595215,
      "learning_rate": 4.520266077815899e-05,
      "loss": 0.7405,
      "step": 630900
    },
    {
      "epoch": 5.757719541572378,
      "grad_norm": 3.775200605392456,
      "learning_rate": 4.520190038202302e-05,
      "loss": 0.6958,
      "step": 631000
    },
    {
      "epoch": 5.758632016935543,
      "grad_norm": 3.5761802196502686,
      "learning_rate": 4.520113998588705e-05,
      "loss": 0.712,
      "step": 631100
    },
    {
      "epoch": 5.759544492298708,
      "grad_norm": 4.827253818511963,
      "learning_rate": 4.520037958975108e-05,
      "loss": 0.7163,
      "step": 631200
    },
    {
      "epoch": 5.760456967661873,
      "grad_norm": 4.223709583282471,
      "learning_rate": 4.519961919361511e-05,
      "loss": 0.7285,
      "step": 631300
    },
    {
      "epoch": 5.761369443025038,
      "grad_norm": 4.016237735748291,
      "learning_rate": 4.519885879747914e-05,
      "loss": 0.7392,
      "step": 631400
    },
    {
      "epoch": 5.7622819183882035,
      "grad_norm": 4.382522106170654,
      "learning_rate": 4.519809840134316e-05,
      "loss": 0.7192,
      "step": 631500
    },
    {
      "epoch": 5.763194393751369,
      "grad_norm": 3.6976943016052246,
      "learning_rate": 4.51973380052072e-05,
      "loss": 0.7405,
      "step": 631600
    },
    {
      "epoch": 5.764106869114534,
      "grad_norm": 3.923222303390503,
      "learning_rate": 4.519657760907122e-05,
      "loss": 0.7121,
      "step": 631700
    },
    {
      "epoch": 5.765019344477699,
      "grad_norm": 3.5736823081970215,
      "learning_rate": 4.519581721293525e-05,
      "loss": 0.6982,
      "step": 631800
    },
    {
      "epoch": 5.765931819840865,
      "grad_norm": 3.9814014434814453,
      "learning_rate": 4.519505681679928e-05,
      "loss": 0.7075,
      "step": 631900
    },
    {
      "epoch": 5.766844295204029,
      "grad_norm": 4.34462833404541,
      "learning_rate": 4.519429642066331e-05,
      "loss": 0.7129,
      "step": 632000
    },
    {
      "epoch": 5.767756770567194,
      "grad_norm": 5.205185413360596,
      "learning_rate": 4.519353602452734e-05,
      "loss": 0.6811,
      "step": 632100
    },
    {
      "epoch": 5.76866924593036,
      "grad_norm": 4.063653469085693,
      "learning_rate": 4.519277562839137e-05,
      "loss": 0.7292,
      "step": 632200
    },
    {
      "epoch": 5.769581721293525,
      "grad_norm": 3.903305768966675,
      "learning_rate": 4.5192015232255395e-05,
      "loss": 0.7601,
      "step": 632300
    },
    {
      "epoch": 5.77049419665669,
      "grad_norm": 3.9404122829437256,
      "learning_rate": 4.5191254836119425e-05,
      "loss": 0.6842,
      "step": 632400
    },
    {
      "epoch": 5.771406672019856,
      "grad_norm": 3.595114231109619,
      "learning_rate": 4.5190494439983455e-05,
      "loss": 0.7461,
      "step": 632500
    },
    {
      "epoch": 5.772319147383021,
      "grad_norm": 3.9004170894622803,
      "learning_rate": 4.5189734043847486e-05,
      "loss": 0.6826,
      "step": 632600
    },
    {
      "epoch": 5.773231622746186,
      "grad_norm": 2.979531764984131,
      "learning_rate": 4.5188973647711516e-05,
      "loss": 0.7329,
      "step": 632700
    },
    {
      "epoch": 5.7741440981093515,
      "grad_norm": 3.611650228500366,
      "learning_rate": 4.5188213251575546e-05,
      "loss": 0.7011,
      "step": 632800
    },
    {
      "epoch": 5.775056573472516,
      "grad_norm": 4.482967376708984,
      "learning_rate": 4.518745285543957e-05,
      "loss": 0.7213,
      "step": 632900
    },
    {
      "epoch": 5.775969048835681,
      "grad_norm": 4.428642272949219,
      "learning_rate": 4.5186692459303606e-05,
      "loss": 0.7278,
      "step": 633000
    },
    {
      "epoch": 5.7768815241988465,
      "grad_norm": 3.2174155712127686,
      "learning_rate": 4.518593206316763e-05,
      "loss": 0.7233,
      "step": 633100
    },
    {
      "epoch": 5.777793999562012,
      "grad_norm": 5.020467281341553,
      "learning_rate": 4.518517166703166e-05,
      "loss": 0.7099,
      "step": 633200
    },
    {
      "epoch": 5.778706474925177,
      "grad_norm": 3.4945626258850098,
      "learning_rate": 4.518441127089569e-05,
      "loss": 0.7016,
      "step": 633300
    },
    {
      "epoch": 5.779618950288342,
      "grad_norm": 3.9721720218658447,
      "learning_rate": 4.518365087475971e-05,
      "loss": 0.7079,
      "step": 633400
    },
    {
      "epoch": 5.780531425651508,
      "grad_norm": 4.291757583618164,
      "learning_rate": 4.518289047862375e-05,
      "loss": 0.7187,
      "step": 633500
    },
    {
      "epoch": 5.781443901014673,
      "grad_norm": 3.404372453689575,
      "learning_rate": 4.518213008248777e-05,
      "loss": 0.7321,
      "step": 633600
    },
    {
      "epoch": 5.782356376377837,
      "grad_norm": 4.45369291305542,
      "learning_rate": 4.51813696863518e-05,
      "loss": 0.7207,
      "step": 633700
    },
    {
      "epoch": 5.783268851741003,
      "grad_norm": 4.984447956085205,
      "learning_rate": 4.518060929021583e-05,
      "loss": 0.729,
      "step": 633800
    },
    {
      "epoch": 5.784181327104168,
      "grad_norm": 5.107897758483887,
      "learning_rate": 4.517984889407986e-05,
      "loss": 0.6905,
      "step": 633900
    },
    {
      "epoch": 5.785093802467333,
      "grad_norm": 3.497434377670288,
      "learning_rate": 4.5179088497943886e-05,
      "loss": 0.7157,
      "step": 634000
    },
    {
      "epoch": 5.786006277830499,
      "grad_norm": 2.786180257797241,
      "learning_rate": 4.517832810180792e-05,
      "loss": 0.6998,
      "step": 634100
    },
    {
      "epoch": 5.786918753193664,
      "grad_norm": 4.737015724182129,
      "learning_rate": 4.5177567705671946e-05,
      "loss": 0.7576,
      "step": 634200
    },
    {
      "epoch": 5.787831228556829,
      "grad_norm": 3.837176561355591,
      "learning_rate": 4.5176807309535976e-05,
      "loss": 0.7087,
      "step": 634300
    },
    {
      "epoch": 5.7887437039199945,
      "grad_norm": 3.9397664070129395,
      "learning_rate": 4.5176046913400006e-05,
      "loss": 0.6844,
      "step": 634400
    },
    {
      "epoch": 5.78965617928316,
      "grad_norm": 4.431549549102783,
      "learning_rate": 4.5175286517264036e-05,
      "loss": 0.7097,
      "step": 634500
    },
    {
      "epoch": 5.790568654646324,
      "grad_norm": 4.106360912322998,
      "learning_rate": 4.5174526121128067e-05,
      "loss": 0.7069,
      "step": 634600
    },
    {
      "epoch": 5.7914811300094895,
      "grad_norm": 3.6760590076446533,
      "learning_rate": 4.51737657249921e-05,
      "loss": 0.7291,
      "step": 634700
    },
    {
      "epoch": 5.792393605372655,
      "grad_norm": 3.3029651641845703,
      "learning_rate": 4.517300532885612e-05,
      "loss": 0.7127,
      "step": 634800
    },
    {
      "epoch": 5.79330608073582,
      "grad_norm": 4.123733043670654,
      "learning_rate": 4.517224493272016e-05,
      "loss": 0.7216,
      "step": 634900
    },
    {
      "epoch": 5.794218556098985,
      "grad_norm": 3.9727330207824707,
      "learning_rate": 4.517148453658418e-05,
      "loss": 0.7204,
      "step": 635000
    },
    {
      "epoch": 5.795131031462151,
      "grad_norm": 4.451687335968018,
      "learning_rate": 4.517072414044821e-05,
      "loss": 0.6864,
      "step": 635100
    },
    {
      "epoch": 5.796043506825316,
      "grad_norm": 3.5610828399658203,
      "learning_rate": 4.516996374431224e-05,
      "loss": 0.7086,
      "step": 635200
    },
    {
      "epoch": 5.796955982188481,
      "grad_norm": 3.770270586013794,
      "learning_rate": 4.516920334817627e-05,
      "loss": 0.728,
      "step": 635300
    },
    {
      "epoch": 5.797868457551646,
      "grad_norm": 4.4407124519348145,
      "learning_rate": 4.5168442952040294e-05,
      "loss": 0.7128,
      "step": 635400
    },
    {
      "epoch": 5.798780932914811,
      "grad_norm": 4.340844631195068,
      "learning_rate": 4.516768255590433e-05,
      "loss": 0.7115,
      "step": 635500
    },
    {
      "epoch": 5.799693408277976,
      "grad_norm": 3.9150757789611816,
      "learning_rate": 4.5166922159768354e-05,
      "loss": 0.7343,
      "step": 635600
    },
    {
      "epoch": 5.800605883641142,
      "grad_norm": 4.324230194091797,
      "learning_rate": 4.5166161763632384e-05,
      "loss": 0.6885,
      "step": 635700
    },
    {
      "epoch": 5.801518359004307,
      "grad_norm": 3.8186404705047607,
      "learning_rate": 4.5165401367496414e-05,
      "loss": 0.6469,
      "step": 635800
    },
    {
      "epoch": 5.802430834367472,
      "grad_norm": 2.9375877380371094,
      "learning_rate": 4.5164640971360444e-05,
      "loss": 0.705,
      "step": 635900
    },
    {
      "epoch": 5.8033433097306375,
      "grad_norm": 3.431105852127075,
      "learning_rate": 4.5163880575224474e-05,
      "loss": 0.7099,
      "step": 636000
    },
    {
      "epoch": 5.804255785093803,
      "grad_norm": 4.189877986907959,
      "learning_rate": 4.5163120179088504e-05,
      "loss": 0.7059,
      "step": 636100
    },
    {
      "epoch": 5.805168260456968,
      "grad_norm": 3.6476943492889404,
      "learning_rate": 4.516235978295253e-05,
      "loss": 0.662,
      "step": 636200
    },
    {
      "epoch": 5.8060807358201325,
      "grad_norm": 3.6535568237304688,
      "learning_rate": 4.516159938681656e-05,
      "loss": 0.7016,
      "step": 636300
    },
    {
      "epoch": 5.806993211183298,
      "grad_norm": 3.963336229324341,
      "learning_rate": 4.516083899068059e-05,
      "loss": 0.7339,
      "step": 636400
    },
    {
      "epoch": 5.807905686546463,
      "grad_norm": 3.1404995918273926,
      "learning_rate": 4.516007859454461e-05,
      "loss": 0.7195,
      "step": 636500
    },
    {
      "epoch": 5.808818161909628,
      "grad_norm": 4.086606025695801,
      "learning_rate": 4.515931819840865e-05,
      "loss": 0.7074,
      "step": 636600
    },
    {
      "epoch": 5.809730637272794,
      "grad_norm": 4.493069648742676,
      "learning_rate": 4.515855780227267e-05,
      "loss": 0.7183,
      "step": 636700
    },
    {
      "epoch": 5.810643112635959,
      "grad_norm": 3.5181143283843994,
      "learning_rate": 4.51577974061367e-05,
      "loss": 0.6975,
      "step": 636800
    },
    {
      "epoch": 5.811555587999124,
      "grad_norm": 2.817087411880493,
      "learning_rate": 4.515703701000073e-05,
      "loss": 0.689,
      "step": 636900
    },
    {
      "epoch": 5.81246806336229,
      "grad_norm": 3.4661357402801514,
      "learning_rate": 4.515627661386476e-05,
      "loss": 0.7136,
      "step": 637000
    },
    {
      "epoch": 5.813380538725454,
      "grad_norm": 4.872366905212402,
      "learning_rate": 4.515551621772879e-05,
      "loss": 0.717,
      "step": 637100
    },
    {
      "epoch": 5.814293014088619,
      "grad_norm": 4.146517276763916,
      "learning_rate": 4.515475582159282e-05,
      "loss": 0.7786,
      "step": 637200
    },
    {
      "epoch": 5.815205489451785,
      "grad_norm": 6.737903594970703,
      "learning_rate": 4.5153995425456844e-05,
      "loss": 0.6952,
      "step": 637300
    },
    {
      "epoch": 5.81611796481495,
      "grad_norm": 4.050350189208984,
      "learning_rate": 4.515323502932088e-05,
      "loss": 0.7064,
      "step": 637400
    },
    {
      "epoch": 5.817030440178115,
      "grad_norm": 3.595327615737915,
      "learning_rate": 4.5152474633184905e-05,
      "loss": 0.7001,
      "step": 637500
    },
    {
      "epoch": 5.8179429155412805,
      "grad_norm": 4.015720367431641,
      "learning_rate": 4.5151714237048935e-05,
      "loss": 0.7009,
      "step": 637600
    },
    {
      "epoch": 5.818855390904446,
      "grad_norm": 3.5225508213043213,
      "learning_rate": 4.5150953840912965e-05,
      "loss": 0.74,
      "step": 637700
    },
    {
      "epoch": 5.819767866267611,
      "grad_norm": 4.6375908851623535,
      "learning_rate": 4.5150193444776995e-05,
      "loss": 0.7258,
      "step": 637800
    },
    {
      "epoch": 5.820680341630776,
      "grad_norm": 2.222691535949707,
      "learning_rate": 4.514943304864102e-05,
      "loss": 0.7282,
      "step": 637900
    },
    {
      "epoch": 5.821592816993941,
      "grad_norm": 3.8551104068756104,
      "learning_rate": 4.5148672652505055e-05,
      "loss": 0.7184,
      "step": 638000
    },
    {
      "epoch": 5.822505292357106,
      "grad_norm": 3.2167904376983643,
      "learning_rate": 4.514791225636908e-05,
      "loss": 0.6966,
      "step": 638100
    },
    {
      "epoch": 5.823417767720271,
      "grad_norm": 5.1817803382873535,
      "learning_rate": 4.514715186023311e-05,
      "loss": 0.741,
      "step": 638200
    },
    {
      "epoch": 5.824330243083437,
      "grad_norm": 4.594319820404053,
      "learning_rate": 4.514639146409714e-05,
      "loss": 0.6802,
      "step": 638300
    },
    {
      "epoch": 5.825242718446602,
      "grad_norm": 3.9494950771331787,
      "learning_rate": 4.514563106796117e-05,
      "loss": 0.6836,
      "step": 638400
    },
    {
      "epoch": 5.826155193809767,
      "grad_norm": 4.597925186157227,
      "learning_rate": 4.51448706718252e-05,
      "loss": 0.7362,
      "step": 638500
    },
    {
      "epoch": 5.827067669172933,
      "grad_norm": 3.498577833175659,
      "learning_rate": 4.514411027568923e-05,
      "loss": 0.6868,
      "step": 638600
    },
    {
      "epoch": 5.827980144536097,
      "grad_norm": 4.318527698516846,
      "learning_rate": 4.514334987955325e-05,
      "loss": 0.7169,
      "step": 638700
    },
    {
      "epoch": 5.828892619899262,
      "grad_norm": 3.9103736877441406,
      "learning_rate": 4.514258948341729e-05,
      "loss": 0.6921,
      "step": 638800
    },
    {
      "epoch": 5.829805095262428,
      "grad_norm": 4.305480003356934,
      "learning_rate": 4.514182908728131e-05,
      "loss": 0.7129,
      "step": 638900
    },
    {
      "epoch": 5.830717570625593,
      "grad_norm": 3.26253342628479,
      "learning_rate": 4.514106869114534e-05,
      "loss": 0.7034,
      "step": 639000
    },
    {
      "epoch": 5.831630045988758,
      "grad_norm": 3.886066436767578,
      "learning_rate": 4.514030829500937e-05,
      "loss": 0.6963,
      "step": 639100
    },
    {
      "epoch": 5.8325425213519235,
      "grad_norm": 4.0704474449157715,
      "learning_rate": 4.5139547898873395e-05,
      "loss": 0.6697,
      "step": 639200
    },
    {
      "epoch": 5.833454996715089,
      "grad_norm": 3.282294988632202,
      "learning_rate": 4.5138787502737426e-05,
      "loss": 0.7075,
      "step": 639300
    },
    {
      "epoch": 5.834367472078254,
      "grad_norm": 3.9784369468688965,
      "learning_rate": 4.5138027106601456e-05,
      "loss": 0.716,
      "step": 639400
    },
    {
      "epoch": 5.835279947441419,
      "grad_norm": 3.9394290447235107,
      "learning_rate": 4.5137266710465486e-05,
      "loss": 0.7098,
      "step": 639500
    },
    {
      "epoch": 5.836192422804585,
      "grad_norm": 3.0493388175964355,
      "learning_rate": 4.5136506314329516e-05,
      "loss": 0.7045,
      "step": 639600
    },
    {
      "epoch": 5.837104898167749,
      "grad_norm": 4.9796319007873535,
      "learning_rate": 4.5135745918193546e-05,
      "loss": 0.7213,
      "step": 639700
    },
    {
      "epoch": 5.838017373530914,
      "grad_norm": 4.706948280334473,
      "learning_rate": 4.513498552205757e-05,
      "loss": 0.6934,
      "step": 639800
    },
    {
      "epoch": 5.83892984889408,
      "grad_norm": 3.8728740215301514,
      "learning_rate": 4.5134225125921606e-05,
      "loss": 0.6888,
      "step": 639900
    },
    {
      "epoch": 5.839842324257245,
      "grad_norm": 4.4040093421936035,
      "learning_rate": 4.513346472978563e-05,
      "loss": 0.693,
      "step": 640000
    },
    {
      "epoch": 5.84075479962041,
      "grad_norm": 4.350600719451904,
      "learning_rate": 4.513270433364966e-05,
      "loss": 0.6962,
      "step": 640100
    },
    {
      "epoch": 5.841667274983576,
      "grad_norm": 3.656294822692871,
      "learning_rate": 4.513194393751369e-05,
      "loss": 0.721,
      "step": 640200
    },
    {
      "epoch": 5.842579750346741,
      "grad_norm": 4.151819229125977,
      "learning_rate": 4.513118354137772e-05,
      "loss": 0.6899,
      "step": 640300
    },
    {
      "epoch": 5.843492225709905,
      "grad_norm": 3.9801418781280518,
      "learning_rate": 4.513042314524174e-05,
      "loss": 0.7006,
      "step": 640400
    },
    {
      "epoch": 5.844404701073071,
      "grad_norm": 3.786748170852661,
      "learning_rate": 4.512966274910578e-05,
      "loss": 0.7287,
      "step": 640500
    },
    {
      "epoch": 5.845317176436236,
      "grad_norm": 2.8772828578948975,
      "learning_rate": 4.51289023529698e-05,
      "loss": 0.7222,
      "step": 640600
    },
    {
      "epoch": 5.846229651799401,
      "grad_norm": 5.02664852142334,
      "learning_rate": 4.512814195683383e-05,
      "loss": 0.7312,
      "step": 640700
    },
    {
      "epoch": 5.8471421271625665,
      "grad_norm": 3.864832878112793,
      "learning_rate": 4.512738156069786e-05,
      "loss": 0.691,
      "step": 640800
    },
    {
      "epoch": 5.848054602525732,
      "grad_norm": 4.0499267578125,
      "learning_rate": 4.512662116456189e-05,
      "loss": 0.7403,
      "step": 640900
    },
    {
      "epoch": 5.848967077888897,
      "grad_norm": 4.747018337249756,
      "learning_rate": 4.512586076842592e-05,
      "loss": 0.771,
      "step": 641000
    },
    {
      "epoch": 5.849879553252062,
      "grad_norm": 5.068048000335693,
      "learning_rate": 4.512510037228995e-05,
      "loss": 0.6958,
      "step": 641100
    },
    {
      "epoch": 5.850792028615228,
      "grad_norm": 3.451937675476074,
      "learning_rate": 4.5124339976153976e-05,
      "loss": 0.7289,
      "step": 641200
    },
    {
      "epoch": 5.851704503978393,
      "grad_norm": 3.1804299354553223,
      "learning_rate": 4.512357958001801e-05,
      "loss": 0.7055,
      "step": 641300
    },
    {
      "epoch": 5.8526169793415574,
      "grad_norm": 3.80085825920105,
      "learning_rate": 4.5122819183882037e-05,
      "loss": 0.7294,
      "step": 641400
    },
    {
      "epoch": 5.853529454704723,
      "grad_norm": 2.394021511077881,
      "learning_rate": 4.512205878774607e-05,
      "loss": 0.6778,
      "step": 641500
    },
    {
      "epoch": 5.854441930067888,
      "grad_norm": 2.9655544757843018,
      "learning_rate": 4.51212983916101e-05,
      "loss": 0.6738,
      "step": 641600
    },
    {
      "epoch": 5.855354405431053,
      "grad_norm": 4.709043502807617,
      "learning_rate": 4.512053799547413e-05,
      "loss": 0.6778,
      "step": 641700
    },
    {
      "epoch": 5.856266880794219,
      "grad_norm": 3.6440024375915527,
      "learning_rate": 4.511977759933815e-05,
      "loss": 0.6986,
      "step": 641800
    },
    {
      "epoch": 5.857179356157384,
      "grad_norm": 4.1992316246032715,
      "learning_rate": 4.511901720320218e-05,
      "loss": 0.7408,
      "step": 641900
    },
    {
      "epoch": 5.858091831520549,
      "grad_norm": 4.03857946395874,
      "learning_rate": 4.511825680706621e-05,
      "loss": 0.7128,
      "step": 642000
    },
    {
      "epoch": 5.859004306883714,
      "grad_norm": 4.126353740692139,
      "learning_rate": 4.511749641093024e-05,
      "loss": 0.6753,
      "step": 642100
    },
    {
      "epoch": 5.859916782246879,
      "grad_norm": 4.58637809753418,
      "learning_rate": 4.511673601479427e-05,
      "loss": 0.7158,
      "step": 642200
    },
    {
      "epoch": 5.860829257610044,
      "grad_norm": 4.50922966003418,
      "learning_rate": 4.5115975618658294e-05,
      "loss": 0.6798,
      "step": 642300
    },
    {
      "epoch": 5.8617417329732096,
      "grad_norm": 3.831784963607788,
      "learning_rate": 4.511521522252233e-05,
      "loss": 0.7196,
      "step": 642400
    },
    {
      "epoch": 5.862654208336375,
      "grad_norm": 4.82299280166626,
      "learning_rate": 4.5114454826386354e-05,
      "loss": 0.7444,
      "step": 642500
    },
    {
      "epoch": 5.86356668369954,
      "grad_norm": 4.098358154296875,
      "learning_rate": 4.5113694430250384e-05,
      "loss": 0.6984,
      "step": 642600
    },
    {
      "epoch": 5.8644791590627054,
      "grad_norm": 4.118490219116211,
      "learning_rate": 4.5112934034114414e-05,
      "loss": 0.6857,
      "step": 642700
    },
    {
      "epoch": 5.865391634425871,
      "grad_norm": 4.2081298828125,
      "learning_rate": 4.5112173637978444e-05,
      "loss": 0.6976,
      "step": 642800
    },
    {
      "epoch": 5.866304109789036,
      "grad_norm": 4.034109115600586,
      "learning_rate": 4.511141324184247e-05,
      "loss": 0.7043,
      "step": 642900
    },
    {
      "epoch": 5.867216585152201,
      "grad_norm": 4.071435451507568,
      "learning_rate": 4.5110652845706504e-05,
      "loss": 0.7415,
      "step": 643000
    },
    {
      "epoch": 5.868129060515366,
      "grad_norm": 5.408768653869629,
      "learning_rate": 4.510989244957053e-05,
      "loss": 0.702,
      "step": 643100
    },
    {
      "epoch": 5.869041535878531,
      "grad_norm": 3.925532579421997,
      "learning_rate": 4.510913205343456e-05,
      "loss": 0.7045,
      "step": 643200
    },
    {
      "epoch": 5.869954011241696,
      "grad_norm": 4.269707202911377,
      "learning_rate": 4.510837165729859e-05,
      "loss": 0.6887,
      "step": 643300
    },
    {
      "epoch": 5.870866486604862,
      "grad_norm": 4.657344818115234,
      "learning_rate": 4.510761126116262e-05,
      "loss": 0.7078,
      "step": 643400
    },
    {
      "epoch": 5.871778961968027,
      "grad_norm": 3.886918544769287,
      "learning_rate": 4.510685086502665e-05,
      "loss": 0.7026,
      "step": 643500
    },
    {
      "epoch": 5.872691437331192,
      "grad_norm": 4.404752254486084,
      "learning_rate": 4.510609046889068e-05,
      "loss": 0.7039,
      "step": 643600
    },
    {
      "epoch": 5.8736039126943576,
      "grad_norm": 3.6790976524353027,
      "learning_rate": 4.51053300727547e-05,
      "loss": 0.6905,
      "step": 643700
    },
    {
      "epoch": 5.874516388057522,
      "grad_norm": 3.079735517501831,
      "learning_rate": 4.510456967661874e-05,
      "loss": 0.7208,
      "step": 643800
    },
    {
      "epoch": 5.875428863420687,
      "grad_norm": 4.1439619064331055,
      "learning_rate": 4.510380928048276e-05,
      "loss": 0.6453,
      "step": 643900
    },
    {
      "epoch": 5.876341338783853,
      "grad_norm": 3.355517864227295,
      "learning_rate": 4.510304888434679e-05,
      "loss": 0.7104,
      "step": 644000
    },
    {
      "epoch": 5.877253814147018,
      "grad_norm": 3.5205748081207275,
      "learning_rate": 4.510228848821082e-05,
      "loss": 0.6967,
      "step": 644100
    },
    {
      "epoch": 5.878166289510183,
      "grad_norm": 3.9368972778320312,
      "learning_rate": 4.510152809207485e-05,
      "loss": 0.7639,
      "step": 644200
    },
    {
      "epoch": 5.8790787648733485,
      "grad_norm": 4.168262958526611,
      "learning_rate": 4.510076769593888e-05,
      "loss": 0.6715,
      "step": 644300
    },
    {
      "epoch": 5.879991240236514,
      "grad_norm": 3.939626932144165,
      "learning_rate": 4.510000729980291e-05,
      "loss": 0.6608,
      "step": 644400
    },
    {
      "epoch": 5.880903715599679,
      "grad_norm": 4.248400688171387,
      "learning_rate": 4.5099246903666935e-05,
      "loss": 0.7179,
      "step": 644500
    },
    {
      "epoch": 5.881816190962844,
      "grad_norm": 3.8784143924713135,
      "learning_rate": 4.5098486507530965e-05,
      "loss": 0.6897,
      "step": 644600
    },
    {
      "epoch": 5.88272866632601,
      "grad_norm": 4.9606499671936035,
      "learning_rate": 4.5097726111394995e-05,
      "loss": 0.6894,
      "step": 644700
    },
    {
      "epoch": 5.883641141689174,
      "grad_norm": 4.05197811126709,
      "learning_rate": 4.509696571525902e-05,
      "loss": 0.6951,
      "step": 644800
    },
    {
      "epoch": 5.884553617052339,
      "grad_norm": 4.751295566558838,
      "learning_rate": 4.5096205319123055e-05,
      "loss": 0.7047,
      "step": 644900
    },
    {
      "epoch": 5.885466092415505,
      "grad_norm": 4.872359275817871,
      "learning_rate": 4.509544492298708e-05,
      "loss": 0.701,
      "step": 645000
    },
    {
      "epoch": 5.88637856777867,
      "grad_norm": 3.9854345321655273,
      "learning_rate": 4.509468452685111e-05,
      "loss": 0.6933,
      "step": 645100
    },
    {
      "epoch": 5.887291043141835,
      "grad_norm": 4.151093482971191,
      "learning_rate": 4.509392413071514e-05,
      "loss": 0.6688,
      "step": 645200
    },
    {
      "epoch": 5.888203518505001,
      "grad_norm": 3.227677583694458,
      "learning_rate": 4.509316373457917e-05,
      "loss": 0.6976,
      "step": 645300
    },
    {
      "epoch": 5.889115993868166,
      "grad_norm": 3.828230857849121,
      "learning_rate": 4.50924033384432e-05,
      "loss": 0.6935,
      "step": 645400
    },
    {
      "epoch": 5.89002846923133,
      "grad_norm": 3.7528765201568604,
      "learning_rate": 4.509164294230723e-05,
      "loss": 0.7494,
      "step": 645500
    },
    {
      "epoch": 5.890940944594496,
      "grad_norm": 3.5217556953430176,
      "learning_rate": 4.509088254617125e-05,
      "loss": 0.717,
      "step": 645600
    },
    {
      "epoch": 5.891853419957661,
      "grad_norm": 4.602451801300049,
      "learning_rate": 4.509012215003529e-05,
      "loss": 0.6931,
      "step": 645700
    },
    {
      "epoch": 5.892765895320826,
      "grad_norm": 3.0352280139923096,
      "learning_rate": 4.508936175389931e-05,
      "loss": 0.7078,
      "step": 645800
    },
    {
      "epoch": 5.8936783706839915,
      "grad_norm": 4.492842674255371,
      "learning_rate": 4.508860135776334e-05,
      "loss": 0.7179,
      "step": 645900
    },
    {
      "epoch": 5.894590846047157,
      "grad_norm": 4.684051036834717,
      "learning_rate": 4.508784096162737e-05,
      "loss": 0.6946,
      "step": 646000
    },
    {
      "epoch": 5.895503321410322,
      "grad_norm": 4.126565933227539,
      "learning_rate": 4.50870805654914e-05,
      "loss": 0.7215,
      "step": 646100
    },
    {
      "epoch": 5.896415796773487,
      "grad_norm": 3.9941487312316895,
      "learning_rate": 4.5086320169355426e-05,
      "loss": 0.718,
      "step": 646200
    },
    {
      "epoch": 5.897328272136653,
      "grad_norm": 5.586701393127441,
      "learning_rate": 4.508555977321946e-05,
      "loss": 0.7061,
      "step": 646300
    },
    {
      "epoch": 5.898240747499818,
      "grad_norm": 3.9510130882263184,
      "learning_rate": 4.5084799377083486e-05,
      "loss": 0.706,
      "step": 646400
    },
    {
      "epoch": 5.899153222862982,
      "grad_norm": 3.7619411945343018,
      "learning_rate": 4.5084038980947516e-05,
      "loss": 0.7019,
      "step": 646500
    },
    {
      "epoch": 5.900065698226148,
      "grad_norm": 4.290578842163086,
      "learning_rate": 4.5083278584811546e-05,
      "loss": 0.7194,
      "step": 646600
    },
    {
      "epoch": 5.900978173589313,
      "grad_norm": 3.835458278656006,
      "learning_rate": 4.5082518188675576e-05,
      "loss": 0.715,
      "step": 646700
    },
    {
      "epoch": 5.901890648952478,
      "grad_norm": 4.063616752624512,
      "learning_rate": 4.5081757792539606e-05,
      "loss": 0.7012,
      "step": 646800
    },
    {
      "epoch": 5.902803124315644,
      "grad_norm": 3.8508665561676025,
      "learning_rate": 4.5080997396403636e-05,
      "loss": 0.7032,
      "step": 646900
    },
    {
      "epoch": 5.903715599678809,
      "grad_norm": 3.847777843475342,
      "learning_rate": 4.508023700026766e-05,
      "loss": 0.717,
      "step": 647000
    },
    {
      "epoch": 5.904628075041974,
      "grad_norm": 4.718547821044922,
      "learning_rate": 4.5079476604131696e-05,
      "loss": 0.6734,
      "step": 647100
    },
    {
      "epoch": 5.905540550405139,
      "grad_norm": 3.8734490871429443,
      "learning_rate": 4.507871620799572e-05,
      "loss": 0.7274,
      "step": 647200
    },
    {
      "epoch": 5.906453025768304,
      "grad_norm": 3.7980775833129883,
      "learning_rate": 4.507795581185975e-05,
      "loss": 0.7002,
      "step": 647300
    },
    {
      "epoch": 5.907365501131469,
      "grad_norm": 4.205161094665527,
      "learning_rate": 4.507719541572378e-05,
      "loss": 0.7332,
      "step": 647400
    },
    {
      "epoch": 5.9082779764946345,
      "grad_norm": 3.348069429397583,
      "learning_rate": 4.507643501958781e-05,
      "loss": 0.6956,
      "step": 647500
    },
    {
      "epoch": 5.9091904518578,
      "grad_norm": 2.9662439823150635,
      "learning_rate": 4.507567462345183e-05,
      "loss": 0.672,
      "step": 647600
    },
    {
      "epoch": 5.910102927220965,
      "grad_norm": 3.769141674041748,
      "learning_rate": 4.507491422731586e-05,
      "loss": 0.7121,
      "step": 647700
    },
    {
      "epoch": 5.91101540258413,
      "grad_norm": 3.9051103591918945,
      "learning_rate": 4.507415383117989e-05,
      "loss": 0.6853,
      "step": 647800
    },
    {
      "epoch": 5.911927877947296,
      "grad_norm": 4.541888236999512,
      "learning_rate": 4.507339343504392e-05,
      "loss": 0.7486,
      "step": 647900
    },
    {
      "epoch": 5.912840353310461,
      "grad_norm": 5.020726203918457,
      "learning_rate": 4.507263303890795e-05,
      "loss": 0.6983,
      "step": 648000
    },
    {
      "epoch": 5.913752828673626,
      "grad_norm": 4.62569522857666,
      "learning_rate": 4.5071872642771977e-05,
      "loss": 0.7048,
      "step": 648100
    },
    {
      "epoch": 5.914665304036791,
      "grad_norm": 4.225620269775391,
      "learning_rate": 4.5071112246636013e-05,
      "loss": 0.6885,
      "step": 648200
    },
    {
      "epoch": 5.915577779399956,
      "grad_norm": 3.6214990615844727,
      "learning_rate": 4.507035185050004e-05,
      "loss": 0.7014,
      "step": 648300
    },
    {
      "epoch": 5.916490254763121,
      "grad_norm": 4.03452205657959,
      "learning_rate": 4.506959145436407e-05,
      "loss": 0.688,
      "step": 648400
    },
    {
      "epoch": 5.917402730126287,
      "grad_norm": 3.7022664546966553,
      "learning_rate": 4.50688310582281e-05,
      "loss": 0.7323,
      "step": 648500
    },
    {
      "epoch": 5.918315205489452,
      "grad_norm": 4.422335624694824,
      "learning_rate": 4.506807066209213e-05,
      "loss": 0.6882,
      "step": 648600
    },
    {
      "epoch": 5.919227680852617,
      "grad_norm": 3.8599867820739746,
      "learning_rate": 4.506731026595615e-05,
      "loss": 0.679,
      "step": 648700
    },
    {
      "epoch": 5.9201401562157825,
      "grad_norm": 3.6046364307403564,
      "learning_rate": 4.506654986982019e-05,
      "loss": 0.6906,
      "step": 648800
    },
    {
      "epoch": 5.921052631578947,
      "grad_norm": 3.9705111980438232,
      "learning_rate": 4.506578947368421e-05,
      "loss": 0.7354,
      "step": 648900
    },
    {
      "epoch": 5.921965106942112,
      "grad_norm": 3.9013359546661377,
      "learning_rate": 4.506502907754824e-05,
      "loss": 0.7182,
      "step": 649000
    },
    {
      "epoch": 5.9228775823052775,
      "grad_norm": 4.412858009338379,
      "learning_rate": 4.506426868141227e-05,
      "loss": 0.7087,
      "step": 649100
    },
    {
      "epoch": 5.923790057668443,
      "grad_norm": 3.7135133743286133,
      "learning_rate": 4.50635082852763e-05,
      "loss": 0.6834,
      "step": 649200
    },
    {
      "epoch": 5.924702533031608,
      "grad_norm": 2.8973495960235596,
      "learning_rate": 4.506274788914033e-05,
      "loss": 0.7166,
      "step": 649300
    },
    {
      "epoch": 5.925615008394773,
      "grad_norm": 3.387843132019043,
      "learning_rate": 4.506198749300436e-05,
      "loss": 0.7128,
      "step": 649400
    },
    {
      "epoch": 5.926527483757939,
      "grad_norm": 4.510059356689453,
      "learning_rate": 4.5061227096868384e-05,
      "loss": 0.7253,
      "step": 649500
    },
    {
      "epoch": 5.927439959121104,
      "grad_norm": 4.765639305114746,
      "learning_rate": 4.506046670073242e-05,
      "loss": 0.7797,
      "step": 649600
    },
    {
      "epoch": 5.928352434484269,
      "grad_norm": 4.025637626647949,
      "learning_rate": 4.5059706304596444e-05,
      "loss": 0.6804,
      "step": 649700
    },
    {
      "epoch": 5.929264909847435,
      "grad_norm": 4.290301322937012,
      "learning_rate": 4.5058945908460474e-05,
      "loss": 0.7208,
      "step": 649800
    },
    {
      "epoch": 5.930177385210599,
      "grad_norm": 4.345695495605469,
      "learning_rate": 4.5058185512324504e-05,
      "loss": 0.6959,
      "step": 649900
    },
    {
      "epoch": 5.931089860573764,
      "grad_norm": 3.4124417304992676,
      "learning_rate": 4.5057425116188534e-05,
      "loss": 0.7172,
      "step": 650000
    },
    {
      "epoch": 5.93200233593693,
      "grad_norm": 5.405890941619873,
      "learning_rate": 4.505666472005256e-05,
      "loss": 0.6648,
      "step": 650100
    },
    {
      "epoch": 5.932914811300095,
      "grad_norm": 5.420505046844482,
      "learning_rate": 4.5055904323916594e-05,
      "loss": 0.7166,
      "step": 650200
    },
    {
      "epoch": 5.93382728666326,
      "grad_norm": 4.814931869506836,
      "learning_rate": 4.505514392778062e-05,
      "loss": 0.7282,
      "step": 650300
    },
    {
      "epoch": 5.9347397620264255,
      "grad_norm": 4.307559967041016,
      "learning_rate": 4.505438353164465e-05,
      "loss": 0.6885,
      "step": 650400
    },
    {
      "epoch": 5.935652237389591,
      "grad_norm": 3.897784471511841,
      "learning_rate": 4.505362313550868e-05,
      "loss": 0.6713,
      "step": 650500
    },
    {
      "epoch": 5.936564712752755,
      "grad_norm": 4.025582313537598,
      "learning_rate": 4.50528627393727e-05,
      "loss": 0.7302,
      "step": 650600
    },
    {
      "epoch": 5.9374771881159205,
      "grad_norm": 4.224391937255859,
      "learning_rate": 4.505210234323674e-05,
      "loss": 0.7231,
      "step": 650700
    },
    {
      "epoch": 5.938389663479086,
      "grad_norm": 3.9779958724975586,
      "learning_rate": 4.505134194710076e-05,
      "loss": 0.7538,
      "step": 650800
    },
    {
      "epoch": 5.939302138842251,
      "grad_norm": 4.959900856018066,
      "learning_rate": 4.505058155096479e-05,
      "loss": 0.736,
      "step": 650900
    },
    {
      "epoch": 5.940214614205416,
      "grad_norm": 4.3021392822265625,
      "learning_rate": 4.504982115482882e-05,
      "loss": 0.6926,
      "step": 651000
    },
    {
      "epoch": 5.941127089568582,
      "grad_norm": 3.994959831237793,
      "learning_rate": 4.504906075869285e-05,
      "loss": 0.7504,
      "step": 651100
    },
    {
      "epoch": 5.942039564931747,
      "grad_norm": 3.580780506134033,
      "learning_rate": 4.5048300362556875e-05,
      "loss": 0.7113,
      "step": 651200
    },
    {
      "epoch": 5.942952040294912,
      "grad_norm": 3.966596841812134,
      "learning_rate": 4.504753996642091e-05,
      "loss": 0.6882,
      "step": 651300
    },
    {
      "epoch": 5.943864515658078,
      "grad_norm": 4.324662685394287,
      "learning_rate": 4.5046779570284935e-05,
      "loss": 0.6927,
      "step": 651400
    },
    {
      "epoch": 5.944776991021243,
      "grad_norm": 4.481501579284668,
      "learning_rate": 4.5046019174148965e-05,
      "loss": 0.6962,
      "step": 651500
    },
    {
      "epoch": 5.945689466384407,
      "grad_norm": 3.4542088508605957,
      "learning_rate": 4.5045258778012995e-05,
      "loss": 0.7227,
      "step": 651600
    },
    {
      "epoch": 5.946601941747573,
      "grad_norm": 4.41151237487793,
      "learning_rate": 4.5044498381877025e-05,
      "loss": 0.7814,
      "step": 651700
    },
    {
      "epoch": 5.947514417110738,
      "grad_norm": 3.837771415710449,
      "learning_rate": 4.5043737985741055e-05,
      "loss": 0.7128,
      "step": 651800
    },
    {
      "epoch": 5.948426892473903,
      "grad_norm": 4.48085355758667,
      "learning_rate": 4.5042977589605085e-05,
      "loss": 0.7138,
      "step": 651900
    },
    {
      "epoch": 5.9493393678370685,
      "grad_norm": 3.5802054405212402,
      "learning_rate": 4.504221719346911e-05,
      "loss": 0.6921,
      "step": 652000
    },
    {
      "epoch": 5.950251843200234,
      "grad_norm": 3.8830010890960693,
      "learning_rate": 4.5041456797333145e-05,
      "loss": 0.7247,
      "step": 652100
    },
    {
      "epoch": 5.951164318563399,
      "grad_norm": 4.920314311981201,
      "learning_rate": 4.504069640119717e-05,
      "loss": 0.6957,
      "step": 652200
    },
    {
      "epoch": 5.9520767939265635,
      "grad_norm": 4.614048957824707,
      "learning_rate": 4.50399360050612e-05,
      "loss": 0.7024,
      "step": 652300
    },
    {
      "epoch": 5.952989269289729,
      "grad_norm": 4.643592834472656,
      "learning_rate": 4.503917560892523e-05,
      "loss": 0.7636,
      "step": 652400
    },
    {
      "epoch": 5.953901744652894,
      "grad_norm": 4.938957214355469,
      "learning_rate": 4.503841521278926e-05,
      "loss": 0.7489,
      "step": 652500
    },
    {
      "epoch": 5.954814220016059,
      "grad_norm": 5.008144378662109,
      "learning_rate": 4.503765481665328e-05,
      "loss": 0.7261,
      "step": 652600
    },
    {
      "epoch": 5.955726695379225,
      "grad_norm": 4.368013858795166,
      "learning_rate": 4.503689442051732e-05,
      "loss": 0.6882,
      "step": 652700
    },
    {
      "epoch": 5.95663917074239,
      "grad_norm": 4.494246959686279,
      "learning_rate": 4.503613402438134e-05,
      "loss": 0.748,
      "step": 652800
    },
    {
      "epoch": 5.957551646105555,
      "grad_norm": 3.944718837738037,
      "learning_rate": 4.503537362824537e-05,
      "loss": 0.7455,
      "step": 652900
    },
    {
      "epoch": 5.958464121468721,
      "grad_norm": 3.321545362472534,
      "learning_rate": 4.50346132321094e-05,
      "loss": 0.7161,
      "step": 653000
    },
    {
      "epoch": 5.959376596831886,
      "grad_norm": 4.039681434631348,
      "learning_rate": 4.503385283597343e-05,
      "loss": 0.7488,
      "step": 653100
    },
    {
      "epoch": 5.960289072195051,
      "grad_norm": 3.0901033878326416,
      "learning_rate": 4.503309243983746e-05,
      "loss": 0.7246,
      "step": 653200
    },
    {
      "epoch": 5.961201547558216,
      "grad_norm": 3.8929436206817627,
      "learning_rate": 4.5032332043701486e-05,
      "loss": 0.717,
      "step": 653300
    },
    {
      "epoch": 5.962114022921381,
      "grad_norm": 3.8088672161102295,
      "learning_rate": 4.5031571647565516e-05,
      "loss": 0.6919,
      "step": 653400
    },
    {
      "epoch": 5.963026498284546,
      "grad_norm": 4.115588188171387,
      "learning_rate": 4.5030811251429546e-05,
      "loss": 0.6837,
      "step": 653500
    },
    {
      "epoch": 5.9639389736477115,
      "grad_norm": 4.305291175842285,
      "learning_rate": 4.5030050855293576e-05,
      "loss": 0.7217,
      "step": 653600
    },
    {
      "epoch": 5.964851449010877,
      "grad_norm": 3.775848150253296,
      "learning_rate": 4.50292904591576e-05,
      "loss": 0.7252,
      "step": 653700
    },
    {
      "epoch": 5.965763924374042,
      "grad_norm": 4.690791606903076,
      "learning_rate": 4.5028530063021636e-05,
      "loss": 0.6824,
      "step": 653800
    },
    {
      "epoch": 5.966676399737207,
      "grad_norm": 4.126237392425537,
      "learning_rate": 4.502776966688566e-05,
      "loss": 0.6821,
      "step": 653900
    },
    {
      "epoch": 5.967588875100372,
      "grad_norm": 4.62702751159668,
      "learning_rate": 4.502700927074969e-05,
      "loss": 0.714,
      "step": 654000
    },
    {
      "epoch": 5.968501350463537,
      "grad_norm": 3.5706870555877686,
      "learning_rate": 4.502624887461372e-05,
      "loss": 0.6975,
      "step": 654100
    },
    {
      "epoch": 5.969413825826702,
      "grad_norm": 4.209135055541992,
      "learning_rate": 4.502548847847775e-05,
      "loss": 0.7134,
      "step": 654200
    },
    {
      "epoch": 5.970326301189868,
      "grad_norm": 3.395967483520508,
      "learning_rate": 4.502472808234178e-05,
      "loss": 0.7036,
      "step": 654300
    },
    {
      "epoch": 5.971238776553033,
      "grad_norm": 4.901859283447266,
      "learning_rate": 4.502396768620581e-05,
      "loss": 0.7304,
      "step": 654400
    },
    {
      "epoch": 5.972151251916198,
      "grad_norm": 4.528417110443115,
      "learning_rate": 4.502320729006983e-05,
      "loss": 0.7345,
      "step": 654500
    },
    {
      "epoch": 5.973063727279364,
      "grad_norm": 3.4251792430877686,
      "learning_rate": 4.502244689393387e-05,
      "loss": 0.7317,
      "step": 654600
    },
    {
      "epoch": 5.973976202642529,
      "grad_norm": 3.5474424362182617,
      "learning_rate": 4.502168649779789e-05,
      "loss": 0.755,
      "step": 654700
    },
    {
      "epoch": 5.974888678005694,
      "grad_norm": 4.031939506530762,
      "learning_rate": 4.502092610166192e-05,
      "loss": 0.6855,
      "step": 654800
    },
    {
      "epoch": 5.9758011533688595,
      "grad_norm": 3.6332454681396484,
      "learning_rate": 4.502016570552595e-05,
      "loss": 0.7351,
      "step": 654900
    },
    {
      "epoch": 5.976713628732024,
      "grad_norm": 4.314332008361816,
      "learning_rate": 4.5019405309389983e-05,
      "loss": 0.707,
      "step": 655000
    },
    {
      "epoch": 5.977626104095189,
      "grad_norm": 4.362024307250977,
      "learning_rate": 4.501864491325401e-05,
      "loss": 0.6823,
      "step": 655100
    },
    {
      "epoch": 5.9785385794583545,
      "grad_norm": 3.7461531162261963,
      "learning_rate": 4.5017884517118044e-05,
      "loss": 0.7097,
      "step": 655200
    },
    {
      "epoch": 5.97945105482152,
      "grad_norm": 3.7027153968811035,
      "learning_rate": 4.501712412098207e-05,
      "loss": 0.7058,
      "step": 655300
    },
    {
      "epoch": 5.980363530184685,
      "grad_norm": 4.001744747161865,
      "learning_rate": 4.50163637248461e-05,
      "loss": 0.6997,
      "step": 655400
    },
    {
      "epoch": 5.98127600554785,
      "grad_norm": 4.157708168029785,
      "learning_rate": 4.501560332871013e-05,
      "loss": 0.7101,
      "step": 655500
    },
    {
      "epoch": 5.982188480911016,
      "grad_norm": 2.9186205863952637,
      "learning_rate": 4.501484293257416e-05,
      "loss": 0.6539,
      "step": 655600
    },
    {
      "epoch": 5.98310095627418,
      "grad_norm": 2.9100112915039062,
      "learning_rate": 4.501408253643819e-05,
      "loss": 0.7075,
      "step": 655700
    },
    {
      "epoch": 5.9840134316373454,
      "grad_norm": 3.885317087173462,
      "learning_rate": 4.501332214030222e-05,
      "loss": 0.7418,
      "step": 655800
    },
    {
      "epoch": 5.984925907000511,
      "grad_norm": 4.449939250946045,
      "learning_rate": 4.501256174416624e-05,
      "loss": 0.6941,
      "step": 655900
    },
    {
      "epoch": 5.985838382363676,
      "grad_norm": 2.999995470046997,
      "learning_rate": 4.501180134803028e-05,
      "loss": 0.7012,
      "step": 656000
    },
    {
      "epoch": 5.986750857726841,
      "grad_norm": 2.75352144241333,
      "learning_rate": 4.50110409518943e-05,
      "loss": 0.7425,
      "step": 656100
    },
    {
      "epoch": 5.987663333090007,
      "grad_norm": 3.7488832473754883,
      "learning_rate": 4.501028055575833e-05,
      "loss": 0.6905,
      "step": 656200
    },
    {
      "epoch": 5.988575808453172,
      "grad_norm": 4.610507011413574,
      "learning_rate": 4.500952015962236e-05,
      "loss": 0.7275,
      "step": 656300
    },
    {
      "epoch": 5.989488283816337,
      "grad_norm": 4.482577800750732,
      "learning_rate": 4.5008759763486384e-05,
      "loss": 0.6794,
      "step": 656400
    },
    {
      "epoch": 5.9904007591795025,
      "grad_norm": 3.2577223777770996,
      "learning_rate": 4.5007999367350414e-05,
      "loss": 0.7269,
      "step": 656500
    },
    {
      "epoch": 5.991313234542668,
      "grad_norm": 3.16910982131958,
      "learning_rate": 4.5007238971214444e-05,
      "loss": 0.6677,
      "step": 656600
    },
    {
      "epoch": 5.992225709905832,
      "grad_norm": 4.148280620574951,
      "learning_rate": 4.5006478575078474e-05,
      "loss": 0.7243,
      "step": 656700
    },
    {
      "epoch": 5.9931381852689976,
      "grad_norm": 3.7035160064697266,
      "learning_rate": 4.5005718178942504e-05,
      "loss": 0.7016,
      "step": 656800
    },
    {
      "epoch": 5.994050660632163,
      "grad_norm": 2.961906909942627,
      "learning_rate": 4.5004957782806534e-05,
      "loss": 0.7223,
      "step": 656900
    },
    {
      "epoch": 5.994963135995328,
      "grad_norm": 3.6731808185577393,
      "learning_rate": 4.500419738667056e-05,
      "loss": 0.6982,
      "step": 657000
    },
    {
      "epoch": 5.9958756113584935,
      "grad_norm": 4.099311351776123,
      "learning_rate": 4.5003436990534595e-05,
      "loss": 0.6913,
      "step": 657100
    },
    {
      "epoch": 5.996788086721659,
      "grad_norm": 5.077387809753418,
      "learning_rate": 4.500267659439862e-05,
      "loss": 0.7255,
      "step": 657200
    },
    {
      "epoch": 5.997700562084824,
      "grad_norm": 4.378941535949707,
      "learning_rate": 4.500191619826265e-05,
      "loss": 0.7062,
      "step": 657300
    },
    {
      "epoch": 5.9986130374479885,
      "grad_norm": 3.182438373565674,
      "learning_rate": 4.500115580212668e-05,
      "loss": 0.675,
      "step": 657400
    },
    {
      "epoch": 5.999525512811154,
      "grad_norm": 4.703143119812012,
      "learning_rate": 4.500039540599071e-05,
      "loss": 0.694,
      "step": 657500
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5727429389953613,
      "eval_runtime": 25.3522,
      "eval_samples_per_second": 227.554,
      "eval_steps_per_second": 227.554,
      "step": 657552
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5548979640007019,
      "eval_runtime": 481.8366,
      "eval_samples_per_second": 227.446,
      "eval_steps_per_second": 227.446,
      "step": 657552
    },
    {
      "epoch": 6.000437988174319,
      "grad_norm": 4.783563613891602,
      "learning_rate": 4.499963500985474e-05,
      "loss": 0.6991,
      "step": 657600
    },
    {
      "epoch": 6.001350463537484,
      "grad_norm": 3.5358874797821045,
      "learning_rate": 4.499887461371877e-05,
      "loss": 0.6921,
      "step": 657700
    },
    {
      "epoch": 6.00226293890065,
      "grad_norm": 4.66499137878418,
      "learning_rate": 4.499811421758279e-05,
      "loss": 0.7339,
      "step": 657800
    },
    {
      "epoch": 6.003175414263815,
      "grad_norm": 4.518305778503418,
      "learning_rate": 4.499735382144682e-05,
      "loss": 0.7176,
      "step": 657900
    },
    {
      "epoch": 6.00408788962698,
      "grad_norm": 4.6463398933410645,
      "learning_rate": 4.499659342531085e-05,
      "loss": 0.6993,
      "step": 658000
    },
    {
      "epoch": 6.005000364990146,
      "grad_norm": 4.144606113433838,
      "learning_rate": 4.499583302917488e-05,
      "loss": 0.6944,
      "step": 658100
    },
    {
      "epoch": 6.005912840353311,
      "grad_norm": 3.6703386306762695,
      "learning_rate": 4.499507263303891e-05,
      "loss": 0.6986,
      "step": 658200
    },
    {
      "epoch": 6.006825315716475,
      "grad_norm": 4.164997577667236,
      "learning_rate": 4.499431223690294e-05,
      "loss": 0.6927,
      "step": 658300
    },
    {
      "epoch": 6.007737791079641,
      "grad_norm": 3.661802053451538,
      "learning_rate": 4.4993551840766965e-05,
      "loss": 0.718,
      "step": 658400
    },
    {
      "epoch": 6.008650266442806,
      "grad_norm": 2.764716148376465,
      "learning_rate": 4.4992791444631e-05,
      "loss": 0.718,
      "step": 658500
    },
    {
      "epoch": 6.009562741805971,
      "grad_norm": 4.018468856811523,
      "learning_rate": 4.4992031048495025e-05,
      "loss": 0.7331,
      "step": 658600
    },
    {
      "epoch": 6.0104752171691365,
      "grad_norm": 3.6831448078155518,
      "learning_rate": 4.4991270652359055e-05,
      "loss": 0.7152,
      "step": 658700
    },
    {
      "epoch": 6.011387692532302,
      "grad_norm": 4.277090549468994,
      "learning_rate": 4.4990510256223085e-05,
      "loss": 0.7277,
      "step": 658800
    },
    {
      "epoch": 6.012300167895467,
      "grad_norm": 3.6548216342926025,
      "learning_rate": 4.4989749860087115e-05,
      "loss": 0.6957,
      "step": 658900
    },
    {
      "epoch": 6.013212643258632,
      "grad_norm": 4.546633243560791,
      "learning_rate": 4.4988989463951145e-05,
      "loss": 0.7043,
      "step": 659000
    },
    {
      "epoch": 6.014125118621797,
      "grad_norm": 4.153248310089111,
      "learning_rate": 4.498822906781517e-05,
      "loss": 0.687,
      "step": 659100
    },
    {
      "epoch": 6.015037593984962,
      "grad_norm": 3.5150065422058105,
      "learning_rate": 4.49874686716792e-05,
      "loss": 0.6965,
      "step": 659200
    },
    {
      "epoch": 6.015950069348127,
      "grad_norm": 4.066493988037109,
      "learning_rate": 4.498670827554323e-05,
      "loss": 0.6674,
      "step": 659300
    },
    {
      "epoch": 6.016862544711293,
      "grad_norm": 3.4785330295562744,
      "learning_rate": 4.498594787940726e-05,
      "loss": 0.6944,
      "step": 659400
    },
    {
      "epoch": 6.017775020074458,
      "grad_norm": 3.9681379795074463,
      "learning_rate": 4.498518748327128e-05,
      "loss": 0.7331,
      "step": 659500
    },
    {
      "epoch": 6.018687495437623,
      "grad_norm": 3.9086427688598633,
      "learning_rate": 4.498442708713532e-05,
      "loss": 0.6967,
      "step": 659600
    },
    {
      "epoch": 6.019599970800789,
      "grad_norm": 4.246266841888428,
      "learning_rate": 4.498366669099934e-05,
      "loss": 0.6758,
      "step": 659700
    },
    {
      "epoch": 6.020512446163954,
      "grad_norm": 4.139445781707764,
      "learning_rate": 4.498290629486337e-05,
      "loss": 0.7104,
      "step": 659800
    },
    {
      "epoch": 6.021424921527119,
      "grad_norm": 3.4282784461975098,
      "learning_rate": 4.49821458987274e-05,
      "loss": 0.6982,
      "step": 659900
    },
    {
      "epoch": 6.022337396890284,
      "grad_norm": 3.6035754680633545,
      "learning_rate": 4.498138550259143e-05,
      "loss": 0.6919,
      "step": 660000
    },
    {
      "epoch": 6.023249872253449,
      "grad_norm": 4.314492225646973,
      "learning_rate": 4.498062510645546e-05,
      "loss": 0.7064,
      "step": 660100
    },
    {
      "epoch": 6.024162347616614,
      "grad_norm": 6.613515853881836,
      "learning_rate": 4.497986471031949e-05,
      "loss": 0.7355,
      "step": 660200
    },
    {
      "epoch": 6.0250748229797795,
      "grad_norm": 2.380028009414673,
      "learning_rate": 4.4979104314183516e-05,
      "loss": 0.6714,
      "step": 660300
    },
    {
      "epoch": 6.025987298342945,
      "grad_norm": 4.289198875427246,
      "learning_rate": 4.497834391804755e-05,
      "loss": 0.7185,
      "step": 660400
    },
    {
      "epoch": 6.02689977370611,
      "grad_norm": 4.558221817016602,
      "learning_rate": 4.4977583521911576e-05,
      "loss": 0.7125,
      "step": 660500
    },
    {
      "epoch": 6.027812249069275,
      "grad_norm": 3.9230141639709473,
      "learning_rate": 4.4976823125775606e-05,
      "loss": 0.7138,
      "step": 660600
    },
    {
      "epoch": 6.028724724432441,
      "grad_norm": 4.103537082672119,
      "learning_rate": 4.4976062729639636e-05,
      "loss": 0.7185,
      "step": 660700
    },
    {
      "epoch": 6.029637199795605,
      "grad_norm": 3.9496376514434814,
      "learning_rate": 4.4975302333503666e-05,
      "loss": 0.701,
      "step": 660800
    },
    {
      "epoch": 6.03054967515877,
      "grad_norm": 3.8787951469421387,
      "learning_rate": 4.497454193736769e-05,
      "loss": 0.6735,
      "step": 660900
    },
    {
      "epoch": 6.031462150521936,
      "grad_norm": 4.196895122528076,
      "learning_rate": 4.4973781541231726e-05,
      "loss": 0.7208,
      "step": 661000
    },
    {
      "epoch": 6.032374625885101,
      "grad_norm": 5.211982727050781,
      "learning_rate": 4.497302114509575e-05,
      "loss": 0.7276,
      "step": 661100
    },
    {
      "epoch": 6.033287101248266,
      "grad_norm": 3.8485350608825684,
      "learning_rate": 4.497226074895978e-05,
      "loss": 0.6935,
      "step": 661200
    },
    {
      "epoch": 6.034199576611432,
      "grad_norm": 4.1597065925598145,
      "learning_rate": 4.497150035282381e-05,
      "loss": 0.6858,
      "step": 661300
    },
    {
      "epoch": 6.035112051974597,
      "grad_norm": 4.071682453155518,
      "learning_rate": 4.497073995668784e-05,
      "loss": 0.7271,
      "step": 661400
    },
    {
      "epoch": 6.036024527337762,
      "grad_norm": 4.773095607757568,
      "learning_rate": 4.496997956055187e-05,
      "loss": 0.7,
      "step": 661500
    },
    {
      "epoch": 6.0369370027009275,
      "grad_norm": 4.471066474914551,
      "learning_rate": 4.49692191644159e-05,
      "loss": 0.7299,
      "step": 661600
    },
    {
      "epoch": 6.037849478064092,
      "grad_norm": 4.508399963378906,
      "learning_rate": 4.4968458768279923e-05,
      "loss": 0.683,
      "step": 661700
    },
    {
      "epoch": 6.038761953427257,
      "grad_norm": 3.3818976879119873,
      "learning_rate": 4.4967698372143953e-05,
      "loss": 0.7044,
      "step": 661800
    },
    {
      "epoch": 6.0396744287904225,
      "grad_norm": 3.931917667388916,
      "learning_rate": 4.4966937976007984e-05,
      "loss": 0.7229,
      "step": 661900
    },
    {
      "epoch": 6.040586904153588,
      "grad_norm": 3.664498805999756,
      "learning_rate": 4.496617757987201e-05,
      "loss": 0.7166,
      "step": 662000
    },
    {
      "epoch": 6.041499379516753,
      "grad_norm": 4.524367809295654,
      "learning_rate": 4.4965417183736044e-05,
      "loss": 0.6816,
      "step": 662100
    },
    {
      "epoch": 6.042411854879918,
      "grad_norm": 4.35421895980835,
      "learning_rate": 4.496465678760007e-05,
      "loss": 0.6873,
      "step": 662200
    },
    {
      "epoch": 6.043324330243084,
      "grad_norm": 4.378675937652588,
      "learning_rate": 4.49638963914641e-05,
      "loss": 0.7312,
      "step": 662300
    },
    {
      "epoch": 6.044236805606249,
      "grad_norm": 3.0799684524536133,
      "learning_rate": 4.496313599532813e-05,
      "loss": 0.7142,
      "step": 662400
    },
    {
      "epoch": 6.045149280969413,
      "grad_norm": 3.6518898010253906,
      "learning_rate": 4.496237559919216e-05,
      "loss": 0.6972,
      "step": 662500
    },
    {
      "epoch": 6.046061756332579,
      "grad_norm": 3.615407705307007,
      "learning_rate": 4.496161520305619e-05,
      "loss": 0.7232,
      "step": 662600
    },
    {
      "epoch": 6.046974231695744,
      "grad_norm": 4.290450572967529,
      "learning_rate": 4.496085480692022e-05,
      "loss": 0.7183,
      "step": 662700
    },
    {
      "epoch": 6.047886707058909,
      "grad_norm": 3.8253438472747803,
      "learning_rate": 4.496009441078424e-05,
      "loss": 0.693,
      "step": 662800
    },
    {
      "epoch": 6.048799182422075,
      "grad_norm": 4.1921563148498535,
      "learning_rate": 4.495933401464828e-05,
      "loss": 0.7004,
      "step": 662900
    },
    {
      "epoch": 6.04971165778524,
      "grad_norm": 3.8258495330810547,
      "learning_rate": 4.49585736185123e-05,
      "loss": 0.7372,
      "step": 663000
    },
    {
      "epoch": 6.050624133148405,
      "grad_norm": 4.0829315185546875,
      "learning_rate": 4.495781322237633e-05,
      "loss": 0.6953,
      "step": 663100
    },
    {
      "epoch": 6.0515366085115705,
      "grad_norm": 3.842067003250122,
      "learning_rate": 4.495705282624036e-05,
      "loss": 0.7074,
      "step": 663200
    },
    {
      "epoch": 6.052449083874736,
      "grad_norm": 3.6429474353790283,
      "learning_rate": 4.495629243010439e-05,
      "loss": 0.7155,
      "step": 663300
    },
    {
      "epoch": 6.0533615592379,
      "grad_norm": 5.087008476257324,
      "learning_rate": 4.4955532033968414e-05,
      "loss": 0.7062,
      "step": 663400
    },
    {
      "epoch": 6.0542740346010655,
      "grad_norm": 4.26231050491333,
      "learning_rate": 4.495477163783245e-05,
      "loss": 0.7302,
      "step": 663500
    },
    {
      "epoch": 6.055186509964231,
      "grad_norm": 4.090521812438965,
      "learning_rate": 4.4954011241696474e-05,
      "loss": 0.7461,
      "step": 663600
    },
    {
      "epoch": 6.056098985327396,
      "grad_norm": 3.772939920425415,
      "learning_rate": 4.4953250845560504e-05,
      "loss": 0.6708,
      "step": 663700
    },
    {
      "epoch": 6.057011460690561,
      "grad_norm": 4.486741065979004,
      "learning_rate": 4.4952490449424534e-05,
      "loss": 0.7228,
      "step": 663800
    },
    {
      "epoch": 6.057923936053727,
      "grad_norm": 3.910576343536377,
      "learning_rate": 4.4951730053288565e-05,
      "loss": 0.7032,
      "step": 663900
    },
    {
      "epoch": 6.058836411416892,
      "grad_norm": 4.624390602111816,
      "learning_rate": 4.4950969657152595e-05,
      "loss": 0.6887,
      "step": 664000
    },
    {
      "epoch": 6.059748886780057,
      "grad_norm": 3.792123794555664,
      "learning_rate": 4.4950209261016625e-05,
      "loss": 0.7097,
      "step": 664100
    },
    {
      "epoch": 6.060661362143222,
      "grad_norm": 4.456132411956787,
      "learning_rate": 4.494944886488065e-05,
      "loss": 0.7253,
      "step": 664200
    },
    {
      "epoch": 6.061573837506387,
      "grad_norm": 4.185311794281006,
      "learning_rate": 4.4948688468744685e-05,
      "loss": 0.6944,
      "step": 664300
    },
    {
      "epoch": 6.062486312869552,
      "grad_norm": 4.967162609100342,
      "learning_rate": 4.494792807260871e-05,
      "loss": 0.7195,
      "step": 664400
    },
    {
      "epoch": 6.063398788232718,
      "grad_norm": 4.3143696784973145,
      "learning_rate": 4.494716767647274e-05,
      "loss": 0.6933,
      "step": 664500
    },
    {
      "epoch": 6.064311263595883,
      "grad_norm": 4.527093887329102,
      "learning_rate": 4.494640728033677e-05,
      "loss": 0.7252,
      "step": 664600
    },
    {
      "epoch": 6.065223738959048,
      "grad_norm": 4.943764686584473,
      "learning_rate": 4.494564688420079e-05,
      "loss": 0.69,
      "step": 664700
    },
    {
      "epoch": 6.0661362143222135,
      "grad_norm": 3.800891399383545,
      "learning_rate": 4.494488648806482e-05,
      "loss": 0.6837,
      "step": 664800
    },
    {
      "epoch": 6.067048689685379,
      "grad_norm": 2.3539681434631348,
      "learning_rate": 4.494412609192885e-05,
      "loss": 0.6958,
      "step": 664900
    },
    {
      "epoch": 6.067961165048544,
      "grad_norm": 4.198674201965332,
      "learning_rate": 4.494336569579288e-05,
      "loss": 0.7004,
      "step": 665000
    },
    {
      "epoch": 6.0688736404117085,
      "grad_norm": 4.267716407775879,
      "learning_rate": 4.494260529965691e-05,
      "loss": 0.6921,
      "step": 665100
    },
    {
      "epoch": 6.069786115774874,
      "grad_norm": 3.441645622253418,
      "learning_rate": 4.494184490352094e-05,
      "loss": 0.7163,
      "step": 665200
    },
    {
      "epoch": 6.070698591138039,
      "grad_norm": 3.8232948780059814,
      "learning_rate": 4.4941084507384965e-05,
      "loss": 0.7232,
      "step": 665300
    },
    {
      "epoch": 6.071611066501204,
      "grad_norm": 4.500112056732178,
      "learning_rate": 4.4940324111249e-05,
      "loss": 0.6973,
      "step": 665400
    },
    {
      "epoch": 6.07252354186437,
      "grad_norm": 3.188223361968994,
      "learning_rate": 4.4939563715113025e-05,
      "loss": 0.6971,
      "step": 665500
    },
    {
      "epoch": 6.073436017227535,
      "grad_norm": 4.116241931915283,
      "learning_rate": 4.4938803318977055e-05,
      "loss": 0.6945,
      "step": 665600
    },
    {
      "epoch": 6.0743484925907,
      "grad_norm": 4.193192958831787,
      "learning_rate": 4.4938042922841085e-05,
      "loss": 0.6986,
      "step": 665700
    },
    {
      "epoch": 6.075260967953866,
      "grad_norm": 5.12802791595459,
      "learning_rate": 4.4937282526705115e-05,
      "loss": 0.7249,
      "step": 665800
    },
    {
      "epoch": 6.07617344331703,
      "grad_norm": 3.140374183654785,
      "learning_rate": 4.493652213056914e-05,
      "loss": 0.7174,
      "step": 665900
    },
    {
      "epoch": 6.077085918680195,
      "grad_norm": 4.157175064086914,
      "learning_rate": 4.4935761734433176e-05,
      "loss": 0.6946,
      "step": 666000
    },
    {
      "epoch": 6.077998394043361,
      "grad_norm": 3.7353568077087402,
      "learning_rate": 4.49350013382972e-05,
      "loss": 0.6988,
      "step": 666100
    },
    {
      "epoch": 6.078910869406526,
      "grad_norm": 4.670895576477051,
      "learning_rate": 4.493424094216123e-05,
      "loss": 0.7096,
      "step": 666200
    },
    {
      "epoch": 6.079823344769691,
      "grad_norm": 4.302993297576904,
      "learning_rate": 4.493348054602526e-05,
      "loss": 0.7211,
      "step": 666300
    },
    {
      "epoch": 6.0807358201328565,
      "grad_norm": 3.368929386138916,
      "learning_rate": 4.493272014988929e-05,
      "loss": 0.6868,
      "step": 666400
    },
    {
      "epoch": 6.081648295496022,
      "grad_norm": 4.569276809692383,
      "learning_rate": 4.493195975375332e-05,
      "loss": 0.7366,
      "step": 666500
    },
    {
      "epoch": 6.082560770859187,
      "grad_norm": 4.103326797485352,
      "learning_rate": 4.493119935761735e-05,
      "loss": 0.7006,
      "step": 666600
    },
    {
      "epoch": 6.083473246222352,
      "grad_norm": 4.427188396453857,
      "learning_rate": 4.493043896148137e-05,
      "loss": 0.6914,
      "step": 666700
    },
    {
      "epoch": 6.084385721585517,
      "grad_norm": 3.891021251678467,
      "learning_rate": 4.492967856534541e-05,
      "loss": 0.6749,
      "step": 666800
    },
    {
      "epoch": 6.085298196948682,
      "grad_norm": 4.321629524230957,
      "learning_rate": 4.492891816920943e-05,
      "loss": 0.6783,
      "step": 666900
    },
    {
      "epoch": 6.086210672311847,
      "grad_norm": 3.8586976528167725,
      "learning_rate": 4.492815777307346e-05,
      "loss": 0.6805,
      "step": 667000
    },
    {
      "epoch": 6.087123147675013,
      "grad_norm": 4.072860240936279,
      "learning_rate": 4.492739737693749e-05,
      "loss": 0.7262,
      "step": 667100
    },
    {
      "epoch": 6.088035623038178,
      "grad_norm": 5.1797051429748535,
      "learning_rate": 4.492663698080152e-05,
      "loss": 0.682,
      "step": 667200
    },
    {
      "epoch": 6.088948098401343,
      "grad_norm": 3.8488645553588867,
      "learning_rate": 4.4925876584665546e-05,
      "loss": 0.7223,
      "step": 667300
    },
    {
      "epoch": 6.089860573764509,
      "grad_norm": 3.8352904319763184,
      "learning_rate": 4.492511618852958e-05,
      "loss": 0.7185,
      "step": 667400
    },
    {
      "epoch": 6.090773049127674,
      "grad_norm": 4.399896144866943,
      "learning_rate": 4.4924355792393606e-05,
      "loss": 0.6997,
      "step": 667500
    },
    {
      "epoch": 6.091685524490838,
      "grad_norm": 3.4697039127349854,
      "learning_rate": 4.4923595396257636e-05,
      "loss": 0.6933,
      "step": 667600
    },
    {
      "epoch": 6.092597999854004,
      "grad_norm": 3.7955737113952637,
      "learning_rate": 4.4922835000121666e-05,
      "loss": 0.7456,
      "step": 667700
    },
    {
      "epoch": 6.093510475217169,
      "grad_norm": 4.942512512207031,
      "learning_rate": 4.492207460398569e-05,
      "loss": 0.7061,
      "step": 667800
    },
    {
      "epoch": 6.094422950580334,
      "grad_norm": 4.778583526611328,
      "learning_rate": 4.4921314207849727e-05,
      "loss": 0.6941,
      "step": 667900
    },
    {
      "epoch": 6.0953354259434995,
      "grad_norm": 3.9343514442443848,
      "learning_rate": 4.492055381171375e-05,
      "loss": 0.7255,
      "step": 668000
    },
    {
      "epoch": 6.096247901306665,
      "grad_norm": 3.4757912158966064,
      "learning_rate": 4.491979341557778e-05,
      "loss": 0.7091,
      "step": 668100
    },
    {
      "epoch": 6.09716037666983,
      "grad_norm": 3.6782312393188477,
      "learning_rate": 4.491903301944181e-05,
      "loss": 0.7056,
      "step": 668200
    },
    {
      "epoch": 6.098072852032995,
      "grad_norm": 4.697070121765137,
      "learning_rate": 4.491827262330584e-05,
      "loss": 0.6657,
      "step": 668300
    },
    {
      "epoch": 6.098985327396161,
      "grad_norm": 4.083564281463623,
      "learning_rate": 4.491751222716986e-05,
      "loss": 0.6938,
      "step": 668400
    },
    {
      "epoch": 6.099897802759325,
      "grad_norm": 3.9940264225006104,
      "learning_rate": 4.49167518310339e-05,
      "loss": 0.687,
      "step": 668500
    },
    {
      "epoch": 6.10081027812249,
      "grad_norm": 4.367635726928711,
      "learning_rate": 4.4915991434897924e-05,
      "loss": 0.6994,
      "step": 668600
    },
    {
      "epoch": 6.101722753485656,
      "grad_norm": 4.4683613777160645,
      "learning_rate": 4.4915231038761954e-05,
      "loss": 0.671,
      "step": 668700
    },
    {
      "epoch": 6.102635228848821,
      "grad_norm": 5.135393142700195,
      "learning_rate": 4.4914470642625984e-05,
      "loss": 0.71,
      "step": 668800
    },
    {
      "epoch": 6.103547704211986,
      "grad_norm": 3.5945520401000977,
      "learning_rate": 4.4913710246490014e-05,
      "loss": 0.7214,
      "step": 668900
    },
    {
      "epoch": 6.104460179575152,
      "grad_norm": 4.582386016845703,
      "learning_rate": 4.4912949850354044e-05,
      "loss": 0.712,
      "step": 669000
    },
    {
      "epoch": 6.105372654938317,
      "grad_norm": 4.638267993927002,
      "learning_rate": 4.4912189454218074e-05,
      "loss": 0.6973,
      "step": 669100
    },
    {
      "epoch": 6.106285130301482,
      "grad_norm": 3.285783290863037,
      "learning_rate": 4.49114290580821e-05,
      "loss": 0.6788,
      "step": 669200
    },
    {
      "epoch": 6.107197605664647,
      "grad_norm": 4.58412504196167,
      "learning_rate": 4.4910668661946134e-05,
      "loss": 0.6661,
      "step": 669300
    },
    {
      "epoch": 6.108110081027812,
      "grad_norm": 4.020471572875977,
      "learning_rate": 4.490990826581016e-05,
      "loss": 0.7318,
      "step": 669400
    },
    {
      "epoch": 6.109022556390977,
      "grad_norm": 3.299640655517578,
      "learning_rate": 4.490914786967419e-05,
      "loss": 0.6756,
      "step": 669500
    },
    {
      "epoch": 6.1099350317541425,
      "grad_norm": 4.038191318511963,
      "learning_rate": 4.490838747353822e-05,
      "loss": 0.6883,
      "step": 669600
    },
    {
      "epoch": 6.110847507117308,
      "grad_norm": 3.829470634460449,
      "learning_rate": 4.490762707740225e-05,
      "loss": 0.6923,
      "step": 669700
    },
    {
      "epoch": 6.111759982480473,
      "grad_norm": 4.317434787750244,
      "learning_rate": 4.490686668126628e-05,
      "loss": 0.7069,
      "step": 669800
    },
    {
      "epoch": 6.112672457843638,
      "grad_norm": 4.385256290435791,
      "learning_rate": 4.490610628513031e-05,
      "loss": 0.7166,
      "step": 669900
    },
    {
      "epoch": 6.113584933206804,
      "grad_norm": 4.219356536865234,
      "learning_rate": 4.490534588899433e-05,
      "loss": 0.7336,
      "step": 670000
    },
    {
      "epoch": 6.114497408569969,
      "grad_norm": 3.761043071746826,
      "learning_rate": 4.490458549285836e-05,
      "loss": 0.6915,
      "step": 670100
    },
    {
      "epoch": 6.1154098839331335,
      "grad_norm": 3.60140323638916,
      "learning_rate": 4.490382509672239e-05,
      "loss": 0.6638,
      "step": 670200
    },
    {
      "epoch": 6.116322359296299,
      "grad_norm": 4.268237113952637,
      "learning_rate": 4.4903064700586414e-05,
      "loss": 0.719,
      "step": 670300
    },
    {
      "epoch": 6.117234834659464,
      "grad_norm": 6.6377854347229,
      "learning_rate": 4.490230430445045e-05,
      "loss": 0.7036,
      "step": 670400
    },
    {
      "epoch": 6.118147310022629,
      "grad_norm": 4.574065685272217,
      "learning_rate": 4.4901543908314474e-05,
      "loss": 0.7338,
      "step": 670500
    },
    {
      "epoch": 6.119059785385795,
      "grad_norm": 4.556782245635986,
      "learning_rate": 4.4900783512178505e-05,
      "loss": 0.6933,
      "step": 670600
    },
    {
      "epoch": 6.11997226074896,
      "grad_norm": 3.2820208072662354,
      "learning_rate": 4.4900023116042535e-05,
      "loss": 0.7321,
      "step": 670700
    },
    {
      "epoch": 6.120884736112125,
      "grad_norm": 3.9847042560577393,
      "learning_rate": 4.4899262719906565e-05,
      "loss": 0.6724,
      "step": 670800
    },
    {
      "epoch": 6.1217972114752905,
      "grad_norm": 4.106661319732666,
      "learning_rate": 4.4898502323770595e-05,
      "loss": 0.6998,
      "step": 670900
    },
    {
      "epoch": 6.122709686838455,
      "grad_norm": 4.559708118438721,
      "learning_rate": 4.4897741927634625e-05,
      "loss": 0.7027,
      "step": 671000
    },
    {
      "epoch": 6.12362216220162,
      "grad_norm": 3.702141046524048,
      "learning_rate": 4.489698153149865e-05,
      "loss": 0.6869,
      "step": 671100
    },
    {
      "epoch": 6.1245346375647856,
      "grad_norm": 3.8857603073120117,
      "learning_rate": 4.4896221135362685e-05,
      "loss": 0.7021,
      "step": 671200
    },
    {
      "epoch": 6.125447112927951,
      "grad_norm": 4.124631404876709,
      "learning_rate": 4.489546073922671e-05,
      "loss": 0.7292,
      "step": 671300
    },
    {
      "epoch": 6.126359588291116,
      "grad_norm": 4.118966579437256,
      "learning_rate": 4.489470034309074e-05,
      "loss": 0.6987,
      "step": 671400
    },
    {
      "epoch": 6.1272720636542815,
      "grad_norm": 4.260311603546143,
      "learning_rate": 4.489393994695477e-05,
      "loss": 0.662,
      "step": 671500
    },
    {
      "epoch": 6.128184539017447,
      "grad_norm": 4.358447074890137,
      "learning_rate": 4.48931795508188e-05,
      "loss": 0.7166,
      "step": 671600
    },
    {
      "epoch": 6.129097014380612,
      "grad_norm": 4.03070068359375,
      "learning_rate": 4.489241915468282e-05,
      "loss": 0.7102,
      "step": 671700
    },
    {
      "epoch": 6.130009489743777,
      "grad_norm": 3.79685640335083,
      "learning_rate": 4.489165875854686e-05,
      "loss": 0.7186,
      "step": 671800
    },
    {
      "epoch": 6.130921965106942,
      "grad_norm": 3.707993984222412,
      "learning_rate": 4.489089836241088e-05,
      "loss": 0.732,
      "step": 671900
    },
    {
      "epoch": 6.131834440470107,
      "grad_norm": 4.32699728012085,
      "learning_rate": 4.489013796627491e-05,
      "loss": 0.6663,
      "step": 672000
    },
    {
      "epoch": 6.132746915833272,
      "grad_norm": 4.3448944091796875,
      "learning_rate": 4.488937757013894e-05,
      "loss": 0.6973,
      "step": 672100
    },
    {
      "epoch": 6.133659391196438,
      "grad_norm": 4.365200996398926,
      "learning_rate": 4.488861717400297e-05,
      "loss": 0.716,
      "step": 672200
    },
    {
      "epoch": 6.134571866559603,
      "grad_norm": 3.963374137878418,
      "learning_rate": 4.4887856777867e-05,
      "loss": 0.6877,
      "step": 672300
    },
    {
      "epoch": 6.135484341922768,
      "grad_norm": 4.086902618408203,
      "learning_rate": 4.488709638173103e-05,
      "loss": 0.7124,
      "step": 672400
    },
    {
      "epoch": 6.136396817285934,
      "grad_norm": 3.061309337615967,
      "learning_rate": 4.4886335985595055e-05,
      "loss": 0.6787,
      "step": 672500
    },
    {
      "epoch": 6.137309292649099,
      "grad_norm": 3.217332601547241,
      "learning_rate": 4.488557558945909e-05,
      "loss": 0.7066,
      "step": 672600
    },
    {
      "epoch": 6.138221768012263,
      "grad_norm": 3.922881841659546,
      "learning_rate": 4.4884815193323116e-05,
      "loss": 0.6856,
      "step": 672700
    },
    {
      "epoch": 6.139134243375429,
      "grad_norm": 4.443814277648926,
      "learning_rate": 4.4884054797187146e-05,
      "loss": 0.7164,
      "step": 672800
    },
    {
      "epoch": 6.140046718738594,
      "grad_norm": 4.272426605224609,
      "learning_rate": 4.4883294401051176e-05,
      "loss": 0.6562,
      "step": 672900
    },
    {
      "epoch": 6.140959194101759,
      "grad_norm": 3.780320644378662,
      "learning_rate": 4.4882534004915206e-05,
      "loss": 0.7447,
      "step": 673000
    },
    {
      "epoch": 6.1418716694649245,
      "grad_norm": 3.511608600616455,
      "learning_rate": 4.488177360877923e-05,
      "loss": 0.6682,
      "step": 673100
    },
    {
      "epoch": 6.14278414482809,
      "grad_norm": 4.002442836761475,
      "learning_rate": 4.488101321264326e-05,
      "loss": 0.7099,
      "step": 673200
    },
    {
      "epoch": 6.143696620191255,
      "grad_norm": 4.825488090515137,
      "learning_rate": 4.488025281650729e-05,
      "loss": 0.7078,
      "step": 673300
    },
    {
      "epoch": 6.14460909555442,
      "grad_norm": 5.464659214019775,
      "learning_rate": 4.487949242037132e-05,
      "loss": 0.677,
      "step": 673400
    },
    {
      "epoch": 6.145521570917586,
      "grad_norm": 3.9102959632873535,
      "learning_rate": 4.487873202423535e-05,
      "loss": 0.692,
      "step": 673500
    },
    {
      "epoch": 6.14643404628075,
      "grad_norm": 3.754404306411743,
      "learning_rate": 4.487797162809937e-05,
      "loss": 0.7122,
      "step": 673600
    },
    {
      "epoch": 6.147346521643915,
      "grad_norm": 4.625141620635986,
      "learning_rate": 4.487721123196341e-05,
      "loss": 0.692,
      "step": 673700
    },
    {
      "epoch": 6.148258997007081,
      "grad_norm": 3.7901718616485596,
      "learning_rate": 4.487645083582743e-05,
      "loss": 0.7193,
      "step": 673800
    },
    {
      "epoch": 6.149171472370246,
      "grad_norm": 4.171055316925049,
      "learning_rate": 4.487569043969146e-05,
      "loss": 0.7115,
      "step": 673900
    },
    {
      "epoch": 6.150083947733411,
      "grad_norm": 5.145025253295898,
      "learning_rate": 4.487493004355549e-05,
      "loss": 0.7672,
      "step": 674000
    },
    {
      "epoch": 6.150996423096577,
      "grad_norm": 3.2213363647460938,
      "learning_rate": 4.487416964741952e-05,
      "loss": 0.7114,
      "step": 674100
    },
    {
      "epoch": 6.151908898459742,
      "grad_norm": 3.937476873397827,
      "learning_rate": 4.4873409251283546e-05,
      "loss": 0.6849,
      "step": 674200
    },
    {
      "epoch": 6.152821373822907,
      "grad_norm": 3.741544008255005,
      "learning_rate": 4.487264885514758e-05,
      "loss": 0.6783,
      "step": 674300
    },
    {
      "epoch": 6.153733849186072,
      "grad_norm": 3.7926366329193115,
      "learning_rate": 4.4871888459011606e-05,
      "loss": 0.6737,
      "step": 674400
    },
    {
      "epoch": 6.154646324549237,
      "grad_norm": 4.1670756340026855,
      "learning_rate": 4.4871128062875636e-05,
      "loss": 0.689,
      "step": 674500
    },
    {
      "epoch": 6.155558799912402,
      "grad_norm": 3.797177314758301,
      "learning_rate": 4.4870367666739667e-05,
      "loss": 0.725,
      "step": 674600
    },
    {
      "epoch": 6.1564712752755675,
      "grad_norm": 3.15193247795105,
      "learning_rate": 4.48696072706037e-05,
      "loss": 0.6829,
      "step": 674700
    },
    {
      "epoch": 6.157383750638733,
      "grad_norm": 2.5990288257598877,
      "learning_rate": 4.486884687446773e-05,
      "loss": 0.6735,
      "step": 674800
    },
    {
      "epoch": 6.158296226001898,
      "grad_norm": 4.831993103027344,
      "learning_rate": 4.486808647833176e-05,
      "loss": 0.7141,
      "step": 674900
    },
    {
      "epoch": 6.159208701365063,
      "grad_norm": 4.497231483459473,
      "learning_rate": 4.486732608219578e-05,
      "loss": 0.7087,
      "step": 675000
    },
    {
      "epoch": 6.160121176728229,
      "grad_norm": 3.6044137477874756,
      "learning_rate": 4.486656568605982e-05,
      "loss": 0.6863,
      "step": 675100
    },
    {
      "epoch": 6.161033652091394,
      "grad_norm": 4.065199851989746,
      "learning_rate": 4.486580528992384e-05,
      "loss": 0.6992,
      "step": 675200
    },
    {
      "epoch": 6.161946127454558,
      "grad_norm": 4.429364204406738,
      "learning_rate": 4.486504489378787e-05,
      "loss": 0.7313,
      "step": 675300
    },
    {
      "epoch": 6.162858602817724,
      "grad_norm": 3.406137704849243,
      "learning_rate": 4.48642844976519e-05,
      "loss": 0.706,
      "step": 675400
    },
    {
      "epoch": 6.163771078180889,
      "grad_norm": 4.352893352508545,
      "learning_rate": 4.486352410151593e-05,
      "loss": 0.6622,
      "step": 675500
    },
    {
      "epoch": 6.164683553544054,
      "grad_norm": 3.606137275695801,
      "learning_rate": 4.4862763705379954e-05,
      "loss": 0.718,
      "step": 675600
    },
    {
      "epoch": 6.16559602890722,
      "grad_norm": 2.927490711212158,
      "learning_rate": 4.486200330924399e-05,
      "loss": 0.7204,
      "step": 675700
    },
    {
      "epoch": 6.166508504270385,
      "grad_norm": 3.9464287757873535,
      "learning_rate": 4.4861242913108014e-05,
      "loss": 0.6952,
      "step": 675800
    },
    {
      "epoch": 6.16742097963355,
      "grad_norm": 5.318489074707031,
      "learning_rate": 4.4860482516972044e-05,
      "loss": 0.6865,
      "step": 675900
    },
    {
      "epoch": 6.1683334549967155,
      "grad_norm": 2.722031354904175,
      "learning_rate": 4.4859722120836074e-05,
      "loss": 0.6995,
      "step": 676000
    },
    {
      "epoch": 6.16924593035988,
      "grad_norm": 2.417748212814331,
      "learning_rate": 4.48589617247001e-05,
      "loss": 0.7332,
      "step": 676100
    },
    {
      "epoch": 6.170158405723045,
      "grad_norm": 3.6707098484039307,
      "learning_rate": 4.4858201328564134e-05,
      "loss": 0.6674,
      "step": 676200
    },
    {
      "epoch": 6.1710708810862105,
      "grad_norm": 4.0420637130737305,
      "learning_rate": 4.485744093242816e-05,
      "loss": 0.7065,
      "step": 676300
    },
    {
      "epoch": 6.171983356449376,
      "grad_norm": 2.8829824924468994,
      "learning_rate": 4.485668053629219e-05,
      "loss": 0.6946,
      "step": 676400
    },
    {
      "epoch": 6.172895831812541,
      "grad_norm": 4.335200786590576,
      "learning_rate": 4.485592014015622e-05,
      "loss": 0.6855,
      "step": 676500
    },
    {
      "epoch": 6.173808307175706,
      "grad_norm": 4.458539009094238,
      "learning_rate": 4.485515974402025e-05,
      "loss": 0.7043,
      "step": 676600
    },
    {
      "epoch": 6.174720782538872,
      "grad_norm": 4.871562957763672,
      "learning_rate": 4.485439934788427e-05,
      "loss": 0.6613,
      "step": 676700
    },
    {
      "epoch": 6.175633257902037,
      "grad_norm": 4.290918827056885,
      "learning_rate": 4.485363895174831e-05,
      "loss": 0.7362,
      "step": 676800
    },
    {
      "epoch": 6.176545733265202,
      "grad_norm": 4.520313739776611,
      "learning_rate": 4.485287855561233e-05,
      "loss": 0.7213,
      "step": 676900
    },
    {
      "epoch": 6.177458208628367,
      "grad_norm": 4.4802656173706055,
      "learning_rate": 4.485211815947636e-05,
      "loss": 0.7257,
      "step": 677000
    },
    {
      "epoch": 6.178370683991532,
      "grad_norm": 4.517603397369385,
      "learning_rate": 4.485135776334039e-05,
      "loss": 0.7121,
      "step": 677100
    },
    {
      "epoch": 6.179283159354697,
      "grad_norm": 3.7605063915252686,
      "learning_rate": 4.485059736720442e-05,
      "loss": 0.7416,
      "step": 677200
    },
    {
      "epoch": 6.180195634717863,
      "grad_norm": 4.211974620819092,
      "learning_rate": 4.484983697106845e-05,
      "loss": 0.7279,
      "step": 677300
    },
    {
      "epoch": 6.181108110081028,
      "grad_norm": 3.7220404148101807,
      "learning_rate": 4.484907657493248e-05,
      "loss": 0.6891,
      "step": 677400
    },
    {
      "epoch": 6.182020585444193,
      "grad_norm": 3.5340213775634766,
      "learning_rate": 4.4848316178796505e-05,
      "loss": 0.6895,
      "step": 677500
    },
    {
      "epoch": 6.1829330608073585,
      "grad_norm": 3.478844165802002,
      "learning_rate": 4.484755578266054e-05,
      "loss": 0.7034,
      "step": 677600
    },
    {
      "epoch": 6.183845536170524,
      "grad_norm": 3.213836669921875,
      "learning_rate": 4.4846795386524565e-05,
      "loss": 0.7116,
      "step": 677700
    },
    {
      "epoch": 6.184758011533688,
      "grad_norm": 3.5638954639434814,
      "learning_rate": 4.4846034990388595e-05,
      "loss": 0.6755,
      "step": 677800
    },
    {
      "epoch": 6.1856704868968535,
      "grad_norm": 4.410150527954102,
      "learning_rate": 4.4845274594252625e-05,
      "loss": 0.6881,
      "step": 677900
    },
    {
      "epoch": 6.186582962260019,
      "grad_norm": 4.533857822418213,
      "learning_rate": 4.4844514198116655e-05,
      "loss": 0.6916,
      "step": 678000
    },
    {
      "epoch": 6.187495437623184,
      "grad_norm": 4.193152904510498,
      "learning_rate": 4.484375380198068e-05,
      "loss": 0.7265,
      "step": 678100
    },
    {
      "epoch": 6.188407912986349,
      "grad_norm": 2.694835662841797,
      "learning_rate": 4.4842993405844715e-05,
      "loss": 0.7225,
      "step": 678200
    },
    {
      "epoch": 6.189320388349515,
      "grad_norm": 4.0610809326171875,
      "learning_rate": 4.484223300970874e-05,
      "loss": 0.6975,
      "step": 678300
    },
    {
      "epoch": 6.19023286371268,
      "grad_norm": 4.467493534088135,
      "learning_rate": 4.484147261357277e-05,
      "loss": 0.7283,
      "step": 678400
    },
    {
      "epoch": 6.191145339075845,
      "grad_norm": 3.940521001815796,
      "learning_rate": 4.48407122174368e-05,
      "loss": 0.708,
      "step": 678500
    },
    {
      "epoch": 6.192057814439011,
      "grad_norm": 2.5274758338928223,
      "learning_rate": 4.483995182130083e-05,
      "loss": 0.6685,
      "step": 678600
    },
    {
      "epoch": 6.192970289802175,
      "grad_norm": 3.7598049640655518,
      "learning_rate": 4.483919142516486e-05,
      "loss": 0.7353,
      "step": 678700
    },
    {
      "epoch": 6.19388276516534,
      "grad_norm": 4.364467620849609,
      "learning_rate": 4.483843102902888e-05,
      "loss": 0.663,
      "step": 678800
    },
    {
      "epoch": 6.194795240528506,
      "grad_norm": 3.22174334526062,
      "learning_rate": 4.483767063289291e-05,
      "loss": 0.692,
      "step": 678900
    },
    {
      "epoch": 6.195707715891671,
      "grad_norm": 4.458311080932617,
      "learning_rate": 4.483691023675694e-05,
      "loss": 0.7018,
      "step": 679000
    },
    {
      "epoch": 6.196620191254836,
      "grad_norm": 3.959573268890381,
      "learning_rate": 4.483614984062097e-05,
      "loss": 0.7117,
      "step": 679100
    },
    {
      "epoch": 6.1975326666180015,
      "grad_norm": 4.218451976776123,
      "learning_rate": 4.4835389444484995e-05,
      "loss": 0.6612,
      "step": 679200
    },
    {
      "epoch": 6.198445141981167,
      "grad_norm": 4.324655532836914,
      "learning_rate": 4.483462904834903e-05,
      "loss": 0.6606,
      "step": 679300
    },
    {
      "epoch": 6.199357617344332,
      "grad_norm": 4.119804859161377,
      "learning_rate": 4.4833868652213056e-05,
      "loss": 0.7136,
      "step": 679400
    },
    {
      "epoch": 6.2002700927074965,
      "grad_norm": 4.173000812530518,
      "learning_rate": 4.4833108256077086e-05,
      "loss": 0.7316,
      "step": 679500
    },
    {
      "epoch": 6.201182568070662,
      "grad_norm": 3.480100154876709,
      "learning_rate": 4.4832347859941116e-05,
      "loss": 0.709,
      "step": 679600
    },
    {
      "epoch": 6.202095043433827,
      "grad_norm": 4.043144702911377,
      "learning_rate": 4.4831587463805146e-05,
      "loss": 0.7239,
      "step": 679700
    },
    {
      "epoch": 6.203007518796992,
      "grad_norm": 4.042068004608154,
      "learning_rate": 4.4830827067669176e-05,
      "loss": 0.6753,
      "step": 679800
    },
    {
      "epoch": 6.203919994160158,
      "grad_norm": 2.9047462940216064,
      "learning_rate": 4.4830066671533206e-05,
      "loss": 0.6867,
      "step": 679900
    },
    {
      "epoch": 6.204832469523323,
      "grad_norm": 4.4794921875,
      "learning_rate": 4.482930627539723e-05,
      "loss": 0.711,
      "step": 680000
    },
    {
      "epoch": 6.205744944886488,
      "grad_norm": 4.249801158905029,
      "learning_rate": 4.4828545879261266e-05,
      "loss": 0.7403,
      "step": 680100
    },
    {
      "epoch": 6.206657420249654,
      "grad_norm": 4.241300106048584,
      "learning_rate": 4.482778548312529e-05,
      "loss": 0.6767,
      "step": 680200
    },
    {
      "epoch": 6.207569895612819,
      "grad_norm": 4.116776466369629,
      "learning_rate": 4.482702508698932e-05,
      "loss": 0.6713,
      "step": 680300
    },
    {
      "epoch": 6.208482370975983,
      "grad_norm": 3.255354166030884,
      "learning_rate": 4.482626469085335e-05,
      "loss": 0.7195,
      "step": 680400
    },
    {
      "epoch": 6.209394846339149,
      "grad_norm": 4.764317989349365,
      "learning_rate": 4.482550429471738e-05,
      "loss": 0.698,
      "step": 680500
    },
    {
      "epoch": 6.210307321702314,
      "grad_norm": 4.343364238739014,
      "learning_rate": 4.48247438985814e-05,
      "loss": 0.6987,
      "step": 680600
    },
    {
      "epoch": 6.211219797065479,
      "grad_norm": 4.530627727508545,
      "learning_rate": 4.482398350244544e-05,
      "loss": 0.695,
      "step": 680700
    },
    {
      "epoch": 6.2121322724286445,
      "grad_norm": 4.776217937469482,
      "learning_rate": 4.482322310630946e-05,
      "loss": 0.7434,
      "step": 680800
    },
    {
      "epoch": 6.21304474779181,
      "grad_norm": 3.685229778289795,
      "learning_rate": 4.482246271017349e-05,
      "loss": 0.7079,
      "step": 680900
    },
    {
      "epoch": 6.213957223154975,
      "grad_norm": 3.910428047180176,
      "learning_rate": 4.482170231403752e-05,
      "loss": 0.6865,
      "step": 681000
    },
    {
      "epoch": 6.21486969851814,
      "grad_norm": 4.459231376647949,
      "learning_rate": 4.482094191790155e-05,
      "loss": 0.7162,
      "step": 681100
    },
    {
      "epoch": 6.215782173881305,
      "grad_norm": 4.01753044128418,
      "learning_rate": 4.482018152176558e-05,
      "loss": 0.6873,
      "step": 681200
    },
    {
      "epoch": 6.21669464924447,
      "grad_norm": 3.8188533782958984,
      "learning_rate": 4.481942112562961e-05,
      "loss": 0.685,
      "step": 681300
    },
    {
      "epoch": 6.217607124607635,
      "grad_norm": 4.559670448303223,
      "learning_rate": 4.4818660729493637e-05,
      "loss": 0.7399,
      "step": 681400
    },
    {
      "epoch": 6.218519599970801,
      "grad_norm": 4.889669418334961,
      "learning_rate": 4.4817900333357673e-05,
      "loss": 0.7008,
      "step": 681500
    },
    {
      "epoch": 6.219432075333966,
      "grad_norm": 3.8366520404815674,
      "learning_rate": 4.48171399372217e-05,
      "loss": 0.681,
      "step": 681600
    },
    {
      "epoch": 6.220344550697131,
      "grad_norm": 3.8906023502349854,
      "learning_rate": 4.481637954108573e-05,
      "loss": 0.7186,
      "step": 681700
    },
    {
      "epoch": 6.221257026060297,
      "grad_norm": 3.4011175632476807,
      "learning_rate": 4.481561914494976e-05,
      "loss": 0.7222,
      "step": 681800
    },
    {
      "epoch": 6.222169501423462,
      "grad_norm": 5.151028156280518,
      "learning_rate": 4.481485874881378e-05,
      "loss": 0.7096,
      "step": 681900
    },
    {
      "epoch": 6.223081976786626,
      "grad_norm": 3.9700398445129395,
      "learning_rate": 4.481409835267781e-05,
      "loss": 0.6979,
      "step": 682000
    },
    {
      "epoch": 6.223994452149792,
      "grad_norm": 3.654125690460205,
      "learning_rate": 4.481333795654184e-05,
      "loss": 0.7182,
      "step": 682100
    },
    {
      "epoch": 6.224906927512957,
      "grad_norm": 4.409035682678223,
      "learning_rate": 4.481257756040587e-05,
      "loss": 0.7206,
      "step": 682200
    },
    {
      "epoch": 6.225819402876122,
      "grad_norm": 4.139794826507568,
      "learning_rate": 4.48118171642699e-05,
      "loss": 0.6892,
      "step": 682300
    },
    {
      "epoch": 6.2267318782392875,
      "grad_norm": 3.696582555770874,
      "learning_rate": 4.481105676813393e-05,
      "loss": 0.7178,
      "step": 682400
    },
    {
      "epoch": 6.227644353602453,
      "grad_norm": 4.717710494995117,
      "learning_rate": 4.4810296371997954e-05,
      "loss": 0.6952,
      "step": 682500
    },
    {
      "epoch": 6.228556828965618,
      "grad_norm": 4.152105808258057,
      "learning_rate": 4.480953597586199e-05,
      "loss": 0.7182,
      "step": 682600
    },
    {
      "epoch": 6.229469304328783,
      "grad_norm": 3.5222718715667725,
      "learning_rate": 4.4808775579726014e-05,
      "loss": 0.6884,
      "step": 682700
    },
    {
      "epoch": 6.230381779691949,
      "grad_norm": 4.604278087615967,
      "learning_rate": 4.4808015183590044e-05,
      "loss": 0.7107,
      "step": 682800
    },
    {
      "epoch": 6.231294255055113,
      "grad_norm": 2.2207682132720947,
      "learning_rate": 4.4807254787454074e-05,
      "loss": 0.696,
      "step": 682900
    },
    {
      "epoch": 6.232206730418278,
      "grad_norm": 4.517379283905029,
      "learning_rate": 4.4806494391318104e-05,
      "loss": 0.6974,
      "step": 683000
    },
    {
      "epoch": 6.233119205781444,
      "grad_norm": 2.738898277282715,
      "learning_rate": 4.4805733995182134e-05,
      "loss": 0.6988,
      "step": 683100
    },
    {
      "epoch": 6.234031681144609,
      "grad_norm": 3.4690136909484863,
      "learning_rate": 4.4804973599046164e-05,
      "loss": 0.678,
      "step": 683200
    },
    {
      "epoch": 6.234944156507774,
      "grad_norm": 3.797952651977539,
      "learning_rate": 4.480421320291019e-05,
      "loss": 0.7317,
      "step": 683300
    },
    {
      "epoch": 6.23585663187094,
      "grad_norm": 3.9364945888519287,
      "learning_rate": 4.480345280677422e-05,
      "loss": 0.7195,
      "step": 683400
    },
    {
      "epoch": 6.236769107234105,
      "grad_norm": 4.858048439025879,
      "learning_rate": 4.480269241063825e-05,
      "loss": 0.713,
      "step": 683500
    },
    {
      "epoch": 6.23768158259727,
      "grad_norm": 5.0006022453308105,
      "learning_rate": 4.480193201450228e-05,
      "loss": 0.7468,
      "step": 683600
    },
    {
      "epoch": 6.238594057960435,
      "grad_norm": 3.763744592666626,
      "learning_rate": 4.480117161836631e-05,
      "loss": 0.663,
      "step": 683700
    },
    {
      "epoch": 6.2395065333236,
      "grad_norm": 3.4547293186187744,
      "learning_rate": 4.480041122223034e-05,
      "loss": 0.6805,
      "step": 683800
    },
    {
      "epoch": 6.240419008686765,
      "grad_norm": 3.867750644683838,
      "learning_rate": 4.479965082609436e-05,
      "loss": 0.7028,
      "step": 683900
    },
    {
      "epoch": 6.2413314840499305,
      "grad_norm": 4.372367858886719,
      "learning_rate": 4.47988904299584e-05,
      "loss": 0.7105,
      "step": 684000
    },
    {
      "epoch": 6.242243959413096,
      "grad_norm": 3.614494800567627,
      "learning_rate": 4.479813003382242e-05,
      "loss": 0.6888,
      "step": 684100
    },
    {
      "epoch": 6.243156434776261,
      "grad_norm": 4.529193878173828,
      "learning_rate": 4.479736963768645e-05,
      "loss": 0.6672,
      "step": 684200
    },
    {
      "epoch": 6.244068910139426,
      "grad_norm": 3.6282145977020264,
      "learning_rate": 4.479660924155048e-05,
      "loss": 0.6939,
      "step": 684300
    },
    {
      "epoch": 6.244981385502592,
      "grad_norm": 3.792813301086426,
      "learning_rate": 4.479584884541451e-05,
      "loss": 0.7116,
      "step": 684400
    },
    {
      "epoch": 6.245893860865757,
      "grad_norm": 4.263121128082275,
      "learning_rate": 4.479508844927854e-05,
      "loss": 0.7137,
      "step": 684500
    },
    {
      "epoch": 6.2468063362289215,
      "grad_norm": 3.2553234100341797,
      "learning_rate": 4.4794328053142565e-05,
      "loss": 0.6969,
      "step": 684600
    },
    {
      "epoch": 6.247718811592087,
      "grad_norm": 3.8036766052246094,
      "learning_rate": 4.4793567657006595e-05,
      "loss": 0.6957,
      "step": 684700
    },
    {
      "epoch": 6.248631286955252,
      "grad_norm": 3.664477825164795,
      "learning_rate": 4.4792807260870625e-05,
      "loss": 0.6717,
      "step": 684800
    },
    {
      "epoch": 6.249543762318417,
      "grad_norm": 2.89091157913208,
      "learning_rate": 4.4792046864734655e-05,
      "loss": 0.6815,
      "step": 684900
    },
    {
      "epoch": 6.250456237681583,
      "grad_norm": 3.7140140533447266,
      "learning_rate": 4.479128646859868e-05,
      "loss": 0.6736,
      "step": 685000
    },
    {
      "epoch": 6.251368713044748,
      "grad_norm": 3.9840281009674072,
      "learning_rate": 4.4790526072462715e-05,
      "loss": 0.7026,
      "step": 685100
    },
    {
      "epoch": 6.252281188407913,
      "grad_norm": 3.5763652324676514,
      "learning_rate": 4.478976567632674e-05,
      "loss": 0.7089,
      "step": 685200
    },
    {
      "epoch": 6.2531936637710785,
      "grad_norm": 3.092824697494507,
      "learning_rate": 4.478900528019077e-05,
      "loss": 0.6914,
      "step": 685300
    },
    {
      "epoch": 6.254106139134244,
      "grad_norm": 4.4153313636779785,
      "learning_rate": 4.47882448840548e-05,
      "loss": 0.7045,
      "step": 685400
    },
    {
      "epoch": 6.255018614497408,
      "grad_norm": 4.326448440551758,
      "learning_rate": 4.478748448791883e-05,
      "loss": 0.7407,
      "step": 685500
    },
    {
      "epoch": 6.255931089860574,
      "grad_norm": 4.987760066986084,
      "learning_rate": 4.478672409178286e-05,
      "loss": 0.6969,
      "step": 685600
    },
    {
      "epoch": 6.256843565223739,
      "grad_norm": 4.481852054595947,
      "learning_rate": 4.478596369564689e-05,
      "loss": 0.6981,
      "step": 685700
    },
    {
      "epoch": 6.257756040586904,
      "grad_norm": 3.9913620948791504,
      "learning_rate": 4.478520329951091e-05,
      "loss": 0.7168,
      "step": 685800
    },
    {
      "epoch": 6.2586685159500695,
      "grad_norm": 3.660290479660034,
      "learning_rate": 4.478444290337495e-05,
      "loss": 0.7215,
      "step": 685900
    },
    {
      "epoch": 6.259580991313235,
      "grad_norm": 4.193255424499512,
      "learning_rate": 4.478368250723897e-05,
      "loss": 0.6804,
      "step": 686000
    },
    {
      "epoch": 6.2604934666764,
      "grad_norm": 4.287664413452148,
      "learning_rate": 4.4782922111103e-05,
      "loss": 0.7087,
      "step": 686100
    },
    {
      "epoch": 6.2614059420395645,
      "grad_norm": 4.701836585998535,
      "learning_rate": 4.478216171496703e-05,
      "loss": 0.6667,
      "step": 686200
    },
    {
      "epoch": 6.26231841740273,
      "grad_norm": 2.7464466094970703,
      "learning_rate": 4.478140131883106e-05,
      "loss": 0.7109,
      "step": 686300
    },
    {
      "epoch": 6.263230892765895,
      "grad_norm": 3.69285249710083,
      "learning_rate": 4.4780640922695086e-05,
      "loss": 0.7382,
      "step": 686400
    },
    {
      "epoch": 6.26414336812906,
      "grad_norm": 3.2484943866729736,
      "learning_rate": 4.477988052655912e-05,
      "loss": 0.7057,
      "step": 686500
    },
    {
      "epoch": 6.265055843492226,
      "grad_norm": 4.105279445648193,
      "learning_rate": 4.4779120130423146e-05,
      "loss": 0.7154,
      "step": 686600
    },
    {
      "epoch": 6.265968318855391,
      "grad_norm": 4.311297416687012,
      "learning_rate": 4.4778359734287176e-05,
      "loss": 0.6801,
      "step": 686700
    },
    {
      "epoch": 6.266880794218556,
      "grad_norm": 5.010723114013672,
      "learning_rate": 4.4777599338151206e-05,
      "loss": 0.7063,
      "step": 686800
    },
    {
      "epoch": 6.267793269581722,
      "grad_norm": 3.332998037338257,
      "learning_rate": 4.4776838942015236e-05,
      "loss": 0.6798,
      "step": 686900
    },
    {
      "epoch": 6.268705744944887,
      "grad_norm": 3.5556843280792236,
      "learning_rate": 4.4776078545879266e-05,
      "loss": 0.7582,
      "step": 687000
    },
    {
      "epoch": 6.269618220308051,
      "grad_norm": 4.221836090087891,
      "learning_rate": 4.4775318149743296e-05,
      "loss": 0.6818,
      "step": 687100
    },
    {
      "epoch": 6.270530695671217,
      "grad_norm": 3.8253846168518066,
      "learning_rate": 4.477455775360732e-05,
      "loss": 0.7017,
      "step": 687200
    },
    {
      "epoch": 6.271443171034382,
      "grad_norm": 4.304008483886719,
      "learning_rate": 4.4773797357471356e-05,
      "loss": 0.6706,
      "step": 687300
    },
    {
      "epoch": 6.272355646397547,
      "grad_norm": 4.716495037078857,
      "learning_rate": 4.477303696133538e-05,
      "loss": 0.7344,
      "step": 687400
    },
    {
      "epoch": 6.2732681217607125,
      "grad_norm": 4.085420608520508,
      "learning_rate": 4.47722765651994e-05,
      "loss": 0.7013,
      "step": 687500
    },
    {
      "epoch": 6.274180597123878,
      "grad_norm": 4.057220458984375,
      "learning_rate": 4.477151616906344e-05,
      "loss": 0.6912,
      "step": 687600
    },
    {
      "epoch": 6.275093072487043,
      "grad_norm": 2.05401873588562,
      "learning_rate": 4.477075577292746e-05,
      "loss": 0.6794,
      "step": 687700
    },
    {
      "epoch": 6.276005547850208,
      "grad_norm": 4.365731716156006,
      "learning_rate": 4.476999537679149e-05,
      "loss": 0.7196,
      "step": 687800
    },
    {
      "epoch": 6.276918023213373,
      "grad_norm": 3.7508161067962646,
      "learning_rate": 4.476923498065552e-05,
      "loss": 0.7075,
      "step": 687900
    },
    {
      "epoch": 6.277830498576538,
      "grad_norm": 3.664736270904541,
      "learning_rate": 4.476847458451955e-05,
      "loss": 0.6964,
      "step": 688000
    },
    {
      "epoch": 6.278742973939703,
      "grad_norm": 3.7734909057617188,
      "learning_rate": 4.476771418838358e-05,
      "loss": 0.6831,
      "step": 688100
    },
    {
      "epoch": 6.279655449302869,
      "grad_norm": 4.2096476554870605,
      "learning_rate": 4.476695379224761e-05,
      "loss": 0.6889,
      "step": 688200
    },
    {
      "epoch": 6.280567924666034,
      "grad_norm": 4.114940643310547,
      "learning_rate": 4.476619339611164e-05,
      "loss": 0.6878,
      "step": 688300
    },
    {
      "epoch": 6.281480400029199,
      "grad_norm": 4.008427619934082,
      "learning_rate": 4.4765432999975674e-05,
      "loss": 0.6951,
      "step": 688400
    },
    {
      "epoch": 6.282392875392365,
      "grad_norm": 3.901338577270508,
      "learning_rate": 4.47646726038397e-05,
      "loss": 0.7645,
      "step": 688500
    },
    {
      "epoch": 6.28330535075553,
      "grad_norm": 4.607513904571533,
      "learning_rate": 4.476391220770373e-05,
      "loss": 0.6871,
      "step": 688600
    },
    {
      "epoch": 6.284217826118695,
      "grad_norm": 4.559618949890137,
      "learning_rate": 4.476315181156776e-05,
      "loss": 0.7057,
      "step": 688700
    },
    {
      "epoch": 6.28513030148186,
      "grad_norm": 4.2690019607543945,
      "learning_rate": 4.476239141543179e-05,
      "loss": 0.7004,
      "step": 688800
    },
    {
      "epoch": 6.286042776845025,
      "grad_norm": 4.131249904632568,
      "learning_rate": 4.476163101929581e-05,
      "loss": 0.6523,
      "step": 688900
    },
    {
      "epoch": 6.28695525220819,
      "grad_norm": 4.3812255859375,
      "learning_rate": 4.476087062315985e-05,
      "loss": 0.6886,
      "step": 689000
    },
    {
      "epoch": 6.2878677275713555,
      "grad_norm": 4.43854284286499,
      "learning_rate": 4.476011022702387e-05,
      "loss": 0.6709,
      "step": 689100
    },
    {
      "epoch": 6.288780202934521,
      "grad_norm": 4.037818908691406,
      "learning_rate": 4.47593498308879e-05,
      "loss": 0.692,
      "step": 689200
    },
    {
      "epoch": 6.289692678297686,
      "grad_norm": 3.4381651878356934,
      "learning_rate": 4.475858943475193e-05,
      "loss": 0.6701,
      "step": 689300
    },
    {
      "epoch": 6.290605153660851,
      "grad_norm": 4.327359676361084,
      "learning_rate": 4.475782903861596e-05,
      "loss": 0.7013,
      "step": 689400
    },
    {
      "epoch": 6.291517629024017,
      "grad_norm": 3.9035091400146484,
      "learning_rate": 4.475706864247999e-05,
      "loss": 0.7219,
      "step": 689500
    },
    {
      "epoch": 6.292430104387181,
      "grad_norm": 3.9971132278442383,
      "learning_rate": 4.475630824634402e-05,
      "loss": 0.7123,
      "step": 689600
    },
    {
      "epoch": 6.293342579750346,
      "grad_norm": 3.9607608318328857,
      "learning_rate": 4.4755547850208044e-05,
      "loss": 0.7225,
      "step": 689700
    },
    {
      "epoch": 6.294255055113512,
      "grad_norm": 3.2775864601135254,
      "learning_rate": 4.475478745407208e-05,
      "loss": 0.6718,
      "step": 689800
    },
    {
      "epoch": 6.295167530476677,
      "grad_norm": 4.625495910644531,
      "learning_rate": 4.4754027057936104e-05,
      "loss": 0.7343,
      "step": 689900
    },
    {
      "epoch": 6.296080005839842,
      "grad_norm": 4.961603164672852,
      "learning_rate": 4.4753266661800134e-05,
      "loss": 0.7166,
      "step": 690000
    },
    {
      "epoch": 6.296992481203008,
      "grad_norm": 3.2786524295806885,
      "learning_rate": 4.4752506265664164e-05,
      "loss": 0.7001,
      "step": 690100
    },
    {
      "epoch": 6.297904956566173,
      "grad_norm": 4.236326217651367,
      "learning_rate": 4.475174586952819e-05,
      "loss": 0.7005,
      "step": 690200
    },
    {
      "epoch": 6.298817431929338,
      "grad_norm": 2.969337224960327,
      "learning_rate": 4.475098547339222e-05,
      "loss": 0.7213,
      "step": 690300
    },
    {
      "epoch": 6.2997299072925035,
      "grad_norm": 5.126743793487549,
      "learning_rate": 4.475022507725625e-05,
      "loss": 0.7272,
      "step": 690400
    },
    {
      "epoch": 6.300642382655668,
      "grad_norm": 3.9846060276031494,
      "learning_rate": 4.474946468112028e-05,
      "loss": 0.7178,
      "step": 690500
    },
    {
      "epoch": 6.301554858018833,
      "grad_norm": 3.7530550956726074,
      "learning_rate": 4.474870428498431e-05,
      "loss": 0.6778,
      "step": 690600
    },
    {
      "epoch": 6.3024673333819985,
      "grad_norm": 3.2550301551818848,
      "learning_rate": 4.474794388884834e-05,
      "loss": 0.6965,
      "step": 690700
    },
    {
      "epoch": 6.303379808745164,
      "grad_norm": 5.077033996582031,
      "learning_rate": 4.474718349271236e-05,
      "loss": 0.7002,
      "step": 690800
    },
    {
      "epoch": 6.304292284108329,
      "grad_norm": 4.543318271636963,
      "learning_rate": 4.47464230965764e-05,
      "loss": 0.7022,
      "step": 690900
    },
    {
      "epoch": 6.305204759471494,
      "grad_norm": 107.75345611572266,
      "learning_rate": 4.474566270044042e-05,
      "loss": 0.7148,
      "step": 691000
    },
    {
      "epoch": 6.30611723483466,
      "grad_norm": 4.045220375061035,
      "learning_rate": 4.474490230430445e-05,
      "loss": 0.7262,
      "step": 691100
    },
    {
      "epoch": 6.307029710197825,
      "grad_norm": 3.9484450817108154,
      "learning_rate": 4.474414190816848e-05,
      "loss": 0.6723,
      "step": 691200
    },
    {
      "epoch": 6.307942185560989,
      "grad_norm": 4.016618251800537,
      "learning_rate": 4.474338151203251e-05,
      "loss": 0.6867,
      "step": 691300
    },
    {
      "epoch": 6.308854660924155,
      "grad_norm": 4.630461692810059,
      "learning_rate": 4.4742621115896535e-05,
      "loss": 0.6669,
      "step": 691400
    },
    {
      "epoch": 6.30976713628732,
      "grad_norm": 4.176901817321777,
      "learning_rate": 4.474186071976057e-05,
      "loss": 0.7223,
      "step": 691500
    },
    {
      "epoch": 6.310679611650485,
      "grad_norm": 3.7960195541381836,
      "learning_rate": 4.4741100323624595e-05,
      "loss": 0.7099,
      "step": 691600
    },
    {
      "epoch": 6.311592087013651,
      "grad_norm": 3.717036724090576,
      "learning_rate": 4.4740339927488625e-05,
      "loss": 0.7074,
      "step": 691700
    },
    {
      "epoch": 6.312504562376816,
      "grad_norm": 4.456057548522949,
      "learning_rate": 4.4739579531352655e-05,
      "loss": 0.7129,
      "step": 691800
    },
    {
      "epoch": 6.313417037739981,
      "grad_norm": 3.6421706676483154,
      "learning_rate": 4.4738819135216685e-05,
      "loss": 0.7243,
      "step": 691900
    },
    {
      "epoch": 6.3143295131031465,
      "grad_norm": 3.7030856609344482,
      "learning_rate": 4.4738058739080715e-05,
      "loss": 0.6925,
      "step": 692000
    },
    {
      "epoch": 6.315241988466312,
      "grad_norm": 4.182947158813477,
      "learning_rate": 4.4737298342944745e-05,
      "loss": 0.7067,
      "step": 692100
    },
    {
      "epoch": 6.316154463829476,
      "grad_norm": 3.6232457160949707,
      "learning_rate": 4.473653794680877e-05,
      "loss": 0.6942,
      "step": 692200
    },
    {
      "epoch": 6.3170669391926415,
      "grad_norm": 5.877106189727783,
      "learning_rate": 4.4735777550672805e-05,
      "loss": 0.695,
      "step": 692300
    },
    {
      "epoch": 6.317979414555807,
      "grad_norm": 3.3420276641845703,
      "learning_rate": 4.473501715453683e-05,
      "loss": 0.7059,
      "step": 692400
    },
    {
      "epoch": 6.318891889918972,
      "grad_norm": 2.900930881500244,
      "learning_rate": 4.473425675840086e-05,
      "loss": 0.6895,
      "step": 692500
    },
    {
      "epoch": 6.319804365282137,
      "grad_norm": 4.071321964263916,
      "learning_rate": 4.473349636226489e-05,
      "loss": 0.7089,
      "step": 692600
    },
    {
      "epoch": 6.320716840645303,
      "grad_norm": 3.818143606185913,
      "learning_rate": 4.473273596612892e-05,
      "loss": 0.6911,
      "step": 692700
    },
    {
      "epoch": 6.321629316008468,
      "grad_norm": 2.466346263885498,
      "learning_rate": 4.473197556999294e-05,
      "loss": 0.682,
      "step": 692800
    },
    {
      "epoch": 6.322541791371633,
      "grad_norm": 3.56526780128479,
      "learning_rate": 4.473121517385698e-05,
      "loss": 0.7256,
      "step": 692900
    },
    {
      "epoch": 6.323454266734798,
      "grad_norm": 4.257353782653809,
      "learning_rate": 4.4730454777721e-05,
      "loss": 0.7478,
      "step": 693000
    },
    {
      "epoch": 6.324366742097963,
      "grad_norm": 3.669778347015381,
      "learning_rate": 4.472969438158503e-05,
      "loss": 0.733,
      "step": 693100
    },
    {
      "epoch": 6.325279217461128,
      "grad_norm": 3.126323938369751,
      "learning_rate": 4.472893398544906e-05,
      "loss": 0.735,
      "step": 693200
    },
    {
      "epoch": 6.326191692824294,
      "grad_norm": 3.3292510509490967,
      "learning_rate": 4.4728173589313086e-05,
      "loss": 0.6997,
      "step": 693300
    },
    {
      "epoch": 6.327104168187459,
      "grad_norm": 4.698624134063721,
      "learning_rate": 4.472741319317712e-05,
      "loss": 0.7118,
      "step": 693400
    },
    {
      "epoch": 6.328016643550624,
      "grad_norm": 3.909900188446045,
      "learning_rate": 4.4726652797041146e-05,
      "loss": 0.7034,
      "step": 693500
    },
    {
      "epoch": 6.3289291189137895,
      "grad_norm": 4.3546061515808105,
      "learning_rate": 4.4725892400905176e-05,
      "loss": 0.6809,
      "step": 693600
    },
    {
      "epoch": 6.329841594276955,
      "grad_norm": 4.520951747894287,
      "learning_rate": 4.4725132004769206e-05,
      "loss": 0.6728,
      "step": 693700
    },
    {
      "epoch": 6.33075406964012,
      "grad_norm": 3.9306085109710693,
      "learning_rate": 4.4724371608633236e-05,
      "loss": 0.7356,
      "step": 693800
    },
    {
      "epoch": 6.3316665450032845,
      "grad_norm": 4.1754631996154785,
      "learning_rate": 4.472361121249726e-05,
      "loss": 0.7008,
      "step": 693900
    },
    {
      "epoch": 6.33257902036645,
      "grad_norm": 3.8919517993927,
      "learning_rate": 4.4722850816361296e-05,
      "loss": 0.7106,
      "step": 694000
    },
    {
      "epoch": 6.333491495729615,
      "grad_norm": 4.420609474182129,
      "learning_rate": 4.472209042022532e-05,
      "loss": 0.6872,
      "step": 694100
    },
    {
      "epoch": 6.33440397109278,
      "grad_norm": 3.2384567260742188,
      "learning_rate": 4.472133002408935e-05,
      "loss": 0.7025,
      "step": 694200
    },
    {
      "epoch": 6.335316446455946,
      "grad_norm": 3.1641340255737305,
      "learning_rate": 4.472056962795338e-05,
      "loss": 0.7011,
      "step": 694300
    },
    {
      "epoch": 6.336228921819111,
      "grad_norm": 4.882298946380615,
      "learning_rate": 4.471980923181741e-05,
      "loss": 0.7402,
      "step": 694400
    },
    {
      "epoch": 6.337141397182276,
      "grad_norm": 4.924520015716553,
      "learning_rate": 4.471904883568144e-05,
      "loss": 0.6742,
      "step": 694500
    },
    {
      "epoch": 6.338053872545442,
      "grad_norm": 3.2264628410339355,
      "learning_rate": 4.471828843954547e-05,
      "loss": 0.668,
      "step": 694600
    },
    {
      "epoch": 6.338966347908606,
      "grad_norm": 4.11074686050415,
      "learning_rate": 4.471752804340949e-05,
      "loss": 0.7315,
      "step": 694700
    },
    {
      "epoch": 6.339878823271771,
      "grad_norm": 2.8160958290100098,
      "learning_rate": 4.471676764727353e-05,
      "loss": 0.6831,
      "step": 694800
    },
    {
      "epoch": 6.340791298634937,
      "grad_norm": 3.4243783950805664,
      "learning_rate": 4.471600725113755e-05,
      "loss": 0.7038,
      "step": 694900
    },
    {
      "epoch": 6.341703773998102,
      "grad_norm": 4.318795204162598,
      "learning_rate": 4.4715246855001583e-05,
      "loss": 0.6482,
      "step": 695000
    },
    {
      "epoch": 6.342616249361267,
      "grad_norm": 4.3765058517456055,
      "learning_rate": 4.4714486458865613e-05,
      "loss": 0.7378,
      "step": 695100
    },
    {
      "epoch": 6.3435287247244325,
      "grad_norm": 4.888632774353027,
      "learning_rate": 4.4713726062729644e-05,
      "loss": 0.6901,
      "step": 695200
    },
    {
      "epoch": 6.344441200087598,
      "grad_norm": 3.5568912029266357,
      "learning_rate": 4.4712965666593674e-05,
      "loss": 0.7229,
      "step": 695300
    },
    {
      "epoch": 6.345353675450763,
      "grad_norm": 4.204281330108643,
      "learning_rate": 4.4712205270457704e-05,
      "loss": 0.6985,
      "step": 695400
    },
    {
      "epoch": 6.346266150813928,
      "grad_norm": 4.79545259475708,
      "learning_rate": 4.471144487432173e-05,
      "loss": 0.7078,
      "step": 695500
    },
    {
      "epoch": 6.347178626177093,
      "grad_norm": 4.159609794616699,
      "learning_rate": 4.471068447818576e-05,
      "loss": 0.7018,
      "step": 695600
    },
    {
      "epoch": 6.348091101540258,
      "grad_norm": 4.41904354095459,
      "learning_rate": 4.470992408204979e-05,
      "loss": 0.7343,
      "step": 695700
    },
    {
      "epoch": 6.349003576903423,
      "grad_norm": 3.860182762145996,
      "learning_rate": 4.470916368591382e-05,
      "loss": 0.7109,
      "step": 695800
    },
    {
      "epoch": 6.349916052266589,
      "grad_norm": 3.7417871952056885,
      "learning_rate": 4.470840328977785e-05,
      "loss": 0.6982,
      "step": 695900
    },
    {
      "epoch": 6.350828527629754,
      "grad_norm": 5.232280731201172,
      "learning_rate": 4.470764289364187e-05,
      "loss": 0.7242,
      "step": 696000
    },
    {
      "epoch": 6.351741002992919,
      "grad_norm": 4.830164909362793,
      "learning_rate": 4.47068824975059e-05,
      "loss": 0.6914,
      "step": 696100
    },
    {
      "epoch": 6.352653478356085,
      "grad_norm": 3.937892436981201,
      "learning_rate": 4.470612210136993e-05,
      "loss": 0.7263,
      "step": 696200
    },
    {
      "epoch": 6.35356595371925,
      "grad_norm": 4.952809810638428,
      "learning_rate": 4.470536170523396e-05,
      "loss": 0.7126,
      "step": 696300
    },
    {
      "epoch": 6.354478429082414,
      "grad_norm": 3.408900737762451,
      "learning_rate": 4.470460130909799e-05,
      "loss": 0.7022,
      "step": 696400
    },
    {
      "epoch": 6.35539090444558,
      "grad_norm": 4.470297813415527,
      "learning_rate": 4.470384091296202e-05,
      "loss": 0.6509,
      "step": 696500
    },
    {
      "epoch": 6.356303379808745,
      "grad_norm": 4.335177421569824,
      "learning_rate": 4.4703080516826044e-05,
      "loss": 0.6969,
      "step": 696600
    },
    {
      "epoch": 6.35721585517191,
      "grad_norm": 3.9134209156036377,
      "learning_rate": 4.470232012069008e-05,
      "loss": 0.7326,
      "step": 696700
    },
    {
      "epoch": 6.3581283305350755,
      "grad_norm": 3.746108293533325,
      "learning_rate": 4.4701559724554104e-05,
      "loss": 0.7094,
      "step": 696800
    },
    {
      "epoch": 6.359040805898241,
      "grad_norm": 4.282281398773193,
      "learning_rate": 4.4700799328418134e-05,
      "loss": 0.709,
      "step": 696900
    },
    {
      "epoch": 6.359953281261406,
      "grad_norm": 3.9485955238342285,
      "learning_rate": 4.4700038932282164e-05,
      "loss": 0.6693,
      "step": 697000
    },
    {
      "epoch": 6.360865756624571,
      "grad_norm": 3.5442638397216797,
      "learning_rate": 4.4699278536146195e-05,
      "loss": 0.7155,
      "step": 697100
    },
    {
      "epoch": 6.361778231987737,
      "grad_norm": 3.1407370567321777,
      "learning_rate": 4.469851814001022e-05,
      "loss": 0.7214,
      "step": 697200
    },
    {
      "epoch": 6.362690707350901,
      "grad_norm": 4.25857400894165,
      "learning_rate": 4.4697757743874255e-05,
      "loss": 0.6976,
      "step": 697300
    },
    {
      "epoch": 6.363603182714066,
      "grad_norm": 5.162753582000732,
      "learning_rate": 4.469699734773828e-05,
      "loss": 0.7095,
      "step": 697400
    },
    {
      "epoch": 6.364515658077232,
      "grad_norm": 4.406615734100342,
      "learning_rate": 4.469623695160231e-05,
      "loss": 0.7047,
      "step": 697500
    },
    {
      "epoch": 6.365428133440397,
      "grad_norm": 4.244827747344971,
      "learning_rate": 4.469547655546634e-05,
      "loss": 0.7219,
      "step": 697600
    },
    {
      "epoch": 6.366340608803562,
      "grad_norm": 4.456433296203613,
      "learning_rate": 4.469471615933037e-05,
      "loss": 0.6806,
      "step": 697700
    },
    {
      "epoch": 6.367253084166728,
      "grad_norm": 4.144738674163818,
      "learning_rate": 4.46939557631944e-05,
      "loss": 0.7357,
      "step": 697800
    },
    {
      "epoch": 6.368165559529893,
      "grad_norm": 4.599618434906006,
      "learning_rate": 4.469319536705843e-05,
      "loss": 0.7156,
      "step": 697900
    },
    {
      "epoch": 6.369078034893058,
      "grad_norm": 4.319106578826904,
      "learning_rate": 4.469243497092245e-05,
      "loss": 0.7241,
      "step": 698000
    },
    {
      "epoch": 6.369990510256223,
      "grad_norm": 4.246203899383545,
      "learning_rate": 4.469167457478649e-05,
      "loss": 0.7027,
      "step": 698100
    },
    {
      "epoch": 6.370902985619388,
      "grad_norm": 4.522202014923096,
      "learning_rate": 4.469091417865051e-05,
      "loss": 0.6987,
      "step": 698200
    },
    {
      "epoch": 6.371815460982553,
      "grad_norm": 4.162022113800049,
      "learning_rate": 4.469015378251454e-05,
      "loss": 0.6961,
      "step": 698300
    },
    {
      "epoch": 6.3727279363457185,
      "grad_norm": 4.788463115692139,
      "learning_rate": 4.468939338637857e-05,
      "loss": 0.7223,
      "step": 698400
    },
    {
      "epoch": 6.373640411708884,
      "grad_norm": 4.3066630363464355,
      "learning_rate": 4.46886329902426e-05,
      "loss": 0.6997,
      "step": 698500
    },
    {
      "epoch": 6.374552887072049,
      "grad_norm": 4.047451019287109,
      "learning_rate": 4.4687872594106625e-05,
      "loss": 0.7094,
      "step": 698600
    },
    {
      "epoch": 6.3754653624352144,
      "grad_norm": 4.034038066864014,
      "learning_rate": 4.4687112197970655e-05,
      "loss": 0.7053,
      "step": 698700
    },
    {
      "epoch": 6.37637783779838,
      "grad_norm": 3.231065273284912,
      "learning_rate": 4.4686351801834685e-05,
      "loss": 0.7093,
      "step": 698800
    },
    {
      "epoch": 6.377290313161545,
      "grad_norm": 3.718654155731201,
      "learning_rate": 4.4685591405698715e-05,
      "loss": 0.6843,
      "step": 698900
    },
    {
      "epoch": 6.3782027885247095,
      "grad_norm": 3.716557264328003,
      "learning_rate": 4.4684831009562745e-05,
      "loss": 0.7419,
      "step": 699000
    },
    {
      "epoch": 6.379115263887875,
      "grad_norm": 4.035191535949707,
      "learning_rate": 4.468407061342677e-05,
      "loss": 0.7179,
      "step": 699100
    },
    {
      "epoch": 6.38002773925104,
      "grad_norm": 4.0738420486450195,
      "learning_rate": 4.4683310217290806e-05,
      "loss": 0.6972,
      "step": 699200
    },
    {
      "epoch": 6.380940214614205,
      "grad_norm": 4.558052062988281,
      "learning_rate": 4.468254982115483e-05,
      "loss": 0.6857,
      "step": 699300
    },
    {
      "epoch": 6.381852689977371,
      "grad_norm": 3.6057064533233643,
      "learning_rate": 4.468178942501886e-05,
      "loss": 0.6949,
      "step": 699400
    },
    {
      "epoch": 6.382765165340536,
      "grad_norm": 3.573723554611206,
      "learning_rate": 4.468102902888289e-05,
      "loss": 0.7227,
      "step": 699500
    },
    {
      "epoch": 6.383677640703701,
      "grad_norm": 3.278791904449463,
      "learning_rate": 4.468026863274692e-05,
      "loss": 0.7129,
      "step": 699600
    },
    {
      "epoch": 6.3845901160668665,
      "grad_norm": 3.2907752990722656,
      "learning_rate": 4.467950823661094e-05,
      "loss": 0.7334,
      "step": 699700
    },
    {
      "epoch": 6.385502591430031,
      "grad_norm": 4.099874973297119,
      "learning_rate": 4.467874784047498e-05,
      "loss": 0.7059,
      "step": 699800
    },
    {
      "epoch": 6.386415066793196,
      "grad_norm": 3.785590171813965,
      "learning_rate": 4.4677987444339e-05,
      "loss": 0.6619,
      "step": 699900
    },
    {
      "epoch": 6.387327542156362,
      "grad_norm": 4.491004943847656,
      "learning_rate": 4.467722704820303e-05,
      "loss": 0.6646,
      "step": 700000
    },
    {
      "epoch": 6.388240017519527,
      "grad_norm": 4.648149490356445,
      "learning_rate": 4.467646665206706e-05,
      "loss": 0.7198,
      "step": 700100
    },
    {
      "epoch": 6.389152492882692,
      "grad_norm": 4.0936994552612305,
      "learning_rate": 4.467570625593109e-05,
      "loss": 0.6922,
      "step": 700200
    },
    {
      "epoch": 6.3900649682458575,
      "grad_norm": 3.7014830112457275,
      "learning_rate": 4.467494585979512e-05,
      "loss": 0.6489,
      "step": 700300
    },
    {
      "epoch": 6.390977443609023,
      "grad_norm": 3.5355026721954346,
      "learning_rate": 4.467418546365915e-05,
      "loss": 0.7419,
      "step": 700400
    },
    {
      "epoch": 6.391889918972188,
      "grad_norm": 3.2693264484405518,
      "learning_rate": 4.4673425067523176e-05,
      "loss": 0.72,
      "step": 700500
    },
    {
      "epoch": 6.392802394335353,
      "grad_norm": 4.375055313110352,
      "learning_rate": 4.467266467138721e-05,
      "loss": 0.6798,
      "step": 700600
    },
    {
      "epoch": 6.393714869698518,
      "grad_norm": 3.9935219287872314,
      "learning_rate": 4.4671904275251236e-05,
      "loss": 0.7258,
      "step": 700700
    },
    {
      "epoch": 6.394627345061683,
      "grad_norm": 4.321221828460693,
      "learning_rate": 4.4671143879115266e-05,
      "loss": 0.6873,
      "step": 700800
    },
    {
      "epoch": 6.395539820424848,
      "grad_norm": 4.185027122497559,
      "learning_rate": 4.4670383482979296e-05,
      "loss": 0.694,
      "step": 700900
    },
    {
      "epoch": 6.396452295788014,
      "grad_norm": 5.1293044090271,
      "learning_rate": 4.4669623086843326e-05,
      "loss": 0.6978,
      "step": 701000
    },
    {
      "epoch": 6.397364771151179,
      "grad_norm": 4.277797698974609,
      "learning_rate": 4.466886269070735e-05,
      "loss": 0.6961,
      "step": 701100
    },
    {
      "epoch": 6.398277246514344,
      "grad_norm": 3.502347707748413,
      "learning_rate": 4.4668102294571387e-05,
      "loss": 0.7395,
      "step": 701200
    },
    {
      "epoch": 6.39918972187751,
      "grad_norm": 3.7673144340515137,
      "learning_rate": 4.466734189843541e-05,
      "loss": 0.7119,
      "step": 701300
    },
    {
      "epoch": 6.400102197240675,
      "grad_norm": 4.254380226135254,
      "learning_rate": 4.466658150229944e-05,
      "loss": 0.695,
      "step": 701400
    },
    {
      "epoch": 6.401014672603839,
      "grad_norm": 4.1248626708984375,
      "learning_rate": 4.466582110616347e-05,
      "loss": 0.7003,
      "step": 701500
    },
    {
      "epoch": 6.401927147967005,
      "grad_norm": 3.5167407989501953,
      "learning_rate": 4.466506071002749e-05,
      "loss": 0.6925,
      "step": 701600
    },
    {
      "epoch": 6.40283962333017,
      "grad_norm": 4.531363487243652,
      "learning_rate": 4.466430031389153e-05,
      "loss": 0.7,
      "step": 701700
    },
    {
      "epoch": 6.403752098693335,
      "grad_norm": 2.416163444519043,
      "learning_rate": 4.4663539917755553e-05,
      "loss": 0.6959,
      "step": 701800
    },
    {
      "epoch": 6.4046645740565005,
      "grad_norm": 3.4445013999938965,
      "learning_rate": 4.4662779521619584e-05,
      "loss": 0.7375,
      "step": 701900
    },
    {
      "epoch": 6.405577049419666,
      "grad_norm": 4.013191223144531,
      "learning_rate": 4.4662019125483614e-05,
      "loss": 0.691,
      "step": 702000
    },
    {
      "epoch": 6.406489524782831,
      "grad_norm": 3.789790391921997,
      "learning_rate": 4.4661258729347644e-05,
      "loss": 0.7012,
      "step": 702100
    },
    {
      "epoch": 6.407402000145996,
      "grad_norm": 4.513006687164307,
      "learning_rate": 4.466049833321167e-05,
      "loss": 0.7079,
      "step": 702200
    },
    {
      "epoch": 6.408314475509162,
      "grad_norm": 4.636962890625,
      "learning_rate": 4.4659737937075704e-05,
      "loss": 0.7046,
      "step": 702300
    },
    {
      "epoch": 6.409226950872326,
      "grad_norm": 4.280466556549072,
      "learning_rate": 4.465897754093973e-05,
      "loss": 0.7143,
      "step": 702400
    },
    {
      "epoch": 6.410139426235491,
      "grad_norm": 3.1998250484466553,
      "learning_rate": 4.465821714480376e-05,
      "loss": 0.7248,
      "step": 702500
    },
    {
      "epoch": 6.411051901598657,
      "grad_norm": 3.336534261703491,
      "learning_rate": 4.465745674866779e-05,
      "loss": 0.7506,
      "step": 702600
    },
    {
      "epoch": 6.411964376961822,
      "grad_norm": 3.915518283843994,
      "learning_rate": 4.465669635253182e-05,
      "loss": 0.6753,
      "step": 702700
    },
    {
      "epoch": 6.412876852324987,
      "grad_norm": 4.794569492340088,
      "learning_rate": 4.465593595639585e-05,
      "loss": 0.7333,
      "step": 702800
    },
    {
      "epoch": 6.413789327688153,
      "grad_norm": 4.10563325881958,
      "learning_rate": 4.465517556025988e-05,
      "loss": 0.7205,
      "step": 702900
    },
    {
      "epoch": 6.414701803051318,
      "grad_norm": 5.2587456703186035,
      "learning_rate": 4.46544151641239e-05,
      "loss": 0.703,
      "step": 703000
    },
    {
      "epoch": 6.415614278414483,
      "grad_norm": 4.324973106384277,
      "learning_rate": 4.465365476798794e-05,
      "loss": 0.687,
      "step": 703100
    },
    {
      "epoch": 6.416526753777648,
      "grad_norm": 4.139885425567627,
      "learning_rate": 4.465289437185196e-05,
      "loss": 0.6789,
      "step": 703200
    },
    {
      "epoch": 6.417439229140813,
      "grad_norm": 3.921635150909424,
      "learning_rate": 4.465213397571599e-05,
      "loss": 0.7088,
      "step": 703300
    },
    {
      "epoch": 6.418351704503978,
      "grad_norm": 3.455991744995117,
      "learning_rate": 4.465137357958002e-05,
      "loss": 0.7194,
      "step": 703400
    },
    {
      "epoch": 6.4192641798671435,
      "grad_norm": 3.4323253631591797,
      "learning_rate": 4.465061318344405e-05,
      "loss": 0.6734,
      "step": 703500
    },
    {
      "epoch": 6.420176655230309,
      "grad_norm": 4.738193988800049,
      "learning_rate": 4.4649852787308074e-05,
      "loss": 0.6657,
      "step": 703600
    },
    {
      "epoch": 6.421089130593474,
      "grad_norm": 3.491605043411255,
      "learning_rate": 4.464909239117211e-05,
      "loss": 0.6721,
      "step": 703700
    },
    {
      "epoch": 6.422001605956639,
      "grad_norm": 5.177174091339111,
      "learning_rate": 4.4648331995036134e-05,
      "loss": 0.7146,
      "step": 703800
    },
    {
      "epoch": 6.422914081319805,
      "grad_norm": 4.628250598907471,
      "learning_rate": 4.4647571598900165e-05,
      "loss": 0.7284,
      "step": 703900
    },
    {
      "epoch": 6.42382655668297,
      "grad_norm": 4.67479133605957,
      "learning_rate": 4.4646811202764195e-05,
      "loss": 0.6803,
      "step": 704000
    },
    {
      "epoch": 6.424739032046134,
      "grad_norm": 4.41634464263916,
      "learning_rate": 4.4646050806628225e-05,
      "loss": 0.7182,
      "step": 704100
    },
    {
      "epoch": 6.4256515074093,
      "grad_norm": 5.128959655761719,
      "learning_rate": 4.4645290410492255e-05,
      "loss": 0.7288,
      "step": 704200
    },
    {
      "epoch": 6.426563982772465,
      "grad_norm": 3.6928679943084717,
      "learning_rate": 4.4644530014356285e-05,
      "loss": 0.6768,
      "step": 704300
    },
    {
      "epoch": 6.42747645813563,
      "grad_norm": 6.137889385223389,
      "learning_rate": 4.464376961822031e-05,
      "loss": 0.6995,
      "step": 704400
    },
    {
      "epoch": 6.428388933498796,
      "grad_norm": 3.460669994354248,
      "learning_rate": 4.464300922208434e-05,
      "loss": 0.7444,
      "step": 704500
    },
    {
      "epoch": 6.429301408861961,
      "grad_norm": 4.436398029327393,
      "learning_rate": 4.464224882594837e-05,
      "loss": 0.7073,
      "step": 704600
    },
    {
      "epoch": 6.430213884225126,
      "grad_norm": 3.3492722511291504,
      "learning_rate": 4.464148842981239e-05,
      "loss": 0.6946,
      "step": 704700
    },
    {
      "epoch": 6.4311263595882915,
      "grad_norm": 3.972666025161743,
      "learning_rate": 4.464072803367643e-05,
      "loss": 0.7012,
      "step": 704800
    },
    {
      "epoch": 6.432038834951456,
      "grad_norm": 3.8245604038238525,
      "learning_rate": 4.463996763754045e-05,
      "loss": 0.6743,
      "step": 704900
    },
    {
      "epoch": 6.432951310314621,
      "grad_norm": 4.450750350952148,
      "learning_rate": 4.463920724140448e-05,
      "loss": 0.7425,
      "step": 705000
    },
    {
      "epoch": 6.4338637856777865,
      "grad_norm": 4.496067523956299,
      "learning_rate": 4.463844684526851e-05,
      "loss": 0.7184,
      "step": 705100
    },
    {
      "epoch": 6.434776261040952,
      "grad_norm": 3.7107722759246826,
      "learning_rate": 4.463768644913254e-05,
      "loss": 0.7178,
      "step": 705200
    },
    {
      "epoch": 6.435688736404117,
      "grad_norm": 8.249271392822266,
      "learning_rate": 4.463692605299657e-05,
      "loss": 0.6862,
      "step": 705300
    },
    {
      "epoch": 6.436601211767282,
      "grad_norm": 4.394241809844971,
      "learning_rate": 4.46361656568606e-05,
      "loss": 0.7003,
      "step": 705400
    },
    {
      "epoch": 6.437513687130448,
      "grad_norm": 4.176745414733887,
      "learning_rate": 4.4635405260724625e-05,
      "loss": 0.6917,
      "step": 705500
    },
    {
      "epoch": 6.438426162493613,
      "grad_norm": 2.493168830871582,
      "learning_rate": 4.463464486458866e-05,
      "loss": 0.7033,
      "step": 705600
    },
    {
      "epoch": 6.439338637856778,
      "grad_norm": 5.0042314529418945,
      "learning_rate": 4.4633884468452685e-05,
      "loss": 0.7017,
      "step": 705700
    },
    {
      "epoch": 6.440251113219943,
      "grad_norm": 2.857969284057617,
      "learning_rate": 4.4633124072316715e-05,
      "loss": 0.6936,
      "step": 705800
    },
    {
      "epoch": 6.441163588583108,
      "grad_norm": 4.311275959014893,
      "learning_rate": 4.4632363676180746e-05,
      "loss": 0.7182,
      "step": 705900
    },
    {
      "epoch": 6.442076063946273,
      "grad_norm": 3.810927152633667,
      "learning_rate": 4.4631603280044776e-05,
      "loss": 0.6985,
      "step": 706000
    },
    {
      "epoch": 6.442988539309439,
      "grad_norm": 3.2783944606781006,
      "learning_rate": 4.46308428839088e-05,
      "loss": 0.7136,
      "step": 706100
    },
    {
      "epoch": 6.443901014672604,
      "grad_norm": 4.508196830749512,
      "learning_rate": 4.4630082487772836e-05,
      "loss": 0.7085,
      "step": 706200
    },
    {
      "epoch": 6.444813490035769,
      "grad_norm": 3.9224298000335693,
      "learning_rate": 4.462932209163686e-05,
      "loss": 0.6951,
      "step": 706300
    },
    {
      "epoch": 6.4457259653989345,
      "grad_norm": 3.7238152027130127,
      "learning_rate": 4.462856169550089e-05,
      "loss": 0.6824,
      "step": 706400
    },
    {
      "epoch": 6.4466384407621,
      "grad_norm": 3.4653279781341553,
      "learning_rate": 4.462780129936492e-05,
      "loss": 0.6544,
      "step": 706500
    },
    {
      "epoch": 6.447550916125264,
      "grad_norm": 3.9278295040130615,
      "learning_rate": 4.462704090322895e-05,
      "loss": 0.6978,
      "step": 706600
    },
    {
      "epoch": 6.4484633914884295,
      "grad_norm": 3.751295566558838,
      "learning_rate": 4.462628050709298e-05,
      "loss": 0.7232,
      "step": 706700
    },
    {
      "epoch": 6.449375866851595,
      "grad_norm": 4.970921993255615,
      "learning_rate": 4.462552011095701e-05,
      "loss": 0.6917,
      "step": 706800
    },
    {
      "epoch": 6.45028834221476,
      "grad_norm": 2.6971561908721924,
      "learning_rate": 4.462475971482103e-05,
      "loss": 0.6946,
      "step": 706900
    },
    {
      "epoch": 6.451200817577925,
      "grad_norm": 3.5991437435150146,
      "learning_rate": 4.462399931868507e-05,
      "loss": 0.6734,
      "step": 707000
    },
    {
      "epoch": 6.452113292941091,
      "grad_norm": 3.4192957878112793,
      "learning_rate": 4.462323892254909e-05,
      "loss": 0.6763,
      "step": 707100
    },
    {
      "epoch": 6.453025768304256,
      "grad_norm": 3.7984044551849365,
      "learning_rate": 4.462247852641312e-05,
      "loss": 0.6835,
      "step": 707200
    },
    {
      "epoch": 6.453938243667421,
      "grad_norm": 3.8760898113250732,
      "learning_rate": 4.462171813027715e-05,
      "loss": 0.7046,
      "step": 707300
    },
    {
      "epoch": 6.454850719030587,
      "grad_norm": 3.926079511642456,
      "learning_rate": 4.4620957734141176e-05,
      "loss": 0.6856,
      "step": 707400
    },
    {
      "epoch": 6.455763194393751,
      "grad_norm": 3.504823684692383,
      "learning_rate": 4.4620197338005206e-05,
      "loss": 0.6879,
      "step": 707500
    },
    {
      "epoch": 6.456675669756916,
      "grad_norm": 4.8127336502075195,
      "learning_rate": 4.4619436941869236e-05,
      "loss": 0.6918,
      "step": 707600
    },
    {
      "epoch": 6.457588145120082,
      "grad_norm": 3.3806562423706055,
      "learning_rate": 4.4618676545733266e-05,
      "loss": 0.7504,
      "step": 707700
    },
    {
      "epoch": 6.458500620483247,
      "grad_norm": 3.3952138423919678,
      "learning_rate": 4.4617916149597296e-05,
      "loss": 0.7153,
      "step": 707800
    },
    {
      "epoch": 6.459413095846412,
      "grad_norm": 4.345174312591553,
      "learning_rate": 4.4617155753461327e-05,
      "loss": 0.7001,
      "step": 707900
    },
    {
      "epoch": 6.4603255712095775,
      "grad_norm": 4.307764530181885,
      "learning_rate": 4.461639535732535e-05,
      "loss": 0.7107,
      "step": 708000
    },
    {
      "epoch": 6.461238046572743,
      "grad_norm": 4.212578296661377,
      "learning_rate": 4.461563496118939e-05,
      "loss": 0.7181,
      "step": 708100
    },
    {
      "epoch": 6.462150521935908,
      "grad_norm": 3.68623685836792,
      "learning_rate": 4.461487456505341e-05,
      "loss": 0.711,
      "step": 708200
    },
    {
      "epoch": 6.4630629972990725,
      "grad_norm": 4.2654547691345215,
      "learning_rate": 4.461411416891744e-05,
      "loss": 0.6851,
      "step": 708300
    },
    {
      "epoch": 6.463975472662238,
      "grad_norm": 4.048349380493164,
      "learning_rate": 4.461335377278147e-05,
      "loss": 0.6871,
      "step": 708400
    },
    {
      "epoch": 6.464887948025403,
      "grad_norm": 3.489960193634033,
      "learning_rate": 4.46125933766455e-05,
      "loss": 0.7387,
      "step": 708500
    },
    {
      "epoch": 6.465800423388568,
      "grad_norm": 6.12792444229126,
      "learning_rate": 4.461183298050953e-05,
      "loss": 0.7221,
      "step": 708600
    },
    {
      "epoch": 6.466712898751734,
      "grad_norm": 4.351023197174072,
      "learning_rate": 4.461107258437356e-05,
      "loss": 0.6659,
      "step": 708700
    },
    {
      "epoch": 6.467625374114899,
      "grad_norm": 3.979740858078003,
      "learning_rate": 4.4610312188237584e-05,
      "loss": 0.7099,
      "step": 708800
    },
    {
      "epoch": 6.468537849478064,
      "grad_norm": 3.9509341716766357,
      "learning_rate": 4.4609551792101614e-05,
      "loss": 0.7027,
      "step": 708900
    },
    {
      "epoch": 6.46945032484123,
      "grad_norm": 3.408679962158203,
      "learning_rate": 4.4608791395965644e-05,
      "loss": 0.6854,
      "step": 709000
    },
    {
      "epoch": 6.470362800204395,
      "grad_norm": 5.241026878356934,
      "learning_rate": 4.4608030999829674e-05,
      "loss": 0.695,
      "step": 709100
    },
    {
      "epoch": 6.471275275567559,
      "grad_norm": 4.043253421783447,
      "learning_rate": 4.4607270603693704e-05,
      "loss": 0.7323,
      "step": 709200
    },
    {
      "epoch": 6.472187750930725,
      "grad_norm": 4.795631408691406,
      "learning_rate": 4.4606510207557734e-05,
      "loss": 0.7197,
      "step": 709300
    },
    {
      "epoch": 6.47310022629389,
      "grad_norm": 3.1911144256591797,
      "learning_rate": 4.460574981142176e-05,
      "loss": 0.6918,
      "step": 709400
    },
    {
      "epoch": 6.474012701657055,
      "grad_norm": 5.090374946594238,
      "learning_rate": 4.4604989415285794e-05,
      "loss": 0.6971,
      "step": 709500
    },
    {
      "epoch": 6.4749251770202205,
      "grad_norm": 4.042408466339111,
      "learning_rate": 4.460422901914982e-05,
      "loss": 0.7293,
      "step": 709600
    },
    {
      "epoch": 6.475837652383386,
      "grad_norm": 3.5175609588623047,
      "learning_rate": 4.460346862301385e-05,
      "loss": 0.6979,
      "step": 709700
    },
    {
      "epoch": 6.476750127746551,
      "grad_norm": 3.00980544090271,
      "learning_rate": 4.460270822687788e-05,
      "loss": 0.7168,
      "step": 709800
    },
    {
      "epoch": 6.477662603109716,
      "grad_norm": 4.397878646850586,
      "learning_rate": 4.460194783074191e-05,
      "loss": 0.7154,
      "step": 709900
    },
    {
      "epoch": 6.478575078472881,
      "grad_norm": 4.721141338348389,
      "learning_rate": 4.460118743460594e-05,
      "loss": 0.6959,
      "step": 710000
    },
    {
      "epoch": 6.479487553836046,
      "grad_norm": 4.745687961578369,
      "learning_rate": 4.460042703846996e-05,
      "loss": 0.7199,
      "step": 710100
    },
    {
      "epoch": 6.480400029199211,
      "grad_norm": 4.395127773284912,
      "learning_rate": 4.459966664233399e-05,
      "loss": 0.7131,
      "step": 710200
    },
    {
      "epoch": 6.481312504562377,
      "grad_norm": 4.402667999267578,
      "learning_rate": 4.459890624619802e-05,
      "loss": 0.7241,
      "step": 710300
    },
    {
      "epoch": 6.482224979925542,
      "grad_norm": 3.785511016845703,
      "learning_rate": 4.459814585006205e-05,
      "loss": 0.6975,
      "step": 710400
    },
    {
      "epoch": 6.483137455288707,
      "grad_norm": 3.693634033203125,
      "learning_rate": 4.4597385453926074e-05,
      "loss": 0.6717,
      "step": 710500
    },
    {
      "epoch": 6.484049930651873,
      "grad_norm": 4.594852447509766,
      "learning_rate": 4.459662505779011e-05,
      "loss": 0.6969,
      "step": 710600
    },
    {
      "epoch": 6.484962406015038,
      "grad_norm": 4.296628475189209,
      "learning_rate": 4.4595864661654135e-05,
      "loss": 0.7179,
      "step": 710700
    },
    {
      "epoch": 6.485874881378203,
      "grad_norm": 3.5903851985931396,
      "learning_rate": 4.4595104265518165e-05,
      "loss": 0.6629,
      "step": 710800
    },
    {
      "epoch": 6.486787356741368,
      "grad_norm": 3.6911373138427734,
      "learning_rate": 4.4594343869382195e-05,
      "loss": 0.6691,
      "step": 710900
    },
    {
      "epoch": 6.487699832104533,
      "grad_norm": 4.220050811767578,
      "learning_rate": 4.4593583473246225e-05,
      "loss": 0.7153,
      "step": 711000
    },
    {
      "epoch": 6.488612307467698,
      "grad_norm": 5.117668151855469,
      "learning_rate": 4.4592823077110255e-05,
      "loss": 0.6942,
      "step": 711100
    },
    {
      "epoch": 6.4895247828308635,
      "grad_norm": 4.376303672790527,
      "learning_rate": 4.4592062680974285e-05,
      "loss": 0.649,
      "step": 711200
    },
    {
      "epoch": 6.490437258194029,
      "grad_norm": 3.3516407012939453,
      "learning_rate": 4.459130228483831e-05,
      "loss": 0.7546,
      "step": 711300
    },
    {
      "epoch": 6.491349733557194,
      "grad_norm": 3.6307530403137207,
      "learning_rate": 4.4590541888702345e-05,
      "loss": 0.6599,
      "step": 711400
    },
    {
      "epoch": 6.492262208920359,
      "grad_norm": 3.867703676223755,
      "learning_rate": 4.458978149256637e-05,
      "loss": 0.6987,
      "step": 711500
    },
    {
      "epoch": 6.493174684283525,
      "grad_norm": 4.675803184509277,
      "learning_rate": 4.45890210964304e-05,
      "loss": 0.7497,
      "step": 711600
    },
    {
      "epoch": 6.494087159646689,
      "grad_norm": 3.913264751434326,
      "learning_rate": 4.458826070029443e-05,
      "loss": 0.774,
      "step": 711700
    },
    {
      "epoch": 6.494999635009854,
      "grad_norm": 3.9184772968292236,
      "learning_rate": 4.458750030415846e-05,
      "loss": 0.6938,
      "step": 711800
    },
    {
      "epoch": 6.49591211037302,
      "grad_norm": 4.177427291870117,
      "learning_rate": 4.458673990802248e-05,
      "loss": 0.7074,
      "step": 711900
    },
    {
      "epoch": 6.496824585736185,
      "grad_norm": 4.546198844909668,
      "learning_rate": 4.458597951188652e-05,
      "loss": 0.7058,
      "step": 712000
    },
    {
      "epoch": 6.49773706109935,
      "grad_norm": 3.8760476112365723,
      "learning_rate": 4.458521911575054e-05,
      "loss": 0.7034,
      "step": 712100
    },
    {
      "epoch": 6.498649536462516,
      "grad_norm": 4.645260334014893,
      "learning_rate": 4.458445871961457e-05,
      "loss": 0.6956,
      "step": 712200
    },
    {
      "epoch": 6.499562011825681,
      "grad_norm": 3.0677971839904785,
      "learning_rate": 4.45836983234786e-05,
      "loss": 0.683,
      "step": 712300
    },
    {
      "epoch": 6.500474487188846,
      "grad_norm": 4.556290149688721,
      "learning_rate": 4.458293792734263e-05,
      "loss": 0.7171,
      "step": 712400
    },
    {
      "epoch": 6.5013869625520115,
      "grad_norm": 3.960597038269043,
      "learning_rate": 4.458217753120666e-05,
      "loss": 0.6909,
      "step": 712500
    },
    {
      "epoch": 6.502299437915176,
      "grad_norm": 4.121243000030518,
      "learning_rate": 4.458141713507069e-05,
      "loss": 0.7136,
      "step": 712600
    },
    {
      "epoch": 6.503211913278341,
      "grad_norm": 3.448287010192871,
      "learning_rate": 4.4580656738934716e-05,
      "loss": 0.6823,
      "step": 712700
    },
    {
      "epoch": 6.5041243886415065,
      "grad_norm": 4.049189567565918,
      "learning_rate": 4.457989634279875e-05,
      "loss": 0.6746,
      "step": 712800
    },
    {
      "epoch": 6.505036864004672,
      "grad_norm": 4.08564567565918,
      "learning_rate": 4.4579135946662776e-05,
      "loss": 0.7236,
      "step": 712900
    },
    {
      "epoch": 6.505949339367837,
      "grad_norm": 4.219576835632324,
      "learning_rate": 4.45783755505268e-05,
      "loss": 0.7393,
      "step": 713000
    },
    {
      "epoch": 6.5068618147310024,
      "grad_norm": 4.23715877532959,
      "learning_rate": 4.4577615154390836e-05,
      "loss": 0.7,
      "step": 713100
    },
    {
      "epoch": 6.507774290094168,
      "grad_norm": 5.221005439758301,
      "learning_rate": 4.457685475825486e-05,
      "loss": 0.6501,
      "step": 713200
    },
    {
      "epoch": 6.508686765457332,
      "grad_norm": 4.455616474151611,
      "learning_rate": 4.457609436211889e-05,
      "loss": 0.6828,
      "step": 713300
    },
    {
      "epoch": 6.5095992408204975,
      "grad_norm": 2.8996222019195557,
      "learning_rate": 4.457533396598292e-05,
      "loss": 0.703,
      "step": 713400
    },
    {
      "epoch": 6.510511716183663,
      "grad_norm": 3.641951084136963,
      "learning_rate": 4.457457356984695e-05,
      "loss": 0.7178,
      "step": 713500
    },
    {
      "epoch": 6.511424191546828,
      "grad_norm": 4.81974983215332,
      "learning_rate": 4.457381317371098e-05,
      "loss": 0.7166,
      "step": 713600
    },
    {
      "epoch": 6.512336666909993,
      "grad_norm": 3.3156590461730957,
      "learning_rate": 4.457305277757501e-05,
      "loss": 0.7333,
      "step": 713700
    },
    {
      "epoch": 6.513249142273159,
      "grad_norm": 4.742046356201172,
      "learning_rate": 4.457229238143903e-05,
      "loss": 0.6688,
      "step": 713800
    },
    {
      "epoch": 6.514161617636324,
      "grad_norm": 4.331933498382568,
      "learning_rate": 4.457153198530307e-05,
      "loss": 0.7346,
      "step": 713900
    },
    {
      "epoch": 6.515074092999489,
      "grad_norm": 3.8878588676452637,
      "learning_rate": 4.457077158916709e-05,
      "loss": 0.7351,
      "step": 714000
    },
    {
      "epoch": 6.5159865683626546,
      "grad_norm": 4.095713138580322,
      "learning_rate": 4.457001119303112e-05,
      "loss": 0.7213,
      "step": 714100
    },
    {
      "epoch": 6.51689904372582,
      "grad_norm": 4.183657646179199,
      "learning_rate": 4.456925079689515e-05,
      "loss": 0.7156,
      "step": 714200
    },
    {
      "epoch": 6.517811519088984,
      "grad_norm": 4.805415153503418,
      "learning_rate": 4.456849040075918e-05,
      "loss": 0.6754,
      "step": 714300
    },
    {
      "epoch": 6.51872399445215,
      "grad_norm": 4.008872985839844,
      "learning_rate": 4.4567730004623206e-05,
      "loss": 0.757,
      "step": 714400
    },
    {
      "epoch": 6.519636469815315,
      "grad_norm": 5.25453519821167,
      "learning_rate": 4.456696960848724e-05,
      "loss": 0.7326,
      "step": 714500
    },
    {
      "epoch": 6.52054894517848,
      "grad_norm": 4.233522891998291,
      "learning_rate": 4.4566209212351267e-05,
      "loss": 0.672,
      "step": 714600
    },
    {
      "epoch": 6.5214614205416455,
      "grad_norm": 4.072198867797852,
      "learning_rate": 4.4565448816215297e-05,
      "loss": 0.7338,
      "step": 714700
    },
    {
      "epoch": 6.522373895904811,
      "grad_norm": 4.125819206237793,
      "learning_rate": 4.456468842007933e-05,
      "loss": 0.7447,
      "step": 714800
    },
    {
      "epoch": 6.523286371267976,
      "grad_norm": 3.38407564163208,
      "learning_rate": 4.456392802394336e-05,
      "loss": 0.6873,
      "step": 714900
    },
    {
      "epoch": 6.5241988466311405,
      "grad_norm": 3.6195685863494873,
      "learning_rate": 4.456316762780739e-05,
      "loss": 0.7154,
      "step": 715000
    },
    {
      "epoch": 6.525111321994306,
      "grad_norm": 3.4571917057037354,
      "learning_rate": 4.456240723167142e-05,
      "loss": 0.6965,
      "step": 715100
    },
    {
      "epoch": 6.526023797357471,
      "grad_norm": 3.4117352962493896,
      "learning_rate": 4.456164683553544e-05,
      "loss": 0.7076,
      "step": 715200
    },
    {
      "epoch": 6.526936272720636,
      "grad_norm": 3.8629064559936523,
      "learning_rate": 4.456088643939948e-05,
      "loss": 0.6798,
      "step": 715300
    },
    {
      "epoch": 6.527848748083802,
      "grad_norm": 4.465033531188965,
      "learning_rate": 4.45601260432635e-05,
      "loss": 0.6663,
      "step": 715400
    },
    {
      "epoch": 6.528761223446967,
      "grad_norm": 3.4242217540740967,
      "learning_rate": 4.455936564712753e-05,
      "loss": 0.6865,
      "step": 715500
    },
    {
      "epoch": 6.529673698810132,
      "grad_norm": 4.422535419464111,
      "learning_rate": 4.455860525099156e-05,
      "loss": 0.7131,
      "step": 715600
    },
    {
      "epoch": 6.530586174173298,
      "grad_norm": 3.853999376296997,
      "learning_rate": 4.455784485485559e-05,
      "loss": 0.7042,
      "step": 715700
    },
    {
      "epoch": 6.531498649536463,
      "grad_norm": 3.836799144744873,
      "learning_rate": 4.4557084458719614e-05,
      "loss": 0.7118,
      "step": 715800
    },
    {
      "epoch": 6.532411124899628,
      "grad_norm": 4.575750350952148,
      "learning_rate": 4.4556324062583644e-05,
      "loss": 0.6469,
      "step": 715900
    },
    {
      "epoch": 6.533323600262793,
      "grad_norm": 3.6740121841430664,
      "learning_rate": 4.4555563666447674e-05,
      "loss": 0.6737,
      "step": 716000
    },
    {
      "epoch": 6.534236075625958,
      "grad_norm": 4.885090351104736,
      "learning_rate": 4.4554803270311704e-05,
      "loss": 0.6882,
      "step": 716100
    },
    {
      "epoch": 6.535148550989123,
      "grad_norm": 4.055964946746826,
      "learning_rate": 4.4554042874175734e-05,
      "loss": 0.6797,
      "step": 716200
    },
    {
      "epoch": 6.5360610263522885,
      "grad_norm": 4.55148983001709,
      "learning_rate": 4.455328247803976e-05,
      "loss": 0.6491,
      "step": 716300
    },
    {
      "epoch": 6.536973501715454,
      "grad_norm": 3.6171011924743652,
      "learning_rate": 4.4552522081903794e-05,
      "loss": 0.6711,
      "step": 716400
    },
    {
      "epoch": 6.537885977078619,
      "grad_norm": 8.037765502929688,
      "learning_rate": 4.455176168576782e-05,
      "loss": 0.6707,
      "step": 716500
    },
    {
      "epoch": 6.538798452441784,
      "grad_norm": 3.0856127738952637,
      "learning_rate": 4.455100128963185e-05,
      "loss": 0.709,
      "step": 716600
    },
    {
      "epoch": 6.539710927804949,
      "grad_norm": 4.03224515914917,
      "learning_rate": 4.455024089349588e-05,
      "loss": 0.7173,
      "step": 716700
    },
    {
      "epoch": 6.540623403168114,
      "grad_norm": 4.316065311431885,
      "learning_rate": 4.454948049735991e-05,
      "loss": 0.7182,
      "step": 716800
    },
    {
      "epoch": 6.541535878531279,
      "grad_norm": 4.72383451461792,
      "learning_rate": 4.454872010122393e-05,
      "loss": 0.6795,
      "step": 716900
    },
    {
      "epoch": 6.542448353894445,
      "grad_norm": 4.954562187194824,
      "learning_rate": 4.454795970508797e-05,
      "loss": 0.7325,
      "step": 717000
    },
    {
      "epoch": 6.54336082925761,
      "grad_norm": 5.044348239898682,
      "learning_rate": 4.454719930895199e-05,
      "loss": 0.7132,
      "step": 717100
    },
    {
      "epoch": 6.544273304620775,
      "grad_norm": 5.193408012390137,
      "learning_rate": 4.454643891281602e-05,
      "loss": 0.6939,
      "step": 717200
    },
    {
      "epoch": 6.545185779983941,
      "grad_norm": 3.7334678173065186,
      "learning_rate": 4.454567851668005e-05,
      "loss": 0.7166,
      "step": 717300
    },
    {
      "epoch": 6.546098255347106,
      "grad_norm": 3.983313798904419,
      "learning_rate": 4.454491812054408e-05,
      "loss": 0.7092,
      "step": 717400
    },
    {
      "epoch": 6.547010730710271,
      "grad_norm": 3.202629327774048,
      "learning_rate": 4.454415772440811e-05,
      "loss": 0.698,
      "step": 717500
    },
    {
      "epoch": 6.5479232060734365,
      "grad_norm": 3.549325466156006,
      "learning_rate": 4.454339732827214e-05,
      "loss": 0.7331,
      "step": 717600
    },
    {
      "epoch": 6.548835681436601,
      "grad_norm": 3.9059600830078125,
      "learning_rate": 4.4542636932136165e-05,
      "loss": 0.7009,
      "step": 717700
    },
    {
      "epoch": 6.549748156799766,
      "grad_norm": 3.2852306365966797,
      "learning_rate": 4.45418765360002e-05,
      "loss": 0.6769,
      "step": 717800
    },
    {
      "epoch": 6.5506606321629315,
      "grad_norm": 4.055183410644531,
      "learning_rate": 4.4541116139864225e-05,
      "loss": 0.6578,
      "step": 717900
    },
    {
      "epoch": 6.551573107526097,
      "grad_norm": 4.7334489822387695,
      "learning_rate": 4.4540355743728255e-05,
      "loss": 0.7016,
      "step": 718000
    },
    {
      "epoch": 6.552485582889262,
      "grad_norm": 3.9676012992858887,
      "learning_rate": 4.4539595347592285e-05,
      "loss": 0.6694,
      "step": 718100
    },
    {
      "epoch": 6.553398058252427,
      "grad_norm": 4.299698352813721,
      "learning_rate": 4.4538834951456315e-05,
      "loss": 0.7164,
      "step": 718200
    },
    {
      "epoch": 6.554310533615593,
      "grad_norm": 4.35547399520874,
      "learning_rate": 4.453807455532034e-05,
      "loss": 0.7209,
      "step": 718300
    },
    {
      "epoch": 6.555223008978757,
      "grad_norm": 5.994194984436035,
      "learning_rate": 4.4537314159184375e-05,
      "loss": 0.7075,
      "step": 718400
    },
    {
      "epoch": 6.556135484341922,
      "grad_norm": 3.716050863265991,
      "learning_rate": 4.45365537630484e-05,
      "loss": 0.6982,
      "step": 718500
    },
    {
      "epoch": 6.557047959705088,
      "grad_norm": 4.502295970916748,
      "learning_rate": 4.453579336691243e-05,
      "loss": 0.7204,
      "step": 718600
    },
    {
      "epoch": 6.557960435068253,
      "grad_norm": 4.814696788787842,
      "learning_rate": 4.453503297077646e-05,
      "loss": 0.7166,
      "step": 718700
    },
    {
      "epoch": 6.558872910431418,
      "grad_norm": 5.019121170043945,
      "learning_rate": 4.453427257464048e-05,
      "loss": 0.6775,
      "step": 718800
    },
    {
      "epoch": 6.559785385794584,
      "grad_norm": 3.45759916305542,
      "learning_rate": 4.453351217850452e-05,
      "loss": 0.7336,
      "step": 718900
    },
    {
      "epoch": 6.560697861157749,
      "grad_norm": 3.812476634979248,
      "learning_rate": 4.453275178236854e-05,
      "loss": 0.6999,
      "step": 719000
    },
    {
      "epoch": 6.561610336520914,
      "grad_norm": 4.1443305015563965,
      "learning_rate": 4.453199138623257e-05,
      "loss": 0.6544,
      "step": 719100
    },
    {
      "epoch": 6.5625228118840795,
      "grad_norm": 3.9697933197021484,
      "learning_rate": 4.45312309900966e-05,
      "loss": 0.7215,
      "step": 719200
    },
    {
      "epoch": 6.563435287247245,
      "grad_norm": 3.2928266525268555,
      "learning_rate": 4.453047059396063e-05,
      "loss": 0.7248,
      "step": 719300
    },
    {
      "epoch": 6.564347762610409,
      "grad_norm": 4.111701011657715,
      "learning_rate": 4.4529710197824656e-05,
      "loss": 0.6992,
      "step": 719400
    },
    {
      "epoch": 6.5652602379735745,
      "grad_norm": 2.6197049617767334,
      "learning_rate": 4.452894980168869e-05,
      "loss": 0.7335,
      "step": 719500
    },
    {
      "epoch": 6.56617271333674,
      "grad_norm": 3.798903226852417,
      "learning_rate": 4.4528189405552716e-05,
      "loss": 0.7303,
      "step": 719600
    },
    {
      "epoch": 6.567085188699905,
      "grad_norm": 3.518203020095825,
      "learning_rate": 4.4527429009416746e-05,
      "loss": 0.6815,
      "step": 719700
    },
    {
      "epoch": 6.56799766406307,
      "grad_norm": 4.379878997802734,
      "learning_rate": 4.4526668613280776e-05,
      "loss": 0.7018,
      "step": 719800
    },
    {
      "epoch": 6.568910139426236,
      "grad_norm": 2.138880968093872,
      "learning_rate": 4.4525908217144806e-05,
      "loss": 0.6689,
      "step": 719900
    },
    {
      "epoch": 6.569822614789401,
      "grad_norm": 4.036642551422119,
      "learning_rate": 4.4525147821008836e-05,
      "loss": 0.696,
      "step": 720000
    },
    {
      "epoch": 6.570735090152565,
      "grad_norm": 3.8827269077301025,
      "learning_rate": 4.4524387424872866e-05,
      "loss": 0.7092,
      "step": 720100
    },
    {
      "epoch": 6.571647565515731,
      "grad_norm": 4.220145225524902,
      "learning_rate": 4.452362702873689e-05,
      "loss": 0.714,
      "step": 720200
    },
    {
      "epoch": 6.572560040878896,
      "grad_norm": 3.693249464035034,
      "learning_rate": 4.4522866632600926e-05,
      "loss": 0.6868,
      "step": 720300
    },
    {
      "epoch": 6.573472516242061,
      "grad_norm": 3.38566255569458,
      "learning_rate": 4.452210623646495e-05,
      "loss": 0.702,
      "step": 720400
    },
    {
      "epoch": 6.574384991605227,
      "grad_norm": 3.5486221313476562,
      "learning_rate": 4.452134584032898e-05,
      "loss": 0.7056,
      "step": 720500
    },
    {
      "epoch": 6.575297466968392,
      "grad_norm": 4.1944708824157715,
      "learning_rate": 4.452058544419301e-05,
      "loss": 0.6964,
      "step": 720600
    },
    {
      "epoch": 6.576209942331557,
      "grad_norm": 3.8776445388793945,
      "learning_rate": 4.451982504805704e-05,
      "loss": 0.7175,
      "step": 720700
    },
    {
      "epoch": 6.5771224176947225,
      "grad_norm": 4.536561489105225,
      "learning_rate": 4.451906465192107e-05,
      "loss": 0.6927,
      "step": 720800
    },
    {
      "epoch": 6.578034893057888,
      "grad_norm": 3.569322347640991,
      "learning_rate": 4.45183042557851e-05,
      "loss": 0.6821,
      "step": 720900
    },
    {
      "epoch": 6.578947368421053,
      "grad_norm": 3.7804858684539795,
      "learning_rate": 4.451754385964912e-05,
      "loss": 0.7196,
      "step": 721000
    },
    {
      "epoch": 6.5798598437842175,
      "grad_norm": 4.34857702255249,
      "learning_rate": 4.451678346351315e-05,
      "loss": 0.7299,
      "step": 721100
    },
    {
      "epoch": 6.580772319147383,
      "grad_norm": 3.7722973823547363,
      "learning_rate": 4.451602306737718e-05,
      "loss": 0.7328,
      "step": 721200
    },
    {
      "epoch": 6.581684794510548,
      "grad_norm": 4.1584792137146,
      "learning_rate": 4.451526267124121e-05,
      "loss": 0.6698,
      "step": 721300
    },
    {
      "epoch": 6.582597269873713,
      "grad_norm": 3.574951171875,
      "learning_rate": 4.451450227510524e-05,
      "loss": 0.6974,
      "step": 721400
    },
    {
      "epoch": 6.583509745236879,
      "grad_norm": 4.055882453918457,
      "learning_rate": 4.451374187896927e-05,
      "loss": 0.6999,
      "step": 721500
    },
    {
      "epoch": 6.584422220600044,
      "grad_norm": 3.867506980895996,
      "learning_rate": 4.45129814828333e-05,
      "loss": 0.6968,
      "step": 721600
    },
    {
      "epoch": 6.585334695963209,
      "grad_norm": 3.2217588424682617,
      "learning_rate": 4.451222108669733e-05,
      "loss": 0.6609,
      "step": 721700
    },
    {
      "epoch": 6.586247171326374,
      "grad_norm": 4.148773193359375,
      "learning_rate": 4.451146069056136e-05,
      "loss": 0.7025,
      "step": 721800
    },
    {
      "epoch": 6.587159646689539,
      "grad_norm": 4.120987415313721,
      "learning_rate": 4.451070029442539e-05,
      "loss": 0.7151,
      "step": 721900
    },
    {
      "epoch": 6.588072122052704,
      "grad_norm": 4.799786567687988,
      "learning_rate": 4.450993989828942e-05,
      "loss": 0.7034,
      "step": 722000
    },
    {
      "epoch": 6.58898459741587,
      "grad_norm": 4.004027843475342,
      "learning_rate": 4.450917950215344e-05,
      "loss": 0.7116,
      "step": 722100
    },
    {
      "epoch": 6.589897072779035,
      "grad_norm": 3.8396151065826416,
      "learning_rate": 4.450841910601748e-05,
      "loss": 0.6921,
      "step": 722200
    },
    {
      "epoch": 6.5908095481422,
      "grad_norm": 4.145822048187256,
      "learning_rate": 4.45076587098815e-05,
      "loss": 0.708,
      "step": 722300
    },
    {
      "epoch": 6.5917220235053655,
      "grad_norm": 4.029464244842529,
      "learning_rate": 4.450689831374553e-05,
      "loss": 0.6933,
      "step": 722400
    },
    {
      "epoch": 6.592634498868531,
      "grad_norm": 3.9036879539489746,
      "learning_rate": 4.450613791760956e-05,
      "loss": 0.7121,
      "step": 722500
    },
    {
      "epoch": 6.593546974231696,
      "grad_norm": 4.004250526428223,
      "learning_rate": 4.450537752147359e-05,
      "loss": 0.7035,
      "step": 722600
    },
    {
      "epoch": 6.594459449594861,
      "grad_norm": 4.338654041290283,
      "learning_rate": 4.4504617125337614e-05,
      "loss": 0.6774,
      "step": 722700
    },
    {
      "epoch": 6.595371924958026,
      "grad_norm": 3.503462076187134,
      "learning_rate": 4.450385672920165e-05,
      "loss": 0.733,
      "step": 722800
    },
    {
      "epoch": 6.596284400321191,
      "grad_norm": 4.234214782714844,
      "learning_rate": 4.4503096333065674e-05,
      "loss": 0.6952,
      "step": 722900
    },
    {
      "epoch": 6.597196875684356,
      "grad_norm": 4.356234073638916,
      "learning_rate": 4.4502335936929704e-05,
      "loss": 0.7024,
      "step": 723000
    },
    {
      "epoch": 6.598109351047522,
      "grad_norm": 3.390763759613037,
      "learning_rate": 4.4501575540793734e-05,
      "loss": 0.6807,
      "step": 723100
    },
    {
      "epoch": 6.599021826410687,
      "grad_norm": 3.37164568901062,
      "learning_rate": 4.4500815144657764e-05,
      "loss": 0.7086,
      "step": 723200
    },
    {
      "epoch": 6.599934301773852,
      "grad_norm": 4.469120502471924,
      "learning_rate": 4.4500054748521794e-05,
      "loss": 0.697,
      "step": 723300
    },
    {
      "epoch": 6.600846777137018,
      "grad_norm": 5.4676642417907715,
      "learning_rate": 4.4499294352385824e-05,
      "loss": 0.7412,
      "step": 723400
    },
    {
      "epoch": 6.601759252500182,
      "grad_norm": 5.532982349395752,
      "learning_rate": 4.449853395624985e-05,
      "loss": 0.7119,
      "step": 723500
    },
    {
      "epoch": 6.602671727863347,
      "grad_norm": 4.9981536865234375,
      "learning_rate": 4.4497773560113884e-05,
      "loss": 0.6912,
      "step": 723600
    },
    {
      "epoch": 6.603584203226513,
      "grad_norm": 4.230380058288574,
      "learning_rate": 4.449701316397791e-05,
      "loss": 0.7141,
      "step": 723700
    },
    {
      "epoch": 6.604496678589678,
      "grad_norm": 3.702205181121826,
      "learning_rate": 4.449625276784194e-05,
      "loss": 0.7138,
      "step": 723800
    },
    {
      "epoch": 6.605409153952843,
      "grad_norm": 3.768923759460449,
      "learning_rate": 4.449549237170597e-05,
      "loss": 0.6915,
      "step": 723900
    },
    {
      "epoch": 6.6063216293160085,
      "grad_norm": 4.871425628662109,
      "learning_rate": 4.449473197557e-05,
      "loss": 0.6552,
      "step": 724000
    },
    {
      "epoch": 6.607234104679174,
      "grad_norm": 4.533814430236816,
      "learning_rate": 4.449397157943402e-05,
      "loss": 0.7117,
      "step": 724100
    },
    {
      "epoch": 6.608146580042339,
      "grad_norm": 3.656559705734253,
      "learning_rate": 4.449321118329806e-05,
      "loss": 0.724,
      "step": 724200
    },
    {
      "epoch": 6.609059055405504,
      "grad_norm": 4.189270496368408,
      "learning_rate": 4.449245078716208e-05,
      "loss": 0.6778,
      "step": 724300
    },
    {
      "epoch": 6.60997153076867,
      "grad_norm": 4.043576717376709,
      "learning_rate": 4.449169039102611e-05,
      "loss": 0.6853,
      "step": 724400
    },
    {
      "epoch": 6.610884006131834,
      "grad_norm": 4.239159107208252,
      "learning_rate": 4.449092999489014e-05,
      "loss": 0.6973,
      "step": 724500
    },
    {
      "epoch": 6.611796481494999,
      "grad_norm": 4.663426876068115,
      "learning_rate": 4.4490169598754165e-05,
      "loss": 0.6972,
      "step": 724600
    },
    {
      "epoch": 6.612708956858165,
      "grad_norm": 3.55497670173645,
      "learning_rate": 4.44894092026182e-05,
      "loss": 0.6939,
      "step": 724700
    },
    {
      "epoch": 6.61362143222133,
      "grad_norm": 4.421754837036133,
      "learning_rate": 4.4488648806482225e-05,
      "loss": 0.7041,
      "step": 724800
    },
    {
      "epoch": 6.614533907584495,
      "grad_norm": 5.0678887367248535,
      "learning_rate": 4.4487888410346255e-05,
      "loss": 0.6901,
      "step": 724900
    },
    {
      "epoch": 6.615446382947661,
      "grad_norm": 4.436197280883789,
      "learning_rate": 4.4487128014210285e-05,
      "loss": 0.7005,
      "step": 725000
    },
    {
      "epoch": 6.616358858310826,
      "grad_norm": 2.712919235229492,
      "learning_rate": 4.4486367618074315e-05,
      "loss": 0.6914,
      "step": 725100
    },
    {
      "epoch": 6.61727133367399,
      "grad_norm": 4.20569372177124,
      "learning_rate": 4.448560722193834e-05,
      "loss": 0.7026,
      "step": 725200
    },
    {
      "epoch": 6.618183809037156,
      "grad_norm": 3.7540769577026367,
      "learning_rate": 4.4484846825802375e-05,
      "loss": 0.7219,
      "step": 725300
    },
    {
      "epoch": 6.619096284400321,
      "grad_norm": 3.7413291931152344,
      "learning_rate": 4.44840864296664e-05,
      "loss": 0.7199,
      "step": 725400
    },
    {
      "epoch": 6.620008759763486,
      "grad_norm": 3.9223883152008057,
      "learning_rate": 4.448332603353043e-05,
      "loss": 0.7207,
      "step": 725500
    },
    {
      "epoch": 6.6209212351266515,
      "grad_norm": 3.437610149383545,
      "learning_rate": 4.448256563739446e-05,
      "loss": 0.7261,
      "step": 725600
    },
    {
      "epoch": 6.621833710489817,
      "grad_norm": 3.578040838241577,
      "learning_rate": 4.448180524125849e-05,
      "loss": 0.6921,
      "step": 725700
    },
    {
      "epoch": 6.622746185852982,
      "grad_norm": 4.087769031524658,
      "learning_rate": 4.448104484512252e-05,
      "loss": 0.6999,
      "step": 725800
    },
    {
      "epoch": 6.623658661216147,
      "grad_norm": 4.751410961151123,
      "learning_rate": 4.448028444898655e-05,
      "loss": 0.7302,
      "step": 725900
    },
    {
      "epoch": 6.624571136579313,
      "grad_norm": 3.758004665374756,
      "learning_rate": 4.447952405285057e-05,
      "loss": 0.7352,
      "step": 726000
    },
    {
      "epoch": 6.625483611942478,
      "grad_norm": 3.2785520553588867,
      "learning_rate": 4.447876365671461e-05,
      "loss": 0.7121,
      "step": 726100
    },
    {
      "epoch": 6.6263960873056424,
      "grad_norm": 4.188968181610107,
      "learning_rate": 4.447800326057863e-05,
      "loss": 0.7024,
      "step": 726200
    },
    {
      "epoch": 6.627308562668808,
      "grad_norm": 4.367515563964844,
      "learning_rate": 4.447724286444266e-05,
      "loss": 0.6556,
      "step": 726300
    },
    {
      "epoch": 6.628221038031973,
      "grad_norm": 4.3390631675720215,
      "learning_rate": 4.447648246830669e-05,
      "loss": 0.6594,
      "step": 726400
    },
    {
      "epoch": 6.629133513395138,
      "grad_norm": 3.5793325901031494,
      "learning_rate": 4.447572207217072e-05,
      "loss": 0.7108,
      "step": 726500
    },
    {
      "epoch": 6.630045988758304,
      "grad_norm": 3.8902511596679688,
      "learning_rate": 4.4474961676034746e-05,
      "loss": 0.7256,
      "step": 726600
    },
    {
      "epoch": 6.630958464121469,
      "grad_norm": 5.173965930938721,
      "learning_rate": 4.447420127989878e-05,
      "loss": 0.7003,
      "step": 726700
    },
    {
      "epoch": 6.631870939484634,
      "grad_norm": 3.573756217956543,
      "learning_rate": 4.4473440883762806e-05,
      "loss": 0.7117,
      "step": 726800
    },
    {
      "epoch": 6.632783414847799,
      "grad_norm": 4.272743225097656,
      "learning_rate": 4.4472680487626836e-05,
      "loss": 0.7341,
      "step": 726900
    },
    {
      "epoch": 6.633695890210964,
      "grad_norm": 3.9871761798858643,
      "learning_rate": 4.4471920091490866e-05,
      "loss": 0.699,
      "step": 727000
    },
    {
      "epoch": 6.634608365574129,
      "grad_norm": 3.8794915676116943,
      "learning_rate": 4.447115969535489e-05,
      "loss": 0.7048,
      "step": 727100
    },
    {
      "epoch": 6.6355208409372946,
      "grad_norm": 3.37203049659729,
      "learning_rate": 4.4470399299218926e-05,
      "loss": 0.6775,
      "step": 727200
    },
    {
      "epoch": 6.63643331630046,
      "grad_norm": 3.5843281745910645,
      "learning_rate": 4.446963890308295e-05,
      "loss": 0.6988,
      "step": 727300
    },
    {
      "epoch": 6.637345791663625,
      "grad_norm": 4.1563801765441895,
      "learning_rate": 4.446887850694698e-05,
      "loss": 0.7228,
      "step": 727400
    },
    {
      "epoch": 6.6382582670267904,
      "grad_norm": 4.783132553100586,
      "learning_rate": 4.446811811081101e-05,
      "loss": 0.697,
      "step": 727500
    },
    {
      "epoch": 6.639170742389956,
      "grad_norm": 3.7274701595306396,
      "learning_rate": 4.446735771467504e-05,
      "loss": 0.72,
      "step": 727600
    },
    {
      "epoch": 6.640083217753121,
      "grad_norm": 3.9102237224578857,
      "learning_rate": 4.446659731853906e-05,
      "loss": 0.7075,
      "step": 727700
    },
    {
      "epoch": 6.640995693116286,
      "grad_norm": 3.717060089111328,
      "learning_rate": 4.44658369224031e-05,
      "loss": 0.6778,
      "step": 727800
    },
    {
      "epoch": 6.641908168479451,
      "grad_norm": 3.9246695041656494,
      "learning_rate": 4.446507652626712e-05,
      "loss": 0.7208,
      "step": 727900
    },
    {
      "epoch": 6.642820643842616,
      "grad_norm": 4.547123432159424,
      "learning_rate": 4.446431613013115e-05,
      "loss": 0.7089,
      "step": 728000
    },
    {
      "epoch": 6.643733119205781,
      "grad_norm": 4.323455810546875,
      "learning_rate": 4.446355573399518e-05,
      "loss": 0.7117,
      "step": 728100
    },
    {
      "epoch": 6.644645594568947,
      "grad_norm": 4.26030969619751,
      "learning_rate": 4.446279533785921e-05,
      "loss": 0.7239,
      "step": 728200
    },
    {
      "epoch": 6.645558069932112,
      "grad_norm": 3.388444662094116,
      "learning_rate": 4.4462034941723243e-05,
      "loss": 0.6886,
      "step": 728300
    },
    {
      "epoch": 6.646470545295277,
      "grad_norm": 2.849684715270996,
      "learning_rate": 4.4461274545587274e-05,
      "loss": 0.7056,
      "step": 728400
    },
    {
      "epoch": 6.6473830206584426,
      "grad_norm": 3.8320064544677734,
      "learning_rate": 4.44605141494513e-05,
      "loss": 0.7179,
      "step": 728500
    },
    {
      "epoch": 6.648295496021607,
      "grad_norm": 4.119924068450928,
      "learning_rate": 4.4459753753315334e-05,
      "loss": 0.6862,
      "step": 728600
    },
    {
      "epoch": 6.649207971384772,
      "grad_norm": 4.311158180236816,
      "learning_rate": 4.445899335717936e-05,
      "loss": 0.6822,
      "step": 728700
    },
    {
      "epoch": 6.650120446747938,
      "grad_norm": 3.67911696434021,
      "learning_rate": 4.445823296104339e-05,
      "loss": 0.6966,
      "step": 728800
    },
    {
      "epoch": 6.651032922111103,
      "grad_norm": 3.606954336166382,
      "learning_rate": 4.445747256490742e-05,
      "loss": 0.7008,
      "step": 728900
    },
    {
      "epoch": 6.651945397474268,
      "grad_norm": 3.9760751724243164,
      "learning_rate": 4.445671216877145e-05,
      "loss": 0.692,
      "step": 729000
    },
    {
      "epoch": 6.6528578728374335,
      "grad_norm": 3.965667486190796,
      "learning_rate": 4.445595177263547e-05,
      "loss": 0.7124,
      "step": 729100
    },
    {
      "epoch": 6.653770348200599,
      "grad_norm": 4.463765621185303,
      "learning_rate": 4.445519137649951e-05,
      "loss": 0.7418,
      "step": 729200
    },
    {
      "epoch": 6.654682823563764,
      "grad_norm": 4.05556058883667,
      "learning_rate": 4.445443098036353e-05,
      "loss": 0.7017,
      "step": 729300
    },
    {
      "epoch": 6.655595298926929,
      "grad_norm": 4.598114013671875,
      "learning_rate": 4.445367058422756e-05,
      "loss": 0.6863,
      "step": 729400
    },
    {
      "epoch": 6.656507774290095,
      "grad_norm": 4.410822868347168,
      "learning_rate": 4.445291018809159e-05,
      "loss": 0.7074,
      "step": 729500
    },
    {
      "epoch": 6.657420249653259,
      "grad_norm": 5.363226413726807,
      "learning_rate": 4.445214979195562e-05,
      "loss": 0.6954,
      "step": 729600
    },
    {
      "epoch": 6.658332725016424,
      "grad_norm": 4.885540962219238,
      "learning_rate": 4.445138939581965e-05,
      "loss": 0.7114,
      "step": 729700
    },
    {
      "epoch": 6.65924520037959,
      "grad_norm": 4.296926021575928,
      "learning_rate": 4.445062899968368e-05,
      "loss": 0.6611,
      "step": 729800
    },
    {
      "epoch": 6.660157675742755,
      "grad_norm": 4.178254127502441,
      "learning_rate": 4.4449868603547704e-05,
      "loss": 0.6712,
      "step": 729900
    },
    {
      "epoch": 6.66107015110592,
      "grad_norm": 4.052856922149658,
      "learning_rate": 4.4449108207411734e-05,
      "loss": 0.7166,
      "step": 730000
    },
    {
      "epoch": 6.661982626469086,
      "grad_norm": 3.3113608360290527,
      "learning_rate": 4.4448347811275764e-05,
      "loss": 0.7077,
      "step": 730100
    },
    {
      "epoch": 6.662895101832251,
      "grad_norm": 4.300899028778076,
      "learning_rate": 4.444758741513979e-05,
      "loss": 0.6837,
      "step": 730200
    },
    {
      "epoch": 6.663807577195415,
      "grad_norm": 3.702941417694092,
      "learning_rate": 4.4446827019003824e-05,
      "loss": 0.7057,
      "step": 730300
    },
    {
      "epoch": 6.664720052558581,
      "grad_norm": 4.549328327178955,
      "learning_rate": 4.444606662286785e-05,
      "loss": 0.7412,
      "step": 730400
    },
    {
      "epoch": 6.665632527921746,
      "grad_norm": 4.521500110626221,
      "learning_rate": 4.444530622673188e-05,
      "loss": 0.6995,
      "step": 730500
    },
    {
      "epoch": 6.666545003284911,
      "grad_norm": 3.9318525791168213,
      "learning_rate": 4.444454583059591e-05,
      "loss": 0.7104,
      "step": 730600
    },
    {
      "epoch": 6.6674574786480765,
      "grad_norm": 3.5674266815185547,
      "learning_rate": 4.444378543445994e-05,
      "loss": 0.7052,
      "step": 730700
    },
    {
      "epoch": 6.668369954011242,
      "grad_norm": 4.281991004943848,
      "learning_rate": 4.444302503832397e-05,
      "loss": 0.6994,
      "step": 730800
    },
    {
      "epoch": 6.669282429374407,
      "grad_norm": 4.844859600067139,
      "learning_rate": 4.4442264642188e-05,
      "loss": 0.7306,
      "step": 730900
    },
    {
      "epoch": 6.670194904737572,
      "grad_norm": 4.929707050323486,
      "learning_rate": 4.444150424605202e-05,
      "loss": 0.7685,
      "step": 731000
    },
    {
      "epoch": 6.671107380100738,
      "grad_norm": 4.007150173187256,
      "learning_rate": 4.444074384991606e-05,
      "loss": 0.6742,
      "step": 731100
    },
    {
      "epoch": 6.672019855463903,
      "grad_norm": 4.452146053314209,
      "learning_rate": 4.443998345378008e-05,
      "loss": 0.716,
      "step": 731200
    },
    {
      "epoch": 6.672932330827067,
      "grad_norm": 4.951437950134277,
      "learning_rate": 4.443922305764411e-05,
      "loss": 0.7149,
      "step": 731300
    },
    {
      "epoch": 6.673844806190233,
      "grad_norm": 4.029758453369141,
      "learning_rate": 4.443846266150814e-05,
      "loss": 0.7125,
      "step": 731400
    },
    {
      "epoch": 6.674757281553398,
      "grad_norm": 4.557026386260986,
      "learning_rate": 4.443770226537217e-05,
      "loss": 0.722,
      "step": 731500
    },
    {
      "epoch": 6.675669756916563,
      "grad_norm": 3.565103769302368,
      "learning_rate": 4.4436941869236195e-05,
      "loss": 0.7163,
      "step": 731600
    },
    {
      "epoch": 6.676582232279729,
      "grad_norm": 4.099513530731201,
      "learning_rate": 4.443618147310023e-05,
      "loss": 0.7119,
      "step": 731700
    },
    {
      "epoch": 6.677494707642894,
      "grad_norm": 4.46987247467041,
      "learning_rate": 4.4435421076964255e-05,
      "loss": 0.7034,
      "step": 731800
    },
    {
      "epoch": 6.678407183006059,
      "grad_norm": 3.5350115299224854,
      "learning_rate": 4.4434660680828285e-05,
      "loss": 0.7076,
      "step": 731900
    },
    {
      "epoch": 6.679319658369224,
      "grad_norm": 4.960839748382568,
      "learning_rate": 4.4433900284692315e-05,
      "loss": 0.6996,
      "step": 732000
    },
    {
      "epoch": 6.680232133732389,
      "grad_norm": 5.0737223625183105,
      "learning_rate": 4.4433139888556345e-05,
      "loss": 0.7224,
      "step": 732100
    },
    {
      "epoch": 6.681144609095554,
      "grad_norm": 3.8274142742156982,
      "learning_rate": 4.4432379492420375e-05,
      "loss": 0.7033,
      "step": 732200
    },
    {
      "epoch": 6.6820570844587195,
      "grad_norm": 4.395977973937988,
      "learning_rate": 4.4431619096284405e-05,
      "loss": 0.7184,
      "step": 732300
    },
    {
      "epoch": 6.682969559821885,
      "grad_norm": 4.50880765914917,
      "learning_rate": 4.443085870014843e-05,
      "loss": 0.7448,
      "step": 732400
    },
    {
      "epoch": 6.68388203518505,
      "grad_norm": 3.545950174331665,
      "learning_rate": 4.4430098304012466e-05,
      "loss": 0.7032,
      "step": 732500
    },
    {
      "epoch": 6.684794510548215,
      "grad_norm": 3.408844232559204,
      "learning_rate": 4.442933790787649e-05,
      "loss": 0.6682,
      "step": 732600
    },
    {
      "epoch": 6.685706985911381,
      "grad_norm": 2.757239818572998,
      "learning_rate": 4.442857751174052e-05,
      "loss": 0.7152,
      "step": 732700
    },
    {
      "epoch": 6.686619461274546,
      "grad_norm": 3.240858554840088,
      "learning_rate": 4.442781711560455e-05,
      "loss": 0.7142,
      "step": 732800
    },
    {
      "epoch": 6.68753193663771,
      "grad_norm": 4.134974956512451,
      "learning_rate": 4.442705671946857e-05,
      "loss": 0.6951,
      "step": 732900
    },
    {
      "epoch": 6.688444412000876,
      "grad_norm": 4.187920570373535,
      "learning_rate": 4.44262963233326e-05,
      "loss": 0.717,
      "step": 733000
    },
    {
      "epoch": 6.689356887364041,
      "grad_norm": 3.844872236251831,
      "learning_rate": 4.442553592719663e-05,
      "loss": 0.7157,
      "step": 733100
    },
    {
      "epoch": 6.690269362727206,
      "grad_norm": 3.3457143306732178,
      "learning_rate": 4.442477553106066e-05,
      "loss": 0.7016,
      "step": 733200
    },
    {
      "epoch": 6.691181838090372,
      "grad_norm": 3.241088628768921,
      "learning_rate": 4.442401513492469e-05,
      "loss": 0.6849,
      "step": 733300
    },
    {
      "epoch": 6.692094313453537,
      "grad_norm": 4.951421737670898,
      "learning_rate": 4.442325473878872e-05,
      "loss": 0.7064,
      "step": 733400
    },
    {
      "epoch": 6.693006788816702,
      "grad_norm": 4.406996250152588,
      "learning_rate": 4.4422494342652746e-05,
      "loss": 0.723,
      "step": 733500
    },
    {
      "epoch": 6.6939192641798675,
      "grad_norm": 4.3170671463012695,
      "learning_rate": 4.442173394651678e-05,
      "loss": 0.7079,
      "step": 733600
    },
    {
      "epoch": 6.694831739543032,
      "grad_norm": 4.31807804107666,
      "learning_rate": 4.4420973550380806e-05,
      "loss": 0.733,
      "step": 733700
    },
    {
      "epoch": 6.695744214906197,
      "grad_norm": 5.191983222961426,
      "learning_rate": 4.4420213154244836e-05,
      "loss": 0.7367,
      "step": 733800
    },
    {
      "epoch": 6.6966566902693625,
      "grad_norm": 3.8525192737579346,
      "learning_rate": 4.4419452758108866e-05,
      "loss": 0.6804,
      "step": 733900
    },
    {
      "epoch": 6.697569165632528,
      "grad_norm": 5.206203937530518,
      "learning_rate": 4.4418692361972896e-05,
      "loss": 0.6843,
      "step": 734000
    },
    {
      "epoch": 6.698481640995693,
      "grad_norm": 3.888792037963867,
      "learning_rate": 4.4417931965836926e-05,
      "loss": 0.7583,
      "step": 734100
    },
    {
      "epoch": 6.699394116358858,
      "grad_norm": 3.7809338569641113,
      "learning_rate": 4.4417171569700956e-05,
      "loss": 0.6793,
      "step": 734200
    },
    {
      "epoch": 6.700306591722024,
      "grad_norm": 3.568694591522217,
      "learning_rate": 4.441641117356498e-05,
      "loss": 0.7135,
      "step": 734300
    },
    {
      "epoch": 6.701219067085189,
      "grad_norm": 3.4245831966400146,
      "learning_rate": 4.441565077742901e-05,
      "loss": 0.6949,
      "step": 734400
    },
    {
      "epoch": 6.702131542448354,
      "grad_norm": 4.643950939178467,
      "learning_rate": 4.441489038129304e-05,
      "loss": 0.6852,
      "step": 734500
    },
    {
      "epoch": 6.703044017811519,
      "grad_norm": 3.9851582050323486,
      "learning_rate": 4.441412998515707e-05,
      "loss": 0.7024,
      "step": 734600
    },
    {
      "epoch": 6.703956493174684,
      "grad_norm": 3.768235921859741,
      "learning_rate": 4.44133695890211e-05,
      "loss": 0.6959,
      "step": 734700
    },
    {
      "epoch": 6.704868968537849,
      "grad_norm": 4.844738006591797,
      "learning_rate": 4.441260919288513e-05,
      "loss": 0.7478,
      "step": 734800
    },
    {
      "epoch": 6.705781443901015,
      "grad_norm": 3.673473358154297,
      "learning_rate": 4.441184879674915e-05,
      "loss": 0.6951,
      "step": 734900
    },
    {
      "epoch": 6.70669391926418,
      "grad_norm": 3.694164514541626,
      "learning_rate": 4.441108840061319e-05,
      "loss": 0.6805,
      "step": 735000
    },
    {
      "epoch": 6.707606394627345,
      "grad_norm": 3.849614381790161,
      "learning_rate": 4.4410328004477213e-05,
      "loss": 0.691,
      "step": 735100
    },
    {
      "epoch": 6.7085188699905105,
      "grad_norm": 3.9635112285614014,
      "learning_rate": 4.4409567608341244e-05,
      "loss": 0.7028,
      "step": 735200
    },
    {
      "epoch": 6.709431345353676,
      "grad_norm": 4.933084964752197,
      "learning_rate": 4.4408807212205274e-05,
      "loss": 0.6905,
      "step": 735300
    },
    {
      "epoch": 6.71034382071684,
      "grad_norm": 4.8230133056640625,
      "learning_rate": 4.4408046816069304e-05,
      "loss": 0.6962,
      "step": 735400
    },
    {
      "epoch": 6.7112562960800055,
      "grad_norm": 3.792449951171875,
      "learning_rate": 4.4407286419933334e-05,
      "loss": 0.7109,
      "step": 735500
    },
    {
      "epoch": 6.712168771443171,
      "grad_norm": 3.2769789695739746,
      "learning_rate": 4.440652602379736e-05,
      "loss": 0.6595,
      "step": 735600
    },
    {
      "epoch": 6.713081246806336,
      "grad_norm": 2.821986436843872,
      "learning_rate": 4.440576562766139e-05,
      "loss": 0.7142,
      "step": 735700
    },
    {
      "epoch": 6.713993722169501,
      "grad_norm": 3.366753578186035,
      "learning_rate": 4.440500523152542e-05,
      "loss": 0.7018,
      "step": 735800
    },
    {
      "epoch": 6.714906197532667,
      "grad_norm": 2.909071445465088,
      "learning_rate": 4.440424483538945e-05,
      "loss": 0.7084,
      "step": 735900
    },
    {
      "epoch": 6.715818672895832,
      "grad_norm": 4.925312519073486,
      "learning_rate": 4.440348443925347e-05,
      "loss": 0.6846,
      "step": 736000
    },
    {
      "epoch": 6.716731148258997,
      "grad_norm": 4.782595157623291,
      "learning_rate": 4.440272404311751e-05,
      "loss": 0.7569,
      "step": 736100
    },
    {
      "epoch": 6.717643623622163,
      "grad_norm": 4.492761135101318,
      "learning_rate": 4.440196364698153e-05,
      "loss": 0.7226,
      "step": 736200
    },
    {
      "epoch": 6.718556098985327,
      "grad_norm": 3.571323871612549,
      "learning_rate": 4.440120325084556e-05,
      "loss": 0.7132,
      "step": 736300
    },
    {
      "epoch": 6.719468574348492,
      "grad_norm": 3.4130306243896484,
      "learning_rate": 4.440044285470959e-05,
      "loss": 0.693,
      "step": 736400
    },
    {
      "epoch": 6.720381049711658,
      "grad_norm": 4.321424961090088,
      "learning_rate": 4.439968245857362e-05,
      "loss": 0.7217,
      "step": 736500
    },
    {
      "epoch": 6.721293525074823,
      "grad_norm": 4.386263370513916,
      "learning_rate": 4.439892206243765e-05,
      "loss": 0.725,
      "step": 736600
    },
    {
      "epoch": 6.722206000437988,
      "grad_norm": 4.727959156036377,
      "learning_rate": 4.439816166630168e-05,
      "loss": 0.6968,
      "step": 736700
    },
    {
      "epoch": 6.7231184758011535,
      "grad_norm": 3.237840414047241,
      "learning_rate": 4.4397401270165704e-05,
      "loss": 0.7044,
      "step": 736800
    },
    {
      "epoch": 6.724030951164319,
      "grad_norm": 4.079392910003662,
      "learning_rate": 4.439664087402974e-05,
      "loss": 0.7108,
      "step": 736900
    },
    {
      "epoch": 6.724943426527484,
      "grad_norm": 4.3344292640686035,
      "learning_rate": 4.4395880477893764e-05,
      "loss": 0.7084,
      "step": 737000
    },
    {
      "epoch": 6.7258559018906485,
      "grad_norm": 3.78974986076355,
      "learning_rate": 4.4395120081757794e-05,
      "loss": 0.7008,
      "step": 737100
    },
    {
      "epoch": 6.726768377253814,
      "grad_norm": 4.154512405395508,
      "learning_rate": 4.4394359685621825e-05,
      "loss": 0.7144,
      "step": 737200
    },
    {
      "epoch": 6.727680852616979,
      "grad_norm": 4.124074459075928,
      "learning_rate": 4.4393599289485855e-05,
      "loss": 0.6841,
      "step": 737300
    },
    {
      "epoch": 6.728593327980144,
      "grad_norm": 2.7904844284057617,
      "learning_rate": 4.439283889334988e-05,
      "loss": 0.6844,
      "step": 737400
    },
    {
      "epoch": 6.72950580334331,
      "grad_norm": 2.537768602371216,
      "learning_rate": 4.4392078497213915e-05,
      "loss": 0.7446,
      "step": 737500
    },
    {
      "epoch": 6.730418278706475,
      "grad_norm": 4.032204627990723,
      "learning_rate": 4.439131810107794e-05,
      "loss": 0.6534,
      "step": 737600
    },
    {
      "epoch": 6.73133075406964,
      "grad_norm": 2.9418997764587402,
      "learning_rate": 4.439055770494197e-05,
      "loss": 0.7103,
      "step": 737700
    },
    {
      "epoch": 6.732243229432806,
      "grad_norm": 3.139946937561035,
      "learning_rate": 4.4389797308806e-05,
      "loss": 0.6818,
      "step": 737800
    },
    {
      "epoch": 6.733155704795971,
      "grad_norm": 4.624974250793457,
      "learning_rate": 4.438903691267003e-05,
      "loss": 0.7254,
      "step": 737900
    },
    {
      "epoch": 6.734068180159135,
      "grad_norm": 4.481121063232422,
      "learning_rate": 4.438827651653406e-05,
      "loss": 0.7268,
      "step": 738000
    },
    {
      "epoch": 6.734980655522301,
      "grad_norm": 2.023796319961548,
      "learning_rate": 4.438751612039809e-05,
      "loss": 0.7038,
      "step": 738100
    },
    {
      "epoch": 6.735893130885466,
      "grad_norm": 3.899416446685791,
      "learning_rate": 4.438675572426211e-05,
      "loss": 0.677,
      "step": 738200
    },
    {
      "epoch": 6.736805606248631,
      "grad_norm": 4.331249713897705,
      "learning_rate": 4.438599532812615e-05,
      "loss": 0.7047,
      "step": 738300
    },
    {
      "epoch": 6.7377180816117965,
      "grad_norm": 3.664107084274292,
      "learning_rate": 4.438523493199017e-05,
      "loss": 0.7249,
      "step": 738400
    },
    {
      "epoch": 6.738630556974962,
      "grad_norm": 3.2604713439941406,
      "learning_rate": 4.4384474535854195e-05,
      "loss": 0.6893,
      "step": 738500
    },
    {
      "epoch": 6.739543032338127,
      "grad_norm": 8.751185417175293,
      "learning_rate": 4.438371413971823e-05,
      "loss": 0.6592,
      "step": 738600
    },
    {
      "epoch": 6.740455507701292,
      "grad_norm": 3.802650213241577,
      "learning_rate": 4.4382953743582255e-05,
      "loss": 0.7226,
      "step": 738700
    },
    {
      "epoch": 6.741367983064457,
      "grad_norm": 3.954979181289673,
      "learning_rate": 4.4382193347446285e-05,
      "loss": 0.6887,
      "step": 738800
    },
    {
      "epoch": 6.742280458427622,
      "grad_norm": 3.3518459796905518,
      "learning_rate": 4.4381432951310315e-05,
      "loss": 0.7344,
      "step": 738900
    },
    {
      "epoch": 6.743192933790787,
      "grad_norm": 3.5131630897521973,
      "learning_rate": 4.4380672555174345e-05,
      "loss": 0.7566,
      "step": 739000
    },
    {
      "epoch": 6.744105409153953,
      "grad_norm": 3.6592442989349365,
      "learning_rate": 4.4379912159038375e-05,
      "loss": 0.7427,
      "step": 739100
    },
    {
      "epoch": 6.745017884517118,
      "grad_norm": 3.933202028274536,
      "learning_rate": 4.4379151762902406e-05,
      "loss": 0.7144,
      "step": 739200
    },
    {
      "epoch": 6.745930359880283,
      "grad_norm": 3.6201155185699463,
      "learning_rate": 4.437839136676643e-05,
      "loss": 0.7313,
      "step": 739300
    },
    {
      "epoch": 6.746842835243449,
      "grad_norm": 3.941542863845825,
      "learning_rate": 4.4377630970630466e-05,
      "loss": 0.7019,
      "step": 739400
    },
    {
      "epoch": 6.747755310606614,
      "grad_norm": 4.073419570922852,
      "learning_rate": 4.437687057449449e-05,
      "loss": 0.7359,
      "step": 739500
    },
    {
      "epoch": 6.748667785969779,
      "grad_norm": 3.25899338722229,
      "learning_rate": 4.437611017835852e-05,
      "loss": 0.6636,
      "step": 739600
    },
    {
      "epoch": 6.749580261332944,
      "grad_norm": 4.288671970367432,
      "learning_rate": 4.437534978222255e-05,
      "loss": 0.7113,
      "step": 739700
    },
    {
      "epoch": 6.750492736696109,
      "grad_norm": 3.4881269931793213,
      "learning_rate": 4.437458938608658e-05,
      "loss": 0.6652,
      "step": 739800
    },
    {
      "epoch": 6.751405212059274,
      "grad_norm": 4.178130626678467,
      "learning_rate": 4.43738289899506e-05,
      "loss": 0.7201,
      "step": 739900
    },
    {
      "epoch": 6.7523176874224395,
      "grad_norm": 3.3824265003204346,
      "learning_rate": 4.437306859381464e-05,
      "loss": 0.68,
      "step": 740000
    },
    {
      "epoch": 6.753230162785605,
      "grad_norm": 3.495905876159668,
      "learning_rate": 4.437230819767866e-05,
      "loss": 0.6993,
      "step": 740100
    },
    {
      "epoch": 6.75414263814877,
      "grad_norm": 3.7055509090423584,
      "learning_rate": 4.437154780154269e-05,
      "loss": 0.716,
      "step": 740200
    },
    {
      "epoch": 6.755055113511935,
      "grad_norm": 3.7263777256011963,
      "learning_rate": 4.437078740540672e-05,
      "loss": 0.7196,
      "step": 740300
    },
    {
      "epoch": 6.755967588875101,
      "grad_norm": 2.9136881828308105,
      "learning_rate": 4.437002700927075e-05,
      "loss": 0.7284,
      "step": 740400
    },
    {
      "epoch": 6.756880064238265,
      "grad_norm": 4.410844802856445,
      "learning_rate": 4.436926661313478e-05,
      "loss": 0.7341,
      "step": 740500
    },
    {
      "epoch": 6.7577925396014304,
      "grad_norm": 3.6575000286102295,
      "learning_rate": 4.436850621699881e-05,
      "loss": 0.6886,
      "step": 740600
    },
    {
      "epoch": 6.758705014964596,
      "grad_norm": 4.043463230133057,
      "learning_rate": 4.4367745820862836e-05,
      "loss": 0.6907,
      "step": 740700
    },
    {
      "epoch": 6.759617490327761,
      "grad_norm": 3.5906784534454346,
      "learning_rate": 4.436698542472687e-05,
      "loss": 0.6934,
      "step": 740800
    },
    {
      "epoch": 6.760529965690926,
      "grad_norm": 4.209700584411621,
      "learning_rate": 4.4366225028590896e-05,
      "loss": 0.6801,
      "step": 740900
    },
    {
      "epoch": 6.761442441054092,
      "grad_norm": 4.521480560302734,
      "learning_rate": 4.4365464632454926e-05,
      "loss": 0.7013,
      "step": 741000
    },
    {
      "epoch": 6.762354916417257,
      "grad_norm": 3.582505702972412,
      "learning_rate": 4.4364704236318957e-05,
      "loss": 0.7151,
      "step": 741100
    },
    {
      "epoch": 6.763267391780422,
      "grad_norm": 3.7679667472839355,
      "learning_rate": 4.4363943840182987e-05,
      "loss": 0.6926,
      "step": 741200
    },
    {
      "epoch": 6.7641798671435875,
      "grad_norm": 4.328427314758301,
      "learning_rate": 4.436318344404701e-05,
      "loss": 0.6969,
      "step": 741300
    },
    {
      "epoch": 6.765092342506752,
      "grad_norm": 3.562167167663574,
      "learning_rate": 4.436242304791104e-05,
      "loss": 0.6786,
      "step": 741400
    },
    {
      "epoch": 6.766004817869917,
      "grad_norm": 4.23756217956543,
      "learning_rate": 4.436166265177507e-05,
      "loss": 0.7043,
      "step": 741500
    },
    {
      "epoch": 6.7669172932330826,
      "grad_norm": 4.952280044555664,
      "learning_rate": 4.43609022556391e-05,
      "loss": 0.725,
      "step": 741600
    },
    {
      "epoch": 6.767829768596248,
      "grad_norm": 3.479663372039795,
      "learning_rate": 4.436014185950313e-05,
      "loss": 0.7203,
      "step": 741700
    },
    {
      "epoch": 6.768742243959413,
      "grad_norm": 4.160818099975586,
      "learning_rate": 4.4359381463367153e-05,
      "loss": 0.7136,
      "step": 741800
    },
    {
      "epoch": 6.7696547193225785,
      "grad_norm": 3.4889862537384033,
      "learning_rate": 4.435862106723119e-05,
      "loss": 0.751,
      "step": 741900
    },
    {
      "epoch": 6.770567194685744,
      "grad_norm": 3.7035558223724365,
      "learning_rate": 4.4357860671095214e-05,
      "loss": 0.6789,
      "step": 742000
    },
    {
      "epoch": 6.771479670048909,
      "grad_norm": 4.617015838623047,
      "learning_rate": 4.4357100274959244e-05,
      "loss": 0.724,
      "step": 742100
    },
    {
      "epoch": 6.7723921454120735,
      "grad_norm": 3.716423749923706,
      "learning_rate": 4.4356339878823274e-05,
      "loss": 0.7051,
      "step": 742200
    },
    {
      "epoch": 6.773304620775239,
      "grad_norm": 3.5198276042938232,
      "learning_rate": 4.4355579482687304e-05,
      "loss": 0.6773,
      "step": 742300
    },
    {
      "epoch": 6.774217096138404,
      "grad_norm": 3.8777401447296143,
      "learning_rate": 4.435481908655133e-05,
      "loss": 0.6665,
      "step": 742400
    },
    {
      "epoch": 6.775129571501569,
      "grad_norm": 4.546194553375244,
      "learning_rate": 4.4354058690415364e-05,
      "loss": 0.7275,
      "step": 742500
    },
    {
      "epoch": 6.776042046864735,
      "grad_norm": 4.278651714324951,
      "learning_rate": 4.435329829427939e-05,
      "loss": 0.7199,
      "step": 742600
    },
    {
      "epoch": 6.7769545222279,
      "grad_norm": 3.3725273609161377,
      "learning_rate": 4.435253789814342e-05,
      "loss": 0.6777,
      "step": 742700
    },
    {
      "epoch": 6.777866997591065,
      "grad_norm": 5.338140964508057,
      "learning_rate": 4.435177750200745e-05,
      "loss": 0.7,
      "step": 742800
    },
    {
      "epoch": 6.778779472954231,
      "grad_norm": 2.9157395362854004,
      "learning_rate": 4.435101710587148e-05,
      "loss": 0.7019,
      "step": 742900
    },
    {
      "epoch": 6.779691948317396,
      "grad_norm": 4.306320667266846,
      "learning_rate": 4.435025670973551e-05,
      "loss": 0.7245,
      "step": 743000
    },
    {
      "epoch": 6.78060442368056,
      "grad_norm": 2.9928512573242188,
      "learning_rate": 4.434949631359954e-05,
      "loss": 0.7015,
      "step": 743100
    },
    {
      "epoch": 6.781516899043726,
      "grad_norm": 5.256629943847656,
      "learning_rate": 4.434873591746356e-05,
      "loss": 0.7069,
      "step": 743200
    },
    {
      "epoch": 6.782429374406891,
      "grad_norm": 2.70690655708313,
      "learning_rate": 4.43479755213276e-05,
      "loss": 0.7088,
      "step": 743300
    },
    {
      "epoch": 6.783341849770056,
      "grad_norm": 3.591491460800171,
      "learning_rate": 4.434721512519162e-05,
      "loss": 0.7207,
      "step": 743400
    },
    {
      "epoch": 6.7842543251332215,
      "grad_norm": 3.458916664123535,
      "learning_rate": 4.434645472905565e-05,
      "loss": 0.6658,
      "step": 743500
    },
    {
      "epoch": 6.785166800496387,
      "grad_norm": 4.132885456085205,
      "learning_rate": 4.434569433291968e-05,
      "loss": 0.6965,
      "step": 743600
    },
    {
      "epoch": 6.786079275859552,
      "grad_norm": 4.322997093200684,
      "learning_rate": 4.434493393678371e-05,
      "loss": 0.6749,
      "step": 743700
    },
    {
      "epoch": 6.786991751222717,
      "grad_norm": 3.2161431312561035,
      "learning_rate": 4.4344173540647734e-05,
      "loss": 0.7136,
      "step": 743800
    },
    {
      "epoch": 6.787904226585882,
      "grad_norm": 3.7335684299468994,
      "learning_rate": 4.434341314451177e-05,
      "loss": 0.6924,
      "step": 743900
    },
    {
      "epoch": 6.788816701949047,
      "grad_norm": 4.320126056671143,
      "learning_rate": 4.4342652748375795e-05,
      "loss": 0.7302,
      "step": 744000
    },
    {
      "epoch": 6.789729177312212,
      "grad_norm": 4.0258684158325195,
      "learning_rate": 4.4341892352239825e-05,
      "loss": 0.6907,
      "step": 744100
    },
    {
      "epoch": 6.790641652675378,
      "grad_norm": 4.614102840423584,
      "learning_rate": 4.4341131956103855e-05,
      "loss": 0.7091,
      "step": 744200
    },
    {
      "epoch": 6.791554128038543,
      "grad_norm": 4.107690811157227,
      "learning_rate": 4.434037155996788e-05,
      "loss": 0.7251,
      "step": 744300
    },
    {
      "epoch": 6.792466603401708,
      "grad_norm": 3.7915849685668945,
      "learning_rate": 4.4339611163831915e-05,
      "loss": 0.7188,
      "step": 744400
    },
    {
      "epoch": 6.793379078764874,
      "grad_norm": 4.2940874099731445,
      "learning_rate": 4.433885076769594e-05,
      "loss": 0.712,
      "step": 744500
    },
    {
      "epoch": 6.794291554128039,
      "grad_norm": 4.715897560119629,
      "learning_rate": 4.433809037155997e-05,
      "loss": 0.7128,
      "step": 744600
    },
    {
      "epoch": 6.795204029491204,
      "grad_norm": 4.6047043800354,
      "learning_rate": 4.4337329975424e-05,
      "loss": 0.724,
      "step": 744700
    },
    {
      "epoch": 6.796116504854369,
      "grad_norm": 4.7720561027526855,
      "learning_rate": 4.433656957928803e-05,
      "loss": 0.6875,
      "step": 744800
    },
    {
      "epoch": 6.797028980217534,
      "grad_norm": 4.444003582000732,
      "learning_rate": 4.433580918315205e-05,
      "loss": 0.7274,
      "step": 744900
    },
    {
      "epoch": 6.797941455580699,
      "grad_norm": 5.09407901763916,
      "learning_rate": 4.433504878701609e-05,
      "loss": 0.6792,
      "step": 745000
    },
    {
      "epoch": 6.7988539309438645,
      "grad_norm": 3.5822579860687256,
      "learning_rate": 4.433428839088011e-05,
      "loss": 0.704,
      "step": 745100
    },
    {
      "epoch": 6.79976640630703,
      "grad_norm": 4.202481269836426,
      "learning_rate": 4.433352799474414e-05,
      "loss": 0.7614,
      "step": 745200
    },
    {
      "epoch": 6.800678881670195,
      "grad_norm": 4.1388115882873535,
      "learning_rate": 4.433276759860817e-05,
      "loss": 0.7325,
      "step": 745300
    },
    {
      "epoch": 6.80159135703336,
      "grad_norm": 3.911405324935913,
      "learning_rate": 4.43320072024722e-05,
      "loss": 0.6872,
      "step": 745400
    },
    {
      "epoch": 6.802503832396526,
      "grad_norm": 3.6420388221740723,
      "learning_rate": 4.433124680633623e-05,
      "loss": 0.6762,
      "step": 745500
    },
    {
      "epoch": 6.80341630775969,
      "grad_norm": 4.0267486572265625,
      "learning_rate": 4.433048641020026e-05,
      "loss": 0.6696,
      "step": 745600
    },
    {
      "epoch": 6.804328783122855,
      "grad_norm": 4.247797966003418,
      "learning_rate": 4.4329726014064285e-05,
      "loss": 0.6745,
      "step": 745700
    },
    {
      "epoch": 6.805241258486021,
      "grad_norm": 3.6337947845458984,
      "learning_rate": 4.432896561792832e-05,
      "loss": 0.6908,
      "step": 745800
    },
    {
      "epoch": 6.806153733849186,
      "grad_norm": 3.6098008155822754,
      "learning_rate": 4.4328205221792346e-05,
      "loss": 0.6907,
      "step": 745900
    },
    {
      "epoch": 6.807066209212351,
      "grad_norm": 3.7660958766937256,
      "learning_rate": 4.4327444825656376e-05,
      "loss": 0.7001,
      "step": 746000
    },
    {
      "epoch": 6.807978684575517,
      "grad_norm": 4.2920026779174805,
      "learning_rate": 4.4326684429520406e-05,
      "loss": 0.6911,
      "step": 746100
    },
    {
      "epoch": 6.808891159938682,
      "grad_norm": 4.674478054046631,
      "learning_rate": 4.4325924033384436e-05,
      "loss": 0.7047,
      "step": 746200
    },
    {
      "epoch": 6.809803635301847,
      "grad_norm": 4.140860557556152,
      "learning_rate": 4.4325163637248466e-05,
      "loss": 0.7057,
      "step": 746300
    },
    {
      "epoch": 6.8107161106650125,
      "grad_norm": 4.691830158233643,
      "learning_rate": 4.4324403241112496e-05,
      "loss": 0.7057,
      "step": 746400
    },
    {
      "epoch": 6.811628586028177,
      "grad_norm": 5.511941909790039,
      "learning_rate": 4.432364284497652e-05,
      "loss": 0.6973,
      "step": 746500
    },
    {
      "epoch": 6.812541061391342,
      "grad_norm": 4.710936069488525,
      "learning_rate": 4.432288244884055e-05,
      "loss": 0.704,
      "step": 746600
    },
    {
      "epoch": 6.8134535367545075,
      "grad_norm": 3.523682117462158,
      "learning_rate": 4.432212205270458e-05,
      "loss": 0.6781,
      "step": 746700
    },
    {
      "epoch": 6.814366012117673,
      "grad_norm": 5.048026084899902,
      "learning_rate": 4.432136165656861e-05,
      "loss": 0.6829,
      "step": 746800
    },
    {
      "epoch": 6.815278487480838,
      "grad_norm": 3.5604755878448486,
      "learning_rate": 4.432060126043264e-05,
      "loss": 0.7417,
      "step": 746900
    },
    {
      "epoch": 6.816190962844003,
      "grad_norm": 3.810058116912842,
      "learning_rate": 4.431984086429666e-05,
      "loss": 0.7213,
      "step": 747000
    },
    {
      "epoch": 6.817103438207169,
      "grad_norm": 3.9886584281921387,
      "learning_rate": 4.431908046816069e-05,
      "loss": 0.7353,
      "step": 747100
    },
    {
      "epoch": 6.818015913570334,
      "grad_norm": 3.7051286697387695,
      "learning_rate": 4.431832007202472e-05,
      "loss": 0.7044,
      "step": 747200
    },
    {
      "epoch": 6.818928388933498,
      "grad_norm": 3.8836965560913086,
      "learning_rate": 4.431755967588875e-05,
      "loss": 0.6809,
      "step": 747300
    },
    {
      "epoch": 6.819840864296664,
      "grad_norm": 4.175586223602295,
      "learning_rate": 4.431679927975278e-05,
      "loss": 0.7109,
      "step": 747400
    },
    {
      "epoch": 6.820753339659829,
      "grad_norm": 4.588982105255127,
      "learning_rate": 4.431603888361681e-05,
      "loss": 0.6596,
      "step": 747500
    },
    {
      "epoch": 6.821665815022994,
      "grad_norm": 3.472280979156494,
      "learning_rate": 4.4315278487480836e-05,
      "loss": 0.6802,
      "step": 747600
    },
    {
      "epoch": 6.82257829038616,
      "grad_norm": 7.078397274017334,
      "learning_rate": 4.431451809134487e-05,
      "loss": 0.6743,
      "step": 747700
    },
    {
      "epoch": 6.823490765749325,
      "grad_norm": 4.379862308502197,
      "learning_rate": 4.4313757695208896e-05,
      "loss": 0.6905,
      "step": 747800
    },
    {
      "epoch": 6.82440324111249,
      "grad_norm": 4.136855125427246,
      "learning_rate": 4.4312997299072927e-05,
      "loss": 0.72,
      "step": 747900
    },
    {
      "epoch": 6.8253157164756555,
      "grad_norm": 4.189576625823975,
      "learning_rate": 4.431223690293696e-05,
      "loss": 0.7366,
      "step": 748000
    },
    {
      "epoch": 6.826228191838821,
      "grad_norm": 3.8314642906188965,
      "learning_rate": 4.431147650680099e-05,
      "loss": 0.7101,
      "step": 748100
    },
    {
      "epoch": 6.827140667201985,
      "grad_norm": 3.561613082885742,
      "learning_rate": 4.431071611066501e-05,
      "loss": 0.7504,
      "step": 748200
    },
    {
      "epoch": 6.8280531425651505,
      "grad_norm": 4.125001907348633,
      "learning_rate": 4.430995571452905e-05,
      "loss": 0.7132,
      "step": 748300
    },
    {
      "epoch": 6.828965617928316,
      "grad_norm": 5.287569522857666,
      "learning_rate": 4.430919531839307e-05,
      "loss": 0.7093,
      "step": 748400
    },
    {
      "epoch": 6.829878093291481,
      "grad_norm": 4.988315105438232,
      "learning_rate": 4.43084349222571e-05,
      "loss": 0.7094,
      "step": 748500
    },
    {
      "epoch": 6.830790568654646,
      "grad_norm": 3.007150411605835,
      "learning_rate": 4.430767452612113e-05,
      "loss": 0.7131,
      "step": 748600
    },
    {
      "epoch": 6.831703044017812,
      "grad_norm": 3.850097894668579,
      "learning_rate": 4.430691412998516e-05,
      "loss": 0.6871,
      "step": 748700
    },
    {
      "epoch": 6.832615519380977,
      "grad_norm": 4.1678338050842285,
      "learning_rate": 4.430615373384919e-05,
      "loss": 0.6926,
      "step": 748800
    },
    {
      "epoch": 6.833527994744141,
      "grad_norm": 4.1137309074401855,
      "learning_rate": 4.430539333771322e-05,
      "loss": 0.6836,
      "step": 748900
    },
    {
      "epoch": 6.834440470107307,
      "grad_norm": 3.8785274028778076,
      "learning_rate": 4.4304632941577244e-05,
      "loss": 0.7036,
      "step": 749000
    },
    {
      "epoch": 6.835352945470472,
      "grad_norm": 4.301828861236572,
      "learning_rate": 4.430387254544128e-05,
      "loss": 0.6725,
      "step": 749100
    },
    {
      "epoch": 6.836265420833637,
      "grad_norm": 2.450716733932495,
      "learning_rate": 4.4303112149305304e-05,
      "loss": 0.7305,
      "step": 749200
    },
    {
      "epoch": 6.837177896196803,
      "grad_norm": 4.073756694793701,
      "learning_rate": 4.4302351753169334e-05,
      "loss": 0.6801,
      "step": 749300
    },
    {
      "epoch": 6.838090371559968,
      "grad_norm": 5.553459167480469,
      "learning_rate": 4.4301591357033364e-05,
      "loss": 0.7192,
      "step": 749400
    },
    {
      "epoch": 6.839002846923133,
      "grad_norm": 4.040101051330566,
      "learning_rate": 4.4300830960897394e-05,
      "loss": 0.6849,
      "step": 749500
    },
    {
      "epoch": 6.8399153222862985,
      "grad_norm": 4.0107035636901855,
      "learning_rate": 4.430007056476142e-05,
      "loss": 0.7087,
      "step": 749600
    },
    {
      "epoch": 6.840827797649464,
      "grad_norm": 4.207722187042236,
      "learning_rate": 4.4299310168625454e-05,
      "loss": 0.7144,
      "step": 749700
    },
    {
      "epoch": 6.841740273012629,
      "grad_norm": 4.127974987030029,
      "learning_rate": 4.429854977248948e-05,
      "loss": 0.724,
      "step": 749800
    },
    {
      "epoch": 6.8426527483757935,
      "grad_norm": 3.862074136734009,
      "learning_rate": 4.429778937635351e-05,
      "loss": 0.7111,
      "step": 749900
    },
    {
      "epoch": 6.843565223738959,
      "grad_norm": 4.88274621963501,
      "learning_rate": 4.429702898021754e-05,
      "loss": 0.6754,
      "step": 750000
    },
    {
      "epoch": 6.844477699102124,
      "grad_norm": 4.459415435791016,
      "learning_rate": 4.429626858408156e-05,
      "loss": 0.6892,
      "step": 750100
    },
    {
      "epoch": 6.845390174465289,
      "grad_norm": 3.56453275680542,
      "learning_rate": 4.42955081879456e-05,
      "loss": 0.6817,
      "step": 750200
    },
    {
      "epoch": 6.846302649828455,
      "grad_norm": 4.7116289138793945,
      "learning_rate": 4.429474779180962e-05,
      "loss": 0.6836,
      "step": 750300
    },
    {
      "epoch": 6.84721512519162,
      "grad_norm": 4.424437046051025,
      "learning_rate": 4.429398739567365e-05,
      "loss": 0.7156,
      "step": 750400
    },
    {
      "epoch": 6.848127600554785,
      "grad_norm": 4.192497730255127,
      "learning_rate": 4.429322699953768e-05,
      "loss": 0.6746,
      "step": 750500
    },
    {
      "epoch": 6.84904007591795,
      "grad_norm": 4.400852203369141,
      "learning_rate": 4.429246660340171e-05,
      "loss": 0.7282,
      "step": 750600
    },
    {
      "epoch": 6.849952551281115,
      "grad_norm": 4.271945476531982,
      "learning_rate": 4.4291706207265735e-05,
      "loss": 0.6784,
      "step": 750700
    },
    {
      "epoch": 6.85086502664428,
      "grad_norm": 3.768873691558838,
      "learning_rate": 4.429094581112977e-05,
      "loss": 0.6765,
      "step": 750800
    },
    {
      "epoch": 6.851777502007446,
      "grad_norm": 2.6098036766052246,
      "learning_rate": 4.4290185414993795e-05,
      "loss": 0.7126,
      "step": 750900
    },
    {
      "epoch": 6.852689977370611,
      "grad_norm": 4.004312992095947,
      "learning_rate": 4.4289425018857825e-05,
      "loss": 0.6729,
      "step": 751000
    },
    {
      "epoch": 6.853602452733776,
      "grad_norm": 3.9126482009887695,
      "learning_rate": 4.4288664622721855e-05,
      "loss": 0.6664,
      "step": 751100
    },
    {
      "epoch": 6.8545149280969415,
      "grad_norm": 4.046774864196777,
      "learning_rate": 4.4287904226585885e-05,
      "loss": 0.6987,
      "step": 751200
    },
    {
      "epoch": 6.855427403460107,
      "grad_norm": 3.930245876312256,
      "learning_rate": 4.4287143830449915e-05,
      "loss": 0.6883,
      "step": 751300
    },
    {
      "epoch": 6.856339878823272,
      "grad_norm": 3.631669759750366,
      "learning_rate": 4.4286383434313945e-05,
      "loss": 0.6744,
      "step": 751400
    },
    {
      "epoch": 6.857252354186437,
      "grad_norm": 3.838818073272705,
      "learning_rate": 4.428562303817797e-05,
      "loss": 0.7041,
      "step": 751500
    },
    {
      "epoch": 6.858164829549602,
      "grad_norm": 4.802136421203613,
      "learning_rate": 4.4284862642042005e-05,
      "loss": 0.7401,
      "step": 751600
    },
    {
      "epoch": 6.859077304912767,
      "grad_norm": 3.828219413757324,
      "learning_rate": 4.428410224590603e-05,
      "loss": 0.7035,
      "step": 751700
    },
    {
      "epoch": 6.859989780275932,
      "grad_norm": 3.774829149246216,
      "learning_rate": 4.428334184977006e-05,
      "loss": 0.7078,
      "step": 751800
    },
    {
      "epoch": 6.860902255639098,
      "grad_norm": 4.1249003410339355,
      "learning_rate": 4.428258145363409e-05,
      "loss": 0.6837,
      "step": 751900
    },
    {
      "epoch": 6.861814731002263,
      "grad_norm": 4.04304838180542,
      "learning_rate": 4.428182105749812e-05,
      "loss": 0.7191,
      "step": 752000
    },
    {
      "epoch": 6.862727206365428,
      "grad_norm": 4.7974371910095215,
      "learning_rate": 4.428106066136214e-05,
      "loss": 0.7193,
      "step": 752100
    },
    {
      "epoch": 6.863639681728594,
      "grad_norm": 4.0334858894348145,
      "learning_rate": 4.428030026522618e-05,
      "loss": 0.684,
      "step": 752200
    },
    {
      "epoch": 6.864552157091758,
      "grad_norm": 3.3113505840301514,
      "learning_rate": 4.42795398690902e-05,
      "loss": 0.729,
      "step": 752300
    },
    {
      "epoch": 6.865464632454923,
      "grad_norm": 3.9025087356567383,
      "learning_rate": 4.427877947295423e-05,
      "loss": 0.6869,
      "step": 752400
    },
    {
      "epoch": 6.866377107818089,
      "grad_norm": 2.8964457511901855,
      "learning_rate": 4.427801907681826e-05,
      "loss": 0.7365,
      "step": 752500
    },
    {
      "epoch": 6.867289583181254,
      "grad_norm": 4.27467679977417,
      "learning_rate": 4.427725868068229e-05,
      "loss": 0.691,
      "step": 752600
    },
    {
      "epoch": 6.868202058544419,
      "grad_norm": 3.9670941829681396,
      "learning_rate": 4.427649828454632e-05,
      "loss": 0.6932,
      "step": 752700
    },
    {
      "epoch": 6.8691145339075845,
      "grad_norm": 3.6182785034179688,
      "learning_rate": 4.4275737888410346e-05,
      "loss": 0.6905,
      "step": 752800
    },
    {
      "epoch": 6.87002700927075,
      "grad_norm": 4.242321968078613,
      "learning_rate": 4.4274977492274376e-05,
      "loss": 0.7371,
      "step": 752900
    },
    {
      "epoch": 6.870939484633915,
      "grad_norm": 3.959416627883911,
      "learning_rate": 4.4274217096138406e-05,
      "loss": 0.7026,
      "step": 753000
    },
    {
      "epoch": 6.87185195999708,
      "grad_norm": 2.844425678253174,
      "learning_rate": 4.4273456700002436e-05,
      "loss": 0.698,
      "step": 753100
    },
    {
      "epoch": 6.872764435360246,
      "grad_norm": 4.3412628173828125,
      "learning_rate": 4.427269630386646e-05,
      "loss": 0.7222,
      "step": 753200
    },
    {
      "epoch": 6.87367691072341,
      "grad_norm": 4.443614482879639,
      "learning_rate": 4.4271935907730496e-05,
      "loss": 0.7211,
      "step": 753300
    },
    {
      "epoch": 6.874589386086575,
      "grad_norm": 4.441691875457764,
      "learning_rate": 4.427117551159452e-05,
      "loss": 0.7125,
      "step": 753400
    },
    {
      "epoch": 6.875501861449741,
      "grad_norm": 5.149247169494629,
      "learning_rate": 4.427041511545855e-05,
      "loss": 0.7246,
      "step": 753500
    },
    {
      "epoch": 6.876414336812906,
      "grad_norm": 4.487112522125244,
      "learning_rate": 4.426965471932258e-05,
      "loss": 0.6479,
      "step": 753600
    },
    {
      "epoch": 6.877326812176071,
      "grad_norm": 4.363757610321045,
      "learning_rate": 4.426889432318661e-05,
      "loss": 0.7019,
      "step": 753700
    },
    {
      "epoch": 6.878239287539237,
      "grad_norm": 3.9420247077941895,
      "learning_rate": 4.426813392705064e-05,
      "loss": 0.7153,
      "step": 753800
    },
    {
      "epoch": 6.879151762902402,
      "grad_norm": 4.4397759437561035,
      "learning_rate": 4.426737353091467e-05,
      "loss": 0.67,
      "step": 753900
    },
    {
      "epoch": 6.880064238265566,
      "grad_norm": 3.0035791397094727,
      "learning_rate": 4.426661313477869e-05,
      "loss": 0.6651,
      "step": 754000
    },
    {
      "epoch": 6.880976713628732,
      "grad_norm": 4.516870498657227,
      "learning_rate": 4.426585273864273e-05,
      "loss": 0.6701,
      "step": 754100
    },
    {
      "epoch": 6.881889188991897,
      "grad_norm": 3.8281314373016357,
      "learning_rate": 4.426509234250675e-05,
      "loss": 0.6903,
      "step": 754200
    },
    {
      "epoch": 6.882801664355062,
      "grad_norm": 4.918989181518555,
      "learning_rate": 4.426433194637078e-05,
      "loss": 0.6626,
      "step": 754300
    },
    {
      "epoch": 6.8837141397182275,
      "grad_norm": 3.9032037258148193,
      "learning_rate": 4.426357155023481e-05,
      "loss": 0.6942,
      "step": 754400
    },
    {
      "epoch": 6.884626615081393,
      "grad_norm": 4.305855751037598,
      "learning_rate": 4.426281115409884e-05,
      "loss": 0.6985,
      "step": 754500
    },
    {
      "epoch": 6.885539090444558,
      "grad_norm": 3.2492048740386963,
      "learning_rate": 4.4262050757962867e-05,
      "loss": 0.7075,
      "step": 754600
    },
    {
      "epoch": 6.886451565807723,
      "grad_norm": 3.8114523887634277,
      "learning_rate": 4.42612903618269e-05,
      "loss": 0.6999,
      "step": 754700
    },
    {
      "epoch": 6.887364041170889,
      "grad_norm": 4.0102949142456055,
      "learning_rate": 4.426052996569093e-05,
      "loss": 0.6846,
      "step": 754800
    },
    {
      "epoch": 6.888276516534054,
      "grad_norm": 3.8480043411254883,
      "learning_rate": 4.425976956955496e-05,
      "loss": 0.7264,
      "step": 754900
    },
    {
      "epoch": 6.8891889918972185,
      "grad_norm": 3.6795809268951416,
      "learning_rate": 4.425900917341899e-05,
      "loss": 0.7036,
      "step": 755000
    },
    {
      "epoch": 6.890101467260384,
      "grad_norm": 3.1680853366851807,
      "learning_rate": 4.425824877728302e-05,
      "loss": 0.7009,
      "step": 755100
    },
    {
      "epoch": 6.891013942623549,
      "grad_norm": 5.037182807922363,
      "learning_rate": 4.425748838114705e-05,
      "loss": 0.6906,
      "step": 755200
    },
    {
      "epoch": 6.891926417986714,
      "grad_norm": 3.838935375213623,
      "learning_rate": 4.425672798501108e-05,
      "loss": 0.731,
      "step": 755300
    },
    {
      "epoch": 6.89283889334988,
      "grad_norm": 4.170586585998535,
      "learning_rate": 4.42559675888751e-05,
      "loss": 0.691,
      "step": 755400
    },
    {
      "epoch": 6.893751368713045,
      "grad_norm": 4.504696369171143,
      "learning_rate": 4.425520719273913e-05,
      "loss": 0.6802,
      "step": 755500
    },
    {
      "epoch": 6.89466384407621,
      "grad_norm": 4.1281328201293945,
      "learning_rate": 4.425444679660316e-05,
      "loss": 0.7052,
      "step": 755600
    },
    {
      "epoch": 6.895576319439375,
      "grad_norm": 4.347054958343506,
      "learning_rate": 4.4253686400467184e-05,
      "loss": 0.707,
      "step": 755700
    },
    {
      "epoch": 6.89648879480254,
      "grad_norm": 4.457136631011963,
      "learning_rate": 4.425292600433122e-05,
      "loss": 0.6929,
      "step": 755800
    },
    {
      "epoch": 6.897401270165705,
      "grad_norm": 3.753711700439453,
      "learning_rate": 4.4252165608195244e-05,
      "loss": 0.6808,
      "step": 755900
    },
    {
      "epoch": 6.8983137455288706,
      "grad_norm": 2.46919846534729,
      "learning_rate": 4.4251405212059274e-05,
      "loss": 0.7323,
      "step": 756000
    },
    {
      "epoch": 6.899226220892036,
      "grad_norm": 4.140567779541016,
      "learning_rate": 4.4250644815923304e-05,
      "loss": 0.7017,
      "step": 756100
    },
    {
      "epoch": 6.900138696255201,
      "grad_norm": 4.375730514526367,
      "learning_rate": 4.4249884419787334e-05,
      "loss": 0.6927,
      "step": 756200
    },
    {
      "epoch": 6.9010511716183665,
      "grad_norm": 4.159852027893066,
      "learning_rate": 4.4249124023651364e-05,
      "loss": 0.7502,
      "step": 756300
    },
    {
      "epoch": 6.901963646981532,
      "grad_norm": 3.6553709506988525,
      "learning_rate": 4.4248363627515394e-05,
      "loss": 0.6982,
      "step": 756400
    },
    {
      "epoch": 6.902876122344697,
      "grad_norm": 4.199901103973389,
      "learning_rate": 4.424760323137942e-05,
      "loss": 0.7087,
      "step": 756500
    },
    {
      "epoch": 6.903788597707862,
      "grad_norm": 3.5866944789886475,
      "learning_rate": 4.4246842835243454e-05,
      "loss": 0.6856,
      "step": 756600
    },
    {
      "epoch": 6.904701073071027,
      "grad_norm": 3.663930654525757,
      "learning_rate": 4.424608243910748e-05,
      "loss": 0.6605,
      "step": 756700
    },
    {
      "epoch": 6.905613548434192,
      "grad_norm": 3.428816795349121,
      "learning_rate": 4.424532204297151e-05,
      "loss": 0.7138,
      "step": 756800
    },
    {
      "epoch": 6.906526023797357,
      "grad_norm": 2.8503713607788086,
      "learning_rate": 4.424456164683554e-05,
      "loss": 0.7063,
      "step": 756900
    },
    {
      "epoch": 6.907438499160523,
      "grad_norm": 3.542673110961914,
      "learning_rate": 4.424380125069957e-05,
      "loss": 0.6601,
      "step": 757000
    },
    {
      "epoch": 6.908350974523688,
      "grad_norm": 4.400412559509277,
      "learning_rate": 4.424304085456359e-05,
      "loss": 0.709,
      "step": 757100
    },
    {
      "epoch": 6.909263449886853,
      "grad_norm": 3.6823577880859375,
      "learning_rate": 4.424228045842763e-05,
      "loss": 0.6604,
      "step": 757200
    },
    {
      "epoch": 6.910175925250019,
      "grad_norm": 3.779933452606201,
      "learning_rate": 4.424152006229165e-05,
      "loss": 0.6933,
      "step": 757300
    },
    {
      "epoch": 6.911088400613183,
      "grad_norm": 4.390562057495117,
      "learning_rate": 4.424075966615568e-05,
      "loss": 0.7051,
      "step": 757400
    },
    {
      "epoch": 6.912000875976348,
      "grad_norm": 3.8317856788635254,
      "learning_rate": 4.423999927001971e-05,
      "loss": 0.7034,
      "step": 757500
    },
    {
      "epoch": 6.912913351339514,
      "grad_norm": 4.930249214172363,
      "learning_rate": 4.423923887388374e-05,
      "loss": 0.7095,
      "step": 757600
    },
    {
      "epoch": 6.913825826702679,
      "grad_norm": 3.6359102725982666,
      "learning_rate": 4.423847847774777e-05,
      "loss": 0.6814,
      "step": 757700
    },
    {
      "epoch": 6.914738302065844,
      "grad_norm": 5.011292457580566,
      "learning_rate": 4.42377180816118e-05,
      "loss": 0.7084,
      "step": 757800
    },
    {
      "epoch": 6.9156507774290095,
      "grad_norm": 4.0985798835754395,
      "learning_rate": 4.4236957685475825e-05,
      "loss": 0.7143,
      "step": 757900
    },
    {
      "epoch": 6.916563252792175,
      "grad_norm": 3.7709717750549316,
      "learning_rate": 4.423619728933986e-05,
      "loss": 0.6935,
      "step": 758000
    },
    {
      "epoch": 6.91747572815534,
      "grad_norm": 4.816951751708984,
      "learning_rate": 4.4235436893203885e-05,
      "loss": 0.7093,
      "step": 758100
    },
    {
      "epoch": 6.918388203518505,
      "grad_norm": 3.431016683578491,
      "learning_rate": 4.4234676497067915e-05,
      "loss": 0.715,
      "step": 758200
    },
    {
      "epoch": 6.919300678881671,
      "grad_norm": 4.647478103637695,
      "learning_rate": 4.4233916100931945e-05,
      "loss": 0.7233,
      "step": 758300
    },
    {
      "epoch": 6.920213154244835,
      "grad_norm": 4.694158554077148,
      "learning_rate": 4.423315570479597e-05,
      "loss": 0.7398,
      "step": 758400
    },
    {
      "epoch": 6.921125629608,
      "grad_norm": 3.518735885620117,
      "learning_rate": 4.423239530866e-05,
      "loss": 0.7086,
      "step": 758500
    },
    {
      "epoch": 6.922038104971166,
      "grad_norm": 4.500468730926514,
      "learning_rate": 4.423163491252403e-05,
      "loss": 0.6725,
      "step": 758600
    },
    {
      "epoch": 6.922950580334331,
      "grad_norm": 3.3827261924743652,
      "learning_rate": 4.423087451638806e-05,
      "loss": 0.7206,
      "step": 758700
    },
    {
      "epoch": 6.923863055697496,
      "grad_norm": 4.481821060180664,
      "learning_rate": 4.423011412025209e-05,
      "loss": 0.6962,
      "step": 758800
    },
    {
      "epoch": 6.924775531060662,
      "grad_norm": 4.394136905670166,
      "learning_rate": 4.422935372411612e-05,
      "loss": 0.6772,
      "step": 758900
    },
    {
      "epoch": 6.925688006423827,
      "grad_norm": 2.4604053497314453,
      "learning_rate": 4.422859332798014e-05,
      "loss": 0.6886,
      "step": 759000
    },
    {
      "epoch": 6.926600481786991,
      "grad_norm": 4.551462173461914,
      "learning_rate": 4.422783293184418e-05,
      "loss": 0.7217,
      "step": 759100
    },
    {
      "epoch": 6.927512957150157,
      "grad_norm": 3.953071117401123,
      "learning_rate": 4.42270725357082e-05,
      "loss": 0.7256,
      "step": 759200
    },
    {
      "epoch": 6.928425432513322,
      "grad_norm": 4.26982307434082,
      "learning_rate": 4.422631213957223e-05,
      "loss": 0.7075,
      "step": 759300
    },
    {
      "epoch": 6.929337907876487,
      "grad_norm": 2.892392158508301,
      "learning_rate": 4.422555174343626e-05,
      "loss": 0.7166,
      "step": 759400
    },
    {
      "epoch": 6.9302503832396525,
      "grad_norm": 3.398183822631836,
      "learning_rate": 4.422479134730029e-05,
      "loss": 0.7028,
      "step": 759500
    },
    {
      "epoch": 6.931162858602818,
      "grad_norm": 3.6208581924438477,
      "learning_rate": 4.422403095116432e-05,
      "loss": 0.7185,
      "step": 759600
    },
    {
      "epoch": 6.932075333965983,
      "grad_norm": 3.7753610610961914,
      "learning_rate": 4.422327055502835e-05,
      "loss": 0.7173,
      "step": 759700
    },
    {
      "epoch": 6.932987809329148,
      "grad_norm": 4.279267311096191,
      "learning_rate": 4.4222510158892376e-05,
      "loss": 0.6525,
      "step": 759800
    },
    {
      "epoch": 6.933900284692314,
      "grad_norm": 4.319032192230225,
      "learning_rate": 4.4221749762756406e-05,
      "loss": 0.7316,
      "step": 759900
    },
    {
      "epoch": 6.934812760055479,
      "grad_norm": 3.8253238201141357,
      "learning_rate": 4.4220989366620436e-05,
      "loss": 0.6905,
      "step": 760000
    },
    {
      "epoch": 6.935725235418643,
      "grad_norm": 3.610482692718506,
      "learning_rate": 4.4220228970484466e-05,
      "loss": 0.7243,
      "step": 760100
    },
    {
      "epoch": 6.936637710781809,
      "grad_norm": 3.54036283493042,
      "learning_rate": 4.4219468574348496e-05,
      "loss": 0.7282,
      "step": 760200
    },
    {
      "epoch": 6.937550186144974,
      "grad_norm": 3.2633981704711914,
      "learning_rate": 4.4218708178212526e-05,
      "loss": 0.7074,
      "step": 760300
    },
    {
      "epoch": 6.938462661508139,
      "grad_norm": 5.031848907470703,
      "learning_rate": 4.421794778207655e-05,
      "loss": 0.6814,
      "step": 760400
    },
    {
      "epoch": 6.939375136871305,
      "grad_norm": 3.7186851501464844,
      "learning_rate": 4.4217187385940586e-05,
      "loss": 0.691,
      "step": 760500
    },
    {
      "epoch": 6.94028761223447,
      "grad_norm": 2.110105037689209,
      "learning_rate": 4.421642698980461e-05,
      "loss": 0.7392,
      "step": 760600
    },
    {
      "epoch": 6.941200087597635,
      "grad_norm": 4.393390655517578,
      "learning_rate": 4.421566659366864e-05,
      "loss": 0.7137,
      "step": 760700
    },
    {
      "epoch": 6.9421125629608,
      "grad_norm": 4.045477390289307,
      "learning_rate": 4.421490619753267e-05,
      "loss": 0.7104,
      "step": 760800
    },
    {
      "epoch": 6.943025038323965,
      "grad_norm": 4.001308441162109,
      "learning_rate": 4.42141458013967e-05,
      "loss": 0.7118,
      "step": 760900
    },
    {
      "epoch": 6.94393751368713,
      "grad_norm": 3.7559447288513184,
      "learning_rate": 4.421338540526073e-05,
      "loss": 0.6697,
      "step": 761000
    },
    {
      "epoch": 6.9448499890502955,
      "grad_norm": 4.6031413078308105,
      "learning_rate": 4.421262500912476e-05,
      "loss": 0.713,
      "step": 761100
    },
    {
      "epoch": 6.945762464413461,
      "grad_norm": 3.3177239894866943,
      "learning_rate": 4.421186461298878e-05,
      "loss": 0.6877,
      "step": 761200
    },
    {
      "epoch": 6.946674939776626,
      "grad_norm": 4.318973541259766,
      "learning_rate": 4.421110421685281e-05,
      "loss": 0.6749,
      "step": 761300
    },
    {
      "epoch": 6.947587415139791,
      "grad_norm": 4.603702545166016,
      "learning_rate": 4.421034382071684e-05,
      "loss": 0.6986,
      "step": 761400
    },
    {
      "epoch": 6.948499890502957,
      "grad_norm": 3.91684889793396,
      "learning_rate": 4.4209583424580867e-05,
      "loss": 0.6908,
      "step": 761500
    },
    {
      "epoch": 6.949412365866122,
      "grad_norm": 3.7648966312408447,
      "learning_rate": 4.4208823028444903e-05,
      "loss": 0.6848,
      "step": 761600
    },
    {
      "epoch": 6.950324841229287,
      "grad_norm": 3.9700818061828613,
      "learning_rate": 4.420806263230893e-05,
      "loss": 0.7423,
      "step": 761700
    },
    {
      "epoch": 6.951237316592452,
      "grad_norm": 4.539159774780273,
      "learning_rate": 4.420730223617296e-05,
      "loss": 0.6875,
      "step": 761800
    },
    {
      "epoch": 6.952149791955617,
      "grad_norm": 4.463672637939453,
      "learning_rate": 4.420654184003699e-05,
      "loss": 0.665,
      "step": 761900
    },
    {
      "epoch": 6.953062267318782,
      "grad_norm": 3.9229891300201416,
      "learning_rate": 4.420578144390102e-05,
      "loss": 0.7272,
      "step": 762000
    },
    {
      "epoch": 6.953974742681948,
      "grad_norm": 4.347483158111572,
      "learning_rate": 4.420502104776505e-05,
      "loss": 0.6878,
      "step": 762100
    },
    {
      "epoch": 6.954887218045113,
      "grad_norm": 2.973703622817993,
      "learning_rate": 4.420426065162908e-05,
      "loss": 0.702,
      "step": 762200
    },
    {
      "epoch": 6.955799693408278,
      "grad_norm": 3.539077043533325,
      "learning_rate": 4.42035002554931e-05,
      "loss": 0.6786,
      "step": 762300
    },
    {
      "epoch": 6.9567121687714435,
      "grad_norm": 3.812999725341797,
      "learning_rate": 4.420273985935714e-05,
      "loss": 0.7249,
      "step": 762400
    },
    {
      "epoch": 6.957624644134608,
      "grad_norm": 3.742833375930786,
      "learning_rate": 4.420197946322116e-05,
      "loss": 0.7137,
      "step": 762500
    },
    {
      "epoch": 6.958537119497773,
      "grad_norm": 3.8551244735717773,
      "learning_rate": 4.420121906708519e-05,
      "loss": 0.7364,
      "step": 762600
    },
    {
      "epoch": 6.9594495948609385,
      "grad_norm": 4.318471908569336,
      "learning_rate": 4.420045867094922e-05,
      "loss": 0.7026,
      "step": 762700
    },
    {
      "epoch": 6.960362070224104,
      "grad_norm": 7.096879005432129,
      "learning_rate": 4.419969827481325e-05,
      "loss": 0.7363,
      "step": 762800
    },
    {
      "epoch": 6.961274545587269,
      "grad_norm": 5.680782318115234,
      "learning_rate": 4.4198937878677274e-05,
      "loss": 0.6985,
      "step": 762900
    },
    {
      "epoch": 6.962187020950434,
      "grad_norm": 4.726323127746582,
      "learning_rate": 4.419817748254131e-05,
      "loss": 0.6641,
      "step": 763000
    },
    {
      "epoch": 6.9630994963136,
      "grad_norm": 3.5827507972717285,
      "learning_rate": 4.4197417086405334e-05,
      "loss": 0.6809,
      "step": 763100
    },
    {
      "epoch": 6.964011971676765,
      "grad_norm": 3.7593541145324707,
      "learning_rate": 4.4196656690269364e-05,
      "loss": 0.7024,
      "step": 763200
    },
    {
      "epoch": 6.96492444703993,
      "grad_norm": 4.56695556640625,
      "learning_rate": 4.4195896294133394e-05,
      "loss": 0.6971,
      "step": 763300
    },
    {
      "epoch": 6.965836922403096,
      "grad_norm": 3.3397209644317627,
      "learning_rate": 4.4195135897997424e-05,
      "loss": 0.7076,
      "step": 763400
    },
    {
      "epoch": 6.96674939776626,
      "grad_norm": 3.995863437652588,
      "learning_rate": 4.4194375501861454e-05,
      "loss": 0.6621,
      "step": 763500
    },
    {
      "epoch": 6.967661873129425,
      "grad_norm": 4.006987571716309,
      "learning_rate": 4.4193615105725484e-05,
      "loss": 0.7119,
      "step": 763600
    },
    {
      "epoch": 6.968574348492591,
      "grad_norm": 4.722498416900635,
      "learning_rate": 4.419285470958951e-05,
      "loss": 0.6984,
      "step": 763700
    },
    {
      "epoch": 6.969486823855756,
      "grad_norm": 4.61163854598999,
      "learning_rate": 4.4192094313453545e-05,
      "loss": 0.7487,
      "step": 763800
    },
    {
      "epoch": 6.970399299218921,
      "grad_norm": 4.096593379974365,
      "learning_rate": 4.419133391731757e-05,
      "loss": 0.6897,
      "step": 763900
    },
    {
      "epoch": 6.9713117745820865,
      "grad_norm": 4.654801845550537,
      "learning_rate": 4.41905735211816e-05,
      "loss": 0.6532,
      "step": 764000
    },
    {
      "epoch": 6.972224249945252,
      "grad_norm": 3.6400182247161865,
      "learning_rate": 4.418981312504563e-05,
      "loss": 0.6644,
      "step": 764100
    },
    {
      "epoch": 6.973136725308416,
      "grad_norm": 4.058648109436035,
      "learning_rate": 4.418905272890965e-05,
      "loss": 0.6708,
      "step": 764200
    },
    {
      "epoch": 6.9740492006715815,
      "grad_norm": 4.034899711608887,
      "learning_rate": 4.418829233277368e-05,
      "loss": 0.6878,
      "step": 764300
    },
    {
      "epoch": 6.974961676034747,
      "grad_norm": 3.5743041038513184,
      "learning_rate": 4.418753193663771e-05,
      "loss": 0.7154,
      "step": 764400
    },
    {
      "epoch": 6.975874151397912,
      "grad_norm": 3.2817540168762207,
      "learning_rate": 4.418677154050174e-05,
      "loss": 0.7144,
      "step": 764500
    },
    {
      "epoch": 6.976786626761077,
      "grad_norm": 4.046992778778076,
      "learning_rate": 4.418601114436577e-05,
      "loss": 0.7254,
      "step": 764600
    },
    {
      "epoch": 6.977699102124243,
      "grad_norm": 4.105567932128906,
      "learning_rate": 4.41852507482298e-05,
      "loss": 0.7036,
      "step": 764700
    },
    {
      "epoch": 6.978611577487408,
      "grad_norm": 3.691520929336548,
      "learning_rate": 4.4184490352093825e-05,
      "loss": 0.655,
      "step": 764800
    },
    {
      "epoch": 6.979524052850573,
      "grad_norm": 3.6415297985076904,
      "learning_rate": 4.418372995595786e-05,
      "loss": 0.7065,
      "step": 764900
    },
    {
      "epoch": 6.980436528213739,
      "grad_norm": 2.810953140258789,
      "learning_rate": 4.4182969559821885e-05,
      "loss": 0.6813,
      "step": 765000
    },
    {
      "epoch": 6.981349003576904,
      "grad_norm": 3.784106731414795,
      "learning_rate": 4.4182209163685915e-05,
      "loss": 0.6774,
      "step": 765100
    },
    {
      "epoch": 6.982261478940068,
      "grad_norm": 3.663634777069092,
      "learning_rate": 4.4181448767549945e-05,
      "loss": 0.6986,
      "step": 765200
    },
    {
      "epoch": 6.983173954303234,
      "grad_norm": 4.353752613067627,
      "learning_rate": 4.4180688371413975e-05,
      "loss": 0.7016,
      "step": 765300
    },
    {
      "epoch": 6.984086429666399,
      "grad_norm": 5.173269271850586,
      "learning_rate": 4.4179927975278e-05,
      "loss": 0.6991,
      "step": 765400
    },
    {
      "epoch": 6.984998905029564,
      "grad_norm": 4.022024631500244,
      "learning_rate": 4.4179167579142035e-05,
      "loss": 0.716,
      "step": 765500
    },
    {
      "epoch": 6.9859113803927295,
      "grad_norm": 4.737987041473389,
      "learning_rate": 4.417840718300606e-05,
      "loss": 0.7459,
      "step": 765600
    },
    {
      "epoch": 6.986823855755895,
      "grad_norm": 4.492669582366943,
      "learning_rate": 4.417764678687009e-05,
      "loss": 0.6885,
      "step": 765700
    },
    {
      "epoch": 6.98773633111906,
      "grad_norm": 4.1784281730651855,
      "learning_rate": 4.417688639073412e-05,
      "loss": 0.7099,
      "step": 765800
    },
    {
      "epoch": 6.9886488064822245,
      "grad_norm": 3.259091854095459,
      "learning_rate": 4.417612599459815e-05,
      "loss": 0.6931,
      "step": 765900
    },
    {
      "epoch": 6.98956128184539,
      "grad_norm": 4.339446544647217,
      "learning_rate": 4.417536559846218e-05,
      "loss": 0.7201,
      "step": 766000
    },
    {
      "epoch": 6.990473757208555,
      "grad_norm": 4.2547502517700195,
      "learning_rate": 4.417460520232621e-05,
      "loss": 0.7022,
      "step": 766100
    },
    {
      "epoch": 6.99138623257172,
      "grad_norm": 4.454148769378662,
      "learning_rate": 4.417384480619023e-05,
      "loss": 0.7241,
      "step": 766200
    },
    {
      "epoch": 6.992298707934886,
      "grad_norm": 3.1653010845184326,
      "learning_rate": 4.417308441005427e-05,
      "loss": 0.6755,
      "step": 766300
    },
    {
      "epoch": 6.993211183298051,
      "grad_norm": 3.728367567062378,
      "learning_rate": 4.417232401391829e-05,
      "loss": 0.7025,
      "step": 766400
    },
    {
      "epoch": 6.994123658661216,
      "grad_norm": 4.274419784545898,
      "learning_rate": 4.417156361778232e-05,
      "loss": 0.7444,
      "step": 766500
    },
    {
      "epoch": 6.995036134024382,
      "grad_norm": 3.5104007720947266,
      "learning_rate": 4.417080322164635e-05,
      "loss": 0.6976,
      "step": 766600
    },
    {
      "epoch": 6.995948609387547,
      "grad_norm": 3.6631722450256348,
      "learning_rate": 4.417004282551038e-05,
      "loss": 0.7238,
      "step": 766700
    },
    {
      "epoch": 6.996861084750712,
      "grad_norm": 3.7651145458221436,
      "learning_rate": 4.4169282429374406e-05,
      "loss": 0.6943,
      "step": 766800
    },
    {
      "epoch": 6.997773560113877,
      "grad_norm": 3.650191068649292,
      "learning_rate": 4.4168522033238436e-05,
      "loss": 0.6837,
      "step": 766900
    },
    {
      "epoch": 6.998686035477042,
      "grad_norm": 3.2611844539642334,
      "learning_rate": 4.4167761637102466e-05,
      "loss": 0.698,
      "step": 767000
    },
    {
      "epoch": 6.999598510840207,
      "grad_norm": 4.520684242248535,
      "learning_rate": 4.4167001240966496e-05,
      "loss": 0.6872,
      "step": 767100
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.570025622844696,
      "eval_runtime": 25.4066,
      "eval_samples_per_second": 227.067,
      "eval_steps_per_second": 227.067,
      "step": 767144
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.5517760515213013,
      "eval_runtime": 493.924,
      "eval_samples_per_second": 221.88,
      "eval_steps_per_second": 221.88,
      "step": 767144
    },
    {
      "epoch": 7.0005109862033725,
      "grad_norm": 3.9517359733581543,
      "learning_rate": 4.4166240844830526e-05,
      "loss": 0.669,
      "step": 767200
    },
    {
      "epoch": 7.001423461566538,
      "grad_norm": 4.4877400398254395,
      "learning_rate": 4.416548044869455e-05,
      "loss": 0.7426,
      "step": 767300
    },
    {
      "epoch": 7.002335936929703,
      "grad_norm": 3.4234349727630615,
      "learning_rate": 4.4164720052558586e-05,
      "loss": 0.6763,
      "step": 767400
    },
    {
      "epoch": 7.003248412292868,
      "grad_norm": 4.420263290405273,
      "learning_rate": 4.416395965642261e-05,
      "loss": 0.7127,
      "step": 767500
    },
    {
      "epoch": 7.004160887656034,
      "grad_norm": 4.555130958557129,
      "learning_rate": 4.416319926028664e-05,
      "loss": 0.6981,
      "step": 767600
    },
    {
      "epoch": 7.005073363019198,
      "grad_norm": 4.342155456542969,
      "learning_rate": 4.416243886415067e-05,
      "loss": 0.6831,
      "step": 767700
    },
    {
      "epoch": 7.005985838382363,
      "grad_norm": 4.541335582733154,
      "learning_rate": 4.41616784680147e-05,
      "loss": 0.7185,
      "step": 767800
    },
    {
      "epoch": 7.006898313745529,
      "grad_norm": 4.1691131591796875,
      "learning_rate": 4.416091807187872e-05,
      "loss": 0.7393,
      "step": 767900
    },
    {
      "epoch": 7.007810789108694,
      "grad_norm": 4.419147491455078,
      "learning_rate": 4.416015767574276e-05,
      "loss": 0.7114,
      "step": 768000
    },
    {
      "epoch": 7.008723264471859,
      "grad_norm": 3.522830009460449,
      "learning_rate": 4.415939727960678e-05,
      "loss": 0.6779,
      "step": 768100
    },
    {
      "epoch": 7.009635739835025,
      "grad_norm": 4.483643054962158,
      "learning_rate": 4.415863688347081e-05,
      "loss": 0.6857,
      "step": 768200
    },
    {
      "epoch": 7.01054821519819,
      "grad_norm": 3.8431434631347656,
      "learning_rate": 4.4157876487334843e-05,
      "loss": 0.6938,
      "step": 768300
    },
    {
      "epoch": 7.011460690561355,
      "grad_norm": 4.57380485534668,
      "learning_rate": 4.4157116091198873e-05,
      "loss": 0.7083,
      "step": 768400
    },
    {
      "epoch": 7.01237316592452,
      "grad_norm": 4.095864295959473,
      "learning_rate": 4.4156355695062904e-05,
      "loss": 0.7083,
      "step": 768500
    },
    {
      "epoch": 7.013285641287685,
      "grad_norm": 2.077208995819092,
      "learning_rate": 4.4155595298926934e-05,
      "loss": 0.7257,
      "step": 768600
    },
    {
      "epoch": 7.01419811665085,
      "grad_norm": 3.946009397506714,
      "learning_rate": 4.415483490279096e-05,
      "loss": 0.7098,
      "step": 768700
    },
    {
      "epoch": 7.0151105920140155,
      "grad_norm": 3.056142568588257,
      "learning_rate": 4.4154074506654994e-05,
      "loss": 0.6577,
      "step": 768800
    },
    {
      "epoch": 7.016023067377181,
      "grad_norm": 4.911185264587402,
      "learning_rate": 4.415331411051902e-05,
      "loss": 0.7061,
      "step": 768900
    },
    {
      "epoch": 7.016935542740346,
      "grad_norm": 4.219922065734863,
      "learning_rate": 4.415255371438305e-05,
      "loss": 0.7036,
      "step": 769000
    },
    {
      "epoch": 7.017848018103511,
      "grad_norm": 3.5912539958953857,
      "learning_rate": 4.415179331824708e-05,
      "loss": 0.6445,
      "step": 769100
    },
    {
      "epoch": 7.018760493466677,
      "grad_norm": 3.1248652935028076,
      "learning_rate": 4.415103292211111e-05,
      "loss": 0.7079,
      "step": 769200
    },
    {
      "epoch": 7.019672968829841,
      "grad_norm": 4.9219183921813965,
      "learning_rate": 4.415027252597513e-05,
      "loss": 0.7188,
      "step": 769300
    },
    {
      "epoch": 7.0205854441930065,
      "grad_norm": 4.050947666168213,
      "learning_rate": 4.414951212983917e-05,
      "loss": 0.6881,
      "step": 769400
    },
    {
      "epoch": 7.021497919556172,
      "grad_norm": 4.457200527191162,
      "learning_rate": 4.414875173370319e-05,
      "loss": 0.6961,
      "step": 769500
    },
    {
      "epoch": 7.022410394919337,
      "grad_norm": 4.140766620635986,
      "learning_rate": 4.414799133756722e-05,
      "loss": 0.7104,
      "step": 769600
    },
    {
      "epoch": 7.023322870282502,
      "grad_norm": 4.855465412139893,
      "learning_rate": 4.414723094143125e-05,
      "loss": 0.7081,
      "step": 769700
    },
    {
      "epoch": 7.024235345645668,
      "grad_norm": 4.283823490142822,
      "learning_rate": 4.4146470545295274e-05,
      "loss": 0.7168,
      "step": 769800
    },
    {
      "epoch": 7.025147821008833,
      "grad_norm": 3.953240156173706,
      "learning_rate": 4.414571014915931e-05,
      "loss": 0.6573,
      "step": 769900
    },
    {
      "epoch": 7.026060296371998,
      "grad_norm": 4.991389274597168,
      "learning_rate": 4.4144949753023334e-05,
      "loss": 0.7495,
      "step": 770000
    },
    {
      "epoch": 7.0269727717351635,
      "grad_norm": 3.5842947959899902,
      "learning_rate": 4.4144189356887364e-05,
      "loss": 0.675,
      "step": 770100
    },
    {
      "epoch": 7.027885247098328,
      "grad_norm": 3.6074118614196777,
      "learning_rate": 4.4143428960751394e-05,
      "loss": 0.6615,
      "step": 770200
    },
    {
      "epoch": 7.028797722461493,
      "grad_norm": 4.386507034301758,
      "learning_rate": 4.4142668564615424e-05,
      "loss": 0.6639,
      "step": 770300
    },
    {
      "epoch": 7.029710197824659,
      "grad_norm": 4.081402778625488,
      "learning_rate": 4.414190816847945e-05,
      "loss": 0.6939,
      "step": 770400
    },
    {
      "epoch": 7.030622673187824,
      "grad_norm": 3.934051752090454,
      "learning_rate": 4.4141147772343485e-05,
      "loss": 0.6891,
      "step": 770500
    },
    {
      "epoch": 7.031535148550989,
      "grad_norm": 4.9994215965271,
      "learning_rate": 4.414038737620751e-05,
      "loss": 0.6729,
      "step": 770600
    },
    {
      "epoch": 7.0324476239141545,
      "grad_norm": 3.09846830368042,
      "learning_rate": 4.413962698007154e-05,
      "loss": 0.704,
      "step": 770700
    },
    {
      "epoch": 7.03336009927732,
      "grad_norm": 3.0120859146118164,
      "learning_rate": 4.413886658393557e-05,
      "loss": 0.6658,
      "step": 770800
    },
    {
      "epoch": 7.034272574640485,
      "grad_norm": 3.984382390975952,
      "learning_rate": 4.41381061877996e-05,
      "loss": 0.6991,
      "step": 770900
    },
    {
      "epoch": 7.0351850500036495,
      "grad_norm": 3.0680432319641113,
      "learning_rate": 4.413734579166363e-05,
      "loss": 0.6787,
      "step": 771000
    },
    {
      "epoch": 7.036097525366815,
      "grad_norm": 3.8654849529266357,
      "learning_rate": 4.413658539552766e-05,
      "loss": 0.6803,
      "step": 771100
    },
    {
      "epoch": 7.03701000072998,
      "grad_norm": 4.454318523406982,
      "learning_rate": 4.413582499939168e-05,
      "loss": 0.707,
      "step": 771200
    },
    {
      "epoch": 7.037922476093145,
      "grad_norm": 4.5684709548950195,
      "learning_rate": 4.413506460325572e-05,
      "loss": 0.7247,
      "step": 771300
    },
    {
      "epoch": 7.038834951456311,
      "grad_norm": 3.667092800140381,
      "learning_rate": 4.413430420711974e-05,
      "loss": 0.7042,
      "step": 771400
    },
    {
      "epoch": 7.039747426819476,
      "grad_norm": 3.718829870223999,
      "learning_rate": 4.413354381098377e-05,
      "loss": 0.6784,
      "step": 771500
    },
    {
      "epoch": 7.040659902182641,
      "grad_norm": 4.579007148742676,
      "learning_rate": 4.41327834148478e-05,
      "loss": 0.6914,
      "step": 771600
    },
    {
      "epoch": 7.041572377545807,
      "grad_norm": 4.352647304534912,
      "learning_rate": 4.413202301871183e-05,
      "loss": 0.6615,
      "step": 771700
    },
    {
      "epoch": 7.042484852908972,
      "grad_norm": 4.604331016540527,
      "learning_rate": 4.413126262257586e-05,
      "loss": 0.6812,
      "step": 771800
    },
    {
      "epoch": 7.043397328272136,
      "grad_norm": 3.5000100135803223,
      "learning_rate": 4.413050222643989e-05,
      "loss": 0.6865,
      "step": 771900
    },
    {
      "epoch": 7.044309803635302,
      "grad_norm": 2.1465187072753906,
      "learning_rate": 4.4129741830303915e-05,
      "loss": 0.7145,
      "step": 772000
    },
    {
      "epoch": 7.045222278998467,
      "grad_norm": 3.4353952407836914,
      "learning_rate": 4.4128981434167945e-05,
      "loss": 0.6998,
      "step": 772100
    },
    {
      "epoch": 7.046134754361632,
      "grad_norm": 4.592819690704346,
      "learning_rate": 4.4128221038031975e-05,
      "loss": 0.6586,
      "step": 772200
    },
    {
      "epoch": 7.0470472297247975,
      "grad_norm": 3.571037530899048,
      "learning_rate": 4.4127460641896005e-05,
      "loss": 0.6642,
      "step": 772300
    },
    {
      "epoch": 7.047959705087963,
      "grad_norm": 4.041794776916504,
      "learning_rate": 4.4126700245760036e-05,
      "loss": 0.6927,
      "step": 772400
    },
    {
      "epoch": 7.048872180451128,
      "grad_norm": 3.3081374168395996,
      "learning_rate": 4.4125939849624066e-05,
      "loss": 0.7348,
      "step": 772500
    },
    {
      "epoch": 7.049784655814293,
      "grad_norm": 4.181865215301514,
      "learning_rate": 4.412517945348809e-05,
      "loss": 0.7196,
      "step": 772600
    },
    {
      "epoch": 7.050697131177458,
      "grad_norm": 3.9900882244110107,
      "learning_rate": 4.412441905735212e-05,
      "loss": 0.7,
      "step": 772700
    },
    {
      "epoch": 7.051609606540623,
      "grad_norm": 4.091845512390137,
      "learning_rate": 4.412365866121615e-05,
      "loss": 0.7025,
      "step": 772800
    },
    {
      "epoch": 7.052522081903788,
      "grad_norm": 4.37125301361084,
      "learning_rate": 4.412289826508018e-05,
      "loss": 0.7032,
      "step": 772900
    },
    {
      "epoch": 7.053434557266954,
      "grad_norm": 4.168264389038086,
      "learning_rate": 4.412213786894421e-05,
      "loss": 0.677,
      "step": 773000
    },
    {
      "epoch": 7.054347032630119,
      "grad_norm": 4.3244099617004395,
      "learning_rate": 4.412137747280823e-05,
      "loss": 0.6685,
      "step": 773100
    },
    {
      "epoch": 7.055259507993284,
      "grad_norm": 4.175966739654541,
      "learning_rate": 4.412061707667227e-05,
      "loss": 0.6743,
      "step": 773200
    },
    {
      "epoch": 7.05617198335645,
      "grad_norm": 4.697596073150635,
      "learning_rate": 4.411985668053629e-05,
      "loss": 0.7219,
      "step": 773300
    },
    {
      "epoch": 7.057084458719615,
      "grad_norm": 4.63954496383667,
      "learning_rate": 4.411909628440032e-05,
      "loss": 0.6925,
      "step": 773400
    },
    {
      "epoch": 7.05799693408278,
      "grad_norm": 4.5432868003845215,
      "learning_rate": 4.411833588826435e-05,
      "loss": 0.7012,
      "step": 773500
    },
    {
      "epoch": 7.058909409445945,
      "grad_norm": 3.8770925998687744,
      "learning_rate": 4.411757549212838e-05,
      "loss": 0.7378,
      "step": 773600
    },
    {
      "epoch": 7.05982188480911,
      "grad_norm": 4.31780481338501,
      "learning_rate": 4.4116815095992406e-05,
      "loss": 0.7115,
      "step": 773700
    },
    {
      "epoch": 7.060734360172275,
      "grad_norm": 4.37352991104126,
      "learning_rate": 4.411605469985644e-05,
      "loss": 0.6711,
      "step": 773800
    },
    {
      "epoch": 7.0616468355354405,
      "grad_norm": 3.513216972351074,
      "learning_rate": 4.4115294303720466e-05,
      "loss": 0.6882,
      "step": 773900
    },
    {
      "epoch": 7.062559310898606,
      "grad_norm": 4.421019554138184,
      "learning_rate": 4.4114533907584496e-05,
      "loss": 0.7045,
      "step": 774000
    },
    {
      "epoch": 7.063471786261771,
      "grad_norm": 4.778985977172852,
      "learning_rate": 4.4113773511448526e-05,
      "loss": 0.707,
      "step": 774100
    },
    {
      "epoch": 7.064384261624936,
      "grad_norm": 3.177471160888672,
      "learning_rate": 4.4113013115312556e-05,
      "loss": 0.6838,
      "step": 774200
    },
    {
      "epoch": 7.065296736988102,
      "grad_norm": 2.9858851432800293,
      "learning_rate": 4.4112252719176586e-05,
      "loss": 0.6792,
      "step": 774300
    },
    {
      "epoch": 7.066209212351266,
      "grad_norm": 4.231459140777588,
      "learning_rate": 4.4111492323040617e-05,
      "loss": 0.7009,
      "step": 774400
    },
    {
      "epoch": 7.067121687714431,
      "grad_norm": 5.2311882972717285,
      "learning_rate": 4.411073192690464e-05,
      "loss": 0.6861,
      "step": 774500
    },
    {
      "epoch": 7.068034163077597,
      "grad_norm": 3.4155473709106445,
      "learning_rate": 4.410997153076868e-05,
      "loss": 0.7209,
      "step": 774600
    },
    {
      "epoch": 7.068946638440762,
      "grad_norm": 3.7196755409240723,
      "learning_rate": 4.41092111346327e-05,
      "loss": 0.7421,
      "step": 774700
    },
    {
      "epoch": 7.069859113803927,
      "grad_norm": 3.7938413619995117,
      "learning_rate": 4.410845073849673e-05,
      "loss": 0.7061,
      "step": 774800
    },
    {
      "epoch": 7.070771589167093,
      "grad_norm": 4.099930286407471,
      "learning_rate": 4.410769034236076e-05,
      "loss": 0.6889,
      "step": 774900
    },
    {
      "epoch": 7.071684064530258,
      "grad_norm": 3.3296778202056885,
      "learning_rate": 4.410692994622479e-05,
      "loss": 0.6761,
      "step": 775000
    },
    {
      "epoch": 7.072596539893423,
      "grad_norm": 3.1117072105407715,
      "learning_rate": 4.4106169550088813e-05,
      "loss": 0.7063,
      "step": 775100
    },
    {
      "epoch": 7.0735090152565885,
      "grad_norm": 4.352723598480225,
      "learning_rate": 4.410540915395285e-05,
      "loss": 0.6962,
      "step": 775200
    },
    {
      "epoch": 7.074421490619753,
      "grad_norm": 4.5133957862854,
      "learning_rate": 4.4104648757816874e-05,
      "loss": 0.6772,
      "step": 775300
    },
    {
      "epoch": 7.075333965982918,
      "grad_norm": 3.3202426433563232,
      "learning_rate": 4.4103888361680904e-05,
      "loss": 0.6732,
      "step": 775400
    },
    {
      "epoch": 7.0762464413460835,
      "grad_norm": 4.378335952758789,
      "learning_rate": 4.4103127965544934e-05,
      "loss": 0.6888,
      "step": 775500
    },
    {
      "epoch": 7.077158916709249,
      "grad_norm": 4.3316755294799805,
      "learning_rate": 4.410236756940896e-05,
      "loss": 0.7258,
      "step": 775600
    },
    {
      "epoch": 7.078071392072414,
      "grad_norm": 4.385164260864258,
      "learning_rate": 4.4101607173272994e-05,
      "loss": 0.7145,
      "step": 775700
    },
    {
      "epoch": 7.078983867435579,
      "grad_norm": 4.469864368438721,
      "learning_rate": 4.410084677713702e-05,
      "loss": 0.7129,
      "step": 775800
    },
    {
      "epoch": 7.079896342798745,
      "grad_norm": 3.3645570278167725,
      "learning_rate": 4.410008638100105e-05,
      "loss": 0.7171,
      "step": 775900
    },
    {
      "epoch": 7.08080881816191,
      "grad_norm": 4.704232215881348,
      "learning_rate": 4.409932598486508e-05,
      "loss": 0.72,
      "step": 776000
    },
    {
      "epoch": 7.081721293525074,
      "grad_norm": 4.124395370483398,
      "learning_rate": 4.409856558872911e-05,
      "loss": 0.7078,
      "step": 776100
    },
    {
      "epoch": 7.08263376888824,
      "grad_norm": 3.584989547729492,
      "learning_rate": 4.409780519259313e-05,
      "loss": 0.7071,
      "step": 776200
    },
    {
      "epoch": 7.083546244251405,
      "grad_norm": 4.7015700340271,
      "learning_rate": 4.409704479645717e-05,
      "loss": 0.7234,
      "step": 776300
    },
    {
      "epoch": 7.08445871961457,
      "grad_norm": 3.7001290321350098,
      "learning_rate": 4.409628440032119e-05,
      "loss": 0.6852,
      "step": 776400
    },
    {
      "epoch": 7.085371194977736,
      "grad_norm": 3.7896039485931396,
      "learning_rate": 4.409552400418522e-05,
      "loss": 0.7157,
      "step": 776500
    },
    {
      "epoch": 7.086283670340901,
      "grad_norm": 4.471704483032227,
      "learning_rate": 4.409476360804925e-05,
      "loss": 0.6956,
      "step": 776600
    },
    {
      "epoch": 7.087196145704066,
      "grad_norm": 3.1246337890625,
      "learning_rate": 4.409400321191328e-05,
      "loss": 0.7038,
      "step": 776700
    },
    {
      "epoch": 7.0881086210672315,
      "grad_norm": 4.0954790115356445,
      "learning_rate": 4.409324281577731e-05,
      "loss": 0.7069,
      "step": 776800
    },
    {
      "epoch": 7.089021096430397,
      "grad_norm": 3.0751028060913086,
      "learning_rate": 4.409248241964134e-05,
      "loss": 0.6995,
      "step": 776900
    },
    {
      "epoch": 7.089933571793561,
      "grad_norm": 4.7882490158081055,
      "learning_rate": 4.4091722023505364e-05,
      "loss": 0.6806,
      "step": 777000
    },
    {
      "epoch": 7.0908460471567265,
      "grad_norm": 4.0457916259765625,
      "learning_rate": 4.40909616273694e-05,
      "loss": 0.6963,
      "step": 777100
    },
    {
      "epoch": 7.091758522519892,
      "grad_norm": 4.120274543762207,
      "learning_rate": 4.4090201231233425e-05,
      "loss": 0.6954,
      "step": 777200
    },
    {
      "epoch": 7.092670997883057,
      "grad_norm": 3.835704803466797,
      "learning_rate": 4.4089440835097455e-05,
      "loss": 0.6703,
      "step": 777300
    },
    {
      "epoch": 7.093583473246222,
      "grad_norm": 3.64601993560791,
      "learning_rate": 4.4088680438961485e-05,
      "loss": 0.7167,
      "step": 777400
    },
    {
      "epoch": 7.094495948609388,
      "grad_norm": 3.7930238246917725,
      "learning_rate": 4.4087920042825515e-05,
      "loss": 0.7382,
      "step": 777500
    },
    {
      "epoch": 7.095408423972553,
      "grad_norm": 3.836552381515503,
      "learning_rate": 4.408715964668954e-05,
      "loss": 0.677,
      "step": 777600
    },
    {
      "epoch": 7.096320899335718,
      "grad_norm": 3.7270028591156006,
      "learning_rate": 4.4086399250553575e-05,
      "loss": 0.6716,
      "step": 777700
    },
    {
      "epoch": 7.097233374698883,
      "grad_norm": 3.857178211212158,
      "learning_rate": 4.40856388544176e-05,
      "loss": 0.7059,
      "step": 777800
    },
    {
      "epoch": 7.098145850062048,
      "grad_norm": 4.10733699798584,
      "learning_rate": 4.408487845828163e-05,
      "loss": 0.6954,
      "step": 777900
    },
    {
      "epoch": 7.099058325425213,
      "grad_norm": 3.9670028686523438,
      "learning_rate": 4.408411806214566e-05,
      "loss": 0.6852,
      "step": 778000
    },
    {
      "epoch": 7.099970800788379,
      "grad_norm": 4.340911388397217,
      "learning_rate": 4.408335766600969e-05,
      "loss": 0.6972,
      "step": 778100
    },
    {
      "epoch": 7.100883276151544,
      "grad_norm": 4.4387311935424805,
      "learning_rate": 4.408259726987372e-05,
      "loss": 0.716,
      "step": 778200
    },
    {
      "epoch": 7.101795751514709,
      "grad_norm": 4.1552886962890625,
      "learning_rate": 4.408183687373774e-05,
      "loss": 0.6842,
      "step": 778300
    },
    {
      "epoch": 7.1027082268778745,
      "grad_norm": 3.9086766242980957,
      "learning_rate": 4.408107647760177e-05,
      "loss": 0.6858,
      "step": 778400
    },
    {
      "epoch": 7.10362070224104,
      "grad_norm": 4.627938747406006,
      "learning_rate": 4.40803160814658e-05,
      "loss": 0.6986,
      "step": 778500
    },
    {
      "epoch": 7.104533177604205,
      "grad_norm": 4.518060684204102,
      "learning_rate": 4.407955568532983e-05,
      "loss": 0.6727,
      "step": 778600
    },
    {
      "epoch": 7.1054456529673695,
      "grad_norm": 3.028123378753662,
      "learning_rate": 4.4078795289193855e-05,
      "loss": 0.6727,
      "step": 778700
    },
    {
      "epoch": 7.106358128330535,
      "grad_norm": 4.486236572265625,
      "learning_rate": 4.407803489305789e-05,
      "loss": 0.6961,
      "step": 778800
    },
    {
      "epoch": 7.1072706036937,
      "grad_norm": 3.4053220748901367,
      "learning_rate": 4.4077274496921915e-05,
      "loss": 0.6776,
      "step": 778900
    },
    {
      "epoch": 7.108183079056865,
      "grad_norm": 4.665427207946777,
      "learning_rate": 4.4076514100785945e-05,
      "loss": 0.7053,
      "step": 779000
    },
    {
      "epoch": 7.109095554420031,
      "grad_norm": 3.8778109550476074,
      "learning_rate": 4.4075753704649975e-05,
      "loss": 0.6831,
      "step": 779100
    },
    {
      "epoch": 7.110008029783196,
      "grad_norm": 4.285707473754883,
      "learning_rate": 4.4074993308514006e-05,
      "loss": 0.7205,
      "step": 779200
    },
    {
      "epoch": 7.110920505146361,
      "grad_norm": 4.650745391845703,
      "learning_rate": 4.4074232912378036e-05,
      "loss": 0.7179,
      "step": 779300
    },
    {
      "epoch": 7.111832980509527,
      "grad_norm": 4.143398284912109,
      "learning_rate": 4.4073472516242066e-05,
      "loss": 0.7127,
      "step": 779400
    },
    {
      "epoch": 7.112745455872691,
      "grad_norm": 3.359959363937378,
      "learning_rate": 4.407271212010609e-05,
      "loss": 0.6804,
      "step": 779500
    },
    {
      "epoch": 7.113657931235856,
      "grad_norm": 4.413231372833252,
      "learning_rate": 4.4071951723970126e-05,
      "loss": 0.6621,
      "step": 779600
    },
    {
      "epoch": 7.114570406599022,
      "grad_norm": 4.2279767990112305,
      "learning_rate": 4.407119132783415e-05,
      "loss": 0.7031,
      "step": 779700
    },
    {
      "epoch": 7.115482881962187,
      "grad_norm": 4.666309833526611,
      "learning_rate": 4.407043093169818e-05,
      "loss": 0.7268,
      "step": 779800
    },
    {
      "epoch": 7.116395357325352,
      "grad_norm": 3.8327720165252686,
      "learning_rate": 4.406967053556221e-05,
      "loss": 0.7072,
      "step": 779900
    },
    {
      "epoch": 7.1173078326885175,
      "grad_norm": 4.043359279632568,
      "learning_rate": 4.406891013942624e-05,
      "loss": 0.7049,
      "step": 780000
    },
    {
      "epoch": 7.118220308051683,
      "grad_norm": 3.0163071155548096,
      "learning_rate": 4.406814974329026e-05,
      "loss": 0.682,
      "step": 780100
    },
    {
      "epoch": 7.119132783414848,
      "grad_norm": 4.300875186920166,
      "learning_rate": 4.40673893471543e-05,
      "loss": 0.6951,
      "step": 780200
    },
    {
      "epoch": 7.120045258778013,
      "grad_norm": 4.390305042266846,
      "learning_rate": 4.406662895101832e-05,
      "loss": 0.6994,
      "step": 780300
    },
    {
      "epoch": 7.120957734141178,
      "grad_norm": 3.4746928215026855,
      "learning_rate": 4.406586855488235e-05,
      "loss": 0.7012,
      "step": 780400
    },
    {
      "epoch": 7.121870209504343,
      "grad_norm": 3.878338575363159,
      "learning_rate": 4.406510815874638e-05,
      "loss": 0.718,
      "step": 780500
    },
    {
      "epoch": 7.122782684867508,
      "grad_norm": 3.185511589050293,
      "learning_rate": 4.406434776261041e-05,
      "loss": 0.7005,
      "step": 780600
    },
    {
      "epoch": 7.123695160230674,
      "grad_norm": 6.239773273468018,
      "learning_rate": 4.406358736647444e-05,
      "loss": 0.6971,
      "step": 780700
    },
    {
      "epoch": 7.124607635593839,
      "grad_norm": 3.3896846771240234,
      "learning_rate": 4.406282697033847e-05,
      "loss": 0.6574,
      "step": 780800
    },
    {
      "epoch": 7.125520110957004,
      "grad_norm": 4.404430866241455,
      "learning_rate": 4.4062066574202496e-05,
      "loss": 0.6962,
      "step": 780900
    },
    {
      "epoch": 7.12643258632017,
      "grad_norm": 3.457149028778076,
      "learning_rate": 4.406130617806653e-05,
      "loss": 0.7233,
      "step": 781000
    },
    {
      "epoch": 7.127345061683335,
      "grad_norm": 4.359443187713623,
      "learning_rate": 4.4060545781930556e-05,
      "loss": 0.6723,
      "step": 781100
    },
    {
      "epoch": 7.128257537046499,
      "grad_norm": 4.025453090667725,
      "learning_rate": 4.405978538579458e-05,
      "loss": 0.6993,
      "step": 781200
    },
    {
      "epoch": 7.129170012409665,
      "grad_norm": 3.9268343448638916,
      "learning_rate": 4.405902498965862e-05,
      "loss": 0.7144,
      "step": 781300
    },
    {
      "epoch": 7.13008248777283,
      "grad_norm": 3.9238245487213135,
      "learning_rate": 4.405826459352264e-05,
      "loss": 0.6896,
      "step": 781400
    },
    {
      "epoch": 7.130994963135995,
      "grad_norm": 3.597262144088745,
      "learning_rate": 4.405750419738667e-05,
      "loss": 0.6876,
      "step": 781500
    },
    {
      "epoch": 7.1319074384991605,
      "grad_norm": 2.877572536468506,
      "learning_rate": 4.40567438012507e-05,
      "loss": 0.7093,
      "step": 781600
    },
    {
      "epoch": 7.132819913862326,
      "grad_norm": 3.581268548965454,
      "learning_rate": 4.405598340511473e-05,
      "loss": 0.7095,
      "step": 781700
    },
    {
      "epoch": 7.133732389225491,
      "grad_norm": 4.0464887619018555,
      "learning_rate": 4.405522300897876e-05,
      "loss": 0.7348,
      "step": 781800
    },
    {
      "epoch": 7.134644864588656,
      "grad_norm": 3.8381459712982178,
      "learning_rate": 4.405446261284279e-05,
      "loss": 0.703,
      "step": 781900
    },
    {
      "epoch": 7.135557339951822,
      "grad_norm": 4.060784339904785,
      "learning_rate": 4.4053702216706814e-05,
      "loss": 0.7062,
      "step": 782000
    },
    {
      "epoch": 7.136469815314986,
      "grad_norm": 4.143705368041992,
      "learning_rate": 4.405294182057085e-05,
      "loss": 0.7156,
      "step": 782100
    },
    {
      "epoch": 7.137382290678151,
      "grad_norm": 4.465351104736328,
      "learning_rate": 4.4052181424434874e-05,
      "loss": 0.7192,
      "step": 782200
    },
    {
      "epoch": 7.138294766041317,
      "grad_norm": 3.36891508102417,
      "learning_rate": 4.4051421028298904e-05,
      "loss": 0.6972,
      "step": 782300
    },
    {
      "epoch": 7.139207241404482,
      "grad_norm": 4.356187343597412,
      "learning_rate": 4.4050660632162934e-05,
      "loss": 0.6777,
      "step": 782400
    },
    {
      "epoch": 7.140119716767647,
      "grad_norm": 4.02525520324707,
      "learning_rate": 4.4049900236026964e-05,
      "loss": 0.7034,
      "step": 782500
    },
    {
      "epoch": 7.141032192130813,
      "grad_norm": 3.415879726409912,
      "learning_rate": 4.404913983989099e-05,
      "loss": 0.7079,
      "step": 782600
    },
    {
      "epoch": 7.141944667493978,
      "grad_norm": 3.701392412185669,
      "learning_rate": 4.4048379443755024e-05,
      "loss": 0.6754,
      "step": 782700
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 4.267332553863525,
      "learning_rate": 4.404761904761905e-05,
      "loss": 0.6901,
      "step": 782800
    },
    {
      "epoch": 7.143769618220308,
      "grad_norm": 4.617811679840088,
      "learning_rate": 4.404685865148308e-05,
      "loss": 0.7222,
      "step": 782900
    },
    {
      "epoch": 7.144682093583473,
      "grad_norm": 3.6442606449127197,
      "learning_rate": 4.404609825534711e-05,
      "loss": 0.7399,
      "step": 783000
    },
    {
      "epoch": 7.145594568946638,
      "grad_norm": 4.219976425170898,
      "learning_rate": 4.404533785921114e-05,
      "loss": 0.7055,
      "step": 783100
    },
    {
      "epoch": 7.1465070443098035,
      "grad_norm": 3.261258125305176,
      "learning_rate": 4.404457746307517e-05,
      "loss": 0.6618,
      "step": 783200
    },
    {
      "epoch": 7.147419519672969,
      "grad_norm": 2.031641960144043,
      "learning_rate": 4.40438170669392e-05,
      "loss": 0.7041,
      "step": 783300
    },
    {
      "epoch": 7.148331995036134,
      "grad_norm": 3.5924694538116455,
      "learning_rate": 4.404305667080322e-05,
      "loss": 0.705,
      "step": 783400
    },
    {
      "epoch": 7.1492444703992994,
      "grad_norm": 3.892404556274414,
      "learning_rate": 4.404229627466726e-05,
      "loss": 0.6805,
      "step": 783500
    },
    {
      "epoch": 7.150156945762465,
      "grad_norm": 3.868241548538208,
      "learning_rate": 4.404153587853128e-05,
      "loss": 0.7079,
      "step": 783600
    },
    {
      "epoch": 7.15106942112563,
      "grad_norm": 6.169383525848389,
      "learning_rate": 4.404077548239531e-05,
      "loss": 0.7111,
      "step": 783700
    },
    {
      "epoch": 7.1519818964887945,
      "grad_norm": 3.9259891510009766,
      "learning_rate": 4.404001508625934e-05,
      "loss": 0.7365,
      "step": 783800
    },
    {
      "epoch": 7.15289437185196,
      "grad_norm": 3.673689126968384,
      "learning_rate": 4.4039254690123364e-05,
      "loss": 0.6853,
      "step": 783900
    },
    {
      "epoch": 7.153806847215125,
      "grad_norm": 3.887834072113037,
      "learning_rate": 4.4038494293987395e-05,
      "loss": 0.7054,
      "step": 784000
    },
    {
      "epoch": 7.15471932257829,
      "grad_norm": 4.456930160522461,
      "learning_rate": 4.4037733897851425e-05,
      "loss": 0.6987,
      "step": 784100
    },
    {
      "epoch": 7.155631797941456,
      "grad_norm": 3.2096781730651855,
      "learning_rate": 4.4036973501715455e-05,
      "loss": 0.6971,
      "step": 784200
    },
    {
      "epoch": 7.156544273304621,
      "grad_norm": 4.447254657745361,
      "learning_rate": 4.4036213105579485e-05,
      "loss": 0.6972,
      "step": 784300
    },
    {
      "epoch": 7.157456748667786,
      "grad_norm": 4.687038898468018,
      "learning_rate": 4.4035452709443515e-05,
      "loss": 0.6921,
      "step": 784400
    },
    {
      "epoch": 7.1583692240309515,
      "grad_norm": 3.6996545791625977,
      "learning_rate": 4.403469231330754e-05,
      "loss": 0.6816,
      "step": 784500
    },
    {
      "epoch": 7.159281699394116,
      "grad_norm": 4.551315784454346,
      "learning_rate": 4.4033931917171575e-05,
      "loss": 0.7238,
      "step": 784600
    },
    {
      "epoch": 7.160194174757281,
      "grad_norm": 5.23851203918457,
      "learning_rate": 4.40331715210356e-05,
      "loss": 0.7295,
      "step": 784700
    },
    {
      "epoch": 7.161106650120447,
      "grad_norm": 4.350287437438965,
      "learning_rate": 4.403241112489963e-05,
      "loss": 0.6615,
      "step": 784800
    },
    {
      "epoch": 7.162019125483612,
      "grad_norm": 4.022897243499756,
      "learning_rate": 4.403165072876366e-05,
      "loss": 0.7264,
      "step": 784900
    },
    {
      "epoch": 7.162931600846777,
      "grad_norm": 4.230446815490723,
      "learning_rate": 4.403089033262769e-05,
      "loss": 0.7107,
      "step": 785000
    },
    {
      "epoch": 7.1638440762099425,
      "grad_norm": 4.445788860321045,
      "learning_rate": 4.403012993649172e-05,
      "loss": 0.6682,
      "step": 785100
    },
    {
      "epoch": 7.164756551573108,
      "grad_norm": 4.296337604522705,
      "learning_rate": 4.402936954035575e-05,
      "loss": 0.7032,
      "step": 785200
    },
    {
      "epoch": 7.165669026936273,
      "grad_norm": 4.191080570220947,
      "learning_rate": 4.402860914421977e-05,
      "loss": 0.6987,
      "step": 785300
    },
    {
      "epoch": 7.166581502299438,
      "grad_norm": 3.7397360801696777,
      "learning_rate": 4.40278487480838e-05,
      "loss": 0.6832,
      "step": 785400
    },
    {
      "epoch": 7.167493977662603,
      "grad_norm": 2.68438720703125,
      "learning_rate": 4.402708835194783e-05,
      "loss": 0.7022,
      "step": 785500
    },
    {
      "epoch": 7.168406453025768,
      "grad_norm": 2.2503299713134766,
      "learning_rate": 4.402632795581186e-05,
      "loss": 0.6665,
      "step": 785600
    },
    {
      "epoch": 7.169318928388933,
      "grad_norm": 3.7389490604400635,
      "learning_rate": 4.402556755967589e-05,
      "loss": 0.7187,
      "step": 785700
    },
    {
      "epoch": 7.170231403752099,
      "grad_norm": 4.339995861053467,
      "learning_rate": 4.402480716353992e-05,
      "loss": 0.7127,
      "step": 785800
    },
    {
      "epoch": 7.171143879115264,
      "grad_norm": 4.4081950187683105,
      "learning_rate": 4.4024046767403946e-05,
      "loss": 0.6618,
      "step": 785900
    },
    {
      "epoch": 7.172056354478429,
      "grad_norm": 7.339504241943359,
      "learning_rate": 4.402328637126798e-05,
      "loss": 0.7154,
      "step": 786000
    },
    {
      "epoch": 7.172968829841595,
      "grad_norm": 3.4693443775177,
      "learning_rate": 4.4022525975132006e-05,
      "loss": 0.6939,
      "step": 786100
    },
    {
      "epoch": 7.17388130520476,
      "grad_norm": 3.6940183639526367,
      "learning_rate": 4.4021765578996036e-05,
      "loss": 0.6858,
      "step": 786200
    },
    {
      "epoch": 7.174793780567924,
      "grad_norm": 3.785064935684204,
      "learning_rate": 4.4021005182860066e-05,
      "loss": 0.6756,
      "step": 786300
    },
    {
      "epoch": 7.17570625593109,
      "grad_norm": 3.8336689472198486,
      "learning_rate": 4.4020244786724096e-05,
      "loss": 0.695,
      "step": 786400
    },
    {
      "epoch": 7.176618731294255,
      "grad_norm": 4.6137919425964355,
      "learning_rate": 4.4019484390588126e-05,
      "loss": 0.7173,
      "step": 786500
    },
    {
      "epoch": 7.17753120665742,
      "grad_norm": 4.432587623596191,
      "learning_rate": 4.4018723994452156e-05,
      "loss": 0.7224,
      "step": 786600
    },
    {
      "epoch": 7.1784436820205855,
      "grad_norm": 4.083642959594727,
      "learning_rate": 4.401796359831618e-05,
      "loss": 0.6698,
      "step": 786700
    },
    {
      "epoch": 7.179356157383751,
      "grad_norm": 3.9152376651763916,
      "learning_rate": 4.401720320218021e-05,
      "loss": 0.6939,
      "step": 786800
    },
    {
      "epoch": 7.180268632746916,
      "grad_norm": 3.68656063079834,
      "learning_rate": 4.401644280604424e-05,
      "loss": 0.701,
      "step": 786900
    },
    {
      "epoch": 7.181181108110081,
      "grad_norm": 4.399816036224365,
      "learning_rate": 4.401568240990826e-05,
      "loss": 0.7086,
      "step": 787000
    },
    {
      "epoch": 7.182093583473247,
      "grad_norm": 3.1940324306488037,
      "learning_rate": 4.40149220137723e-05,
      "loss": 0.7202,
      "step": 787100
    },
    {
      "epoch": 7.183006058836411,
      "grad_norm": 4.242788791656494,
      "learning_rate": 4.401416161763632e-05,
      "loss": 0.716,
      "step": 787200
    },
    {
      "epoch": 7.183918534199576,
      "grad_norm": 5.326297283172607,
      "learning_rate": 4.401340122150035e-05,
      "loss": 0.7165,
      "step": 787300
    },
    {
      "epoch": 7.184831009562742,
      "grad_norm": 4.142185688018799,
      "learning_rate": 4.401264082536438e-05,
      "loss": 0.6905,
      "step": 787400
    },
    {
      "epoch": 7.185743484925907,
      "grad_norm": 4.7417683601379395,
      "learning_rate": 4.401188042922841e-05,
      "loss": 0.6838,
      "step": 787500
    },
    {
      "epoch": 7.186655960289072,
      "grad_norm": 4.144077301025391,
      "learning_rate": 4.401112003309244e-05,
      "loss": 0.7086,
      "step": 787600
    },
    {
      "epoch": 7.187568435652238,
      "grad_norm": 3.742180585861206,
      "learning_rate": 4.401035963695647e-05,
      "loss": 0.6739,
      "step": 787700
    },
    {
      "epoch": 7.188480911015403,
      "grad_norm": 5.190042495727539,
      "learning_rate": 4.4009599240820496e-05,
      "loss": 0.719,
      "step": 787800
    },
    {
      "epoch": 7.189393386378568,
      "grad_norm": 4.519218444824219,
      "learning_rate": 4.400883884468453e-05,
      "loss": 0.7055,
      "step": 787900
    },
    {
      "epoch": 7.190305861741733,
      "grad_norm": 3.654409408569336,
      "learning_rate": 4.4008078448548557e-05,
      "loss": 0.6656,
      "step": 788000
    },
    {
      "epoch": 7.191218337104898,
      "grad_norm": 4.215460777282715,
      "learning_rate": 4.400731805241259e-05,
      "loss": 0.7151,
      "step": 788100
    },
    {
      "epoch": 7.192130812468063,
      "grad_norm": 3.5147616863250732,
      "learning_rate": 4.400655765627662e-05,
      "loss": 0.6869,
      "step": 788200
    },
    {
      "epoch": 7.1930432878312285,
      "grad_norm": 4.534344673156738,
      "learning_rate": 4.400579726014065e-05,
      "loss": 0.7105,
      "step": 788300
    },
    {
      "epoch": 7.193955763194394,
      "grad_norm": 5.463032245635986,
      "learning_rate": 4.400503686400467e-05,
      "loss": 0.7118,
      "step": 788400
    },
    {
      "epoch": 7.194868238557559,
      "grad_norm": 4.187001705169678,
      "learning_rate": 4.400427646786871e-05,
      "loss": 0.6874,
      "step": 788500
    },
    {
      "epoch": 7.195780713920724,
      "grad_norm": 3.3070220947265625,
      "learning_rate": 4.400351607173273e-05,
      "loss": 0.6502,
      "step": 788600
    },
    {
      "epoch": 7.19669318928389,
      "grad_norm": 3.6522090435028076,
      "learning_rate": 4.400275567559676e-05,
      "loss": 0.6981,
      "step": 788700
    },
    {
      "epoch": 7.197605664647055,
      "grad_norm": 6.017216205596924,
      "learning_rate": 4.400199527946079e-05,
      "loss": 0.7312,
      "step": 788800
    },
    {
      "epoch": 7.198518140010219,
      "grad_norm": 2.8778305053710938,
      "learning_rate": 4.400123488332482e-05,
      "loss": 0.6864,
      "step": 788900
    },
    {
      "epoch": 7.199430615373385,
      "grad_norm": 4.753449440002441,
      "learning_rate": 4.400047448718885e-05,
      "loss": 0.7065,
      "step": 789000
    },
    {
      "epoch": 7.20034309073655,
      "grad_norm": 3.5276591777801514,
      "learning_rate": 4.399971409105288e-05,
      "loss": 0.6886,
      "step": 789100
    },
    {
      "epoch": 7.201255566099715,
      "grad_norm": 4.075018405914307,
      "learning_rate": 4.3998953694916904e-05,
      "loss": 0.6907,
      "step": 789200
    },
    {
      "epoch": 7.202168041462881,
      "grad_norm": 4.363877296447754,
      "learning_rate": 4.399819329878094e-05,
      "loss": 0.7437,
      "step": 789300
    },
    {
      "epoch": 7.203080516826046,
      "grad_norm": 3.8531534671783447,
      "learning_rate": 4.3997432902644964e-05,
      "loss": 0.6903,
      "step": 789400
    },
    {
      "epoch": 7.203992992189211,
      "grad_norm": 3.6541733741760254,
      "learning_rate": 4.3996672506508994e-05,
      "loss": 0.7119,
      "step": 789500
    },
    {
      "epoch": 7.2049054675523765,
      "grad_norm": 4.211763381958008,
      "learning_rate": 4.3995912110373024e-05,
      "loss": 0.6431,
      "step": 789600
    },
    {
      "epoch": 7.205817942915541,
      "grad_norm": 2.786571979522705,
      "learning_rate": 4.399515171423705e-05,
      "loss": 0.6615,
      "step": 789700
    },
    {
      "epoch": 7.206730418278706,
      "grad_norm": 5.102870941162109,
      "learning_rate": 4.399439131810108e-05,
      "loss": 0.7191,
      "step": 789800
    },
    {
      "epoch": 7.2076428936418715,
      "grad_norm": 4.136616230010986,
      "learning_rate": 4.399363092196511e-05,
      "loss": 0.7105,
      "step": 789900
    },
    {
      "epoch": 7.208555369005037,
      "grad_norm": 4.2045722007751465,
      "learning_rate": 4.399287052582914e-05,
      "loss": 0.7469,
      "step": 790000
    },
    {
      "epoch": 7.209467844368202,
      "grad_norm": 3.7943735122680664,
      "learning_rate": 4.399211012969317e-05,
      "loss": 0.6883,
      "step": 790100
    },
    {
      "epoch": 7.210380319731367,
      "grad_norm": 4.271440029144287,
      "learning_rate": 4.39913497335572e-05,
      "loss": 0.7206,
      "step": 790200
    },
    {
      "epoch": 7.211292795094533,
      "grad_norm": 1.492713212966919,
      "learning_rate": 4.399058933742122e-05,
      "loss": 0.6676,
      "step": 790300
    },
    {
      "epoch": 7.212205270457698,
      "grad_norm": 3.6480681896209717,
      "learning_rate": 4.398982894128526e-05,
      "loss": 0.6601,
      "step": 790400
    },
    {
      "epoch": 7.213117745820863,
      "grad_norm": 4.574777603149414,
      "learning_rate": 4.398906854514928e-05,
      "loss": 0.669,
      "step": 790500
    },
    {
      "epoch": 7.214030221184028,
      "grad_norm": 3.6681923866271973,
      "learning_rate": 4.398830814901331e-05,
      "loss": 0.6792,
      "step": 790600
    },
    {
      "epoch": 7.214942696547193,
      "grad_norm": 3.4911065101623535,
      "learning_rate": 4.398754775287734e-05,
      "loss": 0.7046,
      "step": 790700
    },
    {
      "epoch": 7.215855171910358,
      "grad_norm": 3.8792388439178467,
      "learning_rate": 4.398678735674137e-05,
      "loss": 0.7021,
      "step": 790800
    },
    {
      "epoch": 7.216767647273524,
      "grad_norm": 8.869109153747559,
      "learning_rate": 4.3986026960605395e-05,
      "loss": 0.6781,
      "step": 790900
    },
    {
      "epoch": 7.217680122636689,
      "grad_norm": 4.009599208831787,
      "learning_rate": 4.398526656446943e-05,
      "loss": 0.6841,
      "step": 791000
    },
    {
      "epoch": 7.218592597999854,
      "grad_norm": 4.213083267211914,
      "learning_rate": 4.3984506168333455e-05,
      "loss": 0.6522,
      "step": 791100
    },
    {
      "epoch": 7.2195050733630195,
      "grad_norm": 4.133686065673828,
      "learning_rate": 4.3983745772197485e-05,
      "loss": 0.7118,
      "step": 791200
    },
    {
      "epoch": 7.220417548726185,
      "grad_norm": 4.441986560821533,
      "learning_rate": 4.3982985376061515e-05,
      "loss": 0.669,
      "step": 791300
    },
    {
      "epoch": 7.221330024089349,
      "grad_norm": 4.694604396820068,
      "learning_rate": 4.3982224979925545e-05,
      "loss": 0.7182,
      "step": 791400
    },
    {
      "epoch": 7.2222424994525145,
      "grad_norm": 3.6673660278320312,
      "learning_rate": 4.3981464583789575e-05,
      "loss": 0.661,
      "step": 791500
    },
    {
      "epoch": 7.22315497481568,
      "grad_norm": 4.090642929077148,
      "learning_rate": 4.3980704187653605e-05,
      "loss": 0.6741,
      "step": 791600
    },
    {
      "epoch": 7.224067450178845,
      "grad_norm": 4.163355827331543,
      "learning_rate": 4.397994379151763e-05,
      "loss": 0.7596,
      "step": 791700
    },
    {
      "epoch": 7.22497992554201,
      "grad_norm": 3.397186279296875,
      "learning_rate": 4.3979183395381665e-05,
      "loss": 0.6809,
      "step": 791800
    },
    {
      "epoch": 7.225892400905176,
      "grad_norm": 3.676542282104492,
      "learning_rate": 4.397842299924569e-05,
      "loss": 0.7446,
      "step": 791900
    },
    {
      "epoch": 7.226804876268341,
      "grad_norm": 4.11844539642334,
      "learning_rate": 4.397766260310972e-05,
      "loss": 0.6923,
      "step": 792000
    },
    {
      "epoch": 7.227717351631506,
      "grad_norm": 3.9368488788604736,
      "learning_rate": 4.397690220697375e-05,
      "loss": 0.6794,
      "step": 792100
    },
    {
      "epoch": 7.228629826994671,
      "grad_norm": 3.507559299468994,
      "learning_rate": 4.397614181083778e-05,
      "loss": 0.6746,
      "step": 792200
    },
    {
      "epoch": 7.229542302357836,
      "grad_norm": 4.128397464752197,
      "learning_rate": 4.39753814147018e-05,
      "loss": 0.6655,
      "step": 792300
    },
    {
      "epoch": 7.230454777721001,
      "grad_norm": 3.694988965988159,
      "learning_rate": 4.397462101856584e-05,
      "loss": 0.7058,
      "step": 792400
    },
    {
      "epoch": 7.231367253084167,
      "grad_norm": 4.406230926513672,
      "learning_rate": 4.397386062242986e-05,
      "loss": 0.7144,
      "step": 792500
    },
    {
      "epoch": 7.232279728447332,
      "grad_norm": 4.178304672241211,
      "learning_rate": 4.397310022629389e-05,
      "loss": 0.7157,
      "step": 792600
    },
    {
      "epoch": 7.233192203810497,
      "grad_norm": 4.194164276123047,
      "learning_rate": 4.397233983015792e-05,
      "loss": 0.709,
      "step": 792700
    },
    {
      "epoch": 7.2341046791736625,
      "grad_norm": 3.9983949661254883,
      "learning_rate": 4.3971579434021946e-05,
      "loss": 0.6876,
      "step": 792800
    },
    {
      "epoch": 7.235017154536828,
      "grad_norm": 3.5883896350860596,
      "learning_rate": 4.397081903788598e-05,
      "loss": 0.7151,
      "step": 792900
    },
    {
      "epoch": 7.235929629899993,
      "grad_norm": 3.2340521812438965,
      "learning_rate": 4.3970058641750006e-05,
      "loss": 0.7013,
      "step": 793000
    },
    {
      "epoch": 7.2368421052631575,
      "grad_norm": 3.7482547760009766,
      "learning_rate": 4.3969298245614036e-05,
      "loss": 0.6944,
      "step": 793100
    },
    {
      "epoch": 7.237754580626323,
      "grad_norm": 3.91239595413208,
      "learning_rate": 4.3968537849478066e-05,
      "loss": 0.6821,
      "step": 793200
    },
    {
      "epoch": 7.238667055989488,
      "grad_norm": 3.048926591873169,
      "learning_rate": 4.3967777453342096e-05,
      "loss": 0.7039,
      "step": 793300
    },
    {
      "epoch": 7.239579531352653,
      "grad_norm": 4.603440761566162,
      "learning_rate": 4.396701705720612e-05,
      "loss": 0.676,
      "step": 793400
    },
    {
      "epoch": 7.240492006715819,
      "grad_norm": 3.904399871826172,
      "learning_rate": 4.3966256661070156e-05,
      "loss": 0.6917,
      "step": 793500
    },
    {
      "epoch": 7.241404482078984,
      "grad_norm": 3.7981605529785156,
      "learning_rate": 4.396549626493418e-05,
      "loss": 0.6846,
      "step": 793600
    },
    {
      "epoch": 7.242316957442149,
      "grad_norm": 3.3273465633392334,
      "learning_rate": 4.396473586879821e-05,
      "loss": 0.6741,
      "step": 793700
    },
    {
      "epoch": 7.243229432805315,
      "grad_norm": 3.833369493484497,
      "learning_rate": 4.396397547266224e-05,
      "loss": 0.7205,
      "step": 793800
    },
    {
      "epoch": 7.244141908168479,
      "grad_norm": 4.009827613830566,
      "learning_rate": 4.396321507652627e-05,
      "loss": 0.7058,
      "step": 793900
    },
    {
      "epoch": 7.245054383531644,
      "grad_norm": 4.112338066101074,
      "learning_rate": 4.39624546803903e-05,
      "loss": 0.7376,
      "step": 794000
    },
    {
      "epoch": 7.24596685889481,
      "grad_norm": 4.449377536773682,
      "learning_rate": 4.396169428425433e-05,
      "loss": 0.6988,
      "step": 794100
    },
    {
      "epoch": 7.246879334257975,
      "grad_norm": 4.137576580047607,
      "learning_rate": 4.396093388811835e-05,
      "loss": 0.709,
      "step": 794200
    },
    {
      "epoch": 7.24779180962114,
      "grad_norm": 4.206430912017822,
      "learning_rate": 4.396017349198239e-05,
      "loss": 0.7412,
      "step": 794300
    },
    {
      "epoch": 7.2487042849843055,
      "grad_norm": 3.6203839778900146,
      "learning_rate": 4.395941309584641e-05,
      "loss": 0.7028,
      "step": 794400
    },
    {
      "epoch": 7.249616760347471,
      "grad_norm": 4.5650434494018555,
      "learning_rate": 4.395865269971044e-05,
      "loss": 0.7028,
      "step": 794500
    },
    {
      "epoch": 7.250529235710636,
      "grad_norm": 4.354389190673828,
      "learning_rate": 4.395789230357447e-05,
      "loss": 0.6855,
      "step": 794600
    },
    {
      "epoch": 7.251441711073801,
      "grad_norm": 4.212982654571533,
      "learning_rate": 4.39571319074385e-05,
      "loss": 0.6946,
      "step": 794700
    },
    {
      "epoch": 7.252354186436966,
      "grad_norm": 3.815408229827881,
      "learning_rate": 4.395637151130253e-05,
      "loss": 0.7594,
      "step": 794800
    },
    {
      "epoch": 7.253266661800131,
      "grad_norm": 3.2473275661468506,
      "learning_rate": 4.3955611115166563e-05,
      "loss": 0.7108,
      "step": 794900
    },
    {
      "epoch": 7.254179137163296,
      "grad_norm": 3.5937154293060303,
      "learning_rate": 4.395485071903059e-05,
      "loss": 0.7082,
      "step": 795000
    },
    {
      "epoch": 7.255091612526462,
      "grad_norm": 4.004167079925537,
      "learning_rate": 4.395409032289462e-05,
      "loss": 0.6847,
      "step": 795100
    },
    {
      "epoch": 7.256004087889627,
      "grad_norm": 4.165943145751953,
      "learning_rate": 4.395332992675865e-05,
      "loss": 0.722,
      "step": 795200
    },
    {
      "epoch": 7.256916563252792,
      "grad_norm": 4.215304851531982,
      "learning_rate": 4.395256953062267e-05,
      "loss": 0.7218,
      "step": 795300
    },
    {
      "epoch": 7.257829038615958,
      "grad_norm": 3.794142246246338,
      "learning_rate": 4.395180913448671e-05,
      "loss": 0.6995,
      "step": 795400
    },
    {
      "epoch": 7.258741513979123,
      "grad_norm": 4.432075500488281,
      "learning_rate": 4.395104873835073e-05,
      "loss": 0.6883,
      "step": 795500
    },
    {
      "epoch": 7.259653989342288,
      "grad_norm": 3.0985536575317383,
      "learning_rate": 4.395028834221476e-05,
      "loss": 0.68,
      "step": 795600
    },
    {
      "epoch": 7.260566464705453,
      "grad_norm": 4.100605487823486,
      "learning_rate": 4.394952794607879e-05,
      "loss": 0.6959,
      "step": 795700
    },
    {
      "epoch": 7.261478940068618,
      "grad_norm": 3.6588876247406006,
      "learning_rate": 4.394876754994282e-05,
      "loss": 0.6945,
      "step": 795800
    },
    {
      "epoch": 7.262391415431783,
      "grad_norm": 2.914518117904663,
      "learning_rate": 4.3948007153806844e-05,
      "loss": 0.6931,
      "step": 795900
    },
    {
      "epoch": 7.2633038907949485,
      "grad_norm": 4.087057113647461,
      "learning_rate": 4.394724675767088e-05,
      "loss": 0.7126,
      "step": 796000
    },
    {
      "epoch": 7.264216366158114,
      "grad_norm": 4.350010395050049,
      "learning_rate": 4.3946486361534904e-05,
      "loss": 0.7157,
      "step": 796100
    },
    {
      "epoch": 7.265128841521279,
      "grad_norm": 5.0188374519348145,
      "learning_rate": 4.3945725965398934e-05,
      "loss": 0.6886,
      "step": 796200
    },
    {
      "epoch": 7.266041316884444,
      "grad_norm": 5.20460319519043,
      "learning_rate": 4.3944965569262964e-05,
      "loss": 0.712,
      "step": 796300
    },
    {
      "epoch": 7.266953792247609,
      "grad_norm": 4.267230033874512,
      "learning_rate": 4.3944205173126994e-05,
      "loss": 0.6644,
      "step": 796400
    },
    {
      "epoch": 7.267866267610774,
      "grad_norm": 3.1255345344543457,
      "learning_rate": 4.3943444776991024e-05,
      "loss": 0.6819,
      "step": 796500
    },
    {
      "epoch": 7.268778742973939,
      "grad_norm": 3.3273065090179443,
      "learning_rate": 4.3942684380855054e-05,
      "loss": 0.7054,
      "step": 796600
    },
    {
      "epoch": 7.269691218337105,
      "grad_norm": 3.79487943649292,
      "learning_rate": 4.394192398471908e-05,
      "loss": 0.7194,
      "step": 796700
    },
    {
      "epoch": 7.27060369370027,
      "grad_norm": 4.1118998527526855,
      "learning_rate": 4.3941163588583114e-05,
      "loss": 0.6704,
      "step": 796800
    },
    {
      "epoch": 7.271516169063435,
      "grad_norm": 4.353735446929932,
      "learning_rate": 4.394040319244714e-05,
      "loss": 0.7257,
      "step": 796900
    },
    {
      "epoch": 7.272428644426601,
      "grad_norm": 3.8319826126098633,
      "learning_rate": 4.393964279631117e-05,
      "loss": 0.7163,
      "step": 797000
    },
    {
      "epoch": 7.273341119789766,
      "grad_norm": 3.902933359146118,
      "learning_rate": 4.39388824001752e-05,
      "loss": 0.705,
      "step": 797100
    },
    {
      "epoch": 7.274253595152931,
      "grad_norm": 3.8039469718933105,
      "learning_rate": 4.393812200403923e-05,
      "loss": 0.7219,
      "step": 797200
    },
    {
      "epoch": 7.275166070516096,
      "grad_norm": 4.2596516609191895,
      "learning_rate": 4.393736160790326e-05,
      "loss": 0.66,
      "step": 797300
    },
    {
      "epoch": 7.276078545879261,
      "grad_norm": 3.9035942554473877,
      "learning_rate": 4.393660121176729e-05,
      "loss": 0.7155,
      "step": 797400
    },
    {
      "epoch": 7.276991021242426,
      "grad_norm": 4.07306432723999,
      "learning_rate": 4.393584081563131e-05,
      "loss": 0.6783,
      "step": 797500
    },
    {
      "epoch": 7.2779034966055915,
      "grad_norm": 4.2867021560668945,
      "learning_rate": 4.393508041949534e-05,
      "loss": 0.7064,
      "step": 797600
    },
    {
      "epoch": 7.278815971968757,
      "grad_norm": 4.100190162658691,
      "learning_rate": 4.393432002335937e-05,
      "loss": 0.6913,
      "step": 797700
    },
    {
      "epoch": 7.279728447331922,
      "grad_norm": 3.724581718444824,
      "learning_rate": 4.39335596272234e-05,
      "loss": 0.7119,
      "step": 797800
    },
    {
      "epoch": 7.2806409226950874,
      "grad_norm": 3.20990252494812,
      "learning_rate": 4.393279923108743e-05,
      "loss": 0.6675,
      "step": 797900
    },
    {
      "epoch": 7.281553398058253,
      "grad_norm": 3.786679267883301,
      "learning_rate": 4.393203883495146e-05,
      "loss": 0.7206,
      "step": 798000
    },
    {
      "epoch": 7.282465873421417,
      "grad_norm": 4.249395370483398,
      "learning_rate": 4.3931278438815485e-05,
      "loss": 0.667,
      "step": 798100
    },
    {
      "epoch": 7.2833783487845825,
      "grad_norm": 3.4726221561431885,
      "learning_rate": 4.3930518042679515e-05,
      "loss": 0.6772,
      "step": 798200
    },
    {
      "epoch": 7.284290824147748,
      "grad_norm": 3.352907657623291,
      "learning_rate": 4.3929757646543545e-05,
      "loss": 0.6967,
      "step": 798300
    },
    {
      "epoch": 7.285203299510913,
      "grad_norm": 4.048654079437256,
      "learning_rate": 4.3928997250407575e-05,
      "loss": 0.6902,
      "step": 798400
    },
    {
      "epoch": 7.286115774874078,
      "grad_norm": 3.3658323287963867,
      "learning_rate": 4.3928236854271605e-05,
      "loss": 0.7096,
      "step": 798500
    },
    {
      "epoch": 7.287028250237244,
      "grad_norm": 4.1788411140441895,
      "learning_rate": 4.392747645813563e-05,
      "loss": 0.6839,
      "step": 798600
    },
    {
      "epoch": 7.287940725600409,
      "grad_norm": 4.447180271148682,
      "learning_rate": 4.3926716061999665e-05,
      "loss": 0.7027,
      "step": 798700
    },
    {
      "epoch": 7.288853200963574,
      "grad_norm": 4.155704021453857,
      "learning_rate": 4.392595566586369e-05,
      "loss": 0.6561,
      "step": 798800
    },
    {
      "epoch": 7.2897656763267396,
      "grad_norm": 4.3874664306640625,
      "learning_rate": 4.392519526972772e-05,
      "loss": 0.7072,
      "step": 798900
    },
    {
      "epoch": 7.290678151689904,
      "grad_norm": 4.505002975463867,
      "learning_rate": 4.392443487359175e-05,
      "loss": 0.7007,
      "step": 799000
    },
    {
      "epoch": 7.291590627053069,
      "grad_norm": 4.366691589355469,
      "learning_rate": 4.392367447745578e-05,
      "loss": 0.7181,
      "step": 799100
    },
    {
      "epoch": 7.292503102416235,
      "grad_norm": 4.0178446769714355,
      "learning_rate": 4.39229140813198e-05,
      "loss": 0.7058,
      "step": 799200
    },
    {
      "epoch": 7.2934155777794,
      "grad_norm": 4.643970966339111,
      "learning_rate": 4.392215368518384e-05,
      "loss": 0.66,
      "step": 799300
    },
    {
      "epoch": 7.294328053142565,
      "grad_norm": 2.8731555938720703,
      "learning_rate": 4.392139328904786e-05,
      "loss": 0.6806,
      "step": 799400
    },
    {
      "epoch": 7.2952405285057305,
      "grad_norm": 4.570672035217285,
      "learning_rate": 4.392063289291189e-05,
      "loss": 0.6958,
      "step": 799500
    },
    {
      "epoch": 7.296153003868896,
      "grad_norm": 4.078376770019531,
      "learning_rate": 4.391987249677592e-05,
      "loss": 0.6958,
      "step": 799600
    },
    {
      "epoch": 7.297065479232061,
      "grad_norm": 3.8705122470855713,
      "learning_rate": 4.391911210063995e-05,
      "loss": 0.7076,
      "step": 799700
    },
    {
      "epoch": 7.2979779545952255,
      "grad_norm": 3.4344398975372314,
      "learning_rate": 4.391835170450398e-05,
      "loss": 0.7121,
      "step": 799800
    },
    {
      "epoch": 7.298890429958391,
      "grad_norm": 3.7644190788269043,
      "learning_rate": 4.391759130836801e-05,
      "loss": 0.6836,
      "step": 799900
    },
    {
      "epoch": 7.299802905321556,
      "grad_norm": 4.407524108886719,
      "learning_rate": 4.3916830912232036e-05,
      "loss": 0.714,
      "step": 800000
    },
    {
      "epoch": 7.300715380684721,
      "grad_norm": 3.4343960285186768,
      "learning_rate": 4.391607051609607e-05,
      "loss": 0.6845,
      "step": 800100
    },
    {
      "epoch": 7.301627856047887,
      "grad_norm": 3.75887393951416,
      "learning_rate": 4.3915310119960096e-05,
      "loss": 0.6886,
      "step": 800200
    },
    {
      "epoch": 7.302540331411052,
      "grad_norm": 3.588996648788452,
      "learning_rate": 4.3914549723824126e-05,
      "loss": 0.7448,
      "step": 800300
    },
    {
      "epoch": 7.303452806774217,
      "grad_norm": 3.3995625972747803,
      "learning_rate": 4.3913789327688156e-05,
      "loss": 0.6842,
      "step": 800400
    },
    {
      "epoch": 7.304365282137383,
      "grad_norm": 4.327515602111816,
      "learning_rate": 4.3913028931552186e-05,
      "loss": 0.6754,
      "step": 800500
    },
    {
      "epoch": 7.305277757500548,
      "grad_norm": 3.465836763381958,
      "learning_rate": 4.391226853541621e-05,
      "loss": 0.6697,
      "step": 800600
    },
    {
      "epoch": 7.306190232863712,
      "grad_norm": 4.164401054382324,
      "learning_rate": 4.3911508139280246e-05,
      "loss": 0.7382,
      "step": 800700
    },
    {
      "epoch": 7.307102708226878,
      "grad_norm": 3.6707468032836914,
      "learning_rate": 4.391074774314427e-05,
      "loss": 0.6861,
      "step": 800800
    },
    {
      "epoch": 7.308015183590043,
      "grad_norm": 3.8111624717712402,
      "learning_rate": 4.39099873470083e-05,
      "loss": 0.7191,
      "step": 800900
    },
    {
      "epoch": 7.308927658953208,
      "grad_norm": 4.394351005554199,
      "learning_rate": 4.390922695087233e-05,
      "loss": 0.7095,
      "step": 801000
    },
    {
      "epoch": 7.3098401343163735,
      "grad_norm": 4.501474857330322,
      "learning_rate": 4.390846655473635e-05,
      "loss": 0.7043,
      "step": 801100
    },
    {
      "epoch": 7.310752609679539,
      "grad_norm": 2.770991802215576,
      "learning_rate": 4.390770615860039e-05,
      "loss": 0.7044,
      "step": 801200
    },
    {
      "epoch": 7.311665085042704,
      "grad_norm": 4.198536396026611,
      "learning_rate": 4.390694576246441e-05,
      "loss": 0.7039,
      "step": 801300
    },
    {
      "epoch": 7.312577560405869,
      "grad_norm": 3.564713478088379,
      "learning_rate": 4.390618536632844e-05,
      "loss": 0.6638,
      "step": 801400
    },
    {
      "epoch": 7.313490035769034,
      "grad_norm": 4.661861419677734,
      "learning_rate": 4.390542497019247e-05,
      "loss": 0.6749,
      "step": 801500
    },
    {
      "epoch": 7.314402511132199,
      "grad_norm": 4.337643146514893,
      "learning_rate": 4.3904664574056503e-05,
      "loss": 0.7184,
      "step": 801600
    },
    {
      "epoch": 7.315314986495364,
      "grad_norm": 4.438243389129639,
      "learning_rate": 4.390390417792053e-05,
      "loss": 0.6827,
      "step": 801700
    },
    {
      "epoch": 7.31622746185853,
      "grad_norm": 4.981945037841797,
      "learning_rate": 4.3903143781784564e-05,
      "loss": 0.7214,
      "step": 801800
    },
    {
      "epoch": 7.317139937221695,
      "grad_norm": 3.5267512798309326,
      "learning_rate": 4.390238338564859e-05,
      "loss": 0.6523,
      "step": 801900
    },
    {
      "epoch": 7.31805241258486,
      "grad_norm": 4.563197612762451,
      "learning_rate": 4.390162298951262e-05,
      "loss": 0.6862,
      "step": 802000
    },
    {
      "epoch": 7.318964887948026,
      "grad_norm": 3.7198734283447266,
      "learning_rate": 4.390086259337665e-05,
      "loss": 0.7337,
      "step": 802100
    },
    {
      "epoch": 7.319877363311191,
      "grad_norm": 4.870389461517334,
      "learning_rate": 4.390010219724068e-05,
      "loss": 0.737,
      "step": 802200
    },
    {
      "epoch": 7.320789838674356,
      "grad_norm": 4.11421012878418,
      "learning_rate": 4.389934180110471e-05,
      "loss": 0.7245,
      "step": 802300
    },
    {
      "epoch": 7.321702314037521,
      "grad_norm": 3.2064483165740967,
      "learning_rate": 4.389858140496874e-05,
      "loss": 0.6708,
      "step": 802400
    },
    {
      "epoch": 7.322614789400686,
      "grad_norm": 4.453713893890381,
      "learning_rate": 4.389782100883276e-05,
      "loss": 0.6956,
      "step": 802500
    },
    {
      "epoch": 7.323527264763851,
      "grad_norm": 4.280559539794922,
      "learning_rate": 4.38970606126968e-05,
      "loss": 0.6733,
      "step": 802600
    },
    {
      "epoch": 7.3244397401270165,
      "grad_norm": 4.467256546020508,
      "learning_rate": 4.389630021656082e-05,
      "loss": 0.7536,
      "step": 802700
    },
    {
      "epoch": 7.325352215490182,
      "grad_norm": 3.936249256134033,
      "learning_rate": 4.389553982042485e-05,
      "loss": 0.6628,
      "step": 802800
    },
    {
      "epoch": 7.326264690853347,
      "grad_norm": 3.4597887992858887,
      "learning_rate": 4.389477942428888e-05,
      "loss": 0.6928,
      "step": 802900
    },
    {
      "epoch": 7.327177166216512,
      "grad_norm": 3.911545991897583,
      "learning_rate": 4.389401902815291e-05,
      "loss": 0.6922,
      "step": 803000
    },
    {
      "epoch": 7.328089641579678,
      "grad_norm": 4.193163871765137,
      "learning_rate": 4.3893258632016934e-05,
      "loss": 0.7453,
      "step": 803100
    },
    {
      "epoch": 7.329002116942842,
      "grad_norm": 3.3723180294036865,
      "learning_rate": 4.389249823588097e-05,
      "loss": 0.6999,
      "step": 803200
    },
    {
      "epoch": 7.329914592306007,
      "grad_norm": 4.490355014801025,
      "learning_rate": 4.3891737839744994e-05,
      "loss": 0.6577,
      "step": 803300
    },
    {
      "epoch": 7.330827067669173,
      "grad_norm": 3.9124362468719482,
      "learning_rate": 4.3890977443609024e-05,
      "loss": 0.6521,
      "step": 803400
    },
    {
      "epoch": 7.331739543032338,
      "grad_norm": 4.717910289764404,
      "learning_rate": 4.3890217047473054e-05,
      "loss": 0.6996,
      "step": 803500
    },
    {
      "epoch": 7.332652018395503,
      "grad_norm": 4.611574172973633,
      "learning_rate": 4.3889456651337084e-05,
      "loss": 0.6883,
      "step": 803600
    },
    {
      "epoch": 7.333564493758669,
      "grad_norm": 4.375025272369385,
      "learning_rate": 4.3888696255201115e-05,
      "loss": 0.7085,
      "step": 803700
    },
    {
      "epoch": 7.334476969121834,
      "grad_norm": 4.705760478973389,
      "learning_rate": 4.388793585906514e-05,
      "loss": 0.7514,
      "step": 803800
    },
    {
      "epoch": 7.335389444484999,
      "grad_norm": 4.789991855621338,
      "learning_rate": 4.388717546292917e-05,
      "loss": 0.6858,
      "step": 803900
    },
    {
      "epoch": 7.3363019198481645,
      "grad_norm": 3.910628318786621,
      "learning_rate": 4.38864150667932e-05,
      "loss": 0.6928,
      "step": 804000
    },
    {
      "epoch": 7.337214395211329,
      "grad_norm": 3.5077285766601562,
      "learning_rate": 4.388565467065723e-05,
      "loss": 0.7196,
      "step": 804100
    },
    {
      "epoch": 7.338126870574494,
      "grad_norm": 4.0957794189453125,
      "learning_rate": 4.388489427452125e-05,
      "loss": 0.7336,
      "step": 804200
    },
    {
      "epoch": 7.3390393459376595,
      "grad_norm": 4.425766944885254,
      "learning_rate": 4.388413387838529e-05,
      "loss": 0.6859,
      "step": 804300
    },
    {
      "epoch": 7.339951821300825,
      "grad_norm": 3.155609607696533,
      "learning_rate": 4.388337348224931e-05,
      "loss": 0.7197,
      "step": 804400
    },
    {
      "epoch": 7.34086429666399,
      "grad_norm": 3.9991185665130615,
      "learning_rate": 4.388261308611334e-05,
      "loss": 0.7182,
      "step": 804500
    },
    {
      "epoch": 7.341776772027155,
      "grad_norm": 3.770934820175171,
      "learning_rate": 4.388185268997737e-05,
      "loss": 0.7195,
      "step": 804600
    },
    {
      "epoch": 7.342689247390321,
      "grad_norm": 2.8729400634765625,
      "learning_rate": 4.38810922938414e-05,
      "loss": 0.6568,
      "step": 804700
    },
    {
      "epoch": 7.343601722753486,
      "grad_norm": 4.561777591705322,
      "learning_rate": 4.388033189770543e-05,
      "loss": 0.697,
      "step": 804800
    },
    {
      "epoch": 7.34451419811665,
      "grad_norm": 4.247983455657959,
      "learning_rate": 4.387957150156946e-05,
      "loss": 0.7057,
      "step": 804900
    },
    {
      "epoch": 7.345426673479816,
      "grad_norm": 3.5122711658477783,
      "learning_rate": 4.3878811105433485e-05,
      "loss": 0.6712,
      "step": 805000
    },
    {
      "epoch": 7.346339148842981,
      "grad_norm": 3.7248153686523438,
      "learning_rate": 4.387805070929752e-05,
      "loss": 0.7168,
      "step": 805100
    },
    {
      "epoch": 7.347251624206146,
      "grad_norm": 4.209258079528809,
      "learning_rate": 4.3877290313161545e-05,
      "loss": 0.6895,
      "step": 805200
    },
    {
      "epoch": 7.348164099569312,
      "grad_norm": 4.267983436584473,
      "learning_rate": 4.3876529917025575e-05,
      "loss": 0.7626,
      "step": 805300
    },
    {
      "epoch": 7.349076574932477,
      "grad_norm": 4.060825824737549,
      "learning_rate": 4.3875769520889605e-05,
      "loss": 0.7022,
      "step": 805400
    },
    {
      "epoch": 7.349989050295642,
      "grad_norm": 4.244182109832764,
      "learning_rate": 4.3875009124753635e-05,
      "loss": 0.7088,
      "step": 805500
    },
    {
      "epoch": 7.3509015256588075,
      "grad_norm": 3.959535598754883,
      "learning_rate": 4.387424872861766e-05,
      "loss": 0.6776,
      "step": 805600
    },
    {
      "epoch": 7.351814001021973,
      "grad_norm": 4.514089584350586,
      "learning_rate": 4.3873488332481696e-05,
      "loss": 0.6942,
      "step": 805700
    },
    {
      "epoch": 7.352726476385137,
      "grad_norm": 3.4181108474731445,
      "learning_rate": 4.387272793634572e-05,
      "loss": 0.7224,
      "step": 805800
    },
    {
      "epoch": 7.3536389517483025,
      "grad_norm": 4.672802925109863,
      "learning_rate": 4.387196754020975e-05,
      "loss": 0.6538,
      "step": 805900
    },
    {
      "epoch": 7.354551427111468,
      "grad_norm": 4.857397556304932,
      "learning_rate": 4.387120714407378e-05,
      "loss": 0.7359,
      "step": 806000
    },
    {
      "epoch": 7.355463902474633,
      "grad_norm": 3.8194074630737305,
      "learning_rate": 4.387044674793781e-05,
      "loss": 0.6701,
      "step": 806100
    },
    {
      "epoch": 7.356376377837798,
      "grad_norm": 5.153172492980957,
      "learning_rate": 4.386968635180184e-05,
      "loss": 0.6889,
      "step": 806200
    },
    {
      "epoch": 7.357288853200964,
      "grad_norm": 3.8318819999694824,
      "learning_rate": 4.386892595566587e-05,
      "loss": 0.7283,
      "step": 806300
    },
    {
      "epoch": 7.358201328564129,
      "grad_norm": 4.073362350463867,
      "learning_rate": 4.386816555952989e-05,
      "loss": 0.6605,
      "step": 806400
    },
    {
      "epoch": 7.359113803927294,
      "grad_norm": 4.003269672393799,
      "learning_rate": 4.386740516339393e-05,
      "loss": 0.6849,
      "step": 806500
    },
    {
      "epoch": 7.360026279290459,
      "grad_norm": 3.6159043312072754,
      "learning_rate": 4.386664476725795e-05,
      "loss": 0.6974,
      "step": 806600
    },
    {
      "epoch": 7.360938754653624,
      "grad_norm": 4.13291072845459,
      "learning_rate": 4.3865884371121976e-05,
      "loss": 0.7177,
      "step": 806700
    },
    {
      "epoch": 7.361851230016789,
      "grad_norm": 4.700050354003906,
      "learning_rate": 4.386512397498601e-05,
      "loss": 0.7033,
      "step": 806800
    },
    {
      "epoch": 7.362763705379955,
      "grad_norm": 3.536982297897339,
      "learning_rate": 4.3864363578850036e-05,
      "loss": 0.7195,
      "step": 806900
    },
    {
      "epoch": 7.36367618074312,
      "grad_norm": 3.542942523956299,
      "learning_rate": 4.3863603182714066e-05,
      "loss": 0.706,
      "step": 807000
    },
    {
      "epoch": 7.364588656106285,
      "grad_norm": 4.860027313232422,
      "learning_rate": 4.3862842786578096e-05,
      "loss": 0.7166,
      "step": 807100
    },
    {
      "epoch": 7.3655011314694505,
      "grad_norm": 4.04811429977417,
      "learning_rate": 4.3862082390442126e-05,
      "loss": 0.6522,
      "step": 807200
    },
    {
      "epoch": 7.366413606832616,
      "grad_norm": 4.662905216217041,
      "learning_rate": 4.3861321994306156e-05,
      "loss": 0.7027,
      "step": 807300
    },
    {
      "epoch": 7.367326082195781,
      "grad_norm": 5.10421895980835,
      "learning_rate": 4.3860561598170186e-05,
      "loss": 0.6948,
      "step": 807400
    },
    {
      "epoch": 7.3682385575589455,
      "grad_norm": 3.9471211433410645,
      "learning_rate": 4.385980120203421e-05,
      "loss": 0.6926,
      "step": 807500
    },
    {
      "epoch": 7.369151032922111,
      "grad_norm": 4.242860794067383,
      "learning_rate": 4.3859040805898246e-05,
      "loss": 0.6965,
      "step": 807600
    },
    {
      "epoch": 7.370063508285276,
      "grad_norm": 4.47153902053833,
      "learning_rate": 4.385828040976227e-05,
      "loss": 0.723,
      "step": 807700
    },
    {
      "epoch": 7.370975983648441,
      "grad_norm": 4.07893705368042,
      "learning_rate": 4.38575200136263e-05,
      "loss": 0.6643,
      "step": 807800
    },
    {
      "epoch": 7.371888459011607,
      "grad_norm": 3.196371555328369,
      "learning_rate": 4.385675961749033e-05,
      "loss": 0.7115,
      "step": 807900
    },
    {
      "epoch": 7.372800934374772,
      "grad_norm": 4.494369029998779,
      "learning_rate": 4.385599922135436e-05,
      "loss": 0.724,
      "step": 808000
    },
    {
      "epoch": 7.373713409737937,
      "grad_norm": 4.750156402587891,
      "learning_rate": 4.385523882521838e-05,
      "loss": 0.6783,
      "step": 808100
    },
    {
      "epoch": 7.374625885101103,
      "grad_norm": 2.6975347995758057,
      "learning_rate": 4.385447842908242e-05,
      "loss": 0.7234,
      "step": 808200
    },
    {
      "epoch": 7.375538360464267,
      "grad_norm": 4.759696960449219,
      "learning_rate": 4.3853718032946443e-05,
      "loss": 0.6944,
      "step": 808300
    },
    {
      "epoch": 7.376450835827432,
      "grad_norm": 4.466505527496338,
      "learning_rate": 4.3852957636810473e-05,
      "loss": 0.693,
      "step": 808400
    },
    {
      "epoch": 7.377363311190598,
      "grad_norm": 3.9122018814086914,
      "learning_rate": 4.3852197240674504e-05,
      "loss": 0.6947,
      "step": 808500
    },
    {
      "epoch": 7.378275786553763,
      "grad_norm": 3.7569854259490967,
      "learning_rate": 4.3851436844538534e-05,
      "loss": 0.715,
      "step": 808600
    },
    {
      "epoch": 7.379188261916928,
      "grad_norm": 4.6999945640563965,
      "learning_rate": 4.3850676448402564e-05,
      "loss": 0.7031,
      "step": 808700
    },
    {
      "epoch": 7.3801007372800935,
      "grad_norm": 3.7958860397338867,
      "learning_rate": 4.3849916052266594e-05,
      "loss": 0.7124,
      "step": 808800
    },
    {
      "epoch": 7.381013212643259,
      "grad_norm": 4.303598880767822,
      "learning_rate": 4.384915565613062e-05,
      "loss": 0.6689,
      "step": 808900
    },
    {
      "epoch": 7.381925688006424,
      "grad_norm": 5.749253749847412,
      "learning_rate": 4.3848395259994654e-05,
      "loss": 0.7131,
      "step": 809000
    },
    {
      "epoch": 7.382838163369589,
      "grad_norm": 4.002306938171387,
      "learning_rate": 4.384763486385868e-05,
      "loss": 0.6996,
      "step": 809100
    },
    {
      "epoch": 7.383750638732754,
      "grad_norm": 20.26498794555664,
      "learning_rate": 4.384687446772271e-05,
      "loss": 0.7197,
      "step": 809200
    },
    {
      "epoch": 7.384663114095919,
      "grad_norm": 3.6949236392974854,
      "learning_rate": 4.384611407158674e-05,
      "loss": 0.6994,
      "step": 809300
    },
    {
      "epoch": 7.385575589459084,
      "grad_norm": 4.884005546569824,
      "learning_rate": 4.384535367545077e-05,
      "loss": 0.7065,
      "step": 809400
    },
    {
      "epoch": 7.38648806482225,
      "grad_norm": 3.53572154045105,
      "learning_rate": 4.384459327931479e-05,
      "loss": 0.67,
      "step": 809500
    },
    {
      "epoch": 7.387400540185415,
      "grad_norm": 4.671032905578613,
      "learning_rate": 4.384383288317882e-05,
      "loss": 0.7248,
      "step": 809600
    },
    {
      "epoch": 7.38831301554858,
      "grad_norm": 3.867337703704834,
      "learning_rate": 4.384307248704285e-05,
      "loss": 0.7121,
      "step": 809700
    },
    {
      "epoch": 7.389225490911746,
      "grad_norm": 4.192674160003662,
      "learning_rate": 4.384231209090688e-05,
      "loss": 0.6902,
      "step": 809800
    },
    {
      "epoch": 7.390137966274911,
      "grad_norm": 4.857807636260986,
      "learning_rate": 4.384155169477091e-05,
      "loss": 0.6875,
      "step": 809900
    },
    {
      "epoch": 7.391050441638075,
      "grad_norm": 4.064435958862305,
      "learning_rate": 4.3840791298634934e-05,
      "loss": 0.677,
      "step": 810000
    },
    {
      "epoch": 7.391962917001241,
      "grad_norm": 4.384884357452393,
      "learning_rate": 4.384003090249897e-05,
      "loss": 0.6925,
      "step": 810100
    },
    {
      "epoch": 7.392875392364406,
      "grad_norm": 4.195639610290527,
      "learning_rate": 4.3839270506362994e-05,
      "loss": 0.6861,
      "step": 810200
    },
    {
      "epoch": 7.393787867727571,
      "grad_norm": 3.7641241550445557,
      "learning_rate": 4.3838510110227024e-05,
      "loss": 0.6859,
      "step": 810300
    },
    {
      "epoch": 7.3947003430907365,
      "grad_norm": 3.755934476852417,
      "learning_rate": 4.3837749714091054e-05,
      "loss": 0.6585,
      "step": 810400
    },
    {
      "epoch": 7.395612818453902,
      "grad_norm": 4.554218292236328,
      "learning_rate": 4.3836989317955085e-05,
      "loss": 0.7045,
      "step": 810500
    },
    {
      "epoch": 7.396525293817067,
      "grad_norm": 4.531383037567139,
      "learning_rate": 4.3836228921819115e-05,
      "loss": 0.7121,
      "step": 810600
    },
    {
      "epoch": 7.397437769180232,
      "grad_norm": 4.265566349029541,
      "learning_rate": 4.3835468525683145e-05,
      "loss": 0.7221,
      "step": 810700
    },
    {
      "epoch": 7.398350244543398,
      "grad_norm": 4.317162990570068,
      "learning_rate": 4.383470812954717e-05,
      "loss": 0.7093,
      "step": 810800
    },
    {
      "epoch": 7.399262719906562,
      "grad_norm": 3.804091215133667,
      "learning_rate": 4.38339477334112e-05,
      "loss": 0.6666,
      "step": 810900
    },
    {
      "epoch": 7.4001751952697274,
      "grad_norm": 3.574925184249878,
      "learning_rate": 4.383318733727523e-05,
      "loss": 0.6754,
      "step": 811000
    },
    {
      "epoch": 7.401087670632893,
      "grad_norm": 3.855776071548462,
      "learning_rate": 4.383242694113926e-05,
      "loss": 0.6571,
      "step": 811100
    },
    {
      "epoch": 7.402000145996058,
      "grad_norm": 4.040219306945801,
      "learning_rate": 4.383166654500329e-05,
      "loss": 0.6997,
      "step": 811200
    },
    {
      "epoch": 7.402912621359223,
      "grad_norm": 3.7689504623413086,
      "learning_rate": 4.383090614886732e-05,
      "loss": 0.6951,
      "step": 811300
    },
    {
      "epoch": 7.403825096722389,
      "grad_norm": 5.051730632781982,
      "learning_rate": 4.383014575273134e-05,
      "loss": 0.7321,
      "step": 811400
    },
    {
      "epoch": 7.404737572085554,
      "grad_norm": 3.7448947429656982,
      "learning_rate": 4.382938535659538e-05,
      "loss": 0.6624,
      "step": 811500
    },
    {
      "epoch": 7.405650047448719,
      "grad_norm": 3.725383758544922,
      "learning_rate": 4.38286249604594e-05,
      "loss": 0.7128,
      "step": 811600
    },
    {
      "epoch": 7.406562522811884,
      "grad_norm": 4.444459438323975,
      "learning_rate": 4.382786456432343e-05,
      "loss": 0.7213,
      "step": 811700
    },
    {
      "epoch": 7.407474998175049,
      "grad_norm": 3.242053270339966,
      "learning_rate": 4.382710416818746e-05,
      "loss": 0.6807,
      "step": 811800
    },
    {
      "epoch": 7.408387473538214,
      "grad_norm": 4.055274486541748,
      "learning_rate": 4.382634377205149e-05,
      "loss": 0.7178,
      "step": 811900
    },
    {
      "epoch": 7.4092999489013796,
      "grad_norm": 3.721242904663086,
      "learning_rate": 4.382558337591552e-05,
      "loss": 0.7222,
      "step": 812000
    },
    {
      "epoch": 7.410212424264545,
      "grad_norm": 3.7304654121398926,
      "learning_rate": 4.382482297977955e-05,
      "loss": 0.7084,
      "step": 812100
    },
    {
      "epoch": 7.41112489962771,
      "grad_norm": 3.2400362491607666,
      "learning_rate": 4.3824062583643575e-05,
      "loss": 0.7105,
      "step": 812200
    },
    {
      "epoch": 7.4120373749908754,
      "grad_norm": 1.9988242387771606,
      "learning_rate": 4.3823302187507605e-05,
      "loss": 0.6636,
      "step": 812300
    },
    {
      "epoch": 7.412949850354041,
      "grad_norm": 4.724806785583496,
      "learning_rate": 4.3822541791371635e-05,
      "loss": 0.6801,
      "step": 812400
    },
    {
      "epoch": 7.413862325717206,
      "grad_norm": 5.283994197845459,
      "learning_rate": 4.382178139523566e-05,
      "loss": 0.7219,
      "step": 812500
    },
    {
      "epoch": 7.4147748010803705,
      "grad_norm": 3.6981236934661865,
      "learning_rate": 4.3821020999099696e-05,
      "loss": 0.6873,
      "step": 812600
    },
    {
      "epoch": 7.415687276443536,
      "grad_norm": 4.981631755828857,
      "learning_rate": 4.382026060296372e-05,
      "loss": 0.7019,
      "step": 812700
    },
    {
      "epoch": 7.416599751806701,
      "grad_norm": 4.458011150360107,
      "learning_rate": 4.381950020682775e-05,
      "loss": 0.6946,
      "step": 812800
    },
    {
      "epoch": 7.417512227169866,
      "grad_norm": 4.185325622558594,
      "learning_rate": 4.381873981069178e-05,
      "loss": 0.7015,
      "step": 812900
    },
    {
      "epoch": 7.418424702533032,
      "grad_norm": 4.218400955200195,
      "learning_rate": 4.381797941455581e-05,
      "loss": 0.6875,
      "step": 813000
    },
    {
      "epoch": 7.419337177896197,
      "grad_norm": 3.621615409851074,
      "learning_rate": 4.381721901841984e-05,
      "loss": 0.7114,
      "step": 813100
    },
    {
      "epoch": 7.420249653259362,
      "grad_norm": 4.011375427246094,
      "learning_rate": 4.381645862228387e-05,
      "loss": 0.693,
      "step": 813200
    },
    {
      "epoch": 7.4211621286225276,
      "grad_norm": 3.7475931644439697,
      "learning_rate": 4.381569822614789e-05,
      "loss": 0.6827,
      "step": 813300
    },
    {
      "epoch": 7.422074603985692,
      "grad_norm": 4.077504634857178,
      "learning_rate": 4.381493783001193e-05,
      "loss": 0.7078,
      "step": 813400
    },
    {
      "epoch": 7.422987079348857,
      "grad_norm": 3.665560007095337,
      "learning_rate": 4.381417743387595e-05,
      "loss": 0.6651,
      "step": 813500
    },
    {
      "epoch": 7.423899554712023,
      "grad_norm": 4.735321044921875,
      "learning_rate": 4.381341703773998e-05,
      "loss": 0.6684,
      "step": 813600
    },
    {
      "epoch": 7.424812030075188,
      "grad_norm": 3.5916223526000977,
      "learning_rate": 4.381265664160401e-05,
      "loss": 0.6453,
      "step": 813700
    },
    {
      "epoch": 7.425724505438353,
      "grad_norm": 4.051627159118652,
      "learning_rate": 4.381189624546804e-05,
      "loss": 0.7274,
      "step": 813800
    },
    {
      "epoch": 7.4266369808015185,
      "grad_norm": 5.178016185760498,
      "learning_rate": 4.3811135849332066e-05,
      "loss": 0.7165,
      "step": 813900
    },
    {
      "epoch": 7.427549456164684,
      "grad_norm": 4.38910436630249,
      "learning_rate": 4.38103754531961e-05,
      "loss": 0.6707,
      "step": 814000
    },
    {
      "epoch": 7.428461931527849,
      "grad_norm": 4.468398094177246,
      "learning_rate": 4.3809615057060126e-05,
      "loss": 0.7092,
      "step": 814100
    },
    {
      "epoch": 7.429374406891014,
      "grad_norm": 3.461245536804199,
      "learning_rate": 4.3808854660924156e-05,
      "loss": 0.6793,
      "step": 814200
    },
    {
      "epoch": 7.430286882254179,
      "grad_norm": 3.868239641189575,
      "learning_rate": 4.3808094264788186e-05,
      "loss": 0.6821,
      "step": 814300
    },
    {
      "epoch": 7.431199357617344,
      "grad_norm": 4.25483512878418,
      "learning_rate": 4.3807333868652217e-05,
      "loss": 0.6673,
      "step": 814400
    },
    {
      "epoch": 7.432111832980509,
      "grad_norm": 4.042174339294434,
      "learning_rate": 4.3806573472516247e-05,
      "loss": 0.6838,
      "step": 814500
    },
    {
      "epoch": 7.433024308343675,
      "grad_norm": 3.359243392944336,
      "learning_rate": 4.380581307638028e-05,
      "loss": 0.7219,
      "step": 814600
    },
    {
      "epoch": 7.43393678370684,
      "grad_norm": 3.8243355751037598,
      "learning_rate": 4.38050526802443e-05,
      "loss": 0.7119,
      "step": 814700
    },
    {
      "epoch": 7.434849259070005,
      "grad_norm": 3.892812728881836,
      "learning_rate": 4.380429228410834e-05,
      "loss": 0.6645,
      "step": 814800
    },
    {
      "epoch": 7.435761734433171,
      "grad_norm": 4.426499843597412,
      "learning_rate": 4.380353188797236e-05,
      "loss": 0.6923,
      "step": 814900
    },
    {
      "epoch": 7.436674209796336,
      "grad_norm": 3.78376841545105,
      "learning_rate": 4.380277149183639e-05,
      "loss": 0.731,
      "step": 815000
    },
    {
      "epoch": 7.4375866851595,
      "grad_norm": 4.355355262756348,
      "learning_rate": 4.380201109570042e-05,
      "loss": 0.7135,
      "step": 815100
    },
    {
      "epoch": 7.438499160522666,
      "grad_norm": 4.117298126220703,
      "learning_rate": 4.3801250699564443e-05,
      "loss": 0.6825,
      "step": 815200
    },
    {
      "epoch": 7.439411635885831,
      "grad_norm": 4.037682056427002,
      "learning_rate": 4.3800490303428474e-05,
      "loss": 0.7047,
      "step": 815300
    },
    {
      "epoch": 7.440324111248996,
      "grad_norm": 3.444333076477051,
      "learning_rate": 4.3799729907292504e-05,
      "loss": 0.6771,
      "step": 815400
    },
    {
      "epoch": 7.4412365866121615,
      "grad_norm": 3.288004159927368,
      "learning_rate": 4.3798969511156534e-05,
      "loss": 0.6703,
      "step": 815500
    },
    {
      "epoch": 7.442149061975327,
      "grad_norm": 3.3673527240753174,
      "learning_rate": 4.3798209115020564e-05,
      "loss": 0.6746,
      "step": 815600
    },
    {
      "epoch": 7.443061537338492,
      "grad_norm": 3.740640878677368,
      "learning_rate": 4.3797448718884594e-05,
      "loss": 0.7263,
      "step": 815700
    },
    {
      "epoch": 7.443974012701657,
      "grad_norm": 4.454596042633057,
      "learning_rate": 4.379668832274862e-05,
      "loss": 0.6759,
      "step": 815800
    },
    {
      "epoch": 7.444886488064823,
      "grad_norm": 4.093662261962891,
      "learning_rate": 4.3795927926612654e-05,
      "loss": 0.6987,
      "step": 815900
    },
    {
      "epoch": 7.445798963427987,
      "grad_norm": 4.220093727111816,
      "learning_rate": 4.379516753047668e-05,
      "loss": 0.6495,
      "step": 816000
    },
    {
      "epoch": 7.446711438791152,
      "grad_norm": 3.901853084564209,
      "learning_rate": 4.379440713434071e-05,
      "loss": 0.6693,
      "step": 816100
    },
    {
      "epoch": 7.447623914154318,
      "grad_norm": 3.668553352355957,
      "learning_rate": 4.379364673820474e-05,
      "loss": 0.7124,
      "step": 816200
    },
    {
      "epoch": 7.448536389517483,
      "grad_norm": 2.8004956245422363,
      "learning_rate": 4.379288634206877e-05,
      "loss": 0.6887,
      "step": 816300
    },
    {
      "epoch": 7.449448864880648,
      "grad_norm": 4.830202102661133,
      "learning_rate": 4.379212594593279e-05,
      "loss": 0.7209,
      "step": 816400
    },
    {
      "epoch": 7.450361340243814,
      "grad_norm": 3.9181456565856934,
      "learning_rate": 4.379136554979683e-05,
      "loss": 0.7282,
      "step": 816500
    },
    {
      "epoch": 7.451273815606979,
      "grad_norm": 3.2210447788238525,
      "learning_rate": 4.379060515366085e-05,
      "loss": 0.6766,
      "step": 816600
    },
    {
      "epoch": 7.452186290970144,
      "grad_norm": 4.047525405883789,
      "learning_rate": 4.378984475752488e-05,
      "loss": 0.7363,
      "step": 816700
    },
    {
      "epoch": 7.453098766333309,
      "grad_norm": 4.4684343338012695,
      "learning_rate": 4.378908436138891e-05,
      "loss": 0.6804,
      "step": 816800
    },
    {
      "epoch": 7.454011241696474,
      "grad_norm": 3.62174654006958,
      "learning_rate": 4.378832396525294e-05,
      "loss": 0.6704,
      "step": 816900
    },
    {
      "epoch": 7.454923717059639,
      "grad_norm": 3.2828948497772217,
      "learning_rate": 4.378756356911697e-05,
      "loss": 0.6925,
      "step": 817000
    },
    {
      "epoch": 7.4558361924228045,
      "grad_norm": 4.351017951965332,
      "learning_rate": 4.3786803172981e-05,
      "loss": 0.683,
      "step": 817100
    },
    {
      "epoch": 7.45674866778597,
      "grad_norm": 3.6233348846435547,
      "learning_rate": 4.3786042776845025e-05,
      "loss": 0.711,
      "step": 817200
    },
    {
      "epoch": 7.457661143149135,
      "grad_norm": 3.482828140258789,
      "learning_rate": 4.378528238070906e-05,
      "loss": 0.7022,
      "step": 817300
    },
    {
      "epoch": 7.4585736185123,
      "grad_norm": 3.889434814453125,
      "learning_rate": 4.3784521984573085e-05,
      "loss": 0.732,
      "step": 817400
    },
    {
      "epoch": 7.459486093875466,
      "grad_norm": 3.991011381149292,
      "learning_rate": 4.3783761588437115e-05,
      "loss": 0.6884,
      "step": 817500
    },
    {
      "epoch": 7.460398569238631,
      "grad_norm": 4.093149185180664,
      "learning_rate": 4.3783001192301145e-05,
      "loss": 0.7423,
      "step": 817600
    },
    {
      "epoch": 7.461311044601795,
      "grad_norm": 3.9885151386260986,
      "learning_rate": 4.3782240796165175e-05,
      "loss": 0.6881,
      "step": 817700
    },
    {
      "epoch": 7.462223519964961,
      "grad_norm": 3.846951484680176,
      "learning_rate": 4.37814804000292e-05,
      "loss": 0.7031,
      "step": 817800
    },
    {
      "epoch": 7.463135995328126,
      "grad_norm": 4.265954971313477,
      "learning_rate": 4.3780720003893235e-05,
      "loss": 0.6979,
      "step": 817900
    },
    {
      "epoch": 7.464048470691291,
      "grad_norm": 3.693903923034668,
      "learning_rate": 4.377995960775726e-05,
      "loss": 0.6798,
      "step": 818000
    },
    {
      "epoch": 7.464960946054457,
      "grad_norm": 4.511372089385986,
      "learning_rate": 4.377919921162129e-05,
      "loss": 0.6861,
      "step": 818100
    },
    {
      "epoch": 7.465873421417622,
      "grad_norm": 3.984600305557251,
      "learning_rate": 4.377843881548532e-05,
      "loss": 0.7479,
      "step": 818200
    },
    {
      "epoch": 7.466785896780787,
      "grad_norm": 4.445286750793457,
      "learning_rate": 4.377767841934934e-05,
      "loss": 0.7141,
      "step": 818300
    },
    {
      "epoch": 7.4676983721439525,
      "grad_norm": 4.180960178375244,
      "learning_rate": 4.377691802321338e-05,
      "loss": 0.6918,
      "step": 818400
    },
    {
      "epoch": 7.468610847507117,
      "grad_norm": 3.499938488006592,
      "learning_rate": 4.37761576270774e-05,
      "loss": 0.6924,
      "step": 818500
    },
    {
      "epoch": 7.469523322870282,
      "grad_norm": 4.78202486038208,
      "learning_rate": 4.377539723094143e-05,
      "loss": 0.7129,
      "step": 818600
    },
    {
      "epoch": 7.4704357982334475,
      "grad_norm": 3.6795654296875,
      "learning_rate": 4.377463683480546e-05,
      "loss": 0.7176,
      "step": 818700
    },
    {
      "epoch": 7.471348273596613,
      "grad_norm": 3.9754703044891357,
      "learning_rate": 4.377387643866949e-05,
      "loss": 0.6596,
      "step": 818800
    },
    {
      "epoch": 7.472260748959778,
      "grad_norm": 3.841280698776245,
      "learning_rate": 4.3773116042533515e-05,
      "loss": 0.6648,
      "step": 818900
    },
    {
      "epoch": 7.473173224322943,
      "grad_norm": 4.145411491394043,
      "learning_rate": 4.377235564639755e-05,
      "loss": 0.6963,
      "step": 819000
    },
    {
      "epoch": 7.474085699686109,
      "grad_norm": 4.632399082183838,
      "learning_rate": 4.3771595250261575e-05,
      "loss": 0.7132,
      "step": 819100
    },
    {
      "epoch": 7.474998175049274,
      "grad_norm": 3.1326963901519775,
      "learning_rate": 4.3770834854125606e-05,
      "loss": 0.7012,
      "step": 819200
    },
    {
      "epoch": 7.475910650412439,
      "grad_norm": 4.382837772369385,
      "learning_rate": 4.3770074457989636e-05,
      "loss": 0.7247,
      "step": 819300
    },
    {
      "epoch": 7.476823125775604,
      "grad_norm": 4.352077484130859,
      "learning_rate": 4.3769314061853666e-05,
      "loss": 0.6723,
      "step": 819400
    },
    {
      "epoch": 7.477735601138769,
      "grad_norm": 4.566636562347412,
      "learning_rate": 4.3768553665717696e-05,
      "loss": 0.6732,
      "step": 819500
    },
    {
      "epoch": 7.478648076501934,
      "grad_norm": 4.684793949127197,
      "learning_rate": 4.3767793269581726e-05,
      "loss": 0.7023,
      "step": 819600
    },
    {
      "epoch": 7.4795605518651,
      "grad_norm": 4.580609321594238,
      "learning_rate": 4.376703287344575e-05,
      "loss": 0.6944,
      "step": 819700
    },
    {
      "epoch": 7.480473027228265,
      "grad_norm": 4.413619518280029,
      "learning_rate": 4.3766272477309786e-05,
      "loss": 0.704,
      "step": 819800
    },
    {
      "epoch": 7.48138550259143,
      "grad_norm": 4.022915363311768,
      "learning_rate": 4.376551208117381e-05,
      "loss": 0.6946,
      "step": 819900
    },
    {
      "epoch": 7.4822979779545955,
      "grad_norm": 3.7413928508758545,
      "learning_rate": 4.376475168503784e-05,
      "loss": 0.6963,
      "step": 820000
    },
    {
      "epoch": 7.483210453317761,
      "grad_norm": 4.0756001472473145,
      "learning_rate": 4.376399128890187e-05,
      "loss": 0.6959,
      "step": 820100
    },
    {
      "epoch": 7.484122928680925,
      "grad_norm": 3.146135091781616,
      "learning_rate": 4.37632308927659e-05,
      "loss": 0.6973,
      "step": 820200
    },
    {
      "epoch": 7.4850354040440905,
      "grad_norm": 3.9242255687713623,
      "learning_rate": 4.376247049662992e-05,
      "loss": 0.6915,
      "step": 820300
    },
    {
      "epoch": 7.485947879407256,
      "grad_norm": 4.6294026374816895,
      "learning_rate": 4.376171010049396e-05,
      "loss": 0.7124,
      "step": 820400
    },
    {
      "epoch": 7.486860354770421,
      "grad_norm": 4.138352394104004,
      "learning_rate": 4.376094970435798e-05,
      "loss": 0.6658,
      "step": 820500
    },
    {
      "epoch": 7.487772830133586,
      "grad_norm": 4.147820949554443,
      "learning_rate": 4.376018930822201e-05,
      "loss": 0.7221,
      "step": 820600
    },
    {
      "epoch": 7.488685305496752,
      "grad_norm": 2.987393379211426,
      "learning_rate": 4.375942891208604e-05,
      "loss": 0.6644,
      "step": 820700
    },
    {
      "epoch": 7.489597780859917,
      "grad_norm": 3.866220712661743,
      "learning_rate": 4.375866851595007e-05,
      "loss": 0.688,
      "step": 820800
    },
    {
      "epoch": 7.490510256223082,
      "grad_norm": 5.262596607208252,
      "learning_rate": 4.37579081198141e-05,
      "loss": 0.6853,
      "step": 820900
    },
    {
      "epoch": 7.491422731586248,
      "grad_norm": 3.8340086936950684,
      "learning_rate": 4.3757147723678126e-05,
      "loss": 0.7173,
      "step": 821000
    },
    {
      "epoch": 7.492335206949412,
      "grad_norm": 4.083425998687744,
      "learning_rate": 4.3756387327542156e-05,
      "loss": 0.6746,
      "step": 821100
    },
    {
      "epoch": 7.493247682312577,
      "grad_norm": 4.152157306671143,
      "learning_rate": 4.3755626931406187e-05,
      "loss": 0.6949,
      "step": 821200
    },
    {
      "epoch": 7.494160157675743,
      "grad_norm": 4.682239532470703,
      "learning_rate": 4.375486653527022e-05,
      "loss": 0.7333,
      "step": 821300
    },
    {
      "epoch": 7.495072633038908,
      "grad_norm": 5.178226470947266,
      "learning_rate": 4.375410613913424e-05,
      "loss": 0.6783,
      "step": 821400
    },
    {
      "epoch": 7.495985108402073,
      "grad_norm": 3.9742045402526855,
      "learning_rate": 4.375334574299828e-05,
      "loss": 0.6888,
      "step": 821500
    },
    {
      "epoch": 7.4968975837652385,
      "grad_norm": 3.544782876968384,
      "learning_rate": 4.37525853468623e-05,
      "loss": 0.7022,
      "step": 821600
    },
    {
      "epoch": 7.497810059128404,
      "grad_norm": 3.898430824279785,
      "learning_rate": 4.375182495072633e-05,
      "loss": 0.6726,
      "step": 821700
    },
    {
      "epoch": 7.498722534491569,
      "grad_norm": 5.01003360748291,
      "learning_rate": 4.375106455459036e-05,
      "loss": 0.6781,
      "step": 821800
    },
    {
      "epoch": 7.4996350098547335,
      "grad_norm": 3.831519842147827,
      "learning_rate": 4.375030415845439e-05,
      "loss": 0.6797,
      "step": 821900
    },
    {
      "epoch": 7.500547485217899,
      "grad_norm": 3.982713460922241,
      "learning_rate": 4.374954376231842e-05,
      "loss": 0.6915,
      "step": 822000
    },
    {
      "epoch": 7.501459960581064,
      "grad_norm": 4.075774669647217,
      "learning_rate": 4.374878336618245e-05,
      "loss": 0.6997,
      "step": 822100
    },
    {
      "epoch": 7.502372435944229,
      "grad_norm": 3.966477870941162,
      "learning_rate": 4.3748022970046474e-05,
      "loss": 0.6915,
      "step": 822200
    },
    {
      "epoch": 7.503284911307395,
      "grad_norm": 3.678098201751709,
      "learning_rate": 4.374726257391051e-05,
      "loss": 0.7291,
      "step": 822300
    },
    {
      "epoch": 7.50419738667056,
      "grad_norm": 3.4443130493164062,
      "learning_rate": 4.3746502177774534e-05,
      "loss": 0.7232,
      "step": 822400
    },
    {
      "epoch": 7.505109862033725,
      "grad_norm": 4.751734733581543,
      "learning_rate": 4.3745741781638564e-05,
      "loss": 0.719,
      "step": 822500
    },
    {
      "epoch": 7.506022337396891,
      "grad_norm": 3.869203567504883,
      "learning_rate": 4.3744981385502594e-05,
      "loss": 0.6921,
      "step": 822600
    },
    {
      "epoch": 7.506934812760056,
      "grad_norm": 3.9284300804138184,
      "learning_rate": 4.3744220989366624e-05,
      "loss": 0.6965,
      "step": 822700
    },
    {
      "epoch": 7.50784728812322,
      "grad_norm": 4.540218830108643,
      "learning_rate": 4.3743460593230654e-05,
      "loss": 0.7054,
      "step": 822800
    },
    {
      "epoch": 7.508759763486386,
      "grad_norm": 4.2862348556518555,
      "learning_rate": 4.3742700197094684e-05,
      "loss": 0.6747,
      "step": 822900
    },
    {
      "epoch": 7.509672238849551,
      "grad_norm": 3.23042368888855,
      "learning_rate": 4.374193980095871e-05,
      "loss": 0.7064,
      "step": 823000
    },
    {
      "epoch": 7.510584714212716,
      "grad_norm": 4.514774322509766,
      "learning_rate": 4.374117940482274e-05,
      "loss": 0.686,
      "step": 823100
    },
    {
      "epoch": 7.5114971895758815,
      "grad_norm": 3.4577839374542236,
      "learning_rate": 4.374041900868677e-05,
      "loss": 0.7064,
      "step": 823200
    },
    {
      "epoch": 7.512409664939047,
      "grad_norm": 4.218812942504883,
      "learning_rate": 4.37396586125508e-05,
      "loss": 0.7092,
      "step": 823300
    },
    {
      "epoch": 7.513322140302212,
      "grad_norm": 3.7847704887390137,
      "learning_rate": 4.373889821641483e-05,
      "loss": 0.7317,
      "step": 823400
    },
    {
      "epoch": 7.5142346156653765,
      "grad_norm": 3.544926643371582,
      "learning_rate": 4.373813782027886e-05,
      "loss": 0.68,
      "step": 823500
    },
    {
      "epoch": 7.515147091028542,
      "grad_norm": 4.131680488586426,
      "learning_rate": 4.373737742414288e-05,
      "loss": 0.6986,
      "step": 823600
    },
    {
      "epoch": 7.516059566391707,
      "grad_norm": 4.0436530113220215,
      "learning_rate": 4.373661702800691e-05,
      "loss": 0.7045,
      "step": 823700
    },
    {
      "epoch": 7.516972041754872,
      "grad_norm": 4.9372172355651855,
      "learning_rate": 4.373585663187094e-05,
      "loss": 0.731,
      "step": 823800
    },
    {
      "epoch": 7.517884517118038,
      "grad_norm": 3.664670467376709,
      "learning_rate": 4.373509623573497e-05,
      "loss": 0.7064,
      "step": 823900
    },
    {
      "epoch": 7.518796992481203,
      "grad_norm": 3.2372419834136963,
      "learning_rate": 4.3734335839599e-05,
      "loss": 0.6899,
      "step": 824000
    },
    {
      "epoch": 7.519709467844368,
      "grad_norm": 4.012200355529785,
      "learning_rate": 4.3733575443463025e-05,
      "loss": 0.7317,
      "step": 824100
    },
    {
      "epoch": 7.520621943207534,
      "grad_norm": 3.728733539581299,
      "learning_rate": 4.373281504732706e-05,
      "loss": 0.7342,
      "step": 824200
    },
    {
      "epoch": 7.521534418570699,
      "grad_norm": 4.813868045806885,
      "learning_rate": 4.3732054651191085e-05,
      "loss": 0.6619,
      "step": 824300
    },
    {
      "epoch": 7.522446893933864,
      "grad_norm": 4.167680740356445,
      "learning_rate": 4.3731294255055115e-05,
      "loss": 0.7056,
      "step": 824400
    },
    {
      "epoch": 7.523359369297029,
      "grad_norm": 4.077917575836182,
      "learning_rate": 4.3730533858919145e-05,
      "loss": 0.6828,
      "step": 824500
    },
    {
      "epoch": 7.524271844660194,
      "grad_norm": 3.61303448677063,
      "learning_rate": 4.3729773462783175e-05,
      "loss": 0.6567,
      "step": 824600
    },
    {
      "epoch": 7.525184320023359,
      "grad_norm": 3.6328482627868652,
      "learning_rate": 4.37290130666472e-05,
      "loss": 0.6868,
      "step": 824700
    },
    {
      "epoch": 7.5260967953865245,
      "grad_norm": 4.165770053863525,
      "learning_rate": 4.3728252670511235e-05,
      "loss": 0.6874,
      "step": 824800
    },
    {
      "epoch": 7.52700927074969,
      "grad_norm": 3.6820948123931885,
      "learning_rate": 4.372749227437526e-05,
      "loss": 0.6806,
      "step": 824900
    },
    {
      "epoch": 7.527921746112855,
      "grad_norm": 4.288150310516357,
      "learning_rate": 4.372673187823929e-05,
      "loss": 0.665,
      "step": 825000
    },
    {
      "epoch": 7.52883422147602,
      "grad_norm": 3.9250566959381104,
      "learning_rate": 4.372597148210332e-05,
      "loss": 0.6919,
      "step": 825100
    },
    {
      "epoch": 7.529746696839185,
      "grad_norm": 4.196986198425293,
      "learning_rate": 4.372521108596735e-05,
      "loss": 0.7387,
      "step": 825200
    },
    {
      "epoch": 7.53065917220235,
      "grad_norm": 5.190121650695801,
      "learning_rate": 4.372445068983138e-05,
      "loss": 0.7083,
      "step": 825300
    },
    {
      "epoch": 7.5315716475655154,
      "grad_norm": 3.8521411418914795,
      "learning_rate": 4.372369029369541e-05,
      "loss": 0.6944,
      "step": 825400
    },
    {
      "epoch": 7.532484122928681,
      "grad_norm": 4.236936569213867,
      "learning_rate": 4.372292989755943e-05,
      "loss": 0.7054,
      "step": 825500
    },
    {
      "epoch": 7.533396598291846,
      "grad_norm": 3.6554908752441406,
      "learning_rate": 4.372216950142347e-05,
      "loss": 0.7059,
      "step": 825600
    },
    {
      "epoch": 7.534309073655011,
      "grad_norm": 4.488113880157471,
      "learning_rate": 4.372140910528749e-05,
      "loss": 0.6965,
      "step": 825700
    },
    {
      "epoch": 7.535221549018177,
      "grad_norm": 4.021990776062012,
      "learning_rate": 4.372064870915152e-05,
      "loss": 0.7225,
      "step": 825800
    },
    {
      "epoch": 7.536134024381342,
      "grad_norm": 3.830493688583374,
      "learning_rate": 4.371988831301555e-05,
      "loss": 0.6831,
      "step": 825900
    },
    {
      "epoch": 7.537046499744507,
      "grad_norm": 4.194352149963379,
      "learning_rate": 4.371912791687958e-05,
      "loss": 0.7511,
      "step": 826000
    },
    {
      "epoch": 7.5379589751076725,
      "grad_norm": 4.081770896911621,
      "learning_rate": 4.3718367520743606e-05,
      "loss": 0.6848,
      "step": 826100
    },
    {
      "epoch": 7.538871450470837,
      "grad_norm": 4.357117652893066,
      "learning_rate": 4.371760712460764e-05,
      "loss": 0.6933,
      "step": 826200
    },
    {
      "epoch": 7.539783925834002,
      "grad_norm": 3.9806480407714844,
      "learning_rate": 4.3716846728471666e-05,
      "loss": 0.684,
      "step": 826300
    },
    {
      "epoch": 7.5406964011971676,
      "grad_norm": 4.1675801277160645,
      "learning_rate": 4.3716086332335696e-05,
      "loss": 0.6813,
      "step": 826400
    },
    {
      "epoch": 7.541608876560333,
      "grad_norm": 3.881354808807373,
      "learning_rate": 4.3715325936199726e-05,
      "loss": 0.6934,
      "step": 826500
    },
    {
      "epoch": 7.542521351923498,
      "grad_norm": 4.137505531311035,
      "learning_rate": 4.371456554006375e-05,
      "loss": 0.6659,
      "step": 826600
    },
    {
      "epoch": 7.5434338272866635,
      "grad_norm": 3.822427272796631,
      "learning_rate": 4.3713805143927786e-05,
      "loss": 0.6968,
      "step": 826700
    },
    {
      "epoch": 7.544346302649829,
      "grad_norm": 3.3752360343933105,
      "learning_rate": 4.371304474779181e-05,
      "loss": 0.6834,
      "step": 826800
    },
    {
      "epoch": 7.545258778012993,
      "grad_norm": 4.44993782043457,
      "learning_rate": 4.371228435165584e-05,
      "loss": 0.7153,
      "step": 826900
    },
    {
      "epoch": 7.5461712533761585,
      "grad_norm": 4.299799919128418,
      "learning_rate": 4.371152395551987e-05,
      "loss": 0.6703,
      "step": 827000
    },
    {
      "epoch": 7.547083728739324,
      "grad_norm": 3.3332912921905518,
      "learning_rate": 4.37107635593839e-05,
      "loss": 0.7072,
      "step": 827100
    },
    {
      "epoch": 7.547996204102489,
      "grad_norm": 4.06351900100708,
      "learning_rate": 4.371000316324792e-05,
      "loss": 0.677,
      "step": 827200
    },
    {
      "epoch": 7.548908679465654,
      "grad_norm": 3.370919704437256,
      "learning_rate": 4.370924276711196e-05,
      "loss": 0.6849,
      "step": 827300
    },
    {
      "epoch": 7.54982115482882,
      "grad_norm": 4.032772541046143,
      "learning_rate": 4.370848237097598e-05,
      "loss": 0.7476,
      "step": 827400
    },
    {
      "epoch": 7.550733630191985,
      "grad_norm": 3.965735673904419,
      "learning_rate": 4.370772197484001e-05,
      "loss": 0.6733,
      "step": 827500
    },
    {
      "epoch": 7.55164610555515,
      "grad_norm": 4.115852355957031,
      "learning_rate": 4.370696157870404e-05,
      "loss": 0.6881,
      "step": 827600
    },
    {
      "epoch": 7.552558580918316,
      "grad_norm": 3.7652809619903564,
      "learning_rate": 4.370620118256807e-05,
      "loss": 0.7074,
      "step": 827700
    },
    {
      "epoch": 7.553471056281481,
      "grad_norm": 3.3622002601623535,
      "learning_rate": 4.37054407864321e-05,
      "loss": 0.69,
      "step": 827800
    },
    {
      "epoch": 7.554383531644645,
      "grad_norm": 3.596393346786499,
      "learning_rate": 4.370468039029613e-05,
      "loss": 0.6809,
      "step": 827900
    },
    {
      "epoch": 7.555296007007811,
      "grad_norm": 4.028471946716309,
      "learning_rate": 4.3703919994160157e-05,
      "loss": 0.6695,
      "step": 828000
    },
    {
      "epoch": 7.556208482370976,
      "grad_norm": 3.8204493522644043,
      "learning_rate": 4.3703159598024193e-05,
      "loss": 0.6757,
      "step": 828100
    },
    {
      "epoch": 7.557120957734141,
      "grad_norm": 3.868447780609131,
      "learning_rate": 4.370239920188822e-05,
      "loss": 0.6815,
      "step": 828200
    },
    {
      "epoch": 7.5580334330973065,
      "grad_norm": 3.7369766235351562,
      "learning_rate": 4.370163880575225e-05,
      "loss": 0.6941,
      "step": 828300
    },
    {
      "epoch": 7.558945908460472,
      "grad_norm": 4.191002368927002,
      "learning_rate": 4.370087840961628e-05,
      "loss": 0.6894,
      "step": 828400
    },
    {
      "epoch": 7.559858383823637,
      "grad_norm": 3.2027854919433594,
      "learning_rate": 4.370011801348031e-05,
      "loss": 0.6825,
      "step": 828500
    },
    {
      "epoch": 7.5607708591868015,
      "grad_norm": 4.6186137199401855,
      "learning_rate": 4.369935761734433e-05,
      "loss": 0.7036,
      "step": 828600
    },
    {
      "epoch": 7.561683334549967,
      "grad_norm": 3.4578468799591064,
      "learning_rate": 4.369859722120837e-05,
      "loss": 0.7118,
      "step": 828700
    },
    {
      "epoch": 7.562595809913132,
      "grad_norm": 4.046095848083496,
      "learning_rate": 4.369783682507239e-05,
      "loss": 0.6942,
      "step": 828800
    },
    {
      "epoch": 7.563508285276297,
      "grad_norm": 5.1434760093688965,
      "learning_rate": 4.369707642893642e-05,
      "loss": 0.6925,
      "step": 828900
    },
    {
      "epoch": 7.564420760639463,
      "grad_norm": 4.134810447692871,
      "learning_rate": 4.369631603280045e-05,
      "loss": 0.6588,
      "step": 829000
    },
    {
      "epoch": 7.565333236002628,
      "grad_norm": 4.2955498695373535,
      "learning_rate": 4.369555563666448e-05,
      "loss": 0.718,
      "step": 829100
    },
    {
      "epoch": 7.566245711365793,
      "grad_norm": 3.9031059741973877,
      "learning_rate": 4.369479524052851e-05,
      "loss": 0.7005,
      "step": 829200
    },
    {
      "epoch": 7.567158186728959,
      "grad_norm": 3.520287275314331,
      "learning_rate": 4.369403484439254e-05,
      "loss": 0.6707,
      "step": 829300
    },
    {
      "epoch": 7.568070662092124,
      "grad_norm": 3.6776628494262695,
      "learning_rate": 4.3693274448256564e-05,
      "loss": 0.698,
      "step": 829400
    },
    {
      "epoch": 7.568983137455289,
      "grad_norm": 4.141726970672607,
      "learning_rate": 4.3692514052120594e-05,
      "loss": 0.6794,
      "step": 829500
    },
    {
      "epoch": 7.569895612818454,
      "grad_norm": 2.781644105911255,
      "learning_rate": 4.3691753655984624e-05,
      "loss": 0.6582,
      "step": 829600
    },
    {
      "epoch": 7.570808088181619,
      "grad_norm": 3.8696815967559814,
      "learning_rate": 4.369099325984865e-05,
      "loss": 0.7447,
      "step": 829700
    },
    {
      "epoch": 7.571720563544784,
      "grad_norm": 4.1930108070373535,
      "learning_rate": 4.3690232863712684e-05,
      "loss": 0.6778,
      "step": 829800
    },
    {
      "epoch": 7.5726330389079495,
      "grad_norm": 3.7408690452575684,
      "learning_rate": 4.368947246757671e-05,
      "loss": 0.7076,
      "step": 829900
    },
    {
      "epoch": 7.573545514271115,
      "grad_norm": 4.45097017288208,
      "learning_rate": 4.368871207144074e-05,
      "loss": 0.6884,
      "step": 830000
    },
    {
      "epoch": 7.57445798963428,
      "grad_norm": 3.9984583854675293,
      "learning_rate": 4.368795167530477e-05,
      "loss": 0.7023,
      "step": 830100
    },
    {
      "epoch": 7.575370464997445,
      "grad_norm": 3.3389997482299805,
      "learning_rate": 4.36871912791688e-05,
      "loss": 0.688,
      "step": 830200
    },
    {
      "epoch": 7.57628294036061,
      "grad_norm": 4.781709671020508,
      "learning_rate": 4.368643088303283e-05,
      "loss": 0.6723,
      "step": 830300
    },
    {
      "epoch": 7.577195415723775,
      "grad_norm": 3.201582908630371,
      "learning_rate": 4.368567048689686e-05,
      "loss": 0.6884,
      "step": 830400
    },
    {
      "epoch": 7.57810789108694,
      "grad_norm": 4.436415195465088,
      "learning_rate": 4.368491009076088e-05,
      "loss": 0.6942,
      "step": 830500
    },
    {
      "epoch": 7.579020366450106,
      "grad_norm": 4.650563716888428,
      "learning_rate": 4.368414969462492e-05,
      "loss": 0.7031,
      "step": 830600
    },
    {
      "epoch": 7.579932841813271,
      "grad_norm": 2.9957218170166016,
      "learning_rate": 4.368338929848894e-05,
      "loss": 0.7175,
      "step": 830700
    },
    {
      "epoch": 7.580845317176436,
      "grad_norm": 3.1953506469726562,
      "learning_rate": 4.368262890235297e-05,
      "loss": 0.6876,
      "step": 830800
    },
    {
      "epoch": 7.581757792539602,
      "grad_norm": 4.726193428039551,
      "learning_rate": 4.3681868506217e-05,
      "loss": 0.6914,
      "step": 830900
    },
    {
      "epoch": 7.582670267902767,
      "grad_norm": 4.671447277069092,
      "learning_rate": 4.368110811008103e-05,
      "loss": 0.7047,
      "step": 831000
    },
    {
      "epoch": 7.583582743265932,
      "grad_norm": 4.393194675445557,
      "learning_rate": 4.3680347713945055e-05,
      "loss": 0.7143,
      "step": 831100
    },
    {
      "epoch": 7.5844952186290975,
      "grad_norm": 4.5350775718688965,
      "learning_rate": 4.367958731780909e-05,
      "loss": 0.7236,
      "step": 831200
    },
    {
      "epoch": 7.585407693992262,
      "grad_norm": 5.109282493591309,
      "learning_rate": 4.3678826921673115e-05,
      "loss": 0.6969,
      "step": 831300
    },
    {
      "epoch": 7.586320169355427,
      "grad_norm": 5.462926864624023,
      "learning_rate": 4.3678066525537145e-05,
      "loss": 0.6541,
      "step": 831400
    },
    {
      "epoch": 7.5872326447185925,
      "grad_norm": 3.6163134574890137,
      "learning_rate": 4.3677306129401175e-05,
      "loss": 0.7051,
      "step": 831500
    },
    {
      "epoch": 7.588145120081758,
      "grad_norm": 4.295520305633545,
      "learning_rate": 4.3676545733265205e-05,
      "loss": 0.6809,
      "step": 831600
    },
    {
      "epoch": 7.589057595444923,
      "grad_norm": 4.211669921875,
      "learning_rate": 4.3675785337129235e-05,
      "loss": 0.6854,
      "step": 831700
    },
    {
      "epoch": 7.589970070808088,
      "grad_norm": 3.9282498359680176,
      "learning_rate": 4.3675024940993265e-05,
      "loss": 0.73,
      "step": 831800
    },
    {
      "epoch": 7.590882546171254,
      "grad_norm": 4.13392972946167,
      "learning_rate": 4.367426454485729e-05,
      "loss": 0.6637,
      "step": 831900
    },
    {
      "epoch": 7.591795021534418,
      "grad_norm": 4.7091240882873535,
      "learning_rate": 4.3673504148721325e-05,
      "loss": 0.7007,
      "step": 832000
    },
    {
      "epoch": 7.592707496897583,
      "grad_norm": 3.454125165939331,
      "learning_rate": 4.367274375258535e-05,
      "loss": 0.6896,
      "step": 832100
    },
    {
      "epoch": 7.593619972260749,
      "grad_norm": 5.05800724029541,
      "learning_rate": 4.367198335644937e-05,
      "loss": 0.7016,
      "step": 832200
    },
    {
      "epoch": 7.594532447623914,
      "grad_norm": 4.15768575668335,
      "learning_rate": 4.367122296031341e-05,
      "loss": 0.7095,
      "step": 832300
    },
    {
      "epoch": 7.595444922987079,
      "grad_norm": 4.145569801330566,
      "learning_rate": 4.367046256417743e-05,
      "loss": 0.6925,
      "step": 832400
    },
    {
      "epoch": 7.596357398350245,
      "grad_norm": 3.3839271068573,
      "learning_rate": 4.366970216804146e-05,
      "loss": 0.7449,
      "step": 832500
    },
    {
      "epoch": 7.59726987371341,
      "grad_norm": 3.603264331817627,
      "learning_rate": 4.366894177190549e-05,
      "loss": 0.7099,
      "step": 832600
    },
    {
      "epoch": 7.598182349076575,
      "grad_norm": 4.35944128036499,
      "learning_rate": 4.366818137576952e-05,
      "loss": 0.7157,
      "step": 832700
    },
    {
      "epoch": 7.5990948244397405,
      "grad_norm": 3.939987897872925,
      "learning_rate": 4.366742097963355e-05,
      "loss": 0.7225,
      "step": 832800
    },
    {
      "epoch": 7.600007299802906,
      "grad_norm": 4.445362091064453,
      "learning_rate": 4.366666058349758e-05,
      "loss": 0.6873,
      "step": 832900
    },
    {
      "epoch": 7.60091977516607,
      "grad_norm": 3.7213358879089355,
      "learning_rate": 4.3665900187361606e-05,
      "loss": 0.6948,
      "step": 833000
    },
    {
      "epoch": 7.6018322505292355,
      "grad_norm": 3.507244348526001,
      "learning_rate": 4.366513979122564e-05,
      "loss": 0.6993,
      "step": 833100
    },
    {
      "epoch": 7.602744725892401,
      "grad_norm": 4.056572437286377,
      "learning_rate": 4.3664379395089666e-05,
      "loss": 0.7003,
      "step": 833200
    },
    {
      "epoch": 7.603657201255566,
      "grad_norm": 4.233511447906494,
      "learning_rate": 4.3663618998953696e-05,
      "loss": 0.732,
      "step": 833300
    },
    {
      "epoch": 7.604569676618731,
      "grad_norm": 4.228979587554932,
      "learning_rate": 4.3662858602817726e-05,
      "loss": 0.6432,
      "step": 833400
    },
    {
      "epoch": 7.605482151981897,
      "grad_norm": 4.127023696899414,
      "learning_rate": 4.3662098206681756e-05,
      "loss": 0.6993,
      "step": 833500
    },
    {
      "epoch": 7.606394627345062,
      "grad_norm": 3.986886978149414,
      "learning_rate": 4.366133781054578e-05,
      "loss": 0.7072,
      "step": 833600
    },
    {
      "epoch": 7.607307102708226,
      "grad_norm": 4.117677688598633,
      "learning_rate": 4.3660577414409816e-05,
      "loss": 0.7067,
      "step": 833700
    },
    {
      "epoch": 7.608219578071392,
      "grad_norm": 4.759803771972656,
      "learning_rate": 4.365981701827384e-05,
      "loss": 0.6815,
      "step": 833800
    },
    {
      "epoch": 7.609132053434557,
      "grad_norm": 3.9599320888519287,
      "learning_rate": 4.365905662213787e-05,
      "loss": 0.6645,
      "step": 833900
    },
    {
      "epoch": 7.610044528797722,
      "grad_norm": 3.7746634483337402,
      "learning_rate": 4.36582962260019e-05,
      "loss": 0.6642,
      "step": 834000
    },
    {
      "epoch": 7.610957004160888,
      "grad_norm": 3.984276056289673,
      "learning_rate": 4.365753582986593e-05,
      "loss": 0.7032,
      "step": 834100
    },
    {
      "epoch": 7.611869479524053,
      "grad_norm": 3.7212278842926025,
      "learning_rate": 4.365677543372996e-05,
      "loss": 0.7008,
      "step": 834200
    },
    {
      "epoch": 7.612781954887218,
      "grad_norm": 4.124815940856934,
      "learning_rate": 4.365601503759399e-05,
      "loss": 0.7031,
      "step": 834300
    },
    {
      "epoch": 7.6136944302503835,
      "grad_norm": 3.649230718612671,
      "learning_rate": 4.365525464145801e-05,
      "loss": 0.7079,
      "step": 834400
    },
    {
      "epoch": 7.614606905613549,
      "grad_norm": 3.7245044708251953,
      "learning_rate": 4.365449424532205e-05,
      "loss": 0.721,
      "step": 834500
    },
    {
      "epoch": 7.615519380976714,
      "grad_norm": 4.341365337371826,
      "learning_rate": 4.365373384918607e-05,
      "loss": 0.7013,
      "step": 834600
    },
    {
      "epoch": 7.6164318563398785,
      "grad_norm": 4.095317840576172,
      "learning_rate": 4.36529734530501e-05,
      "loss": 0.7308,
      "step": 834700
    },
    {
      "epoch": 7.617344331703044,
      "grad_norm": 4.403505802154541,
      "learning_rate": 4.365221305691413e-05,
      "loss": 0.7171,
      "step": 834800
    },
    {
      "epoch": 7.618256807066209,
      "grad_norm": 4.377169609069824,
      "learning_rate": 4.3651452660778163e-05,
      "loss": 0.6975,
      "step": 834900
    },
    {
      "epoch": 7.619169282429374,
      "grad_norm": 4.772034168243408,
      "learning_rate": 4.365069226464219e-05,
      "loss": 0.6813,
      "step": 835000
    },
    {
      "epoch": 7.62008175779254,
      "grad_norm": 4.755535125732422,
      "learning_rate": 4.364993186850622e-05,
      "loss": 0.6323,
      "step": 835100
    },
    {
      "epoch": 7.620994233155705,
      "grad_norm": 3.8567821979522705,
      "learning_rate": 4.364917147237025e-05,
      "loss": 0.7049,
      "step": 835200
    },
    {
      "epoch": 7.62190670851887,
      "grad_norm": 3.736551523208618,
      "learning_rate": 4.364841107623428e-05,
      "loss": 0.7097,
      "step": 835300
    },
    {
      "epoch": 7.622819183882035,
      "grad_norm": 3.2088937759399414,
      "learning_rate": 4.364765068009831e-05,
      "loss": 0.6934,
      "step": 835400
    },
    {
      "epoch": 7.6237316592452,
      "grad_norm": 4.253748893737793,
      "learning_rate": 4.364689028396233e-05,
      "loss": 0.658,
      "step": 835500
    },
    {
      "epoch": 7.624644134608365,
      "grad_norm": 4.3632307052612305,
      "learning_rate": 4.364612988782637e-05,
      "loss": 0.7188,
      "step": 835600
    },
    {
      "epoch": 7.625556609971531,
      "grad_norm": 4.807528972625732,
      "learning_rate": 4.364536949169039e-05,
      "loss": 0.6804,
      "step": 835700
    },
    {
      "epoch": 7.626469085334696,
      "grad_norm": 4.111430644989014,
      "learning_rate": 4.364460909555442e-05,
      "loss": 0.722,
      "step": 835800
    },
    {
      "epoch": 7.627381560697861,
      "grad_norm": 4.732369422912598,
      "learning_rate": 4.364384869941845e-05,
      "loss": 0.6838,
      "step": 835900
    },
    {
      "epoch": 7.6282940360610265,
      "grad_norm": 4.2442402839660645,
      "learning_rate": 4.364308830328248e-05,
      "loss": 0.6692,
      "step": 836000
    },
    {
      "epoch": 7.629206511424192,
      "grad_norm": 4.170345783233643,
      "learning_rate": 4.364232790714651e-05,
      "loss": 0.6976,
      "step": 836100
    },
    {
      "epoch": 7.630118986787357,
      "grad_norm": 3.8851583003997803,
      "learning_rate": 4.364156751101054e-05,
      "loss": 0.6714,
      "step": 836200
    },
    {
      "epoch": 7.631031462150522,
      "grad_norm": 3.6845622062683105,
      "learning_rate": 4.3640807114874564e-05,
      "loss": 0.6751,
      "step": 836300
    },
    {
      "epoch": 7.631943937513687,
      "grad_norm": 5.060897350311279,
      "learning_rate": 4.3640046718738594e-05,
      "loss": 0.6954,
      "step": 836400
    },
    {
      "epoch": 7.632856412876852,
      "grad_norm": 4.863738059997559,
      "learning_rate": 4.3639286322602624e-05,
      "loss": 0.7116,
      "step": 836500
    },
    {
      "epoch": 7.633768888240017,
      "grad_norm": 4.520031929016113,
      "learning_rate": 4.3638525926466654e-05,
      "loss": 0.6704,
      "step": 836600
    },
    {
      "epoch": 7.634681363603183,
      "grad_norm": 3.8468334674835205,
      "learning_rate": 4.3637765530330684e-05,
      "loss": 0.7076,
      "step": 836700
    },
    {
      "epoch": 7.635593838966348,
      "grad_norm": 2.3554232120513916,
      "learning_rate": 4.3637005134194714e-05,
      "loss": 0.7061,
      "step": 836800
    },
    {
      "epoch": 7.636506314329513,
      "grad_norm": 3.812086343765259,
      "learning_rate": 4.363624473805874e-05,
      "loss": 0.6587,
      "step": 836900
    },
    {
      "epoch": 7.637418789692679,
      "grad_norm": 4.588507175445557,
      "learning_rate": 4.3635484341922775e-05,
      "loss": 0.699,
      "step": 837000
    },
    {
      "epoch": 7.638331265055843,
      "grad_norm": 3.4698574542999268,
      "learning_rate": 4.36347239457868e-05,
      "loss": 0.7145,
      "step": 837100
    },
    {
      "epoch": 7.639243740419008,
      "grad_norm": 4.979612827301025,
      "learning_rate": 4.363396354965083e-05,
      "loss": 0.6764,
      "step": 837200
    },
    {
      "epoch": 7.640156215782174,
      "grad_norm": 3.8250253200531006,
      "learning_rate": 4.363320315351486e-05,
      "loss": 0.7028,
      "step": 837300
    },
    {
      "epoch": 7.641068691145339,
      "grad_norm": 3.755314588546753,
      "learning_rate": 4.363244275737889e-05,
      "loss": 0.7046,
      "step": 837400
    },
    {
      "epoch": 7.641981166508504,
      "grad_norm": 4.318455696105957,
      "learning_rate": 4.363168236124292e-05,
      "loss": 0.6998,
      "step": 837500
    },
    {
      "epoch": 7.6428936418716695,
      "grad_norm": 4.198925495147705,
      "learning_rate": 4.363092196510695e-05,
      "loss": 0.6671,
      "step": 837600
    },
    {
      "epoch": 7.643806117234835,
      "grad_norm": 4.466520309448242,
      "learning_rate": 4.363016156897097e-05,
      "loss": 0.6975,
      "step": 837700
    },
    {
      "epoch": 7.644718592598,
      "grad_norm": 4.865048408508301,
      "learning_rate": 4.362940117283501e-05,
      "loss": 0.7048,
      "step": 837800
    },
    {
      "epoch": 7.645631067961165,
      "grad_norm": 4.866092205047607,
      "learning_rate": 4.362864077669903e-05,
      "loss": 0.6817,
      "step": 837900
    },
    {
      "epoch": 7.646543543324331,
      "grad_norm": 3.67604923248291,
      "learning_rate": 4.3627880380563055e-05,
      "loss": 0.671,
      "step": 838000
    },
    {
      "epoch": 7.647456018687495,
      "grad_norm": 4.6847076416015625,
      "learning_rate": 4.362711998442709e-05,
      "loss": 0.6948,
      "step": 838100
    },
    {
      "epoch": 7.64836849405066,
      "grad_norm": 2.3243672847747803,
      "learning_rate": 4.3626359588291115e-05,
      "loss": 0.7183,
      "step": 838200
    },
    {
      "epoch": 7.649280969413826,
      "grad_norm": 5.2218098640441895,
      "learning_rate": 4.3625599192155145e-05,
      "loss": 0.7228,
      "step": 838300
    },
    {
      "epoch": 7.650193444776991,
      "grad_norm": 3.4436283111572266,
      "learning_rate": 4.3624838796019175e-05,
      "loss": 0.6922,
      "step": 838400
    },
    {
      "epoch": 7.651105920140156,
      "grad_norm": 4.251325607299805,
      "learning_rate": 4.3624078399883205e-05,
      "loss": 0.7018,
      "step": 838500
    },
    {
      "epoch": 7.652018395503322,
      "grad_norm": 2.7395238876342773,
      "learning_rate": 4.3623318003747235e-05,
      "loss": 0.6942,
      "step": 838600
    },
    {
      "epoch": 7.652930870866487,
      "grad_norm": 3.7291224002838135,
      "learning_rate": 4.3622557607611265e-05,
      "loss": 0.6955,
      "step": 838700
    },
    {
      "epoch": 7.653843346229651,
      "grad_norm": 4.349228382110596,
      "learning_rate": 4.362179721147529e-05,
      "loss": 0.671,
      "step": 838800
    },
    {
      "epoch": 7.654755821592817,
      "grad_norm": 3.74143385887146,
      "learning_rate": 4.3621036815339325e-05,
      "loss": 0.6896,
      "step": 838900
    },
    {
      "epoch": 7.655668296955982,
      "grad_norm": 4.236349105834961,
      "learning_rate": 4.362027641920335e-05,
      "loss": 0.731,
      "step": 839000
    },
    {
      "epoch": 7.656580772319147,
      "grad_norm": 3.8991634845733643,
      "learning_rate": 4.361951602306738e-05,
      "loss": 0.7122,
      "step": 839100
    },
    {
      "epoch": 7.6574932476823125,
      "grad_norm": 3.6338789463043213,
      "learning_rate": 4.361875562693141e-05,
      "loss": 0.65,
      "step": 839200
    },
    {
      "epoch": 7.658405723045478,
      "grad_norm": 3.7549610137939453,
      "learning_rate": 4.361799523079544e-05,
      "loss": 0.7064,
      "step": 839300
    },
    {
      "epoch": 7.659318198408643,
      "grad_norm": 4.374834060668945,
      "learning_rate": 4.361723483465946e-05,
      "loss": 0.6816,
      "step": 839400
    },
    {
      "epoch": 7.660230673771808,
      "grad_norm": 2.9358136653900146,
      "learning_rate": 4.36164744385235e-05,
      "loss": 0.6953,
      "step": 839500
    },
    {
      "epoch": 7.661143149134974,
      "grad_norm": 3.9897820949554443,
      "learning_rate": 4.361571404238752e-05,
      "loss": 0.7123,
      "step": 839600
    },
    {
      "epoch": 7.662055624498139,
      "grad_norm": 4.111044883728027,
      "learning_rate": 4.361495364625155e-05,
      "loss": 0.6825,
      "step": 839700
    },
    {
      "epoch": 7.6629680998613035,
      "grad_norm": 4.161956787109375,
      "learning_rate": 4.361419325011558e-05,
      "loss": 0.7049,
      "step": 839800
    },
    {
      "epoch": 7.663880575224469,
      "grad_norm": 5.062530517578125,
      "learning_rate": 4.361343285397961e-05,
      "loss": 0.7132,
      "step": 839900
    },
    {
      "epoch": 7.664793050587634,
      "grad_norm": 3.59728741645813,
      "learning_rate": 4.361267245784364e-05,
      "loss": 0.6895,
      "step": 840000
    },
    {
      "epoch": 7.665705525950799,
      "grad_norm": 3.978452205657959,
      "learning_rate": 4.361191206170767e-05,
      "loss": 0.6724,
      "step": 840100
    },
    {
      "epoch": 7.666618001313965,
      "grad_norm": 3.3608036041259766,
      "learning_rate": 4.3611151665571696e-05,
      "loss": 0.6854,
      "step": 840200
    },
    {
      "epoch": 7.66753047667713,
      "grad_norm": 3.4444196224212646,
      "learning_rate": 4.361039126943573e-05,
      "loss": 0.6767,
      "step": 840300
    },
    {
      "epoch": 7.668442952040295,
      "grad_norm": 5.2795891761779785,
      "learning_rate": 4.3609630873299756e-05,
      "loss": 0.7178,
      "step": 840400
    },
    {
      "epoch": 7.66935542740346,
      "grad_norm": 4.020692348480225,
      "learning_rate": 4.3608870477163786e-05,
      "loss": 0.7118,
      "step": 840500
    },
    {
      "epoch": 7.670267902766625,
      "grad_norm": 4.14314079284668,
      "learning_rate": 4.3608110081027816e-05,
      "loss": 0.6675,
      "step": 840600
    },
    {
      "epoch": 7.67118037812979,
      "grad_norm": 4.264001369476318,
      "learning_rate": 4.360734968489184e-05,
      "loss": 0.7016,
      "step": 840700
    },
    {
      "epoch": 7.6720928534929556,
      "grad_norm": 3.927081346511841,
      "learning_rate": 4.360658928875587e-05,
      "loss": 0.6783,
      "step": 840800
    },
    {
      "epoch": 7.673005328856121,
      "grad_norm": 4.4424052238464355,
      "learning_rate": 4.36058288926199e-05,
      "loss": 0.7319,
      "step": 840900
    },
    {
      "epoch": 7.673917804219286,
      "grad_norm": 4.575034141540527,
      "learning_rate": 4.360506849648393e-05,
      "loss": 0.6691,
      "step": 841000
    },
    {
      "epoch": 7.6748302795824515,
      "grad_norm": 4.278447151184082,
      "learning_rate": 4.360430810034796e-05,
      "loss": 0.7179,
      "step": 841100
    },
    {
      "epoch": 7.675742754945617,
      "grad_norm": 4.774869441986084,
      "learning_rate": 4.360354770421199e-05,
      "loss": 0.6771,
      "step": 841200
    },
    {
      "epoch": 7.676655230308782,
      "grad_norm": 4.533443927764893,
      "learning_rate": 4.360278730807601e-05,
      "loss": 0.7408,
      "step": 841300
    },
    {
      "epoch": 7.677567705671947,
      "grad_norm": 4.064189910888672,
      "learning_rate": 4.360202691194005e-05,
      "loss": 0.7074,
      "step": 841400
    },
    {
      "epoch": 7.678480181035112,
      "grad_norm": 3.637604236602783,
      "learning_rate": 4.360126651580407e-05,
      "loss": 0.6992,
      "step": 841500
    },
    {
      "epoch": 7.679392656398277,
      "grad_norm": 3.6784133911132812,
      "learning_rate": 4.3600506119668103e-05,
      "loss": 0.6863,
      "step": 841600
    },
    {
      "epoch": 7.680305131761442,
      "grad_norm": 4.0239176750183105,
      "learning_rate": 4.3599745723532133e-05,
      "loss": 0.6964,
      "step": 841700
    },
    {
      "epoch": 7.681217607124608,
      "grad_norm": 3.8291873931884766,
      "learning_rate": 4.3598985327396164e-05,
      "loss": 0.6945,
      "step": 841800
    },
    {
      "epoch": 7.682130082487773,
      "grad_norm": 2.624408483505249,
      "learning_rate": 4.359822493126019e-05,
      "loss": 0.6929,
      "step": 841900
    },
    {
      "epoch": 7.683042557850938,
      "grad_norm": 3.675644874572754,
      "learning_rate": 4.3597464535124224e-05,
      "loss": 0.7214,
      "step": 842000
    },
    {
      "epoch": 7.683955033214104,
      "grad_norm": 4.690632343292236,
      "learning_rate": 4.359670413898825e-05,
      "loss": 0.7091,
      "step": 842100
    },
    {
      "epoch": 7.684867508577268,
      "grad_norm": 3.606598138809204,
      "learning_rate": 4.359594374285228e-05,
      "loss": 0.7059,
      "step": 842200
    },
    {
      "epoch": 7.685779983940433,
      "grad_norm": 3.8486430644989014,
      "learning_rate": 4.359518334671631e-05,
      "loss": 0.7219,
      "step": 842300
    },
    {
      "epoch": 7.686692459303599,
      "grad_norm": 4.222850322723389,
      "learning_rate": 4.359442295058034e-05,
      "loss": 0.6983,
      "step": 842400
    },
    {
      "epoch": 7.687604934666764,
      "grad_norm": 3.250622034072876,
      "learning_rate": 4.359366255444437e-05,
      "loss": 0.7271,
      "step": 842500
    },
    {
      "epoch": 7.688517410029929,
      "grad_norm": 4.217115879058838,
      "learning_rate": 4.35929021583084e-05,
      "loss": 0.6295,
      "step": 842600
    },
    {
      "epoch": 7.6894298853930945,
      "grad_norm": 3.284071207046509,
      "learning_rate": 4.359214176217242e-05,
      "loss": 0.7037,
      "step": 842700
    },
    {
      "epoch": 7.69034236075626,
      "grad_norm": 3.577777862548828,
      "learning_rate": 4.359138136603646e-05,
      "loss": 0.6558,
      "step": 842800
    },
    {
      "epoch": 7.691254836119425,
      "grad_norm": 4.888125419616699,
      "learning_rate": 4.359062096990048e-05,
      "loss": 0.6969,
      "step": 842900
    },
    {
      "epoch": 7.69216731148259,
      "grad_norm": 4.785491466522217,
      "learning_rate": 4.358986057376451e-05,
      "loss": 0.712,
      "step": 843000
    },
    {
      "epoch": 7.693079786845756,
      "grad_norm": 4.207540035247803,
      "learning_rate": 4.358910017762854e-05,
      "loss": 0.7271,
      "step": 843100
    },
    {
      "epoch": 7.69399226220892,
      "grad_norm": 4.492232799530029,
      "learning_rate": 4.358833978149257e-05,
      "loss": 0.6836,
      "step": 843200
    },
    {
      "epoch": 7.694904737572085,
      "grad_norm": 4.419890880584717,
      "learning_rate": 4.3587579385356594e-05,
      "loss": 0.7303,
      "step": 843300
    },
    {
      "epoch": 7.695817212935251,
      "grad_norm": 3.895454168319702,
      "learning_rate": 4.358681898922063e-05,
      "loss": 0.6938,
      "step": 843400
    },
    {
      "epoch": 7.696729688298416,
      "grad_norm": 4.173753261566162,
      "learning_rate": 4.3586058593084654e-05,
      "loss": 0.6674,
      "step": 843500
    },
    {
      "epoch": 7.697642163661581,
      "grad_norm": 4.391902923583984,
      "learning_rate": 4.3585298196948684e-05,
      "loss": 0.6827,
      "step": 843600
    },
    {
      "epoch": 7.698554639024747,
      "grad_norm": 3.6962802410125732,
      "learning_rate": 4.3584537800812714e-05,
      "loss": 0.72,
      "step": 843700
    },
    {
      "epoch": 7.699467114387912,
      "grad_norm": 3.8635013103485107,
      "learning_rate": 4.358377740467674e-05,
      "loss": 0.6961,
      "step": 843800
    },
    {
      "epoch": 7.700379589751076,
      "grad_norm": 4.633449554443359,
      "learning_rate": 4.3583017008540775e-05,
      "loss": 0.6661,
      "step": 843900
    },
    {
      "epoch": 7.701292065114242,
      "grad_norm": 3.518231153488159,
      "learning_rate": 4.35822566124048e-05,
      "loss": 0.6983,
      "step": 844000
    },
    {
      "epoch": 7.702204540477407,
      "grad_norm": 3.4609429836273193,
      "learning_rate": 4.358149621626883e-05,
      "loss": 0.7057,
      "step": 844100
    },
    {
      "epoch": 7.703117015840572,
      "grad_norm": 3.6925413608551025,
      "learning_rate": 4.358073582013286e-05,
      "loss": 0.7188,
      "step": 844200
    },
    {
      "epoch": 7.7040294912037375,
      "grad_norm": 4.307399749755859,
      "learning_rate": 4.357997542399689e-05,
      "loss": 0.7083,
      "step": 844300
    },
    {
      "epoch": 7.704941966566903,
      "grad_norm": 3.943211793899536,
      "learning_rate": 4.357921502786091e-05,
      "loss": 0.6827,
      "step": 844400
    },
    {
      "epoch": 7.705854441930068,
      "grad_norm": 4.41214656829834,
      "learning_rate": 4.357845463172495e-05,
      "loss": 0.7033,
      "step": 844500
    },
    {
      "epoch": 7.706766917293233,
      "grad_norm": 3.682692766189575,
      "learning_rate": 4.357769423558897e-05,
      "loss": 0.7114,
      "step": 844600
    },
    {
      "epoch": 7.707679392656399,
      "grad_norm": 4.187567234039307,
      "learning_rate": 4.3576933839453e-05,
      "loss": 0.7263,
      "step": 844700
    },
    {
      "epoch": 7.708591868019563,
      "grad_norm": 5.490072727203369,
      "learning_rate": 4.357617344331703e-05,
      "loss": 0.6885,
      "step": 844800
    },
    {
      "epoch": 7.709504343382728,
      "grad_norm": 2.2237164974212646,
      "learning_rate": 4.357541304718106e-05,
      "loss": 0.6941,
      "step": 844900
    },
    {
      "epoch": 7.710416818745894,
      "grad_norm": 3.4317753314971924,
      "learning_rate": 4.357465265104509e-05,
      "loss": 0.7249,
      "step": 845000
    },
    {
      "epoch": 7.711329294109059,
      "grad_norm": 3.756511926651001,
      "learning_rate": 4.357389225490912e-05,
      "loss": 0.7027,
      "step": 845100
    },
    {
      "epoch": 7.712241769472224,
      "grad_norm": 4.627763271331787,
      "learning_rate": 4.3573131858773145e-05,
      "loss": 0.6861,
      "step": 845200
    },
    {
      "epoch": 7.71315424483539,
      "grad_norm": 3.837630033493042,
      "learning_rate": 4.357237146263718e-05,
      "loss": 0.7084,
      "step": 845300
    },
    {
      "epoch": 7.714066720198555,
      "grad_norm": 4.1799397468566895,
      "learning_rate": 4.3571611066501205e-05,
      "loss": 0.7,
      "step": 845400
    },
    {
      "epoch": 7.71497919556172,
      "grad_norm": 3.559089183807373,
      "learning_rate": 4.3570850670365235e-05,
      "loss": 0.6702,
      "step": 845500
    },
    {
      "epoch": 7.715891670924885,
      "grad_norm": 2.5370094776153564,
      "learning_rate": 4.3570090274229265e-05,
      "loss": 0.686,
      "step": 845600
    },
    {
      "epoch": 7.71680414628805,
      "grad_norm": 4.033316135406494,
      "learning_rate": 4.3569329878093296e-05,
      "loss": 0.6929,
      "step": 845700
    },
    {
      "epoch": 7.717716621651215,
      "grad_norm": 3.554568290710449,
      "learning_rate": 4.356856948195732e-05,
      "loss": 0.6947,
      "step": 845800
    },
    {
      "epoch": 7.7186290970143805,
      "grad_norm": 4.2861433029174805,
      "learning_rate": 4.3567809085821356e-05,
      "loss": 0.6995,
      "step": 845900
    },
    {
      "epoch": 7.719541572377546,
      "grad_norm": 3.651843309402466,
      "learning_rate": 4.356704868968538e-05,
      "loss": 0.6705,
      "step": 846000
    },
    {
      "epoch": 7.720454047740711,
      "grad_norm": 4.00740909576416,
      "learning_rate": 4.356628829354941e-05,
      "loss": 0.7244,
      "step": 846100
    },
    {
      "epoch": 7.721366523103876,
      "grad_norm": 3.5091049671173096,
      "learning_rate": 4.356552789741344e-05,
      "loss": 0.7139,
      "step": 846200
    },
    {
      "epoch": 7.722278998467042,
      "grad_norm": 3.953394651412964,
      "learning_rate": 4.356476750127747e-05,
      "loss": 0.7113,
      "step": 846300
    },
    {
      "epoch": 7.723191473830207,
      "grad_norm": 4.236630916595459,
      "learning_rate": 4.35640071051415e-05,
      "loss": 0.722,
      "step": 846400
    },
    {
      "epoch": 7.724103949193371,
      "grad_norm": 3.784419059753418,
      "learning_rate": 4.356324670900552e-05,
      "loss": 0.6808,
      "step": 846500
    },
    {
      "epoch": 7.725016424556537,
      "grad_norm": 4.630743980407715,
      "learning_rate": 4.356248631286955e-05,
      "loss": 0.7252,
      "step": 846600
    },
    {
      "epoch": 7.725928899919702,
      "grad_norm": 3.6180715560913086,
      "learning_rate": 4.356172591673358e-05,
      "loss": 0.6948,
      "step": 846700
    },
    {
      "epoch": 7.726841375282867,
      "grad_norm": 4.504891872406006,
      "learning_rate": 4.356096552059761e-05,
      "loss": 0.6781,
      "step": 846800
    },
    {
      "epoch": 7.727753850646033,
      "grad_norm": 3.0685486793518066,
      "learning_rate": 4.3560205124461636e-05,
      "loss": 0.7037,
      "step": 846900
    },
    {
      "epoch": 7.728666326009198,
      "grad_norm": 3.643460273742676,
      "learning_rate": 4.355944472832567e-05,
      "loss": 0.6709,
      "step": 847000
    },
    {
      "epoch": 7.729578801372363,
      "grad_norm": 3.782557725906372,
      "learning_rate": 4.3558684332189696e-05,
      "loss": 0.7357,
      "step": 847100
    },
    {
      "epoch": 7.7304912767355285,
      "grad_norm": 3.7952468395233154,
      "learning_rate": 4.3557923936053726e-05,
      "loss": 0.6892,
      "step": 847200
    },
    {
      "epoch": 7.731403752098693,
      "grad_norm": 4.267347812652588,
      "learning_rate": 4.3557163539917756e-05,
      "loss": 0.7167,
      "step": 847300
    },
    {
      "epoch": 7.732316227461858,
      "grad_norm": 4.264255046844482,
      "learning_rate": 4.3556403143781786e-05,
      "loss": 0.6742,
      "step": 847400
    },
    {
      "epoch": 7.7332287028250235,
      "grad_norm": 3.9189951419830322,
      "learning_rate": 4.3555642747645816e-05,
      "loss": 0.7173,
      "step": 847500
    },
    {
      "epoch": 7.734141178188189,
      "grad_norm": 4.061638832092285,
      "learning_rate": 4.3554882351509846e-05,
      "loss": 0.7102,
      "step": 847600
    },
    {
      "epoch": 7.735053653551354,
      "grad_norm": 3.955868721008301,
      "learning_rate": 4.355412195537387e-05,
      "loss": 0.7066,
      "step": 847700
    },
    {
      "epoch": 7.735966128914519,
      "grad_norm": 4.513481616973877,
      "learning_rate": 4.3553361559237907e-05,
      "loss": 0.6887,
      "step": 847800
    },
    {
      "epoch": 7.736878604277685,
      "grad_norm": 3.760317087173462,
      "learning_rate": 4.355260116310193e-05,
      "loss": 0.7048,
      "step": 847900
    },
    {
      "epoch": 7.73779107964085,
      "grad_norm": 4.517033576965332,
      "learning_rate": 4.355184076696596e-05,
      "loss": 0.7293,
      "step": 848000
    },
    {
      "epoch": 7.738703555004015,
      "grad_norm": 4.764754295349121,
      "learning_rate": 4.355108037082999e-05,
      "loss": 0.6944,
      "step": 848100
    },
    {
      "epoch": 7.73961603036718,
      "grad_norm": 4.046374797821045,
      "learning_rate": 4.355031997469402e-05,
      "loss": 0.7324,
      "step": 848200
    },
    {
      "epoch": 7.740528505730345,
      "grad_norm": 3.9796879291534424,
      "learning_rate": 4.354955957855805e-05,
      "loss": 0.6772,
      "step": 848300
    },
    {
      "epoch": 7.74144098109351,
      "grad_norm": 4.79267692565918,
      "learning_rate": 4.354879918242208e-05,
      "loss": 0.6745,
      "step": 848400
    },
    {
      "epoch": 7.742353456456676,
      "grad_norm": 4.795390605926514,
      "learning_rate": 4.3548038786286104e-05,
      "loss": 0.6649,
      "step": 848500
    },
    {
      "epoch": 7.743265931819841,
      "grad_norm": 2.938749074935913,
      "learning_rate": 4.3547278390150134e-05,
      "loss": 0.7015,
      "step": 848600
    },
    {
      "epoch": 7.744178407183006,
      "grad_norm": 4.849437713623047,
      "learning_rate": 4.3546517994014164e-05,
      "loss": 0.6911,
      "step": 848700
    },
    {
      "epoch": 7.7450908825461715,
      "grad_norm": 3.484349012374878,
      "learning_rate": 4.3545757597878194e-05,
      "loss": 0.6891,
      "step": 848800
    },
    {
      "epoch": 7.746003357909337,
      "grad_norm": 3.8640384674072266,
      "learning_rate": 4.3544997201742224e-05,
      "loss": 0.7264,
      "step": 848900
    },
    {
      "epoch": 7.746915833272501,
      "grad_norm": 4.102295875549316,
      "learning_rate": 4.3544236805606254e-05,
      "loss": 0.696,
      "step": 849000
    },
    {
      "epoch": 7.7478283086356665,
      "grad_norm": 3.2990448474884033,
      "learning_rate": 4.354347640947028e-05,
      "loss": 0.7275,
      "step": 849100
    },
    {
      "epoch": 7.748740783998832,
      "grad_norm": 4.39278507232666,
      "learning_rate": 4.3542716013334314e-05,
      "loss": 0.7189,
      "step": 849200
    },
    {
      "epoch": 7.749653259361997,
      "grad_norm": 4.6378936767578125,
      "learning_rate": 4.354195561719834e-05,
      "loss": 0.6848,
      "step": 849300
    },
    {
      "epoch": 7.750565734725162,
      "grad_norm": 3.1701884269714355,
      "learning_rate": 4.354119522106237e-05,
      "loss": 0.6682,
      "step": 849400
    },
    {
      "epoch": 7.751478210088328,
      "grad_norm": 4.965383529663086,
      "learning_rate": 4.35404348249264e-05,
      "loss": 0.6879,
      "step": 849500
    },
    {
      "epoch": 7.752390685451493,
      "grad_norm": 4.020119667053223,
      "learning_rate": 4.353967442879042e-05,
      "loss": 0.7211,
      "step": 849600
    },
    {
      "epoch": 7.753303160814658,
      "grad_norm": 4.8893818855285645,
      "learning_rate": 4.353891403265446e-05,
      "loss": 0.6763,
      "step": 849700
    },
    {
      "epoch": 7.754215636177824,
      "grad_norm": 4.293438911437988,
      "learning_rate": 4.353815363651848e-05,
      "loss": 0.6801,
      "step": 849800
    },
    {
      "epoch": 7.755128111540988,
      "grad_norm": 3.4041574001312256,
      "learning_rate": 4.353739324038251e-05,
      "loss": 0.703,
      "step": 849900
    },
    {
      "epoch": 7.756040586904153,
      "grad_norm": 4.238494873046875,
      "learning_rate": 4.353663284424654e-05,
      "loss": 0.695,
      "step": 850000
    },
    {
      "epoch": 7.756953062267319,
      "grad_norm": 3.689260959625244,
      "learning_rate": 4.353587244811057e-05,
      "loss": 0.674,
      "step": 850100
    },
    {
      "epoch": 7.757865537630484,
      "grad_norm": 3.1402108669281006,
      "learning_rate": 4.3535112051974594e-05,
      "loss": 0.6909,
      "step": 850200
    },
    {
      "epoch": 7.758778012993649,
      "grad_norm": 3.619098663330078,
      "learning_rate": 4.353435165583863e-05,
      "loss": 0.7281,
      "step": 850300
    },
    {
      "epoch": 7.7596904883568145,
      "grad_norm": 3.7524330615997314,
      "learning_rate": 4.3533591259702654e-05,
      "loss": 0.6757,
      "step": 850400
    },
    {
      "epoch": 7.76060296371998,
      "grad_norm": 3.647907018661499,
      "learning_rate": 4.3532830863566685e-05,
      "loss": 0.7016,
      "step": 850500
    },
    {
      "epoch": 7.761515439083145,
      "grad_norm": 3.474026679992676,
      "learning_rate": 4.3532070467430715e-05,
      "loss": 0.6968,
      "step": 850600
    },
    {
      "epoch": 7.7624279144463095,
      "grad_norm": 4.941494464874268,
      "learning_rate": 4.3531310071294745e-05,
      "loss": 0.6893,
      "step": 850700
    },
    {
      "epoch": 7.763340389809475,
      "grad_norm": 3.5476479530334473,
      "learning_rate": 4.3530549675158775e-05,
      "loss": 0.674,
      "step": 850800
    },
    {
      "epoch": 7.76425286517264,
      "grad_norm": 4.700377941131592,
      "learning_rate": 4.3529789279022805e-05,
      "loss": 0.6775,
      "step": 850900
    },
    {
      "epoch": 7.765165340535805,
      "grad_norm": 4.093170166015625,
      "learning_rate": 4.352902888288683e-05,
      "loss": 0.6694,
      "step": 851000
    },
    {
      "epoch": 7.766077815898971,
      "grad_norm": 4.099690914154053,
      "learning_rate": 4.3528268486750865e-05,
      "loss": 0.6632,
      "step": 851100
    },
    {
      "epoch": 7.766990291262136,
      "grad_norm": 4.423161506652832,
      "learning_rate": 4.352750809061489e-05,
      "loss": 0.7184,
      "step": 851200
    },
    {
      "epoch": 7.767902766625301,
      "grad_norm": 4.986924648284912,
      "learning_rate": 4.352674769447892e-05,
      "loss": 0.7262,
      "step": 851300
    },
    {
      "epoch": 7.768815241988467,
      "grad_norm": 3.8795673847198486,
      "learning_rate": 4.352598729834295e-05,
      "loss": 0.7059,
      "step": 851400
    },
    {
      "epoch": 7.769727717351632,
      "grad_norm": 4.20343017578125,
      "learning_rate": 4.352522690220698e-05,
      "loss": 0.6854,
      "step": 851500
    },
    {
      "epoch": 7.770640192714796,
      "grad_norm": 3.2198307514190674,
      "learning_rate": 4.3524466506071e-05,
      "loss": 0.6554,
      "step": 851600
    },
    {
      "epoch": 7.771552668077962,
      "grad_norm": 4.697622299194336,
      "learning_rate": 4.352370610993504e-05,
      "loss": 0.6759,
      "step": 851700
    },
    {
      "epoch": 7.772465143441127,
      "grad_norm": 3.74253249168396,
      "learning_rate": 4.352294571379906e-05,
      "loss": 0.7413,
      "step": 851800
    },
    {
      "epoch": 7.773377618804292,
      "grad_norm": 3.812645673751831,
      "learning_rate": 4.352218531766309e-05,
      "loss": 0.6436,
      "step": 851900
    },
    {
      "epoch": 7.7742900941674575,
      "grad_norm": 2.989866256713867,
      "learning_rate": 4.352142492152712e-05,
      "loss": 0.6454,
      "step": 852000
    },
    {
      "epoch": 7.775202569530623,
      "grad_norm": 3.9683871269226074,
      "learning_rate": 4.3520664525391145e-05,
      "loss": 0.6626,
      "step": 852100
    },
    {
      "epoch": 7.776115044893788,
      "grad_norm": 3.813403367996216,
      "learning_rate": 4.351990412925518e-05,
      "loss": 0.6871,
      "step": 852200
    },
    {
      "epoch": 7.777027520256953,
      "grad_norm": 3.9279637336730957,
      "learning_rate": 4.3519143733119205e-05,
      "loss": 0.6828,
      "step": 852300
    },
    {
      "epoch": 7.777939995620118,
      "grad_norm": 4.633453845977783,
      "learning_rate": 4.3518383336983235e-05,
      "loss": 0.6601,
      "step": 852400
    },
    {
      "epoch": 7.778852470983283,
      "grad_norm": 3.6971592903137207,
      "learning_rate": 4.3517622940847266e-05,
      "loss": 0.6724,
      "step": 852500
    },
    {
      "epoch": 7.779764946346448,
      "grad_norm": 5.621857643127441,
      "learning_rate": 4.3516862544711296e-05,
      "loss": 0.6994,
      "step": 852600
    },
    {
      "epoch": 7.780677421709614,
      "grad_norm": 2.9947192668914795,
      "learning_rate": 4.351610214857532e-05,
      "loss": 0.6753,
      "step": 852700
    },
    {
      "epoch": 7.781589897072779,
      "grad_norm": 3.830216407775879,
      "learning_rate": 4.3515341752439356e-05,
      "loss": 0.7128,
      "step": 852800
    },
    {
      "epoch": 7.782502372435944,
      "grad_norm": 3.4657552242279053,
      "learning_rate": 4.351458135630338e-05,
      "loss": 0.6683,
      "step": 852900
    },
    {
      "epoch": 7.78341484779911,
      "grad_norm": 4.220477104187012,
      "learning_rate": 4.351382096016741e-05,
      "loss": 0.6936,
      "step": 853000
    },
    {
      "epoch": 7.784327323162275,
      "grad_norm": 3.562744617462158,
      "learning_rate": 4.351306056403144e-05,
      "loss": 0.6942,
      "step": 853100
    },
    {
      "epoch": 7.78523979852544,
      "grad_norm": 4.11710262298584,
      "learning_rate": 4.351230016789547e-05,
      "loss": 0.6973,
      "step": 853200
    },
    {
      "epoch": 7.786152273888605,
      "grad_norm": 4.757798671722412,
      "learning_rate": 4.35115397717595e-05,
      "loss": 0.7171,
      "step": 853300
    },
    {
      "epoch": 7.78706474925177,
      "grad_norm": 4.33920431137085,
      "learning_rate": 4.351077937562353e-05,
      "loss": 0.6603,
      "step": 853400
    },
    {
      "epoch": 7.787977224614935,
      "grad_norm": 3.700490713119507,
      "learning_rate": 4.351001897948755e-05,
      "loss": 0.6755,
      "step": 853500
    },
    {
      "epoch": 7.7888896999781005,
      "grad_norm": 4.079649925231934,
      "learning_rate": 4.350925858335159e-05,
      "loss": 0.7121,
      "step": 853600
    },
    {
      "epoch": 7.789802175341266,
      "grad_norm": 4.18595552444458,
      "learning_rate": 4.350849818721561e-05,
      "loss": 0.6939,
      "step": 853700
    },
    {
      "epoch": 7.790714650704431,
      "grad_norm": 3.3777828216552734,
      "learning_rate": 4.350773779107964e-05,
      "loss": 0.6985,
      "step": 853800
    },
    {
      "epoch": 7.791627126067596,
      "grad_norm": 3.978780508041382,
      "learning_rate": 4.350697739494367e-05,
      "loss": 0.7098,
      "step": 853900
    },
    {
      "epoch": 7.792539601430762,
      "grad_norm": 3.8084378242492676,
      "learning_rate": 4.35062169988077e-05,
      "loss": 0.6841,
      "step": 854000
    },
    {
      "epoch": 7.793452076793926,
      "grad_norm": 3.3772614002227783,
      "learning_rate": 4.3505456602671726e-05,
      "loss": 0.7282,
      "step": 854100
    },
    {
      "epoch": 7.7943645521570915,
      "grad_norm": 4.279480934143066,
      "learning_rate": 4.350469620653576e-05,
      "loss": 0.7368,
      "step": 854200
    },
    {
      "epoch": 7.795277027520257,
      "grad_norm": 3.7492430210113525,
      "learning_rate": 4.3503935810399786e-05,
      "loss": 0.7409,
      "step": 854300
    },
    {
      "epoch": 7.796189502883422,
      "grad_norm": 3.560288906097412,
      "learning_rate": 4.3503175414263816e-05,
      "loss": 0.6965,
      "step": 854400
    },
    {
      "epoch": 7.797101978246587,
      "grad_norm": 3.824108839035034,
      "learning_rate": 4.3502415018127847e-05,
      "loss": 0.7136,
      "step": 854500
    },
    {
      "epoch": 7.798014453609753,
      "grad_norm": 5.2309675216674805,
      "learning_rate": 4.350165462199188e-05,
      "loss": 0.6914,
      "step": 854600
    },
    {
      "epoch": 7.798926928972918,
      "grad_norm": 3.4307005405426025,
      "learning_rate": 4.350089422585591e-05,
      "loss": 0.7254,
      "step": 854700
    },
    {
      "epoch": 7.799839404336083,
      "grad_norm": 3.8422632217407227,
      "learning_rate": 4.350013382971994e-05,
      "loss": 0.7163,
      "step": 854800
    },
    {
      "epoch": 7.8007518796992485,
      "grad_norm": 3.775348663330078,
      "learning_rate": 4.349937343358396e-05,
      "loss": 0.7012,
      "step": 854900
    },
    {
      "epoch": 7.801664355062413,
      "grad_norm": 4.294026851654053,
      "learning_rate": 4.349861303744799e-05,
      "loss": 0.6757,
      "step": 855000
    },
    {
      "epoch": 7.802576830425578,
      "grad_norm": 4.719831943511963,
      "learning_rate": 4.349785264131202e-05,
      "loss": 0.7175,
      "step": 855100
    },
    {
      "epoch": 7.803489305788744,
      "grad_norm": 3.2588913440704346,
      "learning_rate": 4.3497092245176043e-05,
      "loss": 0.6818,
      "step": 855200
    },
    {
      "epoch": 7.804401781151909,
      "grad_norm": 2.9756851196289062,
      "learning_rate": 4.349633184904008e-05,
      "loss": 0.6773,
      "step": 855300
    },
    {
      "epoch": 7.805314256515074,
      "grad_norm": 4.28321647644043,
      "learning_rate": 4.3495571452904104e-05,
      "loss": 0.7178,
      "step": 855400
    },
    {
      "epoch": 7.8062267318782395,
      "grad_norm": 4.409846305847168,
      "learning_rate": 4.3494811056768134e-05,
      "loss": 0.7079,
      "step": 855500
    },
    {
      "epoch": 7.807139207241405,
      "grad_norm": 3.8418867588043213,
      "learning_rate": 4.3494050660632164e-05,
      "loss": 0.6811,
      "step": 855600
    },
    {
      "epoch": 7.80805168260457,
      "grad_norm": 4.145766258239746,
      "learning_rate": 4.3493290264496194e-05,
      "loss": 0.73,
      "step": 855700
    },
    {
      "epoch": 7.8089641579677345,
      "grad_norm": 3.4753365516662598,
      "learning_rate": 4.3492529868360224e-05,
      "loss": 0.6831,
      "step": 855800
    },
    {
      "epoch": 7.8098766333309,
      "grad_norm": 3.9146058559417725,
      "learning_rate": 4.3491769472224254e-05,
      "loss": 0.726,
      "step": 855900
    },
    {
      "epoch": 7.810789108694065,
      "grad_norm": 3.5522420406341553,
      "learning_rate": 4.349100907608828e-05,
      "loss": 0.6669,
      "step": 856000
    },
    {
      "epoch": 7.81170158405723,
      "grad_norm": 4.227845191955566,
      "learning_rate": 4.3490248679952314e-05,
      "loss": 0.6859,
      "step": 856100
    },
    {
      "epoch": 7.812614059420396,
      "grad_norm": 4.496219158172607,
      "learning_rate": 4.348948828381634e-05,
      "loss": 0.708,
      "step": 856200
    },
    {
      "epoch": 7.813526534783561,
      "grad_norm": 3.531346082687378,
      "learning_rate": 4.348872788768037e-05,
      "loss": 0.649,
      "step": 856300
    },
    {
      "epoch": 7.814439010146726,
      "grad_norm": 4.049829006195068,
      "learning_rate": 4.34879674915444e-05,
      "loss": 0.676,
      "step": 856400
    },
    {
      "epoch": 7.815351485509892,
      "grad_norm": 3.181466579437256,
      "learning_rate": 4.348720709540843e-05,
      "loss": 0.7072,
      "step": 856500
    },
    {
      "epoch": 7.816263960873057,
      "grad_norm": 3.828235387802124,
      "learning_rate": 4.348644669927245e-05,
      "loss": 0.6876,
      "step": 856600
    },
    {
      "epoch": 7.817176436236221,
      "grad_norm": 4.111601829528809,
      "learning_rate": 4.348568630313649e-05,
      "loss": 0.7239,
      "step": 856700
    },
    {
      "epoch": 7.818088911599387,
      "grad_norm": 3.5744972229003906,
      "learning_rate": 4.348492590700051e-05,
      "loss": 0.6822,
      "step": 856800
    },
    {
      "epoch": 7.819001386962552,
      "grad_norm": 4.131752014160156,
      "learning_rate": 4.348416551086454e-05,
      "loss": 0.6805,
      "step": 856900
    },
    {
      "epoch": 7.819913862325717,
      "grad_norm": 2.7949585914611816,
      "learning_rate": 4.348340511472857e-05,
      "loss": 0.6551,
      "step": 857000
    },
    {
      "epoch": 7.8208263376888825,
      "grad_norm": 3.758662700653076,
      "learning_rate": 4.34826447185926e-05,
      "loss": 0.6591,
      "step": 857100
    },
    {
      "epoch": 7.821738813052048,
      "grad_norm": 3.196770191192627,
      "learning_rate": 4.348188432245663e-05,
      "loss": 0.7056,
      "step": 857200
    },
    {
      "epoch": 7.822651288415213,
      "grad_norm": 4.924016952514648,
      "learning_rate": 4.348112392632066e-05,
      "loss": 0.6417,
      "step": 857300
    },
    {
      "epoch": 7.823563763778378,
      "grad_norm": 3.5616772174835205,
      "learning_rate": 4.3480363530184685e-05,
      "loss": 0.726,
      "step": 857400
    },
    {
      "epoch": 7.824476239141543,
      "grad_norm": 3.8229825496673584,
      "learning_rate": 4.347960313404872e-05,
      "loss": 0.6513,
      "step": 857500
    },
    {
      "epoch": 7.825388714504708,
      "grad_norm": 3.6475656032562256,
      "learning_rate": 4.3478842737912745e-05,
      "loss": 0.703,
      "step": 857600
    },
    {
      "epoch": 7.826301189867873,
      "grad_norm": 4.817680835723877,
      "learning_rate": 4.3478082341776775e-05,
      "loss": 0.7228,
      "step": 857700
    },
    {
      "epoch": 7.827213665231039,
      "grad_norm": 3.7174758911132812,
      "learning_rate": 4.3477321945640805e-05,
      "loss": 0.6928,
      "step": 857800
    },
    {
      "epoch": 7.828126140594204,
      "grad_norm": 4.83469295501709,
      "learning_rate": 4.347656154950483e-05,
      "loss": 0.7315,
      "step": 857900
    },
    {
      "epoch": 7.829038615957369,
      "grad_norm": 3.4599924087524414,
      "learning_rate": 4.347580115336886e-05,
      "loss": 0.6918,
      "step": 858000
    },
    {
      "epoch": 7.829951091320535,
      "grad_norm": 4.0931243896484375,
      "learning_rate": 4.347504075723289e-05,
      "loss": 0.7148,
      "step": 858100
    },
    {
      "epoch": 7.8308635666837,
      "grad_norm": 4.361287593841553,
      "learning_rate": 4.347428036109692e-05,
      "loss": 0.7081,
      "step": 858200
    },
    {
      "epoch": 7.831776042046865,
      "grad_norm": 4.244421005249023,
      "learning_rate": 4.347351996496095e-05,
      "loss": 0.746,
      "step": 858300
    },
    {
      "epoch": 7.83268851741003,
      "grad_norm": 4.782170295715332,
      "learning_rate": 4.347275956882498e-05,
      "loss": 0.6756,
      "step": 858400
    },
    {
      "epoch": 7.833600992773195,
      "grad_norm": 3.6980743408203125,
      "learning_rate": 4.3471999172689e-05,
      "loss": 0.6932,
      "step": 858500
    },
    {
      "epoch": 7.83451346813636,
      "grad_norm": 4.378568649291992,
      "learning_rate": 4.347123877655304e-05,
      "loss": 0.6944,
      "step": 858600
    },
    {
      "epoch": 7.8354259434995255,
      "grad_norm": 3.852241039276123,
      "learning_rate": 4.347047838041706e-05,
      "loss": 0.7179,
      "step": 858700
    },
    {
      "epoch": 7.836338418862691,
      "grad_norm": 4.023504734039307,
      "learning_rate": 4.346971798428109e-05,
      "loss": 0.6623,
      "step": 858800
    },
    {
      "epoch": 7.837250894225856,
      "grad_norm": 4.339041233062744,
      "learning_rate": 4.346895758814512e-05,
      "loss": 0.7437,
      "step": 858900
    },
    {
      "epoch": 7.838163369589021,
      "grad_norm": 4.250088691711426,
      "learning_rate": 4.346819719200915e-05,
      "loss": 0.6915,
      "step": 859000
    },
    {
      "epoch": 7.839075844952187,
      "grad_norm": 3.784695863723755,
      "learning_rate": 4.3467436795873175e-05,
      "loss": 0.7242,
      "step": 859100
    },
    {
      "epoch": 7.839988320315351,
      "grad_norm": 3.3948240280151367,
      "learning_rate": 4.346667639973721e-05,
      "loss": 0.7079,
      "step": 859200
    },
    {
      "epoch": 7.840900795678516,
      "grad_norm": 3.016991138458252,
      "learning_rate": 4.3465916003601236e-05,
      "loss": 0.733,
      "step": 859300
    },
    {
      "epoch": 7.841813271041682,
      "grad_norm": 3.6754753589630127,
      "learning_rate": 4.3465155607465266e-05,
      "loss": 0.7132,
      "step": 859400
    },
    {
      "epoch": 7.842725746404847,
      "grad_norm": 3.4645180702209473,
      "learning_rate": 4.3464395211329296e-05,
      "loss": 0.7172,
      "step": 859500
    },
    {
      "epoch": 7.843638221768012,
      "grad_norm": 3.0976171493530273,
      "learning_rate": 4.3463634815193326e-05,
      "loss": 0.6852,
      "step": 859600
    },
    {
      "epoch": 7.844550697131178,
      "grad_norm": 4.506588459014893,
      "learning_rate": 4.3462874419057356e-05,
      "loss": 0.7137,
      "step": 859700
    },
    {
      "epoch": 7.845463172494343,
      "grad_norm": 5.38510799407959,
      "learning_rate": 4.3462114022921386e-05,
      "loss": 0.6972,
      "step": 859800
    },
    {
      "epoch": 7.846375647857508,
      "grad_norm": 3.7222747802734375,
      "learning_rate": 4.346135362678541e-05,
      "loss": 0.6568,
      "step": 859900
    },
    {
      "epoch": 7.8472881232206735,
      "grad_norm": 4.7902374267578125,
      "learning_rate": 4.3460593230649446e-05,
      "loss": 0.6809,
      "step": 860000
    },
    {
      "epoch": 7.848200598583838,
      "grad_norm": 3.957688093185425,
      "learning_rate": 4.345983283451347e-05,
      "loss": 0.7191,
      "step": 860100
    },
    {
      "epoch": 7.849113073947003,
      "grad_norm": 4.420200347900391,
      "learning_rate": 4.34590724383775e-05,
      "loss": 0.6874,
      "step": 860200
    },
    {
      "epoch": 7.8500255493101685,
      "grad_norm": 3.609529733657837,
      "learning_rate": 4.345831204224153e-05,
      "loss": 0.7001,
      "step": 860300
    },
    {
      "epoch": 7.850938024673334,
      "grad_norm": 4.293417453765869,
      "learning_rate": 4.345755164610556e-05,
      "loss": 0.6932,
      "step": 860400
    },
    {
      "epoch": 7.851850500036499,
      "grad_norm": 2.8831512928009033,
      "learning_rate": 4.345679124996958e-05,
      "loss": 0.7113,
      "step": 860500
    },
    {
      "epoch": 7.852762975399664,
      "grad_norm": 4.7625651359558105,
      "learning_rate": 4.345603085383361e-05,
      "loss": 0.676,
      "step": 860600
    },
    {
      "epoch": 7.85367545076283,
      "grad_norm": 4.027315616607666,
      "learning_rate": 4.345527045769764e-05,
      "loss": 0.6756,
      "step": 860700
    },
    {
      "epoch": 7.854587926125994,
      "grad_norm": 3.8554649353027344,
      "learning_rate": 4.345451006156167e-05,
      "loss": 0.7103,
      "step": 860800
    },
    {
      "epoch": 7.855500401489159,
      "grad_norm": 4.872337818145752,
      "learning_rate": 4.34537496654257e-05,
      "loss": 0.6858,
      "step": 860900
    },
    {
      "epoch": 7.856412876852325,
      "grad_norm": 3.257814645767212,
      "learning_rate": 4.3452989269289726e-05,
      "loss": 0.6884,
      "step": 861000
    },
    {
      "epoch": 7.85732535221549,
      "grad_norm": 2.739409923553467,
      "learning_rate": 4.345222887315376e-05,
      "loss": 0.6812,
      "step": 861100
    },
    {
      "epoch": 7.858237827578655,
      "grad_norm": 4.0925750732421875,
      "learning_rate": 4.3451468477017787e-05,
      "loss": 0.6955,
      "step": 861200
    },
    {
      "epoch": 7.859150302941821,
      "grad_norm": 3.9577221870422363,
      "learning_rate": 4.3450708080881817e-05,
      "loss": 0.7279,
      "step": 861300
    },
    {
      "epoch": 7.860062778304986,
      "grad_norm": 3.0375912189483643,
      "learning_rate": 4.344994768474585e-05,
      "loss": 0.7177,
      "step": 861400
    },
    {
      "epoch": 7.860975253668151,
      "grad_norm": 4.1426191329956055,
      "learning_rate": 4.344918728860988e-05,
      "loss": 0.6688,
      "step": 861500
    },
    {
      "epoch": 7.8618877290313165,
      "grad_norm": 4.250489234924316,
      "learning_rate": 4.344842689247391e-05,
      "loss": 0.6727,
      "step": 861600
    },
    {
      "epoch": 7.862800204394482,
      "grad_norm": 3.601443290710449,
      "learning_rate": 4.344766649633794e-05,
      "loss": 0.7214,
      "step": 861700
    },
    {
      "epoch": 7.863712679757646,
      "grad_norm": 3.070139169692993,
      "learning_rate": 4.344690610020196e-05,
      "loss": 0.6943,
      "step": 861800
    },
    {
      "epoch": 7.8646251551208115,
      "grad_norm": 3.5192322731018066,
      "learning_rate": 4.344614570406599e-05,
      "loss": 0.6894,
      "step": 861900
    },
    {
      "epoch": 7.865537630483977,
      "grad_norm": 4.196908473968506,
      "learning_rate": 4.344538530793002e-05,
      "loss": 0.7639,
      "step": 862000
    },
    {
      "epoch": 7.866450105847142,
      "grad_norm": 4.027012348175049,
      "learning_rate": 4.344462491179405e-05,
      "loss": 0.6691,
      "step": 862100
    },
    {
      "epoch": 7.867362581210307,
      "grad_norm": 3.634298324584961,
      "learning_rate": 4.344386451565808e-05,
      "loss": 0.6818,
      "step": 862200
    },
    {
      "epoch": 7.868275056573473,
      "grad_norm": 3.933680534362793,
      "learning_rate": 4.344310411952211e-05,
      "loss": 0.7113,
      "step": 862300
    },
    {
      "epoch": 7.869187531936638,
      "grad_norm": 4.065258979797363,
      "learning_rate": 4.3442343723386134e-05,
      "loss": 0.714,
      "step": 862400
    },
    {
      "epoch": 7.870100007299802,
      "grad_norm": 5.180586338043213,
      "learning_rate": 4.344158332725017e-05,
      "loss": 0.6675,
      "step": 862500
    },
    {
      "epoch": 7.871012482662968,
      "grad_norm": 4.336562633514404,
      "learning_rate": 4.3440822931114194e-05,
      "loss": 0.684,
      "step": 862600
    },
    {
      "epoch": 7.871924958026133,
      "grad_norm": 4.204392910003662,
      "learning_rate": 4.3440062534978224e-05,
      "loss": 0.7055,
      "step": 862700
    },
    {
      "epoch": 7.872837433389298,
      "grad_norm": 4.382106304168701,
      "learning_rate": 4.3439302138842254e-05,
      "loss": 0.6925,
      "step": 862800
    },
    {
      "epoch": 7.873749908752464,
      "grad_norm": 3.4005658626556396,
      "learning_rate": 4.3438541742706284e-05,
      "loss": 0.6788,
      "step": 862900
    },
    {
      "epoch": 7.874662384115629,
      "grad_norm": 3.4492101669311523,
      "learning_rate": 4.3437781346570314e-05,
      "loss": 0.6834,
      "step": 863000
    },
    {
      "epoch": 7.875574859478794,
      "grad_norm": 4.628288745880127,
      "learning_rate": 4.3437020950434344e-05,
      "loss": 0.6472,
      "step": 863100
    },
    {
      "epoch": 7.8764873348419595,
      "grad_norm": 3.0348703861236572,
      "learning_rate": 4.343626055429837e-05,
      "loss": 0.7072,
      "step": 863200
    },
    {
      "epoch": 7.877399810205125,
      "grad_norm": 3.5966954231262207,
      "learning_rate": 4.3435500158162404e-05,
      "loss": 0.6883,
      "step": 863300
    },
    {
      "epoch": 7.87831228556829,
      "grad_norm": 3.992166042327881,
      "learning_rate": 4.343473976202643e-05,
      "loss": 0.675,
      "step": 863400
    },
    {
      "epoch": 7.8792247609314545,
      "grad_norm": 3.226351022720337,
      "learning_rate": 4.343397936589045e-05,
      "loss": 0.7288,
      "step": 863500
    },
    {
      "epoch": 7.88013723629462,
      "grad_norm": 3.369819164276123,
      "learning_rate": 4.343321896975449e-05,
      "loss": 0.6866,
      "step": 863600
    },
    {
      "epoch": 7.881049711657785,
      "grad_norm": 3.2516705989837646,
      "learning_rate": 4.343245857361851e-05,
      "loss": 0.6823,
      "step": 863700
    },
    {
      "epoch": 7.88196218702095,
      "grad_norm": 3.0044312477111816,
      "learning_rate": 4.343169817748254e-05,
      "loss": 0.7001,
      "step": 863800
    },
    {
      "epoch": 7.882874662384116,
      "grad_norm": 4.9921698570251465,
      "learning_rate": 4.343093778134657e-05,
      "loss": 0.696,
      "step": 863900
    },
    {
      "epoch": 7.883787137747281,
      "grad_norm": 3.587230682373047,
      "learning_rate": 4.34301773852106e-05,
      "loss": 0.6511,
      "step": 864000
    },
    {
      "epoch": 7.884699613110446,
      "grad_norm": 4.576704025268555,
      "learning_rate": 4.342941698907463e-05,
      "loss": 0.7092,
      "step": 864100
    },
    {
      "epoch": 7.885612088473611,
      "grad_norm": 4.047885894775391,
      "learning_rate": 4.342865659293866e-05,
      "loss": 0.7391,
      "step": 864200
    },
    {
      "epoch": 7.886524563836776,
      "grad_norm": 3.9760327339172363,
      "learning_rate": 4.3427896196802685e-05,
      "loss": 0.6921,
      "step": 864300
    },
    {
      "epoch": 7.887437039199941,
      "grad_norm": 3.689317226409912,
      "learning_rate": 4.342713580066672e-05,
      "loss": 0.6992,
      "step": 864400
    },
    {
      "epoch": 7.888349514563107,
      "grad_norm": 4.039359092712402,
      "learning_rate": 4.3426375404530745e-05,
      "loss": 0.6688,
      "step": 864500
    },
    {
      "epoch": 7.889261989926272,
      "grad_norm": 3.9917993545532227,
      "learning_rate": 4.3425615008394775e-05,
      "loss": 0.6736,
      "step": 864600
    },
    {
      "epoch": 7.890174465289437,
      "grad_norm": 4.382320880889893,
      "learning_rate": 4.3424854612258805e-05,
      "loss": 0.7167,
      "step": 864700
    },
    {
      "epoch": 7.8910869406526025,
      "grad_norm": 4.02768087387085,
      "learning_rate": 4.3424094216122835e-05,
      "loss": 0.7049,
      "step": 864800
    },
    {
      "epoch": 7.891999416015768,
      "grad_norm": 4.3116278648376465,
      "learning_rate": 4.342333381998686e-05,
      "loss": 0.6966,
      "step": 864900
    },
    {
      "epoch": 7.892911891378933,
      "grad_norm": 3.7664082050323486,
      "learning_rate": 4.3422573423850895e-05,
      "loss": 0.6907,
      "step": 865000
    },
    {
      "epoch": 7.893824366742098,
      "grad_norm": 4.306000709533691,
      "learning_rate": 4.342181302771492e-05,
      "loss": 0.7156,
      "step": 865100
    },
    {
      "epoch": 7.894736842105263,
      "grad_norm": 3.5938572883605957,
      "learning_rate": 4.342105263157895e-05,
      "loss": 0.6947,
      "step": 865200
    },
    {
      "epoch": 7.895649317468428,
      "grad_norm": 4.055775165557861,
      "learning_rate": 4.342029223544298e-05,
      "loss": 0.6632,
      "step": 865300
    },
    {
      "epoch": 7.896561792831593,
      "grad_norm": 2.7179293632507324,
      "learning_rate": 4.341953183930701e-05,
      "loss": 0.6645,
      "step": 865400
    },
    {
      "epoch": 7.897474268194759,
      "grad_norm": 4.585500240325928,
      "learning_rate": 4.341877144317104e-05,
      "loss": 0.6669,
      "step": 865500
    },
    {
      "epoch": 7.898386743557924,
      "grad_norm": 4.93878173828125,
      "learning_rate": 4.341801104703507e-05,
      "loss": 0.6957,
      "step": 865600
    },
    {
      "epoch": 7.899299218921089,
      "grad_norm": 4.576260089874268,
      "learning_rate": 4.341725065089909e-05,
      "loss": 0.7007,
      "step": 865700
    },
    {
      "epoch": 7.900211694284255,
      "grad_norm": 4.035399436950684,
      "learning_rate": 4.341649025476313e-05,
      "loss": 0.7176,
      "step": 865800
    },
    {
      "epoch": 7.901124169647419,
      "grad_norm": 3.2370986938476562,
      "learning_rate": 4.341572985862715e-05,
      "loss": 0.6682,
      "step": 865900
    },
    {
      "epoch": 7.902036645010584,
      "grad_norm": 3.6984243392944336,
      "learning_rate": 4.341496946249118e-05,
      "loss": 0.6809,
      "step": 866000
    },
    {
      "epoch": 7.90294912037375,
      "grad_norm": 3.4616644382476807,
      "learning_rate": 4.341420906635521e-05,
      "loss": 0.7523,
      "step": 866100
    },
    {
      "epoch": 7.903861595736915,
      "grad_norm": 3.734602451324463,
      "learning_rate": 4.341344867021924e-05,
      "loss": 0.6885,
      "step": 866200
    },
    {
      "epoch": 7.90477407110008,
      "grad_norm": 3.831331253051758,
      "learning_rate": 4.3412688274083266e-05,
      "loss": 0.6976,
      "step": 866300
    },
    {
      "epoch": 7.9056865464632455,
      "grad_norm": 3.0695319175720215,
      "learning_rate": 4.3411927877947296e-05,
      "loss": 0.6908,
      "step": 866400
    },
    {
      "epoch": 7.906599021826411,
      "grad_norm": 3.692765474319458,
      "learning_rate": 4.3411167481811326e-05,
      "loss": 0.6907,
      "step": 866500
    },
    {
      "epoch": 7.907511497189576,
      "grad_norm": 4.401780128479004,
      "learning_rate": 4.3410407085675356e-05,
      "loss": 0.6443,
      "step": 866600
    },
    {
      "epoch": 7.908423972552741,
      "grad_norm": 4.643414497375488,
      "learning_rate": 4.3409646689539386e-05,
      "loss": 0.7066,
      "step": 866700
    },
    {
      "epoch": 7.909336447915907,
      "grad_norm": 2.655587673187256,
      "learning_rate": 4.340888629340341e-05,
      "loss": 0.6771,
      "step": 866800
    },
    {
      "epoch": 7.910248923279071,
      "grad_norm": 3.9567294120788574,
      "learning_rate": 4.3408125897267446e-05,
      "loss": 0.6749,
      "step": 866900
    },
    {
      "epoch": 7.911161398642236,
      "grad_norm": 4.482800006866455,
      "learning_rate": 4.340736550113147e-05,
      "loss": 0.7117,
      "step": 867000
    },
    {
      "epoch": 7.912073874005402,
      "grad_norm": 3.3415071964263916,
      "learning_rate": 4.34066051049955e-05,
      "loss": 0.6804,
      "step": 867100
    },
    {
      "epoch": 7.912986349368567,
      "grad_norm": 3.9441092014312744,
      "learning_rate": 4.340584470885953e-05,
      "loss": 0.6975,
      "step": 867200
    },
    {
      "epoch": 7.913898824731732,
      "grad_norm": 3.7106707096099854,
      "learning_rate": 4.340508431272356e-05,
      "loss": 0.6958,
      "step": 867300
    },
    {
      "epoch": 7.914811300094898,
      "grad_norm": 4.398614406585693,
      "learning_rate": 4.340432391658758e-05,
      "loss": 0.6793,
      "step": 867400
    },
    {
      "epoch": 7.915723775458063,
      "grad_norm": 3.641056776046753,
      "learning_rate": 4.340356352045162e-05,
      "loss": 0.6719,
      "step": 867500
    },
    {
      "epoch": 7.916636250821227,
      "grad_norm": 3.4827537536621094,
      "learning_rate": 4.340280312431564e-05,
      "loss": 0.6367,
      "step": 867600
    },
    {
      "epoch": 7.917548726184393,
      "grad_norm": 4.0364089012146,
      "learning_rate": 4.340204272817967e-05,
      "loss": 0.6943,
      "step": 867700
    },
    {
      "epoch": 7.918461201547558,
      "grad_norm": 3.2435288429260254,
      "learning_rate": 4.34012823320437e-05,
      "loss": 0.6796,
      "step": 867800
    },
    {
      "epoch": 7.919373676910723,
      "grad_norm": 5.067698001861572,
      "learning_rate": 4.340052193590773e-05,
      "loss": 0.7071,
      "step": 867900
    },
    {
      "epoch": 7.9202861522738885,
      "grad_norm": 4.308900356292725,
      "learning_rate": 4.339976153977176e-05,
      "loss": 0.7003,
      "step": 868000
    },
    {
      "epoch": 7.921198627637054,
      "grad_norm": 3.991194009780884,
      "learning_rate": 4.3399001143635793e-05,
      "loss": 0.7041,
      "step": 868100
    },
    {
      "epoch": 7.922111103000219,
      "grad_norm": 4.091464042663574,
      "learning_rate": 4.339824074749982e-05,
      "loss": 0.6972,
      "step": 868200
    },
    {
      "epoch": 7.9230235783633844,
      "grad_norm": 4.2221832275390625,
      "learning_rate": 4.3397480351363854e-05,
      "loss": 0.6834,
      "step": 868300
    },
    {
      "epoch": 7.92393605372655,
      "grad_norm": 3.9187965393066406,
      "learning_rate": 4.339671995522788e-05,
      "loss": 0.7141,
      "step": 868400
    },
    {
      "epoch": 7.924848529089715,
      "grad_norm": 4.159482479095459,
      "learning_rate": 4.339595955909191e-05,
      "loss": 0.7027,
      "step": 868500
    },
    {
      "epoch": 7.9257610044528795,
      "grad_norm": 4.4235334396362305,
      "learning_rate": 4.339519916295594e-05,
      "loss": 0.694,
      "step": 868600
    },
    {
      "epoch": 7.926673479816045,
      "grad_norm": 2.959911584854126,
      "learning_rate": 4.339443876681997e-05,
      "loss": 0.7289,
      "step": 868700
    },
    {
      "epoch": 7.92758595517921,
      "grad_norm": 3.785498857498169,
      "learning_rate": 4.339367837068399e-05,
      "loss": 0.7128,
      "step": 868800
    },
    {
      "epoch": 7.928498430542375,
      "grad_norm": 4.038228511810303,
      "learning_rate": 4.339291797454803e-05,
      "loss": 0.6871,
      "step": 868900
    },
    {
      "epoch": 7.929410905905541,
      "grad_norm": 3.2943334579467773,
      "learning_rate": 4.339215757841205e-05,
      "loss": 0.7266,
      "step": 869000
    },
    {
      "epoch": 7.930323381268706,
      "grad_norm": 3.400674819946289,
      "learning_rate": 4.339139718227608e-05,
      "loss": 0.6827,
      "step": 869100
    },
    {
      "epoch": 7.931235856631871,
      "grad_norm": 4.088144302368164,
      "learning_rate": 4.339063678614011e-05,
      "loss": 0.6539,
      "step": 869200
    },
    {
      "epoch": 7.932148331995036,
      "grad_norm": 3.713918447494507,
      "learning_rate": 4.3389876390004134e-05,
      "loss": 0.6623,
      "step": 869300
    },
    {
      "epoch": 7.933060807358201,
      "grad_norm": 4.874606609344482,
      "learning_rate": 4.338911599386817e-05,
      "loss": 0.7086,
      "step": 869400
    },
    {
      "epoch": 7.933973282721366,
      "grad_norm": 4.581846714019775,
      "learning_rate": 4.3388355597732194e-05,
      "loss": 0.7116,
      "step": 869500
    },
    {
      "epoch": 7.934885758084532,
      "grad_norm": 5.607998371124268,
      "learning_rate": 4.3387595201596224e-05,
      "loss": 0.7122,
      "step": 869600
    },
    {
      "epoch": 7.935798233447697,
      "grad_norm": 4.214158535003662,
      "learning_rate": 4.3386834805460254e-05,
      "loss": 0.6869,
      "step": 869700
    },
    {
      "epoch": 7.936710708810862,
      "grad_norm": 4.154017925262451,
      "learning_rate": 4.3386074409324284e-05,
      "loss": 0.6849,
      "step": 869800
    },
    {
      "epoch": 7.9376231841740275,
      "grad_norm": 3.2581844329833984,
      "learning_rate": 4.338531401318831e-05,
      "loss": 0.6951,
      "step": 869900
    },
    {
      "epoch": 7.938535659537193,
      "grad_norm": 2.6630325317382812,
      "learning_rate": 4.3384553617052344e-05,
      "loss": 0.7192,
      "step": 870000
    },
    {
      "epoch": 7.939448134900358,
      "grad_norm": 4.273288249969482,
      "learning_rate": 4.338379322091637e-05,
      "loss": 0.7,
      "step": 870100
    },
    {
      "epoch": 7.940360610263523,
      "grad_norm": 3.617541790008545,
      "learning_rate": 4.33830328247804e-05,
      "loss": 0.7087,
      "step": 870200
    },
    {
      "epoch": 7.941273085626688,
      "grad_norm": 4.746089458465576,
      "learning_rate": 4.338227242864443e-05,
      "loss": 0.6706,
      "step": 870300
    },
    {
      "epoch": 7.942185560989853,
      "grad_norm": 2.5388424396514893,
      "learning_rate": 4.338151203250846e-05,
      "loss": 0.7048,
      "step": 870400
    },
    {
      "epoch": 7.943098036353018,
      "grad_norm": 3.64139723777771,
      "learning_rate": 4.338075163637249e-05,
      "loss": 0.6685,
      "step": 870500
    },
    {
      "epoch": 7.944010511716184,
      "grad_norm": 4.362249374389648,
      "learning_rate": 4.337999124023652e-05,
      "loss": 0.7148,
      "step": 870600
    },
    {
      "epoch": 7.944922987079349,
      "grad_norm": 3.746586322784424,
      "learning_rate": 4.337923084410054e-05,
      "loss": 0.6977,
      "step": 870700
    },
    {
      "epoch": 7.945835462442514,
      "grad_norm": 4.129938125610352,
      "learning_rate": 4.337847044796458e-05,
      "loss": 0.6583,
      "step": 870800
    },
    {
      "epoch": 7.94674793780568,
      "grad_norm": 4.183487415313721,
      "learning_rate": 4.33777100518286e-05,
      "loss": 0.6761,
      "step": 870900
    },
    {
      "epoch": 7.947660413168844,
      "grad_norm": 3.749455690383911,
      "learning_rate": 4.337694965569263e-05,
      "loss": 0.6565,
      "step": 871000
    },
    {
      "epoch": 7.948572888532009,
      "grad_norm": 4.651016712188721,
      "learning_rate": 4.337618925955666e-05,
      "loss": 0.6904,
      "step": 871100
    },
    {
      "epoch": 7.949485363895175,
      "grad_norm": 3.860997438430786,
      "learning_rate": 4.337542886342069e-05,
      "loss": 0.722,
      "step": 871200
    },
    {
      "epoch": 7.95039783925834,
      "grad_norm": 4.566565036773682,
      "learning_rate": 4.3374668467284715e-05,
      "loss": 0.6779,
      "step": 871300
    },
    {
      "epoch": 7.951310314621505,
      "grad_norm": 4.7542595863342285,
      "learning_rate": 4.337390807114875e-05,
      "loss": 0.716,
      "step": 871400
    },
    {
      "epoch": 7.9522227899846705,
      "grad_norm": 4.718204498291016,
      "learning_rate": 4.3373147675012775e-05,
      "loss": 0.6624,
      "step": 871500
    },
    {
      "epoch": 7.953135265347836,
      "grad_norm": 3.6437020301818848,
      "learning_rate": 4.3372387278876805e-05,
      "loss": 0.6882,
      "step": 871600
    },
    {
      "epoch": 7.954047740711001,
      "grad_norm": 4.534060955047607,
      "learning_rate": 4.3371626882740835e-05,
      "loss": 0.7311,
      "step": 871700
    },
    {
      "epoch": 7.954960216074166,
      "grad_norm": 3.5692615509033203,
      "learning_rate": 4.3370866486604865e-05,
      "loss": 0.6898,
      "step": 871800
    },
    {
      "epoch": 7.955872691437332,
      "grad_norm": 2.25280499458313,
      "learning_rate": 4.3370106090468895e-05,
      "loss": 0.6655,
      "step": 871900
    },
    {
      "epoch": 7.956785166800496,
      "grad_norm": 4.0848188400268555,
      "learning_rate": 4.336934569433292e-05,
      "loss": 0.6703,
      "step": 872000
    },
    {
      "epoch": 7.957697642163661,
      "grad_norm": 3.2952089309692383,
      "learning_rate": 4.336858529819695e-05,
      "loss": 0.7255,
      "step": 872100
    },
    {
      "epoch": 7.958610117526827,
      "grad_norm": 3.7512404918670654,
      "learning_rate": 4.336782490206098e-05,
      "loss": 0.6989,
      "step": 872200
    },
    {
      "epoch": 7.959522592889992,
      "grad_norm": 3.2835581302642822,
      "learning_rate": 4.336706450592501e-05,
      "loss": 0.6937,
      "step": 872300
    },
    {
      "epoch": 7.960435068253157,
      "grad_norm": 3.0631871223449707,
      "learning_rate": 4.336630410978903e-05,
      "loss": 0.6474,
      "step": 872400
    },
    {
      "epoch": 7.961347543616323,
      "grad_norm": 5.513552665710449,
      "learning_rate": 4.336554371365307e-05,
      "loss": 0.6979,
      "step": 872500
    },
    {
      "epoch": 7.962260018979488,
      "grad_norm": 3.8734700679779053,
      "learning_rate": 4.336478331751709e-05,
      "loss": 0.6806,
      "step": 872600
    },
    {
      "epoch": 7.963172494342652,
      "grad_norm": 5.11903190612793,
      "learning_rate": 4.336402292138112e-05,
      "loss": 0.6866,
      "step": 872700
    },
    {
      "epoch": 7.964084969705818,
      "grad_norm": 3.9259393215179443,
      "learning_rate": 4.336326252524515e-05,
      "loss": 0.7094,
      "step": 872800
    },
    {
      "epoch": 7.964997445068983,
      "grad_norm": 3.6486432552337646,
      "learning_rate": 4.336250212910918e-05,
      "loss": 0.6916,
      "step": 872900
    },
    {
      "epoch": 7.965909920432148,
      "grad_norm": 4.771013259887695,
      "learning_rate": 4.336174173297321e-05,
      "loss": 0.7016,
      "step": 873000
    },
    {
      "epoch": 7.9668223957953135,
      "grad_norm": 3.9559688568115234,
      "learning_rate": 4.336098133683724e-05,
      "loss": 0.6726,
      "step": 873100
    },
    {
      "epoch": 7.967734871158479,
      "grad_norm": 4.46288537979126,
      "learning_rate": 4.3360220940701266e-05,
      "loss": 0.7353,
      "step": 873200
    },
    {
      "epoch": 7.968647346521644,
      "grad_norm": 3.907543182373047,
      "learning_rate": 4.33594605445653e-05,
      "loss": 0.7117,
      "step": 873300
    },
    {
      "epoch": 7.969559821884809,
      "grad_norm": 3.411651372909546,
      "learning_rate": 4.3358700148429326e-05,
      "loss": 0.6705,
      "step": 873400
    },
    {
      "epoch": 7.970472297247975,
      "grad_norm": 3.3620312213897705,
      "learning_rate": 4.3357939752293356e-05,
      "loss": 0.7276,
      "step": 873500
    },
    {
      "epoch": 7.97138477261114,
      "grad_norm": 3.843477249145508,
      "learning_rate": 4.3357179356157386e-05,
      "loss": 0.6758,
      "step": 873600
    },
    {
      "epoch": 7.972297247974304,
      "grad_norm": 3.8519983291625977,
      "learning_rate": 4.3356418960021416e-05,
      "loss": 0.6876,
      "step": 873700
    },
    {
      "epoch": 7.97320972333747,
      "grad_norm": 3.9106101989746094,
      "learning_rate": 4.3355658563885446e-05,
      "loss": 0.6564,
      "step": 873800
    },
    {
      "epoch": 7.974122198700635,
      "grad_norm": 3.9118096828460693,
      "learning_rate": 4.3354898167749476e-05,
      "loss": 0.7271,
      "step": 873900
    },
    {
      "epoch": 7.9750346740638,
      "grad_norm": 4.199712753295898,
      "learning_rate": 4.33541377716135e-05,
      "loss": 0.6984,
      "step": 874000
    },
    {
      "epoch": 7.975947149426966,
      "grad_norm": 4.224099159240723,
      "learning_rate": 4.335337737547753e-05,
      "loss": 0.7406,
      "step": 874100
    },
    {
      "epoch": 7.976859624790131,
      "grad_norm": 3.848785877227783,
      "learning_rate": 4.335261697934156e-05,
      "loss": 0.7133,
      "step": 874200
    },
    {
      "epoch": 7.977772100153296,
      "grad_norm": 4.614713191986084,
      "learning_rate": 4.335185658320559e-05,
      "loss": 0.6909,
      "step": 874300
    },
    {
      "epoch": 7.978684575516461,
      "grad_norm": 4.045093536376953,
      "learning_rate": 4.335109618706962e-05,
      "loss": 0.7021,
      "step": 874400
    },
    {
      "epoch": 7.979597050879626,
      "grad_norm": 2.941575765609741,
      "learning_rate": 4.335033579093365e-05,
      "loss": 0.7211,
      "step": 874500
    },
    {
      "epoch": 7.980509526242791,
      "grad_norm": 3.814073324203491,
      "learning_rate": 4.334957539479767e-05,
      "loss": 0.6889,
      "step": 874600
    },
    {
      "epoch": 7.9814220016059565,
      "grad_norm": 3.505443572998047,
      "learning_rate": 4.334881499866171e-05,
      "loss": 0.6989,
      "step": 874700
    },
    {
      "epoch": 7.982334476969122,
      "grad_norm": 4.517552852630615,
      "learning_rate": 4.334805460252573e-05,
      "loss": 0.7187,
      "step": 874800
    },
    {
      "epoch": 7.983246952332287,
      "grad_norm": 3.3017594814300537,
      "learning_rate": 4.3347294206389763e-05,
      "loss": 0.7083,
      "step": 874900
    },
    {
      "epoch": 7.984159427695452,
      "grad_norm": 4.8640875816345215,
      "learning_rate": 4.3346533810253794e-05,
      "loss": 0.6663,
      "step": 875000
    },
    {
      "epoch": 7.985071903058618,
      "grad_norm": 3.7033047676086426,
      "learning_rate": 4.334577341411782e-05,
      "loss": 0.6767,
      "step": 875100
    },
    {
      "epoch": 7.985984378421783,
      "grad_norm": 4.218296527862549,
      "learning_rate": 4.3345013017981854e-05,
      "loss": 0.7189,
      "step": 875200
    },
    {
      "epoch": 7.986896853784948,
      "grad_norm": 3.854884147644043,
      "learning_rate": 4.334425262184588e-05,
      "loss": 0.6972,
      "step": 875300
    },
    {
      "epoch": 7.987809329148113,
      "grad_norm": 4.805549144744873,
      "learning_rate": 4.334349222570991e-05,
      "loss": 0.6414,
      "step": 875400
    },
    {
      "epoch": 7.988721804511278,
      "grad_norm": 4.794344425201416,
      "learning_rate": 4.334273182957394e-05,
      "loss": 0.7169,
      "step": 875500
    },
    {
      "epoch": 7.989634279874443,
      "grad_norm": 4.80329704284668,
      "learning_rate": 4.334197143343797e-05,
      "loss": 0.688,
      "step": 875600
    },
    {
      "epoch": 7.990546755237609,
      "grad_norm": 3.9132447242736816,
      "learning_rate": 4.334121103730199e-05,
      "loss": 0.7395,
      "step": 875700
    },
    {
      "epoch": 7.991459230600774,
      "grad_norm": 3.6492092609405518,
      "learning_rate": 4.334045064116603e-05,
      "loss": 0.7276,
      "step": 875800
    },
    {
      "epoch": 7.992371705963939,
      "grad_norm": 2.9623749256134033,
      "learning_rate": 4.333969024503005e-05,
      "loss": 0.6874,
      "step": 875900
    },
    {
      "epoch": 7.9932841813271045,
      "grad_norm": 4.29524040222168,
      "learning_rate": 4.333892984889408e-05,
      "loss": 0.6567,
      "step": 876000
    },
    {
      "epoch": 7.994196656690269,
      "grad_norm": 4.22204065322876,
      "learning_rate": 4.333816945275811e-05,
      "loss": 0.6686,
      "step": 876100
    },
    {
      "epoch": 7.995109132053434,
      "grad_norm": 4.438220977783203,
      "learning_rate": 4.333740905662214e-05,
      "loss": 0.6481,
      "step": 876200
    },
    {
      "epoch": 7.9960216074165995,
      "grad_norm": 3.780810594558716,
      "learning_rate": 4.333664866048617e-05,
      "loss": 0.6757,
      "step": 876300
    },
    {
      "epoch": 7.996934082779765,
      "grad_norm": 3.3005223274230957,
      "learning_rate": 4.33358882643502e-05,
      "loss": 0.7208,
      "step": 876400
    },
    {
      "epoch": 7.99784655814293,
      "grad_norm": 4.103063583374023,
      "learning_rate": 4.3335127868214224e-05,
      "loss": 0.6872,
      "step": 876500
    },
    {
      "epoch": 7.998759033506095,
      "grad_norm": 4.763699531555176,
      "learning_rate": 4.333436747207826e-05,
      "loss": 0.6879,
      "step": 876600
    },
    {
      "epoch": 7.999671508869261,
      "grad_norm": 4.98311710357666,
      "learning_rate": 4.3333607075942284e-05,
      "loss": 0.6903,
      "step": 876700
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.5649169087409973,
      "eval_runtime": 25.823,
      "eval_samples_per_second": 223.406,
      "eval_steps_per_second": 223.406,
      "step": 876736
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.5456418991088867,
      "eval_runtime": 511.1512,
      "eval_samples_per_second": 214.402,
      "eval_steps_per_second": 214.402,
      "step": 876736
    },
    {
      "epoch": 8.000583984232426,
      "grad_norm": 3.9775190353393555,
      "learning_rate": 4.3332846679806314e-05,
      "loss": 0.7011,
      "step": 876800
    },
    {
      "epoch": 8.001496459595591,
      "grad_norm": 4.314237117767334,
      "learning_rate": 4.3332086283670344e-05,
      "loss": 0.7021,
      "step": 876900
    },
    {
      "epoch": 8.002408934958757,
      "grad_norm": 3.072526693344116,
      "learning_rate": 4.3331325887534375e-05,
      "loss": 0.6578,
      "step": 877000
    },
    {
      "epoch": 8.003321410321922,
      "grad_norm": 3.515641212463379,
      "learning_rate": 4.33305654913984e-05,
      "loss": 0.6822,
      "step": 877100
    },
    {
      "epoch": 8.004233885685087,
      "grad_norm": 4.26830530166626,
      "learning_rate": 4.3329805095262435e-05,
      "loss": 0.6969,
      "step": 877200
    },
    {
      "epoch": 8.005146361048253,
      "grad_norm": 4.9181718826293945,
      "learning_rate": 4.332904469912646e-05,
      "loss": 0.6963,
      "step": 877300
    },
    {
      "epoch": 8.006058836411416,
      "grad_norm": 3.2730917930603027,
      "learning_rate": 4.332828430299049e-05,
      "loss": 0.6925,
      "step": 877400
    },
    {
      "epoch": 8.006971311774581,
      "grad_norm": 4.1408491134643555,
      "learning_rate": 4.332752390685452e-05,
      "loss": 0.6831,
      "step": 877500
    },
    {
      "epoch": 8.007883787137747,
      "grad_norm": 4.319575786590576,
      "learning_rate": 4.332676351071855e-05,
      "loss": 0.7176,
      "step": 877600
    },
    {
      "epoch": 8.008796262500912,
      "grad_norm": 3.6790523529052734,
      "learning_rate": 4.332600311458258e-05,
      "loss": 0.6549,
      "step": 877700
    },
    {
      "epoch": 8.009708737864077,
      "grad_norm": 4.936703681945801,
      "learning_rate": 4.33252427184466e-05,
      "loss": 0.6804,
      "step": 877800
    },
    {
      "epoch": 8.010621213227243,
      "grad_norm": 3.0759313106536865,
      "learning_rate": 4.332448232231063e-05,
      "loss": 0.702,
      "step": 877900
    },
    {
      "epoch": 8.011533688590408,
      "grad_norm": 3.6406705379486084,
      "learning_rate": 4.332372192617466e-05,
      "loss": 0.6134,
      "step": 878000
    },
    {
      "epoch": 8.012446163953573,
      "grad_norm": 4.199434280395508,
      "learning_rate": 4.332296153003869e-05,
      "loss": 0.6729,
      "step": 878100
    },
    {
      "epoch": 8.013358639316738,
      "grad_norm": 3.1209604740142822,
      "learning_rate": 4.3322201133902715e-05,
      "loss": 0.68,
      "step": 878200
    },
    {
      "epoch": 8.014271114679904,
      "grad_norm": 4.334529876708984,
      "learning_rate": 4.332144073776675e-05,
      "loss": 0.7087,
      "step": 878300
    },
    {
      "epoch": 8.015183590043069,
      "grad_norm": 3.974224805831909,
      "learning_rate": 4.3320680341630775e-05,
      "loss": 0.7176,
      "step": 878400
    },
    {
      "epoch": 8.016096065406234,
      "grad_norm": 3.5769965648651123,
      "learning_rate": 4.3319919945494805e-05,
      "loss": 0.7314,
      "step": 878500
    },
    {
      "epoch": 8.0170085407694,
      "grad_norm": 3.680960178375244,
      "learning_rate": 4.3319159549358835e-05,
      "loss": 0.685,
      "step": 878600
    },
    {
      "epoch": 8.017921016132565,
      "grad_norm": 4.218897342681885,
      "learning_rate": 4.3318399153222865e-05,
      "loss": 0.6587,
      "step": 878700
    },
    {
      "epoch": 8.01883349149573,
      "grad_norm": 4.122633934020996,
      "learning_rate": 4.3317638757086895e-05,
      "loss": 0.6994,
      "step": 878800
    },
    {
      "epoch": 8.019745966858896,
      "grad_norm": 3.2704017162323,
      "learning_rate": 4.3316878360950925e-05,
      "loss": 0.685,
      "step": 878900
    },
    {
      "epoch": 8.02065844222206,
      "grad_norm": 4.322717189788818,
      "learning_rate": 4.331611796481495e-05,
      "loss": 0.6938,
      "step": 879000
    },
    {
      "epoch": 8.021570917585224,
      "grad_norm": 3.7835934162139893,
      "learning_rate": 4.3315357568678986e-05,
      "loss": 0.6685,
      "step": 879100
    },
    {
      "epoch": 8.02248339294839,
      "grad_norm": 4.263713836669922,
      "learning_rate": 4.331459717254301e-05,
      "loss": 0.6983,
      "step": 879200
    },
    {
      "epoch": 8.023395868311555,
      "grad_norm": 3.3174548149108887,
      "learning_rate": 4.331383677640704e-05,
      "loss": 0.702,
      "step": 879300
    },
    {
      "epoch": 8.02430834367472,
      "grad_norm": 4.73351526260376,
      "learning_rate": 4.331307638027107e-05,
      "loss": 0.6861,
      "step": 879400
    },
    {
      "epoch": 8.025220819037886,
      "grad_norm": 3.7541821002960205,
      "learning_rate": 4.33123159841351e-05,
      "loss": 0.7092,
      "step": 879500
    },
    {
      "epoch": 8.02613329440105,
      "grad_norm": 3.277181625366211,
      "learning_rate": 4.331155558799912e-05,
      "loss": 0.6914,
      "step": 879600
    },
    {
      "epoch": 8.027045769764216,
      "grad_norm": 4.35828161239624,
      "learning_rate": 4.331079519186316e-05,
      "loss": 0.6748,
      "step": 879700
    },
    {
      "epoch": 8.027958245127381,
      "grad_norm": 3.4904863834381104,
      "learning_rate": 4.331003479572718e-05,
      "loss": 0.669,
      "step": 879800
    },
    {
      "epoch": 8.028870720490547,
      "grad_norm": 3.6204140186309814,
      "learning_rate": 4.330927439959121e-05,
      "loss": 0.6883,
      "step": 879900
    },
    {
      "epoch": 8.029783195853712,
      "grad_norm": 3.6118271350860596,
      "learning_rate": 4.330851400345524e-05,
      "loss": 0.6606,
      "step": 880000
    },
    {
      "epoch": 8.030695671216877,
      "grad_norm": 4.161611080169678,
      "learning_rate": 4.330775360731927e-05,
      "loss": 0.6811,
      "step": 880100
    },
    {
      "epoch": 8.031608146580043,
      "grad_norm": 1.8115230798721313,
      "learning_rate": 4.33069932111833e-05,
      "loss": 0.6973,
      "step": 880200
    },
    {
      "epoch": 8.032520621943208,
      "grad_norm": 5.066248893737793,
      "learning_rate": 4.330623281504733e-05,
      "loss": 0.7,
      "step": 880300
    },
    {
      "epoch": 8.033433097306373,
      "grad_norm": 4.063070774078369,
      "learning_rate": 4.3305472418911356e-05,
      "loss": 0.6982,
      "step": 880400
    },
    {
      "epoch": 8.034345572669539,
      "grad_norm": 3.0885229110717773,
      "learning_rate": 4.3304712022775386e-05,
      "loss": 0.7021,
      "step": 880500
    },
    {
      "epoch": 8.035258048032704,
      "grad_norm": 4.498169422149658,
      "learning_rate": 4.3303951626639416e-05,
      "loss": 0.6821,
      "step": 880600
    },
    {
      "epoch": 8.03617052339587,
      "grad_norm": 3.6428446769714355,
      "learning_rate": 4.330319123050344e-05,
      "loss": 0.6783,
      "step": 880700
    },
    {
      "epoch": 8.037082998759033,
      "grad_norm": 3.5136828422546387,
      "learning_rate": 4.3302430834367476e-05,
      "loss": 0.706,
      "step": 880800
    },
    {
      "epoch": 8.037995474122198,
      "grad_norm": 3.883906364440918,
      "learning_rate": 4.33016704382315e-05,
      "loss": 0.6826,
      "step": 880900
    },
    {
      "epoch": 8.038907949485363,
      "grad_norm": 4.2811479568481445,
      "learning_rate": 4.330091004209553e-05,
      "loss": 0.7134,
      "step": 881000
    },
    {
      "epoch": 8.039820424848529,
      "grad_norm": 3.843883991241455,
      "learning_rate": 4.330014964595956e-05,
      "loss": 0.6646,
      "step": 881100
    },
    {
      "epoch": 8.040732900211694,
      "grad_norm": 4.627439022064209,
      "learning_rate": 4.329938924982359e-05,
      "loss": 0.69,
      "step": 881200
    },
    {
      "epoch": 8.04164537557486,
      "grad_norm": 3.4633865356445312,
      "learning_rate": 4.329862885368762e-05,
      "loss": 0.7066,
      "step": 881300
    },
    {
      "epoch": 8.042557850938024,
      "grad_norm": 4.908775806427002,
      "learning_rate": 4.329786845755165e-05,
      "loss": 0.6695,
      "step": 881400
    },
    {
      "epoch": 8.04347032630119,
      "grad_norm": 4.381628513336182,
      "learning_rate": 4.329710806141567e-05,
      "loss": 0.6713,
      "step": 881500
    },
    {
      "epoch": 8.044382801664355,
      "grad_norm": 4.025003433227539,
      "learning_rate": 4.329634766527971e-05,
      "loss": 0.6729,
      "step": 881600
    },
    {
      "epoch": 8.04529527702752,
      "grad_norm": 4.5593647956848145,
      "learning_rate": 4.3295587269143733e-05,
      "loss": 0.7548,
      "step": 881700
    },
    {
      "epoch": 8.046207752390686,
      "grad_norm": 3.548614263534546,
      "learning_rate": 4.3294826873007764e-05,
      "loss": 0.6832,
      "step": 881800
    },
    {
      "epoch": 8.047120227753851,
      "grad_norm": 3.378525733947754,
      "learning_rate": 4.3294066476871794e-05,
      "loss": 0.6786,
      "step": 881900
    },
    {
      "epoch": 8.048032703117016,
      "grad_norm": 3.593996524810791,
      "learning_rate": 4.3293306080735824e-05,
      "loss": 0.6802,
      "step": 882000
    },
    {
      "epoch": 8.048945178480182,
      "grad_norm": 4.1383562088012695,
      "learning_rate": 4.329254568459985e-05,
      "loss": 0.6783,
      "step": 882100
    },
    {
      "epoch": 8.049857653843347,
      "grad_norm": 3.9265568256378174,
      "learning_rate": 4.3291785288463884e-05,
      "loss": 0.7154,
      "step": 882200
    },
    {
      "epoch": 8.050770129206512,
      "grad_norm": 4.454307556152344,
      "learning_rate": 4.329102489232791e-05,
      "loss": 0.7061,
      "step": 882300
    },
    {
      "epoch": 8.051682604569677,
      "grad_norm": 5.734065532684326,
      "learning_rate": 4.329026449619194e-05,
      "loss": 0.691,
      "step": 882400
    },
    {
      "epoch": 8.052595079932841,
      "grad_norm": 3.2836966514587402,
      "learning_rate": 4.328950410005597e-05,
      "loss": 0.6835,
      "step": 882500
    },
    {
      "epoch": 8.053507555296006,
      "grad_norm": 3.9546146392822266,
      "learning_rate": 4.328874370392e-05,
      "loss": 0.7059,
      "step": 882600
    },
    {
      "epoch": 8.054420030659172,
      "grad_norm": 5.426226615905762,
      "learning_rate": 4.328798330778403e-05,
      "loss": 0.7214,
      "step": 882700
    },
    {
      "epoch": 8.055332506022337,
      "grad_norm": 4.201070785522461,
      "learning_rate": 4.328722291164806e-05,
      "loss": 0.6876,
      "step": 882800
    },
    {
      "epoch": 8.056244981385502,
      "grad_norm": 3.8020520210266113,
      "learning_rate": 4.328646251551208e-05,
      "loss": 0.7028,
      "step": 882900
    },
    {
      "epoch": 8.057157456748667,
      "grad_norm": 3.570013999938965,
      "learning_rate": 4.328570211937612e-05,
      "loss": 0.6859,
      "step": 883000
    },
    {
      "epoch": 8.058069932111833,
      "grad_norm": 3.2235732078552246,
      "learning_rate": 4.328494172324014e-05,
      "loss": 0.7057,
      "step": 883100
    },
    {
      "epoch": 8.058982407474998,
      "grad_norm": 4.220862865447998,
      "learning_rate": 4.328418132710417e-05,
      "loss": 0.6829,
      "step": 883200
    },
    {
      "epoch": 8.059894882838163,
      "grad_norm": 4.733626842498779,
      "learning_rate": 4.32834209309682e-05,
      "loss": 0.6786,
      "step": 883300
    },
    {
      "epoch": 8.060807358201329,
      "grad_norm": 4.351741313934326,
      "learning_rate": 4.3282660534832224e-05,
      "loss": 0.7054,
      "step": 883400
    },
    {
      "epoch": 8.061719833564494,
      "grad_norm": 4.87039041519165,
      "learning_rate": 4.3281900138696254e-05,
      "loss": 0.7044,
      "step": 883500
    },
    {
      "epoch": 8.06263230892766,
      "grad_norm": 3.508094072341919,
      "learning_rate": 4.3281139742560284e-05,
      "loss": 0.7375,
      "step": 883600
    },
    {
      "epoch": 8.063544784290825,
      "grad_norm": 4.266427993774414,
      "learning_rate": 4.3280379346424314e-05,
      "loss": 0.6861,
      "step": 883700
    },
    {
      "epoch": 8.06445725965399,
      "grad_norm": 3.726860523223877,
      "learning_rate": 4.3279618950288345e-05,
      "loss": 0.669,
      "step": 883800
    },
    {
      "epoch": 8.065369735017155,
      "grad_norm": 4.1524763107299805,
      "learning_rate": 4.3278858554152375e-05,
      "loss": 0.717,
      "step": 883900
    },
    {
      "epoch": 8.06628221038032,
      "grad_norm": 5.067074775695801,
      "learning_rate": 4.32780981580164e-05,
      "loss": 0.6462,
      "step": 884000
    },
    {
      "epoch": 8.067194685743486,
      "grad_norm": 4.332582473754883,
      "learning_rate": 4.3277337761880435e-05,
      "loss": 0.7073,
      "step": 884100
    },
    {
      "epoch": 8.06810716110665,
      "grad_norm": 4.388003826141357,
      "learning_rate": 4.327657736574446e-05,
      "loss": 0.6933,
      "step": 884200
    },
    {
      "epoch": 8.069019636469815,
      "grad_norm": 3.962632656097412,
      "learning_rate": 4.327581696960849e-05,
      "loss": 0.7015,
      "step": 884300
    },
    {
      "epoch": 8.06993211183298,
      "grad_norm": 3.549198865890503,
      "learning_rate": 4.327505657347252e-05,
      "loss": 0.7319,
      "step": 884400
    },
    {
      "epoch": 8.070844587196145,
      "grad_norm": 3.860858678817749,
      "learning_rate": 4.327429617733655e-05,
      "loss": 0.6937,
      "step": 884500
    },
    {
      "epoch": 8.07175706255931,
      "grad_norm": 3.6518054008483887,
      "learning_rate": 4.327353578120057e-05,
      "loss": 0.7212,
      "step": 884600
    },
    {
      "epoch": 8.072669537922476,
      "grad_norm": 4.59030294418335,
      "learning_rate": 4.327277538506461e-05,
      "loss": 0.7159,
      "step": 884700
    },
    {
      "epoch": 8.073582013285641,
      "grad_norm": 3.8838250637054443,
      "learning_rate": 4.327201498892863e-05,
      "loss": 0.6678,
      "step": 884800
    },
    {
      "epoch": 8.074494488648806,
      "grad_norm": 3.7880687713623047,
      "learning_rate": 4.327125459279266e-05,
      "loss": 0.6875,
      "step": 884900
    },
    {
      "epoch": 8.075406964011972,
      "grad_norm": 2.67380952835083,
      "learning_rate": 4.327049419665669e-05,
      "loss": 0.6853,
      "step": 885000
    },
    {
      "epoch": 8.076319439375137,
      "grad_norm": 4.69459342956543,
      "learning_rate": 4.326973380052072e-05,
      "loss": 0.7099,
      "step": 885100
    },
    {
      "epoch": 8.077231914738302,
      "grad_norm": 3.4387547969818115,
      "learning_rate": 4.326897340438475e-05,
      "loss": 0.684,
      "step": 885200
    },
    {
      "epoch": 8.078144390101468,
      "grad_norm": 3.283226728439331,
      "learning_rate": 4.326821300824878e-05,
      "loss": 0.6559,
      "step": 885300
    },
    {
      "epoch": 8.079056865464633,
      "grad_norm": 3.7468910217285156,
      "learning_rate": 4.3267452612112805e-05,
      "loss": 0.6738,
      "step": 885400
    },
    {
      "epoch": 8.079969340827798,
      "grad_norm": 3.8332009315490723,
      "learning_rate": 4.326669221597684e-05,
      "loss": 0.6902,
      "step": 885500
    },
    {
      "epoch": 8.080881816190963,
      "grad_norm": 3.3816845417022705,
      "learning_rate": 4.3265931819840865e-05,
      "loss": 0.678,
      "step": 885600
    },
    {
      "epoch": 8.081794291554129,
      "grad_norm": 3.414186954498291,
      "learning_rate": 4.3265171423704895e-05,
      "loss": 0.6502,
      "step": 885700
    },
    {
      "epoch": 8.082706766917294,
      "grad_norm": 3.8059346675872803,
      "learning_rate": 4.3264411027568926e-05,
      "loss": 0.6794,
      "step": 885800
    },
    {
      "epoch": 8.083619242280458,
      "grad_norm": 4.209977626800537,
      "learning_rate": 4.3263650631432956e-05,
      "loss": 0.722,
      "step": 885900
    },
    {
      "epoch": 8.084531717643623,
      "grad_norm": 4.510006904602051,
      "learning_rate": 4.326289023529698e-05,
      "loss": 0.7106,
      "step": 886000
    },
    {
      "epoch": 8.085444193006788,
      "grad_norm": 3.892432689666748,
      "learning_rate": 4.3262129839161016e-05,
      "loss": 0.6939,
      "step": 886100
    },
    {
      "epoch": 8.086356668369953,
      "grad_norm": 4.363701820373535,
      "learning_rate": 4.326136944302504e-05,
      "loss": 0.7137,
      "step": 886200
    },
    {
      "epoch": 8.087269143733119,
      "grad_norm": 4.436156272888184,
      "learning_rate": 4.326060904688907e-05,
      "loss": 0.6995,
      "step": 886300
    },
    {
      "epoch": 8.088181619096284,
      "grad_norm": 3.824453353881836,
      "learning_rate": 4.32598486507531e-05,
      "loss": 0.686,
      "step": 886400
    },
    {
      "epoch": 8.08909409445945,
      "grad_norm": 5.568856239318848,
      "learning_rate": 4.325908825461712e-05,
      "loss": 0.7112,
      "step": 886500
    },
    {
      "epoch": 8.090006569822615,
      "grad_norm": 3.8490447998046875,
      "learning_rate": 4.325832785848116e-05,
      "loss": 0.718,
      "step": 886600
    },
    {
      "epoch": 8.09091904518578,
      "grad_norm": 4.063446044921875,
      "learning_rate": 4.325756746234518e-05,
      "loss": 0.7117,
      "step": 886700
    },
    {
      "epoch": 8.091831520548945,
      "grad_norm": 3.9858174324035645,
      "learning_rate": 4.325680706620921e-05,
      "loss": 0.6653,
      "step": 886800
    },
    {
      "epoch": 8.09274399591211,
      "grad_norm": 3.9542622566223145,
      "learning_rate": 4.325604667007324e-05,
      "loss": 0.6706,
      "step": 886900
    },
    {
      "epoch": 8.093656471275276,
      "grad_norm": 3.5861127376556396,
      "learning_rate": 4.325528627393727e-05,
      "loss": 0.6971,
      "step": 887000
    },
    {
      "epoch": 8.094568946638441,
      "grad_norm": 3.9649581909179688,
      "learning_rate": 4.32545258778013e-05,
      "loss": 0.7008,
      "step": 887100
    },
    {
      "epoch": 8.095481422001606,
      "grad_norm": 3.390678644180298,
      "learning_rate": 4.325376548166533e-05,
      "loss": 0.6838,
      "step": 887200
    },
    {
      "epoch": 8.096393897364772,
      "grad_norm": 3.786205291748047,
      "learning_rate": 4.3253005085529356e-05,
      "loss": 0.6828,
      "step": 887300
    },
    {
      "epoch": 8.097306372727937,
      "grad_norm": 4.405398368835449,
      "learning_rate": 4.3252244689393386e-05,
      "loss": 0.6766,
      "step": 887400
    },
    {
      "epoch": 8.098218848091102,
      "grad_norm": 3.915923833847046,
      "learning_rate": 4.3251484293257416e-05,
      "loss": 0.6926,
      "step": 887500
    },
    {
      "epoch": 8.099131323454266,
      "grad_norm": 3.8060507774353027,
      "learning_rate": 4.3250723897121446e-05,
      "loss": 0.7144,
      "step": 887600
    },
    {
      "epoch": 8.100043798817431,
      "grad_norm": 3.388855457305908,
      "learning_rate": 4.3249963500985477e-05,
      "loss": 0.704,
      "step": 887700
    },
    {
      "epoch": 8.100956274180596,
      "grad_norm": 4.152966022491455,
      "learning_rate": 4.3249203104849507e-05,
      "loss": 0.6826,
      "step": 887800
    },
    {
      "epoch": 8.101868749543762,
      "grad_norm": 4.141097545623779,
      "learning_rate": 4.324844270871353e-05,
      "loss": 0.6557,
      "step": 887900
    },
    {
      "epoch": 8.102781224906927,
      "grad_norm": 3.2579894065856934,
      "learning_rate": 4.324768231257757e-05,
      "loss": 0.6724,
      "step": 888000
    },
    {
      "epoch": 8.103693700270092,
      "grad_norm": 2.7759830951690674,
      "learning_rate": 4.324692191644159e-05,
      "loss": 0.7055,
      "step": 888100
    },
    {
      "epoch": 8.104606175633258,
      "grad_norm": 4.737297058105469,
      "learning_rate": 4.324616152030562e-05,
      "loss": 0.7056,
      "step": 888200
    },
    {
      "epoch": 8.105518650996423,
      "grad_norm": 3.9737796783447266,
      "learning_rate": 4.324540112416965e-05,
      "loss": 0.6761,
      "step": 888300
    },
    {
      "epoch": 8.106431126359588,
      "grad_norm": 4.245744228363037,
      "learning_rate": 4.324464072803368e-05,
      "loss": 0.7134,
      "step": 888400
    },
    {
      "epoch": 8.107343601722754,
      "grad_norm": 4.064609527587891,
      "learning_rate": 4.324388033189771e-05,
      "loss": 0.7023,
      "step": 888500
    },
    {
      "epoch": 8.108256077085919,
      "grad_norm": 3.8811569213867188,
      "learning_rate": 4.324311993576174e-05,
      "loss": 0.6744,
      "step": 888600
    },
    {
      "epoch": 8.109168552449084,
      "grad_norm": 3.0728249549865723,
      "learning_rate": 4.3242359539625764e-05,
      "loss": 0.6631,
      "step": 888700
    },
    {
      "epoch": 8.11008102781225,
      "grad_norm": 3.3049979209899902,
      "learning_rate": 4.32415991434898e-05,
      "loss": 0.7191,
      "step": 888800
    },
    {
      "epoch": 8.110993503175415,
      "grad_norm": 4.509120464324951,
      "learning_rate": 4.3240838747353824e-05,
      "loss": 0.6888,
      "step": 888900
    },
    {
      "epoch": 8.11190597853858,
      "grad_norm": 3.950230598449707,
      "learning_rate": 4.324007835121785e-05,
      "loss": 0.7269,
      "step": 889000
    },
    {
      "epoch": 8.112818453901745,
      "grad_norm": 4.241959095001221,
      "learning_rate": 4.3239317955081884e-05,
      "loss": 0.678,
      "step": 889100
    },
    {
      "epoch": 8.11373092926491,
      "grad_norm": 2.6514034271240234,
      "learning_rate": 4.323855755894591e-05,
      "loss": 0.7164,
      "step": 889200
    },
    {
      "epoch": 8.114643404628074,
      "grad_norm": 3.881984233856201,
      "learning_rate": 4.323779716280994e-05,
      "loss": 0.6723,
      "step": 889300
    },
    {
      "epoch": 8.11555587999124,
      "grad_norm": 3.4704999923706055,
      "learning_rate": 4.323703676667397e-05,
      "loss": 0.6931,
      "step": 889400
    },
    {
      "epoch": 8.116468355354405,
      "grad_norm": 4.569070339202881,
      "learning_rate": 4.3236276370538e-05,
      "loss": 0.6792,
      "step": 889500
    },
    {
      "epoch": 8.11738083071757,
      "grad_norm": 3.811114549636841,
      "learning_rate": 4.323551597440203e-05,
      "loss": 0.6873,
      "step": 889600
    },
    {
      "epoch": 8.118293306080735,
      "grad_norm": 3.425370931625366,
      "learning_rate": 4.323475557826606e-05,
      "loss": 0.6951,
      "step": 889700
    },
    {
      "epoch": 8.1192057814439,
      "grad_norm": 4.192529201507568,
      "learning_rate": 4.323399518213008e-05,
      "loss": 0.7028,
      "step": 889800
    },
    {
      "epoch": 8.120118256807066,
      "grad_norm": 3.9600298404693604,
      "learning_rate": 4.323323478599412e-05,
      "loss": 0.671,
      "step": 889900
    },
    {
      "epoch": 8.121030732170231,
      "grad_norm": 3.4883618354797363,
      "learning_rate": 4.323247438985814e-05,
      "loss": 0.7116,
      "step": 890000
    },
    {
      "epoch": 8.121943207533397,
      "grad_norm": 4.53610897064209,
      "learning_rate": 4.323171399372217e-05,
      "loss": 0.7073,
      "step": 890100
    },
    {
      "epoch": 8.122855682896562,
      "grad_norm": 4.171738147735596,
      "learning_rate": 4.32309535975862e-05,
      "loss": 0.6882,
      "step": 890200
    },
    {
      "epoch": 8.123768158259727,
      "grad_norm": 3.852323293685913,
      "learning_rate": 4.323019320145023e-05,
      "loss": 0.7276,
      "step": 890300
    },
    {
      "epoch": 8.124680633622893,
      "grad_norm": 4.766654968261719,
      "learning_rate": 4.3229432805314254e-05,
      "loss": 0.7145,
      "step": 890400
    },
    {
      "epoch": 8.125593108986058,
      "grad_norm": 3.3362784385681152,
      "learning_rate": 4.322867240917829e-05,
      "loss": 0.6828,
      "step": 890500
    },
    {
      "epoch": 8.126505584349223,
      "grad_norm": 3.1535286903381348,
      "learning_rate": 4.3227912013042315e-05,
      "loss": 0.696,
      "step": 890600
    },
    {
      "epoch": 8.127418059712388,
      "grad_norm": 4.336650371551514,
      "learning_rate": 4.3227151616906345e-05,
      "loss": 0.6826,
      "step": 890700
    },
    {
      "epoch": 8.128330535075554,
      "grad_norm": 3.5767645835876465,
      "learning_rate": 4.3226391220770375e-05,
      "loss": 0.6774,
      "step": 890800
    },
    {
      "epoch": 8.129243010438719,
      "grad_norm": 3.994896173477173,
      "learning_rate": 4.3225630824634405e-05,
      "loss": 0.6823,
      "step": 890900
    },
    {
      "epoch": 8.130155485801883,
      "grad_norm": 4.353017330169678,
      "learning_rate": 4.3224870428498435e-05,
      "loss": 0.6969,
      "step": 891000
    },
    {
      "epoch": 8.131067961165048,
      "grad_norm": 3.9876174926757812,
      "learning_rate": 4.3224110032362465e-05,
      "loss": 0.7012,
      "step": 891100
    },
    {
      "epoch": 8.131980436528213,
      "grad_norm": 5.942585468292236,
      "learning_rate": 4.322334963622649e-05,
      "loss": 0.6785,
      "step": 891200
    },
    {
      "epoch": 8.132892911891378,
      "grad_norm": 3.38728404045105,
      "learning_rate": 4.3222589240090525e-05,
      "loss": 0.7018,
      "step": 891300
    },
    {
      "epoch": 8.133805387254544,
      "grad_norm": 4.759131908416748,
      "learning_rate": 4.322182884395455e-05,
      "loss": 0.6754,
      "step": 891400
    },
    {
      "epoch": 8.134717862617709,
      "grad_norm": 3.94195818901062,
      "learning_rate": 4.322106844781858e-05,
      "loss": 0.6863,
      "step": 891500
    },
    {
      "epoch": 8.135630337980874,
      "grad_norm": 4.699409008026123,
      "learning_rate": 4.322030805168261e-05,
      "loss": 0.6867,
      "step": 891600
    },
    {
      "epoch": 8.13654281334404,
      "grad_norm": 4.307948112487793,
      "learning_rate": 4.321954765554664e-05,
      "loss": 0.7325,
      "step": 891700
    },
    {
      "epoch": 8.137455288707205,
      "grad_norm": 2.9956929683685303,
      "learning_rate": 4.321878725941066e-05,
      "loss": 0.7021,
      "step": 891800
    },
    {
      "epoch": 8.13836776407037,
      "grad_norm": 2.9199106693267822,
      "learning_rate": 4.321802686327469e-05,
      "loss": 0.6924,
      "step": 891900
    },
    {
      "epoch": 8.139280239433536,
      "grad_norm": 4.293725490570068,
      "learning_rate": 4.321726646713872e-05,
      "loss": 0.6653,
      "step": 892000
    },
    {
      "epoch": 8.1401927147967,
      "grad_norm": 4.087550163269043,
      "learning_rate": 4.321650607100275e-05,
      "loss": 0.6684,
      "step": 892100
    },
    {
      "epoch": 8.141105190159866,
      "grad_norm": 2.6629273891448975,
      "learning_rate": 4.321574567486678e-05,
      "loss": 0.7231,
      "step": 892200
    },
    {
      "epoch": 8.142017665523031,
      "grad_norm": 4.754263401031494,
      "learning_rate": 4.3214985278730805e-05,
      "loss": 0.6584,
      "step": 892300
    },
    {
      "epoch": 8.142930140886197,
      "grad_norm": 4.072902202606201,
      "learning_rate": 4.321422488259484e-05,
      "loss": 0.6523,
      "step": 892400
    },
    {
      "epoch": 8.143842616249362,
      "grad_norm": 4.543206214904785,
      "learning_rate": 4.3213464486458866e-05,
      "loss": 0.7009,
      "step": 892500
    },
    {
      "epoch": 8.144755091612527,
      "grad_norm": 3.9852404594421387,
      "learning_rate": 4.3212704090322896e-05,
      "loss": 0.7024,
      "step": 892600
    },
    {
      "epoch": 8.14566756697569,
      "grad_norm": 4.111549377441406,
      "learning_rate": 4.3211943694186926e-05,
      "loss": 0.7305,
      "step": 892700
    },
    {
      "epoch": 8.146580042338856,
      "grad_norm": 4.9625043869018555,
      "learning_rate": 4.3211183298050956e-05,
      "loss": 0.7062,
      "step": 892800
    },
    {
      "epoch": 8.147492517702021,
      "grad_norm": 4.17948055267334,
      "learning_rate": 4.321042290191498e-05,
      "loss": 0.6895,
      "step": 892900
    },
    {
      "epoch": 8.148404993065187,
      "grad_norm": 4.158456325531006,
      "learning_rate": 4.3209662505779016e-05,
      "loss": 0.6942,
      "step": 893000
    },
    {
      "epoch": 8.149317468428352,
      "grad_norm": 4.5270304679870605,
      "learning_rate": 4.320890210964304e-05,
      "loss": 0.6801,
      "step": 893100
    },
    {
      "epoch": 8.150229943791517,
      "grad_norm": 4.2289910316467285,
      "learning_rate": 4.320814171350707e-05,
      "loss": 0.6591,
      "step": 893200
    },
    {
      "epoch": 8.151142419154683,
      "grad_norm": 4.521142482757568,
      "learning_rate": 4.32073813173711e-05,
      "loss": 0.7001,
      "step": 893300
    },
    {
      "epoch": 8.152054894517848,
      "grad_norm": 3.555860757827759,
      "learning_rate": 4.320662092123513e-05,
      "loss": 0.7174,
      "step": 893400
    },
    {
      "epoch": 8.152967369881013,
      "grad_norm": 3.7363944053649902,
      "learning_rate": 4.320586052509916e-05,
      "loss": 0.6709,
      "step": 893500
    },
    {
      "epoch": 8.153879845244179,
      "grad_norm": 3.627835512161255,
      "learning_rate": 4.320510012896319e-05,
      "loss": 0.6836,
      "step": 893600
    },
    {
      "epoch": 8.154792320607344,
      "grad_norm": 3.834182024002075,
      "learning_rate": 4.320433973282721e-05,
      "loss": 0.6413,
      "step": 893700
    },
    {
      "epoch": 8.15570479597051,
      "grad_norm": 4.6267266273498535,
      "learning_rate": 4.320357933669125e-05,
      "loss": 0.6909,
      "step": 893800
    },
    {
      "epoch": 8.156617271333674,
      "grad_norm": 4.86889123916626,
      "learning_rate": 4.320281894055527e-05,
      "loss": 0.6922,
      "step": 893900
    },
    {
      "epoch": 8.15752974669684,
      "grad_norm": 3.9707159996032715,
      "learning_rate": 4.32020585444193e-05,
      "loss": 0.6693,
      "step": 894000
    },
    {
      "epoch": 8.158442222060005,
      "grad_norm": 4.868534564971924,
      "learning_rate": 4.320129814828333e-05,
      "loss": 0.6961,
      "step": 894100
    },
    {
      "epoch": 8.15935469742317,
      "grad_norm": 3.6890676021575928,
      "learning_rate": 4.320053775214736e-05,
      "loss": 0.6733,
      "step": 894200
    },
    {
      "epoch": 8.160267172786336,
      "grad_norm": 3.3500993251800537,
      "learning_rate": 4.3199777356011386e-05,
      "loss": 0.6453,
      "step": 894300
    },
    {
      "epoch": 8.1611796481495,
      "grad_norm": 3.971871852874756,
      "learning_rate": 4.319901695987542e-05,
      "loss": 0.6747,
      "step": 894400
    },
    {
      "epoch": 8.162092123512664,
      "grad_norm": 4.054221153259277,
      "learning_rate": 4.3198256563739447e-05,
      "loss": 0.7,
      "step": 894500
    },
    {
      "epoch": 8.16300459887583,
      "grad_norm": 3.780073642730713,
      "learning_rate": 4.3197496167603477e-05,
      "loss": 0.7031,
      "step": 894600
    },
    {
      "epoch": 8.163917074238995,
      "grad_norm": 3.907409191131592,
      "learning_rate": 4.319673577146751e-05,
      "loss": 0.7303,
      "step": 894700
    },
    {
      "epoch": 8.16482954960216,
      "grad_norm": 3.3079910278320312,
      "learning_rate": 4.319597537533153e-05,
      "loss": 0.7021,
      "step": 894800
    },
    {
      "epoch": 8.165742024965326,
      "grad_norm": 4.061936855316162,
      "learning_rate": 4.319521497919557e-05,
      "loss": 0.7142,
      "step": 894900
    },
    {
      "epoch": 8.166654500328491,
      "grad_norm": 4.532439708709717,
      "learning_rate": 4.319445458305959e-05,
      "loss": 0.6592,
      "step": 895000
    },
    {
      "epoch": 8.167566975691656,
      "grad_norm": 4.360556602478027,
      "learning_rate": 4.319369418692362e-05,
      "loss": 0.707,
      "step": 895100
    },
    {
      "epoch": 8.168479451054822,
      "grad_norm": 4.226576805114746,
      "learning_rate": 4.319293379078765e-05,
      "loss": 0.6521,
      "step": 895200
    },
    {
      "epoch": 8.169391926417987,
      "grad_norm": 4.040319919586182,
      "learning_rate": 4.319217339465168e-05,
      "loss": 0.674,
      "step": 895300
    },
    {
      "epoch": 8.170304401781152,
      "grad_norm": 3.609889507293701,
      "learning_rate": 4.3191412998515704e-05,
      "loss": 0.6565,
      "step": 895400
    },
    {
      "epoch": 8.171216877144317,
      "grad_norm": 3.0858211517333984,
      "learning_rate": 4.319065260237974e-05,
      "loss": 0.7041,
      "step": 895500
    },
    {
      "epoch": 8.172129352507483,
      "grad_norm": 4.998082160949707,
      "learning_rate": 4.3189892206243764e-05,
      "loss": 0.6918,
      "step": 895600
    },
    {
      "epoch": 8.173041827870648,
      "grad_norm": 3.58944034576416,
      "learning_rate": 4.3189131810107794e-05,
      "loss": 0.6844,
      "step": 895700
    },
    {
      "epoch": 8.173954303233813,
      "grad_norm": 3.4295198917388916,
      "learning_rate": 4.3188371413971824e-05,
      "loss": 0.6759,
      "step": 895800
    },
    {
      "epoch": 8.174866778596979,
      "grad_norm": 4.035761833190918,
      "learning_rate": 4.3187611017835854e-05,
      "loss": 0.7055,
      "step": 895900
    },
    {
      "epoch": 8.175779253960144,
      "grad_norm": 4.170413494110107,
      "learning_rate": 4.3186850621699884e-05,
      "loss": 0.7076,
      "step": 896000
    },
    {
      "epoch": 8.176691729323307,
      "grad_norm": 3.7264254093170166,
      "learning_rate": 4.3186090225563914e-05,
      "loss": 0.6655,
      "step": 896100
    },
    {
      "epoch": 8.177604204686473,
      "grad_norm": 5.043753147125244,
      "learning_rate": 4.318532982942794e-05,
      "loss": 0.6916,
      "step": 896200
    },
    {
      "epoch": 8.178516680049638,
      "grad_norm": 4.126008987426758,
      "learning_rate": 4.3184569433291974e-05,
      "loss": 0.682,
      "step": 896300
    },
    {
      "epoch": 8.179429155412803,
      "grad_norm": 3.8131465911865234,
      "learning_rate": 4.3183809037156e-05,
      "loss": 0.7158,
      "step": 896400
    },
    {
      "epoch": 8.180341630775969,
      "grad_norm": 4.767441272735596,
      "learning_rate": 4.318304864102003e-05,
      "loss": 0.6786,
      "step": 896500
    },
    {
      "epoch": 8.181254106139134,
      "grad_norm": 4.189358234405518,
      "learning_rate": 4.318228824488406e-05,
      "loss": 0.6681,
      "step": 896600
    },
    {
      "epoch": 8.1821665815023,
      "grad_norm": 3.9355356693267822,
      "learning_rate": 4.318152784874809e-05,
      "loss": 0.6774,
      "step": 896700
    },
    {
      "epoch": 8.183079056865465,
      "grad_norm": 4.683169841766357,
      "learning_rate": 4.318076745261211e-05,
      "loss": 0.6702,
      "step": 896800
    },
    {
      "epoch": 8.18399153222863,
      "grad_norm": 4.143485069274902,
      "learning_rate": 4.318000705647615e-05,
      "loss": 0.7037,
      "step": 896900
    },
    {
      "epoch": 8.184904007591795,
      "grad_norm": 4.185540676116943,
      "learning_rate": 4.317924666034017e-05,
      "loss": 0.6403,
      "step": 897000
    },
    {
      "epoch": 8.18581648295496,
      "grad_norm": 4.6523823738098145,
      "learning_rate": 4.31784862642042e-05,
      "loss": 0.6714,
      "step": 897100
    },
    {
      "epoch": 8.186728958318126,
      "grad_norm": 3.9036600589752197,
      "learning_rate": 4.317772586806823e-05,
      "loss": 0.6847,
      "step": 897200
    },
    {
      "epoch": 8.187641433681291,
      "grad_norm": 3.7624239921569824,
      "learning_rate": 4.317696547193226e-05,
      "loss": 0.6603,
      "step": 897300
    },
    {
      "epoch": 8.188553909044456,
      "grad_norm": 3.4879581928253174,
      "learning_rate": 4.317620507579629e-05,
      "loss": 0.6763,
      "step": 897400
    },
    {
      "epoch": 8.189466384407622,
      "grad_norm": 3.981558084487915,
      "learning_rate": 4.3175444679660315e-05,
      "loss": 0.667,
      "step": 897500
    },
    {
      "epoch": 8.190378859770787,
      "grad_norm": 4.154876708984375,
      "learning_rate": 4.3174684283524345e-05,
      "loss": 0.7427,
      "step": 897600
    },
    {
      "epoch": 8.191291335133952,
      "grad_norm": 3.880878448486328,
      "learning_rate": 4.3173923887388375e-05,
      "loss": 0.7335,
      "step": 897700
    },
    {
      "epoch": 8.192203810497116,
      "grad_norm": 4.001242637634277,
      "learning_rate": 4.3173163491252405e-05,
      "loss": 0.6914,
      "step": 897800
    },
    {
      "epoch": 8.193116285860281,
      "grad_norm": 4.385862350463867,
      "learning_rate": 4.317240309511643e-05,
      "loss": 0.707,
      "step": 897900
    },
    {
      "epoch": 8.194028761223446,
      "grad_norm": 3.708311080932617,
      "learning_rate": 4.3171642698980465e-05,
      "loss": 0.7053,
      "step": 898000
    },
    {
      "epoch": 8.194941236586612,
      "grad_norm": 4.745175361633301,
      "learning_rate": 4.317088230284449e-05,
      "loss": 0.7162,
      "step": 898100
    },
    {
      "epoch": 8.195853711949777,
      "grad_norm": 4.40576171875,
      "learning_rate": 4.317012190670852e-05,
      "loss": 0.7089,
      "step": 898200
    },
    {
      "epoch": 8.196766187312942,
      "grad_norm": 4.508560657501221,
      "learning_rate": 4.316936151057255e-05,
      "loss": 0.7089,
      "step": 898300
    },
    {
      "epoch": 8.197678662676108,
      "grad_norm": 3.155931234359741,
      "learning_rate": 4.316860111443658e-05,
      "loss": 0.6764,
      "step": 898400
    },
    {
      "epoch": 8.198591138039273,
      "grad_norm": 4.994153022766113,
      "learning_rate": 4.316784071830061e-05,
      "loss": 0.7207,
      "step": 898500
    },
    {
      "epoch": 8.199503613402438,
      "grad_norm": 4.705711841583252,
      "learning_rate": 4.316708032216464e-05,
      "loss": 0.7031,
      "step": 898600
    },
    {
      "epoch": 8.200416088765603,
      "grad_norm": 3.54752254486084,
      "learning_rate": 4.316631992602866e-05,
      "loss": 0.6703,
      "step": 898700
    },
    {
      "epoch": 8.201328564128769,
      "grad_norm": 3.7556774616241455,
      "learning_rate": 4.31655595298927e-05,
      "loss": 0.6883,
      "step": 898800
    },
    {
      "epoch": 8.202241039491934,
      "grad_norm": 4.7294182777404785,
      "learning_rate": 4.316479913375672e-05,
      "loss": 0.6724,
      "step": 898900
    },
    {
      "epoch": 8.2031535148551,
      "grad_norm": 3.9801583290100098,
      "learning_rate": 4.316403873762075e-05,
      "loss": 0.6579,
      "step": 899000
    },
    {
      "epoch": 8.204065990218265,
      "grad_norm": 3.748716354370117,
      "learning_rate": 4.316327834148478e-05,
      "loss": 0.6575,
      "step": 899100
    },
    {
      "epoch": 8.20497846558143,
      "grad_norm": 4.499078750610352,
      "learning_rate": 4.316251794534881e-05,
      "loss": 0.7095,
      "step": 899200
    },
    {
      "epoch": 8.205890940944595,
      "grad_norm": 3.9980530738830566,
      "learning_rate": 4.316175754921284e-05,
      "loss": 0.6843,
      "step": 899300
    },
    {
      "epoch": 8.20680341630776,
      "grad_norm": 3.932596445083618,
      "learning_rate": 4.316099715307687e-05,
      "loss": 0.6526,
      "step": 899400
    },
    {
      "epoch": 8.207715891670924,
      "grad_norm": 4.721630573272705,
      "learning_rate": 4.3160236756940896e-05,
      "loss": 0.6749,
      "step": 899500
    },
    {
      "epoch": 8.20862836703409,
      "grad_norm": 3.0601072311401367,
      "learning_rate": 4.3159476360804926e-05,
      "loss": 0.6847,
      "step": 899600
    },
    {
      "epoch": 8.209540842397255,
      "grad_norm": 4.6258111000061035,
      "learning_rate": 4.3158715964668956e-05,
      "loss": 0.7221,
      "step": 899700
    },
    {
      "epoch": 8.21045331776042,
      "grad_norm": 3.8271570205688477,
      "learning_rate": 4.3157955568532986e-05,
      "loss": 0.6776,
      "step": 899800
    },
    {
      "epoch": 8.211365793123585,
      "grad_norm": 4.012001991271973,
      "learning_rate": 4.3157195172397016e-05,
      "loss": 0.6527,
      "step": 899900
    },
    {
      "epoch": 8.21227826848675,
      "grad_norm": 4.902737140655518,
      "learning_rate": 4.3156434776261046e-05,
      "loss": 0.6879,
      "step": 900000
    },
    {
      "epoch": 8.213190743849916,
      "grad_norm": 4.7796430587768555,
      "learning_rate": 4.315567438012507e-05,
      "loss": 0.679,
      "step": 900100
    },
    {
      "epoch": 8.214103219213081,
      "grad_norm": 4.558629989624023,
      "learning_rate": 4.3154913983989106e-05,
      "loss": 0.718,
      "step": 900200
    },
    {
      "epoch": 8.215015694576246,
      "grad_norm": 3.8366947174072266,
      "learning_rate": 4.315415358785313e-05,
      "loss": 0.6955,
      "step": 900300
    },
    {
      "epoch": 8.215928169939412,
      "grad_norm": 3.3010809421539307,
      "learning_rate": 4.315339319171716e-05,
      "loss": 0.6775,
      "step": 900400
    },
    {
      "epoch": 8.216840645302577,
      "grad_norm": 4.0294928550720215,
      "learning_rate": 4.315263279558119e-05,
      "loss": 0.6956,
      "step": 900500
    },
    {
      "epoch": 8.217753120665742,
      "grad_norm": 5.227572441101074,
      "learning_rate": 4.315187239944521e-05,
      "loss": 0.6888,
      "step": 900600
    },
    {
      "epoch": 8.218665596028908,
      "grad_norm": 3.6663079261779785,
      "learning_rate": 4.315111200330925e-05,
      "loss": 0.695,
      "step": 900700
    },
    {
      "epoch": 8.219578071392073,
      "grad_norm": 4.534142017364502,
      "learning_rate": 4.315035160717327e-05,
      "loss": 0.7106,
      "step": 900800
    },
    {
      "epoch": 8.220490546755238,
      "grad_norm": 4.330626964569092,
      "learning_rate": 4.31495912110373e-05,
      "loss": 0.7227,
      "step": 900900
    },
    {
      "epoch": 8.221403022118404,
      "grad_norm": 4.45059871673584,
      "learning_rate": 4.314883081490133e-05,
      "loss": 0.6731,
      "step": 901000
    },
    {
      "epoch": 8.222315497481569,
      "grad_norm": 3.3555421829223633,
      "learning_rate": 4.314807041876536e-05,
      "loss": 0.6915,
      "step": 901100
    },
    {
      "epoch": 8.223227972844732,
      "grad_norm": 4.643892765045166,
      "learning_rate": 4.3147310022629387e-05,
      "loss": 0.6749,
      "step": 901200
    },
    {
      "epoch": 8.224140448207898,
      "grad_norm": 3.6018500328063965,
      "learning_rate": 4.314654962649342e-05,
      "loss": 0.7052,
      "step": 901300
    },
    {
      "epoch": 8.225052923571063,
      "grad_norm": 4.171970367431641,
      "learning_rate": 4.314578923035745e-05,
      "loss": 0.6807,
      "step": 901400
    },
    {
      "epoch": 8.225965398934228,
      "grad_norm": 4.230401515960693,
      "learning_rate": 4.314502883422148e-05,
      "loss": 0.7308,
      "step": 901500
    },
    {
      "epoch": 8.226877874297394,
      "grad_norm": 3.8691318035125732,
      "learning_rate": 4.314426843808551e-05,
      "loss": 0.6757,
      "step": 901600
    },
    {
      "epoch": 8.227790349660559,
      "grad_norm": 3.7881553173065186,
      "learning_rate": 4.314350804194954e-05,
      "loss": 0.6717,
      "step": 901700
    },
    {
      "epoch": 8.228702825023724,
      "grad_norm": 4.49279260635376,
      "learning_rate": 4.314274764581357e-05,
      "loss": 0.7028,
      "step": 901800
    },
    {
      "epoch": 8.22961530038689,
      "grad_norm": 4.617364883422852,
      "learning_rate": 4.31419872496776e-05,
      "loss": 0.6963,
      "step": 901900
    },
    {
      "epoch": 8.230527775750055,
      "grad_norm": 3.781522274017334,
      "learning_rate": 4.314122685354162e-05,
      "loss": 0.7121,
      "step": 902000
    },
    {
      "epoch": 8.23144025111322,
      "grad_norm": 4.476654529571533,
      "learning_rate": 4.314046645740566e-05,
      "loss": 0.7144,
      "step": 902100
    },
    {
      "epoch": 8.232352726476385,
      "grad_norm": 3.893932580947876,
      "learning_rate": 4.313970606126968e-05,
      "loss": 0.7155,
      "step": 902200
    },
    {
      "epoch": 8.23326520183955,
      "grad_norm": 3.0618443489074707,
      "learning_rate": 4.313894566513371e-05,
      "loss": 0.6948,
      "step": 902300
    },
    {
      "epoch": 8.234177677202716,
      "grad_norm": 2.505251407623291,
      "learning_rate": 4.313818526899774e-05,
      "loss": 0.6944,
      "step": 902400
    },
    {
      "epoch": 8.235090152565881,
      "grad_norm": 4.250978469848633,
      "learning_rate": 4.313742487286177e-05,
      "loss": 0.6913,
      "step": 902500
    },
    {
      "epoch": 8.236002627929047,
      "grad_norm": 4.038027286529541,
      "learning_rate": 4.3136664476725794e-05,
      "loss": 0.7197,
      "step": 902600
    },
    {
      "epoch": 8.236915103292212,
      "grad_norm": 5.196727752685547,
      "learning_rate": 4.313590408058983e-05,
      "loss": 0.6755,
      "step": 902700
    },
    {
      "epoch": 8.237827578655377,
      "grad_norm": 4.059708118438721,
      "learning_rate": 4.3135143684453854e-05,
      "loss": 0.6745,
      "step": 902800
    },
    {
      "epoch": 8.23874005401854,
      "grad_norm": 3.5756325721740723,
      "learning_rate": 4.3134383288317884e-05,
      "loss": 0.679,
      "step": 902900
    },
    {
      "epoch": 8.239652529381706,
      "grad_norm": 3.366243839263916,
      "learning_rate": 4.3133622892181914e-05,
      "loss": 0.7075,
      "step": 903000
    },
    {
      "epoch": 8.240565004744871,
      "grad_norm": 4.45531702041626,
      "learning_rate": 4.3132862496045944e-05,
      "loss": 0.7536,
      "step": 903100
    },
    {
      "epoch": 8.241477480108037,
      "grad_norm": 4.610413074493408,
      "learning_rate": 4.3132102099909974e-05,
      "loss": 0.6866,
      "step": 903200
    },
    {
      "epoch": 8.242389955471202,
      "grad_norm": 3.7621357440948486,
      "learning_rate": 4.3131341703774e-05,
      "loss": 0.6615,
      "step": 903300
    },
    {
      "epoch": 8.243302430834367,
      "grad_norm": 3.4255666732788086,
      "learning_rate": 4.313058130763803e-05,
      "loss": 0.6664,
      "step": 903400
    },
    {
      "epoch": 8.244214906197532,
      "grad_norm": 4.340534687042236,
      "learning_rate": 4.312982091150206e-05,
      "loss": 0.7176,
      "step": 903500
    },
    {
      "epoch": 8.245127381560698,
      "grad_norm": 4.347183704376221,
      "learning_rate": 4.312906051536609e-05,
      "loss": 0.6861,
      "step": 903600
    },
    {
      "epoch": 8.246039856923863,
      "grad_norm": 4.301657199859619,
      "learning_rate": 4.312830011923011e-05,
      "loss": 0.6843,
      "step": 903700
    },
    {
      "epoch": 8.246952332287028,
      "grad_norm": 4.106287479400635,
      "learning_rate": 4.312753972309415e-05,
      "loss": 0.6806,
      "step": 903800
    },
    {
      "epoch": 8.247864807650194,
      "grad_norm": 4.724213600158691,
      "learning_rate": 4.312677932695817e-05,
      "loss": 0.6597,
      "step": 903900
    },
    {
      "epoch": 8.248777283013359,
      "grad_norm": 3.779496908187866,
      "learning_rate": 4.31260189308222e-05,
      "loss": 0.6786,
      "step": 904000
    },
    {
      "epoch": 8.249689758376524,
      "grad_norm": 3.773543119430542,
      "learning_rate": 4.312525853468623e-05,
      "loss": 0.7159,
      "step": 904100
    },
    {
      "epoch": 8.25060223373969,
      "grad_norm": 4.424266338348389,
      "learning_rate": 4.312449813855026e-05,
      "loss": 0.6877,
      "step": 904200
    },
    {
      "epoch": 8.251514709102855,
      "grad_norm": 3.420099973678589,
      "learning_rate": 4.312373774241429e-05,
      "loss": 0.6806,
      "step": 904300
    },
    {
      "epoch": 8.25242718446602,
      "grad_norm": 4.212875843048096,
      "learning_rate": 4.312297734627832e-05,
      "loss": 0.6968,
      "step": 904400
    },
    {
      "epoch": 8.253339659829184,
      "grad_norm": 4.4270477294921875,
      "learning_rate": 4.3122216950142345e-05,
      "loss": 0.6904,
      "step": 904500
    },
    {
      "epoch": 8.254252135192349,
      "grad_norm": 4.423628807067871,
      "learning_rate": 4.312145655400638e-05,
      "loss": 0.6984,
      "step": 904600
    },
    {
      "epoch": 8.255164610555514,
      "grad_norm": 3.6957085132598877,
      "learning_rate": 4.3120696157870405e-05,
      "loss": 0.6611,
      "step": 904700
    },
    {
      "epoch": 8.25607708591868,
      "grad_norm": 4.1090922355651855,
      "learning_rate": 4.3119935761734435e-05,
      "loss": 0.6876,
      "step": 904800
    },
    {
      "epoch": 8.256989561281845,
      "grad_norm": 3.475411891937256,
      "learning_rate": 4.3119175365598465e-05,
      "loss": 0.7032,
      "step": 904900
    },
    {
      "epoch": 8.25790203664501,
      "grad_norm": 4.165278911590576,
      "learning_rate": 4.3118414969462495e-05,
      "loss": 0.7083,
      "step": 905000
    },
    {
      "epoch": 8.258814512008176,
      "grad_norm": 3.9837112426757812,
      "learning_rate": 4.311765457332652e-05,
      "loss": 0.752,
      "step": 905100
    },
    {
      "epoch": 8.25972698737134,
      "grad_norm": 4.09998893737793,
      "learning_rate": 4.3116894177190555e-05,
      "loss": 0.6292,
      "step": 905200
    },
    {
      "epoch": 8.260639462734506,
      "grad_norm": 4.435786247253418,
      "learning_rate": 4.311613378105458e-05,
      "loss": 0.7121,
      "step": 905300
    },
    {
      "epoch": 8.261551938097671,
      "grad_norm": 4.343197345733643,
      "learning_rate": 4.311537338491861e-05,
      "loss": 0.6529,
      "step": 905400
    },
    {
      "epoch": 8.262464413460837,
      "grad_norm": 4.317851543426514,
      "learning_rate": 4.311461298878264e-05,
      "loss": 0.6806,
      "step": 905500
    },
    {
      "epoch": 8.263376888824002,
      "grad_norm": 4.024224758148193,
      "learning_rate": 4.311385259264667e-05,
      "loss": 0.7071,
      "step": 905600
    },
    {
      "epoch": 8.264289364187167,
      "grad_norm": 4.498983860015869,
      "learning_rate": 4.31130921965107e-05,
      "loss": 0.7055,
      "step": 905700
    },
    {
      "epoch": 8.265201839550333,
      "grad_norm": 5.214164733886719,
      "learning_rate": 4.311233180037473e-05,
      "loss": 0.6934,
      "step": 905800
    },
    {
      "epoch": 8.266114314913498,
      "grad_norm": 3.593114137649536,
      "learning_rate": 4.311157140423875e-05,
      "loss": 0.6807,
      "step": 905900
    },
    {
      "epoch": 8.267026790276663,
      "grad_norm": 4.346960067749023,
      "learning_rate": 4.311081100810279e-05,
      "loss": 0.6933,
      "step": 906000
    },
    {
      "epoch": 8.267939265639829,
      "grad_norm": 2.850217342376709,
      "learning_rate": 4.311005061196681e-05,
      "loss": 0.7099,
      "step": 906100
    },
    {
      "epoch": 8.268851741002994,
      "grad_norm": 4.4721784591674805,
      "learning_rate": 4.3109290215830836e-05,
      "loss": 0.6911,
      "step": 906200
    },
    {
      "epoch": 8.269764216366157,
      "grad_norm": 4.527761459350586,
      "learning_rate": 4.310852981969487e-05,
      "loss": 0.6891,
      "step": 906300
    },
    {
      "epoch": 8.270676691729323,
      "grad_norm": 3.664778470993042,
      "learning_rate": 4.3107769423558896e-05,
      "loss": 0.69,
      "step": 906400
    },
    {
      "epoch": 8.271589167092488,
      "grad_norm": 4.178751468658447,
      "learning_rate": 4.3107009027422926e-05,
      "loss": 0.7141,
      "step": 906500
    },
    {
      "epoch": 8.272501642455653,
      "grad_norm": 3.8218038082122803,
      "learning_rate": 4.3106248631286956e-05,
      "loss": 0.6813,
      "step": 906600
    },
    {
      "epoch": 8.273414117818819,
      "grad_norm": 4.5780487060546875,
      "learning_rate": 4.3105488235150986e-05,
      "loss": 0.6551,
      "step": 906700
    },
    {
      "epoch": 8.274326593181984,
      "grad_norm": 3.825646162033081,
      "learning_rate": 4.3104727839015016e-05,
      "loss": 0.701,
      "step": 906800
    },
    {
      "epoch": 8.27523906854515,
      "grad_norm": 4.826428413391113,
      "learning_rate": 4.3103967442879046e-05,
      "loss": 0.6803,
      "step": 906900
    },
    {
      "epoch": 8.276151543908314,
      "grad_norm": 4.169136047363281,
      "learning_rate": 4.310320704674307e-05,
      "loss": 0.6624,
      "step": 907000
    },
    {
      "epoch": 8.27706401927148,
      "grad_norm": 4.2183356285095215,
      "learning_rate": 4.3102446650607106e-05,
      "loss": 0.7101,
      "step": 907100
    },
    {
      "epoch": 8.277976494634645,
      "grad_norm": 3.4117443561553955,
      "learning_rate": 4.310168625447113e-05,
      "loss": 0.6822,
      "step": 907200
    },
    {
      "epoch": 8.27888896999781,
      "grad_norm": 3.9635074138641357,
      "learning_rate": 4.310092585833516e-05,
      "loss": 0.6802,
      "step": 907300
    },
    {
      "epoch": 8.279801445360976,
      "grad_norm": 4.059930324554443,
      "learning_rate": 4.310016546219919e-05,
      "loss": 0.709,
      "step": 907400
    },
    {
      "epoch": 8.280713920724141,
      "grad_norm": 3.4787979125976562,
      "learning_rate": 4.309940506606322e-05,
      "loss": 0.6986,
      "step": 907500
    },
    {
      "epoch": 8.281626396087306,
      "grad_norm": 4.356464385986328,
      "learning_rate": 4.309864466992724e-05,
      "loss": 0.6862,
      "step": 907600
    },
    {
      "epoch": 8.282538871450472,
      "grad_norm": 4.17765474319458,
      "learning_rate": 4.309788427379128e-05,
      "loss": 0.7074,
      "step": 907700
    },
    {
      "epoch": 8.283451346813637,
      "grad_norm": 3.659579277038574,
      "learning_rate": 4.30971238776553e-05,
      "loss": 0.6701,
      "step": 907800
    },
    {
      "epoch": 8.2843638221768,
      "grad_norm": 4.417840003967285,
      "learning_rate": 4.309636348151933e-05,
      "loss": 0.6916,
      "step": 907900
    },
    {
      "epoch": 8.285276297539966,
      "grad_norm": 3.980186939239502,
      "learning_rate": 4.309560308538336e-05,
      "loss": 0.6979,
      "step": 908000
    },
    {
      "epoch": 8.286188772903131,
      "grad_norm": 3.924341917037964,
      "learning_rate": 4.309484268924739e-05,
      "loss": 0.6783,
      "step": 908100
    },
    {
      "epoch": 8.287101248266296,
      "grad_norm": 3.7140631675720215,
      "learning_rate": 4.3094082293111423e-05,
      "loss": 0.6978,
      "step": 908200
    },
    {
      "epoch": 8.288013723629462,
      "grad_norm": 3.7031219005584717,
      "learning_rate": 4.3093321896975454e-05,
      "loss": 0.7019,
      "step": 908300
    },
    {
      "epoch": 8.288926198992627,
      "grad_norm": 4.412001132965088,
      "learning_rate": 4.309256150083948e-05,
      "loss": 0.7172,
      "step": 908400
    },
    {
      "epoch": 8.289838674355792,
      "grad_norm": 3.45268177986145,
      "learning_rate": 4.3091801104703514e-05,
      "loss": 0.6667,
      "step": 908500
    },
    {
      "epoch": 8.290751149718957,
      "grad_norm": 4.222031116485596,
      "learning_rate": 4.309104070856754e-05,
      "loss": 0.6519,
      "step": 908600
    },
    {
      "epoch": 8.291663625082123,
      "grad_norm": 4.7961554527282715,
      "learning_rate": 4.309028031243157e-05,
      "loss": 0.7047,
      "step": 908700
    },
    {
      "epoch": 8.292576100445288,
      "grad_norm": 4.562835693359375,
      "learning_rate": 4.30895199162956e-05,
      "loss": 0.6977,
      "step": 908800
    },
    {
      "epoch": 8.293488575808453,
      "grad_norm": 3.532726287841797,
      "learning_rate": 4.308875952015962e-05,
      "loss": 0.6916,
      "step": 908900
    },
    {
      "epoch": 8.294401051171619,
      "grad_norm": 4.449898719787598,
      "learning_rate": 4.308799912402365e-05,
      "loss": 0.701,
      "step": 909000
    },
    {
      "epoch": 8.295313526534784,
      "grad_norm": 4.098489284515381,
      "learning_rate": 4.308723872788768e-05,
      "loss": 0.7316,
      "step": 909100
    },
    {
      "epoch": 8.29622600189795,
      "grad_norm": 3.2459867000579834,
      "learning_rate": 4.308647833175171e-05,
      "loss": 0.6767,
      "step": 909200
    },
    {
      "epoch": 8.297138477261115,
      "grad_norm": 3.9901134967803955,
      "learning_rate": 4.308571793561574e-05,
      "loss": 0.7017,
      "step": 909300
    },
    {
      "epoch": 8.29805095262428,
      "grad_norm": 2.8254435062408447,
      "learning_rate": 4.308495753947977e-05,
      "loss": 0.7075,
      "step": 909400
    },
    {
      "epoch": 8.298963427987445,
      "grad_norm": 4.570394992828369,
      "learning_rate": 4.3084197143343794e-05,
      "loss": 0.6841,
      "step": 909500
    },
    {
      "epoch": 8.29987590335061,
      "grad_norm": 4.292635440826416,
      "learning_rate": 4.308343674720783e-05,
      "loss": 0.6732,
      "step": 909600
    },
    {
      "epoch": 8.300788378713774,
      "grad_norm": 4.236603260040283,
      "learning_rate": 4.3082676351071854e-05,
      "loss": 0.7002,
      "step": 909700
    },
    {
      "epoch": 8.30170085407694,
      "grad_norm": 4.393643856048584,
      "learning_rate": 4.3081915954935884e-05,
      "loss": 0.6848,
      "step": 909800
    },
    {
      "epoch": 8.302613329440105,
      "grad_norm": 3.546294927597046,
      "learning_rate": 4.3081155558799914e-05,
      "loss": 0.6914,
      "step": 909900
    },
    {
      "epoch": 8.30352580480327,
      "grad_norm": 4.545020580291748,
      "learning_rate": 4.3080395162663944e-05,
      "loss": 0.6675,
      "step": 910000
    },
    {
      "epoch": 8.304438280166435,
      "grad_norm": 3.8138036727905273,
      "learning_rate": 4.307963476652797e-05,
      "loss": 0.7348,
      "step": 910100
    },
    {
      "epoch": 8.3053507555296,
      "grad_norm": 2.790085792541504,
      "learning_rate": 4.3078874370392004e-05,
      "loss": 0.6403,
      "step": 910200
    },
    {
      "epoch": 8.306263230892766,
      "grad_norm": 3.7121002674102783,
      "learning_rate": 4.307811397425603e-05,
      "loss": 0.6961,
      "step": 910300
    },
    {
      "epoch": 8.307175706255931,
      "grad_norm": 4.110426902770996,
      "learning_rate": 4.307735357812006e-05,
      "loss": 0.7279,
      "step": 910400
    },
    {
      "epoch": 8.308088181619096,
      "grad_norm": 4.23179817199707,
      "learning_rate": 4.307659318198409e-05,
      "loss": 0.6888,
      "step": 910500
    },
    {
      "epoch": 8.309000656982262,
      "grad_norm": 4.183701992034912,
      "learning_rate": 4.307583278584812e-05,
      "loss": 0.6587,
      "step": 910600
    },
    {
      "epoch": 8.309913132345427,
      "grad_norm": 4.499713897705078,
      "learning_rate": 4.307507238971215e-05,
      "loss": 0.7107,
      "step": 910700
    },
    {
      "epoch": 8.310825607708592,
      "grad_norm": 3.822786569595337,
      "learning_rate": 4.307431199357618e-05,
      "loss": 0.663,
      "step": 910800
    },
    {
      "epoch": 8.311738083071758,
      "grad_norm": 4.923058032989502,
      "learning_rate": 4.30735515974402e-05,
      "loss": 0.6933,
      "step": 910900
    },
    {
      "epoch": 8.312650558434923,
      "grad_norm": 4.95471715927124,
      "learning_rate": 4.307279120130424e-05,
      "loss": 0.6509,
      "step": 911000
    },
    {
      "epoch": 8.313563033798088,
      "grad_norm": 4.137294769287109,
      "learning_rate": 4.307203080516826e-05,
      "loss": 0.7122,
      "step": 911100
    },
    {
      "epoch": 8.314475509161253,
      "grad_norm": 3.432532787322998,
      "learning_rate": 4.307127040903229e-05,
      "loss": 0.6649,
      "step": 911200
    },
    {
      "epoch": 8.315387984524417,
      "grad_norm": 3.1268980503082275,
      "learning_rate": 4.307051001289632e-05,
      "loss": 0.718,
      "step": 911300
    },
    {
      "epoch": 8.316300459887582,
      "grad_norm": 3.6780731678009033,
      "learning_rate": 4.306974961676035e-05,
      "loss": 0.7096,
      "step": 911400
    },
    {
      "epoch": 8.317212935250748,
      "grad_norm": 3.6390068531036377,
      "learning_rate": 4.3068989220624375e-05,
      "loss": 0.6939,
      "step": 911500
    },
    {
      "epoch": 8.318125410613913,
      "grad_norm": 4.536137580871582,
      "learning_rate": 4.306822882448841e-05,
      "loss": 0.7072,
      "step": 911600
    },
    {
      "epoch": 8.319037885977078,
      "grad_norm": 4.0996174812316895,
      "learning_rate": 4.3067468428352435e-05,
      "loss": 0.6375,
      "step": 911700
    },
    {
      "epoch": 8.319950361340243,
      "grad_norm": 3.586677312850952,
      "learning_rate": 4.3066708032216465e-05,
      "loss": 0.7024,
      "step": 911800
    },
    {
      "epoch": 8.320862836703409,
      "grad_norm": 4.077868461608887,
      "learning_rate": 4.3065947636080495e-05,
      "loss": 0.7014,
      "step": 911900
    },
    {
      "epoch": 8.321775312066574,
      "grad_norm": 3.2625229358673096,
      "learning_rate": 4.306518723994452e-05,
      "loss": 0.7158,
      "step": 912000
    },
    {
      "epoch": 8.32268778742974,
      "grad_norm": 4.1829447746276855,
      "learning_rate": 4.3064426843808555e-05,
      "loss": 0.7195,
      "step": 912100
    },
    {
      "epoch": 8.323600262792905,
      "grad_norm": 3.64870285987854,
      "learning_rate": 4.306366644767258e-05,
      "loss": 0.7299,
      "step": 912200
    },
    {
      "epoch": 8.32451273815607,
      "grad_norm": 4.8645453453063965,
      "learning_rate": 4.306290605153661e-05,
      "loss": 0.709,
      "step": 912300
    },
    {
      "epoch": 8.325425213519235,
      "grad_norm": 3.6317102909088135,
      "learning_rate": 4.306214565540064e-05,
      "loss": 0.6695,
      "step": 912400
    },
    {
      "epoch": 8.3263376888824,
      "grad_norm": 4.797938823699951,
      "learning_rate": 4.306138525926467e-05,
      "loss": 0.7204,
      "step": 912500
    },
    {
      "epoch": 8.327250164245566,
      "grad_norm": 5.368269443511963,
      "learning_rate": 4.30606248631287e-05,
      "loss": 0.6377,
      "step": 912600
    },
    {
      "epoch": 8.328162639608731,
      "grad_norm": 3.143399715423584,
      "learning_rate": 4.305986446699273e-05,
      "loss": 0.6955,
      "step": 912700
    },
    {
      "epoch": 8.329075114971896,
      "grad_norm": 3.8928098678588867,
      "learning_rate": 4.305910407085675e-05,
      "loss": 0.7129,
      "step": 912800
    },
    {
      "epoch": 8.329987590335062,
      "grad_norm": 3.7717413902282715,
      "learning_rate": 4.305834367472078e-05,
      "loss": 0.688,
      "step": 912900
    },
    {
      "epoch": 8.330900065698227,
      "grad_norm": 3.6500236988067627,
      "learning_rate": 4.305758327858481e-05,
      "loss": 0.6548,
      "step": 913000
    },
    {
      "epoch": 8.33181254106139,
      "grad_norm": 4.384334087371826,
      "learning_rate": 4.305682288244884e-05,
      "loss": 0.6642,
      "step": 913100
    },
    {
      "epoch": 8.332725016424556,
      "grad_norm": 4.168957710266113,
      "learning_rate": 4.305606248631287e-05,
      "loss": 0.7243,
      "step": 913200
    },
    {
      "epoch": 8.333637491787721,
      "grad_norm": 3.944460391998291,
      "learning_rate": 4.30553020901769e-05,
      "loss": 0.7305,
      "step": 913300
    },
    {
      "epoch": 8.334549967150886,
      "grad_norm": 3.875765562057495,
      "learning_rate": 4.3054541694040926e-05,
      "loss": 0.6629,
      "step": 913400
    },
    {
      "epoch": 8.335462442514052,
      "grad_norm": 3.605272054672241,
      "learning_rate": 4.305378129790496e-05,
      "loss": 0.6857,
      "step": 913500
    },
    {
      "epoch": 8.336374917877217,
      "grad_norm": 3.4019198417663574,
      "learning_rate": 4.3053020901768986e-05,
      "loss": 0.6506,
      "step": 913600
    },
    {
      "epoch": 8.337287393240382,
      "grad_norm": 4.167999267578125,
      "learning_rate": 4.3052260505633016e-05,
      "loss": 0.6704,
      "step": 913700
    },
    {
      "epoch": 8.338199868603548,
      "grad_norm": 3.8956069946289062,
      "learning_rate": 4.3051500109497046e-05,
      "loss": 0.6738,
      "step": 913800
    },
    {
      "epoch": 8.339112343966713,
      "grad_norm": 3.7286765575408936,
      "learning_rate": 4.3050739713361076e-05,
      "loss": 0.6795,
      "step": 913900
    },
    {
      "epoch": 8.340024819329878,
      "grad_norm": 4.028064727783203,
      "learning_rate": 4.3049979317225106e-05,
      "loss": 0.7287,
      "step": 914000
    },
    {
      "epoch": 8.340937294693044,
      "grad_norm": 4.394911289215088,
      "learning_rate": 4.3049218921089136e-05,
      "loss": 0.6192,
      "step": 914100
    },
    {
      "epoch": 8.341849770056209,
      "grad_norm": 4.031632423400879,
      "learning_rate": 4.304845852495316e-05,
      "loss": 0.7197,
      "step": 914200
    },
    {
      "epoch": 8.342762245419374,
      "grad_norm": 3.659471273422241,
      "learning_rate": 4.3047698128817197e-05,
      "loss": 0.6736,
      "step": 914300
    },
    {
      "epoch": 8.34367472078254,
      "grad_norm": 5.047649383544922,
      "learning_rate": 4.304693773268122e-05,
      "loss": 0.6925,
      "step": 914400
    },
    {
      "epoch": 8.344587196145705,
      "grad_norm": 4.884827613830566,
      "learning_rate": 4.304617733654525e-05,
      "loss": 0.7003,
      "step": 914500
    },
    {
      "epoch": 8.34549967150887,
      "grad_norm": 3.7175087928771973,
      "learning_rate": 4.304541694040928e-05,
      "loss": 0.6825,
      "step": 914600
    },
    {
      "epoch": 8.346412146872034,
      "grad_norm": 4.137422561645508,
      "learning_rate": 4.30446565442733e-05,
      "loss": 0.6576,
      "step": 914700
    },
    {
      "epoch": 8.347324622235199,
      "grad_norm": 3.9531753063201904,
      "learning_rate": 4.304389614813733e-05,
      "loss": 0.679,
      "step": 914800
    },
    {
      "epoch": 8.348237097598364,
      "grad_norm": 3.6645960807800293,
      "learning_rate": 4.3043135752001363e-05,
      "loss": 0.7025,
      "step": 914900
    },
    {
      "epoch": 8.34914957296153,
      "grad_norm": 4.361098289489746,
      "learning_rate": 4.3042375355865393e-05,
      "loss": 0.7227,
      "step": 915000
    },
    {
      "epoch": 8.350062048324695,
      "grad_norm": 4.282525062561035,
      "learning_rate": 4.3041614959729424e-05,
      "loss": 0.6859,
      "step": 915100
    },
    {
      "epoch": 8.35097452368786,
      "grad_norm": 3.9958231449127197,
      "learning_rate": 4.3040854563593454e-05,
      "loss": 0.6738,
      "step": 915200
    },
    {
      "epoch": 8.351886999051025,
      "grad_norm": 4.034171104431152,
      "learning_rate": 4.304009416745748e-05,
      "loss": 0.7017,
      "step": 915300
    },
    {
      "epoch": 8.35279947441419,
      "grad_norm": 3.753279685974121,
      "learning_rate": 4.3039333771321514e-05,
      "loss": 0.6557,
      "step": 915400
    },
    {
      "epoch": 8.353711949777356,
      "grad_norm": 3.8640644550323486,
      "learning_rate": 4.303857337518554e-05,
      "loss": 0.6765,
      "step": 915500
    },
    {
      "epoch": 8.354624425140521,
      "grad_norm": 4.093048572540283,
      "learning_rate": 4.303781297904957e-05,
      "loss": 0.7517,
      "step": 915600
    },
    {
      "epoch": 8.355536900503687,
      "grad_norm": 3.8590645790100098,
      "learning_rate": 4.30370525829136e-05,
      "loss": 0.6938,
      "step": 915700
    },
    {
      "epoch": 8.356449375866852,
      "grad_norm": 3.9182865619659424,
      "learning_rate": 4.303629218677763e-05,
      "loss": 0.6717,
      "step": 915800
    },
    {
      "epoch": 8.357361851230017,
      "grad_norm": 3.3352575302124023,
      "learning_rate": 4.303553179064165e-05,
      "loss": 0.6669,
      "step": 915900
    },
    {
      "epoch": 8.358274326593182,
      "grad_norm": 4.507414817810059,
      "learning_rate": 4.303477139450569e-05,
      "loss": 0.6734,
      "step": 916000
    },
    {
      "epoch": 8.359186801956348,
      "grad_norm": 4.116888999938965,
      "learning_rate": 4.303401099836971e-05,
      "loss": 0.693,
      "step": 916100
    },
    {
      "epoch": 8.360099277319513,
      "grad_norm": 4.320826053619385,
      "learning_rate": 4.303325060223374e-05,
      "loss": 0.7426,
      "step": 916200
    },
    {
      "epoch": 8.361011752682678,
      "grad_norm": 4.220562934875488,
      "learning_rate": 4.303249020609777e-05,
      "loss": 0.697,
      "step": 916300
    },
    {
      "epoch": 8.361924228045844,
      "grad_norm": 4.370825290679932,
      "learning_rate": 4.30317298099618e-05,
      "loss": 0.7019,
      "step": 916400
    },
    {
      "epoch": 8.362836703409007,
      "grad_norm": 3.9544689655303955,
      "learning_rate": 4.303096941382583e-05,
      "loss": 0.6976,
      "step": 916500
    },
    {
      "epoch": 8.363749178772172,
      "grad_norm": 3.0649454593658447,
      "learning_rate": 4.303020901768986e-05,
      "loss": 0.7035,
      "step": 916600
    },
    {
      "epoch": 8.364661654135338,
      "grad_norm": 3.9191386699676514,
      "learning_rate": 4.3029448621553884e-05,
      "loss": 0.6865,
      "step": 916700
    },
    {
      "epoch": 8.365574129498503,
      "grad_norm": 4.834989070892334,
      "learning_rate": 4.302868822541792e-05,
      "loss": 0.6594,
      "step": 916800
    },
    {
      "epoch": 8.366486604861668,
      "grad_norm": 4.671690940856934,
      "learning_rate": 4.3027927829281944e-05,
      "loss": 0.7027,
      "step": 916900
    },
    {
      "epoch": 8.367399080224834,
      "grad_norm": 3.6265268325805664,
      "learning_rate": 4.3027167433145974e-05,
      "loss": 0.6747,
      "step": 917000
    },
    {
      "epoch": 8.368311555587999,
      "grad_norm": 3.197741746902466,
      "learning_rate": 4.3026407037010005e-05,
      "loss": 0.6946,
      "step": 917100
    },
    {
      "epoch": 8.369224030951164,
      "grad_norm": 4.67659330368042,
      "learning_rate": 4.3025646640874035e-05,
      "loss": 0.6734,
      "step": 917200
    },
    {
      "epoch": 8.37013650631433,
      "grad_norm": 4.04337215423584,
      "learning_rate": 4.302488624473806e-05,
      "loss": 0.6876,
      "step": 917300
    },
    {
      "epoch": 8.371048981677495,
      "grad_norm": 4.087386608123779,
      "learning_rate": 4.302412584860209e-05,
      "loss": 0.6895,
      "step": 917400
    },
    {
      "epoch": 8.37196145704066,
      "grad_norm": 4.4540839195251465,
      "learning_rate": 4.302336545246612e-05,
      "loss": 0.6428,
      "step": 917500
    },
    {
      "epoch": 8.372873932403825,
      "grad_norm": 3.6259706020355225,
      "learning_rate": 4.302260505633015e-05,
      "loss": 0.6937,
      "step": 917600
    },
    {
      "epoch": 8.37378640776699,
      "grad_norm": 4.359135627746582,
      "learning_rate": 4.302184466019418e-05,
      "loss": 0.7014,
      "step": 917700
    },
    {
      "epoch": 8.374698883130156,
      "grad_norm": 4.209835052490234,
      "learning_rate": 4.30210842640582e-05,
      "loss": 0.6771,
      "step": 917800
    },
    {
      "epoch": 8.375611358493321,
      "grad_norm": 4.714927673339844,
      "learning_rate": 4.302032386792224e-05,
      "loss": 0.6648,
      "step": 917900
    },
    {
      "epoch": 8.376523833856487,
      "grad_norm": 4.23422384262085,
      "learning_rate": 4.301956347178626e-05,
      "loss": 0.7159,
      "step": 918000
    },
    {
      "epoch": 8.37743630921965,
      "grad_norm": 4.5980448722839355,
      "learning_rate": 4.301880307565029e-05,
      "loss": 0.6755,
      "step": 918100
    },
    {
      "epoch": 8.378348784582816,
      "grad_norm": 3.7861366271972656,
      "learning_rate": 4.301804267951432e-05,
      "loss": 0.7299,
      "step": 918200
    },
    {
      "epoch": 8.37926125994598,
      "grad_norm": 4.417375087738037,
      "learning_rate": 4.301728228337835e-05,
      "loss": 0.6774,
      "step": 918300
    },
    {
      "epoch": 8.380173735309146,
      "grad_norm": 3.8312079906463623,
      "learning_rate": 4.3016521887242375e-05,
      "loss": 0.6842,
      "step": 918400
    },
    {
      "epoch": 8.381086210672311,
      "grad_norm": 3.8712775707244873,
      "learning_rate": 4.301576149110641e-05,
      "loss": 0.6804,
      "step": 918500
    },
    {
      "epoch": 8.381998686035477,
      "grad_norm": 3.7621490955352783,
      "learning_rate": 4.3015001094970435e-05,
      "loss": 0.7059,
      "step": 918600
    },
    {
      "epoch": 8.382911161398642,
      "grad_norm": 3.887162923812866,
      "learning_rate": 4.3014240698834465e-05,
      "loss": 0.6502,
      "step": 918700
    },
    {
      "epoch": 8.383823636761807,
      "grad_norm": 3.5102264881134033,
      "learning_rate": 4.3013480302698495e-05,
      "loss": 0.6768,
      "step": 918800
    },
    {
      "epoch": 8.384736112124973,
      "grad_norm": 3.0960044860839844,
      "learning_rate": 4.3012719906562525e-05,
      "loss": 0.7043,
      "step": 918900
    },
    {
      "epoch": 8.385648587488138,
      "grad_norm": 3.2137951850891113,
      "learning_rate": 4.3011959510426556e-05,
      "loss": 0.6535,
      "step": 919000
    },
    {
      "epoch": 8.386561062851303,
      "grad_norm": 3.097700357437134,
      "learning_rate": 4.3011199114290586e-05,
      "loss": 0.701,
      "step": 919100
    },
    {
      "epoch": 8.387473538214469,
      "grad_norm": 4.191669940948486,
      "learning_rate": 4.301043871815461e-05,
      "loss": 0.6809,
      "step": 919200
    },
    {
      "epoch": 8.388386013577634,
      "grad_norm": 4.257297992706299,
      "learning_rate": 4.3009678322018646e-05,
      "loss": 0.6857,
      "step": 919300
    },
    {
      "epoch": 8.389298488940799,
      "grad_norm": 4.010186195373535,
      "learning_rate": 4.300891792588267e-05,
      "loss": 0.6982,
      "step": 919400
    },
    {
      "epoch": 8.390210964303964,
      "grad_norm": 4.996772289276123,
      "learning_rate": 4.30081575297467e-05,
      "loss": 0.6883,
      "step": 919500
    },
    {
      "epoch": 8.39112343966713,
      "grad_norm": 3.5900766849517822,
      "learning_rate": 4.300739713361073e-05,
      "loss": 0.6721,
      "step": 919600
    },
    {
      "epoch": 8.392035915030295,
      "grad_norm": 4.723376274108887,
      "learning_rate": 4.300663673747476e-05,
      "loss": 0.6746,
      "step": 919700
    },
    {
      "epoch": 8.392948390393459,
      "grad_norm": 3.945683479309082,
      "learning_rate": 4.300587634133878e-05,
      "loss": 0.6645,
      "step": 919800
    },
    {
      "epoch": 8.393860865756624,
      "grad_norm": 3.9775657653808594,
      "learning_rate": 4.300511594520282e-05,
      "loss": 0.6932,
      "step": 919900
    },
    {
      "epoch": 8.39477334111979,
      "grad_norm": 4.408447742462158,
      "learning_rate": 4.300435554906684e-05,
      "loss": 0.6741,
      "step": 920000
    },
    {
      "epoch": 8.395685816482954,
      "grad_norm": 3.390110969543457,
      "learning_rate": 4.300359515293087e-05,
      "loss": 0.6671,
      "step": 920100
    },
    {
      "epoch": 8.39659829184612,
      "grad_norm": 4.378751754760742,
      "learning_rate": 4.30028347567949e-05,
      "loss": 0.6917,
      "step": 920200
    },
    {
      "epoch": 8.397510767209285,
      "grad_norm": 4.6700968742370605,
      "learning_rate": 4.3002074360658926e-05,
      "loss": 0.7233,
      "step": 920300
    },
    {
      "epoch": 8.39842324257245,
      "grad_norm": 4.1554999351501465,
      "learning_rate": 4.300131396452296e-05,
      "loss": 0.726,
      "step": 920400
    },
    {
      "epoch": 8.399335717935616,
      "grad_norm": 3.734680652618408,
      "learning_rate": 4.3000553568386986e-05,
      "loss": 0.67,
      "step": 920500
    },
    {
      "epoch": 8.400248193298781,
      "grad_norm": 3.0936365127563477,
      "learning_rate": 4.2999793172251016e-05,
      "loss": 0.705,
      "step": 920600
    },
    {
      "epoch": 8.401160668661946,
      "grad_norm": 3.8574023246765137,
      "learning_rate": 4.2999032776115046e-05,
      "loss": 0.706,
      "step": 920700
    },
    {
      "epoch": 8.402073144025112,
      "grad_norm": 4.2245965003967285,
      "learning_rate": 4.2998272379979076e-05,
      "loss": 0.6874,
      "step": 920800
    },
    {
      "epoch": 8.402985619388277,
      "grad_norm": 4.676945686340332,
      "learning_rate": 4.29975119838431e-05,
      "loss": 0.706,
      "step": 920900
    },
    {
      "epoch": 8.403898094751442,
      "grad_norm": 4.029869556427002,
      "learning_rate": 4.2996751587707137e-05,
      "loss": 0.6947,
      "step": 921000
    },
    {
      "epoch": 8.404810570114607,
      "grad_norm": 4.104787826538086,
      "learning_rate": 4.299599119157116e-05,
      "loss": 0.6799,
      "step": 921100
    },
    {
      "epoch": 8.405723045477773,
      "grad_norm": 4.070987224578857,
      "learning_rate": 4.299523079543519e-05,
      "loss": 0.6753,
      "step": 921200
    },
    {
      "epoch": 8.406635520840938,
      "grad_norm": 3.4864654541015625,
      "learning_rate": 4.299447039929922e-05,
      "loss": 0.6935,
      "step": 921300
    },
    {
      "epoch": 8.407547996204103,
      "grad_norm": 4.106379508972168,
      "learning_rate": 4.299371000316325e-05,
      "loss": 0.6869,
      "step": 921400
    },
    {
      "epoch": 8.408460471567267,
      "grad_norm": 3.673173666000366,
      "learning_rate": 4.299294960702728e-05,
      "loss": 0.7057,
      "step": 921500
    },
    {
      "epoch": 8.409372946930432,
      "grad_norm": 3.478827714920044,
      "learning_rate": 4.299218921089131e-05,
      "loss": 0.6828,
      "step": 921600
    },
    {
      "epoch": 8.410285422293597,
      "grad_norm": 5.402975559234619,
      "learning_rate": 4.2991428814755333e-05,
      "loss": 0.7155,
      "step": 921700
    },
    {
      "epoch": 8.411197897656763,
      "grad_norm": 2.9330787658691406,
      "learning_rate": 4.299066841861937e-05,
      "loss": 0.6768,
      "step": 921800
    },
    {
      "epoch": 8.412110373019928,
      "grad_norm": 2.6334116458892822,
      "learning_rate": 4.2989908022483394e-05,
      "loss": 0.6874,
      "step": 921900
    },
    {
      "epoch": 8.413022848383093,
      "grad_norm": 3.8905439376831055,
      "learning_rate": 4.2989147626347424e-05,
      "loss": 0.6765,
      "step": 922000
    },
    {
      "epoch": 8.413935323746259,
      "grad_norm": 4.473351955413818,
      "learning_rate": 4.2988387230211454e-05,
      "loss": 0.7131,
      "step": 922100
    },
    {
      "epoch": 8.414847799109424,
      "grad_norm": 4.230125427246094,
      "learning_rate": 4.2987626834075484e-05,
      "loss": 0.6428,
      "step": 922200
    },
    {
      "epoch": 8.41576027447259,
      "grad_norm": 4.623242378234863,
      "learning_rate": 4.298686643793951e-05,
      "loss": 0.7429,
      "step": 922300
    },
    {
      "epoch": 8.416672749835755,
      "grad_norm": 4.806014060974121,
      "learning_rate": 4.2986106041803544e-05,
      "loss": 0.7237,
      "step": 922400
    },
    {
      "epoch": 8.41758522519892,
      "grad_norm": 4.563775539398193,
      "learning_rate": 4.298534564566757e-05,
      "loss": 0.7347,
      "step": 922500
    },
    {
      "epoch": 8.418497700562085,
      "grad_norm": 4.240817070007324,
      "learning_rate": 4.29845852495316e-05,
      "loss": 0.6927,
      "step": 922600
    },
    {
      "epoch": 8.41941017592525,
      "grad_norm": 5.562720775604248,
      "learning_rate": 4.298382485339563e-05,
      "loss": 0.6956,
      "step": 922700
    },
    {
      "epoch": 8.420322651288416,
      "grad_norm": 3.5669736862182617,
      "learning_rate": 4.298306445725966e-05,
      "loss": 0.6612,
      "step": 922800
    },
    {
      "epoch": 8.421235126651581,
      "grad_norm": 3.902909994125366,
      "learning_rate": 4.298230406112369e-05,
      "loss": 0.695,
      "step": 922900
    },
    {
      "epoch": 8.422147602014746,
      "grad_norm": 4.012169361114502,
      "learning_rate": 4.298154366498772e-05,
      "loss": 0.7103,
      "step": 923000
    },
    {
      "epoch": 8.423060077377912,
      "grad_norm": 4.462925434112549,
      "learning_rate": 4.298078326885174e-05,
      "loss": 0.6698,
      "step": 923100
    },
    {
      "epoch": 8.423972552741075,
      "grad_norm": 3.3578896522521973,
      "learning_rate": 4.298002287271577e-05,
      "loss": 0.7297,
      "step": 923200
    },
    {
      "epoch": 8.42488502810424,
      "grad_norm": 4.193613529205322,
      "learning_rate": 4.29792624765798e-05,
      "loss": 0.7099,
      "step": 923300
    },
    {
      "epoch": 8.425797503467406,
      "grad_norm": 4.707058906555176,
      "learning_rate": 4.2978502080443824e-05,
      "loss": 0.688,
      "step": 923400
    },
    {
      "epoch": 8.426709978830571,
      "grad_norm": 3.751340866088867,
      "learning_rate": 4.297774168430786e-05,
      "loss": 0.7085,
      "step": 923500
    },
    {
      "epoch": 8.427622454193736,
      "grad_norm": 4.215314865112305,
      "learning_rate": 4.2976981288171884e-05,
      "loss": 0.7161,
      "step": 923600
    },
    {
      "epoch": 8.428534929556902,
      "grad_norm": 4.332005023956299,
      "learning_rate": 4.2976220892035914e-05,
      "loss": 0.6772,
      "step": 923700
    },
    {
      "epoch": 8.429447404920067,
      "grad_norm": 3.397000312805176,
      "learning_rate": 4.2975460495899945e-05,
      "loss": 0.705,
      "step": 923800
    },
    {
      "epoch": 8.430359880283232,
      "grad_norm": 4.273595333099365,
      "learning_rate": 4.2974700099763975e-05,
      "loss": 0.689,
      "step": 923900
    },
    {
      "epoch": 8.431272355646398,
      "grad_norm": 3.7249178886413574,
      "learning_rate": 4.2973939703628005e-05,
      "loss": 0.7084,
      "step": 924000
    },
    {
      "epoch": 8.432184831009563,
      "grad_norm": 4.2104973793029785,
      "learning_rate": 4.2973179307492035e-05,
      "loss": 0.6746,
      "step": 924100
    },
    {
      "epoch": 8.433097306372728,
      "grad_norm": 3.523674488067627,
      "learning_rate": 4.297241891135606e-05,
      "loss": 0.7282,
      "step": 924200
    },
    {
      "epoch": 8.434009781735893,
      "grad_norm": 3.3104758262634277,
      "learning_rate": 4.2971658515220095e-05,
      "loss": 0.6756,
      "step": 924300
    },
    {
      "epoch": 8.434922257099059,
      "grad_norm": 3.598187208175659,
      "learning_rate": 4.297089811908412e-05,
      "loss": 0.6982,
      "step": 924400
    },
    {
      "epoch": 8.435834732462224,
      "grad_norm": 4.198276996612549,
      "learning_rate": 4.297013772294815e-05,
      "loss": 0.705,
      "step": 924500
    },
    {
      "epoch": 8.43674720782539,
      "grad_norm": 4.026358604431152,
      "learning_rate": 4.296937732681218e-05,
      "loss": 0.6651,
      "step": 924600
    },
    {
      "epoch": 8.437659683188555,
      "grad_norm": 4.416149139404297,
      "learning_rate": 4.296861693067621e-05,
      "loss": 0.706,
      "step": 924700
    },
    {
      "epoch": 8.43857215855172,
      "grad_norm": 4.332330703735352,
      "learning_rate": 4.296785653454024e-05,
      "loss": 0.6863,
      "step": 924800
    },
    {
      "epoch": 8.439484633914883,
      "grad_norm": 4.548913478851318,
      "learning_rate": 4.296709613840427e-05,
      "loss": 0.6798,
      "step": 924900
    },
    {
      "epoch": 8.440397109278049,
      "grad_norm": 2.585111141204834,
      "learning_rate": 4.296633574226829e-05,
      "loss": 0.6446,
      "step": 925000
    },
    {
      "epoch": 8.441309584641214,
      "grad_norm": 4.209070205688477,
      "learning_rate": 4.296557534613232e-05,
      "loss": 0.7084,
      "step": 925100
    },
    {
      "epoch": 8.44222206000438,
      "grad_norm": 3.613788366317749,
      "learning_rate": 4.296481494999635e-05,
      "loss": 0.6777,
      "step": 925200
    },
    {
      "epoch": 8.443134535367545,
      "grad_norm": 4.34600305557251,
      "learning_rate": 4.296405455386038e-05,
      "loss": 0.7342,
      "step": 925300
    },
    {
      "epoch": 8.44404701073071,
      "grad_norm": 3.8210103511810303,
      "learning_rate": 4.296329415772441e-05,
      "loss": 0.7179,
      "step": 925400
    },
    {
      "epoch": 8.444959486093875,
      "grad_norm": 3.6017961502075195,
      "learning_rate": 4.296253376158844e-05,
      "loss": 0.6901,
      "step": 925500
    },
    {
      "epoch": 8.44587196145704,
      "grad_norm": 3.413083076477051,
      "learning_rate": 4.2961773365452465e-05,
      "loss": 0.6704,
      "step": 925600
    },
    {
      "epoch": 8.446784436820206,
      "grad_norm": 2.939394474029541,
      "learning_rate": 4.29610129693165e-05,
      "loss": 0.6842,
      "step": 925700
    },
    {
      "epoch": 8.447696912183371,
      "grad_norm": 4.304088592529297,
      "learning_rate": 4.2960252573180526e-05,
      "loss": 0.7179,
      "step": 925800
    },
    {
      "epoch": 8.448609387546536,
      "grad_norm": 3.677213191986084,
      "learning_rate": 4.2959492177044556e-05,
      "loss": 0.711,
      "step": 925900
    },
    {
      "epoch": 8.449521862909702,
      "grad_norm": 3.438796281814575,
      "learning_rate": 4.2958731780908586e-05,
      "loss": 0.7023,
      "step": 926000
    },
    {
      "epoch": 8.450434338272867,
      "grad_norm": 3.4581105709075928,
      "learning_rate": 4.295797138477261e-05,
      "loss": 0.6774,
      "step": 926100
    },
    {
      "epoch": 8.451346813636032,
      "grad_norm": 3.6093826293945312,
      "learning_rate": 4.2957210988636646e-05,
      "loss": 0.717,
      "step": 926200
    },
    {
      "epoch": 8.452259288999198,
      "grad_norm": 3.3679358959198,
      "learning_rate": 4.295645059250067e-05,
      "loss": 0.6905,
      "step": 926300
    },
    {
      "epoch": 8.453171764362363,
      "grad_norm": 4.481640338897705,
      "learning_rate": 4.29556901963647e-05,
      "loss": 0.7099,
      "step": 926400
    },
    {
      "epoch": 8.454084239725528,
      "grad_norm": 4.854067325592041,
      "learning_rate": 4.295492980022873e-05,
      "loss": 0.6794,
      "step": 926500
    },
    {
      "epoch": 8.454996715088692,
      "grad_norm": 3.6919541358947754,
      "learning_rate": 4.295416940409276e-05,
      "loss": 0.7254,
      "step": 926600
    },
    {
      "epoch": 8.455909190451857,
      "grad_norm": 4.612982749938965,
      "learning_rate": 4.295340900795678e-05,
      "loss": 0.6822,
      "step": 926700
    },
    {
      "epoch": 8.456821665815022,
      "grad_norm": 5.219628810882568,
      "learning_rate": 4.295264861182082e-05,
      "loss": 0.7354,
      "step": 926800
    },
    {
      "epoch": 8.457734141178188,
      "grad_norm": 4.474000930786133,
      "learning_rate": 4.295188821568484e-05,
      "loss": 0.655,
      "step": 926900
    },
    {
      "epoch": 8.458646616541353,
      "grad_norm": 4.765678405761719,
      "learning_rate": 4.295112781954887e-05,
      "loss": 0.7356,
      "step": 927000
    },
    {
      "epoch": 8.459559091904518,
      "grad_norm": 4.523629188537598,
      "learning_rate": 4.29503674234129e-05,
      "loss": 0.6878,
      "step": 927100
    },
    {
      "epoch": 8.460471567267684,
      "grad_norm": 4.226830959320068,
      "learning_rate": 4.294960702727693e-05,
      "loss": 0.711,
      "step": 927200
    },
    {
      "epoch": 8.461384042630849,
      "grad_norm": 3.9884538650512695,
      "learning_rate": 4.294884663114096e-05,
      "loss": 0.6869,
      "step": 927300
    },
    {
      "epoch": 8.462296517994014,
      "grad_norm": 2.628687620162964,
      "learning_rate": 4.294808623500499e-05,
      "loss": 0.7413,
      "step": 927400
    },
    {
      "epoch": 8.46320899335718,
      "grad_norm": 3.4836654663085938,
      "learning_rate": 4.2947325838869016e-05,
      "loss": 0.6996,
      "step": 927500
    },
    {
      "epoch": 8.464121468720345,
      "grad_norm": 4.267566204071045,
      "learning_rate": 4.294656544273305e-05,
      "loss": 0.6856,
      "step": 927600
    },
    {
      "epoch": 8.46503394408351,
      "grad_norm": 3.647613525390625,
      "learning_rate": 4.2945805046597076e-05,
      "loss": 0.6782,
      "step": 927700
    },
    {
      "epoch": 8.465946419446675,
      "grad_norm": 3.7128446102142334,
      "learning_rate": 4.2945044650461107e-05,
      "loss": 0.7082,
      "step": 927800
    },
    {
      "epoch": 8.46685889480984,
      "grad_norm": 4.408089637756348,
      "learning_rate": 4.294428425432514e-05,
      "loss": 0.7319,
      "step": 927900
    },
    {
      "epoch": 8.467771370173006,
      "grad_norm": 4.069081783294678,
      "learning_rate": 4.294352385818917e-05,
      "loss": 0.6529,
      "step": 928000
    },
    {
      "epoch": 8.468683845536171,
      "grad_norm": 3.608491897583008,
      "learning_rate": 4.294276346205319e-05,
      "loss": 0.6759,
      "step": 928100
    },
    {
      "epoch": 8.469596320899337,
      "grad_norm": 3.7213845252990723,
      "learning_rate": 4.294200306591723e-05,
      "loss": 0.7076,
      "step": 928200
    },
    {
      "epoch": 8.4705087962625,
      "grad_norm": 4.346243381500244,
      "learning_rate": 4.294124266978125e-05,
      "loss": 0.7046,
      "step": 928300
    },
    {
      "epoch": 8.471421271625665,
      "grad_norm": 4.360228061676025,
      "learning_rate": 4.294048227364528e-05,
      "loss": 0.7307,
      "step": 928400
    },
    {
      "epoch": 8.47233374698883,
      "grad_norm": 3.0334978103637695,
      "learning_rate": 4.293972187750931e-05,
      "loss": 0.6981,
      "step": 928500
    },
    {
      "epoch": 8.473246222351996,
      "grad_norm": 4.282708168029785,
      "learning_rate": 4.293896148137334e-05,
      "loss": 0.636,
      "step": 928600
    },
    {
      "epoch": 8.474158697715161,
      "grad_norm": 3.076488733291626,
      "learning_rate": 4.293820108523737e-05,
      "loss": 0.6848,
      "step": 928700
    },
    {
      "epoch": 8.475071173078327,
      "grad_norm": 4.70486307144165,
      "learning_rate": 4.2937440689101394e-05,
      "loss": 0.7203,
      "step": 928800
    },
    {
      "epoch": 8.475983648441492,
      "grad_norm": 2.994544744491577,
      "learning_rate": 4.2936680292965424e-05,
      "loss": 0.6824,
      "step": 928900
    },
    {
      "epoch": 8.476896123804657,
      "grad_norm": 3.184018611907959,
      "learning_rate": 4.2935919896829454e-05,
      "loss": 0.6973,
      "step": 929000
    },
    {
      "epoch": 8.477808599167822,
      "grad_norm": 3.924750566482544,
      "learning_rate": 4.2935159500693484e-05,
      "loss": 0.6508,
      "step": 929100
    },
    {
      "epoch": 8.478721074530988,
      "grad_norm": 4.318126201629639,
      "learning_rate": 4.293439910455751e-05,
      "loss": 0.6967,
      "step": 929200
    },
    {
      "epoch": 8.479633549894153,
      "grad_norm": 3.914992332458496,
      "learning_rate": 4.2933638708421544e-05,
      "loss": 0.6753,
      "step": 929300
    },
    {
      "epoch": 8.480546025257318,
      "grad_norm": 3.507265567779541,
      "learning_rate": 4.293287831228557e-05,
      "loss": 0.7108,
      "step": 929400
    },
    {
      "epoch": 8.481458500620484,
      "grad_norm": 4.357921123504639,
      "learning_rate": 4.29321179161496e-05,
      "loss": 0.6855,
      "step": 929500
    },
    {
      "epoch": 8.482370975983649,
      "grad_norm": 3.7227354049682617,
      "learning_rate": 4.293135752001363e-05,
      "loss": 0.7208,
      "step": 929600
    },
    {
      "epoch": 8.483283451346814,
      "grad_norm": 5.706446170806885,
      "learning_rate": 4.293059712387766e-05,
      "loss": 0.6991,
      "step": 929700
    },
    {
      "epoch": 8.48419592670998,
      "grad_norm": 4.307857513427734,
      "learning_rate": 4.292983672774169e-05,
      "loss": 0.7243,
      "step": 929800
    },
    {
      "epoch": 8.485108402073145,
      "grad_norm": 4.351646423339844,
      "learning_rate": 4.292907633160572e-05,
      "loss": 0.6959,
      "step": 929900
    },
    {
      "epoch": 8.486020877436308,
      "grad_norm": 2.8590245246887207,
      "learning_rate": 4.292831593546974e-05,
      "loss": 0.719,
      "step": 930000
    },
    {
      "epoch": 8.486933352799474,
      "grad_norm": 4.281290054321289,
      "learning_rate": 4.292755553933378e-05,
      "loss": 0.6829,
      "step": 930100
    },
    {
      "epoch": 8.487845828162639,
      "grad_norm": 3.9819271564483643,
      "learning_rate": 4.29267951431978e-05,
      "loss": 0.6819,
      "step": 930200
    },
    {
      "epoch": 8.488758303525804,
      "grad_norm": 4.524006366729736,
      "learning_rate": 4.292603474706183e-05,
      "loss": 0.659,
      "step": 930300
    },
    {
      "epoch": 8.48967077888897,
      "grad_norm": 4.850165843963623,
      "learning_rate": 4.292527435092586e-05,
      "loss": 0.6846,
      "step": 930400
    },
    {
      "epoch": 8.490583254252135,
      "grad_norm": 4.241346836090088,
      "learning_rate": 4.292451395478989e-05,
      "loss": 0.6959,
      "step": 930500
    },
    {
      "epoch": 8.4914957296153,
      "grad_norm": 4.62457275390625,
      "learning_rate": 4.2923753558653915e-05,
      "loss": 0.6859,
      "step": 930600
    },
    {
      "epoch": 8.492408204978465,
      "grad_norm": 3.6817970275878906,
      "learning_rate": 4.292299316251795e-05,
      "loss": 0.6777,
      "step": 930700
    },
    {
      "epoch": 8.49332068034163,
      "grad_norm": 4.114397048950195,
      "learning_rate": 4.2922232766381975e-05,
      "loss": 0.6945,
      "step": 930800
    },
    {
      "epoch": 8.494233155704796,
      "grad_norm": 4.381923675537109,
      "learning_rate": 4.2921472370246005e-05,
      "loss": 0.682,
      "step": 930900
    },
    {
      "epoch": 8.495145631067961,
      "grad_norm": 4.2336249351501465,
      "learning_rate": 4.2920711974110035e-05,
      "loss": 0.7354,
      "step": 931000
    },
    {
      "epoch": 8.496058106431127,
      "grad_norm": 3.6348021030426025,
      "learning_rate": 4.2919951577974065e-05,
      "loss": 0.7248,
      "step": 931100
    },
    {
      "epoch": 8.496970581794292,
      "grad_norm": 3.9921913146972656,
      "learning_rate": 4.2919191181838095e-05,
      "loss": 0.6875,
      "step": 931200
    },
    {
      "epoch": 8.497883057157457,
      "grad_norm": 3.4525582790374756,
      "learning_rate": 4.2918430785702125e-05,
      "loss": 0.721,
      "step": 931300
    },
    {
      "epoch": 8.498795532520623,
      "grad_norm": 2.828700542449951,
      "learning_rate": 4.291767038956615e-05,
      "loss": 0.7077,
      "step": 931400
    },
    {
      "epoch": 8.499708007883788,
      "grad_norm": 5.164009094238281,
      "learning_rate": 4.2916909993430185e-05,
      "loss": 0.6619,
      "step": 931500
    },
    {
      "epoch": 8.500620483246951,
      "grad_norm": 4.779816627502441,
      "learning_rate": 4.291614959729421e-05,
      "loss": 0.719,
      "step": 931600
    },
    {
      "epoch": 8.501532958610117,
      "grad_norm": 2.5595078468322754,
      "learning_rate": 4.291538920115823e-05,
      "loss": 0.6988,
      "step": 931700
    },
    {
      "epoch": 8.502445433973282,
      "grad_norm": 3.7351009845733643,
      "learning_rate": 4.291462880502227e-05,
      "loss": 0.6864,
      "step": 931800
    },
    {
      "epoch": 8.503357909336447,
      "grad_norm": 3.7129979133605957,
      "learning_rate": 4.291386840888629e-05,
      "loss": 0.6594,
      "step": 931900
    },
    {
      "epoch": 8.504270384699613,
      "grad_norm": 4.723987579345703,
      "learning_rate": 4.291310801275032e-05,
      "loss": 0.6707,
      "step": 932000
    },
    {
      "epoch": 8.505182860062778,
      "grad_norm": 4.564173221588135,
      "learning_rate": 4.291234761661435e-05,
      "loss": 0.695,
      "step": 932100
    },
    {
      "epoch": 8.506095335425943,
      "grad_norm": 3.9447848796844482,
      "learning_rate": 4.291158722047838e-05,
      "loss": 0.6988,
      "step": 932200
    },
    {
      "epoch": 8.507007810789109,
      "grad_norm": 4.316539287567139,
      "learning_rate": 4.291082682434241e-05,
      "loss": 0.6915,
      "step": 932300
    },
    {
      "epoch": 8.507920286152274,
      "grad_norm": 4.80819034576416,
      "learning_rate": 4.291006642820644e-05,
      "loss": 0.7133,
      "step": 932400
    },
    {
      "epoch": 8.508832761515439,
      "grad_norm": 3.713784694671631,
      "learning_rate": 4.2909306032070466e-05,
      "loss": 0.7056,
      "step": 932500
    },
    {
      "epoch": 8.509745236878604,
      "grad_norm": 3.2157106399536133,
      "learning_rate": 4.29085456359345e-05,
      "loss": 0.6935,
      "step": 932600
    },
    {
      "epoch": 8.51065771224177,
      "grad_norm": 4.04204797744751,
      "learning_rate": 4.2907785239798526e-05,
      "loss": 0.6447,
      "step": 932700
    },
    {
      "epoch": 8.511570187604935,
      "grad_norm": 3.763472318649292,
      "learning_rate": 4.2907024843662556e-05,
      "loss": 0.7393,
      "step": 932800
    },
    {
      "epoch": 8.5124826629681,
      "grad_norm": 4.609529495239258,
      "learning_rate": 4.2906264447526586e-05,
      "loss": 0.6944,
      "step": 932900
    },
    {
      "epoch": 8.513395138331266,
      "grad_norm": 4.254741668701172,
      "learning_rate": 4.2905504051390616e-05,
      "loss": 0.6972,
      "step": 933000
    },
    {
      "epoch": 8.514307613694431,
      "grad_norm": 3.924105644226074,
      "learning_rate": 4.290474365525464e-05,
      "loss": 0.6616,
      "step": 933100
    },
    {
      "epoch": 8.515220089057596,
      "grad_norm": 3.558948516845703,
      "learning_rate": 4.2903983259118676e-05,
      "loss": 0.6985,
      "step": 933200
    },
    {
      "epoch": 8.516132564420761,
      "grad_norm": 4.414175510406494,
      "learning_rate": 4.29032228629827e-05,
      "loss": 0.7091,
      "step": 933300
    },
    {
      "epoch": 8.517045039783925,
      "grad_norm": 4.591301918029785,
      "learning_rate": 4.290246246684673e-05,
      "loss": 0.7072,
      "step": 933400
    },
    {
      "epoch": 8.51795751514709,
      "grad_norm": 4.617011547088623,
      "learning_rate": 4.290170207071076e-05,
      "loss": 0.6891,
      "step": 933500
    },
    {
      "epoch": 8.518869990510256,
      "grad_norm": 3.3121681213378906,
      "learning_rate": 4.290094167457479e-05,
      "loss": 0.694,
      "step": 933600
    },
    {
      "epoch": 8.519782465873421,
      "grad_norm": 3.2786638736724854,
      "learning_rate": 4.290018127843882e-05,
      "loss": 0.7168,
      "step": 933700
    },
    {
      "epoch": 8.520694941236586,
      "grad_norm": 4.590315341949463,
      "learning_rate": 4.289942088230285e-05,
      "loss": 0.7033,
      "step": 933800
    },
    {
      "epoch": 8.521607416599752,
      "grad_norm": 4.554507255554199,
      "learning_rate": 4.289866048616687e-05,
      "loss": 0.6998,
      "step": 933900
    },
    {
      "epoch": 8.522519891962917,
      "grad_norm": 4.041472911834717,
      "learning_rate": 4.289790009003091e-05,
      "loss": 0.6794,
      "step": 934000
    },
    {
      "epoch": 8.523432367326082,
      "grad_norm": 3.964993953704834,
      "learning_rate": 4.289713969389493e-05,
      "loss": 0.6646,
      "step": 934100
    },
    {
      "epoch": 8.524344842689247,
      "grad_norm": 4.22160005569458,
      "learning_rate": 4.289637929775896e-05,
      "loss": 0.6876,
      "step": 934200
    },
    {
      "epoch": 8.525257318052413,
      "grad_norm": 3.7093753814697266,
      "learning_rate": 4.289561890162299e-05,
      "loss": 0.7073,
      "step": 934300
    },
    {
      "epoch": 8.526169793415578,
      "grad_norm": 4.52933931350708,
      "learning_rate": 4.289485850548702e-05,
      "loss": 0.6714,
      "step": 934400
    },
    {
      "epoch": 8.527082268778743,
      "grad_norm": 3.3797290325164795,
      "learning_rate": 4.2894098109351047e-05,
      "loss": 0.6592,
      "step": 934500
    },
    {
      "epoch": 8.527994744141909,
      "grad_norm": 3.5388522148132324,
      "learning_rate": 4.2893337713215077e-05,
      "loss": 0.6578,
      "step": 934600
    },
    {
      "epoch": 8.528907219505074,
      "grad_norm": 3.158256769180298,
      "learning_rate": 4.289257731707911e-05,
      "loss": 0.7078,
      "step": 934700
    },
    {
      "epoch": 8.52981969486824,
      "grad_norm": 5.015801429748535,
      "learning_rate": 4.289181692094314e-05,
      "loss": 0.6925,
      "step": 934800
    },
    {
      "epoch": 8.530732170231405,
      "grad_norm": 4.841044902801514,
      "learning_rate": 4.289105652480717e-05,
      "loss": 0.6887,
      "step": 934900
    },
    {
      "epoch": 8.531644645594568,
      "grad_norm": 4.485329627990723,
      "learning_rate": 4.289029612867119e-05,
      "loss": 0.6621,
      "step": 935000
    },
    {
      "epoch": 8.532557120957733,
      "grad_norm": 3.1783039569854736,
      "learning_rate": 4.288953573253523e-05,
      "loss": 0.682,
      "step": 935100
    },
    {
      "epoch": 8.533469596320899,
      "grad_norm": 3.822097063064575,
      "learning_rate": 4.288877533639925e-05,
      "loss": 0.685,
      "step": 935200
    },
    {
      "epoch": 8.534382071684064,
      "grad_norm": 4.128735065460205,
      "learning_rate": 4.288801494026328e-05,
      "loss": 0.6977,
      "step": 935300
    },
    {
      "epoch": 8.53529454704723,
      "grad_norm": 4.7984113693237305,
      "learning_rate": 4.288725454412731e-05,
      "loss": 0.6895,
      "step": 935400
    },
    {
      "epoch": 8.536207022410395,
      "grad_norm": 4.1758341789245605,
      "learning_rate": 4.288649414799134e-05,
      "loss": 0.678,
      "step": 935500
    },
    {
      "epoch": 8.53711949777356,
      "grad_norm": 3.2647829055786133,
      "learning_rate": 4.2885733751855364e-05,
      "loss": 0.6219,
      "step": 935600
    },
    {
      "epoch": 8.538031973136725,
      "grad_norm": 3.612480640411377,
      "learning_rate": 4.28849733557194e-05,
      "loss": 0.658,
      "step": 935700
    },
    {
      "epoch": 8.53894444849989,
      "grad_norm": 3.4187300205230713,
      "learning_rate": 4.2884212959583424e-05,
      "loss": 0.6866,
      "step": 935800
    },
    {
      "epoch": 8.539856923863056,
      "grad_norm": 4.633886337280273,
      "learning_rate": 4.2883452563447454e-05,
      "loss": 0.7208,
      "step": 935900
    },
    {
      "epoch": 8.540769399226221,
      "grad_norm": 4.349327087402344,
      "learning_rate": 4.2882692167311484e-05,
      "loss": 0.6721,
      "step": 936000
    },
    {
      "epoch": 8.541681874589386,
      "grad_norm": 4.011903762817383,
      "learning_rate": 4.2881931771175514e-05,
      "loss": 0.6804,
      "step": 936100
    },
    {
      "epoch": 8.542594349952552,
      "grad_norm": 4.529149532318115,
      "learning_rate": 4.2881171375039544e-05,
      "loss": 0.6578,
      "step": 936200
    },
    {
      "epoch": 8.543506825315717,
      "grad_norm": 3.2186732292175293,
      "learning_rate": 4.2880410978903574e-05,
      "loss": 0.7149,
      "step": 936300
    },
    {
      "epoch": 8.544419300678882,
      "grad_norm": 3.9136898517608643,
      "learning_rate": 4.28796505827676e-05,
      "loss": 0.6861,
      "step": 936400
    },
    {
      "epoch": 8.545331776042048,
      "grad_norm": 4.987484931945801,
      "learning_rate": 4.2878890186631634e-05,
      "loss": 0.7151,
      "step": 936500
    },
    {
      "epoch": 8.546244251405213,
      "grad_norm": 4.216484546661377,
      "learning_rate": 4.287812979049566e-05,
      "loss": 0.683,
      "step": 936600
    },
    {
      "epoch": 8.547156726768378,
      "grad_norm": 4.45297384262085,
      "learning_rate": 4.287736939435969e-05,
      "loss": 0.6641,
      "step": 936700
    },
    {
      "epoch": 8.548069202131542,
      "grad_norm": 4.263337135314941,
      "learning_rate": 4.287660899822372e-05,
      "loss": 0.6833,
      "step": 936800
    },
    {
      "epoch": 8.548981677494707,
      "grad_norm": 4.081191062927246,
      "learning_rate": 4.287584860208775e-05,
      "loss": 0.709,
      "step": 936900
    },
    {
      "epoch": 8.549894152857872,
      "grad_norm": 3.360619068145752,
      "learning_rate": 4.287508820595177e-05,
      "loss": 0.7083,
      "step": 937000
    },
    {
      "epoch": 8.550806628221038,
      "grad_norm": 2.9746217727661133,
      "learning_rate": 4.287432780981581e-05,
      "loss": 0.6761,
      "step": 937100
    },
    {
      "epoch": 8.551719103584203,
      "grad_norm": 3.7696592807769775,
      "learning_rate": 4.287356741367983e-05,
      "loss": 0.6752,
      "step": 937200
    },
    {
      "epoch": 8.552631578947368,
      "grad_norm": 5.242902755737305,
      "learning_rate": 4.287280701754386e-05,
      "loss": 0.7218,
      "step": 937300
    },
    {
      "epoch": 8.553544054310533,
      "grad_norm": 4.019429683685303,
      "learning_rate": 4.287204662140789e-05,
      "loss": 0.6985,
      "step": 937400
    },
    {
      "epoch": 8.554456529673699,
      "grad_norm": 3.8114452362060547,
      "learning_rate": 4.2871286225271915e-05,
      "loss": 0.719,
      "step": 937500
    },
    {
      "epoch": 8.555369005036864,
      "grad_norm": 4.25385856628418,
      "learning_rate": 4.287052582913595e-05,
      "loss": 0.7012,
      "step": 937600
    },
    {
      "epoch": 8.55628148040003,
      "grad_norm": 4.582767963409424,
      "learning_rate": 4.2869765432999975e-05,
      "loss": 0.6721,
      "step": 937700
    },
    {
      "epoch": 8.557193955763195,
      "grad_norm": 3.8929951190948486,
      "learning_rate": 4.2869005036864005e-05,
      "loss": 0.6733,
      "step": 937800
    },
    {
      "epoch": 8.55810643112636,
      "grad_norm": 4.210599899291992,
      "learning_rate": 4.2868244640728035e-05,
      "loss": 0.705,
      "step": 937900
    },
    {
      "epoch": 8.559018906489525,
      "grad_norm": 2.6469385623931885,
      "learning_rate": 4.2867484244592065e-05,
      "loss": 0.671,
      "step": 938000
    },
    {
      "epoch": 8.55993138185269,
      "grad_norm": 3.8719482421875,
      "learning_rate": 4.2866723848456095e-05,
      "loss": 0.6749,
      "step": 938100
    },
    {
      "epoch": 8.560843857215856,
      "grad_norm": 3.97019624710083,
      "learning_rate": 4.2865963452320125e-05,
      "loss": 0.6768,
      "step": 938200
    },
    {
      "epoch": 8.561756332579021,
      "grad_norm": 4.549047946929932,
      "learning_rate": 4.286520305618415e-05,
      "loss": 0.7054,
      "step": 938300
    },
    {
      "epoch": 8.562668807942185,
      "grad_norm": 4.263016700744629,
      "learning_rate": 4.286444266004818e-05,
      "loss": 0.7079,
      "step": 938400
    },
    {
      "epoch": 8.56358128330535,
      "grad_norm": 3.3143537044525146,
      "learning_rate": 4.286368226391221e-05,
      "loss": 0.7084,
      "step": 938500
    },
    {
      "epoch": 8.564493758668515,
      "grad_norm": 4.320733070373535,
      "learning_rate": 4.286292186777624e-05,
      "loss": 0.7118,
      "step": 938600
    },
    {
      "epoch": 8.56540623403168,
      "grad_norm": 3.7243640422821045,
      "learning_rate": 4.286216147164027e-05,
      "loss": 0.6444,
      "step": 938700
    },
    {
      "epoch": 8.566318709394846,
      "grad_norm": 4.2086615562438965,
      "learning_rate": 4.28614010755043e-05,
      "loss": 0.7038,
      "step": 938800
    },
    {
      "epoch": 8.567231184758011,
      "grad_norm": 4.079308986663818,
      "learning_rate": 4.286064067936832e-05,
      "loss": 0.7135,
      "step": 938900
    },
    {
      "epoch": 8.568143660121176,
      "grad_norm": 4.054991245269775,
      "learning_rate": 4.285988028323236e-05,
      "loss": 0.6778,
      "step": 939000
    },
    {
      "epoch": 8.569056135484342,
      "grad_norm": 5.201022148132324,
      "learning_rate": 4.285911988709638e-05,
      "loss": 0.692,
      "step": 939100
    },
    {
      "epoch": 8.569968610847507,
      "grad_norm": 4.209438323974609,
      "learning_rate": 4.285835949096041e-05,
      "loss": 0.6988,
      "step": 939200
    },
    {
      "epoch": 8.570881086210672,
      "grad_norm": 2.8098833560943604,
      "learning_rate": 4.285759909482444e-05,
      "loss": 0.7057,
      "step": 939300
    },
    {
      "epoch": 8.571793561573838,
      "grad_norm": 4.715164661407471,
      "learning_rate": 4.285683869868847e-05,
      "loss": 0.7008,
      "step": 939400
    },
    {
      "epoch": 8.572706036937003,
      "grad_norm": 3.374596118927002,
      "learning_rate": 4.28560783025525e-05,
      "loss": 0.7117,
      "step": 939500
    },
    {
      "epoch": 8.573618512300168,
      "grad_norm": 4.367314338684082,
      "learning_rate": 4.285531790641653e-05,
      "loss": 0.6954,
      "step": 939600
    },
    {
      "epoch": 8.574530987663334,
      "grad_norm": 4.279378414154053,
      "learning_rate": 4.2854557510280556e-05,
      "loss": 0.6708,
      "step": 939700
    },
    {
      "epoch": 8.575443463026499,
      "grad_norm": 4.493437767028809,
      "learning_rate": 4.285379711414459e-05,
      "loss": 0.6974,
      "step": 939800
    },
    {
      "epoch": 8.576355938389664,
      "grad_norm": 3.8691189289093018,
      "learning_rate": 4.2853036718008616e-05,
      "loss": 0.693,
      "step": 939900
    },
    {
      "epoch": 8.57726841375283,
      "grad_norm": 3.7298593521118164,
      "learning_rate": 4.2852276321872646e-05,
      "loss": 0.6809,
      "step": 940000
    },
    {
      "epoch": 8.578180889115995,
      "grad_norm": 3.9047088623046875,
      "learning_rate": 4.2851515925736676e-05,
      "loss": 0.6677,
      "step": 940100
    },
    {
      "epoch": 8.579093364479158,
      "grad_norm": 4.0348615646362305,
      "learning_rate": 4.28507555296007e-05,
      "loss": 0.6567,
      "step": 940200
    },
    {
      "epoch": 8.580005839842324,
      "grad_norm": 3.639444351196289,
      "learning_rate": 4.284999513346473e-05,
      "loss": 0.707,
      "step": 940300
    },
    {
      "epoch": 8.580918315205489,
      "grad_norm": 4.170766353607178,
      "learning_rate": 4.284923473732876e-05,
      "loss": 0.6887,
      "step": 940400
    },
    {
      "epoch": 8.581830790568654,
      "grad_norm": 3.91451358795166,
      "learning_rate": 4.284847434119279e-05,
      "loss": 0.6804,
      "step": 940500
    },
    {
      "epoch": 8.58274326593182,
      "grad_norm": 3.5978353023529053,
      "learning_rate": 4.284771394505682e-05,
      "loss": 0.6871,
      "step": 940600
    },
    {
      "epoch": 8.583655741294985,
      "grad_norm": 4.3831377029418945,
      "learning_rate": 4.284695354892085e-05,
      "loss": 0.7131,
      "step": 940700
    },
    {
      "epoch": 8.58456821665815,
      "grad_norm": 4.880579471588135,
      "learning_rate": 4.284619315278487e-05,
      "loss": 0.6757,
      "step": 940800
    },
    {
      "epoch": 8.585480692021315,
      "grad_norm": 4.934750080108643,
      "learning_rate": 4.284543275664891e-05,
      "loss": 0.6366,
      "step": 940900
    },
    {
      "epoch": 8.58639316738448,
      "grad_norm": 4.129445552825928,
      "learning_rate": 4.284467236051293e-05,
      "loss": 0.7088,
      "step": 941000
    },
    {
      "epoch": 8.587305642747646,
      "grad_norm": 5.523000717163086,
      "learning_rate": 4.284391196437696e-05,
      "loss": 0.6723,
      "step": 941100
    },
    {
      "epoch": 8.588218118110811,
      "grad_norm": 3.2534589767456055,
      "learning_rate": 4.284315156824099e-05,
      "loss": 0.6736,
      "step": 941200
    },
    {
      "epoch": 8.589130593473977,
      "grad_norm": 4.072915077209473,
      "learning_rate": 4.284239117210502e-05,
      "loss": 0.6739,
      "step": 941300
    },
    {
      "epoch": 8.590043068837142,
      "grad_norm": 2.7079646587371826,
      "learning_rate": 4.284163077596905e-05,
      "loss": 0.6843,
      "step": 941400
    },
    {
      "epoch": 8.590955544200307,
      "grad_norm": 3.9166109561920166,
      "learning_rate": 4.2840870379833083e-05,
      "loss": 0.7249,
      "step": 941500
    },
    {
      "epoch": 8.591868019563472,
      "grad_norm": 4.4056549072265625,
      "learning_rate": 4.284010998369711e-05,
      "loss": 0.7104,
      "step": 941600
    },
    {
      "epoch": 8.592780494926638,
      "grad_norm": 3.9633278846740723,
      "learning_rate": 4.283934958756114e-05,
      "loss": 0.6889,
      "step": 941700
    },
    {
      "epoch": 8.593692970289801,
      "grad_norm": 4.455895900726318,
      "learning_rate": 4.283858919142517e-05,
      "loss": 0.6797,
      "step": 941800
    },
    {
      "epoch": 8.594605445652967,
      "grad_norm": 4.649496555328369,
      "learning_rate": 4.28378287952892e-05,
      "loss": 0.6884,
      "step": 941900
    },
    {
      "epoch": 8.595517921016132,
      "grad_norm": 3.24750018119812,
      "learning_rate": 4.283706839915323e-05,
      "loss": 0.6724,
      "step": 942000
    },
    {
      "epoch": 8.596430396379297,
      "grad_norm": 4.171895503997803,
      "learning_rate": 4.283630800301726e-05,
      "loss": 0.7577,
      "step": 942100
    },
    {
      "epoch": 8.597342871742462,
      "grad_norm": 4.207581043243408,
      "learning_rate": 4.283554760688128e-05,
      "loss": 0.6939,
      "step": 942200
    },
    {
      "epoch": 8.598255347105628,
      "grad_norm": 3.5732157230377197,
      "learning_rate": 4.283478721074532e-05,
      "loss": 0.6736,
      "step": 942300
    },
    {
      "epoch": 8.599167822468793,
      "grad_norm": 4.149338245391846,
      "learning_rate": 4.283402681460934e-05,
      "loss": 0.6944,
      "step": 942400
    },
    {
      "epoch": 8.600080297831958,
      "grad_norm": 3.779040813446045,
      "learning_rate": 4.283326641847337e-05,
      "loss": 0.7179,
      "step": 942500
    },
    {
      "epoch": 8.600992773195124,
      "grad_norm": 4.431803226470947,
      "learning_rate": 4.28325060223374e-05,
      "loss": 0.6804,
      "step": 942600
    },
    {
      "epoch": 8.601905248558289,
      "grad_norm": 3.5689868927001953,
      "learning_rate": 4.283174562620143e-05,
      "loss": 0.6979,
      "step": 942700
    },
    {
      "epoch": 8.602817723921454,
      "grad_norm": 3.8213696479797363,
      "learning_rate": 4.2830985230065454e-05,
      "loss": 0.6632,
      "step": 942800
    },
    {
      "epoch": 8.60373019928462,
      "grad_norm": 3.4232378005981445,
      "learning_rate": 4.283022483392949e-05,
      "loss": 0.7016,
      "step": 942900
    },
    {
      "epoch": 8.604642674647785,
      "grad_norm": 3.352644205093384,
      "learning_rate": 4.2829464437793514e-05,
      "loss": 0.6716,
      "step": 943000
    },
    {
      "epoch": 8.60555515001095,
      "grad_norm": 3.443665027618408,
      "learning_rate": 4.2828704041657544e-05,
      "loss": 0.6634,
      "step": 943100
    },
    {
      "epoch": 8.606467625374115,
      "grad_norm": 3.6235392093658447,
      "learning_rate": 4.2827943645521574e-05,
      "loss": 0.7239,
      "step": 943200
    },
    {
      "epoch": 8.60738010073728,
      "grad_norm": 4.51863956451416,
      "learning_rate": 4.28271832493856e-05,
      "loss": 0.7039,
      "step": 943300
    },
    {
      "epoch": 8.608292576100446,
      "grad_norm": 3.762866735458374,
      "learning_rate": 4.2826422853249634e-05,
      "loss": 0.6957,
      "step": 943400
    },
    {
      "epoch": 8.609205051463611,
      "grad_norm": 4.02010440826416,
      "learning_rate": 4.282566245711366e-05,
      "loss": 0.6799,
      "step": 943500
    },
    {
      "epoch": 8.610117526826775,
      "grad_norm": 4.564213275909424,
      "learning_rate": 4.282490206097769e-05,
      "loss": 0.6637,
      "step": 943600
    },
    {
      "epoch": 8.61103000218994,
      "grad_norm": 5.372439861297607,
      "learning_rate": 4.282414166484172e-05,
      "loss": 0.6818,
      "step": 943700
    },
    {
      "epoch": 8.611942477553105,
      "grad_norm": 4.964064598083496,
      "learning_rate": 4.282338126870575e-05,
      "loss": 0.7355,
      "step": 943800
    },
    {
      "epoch": 8.61285495291627,
      "grad_norm": 4.6709885597229,
      "learning_rate": 4.282262087256977e-05,
      "loss": 0.7044,
      "step": 943900
    },
    {
      "epoch": 8.613767428279436,
      "grad_norm": 3.752581834793091,
      "learning_rate": 4.282186047643381e-05,
      "loss": 0.6757,
      "step": 944000
    },
    {
      "epoch": 8.614679903642601,
      "grad_norm": 3.921313762664795,
      "learning_rate": 4.282110008029783e-05,
      "loss": 0.6607,
      "step": 944100
    },
    {
      "epoch": 8.615592379005767,
      "grad_norm": 3.400583505630493,
      "learning_rate": 4.282033968416186e-05,
      "loss": 0.7098,
      "step": 944200
    },
    {
      "epoch": 8.616504854368932,
      "grad_norm": 3.4105567932128906,
      "learning_rate": 4.281957928802589e-05,
      "loss": 0.6967,
      "step": 944300
    },
    {
      "epoch": 8.617417329732097,
      "grad_norm": 3.6789562702178955,
      "learning_rate": 4.281881889188992e-05,
      "loss": 0.6902,
      "step": 944400
    },
    {
      "epoch": 8.618329805095263,
      "grad_norm": 4.854008197784424,
      "learning_rate": 4.281805849575395e-05,
      "loss": 0.6785,
      "step": 944500
    },
    {
      "epoch": 8.619242280458428,
      "grad_norm": 4.339860916137695,
      "learning_rate": 4.281729809961798e-05,
      "loss": 0.707,
      "step": 944600
    },
    {
      "epoch": 8.620154755821593,
      "grad_norm": 4.234857559204102,
      "learning_rate": 4.2816537703482005e-05,
      "loss": 0.6924,
      "step": 944700
    },
    {
      "epoch": 8.621067231184758,
      "grad_norm": 3.3139452934265137,
      "learning_rate": 4.281577730734604e-05,
      "loss": 0.6772,
      "step": 944800
    },
    {
      "epoch": 8.621979706547924,
      "grad_norm": 4.443836688995361,
      "learning_rate": 4.2815016911210065e-05,
      "loss": 0.6559,
      "step": 944900
    },
    {
      "epoch": 8.622892181911089,
      "grad_norm": 4.097362041473389,
      "learning_rate": 4.2814256515074095e-05,
      "loss": 0.6982,
      "step": 945000
    },
    {
      "epoch": 8.623804657274254,
      "grad_norm": 4.113375186920166,
      "learning_rate": 4.2813496118938125e-05,
      "loss": 0.7105,
      "step": 945100
    },
    {
      "epoch": 8.624717132637418,
      "grad_norm": 4.085689067840576,
      "learning_rate": 4.2812735722802155e-05,
      "loss": 0.6646,
      "step": 945200
    },
    {
      "epoch": 8.625629608000583,
      "grad_norm": 4.477710247039795,
      "learning_rate": 4.281197532666618e-05,
      "loss": 0.7008,
      "step": 945300
    },
    {
      "epoch": 8.626542083363749,
      "grad_norm": 3.752272844314575,
      "learning_rate": 4.2811214930530215e-05,
      "loss": 0.6776,
      "step": 945400
    },
    {
      "epoch": 8.627454558726914,
      "grad_norm": 2.892942190170288,
      "learning_rate": 4.281045453439424e-05,
      "loss": 0.6691,
      "step": 945500
    },
    {
      "epoch": 8.628367034090079,
      "grad_norm": 3.641221523284912,
      "learning_rate": 4.280969413825827e-05,
      "loss": 0.6823,
      "step": 945600
    },
    {
      "epoch": 8.629279509453244,
      "grad_norm": 4.327759742736816,
      "learning_rate": 4.28089337421223e-05,
      "loss": 0.6909,
      "step": 945700
    },
    {
      "epoch": 8.63019198481641,
      "grad_norm": 3.9379308223724365,
      "learning_rate": 4.280817334598632e-05,
      "loss": 0.7102,
      "step": 945800
    },
    {
      "epoch": 8.631104460179575,
      "grad_norm": 3.7915306091308594,
      "learning_rate": 4.280741294985036e-05,
      "loss": 0.7136,
      "step": 945900
    },
    {
      "epoch": 8.63201693554274,
      "grad_norm": 4.176239967346191,
      "learning_rate": 4.280665255371438e-05,
      "loss": 0.675,
      "step": 946000
    },
    {
      "epoch": 8.632929410905906,
      "grad_norm": 4.507342338562012,
      "learning_rate": 4.280589215757841e-05,
      "loss": 0.7074,
      "step": 946100
    },
    {
      "epoch": 8.633841886269071,
      "grad_norm": 2.961669445037842,
      "learning_rate": 4.280513176144244e-05,
      "loss": 0.6835,
      "step": 946200
    },
    {
      "epoch": 8.634754361632236,
      "grad_norm": 3.9516522884368896,
      "learning_rate": 4.280437136530647e-05,
      "loss": 0.7025,
      "step": 946300
    },
    {
      "epoch": 8.635666836995401,
      "grad_norm": 3.005063533782959,
      "learning_rate": 4.2803610969170496e-05,
      "loss": 0.6688,
      "step": 946400
    },
    {
      "epoch": 8.636579312358567,
      "grad_norm": 3.5722198486328125,
      "learning_rate": 4.280285057303453e-05,
      "loss": 0.6595,
      "step": 946500
    },
    {
      "epoch": 8.637491787721732,
      "grad_norm": 3.6967077255249023,
      "learning_rate": 4.2802090176898556e-05,
      "loss": 0.6948,
      "step": 946600
    },
    {
      "epoch": 8.638404263084897,
      "grad_norm": 4.265610694885254,
      "learning_rate": 4.2801329780762586e-05,
      "loss": 0.7047,
      "step": 946700
    },
    {
      "epoch": 8.639316738448063,
      "grad_norm": 3.0744972229003906,
      "learning_rate": 4.2800569384626616e-05,
      "loss": 0.6772,
      "step": 946800
    },
    {
      "epoch": 8.640229213811228,
      "grad_norm": 3.619595766067505,
      "learning_rate": 4.2799808988490646e-05,
      "loss": 0.7171,
      "step": 946900
    },
    {
      "epoch": 8.641141689174392,
      "grad_norm": 4.156959533691406,
      "learning_rate": 4.2799048592354676e-05,
      "loss": 0.7018,
      "step": 947000
    },
    {
      "epoch": 8.642054164537557,
      "grad_norm": 3.885859251022339,
      "learning_rate": 4.2798288196218706e-05,
      "loss": 0.6981,
      "step": 947100
    },
    {
      "epoch": 8.642966639900722,
      "grad_norm": 4.54072904586792,
      "learning_rate": 4.279752780008273e-05,
      "loss": 0.7314,
      "step": 947200
    },
    {
      "epoch": 8.643879115263887,
      "grad_norm": 4.696082592010498,
      "learning_rate": 4.2796767403946766e-05,
      "loss": 0.698,
      "step": 947300
    },
    {
      "epoch": 8.644791590627053,
      "grad_norm": 3.039865493774414,
      "learning_rate": 4.279600700781079e-05,
      "loss": 0.7231,
      "step": 947400
    },
    {
      "epoch": 8.645704065990218,
      "grad_norm": 2.2725789546966553,
      "learning_rate": 4.279524661167482e-05,
      "loss": 0.6661,
      "step": 947500
    },
    {
      "epoch": 8.646616541353383,
      "grad_norm": 3.013836622238159,
      "learning_rate": 4.279448621553885e-05,
      "loss": 0.6917,
      "step": 947600
    },
    {
      "epoch": 8.647529016716549,
      "grad_norm": 3.8821909427642822,
      "learning_rate": 4.279372581940288e-05,
      "loss": 0.6826,
      "step": 947700
    },
    {
      "epoch": 8.648441492079714,
      "grad_norm": 4.648346424102783,
      "learning_rate": 4.27929654232669e-05,
      "loss": 0.6642,
      "step": 947800
    },
    {
      "epoch": 8.64935396744288,
      "grad_norm": 3.5333406925201416,
      "learning_rate": 4.279220502713094e-05,
      "loss": 0.6895,
      "step": 947900
    },
    {
      "epoch": 8.650266442806045,
      "grad_norm": 4.730583667755127,
      "learning_rate": 4.279144463099496e-05,
      "loss": 0.6892,
      "step": 948000
    },
    {
      "epoch": 8.65117891816921,
      "grad_norm": 5.138101577758789,
      "learning_rate": 4.279068423485899e-05,
      "loss": 0.663,
      "step": 948100
    },
    {
      "epoch": 8.652091393532375,
      "grad_norm": 4.761519908905029,
      "learning_rate": 4.2789923838723023e-05,
      "loss": 0.6757,
      "step": 948200
    },
    {
      "epoch": 8.65300386889554,
      "grad_norm": 3.8793928623199463,
      "learning_rate": 4.2789163442587053e-05,
      "loss": 0.6721,
      "step": 948300
    },
    {
      "epoch": 8.653916344258706,
      "grad_norm": 3.624521255493164,
      "learning_rate": 4.2788403046451084e-05,
      "loss": 0.6941,
      "step": 948400
    },
    {
      "epoch": 8.654828819621871,
      "grad_norm": 3.8578908443450928,
      "learning_rate": 4.2787642650315114e-05,
      "loss": 0.6935,
      "step": 948500
    },
    {
      "epoch": 8.655741294985035,
      "grad_norm": 2.941619396209717,
      "learning_rate": 4.278688225417914e-05,
      "loss": 0.6953,
      "step": 948600
    },
    {
      "epoch": 8.6566537703482,
      "grad_norm": 3.2808732986450195,
      "learning_rate": 4.278612185804317e-05,
      "loss": 0.701,
      "step": 948700
    },
    {
      "epoch": 8.657566245711365,
      "grad_norm": 3.167703628540039,
      "learning_rate": 4.27853614619072e-05,
      "loss": 0.7374,
      "step": 948800
    },
    {
      "epoch": 8.65847872107453,
      "grad_norm": 4.042928695678711,
      "learning_rate": 4.278460106577122e-05,
      "loss": 0.7114,
      "step": 948900
    },
    {
      "epoch": 8.659391196437696,
      "grad_norm": 3.8759658336639404,
      "learning_rate": 4.278384066963526e-05,
      "loss": 0.7052,
      "step": 949000
    },
    {
      "epoch": 8.660303671800861,
      "grad_norm": 4.585443019866943,
      "learning_rate": 4.278308027349928e-05,
      "loss": 0.6955,
      "step": 949100
    },
    {
      "epoch": 8.661216147164026,
      "grad_norm": 3.7933921813964844,
      "learning_rate": 4.278231987736331e-05,
      "loss": 0.6861,
      "step": 949200
    },
    {
      "epoch": 8.662128622527192,
      "grad_norm": 4.2288618087768555,
      "learning_rate": 4.278155948122734e-05,
      "loss": 0.6729,
      "step": 949300
    },
    {
      "epoch": 8.663041097890357,
      "grad_norm": 4.257462501525879,
      "learning_rate": 4.278079908509137e-05,
      "loss": 0.6596,
      "step": 949400
    },
    {
      "epoch": 8.663953573253522,
      "grad_norm": 2.739384174346924,
      "learning_rate": 4.27800386889554e-05,
      "loss": 0.6749,
      "step": 949500
    },
    {
      "epoch": 8.664866048616688,
      "grad_norm": 3.9148802757263184,
      "learning_rate": 4.277927829281943e-05,
      "loss": 0.6588,
      "step": 949600
    },
    {
      "epoch": 8.665778523979853,
      "grad_norm": 4.026828765869141,
      "learning_rate": 4.2778517896683454e-05,
      "loss": 0.7073,
      "step": 949700
    },
    {
      "epoch": 8.666690999343018,
      "grad_norm": 4.190201759338379,
      "learning_rate": 4.277775750054749e-05,
      "loss": 0.7157,
      "step": 949800
    },
    {
      "epoch": 8.667603474706183,
      "grad_norm": 2.6689484119415283,
      "learning_rate": 4.2776997104411514e-05,
      "loss": 0.7074,
      "step": 949900
    },
    {
      "epoch": 8.668515950069349,
      "grad_norm": 4.347658634185791,
      "learning_rate": 4.2776236708275544e-05,
      "loss": 0.702,
      "step": 950000
    },
    {
      "epoch": 8.669428425432514,
      "grad_norm": 1.980560302734375,
      "learning_rate": 4.2775476312139574e-05,
      "loss": 0.7038,
      "step": 950100
    },
    {
      "epoch": 8.67034090079568,
      "grad_norm": 4.223177433013916,
      "learning_rate": 4.2774715916003604e-05,
      "loss": 0.6752,
      "step": 950200
    },
    {
      "epoch": 8.671253376158845,
      "grad_norm": 4.948660373687744,
      "learning_rate": 4.2773955519867635e-05,
      "loss": 0.6797,
      "step": 950300
    },
    {
      "epoch": 8.672165851522008,
      "grad_norm": 5.884774208068848,
      "learning_rate": 4.2773195123731665e-05,
      "loss": 0.6671,
      "step": 950400
    },
    {
      "epoch": 8.673078326885173,
      "grad_norm": 5.040980815887451,
      "learning_rate": 4.277243472759569e-05,
      "loss": 0.6925,
      "step": 950500
    },
    {
      "epoch": 8.673990802248339,
      "grad_norm": 4.172145366668701,
      "learning_rate": 4.277167433145972e-05,
      "loss": 0.6966,
      "step": 950600
    },
    {
      "epoch": 8.674903277611504,
      "grad_norm": 4.762953281402588,
      "learning_rate": 4.277091393532375e-05,
      "loss": 0.7424,
      "step": 950700
    },
    {
      "epoch": 8.67581575297467,
      "grad_norm": 4.508694171905518,
      "learning_rate": 4.277015353918778e-05,
      "loss": 0.6704,
      "step": 950800
    },
    {
      "epoch": 8.676728228337835,
      "grad_norm": 4.532323837280273,
      "learning_rate": 4.276939314305181e-05,
      "loss": 0.6839,
      "step": 950900
    },
    {
      "epoch": 8.677640703701,
      "grad_norm": 4.2119622230529785,
      "learning_rate": 4.276863274691584e-05,
      "loss": 0.7176,
      "step": 951000
    },
    {
      "epoch": 8.678553179064165,
      "grad_norm": 2.7438385486602783,
      "learning_rate": 4.276787235077986e-05,
      "loss": 0.6731,
      "step": 951100
    },
    {
      "epoch": 8.67946565442733,
      "grad_norm": 4.579495906829834,
      "learning_rate": 4.27671119546439e-05,
      "loss": 0.7147,
      "step": 951200
    },
    {
      "epoch": 8.680378129790496,
      "grad_norm": 4.0099897384643555,
      "learning_rate": 4.276635155850792e-05,
      "loss": 0.7054,
      "step": 951300
    },
    {
      "epoch": 8.681290605153661,
      "grad_norm": 3.1744561195373535,
      "learning_rate": 4.276559116237195e-05,
      "loss": 0.6728,
      "step": 951400
    },
    {
      "epoch": 8.682203080516826,
      "grad_norm": 3.983186960220337,
      "learning_rate": 4.276483076623598e-05,
      "loss": 0.6564,
      "step": 951500
    },
    {
      "epoch": 8.683115555879992,
      "grad_norm": 4.157851219177246,
      "learning_rate": 4.2764070370100005e-05,
      "loss": 0.6705,
      "step": 951600
    },
    {
      "epoch": 8.684028031243157,
      "grad_norm": 3.6804990768432617,
      "learning_rate": 4.276330997396404e-05,
      "loss": 0.649,
      "step": 951700
    },
    {
      "epoch": 8.684940506606322,
      "grad_norm": 3.7482614517211914,
      "learning_rate": 4.2762549577828065e-05,
      "loss": 0.7043,
      "step": 951800
    },
    {
      "epoch": 8.685852981969488,
      "grad_norm": 3.3422369956970215,
      "learning_rate": 4.2761789181692095e-05,
      "loss": 0.678,
      "step": 951900
    },
    {
      "epoch": 8.686765457332651,
      "grad_norm": 4.761162757873535,
      "learning_rate": 4.2761028785556125e-05,
      "loss": 0.6958,
      "step": 952000
    },
    {
      "epoch": 8.687677932695816,
      "grad_norm": 4.869395732879639,
      "learning_rate": 4.2760268389420155e-05,
      "loss": 0.7119,
      "step": 952100
    },
    {
      "epoch": 8.688590408058982,
      "grad_norm": 3.6613521575927734,
      "learning_rate": 4.275950799328418e-05,
      "loss": 0.6634,
      "step": 952200
    },
    {
      "epoch": 8.689502883422147,
      "grad_norm": 3.854647636413574,
      "learning_rate": 4.2758747597148216e-05,
      "loss": 0.6786,
      "step": 952300
    },
    {
      "epoch": 8.690415358785312,
      "grad_norm": 5.423867702484131,
      "learning_rate": 4.275798720101224e-05,
      "loss": 0.7284,
      "step": 952400
    },
    {
      "epoch": 8.691327834148478,
      "grad_norm": 4.318268299102783,
      "learning_rate": 4.275722680487627e-05,
      "loss": 0.6819,
      "step": 952500
    },
    {
      "epoch": 8.692240309511643,
      "grad_norm": 4.132924556732178,
      "learning_rate": 4.27564664087403e-05,
      "loss": 0.6812,
      "step": 952600
    },
    {
      "epoch": 8.693152784874808,
      "grad_norm": 4.392977237701416,
      "learning_rate": 4.275570601260433e-05,
      "loss": 0.7046,
      "step": 952700
    },
    {
      "epoch": 8.694065260237974,
      "grad_norm": 2.962832450866699,
      "learning_rate": 4.275494561646836e-05,
      "loss": 0.6743,
      "step": 952800
    },
    {
      "epoch": 8.694977735601139,
      "grad_norm": 4.429337024688721,
      "learning_rate": 4.275418522033239e-05,
      "loss": 0.6721,
      "step": 952900
    },
    {
      "epoch": 8.695890210964304,
      "grad_norm": 3.2297210693359375,
      "learning_rate": 4.275342482419641e-05,
      "loss": 0.6717,
      "step": 953000
    },
    {
      "epoch": 8.69680268632747,
      "grad_norm": 2.763733386993408,
      "learning_rate": 4.275266442806045e-05,
      "loss": 0.6995,
      "step": 953100
    },
    {
      "epoch": 8.697715161690635,
      "grad_norm": 3.297626256942749,
      "learning_rate": 4.275190403192447e-05,
      "loss": 0.687,
      "step": 953200
    },
    {
      "epoch": 8.6986276370538,
      "grad_norm": 3.9642717838287354,
      "learning_rate": 4.27511436357885e-05,
      "loss": 0.6729,
      "step": 953300
    },
    {
      "epoch": 8.699540112416965,
      "grad_norm": 3.7408149242401123,
      "learning_rate": 4.275038323965253e-05,
      "loss": 0.6795,
      "step": 953400
    },
    {
      "epoch": 8.70045258778013,
      "grad_norm": 3.837465763092041,
      "learning_rate": 4.274962284351656e-05,
      "loss": 0.7189,
      "step": 953500
    },
    {
      "epoch": 8.701365063143296,
      "grad_norm": 4.521764278411865,
      "learning_rate": 4.2748862447380586e-05,
      "loss": 0.6981,
      "step": 953600
    },
    {
      "epoch": 8.702277538506461,
      "grad_norm": 3.45348858833313,
      "learning_rate": 4.274810205124462e-05,
      "loss": 0.705,
      "step": 953700
    },
    {
      "epoch": 8.703190013869625,
      "grad_norm": 3.824486494064331,
      "learning_rate": 4.2747341655108646e-05,
      "loss": 0.7351,
      "step": 953800
    },
    {
      "epoch": 8.70410248923279,
      "grad_norm": 3.805464029312134,
      "learning_rate": 4.2746581258972676e-05,
      "loss": 0.6866,
      "step": 953900
    },
    {
      "epoch": 8.705014964595955,
      "grad_norm": 5.104446887969971,
      "learning_rate": 4.2745820862836706e-05,
      "loss": 0.6916,
      "step": 954000
    },
    {
      "epoch": 8.70592743995912,
      "grad_norm": 4.048432350158691,
      "learning_rate": 4.2745060466700736e-05,
      "loss": 0.688,
      "step": 954100
    },
    {
      "epoch": 8.706839915322286,
      "grad_norm": 4.880617141723633,
      "learning_rate": 4.2744300070564766e-05,
      "loss": 0.6697,
      "step": 954200
    },
    {
      "epoch": 8.707752390685451,
      "grad_norm": 4.509122848510742,
      "learning_rate": 4.274353967442879e-05,
      "loss": 0.6836,
      "step": 954300
    },
    {
      "epoch": 8.708664866048617,
      "grad_norm": 3.939649820327759,
      "learning_rate": 4.274277927829282e-05,
      "loss": 0.6725,
      "step": 954400
    },
    {
      "epoch": 8.709577341411782,
      "grad_norm": 4.62452507019043,
      "learning_rate": 4.274201888215685e-05,
      "loss": 0.704,
      "step": 954500
    },
    {
      "epoch": 8.710489816774947,
      "grad_norm": 3.9115660190582275,
      "learning_rate": 4.274125848602088e-05,
      "loss": 0.7177,
      "step": 954600
    },
    {
      "epoch": 8.711402292138112,
      "grad_norm": 3.7388222217559814,
      "learning_rate": 4.27404980898849e-05,
      "loss": 0.7275,
      "step": 954700
    },
    {
      "epoch": 8.712314767501278,
      "grad_norm": 4.057292461395264,
      "learning_rate": 4.273973769374894e-05,
      "loss": 0.6792,
      "step": 954800
    },
    {
      "epoch": 8.713227242864443,
      "grad_norm": 3.566645860671997,
      "learning_rate": 4.2738977297612963e-05,
      "loss": 0.6944,
      "step": 954900
    },
    {
      "epoch": 8.714139718227608,
      "grad_norm": 3.984506607055664,
      "learning_rate": 4.2738216901476993e-05,
      "loss": 0.6384,
      "step": 955000
    },
    {
      "epoch": 8.715052193590774,
      "grad_norm": 3.8415582180023193,
      "learning_rate": 4.2737456505341024e-05,
      "loss": 0.7013,
      "step": 955100
    },
    {
      "epoch": 8.715964668953939,
      "grad_norm": 3.3115391731262207,
      "learning_rate": 4.2736696109205054e-05,
      "loss": 0.6813,
      "step": 955200
    },
    {
      "epoch": 8.716877144317104,
      "grad_norm": 3.7125182151794434,
      "learning_rate": 4.2735935713069084e-05,
      "loss": 0.6966,
      "step": 955300
    },
    {
      "epoch": 8.717789619680268,
      "grad_norm": 3.7641897201538086,
      "learning_rate": 4.2735175316933114e-05,
      "loss": 0.6847,
      "step": 955400
    },
    {
      "epoch": 8.718702095043433,
      "grad_norm": 4.101402759552002,
      "learning_rate": 4.273441492079714e-05,
      "loss": 0.6817,
      "step": 955500
    },
    {
      "epoch": 8.719614570406598,
      "grad_norm": 3.7674951553344727,
      "learning_rate": 4.2733654524661174e-05,
      "loss": 0.6608,
      "step": 955600
    },
    {
      "epoch": 8.720527045769764,
      "grad_norm": 3.654539108276367,
      "learning_rate": 4.27328941285252e-05,
      "loss": 0.7064,
      "step": 955700
    },
    {
      "epoch": 8.721439521132929,
      "grad_norm": 4.317281246185303,
      "learning_rate": 4.273213373238923e-05,
      "loss": 0.6998,
      "step": 955800
    },
    {
      "epoch": 8.722351996496094,
      "grad_norm": 2.915250778198242,
      "learning_rate": 4.273137333625326e-05,
      "loss": 0.6757,
      "step": 955900
    },
    {
      "epoch": 8.72326447185926,
      "grad_norm": 3.9804320335388184,
      "learning_rate": 4.273061294011729e-05,
      "loss": 0.6877,
      "step": 956000
    },
    {
      "epoch": 8.724176947222425,
      "grad_norm": 3.468193292617798,
      "learning_rate": 4.272985254398131e-05,
      "loss": 0.6976,
      "step": 956100
    },
    {
      "epoch": 8.72508942258559,
      "grad_norm": 3.7970714569091797,
      "learning_rate": 4.272909214784535e-05,
      "loss": 0.6539,
      "step": 956200
    },
    {
      "epoch": 8.726001897948755,
      "grad_norm": 3.5710537433624268,
      "learning_rate": 4.272833175170937e-05,
      "loss": 0.6996,
      "step": 956300
    },
    {
      "epoch": 8.72691437331192,
      "grad_norm": 3.9886796474456787,
      "learning_rate": 4.27275713555734e-05,
      "loss": 0.688,
      "step": 956400
    },
    {
      "epoch": 8.727826848675086,
      "grad_norm": 4.154107093811035,
      "learning_rate": 4.272681095943743e-05,
      "loss": 0.6774,
      "step": 956500
    },
    {
      "epoch": 8.728739324038251,
      "grad_norm": 4.095993518829346,
      "learning_rate": 4.272605056330146e-05,
      "loss": 0.7495,
      "step": 956600
    },
    {
      "epoch": 8.729651799401417,
      "grad_norm": 5.001583099365234,
      "learning_rate": 4.272529016716549e-05,
      "loss": 0.6977,
      "step": 956700
    },
    {
      "epoch": 8.730564274764582,
      "grad_norm": 4.825366973876953,
      "learning_rate": 4.272452977102952e-05,
      "loss": 0.6762,
      "step": 956800
    },
    {
      "epoch": 8.731476750127747,
      "grad_norm": 3.509634494781494,
      "learning_rate": 4.2723769374893544e-05,
      "loss": 0.6866,
      "step": 956900
    },
    {
      "epoch": 8.732389225490913,
      "grad_norm": 4.132373809814453,
      "learning_rate": 4.272300897875758e-05,
      "loss": 0.7187,
      "step": 957000
    },
    {
      "epoch": 8.733301700854078,
      "grad_norm": 3.3202242851257324,
      "learning_rate": 4.2722248582621605e-05,
      "loss": 0.7034,
      "step": 957100
    },
    {
      "epoch": 8.734214176217241,
      "grad_norm": 3.4364876747131348,
      "learning_rate": 4.272148818648563e-05,
      "loss": 0.7088,
      "step": 957200
    },
    {
      "epoch": 8.735126651580407,
      "grad_norm": 4.039809226989746,
      "learning_rate": 4.2720727790349665e-05,
      "loss": 0.698,
      "step": 957300
    },
    {
      "epoch": 8.736039126943572,
      "grad_norm": 4.539731502532959,
      "learning_rate": 4.271996739421369e-05,
      "loss": 0.7046,
      "step": 957400
    },
    {
      "epoch": 8.736951602306737,
      "grad_norm": 3.9276185035705566,
      "learning_rate": 4.271920699807772e-05,
      "loss": 0.7239,
      "step": 957500
    },
    {
      "epoch": 8.737864077669903,
      "grad_norm": 3.3778367042541504,
      "learning_rate": 4.271844660194175e-05,
      "loss": 0.654,
      "step": 957600
    },
    {
      "epoch": 8.738776553033068,
      "grad_norm": 4.037869930267334,
      "learning_rate": 4.271768620580578e-05,
      "loss": 0.7039,
      "step": 957700
    },
    {
      "epoch": 8.739689028396233,
      "grad_norm": 5.022063255310059,
      "learning_rate": 4.271692580966981e-05,
      "loss": 0.7085,
      "step": 957800
    },
    {
      "epoch": 8.740601503759398,
      "grad_norm": 3.910778522491455,
      "learning_rate": 4.271616541353384e-05,
      "loss": 0.7121,
      "step": 957900
    },
    {
      "epoch": 8.741513979122564,
      "grad_norm": 4.541445255279541,
      "learning_rate": 4.271540501739786e-05,
      "loss": 0.691,
      "step": 958000
    },
    {
      "epoch": 8.742426454485729,
      "grad_norm": 4.222489833831787,
      "learning_rate": 4.27146446212619e-05,
      "loss": 0.708,
      "step": 958100
    },
    {
      "epoch": 8.743338929848894,
      "grad_norm": 4.41886043548584,
      "learning_rate": 4.271388422512592e-05,
      "loss": 0.6877,
      "step": 958200
    },
    {
      "epoch": 8.74425140521206,
      "grad_norm": 4.544247627258301,
      "learning_rate": 4.271312382898995e-05,
      "loss": 0.6952,
      "step": 958300
    },
    {
      "epoch": 8.745163880575225,
      "grad_norm": 4.44713020324707,
      "learning_rate": 4.271236343285398e-05,
      "loss": 0.7146,
      "step": 958400
    },
    {
      "epoch": 8.74607635593839,
      "grad_norm": 4.292306423187256,
      "learning_rate": 4.271160303671801e-05,
      "loss": 0.6906,
      "step": 958500
    },
    {
      "epoch": 8.746988831301556,
      "grad_norm": 4.077670097351074,
      "learning_rate": 4.2710842640582035e-05,
      "loss": 0.7044,
      "step": 958600
    },
    {
      "epoch": 8.74790130666472,
      "grad_norm": 3.9459550380706787,
      "learning_rate": 4.271008224444607e-05,
      "loss": 0.6496,
      "step": 958700
    },
    {
      "epoch": 8.748813782027884,
      "grad_norm": 5.114006996154785,
      "learning_rate": 4.2709321848310095e-05,
      "loss": 0.6636,
      "step": 958800
    },
    {
      "epoch": 8.74972625739105,
      "grad_norm": 3.845803737640381,
      "learning_rate": 4.2708561452174125e-05,
      "loss": 0.7236,
      "step": 958900
    },
    {
      "epoch": 8.750638732754215,
      "grad_norm": 4.630256175994873,
      "learning_rate": 4.2707801056038155e-05,
      "loss": 0.6918,
      "step": 959000
    },
    {
      "epoch": 8.75155120811738,
      "grad_norm": 4.044834613800049,
      "learning_rate": 4.2707040659902186e-05,
      "loss": 0.7078,
      "step": 959100
    },
    {
      "epoch": 8.752463683480546,
      "grad_norm": 3.942305088043213,
      "learning_rate": 4.2706280263766216e-05,
      "loss": 0.7005,
      "step": 959200
    },
    {
      "epoch": 8.753376158843711,
      "grad_norm": 3.8921058177948,
      "learning_rate": 4.2705519867630246e-05,
      "loss": 0.6977,
      "step": 959300
    },
    {
      "epoch": 8.754288634206876,
      "grad_norm": 4.075397491455078,
      "learning_rate": 4.270475947149427e-05,
      "loss": 0.7147,
      "step": 959400
    },
    {
      "epoch": 8.755201109570041,
      "grad_norm": 4.687642574310303,
      "learning_rate": 4.2703999075358306e-05,
      "loss": 0.6356,
      "step": 959500
    },
    {
      "epoch": 8.756113584933207,
      "grad_norm": 3.9999165534973145,
      "learning_rate": 4.270323867922233e-05,
      "loss": 0.6726,
      "step": 959600
    },
    {
      "epoch": 8.757026060296372,
      "grad_norm": 5.095276832580566,
      "learning_rate": 4.270247828308636e-05,
      "loss": 0.7083,
      "step": 959700
    },
    {
      "epoch": 8.757938535659537,
      "grad_norm": 3.346327304840088,
      "learning_rate": 4.270171788695039e-05,
      "loss": 0.7098,
      "step": 959800
    },
    {
      "epoch": 8.758851011022703,
      "grad_norm": 3.0817532539367676,
      "learning_rate": 4.270095749081442e-05,
      "loss": 0.6687,
      "step": 959900
    },
    {
      "epoch": 8.759763486385868,
      "grad_norm": 3.3186137676239014,
      "learning_rate": 4.270019709467844e-05,
      "loss": 0.7133,
      "step": 960000
    },
    {
      "epoch": 8.760675961749033,
      "grad_norm": 4.323690414428711,
      "learning_rate": 4.269943669854247e-05,
      "loss": 0.7062,
      "step": 960100
    },
    {
      "epoch": 8.761588437112199,
      "grad_norm": 4.136162281036377,
      "learning_rate": 4.26986763024065e-05,
      "loss": 0.719,
      "step": 960200
    },
    {
      "epoch": 8.762500912475364,
      "grad_norm": 3.745687961578369,
      "learning_rate": 4.269791590627053e-05,
      "loss": 0.6741,
      "step": 960300
    },
    {
      "epoch": 8.76341338783853,
      "grad_norm": 3.7787938117980957,
      "learning_rate": 4.269715551013456e-05,
      "loss": 0.6914,
      "step": 960400
    },
    {
      "epoch": 8.764325863201694,
      "grad_norm": 3.592756748199463,
      "learning_rate": 4.2696395113998586e-05,
      "loss": 0.7169,
      "step": 960500
    },
    {
      "epoch": 8.765238338564858,
      "grad_norm": 2.9527242183685303,
      "learning_rate": 4.269563471786262e-05,
      "loss": 0.6647,
      "step": 960600
    },
    {
      "epoch": 8.766150813928023,
      "grad_norm": 3.627807855606079,
      "learning_rate": 4.2694874321726646e-05,
      "loss": 0.6745,
      "step": 960700
    },
    {
      "epoch": 8.767063289291189,
      "grad_norm": 4.824066162109375,
      "learning_rate": 4.2694113925590676e-05,
      "loss": 0.6737,
      "step": 960800
    },
    {
      "epoch": 8.767975764654354,
      "grad_norm": 3.9421231746673584,
      "learning_rate": 4.2693353529454706e-05,
      "loss": 0.6798,
      "step": 960900
    },
    {
      "epoch": 8.76888824001752,
      "grad_norm": 4.154472827911377,
      "learning_rate": 4.2692593133318737e-05,
      "loss": 0.701,
      "step": 961000
    },
    {
      "epoch": 8.769800715380685,
      "grad_norm": 3.992810010910034,
      "learning_rate": 4.269183273718276e-05,
      "loss": 0.6527,
      "step": 961100
    },
    {
      "epoch": 8.77071319074385,
      "grad_norm": 4.664453506469727,
      "learning_rate": 4.26910723410468e-05,
      "loss": 0.6971,
      "step": 961200
    },
    {
      "epoch": 8.771625666107015,
      "grad_norm": 2.5699820518493652,
      "learning_rate": 4.269031194491082e-05,
      "loss": 0.6942,
      "step": 961300
    },
    {
      "epoch": 8.77253814147018,
      "grad_norm": 4.167699337005615,
      "learning_rate": 4.268955154877485e-05,
      "loss": 0.6859,
      "step": 961400
    },
    {
      "epoch": 8.773450616833346,
      "grad_norm": 3.6339850425720215,
      "learning_rate": 4.268879115263888e-05,
      "loss": 0.6647,
      "step": 961500
    },
    {
      "epoch": 8.774363092196511,
      "grad_norm": 4.597803115844727,
      "learning_rate": 4.268803075650291e-05,
      "loss": 0.7017,
      "step": 961600
    },
    {
      "epoch": 8.775275567559676,
      "grad_norm": 4.96338415145874,
      "learning_rate": 4.268727036036694e-05,
      "loss": 0.6417,
      "step": 961700
    },
    {
      "epoch": 8.776188042922842,
      "grad_norm": 3.4920401573181152,
      "learning_rate": 4.268650996423097e-05,
      "loss": 0.6955,
      "step": 961800
    },
    {
      "epoch": 8.777100518286007,
      "grad_norm": 3.7826225757598877,
      "learning_rate": 4.2685749568094994e-05,
      "loss": 0.6715,
      "step": 961900
    },
    {
      "epoch": 8.778012993649172,
      "grad_norm": 4.014893531799316,
      "learning_rate": 4.268498917195903e-05,
      "loss": 0.6776,
      "step": 962000
    },
    {
      "epoch": 8.778925469012336,
      "grad_norm": 4.656087398529053,
      "learning_rate": 4.2684228775823054e-05,
      "loss": 0.7203,
      "step": 962100
    },
    {
      "epoch": 8.779837944375501,
      "grad_norm": 3.6185648441314697,
      "learning_rate": 4.2683468379687084e-05,
      "loss": 0.701,
      "step": 962200
    },
    {
      "epoch": 8.780750419738666,
      "grad_norm": 4.658039093017578,
      "learning_rate": 4.2682707983551114e-05,
      "loss": 0.6771,
      "step": 962300
    },
    {
      "epoch": 8.781662895101832,
      "grad_norm": 4.172976970672607,
      "learning_rate": 4.2681947587415144e-05,
      "loss": 0.6842,
      "step": 962400
    },
    {
      "epoch": 8.782575370464997,
      "grad_norm": 3.732041835784912,
      "learning_rate": 4.268118719127917e-05,
      "loss": 0.6408,
      "step": 962500
    },
    {
      "epoch": 8.783487845828162,
      "grad_norm": 3.8778975009918213,
      "learning_rate": 4.2680426795143204e-05,
      "loss": 0.6851,
      "step": 962600
    },
    {
      "epoch": 8.784400321191328,
      "grad_norm": 4.842998027801514,
      "learning_rate": 4.267966639900723e-05,
      "loss": 0.6881,
      "step": 962700
    },
    {
      "epoch": 8.785312796554493,
      "grad_norm": 4.5958251953125,
      "learning_rate": 4.267890600287126e-05,
      "loss": 0.6707,
      "step": 962800
    },
    {
      "epoch": 8.786225271917658,
      "grad_norm": 3.8122715950012207,
      "learning_rate": 4.267814560673529e-05,
      "loss": 0.7118,
      "step": 962900
    },
    {
      "epoch": 8.787137747280823,
      "grad_norm": 4.111536026000977,
      "learning_rate": 4.267738521059931e-05,
      "loss": 0.6943,
      "step": 963000
    },
    {
      "epoch": 8.788050222643989,
      "grad_norm": 3.567204475402832,
      "learning_rate": 4.267662481446335e-05,
      "loss": 0.7047,
      "step": 963100
    },
    {
      "epoch": 8.788962698007154,
      "grad_norm": 4.304591655731201,
      "learning_rate": 4.267586441832737e-05,
      "loss": 0.6926,
      "step": 963200
    },
    {
      "epoch": 8.78987517337032,
      "grad_norm": 3.614922046661377,
      "learning_rate": 4.26751040221914e-05,
      "loss": 0.6903,
      "step": 963300
    },
    {
      "epoch": 8.790787648733485,
      "grad_norm": 4.5252604484558105,
      "learning_rate": 4.267434362605543e-05,
      "loss": 0.716,
      "step": 963400
    },
    {
      "epoch": 8.79170012409665,
      "grad_norm": 3.0766637325286865,
      "learning_rate": 4.267358322991946e-05,
      "loss": 0.6629,
      "step": 963500
    },
    {
      "epoch": 8.792612599459815,
      "grad_norm": 4.344666481018066,
      "learning_rate": 4.267282283378349e-05,
      "loss": 0.6789,
      "step": 963600
    },
    {
      "epoch": 8.79352507482298,
      "grad_norm": 3.475626230239868,
      "learning_rate": 4.267206243764752e-05,
      "loss": 0.7017,
      "step": 963700
    },
    {
      "epoch": 8.794437550186146,
      "grad_norm": 3.91457462310791,
      "learning_rate": 4.2671302041511545e-05,
      "loss": 0.6989,
      "step": 963800
    },
    {
      "epoch": 8.79535002554931,
      "grad_norm": 4.608712673187256,
      "learning_rate": 4.267054164537558e-05,
      "loss": 0.6997,
      "step": 963900
    },
    {
      "epoch": 8.796262500912475,
      "grad_norm": 4.0716233253479,
      "learning_rate": 4.2669781249239605e-05,
      "loss": 0.6947,
      "step": 964000
    },
    {
      "epoch": 8.79717497627564,
      "grad_norm": 3.842085361480713,
      "learning_rate": 4.2669020853103635e-05,
      "loss": 0.7221,
      "step": 964100
    },
    {
      "epoch": 8.798087451638805,
      "grad_norm": 3.113224506378174,
      "learning_rate": 4.2668260456967665e-05,
      "loss": 0.6536,
      "step": 964200
    },
    {
      "epoch": 8.79899992700197,
      "grad_norm": 3.90539813041687,
      "learning_rate": 4.2667500060831695e-05,
      "loss": 0.6982,
      "step": 964300
    },
    {
      "epoch": 8.799912402365136,
      "grad_norm": 4.669756889343262,
      "learning_rate": 4.266673966469572e-05,
      "loss": 0.717,
      "step": 964400
    },
    {
      "epoch": 8.800824877728301,
      "grad_norm": 3.6964244842529297,
      "learning_rate": 4.2665979268559755e-05,
      "loss": 0.6695,
      "step": 964500
    },
    {
      "epoch": 8.801737353091466,
      "grad_norm": 4.326542377471924,
      "learning_rate": 4.266521887242378e-05,
      "loss": 0.7306,
      "step": 964600
    },
    {
      "epoch": 8.802649828454632,
      "grad_norm": 3.681429386138916,
      "learning_rate": 4.266445847628781e-05,
      "loss": 0.7008,
      "step": 964700
    },
    {
      "epoch": 8.803562303817797,
      "grad_norm": 4.100143909454346,
      "learning_rate": 4.266369808015184e-05,
      "loss": 0.6783,
      "step": 964800
    },
    {
      "epoch": 8.804474779180962,
      "grad_norm": 4.836963176727295,
      "learning_rate": 4.266293768401587e-05,
      "loss": 0.6867,
      "step": 964900
    },
    {
      "epoch": 8.805387254544128,
      "grad_norm": 3.574833631515503,
      "learning_rate": 4.26621772878799e-05,
      "loss": 0.6767,
      "step": 965000
    },
    {
      "epoch": 8.806299729907293,
      "grad_norm": 3.314471483230591,
      "learning_rate": 4.266141689174393e-05,
      "loss": 0.7058,
      "step": 965100
    },
    {
      "epoch": 8.807212205270458,
      "grad_norm": 3.8267693519592285,
      "learning_rate": 4.266065649560795e-05,
      "loss": 0.6744,
      "step": 965200
    },
    {
      "epoch": 8.808124680633624,
      "grad_norm": 4.0914306640625,
      "learning_rate": 4.265989609947199e-05,
      "loss": 0.7022,
      "step": 965300
    },
    {
      "epoch": 8.809037155996789,
      "grad_norm": 4.448312282562256,
      "learning_rate": 4.265913570333601e-05,
      "loss": 0.6846,
      "step": 965400
    },
    {
      "epoch": 8.809949631359952,
      "grad_norm": 2.603837490081787,
      "learning_rate": 4.265837530720004e-05,
      "loss": 0.682,
      "step": 965500
    },
    {
      "epoch": 8.810862106723118,
      "grad_norm": 5.772803783416748,
      "learning_rate": 4.265761491106407e-05,
      "loss": 0.6989,
      "step": 965600
    },
    {
      "epoch": 8.811774582086283,
      "grad_norm": 3.6974542140960693,
      "learning_rate": 4.2656854514928095e-05,
      "loss": 0.6887,
      "step": 965700
    },
    {
      "epoch": 8.812687057449448,
      "grad_norm": 3.339946746826172,
      "learning_rate": 4.2656094118792126e-05,
      "loss": 0.6667,
      "step": 965800
    },
    {
      "epoch": 8.813599532812614,
      "grad_norm": 3.5575671195983887,
      "learning_rate": 4.2655333722656156e-05,
      "loss": 0.6919,
      "step": 965900
    },
    {
      "epoch": 8.814512008175779,
      "grad_norm": 3.403733015060425,
      "learning_rate": 4.2654573326520186e-05,
      "loss": 0.661,
      "step": 966000
    },
    {
      "epoch": 8.815424483538944,
      "grad_norm": 3.4373631477355957,
      "learning_rate": 4.2653812930384216e-05,
      "loss": 0.7332,
      "step": 966100
    },
    {
      "epoch": 8.81633695890211,
      "grad_norm": 3.8577263355255127,
      "learning_rate": 4.2653052534248246e-05,
      "loss": 0.6787,
      "step": 966200
    },
    {
      "epoch": 8.817249434265275,
      "grad_norm": 4.075719356536865,
      "learning_rate": 4.265229213811227e-05,
      "loss": 0.6748,
      "step": 966300
    },
    {
      "epoch": 8.81816190962844,
      "grad_norm": 4.192840099334717,
      "learning_rate": 4.2651531741976306e-05,
      "loss": 0.7084,
      "step": 966400
    },
    {
      "epoch": 8.819074384991605,
      "grad_norm": 3.5924532413482666,
      "learning_rate": 4.265077134584033e-05,
      "loss": 0.6797,
      "step": 966500
    },
    {
      "epoch": 8.81998686035477,
      "grad_norm": 4.660105228424072,
      "learning_rate": 4.265001094970436e-05,
      "loss": 0.6794,
      "step": 966600
    },
    {
      "epoch": 8.820899335717936,
      "grad_norm": 2.9914777278900146,
      "learning_rate": 4.264925055356839e-05,
      "loss": 0.6729,
      "step": 966700
    },
    {
      "epoch": 8.821811811081101,
      "grad_norm": 3.015983819961548,
      "learning_rate": 4.264849015743242e-05,
      "loss": 0.7003,
      "step": 966800
    },
    {
      "epoch": 8.822724286444267,
      "grad_norm": 3.585306167602539,
      "learning_rate": 4.264772976129644e-05,
      "loss": 0.7243,
      "step": 966900
    },
    {
      "epoch": 8.823636761807432,
      "grad_norm": 4.3318772315979,
      "learning_rate": 4.264696936516048e-05,
      "loss": 0.709,
      "step": 967000
    },
    {
      "epoch": 8.824549237170597,
      "grad_norm": 3.6665332317352295,
      "learning_rate": 4.26462089690245e-05,
      "loss": 0.7393,
      "step": 967100
    },
    {
      "epoch": 8.825461712533762,
      "grad_norm": 6.201006889343262,
      "learning_rate": 4.264544857288853e-05,
      "loss": 0.6575,
      "step": 967200
    },
    {
      "epoch": 8.826374187896926,
      "grad_norm": 3.8110511302948,
      "learning_rate": 4.264468817675256e-05,
      "loss": 0.6798,
      "step": 967300
    },
    {
      "epoch": 8.827286663260091,
      "grad_norm": 4.3300557136535645,
      "learning_rate": 4.264392778061659e-05,
      "loss": 0.701,
      "step": 967400
    },
    {
      "epoch": 8.828199138623257,
      "grad_norm": 3.091665506362915,
      "learning_rate": 4.264316738448062e-05,
      "loss": 0.6797,
      "step": 967500
    },
    {
      "epoch": 8.829111613986422,
      "grad_norm": 4.791927814483643,
      "learning_rate": 4.264240698834465e-05,
      "loss": 0.6531,
      "step": 967600
    },
    {
      "epoch": 8.830024089349587,
      "grad_norm": 4.264462471008301,
      "learning_rate": 4.2641646592208676e-05,
      "loss": 0.6555,
      "step": 967700
    },
    {
      "epoch": 8.830936564712752,
      "grad_norm": 4.081413745880127,
      "learning_rate": 4.264088619607271e-05,
      "loss": 0.6693,
      "step": 967800
    },
    {
      "epoch": 8.831849040075918,
      "grad_norm": 3.4459822177886963,
      "learning_rate": 4.2640125799936737e-05,
      "loss": 0.6777,
      "step": 967900
    },
    {
      "epoch": 8.832761515439083,
      "grad_norm": 3.683292865753174,
      "learning_rate": 4.263936540380077e-05,
      "loss": 0.6975,
      "step": 968000
    },
    {
      "epoch": 8.833673990802248,
      "grad_norm": 3.649143695831299,
      "learning_rate": 4.26386050076648e-05,
      "loss": 0.6794,
      "step": 968100
    },
    {
      "epoch": 8.834586466165414,
      "grad_norm": 3.312363624572754,
      "learning_rate": 4.263784461152883e-05,
      "loss": 0.6821,
      "step": 968200
    },
    {
      "epoch": 8.835498941528579,
      "grad_norm": 4.887383460998535,
      "learning_rate": 4.263708421539285e-05,
      "loss": 0.6634,
      "step": 968300
    },
    {
      "epoch": 8.836411416891744,
      "grad_norm": 4.086462020874023,
      "learning_rate": 4.263632381925689e-05,
      "loss": 0.6691,
      "step": 968400
    },
    {
      "epoch": 8.83732389225491,
      "grad_norm": 3.653653860092163,
      "learning_rate": 4.263556342312091e-05,
      "loss": 0.7063,
      "step": 968500
    },
    {
      "epoch": 8.838236367618075,
      "grad_norm": 5.171545028686523,
      "learning_rate": 4.263480302698494e-05,
      "loss": 0.6781,
      "step": 968600
    },
    {
      "epoch": 8.83914884298124,
      "grad_norm": 3.8796565532684326,
      "learning_rate": 4.263404263084897e-05,
      "loss": 0.6633,
      "step": 968700
    },
    {
      "epoch": 8.840061318344405,
      "grad_norm": 4.339902877807617,
      "learning_rate": 4.2633282234712994e-05,
      "loss": 0.6587,
      "step": 968800
    },
    {
      "epoch": 8.840973793707569,
      "grad_norm": 3.77559494972229,
      "learning_rate": 4.263252183857703e-05,
      "loss": 0.6825,
      "step": 968900
    },
    {
      "epoch": 8.841886269070734,
      "grad_norm": 3.9587137699127197,
      "learning_rate": 4.2631761442441054e-05,
      "loss": 0.7283,
      "step": 969000
    },
    {
      "epoch": 8.8427987444339,
      "grad_norm": 3.916862964630127,
      "learning_rate": 4.2631001046305084e-05,
      "loss": 0.684,
      "step": 969100
    },
    {
      "epoch": 8.843711219797065,
      "grad_norm": 4.690819263458252,
      "learning_rate": 4.2630240650169114e-05,
      "loss": 0.7159,
      "step": 969200
    },
    {
      "epoch": 8.84462369516023,
      "grad_norm": 4.0259904861450195,
      "learning_rate": 4.2629480254033144e-05,
      "loss": 0.7072,
      "step": 969300
    },
    {
      "epoch": 8.845536170523395,
      "grad_norm": 4.327505588531494,
      "learning_rate": 4.262871985789717e-05,
      "loss": 0.6554,
      "step": 969400
    },
    {
      "epoch": 8.84644864588656,
      "grad_norm": 2.819697380065918,
      "learning_rate": 4.2627959461761204e-05,
      "loss": 0.6812,
      "step": 969500
    },
    {
      "epoch": 8.847361121249726,
      "grad_norm": 4.191427230834961,
      "learning_rate": 4.262719906562523e-05,
      "loss": 0.7111,
      "step": 969600
    },
    {
      "epoch": 8.848273596612891,
      "grad_norm": 3.7609612941741943,
      "learning_rate": 4.262643866948926e-05,
      "loss": 0.6785,
      "step": 969700
    },
    {
      "epoch": 8.849186071976057,
      "grad_norm": 3.8935389518737793,
      "learning_rate": 4.262567827335329e-05,
      "loss": 0.6807,
      "step": 969800
    },
    {
      "epoch": 8.850098547339222,
      "grad_norm": 3.9283199310302734,
      "learning_rate": 4.262491787721732e-05,
      "loss": 0.6592,
      "step": 969900
    },
    {
      "epoch": 8.851011022702387,
      "grad_norm": 4.766818523406982,
      "learning_rate": 4.262415748108135e-05,
      "loss": 0.6523,
      "step": 970000
    },
    {
      "epoch": 8.851923498065553,
      "grad_norm": 4.118963718414307,
      "learning_rate": 4.262339708494538e-05,
      "loss": 0.6734,
      "step": 970100
    },
    {
      "epoch": 8.852835973428718,
      "grad_norm": 3.959655284881592,
      "learning_rate": 4.26226366888094e-05,
      "loss": 0.7026,
      "step": 970200
    },
    {
      "epoch": 8.853748448791883,
      "grad_norm": 3.698868989944458,
      "learning_rate": 4.262187629267344e-05,
      "loss": 0.6897,
      "step": 970300
    },
    {
      "epoch": 8.854660924155048,
      "grad_norm": 5.0061354637146,
      "learning_rate": 4.262111589653746e-05,
      "loss": 0.6968,
      "step": 970400
    },
    {
      "epoch": 8.855573399518214,
      "grad_norm": 3.5642948150634766,
      "learning_rate": 4.262035550040149e-05,
      "loss": 0.673,
      "step": 970500
    },
    {
      "epoch": 8.856485874881379,
      "grad_norm": 4.247872352600098,
      "learning_rate": 4.261959510426552e-05,
      "loss": 0.7118,
      "step": 970600
    },
    {
      "epoch": 8.857398350244543,
      "grad_norm": 4.247005462646484,
      "learning_rate": 4.261883470812955e-05,
      "loss": 0.6569,
      "step": 970700
    },
    {
      "epoch": 8.858310825607708,
      "grad_norm": 3.723088502883911,
      "learning_rate": 4.2618074311993575e-05,
      "loss": 0.6859,
      "step": 970800
    },
    {
      "epoch": 8.859223300970873,
      "grad_norm": 3.3036091327667236,
      "learning_rate": 4.261731391585761e-05,
      "loss": 0.7018,
      "step": 970900
    },
    {
      "epoch": 8.860135776334038,
      "grad_norm": 3.803433656692505,
      "learning_rate": 4.2616553519721635e-05,
      "loss": 0.695,
      "step": 971000
    },
    {
      "epoch": 8.861048251697204,
      "grad_norm": 4.069474220275879,
      "learning_rate": 4.2615793123585665e-05,
      "loss": 0.6779,
      "step": 971100
    },
    {
      "epoch": 8.861960727060369,
      "grad_norm": 4.46933650970459,
      "learning_rate": 4.2615032727449695e-05,
      "loss": 0.6679,
      "step": 971200
    },
    {
      "epoch": 8.862873202423534,
      "grad_norm": 3.8928825855255127,
      "learning_rate": 4.2614272331313725e-05,
      "loss": 0.6859,
      "step": 971300
    },
    {
      "epoch": 8.8637856777867,
      "grad_norm": 3.8821773529052734,
      "learning_rate": 4.2613511935177755e-05,
      "loss": 0.7055,
      "step": 971400
    },
    {
      "epoch": 8.864698153149865,
      "grad_norm": 4.296620845794678,
      "learning_rate": 4.261275153904178e-05,
      "loss": 0.7005,
      "step": 971500
    },
    {
      "epoch": 8.86561062851303,
      "grad_norm": 3.9460201263427734,
      "learning_rate": 4.261199114290581e-05,
      "loss": 0.6918,
      "step": 971600
    },
    {
      "epoch": 8.866523103876196,
      "grad_norm": 4.794704914093018,
      "learning_rate": 4.261123074676984e-05,
      "loss": 0.7275,
      "step": 971700
    },
    {
      "epoch": 8.86743557923936,
      "grad_norm": 3.435999870300293,
      "learning_rate": 4.261047035063387e-05,
      "loss": 0.7297,
      "step": 971800
    },
    {
      "epoch": 8.868348054602526,
      "grad_norm": 3.7739779949188232,
      "learning_rate": 4.260970995449789e-05,
      "loss": 0.6776,
      "step": 971900
    },
    {
      "epoch": 8.869260529965691,
      "grad_norm": 4.737090587615967,
      "learning_rate": 4.260894955836193e-05,
      "loss": 0.7312,
      "step": 972000
    },
    {
      "epoch": 8.870173005328857,
      "grad_norm": 3.9797022342681885,
      "learning_rate": 4.260818916222595e-05,
      "loss": 0.697,
      "step": 972100
    },
    {
      "epoch": 8.871085480692022,
      "grad_norm": 4.36033821105957,
      "learning_rate": 4.260742876608998e-05,
      "loss": 0.728,
      "step": 972200
    },
    {
      "epoch": 8.871997956055186,
      "grad_norm": 4.606149673461914,
      "learning_rate": 4.260666836995401e-05,
      "loss": 0.6962,
      "step": 972300
    },
    {
      "epoch": 8.872910431418351,
      "grad_norm": 4.357407569885254,
      "learning_rate": 4.260590797381804e-05,
      "loss": 0.7092,
      "step": 972400
    },
    {
      "epoch": 8.873822906781516,
      "grad_norm": 3.7970051765441895,
      "learning_rate": 4.260514757768207e-05,
      "loss": 0.6806,
      "step": 972500
    },
    {
      "epoch": 8.874735382144681,
      "grad_norm": 4.218258380889893,
      "learning_rate": 4.26043871815461e-05,
      "loss": 0.6971,
      "step": 972600
    },
    {
      "epoch": 8.875647857507847,
      "grad_norm": 3.048454999923706,
      "learning_rate": 4.2603626785410126e-05,
      "loss": 0.6905,
      "step": 972700
    },
    {
      "epoch": 8.876560332871012,
      "grad_norm": 3.9828670024871826,
      "learning_rate": 4.260286638927416e-05,
      "loss": 0.6997,
      "step": 972800
    },
    {
      "epoch": 8.877472808234177,
      "grad_norm": 4.71586275100708,
      "learning_rate": 4.2602105993138186e-05,
      "loss": 0.7028,
      "step": 972900
    },
    {
      "epoch": 8.878385283597343,
      "grad_norm": 4.024412631988525,
      "learning_rate": 4.2601345597002216e-05,
      "loss": 0.6773,
      "step": 973000
    },
    {
      "epoch": 8.879297758960508,
      "grad_norm": 4.549737930297852,
      "learning_rate": 4.2600585200866246e-05,
      "loss": 0.7335,
      "step": 973100
    },
    {
      "epoch": 8.880210234323673,
      "grad_norm": 3.5103609561920166,
      "learning_rate": 4.2599824804730276e-05,
      "loss": 0.6873,
      "step": 973200
    },
    {
      "epoch": 8.881122709686839,
      "grad_norm": 3.6854872703552246,
      "learning_rate": 4.25990644085943e-05,
      "loss": 0.6961,
      "step": 973300
    },
    {
      "epoch": 8.882035185050004,
      "grad_norm": 3.814143657684326,
      "learning_rate": 4.2598304012458336e-05,
      "loss": 0.7137,
      "step": 973400
    },
    {
      "epoch": 8.88294766041317,
      "grad_norm": 4.454617977142334,
      "learning_rate": 4.259754361632236e-05,
      "loss": 0.7006,
      "step": 973500
    },
    {
      "epoch": 8.883860135776334,
      "grad_norm": 3.2619056701660156,
      "learning_rate": 4.259678322018639e-05,
      "loss": 0.7079,
      "step": 973600
    },
    {
      "epoch": 8.8847726111395,
      "grad_norm": 4.446093559265137,
      "learning_rate": 4.259602282405042e-05,
      "loss": 0.6987,
      "step": 973700
    },
    {
      "epoch": 8.885685086502665,
      "grad_norm": 4.026805400848389,
      "learning_rate": 4.259526242791445e-05,
      "loss": 0.6427,
      "step": 973800
    },
    {
      "epoch": 8.88659756186583,
      "grad_norm": 4.254736423492432,
      "learning_rate": 4.259450203177848e-05,
      "loss": 0.7177,
      "step": 973900
    },
    {
      "epoch": 8.887510037228996,
      "grad_norm": 4.985766887664795,
      "learning_rate": 4.259374163564251e-05,
      "loss": 0.7144,
      "step": 974000
    },
    {
      "epoch": 8.88842251259216,
      "grad_norm": 4.3003339767456055,
      "learning_rate": 4.259298123950653e-05,
      "loss": 0.7008,
      "step": 974100
    },
    {
      "epoch": 8.889334987955325,
      "grad_norm": 4.049459934234619,
      "learning_rate": 4.259222084337056e-05,
      "loss": 0.6636,
      "step": 974200
    },
    {
      "epoch": 8.89024746331849,
      "grad_norm": 5.611355781555176,
      "learning_rate": 4.259146044723459e-05,
      "loss": 0.6768,
      "step": 974300
    },
    {
      "epoch": 8.891159938681655,
      "grad_norm": 5.105888843536377,
      "learning_rate": 4.2590700051098616e-05,
      "loss": 0.6824,
      "step": 974400
    },
    {
      "epoch": 8.89207241404482,
      "grad_norm": 4.28756856918335,
      "learning_rate": 4.258993965496265e-05,
      "loss": 0.7135,
      "step": 974500
    },
    {
      "epoch": 8.892984889407986,
      "grad_norm": 4.5187296867370605,
      "learning_rate": 4.2589179258826677e-05,
      "loss": 0.6991,
      "step": 974600
    },
    {
      "epoch": 8.893897364771151,
      "grad_norm": 4.045691967010498,
      "learning_rate": 4.258841886269071e-05,
      "loss": 0.6764,
      "step": 974700
    },
    {
      "epoch": 8.894809840134316,
      "grad_norm": 4.906615734100342,
      "learning_rate": 4.258765846655474e-05,
      "loss": 0.696,
      "step": 974800
    },
    {
      "epoch": 8.895722315497482,
      "grad_norm": 3.753084897994995,
      "learning_rate": 4.258689807041877e-05,
      "loss": 0.6966,
      "step": 974900
    },
    {
      "epoch": 8.896634790860647,
      "grad_norm": 3.6894943714141846,
      "learning_rate": 4.25861376742828e-05,
      "loss": 0.6953,
      "step": 975000
    },
    {
      "epoch": 8.897547266223812,
      "grad_norm": 3.8191959857940674,
      "learning_rate": 4.258537727814683e-05,
      "loss": 0.689,
      "step": 975100
    },
    {
      "epoch": 8.898459741586978,
      "grad_norm": 3.564814805984497,
      "learning_rate": 4.258461688201085e-05,
      "loss": 0.6958,
      "step": 975200
    },
    {
      "epoch": 8.899372216950143,
      "grad_norm": 3.555805206298828,
      "learning_rate": 4.258385648587489e-05,
      "loss": 0.6664,
      "step": 975300
    },
    {
      "epoch": 8.900284692313308,
      "grad_norm": 4.072283744812012,
      "learning_rate": 4.258309608973891e-05,
      "loss": 0.6906,
      "step": 975400
    },
    {
      "epoch": 8.901197167676473,
      "grad_norm": 3.8328068256378174,
      "learning_rate": 4.258233569360294e-05,
      "loss": 0.7026,
      "step": 975500
    },
    {
      "epoch": 8.902109643039639,
      "grad_norm": 4.885941028594971,
      "learning_rate": 4.258157529746697e-05,
      "loss": 0.6872,
      "step": 975600
    },
    {
      "epoch": 8.903022118402802,
      "grad_norm": 3.3379263877868652,
      "learning_rate": 4.2580814901331e-05,
      "loss": 0.7082,
      "step": 975700
    },
    {
      "epoch": 8.903934593765968,
      "grad_norm": 4.393401145935059,
      "learning_rate": 4.258005450519503e-05,
      "loss": 0.6994,
      "step": 975800
    },
    {
      "epoch": 8.904847069129133,
      "grad_norm": 3.479818105697632,
      "learning_rate": 4.257929410905906e-05,
      "loss": 0.7322,
      "step": 975900
    },
    {
      "epoch": 8.905759544492298,
      "grad_norm": 5.140160083770752,
      "learning_rate": 4.2578533712923084e-05,
      "loss": 0.6886,
      "step": 976000
    },
    {
      "epoch": 8.906672019855463,
      "grad_norm": 4.014913082122803,
      "learning_rate": 4.2577773316787114e-05,
      "loss": 0.6801,
      "step": 976100
    },
    {
      "epoch": 8.907584495218629,
      "grad_norm": 4.239692687988281,
      "learning_rate": 4.2577012920651144e-05,
      "loss": 0.6843,
      "step": 976200
    },
    {
      "epoch": 8.908496970581794,
      "grad_norm": 4.503061294555664,
      "learning_rate": 4.2576252524515174e-05,
      "loss": 0.667,
      "step": 976300
    },
    {
      "epoch": 8.90940944594496,
      "grad_norm": 4.31625509262085,
      "learning_rate": 4.2575492128379204e-05,
      "loss": 0.6942,
      "step": 976400
    },
    {
      "epoch": 8.910321921308125,
      "grad_norm": 3.8174664974212646,
      "learning_rate": 4.2574731732243234e-05,
      "loss": 0.6931,
      "step": 976500
    },
    {
      "epoch": 8.91123439667129,
      "grad_norm": 3.0811965465545654,
      "learning_rate": 4.257397133610726e-05,
      "loss": 0.682,
      "step": 976600
    },
    {
      "epoch": 8.912146872034455,
      "grad_norm": 3.901123285293579,
      "learning_rate": 4.2573210939971294e-05,
      "loss": 0.7309,
      "step": 976700
    },
    {
      "epoch": 8.91305934739762,
      "grad_norm": 3.3033480644226074,
      "learning_rate": 4.257245054383532e-05,
      "loss": 0.6565,
      "step": 976800
    },
    {
      "epoch": 8.913971822760786,
      "grad_norm": 4.025848865509033,
      "learning_rate": 4.257169014769935e-05,
      "loss": 0.6799,
      "step": 976900
    },
    {
      "epoch": 8.914884298123951,
      "grad_norm": 4.060513496398926,
      "learning_rate": 4.257092975156338e-05,
      "loss": 0.6992,
      "step": 977000
    },
    {
      "epoch": 8.915796773487116,
      "grad_norm": 4.488919734954834,
      "learning_rate": 4.25701693554274e-05,
      "loss": 0.724,
      "step": 977100
    },
    {
      "epoch": 8.916709248850282,
      "grad_norm": 3.847275972366333,
      "learning_rate": 4.256940895929144e-05,
      "loss": 0.7274,
      "step": 977200
    },
    {
      "epoch": 8.917621724213447,
      "grad_norm": 3.472229242324829,
      "learning_rate": 4.256864856315546e-05,
      "loss": 0.6786,
      "step": 977300
    },
    {
      "epoch": 8.918534199576612,
      "grad_norm": 4.949853420257568,
      "learning_rate": 4.256788816701949e-05,
      "loss": 0.7336,
      "step": 977400
    },
    {
      "epoch": 8.919446674939776,
      "grad_norm": 5.516632080078125,
      "learning_rate": 4.256712777088352e-05,
      "loss": 0.6893,
      "step": 977500
    },
    {
      "epoch": 8.920359150302941,
      "grad_norm": 4.097377300262451,
      "learning_rate": 4.256636737474755e-05,
      "loss": 0.7164,
      "step": 977600
    },
    {
      "epoch": 8.921271625666106,
      "grad_norm": 4.62138557434082,
      "learning_rate": 4.2565606978611575e-05,
      "loss": 0.6645,
      "step": 977700
    },
    {
      "epoch": 8.922184101029272,
      "grad_norm": 3.952422618865967,
      "learning_rate": 4.256484658247561e-05,
      "loss": 0.6754,
      "step": 977800
    },
    {
      "epoch": 8.923096576392437,
      "grad_norm": 4.331683158874512,
      "learning_rate": 4.2564086186339635e-05,
      "loss": 0.7406,
      "step": 977900
    },
    {
      "epoch": 8.924009051755602,
      "grad_norm": 3.618962287902832,
      "learning_rate": 4.2563325790203665e-05,
      "loss": 0.6982,
      "step": 978000
    },
    {
      "epoch": 8.924921527118768,
      "grad_norm": 3.6842124462127686,
      "learning_rate": 4.2562565394067695e-05,
      "loss": 0.6877,
      "step": 978100
    },
    {
      "epoch": 8.925834002481933,
      "grad_norm": 5.848505020141602,
      "learning_rate": 4.2561804997931725e-05,
      "loss": 0.6947,
      "step": 978200
    },
    {
      "epoch": 8.926746477845098,
      "grad_norm": 4.233847618103027,
      "learning_rate": 4.2561044601795755e-05,
      "loss": 0.6552,
      "step": 978300
    },
    {
      "epoch": 8.927658953208264,
      "grad_norm": 3.6912741661071777,
      "learning_rate": 4.2560284205659785e-05,
      "loss": 0.686,
      "step": 978400
    },
    {
      "epoch": 8.928571428571429,
      "grad_norm": 3.864701271057129,
      "learning_rate": 4.255952380952381e-05,
      "loss": 0.6591,
      "step": 978500
    },
    {
      "epoch": 8.929483903934594,
      "grad_norm": 2.9554660320281982,
      "learning_rate": 4.2558763413387845e-05,
      "loss": 0.6692,
      "step": 978600
    },
    {
      "epoch": 8.93039637929776,
      "grad_norm": 3.4007043838500977,
      "learning_rate": 4.255800301725187e-05,
      "loss": 0.6767,
      "step": 978700
    },
    {
      "epoch": 8.931308854660925,
      "grad_norm": 4.415915012359619,
      "learning_rate": 4.25572426211159e-05,
      "loss": 0.6861,
      "step": 978800
    },
    {
      "epoch": 8.93222133002409,
      "grad_norm": 4.235389709472656,
      "learning_rate": 4.255648222497993e-05,
      "loss": 0.7167,
      "step": 978900
    },
    {
      "epoch": 8.933133805387255,
      "grad_norm": 3.5334911346435547,
      "learning_rate": 4.255572182884396e-05,
      "loss": 0.6995,
      "step": 979000
    },
    {
      "epoch": 8.934046280750419,
      "grad_norm": 4.265587329864502,
      "learning_rate": 4.255496143270798e-05,
      "loss": 0.6723,
      "step": 979100
    },
    {
      "epoch": 8.934958756113584,
      "grad_norm": 4.17864990234375,
      "learning_rate": 4.255420103657202e-05,
      "loss": 0.7324,
      "step": 979200
    },
    {
      "epoch": 8.93587123147675,
      "grad_norm": 3.16629695892334,
      "learning_rate": 4.255344064043604e-05,
      "loss": 0.696,
      "step": 979300
    },
    {
      "epoch": 8.936783706839915,
      "grad_norm": 4.435820579528809,
      "learning_rate": 4.255268024430007e-05,
      "loss": 0.6671,
      "step": 979400
    },
    {
      "epoch": 8.93769618220308,
      "grad_norm": 5.17104434967041,
      "learning_rate": 4.25519198481641e-05,
      "loss": 0.695,
      "step": 979500
    },
    {
      "epoch": 8.938608657566245,
      "grad_norm": 4.29407262802124,
      "learning_rate": 4.255115945202813e-05,
      "loss": 0.6805,
      "step": 979600
    },
    {
      "epoch": 8.93952113292941,
      "grad_norm": 3.3571269512176514,
      "learning_rate": 4.255039905589216e-05,
      "loss": 0.665,
      "step": 979700
    },
    {
      "epoch": 8.940433608292576,
      "grad_norm": 4.182502746582031,
      "learning_rate": 4.254963865975619e-05,
      "loss": 0.6835,
      "step": 979800
    },
    {
      "epoch": 8.941346083655741,
      "grad_norm": 3.2455050945281982,
      "learning_rate": 4.2548878263620216e-05,
      "loss": 0.7288,
      "step": 979900
    },
    {
      "epoch": 8.942258559018907,
      "grad_norm": 3.602226972579956,
      "learning_rate": 4.2548117867484246e-05,
      "loss": 0.6937,
      "step": 980000
    },
    {
      "epoch": 8.943171034382072,
      "grad_norm": 2.4655003547668457,
      "learning_rate": 4.2547357471348276e-05,
      "loss": 0.6854,
      "step": 980100
    },
    {
      "epoch": 8.944083509745237,
      "grad_norm": 5.516621112823486,
      "learning_rate": 4.25465970752123e-05,
      "loss": 0.6804,
      "step": 980200
    },
    {
      "epoch": 8.944995985108402,
      "grad_norm": 3.9137539863586426,
      "learning_rate": 4.2545836679076336e-05,
      "loss": 0.6711,
      "step": 980300
    },
    {
      "epoch": 8.945908460471568,
      "grad_norm": 3.7866687774658203,
      "learning_rate": 4.254507628294036e-05,
      "loss": 0.7042,
      "step": 980400
    },
    {
      "epoch": 8.946820935834733,
      "grad_norm": 4.188610076904297,
      "learning_rate": 4.254431588680439e-05,
      "loss": 0.6845,
      "step": 980500
    },
    {
      "epoch": 8.947733411197898,
      "grad_norm": 4.185877323150635,
      "learning_rate": 4.254355549066842e-05,
      "loss": 0.6984,
      "step": 980600
    },
    {
      "epoch": 8.948645886561064,
      "grad_norm": 3.699636220932007,
      "learning_rate": 4.254279509453245e-05,
      "loss": 0.6722,
      "step": 980700
    },
    {
      "epoch": 8.949558361924229,
      "grad_norm": 3.8984737396240234,
      "learning_rate": 4.254203469839648e-05,
      "loss": 0.6837,
      "step": 980800
    },
    {
      "epoch": 8.950470837287392,
      "grad_norm": 4.510892868041992,
      "learning_rate": 4.254127430226051e-05,
      "loss": 0.7059,
      "step": 980900
    },
    {
      "epoch": 8.951383312650558,
      "grad_norm": 3.184582471847534,
      "learning_rate": 4.254051390612453e-05,
      "loss": 0.6824,
      "step": 981000
    },
    {
      "epoch": 8.952295788013723,
      "grad_norm": 3.5234577655792236,
      "learning_rate": 4.253975350998857e-05,
      "loss": 0.7056,
      "step": 981100
    },
    {
      "epoch": 8.953208263376888,
      "grad_norm": 3.968106746673584,
      "learning_rate": 4.253899311385259e-05,
      "loss": 0.715,
      "step": 981200
    },
    {
      "epoch": 8.954120738740054,
      "grad_norm": 4.266195774078369,
      "learning_rate": 4.253823271771662e-05,
      "loss": 0.714,
      "step": 981300
    },
    {
      "epoch": 8.955033214103219,
      "grad_norm": 4.088198184967041,
      "learning_rate": 4.253747232158065e-05,
      "loss": 0.7357,
      "step": 981400
    },
    {
      "epoch": 8.955945689466384,
      "grad_norm": 3.28603458404541,
      "learning_rate": 4.2536711925444683e-05,
      "loss": 0.677,
      "step": 981500
    },
    {
      "epoch": 8.95685816482955,
      "grad_norm": 3.368438482284546,
      "learning_rate": 4.253595152930871e-05,
      "loss": 0.6905,
      "step": 981600
    },
    {
      "epoch": 8.957770640192715,
      "grad_norm": 4.090303421020508,
      "learning_rate": 4.2535191133172744e-05,
      "loss": 0.676,
      "step": 981700
    },
    {
      "epoch": 8.95868311555588,
      "grad_norm": 4.308651924133301,
      "learning_rate": 4.253443073703677e-05,
      "loss": 0.6757,
      "step": 981800
    },
    {
      "epoch": 8.959595590919045,
      "grad_norm": 3.2010107040405273,
      "learning_rate": 4.25336703409008e-05,
      "loss": 0.7036,
      "step": 981900
    },
    {
      "epoch": 8.96050806628221,
      "grad_norm": 3.9413185119628906,
      "learning_rate": 4.253290994476483e-05,
      "loss": 0.6629,
      "step": 982000
    },
    {
      "epoch": 8.961420541645376,
      "grad_norm": 3.803697347640991,
      "learning_rate": 4.253214954862886e-05,
      "loss": 0.709,
      "step": 982100
    },
    {
      "epoch": 8.962333017008541,
      "grad_norm": 3.721827983856201,
      "learning_rate": 4.253138915249289e-05,
      "loss": 0.6967,
      "step": 982200
    },
    {
      "epoch": 8.963245492371707,
      "grad_norm": 3.523149251937866,
      "learning_rate": 4.253062875635692e-05,
      "loss": 0.6604,
      "step": 982300
    },
    {
      "epoch": 8.964157967734872,
      "grad_norm": 4.1168389320373535,
      "learning_rate": 4.252986836022094e-05,
      "loss": 0.6811,
      "step": 982400
    },
    {
      "epoch": 8.965070443098035,
      "grad_norm": 3.890875816345215,
      "learning_rate": 4.252910796408498e-05,
      "loss": 0.7014,
      "step": 982500
    },
    {
      "epoch": 8.9659829184612,
      "grad_norm": 3.9635698795318604,
      "learning_rate": 4.2528347567949e-05,
      "loss": 0.7823,
      "step": 982600
    },
    {
      "epoch": 8.966895393824366,
      "grad_norm": 4.073853969573975,
      "learning_rate": 4.2527587171813024e-05,
      "loss": 0.6875,
      "step": 982700
    },
    {
      "epoch": 8.967807869187531,
      "grad_norm": 4.25563383102417,
      "learning_rate": 4.252682677567706e-05,
      "loss": 0.6796,
      "step": 982800
    },
    {
      "epoch": 8.968720344550697,
      "grad_norm": 3.7431252002716064,
      "learning_rate": 4.2526066379541084e-05,
      "loss": 0.6868,
      "step": 982900
    },
    {
      "epoch": 8.969632819913862,
      "grad_norm": 4.568156719207764,
      "learning_rate": 4.2525305983405114e-05,
      "loss": 0.7127,
      "step": 983000
    },
    {
      "epoch": 8.970545295277027,
      "grad_norm": 3.7569611072540283,
      "learning_rate": 4.2524545587269144e-05,
      "loss": 0.6839,
      "step": 983100
    },
    {
      "epoch": 8.971457770640193,
      "grad_norm": 4.601203918457031,
      "learning_rate": 4.2523785191133174e-05,
      "loss": 0.6729,
      "step": 983200
    },
    {
      "epoch": 8.972370246003358,
      "grad_norm": 3.9025678634643555,
      "learning_rate": 4.2523024794997204e-05,
      "loss": 0.7092,
      "step": 983300
    },
    {
      "epoch": 8.973282721366523,
      "grad_norm": 4.672538757324219,
      "learning_rate": 4.2522264398861234e-05,
      "loss": 0.6758,
      "step": 983400
    },
    {
      "epoch": 8.974195196729688,
      "grad_norm": 3.9124279022216797,
      "learning_rate": 4.252150400272526e-05,
      "loss": 0.7321,
      "step": 983500
    },
    {
      "epoch": 8.975107672092854,
      "grad_norm": 3.6747264862060547,
      "learning_rate": 4.2520743606589295e-05,
      "loss": 0.6712,
      "step": 983600
    },
    {
      "epoch": 8.976020147456019,
      "grad_norm": 4.220804691314697,
      "learning_rate": 4.251998321045332e-05,
      "loss": 0.6817,
      "step": 983700
    },
    {
      "epoch": 8.976932622819184,
      "grad_norm": 4.119688034057617,
      "learning_rate": 4.251922281431735e-05,
      "loss": 0.7039,
      "step": 983800
    },
    {
      "epoch": 8.97784509818235,
      "grad_norm": 2.876044988632202,
      "learning_rate": 4.251846241818138e-05,
      "loss": 0.6898,
      "step": 983900
    },
    {
      "epoch": 8.978757573545515,
      "grad_norm": 4.317413330078125,
      "learning_rate": 4.251770202204541e-05,
      "loss": 0.6524,
      "step": 984000
    },
    {
      "epoch": 8.97967004890868,
      "grad_norm": 3.7559638023376465,
      "learning_rate": 4.251694162590943e-05,
      "loss": 0.7131,
      "step": 984100
    },
    {
      "epoch": 8.980582524271846,
      "grad_norm": 3.848132610321045,
      "learning_rate": 4.251618122977347e-05,
      "loss": 0.671,
      "step": 984200
    },
    {
      "epoch": 8.981494999635009,
      "grad_norm": 4.528705596923828,
      "learning_rate": 4.251542083363749e-05,
      "loss": 0.7117,
      "step": 984300
    },
    {
      "epoch": 8.982407474998174,
      "grad_norm": 4.07546329498291,
      "learning_rate": 4.251466043750152e-05,
      "loss": 0.6446,
      "step": 984400
    },
    {
      "epoch": 8.98331995036134,
      "grad_norm": 4.483000755310059,
      "learning_rate": 4.251390004136555e-05,
      "loss": 0.6967,
      "step": 984500
    },
    {
      "epoch": 8.984232425724505,
      "grad_norm": 3.7132668495178223,
      "learning_rate": 4.251313964522958e-05,
      "loss": 0.6924,
      "step": 984600
    },
    {
      "epoch": 8.98514490108767,
      "grad_norm": 4.557685375213623,
      "learning_rate": 4.251237924909361e-05,
      "loss": 0.6687,
      "step": 984700
    },
    {
      "epoch": 8.986057376450836,
      "grad_norm": 4.4680256843566895,
      "learning_rate": 4.251161885295764e-05,
      "loss": 0.7391,
      "step": 984800
    },
    {
      "epoch": 8.986969851814,
      "grad_norm": 4.282927513122559,
      "learning_rate": 4.2510858456821665e-05,
      "loss": 0.7178,
      "step": 984900
    },
    {
      "epoch": 8.987882327177166,
      "grad_norm": 4.972204208374023,
      "learning_rate": 4.25100980606857e-05,
      "loss": 0.7111,
      "step": 985000
    },
    {
      "epoch": 8.988794802540331,
      "grad_norm": 5.274710655212402,
      "learning_rate": 4.2509337664549725e-05,
      "loss": 0.7196,
      "step": 985100
    },
    {
      "epoch": 8.989707277903497,
      "grad_norm": 4.070643901824951,
      "learning_rate": 4.2508577268413755e-05,
      "loss": 0.6942,
      "step": 985200
    },
    {
      "epoch": 8.990619753266662,
      "grad_norm": 3.8642005920410156,
      "learning_rate": 4.2507816872277785e-05,
      "loss": 0.6815,
      "step": 985300
    },
    {
      "epoch": 8.991532228629827,
      "grad_norm": 4.58231782913208,
      "learning_rate": 4.2507056476141815e-05,
      "loss": 0.7196,
      "step": 985400
    },
    {
      "epoch": 8.992444703992993,
      "grad_norm": 3.6722021102905273,
      "learning_rate": 4.250629608000584e-05,
      "loss": 0.6524,
      "step": 985500
    },
    {
      "epoch": 8.993357179356158,
      "grad_norm": 3.3232955932617188,
      "learning_rate": 4.250553568386987e-05,
      "loss": 0.7483,
      "step": 985600
    },
    {
      "epoch": 8.994269654719323,
      "grad_norm": 4.083379745483398,
      "learning_rate": 4.25047752877339e-05,
      "loss": 0.7379,
      "step": 985700
    },
    {
      "epoch": 8.995182130082489,
      "grad_norm": 3.5726840496063232,
      "learning_rate": 4.250401489159793e-05,
      "loss": 0.7035,
      "step": 985800
    },
    {
      "epoch": 8.996094605445652,
      "grad_norm": 4.1744561195373535,
      "learning_rate": 4.250325449546196e-05,
      "loss": 0.7134,
      "step": 985900
    },
    {
      "epoch": 8.997007080808817,
      "grad_norm": 3.070491313934326,
      "learning_rate": 4.250249409932598e-05,
      "loss": 0.6884,
      "step": 986000
    },
    {
      "epoch": 8.997919556171983,
      "grad_norm": 3.77071213722229,
      "learning_rate": 4.250173370319002e-05,
      "loss": 0.6986,
      "step": 986100
    },
    {
      "epoch": 8.998832031535148,
      "grad_norm": 5.052147388458252,
      "learning_rate": 4.250097330705404e-05,
      "loss": 0.6916,
      "step": 986200
    },
    {
      "epoch": 8.999744506898313,
      "grad_norm": 4.31129789352417,
      "learning_rate": 4.250021291091807e-05,
      "loss": 0.6874,
      "step": 986300
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.5616425275802612,
      "eval_runtime": 25.7659,
      "eval_samples_per_second": 223.9,
      "eval_steps_per_second": 223.9,
      "step": 986328
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.5423380136489868,
      "eval_runtime": 484.209,
      "eval_samples_per_second": 226.332,
      "eval_steps_per_second": 226.332,
      "step": 986328
    },
    {
      "epoch": 9.000656982261479,
      "grad_norm": 4.276936054229736,
      "learning_rate": 4.24994525147821e-05,
      "loss": 0.692,
      "step": 986400
    },
    {
      "epoch": 9.001569457624644,
      "grad_norm": 5.083064079284668,
      "learning_rate": 4.249869211864613e-05,
      "loss": 0.6901,
      "step": 986500
    },
    {
      "epoch": 9.00248193298781,
      "grad_norm": 3.212932586669922,
      "learning_rate": 4.2497931722510156e-05,
      "loss": 0.6954,
      "step": 986600
    },
    {
      "epoch": 9.003394408350974,
      "grad_norm": 4.656891822814941,
      "learning_rate": 4.249717132637419e-05,
      "loss": 0.7181,
      "step": 986700
    },
    {
      "epoch": 9.00430688371414,
      "grad_norm": 4.440963268280029,
      "learning_rate": 4.2496410930238216e-05,
      "loss": 0.6686,
      "step": 986800
    },
    {
      "epoch": 9.005219359077305,
      "grad_norm": 4.48715877532959,
      "learning_rate": 4.2495650534102246e-05,
      "loss": 0.6826,
      "step": 986900
    },
    {
      "epoch": 9.00613183444047,
      "grad_norm": 3.9602630138397217,
      "learning_rate": 4.2494890137966276e-05,
      "loss": 0.6994,
      "step": 987000
    },
    {
      "epoch": 9.007044309803636,
      "grad_norm": 4.42113733291626,
      "learning_rate": 4.2494129741830306e-05,
      "loss": 0.6771,
      "step": 987100
    },
    {
      "epoch": 9.007956785166801,
      "grad_norm": 4.0755696296691895,
      "learning_rate": 4.2493369345694336e-05,
      "loss": 0.6684,
      "step": 987200
    },
    {
      "epoch": 9.008869260529966,
      "grad_norm": 3.9511191844940186,
      "learning_rate": 4.2492608949558366e-05,
      "loss": 0.6584,
      "step": 987300
    },
    {
      "epoch": 9.009781735893132,
      "grad_norm": 4.7913665771484375,
      "learning_rate": 4.249184855342239e-05,
      "loss": 0.6663,
      "step": 987400
    },
    {
      "epoch": 9.010694211256297,
      "grad_norm": 3.9952473640441895,
      "learning_rate": 4.2491088157286426e-05,
      "loss": 0.7074,
      "step": 987500
    },
    {
      "epoch": 9.01160668661946,
      "grad_norm": 3.9225335121154785,
      "learning_rate": 4.249032776115045e-05,
      "loss": 0.7303,
      "step": 987600
    },
    {
      "epoch": 9.012519161982626,
      "grad_norm": 3.5807600021362305,
      "learning_rate": 4.248956736501448e-05,
      "loss": 0.6462,
      "step": 987700
    },
    {
      "epoch": 9.013431637345791,
      "grad_norm": 4.3838276863098145,
      "learning_rate": 4.248880696887851e-05,
      "loss": 0.6596,
      "step": 987800
    },
    {
      "epoch": 9.014344112708956,
      "grad_norm": 4.198085784912109,
      "learning_rate": 4.248804657274254e-05,
      "loss": 0.665,
      "step": 987900
    },
    {
      "epoch": 9.015256588072122,
      "grad_norm": 3.187870502471924,
      "learning_rate": 4.248728617660656e-05,
      "loss": 0.6802,
      "step": 988000
    },
    {
      "epoch": 9.016169063435287,
      "grad_norm": 3.492354393005371,
      "learning_rate": 4.24865257804706e-05,
      "loss": 0.7021,
      "step": 988100
    },
    {
      "epoch": 9.017081538798452,
      "grad_norm": 4.008871078491211,
      "learning_rate": 4.2485765384334623e-05,
      "loss": 0.7094,
      "step": 988200
    },
    {
      "epoch": 9.017994014161617,
      "grad_norm": 3.934966564178467,
      "learning_rate": 4.2485004988198653e-05,
      "loss": 0.678,
      "step": 988300
    },
    {
      "epoch": 9.018906489524783,
      "grad_norm": 4.461606502532959,
      "learning_rate": 4.2484244592062684e-05,
      "loss": 0.7047,
      "step": 988400
    },
    {
      "epoch": 9.019818964887948,
      "grad_norm": 3.586822032928467,
      "learning_rate": 4.248348419592671e-05,
      "loss": 0.6826,
      "step": 988500
    },
    {
      "epoch": 9.020731440251113,
      "grad_norm": 4.383121490478516,
      "learning_rate": 4.2482723799790744e-05,
      "loss": 0.7089,
      "step": 988600
    },
    {
      "epoch": 9.021643915614279,
      "grad_norm": 2.364818811416626,
      "learning_rate": 4.248196340365477e-05,
      "loss": 0.6355,
      "step": 988700
    },
    {
      "epoch": 9.022556390977444,
      "grad_norm": 4.50720739364624,
      "learning_rate": 4.24812030075188e-05,
      "loss": 0.6659,
      "step": 988800
    },
    {
      "epoch": 9.02346886634061,
      "grad_norm": 4.1467156410217285,
      "learning_rate": 4.248044261138283e-05,
      "loss": 0.6426,
      "step": 988900
    },
    {
      "epoch": 9.024381341703775,
      "grad_norm": 3.5688953399658203,
      "learning_rate": 4.247968221524686e-05,
      "loss": 0.6863,
      "step": 989000
    },
    {
      "epoch": 9.02529381706694,
      "grad_norm": 4.689392566680908,
      "learning_rate": 4.247892181911089e-05,
      "loss": 0.6774,
      "step": 989100
    },
    {
      "epoch": 9.026206292430105,
      "grad_norm": 4.0082783699035645,
      "learning_rate": 4.247816142297492e-05,
      "loss": 0.6438,
      "step": 989200
    },
    {
      "epoch": 9.027118767793269,
      "grad_norm": 3.4840993881225586,
      "learning_rate": 4.247740102683894e-05,
      "loss": 0.6693,
      "step": 989300
    },
    {
      "epoch": 9.028031243156434,
      "grad_norm": 4.507316589355469,
      "learning_rate": 4.247664063070298e-05,
      "loss": 0.6855,
      "step": 989400
    },
    {
      "epoch": 9.0289437185196,
      "grad_norm": 3.783522367477417,
      "learning_rate": 4.2475880234567e-05,
      "loss": 0.6618,
      "step": 989500
    },
    {
      "epoch": 9.029856193882765,
      "grad_norm": 3.4614219665527344,
      "learning_rate": 4.247511983843103e-05,
      "loss": 0.6974,
      "step": 989600
    },
    {
      "epoch": 9.03076866924593,
      "grad_norm": 3.1566855907440186,
      "learning_rate": 4.247435944229506e-05,
      "loss": 0.6739,
      "step": 989700
    },
    {
      "epoch": 9.031681144609095,
      "grad_norm": 4.143608570098877,
      "learning_rate": 4.247359904615909e-05,
      "loss": 0.6758,
      "step": 989800
    },
    {
      "epoch": 9.03259361997226,
      "grad_norm": 4.534526824951172,
      "learning_rate": 4.2472838650023114e-05,
      "loss": 0.667,
      "step": 989900
    },
    {
      "epoch": 9.033506095335426,
      "grad_norm": 3.8564565181732178,
      "learning_rate": 4.247207825388715e-05,
      "loss": 0.6624,
      "step": 990000
    },
    {
      "epoch": 9.034418570698591,
      "grad_norm": 3.769296407699585,
      "learning_rate": 4.2471317857751174e-05,
      "loss": 0.6614,
      "step": 990100
    },
    {
      "epoch": 9.035331046061756,
      "grad_norm": 4.421175003051758,
      "learning_rate": 4.2470557461615204e-05,
      "loss": 0.6834,
      "step": 990200
    },
    {
      "epoch": 9.036243521424922,
      "grad_norm": 2.7274229526519775,
      "learning_rate": 4.2469797065479234e-05,
      "loss": 0.6746,
      "step": 990300
    },
    {
      "epoch": 9.037155996788087,
      "grad_norm": 4.410780906677246,
      "learning_rate": 4.2469036669343265e-05,
      "loss": 0.6668,
      "step": 990400
    },
    {
      "epoch": 9.038068472151252,
      "grad_norm": 5.216732025146484,
      "learning_rate": 4.2468276273207295e-05,
      "loss": 0.6508,
      "step": 990500
    },
    {
      "epoch": 9.038980947514418,
      "grad_norm": 3.5199804306030273,
      "learning_rate": 4.2467515877071325e-05,
      "loss": 0.698,
      "step": 990600
    },
    {
      "epoch": 9.039893422877583,
      "grad_norm": 3.8263485431671143,
      "learning_rate": 4.246675548093535e-05,
      "loss": 0.7003,
      "step": 990700
    },
    {
      "epoch": 9.040805898240748,
      "grad_norm": 4.263550758361816,
      "learning_rate": 4.2465995084799385e-05,
      "loss": 0.646,
      "step": 990800
    },
    {
      "epoch": 9.041718373603914,
      "grad_norm": 4.296667098999023,
      "learning_rate": 4.246523468866341e-05,
      "loss": 0.696,
      "step": 990900
    },
    {
      "epoch": 9.042630848967077,
      "grad_norm": 4.087649822235107,
      "learning_rate": 4.246447429252744e-05,
      "loss": 0.7112,
      "step": 991000
    },
    {
      "epoch": 9.043543324330242,
      "grad_norm": 4.016242980957031,
      "learning_rate": 4.246371389639147e-05,
      "loss": 0.6371,
      "step": 991100
    },
    {
      "epoch": 9.044455799693408,
      "grad_norm": 3.720762014389038,
      "learning_rate": 4.24629535002555e-05,
      "loss": 0.6434,
      "step": 991200
    },
    {
      "epoch": 9.045368275056573,
      "grad_norm": 3.59704327583313,
      "learning_rate": 4.246219310411952e-05,
      "loss": 0.708,
      "step": 991300
    },
    {
      "epoch": 9.046280750419738,
      "grad_norm": 3.0873520374298096,
      "learning_rate": 4.246143270798355e-05,
      "loss": 0.6978,
      "step": 991400
    },
    {
      "epoch": 9.047193225782904,
      "grad_norm": 4.333673000335693,
      "learning_rate": 4.246067231184758e-05,
      "loss": 0.6914,
      "step": 991500
    },
    {
      "epoch": 9.048105701146069,
      "grad_norm": 3.8438847064971924,
      "learning_rate": 4.245991191571161e-05,
      "loss": 0.7219,
      "step": 991600
    },
    {
      "epoch": 9.049018176509234,
      "grad_norm": 4.174044132232666,
      "learning_rate": 4.245915151957564e-05,
      "loss": 0.7033,
      "step": 991700
    },
    {
      "epoch": 9.0499306518724,
      "grad_norm": 4.430037498474121,
      "learning_rate": 4.2458391123439665e-05,
      "loss": 0.6862,
      "step": 991800
    },
    {
      "epoch": 9.050843127235565,
      "grad_norm": 3.934051275253296,
      "learning_rate": 4.24576307273037e-05,
      "loss": 0.6888,
      "step": 991900
    },
    {
      "epoch": 9.05175560259873,
      "grad_norm": 3.5119009017944336,
      "learning_rate": 4.2456870331167725e-05,
      "loss": 0.7237,
      "step": 992000
    },
    {
      "epoch": 9.052668077961895,
      "grad_norm": 3.859905242919922,
      "learning_rate": 4.2456109935031755e-05,
      "loss": 0.7109,
      "step": 992100
    },
    {
      "epoch": 9.05358055332506,
      "grad_norm": 3.428394079208374,
      "learning_rate": 4.2455349538895785e-05,
      "loss": 0.6739,
      "step": 992200
    },
    {
      "epoch": 9.054493028688226,
      "grad_norm": 3.930764675140381,
      "learning_rate": 4.2454589142759816e-05,
      "loss": 0.6464,
      "step": 992300
    },
    {
      "epoch": 9.055405504051391,
      "grad_norm": 4.273039817810059,
      "learning_rate": 4.245382874662384e-05,
      "loss": 0.7086,
      "step": 992400
    },
    {
      "epoch": 9.056317979414557,
      "grad_norm": 3.897806406021118,
      "learning_rate": 4.2453068350487876e-05,
      "loss": 0.7067,
      "step": 992500
    },
    {
      "epoch": 9.057230454777722,
      "grad_norm": 4.1897969245910645,
      "learning_rate": 4.24523079543519e-05,
      "loss": 0.6602,
      "step": 992600
    },
    {
      "epoch": 9.058142930140885,
      "grad_norm": 3.3308255672454834,
      "learning_rate": 4.245154755821593e-05,
      "loss": 0.6776,
      "step": 992700
    },
    {
      "epoch": 9.05905540550405,
      "grad_norm": 3.704439163208008,
      "learning_rate": 4.245078716207996e-05,
      "loss": 0.7052,
      "step": 992800
    },
    {
      "epoch": 9.059967880867216,
      "grad_norm": 4.208245277404785,
      "learning_rate": 4.245002676594399e-05,
      "loss": 0.6295,
      "step": 992900
    },
    {
      "epoch": 9.060880356230381,
      "grad_norm": 4.165113925933838,
      "learning_rate": 4.244926636980802e-05,
      "loss": 0.6216,
      "step": 993000
    },
    {
      "epoch": 9.061792831593547,
      "grad_norm": 4.137325763702393,
      "learning_rate": 4.244850597367205e-05,
      "loss": 0.7053,
      "step": 993100
    },
    {
      "epoch": 9.062705306956712,
      "grad_norm": 3.1956424713134766,
      "learning_rate": 4.244774557753607e-05,
      "loss": 0.6734,
      "step": 993200
    },
    {
      "epoch": 9.063617782319877,
      "grad_norm": 4.168454647064209,
      "learning_rate": 4.244698518140011e-05,
      "loss": 0.6325,
      "step": 993300
    },
    {
      "epoch": 9.064530257683042,
      "grad_norm": 3.9402401447296143,
      "learning_rate": 4.244622478526413e-05,
      "loss": 0.6728,
      "step": 993400
    },
    {
      "epoch": 9.065442733046208,
      "grad_norm": 3.6701130867004395,
      "learning_rate": 4.244546438912816e-05,
      "loss": 0.6946,
      "step": 993500
    },
    {
      "epoch": 9.066355208409373,
      "grad_norm": 4.076098442077637,
      "learning_rate": 4.244470399299219e-05,
      "loss": 0.6736,
      "step": 993600
    },
    {
      "epoch": 9.067267683772538,
      "grad_norm": 4.072411060333252,
      "learning_rate": 4.244394359685622e-05,
      "loss": 0.7003,
      "step": 993700
    },
    {
      "epoch": 9.068180159135704,
      "grad_norm": 4.173089504241943,
      "learning_rate": 4.2443183200720246e-05,
      "loss": 0.6624,
      "step": 993800
    },
    {
      "epoch": 9.069092634498869,
      "grad_norm": 3.0148279666900635,
      "learning_rate": 4.244242280458428e-05,
      "loss": 0.6733,
      "step": 993900
    },
    {
      "epoch": 9.070005109862034,
      "grad_norm": 4.165518760681152,
      "learning_rate": 4.2441662408448306e-05,
      "loss": 0.6577,
      "step": 994000
    },
    {
      "epoch": 9.0709175852252,
      "grad_norm": 3.6839239597320557,
      "learning_rate": 4.2440902012312336e-05,
      "loss": 0.7542,
      "step": 994100
    },
    {
      "epoch": 9.071830060588365,
      "grad_norm": 3.983856439590454,
      "learning_rate": 4.2440141616176366e-05,
      "loss": 0.6949,
      "step": 994200
    },
    {
      "epoch": 9.07274253595153,
      "grad_norm": 4.021657943725586,
      "learning_rate": 4.243938122004039e-05,
      "loss": 0.6666,
      "step": 994300
    },
    {
      "epoch": 9.073655011314694,
      "grad_norm": 4.538328170776367,
      "learning_rate": 4.2438620823904427e-05,
      "loss": 0.6901,
      "step": 994400
    },
    {
      "epoch": 9.074567486677859,
      "grad_norm": 4.030138969421387,
      "learning_rate": 4.243786042776845e-05,
      "loss": 0.7105,
      "step": 994500
    },
    {
      "epoch": 9.075479962041024,
      "grad_norm": 4.086736679077148,
      "learning_rate": 4.243710003163248e-05,
      "loss": 0.7133,
      "step": 994600
    },
    {
      "epoch": 9.07639243740419,
      "grad_norm": 4.63868522644043,
      "learning_rate": 4.243633963549651e-05,
      "loss": 0.7152,
      "step": 994700
    },
    {
      "epoch": 9.077304912767355,
      "grad_norm": 4.214611530303955,
      "learning_rate": 4.243557923936054e-05,
      "loss": 0.6962,
      "step": 994800
    },
    {
      "epoch": 9.07821738813052,
      "grad_norm": 3.221776247024536,
      "learning_rate": 4.243481884322456e-05,
      "loss": 0.6652,
      "step": 994900
    },
    {
      "epoch": 9.079129863493685,
      "grad_norm": 4.792639255523682,
      "learning_rate": 4.24340584470886e-05,
      "loss": 0.6466,
      "step": 995000
    },
    {
      "epoch": 9.08004233885685,
      "grad_norm": 4.947932720184326,
      "learning_rate": 4.2433298050952624e-05,
      "loss": 0.687,
      "step": 995100
    },
    {
      "epoch": 9.080954814220016,
      "grad_norm": 4.241794586181641,
      "learning_rate": 4.2432537654816654e-05,
      "loss": 0.6634,
      "step": 995200
    },
    {
      "epoch": 9.081867289583181,
      "grad_norm": 3.9629063606262207,
      "learning_rate": 4.2431777258680684e-05,
      "loss": 0.6616,
      "step": 995300
    },
    {
      "epoch": 9.082779764946347,
      "grad_norm": 3.3251664638519287,
      "learning_rate": 4.2431016862544714e-05,
      "loss": 0.6905,
      "step": 995400
    },
    {
      "epoch": 9.083692240309512,
      "grad_norm": 4.233701705932617,
      "learning_rate": 4.2430256466408744e-05,
      "loss": 0.6868,
      "step": 995500
    },
    {
      "epoch": 9.084604715672677,
      "grad_norm": 3.152334690093994,
      "learning_rate": 4.2429496070272774e-05,
      "loss": 0.6581,
      "step": 995600
    },
    {
      "epoch": 9.085517191035843,
      "grad_norm": 3.5622880458831787,
      "learning_rate": 4.24287356741368e-05,
      "loss": 0.7001,
      "step": 995700
    },
    {
      "epoch": 9.086429666399008,
      "grad_norm": 2.8820688724517822,
      "learning_rate": 4.2427975278000834e-05,
      "loss": 0.6901,
      "step": 995800
    },
    {
      "epoch": 9.087342141762173,
      "grad_norm": 4.606955051422119,
      "learning_rate": 4.242721488186486e-05,
      "loss": 0.6678,
      "step": 995900
    },
    {
      "epoch": 9.088254617125338,
      "grad_norm": 3.866661310195923,
      "learning_rate": 4.242645448572889e-05,
      "loss": 0.6879,
      "step": 996000
    },
    {
      "epoch": 9.089167092488502,
      "grad_norm": 3.651728630065918,
      "learning_rate": 4.242569408959292e-05,
      "loss": 0.6887,
      "step": 996100
    },
    {
      "epoch": 9.090079567851667,
      "grad_norm": 3.9135489463806152,
      "learning_rate": 4.242493369345695e-05,
      "loss": 0.6478,
      "step": 996200
    },
    {
      "epoch": 9.090992043214833,
      "grad_norm": 3.5546422004699707,
      "learning_rate": 4.242417329732097e-05,
      "loss": 0.676,
      "step": 996300
    },
    {
      "epoch": 9.091904518577998,
      "grad_norm": 3.7210657596588135,
      "learning_rate": 4.242341290118501e-05,
      "loss": 0.7003,
      "step": 996400
    },
    {
      "epoch": 9.092816993941163,
      "grad_norm": 4.014716148376465,
      "learning_rate": 4.242265250504903e-05,
      "loss": 0.6483,
      "step": 996500
    },
    {
      "epoch": 9.093729469304328,
      "grad_norm": 3.373776912689209,
      "learning_rate": 4.242189210891306e-05,
      "loss": 0.6766,
      "step": 996600
    },
    {
      "epoch": 9.094641944667494,
      "grad_norm": 4.519649028778076,
      "learning_rate": 4.242113171277709e-05,
      "loss": 0.6992,
      "step": 996700
    },
    {
      "epoch": 9.095554420030659,
      "grad_norm": 3.713190793991089,
      "learning_rate": 4.242037131664112e-05,
      "loss": 0.6763,
      "step": 996800
    },
    {
      "epoch": 9.096466895393824,
      "grad_norm": 3.755406618118286,
      "learning_rate": 4.241961092050515e-05,
      "loss": 0.671,
      "step": 996900
    },
    {
      "epoch": 9.09737937075699,
      "grad_norm": 5.0221991539001465,
      "learning_rate": 4.2418850524369174e-05,
      "loss": 0.6962,
      "step": 997000
    },
    {
      "epoch": 9.098291846120155,
      "grad_norm": 4.401358127593994,
      "learning_rate": 4.2418090128233205e-05,
      "loss": 0.6869,
      "step": 997100
    },
    {
      "epoch": 9.09920432148332,
      "grad_norm": 4.001951694488525,
      "learning_rate": 4.2417329732097235e-05,
      "loss": 0.7031,
      "step": 997200
    },
    {
      "epoch": 9.100116796846486,
      "grad_norm": 6.085352897644043,
      "learning_rate": 4.2416569335961265e-05,
      "loss": 0.6724,
      "step": 997300
    },
    {
      "epoch": 9.10102927220965,
      "grad_norm": 3.0352272987365723,
      "learning_rate": 4.241580893982529e-05,
      "loss": 0.6622,
      "step": 997400
    },
    {
      "epoch": 9.101941747572816,
      "grad_norm": 3.335618495941162,
      "learning_rate": 4.2415048543689325e-05,
      "loss": 0.6978,
      "step": 997500
    },
    {
      "epoch": 9.102854222935981,
      "grad_norm": 4.732894420623779,
      "learning_rate": 4.241428814755335e-05,
      "loss": 0.7009,
      "step": 997600
    },
    {
      "epoch": 9.103766698299147,
      "grad_norm": 4.223591327667236,
      "learning_rate": 4.241352775141738e-05,
      "loss": 0.6973,
      "step": 997700
    },
    {
      "epoch": 9.10467917366231,
      "grad_norm": 5.024528980255127,
      "learning_rate": 4.241276735528141e-05,
      "loss": 0.7259,
      "step": 997800
    },
    {
      "epoch": 9.105591649025476,
      "grad_norm": 4.221793174743652,
      "learning_rate": 4.241200695914544e-05,
      "loss": 0.6943,
      "step": 997900
    },
    {
      "epoch": 9.10650412438864,
      "grad_norm": 4.731501579284668,
      "learning_rate": 4.241124656300947e-05,
      "loss": 0.7033,
      "step": 998000
    },
    {
      "epoch": 9.107416599751806,
      "grad_norm": 3.83807635307312,
      "learning_rate": 4.24104861668735e-05,
      "loss": 0.6307,
      "step": 998100
    },
    {
      "epoch": 9.108329075114971,
      "grad_norm": 3.430365800857544,
      "learning_rate": 4.240972577073752e-05,
      "loss": 0.6919,
      "step": 998200
    },
    {
      "epoch": 9.109241550478137,
      "grad_norm": 3.7149484157562256,
      "learning_rate": 4.240896537460156e-05,
      "loss": 0.6883,
      "step": 998300
    },
    {
      "epoch": 9.110154025841302,
      "grad_norm": 3.157634973526001,
      "learning_rate": 4.240820497846558e-05,
      "loss": 0.6787,
      "step": 998400
    },
    {
      "epoch": 9.111066501204467,
      "grad_norm": 4.0279221534729,
      "learning_rate": 4.240744458232961e-05,
      "loss": 0.6814,
      "step": 998500
    },
    {
      "epoch": 9.111978976567633,
      "grad_norm": 3.6775453090667725,
      "learning_rate": 4.240668418619364e-05,
      "loss": 0.6885,
      "step": 998600
    },
    {
      "epoch": 9.112891451930798,
      "grad_norm": 4.467973709106445,
      "learning_rate": 4.240592379005767e-05,
      "loss": 0.6578,
      "step": 998700
    },
    {
      "epoch": 9.113803927293963,
      "grad_norm": 4.173099517822266,
      "learning_rate": 4.2405163393921695e-05,
      "loss": 0.6794,
      "step": 998800
    },
    {
      "epoch": 9.114716402657129,
      "grad_norm": 4.3006768226623535,
      "learning_rate": 4.240440299778573e-05,
      "loss": 0.6794,
      "step": 998900
    },
    {
      "epoch": 9.115628878020294,
      "grad_norm": 3.8174400329589844,
      "learning_rate": 4.2403642601649755e-05,
      "loss": 0.7206,
      "step": 999000
    },
    {
      "epoch": 9.11654135338346,
      "grad_norm": 3.9872238636016846,
      "learning_rate": 4.2402882205513786e-05,
      "loss": 0.7011,
      "step": 999100
    },
    {
      "epoch": 9.117453828746624,
      "grad_norm": 4.151017189025879,
      "learning_rate": 4.2402121809377816e-05,
      "loss": 0.7038,
      "step": 999200
    },
    {
      "epoch": 9.11836630410979,
      "grad_norm": 3.9730658531188965,
      "learning_rate": 4.2401361413241846e-05,
      "loss": 0.6977,
      "step": 999300
    },
    {
      "epoch": 9.119278779472955,
      "grad_norm": 3.1285080909729004,
      "learning_rate": 4.2400601017105876e-05,
      "loss": 0.7046,
      "step": 999400
    },
    {
      "epoch": 9.120191254836119,
      "grad_norm": 3.809692144393921,
      "learning_rate": 4.2399840620969906e-05,
      "loss": 0.6728,
      "step": 999500
    },
    {
      "epoch": 9.121103730199284,
      "grad_norm": 4.480796813964844,
      "learning_rate": 4.239908022483393e-05,
      "loss": 0.6784,
      "step": 999600
    },
    {
      "epoch": 9.12201620556245,
      "grad_norm": 3.5998494625091553,
      "learning_rate": 4.2398319828697966e-05,
      "loss": 0.6925,
      "step": 999700
    },
    {
      "epoch": 9.122928680925614,
      "grad_norm": 4.162919998168945,
      "learning_rate": 4.239755943256199e-05,
      "loss": 0.701,
      "step": 999800
    },
    {
      "epoch": 9.12384115628878,
      "grad_norm": 3.7808918952941895,
      "learning_rate": 4.239679903642601e-05,
      "loss": 0.7056,
      "step": 999900
    },
    {
      "epoch": 9.124753631651945,
      "grad_norm": 4.181920051574707,
      "learning_rate": 4.239603864029005e-05,
      "loss": 0.7042,
      "step": 1000000
    },
    {
      "epoch": 9.12566610701511,
      "grad_norm": 3.8921587467193604,
      "learning_rate": 4.239527824415407e-05,
      "loss": 0.6899,
      "step": 1000100
    },
    {
      "epoch": 9.126578582378276,
      "grad_norm": 3.771798610687256,
      "learning_rate": 4.23945178480181e-05,
      "loss": 0.6907,
      "step": 1000200
    },
    {
      "epoch": 9.127491057741441,
      "grad_norm": 4.5093994140625,
      "learning_rate": 4.239375745188213e-05,
      "loss": 0.6631,
      "step": 1000300
    },
    {
      "epoch": 9.128403533104606,
      "grad_norm": 4.18259334564209,
      "learning_rate": 4.239299705574616e-05,
      "loss": 0.7153,
      "step": 1000400
    },
    {
      "epoch": 9.129316008467772,
      "grad_norm": 4.210383415222168,
      "learning_rate": 4.239223665961019e-05,
      "loss": 0.6731,
      "step": 1000500
    },
    {
      "epoch": 9.130228483830937,
      "grad_norm": 3.9406986236572266,
      "learning_rate": 4.239147626347422e-05,
      "loss": 0.6795,
      "step": 1000600
    },
    {
      "epoch": 9.131140959194102,
      "grad_norm": 3.81520676612854,
      "learning_rate": 4.2390715867338246e-05,
      "loss": 0.7038,
      "step": 1000700
    },
    {
      "epoch": 9.132053434557267,
      "grad_norm": 3.586968421936035,
      "learning_rate": 4.238995547120228e-05,
      "loss": 0.6656,
      "step": 1000800
    },
    {
      "epoch": 9.132965909920433,
      "grad_norm": 3.7744219303131104,
      "learning_rate": 4.2389195075066306e-05,
      "loss": 0.7101,
      "step": 1000900
    },
    {
      "epoch": 9.133878385283598,
      "grad_norm": 3.7650699615478516,
      "learning_rate": 4.2388434678930336e-05,
      "loss": 0.6643,
      "step": 1001000
    },
    {
      "epoch": 9.134790860646763,
      "grad_norm": 4.137081146240234,
      "learning_rate": 4.2387674282794367e-05,
      "loss": 0.6492,
      "step": 1001100
    },
    {
      "epoch": 9.135703336009927,
      "grad_norm": 4.845391273498535,
      "learning_rate": 4.23869138866584e-05,
      "loss": 0.7107,
      "step": 1001200
    },
    {
      "epoch": 9.136615811373092,
      "grad_norm": 3.627626657485962,
      "learning_rate": 4.238615349052243e-05,
      "loss": 0.6901,
      "step": 1001300
    },
    {
      "epoch": 9.137528286736257,
      "grad_norm": 3.3040833473205566,
      "learning_rate": 4.238539309438646e-05,
      "loss": 0.6823,
      "step": 1001400
    },
    {
      "epoch": 9.138440762099423,
      "grad_norm": 3.970717191696167,
      "learning_rate": 4.238463269825048e-05,
      "loss": 0.6864,
      "step": 1001500
    },
    {
      "epoch": 9.139353237462588,
      "grad_norm": 3.045030355453491,
      "learning_rate": 4.238387230211451e-05,
      "loss": 0.6857,
      "step": 1001600
    },
    {
      "epoch": 9.140265712825753,
      "grad_norm": 4.852859973907471,
      "learning_rate": 4.238311190597854e-05,
      "loss": 0.6678,
      "step": 1001700
    },
    {
      "epoch": 9.141178188188919,
      "grad_norm": 4.2646894454956055,
      "learning_rate": 4.238235150984257e-05,
      "loss": 0.6639,
      "step": 1001800
    },
    {
      "epoch": 9.142090663552084,
      "grad_norm": 3.6171088218688965,
      "learning_rate": 4.23815911137066e-05,
      "loss": 0.647,
      "step": 1001900
    },
    {
      "epoch": 9.14300313891525,
      "grad_norm": 3.789923667907715,
      "learning_rate": 4.238083071757063e-05,
      "loss": 0.6963,
      "step": 1002000
    },
    {
      "epoch": 9.143915614278415,
      "grad_norm": 3.3128576278686523,
      "learning_rate": 4.2380070321434654e-05,
      "loss": 0.7205,
      "step": 1002100
    },
    {
      "epoch": 9.14482808964158,
      "grad_norm": 2.945993423461914,
      "learning_rate": 4.237930992529869e-05,
      "loss": 0.7138,
      "step": 1002200
    },
    {
      "epoch": 9.145740565004745,
      "grad_norm": 3.9419608116149902,
      "learning_rate": 4.2378549529162714e-05,
      "loss": 0.7193,
      "step": 1002300
    },
    {
      "epoch": 9.14665304036791,
      "grad_norm": 4.185678958892822,
      "learning_rate": 4.2377789133026744e-05,
      "loss": 0.6758,
      "step": 1002400
    },
    {
      "epoch": 9.147565515731076,
      "grad_norm": 4.248563766479492,
      "learning_rate": 4.2377028736890774e-05,
      "loss": 0.6843,
      "step": 1002500
    },
    {
      "epoch": 9.148477991094241,
      "grad_norm": 4.288710117340088,
      "learning_rate": 4.23762683407548e-05,
      "loss": 0.7013,
      "step": 1002600
    },
    {
      "epoch": 9.149390466457406,
      "grad_norm": 3.185872793197632,
      "learning_rate": 4.2375507944618834e-05,
      "loss": 0.7392,
      "step": 1002700
    },
    {
      "epoch": 9.150302941820572,
      "grad_norm": 4.106564998626709,
      "learning_rate": 4.237474754848286e-05,
      "loss": 0.6846,
      "step": 1002800
    },
    {
      "epoch": 9.151215417183735,
      "grad_norm": 4.609188079833984,
      "learning_rate": 4.237398715234689e-05,
      "loss": 0.6607,
      "step": 1002900
    },
    {
      "epoch": 9.1521278925469,
      "grad_norm": 3.255047559738159,
      "learning_rate": 4.237322675621092e-05,
      "loss": 0.715,
      "step": 1003000
    },
    {
      "epoch": 9.153040367910066,
      "grad_norm": 3.7732810974121094,
      "learning_rate": 4.237246636007495e-05,
      "loss": 0.7017,
      "step": 1003100
    },
    {
      "epoch": 9.153952843273231,
      "grad_norm": 4.469143867492676,
      "learning_rate": 4.237170596393897e-05,
      "loss": 0.6606,
      "step": 1003200
    },
    {
      "epoch": 9.154865318636396,
      "grad_norm": 3.9850594997406006,
      "learning_rate": 4.237094556780301e-05,
      "loss": 0.6865,
      "step": 1003300
    },
    {
      "epoch": 9.155777793999562,
      "grad_norm": 3.56988525390625,
      "learning_rate": 4.237018517166703e-05,
      "loss": 0.7033,
      "step": 1003400
    },
    {
      "epoch": 9.156690269362727,
      "grad_norm": 3.3262221813201904,
      "learning_rate": 4.236942477553106e-05,
      "loss": 0.696,
      "step": 1003500
    },
    {
      "epoch": 9.157602744725892,
      "grad_norm": 3.8416996002197266,
      "learning_rate": 4.236866437939509e-05,
      "loss": 0.6616,
      "step": 1003600
    },
    {
      "epoch": 9.158515220089058,
      "grad_norm": 3.5708489418029785,
      "learning_rate": 4.236790398325912e-05,
      "loss": 0.6669,
      "step": 1003700
    },
    {
      "epoch": 9.159427695452223,
      "grad_norm": 3.516906261444092,
      "learning_rate": 4.236714358712315e-05,
      "loss": 0.6914,
      "step": 1003800
    },
    {
      "epoch": 9.160340170815388,
      "grad_norm": 4.010988235473633,
      "learning_rate": 4.236638319098718e-05,
      "loss": 0.675,
      "step": 1003900
    },
    {
      "epoch": 9.161252646178554,
      "grad_norm": 4.592622756958008,
      "learning_rate": 4.2365622794851205e-05,
      "loss": 0.704,
      "step": 1004000
    },
    {
      "epoch": 9.162165121541719,
      "grad_norm": 4.720539093017578,
      "learning_rate": 4.236486239871524e-05,
      "loss": 0.6893,
      "step": 1004100
    },
    {
      "epoch": 9.163077596904884,
      "grad_norm": 4.3479084968566895,
      "learning_rate": 4.2364102002579265e-05,
      "loss": 0.7065,
      "step": 1004200
    },
    {
      "epoch": 9.16399007226805,
      "grad_norm": 4.332796096801758,
      "learning_rate": 4.2363341606443295e-05,
      "loss": 0.7484,
      "step": 1004300
    },
    {
      "epoch": 9.164902547631215,
      "grad_norm": 4.325990200042725,
      "learning_rate": 4.2362581210307325e-05,
      "loss": 0.6604,
      "step": 1004400
    },
    {
      "epoch": 9.16581502299438,
      "grad_norm": 4.431541919708252,
      "learning_rate": 4.2361820814171355e-05,
      "loss": 0.6884,
      "step": 1004500
    },
    {
      "epoch": 9.166727498357544,
      "grad_norm": 4.895318031311035,
      "learning_rate": 4.236106041803538e-05,
      "loss": 0.6667,
      "step": 1004600
    },
    {
      "epoch": 9.167639973720709,
      "grad_norm": 4.164173126220703,
      "learning_rate": 4.2360300021899415e-05,
      "loss": 0.661,
      "step": 1004700
    },
    {
      "epoch": 9.168552449083874,
      "grad_norm": 4.608367919921875,
      "learning_rate": 4.235953962576344e-05,
      "loss": 0.6789,
      "step": 1004800
    },
    {
      "epoch": 9.16946492444704,
      "grad_norm": 5.364264488220215,
      "learning_rate": 4.235877922962747e-05,
      "loss": 0.7002,
      "step": 1004900
    },
    {
      "epoch": 9.170377399810205,
      "grad_norm": 3.937777519226074,
      "learning_rate": 4.23580188334915e-05,
      "loss": 0.7063,
      "step": 1005000
    },
    {
      "epoch": 9.17128987517337,
      "grad_norm": 4.057193756103516,
      "learning_rate": 4.235725843735553e-05,
      "loss": 0.7001,
      "step": 1005100
    },
    {
      "epoch": 9.172202350536535,
      "grad_norm": 4.082249164581299,
      "learning_rate": 4.235649804121956e-05,
      "loss": 0.7114,
      "step": 1005200
    },
    {
      "epoch": 9.1731148258997,
      "grad_norm": 2.518195390701294,
      "learning_rate": 4.235573764508359e-05,
      "loss": 0.6648,
      "step": 1005300
    },
    {
      "epoch": 9.174027301262866,
      "grad_norm": 3.8938252925872803,
      "learning_rate": 4.235497724894761e-05,
      "loss": 0.6767,
      "step": 1005400
    },
    {
      "epoch": 9.174939776626031,
      "grad_norm": 4.098297595977783,
      "learning_rate": 4.235421685281164e-05,
      "loss": 0.6836,
      "step": 1005500
    },
    {
      "epoch": 9.175852251989197,
      "grad_norm": 3.6391074657440186,
      "learning_rate": 4.235345645667567e-05,
      "loss": 0.6689,
      "step": 1005600
    },
    {
      "epoch": 9.176764727352362,
      "grad_norm": 3.4406306743621826,
      "learning_rate": 4.2352696060539695e-05,
      "loss": 0.6881,
      "step": 1005700
    },
    {
      "epoch": 9.177677202715527,
      "grad_norm": 4.644636154174805,
      "learning_rate": 4.235193566440373e-05,
      "loss": 0.6764,
      "step": 1005800
    },
    {
      "epoch": 9.178589678078692,
      "grad_norm": 3.629600763320923,
      "learning_rate": 4.2351175268267756e-05,
      "loss": 0.6882,
      "step": 1005900
    },
    {
      "epoch": 9.179502153441858,
      "grad_norm": 3.802757501602173,
      "learning_rate": 4.2350414872131786e-05,
      "loss": 0.6703,
      "step": 1006000
    },
    {
      "epoch": 9.180414628805023,
      "grad_norm": 3.07163143157959,
      "learning_rate": 4.2349654475995816e-05,
      "loss": 0.6973,
      "step": 1006100
    },
    {
      "epoch": 9.181327104168188,
      "grad_norm": 4.426400184631348,
      "learning_rate": 4.2348894079859846e-05,
      "loss": 0.7035,
      "step": 1006200
    },
    {
      "epoch": 9.182239579531352,
      "grad_norm": 5.614645004272461,
      "learning_rate": 4.2348133683723876e-05,
      "loss": 0.6814,
      "step": 1006300
    },
    {
      "epoch": 9.183152054894517,
      "grad_norm": 4.170994281768799,
      "learning_rate": 4.2347373287587906e-05,
      "loss": 0.6926,
      "step": 1006400
    },
    {
      "epoch": 9.184064530257682,
      "grad_norm": 4.175393104553223,
      "learning_rate": 4.234661289145193e-05,
      "loss": 0.6878,
      "step": 1006500
    },
    {
      "epoch": 9.184977005620848,
      "grad_norm": 3.450143575668335,
      "learning_rate": 4.2345852495315966e-05,
      "loss": 0.692,
      "step": 1006600
    },
    {
      "epoch": 9.185889480984013,
      "grad_norm": 3.1755027770996094,
      "learning_rate": 4.234509209917999e-05,
      "loss": 0.6837,
      "step": 1006700
    },
    {
      "epoch": 9.186801956347178,
      "grad_norm": 4.06076192855835,
      "learning_rate": 4.234433170304402e-05,
      "loss": 0.6835,
      "step": 1006800
    },
    {
      "epoch": 9.187714431710344,
      "grad_norm": 3.4100589752197266,
      "learning_rate": 4.234357130690805e-05,
      "loss": 0.7032,
      "step": 1006900
    },
    {
      "epoch": 9.188626907073509,
      "grad_norm": 4.261612892150879,
      "learning_rate": 4.234281091077208e-05,
      "loss": 0.7164,
      "step": 1007000
    },
    {
      "epoch": 9.189539382436674,
      "grad_norm": 3.8950917720794678,
      "learning_rate": 4.23420505146361e-05,
      "loss": 0.6754,
      "step": 1007100
    },
    {
      "epoch": 9.19045185779984,
      "grad_norm": 3.179936647415161,
      "learning_rate": 4.234129011850014e-05,
      "loss": 0.6921,
      "step": 1007200
    },
    {
      "epoch": 9.191364333163005,
      "grad_norm": 3.08827805519104,
      "learning_rate": 4.234052972236416e-05,
      "loss": 0.6633,
      "step": 1007300
    },
    {
      "epoch": 9.19227680852617,
      "grad_norm": 4.570652961730957,
      "learning_rate": 4.233976932622819e-05,
      "loss": 0.7,
      "step": 1007400
    },
    {
      "epoch": 9.193189283889335,
      "grad_norm": 4.040396213531494,
      "learning_rate": 4.233900893009222e-05,
      "loss": 0.6867,
      "step": 1007500
    },
    {
      "epoch": 9.1941017592525,
      "grad_norm": 3.4311628341674805,
      "learning_rate": 4.233824853395625e-05,
      "loss": 0.7108,
      "step": 1007600
    },
    {
      "epoch": 9.195014234615666,
      "grad_norm": 4.841002941131592,
      "learning_rate": 4.233748813782028e-05,
      "loss": 0.6417,
      "step": 1007700
    },
    {
      "epoch": 9.195926709978831,
      "grad_norm": 2.9645678997039795,
      "learning_rate": 4.233672774168431e-05,
      "loss": 0.6965,
      "step": 1007800
    },
    {
      "epoch": 9.196839185341997,
      "grad_norm": 3.486936092376709,
      "learning_rate": 4.2335967345548337e-05,
      "loss": 0.6834,
      "step": 1007900
    },
    {
      "epoch": 9.19775166070516,
      "grad_norm": 3.477900743484497,
      "learning_rate": 4.2335206949412373e-05,
      "loss": 0.678,
      "step": 1008000
    },
    {
      "epoch": 9.198664136068325,
      "grad_norm": 4.0541605949401855,
      "learning_rate": 4.23344465532764e-05,
      "loss": 0.6641,
      "step": 1008100
    },
    {
      "epoch": 9.19957661143149,
      "grad_norm": 3.9712073802948,
      "learning_rate": 4.233368615714043e-05,
      "loss": 0.6776,
      "step": 1008200
    },
    {
      "epoch": 9.200489086794656,
      "grad_norm": 4.718518257141113,
      "learning_rate": 4.233292576100446e-05,
      "loss": 0.6823,
      "step": 1008300
    },
    {
      "epoch": 9.201401562157821,
      "grad_norm": 4.989162921905518,
      "learning_rate": 4.233216536486848e-05,
      "loss": 0.6806,
      "step": 1008400
    },
    {
      "epoch": 9.202314037520987,
      "grad_norm": 4.022651195526123,
      "learning_rate": 4.233140496873251e-05,
      "loss": 0.6841,
      "step": 1008500
    },
    {
      "epoch": 9.203226512884152,
      "grad_norm": 3.273362398147583,
      "learning_rate": 4.233064457259654e-05,
      "loss": 0.7221,
      "step": 1008600
    },
    {
      "epoch": 9.204138988247317,
      "grad_norm": 3.0761396884918213,
      "learning_rate": 4.232988417646057e-05,
      "loss": 0.7094,
      "step": 1008700
    },
    {
      "epoch": 9.205051463610483,
      "grad_norm": 3.5660104751586914,
      "learning_rate": 4.23291237803246e-05,
      "loss": 0.6798,
      "step": 1008800
    },
    {
      "epoch": 9.205963938973648,
      "grad_norm": 4.331326007843018,
      "learning_rate": 4.232836338418863e-05,
      "loss": 0.696,
      "step": 1008900
    },
    {
      "epoch": 9.206876414336813,
      "grad_norm": 3.1985578536987305,
      "learning_rate": 4.2327602988052654e-05,
      "loss": 0.7166,
      "step": 1009000
    },
    {
      "epoch": 9.207788889699978,
      "grad_norm": 4.516120910644531,
      "learning_rate": 4.232684259191669e-05,
      "loss": 0.694,
      "step": 1009100
    },
    {
      "epoch": 9.208701365063144,
      "grad_norm": 4.021616458892822,
      "learning_rate": 4.2326082195780714e-05,
      "loss": 0.6463,
      "step": 1009200
    },
    {
      "epoch": 9.209613840426309,
      "grad_norm": 3.9598968029022217,
      "learning_rate": 4.2325321799644744e-05,
      "loss": 0.6674,
      "step": 1009300
    },
    {
      "epoch": 9.210526315789474,
      "grad_norm": 3.989555835723877,
      "learning_rate": 4.2324561403508774e-05,
      "loss": 0.6669,
      "step": 1009400
    },
    {
      "epoch": 9.21143879115264,
      "grad_norm": 4.239436149597168,
      "learning_rate": 4.2323801007372804e-05,
      "loss": 0.6964,
      "step": 1009500
    },
    {
      "epoch": 9.212351266515805,
      "grad_norm": 4.134191036224365,
      "learning_rate": 4.232304061123683e-05,
      "loss": 0.6551,
      "step": 1009600
    },
    {
      "epoch": 9.213263741878968,
      "grad_norm": 4.292231559753418,
      "learning_rate": 4.2322280215100864e-05,
      "loss": 0.7104,
      "step": 1009700
    },
    {
      "epoch": 9.214176217242134,
      "grad_norm": 4.343762397766113,
      "learning_rate": 4.232151981896489e-05,
      "loss": 0.6656,
      "step": 1009800
    },
    {
      "epoch": 9.215088692605299,
      "grad_norm": 4.013726234436035,
      "learning_rate": 4.232075942282892e-05,
      "loss": 0.705,
      "step": 1009900
    },
    {
      "epoch": 9.216001167968464,
      "grad_norm": 3.3336939811706543,
      "learning_rate": 4.231999902669295e-05,
      "loss": 0.6989,
      "step": 1010000
    },
    {
      "epoch": 9.21691364333163,
      "grad_norm": 3.6037824153900146,
      "learning_rate": 4.231923863055698e-05,
      "loss": 0.6936,
      "step": 1010100
    },
    {
      "epoch": 9.217826118694795,
      "grad_norm": 4.106587886810303,
      "learning_rate": 4.231847823442101e-05,
      "loss": 0.6726,
      "step": 1010200
    },
    {
      "epoch": 9.21873859405796,
      "grad_norm": 3.5812506675720215,
      "learning_rate": 4.231771783828504e-05,
      "loss": 0.6919,
      "step": 1010300
    },
    {
      "epoch": 9.219651069421126,
      "grad_norm": 5.154552936553955,
      "learning_rate": 4.231695744214906e-05,
      "loss": 0.6825,
      "step": 1010400
    },
    {
      "epoch": 9.22056354478429,
      "grad_norm": 4.373902797698975,
      "learning_rate": 4.23161970460131e-05,
      "loss": 0.7127,
      "step": 1010500
    },
    {
      "epoch": 9.221476020147456,
      "grad_norm": 4.004782199859619,
      "learning_rate": 4.231543664987712e-05,
      "loss": 0.6588,
      "step": 1010600
    },
    {
      "epoch": 9.222388495510621,
      "grad_norm": 3.5797154903411865,
      "learning_rate": 4.231467625374115e-05,
      "loss": 0.6461,
      "step": 1010700
    },
    {
      "epoch": 9.223300970873787,
      "grad_norm": 5.386041164398193,
      "learning_rate": 4.231391585760518e-05,
      "loss": 0.657,
      "step": 1010800
    },
    {
      "epoch": 9.224213446236952,
      "grad_norm": 3.914717674255371,
      "learning_rate": 4.231315546146921e-05,
      "loss": 0.6744,
      "step": 1010900
    },
    {
      "epoch": 9.225125921600117,
      "grad_norm": 4.762959003448486,
      "learning_rate": 4.2312395065333235e-05,
      "loss": 0.6946,
      "step": 1011000
    },
    {
      "epoch": 9.226038396963283,
      "grad_norm": 4.522224426269531,
      "learning_rate": 4.2311634669197265e-05,
      "loss": 0.6864,
      "step": 1011100
    },
    {
      "epoch": 9.226950872326448,
      "grad_norm": 4.159684181213379,
      "learning_rate": 4.2310874273061295e-05,
      "loss": 0.6302,
      "step": 1011200
    },
    {
      "epoch": 9.227863347689613,
      "grad_norm": 2.8297996520996094,
      "learning_rate": 4.2310113876925325e-05,
      "loss": 0.7037,
      "step": 1011300
    },
    {
      "epoch": 9.228775823052777,
      "grad_norm": 3.6565046310424805,
      "learning_rate": 4.2309353480789355e-05,
      "loss": 0.6923,
      "step": 1011400
    },
    {
      "epoch": 9.229688298415942,
      "grad_norm": 5.061120510101318,
      "learning_rate": 4.230859308465338e-05,
      "loss": 0.698,
      "step": 1011500
    },
    {
      "epoch": 9.230600773779107,
      "grad_norm": 3.881319999694824,
      "learning_rate": 4.2307832688517415e-05,
      "loss": 0.6952,
      "step": 1011600
    },
    {
      "epoch": 9.231513249142273,
      "grad_norm": 3.6889965534210205,
      "learning_rate": 4.230707229238144e-05,
      "loss": 0.7111,
      "step": 1011700
    },
    {
      "epoch": 9.232425724505438,
      "grad_norm": 3.960726737976074,
      "learning_rate": 4.230631189624547e-05,
      "loss": 0.6849,
      "step": 1011800
    },
    {
      "epoch": 9.233338199868603,
      "grad_norm": 4.267967224121094,
      "learning_rate": 4.23055515001095e-05,
      "loss": 0.6691,
      "step": 1011900
    },
    {
      "epoch": 9.234250675231769,
      "grad_norm": 4.831088542938232,
      "learning_rate": 4.230479110397353e-05,
      "loss": 0.6941,
      "step": 1012000
    },
    {
      "epoch": 9.235163150594934,
      "grad_norm": 3.9191765785217285,
      "learning_rate": 4.230403070783755e-05,
      "loss": 0.6845,
      "step": 1012100
    },
    {
      "epoch": 9.2360756259581,
      "grad_norm": 2.54862904548645,
      "learning_rate": 4.230327031170159e-05,
      "loss": 0.6553,
      "step": 1012200
    },
    {
      "epoch": 9.236988101321264,
      "grad_norm": 4.1474223136901855,
      "learning_rate": 4.230250991556561e-05,
      "loss": 0.695,
      "step": 1012300
    },
    {
      "epoch": 9.23790057668443,
      "grad_norm": 3.6521244049072266,
      "learning_rate": 4.230174951942964e-05,
      "loss": 0.6729,
      "step": 1012400
    },
    {
      "epoch": 9.238813052047595,
      "grad_norm": 3.705174207687378,
      "learning_rate": 4.230098912329367e-05,
      "loss": 0.7103,
      "step": 1012500
    },
    {
      "epoch": 9.23972552741076,
      "grad_norm": 3.4640910625457764,
      "learning_rate": 4.23002287271577e-05,
      "loss": 0.6527,
      "step": 1012600
    },
    {
      "epoch": 9.240638002773926,
      "grad_norm": 4.0576252937316895,
      "learning_rate": 4.229946833102173e-05,
      "loss": 0.6689,
      "step": 1012700
    },
    {
      "epoch": 9.241550478137091,
      "grad_norm": 3.9828617572784424,
      "learning_rate": 4.229870793488576e-05,
      "loss": 0.7153,
      "step": 1012800
    },
    {
      "epoch": 9.242462953500256,
      "grad_norm": 4.265980243682861,
      "learning_rate": 4.2297947538749786e-05,
      "loss": 0.6857,
      "step": 1012900
    },
    {
      "epoch": 9.243375428863422,
      "grad_norm": 5.168938636779785,
      "learning_rate": 4.229718714261382e-05,
      "loss": 0.6814,
      "step": 1013000
    },
    {
      "epoch": 9.244287904226585,
      "grad_norm": 4.3421311378479,
      "learning_rate": 4.2296426746477846e-05,
      "loss": 0.6584,
      "step": 1013100
    },
    {
      "epoch": 9.24520037958975,
      "grad_norm": 3.791320562362671,
      "learning_rate": 4.2295666350341876e-05,
      "loss": 0.6543,
      "step": 1013200
    },
    {
      "epoch": 9.246112854952916,
      "grad_norm": 4.398874282836914,
      "learning_rate": 4.2294905954205906e-05,
      "loss": 0.7006,
      "step": 1013300
    },
    {
      "epoch": 9.247025330316081,
      "grad_norm": 5.315958023071289,
      "learning_rate": 4.2294145558069936e-05,
      "loss": 0.6976,
      "step": 1013400
    },
    {
      "epoch": 9.247937805679246,
      "grad_norm": 3.5351247787475586,
      "learning_rate": 4.229338516193396e-05,
      "loss": 0.6835,
      "step": 1013500
    },
    {
      "epoch": 9.248850281042412,
      "grad_norm": 3.931973695755005,
      "learning_rate": 4.2292624765797996e-05,
      "loss": 0.7401,
      "step": 1013600
    },
    {
      "epoch": 9.249762756405577,
      "grad_norm": 4.228905200958252,
      "learning_rate": 4.229186436966202e-05,
      "loss": 0.6801,
      "step": 1013700
    },
    {
      "epoch": 9.250675231768742,
      "grad_norm": 4.39605712890625,
      "learning_rate": 4.229110397352605e-05,
      "loss": 0.73,
      "step": 1013800
    },
    {
      "epoch": 9.251587707131907,
      "grad_norm": 2.8522212505340576,
      "learning_rate": 4.229034357739008e-05,
      "loss": 0.7078,
      "step": 1013900
    },
    {
      "epoch": 9.252500182495073,
      "grad_norm": 3.7691872119903564,
      "learning_rate": 4.22895831812541e-05,
      "loss": 0.7145,
      "step": 1014000
    },
    {
      "epoch": 9.253412657858238,
      "grad_norm": 4.155939102172852,
      "learning_rate": 4.228882278511814e-05,
      "loss": 0.6987,
      "step": 1014100
    },
    {
      "epoch": 9.254325133221403,
      "grad_norm": 3.6233208179473877,
      "learning_rate": 4.228806238898216e-05,
      "loss": 0.7325,
      "step": 1014200
    },
    {
      "epoch": 9.255237608584569,
      "grad_norm": 3.9881649017333984,
      "learning_rate": 4.228730199284619e-05,
      "loss": 0.6989,
      "step": 1014300
    },
    {
      "epoch": 9.256150083947734,
      "grad_norm": 4.387468338012695,
      "learning_rate": 4.228654159671022e-05,
      "loss": 0.67,
      "step": 1014400
    },
    {
      "epoch": 9.2570625593109,
      "grad_norm": 4.053160190582275,
      "learning_rate": 4.228578120057425e-05,
      "loss": 0.6961,
      "step": 1014500
    },
    {
      "epoch": 9.257975034674065,
      "grad_norm": 4.239496231079102,
      "learning_rate": 4.228502080443828e-05,
      "loss": 0.6848,
      "step": 1014600
    },
    {
      "epoch": 9.25888751003723,
      "grad_norm": 3.8580050468444824,
      "learning_rate": 4.2284260408302313e-05,
      "loss": 0.673,
      "step": 1014700
    },
    {
      "epoch": 9.259799985400393,
      "grad_norm": 4.1181488037109375,
      "learning_rate": 4.228350001216634e-05,
      "loss": 0.7052,
      "step": 1014800
    },
    {
      "epoch": 9.260712460763559,
      "grad_norm": 4.0159406661987305,
      "learning_rate": 4.2282739616030374e-05,
      "loss": 0.7026,
      "step": 1014900
    },
    {
      "epoch": 9.261624936126724,
      "grad_norm": 3.1607868671417236,
      "learning_rate": 4.22819792198944e-05,
      "loss": 0.6661,
      "step": 1015000
    },
    {
      "epoch": 9.26253741148989,
      "grad_norm": 3.965627908706665,
      "learning_rate": 4.228121882375843e-05,
      "loss": 0.6684,
      "step": 1015100
    },
    {
      "epoch": 9.263449886853055,
      "grad_norm": 3.9322593212127686,
      "learning_rate": 4.228045842762246e-05,
      "loss": 0.7445,
      "step": 1015200
    },
    {
      "epoch": 9.26436236221622,
      "grad_norm": 3.829540252685547,
      "learning_rate": 4.227969803148649e-05,
      "loss": 0.6862,
      "step": 1015300
    },
    {
      "epoch": 9.265274837579385,
      "grad_norm": 3.6224098205566406,
      "learning_rate": 4.227893763535051e-05,
      "loss": 0.6889,
      "step": 1015400
    },
    {
      "epoch": 9.26618731294255,
      "grad_norm": 5.514328956604004,
      "learning_rate": 4.227817723921455e-05,
      "loss": 0.7159,
      "step": 1015500
    },
    {
      "epoch": 9.267099788305716,
      "grad_norm": 2.558812379837036,
      "learning_rate": 4.227741684307857e-05,
      "loss": 0.7097,
      "step": 1015600
    },
    {
      "epoch": 9.268012263668881,
      "grad_norm": 4.051952838897705,
      "learning_rate": 4.22766564469426e-05,
      "loss": 0.6882,
      "step": 1015700
    },
    {
      "epoch": 9.268924739032046,
      "grad_norm": 3.9301822185516357,
      "learning_rate": 4.227589605080663e-05,
      "loss": 0.6652,
      "step": 1015800
    },
    {
      "epoch": 9.269837214395212,
      "grad_norm": 3.9218590259552,
      "learning_rate": 4.227513565467066e-05,
      "loss": 0.6877,
      "step": 1015900
    },
    {
      "epoch": 9.270749689758377,
      "grad_norm": 3.7582788467407227,
      "learning_rate": 4.227437525853469e-05,
      "loss": 0.689,
      "step": 1016000
    },
    {
      "epoch": 9.271662165121542,
      "grad_norm": 4.520484447479248,
      "learning_rate": 4.227361486239872e-05,
      "loss": 0.6961,
      "step": 1016100
    },
    {
      "epoch": 9.272574640484708,
      "grad_norm": 3.2525827884674072,
      "learning_rate": 4.2272854466262744e-05,
      "loss": 0.6392,
      "step": 1016200
    },
    {
      "epoch": 9.273487115847873,
      "grad_norm": 4.711410045623779,
      "learning_rate": 4.227209407012678e-05,
      "loss": 0.6889,
      "step": 1016300
    },
    {
      "epoch": 9.274399591211036,
      "grad_norm": 4.287329196929932,
      "learning_rate": 4.2271333673990804e-05,
      "loss": 0.714,
      "step": 1016400
    },
    {
      "epoch": 9.275312066574202,
      "grad_norm": 3.231738328933716,
      "learning_rate": 4.2270573277854834e-05,
      "loss": 0.6499,
      "step": 1016500
    },
    {
      "epoch": 9.276224541937367,
      "grad_norm": 6.4894914627075195,
      "learning_rate": 4.2269812881718864e-05,
      "loss": 0.6889,
      "step": 1016600
    },
    {
      "epoch": 9.277137017300532,
      "grad_norm": 4.37999963760376,
      "learning_rate": 4.2269052485582894e-05,
      "loss": 0.6896,
      "step": 1016700
    },
    {
      "epoch": 9.278049492663698,
      "grad_norm": 4.245206356048584,
      "learning_rate": 4.226829208944692e-05,
      "loss": 0.6912,
      "step": 1016800
    },
    {
      "epoch": 9.278961968026863,
      "grad_norm": 4.129788398742676,
      "learning_rate": 4.226753169331095e-05,
      "loss": 0.6612,
      "step": 1016900
    },
    {
      "epoch": 9.279874443390028,
      "grad_norm": 4.833460807800293,
      "learning_rate": 4.226677129717498e-05,
      "loss": 0.6362,
      "step": 1017000
    },
    {
      "epoch": 9.280786918753194,
      "grad_norm": 3.9440596103668213,
      "learning_rate": 4.226601090103901e-05,
      "loss": 0.6664,
      "step": 1017100
    },
    {
      "epoch": 9.281699394116359,
      "grad_norm": 3.274395227432251,
      "learning_rate": 4.226525050490304e-05,
      "loss": 0.6995,
      "step": 1017200
    },
    {
      "epoch": 9.282611869479524,
      "grad_norm": 4.039734840393066,
      "learning_rate": 4.226449010876706e-05,
      "loss": 0.676,
      "step": 1017300
    },
    {
      "epoch": 9.28352434484269,
      "grad_norm": 4.390150547027588,
      "learning_rate": 4.22637297126311e-05,
      "loss": 0.6856,
      "step": 1017400
    },
    {
      "epoch": 9.284436820205855,
      "grad_norm": 3.6353976726531982,
      "learning_rate": 4.226296931649512e-05,
      "loss": 0.6537,
      "step": 1017500
    },
    {
      "epoch": 9.28534929556902,
      "grad_norm": 4.269376754760742,
      "learning_rate": 4.226220892035915e-05,
      "loss": 0.7018,
      "step": 1017600
    },
    {
      "epoch": 9.286261770932185,
      "grad_norm": 4.020158767700195,
      "learning_rate": 4.226144852422318e-05,
      "loss": 0.6917,
      "step": 1017700
    },
    {
      "epoch": 9.28717424629535,
      "grad_norm": 3.552337169647217,
      "learning_rate": 4.226068812808721e-05,
      "loss": 0.7003,
      "step": 1017800
    },
    {
      "epoch": 9.288086721658516,
      "grad_norm": 4.80539083480835,
      "learning_rate": 4.2259927731951235e-05,
      "loss": 0.6817,
      "step": 1017900
    },
    {
      "epoch": 9.288999197021681,
      "grad_norm": 3.7126855850219727,
      "learning_rate": 4.225916733581527e-05,
      "loss": 0.6901,
      "step": 1018000
    },
    {
      "epoch": 9.289911672384846,
      "grad_norm": 3.564615249633789,
      "learning_rate": 4.2258406939679295e-05,
      "loss": 0.6722,
      "step": 1018100
    },
    {
      "epoch": 9.29082414774801,
      "grad_norm": 3.475800037384033,
      "learning_rate": 4.2257646543543325e-05,
      "loss": 0.6456,
      "step": 1018200
    },
    {
      "epoch": 9.291736623111175,
      "grad_norm": 4.61053466796875,
      "learning_rate": 4.2256886147407355e-05,
      "loss": 0.6818,
      "step": 1018300
    },
    {
      "epoch": 9.29264909847434,
      "grad_norm": 4.546444416046143,
      "learning_rate": 4.2256125751271385e-05,
      "loss": 0.6927,
      "step": 1018400
    },
    {
      "epoch": 9.293561573837506,
      "grad_norm": 3.8891592025756836,
      "learning_rate": 4.2255365355135415e-05,
      "loss": 0.6872,
      "step": 1018500
    },
    {
      "epoch": 9.294474049200671,
      "grad_norm": 4.825060844421387,
      "learning_rate": 4.2254604958999445e-05,
      "loss": 0.6995,
      "step": 1018600
    },
    {
      "epoch": 9.295386524563837,
      "grad_norm": 3.5443131923675537,
      "learning_rate": 4.225384456286347e-05,
      "loss": 0.6921,
      "step": 1018700
    },
    {
      "epoch": 9.296298999927002,
      "grad_norm": 3.7513833045959473,
      "learning_rate": 4.2253084166727505e-05,
      "loss": 0.6888,
      "step": 1018800
    },
    {
      "epoch": 9.297211475290167,
      "grad_norm": 3.952662467956543,
      "learning_rate": 4.225232377059153e-05,
      "loss": 0.6909,
      "step": 1018900
    },
    {
      "epoch": 9.298123950653332,
      "grad_norm": 4.286088943481445,
      "learning_rate": 4.225156337445556e-05,
      "loss": 0.6722,
      "step": 1019000
    },
    {
      "epoch": 9.299036426016498,
      "grad_norm": 3.9760892391204834,
      "learning_rate": 4.225080297831959e-05,
      "loss": 0.7006,
      "step": 1019100
    },
    {
      "epoch": 9.299948901379663,
      "grad_norm": 4.208019733428955,
      "learning_rate": 4.225004258218362e-05,
      "loss": 0.7083,
      "step": 1019200
    },
    {
      "epoch": 9.300861376742828,
      "grad_norm": 3.7811970710754395,
      "learning_rate": 4.224928218604764e-05,
      "loss": 0.6995,
      "step": 1019300
    },
    {
      "epoch": 9.301773852105994,
      "grad_norm": 4.674227714538574,
      "learning_rate": 4.224852178991168e-05,
      "loss": 0.6656,
      "step": 1019400
    },
    {
      "epoch": 9.302686327469159,
      "grad_norm": 4.22041130065918,
      "learning_rate": 4.22477613937757e-05,
      "loss": 0.6891,
      "step": 1019500
    },
    {
      "epoch": 9.303598802832324,
      "grad_norm": 4.474923133850098,
      "learning_rate": 4.224700099763973e-05,
      "loss": 0.7189,
      "step": 1019600
    },
    {
      "epoch": 9.30451127819549,
      "grad_norm": 4.010810852050781,
      "learning_rate": 4.224624060150376e-05,
      "loss": 0.6887,
      "step": 1019700
    },
    {
      "epoch": 9.305423753558653,
      "grad_norm": 4.436617851257324,
      "learning_rate": 4.2245480205367786e-05,
      "loss": 0.708,
      "step": 1019800
    },
    {
      "epoch": 9.306336228921818,
      "grad_norm": 4.429542541503906,
      "learning_rate": 4.224471980923182e-05,
      "loss": 0.6857,
      "step": 1019900
    },
    {
      "epoch": 9.307248704284984,
      "grad_norm": 3.8179898262023926,
      "learning_rate": 4.2243959413095846e-05,
      "loss": 0.6768,
      "step": 1020000
    },
    {
      "epoch": 9.308161179648149,
      "grad_norm": 3.9111714363098145,
      "learning_rate": 4.2243199016959876e-05,
      "loss": 0.7085,
      "step": 1020100
    },
    {
      "epoch": 9.309073655011314,
      "grad_norm": 4.15036678314209,
      "learning_rate": 4.2242438620823906e-05,
      "loss": 0.7078,
      "step": 1020200
    },
    {
      "epoch": 9.30998613037448,
      "grad_norm": 3.772925615310669,
      "learning_rate": 4.2241678224687936e-05,
      "loss": 0.7185,
      "step": 1020300
    },
    {
      "epoch": 9.310898605737645,
      "grad_norm": 4.021751403808594,
      "learning_rate": 4.224091782855196e-05,
      "loss": 0.6623,
      "step": 1020400
    },
    {
      "epoch": 9.31181108110081,
      "grad_norm": 1.6022757291793823,
      "learning_rate": 4.2240157432415996e-05,
      "loss": 0.7244,
      "step": 1020500
    },
    {
      "epoch": 9.312723556463975,
      "grad_norm": 3.8309433460235596,
      "learning_rate": 4.223939703628002e-05,
      "loss": 0.6819,
      "step": 1020600
    },
    {
      "epoch": 9.31363603182714,
      "grad_norm": 3.2187881469726562,
      "learning_rate": 4.223863664014405e-05,
      "loss": 0.6433,
      "step": 1020700
    },
    {
      "epoch": 9.314548507190306,
      "grad_norm": 4.343905925750732,
      "learning_rate": 4.223787624400808e-05,
      "loss": 0.6633,
      "step": 1020800
    },
    {
      "epoch": 9.315460982553471,
      "grad_norm": 2.7611773014068604,
      "learning_rate": 4.223711584787211e-05,
      "loss": 0.6921,
      "step": 1020900
    },
    {
      "epoch": 9.316373457916637,
      "grad_norm": 3.793440341949463,
      "learning_rate": 4.223635545173614e-05,
      "loss": 0.6561,
      "step": 1021000
    },
    {
      "epoch": 9.317285933279802,
      "grad_norm": 4.58931827545166,
      "learning_rate": 4.223559505560017e-05,
      "loss": 0.6809,
      "step": 1021100
    },
    {
      "epoch": 9.318198408642967,
      "grad_norm": 3.9630322456359863,
      "learning_rate": 4.223483465946419e-05,
      "loss": 0.6747,
      "step": 1021200
    },
    {
      "epoch": 9.319110884006133,
      "grad_norm": 4.758917808532715,
      "learning_rate": 4.223407426332823e-05,
      "loss": 0.6808,
      "step": 1021300
    },
    {
      "epoch": 9.320023359369298,
      "grad_norm": 3.2800397872924805,
      "learning_rate": 4.223331386719225e-05,
      "loss": 0.6832,
      "step": 1021400
    },
    {
      "epoch": 9.320935834732463,
      "grad_norm": 4.199540138244629,
      "learning_rate": 4.2232553471056283e-05,
      "loss": 0.6776,
      "step": 1021500
    },
    {
      "epoch": 9.321848310095627,
      "grad_norm": 3.8151707649230957,
      "learning_rate": 4.2231793074920313e-05,
      "loss": 0.6671,
      "step": 1021600
    },
    {
      "epoch": 9.322760785458792,
      "grad_norm": 3.8260858058929443,
      "learning_rate": 4.2231032678784344e-05,
      "loss": 0.7061,
      "step": 1021700
    },
    {
      "epoch": 9.323673260821957,
      "grad_norm": 3.62874698638916,
      "learning_rate": 4.223027228264837e-05,
      "loss": 0.6931,
      "step": 1021800
    },
    {
      "epoch": 9.324585736185123,
      "grad_norm": 4.170671463012695,
      "learning_rate": 4.2229511886512404e-05,
      "loss": 0.7058,
      "step": 1021900
    },
    {
      "epoch": 9.325498211548288,
      "grad_norm": 4.644150257110596,
      "learning_rate": 4.222875149037643e-05,
      "loss": 0.6988,
      "step": 1022000
    },
    {
      "epoch": 9.326410686911453,
      "grad_norm": 4.101475238800049,
      "learning_rate": 4.222799109424046e-05,
      "loss": 0.664,
      "step": 1022100
    },
    {
      "epoch": 9.327323162274618,
      "grad_norm": 4.035585880279541,
      "learning_rate": 4.222723069810449e-05,
      "loss": 0.6541,
      "step": 1022200
    },
    {
      "epoch": 9.328235637637784,
      "grad_norm": 4.317383289337158,
      "learning_rate": 4.222647030196852e-05,
      "loss": 0.6994,
      "step": 1022300
    },
    {
      "epoch": 9.329148113000949,
      "grad_norm": 2.189788818359375,
      "learning_rate": 4.222570990583255e-05,
      "loss": 0.705,
      "step": 1022400
    },
    {
      "epoch": 9.330060588364114,
      "grad_norm": 4.471169471740723,
      "learning_rate": 4.222494950969657e-05,
      "loss": 0.6862,
      "step": 1022500
    },
    {
      "epoch": 9.33097306372728,
      "grad_norm": 3.884019374847412,
      "learning_rate": 4.22241891135606e-05,
      "loss": 0.6907,
      "step": 1022600
    },
    {
      "epoch": 9.331885539090445,
      "grad_norm": 4.912066459655762,
      "learning_rate": 4.222342871742463e-05,
      "loss": 0.681,
      "step": 1022700
    },
    {
      "epoch": 9.33279801445361,
      "grad_norm": 4.144530296325684,
      "learning_rate": 4.222266832128866e-05,
      "loss": 0.704,
      "step": 1022800
    },
    {
      "epoch": 9.333710489816776,
      "grad_norm": 4.401227951049805,
      "learning_rate": 4.2221907925152684e-05,
      "loss": 0.65,
      "step": 1022900
    },
    {
      "epoch": 9.33462296517994,
      "grad_norm": 3.0993239879608154,
      "learning_rate": 4.222114752901672e-05,
      "loss": 0.7153,
      "step": 1023000
    },
    {
      "epoch": 9.335535440543106,
      "grad_norm": 3.876152515411377,
      "learning_rate": 4.2220387132880744e-05,
      "loss": 0.745,
      "step": 1023100
    },
    {
      "epoch": 9.33644791590627,
      "grad_norm": 4.030261993408203,
      "learning_rate": 4.2219626736744774e-05,
      "loss": 0.7046,
      "step": 1023200
    },
    {
      "epoch": 9.337360391269435,
      "grad_norm": 4.547581672668457,
      "learning_rate": 4.2218866340608804e-05,
      "loss": 0.6727,
      "step": 1023300
    },
    {
      "epoch": 9.3382728666326,
      "grad_norm": 3.5449070930480957,
      "learning_rate": 4.2218105944472834e-05,
      "loss": 0.711,
      "step": 1023400
    },
    {
      "epoch": 9.339185341995766,
      "grad_norm": 3.7230823040008545,
      "learning_rate": 4.2217345548336864e-05,
      "loss": 0.6717,
      "step": 1023500
    },
    {
      "epoch": 9.34009781735893,
      "grad_norm": 3.7901601791381836,
      "learning_rate": 4.2216585152200895e-05,
      "loss": 0.696,
      "step": 1023600
    },
    {
      "epoch": 9.341010292722096,
      "grad_norm": 3.189918279647827,
      "learning_rate": 4.221582475606492e-05,
      "loss": 0.7038,
      "step": 1023700
    },
    {
      "epoch": 9.341922768085261,
      "grad_norm": 3.8802945613861084,
      "learning_rate": 4.2215064359928955e-05,
      "loss": 0.6498,
      "step": 1023800
    },
    {
      "epoch": 9.342835243448427,
      "grad_norm": 4.451180934906006,
      "learning_rate": 4.221430396379298e-05,
      "loss": 0.6603,
      "step": 1023900
    },
    {
      "epoch": 9.343747718811592,
      "grad_norm": 3.2270004749298096,
      "learning_rate": 4.221354356765701e-05,
      "loss": 0.674,
      "step": 1024000
    },
    {
      "epoch": 9.344660194174757,
      "grad_norm": 4.3284010887146,
      "learning_rate": 4.221278317152104e-05,
      "loss": 0.6838,
      "step": 1024100
    },
    {
      "epoch": 9.345572669537923,
      "grad_norm": 3.5581772327423096,
      "learning_rate": 4.221202277538507e-05,
      "loss": 0.7421,
      "step": 1024200
    },
    {
      "epoch": 9.346485144901088,
      "grad_norm": 4.1267499923706055,
      "learning_rate": 4.221126237924909e-05,
      "loss": 0.6551,
      "step": 1024300
    },
    {
      "epoch": 9.347397620264253,
      "grad_norm": 2.890957832336426,
      "learning_rate": 4.221050198311313e-05,
      "loss": 0.7348,
      "step": 1024400
    },
    {
      "epoch": 9.348310095627419,
      "grad_norm": 4.641096591949463,
      "learning_rate": 4.220974158697715e-05,
      "loss": 0.7141,
      "step": 1024500
    },
    {
      "epoch": 9.349222570990584,
      "grad_norm": 4.120054721832275,
      "learning_rate": 4.220898119084118e-05,
      "loss": 0.7042,
      "step": 1024600
    },
    {
      "epoch": 9.35013504635375,
      "grad_norm": 4.058304309844971,
      "learning_rate": 4.220822079470521e-05,
      "loss": 0.661,
      "step": 1024700
    },
    {
      "epoch": 9.351047521716914,
      "grad_norm": 3.3362340927124023,
      "learning_rate": 4.220746039856924e-05,
      "loss": 0.6515,
      "step": 1024800
    },
    {
      "epoch": 9.35195999708008,
      "grad_norm": 4.1841020584106445,
      "learning_rate": 4.220670000243327e-05,
      "loss": 0.6884,
      "step": 1024900
    },
    {
      "epoch": 9.352872472443243,
      "grad_norm": 3.045286178588867,
      "learning_rate": 4.22059396062973e-05,
      "loss": 0.6843,
      "step": 1025000
    },
    {
      "epoch": 9.353784947806409,
      "grad_norm": 4.516927242279053,
      "learning_rate": 4.2205179210161325e-05,
      "loss": 0.6692,
      "step": 1025100
    },
    {
      "epoch": 9.354697423169574,
      "grad_norm": 3.623349666595459,
      "learning_rate": 4.220441881402536e-05,
      "loss": 0.6926,
      "step": 1025200
    },
    {
      "epoch": 9.35560989853274,
      "grad_norm": 4.340774059295654,
      "learning_rate": 4.2203658417889385e-05,
      "loss": 0.6815,
      "step": 1025300
    },
    {
      "epoch": 9.356522373895904,
      "grad_norm": 3.7029523849487305,
      "learning_rate": 4.2202898021753415e-05,
      "loss": 0.7011,
      "step": 1025400
    },
    {
      "epoch": 9.35743484925907,
      "grad_norm": 4.562255382537842,
      "learning_rate": 4.2202137625617445e-05,
      "loss": 0.6786,
      "step": 1025500
    },
    {
      "epoch": 9.358347324622235,
      "grad_norm": 4.227823257446289,
      "learning_rate": 4.220137722948147e-05,
      "loss": 0.6734,
      "step": 1025600
    },
    {
      "epoch": 9.3592597999854,
      "grad_norm": 3.775744676589966,
      "learning_rate": 4.22006168333455e-05,
      "loss": 0.722,
      "step": 1025700
    },
    {
      "epoch": 9.360172275348566,
      "grad_norm": 4.218776226043701,
      "learning_rate": 4.219985643720953e-05,
      "loss": 0.6839,
      "step": 1025800
    },
    {
      "epoch": 9.361084750711731,
      "grad_norm": 4.53493595123291,
      "learning_rate": 4.219909604107356e-05,
      "loss": 0.7132,
      "step": 1025900
    },
    {
      "epoch": 9.361997226074896,
      "grad_norm": 3.9366002082824707,
      "learning_rate": 4.219833564493759e-05,
      "loss": 0.6819,
      "step": 1026000
    },
    {
      "epoch": 9.362909701438062,
      "grad_norm": 3.505176305770874,
      "learning_rate": 4.219757524880162e-05,
      "loss": 0.6775,
      "step": 1026100
    },
    {
      "epoch": 9.363822176801227,
      "grad_norm": 4.581604957580566,
      "learning_rate": 4.219681485266564e-05,
      "loss": 0.6779,
      "step": 1026200
    },
    {
      "epoch": 9.364734652164392,
      "grad_norm": 3.737510919570923,
      "learning_rate": 4.219605445652968e-05,
      "loss": 0.6864,
      "step": 1026300
    },
    {
      "epoch": 9.365647127527557,
      "grad_norm": 3.1642637252807617,
      "learning_rate": 4.21952940603937e-05,
      "loss": 0.6776,
      "step": 1026400
    },
    {
      "epoch": 9.366559602890723,
      "grad_norm": 4.4910359382629395,
      "learning_rate": 4.219453366425773e-05,
      "loss": 0.6879,
      "step": 1026500
    },
    {
      "epoch": 9.367472078253886,
      "grad_norm": 3.2121098041534424,
      "learning_rate": 4.219377326812176e-05,
      "loss": 0.7223,
      "step": 1026600
    },
    {
      "epoch": 9.368384553617052,
      "grad_norm": 3.8942413330078125,
      "learning_rate": 4.219301287198579e-05,
      "loss": 0.6805,
      "step": 1026700
    },
    {
      "epoch": 9.369297028980217,
      "grad_norm": 3.969207286834717,
      "learning_rate": 4.219225247584982e-05,
      "loss": 0.6882,
      "step": 1026800
    },
    {
      "epoch": 9.370209504343382,
      "grad_norm": 3.6383981704711914,
      "learning_rate": 4.219149207971385e-05,
      "loss": 0.678,
      "step": 1026900
    },
    {
      "epoch": 9.371121979706547,
      "grad_norm": 3.969862937927246,
      "learning_rate": 4.2190731683577876e-05,
      "loss": 0.656,
      "step": 1027000
    },
    {
      "epoch": 9.372034455069713,
      "grad_norm": 3.0815067291259766,
      "learning_rate": 4.2189971287441906e-05,
      "loss": 0.7051,
      "step": 1027100
    },
    {
      "epoch": 9.372946930432878,
      "grad_norm": 4.727599143981934,
      "learning_rate": 4.2189210891305936e-05,
      "loss": 0.6994,
      "step": 1027200
    },
    {
      "epoch": 9.373859405796043,
      "grad_norm": 3.633721113204956,
      "learning_rate": 4.2188450495169966e-05,
      "loss": 0.6947,
      "step": 1027300
    },
    {
      "epoch": 9.374771881159209,
      "grad_norm": 5.670085430145264,
      "learning_rate": 4.2187690099033996e-05,
      "loss": 0.672,
      "step": 1027400
    },
    {
      "epoch": 9.375684356522374,
      "grad_norm": 4.109684467315674,
      "learning_rate": 4.2186929702898026e-05,
      "loss": 0.7012,
      "step": 1027500
    },
    {
      "epoch": 9.37659683188554,
      "grad_norm": 4.187488555908203,
      "learning_rate": 4.218616930676205e-05,
      "loss": 0.7111,
      "step": 1027600
    },
    {
      "epoch": 9.377509307248705,
      "grad_norm": 4.396571636199951,
      "learning_rate": 4.218540891062609e-05,
      "loss": 0.7067,
      "step": 1027700
    },
    {
      "epoch": 9.37842178261187,
      "grad_norm": 4.36080265045166,
      "learning_rate": 4.218464851449011e-05,
      "loss": 0.664,
      "step": 1027800
    },
    {
      "epoch": 9.379334257975035,
      "grad_norm": 3.936532735824585,
      "learning_rate": 4.218388811835414e-05,
      "loss": 0.668,
      "step": 1027900
    },
    {
      "epoch": 9.3802467333382,
      "grad_norm": 3.9691689014434814,
      "learning_rate": 4.218312772221817e-05,
      "loss": 0.6864,
      "step": 1028000
    },
    {
      "epoch": 9.381159208701366,
      "grad_norm": 3.563363790512085,
      "learning_rate": 4.21823673260822e-05,
      "loss": 0.6853,
      "step": 1028100
    },
    {
      "epoch": 9.382071684064531,
      "grad_norm": 3.337952136993408,
      "learning_rate": 4.218160692994623e-05,
      "loss": 0.6751,
      "step": 1028200
    },
    {
      "epoch": 9.382984159427695,
      "grad_norm": 3.7961535453796387,
      "learning_rate": 4.2180846533810253e-05,
      "loss": 0.7042,
      "step": 1028300
    },
    {
      "epoch": 9.38389663479086,
      "grad_norm": 3.434385299682617,
      "learning_rate": 4.2180086137674284e-05,
      "loss": 0.7172,
      "step": 1028400
    },
    {
      "epoch": 9.384809110154025,
      "grad_norm": 2.9598658084869385,
      "learning_rate": 4.2179325741538314e-05,
      "loss": 0.6887,
      "step": 1028500
    },
    {
      "epoch": 9.38572158551719,
      "grad_norm": 3.767016649246216,
      "learning_rate": 4.2178565345402344e-05,
      "loss": 0.6579,
      "step": 1028600
    },
    {
      "epoch": 9.386634060880356,
      "grad_norm": 4.5087127685546875,
      "learning_rate": 4.217780494926637e-05,
      "loss": 0.6848,
      "step": 1028700
    },
    {
      "epoch": 9.387546536243521,
      "grad_norm": 4.655892372131348,
      "learning_rate": 4.2177044553130404e-05,
      "loss": 0.6778,
      "step": 1028800
    },
    {
      "epoch": 9.388459011606686,
      "grad_norm": 4.468744277954102,
      "learning_rate": 4.217628415699443e-05,
      "loss": 0.6812,
      "step": 1028900
    },
    {
      "epoch": 9.389371486969852,
      "grad_norm": 3.947286605834961,
      "learning_rate": 4.217552376085846e-05,
      "loss": 0.6779,
      "step": 1029000
    },
    {
      "epoch": 9.390283962333017,
      "grad_norm": 5.115045070648193,
      "learning_rate": 4.217476336472249e-05,
      "loss": 0.7053,
      "step": 1029100
    },
    {
      "epoch": 9.391196437696182,
      "grad_norm": 5.429532527923584,
      "learning_rate": 4.217400296858652e-05,
      "loss": 0.693,
      "step": 1029200
    },
    {
      "epoch": 9.392108913059348,
      "grad_norm": 3.999176502227783,
      "learning_rate": 4.217324257245055e-05,
      "loss": 0.7067,
      "step": 1029300
    },
    {
      "epoch": 9.393021388422513,
      "grad_norm": 3.5333006381988525,
      "learning_rate": 4.217248217631458e-05,
      "loss": 0.6554,
      "step": 1029400
    },
    {
      "epoch": 9.393933863785678,
      "grad_norm": 4.387144088745117,
      "learning_rate": 4.21717217801786e-05,
      "loss": 0.6912,
      "step": 1029500
    },
    {
      "epoch": 9.394846339148843,
      "grad_norm": 4.144079685211182,
      "learning_rate": 4.217096138404264e-05,
      "loss": 0.7165,
      "step": 1029600
    },
    {
      "epoch": 9.395758814512009,
      "grad_norm": 4.352203845977783,
      "learning_rate": 4.217020098790666e-05,
      "loss": 0.71,
      "step": 1029700
    },
    {
      "epoch": 9.396671289875174,
      "grad_norm": 4.380428314208984,
      "learning_rate": 4.216944059177069e-05,
      "loss": 0.6347,
      "step": 1029800
    },
    {
      "epoch": 9.39758376523834,
      "grad_norm": 5.298384189605713,
      "learning_rate": 4.216868019563472e-05,
      "loss": 0.686,
      "step": 1029900
    },
    {
      "epoch": 9.398496240601503,
      "grad_norm": 3.334346055984497,
      "learning_rate": 4.216791979949875e-05,
      "loss": 0.6631,
      "step": 1030000
    },
    {
      "epoch": 9.399408715964668,
      "grad_norm": 3.9900102615356445,
      "learning_rate": 4.2167159403362774e-05,
      "loss": 0.7124,
      "step": 1030100
    },
    {
      "epoch": 9.400321191327834,
      "grad_norm": 4.688041687011719,
      "learning_rate": 4.216639900722681e-05,
      "loss": 0.7027,
      "step": 1030200
    },
    {
      "epoch": 9.401233666690999,
      "grad_norm": 3.9362540245056152,
      "learning_rate": 4.2165638611090834e-05,
      "loss": 0.6971,
      "step": 1030300
    },
    {
      "epoch": 9.402146142054164,
      "grad_norm": 3.387500047683716,
      "learning_rate": 4.2164878214954865e-05,
      "loss": 0.6748,
      "step": 1030400
    },
    {
      "epoch": 9.40305861741733,
      "grad_norm": 3.3872947692871094,
      "learning_rate": 4.2164117818818895e-05,
      "loss": 0.7142,
      "step": 1030500
    },
    {
      "epoch": 9.403971092780495,
      "grad_norm": 4.196599006652832,
      "learning_rate": 4.2163357422682925e-05,
      "loss": 0.652,
      "step": 1030600
    },
    {
      "epoch": 9.40488356814366,
      "grad_norm": 3.8548102378845215,
      "learning_rate": 4.2162597026546955e-05,
      "loss": 0.6756,
      "step": 1030700
    },
    {
      "epoch": 9.405796043506825,
      "grad_norm": 3.7686855792999268,
      "learning_rate": 4.2161836630410985e-05,
      "loss": 0.7112,
      "step": 1030800
    },
    {
      "epoch": 9.40670851886999,
      "grad_norm": 4.275586128234863,
      "learning_rate": 4.216107623427501e-05,
      "loss": 0.6381,
      "step": 1030900
    },
    {
      "epoch": 9.407620994233156,
      "grad_norm": 4.166195392608643,
      "learning_rate": 4.216031583813904e-05,
      "loss": 0.7134,
      "step": 1031000
    },
    {
      "epoch": 9.408533469596321,
      "grad_norm": 3.7396562099456787,
      "learning_rate": 4.215955544200307e-05,
      "loss": 0.7099,
      "step": 1031100
    },
    {
      "epoch": 9.409445944959486,
      "grad_norm": 3.7397916316986084,
      "learning_rate": 4.215879504586709e-05,
      "loss": 0.6622,
      "step": 1031200
    },
    {
      "epoch": 9.410358420322652,
      "grad_norm": 4.1400465965271,
      "learning_rate": 4.215803464973113e-05,
      "loss": 0.6799,
      "step": 1031300
    },
    {
      "epoch": 9.411270895685817,
      "grad_norm": 4.387306213378906,
      "learning_rate": 4.215727425359515e-05,
      "loss": 0.6862,
      "step": 1031400
    },
    {
      "epoch": 9.412183371048982,
      "grad_norm": 2.917870283126831,
      "learning_rate": 4.215651385745918e-05,
      "loss": 0.6977,
      "step": 1031500
    },
    {
      "epoch": 9.413095846412148,
      "grad_norm": 4.672044277191162,
      "learning_rate": 4.215575346132321e-05,
      "loss": 0.7276,
      "step": 1031600
    },
    {
      "epoch": 9.414008321775311,
      "grad_norm": 3.342397689819336,
      "learning_rate": 4.215499306518724e-05,
      "loss": 0.6886,
      "step": 1031700
    },
    {
      "epoch": 9.414920797138477,
      "grad_norm": 3.654146432876587,
      "learning_rate": 4.215423266905127e-05,
      "loss": 0.7382,
      "step": 1031800
    },
    {
      "epoch": 9.415833272501642,
      "grad_norm": 4.2851057052612305,
      "learning_rate": 4.21534722729153e-05,
      "loss": 0.7302,
      "step": 1031900
    },
    {
      "epoch": 9.416745747864807,
      "grad_norm": 3.5406501293182373,
      "learning_rate": 4.2152711876779325e-05,
      "loss": 0.7203,
      "step": 1032000
    },
    {
      "epoch": 9.417658223227972,
      "grad_norm": 3.538540840148926,
      "learning_rate": 4.215195148064336e-05,
      "loss": 0.6657,
      "step": 1032100
    },
    {
      "epoch": 9.418570698591138,
      "grad_norm": 4.5822672843933105,
      "learning_rate": 4.2151191084507385e-05,
      "loss": 0.6771,
      "step": 1032200
    },
    {
      "epoch": 9.419483173954303,
      "grad_norm": 3.3681259155273438,
      "learning_rate": 4.2150430688371415e-05,
      "loss": 0.6735,
      "step": 1032300
    },
    {
      "epoch": 9.420395649317468,
      "grad_norm": 4.465587139129639,
      "learning_rate": 4.2149670292235446e-05,
      "loss": 0.7025,
      "step": 1032400
    },
    {
      "epoch": 9.421308124680634,
      "grad_norm": 3.9485673904418945,
      "learning_rate": 4.2148909896099476e-05,
      "loss": 0.6778,
      "step": 1032500
    },
    {
      "epoch": 9.422220600043799,
      "grad_norm": 4.185150623321533,
      "learning_rate": 4.21481494999635e-05,
      "loss": 0.6952,
      "step": 1032600
    },
    {
      "epoch": 9.423133075406964,
      "grad_norm": 3.4424149990081787,
      "learning_rate": 4.2147389103827536e-05,
      "loss": 0.659,
      "step": 1032700
    },
    {
      "epoch": 9.42404555077013,
      "grad_norm": 4.3244709968566895,
      "learning_rate": 4.214662870769156e-05,
      "loss": 0.703,
      "step": 1032800
    },
    {
      "epoch": 9.424958026133295,
      "grad_norm": 3.868161916732788,
      "learning_rate": 4.214586831155559e-05,
      "loss": 0.6987,
      "step": 1032900
    },
    {
      "epoch": 9.42587050149646,
      "grad_norm": 4.465963840484619,
      "learning_rate": 4.214510791541962e-05,
      "loss": 0.6754,
      "step": 1033000
    },
    {
      "epoch": 9.426782976859625,
      "grad_norm": 4.142523288726807,
      "learning_rate": 4.214434751928365e-05,
      "loss": 0.6757,
      "step": 1033100
    },
    {
      "epoch": 9.42769545222279,
      "grad_norm": 4.112814903259277,
      "learning_rate": 4.214358712314768e-05,
      "loss": 0.6993,
      "step": 1033200
    },
    {
      "epoch": 9.428607927585956,
      "grad_norm": 3.32192325592041,
      "learning_rate": 4.214282672701171e-05,
      "loss": 0.7077,
      "step": 1033300
    },
    {
      "epoch": 9.42952040294912,
      "grad_norm": 4.048878192901611,
      "learning_rate": 4.214206633087573e-05,
      "loss": 0.7109,
      "step": 1033400
    },
    {
      "epoch": 9.430432878312285,
      "grad_norm": 3.98360538482666,
      "learning_rate": 4.214130593473977e-05,
      "loss": 0.6685,
      "step": 1033500
    },
    {
      "epoch": 9.43134535367545,
      "grad_norm": 3.5738611221313477,
      "learning_rate": 4.214054553860379e-05,
      "loss": 0.6631,
      "step": 1033600
    },
    {
      "epoch": 9.432257829038615,
      "grad_norm": 4.442104816436768,
      "learning_rate": 4.213978514246782e-05,
      "loss": 0.6797,
      "step": 1033700
    },
    {
      "epoch": 9.43317030440178,
      "grad_norm": 3.6942780017852783,
      "learning_rate": 4.213902474633185e-05,
      "loss": 0.7051,
      "step": 1033800
    },
    {
      "epoch": 9.434082779764946,
      "grad_norm": 3.77108097076416,
      "learning_rate": 4.2138264350195876e-05,
      "loss": 0.6818,
      "step": 1033900
    },
    {
      "epoch": 9.434995255128111,
      "grad_norm": 3.515017509460449,
      "learning_rate": 4.2137503954059906e-05,
      "loss": 0.6999,
      "step": 1034000
    },
    {
      "epoch": 9.435907730491277,
      "grad_norm": 3.8200902938842773,
      "learning_rate": 4.2136743557923936e-05,
      "loss": 0.6878,
      "step": 1034100
    },
    {
      "epoch": 9.436820205854442,
      "grad_norm": 5.337068557739258,
      "learning_rate": 4.2135983161787966e-05,
      "loss": 0.698,
      "step": 1034200
    },
    {
      "epoch": 9.437732681217607,
      "grad_norm": 3.662679433822632,
      "learning_rate": 4.2135222765651997e-05,
      "loss": 0.6915,
      "step": 1034300
    },
    {
      "epoch": 9.438645156580773,
      "grad_norm": 4.556603908538818,
      "learning_rate": 4.2134462369516027e-05,
      "loss": 0.6658,
      "step": 1034400
    },
    {
      "epoch": 9.439557631943938,
      "grad_norm": 4.178386211395264,
      "learning_rate": 4.213370197338005e-05,
      "loss": 0.671,
      "step": 1034500
    },
    {
      "epoch": 9.440470107307103,
      "grad_norm": 3.7823872566223145,
      "learning_rate": 4.213294157724409e-05,
      "loss": 0.7022,
      "step": 1034600
    },
    {
      "epoch": 9.441382582670268,
      "grad_norm": 3.698256254196167,
      "learning_rate": 4.213218118110811e-05,
      "loss": 0.699,
      "step": 1034700
    },
    {
      "epoch": 9.442295058033434,
      "grad_norm": 3.384831666946411,
      "learning_rate": 4.213142078497214e-05,
      "loss": 0.7006,
      "step": 1034800
    },
    {
      "epoch": 9.443207533396599,
      "grad_norm": 4.678988456726074,
      "learning_rate": 4.213066038883617e-05,
      "loss": 0.6883,
      "step": 1034900
    },
    {
      "epoch": 9.444120008759764,
      "grad_norm": 4.905159950256348,
      "learning_rate": 4.21298999927002e-05,
      "loss": 0.6498,
      "step": 1035000
    },
    {
      "epoch": 9.445032484122928,
      "grad_norm": 4.5042619705200195,
      "learning_rate": 4.2129139596564223e-05,
      "loss": 0.6774,
      "step": 1035100
    },
    {
      "epoch": 9.445944959486093,
      "grad_norm": 3.8539535999298096,
      "learning_rate": 4.212837920042826e-05,
      "loss": 0.6703,
      "step": 1035200
    },
    {
      "epoch": 9.446857434849258,
      "grad_norm": 3.6338253021240234,
      "learning_rate": 4.2127618804292284e-05,
      "loss": 0.69,
      "step": 1035300
    },
    {
      "epoch": 9.447769910212424,
      "grad_norm": 3.355351448059082,
      "learning_rate": 4.2126858408156314e-05,
      "loss": 0.6602,
      "step": 1035400
    },
    {
      "epoch": 9.448682385575589,
      "grad_norm": 4.224224090576172,
      "learning_rate": 4.2126098012020344e-05,
      "loss": 0.7233,
      "step": 1035500
    },
    {
      "epoch": 9.449594860938754,
      "grad_norm": 2.8754894733428955,
      "learning_rate": 4.2125337615884374e-05,
      "loss": 0.6707,
      "step": 1035600
    },
    {
      "epoch": 9.45050733630192,
      "grad_norm": 4.261956214904785,
      "learning_rate": 4.2124577219748404e-05,
      "loss": 0.7036,
      "step": 1035700
    },
    {
      "epoch": 9.451419811665085,
      "grad_norm": 3.678805351257324,
      "learning_rate": 4.2123816823612434e-05,
      "loss": 0.6802,
      "step": 1035800
    },
    {
      "epoch": 9.45233228702825,
      "grad_norm": 3.9402546882629395,
      "learning_rate": 4.212305642747646e-05,
      "loss": 0.6732,
      "step": 1035900
    },
    {
      "epoch": 9.453244762391416,
      "grad_norm": 4.613270282745361,
      "learning_rate": 4.2122296031340494e-05,
      "loss": 0.6584,
      "step": 1036000
    },
    {
      "epoch": 9.45415723775458,
      "grad_norm": 4.25130558013916,
      "learning_rate": 4.212153563520452e-05,
      "loss": 0.671,
      "step": 1036100
    },
    {
      "epoch": 9.455069713117746,
      "grad_norm": 3.9758834838867188,
      "learning_rate": 4.212077523906855e-05,
      "loss": 0.6864,
      "step": 1036200
    },
    {
      "epoch": 9.455982188480911,
      "grad_norm": 3.9804608821868896,
      "learning_rate": 4.212001484293258e-05,
      "loss": 0.6892,
      "step": 1036300
    },
    {
      "epoch": 9.456894663844077,
      "grad_norm": 4.855099201202393,
      "learning_rate": 4.211925444679661e-05,
      "loss": 0.6933,
      "step": 1036400
    },
    {
      "epoch": 9.457807139207242,
      "grad_norm": 1.7767120599746704,
      "learning_rate": 4.211849405066063e-05,
      "loss": 0.67,
      "step": 1036500
    },
    {
      "epoch": 9.458719614570407,
      "grad_norm": 3.8884294033050537,
      "learning_rate": 4.211773365452467e-05,
      "loss": 0.6938,
      "step": 1036600
    },
    {
      "epoch": 9.459632089933573,
      "grad_norm": 3.6226065158843994,
      "learning_rate": 4.211697325838869e-05,
      "loss": 0.6866,
      "step": 1036700
    },
    {
      "epoch": 9.460544565296736,
      "grad_norm": 4.411003112792969,
      "learning_rate": 4.211621286225272e-05,
      "loss": 0.6853,
      "step": 1036800
    },
    {
      "epoch": 9.461457040659901,
      "grad_norm": 3.52571964263916,
      "learning_rate": 4.211545246611675e-05,
      "loss": 0.6902,
      "step": 1036900
    },
    {
      "epoch": 9.462369516023067,
      "grad_norm": 3.4550225734710693,
      "learning_rate": 4.2114692069980774e-05,
      "loss": 0.6863,
      "step": 1037000
    },
    {
      "epoch": 9.463281991386232,
      "grad_norm": 4.23763370513916,
      "learning_rate": 4.211393167384481e-05,
      "loss": 0.6667,
      "step": 1037100
    },
    {
      "epoch": 9.464194466749397,
      "grad_norm": 4.281533718109131,
      "learning_rate": 4.2113171277708835e-05,
      "loss": 0.7041,
      "step": 1037200
    },
    {
      "epoch": 9.465106942112563,
      "grad_norm": 4.536357402801514,
      "learning_rate": 4.2112410881572865e-05,
      "loss": 0.7498,
      "step": 1037300
    },
    {
      "epoch": 9.466019417475728,
      "grad_norm": 5.004851818084717,
      "learning_rate": 4.2111650485436895e-05,
      "loss": 0.6842,
      "step": 1037400
    },
    {
      "epoch": 9.466931892838893,
      "grad_norm": 3.698444366455078,
      "learning_rate": 4.2110890089300925e-05,
      "loss": 0.6768,
      "step": 1037500
    },
    {
      "epoch": 9.467844368202059,
      "grad_norm": 4.564634323120117,
      "learning_rate": 4.211012969316495e-05,
      "loss": 0.6976,
      "step": 1037600
    },
    {
      "epoch": 9.468756843565224,
      "grad_norm": 3.567201614379883,
      "learning_rate": 4.2109369297028985e-05,
      "loss": 0.6463,
      "step": 1037700
    },
    {
      "epoch": 9.46966931892839,
      "grad_norm": 4.494640827178955,
      "learning_rate": 4.210860890089301e-05,
      "loss": 0.6866,
      "step": 1037800
    },
    {
      "epoch": 9.470581794291554,
      "grad_norm": 3.211843729019165,
      "learning_rate": 4.210784850475704e-05,
      "loss": 0.6411,
      "step": 1037900
    },
    {
      "epoch": 9.47149426965472,
      "grad_norm": 4.310295581817627,
      "learning_rate": 4.210708810862107e-05,
      "loss": 0.692,
      "step": 1038000
    },
    {
      "epoch": 9.472406745017885,
      "grad_norm": 4.61641263961792,
      "learning_rate": 4.21063277124851e-05,
      "loss": 0.6789,
      "step": 1038100
    },
    {
      "epoch": 9.47331922038105,
      "grad_norm": 4.029472351074219,
      "learning_rate": 4.210556731634913e-05,
      "loss": 0.69,
      "step": 1038200
    },
    {
      "epoch": 9.474231695744216,
      "grad_norm": 4.19501256942749,
      "learning_rate": 4.210480692021316e-05,
      "loss": 0.6687,
      "step": 1038300
    },
    {
      "epoch": 9.475144171107381,
      "grad_norm": 4.1268720626831055,
      "learning_rate": 4.210404652407718e-05,
      "loss": 0.7034,
      "step": 1038400
    },
    {
      "epoch": 9.476056646470544,
      "grad_norm": 4.213670253753662,
      "learning_rate": 4.210328612794122e-05,
      "loss": 0.7072,
      "step": 1038500
    },
    {
      "epoch": 9.47696912183371,
      "grad_norm": 4.534756660461426,
      "learning_rate": 4.210252573180524e-05,
      "loss": 0.6943,
      "step": 1038600
    },
    {
      "epoch": 9.477881597196875,
      "grad_norm": 3.8916821479797363,
      "learning_rate": 4.210176533566927e-05,
      "loss": 0.7583,
      "step": 1038700
    },
    {
      "epoch": 9.47879407256004,
      "grad_norm": 3.6414594650268555,
      "learning_rate": 4.21010049395333e-05,
      "loss": 0.6654,
      "step": 1038800
    },
    {
      "epoch": 9.479706547923206,
      "grad_norm": 3.777022123336792,
      "learning_rate": 4.210024454339733e-05,
      "loss": 0.6971,
      "step": 1038900
    },
    {
      "epoch": 9.480619023286371,
      "grad_norm": 3.8468356132507324,
      "learning_rate": 4.2099484147261355e-05,
      "loss": 0.7489,
      "step": 1039000
    },
    {
      "epoch": 9.481531498649536,
      "grad_norm": 4.0302557945251465,
      "learning_rate": 4.209872375112539e-05,
      "loss": 0.6913,
      "step": 1039100
    },
    {
      "epoch": 9.482443974012702,
      "grad_norm": 4.298581123352051,
      "learning_rate": 4.2097963354989416e-05,
      "loss": 0.6966,
      "step": 1039200
    },
    {
      "epoch": 9.483356449375867,
      "grad_norm": 4.752712726593018,
      "learning_rate": 4.2097202958853446e-05,
      "loss": 0.6718,
      "step": 1039300
    },
    {
      "epoch": 9.484268924739032,
      "grad_norm": 3.8899855613708496,
      "learning_rate": 4.2096442562717476e-05,
      "loss": 0.7096,
      "step": 1039400
    },
    {
      "epoch": 9.485181400102197,
      "grad_norm": 3.785313606262207,
      "learning_rate": 4.20956821665815e-05,
      "loss": 0.6622,
      "step": 1039500
    },
    {
      "epoch": 9.486093875465363,
      "grad_norm": 4.2725043296813965,
      "learning_rate": 4.2094921770445536e-05,
      "loss": 0.6799,
      "step": 1039600
    },
    {
      "epoch": 9.487006350828528,
      "grad_norm": 4.050200939178467,
      "learning_rate": 4.209416137430956e-05,
      "loss": 0.6879,
      "step": 1039700
    },
    {
      "epoch": 9.487918826191693,
      "grad_norm": 3.804880142211914,
      "learning_rate": 4.209340097817359e-05,
      "loss": 0.6732,
      "step": 1039800
    },
    {
      "epoch": 9.488831301554859,
      "grad_norm": 4.241050720214844,
      "learning_rate": 4.209264058203762e-05,
      "loss": 0.6805,
      "step": 1039900
    },
    {
      "epoch": 9.489743776918024,
      "grad_norm": 3.9039697647094727,
      "learning_rate": 4.209188018590165e-05,
      "loss": 0.6587,
      "step": 1040000
    },
    {
      "epoch": 9.49065625228119,
      "grad_norm": 4.264087200164795,
      "learning_rate": 4.209111978976568e-05,
      "loss": 0.6926,
      "step": 1040100
    },
    {
      "epoch": 9.491568727644353,
      "grad_norm": 4.023120880126953,
      "learning_rate": 4.209035939362971e-05,
      "loss": 0.6635,
      "step": 1040200
    },
    {
      "epoch": 9.492481203007518,
      "grad_norm": 5.010732650756836,
      "learning_rate": 4.208959899749373e-05,
      "loss": 0.6695,
      "step": 1040300
    },
    {
      "epoch": 9.493393678370683,
      "grad_norm": 3.896918535232544,
      "learning_rate": 4.208883860135777e-05,
      "loss": 0.6768,
      "step": 1040400
    },
    {
      "epoch": 9.494306153733849,
      "grad_norm": 3.8482110500335693,
      "learning_rate": 4.208807820522179e-05,
      "loss": 0.6808,
      "step": 1040500
    },
    {
      "epoch": 9.495218629097014,
      "grad_norm": 4.969793796539307,
      "learning_rate": 4.208731780908582e-05,
      "loss": 0.6846,
      "step": 1040600
    },
    {
      "epoch": 9.49613110446018,
      "grad_norm": 3.9417967796325684,
      "learning_rate": 4.208655741294985e-05,
      "loss": 0.6656,
      "step": 1040700
    },
    {
      "epoch": 9.497043579823345,
      "grad_norm": 4.068247318267822,
      "learning_rate": 4.208579701681388e-05,
      "loss": 0.679,
      "step": 1040800
    },
    {
      "epoch": 9.49795605518651,
      "grad_norm": 4.062982559204102,
      "learning_rate": 4.2085036620677906e-05,
      "loss": 0.6969,
      "step": 1040900
    },
    {
      "epoch": 9.498868530549675,
      "grad_norm": 3.024477481842041,
      "learning_rate": 4.208427622454194e-05,
      "loss": 0.6786,
      "step": 1041000
    },
    {
      "epoch": 9.49978100591284,
      "grad_norm": 4.142333507537842,
      "learning_rate": 4.2083515828405967e-05,
      "loss": 0.7044,
      "step": 1041100
    },
    {
      "epoch": 9.500693481276006,
      "grad_norm": 3.750549793243408,
      "learning_rate": 4.2082755432269997e-05,
      "loss": 0.7023,
      "step": 1041200
    },
    {
      "epoch": 9.501605956639171,
      "grad_norm": 3.6089847087860107,
      "learning_rate": 4.208199503613403e-05,
      "loss": 0.7263,
      "step": 1041300
    },
    {
      "epoch": 9.502518432002336,
      "grad_norm": 3.7693068981170654,
      "learning_rate": 4.208123463999806e-05,
      "loss": 0.6903,
      "step": 1041400
    },
    {
      "epoch": 9.503430907365502,
      "grad_norm": 4.3945488929748535,
      "learning_rate": 4.208047424386209e-05,
      "loss": 0.6552,
      "step": 1041500
    },
    {
      "epoch": 9.504343382728667,
      "grad_norm": 4.256235599517822,
      "learning_rate": 4.207971384772612e-05,
      "loss": 0.7061,
      "step": 1041600
    },
    {
      "epoch": 9.505255858091832,
      "grad_norm": 4.021578788757324,
      "learning_rate": 4.207895345159014e-05,
      "loss": 0.6741,
      "step": 1041700
    },
    {
      "epoch": 9.506168333454998,
      "grad_norm": 4.930951118469238,
      "learning_rate": 4.207819305545418e-05,
      "loss": 0.6945,
      "step": 1041800
    },
    {
      "epoch": 9.507080808818161,
      "grad_norm": 4.447865009307861,
      "learning_rate": 4.20774326593182e-05,
      "loss": 0.6951,
      "step": 1041900
    },
    {
      "epoch": 9.507993284181326,
      "grad_norm": 4.038752555847168,
      "learning_rate": 4.207667226318223e-05,
      "loss": 0.7137,
      "step": 1042000
    },
    {
      "epoch": 9.508905759544492,
      "grad_norm": 3.456648111343384,
      "learning_rate": 4.207591186704626e-05,
      "loss": 0.6471,
      "step": 1042100
    },
    {
      "epoch": 9.509818234907657,
      "grad_norm": 4.671725749969482,
      "learning_rate": 4.207515147091029e-05,
      "loss": 0.7138,
      "step": 1042200
    },
    {
      "epoch": 9.510730710270822,
      "grad_norm": 4.347525596618652,
      "learning_rate": 4.2074391074774314e-05,
      "loss": 0.7143,
      "step": 1042300
    },
    {
      "epoch": 9.511643185633988,
      "grad_norm": 4.800418853759766,
      "learning_rate": 4.2073630678638344e-05,
      "loss": 0.6924,
      "step": 1042400
    },
    {
      "epoch": 9.512555660997153,
      "grad_norm": 3.5134122371673584,
      "learning_rate": 4.2072870282502374e-05,
      "loss": 0.6804,
      "step": 1042500
    },
    {
      "epoch": 9.513468136360318,
      "grad_norm": 3.979417085647583,
      "learning_rate": 4.2072109886366404e-05,
      "loss": 0.6599,
      "step": 1042600
    },
    {
      "epoch": 9.514380611723483,
      "grad_norm": 2.9027223587036133,
      "learning_rate": 4.2071349490230434e-05,
      "loss": 0.6655,
      "step": 1042700
    },
    {
      "epoch": 9.515293087086649,
      "grad_norm": 4.3939433097839355,
      "learning_rate": 4.207058909409446e-05,
      "loss": 0.7077,
      "step": 1042800
    },
    {
      "epoch": 9.516205562449814,
      "grad_norm": 4.298759460449219,
      "learning_rate": 4.2069828697958494e-05,
      "loss": 0.6904,
      "step": 1042900
    },
    {
      "epoch": 9.51711803781298,
      "grad_norm": 3.4967455863952637,
      "learning_rate": 4.206906830182252e-05,
      "loss": 0.6719,
      "step": 1043000
    },
    {
      "epoch": 9.518030513176145,
      "grad_norm": 3.2115955352783203,
      "learning_rate": 4.206830790568655e-05,
      "loss": 0.7158,
      "step": 1043100
    },
    {
      "epoch": 9.51894298853931,
      "grad_norm": 4.071151256561279,
      "learning_rate": 4.206754750955058e-05,
      "loss": 0.6939,
      "step": 1043200
    },
    {
      "epoch": 9.519855463902475,
      "grad_norm": 5.4955153465271,
      "learning_rate": 4.206678711341461e-05,
      "loss": 0.7202,
      "step": 1043300
    },
    {
      "epoch": 9.52076793926564,
      "grad_norm": 3.6601269245147705,
      "learning_rate": 4.206602671727863e-05,
      "loss": 0.6618,
      "step": 1043400
    },
    {
      "epoch": 9.521680414628804,
      "grad_norm": 4.346809387207031,
      "learning_rate": 4.206526632114267e-05,
      "loss": 0.682,
      "step": 1043500
    },
    {
      "epoch": 9.52259288999197,
      "grad_norm": 3.981787919998169,
      "learning_rate": 4.206450592500669e-05,
      "loss": 0.717,
      "step": 1043600
    },
    {
      "epoch": 9.523505365355135,
      "grad_norm": 4.160004615783691,
      "learning_rate": 4.206374552887072e-05,
      "loss": 0.7011,
      "step": 1043700
    },
    {
      "epoch": 9.5244178407183,
      "grad_norm": 4.251143932342529,
      "learning_rate": 4.206298513273475e-05,
      "loss": 0.6518,
      "step": 1043800
    },
    {
      "epoch": 9.525330316081465,
      "grad_norm": 3.5131258964538574,
      "learning_rate": 4.206222473659878e-05,
      "loss": 0.7541,
      "step": 1043900
    },
    {
      "epoch": 9.52624279144463,
      "grad_norm": 4.312569618225098,
      "learning_rate": 4.206146434046281e-05,
      "loss": 0.6988,
      "step": 1044000
    },
    {
      "epoch": 9.527155266807796,
      "grad_norm": 4.609282970428467,
      "learning_rate": 4.206070394432684e-05,
      "loss": 0.7047,
      "step": 1044100
    },
    {
      "epoch": 9.528067742170961,
      "grad_norm": 4.220204830169678,
      "learning_rate": 4.2059943548190865e-05,
      "loss": 0.6644,
      "step": 1044200
    },
    {
      "epoch": 9.528980217534126,
      "grad_norm": 4.0241475105285645,
      "learning_rate": 4.20591831520549e-05,
      "loss": 0.6967,
      "step": 1044300
    },
    {
      "epoch": 9.529892692897292,
      "grad_norm": 3.689612627029419,
      "learning_rate": 4.2058422755918925e-05,
      "loss": 0.6634,
      "step": 1044400
    },
    {
      "epoch": 9.530805168260457,
      "grad_norm": 3.9827778339385986,
      "learning_rate": 4.2057662359782955e-05,
      "loss": 0.6628,
      "step": 1044500
    },
    {
      "epoch": 9.531717643623622,
      "grad_norm": 3.682453155517578,
      "learning_rate": 4.2056901963646985e-05,
      "loss": 0.6959,
      "step": 1044600
    },
    {
      "epoch": 9.532630118986788,
      "grad_norm": 4.6917243003845215,
      "learning_rate": 4.2056141567511015e-05,
      "loss": 0.6731,
      "step": 1044700
    },
    {
      "epoch": 9.533542594349953,
      "grad_norm": 4.0317792892456055,
      "learning_rate": 4.205538117137504e-05,
      "loss": 0.6561,
      "step": 1044800
    },
    {
      "epoch": 9.534455069713118,
      "grad_norm": 3.3319289684295654,
      "learning_rate": 4.2054620775239075e-05,
      "loss": 0.7069,
      "step": 1044900
    },
    {
      "epoch": 9.535367545076284,
      "grad_norm": 4.421604633331299,
      "learning_rate": 4.20538603791031e-05,
      "loss": 0.6843,
      "step": 1045000
    },
    {
      "epoch": 9.536280020439449,
      "grad_norm": 4.572640419006348,
      "learning_rate": 4.205309998296713e-05,
      "loss": 0.6757,
      "step": 1045100
    },
    {
      "epoch": 9.537192495802614,
      "grad_norm": 3.5042576789855957,
      "learning_rate": 4.205233958683116e-05,
      "loss": 0.7098,
      "step": 1045200
    },
    {
      "epoch": 9.538104971165778,
      "grad_norm": 3.265349864959717,
      "learning_rate": 4.205157919069518e-05,
      "loss": 0.7082,
      "step": 1045300
    },
    {
      "epoch": 9.539017446528943,
      "grad_norm": 3.222318172454834,
      "learning_rate": 4.205081879455922e-05,
      "loss": 0.6898,
      "step": 1045400
    },
    {
      "epoch": 9.539929921892108,
      "grad_norm": 3.116952419281006,
      "learning_rate": 4.205005839842324e-05,
      "loss": 0.6957,
      "step": 1045500
    },
    {
      "epoch": 9.540842397255274,
      "grad_norm": 4.492216110229492,
      "learning_rate": 4.204929800228727e-05,
      "loss": 0.6863,
      "step": 1045600
    },
    {
      "epoch": 9.541754872618439,
      "grad_norm": 3.4077975749969482,
      "learning_rate": 4.20485376061513e-05,
      "loss": 0.6581,
      "step": 1045700
    },
    {
      "epoch": 9.542667347981604,
      "grad_norm": 3.827552080154419,
      "learning_rate": 4.204777721001533e-05,
      "loss": 0.6742,
      "step": 1045800
    },
    {
      "epoch": 9.54357982334477,
      "grad_norm": 3.8205513954162598,
      "learning_rate": 4.2047016813879356e-05,
      "loss": 0.7325,
      "step": 1045900
    },
    {
      "epoch": 9.544492298707935,
      "grad_norm": 4.461954116821289,
      "learning_rate": 4.204625641774339e-05,
      "loss": 0.7183,
      "step": 1046000
    },
    {
      "epoch": 9.5454047740711,
      "grad_norm": 3.6666338443756104,
      "learning_rate": 4.2045496021607416e-05,
      "loss": 0.7224,
      "step": 1046100
    },
    {
      "epoch": 9.546317249434265,
      "grad_norm": 3.404446840286255,
      "learning_rate": 4.2044735625471446e-05,
      "loss": 0.7108,
      "step": 1046200
    },
    {
      "epoch": 9.54722972479743,
      "grad_norm": 3.2446210384368896,
      "learning_rate": 4.2043975229335476e-05,
      "loss": 0.719,
      "step": 1046300
    },
    {
      "epoch": 9.548142200160596,
      "grad_norm": 2.3229868412017822,
      "learning_rate": 4.2043214833199506e-05,
      "loss": 0.6844,
      "step": 1046400
    },
    {
      "epoch": 9.549054675523761,
      "grad_norm": 3.542642593383789,
      "learning_rate": 4.2042454437063536e-05,
      "loss": 0.7059,
      "step": 1046500
    },
    {
      "epoch": 9.549967150886927,
      "grad_norm": 3.2048752307891846,
      "learning_rate": 4.2041694040927566e-05,
      "loss": 0.6879,
      "step": 1046600
    },
    {
      "epoch": 9.550879626250092,
      "grad_norm": 4.606025695800781,
      "learning_rate": 4.204093364479159e-05,
      "loss": 0.6798,
      "step": 1046700
    },
    {
      "epoch": 9.551792101613257,
      "grad_norm": 4.266276836395264,
      "learning_rate": 4.2040173248655626e-05,
      "loss": 0.7089,
      "step": 1046800
    },
    {
      "epoch": 9.55270457697642,
      "grad_norm": 3.7432758808135986,
      "learning_rate": 4.203941285251965e-05,
      "loss": 0.6924,
      "step": 1046900
    },
    {
      "epoch": 9.553617052339586,
      "grad_norm": 4.585308074951172,
      "learning_rate": 4.203865245638368e-05,
      "loss": 0.6319,
      "step": 1047000
    },
    {
      "epoch": 9.554529527702751,
      "grad_norm": 4.167628765106201,
      "learning_rate": 4.203789206024771e-05,
      "loss": 0.7387,
      "step": 1047100
    },
    {
      "epoch": 9.555442003065917,
      "grad_norm": 3.699842929840088,
      "learning_rate": 4.203713166411174e-05,
      "loss": 0.6737,
      "step": 1047200
    },
    {
      "epoch": 9.556354478429082,
      "grad_norm": 3.5696611404418945,
      "learning_rate": 4.203637126797576e-05,
      "loss": 0.6757,
      "step": 1047300
    },
    {
      "epoch": 9.557266953792247,
      "grad_norm": 4.542766571044922,
      "learning_rate": 4.20356108718398e-05,
      "loss": 0.65,
      "step": 1047400
    },
    {
      "epoch": 9.558179429155413,
      "grad_norm": 4.582049369812012,
      "learning_rate": 4.203485047570382e-05,
      "loss": 0.6586,
      "step": 1047500
    },
    {
      "epoch": 9.559091904518578,
      "grad_norm": 4.131943702697754,
      "learning_rate": 4.203409007956785e-05,
      "loss": 0.7194,
      "step": 1047600
    },
    {
      "epoch": 9.560004379881743,
      "grad_norm": 4.021688461303711,
      "learning_rate": 4.203332968343188e-05,
      "loss": 0.696,
      "step": 1047700
    },
    {
      "epoch": 9.560916855244908,
      "grad_norm": 4.867702960968018,
      "learning_rate": 4.203256928729591e-05,
      "loss": 0.7026,
      "step": 1047800
    },
    {
      "epoch": 9.561829330608074,
      "grad_norm": 4.774940490722656,
      "learning_rate": 4.203180889115994e-05,
      "loss": 0.65,
      "step": 1047900
    },
    {
      "epoch": 9.562741805971239,
      "grad_norm": 4.314733505249023,
      "learning_rate": 4.2031048495023973e-05,
      "loss": 0.6883,
      "step": 1048000
    },
    {
      "epoch": 9.563654281334404,
      "grad_norm": 4.074741840362549,
      "learning_rate": 4.2030288098888e-05,
      "loss": 0.6882,
      "step": 1048100
    },
    {
      "epoch": 9.56456675669757,
      "grad_norm": 4.547235488891602,
      "learning_rate": 4.202952770275203e-05,
      "loss": 0.6874,
      "step": 1048200
    },
    {
      "epoch": 9.565479232060735,
      "grad_norm": 3.8238556385040283,
      "learning_rate": 4.202876730661606e-05,
      "loss": 0.6914,
      "step": 1048300
    },
    {
      "epoch": 9.5663917074239,
      "grad_norm": 3.6361663341522217,
      "learning_rate": 4.202800691048008e-05,
      "loss": 0.7013,
      "step": 1048400
    },
    {
      "epoch": 9.567304182787066,
      "grad_norm": 4.329410076141357,
      "learning_rate": 4.202724651434412e-05,
      "loss": 0.645,
      "step": 1048500
    },
    {
      "epoch": 9.56821665815023,
      "grad_norm": 3.9513325691223145,
      "learning_rate": 4.202648611820814e-05,
      "loss": 0.7034,
      "step": 1048600
    },
    {
      "epoch": 9.569129133513394,
      "grad_norm": 5.021946907043457,
      "learning_rate": 4.202572572207217e-05,
      "loss": 0.7073,
      "step": 1048700
    },
    {
      "epoch": 9.57004160887656,
      "grad_norm": 4.9767069816589355,
      "learning_rate": 4.20249653259362e-05,
      "loss": 0.6845,
      "step": 1048800
    },
    {
      "epoch": 9.570954084239725,
      "grad_norm": 3.8401670455932617,
      "learning_rate": 4.202420492980023e-05,
      "loss": 0.7397,
      "step": 1048900
    },
    {
      "epoch": 9.57186655960289,
      "grad_norm": 3.826986074447632,
      "learning_rate": 4.202344453366426e-05,
      "loss": 0.7141,
      "step": 1049000
    },
    {
      "epoch": 9.572779034966056,
      "grad_norm": 4.110471725463867,
      "learning_rate": 4.202268413752829e-05,
      "loss": 0.6596,
      "step": 1049100
    },
    {
      "epoch": 9.57369151032922,
      "grad_norm": 4.412177562713623,
      "learning_rate": 4.2021923741392314e-05,
      "loss": 0.6902,
      "step": 1049200
    },
    {
      "epoch": 9.574603985692386,
      "grad_norm": 4.547984600067139,
      "learning_rate": 4.202116334525635e-05,
      "loss": 0.6953,
      "step": 1049300
    },
    {
      "epoch": 9.575516461055551,
      "grad_norm": 4.561061859130859,
      "learning_rate": 4.2020402949120374e-05,
      "loss": 0.7099,
      "step": 1049400
    },
    {
      "epoch": 9.576428936418717,
      "grad_norm": 3.9724531173706055,
      "learning_rate": 4.2019642552984404e-05,
      "loss": 0.6899,
      "step": 1049500
    },
    {
      "epoch": 9.577341411781882,
      "grad_norm": 4.019392967224121,
      "learning_rate": 4.2018882156848434e-05,
      "loss": 0.694,
      "step": 1049600
    },
    {
      "epoch": 9.578253887145047,
      "grad_norm": 3.525061845779419,
      "learning_rate": 4.2018121760712464e-05,
      "loss": 0.7093,
      "step": 1049700
    },
    {
      "epoch": 9.579166362508213,
      "grad_norm": 4.418355941772461,
      "learning_rate": 4.201736136457649e-05,
      "loss": 0.6838,
      "step": 1049800
    },
    {
      "epoch": 9.580078837871378,
      "grad_norm": 4.414090633392334,
      "learning_rate": 4.2016600968440524e-05,
      "loss": 0.6574,
      "step": 1049900
    },
    {
      "epoch": 9.580991313234543,
      "grad_norm": 4.145714282989502,
      "learning_rate": 4.201584057230455e-05,
      "loss": 0.6918,
      "step": 1050000
    },
    {
      "epoch": 9.581903788597709,
      "grad_norm": 5.502116680145264,
      "learning_rate": 4.201508017616858e-05,
      "loss": 0.7197,
      "step": 1050100
    },
    {
      "epoch": 9.582816263960874,
      "grad_norm": 3.6928348541259766,
      "learning_rate": 4.201431978003261e-05,
      "loss": 0.7345,
      "step": 1050200
    },
    {
      "epoch": 9.583728739324037,
      "grad_norm": 4.231928825378418,
      "learning_rate": 4.201355938389664e-05,
      "loss": 0.684,
      "step": 1050300
    },
    {
      "epoch": 9.584641214687203,
      "grad_norm": 3.6863291263580322,
      "learning_rate": 4.201279898776067e-05,
      "loss": 0.6645,
      "step": 1050400
    },
    {
      "epoch": 9.585553690050368,
      "grad_norm": 3.365999937057495,
      "learning_rate": 4.20120385916247e-05,
      "loss": 0.6732,
      "step": 1050500
    },
    {
      "epoch": 9.586466165413533,
      "grad_norm": 4.652895450592041,
      "learning_rate": 4.201127819548872e-05,
      "loss": 0.7222,
      "step": 1050600
    },
    {
      "epoch": 9.587378640776699,
      "grad_norm": 4.278105735778809,
      "learning_rate": 4.201051779935276e-05,
      "loss": 0.6844,
      "step": 1050700
    },
    {
      "epoch": 9.588291116139864,
      "grad_norm": 3.9010274410247803,
      "learning_rate": 4.200975740321678e-05,
      "loss": 0.6942,
      "step": 1050800
    },
    {
      "epoch": 9.58920359150303,
      "grad_norm": 3.470146894454956,
      "learning_rate": 4.200899700708081e-05,
      "loss": 0.7054,
      "step": 1050900
    },
    {
      "epoch": 9.590116066866194,
      "grad_norm": 3.0673882961273193,
      "learning_rate": 4.200823661094484e-05,
      "loss": 0.7045,
      "step": 1051000
    },
    {
      "epoch": 9.59102854222936,
      "grad_norm": 3.7533535957336426,
      "learning_rate": 4.2007476214808865e-05,
      "loss": 0.6738,
      "step": 1051100
    },
    {
      "epoch": 9.591941017592525,
      "grad_norm": 2.9501259326934814,
      "learning_rate": 4.2006715818672895e-05,
      "loss": 0.6642,
      "step": 1051200
    },
    {
      "epoch": 9.59285349295569,
      "grad_norm": 5.285445213317871,
      "learning_rate": 4.2005955422536925e-05,
      "loss": 0.66,
      "step": 1051300
    },
    {
      "epoch": 9.593765968318856,
      "grad_norm": 4.100728988647461,
      "learning_rate": 4.2005195026400955e-05,
      "loss": 0.6684,
      "step": 1051400
    },
    {
      "epoch": 9.594678443682021,
      "grad_norm": 4.721038341522217,
      "learning_rate": 4.2004434630264985e-05,
      "loss": 0.6736,
      "step": 1051500
    },
    {
      "epoch": 9.595590919045186,
      "grad_norm": 3.9387316703796387,
      "learning_rate": 4.2003674234129015e-05,
      "loss": 0.688,
      "step": 1051600
    },
    {
      "epoch": 9.596503394408352,
      "grad_norm": 4.484405040740967,
      "learning_rate": 4.200291383799304e-05,
      "loss": 0.7252,
      "step": 1051700
    },
    {
      "epoch": 9.597415869771517,
      "grad_norm": 3.919447660446167,
      "learning_rate": 4.2002153441857075e-05,
      "loss": 0.6997,
      "step": 1051800
    },
    {
      "epoch": 9.598328345134682,
      "grad_norm": 4.433726787567139,
      "learning_rate": 4.20013930457211e-05,
      "loss": 0.6772,
      "step": 1051900
    },
    {
      "epoch": 9.599240820497847,
      "grad_norm": 4.244019031524658,
      "learning_rate": 4.200063264958513e-05,
      "loss": 0.6848,
      "step": 1052000
    },
    {
      "epoch": 9.600153295861011,
      "grad_norm": 4.038148880004883,
      "learning_rate": 4.199987225344916e-05,
      "loss": 0.6818,
      "step": 1052100
    },
    {
      "epoch": 9.601065771224176,
      "grad_norm": 3.646914005279541,
      "learning_rate": 4.199911185731319e-05,
      "loss": 0.7062,
      "step": 1052200
    },
    {
      "epoch": 9.601978246587342,
      "grad_norm": 3.974599599838257,
      "learning_rate": 4.199835146117722e-05,
      "loss": 0.6915,
      "step": 1052300
    },
    {
      "epoch": 9.602890721950507,
      "grad_norm": 4.956359386444092,
      "learning_rate": 4.199759106504125e-05,
      "loss": 0.6464,
      "step": 1052400
    },
    {
      "epoch": 9.603803197313672,
      "grad_norm": 4.081851959228516,
      "learning_rate": 4.199683066890527e-05,
      "loss": 0.6666,
      "step": 1052500
    },
    {
      "epoch": 9.604715672676837,
      "grad_norm": 3.049833059310913,
      "learning_rate": 4.19960702727693e-05,
      "loss": 0.6563,
      "step": 1052600
    },
    {
      "epoch": 9.605628148040003,
      "grad_norm": 4.397568702697754,
      "learning_rate": 4.199530987663333e-05,
      "loss": 0.6653,
      "step": 1052700
    },
    {
      "epoch": 9.606540623403168,
      "grad_norm": 3.842686414718628,
      "learning_rate": 4.199454948049736e-05,
      "loss": 0.6806,
      "step": 1052800
    },
    {
      "epoch": 9.607453098766333,
      "grad_norm": 3.242382764816284,
      "learning_rate": 4.199378908436139e-05,
      "loss": 0.6885,
      "step": 1052900
    },
    {
      "epoch": 9.608365574129499,
      "grad_norm": 4.145986557006836,
      "learning_rate": 4.199302868822542e-05,
      "loss": 0.7169,
      "step": 1053000
    },
    {
      "epoch": 9.609278049492664,
      "grad_norm": 3.9116647243499756,
      "learning_rate": 4.1992268292089446e-05,
      "loss": 0.6903,
      "step": 1053100
    },
    {
      "epoch": 9.61019052485583,
      "grad_norm": 3.1816465854644775,
      "learning_rate": 4.199150789595348e-05,
      "loss": 0.6914,
      "step": 1053200
    },
    {
      "epoch": 9.611103000218995,
      "grad_norm": 4.358950614929199,
      "learning_rate": 4.1990747499817506e-05,
      "loss": 0.6611,
      "step": 1053300
    },
    {
      "epoch": 9.61201547558216,
      "grad_norm": 5.058081150054932,
      "learning_rate": 4.1989987103681536e-05,
      "loss": 0.7038,
      "step": 1053400
    },
    {
      "epoch": 9.612927950945325,
      "grad_norm": 5.519130706787109,
      "learning_rate": 4.1989226707545566e-05,
      "loss": 0.7224,
      "step": 1053500
    },
    {
      "epoch": 9.61384042630849,
      "grad_norm": 3.9541704654693604,
      "learning_rate": 4.1988466311409596e-05,
      "loss": 0.7077,
      "step": 1053600
    },
    {
      "epoch": 9.614752901671654,
      "grad_norm": 3.971968173980713,
      "learning_rate": 4.1987705915273626e-05,
      "loss": 0.7062,
      "step": 1053700
    },
    {
      "epoch": 9.61566537703482,
      "grad_norm": 4.095557689666748,
      "learning_rate": 4.198694551913765e-05,
      "loss": 0.6951,
      "step": 1053800
    },
    {
      "epoch": 9.616577852397985,
      "grad_norm": 3.8572230339050293,
      "learning_rate": 4.198618512300168e-05,
      "loss": 0.6927,
      "step": 1053900
    },
    {
      "epoch": 9.61749032776115,
      "grad_norm": 4.456506729125977,
      "learning_rate": 4.198542472686571e-05,
      "loss": 0.7032,
      "step": 1054000
    },
    {
      "epoch": 9.618402803124315,
      "grad_norm": 5.575626850128174,
      "learning_rate": 4.198466433072974e-05,
      "loss": 0.6699,
      "step": 1054100
    },
    {
      "epoch": 9.61931527848748,
      "grad_norm": 4.365490913391113,
      "learning_rate": 4.198390393459376e-05,
      "loss": 0.6288,
      "step": 1054200
    },
    {
      "epoch": 9.620227753850646,
      "grad_norm": 3.705909013748169,
      "learning_rate": 4.19831435384578e-05,
      "loss": 0.6654,
      "step": 1054300
    },
    {
      "epoch": 9.621140229213811,
      "grad_norm": 3.9123282432556152,
      "learning_rate": 4.198238314232182e-05,
      "loss": 0.6403,
      "step": 1054400
    },
    {
      "epoch": 9.622052704576976,
      "grad_norm": 4.580163955688477,
      "learning_rate": 4.198162274618585e-05,
      "loss": 0.6704,
      "step": 1054500
    },
    {
      "epoch": 9.622965179940142,
      "grad_norm": 4.069721698760986,
      "learning_rate": 4.198086235004988e-05,
      "loss": 0.7338,
      "step": 1054600
    },
    {
      "epoch": 9.623877655303307,
      "grad_norm": 3.482910633087158,
      "learning_rate": 4.198010195391391e-05,
      "loss": 0.71,
      "step": 1054700
    },
    {
      "epoch": 9.624790130666472,
      "grad_norm": 3.7731246948242188,
      "learning_rate": 4.1979341557777943e-05,
      "loss": 0.7013,
      "step": 1054800
    },
    {
      "epoch": 9.625702606029638,
      "grad_norm": 4.067139625549316,
      "learning_rate": 4.1978581161641974e-05,
      "loss": 0.686,
      "step": 1054900
    },
    {
      "epoch": 9.626615081392803,
      "grad_norm": 4.635762691497803,
      "learning_rate": 4.1977820765506e-05,
      "loss": 0.7038,
      "step": 1055000
    },
    {
      "epoch": 9.627527556755968,
      "grad_norm": 4.121212959289551,
      "learning_rate": 4.1977060369370034e-05,
      "loss": 0.7065,
      "step": 1055100
    },
    {
      "epoch": 9.628440032119133,
      "grad_norm": 4.379951000213623,
      "learning_rate": 4.197629997323406e-05,
      "loss": 0.6658,
      "step": 1055200
    },
    {
      "epoch": 9.629352507482299,
      "grad_norm": 5.830300331115723,
      "learning_rate": 4.197553957709809e-05,
      "loss": 0.7217,
      "step": 1055300
    },
    {
      "epoch": 9.630264982845464,
      "grad_norm": 4.555906772613525,
      "learning_rate": 4.197477918096212e-05,
      "loss": 0.7072,
      "step": 1055400
    },
    {
      "epoch": 9.631177458208628,
      "grad_norm": 4.320178031921387,
      "learning_rate": 4.197401878482615e-05,
      "loss": 0.6746,
      "step": 1055500
    },
    {
      "epoch": 9.632089933571793,
      "grad_norm": 4.969639778137207,
      "learning_rate": 4.197325838869017e-05,
      "loss": 0.6826,
      "step": 1055600
    },
    {
      "epoch": 9.633002408934958,
      "grad_norm": 3.8673505783081055,
      "learning_rate": 4.197249799255421e-05,
      "loss": 0.681,
      "step": 1055700
    },
    {
      "epoch": 9.633914884298123,
      "grad_norm": 5.921220302581787,
      "learning_rate": 4.197173759641823e-05,
      "loss": 0.7009,
      "step": 1055800
    },
    {
      "epoch": 9.634827359661289,
      "grad_norm": 3.0905895233154297,
      "learning_rate": 4.197097720028226e-05,
      "loss": 0.6538,
      "step": 1055900
    },
    {
      "epoch": 9.635739835024454,
      "grad_norm": 4.552901744842529,
      "learning_rate": 4.197021680414629e-05,
      "loss": 0.6961,
      "step": 1056000
    },
    {
      "epoch": 9.63665231038762,
      "grad_norm": 4.227990627288818,
      "learning_rate": 4.196945640801032e-05,
      "loss": 0.6723,
      "step": 1056100
    },
    {
      "epoch": 9.637564785750785,
      "grad_norm": 3.6011509895324707,
      "learning_rate": 4.196869601187435e-05,
      "loss": 0.7155,
      "step": 1056200
    },
    {
      "epoch": 9.63847726111395,
      "grad_norm": 4.78896951675415,
      "learning_rate": 4.196793561573838e-05,
      "loss": 0.6965,
      "step": 1056300
    },
    {
      "epoch": 9.639389736477115,
      "grad_norm": 3.8631222248077393,
      "learning_rate": 4.1967175219602404e-05,
      "loss": 0.6893,
      "step": 1056400
    },
    {
      "epoch": 9.64030221184028,
      "grad_norm": 2.9405694007873535,
      "learning_rate": 4.196641482346644e-05,
      "loss": 0.6949,
      "step": 1056500
    },
    {
      "epoch": 9.641214687203446,
      "grad_norm": 4.150258541107178,
      "learning_rate": 4.1965654427330464e-05,
      "loss": 0.683,
      "step": 1056600
    },
    {
      "epoch": 9.642127162566611,
      "grad_norm": 4.678225994110107,
      "learning_rate": 4.196489403119449e-05,
      "loss": 0.6978,
      "step": 1056700
    },
    {
      "epoch": 9.643039637929776,
      "grad_norm": 4.05911111831665,
      "learning_rate": 4.1964133635058524e-05,
      "loss": 0.6759,
      "step": 1056800
    },
    {
      "epoch": 9.643952113292942,
      "grad_norm": 4.617972373962402,
      "learning_rate": 4.196337323892255e-05,
      "loss": 0.6722,
      "step": 1056900
    },
    {
      "epoch": 9.644864588656107,
      "grad_norm": 4.3874640464782715,
      "learning_rate": 4.196261284278658e-05,
      "loss": 0.7372,
      "step": 1057000
    },
    {
      "epoch": 9.64577706401927,
      "grad_norm": 3.5969693660736084,
      "learning_rate": 4.196185244665061e-05,
      "loss": 0.7188,
      "step": 1057100
    },
    {
      "epoch": 9.646689539382436,
      "grad_norm": 4.597809314727783,
      "learning_rate": 4.196109205051464e-05,
      "loss": 0.7033,
      "step": 1057200
    },
    {
      "epoch": 9.647602014745601,
      "grad_norm": 4.524618625640869,
      "learning_rate": 4.196033165437867e-05,
      "loss": 0.7119,
      "step": 1057300
    },
    {
      "epoch": 9.648514490108766,
      "grad_norm": 4.104069232940674,
      "learning_rate": 4.19595712582427e-05,
      "loss": 0.6918,
      "step": 1057400
    },
    {
      "epoch": 9.649426965471932,
      "grad_norm": 3.1397199630737305,
      "learning_rate": 4.195881086210672e-05,
      "loss": 0.6943,
      "step": 1057500
    },
    {
      "epoch": 9.650339440835097,
      "grad_norm": 4.9059247970581055,
      "learning_rate": 4.195805046597076e-05,
      "loss": 0.6877,
      "step": 1057600
    },
    {
      "epoch": 9.651251916198262,
      "grad_norm": 4.1606245040893555,
      "learning_rate": 4.195729006983478e-05,
      "loss": 0.6731,
      "step": 1057700
    },
    {
      "epoch": 9.652164391561428,
      "grad_norm": 4.631048202514648,
      "learning_rate": 4.195652967369881e-05,
      "loss": 0.6976,
      "step": 1057800
    },
    {
      "epoch": 9.653076866924593,
      "grad_norm": 3.482191801071167,
      "learning_rate": 4.195576927756284e-05,
      "loss": 0.7046,
      "step": 1057900
    },
    {
      "epoch": 9.653989342287758,
      "grad_norm": 3.760122060775757,
      "learning_rate": 4.195500888142687e-05,
      "loss": 0.7111,
      "step": 1058000
    },
    {
      "epoch": 9.654901817650924,
      "grad_norm": 3.3817927837371826,
      "learning_rate": 4.1954248485290895e-05,
      "loss": 0.694,
      "step": 1058100
    },
    {
      "epoch": 9.655814293014089,
      "grad_norm": 3.7892026901245117,
      "learning_rate": 4.195348808915493e-05,
      "loss": 0.6778,
      "step": 1058200
    },
    {
      "epoch": 9.656726768377254,
      "grad_norm": 5.264703273773193,
      "learning_rate": 4.1952727693018955e-05,
      "loss": 0.6721,
      "step": 1058300
    },
    {
      "epoch": 9.65763924374042,
      "grad_norm": 4.024043083190918,
      "learning_rate": 4.1951967296882985e-05,
      "loss": 0.7071,
      "step": 1058400
    },
    {
      "epoch": 9.658551719103585,
      "grad_norm": 4.027953147888184,
      "learning_rate": 4.1951206900747015e-05,
      "loss": 0.6987,
      "step": 1058500
    },
    {
      "epoch": 9.65946419446675,
      "grad_norm": 4.438676357269287,
      "learning_rate": 4.1950446504611045e-05,
      "loss": 0.7106,
      "step": 1058600
    },
    {
      "epoch": 9.660376669829915,
      "grad_norm": 3.8508079051971436,
      "learning_rate": 4.1949686108475075e-05,
      "loss": 0.6874,
      "step": 1058700
    },
    {
      "epoch": 9.66128914519308,
      "grad_norm": 3.891989231109619,
      "learning_rate": 4.1948925712339105e-05,
      "loss": 0.6733,
      "step": 1058800
    },
    {
      "epoch": 9.662201620556244,
      "grad_norm": 4.765913486480713,
      "learning_rate": 4.194816531620313e-05,
      "loss": 0.7123,
      "step": 1058900
    },
    {
      "epoch": 9.66311409591941,
      "grad_norm": 3.556466817855835,
      "learning_rate": 4.1947404920067166e-05,
      "loss": 0.721,
      "step": 1059000
    },
    {
      "epoch": 9.664026571282575,
      "grad_norm": 5.408646583557129,
      "learning_rate": 4.194664452393119e-05,
      "loss": 0.6824,
      "step": 1059100
    },
    {
      "epoch": 9.66493904664574,
      "grad_norm": 3.458488702774048,
      "learning_rate": 4.194588412779522e-05,
      "loss": 0.7151,
      "step": 1059200
    },
    {
      "epoch": 9.665851522008905,
      "grad_norm": 3.5938150882720947,
      "learning_rate": 4.194512373165925e-05,
      "loss": 0.6679,
      "step": 1059300
    },
    {
      "epoch": 9.66676399737207,
      "grad_norm": 4.304788589477539,
      "learning_rate": 4.194436333552327e-05,
      "loss": 0.6932,
      "step": 1059400
    },
    {
      "epoch": 9.667676472735236,
      "grad_norm": 4.056553840637207,
      "learning_rate": 4.19436029393873e-05,
      "loss": 0.7166,
      "step": 1059500
    },
    {
      "epoch": 9.668588948098401,
      "grad_norm": 3.9915618896484375,
      "learning_rate": 4.194284254325133e-05,
      "loss": 0.6716,
      "step": 1059600
    },
    {
      "epoch": 9.669501423461567,
      "grad_norm": 3.6864147186279297,
      "learning_rate": 4.194208214711536e-05,
      "loss": 0.6893,
      "step": 1059700
    },
    {
      "epoch": 9.670413898824732,
      "grad_norm": 3.8682379722595215,
      "learning_rate": 4.194132175097939e-05,
      "loss": 0.6975,
      "step": 1059800
    },
    {
      "epoch": 9.671326374187897,
      "grad_norm": 3.8015639781951904,
      "learning_rate": 4.194056135484342e-05,
      "loss": 0.6828,
      "step": 1059900
    },
    {
      "epoch": 9.672238849551063,
      "grad_norm": 3.200275421142578,
      "learning_rate": 4.1939800958707446e-05,
      "loss": 0.6536,
      "step": 1060000
    },
    {
      "epoch": 9.673151324914228,
      "grad_norm": 4.007441997528076,
      "learning_rate": 4.193904056257148e-05,
      "loss": 0.6906,
      "step": 1060100
    },
    {
      "epoch": 9.674063800277393,
      "grad_norm": 4.314164638519287,
      "learning_rate": 4.1938280166435506e-05,
      "loss": 0.7011,
      "step": 1060200
    },
    {
      "epoch": 9.674976275640558,
      "grad_norm": 4.423404216766357,
      "learning_rate": 4.1937519770299536e-05,
      "loss": 0.6584,
      "step": 1060300
    },
    {
      "epoch": 9.675888751003724,
      "grad_norm": 4.633143901824951,
      "learning_rate": 4.1936759374163566e-05,
      "loss": 0.6914,
      "step": 1060400
    },
    {
      "epoch": 9.676801226366887,
      "grad_norm": 3.8118209838867188,
      "learning_rate": 4.1935998978027596e-05,
      "loss": 0.6855,
      "step": 1060500
    },
    {
      "epoch": 9.677713701730053,
      "grad_norm": 2.8885581493377686,
      "learning_rate": 4.193523858189162e-05,
      "loss": 0.6795,
      "step": 1060600
    },
    {
      "epoch": 9.678626177093218,
      "grad_norm": 4.312844753265381,
      "learning_rate": 4.1934478185755656e-05,
      "loss": 0.6933,
      "step": 1060700
    },
    {
      "epoch": 9.679538652456383,
      "grad_norm": 4.259014129638672,
      "learning_rate": 4.193371778961968e-05,
      "loss": 0.6736,
      "step": 1060800
    },
    {
      "epoch": 9.680451127819548,
      "grad_norm": 3.685227155685425,
      "learning_rate": 4.193295739348371e-05,
      "loss": 0.6832,
      "step": 1060900
    },
    {
      "epoch": 9.681363603182714,
      "grad_norm": 5.086840629577637,
      "learning_rate": 4.193219699734774e-05,
      "loss": 0.6866,
      "step": 1061000
    },
    {
      "epoch": 9.682276078545879,
      "grad_norm": 3.655785083770752,
      "learning_rate": 4.193143660121177e-05,
      "loss": 0.7066,
      "step": 1061100
    },
    {
      "epoch": 9.683188553909044,
      "grad_norm": 4.160398006439209,
      "learning_rate": 4.19306762050758e-05,
      "loss": 0.6708,
      "step": 1061200
    },
    {
      "epoch": 9.68410102927221,
      "grad_norm": 3.393552303314209,
      "learning_rate": 4.192991580893983e-05,
      "loss": 0.6899,
      "step": 1061300
    },
    {
      "epoch": 9.685013504635375,
      "grad_norm": 3.623995542526245,
      "learning_rate": 4.192915541280385e-05,
      "loss": 0.6674,
      "step": 1061400
    },
    {
      "epoch": 9.68592597999854,
      "grad_norm": 3.689476728439331,
      "learning_rate": 4.192839501666789e-05,
      "loss": 0.6632,
      "step": 1061500
    },
    {
      "epoch": 9.686838455361706,
      "grad_norm": 4.1099677085876465,
      "learning_rate": 4.1927634620531913e-05,
      "loss": 0.665,
      "step": 1061600
    },
    {
      "epoch": 9.68775093072487,
      "grad_norm": 5.07739782333374,
      "learning_rate": 4.1926874224395944e-05,
      "loss": 0.6784,
      "step": 1061700
    },
    {
      "epoch": 9.688663406088036,
      "grad_norm": 4.6538872718811035,
      "learning_rate": 4.1926113828259974e-05,
      "loss": 0.7072,
      "step": 1061800
    },
    {
      "epoch": 9.689575881451201,
      "grad_norm": 4.093661785125732,
      "learning_rate": 4.1925353432124004e-05,
      "loss": 0.6775,
      "step": 1061900
    },
    {
      "epoch": 9.690488356814367,
      "grad_norm": 4.046497344970703,
      "learning_rate": 4.192459303598803e-05,
      "loss": 0.6582,
      "step": 1062000
    },
    {
      "epoch": 9.691400832177532,
      "grad_norm": 3.0489304065704346,
      "learning_rate": 4.1923832639852064e-05,
      "loss": 0.7119,
      "step": 1062100
    },
    {
      "epoch": 9.692313307540697,
      "grad_norm": 4.07534646987915,
      "learning_rate": 4.192307224371609e-05,
      "loss": 0.6971,
      "step": 1062200
    },
    {
      "epoch": 9.69322578290386,
      "grad_norm": 3.1935081481933594,
      "learning_rate": 4.192231184758012e-05,
      "loss": 0.6575,
      "step": 1062300
    },
    {
      "epoch": 9.694138258267026,
      "grad_norm": 4.939996242523193,
      "learning_rate": 4.192155145144415e-05,
      "loss": 0.6553,
      "step": 1062400
    },
    {
      "epoch": 9.695050733630191,
      "grad_norm": 3.616520404815674,
      "learning_rate": 4.192079105530817e-05,
      "loss": 0.672,
      "step": 1062500
    },
    {
      "epoch": 9.695963208993357,
      "grad_norm": 4.151928901672363,
      "learning_rate": 4.192003065917221e-05,
      "loss": 0.6431,
      "step": 1062600
    },
    {
      "epoch": 9.696875684356522,
      "grad_norm": 3.998534679412842,
      "learning_rate": 4.191927026303623e-05,
      "loss": 0.6887,
      "step": 1062700
    },
    {
      "epoch": 9.697788159719687,
      "grad_norm": 4.566169738769531,
      "learning_rate": 4.191850986690026e-05,
      "loss": 0.6615,
      "step": 1062800
    },
    {
      "epoch": 9.698700635082853,
      "grad_norm": 4.438015937805176,
      "learning_rate": 4.191774947076429e-05,
      "loss": 0.7018,
      "step": 1062900
    },
    {
      "epoch": 9.699613110446018,
      "grad_norm": 4.431830406188965,
      "learning_rate": 4.191698907462832e-05,
      "loss": 0.662,
      "step": 1063000
    },
    {
      "epoch": 9.700525585809183,
      "grad_norm": 3.532010316848755,
      "learning_rate": 4.1916228678492344e-05,
      "loss": 0.6636,
      "step": 1063100
    },
    {
      "epoch": 9.701438061172349,
      "grad_norm": 3.482034683227539,
      "learning_rate": 4.191546828235638e-05,
      "loss": 0.7323,
      "step": 1063200
    },
    {
      "epoch": 9.702350536535514,
      "grad_norm": 3.5129106044769287,
      "learning_rate": 4.1914707886220404e-05,
      "loss": 0.715,
      "step": 1063300
    },
    {
      "epoch": 9.70326301189868,
      "grad_norm": 3.1632468700408936,
      "learning_rate": 4.1913947490084434e-05,
      "loss": 0.6814,
      "step": 1063400
    },
    {
      "epoch": 9.704175487261844,
      "grad_norm": 4.158954620361328,
      "learning_rate": 4.1913187093948464e-05,
      "loss": 0.689,
      "step": 1063500
    },
    {
      "epoch": 9.70508796262501,
      "grad_norm": 4.166699409484863,
      "learning_rate": 4.1912426697812494e-05,
      "loss": 0.6583,
      "step": 1063600
    },
    {
      "epoch": 9.706000437988175,
      "grad_norm": 3.932544231414795,
      "learning_rate": 4.1911666301676525e-05,
      "loss": 0.6536,
      "step": 1063700
    },
    {
      "epoch": 9.70691291335134,
      "grad_norm": 4.355273246765137,
      "learning_rate": 4.1910905905540555e-05,
      "loss": 0.6304,
      "step": 1063800
    },
    {
      "epoch": 9.707825388714504,
      "grad_norm": 3.9441826343536377,
      "learning_rate": 4.191014550940458e-05,
      "loss": 0.7128,
      "step": 1063900
    },
    {
      "epoch": 9.70873786407767,
      "grad_norm": 3.798729658126831,
      "learning_rate": 4.1909385113268615e-05,
      "loss": 0.7166,
      "step": 1064000
    },
    {
      "epoch": 9.709650339440834,
      "grad_norm": 3.9510438442230225,
      "learning_rate": 4.190862471713264e-05,
      "loss": 0.6818,
      "step": 1064100
    },
    {
      "epoch": 9.710562814804,
      "grad_norm": 3.837585926055908,
      "learning_rate": 4.190786432099667e-05,
      "loss": 0.6784,
      "step": 1064200
    },
    {
      "epoch": 9.711475290167165,
      "grad_norm": 3.9772307872772217,
      "learning_rate": 4.19071039248607e-05,
      "loss": 0.6852,
      "step": 1064300
    },
    {
      "epoch": 9.71238776553033,
      "grad_norm": 3.737553834915161,
      "learning_rate": 4.190634352872473e-05,
      "loss": 0.6622,
      "step": 1064400
    },
    {
      "epoch": 9.713300240893496,
      "grad_norm": 4.534098148345947,
      "learning_rate": 4.190558313258875e-05,
      "loss": 0.6669,
      "step": 1064500
    },
    {
      "epoch": 9.714212716256661,
      "grad_norm": 3.83590030670166,
      "learning_rate": 4.190482273645279e-05,
      "loss": 0.7134,
      "step": 1064600
    },
    {
      "epoch": 9.715125191619826,
      "grad_norm": 3.872997522354126,
      "learning_rate": 4.190406234031681e-05,
      "loss": 0.6506,
      "step": 1064700
    },
    {
      "epoch": 9.716037666982992,
      "grad_norm": 4.036196231842041,
      "learning_rate": 4.190330194418084e-05,
      "loss": 0.6839,
      "step": 1064800
    },
    {
      "epoch": 9.716950142346157,
      "grad_norm": 4.841969966888428,
      "learning_rate": 4.190254154804487e-05,
      "loss": 0.6767,
      "step": 1064900
    },
    {
      "epoch": 9.717862617709322,
      "grad_norm": 4.003310203552246,
      "learning_rate": 4.19017811519089e-05,
      "loss": 0.681,
      "step": 1065000
    },
    {
      "epoch": 9.718775093072487,
      "grad_norm": 3.335698127746582,
      "learning_rate": 4.190102075577293e-05,
      "loss": 0.6643,
      "step": 1065100
    },
    {
      "epoch": 9.719687568435653,
      "grad_norm": 3.7285609245300293,
      "learning_rate": 4.1900260359636955e-05,
      "loss": 0.6376,
      "step": 1065200
    },
    {
      "epoch": 9.720600043798818,
      "grad_norm": 2.729830503463745,
      "learning_rate": 4.1899499963500985e-05,
      "loss": 0.7109,
      "step": 1065300
    },
    {
      "epoch": 9.721512519161983,
      "grad_norm": 3.879340171813965,
      "learning_rate": 4.1898739567365015e-05,
      "loss": 0.6809,
      "step": 1065400
    },
    {
      "epoch": 9.722424994525149,
      "grad_norm": 3.8698570728302,
      "learning_rate": 4.1897979171229045e-05,
      "loss": 0.6839,
      "step": 1065500
    },
    {
      "epoch": 9.723337469888314,
      "grad_norm": 4.166755676269531,
      "learning_rate": 4.1897218775093076e-05,
      "loss": 0.6986,
      "step": 1065600
    },
    {
      "epoch": 9.724249945251477,
      "grad_norm": 4.644587516784668,
      "learning_rate": 4.1896458378957106e-05,
      "loss": 0.6648,
      "step": 1065700
    },
    {
      "epoch": 9.725162420614643,
      "grad_norm": 3.9673092365264893,
      "learning_rate": 4.189569798282113e-05,
      "loss": 0.6952,
      "step": 1065800
    },
    {
      "epoch": 9.726074895977808,
      "grad_norm": 4.034038543701172,
      "learning_rate": 4.1894937586685166e-05,
      "loss": 0.6756,
      "step": 1065900
    },
    {
      "epoch": 9.726987371340973,
      "grad_norm": 3.3599419593811035,
      "learning_rate": 4.189417719054919e-05,
      "loss": 0.658,
      "step": 1066000
    },
    {
      "epoch": 9.727899846704139,
      "grad_norm": 4.23164176940918,
      "learning_rate": 4.189341679441322e-05,
      "loss": 0.653,
      "step": 1066100
    },
    {
      "epoch": 9.728812322067304,
      "grad_norm": 4.437072277069092,
      "learning_rate": 4.189265639827725e-05,
      "loss": 0.7058,
      "step": 1066200
    },
    {
      "epoch": 9.72972479743047,
      "grad_norm": 4.788966655731201,
      "learning_rate": 4.189189600214128e-05,
      "loss": 0.6934,
      "step": 1066300
    },
    {
      "epoch": 9.730637272793635,
      "grad_norm": 3.7474687099456787,
      "learning_rate": 4.18911356060053e-05,
      "loss": 0.7153,
      "step": 1066400
    },
    {
      "epoch": 9.7315497481568,
      "grad_norm": 4.278707981109619,
      "learning_rate": 4.189037520986934e-05,
      "loss": 0.71,
      "step": 1066500
    },
    {
      "epoch": 9.732462223519965,
      "grad_norm": 3.660294771194458,
      "learning_rate": 4.188961481373336e-05,
      "loss": 0.6475,
      "step": 1066600
    },
    {
      "epoch": 9.73337469888313,
      "grad_norm": 3.8177194595336914,
      "learning_rate": 4.188885441759739e-05,
      "loss": 0.6814,
      "step": 1066700
    },
    {
      "epoch": 9.734287174246296,
      "grad_norm": 4.334066867828369,
      "learning_rate": 4.188809402146142e-05,
      "loss": 0.6907,
      "step": 1066800
    },
    {
      "epoch": 9.735199649609461,
      "grad_norm": 3.9861886501312256,
      "learning_rate": 4.188733362532545e-05,
      "loss": 0.6839,
      "step": 1066900
    },
    {
      "epoch": 9.736112124972626,
      "grad_norm": 3.6913440227508545,
      "learning_rate": 4.188657322918948e-05,
      "loss": 0.6951,
      "step": 1067000
    },
    {
      "epoch": 9.737024600335792,
      "grad_norm": 4.310192108154297,
      "learning_rate": 4.188581283305351e-05,
      "loss": 0.6779,
      "step": 1067100
    },
    {
      "epoch": 9.737937075698957,
      "grad_norm": 4.041154384613037,
      "learning_rate": 4.1885052436917536e-05,
      "loss": 0.7051,
      "step": 1067200
    },
    {
      "epoch": 9.73884955106212,
      "grad_norm": 4.028541088104248,
      "learning_rate": 4.188429204078157e-05,
      "loss": 0.6363,
      "step": 1067300
    },
    {
      "epoch": 9.739762026425286,
      "grad_norm": 4.498058319091797,
      "learning_rate": 4.1883531644645596e-05,
      "loss": 0.6753,
      "step": 1067400
    },
    {
      "epoch": 9.740674501788451,
      "grad_norm": 4.1611528396606445,
      "learning_rate": 4.1882771248509626e-05,
      "loss": 0.6797,
      "step": 1067500
    },
    {
      "epoch": 9.741586977151616,
      "grad_norm": 3.9858245849609375,
      "learning_rate": 4.1882010852373657e-05,
      "loss": 0.6844,
      "step": 1067600
    },
    {
      "epoch": 9.742499452514782,
      "grad_norm": 4.6148600578308105,
      "learning_rate": 4.1881250456237687e-05,
      "loss": 0.6602,
      "step": 1067700
    },
    {
      "epoch": 9.743411927877947,
      "grad_norm": 3.8077430725097656,
      "learning_rate": 4.188049006010171e-05,
      "loss": 0.6797,
      "step": 1067800
    },
    {
      "epoch": 9.744324403241112,
      "grad_norm": 4.297841548919678,
      "learning_rate": 4.187972966396574e-05,
      "loss": 0.6845,
      "step": 1067900
    },
    {
      "epoch": 9.745236878604278,
      "grad_norm": 4.838282585144043,
      "learning_rate": 4.187896926782977e-05,
      "loss": 0.689,
      "step": 1068000
    },
    {
      "epoch": 9.746149353967443,
      "grad_norm": 4.280093669891357,
      "learning_rate": 4.18782088716938e-05,
      "loss": 0.6834,
      "step": 1068100
    },
    {
      "epoch": 9.747061829330608,
      "grad_norm": 4.675840377807617,
      "learning_rate": 4.187744847555783e-05,
      "loss": 0.6872,
      "step": 1068200
    },
    {
      "epoch": 9.747974304693773,
      "grad_norm": 3.7948310375213623,
      "learning_rate": 4.1876688079421853e-05,
      "loss": 0.6445,
      "step": 1068300
    },
    {
      "epoch": 9.748886780056939,
      "grad_norm": 3.879864454269409,
      "learning_rate": 4.187592768328589e-05,
      "loss": 0.6389,
      "step": 1068400
    },
    {
      "epoch": 9.749799255420104,
      "grad_norm": 4.317088603973389,
      "learning_rate": 4.1875167287149914e-05,
      "loss": 0.7085,
      "step": 1068500
    },
    {
      "epoch": 9.75071173078327,
      "grad_norm": 4.540278434753418,
      "learning_rate": 4.1874406891013944e-05,
      "loss": 0.6744,
      "step": 1068600
    },
    {
      "epoch": 9.751624206146435,
      "grad_norm": 3.069707155227661,
      "learning_rate": 4.1873646494877974e-05,
      "loss": 0.6883,
      "step": 1068700
    },
    {
      "epoch": 9.7525366815096,
      "grad_norm": 4.139070987701416,
      "learning_rate": 4.1872886098742004e-05,
      "loss": 0.6875,
      "step": 1068800
    },
    {
      "epoch": 9.753449156872765,
      "grad_norm": 3.364851474761963,
      "learning_rate": 4.187212570260603e-05,
      "loss": 0.6656,
      "step": 1068900
    },
    {
      "epoch": 9.75436163223593,
      "grad_norm": 4.868878364562988,
      "learning_rate": 4.1871365306470064e-05,
      "loss": 0.6525,
      "step": 1069000
    },
    {
      "epoch": 9.755274107599094,
      "grad_norm": 4.236691474914551,
      "learning_rate": 4.187060491033409e-05,
      "loss": 0.7091,
      "step": 1069100
    },
    {
      "epoch": 9.75618658296226,
      "grad_norm": 3.5051474571228027,
      "learning_rate": 4.186984451419812e-05,
      "loss": 0.7154,
      "step": 1069200
    },
    {
      "epoch": 9.757099058325425,
      "grad_norm": 3.14882493019104,
      "learning_rate": 4.186908411806215e-05,
      "loss": 0.6773,
      "step": 1069300
    },
    {
      "epoch": 9.75801153368859,
      "grad_norm": 4.103671073913574,
      "learning_rate": 4.186832372192618e-05,
      "loss": 0.6826,
      "step": 1069400
    },
    {
      "epoch": 9.758924009051755,
      "grad_norm": 4.309864044189453,
      "learning_rate": 4.186756332579021e-05,
      "loss": 0.7132,
      "step": 1069500
    },
    {
      "epoch": 9.75983648441492,
      "grad_norm": 3.999403238296509,
      "learning_rate": 4.186680292965424e-05,
      "loss": 0.6705,
      "step": 1069600
    },
    {
      "epoch": 9.760748959778086,
      "grad_norm": 3.775251626968384,
      "learning_rate": 4.186604253351826e-05,
      "loss": 0.6348,
      "step": 1069700
    },
    {
      "epoch": 9.761661435141251,
      "grad_norm": 4.288086414337158,
      "learning_rate": 4.18652821373823e-05,
      "loss": 0.6966,
      "step": 1069800
    },
    {
      "epoch": 9.762573910504416,
      "grad_norm": 4.032319068908691,
      "learning_rate": 4.186452174124632e-05,
      "loss": 0.7402,
      "step": 1069900
    },
    {
      "epoch": 9.763486385867582,
      "grad_norm": 4.648684501647949,
      "learning_rate": 4.186376134511035e-05,
      "loss": 0.708,
      "step": 1070000
    },
    {
      "epoch": 9.764398861230747,
      "grad_norm": 2.190603256225586,
      "learning_rate": 4.186300094897438e-05,
      "loss": 0.703,
      "step": 1070100
    },
    {
      "epoch": 9.765311336593912,
      "grad_norm": 4.0288825035095215,
      "learning_rate": 4.186224055283841e-05,
      "loss": 0.6915,
      "step": 1070200
    },
    {
      "epoch": 9.766223811957078,
      "grad_norm": 4.220302104949951,
      "learning_rate": 4.1861480156702434e-05,
      "loss": 0.6636,
      "step": 1070300
    },
    {
      "epoch": 9.767136287320243,
      "grad_norm": 3.6990349292755127,
      "learning_rate": 4.186071976056647e-05,
      "loss": 0.6618,
      "step": 1070400
    },
    {
      "epoch": 9.768048762683408,
      "grad_norm": 3.287961721420288,
      "learning_rate": 4.1859959364430495e-05,
      "loss": 0.6944,
      "step": 1070500
    },
    {
      "epoch": 9.768961238046574,
      "grad_norm": 4.083730220794678,
      "learning_rate": 4.1859198968294525e-05,
      "loss": 0.6739,
      "step": 1070600
    },
    {
      "epoch": 9.769873713409737,
      "grad_norm": 2.6838536262512207,
      "learning_rate": 4.1858438572158555e-05,
      "loss": 0.6908,
      "step": 1070700
    },
    {
      "epoch": 9.770786188772902,
      "grad_norm": 4.730576038360596,
      "learning_rate": 4.185767817602258e-05,
      "loss": 0.7103,
      "step": 1070800
    },
    {
      "epoch": 9.771698664136068,
      "grad_norm": 3.830930233001709,
      "learning_rate": 4.1856917779886615e-05,
      "loss": 0.6831,
      "step": 1070900
    },
    {
      "epoch": 9.772611139499233,
      "grad_norm": 2.629645586013794,
      "learning_rate": 4.185615738375064e-05,
      "loss": 0.6832,
      "step": 1071000
    },
    {
      "epoch": 9.773523614862398,
      "grad_norm": 4.555361270904541,
      "learning_rate": 4.185539698761467e-05,
      "loss": 0.7151,
      "step": 1071100
    },
    {
      "epoch": 9.774436090225564,
      "grad_norm": 3.3087306022644043,
      "learning_rate": 4.18546365914787e-05,
      "loss": 0.6873,
      "step": 1071200
    },
    {
      "epoch": 9.775348565588729,
      "grad_norm": 4.264929294586182,
      "learning_rate": 4.185387619534273e-05,
      "loss": 0.6781,
      "step": 1071300
    },
    {
      "epoch": 9.776261040951894,
      "grad_norm": 4.27105188369751,
      "learning_rate": 4.185311579920675e-05,
      "loss": 0.6921,
      "step": 1071400
    },
    {
      "epoch": 9.77717351631506,
      "grad_norm": 3.347308397293091,
      "learning_rate": 4.185235540307079e-05,
      "loss": 0.6809,
      "step": 1071500
    },
    {
      "epoch": 9.778085991678225,
      "grad_norm": 4.46964693069458,
      "learning_rate": 4.185159500693481e-05,
      "loss": 0.713,
      "step": 1071600
    },
    {
      "epoch": 9.77899846704139,
      "grad_norm": 4.4622111320495605,
      "learning_rate": 4.185083461079884e-05,
      "loss": 0.7255,
      "step": 1071700
    },
    {
      "epoch": 9.779910942404555,
      "grad_norm": 3.097506046295166,
      "learning_rate": 4.185007421466287e-05,
      "loss": 0.7227,
      "step": 1071800
    },
    {
      "epoch": 9.78082341776772,
      "grad_norm": 4.088296890258789,
      "learning_rate": 4.18493138185269e-05,
      "loss": 0.7057,
      "step": 1071900
    },
    {
      "epoch": 9.781735893130886,
      "grad_norm": 4.35189151763916,
      "learning_rate": 4.184855342239093e-05,
      "loss": 0.6975,
      "step": 1072000
    },
    {
      "epoch": 9.782648368494051,
      "grad_norm": 4.714570045471191,
      "learning_rate": 4.184779302625496e-05,
      "loss": 0.6775,
      "step": 1072100
    },
    {
      "epoch": 9.783560843857217,
      "grad_norm": 3.7683706283569336,
      "learning_rate": 4.1847032630118985e-05,
      "loss": 0.707,
      "step": 1072200
    },
    {
      "epoch": 9.784473319220382,
      "grad_norm": 3.5355465412139893,
      "learning_rate": 4.184627223398302e-05,
      "loss": 0.6956,
      "step": 1072300
    },
    {
      "epoch": 9.785385794583547,
      "grad_norm": 4.836661338806152,
      "learning_rate": 4.1845511837847046e-05,
      "loss": 0.7016,
      "step": 1072400
    },
    {
      "epoch": 9.78629826994671,
      "grad_norm": 3.774085521697998,
      "learning_rate": 4.1844751441711076e-05,
      "loss": 0.715,
      "step": 1072500
    },
    {
      "epoch": 9.787210745309876,
      "grad_norm": 3.9787843227386475,
      "learning_rate": 4.1843991045575106e-05,
      "loss": 0.6766,
      "step": 1072600
    },
    {
      "epoch": 9.788123220673041,
      "grad_norm": 5.333104610443115,
      "learning_rate": 4.1843230649439136e-05,
      "loss": 0.6874,
      "step": 1072700
    },
    {
      "epoch": 9.789035696036207,
      "grad_norm": 4.477747917175293,
      "learning_rate": 4.184247025330316e-05,
      "loss": 0.6723,
      "step": 1072800
    },
    {
      "epoch": 9.789948171399372,
      "grad_norm": 3.032205581665039,
      "learning_rate": 4.1841709857167196e-05,
      "loss": 0.683,
      "step": 1072900
    },
    {
      "epoch": 9.790860646762537,
      "grad_norm": 4.473459243774414,
      "learning_rate": 4.184094946103122e-05,
      "loss": 0.7047,
      "step": 1073000
    },
    {
      "epoch": 9.791773122125702,
      "grad_norm": 3.8464105129241943,
      "learning_rate": 4.184018906489525e-05,
      "loss": 0.6659,
      "step": 1073100
    },
    {
      "epoch": 9.792685597488868,
      "grad_norm": 4.495197772979736,
      "learning_rate": 4.183942866875928e-05,
      "loss": 0.6858,
      "step": 1073200
    },
    {
      "epoch": 9.793598072852033,
      "grad_norm": 4.659499645233154,
      "learning_rate": 4.183866827262331e-05,
      "loss": 0.6832,
      "step": 1073300
    },
    {
      "epoch": 9.794510548215198,
      "grad_norm": 4.937007904052734,
      "learning_rate": 4.183790787648734e-05,
      "loss": 0.7056,
      "step": 1073400
    },
    {
      "epoch": 9.795423023578364,
      "grad_norm": 5.356139659881592,
      "learning_rate": 4.183714748035137e-05,
      "loss": 0.705,
      "step": 1073500
    },
    {
      "epoch": 9.796335498941529,
      "grad_norm": 4.171779632568359,
      "learning_rate": 4.183638708421539e-05,
      "loss": 0.733,
      "step": 1073600
    },
    {
      "epoch": 9.797247974304694,
      "grad_norm": 3.81846284866333,
      "learning_rate": 4.183562668807942e-05,
      "loss": 0.6929,
      "step": 1073700
    },
    {
      "epoch": 9.79816044966786,
      "grad_norm": 3.701054334640503,
      "learning_rate": 4.183486629194345e-05,
      "loss": 0.671,
      "step": 1073800
    },
    {
      "epoch": 9.799072925031025,
      "grad_norm": 3.5856521129608154,
      "learning_rate": 4.1834105895807476e-05,
      "loss": 0.676,
      "step": 1073900
    },
    {
      "epoch": 9.799985400394188,
      "grad_norm": 4.248843669891357,
      "learning_rate": 4.183334549967151e-05,
      "loss": 0.7206,
      "step": 1074000
    },
    {
      "epoch": 9.800897875757354,
      "grad_norm": 4.307608604431152,
      "learning_rate": 4.1832585103535536e-05,
      "loss": 0.6774,
      "step": 1074100
    },
    {
      "epoch": 9.801810351120519,
      "grad_norm": 2.4320456981658936,
      "learning_rate": 4.1831824707399566e-05,
      "loss": 0.6914,
      "step": 1074200
    },
    {
      "epoch": 9.802722826483684,
      "grad_norm": 3.8462095260620117,
      "learning_rate": 4.1831064311263596e-05,
      "loss": 0.7282,
      "step": 1074300
    },
    {
      "epoch": 9.80363530184685,
      "grad_norm": 4.869100093841553,
      "learning_rate": 4.1830303915127627e-05,
      "loss": 0.706,
      "step": 1074400
    },
    {
      "epoch": 9.804547777210015,
      "grad_norm": 3.9670910835266113,
      "learning_rate": 4.182954351899166e-05,
      "loss": 0.6863,
      "step": 1074500
    },
    {
      "epoch": 9.80546025257318,
      "grad_norm": 3.047347068786621,
      "learning_rate": 4.182878312285569e-05,
      "loss": 0.7064,
      "step": 1074600
    },
    {
      "epoch": 9.806372727936346,
      "grad_norm": 3.7152090072631836,
      "learning_rate": 4.182802272671971e-05,
      "loss": 0.7149,
      "step": 1074700
    },
    {
      "epoch": 9.80728520329951,
      "grad_norm": 4.340177536010742,
      "learning_rate": 4.182726233058375e-05,
      "loss": 0.6938,
      "step": 1074800
    },
    {
      "epoch": 9.808197678662676,
      "grad_norm": 4.365852355957031,
      "learning_rate": 4.182650193444777e-05,
      "loss": 0.7038,
      "step": 1074900
    },
    {
      "epoch": 9.809110154025841,
      "grad_norm": 3.845046043395996,
      "learning_rate": 4.18257415383118e-05,
      "loss": 0.6976,
      "step": 1075000
    },
    {
      "epoch": 9.810022629389007,
      "grad_norm": 2.691045045852661,
      "learning_rate": 4.182498114217583e-05,
      "loss": 0.6581,
      "step": 1075100
    },
    {
      "epoch": 9.810935104752172,
      "grad_norm": 3.557662010192871,
      "learning_rate": 4.182422074603986e-05,
      "loss": 0.6656,
      "step": 1075200
    },
    {
      "epoch": 9.811847580115337,
      "grad_norm": 4.238551616668701,
      "learning_rate": 4.1823460349903884e-05,
      "loss": 0.6733,
      "step": 1075300
    },
    {
      "epoch": 9.812760055478503,
      "grad_norm": 4.070290565490723,
      "learning_rate": 4.182269995376792e-05,
      "loss": 0.7182,
      "step": 1075400
    },
    {
      "epoch": 9.813672530841668,
      "grad_norm": 4.24527645111084,
      "learning_rate": 4.1821939557631944e-05,
      "loss": 0.6706,
      "step": 1075500
    },
    {
      "epoch": 9.814585006204833,
      "grad_norm": 3.646772861480713,
      "learning_rate": 4.1821179161495974e-05,
      "loss": 0.6809,
      "step": 1075600
    },
    {
      "epoch": 9.815497481567999,
      "grad_norm": 4.201425552368164,
      "learning_rate": 4.1820418765360004e-05,
      "loss": 0.7017,
      "step": 1075700
    },
    {
      "epoch": 9.816409956931162,
      "grad_norm": 4.008927822113037,
      "learning_rate": 4.1819658369224034e-05,
      "loss": 0.7108,
      "step": 1075800
    },
    {
      "epoch": 9.817322432294327,
      "grad_norm": 4.349188804626465,
      "learning_rate": 4.1818897973088064e-05,
      "loss": 0.6843,
      "step": 1075900
    },
    {
      "epoch": 9.818234907657493,
      "grad_norm": 3.7651360034942627,
      "learning_rate": 4.1818137576952094e-05,
      "loss": 0.6583,
      "step": 1076000
    },
    {
      "epoch": 9.819147383020658,
      "grad_norm": 4.020369529724121,
      "learning_rate": 4.181737718081612e-05,
      "loss": 0.668,
      "step": 1076100
    },
    {
      "epoch": 9.820059858383823,
      "grad_norm": 3.600757360458374,
      "learning_rate": 4.1816616784680154e-05,
      "loss": 0.6597,
      "step": 1076200
    },
    {
      "epoch": 9.820972333746989,
      "grad_norm": 4.261402606964111,
      "learning_rate": 4.181585638854418e-05,
      "loss": 0.6679,
      "step": 1076300
    },
    {
      "epoch": 9.821884809110154,
      "grad_norm": 4.175405979156494,
      "learning_rate": 4.181509599240821e-05,
      "loss": 0.6676,
      "step": 1076400
    },
    {
      "epoch": 9.82279728447332,
      "grad_norm": 3.7964932918548584,
      "learning_rate": 4.181433559627224e-05,
      "loss": 0.6812,
      "step": 1076500
    },
    {
      "epoch": 9.823709759836484,
      "grad_norm": 3.5396568775177,
      "learning_rate": 4.181357520013626e-05,
      "loss": 0.6965,
      "step": 1076600
    },
    {
      "epoch": 9.82462223519965,
      "grad_norm": 2.9596166610717773,
      "learning_rate": 4.181281480400029e-05,
      "loss": 0.6637,
      "step": 1076700
    },
    {
      "epoch": 9.825534710562815,
      "grad_norm": 4.009225845336914,
      "learning_rate": 4.181205440786432e-05,
      "loss": 0.6754,
      "step": 1076800
    },
    {
      "epoch": 9.82644718592598,
      "grad_norm": 3.543701410293579,
      "learning_rate": 4.181129401172835e-05,
      "loss": 0.7389,
      "step": 1076900
    },
    {
      "epoch": 9.827359661289146,
      "grad_norm": 3.7933387756347656,
      "learning_rate": 4.181053361559238e-05,
      "loss": 0.6789,
      "step": 1077000
    },
    {
      "epoch": 9.828272136652311,
      "grad_norm": 4.033106327056885,
      "learning_rate": 4.180977321945641e-05,
      "loss": 0.7032,
      "step": 1077100
    },
    {
      "epoch": 9.829184612015476,
      "grad_norm": 4.440486907958984,
      "learning_rate": 4.1809012823320435e-05,
      "loss": 0.7009,
      "step": 1077200
    },
    {
      "epoch": 9.830097087378642,
      "grad_norm": 4.483726978302002,
      "learning_rate": 4.180825242718447e-05,
      "loss": 0.6957,
      "step": 1077300
    },
    {
      "epoch": 9.831009562741805,
      "grad_norm": 4.532613277435303,
      "learning_rate": 4.1807492031048495e-05,
      "loss": 0.6637,
      "step": 1077400
    },
    {
      "epoch": 9.83192203810497,
      "grad_norm": 4.516092777252197,
      "learning_rate": 4.1806731634912525e-05,
      "loss": 0.6774,
      "step": 1077500
    },
    {
      "epoch": 9.832834513468136,
      "grad_norm": 3.674896240234375,
      "learning_rate": 4.1805971238776555e-05,
      "loss": 0.6804,
      "step": 1077600
    },
    {
      "epoch": 9.833746988831301,
      "grad_norm": 4.666006565093994,
      "learning_rate": 4.1805210842640585e-05,
      "loss": 0.6697,
      "step": 1077700
    },
    {
      "epoch": 9.834659464194466,
      "grad_norm": 3.386629581451416,
      "learning_rate": 4.1804450446504615e-05,
      "loss": 0.6669,
      "step": 1077800
    },
    {
      "epoch": 9.835571939557632,
      "grad_norm": 3.922884941101074,
      "learning_rate": 4.1803690050368645e-05,
      "loss": 0.6762,
      "step": 1077900
    },
    {
      "epoch": 9.836484414920797,
      "grad_norm": 4.057193279266357,
      "learning_rate": 4.180292965423267e-05,
      "loss": 0.664,
      "step": 1078000
    },
    {
      "epoch": 9.837396890283962,
      "grad_norm": 4.614736557006836,
      "learning_rate": 4.18021692580967e-05,
      "loss": 0.6504,
      "step": 1078100
    },
    {
      "epoch": 9.838309365647127,
      "grad_norm": 3.8318333625793457,
      "learning_rate": 4.180140886196073e-05,
      "loss": 0.6869,
      "step": 1078200
    },
    {
      "epoch": 9.839221841010293,
      "grad_norm": 4.725569725036621,
      "learning_rate": 4.180064846582476e-05,
      "loss": 0.7064,
      "step": 1078300
    },
    {
      "epoch": 9.840134316373458,
      "grad_norm": 4.434678077697754,
      "learning_rate": 4.179988806968879e-05,
      "loss": 0.6982,
      "step": 1078400
    },
    {
      "epoch": 9.841046791736623,
      "grad_norm": 3.9052178859710693,
      "learning_rate": 4.179912767355282e-05,
      "loss": 0.6926,
      "step": 1078500
    },
    {
      "epoch": 9.841959267099789,
      "grad_norm": 3.0446274280548096,
      "learning_rate": 4.179836727741684e-05,
      "loss": 0.6929,
      "step": 1078600
    },
    {
      "epoch": 9.842871742462954,
      "grad_norm": 4.953341960906982,
      "learning_rate": 4.179760688128088e-05,
      "loss": 0.6842,
      "step": 1078700
    },
    {
      "epoch": 9.84378421782612,
      "grad_norm": 4.305928707122803,
      "learning_rate": 4.17968464851449e-05,
      "loss": 0.7085,
      "step": 1078800
    },
    {
      "epoch": 9.844696693189285,
      "grad_norm": 2.816901206970215,
      "learning_rate": 4.179608608900893e-05,
      "loss": 0.7047,
      "step": 1078900
    },
    {
      "epoch": 9.84560916855245,
      "grad_norm": 3.7608821392059326,
      "learning_rate": 4.179532569287296e-05,
      "loss": 0.6572,
      "step": 1079000
    },
    {
      "epoch": 9.846521643915615,
      "grad_norm": 3.5631749629974365,
      "learning_rate": 4.179456529673699e-05,
      "loss": 0.662,
      "step": 1079100
    },
    {
      "epoch": 9.847434119278779,
      "grad_norm": 3.7217915058135986,
      "learning_rate": 4.179380490060102e-05,
      "loss": 0.6984,
      "step": 1079200
    },
    {
      "epoch": 9.848346594641944,
      "grad_norm": 3.0720512866973877,
      "learning_rate": 4.1793044504465046e-05,
      "loss": 0.7199,
      "step": 1079300
    },
    {
      "epoch": 9.84925907000511,
      "grad_norm": 3.695255756378174,
      "learning_rate": 4.1792284108329076e-05,
      "loss": 0.6918,
      "step": 1079400
    },
    {
      "epoch": 9.850171545368275,
      "grad_norm": 3.5363595485687256,
      "learning_rate": 4.1791523712193106e-05,
      "loss": 0.6692,
      "step": 1079500
    },
    {
      "epoch": 9.85108402073144,
      "grad_norm": 3.6338582038879395,
      "learning_rate": 4.1790763316057136e-05,
      "loss": 0.6936,
      "step": 1079600
    },
    {
      "epoch": 9.851996496094605,
      "grad_norm": 3.4482288360595703,
      "learning_rate": 4.179000291992116e-05,
      "loss": 0.7136,
      "step": 1079700
    },
    {
      "epoch": 9.85290897145777,
      "grad_norm": 4.164356708526611,
      "learning_rate": 4.1789242523785196e-05,
      "loss": 0.6827,
      "step": 1079800
    },
    {
      "epoch": 9.853821446820936,
      "grad_norm": 4.853909492492676,
      "learning_rate": 4.178848212764922e-05,
      "loss": 0.6826,
      "step": 1079900
    },
    {
      "epoch": 9.854733922184101,
      "grad_norm": 3.7776477336883545,
      "learning_rate": 4.178772173151325e-05,
      "loss": 0.6926,
      "step": 1080000
    },
    {
      "epoch": 9.855646397547266,
      "grad_norm": 4.476935863494873,
      "learning_rate": 4.178696133537728e-05,
      "loss": 0.6797,
      "step": 1080100
    },
    {
      "epoch": 9.856558872910432,
      "grad_norm": 4.593636989593506,
      "learning_rate": 4.178620093924131e-05,
      "loss": 0.6837,
      "step": 1080200
    },
    {
      "epoch": 9.857471348273597,
      "grad_norm": 4.043949604034424,
      "learning_rate": 4.178544054310534e-05,
      "loss": 0.6592,
      "step": 1080300
    },
    {
      "epoch": 9.858383823636762,
      "grad_norm": 4.319063663482666,
      "learning_rate": 4.178468014696937e-05,
      "loss": 0.6895,
      "step": 1080400
    },
    {
      "epoch": 9.859296298999928,
      "grad_norm": 3.3835883140563965,
      "learning_rate": 4.178391975083339e-05,
      "loss": 0.6756,
      "step": 1080500
    },
    {
      "epoch": 9.860208774363093,
      "grad_norm": 3.8298180103302,
      "learning_rate": 4.178315935469743e-05,
      "loss": 0.6906,
      "step": 1080600
    },
    {
      "epoch": 9.861121249726258,
      "grad_norm": 3.656163454055786,
      "learning_rate": 4.178239895856145e-05,
      "loss": 0.6753,
      "step": 1080700
    },
    {
      "epoch": 9.862033725089422,
      "grad_norm": 4.362341403961182,
      "learning_rate": 4.178163856242548e-05,
      "loss": 0.6907,
      "step": 1080800
    },
    {
      "epoch": 9.862946200452587,
      "grad_norm": 4.376436710357666,
      "learning_rate": 4.178087816628951e-05,
      "loss": 0.6649,
      "step": 1080900
    },
    {
      "epoch": 9.863858675815752,
      "grad_norm": 3.990239143371582,
      "learning_rate": 4.178011777015354e-05,
      "loss": 0.7042,
      "step": 1081000
    },
    {
      "epoch": 9.864771151178918,
      "grad_norm": 4.455503463745117,
      "learning_rate": 4.1779357374017567e-05,
      "loss": 0.6881,
      "step": 1081100
    },
    {
      "epoch": 9.865683626542083,
      "grad_norm": 3.691786289215088,
      "learning_rate": 4.17785969778816e-05,
      "loss": 0.6766,
      "step": 1081200
    },
    {
      "epoch": 9.866596101905248,
      "grad_norm": 4.2812180519104,
      "learning_rate": 4.177783658174563e-05,
      "loss": 0.7374,
      "step": 1081300
    },
    {
      "epoch": 9.867508577268413,
      "grad_norm": 3.307929277420044,
      "learning_rate": 4.177707618560966e-05,
      "loss": 0.6988,
      "step": 1081400
    },
    {
      "epoch": 9.868421052631579,
      "grad_norm": 4.2818145751953125,
      "learning_rate": 4.177631578947369e-05,
      "loss": 0.7123,
      "step": 1081500
    },
    {
      "epoch": 9.869333527994744,
      "grad_norm": 4.513278961181641,
      "learning_rate": 4.177555539333772e-05,
      "loss": 0.6732,
      "step": 1081600
    },
    {
      "epoch": 9.87024600335791,
      "grad_norm": 4.131185054779053,
      "learning_rate": 4.177479499720175e-05,
      "loss": 0.7223,
      "step": 1081700
    },
    {
      "epoch": 9.871158478721075,
      "grad_norm": 2.6593916416168213,
      "learning_rate": 4.177403460106578e-05,
      "loss": 0.6598,
      "step": 1081800
    },
    {
      "epoch": 9.87207095408424,
      "grad_norm": 3.519040107727051,
      "learning_rate": 4.17732742049298e-05,
      "loss": 0.6558,
      "step": 1081900
    },
    {
      "epoch": 9.872983429447405,
      "grad_norm": 3.612779140472412,
      "learning_rate": 4.177251380879384e-05,
      "loss": 0.6889,
      "step": 1082000
    },
    {
      "epoch": 9.87389590481057,
      "grad_norm": 4.206757545471191,
      "learning_rate": 4.177175341265786e-05,
      "loss": 0.6782,
      "step": 1082100
    },
    {
      "epoch": 9.874808380173736,
      "grad_norm": 4.057595729827881,
      "learning_rate": 4.1770993016521884e-05,
      "loss": 0.6701,
      "step": 1082200
    },
    {
      "epoch": 9.875720855536901,
      "grad_norm": 3.8856046199798584,
      "learning_rate": 4.177023262038592e-05,
      "loss": 0.6872,
      "step": 1082300
    },
    {
      "epoch": 9.876633330900066,
      "grad_norm": 3.956207513809204,
      "learning_rate": 4.1769472224249944e-05,
      "loss": 0.6627,
      "step": 1082400
    },
    {
      "epoch": 9.877545806263232,
      "grad_norm": 3.5118513107299805,
      "learning_rate": 4.1768711828113974e-05,
      "loss": 0.686,
      "step": 1082500
    },
    {
      "epoch": 9.878458281626395,
      "grad_norm": 3.9782936573028564,
      "learning_rate": 4.1767951431978004e-05,
      "loss": 0.6974,
      "step": 1082600
    },
    {
      "epoch": 9.87937075698956,
      "grad_norm": 3.7906932830810547,
      "learning_rate": 4.1767191035842034e-05,
      "loss": 0.6982,
      "step": 1082700
    },
    {
      "epoch": 9.880283232352726,
      "grad_norm": 4.817989826202393,
      "learning_rate": 4.1766430639706064e-05,
      "loss": 0.7062,
      "step": 1082800
    },
    {
      "epoch": 9.881195707715891,
      "grad_norm": 4.199226379394531,
      "learning_rate": 4.1765670243570094e-05,
      "loss": 0.7089,
      "step": 1082900
    },
    {
      "epoch": 9.882108183079056,
      "grad_norm": 3.7058656215667725,
      "learning_rate": 4.176490984743412e-05,
      "loss": 0.6629,
      "step": 1083000
    },
    {
      "epoch": 9.883020658442222,
      "grad_norm": 4.20803689956665,
      "learning_rate": 4.1764149451298154e-05,
      "loss": 0.6719,
      "step": 1083100
    },
    {
      "epoch": 9.883933133805387,
      "grad_norm": 4.193877220153809,
      "learning_rate": 4.176338905516218e-05,
      "loss": 0.6997,
      "step": 1083200
    },
    {
      "epoch": 9.884845609168552,
      "grad_norm": 3.838634490966797,
      "learning_rate": 4.176262865902621e-05,
      "loss": 0.6876,
      "step": 1083300
    },
    {
      "epoch": 9.885758084531718,
      "grad_norm": 4.541000843048096,
      "learning_rate": 4.176186826289024e-05,
      "loss": 0.7148,
      "step": 1083400
    },
    {
      "epoch": 9.886670559894883,
      "grad_norm": 4.327784061431885,
      "learning_rate": 4.176110786675427e-05,
      "loss": 0.6965,
      "step": 1083500
    },
    {
      "epoch": 9.887583035258048,
      "grad_norm": 4.037759304046631,
      "learning_rate": 4.176034747061829e-05,
      "loss": 0.6996,
      "step": 1083600
    },
    {
      "epoch": 9.888495510621214,
      "grad_norm": 3.5808568000793457,
      "learning_rate": 4.175958707448233e-05,
      "loss": 0.7122,
      "step": 1083700
    },
    {
      "epoch": 9.889407985984379,
      "grad_norm": 4.112850189208984,
      "learning_rate": 4.175882667834635e-05,
      "loss": 0.6854,
      "step": 1083800
    },
    {
      "epoch": 9.890320461347544,
      "grad_norm": 4.533609390258789,
      "learning_rate": 4.175806628221038e-05,
      "loss": 0.7147,
      "step": 1083900
    },
    {
      "epoch": 9.89123293671071,
      "grad_norm": 4.154815673828125,
      "learning_rate": 4.175730588607441e-05,
      "loss": 0.6983,
      "step": 1084000
    },
    {
      "epoch": 9.892145412073875,
      "grad_norm": 4.079634189605713,
      "learning_rate": 4.175654548993844e-05,
      "loss": 0.6791,
      "step": 1084100
    },
    {
      "epoch": 9.893057887437038,
      "grad_norm": 4.862916469573975,
      "learning_rate": 4.175578509380247e-05,
      "loss": 0.702,
      "step": 1084200
    },
    {
      "epoch": 9.893970362800204,
      "grad_norm": 3.1713552474975586,
      "learning_rate": 4.17550246976665e-05,
      "loss": 0.7043,
      "step": 1084300
    },
    {
      "epoch": 9.894882838163369,
      "grad_norm": 4.301878929138184,
      "learning_rate": 4.1754264301530525e-05,
      "loss": 0.6478,
      "step": 1084400
    },
    {
      "epoch": 9.895795313526534,
      "grad_norm": 3.9492499828338623,
      "learning_rate": 4.175350390539456e-05,
      "loss": 0.7329,
      "step": 1084500
    },
    {
      "epoch": 9.8967077888897,
      "grad_norm": 4.099228858947754,
      "learning_rate": 4.1752743509258585e-05,
      "loss": 0.7095,
      "step": 1084600
    },
    {
      "epoch": 9.897620264252865,
      "grad_norm": 3.5460424423217773,
      "learning_rate": 4.1751983113122615e-05,
      "loss": 0.681,
      "step": 1084700
    },
    {
      "epoch": 9.89853273961603,
      "grad_norm": 3.620833158493042,
      "learning_rate": 4.1751222716986645e-05,
      "loss": 0.6676,
      "step": 1084800
    },
    {
      "epoch": 9.899445214979195,
      "grad_norm": 2.871201992034912,
      "learning_rate": 4.1750462320850675e-05,
      "loss": 0.654,
      "step": 1084900
    },
    {
      "epoch": 9.90035769034236,
      "grad_norm": 4.50564432144165,
      "learning_rate": 4.17497019247147e-05,
      "loss": 0.7132,
      "step": 1085000
    },
    {
      "epoch": 9.901270165705526,
      "grad_norm": 3.315309762954712,
      "learning_rate": 4.174894152857873e-05,
      "loss": 0.6691,
      "step": 1085100
    },
    {
      "epoch": 9.902182641068691,
      "grad_norm": 4.223397254943848,
      "learning_rate": 4.174818113244276e-05,
      "loss": 0.6996,
      "step": 1085200
    },
    {
      "epoch": 9.903095116431857,
      "grad_norm": 2.8709983825683594,
      "learning_rate": 4.174742073630679e-05,
      "loss": 0.6881,
      "step": 1085300
    },
    {
      "epoch": 9.904007591795022,
      "grad_norm": 4.607710361480713,
      "learning_rate": 4.174666034017082e-05,
      "loss": 0.6484,
      "step": 1085400
    },
    {
      "epoch": 9.904920067158187,
      "grad_norm": 4.76191520690918,
      "learning_rate": 4.174589994403484e-05,
      "loss": 0.6932,
      "step": 1085500
    },
    {
      "epoch": 9.905832542521352,
      "grad_norm": 4.637314796447754,
      "learning_rate": 4.174513954789888e-05,
      "loss": 0.6615,
      "step": 1085600
    },
    {
      "epoch": 9.906745017884518,
      "grad_norm": 3.862023115158081,
      "learning_rate": 4.17443791517629e-05,
      "loss": 0.6496,
      "step": 1085700
    },
    {
      "epoch": 9.907657493247683,
      "grad_norm": 2.8924710750579834,
      "learning_rate": 4.174361875562693e-05,
      "loss": 0.6914,
      "step": 1085800
    },
    {
      "epoch": 9.908569968610848,
      "grad_norm": 3.519364833831787,
      "learning_rate": 4.174285835949096e-05,
      "loss": 0.7093,
      "step": 1085900
    },
    {
      "epoch": 9.909482443974012,
      "grad_norm": 4.1272993087768555,
      "learning_rate": 4.174209796335499e-05,
      "loss": 0.6953,
      "step": 1086000
    },
    {
      "epoch": 9.910394919337177,
      "grad_norm": 4.56439733505249,
      "learning_rate": 4.1741337567219016e-05,
      "loss": 0.7124,
      "step": 1086100
    },
    {
      "epoch": 9.911307394700342,
      "grad_norm": 5.008925914764404,
      "learning_rate": 4.174057717108305e-05,
      "loss": 0.6996,
      "step": 1086200
    },
    {
      "epoch": 9.912219870063508,
      "grad_norm": 4.011565685272217,
      "learning_rate": 4.1739816774947076e-05,
      "loss": 0.6792,
      "step": 1086300
    },
    {
      "epoch": 9.913132345426673,
      "grad_norm": 3.9535489082336426,
      "learning_rate": 4.1739056378811106e-05,
      "loss": 0.6729,
      "step": 1086400
    },
    {
      "epoch": 9.914044820789838,
      "grad_norm": 18.768686294555664,
      "learning_rate": 4.1738295982675136e-05,
      "loss": 0.7149,
      "step": 1086500
    },
    {
      "epoch": 9.914957296153004,
      "grad_norm": 4.185816287994385,
      "learning_rate": 4.1737535586539166e-05,
      "loss": 0.668,
      "step": 1086600
    },
    {
      "epoch": 9.915869771516169,
      "grad_norm": 4.570326328277588,
      "learning_rate": 4.1736775190403196e-05,
      "loss": 0.7276,
      "step": 1086700
    },
    {
      "epoch": 9.916782246879334,
      "grad_norm": 3.9911105632781982,
      "learning_rate": 4.1736014794267226e-05,
      "loss": 0.7111,
      "step": 1086800
    },
    {
      "epoch": 9.9176947222425,
      "grad_norm": 3.8183393478393555,
      "learning_rate": 4.173525439813125e-05,
      "loss": 0.6862,
      "step": 1086900
    },
    {
      "epoch": 9.918607197605665,
      "grad_norm": 3.939756155014038,
      "learning_rate": 4.1734494001995286e-05,
      "loss": 0.6405,
      "step": 1087000
    },
    {
      "epoch": 9.91951967296883,
      "grad_norm": 4.637406826019287,
      "learning_rate": 4.173373360585931e-05,
      "loss": 0.64,
      "step": 1087100
    },
    {
      "epoch": 9.920432148331995,
      "grad_norm": 3.049759864807129,
      "learning_rate": 4.173297320972334e-05,
      "loss": 0.6502,
      "step": 1087200
    },
    {
      "epoch": 9.92134462369516,
      "grad_norm": 3.9197216033935547,
      "learning_rate": 4.173221281358737e-05,
      "loss": 0.65,
      "step": 1087300
    },
    {
      "epoch": 9.922257099058326,
      "grad_norm": 3.5783610343933105,
      "learning_rate": 4.17314524174514e-05,
      "loss": 0.6567,
      "step": 1087400
    },
    {
      "epoch": 9.923169574421491,
      "grad_norm": 4.015076160430908,
      "learning_rate": 4.173069202131542e-05,
      "loss": 0.6942,
      "step": 1087500
    },
    {
      "epoch": 9.924082049784655,
      "grad_norm": 3.6587953567504883,
      "learning_rate": 4.172993162517946e-05,
      "loss": 0.689,
      "step": 1087600
    },
    {
      "epoch": 9.92499452514782,
      "grad_norm": 3.2867300510406494,
      "learning_rate": 4.172917122904348e-05,
      "loss": 0.6836,
      "step": 1087700
    },
    {
      "epoch": 9.925907000510986,
      "grad_norm": 4.322750091552734,
      "learning_rate": 4.172841083290751e-05,
      "loss": 0.6775,
      "step": 1087800
    },
    {
      "epoch": 9.92681947587415,
      "grad_norm": 3.8682947158813477,
      "learning_rate": 4.172765043677154e-05,
      "loss": 0.666,
      "step": 1087900
    },
    {
      "epoch": 9.927731951237316,
      "grad_norm": 2.643592119216919,
      "learning_rate": 4.172689004063557e-05,
      "loss": 0.6843,
      "step": 1088000
    },
    {
      "epoch": 9.928644426600481,
      "grad_norm": 3.6793460845947266,
      "learning_rate": 4.1726129644499603e-05,
      "loss": 0.7151,
      "step": 1088100
    },
    {
      "epoch": 9.929556901963647,
      "grad_norm": 4.723134994506836,
      "learning_rate": 4.172536924836363e-05,
      "loss": 0.6437,
      "step": 1088200
    },
    {
      "epoch": 9.930469377326812,
      "grad_norm": 5.10726261138916,
      "learning_rate": 4.172460885222766e-05,
      "loss": 0.6736,
      "step": 1088300
    },
    {
      "epoch": 9.931381852689977,
      "grad_norm": 4.632194519042969,
      "learning_rate": 4.172384845609169e-05,
      "loss": 0.741,
      "step": 1088400
    },
    {
      "epoch": 9.932294328053143,
      "grad_norm": 3.5483593940734863,
      "learning_rate": 4.172308805995572e-05,
      "loss": 0.7019,
      "step": 1088500
    },
    {
      "epoch": 9.933206803416308,
      "grad_norm": 3.3958306312561035,
      "learning_rate": 4.172232766381974e-05,
      "loss": 0.721,
      "step": 1088600
    },
    {
      "epoch": 9.934119278779473,
      "grad_norm": 4.384274005889893,
      "learning_rate": 4.172156726768378e-05,
      "loss": 0.6785,
      "step": 1088700
    },
    {
      "epoch": 9.935031754142639,
      "grad_norm": 3.9057810306549072,
      "learning_rate": 4.17208068715478e-05,
      "loss": 0.713,
      "step": 1088800
    },
    {
      "epoch": 9.935944229505804,
      "grad_norm": 4.1857404708862305,
      "learning_rate": 4.172004647541183e-05,
      "loss": 0.6823,
      "step": 1088900
    },
    {
      "epoch": 9.936856704868969,
      "grad_norm": 4.099109172821045,
      "learning_rate": 4.171928607927586e-05,
      "loss": 0.6828,
      "step": 1089000
    },
    {
      "epoch": 9.937769180232134,
      "grad_norm": 4.265744209289551,
      "learning_rate": 4.171852568313989e-05,
      "loss": 0.6976,
      "step": 1089100
    },
    {
      "epoch": 9.9386816555953,
      "grad_norm": 4.495561122894287,
      "learning_rate": 4.171776528700392e-05,
      "loss": 0.6609,
      "step": 1089200
    },
    {
      "epoch": 9.939594130958465,
      "grad_norm": 4.029120922088623,
      "learning_rate": 4.171700489086795e-05,
      "loss": 0.6658,
      "step": 1089300
    },
    {
      "epoch": 9.940506606321629,
      "grad_norm": 4.594613075256348,
      "learning_rate": 4.1716244494731974e-05,
      "loss": 0.6623,
      "step": 1089400
    },
    {
      "epoch": 9.941419081684794,
      "grad_norm": 3.842564344406128,
      "learning_rate": 4.171548409859601e-05,
      "loss": 0.6643,
      "step": 1089500
    },
    {
      "epoch": 9.94233155704796,
      "grad_norm": 3.773674964904785,
      "learning_rate": 4.1714723702460034e-05,
      "loss": 0.6552,
      "step": 1089600
    },
    {
      "epoch": 9.943244032411124,
      "grad_norm": 4.446870803833008,
      "learning_rate": 4.1713963306324064e-05,
      "loss": 0.7191,
      "step": 1089700
    },
    {
      "epoch": 9.94415650777429,
      "grad_norm": 4.8916544914245605,
      "learning_rate": 4.1713202910188094e-05,
      "loss": 0.6875,
      "step": 1089800
    },
    {
      "epoch": 9.945068983137455,
      "grad_norm": 4.054086685180664,
      "learning_rate": 4.1712442514052124e-05,
      "loss": 0.6866,
      "step": 1089900
    },
    {
      "epoch": 9.94598145850062,
      "grad_norm": 3.9155263900756836,
      "learning_rate": 4.171168211791615e-05,
      "loss": 0.6744,
      "step": 1090000
    },
    {
      "epoch": 9.946893933863786,
      "grad_norm": 4.255630970001221,
      "learning_rate": 4.1710921721780184e-05,
      "loss": 0.7142,
      "step": 1090100
    },
    {
      "epoch": 9.947806409226951,
      "grad_norm": 3.4772098064422607,
      "learning_rate": 4.171016132564421e-05,
      "loss": 0.6705,
      "step": 1090200
    },
    {
      "epoch": 9.948718884590116,
      "grad_norm": 4.614444255828857,
      "learning_rate": 4.170940092950824e-05,
      "loss": 0.6567,
      "step": 1090300
    },
    {
      "epoch": 9.949631359953282,
      "grad_norm": 3.699232339859009,
      "learning_rate": 4.170864053337227e-05,
      "loss": 0.6482,
      "step": 1090400
    },
    {
      "epoch": 9.950543835316447,
      "grad_norm": 3.447265148162842,
      "learning_rate": 4.17078801372363e-05,
      "loss": 0.6625,
      "step": 1090500
    },
    {
      "epoch": 9.951456310679612,
      "grad_norm": 3.383255958557129,
      "learning_rate": 4.170711974110033e-05,
      "loss": 0.6479,
      "step": 1090600
    },
    {
      "epoch": 9.952368786042777,
      "grad_norm": 3.4827160835266113,
      "learning_rate": 4.170635934496435e-05,
      "loss": 0.6758,
      "step": 1090700
    },
    {
      "epoch": 9.953281261405943,
      "grad_norm": 3.7139720916748047,
      "learning_rate": 4.170559894882838e-05,
      "loss": 0.6893,
      "step": 1090800
    },
    {
      "epoch": 9.954193736769108,
      "grad_norm": 3.779874086380005,
      "learning_rate": 4.170483855269241e-05,
      "loss": 0.6996,
      "step": 1090900
    },
    {
      "epoch": 9.955106212132272,
      "grad_norm": 4.6512298583984375,
      "learning_rate": 4.170407815655644e-05,
      "loss": 0.6747,
      "step": 1091000
    },
    {
      "epoch": 9.956018687495437,
      "grad_norm": 4.583681583404541,
      "learning_rate": 4.170331776042047e-05,
      "loss": 0.6862,
      "step": 1091100
    },
    {
      "epoch": 9.956931162858602,
      "grad_norm": 3.8051950931549072,
      "learning_rate": 4.17025573642845e-05,
      "loss": 0.7007,
      "step": 1091200
    },
    {
      "epoch": 9.957843638221767,
      "grad_norm": 4.0058112144470215,
      "learning_rate": 4.1701796968148525e-05,
      "loss": 0.7083,
      "step": 1091300
    },
    {
      "epoch": 9.958756113584933,
      "grad_norm": 3.725111484527588,
      "learning_rate": 4.170103657201256e-05,
      "loss": 0.7007,
      "step": 1091400
    },
    {
      "epoch": 9.959668588948098,
      "grad_norm": 4.4567437171936035,
      "learning_rate": 4.1700276175876585e-05,
      "loss": 0.6805,
      "step": 1091500
    },
    {
      "epoch": 9.960581064311263,
      "grad_norm": 4.189384460449219,
      "learning_rate": 4.1699515779740615e-05,
      "loss": 0.6998,
      "step": 1091600
    },
    {
      "epoch": 9.961493539674429,
      "grad_norm": 3.6324872970581055,
      "learning_rate": 4.1698755383604645e-05,
      "loss": 0.6559,
      "step": 1091700
    },
    {
      "epoch": 9.962406015037594,
      "grad_norm": 4.571900367736816,
      "learning_rate": 4.1697994987468675e-05,
      "loss": 0.6714,
      "step": 1091800
    },
    {
      "epoch": 9.96331849040076,
      "grad_norm": 4.621914386749268,
      "learning_rate": 4.16972345913327e-05,
      "loss": 0.7121,
      "step": 1091900
    },
    {
      "epoch": 9.964230965763925,
      "grad_norm": 4.500577449798584,
      "learning_rate": 4.1696474195196735e-05,
      "loss": 0.6578,
      "step": 1092000
    },
    {
      "epoch": 9.96514344112709,
      "grad_norm": 4.2316179275512695,
      "learning_rate": 4.169571379906076e-05,
      "loss": 0.6405,
      "step": 1092100
    },
    {
      "epoch": 9.966055916490255,
      "grad_norm": 4.110091209411621,
      "learning_rate": 4.169495340292479e-05,
      "loss": 0.6971,
      "step": 1092200
    },
    {
      "epoch": 9.96696839185342,
      "grad_norm": 3.3607702255249023,
      "learning_rate": 4.169419300678882e-05,
      "loss": 0.6868,
      "step": 1092300
    },
    {
      "epoch": 9.967880867216586,
      "grad_norm": 2.9718070030212402,
      "learning_rate": 4.169343261065285e-05,
      "loss": 0.6577,
      "step": 1092400
    },
    {
      "epoch": 9.968793342579751,
      "grad_norm": 3.679885149002075,
      "learning_rate": 4.169267221451688e-05,
      "loss": 0.6919,
      "step": 1092500
    },
    {
      "epoch": 9.969705817942916,
      "grad_norm": 3.9954614639282227,
      "learning_rate": 4.169191181838091e-05,
      "loss": 0.6549,
      "step": 1092600
    },
    {
      "epoch": 9.970618293306082,
      "grad_norm": 3.9992666244506836,
      "learning_rate": 4.169115142224493e-05,
      "loss": 0.691,
      "step": 1092700
    },
    {
      "epoch": 9.971530768669245,
      "grad_norm": 3.162036895751953,
      "learning_rate": 4.169039102610897e-05,
      "loss": 0.682,
      "step": 1092800
    },
    {
      "epoch": 9.97244324403241,
      "grad_norm": 3.902311086654663,
      "learning_rate": 4.168963062997299e-05,
      "loss": 0.7052,
      "step": 1092900
    },
    {
      "epoch": 9.973355719395576,
      "grad_norm": 3.9927899837493896,
      "learning_rate": 4.168887023383702e-05,
      "loss": 0.7184,
      "step": 1093000
    },
    {
      "epoch": 9.974268194758741,
      "grad_norm": 3.5331716537475586,
      "learning_rate": 4.168810983770105e-05,
      "loss": 0.6622,
      "step": 1093100
    },
    {
      "epoch": 9.975180670121906,
      "grad_norm": 4.205077171325684,
      "learning_rate": 4.168734944156508e-05,
      "loss": 0.6801,
      "step": 1093200
    },
    {
      "epoch": 9.976093145485072,
      "grad_norm": 3.86080265045166,
      "learning_rate": 4.1686589045429106e-05,
      "loss": 0.7156,
      "step": 1093300
    },
    {
      "epoch": 9.977005620848237,
      "grad_norm": 3.912292242050171,
      "learning_rate": 4.168582864929314e-05,
      "loss": 0.7256,
      "step": 1093400
    },
    {
      "epoch": 9.977918096211402,
      "grad_norm": 4.467443466186523,
      "learning_rate": 4.1685068253157166e-05,
      "loss": 0.6958,
      "step": 1093500
    },
    {
      "epoch": 9.978830571574568,
      "grad_norm": 3.5675339698791504,
      "learning_rate": 4.1684307857021196e-05,
      "loss": 0.6321,
      "step": 1093600
    },
    {
      "epoch": 9.979743046937733,
      "grad_norm": 3.9915008544921875,
      "learning_rate": 4.1683547460885226e-05,
      "loss": 0.6465,
      "step": 1093700
    },
    {
      "epoch": 9.980655522300898,
      "grad_norm": 4.274028301239014,
      "learning_rate": 4.168278706474925e-05,
      "loss": 0.6822,
      "step": 1093800
    },
    {
      "epoch": 9.981567997664063,
      "grad_norm": 3.5114493370056152,
      "learning_rate": 4.1682026668613286e-05,
      "loss": 0.7036,
      "step": 1093900
    },
    {
      "epoch": 9.982480473027229,
      "grad_norm": 3.940976619720459,
      "learning_rate": 4.168126627247731e-05,
      "loss": 0.7007,
      "step": 1094000
    },
    {
      "epoch": 9.983392948390394,
      "grad_norm": 4.491188049316406,
      "learning_rate": 4.168050587634134e-05,
      "loss": 0.6725,
      "step": 1094100
    },
    {
      "epoch": 9.98430542375356,
      "grad_norm": 4.586746692657471,
      "learning_rate": 4.167974548020537e-05,
      "loss": 0.6939,
      "step": 1094200
    },
    {
      "epoch": 9.985217899116725,
      "grad_norm": 4.403792381286621,
      "learning_rate": 4.16789850840694e-05,
      "loss": 0.6927,
      "step": 1094300
    },
    {
      "epoch": 9.986130374479888,
      "grad_norm": 4.30879545211792,
      "learning_rate": 4.167822468793342e-05,
      "loss": 0.7065,
      "step": 1094400
    },
    {
      "epoch": 9.987042849843053,
      "grad_norm": 3.86307954788208,
      "learning_rate": 4.167746429179746e-05,
      "loss": 0.6941,
      "step": 1094500
    },
    {
      "epoch": 9.987955325206219,
      "grad_norm": 4.788852214813232,
      "learning_rate": 4.167670389566148e-05,
      "loss": 0.6841,
      "step": 1094600
    },
    {
      "epoch": 9.988867800569384,
      "grad_norm": 3.802431583404541,
      "learning_rate": 4.167594349952551e-05,
      "loss": 0.7241,
      "step": 1094700
    },
    {
      "epoch": 9.98978027593255,
      "grad_norm": 3.497995138168335,
      "learning_rate": 4.1675183103389543e-05,
      "loss": 0.676,
      "step": 1094800
    },
    {
      "epoch": 9.990692751295715,
      "grad_norm": 3.505768060684204,
      "learning_rate": 4.1674422707253573e-05,
      "loss": 0.7157,
      "step": 1094900
    },
    {
      "epoch": 9.99160522665888,
      "grad_norm": 4.475107669830322,
      "learning_rate": 4.1673662311117604e-05,
      "loss": 0.6968,
      "step": 1095000
    },
    {
      "epoch": 9.992517702022045,
      "grad_norm": 4.231032848358154,
      "learning_rate": 4.1672901914981634e-05,
      "loss": 0.6908,
      "step": 1095100
    },
    {
      "epoch": 9.99343017738521,
      "grad_norm": 4.625504493713379,
      "learning_rate": 4.167214151884566e-05,
      "loss": 0.7024,
      "step": 1095200
    },
    {
      "epoch": 9.994342652748376,
      "grad_norm": 4.1114654541015625,
      "learning_rate": 4.1671381122709694e-05,
      "loss": 0.6969,
      "step": 1095300
    },
    {
      "epoch": 9.995255128111541,
      "grad_norm": 4.365017414093018,
      "learning_rate": 4.167062072657372e-05,
      "loss": 0.6748,
      "step": 1095400
    },
    {
      "epoch": 9.996167603474706,
      "grad_norm": 4.00681734085083,
      "learning_rate": 4.166986033043775e-05,
      "loss": 0.6721,
      "step": 1095500
    },
    {
      "epoch": 9.997080078837872,
      "grad_norm": 3.854417324066162,
      "learning_rate": 4.166909993430178e-05,
      "loss": 0.6849,
      "step": 1095600
    },
    {
      "epoch": 9.997992554201037,
      "grad_norm": 3.249636650085449,
      "learning_rate": 4.166833953816581e-05,
      "loss": 0.6933,
      "step": 1095700
    },
    {
      "epoch": 9.998905029564202,
      "grad_norm": 4.197507381439209,
      "learning_rate": 4.166757914202983e-05,
      "loss": 0.7009,
      "step": 1095800
    },
    {
      "epoch": 9.999817504927368,
      "grad_norm": 4.249133586883545,
      "learning_rate": 4.166681874589387e-05,
      "loss": 0.6875,
      "step": 1095900
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.5576725602149963,
      "eval_runtime": 25.5242,
      "eval_samples_per_second": 226.021,
      "eval_steps_per_second": 226.021,
      "step": 1095920
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.5375607013702393,
      "eval_runtime": 487.2938,
      "eval_samples_per_second": 224.899,
      "eval_steps_per_second": 224.899,
      "step": 1095920
    },
    {
      "epoch": 10.000729980290533,
      "grad_norm": 4.032110691070557,
      "learning_rate": 4.166605834975789e-05,
      "loss": 0.6506,
      "step": 1096000
    },
    {
      "epoch": 10.001642455653696,
      "grad_norm": 4.0544538497924805,
      "learning_rate": 4.166529795362192e-05,
      "loss": 0.6944,
      "step": 1096100
    },
    {
      "epoch": 10.002554931016862,
      "grad_norm": 4.391436576843262,
      "learning_rate": 4.166453755748595e-05,
      "loss": 0.7307,
      "step": 1096200
    },
    {
      "epoch": 10.003467406380027,
      "grad_norm": 4.551363468170166,
      "learning_rate": 4.1663777161349974e-05,
      "loss": 0.7209,
      "step": 1096300
    },
    {
      "epoch": 10.004379881743192,
      "grad_norm": 4.043798446655273,
      "learning_rate": 4.166301676521401e-05,
      "loss": 0.6983,
      "step": 1096400
    },
    {
      "epoch": 10.005292357106358,
      "grad_norm": 4.16033411026001,
      "learning_rate": 4.1662256369078034e-05,
      "loss": 0.6517,
      "step": 1096500
    },
    {
      "epoch": 10.006204832469523,
      "grad_norm": 5.307289123535156,
      "learning_rate": 4.1661495972942064e-05,
      "loss": 0.7335,
      "step": 1096600
    },
    {
      "epoch": 10.007117307832688,
      "grad_norm": 6.852457046508789,
      "learning_rate": 4.1660735576806094e-05,
      "loss": 0.6762,
      "step": 1096700
    },
    {
      "epoch": 10.008029783195854,
      "grad_norm": 3.845762252807617,
      "learning_rate": 4.1659975180670124e-05,
      "loss": 0.6482,
      "step": 1096800
    },
    {
      "epoch": 10.008942258559019,
      "grad_norm": 4.359537124633789,
      "learning_rate": 4.165921478453415e-05,
      "loss": 0.6858,
      "step": 1096900
    },
    {
      "epoch": 10.009854733922184,
      "grad_norm": 3.93426251411438,
      "learning_rate": 4.1658454388398185e-05,
      "loss": 0.6817,
      "step": 1097000
    },
    {
      "epoch": 10.01076720928535,
      "grad_norm": 3.5894880294799805,
      "learning_rate": 4.165769399226221e-05,
      "loss": 0.6883,
      "step": 1097100
    },
    {
      "epoch": 10.011679684648515,
      "grad_norm": 3.871870994567871,
      "learning_rate": 4.165693359612624e-05,
      "loss": 0.6759,
      "step": 1097200
    },
    {
      "epoch": 10.01259216001168,
      "grad_norm": 3.327244281768799,
      "learning_rate": 4.165617319999027e-05,
      "loss": 0.71,
      "step": 1097300
    },
    {
      "epoch": 10.013504635374845,
      "grad_norm": 3.470867872238159,
      "learning_rate": 4.16554128038543e-05,
      "loss": 0.6807,
      "step": 1097400
    },
    {
      "epoch": 10.01441711073801,
      "grad_norm": 4.5030598640441895,
      "learning_rate": 4.165465240771833e-05,
      "loss": 0.6639,
      "step": 1097500
    },
    {
      "epoch": 10.015329586101176,
      "grad_norm": 4.244508266448975,
      "learning_rate": 4.165389201158236e-05,
      "loss": 0.6977,
      "step": 1097600
    },
    {
      "epoch": 10.016242061464341,
      "grad_norm": 4.389786243438721,
      "learning_rate": 4.165313161544638e-05,
      "loss": 0.6809,
      "step": 1097700
    },
    {
      "epoch": 10.017154536827505,
      "grad_norm": 4.478262424468994,
      "learning_rate": 4.165237121931042e-05,
      "loss": 0.7043,
      "step": 1097800
    },
    {
      "epoch": 10.01806701219067,
      "grad_norm": 2.6302006244659424,
      "learning_rate": 4.165161082317444e-05,
      "loss": 0.6999,
      "step": 1097900
    },
    {
      "epoch": 10.018979487553835,
      "grad_norm": 3.871281623840332,
      "learning_rate": 4.165085042703847e-05,
      "loss": 0.6636,
      "step": 1098000
    },
    {
      "epoch": 10.019891962917,
      "grad_norm": 3.8359339237213135,
      "learning_rate": 4.16500900309025e-05,
      "loss": 0.6784,
      "step": 1098100
    },
    {
      "epoch": 10.020804438280166,
      "grad_norm": 3.885891914367676,
      "learning_rate": 4.164932963476653e-05,
      "loss": 0.6975,
      "step": 1098200
    },
    {
      "epoch": 10.021716913643331,
      "grad_norm": 4.004022121429443,
      "learning_rate": 4.1648569238630555e-05,
      "loss": 0.6941,
      "step": 1098300
    },
    {
      "epoch": 10.022629389006497,
      "grad_norm": 3.8793625831604004,
      "learning_rate": 4.164780884249459e-05,
      "loss": 0.6685,
      "step": 1098400
    },
    {
      "epoch": 10.023541864369662,
      "grad_norm": 4.866763591766357,
      "learning_rate": 4.1647048446358615e-05,
      "loss": 0.7128,
      "step": 1098500
    },
    {
      "epoch": 10.024454339732827,
      "grad_norm": 3.731227159500122,
      "learning_rate": 4.1646288050222645e-05,
      "loss": 0.7165,
      "step": 1098600
    },
    {
      "epoch": 10.025366815095992,
      "grad_norm": 3.9413564205169678,
      "learning_rate": 4.1645527654086675e-05,
      "loss": 0.666,
      "step": 1098700
    },
    {
      "epoch": 10.026279290459158,
      "grad_norm": 4.061250686645508,
      "learning_rate": 4.1644767257950705e-05,
      "loss": 0.6576,
      "step": 1098800
    },
    {
      "epoch": 10.027191765822323,
      "grad_norm": 4.272238731384277,
      "learning_rate": 4.1644006861814736e-05,
      "loss": 0.6712,
      "step": 1098900
    },
    {
      "epoch": 10.028104241185488,
      "grad_norm": 4.699349880218506,
      "learning_rate": 4.1643246465678766e-05,
      "loss": 0.6888,
      "step": 1099000
    },
    {
      "epoch": 10.029016716548654,
      "grad_norm": 4.696232318878174,
      "learning_rate": 4.164248606954279e-05,
      "loss": 0.6416,
      "step": 1099100
    },
    {
      "epoch": 10.029929191911819,
      "grad_norm": 3.5170907974243164,
      "learning_rate": 4.164172567340682e-05,
      "loss": 0.6415,
      "step": 1099200
    },
    {
      "epoch": 10.030841667274984,
      "grad_norm": 4.726108074188232,
      "learning_rate": 4.164096527727085e-05,
      "loss": 0.7085,
      "step": 1099300
    },
    {
      "epoch": 10.03175414263815,
      "grad_norm": 2.7364704608917236,
      "learning_rate": 4.164020488113487e-05,
      "loss": 0.6136,
      "step": 1099400
    },
    {
      "epoch": 10.032666618001313,
      "grad_norm": 4.508815765380859,
      "learning_rate": 4.163944448499891e-05,
      "loss": 0.6766,
      "step": 1099500
    },
    {
      "epoch": 10.033579093364478,
      "grad_norm": 3.8023078441619873,
      "learning_rate": 4.163868408886293e-05,
      "loss": 0.7144,
      "step": 1099600
    },
    {
      "epoch": 10.034491568727644,
      "grad_norm": 4.560065269470215,
      "learning_rate": 4.163792369272696e-05,
      "loss": 0.7092,
      "step": 1099700
    },
    {
      "epoch": 10.035404044090809,
      "grad_norm": 3.4195120334625244,
      "learning_rate": 4.163716329659099e-05,
      "loss": 0.6864,
      "step": 1099800
    },
    {
      "epoch": 10.036316519453974,
      "grad_norm": 3.6843690872192383,
      "learning_rate": 4.163640290045502e-05,
      "loss": 0.6837,
      "step": 1099900
    },
    {
      "epoch": 10.03722899481714,
      "grad_norm": 4.089176654815674,
      "learning_rate": 4.163564250431905e-05,
      "loss": 0.6754,
      "step": 1100000
    },
    {
      "epoch": 10.038141470180305,
      "grad_norm": 4.038895606994629,
      "learning_rate": 4.163488210818308e-05,
      "loss": 0.696,
      "step": 1100100
    },
    {
      "epoch": 10.03905394554347,
      "grad_norm": 4.653671741485596,
      "learning_rate": 4.1634121712047106e-05,
      "loss": 0.6956,
      "step": 1100200
    },
    {
      "epoch": 10.039966420906635,
      "grad_norm": 3.6438310146331787,
      "learning_rate": 4.163336131591114e-05,
      "loss": 0.6324,
      "step": 1100300
    },
    {
      "epoch": 10.0408788962698,
      "grad_norm": 4.219080924987793,
      "learning_rate": 4.1632600919775166e-05,
      "loss": 0.6681,
      "step": 1100400
    },
    {
      "epoch": 10.041791371632966,
      "grad_norm": 3.8751070499420166,
      "learning_rate": 4.1631840523639196e-05,
      "loss": 0.6775,
      "step": 1100500
    },
    {
      "epoch": 10.042703846996131,
      "grad_norm": 4.834622383117676,
      "learning_rate": 4.1631080127503226e-05,
      "loss": 0.6792,
      "step": 1100600
    },
    {
      "epoch": 10.043616322359297,
      "grad_norm": 3.606112480163574,
      "learning_rate": 4.1630319731367256e-05,
      "loss": 0.6939,
      "step": 1100700
    },
    {
      "epoch": 10.044528797722462,
      "grad_norm": 4.224003314971924,
      "learning_rate": 4.162955933523128e-05,
      "loss": 0.6608,
      "step": 1100800
    },
    {
      "epoch": 10.045441273085627,
      "grad_norm": 3.712228298187256,
      "learning_rate": 4.1628798939095317e-05,
      "loss": 0.6695,
      "step": 1100900
    },
    {
      "epoch": 10.046353748448793,
      "grad_norm": 4.261478424072266,
      "learning_rate": 4.162803854295934e-05,
      "loss": 0.6632,
      "step": 1101000
    },
    {
      "epoch": 10.047266223811958,
      "grad_norm": 2.715073347091675,
      "learning_rate": 4.162727814682337e-05,
      "loss": 0.6798,
      "step": 1101100
    },
    {
      "epoch": 10.048178699175121,
      "grad_norm": 3.24246883392334,
      "learning_rate": 4.16265177506874e-05,
      "loss": 0.7236,
      "step": 1101200
    },
    {
      "epoch": 10.049091174538287,
      "grad_norm": 4.540600299835205,
      "learning_rate": 4.162575735455143e-05,
      "loss": 0.6839,
      "step": 1101300
    },
    {
      "epoch": 10.050003649901452,
      "grad_norm": 4.135721206665039,
      "learning_rate": 4.162499695841546e-05,
      "loss": 0.7141,
      "step": 1101400
    },
    {
      "epoch": 10.050916125264617,
      "grad_norm": 4.55635929107666,
      "learning_rate": 4.162423656227949e-05,
      "loss": 0.6894,
      "step": 1101500
    },
    {
      "epoch": 10.051828600627783,
      "grad_norm": 2.9954586029052734,
      "learning_rate": 4.1623476166143513e-05,
      "loss": 0.6904,
      "step": 1101600
    },
    {
      "epoch": 10.052741075990948,
      "grad_norm": 4.04130220413208,
      "learning_rate": 4.162271577000755e-05,
      "loss": 0.7243,
      "step": 1101700
    },
    {
      "epoch": 10.053653551354113,
      "grad_norm": 3.508291006088257,
      "learning_rate": 4.1621955373871574e-05,
      "loss": 0.6698,
      "step": 1101800
    },
    {
      "epoch": 10.054566026717279,
      "grad_norm": 4.166234016418457,
      "learning_rate": 4.1621194977735604e-05,
      "loss": 0.7114,
      "step": 1101900
    },
    {
      "epoch": 10.055478502080444,
      "grad_norm": 4.545660495758057,
      "learning_rate": 4.1620434581599634e-05,
      "loss": 0.6477,
      "step": 1102000
    },
    {
      "epoch": 10.056390977443609,
      "grad_norm": 3.5531389713287354,
      "learning_rate": 4.161967418546366e-05,
      "loss": 0.7087,
      "step": 1102100
    },
    {
      "epoch": 10.057303452806774,
      "grad_norm": 4.510547161102295,
      "learning_rate": 4.161891378932769e-05,
      "loss": 0.6825,
      "step": 1102200
    },
    {
      "epoch": 10.05821592816994,
      "grad_norm": 4.672922134399414,
      "learning_rate": 4.161815339319172e-05,
      "loss": 0.6511,
      "step": 1102300
    },
    {
      "epoch": 10.059128403533105,
      "grad_norm": 3.385443687438965,
      "learning_rate": 4.161739299705575e-05,
      "loss": 0.6803,
      "step": 1102400
    },
    {
      "epoch": 10.06004087889627,
      "grad_norm": 4.763638973236084,
      "learning_rate": 4.161663260091978e-05,
      "loss": 0.6561,
      "step": 1102500
    },
    {
      "epoch": 10.060953354259436,
      "grad_norm": 3.445688486099243,
      "learning_rate": 4.161587220478381e-05,
      "loss": 0.6939,
      "step": 1102600
    },
    {
      "epoch": 10.061865829622601,
      "grad_norm": 3.3037941455841064,
      "learning_rate": 4.161511180864783e-05,
      "loss": 0.6661,
      "step": 1102700
    },
    {
      "epoch": 10.062778304985766,
      "grad_norm": 4.7900238037109375,
      "learning_rate": 4.161435141251187e-05,
      "loss": 0.6405,
      "step": 1102800
    },
    {
      "epoch": 10.06369078034893,
      "grad_norm": 4.580443382263184,
      "learning_rate": 4.161359101637589e-05,
      "loss": 0.6981,
      "step": 1102900
    },
    {
      "epoch": 10.064603255712095,
      "grad_norm": 3.7026591300964355,
      "learning_rate": 4.161283062023992e-05,
      "loss": 0.6675,
      "step": 1103000
    },
    {
      "epoch": 10.06551573107526,
      "grad_norm": 4.318004131317139,
      "learning_rate": 4.161207022410395e-05,
      "loss": 0.6598,
      "step": 1103100
    },
    {
      "epoch": 10.066428206438426,
      "grad_norm": 4.64953088760376,
      "learning_rate": 4.161130982796798e-05,
      "loss": 0.6672,
      "step": 1103200
    },
    {
      "epoch": 10.067340681801591,
      "grad_norm": 4.814579486846924,
      "learning_rate": 4.161054943183201e-05,
      "loss": 0.6918,
      "step": 1103300
    },
    {
      "epoch": 10.068253157164756,
      "grad_norm": 5.185426235198975,
      "learning_rate": 4.160978903569604e-05,
      "loss": 0.6806,
      "step": 1103400
    },
    {
      "epoch": 10.069165632527922,
      "grad_norm": 4.131999492645264,
      "learning_rate": 4.1609028639560064e-05,
      "loss": 0.6909,
      "step": 1103500
    },
    {
      "epoch": 10.070078107891087,
      "grad_norm": 3.932210683822632,
      "learning_rate": 4.1608268243424094e-05,
      "loss": 0.685,
      "step": 1103600
    },
    {
      "epoch": 10.070990583254252,
      "grad_norm": 4.7345476150512695,
      "learning_rate": 4.1607507847288125e-05,
      "loss": 0.6529,
      "step": 1103700
    },
    {
      "epoch": 10.071903058617417,
      "grad_norm": 3.6044182777404785,
      "learning_rate": 4.1606747451152155e-05,
      "loss": 0.6917,
      "step": 1103800
    },
    {
      "epoch": 10.072815533980583,
      "grad_norm": 3.8753976821899414,
      "learning_rate": 4.1605987055016185e-05,
      "loss": 0.6943,
      "step": 1103900
    },
    {
      "epoch": 10.073728009343748,
      "grad_norm": 4.047994613647461,
      "learning_rate": 4.1605226658880215e-05,
      "loss": 0.6606,
      "step": 1104000
    },
    {
      "epoch": 10.074640484706913,
      "grad_norm": 2.965244770050049,
      "learning_rate": 4.160446626274424e-05,
      "loss": 0.6993,
      "step": 1104100
    },
    {
      "epoch": 10.075552960070079,
      "grad_norm": 3.569525957107544,
      "learning_rate": 4.1603705866608275e-05,
      "loss": 0.6893,
      "step": 1104200
    },
    {
      "epoch": 10.076465435433244,
      "grad_norm": 4.546481609344482,
      "learning_rate": 4.16029454704723e-05,
      "loss": 0.6922,
      "step": 1104300
    },
    {
      "epoch": 10.07737791079641,
      "grad_norm": 3.2011122703552246,
      "learning_rate": 4.160218507433633e-05,
      "loss": 0.7186,
      "step": 1104400
    },
    {
      "epoch": 10.078290386159575,
      "grad_norm": 3.794428825378418,
      "learning_rate": 4.160142467820036e-05,
      "loss": 0.6892,
      "step": 1104500
    },
    {
      "epoch": 10.079202861522738,
      "grad_norm": 4.020323276519775,
      "learning_rate": 4.160066428206439e-05,
      "loss": 0.6906,
      "step": 1104600
    },
    {
      "epoch": 10.080115336885903,
      "grad_norm": 3.74570369720459,
      "learning_rate": 4.159990388592842e-05,
      "loss": 0.6963,
      "step": 1104700
    },
    {
      "epoch": 10.081027812249069,
      "grad_norm": 4.326459884643555,
      "learning_rate": 4.159914348979245e-05,
      "loss": 0.7094,
      "step": 1104800
    },
    {
      "epoch": 10.081940287612234,
      "grad_norm": 3.861342191696167,
      "learning_rate": 4.159838309365647e-05,
      "loss": 0.6328,
      "step": 1104900
    },
    {
      "epoch": 10.0828527629754,
      "grad_norm": 3.4704456329345703,
      "learning_rate": 4.15976226975205e-05,
      "loss": 0.6666,
      "step": 1105000
    },
    {
      "epoch": 10.083765238338565,
      "grad_norm": 4.873936176300049,
      "learning_rate": 4.159686230138453e-05,
      "loss": 0.6595,
      "step": 1105100
    },
    {
      "epoch": 10.08467771370173,
      "grad_norm": 3.861872673034668,
      "learning_rate": 4.1596101905248555e-05,
      "loss": 0.6959,
      "step": 1105200
    },
    {
      "epoch": 10.085590189064895,
      "grad_norm": 4.134941577911377,
      "learning_rate": 4.159534150911259e-05,
      "loss": 0.6919,
      "step": 1105300
    },
    {
      "epoch": 10.08650266442806,
      "grad_norm": 4.438479900360107,
      "learning_rate": 4.1594581112976615e-05,
      "loss": 0.6973,
      "step": 1105400
    },
    {
      "epoch": 10.087415139791226,
      "grad_norm": 4.130478858947754,
      "learning_rate": 4.1593820716840645e-05,
      "loss": 0.6911,
      "step": 1105500
    },
    {
      "epoch": 10.088327615154391,
      "grad_norm": 4.3000311851501465,
      "learning_rate": 4.1593060320704675e-05,
      "loss": 0.6652,
      "step": 1105600
    },
    {
      "epoch": 10.089240090517556,
      "grad_norm": 4.609048843383789,
      "learning_rate": 4.1592299924568706e-05,
      "loss": 0.6602,
      "step": 1105700
    },
    {
      "epoch": 10.090152565880722,
      "grad_norm": 3.9510607719421387,
      "learning_rate": 4.1591539528432736e-05,
      "loss": 0.6511,
      "step": 1105800
    },
    {
      "epoch": 10.091065041243887,
      "grad_norm": 3.4100472927093506,
      "learning_rate": 4.1590779132296766e-05,
      "loss": 0.6745,
      "step": 1105900
    },
    {
      "epoch": 10.091977516607052,
      "grad_norm": 4.873274326324463,
      "learning_rate": 4.159001873616079e-05,
      "loss": 0.6669,
      "step": 1106000
    },
    {
      "epoch": 10.092889991970218,
      "grad_norm": 3.3532259464263916,
      "learning_rate": 4.1589258340024826e-05,
      "loss": 0.6655,
      "step": 1106100
    },
    {
      "epoch": 10.093802467333383,
      "grad_norm": 3.0150561332702637,
      "learning_rate": 4.158849794388885e-05,
      "loss": 0.683,
      "step": 1106200
    },
    {
      "epoch": 10.094714942696546,
      "grad_norm": 5.010509490966797,
      "learning_rate": 4.158773754775288e-05,
      "loss": 0.6609,
      "step": 1106300
    },
    {
      "epoch": 10.095627418059712,
      "grad_norm": 4.018746852874756,
      "learning_rate": 4.158697715161691e-05,
      "loss": 0.6703,
      "step": 1106400
    },
    {
      "epoch": 10.096539893422877,
      "grad_norm": 3.5700271129608154,
      "learning_rate": 4.158621675548094e-05,
      "loss": 0.6662,
      "step": 1106500
    },
    {
      "epoch": 10.097452368786042,
      "grad_norm": 4.656800746917725,
      "learning_rate": 4.158545635934496e-05,
      "loss": 0.6643,
      "step": 1106600
    },
    {
      "epoch": 10.098364844149208,
      "grad_norm": 4.47524356842041,
      "learning_rate": 4.1584695963209e-05,
      "loss": 0.7183,
      "step": 1106700
    },
    {
      "epoch": 10.099277319512373,
      "grad_norm": 5.53339147567749,
      "learning_rate": 4.158393556707302e-05,
      "loss": 0.699,
      "step": 1106800
    },
    {
      "epoch": 10.100189794875538,
      "grad_norm": 4.215248107910156,
      "learning_rate": 4.158317517093705e-05,
      "loss": 0.6504,
      "step": 1106900
    },
    {
      "epoch": 10.101102270238703,
      "grad_norm": 3.715240240097046,
      "learning_rate": 4.158241477480108e-05,
      "loss": 0.6607,
      "step": 1107000
    },
    {
      "epoch": 10.102014745601869,
      "grad_norm": 3.816521644592285,
      "learning_rate": 4.158165437866511e-05,
      "loss": 0.6875,
      "step": 1107100
    },
    {
      "epoch": 10.102927220965034,
      "grad_norm": 4.30235481262207,
      "learning_rate": 4.158089398252914e-05,
      "loss": 0.6915,
      "step": 1107200
    },
    {
      "epoch": 10.1038396963282,
      "grad_norm": 3.9953746795654297,
      "learning_rate": 4.158013358639317e-05,
      "loss": 0.6986,
      "step": 1107300
    },
    {
      "epoch": 10.104752171691365,
      "grad_norm": 4.656991004943848,
      "learning_rate": 4.1579373190257196e-05,
      "loss": 0.668,
      "step": 1107400
    },
    {
      "epoch": 10.10566464705453,
      "grad_norm": 3.970113515853882,
      "learning_rate": 4.157861279412123e-05,
      "loss": 0.6785,
      "step": 1107500
    },
    {
      "epoch": 10.106577122417695,
      "grad_norm": 4.453332424163818,
      "learning_rate": 4.1577852397985257e-05,
      "loss": 0.6564,
      "step": 1107600
    },
    {
      "epoch": 10.10748959778086,
      "grad_norm": 4.598830223083496,
      "learning_rate": 4.157709200184928e-05,
      "loss": 0.7065,
      "step": 1107700
    },
    {
      "epoch": 10.108402073144026,
      "grad_norm": 3.3363568782806396,
      "learning_rate": 4.157633160571332e-05,
      "loss": 0.6851,
      "step": 1107800
    },
    {
      "epoch": 10.109314548507191,
      "grad_norm": 4.303375244140625,
      "learning_rate": 4.157557120957734e-05,
      "loss": 0.6539,
      "step": 1107900
    },
    {
      "epoch": 10.110227023870355,
      "grad_norm": 3.931325912475586,
      "learning_rate": 4.157481081344137e-05,
      "loss": 0.7105,
      "step": 1108000
    },
    {
      "epoch": 10.11113949923352,
      "grad_norm": 4.302798748016357,
      "learning_rate": 4.15740504173054e-05,
      "loss": 0.6911,
      "step": 1108100
    },
    {
      "epoch": 10.112051974596685,
      "grad_norm": 4.545219898223877,
      "learning_rate": 4.157329002116943e-05,
      "loss": 0.688,
      "step": 1108200
    },
    {
      "epoch": 10.11296444995985,
      "grad_norm": 3.3692498207092285,
      "learning_rate": 4.157252962503346e-05,
      "loss": 0.7238,
      "step": 1108300
    },
    {
      "epoch": 10.113876925323016,
      "grad_norm": 3.969691038131714,
      "learning_rate": 4.157176922889749e-05,
      "loss": 0.6815,
      "step": 1108400
    },
    {
      "epoch": 10.114789400686181,
      "grad_norm": 3.5386292934417725,
      "learning_rate": 4.1571008832761514e-05,
      "loss": 0.6548,
      "step": 1108500
    },
    {
      "epoch": 10.115701876049346,
      "grad_norm": 2.9649839401245117,
      "learning_rate": 4.157024843662555e-05,
      "loss": 0.6793,
      "step": 1108600
    },
    {
      "epoch": 10.116614351412512,
      "grad_norm": 3.916200637817383,
      "learning_rate": 4.1569488040489574e-05,
      "loss": 0.6664,
      "step": 1108700
    },
    {
      "epoch": 10.117526826775677,
      "grad_norm": 5.245735168457031,
      "learning_rate": 4.1568727644353604e-05,
      "loss": 0.7209,
      "step": 1108800
    },
    {
      "epoch": 10.118439302138842,
      "grad_norm": 3.7890689373016357,
      "learning_rate": 4.1567967248217634e-05,
      "loss": 0.7127,
      "step": 1108900
    },
    {
      "epoch": 10.119351777502008,
      "grad_norm": 4.182442665100098,
      "learning_rate": 4.1567206852081664e-05,
      "loss": 0.6801,
      "step": 1109000
    },
    {
      "epoch": 10.120264252865173,
      "grad_norm": 4.160095691680908,
      "learning_rate": 4.156644645594569e-05,
      "loss": 0.6718,
      "step": 1109100
    },
    {
      "epoch": 10.121176728228338,
      "grad_norm": 4.620728969573975,
      "learning_rate": 4.1565686059809724e-05,
      "loss": 0.6739,
      "step": 1109200
    },
    {
      "epoch": 10.122089203591504,
      "grad_norm": 3.7806434631347656,
      "learning_rate": 4.156492566367375e-05,
      "loss": 0.6584,
      "step": 1109300
    },
    {
      "epoch": 10.123001678954669,
      "grad_norm": 3.2001302242279053,
      "learning_rate": 4.156416526753778e-05,
      "loss": 0.6838,
      "step": 1109400
    },
    {
      "epoch": 10.123914154317834,
      "grad_norm": 4.2936320304870605,
      "learning_rate": 4.156340487140181e-05,
      "loss": 0.6816,
      "step": 1109500
    },
    {
      "epoch": 10.124826629681,
      "grad_norm": 5.427567958831787,
      "learning_rate": 4.156264447526584e-05,
      "loss": 0.6782,
      "step": 1109600
    },
    {
      "epoch": 10.125739105044163,
      "grad_norm": 2.4923129081726074,
      "learning_rate": 4.156188407912987e-05,
      "loss": 0.6773,
      "step": 1109700
    },
    {
      "epoch": 10.126651580407328,
      "grad_norm": 4.485762596130371,
      "learning_rate": 4.15611236829939e-05,
      "loss": 0.6711,
      "step": 1109800
    },
    {
      "epoch": 10.127564055770494,
      "grad_norm": 4.75675106048584,
      "learning_rate": 4.156036328685792e-05,
      "loss": 0.6409,
      "step": 1109900
    },
    {
      "epoch": 10.128476531133659,
      "grad_norm": 5.295042514801025,
      "learning_rate": 4.155960289072196e-05,
      "loss": 0.6785,
      "step": 1110000
    },
    {
      "epoch": 10.129389006496824,
      "grad_norm": 3.8823184967041016,
      "learning_rate": 4.155884249458598e-05,
      "loss": 0.7229,
      "step": 1110100
    },
    {
      "epoch": 10.13030148185999,
      "grad_norm": 4.058512210845947,
      "learning_rate": 4.155808209845001e-05,
      "loss": 0.6931,
      "step": 1110200
    },
    {
      "epoch": 10.131213957223155,
      "grad_norm": 3.9835915565490723,
      "learning_rate": 4.155732170231404e-05,
      "loss": 0.6416,
      "step": 1110300
    },
    {
      "epoch": 10.13212643258632,
      "grad_norm": 3.9771904945373535,
      "learning_rate": 4.155656130617807e-05,
      "loss": 0.6836,
      "step": 1110400
    },
    {
      "epoch": 10.133038907949485,
      "grad_norm": 4.175872802734375,
      "learning_rate": 4.1555800910042095e-05,
      "loss": 0.6892,
      "step": 1110500
    },
    {
      "epoch": 10.13395138331265,
      "grad_norm": 4.213757514953613,
      "learning_rate": 4.1555040513906125e-05,
      "loss": 0.6691,
      "step": 1110600
    },
    {
      "epoch": 10.134863858675816,
      "grad_norm": 4.111266136169434,
      "learning_rate": 4.1554280117770155e-05,
      "loss": 0.6898,
      "step": 1110700
    },
    {
      "epoch": 10.135776334038981,
      "grad_norm": 4.358707427978516,
      "learning_rate": 4.1553519721634185e-05,
      "loss": 0.7197,
      "step": 1110800
    },
    {
      "epoch": 10.136688809402147,
      "grad_norm": 4.322549819946289,
      "learning_rate": 4.1552759325498215e-05,
      "loss": 0.7526,
      "step": 1110900
    },
    {
      "epoch": 10.137601284765312,
      "grad_norm": 3.7280936241149902,
      "learning_rate": 4.155199892936224e-05,
      "loss": 0.6974,
      "step": 1111000
    },
    {
      "epoch": 10.138513760128477,
      "grad_norm": 3.058142900466919,
      "learning_rate": 4.1551238533226275e-05,
      "loss": 0.7021,
      "step": 1111100
    },
    {
      "epoch": 10.139426235491642,
      "grad_norm": 3.2994253635406494,
      "learning_rate": 4.15504781370903e-05,
      "loss": 0.6825,
      "step": 1111200
    },
    {
      "epoch": 10.140338710854808,
      "grad_norm": 5.383452892303467,
      "learning_rate": 4.154971774095433e-05,
      "loss": 0.657,
      "step": 1111300
    },
    {
      "epoch": 10.141251186217971,
      "grad_norm": 2.7189857959747314,
      "learning_rate": 4.154895734481836e-05,
      "loss": 0.6815,
      "step": 1111400
    },
    {
      "epoch": 10.142163661581137,
      "grad_norm": 4.313912868499756,
      "learning_rate": 4.154819694868239e-05,
      "loss": 0.6727,
      "step": 1111500
    },
    {
      "epoch": 10.143076136944302,
      "grad_norm": 3.7907631397247314,
      "learning_rate": 4.154743655254641e-05,
      "loss": 0.7002,
      "step": 1111600
    },
    {
      "epoch": 10.143988612307467,
      "grad_norm": 4.138967990875244,
      "learning_rate": 4.154667615641045e-05,
      "loss": 0.6739,
      "step": 1111700
    },
    {
      "epoch": 10.144901087670632,
      "grad_norm": 3.1306912899017334,
      "learning_rate": 4.154591576027447e-05,
      "loss": 0.6841,
      "step": 1111800
    },
    {
      "epoch": 10.145813563033798,
      "grad_norm": 4.153914451599121,
      "learning_rate": 4.15451553641385e-05,
      "loss": 0.7065,
      "step": 1111900
    },
    {
      "epoch": 10.146726038396963,
      "grad_norm": 4.584527015686035,
      "learning_rate": 4.154439496800253e-05,
      "loss": 0.7183,
      "step": 1112000
    },
    {
      "epoch": 10.147638513760128,
      "grad_norm": 3.7861034870147705,
      "learning_rate": 4.154363457186656e-05,
      "loss": 0.6598,
      "step": 1112100
    },
    {
      "epoch": 10.148550989123294,
      "grad_norm": 3.74422025680542,
      "learning_rate": 4.154287417573059e-05,
      "loss": 0.6781,
      "step": 1112200
    },
    {
      "epoch": 10.149463464486459,
      "grad_norm": 4.125439167022705,
      "learning_rate": 4.154211377959462e-05,
      "loss": 0.6557,
      "step": 1112300
    },
    {
      "epoch": 10.150375939849624,
      "grad_norm": 4.074431896209717,
      "learning_rate": 4.1541353383458646e-05,
      "loss": 0.6873,
      "step": 1112400
    },
    {
      "epoch": 10.15128841521279,
      "grad_norm": 3.5686733722686768,
      "learning_rate": 4.154059298732268e-05,
      "loss": 0.6682,
      "step": 1112500
    },
    {
      "epoch": 10.152200890575955,
      "grad_norm": 3.5585999488830566,
      "learning_rate": 4.1539832591186706e-05,
      "loss": 0.7074,
      "step": 1112600
    },
    {
      "epoch": 10.15311336593912,
      "grad_norm": 4.018115043640137,
      "learning_rate": 4.1539072195050736e-05,
      "loss": 0.6867,
      "step": 1112700
    },
    {
      "epoch": 10.154025841302285,
      "grad_norm": 3.997462749481201,
      "learning_rate": 4.1538311798914766e-05,
      "loss": 0.7028,
      "step": 1112800
    },
    {
      "epoch": 10.15493831666545,
      "grad_norm": 4.1100335121154785,
      "learning_rate": 4.1537551402778796e-05,
      "loss": 0.696,
      "step": 1112900
    },
    {
      "epoch": 10.155850792028616,
      "grad_norm": 4.206778526306152,
      "learning_rate": 4.153679100664282e-05,
      "loss": 0.6624,
      "step": 1113000
    },
    {
      "epoch": 10.15676326739178,
      "grad_norm": 3.8178153038024902,
      "learning_rate": 4.1536030610506856e-05,
      "loss": 0.7037,
      "step": 1113100
    },
    {
      "epoch": 10.157675742754945,
      "grad_norm": 4.769105911254883,
      "learning_rate": 4.153527021437088e-05,
      "loss": 0.6557,
      "step": 1113200
    },
    {
      "epoch": 10.15858821811811,
      "grad_norm": 4.2231621742248535,
      "learning_rate": 4.153450981823491e-05,
      "loss": 0.6783,
      "step": 1113300
    },
    {
      "epoch": 10.159500693481275,
      "grad_norm": 5.2468581199646,
      "learning_rate": 4.153374942209894e-05,
      "loss": 0.6534,
      "step": 1113400
    },
    {
      "epoch": 10.16041316884444,
      "grad_norm": 4.350721836090088,
      "learning_rate": 4.153298902596296e-05,
      "loss": 0.692,
      "step": 1113500
    },
    {
      "epoch": 10.161325644207606,
      "grad_norm": 3.690389394760132,
      "learning_rate": 4.1532228629827e-05,
      "loss": 0.6397,
      "step": 1113600
    },
    {
      "epoch": 10.162238119570771,
      "grad_norm": 3.811415195465088,
      "learning_rate": 4.153146823369102e-05,
      "loss": 0.6619,
      "step": 1113700
    },
    {
      "epoch": 10.163150594933937,
      "grad_norm": 4.10767126083374,
      "learning_rate": 4.153070783755505e-05,
      "loss": 0.6803,
      "step": 1113800
    },
    {
      "epoch": 10.164063070297102,
      "grad_norm": 4.032208442687988,
      "learning_rate": 4.152994744141908e-05,
      "loss": 0.6803,
      "step": 1113900
    },
    {
      "epoch": 10.164975545660267,
      "grad_norm": 4.972049236297607,
      "learning_rate": 4.152918704528311e-05,
      "loss": 0.6675,
      "step": 1114000
    },
    {
      "epoch": 10.165888021023433,
      "grad_norm": 4.025146484375,
      "learning_rate": 4.1528426649147136e-05,
      "loss": 0.695,
      "step": 1114100
    },
    {
      "epoch": 10.166800496386598,
      "grad_norm": 3.0976507663726807,
      "learning_rate": 4.152766625301117e-05,
      "loss": 0.6781,
      "step": 1114200
    },
    {
      "epoch": 10.167712971749763,
      "grad_norm": 3.3476202487945557,
      "learning_rate": 4.1526905856875196e-05,
      "loss": 0.7005,
      "step": 1114300
    },
    {
      "epoch": 10.168625447112928,
      "grad_norm": 3.4151811599731445,
      "learning_rate": 4.1526145460739227e-05,
      "loss": 0.6708,
      "step": 1114400
    },
    {
      "epoch": 10.169537922476094,
      "grad_norm": 4.191957950592041,
      "learning_rate": 4.1525385064603257e-05,
      "loss": 0.6635,
      "step": 1114500
    },
    {
      "epoch": 10.170450397839259,
      "grad_norm": 2.8542354106903076,
      "learning_rate": 4.152462466846729e-05,
      "loss": 0.6938,
      "step": 1114600
    },
    {
      "epoch": 10.171362873202424,
      "grad_norm": 4.537464141845703,
      "learning_rate": 4.152386427233132e-05,
      "loss": 0.6564,
      "step": 1114700
    },
    {
      "epoch": 10.172275348565588,
      "grad_norm": 3.70930552482605,
      "learning_rate": 4.152310387619535e-05,
      "loss": 0.6639,
      "step": 1114800
    },
    {
      "epoch": 10.173187823928753,
      "grad_norm": 4.7929558753967285,
      "learning_rate": 4.152234348005937e-05,
      "loss": 0.7092,
      "step": 1114900
    },
    {
      "epoch": 10.174100299291919,
      "grad_norm": 3.7366981506347656,
      "learning_rate": 4.152158308392341e-05,
      "loss": 0.6802,
      "step": 1115000
    },
    {
      "epoch": 10.175012774655084,
      "grad_norm": 3.2927777767181396,
      "learning_rate": 4.152082268778743e-05,
      "loss": 0.6825,
      "step": 1115100
    },
    {
      "epoch": 10.175925250018249,
      "grad_norm": 3.2173705101013184,
      "learning_rate": 4.152006229165146e-05,
      "loss": 0.6723,
      "step": 1115200
    },
    {
      "epoch": 10.176837725381414,
      "grad_norm": 4.469080924987793,
      "learning_rate": 4.151930189551549e-05,
      "loss": 0.6589,
      "step": 1115300
    },
    {
      "epoch": 10.17775020074458,
      "grad_norm": 4.11226224899292,
      "learning_rate": 4.151854149937952e-05,
      "loss": 0.6644,
      "step": 1115400
    },
    {
      "epoch": 10.178662676107745,
      "grad_norm": 4.261018753051758,
      "learning_rate": 4.1517781103243544e-05,
      "loss": 0.7085,
      "step": 1115500
    },
    {
      "epoch": 10.17957515147091,
      "grad_norm": 3.7228925228118896,
      "learning_rate": 4.151702070710758e-05,
      "loss": 0.6887,
      "step": 1115600
    },
    {
      "epoch": 10.180487626834076,
      "grad_norm": 3.787104845046997,
      "learning_rate": 4.1516260310971604e-05,
      "loss": 0.6476,
      "step": 1115700
    },
    {
      "epoch": 10.181400102197241,
      "grad_norm": 3.9569993019104004,
      "learning_rate": 4.1515499914835634e-05,
      "loss": 0.6726,
      "step": 1115800
    },
    {
      "epoch": 10.182312577560406,
      "grad_norm": 3.7893238067626953,
      "learning_rate": 4.1514739518699664e-05,
      "loss": 0.6975,
      "step": 1115900
    },
    {
      "epoch": 10.183225052923571,
      "grad_norm": 3.1609737873077393,
      "learning_rate": 4.1513979122563694e-05,
      "loss": 0.6736,
      "step": 1116000
    },
    {
      "epoch": 10.184137528286737,
      "grad_norm": 3.912599802017212,
      "learning_rate": 4.1513218726427724e-05,
      "loss": 0.652,
      "step": 1116100
    },
    {
      "epoch": 10.185050003649902,
      "grad_norm": 3.8560519218444824,
      "learning_rate": 4.151245833029175e-05,
      "loss": 0.6565,
      "step": 1116200
    },
    {
      "epoch": 10.185962479013067,
      "grad_norm": 3.655379056930542,
      "learning_rate": 4.151169793415578e-05,
      "loss": 0.6785,
      "step": 1116300
    },
    {
      "epoch": 10.186874954376233,
      "grad_norm": 2.9691531658172607,
      "learning_rate": 4.151093753801981e-05,
      "loss": 0.6578,
      "step": 1116400
    },
    {
      "epoch": 10.187787429739396,
      "grad_norm": 4.030835151672363,
      "learning_rate": 4.151017714188384e-05,
      "loss": 0.6668,
      "step": 1116500
    },
    {
      "epoch": 10.188699905102562,
      "grad_norm": 3.64927339553833,
      "learning_rate": 4.150941674574787e-05,
      "loss": 0.6832,
      "step": 1116600
    },
    {
      "epoch": 10.189612380465727,
      "grad_norm": 4.826096534729004,
      "learning_rate": 4.15086563496119e-05,
      "loss": 0.6937,
      "step": 1116700
    },
    {
      "epoch": 10.190524855828892,
      "grad_norm": 3.0986342430114746,
      "learning_rate": 4.150789595347592e-05,
      "loss": 0.6796,
      "step": 1116800
    },
    {
      "epoch": 10.191437331192057,
      "grad_norm": 4.0828399658203125,
      "learning_rate": 4.150713555733996e-05,
      "loss": 0.7105,
      "step": 1116900
    },
    {
      "epoch": 10.192349806555223,
      "grad_norm": 4.407886505126953,
      "learning_rate": 4.150637516120398e-05,
      "loss": 0.6605,
      "step": 1117000
    },
    {
      "epoch": 10.193262281918388,
      "grad_norm": 4.418300628662109,
      "learning_rate": 4.150561476506801e-05,
      "loss": 0.6987,
      "step": 1117100
    },
    {
      "epoch": 10.194174757281553,
      "grad_norm": 5.187317371368408,
      "learning_rate": 4.150485436893204e-05,
      "loss": 0.7445,
      "step": 1117200
    },
    {
      "epoch": 10.195087232644719,
      "grad_norm": 3.5744740962982178,
      "learning_rate": 4.150409397279607e-05,
      "loss": 0.6769,
      "step": 1117300
    },
    {
      "epoch": 10.195999708007884,
      "grad_norm": 3.719101667404175,
      "learning_rate": 4.1503333576660095e-05,
      "loss": 0.6743,
      "step": 1117400
    },
    {
      "epoch": 10.19691218337105,
      "grad_norm": 4.571559429168701,
      "learning_rate": 4.150257318052413e-05,
      "loss": 0.7156,
      "step": 1117500
    },
    {
      "epoch": 10.197824658734215,
      "grad_norm": 4.253188133239746,
      "learning_rate": 4.1501812784388155e-05,
      "loss": 0.7155,
      "step": 1117600
    },
    {
      "epoch": 10.19873713409738,
      "grad_norm": 4.425680160522461,
      "learning_rate": 4.1501052388252185e-05,
      "loss": 0.6921,
      "step": 1117700
    },
    {
      "epoch": 10.199649609460545,
      "grad_norm": 3.7289063930511475,
      "learning_rate": 4.1500291992116215e-05,
      "loss": 0.6734,
      "step": 1117800
    },
    {
      "epoch": 10.20056208482371,
      "grad_norm": 4.112410545349121,
      "learning_rate": 4.1499531595980245e-05,
      "loss": 0.6794,
      "step": 1117900
    },
    {
      "epoch": 10.201474560186876,
      "grad_norm": 3.2707109451293945,
      "learning_rate": 4.1498771199844275e-05,
      "loss": 0.6991,
      "step": 1118000
    },
    {
      "epoch": 10.202387035550041,
      "grad_norm": 4.777492046356201,
      "learning_rate": 4.1498010803708305e-05,
      "loss": 0.6683,
      "step": 1118100
    },
    {
      "epoch": 10.203299510913205,
      "grad_norm": 4.283174514770508,
      "learning_rate": 4.149725040757233e-05,
      "loss": 0.6814,
      "step": 1118200
    },
    {
      "epoch": 10.20421198627637,
      "grad_norm": 3.8810107707977295,
      "learning_rate": 4.1496490011436365e-05,
      "loss": 0.662,
      "step": 1118300
    },
    {
      "epoch": 10.205124461639535,
      "grad_norm": 3.0903031826019287,
      "learning_rate": 4.149572961530039e-05,
      "loss": 0.6718,
      "step": 1118400
    },
    {
      "epoch": 10.2060369370027,
      "grad_norm": 3.8956267833709717,
      "learning_rate": 4.149496921916442e-05,
      "loss": 0.668,
      "step": 1118500
    },
    {
      "epoch": 10.206949412365866,
      "grad_norm": 4.2334303855896,
      "learning_rate": 4.149420882302845e-05,
      "loss": 0.6704,
      "step": 1118600
    },
    {
      "epoch": 10.207861887729031,
      "grad_norm": 4.013630390167236,
      "learning_rate": 4.149344842689248e-05,
      "loss": 0.6954,
      "step": 1118700
    },
    {
      "epoch": 10.208774363092196,
      "grad_norm": 4.061002731323242,
      "learning_rate": 4.14926880307565e-05,
      "loss": 0.6397,
      "step": 1118800
    },
    {
      "epoch": 10.209686838455362,
      "grad_norm": 3.5292446613311768,
      "learning_rate": 4.149192763462054e-05,
      "loss": 0.6785,
      "step": 1118900
    },
    {
      "epoch": 10.210599313818527,
      "grad_norm": 3.8659322261810303,
      "learning_rate": 4.149116723848456e-05,
      "loss": 0.7142,
      "step": 1119000
    },
    {
      "epoch": 10.211511789181692,
      "grad_norm": 4.992618560791016,
      "learning_rate": 4.149040684234859e-05,
      "loss": 0.7158,
      "step": 1119100
    },
    {
      "epoch": 10.212424264544858,
      "grad_norm": 4.964994430541992,
      "learning_rate": 4.148964644621262e-05,
      "loss": 0.6615,
      "step": 1119200
    },
    {
      "epoch": 10.213336739908023,
      "grad_norm": 3.08927583694458,
      "learning_rate": 4.1488886050076646e-05,
      "loss": 0.6842,
      "step": 1119300
    },
    {
      "epoch": 10.214249215271188,
      "grad_norm": 4.575221061706543,
      "learning_rate": 4.148812565394068e-05,
      "loss": 0.6814,
      "step": 1119400
    },
    {
      "epoch": 10.215161690634353,
      "grad_norm": 2.800182580947876,
      "learning_rate": 4.1487365257804706e-05,
      "loss": 0.6668,
      "step": 1119500
    },
    {
      "epoch": 10.216074165997519,
      "grad_norm": 4.043365001678467,
      "learning_rate": 4.1486604861668736e-05,
      "loss": 0.7057,
      "step": 1119600
    },
    {
      "epoch": 10.216986641360684,
      "grad_norm": 3.6342153549194336,
      "learning_rate": 4.1485844465532766e-05,
      "loss": 0.6689,
      "step": 1119700
    },
    {
      "epoch": 10.21789911672385,
      "grad_norm": 3.847754716873169,
      "learning_rate": 4.1485084069396796e-05,
      "loss": 0.679,
      "step": 1119800
    },
    {
      "epoch": 10.218811592087013,
      "grad_norm": 3.9798591136932373,
      "learning_rate": 4.148432367326082e-05,
      "loss": 0.6857,
      "step": 1119900
    },
    {
      "epoch": 10.219724067450178,
      "grad_norm": 3.410796642303467,
      "learning_rate": 4.1483563277124856e-05,
      "loss": 0.664,
      "step": 1120000
    },
    {
      "epoch": 10.220636542813343,
      "grad_norm": 4.433970928192139,
      "learning_rate": 4.148280288098888e-05,
      "loss": 0.6812,
      "step": 1120100
    },
    {
      "epoch": 10.221549018176509,
      "grad_norm": 3.1762642860412598,
      "learning_rate": 4.148204248485291e-05,
      "loss": 0.699,
      "step": 1120200
    },
    {
      "epoch": 10.222461493539674,
      "grad_norm": 4.3163371086120605,
      "learning_rate": 4.148128208871694e-05,
      "loss": 0.7149,
      "step": 1120300
    },
    {
      "epoch": 10.22337396890284,
      "grad_norm": 3.9777281284332275,
      "learning_rate": 4.148052169258097e-05,
      "loss": 0.691,
      "step": 1120400
    },
    {
      "epoch": 10.224286444266005,
      "grad_norm": 3.7282774448394775,
      "learning_rate": 4.1479761296445e-05,
      "loss": 0.7115,
      "step": 1120500
    },
    {
      "epoch": 10.22519891962917,
      "grad_norm": 2.454021692276001,
      "learning_rate": 4.147900090030903e-05,
      "loss": 0.7103,
      "step": 1120600
    },
    {
      "epoch": 10.226111394992335,
      "grad_norm": 4.44895601272583,
      "learning_rate": 4.147824050417305e-05,
      "loss": 0.686,
      "step": 1120700
    },
    {
      "epoch": 10.2270238703555,
      "grad_norm": 4.823480129241943,
      "learning_rate": 4.147748010803709e-05,
      "loss": 0.6731,
      "step": 1120800
    },
    {
      "epoch": 10.227936345718666,
      "grad_norm": 4.1553778648376465,
      "learning_rate": 4.147671971190111e-05,
      "loss": 0.7012,
      "step": 1120900
    },
    {
      "epoch": 10.228848821081831,
      "grad_norm": 4.164741039276123,
      "learning_rate": 4.147595931576514e-05,
      "loss": 0.6945,
      "step": 1121000
    },
    {
      "epoch": 10.229761296444996,
      "grad_norm": 3.4156603813171387,
      "learning_rate": 4.147519891962917e-05,
      "loss": 0.6633,
      "step": 1121100
    },
    {
      "epoch": 10.230673771808162,
      "grad_norm": 3.6717379093170166,
      "learning_rate": 4.14744385234932e-05,
      "loss": 0.6736,
      "step": 1121200
    },
    {
      "epoch": 10.231586247171327,
      "grad_norm": 3.896435499191284,
      "learning_rate": 4.147367812735723e-05,
      "loss": 0.6977,
      "step": 1121300
    },
    {
      "epoch": 10.232498722534492,
      "grad_norm": 3.6686952114105225,
      "learning_rate": 4.1472917731221263e-05,
      "loss": 0.6727,
      "step": 1121400
    },
    {
      "epoch": 10.233411197897658,
      "grad_norm": 3.7041268348693848,
      "learning_rate": 4.147215733508529e-05,
      "loss": 0.6442,
      "step": 1121500
    },
    {
      "epoch": 10.234323673260821,
      "grad_norm": 4.027533531188965,
      "learning_rate": 4.147139693894932e-05,
      "loss": 0.6632,
      "step": 1121600
    },
    {
      "epoch": 10.235236148623986,
      "grad_norm": 4.386765480041504,
      "learning_rate": 4.147063654281335e-05,
      "loss": 0.6728,
      "step": 1121700
    },
    {
      "epoch": 10.236148623987152,
      "grad_norm": 4.850071907043457,
      "learning_rate": 4.146987614667738e-05,
      "loss": 0.6897,
      "step": 1121800
    },
    {
      "epoch": 10.237061099350317,
      "grad_norm": 3.4004950523376465,
      "learning_rate": 4.146911575054141e-05,
      "loss": 0.6971,
      "step": 1121900
    },
    {
      "epoch": 10.237973574713482,
      "grad_norm": 4.152105808258057,
      "learning_rate": 4.146835535440543e-05,
      "loss": 0.7216,
      "step": 1122000
    },
    {
      "epoch": 10.238886050076648,
      "grad_norm": 4.5417985916137695,
      "learning_rate": 4.146759495826946e-05,
      "loss": 0.6603,
      "step": 1122100
    },
    {
      "epoch": 10.239798525439813,
      "grad_norm": 3.7230224609375,
      "learning_rate": 4.146683456213349e-05,
      "loss": 0.6818,
      "step": 1122200
    },
    {
      "epoch": 10.240711000802978,
      "grad_norm": 4.249044418334961,
      "learning_rate": 4.146607416599752e-05,
      "loss": 0.6914,
      "step": 1122300
    },
    {
      "epoch": 10.241623476166144,
      "grad_norm": 4.2731404304504395,
      "learning_rate": 4.1465313769861544e-05,
      "loss": 0.6963,
      "step": 1122400
    },
    {
      "epoch": 10.242535951529309,
      "grad_norm": 3.6798343658447266,
      "learning_rate": 4.146455337372558e-05,
      "loss": 0.6638,
      "step": 1122500
    },
    {
      "epoch": 10.243448426892474,
      "grad_norm": 3.8704967498779297,
      "learning_rate": 4.1463792977589604e-05,
      "loss": 0.682,
      "step": 1122600
    },
    {
      "epoch": 10.24436090225564,
      "grad_norm": 3.778909206390381,
      "learning_rate": 4.1463032581453634e-05,
      "loss": 0.6518,
      "step": 1122700
    },
    {
      "epoch": 10.245273377618805,
      "grad_norm": 4.442774295806885,
      "learning_rate": 4.1462272185317664e-05,
      "loss": 0.6833,
      "step": 1122800
    },
    {
      "epoch": 10.24618585298197,
      "grad_norm": 4.430440425872803,
      "learning_rate": 4.1461511789181694e-05,
      "loss": 0.6609,
      "step": 1122900
    },
    {
      "epoch": 10.247098328345135,
      "grad_norm": 3.698319673538208,
      "learning_rate": 4.1460751393045724e-05,
      "loss": 0.714,
      "step": 1123000
    },
    {
      "epoch": 10.2480108037083,
      "grad_norm": 3.2824034690856934,
      "learning_rate": 4.1459990996909754e-05,
      "loss": 0.7272,
      "step": 1123100
    },
    {
      "epoch": 10.248923279071466,
      "grad_norm": 3.3498501777648926,
      "learning_rate": 4.145923060077378e-05,
      "loss": 0.6445,
      "step": 1123200
    },
    {
      "epoch": 10.24983575443463,
      "grad_norm": 3.5833232402801514,
      "learning_rate": 4.1458470204637814e-05,
      "loss": 0.6769,
      "step": 1123300
    },
    {
      "epoch": 10.250748229797795,
      "grad_norm": 3.845095634460449,
      "learning_rate": 4.145770980850184e-05,
      "loss": 0.6958,
      "step": 1123400
    },
    {
      "epoch": 10.25166070516096,
      "grad_norm": 3.3391072750091553,
      "learning_rate": 4.145694941236587e-05,
      "loss": 0.6811,
      "step": 1123500
    },
    {
      "epoch": 10.252573180524125,
      "grad_norm": 4.101563930511475,
      "learning_rate": 4.14561890162299e-05,
      "loss": 0.7026,
      "step": 1123600
    },
    {
      "epoch": 10.25348565588729,
      "grad_norm": 3.7479615211486816,
      "learning_rate": 4.145542862009393e-05,
      "loss": 0.6993,
      "step": 1123700
    },
    {
      "epoch": 10.254398131250456,
      "grad_norm": 4.454775333404541,
      "learning_rate": 4.145466822395795e-05,
      "loss": 0.6731,
      "step": 1123800
    },
    {
      "epoch": 10.255310606613621,
      "grad_norm": 3.8462090492248535,
      "learning_rate": 4.145390782782199e-05,
      "loss": 0.6958,
      "step": 1123900
    },
    {
      "epoch": 10.256223081976787,
      "grad_norm": 3.6414599418640137,
      "learning_rate": 4.145314743168601e-05,
      "loss": 0.681,
      "step": 1124000
    },
    {
      "epoch": 10.257135557339952,
      "grad_norm": 3.709587574005127,
      "learning_rate": 4.145238703555004e-05,
      "loss": 0.7071,
      "step": 1124100
    },
    {
      "epoch": 10.258048032703117,
      "grad_norm": 4.780714988708496,
      "learning_rate": 4.145162663941407e-05,
      "loss": 0.6943,
      "step": 1124200
    },
    {
      "epoch": 10.258960508066282,
      "grad_norm": 3.7363710403442383,
      "learning_rate": 4.14508662432781e-05,
      "loss": 0.6785,
      "step": 1124300
    },
    {
      "epoch": 10.259872983429448,
      "grad_norm": 3.9748024940490723,
      "learning_rate": 4.145010584714213e-05,
      "loss": 0.6821,
      "step": 1124400
    },
    {
      "epoch": 10.260785458792613,
      "grad_norm": 3.5716352462768555,
      "learning_rate": 4.144934545100616e-05,
      "loss": 0.6725,
      "step": 1124500
    },
    {
      "epoch": 10.261697934155778,
      "grad_norm": 4.036770343780518,
      "learning_rate": 4.1448585054870185e-05,
      "loss": 0.6596,
      "step": 1124600
    },
    {
      "epoch": 10.262610409518944,
      "grad_norm": 4.395716667175293,
      "learning_rate": 4.144782465873422e-05,
      "loss": 0.6717,
      "step": 1124700
    },
    {
      "epoch": 10.263522884882109,
      "grad_norm": 3.5066888332366943,
      "learning_rate": 4.1447064262598245e-05,
      "loss": 0.6949,
      "step": 1124800
    },
    {
      "epoch": 10.264435360245272,
      "grad_norm": 3.942660093307495,
      "learning_rate": 4.144630386646227e-05,
      "loss": 0.6811,
      "step": 1124900
    },
    {
      "epoch": 10.265347835608438,
      "grad_norm": 3.0186045169830322,
      "learning_rate": 4.1445543470326305e-05,
      "loss": 0.6908,
      "step": 1125000
    },
    {
      "epoch": 10.266260310971603,
      "grad_norm": 3.9892685413360596,
      "learning_rate": 4.144478307419033e-05,
      "loss": 0.699,
      "step": 1125100
    },
    {
      "epoch": 10.267172786334768,
      "grad_norm": 4.665522575378418,
      "learning_rate": 4.144402267805436e-05,
      "loss": 0.6779,
      "step": 1125200
    },
    {
      "epoch": 10.268085261697934,
      "grad_norm": 3.2199602127075195,
      "learning_rate": 4.144326228191839e-05,
      "loss": 0.6651,
      "step": 1125300
    },
    {
      "epoch": 10.268997737061099,
      "grad_norm": 3.606294870376587,
      "learning_rate": 4.144250188578242e-05,
      "loss": 0.6575,
      "step": 1125400
    },
    {
      "epoch": 10.269910212424264,
      "grad_norm": 4.482204914093018,
      "learning_rate": 4.144174148964645e-05,
      "loss": 0.7055,
      "step": 1125500
    },
    {
      "epoch": 10.27082268778743,
      "grad_norm": 3.387014627456665,
      "learning_rate": 4.144098109351048e-05,
      "loss": 0.7197,
      "step": 1125600
    },
    {
      "epoch": 10.271735163150595,
      "grad_norm": 4.057211399078369,
      "learning_rate": 4.14402206973745e-05,
      "loss": 0.6891,
      "step": 1125700
    },
    {
      "epoch": 10.27264763851376,
      "grad_norm": 3.6545565128326416,
      "learning_rate": 4.143946030123854e-05,
      "loss": 0.6685,
      "step": 1125800
    },
    {
      "epoch": 10.273560113876925,
      "grad_norm": 4.623692989349365,
      "learning_rate": 4.143869990510256e-05,
      "loss": 0.6658,
      "step": 1125900
    },
    {
      "epoch": 10.27447258924009,
      "grad_norm": 3.7540314197540283,
      "learning_rate": 4.143793950896659e-05,
      "loss": 0.6542,
      "step": 1126000
    },
    {
      "epoch": 10.275385064603256,
      "grad_norm": 4.579720497131348,
      "learning_rate": 4.143717911283062e-05,
      "loss": 0.7082,
      "step": 1126100
    },
    {
      "epoch": 10.276297539966421,
      "grad_norm": 4.559085369110107,
      "learning_rate": 4.143641871669465e-05,
      "loss": 0.6891,
      "step": 1126200
    },
    {
      "epoch": 10.277210015329587,
      "grad_norm": 4.867029190063477,
      "learning_rate": 4.1435658320558676e-05,
      "loss": 0.6955,
      "step": 1126300
    },
    {
      "epoch": 10.278122490692752,
      "grad_norm": 4.514400959014893,
      "learning_rate": 4.143489792442271e-05,
      "loss": 0.6688,
      "step": 1126400
    },
    {
      "epoch": 10.279034966055917,
      "grad_norm": 3.517005443572998,
      "learning_rate": 4.1434137528286736e-05,
      "loss": 0.6989,
      "step": 1126500
    },
    {
      "epoch": 10.279947441419083,
      "grad_norm": 3.6733877658843994,
      "learning_rate": 4.1433377132150766e-05,
      "loss": 0.6848,
      "step": 1126600
    },
    {
      "epoch": 10.280859916782246,
      "grad_norm": 4.425020217895508,
      "learning_rate": 4.1432616736014796e-05,
      "loss": 0.6966,
      "step": 1126700
    },
    {
      "epoch": 10.281772392145411,
      "grad_norm": 3.975466251373291,
      "learning_rate": 4.1431856339878826e-05,
      "loss": 0.699,
      "step": 1126800
    },
    {
      "epoch": 10.282684867508577,
      "grad_norm": 3.334683895111084,
      "learning_rate": 4.1431095943742856e-05,
      "loss": 0.7065,
      "step": 1126900
    },
    {
      "epoch": 10.283597342871742,
      "grad_norm": 4.4241790771484375,
      "learning_rate": 4.1430335547606886e-05,
      "loss": 0.7065,
      "step": 1127000
    },
    {
      "epoch": 10.284509818234907,
      "grad_norm": 4.009767055511475,
      "learning_rate": 4.142957515147091e-05,
      "loss": 0.7113,
      "step": 1127100
    },
    {
      "epoch": 10.285422293598073,
      "grad_norm": 4.286820888519287,
      "learning_rate": 4.1428814755334946e-05,
      "loss": 0.6888,
      "step": 1127200
    },
    {
      "epoch": 10.286334768961238,
      "grad_norm": 4.308892726898193,
      "learning_rate": 4.142805435919897e-05,
      "loss": 0.6792,
      "step": 1127300
    },
    {
      "epoch": 10.287247244324403,
      "grad_norm": 2.303757905960083,
      "learning_rate": 4.1427293963063e-05,
      "loss": 0.6752,
      "step": 1127400
    },
    {
      "epoch": 10.288159719687568,
      "grad_norm": 4.649117946624756,
      "learning_rate": 4.142653356692703e-05,
      "loss": 0.6799,
      "step": 1127500
    },
    {
      "epoch": 10.289072195050734,
      "grad_norm": 3.556835651397705,
      "learning_rate": 4.142577317079105e-05,
      "loss": 0.7163,
      "step": 1127600
    },
    {
      "epoch": 10.289984670413899,
      "grad_norm": 4.284279823303223,
      "learning_rate": 4.142501277465508e-05,
      "loss": 0.7151,
      "step": 1127700
    },
    {
      "epoch": 10.290897145777064,
      "grad_norm": 3.5427663326263428,
      "learning_rate": 4.142425237851911e-05,
      "loss": 0.6956,
      "step": 1127800
    },
    {
      "epoch": 10.29180962114023,
      "grad_norm": 3.9497387409210205,
      "learning_rate": 4.142349198238314e-05,
      "loss": 0.6902,
      "step": 1127900
    },
    {
      "epoch": 10.292722096503395,
      "grad_norm": 4.091540336608887,
      "learning_rate": 4.142273158624717e-05,
      "loss": 0.6724,
      "step": 1128000
    },
    {
      "epoch": 10.29363457186656,
      "grad_norm": 3.8751020431518555,
      "learning_rate": 4.1421971190111203e-05,
      "loss": 0.6899,
      "step": 1128100
    },
    {
      "epoch": 10.294547047229726,
      "grad_norm": 4.0143914222717285,
      "learning_rate": 4.142121079397523e-05,
      "loss": 0.7317,
      "step": 1128200
    },
    {
      "epoch": 10.295459522592889,
      "grad_norm": 4.40580415725708,
      "learning_rate": 4.1420450397839264e-05,
      "loss": 0.6808,
      "step": 1128300
    },
    {
      "epoch": 10.296371997956054,
      "grad_norm": 4.5636138916015625,
      "learning_rate": 4.141969000170329e-05,
      "loss": 0.6972,
      "step": 1128400
    },
    {
      "epoch": 10.29728447331922,
      "grad_norm": 4.354910850524902,
      "learning_rate": 4.141892960556732e-05,
      "loss": 0.6946,
      "step": 1128500
    },
    {
      "epoch": 10.298196948682385,
      "grad_norm": 3.7901723384857178,
      "learning_rate": 4.141816920943135e-05,
      "loss": 0.7094,
      "step": 1128600
    },
    {
      "epoch": 10.29910942404555,
      "grad_norm": 4.035014629364014,
      "learning_rate": 4.141740881329538e-05,
      "loss": 0.671,
      "step": 1128700
    },
    {
      "epoch": 10.300021899408716,
      "grad_norm": 3.5640697479248047,
      "learning_rate": 4.141664841715941e-05,
      "loss": 0.6907,
      "step": 1128800
    },
    {
      "epoch": 10.300934374771881,
      "grad_norm": 4.552933216094971,
      "learning_rate": 4.141588802102344e-05,
      "loss": 0.6851,
      "step": 1128900
    },
    {
      "epoch": 10.301846850135046,
      "grad_norm": 2.286790370941162,
      "learning_rate": 4.141512762488746e-05,
      "loss": 0.6945,
      "step": 1129000
    },
    {
      "epoch": 10.302759325498211,
      "grad_norm": 3.9468958377838135,
      "learning_rate": 4.141436722875149e-05,
      "loss": 0.652,
      "step": 1129100
    },
    {
      "epoch": 10.303671800861377,
      "grad_norm": 3.5110623836517334,
      "learning_rate": 4.141360683261552e-05,
      "loss": 0.6916,
      "step": 1129200
    },
    {
      "epoch": 10.304584276224542,
      "grad_norm": 3.9672133922576904,
      "learning_rate": 4.141284643647955e-05,
      "loss": 0.6678,
      "step": 1129300
    },
    {
      "epoch": 10.305496751587707,
      "grad_norm": 3.6461448669433594,
      "learning_rate": 4.141208604034358e-05,
      "loss": 0.7011,
      "step": 1129400
    },
    {
      "epoch": 10.306409226950873,
      "grad_norm": 4.07055139541626,
      "learning_rate": 4.141132564420761e-05,
      "loss": 0.6552,
      "step": 1129500
    },
    {
      "epoch": 10.307321702314038,
      "grad_norm": 5.1020355224609375,
      "learning_rate": 4.1410565248071634e-05,
      "loss": 0.6698,
      "step": 1129600
    },
    {
      "epoch": 10.308234177677203,
      "grad_norm": 2.5752251148223877,
      "learning_rate": 4.140980485193567e-05,
      "loss": 0.6803,
      "step": 1129700
    },
    {
      "epoch": 10.309146653040369,
      "grad_norm": 4.16689920425415,
      "learning_rate": 4.1409044455799694e-05,
      "loss": 0.6811,
      "step": 1129800
    },
    {
      "epoch": 10.310059128403534,
      "grad_norm": 3.702120542526245,
      "learning_rate": 4.1408284059663724e-05,
      "loss": 0.6849,
      "step": 1129900
    },
    {
      "epoch": 10.3109716037667,
      "grad_norm": 4.710087776184082,
      "learning_rate": 4.1407523663527754e-05,
      "loss": 0.6902,
      "step": 1130000
    },
    {
      "epoch": 10.311884079129863,
      "grad_norm": 3.7983810901641846,
      "learning_rate": 4.1406763267391784e-05,
      "loss": 0.6962,
      "step": 1130100
    },
    {
      "epoch": 10.312796554493028,
      "grad_norm": 2.9344184398651123,
      "learning_rate": 4.1406002871255815e-05,
      "loss": 0.6743,
      "step": 1130200
    },
    {
      "epoch": 10.313709029856193,
      "grad_norm": 4.007084846496582,
      "learning_rate": 4.1405242475119845e-05,
      "loss": 0.6281,
      "step": 1130300
    },
    {
      "epoch": 10.314621505219359,
      "grad_norm": 3.3157873153686523,
      "learning_rate": 4.140448207898387e-05,
      "loss": 0.6758,
      "step": 1130400
    },
    {
      "epoch": 10.315533980582524,
      "grad_norm": 4.515071868896484,
      "learning_rate": 4.14037216828479e-05,
      "loss": 0.6742,
      "step": 1130500
    },
    {
      "epoch": 10.31644645594569,
      "grad_norm": 3.5837557315826416,
      "learning_rate": 4.140296128671193e-05,
      "loss": 0.6549,
      "step": 1130600
    },
    {
      "epoch": 10.317358931308855,
      "grad_norm": 4.321076393127441,
      "learning_rate": 4.140220089057595e-05,
      "loss": 0.6099,
      "step": 1130700
    },
    {
      "epoch": 10.31827140667202,
      "grad_norm": 2.830198287963867,
      "learning_rate": 4.140144049443999e-05,
      "loss": 0.6665,
      "step": 1130800
    },
    {
      "epoch": 10.319183882035185,
      "grad_norm": 4.053259372711182,
      "learning_rate": 4.140068009830401e-05,
      "loss": 0.6739,
      "step": 1130900
    },
    {
      "epoch": 10.32009635739835,
      "grad_norm": 4.008164882659912,
      "learning_rate": 4.139991970216804e-05,
      "loss": 0.7159,
      "step": 1131000
    },
    {
      "epoch": 10.321008832761516,
      "grad_norm": 4.394692897796631,
      "learning_rate": 4.139915930603207e-05,
      "loss": 0.684,
      "step": 1131100
    },
    {
      "epoch": 10.321921308124681,
      "grad_norm": 3.959657669067383,
      "learning_rate": 4.13983989098961e-05,
      "loss": 0.7041,
      "step": 1131200
    },
    {
      "epoch": 10.322833783487846,
      "grad_norm": 4.691894054412842,
      "learning_rate": 4.139763851376013e-05,
      "loss": 0.7193,
      "step": 1131300
    },
    {
      "epoch": 10.323746258851012,
      "grad_norm": 5.702120304107666,
      "learning_rate": 4.139687811762416e-05,
      "loss": 0.6787,
      "step": 1131400
    },
    {
      "epoch": 10.324658734214177,
      "grad_norm": 4.6146368980407715,
      "learning_rate": 4.1396117721488185e-05,
      "loss": 0.6777,
      "step": 1131500
    },
    {
      "epoch": 10.325571209577342,
      "grad_norm": 3.792151927947998,
      "learning_rate": 4.139535732535222e-05,
      "loss": 0.676,
      "step": 1131600
    },
    {
      "epoch": 10.326483684940506,
      "grad_norm": 2.9474215507507324,
      "learning_rate": 4.1394596929216245e-05,
      "loss": 0.6862,
      "step": 1131700
    },
    {
      "epoch": 10.327396160303671,
      "grad_norm": 3.6230709552764893,
      "learning_rate": 4.1393836533080275e-05,
      "loss": 0.7035,
      "step": 1131800
    },
    {
      "epoch": 10.328308635666836,
      "grad_norm": 3.2422544956207275,
      "learning_rate": 4.1393076136944305e-05,
      "loss": 0.6682,
      "step": 1131900
    },
    {
      "epoch": 10.329221111030002,
      "grad_norm": 3.8594396114349365,
      "learning_rate": 4.1392315740808335e-05,
      "loss": 0.6721,
      "step": 1132000
    },
    {
      "epoch": 10.330133586393167,
      "grad_norm": 4.015964031219482,
      "learning_rate": 4.139155534467236e-05,
      "loss": 0.6354,
      "step": 1132100
    },
    {
      "epoch": 10.331046061756332,
      "grad_norm": 3.5830774307250977,
      "learning_rate": 4.1390794948536396e-05,
      "loss": 0.7093,
      "step": 1132200
    },
    {
      "epoch": 10.331958537119498,
      "grad_norm": 4.562864780426025,
      "learning_rate": 4.139003455240042e-05,
      "loss": 0.6737,
      "step": 1132300
    },
    {
      "epoch": 10.332871012482663,
      "grad_norm": 4.5110955238342285,
      "learning_rate": 4.138927415626445e-05,
      "loss": 0.6639,
      "step": 1132400
    },
    {
      "epoch": 10.333783487845828,
      "grad_norm": 4.1604905128479,
      "learning_rate": 4.138851376012848e-05,
      "loss": 0.7057,
      "step": 1132500
    },
    {
      "epoch": 10.334695963208993,
      "grad_norm": 3.569056510925293,
      "learning_rate": 4.138775336399251e-05,
      "loss": 0.7099,
      "step": 1132600
    },
    {
      "epoch": 10.335608438572159,
      "grad_norm": 3.6733272075653076,
      "learning_rate": 4.138699296785654e-05,
      "loss": 0.6734,
      "step": 1132700
    },
    {
      "epoch": 10.336520913935324,
      "grad_norm": 3.8785557746887207,
      "learning_rate": 4.138623257172057e-05,
      "loss": 0.6969,
      "step": 1132800
    },
    {
      "epoch": 10.33743338929849,
      "grad_norm": 3.984398603439331,
      "learning_rate": 4.138547217558459e-05,
      "loss": 0.7044,
      "step": 1132900
    },
    {
      "epoch": 10.338345864661655,
      "grad_norm": 5.074588775634766,
      "learning_rate": 4.138471177944863e-05,
      "loss": 0.7023,
      "step": 1133000
    },
    {
      "epoch": 10.33925834002482,
      "grad_norm": 4.17480993270874,
      "learning_rate": 4.138395138331265e-05,
      "loss": 0.707,
      "step": 1133100
    },
    {
      "epoch": 10.340170815387985,
      "grad_norm": 3.641235113143921,
      "learning_rate": 4.138319098717668e-05,
      "loss": 0.6818,
      "step": 1133200
    },
    {
      "epoch": 10.34108329075115,
      "grad_norm": 4.146754264831543,
      "learning_rate": 4.138243059104071e-05,
      "loss": 0.7196,
      "step": 1133300
    },
    {
      "epoch": 10.341995766114316,
      "grad_norm": 3.2387161254882812,
      "learning_rate": 4.1381670194904736e-05,
      "loss": 0.7095,
      "step": 1133400
    },
    {
      "epoch": 10.34290824147748,
      "grad_norm": 3.0874807834625244,
      "learning_rate": 4.1380909798768766e-05,
      "loss": 0.679,
      "step": 1133500
    },
    {
      "epoch": 10.343820716840645,
      "grad_norm": 4.056119918823242,
      "learning_rate": 4.1380149402632796e-05,
      "loss": 0.7014,
      "step": 1133600
    },
    {
      "epoch": 10.34473319220381,
      "grad_norm": 3.8656866550445557,
      "learning_rate": 4.1379389006496826e-05,
      "loss": 0.6739,
      "step": 1133700
    },
    {
      "epoch": 10.345645667566975,
      "grad_norm": 3.732609748840332,
      "learning_rate": 4.1378628610360856e-05,
      "loss": 0.69,
      "step": 1133800
    },
    {
      "epoch": 10.34655814293014,
      "grad_norm": 4.515535354614258,
      "learning_rate": 4.1377868214224886e-05,
      "loss": 0.7081,
      "step": 1133900
    },
    {
      "epoch": 10.347470618293306,
      "grad_norm": 3.7096729278564453,
      "learning_rate": 4.137710781808891e-05,
      "loss": 0.6857,
      "step": 1134000
    },
    {
      "epoch": 10.348383093656471,
      "grad_norm": 3.8561816215515137,
      "learning_rate": 4.1376347421952946e-05,
      "loss": 0.6636,
      "step": 1134100
    },
    {
      "epoch": 10.349295569019636,
      "grad_norm": 4.065492630004883,
      "learning_rate": 4.137558702581697e-05,
      "loss": 0.6643,
      "step": 1134200
    },
    {
      "epoch": 10.350208044382802,
      "grad_norm": 4.184448719024658,
      "learning_rate": 4.1374826629681e-05,
      "loss": 0.6612,
      "step": 1134300
    },
    {
      "epoch": 10.351120519745967,
      "grad_norm": 4.240149974822998,
      "learning_rate": 4.137406623354503e-05,
      "loss": 0.694,
      "step": 1134400
    },
    {
      "epoch": 10.352032995109132,
      "grad_norm": 3.8473575115203857,
      "learning_rate": 4.137330583740906e-05,
      "loss": 0.6942,
      "step": 1134500
    },
    {
      "epoch": 10.352945470472298,
      "grad_norm": 4.5089111328125,
      "learning_rate": 4.137254544127308e-05,
      "loss": 0.6656,
      "step": 1134600
    },
    {
      "epoch": 10.353857945835463,
      "grad_norm": 4.266933917999268,
      "learning_rate": 4.137178504513712e-05,
      "loss": 0.6886,
      "step": 1134700
    },
    {
      "epoch": 10.354770421198628,
      "grad_norm": 3.5971262454986572,
      "learning_rate": 4.1371024649001143e-05,
      "loss": 0.6464,
      "step": 1134800
    },
    {
      "epoch": 10.355682896561794,
      "grad_norm": 4.095631122589111,
      "learning_rate": 4.1370264252865173e-05,
      "loss": 0.6745,
      "step": 1134900
    },
    {
      "epoch": 10.356595371924959,
      "grad_norm": 4.282896518707275,
      "learning_rate": 4.1369503856729204e-05,
      "loss": 0.6779,
      "step": 1135000
    },
    {
      "epoch": 10.357507847288122,
      "grad_norm": 4.142833232879639,
      "learning_rate": 4.1368743460593234e-05,
      "loss": 0.6912,
      "step": 1135100
    },
    {
      "epoch": 10.358420322651288,
      "grad_norm": 3.8465781211853027,
      "learning_rate": 4.1367983064457264e-05,
      "loss": 0.688,
      "step": 1135200
    },
    {
      "epoch": 10.359332798014453,
      "grad_norm": 3.405673027038574,
      "learning_rate": 4.1367222668321294e-05,
      "loss": 0.6961,
      "step": 1135300
    },
    {
      "epoch": 10.360245273377618,
      "grad_norm": 3.652822971343994,
      "learning_rate": 4.136646227218532e-05,
      "loss": 0.6479,
      "step": 1135400
    },
    {
      "epoch": 10.361157748740784,
      "grad_norm": 4.153541564941406,
      "learning_rate": 4.1365701876049354e-05,
      "loss": 0.6425,
      "step": 1135500
    },
    {
      "epoch": 10.362070224103949,
      "grad_norm": 3.6019256114959717,
      "learning_rate": 4.136494147991338e-05,
      "loss": 0.6816,
      "step": 1135600
    },
    {
      "epoch": 10.362982699467114,
      "grad_norm": 4.619963645935059,
      "learning_rate": 4.136418108377741e-05,
      "loss": 0.6653,
      "step": 1135700
    },
    {
      "epoch": 10.36389517483028,
      "grad_norm": 4.049719333648682,
      "learning_rate": 4.136342068764144e-05,
      "loss": 0.7017,
      "step": 1135800
    },
    {
      "epoch": 10.364807650193445,
      "grad_norm": 4.610864639282227,
      "learning_rate": 4.136266029150547e-05,
      "loss": 0.6817,
      "step": 1135900
    },
    {
      "epoch": 10.36572012555661,
      "grad_norm": 5.2865142822265625,
      "learning_rate": 4.136189989536949e-05,
      "loss": 0.6847,
      "step": 1136000
    },
    {
      "epoch": 10.366632600919775,
      "grad_norm": 3.612414598464966,
      "learning_rate": 4.136113949923352e-05,
      "loss": 0.7146,
      "step": 1136100
    },
    {
      "epoch": 10.36754507628294,
      "grad_norm": 3.3173000812530518,
      "learning_rate": 4.136037910309755e-05,
      "loss": 0.703,
      "step": 1136200
    },
    {
      "epoch": 10.368457551646106,
      "grad_norm": 3.924058675765991,
      "learning_rate": 4.135961870696158e-05,
      "loss": 0.6678,
      "step": 1136300
    },
    {
      "epoch": 10.369370027009271,
      "grad_norm": 4.444211483001709,
      "learning_rate": 4.135885831082561e-05,
      "loss": 0.6946,
      "step": 1136400
    },
    {
      "epoch": 10.370282502372437,
      "grad_norm": 3.3061227798461914,
      "learning_rate": 4.1358097914689634e-05,
      "loss": 0.7045,
      "step": 1136500
    },
    {
      "epoch": 10.371194977735602,
      "grad_norm": 4.0420026779174805,
      "learning_rate": 4.135733751855367e-05,
      "loss": 0.7119,
      "step": 1136600
    },
    {
      "epoch": 10.372107453098767,
      "grad_norm": 3.708271026611328,
      "learning_rate": 4.1356577122417694e-05,
      "loss": 0.6766,
      "step": 1136700
    },
    {
      "epoch": 10.373019928461932,
      "grad_norm": 3.776653289794922,
      "learning_rate": 4.1355816726281724e-05,
      "loss": 0.6761,
      "step": 1136800
    },
    {
      "epoch": 10.373932403825096,
      "grad_norm": 5.249359607696533,
      "learning_rate": 4.1355056330145754e-05,
      "loss": 0.6445,
      "step": 1136900
    },
    {
      "epoch": 10.374844879188261,
      "grad_norm": 3.438518524169922,
      "learning_rate": 4.1354295934009785e-05,
      "loss": 0.7036,
      "step": 1137000
    },
    {
      "epoch": 10.375757354551427,
      "grad_norm": 4.900442600250244,
      "learning_rate": 4.135353553787381e-05,
      "loss": 0.7132,
      "step": 1137100
    },
    {
      "epoch": 10.376669829914592,
      "grad_norm": 4.1253156661987305,
      "learning_rate": 4.1352775141737845e-05,
      "loss": 0.6943,
      "step": 1137200
    },
    {
      "epoch": 10.377582305277757,
      "grad_norm": 4.18127965927124,
      "learning_rate": 4.135201474560187e-05,
      "loss": 0.7168,
      "step": 1137300
    },
    {
      "epoch": 10.378494780640922,
      "grad_norm": 3.995927333831787,
      "learning_rate": 4.13512543494659e-05,
      "loss": 0.7176,
      "step": 1137400
    },
    {
      "epoch": 10.379407256004088,
      "grad_norm": 3.81245756149292,
      "learning_rate": 4.135049395332993e-05,
      "loss": 0.6116,
      "step": 1137500
    },
    {
      "epoch": 10.380319731367253,
      "grad_norm": 4.178952217102051,
      "learning_rate": 4.134973355719396e-05,
      "loss": 0.6969,
      "step": 1137600
    },
    {
      "epoch": 10.381232206730418,
      "grad_norm": 3.484442949295044,
      "learning_rate": 4.134897316105799e-05,
      "loss": 0.6992,
      "step": 1137700
    },
    {
      "epoch": 10.382144682093584,
      "grad_norm": 3.7260820865631104,
      "learning_rate": 4.134821276492202e-05,
      "loss": 0.6891,
      "step": 1137800
    },
    {
      "epoch": 10.383057157456749,
      "grad_norm": 3.3912131786346436,
      "learning_rate": 4.134745236878604e-05,
      "loss": 0.6812,
      "step": 1137900
    },
    {
      "epoch": 10.383969632819914,
      "grad_norm": 4.099640369415283,
      "learning_rate": 4.134669197265008e-05,
      "loss": 0.6627,
      "step": 1138000
    },
    {
      "epoch": 10.38488210818308,
      "grad_norm": 3.9090497493743896,
      "learning_rate": 4.13459315765141e-05,
      "loss": 0.6715,
      "step": 1138100
    },
    {
      "epoch": 10.385794583546245,
      "grad_norm": 3.2782745361328125,
      "learning_rate": 4.134517118037813e-05,
      "loss": 0.6729,
      "step": 1138200
    },
    {
      "epoch": 10.38670705890941,
      "grad_norm": 4.034095287322998,
      "learning_rate": 4.134441078424216e-05,
      "loss": 0.6757,
      "step": 1138300
    },
    {
      "epoch": 10.387619534272575,
      "grad_norm": 3.287043809890747,
      "learning_rate": 4.134365038810619e-05,
      "loss": 0.7373,
      "step": 1138400
    },
    {
      "epoch": 10.388532009635739,
      "grad_norm": 4.385336875915527,
      "learning_rate": 4.1342889991970215e-05,
      "loss": 0.69,
      "step": 1138500
    },
    {
      "epoch": 10.389444484998904,
      "grad_norm": 4.256792068481445,
      "learning_rate": 4.134212959583425e-05,
      "loss": 0.7142,
      "step": 1138600
    },
    {
      "epoch": 10.39035696036207,
      "grad_norm": 4.845431804656982,
      "learning_rate": 4.1341369199698275e-05,
      "loss": 0.6975,
      "step": 1138700
    },
    {
      "epoch": 10.391269435725235,
      "grad_norm": 4.51713752746582,
      "learning_rate": 4.1340608803562305e-05,
      "loss": 0.6932,
      "step": 1138800
    },
    {
      "epoch": 10.3921819110884,
      "grad_norm": 3.796037435531616,
      "learning_rate": 4.1339848407426336e-05,
      "loss": 0.6655,
      "step": 1138900
    },
    {
      "epoch": 10.393094386451565,
      "grad_norm": 3.7609190940856934,
      "learning_rate": 4.133908801129036e-05,
      "loss": 0.6625,
      "step": 1139000
    },
    {
      "epoch": 10.39400686181473,
      "grad_norm": 3.0582919120788574,
      "learning_rate": 4.1338327615154396e-05,
      "loss": 0.6708,
      "step": 1139100
    },
    {
      "epoch": 10.394919337177896,
      "grad_norm": 3.5855906009674072,
      "learning_rate": 4.133756721901842e-05,
      "loss": 0.6989,
      "step": 1139200
    },
    {
      "epoch": 10.395831812541061,
      "grad_norm": 3.7920169830322266,
      "learning_rate": 4.133680682288245e-05,
      "loss": 0.6488,
      "step": 1139300
    },
    {
      "epoch": 10.396744287904227,
      "grad_norm": 3.755222797393799,
      "learning_rate": 4.133604642674648e-05,
      "loss": 0.7102,
      "step": 1139400
    },
    {
      "epoch": 10.397656763267392,
      "grad_norm": 3.612161636352539,
      "learning_rate": 4.133528603061051e-05,
      "loss": 0.6625,
      "step": 1139500
    },
    {
      "epoch": 10.398569238630557,
      "grad_norm": 3.43886137008667,
      "learning_rate": 4.133452563447453e-05,
      "loss": 0.6634,
      "step": 1139600
    },
    {
      "epoch": 10.399481713993723,
      "grad_norm": 3.406872034072876,
      "learning_rate": 4.133376523833857e-05,
      "loss": 0.6282,
      "step": 1139700
    },
    {
      "epoch": 10.400394189356888,
      "grad_norm": 4.344861030578613,
      "learning_rate": 4.133300484220259e-05,
      "loss": 0.6811,
      "step": 1139800
    },
    {
      "epoch": 10.401306664720053,
      "grad_norm": 4.839539527893066,
      "learning_rate": 4.133224444606662e-05,
      "loss": 0.7025,
      "step": 1139900
    },
    {
      "epoch": 10.402219140083218,
      "grad_norm": 4.088135719299316,
      "learning_rate": 4.133148404993065e-05,
      "loss": 0.6602,
      "step": 1140000
    },
    {
      "epoch": 10.403131615446384,
      "grad_norm": 5.909745693206787,
      "learning_rate": 4.133072365379468e-05,
      "loss": 0.676,
      "step": 1140100
    },
    {
      "epoch": 10.404044090809547,
      "grad_norm": 4.3854265213012695,
      "learning_rate": 4.132996325765871e-05,
      "loss": 0.6755,
      "step": 1140200
    },
    {
      "epoch": 10.404956566172713,
      "grad_norm": 3.915684223175049,
      "learning_rate": 4.132920286152274e-05,
      "loss": 0.6735,
      "step": 1140300
    },
    {
      "epoch": 10.405869041535878,
      "grad_norm": 2.496889114379883,
      "learning_rate": 4.1328442465386766e-05,
      "loss": 0.6958,
      "step": 1140400
    },
    {
      "epoch": 10.406781516899043,
      "grad_norm": 3.661151170730591,
      "learning_rate": 4.13276820692508e-05,
      "loss": 0.6943,
      "step": 1140500
    },
    {
      "epoch": 10.407693992262208,
      "grad_norm": 5.175317287445068,
      "learning_rate": 4.1326921673114826e-05,
      "loss": 0.7023,
      "step": 1140600
    },
    {
      "epoch": 10.408606467625374,
      "grad_norm": 4.428572654724121,
      "learning_rate": 4.1326161276978856e-05,
      "loss": 0.6623,
      "step": 1140700
    },
    {
      "epoch": 10.409518942988539,
      "grad_norm": 2.0325822830200195,
      "learning_rate": 4.1325400880842886e-05,
      "loss": 0.667,
      "step": 1140800
    },
    {
      "epoch": 10.410431418351704,
      "grad_norm": 4.26780891418457,
      "learning_rate": 4.1324640484706917e-05,
      "loss": 0.6665,
      "step": 1140900
    },
    {
      "epoch": 10.41134389371487,
      "grad_norm": 2.5767877101898193,
      "learning_rate": 4.132388008857094e-05,
      "loss": 0.6749,
      "step": 1141000
    },
    {
      "epoch": 10.412256369078035,
      "grad_norm": 3.430142402648926,
      "learning_rate": 4.132311969243498e-05,
      "loss": 0.7249,
      "step": 1141100
    },
    {
      "epoch": 10.4131688444412,
      "grad_norm": 3.1956586837768555,
      "learning_rate": 4.1322359296299e-05,
      "loss": 0.722,
      "step": 1141200
    },
    {
      "epoch": 10.414081319804366,
      "grad_norm": 4.123310565948486,
      "learning_rate": 4.132159890016303e-05,
      "loss": 0.7047,
      "step": 1141300
    },
    {
      "epoch": 10.41499379516753,
      "grad_norm": 4.441043853759766,
      "learning_rate": 4.132083850402706e-05,
      "loss": 0.6541,
      "step": 1141400
    },
    {
      "epoch": 10.415906270530696,
      "grad_norm": 3.636070728302002,
      "learning_rate": 4.132007810789109e-05,
      "loss": 0.6702,
      "step": 1141500
    },
    {
      "epoch": 10.416818745893861,
      "grad_norm": 4.411285400390625,
      "learning_rate": 4.131931771175512e-05,
      "loss": 0.7271,
      "step": 1141600
    },
    {
      "epoch": 10.417731221257027,
      "grad_norm": 4.287536144256592,
      "learning_rate": 4.131855731561915e-05,
      "loss": 0.6939,
      "step": 1141700
    },
    {
      "epoch": 10.418643696620192,
      "grad_norm": 4.033844947814941,
      "learning_rate": 4.1317796919483174e-05,
      "loss": 0.6571,
      "step": 1141800
    },
    {
      "epoch": 10.419556171983356,
      "grad_norm": 4.2807936668396,
      "learning_rate": 4.1317036523347204e-05,
      "loss": 0.6804,
      "step": 1141900
    },
    {
      "epoch": 10.420468647346521,
      "grad_norm": 4.522731781005859,
      "learning_rate": 4.1316276127211234e-05,
      "loss": 0.681,
      "step": 1142000
    },
    {
      "epoch": 10.421381122709686,
      "grad_norm": 3.0611064434051514,
      "learning_rate": 4.1315515731075264e-05,
      "loss": 0.6897,
      "step": 1142100
    },
    {
      "epoch": 10.422293598072851,
      "grad_norm": 3.367555856704712,
      "learning_rate": 4.1314755334939294e-05,
      "loss": 0.7025,
      "step": 1142200
    },
    {
      "epoch": 10.423206073436017,
      "grad_norm": 3.198064088821411,
      "learning_rate": 4.131399493880332e-05,
      "loss": 0.6764,
      "step": 1142300
    },
    {
      "epoch": 10.424118548799182,
      "grad_norm": 3.9652981758117676,
      "learning_rate": 4.1313234542667354e-05,
      "loss": 0.6964,
      "step": 1142400
    },
    {
      "epoch": 10.425031024162347,
      "grad_norm": 3.081190347671509,
      "learning_rate": 4.131247414653138e-05,
      "loss": 0.6855,
      "step": 1142500
    },
    {
      "epoch": 10.425943499525513,
      "grad_norm": 5.616499900817871,
      "learning_rate": 4.131171375039541e-05,
      "loss": 0.7026,
      "step": 1142600
    },
    {
      "epoch": 10.426855974888678,
      "grad_norm": 3.622236967086792,
      "learning_rate": 4.131095335425944e-05,
      "loss": 0.6366,
      "step": 1142700
    },
    {
      "epoch": 10.427768450251843,
      "grad_norm": 4.444293975830078,
      "learning_rate": 4.131019295812347e-05,
      "loss": 0.6976,
      "step": 1142800
    },
    {
      "epoch": 10.428680925615009,
      "grad_norm": 4.398223400115967,
      "learning_rate": 4.130943256198749e-05,
      "loss": 0.7065,
      "step": 1142900
    },
    {
      "epoch": 10.429593400978174,
      "grad_norm": 5.440814971923828,
      "learning_rate": 4.130867216585153e-05,
      "loss": 0.7052,
      "step": 1143000
    },
    {
      "epoch": 10.43050587634134,
      "grad_norm": 3.6413917541503906,
      "learning_rate": 4.130791176971555e-05,
      "loss": 0.6641,
      "step": 1143100
    },
    {
      "epoch": 10.431418351704504,
      "grad_norm": 4.137053966522217,
      "learning_rate": 4.130715137357958e-05,
      "loss": 0.6857,
      "step": 1143200
    },
    {
      "epoch": 10.43233082706767,
      "grad_norm": 3.815110921859741,
      "learning_rate": 4.130639097744361e-05,
      "loss": 0.676,
      "step": 1143300
    },
    {
      "epoch": 10.433243302430835,
      "grad_norm": 3.529961347579956,
      "learning_rate": 4.130563058130764e-05,
      "loss": 0.7194,
      "step": 1143400
    },
    {
      "epoch": 10.434155777794,
      "grad_norm": 3.460371971130371,
      "learning_rate": 4.130487018517167e-05,
      "loss": 0.6688,
      "step": 1143500
    },
    {
      "epoch": 10.435068253157164,
      "grad_norm": 3.1473755836486816,
      "learning_rate": 4.13041097890357e-05,
      "loss": 0.6781,
      "step": 1143600
    },
    {
      "epoch": 10.43598072852033,
      "grad_norm": 3.6042497158050537,
      "learning_rate": 4.1303349392899725e-05,
      "loss": 0.6678,
      "step": 1143700
    },
    {
      "epoch": 10.436893203883495,
      "grad_norm": 3.8829855918884277,
      "learning_rate": 4.130258899676376e-05,
      "loss": 0.6805,
      "step": 1143800
    },
    {
      "epoch": 10.43780567924666,
      "grad_norm": 4.181209564208984,
      "learning_rate": 4.1301828600627785e-05,
      "loss": 0.6422,
      "step": 1143900
    },
    {
      "epoch": 10.438718154609825,
      "grad_norm": 3.975099563598633,
      "learning_rate": 4.1301068204491815e-05,
      "loss": 0.7157,
      "step": 1144000
    },
    {
      "epoch": 10.43963062997299,
      "grad_norm": 4.13279390335083,
      "learning_rate": 4.1300307808355845e-05,
      "loss": 0.6739,
      "step": 1144100
    },
    {
      "epoch": 10.440543105336156,
      "grad_norm": 3.076829433441162,
      "learning_rate": 4.1299547412219875e-05,
      "loss": 0.6961,
      "step": 1144200
    },
    {
      "epoch": 10.441455580699321,
      "grad_norm": 4.594577312469482,
      "learning_rate": 4.12987870160839e-05,
      "loss": 0.6704,
      "step": 1144300
    },
    {
      "epoch": 10.442368056062486,
      "grad_norm": 3.42207932472229,
      "learning_rate": 4.1298026619947935e-05,
      "loss": 0.6724,
      "step": 1144400
    },
    {
      "epoch": 10.443280531425652,
      "grad_norm": 4.121969699859619,
      "learning_rate": 4.129726622381196e-05,
      "loss": 0.6971,
      "step": 1144500
    },
    {
      "epoch": 10.444193006788817,
      "grad_norm": 4.183529853820801,
      "learning_rate": 4.129650582767599e-05,
      "loss": 0.6694,
      "step": 1144600
    },
    {
      "epoch": 10.445105482151982,
      "grad_norm": 4.252328872680664,
      "learning_rate": 4.129574543154002e-05,
      "loss": 0.6939,
      "step": 1144700
    },
    {
      "epoch": 10.446017957515148,
      "grad_norm": 3.9500091075897217,
      "learning_rate": 4.129498503540404e-05,
      "loss": 0.7026,
      "step": 1144800
    },
    {
      "epoch": 10.446930432878313,
      "grad_norm": 3.040083169937134,
      "learning_rate": 4.129422463926808e-05,
      "loss": 0.6955,
      "step": 1144900
    },
    {
      "epoch": 10.447842908241478,
      "grad_norm": 3.6609373092651367,
      "learning_rate": 4.12934642431321e-05,
      "loss": 0.6664,
      "step": 1145000
    },
    {
      "epoch": 10.448755383604643,
      "grad_norm": 4.535134792327881,
      "learning_rate": 4.129270384699613e-05,
      "loss": 0.6927,
      "step": 1145100
    },
    {
      "epoch": 10.449667858967809,
      "grad_norm": 3.0927937030792236,
      "learning_rate": 4.129194345086016e-05,
      "loss": 0.7143,
      "step": 1145200
    },
    {
      "epoch": 10.450580334330972,
      "grad_norm": 4.209709167480469,
      "learning_rate": 4.129118305472419e-05,
      "loss": 0.669,
      "step": 1145300
    },
    {
      "epoch": 10.451492809694138,
      "grad_norm": 4.52752685546875,
      "learning_rate": 4.1290422658588215e-05,
      "loss": 0.6925,
      "step": 1145400
    },
    {
      "epoch": 10.452405285057303,
      "grad_norm": 3.50319242477417,
      "learning_rate": 4.128966226245225e-05,
      "loss": 0.6295,
      "step": 1145500
    },
    {
      "epoch": 10.453317760420468,
      "grad_norm": 2.925102949142456,
      "learning_rate": 4.1288901866316275e-05,
      "loss": 0.6505,
      "step": 1145600
    },
    {
      "epoch": 10.454230235783633,
      "grad_norm": 4.46778678894043,
      "learning_rate": 4.1288141470180306e-05,
      "loss": 0.6853,
      "step": 1145700
    },
    {
      "epoch": 10.455142711146799,
      "grad_norm": 4.444465637207031,
      "learning_rate": 4.1287381074044336e-05,
      "loss": 0.6849,
      "step": 1145800
    },
    {
      "epoch": 10.456055186509964,
      "grad_norm": 3.774199962615967,
      "learning_rate": 4.1286620677908366e-05,
      "loss": 0.6905,
      "step": 1145900
    },
    {
      "epoch": 10.45696766187313,
      "grad_norm": 5.253992557525635,
      "learning_rate": 4.1285860281772396e-05,
      "loss": 0.6887,
      "step": 1146000
    },
    {
      "epoch": 10.457880137236295,
      "grad_norm": 4.982445240020752,
      "learning_rate": 4.1285099885636426e-05,
      "loss": 0.6549,
      "step": 1146100
    },
    {
      "epoch": 10.45879261259946,
      "grad_norm": 3.277463436126709,
      "learning_rate": 4.128433948950045e-05,
      "loss": 0.6465,
      "step": 1146200
    },
    {
      "epoch": 10.459705087962625,
      "grad_norm": 2.965344190597534,
      "learning_rate": 4.1283579093364486e-05,
      "loss": 0.6782,
      "step": 1146300
    },
    {
      "epoch": 10.46061756332579,
      "grad_norm": 4.919253826141357,
      "learning_rate": 4.128281869722851e-05,
      "loss": 0.6853,
      "step": 1146400
    },
    {
      "epoch": 10.461530038688956,
      "grad_norm": 3.7632131576538086,
      "learning_rate": 4.128205830109254e-05,
      "loss": 0.6653,
      "step": 1146500
    },
    {
      "epoch": 10.462442514052121,
      "grad_norm": 3.3696587085723877,
      "learning_rate": 4.128129790495657e-05,
      "loss": 0.7246,
      "step": 1146600
    },
    {
      "epoch": 10.463354989415286,
      "grad_norm": 3.7683072090148926,
      "learning_rate": 4.12805375088206e-05,
      "loss": 0.6661,
      "step": 1146700
    },
    {
      "epoch": 10.464267464778452,
      "grad_norm": 2.492964267730713,
      "learning_rate": 4.127977711268462e-05,
      "loss": 0.6936,
      "step": 1146800
    },
    {
      "epoch": 10.465179940141617,
      "grad_norm": 4.459484577178955,
      "learning_rate": 4.127901671654866e-05,
      "loss": 0.6508,
      "step": 1146900
    },
    {
      "epoch": 10.46609241550478,
      "grad_norm": 4.109147071838379,
      "learning_rate": 4.127825632041268e-05,
      "loss": 0.6995,
      "step": 1147000
    },
    {
      "epoch": 10.467004890867946,
      "grad_norm": 4.23118782043457,
      "learning_rate": 4.127749592427671e-05,
      "loss": 0.6621,
      "step": 1147100
    },
    {
      "epoch": 10.467917366231111,
      "grad_norm": 4.283363342285156,
      "learning_rate": 4.127673552814074e-05,
      "loss": 0.7142,
      "step": 1147200
    },
    {
      "epoch": 10.468829841594276,
      "grad_norm": 3.7029061317443848,
      "learning_rate": 4.127597513200477e-05,
      "loss": 0.6344,
      "step": 1147300
    },
    {
      "epoch": 10.469742316957442,
      "grad_norm": 2.9915246963500977,
      "learning_rate": 4.12752147358688e-05,
      "loss": 0.7017,
      "step": 1147400
    },
    {
      "epoch": 10.470654792320607,
      "grad_norm": 4.507800579071045,
      "learning_rate": 4.1274454339732826e-05,
      "loss": 0.6646,
      "step": 1147500
    },
    {
      "epoch": 10.471567267683772,
      "grad_norm": 3.6882288455963135,
      "learning_rate": 4.1273693943596856e-05,
      "loss": 0.6539,
      "step": 1147600
    },
    {
      "epoch": 10.472479743046938,
      "grad_norm": 4.48362398147583,
      "learning_rate": 4.1272933547460887e-05,
      "loss": 0.6892,
      "step": 1147700
    },
    {
      "epoch": 10.473392218410103,
      "grad_norm": 3.344693660736084,
      "learning_rate": 4.127217315132492e-05,
      "loss": 0.6727,
      "step": 1147800
    },
    {
      "epoch": 10.474304693773268,
      "grad_norm": 4.032058238983154,
      "learning_rate": 4.127141275518894e-05,
      "loss": 0.6654,
      "step": 1147900
    },
    {
      "epoch": 10.475217169136434,
      "grad_norm": 4.497089385986328,
      "learning_rate": 4.127065235905298e-05,
      "loss": 0.6724,
      "step": 1148000
    },
    {
      "epoch": 10.476129644499599,
      "grad_norm": 3.369415521621704,
      "learning_rate": 4.1269891962917e-05,
      "loss": 0.7158,
      "step": 1148100
    },
    {
      "epoch": 10.477042119862764,
      "grad_norm": 2.820770025253296,
      "learning_rate": 4.126913156678103e-05,
      "loss": 0.6634,
      "step": 1148200
    },
    {
      "epoch": 10.47795459522593,
      "grad_norm": 4.144734859466553,
      "learning_rate": 4.126837117064506e-05,
      "loss": 0.6787,
      "step": 1148300
    },
    {
      "epoch": 10.478867070589095,
      "grad_norm": 3.037391424179077,
      "learning_rate": 4.126761077450909e-05,
      "loss": 0.6303,
      "step": 1148400
    },
    {
      "epoch": 10.47977954595226,
      "grad_norm": 4.5391926765441895,
      "learning_rate": 4.126685037837312e-05,
      "loss": 0.7182,
      "step": 1148500
    },
    {
      "epoch": 10.480692021315425,
      "grad_norm": 2.8926584720611572,
      "learning_rate": 4.126608998223715e-05,
      "loss": 0.6952,
      "step": 1148600
    },
    {
      "epoch": 10.481604496678589,
      "grad_norm": 3.795322895050049,
      "learning_rate": 4.1265329586101174e-05,
      "loss": 0.7061,
      "step": 1148700
    },
    {
      "epoch": 10.482516972041754,
      "grad_norm": 3.402419090270996,
      "learning_rate": 4.126456918996521e-05,
      "loss": 0.6314,
      "step": 1148800
    },
    {
      "epoch": 10.48342944740492,
      "grad_norm": 5.269103527069092,
      "learning_rate": 4.1263808793829234e-05,
      "loss": 0.6655,
      "step": 1148900
    },
    {
      "epoch": 10.484341922768085,
      "grad_norm": 3.896224021911621,
      "learning_rate": 4.1263048397693264e-05,
      "loss": 0.7096,
      "step": 1149000
    },
    {
      "epoch": 10.48525439813125,
      "grad_norm": 4.627255916595459,
      "learning_rate": 4.1262288001557294e-05,
      "loss": 0.6717,
      "step": 1149100
    },
    {
      "epoch": 10.486166873494415,
      "grad_norm": 3.9480185508728027,
      "learning_rate": 4.1261527605421324e-05,
      "loss": 0.6755,
      "step": 1149200
    },
    {
      "epoch": 10.48707934885758,
      "grad_norm": 4.094229221343994,
      "learning_rate": 4.126076720928535e-05,
      "loss": 0.7048,
      "step": 1149300
    },
    {
      "epoch": 10.487991824220746,
      "grad_norm": 3.3226637840270996,
      "learning_rate": 4.1260006813149384e-05,
      "loss": 0.6955,
      "step": 1149400
    },
    {
      "epoch": 10.488904299583911,
      "grad_norm": 4.0361762046813965,
      "learning_rate": 4.125924641701341e-05,
      "loss": 0.6509,
      "step": 1149500
    },
    {
      "epoch": 10.489816774947077,
      "grad_norm": 4.5727667808532715,
      "learning_rate": 4.125848602087744e-05,
      "loss": 0.6812,
      "step": 1149600
    },
    {
      "epoch": 10.490729250310242,
      "grad_norm": 4.150864601135254,
      "learning_rate": 4.125772562474147e-05,
      "loss": 0.6793,
      "step": 1149700
    },
    {
      "epoch": 10.491641725673407,
      "grad_norm": 3.6002209186553955,
      "learning_rate": 4.12569652286055e-05,
      "loss": 0.703,
      "step": 1149800
    },
    {
      "epoch": 10.492554201036572,
      "grad_norm": 4.1994123458862305,
      "learning_rate": 4.125620483246953e-05,
      "loss": 0.6921,
      "step": 1149900
    },
    {
      "epoch": 10.493466676399738,
      "grad_norm": 4.274290084838867,
      "learning_rate": 4.125544443633356e-05,
      "loss": 0.6866,
      "step": 1150000
    },
    {
      "epoch": 10.494379151762903,
      "grad_norm": 4.124440670013428,
      "learning_rate": 4.125468404019758e-05,
      "loss": 0.7318,
      "step": 1150100
    },
    {
      "epoch": 10.495291627126068,
      "grad_norm": 3.347363233566284,
      "learning_rate": 4.125392364406162e-05,
      "loss": 0.6547,
      "step": 1150200
    },
    {
      "epoch": 10.496204102489234,
      "grad_norm": 3.679684638977051,
      "learning_rate": 4.125316324792564e-05,
      "loss": 0.6988,
      "step": 1150300
    },
    {
      "epoch": 10.497116577852397,
      "grad_norm": 4.335762977600098,
      "learning_rate": 4.1252402851789664e-05,
      "loss": 0.6852,
      "step": 1150400
    },
    {
      "epoch": 10.498029053215562,
      "grad_norm": 3.8363983631134033,
      "learning_rate": 4.12516424556537e-05,
      "loss": 0.7121,
      "step": 1150500
    },
    {
      "epoch": 10.498941528578728,
      "grad_norm": 3.526716947555542,
      "learning_rate": 4.1250882059517725e-05,
      "loss": 0.6685,
      "step": 1150600
    },
    {
      "epoch": 10.499854003941893,
      "grad_norm": 4.447433948516846,
      "learning_rate": 4.1250121663381755e-05,
      "loss": 0.6923,
      "step": 1150700
    },
    {
      "epoch": 10.500766479305058,
      "grad_norm": 3.854414939880371,
      "learning_rate": 4.1249361267245785e-05,
      "loss": 0.71,
      "step": 1150800
    },
    {
      "epoch": 10.501678954668224,
      "grad_norm": 3.6847193241119385,
      "learning_rate": 4.1248600871109815e-05,
      "loss": 0.6619,
      "step": 1150900
    },
    {
      "epoch": 10.502591430031389,
      "grad_norm": 4.2116217613220215,
      "learning_rate": 4.1247840474973845e-05,
      "loss": 0.6925,
      "step": 1151000
    },
    {
      "epoch": 10.503503905394554,
      "grad_norm": 3.972522020339966,
      "learning_rate": 4.1247080078837875e-05,
      "loss": 0.686,
      "step": 1151100
    },
    {
      "epoch": 10.50441638075772,
      "grad_norm": 4.067233085632324,
      "learning_rate": 4.12463196827019e-05,
      "loss": 0.6801,
      "step": 1151200
    },
    {
      "epoch": 10.505328856120885,
      "grad_norm": 4.248232364654541,
      "learning_rate": 4.1245559286565935e-05,
      "loss": 0.6706,
      "step": 1151300
    },
    {
      "epoch": 10.50624133148405,
      "grad_norm": 3.5066800117492676,
      "learning_rate": 4.124479889042996e-05,
      "loss": 0.7001,
      "step": 1151400
    },
    {
      "epoch": 10.507153806847215,
      "grad_norm": 3.8921566009521484,
      "learning_rate": 4.124403849429399e-05,
      "loss": 0.6575,
      "step": 1151500
    },
    {
      "epoch": 10.50806628221038,
      "grad_norm": 4.392354488372803,
      "learning_rate": 4.124327809815802e-05,
      "loss": 0.6804,
      "step": 1151600
    },
    {
      "epoch": 10.508978757573546,
      "grad_norm": 3.19856595993042,
      "learning_rate": 4.124251770202205e-05,
      "loss": 0.68,
      "step": 1151700
    },
    {
      "epoch": 10.509891232936711,
      "grad_norm": 3.2705140113830566,
      "learning_rate": 4.124175730588607e-05,
      "loss": 0.6443,
      "step": 1151800
    },
    {
      "epoch": 10.510803708299877,
      "grad_norm": 3.0605404376983643,
      "learning_rate": 4.124099690975011e-05,
      "loss": 0.667,
      "step": 1151900
    },
    {
      "epoch": 10.51171618366304,
      "grad_norm": 3.919301748275757,
      "learning_rate": 4.124023651361413e-05,
      "loss": 0.658,
      "step": 1152000
    },
    {
      "epoch": 10.512628659026205,
      "grad_norm": 4.4985175132751465,
      "learning_rate": 4.123947611747816e-05,
      "loss": 0.6994,
      "step": 1152100
    },
    {
      "epoch": 10.51354113438937,
      "grad_norm": 3.942169189453125,
      "learning_rate": 4.123871572134219e-05,
      "loss": 0.6986,
      "step": 1152200
    },
    {
      "epoch": 10.514453609752536,
      "grad_norm": 3.5352745056152344,
      "learning_rate": 4.123795532520622e-05,
      "loss": 0.6974,
      "step": 1152300
    },
    {
      "epoch": 10.515366085115701,
      "grad_norm": 4.565759658813477,
      "learning_rate": 4.123719492907025e-05,
      "loss": 0.697,
      "step": 1152400
    },
    {
      "epoch": 10.516278560478867,
      "grad_norm": 3.3862228393554688,
      "learning_rate": 4.123643453293428e-05,
      "loss": 0.6655,
      "step": 1152500
    },
    {
      "epoch": 10.517191035842032,
      "grad_norm": 3.6444127559661865,
      "learning_rate": 4.1235674136798306e-05,
      "loss": 0.7286,
      "step": 1152600
    },
    {
      "epoch": 10.518103511205197,
      "grad_norm": 4.7705254554748535,
      "learning_rate": 4.123491374066234e-05,
      "loss": 0.6823,
      "step": 1152700
    },
    {
      "epoch": 10.519015986568363,
      "grad_norm": 4.429014682769775,
      "learning_rate": 4.1234153344526366e-05,
      "loss": 0.6695,
      "step": 1152800
    },
    {
      "epoch": 10.519928461931528,
      "grad_norm": 3.729670524597168,
      "learning_rate": 4.1233392948390396e-05,
      "loss": 0.7152,
      "step": 1152900
    },
    {
      "epoch": 10.520840937294693,
      "grad_norm": 4.08680534362793,
      "learning_rate": 4.1232632552254426e-05,
      "loss": 0.6322,
      "step": 1153000
    },
    {
      "epoch": 10.521753412657858,
      "grad_norm": 3.516972303390503,
      "learning_rate": 4.1231872156118456e-05,
      "loss": 0.7282,
      "step": 1153100
    },
    {
      "epoch": 10.522665888021024,
      "grad_norm": 4.553032875061035,
      "learning_rate": 4.123111175998248e-05,
      "loss": 0.6909,
      "step": 1153200
    },
    {
      "epoch": 10.523578363384189,
      "grad_norm": 4.472389221191406,
      "learning_rate": 4.123035136384651e-05,
      "loss": 0.6789,
      "step": 1153300
    },
    {
      "epoch": 10.524490838747354,
      "grad_norm": 4.434310436248779,
      "learning_rate": 4.122959096771054e-05,
      "loss": 0.6932,
      "step": 1153400
    },
    {
      "epoch": 10.52540331411052,
      "grad_norm": 4.206354141235352,
      "learning_rate": 4.122883057157457e-05,
      "loss": 0.6848,
      "step": 1153500
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 3.294076681137085,
      "learning_rate": 4.12280701754386e-05,
      "loss": 0.6818,
      "step": 1153600
    },
    {
      "epoch": 10.52722826483685,
      "grad_norm": 4.3101959228515625,
      "learning_rate": 4.122730977930262e-05,
      "loss": 0.6813,
      "step": 1153700
    },
    {
      "epoch": 10.528140740200014,
      "grad_norm": 2.974984884262085,
      "learning_rate": 4.122654938316666e-05,
      "loss": 0.6934,
      "step": 1153800
    },
    {
      "epoch": 10.529053215563179,
      "grad_norm": 3.1735033988952637,
      "learning_rate": 4.122578898703068e-05,
      "loss": 0.6632,
      "step": 1153900
    },
    {
      "epoch": 10.529965690926344,
      "grad_norm": 4.6392412185668945,
      "learning_rate": 4.122502859089471e-05,
      "loss": 0.7094,
      "step": 1154000
    },
    {
      "epoch": 10.53087816628951,
      "grad_norm": 3.3865630626678467,
      "learning_rate": 4.122426819475874e-05,
      "loss": 0.6926,
      "step": 1154100
    },
    {
      "epoch": 10.531790641652675,
      "grad_norm": 4.121804237365723,
      "learning_rate": 4.122350779862277e-05,
      "loss": 0.6995,
      "step": 1154200
    },
    {
      "epoch": 10.53270311701584,
      "grad_norm": 3.4007108211517334,
      "learning_rate": 4.12227474024868e-05,
      "loss": 0.6473,
      "step": 1154300
    },
    {
      "epoch": 10.533615592379006,
      "grad_norm": 5.500599384307861,
      "learning_rate": 4.122198700635083e-05,
      "loss": 0.7135,
      "step": 1154400
    },
    {
      "epoch": 10.53452806774217,
      "grad_norm": 3.485058307647705,
      "learning_rate": 4.1221226610214857e-05,
      "loss": 0.6578,
      "step": 1154500
    },
    {
      "epoch": 10.535440543105336,
      "grad_norm": 4.907289505004883,
      "learning_rate": 4.122046621407889e-05,
      "loss": 0.6801,
      "step": 1154600
    },
    {
      "epoch": 10.536353018468501,
      "grad_norm": 3.8569161891937256,
      "learning_rate": 4.121970581794292e-05,
      "loss": 0.6768,
      "step": 1154700
    },
    {
      "epoch": 10.537265493831667,
      "grad_norm": 3.0802605152130127,
      "learning_rate": 4.121894542180695e-05,
      "loss": 0.721,
      "step": 1154800
    },
    {
      "epoch": 10.538177969194832,
      "grad_norm": 4.278191089630127,
      "learning_rate": 4.121818502567098e-05,
      "loss": 0.6634,
      "step": 1154900
    },
    {
      "epoch": 10.539090444557997,
      "grad_norm": 4.732533931732178,
      "learning_rate": 4.121742462953501e-05,
      "loss": 0.6782,
      "step": 1155000
    },
    {
      "epoch": 10.540002919921163,
      "grad_norm": 4.031412601470947,
      "learning_rate": 4.121666423339903e-05,
      "loss": 0.7154,
      "step": 1155100
    },
    {
      "epoch": 10.540915395284328,
      "grad_norm": 3.672551155090332,
      "learning_rate": 4.121590383726307e-05,
      "loss": 0.6739,
      "step": 1155200
    },
    {
      "epoch": 10.541827870647493,
      "grad_norm": 4.059927940368652,
      "learning_rate": 4.121514344112709e-05,
      "loss": 0.6809,
      "step": 1155300
    },
    {
      "epoch": 10.542740346010657,
      "grad_norm": 3.450221538543701,
      "learning_rate": 4.121438304499112e-05,
      "loss": 0.7103,
      "step": 1155400
    },
    {
      "epoch": 10.543652821373822,
      "grad_norm": 4.871428489685059,
      "learning_rate": 4.121362264885515e-05,
      "loss": 0.6295,
      "step": 1155500
    },
    {
      "epoch": 10.544565296736987,
      "grad_norm": 4.045431137084961,
      "learning_rate": 4.121286225271918e-05,
      "loss": 0.6314,
      "step": 1155600
    },
    {
      "epoch": 10.545477772100153,
      "grad_norm": 3.6373207569122314,
      "learning_rate": 4.121210185658321e-05,
      "loss": 0.6911,
      "step": 1155700
    },
    {
      "epoch": 10.546390247463318,
      "grad_norm": 3.5392255783081055,
      "learning_rate": 4.121134146044724e-05,
      "loss": 0.6577,
      "step": 1155800
    },
    {
      "epoch": 10.547302722826483,
      "grad_norm": 3.198810577392578,
      "learning_rate": 4.1210581064311264e-05,
      "loss": 0.7052,
      "step": 1155900
    },
    {
      "epoch": 10.548215198189649,
      "grad_norm": 3.085756778717041,
      "learning_rate": 4.1209820668175294e-05,
      "loss": 0.6346,
      "step": 1156000
    },
    {
      "epoch": 10.549127673552814,
      "grad_norm": 3.7429351806640625,
      "learning_rate": 4.1209060272039324e-05,
      "loss": 0.7008,
      "step": 1156100
    },
    {
      "epoch": 10.55004014891598,
      "grad_norm": 4.857278823852539,
      "learning_rate": 4.120829987590335e-05,
      "loss": 0.6967,
      "step": 1156200
    },
    {
      "epoch": 10.550952624279144,
      "grad_norm": 3.810079336166382,
      "learning_rate": 4.1207539479767384e-05,
      "loss": 0.6616,
      "step": 1156300
    },
    {
      "epoch": 10.55186509964231,
      "grad_norm": 1.581868290901184,
      "learning_rate": 4.120677908363141e-05,
      "loss": 0.6818,
      "step": 1156400
    },
    {
      "epoch": 10.552777575005475,
      "grad_norm": 4.8937907218933105,
      "learning_rate": 4.120601868749544e-05,
      "loss": 0.6774,
      "step": 1156500
    },
    {
      "epoch": 10.55369005036864,
      "grad_norm": 3.683910608291626,
      "learning_rate": 4.120525829135947e-05,
      "loss": 0.6378,
      "step": 1156600
    },
    {
      "epoch": 10.554602525731806,
      "grad_norm": 4.542978763580322,
      "learning_rate": 4.12044978952235e-05,
      "loss": 0.6395,
      "step": 1156700
    },
    {
      "epoch": 10.555515001094971,
      "grad_norm": 4.470880508422852,
      "learning_rate": 4.120373749908753e-05,
      "loss": 0.6969,
      "step": 1156800
    },
    {
      "epoch": 10.556427476458136,
      "grad_norm": 3.494943618774414,
      "learning_rate": 4.120297710295156e-05,
      "loss": 0.6764,
      "step": 1156900
    },
    {
      "epoch": 10.557339951821302,
      "grad_norm": 3.75293231010437,
      "learning_rate": 4.120221670681558e-05,
      "loss": 0.7087,
      "step": 1157000
    },
    {
      "epoch": 10.558252427184467,
      "grad_norm": 3.1129648685455322,
      "learning_rate": 4.120145631067962e-05,
      "loss": 0.7072,
      "step": 1157100
    },
    {
      "epoch": 10.55916490254763,
      "grad_norm": 4.010822296142578,
      "learning_rate": 4.120069591454364e-05,
      "loss": 0.6922,
      "step": 1157200
    },
    {
      "epoch": 10.560077377910796,
      "grad_norm": 3.929701566696167,
      "learning_rate": 4.119993551840767e-05,
      "loss": 0.6937,
      "step": 1157300
    },
    {
      "epoch": 10.560989853273961,
      "grad_norm": 3.7845265865325928,
      "learning_rate": 4.11991751222717e-05,
      "loss": 0.6705,
      "step": 1157400
    },
    {
      "epoch": 10.561902328637126,
      "grad_norm": 4.19117546081543,
      "learning_rate": 4.119841472613573e-05,
      "loss": 0.6868,
      "step": 1157500
    },
    {
      "epoch": 10.562814804000292,
      "grad_norm": 3.056232452392578,
      "learning_rate": 4.1197654329999755e-05,
      "loss": 0.6758,
      "step": 1157600
    },
    {
      "epoch": 10.563727279363457,
      "grad_norm": 3.7689216136932373,
      "learning_rate": 4.119689393386379e-05,
      "loss": 0.6785,
      "step": 1157700
    },
    {
      "epoch": 10.564639754726622,
      "grad_norm": 3.6883623600006104,
      "learning_rate": 4.1196133537727815e-05,
      "loss": 0.7091,
      "step": 1157800
    },
    {
      "epoch": 10.565552230089787,
      "grad_norm": 3.645307779312134,
      "learning_rate": 4.1195373141591845e-05,
      "loss": 0.6564,
      "step": 1157900
    },
    {
      "epoch": 10.566464705452953,
      "grad_norm": 4.4166579246521,
      "learning_rate": 4.1194612745455875e-05,
      "loss": 0.6687,
      "step": 1158000
    },
    {
      "epoch": 10.567377180816118,
      "grad_norm": 2.487509250640869,
      "learning_rate": 4.1193852349319905e-05,
      "loss": 0.652,
      "step": 1158100
    },
    {
      "epoch": 10.568289656179283,
      "grad_norm": 2.8040072917938232,
      "learning_rate": 4.1193091953183935e-05,
      "loss": 0.6899,
      "step": 1158200
    },
    {
      "epoch": 10.569202131542449,
      "grad_norm": 3.8336708545684814,
      "learning_rate": 4.1192331557047965e-05,
      "loss": 0.689,
      "step": 1158300
    },
    {
      "epoch": 10.570114606905614,
      "grad_norm": 4.014564037322998,
      "learning_rate": 4.119157116091199e-05,
      "loss": 0.6606,
      "step": 1158400
    },
    {
      "epoch": 10.57102708226878,
      "grad_norm": 2.8937041759490967,
      "learning_rate": 4.1190810764776025e-05,
      "loss": 0.6833,
      "step": 1158500
    },
    {
      "epoch": 10.571939557631945,
      "grad_norm": 4.415126323699951,
      "learning_rate": 4.119005036864005e-05,
      "loss": 0.667,
      "step": 1158600
    },
    {
      "epoch": 10.57285203299511,
      "grad_norm": 3.751927614212036,
      "learning_rate": 4.118928997250408e-05,
      "loss": 0.7069,
      "step": 1158700
    },
    {
      "epoch": 10.573764508358273,
      "grad_norm": 3.5782036781311035,
      "learning_rate": 4.118852957636811e-05,
      "loss": 0.6575,
      "step": 1158800
    },
    {
      "epoch": 10.574676983721439,
      "grad_norm": 3.6496083736419678,
      "learning_rate": 4.118776918023213e-05,
      "loss": 0.6719,
      "step": 1158900
    },
    {
      "epoch": 10.575589459084604,
      "grad_norm": 4.1788835525512695,
      "learning_rate": 4.118700878409616e-05,
      "loss": 0.7103,
      "step": 1159000
    },
    {
      "epoch": 10.57650193444777,
      "grad_norm": 4.0024495124816895,
      "learning_rate": 4.118624838796019e-05,
      "loss": 0.705,
      "step": 1159100
    },
    {
      "epoch": 10.577414409810935,
      "grad_norm": 4.121754169464111,
      "learning_rate": 4.118548799182422e-05,
      "loss": 0.6948,
      "step": 1159200
    },
    {
      "epoch": 10.5783268851741,
      "grad_norm": 3.4186112880706787,
      "learning_rate": 4.118472759568825e-05,
      "loss": 0.6724,
      "step": 1159300
    },
    {
      "epoch": 10.579239360537265,
      "grad_norm": 4.556149005889893,
      "learning_rate": 4.118396719955228e-05,
      "loss": 0.7134,
      "step": 1159400
    },
    {
      "epoch": 10.58015183590043,
      "grad_norm": 3.789146900177002,
      "learning_rate": 4.1183206803416306e-05,
      "loss": 0.6952,
      "step": 1159500
    },
    {
      "epoch": 10.581064311263596,
      "grad_norm": 3.6148526668548584,
      "learning_rate": 4.118244640728034e-05,
      "loss": 0.6741,
      "step": 1159600
    },
    {
      "epoch": 10.581976786626761,
      "grad_norm": 3.8006505966186523,
      "learning_rate": 4.1181686011144366e-05,
      "loss": 0.6815,
      "step": 1159700
    },
    {
      "epoch": 10.582889261989926,
      "grad_norm": 3.603712320327759,
      "learning_rate": 4.1180925615008396e-05,
      "loss": 0.6607,
      "step": 1159800
    },
    {
      "epoch": 10.583801737353092,
      "grad_norm": 2.7905375957489014,
      "learning_rate": 4.1180165218872426e-05,
      "loss": 0.6715,
      "step": 1159900
    },
    {
      "epoch": 10.584714212716257,
      "grad_norm": 3.5673422813415527,
      "learning_rate": 4.1179404822736456e-05,
      "loss": 0.6867,
      "step": 1160000
    },
    {
      "epoch": 10.585626688079422,
      "grad_norm": 2.563302755355835,
      "learning_rate": 4.117864442660048e-05,
      "loss": 0.6673,
      "step": 1160100
    },
    {
      "epoch": 10.586539163442588,
      "grad_norm": 4.759608268737793,
      "learning_rate": 4.1177884030464516e-05,
      "loss": 0.7224,
      "step": 1160200
    },
    {
      "epoch": 10.587451638805753,
      "grad_norm": 3.7710962295532227,
      "learning_rate": 4.117712363432854e-05,
      "loss": 0.6842,
      "step": 1160300
    },
    {
      "epoch": 10.588364114168918,
      "grad_norm": 4.568287372589111,
      "learning_rate": 4.117636323819257e-05,
      "loss": 0.663,
      "step": 1160400
    },
    {
      "epoch": 10.589276589532084,
      "grad_norm": 3.8967490196228027,
      "learning_rate": 4.11756028420566e-05,
      "loss": 0.7054,
      "step": 1160500
    },
    {
      "epoch": 10.590189064895247,
      "grad_norm": 3.3729450702667236,
      "learning_rate": 4.117484244592063e-05,
      "loss": 0.6841,
      "step": 1160600
    },
    {
      "epoch": 10.591101540258412,
      "grad_norm": 3.558196783065796,
      "learning_rate": 4.117408204978466e-05,
      "loss": 0.6628,
      "step": 1160700
    },
    {
      "epoch": 10.592014015621578,
      "grad_norm": 3.6295955181121826,
      "learning_rate": 4.117332165364869e-05,
      "loss": 0.6818,
      "step": 1160800
    },
    {
      "epoch": 10.592926490984743,
      "grad_norm": 4.1132049560546875,
      "learning_rate": 4.117256125751271e-05,
      "loss": 0.7109,
      "step": 1160900
    },
    {
      "epoch": 10.593838966347908,
      "grad_norm": 4.384803295135498,
      "learning_rate": 4.117180086137675e-05,
      "loss": 0.7097,
      "step": 1161000
    },
    {
      "epoch": 10.594751441711074,
      "grad_norm": 4.231593608856201,
      "learning_rate": 4.117104046524077e-05,
      "loss": 0.6749,
      "step": 1161100
    },
    {
      "epoch": 10.595663917074239,
      "grad_norm": 2.972367286682129,
      "learning_rate": 4.11702800691048e-05,
      "loss": 0.6771,
      "step": 1161200
    },
    {
      "epoch": 10.596576392437404,
      "grad_norm": 3.1541783809661865,
      "learning_rate": 4.1169519672968833e-05,
      "loss": 0.6489,
      "step": 1161300
    },
    {
      "epoch": 10.59748886780057,
      "grad_norm": 4.454240798950195,
      "learning_rate": 4.1168759276832863e-05,
      "loss": 0.6734,
      "step": 1161400
    },
    {
      "epoch": 10.598401343163735,
      "grad_norm": 4.094655990600586,
      "learning_rate": 4.116799888069689e-05,
      "loss": 0.6986,
      "step": 1161500
    },
    {
      "epoch": 10.5993138185269,
      "grad_norm": 3.8825838565826416,
      "learning_rate": 4.1167238484560924e-05,
      "loss": 0.6591,
      "step": 1161600
    },
    {
      "epoch": 10.600226293890065,
      "grad_norm": 3.1363353729248047,
      "learning_rate": 4.116647808842495e-05,
      "loss": 0.6682,
      "step": 1161700
    },
    {
      "epoch": 10.60113876925323,
      "grad_norm": 3.6623315811157227,
      "learning_rate": 4.116571769228898e-05,
      "loss": 0.7148,
      "step": 1161800
    },
    {
      "epoch": 10.602051244616396,
      "grad_norm": 3.1571714878082275,
      "learning_rate": 4.116495729615301e-05,
      "loss": 0.697,
      "step": 1161900
    },
    {
      "epoch": 10.602963719979561,
      "grad_norm": 4.411921977996826,
      "learning_rate": 4.116419690001703e-05,
      "loss": 0.6686,
      "step": 1162000
    },
    {
      "epoch": 10.603876195342727,
      "grad_norm": 4.401771545410156,
      "learning_rate": 4.116343650388107e-05,
      "loss": 0.6753,
      "step": 1162100
    },
    {
      "epoch": 10.60478867070589,
      "grad_norm": 4.16602897644043,
      "learning_rate": 4.116267610774509e-05,
      "loss": 0.7123,
      "step": 1162200
    },
    {
      "epoch": 10.605701146069055,
      "grad_norm": 3.2232298851013184,
      "learning_rate": 4.116191571160912e-05,
      "loss": 0.6852,
      "step": 1162300
    },
    {
      "epoch": 10.60661362143222,
      "grad_norm": 3.8022029399871826,
      "learning_rate": 4.116115531547315e-05,
      "loss": 0.6573,
      "step": 1162400
    },
    {
      "epoch": 10.607526096795386,
      "grad_norm": 3.631504774093628,
      "learning_rate": 4.116039491933718e-05,
      "loss": 0.6749,
      "step": 1162500
    },
    {
      "epoch": 10.608438572158551,
      "grad_norm": 3.6564953327178955,
      "learning_rate": 4.1159634523201204e-05,
      "loss": 0.6858,
      "step": 1162600
    },
    {
      "epoch": 10.609351047521717,
      "grad_norm": 4.585268020629883,
      "learning_rate": 4.115887412706524e-05,
      "loss": 0.6614,
      "step": 1162700
    },
    {
      "epoch": 10.610263522884882,
      "grad_norm": 3.9619598388671875,
      "learning_rate": 4.1158113730929264e-05,
      "loss": 0.6682,
      "step": 1162800
    },
    {
      "epoch": 10.611175998248047,
      "grad_norm": 3.678009510040283,
      "learning_rate": 4.1157353334793294e-05,
      "loss": 0.6927,
      "step": 1162900
    },
    {
      "epoch": 10.612088473611212,
      "grad_norm": 4.300121307373047,
      "learning_rate": 4.1156592938657324e-05,
      "loss": 0.6498,
      "step": 1163000
    },
    {
      "epoch": 10.613000948974378,
      "grad_norm": 4.084217071533203,
      "learning_rate": 4.1155832542521354e-05,
      "loss": 0.6847,
      "step": 1163100
    },
    {
      "epoch": 10.613913424337543,
      "grad_norm": 4.304009437561035,
      "learning_rate": 4.1155072146385384e-05,
      "loss": 0.6718,
      "step": 1163200
    },
    {
      "epoch": 10.614825899700708,
      "grad_norm": 3.999497175216675,
      "learning_rate": 4.1154311750249414e-05,
      "loss": 0.6615,
      "step": 1163300
    },
    {
      "epoch": 10.615738375063874,
      "grad_norm": 4.394874572753906,
      "learning_rate": 4.115355135411344e-05,
      "loss": 0.7053,
      "step": 1163400
    },
    {
      "epoch": 10.616650850427039,
      "grad_norm": 3.338169574737549,
      "learning_rate": 4.1152790957977475e-05,
      "loss": 0.6745,
      "step": 1163500
    },
    {
      "epoch": 10.617563325790204,
      "grad_norm": 4.3291916847229,
      "learning_rate": 4.11520305618415e-05,
      "loss": 0.6917,
      "step": 1163600
    },
    {
      "epoch": 10.61847580115337,
      "grad_norm": 3.6345949172973633,
      "learning_rate": 4.115127016570553e-05,
      "loss": 0.6796,
      "step": 1163700
    },
    {
      "epoch": 10.619388276516535,
      "grad_norm": 4.514980316162109,
      "learning_rate": 4.115050976956956e-05,
      "loss": 0.6978,
      "step": 1163800
    },
    {
      "epoch": 10.6203007518797,
      "grad_norm": 4.12484073638916,
      "learning_rate": 4.114974937343359e-05,
      "loss": 0.6865,
      "step": 1163900
    },
    {
      "epoch": 10.621213227242864,
      "grad_norm": 3.6842451095581055,
      "learning_rate": 4.114898897729761e-05,
      "loss": 0.7155,
      "step": 1164000
    },
    {
      "epoch": 10.622125702606029,
      "grad_norm": 2.991682529449463,
      "learning_rate": 4.114822858116165e-05,
      "loss": 0.6662,
      "step": 1164100
    },
    {
      "epoch": 10.623038177969194,
      "grad_norm": 3.3148610591888428,
      "learning_rate": 4.114746818502567e-05,
      "loss": 0.6676,
      "step": 1164200
    },
    {
      "epoch": 10.62395065333236,
      "grad_norm": 3.3915224075317383,
      "learning_rate": 4.11467077888897e-05,
      "loss": 0.696,
      "step": 1164300
    },
    {
      "epoch": 10.624863128695525,
      "grad_norm": 4.295526027679443,
      "learning_rate": 4.114594739275373e-05,
      "loss": 0.6697,
      "step": 1164400
    },
    {
      "epoch": 10.62577560405869,
      "grad_norm": 3.7597999572753906,
      "learning_rate": 4.1145186996617755e-05,
      "loss": 0.673,
      "step": 1164500
    },
    {
      "epoch": 10.626688079421855,
      "grad_norm": 4.009448528289795,
      "learning_rate": 4.114442660048179e-05,
      "loss": 0.6932,
      "step": 1164600
    },
    {
      "epoch": 10.62760055478502,
      "grad_norm": 4.41350793838501,
      "learning_rate": 4.1143666204345815e-05,
      "loss": 0.6581,
      "step": 1164700
    },
    {
      "epoch": 10.628513030148186,
      "grad_norm": 3.7668824195861816,
      "learning_rate": 4.1142905808209845e-05,
      "loss": 0.7363,
      "step": 1164800
    },
    {
      "epoch": 10.629425505511351,
      "grad_norm": 3.897934913635254,
      "learning_rate": 4.1142145412073875e-05,
      "loss": 0.6821,
      "step": 1164900
    },
    {
      "epoch": 10.630337980874517,
      "grad_norm": 3.585217237472534,
      "learning_rate": 4.1141385015937905e-05,
      "loss": 0.7043,
      "step": 1165000
    },
    {
      "epoch": 10.631250456237682,
      "grad_norm": 4.592435359954834,
      "learning_rate": 4.114062461980193e-05,
      "loss": 0.6547,
      "step": 1165100
    },
    {
      "epoch": 10.632162931600847,
      "grad_norm": 3.0676863193511963,
      "learning_rate": 4.1139864223665965e-05,
      "loss": 0.6921,
      "step": 1165200
    },
    {
      "epoch": 10.633075406964013,
      "grad_norm": 3.471418619155884,
      "learning_rate": 4.113910382752999e-05,
      "loss": 0.665,
      "step": 1165300
    },
    {
      "epoch": 10.633987882327178,
      "grad_norm": 4.106420993804932,
      "learning_rate": 4.113834343139402e-05,
      "loss": 0.6866,
      "step": 1165400
    },
    {
      "epoch": 10.634900357690343,
      "grad_norm": 3.0074474811553955,
      "learning_rate": 4.113758303525805e-05,
      "loss": 0.6883,
      "step": 1165500
    },
    {
      "epoch": 10.635812833053507,
      "grad_norm": 5.417276382446289,
      "learning_rate": 4.113682263912208e-05,
      "loss": 0.6709,
      "step": 1165600
    },
    {
      "epoch": 10.636725308416672,
      "grad_norm": 4.514575004577637,
      "learning_rate": 4.113606224298611e-05,
      "loss": 0.6564,
      "step": 1165700
    },
    {
      "epoch": 10.637637783779837,
      "grad_norm": 3.525679349899292,
      "learning_rate": 4.113530184685014e-05,
      "loss": 0.7127,
      "step": 1165800
    },
    {
      "epoch": 10.638550259143003,
      "grad_norm": 5.347098350524902,
      "learning_rate": 4.113454145071416e-05,
      "loss": 0.6665,
      "step": 1165900
    },
    {
      "epoch": 10.639462734506168,
      "grad_norm": 3.8513035774230957,
      "learning_rate": 4.11337810545782e-05,
      "loss": 0.6809,
      "step": 1166000
    },
    {
      "epoch": 10.640375209869333,
      "grad_norm": 4.085393905639648,
      "learning_rate": 4.113302065844222e-05,
      "loss": 0.6713,
      "step": 1166100
    },
    {
      "epoch": 10.641287685232498,
      "grad_norm": 3.1190390586853027,
      "learning_rate": 4.113226026230625e-05,
      "loss": 0.7161,
      "step": 1166200
    },
    {
      "epoch": 10.642200160595664,
      "grad_norm": 2.8814873695373535,
      "learning_rate": 4.113149986617028e-05,
      "loss": 0.6882,
      "step": 1166300
    },
    {
      "epoch": 10.643112635958829,
      "grad_norm": 3.447535991668701,
      "learning_rate": 4.113073947003431e-05,
      "loss": 0.6696,
      "step": 1166400
    },
    {
      "epoch": 10.644025111321994,
      "grad_norm": 3.941591262817383,
      "learning_rate": 4.1129979073898336e-05,
      "loss": 0.7204,
      "step": 1166500
    },
    {
      "epoch": 10.64493758668516,
      "grad_norm": 3.966763973236084,
      "learning_rate": 4.112921867776237e-05,
      "loss": 0.7225,
      "step": 1166600
    },
    {
      "epoch": 10.645850062048325,
      "grad_norm": 3.9122872352600098,
      "learning_rate": 4.1128458281626396e-05,
      "loss": 0.6736,
      "step": 1166700
    },
    {
      "epoch": 10.64676253741149,
      "grad_norm": 3.64304780960083,
      "learning_rate": 4.1127697885490426e-05,
      "loss": 0.7303,
      "step": 1166800
    },
    {
      "epoch": 10.647675012774656,
      "grad_norm": 3.73254656791687,
      "learning_rate": 4.1126937489354456e-05,
      "loss": 0.6773,
      "step": 1166900
    },
    {
      "epoch": 10.64858748813782,
      "grad_norm": 3.3777008056640625,
      "learning_rate": 4.1126177093218486e-05,
      "loss": 0.6542,
      "step": 1167000
    },
    {
      "epoch": 10.649499963500986,
      "grad_norm": 4.038203239440918,
      "learning_rate": 4.1125416697082516e-05,
      "loss": 0.691,
      "step": 1167100
    },
    {
      "epoch": 10.650412438864151,
      "grad_norm": 3.750669479370117,
      "learning_rate": 4.1124656300946546e-05,
      "loss": 0.7225,
      "step": 1167200
    },
    {
      "epoch": 10.651324914227317,
      "grad_norm": 3.4309937953948975,
      "learning_rate": 4.112389590481057e-05,
      "loss": 0.6992,
      "step": 1167300
    },
    {
      "epoch": 10.65223738959048,
      "grad_norm": 4.610215663909912,
      "learning_rate": 4.11231355086746e-05,
      "loss": 0.7066,
      "step": 1167400
    },
    {
      "epoch": 10.653149864953646,
      "grad_norm": 3.462082862854004,
      "learning_rate": 4.112237511253863e-05,
      "loss": 0.6982,
      "step": 1167500
    },
    {
      "epoch": 10.65406234031681,
      "grad_norm": 4.209133625030518,
      "learning_rate": 4.112161471640266e-05,
      "loss": 0.6877,
      "step": 1167600
    },
    {
      "epoch": 10.654974815679976,
      "grad_norm": 4.767814636230469,
      "learning_rate": 4.112085432026669e-05,
      "loss": 0.6755,
      "step": 1167700
    },
    {
      "epoch": 10.655887291043141,
      "grad_norm": 3.981607675552368,
      "learning_rate": 4.112009392413071e-05,
      "loss": 0.7172,
      "step": 1167800
    },
    {
      "epoch": 10.656799766406307,
      "grad_norm": 3.930534839630127,
      "learning_rate": 4.111933352799475e-05,
      "loss": 0.7202,
      "step": 1167900
    },
    {
      "epoch": 10.657712241769472,
      "grad_norm": 4.101893901824951,
      "learning_rate": 4.111857313185877e-05,
      "loss": 0.6585,
      "step": 1168000
    },
    {
      "epoch": 10.658624717132637,
      "grad_norm": 4.9971137046813965,
      "learning_rate": 4.1117812735722803e-05,
      "loss": 0.696,
      "step": 1168100
    },
    {
      "epoch": 10.659537192495803,
      "grad_norm": 3.9988367557525635,
      "learning_rate": 4.1117052339586833e-05,
      "loss": 0.6748,
      "step": 1168200
    },
    {
      "epoch": 10.660449667858968,
      "grad_norm": 4.083387851715088,
      "learning_rate": 4.1116291943450864e-05,
      "loss": 0.7127,
      "step": 1168300
    },
    {
      "epoch": 10.661362143222133,
      "grad_norm": 3.0386741161346436,
      "learning_rate": 4.111553154731489e-05,
      "loss": 0.6878,
      "step": 1168400
    },
    {
      "epoch": 10.662274618585299,
      "grad_norm": 3.4581146240234375,
      "learning_rate": 4.1114771151178924e-05,
      "loss": 0.6651,
      "step": 1168500
    },
    {
      "epoch": 10.663187093948464,
      "grad_norm": 4.492913246154785,
      "learning_rate": 4.111401075504295e-05,
      "loss": 0.6782,
      "step": 1168600
    },
    {
      "epoch": 10.66409956931163,
      "grad_norm": 4.104873180389404,
      "learning_rate": 4.111325035890698e-05,
      "loss": 0.7436,
      "step": 1168700
    },
    {
      "epoch": 10.665012044674794,
      "grad_norm": 1.7008330821990967,
      "learning_rate": 4.111248996277101e-05,
      "loss": 0.6907,
      "step": 1168800
    },
    {
      "epoch": 10.66592452003796,
      "grad_norm": 4.50604772567749,
      "learning_rate": 4.111172956663504e-05,
      "loss": 0.6092,
      "step": 1168900
    },
    {
      "epoch": 10.666836995401123,
      "grad_norm": 4.499532222747803,
      "learning_rate": 4.111096917049907e-05,
      "loss": 0.6989,
      "step": 1169000
    },
    {
      "epoch": 10.667749470764289,
      "grad_norm": 4.725339889526367,
      "learning_rate": 4.11102087743631e-05,
      "loss": 0.6635,
      "step": 1169100
    },
    {
      "epoch": 10.668661946127454,
      "grad_norm": 4.687377452850342,
      "learning_rate": 4.110944837822712e-05,
      "loss": 0.6972,
      "step": 1169200
    },
    {
      "epoch": 10.66957442149062,
      "grad_norm": 5.016127109527588,
      "learning_rate": 4.110868798209116e-05,
      "loss": 0.6645,
      "step": 1169300
    },
    {
      "epoch": 10.670486896853784,
      "grad_norm": 4.147139072418213,
      "learning_rate": 4.110792758595518e-05,
      "loss": 0.6811,
      "step": 1169400
    },
    {
      "epoch": 10.67139937221695,
      "grad_norm": 4.180774688720703,
      "learning_rate": 4.110716718981921e-05,
      "loss": 0.6875,
      "step": 1169500
    },
    {
      "epoch": 10.672311847580115,
      "grad_norm": 3.8708574771881104,
      "learning_rate": 4.110640679368324e-05,
      "loss": 0.6818,
      "step": 1169600
    },
    {
      "epoch": 10.67322432294328,
      "grad_norm": 4.421957969665527,
      "learning_rate": 4.110564639754727e-05,
      "loss": 0.6609,
      "step": 1169700
    },
    {
      "epoch": 10.674136798306446,
      "grad_norm": 4.583136081695557,
      "learning_rate": 4.1104886001411294e-05,
      "loss": 0.6996,
      "step": 1169800
    },
    {
      "epoch": 10.675049273669611,
      "grad_norm": 2.988386631011963,
      "learning_rate": 4.110412560527533e-05,
      "loss": 0.6584,
      "step": 1169900
    },
    {
      "epoch": 10.675961749032776,
      "grad_norm": 3.919238567352295,
      "learning_rate": 4.1103365209139354e-05,
      "loss": 0.6882,
      "step": 1170000
    },
    {
      "epoch": 10.676874224395942,
      "grad_norm": 2.7968063354492188,
      "learning_rate": 4.1102604813003384e-05,
      "loss": 0.6864,
      "step": 1170100
    },
    {
      "epoch": 10.677786699759107,
      "grad_norm": 3.369798183441162,
      "learning_rate": 4.1101844416867415e-05,
      "loss": 0.6664,
      "step": 1170200
    },
    {
      "epoch": 10.678699175122272,
      "grad_norm": 3.445125102996826,
      "learning_rate": 4.110108402073144e-05,
      "loss": 0.6848,
      "step": 1170300
    },
    {
      "epoch": 10.679611650485437,
      "grad_norm": 4.3721842765808105,
      "learning_rate": 4.1100323624595475e-05,
      "loss": 0.6874,
      "step": 1170400
    },
    {
      "epoch": 10.680524125848603,
      "grad_norm": 3.6328072547912598,
      "learning_rate": 4.10995632284595e-05,
      "loss": 0.6736,
      "step": 1170500
    },
    {
      "epoch": 10.681436601211768,
      "grad_norm": 3.0395240783691406,
      "learning_rate": 4.109880283232353e-05,
      "loss": 0.6875,
      "step": 1170600
    },
    {
      "epoch": 10.682349076574933,
      "grad_norm": 3.738057851791382,
      "learning_rate": 4.109804243618756e-05,
      "loss": 0.7054,
      "step": 1170700
    },
    {
      "epoch": 10.683261551938097,
      "grad_norm": 4.4355292320251465,
      "learning_rate": 4.109728204005159e-05,
      "loss": 0.6984,
      "step": 1170800
    },
    {
      "epoch": 10.684174027301262,
      "grad_norm": 3.5454206466674805,
      "learning_rate": 4.109652164391561e-05,
      "loss": 0.6607,
      "step": 1170900
    },
    {
      "epoch": 10.685086502664427,
      "grad_norm": 4.113931655883789,
      "learning_rate": 4.109576124777965e-05,
      "loss": 0.6519,
      "step": 1171000
    },
    {
      "epoch": 10.685998978027593,
      "grad_norm": 4.596959590911865,
      "learning_rate": 4.109500085164367e-05,
      "loss": 0.7059,
      "step": 1171100
    },
    {
      "epoch": 10.686911453390758,
      "grad_norm": 3.768862009048462,
      "learning_rate": 4.10942404555077e-05,
      "loss": 0.7244,
      "step": 1171200
    },
    {
      "epoch": 10.687823928753923,
      "grad_norm": 4.512411117553711,
      "learning_rate": 4.109348005937173e-05,
      "loss": 0.683,
      "step": 1171300
    },
    {
      "epoch": 10.688736404117089,
      "grad_norm": 4.893408298492432,
      "learning_rate": 4.109271966323576e-05,
      "loss": 0.6504,
      "step": 1171400
    },
    {
      "epoch": 10.689648879480254,
      "grad_norm": 3.0446722507476807,
      "learning_rate": 4.109195926709979e-05,
      "loss": 0.6565,
      "step": 1171500
    },
    {
      "epoch": 10.69056135484342,
      "grad_norm": 2.731663465499878,
      "learning_rate": 4.109119887096382e-05,
      "loss": 0.6611,
      "step": 1171600
    },
    {
      "epoch": 10.691473830206585,
      "grad_norm": 3.5130903720855713,
      "learning_rate": 4.1090438474827845e-05,
      "loss": 0.6821,
      "step": 1171700
    },
    {
      "epoch": 10.69238630556975,
      "grad_norm": 5.904264450073242,
      "learning_rate": 4.108967807869188e-05,
      "loss": 0.6449,
      "step": 1171800
    },
    {
      "epoch": 10.693298780932915,
      "grad_norm": 3.5547873973846436,
      "learning_rate": 4.1088917682555905e-05,
      "loss": 0.6682,
      "step": 1171900
    },
    {
      "epoch": 10.69421125629608,
      "grad_norm": 3.7529609203338623,
      "learning_rate": 4.1088157286419935e-05,
      "loss": 0.6812,
      "step": 1172000
    },
    {
      "epoch": 10.695123731659246,
      "grad_norm": 3.35539174079895,
      "learning_rate": 4.1087396890283965e-05,
      "loss": 0.681,
      "step": 1172100
    },
    {
      "epoch": 10.696036207022411,
      "grad_norm": 4.433065891265869,
      "learning_rate": 4.1086636494147996e-05,
      "loss": 0.681,
      "step": 1172200
    },
    {
      "epoch": 10.696948682385576,
      "grad_norm": 3.6206047534942627,
      "learning_rate": 4.108587609801202e-05,
      "loss": 0.7394,
      "step": 1172300
    },
    {
      "epoch": 10.69786115774874,
      "grad_norm": 3.4153168201446533,
      "learning_rate": 4.1085115701876056e-05,
      "loss": 0.7134,
      "step": 1172400
    },
    {
      "epoch": 10.698773633111905,
      "grad_norm": 4.613244533538818,
      "learning_rate": 4.108435530574008e-05,
      "loss": 0.693,
      "step": 1172500
    },
    {
      "epoch": 10.69968610847507,
      "grad_norm": 4.088135719299316,
      "learning_rate": 4.108359490960411e-05,
      "loss": 0.6932,
      "step": 1172600
    },
    {
      "epoch": 10.700598583838236,
      "grad_norm": 3.3681344985961914,
      "learning_rate": 4.108283451346814e-05,
      "loss": 0.6891,
      "step": 1172700
    },
    {
      "epoch": 10.701511059201401,
      "grad_norm": 3.7698421478271484,
      "learning_rate": 4.108207411733217e-05,
      "loss": 0.7135,
      "step": 1172800
    },
    {
      "epoch": 10.702423534564566,
      "grad_norm": 4.496029853820801,
      "learning_rate": 4.10813137211962e-05,
      "loss": 0.7026,
      "step": 1172900
    },
    {
      "epoch": 10.703336009927732,
      "grad_norm": 3.1041927337646484,
      "learning_rate": 4.108055332506022e-05,
      "loss": 0.6626,
      "step": 1173000
    },
    {
      "epoch": 10.704248485290897,
      "grad_norm": 4.348824977874756,
      "learning_rate": 4.107979292892425e-05,
      "loss": 0.6863,
      "step": 1173100
    },
    {
      "epoch": 10.705160960654062,
      "grad_norm": 4.198923587799072,
      "learning_rate": 4.107903253278828e-05,
      "loss": 0.7052,
      "step": 1173200
    },
    {
      "epoch": 10.706073436017228,
      "grad_norm": 4.029680252075195,
      "learning_rate": 4.107827213665231e-05,
      "loss": 0.6472,
      "step": 1173300
    },
    {
      "epoch": 10.706985911380393,
      "grad_norm": 4.09663200378418,
      "learning_rate": 4.1077511740516336e-05,
      "loss": 0.6489,
      "step": 1173400
    },
    {
      "epoch": 10.707898386743558,
      "grad_norm": 2.292884588241577,
      "learning_rate": 4.107675134438037e-05,
      "loss": 0.7137,
      "step": 1173500
    },
    {
      "epoch": 10.708810862106724,
      "grad_norm": 4.132826805114746,
      "learning_rate": 4.1075990948244396e-05,
      "loss": 0.6809,
      "step": 1173600
    },
    {
      "epoch": 10.709723337469889,
      "grad_norm": 3.9977712631225586,
      "learning_rate": 4.1075230552108426e-05,
      "loss": 0.6901,
      "step": 1173700
    },
    {
      "epoch": 10.710635812833054,
      "grad_norm": 3.817667245864868,
      "learning_rate": 4.1074470155972456e-05,
      "loss": 0.7071,
      "step": 1173800
    },
    {
      "epoch": 10.71154828819622,
      "grad_norm": 3.5061469078063965,
      "learning_rate": 4.1073709759836486e-05,
      "loss": 0.6811,
      "step": 1173900
    },
    {
      "epoch": 10.712460763559385,
      "grad_norm": 3.7008981704711914,
      "learning_rate": 4.1072949363700516e-05,
      "loss": 0.6518,
      "step": 1174000
    },
    {
      "epoch": 10.71337323892255,
      "grad_norm": 4.2968831062316895,
      "learning_rate": 4.1072188967564546e-05,
      "loss": 0.7374,
      "step": 1174100
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 4.130925178527832,
      "learning_rate": 4.107142857142857e-05,
      "loss": 0.7208,
      "step": 1174200
    },
    {
      "epoch": 10.715198189648879,
      "grad_norm": 3.2680869102478027,
      "learning_rate": 4.1070668175292607e-05,
      "loss": 0.6795,
      "step": 1174300
    },
    {
      "epoch": 10.716110665012044,
      "grad_norm": 4.603827953338623,
      "learning_rate": 4.106990777915663e-05,
      "loss": 0.6859,
      "step": 1174400
    },
    {
      "epoch": 10.71702314037521,
      "grad_norm": 3.2630093097686768,
      "learning_rate": 4.106914738302066e-05,
      "loss": 0.6712,
      "step": 1174500
    },
    {
      "epoch": 10.717935615738375,
      "grad_norm": 3.698188304901123,
      "learning_rate": 4.106838698688469e-05,
      "loss": 0.6833,
      "step": 1174600
    },
    {
      "epoch": 10.71884809110154,
      "grad_norm": 4.248188018798828,
      "learning_rate": 4.106762659074872e-05,
      "loss": 0.7059,
      "step": 1174700
    },
    {
      "epoch": 10.719760566464705,
      "grad_norm": 3.8950917720794678,
      "learning_rate": 4.1066866194612743e-05,
      "loss": 0.696,
      "step": 1174800
    },
    {
      "epoch": 10.72067304182787,
      "grad_norm": 3.4168546199798584,
      "learning_rate": 4.106610579847678e-05,
      "loss": 0.6983,
      "step": 1174900
    },
    {
      "epoch": 10.721585517191036,
      "grad_norm": 4.090062141418457,
      "learning_rate": 4.1065345402340804e-05,
      "loss": 0.6717,
      "step": 1175000
    },
    {
      "epoch": 10.722497992554201,
      "grad_norm": 3.267573356628418,
      "learning_rate": 4.1064585006204834e-05,
      "loss": 0.6608,
      "step": 1175100
    },
    {
      "epoch": 10.723410467917367,
      "grad_norm": 3.3852438926696777,
      "learning_rate": 4.1063824610068864e-05,
      "loss": 0.7033,
      "step": 1175200
    },
    {
      "epoch": 10.724322943280532,
      "grad_norm": 4.142153739929199,
      "learning_rate": 4.1063064213932894e-05,
      "loss": 0.6791,
      "step": 1175300
    },
    {
      "epoch": 10.725235418643697,
      "grad_norm": 4.100742340087891,
      "learning_rate": 4.1062303817796924e-05,
      "loss": 0.6475,
      "step": 1175400
    },
    {
      "epoch": 10.726147894006862,
      "grad_norm": 3.6040170192718506,
      "learning_rate": 4.1061543421660954e-05,
      "loss": 0.6582,
      "step": 1175500
    },
    {
      "epoch": 10.727060369370028,
      "grad_norm": 4.751911163330078,
      "learning_rate": 4.106078302552498e-05,
      "loss": 0.6849,
      "step": 1175600
    },
    {
      "epoch": 10.727972844733193,
      "grad_norm": 3.7420568466186523,
      "learning_rate": 4.1060022629389014e-05,
      "loss": 0.7095,
      "step": 1175700
    },
    {
      "epoch": 10.728885320096357,
      "grad_norm": 3.9733164310455322,
      "learning_rate": 4.105926223325304e-05,
      "loss": 0.6587,
      "step": 1175800
    },
    {
      "epoch": 10.729797795459522,
      "grad_norm": 5.252695083618164,
      "learning_rate": 4.105850183711706e-05,
      "loss": 0.684,
      "step": 1175900
    },
    {
      "epoch": 10.730710270822687,
      "grad_norm": 3.998140335083008,
      "learning_rate": 4.10577414409811e-05,
      "loss": 0.7024,
      "step": 1176000
    },
    {
      "epoch": 10.731622746185852,
      "grad_norm": 3.1897165775299072,
      "learning_rate": 4.105698104484512e-05,
      "loss": 0.664,
      "step": 1176100
    },
    {
      "epoch": 10.732535221549018,
      "grad_norm": 3.678983688354492,
      "learning_rate": 4.105622064870915e-05,
      "loss": 0.661,
      "step": 1176200
    },
    {
      "epoch": 10.733447696912183,
      "grad_norm": 3.680936813354492,
      "learning_rate": 4.105546025257318e-05,
      "loss": 0.6902,
      "step": 1176300
    },
    {
      "epoch": 10.734360172275348,
      "grad_norm": 3.2564470767974854,
      "learning_rate": 4.105469985643721e-05,
      "loss": 0.6797,
      "step": 1176400
    },
    {
      "epoch": 10.735272647638514,
      "grad_norm": 4.058914661407471,
      "learning_rate": 4.105393946030124e-05,
      "loss": 0.6508,
      "step": 1176500
    },
    {
      "epoch": 10.736185123001679,
      "grad_norm": 3.8357603549957275,
      "learning_rate": 4.105317906416527e-05,
      "loss": 0.6739,
      "step": 1176600
    },
    {
      "epoch": 10.737097598364844,
      "grad_norm": 3.7454264163970947,
      "learning_rate": 4.1052418668029294e-05,
      "loss": 0.6717,
      "step": 1176700
    },
    {
      "epoch": 10.73801007372801,
      "grad_norm": 3.44797682762146,
      "learning_rate": 4.105165827189333e-05,
      "loss": 0.7071,
      "step": 1176800
    },
    {
      "epoch": 10.738922549091175,
      "grad_norm": 2.9699509143829346,
      "learning_rate": 4.1050897875757354e-05,
      "loss": 0.6652,
      "step": 1176900
    },
    {
      "epoch": 10.73983502445434,
      "grad_norm": 4.2150444984436035,
      "learning_rate": 4.1050137479621385e-05,
      "loss": 0.6775,
      "step": 1177000
    },
    {
      "epoch": 10.740747499817505,
      "grad_norm": 3.0660736560821533,
      "learning_rate": 4.1049377083485415e-05,
      "loss": 0.6941,
      "step": 1177100
    },
    {
      "epoch": 10.74165997518067,
      "grad_norm": 3.6062662601470947,
      "learning_rate": 4.1048616687349445e-05,
      "loss": 0.6823,
      "step": 1177200
    },
    {
      "epoch": 10.742572450543836,
      "grad_norm": 4.321138381958008,
      "learning_rate": 4.104785629121347e-05,
      "loss": 0.6702,
      "step": 1177300
    },
    {
      "epoch": 10.743484925907001,
      "grad_norm": 4.243868827819824,
      "learning_rate": 4.1047095895077505e-05,
      "loss": 0.7006,
      "step": 1177400
    },
    {
      "epoch": 10.744397401270167,
      "grad_norm": 4.084316730499268,
      "learning_rate": 4.104633549894153e-05,
      "loss": 0.6911,
      "step": 1177500
    },
    {
      "epoch": 10.74530987663333,
      "grad_norm": 2.924351930618286,
      "learning_rate": 4.104557510280556e-05,
      "loss": 0.7005,
      "step": 1177600
    },
    {
      "epoch": 10.746222351996495,
      "grad_norm": 3.850409984588623,
      "learning_rate": 4.104481470666959e-05,
      "loss": 0.6846,
      "step": 1177700
    },
    {
      "epoch": 10.74713482735966,
      "grad_norm": 4.305899143218994,
      "learning_rate": 4.104405431053362e-05,
      "loss": 0.6889,
      "step": 1177800
    },
    {
      "epoch": 10.748047302722826,
      "grad_norm": 4.149167060852051,
      "learning_rate": 4.104329391439765e-05,
      "loss": 0.7,
      "step": 1177900
    },
    {
      "epoch": 10.748959778085991,
      "grad_norm": 3.505366086959839,
      "learning_rate": 4.104253351826168e-05,
      "loss": 0.6653,
      "step": 1178000
    },
    {
      "epoch": 10.749872253449157,
      "grad_norm": 3.5413339138031006,
      "learning_rate": 4.10417731221257e-05,
      "loss": 0.6743,
      "step": 1178100
    },
    {
      "epoch": 10.750784728812322,
      "grad_norm": 2.9896047115325928,
      "learning_rate": 4.104101272598974e-05,
      "loss": 0.6866,
      "step": 1178200
    },
    {
      "epoch": 10.751697204175487,
      "grad_norm": 3.6794817447662354,
      "learning_rate": 4.104025232985376e-05,
      "loss": 0.683,
      "step": 1178300
    },
    {
      "epoch": 10.752609679538653,
      "grad_norm": 2.709667921066284,
      "learning_rate": 4.103949193371779e-05,
      "loss": 0.7241,
      "step": 1178400
    },
    {
      "epoch": 10.753522154901818,
      "grad_norm": 3.53775954246521,
      "learning_rate": 4.103873153758182e-05,
      "loss": 0.6423,
      "step": 1178500
    },
    {
      "epoch": 10.754434630264983,
      "grad_norm": 4.201366901397705,
      "learning_rate": 4.103797114144585e-05,
      "loss": 0.6965,
      "step": 1178600
    },
    {
      "epoch": 10.755347105628148,
      "grad_norm": 3.3058114051818848,
      "learning_rate": 4.1037210745309875e-05,
      "loss": 0.6976,
      "step": 1178700
    },
    {
      "epoch": 10.756259580991314,
      "grad_norm": 4.015970706939697,
      "learning_rate": 4.1036450349173905e-05,
      "loss": 0.6819,
      "step": 1178800
    },
    {
      "epoch": 10.757172056354479,
      "grad_norm": 3.884403944015503,
      "learning_rate": 4.1035689953037935e-05,
      "loss": 0.7355,
      "step": 1178900
    },
    {
      "epoch": 10.758084531717644,
      "grad_norm": 4.197669506072998,
      "learning_rate": 4.1034929556901966e-05,
      "loss": 0.7041,
      "step": 1179000
    },
    {
      "epoch": 10.75899700708081,
      "grad_norm": 4.05072021484375,
      "learning_rate": 4.1034169160765996e-05,
      "loss": 0.6581,
      "step": 1179100
    },
    {
      "epoch": 10.759909482443973,
      "grad_norm": 3.534609079360962,
      "learning_rate": 4.103340876463002e-05,
      "loss": 0.6827,
      "step": 1179200
    },
    {
      "epoch": 10.760821957807138,
      "grad_norm": 4.330593585968018,
      "learning_rate": 4.1032648368494056e-05,
      "loss": 0.6893,
      "step": 1179300
    },
    {
      "epoch": 10.761734433170304,
      "grad_norm": 3.819833517074585,
      "learning_rate": 4.103188797235808e-05,
      "loss": 0.6699,
      "step": 1179400
    },
    {
      "epoch": 10.762646908533469,
      "grad_norm": 4.570687294006348,
      "learning_rate": 4.103112757622211e-05,
      "loss": 0.6676,
      "step": 1179500
    },
    {
      "epoch": 10.763559383896634,
      "grad_norm": 4.132188320159912,
      "learning_rate": 4.103036718008614e-05,
      "loss": 0.6913,
      "step": 1179600
    },
    {
      "epoch": 10.7644718592598,
      "grad_norm": 4.793062686920166,
      "learning_rate": 4.102960678395017e-05,
      "loss": 0.6857,
      "step": 1179700
    },
    {
      "epoch": 10.765384334622965,
      "grad_norm": 3.941833019256592,
      "learning_rate": 4.10288463878142e-05,
      "loss": 0.6519,
      "step": 1179800
    },
    {
      "epoch": 10.76629680998613,
      "grad_norm": 3.0733511447906494,
      "learning_rate": 4.102808599167823e-05,
      "loss": 0.6651,
      "step": 1179900
    },
    {
      "epoch": 10.767209285349296,
      "grad_norm": 3.951796054840088,
      "learning_rate": 4.102732559554225e-05,
      "loss": 0.696,
      "step": 1180000
    },
    {
      "epoch": 10.76812176071246,
      "grad_norm": 4.504085540771484,
      "learning_rate": 4.102656519940628e-05,
      "loss": 0.663,
      "step": 1180100
    },
    {
      "epoch": 10.769034236075626,
      "grad_norm": 4.068248271942139,
      "learning_rate": 4.102580480327031e-05,
      "loss": 0.6617,
      "step": 1180200
    },
    {
      "epoch": 10.769946711438791,
      "grad_norm": 3.4570059776306152,
      "learning_rate": 4.102504440713434e-05,
      "loss": 0.708,
      "step": 1180300
    },
    {
      "epoch": 10.770859186801957,
      "grad_norm": 3.2784886360168457,
      "learning_rate": 4.102428401099837e-05,
      "loss": 0.6655,
      "step": 1180400
    },
    {
      "epoch": 10.771771662165122,
      "grad_norm": 3.623307466506958,
      "learning_rate": 4.10235236148624e-05,
      "loss": 0.7201,
      "step": 1180500
    },
    {
      "epoch": 10.772684137528287,
      "grad_norm": 5.559895038604736,
      "learning_rate": 4.1022763218726426e-05,
      "loss": 0.7125,
      "step": 1180600
    },
    {
      "epoch": 10.773596612891453,
      "grad_norm": 4.294679164886475,
      "learning_rate": 4.102200282259046e-05,
      "loss": 0.6853,
      "step": 1180700
    },
    {
      "epoch": 10.774509088254618,
      "grad_norm": 4.437056064605713,
      "learning_rate": 4.1021242426454486e-05,
      "loss": 0.6859,
      "step": 1180800
    },
    {
      "epoch": 10.775421563617783,
      "grad_norm": 4.672291278839111,
      "learning_rate": 4.1020482030318517e-05,
      "loss": 0.6878,
      "step": 1180900
    },
    {
      "epoch": 10.776334038980947,
      "grad_norm": 4.961477756500244,
      "learning_rate": 4.1019721634182547e-05,
      "loss": 0.6704,
      "step": 1181000
    },
    {
      "epoch": 10.777246514344112,
      "grad_norm": 3.927098274230957,
      "learning_rate": 4.101896123804658e-05,
      "loss": 0.6951,
      "step": 1181100
    },
    {
      "epoch": 10.778158989707277,
      "grad_norm": 3.99882435798645,
      "learning_rate": 4.101820084191061e-05,
      "loss": 0.6912,
      "step": 1181200
    },
    {
      "epoch": 10.779071465070443,
      "grad_norm": 3.7910244464874268,
      "learning_rate": 4.101744044577464e-05,
      "loss": 0.6689,
      "step": 1181300
    },
    {
      "epoch": 10.779983940433608,
      "grad_norm": 4.259517669677734,
      "learning_rate": 4.101668004963866e-05,
      "loss": 0.6868,
      "step": 1181400
    },
    {
      "epoch": 10.780896415796773,
      "grad_norm": 4.457818508148193,
      "learning_rate": 4.101591965350269e-05,
      "loss": 0.6733,
      "step": 1181500
    },
    {
      "epoch": 10.781808891159939,
      "grad_norm": 3.839761257171631,
      "learning_rate": 4.101515925736672e-05,
      "loss": 0.6478,
      "step": 1181600
    },
    {
      "epoch": 10.782721366523104,
      "grad_norm": 4.965792179107666,
      "learning_rate": 4.1014398861230743e-05,
      "loss": 0.6923,
      "step": 1181700
    },
    {
      "epoch": 10.78363384188627,
      "grad_norm": 5.33595609664917,
      "learning_rate": 4.101363846509478e-05,
      "loss": 0.6375,
      "step": 1181800
    },
    {
      "epoch": 10.784546317249434,
      "grad_norm": 3.500781536102295,
      "learning_rate": 4.1012878068958804e-05,
      "loss": 0.7235,
      "step": 1181900
    },
    {
      "epoch": 10.7854587926126,
      "grad_norm": 4.308375835418701,
      "learning_rate": 4.1012117672822834e-05,
      "loss": 0.6811,
      "step": 1182000
    },
    {
      "epoch": 10.786371267975765,
      "grad_norm": 4.01135778427124,
      "learning_rate": 4.1011357276686864e-05,
      "loss": 0.6962,
      "step": 1182100
    },
    {
      "epoch": 10.78728374333893,
      "grad_norm": 3.9221434593200684,
      "learning_rate": 4.1010596880550894e-05,
      "loss": 0.6648,
      "step": 1182200
    },
    {
      "epoch": 10.788196218702096,
      "grad_norm": 4.5446062088012695,
      "learning_rate": 4.1009836484414924e-05,
      "loss": 0.6859,
      "step": 1182300
    },
    {
      "epoch": 10.789108694065261,
      "grad_norm": 3.87507700920105,
      "learning_rate": 4.1009076088278954e-05,
      "loss": 0.6532,
      "step": 1182400
    },
    {
      "epoch": 10.790021169428426,
      "grad_norm": 3.788745403289795,
      "learning_rate": 4.100831569214298e-05,
      "loss": 0.729,
      "step": 1182500
    },
    {
      "epoch": 10.79093364479159,
      "grad_norm": 3.063917398452759,
      "learning_rate": 4.1007555296007014e-05,
      "loss": 0.6975,
      "step": 1182600
    },
    {
      "epoch": 10.791846120154755,
      "grad_norm": 4.257795333862305,
      "learning_rate": 4.100679489987104e-05,
      "loss": 0.6832,
      "step": 1182700
    },
    {
      "epoch": 10.79275859551792,
      "grad_norm": 3.6763861179351807,
      "learning_rate": 4.100603450373507e-05,
      "loss": 0.6772,
      "step": 1182800
    },
    {
      "epoch": 10.793671070881086,
      "grad_norm": 3.5327980518341064,
      "learning_rate": 4.10052741075991e-05,
      "loss": 0.7108,
      "step": 1182900
    },
    {
      "epoch": 10.794583546244251,
      "grad_norm": 3.4104127883911133,
      "learning_rate": 4.100451371146313e-05,
      "loss": 0.71,
      "step": 1183000
    },
    {
      "epoch": 10.795496021607416,
      "grad_norm": 3.848845958709717,
      "learning_rate": 4.100375331532715e-05,
      "loss": 0.6545,
      "step": 1183100
    },
    {
      "epoch": 10.796408496970582,
      "grad_norm": 3.524716377258301,
      "learning_rate": 4.100299291919119e-05,
      "loss": 0.6372,
      "step": 1183200
    },
    {
      "epoch": 10.797320972333747,
      "grad_norm": 4.114880084991455,
      "learning_rate": 4.100223252305521e-05,
      "loss": 0.6711,
      "step": 1183300
    },
    {
      "epoch": 10.798233447696912,
      "grad_norm": 3.9862916469573975,
      "learning_rate": 4.100147212691924e-05,
      "loss": 0.7141,
      "step": 1183400
    },
    {
      "epoch": 10.799145923060077,
      "grad_norm": 4.137300968170166,
      "learning_rate": 4.100071173078327e-05,
      "loss": 0.7054,
      "step": 1183500
    },
    {
      "epoch": 10.800058398423243,
      "grad_norm": 3.8660802841186523,
      "learning_rate": 4.09999513346473e-05,
      "loss": 0.6581,
      "step": 1183600
    },
    {
      "epoch": 10.800970873786408,
      "grad_norm": 3.4723730087280273,
      "learning_rate": 4.099919093851133e-05,
      "loss": 0.6495,
      "step": 1183700
    },
    {
      "epoch": 10.801883349149573,
      "grad_norm": 4.240847110748291,
      "learning_rate": 4.099843054237536e-05,
      "loss": 0.6424,
      "step": 1183800
    },
    {
      "epoch": 10.802795824512739,
      "grad_norm": 2.8596558570861816,
      "learning_rate": 4.0997670146239385e-05,
      "loss": 0.6429,
      "step": 1183900
    },
    {
      "epoch": 10.803708299875904,
      "grad_norm": 3.9133505821228027,
      "learning_rate": 4.099690975010342e-05,
      "loss": 0.6744,
      "step": 1184000
    },
    {
      "epoch": 10.80462077523907,
      "grad_norm": 4.10087776184082,
      "learning_rate": 4.0996149353967445e-05,
      "loss": 0.6722,
      "step": 1184100
    },
    {
      "epoch": 10.805533250602235,
      "grad_norm": 3.6272099018096924,
      "learning_rate": 4.0995388957831475e-05,
      "loss": 0.6685,
      "step": 1184200
    },
    {
      "epoch": 10.8064457259654,
      "grad_norm": 5.270588397979736,
      "learning_rate": 4.0994628561695505e-05,
      "loss": 0.6951,
      "step": 1184300
    },
    {
      "epoch": 10.807358201328563,
      "grad_norm": 4.120544910430908,
      "learning_rate": 4.099386816555953e-05,
      "loss": 0.6882,
      "step": 1184400
    },
    {
      "epoch": 10.808270676691729,
      "grad_norm": 3.23502516746521,
      "learning_rate": 4.099310776942356e-05,
      "loss": 0.6521,
      "step": 1184500
    },
    {
      "epoch": 10.809183152054894,
      "grad_norm": 3.7605960369110107,
      "learning_rate": 4.099234737328759e-05,
      "loss": 0.6267,
      "step": 1184600
    },
    {
      "epoch": 10.81009562741806,
      "grad_norm": 4.629787921905518,
      "learning_rate": 4.099158697715162e-05,
      "loss": 0.6899,
      "step": 1184700
    },
    {
      "epoch": 10.811008102781225,
      "grad_norm": 4.326664924621582,
      "learning_rate": 4.099082658101565e-05,
      "loss": 0.6656,
      "step": 1184800
    },
    {
      "epoch": 10.81192057814439,
      "grad_norm": 3.9515504837036133,
      "learning_rate": 4.099006618487968e-05,
      "loss": 0.7011,
      "step": 1184900
    },
    {
      "epoch": 10.812833053507555,
      "grad_norm": 3.3943819999694824,
      "learning_rate": 4.09893057887437e-05,
      "loss": 0.6948,
      "step": 1185000
    },
    {
      "epoch": 10.81374552887072,
      "grad_norm": 4.262017726898193,
      "learning_rate": 4.098854539260774e-05,
      "loss": 0.6644,
      "step": 1185100
    },
    {
      "epoch": 10.814658004233886,
      "grad_norm": 3.8345563411712646,
      "learning_rate": 4.098778499647176e-05,
      "loss": 0.6995,
      "step": 1185200
    },
    {
      "epoch": 10.815570479597051,
      "grad_norm": 3.9862046241760254,
      "learning_rate": 4.098702460033579e-05,
      "loss": 0.6819,
      "step": 1185300
    },
    {
      "epoch": 10.816482954960216,
      "grad_norm": 3.9711101055145264,
      "learning_rate": 4.098626420419982e-05,
      "loss": 0.7272,
      "step": 1185400
    },
    {
      "epoch": 10.817395430323382,
      "grad_norm": 4.296701431274414,
      "learning_rate": 4.098550380806385e-05,
      "loss": 0.656,
      "step": 1185500
    },
    {
      "epoch": 10.818307905686547,
      "grad_norm": 2.9324986934661865,
      "learning_rate": 4.0984743411927875e-05,
      "loss": 0.7027,
      "step": 1185600
    },
    {
      "epoch": 10.819220381049712,
      "grad_norm": 4.2023396492004395,
      "learning_rate": 4.098398301579191e-05,
      "loss": 0.6172,
      "step": 1185700
    },
    {
      "epoch": 10.820132856412878,
      "grad_norm": 3.955446481704712,
      "learning_rate": 4.0983222619655936e-05,
      "loss": 0.6628,
      "step": 1185800
    },
    {
      "epoch": 10.821045331776041,
      "grad_norm": 4.184511661529541,
      "learning_rate": 4.0982462223519966e-05,
      "loss": 0.6943,
      "step": 1185900
    },
    {
      "epoch": 10.821957807139206,
      "grad_norm": 3.875340461730957,
      "learning_rate": 4.0981701827383996e-05,
      "loss": 0.6815,
      "step": 1186000
    },
    {
      "epoch": 10.822870282502372,
      "grad_norm": 4.495632171630859,
      "learning_rate": 4.0980941431248026e-05,
      "loss": 0.6784,
      "step": 1186100
    },
    {
      "epoch": 10.823782757865537,
      "grad_norm": 4.455993175506592,
      "learning_rate": 4.0980181035112056e-05,
      "loss": 0.6752,
      "step": 1186200
    },
    {
      "epoch": 10.824695233228702,
      "grad_norm": 4.045356273651123,
      "learning_rate": 4.0979420638976086e-05,
      "loss": 0.668,
      "step": 1186300
    },
    {
      "epoch": 10.825607708591868,
      "grad_norm": 3.6962289810180664,
      "learning_rate": 4.097866024284011e-05,
      "loss": 0.682,
      "step": 1186400
    },
    {
      "epoch": 10.826520183955033,
      "grad_norm": 4.444454193115234,
      "learning_rate": 4.0977899846704146e-05,
      "loss": 0.6591,
      "step": 1186500
    },
    {
      "epoch": 10.827432659318198,
      "grad_norm": 3.516669988632202,
      "learning_rate": 4.097713945056817e-05,
      "loss": 0.699,
      "step": 1186600
    },
    {
      "epoch": 10.828345134681364,
      "grad_norm": 4.048022270202637,
      "learning_rate": 4.09763790544322e-05,
      "loss": 0.7066,
      "step": 1186700
    },
    {
      "epoch": 10.829257610044529,
      "grad_norm": 4.255631446838379,
      "learning_rate": 4.097561865829623e-05,
      "loss": 0.6916,
      "step": 1186800
    },
    {
      "epoch": 10.830170085407694,
      "grad_norm": 3.462578058242798,
      "learning_rate": 4.097485826216026e-05,
      "loss": 0.712,
      "step": 1186900
    },
    {
      "epoch": 10.83108256077086,
      "grad_norm": 4.273218631744385,
      "learning_rate": 4.097409786602428e-05,
      "loss": 0.6679,
      "step": 1187000
    },
    {
      "epoch": 10.831995036134025,
      "grad_norm": 4.108990669250488,
      "learning_rate": 4.097333746988832e-05,
      "loss": 0.6812,
      "step": 1187100
    },
    {
      "epoch": 10.83290751149719,
      "grad_norm": 4.165452003479004,
      "learning_rate": 4.097257707375234e-05,
      "loss": 0.6999,
      "step": 1187200
    },
    {
      "epoch": 10.833819986860355,
      "grad_norm": 3.8416860103607178,
      "learning_rate": 4.097181667761637e-05,
      "loss": 0.6527,
      "step": 1187300
    },
    {
      "epoch": 10.83473246222352,
      "grad_norm": 4.151638984680176,
      "learning_rate": 4.09710562814804e-05,
      "loss": 0.691,
      "step": 1187400
    },
    {
      "epoch": 10.835644937586686,
      "grad_norm": 3.4465396404266357,
      "learning_rate": 4.0970295885344426e-05,
      "loss": 0.6733,
      "step": 1187500
    },
    {
      "epoch": 10.836557412949851,
      "grad_norm": 2.7921273708343506,
      "learning_rate": 4.096953548920846e-05,
      "loss": 0.6547,
      "step": 1187600
    },
    {
      "epoch": 10.837469888313015,
      "grad_norm": 3.5173964500427246,
      "learning_rate": 4.0968775093072487e-05,
      "loss": 0.6905,
      "step": 1187700
    },
    {
      "epoch": 10.83838236367618,
      "grad_norm": 4.039196968078613,
      "learning_rate": 4.0968014696936517e-05,
      "loss": 0.6785,
      "step": 1187800
    },
    {
      "epoch": 10.839294839039345,
      "grad_norm": 3.9688446521759033,
      "learning_rate": 4.096725430080055e-05,
      "loss": 0.7122,
      "step": 1187900
    },
    {
      "epoch": 10.84020731440251,
      "grad_norm": 3.5155153274536133,
      "learning_rate": 4.096649390466458e-05,
      "loss": 0.6712,
      "step": 1188000
    },
    {
      "epoch": 10.841119789765676,
      "grad_norm": 4.276275157928467,
      "learning_rate": 4.09657335085286e-05,
      "loss": 0.7436,
      "step": 1188100
    },
    {
      "epoch": 10.842032265128841,
      "grad_norm": 3.8947839736938477,
      "learning_rate": 4.096497311239264e-05,
      "loss": 0.6523,
      "step": 1188200
    },
    {
      "epoch": 10.842944740492007,
      "grad_norm": 4.213495254516602,
      "learning_rate": 4.096421271625666e-05,
      "loss": 0.7432,
      "step": 1188300
    },
    {
      "epoch": 10.843857215855172,
      "grad_norm": 4.191660404205322,
      "learning_rate": 4.096345232012069e-05,
      "loss": 0.69,
      "step": 1188400
    },
    {
      "epoch": 10.844769691218337,
      "grad_norm": 4.6692094802856445,
      "learning_rate": 4.096269192398472e-05,
      "loss": 0.6972,
      "step": 1188500
    },
    {
      "epoch": 10.845682166581502,
      "grad_norm": 3.9817185401916504,
      "learning_rate": 4.096193152784875e-05,
      "loss": 0.7011,
      "step": 1188600
    },
    {
      "epoch": 10.846594641944668,
      "grad_norm": 4.118303298950195,
      "learning_rate": 4.096117113171278e-05,
      "loss": 0.6523,
      "step": 1188700
    },
    {
      "epoch": 10.847507117307833,
      "grad_norm": 3.546231269836426,
      "learning_rate": 4.096041073557681e-05,
      "loss": 0.6805,
      "step": 1188800
    },
    {
      "epoch": 10.848419592670998,
      "grad_norm": 3.8017690181732178,
      "learning_rate": 4.0959650339440834e-05,
      "loss": 0.7185,
      "step": 1188900
    },
    {
      "epoch": 10.849332068034164,
      "grad_norm": 4.384207725524902,
      "learning_rate": 4.095888994330487e-05,
      "loss": 0.7474,
      "step": 1189000
    },
    {
      "epoch": 10.850244543397329,
      "grad_norm": 3.237130880355835,
      "learning_rate": 4.0958129547168894e-05,
      "loss": 0.6825,
      "step": 1189100
    },
    {
      "epoch": 10.851157018760494,
      "grad_norm": 2.917403221130371,
      "learning_rate": 4.0957369151032924e-05,
      "loss": 0.6607,
      "step": 1189200
    },
    {
      "epoch": 10.852069494123658,
      "grad_norm": 3.9273312091827393,
      "learning_rate": 4.0956608754896954e-05,
      "loss": 0.666,
      "step": 1189300
    },
    {
      "epoch": 10.852981969486823,
      "grad_norm": 4.588377475738525,
      "learning_rate": 4.0955848358760984e-05,
      "loss": 0.6847,
      "step": 1189400
    },
    {
      "epoch": 10.853894444849988,
      "grad_norm": 4.172065258026123,
      "learning_rate": 4.095508796262501e-05,
      "loss": 0.6873,
      "step": 1189500
    },
    {
      "epoch": 10.854806920213154,
      "grad_norm": 2.8364360332489014,
      "learning_rate": 4.0954327566489044e-05,
      "loss": 0.7203,
      "step": 1189600
    },
    {
      "epoch": 10.855719395576319,
      "grad_norm": 3.3600361347198486,
      "learning_rate": 4.095356717035307e-05,
      "loss": 0.6733,
      "step": 1189700
    },
    {
      "epoch": 10.856631870939484,
      "grad_norm": 4.178929805755615,
      "learning_rate": 4.09528067742171e-05,
      "loss": 0.6854,
      "step": 1189800
    },
    {
      "epoch": 10.85754434630265,
      "grad_norm": 4.594264030456543,
      "learning_rate": 4.095204637808113e-05,
      "loss": 0.633,
      "step": 1189900
    },
    {
      "epoch": 10.858456821665815,
      "grad_norm": 5.405091762542725,
      "learning_rate": 4.095128598194516e-05,
      "loss": 0.6547,
      "step": 1190000
    },
    {
      "epoch": 10.85936929702898,
      "grad_norm": 3.648953676223755,
      "learning_rate": 4.095052558580919e-05,
      "loss": 0.6605,
      "step": 1190100
    },
    {
      "epoch": 10.860281772392145,
      "grad_norm": 4.212681293487549,
      "learning_rate": 4.094976518967321e-05,
      "loss": 0.7041,
      "step": 1190200
    },
    {
      "epoch": 10.86119424775531,
      "grad_norm": 4.602566242218018,
      "learning_rate": 4.094900479353724e-05,
      "loss": 0.6828,
      "step": 1190300
    },
    {
      "epoch": 10.862106723118476,
      "grad_norm": 3.7865149974823,
      "learning_rate": 4.094824439740127e-05,
      "loss": 0.6582,
      "step": 1190400
    },
    {
      "epoch": 10.863019198481641,
      "grad_norm": 3.5273592472076416,
      "learning_rate": 4.09474840012653e-05,
      "loss": 0.7073,
      "step": 1190500
    },
    {
      "epoch": 10.863931673844807,
      "grad_norm": 4.385142803192139,
      "learning_rate": 4.0946723605129325e-05,
      "loss": 0.7132,
      "step": 1190600
    },
    {
      "epoch": 10.864844149207972,
      "grad_norm": 3.4123482704162598,
      "learning_rate": 4.094596320899336e-05,
      "loss": 0.668,
      "step": 1190700
    },
    {
      "epoch": 10.865756624571137,
      "grad_norm": 4.540480613708496,
      "learning_rate": 4.0945202812857385e-05,
      "loss": 0.6862,
      "step": 1190800
    },
    {
      "epoch": 10.866669099934303,
      "grad_norm": 3.937950611114502,
      "learning_rate": 4.0944442416721415e-05,
      "loss": 0.6651,
      "step": 1190900
    },
    {
      "epoch": 10.867581575297468,
      "grad_norm": 4.744482517242432,
      "learning_rate": 4.0943682020585445e-05,
      "loss": 0.668,
      "step": 1191000
    },
    {
      "epoch": 10.868494050660631,
      "grad_norm": 3.7469654083251953,
      "learning_rate": 4.0942921624449475e-05,
      "loss": 0.6536,
      "step": 1191100
    },
    {
      "epoch": 10.869406526023797,
      "grad_norm": 4.465605735778809,
      "learning_rate": 4.0942161228313505e-05,
      "loss": 0.6877,
      "step": 1191200
    },
    {
      "epoch": 10.870319001386962,
      "grad_norm": 4.572030544281006,
      "learning_rate": 4.0941400832177535e-05,
      "loss": 0.6728,
      "step": 1191300
    },
    {
      "epoch": 10.871231476750127,
      "grad_norm": 3.695134401321411,
      "learning_rate": 4.094064043604156e-05,
      "loss": 0.7034,
      "step": 1191400
    },
    {
      "epoch": 10.872143952113293,
      "grad_norm": 4.449658393859863,
      "learning_rate": 4.0939880039905595e-05,
      "loss": 0.6691,
      "step": 1191500
    },
    {
      "epoch": 10.873056427476458,
      "grad_norm": 4.447728157043457,
      "learning_rate": 4.093911964376962e-05,
      "loss": 0.6704,
      "step": 1191600
    },
    {
      "epoch": 10.873968902839623,
      "grad_norm": 3.667295217514038,
      "learning_rate": 4.093835924763365e-05,
      "loss": 0.6767,
      "step": 1191700
    },
    {
      "epoch": 10.874881378202788,
      "grad_norm": 3.5464537143707275,
      "learning_rate": 4.093759885149768e-05,
      "loss": 0.6612,
      "step": 1191800
    },
    {
      "epoch": 10.875793853565954,
      "grad_norm": 3.810842514038086,
      "learning_rate": 4.093683845536171e-05,
      "loss": 0.6492,
      "step": 1191900
    },
    {
      "epoch": 10.876706328929119,
      "grad_norm": 4.536295413970947,
      "learning_rate": 4.093607805922573e-05,
      "loss": 0.6831,
      "step": 1192000
    },
    {
      "epoch": 10.877618804292284,
      "grad_norm": 5.129048824310303,
      "learning_rate": 4.093531766308977e-05,
      "loss": 0.6974,
      "step": 1192100
    },
    {
      "epoch": 10.87853127965545,
      "grad_norm": 3.8330280780792236,
      "learning_rate": 4.093455726695379e-05,
      "loss": 0.6544,
      "step": 1192200
    },
    {
      "epoch": 10.879443755018615,
      "grad_norm": 3.8404574394226074,
      "learning_rate": 4.093379687081782e-05,
      "loss": 0.6658,
      "step": 1192300
    },
    {
      "epoch": 10.88035623038178,
      "grad_norm": 3.7692947387695312,
      "learning_rate": 4.093303647468185e-05,
      "loss": 0.6997,
      "step": 1192400
    },
    {
      "epoch": 10.881268705744946,
      "grad_norm": 4.106546401977539,
      "learning_rate": 4.093227607854588e-05,
      "loss": 0.6795,
      "step": 1192500
    },
    {
      "epoch": 10.88218118110811,
      "grad_norm": 4.167539119720459,
      "learning_rate": 4.093151568240991e-05,
      "loss": 0.6638,
      "step": 1192600
    },
    {
      "epoch": 10.883093656471274,
      "grad_norm": 4.361701488494873,
      "learning_rate": 4.093075528627394e-05,
      "loss": 0.6686,
      "step": 1192700
    },
    {
      "epoch": 10.88400613183444,
      "grad_norm": 3.7117984294891357,
      "learning_rate": 4.0929994890137966e-05,
      "loss": 0.6665,
      "step": 1192800
    },
    {
      "epoch": 10.884918607197605,
      "grad_norm": 3.4046854972839355,
      "learning_rate": 4.0929234494001996e-05,
      "loss": 0.7142,
      "step": 1192900
    },
    {
      "epoch": 10.88583108256077,
      "grad_norm": 4.393290996551514,
      "learning_rate": 4.0928474097866026e-05,
      "loss": 0.7048,
      "step": 1193000
    },
    {
      "epoch": 10.886743557923936,
      "grad_norm": 4.340880393981934,
      "learning_rate": 4.0927713701730056e-05,
      "loss": 0.6669,
      "step": 1193100
    },
    {
      "epoch": 10.8876560332871,
      "grad_norm": 4.832170009613037,
      "learning_rate": 4.0926953305594086e-05,
      "loss": 0.679,
      "step": 1193200
    },
    {
      "epoch": 10.888568508650266,
      "grad_norm": 4.318721771240234,
      "learning_rate": 4.092619290945811e-05,
      "loss": 0.7095,
      "step": 1193300
    },
    {
      "epoch": 10.889480984013431,
      "grad_norm": 4.101838111877441,
      "learning_rate": 4.0925432513322146e-05,
      "loss": 0.6826,
      "step": 1193400
    },
    {
      "epoch": 10.890393459376597,
      "grad_norm": 3.821138858795166,
      "learning_rate": 4.092467211718617e-05,
      "loss": 0.7083,
      "step": 1193500
    },
    {
      "epoch": 10.891305934739762,
      "grad_norm": 3.003925323486328,
      "learning_rate": 4.09239117210502e-05,
      "loss": 0.7058,
      "step": 1193600
    },
    {
      "epoch": 10.892218410102927,
      "grad_norm": 3.847038984298706,
      "learning_rate": 4.092315132491423e-05,
      "loss": 0.6718,
      "step": 1193700
    },
    {
      "epoch": 10.893130885466093,
      "grad_norm": 5.603407859802246,
      "learning_rate": 4.092239092877826e-05,
      "loss": 0.6291,
      "step": 1193800
    },
    {
      "epoch": 10.894043360829258,
      "grad_norm": 3.32938289642334,
      "learning_rate": 4.092163053264228e-05,
      "loss": 0.7263,
      "step": 1193900
    },
    {
      "epoch": 10.894955836192423,
      "grad_norm": 3.651963472366333,
      "learning_rate": 4.092087013650632e-05,
      "loss": 0.684,
      "step": 1194000
    },
    {
      "epoch": 10.895868311555589,
      "grad_norm": 4.029777526855469,
      "learning_rate": 4.092010974037034e-05,
      "loss": 0.6766,
      "step": 1194100
    },
    {
      "epoch": 10.896780786918754,
      "grad_norm": 3.637314558029175,
      "learning_rate": 4.091934934423437e-05,
      "loss": 0.6626,
      "step": 1194200
    },
    {
      "epoch": 10.89769326228192,
      "grad_norm": 4.653385639190674,
      "learning_rate": 4.09185889480984e-05,
      "loss": 0.733,
      "step": 1194300
    },
    {
      "epoch": 10.898605737645084,
      "grad_norm": 4.290853977203369,
      "learning_rate": 4.091782855196243e-05,
      "loss": 0.6947,
      "step": 1194400
    },
    {
      "epoch": 10.899518213008248,
      "grad_norm": 3.814967155456543,
      "learning_rate": 4.091706815582646e-05,
      "loss": 0.6632,
      "step": 1194500
    },
    {
      "epoch": 10.900430688371413,
      "grad_norm": 4.280088424682617,
      "learning_rate": 4.0916307759690493e-05,
      "loss": 0.6898,
      "step": 1194600
    },
    {
      "epoch": 10.901343163734579,
      "grad_norm": 3.9216816425323486,
      "learning_rate": 4.091554736355452e-05,
      "loss": 0.6894,
      "step": 1194700
    },
    {
      "epoch": 10.902255639097744,
      "grad_norm": 3.3955092430114746,
      "learning_rate": 4.0914786967418554e-05,
      "loss": 0.6991,
      "step": 1194800
    },
    {
      "epoch": 10.90316811446091,
      "grad_norm": 4.128474712371826,
      "learning_rate": 4.091402657128258e-05,
      "loss": 0.6917,
      "step": 1194900
    },
    {
      "epoch": 10.904080589824074,
      "grad_norm": 2.8009185791015625,
      "learning_rate": 4.091326617514661e-05,
      "loss": 0.6537,
      "step": 1195000
    },
    {
      "epoch": 10.90499306518724,
      "grad_norm": 4.161826133728027,
      "learning_rate": 4.091250577901064e-05,
      "loss": 0.6749,
      "step": 1195100
    },
    {
      "epoch": 10.905905540550405,
      "grad_norm": 4.004899978637695,
      "learning_rate": 4.091174538287467e-05,
      "loss": 0.6803,
      "step": 1195200
    },
    {
      "epoch": 10.90681801591357,
      "grad_norm": 3.7803008556365967,
      "learning_rate": 4.091098498673869e-05,
      "loss": 0.6867,
      "step": 1195300
    },
    {
      "epoch": 10.907730491276736,
      "grad_norm": 4.767342567443848,
      "learning_rate": 4.091022459060273e-05,
      "loss": 0.6602,
      "step": 1195400
    },
    {
      "epoch": 10.908642966639901,
      "grad_norm": 4.240087985992432,
      "learning_rate": 4.090946419446675e-05,
      "loss": 0.7027,
      "step": 1195500
    },
    {
      "epoch": 10.909555442003066,
      "grad_norm": 4.699371814727783,
      "learning_rate": 4.090870379833078e-05,
      "loss": 0.6749,
      "step": 1195600
    },
    {
      "epoch": 10.910467917366232,
      "grad_norm": 4.826124668121338,
      "learning_rate": 4.090794340219481e-05,
      "loss": 0.6781,
      "step": 1195700
    },
    {
      "epoch": 10.911380392729397,
      "grad_norm": 3.7862389087677,
      "learning_rate": 4.0907183006058834e-05,
      "loss": 0.6557,
      "step": 1195800
    },
    {
      "epoch": 10.912292868092562,
      "grad_norm": 2.9689433574676514,
      "learning_rate": 4.090642260992287e-05,
      "loss": 0.6906,
      "step": 1195900
    },
    {
      "epoch": 10.913205343455727,
      "grad_norm": 4.2420125007629395,
      "learning_rate": 4.0905662213786894e-05,
      "loss": 0.6559,
      "step": 1196000
    },
    {
      "epoch": 10.914117818818891,
      "grad_norm": 3.676323652267456,
      "learning_rate": 4.0904901817650924e-05,
      "loss": 0.6724,
      "step": 1196100
    },
    {
      "epoch": 10.915030294182056,
      "grad_norm": 3.507910966873169,
      "learning_rate": 4.0904141421514954e-05,
      "loss": 0.6721,
      "step": 1196200
    },
    {
      "epoch": 10.915942769545222,
      "grad_norm": 3.28244686126709,
      "learning_rate": 4.0903381025378984e-05,
      "loss": 0.6965,
      "step": 1196300
    },
    {
      "epoch": 10.916855244908387,
      "grad_norm": 3.8276925086975098,
      "learning_rate": 4.090262062924301e-05,
      "loss": 0.6724,
      "step": 1196400
    },
    {
      "epoch": 10.917767720271552,
      "grad_norm": 3.509425640106201,
      "learning_rate": 4.0901860233107044e-05,
      "loss": 0.6873,
      "step": 1196500
    },
    {
      "epoch": 10.918680195634717,
      "grad_norm": 2.804708480834961,
      "learning_rate": 4.090109983697107e-05,
      "loss": 0.6373,
      "step": 1196600
    },
    {
      "epoch": 10.919592670997883,
      "grad_norm": 4.623965263366699,
      "learning_rate": 4.09003394408351e-05,
      "loss": 0.689,
      "step": 1196700
    },
    {
      "epoch": 10.920505146361048,
      "grad_norm": 4.064855098724365,
      "learning_rate": 4.089957904469913e-05,
      "loss": 0.7146,
      "step": 1196800
    },
    {
      "epoch": 10.921417621724213,
      "grad_norm": 3.596831798553467,
      "learning_rate": 4.089881864856316e-05,
      "loss": 0.6821,
      "step": 1196900
    },
    {
      "epoch": 10.922330097087379,
      "grad_norm": 3.6718785762786865,
      "learning_rate": 4.089805825242719e-05,
      "loss": 0.6852,
      "step": 1197000
    },
    {
      "epoch": 10.923242572450544,
      "grad_norm": 3.8216333389282227,
      "learning_rate": 4.089729785629122e-05,
      "loss": 0.6304,
      "step": 1197100
    },
    {
      "epoch": 10.92415504781371,
      "grad_norm": 3.4626801013946533,
      "learning_rate": 4.089653746015524e-05,
      "loss": 0.7015,
      "step": 1197200
    },
    {
      "epoch": 10.925067523176875,
      "grad_norm": 3.319082260131836,
      "learning_rate": 4.089577706401928e-05,
      "loss": 0.6773,
      "step": 1197300
    },
    {
      "epoch": 10.92597999854004,
      "grad_norm": 3.3782637119293213,
      "learning_rate": 4.08950166678833e-05,
      "loss": 0.6921,
      "step": 1197400
    },
    {
      "epoch": 10.926892473903205,
      "grad_norm": 4.292566776275635,
      "learning_rate": 4.089425627174733e-05,
      "loss": 0.6906,
      "step": 1197500
    },
    {
      "epoch": 10.92780494926637,
      "grad_norm": 4.500789642333984,
      "learning_rate": 4.089349587561136e-05,
      "loss": 0.6923,
      "step": 1197600
    },
    {
      "epoch": 10.928717424629536,
      "grad_norm": 2.803957462310791,
      "learning_rate": 4.089273547947539e-05,
      "loss": 0.6917,
      "step": 1197700
    },
    {
      "epoch": 10.929629899992701,
      "grad_norm": 3.945859432220459,
      "learning_rate": 4.0891975083339415e-05,
      "loss": 0.6682,
      "step": 1197800
    },
    {
      "epoch": 10.930542375355865,
      "grad_norm": 4.015083312988281,
      "learning_rate": 4.089121468720345e-05,
      "loss": 0.662,
      "step": 1197900
    },
    {
      "epoch": 10.93145485071903,
      "grad_norm": 4.080441951751709,
      "learning_rate": 4.0890454291067475e-05,
      "loss": 0.6497,
      "step": 1198000
    },
    {
      "epoch": 10.932367326082195,
      "grad_norm": 3.7225027084350586,
      "learning_rate": 4.0889693894931505e-05,
      "loss": 0.686,
      "step": 1198100
    },
    {
      "epoch": 10.93327980144536,
      "grad_norm": 3.8556950092315674,
      "learning_rate": 4.0888933498795535e-05,
      "loss": 0.6833,
      "step": 1198200
    },
    {
      "epoch": 10.934192276808526,
      "grad_norm": 2.2315473556518555,
      "learning_rate": 4.0888173102659565e-05,
      "loss": 0.6922,
      "step": 1198300
    },
    {
      "epoch": 10.935104752171691,
      "grad_norm": 3.992647647857666,
      "learning_rate": 4.0887412706523595e-05,
      "loss": 0.6678,
      "step": 1198400
    },
    {
      "epoch": 10.936017227534856,
      "grad_norm": 3.6642708778381348,
      "learning_rate": 4.0886652310387625e-05,
      "loss": 0.6926,
      "step": 1198500
    },
    {
      "epoch": 10.936929702898022,
      "grad_norm": 4.205406188964844,
      "learning_rate": 4.088589191425165e-05,
      "loss": 0.7186,
      "step": 1198600
    },
    {
      "epoch": 10.937842178261187,
      "grad_norm": 4.0556230545043945,
      "learning_rate": 4.088513151811568e-05,
      "loss": 0.7038,
      "step": 1198700
    },
    {
      "epoch": 10.938754653624352,
      "grad_norm": 4.014504432678223,
      "learning_rate": 4.088437112197971e-05,
      "loss": 0.7164,
      "step": 1198800
    },
    {
      "epoch": 10.939667128987518,
      "grad_norm": 3.8621673583984375,
      "learning_rate": 4.088361072584373e-05,
      "loss": 0.6485,
      "step": 1198900
    },
    {
      "epoch": 10.940579604350683,
      "grad_norm": 4.63020133972168,
      "learning_rate": 4.088285032970777e-05,
      "loss": 0.7071,
      "step": 1199000
    },
    {
      "epoch": 10.941492079713848,
      "grad_norm": 4.134640693664551,
      "learning_rate": 4.088208993357179e-05,
      "loss": 0.6906,
      "step": 1199100
    },
    {
      "epoch": 10.942404555077013,
      "grad_norm": 3.8909690380096436,
      "learning_rate": 4.088132953743582e-05,
      "loss": 0.7242,
      "step": 1199200
    },
    {
      "epoch": 10.943317030440179,
      "grad_norm": 3.7834062576293945,
      "learning_rate": 4.088056914129985e-05,
      "loss": 0.6634,
      "step": 1199300
    },
    {
      "epoch": 10.944229505803344,
      "grad_norm": 4.65818977355957,
      "learning_rate": 4.087980874516388e-05,
      "loss": 0.6655,
      "step": 1199400
    },
    {
      "epoch": 10.945141981166508,
      "grad_norm": 3.864790678024292,
      "learning_rate": 4.087904834902791e-05,
      "loss": 0.6762,
      "step": 1199500
    },
    {
      "epoch": 10.946054456529673,
      "grad_norm": 5.155157566070557,
      "learning_rate": 4.087828795289194e-05,
      "loss": 0.6735,
      "step": 1199600
    },
    {
      "epoch": 10.946966931892838,
      "grad_norm": 3.3857429027557373,
      "learning_rate": 4.0877527556755966e-05,
      "loss": 0.6705,
      "step": 1199700
    },
    {
      "epoch": 10.947879407256004,
      "grad_norm": 4.070910453796387,
      "learning_rate": 4.087676716062e-05,
      "loss": 0.7454,
      "step": 1199800
    },
    {
      "epoch": 10.948791882619169,
      "grad_norm": 4.143996715545654,
      "learning_rate": 4.0876006764484026e-05,
      "loss": 0.6654,
      "step": 1199900
    },
    {
      "epoch": 10.949704357982334,
      "grad_norm": 4.099580764770508,
      "learning_rate": 4.0875246368348056e-05,
      "loss": 0.6876,
      "step": 1200000
    },
    {
      "epoch": 10.9506168333455,
      "grad_norm": 4.59835958480835,
      "learning_rate": 4.0874485972212086e-05,
      "loss": 0.6942,
      "step": 1200100
    },
    {
      "epoch": 10.951529308708665,
      "grad_norm": 3.5373010635375977,
      "learning_rate": 4.0873725576076116e-05,
      "loss": 0.6305,
      "step": 1200200
    },
    {
      "epoch": 10.95244178407183,
      "grad_norm": 4.310494899749756,
      "learning_rate": 4.087296517994014e-05,
      "loss": 0.7169,
      "step": 1200300
    },
    {
      "epoch": 10.953354259434995,
      "grad_norm": 4.377114295959473,
      "learning_rate": 4.0872204783804176e-05,
      "loss": 0.6703,
      "step": 1200400
    },
    {
      "epoch": 10.95426673479816,
      "grad_norm": 4.228669166564941,
      "learning_rate": 4.08714443876682e-05,
      "loss": 0.6547,
      "step": 1200500
    },
    {
      "epoch": 10.955179210161326,
      "grad_norm": 3.716495990753174,
      "learning_rate": 4.087068399153223e-05,
      "loss": 0.6769,
      "step": 1200600
    },
    {
      "epoch": 10.956091685524491,
      "grad_norm": 3.808326244354248,
      "learning_rate": 4.086992359539626e-05,
      "loss": 0.7129,
      "step": 1200700
    },
    {
      "epoch": 10.957004160887656,
      "grad_norm": 3.621891736984253,
      "learning_rate": 4.086916319926029e-05,
      "loss": 0.6809,
      "step": 1200800
    },
    {
      "epoch": 10.957916636250822,
      "grad_norm": 2.975729465484619,
      "learning_rate": 4.086840280312432e-05,
      "loss": 0.6621,
      "step": 1200900
    },
    {
      "epoch": 10.958829111613987,
      "grad_norm": 3.9915995597839355,
      "learning_rate": 4.086764240698835e-05,
      "loss": 0.6351,
      "step": 1201000
    },
    {
      "epoch": 10.959741586977152,
      "grad_norm": 4.228712558746338,
      "learning_rate": 4.086688201085237e-05,
      "loss": 0.6705,
      "step": 1201100
    },
    {
      "epoch": 10.960654062340318,
      "grad_norm": 3.6200661659240723,
      "learning_rate": 4.086612161471641e-05,
      "loss": 0.7049,
      "step": 1201200
    },
    {
      "epoch": 10.961566537703481,
      "grad_norm": 4.341183662414551,
      "learning_rate": 4.086536121858043e-05,
      "loss": 0.6719,
      "step": 1201300
    },
    {
      "epoch": 10.962479013066647,
      "grad_norm": 3.318155288696289,
      "learning_rate": 4.086460082244446e-05,
      "loss": 0.6842,
      "step": 1201400
    },
    {
      "epoch": 10.963391488429812,
      "grad_norm": 3.952428102493286,
      "learning_rate": 4.0863840426308494e-05,
      "loss": 0.6574,
      "step": 1201500
    },
    {
      "epoch": 10.964303963792977,
      "grad_norm": 3.9440572261810303,
      "learning_rate": 4.086308003017252e-05,
      "loss": 0.7043,
      "step": 1201600
    },
    {
      "epoch": 10.965216439156142,
      "grad_norm": 3.967111349105835,
      "learning_rate": 4.086231963403655e-05,
      "loss": 0.6688,
      "step": 1201700
    },
    {
      "epoch": 10.966128914519308,
      "grad_norm": 2.3389830589294434,
      "learning_rate": 4.086155923790058e-05,
      "loss": 0.7074,
      "step": 1201800
    },
    {
      "epoch": 10.967041389882473,
      "grad_norm": 3.96254825592041,
      "learning_rate": 4.086079884176461e-05,
      "loss": 0.6995,
      "step": 1201900
    },
    {
      "epoch": 10.967953865245638,
      "grad_norm": 4.497581481933594,
      "learning_rate": 4.086003844562864e-05,
      "loss": 0.6783,
      "step": 1202000
    },
    {
      "epoch": 10.968866340608804,
      "grad_norm": 4.127213954925537,
      "learning_rate": 4.085927804949267e-05,
      "loss": 0.6805,
      "step": 1202100
    },
    {
      "epoch": 10.969778815971969,
      "grad_norm": 3.3545219898223877,
      "learning_rate": 4.085851765335669e-05,
      "loss": 0.6907,
      "step": 1202200
    },
    {
      "epoch": 10.970691291335134,
      "grad_norm": 3.57071852684021,
      "learning_rate": 4.085775725722073e-05,
      "loss": 0.6779,
      "step": 1202300
    },
    {
      "epoch": 10.9716037666983,
      "grad_norm": 2.6106741428375244,
      "learning_rate": 4.085699686108475e-05,
      "loss": 0.6532,
      "step": 1202400
    },
    {
      "epoch": 10.972516242061465,
      "grad_norm": 4.02244758605957,
      "learning_rate": 4.085623646494878e-05,
      "loss": 0.6925,
      "step": 1202500
    },
    {
      "epoch": 10.97342871742463,
      "grad_norm": 3.0502078533172607,
      "learning_rate": 4.085547606881281e-05,
      "loss": 0.6753,
      "step": 1202600
    },
    {
      "epoch": 10.974341192787795,
      "grad_norm": 3.269728183746338,
      "learning_rate": 4.085471567267684e-05,
      "loss": 0.6627,
      "step": 1202700
    },
    {
      "epoch": 10.97525366815096,
      "grad_norm": 3.8690500259399414,
      "learning_rate": 4.0853955276540864e-05,
      "loss": 0.701,
      "step": 1202800
    },
    {
      "epoch": 10.976166143514124,
      "grad_norm": 3.4846932888031006,
      "learning_rate": 4.08531948804049e-05,
      "loss": 0.6636,
      "step": 1202900
    },
    {
      "epoch": 10.97707861887729,
      "grad_norm": 4.375591278076172,
      "learning_rate": 4.0852434484268924e-05,
      "loss": 0.6796,
      "step": 1203000
    },
    {
      "epoch": 10.977991094240455,
      "grad_norm": 4.347261428833008,
      "learning_rate": 4.0851674088132954e-05,
      "loss": 0.6477,
      "step": 1203100
    },
    {
      "epoch": 10.97890356960362,
      "grad_norm": 2.5424275398254395,
      "learning_rate": 4.0850913691996984e-05,
      "loss": 0.6583,
      "step": 1203200
    },
    {
      "epoch": 10.979816044966785,
      "grad_norm": 4.632731914520264,
      "learning_rate": 4.0850153295861014e-05,
      "loss": 0.681,
      "step": 1203300
    },
    {
      "epoch": 10.98072852032995,
      "grad_norm": 3.4692440032958984,
      "learning_rate": 4.0849392899725044e-05,
      "loss": 0.7178,
      "step": 1203400
    },
    {
      "epoch": 10.981640995693116,
      "grad_norm": 4.3382887840271,
      "learning_rate": 4.0848632503589075e-05,
      "loss": 0.6804,
      "step": 1203500
    },
    {
      "epoch": 10.982553471056281,
      "grad_norm": 3.7015960216522217,
      "learning_rate": 4.08478721074531e-05,
      "loss": 0.6577,
      "step": 1203600
    },
    {
      "epoch": 10.983465946419447,
      "grad_norm": 4.8368353843688965,
      "learning_rate": 4.0847111711317135e-05,
      "loss": 0.6742,
      "step": 1203700
    },
    {
      "epoch": 10.984378421782612,
      "grad_norm": 4.648566722869873,
      "learning_rate": 4.084635131518116e-05,
      "loss": 0.6489,
      "step": 1203800
    },
    {
      "epoch": 10.985290897145777,
      "grad_norm": 3.920438289642334,
      "learning_rate": 4.084559091904519e-05,
      "loss": 0.6683,
      "step": 1203900
    },
    {
      "epoch": 10.986203372508943,
      "grad_norm": 1.767621636390686,
      "learning_rate": 4.084483052290922e-05,
      "loss": 0.6789,
      "step": 1204000
    },
    {
      "epoch": 10.987115847872108,
      "grad_norm": 4.42210054397583,
      "learning_rate": 4.084407012677325e-05,
      "loss": 0.6548,
      "step": 1204100
    },
    {
      "epoch": 10.988028323235273,
      "grad_norm": 4.945591449737549,
      "learning_rate": 4.084330973063727e-05,
      "loss": 0.7143,
      "step": 1204200
    },
    {
      "epoch": 10.988940798598438,
      "grad_norm": 4.27908992767334,
      "learning_rate": 4.08425493345013e-05,
      "loss": 0.6629,
      "step": 1204300
    },
    {
      "epoch": 10.989853273961604,
      "grad_norm": 4.453904628753662,
      "learning_rate": 4.084178893836533e-05,
      "loss": 0.6742,
      "step": 1204400
    },
    {
      "epoch": 10.990765749324769,
      "grad_norm": 4.100666046142578,
      "learning_rate": 4.084102854222936e-05,
      "loss": 0.6736,
      "step": 1204500
    },
    {
      "epoch": 10.991678224687934,
      "grad_norm": 3.1846015453338623,
      "learning_rate": 4.084026814609339e-05,
      "loss": 0.6678,
      "step": 1204600
    },
    {
      "epoch": 10.992590700051098,
      "grad_norm": 2.967144012451172,
      "learning_rate": 4.0839507749957415e-05,
      "loss": 0.6735,
      "step": 1204700
    },
    {
      "epoch": 10.993503175414263,
      "grad_norm": 3.3016879558563232,
      "learning_rate": 4.083874735382145e-05,
      "loss": 0.6686,
      "step": 1204800
    },
    {
      "epoch": 10.994415650777428,
      "grad_norm": 3.880164861679077,
      "learning_rate": 4.0837986957685475e-05,
      "loss": 0.7235,
      "step": 1204900
    },
    {
      "epoch": 10.995328126140594,
      "grad_norm": 5.6422224044799805,
      "learning_rate": 4.0837226561549505e-05,
      "loss": 0.7133,
      "step": 1205000
    },
    {
      "epoch": 10.996240601503759,
      "grad_norm": 3.4211456775665283,
      "learning_rate": 4.0836466165413535e-05,
      "loss": 0.7108,
      "step": 1205100
    },
    {
      "epoch": 10.997153076866924,
      "grad_norm": 3.895325183868408,
      "learning_rate": 4.0835705769277565e-05,
      "loss": 0.6587,
      "step": 1205200
    },
    {
      "epoch": 10.99806555223009,
      "grad_norm": 3.1490237712860107,
      "learning_rate": 4.0834945373141595e-05,
      "loss": 0.6848,
      "step": 1205300
    },
    {
      "epoch": 10.998978027593255,
      "grad_norm": 4.790390491485596,
      "learning_rate": 4.0834184977005625e-05,
      "loss": 0.6797,
      "step": 1205400
    },
    {
      "epoch": 10.99989050295642,
      "grad_norm": 5.052184581756592,
      "learning_rate": 4.083342458086965e-05,
      "loss": 0.6696,
      "step": 1205500
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.5540444850921631,
      "eval_runtime": 25.4464,
      "eval_samples_per_second": 226.712,
      "eval_steps_per_second": 226.712,
      "step": 1205512
    },
    {
      "epoch": 11.0,
      "eval_loss": 0.5335428714752197,
      "eval_runtime": 484.1759,
      "eval_samples_per_second": 226.347,
      "eval_steps_per_second": 226.347,
      "step": 1205512
    },
    {
      "epoch": 11.000802978319586,
      "grad_norm": 4.173129558563232,
      "learning_rate": 4.083266418473368e-05,
      "loss": 0.6696,
      "step": 1205600
    },
    {
      "epoch": 11.00171545368275,
      "grad_norm": 3.181866407394409,
      "learning_rate": 4.083190378859771e-05,
      "loss": 0.6601,
      "step": 1205700
    },
    {
      "epoch": 11.002627929045916,
      "grad_norm": 3.823810338973999,
      "learning_rate": 4.083114339246174e-05,
      "loss": 0.7094,
      "step": 1205800
    },
    {
      "epoch": 11.003540404409081,
      "grad_norm": 4.353285312652588,
      "learning_rate": 4.083038299632577e-05,
      "loss": 0.6537,
      "step": 1205900
    },
    {
      "epoch": 11.004452879772247,
      "grad_norm": 3.3583781719207764,
      "learning_rate": 4.08296226001898e-05,
      "loss": 0.6321,
      "step": 1206000
    },
    {
      "epoch": 11.005365355135412,
      "grad_norm": 4.5215044021606445,
      "learning_rate": 4.082886220405382e-05,
      "loss": 0.6532,
      "step": 1206100
    },
    {
      "epoch": 11.006277830498577,
      "grad_norm": 3.8279359340667725,
      "learning_rate": 4.082810180791786e-05,
      "loss": 0.6488,
      "step": 1206200
    },
    {
      "epoch": 11.00719030586174,
      "grad_norm": 4.141509056091309,
      "learning_rate": 4.082734141178188e-05,
      "loss": 0.6666,
      "step": 1206300
    },
    {
      "epoch": 11.008102781224906,
      "grad_norm": 3.8382010459899902,
      "learning_rate": 4.082658101564591e-05,
      "loss": 0.7091,
      "step": 1206400
    },
    {
      "epoch": 11.009015256588071,
      "grad_norm": 4.3014092445373535,
      "learning_rate": 4.082582061950994e-05,
      "loss": 0.7034,
      "step": 1206500
    },
    {
      "epoch": 11.009927731951237,
      "grad_norm": 3.707764148712158,
      "learning_rate": 4.082506022337397e-05,
      "loss": 0.6849,
      "step": 1206600
    },
    {
      "epoch": 11.010840207314402,
      "grad_norm": 4.328072547912598,
      "learning_rate": 4.0824299827238e-05,
      "loss": 0.7,
      "step": 1206700
    },
    {
      "epoch": 11.011752682677567,
      "grad_norm": 4.446226119995117,
      "learning_rate": 4.082353943110203e-05,
      "loss": 0.6688,
      "step": 1206800
    },
    {
      "epoch": 11.012665158040733,
      "grad_norm": 3.5333845615386963,
      "learning_rate": 4.0822779034966056e-05,
      "loss": 0.6792,
      "step": 1206900
    },
    {
      "epoch": 11.013577633403898,
      "grad_norm": 4.585615158081055,
      "learning_rate": 4.0822018638830086e-05,
      "loss": 0.6233,
      "step": 1207000
    },
    {
      "epoch": 11.014490108767063,
      "grad_norm": 3.787630319595337,
      "learning_rate": 4.0821258242694116e-05,
      "loss": 0.6963,
      "step": 1207100
    },
    {
      "epoch": 11.015402584130229,
      "grad_norm": 3.8197638988494873,
      "learning_rate": 4.082049784655814e-05,
      "loss": 0.6644,
      "step": 1207200
    },
    {
      "epoch": 11.016315059493394,
      "grad_norm": 3.332361936569214,
      "learning_rate": 4.0819737450422176e-05,
      "loss": 0.6362,
      "step": 1207300
    },
    {
      "epoch": 11.01722753485656,
      "grad_norm": 4.209045886993408,
      "learning_rate": 4.08189770542862e-05,
      "loss": 0.6655,
      "step": 1207400
    },
    {
      "epoch": 11.018140010219724,
      "grad_norm": 4.32928466796875,
      "learning_rate": 4.081821665815023e-05,
      "loss": 0.6695,
      "step": 1207500
    },
    {
      "epoch": 11.01905248558289,
      "grad_norm": 4.506968021392822,
      "learning_rate": 4.081745626201426e-05,
      "loss": 0.6873,
      "step": 1207600
    },
    {
      "epoch": 11.019964960946055,
      "grad_norm": 3.910317897796631,
      "learning_rate": 4.081669586587829e-05,
      "loss": 0.6643,
      "step": 1207700
    },
    {
      "epoch": 11.02087743630922,
      "grad_norm": 3.676140785217285,
      "learning_rate": 4.081593546974232e-05,
      "loss": 0.6736,
      "step": 1207800
    },
    {
      "epoch": 11.021789911672386,
      "grad_norm": 3.7302157878875732,
      "learning_rate": 4.081517507360635e-05,
      "loss": 0.7023,
      "step": 1207900
    },
    {
      "epoch": 11.02270238703555,
      "grad_norm": 3.74509596824646,
      "learning_rate": 4.081441467747037e-05,
      "loss": 0.6631,
      "step": 1208000
    },
    {
      "epoch": 11.023614862398714,
      "grad_norm": 3.7578606605529785,
      "learning_rate": 4.081365428133441e-05,
      "loss": 0.6567,
      "step": 1208100
    },
    {
      "epoch": 11.02452733776188,
      "grad_norm": 3.167295455932617,
      "learning_rate": 4.0812893885198433e-05,
      "loss": 0.6579,
      "step": 1208200
    },
    {
      "epoch": 11.025439813125045,
      "grad_norm": 3.3953514099121094,
      "learning_rate": 4.0812133489062464e-05,
      "loss": 0.6829,
      "step": 1208300
    },
    {
      "epoch": 11.02635228848821,
      "grad_norm": 4.035823345184326,
      "learning_rate": 4.0811373092926494e-05,
      "loss": 0.69,
      "step": 1208400
    },
    {
      "epoch": 11.027264763851376,
      "grad_norm": 2.7433652877807617,
      "learning_rate": 4.0810612696790524e-05,
      "loss": 0.6841,
      "step": 1208500
    },
    {
      "epoch": 11.028177239214541,
      "grad_norm": 4.387208938598633,
      "learning_rate": 4.080985230065455e-05,
      "loss": 0.6731,
      "step": 1208600
    },
    {
      "epoch": 11.029089714577706,
      "grad_norm": 3.3448493480682373,
      "learning_rate": 4.0809091904518584e-05,
      "loss": 0.6646,
      "step": 1208700
    },
    {
      "epoch": 11.030002189940872,
      "grad_norm": 4.4287495613098145,
      "learning_rate": 4.080833150838261e-05,
      "loss": 0.6683,
      "step": 1208800
    },
    {
      "epoch": 11.030914665304037,
      "grad_norm": 3.6260969638824463,
      "learning_rate": 4.080757111224664e-05,
      "loss": 0.6751,
      "step": 1208900
    },
    {
      "epoch": 11.031827140667202,
      "grad_norm": 3.4362804889678955,
      "learning_rate": 4.080681071611067e-05,
      "loss": 0.6937,
      "step": 1209000
    },
    {
      "epoch": 11.032739616030367,
      "grad_norm": 4.639946937561035,
      "learning_rate": 4.08060503199747e-05,
      "loss": 0.6504,
      "step": 1209100
    },
    {
      "epoch": 11.033652091393533,
      "grad_norm": 3.6014773845672607,
      "learning_rate": 4.080528992383873e-05,
      "loss": 0.6538,
      "step": 1209200
    },
    {
      "epoch": 11.034564566756698,
      "grad_norm": 4.39268159866333,
      "learning_rate": 4.080452952770276e-05,
      "loss": 0.6989,
      "step": 1209300
    },
    {
      "epoch": 11.035477042119863,
      "grad_norm": 4.961246013641357,
      "learning_rate": 4.080376913156678e-05,
      "loss": 0.6673,
      "step": 1209400
    },
    {
      "epoch": 11.036389517483029,
      "grad_norm": 3.321977138519287,
      "learning_rate": 4.080300873543082e-05,
      "loss": 0.6831,
      "step": 1209500
    },
    {
      "epoch": 11.037301992846194,
      "grad_norm": 4.918232440948486,
      "learning_rate": 4.080224833929484e-05,
      "loss": 0.687,
      "step": 1209600
    },
    {
      "epoch": 11.038214468209357,
      "grad_norm": 3.9507625102996826,
      "learning_rate": 4.080148794315887e-05,
      "loss": 0.7062,
      "step": 1209700
    },
    {
      "epoch": 11.039126943572523,
      "grad_norm": 4.63604736328125,
      "learning_rate": 4.08007275470229e-05,
      "loss": 0.7152,
      "step": 1209800
    },
    {
      "epoch": 11.040039418935688,
      "grad_norm": 4.646247863769531,
      "learning_rate": 4.079996715088693e-05,
      "loss": 0.6513,
      "step": 1209900
    },
    {
      "epoch": 11.040951894298853,
      "grad_norm": 3.7683064937591553,
      "learning_rate": 4.0799206754750954e-05,
      "loss": 0.6562,
      "step": 1210000
    },
    {
      "epoch": 11.041864369662019,
      "grad_norm": 3.7271268367767334,
      "learning_rate": 4.0798446358614984e-05,
      "loss": 0.7007,
      "step": 1210100
    },
    {
      "epoch": 11.042776845025184,
      "grad_norm": 5.231029987335205,
      "learning_rate": 4.0797685962479014e-05,
      "loss": 0.706,
      "step": 1210200
    },
    {
      "epoch": 11.04368932038835,
      "grad_norm": 3.7518022060394287,
      "learning_rate": 4.0796925566343045e-05,
      "loss": 0.7,
      "step": 1210300
    },
    {
      "epoch": 11.044601795751515,
      "grad_norm": 4.3365068435668945,
      "learning_rate": 4.0796165170207075e-05,
      "loss": 0.6982,
      "step": 1210400
    },
    {
      "epoch": 11.04551427111468,
      "grad_norm": 3.156379222869873,
      "learning_rate": 4.07954047740711e-05,
      "loss": 0.697,
      "step": 1210500
    },
    {
      "epoch": 11.046426746477845,
      "grad_norm": 3.249371290206909,
      "learning_rate": 4.0794644377935135e-05,
      "loss": 0.6838,
      "step": 1210600
    },
    {
      "epoch": 11.04733922184101,
      "grad_norm": 3.5926051139831543,
      "learning_rate": 4.079388398179916e-05,
      "loss": 0.6948,
      "step": 1210700
    },
    {
      "epoch": 11.048251697204176,
      "grad_norm": 3.395047664642334,
      "learning_rate": 4.079312358566319e-05,
      "loss": 0.7053,
      "step": 1210800
    },
    {
      "epoch": 11.049164172567341,
      "grad_norm": 3.8611679077148438,
      "learning_rate": 4.079236318952722e-05,
      "loss": 0.7298,
      "step": 1210900
    },
    {
      "epoch": 11.050076647930506,
      "grad_norm": 3.826056957244873,
      "learning_rate": 4.079160279339125e-05,
      "loss": 0.6463,
      "step": 1211000
    },
    {
      "epoch": 11.050989123293672,
      "grad_norm": 4.889832496643066,
      "learning_rate": 4.079084239725527e-05,
      "loss": 0.6875,
      "step": 1211100
    },
    {
      "epoch": 11.051901598656837,
      "grad_norm": 4.499660968780518,
      "learning_rate": 4.079008200111931e-05,
      "loss": 0.7242,
      "step": 1211200
    },
    {
      "epoch": 11.052814074020002,
      "grad_norm": 3.7043609619140625,
      "learning_rate": 4.078932160498333e-05,
      "loss": 0.7005,
      "step": 1211300
    },
    {
      "epoch": 11.053726549383166,
      "grad_norm": 3.636655569076538,
      "learning_rate": 4.078856120884736e-05,
      "loss": 0.6824,
      "step": 1211400
    },
    {
      "epoch": 11.054639024746331,
      "grad_norm": 3.143247604370117,
      "learning_rate": 4.078780081271139e-05,
      "loss": 0.6995,
      "step": 1211500
    },
    {
      "epoch": 11.055551500109496,
      "grad_norm": 3.0442140102386475,
      "learning_rate": 4.078704041657542e-05,
      "loss": 0.6504,
      "step": 1211600
    },
    {
      "epoch": 11.056463975472662,
      "grad_norm": 3.5337135791778564,
      "learning_rate": 4.078628002043945e-05,
      "loss": 0.6843,
      "step": 1211700
    },
    {
      "epoch": 11.057376450835827,
      "grad_norm": 4.417400360107422,
      "learning_rate": 4.078551962430348e-05,
      "loss": 0.6725,
      "step": 1211800
    },
    {
      "epoch": 11.058288926198992,
      "grad_norm": 4.900489330291748,
      "learning_rate": 4.0784759228167505e-05,
      "loss": 0.6792,
      "step": 1211900
    },
    {
      "epoch": 11.059201401562158,
      "grad_norm": 4.508402347564697,
      "learning_rate": 4.078399883203154e-05,
      "loss": 0.7106,
      "step": 1212000
    },
    {
      "epoch": 11.060113876925323,
      "grad_norm": 4.326951026916504,
      "learning_rate": 4.0783238435895565e-05,
      "loss": 0.6829,
      "step": 1212100
    },
    {
      "epoch": 11.061026352288488,
      "grad_norm": 3.1971678733825684,
      "learning_rate": 4.0782478039759596e-05,
      "loss": 0.6986,
      "step": 1212200
    },
    {
      "epoch": 11.061938827651653,
      "grad_norm": 3.6101317405700684,
      "learning_rate": 4.0781717643623626e-05,
      "loss": 0.642,
      "step": 1212300
    },
    {
      "epoch": 11.062851303014819,
      "grad_norm": 3.7077207565307617,
      "learning_rate": 4.0780957247487656e-05,
      "loss": 0.7231,
      "step": 1212400
    },
    {
      "epoch": 11.063763778377984,
      "grad_norm": 3.596256732940674,
      "learning_rate": 4.078019685135168e-05,
      "loss": 0.6492,
      "step": 1212500
    },
    {
      "epoch": 11.06467625374115,
      "grad_norm": 4.070427894592285,
      "learning_rate": 4.0779436455215716e-05,
      "loss": 0.6949,
      "step": 1212600
    },
    {
      "epoch": 11.065588729104315,
      "grad_norm": 4.136047840118408,
      "learning_rate": 4.077867605907974e-05,
      "loss": 0.6917,
      "step": 1212700
    },
    {
      "epoch": 11.06650120446748,
      "grad_norm": 3.4293994903564453,
      "learning_rate": 4.077791566294377e-05,
      "loss": 0.72,
      "step": 1212800
    },
    {
      "epoch": 11.067413679830645,
      "grad_norm": 3.191465139389038,
      "learning_rate": 4.07771552668078e-05,
      "loss": 0.7123,
      "step": 1212900
    },
    {
      "epoch": 11.06832615519381,
      "grad_norm": 4.056778430938721,
      "learning_rate": 4.077639487067182e-05,
      "loss": 0.6326,
      "step": 1213000
    },
    {
      "epoch": 11.069238630556974,
      "grad_norm": 5.469588279724121,
      "learning_rate": 4.077563447453586e-05,
      "loss": 0.6655,
      "step": 1213100
    },
    {
      "epoch": 11.07015110592014,
      "grad_norm": 3.7149603366851807,
      "learning_rate": 4.077487407839988e-05,
      "loss": 0.6679,
      "step": 1213200
    },
    {
      "epoch": 11.071063581283305,
      "grad_norm": 4.364691257476807,
      "learning_rate": 4.077411368226391e-05,
      "loss": 0.6957,
      "step": 1213300
    },
    {
      "epoch": 11.07197605664647,
      "grad_norm": 3.4508042335510254,
      "learning_rate": 4.077335328612794e-05,
      "loss": 0.6585,
      "step": 1213400
    },
    {
      "epoch": 11.072888532009635,
      "grad_norm": 3.9145352840423584,
      "learning_rate": 4.077259288999197e-05,
      "loss": 0.6589,
      "step": 1213500
    },
    {
      "epoch": 11.0738010073728,
      "grad_norm": 3.415395975112915,
      "learning_rate": 4.0771832493855996e-05,
      "loss": 0.6408,
      "step": 1213600
    },
    {
      "epoch": 11.074713482735966,
      "grad_norm": 4.002541542053223,
      "learning_rate": 4.077107209772003e-05,
      "loss": 0.6311,
      "step": 1213700
    },
    {
      "epoch": 11.075625958099131,
      "grad_norm": 4.664734363555908,
      "learning_rate": 4.0770311701584056e-05,
      "loss": 0.6844,
      "step": 1213800
    },
    {
      "epoch": 11.076538433462296,
      "grad_norm": 3.2267467975616455,
      "learning_rate": 4.0769551305448086e-05,
      "loss": 0.6332,
      "step": 1213900
    },
    {
      "epoch": 11.077450908825462,
      "grad_norm": 4.226105690002441,
      "learning_rate": 4.0768790909312116e-05,
      "loss": 0.6948,
      "step": 1214000
    },
    {
      "epoch": 11.078363384188627,
      "grad_norm": 4.649284362792969,
      "learning_rate": 4.0768030513176146e-05,
      "loss": 0.6826,
      "step": 1214100
    },
    {
      "epoch": 11.079275859551792,
      "grad_norm": 4.059819221496582,
      "learning_rate": 4.0767270117040177e-05,
      "loss": 0.6426,
      "step": 1214200
    },
    {
      "epoch": 11.080188334914958,
      "grad_norm": 4.016116142272949,
      "learning_rate": 4.0766509720904207e-05,
      "loss": 0.7129,
      "step": 1214300
    },
    {
      "epoch": 11.081100810278123,
      "grad_norm": 3.4351611137390137,
      "learning_rate": 4.076574932476823e-05,
      "loss": 0.6416,
      "step": 1214400
    },
    {
      "epoch": 11.082013285641288,
      "grad_norm": 4.166073322296143,
      "learning_rate": 4.076498892863227e-05,
      "loss": 0.659,
      "step": 1214500
    },
    {
      "epoch": 11.082925761004454,
      "grad_norm": 5.084620475769043,
      "learning_rate": 4.076422853249629e-05,
      "loss": 0.6865,
      "step": 1214600
    },
    {
      "epoch": 11.083838236367619,
      "grad_norm": 4.505485534667969,
      "learning_rate": 4.076346813636032e-05,
      "loss": 0.694,
      "step": 1214700
    },
    {
      "epoch": 11.084750711730782,
      "grad_norm": 3.0001413822174072,
      "learning_rate": 4.076270774022435e-05,
      "loss": 0.6634,
      "step": 1214800
    },
    {
      "epoch": 11.085663187093948,
      "grad_norm": 3.275897741317749,
      "learning_rate": 4.076194734408838e-05,
      "loss": 0.6841,
      "step": 1214900
    },
    {
      "epoch": 11.086575662457113,
      "grad_norm": 3.933802843093872,
      "learning_rate": 4.0761186947952404e-05,
      "loss": 0.6723,
      "step": 1215000
    },
    {
      "epoch": 11.087488137820278,
      "grad_norm": 3.5973074436187744,
      "learning_rate": 4.076042655181644e-05,
      "loss": 0.6767,
      "step": 1215100
    },
    {
      "epoch": 11.088400613183444,
      "grad_norm": 2.7623705863952637,
      "learning_rate": 4.0759666155680464e-05,
      "loss": 0.6874,
      "step": 1215200
    },
    {
      "epoch": 11.089313088546609,
      "grad_norm": 4.174436092376709,
      "learning_rate": 4.0758905759544494e-05,
      "loss": 0.6953,
      "step": 1215300
    },
    {
      "epoch": 11.090225563909774,
      "grad_norm": 3.818340539932251,
      "learning_rate": 4.0758145363408524e-05,
      "loss": 0.6928,
      "step": 1215400
    },
    {
      "epoch": 11.09113803927294,
      "grad_norm": 4.686886787414551,
      "learning_rate": 4.0757384967272554e-05,
      "loss": 0.6923,
      "step": 1215500
    },
    {
      "epoch": 11.092050514636105,
      "grad_norm": 3.41860294342041,
      "learning_rate": 4.0756624571136584e-05,
      "loss": 0.6817,
      "step": 1215600
    },
    {
      "epoch": 11.09296298999927,
      "grad_norm": 3.908869981765747,
      "learning_rate": 4.075586417500061e-05,
      "loss": 0.6698,
      "step": 1215700
    },
    {
      "epoch": 11.093875465362435,
      "grad_norm": 4.015451431274414,
      "learning_rate": 4.075510377886464e-05,
      "loss": 0.6679,
      "step": 1215800
    },
    {
      "epoch": 11.0947879407256,
      "grad_norm": 2.6550991535186768,
      "learning_rate": 4.075434338272867e-05,
      "loss": 0.6668,
      "step": 1215900
    },
    {
      "epoch": 11.095700416088766,
      "grad_norm": 3.762019634246826,
      "learning_rate": 4.07535829865927e-05,
      "loss": 0.6612,
      "step": 1216000
    },
    {
      "epoch": 11.096612891451931,
      "grad_norm": 4.0051116943359375,
      "learning_rate": 4.075282259045672e-05,
      "loss": 0.7042,
      "step": 1216100
    },
    {
      "epoch": 11.097525366815097,
      "grad_norm": 3.9668166637420654,
      "learning_rate": 4.075206219432076e-05,
      "loss": 0.6691,
      "step": 1216200
    },
    {
      "epoch": 11.098437842178262,
      "grad_norm": 4.112626075744629,
      "learning_rate": 4.075130179818478e-05,
      "loss": 0.6784,
      "step": 1216300
    },
    {
      "epoch": 11.099350317541427,
      "grad_norm": 3.2931649684906006,
      "learning_rate": 4.075054140204881e-05,
      "loss": 0.6439,
      "step": 1216400
    },
    {
      "epoch": 11.10026279290459,
      "grad_norm": 5.247466564178467,
      "learning_rate": 4.074978100591284e-05,
      "loss": 0.6688,
      "step": 1216500
    },
    {
      "epoch": 11.101175268267756,
      "grad_norm": 4.112855434417725,
      "learning_rate": 4.074902060977687e-05,
      "loss": 0.6678,
      "step": 1216600
    },
    {
      "epoch": 11.102087743630921,
      "grad_norm": 4.2932257652282715,
      "learning_rate": 4.07482602136409e-05,
      "loss": 0.7077,
      "step": 1216700
    },
    {
      "epoch": 11.103000218994087,
      "grad_norm": 4.059686183929443,
      "learning_rate": 4.074749981750493e-05,
      "loss": 0.6786,
      "step": 1216800
    },
    {
      "epoch": 11.103912694357252,
      "grad_norm": 4.429508686065674,
      "learning_rate": 4.0746739421368954e-05,
      "loss": 0.6808,
      "step": 1216900
    },
    {
      "epoch": 11.104825169720417,
      "grad_norm": 3.906538724899292,
      "learning_rate": 4.074597902523299e-05,
      "loss": 0.6629,
      "step": 1217000
    },
    {
      "epoch": 11.105737645083583,
      "grad_norm": 4.569287300109863,
      "learning_rate": 4.0745218629097015e-05,
      "loss": 0.6771,
      "step": 1217100
    },
    {
      "epoch": 11.106650120446748,
      "grad_norm": 4.247926712036133,
      "learning_rate": 4.0744458232961045e-05,
      "loss": 0.668,
      "step": 1217200
    },
    {
      "epoch": 11.107562595809913,
      "grad_norm": 4.367856979370117,
      "learning_rate": 4.0743697836825075e-05,
      "loss": 0.6607,
      "step": 1217300
    },
    {
      "epoch": 11.108475071173078,
      "grad_norm": 3.8350112438201904,
      "learning_rate": 4.0742937440689105e-05,
      "loss": 0.6707,
      "step": 1217400
    },
    {
      "epoch": 11.109387546536244,
      "grad_norm": 3.530341148376465,
      "learning_rate": 4.074217704455313e-05,
      "loss": 0.6957,
      "step": 1217500
    },
    {
      "epoch": 11.110300021899409,
      "grad_norm": 3.750952959060669,
      "learning_rate": 4.0741416648417165e-05,
      "loss": 0.669,
      "step": 1217600
    },
    {
      "epoch": 11.111212497262574,
      "grad_norm": 3.1663191318511963,
      "learning_rate": 4.074065625228119e-05,
      "loss": 0.6819,
      "step": 1217700
    },
    {
      "epoch": 11.11212497262574,
      "grad_norm": 3.488603353500366,
      "learning_rate": 4.073989585614522e-05,
      "loss": 0.6567,
      "step": 1217800
    },
    {
      "epoch": 11.113037447988905,
      "grad_norm": 4.605747222900391,
      "learning_rate": 4.073913546000925e-05,
      "loss": 0.6636,
      "step": 1217900
    },
    {
      "epoch": 11.11394992335207,
      "grad_norm": 2.689072847366333,
      "learning_rate": 4.073837506387328e-05,
      "loss": 0.6897,
      "step": 1218000
    },
    {
      "epoch": 11.114862398715236,
      "grad_norm": 3.8762927055358887,
      "learning_rate": 4.073761466773731e-05,
      "loss": 0.6956,
      "step": 1218100
    },
    {
      "epoch": 11.115774874078399,
      "grad_norm": 3.4265575408935547,
      "learning_rate": 4.073685427160134e-05,
      "loss": 0.6697,
      "step": 1218200
    },
    {
      "epoch": 11.116687349441564,
      "grad_norm": 4.466668605804443,
      "learning_rate": 4.073609387546536e-05,
      "loss": 0.6733,
      "step": 1218300
    },
    {
      "epoch": 11.11759982480473,
      "grad_norm": 5.256966590881348,
      "learning_rate": 4.07353334793294e-05,
      "loss": 0.6876,
      "step": 1218400
    },
    {
      "epoch": 11.118512300167895,
      "grad_norm": 3.533700466156006,
      "learning_rate": 4.073457308319342e-05,
      "loss": 0.691,
      "step": 1218500
    },
    {
      "epoch": 11.11942477553106,
      "grad_norm": 4.336123943328857,
      "learning_rate": 4.073381268705745e-05,
      "loss": 0.672,
      "step": 1218600
    },
    {
      "epoch": 11.120337250894226,
      "grad_norm": 3.8490612506866455,
      "learning_rate": 4.073305229092148e-05,
      "loss": 0.6804,
      "step": 1218700
    },
    {
      "epoch": 11.12124972625739,
      "grad_norm": 4.114353656768799,
      "learning_rate": 4.0732291894785505e-05,
      "loss": 0.6995,
      "step": 1218800
    },
    {
      "epoch": 11.122162201620556,
      "grad_norm": 4.205981254577637,
      "learning_rate": 4.073153149864954e-05,
      "loss": 0.6696,
      "step": 1218900
    },
    {
      "epoch": 11.123074676983721,
      "grad_norm": 3.9717676639556885,
      "learning_rate": 4.0730771102513566e-05,
      "loss": 0.6697,
      "step": 1219000
    },
    {
      "epoch": 11.123987152346887,
      "grad_norm": 5.635853290557861,
      "learning_rate": 4.0730010706377596e-05,
      "loss": 0.6532,
      "step": 1219100
    },
    {
      "epoch": 11.124899627710052,
      "grad_norm": 3.557333469390869,
      "learning_rate": 4.0729250310241626e-05,
      "loss": 0.6859,
      "step": 1219200
    },
    {
      "epoch": 11.125812103073217,
      "grad_norm": 3.234738826751709,
      "learning_rate": 4.0728489914105656e-05,
      "loss": 0.6985,
      "step": 1219300
    },
    {
      "epoch": 11.126724578436383,
      "grad_norm": 4.403073310852051,
      "learning_rate": 4.072772951796968e-05,
      "loss": 0.6868,
      "step": 1219400
    },
    {
      "epoch": 11.127637053799548,
      "grad_norm": 4.352898597717285,
      "learning_rate": 4.0726969121833716e-05,
      "loss": 0.6735,
      "step": 1219500
    },
    {
      "epoch": 11.128549529162713,
      "grad_norm": 3.4005286693573,
      "learning_rate": 4.072620872569774e-05,
      "loss": 0.6318,
      "step": 1219600
    },
    {
      "epoch": 11.129462004525879,
      "grad_norm": 3.458791494369507,
      "learning_rate": 4.072544832956177e-05,
      "loss": 0.6586,
      "step": 1219700
    },
    {
      "epoch": 11.130374479889044,
      "grad_norm": 3.9865007400512695,
      "learning_rate": 4.07246879334258e-05,
      "loss": 0.6745,
      "step": 1219800
    },
    {
      "epoch": 11.131286955252207,
      "grad_norm": 4.158751964569092,
      "learning_rate": 4.072392753728983e-05,
      "loss": 0.7003,
      "step": 1219900
    },
    {
      "epoch": 11.132199430615373,
      "grad_norm": 4.289570331573486,
      "learning_rate": 4.072316714115386e-05,
      "loss": 0.6796,
      "step": 1220000
    },
    {
      "epoch": 11.133111905978538,
      "grad_norm": 5.1358962059021,
      "learning_rate": 4.072240674501789e-05,
      "loss": 0.7008,
      "step": 1220100
    },
    {
      "epoch": 11.134024381341703,
      "grad_norm": 3.943679094314575,
      "learning_rate": 4.072164634888191e-05,
      "loss": 0.6885,
      "step": 1220200
    },
    {
      "epoch": 11.134936856704869,
      "grad_norm": 4.099186897277832,
      "learning_rate": 4.072088595274595e-05,
      "loss": 0.6733,
      "step": 1220300
    },
    {
      "epoch": 11.135849332068034,
      "grad_norm": 3.2639074325561523,
      "learning_rate": 4.072012555660997e-05,
      "loss": 0.6651,
      "step": 1220400
    },
    {
      "epoch": 11.1367618074312,
      "grad_norm": 4.3579816818237305,
      "learning_rate": 4.0719365160474e-05,
      "loss": 0.6707,
      "step": 1220500
    },
    {
      "epoch": 11.137674282794364,
      "grad_norm": 4.11276388168335,
      "learning_rate": 4.071860476433803e-05,
      "loss": 0.6944,
      "step": 1220600
    },
    {
      "epoch": 11.13858675815753,
      "grad_norm": 3.4382412433624268,
      "learning_rate": 4.071784436820206e-05,
      "loss": 0.6623,
      "step": 1220700
    },
    {
      "epoch": 11.139499233520695,
      "grad_norm": 4.488234043121338,
      "learning_rate": 4.0717083972066086e-05,
      "loss": 0.6626,
      "step": 1220800
    },
    {
      "epoch": 11.14041170888386,
      "grad_norm": 4.250439167022705,
      "learning_rate": 4.071632357593012e-05,
      "loss": 0.6563,
      "step": 1220900
    },
    {
      "epoch": 11.141324184247026,
      "grad_norm": 3.978203535079956,
      "learning_rate": 4.0715563179794147e-05,
      "loss": 0.6779,
      "step": 1221000
    },
    {
      "epoch": 11.142236659610191,
      "grad_norm": 4.0530595779418945,
      "learning_rate": 4.071480278365818e-05,
      "loss": 0.6586,
      "step": 1221100
    },
    {
      "epoch": 11.143149134973356,
      "grad_norm": 4.423117637634277,
      "learning_rate": 4.071404238752221e-05,
      "loss": 0.6859,
      "step": 1221200
    },
    {
      "epoch": 11.144061610336522,
      "grad_norm": 4.408047676086426,
      "learning_rate": 4.071328199138623e-05,
      "loss": 0.6954,
      "step": 1221300
    },
    {
      "epoch": 11.144974085699687,
      "grad_norm": 3.72387957572937,
      "learning_rate": 4.071252159525027e-05,
      "loss": 0.6897,
      "step": 1221400
    },
    {
      "epoch": 11.145886561062852,
      "grad_norm": 3.81595516204834,
      "learning_rate": 4.071176119911429e-05,
      "loss": 0.6864,
      "step": 1221500
    },
    {
      "epoch": 11.146799036426016,
      "grad_norm": 3.1725966930389404,
      "learning_rate": 4.071100080297832e-05,
      "loss": 0.6845,
      "step": 1221600
    },
    {
      "epoch": 11.147711511789181,
      "grad_norm": 4.560470104217529,
      "learning_rate": 4.071024040684235e-05,
      "loss": 0.7113,
      "step": 1221700
    },
    {
      "epoch": 11.148623987152346,
      "grad_norm": 3.8049774169921875,
      "learning_rate": 4.070948001070638e-05,
      "loss": 0.7035,
      "step": 1221800
    },
    {
      "epoch": 11.149536462515512,
      "grad_norm": 3.440855026245117,
      "learning_rate": 4.0708719614570404e-05,
      "loss": 0.6757,
      "step": 1221900
    },
    {
      "epoch": 11.150448937878677,
      "grad_norm": 4.387557029724121,
      "learning_rate": 4.070795921843444e-05,
      "loss": 0.7046,
      "step": 1222000
    },
    {
      "epoch": 11.151361413241842,
      "grad_norm": 3.6858980655670166,
      "learning_rate": 4.0707198822298464e-05,
      "loss": 0.7363,
      "step": 1222100
    },
    {
      "epoch": 11.152273888605007,
      "grad_norm": 4.034265995025635,
      "learning_rate": 4.0706438426162494e-05,
      "loss": 0.7022,
      "step": 1222200
    },
    {
      "epoch": 11.153186363968173,
      "grad_norm": 3.5232322216033936,
      "learning_rate": 4.0705678030026524e-05,
      "loss": 0.6716,
      "step": 1222300
    },
    {
      "epoch": 11.154098839331338,
      "grad_norm": 4.405521392822266,
      "learning_rate": 4.0704917633890554e-05,
      "loss": 0.6658,
      "step": 1222400
    },
    {
      "epoch": 11.155011314694503,
      "grad_norm": 5.559545516967773,
      "learning_rate": 4.0704157237754584e-05,
      "loss": 0.6693,
      "step": 1222500
    },
    {
      "epoch": 11.155923790057669,
      "grad_norm": 4.403949737548828,
      "learning_rate": 4.0703396841618614e-05,
      "loss": 0.7041,
      "step": 1222600
    },
    {
      "epoch": 11.156836265420834,
      "grad_norm": 5.358492851257324,
      "learning_rate": 4.070263644548264e-05,
      "loss": 0.6829,
      "step": 1222700
    },
    {
      "epoch": 11.157748740784,
      "grad_norm": 3.505105972290039,
      "learning_rate": 4.0701876049346674e-05,
      "loss": 0.6879,
      "step": 1222800
    },
    {
      "epoch": 11.158661216147165,
      "grad_norm": 3.5839293003082275,
      "learning_rate": 4.07011156532107e-05,
      "loss": 0.6588,
      "step": 1222900
    },
    {
      "epoch": 11.15957369151033,
      "grad_norm": 4.188766956329346,
      "learning_rate": 4.070035525707473e-05,
      "loss": 0.738,
      "step": 1223000
    },
    {
      "epoch": 11.160486166873495,
      "grad_norm": 4.0918989181518555,
      "learning_rate": 4.069959486093876e-05,
      "loss": 0.6422,
      "step": 1223100
    },
    {
      "epoch": 11.16139864223666,
      "grad_norm": 4.182902812957764,
      "learning_rate": 4.069883446480279e-05,
      "loss": 0.6659,
      "step": 1223200
    },
    {
      "epoch": 11.162311117599824,
      "grad_norm": 3.084747791290283,
      "learning_rate": 4.069807406866681e-05,
      "loss": 0.6548,
      "step": 1223300
    },
    {
      "epoch": 11.16322359296299,
      "grad_norm": 3.728534460067749,
      "learning_rate": 4.069731367253085e-05,
      "loss": 0.7031,
      "step": 1223400
    },
    {
      "epoch": 11.164136068326155,
      "grad_norm": 2.8979036808013916,
      "learning_rate": 4.069655327639487e-05,
      "loss": 0.6678,
      "step": 1223500
    },
    {
      "epoch": 11.16504854368932,
      "grad_norm": 4.967927932739258,
      "learning_rate": 4.06957928802589e-05,
      "loss": 0.6722,
      "step": 1223600
    },
    {
      "epoch": 11.165961019052485,
      "grad_norm": 4.0371222496032715,
      "learning_rate": 4.069503248412293e-05,
      "loss": 0.6681,
      "step": 1223700
    },
    {
      "epoch": 11.16687349441565,
      "grad_norm": 3.481019973754883,
      "learning_rate": 4.069427208798696e-05,
      "loss": 0.6477,
      "step": 1223800
    },
    {
      "epoch": 11.167785969778816,
      "grad_norm": 3.8173153400421143,
      "learning_rate": 4.069351169185099e-05,
      "loss": 0.7288,
      "step": 1223900
    },
    {
      "epoch": 11.168698445141981,
      "grad_norm": 4.17336893081665,
      "learning_rate": 4.069275129571502e-05,
      "loss": 0.6859,
      "step": 1224000
    },
    {
      "epoch": 11.169610920505146,
      "grad_norm": 4.423532009124756,
      "learning_rate": 4.0691990899579045e-05,
      "loss": 0.6516,
      "step": 1224100
    },
    {
      "epoch": 11.170523395868312,
      "grad_norm": 4.39072322845459,
      "learning_rate": 4.0691230503443075e-05,
      "loss": 0.6615,
      "step": 1224200
    },
    {
      "epoch": 11.171435871231477,
      "grad_norm": 4.8045334815979,
      "learning_rate": 4.0690470107307105e-05,
      "loss": 0.6713,
      "step": 1224300
    },
    {
      "epoch": 11.172348346594642,
      "grad_norm": 2.878371238708496,
      "learning_rate": 4.068970971117113e-05,
      "loss": 0.652,
      "step": 1224400
    },
    {
      "epoch": 11.173260821957808,
      "grad_norm": 3.082869052886963,
      "learning_rate": 4.0688949315035165e-05,
      "loss": 0.6803,
      "step": 1224500
    },
    {
      "epoch": 11.174173297320973,
      "grad_norm": 3.6829135417938232,
      "learning_rate": 4.068818891889919e-05,
      "loss": 0.691,
      "step": 1224600
    },
    {
      "epoch": 11.175085772684138,
      "grad_norm": 3.655902862548828,
      "learning_rate": 4.068742852276322e-05,
      "loss": 0.6878,
      "step": 1224700
    },
    {
      "epoch": 11.175998248047303,
      "grad_norm": 3.8648905754089355,
      "learning_rate": 4.068666812662725e-05,
      "loss": 0.6341,
      "step": 1224800
    },
    {
      "epoch": 11.176910723410469,
      "grad_norm": 4.099759578704834,
      "learning_rate": 4.068590773049128e-05,
      "loss": 0.6312,
      "step": 1224900
    },
    {
      "epoch": 11.177823198773632,
      "grad_norm": 4.510827541351318,
      "learning_rate": 4.068514733435531e-05,
      "loss": 0.6905,
      "step": 1225000
    },
    {
      "epoch": 11.178735674136798,
      "grad_norm": 3.801680564880371,
      "learning_rate": 4.068438693821934e-05,
      "loss": 0.6861,
      "step": 1225100
    },
    {
      "epoch": 11.179648149499963,
      "grad_norm": 3.1415209770202637,
      "learning_rate": 4.068362654208336e-05,
      "loss": 0.613,
      "step": 1225200
    },
    {
      "epoch": 11.180560624863128,
      "grad_norm": 3.8521125316619873,
      "learning_rate": 4.06828661459474e-05,
      "loss": 0.6923,
      "step": 1225300
    },
    {
      "epoch": 11.181473100226293,
      "grad_norm": 1.9724353551864624,
      "learning_rate": 4.068210574981142e-05,
      "loss": 0.6718,
      "step": 1225400
    },
    {
      "epoch": 11.182385575589459,
      "grad_norm": 3.681370258331299,
      "learning_rate": 4.068134535367545e-05,
      "loss": 0.6521,
      "step": 1225500
    },
    {
      "epoch": 11.183298050952624,
      "grad_norm": 4.355123043060303,
      "learning_rate": 4.068058495753948e-05,
      "loss": 0.6894,
      "step": 1225600
    },
    {
      "epoch": 11.18421052631579,
      "grad_norm": 5.4121503829956055,
      "learning_rate": 4.067982456140351e-05,
      "loss": 0.6819,
      "step": 1225700
    },
    {
      "epoch": 11.185123001678955,
      "grad_norm": 3.628812789916992,
      "learning_rate": 4.0679064165267536e-05,
      "loss": 0.6864,
      "step": 1225800
    },
    {
      "epoch": 11.18603547704212,
      "grad_norm": 3.9997639656066895,
      "learning_rate": 4.067830376913157e-05,
      "loss": 0.6969,
      "step": 1225900
    },
    {
      "epoch": 11.186947952405285,
      "grad_norm": 4.42482328414917,
      "learning_rate": 4.0677543372995596e-05,
      "loss": 0.6886,
      "step": 1226000
    },
    {
      "epoch": 11.18786042776845,
      "grad_norm": 3.9359331130981445,
      "learning_rate": 4.0676782976859626e-05,
      "loss": 0.6369,
      "step": 1226100
    },
    {
      "epoch": 11.188772903131616,
      "grad_norm": 3.7906291484832764,
      "learning_rate": 4.0676022580723656e-05,
      "loss": 0.6406,
      "step": 1226200
    },
    {
      "epoch": 11.189685378494781,
      "grad_norm": 3.592275619506836,
      "learning_rate": 4.0675262184587686e-05,
      "loss": 0.7058,
      "step": 1226300
    },
    {
      "epoch": 11.190597853857946,
      "grad_norm": 4.030984401702881,
      "learning_rate": 4.0674501788451716e-05,
      "loss": 0.71,
      "step": 1226400
    },
    {
      "epoch": 11.191510329221112,
      "grad_norm": 3.284000873565674,
      "learning_rate": 4.0673741392315746e-05,
      "loss": 0.6776,
      "step": 1226500
    },
    {
      "epoch": 11.192422804584277,
      "grad_norm": 3.4302165508270264,
      "learning_rate": 4.067298099617977e-05,
      "loss": 0.6868,
      "step": 1226600
    },
    {
      "epoch": 11.19333527994744,
      "grad_norm": 3.566046714782715,
      "learning_rate": 4.0672220600043806e-05,
      "loss": 0.6688,
      "step": 1226700
    },
    {
      "epoch": 11.194247755310606,
      "grad_norm": 4.8846564292907715,
      "learning_rate": 4.067146020390783e-05,
      "loss": 0.6444,
      "step": 1226800
    },
    {
      "epoch": 11.195160230673771,
      "grad_norm": 5.151763439178467,
      "learning_rate": 4.067069980777186e-05,
      "loss": 0.6784,
      "step": 1226900
    },
    {
      "epoch": 11.196072706036936,
      "grad_norm": 4.571631908416748,
      "learning_rate": 4.066993941163589e-05,
      "loss": 0.7271,
      "step": 1227000
    },
    {
      "epoch": 11.196985181400102,
      "grad_norm": 4.199975967407227,
      "learning_rate": 4.066917901549991e-05,
      "loss": 0.6603,
      "step": 1227100
    },
    {
      "epoch": 11.197897656763267,
      "grad_norm": 4.0848283767700195,
      "learning_rate": 4.066841861936394e-05,
      "loss": 0.6735,
      "step": 1227200
    },
    {
      "epoch": 11.198810132126432,
      "grad_norm": 4.457926273345947,
      "learning_rate": 4.066765822322797e-05,
      "loss": 0.6623,
      "step": 1227300
    },
    {
      "epoch": 11.199722607489598,
      "grad_norm": 3.650078058242798,
      "learning_rate": 4.0666897827092e-05,
      "loss": 0.6757,
      "step": 1227400
    },
    {
      "epoch": 11.200635082852763,
      "grad_norm": 3.851567268371582,
      "learning_rate": 4.066613743095603e-05,
      "loss": 0.7193,
      "step": 1227500
    },
    {
      "epoch": 11.201547558215928,
      "grad_norm": 4.532015800476074,
      "learning_rate": 4.066537703482006e-05,
      "loss": 0.6972,
      "step": 1227600
    },
    {
      "epoch": 11.202460033579094,
      "grad_norm": 4.030160903930664,
      "learning_rate": 4.0664616638684087e-05,
      "loss": 0.6654,
      "step": 1227700
    },
    {
      "epoch": 11.203372508942259,
      "grad_norm": 4.006389617919922,
      "learning_rate": 4.066385624254812e-05,
      "loss": 0.6479,
      "step": 1227800
    },
    {
      "epoch": 11.204284984305424,
      "grad_norm": 4.44655704498291,
      "learning_rate": 4.066309584641215e-05,
      "loss": 0.6721,
      "step": 1227900
    },
    {
      "epoch": 11.20519745966859,
      "grad_norm": 4.384151935577393,
      "learning_rate": 4.066233545027618e-05,
      "loss": 0.6863,
      "step": 1228000
    },
    {
      "epoch": 11.206109935031755,
      "grad_norm": 4.158300399780273,
      "learning_rate": 4.066157505414021e-05,
      "loss": 0.658,
      "step": 1228100
    },
    {
      "epoch": 11.20702241039492,
      "grad_norm": 3.8244457244873047,
      "learning_rate": 4.066081465800424e-05,
      "loss": 0.6825,
      "step": 1228200
    },
    {
      "epoch": 11.207934885758085,
      "grad_norm": 4.422351360321045,
      "learning_rate": 4.066005426186826e-05,
      "loss": 0.6662,
      "step": 1228300
    },
    {
      "epoch": 11.208847361121249,
      "grad_norm": 3.5127382278442383,
      "learning_rate": 4.06592938657323e-05,
      "loss": 0.6905,
      "step": 1228400
    },
    {
      "epoch": 11.209759836484414,
      "grad_norm": 3.756605625152588,
      "learning_rate": 4.065853346959632e-05,
      "loss": 0.7028,
      "step": 1228500
    },
    {
      "epoch": 11.21067231184758,
      "grad_norm": 2.98561692237854,
      "learning_rate": 4.065777307346035e-05,
      "loss": 0.677,
      "step": 1228600
    },
    {
      "epoch": 11.211584787210745,
      "grad_norm": 4.120630741119385,
      "learning_rate": 4.065701267732438e-05,
      "loss": 0.6877,
      "step": 1228700
    },
    {
      "epoch": 11.21249726257391,
      "grad_norm": 4.399450302124023,
      "learning_rate": 4.065625228118841e-05,
      "loss": 0.6966,
      "step": 1228800
    },
    {
      "epoch": 11.213409737937075,
      "grad_norm": 3.627337694168091,
      "learning_rate": 4.065549188505244e-05,
      "loss": 0.6815,
      "step": 1228900
    },
    {
      "epoch": 11.21432221330024,
      "grad_norm": 3.8435590267181396,
      "learning_rate": 4.065473148891647e-05,
      "loss": 0.6528,
      "step": 1229000
    },
    {
      "epoch": 11.215234688663406,
      "grad_norm": 3.3445420265197754,
      "learning_rate": 4.0653971092780494e-05,
      "loss": 0.6858,
      "step": 1229100
    },
    {
      "epoch": 11.216147164026571,
      "grad_norm": 4.092921733856201,
      "learning_rate": 4.065321069664453e-05,
      "loss": 0.678,
      "step": 1229200
    },
    {
      "epoch": 11.217059639389737,
      "grad_norm": 4.409746170043945,
      "learning_rate": 4.0652450300508554e-05,
      "loss": 0.6897,
      "step": 1229300
    },
    {
      "epoch": 11.217972114752902,
      "grad_norm": 4.612039566040039,
      "learning_rate": 4.0651689904372584e-05,
      "loss": 0.6728,
      "step": 1229400
    },
    {
      "epoch": 11.218884590116067,
      "grad_norm": 4.418595314025879,
      "learning_rate": 4.0650929508236614e-05,
      "loss": 0.6628,
      "step": 1229500
    },
    {
      "epoch": 11.219797065479233,
      "grad_norm": 3.2997450828552246,
      "learning_rate": 4.0650169112100644e-05,
      "loss": 0.6128,
      "step": 1229600
    },
    {
      "epoch": 11.220709540842398,
      "grad_norm": 3.770327568054199,
      "learning_rate": 4.064940871596467e-05,
      "loss": 0.6799,
      "step": 1229700
    },
    {
      "epoch": 11.221622016205563,
      "grad_norm": 4.1162872314453125,
      "learning_rate": 4.06486483198287e-05,
      "loss": 0.658,
      "step": 1229800
    },
    {
      "epoch": 11.222534491568728,
      "grad_norm": 4.462932109832764,
      "learning_rate": 4.064788792369273e-05,
      "loss": 0.6952,
      "step": 1229900
    },
    {
      "epoch": 11.223446966931894,
      "grad_norm": 4.586045265197754,
      "learning_rate": 4.064712752755676e-05,
      "loss": 0.6933,
      "step": 1230000
    },
    {
      "epoch": 11.224359442295057,
      "grad_norm": 4.231228351593018,
      "learning_rate": 4.064636713142079e-05,
      "loss": 0.6645,
      "step": 1230100
    },
    {
      "epoch": 11.225271917658223,
      "grad_norm": 3.9671599864959717,
      "learning_rate": 4.064560673528481e-05,
      "loss": 0.6638,
      "step": 1230200
    },
    {
      "epoch": 11.226184393021388,
      "grad_norm": 3.822120428085327,
      "learning_rate": 4.064484633914885e-05,
      "loss": 0.6424,
      "step": 1230300
    },
    {
      "epoch": 11.227096868384553,
      "grad_norm": 3.3831915855407715,
      "learning_rate": 4.064408594301287e-05,
      "loss": 0.6177,
      "step": 1230400
    },
    {
      "epoch": 11.228009343747718,
      "grad_norm": 3.647261381149292,
      "learning_rate": 4.06433255468769e-05,
      "loss": 0.6405,
      "step": 1230500
    },
    {
      "epoch": 11.228921819110884,
      "grad_norm": 3.592103958129883,
      "learning_rate": 4.064256515074093e-05,
      "loss": 0.7086,
      "step": 1230600
    },
    {
      "epoch": 11.229834294474049,
      "grad_norm": 4.087652683258057,
      "learning_rate": 4.064180475460496e-05,
      "loss": 0.6905,
      "step": 1230700
    },
    {
      "epoch": 11.230746769837214,
      "grad_norm": 4.2980194091796875,
      "learning_rate": 4.064104435846899e-05,
      "loss": 0.6753,
      "step": 1230800
    },
    {
      "epoch": 11.23165924520038,
      "grad_norm": 5.25050687789917,
      "learning_rate": 4.064028396233302e-05,
      "loss": 0.7078,
      "step": 1230900
    },
    {
      "epoch": 11.232571720563545,
      "grad_norm": 4.04351806640625,
      "learning_rate": 4.0639523566197045e-05,
      "loss": 0.6723,
      "step": 1231000
    },
    {
      "epoch": 11.23348419592671,
      "grad_norm": 4.704914569854736,
      "learning_rate": 4.0638763170061075e-05,
      "loss": 0.7042,
      "step": 1231100
    },
    {
      "epoch": 11.234396671289876,
      "grad_norm": 5.078738212585449,
      "learning_rate": 4.0638002773925105e-05,
      "loss": 0.6425,
      "step": 1231200
    },
    {
      "epoch": 11.23530914665304,
      "grad_norm": 4.002060890197754,
      "learning_rate": 4.0637242377789135e-05,
      "loss": 0.6529,
      "step": 1231300
    },
    {
      "epoch": 11.236221622016206,
      "grad_norm": 3.8174362182617188,
      "learning_rate": 4.0636481981653165e-05,
      "loss": 0.7096,
      "step": 1231400
    },
    {
      "epoch": 11.237134097379371,
      "grad_norm": 4.505671977996826,
      "learning_rate": 4.0635721585517195e-05,
      "loss": 0.7004,
      "step": 1231500
    },
    {
      "epoch": 11.238046572742537,
      "grad_norm": 4.407524585723877,
      "learning_rate": 4.063496118938122e-05,
      "loss": 0.6662,
      "step": 1231600
    },
    {
      "epoch": 11.238959048105702,
      "grad_norm": 4.8963823318481445,
      "learning_rate": 4.0634200793245255e-05,
      "loss": 0.6819,
      "step": 1231700
    },
    {
      "epoch": 11.239871523468866,
      "grad_norm": 4.366970539093018,
      "learning_rate": 4.063344039710928e-05,
      "loss": 0.696,
      "step": 1231800
    },
    {
      "epoch": 11.24078399883203,
      "grad_norm": 2.7683987617492676,
      "learning_rate": 4.063268000097331e-05,
      "loss": 0.6617,
      "step": 1231900
    },
    {
      "epoch": 11.241696474195196,
      "grad_norm": 4.039894104003906,
      "learning_rate": 4.063191960483734e-05,
      "loss": 0.7135,
      "step": 1232000
    },
    {
      "epoch": 11.242608949558361,
      "grad_norm": 4.24031925201416,
      "learning_rate": 4.063115920870137e-05,
      "loss": 0.6967,
      "step": 1232100
    },
    {
      "epoch": 11.243521424921527,
      "grad_norm": 3.8979432582855225,
      "learning_rate": 4.06303988125654e-05,
      "loss": 0.6748,
      "step": 1232200
    },
    {
      "epoch": 11.244433900284692,
      "grad_norm": 3.830700397491455,
      "learning_rate": 4.062963841642943e-05,
      "loss": 0.6809,
      "step": 1232300
    },
    {
      "epoch": 11.245346375647857,
      "grad_norm": 3.7232069969177246,
      "learning_rate": 4.062887802029345e-05,
      "loss": 0.6836,
      "step": 1232400
    },
    {
      "epoch": 11.246258851011023,
      "grad_norm": 4.5031633377075195,
      "learning_rate": 4.062811762415748e-05,
      "loss": 0.7328,
      "step": 1232500
    },
    {
      "epoch": 11.247171326374188,
      "grad_norm": 3.5803709030151367,
      "learning_rate": 4.062735722802151e-05,
      "loss": 0.6684,
      "step": 1232600
    },
    {
      "epoch": 11.248083801737353,
      "grad_norm": 3.9187610149383545,
      "learning_rate": 4.0626596831885536e-05,
      "loss": 0.6425,
      "step": 1232700
    },
    {
      "epoch": 11.248996277100519,
      "grad_norm": 3.2788116931915283,
      "learning_rate": 4.062583643574957e-05,
      "loss": 0.6661,
      "step": 1232800
    },
    {
      "epoch": 11.249908752463684,
      "grad_norm": 3.24845552444458,
      "learning_rate": 4.0625076039613596e-05,
      "loss": 0.6519,
      "step": 1232900
    },
    {
      "epoch": 11.25082122782685,
      "grad_norm": 4.1064958572387695,
      "learning_rate": 4.0624315643477626e-05,
      "loss": 0.6676,
      "step": 1233000
    },
    {
      "epoch": 11.251733703190014,
      "grad_norm": 4.054828643798828,
      "learning_rate": 4.0623555247341656e-05,
      "loss": 0.6687,
      "step": 1233100
    },
    {
      "epoch": 11.25264617855318,
      "grad_norm": 3.931011915206909,
      "learning_rate": 4.0622794851205686e-05,
      "loss": 0.634,
      "step": 1233200
    },
    {
      "epoch": 11.253558653916345,
      "grad_norm": 3.5868804454803467,
      "learning_rate": 4.0622034455069716e-05,
      "loss": 0.6661,
      "step": 1233300
    },
    {
      "epoch": 11.254471129279509,
      "grad_norm": 4.475002765655518,
      "learning_rate": 4.0621274058933746e-05,
      "loss": 0.7022,
      "step": 1233400
    },
    {
      "epoch": 11.255383604642674,
      "grad_norm": 3.6462838649749756,
      "learning_rate": 4.062051366279777e-05,
      "loss": 0.6909,
      "step": 1233500
    },
    {
      "epoch": 11.25629608000584,
      "grad_norm": 4.014317512512207,
      "learning_rate": 4.0619753266661806e-05,
      "loss": 0.692,
      "step": 1233600
    },
    {
      "epoch": 11.257208555369004,
      "grad_norm": 4.923387050628662,
      "learning_rate": 4.061899287052583e-05,
      "loss": 0.6429,
      "step": 1233700
    },
    {
      "epoch": 11.25812103073217,
      "grad_norm": 3.3962395191192627,
      "learning_rate": 4.061823247438986e-05,
      "loss": 0.6878,
      "step": 1233800
    },
    {
      "epoch": 11.259033506095335,
      "grad_norm": 4.416128635406494,
      "learning_rate": 4.061747207825389e-05,
      "loss": 0.6884,
      "step": 1233900
    },
    {
      "epoch": 11.2599459814585,
      "grad_norm": 3.880293369293213,
      "learning_rate": 4.061671168211792e-05,
      "loss": 0.682,
      "step": 1234000
    },
    {
      "epoch": 11.260858456821666,
      "grad_norm": 3.918750524520874,
      "learning_rate": 4.061595128598194e-05,
      "loss": 0.6853,
      "step": 1234100
    },
    {
      "epoch": 11.261770932184831,
      "grad_norm": 3.4652791023254395,
      "learning_rate": 4.061519088984598e-05,
      "loss": 0.6611,
      "step": 1234200
    },
    {
      "epoch": 11.262683407547996,
      "grad_norm": 3.81772518157959,
      "learning_rate": 4.061443049371e-05,
      "loss": 0.709,
      "step": 1234300
    },
    {
      "epoch": 11.263595882911162,
      "grad_norm": 4.595225811004639,
      "learning_rate": 4.061367009757403e-05,
      "loss": 0.6672,
      "step": 1234400
    },
    {
      "epoch": 11.264508358274327,
      "grad_norm": 4.2533087730407715,
      "learning_rate": 4.061290970143806e-05,
      "loss": 0.6609,
      "step": 1234500
    },
    {
      "epoch": 11.265420833637492,
      "grad_norm": 4.162652015686035,
      "learning_rate": 4.0612149305302093e-05,
      "loss": 0.6822,
      "step": 1234600
    },
    {
      "epoch": 11.266333309000657,
      "grad_norm": 3.974276065826416,
      "learning_rate": 4.0611388909166123e-05,
      "loss": 0.6799,
      "step": 1234700
    },
    {
      "epoch": 11.267245784363823,
      "grad_norm": 4.513637542724609,
      "learning_rate": 4.0610628513030154e-05,
      "loss": 0.6684,
      "step": 1234800
    },
    {
      "epoch": 11.268158259726988,
      "grad_norm": 4.856778144836426,
      "learning_rate": 4.060986811689418e-05,
      "loss": 0.7294,
      "step": 1234900
    },
    {
      "epoch": 11.269070735090153,
      "grad_norm": 4.405491352081299,
      "learning_rate": 4.0609107720758214e-05,
      "loss": 0.6571,
      "step": 1235000
    },
    {
      "epoch": 11.269983210453319,
      "grad_norm": 3.4647490978240967,
      "learning_rate": 4.060834732462224e-05,
      "loss": 0.6467,
      "step": 1235100
    },
    {
      "epoch": 11.270895685816482,
      "grad_norm": 3.892605781555176,
      "learning_rate": 4.060758692848627e-05,
      "loss": 0.682,
      "step": 1235200
    },
    {
      "epoch": 11.271808161179647,
      "grad_norm": 4.089158535003662,
      "learning_rate": 4.06068265323503e-05,
      "loss": 0.7159,
      "step": 1235300
    },
    {
      "epoch": 11.272720636542813,
      "grad_norm": 4.262422561645508,
      "learning_rate": 4.060606613621433e-05,
      "loss": 0.6384,
      "step": 1235400
    },
    {
      "epoch": 11.273633111905978,
      "grad_norm": 2.612454414367676,
      "learning_rate": 4.060530574007835e-05,
      "loss": 0.6661,
      "step": 1235500
    },
    {
      "epoch": 11.274545587269143,
      "grad_norm": 4.24224328994751,
      "learning_rate": 4.060454534394238e-05,
      "loss": 0.6979,
      "step": 1235600
    },
    {
      "epoch": 11.275458062632309,
      "grad_norm": 4.213347434997559,
      "learning_rate": 4.060378494780641e-05,
      "loss": 0.677,
      "step": 1235700
    },
    {
      "epoch": 11.276370537995474,
      "grad_norm": 4.494773864746094,
      "learning_rate": 4.060302455167044e-05,
      "loss": 0.6702,
      "step": 1235800
    },
    {
      "epoch": 11.27728301335864,
      "grad_norm": 4.254977703094482,
      "learning_rate": 4.060226415553447e-05,
      "loss": 0.6736,
      "step": 1235900
    },
    {
      "epoch": 11.278195488721805,
      "grad_norm": 4.590306758880615,
      "learning_rate": 4.0601503759398494e-05,
      "loss": 0.6914,
      "step": 1236000
    },
    {
      "epoch": 11.27910796408497,
      "grad_norm": 3.3978142738342285,
      "learning_rate": 4.060074336326253e-05,
      "loss": 0.6512,
      "step": 1236100
    },
    {
      "epoch": 11.280020439448135,
      "grad_norm": 5.029586315155029,
      "learning_rate": 4.0599982967126554e-05,
      "loss": 0.6657,
      "step": 1236200
    },
    {
      "epoch": 11.2809329148113,
      "grad_norm": 3.559652090072632,
      "learning_rate": 4.0599222570990584e-05,
      "loss": 0.6964,
      "step": 1236300
    },
    {
      "epoch": 11.281845390174466,
      "grad_norm": 3.6514954566955566,
      "learning_rate": 4.0598462174854614e-05,
      "loss": 0.6956,
      "step": 1236400
    },
    {
      "epoch": 11.282757865537631,
      "grad_norm": 3.751180410385132,
      "learning_rate": 4.0597701778718644e-05,
      "loss": 0.6805,
      "step": 1236500
    },
    {
      "epoch": 11.283670340900796,
      "grad_norm": 3.4828572273254395,
      "learning_rate": 4.059694138258267e-05,
      "loss": 0.6622,
      "step": 1236600
    },
    {
      "epoch": 11.284582816263962,
      "grad_norm": 4.1560750007629395,
      "learning_rate": 4.0596180986446704e-05,
      "loss": 0.6561,
      "step": 1236700
    },
    {
      "epoch": 11.285495291627125,
      "grad_norm": 4.697671413421631,
      "learning_rate": 4.059542059031073e-05,
      "loss": 0.7182,
      "step": 1236800
    },
    {
      "epoch": 11.28640776699029,
      "grad_norm": 2.4992613792419434,
      "learning_rate": 4.059466019417476e-05,
      "loss": 0.6767,
      "step": 1236900
    },
    {
      "epoch": 11.287320242353456,
      "grad_norm": 3.746027708053589,
      "learning_rate": 4.059389979803879e-05,
      "loss": 0.6805,
      "step": 1237000
    },
    {
      "epoch": 11.288232717716621,
      "grad_norm": 3.3551249504089355,
      "learning_rate": 4.059313940190282e-05,
      "loss": 0.6799,
      "step": 1237100
    },
    {
      "epoch": 11.289145193079786,
      "grad_norm": 4.595090389251709,
      "learning_rate": 4.059237900576685e-05,
      "loss": 0.6812,
      "step": 1237200
    },
    {
      "epoch": 11.290057668442952,
      "grad_norm": 4.891768932342529,
      "learning_rate": 4.059161860963088e-05,
      "loss": 0.6799,
      "step": 1237300
    },
    {
      "epoch": 11.290970143806117,
      "grad_norm": 4.9565019607543945,
      "learning_rate": 4.05908582134949e-05,
      "loss": 0.6944,
      "step": 1237400
    },
    {
      "epoch": 11.291882619169282,
      "grad_norm": 4.185500621795654,
      "learning_rate": 4.059009781735894e-05,
      "loss": 0.6884,
      "step": 1237500
    },
    {
      "epoch": 11.292795094532448,
      "grad_norm": 4.532868385314941,
      "learning_rate": 4.058933742122296e-05,
      "loss": 0.6694,
      "step": 1237600
    },
    {
      "epoch": 11.293707569895613,
      "grad_norm": 3.821495532989502,
      "learning_rate": 4.058857702508699e-05,
      "loss": 0.7076,
      "step": 1237700
    },
    {
      "epoch": 11.294620045258778,
      "grad_norm": 5.385601997375488,
      "learning_rate": 4.058781662895102e-05,
      "loss": 0.6873,
      "step": 1237800
    },
    {
      "epoch": 11.295532520621943,
      "grad_norm": 4.231084823608398,
      "learning_rate": 4.058705623281505e-05,
      "loss": 0.6244,
      "step": 1237900
    },
    {
      "epoch": 11.296444995985109,
      "grad_norm": 4.747055530548096,
      "learning_rate": 4.0586295836679075e-05,
      "loss": 0.6704,
      "step": 1238000
    },
    {
      "epoch": 11.297357471348274,
      "grad_norm": 4.319138050079346,
      "learning_rate": 4.058553544054311e-05,
      "loss": 0.7094,
      "step": 1238100
    },
    {
      "epoch": 11.29826994671144,
      "grad_norm": 3.5530059337615967,
      "learning_rate": 4.0584775044407135e-05,
      "loss": 0.7076,
      "step": 1238200
    },
    {
      "epoch": 11.299182422074605,
      "grad_norm": 4.0390753746032715,
      "learning_rate": 4.0584014648271165e-05,
      "loss": 0.6667,
      "step": 1238300
    },
    {
      "epoch": 11.30009489743777,
      "grad_norm": 3.771124839782715,
      "learning_rate": 4.0583254252135195e-05,
      "loss": 0.7109,
      "step": 1238400
    },
    {
      "epoch": 11.301007372800935,
      "grad_norm": 3.5431971549987793,
      "learning_rate": 4.058249385599922e-05,
      "loss": 0.6577,
      "step": 1238500
    },
    {
      "epoch": 11.301919848164099,
      "grad_norm": 2.997021198272705,
      "learning_rate": 4.0581733459863255e-05,
      "loss": 0.6608,
      "step": 1238600
    },
    {
      "epoch": 11.302832323527264,
      "grad_norm": 4.333474159240723,
      "learning_rate": 4.058097306372728e-05,
      "loss": 0.7053,
      "step": 1238700
    },
    {
      "epoch": 11.30374479889043,
      "grad_norm": 4.089588165283203,
      "learning_rate": 4.058021266759131e-05,
      "loss": 0.6397,
      "step": 1238800
    },
    {
      "epoch": 11.304657274253595,
      "grad_norm": 3.9868733882904053,
      "learning_rate": 4.057945227145534e-05,
      "loss": 0.6787,
      "step": 1238900
    },
    {
      "epoch": 11.30556974961676,
      "grad_norm": 4.614152908325195,
      "learning_rate": 4.057869187531937e-05,
      "loss": 0.6647,
      "step": 1239000
    },
    {
      "epoch": 11.306482224979925,
      "grad_norm": 4.379273414611816,
      "learning_rate": 4.057793147918339e-05,
      "loss": 0.6928,
      "step": 1239100
    },
    {
      "epoch": 11.30739470034309,
      "grad_norm": 4.046065330505371,
      "learning_rate": 4.057717108304743e-05,
      "loss": 0.7076,
      "step": 1239200
    },
    {
      "epoch": 11.308307175706256,
      "grad_norm": 3.543490409851074,
      "learning_rate": 4.057641068691145e-05,
      "loss": 0.6895,
      "step": 1239300
    },
    {
      "epoch": 11.309219651069421,
      "grad_norm": 4.090541362762451,
      "learning_rate": 4.057565029077548e-05,
      "loss": 0.6874,
      "step": 1239400
    },
    {
      "epoch": 11.310132126432586,
      "grad_norm": 3.8179640769958496,
      "learning_rate": 4.057488989463951e-05,
      "loss": 0.6849,
      "step": 1239500
    },
    {
      "epoch": 11.311044601795752,
      "grad_norm": 4.431784152984619,
      "learning_rate": 4.057412949850354e-05,
      "loss": 0.677,
      "step": 1239600
    },
    {
      "epoch": 11.311957077158917,
      "grad_norm": 3.605180501937866,
      "learning_rate": 4.057336910236757e-05,
      "loss": 0.7072,
      "step": 1239700
    },
    {
      "epoch": 11.312869552522082,
      "grad_norm": 4.3078718185424805,
      "learning_rate": 4.05726087062316e-05,
      "loss": 0.6857,
      "step": 1239800
    },
    {
      "epoch": 11.313782027885248,
      "grad_norm": 4.213192462921143,
      "learning_rate": 4.0571848310095626e-05,
      "loss": 0.6782,
      "step": 1239900
    },
    {
      "epoch": 11.314694503248413,
      "grad_norm": 4.0251007080078125,
      "learning_rate": 4.057108791395966e-05,
      "loss": 0.6594,
      "step": 1240000
    },
    {
      "epoch": 11.315606978611578,
      "grad_norm": 4.050027370452881,
      "learning_rate": 4.0570327517823686e-05,
      "loss": 0.692,
      "step": 1240100
    },
    {
      "epoch": 11.316519453974742,
      "grad_norm": 4.741575717926025,
      "learning_rate": 4.0569567121687716e-05,
      "loss": 0.6881,
      "step": 1240200
    },
    {
      "epoch": 11.317431929337907,
      "grad_norm": 6.008172035217285,
      "learning_rate": 4.0568806725551746e-05,
      "loss": 0.6972,
      "step": 1240300
    },
    {
      "epoch": 11.318344404701072,
      "grad_norm": 3.6569442749023438,
      "learning_rate": 4.0568046329415776e-05,
      "loss": 0.6782,
      "step": 1240400
    },
    {
      "epoch": 11.319256880064238,
      "grad_norm": 3.3402092456817627,
      "learning_rate": 4.05672859332798e-05,
      "loss": 0.6663,
      "step": 1240500
    },
    {
      "epoch": 11.320169355427403,
      "grad_norm": 4.273677349090576,
      "learning_rate": 4.0566525537143836e-05,
      "loss": 0.6808,
      "step": 1240600
    },
    {
      "epoch": 11.321081830790568,
      "grad_norm": 4.331298828125,
      "learning_rate": 4.056576514100786e-05,
      "loss": 0.7023,
      "step": 1240700
    },
    {
      "epoch": 11.321994306153734,
      "grad_norm": 3.2048025131225586,
      "learning_rate": 4.056500474487189e-05,
      "loss": 0.6543,
      "step": 1240800
    },
    {
      "epoch": 11.322906781516899,
      "grad_norm": 3.104630470275879,
      "learning_rate": 4.056424434873592e-05,
      "loss": 0.6859,
      "step": 1240900
    },
    {
      "epoch": 11.323819256880064,
      "grad_norm": 4.322225093841553,
      "learning_rate": 4.056348395259995e-05,
      "loss": 0.7018,
      "step": 1241000
    },
    {
      "epoch": 11.32473173224323,
      "grad_norm": 3.8436830043792725,
      "learning_rate": 4.056272355646398e-05,
      "loss": 0.7136,
      "step": 1241100
    },
    {
      "epoch": 11.325644207606395,
      "grad_norm": 3.944516897201538,
      "learning_rate": 4.0561963160328e-05,
      "loss": 0.6903,
      "step": 1241200
    },
    {
      "epoch": 11.32655668296956,
      "grad_norm": 4.0668487548828125,
      "learning_rate": 4.056120276419203e-05,
      "loss": 0.6899,
      "step": 1241300
    },
    {
      "epoch": 11.327469158332725,
      "grad_norm": 3.436347007751465,
      "learning_rate": 4.0560442368056063e-05,
      "loss": 0.6603,
      "step": 1241400
    },
    {
      "epoch": 11.32838163369589,
      "grad_norm": 3.9721951484680176,
      "learning_rate": 4.0559681971920093e-05,
      "loss": 0.6736,
      "step": 1241500
    },
    {
      "epoch": 11.329294109059056,
      "grad_norm": 3.967672824859619,
      "learning_rate": 4.055892157578412e-05,
      "loss": 0.684,
      "step": 1241600
    },
    {
      "epoch": 11.330206584422221,
      "grad_norm": 4.5921454429626465,
      "learning_rate": 4.0558161179648154e-05,
      "loss": 0.7183,
      "step": 1241700
    },
    {
      "epoch": 11.331119059785387,
      "grad_norm": 3.958467960357666,
      "learning_rate": 4.055740078351218e-05,
      "loss": 0.7104,
      "step": 1241800
    },
    {
      "epoch": 11.332031535148552,
      "grad_norm": 3.542004108428955,
      "learning_rate": 4.055664038737621e-05,
      "loss": 0.6671,
      "step": 1241900
    },
    {
      "epoch": 11.332944010511715,
      "grad_norm": 4.458438396453857,
      "learning_rate": 4.055587999124024e-05,
      "loss": 0.6778,
      "step": 1242000
    },
    {
      "epoch": 11.33385648587488,
      "grad_norm": 3.696819543838501,
      "learning_rate": 4.055511959510427e-05,
      "loss": 0.6406,
      "step": 1242100
    },
    {
      "epoch": 11.334768961238046,
      "grad_norm": 3.4737539291381836,
      "learning_rate": 4.05543591989683e-05,
      "loss": 0.6699,
      "step": 1242200
    },
    {
      "epoch": 11.335681436601211,
      "grad_norm": 2.888718366622925,
      "learning_rate": 4.055359880283233e-05,
      "loss": 0.7073,
      "step": 1242300
    },
    {
      "epoch": 11.336593911964377,
      "grad_norm": 4.228734970092773,
      "learning_rate": 4.055283840669635e-05,
      "loss": 0.6828,
      "step": 1242400
    },
    {
      "epoch": 11.337506387327542,
      "grad_norm": 3.962184190750122,
      "learning_rate": 4.055207801056039e-05,
      "loss": 0.6405,
      "step": 1242500
    },
    {
      "epoch": 11.338418862690707,
      "grad_norm": 3.833376407623291,
      "learning_rate": 4.055131761442441e-05,
      "loss": 0.688,
      "step": 1242600
    },
    {
      "epoch": 11.339331338053872,
      "grad_norm": 4.366915225982666,
      "learning_rate": 4.055055721828844e-05,
      "loss": 0.6681,
      "step": 1242700
    },
    {
      "epoch": 11.340243813417038,
      "grad_norm": 4.169950008392334,
      "learning_rate": 4.054979682215247e-05,
      "loss": 0.694,
      "step": 1242800
    },
    {
      "epoch": 11.341156288780203,
      "grad_norm": 3.8043577671051025,
      "learning_rate": 4.05490364260165e-05,
      "loss": 0.6905,
      "step": 1242900
    },
    {
      "epoch": 11.342068764143368,
      "grad_norm": 3.5145199298858643,
      "learning_rate": 4.0548276029880524e-05,
      "loss": 0.689,
      "step": 1243000
    },
    {
      "epoch": 11.342981239506534,
      "grad_norm": 3.593088150024414,
      "learning_rate": 4.054751563374456e-05,
      "loss": 0.6604,
      "step": 1243100
    },
    {
      "epoch": 11.343893714869699,
      "grad_norm": 3.722212314605713,
      "learning_rate": 4.0546755237608584e-05,
      "loss": 0.6834,
      "step": 1243200
    },
    {
      "epoch": 11.344806190232864,
      "grad_norm": 4.020732879638672,
      "learning_rate": 4.0545994841472614e-05,
      "loss": 0.6786,
      "step": 1243300
    },
    {
      "epoch": 11.34571866559603,
      "grad_norm": 4.473577499389648,
      "learning_rate": 4.0545234445336644e-05,
      "loss": 0.6983,
      "step": 1243400
    },
    {
      "epoch": 11.346631140959195,
      "grad_norm": 3.8189637660980225,
      "learning_rate": 4.0544474049200675e-05,
      "loss": 0.6704,
      "step": 1243500
    },
    {
      "epoch": 11.347543616322358,
      "grad_norm": 3.7939717769622803,
      "learning_rate": 4.0543713653064705e-05,
      "loss": 0.6597,
      "step": 1243600
    },
    {
      "epoch": 11.348456091685524,
      "grad_norm": 2.9562125205993652,
      "learning_rate": 4.0542953256928735e-05,
      "loss": 0.6623,
      "step": 1243700
    },
    {
      "epoch": 11.349368567048689,
      "grad_norm": 3.9652669429779053,
      "learning_rate": 4.054219286079276e-05,
      "loss": 0.6776,
      "step": 1243800
    },
    {
      "epoch": 11.350281042411854,
      "grad_norm": 3.9271318912506104,
      "learning_rate": 4.0541432464656795e-05,
      "loss": 0.6689,
      "step": 1243900
    },
    {
      "epoch": 11.35119351777502,
      "grad_norm": 4.398287773132324,
      "learning_rate": 4.054067206852082e-05,
      "loss": 0.6706,
      "step": 1244000
    },
    {
      "epoch": 11.352105993138185,
      "grad_norm": 4.1967453956604,
      "learning_rate": 4.053991167238485e-05,
      "loss": 0.6486,
      "step": 1244100
    },
    {
      "epoch": 11.35301846850135,
      "grad_norm": 4.618239879608154,
      "learning_rate": 4.053915127624888e-05,
      "loss": 0.6573,
      "step": 1244200
    },
    {
      "epoch": 11.353930943864516,
      "grad_norm": 4.200567245483398,
      "learning_rate": 4.05383908801129e-05,
      "loss": 0.6758,
      "step": 1244300
    },
    {
      "epoch": 11.35484341922768,
      "grad_norm": 7.561784744262695,
      "learning_rate": 4.053763048397694e-05,
      "loss": 0.6402,
      "step": 1244400
    },
    {
      "epoch": 11.355755894590846,
      "grad_norm": 4.199094295501709,
      "learning_rate": 4.053687008784096e-05,
      "loss": 0.6814,
      "step": 1244500
    },
    {
      "epoch": 11.356668369954011,
      "grad_norm": 3.4991343021392822,
      "learning_rate": 4.053610969170499e-05,
      "loss": 0.6618,
      "step": 1244600
    },
    {
      "epoch": 11.357580845317177,
      "grad_norm": 3.834984302520752,
      "learning_rate": 4.053534929556902e-05,
      "loss": 0.687,
      "step": 1244700
    },
    {
      "epoch": 11.358493320680342,
      "grad_norm": 3.7526185512542725,
      "learning_rate": 4.053458889943305e-05,
      "loss": 0.6704,
      "step": 1244800
    },
    {
      "epoch": 11.359405796043507,
      "grad_norm": 4.656962871551514,
      "learning_rate": 4.0533828503297075e-05,
      "loss": 0.6795,
      "step": 1244900
    },
    {
      "epoch": 11.360318271406673,
      "grad_norm": 4.379597187042236,
      "learning_rate": 4.053306810716111e-05,
      "loss": 0.6704,
      "step": 1245000
    },
    {
      "epoch": 11.361230746769838,
      "grad_norm": 3.817061424255371,
      "learning_rate": 4.0532307711025135e-05,
      "loss": 0.6609,
      "step": 1245100
    },
    {
      "epoch": 11.362143222133003,
      "grad_norm": 4.330591678619385,
      "learning_rate": 4.0531547314889165e-05,
      "loss": 0.6881,
      "step": 1245200
    },
    {
      "epoch": 11.363055697496169,
      "grad_norm": 3.796142101287842,
      "learning_rate": 4.0530786918753195e-05,
      "loss": 0.6549,
      "step": 1245300
    },
    {
      "epoch": 11.363968172859332,
      "grad_norm": 4.037001609802246,
      "learning_rate": 4.0530026522617225e-05,
      "loss": 0.6436,
      "step": 1245400
    },
    {
      "epoch": 11.364880648222497,
      "grad_norm": 4.187772274017334,
      "learning_rate": 4.0529266126481256e-05,
      "loss": 0.6843,
      "step": 1245500
    },
    {
      "epoch": 11.365793123585663,
      "grad_norm": 4.562337398529053,
      "learning_rate": 4.0528505730345286e-05,
      "loss": 0.6918,
      "step": 1245600
    },
    {
      "epoch": 11.366705598948828,
      "grad_norm": 3.6972501277923584,
      "learning_rate": 4.052774533420931e-05,
      "loss": 0.669,
      "step": 1245700
    },
    {
      "epoch": 11.367618074311993,
      "grad_norm": 3.960566759109497,
      "learning_rate": 4.0526984938073346e-05,
      "loss": 0.661,
      "step": 1245800
    },
    {
      "epoch": 11.368530549675159,
      "grad_norm": 4.337591648101807,
      "learning_rate": 4.052622454193737e-05,
      "loss": 0.6754,
      "step": 1245900
    },
    {
      "epoch": 11.369443025038324,
      "grad_norm": 4.055062770843506,
      "learning_rate": 4.05254641458014e-05,
      "loss": 0.6464,
      "step": 1246000
    },
    {
      "epoch": 11.37035550040149,
      "grad_norm": 3.8191757202148438,
      "learning_rate": 4.052470374966543e-05,
      "loss": 0.6737,
      "step": 1246100
    },
    {
      "epoch": 11.371267975764654,
      "grad_norm": 3.502833604812622,
      "learning_rate": 4.052394335352946e-05,
      "loss": 0.6783,
      "step": 1246200
    },
    {
      "epoch": 11.37218045112782,
      "grad_norm": 4.282728672027588,
      "learning_rate": 4.052318295739348e-05,
      "loss": 0.6762,
      "step": 1246300
    },
    {
      "epoch": 11.373092926490985,
      "grad_norm": 4.317743301391602,
      "learning_rate": 4.052242256125752e-05,
      "loss": 0.6894,
      "step": 1246400
    },
    {
      "epoch": 11.37400540185415,
      "grad_norm": 3.9533848762512207,
      "learning_rate": 4.052166216512154e-05,
      "loss": 0.6646,
      "step": 1246500
    },
    {
      "epoch": 11.374917877217316,
      "grad_norm": 3.415687322616577,
      "learning_rate": 4.052090176898557e-05,
      "loss": 0.6864,
      "step": 1246600
    },
    {
      "epoch": 11.375830352580481,
      "grad_norm": 4.739127159118652,
      "learning_rate": 4.05201413728496e-05,
      "loss": 0.7042,
      "step": 1246700
    },
    {
      "epoch": 11.376742827943646,
      "grad_norm": 5.328426361083984,
      "learning_rate": 4.051938097671363e-05,
      "loss": 0.6335,
      "step": 1246800
    },
    {
      "epoch": 11.377655303306812,
      "grad_norm": 3.9481136798858643,
      "learning_rate": 4.051862058057766e-05,
      "loss": 0.6667,
      "step": 1246900
    },
    {
      "epoch": 11.378567778669975,
      "grad_norm": 3.277719020843506,
      "learning_rate": 4.0517860184441686e-05,
      "loss": 0.6807,
      "step": 1247000
    },
    {
      "epoch": 11.37948025403314,
      "grad_norm": 3.640887498855591,
      "learning_rate": 4.0517099788305716e-05,
      "loss": 0.6494,
      "step": 1247100
    },
    {
      "epoch": 11.380392729396306,
      "grad_norm": 5.400973320007324,
      "learning_rate": 4.0516339392169746e-05,
      "loss": 0.6934,
      "step": 1247200
    },
    {
      "epoch": 11.381305204759471,
      "grad_norm": 4.196345329284668,
      "learning_rate": 4.0515578996033776e-05,
      "loss": 0.7188,
      "step": 1247300
    },
    {
      "epoch": 11.382217680122636,
      "grad_norm": 4.393205642700195,
      "learning_rate": 4.05148185998978e-05,
      "loss": 0.6915,
      "step": 1247400
    },
    {
      "epoch": 11.383130155485802,
      "grad_norm": 3.359405994415283,
      "learning_rate": 4.0514058203761837e-05,
      "loss": 0.6516,
      "step": 1247500
    },
    {
      "epoch": 11.384042630848967,
      "grad_norm": 4.841004848480225,
      "learning_rate": 4.051329780762586e-05,
      "loss": 0.6668,
      "step": 1247600
    },
    {
      "epoch": 11.384955106212132,
      "grad_norm": 3.687098741531372,
      "learning_rate": 4.051253741148989e-05,
      "loss": 0.6977,
      "step": 1247700
    },
    {
      "epoch": 11.385867581575297,
      "grad_norm": 4.559927940368652,
      "learning_rate": 4.051177701535392e-05,
      "loss": 0.6875,
      "step": 1247800
    },
    {
      "epoch": 11.386780056938463,
      "grad_norm": 3.551468849182129,
      "learning_rate": 4.051101661921795e-05,
      "loss": 0.6829,
      "step": 1247900
    },
    {
      "epoch": 11.387692532301628,
      "grad_norm": 4.076393127441406,
      "learning_rate": 4.051025622308198e-05,
      "loss": 0.6569,
      "step": 1248000
    },
    {
      "epoch": 11.388605007664793,
      "grad_norm": 4.658052444458008,
      "learning_rate": 4.050949582694601e-05,
      "loss": 0.6722,
      "step": 1248100
    },
    {
      "epoch": 11.389517483027959,
      "grad_norm": 4.470733642578125,
      "learning_rate": 4.0508735430810033e-05,
      "loss": 0.6792,
      "step": 1248200
    },
    {
      "epoch": 11.390429958391124,
      "grad_norm": 4.5295233726501465,
      "learning_rate": 4.050797503467407e-05,
      "loss": 0.6686,
      "step": 1248300
    },
    {
      "epoch": 11.39134243375429,
      "grad_norm": 3.8882803916931152,
      "learning_rate": 4.0507214638538094e-05,
      "loss": 0.6912,
      "step": 1248400
    },
    {
      "epoch": 11.392254909117455,
      "grad_norm": 3.515268564224243,
      "learning_rate": 4.0506454242402124e-05,
      "loss": 0.6744,
      "step": 1248500
    },
    {
      "epoch": 11.39316738448062,
      "grad_norm": 4.566099166870117,
      "learning_rate": 4.0505693846266154e-05,
      "loss": 0.7044,
      "step": 1248600
    },
    {
      "epoch": 11.394079859843785,
      "grad_norm": 3.0190112590789795,
      "learning_rate": 4.0504933450130184e-05,
      "loss": 0.6763,
      "step": 1248700
    },
    {
      "epoch": 11.394992335206949,
      "grad_norm": 3.267000913619995,
      "learning_rate": 4.050417305399421e-05,
      "loss": 0.6982,
      "step": 1248800
    },
    {
      "epoch": 11.395904810570114,
      "grad_norm": 4.170365333557129,
      "learning_rate": 4.0503412657858244e-05,
      "loss": 0.6616,
      "step": 1248900
    },
    {
      "epoch": 11.39681728593328,
      "grad_norm": 3.5895979404449463,
      "learning_rate": 4.050265226172227e-05,
      "loss": 0.6763,
      "step": 1249000
    },
    {
      "epoch": 11.397729761296445,
      "grad_norm": 4.179999351501465,
      "learning_rate": 4.05018918655863e-05,
      "loss": 0.676,
      "step": 1249100
    },
    {
      "epoch": 11.39864223665961,
      "grad_norm": 4.103116989135742,
      "learning_rate": 4.050113146945033e-05,
      "loss": 0.711,
      "step": 1249200
    },
    {
      "epoch": 11.399554712022775,
      "grad_norm": 3.367971897125244,
      "learning_rate": 4.050037107331436e-05,
      "loss": 0.6387,
      "step": 1249300
    },
    {
      "epoch": 11.40046718738594,
      "grad_norm": 4.608523368835449,
      "learning_rate": 4.049961067717839e-05,
      "loss": 0.6544,
      "step": 1249400
    },
    {
      "epoch": 11.401379662749106,
      "grad_norm": 3.674985647201538,
      "learning_rate": 4.049885028104242e-05,
      "loss": 0.7032,
      "step": 1249500
    },
    {
      "epoch": 11.402292138112271,
      "grad_norm": 3.451788902282715,
      "learning_rate": 4.049808988490644e-05,
      "loss": 0.6963,
      "step": 1249600
    },
    {
      "epoch": 11.403204613475436,
      "grad_norm": 3.5465142726898193,
      "learning_rate": 4.049732948877047e-05,
      "loss": 0.6721,
      "step": 1249700
    },
    {
      "epoch": 11.404117088838602,
      "grad_norm": 4.614711284637451,
      "learning_rate": 4.04965690926345e-05,
      "loss": 0.7286,
      "step": 1249800
    },
    {
      "epoch": 11.405029564201767,
      "grad_norm": 4.195858955383301,
      "learning_rate": 4.0495808696498524e-05,
      "loss": 0.668,
      "step": 1249900
    },
    {
      "epoch": 11.405942039564932,
      "grad_norm": 3.1082170009613037,
      "learning_rate": 4.049504830036256e-05,
      "loss": 0.6892,
      "step": 1250000
    },
    {
      "epoch": 11.406854514928098,
      "grad_norm": 2.8608572483062744,
      "learning_rate": 4.0494287904226584e-05,
      "loss": 0.711,
      "step": 1250100
    },
    {
      "epoch": 11.407766990291263,
      "grad_norm": 5.103142738342285,
      "learning_rate": 4.0493527508090614e-05,
      "loss": 0.6639,
      "step": 1250200
    },
    {
      "epoch": 11.408679465654428,
      "grad_norm": 3.60402512550354,
      "learning_rate": 4.0492767111954645e-05,
      "loss": 0.6736,
      "step": 1250300
    },
    {
      "epoch": 11.409591941017592,
      "grad_norm": 3.992849588394165,
      "learning_rate": 4.0492006715818675e-05,
      "loss": 0.6823,
      "step": 1250400
    },
    {
      "epoch": 11.410504416380757,
      "grad_norm": 3.7312586307525635,
      "learning_rate": 4.0491246319682705e-05,
      "loss": 0.6615,
      "step": 1250500
    },
    {
      "epoch": 11.411416891743922,
      "grad_norm": 3.6840384006500244,
      "learning_rate": 4.0490485923546735e-05,
      "loss": 0.6984,
      "step": 1250600
    },
    {
      "epoch": 11.412329367107088,
      "grad_norm": 4.767989158630371,
      "learning_rate": 4.048972552741076e-05,
      "loss": 0.6348,
      "step": 1250700
    },
    {
      "epoch": 11.413241842470253,
      "grad_norm": 4.097606182098389,
      "learning_rate": 4.0488965131274795e-05,
      "loss": 0.6705,
      "step": 1250800
    },
    {
      "epoch": 11.414154317833418,
      "grad_norm": 3.9815049171447754,
      "learning_rate": 4.048820473513882e-05,
      "loss": 0.7084,
      "step": 1250900
    },
    {
      "epoch": 11.415066793196583,
      "grad_norm": 4.1173906326293945,
      "learning_rate": 4.048744433900285e-05,
      "loss": 0.6506,
      "step": 1251000
    },
    {
      "epoch": 11.415979268559749,
      "grad_norm": 4.028141498565674,
      "learning_rate": 4.048668394286688e-05,
      "loss": 0.6835,
      "step": 1251100
    },
    {
      "epoch": 11.416891743922914,
      "grad_norm": 4.546876907348633,
      "learning_rate": 4.048592354673091e-05,
      "loss": 0.6811,
      "step": 1251200
    },
    {
      "epoch": 11.41780421928608,
      "grad_norm": 3.3718862533569336,
      "learning_rate": 4.048516315059493e-05,
      "loss": 0.6741,
      "step": 1251300
    },
    {
      "epoch": 11.418716694649245,
      "grad_norm": 3.102890729904175,
      "learning_rate": 4.048440275445897e-05,
      "loss": 0.6782,
      "step": 1251400
    },
    {
      "epoch": 11.41962917001241,
      "grad_norm": 5.444402694702148,
      "learning_rate": 4.048364235832299e-05,
      "loss": 0.682,
      "step": 1251500
    },
    {
      "epoch": 11.420541645375575,
      "grad_norm": 4.145523548126221,
      "learning_rate": 4.048288196218702e-05,
      "loss": 0.696,
      "step": 1251600
    },
    {
      "epoch": 11.42145412073874,
      "grad_norm": 4.3329010009765625,
      "learning_rate": 4.048212156605105e-05,
      "loss": 0.6615,
      "step": 1251700
    },
    {
      "epoch": 11.422366596101906,
      "grad_norm": 3.6129183769226074,
      "learning_rate": 4.048136116991508e-05,
      "loss": 0.7038,
      "step": 1251800
    },
    {
      "epoch": 11.423279071465071,
      "grad_norm": 4.210919380187988,
      "learning_rate": 4.048060077377911e-05,
      "loss": 0.6873,
      "step": 1251900
    },
    {
      "epoch": 11.424191546828236,
      "grad_norm": 2.529468297958374,
      "learning_rate": 4.047984037764314e-05,
      "loss": 0.6426,
      "step": 1252000
    },
    {
      "epoch": 11.4251040221914,
      "grad_norm": 4.4720892906188965,
      "learning_rate": 4.0479079981507165e-05,
      "loss": 0.6643,
      "step": 1252100
    },
    {
      "epoch": 11.426016497554565,
      "grad_norm": 3.450706720352173,
      "learning_rate": 4.04783195853712e-05,
      "loss": 0.6834,
      "step": 1252200
    },
    {
      "epoch": 11.42692897291773,
      "grad_norm": 4.099541187286377,
      "learning_rate": 4.0477559189235226e-05,
      "loss": 0.6563,
      "step": 1252300
    },
    {
      "epoch": 11.427841448280896,
      "grad_norm": 4.064407825469971,
      "learning_rate": 4.0476798793099256e-05,
      "loss": 0.6731,
      "step": 1252400
    },
    {
      "epoch": 11.428753923644061,
      "grad_norm": 3.542449474334717,
      "learning_rate": 4.0476038396963286e-05,
      "loss": 0.7039,
      "step": 1252500
    },
    {
      "epoch": 11.429666399007226,
      "grad_norm": 2.439638137817383,
      "learning_rate": 4.047527800082731e-05,
      "loss": 0.6862,
      "step": 1252600
    },
    {
      "epoch": 11.430578874370392,
      "grad_norm": 4.397129535675049,
      "learning_rate": 4.047451760469134e-05,
      "loss": 0.6625,
      "step": 1252700
    },
    {
      "epoch": 11.431491349733557,
      "grad_norm": 3.468719959259033,
      "learning_rate": 4.047375720855537e-05,
      "loss": 0.6826,
      "step": 1252800
    },
    {
      "epoch": 11.432403825096722,
      "grad_norm": 3.8504467010498047,
      "learning_rate": 4.04729968124194e-05,
      "loss": 0.6438,
      "step": 1252900
    },
    {
      "epoch": 11.433316300459888,
      "grad_norm": 4.432097434997559,
      "learning_rate": 4.047223641628343e-05,
      "loss": 0.6727,
      "step": 1253000
    },
    {
      "epoch": 11.434228775823053,
      "grad_norm": 4.751587867736816,
      "learning_rate": 4.047147602014746e-05,
      "loss": 0.6389,
      "step": 1253100
    },
    {
      "epoch": 11.435141251186218,
      "grad_norm": 4.632504463195801,
      "learning_rate": 4.047071562401148e-05,
      "loss": 0.6871,
      "step": 1253200
    },
    {
      "epoch": 11.436053726549384,
      "grad_norm": 4.460766792297363,
      "learning_rate": 4.046995522787552e-05,
      "loss": 0.6906,
      "step": 1253300
    },
    {
      "epoch": 11.436966201912549,
      "grad_norm": 4.5837907791137695,
      "learning_rate": 4.046919483173954e-05,
      "loss": 0.6577,
      "step": 1253400
    },
    {
      "epoch": 11.437878677275714,
      "grad_norm": 3.5694217681884766,
      "learning_rate": 4.046843443560357e-05,
      "loss": 0.7029,
      "step": 1253500
    },
    {
      "epoch": 11.43879115263888,
      "grad_norm": 4.035702705383301,
      "learning_rate": 4.04676740394676e-05,
      "loss": 0.6884,
      "step": 1253600
    },
    {
      "epoch": 11.439703628002045,
      "grad_norm": 4.053528785705566,
      "learning_rate": 4.046691364333163e-05,
      "loss": 0.6981,
      "step": 1253700
    },
    {
      "epoch": 11.440616103365208,
      "grad_norm": 3.1069839000701904,
      "learning_rate": 4.0466153247195656e-05,
      "loss": 0.7001,
      "step": 1253800
    },
    {
      "epoch": 11.441528578728374,
      "grad_norm": 3.6345837116241455,
      "learning_rate": 4.046539285105969e-05,
      "loss": 0.686,
      "step": 1253900
    },
    {
      "epoch": 11.442441054091539,
      "grad_norm": 3.469761848449707,
      "learning_rate": 4.0464632454923716e-05,
      "loss": 0.6333,
      "step": 1254000
    },
    {
      "epoch": 11.443353529454704,
      "grad_norm": 4.666431903839111,
      "learning_rate": 4.0463872058787746e-05,
      "loss": 0.6716,
      "step": 1254100
    },
    {
      "epoch": 11.44426600481787,
      "grad_norm": 3.9180264472961426,
      "learning_rate": 4.0463111662651777e-05,
      "loss": 0.6732,
      "step": 1254200
    },
    {
      "epoch": 11.445178480181035,
      "grad_norm": 3.1407406330108643,
      "learning_rate": 4.0462351266515807e-05,
      "loss": 0.6712,
      "step": 1254300
    },
    {
      "epoch": 11.4460909555442,
      "grad_norm": 4.2945027351379395,
      "learning_rate": 4.046159087037984e-05,
      "loss": 0.7024,
      "step": 1254400
    },
    {
      "epoch": 11.447003430907365,
      "grad_norm": 2.0855603218078613,
      "learning_rate": 4.046083047424387e-05,
      "loss": 0.6651,
      "step": 1254500
    },
    {
      "epoch": 11.44791590627053,
      "grad_norm": 3.6564455032348633,
      "learning_rate": 4.046007007810789e-05,
      "loss": 0.6894,
      "step": 1254600
    },
    {
      "epoch": 11.448828381633696,
      "grad_norm": 3.792576551437378,
      "learning_rate": 4.045930968197193e-05,
      "loss": 0.6803,
      "step": 1254700
    },
    {
      "epoch": 11.449740856996861,
      "grad_norm": 3.9488167762756348,
      "learning_rate": 4.045854928583595e-05,
      "loss": 0.7039,
      "step": 1254800
    },
    {
      "epoch": 11.450653332360027,
      "grad_norm": 4.223462104797363,
      "learning_rate": 4.045778888969998e-05,
      "loss": 0.675,
      "step": 1254900
    },
    {
      "epoch": 11.451565807723192,
      "grad_norm": 3.941082239151001,
      "learning_rate": 4.045702849356401e-05,
      "loss": 0.6656,
      "step": 1255000
    },
    {
      "epoch": 11.452478283086357,
      "grad_norm": 2.973940372467041,
      "learning_rate": 4.045626809742804e-05,
      "loss": 0.6932,
      "step": 1255100
    },
    {
      "epoch": 11.453390758449522,
      "grad_norm": 3.9198238849639893,
      "learning_rate": 4.0455507701292064e-05,
      "loss": 0.7055,
      "step": 1255200
    },
    {
      "epoch": 11.454303233812688,
      "grad_norm": 4.351856231689453,
      "learning_rate": 4.04547473051561e-05,
      "loss": 0.6613,
      "step": 1255300
    },
    {
      "epoch": 11.455215709175853,
      "grad_norm": 4.863155841827393,
      "learning_rate": 4.0453986909020124e-05,
      "loss": 0.6855,
      "step": 1255400
    },
    {
      "epoch": 11.456128184539017,
      "grad_norm": 4.453055381774902,
      "learning_rate": 4.0453226512884154e-05,
      "loss": 0.7002,
      "step": 1255500
    },
    {
      "epoch": 11.457040659902182,
      "grad_norm": 4.257098197937012,
      "learning_rate": 4.0452466116748184e-05,
      "loss": 0.69,
      "step": 1255600
    },
    {
      "epoch": 11.457953135265347,
      "grad_norm": 5.337782382965088,
      "learning_rate": 4.045170572061221e-05,
      "loss": 0.685,
      "step": 1255700
    },
    {
      "epoch": 11.458865610628512,
      "grad_norm": 3.483729600906372,
      "learning_rate": 4.0450945324476244e-05,
      "loss": 0.6817,
      "step": 1255800
    },
    {
      "epoch": 11.459778085991678,
      "grad_norm": 3.696183204650879,
      "learning_rate": 4.045018492834027e-05,
      "loss": 0.6546,
      "step": 1255900
    },
    {
      "epoch": 11.460690561354843,
      "grad_norm": 3.75166916847229,
      "learning_rate": 4.04494245322043e-05,
      "loss": 0.6859,
      "step": 1256000
    },
    {
      "epoch": 11.461603036718008,
      "grad_norm": 4.593221664428711,
      "learning_rate": 4.044866413606833e-05,
      "loss": 0.678,
      "step": 1256100
    },
    {
      "epoch": 11.462515512081174,
      "grad_norm": 3.9544057846069336,
      "learning_rate": 4.044790373993236e-05,
      "loss": 0.6934,
      "step": 1256200
    },
    {
      "epoch": 11.463427987444339,
      "grad_norm": 3.8577194213867188,
      "learning_rate": 4.044714334379639e-05,
      "loss": 0.6793,
      "step": 1256300
    },
    {
      "epoch": 11.464340462807504,
      "grad_norm": 3.418442964553833,
      "learning_rate": 4.044638294766042e-05,
      "loss": 0.6578,
      "step": 1256400
    },
    {
      "epoch": 11.46525293817067,
      "grad_norm": 3.914750099182129,
      "learning_rate": 4.044562255152444e-05,
      "loss": 0.6781,
      "step": 1256500
    },
    {
      "epoch": 11.466165413533835,
      "grad_norm": 4.110756874084473,
      "learning_rate": 4.044486215538847e-05,
      "loss": 0.6975,
      "step": 1256600
    },
    {
      "epoch": 11.467077888897,
      "grad_norm": 4.0472822189331055,
      "learning_rate": 4.04441017592525e-05,
      "loss": 0.6796,
      "step": 1256700
    },
    {
      "epoch": 11.467990364260165,
      "grad_norm": 3.0504648685455322,
      "learning_rate": 4.044334136311653e-05,
      "loss": 0.6375,
      "step": 1256800
    },
    {
      "epoch": 11.46890283962333,
      "grad_norm": 3.3044679164886475,
      "learning_rate": 4.044258096698056e-05,
      "loss": 0.6436,
      "step": 1256900
    },
    {
      "epoch": 11.469815314986496,
      "grad_norm": 3.3920040130615234,
      "learning_rate": 4.044182057084459e-05,
      "loss": 0.6731,
      "step": 1257000
    },
    {
      "epoch": 11.470727790349661,
      "grad_norm": 3.4105539321899414,
      "learning_rate": 4.0441060174708615e-05,
      "loss": 0.6867,
      "step": 1257100
    },
    {
      "epoch": 11.471640265712825,
      "grad_norm": 2.3487961292266846,
      "learning_rate": 4.044029977857265e-05,
      "loss": 0.6607,
      "step": 1257200
    },
    {
      "epoch": 11.47255274107599,
      "grad_norm": 4.4582672119140625,
      "learning_rate": 4.0439539382436675e-05,
      "loss": 0.7251,
      "step": 1257300
    },
    {
      "epoch": 11.473465216439156,
      "grad_norm": 3.8204236030578613,
      "learning_rate": 4.0438778986300705e-05,
      "loss": 0.6414,
      "step": 1257400
    },
    {
      "epoch": 11.47437769180232,
      "grad_norm": 3.7845520973205566,
      "learning_rate": 4.0438018590164735e-05,
      "loss": 0.6689,
      "step": 1257500
    },
    {
      "epoch": 11.475290167165486,
      "grad_norm": 3.7602741718292236,
      "learning_rate": 4.0437258194028765e-05,
      "loss": 0.708,
      "step": 1257600
    },
    {
      "epoch": 11.476202642528651,
      "grad_norm": 5.411746025085449,
      "learning_rate": 4.0436497797892795e-05,
      "loss": 0.714,
      "step": 1257700
    },
    {
      "epoch": 11.477115117891817,
      "grad_norm": 4.3742194175720215,
      "learning_rate": 4.0435737401756825e-05,
      "loss": 0.6941,
      "step": 1257800
    },
    {
      "epoch": 11.478027593254982,
      "grad_norm": 4.036316394805908,
      "learning_rate": 4.043497700562085e-05,
      "loss": 0.649,
      "step": 1257900
    },
    {
      "epoch": 11.478940068618147,
      "grad_norm": 3.8098549842834473,
      "learning_rate": 4.043421660948488e-05,
      "loss": 0.7052,
      "step": 1258000
    },
    {
      "epoch": 11.479852543981313,
      "grad_norm": 4.6894354820251465,
      "learning_rate": 4.043345621334891e-05,
      "loss": 0.6825,
      "step": 1258100
    },
    {
      "epoch": 11.480765019344478,
      "grad_norm": 2.996039390563965,
      "learning_rate": 4.043269581721293e-05,
      "loss": 0.6547,
      "step": 1258200
    },
    {
      "epoch": 11.481677494707643,
      "grad_norm": 4.312194347381592,
      "learning_rate": 4.043193542107697e-05,
      "loss": 0.6984,
      "step": 1258300
    },
    {
      "epoch": 11.482589970070809,
      "grad_norm": 3.94417405128479,
      "learning_rate": 4.043117502494099e-05,
      "loss": 0.674,
      "step": 1258400
    },
    {
      "epoch": 11.483502445433974,
      "grad_norm": 2.4667956829071045,
      "learning_rate": 4.043041462880502e-05,
      "loss": 0.7025,
      "step": 1258500
    },
    {
      "epoch": 11.484414920797139,
      "grad_norm": 2.7952218055725098,
      "learning_rate": 4.042965423266905e-05,
      "loss": 0.7162,
      "step": 1258600
    },
    {
      "epoch": 11.485327396160304,
      "grad_norm": 3.594623327255249,
      "learning_rate": 4.042889383653308e-05,
      "loss": 0.7181,
      "step": 1258700
    },
    {
      "epoch": 11.48623987152347,
      "grad_norm": 4.26820182800293,
      "learning_rate": 4.042813344039711e-05,
      "loss": 0.6919,
      "step": 1258800
    },
    {
      "epoch": 11.487152346886633,
      "grad_norm": 3.3467538356781006,
      "learning_rate": 4.042737304426114e-05,
      "loss": 0.6614,
      "step": 1258900
    },
    {
      "epoch": 11.488064822249799,
      "grad_norm": 4.057042121887207,
      "learning_rate": 4.0426612648125166e-05,
      "loss": 0.6548,
      "step": 1259000
    },
    {
      "epoch": 11.488977297612964,
      "grad_norm": 4.095626354217529,
      "learning_rate": 4.04258522519892e-05,
      "loss": 0.6955,
      "step": 1259100
    },
    {
      "epoch": 11.48988977297613,
      "grad_norm": 4.425443172454834,
      "learning_rate": 4.0425091855853226e-05,
      "loss": 0.6549,
      "step": 1259200
    },
    {
      "epoch": 11.490802248339294,
      "grad_norm": 3.946049690246582,
      "learning_rate": 4.0424331459717256e-05,
      "loss": 0.6805,
      "step": 1259300
    },
    {
      "epoch": 11.49171472370246,
      "grad_norm": 4.146040439605713,
      "learning_rate": 4.0423571063581286e-05,
      "loss": 0.6588,
      "step": 1259400
    },
    {
      "epoch": 11.492627199065625,
      "grad_norm": 3.004460573196411,
      "learning_rate": 4.0422810667445316e-05,
      "loss": 0.6963,
      "step": 1259500
    },
    {
      "epoch": 11.49353967442879,
      "grad_norm": 3.382550001144409,
      "learning_rate": 4.042205027130934e-05,
      "loss": 0.6675,
      "step": 1259600
    },
    {
      "epoch": 11.494452149791956,
      "grad_norm": 4.6309895515441895,
      "learning_rate": 4.0421289875173376e-05,
      "loss": 0.7217,
      "step": 1259700
    },
    {
      "epoch": 11.495364625155121,
      "grad_norm": 3.7238025665283203,
      "learning_rate": 4.04205294790374e-05,
      "loss": 0.6831,
      "step": 1259800
    },
    {
      "epoch": 11.496277100518286,
      "grad_norm": 3.656076192855835,
      "learning_rate": 4.041976908290143e-05,
      "loss": 0.7194,
      "step": 1259900
    },
    {
      "epoch": 11.497189575881452,
      "grad_norm": 3.930875062942505,
      "learning_rate": 4.041900868676546e-05,
      "loss": 0.6667,
      "step": 1260000
    },
    {
      "epoch": 11.498102051244617,
      "grad_norm": 3.7584264278411865,
      "learning_rate": 4.041824829062949e-05,
      "loss": 0.6489,
      "step": 1260100
    },
    {
      "epoch": 11.499014526607782,
      "grad_norm": 4.137409687042236,
      "learning_rate": 4.041748789449352e-05,
      "loss": 0.6464,
      "step": 1260200
    },
    {
      "epoch": 11.499927001970947,
      "grad_norm": 3.3427207469940186,
      "learning_rate": 4.041672749835755e-05,
      "loss": 0.6868,
      "step": 1260300
    },
    {
      "epoch": 11.500839477334113,
      "grad_norm": 3.8281145095825195,
      "learning_rate": 4.041596710222157e-05,
      "loss": 0.6769,
      "step": 1260400
    },
    {
      "epoch": 11.501751952697276,
      "grad_norm": 3.789193630218506,
      "learning_rate": 4.041520670608561e-05,
      "loss": 0.6718,
      "step": 1260500
    },
    {
      "epoch": 11.502664428060442,
      "grad_norm": 4.311471939086914,
      "learning_rate": 4.041444630994963e-05,
      "loss": 0.7504,
      "step": 1260600
    },
    {
      "epoch": 11.503576903423607,
      "grad_norm": 3.5820412635803223,
      "learning_rate": 4.041368591381366e-05,
      "loss": 0.6997,
      "step": 1260700
    },
    {
      "epoch": 11.504489378786772,
      "grad_norm": 3.4088549613952637,
      "learning_rate": 4.041292551767769e-05,
      "loss": 0.6682,
      "step": 1260800
    },
    {
      "epoch": 11.505401854149937,
      "grad_norm": 4.7742815017700195,
      "learning_rate": 4.041216512154172e-05,
      "loss": 0.6834,
      "step": 1260900
    },
    {
      "epoch": 11.506314329513103,
      "grad_norm": 4.134243488311768,
      "learning_rate": 4.0411404725405747e-05,
      "loss": 0.6307,
      "step": 1261000
    },
    {
      "epoch": 11.507226804876268,
      "grad_norm": 4.258386135101318,
      "learning_rate": 4.0410644329269777e-05,
      "loss": 0.6753,
      "step": 1261100
    },
    {
      "epoch": 11.508139280239433,
      "grad_norm": 3.2631282806396484,
      "learning_rate": 4.040988393313381e-05,
      "loss": 0.643,
      "step": 1261200
    },
    {
      "epoch": 11.509051755602599,
      "grad_norm": 4.044616222381592,
      "learning_rate": 4.040912353699784e-05,
      "loss": 0.6643,
      "step": 1261300
    },
    {
      "epoch": 11.509964230965764,
      "grad_norm": 4.144706726074219,
      "learning_rate": 4.040836314086187e-05,
      "loss": 0.6549,
      "step": 1261400
    },
    {
      "epoch": 11.51087670632893,
      "grad_norm": 4.12626838684082,
      "learning_rate": 4.040760274472589e-05,
      "loss": 0.6493,
      "step": 1261500
    },
    {
      "epoch": 11.511789181692095,
      "grad_norm": 4.517987251281738,
      "learning_rate": 4.040684234858993e-05,
      "loss": 0.6848,
      "step": 1261600
    },
    {
      "epoch": 11.51270165705526,
      "grad_norm": 3.7059218883514404,
      "learning_rate": 4.040608195245395e-05,
      "loss": 0.6921,
      "step": 1261700
    },
    {
      "epoch": 11.513614132418425,
      "grad_norm": 3.376786947250366,
      "learning_rate": 4.040532155631798e-05,
      "loss": 0.7178,
      "step": 1261800
    },
    {
      "epoch": 11.51452660778159,
      "grad_norm": 3.2899169921875,
      "learning_rate": 4.040456116018201e-05,
      "loss": 0.695,
      "step": 1261900
    },
    {
      "epoch": 11.515439083144756,
      "grad_norm": 3.3289706707000732,
      "learning_rate": 4.040380076404604e-05,
      "loss": 0.6608,
      "step": 1262000
    },
    {
      "epoch": 11.516351558507921,
      "grad_norm": 4.098151683807373,
      "learning_rate": 4.0403040367910064e-05,
      "loss": 0.7018,
      "step": 1262100
    },
    {
      "epoch": 11.517264033871086,
      "grad_norm": 3.6620702743530273,
      "learning_rate": 4.04022799717741e-05,
      "loss": 0.6632,
      "step": 1262200
    },
    {
      "epoch": 11.51817650923425,
      "grad_norm": 3.4855525493621826,
      "learning_rate": 4.0401519575638124e-05,
      "loss": 0.6642,
      "step": 1262300
    },
    {
      "epoch": 11.519088984597415,
      "grad_norm": 3.9180784225463867,
      "learning_rate": 4.0400759179502154e-05,
      "loss": 0.6609,
      "step": 1262400
    },
    {
      "epoch": 11.52000145996058,
      "grad_norm": 3.691525936126709,
      "learning_rate": 4.0399998783366184e-05,
      "loss": 0.688,
      "step": 1262500
    },
    {
      "epoch": 11.520913935323746,
      "grad_norm": 4.82691764831543,
      "learning_rate": 4.0399238387230214e-05,
      "loss": 0.6913,
      "step": 1262600
    },
    {
      "epoch": 11.521826410686911,
      "grad_norm": 3.887423038482666,
      "learning_rate": 4.0398477991094244e-05,
      "loss": 0.6844,
      "step": 1262700
    },
    {
      "epoch": 11.522738886050076,
      "grad_norm": 4.448770999908447,
      "learning_rate": 4.0397717594958274e-05,
      "loss": 0.7274,
      "step": 1262800
    },
    {
      "epoch": 11.523651361413242,
      "grad_norm": 3.9031500816345215,
      "learning_rate": 4.03969571988223e-05,
      "loss": 0.6663,
      "step": 1262900
    },
    {
      "epoch": 11.524563836776407,
      "grad_norm": 3.540196418762207,
      "learning_rate": 4.0396196802686334e-05,
      "loss": 0.686,
      "step": 1263000
    },
    {
      "epoch": 11.525476312139572,
      "grad_norm": 3.703747272491455,
      "learning_rate": 4.039543640655036e-05,
      "loss": 0.6932,
      "step": 1263100
    },
    {
      "epoch": 11.526388787502738,
      "grad_norm": 3.5832254886627197,
      "learning_rate": 4.039467601041439e-05,
      "loss": 0.7004,
      "step": 1263200
    },
    {
      "epoch": 11.527301262865903,
      "grad_norm": 3.9604740142822266,
      "learning_rate": 4.039391561427842e-05,
      "loss": 0.6923,
      "step": 1263300
    },
    {
      "epoch": 11.528213738229068,
      "grad_norm": 4.014699459075928,
      "learning_rate": 4.039315521814245e-05,
      "loss": 0.661,
      "step": 1263400
    },
    {
      "epoch": 11.529126213592233,
      "grad_norm": 3.3578579425811768,
      "learning_rate": 4.039239482200647e-05,
      "loss": 0.6814,
      "step": 1263500
    },
    {
      "epoch": 11.530038688955399,
      "grad_norm": 4.945426940917969,
      "learning_rate": 4.039163442587051e-05,
      "loss": 0.6887,
      "step": 1263600
    },
    {
      "epoch": 11.530951164318564,
      "grad_norm": 5.095638751983643,
      "learning_rate": 4.039087402973453e-05,
      "loss": 0.6275,
      "step": 1263700
    },
    {
      "epoch": 11.53186363968173,
      "grad_norm": 3.523580551147461,
      "learning_rate": 4.039011363359856e-05,
      "loss": 0.6322,
      "step": 1263800
    },
    {
      "epoch": 11.532776115044893,
      "grad_norm": 3.7535507678985596,
      "learning_rate": 4.038935323746259e-05,
      "loss": 0.6758,
      "step": 1263900
    },
    {
      "epoch": 11.533688590408058,
      "grad_norm": 4.030749797821045,
      "learning_rate": 4.0388592841326615e-05,
      "loss": 0.7027,
      "step": 1264000
    },
    {
      "epoch": 11.534601065771223,
      "grad_norm": 4.255946159362793,
      "learning_rate": 4.038783244519065e-05,
      "loss": 0.6728,
      "step": 1264100
    },
    {
      "epoch": 11.535513541134389,
      "grad_norm": 4.736239910125732,
      "learning_rate": 4.0387072049054675e-05,
      "loss": 0.6684,
      "step": 1264200
    },
    {
      "epoch": 11.536426016497554,
      "grad_norm": 3.69327974319458,
      "learning_rate": 4.0386311652918705e-05,
      "loss": 0.7259,
      "step": 1264300
    },
    {
      "epoch": 11.53733849186072,
      "grad_norm": 3.600738286972046,
      "learning_rate": 4.0385551256782735e-05,
      "loss": 0.6922,
      "step": 1264400
    },
    {
      "epoch": 11.538250967223885,
      "grad_norm": 1.7583341598510742,
      "learning_rate": 4.0384790860646765e-05,
      "loss": 0.6564,
      "step": 1264500
    },
    {
      "epoch": 11.53916344258705,
      "grad_norm": 3.8872458934783936,
      "learning_rate": 4.038403046451079e-05,
      "loss": 0.6916,
      "step": 1264600
    },
    {
      "epoch": 11.540075917950215,
      "grad_norm": 4.639411926269531,
      "learning_rate": 4.0383270068374825e-05,
      "loss": 0.7166,
      "step": 1264700
    },
    {
      "epoch": 11.54098839331338,
      "grad_norm": 4.322247505187988,
      "learning_rate": 4.038250967223885e-05,
      "loss": 0.6963,
      "step": 1264800
    },
    {
      "epoch": 11.541900868676546,
      "grad_norm": 4.397933483123779,
      "learning_rate": 4.038174927610288e-05,
      "loss": 0.7095,
      "step": 1264900
    },
    {
      "epoch": 11.542813344039711,
      "grad_norm": 4.099010467529297,
      "learning_rate": 4.038098887996691e-05,
      "loss": 0.6745,
      "step": 1265000
    },
    {
      "epoch": 11.543725819402876,
      "grad_norm": 4.193522930145264,
      "learning_rate": 4.038022848383094e-05,
      "loss": 0.6738,
      "step": 1265100
    },
    {
      "epoch": 11.544638294766042,
      "grad_norm": 3.731123447418213,
      "learning_rate": 4.037946808769497e-05,
      "loss": 0.6542,
      "step": 1265200
    },
    {
      "epoch": 11.545550770129207,
      "grad_norm": 3.2819879055023193,
      "learning_rate": 4.0378707691559e-05,
      "loss": 0.7168,
      "step": 1265300
    },
    {
      "epoch": 11.546463245492372,
      "grad_norm": 3.9231412410736084,
      "learning_rate": 4.037794729542302e-05,
      "loss": 0.6942,
      "step": 1265400
    },
    {
      "epoch": 11.547375720855538,
      "grad_norm": 4.395088195800781,
      "learning_rate": 4.037718689928706e-05,
      "loss": 0.6482,
      "step": 1265500
    },
    {
      "epoch": 11.548288196218703,
      "grad_norm": 3.135450601577759,
      "learning_rate": 4.037642650315108e-05,
      "loss": 0.6539,
      "step": 1265600
    },
    {
      "epoch": 11.549200671581866,
      "grad_norm": 3.1884195804595947,
      "learning_rate": 4.037566610701511e-05,
      "loss": 0.6902,
      "step": 1265700
    },
    {
      "epoch": 11.550113146945032,
      "grad_norm": 4.223402500152588,
      "learning_rate": 4.037490571087914e-05,
      "loss": 0.6477,
      "step": 1265800
    },
    {
      "epoch": 11.551025622308197,
      "grad_norm": 3.8851587772369385,
      "learning_rate": 4.037414531474317e-05,
      "loss": 0.671,
      "step": 1265900
    },
    {
      "epoch": 11.551938097671362,
      "grad_norm": 4.348852634429932,
      "learning_rate": 4.0373384918607196e-05,
      "loss": 0.6579,
      "step": 1266000
    },
    {
      "epoch": 11.552850573034528,
      "grad_norm": 3.9300782680511475,
      "learning_rate": 4.037262452247123e-05,
      "loss": 0.6985,
      "step": 1266100
    },
    {
      "epoch": 11.553763048397693,
      "grad_norm": 4.250855445861816,
      "learning_rate": 4.0371864126335256e-05,
      "loss": 0.6852,
      "step": 1266200
    },
    {
      "epoch": 11.554675523760858,
      "grad_norm": 4.038025856018066,
      "learning_rate": 4.0371103730199286e-05,
      "loss": 0.7009,
      "step": 1266300
    },
    {
      "epoch": 11.555587999124024,
      "grad_norm": 3.321011781692505,
      "learning_rate": 4.0370343334063316e-05,
      "loss": 0.6633,
      "step": 1266400
    },
    {
      "epoch": 11.556500474487189,
      "grad_norm": 2.762796640396118,
      "learning_rate": 4.0369582937927346e-05,
      "loss": 0.649,
      "step": 1266500
    },
    {
      "epoch": 11.557412949850354,
      "grad_norm": 3.2016761302948,
      "learning_rate": 4.0368822541791376e-05,
      "loss": 0.6434,
      "step": 1266600
    },
    {
      "epoch": 11.55832542521352,
      "grad_norm": 3.1221516132354736,
      "learning_rate": 4.0368062145655406e-05,
      "loss": 0.6795,
      "step": 1266700
    },
    {
      "epoch": 11.559237900576685,
      "grad_norm": 4.246713161468506,
      "learning_rate": 4.036730174951943e-05,
      "loss": 0.6738,
      "step": 1266800
    },
    {
      "epoch": 11.56015037593985,
      "grad_norm": 5.070899486541748,
      "learning_rate": 4.036654135338346e-05,
      "loss": 0.6816,
      "step": 1266900
    },
    {
      "epoch": 11.561062851303015,
      "grad_norm": 3.024428606033325,
      "learning_rate": 4.036578095724749e-05,
      "loss": 0.6579,
      "step": 1267000
    },
    {
      "epoch": 11.56197532666618,
      "grad_norm": 4.024623870849609,
      "learning_rate": 4.036502056111151e-05,
      "loss": 0.6803,
      "step": 1267100
    },
    {
      "epoch": 11.562887802029346,
      "grad_norm": 4.59251594543457,
      "learning_rate": 4.036426016497555e-05,
      "loss": 0.7171,
      "step": 1267200
    },
    {
      "epoch": 11.56380027739251,
      "grad_norm": 2.1879937648773193,
      "learning_rate": 4.036349976883957e-05,
      "loss": 0.6593,
      "step": 1267300
    },
    {
      "epoch": 11.564712752755675,
      "grad_norm": 4.100841045379639,
      "learning_rate": 4.03627393727036e-05,
      "loss": 0.6038,
      "step": 1267400
    },
    {
      "epoch": 11.56562522811884,
      "grad_norm": 4.386162757873535,
      "learning_rate": 4.036197897656763e-05,
      "loss": 0.6804,
      "step": 1267500
    },
    {
      "epoch": 11.566537703482005,
      "grad_norm": 4.038549900054932,
      "learning_rate": 4.036121858043166e-05,
      "loss": 0.6836,
      "step": 1267600
    },
    {
      "epoch": 11.56745017884517,
      "grad_norm": 4.619579315185547,
      "learning_rate": 4.036045818429569e-05,
      "loss": 0.6768,
      "step": 1267700
    },
    {
      "epoch": 11.568362654208336,
      "grad_norm": 4.516580104827881,
      "learning_rate": 4.035969778815972e-05,
      "loss": 0.676,
      "step": 1267800
    },
    {
      "epoch": 11.569275129571501,
      "grad_norm": 4.646422386169434,
      "learning_rate": 4.035893739202375e-05,
      "loss": 0.6605,
      "step": 1267900
    },
    {
      "epoch": 11.570187604934667,
      "grad_norm": 4.3485236167907715,
      "learning_rate": 4.0358176995887783e-05,
      "loss": 0.6906,
      "step": 1268000
    },
    {
      "epoch": 11.571100080297832,
      "grad_norm": 3.7468976974487305,
      "learning_rate": 4.035741659975181e-05,
      "loss": 0.6808,
      "step": 1268100
    },
    {
      "epoch": 11.572012555660997,
      "grad_norm": 3.750617504119873,
      "learning_rate": 4.035665620361584e-05,
      "loss": 0.641,
      "step": 1268200
    },
    {
      "epoch": 11.572925031024162,
      "grad_norm": 4.058049201965332,
      "learning_rate": 4.035589580747987e-05,
      "loss": 0.6946,
      "step": 1268300
    },
    {
      "epoch": 11.573837506387328,
      "grad_norm": 3.435049295425415,
      "learning_rate": 4.03551354113439e-05,
      "loss": 0.6868,
      "step": 1268400
    },
    {
      "epoch": 11.574749981750493,
      "grad_norm": 3.593961715698242,
      "learning_rate": 4.035437501520792e-05,
      "loss": 0.6988,
      "step": 1268500
    },
    {
      "epoch": 11.575662457113658,
      "grad_norm": 3.7056684494018555,
      "learning_rate": 4.035361461907196e-05,
      "loss": 0.6584,
      "step": 1268600
    },
    {
      "epoch": 11.576574932476824,
      "grad_norm": 3.9786860942840576,
      "learning_rate": 4.035285422293598e-05,
      "loss": 0.7152,
      "step": 1268700
    },
    {
      "epoch": 11.577487407839989,
      "grad_norm": 4.227823257446289,
      "learning_rate": 4.035209382680001e-05,
      "loss": 0.7072,
      "step": 1268800
    },
    {
      "epoch": 11.578399883203154,
      "grad_norm": 3.3336002826690674,
      "learning_rate": 4.035133343066404e-05,
      "loss": 0.6788,
      "step": 1268900
    },
    {
      "epoch": 11.57931235856632,
      "grad_norm": 3.671156883239746,
      "learning_rate": 4.035057303452807e-05,
      "loss": 0.6488,
      "step": 1269000
    },
    {
      "epoch": 11.580224833929483,
      "grad_norm": 3.884519100189209,
      "learning_rate": 4.03498126383921e-05,
      "loss": 0.702,
      "step": 1269100
    },
    {
      "epoch": 11.581137309292648,
      "grad_norm": 3.3811419010162354,
      "learning_rate": 4.034905224225613e-05,
      "loss": 0.6977,
      "step": 1269200
    },
    {
      "epoch": 11.582049784655814,
      "grad_norm": 4.198413372039795,
      "learning_rate": 4.0348291846120154e-05,
      "loss": 0.7313,
      "step": 1269300
    },
    {
      "epoch": 11.582962260018979,
      "grad_norm": 5.047015190124512,
      "learning_rate": 4.034753144998419e-05,
      "loss": 0.6698,
      "step": 1269400
    },
    {
      "epoch": 11.583874735382144,
      "grad_norm": 3.8875269889831543,
      "learning_rate": 4.0346771053848214e-05,
      "loss": 0.6967,
      "step": 1269500
    },
    {
      "epoch": 11.58478721074531,
      "grad_norm": 3.8906874656677246,
      "learning_rate": 4.0346010657712244e-05,
      "loss": 0.6788,
      "step": 1269600
    },
    {
      "epoch": 11.585699686108475,
      "grad_norm": 3.296022415161133,
      "learning_rate": 4.0345250261576274e-05,
      "loss": 0.7168,
      "step": 1269700
    },
    {
      "epoch": 11.58661216147164,
      "grad_norm": 3.813206434249878,
      "learning_rate": 4.03444898654403e-05,
      "loss": 0.7,
      "step": 1269800
    },
    {
      "epoch": 11.587524636834805,
      "grad_norm": 3.250574827194214,
      "learning_rate": 4.0343729469304334e-05,
      "loss": 0.6777,
      "step": 1269900
    },
    {
      "epoch": 11.58843711219797,
      "grad_norm": 4.067287445068359,
      "learning_rate": 4.034296907316836e-05,
      "loss": 0.6364,
      "step": 1270000
    },
    {
      "epoch": 11.589349587561136,
      "grad_norm": 4.568563938140869,
      "learning_rate": 4.034220867703239e-05,
      "loss": 0.7065,
      "step": 1270100
    },
    {
      "epoch": 11.590262062924301,
      "grad_norm": 3.0506961345672607,
      "learning_rate": 4.034144828089642e-05,
      "loss": 0.6805,
      "step": 1270200
    },
    {
      "epoch": 11.591174538287467,
      "grad_norm": 3.3995683193206787,
      "learning_rate": 4.034068788476045e-05,
      "loss": 0.6464,
      "step": 1270300
    },
    {
      "epoch": 11.592087013650632,
      "grad_norm": 3.830214262008667,
      "learning_rate": 4.033992748862447e-05,
      "loss": 0.6779,
      "step": 1270400
    },
    {
      "epoch": 11.592999489013797,
      "grad_norm": 3.558742046356201,
      "learning_rate": 4.033916709248851e-05,
      "loss": 0.709,
      "step": 1270500
    },
    {
      "epoch": 11.593911964376963,
      "grad_norm": 4.430253982543945,
      "learning_rate": 4.033840669635253e-05,
      "loss": 0.6907,
      "step": 1270600
    },
    {
      "epoch": 11.594824439740126,
      "grad_norm": 3.6761908531188965,
      "learning_rate": 4.033764630021656e-05,
      "loss": 0.6213,
      "step": 1270700
    },
    {
      "epoch": 11.595736915103291,
      "grad_norm": 3.7655537128448486,
      "learning_rate": 4.033688590408059e-05,
      "loss": 0.6788,
      "step": 1270800
    },
    {
      "epoch": 11.596649390466457,
      "grad_norm": 3.603569269180298,
      "learning_rate": 4.033612550794462e-05,
      "loss": 0.6937,
      "step": 1270900
    },
    {
      "epoch": 11.597561865829622,
      "grad_norm": 3.566002368927002,
      "learning_rate": 4.033536511180865e-05,
      "loss": 0.6739,
      "step": 1271000
    },
    {
      "epoch": 11.598474341192787,
      "grad_norm": 5.815014362335205,
      "learning_rate": 4.033460471567268e-05,
      "loss": 0.6771,
      "step": 1271100
    },
    {
      "epoch": 11.599386816555953,
      "grad_norm": 3.3525633811950684,
      "learning_rate": 4.0333844319536705e-05,
      "loss": 0.6681,
      "step": 1271200
    },
    {
      "epoch": 11.600299291919118,
      "grad_norm": 3.5164973735809326,
      "learning_rate": 4.033308392340074e-05,
      "loss": 0.7126,
      "step": 1271300
    },
    {
      "epoch": 11.601211767282283,
      "grad_norm": 4.553615093231201,
      "learning_rate": 4.0332323527264765e-05,
      "loss": 0.6939,
      "step": 1271400
    },
    {
      "epoch": 11.602124242645449,
      "grad_norm": 4.699880123138428,
      "learning_rate": 4.0331563131128795e-05,
      "loss": 0.6725,
      "step": 1271500
    },
    {
      "epoch": 11.603036718008614,
      "grad_norm": 4.020632743835449,
      "learning_rate": 4.0330802734992825e-05,
      "loss": 0.6888,
      "step": 1271600
    },
    {
      "epoch": 11.603949193371779,
      "grad_norm": 2.760612964630127,
      "learning_rate": 4.0330042338856855e-05,
      "loss": 0.67,
      "step": 1271700
    },
    {
      "epoch": 11.604861668734944,
      "grad_norm": 3.523865222930908,
      "learning_rate": 4.032928194272088e-05,
      "loss": 0.6709,
      "step": 1271800
    },
    {
      "epoch": 11.60577414409811,
      "grad_norm": 4.428045749664307,
      "learning_rate": 4.0328521546584915e-05,
      "loss": 0.7017,
      "step": 1271900
    },
    {
      "epoch": 11.606686619461275,
      "grad_norm": 4.386865615844727,
      "learning_rate": 4.032776115044894e-05,
      "loss": 0.6758,
      "step": 1272000
    },
    {
      "epoch": 11.60759909482444,
      "grad_norm": 2.62715220451355,
      "learning_rate": 4.032700075431297e-05,
      "loss": 0.6447,
      "step": 1272100
    },
    {
      "epoch": 11.608511570187606,
      "grad_norm": 4.823431491851807,
      "learning_rate": 4.0326240358177e-05,
      "loss": 0.7004,
      "step": 1272200
    },
    {
      "epoch": 11.609424045550771,
      "grad_norm": 4.175288200378418,
      "learning_rate": 4.032547996204103e-05,
      "loss": 0.7141,
      "step": 1272300
    },
    {
      "epoch": 11.610336520913936,
      "grad_norm": 5.147892475128174,
      "learning_rate": 4.032471956590506e-05,
      "loss": 0.6802,
      "step": 1272400
    },
    {
      "epoch": 11.6112489962771,
      "grad_norm": 4.243515491485596,
      "learning_rate": 4.032395916976908e-05,
      "loss": 0.7154,
      "step": 1272500
    },
    {
      "epoch": 11.612161471640265,
      "grad_norm": 3.088327646255493,
      "learning_rate": 4.032319877363311e-05,
      "loss": 0.6751,
      "step": 1272600
    },
    {
      "epoch": 11.61307394700343,
      "grad_norm": 2.7284204959869385,
      "learning_rate": 4.032243837749714e-05,
      "loss": 0.6946,
      "step": 1272700
    },
    {
      "epoch": 11.613986422366596,
      "grad_norm": 4.703111171722412,
      "learning_rate": 4.032167798136117e-05,
      "loss": 0.6699,
      "step": 1272800
    },
    {
      "epoch": 11.614898897729761,
      "grad_norm": 3.996821641921997,
      "learning_rate": 4.0320917585225196e-05,
      "loss": 0.6999,
      "step": 1272900
    },
    {
      "epoch": 11.615811373092926,
      "grad_norm": 4.236700534820557,
      "learning_rate": 4.032015718908923e-05,
      "loss": 0.6713,
      "step": 1273000
    },
    {
      "epoch": 11.616723848456092,
      "grad_norm": 4.909487724304199,
      "learning_rate": 4.0319396792953256e-05,
      "loss": 0.6586,
      "step": 1273100
    },
    {
      "epoch": 11.617636323819257,
      "grad_norm": 4.229362964630127,
      "learning_rate": 4.0318636396817286e-05,
      "loss": 0.6883,
      "step": 1273200
    },
    {
      "epoch": 11.618548799182422,
      "grad_norm": 4.505565643310547,
      "learning_rate": 4.0317876000681316e-05,
      "loss": 0.6764,
      "step": 1273300
    },
    {
      "epoch": 11.619461274545587,
      "grad_norm": 3.2847187519073486,
      "learning_rate": 4.0317115604545346e-05,
      "loss": 0.6852,
      "step": 1273400
    },
    {
      "epoch": 11.620373749908753,
      "grad_norm": 5.157742977142334,
      "learning_rate": 4.0316355208409376e-05,
      "loss": 0.6592,
      "step": 1273500
    },
    {
      "epoch": 11.621286225271918,
      "grad_norm": 3.0369858741760254,
      "learning_rate": 4.0315594812273406e-05,
      "loss": 0.7039,
      "step": 1273600
    },
    {
      "epoch": 11.622198700635083,
      "grad_norm": 3.9122214317321777,
      "learning_rate": 4.031483441613743e-05,
      "loss": 0.6657,
      "step": 1273700
    },
    {
      "epoch": 11.623111175998249,
      "grad_norm": 3.979116916656494,
      "learning_rate": 4.0314074020001466e-05,
      "loss": 0.7117,
      "step": 1273800
    },
    {
      "epoch": 11.624023651361414,
      "grad_norm": 3.906477212905884,
      "learning_rate": 4.031331362386549e-05,
      "loss": 0.675,
      "step": 1273900
    },
    {
      "epoch": 11.62493612672458,
      "grad_norm": 4.4041900634765625,
      "learning_rate": 4.031255322772952e-05,
      "loss": 0.6738,
      "step": 1274000
    },
    {
      "epoch": 11.625848602087743,
      "grad_norm": 3.7716825008392334,
      "learning_rate": 4.031179283159355e-05,
      "loss": 0.6966,
      "step": 1274100
    },
    {
      "epoch": 11.626761077450908,
      "grad_norm": 3.3220558166503906,
      "learning_rate": 4.031103243545758e-05,
      "loss": 0.6647,
      "step": 1274200
    },
    {
      "epoch": 11.627673552814073,
      "grad_norm": 1.9865953922271729,
      "learning_rate": 4.03102720393216e-05,
      "loss": 0.6674,
      "step": 1274300
    },
    {
      "epoch": 11.628586028177239,
      "grad_norm": 4.548221588134766,
      "learning_rate": 4.030951164318564e-05,
      "loss": 0.6661,
      "step": 1274400
    },
    {
      "epoch": 11.629498503540404,
      "grad_norm": 2.2568705081939697,
      "learning_rate": 4.030875124704966e-05,
      "loss": 0.6765,
      "step": 1274500
    },
    {
      "epoch": 11.63041097890357,
      "grad_norm": 4.599621295928955,
      "learning_rate": 4.030799085091369e-05,
      "loss": 0.7119,
      "step": 1274600
    },
    {
      "epoch": 11.631323454266735,
      "grad_norm": 3.4077367782592773,
      "learning_rate": 4.0307230454777723e-05,
      "loss": 0.6845,
      "step": 1274700
    },
    {
      "epoch": 11.6322359296299,
      "grad_norm": 4.119330406188965,
      "learning_rate": 4.0306470058641754e-05,
      "loss": 0.6719,
      "step": 1274800
    },
    {
      "epoch": 11.633148404993065,
      "grad_norm": 2.864067554473877,
      "learning_rate": 4.0305709662505784e-05,
      "loss": 0.74,
      "step": 1274900
    },
    {
      "epoch": 11.63406088035623,
      "grad_norm": 4.313190937042236,
      "learning_rate": 4.0304949266369814e-05,
      "loss": 0.6968,
      "step": 1275000
    },
    {
      "epoch": 11.634973355719396,
      "grad_norm": 3.7841203212738037,
      "learning_rate": 4.030418887023384e-05,
      "loss": 0.6741,
      "step": 1275100
    },
    {
      "epoch": 11.635885831082561,
      "grad_norm": 4.231256008148193,
      "learning_rate": 4.0303428474097874e-05,
      "loss": 0.6807,
      "step": 1275200
    },
    {
      "epoch": 11.636798306445726,
      "grad_norm": 4.2564311027526855,
      "learning_rate": 4.03026680779619e-05,
      "loss": 0.7031,
      "step": 1275300
    },
    {
      "epoch": 11.637710781808892,
      "grad_norm": 4.454003810882568,
      "learning_rate": 4.030190768182592e-05,
      "loss": 0.7002,
      "step": 1275400
    },
    {
      "epoch": 11.638623257172057,
      "grad_norm": 4.398553371429443,
      "learning_rate": 4.030114728568996e-05,
      "loss": 0.6788,
      "step": 1275500
    },
    {
      "epoch": 11.639535732535222,
      "grad_norm": 3.695678472518921,
      "learning_rate": 4.030038688955398e-05,
      "loss": 0.6526,
      "step": 1275600
    },
    {
      "epoch": 11.640448207898388,
      "grad_norm": 2.9880290031433105,
      "learning_rate": 4.029962649341801e-05,
      "loss": 0.6986,
      "step": 1275700
    },
    {
      "epoch": 11.641360683261553,
      "grad_norm": 4.588450908660889,
      "learning_rate": 4.029886609728204e-05,
      "loss": 0.7026,
      "step": 1275800
    },
    {
      "epoch": 11.642273158624716,
      "grad_norm": 3.446568727493286,
      "learning_rate": 4.029810570114607e-05,
      "loss": 0.6868,
      "step": 1275900
    },
    {
      "epoch": 11.643185633987882,
      "grad_norm": 3.5721042156219482,
      "learning_rate": 4.02973453050101e-05,
      "loss": 0.6857,
      "step": 1276000
    },
    {
      "epoch": 11.644098109351047,
      "grad_norm": 3.5254759788513184,
      "learning_rate": 4.029658490887413e-05,
      "loss": 0.6596,
      "step": 1276100
    },
    {
      "epoch": 11.645010584714212,
      "grad_norm": 4.163304805755615,
      "learning_rate": 4.0295824512738154e-05,
      "loss": 0.6994,
      "step": 1276200
    },
    {
      "epoch": 11.645923060077378,
      "grad_norm": 4.401249885559082,
      "learning_rate": 4.029506411660219e-05,
      "loss": 0.6692,
      "step": 1276300
    },
    {
      "epoch": 11.646835535440543,
      "grad_norm": 5.188823699951172,
      "learning_rate": 4.0294303720466214e-05,
      "loss": 0.6757,
      "step": 1276400
    },
    {
      "epoch": 11.647748010803708,
      "grad_norm": 4.158866882324219,
      "learning_rate": 4.0293543324330244e-05,
      "loss": 0.6924,
      "step": 1276500
    },
    {
      "epoch": 11.648660486166873,
      "grad_norm": 3.2524497509002686,
      "learning_rate": 4.0292782928194274e-05,
      "loss": 0.6882,
      "step": 1276600
    },
    {
      "epoch": 11.649572961530039,
      "grad_norm": 3.8978757858276367,
      "learning_rate": 4.0292022532058304e-05,
      "loss": 0.7233,
      "step": 1276700
    },
    {
      "epoch": 11.650485436893204,
      "grad_norm": 3.619386911392212,
      "learning_rate": 4.029126213592233e-05,
      "loss": 0.6609,
      "step": 1276800
    },
    {
      "epoch": 11.65139791225637,
      "grad_norm": 4.074338436126709,
      "learning_rate": 4.0290501739786365e-05,
      "loss": 0.6667,
      "step": 1276900
    },
    {
      "epoch": 11.652310387619535,
      "grad_norm": 3.853916645050049,
      "learning_rate": 4.028974134365039e-05,
      "loss": 0.6587,
      "step": 1277000
    },
    {
      "epoch": 11.6532228629827,
      "grad_norm": 2.788299560546875,
      "learning_rate": 4.028898094751442e-05,
      "loss": 0.6662,
      "step": 1277100
    },
    {
      "epoch": 11.654135338345865,
      "grad_norm": 4.131143093109131,
      "learning_rate": 4.028822055137845e-05,
      "loss": 0.6815,
      "step": 1277200
    },
    {
      "epoch": 11.65504781370903,
      "grad_norm": 3.6234211921691895,
      "learning_rate": 4.028746015524248e-05,
      "loss": 0.6791,
      "step": 1277300
    },
    {
      "epoch": 11.655960289072196,
      "grad_norm": 3.757314443588257,
      "learning_rate": 4.028669975910651e-05,
      "loss": 0.6593,
      "step": 1277400
    },
    {
      "epoch": 11.65687276443536,
      "grad_norm": 3.8202497959136963,
      "learning_rate": 4.028593936297054e-05,
      "loss": 0.6863,
      "step": 1277500
    },
    {
      "epoch": 11.657785239798525,
      "grad_norm": 5.276157379150391,
      "learning_rate": 4.028517896683456e-05,
      "loss": 0.6609,
      "step": 1277600
    },
    {
      "epoch": 11.65869771516169,
      "grad_norm": 4.006483554840088,
      "learning_rate": 4.02844185706986e-05,
      "loss": 0.6886,
      "step": 1277700
    },
    {
      "epoch": 11.659610190524855,
      "grad_norm": 3.5208916664123535,
      "learning_rate": 4.028365817456262e-05,
      "loss": 0.6723,
      "step": 1277800
    },
    {
      "epoch": 11.66052266588802,
      "grad_norm": 4.280052185058594,
      "learning_rate": 4.028289777842665e-05,
      "loss": 0.6662,
      "step": 1277900
    },
    {
      "epoch": 11.661435141251186,
      "grad_norm": 4.014196872711182,
      "learning_rate": 4.028213738229068e-05,
      "loss": 0.6642,
      "step": 1278000
    },
    {
      "epoch": 11.662347616614351,
      "grad_norm": 2.9036312103271484,
      "learning_rate": 4.0281376986154705e-05,
      "loss": 0.6716,
      "step": 1278100
    },
    {
      "epoch": 11.663260091977516,
      "grad_norm": 4.0696821212768555,
      "learning_rate": 4.0280616590018735e-05,
      "loss": 0.6714,
      "step": 1278200
    },
    {
      "epoch": 11.664172567340682,
      "grad_norm": 3.3122003078460693,
      "learning_rate": 4.0279856193882765e-05,
      "loss": 0.6719,
      "step": 1278300
    },
    {
      "epoch": 11.665085042703847,
      "grad_norm": 3.9293878078460693,
      "learning_rate": 4.0279095797746795e-05,
      "loss": 0.681,
      "step": 1278400
    },
    {
      "epoch": 11.665997518067012,
      "grad_norm": 4.202880859375,
      "learning_rate": 4.0278335401610825e-05,
      "loss": 0.6764,
      "step": 1278500
    },
    {
      "epoch": 11.666909993430178,
      "grad_norm": 4.672398567199707,
      "learning_rate": 4.0277575005474855e-05,
      "loss": 0.6629,
      "step": 1278600
    },
    {
      "epoch": 11.667822468793343,
      "grad_norm": 4.158327102661133,
      "learning_rate": 4.027681460933888e-05,
      "loss": 0.6705,
      "step": 1278700
    },
    {
      "epoch": 11.668734944156508,
      "grad_norm": 4.765753269195557,
      "learning_rate": 4.0276054213202916e-05,
      "loss": 0.6934,
      "step": 1278800
    },
    {
      "epoch": 11.669647419519674,
      "grad_norm": 3.328554630279541,
      "learning_rate": 4.027529381706694e-05,
      "loss": 0.6801,
      "step": 1278900
    },
    {
      "epoch": 11.670559894882839,
      "grad_norm": 4.694535732269287,
      "learning_rate": 4.027453342093097e-05,
      "loss": 0.6487,
      "step": 1279000
    },
    {
      "epoch": 11.671472370246004,
      "grad_norm": 3.775561809539795,
      "learning_rate": 4.0273773024795e-05,
      "loss": 0.6992,
      "step": 1279100
    },
    {
      "epoch": 11.67238484560917,
      "grad_norm": 4.445415019989014,
      "learning_rate": 4.027301262865903e-05,
      "loss": 0.6567,
      "step": 1279200
    },
    {
      "epoch": 11.673297320972333,
      "grad_norm": 3.7575130462646484,
      "learning_rate": 4.027225223252305e-05,
      "loss": 0.6703,
      "step": 1279300
    },
    {
      "epoch": 11.674209796335498,
      "grad_norm": 2.612583875656128,
      "learning_rate": 4.027149183638709e-05,
      "loss": 0.7036,
      "step": 1279400
    },
    {
      "epoch": 11.675122271698664,
      "grad_norm": 4.002603530883789,
      "learning_rate": 4.027073144025111e-05,
      "loss": 0.6416,
      "step": 1279500
    },
    {
      "epoch": 11.676034747061829,
      "grad_norm": 2.9349048137664795,
      "learning_rate": 4.026997104411514e-05,
      "loss": 0.6707,
      "step": 1279600
    },
    {
      "epoch": 11.676947222424994,
      "grad_norm": 3.6396028995513916,
      "learning_rate": 4.026921064797917e-05,
      "loss": 0.6474,
      "step": 1279700
    },
    {
      "epoch": 11.67785969778816,
      "grad_norm": 4.320733070373535,
      "learning_rate": 4.02684502518432e-05,
      "loss": 0.6713,
      "step": 1279800
    },
    {
      "epoch": 11.678772173151325,
      "grad_norm": 3.68538498878479,
      "learning_rate": 4.026768985570723e-05,
      "loss": 0.6715,
      "step": 1279900
    },
    {
      "epoch": 11.67968464851449,
      "grad_norm": 4.637547016143799,
      "learning_rate": 4.026692945957126e-05,
      "loss": 0.6338,
      "step": 1280000
    },
    {
      "epoch": 11.680597123877655,
      "grad_norm": 4.38911771774292,
      "learning_rate": 4.0266169063435286e-05,
      "loss": 0.6574,
      "step": 1280100
    },
    {
      "epoch": 11.68150959924082,
      "grad_norm": 4.027562618255615,
      "learning_rate": 4.026540866729932e-05,
      "loss": 0.6826,
      "step": 1280200
    },
    {
      "epoch": 11.682422074603986,
      "grad_norm": 4.24697208404541,
      "learning_rate": 4.0264648271163346e-05,
      "loss": 0.6626,
      "step": 1280300
    },
    {
      "epoch": 11.683334549967151,
      "grad_norm": 4.528909206390381,
      "learning_rate": 4.0263887875027376e-05,
      "loss": 0.6936,
      "step": 1280400
    },
    {
      "epoch": 11.684247025330317,
      "grad_norm": 3.6233651638031006,
      "learning_rate": 4.0263127478891406e-05,
      "loss": 0.6922,
      "step": 1280500
    },
    {
      "epoch": 11.685159500693482,
      "grad_norm": 3.3819286823272705,
      "learning_rate": 4.0262367082755436e-05,
      "loss": 0.6675,
      "step": 1280600
    },
    {
      "epoch": 11.686071976056647,
      "grad_norm": 3.7836668491363525,
      "learning_rate": 4.026160668661946e-05,
      "loss": 0.6914,
      "step": 1280700
    },
    {
      "epoch": 11.686984451419812,
      "grad_norm": 4.335630893707275,
      "learning_rate": 4.0260846290483497e-05,
      "loss": 0.6675,
      "step": 1280800
    },
    {
      "epoch": 11.687896926782976,
      "grad_norm": 4.318832874298096,
      "learning_rate": 4.026008589434752e-05,
      "loss": 0.6925,
      "step": 1280900
    },
    {
      "epoch": 11.688809402146141,
      "grad_norm": 4.520388603210449,
      "learning_rate": 4.025932549821155e-05,
      "loss": 0.7313,
      "step": 1281000
    },
    {
      "epoch": 11.689721877509307,
      "grad_norm": 3.2654104232788086,
      "learning_rate": 4.025856510207558e-05,
      "loss": 0.6981,
      "step": 1281100
    },
    {
      "epoch": 11.690634352872472,
      "grad_norm": 3.8566360473632812,
      "learning_rate": 4.02578047059396e-05,
      "loss": 0.707,
      "step": 1281200
    },
    {
      "epoch": 11.691546828235637,
      "grad_norm": 3.5750508308410645,
      "learning_rate": 4.025704430980364e-05,
      "loss": 0.6833,
      "step": 1281300
    },
    {
      "epoch": 11.692459303598802,
      "grad_norm": 4.658380031585693,
      "learning_rate": 4.0256283913667663e-05,
      "loss": 0.6763,
      "step": 1281400
    },
    {
      "epoch": 11.693371778961968,
      "grad_norm": 3.8168892860412598,
      "learning_rate": 4.0255523517531693e-05,
      "loss": 0.7251,
      "step": 1281500
    },
    {
      "epoch": 11.694284254325133,
      "grad_norm": 3.8344533443450928,
      "learning_rate": 4.0254763121395724e-05,
      "loss": 0.6635,
      "step": 1281600
    },
    {
      "epoch": 11.695196729688298,
      "grad_norm": 3.6733574867248535,
      "learning_rate": 4.0254002725259754e-05,
      "loss": 0.6968,
      "step": 1281700
    },
    {
      "epoch": 11.696109205051464,
      "grad_norm": 4.077118873596191,
      "learning_rate": 4.0253242329123784e-05,
      "loss": 0.681,
      "step": 1281800
    },
    {
      "epoch": 11.697021680414629,
      "grad_norm": 2.2846872806549072,
      "learning_rate": 4.0252481932987814e-05,
      "loss": 0.6591,
      "step": 1281900
    },
    {
      "epoch": 11.697934155777794,
      "grad_norm": 5.357922554016113,
      "learning_rate": 4.025172153685184e-05,
      "loss": 0.6359,
      "step": 1282000
    },
    {
      "epoch": 11.69884663114096,
      "grad_norm": 4.32045841217041,
      "learning_rate": 4.025096114071587e-05,
      "loss": 0.6924,
      "step": 1282100
    },
    {
      "epoch": 11.699759106504125,
      "grad_norm": 4.188820838928223,
      "learning_rate": 4.02502007445799e-05,
      "loss": 0.6673,
      "step": 1282200
    },
    {
      "epoch": 11.70067158186729,
      "grad_norm": 4.306614875793457,
      "learning_rate": 4.024944034844393e-05,
      "loss": 0.6745,
      "step": 1282300
    },
    {
      "epoch": 11.701584057230455,
      "grad_norm": 3.994863271713257,
      "learning_rate": 4.024867995230796e-05,
      "loss": 0.7008,
      "step": 1282400
    },
    {
      "epoch": 11.70249653259362,
      "grad_norm": 3.809347629547119,
      "learning_rate": 4.024791955617199e-05,
      "loss": 0.6694,
      "step": 1282500
    },
    {
      "epoch": 11.703409007956786,
      "grad_norm": 3.665424108505249,
      "learning_rate": 4.024715916003601e-05,
      "loss": 0.6758,
      "step": 1282600
    },
    {
      "epoch": 11.70432148331995,
      "grad_norm": 3.6738152503967285,
      "learning_rate": 4.024639876390005e-05,
      "loss": 0.7126,
      "step": 1282700
    },
    {
      "epoch": 11.705233958683115,
      "grad_norm": 3.541609048843384,
      "learning_rate": 4.024563836776407e-05,
      "loss": 0.6973,
      "step": 1282800
    },
    {
      "epoch": 11.70614643404628,
      "grad_norm": 4.402698516845703,
      "learning_rate": 4.02448779716281e-05,
      "loss": 0.6992,
      "step": 1282900
    },
    {
      "epoch": 11.707058909409445,
      "grad_norm": 3.8300669193267822,
      "learning_rate": 4.024411757549213e-05,
      "loss": 0.6778,
      "step": 1283000
    },
    {
      "epoch": 11.70797138477261,
      "grad_norm": 2.4496994018554688,
      "learning_rate": 4.024335717935616e-05,
      "loss": 0.6752,
      "step": 1283100
    },
    {
      "epoch": 11.708883860135776,
      "grad_norm": 4.605483055114746,
      "learning_rate": 4.024259678322019e-05,
      "loss": 0.6934,
      "step": 1283200
    },
    {
      "epoch": 11.709796335498941,
      "grad_norm": 3.538557529449463,
      "learning_rate": 4.024183638708422e-05,
      "loss": 0.6369,
      "step": 1283300
    },
    {
      "epoch": 11.710708810862107,
      "grad_norm": 4.112456321716309,
      "learning_rate": 4.0241075990948244e-05,
      "loss": 0.6567,
      "step": 1283400
    },
    {
      "epoch": 11.711621286225272,
      "grad_norm": 4.218247413635254,
      "learning_rate": 4.0240315594812274e-05,
      "loss": 0.6391,
      "step": 1283500
    },
    {
      "epoch": 11.712533761588437,
      "grad_norm": 4.032628536224365,
      "learning_rate": 4.0239555198676305e-05,
      "loss": 0.6568,
      "step": 1283600
    },
    {
      "epoch": 11.713446236951603,
      "grad_norm": 3.0146186351776123,
      "learning_rate": 4.0238794802540335e-05,
      "loss": 0.6589,
      "step": 1283700
    },
    {
      "epoch": 11.714358712314768,
      "grad_norm": 4.928535461425781,
      "learning_rate": 4.0238034406404365e-05,
      "loss": 0.7212,
      "step": 1283800
    },
    {
      "epoch": 11.715271187677933,
      "grad_norm": 4.247893333435059,
      "learning_rate": 4.023727401026839e-05,
      "loss": 0.6871,
      "step": 1283900
    },
    {
      "epoch": 11.716183663041098,
      "grad_norm": 3.768995523452759,
      "learning_rate": 4.023651361413242e-05,
      "loss": 0.724,
      "step": 1284000
    },
    {
      "epoch": 11.717096138404264,
      "grad_norm": 3.6622912883758545,
      "learning_rate": 4.023575321799645e-05,
      "loss": 0.6829,
      "step": 1284100
    },
    {
      "epoch": 11.718008613767429,
      "grad_norm": 4.9797186851501465,
      "learning_rate": 4.023499282186048e-05,
      "loss": 0.7219,
      "step": 1284200
    },
    {
      "epoch": 11.718921089130593,
      "grad_norm": 4.2806854248046875,
      "learning_rate": 4.023423242572451e-05,
      "loss": 0.68,
      "step": 1284300
    },
    {
      "epoch": 11.719833564493758,
      "grad_norm": 3.3760554790496826,
      "learning_rate": 4.023347202958854e-05,
      "loss": 0.7197,
      "step": 1284400
    },
    {
      "epoch": 11.720746039856923,
      "grad_norm": 4.424276351928711,
      "learning_rate": 4.023271163345256e-05,
      "loss": 0.6523,
      "step": 1284500
    },
    {
      "epoch": 11.721658515220089,
      "grad_norm": 3.399744749069214,
      "learning_rate": 4.02319512373166e-05,
      "loss": 0.7102,
      "step": 1284600
    },
    {
      "epoch": 11.722570990583254,
      "grad_norm": 4.006892204284668,
      "learning_rate": 4.023119084118062e-05,
      "loss": 0.6871,
      "step": 1284700
    },
    {
      "epoch": 11.723483465946419,
      "grad_norm": 3.2545018196105957,
      "learning_rate": 4.023043044504465e-05,
      "loss": 0.6787,
      "step": 1284800
    },
    {
      "epoch": 11.724395941309584,
      "grad_norm": 4.091911792755127,
      "learning_rate": 4.022967004890868e-05,
      "loss": 0.6999,
      "step": 1284900
    },
    {
      "epoch": 11.72530841667275,
      "grad_norm": 3.3966822624206543,
      "learning_rate": 4.022890965277271e-05,
      "loss": 0.6543,
      "step": 1285000
    },
    {
      "epoch": 11.726220892035915,
      "grad_norm": 4.466001033782959,
      "learning_rate": 4.0228149256636735e-05,
      "loss": 0.6782,
      "step": 1285100
    },
    {
      "epoch": 11.72713336739908,
      "grad_norm": 4.175145149230957,
      "learning_rate": 4.022738886050077e-05,
      "loss": 0.6813,
      "step": 1285200
    },
    {
      "epoch": 11.728045842762246,
      "grad_norm": 3.771252155303955,
      "learning_rate": 4.0226628464364795e-05,
      "loss": 0.7037,
      "step": 1285300
    },
    {
      "epoch": 11.728958318125411,
      "grad_norm": 4.208083629608154,
      "learning_rate": 4.0225868068228825e-05,
      "loss": 0.6746,
      "step": 1285400
    },
    {
      "epoch": 11.729870793488576,
      "grad_norm": 4.292312145233154,
      "learning_rate": 4.0225107672092856e-05,
      "loss": 0.6851,
      "step": 1285500
    },
    {
      "epoch": 11.730783268851741,
      "grad_norm": 4.261463165283203,
      "learning_rate": 4.0224347275956886e-05,
      "loss": 0.6806,
      "step": 1285600
    },
    {
      "epoch": 11.731695744214907,
      "grad_norm": 3.6827118396759033,
      "learning_rate": 4.0223586879820916e-05,
      "loss": 0.6951,
      "step": 1285700
    },
    {
      "epoch": 11.732608219578072,
      "grad_norm": 4.38300895690918,
      "learning_rate": 4.0222826483684946e-05,
      "loss": 0.6832,
      "step": 1285800
    },
    {
      "epoch": 11.733520694941237,
      "grad_norm": 4.158820152282715,
      "learning_rate": 4.022206608754897e-05,
      "loss": 0.692,
      "step": 1285900
    },
    {
      "epoch": 11.734433170304403,
      "grad_norm": 3.9197068214416504,
      "learning_rate": 4.0221305691413006e-05,
      "loss": 0.6792,
      "step": 1286000
    },
    {
      "epoch": 11.735345645667566,
      "grad_norm": 2.081393241882324,
      "learning_rate": 4.022054529527703e-05,
      "loss": 0.7108,
      "step": 1286100
    },
    {
      "epoch": 11.736258121030732,
      "grad_norm": 4.651513576507568,
      "learning_rate": 4.021978489914106e-05,
      "loss": 0.6624,
      "step": 1286200
    },
    {
      "epoch": 11.737170596393897,
      "grad_norm": 4.167853832244873,
      "learning_rate": 4.021902450300509e-05,
      "loss": 0.682,
      "step": 1286300
    },
    {
      "epoch": 11.738083071757062,
      "grad_norm": 3.873868465423584,
      "learning_rate": 4.021826410686912e-05,
      "loss": 0.6659,
      "step": 1286400
    },
    {
      "epoch": 11.738995547120227,
      "grad_norm": 2.9215943813323975,
      "learning_rate": 4.021750371073314e-05,
      "loss": 0.699,
      "step": 1286500
    },
    {
      "epoch": 11.739908022483393,
      "grad_norm": 3.3457744121551514,
      "learning_rate": 4.021674331459717e-05,
      "loss": 0.6499,
      "step": 1286600
    },
    {
      "epoch": 11.740820497846558,
      "grad_norm": 4.030000686645508,
      "learning_rate": 4.02159829184612e-05,
      "loss": 0.6639,
      "step": 1286700
    },
    {
      "epoch": 11.741732973209723,
      "grad_norm": 4.008145332336426,
      "learning_rate": 4.021522252232523e-05,
      "loss": 0.6503,
      "step": 1286800
    },
    {
      "epoch": 11.742645448572889,
      "grad_norm": 3.6702535152435303,
      "learning_rate": 4.021446212618926e-05,
      "loss": 0.7082,
      "step": 1286900
    },
    {
      "epoch": 11.743557923936054,
      "grad_norm": 4.891660690307617,
      "learning_rate": 4.0213701730053286e-05,
      "loss": 0.7269,
      "step": 1287000
    },
    {
      "epoch": 11.74447039929922,
      "grad_norm": 4.126719951629639,
      "learning_rate": 4.021294133391732e-05,
      "loss": 0.6638,
      "step": 1287100
    },
    {
      "epoch": 11.745382874662385,
      "grad_norm": 3.6521570682525635,
      "learning_rate": 4.0212180937781346e-05,
      "loss": 0.6482,
      "step": 1287200
    },
    {
      "epoch": 11.74629535002555,
      "grad_norm": 3.5028674602508545,
      "learning_rate": 4.0211420541645376e-05,
      "loss": 0.667,
      "step": 1287300
    },
    {
      "epoch": 11.747207825388715,
      "grad_norm": 3.682527542114258,
      "learning_rate": 4.0210660145509406e-05,
      "loss": 0.696,
      "step": 1287400
    },
    {
      "epoch": 11.74812030075188,
      "grad_norm": 4.098587512969971,
      "learning_rate": 4.0209899749373437e-05,
      "loss": 0.6426,
      "step": 1287500
    },
    {
      "epoch": 11.749032776115046,
      "grad_norm": 3.473855972290039,
      "learning_rate": 4.020913935323746e-05,
      "loss": 0.7175,
      "step": 1287600
    },
    {
      "epoch": 11.74994525147821,
      "grad_norm": 3.8749711513519287,
      "learning_rate": 4.02083789571015e-05,
      "loss": 0.6775,
      "step": 1287700
    },
    {
      "epoch": 11.750857726841375,
      "grad_norm": 4.615688323974609,
      "learning_rate": 4.020761856096552e-05,
      "loss": 0.6486,
      "step": 1287800
    },
    {
      "epoch": 11.75177020220454,
      "grad_norm": 3.6281394958496094,
      "learning_rate": 4.020685816482955e-05,
      "loss": 0.6759,
      "step": 1287900
    },
    {
      "epoch": 11.752682677567705,
      "grad_norm": 4.8701276779174805,
      "learning_rate": 4.020609776869358e-05,
      "loss": 0.6688,
      "step": 1288000
    },
    {
      "epoch": 11.75359515293087,
      "grad_norm": 5.4353766441345215,
      "learning_rate": 4.020533737255761e-05,
      "loss": 0.677,
      "step": 1288100
    },
    {
      "epoch": 11.754507628294036,
      "grad_norm": 3.38730788230896,
      "learning_rate": 4.020457697642164e-05,
      "loss": 0.6923,
      "step": 1288200
    },
    {
      "epoch": 11.755420103657201,
      "grad_norm": 4.5801777839660645,
      "learning_rate": 4.020381658028567e-05,
      "loss": 0.681,
      "step": 1288300
    },
    {
      "epoch": 11.756332579020366,
      "grad_norm": 3.5791497230529785,
      "learning_rate": 4.0203056184149694e-05,
      "loss": 0.6999,
      "step": 1288400
    },
    {
      "epoch": 11.757245054383532,
      "grad_norm": 4.192854881286621,
      "learning_rate": 4.020229578801373e-05,
      "loss": 0.6559,
      "step": 1288500
    },
    {
      "epoch": 11.758157529746697,
      "grad_norm": 3.868668794631958,
      "learning_rate": 4.0201535391877754e-05,
      "loss": 0.6799,
      "step": 1288600
    },
    {
      "epoch": 11.759070005109862,
      "grad_norm": 3.9065446853637695,
      "learning_rate": 4.0200774995741784e-05,
      "loss": 0.6633,
      "step": 1288700
    },
    {
      "epoch": 11.759982480473028,
      "grad_norm": 3.207134246826172,
      "learning_rate": 4.0200014599605814e-05,
      "loss": 0.6612,
      "step": 1288800
    },
    {
      "epoch": 11.760894955836193,
      "grad_norm": 3.794767141342163,
      "learning_rate": 4.0199254203469844e-05,
      "loss": 0.6537,
      "step": 1288900
    },
    {
      "epoch": 11.761807431199358,
      "grad_norm": 4.404263019561768,
      "learning_rate": 4.019849380733387e-05,
      "loss": 0.6724,
      "step": 1289000
    },
    {
      "epoch": 11.762719906562523,
      "grad_norm": 3.411261796951294,
      "learning_rate": 4.0197733411197904e-05,
      "loss": 0.6553,
      "step": 1289100
    },
    {
      "epoch": 11.763632381925689,
      "grad_norm": 4.709330081939697,
      "learning_rate": 4.019697301506193e-05,
      "loss": 0.6483,
      "step": 1289200
    },
    {
      "epoch": 11.764544857288854,
      "grad_norm": 3.6279125213623047,
      "learning_rate": 4.019621261892596e-05,
      "loss": 0.6712,
      "step": 1289300
    },
    {
      "epoch": 11.76545733265202,
      "grad_norm": 3.9608559608459473,
      "learning_rate": 4.019545222278999e-05,
      "loss": 0.6899,
      "step": 1289400
    },
    {
      "epoch": 11.766369808015183,
      "grad_norm": 4.620601654052734,
      "learning_rate": 4.019469182665401e-05,
      "loss": 0.6698,
      "step": 1289500
    },
    {
      "epoch": 11.767282283378348,
      "grad_norm": 2.7250635623931885,
      "learning_rate": 4.019393143051805e-05,
      "loss": 0.6544,
      "step": 1289600
    },
    {
      "epoch": 11.768194758741513,
      "grad_norm": 3.8889386653900146,
      "learning_rate": 4.019317103438207e-05,
      "loss": 0.6419,
      "step": 1289700
    },
    {
      "epoch": 11.769107234104679,
      "grad_norm": 3.359306573867798,
      "learning_rate": 4.01924106382461e-05,
      "loss": 0.6513,
      "step": 1289800
    },
    {
      "epoch": 11.770019709467844,
      "grad_norm": 4.147614479064941,
      "learning_rate": 4.019165024211013e-05,
      "loss": 0.7024,
      "step": 1289900
    },
    {
      "epoch": 11.77093218483101,
      "grad_norm": 3.862950325012207,
      "learning_rate": 4.019088984597416e-05,
      "loss": 0.6734,
      "step": 1290000
    },
    {
      "epoch": 11.771844660194175,
      "grad_norm": 4.316650867462158,
      "learning_rate": 4.0190129449838184e-05,
      "loss": 0.6973,
      "step": 1290100
    },
    {
      "epoch": 11.77275713555734,
      "grad_norm": 4.342571258544922,
      "learning_rate": 4.018936905370222e-05,
      "loss": 0.671,
      "step": 1290200
    },
    {
      "epoch": 11.773669610920505,
      "grad_norm": 3.574510335922241,
      "learning_rate": 4.0188608657566245e-05,
      "loss": 0.6531,
      "step": 1290300
    },
    {
      "epoch": 11.77458208628367,
      "grad_norm": 4.272396564483643,
      "learning_rate": 4.0187848261430275e-05,
      "loss": 0.685,
      "step": 1290400
    },
    {
      "epoch": 11.775494561646836,
      "grad_norm": 3.5882151126861572,
      "learning_rate": 4.0187087865294305e-05,
      "loss": 0.6302,
      "step": 1290500
    },
    {
      "epoch": 11.776407037010001,
      "grad_norm": 4.1008524894714355,
      "learning_rate": 4.0186327469158335e-05,
      "loss": 0.6616,
      "step": 1290600
    },
    {
      "epoch": 11.777319512373166,
      "grad_norm": 3.7268598079681396,
      "learning_rate": 4.0185567073022365e-05,
      "loss": 0.6843,
      "step": 1290700
    },
    {
      "epoch": 11.778231987736332,
      "grad_norm": 3.9724977016448975,
      "learning_rate": 4.0184806676886395e-05,
      "loss": 0.6724,
      "step": 1290800
    },
    {
      "epoch": 11.779144463099497,
      "grad_norm": 3.552877902984619,
      "learning_rate": 4.018404628075042e-05,
      "loss": 0.6587,
      "step": 1290900
    },
    {
      "epoch": 11.780056938462662,
      "grad_norm": 3.348281145095825,
      "learning_rate": 4.0183285884614455e-05,
      "loss": 0.6789,
      "step": 1291000
    },
    {
      "epoch": 11.780969413825826,
      "grad_norm": 3.228489875793457,
      "learning_rate": 4.018252548847848e-05,
      "loss": 0.6642,
      "step": 1291100
    },
    {
      "epoch": 11.781881889188991,
      "grad_norm": 4.239412784576416,
      "learning_rate": 4.018176509234251e-05,
      "loss": 0.6801,
      "step": 1291200
    },
    {
      "epoch": 11.782794364552156,
      "grad_norm": 4.511492729187012,
      "learning_rate": 4.018100469620654e-05,
      "loss": 0.6947,
      "step": 1291300
    },
    {
      "epoch": 11.783706839915322,
      "grad_norm": 4.013190269470215,
      "learning_rate": 4.018024430007057e-05,
      "loss": 0.7098,
      "step": 1291400
    },
    {
      "epoch": 11.784619315278487,
      "grad_norm": 2.6578309535980225,
      "learning_rate": 4.017948390393459e-05,
      "loss": 0.6643,
      "step": 1291500
    },
    {
      "epoch": 11.785531790641652,
      "grad_norm": 4.230347156524658,
      "learning_rate": 4.017872350779863e-05,
      "loss": 0.6635,
      "step": 1291600
    },
    {
      "epoch": 11.786444266004818,
      "grad_norm": 4.2939605712890625,
      "learning_rate": 4.017796311166265e-05,
      "loss": 0.7262,
      "step": 1291700
    },
    {
      "epoch": 11.787356741367983,
      "grad_norm": 4.481505393981934,
      "learning_rate": 4.017720271552668e-05,
      "loss": 0.6874,
      "step": 1291800
    },
    {
      "epoch": 11.788269216731148,
      "grad_norm": 4.675512790679932,
      "learning_rate": 4.017644231939071e-05,
      "loss": 0.6816,
      "step": 1291900
    },
    {
      "epoch": 11.789181692094314,
      "grad_norm": 4.359194278717041,
      "learning_rate": 4.017568192325474e-05,
      "loss": 0.6783,
      "step": 1292000
    },
    {
      "epoch": 11.790094167457479,
      "grad_norm": 4.146300315856934,
      "learning_rate": 4.017492152711877e-05,
      "loss": 0.6316,
      "step": 1292100
    },
    {
      "epoch": 11.791006642820644,
      "grad_norm": 4.324376583099365,
      "learning_rate": 4.01741611309828e-05,
      "loss": 0.7413,
      "step": 1292200
    },
    {
      "epoch": 11.79191911818381,
      "grad_norm": 3.9585587978363037,
      "learning_rate": 4.0173400734846826e-05,
      "loss": 0.7278,
      "step": 1292300
    },
    {
      "epoch": 11.792831593546975,
      "grad_norm": 3.094426155090332,
      "learning_rate": 4.0172640338710856e-05,
      "loss": 0.682,
      "step": 1292400
    },
    {
      "epoch": 11.79374406891014,
      "grad_norm": 3.8327927589416504,
      "learning_rate": 4.0171879942574886e-05,
      "loss": 0.6877,
      "step": 1292500
    },
    {
      "epoch": 11.794656544273305,
      "grad_norm": 3.115838050842285,
      "learning_rate": 4.017111954643891e-05,
      "loss": 0.693,
      "step": 1292600
    },
    {
      "epoch": 11.79556901963647,
      "grad_norm": 4.807876110076904,
      "learning_rate": 4.0170359150302946e-05,
      "loss": 0.6875,
      "step": 1292700
    },
    {
      "epoch": 11.796481494999636,
      "grad_norm": 3.9961843490600586,
      "learning_rate": 4.016959875416697e-05,
      "loss": 0.7086,
      "step": 1292800
    },
    {
      "epoch": 11.7973939703628,
      "grad_norm": 3.608848810195923,
      "learning_rate": 4.0168838358031e-05,
      "loss": 0.691,
      "step": 1292900
    },
    {
      "epoch": 11.798306445725965,
      "grad_norm": 4.232747554779053,
      "learning_rate": 4.016807796189503e-05,
      "loss": 0.6657,
      "step": 1293000
    },
    {
      "epoch": 11.79921892108913,
      "grad_norm": 2.9001035690307617,
      "learning_rate": 4.016731756575906e-05,
      "loss": 0.6818,
      "step": 1293100
    },
    {
      "epoch": 11.800131396452295,
      "grad_norm": 3.938257932662964,
      "learning_rate": 4.016655716962309e-05,
      "loss": 0.6755,
      "step": 1293200
    },
    {
      "epoch": 11.80104387181546,
      "grad_norm": 4.299366474151611,
      "learning_rate": 4.016579677348712e-05,
      "loss": 0.6558,
      "step": 1293300
    },
    {
      "epoch": 11.801956347178626,
      "grad_norm": 3.344550132751465,
      "learning_rate": 4.016503637735114e-05,
      "loss": 0.7258,
      "step": 1293400
    },
    {
      "epoch": 11.802868822541791,
      "grad_norm": 4.234736919403076,
      "learning_rate": 4.016427598121518e-05,
      "loss": 0.6682,
      "step": 1293500
    },
    {
      "epoch": 11.803781297904957,
      "grad_norm": 3.652801990509033,
      "learning_rate": 4.01635155850792e-05,
      "loss": 0.6862,
      "step": 1293600
    },
    {
      "epoch": 11.804693773268122,
      "grad_norm": 4.730545997619629,
      "learning_rate": 4.016275518894323e-05,
      "loss": 0.6762,
      "step": 1293700
    },
    {
      "epoch": 11.805606248631287,
      "grad_norm": 3.8451499938964844,
      "learning_rate": 4.016199479280726e-05,
      "loss": 0.6724,
      "step": 1293800
    },
    {
      "epoch": 11.806518723994452,
      "grad_norm": 4.43277645111084,
      "learning_rate": 4.016123439667129e-05,
      "loss": 0.6427,
      "step": 1293900
    },
    {
      "epoch": 11.807431199357618,
      "grad_norm": 3.2539875507354736,
      "learning_rate": 4.0160474000535316e-05,
      "loss": 0.7171,
      "step": 1294000
    },
    {
      "epoch": 11.808343674720783,
      "grad_norm": 4.231773376464844,
      "learning_rate": 4.015971360439935e-05,
      "loss": 0.7091,
      "step": 1294100
    },
    {
      "epoch": 11.809256150083948,
      "grad_norm": 3.6896486282348633,
      "learning_rate": 4.0158953208263376e-05,
      "loss": 0.6502,
      "step": 1294200
    },
    {
      "epoch": 11.810168625447114,
      "grad_norm": 3.5097742080688477,
      "learning_rate": 4.0158192812127407e-05,
      "loss": 0.6897,
      "step": 1294300
    },
    {
      "epoch": 11.811081100810279,
      "grad_norm": 4.637907028198242,
      "learning_rate": 4.015743241599144e-05,
      "loss": 0.6518,
      "step": 1294400
    },
    {
      "epoch": 11.811993576173442,
      "grad_norm": 4.201749324798584,
      "learning_rate": 4.015667201985547e-05,
      "loss": 0.6716,
      "step": 1294500
    },
    {
      "epoch": 11.812906051536608,
      "grad_norm": 3.1173644065856934,
      "learning_rate": 4.01559116237195e-05,
      "loss": 0.6773,
      "step": 1294600
    },
    {
      "epoch": 11.813818526899773,
      "grad_norm": 3.444136142730713,
      "learning_rate": 4.015515122758353e-05,
      "loss": 0.6867,
      "step": 1294700
    },
    {
      "epoch": 11.814731002262938,
      "grad_norm": 4.061328887939453,
      "learning_rate": 4.015439083144755e-05,
      "loss": 0.6849,
      "step": 1294800
    },
    {
      "epoch": 11.815643477626104,
      "grad_norm": 3.5906341075897217,
      "learning_rate": 4.015363043531159e-05,
      "loss": 0.6967,
      "step": 1294900
    },
    {
      "epoch": 11.816555952989269,
      "grad_norm": 4.296485900878906,
      "learning_rate": 4.015287003917561e-05,
      "loss": 0.672,
      "step": 1295000
    },
    {
      "epoch": 11.817468428352434,
      "grad_norm": 3.920457363128662,
      "learning_rate": 4.015210964303964e-05,
      "loss": 0.6934,
      "step": 1295100
    },
    {
      "epoch": 11.8183809037156,
      "grad_norm": 4.1006340980529785,
      "learning_rate": 4.015134924690367e-05,
      "loss": 0.7409,
      "step": 1295200
    },
    {
      "epoch": 11.819293379078765,
      "grad_norm": 3.626774787902832,
      "learning_rate": 4.0150588850767694e-05,
      "loss": 0.6973,
      "step": 1295300
    },
    {
      "epoch": 11.82020585444193,
      "grad_norm": 4.031676769256592,
      "learning_rate": 4.014982845463173e-05,
      "loss": 0.6858,
      "step": 1295400
    },
    {
      "epoch": 11.821118329805095,
      "grad_norm": 4.405142784118652,
      "learning_rate": 4.0149068058495754e-05,
      "loss": 0.674,
      "step": 1295500
    },
    {
      "epoch": 11.82203080516826,
      "grad_norm": 4.118856430053711,
      "learning_rate": 4.0148307662359784e-05,
      "loss": 0.7175,
      "step": 1295600
    },
    {
      "epoch": 11.822943280531426,
      "grad_norm": 4.145010948181152,
      "learning_rate": 4.0147547266223814e-05,
      "loss": 0.654,
      "step": 1295700
    },
    {
      "epoch": 11.823855755894591,
      "grad_norm": 3.6093626022338867,
      "learning_rate": 4.0146786870087844e-05,
      "loss": 0.6467,
      "step": 1295800
    },
    {
      "epoch": 11.824768231257757,
      "grad_norm": 3.6544077396392822,
      "learning_rate": 4.014602647395187e-05,
      "loss": 0.6764,
      "step": 1295900
    },
    {
      "epoch": 11.825680706620922,
      "grad_norm": 2.607083559036255,
      "learning_rate": 4.0145266077815904e-05,
      "loss": 0.7201,
      "step": 1296000
    },
    {
      "epoch": 11.826593181984087,
      "grad_norm": 2.957228660583496,
      "learning_rate": 4.014450568167993e-05,
      "loss": 0.6882,
      "step": 1296100
    },
    {
      "epoch": 11.827505657347253,
      "grad_norm": 3.959618091583252,
      "learning_rate": 4.014374528554396e-05,
      "loss": 0.6616,
      "step": 1296200
    },
    {
      "epoch": 11.828418132710416,
      "grad_norm": 3.643256902694702,
      "learning_rate": 4.014298488940799e-05,
      "loss": 0.6467,
      "step": 1296300
    },
    {
      "epoch": 11.829330608073581,
      "grad_norm": 5.075375080108643,
      "learning_rate": 4.014222449327202e-05,
      "loss": 0.6636,
      "step": 1296400
    },
    {
      "epoch": 11.830243083436747,
      "grad_norm": 4.438894271850586,
      "learning_rate": 4.014146409713605e-05,
      "loss": 0.6856,
      "step": 1296500
    },
    {
      "epoch": 11.831155558799912,
      "grad_norm": 2.768714189529419,
      "learning_rate": 4.014070370100008e-05,
      "loss": 0.6873,
      "step": 1296600
    },
    {
      "epoch": 11.832068034163077,
      "grad_norm": 3.9509449005126953,
      "learning_rate": 4.01399433048641e-05,
      "loss": 0.6801,
      "step": 1296700
    },
    {
      "epoch": 11.832980509526243,
      "grad_norm": 4.21771240234375,
      "learning_rate": 4.013918290872814e-05,
      "loss": 0.6683,
      "step": 1296800
    },
    {
      "epoch": 11.833892984889408,
      "grad_norm": 4.526158332824707,
      "learning_rate": 4.013842251259216e-05,
      "loss": 0.6528,
      "step": 1296900
    },
    {
      "epoch": 11.834805460252573,
      "grad_norm": 4.1427812576293945,
      "learning_rate": 4.013766211645619e-05,
      "loss": 0.6817,
      "step": 1297000
    },
    {
      "epoch": 11.835717935615738,
      "grad_norm": 3.968546152114868,
      "learning_rate": 4.013690172032022e-05,
      "loss": 0.6685,
      "step": 1297100
    },
    {
      "epoch": 11.836630410978904,
      "grad_norm": 3.494819164276123,
      "learning_rate": 4.013614132418425e-05,
      "loss": 0.6989,
      "step": 1297200
    },
    {
      "epoch": 11.837542886342069,
      "grad_norm": 3.8812479972839355,
      "learning_rate": 4.0135380928048275e-05,
      "loss": 0.6436,
      "step": 1297300
    },
    {
      "epoch": 11.838455361705234,
      "grad_norm": 3.901278018951416,
      "learning_rate": 4.013462053191231e-05,
      "loss": 0.6507,
      "step": 1297400
    },
    {
      "epoch": 11.8393678370684,
      "grad_norm": 3.058864116668701,
      "learning_rate": 4.0133860135776335e-05,
      "loss": 0.6611,
      "step": 1297500
    },
    {
      "epoch": 11.840280312431565,
      "grad_norm": 3.6051740646362305,
      "learning_rate": 4.0133099739640365e-05,
      "loss": 0.6625,
      "step": 1297600
    },
    {
      "epoch": 11.84119278779473,
      "grad_norm": 3.021233081817627,
      "learning_rate": 4.0132339343504395e-05,
      "loss": 0.6917,
      "step": 1297700
    },
    {
      "epoch": 11.842105263157894,
      "grad_norm": 4.661870956420898,
      "learning_rate": 4.0131578947368425e-05,
      "loss": 0.6427,
      "step": 1297800
    },
    {
      "epoch": 11.843017738521059,
      "grad_norm": 3.909061908721924,
      "learning_rate": 4.0130818551232455e-05,
      "loss": 0.6828,
      "step": 1297900
    },
    {
      "epoch": 11.843930213884224,
      "grad_norm": 3.9048070907592773,
      "learning_rate": 4.013005815509648e-05,
      "loss": 0.6849,
      "step": 1298000
    },
    {
      "epoch": 11.84484268924739,
      "grad_norm": 4.283480644226074,
      "learning_rate": 4.012929775896051e-05,
      "loss": 0.6765,
      "step": 1298100
    },
    {
      "epoch": 11.845755164610555,
      "grad_norm": 3.313293695449829,
      "learning_rate": 4.012853736282454e-05,
      "loss": 0.6662,
      "step": 1298200
    },
    {
      "epoch": 11.84666763997372,
      "grad_norm": 4.522097110748291,
      "learning_rate": 4.012777696668857e-05,
      "loss": 0.7038,
      "step": 1298300
    },
    {
      "epoch": 11.847580115336886,
      "grad_norm": 4.4590678215026855,
      "learning_rate": 4.012701657055259e-05,
      "loss": 0.7326,
      "step": 1298400
    },
    {
      "epoch": 11.848492590700051,
      "grad_norm": 4.409082412719727,
      "learning_rate": 4.012625617441663e-05,
      "loss": 0.6521,
      "step": 1298500
    },
    {
      "epoch": 11.849405066063216,
      "grad_norm": 4.013707637786865,
      "learning_rate": 4.012549577828065e-05,
      "loss": 0.6574,
      "step": 1298600
    },
    {
      "epoch": 11.850317541426381,
      "grad_norm": 5.8282976150512695,
      "learning_rate": 4.012473538214468e-05,
      "loss": 0.6841,
      "step": 1298700
    },
    {
      "epoch": 11.851230016789547,
      "grad_norm": 3.5006210803985596,
      "learning_rate": 4.012397498600871e-05,
      "loss": 0.6751,
      "step": 1298800
    },
    {
      "epoch": 11.852142492152712,
      "grad_norm": 5.215115070343018,
      "learning_rate": 4.012321458987274e-05,
      "loss": 0.6944,
      "step": 1298900
    },
    {
      "epoch": 11.853054967515877,
      "grad_norm": 4.1742024421691895,
      "learning_rate": 4.012245419373677e-05,
      "loss": 0.6669,
      "step": 1299000
    },
    {
      "epoch": 11.853967442879043,
      "grad_norm": 4.345017433166504,
      "learning_rate": 4.01216937976008e-05,
      "loss": 0.7314,
      "step": 1299100
    },
    {
      "epoch": 11.854879918242208,
      "grad_norm": 5.661180019378662,
      "learning_rate": 4.0120933401464826e-05,
      "loss": 0.6378,
      "step": 1299200
    },
    {
      "epoch": 11.855792393605373,
      "grad_norm": 5.13576602935791,
      "learning_rate": 4.012017300532886e-05,
      "loss": 0.6927,
      "step": 1299300
    },
    {
      "epoch": 11.856704868968539,
      "grad_norm": 4.0553812980651855,
      "learning_rate": 4.0119412609192886e-05,
      "loss": 0.6877,
      "step": 1299400
    },
    {
      "epoch": 11.857617344331704,
      "grad_norm": 3.9266223907470703,
      "learning_rate": 4.0118652213056916e-05,
      "loss": 0.7055,
      "step": 1299500
    },
    {
      "epoch": 11.858529819694867,
      "grad_norm": 3.8080179691314697,
      "learning_rate": 4.0117891816920946e-05,
      "loss": 0.7023,
      "step": 1299600
    },
    {
      "epoch": 11.859442295058033,
      "grad_norm": 3.9729301929473877,
      "learning_rate": 4.0117131420784976e-05,
      "loss": 0.7008,
      "step": 1299700
    },
    {
      "epoch": 11.860354770421198,
      "grad_norm": 3.9673352241516113,
      "learning_rate": 4.0116371024649e-05,
      "loss": 0.6374,
      "step": 1299800
    },
    {
      "epoch": 11.861267245784363,
      "grad_norm": 3.690009117126465,
      "learning_rate": 4.0115610628513036e-05,
      "loss": 0.7044,
      "step": 1299900
    },
    {
      "epoch": 11.862179721147529,
      "grad_norm": 4.127712249755859,
      "learning_rate": 4.011485023237706e-05,
      "loss": 0.6877,
      "step": 1300000
    },
    {
      "epoch": 11.863092196510694,
      "grad_norm": 2.4148998260498047,
      "learning_rate": 4.011408983624109e-05,
      "loss": 0.6996,
      "step": 1300100
    },
    {
      "epoch": 11.86400467187386,
      "grad_norm": 3.1627864837646484,
      "learning_rate": 4.011332944010512e-05,
      "loss": 0.677,
      "step": 1300200
    },
    {
      "epoch": 11.864917147237025,
      "grad_norm": 3.7802045345306396,
      "learning_rate": 4.011256904396915e-05,
      "loss": 0.6861,
      "step": 1300300
    },
    {
      "epoch": 11.86582962260019,
      "grad_norm": 3.3825554847717285,
      "learning_rate": 4.011180864783318e-05,
      "loss": 0.6537,
      "step": 1300400
    },
    {
      "epoch": 11.866742097963355,
      "grad_norm": 2.8783318996429443,
      "learning_rate": 4.011104825169721e-05,
      "loss": 0.6934,
      "step": 1300500
    },
    {
      "epoch": 11.86765457332652,
      "grad_norm": 3.7306904792785645,
      "learning_rate": 4.011028785556123e-05,
      "loss": 0.6795,
      "step": 1300600
    },
    {
      "epoch": 11.868567048689686,
      "grad_norm": 4.786916732788086,
      "learning_rate": 4.010952745942527e-05,
      "loss": 0.7095,
      "step": 1300700
    },
    {
      "epoch": 11.869479524052851,
      "grad_norm": 4.809361934661865,
      "learning_rate": 4.010876706328929e-05,
      "loss": 0.6968,
      "step": 1300800
    },
    {
      "epoch": 11.870391999416016,
      "grad_norm": 3.1129298210144043,
      "learning_rate": 4.0108006667153316e-05,
      "loss": 0.654,
      "step": 1300900
    },
    {
      "epoch": 11.871304474779182,
      "grad_norm": 3.934847116470337,
      "learning_rate": 4.010724627101735e-05,
      "loss": 0.6789,
      "step": 1301000
    },
    {
      "epoch": 11.872216950142347,
      "grad_norm": 3.8028392791748047,
      "learning_rate": 4.0106485874881377e-05,
      "loss": 0.6724,
      "step": 1301100
    },
    {
      "epoch": 11.87312942550551,
      "grad_norm": 3.4587185382843018,
      "learning_rate": 4.010572547874541e-05,
      "loss": 0.6586,
      "step": 1301200
    },
    {
      "epoch": 11.874041900868676,
      "grad_norm": 4.476672649383545,
      "learning_rate": 4.010496508260944e-05,
      "loss": 0.6795,
      "step": 1301300
    },
    {
      "epoch": 11.874954376231841,
      "grad_norm": 3.0589406490325928,
      "learning_rate": 4.010420468647347e-05,
      "loss": 0.716,
      "step": 1301400
    },
    {
      "epoch": 11.875866851595006,
      "grad_norm": 3.8684802055358887,
      "learning_rate": 4.01034442903375e-05,
      "loss": 0.6739,
      "step": 1301500
    },
    {
      "epoch": 11.876779326958172,
      "grad_norm": 4.269725799560547,
      "learning_rate": 4.010268389420153e-05,
      "loss": 0.6802,
      "step": 1301600
    },
    {
      "epoch": 11.877691802321337,
      "grad_norm": 4.014540672302246,
      "learning_rate": 4.010192349806555e-05,
      "loss": 0.6738,
      "step": 1301700
    },
    {
      "epoch": 11.878604277684502,
      "grad_norm": 4.558277130126953,
      "learning_rate": 4.010116310192959e-05,
      "loss": 0.7199,
      "step": 1301800
    },
    {
      "epoch": 11.879516753047668,
      "grad_norm": 3.7506041526794434,
      "learning_rate": 4.010040270579361e-05,
      "loss": 0.6855,
      "step": 1301900
    },
    {
      "epoch": 11.880429228410833,
      "grad_norm": 3.430391550064087,
      "learning_rate": 4.009964230965764e-05,
      "loss": 0.711,
      "step": 1302000
    },
    {
      "epoch": 11.881341703773998,
      "grad_norm": 2.818507671356201,
      "learning_rate": 4.009888191352167e-05,
      "loss": 0.6897,
      "step": 1302100
    },
    {
      "epoch": 11.882254179137163,
      "grad_norm": 4.787791728973389,
      "learning_rate": 4.00981215173857e-05,
      "loss": 0.7049,
      "step": 1302200
    },
    {
      "epoch": 11.883166654500329,
      "grad_norm": 3.900374412536621,
      "learning_rate": 4.0097361121249724e-05,
      "loss": 0.6565,
      "step": 1302300
    },
    {
      "epoch": 11.884079129863494,
      "grad_norm": 3.8803863525390625,
      "learning_rate": 4.009660072511376e-05,
      "loss": 0.6443,
      "step": 1302400
    },
    {
      "epoch": 11.88499160522666,
      "grad_norm": 3.7875614166259766,
      "learning_rate": 4.0095840328977784e-05,
      "loss": 0.7035,
      "step": 1302500
    },
    {
      "epoch": 11.885904080589825,
      "grad_norm": 4.132124900817871,
      "learning_rate": 4.0095079932841814e-05,
      "loss": 0.6869,
      "step": 1302600
    },
    {
      "epoch": 11.88681655595299,
      "grad_norm": 4.060182571411133,
      "learning_rate": 4.0094319536705844e-05,
      "loss": 0.7222,
      "step": 1302700
    },
    {
      "epoch": 11.887729031316155,
      "grad_norm": 3.73572039604187,
      "learning_rate": 4.0093559140569874e-05,
      "loss": 0.6933,
      "step": 1302800
    },
    {
      "epoch": 11.88864150667932,
      "grad_norm": 4.146483421325684,
      "learning_rate": 4.0092798744433904e-05,
      "loss": 0.6649,
      "step": 1302900
    },
    {
      "epoch": 11.889553982042484,
      "grad_norm": 3.0736777782440186,
      "learning_rate": 4.0092038348297934e-05,
      "loss": 0.6437,
      "step": 1303000
    },
    {
      "epoch": 11.89046645740565,
      "grad_norm": 4.092207908630371,
      "learning_rate": 4.009127795216196e-05,
      "loss": 0.6701,
      "step": 1303100
    },
    {
      "epoch": 11.891378932768815,
      "grad_norm": 7.5179901123046875,
      "learning_rate": 4.0090517556025994e-05,
      "loss": 0.6498,
      "step": 1303200
    },
    {
      "epoch": 11.89229140813198,
      "grad_norm": 3.6834607124328613,
      "learning_rate": 4.008975715989002e-05,
      "loss": 0.6603,
      "step": 1303300
    },
    {
      "epoch": 11.893203883495145,
      "grad_norm": 4.312807559967041,
      "learning_rate": 4.008899676375405e-05,
      "loss": 0.6862,
      "step": 1303400
    },
    {
      "epoch": 11.89411635885831,
      "grad_norm": 2.9474496841430664,
      "learning_rate": 4.008823636761808e-05,
      "loss": 0.6844,
      "step": 1303500
    },
    {
      "epoch": 11.895028834221476,
      "grad_norm": 3.978854179382324,
      "learning_rate": 4.008747597148211e-05,
      "loss": 0.6923,
      "step": 1303600
    },
    {
      "epoch": 11.895941309584641,
      "grad_norm": 3.7529096603393555,
      "learning_rate": 4.008671557534613e-05,
      "loss": 0.6357,
      "step": 1303700
    },
    {
      "epoch": 11.896853784947806,
      "grad_norm": 3.942185401916504,
      "learning_rate": 4.008595517921016e-05,
      "loss": 0.6645,
      "step": 1303800
    },
    {
      "epoch": 11.897766260310972,
      "grad_norm": 3.0566697120666504,
      "learning_rate": 4.008519478307419e-05,
      "loss": 0.6235,
      "step": 1303900
    },
    {
      "epoch": 11.898678735674137,
      "grad_norm": 4.744754314422607,
      "learning_rate": 4.008443438693822e-05,
      "loss": 0.7051,
      "step": 1304000
    },
    {
      "epoch": 11.899591211037302,
      "grad_norm": 4.135851860046387,
      "learning_rate": 4.008367399080225e-05,
      "loss": 0.6847,
      "step": 1304100
    },
    {
      "epoch": 11.900503686400468,
      "grad_norm": 4.566710948944092,
      "learning_rate": 4.0082913594666275e-05,
      "loss": 0.6763,
      "step": 1304200
    },
    {
      "epoch": 11.901416161763633,
      "grad_norm": 4.084167957305908,
      "learning_rate": 4.008215319853031e-05,
      "loss": 0.6882,
      "step": 1304300
    },
    {
      "epoch": 11.902328637126798,
      "grad_norm": 2.7805161476135254,
      "learning_rate": 4.0081392802394335e-05,
      "loss": 0.6826,
      "step": 1304400
    },
    {
      "epoch": 11.903241112489964,
      "grad_norm": 4.124208927154541,
      "learning_rate": 4.0080632406258365e-05,
      "loss": 0.669,
      "step": 1304500
    },
    {
      "epoch": 11.904153587853127,
      "grad_norm": 3.637079954147339,
      "learning_rate": 4.0079872010122395e-05,
      "loss": 0.7137,
      "step": 1304600
    },
    {
      "epoch": 11.905066063216292,
      "grad_norm": 3.7013614177703857,
      "learning_rate": 4.0079111613986425e-05,
      "loss": 0.686,
      "step": 1304700
    },
    {
      "epoch": 11.905978538579458,
      "grad_norm": 2.760610818862915,
      "learning_rate": 4.007835121785045e-05,
      "loss": 0.6852,
      "step": 1304800
    },
    {
      "epoch": 11.906891013942623,
      "grad_norm": 3.779914140701294,
      "learning_rate": 4.0077590821714485e-05,
      "loss": 0.6607,
      "step": 1304900
    },
    {
      "epoch": 11.907803489305788,
      "grad_norm": 3.815505027770996,
      "learning_rate": 4.007683042557851e-05,
      "loss": 0.6683,
      "step": 1305000
    },
    {
      "epoch": 11.908715964668954,
      "grad_norm": 3.9648773670196533,
      "learning_rate": 4.007607002944254e-05,
      "loss": 0.6603,
      "step": 1305100
    },
    {
      "epoch": 11.909628440032119,
      "grad_norm": 4.0827836990356445,
      "learning_rate": 4.007530963330657e-05,
      "loss": 0.6657,
      "step": 1305200
    },
    {
      "epoch": 11.910540915395284,
      "grad_norm": 3.9758288860321045,
      "learning_rate": 4.00745492371706e-05,
      "loss": 0.6553,
      "step": 1305300
    },
    {
      "epoch": 11.91145339075845,
      "grad_norm": 3.676104784011841,
      "learning_rate": 4.007378884103463e-05,
      "loss": 0.6938,
      "step": 1305400
    },
    {
      "epoch": 11.912365866121615,
      "grad_norm": 4.18787145614624,
      "learning_rate": 4.007302844489866e-05,
      "loss": 0.6737,
      "step": 1305500
    },
    {
      "epoch": 11.91327834148478,
      "grad_norm": 3.8419320583343506,
      "learning_rate": 4.007226804876268e-05,
      "loss": 0.685,
      "step": 1305600
    },
    {
      "epoch": 11.914190816847945,
      "grad_norm": 3.7557406425476074,
      "learning_rate": 4.007150765262672e-05,
      "loss": 0.7207,
      "step": 1305700
    },
    {
      "epoch": 11.91510329221111,
      "grad_norm": 4.561410427093506,
      "learning_rate": 4.007074725649074e-05,
      "loss": 0.6611,
      "step": 1305800
    },
    {
      "epoch": 11.916015767574276,
      "grad_norm": 3.571450710296631,
      "learning_rate": 4.006998686035477e-05,
      "loss": 0.6714,
      "step": 1305900
    },
    {
      "epoch": 11.916928242937441,
      "grad_norm": 4.285372257232666,
      "learning_rate": 4.00692264642188e-05,
      "loss": 0.6798,
      "step": 1306000
    },
    {
      "epoch": 11.917840718300607,
      "grad_norm": 4.204955101013184,
      "learning_rate": 4.006846606808283e-05,
      "loss": 0.6759,
      "step": 1306100
    },
    {
      "epoch": 11.918753193663772,
      "grad_norm": 4.1741814613342285,
      "learning_rate": 4.0067705671946856e-05,
      "loss": 0.6868,
      "step": 1306200
    },
    {
      "epoch": 11.919665669026937,
      "grad_norm": 3.9328243732452393,
      "learning_rate": 4.006694527581089e-05,
      "loss": 0.6497,
      "step": 1306300
    },
    {
      "epoch": 11.9205781443901,
      "grad_norm": 3.94535231590271,
      "learning_rate": 4.0066184879674916e-05,
      "loss": 0.6628,
      "step": 1306400
    },
    {
      "epoch": 11.921490619753266,
      "grad_norm": 3.3573837280273438,
      "learning_rate": 4.0065424483538946e-05,
      "loss": 0.6867,
      "step": 1306500
    },
    {
      "epoch": 11.922403095116431,
      "grad_norm": 2.7950050830841064,
      "learning_rate": 4.0064664087402976e-05,
      "loss": 0.7193,
      "step": 1306600
    },
    {
      "epoch": 11.923315570479597,
      "grad_norm": 3.4392480850219727,
      "learning_rate": 4.0063903691267e-05,
      "loss": 0.6561,
      "step": 1306700
    },
    {
      "epoch": 11.924228045842762,
      "grad_norm": 3.6130361557006836,
      "learning_rate": 4.0063143295131036e-05,
      "loss": 0.6639,
      "step": 1306800
    },
    {
      "epoch": 11.925140521205927,
      "grad_norm": 4.949577331542969,
      "learning_rate": 4.006238289899506e-05,
      "loss": 0.6689,
      "step": 1306900
    },
    {
      "epoch": 11.926052996569092,
      "grad_norm": 4.2032341957092285,
      "learning_rate": 4.006162250285909e-05,
      "loss": 0.6547,
      "step": 1307000
    },
    {
      "epoch": 11.926965471932258,
      "grad_norm": 4.176008224487305,
      "learning_rate": 4.006086210672312e-05,
      "loss": 0.6789,
      "step": 1307100
    },
    {
      "epoch": 11.927877947295423,
      "grad_norm": 3.1854512691497803,
      "learning_rate": 4.006010171058715e-05,
      "loss": 0.663,
      "step": 1307200
    },
    {
      "epoch": 11.928790422658588,
      "grad_norm": 4.420022010803223,
      "learning_rate": 4.005934131445118e-05,
      "loss": 0.6602,
      "step": 1307300
    },
    {
      "epoch": 11.929702898021754,
      "grad_norm": 3.918097496032715,
      "learning_rate": 4.005858091831521e-05,
      "loss": 0.6705,
      "step": 1307400
    },
    {
      "epoch": 11.930615373384919,
      "grad_norm": 4.735918998718262,
      "learning_rate": 4.005782052217923e-05,
      "loss": 0.6959,
      "step": 1307500
    },
    {
      "epoch": 11.931527848748084,
      "grad_norm": 4.872513771057129,
      "learning_rate": 4.005706012604326e-05,
      "loss": 0.6731,
      "step": 1307600
    },
    {
      "epoch": 11.93244032411125,
      "grad_norm": 3.0357179641723633,
      "learning_rate": 4.005629972990729e-05,
      "loss": 0.682,
      "step": 1307700
    },
    {
      "epoch": 11.933352799474415,
      "grad_norm": 5.000649452209473,
      "learning_rate": 4.005553933377132e-05,
      "loss": 0.6776,
      "step": 1307800
    },
    {
      "epoch": 11.93426527483758,
      "grad_norm": 4.1885151863098145,
      "learning_rate": 4.0054778937635353e-05,
      "loss": 0.6581,
      "step": 1307900
    },
    {
      "epoch": 11.935177750200744,
      "grad_norm": 3.238882064819336,
      "learning_rate": 4.0054018541499383e-05,
      "loss": 0.6537,
      "step": 1308000
    },
    {
      "epoch": 11.936090225563909,
      "grad_norm": 3.065566062927246,
      "learning_rate": 4.005325814536341e-05,
      "loss": 0.6691,
      "step": 1308100
    },
    {
      "epoch": 11.937002700927074,
      "grad_norm": 3.7278904914855957,
      "learning_rate": 4.0052497749227444e-05,
      "loss": 0.6752,
      "step": 1308200
    },
    {
      "epoch": 11.93791517629024,
      "grad_norm": 2.179111957550049,
      "learning_rate": 4.005173735309147e-05,
      "loss": 0.692,
      "step": 1308300
    },
    {
      "epoch": 11.938827651653405,
      "grad_norm": 4.895534038543701,
      "learning_rate": 4.00509769569555e-05,
      "loss": 0.6842,
      "step": 1308400
    },
    {
      "epoch": 11.93974012701657,
      "grad_norm": 3.806088924407959,
      "learning_rate": 4.005021656081953e-05,
      "loss": 0.649,
      "step": 1308500
    },
    {
      "epoch": 11.940652602379735,
      "grad_norm": 3.883552312850952,
      "learning_rate": 4.004945616468356e-05,
      "loss": 0.7157,
      "step": 1308600
    },
    {
      "epoch": 11.9415650777429,
      "grad_norm": 4.061610698699951,
      "learning_rate": 4.004869576854759e-05,
      "loss": 0.621,
      "step": 1308700
    },
    {
      "epoch": 11.942477553106066,
      "grad_norm": 3.927053213119507,
      "learning_rate": 4.004793537241162e-05,
      "loss": 0.678,
      "step": 1308800
    },
    {
      "epoch": 11.943390028469231,
      "grad_norm": 3.3469343185424805,
      "learning_rate": 4.004717497627564e-05,
      "loss": 0.6875,
      "step": 1308900
    },
    {
      "epoch": 11.944302503832397,
      "grad_norm": 4.491738796234131,
      "learning_rate": 4.004641458013967e-05,
      "loss": 0.709,
      "step": 1309000
    },
    {
      "epoch": 11.945214979195562,
      "grad_norm": 3.9309797286987305,
      "learning_rate": 4.00456541840037e-05,
      "loss": 0.7161,
      "step": 1309100
    },
    {
      "epoch": 11.946127454558727,
      "grad_norm": 3.675997257232666,
      "learning_rate": 4.004489378786773e-05,
      "loss": 0.6753,
      "step": 1309200
    },
    {
      "epoch": 11.947039929921893,
      "grad_norm": 3.6477932929992676,
      "learning_rate": 4.004413339173176e-05,
      "loss": 0.6778,
      "step": 1309300
    },
    {
      "epoch": 11.947952405285058,
      "grad_norm": 4.65998649597168,
      "learning_rate": 4.0043372995595784e-05,
      "loss": 0.6651,
      "step": 1309400
    },
    {
      "epoch": 11.948864880648223,
      "grad_norm": 3.6068570613861084,
      "learning_rate": 4.0042612599459814e-05,
      "loss": 0.7014,
      "step": 1309500
    },
    {
      "epoch": 11.949777356011388,
      "grad_norm": 4.710172176361084,
      "learning_rate": 4.0041852203323844e-05,
      "loss": 0.6704,
      "step": 1309600
    },
    {
      "epoch": 11.950689831374554,
      "grad_norm": 3.671600580215454,
      "learning_rate": 4.0041091807187874e-05,
      "loss": 0.6756,
      "step": 1309700
    },
    {
      "epoch": 11.951602306737717,
      "grad_norm": 3.185861825942993,
      "learning_rate": 4.0040331411051904e-05,
      "loss": 0.7083,
      "step": 1309800
    },
    {
      "epoch": 11.952514782100883,
      "grad_norm": 4.600749492645264,
      "learning_rate": 4.0039571014915934e-05,
      "loss": 0.6582,
      "step": 1309900
    },
    {
      "epoch": 11.953427257464048,
      "grad_norm": 3.9555649757385254,
      "learning_rate": 4.003881061877996e-05,
      "loss": 0.694,
      "step": 1310000
    },
    {
      "epoch": 11.954339732827213,
      "grad_norm": 2.9219822883605957,
      "learning_rate": 4.0038050222643995e-05,
      "loss": 0.6688,
      "step": 1310100
    },
    {
      "epoch": 11.955252208190378,
      "grad_norm": 3.456911563873291,
      "learning_rate": 4.003728982650802e-05,
      "loss": 0.6673,
      "step": 1310200
    },
    {
      "epoch": 11.956164683553544,
      "grad_norm": 3.8729681968688965,
      "learning_rate": 4.003652943037205e-05,
      "loss": 0.6608,
      "step": 1310300
    },
    {
      "epoch": 11.957077158916709,
      "grad_norm": 2.5537514686584473,
      "learning_rate": 4.003576903423608e-05,
      "loss": 0.6754,
      "step": 1310400
    },
    {
      "epoch": 11.957989634279874,
      "grad_norm": 4.038972854614258,
      "learning_rate": 4.003500863810011e-05,
      "loss": 0.6582,
      "step": 1310500
    },
    {
      "epoch": 11.95890210964304,
      "grad_norm": 4.053279876708984,
      "learning_rate": 4.003424824196413e-05,
      "loss": 0.7082,
      "step": 1310600
    },
    {
      "epoch": 11.959814585006205,
      "grad_norm": 4.921485424041748,
      "learning_rate": 4.003348784582817e-05,
      "loss": 0.6687,
      "step": 1310700
    },
    {
      "epoch": 11.96072706036937,
      "grad_norm": 4.416378974914551,
      "learning_rate": 4.003272744969219e-05,
      "loss": 0.6515,
      "step": 1310800
    },
    {
      "epoch": 11.961639535732536,
      "grad_norm": 3.1578476428985596,
      "learning_rate": 4.003196705355622e-05,
      "loss": 0.6836,
      "step": 1310900
    },
    {
      "epoch": 11.9625520110957,
      "grad_norm": 4.513018608093262,
      "learning_rate": 4.003120665742025e-05,
      "loss": 0.658,
      "step": 1311000
    },
    {
      "epoch": 11.963464486458866,
      "grad_norm": 3.67594575881958,
      "learning_rate": 4.003044626128428e-05,
      "loss": 0.7036,
      "step": 1311100
    },
    {
      "epoch": 11.964376961822031,
      "grad_norm": 3.9940872192382812,
      "learning_rate": 4.002968586514831e-05,
      "loss": 0.7054,
      "step": 1311200
    },
    {
      "epoch": 11.965289437185197,
      "grad_norm": 3.990324020385742,
      "learning_rate": 4.002892546901234e-05,
      "loss": 0.7055,
      "step": 1311300
    },
    {
      "epoch": 11.96620191254836,
      "grad_norm": 4.293222904205322,
      "learning_rate": 4.0028165072876365e-05,
      "loss": 0.6673,
      "step": 1311400
    },
    {
      "epoch": 11.967114387911526,
      "grad_norm": 3.612170696258545,
      "learning_rate": 4.00274046767404e-05,
      "loss": 0.6897,
      "step": 1311500
    },
    {
      "epoch": 11.968026863274691,
      "grad_norm": 4.195778846740723,
      "learning_rate": 4.0026644280604425e-05,
      "loss": 0.695,
      "step": 1311600
    },
    {
      "epoch": 11.968939338637856,
      "grad_norm": 3.527243137359619,
      "learning_rate": 4.0025883884468455e-05,
      "loss": 0.6722,
      "step": 1311700
    },
    {
      "epoch": 11.969851814001021,
      "grad_norm": 4.431530475616455,
      "learning_rate": 4.0025123488332485e-05,
      "loss": 0.6791,
      "step": 1311800
    },
    {
      "epoch": 11.970764289364187,
      "grad_norm": 4.18510627746582,
      "learning_rate": 4.0024363092196515e-05,
      "loss": 0.7169,
      "step": 1311900
    },
    {
      "epoch": 11.971676764727352,
      "grad_norm": 4.387714862823486,
      "learning_rate": 4.002360269606054e-05,
      "loss": 0.7047,
      "step": 1312000
    },
    {
      "epoch": 11.972589240090517,
      "grad_norm": 4.349844932556152,
      "learning_rate": 4.0022842299924576e-05,
      "loss": 0.6506,
      "step": 1312100
    },
    {
      "epoch": 11.973501715453683,
      "grad_norm": 3.8782711029052734,
      "learning_rate": 4.00220819037886e-05,
      "loss": 0.6599,
      "step": 1312200
    },
    {
      "epoch": 11.974414190816848,
      "grad_norm": 3.099215269088745,
      "learning_rate": 4.002132150765263e-05,
      "loss": 0.6886,
      "step": 1312300
    },
    {
      "epoch": 11.975326666180013,
      "grad_norm": 4.0878424644470215,
      "learning_rate": 4.002056111151666e-05,
      "loss": 0.6966,
      "step": 1312400
    },
    {
      "epoch": 11.976239141543179,
      "grad_norm": 4.147154808044434,
      "learning_rate": 4.001980071538068e-05,
      "loss": 0.6762,
      "step": 1312500
    },
    {
      "epoch": 11.977151616906344,
      "grad_norm": 4.034717559814453,
      "learning_rate": 4.001904031924472e-05,
      "loss": 0.6756,
      "step": 1312600
    },
    {
      "epoch": 11.97806409226951,
      "grad_norm": 3.295147657394409,
      "learning_rate": 4.001827992310874e-05,
      "loss": 0.7006,
      "step": 1312700
    },
    {
      "epoch": 11.978976567632674,
      "grad_norm": 4.234008312225342,
      "learning_rate": 4.001751952697277e-05,
      "loss": 0.7281,
      "step": 1312800
    },
    {
      "epoch": 11.97988904299584,
      "grad_norm": 4.719327449798584,
      "learning_rate": 4.00167591308368e-05,
      "loss": 0.6775,
      "step": 1312900
    },
    {
      "epoch": 11.980801518359005,
      "grad_norm": 4.3523945808410645,
      "learning_rate": 4.001599873470083e-05,
      "loss": 0.6817,
      "step": 1313000
    },
    {
      "epoch": 11.98171399372217,
      "grad_norm": 4.610622882843018,
      "learning_rate": 4.0015238338564856e-05,
      "loss": 0.6852,
      "step": 1313100
    },
    {
      "epoch": 11.982626469085334,
      "grad_norm": 3.875957489013672,
      "learning_rate": 4.001447794242889e-05,
      "loss": 0.6579,
      "step": 1313200
    },
    {
      "epoch": 11.9835389444485,
      "grad_norm": 3.336782693862915,
      "learning_rate": 4.0013717546292916e-05,
      "loss": 0.6733,
      "step": 1313300
    },
    {
      "epoch": 11.984451419811665,
      "grad_norm": 3.5791847705841064,
      "learning_rate": 4.0012957150156946e-05,
      "loss": 0.6958,
      "step": 1313400
    },
    {
      "epoch": 11.98536389517483,
      "grad_norm": 4.266664981842041,
      "learning_rate": 4.0012196754020976e-05,
      "loss": 0.6673,
      "step": 1313500
    },
    {
      "epoch": 11.986276370537995,
      "grad_norm": 3.789238214492798,
      "learning_rate": 4.0011436357885006e-05,
      "loss": 0.6793,
      "step": 1313600
    },
    {
      "epoch": 11.98718884590116,
      "grad_norm": 4.506328105926514,
      "learning_rate": 4.0010675961749036e-05,
      "loss": 0.6561,
      "step": 1313700
    },
    {
      "epoch": 11.988101321264326,
      "grad_norm": 4.072704315185547,
      "learning_rate": 4.0009915565613066e-05,
      "loss": 0.6916,
      "step": 1313800
    },
    {
      "epoch": 11.989013796627491,
      "grad_norm": 4.3890485763549805,
      "learning_rate": 4.000915516947709e-05,
      "loss": 0.6886,
      "step": 1313900
    },
    {
      "epoch": 11.989926271990656,
      "grad_norm": 4.334371089935303,
      "learning_rate": 4.0008394773341127e-05,
      "loss": 0.6806,
      "step": 1314000
    },
    {
      "epoch": 11.990838747353822,
      "grad_norm": 3.985623836517334,
      "learning_rate": 4.000763437720515e-05,
      "loss": 0.6518,
      "step": 1314100
    },
    {
      "epoch": 11.991751222716987,
      "grad_norm": 4.116730213165283,
      "learning_rate": 4.000687398106918e-05,
      "loss": 0.6826,
      "step": 1314200
    },
    {
      "epoch": 11.992663698080152,
      "grad_norm": 4.203741073608398,
      "learning_rate": 4.000611358493321e-05,
      "loss": 0.6669,
      "step": 1314300
    },
    {
      "epoch": 11.993576173443318,
      "grad_norm": 3.556877374649048,
      "learning_rate": 4.000535318879724e-05,
      "loss": 0.664,
      "step": 1314400
    },
    {
      "epoch": 11.994488648806483,
      "grad_norm": 5.306312084197998,
      "learning_rate": 4.000459279266126e-05,
      "loss": 0.6927,
      "step": 1314500
    },
    {
      "epoch": 11.995401124169648,
      "grad_norm": 2.3850629329681396,
      "learning_rate": 4.00038323965253e-05,
      "loss": 0.6863,
      "step": 1314600
    },
    {
      "epoch": 11.996313599532813,
      "grad_norm": 3.745579719543457,
      "learning_rate": 4.0003072000389323e-05,
      "loss": 0.6782,
      "step": 1314700
    },
    {
      "epoch": 11.997226074895977,
      "grad_norm": 3.9138808250427246,
      "learning_rate": 4.0002311604253353e-05,
      "loss": 0.6686,
      "step": 1314800
    },
    {
      "epoch": 11.998138550259142,
      "grad_norm": 4.539620876312256,
      "learning_rate": 4.0001551208117384e-05,
      "loss": 0.6965,
      "step": 1314900
    },
    {
      "epoch": 11.999051025622308,
      "grad_norm": 5.062474727630615,
      "learning_rate": 4.000079081198141e-05,
      "loss": 0.6748,
      "step": 1315000
    },
    {
      "epoch": 11.999963500985473,
      "grad_norm": 3.101778268814087,
      "learning_rate": 4.0000030415845444e-05,
      "loss": 0.705,
      "step": 1315100
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.5541903376579285,
      "eval_runtime": 25.2962,
      "eval_samples_per_second": 228.058,
      "eval_steps_per_second": 228.058,
      "step": 1315104
    },
    {
      "epoch": 12.0,
      "eval_loss": 0.5336611866950989,
      "eval_runtime": 496.1137,
      "eval_samples_per_second": 220.901,
      "eval_steps_per_second": 220.901,
      "step": 1315104
    },
    {
      "epoch": 12.000875976348638,
      "grad_norm": 3.6853394508361816,
      "learning_rate": 3.999927001970947e-05,
      "loss": 0.6772,
      "step": 1315200
    },
    {
      "epoch": 12.001788451711803,
      "grad_norm": 3.6626737117767334,
      "learning_rate": 3.99985096235735e-05,
      "loss": 0.6732,
      "step": 1315300
    },
    {
      "epoch": 12.002700927074969,
      "grad_norm": 4.439030647277832,
      "learning_rate": 3.999774922743753e-05,
      "loss": 0.6708,
      "step": 1315400
    },
    {
      "epoch": 12.003613402438134,
      "grad_norm": 4.333011150360107,
      "learning_rate": 3.999698883130156e-05,
      "loss": 0.6812,
      "step": 1315500
    },
    {
      "epoch": 12.0045258778013,
      "grad_norm": 4.367044925689697,
      "learning_rate": 3.999622843516558e-05,
      "loss": 0.6606,
      "step": 1315600
    },
    {
      "epoch": 12.005438353164465,
      "grad_norm": 4.206172943115234,
      "learning_rate": 3.999546803902962e-05,
      "loss": 0.6424,
      "step": 1315700
    },
    {
      "epoch": 12.00635082852763,
      "grad_norm": 4.661713123321533,
      "learning_rate": 3.999470764289364e-05,
      "loss": 0.673,
      "step": 1315800
    },
    {
      "epoch": 12.007263303890795,
      "grad_norm": 3.515852689743042,
      "learning_rate": 3.999394724675767e-05,
      "loss": 0.643,
      "step": 1315900
    },
    {
      "epoch": 12.00817577925396,
      "grad_norm": 3.994349956512451,
      "learning_rate": 3.99931868506217e-05,
      "loss": 0.64,
      "step": 1316000
    },
    {
      "epoch": 12.009088254617126,
      "grad_norm": 3.7107505798339844,
      "learning_rate": 3.999242645448573e-05,
      "loss": 0.6347,
      "step": 1316100
    },
    {
      "epoch": 12.010000729980291,
      "grad_norm": 3.936389446258545,
      "learning_rate": 3.999166605834976e-05,
      "loss": 0.6675,
      "step": 1316200
    },
    {
      "epoch": 12.010913205343456,
      "grad_norm": 3.704314947128296,
      "learning_rate": 3.999090566221379e-05,
      "loss": 0.6645,
      "step": 1316300
    },
    {
      "epoch": 12.011825680706622,
      "grad_norm": 4.220206260681152,
      "learning_rate": 3.9990145266077814e-05,
      "loss": 0.6872,
      "step": 1316400
    },
    {
      "epoch": 12.012738156069785,
      "grad_norm": 3.4786858558654785,
      "learning_rate": 3.998938486994185e-05,
      "loss": 0.6553,
      "step": 1316500
    },
    {
      "epoch": 12.01365063143295,
      "grad_norm": 3.608783721923828,
      "learning_rate": 3.9988624473805874e-05,
      "loss": 0.6474,
      "step": 1316600
    },
    {
      "epoch": 12.014563106796116,
      "grad_norm": 4.2558817863464355,
      "learning_rate": 3.9987864077669904e-05,
      "loss": 0.6826,
      "step": 1316700
    },
    {
      "epoch": 12.015475582159281,
      "grad_norm": 4.225661754608154,
      "learning_rate": 3.9987103681533935e-05,
      "loss": 0.6649,
      "step": 1316800
    },
    {
      "epoch": 12.016388057522446,
      "grad_norm": 3.7297654151916504,
      "learning_rate": 3.9986343285397965e-05,
      "loss": 0.6723,
      "step": 1316900
    },
    {
      "epoch": 12.017300532885612,
      "grad_norm": 4.701130390167236,
      "learning_rate": 3.998558288926199e-05,
      "loss": 0.6414,
      "step": 1317000
    },
    {
      "epoch": 12.018213008248777,
      "grad_norm": 3.036938190460205,
      "learning_rate": 3.9984822493126025e-05,
      "loss": 0.6734,
      "step": 1317100
    },
    {
      "epoch": 12.019125483611942,
      "grad_norm": 4.047404766082764,
      "learning_rate": 3.998406209699005e-05,
      "loss": 0.6583,
      "step": 1317200
    },
    {
      "epoch": 12.020037958975108,
      "grad_norm": 4.179483890533447,
      "learning_rate": 3.998330170085408e-05,
      "loss": 0.6518,
      "step": 1317300
    },
    {
      "epoch": 12.020950434338273,
      "grad_norm": 4.092060565948486,
      "learning_rate": 3.998254130471811e-05,
      "loss": 0.6678,
      "step": 1317400
    },
    {
      "epoch": 12.021862909701438,
      "grad_norm": 4.826314449310303,
      "learning_rate": 3.998178090858214e-05,
      "loss": 0.6703,
      "step": 1317500
    },
    {
      "epoch": 12.022775385064604,
      "grad_norm": 3.8784892559051514,
      "learning_rate": 3.998102051244617e-05,
      "loss": 0.7026,
      "step": 1317600
    },
    {
      "epoch": 12.023687860427769,
      "grad_norm": 3.9848861694335938,
      "learning_rate": 3.99802601163102e-05,
      "loss": 0.676,
      "step": 1317700
    },
    {
      "epoch": 12.024600335790934,
      "grad_norm": 3.1342291831970215,
      "learning_rate": 3.997949972017422e-05,
      "loss": 0.6694,
      "step": 1317800
    },
    {
      "epoch": 12.0255128111541,
      "grad_norm": 4.062370300292969,
      "learning_rate": 3.997873932403825e-05,
      "loss": 0.6634,
      "step": 1317900
    },
    {
      "epoch": 12.026425286517265,
      "grad_norm": 4.447900295257568,
      "learning_rate": 3.997797892790228e-05,
      "loss": 0.6948,
      "step": 1318000
    },
    {
      "epoch": 12.02733776188043,
      "grad_norm": 4.819718837738037,
      "learning_rate": 3.9977218531766305e-05,
      "loss": 0.6891,
      "step": 1318100
    },
    {
      "epoch": 12.028250237243594,
      "grad_norm": 3.757293701171875,
      "learning_rate": 3.997645813563034e-05,
      "loss": 0.6395,
      "step": 1318200
    },
    {
      "epoch": 12.029162712606759,
      "grad_norm": 3.798964023590088,
      "learning_rate": 3.9975697739494365e-05,
      "loss": 0.6992,
      "step": 1318300
    },
    {
      "epoch": 12.030075187969924,
      "grad_norm": 3.841251850128174,
      "learning_rate": 3.9974937343358395e-05,
      "loss": 0.7041,
      "step": 1318400
    },
    {
      "epoch": 12.03098766333309,
      "grad_norm": 3.814150333404541,
      "learning_rate": 3.9974176947222425e-05,
      "loss": 0.7039,
      "step": 1318500
    },
    {
      "epoch": 12.031900138696255,
      "grad_norm": 4.1160173416137695,
      "learning_rate": 3.9973416551086455e-05,
      "loss": 0.6859,
      "step": 1318600
    },
    {
      "epoch": 12.03281261405942,
      "grad_norm": 4.298772811889648,
      "learning_rate": 3.9972656154950485e-05,
      "loss": 0.6717,
      "step": 1318700
    },
    {
      "epoch": 12.033725089422585,
      "grad_norm": 4.049582004547119,
      "learning_rate": 3.9971895758814516e-05,
      "loss": 0.6745,
      "step": 1318800
    },
    {
      "epoch": 12.03463756478575,
      "grad_norm": 4.186282634735107,
      "learning_rate": 3.997113536267854e-05,
      "loss": 0.6639,
      "step": 1318900
    },
    {
      "epoch": 12.035550040148916,
      "grad_norm": 4.1878180503845215,
      "learning_rate": 3.9970374966542576e-05,
      "loss": 0.6461,
      "step": 1319000
    },
    {
      "epoch": 12.036462515512081,
      "grad_norm": 5.76495361328125,
      "learning_rate": 3.99696145704066e-05,
      "loss": 0.687,
      "step": 1319100
    },
    {
      "epoch": 12.037374990875247,
      "grad_norm": 3.867387294769287,
      "learning_rate": 3.996885417427063e-05,
      "loss": 0.7096,
      "step": 1319200
    },
    {
      "epoch": 12.038287466238412,
      "grad_norm": 3.8325741291046143,
      "learning_rate": 3.996809377813466e-05,
      "loss": 0.6879,
      "step": 1319300
    },
    {
      "epoch": 12.039199941601577,
      "grad_norm": 4.1410603523254395,
      "learning_rate": 3.996733338199869e-05,
      "loss": 0.6832,
      "step": 1319400
    },
    {
      "epoch": 12.040112416964742,
      "grad_norm": 4.667384624481201,
      "learning_rate": 3.996657298586271e-05,
      "loss": 0.6861,
      "step": 1319500
    },
    {
      "epoch": 12.041024892327908,
      "grad_norm": 3.0292415618896484,
      "learning_rate": 3.996581258972675e-05,
      "loss": 0.6745,
      "step": 1319600
    },
    {
      "epoch": 12.041937367691073,
      "grad_norm": 3.7185285091400146,
      "learning_rate": 3.996505219359077e-05,
      "loss": 0.6436,
      "step": 1319700
    },
    {
      "epoch": 12.042849843054238,
      "grad_norm": 5.606848239898682,
      "learning_rate": 3.99642917974548e-05,
      "loss": 0.6954,
      "step": 1319800
    },
    {
      "epoch": 12.043762318417402,
      "grad_norm": 3.853959083557129,
      "learning_rate": 3.996353140131883e-05,
      "loss": 0.6632,
      "step": 1319900
    },
    {
      "epoch": 12.044674793780567,
      "grad_norm": 2.536792755126953,
      "learning_rate": 3.996277100518286e-05,
      "loss": 0.6591,
      "step": 1320000
    },
    {
      "epoch": 12.045587269143732,
      "grad_norm": 3.9080958366394043,
      "learning_rate": 3.996201060904689e-05,
      "loss": 0.7098,
      "step": 1320100
    },
    {
      "epoch": 12.046499744506898,
      "grad_norm": 3.654540538787842,
      "learning_rate": 3.996125021291092e-05,
      "loss": 0.7122,
      "step": 1320200
    },
    {
      "epoch": 12.047412219870063,
      "grad_norm": 4.048666954040527,
      "learning_rate": 3.9960489816774946e-05,
      "loss": 0.6869,
      "step": 1320300
    },
    {
      "epoch": 12.048324695233228,
      "grad_norm": 4.024424076080322,
      "learning_rate": 3.995972942063898e-05,
      "loss": 0.6678,
      "step": 1320400
    },
    {
      "epoch": 12.049237170596394,
      "grad_norm": 4.184405326843262,
      "learning_rate": 3.9958969024503006e-05,
      "loss": 0.6566,
      "step": 1320500
    },
    {
      "epoch": 12.050149645959559,
      "grad_norm": 4.2480363845825195,
      "learning_rate": 3.9958208628367036e-05,
      "loss": 0.6662,
      "step": 1320600
    },
    {
      "epoch": 12.051062121322724,
      "grad_norm": 4.609692096710205,
      "learning_rate": 3.9957448232231066e-05,
      "loss": 0.691,
      "step": 1320700
    },
    {
      "epoch": 12.05197459668589,
      "grad_norm": 4.546438694000244,
      "learning_rate": 3.995668783609509e-05,
      "loss": 0.6882,
      "step": 1320800
    },
    {
      "epoch": 12.052887072049055,
      "grad_norm": 3.9499120712280273,
      "learning_rate": 3.9955927439959127e-05,
      "loss": 0.647,
      "step": 1320900
    },
    {
      "epoch": 12.05379954741222,
      "grad_norm": 3.0228145122528076,
      "learning_rate": 3.995516704382315e-05,
      "loss": 0.6872,
      "step": 1321000
    },
    {
      "epoch": 12.054712022775385,
      "grad_norm": 3.3558406829833984,
      "learning_rate": 3.995440664768718e-05,
      "loss": 0.6727,
      "step": 1321100
    },
    {
      "epoch": 12.05562449813855,
      "grad_norm": 3.7415289878845215,
      "learning_rate": 3.995364625155121e-05,
      "loss": 0.6471,
      "step": 1321200
    },
    {
      "epoch": 12.056536973501716,
      "grad_norm": 4.051938533782959,
      "learning_rate": 3.995288585541524e-05,
      "loss": 0.7201,
      "step": 1321300
    },
    {
      "epoch": 12.057449448864881,
      "grad_norm": 4.53470516204834,
      "learning_rate": 3.9952125459279263e-05,
      "loss": 0.6662,
      "step": 1321400
    },
    {
      "epoch": 12.058361924228047,
      "grad_norm": 4.16403341293335,
      "learning_rate": 3.99513650631433e-05,
      "loss": 0.6546,
      "step": 1321500
    },
    {
      "epoch": 12.05927439959121,
      "grad_norm": 4.304532051086426,
      "learning_rate": 3.9950604667007324e-05,
      "loss": 0.665,
      "step": 1321600
    },
    {
      "epoch": 12.060186874954375,
      "grad_norm": 3.984628677368164,
      "learning_rate": 3.9949844270871354e-05,
      "loss": 0.6588,
      "step": 1321700
    },
    {
      "epoch": 12.06109935031754,
      "grad_norm": 4.16419792175293,
      "learning_rate": 3.9949083874735384e-05,
      "loss": 0.6723,
      "step": 1321800
    },
    {
      "epoch": 12.062011825680706,
      "grad_norm": 4.122877597808838,
      "learning_rate": 3.9948323478599414e-05,
      "loss": 0.6634,
      "step": 1321900
    },
    {
      "epoch": 12.062924301043871,
      "grad_norm": 3.4316301345825195,
      "learning_rate": 3.9947563082463444e-05,
      "loss": 0.6883,
      "step": 1322000
    },
    {
      "epoch": 12.063836776407037,
      "grad_norm": 4.226222515106201,
      "learning_rate": 3.9946802686327474e-05,
      "loss": 0.7219,
      "step": 1322100
    },
    {
      "epoch": 12.064749251770202,
      "grad_norm": 4.528692722320557,
      "learning_rate": 3.99460422901915e-05,
      "loss": 0.6993,
      "step": 1322200
    },
    {
      "epoch": 12.065661727133367,
      "grad_norm": 5.300359725952148,
      "learning_rate": 3.9945281894055534e-05,
      "loss": 0.6605,
      "step": 1322300
    },
    {
      "epoch": 12.066574202496533,
      "grad_norm": 5.0474958419799805,
      "learning_rate": 3.994452149791956e-05,
      "loss": 0.6932,
      "step": 1322400
    },
    {
      "epoch": 12.067486677859698,
      "grad_norm": 4.353549957275391,
      "learning_rate": 3.994376110178359e-05,
      "loss": 0.6851,
      "step": 1322500
    },
    {
      "epoch": 12.068399153222863,
      "grad_norm": 4.774267196655273,
      "learning_rate": 3.994300070564762e-05,
      "loss": 0.6528,
      "step": 1322600
    },
    {
      "epoch": 12.069311628586028,
      "grad_norm": 4.12001371383667,
      "learning_rate": 3.994224030951165e-05,
      "loss": 0.6932,
      "step": 1322700
    },
    {
      "epoch": 12.070224103949194,
      "grad_norm": 3.768381118774414,
      "learning_rate": 3.994147991337567e-05,
      "loss": 0.6972,
      "step": 1322800
    },
    {
      "epoch": 12.071136579312359,
      "grad_norm": 4.579824924468994,
      "learning_rate": 3.994071951723971e-05,
      "loss": 0.7233,
      "step": 1322900
    },
    {
      "epoch": 12.072049054675524,
      "grad_norm": 4.371584415435791,
      "learning_rate": 3.993995912110373e-05,
      "loss": 0.6188,
      "step": 1323000
    },
    {
      "epoch": 12.07296153003869,
      "grad_norm": 4.336960792541504,
      "learning_rate": 3.993919872496776e-05,
      "loss": 0.7066,
      "step": 1323100
    },
    {
      "epoch": 12.073874005401855,
      "grad_norm": 4.379324913024902,
      "learning_rate": 3.993843832883179e-05,
      "loss": 0.6925,
      "step": 1323200
    },
    {
      "epoch": 12.074786480765018,
      "grad_norm": 3.9312591552734375,
      "learning_rate": 3.993767793269582e-05,
      "loss": 0.6479,
      "step": 1323300
    },
    {
      "epoch": 12.075698956128184,
      "grad_norm": 3.969789981842041,
      "learning_rate": 3.993691753655985e-05,
      "loss": 0.7092,
      "step": 1323400
    },
    {
      "epoch": 12.076611431491349,
      "grad_norm": 4.053459644317627,
      "learning_rate": 3.993615714042388e-05,
      "loss": 0.6668,
      "step": 1323500
    },
    {
      "epoch": 12.077523906854514,
      "grad_norm": 3.734127998352051,
      "learning_rate": 3.9935396744287905e-05,
      "loss": 0.6856,
      "step": 1323600
    },
    {
      "epoch": 12.07843638221768,
      "grad_norm": 4.261792182922363,
      "learning_rate": 3.9934636348151935e-05,
      "loss": 0.6607,
      "step": 1323700
    },
    {
      "epoch": 12.079348857580845,
      "grad_norm": 3.4452128410339355,
      "learning_rate": 3.9933875952015965e-05,
      "loss": 0.6957,
      "step": 1323800
    },
    {
      "epoch": 12.08026133294401,
      "grad_norm": 5.088321685791016,
      "learning_rate": 3.993311555587999e-05,
      "loss": 0.6603,
      "step": 1323900
    },
    {
      "epoch": 12.081173808307176,
      "grad_norm": 4.595058441162109,
      "learning_rate": 3.9932355159744025e-05,
      "loss": 0.656,
      "step": 1324000
    },
    {
      "epoch": 12.08208628367034,
      "grad_norm": 3.2488200664520264,
      "learning_rate": 3.993159476360805e-05,
      "loss": 0.6552,
      "step": 1324100
    },
    {
      "epoch": 12.082998759033506,
      "grad_norm": 4.145401477813721,
      "learning_rate": 3.993083436747208e-05,
      "loss": 0.6586,
      "step": 1324200
    },
    {
      "epoch": 12.083911234396671,
      "grad_norm": 4.8376054763793945,
      "learning_rate": 3.993007397133611e-05,
      "loss": 0.655,
      "step": 1324300
    },
    {
      "epoch": 12.084823709759837,
      "grad_norm": 3.4340243339538574,
      "learning_rate": 3.992931357520014e-05,
      "loss": 0.6593,
      "step": 1324400
    },
    {
      "epoch": 12.085736185123002,
      "grad_norm": 4.754807472229004,
      "learning_rate": 3.992855317906417e-05,
      "loss": 0.6981,
      "step": 1324500
    },
    {
      "epoch": 12.086648660486167,
      "grad_norm": 4.022479057312012,
      "learning_rate": 3.99277927829282e-05,
      "loss": 0.71,
      "step": 1324600
    },
    {
      "epoch": 12.087561135849333,
      "grad_norm": 2.96118426322937,
      "learning_rate": 3.992703238679222e-05,
      "loss": 0.638,
      "step": 1324700
    },
    {
      "epoch": 12.088473611212498,
      "grad_norm": 3.8951711654663086,
      "learning_rate": 3.992627199065626e-05,
      "loss": 0.6564,
      "step": 1324800
    },
    {
      "epoch": 12.089386086575663,
      "grad_norm": 4.641416549682617,
      "learning_rate": 3.992551159452028e-05,
      "loss": 0.6574,
      "step": 1324900
    },
    {
      "epoch": 12.090298561938827,
      "grad_norm": 3.3109729290008545,
      "learning_rate": 3.992475119838431e-05,
      "loss": 0.6436,
      "step": 1325000
    },
    {
      "epoch": 12.091211037301992,
      "grad_norm": 3.900649070739746,
      "learning_rate": 3.992399080224834e-05,
      "loss": 0.647,
      "step": 1325100
    },
    {
      "epoch": 12.092123512665157,
      "grad_norm": 4.377061367034912,
      "learning_rate": 3.992323040611237e-05,
      "loss": 0.6811,
      "step": 1325200
    },
    {
      "epoch": 12.093035988028323,
      "grad_norm": 3.4100399017333984,
      "learning_rate": 3.9922470009976395e-05,
      "loss": 0.6505,
      "step": 1325300
    },
    {
      "epoch": 12.093948463391488,
      "grad_norm": 4.318075180053711,
      "learning_rate": 3.992170961384043e-05,
      "loss": 0.6447,
      "step": 1325400
    },
    {
      "epoch": 12.094860938754653,
      "grad_norm": 3.8415894508361816,
      "learning_rate": 3.9920949217704455e-05,
      "loss": 0.663,
      "step": 1325500
    },
    {
      "epoch": 12.095773414117819,
      "grad_norm": 3.528536319732666,
      "learning_rate": 3.9920188821568486e-05,
      "loss": 0.662,
      "step": 1325600
    },
    {
      "epoch": 12.096685889480984,
      "grad_norm": 3.853543281555176,
      "learning_rate": 3.9919428425432516e-05,
      "loss": 0.6819,
      "step": 1325700
    },
    {
      "epoch": 12.09759836484415,
      "grad_norm": 4.092765808105469,
      "learning_rate": 3.9918668029296546e-05,
      "loss": 0.6791,
      "step": 1325800
    },
    {
      "epoch": 12.098510840207314,
      "grad_norm": 3.923780679702759,
      "learning_rate": 3.9917907633160576e-05,
      "loss": 0.6834,
      "step": 1325900
    },
    {
      "epoch": 12.09942331557048,
      "grad_norm": 3.952019453048706,
      "learning_rate": 3.9917147237024606e-05,
      "loss": 0.6552,
      "step": 1326000
    },
    {
      "epoch": 12.100335790933645,
      "grad_norm": 3.6406779289245605,
      "learning_rate": 3.991638684088863e-05,
      "loss": 0.6844,
      "step": 1326100
    },
    {
      "epoch": 12.10124826629681,
      "grad_norm": 4.6815290451049805,
      "learning_rate": 3.9915626444752666e-05,
      "loss": 0.6817,
      "step": 1326200
    },
    {
      "epoch": 12.102160741659976,
      "grad_norm": 3.7813093662261963,
      "learning_rate": 3.991486604861669e-05,
      "loss": 0.6837,
      "step": 1326300
    },
    {
      "epoch": 12.103073217023141,
      "grad_norm": 4.228128910064697,
      "learning_rate": 3.991410565248071e-05,
      "loss": 0.6758,
      "step": 1326400
    },
    {
      "epoch": 12.103985692386306,
      "grad_norm": 3.6836583614349365,
      "learning_rate": 3.991334525634475e-05,
      "loss": 0.6839,
      "step": 1326500
    },
    {
      "epoch": 12.104898167749472,
      "grad_norm": 3.5084352493286133,
      "learning_rate": 3.991258486020877e-05,
      "loss": 0.6845,
      "step": 1326600
    },
    {
      "epoch": 12.105810643112635,
      "grad_norm": 3.869063138961792,
      "learning_rate": 3.99118244640728e-05,
      "loss": 0.6912,
      "step": 1326700
    },
    {
      "epoch": 12.1067231184758,
      "grad_norm": 3.7862281799316406,
      "learning_rate": 3.991106406793683e-05,
      "loss": 0.6846,
      "step": 1326800
    },
    {
      "epoch": 12.107635593838966,
      "grad_norm": 4.296175479888916,
      "learning_rate": 3.991030367180086e-05,
      "loss": 0.6939,
      "step": 1326900
    },
    {
      "epoch": 12.108548069202131,
      "grad_norm": 7.281255722045898,
      "learning_rate": 3.990954327566489e-05,
      "loss": 0.6627,
      "step": 1327000
    },
    {
      "epoch": 12.109460544565296,
      "grad_norm": 4.053110599517822,
      "learning_rate": 3.990878287952892e-05,
      "loss": 0.6618,
      "step": 1327100
    },
    {
      "epoch": 12.110373019928462,
      "grad_norm": 3.966611623764038,
      "learning_rate": 3.9908022483392946e-05,
      "loss": 0.6289,
      "step": 1327200
    },
    {
      "epoch": 12.111285495291627,
      "grad_norm": 3.759389877319336,
      "learning_rate": 3.990726208725698e-05,
      "loss": 0.6663,
      "step": 1327300
    },
    {
      "epoch": 12.112197970654792,
      "grad_norm": 4.224648475646973,
      "learning_rate": 3.9906501691121006e-05,
      "loss": 0.6998,
      "step": 1327400
    },
    {
      "epoch": 12.113110446017957,
      "grad_norm": 3.9846746921539307,
      "learning_rate": 3.9905741294985037e-05,
      "loss": 0.6617,
      "step": 1327500
    },
    {
      "epoch": 12.114022921381123,
      "grad_norm": 4.6822614669799805,
      "learning_rate": 3.9904980898849067e-05,
      "loss": 0.6667,
      "step": 1327600
    },
    {
      "epoch": 12.114935396744288,
      "grad_norm": 4.076799392700195,
      "learning_rate": 3.99042205027131e-05,
      "loss": 0.6578,
      "step": 1327700
    },
    {
      "epoch": 12.115847872107453,
      "grad_norm": 4.454769611358643,
      "learning_rate": 3.990346010657712e-05,
      "loss": 0.6749,
      "step": 1327800
    },
    {
      "epoch": 12.116760347470619,
      "grad_norm": 3.6287591457366943,
      "learning_rate": 3.990269971044116e-05,
      "loss": 0.7004,
      "step": 1327900
    },
    {
      "epoch": 12.117672822833784,
      "grad_norm": 4.458651065826416,
      "learning_rate": 3.990193931430518e-05,
      "loss": 0.6963,
      "step": 1328000
    },
    {
      "epoch": 12.11858529819695,
      "grad_norm": 3.8448848724365234,
      "learning_rate": 3.990117891816921e-05,
      "loss": 0.6455,
      "step": 1328100
    },
    {
      "epoch": 12.119497773560115,
      "grad_norm": 3.4640591144561768,
      "learning_rate": 3.990041852203324e-05,
      "loss": 0.6783,
      "step": 1328200
    },
    {
      "epoch": 12.12041024892328,
      "grad_norm": 3.7264301776885986,
      "learning_rate": 3.989965812589727e-05,
      "loss": 0.6651,
      "step": 1328300
    },
    {
      "epoch": 12.121322724286443,
      "grad_norm": 3.736609697341919,
      "learning_rate": 3.98988977297613e-05,
      "loss": 0.6802,
      "step": 1328400
    },
    {
      "epoch": 12.122235199649609,
      "grad_norm": 4.801330089569092,
      "learning_rate": 3.989813733362533e-05,
      "loss": 0.6858,
      "step": 1328500
    },
    {
      "epoch": 12.123147675012774,
      "grad_norm": 4.029082775115967,
      "learning_rate": 3.9897376937489354e-05,
      "loss": 0.6424,
      "step": 1328600
    },
    {
      "epoch": 12.12406015037594,
      "grad_norm": 3.6323599815368652,
      "learning_rate": 3.989661654135339e-05,
      "loss": 0.6484,
      "step": 1328700
    },
    {
      "epoch": 12.124972625739105,
      "grad_norm": 4.119170665740967,
      "learning_rate": 3.9895856145217414e-05,
      "loss": 0.6737,
      "step": 1328800
    },
    {
      "epoch": 12.12588510110227,
      "grad_norm": 4.104302406311035,
      "learning_rate": 3.9895095749081444e-05,
      "loss": 0.6437,
      "step": 1328900
    },
    {
      "epoch": 12.126797576465435,
      "grad_norm": 2.890393018722534,
      "learning_rate": 3.9894335352945474e-05,
      "loss": 0.677,
      "step": 1329000
    },
    {
      "epoch": 12.1277100518286,
      "grad_norm": 4.758382320404053,
      "learning_rate": 3.9893574956809504e-05,
      "loss": 0.662,
      "step": 1329100
    },
    {
      "epoch": 12.128622527191766,
      "grad_norm": 4.526073932647705,
      "learning_rate": 3.989281456067353e-05,
      "loss": 0.7267,
      "step": 1329200
    },
    {
      "epoch": 12.129535002554931,
      "grad_norm": 4.519822597503662,
      "learning_rate": 3.989205416453756e-05,
      "loss": 0.6807,
      "step": 1329300
    },
    {
      "epoch": 12.130447477918096,
      "grad_norm": 5.250434875488281,
      "learning_rate": 3.989129376840159e-05,
      "loss": 0.6603,
      "step": 1329400
    },
    {
      "epoch": 12.131359953281262,
      "grad_norm": 3.60483455657959,
      "learning_rate": 3.989053337226562e-05,
      "loss": 0.6869,
      "step": 1329500
    },
    {
      "epoch": 12.132272428644427,
      "grad_norm": 4.5269694328308105,
      "learning_rate": 3.988977297612965e-05,
      "loss": 0.6702,
      "step": 1329600
    },
    {
      "epoch": 12.133184904007592,
      "grad_norm": 2.960076093673706,
      "learning_rate": 3.988901257999367e-05,
      "loss": 0.672,
      "step": 1329700
    },
    {
      "epoch": 12.134097379370758,
      "grad_norm": 4.833197116851807,
      "learning_rate": 3.988825218385771e-05,
      "loss": 0.6941,
      "step": 1329800
    },
    {
      "epoch": 12.135009854733923,
      "grad_norm": 4.237429618835449,
      "learning_rate": 3.988749178772173e-05,
      "loss": 0.6959,
      "step": 1329900
    },
    {
      "epoch": 12.135922330097088,
      "grad_norm": 4.154138565063477,
      "learning_rate": 3.988673139158576e-05,
      "loss": 0.6478,
      "step": 1330000
    },
    {
      "epoch": 12.136834805460252,
      "grad_norm": 4.781589984893799,
      "learning_rate": 3.988597099544979e-05,
      "loss": 0.6694,
      "step": 1330100
    },
    {
      "epoch": 12.137747280823417,
      "grad_norm": 4.133075714111328,
      "learning_rate": 3.988521059931382e-05,
      "loss": 0.6768,
      "step": 1330200
    },
    {
      "epoch": 12.138659756186582,
      "grad_norm": 4.320446968078613,
      "learning_rate": 3.9884450203177845e-05,
      "loss": 0.6769,
      "step": 1330300
    },
    {
      "epoch": 12.139572231549748,
      "grad_norm": 4.581970691680908,
      "learning_rate": 3.988368980704188e-05,
      "loss": 0.6665,
      "step": 1330400
    },
    {
      "epoch": 12.140484706912913,
      "grad_norm": 3.892615556716919,
      "learning_rate": 3.9882929410905905e-05,
      "loss": 0.6449,
      "step": 1330500
    },
    {
      "epoch": 12.141397182276078,
      "grad_norm": 4.653928756713867,
      "learning_rate": 3.9882169014769935e-05,
      "loss": 0.6701,
      "step": 1330600
    },
    {
      "epoch": 12.142309657639244,
      "grad_norm": 4.300570487976074,
      "learning_rate": 3.9881408618633965e-05,
      "loss": 0.6344,
      "step": 1330700
    },
    {
      "epoch": 12.143222133002409,
      "grad_norm": 4.296442031860352,
      "learning_rate": 3.9880648222497995e-05,
      "loss": 0.678,
      "step": 1330800
    },
    {
      "epoch": 12.144134608365574,
      "grad_norm": 4.302161693572998,
      "learning_rate": 3.9879887826362025e-05,
      "loss": 0.669,
      "step": 1330900
    },
    {
      "epoch": 12.14504708372874,
      "grad_norm": 4.51198148727417,
      "learning_rate": 3.9879127430226055e-05,
      "loss": 0.6943,
      "step": 1331000
    },
    {
      "epoch": 12.145959559091905,
      "grad_norm": 3.872480869293213,
      "learning_rate": 3.987836703409008e-05,
      "loss": 0.6858,
      "step": 1331100
    },
    {
      "epoch": 12.14687203445507,
      "grad_norm": 4.1415605545043945,
      "learning_rate": 3.9877606637954115e-05,
      "loss": 0.7023,
      "step": 1331200
    },
    {
      "epoch": 12.147784509818235,
      "grad_norm": 4.487285137176514,
      "learning_rate": 3.987684624181814e-05,
      "loss": 0.7161,
      "step": 1331300
    },
    {
      "epoch": 12.1486969851814,
      "grad_norm": 4.243028163909912,
      "learning_rate": 3.987608584568217e-05,
      "loss": 0.6978,
      "step": 1331400
    },
    {
      "epoch": 12.149609460544566,
      "grad_norm": 4.762969970703125,
      "learning_rate": 3.98753254495462e-05,
      "loss": 0.662,
      "step": 1331500
    },
    {
      "epoch": 12.150521935907731,
      "grad_norm": 4.5979533195495605,
      "learning_rate": 3.987456505341023e-05,
      "loss": 0.6766,
      "step": 1331600
    },
    {
      "epoch": 12.151434411270897,
      "grad_norm": 3.5210330486297607,
      "learning_rate": 3.987380465727425e-05,
      "loss": 0.6476,
      "step": 1331700
    },
    {
      "epoch": 12.15234688663406,
      "grad_norm": 3.8740947246551514,
      "learning_rate": 3.987304426113829e-05,
      "loss": 0.6585,
      "step": 1331800
    },
    {
      "epoch": 12.153259361997225,
      "grad_norm": 4.7529730796813965,
      "learning_rate": 3.987228386500231e-05,
      "loss": 0.6945,
      "step": 1331900
    },
    {
      "epoch": 12.15417183736039,
      "grad_norm": 4.328099727630615,
      "learning_rate": 3.987152346886634e-05,
      "loss": 0.7256,
      "step": 1332000
    },
    {
      "epoch": 12.155084312723556,
      "grad_norm": 4.287292003631592,
      "learning_rate": 3.987076307273037e-05,
      "loss": 0.7087,
      "step": 1332100
    },
    {
      "epoch": 12.155996788086721,
      "grad_norm": 3.8967604637145996,
      "learning_rate": 3.9870002676594395e-05,
      "loss": 0.6504,
      "step": 1332200
    },
    {
      "epoch": 12.156909263449887,
      "grad_norm": 4.439307689666748,
      "learning_rate": 3.986924228045843e-05,
      "loss": 0.6877,
      "step": 1332300
    },
    {
      "epoch": 12.157821738813052,
      "grad_norm": 4.158921718597412,
      "learning_rate": 3.9868481884322456e-05,
      "loss": 0.6493,
      "step": 1332400
    },
    {
      "epoch": 12.158734214176217,
      "grad_norm": 3.5977535247802734,
      "learning_rate": 3.9867721488186486e-05,
      "loss": 0.6939,
      "step": 1332500
    },
    {
      "epoch": 12.159646689539382,
      "grad_norm": 3.791868209838867,
      "learning_rate": 3.9866961092050516e-05,
      "loss": 0.6721,
      "step": 1332600
    },
    {
      "epoch": 12.160559164902548,
      "grad_norm": 4.043662071228027,
      "learning_rate": 3.9866200695914546e-05,
      "loss": 0.6684,
      "step": 1332700
    },
    {
      "epoch": 12.161471640265713,
      "grad_norm": 3.7218315601348877,
      "learning_rate": 3.9865440299778576e-05,
      "loss": 0.6478,
      "step": 1332800
    },
    {
      "epoch": 12.162384115628878,
      "grad_norm": 4.190491199493408,
      "learning_rate": 3.9864679903642606e-05,
      "loss": 0.6965,
      "step": 1332900
    },
    {
      "epoch": 12.163296590992044,
      "grad_norm": 3.7658755779266357,
      "learning_rate": 3.986391950750663e-05,
      "loss": 0.6483,
      "step": 1333000
    },
    {
      "epoch": 12.164209066355209,
      "grad_norm": 3.734827756881714,
      "learning_rate": 3.986315911137066e-05,
      "loss": 0.6555,
      "step": 1333100
    },
    {
      "epoch": 12.165121541718374,
      "grad_norm": 3.5534613132476807,
      "learning_rate": 3.986239871523469e-05,
      "loss": 0.6819,
      "step": 1333200
    },
    {
      "epoch": 12.16603401708154,
      "grad_norm": 3.873342275619507,
      "learning_rate": 3.986163831909872e-05,
      "loss": 0.7174,
      "step": 1333300
    },
    {
      "epoch": 12.166946492444705,
      "grad_norm": 5.004810810089111,
      "learning_rate": 3.986087792296275e-05,
      "loss": 0.6665,
      "step": 1333400
    },
    {
      "epoch": 12.167858967807868,
      "grad_norm": 3.3586578369140625,
      "learning_rate": 3.986011752682678e-05,
      "loss": 0.6448,
      "step": 1333500
    },
    {
      "epoch": 12.168771443171034,
      "grad_norm": 4.677161693572998,
      "learning_rate": 3.98593571306908e-05,
      "loss": 0.6794,
      "step": 1333600
    },
    {
      "epoch": 12.169683918534199,
      "grad_norm": 4.400946617126465,
      "learning_rate": 3.985859673455484e-05,
      "loss": 0.6681,
      "step": 1333700
    },
    {
      "epoch": 12.170596393897364,
      "grad_norm": 4.948021411895752,
      "learning_rate": 3.985783633841886e-05,
      "loss": 0.6515,
      "step": 1333800
    },
    {
      "epoch": 12.17150886926053,
      "grad_norm": 3.3344109058380127,
      "learning_rate": 3.985707594228289e-05,
      "loss": 0.6572,
      "step": 1333900
    },
    {
      "epoch": 12.172421344623695,
      "grad_norm": 3.6721272468566895,
      "learning_rate": 3.985631554614692e-05,
      "loss": 0.7002,
      "step": 1334000
    },
    {
      "epoch": 12.17333381998686,
      "grad_norm": 3.3415474891662598,
      "learning_rate": 3.985555515001095e-05,
      "loss": 0.6687,
      "step": 1334100
    },
    {
      "epoch": 12.174246295350025,
      "grad_norm": 3.730147123336792,
      "learning_rate": 3.985479475387498e-05,
      "loss": 0.632,
      "step": 1334200
    },
    {
      "epoch": 12.17515877071319,
      "grad_norm": 4.226407527923584,
      "learning_rate": 3.985403435773901e-05,
      "loss": 0.6983,
      "step": 1334300
    },
    {
      "epoch": 12.176071246076356,
      "grad_norm": 4.542181491851807,
      "learning_rate": 3.9853273961603037e-05,
      "loss": 0.7051,
      "step": 1334400
    },
    {
      "epoch": 12.176983721439521,
      "grad_norm": 3.8999762535095215,
      "learning_rate": 3.985251356546707e-05,
      "loss": 0.7083,
      "step": 1334500
    },
    {
      "epoch": 12.177896196802687,
      "grad_norm": 4.925268650054932,
      "learning_rate": 3.98517531693311e-05,
      "loss": 0.6857,
      "step": 1334600
    },
    {
      "epoch": 12.178808672165852,
      "grad_norm": 4.335191249847412,
      "learning_rate": 3.985099277319513e-05,
      "loss": 0.6871,
      "step": 1334700
    },
    {
      "epoch": 12.179721147529017,
      "grad_norm": 3.886906385421753,
      "learning_rate": 3.985023237705916e-05,
      "loss": 0.6645,
      "step": 1334800
    },
    {
      "epoch": 12.180633622892183,
      "grad_norm": 4.331469535827637,
      "learning_rate": 3.984947198092318e-05,
      "loss": 0.6392,
      "step": 1334900
    },
    {
      "epoch": 12.181546098255348,
      "grad_norm": 4.68488883972168,
      "learning_rate": 3.984871158478721e-05,
      "loss": 0.6626,
      "step": 1335000
    },
    {
      "epoch": 12.182458573618513,
      "grad_norm": 3.942423105239868,
      "learning_rate": 3.984795118865124e-05,
      "loss": 0.6351,
      "step": 1335100
    },
    {
      "epoch": 12.183371048981677,
      "grad_norm": 4.219382286071777,
      "learning_rate": 3.984719079251527e-05,
      "loss": 0.6516,
      "step": 1335200
    },
    {
      "epoch": 12.184283524344842,
      "grad_norm": 3.549884796142578,
      "learning_rate": 3.98464303963793e-05,
      "loss": 0.7007,
      "step": 1335300
    },
    {
      "epoch": 12.185195999708007,
      "grad_norm": 4.537405490875244,
      "learning_rate": 3.984567000024333e-05,
      "loss": 0.6856,
      "step": 1335400
    },
    {
      "epoch": 12.186108475071173,
      "grad_norm": 3.5076749324798584,
      "learning_rate": 3.9844909604107354e-05,
      "loss": 0.7016,
      "step": 1335500
    },
    {
      "epoch": 12.187020950434338,
      "grad_norm": 2.8257899284362793,
      "learning_rate": 3.984414920797139e-05,
      "loss": 0.6649,
      "step": 1335600
    },
    {
      "epoch": 12.187933425797503,
      "grad_norm": 3.627105474472046,
      "learning_rate": 3.9843388811835414e-05,
      "loss": 0.7028,
      "step": 1335700
    },
    {
      "epoch": 12.188845901160668,
      "grad_norm": 4.278860092163086,
      "learning_rate": 3.9842628415699444e-05,
      "loss": 0.665,
      "step": 1335800
    },
    {
      "epoch": 12.189758376523834,
      "grad_norm": 3.965797185897827,
      "learning_rate": 3.9841868019563474e-05,
      "loss": 0.6674,
      "step": 1335900
    },
    {
      "epoch": 12.190670851886999,
      "grad_norm": 3.913731098175049,
      "learning_rate": 3.9841107623427504e-05,
      "loss": 0.6734,
      "step": 1336000
    },
    {
      "epoch": 12.191583327250164,
      "grad_norm": 3.8180675506591797,
      "learning_rate": 3.984034722729153e-05,
      "loss": 0.6789,
      "step": 1336100
    },
    {
      "epoch": 12.19249580261333,
      "grad_norm": 4.257059097290039,
      "learning_rate": 3.9839586831155564e-05,
      "loss": 0.6444,
      "step": 1336200
    },
    {
      "epoch": 12.193408277976495,
      "grad_norm": 4.88026237487793,
      "learning_rate": 3.983882643501959e-05,
      "loss": 0.69,
      "step": 1336300
    },
    {
      "epoch": 12.19432075333966,
      "grad_norm": 4.8050665855407715,
      "learning_rate": 3.983806603888362e-05,
      "loss": 0.6679,
      "step": 1336400
    },
    {
      "epoch": 12.195233228702826,
      "grad_norm": 3.8075788021087646,
      "learning_rate": 3.983730564274765e-05,
      "loss": 0.6867,
      "step": 1336500
    },
    {
      "epoch": 12.19614570406599,
      "grad_norm": 4.692328453063965,
      "learning_rate": 3.983654524661168e-05,
      "loss": 0.6837,
      "step": 1336600
    },
    {
      "epoch": 12.197058179429156,
      "grad_norm": 4.398111820220947,
      "learning_rate": 3.983578485047571e-05,
      "loss": 0.6812,
      "step": 1336700
    },
    {
      "epoch": 12.197970654792321,
      "grad_norm": 3.422658920288086,
      "learning_rate": 3.983502445433974e-05,
      "loss": 0.6818,
      "step": 1336800
    },
    {
      "epoch": 12.198883130155485,
      "grad_norm": 2.3484015464782715,
      "learning_rate": 3.983426405820376e-05,
      "loss": 0.6499,
      "step": 1336900
    },
    {
      "epoch": 12.19979560551865,
      "grad_norm": 3.7620110511779785,
      "learning_rate": 3.98335036620678e-05,
      "loss": 0.6627,
      "step": 1337000
    },
    {
      "epoch": 12.200708080881816,
      "grad_norm": 3.5498499870300293,
      "learning_rate": 3.983274326593182e-05,
      "loss": 0.6638,
      "step": 1337100
    },
    {
      "epoch": 12.20162055624498,
      "grad_norm": 3.7756521701812744,
      "learning_rate": 3.983198286979585e-05,
      "loss": 0.646,
      "step": 1337200
    },
    {
      "epoch": 12.202533031608146,
      "grad_norm": 3.329464912414551,
      "learning_rate": 3.983122247365988e-05,
      "loss": 0.6419,
      "step": 1337300
    },
    {
      "epoch": 12.203445506971311,
      "grad_norm": 2.8571054935455322,
      "learning_rate": 3.983046207752391e-05,
      "loss": 0.6865,
      "step": 1337400
    },
    {
      "epoch": 12.204357982334477,
      "grad_norm": 4.233925819396973,
      "learning_rate": 3.9829701681387935e-05,
      "loss": 0.6714,
      "step": 1337500
    },
    {
      "epoch": 12.205270457697642,
      "grad_norm": 4.474693775177002,
      "learning_rate": 3.982894128525197e-05,
      "loss": 0.6878,
      "step": 1337600
    },
    {
      "epoch": 12.206182933060807,
      "grad_norm": 4.256560325622559,
      "learning_rate": 3.9828180889115995e-05,
      "loss": 0.7022,
      "step": 1337700
    },
    {
      "epoch": 12.207095408423973,
      "grad_norm": 4.194614410400391,
      "learning_rate": 3.9827420492980025e-05,
      "loss": 0.6676,
      "step": 1337800
    },
    {
      "epoch": 12.208007883787138,
      "grad_norm": 3.9969897270202637,
      "learning_rate": 3.9826660096844055e-05,
      "loss": 0.7032,
      "step": 1337900
    },
    {
      "epoch": 12.208920359150303,
      "grad_norm": 4.437022686004639,
      "learning_rate": 3.982589970070808e-05,
      "loss": 0.7059,
      "step": 1338000
    },
    {
      "epoch": 12.209832834513469,
      "grad_norm": 4.038156509399414,
      "learning_rate": 3.9825139304572115e-05,
      "loss": 0.6561,
      "step": 1338100
    },
    {
      "epoch": 12.210745309876634,
      "grad_norm": 3.633601188659668,
      "learning_rate": 3.982437890843614e-05,
      "loss": 0.6773,
      "step": 1338200
    },
    {
      "epoch": 12.2116577852398,
      "grad_norm": 3.117795944213867,
      "learning_rate": 3.982361851230017e-05,
      "loss": 0.6697,
      "step": 1338300
    },
    {
      "epoch": 12.212570260602964,
      "grad_norm": 3.3619978427886963,
      "learning_rate": 3.98228581161642e-05,
      "loss": 0.6596,
      "step": 1338400
    },
    {
      "epoch": 12.21348273596613,
      "grad_norm": 4.424301624298096,
      "learning_rate": 3.982209772002823e-05,
      "loss": 0.6529,
      "step": 1338500
    },
    {
      "epoch": 12.214395211329293,
      "grad_norm": 4.363234519958496,
      "learning_rate": 3.982133732389225e-05,
      "loss": 0.6641,
      "step": 1338600
    },
    {
      "epoch": 12.215307686692459,
      "grad_norm": 3.7376158237457275,
      "learning_rate": 3.982057692775629e-05,
      "loss": 0.651,
      "step": 1338700
    },
    {
      "epoch": 12.216220162055624,
      "grad_norm": 4.2907938957214355,
      "learning_rate": 3.981981653162031e-05,
      "loss": 0.6378,
      "step": 1338800
    },
    {
      "epoch": 12.21713263741879,
      "grad_norm": 3.5819358825683594,
      "learning_rate": 3.981905613548434e-05,
      "loss": 0.664,
      "step": 1338900
    },
    {
      "epoch": 12.218045112781954,
      "grad_norm": 4.25852108001709,
      "learning_rate": 3.981829573934837e-05,
      "loss": 0.6508,
      "step": 1339000
    },
    {
      "epoch": 12.21895758814512,
      "grad_norm": 4.203636169433594,
      "learning_rate": 3.98175353432124e-05,
      "loss": 0.6835,
      "step": 1339100
    },
    {
      "epoch": 12.219870063508285,
      "grad_norm": 3.9455740451812744,
      "learning_rate": 3.981677494707643e-05,
      "loss": 0.7072,
      "step": 1339200
    },
    {
      "epoch": 12.22078253887145,
      "grad_norm": 3.5132436752319336,
      "learning_rate": 3.981601455094046e-05,
      "loss": 0.6624,
      "step": 1339300
    },
    {
      "epoch": 12.221695014234616,
      "grad_norm": 4.220176696777344,
      "learning_rate": 3.9815254154804486e-05,
      "loss": 0.6793,
      "step": 1339400
    },
    {
      "epoch": 12.222607489597781,
      "grad_norm": 3.9023349285125732,
      "learning_rate": 3.981449375866852e-05,
      "loss": 0.6737,
      "step": 1339500
    },
    {
      "epoch": 12.223519964960946,
      "grad_norm": 3.9699223041534424,
      "learning_rate": 3.9813733362532546e-05,
      "loss": 0.7003,
      "step": 1339600
    },
    {
      "epoch": 12.224432440324112,
      "grad_norm": 4.6871843338012695,
      "learning_rate": 3.9812972966396576e-05,
      "loss": 0.6598,
      "step": 1339700
    },
    {
      "epoch": 12.225344915687277,
      "grad_norm": 3.6283679008483887,
      "learning_rate": 3.9812212570260606e-05,
      "loss": 0.6037,
      "step": 1339800
    },
    {
      "epoch": 12.226257391050442,
      "grad_norm": 4.351398468017578,
      "learning_rate": 3.9811452174124636e-05,
      "loss": 0.6715,
      "step": 1339900
    },
    {
      "epoch": 12.227169866413607,
      "grad_norm": 3.6601784229278564,
      "learning_rate": 3.981069177798866e-05,
      "loss": 0.6495,
      "step": 1340000
    },
    {
      "epoch": 12.228082341776773,
      "grad_norm": 4.393774032592773,
      "learning_rate": 3.9809931381852696e-05,
      "loss": 0.7058,
      "step": 1340100
    },
    {
      "epoch": 12.228994817139938,
      "grad_norm": 4.220901012420654,
      "learning_rate": 3.980917098571672e-05,
      "loss": 0.6788,
      "step": 1340200
    },
    {
      "epoch": 12.229907292503102,
      "grad_norm": 3.301004409790039,
      "learning_rate": 3.980841058958075e-05,
      "loss": 0.6654,
      "step": 1340300
    },
    {
      "epoch": 12.230819767866267,
      "grad_norm": 4.318072319030762,
      "learning_rate": 3.980765019344478e-05,
      "loss": 0.6251,
      "step": 1340400
    },
    {
      "epoch": 12.231732243229432,
      "grad_norm": 3.953888416290283,
      "learning_rate": 3.980688979730881e-05,
      "loss": 0.6482,
      "step": 1340500
    },
    {
      "epoch": 12.232644718592597,
      "grad_norm": 4.161806106567383,
      "learning_rate": 3.980612940117284e-05,
      "loss": 0.6842,
      "step": 1340600
    },
    {
      "epoch": 12.233557193955763,
      "grad_norm": 4.758749485015869,
      "learning_rate": 3.980536900503686e-05,
      "loss": 0.676,
      "step": 1340700
    },
    {
      "epoch": 12.234469669318928,
      "grad_norm": 3.9514877796173096,
      "learning_rate": 3.980460860890089e-05,
      "loss": 0.6787,
      "step": 1340800
    },
    {
      "epoch": 12.235382144682093,
      "grad_norm": 3.6636035442352295,
      "learning_rate": 3.980384821276492e-05,
      "loss": 0.6606,
      "step": 1340900
    },
    {
      "epoch": 12.236294620045259,
      "grad_norm": 3.5813469886779785,
      "learning_rate": 3.980308781662895e-05,
      "loss": 0.6592,
      "step": 1341000
    },
    {
      "epoch": 12.237207095408424,
      "grad_norm": 4.346972942352295,
      "learning_rate": 3.9802327420492977e-05,
      "loss": 0.7063,
      "step": 1341100
    },
    {
      "epoch": 12.23811957077159,
      "grad_norm": 4.13950252532959,
      "learning_rate": 3.9801567024357013e-05,
      "loss": 0.6632,
      "step": 1341200
    },
    {
      "epoch": 12.239032046134755,
      "grad_norm": 3.9107961654663086,
      "learning_rate": 3.980080662822104e-05,
      "loss": 0.6493,
      "step": 1341300
    },
    {
      "epoch": 12.23994452149792,
      "grad_norm": 4.606551170349121,
      "learning_rate": 3.980004623208507e-05,
      "loss": 0.6967,
      "step": 1341400
    },
    {
      "epoch": 12.240856996861085,
      "grad_norm": 4.905243873596191,
      "learning_rate": 3.97992858359491e-05,
      "loss": 0.7334,
      "step": 1341500
    },
    {
      "epoch": 12.24176947222425,
      "grad_norm": 4.3107380867004395,
      "learning_rate": 3.979852543981313e-05,
      "loss": 0.6963,
      "step": 1341600
    },
    {
      "epoch": 12.242681947587416,
      "grad_norm": 4.190301418304443,
      "learning_rate": 3.979776504367716e-05,
      "loss": 0.6733,
      "step": 1341700
    },
    {
      "epoch": 12.243594422950581,
      "grad_norm": 3.7784626483917236,
      "learning_rate": 3.979700464754119e-05,
      "loss": 0.7148,
      "step": 1341800
    },
    {
      "epoch": 12.244506898313746,
      "grad_norm": 3.553978681564331,
      "learning_rate": 3.979624425140521e-05,
      "loss": 0.6648,
      "step": 1341900
    },
    {
      "epoch": 12.24541937367691,
      "grad_norm": 3.4289650917053223,
      "learning_rate": 3.979548385526925e-05,
      "loss": 0.6399,
      "step": 1342000
    },
    {
      "epoch": 12.246331849040075,
      "grad_norm": 3.5297834873199463,
      "learning_rate": 3.979472345913327e-05,
      "loss": 0.6596,
      "step": 1342100
    },
    {
      "epoch": 12.24724432440324,
      "grad_norm": 3.9068989753723145,
      "learning_rate": 3.97939630629973e-05,
      "loss": 0.6986,
      "step": 1342200
    },
    {
      "epoch": 12.248156799766406,
      "grad_norm": 3.4277870655059814,
      "learning_rate": 3.979320266686133e-05,
      "loss": 0.6866,
      "step": 1342300
    },
    {
      "epoch": 12.249069275129571,
      "grad_norm": 4.353151321411133,
      "learning_rate": 3.979244227072536e-05,
      "loss": 0.6683,
      "step": 1342400
    },
    {
      "epoch": 12.249981750492736,
      "grad_norm": 3.728139877319336,
      "learning_rate": 3.9791681874589384e-05,
      "loss": 0.6663,
      "step": 1342500
    },
    {
      "epoch": 12.250894225855902,
      "grad_norm": 4.674510478973389,
      "learning_rate": 3.979092147845342e-05,
      "loss": 0.6589,
      "step": 1342600
    },
    {
      "epoch": 12.251806701219067,
      "grad_norm": 3.8132758140563965,
      "learning_rate": 3.9790161082317444e-05,
      "loss": 0.6547,
      "step": 1342700
    },
    {
      "epoch": 12.252719176582232,
      "grad_norm": 2.7907440662384033,
      "learning_rate": 3.9789400686181474e-05,
      "loss": 0.6626,
      "step": 1342800
    },
    {
      "epoch": 12.253631651945398,
      "grad_norm": 3.7439188957214355,
      "learning_rate": 3.9788640290045504e-05,
      "loss": 0.6532,
      "step": 1342900
    },
    {
      "epoch": 12.254544127308563,
      "grad_norm": 4.241408348083496,
      "learning_rate": 3.9787879893909534e-05,
      "loss": 0.6728,
      "step": 1343000
    },
    {
      "epoch": 12.255456602671728,
      "grad_norm": 4.30990743637085,
      "learning_rate": 3.9787119497773564e-05,
      "loss": 0.6895,
      "step": 1343100
    },
    {
      "epoch": 12.256369078034894,
      "grad_norm": 4.310577392578125,
      "learning_rate": 3.9786359101637594e-05,
      "loss": 0.6613,
      "step": 1343200
    },
    {
      "epoch": 12.257281553398059,
      "grad_norm": 3.4812567234039307,
      "learning_rate": 3.978559870550162e-05,
      "loss": 0.6571,
      "step": 1343300
    },
    {
      "epoch": 12.258194028761224,
      "grad_norm": 1.9528053998947144,
      "learning_rate": 3.978483830936565e-05,
      "loss": 0.6676,
      "step": 1343400
    },
    {
      "epoch": 12.25910650412439,
      "grad_norm": 3.554004192352295,
      "learning_rate": 3.978407791322968e-05,
      "loss": 0.6898,
      "step": 1343500
    },
    {
      "epoch": 12.260018979487555,
      "grad_norm": 4.586920738220215,
      "learning_rate": 3.97833175170937e-05,
      "loss": 0.6907,
      "step": 1343600
    },
    {
      "epoch": 12.260931454850718,
      "grad_norm": 5.861287593841553,
      "learning_rate": 3.978255712095774e-05,
      "loss": 0.6442,
      "step": 1343700
    },
    {
      "epoch": 12.261843930213884,
      "grad_norm": 4.2374773025512695,
      "learning_rate": 3.978179672482176e-05,
      "loss": 0.6537,
      "step": 1343800
    },
    {
      "epoch": 12.262756405577049,
      "grad_norm": 3.731809139251709,
      "learning_rate": 3.978103632868579e-05,
      "loss": 0.6935,
      "step": 1343900
    },
    {
      "epoch": 12.263668880940214,
      "grad_norm": 3.948317766189575,
      "learning_rate": 3.978027593254982e-05,
      "loss": 0.6592,
      "step": 1344000
    },
    {
      "epoch": 12.26458135630338,
      "grad_norm": 2.8046376705169678,
      "learning_rate": 3.977951553641385e-05,
      "loss": 0.6432,
      "step": 1344100
    },
    {
      "epoch": 12.265493831666545,
      "grad_norm": 3.8764538764953613,
      "learning_rate": 3.977875514027788e-05,
      "loss": 0.678,
      "step": 1344200
    },
    {
      "epoch": 12.26640630702971,
      "grad_norm": 3.657752513885498,
      "learning_rate": 3.977799474414191e-05,
      "loss": 0.6648,
      "step": 1344300
    },
    {
      "epoch": 12.267318782392875,
      "grad_norm": 4.088776111602783,
      "learning_rate": 3.9777234348005935e-05,
      "loss": 0.6392,
      "step": 1344400
    },
    {
      "epoch": 12.26823125775604,
      "grad_norm": 4.141227722167969,
      "learning_rate": 3.977647395186997e-05,
      "loss": 0.6861,
      "step": 1344500
    },
    {
      "epoch": 12.269143733119206,
      "grad_norm": 4.225869178771973,
      "learning_rate": 3.9775713555733995e-05,
      "loss": 0.6754,
      "step": 1344600
    },
    {
      "epoch": 12.270056208482371,
      "grad_norm": 3.8842523097991943,
      "learning_rate": 3.9774953159598025e-05,
      "loss": 0.7033,
      "step": 1344700
    },
    {
      "epoch": 12.270968683845537,
      "grad_norm": 4.496241092681885,
      "learning_rate": 3.9774192763462055e-05,
      "loss": 0.6862,
      "step": 1344800
    },
    {
      "epoch": 12.271881159208702,
      "grad_norm": 3.4999282360076904,
      "learning_rate": 3.9773432367326085e-05,
      "loss": 0.7007,
      "step": 1344900
    },
    {
      "epoch": 12.272793634571867,
      "grad_norm": 4.129291534423828,
      "learning_rate": 3.977267197119011e-05,
      "loss": 0.6496,
      "step": 1345000
    },
    {
      "epoch": 12.273706109935032,
      "grad_norm": 4.377084732055664,
      "learning_rate": 3.9771911575054145e-05,
      "loss": 0.6685,
      "step": 1345100
    },
    {
      "epoch": 12.274618585298198,
      "grad_norm": 4.049413681030273,
      "learning_rate": 3.977115117891817e-05,
      "loss": 0.6794,
      "step": 1345200
    },
    {
      "epoch": 12.275531060661361,
      "grad_norm": 4.282479763031006,
      "learning_rate": 3.97703907827822e-05,
      "loss": 0.663,
      "step": 1345300
    },
    {
      "epoch": 12.276443536024527,
      "grad_norm": 3.9767940044403076,
      "learning_rate": 3.976963038664623e-05,
      "loss": 0.6948,
      "step": 1345400
    },
    {
      "epoch": 12.277356011387692,
      "grad_norm": 2.138615846633911,
      "learning_rate": 3.976886999051026e-05,
      "loss": 0.6847,
      "step": 1345500
    },
    {
      "epoch": 12.278268486750857,
      "grad_norm": 3.2351186275482178,
      "learning_rate": 3.976810959437429e-05,
      "loss": 0.6544,
      "step": 1345600
    },
    {
      "epoch": 12.279180962114022,
      "grad_norm": 3.977586030960083,
      "learning_rate": 3.976734919823832e-05,
      "loss": 0.7006,
      "step": 1345700
    },
    {
      "epoch": 12.280093437477188,
      "grad_norm": 3.9046807289123535,
      "learning_rate": 3.976658880210234e-05,
      "loss": 0.6941,
      "step": 1345800
    },
    {
      "epoch": 12.281005912840353,
      "grad_norm": 3.917301654815674,
      "learning_rate": 3.976582840596638e-05,
      "loss": 0.6754,
      "step": 1345900
    },
    {
      "epoch": 12.281918388203518,
      "grad_norm": 5.2700324058532715,
      "learning_rate": 3.97650680098304e-05,
      "loss": 0.7212,
      "step": 1346000
    },
    {
      "epoch": 12.282830863566684,
      "grad_norm": 4.524605751037598,
      "learning_rate": 3.976430761369443e-05,
      "loss": 0.6721,
      "step": 1346100
    },
    {
      "epoch": 12.283743338929849,
      "grad_norm": 3.9524526596069336,
      "learning_rate": 3.976354721755846e-05,
      "loss": 0.6671,
      "step": 1346200
    },
    {
      "epoch": 12.284655814293014,
      "grad_norm": 4.278698921203613,
      "learning_rate": 3.9762786821422486e-05,
      "loss": 0.6591,
      "step": 1346300
    },
    {
      "epoch": 12.28556828965618,
      "grad_norm": 3.4606966972351074,
      "learning_rate": 3.976202642528652e-05,
      "loss": 0.6604,
      "step": 1346400
    },
    {
      "epoch": 12.286480765019345,
      "grad_norm": 4.177457809448242,
      "learning_rate": 3.9761266029150546e-05,
      "loss": 0.6591,
      "step": 1346500
    },
    {
      "epoch": 12.28739324038251,
      "grad_norm": 4.314151287078857,
      "learning_rate": 3.9760505633014576e-05,
      "loss": 0.7048,
      "step": 1346600
    },
    {
      "epoch": 12.288305715745675,
      "grad_norm": 3.846208095550537,
      "learning_rate": 3.9759745236878606e-05,
      "loss": 0.6578,
      "step": 1346700
    },
    {
      "epoch": 12.28921819110884,
      "grad_norm": 3.782470226287842,
      "learning_rate": 3.9758984840742636e-05,
      "loss": 0.6534,
      "step": 1346800
    },
    {
      "epoch": 12.290130666472006,
      "grad_norm": 3.416168451309204,
      "learning_rate": 3.975822444460666e-05,
      "loss": 0.6851,
      "step": 1346900
    },
    {
      "epoch": 12.291043141835171,
      "grad_norm": 4.1796979904174805,
      "learning_rate": 3.9757464048470696e-05,
      "loss": 0.667,
      "step": 1347000
    },
    {
      "epoch": 12.291955617198335,
      "grad_norm": 3.2694032192230225,
      "learning_rate": 3.975670365233472e-05,
      "loss": 0.6624,
      "step": 1347100
    },
    {
      "epoch": 12.2928680925615,
      "grad_norm": 4.201051235198975,
      "learning_rate": 3.975594325619875e-05,
      "loss": 0.6384,
      "step": 1347200
    },
    {
      "epoch": 12.293780567924665,
      "grad_norm": 3.997603416442871,
      "learning_rate": 3.975518286006278e-05,
      "loss": 0.6829,
      "step": 1347300
    },
    {
      "epoch": 12.29469304328783,
      "grad_norm": 4.50974702835083,
      "learning_rate": 3.975442246392681e-05,
      "loss": 0.6514,
      "step": 1347400
    },
    {
      "epoch": 12.295605518650996,
      "grad_norm": 3.954103708267212,
      "learning_rate": 3.975366206779084e-05,
      "loss": 0.694,
      "step": 1347500
    },
    {
      "epoch": 12.296517994014161,
      "grad_norm": 3.2544338703155518,
      "learning_rate": 3.975290167165487e-05,
      "loss": 0.6848,
      "step": 1347600
    },
    {
      "epoch": 12.297430469377327,
      "grad_norm": 4.422313690185547,
      "learning_rate": 3.975214127551889e-05,
      "loss": 0.7028,
      "step": 1347700
    },
    {
      "epoch": 12.298342944740492,
      "grad_norm": 4.1182966232299805,
      "learning_rate": 3.975138087938293e-05,
      "loss": 0.6601,
      "step": 1347800
    },
    {
      "epoch": 12.299255420103657,
      "grad_norm": 3.579335927963257,
      "learning_rate": 3.975062048324695e-05,
      "loss": 0.6454,
      "step": 1347900
    },
    {
      "epoch": 12.300167895466823,
      "grad_norm": 3.6104049682617188,
      "learning_rate": 3.9749860087110983e-05,
      "loss": 0.7011,
      "step": 1348000
    },
    {
      "epoch": 12.301080370829988,
      "grad_norm": 3.300802230834961,
      "learning_rate": 3.9749099690975014e-05,
      "loss": 0.6797,
      "step": 1348100
    },
    {
      "epoch": 12.301992846193153,
      "grad_norm": 4.273622512817383,
      "learning_rate": 3.9748339294839044e-05,
      "loss": 0.6616,
      "step": 1348200
    },
    {
      "epoch": 12.302905321556318,
      "grad_norm": 3.6643853187561035,
      "learning_rate": 3.974757889870307e-05,
      "loss": 0.6698,
      "step": 1348300
    },
    {
      "epoch": 12.303817796919484,
      "grad_norm": 4.461703777313232,
      "learning_rate": 3.9746818502567104e-05,
      "loss": 0.6956,
      "step": 1348400
    },
    {
      "epoch": 12.304730272282649,
      "grad_norm": 3.9287919998168945,
      "learning_rate": 3.974605810643113e-05,
      "loss": 0.6883,
      "step": 1348500
    },
    {
      "epoch": 12.305642747645814,
      "grad_norm": 4.179263114929199,
      "learning_rate": 3.974529771029516e-05,
      "loss": 0.6692,
      "step": 1348600
    },
    {
      "epoch": 12.306555223008978,
      "grad_norm": 4.258245468139648,
      "learning_rate": 3.974453731415919e-05,
      "loss": 0.6643,
      "step": 1348700
    },
    {
      "epoch": 12.307467698372143,
      "grad_norm": 4.081961631774902,
      "learning_rate": 3.974377691802322e-05,
      "loss": 0.6765,
      "step": 1348800
    },
    {
      "epoch": 12.308380173735308,
      "grad_norm": 3.5556581020355225,
      "learning_rate": 3.974301652188725e-05,
      "loss": 0.706,
      "step": 1348900
    },
    {
      "epoch": 12.309292649098474,
      "grad_norm": 4.684401988983154,
      "learning_rate": 3.974225612575128e-05,
      "loss": 0.6726,
      "step": 1349000
    },
    {
      "epoch": 12.310205124461639,
      "grad_norm": 4.625760078430176,
      "learning_rate": 3.97414957296153e-05,
      "loss": 0.7024,
      "step": 1349100
    },
    {
      "epoch": 12.311117599824804,
      "grad_norm": 3.8018240928649902,
      "learning_rate": 3.974073533347933e-05,
      "loss": 0.6773,
      "step": 1349200
    },
    {
      "epoch": 12.31203007518797,
      "grad_norm": 4.035287857055664,
      "learning_rate": 3.973997493734336e-05,
      "loss": 0.6968,
      "step": 1349300
    },
    {
      "epoch": 12.312942550551135,
      "grad_norm": 3.6103899478912354,
      "learning_rate": 3.9739214541207384e-05,
      "loss": 0.6952,
      "step": 1349400
    },
    {
      "epoch": 12.3138550259143,
      "grad_norm": 3.6522459983825684,
      "learning_rate": 3.973845414507142e-05,
      "loss": 0.6939,
      "step": 1349500
    },
    {
      "epoch": 12.314767501277466,
      "grad_norm": 4.012833595275879,
      "learning_rate": 3.9737693748935444e-05,
      "loss": 0.6534,
      "step": 1349600
    },
    {
      "epoch": 12.31567997664063,
      "grad_norm": 4.362954139709473,
      "learning_rate": 3.9736933352799474e-05,
      "loss": 0.6676,
      "step": 1349700
    },
    {
      "epoch": 12.316592452003796,
      "grad_norm": 4.4963788986206055,
      "learning_rate": 3.9736172956663504e-05,
      "loss": 0.6806,
      "step": 1349800
    },
    {
      "epoch": 12.317504927366961,
      "grad_norm": 4.292359352111816,
      "learning_rate": 3.9735412560527534e-05,
      "loss": 0.66,
      "step": 1349900
    },
    {
      "epoch": 12.318417402730127,
      "grad_norm": 3.3764591217041016,
      "learning_rate": 3.9734652164391564e-05,
      "loss": 0.7036,
      "step": 1350000
    },
    {
      "epoch": 12.319329878093292,
      "grad_norm": 4.086789608001709,
      "learning_rate": 3.9733891768255595e-05,
      "loss": 0.6753,
      "step": 1350100
    },
    {
      "epoch": 12.320242353456457,
      "grad_norm": 4.164106369018555,
      "learning_rate": 3.973313137211962e-05,
      "loss": 0.6686,
      "step": 1350200
    },
    {
      "epoch": 12.321154828819623,
      "grad_norm": 4.355865001678467,
      "learning_rate": 3.9732370975983655e-05,
      "loss": 0.6853,
      "step": 1350300
    },
    {
      "epoch": 12.322067304182788,
      "grad_norm": 3.280010938644409,
      "learning_rate": 3.973161057984768e-05,
      "loss": 0.6763,
      "step": 1350400
    },
    {
      "epoch": 12.322979779545951,
      "grad_norm": 4.329034328460693,
      "learning_rate": 3.973085018371171e-05,
      "loss": 0.6934,
      "step": 1350500
    },
    {
      "epoch": 12.323892254909117,
      "grad_norm": 3.9360156059265137,
      "learning_rate": 3.973008978757574e-05,
      "loss": 0.6748,
      "step": 1350600
    },
    {
      "epoch": 12.324804730272282,
      "grad_norm": 4.46719217300415,
      "learning_rate": 3.972932939143977e-05,
      "loss": 0.6897,
      "step": 1350700
    },
    {
      "epoch": 12.325717205635447,
      "grad_norm": 4.380502700805664,
      "learning_rate": 3.972856899530379e-05,
      "loss": 0.6771,
      "step": 1350800
    },
    {
      "epoch": 12.326629680998613,
      "grad_norm": 4.469130516052246,
      "learning_rate": 3.972780859916783e-05,
      "loss": 0.6989,
      "step": 1350900
    },
    {
      "epoch": 12.327542156361778,
      "grad_norm": 3.8145058155059814,
      "learning_rate": 3.972704820303185e-05,
      "loss": 0.6765,
      "step": 1351000
    },
    {
      "epoch": 12.328454631724943,
      "grad_norm": 4.043845176696777,
      "learning_rate": 3.972628780689588e-05,
      "loss": 0.7057,
      "step": 1351100
    },
    {
      "epoch": 12.329367107088109,
      "grad_norm": 3.6654210090637207,
      "learning_rate": 3.972552741075991e-05,
      "loss": 0.6778,
      "step": 1351200
    },
    {
      "epoch": 12.330279582451274,
      "grad_norm": 4.039319038391113,
      "learning_rate": 3.972476701462394e-05,
      "loss": 0.6687,
      "step": 1351300
    },
    {
      "epoch": 12.33119205781444,
      "grad_norm": 3.9258317947387695,
      "learning_rate": 3.972400661848797e-05,
      "loss": 0.6987,
      "step": 1351400
    },
    {
      "epoch": 12.332104533177604,
      "grad_norm": 4.295949935913086,
      "learning_rate": 3.9723246222352e-05,
      "loss": 0.6803,
      "step": 1351500
    },
    {
      "epoch": 12.33301700854077,
      "grad_norm": 3.22965669631958,
      "learning_rate": 3.9722485826216025e-05,
      "loss": 0.6487,
      "step": 1351600
    },
    {
      "epoch": 12.333929483903935,
      "grad_norm": 3.7318038940429688,
      "learning_rate": 3.972172543008006e-05,
      "loss": 0.7141,
      "step": 1351700
    },
    {
      "epoch": 12.3348419592671,
      "grad_norm": 3.981482982635498,
      "learning_rate": 3.9720965033944085e-05,
      "loss": 0.6583,
      "step": 1351800
    },
    {
      "epoch": 12.335754434630266,
      "grad_norm": 3.302687406539917,
      "learning_rate": 3.9720204637808115e-05,
      "loss": 0.6588,
      "step": 1351900
    },
    {
      "epoch": 12.336666909993431,
      "grad_norm": 3.4203274250030518,
      "learning_rate": 3.9719444241672145e-05,
      "loss": 0.7276,
      "step": 1352000
    },
    {
      "epoch": 12.337579385356594,
      "grad_norm": 4.364673137664795,
      "learning_rate": 3.971868384553617e-05,
      "loss": 0.6788,
      "step": 1352100
    },
    {
      "epoch": 12.33849186071976,
      "grad_norm": 4.6673150062561035,
      "learning_rate": 3.97179234494002e-05,
      "loss": 0.6907,
      "step": 1352200
    },
    {
      "epoch": 12.339404336082925,
      "grad_norm": 3.11321759223938,
      "learning_rate": 3.971716305326423e-05,
      "loss": 0.682,
      "step": 1352300
    },
    {
      "epoch": 12.34031681144609,
      "grad_norm": 4.275173187255859,
      "learning_rate": 3.971640265712826e-05,
      "loss": 0.7168,
      "step": 1352400
    },
    {
      "epoch": 12.341229286809256,
      "grad_norm": 4.206845283508301,
      "learning_rate": 3.971564226099229e-05,
      "loss": 0.7059,
      "step": 1352500
    },
    {
      "epoch": 12.342141762172421,
      "grad_norm": 3.737874746322632,
      "learning_rate": 3.971488186485632e-05,
      "loss": 0.6799,
      "step": 1352600
    },
    {
      "epoch": 12.343054237535586,
      "grad_norm": 3.557047128677368,
      "learning_rate": 3.971412146872034e-05,
      "loss": 0.6499,
      "step": 1352700
    },
    {
      "epoch": 12.343966712898752,
      "grad_norm": 4.015947341918945,
      "learning_rate": 3.971336107258438e-05,
      "loss": 0.6321,
      "step": 1352800
    },
    {
      "epoch": 12.344879188261917,
      "grad_norm": 4.103269100189209,
      "learning_rate": 3.97126006764484e-05,
      "loss": 0.6691,
      "step": 1352900
    },
    {
      "epoch": 12.345791663625082,
      "grad_norm": 4.776555061340332,
      "learning_rate": 3.971184028031243e-05,
      "loss": 0.6776,
      "step": 1353000
    },
    {
      "epoch": 12.346704138988247,
      "grad_norm": 4.332473278045654,
      "learning_rate": 3.971107988417646e-05,
      "loss": 0.6785,
      "step": 1353100
    },
    {
      "epoch": 12.347616614351413,
      "grad_norm": 3.1752212047576904,
      "learning_rate": 3.971031948804049e-05,
      "loss": 0.6656,
      "step": 1353200
    },
    {
      "epoch": 12.348529089714578,
      "grad_norm": 4.296579360961914,
      "learning_rate": 3.9709559091904516e-05,
      "loss": 0.6621,
      "step": 1353300
    },
    {
      "epoch": 12.349441565077743,
      "grad_norm": 4.081934452056885,
      "learning_rate": 3.970879869576855e-05,
      "loss": 0.6297,
      "step": 1353400
    },
    {
      "epoch": 12.350354040440909,
      "grad_norm": 3.2422943115234375,
      "learning_rate": 3.9708038299632576e-05,
      "loss": 0.6671,
      "step": 1353500
    },
    {
      "epoch": 12.351266515804074,
      "grad_norm": 4.329543113708496,
      "learning_rate": 3.9707277903496606e-05,
      "loss": 0.6562,
      "step": 1353600
    },
    {
      "epoch": 12.35217899116724,
      "grad_norm": 3.1104724407196045,
      "learning_rate": 3.9706517507360636e-05,
      "loss": 0.6984,
      "step": 1353700
    },
    {
      "epoch": 12.353091466530405,
      "grad_norm": 3.7531368732452393,
      "learning_rate": 3.9705757111224666e-05,
      "loss": 0.649,
      "step": 1353800
    },
    {
      "epoch": 12.354003941893568,
      "grad_norm": 3.8177082538604736,
      "learning_rate": 3.9704996715088696e-05,
      "loss": 0.6636,
      "step": 1353900
    },
    {
      "epoch": 12.354916417256733,
      "grad_norm": 3.659630060195923,
      "learning_rate": 3.9704236318952726e-05,
      "loss": 0.6744,
      "step": 1354000
    },
    {
      "epoch": 12.355828892619899,
      "grad_norm": 4.434305191040039,
      "learning_rate": 3.970347592281675e-05,
      "loss": 0.6818,
      "step": 1354100
    },
    {
      "epoch": 12.356741367983064,
      "grad_norm": 4.464044570922852,
      "learning_rate": 3.970271552668079e-05,
      "loss": 0.6707,
      "step": 1354200
    },
    {
      "epoch": 12.35765384334623,
      "grad_norm": 2.9751532077789307,
      "learning_rate": 3.970195513054481e-05,
      "loss": 0.6401,
      "step": 1354300
    },
    {
      "epoch": 12.358566318709395,
      "grad_norm": 3.9630799293518066,
      "learning_rate": 3.970119473440884e-05,
      "loss": 0.6518,
      "step": 1354400
    },
    {
      "epoch": 12.35947879407256,
      "grad_norm": 3.0215213298797607,
      "learning_rate": 3.970043433827287e-05,
      "loss": 0.6617,
      "step": 1354500
    },
    {
      "epoch": 12.360391269435725,
      "grad_norm": 4.364070415496826,
      "learning_rate": 3.96996739421369e-05,
      "loss": 0.6754,
      "step": 1354600
    },
    {
      "epoch": 12.36130374479889,
      "grad_norm": 4.29319953918457,
      "learning_rate": 3.9698913546000923e-05,
      "loss": 0.6758,
      "step": 1354700
    },
    {
      "epoch": 12.362216220162056,
      "grad_norm": 4.0071587562561035,
      "learning_rate": 3.9698153149864953e-05,
      "loss": 0.6715,
      "step": 1354800
    },
    {
      "epoch": 12.363128695525221,
      "grad_norm": 4.4335103034973145,
      "learning_rate": 3.9697392753728984e-05,
      "loss": 0.691,
      "step": 1354900
    },
    {
      "epoch": 12.364041170888386,
      "grad_norm": 4.280450820922852,
      "learning_rate": 3.9696632357593014e-05,
      "loss": 0.6709,
      "step": 1355000
    },
    {
      "epoch": 12.364953646251552,
      "grad_norm": 4.995495319366455,
      "learning_rate": 3.9695871961457044e-05,
      "loss": 0.6735,
      "step": 1355100
    },
    {
      "epoch": 12.365866121614717,
      "grad_norm": 4.243288516998291,
      "learning_rate": 3.969511156532107e-05,
      "loss": 0.6717,
      "step": 1355200
    },
    {
      "epoch": 12.366778596977882,
      "grad_norm": 4.347921848297119,
      "learning_rate": 3.9694351169185104e-05,
      "loss": 0.684,
      "step": 1355300
    },
    {
      "epoch": 12.367691072341048,
      "grad_norm": 3.500096559524536,
      "learning_rate": 3.969359077304913e-05,
      "loss": 0.6536,
      "step": 1355400
    },
    {
      "epoch": 12.368603547704211,
      "grad_norm": 4.4082350730896,
      "learning_rate": 3.969283037691316e-05,
      "loss": 0.6456,
      "step": 1355500
    },
    {
      "epoch": 12.369516023067376,
      "grad_norm": 1.8044413328170776,
      "learning_rate": 3.969206998077719e-05,
      "loss": 0.6831,
      "step": 1355600
    },
    {
      "epoch": 12.370428498430542,
      "grad_norm": 3.7343344688415527,
      "learning_rate": 3.969130958464122e-05,
      "loss": 0.6826,
      "step": 1355700
    },
    {
      "epoch": 12.371340973793707,
      "grad_norm": 3.6583378314971924,
      "learning_rate": 3.969054918850524e-05,
      "loss": 0.6431,
      "step": 1355800
    },
    {
      "epoch": 12.372253449156872,
      "grad_norm": 2.7721054553985596,
      "learning_rate": 3.968978879236928e-05,
      "loss": 0.6895,
      "step": 1355900
    },
    {
      "epoch": 12.373165924520038,
      "grad_norm": 4.635746002197266,
      "learning_rate": 3.96890283962333e-05,
      "loss": 0.6906,
      "step": 1356000
    },
    {
      "epoch": 12.374078399883203,
      "grad_norm": 4.612853527069092,
      "learning_rate": 3.968826800009733e-05,
      "loss": 0.6688,
      "step": 1356100
    },
    {
      "epoch": 12.374990875246368,
      "grad_norm": 3.7337734699249268,
      "learning_rate": 3.968750760396136e-05,
      "loss": 0.7048,
      "step": 1356200
    },
    {
      "epoch": 12.375903350609534,
      "grad_norm": 3.435887575149536,
      "learning_rate": 3.968674720782539e-05,
      "loss": 0.6833,
      "step": 1356300
    },
    {
      "epoch": 12.376815825972699,
      "grad_norm": 3.992875337600708,
      "learning_rate": 3.968598681168942e-05,
      "loss": 0.7323,
      "step": 1356400
    },
    {
      "epoch": 12.377728301335864,
      "grad_norm": 3.0845892429351807,
      "learning_rate": 3.968522641555345e-05,
      "loss": 0.7076,
      "step": 1356500
    },
    {
      "epoch": 12.37864077669903,
      "grad_norm": 4.548949241638184,
      "learning_rate": 3.9684466019417474e-05,
      "loss": 0.6586,
      "step": 1356600
    },
    {
      "epoch": 12.379553252062195,
      "grad_norm": 3.786929130554199,
      "learning_rate": 3.968370562328151e-05,
      "loss": 0.6509,
      "step": 1356700
    },
    {
      "epoch": 12.38046572742536,
      "grad_norm": 4.263677597045898,
      "learning_rate": 3.9682945227145534e-05,
      "loss": 0.6689,
      "step": 1356800
    },
    {
      "epoch": 12.381378202788525,
      "grad_norm": 4.186014175415039,
      "learning_rate": 3.9682184831009565e-05,
      "loss": 0.6517,
      "step": 1356900
    },
    {
      "epoch": 12.38229067815169,
      "grad_norm": 3.7614517211914062,
      "learning_rate": 3.9681424434873595e-05,
      "loss": 0.6927,
      "step": 1357000
    },
    {
      "epoch": 12.383203153514856,
      "grad_norm": 4.377279758453369,
      "learning_rate": 3.9680664038737625e-05,
      "loss": 0.692,
      "step": 1357100
    },
    {
      "epoch": 12.384115628878021,
      "grad_norm": 3.8721745014190674,
      "learning_rate": 3.967990364260165e-05,
      "loss": 0.6984,
      "step": 1357200
    },
    {
      "epoch": 12.385028104241185,
      "grad_norm": 3.9380011558532715,
      "learning_rate": 3.9679143246465685e-05,
      "loss": 0.6582,
      "step": 1357300
    },
    {
      "epoch": 12.38594057960435,
      "grad_norm": 3.455352306365967,
      "learning_rate": 3.967838285032971e-05,
      "loss": 0.7221,
      "step": 1357400
    },
    {
      "epoch": 12.386853054967515,
      "grad_norm": 4.307465553283691,
      "learning_rate": 3.967762245419374e-05,
      "loss": 0.6664,
      "step": 1357500
    },
    {
      "epoch": 12.38776553033068,
      "grad_norm": 3.9037516117095947,
      "learning_rate": 3.967686205805777e-05,
      "loss": 0.6463,
      "step": 1357600
    },
    {
      "epoch": 12.388678005693846,
      "grad_norm": 4.514156341552734,
      "learning_rate": 3.967610166192179e-05,
      "loss": 0.6647,
      "step": 1357700
    },
    {
      "epoch": 12.389590481057011,
      "grad_norm": 4.304577827453613,
      "learning_rate": 3.967534126578583e-05,
      "loss": 0.6728,
      "step": 1357800
    },
    {
      "epoch": 12.390502956420177,
      "grad_norm": 3.0595483779907227,
      "learning_rate": 3.967458086964985e-05,
      "loss": 0.6943,
      "step": 1357900
    },
    {
      "epoch": 12.391415431783342,
      "grad_norm": 4.120588302612305,
      "learning_rate": 3.967382047351388e-05,
      "loss": 0.6902,
      "step": 1358000
    },
    {
      "epoch": 12.392327907146507,
      "grad_norm": 3.7693965435028076,
      "learning_rate": 3.967306007737791e-05,
      "loss": 0.6653,
      "step": 1358100
    },
    {
      "epoch": 12.393240382509672,
      "grad_norm": 3.0656278133392334,
      "learning_rate": 3.967229968124194e-05,
      "loss": 0.6523,
      "step": 1358200
    },
    {
      "epoch": 12.394152857872838,
      "grad_norm": 4.216495990753174,
      "learning_rate": 3.967153928510597e-05,
      "loss": 0.6591,
      "step": 1358300
    },
    {
      "epoch": 12.395065333236003,
      "grad_norm": 3.819692373275757,
      "learning_rate": 3.967077888897e-05,
      "loss": 0.715,
      "step": 1358400
    },
    {
      "epoch": 12.395977808599168,
      "grad_norm": 3.9928202629089355,
      "learning_rate": 3.9670018492834025e-05,
      "loss": 0.6858,
      "step": 1358500
    },
    {
      "epoch": 12.396890283962334,
      "grad_norm": 3.87319278717041,
      "learning_rate": 3.9669258096698055e-05,
      "loss": 0.6932,
      "step": 1358600
    },
    {
      "epoch": 12.397802759325499,
      "grad_norm": 3.5645833015441895,
      "learning_rate": 3.9668497700562085e-05,
      "loss": 0.7,
      "step": 1358700
    },
    {
      "epoch": 12.398715234688664,
      "grad_norm": 4.522749423980713,
      "learning_rate": 3.9667737304426116e-05,
      "loss": 0.6762,
      "step": 1358800
    },
    {
      "epoch": 12.399627710051828,
      "grad_norm": 4.193994998931885,
      "learning_rate": 3.9666976908290146e-05,
      "loss": 0.6813,
      "step": 1358900
    },
    {
      "epoch": 12.400540185414993,
      "grad_norm": 4.133977890014648,
      "learning_rate": 3.9666216512154176e-05,
      "loss": 0.6543,
      "step": 1359000
    },
    {
      "epoch": 12.401452660778158,
      "grad_norm": 3.692570209503174,
      "learning_rate": 3.96654561160182e-05,
      "loss": 0.6451,
      "step": 1359100
    },
    {
      "epoch": 12.402365136141324,
      "grad_norm": 4.796746730804443,
      "learning_rate": 3.9664695719882236e-05,
      "loss": 0.6227,
      "step": 1359200
    },
    {
      "epoch": 12.403277611504489,
      "grad_norm": 3.6719555854797363,
      "learning_rate": 3.966393532374626e-05,
      "loss": 0.6498,
      "step": 1359300
    },
    {
      "epoch": 12.404190086867654,
      "grad_norm": 3.7928380966186523,
      "learning_rate": 3.966317492761029e-05,
      "loss": 0.666,
      "step": 1359400
    },
    {
      "epoch": 12.40510256223082,
      "grad_norm": 4.072494029998779,
      "learning_rate": 3.966241453147432e-05,
      "loss": 0.6528,
      "step": 1359500
    },
    {
      "epoch": 12.406015037593985,
      "grad_norm": 2.802551031112671,
      "learning_rate": 3.966165413533835e-05,
      "loss": 0.6534,
      "step": 1359600
    },
    {
      "epoch": 12.40692751295715,
      "grad_norm": 3.7488744258880615,
      "learning_rate": 3.966089373920238e-05,
      "loss": 0.6437,
      "step": 1359700
    },
    {
      "epoch": 12.407839988320315,
      "grad_norm": 4.548552513122559,
      "learning_rate": 3.966013334306641e-05,
      "loss": 0.6818,
      "step": 1359800
    },
    {
      "epoch": 12.40875246368348,
      "grad_norm": 4.168771266937256,
      "learning_rate": 3.965937294693043e-05,
      "loss": 0.7178,
      "step": 1359900
    },
    {
      "epoch": 12.409664939046646,
      "grad_norm": 4.215096473693848,
      "learning_rate": 3.965861255079446e-05,
      "loss": 0.6554,
      "step": 1360000
    },
    {
      "epoch": 12.410577414409811,
      "grad_norm": 5.020806312561035,
      "learning_rate": 3.965785215465849e-05,
      "loss": 0.686,
      "step": 1360100
    },
    {
      "epoch": 12.411489889772977,
      "grad_norm": 4.788466930389404,
      "learning_rate": 3.965709175852252e-05,
      "loss": 0.7082,
      "step": 1360200
    },
    {
      "epoch": 12.412402365136142,
      "grad_norm": 3.1945581436157227,
      "learning_rate": 3.965633136238655e-05,
      "loss": 0.6628,
      "step": 1360300
    },
    {
      "epoch": 12.413314840499307,
      "grad_norm": 7.254088401794434,
      "learning_rate": 3.965557096625058e-05,
      "loss": 0.6702,
      "step": 1360400
    },
    {
      "epoch": 12.414227315862473,
      "grad_norm": 3.2076449394226074,
      "learning_rate": 3.9654810570114606e-05,
      "loss": 0.6922,
      "step": 1360500
    },
    {
      "epoch": 12.415139791225638,
      "grad_norm": 3.26554274559021,
      "learning_rate": 3.9654050173978636e-05,
      "loss": 0.6744,
      "step": 1360600
    },
    {
      "epoch": 12.416052266588801,
      "grad_norm": 3.3947532176971436,
      "learning_rate": 3.9653289777842666e-05,
      "loss": 0.676,
      "step": 1360700
    },
    {
      "epoch": 12.416964741951967,
      "grad_norm": 3.910369396209717,
      "learning_rate": 3.9652529381706697e-05,
      "loss": 0.684,
      "step": 1360800
    },
    {
      "epoch": 12.417877217315132,
      "grad_norm": 4.043673515319824,
      "learning_rate": 3.9651768985570727e-05,
      "loss": 0.6792,
      "step": 1360900
    },
    {
      "epoch": 12.418789692678297,
      "grad_norm": 3.80625057220459,
      "learning_rate": 3.965100858943475e-05,
      "loss": 0.7046,
      "step": 1361000
    },
    {
      "epoch": 12.419702168041463,
      "grad_norm": 3.6351547241210938,
      "learning_rate": 3.965024819329879e-05,
      "loss": 0.6368,
      "step": 1361100
    },
    {
      "epoch": 12.420614643404628,
      "grad_norm": 3.2570059299468994,
      "learning_rate": 3.964948779716281e-05,
      "loss": 0.6678,
      "step": 1361200
    },
    {
      "epoch": 12.421527118767793,
      "grad_norm": 3.9753763675689697,
      "learning_rate": 3.964872740102684e-05,
      "loss": 0.6607,
      "step": 1361300
    },
    {
      "epoch": 12.422439594130958,
      "grad_norm": 4.225301742553711,
      "learning_rate": 3.964796700489087e-05,
      "loss": 0.6896,
      "step": 1361400
    },
    {
      "epoch": 12.423352069494124,
      "grad_norm": 2.8893706798553467,
      "learning_rate": 3.96472066087549e-05,
      "loss": 0.677,
      "step": 1361500
    },
    {
      "epoch": 12.424264544857289,
      "grad_norm": 4.332460880279541,
      "learning_rate": 3.9646446212618924e-05,
      "loss": 0.6894,
      "step": 1361600
    },
    {
      "epoch": 12.425177020220454,
      "grad_norm": 4.1856794357299805,
      "learning_rate": 3.964568581648296e-05,
      "loss": 0.6697,
      "step": 1361700
    },
    {
      "epoch": 12.42608949558362,
      "grad_norm": 4.640829563140869,
      "learning_rate": 3.9644925420346984e-05,
      "loss": 0.684,
      "step": 1361800
    },
    {
      "epoch": 12.427001970946785,
      "grad_norm": 4.003359794616699,
      "learning_rate": 3.9644165024211014e-05,
      "loss": 0.7028,
      "step": 1361900
    },
    {
      "epoch": 12.42791444630995,
      "grad_norm": 3.335326910018921,
      "learning_rate": 3.9643404628075044e-05,
      "loss": 0.7037,
      "step": 1362000
    },
    {
      "epoch": 12.428826921673116,
      "grad_norm": 4.820993900299072,
      "learning_rate": 3.9642644231939074e-05,
      "loss": 0.708,
      "step": 1362100
    },
    {
      "epoch": 12.42973939703628,
      "grad_norm": 5.3827805519104,
      "learning_rate": 3.9641883835803104e-05,
      "loss": 0.6936,
      "step": 1362200
    },
    {
      "epoch": 12.430651872399444,
      "grad_norm": 4.2299675941467285,
      "learning_rate": 3.9641123439667134e-05,
      "loss": 0.6612,
      "step": 1362300
    },
    {
      "epoch": 12.43156434776261,
      "grad_norm": 3.166938543319702,
      "learning_rate": 3.964036304353116e-05,
      "loss": 0.6617,
      "step": 1362400
    },
    {
      "epoch": 12.432476823125775,
      "grad_norm": 4.348299503326416,
      "learning_rate": 3.9639602647395194e-05,
      "loss": 0.6515,
      "step": 1362500
    },
    {
      "epoch": 12.43338929848894,
      "grad_norm": 2.5461227893829346,
      "learning_rate": 3.963884225125922e-05,
      "loss": 0.6451,
      "step": 1362600
    },
    {
      "epoch": 12.434301773852106,
      "grad_norm": 4.300509452819824,
      "learning_rate": 3.963808185512325e-05,
      "loss": 0.6908,
      "step": 1362700
    },
    {
      "epoch": 12.43521424921527,
      "grad_norm": 4.2960896492004395,
      "learning_rate": 3.963732145898728e-05,
      "loss": 0.6518,
      "step": 1362800
    },
    {
      "epoch": 12.436126724578436,
      "grad_norm": 4.040400981903076,
      "learning_rate": 3.963656106285131e-05,
      "loss": 0.7077,
      "step": 1362900
    },
    {
      "epoch": 12.437039199941601,
      "grad_norm": 4.158330917358398,
      "learning_rate": 3.963580066671533e-05,
      "loss": 0.6989,
      "step": 1363000
    },
    {
      "epoch": 12.437951675304767,
      "grad_norm": 4.374322414398193,
      "learning_rate": 3.963504027057937e-05,
      "loss": 0.6916,
      "step": 1363100
    },
    {
      "epoch": 12.438864150667932,
      "grad_norm": 3.210840940475464,
      "learning_rate": 3.963427987444339e-05,
      "loss": 0.6564,
      "step": 1363200
    },
    {
      "epoch": 12.439776626031097,
      "grad_norm": 4.352785587310791,
      "learning_rate": 3.963351947830742e-05,
      "loss": 0.6827,
      "step": 1363300
    },
    {
      "epoch": 12.440689101394263,
      "grad_norm": 3.1948180198669434,
      "learning_rate": 3.963275908217145e-05,
      "loss": 0.6672,
      "step": 1363400
    },
    {
      "epoch": 12.441601576757428,
      "grad_norm": 3.845703363418579,
      "learning_rate": 3.9631998686035474e-05,
      "loss": 0.6626,
      "step": 1363500
    },
    {
      "epoch": 12.442514052120593,
      "grad_norm": 3.5560081005096436,
      "learning_rate": 3.963123828989951e-05,
      "loss": 0.6441,
      "step": 1363600
    },
    {
      "epoch": 12.443426527483759,
      "grad_norm": 4.503490924835205,
      "learning_rate": 3.9630477893763535e-05,
      "loss": 0.6699,
      "step": 1363700
    },
    {
      "epoch": 12.444339002846924,
      "grad_norm": 3.7804694175720215,
      "learning_rate": 3.9629717497627565e-05,
      "loss": 0.671,
      "step": 1363800
    },
    {
      "epoch": 12.44525147821009,
      "grad_norm": 4.20053243637085,
      "learning_rate": 3.9628957101491595e-05,
      "loss": 0.6609,
      "step": 1363900
    },
    {
      "epoch": 12.446163953573253,
      "grad_norm": 4.08699369430542,
      "learning_rate": 3.9628196705355625e-05,
      "loss": 0.6947,
      "step": 1364000
    },
    {
      "epoch": 12.447076428936418,
      "grad_norm": 4.165218353271484,
      "learning_rate": 3.962743630921965e-05,
      "loss": 0.7287,
      "step": 1364100
    },
    {
      "epoch": 12.447988904299583,
      "grad_norm": 4.354282379150391,
      "learning_rate": 3.9626675913083685e-05,
      "loss": 0.6718,
      "step": 1364200
    },
    {
      "epoch": 12.448901379662749,
      "grad_norm": 3.792769432067871,
      "learning_rate": 3.962591551694771e-05,
      "loss": 0.7196,
      "step": 1364300
    },
    {
      "epoch": 12.449813855025914,
      "grad_norm": 4.189795017242432,
      "learning_rate": 3.962515512081174e-05,
      "loss": 0.6638,
      "step": 1364400
    },
    {
      "epoch": 12.45072633038908,
      "grad_norm": 4.870813369750977,
      "learning_rate": 3.962439472467577e-05,
      "loss": 0.6798,
      "step": 1364500
    },
    {
      "epoch": 12.451638805752244,
      "grad_norm": 4.086286544799805,
      "learning_rate": 3.96236343285398e-05,
      "loss": 0.659,
      "step": 1364600
    },
    {
      "epoch": 12.45255128111541,
      "grad_norm": 4.289510250091553,
      "learning_rate": 3.962287393240383e-05,
      "loss": 0.6792,
      "step": 1364700
    },
    {
      "epoch": 12.453463756478575,
      "grad_norm": 3.6746459007263184,
      "learning_rate": 3.962211353626786e-05,
      "loss": 0.6683,
      "step": 1364800
    },
    {
      "epoch": 12.45437623184174,
      "grad_norm": 2.9107825756073,
      "learning_rate": 3.962135314013188e-05,
      "loss": 0.6718,
      "step": 1364900
    },
    {
      "epoch": 12.455288707204906,
      "grad_norm": 3.876626491546631,
      "learning_rate": 3.962059274399592e-05,
      "loss": 0.6804,
      "step": 1365000
    },
    {
      "epoch": 12.456201182568071,
      "grad_norm": 4.3592448234558105,
      "learning_rate": 3.961983234785994e-05,
      "loss": 0.6881,
      "step": 1365100
    },
    {
      "epoch": 12.457113657931236,
      "grad_norm": 4.476804733276367,
      "learning_rate": 3.961907195172397e-05,
      "loss": 0.6721,
      "step": 1365200
    },
    {
      "epoch": 12.458026133294402,
      "grad_norm": 4.553752422332764,
      "learning_rate": 3.9618311555588e-05,
      "loss": 0.7083,
      "step": 1365300
    },
    {
      "epoch": 12.458938608657567,
      "grad_norm": 4.487802982330322,
      "learning_rate": 3.961755115945203e-05,
      "loss": 0.6608,
      "step": 1365400
    },
    {
      "epoch": 12.459851084020732,
      "grad_norm": 3.957181453704834,
      "learning_rate": 3.9616790763316055e-05,
      "loss": 0.6911,
      "step": 1365500
    },
    {
      "epoch": 12.460763559383897,
      "grad_norm": 3.654144763946533,
      "learning_rate": 3.961603036718009e-05,
      "loss": 0.6994,
      "step": 1365600
    },
    {
      "epoch": 12.461676034747061,
      "grad_norm": 3.7739644050598145,
      "learning_rate": 3.9615269971044116e-05,
      "loss": 0.6612,
      "step": 1365700
    },
    {
      "epoch": 12.462588510110226,
      "grad_norm": 4.656351089477539,
      "learning_rate": 3.9614509574908146e-05,
      "loss": 0.6771,
      "step": 1365800
    },
    {
      "epoch": 12.463500985473392,
      "grad_norm": 4.760759353637695,
      "learning_rate": 3.9613749178772176e-05,
      "loss": 0.7204,
      "step": 1365900
    },
    {
      "epoch": 12.464413460836557,
      "grad_norm": 3.7282023429870605,
      "learning_rate": 3.9612988782636206e-05,
      "loss": 0.7249,
      "step": 1366000
    },
    {
      "epoch": 12.465325936199722,
      "grad_norm": 3.094254493713379,
      "learning_rate": 3.9612228386500236e-05,
      "loss": 0.6517,
      "step": 1366100
    },
    {
      "epoch": 12.466238411562887,
      "grad_norm": 4.282639026641846,
      "learning_rate": 3.961146799036426e-05,
      "loss": 0.6582,
      "step": 1366200
    },
    {
      "epoch": 12.467150886926053,
      "grad_norm": 4.06898832321167,
      "learning_rate": 3.961070759422829e-05,
      "loss": 0.6519,
      "step": 1366300
    },
    {
      "epoch": 12.468063362289218,
      "grad_norm": 4.269717693328857,
      "learning_rate": 3.960994719809232e-05,
      "loss": 0.6782,
      "step": 1366400
    },
    {
      "epoch": 12.468975837652383,
      "grad_norm": 3.965686559677124,
      "learning_rate": 3.960918680195635e-05,
      "loss": 0.6882,
      "step": 1366500
    },
    {
      "epoch": 12.469888313015549,
      "grad_norm": 4.031063079833984,
      "learning_rate": 3.960842640582037e-05,
      "loss": 0.6824,
      "step": 1366600
    },
    {
      "epoch": 12.470800788378714,
      "grad_norm": 4.017075538635254,
      "learning_rate": 3.960766600968441e-05,
      "loss": 0.6626,
      "step": 1366700
    },
    {
      "epoch": 12.47171326374188,
      "grad_norm": 4.793610572814941,
      "learning_rate": 3.960690561354843e-05,
      "loss": 0.6596,
      "step": 1366800
    },
    {
      "epoch": 12.472625739105045,
      "grad_norm": 4.357312202453613,
      "learning_rate": 3.960614521741246e-05,
      "loss": 0.6764,
      "step": 1366900
    },
    {
      "epoch": 12.47353821446821,
      "grad_norm": 4.208858013153076,
      "learning_rate": 3.960538482127649e-05,
      "loss": 0.6912,
      "step": 1367000
    },
    {
      "epoch": 12.474450689831375,
      "grad_norm": 3.6652023792266846,
      "learning_rate": 3.960462442514052e-05,
      "loss": 0.6884,
      "step": 1367100
    },
    {
      "epoch": 12.47536316519454,
      "grad_norm": 4.116911888122559,
      "learning_rate": 3.960386402900455e-05,
      "loss": 0.7142,
      "step": 1367200
    },
    {
      "epoch": 12.476275640557706,
      "grad_norm": 3.9552927017211914,
      "learning_rate": 3.960310363286858e-05,
      "loss": 0.6844,
      "step": 1367300
    },
    {
      "epoch": 12.47718811592087,
      "grad_norm": 4.272430896759033,
      "learning_rate": 3.9602343236732606e-05,
      "loss": 0.6461,
      "step": 1367400
    },
    {
      "epoch": 12.478100591284035,
      "grad_norm": 4.525100231170654,
      "learning_rate": 3.960158284059664e-05,
      "loss": 0.6794,
      "step": 1367500
    },
    {
      "epoch": 12.4790130666472,
      "grad_norm": 3.9470596313476562,
      "learning_rate": 3.9600822444460667e-05,
      "loss": 0.6759,
      "step": 1367600
    },
    {
      "epoch": 12.479925542010365,
      "grad_norm": 3.4837207794189453,
      "learning_rate": 3.96000620483247e-05,
      "loss": 0.6811,
      "step": 1367700
    },
    {
      "epoch": 12.48083801737353,
      "grad_norm": 4.5905070304870605,
      "learning_rate": 3.959930165218873e-05,
      "loss": 0.726,
      "step": 1367800
    },
    {
      "epoch": 12.481750492736696,
      "grad_norm": 3.5519821643829346,
      "learning_rate": 3.959854125605276e-05,
      "loss": 0.6916,
      "step": 1367900
    },
    {
      "epoch": 12.482662968099861,
      "grad_norm": 4.392908096313477,
      "learning_rate": 3.959778085991678e-05,
      "loss": 0.6685,
      "step": 1368000
    },
    {
      "epoch": 12.483575443463026,
      "grad_norm": 3.7392852306365967,
      "learning_rate": 3.959702046378082e-05,
      "loss": 0.6867,
      "step": 1368100
    },
    {
      "epoch": 12.484487918826192,
      "grad_norm": 3.064218282699585,
      "learning_rate": 3.959626006764484e-05,
      "loss": 0.6392,
      "step": 1368200
    },
    {
      "epoch": 12.485400394189357,
      "grad_norm": 3.777968168258667,
      "learning_rate": 3.959549967150887e-05,
      "loss": 0.6446,
      "step": 1368300
    },
    {
      "epoch": 12.486312869552522,
      "grad_norm": 3.3677027225494385,
      "learning_rate": 3.95947392753729e-05,
      "loss": 0.6496,
      "step": 1368400
    },
    {
      "epoch": 12.487225344915688,
      "grad_norm": 3.187156915664673,
      "learning_rate": 3.959397887923693e-05,
      "loss": 0.6627,
      "step": 1368500
    },
    {
      "epoch": 12.488137820278853,
      "grad_norm": 3.9216575622558594,
      "learning_rate": 3.959321848310096e-05,
      "loss": 0.7261,
      "step": 1368600
    },
    {
      "epoch": 12.489050295642018,
      "grad_norm": 3.8937110900878906,
      "learning_rate": 3.959245808696499e-05,
      "loss": 0.6884,
      "step": 1368700
    },
    {
      "epoch": 12.489962771005183,
      "grad_norm": 3.113645553588867,
      "learning_rate": 3.9591697690829014e-05,
      "loss": 0.7112,
      "step": 1368800
    },
    {
      "epoch": 12.490875246368349,
      "grad_norm": 4.135040283203125,
      "learning_rate": 3.959093729469305e-05,
      "loss": 0.6496,
      "step": 1368900
    },
    {
      "epoch": 12.491787721731514,
      "grad_norm": 3.248267650604248,
      "learning_rate": 3.9590176898557074e-05,
      "loss": 0.6653,
      "step": 1369000
    },
    {
      "epoch": 12.492700197094678,
      "grad_norm": 3.6249473094940186,
      "learning_rate": 3.95894165024211e-05,
      "loss": 0.6954,
      "step": 1369100
    },
    {
      "epoch": 12.493612672457843,
      "grad_norm": 3.1024222373962402,
      "learning_rate": 3.9588656106285134e-05,
      "loss": 0.6307,
      "step": 1369200
    },
    {
      "epoch": 12.494525147821008,
      "grad_norm": 3.1086554527282715,
      "learning_rate": 3.958789571014916e-05,
      "loss": 0.6769,
      "step": 1369300
    },
    {
      "epoch": 12.495437623184174,
      "grad_norm": 3.401822805404663,
      "learning_rate": 3.958713531401319e-05,
      "loss": 0.6685,
      "step": 1369400
    },
    {
      "epoch": 12.496350098547339,
      "grad_norm": 3.483274221420288,
      "learning_rate": 3.958637491787722e-05,
      "loss": 0.7325,
      "step": 1369500
    },
    {
      "epoch": 12.497262573910504,
      "grad_norm": 4.409423351287842,
      "learning_rate": 3.958561452174125e-05,
      "loss": 0.6689,
      "step": 1369600
    },
    {
      "epoch": 12.49817504927367,
      "grad_norm": 4.483376502990723,
      "learning_rate": 3.958485412560528e-05,
      "loss": 0.6775,
      "step": 1369700
    },
    {
      "epoch": 12.499087524636835,
      "grad_norm": 4.135641098022461,
      "learning_rate": 3.958409372946931e-05,
      "loss": 0.6526,
      "step": 1369800
    },
    {
      "epoch": 12.5,
      "grad_norm": 3.842613935470581,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.6864,
      "step": 1369900
    },
    {
      "epoch": 12.500912475363165,
      "grad_norm": 3.9042365550994873,
      "learning_rate": 3.958257293719737e-05,
      "loss": 0.6512,
      "step": 1370000
    },
    {
      "epoch": 12.50182495072633,
      "grad_norm": 3.2096571922302246,
      "learning_rate": 3.958181254106139e-05,
      "loss": 0.668,
      "step": 1370100
    },
    {
      "epoch": 12.502737426089496,
      "grad_norm": 4.15022611618042,
      "learning_rate": 3.958105214492542e-05,
      "loss": 0.687,
      "step": 1370200
    },
    {
      "epoch": 12.503649901452661,
      "grad_norm": 3.302779197692871,
      "learning_rate": 3.958029174878945e-05,
      "loss": 0.6936,
      "step": 1370300
    },
    {
      "epoch": 12.504562376815826,
      "grad_norm": 3.4348812103271484,
      "learning_rate": 3.957953135265348e-05,
      "loss": 0.6717,
      "step": 1370400
    },
    {
      "epoch": 12.505474852178992,
      "grad_norm": 3.165379762649536,
      "learning_rate": 3.9578770956517505e-05,
      "loss": 0.6966,
      "step": 1370500
    },
    {
      "epoch": 12.506387327542157,
      "grad_norm": 4.280551433563232,
      "learning_rate": 3.957801056038154e-05,
      "loss": 0.6857,
      "step": 1370600
    },
    {
      "epoch": 12.507299802905322,
      "grad_norm": 3.5534536838531494,
      "learning_rate": 3.9577250164245565e-05,
      "loss": 0.6941,
      "step": 1370700
    },
    {
      "epoch": 12.508212278268488,
      "grad_norm": 3.6898269653320312,
      "learning_rate": 3.9576489768109595e-05,
      "loss": 0.7084,
      "step": 1370800
    },
    {
      "epoch": 12.509124753631651,
      "grad_norm": 3.4190924167633057,
      "learning_rate": 3.9575729371973625e-05,
      "loss": 0.6963,
      "step": 1370900
    },
    {
      "epoch": 12.510037228994817,
      "grad_norm": 4.146388530731201,
      "learning_rate": 3.9574968975837655e-05,
      "loss": 0.7216,
      "step": 1371000
    },
    {
      "epoch": 12.510949704357982,
      "grad_norm": 4.151220321655273,
      "learning_rate": 3.9574208579701685e-05,
      "loss": 0.6714,
      "step": 1371100
    },
    {
      "epoch": 12.511862179721147,
      "grad_norm": 3.2249083518981934,
      "learning_rate": 3.9573448183565715e-05,
      "loss": 0.6649,
      "step": 1371200
    },
    {
      "epoch": 12.512774655084312,
      "grad_norm": 3.8849480152130127,
      "learning_rate": 3.957268778742974e-05,
      "loss": 0.6807,
      "step": 1371300
    },
    {
      "epoch": 12.513687130447478,
      "grad_norm": 4.4151129722595215,
      "learning_rate": 3.9571927391293775e-05,
      "loss": 0.6964,
      "step": 1371400
    },
    {
      "epoch": 12.514599605810643,
      "grad_norm": 4.531284809112549,
      "learning_rate": 3.95711669951578e-05,
      "loss": 0.6725,
      "step": 1371500
    },
    {
      "epoch": 12.515512081173808,
      "grad_norm": 2.7208456993103027,
      "learning_rate": 3.957040659902183e-05,
      "loss": 0.6741,
      "step": 1371600
    },
    {
      "epoch": 12.516424556536974,
      "grad_norm": 3.4458985328674316,
      "learning_rate": 3.956964620288586e-05,
      "loss": 0.6831,
      "step": 1371700
    },
    {
      "epoch": 12.517337031900139,
      "grad_norm": 3.026216983795166,
      "learning_rate": 3.956888580674988e-05,
      "loss": 0.657,
      "step": 1371800
    },
    {
      "epoch": 12.518249507263304,
      "grad_norm": 4.216224193572998,
      "learning_rate": 3.956812541061392e-05,
      "loss": 0.6514,
      "step": 1371900
    },
    {
      "epoch": 12.51916198262647,
      "grad_norm": 3.4961462020874023,
      "learning_rate": 3.956736501447794e-05,
      "loss": 0.6713,
      "step": 1372000
    },
    {
      "epoch": 12.520074457989635,
      "grad_norm": 4.506140232086182,
      "learning_rate": 3.956660461834197e-05,
      "loss": 0.6769,
      "step": 1372100
    },
    {
      "epoch": 12.5209869333528,
      "grad_norm": 4.385555267333984,
      "learning_rate": 3.9565844222206e-05,
      "loss": 0.6743,
      "step": 1372200
    },
    {
      "epoch": 12.521899408715965,
      "grad_norm": 5.169951915740967,
      "learning_rate": 3.956508382607003e-05,
      "loss": 0.6737,
      "step": 1372300
    },
    {
      "epoch": 12.522811884079129,
      "grad_norm": 3.872429370880127,
      "learning_rate": 3.9564323429934056e-05,
      "loss": 0.6959,
      "step": 1372400
    },
    {
      "epoch": 12.523724359442294,
      "grad_norm": 4.5737128257751465,
      "learning_rate": 3.956356303379809e-05,
      "loss": 0.7064,
      "step": 1372500
    },
    {
      "epoch": 12.52463683480546,
      "grad_norm": 4.595102787017822,
      "learning_rate": 3.9562802637662116e-05,
      "loss": 0.6609,
      "step": 1372600
    },
    {
      "epoch": 12.525549310168625,
      "grad_norm": 3.9327399730682373,
      "learning_rate": 3.9562042241526146e-05,
      "loss": 0.717,
      "step": 1372700
    },
    {
      "epoch": 12.52646178553179,
      "grad_norm": 4.641706466674805,
      "learning_rate": 3.9561281845390176e-05,
      "loss": 0.6654,
      "step": 1372800
    },
    {
      "epoch": 12.527374260894955,
      "grad_norm": 4.402507305145264,
      "learning_rate": 3.9560521449254206e-05,
      "loss": 0.6842,
      "step": 1372900
    },
    {
      "epoch": 12.52828673625812,
      "grad_norm": 4.874622821807861,
      "learning_rate": 3.9559761053118236e-05,
      "loss": 0.7142,
      "step": 1373000
    },
    {
      "epoch": 12.529199211621286,
      "grad_norm": 3.4011380672454834,
      "learning_rate": 3.9559000656982266e-05,
      "loss": 0.6663,
      "step": 1373100
    },
    {
      "epoch": 12.530111686984451,
      "grad_norm": 4.161551475524902,
      "learning_rate": 3.955824026084629e-05,
      "loss": 0.6406,
      "step": 1373200
    },
    {
      "epoch": 12.531024162347617,
      "grad_norm": 4.259857654571533,
      "learning_rate": 3.9557479864710326e-05,
      "loss": 0.6857,
      "step": 1373300
    },
    {
      "epoch": 12.531936637710782,
      "grad_norm": 4.00197172164917,
      "learning_rate": 3.955671946857435e-05,
      "loss": 0.6631,
      "step": 1373400
    },
    {
      "epoch": 12.532849113073947,
      "grad_norm": 3.7416841983795166,
      "learning_rate": 3.955595907243838e-05,
      "loss": 0.6678,
      "step": 1373500
    },
    {
      "epoch": 12.533761588437113,
      "grad_norm": 3.8545210361480713,
      "learning_rate": 3.955519867630241e-05,
      "loss": 0.6557,
      "step": 1373600
    },
    {
      "epoch": 12.534674063800278,
      "grad_norm": 3.950232744216919,
      "learning_rate": 3.955443828016644e-05,
      "loss": 0.6541,
      "step": 1373700
    },
    {
      "epoch": 12.535586539163443,
      "grad_norm": 3.607150077819824,
      "learning_rate": 3.955367788403046e-05,
      "loss": 0.6996,
      "step": 1373800
    },
    {
      "epoch": 12.536499014526608,
      "grad_norm": 3.6078670024871826,
      "learning_rate": 3.95529174878945e-05,
      "loss": 0.653,
      "step": 1373900
    },
    {
      "epoch": 12.537411489889774,
      "grad_norm": 3.4405078887939453,
      "learning_rate": 3.955215709175852e-05,
      "loss": 0.6972,
      "step": 1374000
    },
    {
      "epoch": 12.538323965252939,
      "grad_norm": 4.105294704437256,
      "learning_rate": 3.955139669562255e-05,
      "loss": 0.6887,
      "step": 1374100
    },
    {
      "epoch": 12.539236440616103,
      "grad_norm": 3.5312726497650146,
      "learning_rate": 3.955063629948658e-05,
      "loss": 0.6944,
      "step": 1374200
    },
    {
      "epoch": 12.540148915979268,
      "grad_norm": 4.088874340057373,
      "learning_rate": 3.954987590335061e-05,
      "loss": 0.6453,
      "step": 1374300
    },
    {
      "epoch": 12.541061391342433,
      "grad_norm": 4.070346355438232,
      "learning_rate": 3.954911550721464e-05,
      "loss": 0.7057,
      "step": 1374400
    },
    {
      "epoch": 12.541973866705598,
      "grad_norm": 4.571031093597412,
      "learning_rate": 3.9548355111078673e-05,
      "loss": 0.6744,
      "step": 1374500
    },
    {
      "epoch": 12.542886342068764,
      "grad_norm": 4.060108661651611,
      "learning_rate": 3.95475947149427e-05,
      "loss": 0.653,
      "step": 1374600
    },
    {
      "epoch": 12.543798817431929,
      "grad_norm": 4.415556907653809,
      "learning_rate": 3.954683431880673e-05,
      "loss": 0.6589,
      "step": 1374700
    },
    {
      "epoch": 12.544711292795094,
      "grad_norm": 3.856027126312256,
      "learning_rate": 3.954607392267076e-05,
      "loss": 0.6745,
      "step": 1374800
    },
    {
      "epoch": 12.54562376815826,
      "grad_norm": 4.048215866088867,
      "learning_rate": 3.954531352653478e-05,
      "loss": 0.6665,
      "step": 1374900
    },
    {
      "epoch": 12.546536243521425,
      "grad_norm": 4.145839691162109,
      "learning_rate": 3.954455313039882e-05,
      "loss": 0.6834,
      "step": 1375000
    },
    {
      "epoch": 12.54744871888459,
      "grad_norm": 4.599294662475586,
      "learning_rate": 3.954379273426284e-05,
      "loss": 0.6468,
      "step": 1375100
    },
    {
      "epoch": 12.548361194247756,
      "grad_norm": 3.9662020206451416,
      "learning_rate": 3.954303233812687e-05,
      "loss": 0.6485,
      "step": 1375200
    },
    {
      "epoch": 12.54927366961092,
      "grad_norm": 3.8436567783355713,
      "learning_rate": 3.95422719419909e-05,
      "loss": 0.7028,
      "step": 1375300
    },
    {
      "epoch": 12.550186144974086,
      "grad_norm": 3.7515804767608643,
      "learning_rate": 3.954151154585493e-05,
      "loss": 0.6527,
      "step": 1375400
    },
    {
      "epoch": 12.551098620337251,
      "grad_norm": 4.404698371887207,
      "learning_rate": 3.954075114971896e-05,
      "loss": 0.6594,
      "step": 1375500
    },
    {
      "epoch": 12.552011095700417,
      "grad_norm": 4.778217315673828,
      "learning_rate": 3.953999075358299e-05,
      "loss": 0.7076,
      "step": 1375600
    },
    {
      "epoch": 12.552923571063582,
      "grad_norm": 3.968437433242798,
      "learning_rate": 3.9539230357447014e-05,
      "loss": 0.6721,
      "step": 1375700
    },
    {
      "epoch": 12.553836046426746,
      "grad_norm": 4.631938934326172,
      "learning_rate": 3.953846996131105e-05,
      "loss": 0.6637,
      "step": 1375800
    },
    {
      "epoch": 12.55474852178991,
      "grad_norm": 3.5348124504089355,
      "learning_rate": 3.9537709565175074e-05,
      "loss": 0.6829,
      "step": 1375900
    },
    {
      "epoch": 12.555660997153076,
      "grad_norm": 4.41452169418335,
      "learning_rate": 3.9536949169039104e-05,
      "loss": 0.6787,
      "step": 1376000
    },
    {
      "epoch": 12.556573472516241,
      "grad_norm": 4.040842533111572,
      "learning_rate": 3.9536188772903134e-05,
      "loss": 0.6526,
      "step": 1376100
    },
    {
      "epoch": 12.557485947879407,
      "grad_norm": 4.726688385009766,
      "learning_rate": 3.9535428376767164e-05,
      "loss": 0.647,
      "step": 1376200
    },
    {
      "epoch": 12.558398423242572,
      "grad_norm": 4.409762859344482,
      "learning_rate": 3.953466798063119e-05,
      "loss": 0.6475,
      "step": 1376300
    },
    {
      "epoch": 12.559310898605737,
      "grad_norm": 4.074401378631592,
      "learning_rate": 3.9533907584495224e-05,
      "loss": 0.6499,
      "step": 1376400
    },
    {
      "epoch": 12.560223373968903,
      "grad_norm": 4.077742576599121,
      "learning_rate": 3.953314718835925e-05,
      "loss": 0.6639,
      "step": 1376500
    },
    {
      "epoch": 12.561135849332068,
      "grad_norm": 4.709282875061035,
      "learning_rate": 3.953238679222328e-05,
      "loss": 0.6461,
      "step": 1376600
    },
    {
      "epoch": 12.562048324695233,
      "grad_norm": 4.434901237487793,
      "learning_rate": 3.953162639608731e-05,
      "loss": 0.6957,
      "step": 1376700
    },
    {
      "epoch": 12.562960800058399,
      "grad_norm": 3.267476797103882,
      "learning_rate": 3.953086599995134e-05,
      "loss": 0.659,
      "step": 1376800
    },
    {
      "epoch": 12.563873275421564,
      "grad_norm": 4.03880500793457,
      "learning_rate": 3.953010560381537e-05,
      "loss": 0.6907,
      "step": 1376900
    },
    {
      "epoch": 12.56478575078473,
      "grad_norm": 3.0338356494903564,
      "learning_rate": 3.95293452076794e-05,
      "loss": 0.6845,
      "step": 1377000
    },
    {
      "epoch": 12.565698226147894,
      "grad_norm": 3.8885579109191895,
      "learning_rate": 3.952858481154342e-05,
      "loss": 0.6315,
      "step": 1377100
    },
    {
      "epoch": 12.56661070151106,
      "grad_norm": 3.6268484592437744,
      "learning_rate": 3.952782441540746e-05,
      "loss": 0.6905,
      "step": 1377200
    },
    {
      "epoch": 12.567523176874225,
      "grad_norm": 4.380931854248047,
      "learning_rate": 3.952706401927148e-05,
      "loss": 0.7005,
      "step": 1377300
    },
    {
      "epoch": 12.56843565223739,
      "grad_norm": 3.4697484970092773,
      "learning_rate": 3.952630362313551e-05,
      "loss": 0.6801,
      "step": 1377400
    },
    {
      "epoch": 12.569348127600556,
      "grad_norm": 3.8094611167907715,
      "learning_rate": 3.952554322699954e-05,
      "loss": 0.6483,
      "step": 1377500
    },
    {
      "epoch": 12.57026060296372,
      "grad_norm": 3.2984704971313477,
      "learning_rate": 3.9524782830863565e-05,
      "loss": 0.6824,
      "step": 1377600
    },
    {
      "epoch": 12.571173078326884,
      "grad_norm": 4.0810370445251465,
      "learning_rate": 3.9524022434727595e-05,
      "loss": 0.6675,
      "step": 1377700
    },
    {
      "epoch": 12.57208555369005,
      "grad_norm": 4.096947193145752,
      "learning_rate": 3.9523262038591625e-05,
      "loss": 0.7148,
      "step": 1377800
    },
    {
      "epoch": 12.572998029053215,
      "grad_norm": 3.512960195541382,
      "learning_rate": 3.9522501642455655e-05,
      "loss": 0.6615,
      "step": 1377900
    },
    {
      "epoch": 12.57391050441638,
      "grad_norm": 3.8879570960998535,
      "learning_rate": 3.9521741246319685e-05,
      "loss": 0.6916,
      "step": 1378000
    },
    {
      "epoch": 12.574822979779546,
      "grad_norm": 3.51520037651062,
      "learning_rate": 3.9520980850183715e-05,
      "loss": 0.647,
      "step": 1378100
    },
    {
      "epoch": 12.575735455142711,
      "grad_norm": 3.5086631774902344,
      "learning_rate": 3.952022045404774e-05,
      "loss": 0.6676,
      "step": 1378200
    },
    {
      "epoch": 12.576647930505876,
      "grad_norm": 4.9802656173706055,
      "learning_rate": 3.9519460057911775e-05,
      "loss": 0.6899,
      "step": 1378300
    },
    {
      "epoch": 12.577560405869042,
      "grad_norm": 3.841740608215332,
      "learning_rate": 3.95186996617758e-05,
      "loss": 0.6779,
      "step": 1378400
    },
    {
      "epoch": 12.578472881232207,
      "grad_norm": 3.8695340156555176,
      "learning_rate": 3.951793926563983e-05,
      "loss": 0.6525,
      "step": 1378500
    },
    {
      "epoch": 12.579385356595372,
      "grad_norm": 3.9944307804107666,
      "learning_rate": 3.951717886950386e-05,
      "loss": 0.6991,
      "step": 1378600
    },
    {
      "epoch": 12.580297831958537,
      "grad_norm": 4.506699562072754,
      "learning_rate": 3.951641847336789e-05,
      "loss": 0.6798,
      "step": 1378700
    },
    {
      "epoch": 12.581210307321703,
      "grad_norm": 4.392820835113525,
      "learning_rate": 3.951565807723191e-05,
      "loss": 0.6941,
      "step": 1378800
    },
    {
      "epoch": 12.582122782684868,
      "grad_norm": 3.7245914936065674,
      "learning_rate": 3.951489768109595e-05,
      "loss": 0.6313,
      "step": 1378900
    },
    {
      "epoch": 12.583035258048033,
      "grad_norm": 3.853269338607788,
      "learning_rate": 3.951413728495997e-05,
      "loss": 0.6956,
      "step": 1379000
    },
    {
      "epoch": 12.583947733411199,
      "grad_norm": 4.193112850189209,
      "learning_rate": 3.9513376888824e-05,
      "loss": 0.7583,
      "step": 1379100
    },
    {
      "epoch": 12.584860208774362,
      "grad_norm": 5.189449310302734,
      "learning_rate": 3.951261649268803e-05,
      "loss": 0.6672,
      "step": 1379200
    },
    {
      "epoch": 12.585772684137527,
      "grad_norm": 3.8446083068847656,
      "learning_rate": 3.951185609655206e-05,
      "loss": 0.6872,
      "step": 1379300
    },
    {
      "epoch": 12.586685159500693,
      "grad_norm": 3.2689621448516846,
      "learning_rate": 3.951109570041609e-05,
      "loss": 0.6773,
      "step": 1379400
    },
    {
      "epoch": 12.587597634863858,
      "grad_norm": 3.269200325012207,
      "learning_rate": 3.951033530428012e-05,
      "loss": 0.6861,
      "step": 1379500
    },
    {
      "epoch": 12.588510110227023,
      "grad_norm": 3.7087535858154297,
      "learning_rate": 3.9509574908144146e-05,
      "loss": 0.6663,
      "step": 1379600
    },
    {
      "epoch": 12.589422585590189,
      "grad_norm": 3.7914490699768066,
      "learning_rate": 3.950881451200818e-05,
      "loss": 0.679,
      "step": 1379700
    },
    {
      "epoch": 12.590335060953354,
      "grad_norm": 3.9052939414978027,
      "learning_rate": 3.9508054115872206e-05,
      "loss": 0.6643,
      "step": 1379800
    },
    {
      "epoch": 12.59124753631652,
      "grad_norm": 4.267569065093994,
      "learning_rate": 3.9507293719736236e-05,
      "loss": 0.6986,
      "step": 1379900
    },
    {
      "epoch": 12.592160011679685,
      "grad_norm": 3.2305307388305664,
      "learning_rate": 3.9506533323600266e-05,
      "loss": 0.7051,
      "step": 1380000
    },
    {
      "epoch": 12.59307248704285,
      "grad_norm": 4.223461627960205,
      "learning_rate": 3.9505772927464296e-05,
      "loss": 0.6719,
      "step": 1380100
    },
    {
      "epoch": 12.593984962406015,
      "grad_norm": 3.5543808937072754,
      "learning_rate": 3.950501253132832e-05,
      "loss": 0.6545,
      "step": 1380200
    },
    {
      "epoch": 12.59489743776918,
      "grad_norm": 4.981037616729736,
      "learning_rate": 3.9504252135192356e-05,
      "loss": 0.6694,
      "step": 1380300
    },
    {
      "epoch": 12.595809913132346,
      "grad_norm": 3.762863874435425,
      "learning_rate": 3.950349173905638e-05,
      "loss": 0.685,
      "step": 1380400
    },
    {
      "epoch": 12.596722388495511,
      "grad_norm": 4.329459190368652,
      "learning_rate": 3.950273134292041e-05,
      "loss": 0.7055,
      "step": 1380500
    },
    {
      "epoch": 12.597634863858676,
      "grad_norm": 4.335184574127197,
      "learning_rate": 3.950197094678444e-05,
      "loss": 0.6627,
      "step": 1380600
    },
    {
      "epoch": 12.598547339221842,
      "grad_norm": 3.8282036781311035,
      "learning_rate": 3.950121055064846e-05,
      "loss": 0.6441,
      "step": 1380700
    },
    {
      "epoch": 12.599459814585007,
      "grad_norm": 4.780623435974121,
      "learning_rate": 3.95004501545125e-05,
      "loss": 0.6965,
      "step": 1380800
    },
    {
      "epoch": 12.600372289948172,
      "grad_norm": 4.0001068115234375,
      "learning_rate": 3.949968975837652e-05,
      "loss": 0.6669,
      "step": 1380900
    },
    {
      "epoch": 12.601284765311336,
      "grad_norm": 3.9079031944274902,
      "learning_rate": 3.949892936224055e-05,
      "loss": 0.6635,
      "step": 1381000
    },
    {
      "epoch": 12.602197240674501,
      "grad_norm": 4.978888988494873,
      "learning_rate": 3.949816896610458e-05,
      "loss": 0.6819,
      "step": 1381100
    },
    {
      "epoch": 12.603109716037666,
      "grad_norm": 4.253796577453613,
      "learning_rate": 3.9497408569968613e-05,
      "loss": 0.6815,
      "step": 1381200
    },
    {
      "epoch": 12.604022191400832,
      "grad_norm": 4.082173824310303,
      "learning_rate": 3.949664817383264e-05,
      "loss": 0.6559,
      "step": 1381300
    },
    {
      "epoch": 12.604934666763997,
      "grad_norm": 4.757795333862305,
      "learning_rate": 3.9495887777696674e-05,
      "loss": 0.6831,
      "step": 1381400
    },
    {
      "epoch": 12.605847142127162,
      "grad_norm": 4.305729389190674,
      "learning_rate": 3.94951273815607e-05,
      "loss": 0.6726,
      "step": 1381500
    },
    {
      "epoch": 12.606759617490328,
      "grad_norm": 3.9062399864196777,
      "learning_rate": 3.949436698542473e-05,
      "loss": 0.689,
      "step": 1381600
    },
    {
      "epoch": 12.607672092853493,
      "grad_norm": 4.731476306915283,
      "learning_rate": 3.949360658928876e-05,
      "loss": 0.6496,
      "step": 1381700
    },
    {
      "epoch": 12.608584568216658,
      "grad_norm": 3.7613472938537598,
      "learning_rate": 3.949284619315279e-05,
      "loss": 0.6693,
      "step": 1381800
    },
    {
      "epoch": 12.609497043579823,
      "grad_norm": 4.032171249389648,
      "learning_rate": 3.949208579701682e-05,
      "loss": 0.6666,
      "step": 1381900
    },
    {
      "epoch": 12.610409518942989,
      "grad_norm": 2.803159713745117,
      "learning_rate": 3.949132540088085e-05,
      "loss": 0.6711,
      "step": 1382000
    },
    {
      "epoch": 12.611321994306154,
      "grad_norm": 2.3560590744018555,
      "learning_rate": 3.949056500474487e-05,
      "loss": 0.7079,
      "step": 1382100
    },
    {
      "epoch": 12.61223446966932,
      "grad_norm": 3.7349941730499268,
      "learning_rate": 3.948980460860891e-05,
      "loss": 0.6373,
      "step": 1382200
    },
    {
      "epoch": 12.613146945032485,
      "grad_norm": 4.428465843200684,
      "learning_rate": 3.948904421247293e-05,
      "loss": 0.712,
      "step": 1382300
    },
    {
      "epoch": 12.61405942039565,
      "grad_norm": 4.261383056640625,
      "learning_rate": 3.948828381633696e-05,
      "loss": 0.6681,
      "step": 1382400
    },
    {
      "epoch": 12.614971895758815,
      "grad_norm": 4.049922943115234,
      "learning_rate": 3.948752342020099e-05,
      "loss": 0.6751,
      "step": 1382500
    },
    {
      "epoch": 12.615884371121979,
      "grad_norm": 4.545138835906982,
      "learning_rate": 3.948676302406502e-05,
      "loss": 0.6744,
      "step": 1382600
    },
    {
      "epoch": 12.616796846485144,
      "grad_norm": 4.779354572296143,
      "learning_rate": 3.9486002627929044e-05,
      "loss": 0.6853,
      "step": 1382700
    },
    {
      "epoch": 12.61770932184831,
      "grad_norm": 3.5320639610290527,
      "learning_rate": 3.948524223179308e-05,
      "loss": 0.6917,
      "step": 1382800
    },
    {
      "epoch": 12.618621797211475,
      "grad_norm": 4.497912406921387,
      "learning_rate": 3.9484481835657104e-05,
      "loss": 0.699,
      "step": 1382900
    },
    {
      "epoch": 12.61953427257464,
      "grad_norm": 4.544314861297607,
      "learning_rate": 3.9483721439521134e-05,
      "loss": 0.6478,
      "step": 1383000
    },
    {
      "epoch": 12.620446747937805,
      "grad_norm": 2.817065715789795,
      "learning_rate": 3.9482961043385164e-05,
      "loss": 0.6768,
      "step": 1383100
    },
    {
      "epoch": 12.62135922330097,
      "grad_norm": 4.314816951751709,
      "learning_rate": 3.948220064724919e-05,
      "loss": 0.6723,
      "step": 1383200
    },
    {
      "epoch": 12.622271698664136,
      "grad_norm": 3.61030650138855,
      "learning_rate": 3.9481440251113224e-05,
      "loss": 0.6438,
      "step": 1383300
    },
    {
      "epoch": 12.623184174027301,
      "grad_norm": 3.423509120941162,
      "learning_rate": 3.948067985497725e-05,
      "loss": 0.6944,
      "step": 1383400
    },
    {
      "epoch": 12.624096649390466,
      "grad_norm": 4.533819675445557,
      "learning_rate": 3.947991945884128e-05,
      "loss": 0.6876,
      "step": 1383500
    },
    {
      "epoch": 12.625009124753632,
      "grad_norm": 3.8661086559295654,
      "learning_rate": 3.947915906270531e-05,
      "loss": 0.6397,
      "step": 1383600
    },
    {
      "epoch": 12.625921600116797,
      "grad_norm": 3.318070888519287,
      "learning_rate": 3.947839866656934e-05,
      "loss": 0.6633,
      "step": 1383700
    },
    {
      "epoch": 12.626834075479962,
      "grad_norm": 3.778153419494629,
      "learning_rate": 3.947763827043337e-05,
      "loss": 0.6396,
      "step": 1383800
    },
    {
      "epoch": 12.627746550843128,
      "grad_norm": 3.4646592140197754,
      "learning_rate": 3.94768778742974e-05,
      "loss": 0.7009,
      "step": 1383900
    },
    {
      "epoch": 12.628659026206293,
      "grad_norm": 5.637784481048584,
      "learning_rate": 3.947611747816142e-05,
      "loss": 0.6632,
      "step": 1384000
    },
    {
      "epoch": 12.629571501569458,
      "grad_norm": 4.225871562957764,
      "learning_rate": 3.947535708202545e-05,
      "loss": 0.6853,
      "step": 1384100
    },
    {
      "epoch": 12.630483976932624,
      "grad_norm": 4.71577262878418,
      "learning_rate": 3.947459668588948e-05,
      "loss": 0.6868,
      "step": 1384200
    },
    {
      "epoch": 12.631396452295789,
      "grad_norm": 3.7637927532196045,
      "learning_rate": 3.947383628975351e-05,
      "loss": 0.6805,
      "step": 1384300
    },
    {
      "epoch": 12.632308927658952,
      "grad_norm": 2.995227813720703,
      "learning_rate": 3.947307589361754e-05,
      "loss": 0.6795,
      "step": 1384400
    },
    {
      "epoch": 12.633221403022118,
      "grad_norm": 3.6125807762145996,
      "learning_rate": 3.947231549748157e-05,
      "loss": 0.6442,
      "step": 1384500
    },
    {
      "epoch": 12.634133878385283,
      "grad_norm": 4.488558769226074,
      "learning_rate": 3.9471555101345595e-05,
      "loss": 0.698,
      "step": 1384600
    },
    {
      "epoch": 12.635046353748448,
      "grad_norm": 4.281418323516846,
      "learning_rate": 3.947079470520963e-05,
      "loss": 0.6572,
      "step": 1384700
    },
    {
      "epoch": 12.635958829111614,
      "grad_norm": 3.9696857929229736,
      "learning_rate": 3.9470034309073655e-05,
      "loss": 0.6704,
      "step": 1384800
    },
    {
      "epoch": 12.636871304474779,
      "grad_norm": 4.177383899688721,
      "learning_rate": 3.9469273912937685e-05,
      "loss": 0.6833,
      "step": 1384900
    },
    {
      "epoch": 12.637783779837944,
      "grad_norm": 3.182621479034424,
      "learning_rate": 3.9468513516801715e-05,
      "loss": 0.6915,
      "step": 1385000
    },
    {
      "epoch": 12.63869625520111,
      "grad_norm": 5.029366970062256,
      "learning_rate": 3.9467753120665745e-05,
      "loss": 0.725,
      "step": 1385100
    },
    {
      "epoch": 12.639608730564275,
      "grad_norm": 4.166762828826904,
      "learning_rate": 3.9466992724529775e-05,
      "loss": 0.6529,
      "step": 1385200
    },
    {
      "epoch": 12.64052120592744,
      "grad_norm": 3.339796304702759,
      "learning_rate": 3.9466232328393805e-05,
      "loss": 0.6648,
      "step": 1385300
    },
    {
      "epoch": 12.641433681290605,
      "grad_norm": 4.640089988708496,
      "learning_rate": 3.946547193225783e-05,
      "loss": 0.7179,
      "step": 1385400
    },
    {
      "epoch": 12.64234615665377,
      "grad_norm": 3.948929786682129,
      "learning_rate": 3.946471153612186e-05,
      "loss": 0.6771,
      "step": 1385500
    },
    {
      "epoch": 12.643258632016936,
      "grad_norm": 4.494457244873047,
      "learning_rate": 3.946395113998589e-05,
      "loss": 0.6736,
      "step": 1385600
    },
    {
      "epoch": 12.644171107380101,
      "grad_norm": 3.571401357650757,
      "learning_rate": 3.946319074384992e-05,
      "loss": 0.6636,
      "step": 1385700
    },
    {
      "epoch": 12.645083582743267,
      "grad_norm": 4.837348461151123,
      "learning_rate": 3.946243034771395e-05,
      "loss": 0.699,
      "step": 1385800
    },
    {
      "epoch": 12.645996058106432,
      "grad_norm": 3.9069414138793945,
      "learning_rate": 3.946166995157798e-05,
      "loss": 0.6628,
      "step": 1385900
    },
    {
      "epoch": 12.646908533469595,
      "grad_norm": 4.660747051239014,
      "learning_rate": 3.9460909555442e-05,
      "loss": 0.6494,
      "step": 1386000
    },
    {
      "epoch": 12.64782100883276,
      "grad_norm": 4.283557415008545,
      "learning_rate": 3.946014915930603e-05,
      "loss": 0.6444,
      "step": 1386100
    },
    {
      "epoch": 12.648733484195926,
      "grad_norm": 3.8836050033569336,
      "learning_rate": 3.945938876317006e-05,
      "loss": 0.6677,
      "step": 1386200
    },
    {
      "epoch": 12.649645959559091,
      "grad_norm": 3.4146018028259277,
      "learning_rate": 3.945862836703409e-05,
      "loss": 0.7012,
      "step": 1386300
    },
    {
      "epoch": 12.650558434922257,
      "grad_norm": 3.619007110595703,
      "learning_rate": 3.945786797089812e-05,
      "loss": 0.6807,
      "step": 1386400
    },
    {
      "epoch": 12.651470910285422,
      "grad_norm": 3.424252986907959,
      "learning_rate": 3.9457107574762146e-05,
      "loss": 0.663,
      "step": 1386500
    },
    {
      "epoch": 12.652383385648587,
      "grad_norm": 3.6058850288391113,
      "learning_rate": 3.945634717862618e-05,
      "loss": 0.6738,
      "step": 1386600
    },
    {
      "epoch": 12.653295861011753,
      "grad_norm": 3.7977490425109863,
      "learning_rate": 3.9455586782490206e-05,
      "loss": 0.6775,
      "step": 1386700
    },
    {
      "epoch": 12.654208336374918,
      "grad_norm": 4.989726543426514,
      "learning_rate": 3.9454826386354236e-05,
      "loss": 0.6951,
      "step": 1386800
    },
    {
      "epoch": 12.655120811738083,
      "grad_norm": 4.379293441772461,
      "learning_rate": 3.9454065990218266e-05,
      "loss": 0.6536,
      "step": 1386900
    },
    {
      "epoch": 12.656033287101248,
      "grad_norm": 2.6764609813690186,
      "learning_rate": 3.9453305594082296e-05,
      "loss": 0.6844,
      "step": 1387000
    },
    {
      "epoch": 12.656945762464414,
      "grad_norm": 4.800755500793457,
      "learning_rate": 3.945254519794632e-05,
      "loss": 0.6784,
      "step": 1387100
    },
    {
      "epoch": 12.657858237827579,
      "grad_norm": 3.3900716304779053,
      "learning_rate": 3.9451784801810356e-05,
      "loss": 0.6679,
      "step": 1387200
    },
    {
      "epoch": 12.658770713190744,
      "grad_norm": 4.495978355407715,
      "learning_rate": 3.945102440567438e-05,
      "loss": 0.6947,
      "step": 1387300
    },
    {
      "epoch": 12.65968318855391,
      "grad_norm": 4.507486820220947,
      "learning_rate": 3.945026400953841e-05,
      "loss": 0.6939,
      "step": 1387400
    },
    {
      "epoch": 12.660595663917075,
      "grad_norm": 4.624179840087891,
      "learning_rate": 3.944950361340244e-05,
      "loss": 0.6995,
      "step": 1387500
    },
    {
      "epoch": 12.66150813928024,
      "grad_norm": 3.511132001876831,
      "learning_rate": 3.944874321726647e-05,
      "loss": 0.6741,
      "step": 1387600
    },
    {
      "epoch": 12.662420614643406,
      "grad_norm": 3.9606244564056396,
      "learning_rate": 3.94479828211305e-05,
      "loss": 0.6645,
      "step": 1387700
    },
    {
      "epoch": 12.663333090006569,
      "grad_norm": 4.14075231552124,
      "learning_rate": 3.944722242499453e-05,
      "loss": 0.6547,
      "step": 1387800
    },
    {
      "epoch": 12.664245565369734,
      "grad_norm": 3.7390120029449463,
      "learning_rate": 3.944646202885855e-05,
      "loss": 0.6593,
      "step": 1387900
    },
    {
      "epoch": 12.6651580407329,
      "grad_norm": 4.082788467407227,
      "learning_rate": 3.944570163272259e-05,
      "loss": 0.6574,
      "step": 1388000
    },
    {
      "epoch": 12.666070516096065,
      "grad_norm": 4.1709184646606445,
      "learning_rate": 3.9444941236586613e-05,
      "loss": 0.6589,
      "step": 1388100
    },
    {
      "epoch": 12.66698299145923,
      "grad_norm": 4.302464008331299,
      "learning_rate": 3.9444180840450644e-05,
      "loss": 0.6904,
      "step": 1388200
    },
    {
      "epoch": 12.667895466822396,
      "grad_norm": 3.871884822845459,
      "learning_rate": 3.9443420444314674e-05,
      "loss": 0.6487,
      "step": 1388300
    },
    {
      "epoch": 12.66880794218556,
      "grad_norm": 3.7277274131774902,
      "learning_rate": 3.9442660048178704e-05,
      "loss": 0.6878,
      "step": 1388400
    },
    {
      "epoch": 12.669720417548726,
      "grad_norm": 4.118287086486816,
      "learning_rate": 3.944189965204273e-05,
      "loss": 0.6945,
      "step": 1388500
    },
    {
      "epoch": 12.670632892911891,
      "grad_norm": 3.7161664962768555,
      "learning_rate": 3.9441139255906764e-05,
      "loss": 0.7133,
      "step": 1388600
    },
    {
      "epoch": 12.671545368275057,
      "grad_norm": 4.1819305419921875,
      "learning_rate": 3.944037885977079e-05,
      "loss": 0.6913,
      "step": 1388700
    },
    {
      "epoch": 12.672457843638222,
      "grad_norm": 3.57894229888916,
      "learning_rate": 3.943961846363482e-05,
      "loss": 0.6572,
      "step": 1388800
    },
    {
      "epoch": 12.673370319001387,
      "grad_norm": 4.4010138511657715,
      "learning_rate": 3.943885806749885e-05,
      "loss": 0.6303,
      "step": 1388900
    },
    {
      "epoch": 12.674282794364553,
      "grad_norm": 4.489017486572266,
      "learning_rate": 3.943809767136287e-05,
      "loss": 0.6809,
      "step": 1389000
    },
    {
      "epoch": 12.675195269727718,
      "grad_norm": 4.043715953826904,
      "learning_rate": 3.943733727522691e-05,
      "loss": 0.6392,
      "step": 1389100
    },
    {
      "epoch": 12.676107745090883,
      "grad_norm": 3.3749682903289795,
      "learning_rate": 3.943657687909093e-05,
      "loss": 0.6783,
      "step": 1389200
    },
    {
      "epoch": 12.677020220454049,
      "grad_norm": 3.914248466491699,
      "learning_rate": 3.943581648295496e-05,
      "loss": 0.6697,
      "step": 1389300
    },
    {
      "epoch": 12.677932695817212,
      "grad_norm": 3.91511607170105,
      "learning_rate": 3.943505608681899e-05,
      "loss": 0.6764,
      "step": 1389400
    },
    {
      "epoch": 12.678845171180377,
      "grad_norm": 3.7894649505615234,
      "learning_rate": 3.943429569068302e-05,
      "loss": 0.6733,
      "step": 1389500
    },
    {
      "epoch": 12.679757646543543,
      "grad_norm": 4.24914026260376,
      "learning_rate": 3.9433535294547044e-05,
      "loss": 0.6578,
      "step": 1389600
    },
    {
      "epoch": 12.680670121906708,
      "grad_norm": 4.359776973724365,
      "learning_rate": 3.943277489841108e-05,
      "loss": 0.6306,
      "step": 1389700
    },
    {
      "epoch": 12.681582597269873,
      "grad_norm": 3.8157029151916504,
      "learning_rate": 3.9432014502275104e-05,
      "loss": 0.7063,
      "step": 1389800
    },
    {
      "epoch": 12.682495072633039,
      "grad_norm": 4.280022144317627,
      "learning_rate": 3.9431254106139134e-05,
      "loss": 0.6986,
      "step": 1389900
    },
    {
      "epoch": 12.683407547996204,
      "grad_norm": 5.482882976531982,
      "learning_rate": 3.9430493710003164e-05,
      "loss": 0.6825,
      "step": 1390000
    },
    {
      "epoch": 12.68432002335937,
      "grad_norm": 3.913850784301758,
      "learning_rate": 3.9429733313867195e-05,
      "loss": 0.7002,
      "step": 1390100
    },
    {
      "epoch": 12.685232498722534,
      "grad_norm": 3.5142011642456055,
      "learning_rate": 3.9428972917731225e-05,
      "loss": 0.6927,
      "step": 1390200
    },
    {
      "epoch": 12.6861449740857,
      "grad_norm": 2.2062392234802246,
      "learning_rate": 3.9428212521595255e-05,
      "loss": 0.6841,
      "step": 1390300
    },
    {
      "epoch": 12.687057449448865,
      "grad_norm": 2.0543105602264404,
      "learning_rate": 3.942745212545928e-05,
      "loss": 0.6555,
      "step": 1390400
    },
    {
      "epoch": 12.68796992481203,
      "grad_norm": 2.417038917541504,
      "learning_rate": 3.9426691729323315e-05,
      "loss": 0.6693,
      "step": 1390500
    },
    {
      "epoch": 12.688882400175196,
      "grad_norm": 3.3411030769348145,
      "learning_rate": 3.942593133318734e-05,
      "loss": 0.67,
      "step": 1390600
    },
    {
      "epoch": 12.689794875538361,
      "grad_norm": 3.548006772994995,
      "learning_rate": 3.942517093705137e-05,
      "loss": 0.6678,
      "step": 1390700
    },
    {
      "epoch": 12.690707350901526,
      "grad_norm": 4.034696578979492,
      "learning_rate": 3.94244105409154e-05,
      "loss": 0.6814,
      "step": 1390800
    },
    {
      "epoch": 12.691619826264692,
      "grad_norm": 3.457446575164795,
      "learning_rate": 3.942365014477943e-05,
      "loss": 0.6639,
      "step": 1390900
    },
    {
      "epoch": 12.692532301627857,
      "grad_norm": 4.303843975067139,
      "learning_rate": 3.942288974864345e-05,
      "loss": 0.6034,
      "step": 1391000
    },
    {
      "epoch": 12.693444776991022,
      "grad_norm": 3.6925840377807617,
      "learning_rate": 3.942212935250749e-05,
      "loss": 0.6787,
      "step": 1391100
    },
    {
      "epoch": 12.694357252354186,
      "grad_norm": 4.23948860168457,
      "learning_rate": 3.942136895637151e-05,
      "loss": 0.7041,
      "step": 1391200
    },
    {
      "epoch": 12.695269727717351,
      "grad_norm": 4.180072784423828,
      "learning_rate": 3.942060856023554e-05,
      "loss": 0.6843,
      "step": 1391300
    },
    {
      "epoch": 12.696182203080516,
      "grad_norm": 3.7794108390808105,
      "learning_rate": 3.941984816409957e-05,
      "loss": 0.6925,
      "step": 1391400
    },
    {
      "epoch": 12.697094678443682,
      "grad_norm": 4.722840785980225,
      "learning_rate": 3.94190877679636e-05,
      "loss": 0.6636,
      "step": 1391500
    },
    {
      "epoch": 12.698007153806847,
      "grad_norm": 4.8653998374938965,
      "learning_rate": 3.941832737182763e-05,
      "loss": 0.6565,
      "step": 1391600
    },
    {
      "epoch": 12.698919629170012,
      "grad_norm": 3.910134792327881,
      "learning_rate": 3.9417566975691655e-05,
      "loss": 0.6763,
      "step": 1391700
    },
    {
      "epoch": 12.699832104533177,
      "grad_norm": 4.572270393371582,
      "learning_rate": 3.9416806579555685e-05,
      "loss": 0.687,
      "step": 1391800
    },
    {
      "epoch": 12.700744579896343,
      "grad_norm": 3.2642533779144287,
      "learning_rate": 3.9416046183419715e-05,
      "loss": 0.6834,
      "step": 1391900
    },
    {
      "epoch": 12.701657055259508,
      "grad_norm": 4.541407585144043,
      "learning_rate": 3.9415285787283745e-05,
      "loss": 0.678,
      "step": 1392000
    },
    {
      "epoch": 12.702569530622673,
      "grad_norm": 3.827336311340332,
      "learning_rate": 3.941452539114777e-05,
      "loss": 0.6833,
      "step": 1392100
    },
    {
      "epoch": 12.703482005985839,
      "grad_norm": 2.406881809234619,
      "learning_rate": 3.9413764995011806e-05,
      "loss": 0.6806,
      "step": 1392200
    },
    {
      "epoch": 12.704394481349004,
      "grad_norm": 4.814016342163086,
      "learning_rate": 3.941300459887583e-05,
      "loss": 0.6917,
      "step": 1392300
    },
    {
      "epoch": 12.70530695671217,
      "grad_norm": 5.24727725982666,
      "learning_rate": 3.941224420273986e-05,
      "loss": 0.6813,
      "step": 1392400
    },
    {
      "epoch": 12.706219432075335,
      "grad_norm": 4.3423309326171875,
      "learning_rate": 3.941148380660389e-05,
      "loss": 0.6895,
      "step": 1392500
    },
    {
      "epoch": 12.7071319074385,
      "grad_norm": 5.092472076416016,
      "learning_rate": 3.941072341046792e-05,
      "loss": 0.6762,
      "step": 1392600
    },
    {
      "epoch": 12.708044382801665,
      "grad_norm": 3.2464332580566406,
      "learning_rate": 3.940996301433195e-05,
      "loss": 0.6799,
      "step": 1392700
    },
    {
      "epoch": 12.708956858164829,
      "grad_norm": 3.7618422508239746,
      "learning_rate": 3.940920261819598e-05,
      "loss": 0.6794,
      "step": 1392800
    },
    {
      "epoch": 12.709869333527994,
      "grad_norm": 4.037052631378174,
      "learning_rate": 3.940844222206e-05,
      "loss": 0.7106,
      "step": 1392900
    },
    {
      "epoch": 12.71078180889116,
      "grad_norm": 3.7837226390838623,
      "learning_rate": 3.940768182592404e-05,
      "loss": 0.67,
      "step": 1393000
    },
    {
      "epoch": 12.711694284254325,
      "grad_norm": 3.847385883331299,
      "learning_rate": 3.940692142978806e-05,
      "loss": 0.6478,
      "step": 1393100
    },
    {
      "epoch": 12.71260675961749,
      "grad_norm": 3.944125175476074,
      "learning_rate": 3.940616103365209e-05,
      "loss": 0.6948,
      "step": 1393200
    },
    {
      "epoch": 12.713519234980655,
      "grad_norm": 4.340508460998535,
      "learning_rate": 3.940540063751612e-05,
      "loss": 0.7019,
      "step": 1393300
    },
    {
      "epoch": 12.71443171034382,
      "grad_norm": 3.917055130004883,
      "learning_rate": 3.940464024138015e-05,
      "loss": 0.6988,
      "step": 1393400
    },
    {
      "epoch": 12.715344185706986,
      "grad_norm": 4.24987268447876,
      "learning_rate": 3.9403879845244176e-05,
      "loss": 0.6473,
      "step": 1393500
    },
    {
      "epoch": 12.716256661070151,
      "grad_norm": 4.498161315917969,
      "learning_rate": 3.940311944910821e-05,
      "loss": 0.6849,
      "step": 1393600
    },
    {
      "epoch": 12.717169136433316,
      "grad_norm": 3.3941049575805664,
      "learning_rate": 3.9402359052972236e-05,
      "loss": 0.6851,
      "step": 1393700
    },
    {
      "epoch": 12.718081611796482,
      "grad_norm": 3.3750216960906982,
      "learning_rate": 3.9401598656836266e-05,
      "loss": 0.6788,
      "step": 1393800
    },
    {
      "epoch": 12.718994087159647,
      "grad_norm": 4.220274448394775,
      "learning_rate": 3.9400838260700296e-05,
      "loss": 0.6727,
      "step": 1393900
    },
    {
      "epoch": 12.719906562522812,
      "grad_norm": 4.281009674072266,
      "learning_rate": 3.9400077864564326e-05,
      "loss": 0.7272,
      "step": 1394000
    },
    {
      "epoch": 12.720819037885978,
      "grad_norm": 3.397932529449463,
      "learning_rate": 3.9399317468428357e-05,
      "loss": 0.6538,
      "step": 1394100
    },
    {
      "epoch": 12.721731513249143,
      "grad_norm": 4.103469371795654,
      "learning_rate": 3.9398557072292387e-05,
      "loss": 0.6905,
      "step": 1394200
    },
    {
      "epoch": 12.722643988612308,
      "grad_norm": 4.857908248901367,
      "learning_rate": 3.939779667615641e-05,
      "loss": 0.6687,
      "step": 1394300
    },
    {
      "epoch": 12.723556463975473,
      "grad_norm": 4.392653465270996,
      "learning_rate": 3.939703628002045e-05,
      "loss": 0.6561,
      "step": 1394400
    },
    {
      "epoch": 12.724468939338639,
      "grad_norm": 3.8739678859710693,
      "learning_rate": 3.939627588388447e-05,
      "loss": 0.6827,
      "step": 1394500
    },
    {
      "epoch": 12.725381414701802,
      "grad_norm": 3.592587947845459,
      "learning_rate": 3.939551548774849e-05,
      "loss": 0.7111,
      "step": 1394600
    },
    {
      "epoch": 12.726293890064968,
      "grad_norm": 3.511218547821045,
      "learning_rate": 3.939475509161253e-05,
      "loss": 0.6555,
      "step": 1394700
    },
    {
      "epoch": 12.727206365428133,
      "grad_norm": 3.2931885719299316,
      "learning_rate": 3.9393994695476553e-05,
      "loss": 0.7363,
      "step": 1394800
    },
    {
      "epoch": 12.728118840791298,
      "grad_norm": 3.748136043548584,
      "learning_rate": 3.9393234299340584e-05,
      "loss": 0.6878,
      "step": 1394900
    },
    {
      "epoch": 12.729031316154463,
      "grad_norm": 3.650278091430664,
      "learning_rate": 3.9392473903204614e-05,
      "loss": 0.6669,
      "step": 1395000
    },
    {
      "epoch": 12.729943791517629,
      "grad_norm": 3.7548305988311768,
      "learning_rate": 3.9391713507068644e-05,
      "loss": 0.6536,
      "step": 1395100
    },
    {
      "epoch": 12.730856266880794,
      "grad_norm": 3.7082138061523438,
      "learning_rate": 3.9390953110932674e-05,
      "loss": 0.6592,
      "step": 1395200
    },
    {
      "epoch": 12.73176874224396,
      "grad_norm": 3.8225741386413574,
      "learning_rate": 3.9390192714796704e-05,
      "loss": 0.5979,
      "step": 1395300
    },
    {
      "epoch": 12.732681217607125,
      "grad_norm": 4.789803981781006,
      "learning_rate": 3.938943231866073e-05,
      "loss": 0.6917,
      "step": 1395400
    },
    {
      "epoch": 12.73359369297029,
      "grad_norm": 3.9831764698028564,
      "learning_rate": 3.9388671922524764e-05,
      "loss": 0.664,
      "step": 1395500
    },
    {
      "epoch": 12.734506168333455,
      "grad_norm": 4.311690330505371,
      "learning_rate": 3.938791152638879e-05,
      "loss": 0.6594,
      "step": 1395600
    },
    {
      "epoch": 12.73541864369662,
      "grad_norm": 3.856764316558838,
      "learning_rate": 3.938715113025282e-05,
      "loss": 0.6872,
      "step": 1395700
    },
    {
      "epoch": 12.736331119059786,
      "grad_norm": 4.056186676025391,
      "learning_rate": 3.938639073411685e-05,
      "loss": 0.6799,
      "step": 1395800
    },
    {
      "epoch": 12.737243594422951,
      "grad_norm": 4.380985736846924,
      "learning_rate": 3.938563033798088e-05,
      "loss": 0.6877,
      "step": 1395900
    },
    {
      "epoch": 12.738156069786116,
      "grad_norm": 4.08017110824585,
      "learning_rate": 3.93848699418449e-05,
      "loss": 0.6744,
      "step": 1396000
    },
    {
      "epoch": 12.739068545149282,
      "grad_norm": 3.612090826034546,
      "learning_rate": 3.938410954570894e-05,
      "loss": 0.6981,
      "step": 1396100
    },
    {
      "epoch": 12.739981020512445,
      "grad_norm": 3.1617989540100098,
      "learning_rate": 3.938334914957296e-05,
      "loss": 0.6729,
      "step": 1396200
    },
    {
      "epoch": 12.74089349587561,
      "grad_norm": 3.755162000656128,
      "learning_rate": 3.938258875343699e-05,
      "loss": 0.6622,
      "step": 1396300
    },
    {
      "epoch": 12.741805971238776,
      "grad_norm": 4.082089424133301,
      "learning_rate": 3.938182835730102e-05,
      "loss": 0.687,
      "step": 1396400
    },
    {
      "epoch": 12.742718446601941,
      "grad_norm": 3.3641839027404785,
      "learning_rate": 3.938106796116505e-05,
      "loss": 0.639,
      "step": 1396500
    },
    {
      "epoch": 12.743630921965106,
      "grad_norm": 3.863975763320923,
      "learning_rate": 3.938030756502908e-05,
      "loss": 0.667,
      "step": 1396600
    },
    {
      "epoch": 12.744543397328272,
      "grad_norm": 4.340932846069336,
      "learning_rate": 3.937954716889311e-05,
      "loss": 0.6967,
      "step": 1396700
    },
    {
      "epoch": 12.745455872691437,
      "grad_norm": 3.5313427448272705,
      "learning_rate": 3.9378786772757134e-05,
      "loss": 0.6467,
      "step": 1396800
    },
    {
      "epoch": 12.746368348054602,
      "grad_norm": 3.248323917388916,
      "learning_rate": 3.937802637662117e-05,
      "loss": 0.6653,
      "step": 1396900
    },
    {
      "epoch": 12.747280823417768,
      "grad_norm": 3.1000900268554688,
      "learning_rate": 3.9377265980485195e-05,
      "loss": 0.7098,
      "step": 1397000
    },
    {
      "epoch": 12.748193298780933,
      "grad_norm": 4.333337783813477,
      "learning_rate": 3.9376505584349225e-05,
      "loss": 0.671,
      "step": 1397100
    },
    {
      "epoch": 12.749105774144098,
      "grad_norm": 3.1271331310272217,
      "learning_rate": 3.9375745188213255e-05,
      "loss": 0.6599,
      "step": 1397200
    },
    {
      "epoch": 12.750018249507264,
      "grad_norm": 3.579545259475708,
      "learning_rate": 3.9374984792077285e-05,
      "loss": 0.6825,
      "step": 1397300
    },
    {
      "epoch": 12.750930724870429,
      "grad_norm": 3.6293234825134277,
      "learning_rate": 3.9374224395941315e-05,
      "loss": 0.6801,
      "step": 1397400
    },
    {
      "epoch": 12.751843200233594,
      "grad_norm": 4.326933860778809,
      "learning_rate": 3.937346399980534e-05,
      "loss": 0.6789,
      "step": 1397500
    },
    {
      "epoch": 12.75275567559676,
      "grad_norm": 3.8496272563934326,
      "learning_rate": 3.937270360366937e-05,
      "loss": 0.6852,
      "step": 1397600
    },
    {
      "epoch": 12.753668150959925,
      "grad_norm": 4.1770124435424805,
      "learning_rate": 3.93719432075334e-05,
      "loss": 0.6533,
      "step": 1397700
    },
    {
      "epoch": 12.75458062632309,
      "grad_norm": 3.491811513900757,
      "learning_rate": 3.937118281139743e-05,
      "loss": 0.6658,
      "step": 1397800
    },
    {
      "epoch": 12.755493101686255,
      "grad_norm": 3.64536714553833,
      "learning_rate": 3.937042241526145e-05,
      "loss": 0.696,
      "step": 1397900
    },
    {
      "epoch": 12.756405577049419,
      "grad_norm": 3.398737668991089,
      "learning_rate": 3.936966201912549e-05,
      "loss": 0.6843,
      "step": 1398000
    },
    {
      "epoch": 12.757318052412584,
      "grad_norm": 4.446409702301025,
      "learning_rate": 3.936890162298951e-05,
      "loss": 0.6785,
      "step": 1398100
    },
    {
      "epoch": 12.75823052777575,
      "grad_norm": 3.60581111907959,
      "learning_rate": 3.936814122685354e-05,
      "loss": 0.6587,
      "step": 1398200
    },
    {
      "epoch": 12.759143003138915,
      "grad_norm": 3.6019506454467773,
      "learning_rate": 3.936738083071757e-05,
      "loss": 0.7391,
      "step": 1398300
    },
    {
      "epoch": 12.76005547850208,
      "grad_norm": 4.568248271942139,
      "learning_rate": 3.93666204345816e-05,
      "loss": 0.6734,
      "step": 1398400
    },
    {
      "epoch": 12.760967953865245,
      "grad_norm": 4.539272785186768,
      "learning_rate": 3.936586003844563e-05,
      "loss": 0.709,
      "step": 1398500
    },
    {
      "epoch": 12.76188042922841,
      "grad_norm": 3.5543158054351807,
      "learning_rate": 3.936509964230966e-05,
      "loss": 0.684,
      "step": 1398600
    },
    {
      "epoch": 12.762792904591576,
      "grad_norm": 3.058610200881958,
      "learning_rate": 3.9364339246173685e-05,
      "loss": 0.6526,
      "step": 1398700
    },
    {
      "epoch": 12.763705379954741,
      "grad_norm": 3.313535690307617,
      "learning_rate": 3.936357885003772e-05,
      "loss": 0.6973,
      "step": 1398800
    },
    {
      "epoch": 12.764617855317907,
      "grad_norm": 3.887892723083496,
      "learning_rate": 3.9362818453901746e-05,
      "loss": 0.6706,
      "step": 1398900
    },
    {
      "epoch": 12.765530330681072,
      "grad_norm": 3.45157790184021,
      "learning_rate": 3.9362058057765776e-05,
      "loss": 0.6608,
      "step": 1399000
    },
    {
      "epoch": 12.766442806044237,
      "grad_norm": 4.410256385803223,
      "learning_rate": 3.9361297661629806e-05,
      "loss": 0.6748,
      "step": 1399100
    },
    {
      "epoch": 12.767355281407403,
      "grad_norm": 4.006584644317627,
      "learning_rate": 3.9360537265493836e-05,
      "loss": 0.6689,
      "step": 1399200
    },
    {
      "epoch": 12.768267756770568,
      "grad_norm": 3.9452667236328125,
      "learning_rate": 3.935977686935786e-05,
      "loss": 0.6714,
      "step": 1399300
    },
    {
      "epoch": 12.769180232133733,
      "grad_norm": 3.1953821182250977,
      "learning_rate": 3.9359016473221896e-05,
      "loss": 0.674,
      "step": 1399400
    },
    {
      "epoch": 12.770092707496898,
      "grad_norm": 5.515374183654785,
      "learning_rate": 3.935825607708592e-05,
      "loss": 0.6884,
      "step": 1399500
    },
    {
      "epoch": 12.771005182860062,
      "grad_norm": 4.998974323272705,
      "learning_rate": 3.935749568094995e-05,
      "loss": 0.6744,
      "step": 1399600
    },
    {
      "epoch": 12.771917658223227,
      "grad_norm": 6.448390483856201,
      "learning_rate": 3.935673528481398e-05,
      "loss": 0.652,
      "step": 1399700
    },
    {
      "epoch": 12.772830133586393,
      "grad_norm": 3.5650265216827393,
      "learning_rate": 3.935597488867801e-05,
      "loss": 0.705,
      "step": 1399800
    },
    {
      "epoch": 12.773742608949558,
      "grad_norm": 3.349464178085327,
      "learning_rate": 3.935521449254204e-05,
      "loss": 0.6967,
      "step": 1399900
    },
    {
      "epoch": 12.774655084312723,
      "grad_norm": 4.516040802001953,
      "learning_rate": 3.935445409640607e-05,
      "loss": 0.6489,
      "step": 1400000
    },
    {
      "epoch": 12.775567559675888,
      "grad_norm": 3.2944984436035156,
      "learning_rate": 3.935369370027009e-05,
      "loss": 0.6824,
      "step": 1400100
    },
    {
      "epoch": 12.776480035039054,
      "grad_norm": 4.5116286277771,
      "learning_rate": 3.935293330413412e-05,
      "loss": 0.6848,
      "step": 1400200
    },
    {
      "epoch": 12.777392510402219,
      "grad_norm": 3.9061050415039062,
      "learning_rate": 3.935217290799815e-05,
      "loss": 0.6663,
      "step": 1400300
    },
    {
      "epoch": 12.778304985765384,
      "grad_norm": 3.6511831283569336,
      "learning_rate": 3.9351412511862176e-05,
      "loss": 0.6841,
      "step": 1400400
    },
    {
      "epoch": 12.77921746112855,
      "grad_norm": 4.340670585632324,
      "learning_rate": 3.935065211572621e-05,
      "loss": 0.6549,
      "step": 1400500
    },
    {
      "epoch": 12.780129936491715,
      "grad_norm": 4.6970014572143555,
      "learning_rate": 3.9349891719590236e-05,
      "loss": 0.6556,
      "step": 1400600
    },
    {
      "epoch": 12.78104241185488,
      "grad_norm": 3.6130287647247314,
      "learning_rate": 3.9349131323454266e-05,
      "loss": 0.6764,
      "step": 1400700
    },
    {
      "epoch": 12.781954887218046,
      "grad_norm": 4.21330451965332,
      "learning_rate": 3.9348370927318297e-05,
      "loss": 0.6942,
      "step": 1400800
    },
    {
      "epoch": 12.78286736258121,
      "grad_norm": 3.820927619934082,
      "learning_rate": 3.9347610531182327e-05,
      "loss": 0.719,
      "step": 1400900
    },
    {
      "epoch": 12.783779837944376,
      "grad_norm": 4.602597713470459,
      "learning_rate": 3.934685013504636e-05,
      "loss": 0.6997,
      "step": 1401000
    },
    {
      "epoch": 12.784692313307541,
      "grad_norm": 3.7270476818084717,
      "learning_rate": 3.934608973891039e-05,
      "loss": 0.6648,
      "step": 1401100
    },
    {
      "epoch": 12.785604788670707,
      "grad_norm": 4.2557268142700195,
      "learning_rate": 3.934532934277441e-05,
      "loss": 0.699,
      "step": 1401200
    },
    {
      "epoch": 12.786517264033872,
      "grad_norm": 4.278853416442871,
      "learning_rate": 3.934456894663845e-05,
      "loss": 0.67,
      "step": 1401300
    },
    {
      "epoch": 12.787429739397036,
      "grad_norm": 3.8172895908355713,
      "learning_rate": 3.934380855050247e-05,
      "loss": 0.6707,
      "step": 1401400
    },
    {
      "epoch": 12.7883422147602,
      "grad_norm": 3.4023027420043945,
      "learning_rate": 3.93430481543665e-05,
      "loss": 0.673,
      "step": 1401500
    },
    {
      "epoch": 12.789254690123366,
      "grad_norm": 4.056846618652344,
      "learning_rate": 3.934228775823053e-05,
      "loss": 0.67,
      "step": 1401600
    },
    {
      "epoch": 12.790167165486531,
      "grad_norm": 4.392570495605469,
      "learning_rate": 3.934152736209456e-05,
      "loss": 0.7021,
      "step": 1401700
    },
    {
      "epoch": 12.791079640849697,
      "grad_norm": 4.187610626220703,
      "learning_rate": 3.9340766965958584e-05,
      "loss": 0.6565,
      "step": 1401800
    },
    {
      "epoch": 12.791992116212862,
      "grad_norm": 4.088268756866455,
      "learning_rate": 3.934000656982262e-05,
      "loss": 0.6505,
      "step": 1401900
    },
    {
      "epoch": 12.792904591576027,
      "grad_norm": 4.609813213348389,
      "learning_rate": 3.9339246173686644e-05,
      "loss": 0.7198,
      "step": 1402000
    },
    {
      "epoch": 12.793817066939193,
      "grad_norm": 4.158616065979004,
      "learning_rate": 3.9338485777550674e-05,
      "loss": 0.7007,
      "step": 1402100
    },
    {
      "epoch": 12.794729542302358,
      "grad_norm": 3.3952817916870117,
      "learning_rate": 3.9337725381414704e-05,
      "loss": 0.6917,
      "step": 1402200
    },
    {
      "epoch": 12.795642017665523,
      "grad_norm": 4.347166538238525,
      "learning_rate": 3.9336964985278734e-05,
      "loss": 0.6838,
      "step": 1402300
    },
    {
      "epoch": 12.796554493028689,
      "grad_norm": 4.067657470703125,
      "learning_rate": 3.9336204589142764e-05,
      "loss": 0.6508,
      "step": 1402400
    },
    {
      "epoch": 12.797466968391854,
      "grad_norm": 4.3426594734191895,
      "learning_rate": 3.9335444193006794e-05,
      "loss": 0.6678,
      "step": 1402500
    },
    {
      "epoch": 12.79837944375502,
      "grad_norm": 3.361701011657715,
      "learning_rate": 3.933468379687082e-05,
      "loss": 0.6458,
      "step": 1402600
    },
    {
      "epoch": 12.799291919118184,
      "grad_norm": 4.496377468109131,
      "learning_rate": 3.9333923400734854e-05,
      "loss": 0.6862,
      "step": 1402700
    },
    {
      "epoch": 12.80020439448135,
      "grad_norm": 4.086114406585693,
      "learning_rate": 3.933316300459888e-05,
      "loss": 0.6819,
      "step": 1402800
    },
    {
      "epoch": 12.801116869844515,
      "grad_norm": 4.015132427215576,
      "learning_rate": 3.933240260846291e-05,
      "loss": 0.6627,
      "step": 1402900
    },
    {
      "epoch": 12.802029345207679,
      "grad_norm": 4.174901962280273,
      "learning_rate": 3.933164221232694e-05,
      "loss": 0.6732,
      "step": 1403000
    },
    {
      "epoch": 12.802941820570844,
      "grad_norm": 3.907099723815918,
      "learning_rate": 3.933088181619096e-05,
      "loss": 0.6598,
      "step": 1403100
    },
    {
      "epoch": 12.80385429593401,
      "grad_norm": 3.665708303451538,
      "learning_rate": 3.933012142005499e-05,
      "loss": 0.6646,
      "step": 1403200
    },
    {
      "epoch": 12.804766771297174,
      "grad_norm": 3.5110487937927246,
      "learning_rate": 3.932936102391902e-05,
      "loss": 0.6918,
      "step": 1403300
    },
    {
      "epoch": 12.80567924666034,
      "grad_norm": 3.592102527618408,
      "learning_rate": 3.932860062778305e-05,
      "loss": 0.6804,
      "step": 1403400
    },
    {
      "epoch": 12.806591722023505,
      "grad_norm": 3.9199299812316895,
      "learning_rate": 3.932784023164708e-05,
      "loss": 0.7061,
      "step": 1403500
    },
    {
      "epoch": 12.80750419738667,
      "grad_norm": 4.4958295822143555,
      "learning_rate": 3.932707983551111e-05,
      "loss": 0.6907,
      "step": 1403600
    },
    {
      "epoch": 12.808416672749836,
      "grad_norm": 3.9504072666168213,
      "learning_rate": 3.9326319439375135e-05,
      "loss": 0.6701,
      "step": 1403700
    },
    {
      "epoch": 12.809329148113001,
      "grad_norm": 3.194016218185425,
      "learning_rate": 3.932555904323917e-05,
      "loss": 0.6814,
      "step": 1403800
    },
    {
      "epoch": 12.810241623476166,
      "grad_norm": 4.25765323638916,
      "learning_rate": 3.9324798647103195e-05,
      "loss": 0.6801,
      "step": 1403900
    },
    {
      "epoch": 12.811154098839332,
      "grad_norm": 4.424992084503174,
      "learning_rate": 3.9324038250967225e-05,
      "loss": 0.6663,
      "step": 1404000
    },
    {
      "epoch": 12.812066574202497,
      "grad_norm": 4.602733135223389,
      "learning_rate": 3.9323277854831255e-05,
      "loss": 0.646,
      "step": 1404100
    },
    {
      "epoch": 12.812979049565662,
      "grad_norm": 4.111892223358154,
      "learning_rate": 3.9322517458695285e-05,
      "loss": 0.6855,
      "step": 1404200
    },
    {
      "epoch": 12.813891524928827,
      "grad_norm": 4.583364009857178,
      "learning_rate": 3.932175706255931e-05,
      "loss": 0.6705,
      "step": 1404300
    },
    {
      "epoch": 12.814804000291993,
      "grad_norm": 3.831847906112671,
      "learning_rate": 3.9320996666423345e-05,
      "loss": 0.6566,
      "step": 1404400
    },
    {
      "epoch": 12.815716475655158,
      "grad_norm": 3.3124232292175293,
      "learning_rate": 3.932023627028737e-05,
      "loss": 0.6894,
      "step": 1404500
    },
    {
      "epoch": 12.816628951018323,
      "grad_norm": 4.128691673278809,
      "learning_rate": 3.93194758741514e-05,
      "loss": 0.6934,
      "step": 1404600
    },
    {
      "epoch": 12.817541426381489,
      "grad_norm": 3.9373531341552734,
      "learning_rate": 3.931871547801543e-05,
      "loss": 0.6967,
      "step": 1404700
    },
    {
      "epoch": 12.818453901744652,
      "grad_norm": 4.335622310638428,
      "learning_rate": 3.931795508187946e-05,
      "loss": 0.6747,
      "step": 1404800
    },
    {
      "epoch": 12.819366377107817,
      "grad_norm": 3.348062515258789,
      "learning_rate": 3.931719468574349e-05,
      "loss": 0.7154,
      "step": 1404900
    },
    {
      "epoch": 12.820278852470983,
      "grad_norm": 3.4825375080108643,
      "learning_rate": 3.931643428960752e-05,
      "loss": 0.6719,
      "step": 1405000
    },
    {
      "epoch": 12.821191327834148,
      "grad_norm": 3.7641959190368652,
      "learning_rate": 3.931567389347154e-05,
      "loss": 0.6819,
      "step": 1405100
    },
    {
      "epoch": 12.822103803197313,
      "grad_norm": 4.1967997550964355,
      "learning_rate": 3.931491349733558e-05,
      "loss": 0.6714,
      "step": 1405200
    },
    {
      "epoch": 12.823016278560479,
      "grad_norm": 3.6851937770843506,
      "learning_rate": 3.93141531011996e-05,
      "loss": 0.683,
      "step": 1405300
    },
    {
      "epoch": 12.823928753923644,
      "grad_norm": 3.8989782333374023,
      "learning_rate": 3.931339270506363e-05,
      "loss": 0.6801,
      "step": 1405400
    },
    {
      "epoch": 12.82484122928681,
      "grad_norm": 4.001023769378662,
      "learning_rate": 3.931263230892766e-05,
      "loss": 0.6801,
      "step": 1405500
    },
    {
      "epoch": 12.825753704649975,
      "grad_norm": 3.3760757446289062,
      "learning_rate": 3.931187191279169e-05,
      "loss": 0.6883,
      "step": 1405600
    },
    {
      "epoch": 12.82666618001314,
      "grad_norm": 4.407796859741211,
      "learning_rate": 3.9311111516655716e-05,
      "loss": 0.7012,
      "step": 1405700
    },
    {
      "epoch": 12.827578655376305,
      "grad_norm": 4.49682092666626,
      "learning_rate": 3.931035112051975e-05,
      "loss": 0.6532,
      "step": 1405800
    },
    {
      "epoch": 12.82849113073947,
      "grad_norm": 3.643357753753662,
      "learning_rate": 3.9309590724383776e-05,
      "loss": 0.6701,
      "step": 1405900
    },
    {
      "epoch": 12.829403606102636,
      "grad_norm": 4.514275074005127,
      "learning_rate": 3.9308830328247806e-05,
      "loss": 0.7081,
      "step": 1406000
    },
    {
      "epoch": 12.830316081465801,
      "grad_norm": 4.114332675933838,
      "learning_rate": 3.9308069932111836e-05,
      "loss": 0.7067,
      "step": 1406100
    },
    {
      "epoch": 12.831228556828966,
      "grad_norm": 3.2320806980133057,
      "learning_rate": 3.930730953597586e-05,
      "loss": 0.7496,
      "step": 1406200
    },
    {
      "epoch": 12.83214103219213,
      "grad_norm": 4.331226825714111,
      "learning_rate": 3.9306549139839896e-05,
      "loss": 0.6714,
      "step": 1406300
    },
    {
      "epoch": 12.833053507555295,
      "grad_norm": 3.238607406616211,
      "learning_rate": 3.930578874370392e-05,
      "loss": 0.6689,
      "step": 1406400
    },
    {
      "epoch": 12.83396598291846,
      "grad_norm": 3.9396514892578125,
      "learning_rate": 3.930502834756795e-05,
      "loss": 0.6879,
      "step": 1406500
    },
    {
      "epoch": 12.834878458281626,
      "grad_norm": 4.1649603843688965,
      "learning_rate": 3.930426795143198e-05,
      "loss": 0.6538,
      "step": 1406600
    },
    {
      "epoch": 12.835790933644791,
      "grad_norm": 3.353654384613037,
      "learning_rate": 3.930350755529601e-05,
      "loss": 0.6901,
      "step": 1406700
    },
    {
      "epoch": 12.836703409007956,
      "grad_norm": 4.369601249694824,
      "learning_rate": 3.930274715916003e-05,
      "loss": 0.677,
      "step": 1406800
    },
    {
      "epoch": 12.837615884371122,
      "grad_norm": 4.239475727081299,
      "learning_rate": 3.930198676302407e-05,
      "loss": 0.6866,
      "step": 1406900
    },
    {
      "epoch": 12.838528359734287,
      "grad_norm": 3.4277327060699463,
      "learning_rate": 3.930122636688809e-05,
      "loss": 0.6781,
      "step": 1407000
    },
    {
      "epoch": 12.839440835097452,
      "grad_norm": 3.1700239181518555,
      "learning_rate": 3.930046597075212e-05,
      "loss": 0.6981,
      "step": 1407100
    },
    {
      "epoch": 12.840353310460618,
      "grad_norm": 4.334733009338379,
      "learning_rate": 3.929970557461615e-05,
      "loss": 0.6672,
      "step": 1407200
    },
    {
      "epoch": 12.841265785823783,
      "grad_norm": 4.529299259185791,
      "learning_rate": 3.929894517848018e-05,
      "loss": 0.6767,
      "step": 1407300
    },
    {
      "epoch": 12.842178261186948,
      "grad_norm": 4.0097150802612305,
      "learning_rate": 3.929818478234421e-05,
      "loss": 0.6921,
      "step": 1407400
    },
    {
      "epoch": 12.843090736550113,
      "grad_norm": 4.4538798332214355,
      "learning_rate": 3.929742438620824e-05,
      "loss": 0.6532,
      "step": 1407500
    },
    {
      "epoch": 12.844003211913279,
      "grad_norm": 4.134546756744385,
      "learning_rate": 3.9296663990072267e-05,
      "loss": 0.664,
      "step": 1407600
    },
    {
      "epoch": 12.844915687276444,
      "grad_norm": 4.7084879875183105,
      "learning_rate": 3.92959035939363e-05,
      "loss": 0.6848,
      "step": 1407700
    },
    {
      "epoch": 12.84582816263961,
      "grad_norm": 4.102868556976318,
      "learning_rate": 3.929514319780033e-05,
      "loss": 0.6768,
      "step": 1407800
    },
    {
      "epoch": 12.846740638002775,
      "grad_norm": 4.2299723625183105,
      "learning_rate": 3.929438280166436e-05,
      "loss": 0.6928,
      "step": 1407900
    },
    {
      "epoch": 12.84765311336594,
      "grad_norm": 4.629209041595459,
      "learning_rate": 3.929362240552839e-05,
      "loss": 0.6768,
      "step": 1408000
    },
    {
      "epoch": 12.848565588729103,
      "grad_norm": 4.468198299407959,
      "learning_rate": 3.929286200939242e-05,
      "loss": 0.6385,
      "step": 1408100
    },
    {
      "epoch": 12.849478064092269,
      "grad_norm": 4.126067161560059,
      "learning_rate": 3.929210161325644e-05,
      "loss": 0.7043,
      "step": 1408200
    },
    {
      "epoch": 12.850390539455434,
      "grad_norm": 3.8886160850524902,
      "learning_rate": 3.929134121712048e-05,
      "loss": 0.6817,
      "step": 1408300
    },
    {
      "epoch": 12.8513030148186,
      "grad_norm": 2.5870511531829834,
      "learning_rate": 3.92905808209845e-05,
      "loss": 0.685,
      "step": 1408400
    },
    {
      "epoch": 12.852215490181765,
      "grad_norm": 4.169900894165039,
      "learning_rate": 3.928982042484853e-05,
      "loss": 0.6727,
      "step": 1408500
    },
    {
      "epoch": 12.85312796554493,
      "grad_norm": 5.11862850189209,
      "learning_rate": 3.928906002871256e-05,
      "loss": 0.6687,
      "step": 1408600
    },
    {
      "epoch": 12.854040440908095,
      "grad_norm": 3.599527359008789,
      "learning_rate": 3.928829963257659e-05,
      "loss": 0.6562,
      "step": 1408700
    },
    {
      "epoch": 12.85495291627126,
      "grad_norm": 4.414482116699219,
      "learning_rate": 3.928753923644062e-05,
      "loss": 0.6561,
      "step": 1408800
    },
    {
      "epoch": 12.855865391634426,
      "grad_norm": 4.237420082092285,
      "learning_rate": 3.9286778840304644e-05,
      "loss": 0.6925,
      "step": 1408900
    },
    {
      "epoch": 12.856777866997591,
      "grad_norm": 3.7842419147491455,
      "learning_rate": 3.9286018444168674e-05,
      "loss": 0.6783,
      "step": 1409000
    },
    {
      "epoch": 12.857690342360756,
      "grad_norm": 4.684586048126221,
      "learning_rate": 3.9285258048032704e-05,
      "loss": 0.7006,
      "step": 1409100
    },
    {
      "epoch": 12.858602817723922,
      "grad_norm": 3.6362576484680176,
      "learning_rate": 3.9284497651896734e-05,
      "loss": 0.6791,
      "step": 1409200
    },
    {
      "epoch": 12.859515293087087,
      "grad_norm": 4.622374534606934,
      "learning_rate": 3.9283737255760764e-05,
      "loss": 0.6541,
      "step": 1409300
    },
    {
      "epoch": 12.860427768450252,
      "grad_norm": 4.35101318359375,
      "learning_rate": 3.9282976859624794e-05,
      "loss": 0.6471,
      "step": 1409400
    },
    {
      "epoch": 12.861340243813418,
      "grad_norm": 4.9602437019348145,
      "learning_rate": 3.928221646348882e-05,
      "loss": 0.6693,
      "step": 1409500
    },
    {
      "epoch": 12.862252719176583,
      "grad_norm": 4.043781280517578,
      "learning_rate": 3.928145606735285e-05,
      "loss": 0.6665,
      "step": 1409600
    },
    {
      "epoch": 12.863165194539746,
      "grad_norm": 3.8433821201324463,
      "learning_rate": 3.928069567121688e-05,
      "loss": 0.6666,
      "step": 1409700
    },
    {
      "epoch": 12.864077669902912,
      "grad_norm": 3.6391348838806152,
      "learning_rate": 3.927993527508091e-05,
      "loss": 0.6724,
      "step": 1409800
    },
    {
      "epoch": 12.864990145266077,
      "grad_norm": 3.603595018386841,
      "learning_rate": 3.927917487894494e-05,
      "loss": 0.6815,
      "step": 1409900
    },
    {
      "epoch": 12.865902620629242,
      "grad_norm": 3.522731065750122,
      "learning_rate": 3.927841448280897e-05,
      "loss": 0.6887,
      "step": 1410000
    },
    {
      "epoch": 12.866815095992408,
      "grad_norm": 3.5598671436309814,
      "learning_rate": 3.927765408667299e-05,
      "loss": 0.7226,
      "step": 1410100
    },
    {
      "epoch": 12.867727571355573,
      "grad_norm": 4.308746814727783,
      "learning_rate": 3.927689369053703e-05,
      "loss": 0.6753,
      "step": 1410200
    },
    {
      "epoch": 12.868640046718738,
      "grad_norm": 3.5779924392700195,
      "learning_rate": 3.927613329440105e-05,
      "loss": 0.6936,
      "step": 1410300
    },
    {
      "epoch": 12.869552522081904,
      "grad_norm": 2.743623733520508,
      "learning_rate": 3.927537289826508e-05,
      "loss": 0.7006,
      "step": 1410400
    },
    {
      "epoch": 12.870464997445069,
      "grad_norm": 4.487020969390869,
      "learning_rate": 3.927461250212911e-05,
      "loss": 0.6847,
      "step": 1410500
    },
    {
      "epoch": 12.871377472808234,
      "grad_norm": 4.195835113525391,
      "learning_rate": 3.927385210599314e-05,
      "loss": 0.6884,
      "step": 1410600
    },
    {
      "epoch": 12.8722899481714,
      "grad_norm": 4.586029529571533,
      "learning_rate": 3.927309170985717e-05,
      "loss": 0.6731,
      "step": 1410700
    },
    {
      "epoch": 12.873202423534565,
      "grad_norm": 3.775681495666504,
      "learning_rate": 3.92723313137212e-05,
      "loss": 0.6879,
      "step": 1410800
    },
    {
      "epoch": 12.87411489889773,
      "grad_norm": 3.3824970722198486,
      "learning_rate": 3.9271570917585225e-05,
      "loss": 0.6902,
      "step": 1410900
    },
    {
      "epoch": 12.875027374260895,
      "grad_norm": 3.4803905487060547,
      "learning_rate": 3.9270810521449255e-05,
      "loss": 0.6564,
      "step": 1411000
    },
    {
      "epoch": 12.87593984962406,
      "grad_norm": 3.767061710357666,
      "learning_rate": 3.9270050125313285e-05,
      "loss": 0.7029,
      "step": 1411100
    },
    {
      "epoch": 12.876852324987226,
      "grad_norm": 4.707991600036621,
      "learning_rate": 3.9269289729177315e-05,
      "loss": 0.6815,
      "step": 1411200
    },
    {
      "epoch": 12.877764800350391,
      "grad_norm": 2.6783225536346436,
      "learning_rate": 3.9268529333041345e-05,
      "loss": 0.7166,
      "step": 1411300
    },
    {
      "epoch": 12.878677275713557,
      "grad_norm": 3.992088794708252,
      "learning_rate": 3.9267768936905375e-05,
      "loss": 0.6763,
      "step": 1411400
    },
    {
      "epoch": 12.87958975107672,
      "grad_norm": 2.7586870193481445,
      "learning_rate": 3.92670085407694e-05,
      "loss": 0.6979,
      "step": 1411500
    },
    {
      "epoch": 12.880502226439885,
      "grad_norm": 4.030006408691406,
      "learning_rate": 3.926624814463343e-05,
      "loss": 0.6961,
      "step": 1411600
    },
    {
      "epoch": 12.88141470180305,
      "grad_norm": 4.535970211029053,
      "learning_rate": 3.926548774849746e-05,
      "loss": 0.6913,
      "step": 1411700
    },
    {
      "epoch": 12.882327177166216,
      "grad_norm": 3.790410041809082,
      "learning_rate": 3.926472735236149e-05,
      "loss": 0.6646,
      "step": 1411800
    },
    {
      "epoch": 12.883239652529381,
      "grad_norm": 4.0662055015563965,
      "learning_rate": 3.926396695622552e-05,
      "loss": 0.6647,
      "step": 1411900
    },
    {
      "epoch": 12.884152127892547,
      "grad_norm": 3.9100379943847656,
      "learning_rate": 3.926320656008954e-05,
      "loss": 0.6734,
      "step": 1412000
    },
    {
      "epoch": 12.885064603255712,
      "grad_norm": 3.9380404949188232,
      "learning_rate": 3.926244616395358e-05,
      "loss": 0.7316,
      "step": 1412100
    },
    {
      "epoch": 12.885977078618877,
      "grad_norm": 3.7888646125793457,
      "learning_rate": 3.92616857678176e-05,
      "loss": 0.6832,
      "step": 1412200
    },
    {
      "epoch": 12.886889553982043,
      "grad_norm": 3.4718334674835205,
      "learning_rate": 3.926092537168163e-05,
      "loss": 0.6631,
      "step": 1412300
    },
    {
      "epoch": 12.887802029345208,
      "grad_norm": 3.934333562850952,
      "learning_rate": 3.926016497554566e-05,
      "loss": 0.6719,
      "step": 1412400
    },
    {
      "epoch": 12.888714504708373,
      "grad_norm": 3.5710792541503906,
      "learning_rate": 3.925940457940969e-05,
      "loss": 0.6711,
      "step": 1412500
    },
    {
      "epoch": 12.889626980071538,
      "grad_norm": 3.5575661659240723,
      "learning_rate": 3.9258644183273716e-05,
      "loss": 0.6795,
      "step": 1412600
    },
    {
      "epoch": 12.890539455434704,
      "grad_norm": 3.3371224403381348,
      "learning_rate": 3.925788378713775e-05,
      "loss": 0.6907,
      "step": 1412700
    },
    {
      "epoch": 12.891451930797869,
      "grad_norm": 4.086100101470947,
      "learning_rate": 3.9257123391001776e-05,
      "loss": 0.6654,
      "step": 1412800
    },
    {
      "epoch": 12.892364406161034,
      "grad_norm": 4.923399448394775,
      "learning_rate": 3.9256362994865806e-05,
      "loss": 0.685,
      "step": 1412900
    },
    {
      "epoch": 12.8932768815242,
      "grad_norm": 3.5274746417999268,
      "learning_rate": 3.9255602598729836e-05,
      "loss": 0.6568,
      "step": 1413000
    },
    {
      "epoch": 12.894189356887363,
      "grad_norm": 4.356575965881348,
      "learning_rate": 3.9254842202593866e-05,
      "loss": 0.6956,
      "step": 1413100
    },
    {
      "epoch": 12.895101832250528,
      "grad_norm": 4.223466396331787,
      "learning_rate": 3.9254081806457896e-05,
      "loss": 0.6654,
      "step": 1413200
    },
    {
      "epoch": 12.896014307613694,
      "grad_norm": 3.4441933631896973,
      "learning_rate": 3.9253321410321926e-05,
      "loss": 0.6847,
      "step": 1413300
    },
    {
      "epoch": 12.896926782976859,
      "grad_norm": 2.35099720954895,
      "learning_rate": 3.925256101418595e-05,
      "loss": 0.6951,
      "step": 1413400
    },
    {
      "epoch": 12.897839258340024,
      "grad_norm": 3.8207361698150635,
      "learning_rate": 3.9251800618049986e-05,
      "loss": 0.7168,
      "step": 1413500
    },
    {
      "epoch": 12.89875173370319,
      "grad_norm": 4.6841654777526855,
      "learning_rate": 3.925104022191401e-05,
      "loss": 0.6623,
      "step": 1413600
    },
    {
      "epoch": 12.899664209066355,
      "grad_norm": 3.748626470565796,
      "learning_rate": 3.925027982577804e-05,
      "loss": 0.6812,
      "step": 1413700
    },
    {
      "epoch": 12.90057668442952,
      "grad_norm": 3.698118209838867,
      "learning_rate": 3.924951942964207e-05,
      "loss": 0.7013,
      "step": 1413800
    },
    {
      "epoch": 12.901489159792686,
      "grad_norm": 3.01041579246521,
      "learning_rate": 3.92487590335061e-05,
      "loss": 0.6778,
      "step": 1413900
    },
    {
      "epoch": 12.90240163515585,
      "grad_norm": 4.415884017944336,
      "learning_rate": 3.924799863737012e-05,
      "loss": 0.6883,
      "step": 1414000
    },
    {
      "epoch": 12.903314110519016,
      "grad_norm": 3.910365104675293,
      "learning_rate": 3.924723824123416e-05,
      "loss": 0.6903,
      "step": 1414100
    },
    {
      "epoch": 12.904226585882181,
      "grad_norm": 4.800120830535889,
      "learning_rate": 3.924647784509818e-05,
      "loss": 0.6414,
      "step": 1414200
    },
    {
      "epoch": 12.905139061245347,
      "grad_norm": 3.0712430477142334,
      "learning_rate": 3.924571744896221e-05,
      "loss": 0.6973,
      "step": 1414300
    },
    {
      "epoch": 12.906051536608512,
      "grad_norm": 4.894650459289551,
      "learning_rate": 3.924495705282624e-05,
      "loss": 0.6846,
      "step": 1414400
    },
    {
      "epoch": 12.906964011971677,
      "grad_norm": 3.983607292175293,
      "learning_rate": 3.924419665669027e-05,
      "loss": 0.6795,
      "step": 1414500
    },
    {
      "epoch": 12.907876487334843,
      "grad_norm": 4.349780082702637,
      "learning_rate": 3.9243436260554303e-05,
      "loss": 0.6698,
      "step": 1414600
    },
    {
      "epoch": 12.908788962698008,
      "grad_norm": 3.5992064476013184,
      "learning_rate": 3.924267586441833e-05,
      "loss": 0.6632,
      "step": 1414700
    },
    {
      "epoch": 12.909701438061173,
      "grad_norm": 3.814105749130249,
      "learning_rate": 3.924191546828236e-05,
      "loss": 0.6796,
      "step": 1414800
    },
    {
      "epoch": 12.910613913424337,
      "grad_norm": 6.200403690338135,
      "learning_rate": 3.924115507214639e-05,
      "loss": 0.7005,
      "step": 1414900
    },
    {
      "epoch": 12.911526388787502,
      "grad_norm": 3.9959192276000977,
      "learning_rate": 3.924039467601042e-05,
      "loss": 0.6941,
      "step": 1415000
    },
    {
      "epoch": 12.912438864150667,
      "grad_norm": 3.813291072845459,
      "learning_rate": 3.923963427987444e-05,
      "loss": 0.6684,
      "step": 1415100
    },
    {
      "epoch": 12.913351339513833,
      "grad_norm": 4.248201847076416,
      "learning_rate": 3.923887388373848e-05,
      "loss": 0.6571,
      "step": 1415200
    },
    {
      "epoch": 12.914263814876998,
      "grad_norm": 4.006274223327637,
      "learning_rate": 3.92381134876025e-05,
      "loss": 0.6425,
      "step": 1415300
    },
    {
      "epoch": 12.915176290240163,
      "grad_norm": 4.301209449768066,
      "learning_rate": 3.923735309146653e-05,
      "loss": 0.6507,
      "step": 1415400
    },
    {
      "epoch": 12.916088765603329,
      "grad_norm": 3.318466901779175,
      "learning_rate": 3.923659269533056e-05,
      "loss": 0.65,
      "step": 1415500
    },
    {
      "epoch": 12.917001240966494,
      "grad_norm": 4.089709758758545,
      "learning_rate": 3.923583229919459e-05,
      "loss": 0.6942,
      "step": 1415600
    },
    {
      "epoch": 12.91791371632966,
      "grad_norm": 4.100810527801514,
      "learning_rate": 3.923507190305862e-05,
      "loss": 0.6923,
      "step": 1415700
    },
    {
      "epoch": 12.918826191692824,
      "grad_norm": 3.840817451477051,
      "learning_rate": 3.923431150692265e-05,
      "loss": 0.6583,
      "step": 1415800
    },
    {
      "epoch": 12.91973866705599,
      "grad_norm": 4.172488212585449,
      "learning_rate": 3.9233551110786674e-05,
      "loss": 0.6804,
      "step": 1415900
    },
    {
      "epoch": 12.920651142419155,
      "grad_norm": 3.5655367374420166,
      "learning_rate": 3.923279071465071e-05,
      "loss": 0.686,
      "step": 1416000
    },
    {
      "epoch": 12.92156361778232,
      "grad_norm": 4.838350772857666,
      "learning_rate": 3.9232030318514734e-05,
      "loss": 0.6995,
      "step": 1416100
    },
    {
      "epoch": 12.922476093145486,
      "grad_norm": 3.8878684043884277,
      "learning_rate": 3.9231269922378764e-05,
      "loss": 0.6317,
      "step": 1416200
    },
    {
      "epoch": 12.923388568508651,
      "grad_norm": 4.345451831817627,
      "learning_rate": 3.9230509526242794e-05,
      "loss": 0.6799,
      "step": 1416300
    },
    {
      "epoch": 12.924301043871816,
      "grad_norm": 3.6668474674224854,
      "learning_rate": 3.9229749130106824e-05,
      "loss": 0.7037,
      "step": 1416400
    },
    {
      "epoch": 12.92521351923498,
      "grad_norm": 4.11537504196167,
      "learning_rate": 3.922898873397085e-05,
      "loss": 0.6722,
      "step": 1416500
    },
    {
      "epoch": 12.926125994598145,
      "grad_norm": 3.766913890838623,
      "learning_rate": 3.9228228337834884e-05,
      "loss": 0.6799,
      "step": 1416600
    },
    {
      "epoch": 12.92703846996131,
      "grad_norm": 3.69472336769104,
      "learning_rate": 3.922746794169891e-05,
      "loss": 0.6594,
      "step": 1416700
    },
    {
      "epoch": 12.927950945324476,
      "grad_norm": 3.6874921321868896,
      "learning_rate": 3.922670754556294e-05,
      "loss": 0.6776,
      "step": 1416800
    },
    {
      "epoch": 12.928863420687641,
      "grad_norm": 3.2925591468811035,
      "learning_rate": 3.922594714942697e-05,
      "loss": 0.6742,
      "step": 1416900
    },
    {
      "epoch": 12.929775896050806,
      "grad_norm": 4.507406234741211,
      "learning_rate": 3.9225186753291e-05,
      "loss": 0.6764,
      "step": 1417000
    },
    {
      "epoch": 12.930688371413972,
      "grad_norm": 3.5919723510742188,
      "learning_rate": 3.922442635715503e-05,
      "loss": 0.7285,
      "step": 1417100
    },
    {
      "epoch": 12.931600846777137,
      "grad_norm": 4.313333988189697,
      "learning_rate": 3.922366596101906e-05,
      "loss": 0.6702,
      "step": 1417200
    },
    {
      "epoch": 12.932513322140302,
      "grad_norm": 3.5664913654327393,
      "learning_rate": 3.922290556488308e-05,
      "loss": 0.6739,
      "step": 1417300
    },
    {
      "epoch": 12.933425797503467,
      "grad_norm": 5.1841607093811035,
      "learning_rate": 3.922214516874711e-05,
      "loss": 0.6602,
      "step": 1417400
    },
    {
      "epoch": 12.934338272866633,
      "grad_norm": 3.7910900115966797,
      "learning_rate": 3.922138477261114e-05,
      "loss": 0.7022,
      "step": 1417500
    },
    {
      "epoch": 12.935250748229798,
      "grad_norm": 3.0481619834899902,
      "learning_rate": 3.9220624376475165e-05,
      "loss": 0.706,
      "step": 1417600
    },
    {
      "epoch": 12.936163223592963,
      "grad_norm": 4.282775402069092,
      "learning_rate": 3.92198639803392e-05,
      "loss": 0.6579,
      "step": 1417700
    },
    {
      "epoch": 12.937075698956129,
      "grad_norm": 4.638486862182617,
      "learning_rate": 3.9219103584203225e-05,
      "loss": 0.6967,
      "step": 1417800
    },
    {
      "epoch": 12.937988174319294,
      "grad_norm": 4.1768035888671875,
      "learning_rate": 3.9218343188067255e-05,
      "loss": 0.6434,
      "step": 1417900
    },
    {
      "epoch": 12.93890064968246,
      "grad_norm": 4.920854091644287,
      "learning_rate": 3.9217582791931285e-05,
      "loss": 0.6505,
      "step": 1418000
    },
    {
      "epoch": 12.939813125045625,
      "grad_norm": 4.574422836303711,
      "learning_rate": 3.9216822395795315e-05,
      "loss": 0.658,
      "step": 1418100
    },
    {
      "epoch": 12.94072560040879,
      "grad_norm": 3.327658176422119,
      "learning_rate": 3.9216061999659345e-05,
      "loss": 0.6272,
      "step": 1418200
    },
    {
      "epoch": 12.941638075771953,
      "grad_norm": 2.661412000656128,
      "learning_rate": 3.9215301603523375e-05,
      "loss": 0.7063,
      "step": 1418300
    },
    {
      "epoch": 12.942550551135119,
      "grad_norm": 3.827039957046509,
      "learning_rate": 3.92145412073874e-05,
      "loss": 0.684,
      "step": 1418400
    },
    {
      "epoch": 12.943463026498284,
      "grad_norm": 4.099288463592529,
      "learning_rate": 3.9213780811251435e-05,
      "loss": 0.6662,
      "step": 1418500
    },
    {
      "epoch": 12.94437550186145,
      "grad_norm": 4.041752338409424,
      "learning_rate": 3.921302041511546e-05,
      "loss": 0.6894,
      "step": 1418600
    },
    {
      "epoch": 12.945287977224615,
      "grad_norm": 3.8000264167785645,
      "learning_rate": 3.921226001897949e-05,
      "loss": 0.6894,
      "step": 1418700
    },
    {
      "epoch": 12.94620045258778,
      "grad_norm": 3.4446661472320557,
      "learning_rate": 3.921149962284352e-05,
      "loss": 0.6874,
      "step": 1418800
    },
    {
      "epoch": 12.947112927950945,
      "grad_norm": 4.083657264709473,
      "learning_rate": 3.921073922670755e-05,
      "loss": 0.6999,
      "step": 1418900
    },
    {
      "epoch": 12.94802540331411,
      "grad_norm": 4.233622074127197,
      "learning_rate": 3.920997883057157e-05,
      "loss": 0.7222,
      "step": 1419000
    },
    {
      "epoch": 12.948937878677276,
      "grad_norm": 4.252343654632568,
      "learning_rate": 3.920921843443561e-05,
      "loss": 0.6719,
      "step": 1419100
    },
    {
      "epoch": 12.949850354040441,
      "grad_norm": 4.065112590789795,
      "learning_rate": 3.920845803829963e-05,
      "loss": 0.7119,
      "step": 1419200
    },
    {
      "epoch": 12.950762829403606,
      "grad_norm": 3.9498441219329834,
      "learning_rate": 3.920769764216366e-05,
      "loss": 0.6619,
      "step": 1419300
    },
    {
      "epoch": 12.951675304766772,
      "grad_norm": 3.6027026176452637,
      "learning_rate": 3.920693724602769e-05,
      "loss": 0.6635,
      "step": 1419400
    },
    {
      "epoch": 12.952587780129937,
      "grad_norm": 4.0593390464782715,
      "learning_rate": 3.920617684989172e-05,
      "loss": 0.6814,
      "step": 1419500
    },
    {
      "epoch": 12.953500255493102,
      "grad_norm": 4.8475470542907715,
      "learning_rate": 3.920541645375575e-05,
      "loss": 0.657,
      "step": 1419600
    },
    {
      "epoch": 12.954412730856268,
      "grad_norm": 3.806800365447998,
      "learning_rate": 3.920465605761978e-05,
      "loss": 0.6797,
      "step": 1419700
    },
    {
      "epoch": 12.955325206219433,
      "grad_norm": 4.459456443786621,
      "learning_rate": 3.9203895661483806e-05,
      "loss": 0.6598,
      "step": 1419800
    },
    {
      "epoch": 12.956237681582596,
      "grad_norm": 4.038919925689697,
      "learning_rate": 3.920313526534784e-05,
      "loss": 0.6477,
      "step": 1419900
    },
    {
      "epoch": 12.957150156945762,
      "grad_norm": 3.8731465339660645,
      "learning_rate": 3.9202374869211866e-05,
      "loss": 0.6625,
      "step": 1420000
    },
    {
      "epoch": 12.958062632308927,
      "grad_norm": 3.898569345474243,
      "learning_rate": 3.920161447307589e-05,
      "loss": 0.635,
      "step": 1420100
    },
    {
      "epoch": 12.958975107672092,
      "grad_norm": 3.878936290740967,
      "learning_rate": 3.9200854076939926e-05,
      "loss": 0.7329,
      "step": 1420200
    },
    {
      "epoch": 12.959887583035258,
      "grad_norm": 5.089770793914795,
      "learning_rate": 3.920009368080395e-05,
      "loss": 0.6719,
      "step": 1420300
    },
    {
      "epoch": 12.960800058398423,
      "grad_norm": 3.5845110416412354,
      "learning_rate": 3.919933328466798e-05,
      "loss": 0.6699,
      "step": 1420400
    },
    {
      "epoch": 12.961712533761588,
      "grad_norm": 4.049802303314209,
      "learning_rate": 3.919857288853201e-05,
      "loss": 0.648,
      "step": 1420500
    },
    {
      "epoch": 12.962625009124753,
      "grad_norm": 3.691650629043579,
      "learning_rate": 3.919781249239604e-05,
      "loss": 0.6347,
      "step": 1420600
    },
    {
      "epoch": 12.963537484487919,
      "grad_norm": 5.223977565765381,
      "learning_rate": 3.919705209626007e-05,
      "loss": 0.678,
      "step": 1420700
    },
    {
      "epoch": 12.964449959851084,
      "grad_norm": 3.56923770904541,
      "learning_rate": 3.91962917001241e-05,
      "loss": 0.6783,
      "step": 1420800
    },
    {
      "epoch": 12.96536243521425,
      "grad_norm": 3.768883228302002,
      "learning_rate": 3.919553130398812e-05,
      "loss": 0.6907,
      "step": 1420900
    },
    {
      "epoch": 12.966274910577415,
      "grad_norm": 4.413498878479004,
      "learning_rate": 3.919477090785216e-05,
      "loss": 0.6594,
      "step": 1421000
    },
    {
      "epoch": 12.96718738594058,
      "grad_norm": 4.143632888793945,
      "learning_rate": 3.919401051171618e-05,
      "loss": 0.6759,
      "step": 1421100
    },
    {
      "epoch": 12.968099861303745,
      "grad_norm": 3.690627336502075,
      "learning_rate": 3.919325011558021e-05,
      "loss": 0.6598,
      "step": 1421200
    },
    {
      "epoch": 12.96901233666691,
      "grad_norm": 3.4453694820404053,
      "learning_rate": 3.9192489719444243e-05,
      "loss": 0.6417,
      "step": 1421300
    },
    {
      "epoch": 12.969924812030076,
      "grad_norm": 3.6382932662963867,
      "learning_rate": 3.9191729323308274e-05,
      "loss": 0.6815,
      "step": 1421400
    },
    {
      "epoch": 12.970837287393241,
      "grad_norm": 2.5576794147491455,
      "learning_rate": 3.91909689271723e-05,
      "loss": 0.6928,
      "step": 1421500
    },
    {
      "epoch": 12.971749762756406,
      "grad_norm": 4.4894819259643555,
      "learning_rate": 3.9190208531036334e-05,
      "loss": 0.6914,
      "step": 1421600
    },
    {
      "epoch": 12.97266223811957,
      "grad_norm": 3.743574857711792,
      "learning_rate": 3.918944813490036e-05,
      "loss": 0.661,
      "step": 1421700
    },
    {
      "epoch": 12.973574713482735,
      "grad_norm": 3.512014150619507,
      "learning_rate": 3.918868773876439e-05,
      "loss": 0.68,
      "step": 1421800
    },
    {
      "epoch": 12.9744871888459,
      "grad_norm": 4.319032669067383,
      "learning_rate": 3.918792734262842e-05,
      "loss": 0.6479,
      "step": 1421900
    },
    {
      "epoch": 12.975399664209066,
      "grad_norm": 3.602238178253174,
      "learning_rate": 3.918716694649245e-05,
      "loss": 0.6603,
      "step": 1422000
    },
    {
      "epoch": 12.976312139572231,
      "grad_norm": 4.696288108825684,
      "learning_rate": 3.918640655035648e-05,
      "loss": 0.7209,
      "step": 1422100
    },
    {
      "epoch": 12.977224614935396,
      "grad_norm": 4.1585693359375,
      "learning_rate": 3.918564615422051e-05,
      "loss": 0.6916,
      "step": 1422200
    },
    {
      "epoch": 12.978137090298562,
      "grad_norm": 3.9751832485198975,
      "learning_rate": 3.918488575808453e-05,
      "loss": 0.6914,
      "step": 1422300
    },
    {
      "epoch": 12.979049565661727,
      "grad_norm": 3.753432035446167,
      "learning_rate": 3.918412536194857e-05,
      "loss": 0.6753,
      "step": 1422400
    },
    {
      "epoch": 12.979962041024892,
      "grad_norm": 3.6676175594329834,
      "learning_rate": 3.918336496581259e-05,
      "loss": 0.6819,
      "step": 1422500
    },
    {
      "epoch": 12.980874516388058,
      "grad_norm": 4.910478115081787,
      "learning_rate": 3.918260456967662e-05,
      "loss": 0.6527,
      "step": 1422600
    },
    {
      "epoch": 12.981786991751223,
      "grad_norm": 4.387074947357178,
      "learning_rate": 3.918184417354065e-05,
      "loss": 0.7087,
      "step": 1422700
    },
    {
      "epoch": 12.982699467114388,
      "grad_norm": 4.01729679107666,
      "learning_rate": 3.918108377740468e-05,
      "loss": 0.6867,
      "step": 1422800
    },
    {
      "epoch": 12.983611942477554,
      "grad_norm": 4.5215277671813965,
      "learning_rate": 3.918032338126871e-05,
      "loss": 0.6596,
      "step": 1422900
    },
    {
      "epoch": 12.984524417840719,
      "grad_norm": 3.574986219406128,
      "learning_rate": 3.9179562985132734e-05,
      "loss": 0.677,
      "step": 1423000
    },
    {
      "epoch": 12.985436893203884,
      "grad_norm": 4.3712077140808105,
      "learning_rate": 3.9178802588996764e-05,
      "loss": 0.7033,
      "step": 1423100
    },
    {
      "epoch": 12.98634936856705,
      "grad_norm": 3.5138943195343018,
      "learning_rate": 3.9178042192860794e-05,
      "loss": 0.7013,
      "step": 1423200
    },
    {
      "epoch": 12.987261843930213,
      "grad_norm": 3.3581604957580566,
      "learning_rate": 3.9177281796724824e-05,
      "loss": 0.6691,
      "step": 1423300
    },
    {
      "epoch": 12.988174319293378,
      "grad_norm": 3.565850257873535,
      "learning_rate": 3.917652140058885e-05,
      "loss": 0.6666,
      "step": 1423400
    },
    {
      "epoch": 12.989086794656544,
      "grad_norm": 3.7161166667938232,
      "learning_rate": 3.9175761004452885e-05,
      "loss": 0.6851,
      "step": 1423500
    },
    {
      "epoch": 12.989999270019709,
      "grad_norm": 3.5117220878601074,
      "learning_rate": 3.917500060831691e-05,
      "loss": 0.6518,
      "step": 1423600
    },
    {
      "epoch": 12.990911745382874,
      "grad_norm": 2.2057440280914307,
      "learning_rate": 3.917424021218094e-05,
      "loss": 0.6546,
      "step": 1423700
    },
    {
      "epoch": 12.99182422074604,
      "grad_norm": 3.701448678970337,
      "learning_rate": 3.917347981604497e-05,
      "loss": 0.6913,
      "step": 1423800
    },
    {
      "epoch": 12.992736696109205,
      "grad_norm": 3.966689109802246,
      "learning_rate": 3.9172719419909e-05,
      "loss": 0.6826,
      "step": 1423900
    },
    {
      "epoch": 12.99364917147237,
      "grad_norm": 4.247988700866699,
      "learning_rate": 3.917195902377303e-05,
      "loss": 0.6682,
      "step": 1424000
    },
    {
      "epoch": 12.994561646835535,
      "grad_norm": 4.255420684814453,
      "learning_rate": 3.917119862763706e-05,
      "loss": 0.672,
      "step": 1424100
    },
    {
      "epoch": 12.9954741221987,
      "grad_norm": 3.3218798637390137,
      "learning_rate": 3.917043823150108e-05,
      "loss": 0.6852,
      "step": 1424200
    },
    {
      "epoch": 12.996386597561866,
      "grad_norm": 3.6613078117370605,
      "learning_rate": 3.916967783536512e-05,
      "loss": 0.6729,
      "step": 1424300
    },
    {
      "epoch": 12.997299072925031,
      "grad_norm": 5.091186046600342,
      "learning_rate": 3.916891743922914e-05,
      "loss": 0.6908,
      "step": 1424400
    },
    {
      "epoch": 12.998211548288197,
      "grad_norm": 3.28472638130188,
      "learning_rate": 3.916815704309317e-05,
      "loss": 0.675,
      "step": 1424500
    },
    {
      "epoch": 12.999124023651362,
      "grad_norm": 4.003270149230957,
      "learning_rate": 3.91673966469572e-05,
      "loss": 0.6974,
      "step": 1424600
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.5468429923057556,
      "eval_runtime": 27.0084,
      "eval_samples_per_second": 213.6,
      "eval_steps_per_second": 213.6,
      "step": 1424696
    },
    {
      "epoch": 13.0,
      "eval_loss": 0.5253958702087402,
      "eval_runtime": 507.6498,
      "eval_samples_per_second": 215.881,
      "eval_steps_per_second": 215.881,
      "step": 1424696
    },
    {
      "epoch": 13.000036499014527,
      "grad_norm": 3.7112960815429688,
      "learning_rate": 3.916663625082123e-05,
      "loss": 0.6504,
      "step": 1424700
    },
    {
      "epoch": 13.000948974377692,
      "grad_norm": 4.250916957855225,
      "learning_rate": 3.9165875854685255e-05,
      "loss": 0.6116,
      "step": 1424800
    },
    {
      "epoch": 13.001861449740858,
      "grad_norm": 4.308602809906006,
      "learning_rate": 3.916511545854929e-05,
      "loss": 0.6788,
      "step": 1424900
    },
    {
      "epoch": 13.002773925104023,
      "grad_norm": 4.502345561981201,
      "learning_rate": 3.9164355062413315e-05,
      "loss": 0.6328,
      "step": 1425000
    },
    {
      "epoch": 13.003686400467187,
      "grad_norm": 4.120083332061768,
      "learning_rate": 3.9163594666277345e-05,
      "loss": 0.6831,
      "step": 1425100
    },
    {
      "epoch": 13.004598875830352,
      "grad_norm": 5.037665843963623,
      "learning_rate": 3.9162834270141375e-05,
      "loss": 0.6925,
      "step": 1425200
    },
    {
      "epoch": 13.005511351193517,
      "grad_norm": 2.339367389678955,
      "learning_rate": 3.9162073874005405e-05,
      "loss": 0.6539,
      "step": 1425300
    },
    {
      "epoch": 13.006423826556682,
      "grad_norm": 3.971539258956909,
      "learning_rate": 3.9161313477869436e-05,
      "loss": 0.6469,
      "step": 1425400
    },
    {
      "epoch": 13.007336301919848,
      "grad_norm": 3.5213730335235596,
      "learning_rate": 3.9160553081733466e-05,
      "loss": 0.6396,
      "step": 1425500
    },
    {
      "epoch": 13.008248777283013,
      "grad_norm": 4.773686408996582,
      "learning_rate": 3.915979268559749e-05,
      "loss": 0.6589,
      "step": 1425600
    },
    {
      "epoch": 13.009161252646178,
      "grad_norm": 4.265814781188965,
      "learning_rate": 3.9159032289461526e-05,
      "loss": 0.6883,
      "step": 1425700
    },
    {
      "epoch": 13.010073728009344,
      "grad_norm": 4.50884485244751,
      "learning_rate": 3.915827189332555e-05,
      "loss": 0.644,
      "step": 1425800
    },
    {
      "epoch": 13.010986203372509,
      "grad_norm": 3.6044697761535645,
      "learning_rate": 3.915751149718957e-05,
      "loss": 0.7077,
      "step": 1425900
    },
    {
      "epoch": 13.011898678735674,
      "grad_norm": 4.618656635284424,
      "learning_rate": 3.915675110105361e-05,
      "loss": 0.6488,
      "step": 1426000
    },
    {
      "epoch": 13.01281115409884,
      "grad_norm": 3.4018185138702393,
      "learning_rate": 3.915599070491763e-05,
      "loss": 0.6416,
      "step": 1426100
    },
    {
      "epoch": 13.013723629462005,
      "grad_norm": 3.254654884338379,
      "learning_rate": 3.915523030878166e-05,
      "loss": 0.6607,
      "step": 1426200
    },
    {
      "epoch": 13.01463610482517,
      "grad_norm": 4.169939994812012,
      "learning_rate": 3.915446991264569e-05,
      "loss": 0.67,
      "step": 1426300
    },
    {
      "epoch": 13.015548580188335,
      "grad_norm": 3.838364362716675,
      "learning_rate": 3.915370951650972e-05,
      "loss": 0.6542,
      "step": 1426400
    },
    {
      "epoch": 13.0164610555515,
      "grad_norm": 2.1543397903442383,
      "learning_rate": 3.915294912037375e-05,
      "loss": 0.6446,
      "step": 1426500
    },
    {
      "epoch": 13.017373530914666,
      "grad_norm": 4.516173839569092,
      "learning_rate": 3.915218872423778e-05,
      "loss": 0.6775,
      "step": 1426600
    },
    {
      "epoch": 13.01828600627783,
      "grad_norm": 3.2946245670318604,
      "learning_rate": 3.9151428328101806e-05,
      "loss": 0.6347,
      "step": 1426700
    },
    {
      "epoch": 13.019198481640995,
      "grad_norm": 3.986811399459839,
      "learning_rate": 3.915066793196584e-05,
      "loss": 0.687,
      "step": 1426800
    },
    {
      "epoch": 13.02011095700416,
      "grad_norm": 3.896707534790039,
      "learning_rate": 3.9149907535829866e-05,
      "loss": 0.6484,
      "step": 1426900
    },
    {
      "epoch": 13.021023432367326,
      "grad_norm": 4.715330123901367,
      "learning_rate": 3.9149147139693896e-05,
      "loss": 0.6954,
      "step": 1427000
    },
    {
      "epoch": 13.02193590773049,
      "grad_norm": 4.892096519470215,
      "learning_rate": 3.9148386743557926e-05,
      "loss": 0.6277,
      "step": 1427100
    },
    {
      "epoch": 13.022848383093656,
      "grad_norm": 4.134394645690918,
      "learning_rate": 3.9147626347421956e-05,
      "loss": 0.6519,
      "step": 1427200
    },
    {
      "epoch": 13.023760858456821,
      "grad_norm": 4.017654895782471,
      "learning_rate": 3.914686595128598e-05,
      "loss": 0.7069,
      "step": 1427300
    },
    {
      "epoch": 13.024673333819987,
      "grad_norm": 4.001332759857178,
      "learning_rate": 3.9146105555150017e-05,
      "loss": 0.6683,
      "step": 1427400
    },
    {
      "epoch": 13.025585809183152,
      "grad_norm": 3.966547727584839,
      "learning_rate": 3.914534515901404e-05,
      "loss": 0.6612,
      "step": 1427500
    },
    {
      "epoch": 13.026498284546317,
      "grad_norm": 3.3564612865448,
      "learning_rate": 3.914458476287807e-05,
      "loss": 0.6476,
      "step": 1427600
    },
    {
      "epoch": 13.027410759909483,
      "grad_norm": 3.907914638519287,
      "learning_rate": 3.91438243667421e-05,
      "loss": 0.6743,
      "step": 1427700
    },
    {
      "epoch": 13.028323235272648,
      "grad_norm": 4.528105735778809,
      "learning_rate": 3.914306397060613e-05,
      "loss": 0.6736,
      "step": 1427800
    },
    {
      "epoch": 13.029235710635813,
      "grad_norm": 3.8652026653289795,
      "learning_rate": 3.914230357447016e-05,
      "loss": 0.6928,
      "step": 1427900
    },
    {
      "epoch": 13.030148185998979,
      "grad_norm": 4.980919361114502,
      "learning_rate": 3.914154317833419e-05,
      "loss": 0.6811,
      "step": 1428000
    },
    {
      "epoch": 13.031060661362144,
      "grad_norm": 3.1232917308807373,
      "learning_rate": 3.9140782782198213e-05,
      "loss": 0.6815,
      "step": 1428100
    },
    {
      "epoch": 13.031973136725309,
      "grad_norm": 4.309254169464111,
      "learning_rate": 3.914002238606225e-05,
      "loss": 0.7041,
      "step": 1428200
    },
    {
      "epoch": 13.032885612088474,
      "grad_norm": 3.8215808868408203,
      "learning_rate": 3.9139261989926274e-05,
      "loss": 0.6851,
      "step": 1428300
    },
    {
      "epoch": 13.033798087451638,
      "grad_norm": 4.291673183441162,
      "learning_rate": 3.9138501593790304e-05,
      "loss": 0.6399,
      "step": 1428400
    },
    {
      "epoch": 13.034710562814803,
      "grad_norm": 3.3127248287200928,
      "learning_rate": 3.9137741197654334e-05,
      "loss": 0.6785,
      "step": 1428500
    },
    {
      "epoch": 13.035623038177969,
      "grad_norm": 3.830745220184326,
      "learning_rate": 3.913698080151836e-05,
      "loss": 0.6881,
      "step": 1428600
    },
    {
      "epoch": 13.036535513541134,
      "grad_norm": 3.8758790493011475,
      "learning_rate": 3.913622040538239e-05,
      "loss": 0.6526,
      "step": 1428700
    },
    {
      "epoch": 13.0374479889043,
      "grad_norm": 5.007668495178223,
      "learning_rate": 3.913546000924642e-05,
      "loss": 0.6902,
      "step": 1428800
    },
    {
      "epoch": 13.038360464267464,
      "grad_norm": 4.6434407234191895,
      "learning_rate": 3.913469961311045e-05,
      "loss": 0.6359,
      "step": 1428900
    },
    {
      "epoch": 13.03927293963063,
      "grad_norm": 4.446800231933594,
      "learning_rate": 3.913393921697448e-05,
      "loss": 0.6596,
      "step": 1429000
    },
    {
      "epoch": 13.040185414993795,
      "grad_norm": 4.483494758605957,
      "learning_rate": 3.913317882083851e-05,
      "loss": 0.6646,
      "step": 1429100
    },
    {
      "epoch": 13.04109789035696,
      "grad_norm": 3.7910425662994385,
      "learning_rate": 3.913241842470253e-05,
      "loss": 0.6686,
      "step": 1429200
    },
    {
      "epoch": 13.042010365720126,
      "grad_norm": 4.32835054397583,
      "learning_rate": 3.913165802856657e-05,
      "loss": 0.6485,
      "step": 1429300
    },
    {
      "epoch": 13.042922841083291,
      "grad_norm": 5.178742408752441,
      "learning_rate": 3.913089763243059e-05,
      "loss": 0.6453,
      "step": 1429400
    },
    {
      "epoch": 13.043835316446456,
      "grad_norm": 3.591531276702881,
      "learning_rate": 3.913013723629462e-05,
      "loss": 0.6417,
      "step": 1429500
    },
    {
      "epoch": 13.044747791809622,
      "grad_norm": 4.454373359680176,
      "learning_rate": 3.912937684015865e-05,
      "loss": 0.6562,
      "step": 1429600
    },
    {
      "epoch": 13.045660267172787,
      "grad_norm": 4.381999969482422,
      "learning_rate": 3.912861644402268e-05,
      "loss": 0.6389,
      "step": 1429700
    },
    {
      "epoch": 13.046572742535952,
      "grad_norm": 4.721024036407471,
      "learning_rate": 3.9127856047886704e-05,
      "loss": 0.6335,
      "step": 1429800
    },
    {
      "epoch": 13.047485217899117,
      "grad_norm": 5.255453109741211,
      "learning_rate": 3.912709565175074e-05,
      "loss": 0.7071,
      "step": 1429900
    },
    {
      "epoch": 13.048397693262283,
      "grad_norm": 4.800793647766113,
      "learning_rate": 3.9126335255614764e-05,
      "loss": 0.6343,
      "step": 1430000
    },
    {
      "epoch": 13.049310168625446,
      "grad_norm": 2.6497626304626465,
      "learning_rate": 3.9125574859478794e-05,
      "loss": 0.656,
      "step": 1430100
    },
    {
      "epoch": 13.050222643988612,
      "grad_norm": 4.761957168579102,
      "learning_rate": 3.9124814463342825e-05,
      "loss": 0.6711,
      "step": 1430200
    },
    {
      "epoch": 13.051135119351777,
      "grad_norm": 3.990277051925659,
      "learning_rate": 3.9124054067206855e-05,
      "loss": 0.6534,
      "step": 1430300
    },
    {
      "epoch": 13.052047594714942,
      "grad_norm": 4.158090591430664,
      "learning_rate": 3.9123293671070885e-05,
      "loss": 0.6679,
      "step": 1430400
    },
    {
      "epoch": 13.052960070078107,
      "grad_norm": 3.614713430404663,
      "learning_rate": 3.9122533274934915e-05,
      "loss": 0.6737,
      "step": 1430500
    },
    {
      "epoch": 13.053872545441273,
      "grad_norm": 5.213244438171387,
      "learning_rate": 3.912177287879894e-05,
      "loss": 0.6652,
      "step": 1430600
    },
    {
      "epoch": 13.054785020804438,
      "grad_norm": 4.681769371032715,
      "learning_rate": 3.9121012482662975e-05,
      "loss": 0.6432,
      "step": 1430700
    },
    {
      "epoch": 13.055697496167603,
      "grad_norm": 4.340596675872803,
      "learning_rate": 3.9120252086527e-05,
      "loss": 0.6904,
      "step": 1430800
    },
    {
      "epoch": 13.056609971530769,
      "grad_norm": 4.031840801239014,
      "learning_rate": 3.911949169039103e-05,
      "loss": 0.6926,
      "step": 1430900
    },
    {
      "epoch": 13.057522446893934,
      "grad_norm": 3.013465642929077,
      "learning_rate": 3.911873129425506e-05,
      "loss": 0.6792,
      "step": 1431000
    },
    {
      "epoch": 13.0584349222571,
      "grad_norm": 4.428802013397217,
      "learning_rate": 3.911797089811909e-05,
      "loss": 0.6872,
      "step": 1431100
    },
    {
      "epoch": 13.059347397620265,
      "grad_norm": 3.863449811935425,
      "learning_rate": 3.911721050198311e-05,
      "loss": 0.6726,
      "step": 1431200
    },
    {
      "epoch": 13.06025987298343,
      "grad_norm": 2.825286865234375,
      "learning_rate": 3.911645010584715e-05,
      "loss": 0.6476,
      "step": 1431300
    },
    {
      "epoch": 13.061172348346595,
      "grad_norm": 4.280086517333984,
      "learning_rate": 3.911568970971117e-05,
      "loss": 0.6937,
      "step": 1431400
    },
    {
      "epoch": 13.06208482370976,
      "grad_norm": 4.618747711181641,
      "learning_rate": 3.91149293135752e-05,
      "loss": 0.6275,
      "step": 1431500
    },
    {
      "epoch": 13.062997299072926,
      "grad_norm": 4.10546350479126,
      "learning_rate": 3.911416891743923e-05,
      "loss": 0.668,
      "step": 1431600
    },
    {
      "epoch": 13.063909774436091,
      "grad_norm": 3.9833290576934814,
      "learning_rate": 3.9113408521303255e-05,
      "loss": 0.6756,
      "step": 1431700
    },
    {
      "epoch": 13.064822249799255,
      "grad_norm": 3.8734474182128906,
      "learning_rate": 3.911264812516729e-05,
      "loss": 0.6597,
      "step": 1431800
    },
    {
      "epoch": 13.06573472516242,
      "grad_norm": 4.65600061416626,
      "learning_rate": 3.9111887729031315e-05,
      "loss": 0.6977,
      "step": 1431900
    },
    {
      "epoch": 13.066647200525585,
      "grad_norm": 3.958707571029663,
      "learning_rate": 3.9111127332895345e-05,
      "loss": 0.6836,
      "step": 1432000
    },
    {
      "epoch": 13.06755967588875,
      "grad_norm": 3.9766461849212646,
      "learning_rate": 3.9110366936759376e-05,
      "loss": 0.6455,
      "step": 1432100
    },
    {
      "epoch": 13.068472151251916,
      "grad_norm": 3.35762357711792,
      "learning_rate": 3.9109606540623406e-05,
      "loss": 0.6334,
      "step": 1432200
    },
    {
      "epoch": 13.069384626615081,
      "grad_norm": 2.4690935611724854,
      "learning_rate": 3.910884614448743e-05,
      "loss": 0.6939,
      "step": 1432300
    },
    {
      "epoch": 13.070297101978246,
      "grad_norm": 3.9905734062194824,
      "learning_rate": 3.9108085748351466e-05,
      "loss": 0.6644,
      "step": 1432400
    },
    {
      "epoch": 13.071209577341412,
      "grad_norm": 4.105260372161865,
      "learning_rate": 3.910732535221549e-05,
      "loss": 0.6947,
      "step": 1432500
    },
    {
      "epoch": 13.072122052704577,
      "grad_norm": 4.573495388031006,
      "learning_rate": 3.910656495607952e-05,
      "loss": 0.6702,
      "step": 1432600
    },
    {
      "epoch": 13.073034528067742,
      "grad_norm": 4.972686767578125,
      "learning_rate": 3.910580455994355e-05,
      "loss": 0.6659,
      "step": 1432700
    },
    {
      "epoch": 13.073947003430908,
      "grad_norm": 4.059414386749268,
      "learning_rate": 3.910504416380758e-05,
      "loss": 0.671,
      "step": 1432800
    },
    {
      "epoch": 13.074859478794073,
      "grad_norm": 3.9499175548553467,
      "learning_rate": 3.910428376767161e-05,
      "loss": 0.6517,
      "step": 1432900
    },
    {
      "epoch": 13.075771954157238,
      "grad_norm": 2.826406955718994,
      "learning_rate": 3.910352337153564e-05,
      "loss": 0.6649,
      "step": 1433000
    },
    {
      "epoch": 13.076684429520403,
      "grad_norm": 3.675267219543457,
      "learning_rate": 3.910276297539966e-05,
      "loss": 0.6582,
      "step": 1433100
    },
    {
      "epoch": 13.077596904883569,
      "grad_norm": 3.868022918701172,
      "learning_rate": 3.91020025792637e-05,
      "loss": 0.6568,
      "step": 1433200
    },
    {
      "epoch": 13.078509380246734,
      "grad_norm": 2.7961063385009766,
      "learning_rate": 3.910124218312772e-05,
      "loss": 0.6714,
      "step": 1433300
    },
    {
      "epoch": 13.0794218556099,
      "grad_norm": 3.0487992763519287,
      "learning_rate": 3.910048178699175e-05,
      "loss": 0.6601,
      "step": 1433400
    },
    {
      "epoch": 13.080334330973063,
      "grad_norm": 3.607262372970581,
      "learning_rate": 3.909972139085578e-05,
      "loss": 0.6432,
      "step": 1433500
    },
    {
      "epoch": 13.081246806336228,
      "grad_norm": 4.190210819244385,
      "learning_rate": 3.909896099471981e-05,
      "loss": 0.6975,
      "step": 1433600
    },
    {
      "epoch": 13.082159281699393,
      "grad_norm": 4.378073692321777,
      "learning_rate": 3.9098200598583836e-05,
      "loss": 0.6849,
      "step": 1433700
    },
    {
      "epoch": 13.083071757062559,
      "grad_norm": 2.3581814765930176,
      "learning_rate": 3.909744020244787e-05,
      "loss": 0.6572,
      "step": 1433800
    },
    {
      "epoch": 13.083984232425724,
      "grad_norm": 4.720132827758789,
      "learning_rate": 3.9096679806311896e-05,
      "loss": 0.7108,
      "step": 1433900
    },
    {
      "epoch": 13.08489670778889,
      "grad_norm": 4.0639753341674805,
      "learning_rate": 3.9095919410175926e-05,
      "loss": 0.6947,
      "step": 1434000
    },
    {
      "epoch": 13.085809183152055,
      "grad_norm": 3.7366268634796143,
      "learning_rate": 3.9095159014039957e-05,
      "loss": 0.6681,
      "step": 1434100
    },
    {
      "epoch": 13.08672165851522,
      "grad_norm": 5.030182838439941,
      "learning_rate": 3.9094398617903987e-05,
      "loss": 0.6758,
      "step": 1434200
    },
    {
      "epoch": 13.087634133878385,
      "grad_norm": 2.7585649490356445,
      "learning_rate": 3.909363822176802e-05,
      "loss": 0.6881,
      "step": 1434300
    },
    {
      "epoch": 13.08854660924155,
      "grad_norm": 4.204470157623291,
      "learning_rate": 3.909287782563204e-05,
      "loss": 0.638,
      "step": 1434400
    },
    {
      "epoch": 13.089459084604716,
      "grad_norm": 3.4832615852355957,
      "learning_rate": 3.909211742949607e-05,
      "loss": 0.6652,
      "step": 1434500
    },
    {
      "epoch": 13.090371559967881,
      "grad_norm": 4.242568016052246,
      "learning_rate": 3.90913570333601e-05,
      "loss": 0.6565,
      "step": 1434600
    },
    {
      "epoch": 13.091284035331046,
      "grad_norm": 4.18917179107666,
      "learning_rate": 3.909059663722413e-05,
      "loss": 0.6906,
      "step": 1434700
    },
    {
      "epoch": 13.092196510694212,
      "grad_norm": 4.728028297424316,
      "learning_rate": 3.908983624108816e-05,
      "loss": 0.685,
      "step": 1434800
    },
    {
      "epoch": 13.093108986057377,
      "grad_norm": 3.9188661575317383,
      "learning_rate": 3.908907584495219e-05,
      "loss": 0.6926,
      "step": 1434900
    },
    {
      "epoch": 13.094021461420542,
      "grad_norm": 4.206621170043945,
      "learning_rate": 3.9088315448816214e-05,
      "loss": 0.6496,
      "step": 1435000
    },
    {
      "epoch": 13.094933936783708,
      "grad_norm": 3.157294988632202,
      "learning_rate": 3.9087555052680244e-05,
      "loss": 0.6621,
      "step": 1435100
    },
    {
      "epoch": 13.095846412146871,
      "grad_norm": 4.276500701904297,
      "learning_rate": 3.9086794656544274e-05,
      "loss": 0.6496,
      "step": 1435200
    },
    {
      "epoch": 13.096758887510036,
      "grad_norm": 4.881809234619141,
      "learning_rate": 3.9086034260408304e-05,
      "loss": 0.7152,
      "step": 1435300
    },
    {
      "epoch": 13.097671362873202,
      "grad_norm": 4.785695552825928,
      "learning_rate": 3.9085273864272334e-05,
      "loss": 0.6835,
      "step": 1435400
    },
    {
      "epoch": 13.098583838236367,
      "grad_norm": 3.7310338020324707,
      "learning_rate": 3.9084513468136364e-05,
      "loss": 0.6581,
      "step": 1435500
    },
    {
      "epoch": 13.099496313599532,
      "grad_norm": 2.828970193862915,
      "learning_rate": 3.908375307200039e-05,
      "loss": 0.6931,
      "step": 1435600
    },
    {
      "epoch": 13.100408788962698,
      "grad_norm": 4.510997772216797,
      "learning_rate": 3.9082992675864424e-05,
      "loss": 0.6676,
      "step": 1435700
    },
    {
      "epoch": 13.101321264325863,
      "grad_norm": 3.4666101932525635,
      "learning_rate": 3.908223227972845e-05,
      "loss": 0.6396,
      "step": 1435800
    },
    {
      "epoch": 13.102233739689028,
      "grad_norm": 4.347177982330322,
      "learning_rate": 3.908147188359248e-05,
      "loss": 0.6761,
      "step": 1435900
    },
    {
      "epoch": 13.103146215052194,
      "grad_norm": 4.646098613739014,
      "learning_rate": 3.908071148745651e-05,
      "loss": 0.6884,
      "step": 1436000
    },
    {
      "epoch": 13.104058690415359,
      "grad_norm": 4.645236968994141,
      "learning_rate": 3.907995109132054e-05,
      "loss": 0.6432,
      "step": 1436100
    },
    {
      "epoch": 13.104971165778524,
      "grad_norm": 5.529516220092773,
      "learning_rate": 3.907919069518457e-05,
      "loss": 0.6318,
      "step": 1436200
    },
    {
      "epoch": 13.10588364114169,
      "grad_norm": 3.5399367809295654,
      "learning_rate": 3.90784302990486e-05,
      "loss": 0.6807,
      "step": 1436300
    },
    {
      "epoch": 13.106796116504855,
      "grad_norm": 3.3034512996673584,
      "learning_rate": 3.907766990291262e-05,
      "loss": 0.6914,
      "step": 1436400
    },
    {
      "epoch": 13.10770859186802,
      "grad_norm": 5.718025207519531,
      "learning_rate": 3.907690950677665e-05,
      "loss": 0.6548,
      "step": 1436500
    },
    {
      "epoch": 13.108621067231185,
      "grad_norm": 3.414992332458496,
      "learning_rate": 3.907614911064068e-05,
      "loss": 0.6804,
      "step": 1436600
    },
    {
      "epoch": 13.10953354259435,
      "grad_norm": 3.952568531036377,
      "learning_rate": 3.907538871450471e-05,
      "loss": 0.6753,
      "step": 1436700
    },
    {
      "epoch": 13.110446017957516,
      "grad_norm": 3.7303085327148438,
      "learning_rate": 3.907462831836874e-05,
      "loss": 0.6549,
      "step": 1436800
    },
    {
      "epoch": 13.11135849332068,
      "grad_norm": 4.40911865234375,
      "learning_rate": 3.907386792223277e-05,
      "loss": 0.6705,
      "step": 1436900
    },
    {
      "epoch": 13.112270968683845,
      "grad_norm": 4.031291961669922,
      "learning_rate": 3.9073107526096795e-05,
      "loss": 0.7061,
      "step": 1437000
    },
    {
      "epoch": 13.11318344404701,
      "grad_norm": 4.02854061126709,
      "learning_rate": 3.907234712996083e-05,
      "loss": 0.6855,
      "step": 1437100
    },
    {
      "epoch": 13.114095919410175,
      "grad_norm": 4.0715556144714355,
      "learning_rate": 3.9071586733824855e-05,
      "loss": 0.7024,
      "step": 1437200
    },
    {
      "epoch": 13.11500839477334,
      "grad_norm": 4.326440811157227,
      "learning_rate": 3.9070826337688885e-05,
      "loss": 0.709,
      "step": 1437300
    },
    {
      "epoch": 13.115920870136506,
      "grad_norm": 3.947479248046875,
      "learning_rate": 3.9070065941552915e-05,
      "loss": 0.6852,
      "step": 1437400
    },
    {
      "epoch": 13.116833345499671,
      "grad_norm": 4.102457046508789,
      "learning_rate": 3.906930554541694e-05,
      "loss": 0.7016,
      "step": 1437500
    },
    {
      "epoch": 13.117745820862837,
      "grad_norm": 3.0130820274353027,
      "learning_rate": 3.9068545149280975e-05,
      "loss": 0.6594,
      "step": 1437600
    },
    {
      "epoch": 13.118658296226002,
      "grad_norm": 5.235327243804932,
      "learning_rate": 3.9067784753145e-05,
      "loss": 0.7121,
      "step": 1437700
    },
    {
      "epoch": 13.119570771589167,
      "grad_norm": 3.270643711090088,
      "learning_rate": 3.906702435700903e-05,
      "loss": 0.6799,
      "step": 1437800
    },
    {
      "epoch": 13.120483246952332,
      "grad_norm": 4.64224910736084,
      "learning_rate": 3.906626396087306e-05,
      "loss": 0.6955,
      "step": 1437900
    },
    {
      "epoch": 13.121395722315498,
      "grad_norm": 3.57458758354187,
      "learning_rate": 3.906550356473709e-05,
      "loss": 0.653,
      "step": 1438000
    },
    {
      "epoch": 13.122308197678663,
      "grad_norm": 5.453503131866455,
      "learning_rate": 3.906474316860111e-05,
      "loss": 0.6708,
      "step": 1438100
    },
    {
      "epoch": 13.123220673041828,
      "grad_norm": 3.469588279724121,
      "learning_rate": 3.906398277246515e-05,
      "loss": 0.6287,
      "step": 1438200
    },
    {
      "epoch": 13.124133148404994,
      "grad_norm": 3.1918785572052,
      "learning_rate": 3.906322237632917e-05,
      "loss": 0.6564,
      "step": 1438300
    },
    {
      "epoch": 13.125045623768159,
      "grad_norm": 3.687732219696045,
      "learning_rate": 3.90624619801932e-05,
      "loss": 0.6762,
      "step": 1438400
    },
    {
      "epoch": 13.125958099131324,
      "grad_norm": 4.289114952087402,
      "learning_rate": 3.906170158405723e-05,
      "loss": 0.6406,
      "step": 1438500
    },
    {
      "epoch": 13.126870574494488,
      "grad_norm": 3.597907304763794,
      "learning_rate": 3.906094118792126e-05,
      "loss": 0.6331,
      "step": 1438600
    },
    {
      "epoch": 13.127783049857653,
      "grad_norm": 4.942874908447266,
      "learning_rate": 3.906018079178529e-05,
      "loss": 0.6545,
      "step": 1438700
    },
    {
      "epoch": 13.128695525220818,
      "grad_norm": 3.462146520614624,
      "learning_rate": 3.905942039564932e-05,
      "loss": 0.6842,
      "step": 1438800
    },
    {
      "epoch": 13.129608000583984,
      "grad_norm": 3.8501980304718018,
      "learning_rate": 3.9058659999513346e-05,
      "loss": 0.6769,
      "step": 1438900
    },
    {
      "epoch": 13.130520475947149,
      "grad_norm": 4.469719886779785,
      "learning_rate": 3.905789960337738e-05,
      "loss": 0.6519,
      "step": 1439000
    },
    {
      "epoch": 13.131432951310314,
      "grad_norm": 4.221018314361572,
      "learning_rate": 3.9057139207241406e-05,
      "loss": 0.6896,
      "step": 1439100
    },
    {
      "epoch": 13.13234542667348,
      "grad_norm": 4.13150691986084,
      "learning_rate": 3.9056378811105436e-05,
      "loss": 0.6531,
      "step": 1439200
    },
    {
      "epoch": 13.133257902036645,
      "grad_norm": 4.947877407073975,
      "learning_rate": 3.9055618414969466e-05,
      "loss": 0.6313,
      "step": 1439300
    },
    {
      "epoch": 13.13417037739981,
      "grad_norm": 4.017882347106934,
      "learning_rate": 3.9054858018833496e-05,
      "loss": 0.6575,
      "step": 1439400
    },
    {
      "epoch": 13.135082852762975,
      "grad_norm": 4.321075439453125,
      "learning_rate": 3.905409762269752e-05,
      "loss": 0.6764,
      "step": 1439500
    },
    {
      "epoch": 13.13599532812614,
      "grad_norm": 2.5593016147613525,
      "learning_rate": 3.9053337226561556e-05,
      "loss": 0.6836,
      "step": 1439600
    },
    {
      "epoch": 13.136907803489306,
      "grad_norm": 4.0083441734313965,
      "learning_rate": 3.905257683042558e-05,
      "loss": 0.6904,
      "step": 1439700
    },
    {
      "epoch": 13.137820278852471,
      "grad_norm": 4.621308326721191,
      "learning_rate": 3.905181643428961e-05,
      "loss": 0.6394,
      "step": 1439800
    },
    {
      "epoch": 13.138732754215637,
      "grad_norm": 3.8034489154815674,
      "learning_rate": 3.905105603815364e-05,
      "loss": 0.6694,
      "step": 1439900
    },
    {
      "epoch": 13.139645229578802,
      "grad_norm": 4.052719593048096,
      "learning_rate": 3.905029564201766e-05,
      "loss": 0.7322,
      "step": 1440000
    },
    {
      "epoch": 13.140557704941967,
      "grad_norm": 3.724057197570801,
      "learning_rate": 3.90495352458817e-05,
      "loss": 0.6821,
      "step": 1440100
    },
    {
      "epoch": 13.141470180305133,
      "grad_norm": 4.409429550170898,
      "learning_rate": 3.904877484974572e-05,
      "loss": 0.6718,
      "step": 1440200
    },
    {
      "epoch": 13.142382655668296,
      "grad_norm": 4.75480842590332,
      "learning_rate": 3.904801445360975e-05,
      "loss": 0.6877,
      "step": 1440300
    },
    {
      "epoch": 13.143295131031461,
      "grad_norm": 4.629685401916504,
      "learning_rate": 3.904725405747378e-05,
      "loss": 0.6646,
      "step": 1440400
    },
    {
      "epoch": 13.144207606394627,
      "grad_norm": 4.651791572570801,
      "learning_rate": 3.904649366133781e-05,
      "loss": 0.6641,
      "step": 1440500
    },
    {
      "epoch": 13.145120081757792,
      "grad_norm": 3.1542558670043945,
      "learning_rate": 3.9045733265201836e-05,
      "loss": 0.6679,
      "step": 1440600
    },
    {
      "epoch": 13.146032557120957,
      "grad_norm": 3.7061469554901123,
      "learning_rate": 3.904497286906587e-05,
      "loss": 0.6935,
      "step": 1440700
    },
    {
      "epoch": 13.146945032484123,
      "grad_norm": 4.189933776855469,
      "learning_rate": 3.9044212472929896e-05,
      "loss": 0.6983,
      "step": 1440800
    },
    {
      "epoch": 13.147857507847288,
      "grad_norm": 3.9722673892974854,
      "learning_rate": 3.9043452076793927e-05,
      "loss": 0.6986,
      "step": 1440900
    },
    {
      "epoch": 13.148769983210453,
      "grad_norm": 4.480760097503662,
      "learning_rate": 3.904269168065796e-05,
      "loss": 0.6706,
      "step": 1441000
    },
    {
      "epoch": 13.149682458573619,
      "grad_norm": 3.7547783851623535,
      "learning_rate": 3.904193128452199e-05,
      "loss": 0.6882,
      "step": 1441100
    },
    {
      "epoch": 13.150594933936784,
      "grad_norm": 4.590991497039795,
      "learning_rate": 3.904117088838602e-05,
      "loss": 0.6998,
      "step": 1441200
    },
    {
      "epoch": 13.151507409299949,
      "grad_norm": 3.851227283477783,
      "learning_rate": 3.904041049225005e-05,
      "loss": 0.6189,
      "step": 1441300
    },
    {
      "epoch": 13.152419884663114,
      "grad_norm": 4.072377681732178,
      "learning_rate": 3.903965009611407e-05,
      "loss": 0.6661,
      "step": 1441400
    },
    {
      "epoch": 13.15333236002628,
      "grad_norm": 4.0984978675842285,
      "learning_rate": 3.903888969997811e-05,
      "loss": 0.65,
      "step": 1441500
    },
    {
      "epoch": 13.154244835389445,
      "grad_norm": 3.3432323932647705,
      "learning_rate": 3.903812930384213e-05,
      "loss": 0.6957,
      "step": 1441600
    },
    {
      "epoch": 13.15515731075261,
      "grad_norm": 3.2259955406188965,
      "learning_rate": 3.903736890770616e-05,
      "loss": 0.6771,
      "step": 1441700
    },
    {
      "epoch": 13.156069786115776,
      "grad_norm": 2.9422390460968018,
      "learning_rate": 3.903660851157019e-05,
      "loss": 0.6605,
      "step": 1441800
    },
    {
      "epoch": 13.156982261478941,
      "grad_norm": 3.303708553314209,
      "learning_rate": 3.903584811543422e-05,
      "loss": 0.696,
      "step": 1441900
    },
    {
      "epoch": 13.157894736842104,
      "grad_norm": 3.4819326400756836,
      "learning_rate": 3.9035087719298244e-05,
      "loss": 0.6924,
      "step": 1442000
    },
    {
      "epoch": 13.15880721220527,
      "grad_norm": 4.176125526428223,
      "learning_rate": 3.903432732316228e-05,
      "loss": 0.6597,
      "step": 1442100
    },
    {
      "epoch": 13.159719687568435,
      "grad_norm": 3.7519099712371826,
      "learning_rate": 3.9033566927026304e-05,
      "loss": 0.6579,
      "step": 1442200
    },
    {
      "epoch": 13.1606321629316,
      "grad_norm": 3.6525306701660156,
      "learning_rate": 3.9032806530890334e-05,
      "loss": 0.6665,
      "step": 1442300
    },
    {
      "epoch": 13.161544638294766,
      "grad_norm": 2.9369640350341797,
      "learning_rate": 3.9032046134754364e-05,
      "loss": 0.7064,
      "step": 1442400
    },
    {
      "epoch": 13.162457113657931,
      "grad_norm": 3.907498359680176,
      "learning_rate": 3.9031285738618394e-05,
      "loss": 0.6753,
      "step": 1442500
    },
    {
      "epoch": 13.163369589021096,
      "grad_norm": 3.5852692127227783,
      "learning_rate": 3.9030525342482424e-05,
      "loss": 0.6757,
      "step": 1442600
    },
    {
      "epoch": 13.164282064384262,
      "grad_norm": 3.6801953315734863,
      "learning_rate": 3.9029764946346454e-05,
      "loss": 0.6615,
      "step": 1442700
    },
    {
      "epoch": 13.165194539747427,
      "grad_norm": 3.9646191596984863,
      "learning_rate": 3.902900455021048e-05,
      "loss": 0.6517,
      "step": 1442800
    },
    {
      "epoch": 13.166107015110592,
      "grad_norm": 4.517578601837158,
      "learning_rate": 3.902824415407451e-05,
      "loss": 0.6442,
      "step": 1442900
    },
    {
      "epoch": 13.167019490473757,
      "grad_norm": 5.138129711151123,
      "learning_rate": 3.902748375793854e-05,
      "loss": 0.7327,
      "step": 1443000
    },
    {
      "epoch": 13.167931965836923,
      "grad_norm": 4.049099922180176,
      "learning_rate": 3.902672336180256e-05,
      "loss": 0.6449,
      "step": 1443100
    },
    {
      "epoch": 13.168844441200088,
      "grad_norm": 3.459836721420288,
      "learning_rate": 3.90259629656666e-05,
      "loss": 0.6621,
      "step": 1443200
    },
    {
      "epoch": 13.169756916563253,
      "grad_norm": 4.399148464202881,
      "learning_rate": 3.902520256953062e-05,
      "loss": 0.696,
      "step": 1443300
    },
    {
      "epoch": 13.170669391926419,
      "grad_norm": 3.631850004196167,
      "learning_rate": 3.902444217339465e-05,
      "loss": 0.6682,
      "step": 1443400
    },
    {
      "epoch": 13.171581867289584,
      "grad_norm": 4.255935192108154,
      "learning_rate": 3.902368177725868e-05,
      "loss": 0.6799,
      "step": 1443500
    },
    {
      "epoch": 13.17249434265275,
      "grad_norm": 3.5166122913360596,
      "learning_rate": 3.902292138112271e-05,
      "loss": 0.6902,
      "step": 1443600
    },
    {
      "epoch": 13.173406818015913,
      "grad_norm": 4.126502513885498,
      "learning_rate": 3.902216098498674e-05,
      "loss": 0.6551,
      "step": 1443700
    },
    {
      "epoch": 13.174319293379078,
      "grad_norm": 3.9572763442993164,
      "learning_rate": 3.902140058885077e-05,
      "loss": 0.6686,
      "step": 1443800
    },
    {
      "epoch": 13.175231768742243,
      "grad_norm": 3.8045501708984375,
      "learning_rate": 3.9020640192714795e-05,
      "loss": 0.6799,
      "step": 1443900
    },
    {
      "epoch": 13.176144244105409,
      "grad_norm": 4.436625003814697,
      "learning_rate": 3.901987979657883e-05,
      "loss": 0.6674,
      "step": 1444000
    },
    {
      "epoch": 13.177056719468574,
      "grad_norm": 2.9067189693450928,
      "learning_rate": 3.9019119400442855e-05,
      "loss": 0.671,
      "step": 1444100
    },
    {
      "epoch": 13.17796919483174,
      "grad_norm": 3.6444954872131348,
      "learning_rate": 3.9018359004306885e-05,
      "loss": 0.6215,
      "step": 1444200
    },
    {
      "epoch": 13.178881670194905,
      "grad_norm": 4.13814640045166,
      "learning_rate": 3.9017598608170915e-05,
      "loss": 0.6904,
      "step": 1444300
    },
    {
      "epoch": 13.17979414555807,
      "grad_norm": 4.585483551025391,
      "learning_rate": 3.9016838212034945e-05,
      "loss": 0.6623,
      "step": 1444400
    },
    {
      "epoch": 13.180706620921235,
      "grad_norm": 3.849025249481201,
      "learning_rate": 3.901607781589897e-05,
      "loss": 0.6664,
      "step": 1444500
    },
    {
      "epoch": 13.1816190962844,
      "grad_norm": 3.609459638595581,
      "learning_rate": 3.9015317419763005e-05,
      "loss": 0.6638,
      "step": 1444600
    },
    {
      "epoch": 13.182531571647566,
      "grad_norm": 4.547239303588867,
      "learning_rate": 3.901455702362703e-05,
      "loss": 0.7081,
      "step": 1444700
    },
    {
      "epoch": 13.183444047010731,
      "grad_norm": 3.3436365127563477,
      "learning_rate": 3.901379662749106e-05,
      "loss": 0.7134,
      "step": 1444800
    },
    {
      "epoch": 13.184356522373896,
      "grad_norm": 1.9039802551269531,
      "learning_rate": 3.901303623135509e-05,
      "loss": 0.687,
      "step": 1444900
    },
    {
      "epoch": 13.185268997737062,
      "grad_norm": 4.183018684387207,
      "learning_rate": 3.901227583521912e-05,
      "loss": 0.6733,
      "step": 1445000
    },
    {
      "epoch": 13.186181473100227,
      "grad_norm": 3.34328031539917,
      "learning_rate": 3.901151543908315e-05,
      "loss": 0.6779,
      "step": 1445100
    },
    {
      "epoch": 13.187093948463392,
      "grad_norm": 3.560249090194702,
      "learning_rate": 3.901075504294718e-05,
      "loss": 0.6679,
      "step": 1445200
    },
    {
      "epoch": 13.188006423826558,
      "grad_norm": 4.765808582305908,
      "learning_rate": 3.90099946468112e-05,
      "loss": 0.7014,
      "step": 1445300
    },
    {
      "epoch": 13.188918899189721,
      "grad_norm": 4.139008522033691,
      "learning_rate": 3.900923425067524e-05,
      "loss": 0.6739,
      "step": 1445400
    },
    {
      "epoch": 13.189831374552886,
      "grad_norm": 4.570423603057861,
      "learning_rate": 3.900847385453926e-05,
      "loss": 0.7118,
      "step": 1445500
    },
    {
      "epoch": 13.190743849916052,
      "grad_norm": 4.1191792488098145,
      "learning_rate": 3.900771345840329e-05,
      "loss": 0.6965,
      "step": 1445600
    },
    {
      "epoch": 13.191656325279217,
      "grad_norm": 3.243393898010254,
      "learning_rate": 3.900695306226732e-05,
      "loss": 0.6646,
      "step": 1445700
    },
    {
      "epoch": 13.192568800642382,
      "grad_norm": 3.870312452316284,
      "learning_rate": 3.9006192666131346e-05,
      "loss": 0.6537,
      "step": 1445800
    },
    {
      "epoch": 13.193481276005548,
      "grad_norm": 5.036336898803711,
      "learning_rate": 3.9005432269995376e-05,
      "loss": 0.6621,
      "step": 1445900
    },
    {
      "epoch": 13.194393751368713,
      "grad_norm": 4.2694854736328125,
      "learning_rate": 3.9004671873859406e-05,
      "loss": 0.6798,
      "step": 1446000
    },
    {
      "epoch": 13.195306226731878,
      "grad_norm": 3.943509817123413,
      "learning_rate": 3.9003911477723436e-05,
      "loss": 0.6362,
      "step": 1446100
    },
    {
      "epoch": 13.196218702095043,
      "grad_norm": 4.006821632385254,
      "learning_rate": 3.9003151081587466e-05,
      "loss": 0.6739,
      "step": 1446200
    },
    {
      "epoch": 13.197131177458209,
      "grad_norm": 4.618265628814697,
      "learning_rate": 3.9002390685451496e-05,
      "loss": 0.6624,
      "step": 1446300
    },
    {
      "epoch": 13.198043652821374,
      "grad_norm": 4.4319963455200195,
      "learning_rate": 3.900163028931552e-05,
      "loss": 0.6745,
      "step": 1446400
    },
    {
      "epoch": 13.19895612818454,
      "grad_norm": 3.3961167335510254,
      "learning_rate": 3.9000869893179556e-05,
      "loss": 0.7015,
      "step": 1446500
    },
    {
      "epoch": 13.199868603547705,
      "grad_norm": 4.269571781158447,
      "learning_rate": 3.900010949704358e-05,
      "loss": 0.6846,
      "step": 1446600
    },
    {
      "epoch": 13.20078107891087,
      "grad_norm": 3.641361713409424,
      "learning_rate": 3.899934910090761e-05,
      "loss": 0.7199,
      "step": 1446700
    },
    {
      "epoch": 13.201693554274035,
      "grad_norm": 5.689314842224121,
      "learning_rate": 3.899858870477164e-05,
      "loss": 0.6752,
      "step": 1446800
    },
    {
      "epoch": 13.2026060296372,
      "grad_norm": 3.6349849700927734,
      "learning_rate": 3.899782830863567e-05,
      "loss": 0.6588,
      "step": 1446900
    },
    {
      "epoch": 13.203518505000366,
      "grad_norm": 3.904357671737671,
      "learning_rate": 3.899706791249969e-05,
      "loss": 0.6616,
      "step": 1447000
    },
    {
      "epoch": 13.20443098036353,
      "grad_norm": 4.29003381729126,
      "learning_rate": 3.899630751636373e-05,
      "loss": 0.6441,
      "step": 1447100
    },
    {
      "epoch": 13.205343455726695,
      "grad_norm": 4.293573379516602,
      "learning_rate": 3.899554712022775e-05,
      "loss": 0.7144,
      "step": 1447200
    },
    {
      "epoch": 13.20625593108986,
      "grad_norm": 3.3431875705718994,
      "learning_rate": 3.899478672409178e-05,
      "loss": 0.6571,
      "step": 1447300
    },
    {
      "epoch": 13.207168406453025,
      "grad_norm": 3.932325601577759,
      "learning_rate": 3.899402632795581e-05,
      "loss": 0.641,
      "step": 1447400
    },
    {
      "epoch": 13.20808088181619,
      "grad_norm": 4.397590637207031,
      "learning_rate": 3.899326593181984e-05,
      "loss": 0.6811,
      "step": 1447500
    },
    {
      "epoch": 13.208993357179356,
      "grad_norm": 4.367620468139648,
      "learning_rate": 3.899250553568387e-05,
      "loss": 0.6932,
      "step": 1447600
    },
    {
      "epoch": 13.209905832542521,
      "grad_norm": 3.558389902114868,
      "learning_rate": 3.89917451395479e-05,
      "loss": 0.6832,
      "step": 1447700
    },
    {
      "epoch": 13.210818307905686,
      "grad_norm": 4.064213275909424,
      "learning_rate": 3.899098474341193e-05,
      "loss": 0.674,
      "step": 1447800
    },
    {
      "epoch": 13.211730783268852,
      "grad_norm": 3.447360038757324,
      "learning_rate": 3.8990224347275963e-05,
      "loss": 0.6985,
      "step": 1447900
    },
    {
      "epoch": 13.212643258632017,
      "grad_norm": 2.664187431335449,
      "learning_rate": 3.898946395113999e-05,
      "loss": 0.6638,
      "step": 1448000
    },
    {
      "epoch": 13.213555733995182,
      "grad_norm": 3.1343631744384766,
      "learning_rate": 3.898870355500402e-05,
      "loss": 0.6916,
      "step": 1448100
    },
    {
      "epoch": 13.214468209358348,
      "grad_norm": 4.458076477050781,
      "learning_rate": 3.898794315886805e-05,
      "loss": 0.6789,
      "step": 1448200
    },
    {
      "epoch": 13.215380684721513,
      "grad_norm": 4.658411502838135,
      "learning_rate": 3.898718276273208e-05,
      "loss": 0.6595,
      "step": 1448300
    },
    {
      "epoch": 13.216293160084678,
      "grad_norm": 4.153886318206787,
      "learning_rate": 3.898642236659611e-05,
      "loss": 0.6812,
      "step": 1448400
    },
    {
      "epoch": 13.217205635447844,
      "grad_norm": 4.327850818634033,
      "learning_rate": 3.898566197046013e-05,
      "loss": 0.6796,
      "step": 1448500
    },
    {
      "epoch": 13.218118110811009,
      "grad_norm": 4.59542179107666,
      "learning_rate": 3.898490157432416e-05,
      "loss": 0.6849,
      "step": 1448600
    },
    {
      "epoch": 13.219030586174174,
      "grad_norm": 3.730069160461426,
      "learning_rate": 3.898414117818819e-05,
      "loss": 0.6647,
      "step": 1448700
    },
    {
      "epoch": 13.219943061537338,
      "grad_norm": 4.205996036529541,
      "learning_rate": 3.898338078205222e-05,
      "loss": 0.6368,
      "step": 1448800
    },
    {
      "epoch": 13.220855536900503,
      "grad_norm": 4.341501712799072,
      "learning_rate": 3.8982620385916244e-05,
      "loss": 0.6683,
      "step": 1448900
    },
    {
      "epoch": 13.221768012263668,
      "grad_norm": 3.0561041831970215,
      "learning_rate": 3.898185998978028e-05,
      "loss": 0.6528,
      "step": 1449000
    },
    {
      "epoch": 13.222680487626834,
      "grad_norm": 5.368177890777588,
      "learning_rate": 3.8981099593644304e-05,
      "loss": 0.6693,
      "step": 1449100
    },
    {
      "epoch": 13.223592962989999,
      "grad_norm": 4.446130275726318,
      "learning_rate": 3.8980339197508334e-05,
      "loss": 0.6791,
      "step": 1449200
    },
    {
      "epoch": 13.224505438353164,
      "grad_norm": 3.8371214866638184,
      "learning_rate": 3.8979578801372364e-05,
      "loss": 0.6748,
      "step": 1449300
    },
    {
      "epoch": 13.22541791371633,
      "grad_norm": 3.0264639854431152,
      "learning_rate": 3.8978818405236394e-05,
      "loss": 0.6492,
      "step": 1449400
    },
    {
      "epoch": 13.226330389079495,
      "grad_norm": 3.2112860679626465,
      "learning_rate": 3.8978058009100424e-05,
      "loss": 0.6647,
      "step": 1449500
    },
    {
      "epoch": 13.22724286444266,
      "grad_norm": 3.614128351211548,
      "learning_rate": 3.8977297612964454e-05,
      "loss": 0.6485,
      "step": 1449600
    },
    {
      "epoch": 13.228155339805825,
      "grad_norm": 4.174111366271973,
      "learning_rate": 3.897653721682848e-05,
      "loss": 0.6949,
      "step": 1449700
    },
    {
      "epoch": 13.22906781516899,
      "grad_norm": 3.5710904598236084,
      "learning_rate": 3.8975776820692514e-05,
      "loss": 0.6665,
      "step": 1449800
    },
    {
      "epoch": 13.229980290532156,
      "grad_norm": 3.2663183212280273,
      "learning_rate": 3.897501642455654e-05,
      "loss": 0.6682,
      "step": 1449900
    },
    {
      "epoch": 13.230892765895321,
      "grad_norm": 4.1207475662231445,
      "learning_rate": 3.897425602842057e-05,
      "loss": 0.6825,
      "step": 1450000
    },
    {
      "epoch": 13.231805241258487,
      "grad_norm": 4.248769283294678,
      "learning_rate": 3.89734956322846e-05,
      "loss": 0.6434,
      "step": 1450100
    },
    {
      "epoch": 13.232717716621652,
      "grad_norm": 4.608828544616699,
      "learning_rate": 3.897273523614863e-05,
      "loss": 0.6782,
      "step": 1450200
    },
    {
      "epoch": 13.233630191984817,
      "grad_norm": 3.6594040393829346,
      "learning_rate": 3.897197484001265e-05,
      "loss": 0.6606,
      "step": 1450300
    },
    {
      "epoch": 13.234542667347982,
      "grad_norm": 4.328958511352539,
      "learning_rate": 3.897121444387669e-05,
      "loss": 0.6658,
      "step": 1450400
    },
    {
      "epoch": 13.235455142711146,
      "grad_norm": 4.213330268859863,
      "learning_rate": 3.897045404774071e-05,
      "loss": 0.7061,
      "step": 1450500
    },
    {
      "epoch": 13.236367618074311,
      "grad_norm": 4.385878086090088,
      "learning_rate": 3.896969365160474e-05,
      "loss": 0.7061,
      "step": 1450600
    },
    {
      "epoch": 13.237280093437477,
      "grad_norm": 2.6215333938598633,
      "learning_rate": 3.896893325546877e-05,
      "loss": 0.6979,
      "step": 1450700
    },
    {
      "epoch": 13.238192568800642,
      "grad_norm": 4.392728328704834,
      "learning_rate": 3.89681728593328e-05,
      "loss": 0.6402,
      "step": 1450800
    },
    {
      "epoch": 13.239105044163807,
      "grad_norm": 2.152693033218384,
      "learning_rate": 3.896741246319683e-05,
      "loss": 0.6759,
      "step": 1450900
    },
    {
      "epoch": 13.240017519526972,
      "grad_norm": 4.542140007019043,
      "learning_rate": 3.896665206706086e-05,
      "loss": 0.6763,
      "step": 1451000
    },
    {
      "epoch": 13.240929994890138,
      "grad_norm": 4.259960651397705,
      "learning_rate": 3.8965891670924885e-05,
      "loss": 0.6694,
      "step": 1451100
    },
    {
      "epoch": 13.241842470253303,
      "grad_norm": 3.997403860092163,
      "learning_rate": 3.896513127478892e-05,
      "loss": 0.6721,
      "step": 1451200
    },
    {
      "epoch": 13.242754945616468,
      "grad_norm": 4.392477989196777,
      "learning_rate": 3.8964370878652945e-05,
      "loss": 0.685,
      "step": 1451300
    },
    {
      "epoch": 13.243667420979634,
      "grad_norm": 4.837138652801514,
      "learning_rate": 3.896361048251697e-05,
      "loss": 0.6637,
      "step": 1451400
    },
    {
      "epoch": 13.244579896342799,
      "grad_norm": 4.205012798309326,
      "learning_rate": 3.8962850086381005e-05,
      "loss": 0.6423,
      "step": 1451500
    },
    {
      "epoch": 13.245492371705964,
      "grad_norm": 4.534528732299805,
      "learning_rate": 3.896208969024503e-05,
      "loss": 0.6847,
      "step": 1451600
    },
    {
      "epoch": 13.24640484706913,
      "grad_norm": 3.9317967891693115,
      "learning_rate": 3.896132929410906e-05,
      "loss": 0.6777,
      "step": 1451700
    },
    {
      "epoch": 13.247317322432295,
      "grad_norm": 4.121087551116943,
      "learning_rate": 3.896056889797309e-05,
      "loss": 0.686,
      "step": 1451800
    },
    {
      "epoch": 13.24822979779546,
      "grad_norm": 4.185671806335449,
      "learning_rate": 3.895980850183712e-05,
      "loss": 0.6729,
      "step": 1451900
    },
    {
      "epoch": 13.249142273158625,
      "grad_norm": 3.8072056770324707,
      "learning_rate": 3.895904810570115e-05,
      "loss": 0.6294,
      "step": 1452000
    },
    {
      "epoch": 13.25005474852179,
      "grad_norm": 4.033605098724365,
      "learning_rate": 3.895828770956518e-05,
      "loss": 0.659,
      "step": 1452100
    },
    {
      "epoch": 13.250967223884954,
      "grad_norm": 3.0434939861297607,
      "learning_rate": 3.89575273134292e-05,
      "loss": 0.6529,
      "step": 1452200
    },
    {
      "epoch": 13.25187969924812,
      "grad_norm": 4.193633079528809,
      "learning_rate": 3.895676691729324e-05,
      "loss": 0.6799,
      "step": 1452300
    },
    {
      "epoch": 13.252792174611285,
      "grad_norm": 3.4601709842681885,
      "learning_rate": 3.895600652115726e-05,
      "loss": 0.6404,
      "step": 1452400
    },
    {
      "epoch": 13.25370464997445,
      "grad_norm": 4.001591682434082,
      "learning_rate": 3.895524612502129e-05,
      "loss": 0.6601,
      "step": 1452500
    },
    {
      "epoch": 13.254617125337615,
      "grad_norm": 4.7169108390808105,
      "learning_rate": 3.895448572888532e-05,
      "loss": 0.6806,
      "step": 1452600
    },
    {
      "epoch": 13.25552960070078,
      "grad_norm": 3.7899060249328613,
      "learning_rate": 3.895372533274935e-05,
      "loss": 0.6493,
      "step": 1452700
    },
    {
      "epoch": 13.256442076063946,
      "grad_norm": 3.9538516998291016,
      "learning_rate": 3.8952964936613376e-05,
      "loss": 0.6909,
      "step": 1452800
    },
    {
      "epoch": 13.257354551427111,
      "grad_norm": 3.9189257621765137,
      "learning_rate": 3.895220454047741e-05,
      "loss": 0.6826,
      "step": 1452900
    },
    {
      "epoch": 13.258267026790277,
      "grad_norm": 4.300719261169434,
      "learning_rate": 3.8951444144341436e-05,
      "loss": 0.6794,
      "step": 1453000
    },
    {
      "epoch": 13.259179502153442,
      "grad_norm": 3.5289690494537354,
      "learning_rate": 3.8950683748205466e-05,
      "loss": 0.674,
      "step": 1453100
    },
    {
      "epoch": 13.260091977516607,
      "grad_norm": 5.719015598297119,
      "learning_rate": 3.8949923352069496e-05,
      "loss": 0.6479,
      "step": 1453200
    },
    {
      "epoch": 13.261004452879773,
      "grad_norm": 3.8007936477661133,
      "learning_rate": 3.8949162955933526e-05,
      "loss": 0.6686,
      "step": 1453300
    },
    {
      "epoch": 13.261916928242938,
      "grad_norm": 4.228759288787842,
      "learning_rate": 3.8948402559797556e-05,
      "loss": 0.6228,
      "step": 1453400
    },
    {
      "epoch": 13.262829403606103,
      "grad_norm": 4.834667682647705,
      "learning_rate": 3.8947642163661586e-05,
      "loss": 0.629,
      "step": 1453500
    },
    {
      "epoch": 13.263741878969268,
      "grad_norm": 4.639901161193848,
      "learning_rate": 3.894688176752561e-05,
      "loss": 0.6494,
      "step": 1453600
    },
    {
      "epoch": 13.264654354332434,
      "grad_norm": 4.5370917320251465,
      "learning_rate": 3.8946121371389646e-05,
      "loss": 0.6458,
      "step": 1453700
    },
    {
      "epoch": 13.265566829695597,
      "grad_norm": 3.775359630584717,
      "learning_rate": 3.894536097525367e-05,
      "loss": 0.6579,
      "step": 1453800
    },
    {
      "epoch": 13.266479305058763,
      "grad_norm": 3.579477310180664,
      "learning_rate": 3.89446005791177e-05,
      "loss": 0.6668,
      "step": 1453900
    },
    {
      "epoch": 13.267391780421928,
      "grad_norm": 3.4877140522003174,
      "learning_rate": 3.894384018298173e-05,
      "loss": 0.6807,
      "step": 1454000
    },
    {
      "epoch": 13.268304255785093,
      "grad_norm": 3.159064292907715,
      "learning_rate": 3.894307978684576e-05,
      "loss": 0.6442,
      "step": 1454100
    },
    {
      "epoch": 13.269216731148259,
      "grad_norm": 3.573323965072632,
      "learning_rate": 3.894231939070978e-05,
      "loss": 0.7064,
      "step": 1454200
    },
    {
      "epoch": 13.270129206511424,
      "grad_norm": 3.912095785140991,
      "learning_rate": 3.894155899457381e-05,
      "loss": 0.6854,
      "step": 1454300
    },
    {
      "epoch": 13.271041681874589,
      "grad_norm": 3.850167751312256,
      "learning_rate": 3.894079859843784e-05,
      "loss": 0.6522,
      "step": 1454400
    },
    {
      "epoch": 13.271954157237754,
      "grad_norm": 3.4058749675750732,
      "learning_rate": 3.8940038202301873e-05,
      "loss": 0.6689,
      "step": 1454500
    },
    {
      "epoch": 13.27286663260092,
      "grad_norm": 3.2021398544311523,
      "learning_rate": 3.8939277806165903e-05,
      "loss": 0.6908,
      "step": 1454600
    },
    {
      "epoch": 13.273779107964085,
      "grad_norm": 4.352816581726074,
      "learning_rate": 3.893851741002993e-05,
      "loss": 0.6522,
      "step": 1454700
    },
    {
      "epoch": 13.27469158332725,
      "grad_norm": 4.658248424530029,
      "learning_rate": 3.8937757013893964e-05,
      "loss": 0.7231,
      "step": 1454800
    },
    {
      "epoch": 13.275604058690416,
      "grad_norm": 4.622299671173096,
      "learning_rate": 3.893699661775799e-05,
      "loss": 0.6937,
      "step": 1454900
    },
    {
      "epoch": 13.276516534053581,
      "grad_norm": 4.691835403442383,
      "learning_rate": 3.893623622162202e-05,
      "loss": 0.6401,
      "step": 1455000
    },
    {
      "epoch": 13.277429009416746,
      "grad_norm": 3.5830843448638916,
      "learning_rate": 3.893547582548605e-05,
      "loss": 0.6837,
      "step": 1455100
    },
    {
      "epoch": 13.278341484779911,
      "grad_norm": 4.405184745788574,
      "learning_rate": 3.893471542935008e-05,
      "loss": 0.7,
      "step": 1455200
    },
    {
      "epoch": 13.279253960143077,
      "grad_norm": 4.1245269775390625,
      "learning_rate": 3.89339550332141e-05,
      "loss": 0.6845,
      "step": 1455300
    },
    {
      "epoch": 13.280166435506242,
      "grad_norm": 4.148374557495117,
      "learning_rate": 3.893319463707814e-05,
      "loss": 0.6734,
      "step": 1455400
    },
    {
      "epoch": 13.281078910869407,
      "grad_norm": 4.09197998046875,
      "learning_rate": 3.893243424094216e-05,
      "loss": 0.685,
      "step": 1455500
    },
    {
      "epoch": 13.281991386232571,
      "grad_norm": 3.0642342567443848,
      "learning_rate": 3.893167384480619e-05,
      "loss": 0.6689,
      "step": 1455600
    },
    {
      "epoch": 13.282903861595736,
      "grad_norm": 2.5072150230407715,
      "learning_rate": 3.893091344867022e-05,
      "loss": 0.6426,
      "step": 1455700
    },
    {
      "epoch": 13.283816336958902,
      "grad_norm": 3.8771719932556152,
      "learning_rate": 3.893015305253425e-05,
      "loss": 0.7014,
      "step": 1455800
    },
    {
      "epoch": 13.284728812322067,
      "grad_norm": 4.175094127655029,
      "learning_rate": 3.892939265639828e-05,
      "loss": 0.6852,
      "step": 1455900
    },
    {
      "epoch": 13.285641287685232,
      "grad_norm": 4.289163589477539,
      "learning_rate": 3.892863226026231e-05,
      "loss": 0.6884,
      "step": 1456000
    },
    {
      "epoch": 13.286553763048397,
      "grad_norm": 4.1600775718688965,
      "learning_rate": 3.8927871864126334e-05,
      "loss": 0.628,
      "step": 1456100
    },
    {
      "epoch": 13.287466238411563,
      "grad_norm": 4.40260124206543,
      "learning_rate": 3.892711146799037e-05,
      "loss": 0.6991,
      "step": 1456200
    },
    {
      "epoch": 13.288378713774728,
      "grad_norm": 3.4100701808929443,
      "learning_rate": 3.8926351071854394e-05,
      "loss": 0.6913,
      "step": 1456300
    },
    {
      "epoch": 13.289291189137893,
      "grad_norm": 3.514981269836426,
      "learning_rate": 3.8925590675718424e-05,
      "loss": 0.6927,
      "step": 1456400
    },
    {
      "epoch": 13.290203664501059,
      "grad_norm": 3.6418943405151367,
      "learning_rate": 3.8924830279582454e-05,
      "loss": 0.6716,
      "step": 1456500
    },
    {
      "epoch": 13.291116139864224,
      "grad_norm": 4.24973201751709,
      "learning_rate": 3.8924069883446484e-05,
      "loss": 0.7,
      "step": 1456600
    },
    {
      "epoch": 13.29202861522739,
      "grad_norm": 4.160863876342773,
      "learning_rate": 3.892330948731051e-05,
      "loss": 0.7023,
      "step": 1456700
    },
    {
      "epoch": 13.292941090590555,
      "grad_norm": 3.3364133834838867,
      "learning_rate": 3.8922549091174545e-05,
      "loss": 0.6653,
      "step": 1456800
    },
    {
      "epoch": 13.29385356595372,
      "grad_norm": 2.748316526412964,
      "learning_rate": 3.892178869503857e-05,
      "loss": 0.6298,
      "step": 1456900
    },
    {
      "epoch": 13.294766041316885,
      "grad_norm": 3.8154335021972656,
      "learning_rate": 3.89210282989026e-05,
      "loss": 0.695,
      "step": 1457000
    },
    {
      "epoch": 13.29567851668005,
      "grad_norm": 4.108776092529297,
      "learning_rate": 3.892026790276663e-05,
      "loss": 0.6926,
      "step": 1457100
    },
    {
      "epoch": 13.296590992043214,
      "grad_norm": 3.4205236434936523,
      "learning_rate": 3.891950750663065e-05,
      "loss": 0.6774,
      "step": 1457200
    },
    {
      "epoch": 13.29750346740638,
      "grad_norm": 2.3068690299987793,
      "learning_rate": 3.891874711049469e-05,
      "loss": 0.609,
      "step": 1457300
    },
    {
      "epoch": 13.298415942769545,
      "grad_norm": 3.59299635887146,
      "learning_rate": 3.891798671435871e-05,
      "loss": 0.6718,
      "step": 1457400
    },
    {
      "epoch": 13.29932841813271,
      "grad_norm": 4.471527099609375,
      "learning_rate": 3.891722631822274e-05,
      "loss": 0.6371,
      "step": 1457500
    },
    {
      "epoch": 13.300240893495875,
      "grad_norm": 4.738556385040283,
      "learning_rate": 3.891646592208677e-05,
      "loss": 0.6392,
      "step": 1457600
    },
    {
      "epoch": 13.30115336885904,
      "grad_norm": 3.8059206008911133,
      "learning_rate": 3.89157055259508e-05,
      "loss": 0.6859,
      "step": 1457700
    },
    {
      "epoch": 13.302065844222206,
      "grad_norm": 3.52301025390625,
      "learning_rate": 3.8914945129814825e-05,
      "loss": 0.6743,
      "step": 1457800
    },
    {
      "epoch": 13.302978319585371,
      "grad_norm": 4.8486785888671875,
      "learning_rate": 3.891418473367886e-05,
      "loss": 0.6836,
      "step": 1457900
    },
    {
      "epoch": 13.303890794948536,
      "grad_norm": 4.108400821685791,
      "learning_rate": 3.8913424337542885e-05,
      "loss": 0.6348,
      "step": 1458000
    },
    {
      "epoch": 13.304803270311702,
      "grad_norm": 3.413285970687866,
      "learning_rate": 3.8912663941406915e-05,
      "loss": 0.6745,
      "step": 1458100
    },
    {
      "epoch": 13.305715745674867,
      "grad_norm": 4.394941329956055,
      "learning_rate": 3.8911903545270945e-05,
      "loss": 0.6835,
      "step": 1458200
    },
    {
      "epoch": 13.306628221038032,
      "grad_norm": 5.324789047241211,
      "learning_rate": 3.8911143149134975e-05,
      "loss": 0.6971,
      "step": 1458300
    },
    {
      "epoch": 13.307540696401198,
      "grad_norm": 4.063467502593994,
      "learning_rate": 3.8910382752999005e-05,
      "loss": 0.6766,
      "step": 1458400
    },
    {
      "epoch": 13.308453171764363,
      "grad_norm": 3.3629283905029297,
      "learning_rate": 3.8909622356863035e-05,
      "loss": 0.6908,
      "step": 1458500
    },
    {
      "epoch": 13.309365647127528,
      "grad_norm": 3.734415292739868,
      "learning_rate": 3.890886196072706e-05,
      "loss": 0.6647,
      "step": 1458600
    },
    {
      "epoch": 13.310278122490693,
      "grad_norm": 4.609933376312256,
      "learning_rate": 3.8908101564591096e-05,
      "loss": 0.6562,
      "step": 1458700
    },
    {
      "epoch": 13.311190597853859,
      "grad_norm": 4.043185234069824,
      "learning_rate": 3.890734116845512e-05,
      "loss": 0.6525,
      "step": 1458800
    },
    {
      "epoch": 13.312103073217024,
      "grad_norm": 4.472443103790283,
      "learning_rate": 3.890658077231915e-05,
      "loss": 0.719,
      "step": 1458900
    },
    {
      "epoch": 13.313015548580188,
      "grad_norm": 4.182210922241211,
      "learning_rate": 3.890582037618318e-05,
      "loss": 0.6664,
      "step": 1459000
    },
    {
      "epoch": 13.313928023943353,
      "grad_norm": 4.3835673332214355,
      "learning_rate": 3.890505998004721e-05,
      "loss": 0.6638,
      "step": 1459100
    },
    {
      "epoch": 13.314840499306518,
      "grad_norm": 3.5082621574401855,
      "learning_rate": 3.890429958391123e-05,
      "loss": 0.6808,
      "step": 1459200
    },
    {
      "epoch": 13.315752974669683,
      "grad_norm": 3.998147964477539,
      "learning_rate": 3.890353918777527e-05,
      "loss": 0.6788,
      "step": 1459300
    },
    {
      "epoch": 13.316665450032849,
      "grad_norm": 3.088397264480591,
      "learning_rate": 3.890277879163929e-05,
      "loss": 0.649,
      "step": 1459400
    },
    {
      "epoch": 13.317577925396014,
      "grad_norm": 4.053255558013916,
      "learning_rate": 3.890201839550332e-05,
      "loss": 0.6989,
      "step": 1459500
    },
    {
      "epoch": 13.31849040075918,
      "grad_norm": 3.730372428894043,
      "learning_rate": 3.890125799936735e-05,
      "loss": 0.6511,
      "step": 1459600
    },
    {
      "epoch": 13.319402876122345,
      "grad_norm": 3.642274856567383,
      "learning_rate": 3.890049760323138e-05,
      "loss": 0.712,
      "step": 1459700
    },
    {
      "epoch": 13.32031535148551,
      "grad_norm": 3.8159537315368652,
      "learning_rate": 3.889973720709541e-05,
      "loss": 0.6588,
      "step": 1459800
    },
    {
      "epoch": 13.321227826848675,
      "grad_norm": 3.9287526607513428,
      "learning_rate": 3.8898976810959436e-05,
      "loss": 0.6061,
      "step": 1459900
    },
    {
      "epoch": 13.32214030221184,
      "grad_norm": 3.1263673305511475,
      "learning_rate": 3.8898216414823466e-05,
      "loss": 0.6654,
      "step": 1460000
    },
    {
      "epoch": 13.323052777575006,
      "grad_norm": 4.058882713317871,
      "learning_rate": 3.8897456018687496e-05,
      "loss": 0.639,
      "step": 1460100
    },
    {
      "epoch": 13.323965252938171,
      "grad_norm": 4.539241313934326,
      "learning_rate": 3.8896695622551526e-05,
      "loss": 0.6739,
      "step": 1460200
    },
    {
      "epoch": 13.324877728301336,
      "grad_norm": 4.135578155517578,
      "learning_rate": 3.8895935226415556e-05,
      "loss": 0.6732,
      "step": 1460300
    },
    {
      "epoch": 13.325790203664502,
      "grad_norm": 4.775931358337402,
      "learning_rate": 3.8895174830279586e-05,
      "loss": 0.7108,
      "step": 1460400
    },
    {
      "epoch": 13.326702679027667,
      "grad_norm": 3.399066686630249,
      "learning_rate": 3.889441443414361e-05,
      "loss": 0.6809,
      "step": 1460500
    },
    {
      "epoch": 13.32761515439083,
      "grad_norm": 3.543426752090454,
      "learning_rate": 3.889365403800764e-05,
      "loss": 0.6711,
      "step": 1460600
    },
    {
      "epoch": 13.328527629753996,
      "grad_norm": 3.6339337825775146,
      "learning_rate": 3.889289364187167e-05,
      "loss": 0.7141,
      "step": 1460700
    },
    {
      "epoch": 13.329440105117161,
      "grad_norm": 4.627819061279297,
      "learning_rate": 3.88921332457357e-05,
      "loss": 0.647,
      "step": 1460800
    },
    {
      "epoch": 13.330352580480326,
      "grad_norm": 4.367198944091797,
      "learning_rate": 3.889137284959973e-05,
      "loss": 0.6587,
      "step": 1460900
    },
    {
      "epoch": 13.331265055843492,
      "grad_norm": 4.209602355957031,
      "learning_rate": 3.889061245346376e-05,
      "loss": 0.6652,
      "step": 1461000
    },
    {
      "epoch": 13.332177531206657,
      "grad_norm": 3.950228214263916,
      "learning_rate": 3.888985205732778e-05,
      "loss": 0.6627,
      "step": 1461100
    },
    {
      "epoch": 13.333090006569822,
      "grad_norm": 4.591489791870117,
      "learning_rate": 3.888909166119182e-05,
      "loss": 0.646,
      "step": 1461200
    },
    {
      "epoch": 13.334002481932988,
      "grad_norm": 3.941265106201172,
      "learning_rate": 3.8888331265055843e-05,
      "loss": 0.6668,
      "step": 1461300
    },
    {
      "epoch": 13.334914957296153,
      "grad_norm": 3.941448211669922,
      "learning_rate": 3.8887570868919873e-05,
      "loss": 0.6682,
      "step": 1461400
    },
    {
      "epoch": 13.335827432659318,
      "grad_norm": 3.284669876098633,
      "learning_rate": 3.8886810472783904e-05,
      "loss": 0.6677,
      "step": 1461500
    },
    {
      "epoch": 13.336739908022484,
      "grad_norm": 4.070693492889404,
      "learning_rate": 3.8886050076647934e-05,
      "loss": 0.6982,
      "step": 1461600
    },
    {
      "epoch": 13.337652383385649,
      "grad_norm": 3.7159335613250732,
      "learning_rate": 3.8885289680511964e-05,
      "loss": 0.6573,
      "step": 1461700
    },
    {
      "epoch": 13.338564858748814,
      "grad_norm": 3.6885085105895996,
      "learning_rate": 3.8884529284375994e-05,
      "loss": 0.6491,
      "step": 1461800
    },
    {
      "epoch": 13.33947733411198,
      "grad_norm": 4.616779804229736,
      "learning_rate": 3.888376888824002e-05,
      "loss": 0.6924,
      "step": 1461900
    },
    {
      "epoch": 13.340389809475145,
      "grad_norm": 3.9798429012298584,
      "learning_rate": 3.888300849210405e-05,
      "loss": 0.7009,
      "step": 1462000
    },
    {
      "epoch": 13.34130228483831,
      "grad_norm": 3.9746639728546143,
      "learning_rate": 3.888224809596808e-05,
      "loss": 0.6537,
      "step": 1462100
    },
    {
      "epoch": 13.342214760201475,
      "grad_norm": 4.013357639312744,
      "learning_rate": 3.888148769983211e-05,
      "loss": 0.7198,
      "step": 1462200
    },
    {
      "epoch": 13.34312723556464,
      "grad_norm": 3.8750195503234863,
      "learning_rate": 3.888072730369614e-05,
      "loss": 0.6709,
      "step": 1462300
    },
    {
      "epoch": 13.344039710927804,
      "grad_norm": 3.7907543182373047,
      "learning_rate": 3.887996690756017e-05,
      "loss": 0.6576,
      "step": 1462400
    },
    {
      "epoch": 13.34495218629097,
      "grad_norm": 3.366206645965576,
      "learning_rate": 3.887920651142419e-05,
      "loss": 0.6901,
      "step": 1462500
    },
    {
      "epoch": 13.345864661654135,
      "grad_norm": 3.45076584815979,
      "learning_rate": 3.887844611528823e-05,
      "loss": 0.6608,
      "step": 1462600
    },
    {
      "epoch": 13.3467771370173,
      "grad_norm": 3.322911262512207,
      "learning_rate": 3.887768571915225e-05,
      "loss": 0.6758,
      "step": 1462700
    },
    {
      "epoch": 13.347689612380465,
      "grad_norm": 3.98943829536438,
      "learning_rate": 3.887692532301628e-05,
      "loss": 0.667,
      "step": 1462800
    },
    {
      "epoch": 13.34860208774363,
      "grad_norm": 3.7653133869171143,
      "learning_rate": 3.887616492688031e-05,
      "loss": 0.6737,
      "step": 1462900
    },
    {
      "epoch": 13.349514563106796,
      "grad_norm": 3.719528913497925,
      "learning_rate": 3.8875404530744334e-05,
      "loss": 0.6694,
      "step": 1463000
    },
    {
      "epoch": 13.350427038469961,
      "grad_norm": 3.7637083530426025,
      "learning_rate": 3.887464413460837e-05,
      "loss": 0.691,
      "step": 1463100
    },
    {
      "epoch": 13.351339513833127,
      "grad_norm": 3.299997568130493,
      "learning_rate": 3.8873883738472394e-05,
      "loss": 0.6674,
      "step": 1463200
    },
    {
      "epoch": 13.352251989196292,
      "grad_norm": 4.11119270324707,
      "learning_rate": 3.8873123342336424e-05,
      "loss": 0.6781,
      "step": 1463300
    },
    {
      "epoch": 13.353164464559457,
      "grad_norm": 4.717123031616211,
      "learning_rate": 3.8872362946200455e-05,
      "loss": 0.6601,
      "step": 1463400
    },
    {
      "epoch": 13.354076939922622,
      "grad_norm": 4.107890605926514,
      "learning_rate": 3.8871602550064485e-05,
      "loss": 0.6947,
      "step": 1463500
    },
    {
      "epoch": 13.354989415285788,
      "grad_norm": 4.221574306488037,
      "learning_rate": 3.887084215392851e-05,
      "loss": 0.6893,
      "step": 1463600
    },
    {
      "epoch": 13.355901890648953,
      "grad_norm": 3.3109824657440186,
      "learning_rate": 3.8870081757792545e-05,
      "loss": 0.6415,
      "step": 1463700
    },
    {
      "epoch": 13.356814366012118,
      "grad_norm": 4.62929630279541,
      "learning_rate": 3.886932136165657e-05,
      "loss": 0.6547,
      "step": 1463800
    },
    {
      "epoch": 13.357726841375284,
      "grad_norm": 4.2120442390441895,
      "learning_rate": 3.88685609655206e-05,
      "loss": 0.6606,
      "step": 1463900
    },
    {
      "epoch": 13.358639316738447,
      "grad_norm": 4.689602375030518,
      "learning_rate": 3.886780056938463e-05,
      "loss": 0.688,
      "step": 1464000
    },
    {
      "epoch": 13.359551792101612,
      "grad_norm": 3.9994804859161377,
      "learning_rate": 3.886704017324866e-05,
      "loss": 0.6638,
      "step": 1464100
    },
    {
      "epoch": 13.360464267464778,
      "grad_norm": 4.02046012878418,
      "learning_rate": 3.886627977711269e-05,
      "loss": 0.6623,
      "step": 1464200
    },
    {
      "epoch": 13.361376742827943,
      "grad_norm": 4.322592258453369,
      "learning_rate": 3.886551938097672e-05,
      "loss": 0.6786,
      "step": 1464300
    },
    {
      "epoch": 13.362289218191108,
      "grad_norm": 3.730257272720337,
      "learning_rate": 3.886475898484074e-05,
      "loss": 0.6975,
      "step": 1464400
    },
    {
      "epoch": 13.363201693554274,
      "grad_norm": 3.3163270950317383,
      "learning_rate": 3.886399858870478e-05,
      "loss": 0.6667,
      "step": 1464500
    },
    {
      "epoch": 13.364114168917439,
      "grad_norm": 4.253016471862793,
      "learning_rate": 3.88632381925688e-05,
      "loss": 0.6676,
      "step": 1464600
    },
    {
      "epoch": 13.365026644280604,
      "grad_norm": 4.235365867614746,
      "learning_rate": 3.886247779643283e-05,
      "loss": 0.6748,
      "step": 1464700
    },
    {
      "epoch": 13.36593911964377,
      "grad_norm": 4.031916618347168,
      "learning_rate": 3.886171740029686e-05,
      "loss": 0.6676,
      "step": 1464800
    },
    {
      "epoch": 13.366851595006935,
      "grad_norm": 3.275325059890747,
      "learning_rate": 3.886095700416089e-05,
      "loss": 0.6728,
      "step": 1464900
    },
    {
      "epoch": 13.3677640703701,
      "grad_norm": 3.9563045501708984,
      "learning_rate": 3.8860196608024915e-05,
      "loss": 0.7048,
      "step": 1465000
    },
    {
      "epoch": 13.368676545733265,
      "grad_norm": 4.4482011795043945,
      "learning_rate": 3.885943621188895e-05,
      "loss": 0.6825,
      "step": 1465100
    },
    {
      "epoch": 13.36958902109643,
      "grad_norm": 3.750190019607544,
      "learning_rate": 3.8858675815752975e-05,
      "loss": 0.7033,
      "step": 1465200
    },
    {
      "epoch": 13.370501496459596,
      "grad_norm": 2.9455647468566895,
      "learning_rate": 3.8857915419617005e-05,
      "loss": 0.685,
      "step": 1465300
    },
    {
      "epoch": 13.371413971822761,
      "grad_norm": 4.249439239501953,
      "learning_rate": 3.8857155023481036e-05,
      "loss": 0.6639,
      "step": 1465400
    },
    {
      "epoch": 13.372326447185927,
      "grad_norm": 4.876275539398193,
      "learning_rate": 3.8856394627345066e-05,
      "loss": 0.6645,
      "step": 1465500
    },
    {
      "epoch": 13.373238922549092,
      "grad_norm": 4.172928333282471,
      "learning_rate": 3.8855634231209096e-05,
      "loss": 0.6904,
      "step": 1465600
    },
    {
      "epoch": 13.374151397912257,
      "grad_norm": 4.237368583679199,
      "learning_rate": 3.885487383507312e-05,
      "loss": 0.6474,
      "step": 1465700
    },
    {
      "epoch": 13.37506387327542,
      "grad_norm": 3.4204485416412354,
      "learning_rate": 3.885411343893715e-05,
      "loss": 0.6505,
      "step": 1465800
    },
    {
      "epoch": 13.375976348638586,
      "grad_norm": 3.3151564598083496,
      "learning_rate": 3.885335304280118e-05,
      "loss": 0.6659,
      "step": 1465900
    },
    {
      "epoch": 13.376888824001751,
      "grad_norm": 3.0710995197296143,
      "learning_rate": 3.885259264666521e-05,
      "loss": 0.64,
      "step": 1466000
    },
    {
      "epoch": 13.377801299364917,
      "grad_norm": 2.0274505615234375,
      "learning_rate": 3.885183225052923e-05,
      "loss": 0.717,
      "step": 1466100
    },
    {
      "epoch": 13.378713774728082,
      "grad_norm": 3.6473989486694336,
      "learning_rate": 3.885107185439327e-05,
      "loss": 0.6693,
      "step": 1466200
    },
    {
      "epoch": 13.379626250091247,
      "grad_norm": 4.647075653076172,
      "learning_rate": 3.885031145825729e-05,
      "loss": 0.687,
      "step": 1466300
    },
    {
      "epoch": 13.380538725454413,
      "grad_norm": 3.7829842567443848,
      "learning_rate": 3.884955106212132e-05,
      "loss": 0.628,
      "step": 1466400
    },
    {
      "epoch": 13.381451200817578,
      "grad_norm": 2.919240713119507,
      "learning_rate": 3.884879066598535e-05,
      "loss": 0.6967,
      "step": 1466500
    },
    {
      "epoch": 13.382363676180743,
      "grad_norm": 4.408874034881592,
      "learning_rate": 3.884803026984938e-05,
      "loss": 0.645,
      "step": 1466600
    },
    {
      "epoch": 13.383276151543908,
      "grad_norm": 3.9942939281463623,
      "learning_rate": 3.884726987371341e-05,
      "loss": 0.7018,
      "step": 1466700
    },
    {
      "epoch": 13.384188626907074,
      "grad_norm": 4.60214900970459,
      "learning_rate": 3.884650947757744e-05,
      "loss": 0.6926,
      "step": 1466800
    },
    {
      "epoch": 13.385101102270239,
      "grad_norm": 4.457019805908203,
      "learning_rate": 3.8845749081441466e-05,
      "loss": 0.6553,
      "step": 1466900
    },
    {
      "epoch": 13.386013577633404,
      "grad_norm": 4.414348602294922,
      "learning_rate": 3.88449886853055e-05,
      "loss": 0.6174,
      "step": 1467000
    },
    {
      "epoch": 13.38692605299657,
      "grad_norm": 4.591250896453857,
      "learning_rate": 3.8844228289169526e-05,
      "loss": 0.6443,
      "step": 1467100
    },
    {
      "epoch": 13.387838528359735,
      "grad_norm": 3.5934431552886963,
      "learning_rate": 3.8843467893033556e-05,
      "loss": 0.6698,
      "step": 1467200
    },
    {
      "epoch": 13.3887510037229,
      "grad_norm": 3.9568612575531006,
      "learning_rate": 3.8842707496897586e-05,
      "loss": 0.6793,
      "step": 1467300
    },
    {
      "epoch": 13.389663479086064,
      "grad_norm": 3.9362070560455322,
      "learning_rate": 3.8841947100761617e-05,
      "loss": 0.6634,
      "step": 1467400
    },
    {
      "epoch": 13.390575954449229,
      "grad_norm": 4.4259161949157715,
      "learning_rate": 3.884118670462564e-05,
      "loss": 0.6989,
      "step": 1467500
    },
    {
      "epoch": 13.391488429812394,
      "grad_norm": 4.7903032302856445,
      "learning_rate": 3.884042630848968e-05,
      "loss": 0.6665,
      "step": 1467600
    },
    {
      "epoch": 13.39240090517556,
      "grad_norm": 4.467648029327393,
      "learning_rate": 3.88396659123537e-05,
      "loss": 0.7141,
      "step": 1467700
    },
    {
      "epoch": 13.393313380538725,
      "grad_norm": 3.778754234313965,
      "learning_rate": 3.883890551621773e-05,
      "loss": 0.6686,
      "step": 1467800
    },
    {
      "epoch": 13.39422585590189,
      "grad_norm": 4.858087062835693,
      "learning_rate": 3.883814512008176e-05,
      "loss": 0.6715,
      "step": 1467900
    },
    {
      "epoch": 13.395138331265056,
      "grad_norm": 4.149040699005127,
      "learning_rate": 3.883738472394579e-05,
      "loss": 0.7067,
      "step": 1468000
    },
    {
      "epoch": 13.396050806628221,
      "grad_norm": 4.360072135925293,
      "learning_rate": 3.883662432780982e-05,
      "loss": 0.6703,
      "step": 1468100
    },
    {
      "epoch": 13.396963281991386,
      "grad_norm": 4.220943450927734,
      "learning_rate": 3.883586393167385e-05,
      "loss": 0.6697,
      "step": 1468200
    },
    {
      "epoch": 13.397875757354551,
      "grad_norm": 4.928192138671875,
      "learning_rate": 3.8835103535537874e-05,
      "loss": 0.6941,
      "step": 1468300
    },
    {
      "epoch": 13.398788232717717,
      "grad_norm": 3.273681879043579,
      "learning_rate": 3.8834343139401904e-05,
      "loss": 0.673,
      "step": 1468400
    },
    {
      "epoch": 13.399700708080882,
      "grad_norm": 3.8621580600738525,
      "learning_rate": 3.8833582743265934e-05,
      "loss": 0.6535,
      "step": 1468500
    },
    {
      "epoch": 13.400613183444047,
      "grad_norm": 3.9783577919006348,
      "learning_rate": 3.883282234712996e-05,
      "loss": 0.662,
      "step": 1468600
    },
    {
      "epoch": 13.401525658807213,
      "grad_norm": 3.811551332473755,
      "learning_rate": 3.8832061950993994e-05,
      "loss": 0.6925,
      "step": 1468700
    },
    {
      "epoch": 13.402438134170378,
      "grad_norm": 4.945774078369141,
      "learning_rate": 3.883130155485802e-05,
      "loss": 0.6298,
      "step": 1468800
    },
    {
      "epoch": 13.403350609533543,
      "grad_norm": 3.901038646697998,
      "learning_rate": 3.883054115872205e-05,
      "loss": 0.7137,
      "step": 1468900
    },
    {
      "epoch": 13.404263084896709,
      "grad_norm": 4.03125,
      "learning_rate": 3.882978076258608e-05,
      "loss": 0.6792,
      "step": 1469000
    },
    {
      "epoch": 13.405175560259874,
      "grad_norm": 3.7475178241729736,
      "learning_rate": 3.882902036645011e-05,
      "loss": 0.695,
      "step": 1469100
    },
    {
      "epoch": 13.406088035623037,
      "grad_norm": 3.9320127964019775,
      "learning_rate": 3.882825997031414e-05,
      "loss": 0.6592,
      "step": 1469200
    },
    {
      "epoch": 13.407000510986203,
      "grad_norm": 3.6826398372650146,
      "learning_rate": 3.882749957417817e-05,
      "loss": 0.6869,
      "step": 1469300
    },
    {
      "epoch": 13.407912986349368,
      "grad_norm": 3.777050256729126,
      "learning_rate": 3.882673917804219e-05,
      "loss": 0.6544,
      "step": 1469400
    },
    {
      "epoch": 13.408825461712533,
      "grad_norm": 3.7760426998138428,
      "learning_rate": 3.882597878190623e-05,
      "loss": 0.6393,
      "step": 1469500
    },
    {
      "epoch": 13.409737937075699,
      "grad_norm": 3.9980151653289795,
      "learning_rate": 3.882521838577025e-05,
      "loss": 0.6548,
      "step": 1469600
    },
    {
      "epoch": 13.410650412438864,
      "grad_norm": 4.3192338943481445,
      "learning_rate": 3.882445798963428e-05,
      "loss": 0.6643,
      "step": 1469700
    },
    {
      "epoch": 13.41156288780203,
      "grad_norm": 3.226905107498169,
      "learning_rate": 3.882369759349831e-05,
      "loss": 0.6812,
      "step": 1469800
    },
    {
      "epoch": 13.412475363165195,
      "grad_norm": 3.7047903537750244,
      "learning_rate": 3.882293719736234e-05,
      "loss": 0.693,
      "step": 1469900
    },
    {
      "epoch": 13.41338783852836,
      "grad_norm": 3.5428152084350586,
      "learning_rate": 3.8822176801226364e-05,
      "loss": 0.6833,
      "step": 1470000
    },
    {
      "epoch": 13.414300313891525,
      "grad_norm": 3.4306821823120117,
      "learning_rate": 3.88214164050904e-05,
      "loss": 0.6897,
      "step": 1470100
    },
    {
      "epoch": 13.41521278925469,
      "grad_norm": 4.451649188995361,
      "learning_rate": 3.8820656008954425e-05,
      "loss": 0.6668,
      "step": 1470200
    },
    {
      "epoch": 13.416125264617856,
      "grad_norm": 3.6486318111419678,
      "learning_rate": 3.8819895612818455e-05,
      "loss": 0.697,
      "step": 1470300
    },
    {
      "epoch": 13.417037739981021,
      "grad_norm": 3.8944427967071533,
      "learning_rate": 3.8819135216682485e-05,
      "loss": 0.668,
      "step": 1470400
    },
    {
      "epoch": 13.417950215344186,
      "grad_norm": 3.981182813644409,
      "learning_rate": 3.8818374820546515e-05,
      "loss": 0.6382,
      "step": 1470500
    },
    {
      "epoch": 13.418862690707352,
      "grad_norm": 4.0250630378723145,
      "learning_rate": 3.8817614424410545e-05,
      "loss": 0.6942,
      "step": 1470600
    },
    {
      "epoch": 13.419775166070517,
      "grad_norm": 4.265357971191406,
      "learning_rate": 3.8816854028274575e-05,
      "loss": 0.6459,
      "step": 1470700
    },
    {
      "epoch": 13.42068764143368,
      "grad_norm": 3.860062837600708,
      "learning_rate": 3.88160936321386e-05,
      "loss": 0.6553,
      "step": 1470800
    },
    {
      "epoch": 13.421600116796846,
      "grad_norm": 3.986140251159668,
      "learning_rate": 3.8815333236002635e-05,
      "loss": 0.6856,
      "step": 1470900
    },
    {
      "epoch": 13.422512592160011,
      "grad_norm": 4.27871561050415,
      "learning_rate": 3.881457283986666e-05,
      "loss": 0.6487,
      "step": 1471000
    },
    {
      "epoch": 13.423425067523176,
      "grad_norm": 3.434399366378784,
      "learning_rate": 3.881381244373069e-05,
      "loss": 0.6837,
      "step": 1471100
    },
    {
      "epoch": 13.424337542886342,
      "grad_norm": 5.5164923667907715,
      "learning_rate": 3.881305204759472e-05,
      "loss": 0.6779,
      "step": 1471200
    },
    {
      "epoch": 13.425250018249507,
      "grad_norm": 4.040802001953125,
      "learning_rate": 3.881229165145874e-05,
      "loss": 0.6886,
      "step": 1471300
    },
    {
      "epoch": 13.426162493612672,
      "grad_norm": 4.180630683898926,
      "learning_rate": 3.881153125532277e-05,
      "loss": 0.6391,
      "step": 1471400
    },
    {
      "epoch": 13.427074968975838,
      "grad_norm": 3.904287338256836,
      "learning_rate": 3.88107708591868e-05,
      "loss": 0.6828,
      "step": 1471500
    },
    {
      "epoch": 13.427987444339003,
      "grad_norm": 4.173684597015381,
      "learning_rate": 3.881001046305083e-05,
      "loss": 0.6424,
      "step": 1471600
    },
    {
      "epoch": 13.428899919702168,
      "grad_norm": 4.612926959991455,
      "learning_rate": 3.880925006691486e-05,
      "loss": 0.6563,
      "step": 1471700
    },
    {
      "epoch": 13.429812395065333,
      "grad_norm": 3.665666341781616,
      "learning_rate": 3.880848967077889e-05,
      "loss": 0.7075,
      "step": 1471800
    },
    {
      "epoch": 13.430724870428499,
      "grad_norm": 3.15464448928833,
      "learning_rate": 3.8807729274642915e-05,
      "loss": 0.6504,
      "step": 1471900
    },
    {
      "epoch": 13.431637345791664,
      "grad_norm": 3.3042311668395996,
      "learning_rate": 3.880696887850695e-05,
      "loss": 0.6598,
      "step": 1472000
    },
    {
      "epoch": 13.43254982115483,
      "grad_norm": 3.70980167388916,
      "learning_rate": 3.8806208482370975e-05,
      "loss": 0.6512,
      "step": 1472100
    },
    {
      "epoch": 13.433462296517995,
      "grad_norm": 4.838364124298096,
      "learning_rate": 3.8805448086235006e-05,
      "loss": 0.6844,
      "step": 1472200
    },
    {
      "epoch": 13.43437477188116,
      "grad_norm": 3.7335286140441895,
      "learning_rate": 3.8804687690099036e-05,
      "loss": 0.6197,
      "step": 1472300
    },
    {
      "epoch": 13.435287247244325,
      "grad_norm": 4.154088020324707,
      "learning_rate": 3.8803927293963066e-05,
      "loss": 0.6866,
      "step": 1472400
    },
    {
      "epoch": 13.436199722607489,
      "grad_norm": 3.7652080059051514,
      "learning_rate": 3.880316689782709e-05,
      "loss": 0.7042,
      "step": 1472500
    },
    {
      "epoch": 13.437112197970654,
      "grad_norm": 2.599637746810913,
      "learning_rate": 3.8802406501691126e-05,
      "loss": 0.6837,
      "step": 1472600
    },
    {
      "epoch": 13.43802467333382,
      "grad_norm": 4.804825305938721,
      "learning_rate": 3.880164610555515e-05,
      "loss": 0.6878,
      "step": 1472700
    },
    {
      "epoch": 13.438937148696985,
      "grad_norm": 4.673567295074463,
      "learning_rate": 3.880088570941918e-05,
      "loss": 0.639,
      "step": 1472800
    },
    {
      "epoch": 13.43984962406015,
      "grad_norm": 3.4101879596710205,
      "learning_rate": 3.880012531328321e-05,
      "loss": 0.7088,
      "step": 1472900
    },
    {
      "epoch": 13.440762099423315,
      "grad_norm": 3.360426664352417,
      "learning_rate": 3.879936491714724e-05,
      "loss": 0.6801,
      "step": 1473000
    },
    {
      "epoch": 13.44167457478648,
      "grad_norm": 2.376727819442749,
      "learning_rate": 3.879860452101127e-05,
      "loss": 0.6566,
      "step": 1473100
    },
    {
      "epoch": 13.442587050149646,
      "grad_norm": 2.372689962387085,
      "learning_rate": 3.87978441248753e-05,
      "loss": 0.6944,
      "step": 1473200
    },
    {
      "epoch": 13.443499525512811,
      "grad_norm": 3.8496339321136475,
      "learning_rate": 3.879708372873932e-05,
      "loss": 0.6849,
      "step": 1473300
    },
    {
      "epoch": 13.444412000875976,
      "grad_norm": 3.939002275466919,
      "learning_rate": 3.879632333260336e-05,
      "loss": 0.7077,
      "step": 1473400
    },
    {
      "epoch": 13.445324476239142,
      "grad_norm": 3.682006359100342,
      "learning_rate": 3.879556293646738e-05,
      "loss": 0.6383,
      "step": 1473500
    },
    {
      "epoch": 13.446236951602307,
      "grad_norm": 4.058547019958496,
      "learning_rate": 3.879480254033141e-05,
      "loss": 0.6613,
      "step": 1473600
    },
    {
      "epoch": 13.447149426965472,
      "grad_norm": 3.6771700382232666,
      "learning_rate": 3.879404214419544e-05,
      "loss": 0.684,
      "step": 1473700
    },
    {
      "epoch": 13.448061902328638,
      "grad_norm": 3.7926971912384033,
      "learning_rate": 3.879328174805947e-05,
      "loss": 0.6478,
      "step": 1473800
    },
    {
      "epoch": 13.448974377691803,
      "grad_norm": 3.96639347076416,
      "learning_rate": 3.87925213519235e-05,
      "loss": 0.6678,
      "step": 1473900
    },
    {
      "epoch": 13.449886853054968,
      "grad_norm": 3.931680917739868,
      "learning_rate": 3.879176095578753e-05,
      "loss": 0.6737,
      "step": 1474000
    },
    {
      "epoch": 13.450799328418134,
      "grad_norm": 4.265635967254639,
      "learning_rate": 3.8791000559651557e-05,
      "loss": 0.6995,
      "step": 1474100
    },
    {
      "epoch": 13.451711803781297,
      "grad_norm": 4.028778076171875,
      "learning_rate": 3.8790240163515587e-05,
      "loss": 0.6781,
      "step": 1474200
    },
    {
      "epoch": 13.452624279144462,
      "grad_norm": 3.9810171127319336,
      "learning_rate": 3.878947976737962e-05,
      "loss": 0.6789,
      "step": 1474300
    },
    {
      "epoch": 13.453536754507628,
      "grad_norm": 5.001782417297363,
      "learning_rate": 3.878871937124364e-05,
      "loss": 0.6799,
      "step": 1474400
    },
    {
      "epoch": 13.454449229870793,
      "grad_norm": 4.930078983306885,
      "learning_rate": 3.878795897510768e-05,
      "loss": 0.6655,
      "step": 1474500
    },
    {
      "epoch": 13.455361705233958,
      "grad_norm": 3.9249749183654785,
      "learning_rate": 3.87871985789717e-05,
      "loss": 0.6596,
      "step": 1474600
    },
    {
      "epoch": 13.456274180597124,
      "grad_norm": 3.675898790359497,
      "learning_rate": 3.878643818283573e-05,
      "loss": 0.649,
      "step": 1474700
    },
    {
      "epoch": 13.457186655960289,
      "grad_norm": 4.169257640838623,
      "learning_rate": 3.878567778669976e-05,
      "loss": 0.6838,
      "step": 1474800
    },
    {
      "epoch": 13.458099131323454,
      "grad_norm": 3.2968244552612305,
      "learning_rate": 3.878491739056379e-05,
      "loss": 0.6877,
      "step": 1474900
    },
    {
      "epoch": 13.45901160668662,
      "grad_norm": 3.843759298324585,
      "learning_rate": 3.878415699442782e-05,
      "loss": 0.6845,
      "step": 1475000
    },
    {
      "epoch": 13.459924082049785,
      "grad_norm": 3.653810977935791,
      "learning_rate": 3.878339659829185e-05,
      "loss": 0.6941,
      "step": 1475100
    },
    {
      "epoch": 13.46083655741295,
      "grad_norm": 3.720348596572876,
      "learning_rate": 3.8782636202155874e-05,
      "loss": 0.6792,
      "step": 1475200
    },
    {
      "epoch": 13.461749032776115,
      "grad_norm": 3.2134721279144287,
      "learning_rate": 3.878187580601991e-05,
      "loss": 0.6379,
      "step": 1475300
    },
    {
      "epoch": 13.46266150813928,
      "grad_norm": 3.968616008758545,
      "learning_rate": 3.8781115409883934e-05,
      "loss": 0.6834,
      "step": 1475400
    },
    {
      "epoch": 13.463573983502446,
      "grad_norm": 4.643248081207275,
      "learning_rate": 3.8780355013747964e-05,
      "loss": 0.6443,
      "step": 1475500
    },
    {
      "epoch": 13.464486458865611,
      "grad_norm": 4.975377559661865,
      "learning_rate": 3.8779594617611994e-05,
      "loss": 0.6921,
      "step": 1475600
    },
    {
      "epoch": 13.465398934228777,
      "grad_norm": 4.685425281524658,
      "learning_rate": 3.8778834221476024e-05,
      "loss": 0.6625,
      "step": 1475700
    },
    {
      "epoch": 13.466311409591942,
      "grad_norm": 4.278586387634277,
      "learning_rate": 3.877807382534005e-05,
      "loss": 0.7321,
      "step": 1475800
    },
    {
      "epoch": 13.467223884955105,
      "grad_norm": 3.9185526371002197,
      "learning_rate": 3.8777313429204084e-05,
      "loss": 0.6637,
      "step": 1475900
    },
    {
      "epoch": 13.46813636031827,
      "grad_norm": 3.8490166664123535,
      "learning_rate": 3.877655303306811e-05,
      "loss": 0.677,
      "step": 1476000
    },
    {
      "epoch": 13.469048835681436,
      "grad_norm": 4.364234447479248,
      "learning_rate": 3.877579263693214e-05,
      "loss": 0.707,
      "step": 1476100
    },
    {
      "epoch": 13.469961311044601,
      "grad_norm": 3.478878974914551,
      "learning_rate": 3.877503224079617e-05,
      "loss": 0.6621,
      "step": 1476200
    },
    {
      "epoch": 13.470873786407767,
      "grad_norm": 4.067657470703125,
      "learning_rate": 3.87742718446602e-05,
      "loss": 0.658,
      "step": 1476300
    },
    {
      "epoch": 13.471786261770932,
      "grad_norm": 3.7144501209259033,
      "learning_rate": 3.877351144852423e-05,
      "loss": 0.6843,
      "step": 1476400
    },
    {
      "epoch": 13.472698737134097,
      "grad_norm": 3.3761749267578125,
      "learning_rate": 3.877275105238826e-05,
      "loss": 0.7062,
      "step": 1476500
    },
    {
      "epoch": 13.473611212497262,
      "grad_norm": 4.181093215942383,
      "learning_rate": 3.877199065625228e-05,
      "loss": 0.7236,
      "step": 1476600
    },
    {
      "epoch": 13.474523687860428,
      "grad_norm": 4.735819339752197,
      "learning_rate": 3.877123026011632e-05,
      "loss": 0.7138,
      "step": 1476700
    },
    {
      "epoch": 13.475436163223593,
      "grad_norm": 4.552746295928955,
      "learning_rate": 3.877046986398034e-05,
      "loss": 0.6818,
      "step": 1476800
    },
    {
      "epoch": 13.476348638586758,
      "grad_norm": 4.4419355392456055,
      "learning_rate": 3.8769709467844365e-05,
      "loss": 0.6951,
      "step": 1476900
    },
    {
      "epoch": 13.477261113949924,
      "grad_norm": 4.064974308013916,
      "learning_rate": 3.87689490717084e-05,
      "loss": 0.6524,
      "step": 1477000
    },
    {
      "epoch": 13.478173589313089,
      "grad_norm": 3.6165382862091064,
      "learning_rate": 3.8768188675572425e-05,
      "loss": 0.6632,
      "step": 1477100
    },
    {
      "epoch": 13.479086064676254,
      "grad_norm": 3.2902402877807617,
      "learning_rate": 3.8767428279436455e-05,
      "loss": 0.7063,
      "step": 1477200
    },
    {
      "epoch": 13.47999854003942,
      "grad_norm": 3.956125497817993,
      "learning_rate": 3.8766667883300485e-05,
      "loss": 0.6292,
      "step": 1477300
    },
    {
      "epoch": 13.480911015402585,
      "grad_norm": 2.8906033039093018,
      "learning_rate": 3.8765907487164515e-05,
      "loss": 0.6551,
      "step": 1477400
    },
    {
      "epoch": 13.48182349076575,
      "grad_norm": 3.056727409362793,
      "learning_rate": 3.8765147091028545e-05,
      "loss": 0.6578,
      "step": 1477500
    },
    {
      "epoch": 13.482735966128914,
      "grad_norm": 3.8067433834075928,
      "learning_rate": 3.8764386694892575e-05,
      "loss": 0.6721,
      "step": 1477600
    },
    {
      "epoch": 13.483648441492079,
      "grad_norm": 3.958322286605835,
      "learning_rate": 3.87636262987566e-05,
      "loss": 0.6372,
      "step": 1477700
    },
    {
      "epoch": 13.484560916855244,
      "grad_norm": 4.637700080871582,
      "learning_rate": 3.8762865902620635e-05,
      "loss": 0.679,
      "step": 1477800
    },
    {
      "epoch": 13.48547339221841,
      "grad_norm": 3.526716470718384,
      "learning_rate": 3.876210550648466e-05,
      "loss": 0.6672,
      "step": 1477900
    },
    {
      "epoch": 13.486385867581575,
      "grad_norm": 3.7375998497009277,
      "learning_rate": 3.876134511034869e-05,
      "loss": 0.6623,
      "step": 1478000
    },
    {
      "epoch": 13.48729834294474,
      "grad_norm": 2.9283595085144043,
      "learning_rate": 3.876058471421272e-05,
      "loss": 0.6965,
      "step": 1478100
    },
    {
      "epoch": 13.488210818307905,
      "grad_norm": 4.161949634552002,
      "learning_rate": 3.875982431807675e-05,
      "loss": 0.6813,
      "step": 1478200
    },
    {
      "epoch": 13.48912329367107,
      "grad_norm": 4.076164722442627,
      "learning_rate": 3.875906392194077e-05,
      "loss": 0.652,
      "step": 1478300
    },
    {
      "epoch": 13.490035769034236,
      "grad_norm": 4.182167053222656,
      "learning_rate": 3.875830352580481e-05,
      "loss": 0.662,
      "step": 1478400
    },
    {
      "epoch": 13.490948244397401,
      "grad_norm": 4.186101913452148,
      "learning_rate": 3.875754312966883e-05,
      "loss": 0.6839,
      "step": 1478500
    },
    {
      "epoch": 13.491860719760567,
      "grad_norm": 3.3086564540863037,
      "learning_rate": 3.875678273353286e-05,
      "loss": 0.6616,
      "step": 1478600
    },
    {
      "epoch": 13.492773195123732,
      "grad_norm": 3.8566064834594727,
      "learning_rate": 3.875602233739689e-05,
      "loss": 0.6659,
      "step": 1478700
    },
    {
      "epoch": 13.493685670486897,
      "grad_norm": 3.839585781097412,
      "learning_rate": 3.875526194126092e-05,
      "loss": 0.6635,
      "step": 1478800
    },
    {
      "epoch": 13.494598145850063,
      "grad_norm": 4.241000175476074,
      "learning_rate": 3.875450154512495e-05,
      "loss": 0.6805,
      "step": 1478900
    },
    {
      "epoch": 13.495510621213228,
      "grad_norm": 2.388671636581421,
      "learning_rate": 3.875374114898898e-05,
      "loss": 0.663,
      "step": 1479000
    },
    {
      "epoch": 13.496423096576393,
      "grad_norm": 3.8078453540802,
      "learning_rate": 3.8752980752853006e-05,
      "loss": 0.69,
      "step": 1479100
    },
    {
      "epoch": 13.497335571939558,
      "grad_norm": 2.477656364440918,
      "learning_rate": 3.875222035671704e-05,
      "loss": 0.6762,
      "step": 1479200
    },
    {
      "epoch": 13.498248047302722,
      "grad_norm": 4.235654354095459,
      "learning_rate": 3.8751459960581066e-05,
      "loss": 0.6483,
      "step": 1479300
    },
    {
      "epoch": 13.499160522665887,
      "grad_norm": 4.640928268432617,
      "learning_rate": 3.8750699564445096e-05,
      "loss": 0.7071,
      "step": 1479400
    },
    {
      "epoch": 13.500072998029053,
      "grad_norm": 3.4591057300567627,
      "learning_rate": 3.8749939168309126e-05,
      "loss": 0.7154,
      "step": 1479500
    },
    {
      "epoch": 13.500985473392218,
      "grad_norm": 4.599456787109375,
      "learning_rate": 3.8749178772173156e-05,
      "loss": 0.6452,
      "step": 1479600
    },
    {
      "epoch": 13.501897948755383,
      "grad_norm": 3.5470197200775146,
      "learning_rate": 3.874841837603718e-05,
      "loss": 0.6548,
      "step": 1479700
    },
    {
      "epoch": 13.502810424118548,
      "grad_norm": 3.916560411453247,
      "learning_rate": 3.874765797990121e-05,
      "loss": 0.6535,
      "step": 1479800
    },
    {
      "epoch": 13.503722899481714,
      "grad_norm": 4.8827056884765625,
      "learning_rate": 3.874689758376524e-05,
      "loss": 0.6841,
      "step": 1479900
    },
    {
      "epoch": 13.504635374844879,
      "grad_norm": 4.938735008239746,
      "learning_rate": 3.874613718762927e-05,
      "loss": 0.7034,
      "step": 1480000
    },
    {
      "epoch": 13.505547850208044,
      "grad_norm": 4.8535332679748535,
      "learning_rate": 3.87453767914933e-05,
      "loss": 0.6946,
      "step": 1480100
    },
    {
      "epoch": 13.50646032557121,
      "grad_norm": 4.407260417938232,
      "learning_rate": 3.874461639535732e-05,
      "loss": 0.6556,
      "step": 1480200
    },
    {
      "epoch": 13.507372800934375,
      "grad_norm": 4.58603572845459,
      "learning_rate": 3.874385599922136e-05,
      "loss": 0.6802,
      "step": 1480300
    },
    {
      "epoch": 13.50828527629754,
      "grad_norm": 3.9458441734313965,
      "learning_rate": 3.874309560308538e-05,
      "loss": 0.6938,
      "step": 1480400
    },
    {
      "epoch": 13.509197751660706,
      "grad_norm": 3.2699668407440186,
      "learning_rate": 3.874233520694941e-05,
      "loss": 0.6751,
      "step": 1480500
    },
    {
      "epoch": 13.51011022702387,
      "grad_norm": 5.104027271270752,
      "learning_rate": 3.874157481081344e-05,
      "loss": 0.6551,
      "step": 1480600
    },
    {
      "epoch": 13.511022702387036,
      "grad_norm": 3.6519298553466797,
      "learning_rate": 3.874081441467747e-05,
      "loss": 0.6573,
      "step": 1480700
    },
    {
      "epoch": 13.511935177750201,
      "grad_norm": 3.539626121520996,
      "learning_rate": 3.8740054018541496e-05,
      "loss": 0.669,
      "step": 1480800
    },
    {
      "epoch": 13.512847653113365,
      "grad_norm": 4.835097312927246,
      "learning_rate": 3.873929362240553e-05,
      "loss": 0.6967,
      "step": 1480900
    },
    {
      "epoch": 13.51376012847653,
      "grad_norm": 4.565677642822266,
      "learning_rate": 3.8738533226269557e-05,
      "loss": 0.6977,
      "step": 1481000
    },
    {
      "epoch": 13.514672603839696,
      "grad_norm": 4.213099002838135,
      "learning_rate": 3.873777283013359e-05,
      "loss": 0.699,
      "step": 1481100
    },
    {
      "epoch": 13.515585079202861,
      "grad_norm": 1.8268189430236816,
      "learning_rate": 3.873701243399762e-05,
      "loss": 0.6335,
      "step": 1481200
    },
    {
      "epoch": 13.516497554566026,
      "grad_norm": 4.006890773773193,
      "learning_rate": 3.873625203786165e-05,
      "loss": 0.6319,
      "step": 1481300
    },
    {
      "epoch": 13.517410029929191,
      "grad_norm": 4.526523113250732,
      "learning_rate": 3.873549164172568e-05,
      "loss": 0.6967,
      "step": 1481400
    },
    {
      "epoch": 13.518322505292357,
      "grad_norm": 4.777139663696289,
      "learning_rate": 3.873473124558971e-05,
      "loss": 0.6394,
      "step": 1481500
    },
    {
      "epoch": 13.519234980655522,
      "grad_norm": 4.268520832061768,
      "learning_rate": 3.873397084945373e-05,
      "loss": 0.7081,
      "step": 1481600
    },
    {
      "epoch": 13.520147456018687,
      "grad_norm": 4.625517845153809,
      "learning_rate": 3.873321045331777e-05,
      "loss": 0.7159,
      "step": 1481700
    },
    {
      "epoch": 13.521059931381853,
      "grad_norm": 4.284307479858398,
      "learning_rate": 3.873245005718179e-05,
      "loss": 0.7012,
      "step": 1481800
    },
    {
      "epoch": 13.521972406745018,
      "grad_norm": 4.830559730529785,
      "learning_rate": 3.873168966104582e-05,
      "loss": 0.6467,
      "step": 1481900
    },
    {
      "epoch": 13.522884882108183,
      "grad_norm": 3.3502986431121826,
      "learning_rate": 3.873092926490985e-05,
      "loss": 0.6877,
      "step": 1482000
    },
    {
      "epoch": 13.523797357471349,
      "grad_norm": 4.848113536834717,
      "learning_rate": 3.873016886877388e-05,
      "loss": 0.6792,
      "step": 1482100
    },
    {
      "epoch": 13.524709832834514,
      "grad_norm": 3.799245595932007,
      "learning_rate": 3.8729408472637904e-05,
      "loss": 0.6971,
      "step": 1482200
    },
    {
      "epoch": 13.52562230819768,
      "grad_norm": 3.667942523956299,
      "learning_rate": 3.872864807650194e-05,
      "loss": 0.6723,
      "step": 1482300
    },
    {
      "epoch": 13.526534783560844,
      "grad_norm": 3.7352237701416016,
      "learning_rate": 3.8727887680365964e-05,
      "loss": 0.6361,
      "step": 1482400
    },
    {
      "epoch": 13.52744725892401,
      "grad_norm": 4.2431111335754395,
      "learning_rate": 3.8727127284229994e-05,
      "loss": 0.6585,
      "step": 1482500
    },
    {
      "epoch": 13.528359734287175,
      "grad_norm": 4.292515754699707,
      "learning_rate": 3.8726366888094024e-05,
      "loss": 0.6804,
      "step": 1482600
    },
    {
      "epoch": 13.529272209650339,
      "grad_norm": 4.066946983337402,
      "learning_rate": 3.872560649195805e-05,
      "loss": 0.6745,
      "step": 1482700
    },
    {
      "epoch": 13.530184685013504,
      "grad_norm": 3.9165592193603516,
      "learning_rate": 3.8724846095822084e-05,
      "loss": 0.6643,
      "step": 1482800
    },
    {
      "epoch": 13.53109716037667,
      "grad_norm": 2.6749982833862305,
      "learning_rate": 3.872408569968611e-05,
      "loss": 0.6524,
      "step": 1482900
    },
    {
      "epoch": 13.532009635739835,
      "grad_norm": 3.918346881866455,
      "learning_rate": 3.872332530355014e-05,
      "loss": 0.6664,
      "step": 1483000
    },
    {
      "epoch": 13.532922111103,
      "grad_norm": 4.169600009918213,
      "learning_rate": 3.872256490741417e-05,
      "loss": 0.6249,
      "step": 1483100
    },
    {
      "epoch": 13.533834586466165,
      "grad_norm": 4.894669055938721,
      "learning_rate": 3.87218045112782e-05,
      "loss": 0.6605,
      "step": 1483200
    },
    {
      "epoch": 13.53474706182933,
      "grad_norm": 3.8103060722351074,
      "learning_rate": 3.872104411514222e-05,
      "loss": 0.6466,
      "step": 1483300
    },
    {
      "epoch": 13.535659537192496,
      "grad_norm": 4.004698753356934,
      "learning_rate": 3.872028371900626e-05,
      "loss": 0.66,
      "step": 1483400
    },
    {
      "epoch": 13.536572012555661,
      "grad_norm": 3.704216241836548,
      "learning_rate": 3.871952332287028e-05,
      "loss": 0.6646,
      "step": 1483500
    },
    {
      "epoch": 13.537484487918826,
      "grad_norm": 3.2082319259643555,
      "learning_rate": 3.871876292673431e-05,
      "loss": 0.6923,
      "step": 1483600
    },
    {
      "epoch": 13.538396963281992,
      "grad_norm": 3.5860888957977295,
      "learning_rate": 3.871800253059834e-05,
      "loss": 0.6728,
      "step": 1483700
    },
    {
      "epoch": 13.539309438645157,
      "grad_norm": 4.315729141235352,
      "learning_rate": 3.871724213446237e-05,
      "loss": 0.6835,
      "step": 1483800
    },
    {
      "epoch": 13.540221914008322,
      "grad_norm": 3.949373245239258,
      "learning_rate": 3.87164817383264e-05,
      "loss": 0.6793,
      "step": 1483900
    },
    {
      "epoch": 13.541134389371488,
      "grad_norm": 3.926814317703247,
      "learning_rate": 3.871572134219043e-05,
      "loss": 0.7105,
      "step": 1484000
    },
    {
      "epoch": 13.542046864734653,
      "grad_norm": 3.9395251274108887,
      "learning_rate": 3.8714960946054455e-05,
      "loss": 0.6889,
      "step": 1484100
    },
    {
      "epoch": 13.542959340097818,
      "grad_norm": 3.4996800422668457,
      "learning_rate": 3.871420054991849e-05,
      "loss": 0.689,
      "step": 1484200
    },
    {
      "epoch": 13.543871815460982,
      "grad_norm": 4.011775493621826,
      "learning_rate": 3.8713440153782515e-05,
      "loss": 0.6777,
      "step": 1484300
    },
    {
      "epoch": 13.544784290824147,
      "grad_norm": 4.021995544433594,
      "learning_rate": 3.8712679757646545e-05,
      "loss": 0.6597,
      "step": 1484400
    },
    {
      "epoch": 13.545696766187312,
      "grad_norm": 3.9907071590423584,
      "learning_rate": 3.8711919361510575e-05,
      "loss": 0.6273,
      "step": 1484500
    },
    {
      "epoch": 13.546609241550478,
      "grad_norm": 3.5118825435638428,
      "learning_rate": 3.8711158965374605e-05,
      "loss": 0.6873,
      "step": 1484600
    },
    {
      "epoch": 13.547521716913643,
      "grad_norm": 4.92122745513916,
      "learning_rate": 3.871039856923863e-05,
      "loss": 0.6699,
      "step": 1484700
    },
    {
      "epoch": 13.548434192276808,
      "grad_norm": 2.2422263622283936,
      "learning_rate": 3.8709638173102665e-05,
      "loss": 0.6832,
      "step": 1484800
    },
    {
      "epoch": 13.549346667639973,
      "grad_norm": 3.8899571895599365,
      "learning_rate": 3.870887777696669e-05,
      "loss": 0.69,
      "step": 1484900
    },
    {
      "epoch": 13.550259143003139,
      "grad_norm": 3.9532711505889893,
      "learning_rate": 3.870811738083072e-05,
      "loss": 0.6603,
      "step": 1485000
    },
    {
      "epoch": 13.551171618366304,
      "grad_norm": 4.015651702880859,
      "learning_rate": 3.870735698469475e-05,
      "loss": 0.6912,
      "step": 1485100
    },
    {
      "epoch": 13.55208409372947,
      "grad_norm": 3.862086296081543,
      "learning_rate": 3.870659658855878e-05,
      "loss": 0.6983,
      "step": 1485200
    },
    {
      "epoch": 13.552996569092635,
      "grad_norm": 3.877588987350464,
      "learning_rate": 3.870583619242281e-05,
      "loss": 0.6522,
      "step": 1485300
    },
    {
      "epoch": 13.5539090444558,
      "grad_norm": 3.4880759716033936,
      "learning_rate": 3.870507579628683e-05,
      "loss": 0.6498,
      "step": 1485400
    },
    {
      "epoch": 13.554821519818965,
      "grad_norm": 4.199743747711182,
      "learning_rate": 3.870431540015086e-05,
      "loss": 0.6725,
      "step": 1485500
    },
    {
      "epoch": 13.55573399518213,
      "grad_norm": 3.0438473224639893,
      "learning_rate": 3.870355500401489e-05,
      "loss": 0.6797,
      "step": 1485600
    },
    {
      "epoch": 13.556646470545296,
      "grad_norm": 3.208228826522827,
      "learning_rate": 3.870279460787892e-05,
      "loss": 0.7043,
      "step": 1485700
    },
    {
      "epoch": 13.557558945908461,
      "grad_norm": 3.3559608459472656,
      "learning_rate": 3.870203421174295e-05,
      "loss": 0.6471,
      "step": 1485800
    },
    {
      "epoch": 13.558471421271626,
      "grad_norm": 3.559093475341797,
      "learning_rate": 3.870127381560698e-05,
      "loss": 0.7064,
      "step": 1485900
    },
    {
      "epoch": 13.559383896634792,
      "grad_norm": 3.286944627761841,
      "learning_rate": 3.8700513419471006e-05,
      "loss": 0.7009,
      "step": 1486000
    },
    {
      "epoch": 13.560296371997955,
      "grad_norm": 3.849196195602417,
      "learning_rate": 3.8699753023335036e-05,
      "loss": 0.6808,
      "step": 1486100
    },
    {
      "epoch": 13.56120884736112,
      "grad_norm": 3.292518138885498,
      "learning_rate": 3.8698992627199066e-05,
      "loss": 0.6713,
      "step": 1486200
    },
    {
      "epoch": 13.562121322724286,
      "grad_norm": 2.8834495544433594,
      "learning_rate": 3.8698232231063096e-05,
      "loss": 0.6894,
      "step": 1486300
    },
    {
      "epoch": 13.563033798087451,
      "grad_norm": 4.489779472351074,
      "learning_rate": 3.8697471834927126e-05,
      "loss": 0.6774,
      "step": 1486400
    },
    {
      "epoch": 13.563946273450616,
      "grad_norm": 4.029937744140625,
      "learning_rate": 3.8696711438791156e-05,
      "loss": 0.6802,
      "step": 1486500
    },
    {
      "epoch": 13.564858748813782,
      "grad_norm": 3.6605892181396484,
      "learning_rate": 3.869595104265518e-05,
      "loss": 0.6339,
      "step": 1486600
    },
    {
      "epoch": 13.565771224176947,
      "grad_norm": 4.843495845794678,
      "learning_rate": 3.8695190646519216e-05,
      "loss": 0.6976,
      "step": 1486700
    },
    {
      "epoch": 13.566683699540112,
      "grad_norm": 2.4148831367492676,
      "learning_rate": 3.869443025038324e-05,
      "loss": 0.6388,
      "step": 1486800
    },
    {
      "epoch": 13.567596174903278,
      "grad_norm": 3.8423523902893066,
      "learning_rate": 3.869366985424727e-05,
      "loss": 0.6521,
      "step": 1486900
    },
    {
      "epoch": 13.568508650266443,
      "grad_norm": 5.104452610015869,
      "learning_rate": 3.86929094581113e-05,
      "loss": 0.6901,
      "step": 1487000
    },
    {
      "epoch": 13.569421125629608,
      "grad_norm": 4.5351996421813965,
      "learning_rate": 3.869214906197533e-05,
      "loss": 0.6714,
      "step": 1487100
    },
    {
      "epoch": 13.570333600992774,
      "grad_norm": 3.296037197113037,
      "learning_rate": 3.869138866583936e-05,
      "loss": 0.6476,
      "step": 1487200
    },
    {
      "epoch": 13.571246076355939,
      "grad_norm": 3.9188780784606934,
      "learning_rate": 3.869062826970339e-05,
      "loss": 0.6686,
      "step": 1487300
    },
    {
      "epoch": 13.572158551719104,
      "grad_norm": 4.220997333526611,
      "learning_rate": 3.868986787356741e-05,
      "loss": 0.6512,
      "step": 1487400
    },
    {
      "epoch": 13.57307102708227,
      "grad_norm": 3.8877785205841064,
      "learning_rate": 3.868910747743144e-05,
      "loss": 0.6524,
      "step": 1487500
    },
    {
      "epoch": 13.573983502445435,
      "grad_norm": 4.532663345336914,
      "learning_rate": 3.868834708129547e-05,
      "loss": 0.6676,
      "step": 1487600
    },
    {
      "epoch": 13.574895977808598,
      "grad_norm": 3.6957881450653076,
      "learning_rate": 3.86875866851595e-05,
      "loss": 0.6772,
      "step": 1487700
    },
    {
      "epoch": 13.575808453171764,
      "grad_norm": 4.058531761169434,
      "learning_rate": 3.8686826289023533e-05,
      "loss": 0.6998,
      "step": 1487800
    },
    {
      "epoch": 13.576720928534929,
      "grad_norm": 3.336801290512085,
      "learning_rate": 3.8686065892887563e-05,
      "loss": 0.6451,
      "step": 1487900
    },
    {
      "epoch": 13.577633403898094,
      "grad_norm": 3.8986411094665527,
      "learning_rate": 3.868530549675159e-05,
      "loss": 0.6492,
      "step": 1488000
    },
    {
      "epoch": 13.57854587926126,
      "grad_norm": 2.894707202911377,
      "learning_rate": 3.8684545100615624e-05,
      "loss": 0.6507,
      "step": 1488100
    },
    {
      "epoch": 13.579458354624425,
      "grad_norm": 3.2949774265289307,
      "learning_rate": 3.868378470447965e-05,
      "loss": 0.6712,
      "step": 1488200
    },
    {
      "epoch": 13.58037082998759,
      "grad_norm": 4.055728435516357,
      "learning_rate": 3.868302430834368e-05,
      "loss": 0.6675,
      "step": 1488300
    },
    {
      "epoch": 13.581283305350755,
      "grad_norm": 3.01387619972229,
      "learning_rate": 3.868226391220771e-05,
      "loss": 0.6272,
      "step": 1488400
    },
    {
      "epoch": 13.58219578071392,
      "grad_norm": 4.721123695373535,
      "learning_rate": 3.868150351607173e-05,
      "loss": 0.672,
      "step": 1488500
    },
    {
      "epoch": 13.583108256077086,
      "grad_norm": 3.689210891723633,
      "learning_rate": 3.868074311993577e-05,
      "loss": 0.7149,
      "step": 1488600
    },
    {
      "epoch": 13.584020731440251,
      "grad_norm": 2.3865134716033936,
      "learning_rate": 3.867998272379979e-05,
      "loss": 0.7085,
      "step": 1488700
    },
    {
      "epoch": 13.584933206803417,
      "grad_norm": 3.8328046798706055,
      "learning_rate": 3.867922232766382e-05,
      "loss": 0.6927,
      "step": 1488800
    },
    {
      "epoch": 13.585845682166582,
      "grad_norm": 4.619131565093994,
      "learning_rate": 3.867846193152785e-05,
      "loss": 0.6889,
      "step": 1488900
    },
    {
      "epoch": 13.586758157529747,
      "grad_norm": 3.3914053440093994,
      "learning_rate": 3.867770153539188e-05,
      "loss": 0.6351,
      "step": 1489000
    },
    {
      "epoch": 13.587670632892912,
      "grad_norm": 3.3371963500976562,
      "learning_rate": 3.8676941139255904e-05,
      "loss": 0.6719,
      "step": 1489100
    },
    {
      "epoch": 13.588583108256078,
      "grad_norm": 3.5983142852783203,
      "learning_rate": 3.867618074311994e-05,
      "loss": 0.6423,
      "step": 1489200
    },
    {
      "epoch": 13.589495583619243,
      "grad_norm": 3.9638710021972656,
      "learning_rate": 3.8675420346983964e-05,
      "loss": 0.6764,
      "step": 1489300
    },
    {
      "epoch": 13.590408058982408,
      "grad_norm": 3.738693952560425,
      "learning_rate": 3.8674659950847994e-05,
      "loss": 0.6487,
      "step": 1489400
    },
    {
      "epoch": 13.591320534345572,
      "grad_norm": 3.296820878982544,
      "learning_rate": 3.8673899554712024e-05,
      "loss": 0.6328,
      "step": 1489500
    },
    {
      "epoch": 13.592233009708737,
      "grad_norm": 4.032336711883545,
      "learning_rate": 3.8673139158576054e-05,
      "loss": 0.6524,
      "step": 1489600
    },
    {
      "epoch": 13.593145485071902,
      "grad_norm": 2.849856376647949,
      "learning_rate": 3.8672378762440084e-05,
      "loss": 0.7187,
      "step": 1489700
    },
    {
      "epoch": 13.594057960435068,
      "grad_norm": 3.304173231124878,
      "learning_rate": 3.8671618366304114e-05,
      "loss": 0.6758,
      "step": 1489800
    },
    {
      "epoch": 13.594970435798233,
      "grad_norm": 3.771514892578125,
      "learning_rate": 3.867085797016814e-05,
      "loss": 0.7159,
      "step": 1489900
    },
    {
      "epoch": 13.595882911161398,
      "grad_norm": 3.935126304626465,
      "learning_rate": 3.8670097574032175e-05,
      "loss": 0.6946,
      "step": 1490000
    },
    {
      "epoch": 13.596795386524564,
      "grad_norm": 2.8117334842681885,
      "learning_rate": 3.86693371778962e-05,
      "loss": 0.6819,
      "step": 1490100
    },
    {
      "epoch": 13.597707861887729,
      "grad_norm": 3.585036516189575,
      "learning_rate": 3.866857678176023e-05,
      "loss": 0.6532,
      "step": 1490200
    },
    {
      "epoch": 13.598620337250894,
      "grad_norm": 4.399777412414551,
      "learning_rate": 3.866781638562426e-05,
      "loss": 0.6711,
      "step": 1490300
    },
    {
      "epoch": 13.59953281261406,
      "grad_norm": 5.21533727645874,
      "learning_rate": 3.866705598948829e-05,
      "loss": 0.6843,
      "step": 1490400
    },
    {
      "epoch": 13.600445287977225,
      "grad_norm": 3.8353159427642822,
      "learning_rate": 3.866629559335231e-05,
      "loss": 0.665,
      "step": 1490500
    },
    {
      "epoch": 13.60135776334039,
      "grad_norm": 4.740513801574707,
      "learning_rate": 3.866553519721635e-05,
      "loss": 0.679,
      "step": 1490600
    },
    {
      "epoch": 13.602270238703555,
      "grad_norm": 3.3481411933898926,
      "learning_rate": 3.866477480108037e-05,
      "loss": 0.6649,
      "step": 1490700
    },
    {
      "epoch": 13.60318271406672,
      "grad_norm": 3.4612858295440674,
      "learning_rate": 3.86640144049444e-05,
      "loss": 0.6729,
      "step": 1490800
    },
    {
      "epoch": 13.604095189429886,
      "grad_norm": 3.8089489936828613,
      "learning_rate": 3.866325400880843e-05,
      "loss": 0.7009,
      "step": 1490900
    },
    {
      "epoch": 13.605007664793051,
      "grad_norm": 5.500964641571045,
      "learning_rate": 3.866249361267246e-05,
      "loss": 0.6734,
      "step": 1491000
    },
    {
      "epoch": 13.605920140156215,
      "grad_norm": 4.171120643615723,
      "learning_rate": 3.866173321653649e-05,
      "loss": 0.6708,
      "step": 1491100
    },
    {
      "epoch": 13.60683261551938,
      "grad_norm": 3.185232162475586,
      "learning_rate": 3.8660972820400515e-05,
      "loss": 0.6294,
      "step": 1491200
    },
    {
      "epoch": 13.607745090882545,
      "grad_norm": 4.024522304534912,
      "learning_rate": 3.8660212424264545e-05,
      "loss": 0.6673,
      "step": 1491300
    },
    {
      "epoch": 13.60865756624571,
      "grad_norm": 4.393316268920898,
      "learning_rate": 3.8659452028128575e-05,
      "loss": 0.6415,
      "step": 1491400
    },
    {
      "epoch": 13.609570041608876,
      "grad_norm": 3.6216623783111572,
      "learning_rate": 3.8658691631992605e-05,
      "loss": 0.6596,
      "step": 1491500
    },
    {
      "epoch": 13.610482516972041,
      "grad_norm": 3.7462379932403564,
      "learning_rate": 3.865793123585663e-05,
      "loss": 0.7183,
      "step": 1491600
    },
    {
      "epoch": 13.611394992335207,
      "grad_norm": 3.7593162059783936,
      "learning_rate": 3.8657170839720665e-05,
      "loss": 0.7027,
      "step": 1491700
    },
    {
      "epoch": 13.612307467698372,
      "grad_norm": 3.706986904144287,
      "learning_rate": 3.865641044358469e-05,
      "loss": 0.6928,
      "step": 1491800
    },
    {
      "epoch": 13.613219943061537,
      "grad_norm": 4.471996784210205,
      "learning_rate": 3.865565004744872e-05,
      "loss": 0.6441,
      "step": 1491900
    },
    {
      "epoch": 13.614132418424703,
      "grad_norm": 4.139620780944824,
      "learning_rate": 3.865488965131275e-05,
      "loss": 0.6704,
      "step": 1492000
    },
    {
      "epoch": 13.615044893787868,
      "grad_norm": 4.210537910461426,
      "learning_rate": 3.865412925517678e-05,
      "loss": 0.6985,
      "step": 1492100
    },
    {
      "epoch": 13.615957369151033,
      "grad_norm": 3.584876537322998,
      "learning_rate": 3.865336885904081e-05,
      "loss": 0.6444,
      "step": 1492200
    },
    {
      "epoch": 13.616869844514198,
      "grad_norm": 3.917794942855835,
      "learning_rate": 3.865260846290484e-05,
      "loss": 0.6874,
      "step": 1492300
    },
    {
      "epoch": 13.617782319877364,
      "grad_norm": 2.9049408435821533,
      "learning_rate": 3.865184806676886e-05,
      "loss": 0.6679,
      "step": 1492400
    },
    {
      "epoch": 13.618694795240529,
      "grad_norm": 3.680018186569214,
      "learning_rate": 3.86510876706329e-05,
      "loss": 0.6892,
      "step": 1492500
    },
    {
      "epoch": 13.619607270603694,
      "grad_norm": 4.251457214355469,
      "learning_rate": 3.865032727449692e-05,
      "loss": 0.6916,
      "step": 1492600
    },
    {
      "epoch": 13.62051974596686,
      "grad_norm": 4.153677940368652,
      "learning_rate": 3.864956687836095e-05,
      "loss": 0.6386,
      "step": 1492700
    },
    {
      "epoch": 13.621432221330025,
      "grad_norm": 2.9976823329925537,
      "learning_rate": 3.864880648222498e-05,
      "loss": 0.6863,
      "step": 1492800
    },
    {
      "epoch": 13.622344696693188,
      "grad_norm": 4.135072231292725,
      "learning_rate": 3.864804608608901e-05,
      "loss": 0.7133,
      "step": 1492900
    },
    {
      "epoch": 13.623257172056354,
      "grad_norm": 3.8875975608825684,
      "learning_rate": 3.8647285689953036e-05,
      "loss": 0.6874,
      "step": 1493000
    },
    {
      "epoch": 13.624169647419519,
      "grad_norm": 4.152096271514893,
      "learning_rate": 3.864652529381707e-05,
      "loss": 0.6931,
      "step": 1493100
    },
    {
      "epoch": 13.625082122782684,
      "grad_norm": 4.126405715942383,
      "learning_rate": 3.8645764897681096e-05,
      "loss": 0.6773,
      "step": 1493200
    },
    {
      "epoch": 13.62599459814585,
      "grad_norm": 4.888707160949707,
      "learning_rate": 3.8645004501545126e-05,
      "loss": 0.6638,
      "step": 1493300
    },
    {
      "epoch": 13.626907073509015,
      "grad_norm": 3.3974838256835938,
      "learning_rate": 3.8644244105409156e-05,
      "loss": 0.6482,
      "step": 1493400
    },
    {
      "epoch": 13.62781954887218,
      "grad_norm": 3.8911149501800537,
      "learning_rate": 3.8643483709273186e-05,
      "loss": 0.7097,
      "step": 1493500
    },
    {
      "epoch": 13.628732024235346,
      "grad_norm": 3.5937306880950928,
      "learning_rate": 3.8642723313137216e-05,
      "loss": 0.6631,
      "step": 1493600
    },
    {
      "epoch": 13.62964449959851,
      "grad_norm": 4.4876861572265625,
      "learning_rate": 3.8641962917001246e-05,
      "loss": 0.675,
      "step": 1493700
    },
    {
      "epoch": 13.630556974961676,
      "grad_norm": 3.4796226024627686,
      "learning_rate": 3.864120252086527e-05,
      "loss": 0.7002,
      "step": 1493800
    },
    {
      "epoch": 13.631469450324841,
      "grad_norm": 4.254716396331787,
      "learning_rate": 3.8640442124729307e-05,
      "loss": 0.6822,
      "step": 1493900
    },
    {
      "epoch": 13.632381925688007,
      "grad_norm": 4.476828575134277,
      "learning_rate": 3.863968172859333e-05,
      "loss": 0.655,
      "step": 1494000
    },
    {
      "epoch": 13.633294401051172,
      "grad_norm": 4.541525840759277,
      "learning_rate": 3.863892133245735e-05,
      "loss": 0.6994,
      "step": 1494100
    },
    {
      "epoch": 13.634206876414337,
      "grad_norm": 4.03428316116333,
      "learning_rate": 3.863816093632139e-05,
      "loss": 0.6239,
      "step": 1494200
    },
    {
      "epoch": 13.635119351777503,
      "grad_norm": 3.50162672996521,
      "learning_rate": 3.863740054018541e-05,
      "loss": 0.6407,
      "step": 1494300
    },
    {
      "epoch": 13.636031827140668,
      "grad_norm": 3.9710466861724854,
      "learning_rate": 3.863664014404944e-05,
      "loss": 0.6874,
      "step": 1494400
    },
    {
      "epoch": 13.636944302503831,
      "grad_norm": 4.559458255767822,
      "learning_rate": 3.863587974791347e-05,
      "loss": 0.6976,
      "step": 1494500
    },
    {
      "epoch": 13.637856777866997,
      "grad_norm": 4.3069915771484375,
      "learning_rate": 3.8635119351777503e-05,
      "loss": 0.6759,
      "step": 1494600
    },
    {
      "epoch": 13.638769253230162,
      "grad_norm": 4.3235931396484375,
      "learning_rate": 3.8634358955641534e-05,
      "loss": 0.7104,
      "step": 1494700
    },
    {
      "epoch": 13.639681728593327,
      "grad_norm": 3.406996488571167,
      "learning_rate": 3.8633598559505564e-05,
      "loss": 0.6812,
      "step": 1494800
    },
    {
      "epoch": 13.640594203956493,
      "grad_norm": 4.179280757904053,
      "learning_rate": 3.863283816336959e-05,
      "loss": 0.6906,
      "step": 1494900
    },
    {
      "epoch": 13.641506679319658,
      "grad_norm": 3.497201442718506,
      "learning_rate": 3.8632077767233624e-05,
      "loss": 0.6968,
      "step": 1495000
    },
    {
      "epoch": 13.642419154682823,
      "grad_norm": 3.382800579071045,
      "learning_rate": 3.863131737109765e-05,
      "loss": 0.6902,
      "step": 1495100
    },
    {
      "epoch": 13.643331630045989,
      "grad_norm": 2.886054754257202,
      "learning_rate": 3.863055697496168e-05,
      "loss": 0.6665,
      "step": 1495200
    },
    {
      "epoch": 13.644244105409154,
      "grad_norm": 3.651158094406128,
      "learning_rate": 3.862979657882571e-05,
      "loss": 0.6584,
      "step": 1495300
    },
    {
      "epoch": 13.64515658077232,
      "grad_norm": 2.7487640380859375,
      "learning_rate": 3.862903618268974e-05,
      "loss": 0.6922,
      "step": 1495400
    },
    {
      "epoch": 13.646069056135484,
      "grad_norm": 3.927298069000244,
      "learning_rate": 3.862827578655376e-05,
      "loss": 0.7015,
      "step": 1495500
    },
    {
      "epoch": 13.64698153149865,
      "grad_norm": 3.3839199542999268,
      "learning_rate": 3.86275153904178e-05,
      "loss": 0.6718,
      "step": 1495600
    },
    {
      "epoch": 13.647894006861815,
      "grad_norm": 3.515137195587158,
      "learning_rate": 3.862675499428182e-05,
      "loss": 0.6713,
      "step": 1495700
    },
    {
      "epoch": 13.64880648222498,
      "grad_norm": 3.493928909301758,
      "learning_rate": 3.862599459814585e-05,
      "loss": 0.6619,
      "step": 1495800
    },
    {
      "epoch": 13.649718957588146,
      "grad_norm": 3.187774419784546,
      "learning_rate": 3.862523420200988e-05,
      "loss": 0.6931,
      "step": 1495900
    },
    {
      "epoch": 13.650631432951311,
      "grad_norm": 4.060171604156494,
      "learning_rate": 3.862447380587391e-05,
      "loss": 0.6534,
      "step": 1496000
    },
    {
      "epoch": 13.651543908314476,
      "grad_norm": 2.98872709274292,
      "learning_rate": 3.862371340973794e-05,
      "loss": 0.6627,
      "step": 1496100
    },
    {
      "epoch": 13.652456383677642,
      "grad_norm": 3.844290018081665,
      "learning_rate": 3.862295301360197e-05,
      "loss": 0.6962,
      "step": 1496200
    },
    {
      "epoch": 13.653368859040805,
      "grad_norm": 3.576263666152954,
      "learning_rate": 3.8622192617465994e-05,
      "loss": 0.6854,
      "step": 1496300
    },
    {
      "epoch": 13.65428133440397,
      "grad_norm": 4.371119499206543,
      "learning_rate": 3.862143222133003e-05,
      "loss": 0.6632,
      "step": 1496400
    },
    {
      "epoch": 13.655193809767136,
      "grad_norm": 4.2324371337890625,
      "learning_rate": 3.8620671825194054e-05,
      "loss": 0.6646,
      "step": 1496500
    },
    {
      "epoch": 13.656106285130301,
      "grad_norm": 4.018077373504639,
      "learning_rate": 3.8619911429058084e-05,
      "loss": 0.7019,
      "step": 1496600
    },
    {
      "epoch": 13.657018760493466,
      "grad_norm": 3.9759860038757324,
      "learning_rate": 3.8619151032922115e-05,
      "loss": 0.6881,
      "step": 1496700
    },
    {
      "epoch": 13.657931235856632,
      "grad_norm": 3.7779481410980225,
      "learning_rate": 3.861839063678614e-05,
      "loss": 0.6603,
      "step": 1496800
    },
    {
      "epoch": 13.658843711219797,
      "grad_norm": 3.4791104793548584,
      "learning_rate": 3.861763024065017e-05,
      "loss": 0.7156,
      "step": 1496900
    },
    {
      "epoch": 13.659756186582962,
      "grad_norm": 4.68364143371582,
      "learning_rate": 3.86168698445142e-05,
      "loss": 0.6836,
      "step": 1497000
    },
    {
      "epoch": 13.660668661946128,
      "grad_norm": 3.26896595954895,
      "learning_rate": 3.861610944837823e-05,
      "loss": 0.6629,
      "step": 1497100
    },
    {
      "epoch": 13.661581137309293,
      "grad_norm": 2.778719186782837,
      "learning_rate": 3.861534905224226e-05,
      "loss": 0.6553,
      "step": 1497200
    },
    {
      "epoch": 13.662493612672458,
      "grad_norm": 3.4286880493164062,
      "learning_rate": 3.861458865610629e-05,
      "loss": 0.6852,
      "step": 1497300
    },
    {
      "epoch": 13.663406088035623,
      "grad_norm": 4.103122711181641,
      "learning_rate": 3.861382825997031e-05,
      "loss": 0.6855,
      "step": 1497400
    },
    {
      "epoch": 13.664318563398789,
      "grad_norm": 3.4919018745422363,
      "learning_rate": 3.861306786383435e-05,
      "loss": 0.6931,
      "step": 1497500
    },
    {
      "epoch": 13.665231038761954,
      "grad_norm": 3.592482566833496,
      "learning_rate": 3.861230746769837e-05,
      "loss": 0.6846,
      "step": 1497600
    },
    {
      "epoch": 13.66614351412512,
      "grad_norm": 3.861562490463257,
      "learning_rate": 3.86115470715624e-05,
      "loss": 0.6842,
      "step": 1497700
    },
    {
      "epoch": 13.667055989488285,
      "grad_norm": 4.202767848968506,
      "learning_rate": 3.861078667542643e-05,
      "loss": 0.6653,
      "step": 1497800
    },
    {
      "epoch": 13.667968464851448,
      "grad_norm": 4.16202449798584,
      "learning_rate": 3.861002627929046e-05,
      "loss": 0.6762,
      "step": 1497900
    },
    {
      "epoch": 13.668880940214613,
      "grad_norm": 4.0566086769104,
      "learning_rate": 3.8609265883154485e-05,
      "loss": 0.6474,
      "step": 1498000
    },
    {
      "epoch": 13.669793415577779,
      "grad_norm": 4.086228370666504,
      "learning_rate": 3.860850548701852e-05,
      "loss": 0.7177,
      "step": 1498100
    },
    {
      "epoch": 13.670705890940944,
      "grad_norm": 3.3847827911376953,
      "learning_rate": 3.8607745090882545e-05,
      "loss": 0.712,
      "step": 1498200
    },
    {
      "epoch": 13.67161836630411,
      "grad_norm": 4.959172248840332,
      "learning_rate": 3.8606984694746575e-05,
      "loss": 0.643,
      "step": 1498300
    },
    {
      "epoch": 13.672530841667275,
      "grad_norm": 3.907428503036499,
      "learning_rate": 3.8606224298610605e-05,
      "loss": 0.7062,
      "step": 1498400
    },
    {
      "epoch": 13.67344331703044,
      "grad_norm": 3.9516663551330566,
      "learning_rate": 3.8605463902474635e-05,
      "loss": 0.674,
      "step": 1498500
    },
    {
      "epoch": 13.674355792393605,
      "grad_norm": 4.145061016082764,
      "learning_rate": 3.8604703506338665e-05,
      "loss": 0.658,
      "step": 1498600
    },
    {
      "epoch": 13.67526826775677,
      "grad_norm": 3.571805238723755,
      "learning_rate": 3.8603943110202696e-05,
      "loss": 0.6653,
      "step": 1498700
    },
    {
      "epoch": 13.676180743119936,
      "grad_norm": 4.901507377624512,
      "learning_rate": 3.860318271406672e-05,
      "loss": 0.6598,
      "step": 1498800
    },
    {
      "epoch": 13.677093218483101,
      "grad_norm": 3.6737282276153564,
      "learning_rate": 3.8602422317930756e-05,
      "loss": 0.6714,
      "step": 1498900
    },
    {
      "epoch": 13.678005693846266,
      "grad_norm": 2.6563591957092285,
      "learning_rate": 3.860166192179478e-05,
      "loss": 0.6485,
      "step": 1499000
    },
    {
      "epoch": 13.678918169209432,
      "grad_norm": 2.7893500328063965,
      "learning_rate": 3.860090152565881e-05,
      "loss": 0.6656,
      "step": 1499100
    },
    {
      "epoch": 13.679830644572597,
      "grad_norm": 3.754040241241455,
      "learning_rate": 3.860014112952284e-05,
      "loss": 0.6556,
      "step": 1499200
    },
    {
      "epoch": 13.680743119935762,
      "grad_norm": 3.715515613555908,
      "learning_rate": 3.859938073338687e-05,
      "loss": 0.6487,
      "step": 1499300
    },
    {
      "epoch": 13.681655595298928,
      "grad_norm": 3.8703243732452393,
      "learning_rate": 3.85986203372509e-05,
      "loss": 0.6755,
      "step": 1499400
    },
    {
      "epoch": 13.682568070662093,
      "grad_norm": 3.3632872104644775,
      "learning_rate": 3.859785994111493e-05,
      "loss": 0.6666,
      "step": 1499500
    },
    {
      "epoch": 13.683480546025258,
      "grad_norm": 4.28466272354126,
      "learning_rate": 3.859709954497895e-05,
      "loss": 0.6515,
      "step": 1499600
    },
    {
      "epoch": 13.684393021388422,
      "grad_norm": 3.7838692665100098,
      "learning_rate": 3.859633914884298e-05,
      "loss": 0.6633,
      "step": 1499700
    },
    {
      "epoch": 13.685305496751587,
      "grad_norm": 3.816692590713501,
      "learning_rate": 3.859557875270701e-05,
      "loss": 0.7006,
      "step": 1499800
    },
    {
      "epoch": 13.686217972114752,
      "grad_norm": 2.949402093887329,
      "learning_rate": 3.8594818356571036e-05,
      "loss": 0.6746,
      "step": 1499900
    },
    {
      "epoch": 13.687130447477918,
      "grad_norm": 3.946333646774292,
      "learning_rate": 3.859405796043507e-05,
      "loss": 0.6457,
      "step": 1500000
    },
    {
      "epoch": 13.688042922841083,
      "grad_norm": 3.59490966796875,
      "learning_rate": 3.8593297564299096e-05,
      "loss": 0.6553,
      "step": 1500100
    },
    {
      "epoch": 13.688955398204248,
      "grad_norm": 4.821290969848633,
      "learning_rate": 3.8592537168163126e-05,
      "loss": 0.6994,
      "step": 1500200
    },
    {
      "epoch": 13.689867873567414,
      "grad_norm": 4.2516374588012695,
      "learning_rate": 3.8591776772027156e-05,
      "loss": 0.6809,
      "step": 1500300
    },
    {
      "epoch": 13.690780348930579,
      "grad_norm": 4.518625259399414,
      "learning_rate": 3.8591016375891186e-05,
      "loss": 0.7003,
      "step": 1500400
    },
    {
      "epoch": 13.691692824293744,
      "grad_norm": 3.3471009731292725,
      "learning_rate": 3.8590255979755216e-05,
      "loss": 0.6679,
      "step": 1500500
    },
    {
      "epoch": 13.69260529965691,
      "grad_norm": 4.409351348876953,
      "learning_rate": 3.8589495583619246e-05,
      "loss": 0.6502,
      "step": 1500600
    },
    {
      "epoch": 13.693517775020075,
      "grad_norm": 4.419707775115967,
      "learning_rate": 3.858873518748327e-05,
      "loss": 0.6669,
      "step": 1500700
    },
    {
      "epoch": 13.69443025038324,
      "grad_norm": 3.719730854034424,
      "learning_rate": 3.858797479134731e-05,
      "loss": 0.6558,
      "step": 1500800
    },
    {
      "epoch": 13.695342725746405,
      "grad_norm": 3.312760829925537,
      "learning_rate": 3.858721439521133e-05,
      "loss": 0.6566,
      "step": 1500900
    },
    {
      "epoch": 13.69625520110957,
      "grad_norm": 3.6451213359832764,
      "learning_rate": 3.858645399907536e-05,
      "loss": 0.6653,
      "step": 1501000
    },
    {
      "epoch": 13.697167676472736,
      "grad_norm": 4.328754425048828,
      "learning_rate": 3.858569360293939e-05,
      "loss": 0.6432,
      "step": 1501100
    },
    {
      "epoch": 13.698080151835901,
      "grad_norm": 3.914330244064331,
      "learning_rate": 3.858493320680342e-05,
      "loss": 0.6457,
      "step": 1501200
    },
    {
      "epoch": 13.698992627199065,
      "grad_norm": 4.0065484046936035,
      "learning_rate": 3.8584172810667443e-05,
      "loss": 0.6573,
      "step": 1501300
    },
    {
      "epoch": 13.69990510256223,
      "grad_norm": 4.967154026031494,
      "learning_rate": 3.858341241453148e-05,
      "loss": 0.6879,
      "step": 1501400
    },
    {
      "epoch": 13.700817577925395,
      "grad_norm": 4.575387001037598,
      "learning_rate": 3.8582652018395504e-05,
      "loss": 0.6807,
      "step": 1501500
    },
    {
      "epoch": 13.70173005328856,
      "grad_norm": 4.193799018859863,
      "learning_rate": 3.8581891622259534e-05,
      "loss": 0.6603,
      "step": 1501600
    },
    {
      "epoch": 13.702642528651726,
      "grad_norm": 3.3857433795928955,
      "learning_rate": 3.8581131226123564e-05,
      "loss": 0.6651,
      "step": 1501700
    },
    {
      "epoch": 13.703555004014891,
      "grad_norm": 4.562713623046875,
      "learning_rate": 3.8580370829987594e-05,
      "loss": 0.6893,
      "step": 1501800
    },
    {
      "epoch": 13.704467479378057,
      "grad_norm": 3.4023184776306152,
      "learning_rate": 3.8579610433851624e-05,
      "loss": 0.6308,
      "step": 1501900
    },
    {
      "epoch": 13.705379954741222,
      "grad_norm": 4.2371826171875,
      "learning_rate": 3.8578850037715654e-05,
      "loss": 0.6941,
      "step": 1502000
    },
    {
      "epoch": 13.706292430104387,
      "grad_norm": 2.867234706878662,
      "learning_rate": 3.857808964157968e-05,
      "loss": 0.6391,
      "step": 1502100
    },
    {
      "epoch": 13.707204905467552,
      "grad_norm": 4.156838893890381,
      "learning_rate": 3.8577329245443714e-05,
      "loss": 0.6528,
      "step": 1502200
    },
    {
      "epoch": 13.708117380830718,
      "grad_norm": 4.380527496337891,
      "learning_rate": 3.857656884930774e-05,
      "loss": 0.6395,
      "step": 1502300
    },
    {
      "epoch": 13.709029856193883,
      "grad_norm": 5.483708381652832,
      "learning_rate": 3.857580845317177e-05,
      "loss": 0.6325,
      "step": 1502400
    },
    {
      "epoch": 13.709942331557048,
      "grad_norm": 3.176703691482544,
      "learning_rate": 3.85750480570358e-05,
      "loss": 0.6208,
      "step": 1502500
    },
    {
      "epoch": 13.710854806920214,
      "grad_norm": 3.96642804145813,
      "learning_rate": 3.857428766089982e-05,
      "loss": 0.6718,
      "step": 1502600
    },
    {
      "epoch": 13.711767282283379,
      "grad_norm": 4.296114444732666,
      "learning_rate": 3.857352726476385e-05,
      "loss": 0.6457,
      "step": 1502700
    },
    {
      "epoch": 13.712679757646544,
      "grad_norm": 3.838963747024536,
      "learning_rate": 3.857276686862788e-05,
      "loss": 0.6996,
      "step": 1502800
    },
    {
      "epoch": 13.71359223300971,
      "grad_norm": 4.698339939117432,
      "learning_rate": 3.857200647249191e-05,
      "loss": 0.6551,
      "step": 1502900
    },
    {
      "epoch": 13.714504708372875,
      "grad_norm": 3.305189609527588,
      "learning_rate": 3.857124607635594e-05,
      "loss": 0.6867,
      "step": 1503000
    },
    {
      "epoch": 13.715417183736038,
      "grad_norm": 3.179319381713867,
      "learning_rate": 3.857048568021997e-05,
      "loss": 0.6751,
      "step": 1503100
    },
    {
      "epoch": 13.716329659099204,
      "grad_norm": 3.314260721206665,
      "learning_rate": 3.8569725284083994e-05,
      "loss": 0.6459,
      "step": 1503200
    },
    {
      "epoch": 13.717242134462369,
      "grad_norm": 3.633457899093628,
      "learning_rate": 3.856896488794803e-05,
      "loss": 0.6766,
      "step": 1503300
    },
    {
      "epoch": 13.718154609825534,
      "grad_norm": 3.2287847995758057,
      "learning_rate": 3.8568204491812054e-05,
      "loss": 0.6735,
      "step": 1503400
    },
    {
      "epoch": 13.7190670851887,
      "grad_norm": 3.0218403339385986,
      "learning_rate": 3.8567444095676085e-05,
      "loss": 0.6764,
      "step": 1503500
    },
    {
      "epoch": 13.719979560551865,
      "grad_norm": 4.261920928955078,
      "learning_rate": 3.8566683699540115e-05,
      "loss": 0.678,
      "step": 1503600
    },
    {
      "epoch": 13.72089203591503,
      "grad_norm": 3.6616251468658447,
      "learning_rate": 3.8565923303404145e-05,
      "loss": 0.6908,
      "step": 1503700
    },
    {
      "epoch": 13.721804511278195,
      "grad_norm": 3.1656031608581543,
      "learning_rate": 3.856516290726817e-05,
      "loss": 0.6579,
      "step": 1503800
    },
    {
      "epoch": 13.72271698664136,
      "grad_norm": 4.607407093048096,
      "learning_rate": 3.8564402511132205e-05,
      "loss": 0.6841,
      "step": 1503900
    },
    {
      "epoch": 13.723629462004526,
      "grad_norm": 3.621242046356201,
      "learning_rate": 3.856364211499623e-05,
      "loss": 0.6587,
      "step": 1504000
    },
    {
      "epoch": 13.724541937367691,
      "grad_norm": 4.02613639831543,
      "learning_rate": 3.856288171886026e-05,
      "loss": 0.6704,
      "step": 1504100
    },
    {
      "epoch": 13.725454412730857,
      "grad_norm": 4.463131904602051,
      "learning_rate": 3.856212132272429e-05,
      "loss": 0.6703,
      "step": 1504200
    },
    {
      "epoch": 13.726366888094022,
      "grad_norm": 4.322762489318848,
      "learning_rate": 3.856136092658832e-05,
      "loss": 0.7046,
      "step": 1504300
    },
    {
      "epoch": 13.727279363457187,
      "grad_norm": 4.3995747566223145,
      "learning_rate": 3.856060053045235e-05,
      "loss": 0.6533,
      "step": 1504400
    },
    {
      "epoch": 13.728191838820353,
      "grad_norm": 5.189408302307129,
      "learning_rate": 3.855984013431638e-05,
      "loss": 0.672,
      "step": 1504500
    },
    {
      "epoch": 13.729104314183518,
      "grad_norm": 3.2193522453308105,
      "learning_rate": 3.85590797381804e-05,
      "loss": 0.6377,
      "step": 1504600
    },
    {
      "epoch": 13.730016789546681,
      "grad_norm": 3.871652841567993,
      "learning_rate": 3.855831934204444e-05,
      "loss": 0.6988,
      "step": 1504700
    },
    {
      "epoch": 13.730929264909847,
      "grad_norm": 3.9248757362365723,
      "learning_rate": 3.855755894590846e-05,
      "loss": 0.6829,
      "step": 1504800
    },
    {
      "epoch": 13.731841740273012,
      "grad_norm": 4.509891033172607,
      "learning_rate": 3.855679854977249e-05,
      "loss": 0.6774,
      "step": 1504900
    },
    {
      "epoch": 13.732754215636177,
      "grad_norm": 2.987870693206787,
      "learning_rate": 3.855603815363652e-05,
      "loss": 0.6499,
      "step": 1505000
    },
    {
      "epoch": 13.733666690999343,
      "grad_norm": 3.7820589542388916,
      "learning_rate": 3.855527775750055e-05,
      "loss": 0.6418,
      "step": 1505100
    },
    {
      "epoch": 13.734579166362508,
      "grad_norm": 3.7668917179107666,
      "learning_rate": 3.8554517361364575e-05,
      "loss": 0.6863,
      "step": 1505200
    },
    {
      "epoch": 13.735491641725673,
      "grad_norm": 4.99118185043335,
      "learning_rate": 3.8553756965228605e-05,
      "loss": 0.6727,
      "step": 1505300
    },
    {
      "epoch": 13.736404117088838,
      "grad_norm": 3.8667831420898438,
      "learning_rate": 3.8552996569092636e-05,
      "loss": 0.6269,
      "step": 1505400
    },
    {
      "epoch": 13.737316592452004,
      "grad_norm": 3.8754825592041016,
      "learning_rate": 3.8552236172956666e-05,
      "loss": 0.6873,
      "step": 1505500
    },
    {
      "epoch": 13.738229067815169,
      "grad_norm": 3.4109933376312256,
      "learning_rate": 3.8551475776820696e-05,
      "loss": 0.6584,
      "step": 1505600
    },
    {
      "epoch": 13.739141543178334,
      "grad_norm": 3.4358668327331543,
      "learning_rate": 3.855071538068472e-05,
      "loss": 0.6704,
      "step": 1505700
    },
    {
      "epoch": 13.7400540185415,
      "grad_norm": 3.1556897163391113,
      "learning_rate": 3.8549954984548756e-05,
      "loss": 0.6864,
      "step": 1505800
    },
    {
      "epoch": 13.740966493904665,
      "grad_norm": 2.4391746520996094,
      "learning_rate": 3.854919458841278e-05,
      "loss": 0.6822,
      "step": 1505900
    },
    {
      "epoch": 13.74187896926783,
      "grad_norm": 5.036669731140137,
      "learning_rate": 3.854843419227681e-05,
      "loss": 0.6692,
      "step": 1506000
    },
    {
      "epoch": 13.742791444630996,
      "grad_norm": 2.9358839988708496,
      "learning_rate": 3.854767379614084e-05,
      "loss": 0.7025,
      "step": 1506100
    },
    {
      "epoch": 13.74370391999416,
      "grad_norm": 3.9877853393554688,
      "learning_rate": 3.854691340000487e-05,
      "loss": 0.6662,
      "step": 1506200
    },
    {
      "epoch": 13.744616395357326,
      "grad_norm": 4.010438442230225,
      "learning_rate": 3.854615300386889e-05,
      "loss": 0.6557,
      "step": 1506300
    },
    {
      "epoch": 13.745528870720491,
      "grad_norm": 2.283723831176758,
      "learning_rate": 3.854539260773293e-05,
      "loss": 0.6513,
      "step": 1506400
    },
    {
      "epoch": 13.746441346083655,
      "grad_norm": 3.998253107070923,
      "learning_rate": 3.854463221159695e-05,
      "loss": 0.6939,
      "step": 1506500
    },
    {
      "epoch": 13.74735382144682,
      "grad_norm": 4.653169631958008,
      "learning_rate": 3.854387181546098e-05,
      "loss": 0.6731,
      "step": 1506600
    },
    {
      "epoch": 13.748266296809986,
      "grad_norm": 3.6510961055755615,
      "learning_rate": 3.854311141932501e-05,
      "loss": 0.6462,
      "step": 1506700
    },
    {
      "epoch": 13.74917877217315,
      "grad_norm": 4.728334426879883,
      "learning_rate": 3.854235102318904e-05,
      "loss": 0.6677,
      "step": 1506800
    },
    {
      "epoch": 13.750091247536316,
      "grad_norm": 4.452445030212402,
      "learning_rate": 3.854159062705307e-05,
      "loss": 0.6978,
      "step": 1506900
    },
    {
      "epoch": 13.751003722899481,
      "grad_norm": 3.6020467281341553,
      "learning_rate": 3.85408302309171e-05,
      "loss": 0.6731,
      "step": 1507000
    },
    {
      "epoch": 13.751916198262647,
      "grad_norm": 3.734116554260254,
      "learning_rate": 3.8540069834781126e-05,
      "loss": 0.6634,
      "step": 1507100
    },
    {
      "epoch": 13.752828673625812,
      "grad_norm": 3.864246368408203,
      "learning_rate": 3.853930943864516e-05,
      "loss": 0.6924,
      "step": 1507200
    },
    {
      "epoch": 13.753741148988977,
      "grad_norm": 3.676678419113159,
      "learning_rate": 3.8538549042509186e-05,
      "loss": 0.6717,
      "step": 1507300
    },
    {
      "epoch": 13.754653624352143,
      "grad_norm": 4.496355056762695,
      "learning_rate": 3.8537788646373217e-05,
      "loss": 0.6643,
      "step": 1507400
    },
    {
      "epoch": 13.755566099715308,
      "grad_norm": 3.992140769958496,
      "learning_rate": 3.8537028250237247e-05,
      "loss": 0.6871,
      "step": 1507500
    },
    {
      "epoch": 13.756478575078473,
      "grad_norm": 4.290970325469971,
      "learning_rate": 3.853626785410128e-05,
      "loss": 0.688,
      "step": 1507600
    },
    {
      "epoch": 13.757391050441639,
      "grad_norm": 4.279191493988037,
      "learning_rate": 3.85355074579653e-05,
      "loss": 0.6835,
      "step": 1507700
    },
    {
      "epoch": 13.758303525804804,
      "grad_norm": 4.386991024017334,
      "learning_rate": 3.853474706182934e-05,
      "loss": 0.6629,
      "step": 1507800
    },
    {
      "epoch": 13.75921600116797,
      "grad_norm": 3.825592517852783,
      "learning_rate": 3.853398666569336e-05,
      "loss": 0.6437,
      "step": 1507900
    },
    {
      "epoch": 13.760128476531134,
      "grad_norm": 4.214083194732666,
      "learning_rate": 3.853322626955739e-05,
      "loss": 0.6478,
      "step": 1508000
    },
    {
      "epoch": 13.761040951894298,
      "grad_norm": 5.112099647521973,
      "learning_rate": 3.853246587342142e-05,
      "loss": 0.6427,
      "step": 1508100
    },
    {
      "epoch": 13.761953427257463,
      "grad_norm": 2.9628677368164062,
      "learning_rate": 3.8531705477285444e-05,
      "loss": 0.6392,
      "step": 1508200
    },
    {
      "epoch": 13.762865902620629,
      "grad_norm": 4.21759033203125,
      "learning_rate": 3.853094508114948e-05,
      "loss": 0.6693,
      "step": 1508300
    },
    {
      "epoch": 13.763778377983794,
      "grad_norm": 2.4596493244171143,
      "learning_rate": 3.8530184685013504e-05,
      "loss": 0.663,
      "step": 1508400
    },
    {
      "epoch": 13.76469085334696,
      "grad_norm": 3.92999529838562,
      "learning_rate": 3.8529424288877534e-05,
      "loss": 0.6955,
      "step": 1508500
    },
    {
      "epoch": 13.765603328710124,
      "grad_norm": 3.933993339538574,
      "learning_rate": 3.8528663892741564e-05,
      "loss": 0.7117,
      "step": 1508600
    },
    {
      "epoch": 13.76651580407329,
      "grad_norm": 3.3625729084014893,
      "learning_rate": 3.8527903496605594e-05,
      "loss": 0.6555,
      "step": 1508700
    },
    {
      "epoch": 13.767428279436455,
      "grad_norm": 4.203251838684082,
      "learning_rate": 3.852714310046962e-05,
      "loss": 0.6348,
      "step": 1508800
    },
    {
      "epoch": 13.76834075479962,
      "grad_norm": 4.637731552124023,
      "learning_rate": 3.8526382704333654e-05,
      "loss": 0.7104,
      "step": 1508900
    },
    {
      "epoch": 13.769253230162786,
      "grad_norm": 2.997131109237671,
      "learning_rate": 3.852562230819768e-05,
      "loss": 0.6925,
      "step": 1509000
    },
    {
      "epoch": 13.770165705525951,
      "grad_norm": 4.122103214263916,
      "learning_rate": 3.852486191206171e-05,
      "loss": 0.653,
      "step": 1509100
    },
    {
      "epoch": 13.771078180889116,
      "grad_norm": 3.3111507892608643,
      "learning_rate": 3.852410151592574e-05,
      "loss": 0.6682,
      "step": 1509200
    },
    {
      "epoch": 13.771990656252282,
      "grad_norm": 4.181600093841553,
      "learning_rate": 3.852334111978977e-05,
      "loss": 0.6881,
      "step": 1509300
    },
    {
      "epoch": 13.772903131615447,
      "grad_norm": 7.157342910766602,
      "learning_rate": 3.85225807236538e-05,
      "loss": 0.6779,
      "step": 1509400
    },
    {
      "epoch": 13.773815606978612,
      "grad_norm": 3.719740152359009,
      "learning_rate": 3.852182032751783e-05,
      "loss": 0.6691,
      "step": 1509500
    },
    {
      "epoch": 13.774728082341777,
      "grad_norm": 4.388327598571777,
      "learning_rate": 3.852105993138185e-05,
      "loss": 0.6609,
      "step": 1509600
    },
    {
      "epoch": 13.775640557704943,
      "grad_norm": 4.368791580200195,
      "learning_rate": 3.852029953524589e-05,
      "loss": 0.7001,
      "step": 1509700
    },
    {
      "epoch": 13.776553033068108,
      "grad_norm": 4.23728084564209,
      "learning_rate": 3.851953913910991e-05,
      "loss": 0.7154,
      "step": 1509800
    },
    {
      "epoch": 13.777465508431272,
      "grad_norm": 4.285192966461182,
      "learning_rate": 3.851877874297394e-05,
      "loss": 0.6854,
      "step": 1509900
    },
    {
      "epoch": 13.778377983794437,
      "grad_norm": 2.6520466804504395,
      "learning_rate": 3.851801834683797e-05,
      "loss": 0.6566,
      "step": 1510000
    },
    {
      "epoch": 13.779290459157602,
      "grad_norm": 4.931288242340088,
      "learning_rate": 3.8517257950702e-05,
      "loss": 0.672,
      "step": 1510100
    },
    {
      "epoch": 13.780202934520767,
      "grad_norm": 3.1159863471984863,
      "learning_rate": 3.8516497554566025e-05,
      "loss": 0.6272,
      "step": 1510200
    },
    {
      "epoch": 13.781115409883933,
      "grad_norm": 4.138411521911621,
      "learning_rate": 3.851573715843006e-05,
      "loss": 0.6675,
      "step": 1510300
    },
    {
      "epoch": 13.782027885247098,
      "grad_norm": 4.859066009521484,
      "learning_rate": 3.8514976762294085e-05,
      "loss": 0.6872,
      "step": 1510400
    },
    {
      "epoch": 13.782940360610263,
      "grad_norm": 4.457424640655518,
      "learning_rate": 3.8514216366158115e-05,
      "loss": 0.6822,
      "step": 1510500
    },
    {
      "epoch": 13.783852835973429,
      "grad_norm": 4.2807440757751465,
      "learning_rate": 3.8513455970022145e-05,
      "loss": 0.6843,
      "step": 1510600
    },
    {
      "epoch": 13.784765311336594,
      "grad_norm": 3.51582932472229,
      "learning_rate": 3.8512695573886175e-05,
      "loss": 0.6598,
      "step": 1510700
    },
    {
      "epoch": 13.78567778669976,
      "grad_norm": 3.4732208251953125,
      "learning_rate": 3.8511935177750205e-05,
      "loss": 0.6494,
      "step": 1510800
    },
    {
      "epoch": 13.786590262062925,
      "grad_norm": 3.9991250038146973,
      "learning_rate": 3.8511174781614235e-05,
      "loss": 0.6752,
      "step": 1510900
    },
    {
      "epoch": 13.78750273742609,
      "grad_norm": 3.4421024322509766,
      "learning_rate": 3.851041438547826e-05,
      "loss": 0.705,
      "step": 1511000
    },
    {
      "epoch": 13.788415212789255,
      "grad_norm": 4.172432899475098,
      "learning_rate": 3.850965398934229e-05,
      "loss": 0.678,
      "step": 1511100
    },
    {
      "epoch": 13.78932768815242,
      "grad_norm": 4.0376434326171875,
      "learning_rate": 3.850889359320632e-05,
      "loss": 0.6864,
      "step": 1511200
    },
    {
      "epoch": 13.790240163515586,
      "grad_norm": 3.5525619983673096,
      "learning_rate": 3.850813319707035e-05,
      "loss": 0.6704,
      "step": 1511300
    },
    {
      "epoch": 13.791152638878751,
      "grad_norm": 4.135115623474121,
      "learning_rate": 3.850737280093438e-05,
      "loss": 0.6312,
      "step": 1511400
    },
    {
      "epoch": 13.792065114241915,
      "grad_norm": 4.015759468078613,
      "learning_rate": 3.85066124047984e-05,
      "loss": 0.7,
      "step": 1511500
    },
    {
      "epoch": 13.79297758960508,
      "grad_norm": 4.616220951080322,
      "learning_rate": 3.850585200866243e-05,
      "loss": 0.6995,
      "step": 1511600
    },
    {
      "epoch": 13.793890064968245,
      "grad_norm": 3.1815860271453857,
      "learning_rate": 3.850509161252646e-05,
      "loss": 0.6821,
      "step": 1511700
    },
    {
      "epoch": 13.79480254033141,
      "grad_norm": 4.219924449920654,
      "learning_rate": 3.850433121639049e-05,
      "loss": 0.677,
      "step": 1511800
    },
    {
      "epoch": 13.795715015694576,
      "grad_norm": 3.70908260345459,
      "learning_rate": 3.850357082025452e-05,
      "loss": 0.7248,
      "step": 1511900
    },
    {
      "epoch": 13.796627491057741,
      "grad_norm": 4.353572368621826,
      "learning_rate": 3.850281042411855e-05,
      "loss": 0.6756,
      "step": 1512000
    },
    {
      "epoch": 13.797539966420906,
      "grad_norm": 3.2061705589294434,
      "learning_rate": 3.8502050027982575e-05,
      "loss": 0.6654,
      "step": 1512100
    },
    {
      "epoch": 13.798452441784072,
      "grad_norm": 3.939929962158203,
      "learning_rate": 3.850128963184661e-05,
      "loss": 0.6784,
      "step": 1512200
    },
    {
      "epoch": 13.799364917147237,
      "grad_norm": 4.253905773162842,
      "learning_rate": 3.8500529235710636e-05,
      "loss": 0.6645,
      "step": 1512300
    },
    {
      "epoch": 13.800277392510402,
      "grad_norm": 4.0637526512146,
      "learning_rate": 3.8499768839574666e-05,
      "loss": 0.6782,
      "step": 1512400
    },
    {
      "epoch": 13.801189867873568,
      "grad_norm": 4.408275127410889,
      "learning_rate": 3.8499008443438696e-05,
      "loss": 0.7225,
      "step": 1512500
    },
    {
      "epoch": 13.802102343236733,
      "grad_norm": 4.5729851722717285,
      "learning_rate": 3.8498248047302726e-05,
      "loss": 0.6676,
      "step": 1512600
    },
    {
      "epoch": 13.803014818599898,
      "grad_norm": 4.141866207122803,
      "learning_rate": 3.8497487651166756e-05,
      "loss": 0.6953,
      "step": 1512700
    },
    {
      "epoch": 13.803927293963064,
      "grad_norm": 4.251040458679199,
      "learning_rate": 3.8496727255030786e-05,
      "loss": 0.6574,
      "step": 1512800
    },
    {
      "epoch": 13.804839769326229,
      "grad_norm": 3.499093532562256,
      "learning_rate": 3.849596685889481e-05,
      "loss": 0.6598,
      "step": 1512900
    },
    {
      "epoch": 13.805752244689394,
      "grad_norm": 4.258528709411621,
      "learning_rate": 3.849520646275884e-05,
      "loss": 0.6712,
      "step": 1513000
    },
    {
      "epoch": 13.80666472005256,
      "grad_norm": 2.8267502784729004,
      "learning_rate": 3.849444606662287e-05,
      "loss": 0.6586,
      "step": 1513100
    },
    {
      "epoch": 13.807577195415725,
      "grad_norm": 4.118106842041016,
      "learning_rate": 3.84936856704869e-05,
      "loss": 0.6926,
      "step": 1513200
    },
    {
      "epoch": 13.808489670778888,
      "grad_norm": 3.4318861961364746,
      "learning_rate": 3.849292527435093e-05,
      "loss": 0.6584,
      "step": 1513300
    },
    {
      "epoch": 13.809402146142054,
      "grad_norm": 3.416445732116699,
      "learning_rate": 3.849216487821496e-05,
      "loss": 0.6801,
      "step": 1513400
    },
    {
      "epoch": 13.810314621505219,
      "grad_norm": 4.661077499389648,
      "learning_rate": 3.849140448207898e-05,
      "loss": 0.6649,
      "step": 1513500
    },
    {
      "epoch": 13.811227096868384,
      "grad_norm": 3.313509464263916,
      "learning_rate": 3.849064408594302e-05,
      "loss": 0.7076,
      "step": 1513600
    },
    {
      "epoch": 13.81213957223155,
      "grad_norm": 4.392554759979248,
      "learning_rate": 3.848988368980704e-05,
      "loss": 0.6626,
      "step": 1513700
    },
    {
      "epoch": 13.813052047594715,
      "grad_norm": 3.357539415359497,
      "learning_rate": 3.848912329367107e-05,
      "loss": 0.6839,
      "step": 1513800
    },
    {
      "epoch": 13.81396452295788,
      "grad_norm": 3.876242160797119,
      "learning_rate": 3.84883628975351e-05,
      "loss": 0.6605,
      "step": 1513900
    },
    {
      "epoch": 13.814876998321045,
      "grad_norm": 3.9719624519348145,
      "learning_rate": 3.8487602501399126e-05,
      "loss": 0.711,
      "step": 1514000
    },
    {
      "epoch": 13.81578947368421,
      "grad_norm": 3.7575581073760986,
      "learning_rate": 3.848684210526316e-05,
      "loss": 0.6627,
      "step": 1514100
    },
    {
      "epoch": 13.816701949047376,
      "grad_norm": 3.56673264503479,
      "learning_rate": 3.8486081709127187e-05,
      "loss": 0.672,
      "step": 1514200
    },
    {
      "epoch": 13.817614424410541,
      "grad_norm": 4.354305744171143,
      "learning_rate": 3.848532131299122e-05,
      "loss": 0.6709,
      "step": 1514300
    },
    {
      "epoch": 13.818526899773707,
      "grad_norm": 3.6692845821380615,
      "learning_rate": 3.848456091685525e-05,
      "loss": 0.6828,
      "step": 1514400
    },
    {
      "epoch": 13.819439375136872,
      "grad_norm": 3.1502034664154053,
      "learning_rate": 3.848380052071928e-05,
      "loss": 0.6714,
      "step": 1514500
    },
    {
      "epoch": 13.820351850500037,
      "grad_norm": 3.775447368621826,
      "learning_rate": 3.84830401245833e-05,
      "loss": 0.7043,
      "step": 1514600
    },
    {
      "epoch": 13.821264325863202,
      "grad_norm": 4.1478095054626465,
      "learning_rate": 3.848227972844734e-05,
      "loss": 0.7136,
      "step": 1514700
    },
    {
      "epoch": 13.822176801226368,
      "grad_norm": 2.6213903427124023,
      "learning_rate": 3.848151933231136e-05,
      "loss": 0.6776,
      "step": 1514800
    },
    {
      "epoch": 13.823089276589531,
      "grad_norm": 3.8227248191833496,
      "learning_rate": 3.848075893617539e-05,
      "loss": 0.6718,
      "step": 1514900
    },
    {
      "epoch": 13.824001751952697,
      "grad_norm": 4.298756122589111,
      "learning_rate": 3.847999854003942e-05,
      "loss": 0.6529,
      "step": 1515000
    },
    {
      "epoch": 13.824914227315862,
      "grad_norm": 3.639051914215088,
      "learning_rate": 3.847923814390345e-05,
      "loss": 0.6593,
      "step": 1515100
    },
    {
      "epoch": 13.825826702679027,
      "grad_norm": 3.897503137588501,
      "learning_rate": 3.847847774776748e-05,
      "loss": 0.674,
      "step": 1515200
    },
    {
      "epoch": 13.826739178042192,
      "grad_norm": 3.601839542388916,
      "learning_rate": 3.847771735163151e-05,
      "loss": 0.6619,
      "step": 1515300
    },
    {
      "epoch": 13.827651653405358,
      "grad_norm": 4.0755839347839355,
      "learning_rate": 3.8476956955495534e-05,
      "loss": 0.6661,
      "step": 1515400
    },
    {
      "epoch": 13.828564128768523,
      "grad_norm": 3.18819522857666,
      "learning_rate": 3.847619655935957e-05,
      "loss": 0.6888,
      "step": 1515500
    },
    {
      "epoch": 13.829476604131688,
      "grad_norm": 3.345427989959717,
      "learning_rate": 3.8475436163223594e-05,
      "loss": 0.6678,
      "step": 1515600
    },
    {
      "epoch": 13.830389079494854,
      "grad_norm": 4.594505786895752,
      "learning_rate": 3.8474675767087624e-05,
      "loss": 0.6762,
      "step": 1515700
    },
    {
      "epoch": 13.831301554858019,
      "grad_norm": 4.425806999206543,
      "learning_rate": 3.8473915370951654e-05,
      "loss": 0.6958,
      "step": 1515800
    },
    {
      "epoch": 13.832214030221184,
      "grad_norm": 3.8567936420440674,
      "learning_rate": 3.8473154974815684e-05,
      "loss": 0.6445,
      "step": 1515900
    },
    {
      "epoch": 13.83312650558435,
      "grad_norm": 3.5189521312713623,
      "learning_rate": 3.847239457867971e-05,
      "loss": 0.7055,
      "step": 1516000
    },
    {
      "epoch": 13.834038980947515,
      "grad_norm": 4.6586151123046875,
      "learning_rate": 3.8471634182543744e-05,
      "loss": 0.6944,
      "step": 1516100
    },
    {
      "epoch": 13.83495145631068,
      "grad_norm": 3.9149723052978516,
      "learning_rate": 3.847087378640777e-05,
      "loss": 0.7078,
      "step": 1516200
    },
    {
      "epoch": 13.835863931673845,
      "grad_norm": 3.7612452507019043,
      "learning_rate": 3.84701133902718e-05,
      "loss": 0.6763,
      "step": 1516300
    },
    {
      "epoch": 13.83677640703701,
      "grad_norm": 4.086620330810547,
      "learning_rate": 3.846935299413583e-05,
      "loss": 0.6746,
      "step": 1516400
    },
    {
      "epoch": 13.837688882400176,
      "grad_norm": 3.63183331489563,
      "learning_rate": 3.846859259799986e-05,
      "loss": 0.6713,
      "step": 1516500
    },
    {
      "epoch": 13.838601357763341,
      "grad_norm": 3.9000403881073,
      "learning_rate": 3.846783220186389e-05,
      "loss": 0.6654,
      "step": 1516600
    },
    {
      "epoch": 13.839513833126505,
      "grad_norm": 3.0414175987243652,
      "learning_rate": 3.846707180572791e-05,
      "loss": 0.6821,
      "step": 1516700
    },
    {
      "epoch": 13.84042630848967,
      "grad_norm": 4.015636920928955,
      "learning_rate": 3.846631140959194e-05,
      "loss": 0.6589,
      "step": 1516800
    },
    {
      "epoch": 13.841338783852835,
      "grad_norm": 4.293373107910156,
      "learning_rate": 3.846555101345597e-05,
      "loss": 0.6784,
      "step": 1516900
    },
    {
      "epoch": 13.842251259216,
      "grad_norm": 3.829181671142578,
      "learning_rate": 3.846479061732e-05,
      "loss": 0.6552,
      "step": 1517000
    },
    {
      "epoch": 13.843163734579166,
      "grad_norm": 2.1886258125305176,
      "learning_rate": 3.8464030221184025e-05,
      "loss": 0.6749,
      "step": 1517100
    },
    {
      "epoch": 13.844076209942331,
      "grad_norm": 3.4719953536987305,
      "learning_rate": 3.846326982504806e-05,
      "loss": 0.6648,
      "step": 1517200
    },
    {
      "epoch": 13.844988685305497,
      "grad_norm": 5.108778476715088,
      "learning_rate": 3.8462509428912085e-05,
      "loss": 0.6764,
      "step": 1517300
    },
    {
      "epoch": 13.845901160668662,
      "grad_norm": 3.263394594192505,
      "learning_rate": 3.8461749032776115e-05,
      "loss": 0.6706,
      "step": 1517400
    },
    {
      "epoch": 13.846813636031827,
      "grad_norm": 4.164523601531982,
      "learning_rate": 3.8460988636640145e-05,
      "loss": 0.7037,
      "step": 1517500
    },
    {
      "epoch": 13.847726111394993,
      "grad_norm": 4.136800765991211,
      "learning_rate": 3.8460228240504175e-05,
      "loss": 0.6738,
      "step": 1517600
    },
    {
      "epoch": 13.848638586758158,
      "grad_norm": 4.641208171844482,
      "learning_rate": 3.8459467844368205e-05,
      "loss": 0.6791,
      "step": 1517700
    },
    {
      "epoch": 13.849551062121323,
      "grad_norm": 3.4972329139709473,
      "learning_rate": 3.8458707448232235e-05,
      "loss": 0.6945,
      "step": 1517800
    },
    {
      "epoch": 13.850463537484488,
      "grad_norm": 3.5309689044952393,
      "learning_rate": 3.845794705209626e-05,
      "loss": 0.6574,
      "step": 1517900
    },
    {
      "epoch": 13.851376012847654,
      "grad_norm": 4.030910015106201,
      "learning_rate": 3.8457186655960295e-05,
      "loss": 0.6968,
      "step": 1518000
    },
    {
      "epoch": 13.852288488210819,
      "grad_norm": 4.560452461242676,
      "learning_rate": 3.845642625982432e-05,
      "loss": 0.6534,
      "step": 1518100
    },
    {
      "epoch": 13.853200963573983,
      "grad_norm": 3.942521810531616,
      "learning_rate": 3.845566586368835e-05,
      "loss": 0.6842,
      "step": 1518200
    },
    {
      "epoch": 13.854113438937148,
      "grad_norm": 3.8874142169952393,
      "learning_rate": 3.845490546755238e-05,
      "loss": 0.6841,
      "step": 1518300
    },
    {
      "epoch": 13.855025914300313,
      "grad_norm": 3.8152952194213867,
      "learning_rate": 3.845414507141641e-05,
      "loss": 0.6875,
      "step": 1518400
    },
    {
      "epoch": 13.855938389663478,
      "grad_norm": 4.1383562088012695,
      "learning_rate": 3.845338467528043e-05,
      "loss": 0.6701,
      "step": 1518500
    },
    {
      "epoch": 13.856850865026644,
      "grad_norm": 3.194917678833008,
      "learning_rate": 3.845262427914447e-05,
      "loss": 0.6723,
      "step": 1518600
    },
    {
      "epoch": 13.857763340389809,
      "grad_norm": 4.341972827911377,
      "learning_rate": 3.845186388300849e-05,
      "loss": 0.694,
      "step": 1518700
    },
    {
      "epoch": 13.858675815752974,
      "grad_norm": 3.74646258354187,
      "learning_rate": 3.845110348687252e-05,
      "loss": 0.687,
      "step": 1518800
    },
    {
      "epoch": 13.85958829111614,
      "grad_norm": 3.9995408058166504,
      "learning_rate": 3.845034309073655e-05,
      "loss": 0.6529,
      "step": 1518900
    },
    {
      "epoch": 13.860500766479305,
      "grad_norm": 4.8637261390686035,
      "learning_rate": 3.844958269460058e-05,
      "loss": 0.6903,
      "step": 1519000
    },
    {
      "epoch": 13.86141324184247,
      "grad_norm": 2.6529946327209473,
      "learning_rate": 3.844882229846461e-05,
      "loss": 0.6614,
      "step": 1519100
    },
    {
      "epoch": 13.862325717205636,
      "grad_norm": 4.131343841552734,
      "learning_rate": 3.844806190232864e-05,
      "loss": 0.6699,
      "step": 1519200
    },
    {
      "epoch": 13.8632381925688,
      "grad_norm": 3.5956127643585205,
      "learning_rate": 3.8447301506192666e-05,
      "loss": 0.6559,
      "step": 1519300
    },
    {
      "epoch": 13.864150667931966,
      "grad_norm": 4.133368492126465,
      "learning_rate": 3.84465411100567e-05,
      "loss": 0.6956,
      "step": 1519400
    },
    {
      "epoch": 13.865063143295131,
      "grad_norm": 4.105844497680664,
      "learning_rate": 3.8445780713920726e-05,
      "loss": 0.6521,
      "step": 1519500
    },
    {
      "epoch": 13.865975618658297,
      "grad_norm": 3.6547861099243164,
      "learning_rate": 3.844502031778475e-05,
      "loss": 0.6719,
      "step": 1519600
    },
    {
      "epoch": 13.866888094021462,
      "grad_norm": 3.8379087448120117,
      "learning_rate": 3.8444259921648786e-05,
      "loss": 0.7233,
      "step": 1519700
    },
    {
      "epoch": 13.867800569384627,
      "grad_norm": 3.6559250354766846,
      "learning_rate": 3.844349952551281e-05,
      "loss": 0.6716,
      "step": 1519800
    },
    {
      "epoch": 13.868713044747793,
      "grad_norm": 4.7564473152160645,
      "learning_rate": 3.844273912937684e-05,
      "loss": 0.6833,
      "step": 1519900
    },
    {
      "epoch": 13.869625520110956,
      "grad_norm": 3.1669633388519287,
      "learning_rate": 3.844197873324087e-05,
      "loss": 0.6703,
      "step": 1520000
    },
    {
      "epoch": 13.870537995474121,
      "grad_norm": 3.8669183254241943,
      "learning_rate": 3.84412183371049e-05,
      "loss": 0.6589,
      "step": 1520100
    },
    {
      "epoch": 13.871450470837287,
      "grad_norm": 5.349506378173828,
      "learning_rate": 3.844045794096893e-05,
      "loss": 0.6797,
      "step": 1520200
    },
    {
      "epoch": 13.872362946200452,
      "grad_norm": 4.0783305168151855,
      "learning_rate": 3.843969754483296e-05,
      "loss": 0.6811,
      "step": 1520300
    },
    {
      "epoch": 13.873275421563617,
      "grad_norm": 2.8626790046691895,
      "learning_rate": 3.843893714869698e-05,
      "loss": 0.6581,
      "step": 1520400
    },
    {
      "epoch": 13.874187896926783,
      "grad_norm": 4.046671390533447,
      "learning_rate": 3.843817675256102e-05,
      "loss": 0.6674,
      "step": 1520500
    },
    {
      "epoch": 13.875100372289948,
      "grad_norm": 3.421962261199951,
      "learning_rate": 3.843741635642504e-05,
      "loss": 0.6914,
      "step": 1520600
    },
    {
      "epoch": 13.876012847653113,
      "grad_norm": 3.582282066345215,
      "learning_rate": 3.843665596028907e-05,
      "loss": 0.6775,
      "step": 1520700
    },
    {
      "epoch": 13.876925323016279,
      "grad_norm": 3.4881999492645264,
      "learning_rate": 3.84358955641531e-05,
      "loss": 0.6735,
      "step": 1520800
    },
    {
      "epoch": 13.877837798379444,
      "grad_norm": 3.7767624855041504,
      "learning_rate": 3.843513516801713e-05,
      "loss": 0.6473,
      "step": 1520900
    },
    {
      "epoch": 13.87875027374261,
      "grad_norm": 5.127308368682861,
      "learning_rate": 3.8434374771881157e-05,
      "loss": 0.6543,
      "step": 1521000
    },
    {
      "epoch": 13.879662749105774,
      "grad_norm": 3.6930434703826904,
      "learning_rate": 3.8433614375745193e-05,
      "loss": 0.6659,
      "step": 1521100
    },
    {
      "epoch": 13.88057522446894,
      "grad_norm": 3.8613359928131104,
      "learning_rate": 3.843285397960922e-05,
      "loss": 0.6688,
      "step": 1521200
    },
    {
      "epoch": 13.881487699832105,
      "grad_norm": 3.791781187057495,
      "learning_rate": 3.843209358347325e-05,
      "loss": 0.6719,
      "step": 1521300
    },
    {
      "epoch": 13.88240017519527,
      "grad_norm": 4.2027764320373535,
      "learning_rate": 3.843133318733728e-05,
      "loss": 0.7009,
      "step": 1521400
    },
    {
      "epoch": 13.883312650558436,
      "grad_norm": 4.038691997528076,
      "learning_rate": 3.843057279120131e-05,
      "loss": 0.6418,
      "step": 1521500
    },
    {
      "epoch": 13.8842251259216,
      "grad_norm": 4.025810718536377,
      "learning_rate": 3.842981239506534e-05,
      "loss": 0.7131,
      "step": 1521600
    },
    {
      "epoch": 13.885137601284764,
      "grad_norm": 4.06949520111084,
      "learning_rate": 3.842905199892937e-05,
      "loss": 0.6871,
      "step": 1521700
    },
    {
      "epoch": 13.88605007664793,
      "grad_norm": 5.286827564239502,
      "learning_rate": 3.842829160279339e-05,
      "loss": 0.6451,
      "step": 1521800
    },
    {
      "epoch": 13.886962552011095,
      "grad_norm": 3.9446933269500732,
      "learning_rate": 3.842753120665743e-05,
      "loss": 0.7287,
      "step": 1521900
    },
    {
      "epoch": 13.88787502737426,
      "grad_norm": 4.2008256912231445,
      "learning_rate": 3.842677081052145e-05,
      "loss": 0.6803,
      "step": 1522000
    },
    {
      "epoch": 13.888787502737426,
      "grad_norm": 4.432429313659668,
      "learning_rate": 3.842601041438548e-05,
      "loss": 0.6538,
      "step": 1522100
    },
    {
      "epoch": 13.889699978100591,
      "grad_norm": 3.3957929611206055,
      "learning_rate": 3.842525001824951e-05,
      "loss": 0.6758,
      "step": 1522200
    },
    {
      "epoch": 13.890612453463756,
      "grad_norm": 4.603305339813232,
      "learning_rate": 3.842448962211354e-05,
      "loss": 0.6343,
      "step": 1522300
    },
    {
      "epoch": 13.891524928826922,
      "grad_norm": 3.484934091567993,
      "learning_rate": 3.8423729225977564e-05,
      "loss": 0.6942,
      "step": 1522400
    },
    {
      "epoch": 13.892437404190087,
      "grad_norm": 3.791442394256592,
      "learning_rate": 3.8422968829841594e-05,
      "loss": 0.6578,
      "step": 1522500
    },
    {
      "epoch": 13.893349879553252,
      "grad_norm": 3.551626443862915,
      "learning_rate": 3.8422208433705624e-05,
      "loss": 0.6504,
      "step": 1522600
    },
    {
      "epoch": 13.894262354916417,
      "grad_norm": 4.174346446990967,
      "learning_rate": 3.8421448037569654e-05,
      "loss": 0.7017,
      "step": 1522700
    },
    {
      "epoch": 13.895174830279583,
      "grad_norm": 2.833827495574951,
      "learning_rate": 3.8420687641433684e-05,
      "loss": 0.6869,
      "step": 1522800
    },
    {
      "epoch": 13.896087305642748,
      "grad_norm": 4.474206924438477,
      "learning_rate": 3.841992724529771e-05,
      "loss": 0.6625,
      "step": 1522900
    },
    {
      "epoch": 13.896999781005913,
      "grad_norm": 3.8561651706695557,
      "learning_rate": 3.8419166849161744e-05,
      "loss": 0.6693,
      "step": 1523000
    },
    {
      "epoch": 13.897912256369079,
      "grad_norm": 3.532085418701172,
      "learning_rate": 3.841840645302577e-05,
      "loss": 0.6594,
      "step": 1523100
    },
    {
      "epoch": 13.898824731732244,
      "grad_norm": 2.479118824005127,
      "learning_rate": 3.84176460568898e-05,
      "loss": 0.6996,
      "step": 1523200
    },
    {
      "epoch": 13.89973720709541,
      "grad_norm": 4.243807792663574,
      "learning_rate": 3.841688566075383e-05,
      "loss": 0.6468,
      "step": 1523300
    },
    {
      "epoch": 13.900649682458573,
      "grad_norm": 3.878274917602539,
      "learning_rate": 3.841612526461786e-05,
      "loss": 0.6561,
      "step": 1523400
    },
    {
      "epoch": 13.901562157821738,
      "grad_norm": 3.9146339893341064,
      "learning_rate": 3.841536486848188e-05,
      "loss": 0.6847,
      "step": 1523500
    },
    {
      "epoch": 13.902474633184903,
      "grad_norm": 4.324956893920898,
      "learning_rate": 3.841460447234592e-05,
      "loss": 0.6029,
      "step": 1523600
    },
    {
      "epoch": 13.903387108548069,
      "grad_norm": 2.995107889175415,
      "learning_rate": 3.841384407620994e-05,
      "loss": 0.7027,
      "step": 1523700
    },
    {
      "epoch": 13.904299583911234,
      "grad_norm": 3.222756862640381,
      "learning_rate": 3.841308368007397e-05,
      "loss": 0.7239,
      "step": 1523800
    },
    {
      "epoch": 13.9052120592744,
      "grad_norm": 3.8158955574035645,
      "learning_rate": 3.8412323283938e-05,
      "loss": 0.6908,
      "step": 1523900
    },
    {
      "epoch": 13.906124534637565,
      "grad_norm": 4.211662292480469,
      "learning_rate": 3.841156288780203e-05,
      "loss": 0.6865,
      "step": 1524000
    },
    {
      "epoch": 13.90703701000073,
      "grad_norm": 2.808077335357666,
      "learning_rate": 3.841080249166606e-05,
      "loss": 0.6944,
      "step": 1524100
    },
    {
      "epoch": 13.907949485363895,
      "grad_norm": 4.419227600097656,
      "learning_rate": 3.841004209553009e-05,
      "loss": 0.6874,
      "step": 1524200
    },
    {
      "epoch": 13.90886196072706,
      "grad_norm": 4.2606520652771,
      "learning_rate": 3.8409281699394115e-05,
      "loss": 0.6931,
      "step": 1524300
    },
    {
      "epoch": 13.909774436090226,
      "grad_norm": 4.287769794464111,
      "learning_rate": 3.840852130325815e-05,
      "loss": 0.6663,
      "step": 1524400
    },
    {
      "epoch": 13.910686911453391,
      "grad_norm": 4.350734233856201,
      "learning_rate": 3.8407760907122175e-05,
      "loss": 0.6574,
      "step": 1524500
    },
    {
      "epoch": 13.911599386816556,
      "grad_norm": 3.9158132076263428,
      "learning_rate": 3.8407000510986205e-05,
      "loss": 0.6813,
      "step": 1524600
    },
    {
      "epoch": 13.912511862179722,
      "grad_norm": 4.208446979522705,
      "learning_rate": 3.8406240114850235e-05,
      "loss": 0.672,
      "step": 1524700
    },
    {
      "epoch": 13.913424337542887,
      "grad_norm": 3.8590259552001953,
      "learning_rate": 3.8405479718714265e-05,
      "loss": 0.6815,
      "step": 1524800
    },
    {
      "epoch": 13.914336812906052,
      "grad_norm": 4.257192134857178,
      "learning_rate": 3.8404719322578295e-05,
      "loss": 0.6747,
      "step": 1524900
    },
    {
      "epoch": 13.915249288269216,
      "grad_norm": 4.01027774810791,
      "learning_rate": 3.8403958926442325e-05,
      "loss": 0.6694,
      "step": 1525000
    },
    {
      "epoch": 13.916161763632381,
      "grad_norm": 4.35319185256958,
      "learning_rate": 3.840319853030635e-05,
      "loss": 0.704,
      "step": 1525100
    },
    {
      "epoch": 13.917074238995546,
      "grad_norm": 4.937670707702637,
      "learning_rate": 3.840243813417038e-05,
      "loss": 0.6544,
      "step": 1525200
    },
    {
      "epoch": 13.917986714358712,
      "grad_norm": 2.705282688140869,
      "learning_rate": 3.840167773803441e-05,
      "loss": 0.6805,
      "step": 1525300
    },
    {
      "epoch": 13.918899189721877,
      "grad_norm": 3.573892116546631,
      "learning_rate": 3.840091734189843e-05,
      "loss": 0.7137,
      "step": 1525400
    },
    {
      "epoch": 13.919811665085042,
      "grad_norm": 3.726598024368286,
      "learning_rate": 3.840015694576247e-05,
      "loss": 0.6878,
      "step": 1525500
    },
    {
      "epoch": 13.920724140448208,
      "grad_norm": 4.296414852142334,
      "learning_rate": 3.839939654962649e-05,
      "loss": 0.6344,
      "step": 1525600
    },
    {
      "epoch": 13.921636615811373,
      "grad_norm": 4.1903581619262695,
      "learning_rate": 3.839863615349052e-05,
      "loss": 0.6563,
      "step": 1525700
    },
    {
      "epoch": 13.922549091174538,
      "grad_norm": 3.0849921703338623,
      "learning_rate": 3.839787575735455e-05,
      "loss": 0.706,
      "step": 1525800
    },
    {
      "epoch": 13.923461566537704,
      "grad_norm": 3.1005334854125977,
      "learning_rate": 3.839711536121858e-05,
      "loss": 0.6998,
      "step": 1525900
    },
    {
      "epoch": 13.924374041900869,
      "grad_norm": 3.631141185760498,
      "learning_rate": 3.839635496508261e-05,
      "loss": 0.6834,
      "step": 1526000
    },
    {
      "epoch": 13.925286517264034,
      "grad_norm": 3.3624825477600098,
      "learning_rate": 3.839559456894664e-05,
      "loss": 0.6898,
      "step": 1526100
    },
    {
      "epoch": 13.9261989926272,
      "grad_norm": 4.302029609680176,
      "learning_rate": 3.8394834172810666e-05,
      "loss": 0.6341,
      "step": 1526200
    },
    {
      "epoch": 13.927111467990365,
      "grad_norm": 3.501164436340332,
      "learning_rate": 3.83940737766747e-05,
      "loss": 0.697,
      "step": 1526300
    },
    {
      "epoch": 13.92802394335353,
      "grad_norm": 4.955714225769043,
      "learning_rate": 3.8393313380538726e-05,
      "loss": 0.6673,
      "step": 1526400
    },
    {
      "epoch": 13.928936418716695,
      "grad_norm": 3.303942918777466,
      "learning_rate": 3.8392552984402756e-05,
      "loss": 0.672,
      "step": 1526500
    },
    {
      "epoch": 13.92984889407986,
      "grad_norm": 4.35739803314209,
      "learning_rate": 3.8391792588266786e-05,
      "loss": 0.6335,
      "step": 1526600
    },
    {
      "epoch": 13.930761369443026,
      "grad_norm": 4.968595504760742,
      "learning_rate": 3.8391032192130816e-05,
      "loss": 0.66,
      "step": 1526700
    },
    {
      "epoch": 13.93167384480619,
      "grad_norm": 3.484687566757202,
      "learning_rate": 3.839027179599484e-05,
      "loss": 0.6984,
      "step": 1526800
    },
    {
      "epoch": 13.932586320169355,
      "grad_norm": 5.399157524108887,
      "learning_rate": 3.8389511399858876e-05,
      "loss": 0.6972,
      "step": 1526900
    },
    {
      "epoch": 13.93349879553252,
      "grad_norm": 3.704221725463867,
      "learning_rate": 3.83887510037229e-05,
      "loss": 0.6399,
      "step": 1527000
    },
    {
      "epoch": 13.934411270895685,
      "grad_norm": 3.9747307300567627,
      "learning_rate": 3.838799060758693e-05,
      "loss": 0.6622,
      "step": 1527100
    },
    {
      "epoch": 13.93532374625885,
      "grad_norm": 3.331948757171631,
      "learning_rate": 3.838723021145096e-05,
      "loss": 0.6657,
      "step": 1527200
    },
    {
      "epoch": 13.936236221622016,
      "grad_norm": 4.289120197296143,
      "learning_rate": 3.838646981531499e-05,
      "loss": 0.6446,
      "step": 1527300
    },
    {
      "epoch": 13.937148696985181,
      "grad_norm": 3.4989430904388428,
      "learning_rate": 3.838570941917902e-05,
      "loss": 0.6524,
      "step": 1527400
    },
    {
      "epoch": 13.938061172348347,
      "grad_norm": 3.673600435256958,
      "learning_rate": 3.838494902304305e-05,
      "loss": 0.6536,
      "step": 1527500
    },
    {
      "epoch": 13.938973647711512,
      "grad_norm": 4.129255294799805,
      "learning_rate": 3.838418862690707e-05,
      "loss": 0.6616,
      "step": 1527600
    },
    {
      "epoch": 13.939886123074677,
      "grad_norm": 4.237586498260498,
      "learning_rate": 3.838342823077111e-05,
      "loss": 0.6853,
      "step": 1527700
    },
    {
      "epoch": 13.940798598437842,
      "grad_norm": 3.907526731491089,
      "learning_rate": 3.8382667834635133e-05,
      "loss": 0.6633,
      "step": 1527800
    },
    {
      "epoch": 13.941711073801008,
      "grad_norm": 3.5749459266662598,
      "learning_rate": 3.8381907438499163e-05,
      "loss": 0.644,
      "step": 1527900
    },
    {
      "epoch": 13.942623549164173,
      "grad_norm": 4.3180670738220215,
      "learning_rate": 3.8381147042363194e-05,
      "loss": 0.6871,
      "step": 1528000
    },
    {
      "epoch": 13.943536024527338,
      "grad_norm": 3.6537070274353027,
      "learning_rate": 3.838038664622722e-05,
      "loss": 0.6539,
      "step": 1528100
    },
    {
      "epoch": 13.944448499890504,
      "grad_norm": 3.2463114261627197,
      "learning_rate": 3.837962625009125e-05,
      "loss": 0.6922,
      "step": 1528200
    },
    {
      "epoch": 13.945360975253669,
      "grad_norm": 3.716721534729004,
      "learning_rate": 3.837886585395528e-05,
      "loss": 0.6629,
      "step": 1528300
    },
    {
      "epoch": 13.946273450616832,
      "grad_norm": 4.433478355407715,
      "learning_rate": 3.837810545781931e-05,
      "loss": 0.6961,
      "step": 1528400
    },
    {
      "epoch": 13.947185925979998,
      "grad_norm": 3.94793963432312,
      "learning_rate": 3.837734506168334e-05,
      "loss": 0.7035,
      "step": 1528500
    },
    {
      "epoch": 13.948098401343163,
      "grad_norm": 3.8659157752990723,
      "learning_rate": 3.837658466554737e-05,
      "loss": 0.6582,
      "step": 1528600
    },
    {
      "epoch": 13.949010876706328,
      "grad_norm": 3.3770406246185303,
      "learning_rate": 3.837582426941139e-05,
      "loss": 0.6843,
      "step": 1528700
    },
    {
      "epoch": 13.949923352069494,
      "grad_norm": 3.8837428092956543,
      "learning_rate": 3.837506387327543e-05,
      "loss": 0.6573,
      "step": 1528800
    },
    {
      "epoch": 13.950835827432659,
      "grad_norm": 4.194148063659668,
      "learning_rate": 3.837430347713945e-05,
      "loss": 0.7122,
      "step": 1528900
    },
    {
      "epoch": 13.951748302795824,
      "grad_norm": 4.520138263702393,
      "learning_rate": 3.837354308100348e-05,
      "loss": 0.6248,
      "step": 1529000
    },
    {
      "epoch": 13.95266077815899,
      "grad_norm": 3.5906620025634766,
      "learning_rate": 3.837278268486751e-05,
      "loss": 0.687,
      "step": 1529100
    },
    {
      "epoch": 13.953573253522155,
      "grad_norm": 3.4292778968811035,
      "learning_rate": 3.837202228873154e-05,
      "loss": 0.6859,
      "step": 1529200
    },
    {
      "epoch": 13.95448572888532,
      "grad_norm": 4.325379371643066,
      "learning_rate": 3.8371261892595564e-05,
      "loss": 0.642,
      "step": 1529300
    },
    {
      "epoch": 13.955398204248485,
      "grad_norm": 3.3500640392303467,
      "learning_rate": 3.83705014964596e-05,
      "loss": 0.6876,
      "step": 1529400
    },
    {
      "epoch": 13.95631067961165,
      "grad_norm": 3.161581039428711,
      "learning_rate": 3.8369741100323624e-05,
      "loss": 0.6616,
      "step": 1529500
    },
    {
      "epoch": 13.957223154974816,
      "grad_norm": 3.5952329635620117,
      "learning_rate": 3.8368980704187654e-05,
      "loss": 0.6764,
      "step": 1529600
    },
    {
      "epoch": 13.958135630337981,
      "grad_norm": 2.759657859802246,
      "learning_rate": 3.8368220308051684e-05,
      "loss": 0.6667,
      "step": 1529700
    },
    {
      "epoch": 13.959048105701147,
      "grad_norm": 4.150218963623047,
      "learning_rate": 3.8367459911915714e-05,
      "loss": 0.6743,
      "step": 1529800
    },
    {
      "epoch": 13.959960581064312,
      "grad_norm": 3.9733035564422607,
      "learning_rate": 3.8366699515779744e-05,
      "loss": 0.7083,
      "step": 1529900
    },
    {
      "epoch": 13.960873056427477,
      "grad_norm": 3.523838758468628,
      "learning_rate": 3.8365939119643775e-05,
      "loss": 0.6492,
      "step": 1530000
    },
    {
      "epoch": 13.961785531790643,
      "grad_norm": 4.071195125579834,
      "learning_rate": 3.83651787235078e-05,
      "loss": 0.6964,
      "step": 1530100
    },
    {
      "epoch": 13.962698007153806,
      "grad_norm": 4.667868614196777,
      "learning_rate": 3.8364418327371835e-05,
      "loss": 0.6596,
      "step": 1530200
    },
    {
      "epoch": 13.963610482516971,
      "grad_norm": 4.057094573974609,
      "learning_rate": 3.836365793123586e-05,
      "loss": 0.682,
      "step": 1530300
    },
    {
      "epoch": 13.964522957880137,
      "grad_norm": 3.827331304550171,
      "learning_rate": 3.836289753509989e-05,
      "loss": 0.6625,
      "step": 1530400
    },
    {
      "epoch": 13.965435433243302,
      "grad_norm": 4.587181568145752,
      "learning_rate": 3.836213713896392e-05,
      "loss": 0.6888,
      "step": 1530500
    },
    {
      "epoch": 13.966347908606467,
      "grad_norm": 4.943399906158447,
      "learning_rate": 3.836137674282795e-05,
      "loss": 0.7025,
      "step": 1530600
    },
    {
      "epoch": 13.967260383969633,
      "grad_norm": 3.690822124481201,
      "learning_rate": 3.836061634669197e-05,
      "loss": 0.6752,
      "step": 1530700
    },
    {
      "epoch": 13.968172859332798,
      "grad_norm": 3.5929863452911377,
      "learning_rate": 3.835985595055601e-05,
      "loss": 0.6616,
      "step": 1530800
    },
    {
      "epoch": 13.969085334695963,
      "grad_norm": 3.8075754642486572,
      "learning_rate": 3.835909555442003e-05,
      "loss": 0.666,
      "step": 1530900
    },
    {
      "epoch": 13.969997810059128,
      "grad_norm": 3.748084306716919,
      "learning_rate": 3.835833515828406e-05,
      "loss": 0.7053,
      "step": 1531000
    },
    {
      "epoch": 13.970910285422294,
      "grad_norm": 4.248568534851074,
      "learning_rate": 3.835757476214809e-05,
      "loss": 0.6563,
      "step": 1531100
    },
    {
      "epoch": 13.971822760785459,
      "grad_norm": 4.715695381164551,
      "learning_rate": 3.8356814366012115e-05,
      "loss": 0.7062,
      "step": 1531200
    },
    {
      "epoch": 13.972735236148624,
      "grad_norm": 2.48081111907959,
      "learning_rate": 3.835605396987615e-05,
      "loss": 0.6783,
      "step": 1531300
    },
    {
      "epoch": 13.97364771151179,
      "grad_norm": 3.9779579639434814,
      "learning_rate": 3.8355293573740175e-05,
      "loss": 0.6617,
      "step": 1531400
    },
    {
      "epoch": 13.974560186874955,
      "grad_norm": 4.528916835784912,
      "learning_rate": 3.8354533177604205e-05,
      "loss": 0.683,
      "step": 1531500
    },
    {
      "epoch": 13.97547266223812,
      "grad_norm": 3.96860408782959,
      "learning_rate": 3.8353772781468235e-05,
      "loss": 0.6938,
      "step": 1531600
    },
    {
      "epoch": 13.976385137601286,
      "grad_norm": 3.677480459213257,
      "learning_rate": 3.8353012385332265e-05,
      "loss": 0.6809,
      "step": 1531700
    },
    {
      "epoch": 13.977297612964449,
      "grad_norm": 4.321340084075928,
      "learning_rate": 3.835225198919629e-05,
      "loss": 0.6556,
      "step": 1531800
    },
    {
      "epoch": 13.978210088327614,
      "grad_norm": 4.750860214233398,
      "learning_rate": 3.8351491593060325e-05,
      "loss": 0.6479,
      "step": 1531900
    },
    {
      "epoch": 13.97912256369078,
      "grad_norm": 3.2044804096221924,
      "learning_rate": 3.835073119692435e-05,
      "loss": 0.6818,
      "step": 1532000
    },
    {
      "epoch": 13.980035039053945,
      "grad_norm": 3.6215922832489014,
      "learning_rate": 3.834997080078838e-05,
      "loss": 0.6708,
      "step": 1532100
    },
    {
      "epoch": 13.98094751441711,
      "grad_norm": 4.168065547943115,
      "learning_rate": 3.834921040465241e-05,
      "loss": 0.7051,
      "step": 1532200
    },
    {
      "epoch": 13.981859989780276,
      "grad_norm": 4.151939868927002,
      "learning_rate": 3.834845000851644e-05,
      "loss": 0.6211,
      "step": 1532300
    },
    {
      "epoch": 13.98277246514344,
      "grad_norm": 4.38610315322876,
      "learning_rate": 3.834768961238047e-05,
      "loss": 0.6884,
      "step": 1532400
    },
    {
      "epoch": 13.983684940506606,
      "grad_norm": 3.76035213470459,
      "learning_rate": 3.83469292162445e-05,
      "loss": 0.7321,
      "step": 1532500
    },
    {
      "epoch": 13.984597415869771,
      "grad_norm": 3.8199586868286133,
      "learning_rate": 3.834616882010852e-05,
      "loss": 0.6732,
      "step": 1532600
    },
    {
      "epoch": 13.985509891232937,
      "grad_norm": 3.344306468963623,
      "learning_rate": 3.834540842397256e-05,
      "loss": 0.6333,
      "step": 1532700
    },
    {
      "epoch": 13.986422366596102,
      "grad_norm": 4.240762233734131,
      "learning_rate": 3.834464802783658e-05,
      "loss": 0.6893,
      "step": 1532800
    },
    {
      "epoch": 13.987334841959267,
      "grad_norm": 3.6257452964782715,
      "learning_rate": 3.834388763170061e-05,
      "loss": 0.6707,
      "step": 1532900
    },
    {
      "epoch": 13.988247317322433,
      "grad_norm": 3.3058173656463623,
      "learning_rate": 3.834312723556464e-05,
      "loss": 0.6733,
      "step": 1533000
    },
    {
      "epoch": 13.989159792685598,
      "grad_norm": 3.5552961826324463,
      "learning_rate": 3.834236683942867e-05,
      "loss": 0.7066,
      "step": 1533100
    },
    {
      "epoch": 13.990072268048763,
      "grad_norm": 4.091238498687744,
      "learning_rate": 3.8341606443292696e-05,
      "loss": 0.6944,
      "step": 1533200
    },
    {
      "epoch": 13.990984743411929,
      "grad_norm": 4.6377387046813965,
      "learning_rate": 3.834084604715673e-05,
      "loss": 0.6923,
      "step": 1533300
    },
    {
      "epoch": 13.991897218775094,
      "grad_norm": 3.756793975830078,
      "learning_rate": 3.8340085651020756e-05,
      "loss": 0.6708,
      "step": 1533400
    },
    {
      "epoch": 13.99280969413826,
      "grad_norm": 4.064451217651367,
      "learning_rate": 3.8339325254884786e-05,
      "loss": 0.6755,
      "step": 1533500
    },
    {
      "epoch": 13.993722169501423,
      "grad_norm": 4.613162040710449,
      "learning_rate": 3.8338564858748816e-05,
      "loss": 0.6661,
      "step": 1533600
    },
    {
      "epoch": 13.994634644864588,
      "grad_norm": 4.15123176574707,
      "learning_rate": 3.833780446261284e-05,
      "loss": 0.6802,
      "step": 1533700
    },
    {
      "epoch": 13.995547120227753,
      "grad_norm": 3.5343260765075684,
      "learning_rate": 3.8337044066476876e-05,
      "loss": 0.669,
      "step": 1533800
    },
    {
      "epoch": 13.996459595590919,
      "grad_norm": 3.2728428840637207,
      "learning_rate": 3.83362836703409e-05,
      "loss": 0.66,
      "step": 1533900
    },
    {
      "epoch": 13.997372070954084,
      "grad_norm": 4.317866802215576,
      "learning_rate": 3.833552327420493e-05,
      "loss": 0.6371,
      "step": 1534000
    },
    {
      "epoch": 13.99828454631725,
      "grad_norm": 3.4925851821899414,
      "learning_rate": 3.833476287806896e-05,
      "loss": 0.6699,
      "step": 1534100
    },
    {
      "epoch": 13.999197021680414,
      "grad_norm": 3.93631649017334,
      "learning_rate": 3.833400248193299e-05,
      "loss": 0.6882,
      "step": 1534200
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.5495943427085876,
      "eval_runtime": 25.9108,
      "eval_samples_per_second": 222.649,
      "eval_steps_per_second": 222.649,
      "step": 1534288
    },
    {
      "epoch": 14.0,
      "eval_loss": 0.5274224877357483,
      "eval_runtime": 487.4582,
      "eval_samples_per_second": 224.823,
      "eval_steps_per_second": 224.823,
      "step": 1534288
    },
    {
      "epoch": 14.00010949704358,
      "grad_norm": 4.004976272583008,
      "learning_rate": 3.833324208579701e-05,
      "loss": 0.7026,
      "step": 1534300
    },
    {
      "epoch": 14.001021972406745,
      "grad_norm": 2.8525125980377197,
      "learning_rate": 3.833248168966105e-05,
      "loss": 0.666,
      "step": 1534400
    },
    {
      "epoch": 14.00193444776991,
      "grad_norm": 4.108211040496826,
      "learning_rate": 3.833172129352507e-05,
      "loss": 0.6666,
      "step": 1534500
    },
    {
      "epoch": 14.002846923133076,
      "grad_norm": 4.155851364135742,
      "learning_rate": 3.8330960897389103e-05,
      "loss": 0.6654,
      "step": 1534600
    },
    {
      "epoch": 14.003759398496241,
      "grad_norm": 3.2582004070281982,
      "learning_rate": 3.8330200501253133e-05,
      "loss": 0.682,
      "step": 1534700
    },
    {
      "epoch": 14.004671873859406,
      "grad_norm": 4.544623374938965,
      "learning_rate": 3.8329440105117164e-05,
      "loss": 0.6694,
      "step": 1534800
    },
    {
      "epoch": 14.005584349222572,
      "grad_norm": 3.9678597450256348,
      "learning_rate": 3.8328679708981194e-05,
      "loss": 0.6544,
      "step": 1534900
    },
    {
      "epoch": 14.006496824585737,
      "grad_norm": 3.46616530418396,
      "learning_rate": 3.8327919312845224e-05,
      "loss": 0.6916,
      "step": 1535000
    },
    {
      "epoch": 14.007409299948902,
      "grad_norm": 3.895242929458618,
      "learning_rate": 3.832715891670925e-05,
      "loss": 0.661,
      "step": 1535100
    },
    {
      "epoch": 14.008321775312067,
      "grad_norm": 4.227244853973389,
      "learning_rate": 3.8326398520573284e-05,
      "loss": 0.6401,
      "step": 1535200
    },
    {
      "epoch": 14.009234250675231,
      "grad_norm": 4.237702369689941,
      "learning_rate": 3.832563812443731e-05,
      "loss": 0.653,
      "step": 1535300
    },
    {
      "epoch": 14.010146726038396,
      "grad_norm": 3.582853078842163,
      "learning_rate": 3.832487772830134e-05,
      "loss": 0.6203,
      "step": 1535400
    },
    {
      "epoch": 14.011059201401562,
      "grad_norm": 3.5474462509155273,
      "learning_rate": 3.832411733216537e-05,
      "loss": 0.6976,
      "step": 1535500
    },
    {
      "epoch": 14.011971676764727,
      "grad_norm": 3.9772136211395264,
      "learning_rate": 3.83233569360294e-05,
      "loss": 0.6561,
      "step": 1535600
    },
    {
      "epoch": 14.012884152127892,
      "grad_norm": 4.0382256507873535,
      "learning_rate": 3.832259653989342e-05,
      "loss": 0.6742,
      "step": 1535700
    },
    {
      "epoch": 14.013796627491057,
      "grad_norm": 4.272599220275879,
      "learning_rate": 3.832183614375746e-05,
      "loss": 0.6813,
      "step": 1535800
    },
    {
      "epoch": 14.014709102854223,
      "grad_norm": 4.731513500213623,
      "learning_rate": 3.832107574762148e-05,
      "loss": 0.6392,
      "step": 1535900
    },
    {
      "epoch": 14.015621578217388,
      "grad_norm": 4.26790714263916,
      "learning_rate": 3.832031535148551e-05,
      "loss": 0.6508,
      "step": 1536000
    },
    {
      "epoch": 14.016534053580553,
      "grad_norm": 3.910884141921997,
      "learning_rate": 3.831955495534954e-05,
      "loss": 0.685,
      "step": 1536100
    },
    {
      "epoch": 14.017446528943719,
      "grad_norm": 4.119099140167236,
      "learning_rate": 3.831879455921357e-05,
      "loss": 0.6471,
      "step": 1536200
    },
    {
      "epoch": 14.018359004306884,
      "grad_norm": 3.7444825172424316,
      "learning_rate": 3.83180341630776e-05,
      "loss": 0.6869,
      "step": 1536300
    },
    {
      "epoch": 14.01927147967005,
      "grad_norm": 3.7045907974243164,
      "learning_rate": 3.831727376694163e-05,
      "loss": 0.6866,
      "step": 1536400
    },
    {
      "epoch": 14.020183955033215,
      "grad_norm": 4.235080718994141,
      "learning_rate": 3.8316513370805654e-05,
      "loss": 0.6832,
      "step": 1536500
    },
    {
      "epoch": 14.02109643039638,
      "grad_norm": 5.0664963722229,
      "learning_rate": 3.8315752974669684e-05,
      "loss": 0.6621,
      "step": 1536600
    },
    {
      "epoch": 14.022008905759545,
      "grad_norm": 4.785740852355957,
      "learning_rate": 3.8314992578533715e-05,
      "loss": 0.632,
      "step": 1536700
    },
    {
      "epoch": 14.02292138112271,
      "grad_norm": 4.506405353546143,
      "learning_rate": 3.8314232182397745e-05,
      "loss": 0.7093,
      "step": 1536800
    },
    {
      "epoch": 14.023833856485876,
      "grad_norm": 4.294571876525879,
      "learning_rate": 3.8313471786261775e-05,
      "loss": 0.6857,
      "step": 1536900
    },
    {
      "epoch": 14.02474633184904,
      "grad_norm": 3.57619047164917,
      "learning_rate": 3.83127113901258e-05,
      "loss": 0.67,
      "step": 1537000
    },
    {
      "epoch": 14.025658807212205,
      "grad_norm": 4.079627990722656,
      "learning_rate": 3.831195099398983e-05,
      "loss": 0.6521,
      "step": 1537100
    },
    {
      "epoch": 14.02657128257537,
      "grad_norm": 4.511668682098389,
      "learning_rate": 3.831119059785386e-05,
      "loss": 0.7072,
      "step": 1537200
    },
    {
      "epoch": 14.027483757938535,
      "grad_norm": 3.5310614109039307,
      "learning_rate": 3.831043020171789e-05,
      "loss": 0.6587,
      "step": 1537300
    },
    {
      "epoch": 14.0283962333017,
      "grad_norm": 4.223801612854004,
      "learning_rate": 3.830966980558192e-05,
      "loss": 0.665,
      "step": 1537400
    },
    {
      "epoch": 14.029308708664866,
      "grad_norm": 4.6120147705078125,
      "learning_rate": 3.830890940944595e-05,
      "loss": 0.6451,
      "step": 1537500
    },
    {
      "epoch": 14.030221184028031,
      "grad_norm": 2.858680486679077,
      "learning_rate": 3.830814901330997e-05,
      "loss": 0.6647,
      "step": 1537600
    },
    {
      "epoch": 14.031133659391196,
      "grad_norm": 3.8715691566467285,
      "learning_rate": 3.830738861717401e-05,
      "loss": 0.6344,
      "step": 1537700
    },
    {
      "epoch": 14.032046134754362,
      "grad_norm": 3.9364376068115234,
      "learning_rate": 3.830662822103803e-05,
      "loss": 0.6825,
      "step": 1537800
    },
    {
      "epoch": 14.032958610117527,
      "grad_norm": 3.3912034034729004,
      "learning_rate": 3.830586782490206e-05,
      "loss": 0.6424,
      "step": 1537900
    },
    {
      "epoch": 14.033871085480692,
      "grad_norm": 3.9683678150177,
      "learning_rate": 3.830510742876609e-05,
      "loss": 0.6679,
      "step": 1538000
    },
    {
      "epoch": 14.034783560843858,
      "grad_norm": 3.945897102355957,
      "learning_rate": 3.830434703263012e-05,
      "loss": 0.6942,
      "step": 1538100
    },
    {
      "epoch": 14.035696036207023,
      "grad_norm": 2.9468555450439453,
      "learning_rate": 3.830358663649415e-05,
      "loss": 0.6715,
      "step": 1538200
    },
    {
      "epoch": 14.036608511570188,
      "grad_norm": 3.748682975769043,
      "learning_rate": 3.830282624035818e-05,
      "loss": 0.6734,
      "step": 1538300
    },
    {
      "epoch": 14.037520986933353,
      "grad_norm": 3.190779447555542,
      "learning_rate": 3.8302065844222205e-05,
      "loss": 0.665,
      "step": 1538400
    },
    {
      "epoch": 14.038433462296519,
      "grad_norm": 4.19675350189209,
      "learning_rate": 3.8301305448086235e-05,
      "loss": 0.712,
      "step": 1538500
    },
    {
      "epoch": 14.039345937659682,
      "grad_norm": 4.1189775466918945,
      "learning_rate": 3.8300545051950265e-05,
      "loss": 0.6444,
      "step": 1538600
    },
    {
      "epoch": 14.040258413022848,
      "grad_norm": 4.605311870574951,
      "learning_rate": 3.8299784655814296e-05,
      "loss": 0.682,
      "step": 1538700
    },
    {
      "epoch": 14.041170888386013,
      "grad_norm": 2.660738468170166,
      "learning_rate": 3.8299024259678326e-05,
      "loss": 0.6247,
      "step": 1538800
    },
    {
      "epoch": 14.042083363749178,
      "grad_norm": 3.750685930252075,
      "learning_rate": 3.8298263863542356e-05,
      "loss": 0.6815,
      "step": 1538900
    },
    {
      "epoch": 14.042995839112344,
      "grad_norm": 5.115749359130859,
      "learning_rate": 3.829750346740638e-05,
      "loss": 0.699,
      "step": 1539000
    },
    {
      "epoch": 14.043908314475509,
      "grad_norm": 3.5383105278015137,
      "learning_rate": 3.8296743071270416e-05,
      "loss": 0.6809,
      "step": 1539100
    },
    {
      "epoch": 14.044820789838674,
      "grad_norm": 4.013370990753174,
      "learning_rate": 3.829598267513444e-05,
      "loss": 0.6804,
      "step": 1539200
    },
    {
      "epoch": 14.04573326520184,
      "grad_norm": 3.955583333969116,
      "learning_rate": 3.829522227899847e-05,
      "loss": 0.6254,
      "step": 1539300
    },
    {
      "epoch": 14.046645740565005,
      "grad_norm": 3.8792450428009033,
      "learning_rate": 3.82944618828625e-05,
      "loss": 0.6569,
      "step": 1539400
    },
    {
      "epoch": 14.04755821592817,
      "grad_norm": 4.322345733642578,
      "learning_rate": 3.829370148672652e-05,
      "loss": 0.7094,
      "step": 1539500
    },
    {
      "epoch": 14.048470691291335,
      "grad_norm": 3.6033451557159424,
      "learning_rate": 3.829294109059056e-05,
      "loss": 0.6823,
      "step": 1539600
    },
    {
      "epoch": 14.0493831666545,
      "grad_norm": 3.6535942554473877,
      "learning_rate": 3.829218069445458e-05,
      "loss": 0.6596,
      "step": 1539700
    },
    {
      "epoch": 14.050295642017666,
      "grad_norm": 3.529137134552002,
      "learning_rate": 3.829142029831861e-05,
      "loss": 0.6626,
      "step": 1539800
    },
    {
      "epoch": 14.051208117380831,
      "grad_norm": 3.4791316986083984,
      "learning_rate": 3.829065990218264e-05,
      "loss": 0.6551,
      "step": 1539900
    },
    {
      "epoch": 14.052120592743996,
      "grad_norm": 3.4843716621398926,
      "learning_rate": 3.828989950604667e-05,
      "loss": 0.681,
      "step": 1540000
    },
    {
      "epoch": 14.053033068107162,
      "grad_norm": 4.091073036193848,
      "learning_rate": 3.8289139109910696e-05,
      "loss": 0.6525,
      "step": 1540100
    },
    {
      "epoch": 14.053945543470327,
      "grad_norm": 3.7025389671325684,
      "learning_rate": 3.828837871377473e-05,
      "loss": 0.6928,
      "step": 1540200
    },
    {
      "epoch": 14.05485801883349,
      "grad_norm": 4.510673999786377,
      "learning_rate": 3.8287618317638756e-05,
      "loss": 0.647,
      "step": 1540300
    },
    {
      "epoch": 14.055770494196656,
      "grad_norm": 5.073596000671387,
      "learning_rate": 3.8286857921502786e-05,
      "loss": 0.6917,
      "step": 1540400
    },
    {
      "epoch": 14.056682969559821,
      "grad_norm": 4.010095119476318,
      "learning_rate": 3.8286097525366816e-05,
      "loss": 0.7019,
      "step": 1540500
    },
    {
      "epoch": 14.057595444922987,
      "grad_norm": 3.4960081577301025,
      "learning_rate": 3.8285337129230846e-05,
      "loss": 0.6221,
      "step": 1540600
    },
    {
      "epoch": 14.058507920286152,
      "grad_norm": 4.172168254852295,
      "learning_rate": 3.8284576733094877e-05,
      "loss": 0.649,
      "step": 1540700
    },
    {
      "epoch": 14.059420395649317,
      "grad_norm": 3.021777868270874,
      "learning_rate": 3.8283816336958907e-05,
      "loss": 0.6532,
      "step": 1540800
    },
    {
      "epoch": 14.060332871012482,
      "grad_norm": 4.656550407409668,
      "learning_rate": 3.828305594082293e-05,
      "loss": 0.6889,
      "step": 1540900
    },
    {
      "epoch": 14.061245346375648,
      "grad_norm": 4.449530124664307,
      "learning_rate": 3.828229554468697e-05,
      "loss": 0.6876,
      "step": 1541000
    },
    {
      "epoch": 14.062157821738813,
      "grad_norm": 3.2923600673675537,
      "learning_rate": 3.828153514855099e-05,
      "loss": 0.6709,
      "step": 1541100
    },
    {
      "epoch": 14.063070297101978,
      "grad_norm": 3.1715145111083984,
      "learning_rate": 3.828077475241502e-05,
      "loss": 0.6358,
      "step": 1541200
    },
    {
      "epoch": 14.063982772465144,
      "grad_norm": 3.7716586589813232,
      "learning_rate": 3.828001435627905e-05,
      "loss": 0.6555,
      "step": 1541300
    },
    {
      "epoch": 14.064895247828309,
      "grad_norm": 4.488234043121338,
      "learning_rate": 3.827925396014308e-05,
      "loss": 0.6322,
      "step": 1541400
    },
    {
      "epoch": 14.065807723191474,
      "grad_norm": 4.868391036987305,
      "learning_rate": 3.8278493564007104e-05,
      "loss": 0.6439,
      "step": 1541500
    },
    {
      "epoch": 14.06672019855464,
      "grad_norm": 3.360119581222534,
      "learning_rate": 3.827773316787114e-05,
      "loss": 0.7104,
      "step": 1541600
    },
    {
      "epoch": 14.067632673917805,
      "grad_norm": 3.5095012187957764,
      "learning_rate": 3.8276972771735164e-05,
      "loss": 0.6354,
      "step": 1541700
    },
    {
      "epoch": 14.06854514928097,
      "grad_norm": 3.3208730220794678,
      "learning_rate": 3.8276212375599194e-05,
      "loss": 0.7025,
      "step": 1541800
    },
    {
      "epoch": 14.069457624644135,
      "grad_norm": 3.62619686126709,
      "learning_rate": 3.8275451979463224e-05,
      "loss": 0.6907,
      "step": 1541900
    },
    {
      "epoch": 14.070370100007299,
      "grad_norm": 3.904139757156372,
      "learning_rate": 3.8274691583327254e-05,
      "loss": 0.6562,
      "step": 1542000
    },
    {
      "epoch": 14.071282575370464,
      "grad_norm": 3.829897880554199,
      "learning_rate": 3.8273931187191284e-05,
      "loss": 0.6278,
      "step": 1542100
    },
    {
      "epoch": 14.07219505073363,
      "grad_norm": 4.331573963165283,
      "learning_rate": 3.8273170791055314e-05,
      "loss": 0.6523,
      "step": 1542200
    },
    {
      "epoch": 14.073107526096795,
      "grad_norm": 4.528509140014648,
      "learning_rate": 3.827241039491934e-05,
      "loss": 0.6626,
      "step": 1542300
    },
    {
      "epoch": 14.07402000145996,
      "grad_norm": 2.9130678176879883,
      "learning_rate": 3.827164999878337e-05,
      "loss": 0.6621,
      "step": 1542400
    },
    {
      "epoch": 14.074932476823125,
      "grad_norm": 4.315069198608398,
      "learning_rate": 3.82708896026474e-05,
      "loss": 0.6671,
      "step": 1542500
    },
    {
      "epoch": 14.07584495218629,
      "grad_norm": 4.9298319816589355,
      "learning_rate": 3.827012920651142e-05,
      "loss": 0.6177,
      "step": 1542600
    },
    {
      "epoch": 14.076757427549456,
      "grad_norm": 3.604872941970825,
      "learning_rate": 3.826936881037546e-05,
      "loss": 0.6382,
      "step": 1542700
    },
    {
      "epoch": 14.077669902912621,
      "grad_norm": 3.7171127796173096,
      "learning_rate": 3.826860841423948e-05,
      "loss": 0.6736,
      "step": 1542800
    },
    {
      "epoch": 14.078582378275787,
      "grad_norm": 3.4725279808044434,
      "learning_rate": 3.826784801810351e-05,
      "loss": 0.6615,
      "step": 1542900
    },
    {
      "epoch": 14.079494853638952,
      "grad_norm": 4.0661468505859375,
      "learning_rate": 3.826708762196754e-05,
      "loss": 0.7016,
      "step": 1543000
    },
    {
      "epoch": 14.080407329002117,
      "grad_norm": 2.885744094848633,
      "learning_rate": 3.826632722583157e-05,
      "loss": 0.6772,
      "step": 1543100
    },
    {
      "epoch": 14.081319804365283,
      "grad_norm": 3.268575668334961,
      "learning_rate": 3.82655668296956e-05,
      "loss": 0.6934,
      "step": 1543200
    },
    {
      "epoch": 14.082232279728448,
      "grad_norm": 4.57381534576416,
      "learning_rate": 3.826480643355963e-05,
      "loss": 0.6844,
      "step": 1543300
    },
    {
      "epoch": 14.083144755091613,
      "grad_norm": 3.4970223903656006,
      "learning_rate": 3.8264046037423654e-05,
      "loss": 0.6771,
      "step": 1543400
    },
    {
      "epoch": 14.084057230454778,
      "grad_norm": 3.9860198497772217,
      "learning_rate": 3.826328564128769e-05,
      "loss": 0.692,
      "step": 1543500
    },
    {
      "epoch": 14.084969705817944,
      "grad_norm": 5.065049648284912,
      "learning_rate": 3.8262525245151715e-05,
      "loss": 0.6806,
      "step": 1543600
    },
    {
      "epoch": 14.085882181181107,
      "grad_norm": 3.7974131107330322,
      "learning_rate": 3.8261764849015745e-05,
      "loss": 0.6741,
      "step": 1543700
    },
    {
      "epoch": 14.086794656544273,
      "grad_norm": 3.8411128520965576,
      "learning_rate": 3.8261004452879775e-05,
      "loss": 0.6839,
      "step": 1543800
    },
    {
      "epoch": 14.087707131907438,
      "grad_norm": 4.4824910163879395,
      "learning_rate": 3.8260244056743805e-05,
      "loss": 0.6558,
      "step": 1543900
    },
    {
      "epoch": 14.088619607270603,
      "grad_norm": 3.902378559112549,
      "learning_rate": 3.825948366060783e-05,
      "loss": 0.6481,
      "step": 1544000
    },
    {
      "epoch": 14.089532082633768,
      "grad_norm": 3.5537283420562744,
      "learning_rate": 3.8258723264471865e-05,
      "loss": 0.6572,
      "step": 1544100
    },
    {
      "epoch": 14.090444557996934,
      "grad_norm": 3.97709059715271,
      "learning_rate": 3.825796286833589e-05,
      "loss": 0.6431,
      "step": 1544200
    },
    {
      "epoch": 14.091357033360099,
      "grad_norm": 4.321383953094482,
      "learning_rate": 3.825720247219992e-05,
      "loss": 0.6727,
      "step": 1544300
    },
    {
      "epoch": 14.092269508723264,
      "grad_norm": 2.947643756866455,
      "learning_rate": 3.825644207606395e-05,
      "loss": 0.6678,
      "step": 1544400
    },
    {
      "epoch": 14.09318198408643,
      "grad_norm": 4.383781433105469,
      "learning_rate": 3.825568167992798e-05,
      "loss": 0.6892,
      "step": 1544500
    },
    {
      "epoch": 14.094094459449595,
      "grad_norm": 4.336654186248779,
      "learning_rate": 3.825492128379201e-05,
      "loss": 0.6386,
      "step": 1544600
    },
    {
      "epoch": 14.09500693481276,
      "grad_norm": 3.655444860458374,
      "learning_rate": 3.825416088765604e-05,
      "loss": 0.6627,
      "step": 1544700
    },
    {
      "epoch": 14.095919410175926,
      "grad_norm": 4.3416972160339355,
      "learning_rate": 3.825340049152006e-05,
      "loss": 0.6154,
      "step": 1544800
    },
    {
      "epoch": 14.09683188553909,
      "grad_norm": 4.436063289642334,
      "learning_rate": 3.82526400953841e-05,
      "loss": 0.7125,
      "step": 1544900
    },
    {
      "epoch": 14.097744360902256,
      "grad_norm": 3.2024853229522705,
      "learning_rate": 3.825187969924812e-05,
      "loss": 0.6514,
      "step": 1545000
    },
    {
      "epoch": 14.098656836265421,
      "grad_norm": 3.7639777660369873,
      "learning_rate": 3.8251119303112145e-05,
      "loss": 0.697,
      "step": 1545100
    },
    {
      "epoch": 14.099569311628587,
      "grad_norm": 4.367425441741943,
      "learning_rate": 3.825035890697618e-05,
      "loss": 0.6519,
      "step": 1545200
    },
    {
      "epoch": 14.100481786991752,
      "grad_norm": 3.6999034881591797,
      "learning_rate": 3.8249598510840205e-05,
      "loss": 0.6535,
      "step": 1545300
    },
    {
      "epoch": 14.101394262354916,
      "grad_norm": 5.319789886474609,
      "learning_rate": 3.8248838114704235e-05,
      "loss": 0.6488,
      "step": 1545400
    },
    {
      "epoch": 14.10230673771808,
      "grad_norm": 3.2874903678894043,
      "learning_rate": 3.8248077718568266e-05,
      "loss": 0.698,
      "step": 1545500
    },
    {
      "epoch": 14.103219213081246,
      "grad_norm": 3.5660433769226074,
      "learning_rate": 3.8247317322432296e-05,
      "loss": 0.6599,
      "step": 1545600
    },
    {
      "epoch": 14.104131688444411,
      "grad_norm": 4.202029705047607,
      "learning_rate": 3.8246556926296326e-05,
      "loss": 0.663,
      "step": 1545700
    },
    {
      "epoch": 14.105044163807577,
      "grad_norm": 4.407959938049316,
      "learning_rate": 3.8245796530160356e-05,
      "loss": 0.7178,
      "step": 1545800
    },
    {
      "epoch": 14.105956639170742,
      "grad_norm": 2.9476230144500732,
      "learning_rate": 3.824503613402438e-05,
      "loss": 0.6582,
      "step": 1545900
    },
    {
      "epoch": 14.106869114533907,
      "grad_norm": 3.787935733795166,
      "learning_rate": 3.8244275737888416e-05,
      "loss": 0.6586,
      "step": 1546000
    },
    {
      "epoch": 14.107781589897073,
      "grad_norm": 5.168742656707764,
      "learning_rate": 3.824351534175244e-05,
      "loss": 0.6575,
      "step": 1546100
    },
    {
      "epoch": 14.108694065260238,
      "grad_norm": 4.143620014190674,
      "learning_rate": 3.824275494561647e-05,
      "loss": 0.6368,
      "step": 1546200
    },
    {
      "epoch": 14.109606540623403,
      "grad_norm": 4.939865589141846,
      "learning_rate": 3.82419945494805e-05,
      "loss": 0.638,
      "step": 1546300
    },
    {
      "epoch": 14.110519015986569,
      "grad_norm": 4.335586071014404,
      "learning_rate": 3.824123415334453e-05,
      "loss": 0.6648,
      "step": 1546400
    },
    {
      "epoch": 14.111431491349734,
      "grad_norm": 4.012425422668457,
      "learning_rate": 3.824047375720855e-05,
      "loss": 0.6875,
      "step": 1546500
    },
    {
      "epoch": 14.1123439667129,
      "grad_norm": 4.214291095733643,
      "learning_rate": 3.823971336107259e-05,
      "loss": 0.6289,
      "step": 1546600
    },
    {
      "epoch": 14.113256442076064,
      "grad_norm": 4.237001419067383,
      "learning_rate": 3.823895296493661e-05,
      "loss": 0.6535,
      "step": 1546700
    },
    {
      "epoch": 14.11416891743923,
      "grad_norm": 3.913560152053833,
      "learning_rate": 3.823819256880064e-05,
      "loss": 0.6957,
      "step": 1546800
    },
    {
      "epoch": 14.115081392802395,
      "grad_norm": 3.230719566345215,
      "learning_rate": 3.823743217266467e-05,
      "loss": 0.6473,
      "step": 1546900
    },
    {
      "epoch": 14.11599386816556,
      "grad_norm": 3.3033037185668945,
      "learning_rate": 3.82366717765287e-05,
      "loss": 0.6696,
      "step": 1547000
    },
    {
      "epoch": 14.116906343528724,
      "grad_norm": 4.228720188140869,
      "learning_rate": 3.823591138039273e-05,
      "loss": 0.6457,
      "step": 1547100
    },
    {
      "epoch": 14.11781881889189,
      "grad_norm": 3.267834424972534,
      "learning_rate": 3.823515098425676e-05,
      "loss": 0.6528,
      "step": 1547200
    },
    {
      "epoch": 14.118731294255054,
      "grad_norm": 4.193769454956055,
      "learning_rate": 3.8234390588120786e-05,
      "loss": 0.6643,
      "step": 1547300
    },
    {
      "epoch": 14.11964376961822,
      "grad_norm": 4.9707350730896,
      "learning_rate": 3.823363019198482e-05,
      "loss": 0.7201,
      "step": 1547400
    },
    {
      "epoch": 14.120556244981385,
      "grad_norm": 3.283649206161499,
      "learning_rate": 3.8232869795848847e-05,
      "loss": 0.648,
      "step": 1547500
    },
    {
      "epoch": 14.12146872034455,
      "grad_norm": 4.695268630981445,
      "learning_rate": 3.823210939971288e-05,
      "loss": 0.6361,
      "step": 1547600
    },
    {
      "epoch": 14.122381195707716,
      "grad_norm": 3.317582368850708,
      "learning_rate": 3.823134900357691e-05,
      "loss": 0.6651,
      "step": 1547700
    },
    {
      "epoch": 14.123293671070881,
      "grad_norm": 4.631062030792236,
      "learning_rate": 3.823058860744094e-05,
      "loss": 0.6547,
      "step": 1547800
    },
    {
      "epoch": 14.124206146434046,
      "grad_norm": 4.808552265167236,
      "learning_rate": 3.822982821130496e-05,
      "loss": 0.6988,
      "step": 1547900
    },
    {
      "epoch": 14.125118621797212,
      "grad_norm": 4.588236331939697,
      "learning_rate": 3.822906781516899e-05,
      "loss": 0.6588,
      "step": 1548000
    },
    {
      "epoch": 14.126031097160377,
      "grad_norm": 2.7850680351257324,
      "learning_rate": 3.822830741903302e-05,
      "loss": 0.6226,
      "step": 1548100
    },
    {
      "epoch": 14.126943572523542,
      "grad_norm": 3.989680290222168,
      "learning_rate": 3.822754702289705e-05,
      "loss": 0.6646,
      "step": 1548200
    },
    {
      "epoch": 14.127856047886707,
      "grad_norm": 4.0277581214904785,
      "learning_rate": 3.822678662676108e-05,
      "loss": 0.6645,
      "step": 1548300
    },
    {
      "epoch": 14.128768523249873,
      "grad_norm": 3.798151731491089,
      "learning_rate": 3.8226026230625104e-05,
      "loss": 0.6326,
      "step": 1548400
    },
    {
      "epoch": 14.129680998613038,
      "grad_norm": 4.289025783538818,
      "learning_rate": 3.822526583448914e-05,
      "loss": 0.6435,
      "step": 1548500
    },
    {
      "epoch": 14.130593473976203,
      "grad_norm": 4.669455051422119,
      "learning_rate": 3.8224505438353164e-05,
      "loss": 0.688,
      "step": 1548600
    },
    {
      "epoch": 14.131505949339369,
      "grad_norm": 2.9878244400024414,
      "learning_rate": 3.8223745042217194e-05,
      "loss": 0.6877,
      "step": 1548700
    },
    {
      "epoch": 14.132418424702532,
      "grad_norm": 4.355850696563721,
      "learning_rate": 3.8222984646081224e-05,
      "loss": 0.6144,
      "step": 1548800
    },
    {
      "epoch": 14.133330900065697,
      "grad_norm": 4.567279815673828,
      "learning_rate": 3.8222224249945254e-05,
      "loss": 0.6723,
      "step": 1548900
    },
    {
      "epoch": 14.134243375428863,
      "grad_norm": 3.985773801803589,
      "learning_rate": 3.822146385380928e-05,
      "loss": 0.6562,
      "step": 1549000
    },
    {
      "epoch": 14.135155850792028,
      "grad_norm": 4.079164981842041,
      "learning_rate": 3.8220703457673314e-05,
      "loss": 0.6446,
      "step": 1549100
    },
    {
      "epoch": 14.136068326155193,
      "grad_norm": 4.0449066162109375,
      "learning_rate": 3.821994306153734e-05,
      "loss": 0.6398,
      "step": 1549200
    },
    {
      "epoch": 14.136980801518359,
      "grad_norm": 3.2437310218811035,
      "learning_rate": 3.821918266540137e-05,
      "loss": 0.6287,
      "step": 1549300
    },
    {
      "epoch": 14.137893276881524,
      "grad_norm": 4.074520111083984,
      "learning_rate": 3.82184222692654e-05,
      "loss": 0.6545,
      "step": 1549400
    },
    {
      "epoch": 14.13880575224469,
      "grad_norm": 3.86224102973938,
      "learning_rate": 3.821766187312943e-05,
      "loss": 0.6656,
      "step": 1549500
    },
    {
      "epoch": 14.139718227607855,
      "grad_norm": 3.875558853149414,
      "learning_rate": 3.821690147699346e-05,
      "loss": 0.7019,
      "step": 1549600
    },
    {
      "epoch": 14.14063070297102,
      "grad_norm": 3.072202682495117,
      "learning_rate": 3.821614108085749e-05,
      "loss": 0.6795,
      "step": 1549700
    },
    {
      "epoch": 14.141543178334185,
      "grad_norm": 4.775405406951904,
      "learning_rate": 3.821538068472151e-05,
      "loss": 0.661,
      "step": 1549800
    },
    {
      "epoch": 14.14245565369735,
      "grad_norm": 4.38158655166626,
      "learning_rate": 3.821462028858555e-05,
      "loss": 0.6575,
      "step": 1549900
    },
    {
      "epoch": 14.143368129060516,
      "grad_norm": 4.01995325088501,
      "learning_rate": 3.821385989244957e-05,
      "loss": 0.6655,
      "step": 1550000
    },
    {
      "epoch": 14.144280604423681,
      "grad_norm": 4.1659393310546875,
      "learning_rate": 3.82130994963136e-05,
      "loss": 0.6485,
      "step": 1550100
    },
    {
      "epoch": 14.145193079786846,
      "grad_norm": 3.397625207901001,
      "learning_rate": 3.821233910017763e-05,
      "loss": 0.6454,
      "step": 1550200
    },
    {
      "epoch": 14.146105555150012,
      "grad_norm": 4.939972400665283,
      "learning_rate": 3.821157870404166e-05,
      "loss": 0.6913,
      "step": 1550300
    },
    {
      "epoch": 14.147018030513177,
      "grad_norm": 3.005540609359741,
      "learning_rate": 3.821081830790569e-05,
      "loss": 0.6804,
      "step": 1550400
    },
    {
      "epoch": 14.14793050587634,
      "grad_norm": 4.2431721687316895,
      "learning_rate": 3.821005791176972e-05,
      "loss": 0.6753,
      "step": 1550500
    },
    {
      "epoch": 14.148842981239506,
      "grad_norm": 4.040208339691162,
      "learning_rate": 3.8209297515633745e-05,
      "loss": 0.7019,
      "step": 1550600
    },
    {
      "epoch": 14.149755456602671,
      "grad_norm": 3.44992995262146,
      "learning_rate": 3.8208537119497775e-05,
      "loss": 0.6427,
      "step": 1550700
    },
    {
      "epoch": 14.150667931965836,
      "grad_norm": 3.932997941970825,
      "learning_rate": 3.8207776723361805e-05,
      "loss": 0.667,
      "step": 1550800
    },
    {
      "epoch": 14.151580407329002,
      "grad_norm": 4.670185089111328,
      "learning_rate": 3.820701632722583e-05,
      "loss": 0.6836,
      "step": 1550900
    },
    {
      "epoch": 14.152492882692167,
      "grad_norm": 3.8909058570861816,
      "learning_rate": 3.8206255931089865e-05,
      "loss": 0.6675,
      "step": 1551000
    },
    {
      "epoch": 14.153405358055332,
      "grad_norm": 3.449734926223755,
      "learning_rate": 3.820549553495389e-05,
      "loss": 0.6867,
      "step": 1551100
    },
    {
      "epoch": 14.154317833418498,
      "grad_norm": 3.106513738632202,
      "learning_rate": 3.820473513881792e-05,
      "loss": 0.6739,
      "step": 1551200
    },
    {
      "epoch": 14.155230308781663,
      "grad_norm": 4.546497344970703,
      "learning_rate": 3.820397474268195e-05,
      "loss": 0.6257,
      "step": 1551300
    },
    {
      "epoch": 14.156142784144828,
      "grad_norm": 4.211215019226074,
      "learning_rate": 3.820321434654598e-05,
      "loss": 0.6219,
      "step": 1551400
    },
    {
      "epoch": 14.157055259507993,
      "grad_norm": 3.208627462387085,
      "learning_rate": 3.820245395041001e-05,
      "loss": 0.6569,
      "step": 1551500
    },
    {
      "epoch": 14.157967734871159,
      "grad_norm": 3.2541279792785645,
      "learning_rate": 3.820169355427404e-05,
      "loss": 0.6826,
      "step": 1551600
    },
    {
      "epoch": 14.158880210234324,
      "grad_norm": 3.828429698944092,
      "learning_rate": 3.820093315813806e-05,
      "loss": 0.6443,
      "step": 1551700
    },
    {
      "epoch": 14.15979268559749,
      "grad_norm": 3.594420909881592,
      "learning_rate": 3.82001727620021e-05,
      "loss": 0.6841,
      "step": 1551800
    },
    {
      "epoch": 14.160705160960655,
      "grad_norm": 3.6958470344543457,
      "learning_rate": 3.819941236586612e-05,
      "loss": 0.6684,
      "step": 1551900
    },
    {
      "epoch": 14.16161763632382,
      "grad_norm": 3.7514030933380127,
      "learning_rate": 3.819865196973015e-05,
      "loss": 0.6573,
      "step": 1552000
    },
    {
      "epoch": 14.162530111686985,
      "grad_norm": 5.0420145988464355,
      "learning_rate": 3.819789157359418e-05,
      "loss": 0.6518,
      "step": 1552100
    },
    {
      "epoch": 14.163442587050149,
      "grad_norm": 4.005023002624512,
      "learning_rate": 3.819713117745821e-05,
      "loss": 0.6531,
      "step": 1552200
    },
    {
      "epoch": 14.164355062413314,
      "grad_norm": 3.8972787857055664,
      "learning_rate": 3.8196370781322236e-05,
      "loss": 0.6963,
      "step": 1552300
    },
    {
      "epoch": 14.16526753777648,
      "grad_norm": 3.9373159408569336,
      "learning_rate": 3.819561038518627e-05,
      "loss": 0.652,
      "step": 1552400
    },
    {
      "epoch": 14.166180013139645,
      "grad_norm": 5.02643346786499,
      "learning_rate": 3.8194849989050296e-05,
      "loss": 0.6616,
      "step": 1552500
    },
    {
      "epoch": 14.16709248850281,
      "grad_norm": 5.0042948722839355,
      "learning_rate": 3.8194089592914326e-05,
      "loss": 0.6432,
      "step": 1552600
    },
    {
      "epoch": 14.168004963865975,
      "grad_norm": 3.405412197113037,
      "learning_rate": 3.8193329196778356e-05,
      "loss": 0.6562,
      "step": 1552700
    },
    {
      "epoch": 14.16891743922914,
      "grad_norm": 4.796541213989258,
      "learning_rate": 3.8192568800642386e-05,
      "loss": 0.6534,
      "step": 1552800
    },
    {
      "epoch": 14.169829914592306,
      "grad_norm": 4.910869598388672,
      "learning_rate": 3.8191808404506416e-05,
      "loss": 0.7079,
      "step": 1552900
    },
    {
      "epoch": 14.170742389955471,
      "grad_norm": 4.395605087280273,
      "learning_rate": 3.8191048008370446e-05,
      "loss": 0.698,
      "step": 1553000
    },
    {
      "epoch": 14.171654865318636,
      "grad_norm": 4.0543622970581055,
      "learning_rate": 3.819028761223447e-05,
      "loss": 0.6702,
      "step": 1553100
    },
    {
      "epoch": 14.172567340681802,
      "grad_norm": 5.124030113220215,
      "learning_rate": 3.8189527216098506e-05,
      "loss": 0.7083,
      "step": 1553200
    },
    {
      "epoch": 14.173479816044967,
      "grad_norm": 4.346763610839844,
      "learning_rate": 3.818876681996253e-05,
      "loss": 0.6386,
      "step": 1553300
    },
    {
      "epoch": 14.174392291408132,
      "grad_norm": 3.1384973526000977,
      "learning_rate": 3.818800642382656e-05,
      "loss": 0.6998,
      "step": 1553400
    },
    {
      "epoch": 14.175304766771298,
      "grad_norm": 3.7714309692382812,
      "learning_rate": 3.818724602769059e-05,
      "loss": 0.6923,
      "step": 1553500
    },
    {
      "epoch": 14.176217242134463,
      "grad_norm": 4.73727560043335,
      "learning_rate": 3.818648563155461e-05,
      "loss": 0.6846,
      "step": 1553600
    },
    {
      "epoch": 14.177129717497628,
      "grad_norm": 4.1344122886657715,
      "learning_rate": 3.818572523541864e-05,
      "loss": 0.6834,
      "step": 1553700
    },
    {
      "epoch": 14.178042192860794,
      "grad_norm": 4.323554992675781,
      "learning_rate": 3.818496483928267e-05,
      "loss": 0.6684,
      "step": 1553800
    },
    {
      "epoch": 14.178954668223957,
      "grad_norm": 3.764767646789551,
      "learning_rate": 3.81842044431467e-05,
      "loss": 0.6906,
      "step": 1553900
    },
    {
      "epoch": 14.179867143587122,
      "grad_norm": 3.0726191997528076,
      "learning_rate": 3.818344404701073e-05,
      "loss": 0.6579,
      "step": 1554000
    },
    {
      "epoch": 14.180779618950288,
      "grad_norm": 4.024391174316406,
      "learning_rate": 3.818268365087476e-05,
      "loss": 0.687,
      "step": 1554100
    },
    {
      "epoch": 14.181692094313453,
      "grad_norm": 4.056920051574707,
      "learning_rate": 3.8181923254738787e-05,
      "loss": 0.6737,
      "step": 1554200
    },
    {
      "epoch": 14.182604569676618,
      "grad_norm": 4.019782543182373,
      "learning_rate": 3.818116285860282e-05,
      "loss": 0.6607,
      "step": 1554300
    },
    {
      "epoch": 14.183517045039784,
      "grad_norm": 3.7629215717315674,
      "learning_rate": 3.818040246246685e-05,
      "loss": 0.6566,
      "step": 1554400
    },
    {
      "epoch": 14.184429520402949,
      "grad_norm": 4.124838352203369,
      "learning_rate": 3.817964206633088e-05,
      "loss": 0.6443,
      "step": 1554500
    },
    {
      "epoch": 14.185341995766114,
      "grad_norm": 3.6569461822509766,
      "learning_rate": 3.817888167019491e-05,
      "loss": 0.6598,
      "step": 1554600
    },
    {
      "epoch": 14.18625447112928,
      "grad_norm": 4.779633522033691,
      "learning_rate": 3.817812127405894e-05,
      "loss": 0.7017,
      "step": 1554700
    },
    {
      "epoch": 14.187166946492445,
      "grad_norm": 4.793823719024658,
      "learning_rate": 3.817736087792296e-05,
      "loss": 0.6862,
      "step": 1554800
    },
    {
      "epoch": 14.18807942185561,
      "grad_norm": 3.2181243896484375,
      "learning_rate": 3.8176600481787e-05,
      "loss": 0.6738,
      "step": 1554900
    },
    {
      "epoch": 14.188991897218775,
      "grad_norm": 3.903923749923706,
      "learning_rate": 3.817584008565102e-05,
      "loss": 0.6614,
      "step": 1555000
    },
    {
      "epoch": 14.18990437258194,
      "grad_norm": 3.483855962753296,
      "learning_rate": 3.817507968951505e-05,
      "loss": 0.6884,
      "step": 1555100
    },
    {
      "epoch": 14.190816847945106,
      "grad_norm": 2.387507677078247,
      "learning_rate": 3.817431929337908e-05,
      "loss": 0.6381,
      "step": 1555200
    },
    {
      "epoch": 14.191729323308271,
      "grad_norm": 4.490872859954834,
      "learning_rate": 3.817355889724311e-05,
      "loss": 0.6447,
      "step": 1555300
    },
    {
      "epoch": 14.192641798671437,
      "grad_norm": 4.064640522003174,
      "learning_rate": 3.817279850110714e-05,
      "loss": 0.6548,
      "step": 1555400
    },
    {
      "epoch": 14.193554274034602,
      "grad_norm": 3.9793827533721924,
      "learning_rate": 3.817203810497117e-05,
      "loss": 0.6937,
      "step": 1555500
    },
    {
      "epoch": 14.194466749397765,
      "grad_norm": 3.6724307537078857,
      "learning_rate": 3.8171277708835194e-05,
      "loss": 0.6674,
      "step": 1555600
    },
    {
      "epoch": 14.19537922476093,
      "grad_norm": 3.9032177925109863,
      "learning_rate": 3.817051731269923e-05,
      "loss": 0.6796,
      "step": 1555700
    },
    {
      "epoch": 14.196291700124096,
      "grad_norm": 4.370206832885742,
      "learning_rate": 3.8169756916563254e-05,
      "loss": 0.6898,
      "step": 1555800
    },
    {
      "epoch": 14.197204175487261,
      "grad_norm": 3.5466272830963135,
      "learning_rate": 3.8168996520427284e-05,
      "loss": 0.661,
      "step": 1555900
    },
    {
      "epoch": 14.198116650850427,
      "grad_norm": 4.606445789337158,
      "learning_rate": 3.8168236124291314e-05,
      "loss": 0.6725,
      "step": 1556000
    },
    {
      "epoch": 14.199029126213592,
      "grad_norm": 4.199955940246582,
      "learning_rate": 3.8167475728155344e-05,
      "loss": 0.666,
      "step": 1556100
    },
    {
      "epoch": 14.199941601576757,
      "grad_norm": 2.782907247543335,
      "learning_rate": 3.816671533201937e-05,
      "loss": 0.6768,
      "step": 1556200
    },
    {
      "epoch": 14.200854076939923,
      "grad_norm": 5.265343189239502,
      "learning_rate": 3.8165954935883404e-05,
      "loss": 0.7067,
      "step": 1556300
    },
    {
      "epoch": 14.201766552303088,
      "grad_norm": 4.065157890319824,
      "learning_rate": 3.816519453974743e-05,
      "loss": 0.6894,
      "step": 1556400
    },
    {
      "epoch": 14.202679027666253,
      "grad_norm": 4.225507736206055,
      "learning_rate": 3.816443414361146e-05,
      "loss": 0.6636,
      "step": 1556500
    },
    {
      "epoch": 14.203591503029418,
      "grad_norm": 3.696476936340332,
      "learning_rate": 3.816367374747549e-05,
      "loss": 0.6313,
      "step": 1556600
    },
    {
      "epoch": 14.204503978392584,
      "grad_norm": 4.417991638183594,
      "learning_rate": 3.816291335133951e-05,
      "loss": 0.6698,
      "step": 1556700
    },
    {
      "epoch": 14.205416453755749,
      "grad_norm": 3.892172336578369,
      "learning_rate": 3.816215295520355e-05,
      "loss": 0.706,
      "step": 1556800
    },
    {
      "epoch": 14.206328929118914,
      "grad_norm": 4.46550178527832,
      "learning_rate": 3.816139255906757e-05,
      "loss": 0.6684,
      "step": 1556900
    },
    {
      "epoch": 14.20724140448208,
      "grad_norm": 5.090959072113037,
      "learning_rate": 3.81606321629316e-05,
      "loss": 0.6807,
      "step": 1557000
    },
    {
      "epoch": 14.208153879845245,
      "grad_norm": 4.349813938140869,
      "learning_rate": 3.815987176679563e-05,
      "loss": 0.6597,
      "step": 1557100
    },
    {
      "epoch": 14.20906635520841,
      "grad_norm": 4.13120698928833,
      "learning_rate": 3.815911137065966e-05,
      "loss": 0.6772,
      "step": 1557200
    },
    {
      "epoch": 14.209978830571574,
      "grad_norm": 3.301830291748047,
      "learning_rate": 3.8158350974523685e-05,
      "loss": 0.6554,
      "step": 1557300
    },
    {
      "epoch": 14.210891305934739,
      "grad_norm": 4.569162845611572,
      "learning_rate": 3.815759057838772e-05,
      "loss": 0.6656,
      "step": 1557400
    },
    {
      "epoch": 14.211803781297904,
      "grad_norm": 4.241863250732422,
      "learning_rate": 3.8156830182251745e-05,
      "loss": 0.6559,
      "step": 1557500
    },
    {
      "epoch": 14.21271625666107,
      "grad_norm": 4.629705429077148,
      "learning_rate": 3.8156069786115775e-05,
      "loss": 0.6754,
      "step": 1557600
    },
    {
      "epoch": 14.213628732024235,
      "grad_norm": 4.1418938636779785,
      "learning_rate": 3.8155309389979805e-05,
      "loss": 0.6677,
      "step": 1557700
    },
    {
      "epoch": 14.2145412073874,
      "grad_norm": 3.6593382358551025,
      "learning_rate": 3.8154548993843835e-05,
      "loss": 0.6744,
      "step": 1557800
    },
    {
      "epoch": 14.215453682750566,
      "grad_norm": 3.9524636268615723,
      "learning_rate": 3.8153788597707865e-05,
      "loss": 0.6464,
      "step": 1557900
    },
    {
      "epoch": 14.21636615811373,
      "grad_norm": 4.659389972686768,
      "learning_rate": 3.8153028201571895e-05,
      "loss": 0.726,
      "step": 1558000
    },
    {
      "epoch": 14.217278633476896,
      "grad_norm": 4.287086009979248,
      "learning_rate": 3.815226780543592e-05,
      "loss": 0.6897,
      "step": 1558100
    },
    {
      "epoch": 14.218191108840061,
      "grad_norm": 3.6220479011535645,
      "learning_rate": 3.8151507409299955e-05,
      "loss": 0.6592,
      "step": 1558200
    },
    {
      "epoch": 14.219103584203227,
      "grad_norm": 5.294919490814209,
      "learning_rate": 3.815074701316398e-05,
      "loss": 0.6963,
      "step": 1558300
    },
    {
      "epoch": 14.220016059566392,
      "grad_norm": 3.878549337387085,
      "learning_rate": 3.814998661702801e-05,
      "loss": 0.6569,
      "step": 1558400
    },
    {
      "epoch": 14.220928534929557,
      "grad_norm": 3.9901208877563477,
      "learning_rate": 3.814922622089204e-05,
      "loss": 0.6692,
      "step": 1558500
    },
    {
      "epoch": 14.221841010292723,
      "grad_norm": 3.9776549339294434,
      "learning_rate": 3.814846582475607e-05,
      "loss": 0.6503,
      "step": 1558600
    },
    {
      "epoch": 14.222753485655888,
      "grad_norm": 4.182184219360352,
      "learning_rate": 3.814770542862009e-05,
      "loss": 0.6597,
      "step": 1558700
    },
    {
      "epoch": 14.223665961019053,
      "grad_norm": 3.809654474258423,
      "learning_rate": 3.814694503248413e-05,
      "loss": 0.6759,
      "step": 1558800
    },
    {
      "epoch": 14.224578436382219,
      "grad_norm": 4.305976390838623,
      "learning_rate": 3.814618463634815e-05,
      "loss": 0.6715,
      "step": 1558900
    },
    {
      "epoch": 14.225490911745382,
      "grad_norm": 2.7885594367980957,
      "learning_rate": 3.814542424021218e-05,
      "loss": 0.7159,
      "step": 1559000
    },
    {
      "epoch": 14.226403387108547,
      "grad_norm": 4.277605056762695,
      "learning_rate": 3.814466384407621e-05,
      "loss": 0.6885,
      "step": 1559100
    },
    {
      "epoch": 14.227315862471713,
      "grad_norm": 4.378932952880859,
      "learning_rate": 3.814390344794024e-05,
      "loss": 0.6261,
      "step": 1559200
    },
    {
      "epoch": 14.228228337834878,
      "grad_norm": 3.3245606422424316,
      "learning_rate": 3.814314305180427e-05,
      "loss": 0.6745,
      "step": 1559300
    },
    {
      "epoch": 14.229140813198043,
      "grad_norm": 4.305285930633545,
      "learning_rate": 3.8142382655668296e-05,
      "loss": 0.7119,
      "step": 1559400
    },
    {
      "epoch": 14.230053288561209,
      "grad_norm": 4.783362865447998,
      "learning_rate": 3.8141622259532326e-05,
      "loss": 0.6267,
      "step": 1559500
    },
    {
      "epoch": 14.230965763924374,
      "grad_norm": 4.42354679107666,
      "learning_rate": 3.8140861863396356e-05,
      "loss": 0.6766,
      "step": 1559600
    },
    {
      "epoch": 14.23187823928754,
      "grad_norm": 3.944523334503174,
      "learning_rate": 3.8140101467260386e-05,
      "loss": 0.6517,
      "step": 1559700
    },
    {
      "epoch": 14.232790714650704,
      "grad_norm": 4.193072319030762,
      "learning_rate": 3.813934107112441e-05,
      "loss": 0.6992,
      "step": 1559800
    },
    {
      "epoch": 14.23370319001387,
      "grad_norm": 3.299302577972412,
      "learning_rate": 3.8138580674988446e-05,
      "loss": 0.6989,
      "step": 1559900
    },
    {
      "epoch": 14.234615665377035,
      "grad_norm": 3.32008695602417,
      "learning_rate": 3.813782027885247e-05,
      "loss": 0.6929,
      "step": 1560000
    },
    {
      "epoch": 14.2355281407402,
      "grad_norm": 4.111166477203369,
      "learning_rate": 3.81370598827165e-05,
      "loss": 0.6744,
      "step": 1560100
    },
    {
      "epoch": 14.236440616103366,
      "grad_norm": 2.8745076656341553,
      "learning_rate": 3.813629948658053e-05,
      "loss": 0.668,
      "step": 1560200
    },
    {
      "epoch": 14.237353091466531,
      "grad_norm": 3.6908276081085205,
      "learning_rate": 3.813553909044456e-05,
      "loss": 0.6231,
      "step": 1560300
    },
    {
      "epoch": 14.238265566829696,
      "grad_norm": 2.8037872314453125,
      "learning_rate": 3.813477869430859e-05,
      "loss": 0.6631,
      "step": 1560400
    },
    {
      "epoch": 14.239178042192862,
      "grad_norm": 3.9394333362579346,
      "learning_rate": 3.813401829817262e-05,
      "loss": 0.6669,
      "step": 1560500
    },
    {
      "epoch": 14.240090517556027,
      "grad_norm": 4.378172874450684,
      "learning_rate": 3.813325790203664e-05,
      "loss": 0.6682,
      "step": 1560600
    },
    {
      "epoch": 14.24100299291919,
      "grad_norm": 3.9396095275878906,
      "learning_rate": 3.813249750590068e-05,
      "loss": 0.7094,
      "step": 1560700
    },
    {
      "epoch": 14.241915468282356,
      "grad_norm": 4.108534336090088,
      "learning_rate": 3.81317371097647e-05,
      "loss": 0.6436,
      "step": 1560800
    },
    {
      "epoch": 14.242827943645521,
      "grad_norm": 4.007903575897217,
      "learning_rate": 3.813097671362873e-05,
      "loss": 0.6263,
      "step": 1560900
    },
    {
      "epoch": 14.243740419008686,
      "grad_norm": 4.0749006271362305,
      "learning_rate": 3.813021631749276e-05,
      "loss": 0.6421,
      "step": 1561000
    },
    {
      "epoch": 14.244652894371852,
      "grad_norm": 3.7985408306121826,
      "learning_rate": 3.8129455921356793e-05,
      "loss": 0.6723,
      "step": 1561100
    },
    {
      "epoch": 14.245565369735017,
      "grad_norm": 3.846217155456543,
      "learning_rate": 3.812869552522082e-05,
      "loss": 0.6869,
      "step": 1561200
    },
    {
      "epoch": 14.246477845098182,
      "grad_norm": 3.2567484378814697,
      "learning_rate": 3.8127935129084854e-05,
      "loss": 0.6522,
      "step": 1561300
    },
    {
      "epoch": 14.247390320461347,
      "grad_norm": 3.773996591567993,
      "learning_rate": 3.812717473294888e-05,
      "loss": 0.6964,
      "step": 1561400
    },
    {
      "epoch": 14.248302795824513,
      "grad_norm": 3.554605722427368,
      "learning_rate": 3.812641433681291e-05,
      "loss": 0.6707,
      "step": 1561500
    },
    {
      "epoch": 14.249215271187678,
      "grad_norm": 3.5586814880371094,
      "learning_rate": 3.812565394067694e-05,
      "loss": 0.6585,
      "step": 1561600
    },
    {
      "epoch": 14.250127746550843,
      "grad_norm": 3.6559107303619385,
      "learning_rate": 3.812489354454097e-05,
      "loss": 0.6641,
      "step": 1561700
    },
    {
      "epoch": 14.251040221914009,
      "grad_norm": 2.9239795207977295,
      "learning_rate": 3.8124133148405e-05,
      "loss": 0.7059,
      "step": 1561800
    },
    {
      "epoch": 14.251952697277174,
      "grad_norm": 4.280346870422363,
      "learning_rate": 3.812337275226903e-05,
      "loss": 0.6902,
      "step": 1561900
    },
    {
      "epoch": 14.25286517264034,
      "grad_norm": 3.6750969886779785,
      "learning_rate": 3.812261235613305e-05,
      "loss": 0.6866,
      "step": 1562000
    },
    {
      "epoch": 14.253777648003505,
      "grad_norm": 4.59396505355835,
      "learning_rate": 3.812185195999708e-05,
      "loss": 0.6875,
      "step": 1562100
    },
    {
      "epoch": 14.25469012336667,
      "grad_norm": 3.65674090385437,
      "learning_rate": 3.812109156386111e-05,
      "loss": 0.6817,
      "step": 1562200
    },
    {
      "epoch": 14.255602598729835,
      "grad_norm": 2.8802239894866943,
      "learning_rate": 3.812033116772514e-05,
      "loss": 0.6383,
      "step": 1562300
    },
    {
      "epoch": 14.256515074092999,
      "grad_norm": 3.563098430633545,
      "learning_rate": 3.811957077158917e-05,
      "loss": 0.6642,
      "step": 1562400
    },
    {
      "epoch": 14.257427549456164,
      "grad_norm": 3.8138206005096436,
      "learning_rate": 3.8118810375453194e-05,
      "loss": 0.6757,
      "step": 1562500
    },
    {
      "epoch": 14.25834002481933,
      "grad_norm": 3.5496678352355957,
      "learning_rate": 3.8118049979317224e-05,
      "loss": 0.6779,
      "step": 1562600
    },
    {
      "epoch": 14.259252500182495,
      "grad_norm": 4.048525810241699,
      "learning_rate": 3.8117289583181254e-05,
      "loss": 0.6692,
      "step": 1562700
    },
    {
      "epoch": 14.26016497554566,
      "grad_norm": 4.49741792678833,
      "learning_rate": 3.8116529187045284e-05,
      "loss": 0.71,
      "step": 1562800
    },
    {
      "epoch": 14.261077450908825,
      "grad_norm": 5.107154369354248,
      "learning_rate": 3.8115768790909314e-05,
      "loss": 0.661,
      "step": 1562900
    },
    {
      "epoch": 14.26198992627199,
      "grad_norm": 4.083846569061279,
      "learning_rate": 3.8115008394773344e-05,
      "loss": 0.6337,
      "step": 1563000
    },
    {
      "epoch": 14.262902401635156,
      "grad_norm": 3.8219988346099854,
      "learning_rate": 3.811424799863737e-05,
      "loss": 0.658,
      "step": 1563100
    },
    {
      "epoch": 14.263814876998321,
      "grad_norm": 3.460667133331299,
      "learning_rate": 3.8113487602501404e-05,
      "loss": 0.6299,
      "step": 1563200
    },
    {
      "epoch": 14.264727352361486,
      "grad_norm": 3.798137664794922,
      "learning_rate": 3.811272720636543e-05,
      "loss": 0.6742,
      "step": 1563300
    },
    {
      "epoch": 14.265639827724652,
      "grad_norm": 3.4706552028656006,
      "learning_rate": 3.811196681022946e-05,
      "loss": 0.6731,
      "step": 1563400
    },
    {
      "epoch": 14.266552303087817,
      "grad_norm": 4.136127948760986,
      "learning_rate": 3.811120641409349e-05,
      "loss": 0.6502,
      "step": 1563500
    },
    {
      "epoch": 14.267464778450982,
      "grad_norm": 4.133076190948486,
      "learning_rate": 3.811044601795752e-05,
      "loss": 0.6777,
      "step": 1563600
    },
    {
      "epoch": 14.268377253814148,
      "grad_norm": 3.9399120807647705,
      "learning_rate": 3.810968562182155e-05,
      "loss": 0.6204,
      "step": 1563700
    },
    {
      "epoch": 14.269289729177313,
      "grad_norm": 4.556621074676514,
      "learning_rate": 3.810892522568558e-05,
      "loss": 0.6494,
      "step": 1563800
    },
    {
      "epoch": 14.270202204540478,
      "grad_norm": 4.615331172943115,
      "learning_rate": 3.81081648295496e-05,
      "loss": 0.701,
      "step": 1563900
    },
    {
      "epoch": 14.271114679903643,
      "grad_norm": 4.433193206787109,
      "learning_rate": 3.810740443341363e-05,
      "loss": 0.6382,
      "step": 1564000
    },
    {
      "epoch": 14.272027155266807,
      "grad_norm": 3.6484508514404297,
      "learning_rate": 3.810664403727766e-05,
      "loss": 0.6548,
      "step": 1564100
    },
    {
      "epoch": 14.272939630629972,
      "grad_norm": 3.764066696166992,
      "learning_rate": 3.810588364114169e-05,
      "loss": 0.6836,
      "step": 1564200
    },
    {
      "epoch": 14.273852105993138,
      "grad_norm": 3.6778345108032227,
      "learning_rate": 3.810512324500572e-05,
      "loss": 0.6238,
      "step": 1564300
    },
    {
      "epoch": 14.274764581356303,
      "grad_norm": 3.4602837562561035,
      "learning_rate": 3.810436284886975e-05,
      "loss": 0.7092,
      "step": 1564400
    },
    {
      "epoch": 14.275677056719468,
      "grad_norm": 3.4838051795959473,
      "learning_rate": 3.8103602452733775e-05,
      "loss": 0.6457,
      "step": 1564500
    },
    {
      "epoch": 14.276589532082633,
      "grad_norm": 3.9403250217437744,
      "learning_rate": 3.810284205659781e-05,
      "loss": 0.721,
      "step": 1564600
    },
    {
      "epoch": 14.277502007445799,
      "grad_norm": 3.726577043533325,
      "learning_rate": 3.8102081660461835e-05,
      "loss": 0.6585,
      "step": 1564700
    },
    {
      "epoch": 14.278414482808964,
      "grad_norm": 3.537731409072876,
      "learning_rate": 3.8101321264325865e-05,
      "loss": 0.7038,
      "step": 1564800
    },
    {
      "epoch": 14.27932695817213,
      "grad_norm": 3.694209098815918,
      "learning_rate": 3.8100560868189895e-05,
      "loss": 0.6497,
      "step": 1564900
    },
    {
      "epoch": 14.280239433535295,
      "grad_norm": 3.5140233039855957,
      "learning_rate": 3.809980047205392e-05,
      "loss": 0.6612,
      "step": 1565000
    },
    {
      "epoch": 14.28115190889846,
      "grad_norm": 3.650447130203247,
      "learning_rate": 3.8099040075917955e-05,
      "loss": 0.6844,
      "step": 1565100
    },
    {
      "epoch": 14.282064384261625,
      "grad_norm": 3.851459264755249,
      "learning_rate": 3.809827967978198e-05,
      "loss": 0.6872,
      "step": 1565200
    },
    {
      "epoch": 14.28297685962479,
      "grad_norm": 4.2724289894104,
      "learning_rate": 3.809751928364601e-05,
      "loss": 0.6616,
      "step": 1565300
    },
    {
      "epoch": 14.283889334987956,
      "grad_norm": 4.619733810424805,
      "learning_rate": 3.809675888751004e-05,
      "loss": 0.6826,
      "step": 1565400
    },
    {
      "epoch": 14.284801810351121,
      "grad_norm": 4.841492652893066,
      "learning_rate": 3.809599849137407e-05,
      "loss": 0.6536,
      "step": 1565500
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 3.7020134925842285,
      "learning_rate": 3.809523809523809e-05,
      "loss": 0.6845,
      "step": 1565600
    },
    {
      "epoch": 14.28662676107745,
      "grad_norm": 3.592226028442383,
      "learning_rate": 3.809447769910213e-05,
      "loss": 0.6702,
      "step": 1565700
    },
    {
      "epoch": 14.287539236440615,
      "grad_norm": 3.5536491870880127,
      "learning_rate": 3.809371730296615e-05,
      "loss": 0.6409,
      "step": 1565800
    },
    {
      "epoch": 14.28845171180378,
      "grad_norm": 3.9007039070129395,
      "learning_rate": 3.809295690683018e-05,
      "loss": 0.6517,
      "step": 1565900
    },
    {
      "epoch": 14.289364187166946,
      "grad_norm": 3.854384660720825,
      "learning_rate": 3.809219651069421e-05,
      "loss": 0.6649,
      "step": 1566000
    },
    {
      "epoch": 14.290276662530111,
      "grad_norm": 3.548917770385742,
      "learning_rate": 3.809143611455824e-05,
      "loss": 0.6841,
      "step": 1566100
    },
    {
      "epoch": 14.291189137893276,
      "grad_norm": 4.682321071624756,
      "learning_rate": 3.809067571842227e-05,
      "loss": 0.683,
      "step": 1566200
    },
    {
      "epoch": 14.292101613256442,
      "grad_norm": 4.166913032531738,
      "learning_rate": 3.80899153222863e-05,
      "loss": 0.664,
      "step": 1566300
    },
    {
      "epoch": 14.293014088619607,
      "grad_norm": 4.210411548614502,
      "learning_rate": 3.8089154926150326e-05,
      "loss": 0.6639,
      "step": 1566400
    },
    {
      "epoch": 14.293926563982772,
      "grad_norm": 4.346031188964844,
      "learning_rate": 3.808839453001436e-05,
      "loss": 0.6901,
      "step": 1566500
    },
    {
      "epoch": 14.294839039345938,
      "grad_norm": 4.081875324249268,
      "learning_rate": 3.8087634133878386e-05,
      "loss": 0.6773,
      "step": 1566600
    },
    {
      "epoch": 14.295751514709103,
      "grad_norm": 4.037004470825195,
      "learning_rate": 3.8086873737742416e-05,
      "loss": 0.6582,
      "step": 1566700
    },
    {
      "epoch": 14.296663990072268,
      "grad_norm": 4.621155261993408,
      "learning_rate": 3.8086113341606446e-05,
      "loss": 0.6883,
      "step": 1566800
    },
    {
      "epoch": 14.297576465435434,
      "grad_norm": 4.0435357093811035,
      "learning_rate": 3.8085352945470476e-05,
      "loss": 0.6306,
      "step": 1566900
    },
    {
      "epoch": 14.298488940798599,
      "grad_norm": 4.235613822937012,
      "learning_rate": 3.80845925493345e-05,
      "loss": 0.6937,
      "step": 1567000
    },
    {
      "epoch": 14.299401416161764,
      "grad_norm": 4.3059587478637695,
      "learning_rate": 3.8083832153198536e-05,
      "loss": 0.6331,
      "step": 1567100
    },
    {
      "epoch": 14.30031389152493,
      "grad_norm": 4.008391380310059,
      "learning_rate": 3.808307175706256e-05,
      "loss": 0.6502,
      "step": 1567200
    },
    {
      "epoch": 14.301226366888095,
      "grad_norm": 3.1874163150787354,
      "learning_rate": 3.808231136092659e-05,
      "loss": 0.694,
      "step": 1567300
    },
    {
      "epoch": 14.30213884225126,
      "grad_norm": 3.5801525115966797,
      "learning_rate": 3.808155096479062e-05,
      "loss": 0.699,
      "step": 1567400
    },
    {
      "epoch": 14.303051317614424,
      "grad_norm": 3.9181289672851562,
      "learning_rate": 3.808079056865465e-05,
      "loss": 0.6617,
      "step": 1567500
    },
    {
      "epoch": 14.303963792977589,
      "grad_norm": 3.3204362392425537,
      "learning_rate": 3.808003017251868e-05,
      "loss": 0.6724,
      "step": 1567600
    },
    {
      "epoch": 14.304876268340754,
      "grad_norm": 2.893213987350464,
      "learning_rate": 3.807926977638271e-05,
      "loss": 0.6566,
      "step": 1567700
    },
    {
      "epoch": 14.30578874370392,
      "grad_norm": 4.011666774749756,
      "learning_rate": 3.807850938024673e-05,
      "loss": 0.6665,
      "step": 1567800
    },
    {
      "epoch": 14.306701219067085,
      "grad_norm": 3.576231002807617,
      "learning_rate": 3.8077748984110763e-05,
      "loss": 0.6658,
      "step": 1567900
    },
    {
      "epoch": 14.30761369443025,
      "grad_norm": 3.975935220718384,
      "learning_rate": 3.8076988587974794e-05,
      "loss": 0.682,
      "step": 1568000
    },
    {
      "epoch": 14.308526169793415,
      "grad_norm": 3.1844255924224854,
      "learning_rate": 3.807622819183882e-05,
      "loss": 0.6489,
      "step": 1568100
    },
    {
      "epoch": 14.30943864515658,
      "grad_norm": 4.191623210906982,
      "learning_rate": 3.8075467795702854e-05,
      "loss": 0.6681,
      "step": 1568200
    },
    {
      "epoch": 14.310351120519746,
      "grad_norm": 2.9971938133239746,
      "learning_rate": 3.807470739956688e-05,
      "loss": 0.6564,
      "step": 1568300
    },
    {
      "epoch": 14.311263595882911,
      "grad_norm": 3.605729579925537,
      "learning_rate": 3.807394700343091e-05,
      "loss": 0.6935,
      "step": 1568400
    },
    {
      "epoch": 14.312176071246077,
      "grad_norm": 3.4244227409362793,
      "learning_rate": 3.807318660729494e-05,
      "loss": 0.6521,
      "step": 1568500
    },
    {
      "epoch": 14.313088546609242,
      "grad_norm": 4.646191120147705,
      "learning_rate": 3.807242621115897e-05,
      "loss": 0.7019,
      "step": 1568600
    },
    {
      "epoch": 14.314001021972407,
      "grad_norm": 4.2050042152404785,
      "learning_rate": 3.8071665815023e-05,
      "loss": 0.6507,
      "step": 1568700
    },
    {
      "epoch": 14.314913497335573,
      "grad_norm": 3.1829044818878174,
      "learning_rate": 3.807090541888703e-05,
      "loss": 0.6849,
      "step": 1568800
    },
    {
      "epoch": 14.315825972698738,
      "grad_norm": 2.22013258934021,
      "learning_rate": 3.807014502275105e-05,
      "loss": 0.685,
      "step": 1568900
    },
    {
      "epoch": 14.316738448061903,
      "grad_norm": 3.982694387435913,
      "learning_rate": 3.806938462661509e-05,
      "loss": 0.6889,
      "step": 1569000
    },
    {
      "epoch": 14.317650923425067,
      "grad_norm": 3.437013626098633,
      "learning_rate": 3.806862423047911e-05,
      "loss": 0.6759,
      "step": 1569100
    },
    {
      "epoch": 14.318563398788232,
      "grad_norm": 4.183798789978027,
      "learning_rate": 3.806786383434314e-05,
      "loss": 0.6577,
      "step": 1569200
    },
    {
      "epoch": 14.319475874151397,
      "grad_norm": 4.044954776763916,
      "learning_rate": 3.806710343820717e-05,
      "loss": 0.7158,
      "step": 1569300
    },
    {
      "epoch": 14.320388349514563,
      "grad_norm": 4.002997398376465,
      "learning_rate": 3.80663430420712e-05,
      "loss": 0.6218,
      "step": 1569400
    },
    {
      "epoch": 14.321300824877728,
      "grad_norm": 3.8142902851104736,
      "learning_rate": 3.8065582645935224e-05,
      "loss": 0.6554,
      "step": 1569500
    },
    {
      "epoch": 14.322213300240893,
      "grad_norm": 2.4359982013702393,
      "learning_rate": 3.806482224979926e-05,
      "loss": 0.6843,
      "step": 1569600
    },
    {
      "epoch": 14.323125775604058,
      "grad_norm": 3.416350841522217,
      "learning_rate": 3.8064061853663284e-05,
      "loss": 0.6495,
      "step": 1569700
    },
    {
      "epoch": 14.324038250967224,
      "grad_norm": 4.226414203643799,
      "learning_rate": 3.8063301457527314e-05,
      "loss": 0.6669,
      "step": 1569800
    },
    {
      "epoch": 14.324950726330389,
      "grad_norm": 3.3771188259124756,
      "learning_rate": 3.8062541061391344e-05,
      "loss": 0.6472,
      "step": 1569900
    },
    {
      "epoch": 14.325863201693554,
      "grad_norm": 4.255571365356445,
      "learning_rate": 3.8061780665255375e-05,
      "loss": 0.6625,
      "step": 1570000
    },
    {
      "epoch": 14.32677567705672,
      "grad_norm": 4.041053771972656,
      "learning_rate": 3.8061020269119405e-05,
      "loss": 0.6908,
      "step": 1570100
    },
    {
      "epoch": 14.327688152419885,
      "grad_norm": 4.234584808349609,
      "learning_rate": 3.8060259872983435e-05,
      "loss": 0.6625,
      "step": 1570200
    },
    {
      "epoch": 14.32860062778305,
      "grad_norm": 3.600249767303467,
      "learning_rate": 3.805949947684746e-05,
      "loss": 0.6981,
      "step": 1570300
    },
    {
      "epoch": 14.329513103146216,
      "grad_norm": 3.4940686225891113,
      "learning_rate": 3.8058739080711495e-05,
      "loss": 0.6531,
      "step": 1570400
    },
    {
      "epoch": 14.33042557850938,
      "grad_norm": 4.076251983642578,
      "learning_rate": 3.805797868457552e-05,
      "loss": 0.7017,
      "step": 1570500
    },
    {
      "epoch": 14.331338053872546,
      "grad_norm": 3.314361095428467,
      "learning_rate": 3.805721828843955e-05,
      "loss": 0.6929,
      "step": 1570600
    },
    {
      "epoch": 14.332250529235711,
      "grad_norm": 4.193471431732178,
      "learning_rate": 3.805645789230358e-05,
      "loss": 0.6408,
      "step": 1570700
    },
    {
      "epoch": 14.333163004598877,
      "grad_norm": 3.591334342956543,
      "learning_rate": 3.80556974961676e-05,
      "loss": 0.6525,
      "step": 1570800
    },
    {
      "epoch": 14.33407547996204,
      "grad_norm": 4.324670791625977,
      "learning_rate": 3.805493710003163e-05,
      "loss": 0.6882,
      "step": 1570900
    },
    {
      "epoch": 14.334987955325206,
      "grad_norm": 3.2990715503692627,
      "learning_rate": 3.805417670389566e-05,
      "loss": 0.6711,
      "step": 1571000
    },
    {
      "epoch": 14.33590043068837,
      "grad_norm": 3.733705759048462,
      "learning_rate": 3.805341630775969e-05,
      "loss": 0.6694,
      "step": 1571100
    },
    {
      "epoch": 14.336812906051536,
      "grad_norm": 3.24961256980896,
      "learning_rate": 3.805265591162372e-05,
      "loss": 0.6827,
      "step": 1571200
    },
    {
      "epoch": 14.337725381414701,
      "grad_norm": 4.218124866485596,
      "learning_rate": 3.805189551548775e-05,
      "loss": 0.6521,
      "step": 1571300
    },
    {
      "epoch": 14.338637856777867,
      "grad_norm": 4.773294925689697,
      "learning_rate": 3.8051135119351775e-05,
      "loss": 0.6629,
      "step": 1571400
    },
    {
      "epoch": 14.339550332141032,
      "grad_norm": 3.810981035232544,
      "learning_rate": 3.805037472321581e-05,
      "loss": 0.6685,
      "step": 1571500
    },
    {
      "epoch": 14.340462807504197,
      "grad_norm": 3.539830207824707,
      "learning_rate": 3.8049614327079835e-05,
      "loss": 0.6651,
      "step": 1571600
    },
    {
      "epoch": 14.341375282867363,
      "grad_norm": 3.810253858566284,
      "learning_rate": 3.8048853930943865e-05,
      "loss": 0.6506,
      "step": 1571700
    },
    {
      "epoch": 14.342287758230528,
      "grad_norm": 4.807145595550537,
      "learning_rate": 3.8048093534807895e-05,
      "loss": 0.6689,
      "step": 1571800
    },
    {
      "epoch": 14.343200233593693,
      "grad_norm": 4.69027042388916,
      "learning_rate": 3.8047333138671925e-05,
      "loss": 0.6696,
      "step": 1571900
    },
    {
      "epoch": 14.344112708956859,
      "grad_norm": 3.5454161167144775,
      "learning_rate": 3.804657274253595e-05,
      "loss": 0.6767,
      "step": 1572000
    },
    {
      "epoch": 14.345025184320024,
      "grad_norm": 4.034925937652588,
      "learning_rate": 3.8045812346399986e-05,
      "loss": 0.6838,
      "step": 1572100
    },
    {
      "epoch": 14.34593765968319,
      "grad_norm": 4.651001453399658,
      "learning_rate": 3.804505195026401e-05,
      "loss": 0.6698,
      "step": 1572200
    },
    {
      "epoch": 14.346850135046354,
      "grad_norm": 4.84342098236084,
      "learning_rate": 3.804429155412804e-05,
      "loss": 0.6691,
      "step": 1572300
    },
    {
      "epoch": 14.34776261040952,
      "grad_norm": 4.034846782684326,
      "learning_rate": 3.804353115799207e-05,
      "loss": 0.6648,
      "step": 1572400
    },
    {
      "epoch": 14.348675085772683,
      "grad_norm": 3.6956844329833984,
      "learning_rate": 3.80427707618561e-05,
      "loss": 0.6593,
      "step": 1572500
    },
    {
      "epoch": 14.349587561135849,
      "grad_norm": 3.970378875732422,
      "learning_rate": 3.804201036572013e-05,
      "loss": 0.7078,
      "step": 1572600
    },
    {
      "epoch": 14.350500036499014,
      "grad_norm": 3.7264506816864014,
      "learning_rate": 3.804124996958416e-05,
      "loss": 0.6547,
      "step": 1572700
    },
    {
      "epoch": 14.35141251186218,
      "grad_norm": 3.9630320072174072,
      "learning_rate": 3.804048957344818e-05,
      "loss": 0.6595,
      "step": 1572800
    },
    {
      "epoch": 14.352324987225344,
      "grad_norm": 4.142600059509277,
      "learning_rate": 3.803972917731222e-05,
      "loss": 0.661,
      "step": 1572900
    },
    {
      "epoch": 14.35323746258851,
      "grad_norm": 3.6328155994415283,
      "learning_rate": 3.803896878117624e-05,
      "loss": 0.6756,
      "step": 1573000
    },
    {
      "epoch": 14.354149937951675,
      "grad_norm": 4.58073091506958,
      "learning_rate": 3.803820838504027e-05,
      "loss": 0.6593,
      "step": 1573100
    },
    {
      "epoch": 14.35506241331484,
      "grad_norm": 4.498253345489502,
      "learning_rate": 3.80374479889043e-05,
      "loss": 0.7007,
      "step": 1573200
    },
    {
      "epoch": 14.355974888678006,
      "grad_norm": 3.664175271987915,
      "learning_rate": 3.803668759276833e-05,
      "loss": 0.7321,
      "step": 1573300
    },
    {
      "epoch": 14.356887364041171,
      "grad_norm": 3.262486696243286,
      "learning_rate": 3.8035927196632356e-05,
      "loss": 0.6445,
      "step": 1573400
    },
    {
      "epoch": 14.357799839404336,
      "grad_norm": 4.269955635070801,
      "learning_rate": 3.8035166800496386e-05,
      "loss": 0.6885,
      "step": 1573500
    },
    {
      "epoch": 14.358712314767502,
      "grad_norm": 3.688612461090088,
      "learning_rate": 3.8034406404360416e-05,
      "loss": 0.6862,
      "step": 1573600
    },
    {
      "epoch": 14.359624790130667,
      "grad_norm": 3.6695237159729004,
      "learning_rate": 3.8033646008224446e-05,
      "loss": 0.6877,
      "step": 1573700
    },
    {
      "epoch": 14.360537265493832,
      "grad_norm": 3.367466926574707,
      "learning_rate": 3.8032885612088476e-05,
      "loss": 0.6853,
      "step": 1573800
    },
    {
      "epoch": 14.361449740856997,
      "grad_norm": 3.356882333755493,
      "learning_rate": 3.80321252159525e-05,
      "loss": 0.6168,
      "step": 1573900
    },
    {
      "epoch": 14.362362216220163,
      "grad_norm": 4.214877605438232,
      "learning_rate": 3.8031364819816537e-05,
      "loss": 0.6931,
      "step": 1574000
    },
    {
      "epoch": 14.363274691583328,
      "grad_norm": 4.197083473205566,
      "learning_rate": 3.803060442368056e-05,
      "loss": 0.6398,
      "step": 1574100
    },
    {
      "epoch": 14.364187166946493,
      "grad_norm": 3.9602038860321045,
      "learning_rate": 3.802984402754459e-05,
      "loss": 0.6432,
      "step": 1574200
    },
    {
      "epoch": 14.365099642309657,
      "grad_norm": 4.005396366119385,
      "learning_rate": 3.802908363140862e-05,
      "loss": 0.6911,
      "step": 1574300
    },
    {
      "epoch": 14.366012117672822,
      "grad_norm": 4.32087516784668,
      "learning_rate": 3.802832323527265e-05,
      "loss": 0.6772,
      "step": 1574400
    },
    {
      "epoch": 14.366924593035987,
      "grad_norm": 3.7190635204315186,
      "learning_rate": 3.802756283913667e-05,
      "loss": 0.6487,
      "step": 1574500
    },
    {
      "epoch": 14.367837068399153,
      "grad_norm": 4.107353210449219,
      "learning_rate": 3.802680244300071e-05,
      "loss": 0.6735,
      "step": 1574600
    },
    {
      "epoch": 14.368749543762318,
      "grad_norm": 3.9924509525299072,
      "learning_rate": 3.8026042046864733e-05,
      "loss": 0.7008,
      "step": 1574700
    },
    {
      "epoch": 14.369662019125483,
      "grad_norm": 4.260537147521973,
      "learning_rate": 3.8025281650728764e-05,
      "loss": 0.6777,
      "step": 1574800
    },
    {
      "epoch": 14.370574494488649,
      "grad_norm": 3.866556167602539,
      "learning_rate": 3.8024521254592794e-05,
      "loss": 0.7041,
      "step": 1574900
    },
    {
      "epoch": 14.371486969851814,
      "grad_norm": 2.9072604179382324,
      "learning_rate": 3.8023760858456824e-05,
      "loss": 0.6469,
      "step": 1575000
    },
    {
      "epoch": 14.37239944521498,
      "grad_norm": 3.589482069015503,
      "learning_rate": 3.8023000462320854e-05,
      "loss": 0.654,
      "step": 1575100
    },
    {
      "epoch": 14.373311920578145,
      "grad_norm": 3.7465431690216064,
      "learning_rate": 3.8022240066184884e-05,
      "loss": 0.6606,
      "step": 1575200
    },
    {
      "epoch": 14.37422439594131,
      "grad_norm": 4.205102443695068,
      "learning_rate": 3.802147967004891e-05,
      "loss": 0.6711,
      "step": 1575300
    },
    {
      "epoch": 14.375136871304475,
      "grad_norm": 4.042244911193848,
      "learning_rate": 3.8020719273912944e-05,
      "loss": 0.6704,
      "step": 1575400
    },
    {
      "epoch": 14.37604934666764,
      "grad_norm": 4.880221843719482,
      "learning_rate": 3.801995887777697e-05,
      "loss": 0.6907,
      "step": 1575500
    },
    {
      "epoch": 14.376961822030806,
      "grad_norm": 3.851534605026245,
      "learning_rate": 3.8019198481641e-05,
      "loss": 0.7105,
      "step": 1575600
    },
    {
      "epoch": 14.377874297393971,
      "grad_norm": 3.313241481781006,
      "learning_rate": 3.801843808550503e-05,
      "loss": 0.6798,
      "step": 1575700
    },
    {
      "epoch": 14.378786772757136,
      "grad_norm": 4.714477062225342,
      "learning_rate": 3.801767768936906e-05,
      "loss": 0.6645,
      "step": 1575800
    },
    {
      "epoch": 14.3796992481203,
      "grad_norm": 4.105462074279785,
      "learning_rate": 3.801691729323309e-05,
      "loss": 0.6593,
      "step": 1575900
    },
    {
      "epoch": 14.380611723483465,
      "grad_norm": 2.18979811668396,
      "learning_rate": 3.801615689709712e-05,
      "loss": 0.6452,
      "step": 1576000
    },
    {
      "epoch": 14.38152419884663,
      "grad_norm": 3.6925880908966064,
      "learning_rate": 3.801539650096114e-05,
      "loss": 0.6888,
      "step": 1576100
    },
    {
      "epoch": 14.382436674209796,
      "grad_norm": 4.0851054191589355,
      "learning_rate": 3.801463610482517e-05,
      "loss": 0.6559,
      "step": 1576200
    },
    {
      "epoch": 14.383349149572961,
      "grad_norm": 3.3954570293426514,
      "learning_rate": 3.80138757086892e-05,
      "loss": 0.6714,
      "step": 1576300
    },
    {
      "epoch": 14.384261624936126,
      "grad_norm": 4.50442361831665,
      "learning_rate": 3.8013115312553224e-05,
      "loss": 0.7056,
      "step": 1576400
    },
    {
      "epoch": 14.385174100299292,
      "grad_norm": 2.8609371185302734,
      "learning_rate": 3.801235491641726e-05,
      "loss": 0.6233,
      "step": 1576500
    },
    {
      "epoch": 14.386086575662457,
      "grad_norm": 5.0727057456970215,
      "learning_rate": 3.8011594520281284e-05,
      "loss": 0.6844,
      "step": 1576600
    },
    {
      "epoch": 14.386999051025622,
      "grad_norm": 4.192317962646484,
      "learning_rate": 3.8010834124145314e-05,
      "loss": 0.6617,
      "step": 1576700
    },
    {
      "epoch": 14.387911526388788,
      "grad_norm": 4.402007579803467,
      "learning_rate": 3.8010073728009345e-05,
      "loss": 0.6595,
      "step": 1576800
    },
    {
      "epoch": 14.388824001751953,
      "grad_norm": 4.894607067108154,
      "learning_rate": 3.8009313331873375e-05,
      "loss": 0.6731,
      "step": 1576900
    },
    {
      "epoch": 14.389736477115118,
      "grad_norm": 4.821174144744873,
      "learning_rate": 3.8008552935737405e-05,
      "loss": 0.6968,
      "step": 1577000
    },
    {
      "epoch": 14.390648952478283,
      "grad_norm": 4.4747538566589355,
      "learning_rate": 3.8007792539601435e-05,
      "loss": 0.6481,
      "step": 1577100
    },
    {
      "epoch": 14.391561427841449,
      "grad_norm": 4.208845615386963,
      "learning_rate": 3.800703214346546e-05,
      "loss": 0.6598,
      "step": 1577200
    },
    {
      "epoch": 14.392473903204614,
      "grad_norm": 3.0078489780426025,
      "learning_rate": 3.8006271747329495e-05,
      "loss": 0.6687,
      "step": 1577300
    },
    {
      "epoch": 14.39338637856778,
      "grad_norm": 4.218405246734619,
      "learning_rate": 3.800551135119352e-05,
      "loss": 0.6706,
      "step": 1577400
    },
    {
      "epoch": 14.394298853930945,
      "grad_norm": 4.185894012451172,
      "learning_rate": 3.800475095505755e-05,
      "loss": 0.6707,
      "step": 1577500
    },
    {
      "epoch": 14.39521132929411,
      "grad_norm": 2.781749725341797,
      "learning_rate": 3.800399055892158e-05,
      "loss": 0.654,
      "step": 1577600
    },
    {
      "epoch": 14.396123804657273,
      "grad_norm": 4.078127861022949,
      "learning_rate": 3.800323016278561e-05,
      "loss": 0.6565,
      "step": 1577700
    },
    {
      "epoch": 14.397036280020439,
      "grad_norm": 3.554198741912842,
      "learning_rate": 3.800246976664963e-05,
      "loss": 0.6742,
      "step": 1577800
    },
    {
      "epoch": 14.397948755383604,
      "grad_norm": 4.167430877685547,
      "learning_rate": 3.800170937051367e-05,
      "loss": 0.6681,
      "step": 1577900
    },
    {
      "epoch": 14.39886123074677,
      "grad_norm": 3.8445804119110107,
      "learning_rate": 3.800094897437769e-05,
      "loss": 0.7121,
      "step": 1578000
    },
    {
      "epoch": 14.399773706109935,
      "grad_norm": 3.958883762359619,
      "learning_rate": 3.800018857824172e-05,
      "loss": 0.6644,
      "step": 1578100
    },
    {
      "epoch": 14.4006861814731,
      "grad_norm": 3.6175405979156494,
      "learning_rate": 3.799942818210575e-05,
      "loss": 0.6479,
      "step": 1578200
    },
    {
      "epoch": 14.401598656836265,
      "grad_norm": 3.7503514289855957,
      "learning_rate": 3.799866778596978e-05,
      "loss": 0.6649,
      "step": 1578300
    },
    {
      "epoch": 14.40251113219943,
      "grad_norm": 4.4809112548828125,
      "learning_rate": 3.799790738983381e-05,
      "loss": 0.6396,
      "step": 1578400
    },
    {
      "epoch": 14.403423607562596,
      "grad_norm": 3.705120086669922,
      "learning_rate": 3.799714699369784e-05,
      "loss": 0.6915,
      "step": 1578500
    },
    {
      "epoch": 14.404336082925761,
      "grad_norm": 3.9060561656951904,
      "learning_rate": 3.7996386597561865e-05,
      "loss": 0.6498,
      "step": 1578600
    },
    {
      "epoch": 14.405248558288926,
      "grad_norm": 3.952101469039917,
      "learning_rate": 3.79956262014259e-05,
      "loss": 0.6555,
      "step": 1578700
    },
    {
      "epoch": 14.406161033652092,
      "grad_norm": 3.3994972705841064,
      "learning_rate": 3.7994865805289926e-05,
      "loss": 0.6309,
      "step": 1578800
    },
    {
      "epoch": 14.407073509015257,
      "grad_norm": 3.275059938430786,
      "learning_rate": 3.7994105409153956e-05,
      "loss": 0.6822,
      "step": 1578900
    },
    {
      "epoch": 14.407985984378422,
      "grad_norm": 4.770702362060547,
      "learning_rate": 3.7993345013017986e-05,
      "loss": 0.6706,
      "step": 1579000
    },
    {
      "epoch": 14.408898459741588,
      "grad_norm": 4.23703670501709,
      "learning_rate": 3.7992584616882016e-05,
      "loss": 0.7081,
      "step": 1579100
    },
    {
      "epoch": 14.409810935104753,
      "grad_norm": 4.424728870391846,
      "learning_rate": 3.799182422074604e-05,
      "loss": 0.6649,
      "step": 1579200
    },
    {
      "epoch": 14.410723410467916,
      "grad_norm": 4.941779136657715,
      "learning_rate": 3.799106382461007e-05,
      "loss": 0.658,
      "step": 1579300
    },
    {
      "epoch": 14.411635885831082,
      "grad_norm": 4.608896255493164,
      "learning_rate": 3.79903034284741e-05,
      "loss": 0.6539,
      "step": 1579400
    },
    {
      "epoch": 14.412548361194247,
      "grad_norm": 3.993968963623047,
      "learning_rate": 3.798954303233813e-05,
      "loss": 0.7168,
      "step": 1579500
    },
    {
      "epoch": 14.413460836557412,
      "grad_norm": 3.9154622554779053,
      "learning_rate": 3.798878263620216e-05,
      "loss": 0.6454,
      "step": 1579600
    },
    {
      "epoch": 14.414373311920578,
      "grad_norm": 4.487199783325195,
      "learning_rate": 3.798802224006618e-05,
      "loss": 0.663,
      "step": 1579700
    },
    {
      "epoch": 14.415285787283743,
      "grad_norm": 4.3744730949401855,
      "learning_rate": 3.798726184393022e-05,
      "loss": 0.6423,
      "step": 1579800
    },
    {
      "epoch": 14.416198262646908,
      "grad_norm": 3.85457706451416,
      "learning_rate": 3.798650144779424e-05,
      "loss": 0.6969,
      "step": 1579900
    },
    {
      "epoch": 14.417110738010074,
      "grad_norm": 3.4494285583496094,
      "learning_rate": 3.798574105165827e-05,
      "loss": 0.6857,
      "step": 1580000
    },
    {
      "epoch": 14.418023213373239,
      "grad_norm": 4.335450649261475,
      "learning_rate": 3.79849806555223e-05,
      "loss": 0.6662,
      "step": 1580100
    },
    {
      "epoch": 14.418935688736404,
      "grad_norm": 4.301740646362305,
      "learning_rate": 3.798422025938633e-05,
      "loss": 0.701,
      "step": 1580200
    },
    {
      "epoch": 14.41984816409957,
      "grad_norm": 4.073823928833008,
      "learning_rate": 3.7983459863250356e-05,
      "loss": 0.6357,
      "step": 1580300
    },
    {
      "epoch": 14.420760639462735,
      "grad_norm": 4.058257102966309,
      "learning_rate": 3.798269946711439e-05,
      "loss": 0.6643,
      "step": 1580400
    },
    {
      "epoch": 14.4216731148259,
      "grad_norm": 4.227628707885742,
      "learning_rate": 3.7981939070978416e-05,
      "loss": 0.6686,
      "step": 1580500
    },
    {
      "epoch": 14.422585590189065,
      "grad_norm": 3.9716219902038574,
      "learning_rate": 3.7981178674842446e-05,
      "loss": 0.6631,
      "step": 1580600
    },
    {
      "epoch": 14.42349806555223,
      "grad_norm": 5.414939880371094,
      "learning_rate": 3.7980418278706477e-05,
      "loss": 0.6468,
      "step": 1580700
    },
    {
      "epoch": 14.424410540915396,
      "grad_norm": 3.9787614345550537,
      "learning_rate": 3.7979657882570507e-05,
      "loss": 0.6459,
      "step": 1580800
    },
    {
      "epoch": 14.425323016278561,
      "grad_norm": 3.6309964656829834,
      "learning_rate": 3.797889748643454e-05,
      "loss": 0.7001,
      "step": 1580900
    },
    {
      "epoch": 14.426235491641727,
      "grad_norm": 4.092085361480713,
      "learning_rate": 3.797813709029857e-05,
      "loss": 0.6776,
      "step": 1581000
    },
    {
      "epoch": 14.42714796700489,
      "grad_norm": 4.083034515380859,
      "learning_rate": 3.797737669416259e-05,
      "loss": 0.6807,
      "step": 1581100
    },
    {
      "epoch": 14.428060442368055,
      "grad_norm": 3.607135772705078,
      "learning_rate": 3.797661629802663e-05,
      "loss": 0.6816,
      "step": 1581200
    },
    {
      "epoch": 14.42897291773122,
      "grad_norm": 3.5502820014953613,
      "learning_rate": 3.797585590189065e-05,
      "loss": 0.6887,
      "step": 1581300
    },
    {
      "epoch": 14.429885393094386,
      "grad_norm": 3.8845882415771484,
      "learning_rate": 3.797509550575468e-05,
      "loss": 0.6931,
      "step": 1581400
    },
    {
      "epoch": 14.430797868457551,
      "grad_norm": 3.335911512374878,
      "learning_rate": 3.797433510961871e-05,
      "loss": 0.7055,
      "step": 1581500
    },
    {
      "epoch": 14.431710343820717,
      "grad_norm": 3.1556012630462646,
      "learning_rate": 3.797357471348274e-05,
      "loss": 0.6639,
      "step": 1581600
    },
    {
      "epoch": 14.432622819183882,
      "grad_norm": 2.839829444885254,
      "learning_rate": 3.7972814317346764e-05,
      "loss": 0.6364,
      "step": 1581700
    },
    {
      "epoch": 14.433535294547047,
      "grad_norm": 3.849561929702759,
      "learning_rate": 3.79720539212108e-05,
      "loss": 0.6568,
      "step": 1581800
    },
    {
      "epoch": 14.434447769910213,
      "grad_norm": 3.30181884765625,
      "learning_rate": 3.7971293525074824e-05,
      "loss": 0.6285,
      "step": 1581900
    },
    {
      "epoch": 14.435360245273378,
      "grad_norm": 3.563764810562134,
      "learning_rate": 3.7970533128938854e-05,
      "loss": 0.6476,
      "step": 1582000
    },
    {
      "epoch": 14.436272720636543,
      "grad_norm": 4.994283676147461,
      "learning_rate": 3.7969772732802884e-05,
      "loss": 0.6797,
      "step": 1582100
    },
    {
      "epoch": 14.437185195999708,
      "grad_norm": 4.512374401092529,
      "learning_rate": 3.796901233666691e-05,
      "loss": 0.6662,
      "step": 1582200
    },
    {
      "epoch": 14.438097671362874,
      "grad_norm": 3.852283000946045,
      "learning_rate": 3.7968251940530944e-05,
      "loss": 0.631,
      "step": 1582300
    },
    {
      "epoch": 14.439010146726039,
      "grad_norm": 4.869661331176758,
      "learning_rate": 3.796749154439497e-05,
      "loss": 0.6919,
      "step": 1582400
    },
    {
      "epoch": 14.439922622089204,
      "grad_norm": 5.014793872833252,
      "learning_rate": 3.7966731148259e-05,
      "loss": 0.6454,
      "step": 1582500
    },
    {
      "epoch": 14.44083509745237,
      "grad_norm": 3.30234694480896,
      "learning_rate": 3.796597075212303e-05,
      "loss": 0.6763,
      "step": 1582600
    },
    {
      "epoch": 14.441747572815533,
      "grad_norm": 4.091882228851318,
      "learning_rate": 3.796521035598706e-05,
      "loss": 0.6576,
      "step": 1582700
    },
    {
      "epoch": 14.442660048178698,
      "grad_norm": 4.666568279266357,
      "learning_rate": 3.796444995985108e-05,
      "loss": 0.6646,
      "step": 1582800
    },
    {
      "epoch": 14.443572523541864,
      "grad_norm": 3.932260274887085,
      "learning_rate": 3.796368956371512e-05,
      "loss": 0.6592,
      "step": 1582900
    },
    {
      "epoch": 14.444484998905029,
      "grad_norm": 4.431692123413086,
      "learning_rate": 3.796292916757914e-05,
      "loss": 0.6683,
      "step": 1583000
    },
    {
      "epoch": 14.445397474268194,
      "grad_norm": 3.882185220718384,
      "learning_rate": 3.796216877144317e-05,
      "loss": 0.6697,
      "step": 1583100
    },
    {
      "epoch": 14.44630994963136,
      "grad_norm": 4.216055870056152,
      "learning_rate": 3.79614083753072e-05,
      "loss": 0.6377,
      "step": 1583200
    },
    {
      "epoch": 14.447222424994525,
      "grad_norm": 4.554679870605469,
      "learning_rate": 3.796064797917123e-05,
      "loss": 0.6908,
      "step": 1583300
    },
    {
      "epoch": 14.44813490035769,
      "grad_norm": 4.007081985473633,
      "learning_rate": 3.795988758303526e-05,
      "loss": 0.6623,
      "step": 1583400
    },
    {
      "epoch": 14.449047375720856,
      "grad_norm": 3.422881603240967,
      "learning_rate": 3.795912718689929e-05,
      "loss": 0.6482,
      "step": 1583500
    },
    {
      "epoch": 14.44995985108402,
      "grad_norm": 3.853149652481079,
      "learning_rate": 3.7958366790763315e-05,
      "loss": 0.6797,
      "step": 1583600
    },
    {
      "epoch": 14.450872326447186,
      "grad_norm": 3.876326560974121,
      "learning_rate": 3.795760639462735e-05,
      "loss": 0.6886,
      "step": 1583700
    },
    {
      "epoch": 14.451784801810351,
      "grad_norm": 3.8646745681762695,
      "learning_rate": 3.7956845998491375e-05,
      "loss": 0.7121,
      "step": 1583800
    },
    {
      "epoch": 14.452697277173517,
      "grad_norm": 4.649394989013672,
      "learning_rate": 3.7956085602355405e-05,
      "loss": 0.6917,
      "step": 1583900
    },
    {
      "epoch": 14.453609752536682,
      "grad_norm": 4.6287102699279785,
      "learning_rate": 3.7955325206219435e-05,
      "loss": 0.6781,
      "step": 1584000
    },
    {
      "epoch": 14.454522227899847,
      "grad_norm": 3.566082239151001,
      "learning_rate": 3.7954564810083465e-05,
      "loss": 0.7006,
      "step": 1584100
    },
    {
      "epoch": 14.455434703263013,
      "grad_norm": 3.7011077404022217,
      "learning_rate": 3.795380441394749e-05,
      "loss": 0.6546,
      "step": 1584200
    },
    {
      "epoch": 14.456347178626178,
      "grad_norm": 3.9238224029541016,
      "learning_rate": 3.7953044017811525e-05,
      "loss": 0.6807,
      "step": 1584300
    },
    {
      "epoch": 14.457259653989341,
      "grad_norm": 3.4376659393310547,
      "learning_rate": 3.795228362167555e-05,
      "loss": 0.6536,
      "step": 1584400
    },
    {
      "epoch": 14.458172129352507,
      "grad_norm": 4.115895748138428,
      "learning_rate": 3.795152322553958e-05,
      "loss": 0.6643,
      "step": 1584500
    },
    {
      "epoch": 14.459084604715672,
      "grad_norm": 4.259960174560547,
      "learning_rate": 3.795076282940361e-05,
      "loss": 0.6802,
      "step": 1584600
    },
    {
      "epoch": 14.459997080078837,
      "grad_norm": 4.233737468719482,
      "learning_rate": 3.795000243326764e-05,
      "loss": 0.6682,
      "step": 1584700
    },
    {
      "epoch": 14.460909555442003,
      "grad_norm": 4.378249168395996,
      "learning_rate": 3.794924203713167e-05,
      "loss": 0.6696,
      "step": 1584800
    },
    {
      "epoch": 14.461822030805168,
      "grad_norm": 4.051429271697998,
      "learning_rate": 3.794848164099569e-05,
      "loss": 0.671,
      "step": 1584900
    },
    {
      "epoch": 14.462734506168333,
      "grad_norm": 4.365941524505615,
      "learning_rate": 3.794772124485972e-05,
      "loss": 0.6802,
      "step": 1585000
    },
    {
      "epoch": 14.463646981531499,
      "grad_norm": 3.77396297454834,
      "learning_rate": 3.794696084872375e-05,
      "loss": 0.6633,
      "step": 1585100
    },
    {
      "epoch": 14.464559456894664,
      "grad_norm": 3.61263108253479,
      "learning_rate": 3.794620045258778e-05,
      "loss": 0.6868,
      "step": 1585200
    },
    {
      "epoch": 14.46547193225783,
      "grad_norm": 3.9884657859802246,
      "learning_rate": 3.7945440056451805e-05,
      "loss": 0.6418,
      "step": 1585300
    },
    {
      "epoch": 14.466384407620994,
      "grad_norm": 3.6391100883483887,
      "learning_rate": 3.794467966031584e-05,
      "loss": 0.648,
      "step": 1585400
    },
    {
      "epoch": 14.46729688298416,
      "grad_norm": 4.296382904052734,
      "learning_rate": 3.7943919264179866e-05,
      "loss": 0.6724,
      "step": 1585500
    },
    {
      "epoch": 14.468209358347325,
      "grad_norm": 4.724208354949951,
      "learning_rate": 3.7943158868043896e-05,
      "loss": 0.6581,
      "step": 1585600
    },
    {
      "epoch": 14.46912183371049,
      "grad_norm": 3.6616106033325195,
      "learning_rate": 3.7942398471907926e-05,
      "loss": 0.6539,
      "step": 1585700
    },
    {
      "epoch": 14.470034309073656,
      "grad_norm": 4.152658462524414,
      "learning_rate": 3.7941638075771956e-05,
      "loss": 0.6295,
      "step": 1585800
    },
    {
      "epoch": 14.470946784436821,
      "grad_norm": 3.933284282684326,
      "learning_rate": 3.7940877679635986e-05,
      "loss": 0.6538,
      "step": 1585900
    },
    {
      "epoch": 14.471859259799986,
      "grad_norm": 3.1756935119628906,
      "learning_rate": 3.7940117283500016e-05,
      "loss": 0.668,
      "step": 1586000
    },
    {
      "epoch": 14.47277173516315,
      "grad_norm": 3.4805123805999756,
      "learning_rate": 3.793935688736404e-05,
      "loss": 0.6856,
      "step": 1586100
    },
    {
      "epoch": 14.473684210526315,
      "grad_norm": 3.9254372119903564,
      "learning_rate": 3.7938596491228076e-05,
      "loss": 0.6732,
      "step": 1586200
    },
    {
      "epoch": 14.47459668588948,
      "grad_norm": 4.042760372161865,
      "learning_rate": 3.79378360950921e-05,
      "loss": 0.7055,
      "step": 1586300
    },
    {
      "epoch": 14.475509161252646,
      "grad_norm": 4.8195061683654785,
      "learning_rate": 3.793707569895613e-05,
      "loss": 0.628,
      "step": 1586400
    },
    {
      "epoch": 14.476421636615811,
      "grad_norm": 4.189350128173828,
      "learning_rate": 3.793631530282016e-05,
      "loss": 0.6323,
      "step": 1586500
    },
    {
      "epoch": 14.477334111978976,
      "grad_norm": 4.275509834289551,
      "learning_rate": 3.793555490668419e-05,
      "loss": 0.7013,
      "step": 1586600
    },
    {
      "epoch": 14.478246587342142,
      "grad_norm": 3.6100003719329834,
      "learning_rate": 3.793479451054821e-05,
      "loss": 0.6949,
      "step": 1586700
    },
    {
      "epoch": 14.479159062705307,
      "grad_norm": 3.5105786323547363,
      "learning_rate": 3.793403411441225e-05,
      "loss": 0.6188,
      "step": 1586800
    },
    {
      "epoch": 14.480071538068472,
      "grad_norm": 4.2347893714904785,
      "learning_rate": 3.793327371827627e-05,
      "loss": 0.6714,
      "step": 1586900
    },
    {
      "epoch": 14.480984013431637,
      "grad_norm": 3.8563144207000732,
      "learning_rate": 3.79325133221403e-05,
      "loss": 0.6378,
      "step": 1587000
    },
    {
      "epoch": 14.481896488794803,
      "grad_norm": 4.642948627471924,
      "learning_rate": 3.793175292600433e-05,
      "loss": 0.6833,
      "step": 1587100
    },
    {
      "epoch": 14.482808964157968,
      "grad_norm": 3.602581739425659,
      "learning_rate": 3.793099252986836e-05,
      "loss": 0.6335,
      "step": 1587200
    },
    {
      "epoch": 14.483721439521133,
      "grad_norm": 3.018213987350464,
      "learning_rate": 3.793023213373239e-05,
      "loss": 0.6417,
      "step": 1587300
    },
    {
      "epoch": 14.484633914884299,
      "grad_norm": 2.9336471557617188,
      "learning_rate": 3.792947173759642e-05,
      "loss": 0.6573,
      "step": 1587400
    },
    {
      "epoch": 14.485546390247464,
      "grad_norm": 4.074704170227051,
      "learning_rate": 3.7928711341460447e-05,
      "loss": 0.6648,
      "step": 1587500
    },
    {
      "epoch": 14.48645886561063,
      "grad_norm": 4.187012672424316,
      "learning_rate": 3.7927950945324483e-05,
      "loss": 0.6844,
      "step": 1587600
    },
    {
      "epoch": 14.487371340973795,
      "grad_norm": 4.162087917327881,
      "learning_rate": 3.792719054918851e-05,
      "loss": 0.6322,
      "step": 1587700
    },
    {
      "epoch": 14.488283816336958,
      "grad_norm": 4.540057182312012,
      "learning_rate": 3.792643015305254e-05,
      "loss": 0.6567,
      "step": 1587800
    },
    {
      "epoch": 14.489196291700123,
      "grad_norm": 4.144042015075684,
      "learning_rate": 3.792566975691657e-05,
      "loss": 0.6213,
      "step": 1587900
    },
    {
      "epoch": 14.490108767063289,
      "grad_norm": 3.5831918716430664,
      "learning_rate": 3.792490936078059e-05,
      "loss": 0.6695,
      "step": 1588000
    },
    {
      "epoch": 14.491021242426454,
      "grad_norm": 3.283635377883911,
      "learning_rate": 3.792414896464462e-05,
      "loss": 0.6972,
      "step": 1588100
    },
    {
      "epoch": 14.49193371778962,
      "grad_norm": 3.7935001850128174,
      "learning_rate": 3.792338856850865e-05,
      "loss": 0.671,
      "step": 1588200
    },
    {
      "epoch": 14.492846193152785,
      "grad_norm": 3.303330421447754,
      "learning_rate": 3.792262817237268e-05,
      "loss": 0.7099,
      "step": 1588300
    },
    {
      "epoch": 14.49375866851595,
      "grad_norm": 3.7530548572540283,
      "learning_rate": 3.792186777623671e-05,
      "loss": 0.6819,
      "step": 1588400
    },
    {
      "epoch": 14.494671143879115,
      "grad_norm": 3.236762523651123,
      "learning_rate": 3.792110738010074e-05,
      "loss": 0.667,
      "step": 1588500
    },
    {
      "epoch": 14.49558361924228,
      "grad_norm": 4.278419017791748,
      "learning_rate": 3.7920346983964764e-05,
      "loss": 0.67,
      "step": 1588600
    },
    {
      "epoch": 14.496496094605446,
      "grad_norm": 3.4516305923461914,
      "learning_rate": 3.79195865878288e-05,
      "loss": 0.6597,
      "step": 1588700
    },
    {
      "epoch": 14.497408569968611,
      "grad_norm": 4.446235179901123,
      "learning_rate": 3.7918826191692824e-05,
      "loss": 0.6557,
      "step": 1588800
    },
    {
      "epoch": 14.498321045331776,
      "grad_norm": 3.7521302700042725,
      "learning_rate": 3.7918065795556854e-05,
      "loss": 0.6629,
      "step": 1588900
    },
    {
      "epoch": 14.499233520694942,
      "grad_norm": 3.791552782058716,
      "learning_rate": 3.7917305399420884e-05,
      "loss": 0.6994,
      "step": 1589000
    },
    {
      "epoch": 14.500145996058107,
      "grad_norm": 3.0660042762756348,
      "learning_rate": 3.7916545003284914e-05,
      "loss": 0.6846,
      "step": 1589100
    },
    {
      "epoch": 14.501058471421272,
      "grad_norm": 3.7088892459869385,
      "learning_rate": 3.7915784607148944e-05,
      "loss": 0.6904,
      "step": 1589200
    },
    {
      "epoch": 14.501970946784438,
      "grad_norm": 3.9901223182678223,
      "learning_rate": 3.7915024211012974e-05,
      "loss": 0.6361,
      "step": 1589300
    },
    {
      "epoch": 14.502883422147603,
      "grad_norm": 3.523432970046997,
      "learning_rate": 3.7914263814877e-05,
      "loss": 0.6398,
      "step": 1589400
    },
    {
      "epoch": 14.503795897510766,
      "grad_norm": 3.8898768424987793,
      "learning_rate": 3.7913503418741034e-05,
      "loss": 0.6814,
      "step": 1589500
    },
    {
      "epoch": 14.504708372873932,
      "grad_norm": 4.128444194793701,
      "learning_rate": 3.791274302260506e-05,
      "loss": 0.6949,
      "step": 1589600
    },
    {
      "epoch": 14.505620848237097,
      "grad_norm": 4.30138635635376,
      "learning_rate": 3.791198262646909e-05,
      "loss": 0.6599,
      "step": 1589700
    },
    {
      "epoch": 14.506533323600262,
      "grad_norm": 4.450199604034424,
      "learning_rate": 3.791122223033312e-05,
      "loss": 0.6418,
      "step": 1589800
    },
    {
      "epoch": 14.507445798963428,
      "grad_norm": 3.612410545349121,
      "learning_rate": 3.791046183419715e-05,
      "loss": 0.646,
      "step": 1589900
    },
    {
      "epoch": 14.508358274326593,
      "grad_norm": 3.578022003173828,
      "learning_rate": 3.790970143806117e-05,
      "loss": 0.6796,
      "step": 1590000
    },
    {
      "epoch": 14.509270749689758,
      "grad_norm": 3.945796012878418,
      "learning_rate": 3.790894104192521e-05,
      "loss": 0.6549,
      "step": 1590100
    },
    {
      "epoch": 14.510183225052923,
      "grad_norm": 4.179947376251221,
      "learning_rate": 3.790818064578923e-05,
      "loss": 0.6765,
      "step": 1590200
    },
    {
      "epoch": 14.511095700416089,
      "grad_norm": 4.727480888366699,
      "learning_rate": 3.790742024965326e-05,
      "loss": 0.6772,
      "step": 1590300
    },
    {
      "epoch": 14.512008175779254,
      "grad_norm": 3.588331937789917,
      "learning_rate": 3.790665985351729e-05,
      "loss": 0.6676,
      "step": 1590400
    },
    {
      "epoch": 14.51292065114242,
      "grad_norm": 4.256394386291504,
      "learning_rate": 3.7905899457381315e-05,
      "loss": 0.6397,
      "step": 1590500
    },
    {
      "epoch": 14.513833126505585,
      "grad_norm": 5.001105785369873,
      "learning_rate": 3.790513906124535e-05,
      "loss": 0.6733,
      "step": 1590600
    },
    {
      "epoch": 14.51474560186875,
      "grad_norm": 4.28959321975708,
      "learning_rate": 3.7904378665109375e-05,
      "loss": 0.6715,
      "step": 1590700
    },
    {
      "epoch": 14.515658077231915,
      "grad_norm": 3.4828217029571533,
      "learning_rate": 3.7903618268973405e-05,
      "loss": 0.6862,
      "step": 1590800
    },
    {
      "epoch": 14.51657055259508,
      "grad_norm": 4.096951007843018,
      "learning_rate": 3.7902857872837435e-05,
      "loss": 0.7202,
      "step": 1590900
    },
    {
      "epoch": 14.517483027958246,
      "grad_norm": 4.171867847442627,
      "learning_rate": 3.7902097476701465e-05,
      "loss": 0.6553,
      "step": 1591000
    },
    {
      "epoch": 14.518395503321411,
      "grad_norm": 3.7307486534118652,
      "learning_rate": 3.790133708056549e-05,
      "loss": 0.6295,
      "step": 1591100
    },
    {
      "epoch": 14.519307978684576,
      "grad_norm": 4.205406665802002,
      "learning_rate": 3.7900576684429525e-05,
      "loss": 0.6973,
      "step": 1591200
    },
    {
      "epoch": 14.52022045404774,
      "grad_norm": 3.942307949066162,
      "learning_rate": 3.789981628829355e-05,
      "loss": 0.6888,
      "step": 1591300
    },
    {
      "epoch": 14.521132929410905,
      "grad_norm": 3.779014825820923,
      "learning_rate": 3.789905589215758e-05,
      "loss": 0.7081,
      "step": 1591400
    },
    {
      "epoch": 14.52204540477407,
      "grad_norm": 3.9463744163513184,
      "learning_rate": 3.789829549602161e-05,
      "loss": 0.6335,
      "step": 1591500
    },
    {
      "epoch": 14.522957880137236,
      "grad_norm": 4.164280891418457,
      "learning_rate": 3.789753509988564e-05,
      "loss": 0.6594,
      "step": 1591600
    },
    {
      "epoch": 14.523870355500401,
      "grad_norm": 4.115935325622559,
      "learning_rate": 3.789677470374967e-05,
      "loss": 0.6634,
      "step": 1591700
    },
    {
      "epoch": 14.524782830863566,
      "grad_norm": 4.083006381988525,
      "learning_rate": 3.78960143076137e-05,
      "loss": 0.6381,
      "step": 1591800
    },
    {
      "epoch": 14.525695306226732,
      "grad_norm": 3.7718074321746826,
      "learning_rate": 3.789525391147772e-05,
      "loss": 0.6455,
      "step": 1591900
    },
    {
      "epoch": 14.526607781589897,
      "grad_norm": 3.8716840744018555,
      "learning_rate": 3.789449351534176e-05,
      "loss": 0.6594,
      "step": 1592000
    },
    {
      "epoch": 14.527520256953062,
      "grad_norm": 3.8097641468048096,
      "learning_rate": 3.789373311920578e-05,
      "loss": 0.66,
      "step": 1592100
    },
    {
      "epoch": 14.528432732316228,
      "grad_norm": 4.621530532836914,
      "learning_rate": 3.789297272306981e-05,
      "loss": 0.6819,
      "step": 1592200
    },
    {
      "epoch": 14.529345207679393,
      "grad_norm": 3.6402714252471924,
      "learning_rate": 3.789221232693384e-05,
      "loss": 0.6448,
      "step": 1592300
    },
    {
      "epoch": 14.530257683042558,
      "grad_norm": 4.955220699310303,
      "learning_rate": 3.789145193079787e-05,
      "loss": 0.6785,
      "step": 1592400
    },
    {
      "epoch": 14.531170158405724,
      "grad_norm": 4.220139503479004,
      "learning_rate": 3.7890691534661896e-05,
      "loss": 0.6701,
      "step": 1592500
    },
    {
      "epoch": 14.532082633768889,
      "grad_norm": 3.650895595550537,
      "learning_rate": 3.788993113852593e-05,
      "loss": 0.6669,
      "step": 1592600
    },
    {
      "epoch": 14.532995109132054,
      "grad_norm": 3.6401467323303223,
      "learning_rate": 3.7889170742389956e-05,
      "loss": 0.6842,
      "step": 1592700
    },
    {
      "epoch": 14.533907584495218,
      "grad_norm": 3.9583072662353516,
      "learning_rate": 3.7888410346253986e-05,
      "loss": 0.6518,
      "step": 1592800
    },
    {
      "epoch": 14.534820059858383,
      "grad_norm": 3.4219131469726562,
      "learning_rate": 3.7887649950118016e-05,
      "loss": 0.682,
      "step": 1592900
    },
    {
      "epoch": 14.535732535221548,
      "grad_norm": 4.438116550445557,
      "learning_rate": 3.7886889553982046e-05,
      "loss": 0.6769,
      "step": 1593000
    },
    {
      "epoch": 14.536645010584714,
      "grad_norm": 4.301818370819092,
      "learning_rate": 3.7886129157846076e-05,
      "loss": 0.7021,
      "step": 1593100
    },
    {
      "epoch": 14.537557485947879,
      "grad_norm": 3.9893555641174316,
      "learning_rate": 3.7885368761710106e-05,
      "loss": 0.6597,
      "step": 1593200
    },
    {
      "epoch": 14.538469961311044,
      "grad_norm": 5.322585105895996,
      "learning_rate": 3.788460836557413e-05,
      "loss": 0.6787,
      "step": 1593300
    },
    {
      "epoch": 14.53938243667421,
      "grad_norm": 3.4849894046783447,
      "learning_rate": 3.788384796943816e-05,
      "loss": 0.6924,
      "step": 1593400
    },
    {
      "epoch": 14.540294912037375,
      "grad_norm": 3.598522424697876,
      "learning_rate": 3.788308757330219e-05,
      "loss": 0.6544,
      "step": 1593500
    },
    {
      "epoch": 14.54120738740054,
      "grad_norm": 4.598249912261963,
      "learning_rate": 3.788232717716621e-05,
      "loss": 0.6634,
      "step": 1593600
    },
    {
      "epoch": 14.542119862763705,
      "grad_norm": 3.357581615447998,
      "learning_rate": 3.788156678103025e-05,
      "loss": 0.7091,
      "step": 1593700
    },
    {
      "epoch": 14.54303233812687,
      "grad_norm": 4.63197135925293,
      "learning_rate": 3.788080638489427e-05,
      "loss": 0.6728,
      "step": 1593800
    },
    {
      "epoch": 14.543944813490036,
      "grad_norm": 4.2257280349731445,
      "learning_rate": 3.78800459887583e-05,
      "loss": 0.6405,
      "step": 1593900
    },
    {
      "epoch": 14.544857288853201,
      "grad_norm": 4.454439163208008,
      "learning_rate": 3.787928559262233e-05,
      "loss": 0.6589,
      "step": 1594000
    },
    {
      "epoch": 14.545769764216367,
      "grad_norm": 2.6971757411956787,
      "learning_rate": 3.787852519648636e-05,
      "loss": 0.6949,
      "step": 1594100
    },
    {
      "epoch": 14.546682239579532,
      "grad_norm": 3.9871714115142822,
      "learning_rate": 3.787776480035039e-05,
      "loss": 0.6499,
      "step": 1594200
    },
    {
      "epoch": 14.547594714942697,
      "grad_norm": 3.141634464263916,
      "learning_rate": 3.787700440421442e-05,
      "loss": 0.6645,
      "step": 1594300
    },
    {
      "epoch": 14.548507190305862,
      "grad_norm": 4.069398880004883,
      "learning_rate": 3.787624400807845e-05,
      "loss": 0.6838,
      "step": 1594400
    },
    {
      "epoch": 14.549419665669028,
      "grad_norm": 2.9403493404388428,
      "learning_rate": 3.7875483611942483e-05,
      "loss": 0.6491,
      "step": 1594500
    },
    {
      "epoch": 14.550332141032191,
      "grad_norm": 3.742116928100586,
      "learning_rate": 3.787472321580651e-05,
      "loss": 0.6679,
      "step": 1594600
    },
    {
      "epoch": 14.551244616395357,
      "grad_norm": 4.73136568069458,
      "learning_rate": 3.787396281967054e-05,
      "loss": 0.7223,
      "step": 1594700
    },
    {
      "epoch": 14.552157091758522,
      "grad_norm": 3.5216825008392334,
      "learning_rate": 3.787320242353457e-05,
      "loss": 0.6979,
      "step": 1594800
    },
    {
      "epoch": 14.553069567121687,
      "grad_norm": 4.360960483551025,
      "learning_rate": 3.78724420273986e-05,
      "loss": 0.629,
      "step": 1594900
    },
    {
      "epoch": 14.553982042484852,
      "grad_norm": 4.775922775268555,
      "learning_rate": 3.787168163126262e-05,
      "loss": 0.6883,
      "step": 1595000
    },
    {
      "epoch": 14.554894517848018,
      "grad_norm": 4.198147296905518,
      "learning_rate": 3.787092123512666e-05,
      "loss": 0.6868,
      "step": 1595100
    },
    {
      "epoch": 14.555806993211183,
      "grad_norm": 4.079077243804932,
      "learning_rate": 3.787016083899068e-05,
      "loss": 0.6728,
      "step": 1595200
    },
    {
      "epoch": 14.556719468574348,
      "grad_norm": 3.336028814315796,
      "learning_rate": 3.786940044285471e-05,
      "loss": 0.6947,
      "step": 1595300
    },
    {
      "epoch": 14.557631943937514,
      "grad_norm": 4.603262901306152,
      "learning_rate": 3.786864004671874e-05,
      "loss": 0.6795,
      "step": 1595400
    },
    {
      "epoch": 14.558544419300679,
      "grad_norm": 3.3676302433013916,
      "learning_rate": 3.786787965058277e-05,
      "loss": 0.6857,
      "step": 1595500
    },
    {
      "epoch": 14.559456894663844,
      "grad_norm": 4.431982517242432,
      "learning_rate": 3.78671192544468e-05,
      "loss": 0.6849,
      "step": 1595600
    },
    {
      "epoch": 14.56036937002701,
      "grad_norm": 2.2692909240722656,
      "learning_rate": 3.786635885831083e-05,
      "loss": 0.6675,
      "step": 1595700
    },
    {
      "epoch": 14.561281845390175,
      "grad_norm": 3.824772357940674,
      "learning_rate": 3.7865598462174854e-05,
      "loss": 0.6321,
      "step": 1595800
    },
    {
      "epoch": 14.56219432075334,
      "grad_norm": 4.253166198730469,
      "learning_rate": 3.786483806603889e-05,
      "loss": 0.6553,
      "step": 1595900
    },
    {
      "epoch": 14.563106796116505,
      "grad_norm": 4.823853969573975,
      "learning_rate": 3.7864077669902914e-05,
      "loss": 0.6567,
      "step": 1596000
    },
    {
      "epoch": 14.56401927147967,
      "grad_norm": 3.863379955291748,
      "learning_rate": 3.7863317273766944e-05,
      "loss": 0.6313,
      "step": 1596100
    },
    {
      "epoch": 14.564931746842834,
      "grad_norm": 5.202107906341553,
      "learning_rate": 3.7862556877630974e-05,
      "loss": 0.6467,
      "step": 1596200
    },
    {
      "epoch": 14.565844222206,
      "grad_norm": 3.0743143558502197,
      "learning_rate": 3.7861796481495e-05,
      "loss": 0.6454,
      "step": 1596300
    },
    {
      "epoch": 14.566756697569165,
      "grad_norm": 3.9388256072998047,
      "learning_rate": 3.786103608535903e-05,
      "loss": 0.6791,
      "step": 1596400
    },
    {
      "epoch": 14.56766917293233,
      "grad_norm": 4.260994911193848,
      "learning_rate": 3.786027568922306e-05,
      "loss": 0.7006,
      "step": 1596500
    },
    {
      "epoch": 14.568581648295496,
      "grad_norm": 4.472820281982422,
      "learning_rate": 3.785951529308709e-05,
      "loss": 0.671,
      "step": 1596600
    },
    {
      "epoch": 14.56949412365866,
      "grad_norm": 5.557414531707764,
      "learning_rate": 3.785875489695112e-05,
      "loss": 0.6623,
      "step": 1596700
    },
    {
      "epoch": 14.570406599021826,
      "grad_norm": 3.4287424087524414,
      "learning_rate": 3.785799450081515e-05,
      "loss": 0.631,
      "step": 1596800
    },
    {
      "epoch": 14.571319074384991,
      "grad_norm": 4.740298748016357,
      "learning_rate": 3.785723410467917e-05,
      "loss": 0.6889,
      "step": 1596900
    },
    {
      "epoch": 14.572231549748157,
      "grad_norm": 4.128536701202393,
      "learning_rate": 3.785647370854321e-05,
      "loss": 0.6447,
      "step": 1597000
    },
    {
      "epoch": 14.573144025111322,
      "grad_norm": 4.153923034667969,
      "learning_rate": 3.785571331240723e-05,
      "loss": 0.6515,
      "step": 1597100
    },
    {
      "epoch": 14.574056500474487,
      "grad_norm": 3.8141157627105713,
      "learning_rate": 3.785495291627126e-05,
      "loss": 0.6897,
      "step": 1597200
    },
    {
      "epoch": 14.574968975837653,
      "grad_norm": 3.9340105056762695,
      "learning_rate": 3.785419252013529e-05,
      "loss": 0.6792,
      "step": 1597300
    },
    {
      "epoch": 14.575881451200818,
      "grad_norm": 3.338712692260742,
      "learning_rate": 3.785343212399932e-05,
      "loss": 0.6686,
      "step": 1597400
    },
    {
      "epoch": 14.576793926563983,
      "grad_norm": 4.430739402770996,
      "learning_rate": 3.7852671727863345e-05,
      "loss": 0.6708,
      "step": 1597500
    },
    {
      "epoch": 14.577706401927149,
      "grad_norm": 5.744380950927734,
      "learning_rate": 3.785191133172738e-05,
      "loss": 0.6591,
      "step": 1597600
    },
    {
      "epoch": 14.578618877290314,
      "grad_norm": 3.9135797023773193,
      "learning_rate": 3.7851150935591405e-05,
      "loss": 0.6709,
      "step": 1597700
    },
    {
      "epoch": 14.579531352653479,
      "grad_norm": 3.890324831008911,
      "learning_rate": 3.7850390539455435e-05,
      "loss": 0.6839,
      "step": 1597800
    },
    {
      "epoch": 14.580443828016644,
      "grad_norm": 4.150966644287109,
      "learning_rate": 3.7849630143319465e-05,
      "loss": 0.6534,
      "step": 1597900
    },
    {
      "epoch": 14.581356303379808,
      "grad_norm": 4.202001094818115,
      "learning_rate": 3.7848869747183495e-05,
      "loss": 0.6743,
      "step": 1598000
    },
    {
      "epoch": 14.582268778742973,
      "grad_norm": 3.0750677585601807,
      "learning_rate": 3.7848109351047525e-05,
      "loss": 0.6504,
      "step": 1598100
    },
    {
      "epoch": 14.583181254106139,
      "grad_norm": 4.339487075805664,
      "learning_rate": 3.7847348954911555e-05,
      "loss": 0.6736,
      "step": 1598200
    },
    {
      "epoch": 14.584093729469304,
      "grad_norm": 3.3493759632110596,
      "learning_rate": 3.784658855877558e-05,
      "loss": 0.6395,
      "step": 1598300
    },
    {
      "epoch": 14.58500620483247,
      "grad_norm": 3.9475865364074707,
      "learning_rate": 3.7845828162639615e-05,
      "loss": 0.655,
      "step": 1598400
    },
    {
      "epoch": 14.585918680195634,
      "grad_norm": 3.822533130645752,
      "learning_rate": 3.784506776650364e-05,
      "loss": 0.6275,
      "step": 1598500
    },
    {
      "epoch": 14.5868311555588,
      "grad_norm": 2.753106117248535,
      "learning_rate": 3.784430737036767e-05,
      "loss": 0.6579,
      "step": 1598600
    },
    {
      "epoch": 14.587743630921965,
      "grad_norm": 2.9666926860809326,
      "learning_rate": 3.78435469742317e-05,
      "loss": 0.6586,
      "step": 1598700
    },
    {
      "epoch": 14.58865610628513,
      "grad_norm": 4.220024585723877,
      "learning_rate": 3.784278657809573e-05,
      "loss": 0.6826,
      "step": 1598800
    },
    {
      "epoch": 14.589568581648296,
      "grad_norm": 4.187573432922363,
      "learning_rate": 3.784202618195975e-05,
      "loss": 0.692,
      "step": 1598900
    },
    {
      "epoch": 14.590481057011461,
      "grad_norm": 4.7461066246032715,
      "learning_rate": 3.784126578582379e-05,
      "loss": 0.7106,
      "step": 1599000
    },
    {
      "epoch": 14.591393532374626,
      "grad_norm": 4.351525783538818,
      "learning_rate": 3.784050538968781e-05,
      "loss": 0.6411,
      "step": 1599100
    },
    {
      "epoch": 14.592306007737792,
      "grad_norm": 4.181542873382568,
      "learning_rate": 3.783974499355184e-05,
      "loss": 0.6408,
      "step": 1599200
    },
    {
      "epoch": 14.593218483100957,
      "grad_norm": 3.47521710395813,
      "learning_rate": 3.783898459741587e-05,
      "loss": 0.6476,
      "step": 1599300
    },
    {
      "epoch": 14.594130958464122,
      "grad_norm": 3.478114604949951,
      "learning_rate": 3.7838224201279896e-05,
      "loss": 0.6668,
      "step": 1599400
    },
    {
      "epoch": 14.595043433827287,
      "grad_norm": 3.8435652256011963,
      "learning_rate": 3.783746380514393e-05,
      "loss": 0.7049,
      "step": 1599500
    },
    {
      "epoch": 14.595955909190451,
      "grad_norm": 3.3301568031311035,
      "learning_rate": 3.7836703409007956e-05,
      "loss": 0.6458,
      "step": 1599600
    },
    {
      "epoch": 14.596868384553616,
      "grad_norm": 4.423928260803223,
      "learning_rate": 3.7835943012871986e-05,
      "loss": 0.6449,
      "step": 1599700
    },
    {
      "epoch": 14.597780859916782,
      "grad_norm": 3.727562427520752,
      "learning_rate": 3.7835182616736016e-05,
      "loss": 0.6891,
      "step": 1599800
    },
    {
      "epoch": 14.598693335279947,
      "grad_norm": 3.7932000160217285,
      "learning_rate": 3.7834422220600046e-05,
      "loss": 0.7004,
      "step": 1599900
    },
    {
      "epoch": 14.599605810643112,
      "grad_norm": 4.202236175537109,
      "learning_rate": 3.783366182446407e-05,
      "loss": 0.6972,
      "step": 1600000
    },
    {
      "epoch": 14.600518286006277,
      "grad_norm": 4.1209282875061035,
      "learning_rate": 3.7832901428328106e-05,
      "loss": 0.6483,
      "step": 1600100
    },
    {
      "epoch": 14.601430761369443,
      "grad_norm": 3.7532577514648438,
      "learning_rate": 3.783214103219213e-05,
      "loss": 0.6705,
      "step": 1600200
    },
    {
      "epoch": 14.602343236732608,
      "grad_norm": 4.450897216796875,
      "learning_rate": 3.783138063605616e-05,
      "loss": 0.6804,
      "step": 1600300
    },
    {
      "epoch": 14.603255712095773,
      "grad_norm": 3.2948217391967773,
      "learning_rate": 3.783062023992019e-05,
      "loss": 0.6675,
      "step": 1600400
    },
    {
      "epoch": 14.604168187458939,
      "grad_norm": 5.153680324554443,
      "learning_rate": 3.782985984378422e-05,
      "loss": 0.6836,
      "step": 1600500
    },
    {
      "epoch": 14.605080662822104,
      "grad_norm": 3.7383601665496826,
      "learning_rate": 3.782909944764825e-05,
      "loss": 0.6623,
      "step": 1600600
    },
    {
      "epoch": 14.60599313818527,
      "grad_norm": 3.485119104385376,
      "learning_rate": 3.782833905151228e-05,
      "loss": 0.6778,
      "step": 1600700
    },
    {
      "epoch": 14.606905613548435,
      "grad_norm": 4.622981071472168,
      "learning_rate": 3.78275786553763e-05,
      "loss": 0.6546,
      "step": 1600800
    },
    {
      "epoch": 14.6078180889116,
      "grad_norm": 4.037214756011963,
      "learning_rate": 3.782681825924034e-05,
      "loss": 0.6731,
      "step": 1600900
    },
    {
      "epoch": 14.608730564274765,
      "grad_norm": 4.015850067138672,
      "learning_rate": 3.782605786310436e-05,
      "loss": 0.6693,
      "step": 1601000
    },
    {
      "epoch": 14.60964303963793,
      "grad_norm": 3.2707443237304688,
      "learning_rate": 3.7825297466968393e-05,
      "loss": 0.7126,
      "step": 1601100
    },
    {
      "epoch": 14.610555515001096,
      "grad_norm": 3.929255247116089,
      "learning_rate": 3.7824537070832423e-05,
      "loss": 0.692,
      "step": 1601200
    },
    {
      "epoch": 14.611467990364261,
      "grad_norm": 4.1496686935424805,
      "learning_rate": 3.7823776674696454e-05,
      "loss": 0.6834,
      "step": 1601300
    },
    {
      "epoch": 14.612380465727425,
      "grad_norm": 3.4922478199005127,
      "learning_rate": 3.7823016278560484e-05,
      "loss": 0.637,
      "step": 1601400
    },
    {
      "epoch": 14.61329294109059,
      "grad_norm": 3.867856502532959,
      "learning_rate": 3.7822255882424514e-05,
      "loss": 0.6586,
      "step": 1601500
    },
    {
      "epoch": 14.614205416453755,
      "grad_norm": 4.305459499359131,
      "learning_rate": 3.782149548628854e-05,
      "loss": 0.6947,
      "step": 1601600
    },
    {
      "epoch": 14.61511789181692,
      "grad_norm": 4.258346080780029,
      "learning_rate": 3.782073509015257e-05,
      "loss": 0.6939,
      "step": 1601700
    },
    {
      "epoch": 14.616030367180086,
      "grad_norm": 4.163398265838623,
      "learning_rate": 3.78199746940166e-05,
      "loss": 0.6794,
      "step": 1601800
    },
    {
      "epoch": 14.616942842543251,
      "grad_norm": 2.718964099884033,
      "learning_rate": 3.781921429788062e-05,
      "loss": 0.6782,
      "step": 1601900
    },
    {
      "epoch": 14.617855317906416,
      "grad_norm": 4.306397438049316,
      "learning_rate": 3.781845390174466e-05,
      "loss": 0.6571,
      "step": 1602000
    },
    {
      "epoch": 14.618767793269582,
      "grad_norm": 4.567053318023682,
      "learning_rate": 3.781769350560868e-05,
      "loss": 0.6883,
      "step": 1602100
    },
    {
      "epoch": 14.619680268632747,
      "grad_norm": 3.534727096557617,
      "learning_rate": 3.781693310947271e-05,
      "loss": 0.6766,
      "step": 1602200
    },
    {
      "epoch": 14.620592743995912,
      "grad_norm": 4.2281494140625,
      "learning_rate": 3.781617271333674e-05,
      "loss": 0.6645,
      "step": 1602300
    },
    {
      "epoch": 14.621505219359078,
      "grad_norm": 3.615365743637085,
      "learning_rate": 3.781541231720077e-05,
      "loss": 0.663,
      "step": 1602400
    },
    {
      "epoch": 14.622417694722243,
      "grad_norm": 3.2071340084075928,
      "learning_rate": 3.78146519210648e-05,
      "loss": 0.6534,
      "step": 1602500
    },
    {
      "epoch": 14.623330170085408,
      "grad_norm": 3.9848222732543945,
      "learning_rate": 3.781389152492883e-05,
      "loss": 0.6912,
      "step": 1602600
    },
    {
      "epoch": 14.624242645448573,
      "grad_norm": 3.8614869117736816,
      "learning_rate": 3.7813131128792854e-05,
      "loss": 0.6408,
      "step": 1602700
    },
    {
      "epoch": 14.625155120811739,
      "grad_norm": 3.579225778579712,
      "learning_rate": 3.781237073265689e-05,
      "loss": 0.6821,
      "step": 1602800
    },
    {
      "epoch": 14.626067596174904,
      "grad_norm": 3.8564701080322266,
      "learning_rate": 3.7811610336520914e-05,
      "loss": 0.6675,
      "step": 1602900
    },
    {
      "epoch": 14.626980071538068,
      "grad_norm": 3.9091458320617676,
      "learning_rate": 3.7810849940384944e-05,
      "loss": 0.6379,
      "step": 1603000
    },
    {
      "epoch": 14.627892546901233,
      "grad_norm": 4.307199478149414,
      "learning_rate": 3.7810089544248974e-05,
      "loss": 0.6828,
      "step": 1603100
    },
    {
      "epoch": 14.628805022264398,
      "grad_norm": 4.562722206115723,
      "learning_rate": 3.7809329148113004e-05,
      "loss": 0.6799,
      "step": 1603200
    },
    {
      "epoch": 14.629717497627563,
      "grad_norm": 4.709197521209717,
      "learning_rate": 3.780856875197703e-05,
      "loss": 0.6662,
      "step": 1603300
    },
    {
      "epoch": 14.630629972990729,
      "grad_norm": 4.6647748947143555,
      "learning_rate": 3.7807808355841065e-05,
      "loss": 0.6731,
      "step": 1603400
    },
    {
      "epoch": 14.631542448353894,
      "grad_norm": 3.4360601902008057,
      "learning_rate": 3.780704795970509e-05,
      "loss": 0.6894,
      "step": 1603500
    },
    {
      "epoch": 14.63245492371706,
      "grad_norm": 4.046502590179443,
      "learning_rate": 3.780628756356912e-05,
      "loss": 0.7099,
      "step": 1603600
    },
    {
      "epoch": 14.633367399080225,
      "grad_norm": 3.354933023452759,
      "learning_rate": 3.780552716743315e-05,
      "loss": 0.717,
      "step": 1603700
    },
    {
      "epoch": 14.63427987444339,
      "grad_norm": 3.212674856185913,
      "learning_rate": 3.780476677129718e-05,
      "loss": 0.6762,
      "step": 1603800
    },
    {
      "epoch": 14.635192349806555,
      "grad_norm": 3.917604923248291,
      "learning_rate": 3.780400637516121e-05,
      "loss": 0.6422,
      "step": 1603900
    },
    {
      "epoch": 14.63610482516972,
      "grad_norm": 3.6690425872802734,
      "learning_rate": 3.780324597902524e-05,
      "loss": 0.6761,
      "step": 1604000
    },
    {
      "epoch": 14.637017300532886,
      "grad_norm": 4.119565963745117,
      "learning_rate": 3.780248558288926e-05,
      "loss": 0.6873,
      "step": 1604100
    },
    {
      "epoch": 14.637929775896051,
      "grad_norm": 3.5677826404571533,
      "learning_rate": 3.78017251867533e-05,
      "loss": 0.6734,
      "step": 1604200
    },
    {
      "epoch": 14.638842251259216,
      "grad_norm": 3.8884029388427734,
      "learning_rate": 3.780096479061732e-05,
      "loss": 0.6751,
      "step": 1604300
    },
    {
      "epoch": 14.639754726622382,
      "grad_norm": 4.3453369140625,
      "learning_rate": 3.780020439448135e-05,
      "loss": 0.6565,
      "step": 1604400
    },
    {
      "epoch": 14.640667201985547,
      "grad_norm": 4.473058223724365,
      "learning_rate": 3.779944399834538e-05,
      "loss": 0.6845,
      "step": 1604500
    },
    {
      "epoch": 14.641579677348712,
      "grad_norm": 4.186975479125977,
      "learning_rate": 3.779868360220941e-05,
      "loss": 0.6665,
      "step": 1604600
    },
    {
      "epoch": 14.642492152711878,
      "grad_norm": 4.964690685272217,
      "learning_rate": 3.7797923206073435e-05,
      "loss": 0.6653,
      "step": 1604700
    },
    {
      "epoch": 14.643404628075041,
      "grad_norm": 4.307621955871582,
      "learning_rate": 3.7797162809937465e-05,
      "loss": 0.6998,
      "step": 1604800
    },
    {
      "epoch": 14.644317103438206,
      "grad_norm": 3.3124637603759766,
      "learning_rate": 3.7796402413801495e-05,
      "loss": 0.6482,
      "step": 1604900
    },
    {
      "epoch": 14.645229578801372,
      "grad_norm": 3.728660821914673,
      "learning_rate": 3.7795642017665525e-05,
      "loss": 0.6758,
      "step": 1605000
    },
    {
      "epoch": 14.646142054164537,
      "grad_norm": 3.6042327880859375,
      "learning_rate": 3.7794881621529555e-05,
      "loss": 0.6694,
      "step": 1605100
    },
    {
      "epoch": 14.647054529527702,
      "grad_norm": 3.751636505126953,
      "learning_rate": 3.779412122539358e-05,
      "loss": 0.6832,
      "step": 1605200
    },
    {
      "epoch": 14.647967004890868,
      "grad_norm": 3.7338757514953613,
      "learning_rate": 3.7793360829257616e-05,
      "loss": 0.6327,
      "step": 1605300
    },
    {
      "epoch": 14.648879480254033,
      "grad_norm": 3.948436737060547,
      "learning_rate": 3.779260043312164e-05,
      "loss": 0.702,
      "step": 1605400
    },
    {
      "epoch": 14.649791955617198,
      "grad_norm": 3.325956344604492,
      "learning_rate": 3.779184003698567e-05,
      "loss": 0.682,
      "step": 1605500
    },
    {
      "epoch": 14.650704430980364,
      "grad_norm": 3.349609613418579,
      "learning_rate": 3.77910796408497e-05,
      "loss": 0.666,
      "step": 1605600
    },
    {
      "epoch": 14.651616906343529,
      "grad_norm": 3.314232110977173,
      "learning_rate": 3.779031924471373e-05,
      "loss": 0.7001,
      "step": 1605700
    },
    {
      "epoch": 14.652529381706694,
      "grad_norm": 4.457022666931152,
      "learning_rate": 3.778955884857775e-05,
      "loss": 0.6598,
      "step": 1605800
    },
    {
      "epoch": 14.65344185706986,
      "grad_norm": 3.2565672397613525,
      "learning_rate": 3.778879845244179e-05,
      "loss": 0.6875,
      "step": 1605900
    },
    {
      "epoch": 14.654354332433025,
      "grad_norm": 3.6777608394622803,
      "learning_rate": 3.778803805630581e-05,
      "loss": 0.6456,
      "step": 1606000
    },
    {
      "epoch": 14.65526680779619,
      "grad_norm": 4.2461700439453125,
      "learning_rate": 3.778727766016984e-05,
      "loss": 0.6881,
      "step": 1606100
    },
    {
      "epoch": 14.656179283159355,
      "grad_norm": 4.208890914916992,
      "learning_rate": 3.778651726403387e-05,
      "loss": 0.6307,
      "step": 1606200
    },
    {
      "epoch": 14.65709175852252,
      "grad_norm": 4.387909412384033,
      "learning_rate": 3.77857568678979e-05,
      "loss": 0.6856,
      "step": 1606300
    },
    {
      "epoch": 14.658004233885684,
      "grad_norm": 4.486588478088379,
      "learning_rate": 3.778499647176193e-05,
      "loss": 0.6858,
      "step": 1606400
    },
    {
      "epoch": 14.65891670924885,
      "grad_norm": 3.7362537384033203,
      "learning_rate": 3.778423607562596e-05,
      "loss": 0.6884,
      "step": 1606500
    },
    {
      "epoch": 14.659829184612015,
      "grad_norm": 3.7548370361328125,
      "learning_rate": 3.7783475679489986e-05,
      "loss": 0.652,
      "step": 1606600
    },
    {
      "epoch": 14.66074165997518,
      "grad_norm": 4.278879165649414,
      "learning_rate": 3.778271528335402e-05,
      "loss": 0.6671,
      "step": 1606700
    },
    {
      "epoch": 14.661654135338345,
      "grad_norm": 3.8271496295928955,
      "learning_rate": 3.7781954887218046e-05,
      "loss": 0.6927,
      "step": 1606800
    },
    {
      "epoch": 14.66256661070151,
      "grad_norm": 3.5345444679260254,
      "learning_rate": 3.7781194491082076e-05,
      "loss": 0.6864,
      "step": 1606900
    },
    {
      "epoch": 14.663479086064676,
      "grad_norm": 4.0267815589904785,
      "learning_rate": 3.7780434094946106e-05,
      "loss": 0.6565,
      "step": 1607000
    },
    {
      "epoch": 14.664391561427841,
      "grad_norm": 4.025900363922119,
      "learning_rate": 3.7779673698810136e-05,
      "loss": 0.6832,
      "step": 1607100
    },
    {
      "epoch": 14.665304036791007,
      "grad_norm": 4.767233371734619,
      "learning_rate": 3.777891330267416e-05,
      "loss": 0.6715,
      "step": 1607200
    },
    {
      "epoch": 14.666216512154172,
      "grad_norm": 3.3872263431549072,
      "learning_rate": 3.7778152906538197e-05,
      "loss": 0.6862,
      "step": 1607300
    },
    {
      "epoch": 14.667128987517337,
      "grad_norm": 4.578839302062988,
      "learning_rate": 3.777739251040222e-05,
      "loss": 0.6573,
      "step": 1607400
    },
    {
      "epoch": 14.668041462880502,
      "grad_norm": 4.289116859436035,
      "learning_rate": 3.777663211426625e-05,
      "loss": 0.7227,
      "step": 1607500
    },
    {
      "epoch": 14.668953938243668,
      "grad_norm": 4.030599594116211,
      "learning_rate": 3.777587171813028e-05,
      "loss": 0.67,
      "step": 1607600
    },
    {
      "epoch": 14.669866413606833,
      "grad_norm": 3.7066924571990967,
      "learning_rate": 3.77751113219943e-05,
      "loss": 0.6879,
      "step": 1607700
    },
    {
      "epoch": 14.670778888969998,
      "grad_norm": 5.164139270782471,
      "learning_rate": 3.777435092585834e-05,
      "loss": 0.6391,
      "step": 1607800
    },
    {
      "epoch": 14.671691364333164,
      "grad_norm": 4.511807918548584,
      "learning_rate": 3.7773590529722363e-05,
      "loss": 0.6936,
      "step": 1607900
    },
    {
      "epoch": 14.672603839696329,
      "grad_norm": 4.616795539855957,
      "learning_rate": 3.7772830133586393e-05,
      "loss": 0.6804,
      "step": 1608000
    },
    {
      "epoch": 14.673516315059494,
      "grad_norm": 3.802834987640381,
      "learning_rate": 3.7772069737450424e-05,
      "loss": 0.7009,
      "step": 1608100
    },
    {
      "epoch": 14.674428790422658,
      "grad_norm": 3.608711004257202,
      "learning_rate": 3.7771309341314454e-05,
      "loss": 0.6729,
      "step": 1608200
    },
    {
      "epoch": 14.675341265785823,
      "grad_norm": 4.492010116577148,
      "learning_rate": 3.777054894517848e-05,
      "loss": 0.6383,
      "step": 1608300
    },
    {
      "epoch": 14.676253741148988,
      "grad_norm": 3.4158828258514404,
      "learning_rate": 3.7769788549042514e-05,
      "loss": 0.6639,
      "step": 1608400
    },
    {
      "epoch": 14.677166216512154,
      "grad_norm": 4.09708309173584,
      "learning_rate": 3.776902815290654e-05,
      "loss": 0.6471,
      "step": 1608500
    },
    {
      "epoch": 14.678078691875319,
      "grad_norm": 2.429863691329956,
      "learning_rate": 3.776826775677057e-05,
      "loss": 0.6261,
      "step": 1608600
    },
    {
      "epoch": 14.678991167238484,
      "grad_norm": 3.826240062713623,
      "learning_rate": 3.77675073606346e-05,
      "loss": 0.6551,
      "step": 1608700
    },
    {
      "epoch": 14.67990364260165,
      "grad_norm": 3.292724847793579,
      "learning_rate": 3.776674696449863e-05,
      "loss": 0.6729,
      "step": 1608800
    },
    {
      "epoch": 14.680816117964815,
      "grad_norm": 4.227171897888184,
      "learning_rate": 3.776598656836266e-05,
      "loss": 0.6537,
      "step": 1608900
    },
    {
      "epoch": 14.68172859332798,
      "grad_norm": 4.281746864318848,
      "learning_rate": 3.776522617222669e-05,
      "loss": 0.7365,
      "step": 1609000
    },
    {
      "epoch": 14.682641068691145,
      "grad_norm": 3.621706962585449,
      "learning_rate": 3.776446577609071e-05,
      "loss": 0.6971,
      "step": 1609100
    },
    {
      "epoch": 14.68355354405431,
      "grad_norm": 4.004615306854248,
      "learning_rate": 3.776370537995475e-05,
      "loss": 0.6292,
      "step": 1609200
    },
    {
      "epoch": 14.684466019417476,
      "grad_norm": 4.372110366821289,
      "learning_rate": 3.776294498381877e-05,
      "loss": 0.6441,
      "step": 1609300
    },
    {
      "epoch": 14.685378494780641,
      "grad_norm": 4.913508415222168,
      "learning_rate": 3.77621845876828e-05,
      "loss": 0.6962,
      "step": 1609400
    },
    {
      "epoch": 14.686290970143807,
      "grad_norm": 3.7748215198516846,
      "learning_rate": 3.776142419154683e-05,
      "loss": 0.6683,
      "step": 1609500
    },
    {
      "epoch": 14.687203445506972,
      "grad_norm": 3.614945411682129,
      "learning_rate": 3.776066379541086e-05,
      "loss": 0.6746,
      "step": 1609600
    },
    {
      "epoch": 14.688115920870137,
      "grad_norm": 3.845863103866577,
      "learning_rate": 3.7759903399274884e-05,
      "loss": 0.6418,
      "step": 1609700
    },
    {
      "epoch": 14.6890283962333,
      "grad_norm": 4.209920883178711,
      "learning_rate": 3.775914300313892e-05,
      "loss": 0.648,
      "step": 1609800
    },
    {
      "epoch": 14.689940871596466,
      "grad_norm": 4.56561279296875,
      "learning_rate": 3.7758382607002944e-05,
      "loss": 0.6987,
      "step": 1609900
    },
    {
      "epoch": 14.690853346959631,
      "grad_norm": 4.691797733306885,
      "learning_rate": 3.7757622210866975e-05,
      "loss": 0.6793,
      "step": 1610000
    },
    {
      "epoch": 14.691765822322797,
      "grad_norm": 4.24598503112793,
      "learning_rate": 3.7756861814731005e-05,
      "loss": 0.6695,
      "step": 1610100
    },
    {
      "epoch": 14.692678297685962,
      "grad_norm": 5.304908752441406,
      "learning_rate": 3.7756101418595035e-05,
      "loss": 0.649,
      "step": 1610200
    },
    {
      "epoch": 14.693590773049127,
      "grad_norm": 3.850693702697754,
      "learning_rate": 3.7755341022459065e-05,
      "loss": 0.632,
      "step": 1610300
    },
    {
      "epoch": 14.694503248412293,
      "grad_norm": 3.9975874423980713,
      "learning_rate": 3.775458062632309e-05,
      "loss": 0.6628,
      "step": 1610400
    },
    {
      "epoch": 14.695415723775458,
      "grad_norm": 3.8707773685455322,
      "learning_rate": 3.775382023018712e-05,
      "loss": 0.6473,
      "step": 1610500
    },
    {
      "epoch": 14.696328199138623,
      "grad_norm": 4.579521179199219,
      "learning_rate": 3.775305983405115e-05,
      "loss": 0.6635,
      "step": 1610600
    },
    {
      "epoch": 14.697240674501789,
      "grad_norm": 3.9956583976745605,
      "learning_rate": 3.775229943791518e-05,
      "loss": 0.6973,
      "step": 1610700
    },
    {
      "epoch": 14.698153149864954,
      "grad_norm": 3.962939977645874,
      "learning_rate": 3.77515390417792e-05,
      "loss": 0.7032,
      "step": 1610800
    },
    {
      "epoch": 14.699065625228119,
      "grad_norm": 3.9215962886810303,
      "learning_rate": 3.775077864564324e-05,
      "loss": 0.6691,
      "step": 1610900
    },
    {
      "epoch": 14.699978100591284,
      "grad_norm": 3.5199780464172363,
      "learning_rate": 3.775001824950726e-05,
      "loss": 0.6475,
      "step": 1611000
    },
    {
      "epoch": 14.70089057595445,
      "grad_norm": 4.285135269165039,
      "learning_rate": 3.774925785337129e-05,
      "loss": 0.6515,
      "step": 1611100
    },
    {
      "epoch": 14.701803051317615,
      "grad_norm": 3.819746255874634,
      "learning_rate": 3.774849745723532e-05,
      "loss": 0.6485,
      "step": 1611200
    },
    {
      "epoch": 14.70271552668078,
      "grad_norm": 3.17460560798645,
      "learning_rate": 3.774773706109935e-05,
      "loss": 0.6612,
      "step": 1611300
    },
    {
      "epoch": 14.703628002043946,
      "grad_norm": 4.315194606781006,
      "learning_rate": 3.774697666496338e-05,
      "loss": 0.6891,
      "step": 1611400
    },
    {
      "epoch": 14.704540477407111,
      "grad_norm": 2.910862445831299,
      "learning_rate": 3.774621626882741e-05,
      "loss": 0.6773,
      "step": 1611500
    },
    {
      "epoch": 14.705452952770274,
      "grad_norm": 3.894987106323242,
      "learning_rate": 3.7745455872691435e-05,
      "loss": 0.6793,
      "step": 1611600
    },
    {
      "epoch": 14.70636542813344,
      "grad_norm": 4.209601402282715,
      "learning_rate": 3.774469547655547e-05,
      "loss": 0.6687,
      "step": 1611700
    },
    {
      "epoch": 14.707277903496605,
      "grad_norm": 3.428480386734009,
      "learning_rate": 3.7743935080419495e-05,
      "loss": 0.6729,
      "step": 1611800
    },
    {
      "epoch": 14.70819037885977,
      "grad_norm": 4.089812755584717,
      "learning_rate": 3.7743174684283525e-05,
      "loss": 0.6512,
      "step": 1611900
    },
    {
      "epoch": 14.709102854222936,
      "grad_norm": 3.0568318367004395,
      "learning_rate": 3.7742414288147556e-05,
      "loss": 0.6765,
      "step": 1612000
    },
    {
      "epoch": 14.710015329586101,
      "grad_norm": 3.9538021087646484,
      "learning_rate": 3.7741653892011586e-05,
      "loss": 0.6918,
      "step": 1612100
    },
    {
      "epoch": 14.710927804949266,
      "grad_norm": 3.874523162841797,
      "learning_rate": 3.774089349587561e-05,
      "loss": 0.7026,
      "step": 1612200
    },
    {
      "epoch": 14.711840280312432,
      "grad_norm": 3.9325711727142334,
      "learning_rate": 3.7740133099739646e-05,
      "loss": 0.6767,
      "step": 1612300
    },
    {
      "epoch": 14.712752755675597,
      "grad_norm": 4.55080509185791,
      "learning_rate": 3.773937270360367e-05,
      "loss": 0.6483,
      "step": 1612400
    },
    {
      "epoch": 14.713665231038762,
      "grad_norm": 3.8190505504608154,
      "learning_rate": 3.77386123074677e-05,
      "loss": 0.69,
      "step": 1612500
    },
    {
      "epoch": 14.714577706401927,
      "grad_norm": 3.4752883911132812,
      "learning_rate": 3.773785191133173e-05,
      "loss": 0.645,
      "step": 1612600
    },
    {
      "epoch": 14.715490181765093,
      "grad_norm": 3.6714446544647217,
      "learning_rate": 3.773709151519576e-05,
      "loss": 0.6712,
      "step": 1612700
    },
    {
      "epoch": 14.716402657128258,
      "grad_norm": 3.9085631370544434,
      "learning_rate": 3.773633111905979e-05,
      "loss": 0.7008,
      "step": 1612800
    },
    {
      "epoch": 14.717315132491423,
      "grad_norm": 4.561683654785156,
      "learning_rate": 3.773557072292382e-05,
      "loss": 0.6652,
      "step": 1612900
    },
    {
      "epoch": 14.718227607854589,
      "grad_norm": 3.2558600902557373,
      "learning_rate": 3.773481032678784e-05,
      "loss": 0.6823,
      "step": 1613000
    },
    {
      "epoch": 14.719140083217754,
      "grad_norm": 3.8871939182281494,
      "learning_rate": 3.773404993065188e-05,
      "loss": 0.6629,
      "step": 1613100
    },
    {
      "epoch": 14.720052558580917,
      "grad_norm": 4.767256259918213,
      "learning_rate": 3.77332895345159e-05,
      "loss": 0.6593,
      "step": 1613200
    },
    {
      "epoch": 14.720965033944083,
      "grad_norm": 5.230116367340088,
      "learning_rate": 3.773252913837993e-05,
      "loss": 0.6568,
      "step": 1613300
    },
    {
      "epoch": 14.721877509307248,
      "grad_norm": 3.6829376220703125,
      "learning_rate": 3.773176874224396e-05,
      "loss": 0.6971,
      "step": 1613400
    },
    {
      "epoch": 14.722789984670413,
      "grad_norm": 3.291536808013916,
      "learning_rate": 3.7731008346107986e-05,
      "loss": 0.6771,
      "step": 1613500
    },
    {
      "epoch": 14.723702460033579,
      "grad_norm": 4.370102882385254,
      "learning_rate": 3.7730247949972016e-05,
      "loss": 0.676,
      "step": 1613600
    },
    {
      "epoch": 14.724614935396744,
      "grad_norm": 4.182619094848633,
      "learning_rate": 3.7729487553836046e-05,
      "loss": 0.6614,
      "step": 1613700
    },
    {
      "epoch": 14.72552741075991,
      "grad_norm": 4.277038097381592,
      "learning_rate": 3.7728727157700076e-05,
      "loss": 0.644,
      "step": 1613800
    },
    {
      "epoch": 14.726439886123075,
      "grad_norm": 3.423081398010254,
      "learning_rate": 3.7727966761564106e-05,
      "loss": 0.6882,
      "step": 1613900
    },
    {
      "epoch": 14.72735236148624,
      "grad_norm": 3.750051975250244,
      "learning_rate": 3.7727206365428137e-05,
      "loss": 0.7073,
      "step": 1614000
    },
    {
      "epoch": 14.728264836849405,
      "grad_norm": 3.9811336994171143,
      "learning_rate": 3.772644596929216e-05,
      "loss": 0.6902,
      "step": 1614100
    },
    {
      "epoch": 14.72917731221257,
      "grad_norm": 3.923142433166504,
      "learning_rate": 3.77256855731562e-05,
      "loss": 0.6951,
      "step": 1614200
    },
    {
      "epoch": 14.730089787575736,
      "grad_norm": 4.564215660095215,
      "learning_rate": 3.772492517702022e-05,
      "loss": 0.6896,
      "step": 1614300
    },
    {
      "epoch": 14.731002262938901,
      "grad_norm": 3.6699533462524414,
      "learning_rate": 3.772416478088425e-05,
      "loss": 0.6123,
      "step": 1614400
    },
    {
      "epoch": 14.731914738302066,
      "grad_norm": 3.6236183643341064,
      "learning_rate": 3.772340438474828e-05,
      "loss": 0.6701,
      "step": 1614500
    },
    {
      "epoch": 14.732827213665232,
      "grad_norm": 4.4901123046875,
      "learning_rate": 3.772264398861231e-05,
      "loss": 0.6628,
      "step": 1614600
    },
    {
      "epoch": 14.733739689028397,
      "grad_norm": 3.1172661781311035,
      "learning_rate": 3.772188359247634e-05,
      "loss": 0.6696,
      "step": 1614700
    },
    {
      "epoch": 14.734652164391562,
      "grad_norm": 4.821628570556641,
      "learning_rate": 3.772112319634037e-05,
      "loss": 0.6681,
      "step": 1614800
    },
    {
      "epoch": 14.735564639754728,
      "grad_norm": 3.581937789916992,
      "learning_rate": 3.7720362800204394e-05,
      "loss": 0.6685,
      "step": 1614900
    },
    {
      "epoch": 14.736477115117891,
      "grad_norm": 3.9767894744873047,
      "learning_rate": 3.771960240406843e-05,
      "loss": 0.6615,
      "step": 1615000
    },
    {
      "epoch": 14.737389590481056,
      "grad_norm": 3.122750997543335,
      "learning_rate": 3.7718842007932454e-05,
      "loss": 0.6983,
      "step": 1615100
    },
    {
      "epoch": 14.738302065844222,
      "grad_norm": 3.708735466003418,
      "learning_rate": 3.7718081611796484e-05,
      "loss": 0.6622,
      "step": 1615200
    },
    {
      "epoch": 14.739214541207387,
      "grad_norm": 3.5820353031158447,
      "learning_rate": 3.7717321215660514e-05,
      "loss": 0.6568,
      "step": 1615300
    },
    {
      "epoch": 14.740127016570552,
      "grad_norm": 3.5364551544189453,
      "learning_rate": 3.7716560819524544e-05,
      "loss": 0.6418,
      "step": 1615400
    },
    {
      "epoch": 14.741039491933718,
      "grad_norm": 3.4546751976013184,
      "learning_rate": 3.771580042338857e-05,
      "loss": 0.6765,
      "step": 1615500
    },
    {
      "epoch": 14.741951967296883,
      "grad_norm": 3.9484426975250244,
      "learning_rate": 3.7715040027252604e-05,
      "loss": 0.6435,
      "step": 1615600
    },
    {
      "epoch": 14.742864442660048,
      "grad_norm": 4.498814105987549,
      "learning_rate": 3.771427963111663e-05,
      "loss": 0.6685,
      "step": 1615700
    },
    {
      "epoch": 14.743776918023213,
      "grad_norm": 3.4503371715545654,
      "learning_rate": 3.771351923498066e-05,
      "loss": 0.6608,
      "step": 1615800
    },
    {
      "epoch": 14.744689393386379,
      "grad_norm": 3.8723793029785156,
      "learning_rate": 3.771275883884469e-05,
      "loss": 0.6414,
      "step": 1615900
    },
    {
      "epoch": 14.745601868749544,
      "grad_norm": 2.9007821083068848,
      "learning_rate": 3.771199844270872e-05,
      "loss": 0.6523,
      "step": 1616000
    },
    {
      "epoch": 14.74651434411271,
      "grad_norm": 4.006980895996094,
      "learning_rate": 3.771123804657275e-05,
      "loss": 0.6574,
      "step": 1616100
    },
    {
      "epoch": 14.747426819475875,
      "grad_norm": 4.65533447265625,
      "learning_rate": 3.771047765043677e-05,
      "loss": 0.6778,
      "step": 1616200
    },
    {
      "epoch": 14.74833929483904,
      "grad_norm": 4.381519794464111,
      "learning_rate": 3.77097172543008e-05,
      "loss": 0.6733,
      "step": 1616300
    },
    {
      "epoch": 14.749251770202205,
      "grad_norm": 4.601871967315674,
      "learning_rate": 3.770895685816483e-05,
      "loss": 0.6822,
      "step": 1616400
    },
    {
      "epoch": 14.75016424556537,
      "grad_norm": 4.622287750244141,
      "learning_rate": 3.770819646202886e-05,
      "loss": 0.7012,
      "step": 1616500
    },
    {
      "epoch": 14.751076720928534,
      "grad_norm": 4.066891670227051,
      "learning_rate": 3.7707436065892884e-05,
      "loss": 0.6863,
      "step": 1616600
    },
    {
      "epoch": 14.7519891962917,
      "grad_norm": 4.10491943359375,
      "learning_rate": 3.770667566975692e-05,
      "loss": 0.6915,
      "step": 1616700
    },
    {
      "epoch": 14.752901671654865,
      "grad_norm": 3.5784595012664795,
      "learning_rate": 3.7705915273620945e-05,
      "loss": 0.6777,
      "step": 1616800
    },
    {
      "epoch": 14.75381414701803,
      "grad_norm": 4.806334495544434,
      "learning_rate": 3.7705154877484975e-05,
      "loss": 0.6722,
      "step": 1616900
    },
    {
      "epoch": 14.754726622381195,
      "grad_norm": 4.74439001083374,
      "learning_rate": 3.7704394481349005e-05,
      "loss": 0.6625,
      "step": 1617000
    },
    {
      "epoch": 14.75563909774436,
      "grad_norm": 3.2807981967926025,
      "learning_rate": 3.7703634085213035e-05,
      "loss": 0.7072,
      "step": 1617100
    },
    {
      "epoch": 14.756551573107526,
      "grad_norm": 3.8095734119415283,
      "learning_rate": 3.7702873689077065e-05,
      "loss": 0.6953,
      "step": 1617200
    },
    {
      "epoch": 14.757464048470691,
      "grad_norm": 3.8389205932617188,
      "learning_rate": 3.7702113292941095e-05,
      "loss": 0.676,
      "step": 1617300
    },
    {
      "epoch": 14.758376523833856,
      "grad_norm": 3.7004215717315674,
      "learning_rate": 3.770135289680512e-05,
      "loss": 0.6721,
      "step": 1617400
    },
    {
      "epoch": 14.759288999197022,
      "grad_norm": 4.567090034484863,
      "learning_rate": 3.7700592500669155e-05,
      "loss": 0.6935,
      "step": 1617500
    },
    {
      "epoch": 14.760201474560187,
      "grad_norm": 3.526231288909912,
      "learning_rate": 3.769983210453318e-05,
      "loss": 0.6605,
      "step": 1617600
    },
    {
      "epoch": 14.761113949923352,
      "grad_norm": 4.5352606773376465,
      "learning_rate": 3.769907170839721e-05,
      "loss": 0.6649,
      "step": 1617700
    },
    {
      "epoch": 14.762026425286518,
      "grad_norm": 3.7741708755493164,
      "learning_rate": 3.769831131226124e-05,
      "loss": 0.7045,
      "step": 1617800
    },
    {
      "epoch": 14.762938900649683,
      "grad_norm": 4.625333786010742,
      "learning_rate": 3.769755091612527e-05,
      "loss": 0.6907,
      "step": 1617900
    },
    {
      "epoch": 14.763851376012848,
      "grad_norm": 3.8460311889648438,
      "learning_rate": 3.769679051998929e-05,
      "loss": 0.6708,
      "step": 1618000
    },
    {
      "epoch": 14.764763851376014,
      "grad_norm": 1.7885874509811401,
      "learning_rate": 3.769603012385333e-05,
      "loss": 0.698,
      "step": 1618100
    },
    {
      "epoch": 14.765676326739179,
      "grad_norm": 3.6999402046203613,
      "learning_rate": 3.769526972771735e-05,
      "loss": 0.6637,
      "step": 1618200
    },
    {
      "epoch": 14.766588802102344,
      "grad_norm": 4.211911678314209,
      "learning_rate": 3.769450933158138e-05,
      "loss": 0.6443,
      "step": 1618300
    },
    {
      "epoch": 14.767501277465508,
      "grad_norm": 3.199530839920044,
      "learning_rate": 3.769374893544541e-05,
      "loss": 0.6853,
      "step": 1618400
    },
    {
      "epoch": 14.768413752828673,
      "grad_norm": 4.72813081741333,
      "learning_rate": 3.769298853930944e-05,
      "loss": 0.7037,
      "step": 1618500
    },
    {
      "epoch": 14.769326228191838,
      "grad_norm": 2.923055648803711,
      "learning_rate": 3.769222814317347e-05,
      "loss": 0.6834,
      "step": 1618600
    },
    {
      "epoch": 14.770238703555004,
      "grad_norm": 4.216197967529297,
      "learning_rate": 3.76914677470375e-05,
      "loss": 0.7122,
      "step": 1618700
    },
    {
      "epoch": 14.771151178918169,
      "grad_norm": 3.5612010955810547,
      "learning_rate": 3.7690707350901526e-05,
      "loss": 0.6763,
      "step": 1618800
    },
    {
      "epoch": 14.772063654281334,
      "grad_norm": 2.4292962551116943,
      "learning_rate": 3.7689946954765556e-05,
      "loss": 0.6715,
      "step": 1618900
    },
    {
      "epoch": 14.7729761296445,
      "grad_norm": 4.663614749908447,
      "learning_rate": 3.7689186558629586e-05,
      "loss": 0.6647,
      "step": 1619000
    },
    {
      "epoch": 14.773888605007665,
      "grad_norm": 2.9069604873657227,
      "learning_rate": 3.768842616249361e-05,
      "loss": 0.691,
      "step": 1619100
    },
    {
      "epoch": 14.77480108037083,
      "grad_norm": 3.395742654800415,
      "learning_rate": 3.7687665766357646e-05,
      "loss": 0.683,
      "step": 1619200
    },
    {
      "epoch": 14.775713555733995,
      "grad_norm": 3.542060613632202,
      "learning_rate": 3.768690537022167e-05,
      "loss": 0.6692,
      "step": 1619300
    },
    {
      "epoch": 14.77662603109716,
      "grad_norm": 4.219213485717773,
      "learning_rate": 3.76861449740857e-05,
      "loss": 0.6917,
      "step": 1619400
    },
    {
      "epoch": 14.777538506460326,
      "grad_norm": 3.9346303939819336,
      "learning_rate": 3.768538457794973e-05,
      "loss": 0.6842,
      "step": 1619500
    },
    {
      "epoch": 14.778450981823491,
      "grad_norm": 4.100706100463867,
      "learning_rate": 3.768462418181376e-05,
      "loss": 0.6784,
      "step": 1619600
    },
    {
      "epoch": 14.779363457186657,
      "grad_norm": 3.146749258041382,
      "learning_rate": 3.768386378567779e-05,
      "loss": 0.6309,
      "step": 1619700
    },
    {
      "epoch": 14.780275932549822,
      "grad_norm": 3.3829500675201416,
      "learning_rate": 3.768310338954182e-05,
      "loss": 0.6699,
      "step": 1619800
    },
    {
      "epoch": 14.781188407912987,
      "grad_norm": 4.694820404052734,
      "learning_rate": 3.768234299340584e-05,
      "loss": 0.6821,
      "step": 1619900
    },
    {
      "epoch": 14.78210088327615,
      "grad_norm": 4.132617950439453,
      "learning_rate": 3.768158259726988e-05,
      "loss": 0.6448,
      "step": 1620000
    },
    {
      "epoch": 14.783013358639316,
      "grad_norm": 3.9539284706115723,
      "learning_rate": 3.76808222011339e-05,
      "loss": 0.6543,
      "step": 1620100
    },
    {
      "epoch": 14.783925834002481,
      "grad_norm": 5.0541768074035645,
      "learning_rate": 3.768006180499793e-05,
      "loss": 0.689,
      "step": 1620200
    },
    {
      "epoch": 14.784838309365647,
      "grad_norm": 3.9233295917510986,
      "learning_rate": 3.767930140886196e-05,
      "loss": 0.6544,
      "step": 1620300
    },
    {
      "epoch": 14.785750784728812,
      "grad_norm": 5.004560470581055,
      "learning_rate": 3.767854101272599e-05,
      "loss": 0.6192,
      "step": 1620400
    },
    {
      "epoch": 14.786663260091977,
      "grad_norm": 3.3556690216064453,
      "learning_rate": 3.7677780616590016e-05,
      "loss": 0.7091,
      "step": 1620500
    },
    {
      "epoch": 14.787575735455142,
      "grad_norm": 3.7814722061157227,
      "learning_rate": 3.767702022045405e-05,
      "loss": 0.6475,
      "step": 1620600
    },
    {
      "epoch": 14.788488210818308,
      "grad_norm": 3.5809919834136963,
      "learning_rate": 3.7676259824318077e-05,
      "loss": 0.647,
      "step": 1620700
    },
    {
      "epoch": 14.789400686181473,
      "grad_norm": 2.6211094856262207,
      "learning_rate": 3.7675499428182107e-05,
      "loss": 0.6341,
      "step": 1620800
    },
    {
      "epoch": 14.790313161544638,
      "grad_norm": 3.9086577892303467,
      "learning_rate": 3.767473903204614e-05,
      "loss": 0.719,
      "step": 1620900
    },
    {
      "epoch": 14.791225636907804,
      "grad_norm": 4.195003032684326,
      "learning_rate": 3.767397863591017e-05,
      "loss": 0.6505,
      "step": 1621000
    },
    {
      "epoch": 14.792138112270969,
      "grad_norm": 4.174790382385254,
      "learning_rate": 3.76732182397742e-05,
      "loss": 0.6832,
      "step": 1621100
    },
    {
      "epoch": 14.793050587634134,
      "grad_norm": 3.1571884155273438,
      "learning_rate": 3.767245784363823e-05,
      "loss": 0.6582,
      "step": 1621200
    },
    {
      "epoch": 14.7939630629973,
      "grad_norm": 4.901247024536133,
      "learning_rate": 3.767169744750225e-05,
      "loss": 0.7205,
      "step": 1621300
    },
    {
      "epoch": 14.794875538360465,
      "grad_norm": 4.457334518432617,
      "learning_rate": 3.767093705136629e-05,
      "loss": 0.6729,
      "step": 1621400
    },
    {
      "epoch": 14.79578801372363,
      "grad_norm": 5.333219528198242,
      "learning_rate": 3.767017665523031e-05,
      "loss": 0.6712,
      "step": 1621500
    },
    {
      "epoch": 14.796700489086795,
      "grad_norm": 4.507368564605713,
      "learning_rate": 3.766941625909434e-05,
      "loss": 0.6724,
      "step": 1621600
    },
    {
      "epoch": 14.79761296444996,
      "grad_norm": 4.563261032104492,
      "learning_rate": 3.766865586295837e-05,
      "loss": 0.6872,
      "step": 1621700
    },
    {
      "epoch": 14.798525439813124,
      "grad_norm": 3.7917590141296387,
      "learning_rate": 3.7667895466822394e-05,
      "loss": 0.7358,
      "step": 1621800
    },
    {
      "epoch": 14.79943791517629,
      "grad_norm": 3.7971107959747314,
      "learning_rate": 3.7667135070686424e-05,
      "loss": 0.6322,
      "step": 1621900
    },
    {
      "epoch": 14.800350390539455,
      "grad_norm": 4.195364475250244,
      "learning_rate": 3.7666374674550454e-05,
      "loss": 0.6543,
      "step": 1622000
    },
    {
      "epoch": 14.80126286590262,
      "grad_norm": 4.4891510009765625,
      "learning_rate": 3.7665614278414484e-05,
      "loss": 0.6364,
      "step": 1622100
    },
    {
      "epoch": 14.802175341265785,
      "grad_norm": 3.508057117462158,
      "learning_rate": 3.7664853882278514e-05,
      "loss": 0.6368,
      "step": 1622200
    },
    {
      "epoch": 14.80308781662895,
      "grad_norm": 3.9513213634490967,
      "learning_rate": 3.7664093486142544e-05,
      "loss": 0.7457,
      "step": 1622300
    },
    {
      "epoch": 14.804000291992116,
      "grad_norm": 3.441507339477539,
      "learning_rate": 3.766333309000657e-05,
      "loss": 0.6726,
      "step": 1622400
    },
    {
      "epoch": 14.804912767355281,
      "grad_norm": 1.9109148979187012,
      "learning_rate": 3.7662572693870604e-05,
      "loss": 0.6537,
      "step": 1622500
    },
    {
      "epoch": 14.805825242718447,
      "grad_norm": 4.262753963470459,
      "learning_rate": 3.766181229773463e-05,
      "loss": 0.6761,
      "step": 1622600
    },
    {
      "epoch": 14.806737718081612,
      "grad_norm": 4.046159744262695,
      "learning_rate": 3.766105190159866e-05,
      "loss": 0.6822,
      "step": 1622700
    },
    {
      "epoch": 14.807650193444777,
      "grad_norm": 3.2120866775512695,
      "learning_rate": 3.766029150546269e-05,
      "loss": 0.6445,
      "step": 1622800
    },
    {
      "epoch": 14.808562668807943,
      "grad_norm": 4.848048686981201,
      "learning_rate": 3.765953110932672e-05,
      "loss": 0.6735,
      "step": 1622900
    },
    {
      "epoch": 14.809475144171108,
      "grad_norm": 4.092700481414795,
      "learning_rate": 3.765877071319074e-05,
      "loss": 0.6787,
      "step": 1623000
    },
    {
      "epoch": 14.810387619534273,
      "grad_norm": 3.9322516918182373,
      "learning_rate": 3.765801031705478e-05,
      "loss": 0.6541,
      "step": 1623100
    },
    {
      "epoch": 14.811300094897438,
      "grad_norm": 4.302724361419678,
      "learning_rate": 3.76572499209188e-05,
      "loss": 0.6699,
      "step": 1623200
    },
    {
      "epoch": 14.812212570260604,
      "grad_norm": 4.1842451095581055,
      "learning_rate": 3.765648952478283e-05,
      "loss": 0.6854,
      "step": 1623300
    },
    {
      "epoch": 14.813125045623767,
      "grad_norm": 3.905764102935791,
      "learning_rate": 3.765572912864686e-05,
      "loss": 0.6926,
      "step": 1623400
    },
    {
      "epoch": 14.814037520986933,
      "grad_norm": 4.043756008148193,
      "learning_rate": 3.765496873251089e-05,
      "loss": 0.682,
      "step": 1623500
    },
    {
      "epoch": 14.814949996350098,
      "grad_norm": 4.39185094833374,
      "learning_rate": 3.765420833637492e-05,
      "loss": 0.7003,
      "step": 1623600
    },
    {
      "epoch": 14.815862471713263,
      "grad_norm": 4.050072193145752,
      "learning_rate": 3.765344794023895e-05,
      "loss": 0.6786,
      "step": 1623700
    },
    {
      "epoch": 14.816774947076429,
      "grad_norm": 2.5937070846557617,
      "learning_rate": 3.7652687544102975e-05,
      "loss": 0.6287,
      "step": 1623800
    },
    {
      "epoch": 14.817687422439594,
      "grad_norm": 3.869839668273926,
      "learning_rate": 3.765192714796701e-05,
      "loss": 0.6596,
      "step": 1623900
    },
    {
      "epoch": 14.818599897802759,
      "grad_norm": 3.662628650665283,
      "learning_rate": 3.7651166751831035e-05,
      "loss": 0.6778,
      "step": 1624000
    },
    {
      "epoch": 14.819512373165924,
      "grad_norm": 4.639329433441162,
      "learning_rate": 3.7650406355695065e-05,
      "loss": 0.6425,
      "step": 1624100
    },
    {
      "epoch": 14.82042484852909,
      "grad_norm": 4.277357578277588,
      "learning_rate": 3.7649645959559095e-05,
      "loss": 0.6798,
      "step": 1624200
    },
    {
      "epoch": 14.821337323892255,
      "grad_norm": 4.81773042678833,
      "learning_rate": 3.7648885563423125e-05,
      "loss": 0.6644,
      "step": 1624300
    },
    {
      "epoch": 14.82224979925542,
      "grad_norm": 2.656609535217285,
      "learning_rate": 3.764812516728715e-05,
      "loss": 0.6467,
      "step": 1624400
    },
    {
      "epoch": 14.823162274618586,
      "grad_norm": 5.047889232635498,
      "learning_rate": 3.7647364771151185e-05,
      "loss": 0.6985,
      "step": 1624500
    },
    {
      "epoch": 14.824074749981751,
      "grad_norm": 3.7350354194641113,
      "learning_rate": 3.764660437501521e-05,
      "loss": 0.687,
      "step": 1624600
    },
    {
      "epoch": 14.824987225344916,
      "grad_norm": 4.131361484527588,
      "learning_rate": 3.764584397887924e-05,
      "loss": 0.6496,
      "step": 1624700
    },
    {
      "epoch": 14.825899700708081,
      "grad_norm": 3.4573888778686523,
      "learning_rate": 3.764508358274327e-05,
      "loss": 0.685,
      "step": 1624800
    },
    {
      "epoch": 14.826812176071247,
      "grad_norm": 3.0594098567962646,
      "learning_rate": 3.764432318660729e-05,
      "loss": 0.6868,
      "step": 1624900
    },
    {
      "epoch": 14.827724651434412,
      "grad_norm": 4.443728923797607,
      "learning_rate": 3.764356279047133e-05,
      "loss": 0.662,
      "step": 1625000
    },
    {
      "epoch": 14.828637126797577,
      "grad_norm": 3.3299405574798584,
      "learning_rate": 3.764280239433535e-05,
      "loss": 0.6489,
      "step": 1625100
    },
    {
      "epoch": 14.829549602160741,
      "grad_norm": 3.486927032470703,
      "learning_rate": 3.764204199819938e-05,
      "loss": 0.6905,
      "step": 1625200
    },
    {
      "epoch": 14.830462077523906,
      "grad_norm": 3.2108447551727295,
      "learning_rate": 3.764128160206341e-05,
      "loss": 0.7011,
      "step": 1625300
    },
    {
      "epoch": 14.831374552887072,
      "grad_norm": 3.6117615699768066,
      "learning_rate": 3.764052120592744e-05,
      "loss": 0.6808,
      "step": 1625400
    },
    {
      "epoch": 14.832287028250237,
      "grad_norm": 4.1650214195251465,
      "learning_rate": 3.7639760809791466e-05,
      "loss": 0.6795,
      "step": 1625500
    },
    {
      "epoch": 14.833199503613402,
      "grad_norm": 3.7114744186401367,
      "learning_rate": 3.76390004136555e-05,
      "loss": 0.6692,
      "step": 1625600
    },
    {
      "epoch": 14.834111978976567,
      "grad_norm": 5.089845657348633,
      "learning_rate": 3.7638240017519526e-05,
      "loss": 0.6905,
      "step": 1625700
    },
    {
      "epoch": 14.835024454339733,
      "grad_norm": 3.6258788108825684,
      "learning_rate": 3.7637479621383556e-05,
      "loss": 0.6286,
      "step": 1625800
    },
    {
      "epoch": 14.835936929702898,
      "grad_norm": 4.051989555358887,
      "learning_rate": 3.7636719225247586e-05,
      "loss": 0.7081,
      "step": 1625900
    },
    {
      "epoch": 14.836849405066063,
      "grad_norm": 3.60623836517334,
      "learning_rate": 3.7635958829111616e-05,
      "loss": 0.6731,
      "step": 1626000
    },
    {
      "epoch": 14.837761880429229,
      "grad_norm": 4.748560428619385,
      "learning_rate": 3.7635198432975646e-05,
      "loss": 0.6648,
      "step": 1626100
    },
    {
      "epoch": 14.838674355792394,
      "grad_norm": 3.750870704650879,
      "learning_rate": 3.7634438036839676e-05,
      "loss": 0.69,
      "step": 1626200
    },
    {
      "epoch": 14.83958683115556,
      "grad_norm": 3.6613261699676514,
      "learning_rate": 3.76336776407037e-05,
      "loss": 0.6859,
      "step": 1626300
    },
    {
      "epoch": 14.840499306518725,
      "grad_norm": 3.4996864795684814,
      "learning_rate": 3.7632917244567736e-05,
      "loss": 0.6446,
      "step": 1626400
    },
    {
      "epoch": 14.84141178188189,
      "grad_norm": 4.024850368499756,
      "learning_rate": 3.763215684843176e-05,
      "loss": 0.6844,
      "step": 1626500
    },
    {
      "epoch": 14.842324257245055,
      "grad_norm": 4.798610687255859,
      "learning_rate": 3.763139645229579e-05,
      "loss": 0.6377,
      "step": 1626600
    },
    {
      "epoch": 14.84323673260822,
      "grad_norm": 3.415560483932495,
      "learning_rate": 3.763063605615982e-05,
      "loss": 0.6866,
      "step": 1626700
    },
    {
      "epoch": 14.844149207971384,
      "grad_norm": 4.481904029846191,
      "learning_rate": 3.762987566002385e-05,
      "loss": 0.6936,
      "step": 1626800
    },
    {
      "epoch": 14.84506168333455,
      "grad_norm": 4.570972919464111,
      "learning_rate": 3.762911526388788e-05,
      "loss": 0.655,
      "step": 1626900
    },
    {
      "epoch": 14.845974158697715,
      "grad_norm": 4.015141010284424,
      "learning_rate": 3.762835486775191e-05,
      "loss": 0.6649,
      "step": 1627000
    },
    {
      "epoch": 14.84688663406088,
      "grad_norm": 3.8086764812469482,
      "learning_rate": 3.762759447161593e-05,
      "loss": 0.6621,
      "step": 1627100
    },
    {
      "epoch": 14.847799109424045,
      "grad_norm": 3.4290003776550293,
      "learning_rate": 3.762683407547996e-05,
      "loss": 0.6861,
      "step": 1627200
    },
    {
      "epoch": 14.84871158478721,
      "grad_norm": 3.8857078552246094,
      "learning_rate": 3.762607367934399e-05,
      "loss": 0.6803,
      "step": 1627300
    },
    {
      "epoch": 14.849624060150376,
      "grad_norm": 3.480210542678833,
      "learning_rate": 3.762531328320802e-05,
      "loss": 0.7053,
      "step": 1627400
    },
    {
      "epoch": 14.850536535513541,
      "grad_norm": 3.7177329063415527,
      "learning_rate": 3.762455288707205e-05,
      "loss": 0.6604,
      "step": 1627500
    },
    {
      "epoch": 14.851449010876706,
      "grad_norm": 4.7140350341796875,
      "learning_rate": 3.7623792490936077e-05,
      "loss": 0.6735,
      "step": 1627600
    },
    {
      "epoch": 14.852361486239872,
      "grad_norm": 5.4033637046813965,
      "learning_rate": 3.762303209480011e-05,
      "loss": 0.6842,
      "step": 1627700
    },
    {
      "epoch": 14.853273961603037,
      "grad_norm": 4.427834987640381,
      "learning_rate": 3.762227169866414e-05,
      "loss": 0.6482,
      "step": 1627800
    },
    {
      "epoch": 14.854186436966202,
      "grad_norm": 4.12742805480957,
      "learning_rate": 3.762151130252817e-05,
      "loss": 0.6916,
      "step": 1627900
    },
    {
      "epoch": 14.855098912329368,
      "grad_norm": 4.087336540222168,
      "learning_rate": 3.76207509063922e-05,
      "loss": 0.6637,
      "step": 1628000
    },
    {
      "epoch": 14.856011387692533,
      "grad_norm": 3.6813080310821533,
      "learning_rate": 3.761999051025623e-05,
      "loss": 0.6811,
      "step": 1628100
    },
    {
      "epoch": 14.856923863055698,
      "grad_norm": 4.582679271697998,
      "learning_rate": 3.761923011412025e-05,
      "loss": 0.7019,
      "step": 1628200
    },
    {
      "epoch": 14.857836338418863,
      "grad_norm": 4.335056304931641,
      "learning_rate": 3.761846971798429e-05,
      "loss": 0.6608,
      "step": 1628300
    },
    {
      "epoch": 14.858748813782029,
      "grad_norm": 4.557638168334961,
      "learning_rate": 3.761770932184831e-05,
      "loss": 0.692,
      "step": 1628400
    },
    {
      "epoch": 14.859661289145194,
      "grad_norm": 3.526420831680298,
      "learning_rate": 3.761694892571234e-05,
      "loss": 0.6893,
      "step": 1628500
    },
    {
      "epoch": 14.860573764508358,
      "grad_norm": 3.6857354640960693,
      "learning_rate": 3.761618852957637e-05,
      "loss": 0.6773,
      "step": 1628600
    },
    {
      "epoch": 14.861486239871523,
      "grad_norm": 3.0979416370391846,
      "learning_rate": 3.76154281334404e-05,
      "loss": 0.6809,
      "step": 1628700
    },
    {
      "epoch": 14.862398715234688,
      "grad_norm": 4.490753650665283,
      "learning_rate": 3.7614667737304424e-05,
      "loss": 0.6916,
      "step": 1628800
    },
    {
      "epoch": 14.863311190597853,
      "grad_norm": 4.062880516052246,
      "learning_rate": 3.761390734116846e-05,
      "loss": 0.7001,
      "step": 1628900
    },
    {
      "epoch": 14.864223665961019,
      "grad_norm": 3.725447416305542,
      "learning_rate": 3.7613146945032484e-05,
      "loss": 0.7087,
      "step": 1629000
    },
    {
      "epoch": 14.865136141324184,
      "grad_norm": 3.8469631671905518,
      "learning_rate": 3.7612386548896514e-05,
      "loss": 0.669,
      "step": 1629100
    },
    {
      "epoch": 14.86604861668735,
      "grad_norm": 3.841336965560913,
      "learning_rate": 3.7611626152760544e-05,
      "loss": 0.6805,
      "step": 1629200
    },
    {
      "epoch": 14.866961092050515,
      "grad_norm": 2.354811429977417,
      "learning_rate": 3.7610865756624574e-05,
      "loss": 0.6553,
      "step": 1629300
    },
    {
      "epoch": 14.86787356741368,
      "grad_norm": 3.276205539703369,
      "learning_rate": 3.7610105360488604e-05,
      "loss": 0.6909,
      "step": 1629400
    },
    {
      "epoch": 14.868786042776845,
      "grad_norm": 3.479395627975464,
      "learning_rate": 3.7609344964352634e-05,
      "loss": 0.7013,
      "step": 1629500
    },
    {
      "epoch": 14.86969851814001,
      "grad_norm": 4.661661148071289,
      "learning_rate": 3.760858456821666e-05,
      "loss": 0.6449,
      "step": 1629600
    },
    {
      "epoch": 14.870610993503176,
      "grad_norm": 3.1670751571655273,
      "learning_rate": 3.7607824172080694e-05,
      "loss": 0.6334,
      "step": 1629700
    },
    {
      "epoch": 14.871523468866341,
      "grad_norm": 3.373015880584717,
      "learning_rate": 3.760706377594472e-05,
      "loss": 0.6681,
      "step": 1629800
    },
    {
      "epoch": 14.872435944229506,
      "grad_norm": 4.5012006759643555,
      "learning_rate": 3.760630337980875e-05,
      "loss": 0.6902,
      "step": 1629900
    },
    {
      "epoch": 14.873348419592672,
      "grad_norm": 3.9671316146850586,
      "learning_rate": 3.760554298367278e-05,
      "loss": 0.644,
      "step": 1630000
    },
    {
      "epoch": 14.874260894955835,
      "grad_norm": 4.717619895935059,
      "learning_rate": 3.760478258753681e-05,
      "loss": 0.6643,
      "step": 1630100
    },
    {
      "epoch": 14.875173370319,
      "grad_norm": 5.836386680603027,
      "learning_rate": 3.760402219140083e-05,
      "loss": 0.6762,
      "step": 1630200
    },
    {
      "epoch": 14.876085845682166,
      "grad_norm": 3.4300410747528076,
      "learning_rate": 3.760326179526486e-05,
      "loss": 0.6462,
      "step": 1630300
    },
    {
      "epoch": 14.876998321045331,
      "grad_norm": 3.6754674911499023,
      "learning_rate": 3.760250139912889e-05,
      "loss": 0.6691,
      "step": 1630400
    },
    {
      "epoch": 14.877910796408496,
      "grad_norm": 4.988218784332275,
      "learning_rate": 3.760174100299292e-05,
      "loss": 0.6625,
      "step": 1630500
    },
    {
      "epoch": 14.878823271771662,
      "grad_norm": 4.699249744415283,
      "learning_rate": 3.760098060685695e-05,
      "loss": 0.6699,
      "step": 1630600
    },
    {
      "epoch": 14.879735747134827,
      "grad_norm": 3.0733838081359863,
      "learning_rate": 3.7600220210720975e-05,
      "loss": 0.6597,
      "step": 1630700
    },
    {
      "epoch": 14.880648222497992,
      "grad_norm": 3.3119637966156006,
      "learning_rate": 3.759945981458501e-05,
      "loss": 0.6802,
      "step": 1630800
    },
    {
      "epoch": 14.881560697861158,
      "grad_norm": 3.2893476486206055,
      "learning_rate": 3.7598699418449035e-05,
      "loss": 0.6404,
      "step": 1630900
    },
    {
      "epoch": 14.882473173224323,
      "grad_norm": 2.6063895225524902,
      "learning_rate": 3.7597939022313065e-05,
      "loss": 0.6655,
      "step": 1631000
    },
    {
      "epoch": 14.883385648587488,
      "grad_norm": 3.1102540493011475,
      "learning_rate": 3.7597178626177095e-05,
      "loss": 0.6409,
      "step": 1631100
    },
    {
      "epoch": 14.884298123950654,
      "grad_norm": 4.7685041427612305,
      "learning_rate": 3.7596418230041125e-05,
      "loss": 0.6862,
      "step": 1631200
    },
    {
      "epoch": 14.885210599313819,
      "grad_norm": 3.517434597015381,
      "learning_rate": 3.759565783390515e-05,
      "loss": 0.6645,
      "step": 1631300
    },
    {
      "epoch": 14.886123074676984,
      "grad_norm": 3.7994861602783203,
      "learning_rate": 3.7594897437769185e-05,
      "loss": 0.6588,
      "step": 1631400
    },
    {
      "epoch": 14.88703555004015,
      "grad_norm": 3.1961829662323,
      "learning_rate": 3.759413704163321e-05,
      "loss": 0.666,
      "step": 1631500
    },
    {
      "epoch": 14.887948025403315,
      "grad_norm": 4.232332706451416,
      "learning_rate": 3.759337664549724e-05,
      "loss": 0.672,
      "step": 1631600
    },
    {
      "epoch": 14.88886050076648,
      "grad_norm": 5.294427394866943,
      "learning_rate": 3.759261624936127e-05,
      "loss": 0.6359,
      "step": 1631700
    },
    {
      "epoch": 14.889772976129645,
      "grad_norm": 3.7807018756866455,
      "learning_rate": 3.75918558532253e-05,
      "loss": 0.6855,
      "step": 1631800
    },
    {
      "epoch": 14.890685451492809,
      "grad_norm": 4.178213119506836,
      "learning_rate": 3.759109545708933e-05,
      "loss": 0.7224,
      "step": 1631900
    },
    {
      "epoch": 14.891597926855974,
      "grad_norm": 3.6644206047058105,
      "learning_rate": 3.759033506095336e-05,
      "loss": 0.6648,
      "step": 1632000
    },
    {
      "epoch": 14.89251040221914,
      "grad_norm": 4.371769905090332,
      "learning_rate": 3.758957466481738e-05,
      "loss": 0.6905,
      "step": 1632100
    },
    {
      "epoch": 14.893422877582305,
      "grad_norm": 3.7830147743225098,
      "learning_rate": 3.758881426868142e-05,
      "loss": 0.6629,
      "step": 1632200
    },
    {
      "epoch": 14.89433535294547,
      "grad_norm": 3.999371290206909,
      "learning_rate": 3.758805387254544e-05,
      "loss": 0.6774,
      "step": 1632300
    },
    {
      "epoch": 14.895247828308635,
      "grad_norm": 3.9714176654815674,
      "learning_rate": 3.758729347640947e-05,
      "loss": 0.6572,
      "step": 1632400
    },
    {
      "epoch": 14.8961603036718,
      "grad_norm": 5.91425085067749,
      "learning_rate": 3.75865330802735e-05,
      "loss": 0.6562,
      "step": 1632500
    },
    {
      "epoch": 14.897072779034966,
      "grad_norm": 4.145328998565674,
      "learning_rate": 3.758577268413753e-05,
      "loss": 0.6643,
      "step": 1632600
    },
    {
      "epoch": 14.897985254398131,
      "grad_norm": 3.55124831199646,
      "learning_rate": 3.7585012288001556e-05,
      "loss": 0.6563,
      "step": 1632700
    },
    {
      "epoch": 14.898897729761297,
      "grad_norm": 4.8010125160217285,
      "learning_rate": 3.758425189186559e-05,
      "loss": 0.6599,
      "step": 1632800
    },
    {
      "epoch": 14.899810205124462,
      "grad_norm": 3.725522518157959,
      "learning_rate": 3.7583491495729616e-05,
      "loss": 0.6882,
      "step": 1632900
    },
    {
      "epoch": 14.900722680487627,
      "grad_norm": 3.105349063873291,
      "learning_rate": 3.7582731099593646e-05,
      "loss": 0.6535,
      "step": 1633000
    },
    {
      "epoch": 14.901635155850792,
      "grad_norm": 3.7081310749053955,
      "learning_rate": 3.7581970703457676e-05,
      "loss": 0.634,
      "step": 1633100
    },
    {
      "epoch": 14.902547631213958,
      "grad_norm": 3.565622091293335,
      "learning_rate": 3.75812103073217e-05,
      "loss": 0.6647,
      "step": 1633200
    },
    {
      "epoch": 14.903460106577123,
      "grad_norm": 4.218949794769287,
      "learning_rate": 3.7580449911185736e-05,
      "loss": 0.6746,
      "step": 1633300
    },
    {
      "epoch": 14.904372581940288,
      "grad_norm": 4.472891807556152,
      "learning_rate": 3.757968951504976e-05,
      "loss": 0.653,
      "step": 1633400
    },
    {
      "epoch": 14.905285057303452,
      "grad_norm": 4.868770599365234,
      "learning_rate": 3.757892911891379e-05,
      "loss": 0.6901,
      "step": 1633500
    },
    {
      "epoch": 14.906197532666617,
      "grad_norm": 3.7250077724456787,
      "learning_rate": 3.757816872277782e-05,
      "loss": 0.6923,
      "step": 1633600
    },
    {
      "epoch": 14.907110008029782,
      "grad_norm": 3.9190709590911865,
      "learning_rate": 3.757740832664185e-05,
      "loss": 0.6669,
      "step": 1633700
    },
    {
      "epoch": 14.908022483392948,
      "grad_norm": 3.5826661586761475,
      "learning_rate": 3.757664793050587e-05,
      "loss": 0.6816,
      "step": 1633800
    },
    {
      "epoch": 14.908934958756113,
      "grad_norm": 3.128573179244995,
      "learning_rate": 3.757588753436991e-05,
      "loss": 0.6753,
      "step": 1633900
    },
    {
      "epoch": 14.909847434119278,
      "grad_norm": 4.116832733154297,
      "learning_rate": 3.757512713823393e-05,
      "loss": 0.6652,
      "step": 1634000
    },
    {
      "epoch": 14.910759909482444,
      "grad_norm": 3.3170464038848877,
      "learning_rate": 3.757436674209796e-05,
      "loss": 0.6727,
      "step": 1634100
    },
    {
      "epoch": 14.911672384845609,
      "grad_norm": 3.3461813926696777,
      "learning_rate": 3.757360634596199e-05,
      "loss": 0.6864,
      "step": 1634200
    },
    {
      "epoch": 14.912584860208774,
      "grad_norm": 4.502493858337402,
      "learning_rate": 3.757284594982602e-05,
      "loss": 0.6912,
      "step": 1634300
    },
    {
      "epoch": 14.91349733557194,
      "grad_norm": 3.4064383506774902,
      "learning_rate": 3.7572085553690053e-05,
      "loss": 0.6341,
      "step": 1634400
    },
    {
      "epoch": 14.914409810935105,
      "grad_norm": 3.8694186210632324,
      "learning_rate": 3.7571325157554083e-05,
      "loss": 0.702,
      "step": 1634500
    },
    {
      "epoch": 14.91532228629827,
      "grad_norm": 3.832899570465088,
      "learning_rate": 3.757056476141811e-05,
      "loss": 0.6761,
      "step": 1634600
    },
    {
      "epoch": 14.916234761661435,
      "grad_norm": 3.5608370304107666,
      "learning_rate": 3.7569804365282144e-05,
      "loss": 0.6746,
      "step": 1634700
    },
    {
      "epoch": 14.9171472370246,
      "grad_norm": 4.347011089324951,
      "learning_rate": 3.756904396914617e-05,
      "loss": 0.6698,
      "step": 1634800
    },
    {
      "epoch": 14.918059712387766,
      "grad_norm": 3.443274736404419,
      "learning_rate": 3.75682835730102e-05,
      "loss": 0.6777,
      "step": 1634900
    },
    {
      "epoch": 14.918972187750931,
      "grad_norm": 3.7016406059265137,
      "learning_rate": 3.756752317687423e-05,
      "loss": 0.678,
      "step": 1635000
    },
    {
      "epoch": 14.919884663114097,
      "grad_norm": 4.838671684265137,
      "learning_rate": 3.756676278073826e-05,
      "loss": 0.6835,
      "step": 1635100
    },
    {
      "epoch": 14.920797138477262,
      "grad_norm": 4.2375969886779785,
      "learning_rate": 3.756600238460228e-05,
      "loss": 0.6722,
      "step": 1635200
    },
    {
      "epoch": 14.921709613840425,
      "grad_norm": 4.142560958862305,
      "learning_rate": 3.756524198846632e-05,
      "loss": 0.6721,
      "step": 1635300
    },
    {
      "epoch": 14.92262208920359,
      "grad_norm": 4.007508277893066,
      "learning_rate": 3.756448159233034e-05,
      "loss": 0.6965,
      "step": 1635400
    },
    {
      "epoch": 14.923534564566756,
      "grad_norm": 4.326509475708008,
      "learning_rate": 3.756372119619437e-05,
      "loss": 0.6488,
      "step": 1635500
    },
    {
      "epoch": 14.924447039929921,
      "grad_norm": 3.612269639968872,
      "learning_rate": 3.75629608000584e-05,
      "loss": 0.6781,
      "step": 1635600
    },
    {
      "epoch": 14.925359515293087,
      "grad_norm": 4.337271213531494,
      "learning_rate": 3.756220040392243e-05,
      "loss": 0.6396,
      "step": 1635700
    },
    {
      "epoch": 14.926271990656252,
      "grad_norm": 3.548673152923584,
      "learning_rate": 3.756144000778646e-05,
      "loss": 0.6475,
      "step": 1635800
    },
    {
      "epoch": 14.927184466019417,
      "grad_norm": 3.6535463333129883,
      "learning_rate": 3.756067961165049e-05,
      "loss": 0.7099,
      "step": 1635900
    },
    {
      "epoch": 14.928096941382583,
      "grad_norm": 4.8770012855529785,
      "learning_rate": 3.7559919215514514e-05,
      "loss": 0.7001,
      "step": 1636000
    },
    {
      "epoch": 14.929009416745748,
      "grad_norm": 3.7036497592926025,
      "learning_rate": 3.7559158819378544e-05,
      "loss": 0.6642,
      "step": 1636100
    },
    {
      "epoch": 14.929921892108913,
      "grad_norm": 4.317211151123047,
      "learning_rate": 3.7558398423242574e-05,
      "loss": 0.6782,
      "step": 1636200
    },
    {
      "epoch": 14.930834367472078,
      "grad_norm": 3.8698318004608154,
      "learning_rate": 3.75576380271066e-05,
      "loss": 0.6376,
      "step": 1636300
    },
    {
      "epoch": 14.931746842835244,
      "grad_norm": 2.9264612197875977,
      "learning_rate": 3.7556877630970634e-05,
      "loss": 0.6805,
      "step": 1636400
    },
    {
      "epoch": 14.932659318198409,
      "grad_norm": 4.0851969718933105,
      "learning_rate": 3.755611723483466e-05,
      "loss": 0.7125,
      "step": 1636500
    },
    {
      "epoch": 14.933571793561574,
      "grad_norm": 4.301205158233643,
      "learning_rate": 3.755535683869869e-05,
      "loss": 0.6957,
      "step": 1636600
    },
    {
      "epoch": 14.93448426892474,
      "grad_norm": 4.392527103424072,
      "learning_rate": 3.755459644256272e-05,
      "loss": 0.6673,
      "step": 1636700
    },
    {
      "epoch": 14.935396744287905,
      "grad_norm": 4.803315162658691,
      "learning_rate": 3.755383604642675e-05,
      "loss": 0.6894,
      "step": 1636800
    },
    {
      "epoch": 14.936309219651069,
      "grad_norm": 4.3352437019348145,
      "learning_rate": 3.755307565029078e-05,
      "loss": 0.6528,
      "step": 1636900
    },
    {
      "epoch": 14.937221695014234,
      "grad_norm": 4.094061374664307,
      "learning_rate": 3.755231525415481e-05,
      "loss": 0.6531,
      "step": 1637000
    },
    {
      "epoch": 14.938134170377399,
      "grad_norm": 3.714350461959839,
      "learning_rate": 3.755155485801883e-05,
      "loss": 0.6585,
      "step": 1637100
    },
    {
      "epoch": 14.939046645740564,
      "grad_norm": 5.332612991333008,
      "learning_rate": 3.755079446188287e-05,
      "loss": 0.6659,
      "step": 1637200
    },
    {
      "epoch": 14.93995912110373,
      "grad_norm": 4.46309232711792,
      "learning_rate": 3.755003406574689e-05,
      "loss": 0.6971,
      "step": 1637300
    },
    {
      "epoch": 14.940871596466895,
      "grad_norm": 4.646996974945068,
      "learning_rate": 3.754927366961092e-05,
      "loss": 0.6872,
      "step": 1637400
    },
    {
      "epoch": 14.94178407183006,
      "grad_norm": 2.768646717071533,
      "learning_rate": 3.754851327347495e-05,
      "loss": 0.6677,
      "step": 1637500
    },
    {
      "epoch": 14.942696547193226,
      "grad_norm": 4.356661796569824,
      "learning_rate": 3.754775287733898e-05,
      "loss": 0.6457,
      "step": 1637600
    },
    {
      "epoch": 14.943609022556391,
      "grad_norm": 4.240054607391357,
      "learning_rate": 3.7546992481203005e-05,
      "loss": 0.6388,
      "step": 1637700
    },
    {
      "epoch": 14.944521497919556,
      "grad_norm": 4.700239181518555,
      "learning_rate": 3.754623208506704e-05,
      "loss": 0.6636,
      "step": 1637800
    },
    {
      "epoch": 14.945433973282721,
      "grad_norm": 3.4235458374023438,
      "learning_rate": 3.7545471688931065e-05,
      "loss": 0.6377,
      "step": 1637900
    },
    {
      "epoch": 14.946346448645887,
      "grad_norm": 4.248846054077148,
      "learning_rate": 3.7544711292795095e-05,
      "loss": 0.7074,
      "step": 1638000
    },
    {
      "epoch": 14.947258924009052,
      "grad_norm": 3.672483444213867,
      "learning_rate": 3.7543950896659125e-05,
      "loss": 0.6683,
      "step": 1638100
    },
    {
      "epoch": 14.948171399372217,
      "grad_norm": 4.263418674468994,
      "learning_rate": 3.7543190500523155e-05,
      "loss": 0.701,
      "step": 1638200
    },
    {
      "epoch": 14.949083874735383,
      "grad_norm": 3.5198686122894287,
      "learning_rate": 3.7542430104387185e-05,
      "loss": 0.6951,
      "step": 1638300
    },
    {
      "epoch": 14.949996350098548,
      "grad_norm": 3.3775742053985596,
      "learning_rate": 3.7541669708251215e-05,
      "loss": 0.6992,
      "step": 1638400
    },
    {
      "epoch": 14.950908825461713,
      "grad_norm": 4.259331226348877,
      "learning_rate": 3.754090931211524e-05,
      "loss": 0.7124,
      "step": 1638500
    },
    {
      "epoch": 14.951821300824879,
      "grad_norm": 4.509274959564209,
      "learning_rate": 3.7540148915979276e-05,
      "loss": 0.6134,
      "step": 1638600
    },
    {
      "epoch": 14.952733776188042,
      "grad_norm": 2.76060152053833,
      "learning_rate": 3.75393885198433e-05,
      "loss": 0.6819,
      "step": 1638700
    },
    {
      "epoch": 14.953646251551207,
      "grad_norm": 4.224277973175049,
      "learning_rate": 3.753862812370733e-05,
      "loss": 0.6908,
      "step": 1638800
    },
    {
      "epoch": 14.954558726914373,
      "grad_norm": 3.392700433731079,
      "learning_rate": 3.753786772757136e-05,
      "loss": 0.6715,
      "step": 1638900
    },
    {
      "epoch": 14.955471202277538,
      "grad_norm": 3.1084787845611572,
      "learning_rate": 3.753710733143538e-05,
      "loss": 0.6455,
      "step": 1639000
    },
    {
      "epoch": 14.956383677640703,
      "grad_norm": 4.30552339553833,
      "learning_rate": 3.753634693529941e-05,
      "loss": 0.6942,
      "step": 1639100
    },
    {
      "epoch": 14.957296153003869,
      "grad_norm": 3.0884554386138916,
      "learning_rate": 3.753558653916344e-05,
      "loss": 0.6966,
      "step": 1639200
    },
    {
      "epoch": 14.958208628367034,
      "grad_norm": 3.046433210372925,
      "learning_rate": 3.753482614302747e-05,
      "loss": 0.6853,
      "step": 1639300
    },
    {
      "epoch": 14.9591211037302,
      "grad_norm": 3.831265687942505,
      "learning_rate": 3.75340657468915e-05,
      "loss": 0.6694,
      "step": 1639400
    },
    {
      "epoch": 14.960033579093365,
      "grad_norm": 2.933983564376831,
      "learning_rate": 3.753330535075553e-05,
      "loss": 0.685,
      "step": 1639500
    },
    {
      "epoch": 14.96094605445653,
      "grad_norm": 3.537619113922119,
      "learning_rate": 3.7532544954619556e-05,
      "loss": 0.6419,
      "step": 1639600
    },
    {
      "epoch": 14.961858529819695,
      "grad_norm": 4.496822834014893,
      "learning_rate": 3.753178455848359e-05,
      "loss": 0.6926,
      "step": 1639700
    },
    {
      "epoch": 14.96277100518286,
      "grad_norm": 3.6933209896087646,
      "learning_rate": 3.7531024162347616e-05,
      "loss": 0.6501,
      "step": 1639800
    },
    {
      "epoch": 14.963683480546026,
      "grad_norm": 4.132315158843994,
      "learning_rate": 3.7530263766211646e-05,
      "loss": 0.6563,
      "step": 1639900
    },
    {
      "epoch": 14.964595955909191,
      "grad_norm": 3.7701826095581055,
      "learning_rate": 3.7529503370075676e-05,
      "loss": 0.6533,
      "step": 1640000
    },
    {
      "epoch": 14.965508431272356,
      "grad_norm": 3.772038221359253,
      "learning_rate": 3.7528742973939706e-05,
      "loss": 0.6803,
      "step": 1640100
    },
    {
      "epoch": 14.966420906635522,
      "grad_norm": 4.296914100646973,
      "learning_rate": 3.7527982577803736e-05,
      "loss": 0.6847,
      "step": 1640200
    },
    {
      "epoch": 14.967333381998685,
      "grad_norm": 5.264383792877197,
      "learning_rate": 3.7527222181667766e-05,
      "loss": 0.6947,
      "step": 1640300
    },
    {
      "epoch": 14.96824585736185,
      "grad_norm": 4.550229072570801,
      "learning_rate": 3.752646178553179e-05,
      "loss": 0.6661,
      "step": 1640400
    },
    {
      "epoch": 14.969158332725016,
      "grad_norm": 3.744051456451416,
      "learning_rate": 3.7525701389395827e-05,
      "loss": 0.6936,
      "step": 1640500
    },
    {
      "epoch": 14.970070808088181,
      "grad_norm": 3.7595937252044678,
      "learning_rate": 3.752494099325985e-05,
      "loss": 0.677,
      "step": 1640600
    },
    {
      "epoch": 14.970983283451346,
      "grad_norm": 4.392617702484131,
      "learning_rate": 3.752418059712388e-05,
      "loss": 0.6939,
      "step": 1640700
    },
    {
      "epoch": 14.971895758814512,
      "grad_norm": 3.4043092727661133,
      "learning_rate": 3.752342020098791e-05,
      "loss": 0.6732,
      "step": 1640800
    },
    {
      "epoch": 14.972808234177677,
      "grad_norm": 3.9960575103759766,
      "learning_rate": 3.752265980485194e-05,
      "loss": 0.6734,
      "step": 1640900
    },
    {
      "epoch": 14.973720709540842,
      "grad_norm": 4.490583419799805,
      "learning_rate": 3.752189940871596e-05,
      "loss": 0.6306,
      "step": 1641000
    },
    {
      "epoch": 14.974633184904008,
      "grad_norm": 2.836120843887329,
      "learning_rate": 3.752113901258e-05,
      "loss": 0.676,
      "step": 1641100
    },
    {
      "epoch": 14.975545660267173,
      "grad_norm": 4.422893047332764,
      "learning_rate": 3.7520378616444023e-05,
      "loss": 0.7013,
      "step": 1641200
    },
    {
      "epoch": 14.976458135630338,
      "grad_norm": 3.7758240699768066,
      "learning_rate": 3.7519618220308054e-05,
      "loss": 0.6719,
      "step": 1641300
    },
    {
      "epoch": 14.977370610993503,
      "grad_norm": 3.6869382858276367,
      "learning_rate": 3.7518857824172084e-05,
      "loss": 0.688,
      "step": 1641400
    },
    {
      "epoch": 14.978283086356669,
      "grad_norm": 2.195268154144287,
      "learning_rate": 3.7518097428036114e-05,
      "loss": 0.7073,
      "step": 1641500
    },
    {
      "epoch": 14.979195561719834,
      "grad_norm": 3.598076105117798,
      "learning_rate": 3.7517337031900144e-05,
      "loss": 0.6826,
      "step": 1641600
    },
    {
      "epoch": 14.980108037083,
      "grad_norm": 4.1178131103515625,
      "learning_rate": 3.751657663576417e-05,
      "loss": 0.6485,
      "step": 1641700
    },
    {
      "epoch": 14.981020512446165,
      "grad_norm": 3.642573595046997,
      "learning_rate": 3.75158162396282e-05,
      "loss": 0.6679,
      "step": 1641800
    },
    {
      "epoch": 14.98193298780933,
      "grad_norm": 3.184159517288208,
      "learning_rate": 3.751505584349223e-05,
      "loss": 0.6926,
      "step": 1641900
    },
    {
      "epoch": 14.982845463172495,
      "grad_norm": 4.141359329223633,
      "learning_rate": 3.751429544735626e-05,
      "loss": 0.6594,
      "step": 1642000
    },
    {
      "epoch": 14.983757938535659,
      "grad_norm": 3.910710096359253,
      "learning_rate": 3.751353505122028e-05,
      "loss": 0.6508,
      "step": 1642100
    },
    {
      "epoch": 14.984670413898824,
      "grad_norm": 4.027247905731201,
      "learning_rate": 3.751277465508432e-05,
      "loss": 0.6755,
      "step": 1642200
    },
    {
      "epoch": 14.98558288926199,
      "grad_norm": 2.934800386428833,
      "learning_rate": 3.751201425894834e-05,
      "loss": 0.7001,
      "step": 1642300
    },
    {
      "epoch": 14.986495364625155,
      "grad_norm": 4.638821601867676,
      "learning_rate": 3.751125386281237e-05,
      "loss": 0.6931,
      "step": 1642400
    },
    {
      "epoch": 14.98740783998832,
      "grad_norm": 4.158967018127441,
      "learning_rate": 3.75104934666764e-05,
      "loss": 0.6677,
      "step": 1642500
    },
    {
      "epoch": 14.988320315351485,
      "grad_norm": 3.619443655014038,
      "learning_rate": 3.750973307054043e-05,
      "loss": 0.6641,
      "step": 1642600
    },
    {
      "epoch": 14.98923279071465,
      "grad_norm": 3.384876251220703,
      "learning_rate": 3.750897267440446e-05,
      "loss": 0.6534,
      "step": 1642700
    },
    {
      "epoch": 14.990145266077816,
      "grad_norm": 3.436225414276123,
      "learning_rate": 3.750821227826849e-05,
      "loss": 0.667,
      "step": 1642800
    },
    {
      "epoch": 14.991057741440981,
      "grad_norm": 4.310398578643799,
      "learning_rate": 3.7507451882132514e-05,
      "loss": 0.6258,
      "step": 1642900
    },
    {
      "epoch": 14.991970216804146,
      "grad_norm": 5.231319427490234,
      "learning_rate": 3.750669148599655e-05,
      "loss": 0.6491,
      "step": 1643000
    },
    {
      "epoch": 14.992882692167312,
      "grad_norm": 4.3231730461120605,
      "learning_rate": 3.7505931089860574e-05,
      "loss": 0.6994,
      "step": 1643100
    },
    {
      "epoch": 14.993795167530477,
      "grad_norm": 3.061622381210327,
      "learning_rate": 3.7505170693724604e-05,
      "loss": 0.6577,
      "step": 1643200
    },
    {
      "epoch": 14.994707642893642,
      "grad_norm": 2.8019731044769287,
      "learning_rate": 3.7504410297588635e-05,
      "loss": 0.6265,
      "step": 1643300
    },
    {
      "epoch": 14.995620118256808,
      "grad_norm": 3.572751522064209,
      "learning_rate": 3.7503649901452665e-05,
      "loss": 0.6619,
      "step": 1643400
    },
    {
      "epoch": 14.996532593619973,
      "grad_norm": 3.3292746543884277,
      "learning_rate": 3.750288950531669e-05,
      "loss": 0.6768,
      "step": 1643500
    },
    {
      "epoch": 14.997445068983138,
      "grad_norm": 4.369747161865234,
      "learning_rate": 3.7502129109180725e-05,
      "loss": 0.663,
      "step": 1643600
    },
    {
      "epoch": 14.998357544346302,
      "grad_norm": 3.5288193225860596,
      "learning_rate": 3.750136871304475e-05,
      "loss": 0.6666,
      "step": 1643700
    },
    {
      "epoch": 14.999270019709467,
      "grad_norm": 3.5071475505828857,
      "learning_rate": 3.750060831690878e-05,
      "loss": 0.6751,
      "step": 1643800
    },
    {
      "epoch": 15.0,
      "eval_loss": 0.5464866757392883,
      "eval_runtime": 31.483,
      "eval_samples_per_second": 183.242,
      "eval_steps_per_second": 183.242,
      "step": 1643880
    },
    {
      "epoch": 15.0,
      "eval_loss": 0.5245641469955444,
      "eval_runtime": 593.0814,
      "eval_samples_per_second": 184.784,
      "eval_steps_per_second": 184.784,
      "step": 1643880
    },
    {
      "epoch": 15.000182495072632,
      "grad_norm": 3.9745430946350098,
      "learning_rate": 3.749984792077281e-05,
      "loss": 0.6447,
      "step": 1643900
    },
    {
      "epoch": 15.001094970435798,
      "grad_norm": 4.1940155029296875,
      "learning_rate": 3.749908752463684e-05,
      "loss": 0.6504,
      "step": 1644000
    },
    {
      "epoch": 15.002007445798963,
      "grad_norm": 4.041975975036621,
      "learning_rate": 3.749832712850087e-05,
      "loss": 0.6746,
      "step": 1644100
    },
    {
      "epoch": 15.002919921162128,
      "grad_norm": 3.3004961013793945,
      "learning_rate": 3.74975667323649e-05,
      "loss": 0.6928,
      "step": 1644200
    },
    {
      "epoch": 15.003832396525294,
      "grad_norm": 2.7591042518615723,
      "learning_rate": 3.749680633622892e-05,
      "loss": 0.6514,
      "step": 1644300
    },
    {
      "epoch": 15.004744871888459,
      "grad_norm": 4.165473937988281,
      "learning_rate": 3.749604594009296e-05,
      "loss": 0.6595,
      "step": 1644400
    },
    {
      "epoch": 15.005657347251624,
      "grad_norm": 3.9614670276641846,
      "learning_rate": 3.749528554395698e-05,
      "loss": 0.6954,
      "step": 1644500
    },
    {
      "epoch": 15.00656982261479,
      "grad_norm": 3.7264726161956787,
      "learning_rate": 3.7494525147821005e-05,
      "loss": 0.6473,
      "step": 1644600
    },
    {
      "epoch": 15.007482297977955,
      "grad_norm": 3.628959894180298,
      "learning_rate": 3.749376475168504e-05,
      "loss": 0.6769,
      "step": 1644700
    },
    {
      "epoch": 15.00839477334112,
      "grad_norm": 4.1550068855285645,
      "learning_rate": 3.7493004355549065e-05,
      "loss": 0.6691,
      "step": 1644800
    },
    {
      "epoch": 15.009307248704285,
      "grad_norm": 3.45296311378479,
      "learning_rate": 3.7492243959413095e-05,
      "loss": 0.6804,
      "step": 1644900
    },
    {
      "epoch": 15.01021972406745,
      "grad_norm": 3.5978169441223145,
      "learning_rate": 3.7491483563277125e-05,
      "loss": 0.6413,
      "step": 1645000
    },
    {
      "epoch": 15.011132199430616,
      "grad_norm": 5.475029468536377,
      "learning_rate": 3.7490723167141155e-05,
      "loss": 0.6457,
      "step": 1645100
    },
    {
      "epoch": 15.012044674793781,
      "grad_norm": 3.6641180515289307,
      "learning_rate": 3.7489962771005185e-05,
      "loss": 0.6974,
      "step": 1645200
    },
    {
      "epoch": 15.012957150156947,
      "grad_norm": 6.069437026977539,
      "learning_rate": 3.7489202374869216e-05,
      "loss": 0.649,
      "step": 1645300
    },
    {
      "epoch": 15.013869625520112,
      "grad_norm": 2.9623191356658936,
      "learning_rate": 3.748844197873324e-05,
      "loss": 0.6775,
      "step": 1645400
    },
    {
      "epoch": 15.014782100883275,
      "grad_norm": 3.7496631145477295,
      "learning_rate": 3.7487681582597276e-05,
      "loss": 0.6553,
      "step": 1645500
    },
    {
      "epoch": 15.01569457624644,
      "grad_norm": 4.984538555145264,
      "learning_rate": 3.74869211864613e-05,
      "loss": 0.6321,
      "step": 1645600
    },
    {
      "epoch": 15.016607051609606,
      "grad_norm": 3.450021266937256,
      "learning_rate": 3.748616079032533e-05,
      "loss": 0.6364,
      "step": 1645700
    },
    {
      "epoch": 15.017519526972771,
      "grad_norm": 5.061356067657471,
      "learning_rate": 3.748540039418936e-05,
      "loss": 0.7351,
      "step": 1645800
    },
    {
      "epoch": 15.018432002335937,
      "grad_norm": 3.936152935028076,
      "learning_rate": 3.748463999805339e-05,
      "loss": 0.7149,
      "step": 1645900
    },
    {
      "epoch": 15.019344477699102,
      "grad_norm": 3.86555814743042,
      "learning_rate": 3.748387960191741e-05,
      "loss": 0.6755,
      "step": 1646000
    },
    {
      "epoch": 15.020256953062267,
      "grad_norm": 2.2030467987060547,
      "learning_rate": 3.748311920578145e-05,
      "loss": 0.7147,
      "step": 1646100
    },
    {
      "epoch": 15.021169428425432,
      "grad_norm": 3.5754146575927734,
      "learning_rate": 3.748235880964547e-05,
      "loss": 0.6835,
      "step": 1646200
    },
    {
      "epoch": 15.022081903788598,
      "grad_norm": 4.126375198364258,
      "learning_rate": 3.74815984135095e-05,
      "loss": 0.6761,
      "step": 1646300
    },
    {
      "epoch": 15.022994379151763,
      "grad_norm": 4.435609340667725,
      "learning_rate": 3.748083801737353e-05,
      "loss": 0.6734,
      "step": 1646400
    },
    {
      "epoch": 15.023906854514928,
      "grad_norm": 3.2352142333984375,
      "learning_rate": 3.748007762123756e-05,
      "loss": 0.6725,
      "step": 1646500
    },
    {
      "epoch": 15.024819329878094,
      "grad_norm": 4.745031356811523,
      "learning_rate": 3.747931722510159e-05,
      "loss": 0.6519,
      "step": 1646600
    },
    {
      "epoch": 15.025731805241259,
      "grad_norm": 2.972665309906006,
      "learning_rate": 3.747855682896562e-05,
      "loss": 0.6252,
      "step": 1646700
    },
    {
      "epoch": 15.026644280604424,
      "grad_norm": 4.8128862380981445,
      "learning_rate": 3.7477796432829646e-05,
      "loss": 0.6829,
      "step": 1646800
    },
    {
      "epoch": 15.02755675596759,
      "grad_norm": 4.505306243896484,
      "learning_rate": 3.747703603669368e-05,
      "loss": 0.7018,
      "step": 1646900
    },
    {
      "epoch": 15.028469231330755,
      "grad_norm": 3.776211738586426,
      "learning_rate": 3.7476275640557706e-05,
      "loss": 0.6955,
      "step": 1647000
    },
    {
      "epoch": 15.029381706693918,
      "grad_norm": 3.98677134513855,
      "learning_rate": 3.7475515244421736e-05,
      "loss": 0.6623,
      "step": 1647100
    },
    {
      "epoch": 15.030294182057084,
      "grad_norm": 4.013258457183838,
      "learning_rate": 3.7474754848285766e-05,
      "loss": 0.6527,
      "step": 1647200
    },
    {
      "epoch": 15.031206657420249,
      "grad_norm": 3.69482421875,
      "learning_rate": 3.747399445214979e-05,
      "loss": 0.668,
      "step": 1647300
    },
    {
      "epoch": 15.032119132783414,
      "grad_norm": 4.495741367340088,
      "learning_rate": 3.747323405601382e-05,
      "loss": 0.6649,
      "step": 1647400
    },
    {
      "epoch": 15.03303160814658,
      "grad_norm": 3.903951406478882,
      "learning_rate": 3.747247365987785e-05,
      "loss": 0.6616,
      "step": 1647500
    },
    {
      "epoch": 15.033944083509745,
      "grad_norm": 4.050478458404541,
      "learning_rate": 3.747171326374188e-05,
      "loss": 0.6711,
      "step": 1647600
    },
    {
      "epoch": 15.03485655887291,
      "grad_norm": 3.799562931060791,
      "learning_rate": 3.747095286760591e-05,
      "loss": 0.6661,
      "step": 1647700
    },
    {
      "epoch": 15.035769034236075,
      "grad_norm": 3.4560322761535645,
      "learning_rate": 3.747019247146994e-05,
      "loss": 0.6569,
      "step": 1647800
    },
    {
      "epoch": 15.03668150959924,
      "grad_norm": 3.9685564041137695,
      "learning_rate": 3.7469432075333963e-05,
      "loss": 0.6627,
      "step": 1647900
    },
    {
      "epoch": 15.037593984962406,
      "grad_norm": 5.119528770446777,
      "learning_rate": 3.7468671679198e-05,
      "loss": 0.6583,
      "step": 1648000
    },
    {
      "epoch": 15.038506460325571,
      "grad_norm": 4.207850456237793,
      "learning_rate": 3.7467911283062024e-05,
      "loss": 0.6493,
      "step": 1648100
    },
    {
      "epoch": 15.039418935688737,
      "grad_norm": 3.504180669784546,
      "learning_rate": 3.7467150886926054e-05,
      "loss": 0.7045,
      "step": 1648200
    },
    {
      "epoch": 15.040331411051902,
      "grad_norm": 4.811005115509033,
      "learning_rate": 3.7466390490790084e-05,
      "loss": 0.6815,
      "step": 1648300
    },
    {
      "epoch": 15.041243886415067,
      "grad_norm": 3.149782419204712,
      "learning_rate": 3.7465630094654114e-05,
      "loss": 0.616,
      "step": 1648400
    },
    {
      "epoch": 15.042156361778233,
      "grad_norm": 4.287753582000732,
      "learning_rate": 3.746486969851814e-05,
      "loss": 0.657,
      "step": 1648500
    },
    {
      "epoch": 15.043068837141398,
      "grad_norm": 4.0116376876831055,
      "learning_rate": 3.7464109302382174e-05,
      "loss": 0.6834,
      "step": 1648600
    },
    {
      "epoch": 15.043981312504563,
      "grad_norm": 2.7752749919891357,
      "learning_rate": 3.74633489062462e-05,
      "loss": 0.6543,
      "step": 1648700
    },
    {
      "epoch": 15.044893787867728,
      "grad_norm": 2.807832956314087,
      "learning_rate": 3.746258851011023e-05,
      "loss": 0.6857,
      "step": 1648800
    },
    {
      "epoch": 15.045806263230892,
      "grad_norm": 4.241234302520752,
      "learning_rate": 3.746182811397426e-05,
      "loss": 0.6514,
      "step": 1648900
    },
    {
      "epoch": 15.046718738594057,
      "grad_norm": 4.334609031677246,
      "learning_rate": 3.746106771783829e-05,
      "loss": 0.6936,
      "step": 1649000
    },
    {
      "epoch": 15.047631213957223,
      "grad_norm": 4.561577320098877,
      "learning_rate": 3.746030732170232e-05,
      "loss": 0.6635,
      "step": 1649100
    },
    {
      "epoch": 15.048543689320388,
      "grad_norm": 4.141337871551514,
      "learning_rate": 3.745954692556635e-05,
      "loss": 0.637,
      "step": 1649200
    },
    {
      "epoch": 15.049456164683553,
      "grad_norm": 3.047088623046875,
      "learning_rate": 3.745878652943037e-05,
      "loss": 0.6788,
      "step": 1649300
    },
    {
      "epoch": 15.050368640046718,
      "grad_norm": 3.1380910873413086,
      "learning_rate": 3.745802613329441e-05,
      "loss": 0.659,
      "step": 1649400
    },
    {
      "epoch": 15.051281115409884,
      "grad_norm": 5.078658103942871,
      "learning_rate": 3.745726573715843e-05,
      "loss": 0.6762,
      "step": 1649500
    },
    {
      "epoch": 15.052193590773049,
      "grad_norm": 3.635352611541748,
      "learning_rate": 3.745650534102246e-05,
      "loss": 0.6543,
      "step": 1649600
    },
    {
      "epoch": 15.053106066136214,
      "grad_norm": 4.356924533843994,
      "learning_rate": 3.745574494488649e-05,
      "loss": 0.6526,
      "step": 1649700
    },
    {
      "epoch": 15.05401854149938,
      "grad_norm": 3.277200698852539,
      "learning_rate": 3.745498454875052e-05,
      "loss": 0.6499,
      "step": 1649800
    },
    {
      "epoch": 15.054931016862545,
      "grad_norm": 4.096712589263916,
      "learning_rate": 3.7454224152614544e-05,
      "loss": 0.6986,
      "step": 1649900
    },
    {
      "epoch": 15.05584349222571,
      "grad_norm": 3.804630756378174,
      "learning_rate": 3.745346375647858e-05,
      "loss": 0.6459,
      "step": 1650000
    },
    {
      "epoch": 15.056755967588876,
      "grad_norm": 3.8906803131103516,
      "learning_rate": 3.7452703360342605e-05,
      "loss": 0.6556,
      "step": 1650100
    },
    {
      "epoch": 15.05766844295204,
      "grad_norm": 3.6082613468170166,
      "learning_rate": 3.7451942964206635e-05,
      "loss": 0.6703,
      "step": 1650200
    },
    {
      "epoch": 15.058580918315206,
      "grad_norm": 4.552479267120361,
      "learning_rate": 3.7451182568070665e-05,
      "loss": 0.6706,
      "step": 1650300
    },
    {
      "epoch": 15.059493393678371,
      "grad_norm": 3.624911308288574,
      "learning_rate": 3.745042217193469e-05,
      "loss": 0.7249,
      "step": 1650400
    },
    {
      "epoch": 15.060405869041535,
      "grad_norm": 3.780888557434082,
      "learning_rate": 3.7449661775798725e-05,
      "loss": 0.6847,
      "step": 1650500
    },
    {
      "epoch": 15.0613183444047,
      "grad_norm": 3.122999429702759,
      "learning_rate": 3.744890137966275e-05,
      "loss": 0.6701,
      "step": 1650600
    },
    {
      "epoch": 15.062230819767866,
      "grad_norm": 4.103880405426025,
      "learning_rate": 3.744814098352678e-05,
      "loss": 0.6983,
      "step": 1650700
    },
    {
      "epoch": 15.063143295131031,
      "grad_norm": 3.796278476715088,
      "learning_rate": 3.744738058739081e-05,
      "loss": 0.6921,
      "step": 1650800
    },
    {
      "epoch": 15.064055770494196,
      "grad_norm": 4.530433177947998,
      "learning_rate": 3.744662019125484e-05,
      "loss": 0.6996,
      "step": 1650900
    },
    {
      "epoch": 15.064968245857361,
      "grad_norm": 3.7592899799346924,
      "learning_rate": 3.744585979511887e-05,
      "loss": 0.6849,
      "step": 1651000
    },
    {
      "epoch": 15.065880721220527,
      "grad_norm": 1.4413725137710571,
      "learning_rate": 3.74450993989829e-05,
      "loss": 0.6412,
      "step": 1651100
    },
    {
      "epoch": 15.066793196583692,
      "grad_norm": 4.105262279510498,
      "learning_rate": 3.744433900284692e-05,
      "loss": 0.6777,
      "step": 1651200
    },
    {
      "epoch": 15.067705671946857,
      "grad_norm": 4.136904239654541,
      "learning_rate": 3.744357860671095e-05,
      "loss": 0.6553,
      "step": 1651300
    },
    {
      "epoch": 15.068618147310023,
      "grad_norm": 5.2295308113098145,
      "learning_rate": 3.744281821057498e-05,
      "loss": 0.668,
      "step": 1651400
    },
    {
      "epoch": 15.069530622673188,
      "grad_norm": 3.4502134323120117,
      "learning_rate": 3.744205781443901e-05,
      "loss": 0.6849,
      "step": 1651500
    },
    {
      "epoch": 15.070443098036353,
      "grad_norm": 4.041482448577881,
      "learning_rate": 3.744129741830304e-05,
      "loss": 0.6433,
      "step": 1651600
    },
    {
      "epoch": 15.071355573399519,
      "grad_norm": 3.6014909744262695,
      "learning_rate": 3.744053702216707e-05,
      "loss": 0.6292,
      "step": 1651700
    },
    {
      "epoch": 15.072268048762684,
      "grad_norm": 4.226203918457031,
      "learning_rate": 3.7439776626031095e-05,
      "loss": 0.687,
      "step": 1651800
    },
    {
      "epoch": 15.07318052412585,
      "grad_norm": 4.376981735229492,
      "learning_rate": 3.743901622989513e-05,
      "loss": 0.6845,
      "step": 1651900
    },
    {
      "epoch": 15.074092999489014,
      "grad_norm": 4.635516166687012,
      "learning_rate": 3.7438255833759156e-05,
      "loss": 0.667,
      "step": 1652000
    },
    {
      "epoch": 15.07500547485218,
      "grad_norm": 2.8622944355010986,
      "learning_rate": 3.7437495437623186e-05,
      "loss": 0.6821,
      "step": 1652100
    },
    {
      "epoch": 15.075917950215343,
      "grad_norm": 3.884028673171997,
      "learning_rate": 3.7436735041487216e-05,
      "loss": 0.6486,
      "step": 1652200
    },
    {
      "epoch": 15.076830425578509,
      "grad_norm": 4.069635391235352,
      "learning_rate": 3.7435974645351246e-05,
      "loss": 0.6439,
      "step": 1652300
    },
    {
      "epoch": 15.077742900941674,
      "grad_norm": 4.177715301513672,
      "learning_rate": 3.7435214249215276e-05,
      "loss": 0.6737,
      "step": 1652400
    },
    {
      "epoch": 15.07865537630484,
      "grad_norm": 4.655325889587402,
      "learning_rate": 3.7434453853079306e-05,
      "loss": 0.6435,
      "step": 1652500
    },
    {
      "epoch": 15.079567851668005,
      "grad_norm": 3.23112416267395,
      "learning_rate": 3.743369345694333e-05,
      "loss": 0.6337,
      "step": 1652600
    },
    {
      "epoch": 15.08048032703117,
      "grad_norm": 3.4743459224700928,
      "learning_rate": 3.743293306080736e-05,
      "loss": 0.6326,
      "step": 1652700
    },
    {
      "epoch": 15.081392802394335,
      "grad_norm": 3.7248668670654297,
      "learning_rate": 3.743217266467139e-05,
      "loss": 0.6749,
      "step": 1652800
    },
    {
      "epoch": 15.0823052777575,
      "grad_norm": 3.4484164714813232,
      "learning_rate": 3.743141226853542e-05,
      "loss": 0.6974,
      "step": 1652900
    },
    {
      "epoch": 15.083217753120666,
      "grad_norm": 3.155636787414551,
      "learning_rate": 3.743065187239945e-05,
      "loss": 0.6153,
      "step": 1653000
    },
    {
      "epoch": 15.084130228483831,
      "grad_norm": 3.927957773208618,
      "learning_rate": 3.742989147626347e-05,
      "loss": 0.6677,
      "step": 1653100
    },
    {
      "epoch": 15.085042703846996,
      "grad_norm": 4.113211154937744,
      "learning_rate": 3.74291310801275e-05,
      "loss": 0.7074,
      "step": 1653200
    },
    {
      "epoch": 15.085955179210162,
      "grad_norm": 3.7140326499938965,
      "learning_rate": 3.742837068399153e-05,
      "loss": 0.7035,
      "step": 1653300
    },
    {
      "epoch": 15.086867654573327,
      "grad_norm": 3.5814626216888428,
      "learning_rate": 3.742761028785556e-05,
      "loss": 0.6655,
      "step": 1653400
    },
    {
      "epoch": 15.087780129936492,
      "grad_norm": 1.9285202026367188,
      "learning_rate": 3.742684989171959e-05,
      "loss": 0.6382,
      "step": 1653500
    },
    {
      "epoch": 15.088692605299658,
      "grad_norm": 4.173547744750977,
      "learning_rate": 3.742608949558362e-05,
      "loss": 0.661,
      "step": 1653600
    },
    {
      "epoch": 15.089605080662823,
      "grad_norm": 3.646343946456909,
      "learning_rate": 3.7425329099447646e-05,
      "loss": 0.6644,
      "step": 1653700
    },
    {
      "epoch": 15.090517556025988,
      "grad_norm": 4.797693729400635,
      "learning_rate": 3.742456870331168e-05,
      "loss": 0.668,
      "step": 1653800
    },
    {
      "epoch": 15.091430031389152,
      "grad_norm": 4.247889041900635,
      "learning_rate": 3.7423808307175706e-05,
      "loss": 0.6928,
      "step": 1653900
    },
    {
      "epoch": 15.092342506752317,
      "grad_norm": 3.704819440841675,
      "learning_rate": 3.7423047911039737e-05,
      "loss": 0.6242,
      "step": 1654000
    },
    {
      "epoch": 15.093254982115482,
      "grad_norm": 4.746379375457764,
      "learning_rate": 3.7422287514903767e-05,
      "loss": 0.671,
      "step": 1654100
    },
    {
      "epoch": 15.094167457478648,
      "grad_norm": 4.3331475257873535,
      "learning_rate": 3.74215271187678e-05,
      "loss": 0.6801,
      "step": 1654200
    },
    {
      "epoch": 15.095079932841813,
      "grad_norm": 4.400671005249023,
      "learning_rate": 3.742076672263182e-05,
      "loss": 0.6259,
      "step": 1654300
    },
    {
      "epoch": 15.095992408204978,
      "grad_norm": 4.140768527984619,
      "learning_rate": 3.742000632649586e-05,
      "loss": 0.6669,
      "step": 1654400
    },
    {
      "epoch": 15.096904883568143,
      "grad_norm": 4.111525058746338,
      "learning_rate": 3.741924593035988e-05,
      "loss": 0.6823,
      "step": 1654500
    },
    {
      "epoch": 15.097817358931309,
      "grad_norm": 3.435469150543213,
      "learning_rate": 3.741848553422391e-05,
      "loss": 0.6942,
      "step": 1654600
    },
    {
      "epoch": 15.098729834294474,
      "grad_norm": 3.7857754230499268,
      "learning_rate": 3.741772513808794e-05,
      "loss": 0.655,
      "step": 1654700
    },
    {
      "epoch": 15.09964230965764,
      "grad_norm": 3.965815305709839,
      "learning_rate": 3.741696474195197e-05,
      "loss": 0.707,
      "step": 1654800
    },
    {
      "epoch": 15.100554785020805,
      "grad_norm": 4.747468948364258,
      "learning_rate": 3.7416204345816e-05,
      "loss": 0.6586,
      "step": 1654900
    },
    {
      "epoch": 15.10146726038397,
      "grad_norm": 3.723924160003662,
      "learning_rate": 3.741544394968003e-05,
      "loss": 0.6746,
      "step": 1655000
    },
    {
      "epoch": 15.102379735747135,
      "grad_norm": 3.2934153079986572,
      "learning_rate": 3.7414683553544054e-05,
      "loss": 0.6503,
      "step": 1655100
    },
    {
      "epoch": 15.1032922111103,
      "grad_norm": 3.9914252758026123,
      "learning_rate": 3.741392315740809e-05,
      "loss": 0.6231,
      "step": 1655200
    },
    {
      "epoch": 15.104204686473466,
      "grad_norm": 4.255397796630859,
      "learning_rate": 3.7413162761272114e-05,
      "loss": 0.6363,
      "step": 1655300
    },
    {
      "epoch": 15.105117161836631,
      "grad_norm": 2.790213108062744,
      "learning_rate": 3.7412402365136144e-05,
      "loss": 0.6387,
      "step": 1655400
    },
    {
      "epoch": 15.106029637199796,
      "grad_norm": 4.384220123291016,
      "learning_rate": 3.7411641969000174e-05,
      "loss": 0.6558,
      "step": 1655500
    },
    {
      "epoch": 15.10694211256296,
      "grad_norm": 3.816805124282837,
      "learning_rate": 3.7410881572864204e-05,
      "loss": 0.6434,
      "step": 1655600
    },
    {
      "epoch": 15.107854587926125,
      "grad_norm": 4.213472843170166,
      "learning_rate": 3.741012117672823e-05,
      "loss": 0.6417,
      "step": 1655700
    },
    {
      "epoch": 15.10876706328929,
      "grad_norm": 3.078731060028076,
      "learning_rate": 3.7409360780592264e-05,
      "loss": 0.6496,
      "step": 1655800
    },
    {
      "epoch": 15.109679538652456,
      "grad_norm": 3.8533339500427246,
      "learning_rate": 3.740860038445629e-05,
      "loss": 0.6601,
      "step": 1655900
    },
    {
      "epoch": 15.110592014015621,
      "grad_norm": 3.6278512477874756,
      "learning_rate": 3.740783998832032e-05,
      "loss": 0.664,
      "step": 1656000
    },
    {
      "epoch": 15.111504489378786,
      "grad_norm": 4.409103870391846,
      "learning_rate": 3.740707959218435e-05,
      "loss": 0.6443,
      "step": 1656100
    },
    {
      "epoch": 15.112416964741952,
      "grad_norm": 4.070230960845947,
      "learning_rate": 3.740631919604837e-05,
      "loss": 0.6719,
      "step": 1656200
    },
    {
      "epoch": 15.113329440105117,
      "grad_norm": 4.075311660766602,
      "learning_rate": 3.740555879991241e-05,
      "loss": 0.6648,
      "step": 1656300
    },
    {
      "epoch": 15.114241915468282,
      "grad_norm": 3.9133782386779785,
      "learning_rate": 3.740479840377643e-05,
      "loss": 0.6862,
      "step": 1656400
    },
    {
      "epoch": 15.115154390831448,
      "grad_norm": 3.265115261077881,
      "learning_rate": 3.740403800764046e-05,
      "loss": 0.6876,
      "step": 1656500
    },
    {
      "epoch": 15.116066866194613,
      "grad_norm": 4.145989418029785,
      "learning_rate": 3.740327761150449e-05,
      "loss": 0.6832,
      "step": 1656600
    },
    {
      "epoch": 15.116979341557778,
      "grad_norm": 4.950442314147949,
      "learning_rate": 3.740251721536852e-05,
      "loss": 0.656,
      "step": 1656700
    },
    {
      "epoch": 15.117891816920944,
      "grad_norm": 3.6720993518829346,
      "learning_rate": 3.7401756819232545e-05,
      "loss": 0.6912,
      "step": 1656800
    },
    {
      "epoch": 15.118804292284109,
      "grad_norm": 5.056948661804199,
      "learning_rate": 3.740099642309658e-05,
      "loss": 0.6577,
      "step": 1656900
    },
    {
      "epoch": 15.119716767647274,
      "grad_norm": 4.875511169433594,
      "learning_rate": 3.7400236026960605e-05,
      "loss": 0.6377,
      "step": 1657000
    },
    {
      "epoch": 15.12062924301044,
      "grad_norm": 3.8448381423950195,
      "learning_rate": 3.7399475630824635e-05,
      "loss": 0.623,
      "step": 1657100
    },
    {
      "epoch": 15.121541718373605,
      "grad_norm": 4.497708797454834,
      "learning_rate": 3.7398715234688665e-05,
      "loss": 0.687,
      "step": 1657200
    },
    {
      "epoch": 15.122454193736768,
      "grad_norm": 4.11273193359375,
      "learning_rate": 3.7397954838552695e-05,
      "loss": 0.6637,
      "step": 1657300
    },
    {
      "epoch": 15.123366669099934,
      "grad_norm": 4.54718017578125,
      "learning_rate": 3.7397194442416725e-05,
      "loss": 0.6602,
      "step": 1657400
    },
    {
      "epoch": 15.124279144463099,
      "grad_norm": 4.063756465911865,
      "learning_rate": 3.7396434046280755e-05,
      "loss": 0.6637,
      "step": 1657500
    },
    {
      "epoch": 15.125191619826264,
      "grad_norm": 3.3862037658691406,
      "learning_rate": 3.739567365014478e-05,
      "loss": 0.665,
      "step": 1657600
    },
    {
      "epoch": 15.12610409518943,
      "grad_norm": 4.114420413970947,
      "learning_rate": 3.7394913254008815e-05,
      "loss": 0.6397,
      "step": 1657700
    },
    {
      "epoch": 15.127016570552595,
      "grad_norm": 3.5984275341033936,
      "learning_rate": 3.739415285787284e-05,
      "loss": 0.6865,
      "step": 1657800
    },
    {
      "epoch": 15.12792904591576,
      "grad_norm": 3.1243834495544434,
      "learning_rate": 3.739339246173687e-05,
      "loss": 0.6529,
      "step": 1657900
    },
    {
      "epoch": 15.128841521278925,
      "grad_norm": 3.260791063308716,
      "learning_rate": 3.73926320656009e-05,
      "loss": 0.6793,
      "step": 1658000
    },
    {
      "epoch": 15.12975399664209,
      "grad_norm": 3.982089042663574,
      "learning_rate": 3.739187166946493e-05,
      "loss": 0.6748,
      "step": 1658100
    },
    {
      "epoch": 15.130666472005256,
      "grad_norm": 3.3374874591827393,
      "learning_rate": 3.739111127332895e-05,
      "loss": 0.6579,
      "step": 1658200
    },
    {
      "epoch": 15.131578947368421,
      "grad_norm": 3.157818078994751,
      "learning_rate": 3.739035087719299e-05,
      "loss": 0.6702,
      "step": 1658300
    },
    {
      "epoch": 15.132491422731587,
      "grad_norm": 3.477107048034668,
      "learning_rate": 3.738959048105701e-05,
      "loss": 0.6621,
      "step": 1658400
    },
    {
      "epoch": 15.133403898094752,
      "grad_norm": 4.216344356536865,
      "learning_rate": 3.738883008492104e-05,
      "loss": 0.6761,
      "step": 1658500
    },
    {
      "epoch": 15.134316373457917,
      "grad_norm": 3.8036649227142334,
      "learning_rate": 3.738806968878507e-05,
      "loss": 0.6828,
      "step": 1658600
    },
    {
      "epoch": 15.135228848821082,
      "grad_norm": 3.773676633834839,
      "learning_rate": 3.7387309292649095e-05,
      "loss": 0.6235,
      "step": 1658700
    },
    {
      "epoch": 15.136141324184248,
      "grad_norm": 3.001375913619995,
      "learning_rate": 3.738654889651313e-05,
      "loss": 0.6483,
      "step": 1658800
    },
    {
      "epoch": 15.137053799547413,
      "grad_norm": 4.725276470184326,
      "learning_rate": 3.7385788500377156e-05,
      "loss": 0.6352,
      "step": 1658900
    },
    {
      "epoch": 15.137966274910577,
      "grad_norm": 4.357502460479736,
      "learning_rate": 3.7385028104241186e-05,
      "loss": 0.6646,
      "step": 1659000
    },
    {
      "epoch": 15.138878750273742,
      "grad_norm": 3.395756959915161,
      "learning_rate": 3.7384267708105216e-05,
      "loss": 0.6325,
      "step": 1659100
    },
    {
      "epoch": 15.139791225636907,
      "grad_norm": 4.954190731048584,
      "learning_rate": 3.7383507311969246e-05,
      "loss": 0.6273,
      "step": 1659200
    },
    {
      "epoch": 15.140703701000072,
      "grad_norm": 3.6530048847198486,
      "learning_rate": 3.738274691583327e-05,
      "loss": 0.6394,
      "step": 1659300
    },
    {
      "epoch": 15.141616176363238,
      "grad_norm": 3.23539662361145,
      "learning_rate": 3.7381986519697306e-05,
      "loss": 0.6806,
      "step": 1659400
    },
    {
      "epoch": 15.142528651726403,
      "grad_norm": 3.5382206439971924,
      "learning_rate": 3.738122612356133e-05,
      "loss": 0.6295,
      "step": 1659500
    },
    {
      "epoch": 15.143441127089568,
      "grad_norm": 4.091663360595703,
      "learning_rate": 3.738046572742536e-05,
      "loss": 0.6578,
      "step": 1659600
    },
    {
      "epoch": 15.144353602452734,
      "grad_norm": 4.302271366119385,
      "learning_rate": 3.737970533128939e-05,
      "loss": 0.6841,
      "step": 1659700
    },
    {
      "epoch": 15.145266077815899,
      "grad_norm": 3.995100259780884,
      "learning_rate": 3.737894493515342e-05,
      "loss": 0.6752,
      "step": 1659800
    },
    {
      "epoch": 15.146178553179064,
      "grad_norm": 4.181743621826172,
      "learning_rate": 3.737818453901745e-05,
      "loss": 0.6538,
      "step": 1659900
    },
    {
      "epoch": 15.14709102854223,
      "grad_norm": 3.4484193325042725,
      "learning_rate": 3.737742414288148e-05,
      "loss": 0.6702,
      "step": 1660000
    },
    {
      "epoch": 15.148003503905395,
      "grad_norm": 4.23042106628418,
      "learning_rate": 3.73766637467455e-05,
      "loss": 0.6653,
      "step": 1660100
    },
    {
      "epoch": 15.14891597926856,
      "grad_norm": 4.244307518005371,
      "learning_rate": 3.737590335060954e-05,
      "loss": 0.6743,
      "step": 1660200
    },
    {
      "epoch": 15.149828454631725,
      "grad_norm": 3.6650025844573975,
      "learning_rate": 3.737514295447356e-05,
      "loss": 0.6964,
      "step": 1660300
    },
    {
      "epoch": 15.15074092999489,
      "grad_norm": 4.029322147369385,
      "learning_rate": 3.737438255833759e-05,
      "loss": 0.6899,
      "step": 1660400
    },
    {
      "epoch": 15.151653405358056,
      "grad_norm": 3.264347553253174,
      "learning_rate": 3.737362216220162e-05,
      "loss": 0.6917,
      "step": 1660500
    },
    {
      "epoch": 15.152565880721221,
      "grad_norm": 3.8804728984832764,
      "learning_rate": 3.737286176606565e-05,
      "loss": 0.6746,
      "step": 1660600
    },
    {
      "epoch": 15.153478356084385,
      "grad_norm": 3.6472339630126953,
      "learning_rate": 3.7372101369929676e-05,
      "loss": 0.6565,
      "step": 1660700
    },
    {
      "epoch": 15.15439083144755,
      "grad_norm": 3.19899845123291,
      "learning_rate": 3.737134097379371e-05,
      "loss": 0.671,
      "step": 1660800
    },
    {
      "epoch": 15.155303306810715,
      "grad_norm": 4.5231523513793945,
      "learning_rate": 3.737058057765774e-05,
      "loss": 0.679,
      "step": 1660900
    },
    {
      "epoch": 15.15621578217388,
      "grad_norm": 3.7299160957336426,
      "learning_rate": 3.736982018152177e-05,
      "loss": 0.6918,
      "step": 1661000
    },
    {
      "epoch": 15.157128257537046,
      "grad_norm": 4.358590126037598,
      "learning_rate": 3.73690597853858e-05,
      "loss": 0.6454,
      "step": 1661100
    },
    {
      "epoch": 15.158040732900211,
      "grad_norm": 4.599201202392578,
      "learning_rate": 3.736829938924983e-05,
      "loss": 0.6728,
      "step": 1661200
    },
    {
      "epoch": 15.158953208263377,
      "grad_norm": 4.336368083953857,
      "learning_rate": 3.736753899311386e-05,
      "loss": 0.6587,
      "step": 1661300
    },
    {
      "epoch": 15.159865683626542,
      "grad_norm": 3.179549217224121,
      "learning_rate": 3.736677859697789e-05,
      "loss": 0.6361,
      "step": 1661400
    },
    {
      "epoch": 15.160778158989707,
      "grad_norm": 3.9570467472076416,
      "learning_rate": 3.736601820084191e-05,
      "loss": 0.6555,
      "step": 1661500
    },
    {
      "epoch": 15.161690634352873,
      "grad_norm": 3.882565498352051,
      "learning_rate": 3.736525780470594e-05,
      "loss": 0.6503,
      "step": 1661600
    },
    {
      "epoch": 15.162603109716038,
      "grad_norm": 3.0372965335845947,
      "learning_rate": 3.736449740856997e-05,
      "loss": 0.6754,
      "step": 1661700
    },
    {
      "epoch": 15.163515585079203,
      "grad_norm": 3.3667452335357666,
      "learning_rate": 3.7363737012433994e-05,
      "loss": 0.6988,
      "step": 1661800
    },
    {
      "epoch": 15.164428060442368,
      "grad_norm": 3.211895704269409,
      "learning_rate": 3.736297661629803e-05,
      "loss": 0.6657,
      "step": 1661900
    },
    {
      "epoch": 15.165340535805534,
      "grad_norm": 4.004146099090576,
      "learning_rate": 3.7362216220162054e-05,
      "loss": 0.6783,
      "step": 1662000
    },
    {
      "epoch": 15.166253011168699,
      "grad_norm": 3.49354887008667,
      "learning_rate": 3.7361455824026084e-05,
      "loss": 0.6731,
      "step": 1662100
    },
    {
      "epoch": 15.167165486531864,
      "grad_norm": 3.619976282119751,
      "learning_rate": 3.7360695427890114e-05,
      "loss": 0.6453,
      "step": 1662200
    },
    {
      "epoch": 15.16807796189503,
      "grad_norm": 3.1374247074127197,
      "learning_rate": 3.7359935031754144e-05,
      "loss": 0.6456,
      "step": 1662300
    },
    {
      "epoch": 15.168990437258193,
      "grad_norm": 3.938446283340454,
      "learning_rate": 3.7359174635618174e-05,
      "loss": 0.7141,
      "step": 1662400
    },
    {
      "epoch": 15.169902912621358,
      "grad_norm": 3.1002843379974365,
      "learning_rate": 3.7358414239482204e-05,
      "loss": 0.6378,
      "step": 1662500
    },
    {
      "epoch": 15.170815387984524,
      "grad_norm": 5.380953311920166,
      "learning_rate": 3.735765384334623e-05,
      "loss": 0.6539,
      "step": 1662600
    },
    {
      "epoch": 15.171727863347689,
      "grad_norm": 3.7282657623291016,
      "learning_rate": 3.7356893447210264e-05,
      "loss": 0.6405,
      "step": 1662700
    },
    {
      "epoch": 15.172640338710854,
      "grad_norm": 3.9088635444641113,
      "learning_rate": 3.735613305107429e-05,
      "loss": 0.6516,
      "step": 1662800
    },
    {
      "epoch": 15.17355281407402,
      "grad_norm": 4.630849838256836,
      "learning_rate": 3.735537265493832e-05,
      "loss": 0.6851,
      "step": 1662900
    },
    {
      "epoch": 15.174465289437185,
      "grad_norm": 3.7802135944366455,
      "learning_rate": 3.735461225880235e-05,
      "loss": 0.6525,
      "step": 1663000
    },
    {
      "epoch": 15.17537776480035,
      "grad_norm": 3.1690287590026855,
      "learning_rate": 3.735385186266638e-05,
      "loss": 0.6169,
      "step": 1663100
    },
    {
      "epoch": 15.176290240163516,
      "grad_norm": 4.503690242767334,
      "learning_rate": 3.73530914665304e-05,
      "loss": 0.6922,
      "step": 1663200
    },
    {
      "epoch": 15.17720271552668,
      "grad_norm": 3.616325616836548,
      "learning_rate": 3.735233107039444e-05,
      "loss": 0.6856,
      "step": 1663300
    },
    {
      "epoch": 15.178115190889846,
      "grad_norm": 3.9612817764282227,
      "learning_rate": 3.735157067425846e-05,
      "loss": 0.6858,
      "step": 1663400
    },
    {
      "epoch": 15.179027666253011,
      "grad_norm": 3.0824732780456543,
      "learning_rate": 3.735081027812249e-05,
      "loss": 0.6122,
      "step": 1663500
    },
    {
      "epoch": 15.179940141616177,
      "grad_norm": 3.723370313644409,
      "learning_rate": 3.735004988198652e-05,
      "loss": 0.6742,
      "step": 1663600
    },
    {
      "epoch": 15.180852616979342,
      "grad_norm": 3.4924967288970947,
      "learning_rate": 3.734928948585055e-05,
      "loss": 0.6449,
      "step": 1663700
    },
    {
      "epoch": 15.181765092342507,
      "grad_norm": 5.208443641662598,
      "learning_rate": 3.734852908971458e-05,
      "loss": 0.6562,
      "step": 1663800
    },
    {
      "epoch": 15.182677567705673,
      "grad_norm": 3.4319231510162354,
      "learning_rate": 3.734776869357861e-05,
      "loss": 0.6665,
      "step": 1663900
    },
    {
      "epoch": 15.183590043068838,
      "grad_norm": 3.711679458618164,
      "learning_rate": 3.7347008297442635e-05,
      "loss": 0.6979,
      "step": 1664000
    },
    {
      "epoch": 15.184502518432001,
      "grad_norm": 4.175998210906982,
      "learning_rate": 3.734624790130667e-05,
      "loss": 0.674,
      "step": 1664100
    },
    {
      "epoch": 15.185414993795167,
      "grad_norm": 3.0673892498016357,
      "learning_rate": 3.7345487505170695e-05,
      "loss": 0.6201,
      "step": 1664200
    },
    {
      "epoch": 15.186327469158332,
      "grad_norm": 4.07511043548584,
      "learning_rate": 3.7344727109034725e-05,
      "loss": 0.6507,
      "step": 1664300
    },
    {
      "epoch": 15.187239944521497,
      "grad_norm": 3.082441806793213,
      "learning_rate": 3.7343966712898755e-05,
      "loss": 0.6912,
      "step": 1664400
    },
    {
      "epoch": 15.188152419884663,
      "grad_norm": 4.092573165893555,
      "learning_rate": 3.734320631676278e-05,
      "loss": 0.7002,
      "step": 1664500
    },
    {
      "epoch": 15.189064895247828,
      "grad_norm": 4.073203086853027,
      "learning_rate": 3.734244592062681e-05,
      "loss": 0.6867,
      "step": 1664600
    },
    {
      "epoch": 15.189977370610993,
      "grad_norm": 3.2303435802459717,
      "learning_rate": 3.734168552449084e-05,
      "loss": 0.6718,
      "step": 1664700
    },
    {
      "epoch": 15.190889845974159,
      "grad_norm": 4.501302242279053,
      "learning_rate": 3.734092512835487e-05,
      "loss": 0.6275,
      "step": 1664800
    },
    {
      "epoch": 15.191802321337324,
      "grad_norm": 3.657061815261841,
      "learning_rate": 3.73401647322189e-05,
      "loss": 0.6075,
      "step": 1664900
    },
    {
      "epoch": 15.19271479670049,
      "grad_norm": 4.327254772186279,
      "learning_rate": 3.733940433608293e-05,
      "loss": 0.6973,
      "step": 1665000
    },
    {
      "epoch": 15.193627272063654,
      "grad_norm": 3.62493896484375,
      "learning_rate": 3.733864393994695e-05,
      "loss": 0.6972,
      "step": 1665100
    },
    {
      "epoch": 15.19453974742682,
      "grad_norm": 3.3253724575042725,
      "learning_rate": 3.733788354381099e-05,
      "loss": 0.6736,
      "step": 1665200
    },
    {
      "epoch": 15.195452222789985,
      "grad_norm": 3.6112968921661377,
      "learning_rate": 3.733712314767501e-05,
      "loss": 0.6847,
      "step": 1665300
    },
    {
      "epoch": 15.19636469815315,
      "grad_norm": 3.5233073234558105,
      "learning_rate": 3.733636275153904e-05,
      "loss": 0.6539,
      "step": 1665400
    },
    {
      "epoch": 15.197277173516316,
      "grad_norm": 3.9962987899780273,
      "learning_rate": 3.733560235540307e-05,
      "loss": 0.6413,
      "step": 1665500
    },
    {
      "epoch": 15.198189648879481,
      "grad_norm": 3.921363592147827,
      "learning_rate": 3.73348419592671e-05,
      "loss": 0.6869,
      "step": 1665600
    },
    {
      "epoch": 15.199102124242646,
      "grad_norm": 4.0040082931518555,
      "learning_rate": 3.733408156313113e-05,
      "loss": 0.6503,
      "step": 1665700
    },
    {
      "epoch": 15.20001459960581,
      "grad_norm": 3.8201568126678467,
      "learning_rate": 3.733332116699516e-05,
      "loss": 0.6737,
      "step": 1665800
    },
    {
      "epoch": 15.200927074968975,
      "grad_norm": 4.0144524574279785,
      "learning_rate": 3.7332560770859186e-05,
      "loss": 0.6979,
      "step": 1665900
    },
    {
      "epoch": 15.20183955033214,
      "grad_norm": 4.004264831542969,
      "learning_rate": 3.733180037472322e-05,
      "loss": 0.6393,
      "step": 1666000
    },
    {
      "epoch": 15.202752025695306,
      "grad_norm": 3.404672145843506,
      "learning_rate": 3.7331039978587246e-05,
      "loss": 0.6858,
      "step": 1666100
    },
    {
      "epoch": 15.203664501058471,
      "grad_norm": 4.194242477416992,
      "learning_rate": 3.7330279582451276e-05,
      "loss": 0.654,
      "step": 1666200
    },
    {
      "epoch": 15.204576976421636,
      "grad_norm": 3.001023292541504,
      "learning_rate": 3.7329519186315306e-05,
      "loss": 0.6506,
      "step": 1666300
    },
    {
      "epoch": 15.205489451784802,
      "grad_norm": 3.7343170642852783,
      "learning_rate": 3.7328758790179336e-05,
      "loss": 0.6698,
      "step": 1666400
    },
    {
      "epoch": 15.206401927147967,
      "grad_norm": 3.7822458744049072,
      "learning_rate": 3.732799839404336e-05,
      "loss": 0.6709,
      "step": 1666500
    },
    {
      "epoch": 15.207314402511132,
      "grad_norm": 4.793243408203125,
      "learning_rate": 3.7327237997907396e-05,
      "loss": 0.6514,
      "step": 1666600
    },
    {
      "epoch": 15.208226877874298,
      "grad_norm": 4.771538257598877,
      "learning_rate": 3.732647760177142e-05,
      "loss": 0.7011,
      "step": 1666700
    },
    {
      "epoch": 15.209139353237463,
      "grad_norm": 4.689139366149902,
      "learning_rate": 3.732571720563545e-05,
      "loss": 0.7037,
      "step": 1666800
    },
    {
      "epoch": 15.210051828600628,
      "grad_norm": 2.6400146484375,
      "learning_rate": 3.732495680949948e-05,
      "loss": 0.6466,
      "step": 1666900
    },
    {
      "epoch": 15.210964303963793,
      "grad_norm": 4.857399940490723,
      "learning_rate": 3.732419641336351e-05,
      "loss": 0.6754,
      "step": 1667000
    },
    {
      "epoch": 15.211876779326959,
      "grad_norm": 3.768082618713379,
      "learning_rate": 3.732343601722754e-05,
      "loss": 0.6608,
      "step": 1667100
    },
    {
      "epoch": 15.212789254690124,
      "grad_norm": 4.116610050201416,
      "learning_rate": 3.732267562109156e-05,
      "loss": 0.634,
      "step": 1667200
    },
    {
      "epoch": 15.21370173005329,
      "grad_norm": 3.972792863845825,
      "learning_rate": 3.732191522495559e-05,
      "loss": 0.6901,
      "step": 1667300
    },
    {
      "epoch": 15.214614205416455,
      "grad_norm": 3.3618597984313965,
      "learning_rate": 3.732115482881962e-05,
      "loss": 0.6597,
      "step": 1667400
    },
    {
      "epoch": 15.215526680779618,
      "grad_norm": 3.353170871734619,
      "learning_rate": 3.732039443268365e-05,
      "loss": 0.6683,
      "step": 1667500
    },
    {
      "epoch": 15.216439156142783,
      "grad_norm": 3.447432279586792,
      "learning_rate": 3.7319634036547677e-05,
      "loss": 0.6756,
      "step": 1667600
    },
    {
      "epoch": 15.217351631505949,
      "grad_norm": 3.818558931350708,
      "learning_rate": 3.7318873640411713e-05,
      "loss": 0.6434,
      "step": 1667700
    },
    {
      "epoch": 15.218264106869114,
      "grad_norm": 2.914231777191162,
      "learning_rate": 3.731811324427574e-05,
      "loss": 0.6994,
      "step": 1667800
    },
    {
      "epoch": 15.21917658223228,
      "grad_norm": 3.7929959297180176,
      "learning_rate": 3.731735284813977e-05,
      "loss": 0.6442,
      "step": 1667900
    },
    {
      "epoch": 15.220089057595445,
      "grad_norm": 3.6929359436035156,
      "learning_rate": 3.73165924520038e-05,
      "loss": 0.6209,
      "step": 1668000
    },
    {
      "epoch": 15.22100153295861,
      "grad_norm": 3.191423177719116,
      "learning_rate": 3.731583205586783e-05,
      "loss": 0.6771,
      "step": 1668100
    },
    {
      "epoch": 15.221914008321775,
      "grad_norm": 2.9474868774414062,
      "learning_rate": 3.731507165973186e-05,
      "loss": 0.6454,
      "step": 1668200
    },
    {
      "epoch": 15.22282648368494,
      "grad_norm": 3.3654370307922363,
      "learning_rate": 3.731431126359589e-05,
      "loss": 0.6644,
      "step": 1668300
    },
    {
      "epoch": 15.223738959048106,
      "grad_norm": 4.669544219970703,
      "learning_rate": 3.731355086745991e-05,
      "loss": 0.6647,
      "step": 1668400
    },
    {
      "epoch": 15.224651434411271,
      "grad_norm": 3.15220308303833,
      "learning_rate": 3.731279047132395e-05,
      "loss": 0.6902,
      "step": 1668500
    },
    {
      "epoch": 15.225563909774436,
      "grad_norm": 4.467471599578857,
      "learning_rate": 3.731203007518797e-05,
      "loss": 0.6851,
      "step": 1668600
    },
    {
      "epoch": 15.226476385137602,
      "grad_norm": 3.8706555366516113,
      "learning_rate": 3.7311269679052e-05,
      "loss": 0.6409,
      "step": 1668700
    },
    {
      "epoch": 15.227388860500767,
      "grad_norm": 3.4657373428344727,
      "learning_rate": 3.731050928291603e-05,
      "loss": 0.6394,
      "step": 1668800
    },
    {
      "epoch": 15.228301335863932,
      "grad_norm": 3.206791639328003,
      "learning_rate": 3.730974888678006e-05,
      "loss": 0.6828,
      "step": 1668900
    },
    {
      "epoch": 15.229213811227098,
      "grad_norm": 3.8390793800354004,
      "learning_rate": 3.7308988490644084e-05,
      "loss": 0.687,
      "step": 1669000
    },
    {
      "epoch": 15.230126286590263,
      "grad_norm": 4.720397472381592,
      "learning_rate": 3.730822809450812e-05,
      "loss": 0.6911,
      "step": 1669100
    },
    {
      "epoch": 15.231038761953426,
      "grad_norm": 4.655071258544922,
      "learning_rate": 3.7307467698372144e-05,
      "loss": 0.6559,
      "step": 1669200
    },
    {
      "epoch": 15.231951237316592,
      "grad_norm": 4.651363372802734,
      "learning_rate": 3.7306707302236174e-05,
      "loss": 0.662,
      "step": 1669300
    },
    {
      "epoch": 15.232863712679757,
      "grad_norm": 3.3398706912994385,
      "learning_rate": 3.7305946906100204e-05,
      "loss": 0.6754,
      "step": 1669400
    },
    {
      "epoch": 15.233776188042922,
      "grad_norm": 3.5544888973236084,
      "learning_rate": 3.7305186509964234e-05,
      "loss": 0.6806,
      "step": 1669500
    },
    {
      "epoch": 15.234688663406088,
      "grad_norm": 3.5452842712402344,
      "learning_rate": 3.7304426113828264e-05,
      "loss": 0.6301,
      "step": 1669600
    },
    {
      "epoch": 15.235601138769253,
      "grad_norm": 2.7691195011138916,
      "learning_rate": 3.7303665717692294e-05,
      "loss": 0.6661,
      "step": 1669700
    },
    {
      "epoch": 15.236513614132418,
      "grad_norm": 4.021339416503906,
      "learning_rate": 3.730290532155632e-05,
      "loss": 0.6924,
      "step": 1669800
    },
    {
      "epoch": 15.237426089495584,
      "grad_norm": 5.406122207641602,
      "learning_rate": 3.7302144925420355e-05,
      "loss": 0.6571,
      "step": 1669900
    },
    {
      "epoch": 15.238338564858749,
      "grad_norm": 4.121429920196533,
      "learning_rate": 3.730138452928438e-05,
      "loss": 0.6223,
      "step": 1670000
    },
    {
      "epoch": 15.239251040221914,
      "grad_norm": 3.9913101196289062,
      "learning_rate": 3.73006241331484e-05,
      "loss": 0.6975,
      "step": 1670100
    },
    {
      "epoch": 15.24016351558508,
      "grad_norm": 3.729843854904175,
      "learning_rate": 3.729986373701244e-05,
      "loss": 0.6695,
      "step": 1670200
    },
    {
      "epoch": 15.241075990948245,
      "grad_norm": 3.831249713897705,
      "learning_rate": 3.729910334087646e-05,
      "loss": 0.6623,
      "step": 1670300
    },
    {
      "epoch": 15.24198846631141,
      "grad_norm": 3.7886569499969482,
      "learning_rate": 3.729834294474049e-05,
      "loss": 0.6552,
      "step": 1670400
    },
    {
      "epoch": 15.242900941674575,
      "grad_norm": 3.189342975616455,
      "learning_rate": 3.729758254860452e-05,
      "loss": 0.6251,
      "step": 1670500
    },
    {
      "epoch": 15.24381341703774,
      "grad_norm": 4.272584438323975,
      "learning_rate": 3.729682215246855e-05,
      "loss": 0.6576,
      "step": 1670600
    },
    {
      "epoch": 15.244725892400906,
      "grad_norm": 3.972820997238159,
      "learning_rate": 3.729606175633258e-05,
      "loss": 0.6837,
      "step": 1670700
    },
    {
      "epoch": 15.245638367764071,
      "grad_norm": 4.027764320373535,
      "learning_rate": 3.729530136019661e-05,
      "loss": 0.6641,
      "step": 1670800
    },
    {
      "epoch": 15.246550843127235,
      "grad_norm": 4.010717868804932,
      "learning_rate": 3.7294540964060635e-05,
      "loss": 0.6841,
      "step": 1670900
    },
    {
      "epoch": 15.2474633184904,
      "grad_norm": 4.190924167633057,
      "learning_rate": 3.729378056792467e-05,
      "loss": 0.6178,
      "step": 1671000
    },
    {
      "epoch": 15.248375793853565,
      "grad_norm": 3.930943489074707,
      "learning_rate": 3.7293020171788695e-05,
      "loss": 0.6636,
      "step": 1671100
    },
    {
      "epoch": 15.24928826921673,
      "grad_norm": 4.259240627288818,
      "learning_rate": 3.7292259775652725e-05,
      "loss": 0.7187,
      "step": 1671200
    },
    {
      "epoch": 15.250200744579896,
      "grad_norm": 3.796766757965088,
      "learning_rate": 3.7291499379516755e-05,
      "loss": 0.6666,
      "step": 1671300
    },
    {
      "epoch": 15.251113219943061,
      "grad_norm": 3.576458692550659,
      "learning_rate": 3.7290738983380785e-05,
      "loss": 0.6705,
      "step": 1671400
    },
    {
      "epoch": 15.252025695306227,
      "grad_norm": 4.120965957641602,
      "learning_rate": 3.728997858724481e-05,
      "loss": 0.6577,
      "step": 1671500
    },
    {
      "epoch": 15.252938170669392,
      "grad_norm": 3.974499464035034,
      "learning_rate": 3.7289218191108845e-05,
      "loss": 0.6336,
      "step": 1671600
    },
    {
      "epoch": 15.253850646032557,
      "grad_norm": 3.533696174621582,
      "learning_rate": 3.728845779497287e-05,
      "loss": 0.6799,
      "step": 1671700
    },
    {
      "epoch": 15.254763121395722,
      "grad_norm": 4.440123081207275,
      "learning_rate": 3.72876973988369e-05,
      "loss": 0.6675,
      "step": 1671800
    },
    {
      "epoch": 15.255675596758888,
      "grad_norm": 4.196927547454834,
      "learning_rate": 3.728693700270093e-05,
      "loss": 0.6579,
      "step": 1671900
    },
    {
      "epoch": 15.256588072122053,
      "grad_norm": 4.168299198150635,
      "learning_rate": 3.728617660656496e-05,
      "loss": 0.6256,
      "step": 1672000
    },
    {
      "epoch": 15.257500547485218,
      "grad_norm": 4.590878963470459,
      "learning_rate": 3.728541621042899e-05,
      "loss": 0.6621,
      "step": 1672100
    },
    {
      "epoch": 15.258413022848384,
      "grad_norm": 4.088805675506592,
      "learning_rate": 3.728465581429302e-05,
      "loss": 0.6814,
      "step": 1672200
    },
    {
      "epoch": 15.259325498211549,
      "grad_norm": 4.136463642120361,
      "learning_rate": 3.728389541815704e-05,
      "loss": 0.6513,
      "step": 1672300
    },
    {
      "epoch": 15.260237973574714,
      "grad_norm": 4.675407886505127,
      "learning_rate": 3.728313502202108e-05,
      "loss": 0.6893,
      "step": 1672400
    },
    {
      "epoch": 15.26115044893788,
      "grad_norm": 4.18782377243042,
      "learning_rate": 3.72823746258851e-05,
      "loss": 0.6475,
      "step": 1672500
    },
    {
      "epoch": 15.262062924301043,
      "grad_norm": 4.577215671539307,
      "learning_rate": 3.728161422974913e-05,
      "loss": 0.6676,
      "step": 1672600
    },
    {
      "epoch": 15.262975399664208,
      "grad_norm": 4.804050445556641,
      "learning_rate": 3.728085383361316e-05,
      "loss": 0.6599,
      "step": 1672700
    },
    {
      "epoch": 15.263887875027374,
      "grad_norm": 4.60343599319458,
      "learning_rate": 3.728009343747719e-05,
      "loss": 0.6978,
      "step": 1672800
    },
    {
      "epoch": 15.264800350390539,
      "grad_norm": 3.0334012508392334,
      "learning_rate": 3.7279333041341216e-05,
      "loss": 0.6878,
      "step": 1672900
    },
    {
      "epoch": 15.265712825753704,
      "grad_norm": 3.7023746967315674,
      "learning_rate": 3.7278572645205246e-05,
      "loss": 0.6458,
      "step": 1673000
    },
    {
      "epoch": 15.26662530111687,
      "grad_norm": 2.9906105995178223,
      "learning_rate": 3.7277812249069276e-05,
      "loss": 0.6493,
      "step": 1673100
    },
    {
      "epoch": 15.267537776480035,
      "grad_norm": 4.875876426696777,
      "learning_rate": 3.7277051852933306e-05,
      "loss": 0.6923,
      "step": 1673200
    },
    {
      "epoch": 15.2684502518432,
      "grad_norm": 3.016653537750244,
      "learning_rate": 3.7276291456797336e-05,
      "loss": 0.6852,
      "step": 1673300
    },
    {
      "epoch": 15.269362727206365,
      "grad_norm": 3.254631519317627,
      "learning_rate": 3.727553106066136e-05,
      "loss": 0.6912,
      "step": 1673400
    },
    {
      "epoch": 15.27027520256953,
      "grad_norm": 3.8091189861297607,
      "learning_rate": 3.7274770664525396e-05,
      "loss": 0.644,
      "step": 1673500
    },
    {
      "epoch": 15.271187677932696,
      "grad_norm": 3.513540744781494,
      "learning_rate": 3.727401026838942e-05,
      "loss": 0.6545,
      "step": 1673600
    },
    {
      "epoch": 15.272100153295861,
      "grad_norm": 3.334465265274048,
      "learning_rate": 3.727324987225345e-05,
      "loss": 0.6286,
      "step": 1673700
    },
    {
      "epoch": 15.273012628659027,
      "grad_norm": 4.179028034210205,
      "learning_rate": 3.727248947611748e-05,
      "loss": 0.6901,
      "step": 1673800
    },
    {
      "epoch": 15.273925104022192,
      "grad_norm": 4.03378438949585,
      "learning_rate": 3.727172907998151e-05,
      "loss": 0.665,
      "step": 1673900
    },
    {
      "epoch": 15.274837579385357,
      "grad_norm": 3.577045202255249,
      "learning_rate": 3.727096868384553e-05,
      "loss": 0.6635,
      "step": 1674000
    },
    {
      "epoch": 15.275750054748523,
      "grad_norm": 4.199820041656494,
      "learning_rate": 3.727020828770957e-05,
      "loss": 0.6679,
      "step": 1674100
    },
    {
      "epoch": 15.276662530111688,
      "grad_norm": 3.8152015209198,
      "learning_rate": 3.726944789157359e-05,
      "loss": 0.6639,
      "step": 1674200
    },
    {
      "epoch": 15.277575005474851,
      "grad_norm": 3.6493990421295166,
      "learning_rate": 3.726868749543762e-05,
      "loss": 0.6856,
      "step": 1674300
    },
    {
      "epoch": 15.278487480838017,
      "grad_norm": 3.7972285747528076,
      "learning_rate": 3.7267927099301653e-05,
      "loss": 0.7,
      "step": 1674400
    },
    {
      "epoch": 15.279399956201182,
      "grad_norm": 3.585070848464966,
      "learning_rate": 3.7267166703165683e-05,
      "loss": 0.659,
      "step": 1674500
    },
    {
      "epoch": 15.280312431564347,
      "grad_norm": 3.4170897006988525,
      "learning_rate": 3.7266406307029714e-05,
      "loss": 0.7035,
      "step": 1674600
    },
    {
      "epoch": 15.281224906927513,
      "grad_norm": 3.9090023040771484,
      "learning_rate": 3.7265645910893744e-05,
      "loss": 0.6471,
      "step": 1674700
    },
    {
      "epoch": 15.282137382290678,
      "grad_norm": 4.925304889678955,
      "learning_rate": 3.726488551475777e-05,
      "loss": 0.6721,
      "step": 1674800
    },
    {
      "epoch": 15.283049857653843,
      "grad_norm": 3.398428440093994,
      "learning_rate": 3.7264125118621804e-05,
      "loss": 0.6632,
      "step": 1674900
    },
    {
      "epoch": 15.283962333017008,
      "grad_norm": 3.5642130374908447,
      "learning_rate": 3.726336472248583e-05,
      "loss": 0.6908,
      "step": 1675000
    },
    {
      "epoch": 15.284874808380174,
      "grad_norm": 3.8209497928619385,
      "learning_rate": 3.726260432634986e-05,
      "loss": 0.639,
      "step": 1675100
    },
    {
      "epoch": 15.285787283743339,
      "grad_norm": 5.2063775062561035,
      "learning_rate": 3.726184393021389e-05,
      "loss": 0.6425,
      "step": 1675200
    },
    {
      "epoch": 15.286699759106504,
      "grad_norm": 5.086823463439941,
      "learning_rate": 3.726108353407792e-05,
      "loss": 0.6778,
      "step": 1675300
    },
    {
      "epoch": 15.28761223446967,
      "grad_norm": 5.427926540374756,
      "learning_rate": 3.726032313794194e-05,
      "loss": 0.6649,
      "step": 1675400
    },
    {
      "epoch": 15.288524709832835,
      "grad_norm": 4.393688678741455,
      "learning_rate": 3.725956274180598e-05,
      "loss": 0.6906,
      "step": 1675500
    },
    {
      "epoch": 15.289437185196,
      "grad_norm": 4.335293769836426,
      "learning_rate": 3.725880234567e-05,
      "loss": 0.7031,
      "step": 1675600
    },
    {
      "epoch": 15.290349660559166,
      "grad_norm": 4.140436172485352,
      "learning_rate": 3.725804194953403e-05,
      "loss": 0.6849,
      "step": 1675700
    },
    {
      "epoch": 15.29126213592233,
      "grad_norm": 3.9079205989837646,
      "learning_rate": 3.725728155339806e-05,
      "loss": 0.6506,
      "step": 1675800
    },
    {
      "epoch": 15.292174611285496,
      "grad_norm": 4.067906379699707,
      "learning_rate": 3.7256521157262084e-05,
      "loss": 0.6503,
      "step": 1675900
    },
    {
      "epoch": 15.29308708664866,
      "grad_norm": 5.368105411529541,
      "learning_rate": 3.725576076112612e-05,
      "loss": 0.6829,
      "step": 1676000
    },
    {
      "epoch": 15.293999562011825,
      "grad_norm": 5.025604724884033,
      "learning_rate": 3.7255000364990144e-05,
      "loss": 0.6486,
      "step": 1676100
    },
    {
      "epoch": 15.29491203737499,
      "grad_norm": 4.630539894104004,
      "learning_rate": 3.7254239968854174e-05,
      "loss": 0.6457,
      "step": 1676200
    },
    {
      "epoch": 15.295824512738156,
      "grad_norm": 3.878995180130005,
      "learning_rate": 3.7253479572718204e-05,
      "loss": 0.6832,
      "step": 1676300
    },
    {
      "epoch": 15.29673698810132,
      "grad_norm": 3.933199405670166,
      "learning_rate": 3.7252719176582234e-05,
      "loss": 0.6881,
      "step": 1676400
    },
    {
      "epoch": 15.297649463464486,
      "grad_norm": 4.530116558074951,
      "learning_rate": 3.7251958780446264e-05,
      "loss": 0.6893,
      "step": 1676500
    },
    {
      "epoch": 15.298561938827651,
      "grad_norm": 4.655203342437744,
      "learning_rate": 3.7251198384310295e-05,
      "loss": 0.6375,
      "step": 1676600
    },
    {
      "epoch": 15.299474414190817,
      "grad_norm": 3.7884678840637207,
      "learning_rate": 3.725043798817432e-05,
      "loss": 0.6792,
      "step": 1676700
    },
    {
      "epoch": 15.300386889553982,
      "grad_norm": 4.472174644470215,
      "learning_rate": 3.724967759203835e-05,
      "loss": 0.6235,
      "step": 1676800
    },
    {
      "epoch": 15.301299364917147,
      "grad_norm": 3.9298901557922363,
      "learning_rate": 3.724891719590238e-05,
      "loss": 0.6388,
      "step": 1676900
    },
    {
      "epoch": 15.302211840280313,
      "grad_norm": 3.701353073120117,
      "learning_rate": 3.724815679976641e-05,
      "loss": 0.6461,
      "step": 1677000
    },
    {
      "epoch": 15.303124315643478,
      "grad_norm": 4.175757884979248,
      "learning_rate": 3.724739640363044e-05,
      "loss": 0.6376,
      "step": 1677100
    },
    {
      "epoch": 15.304036791006643,
      "grad_norm": 3.6177477836608887,
      "learning_rate": 3.724663600749447e-05,
      "loss": 0.6671,
      "step": 1677200
    },
    {
      "epoch": 15.304949266369809,
      "grad_norm": 4.26121711730957,
      "learning_rate": 3.724587561135849e-05,
      "loss": 0.6894,
      "step": 1677300
    },
    {
      "epoch": 15.305861741732974,
      "grad_norm": 3.932218551635742,
      "learning_rate": 3.724511521522253e-05,
      "loss": 0.6826,
      "step": 1677400
    },
    {
      "epoch": 15.30677421709614,
      "grad_norm": 4.267817497253418,
      "learning_rate": 3.724435481908655e-05,
      "loss": 0.6534,
      "step": 1677500
    },
    {
      "epoch": 15.307686692459303,
      "grad_norm": 4.271073341369629,
      "learning_rate": 3.724359442295058e-05,
      "loss": 0.6704,
      "step": 1677600
    },
    {
      "epoch": 15.308599167822468,
      "grad_norm": 3.929880142211914,
      "learning_rate": 3.724283402681461e-05,
      "loss": 0.6626,
      "step": 1677700
    },
    {
      "epoch": 15.309511643185633,
      "grad_norm": 4.09105110168457,
      "learning_rate": 3.724207363067864e-05,
      "loss": 0.6503,
      "step": 1677800
    },
    {
      "epoch": 15.310424118548799,
      "grad_norm": 3.713160991668701,
      "learning_rate": 3.724131323454267e-05,
      "loss": 0.6771,
      "step": 1677900
    },
    {
      "epoch": 15.311336593911964,
      "grad_norm": 3.792922258377075,
      "learning_rate": 3.72405528384067e-05,
      "loss": 0.666,
      "step": 1678000
    },
    {
      "epoch": 15.31224906927513,
      "grad_norm": 3.3105318546295166,
      "learning_rate": 3.7239792442270725e-05,
      "loss": 0.6738,
      "step": 1678100
    },
    {
      "epoch": 15.313161544638294,
      "grad_norm": 3.9212539196014404,
      "learning_rate": 3.7239032046134755e-05,
      "loss": 0.6593,
      "step": 1678200
    },
    {
      "epoch": 15.31407402000146,
      "grad_norm": 4.2041335105896,
      "learning_rate": 3.7238271649998785e-05,
      "loss": 0.6642,
      "step": 1678300
    },
    {
      "epoch": 15.314986495364625,
      "grad_norm": 3.308140516281128,
      "learning_rate": 3.7237511253862815e-05,
      "loss": 0.6604,
      "step": 1678400
    },
    {
      "epoch": 15.31589897072779,
      "grad_norm": 4.147214412689209,
      "learning_rate": 3.7236750857726845e-05,
      "loss": 0.6756,
      "step": 1678500
    },
    {
      "epoch": 15.316811446090956,
      "grad_norm": 4.386713027954102,
      "learning_rate": 3.723599046159087e-05,
      "loss": 0.6781,
      "step": 1678600
    },
    {
      "epoch": 15.317723921454121,
      "grad_norm": 3.4177439212799072,
      "learning_rate": 3.72352300654549e-05,
      "loss": 0.6432,
      "step": 1678700
    },
    {
      "epoch": 15.318636396817286,
      "grad_norm": 3.3632309436798096,
      "learning_rate": 3.723446966931893e-05,
      "loss": 0.6064,
      "step": 1678800
    },
    {
      "epoch": 15.319548872180452,
      "grad_norm": 3.460444688796997,
      "learning_rate": 3.723370927318296e-05,
      "loss": 0.6586,
      "step": 1678900
    },
    {
      "epoch": 15.320461347543617,
      "grad_norm": 3.4594223499298096,
      "learning_rate": 3.723294887704699e-05,
      "loss": 0.6586,
      "step": 1679000
    },
    {
      "epoch": 15.321373822906782,
      "grad_norm": 4.055149555206299,
      "learning_rate": 3.723218848091102e-05,
      "loss": 0.6695,
      "step": 1679100
    },
    {
      "epoch": 15.322286298269947,
      "grad_norm": 4.465112209320068,
      "learning_rate": 3.723142808477504e-05,
      "loss": 0.6761,
      "step": 1679200
    },
    {
      "epoch": 15.323198773633113,
      "grad_norm": 4.784014701843262,
      "learning_rate": 3.723066768863908e-05,
      "loss": 0.6755,
      "step": 1679300
    },
    {
      "epoch": 15.324111248996276,
      "grad_norm": 3.934075117111206,
      "learning_rate": 3.72299072925031e-05,
      "loss": 0.6505,
      "step": 1679400
    },
    {
      "epoch": 15.325023724359442,
      "grad_norm": 3.6067090034484863,
      "learning_rate": 3.722914689636713e-05,
      "loss": 0.6955,
      "step": 1679500
    },
    {
      "epoch": 15.325936199722607,
      "grad_norm": 3.7965641021728516,
      "learning_rate": 3.722838650023116e-05,
      "loss": 0.6442,
      "step": 1679600
    },
    {
      "epoch": 15.326848675085772,
      "grad_norm": 4.269678115844727,
      "learning_rate": 3.722762610409519e-05,
      "loss": 0.6479,
      "step": 1679700
    },
    {
      "epoch": 15.327761150448937,
      "grad_norm": 3.8056392669677734,
      "learning_rate": 3.7226865707959216e-05,
      "loss": 0.6698,
      "step": 1679800
    },
    {
      "epoch": 15.328673625812103,
      "grad_norm": 4.046310901641846,
      "learning_rate": 3.722610531182325e-05,
      "loss": 0.7061,
      "step": 1679900
    },
    {
      "epoch": 15.329586101175268,
      "grad_norm": 3.9048051834106445,
      "learning_rate": 3.7225344915687276e-05,
      "loss": 0.6511,
      "step": 1680000
    },
    {
      "epoch": 15.330498576538433,
      "grad_norm": 4.780537128448486,
      "learning_rate": 3.7224584519551306e-05,
      "loss": 0.6754,
      "step": 1680100
    },
    {
      "epoch": 15.331411051901599,
      "grad_norm": 3.64043927192688,
      "learning_rate": 3.7223824123415336e-05,
      "loss": 0.7231,
      "step": 1680200
    },
    {
      "epoch": 15.332323527264764,
      "grad_norm": 4.166395664215088,
      "learning_rate": 3.7223063727279366e-05,
      "loss": 0.6736,
      "step": 1680300
    },
    {
      "epoch": 15.33323600262793,
      "grad_norm": 3.7047691345214844,
      "learning_rate": 3.7222303331143396e-05,
      "loss": 0.6869,
      "step": 1680400
    },
    {
      "epoch": 15.334148477991095,
      "grad_norm": 3.832850694656372,
      "learning_rate": 3.7221542935007427e-05,
      "loss": 0.6856,
      "step": 1680500
    },
    {
      "epoch": 15.33506095335426,
      "grad_norm": 4.049182415008545,
      "learning_rate": 3.722078253887145e-05,
      "loss": 0.6887,
      "step": 1680600
    },
    {
      "epoch": 15.335973428717425,
      "grad_norm": 4.010440349578857,
      "learning_rate": 3.722002214273549e-05,
      "loss": 0.7199,
      "step": 1680700
    },
    {
      "epoch": 15.33688590408059,
      "grad_norm": 4.332371711730957,
      "learning_rate": 3.721926174659951e-05,
      "loss": 0.6538,
      "step": 1680800
    },
    {
      "epoch": 15.337798379443756,
      "grad_norm": 3.8366167545318604,
      "learning_rate": 3.721850135046354e-05,
      "loss": 0.6189,
      "step": 1680900
    },
    {
      "epoch": 15.33871085480692,
      "grad_norm": 3.9555304050445557,
      "learning_rate": 3.721774095432757e-05,
      "loss": 0.7202,
      "step": 1681000
    },
    {
      "epoch": 15.339623330170085,
      "grad_norm": 4.0232696533203125,
      "learning_rate": 3.72169805581916e-05,
      "loss": 0.6786,
      "step": 1681100
    },
    {
      "epoch": 15.34053580553325,
      "grad_norm": 4.050383567810059,
      "learning_rate": 3.7216220162055623e-05,
      "loss": 0.6936,
      "step": 1681200
    },
    {
      "epoch": 15.341448280896415,
      "grad_norm": 4.565929412841797,
      "learning_rate": 3.721545976591966e-05,
      "loss": 0.6595,
      "step": 1681300
    },
    {
      "epoch": 15.34236075625958,
      "grad_norm": 2.85007381439209,
      "learning_rate": 3.7214699369783684e-05,
      "loss": 0.6458,
      "step": 1681400
    },
    {
      "epoch": 15.343273231622746,
      "grad_norm": 4.344675540924072,
      "learning_rate": 3.7213938973647714e-05,
      "loss": 0.6813,
      "step": 1681500
    },
    {
      "epoch": 15.344185706985911,
      "grad_norm": 3.550794839859009,
      "learning_rate": 3.7213178577511744e-05,
      "loss": 0.6484,
      "step": 1681600
    },
    {
      "epoch": 15.345098182349076,
      "grad_norm": 3.736096143722534,
      "learning_rate": 3.721241818137577e-05,
      "loss": 0.6572,
      "step": 1681700
    },
    {
      "epoch": 15.346010657712242,
      "grad_norm": 3.0804543495178223,
      "learning_rate": 3.7211657785239804e-05,
      "loss": 0.6221,
      "step": 1681800
    },
    {
      "epoch": 15.346923133075407,
      "grad_norm": 3.7606201171875,
      "learning_rate": 3.721089738910383e-05,
      "loss": 0.67,
      "step": 1681900
    },
    {
      "epoch": 15.347835608438572,
      "grad_norm": 4.163708209991455,
      "learning_rate": 3.721013699296786e-05,
      "loss": 0.6819,
      "step": 1682000
    },
    {
      "epoch": 15.348748083801738,
      "grad_norm": 4.7068586349487305,
      "learning_rate": 3.720937659683189e-05,
      "loss": 0.695,
      "step": 1682100
    },
    {
      "epoch": 15.349660559164903,
      "grad_norm": 4.6939873695373535,
      "learning_rate": 3.720861620069592e-05,
      "loss": 0.6608,
      "step": 1682200
    },
    {
      "epoch": 15.350573034528068,
      "grad_norm": 4.256403923034668,
      "learning_rate": 3.720785580455994e-05,
      "loss": 0.6783,
      "step": 1682300
    },
    {
      "epoch": 15.351485509891234,
      "grad_norm": 3.581333875656128,
      "learning_rate": 3.720709540842398e-05,
      "loss": 0.6479,
      "step": 1682400
    },
    {
      "epoch": 15.352397985254399,
      "grad_norm": 3.2188634872436523,
      "learning_rate": 3.7206335012288e-05,
      "loss": 0.677,
      "step": 1682500
    },
    {
      "epoch": 15.353310460617564,
      "grad_norm": 3.5252790451049805,
      "learning_rate": 3.720557461615203e-05,
      "loss": 0.6639,
      "step": 1682600
    },
    {
      "epoch": 15.35422293598073,
      "grad_norm": 3.4458649158477783,
      "learning_rate": 3.720481422001606e-05,
      "loss": 0.6648,
      "step": 1682700
    },
    {
      "epoch": 15.355135411343893,
      "grad_norm": 3.892937660217285,
      "learning_rate": 3.720405382388009e-05,
      "loss": 0.6876,
      "step": 1682800
    },
    {
      "epoch": 15.356047886707058,
      "grad_norm": 3.8765130043029785,
      "learning_rate": 3.720329342774412e-05,
      "loss": 0.6546,
      "step": 1682900
    },
    {
      "epoch": 15.356960362070224,
      "grad_norm": 4.184248924255371,
      "learning_rate": 3.720253303160815e-05,
      "loss": 0.6471,
      "step": 1683000
    },
    {
      "epoch": 15.357872837433389,
      "grad_norm": 3.4457790851593018,
      "learning_rate": 3.7201772635472174e-05,
      "loss": 0.6277,
      "step": 1683100
    },
    {
      "epoch": 15.358785312796554,
      "grad_norm": 4.2264909744262695,
      "learning_rate": 3.720101223933621e-05,
      "loss": 0.6674,
      "step": 1683200
    },
    {
      "epoch": 15.35969778815972,
      "grad_norm": 4.3868889808654785,
      "learning_rate": 3.7200251843200235e-05,
      "loss": 0.6719,
      "step": 1683300
    },
    {
      "epoch": 15.360610263522885,
      "grad_norm": 4.699107646942139,
      "learning_rate": 3.7199491447064265e-05,
      "loss": 0.6334,
      "step": 1683400
    },
    {
      "epoch": 15.36152273888605,
      "grad_norm": 4.771728992462158,
      "learning_rate": 3.7198731050928295e-05,
      "loss": 0.6959,
      "step": 1683500
    },
    {
      "epoch": 15.362435214249215,
      "grad_norm": 3.672963857650757,
      "learning_rate": 3.7197970654792325e-05,
      "loss": 0.6822,
      "step": 1683600
    },
    {
      "epoch": 15.36334768961238,
      "grad_norm": 5.003133296966553,
      "learning_rate": 3.719721025865635e-05,
      "loss": 0.6504,
      "step": 1683700
    },
    {
      "epoch": 15.364260164975546,
      "grad_norm": 3.829514741897583,
      "learning_rate": 3.7196449862520385e-05,
      "loss": 0.6656,
      "step": 1683800
    },
    {
      "epoch": 15.365172640338711,
      "grad_norm": 3.853015422821045,
      "learning_rate": 3.719568946638441e-05,
      "loss": 0.6536,
      "step": 1683900
    },
    {
      "epoch": 15.366085115701877,
      "grad_norm": 4.7844696044921875,
      "learning_rate": 3.719492907024844e-05,
      "loss": 0.6661,
      "step": 1684000
    },
    {
      "epoch": 15.366997591065042,
      "grad_norm": 3.9033498764038086,
      "learning_rate": 3.719416867411247e-05,
      "loss": 0.6568,
      "step": 1684100
    },
    {
      "epoch": 15.367910066428207,
      "grad_norm": 3.7939023971557617,
      "learning_rate": 3.71934082779765e-05,
      "loss": 0.6484,
      "step": 1684200
    },
    {
      "epoch": 15.368822541791372,
      "grad_norm": 3.3114230632781982,
      "learning_rate": 3.719264788184053e-05,
      "loss": 0.6899,
      "step": 1684300
    },
    {
      "epoch": 15.369735017154536,
      "grad_norm": 4.7485246658325195,
      "learning_rate": 3.719188748570455e-05,
      "loss": 0.6472,
      "step": 1684400
    },
    {
      "epoch": 15.370647492517701,
      "grad_norm": 3.70043683052063,
      "learning_rate": 3.719112708956858e-05,
      "loss": 0.6236,
      "step": 1684500
    },
    {
      "epoch": 15.371559967880867,
      "grad_norm": 3.938237190246582,
      "learning_rate": 3.719036669343261e-05,
      "loss": 0.6491,
      "step": 1684600
    },
    {
      "epoch": 15.372472443244032,
      "grad_norm": 3.179884910583496,
      "learning_rate": 3.718960629729664e-05,
      "loss": 0.6296,
      "step": 1684700
    },
    {
      "epoch": 15.373384918607197,
      "grad_norm": 4.154536247253418,
      "learning_rate": 3.7188845901160665e-05,
      "loss": 0.643,
      "step": 1684800
    },
    {
      "epoch": 15.374297393970362,
      "grad_norm": 3.769340753555298,
      "learning_rate": 3.71880855050247e-05,
      "loss": 0.6665,
      "step": 1684900
    },
    {
      "epoch": 15.375209869333528,
      "grad_norm": 4.45132303237915,
      "learning_rate": 3.7187325108888725e-05,
      "loss": 0.6494,
      "step": 1685000
    },
    {
      "epoch": 15.376122344696693,
      "grad_norm": 2.849466323852539,
      "learning_rate": 3.7186564712752755e-05,
      "loss": 0.685,
      "step": 1685100
    },
    {
      "epoch": 15.377034820059858,
      "grad_norm": 2.3758490085601807,
      "learning_rate": 3.7185804316616785e-05,
      "loss": 0.685,
      "step": 1685200
    },
    {
      "epoch": 15.377947295423024,
      "grad_norm": 4.205621719360352,
      "learning_rate": 3.7185043920480816e-05,
      "loss": 0.6793,
      "step": 1685300
    },
    {
      "epoch": 15.378859770786189,
      "grad_norm": 3.733881950378418,
      "learning_rate": 3.7184283524344846e-05,
      "loss": 0.6538,
      "step": 1685400
    },
    {
      "epoch": 15.379772246149354,
      "grad_norm": 3.4627273082733154,
      "learning_rate": 3.7183523128208876e-05,
      "loss": 0.7256,
      "step": 1685500
    },
    {
      "epoch": 15.38068472151252,
      "grad_norm": 3.9768104553222656,
      "learning_rate": 3.71827627320729e-05,
      "loss": 0.644,
      "step": 1685600
    },
    {
      "epoch": 15.381597196875685,
      "grad_norm": 4.298165798187256,
      "learning_rate": 3.7182002335936936e-05,
      "loss": 0.7014,
      "step": 1685700
    },
    {
      "epoch": 15.38250967223885,
      "grad_norm": 4.060419082641602,
      "learning_rate": 3.718124193980096e-05,
      "loss": 0.688,
      "step": 1685800
    },
    {
      "epoch": 15.383422147602015,
      "grad_norm": 4.059964656829834,
      "learning_rate": 3.718048154366499e-05,
      "loss": 0.6612,
      "step": 1685900
    },
    {
      "epoch": 15.38433462296518,
      "grad_norm": 4.571989059448242,
      "learning_rate": 3.717972114752902e-05,
      "loss": 0.6521,
      "step": 1686000
    },
    {
      "epoch": 15.385247098328346,
      "grad_norm": 3.8640198707580566,
      "learning_rate": 3.717896075139305e-05,
      "loss": 0.6387,
      "step": 1686100
    },
    {
      "epoch": 15.38615957369151,
      "grad_norm": 3.4687137603759766,
      "learning_rate": 3.717820035525707e-05,
      "loss": 0.673,
      "step": 1686200
    },
    {
      "epoch": 15.387072049054675,
      "grad_norm": 3.9207863807678223,
      "learning_rate": 3.717743995912111e-05,
      "loss": 0.6767,
      "step": 1686300
    },
    {
      "epoch": 15.38798452441784,
      "grad_norm": 3.4709596633911133,
      "learning_rate": 3.717667956298513e-05,
      "loss": 0.6672,
      "step": 1686400
    },
    {
      "epoch": 15.388896999781005,
      "grad_norm": 3.955230474472046,
      "learning_rate": 3.717591916684916e-05,
      "loss": 0.6545,
      "step": 1686500
    },
    {
      "epoch": 15.38980947514417,
      "grad_norm": 3.7892634868621826,
      "learning_rate": 3.717515877071319e-05,
      "loss": 0.6841,
      "step": 1686600
    },
    {
      "epoch": 15.390721950507336,
      "grad_norm": 5.324248790740967,
      "learning_rate": 3.717439837457722e-05,
      "loss": 0.6398,
      "step": 1686700
    },
    {
      "epoch": 15.391634425870501,
      "grad_norm": 4.826484203338623,
      "learning_rate": 3.717363797844125e-05,
      "loss": 0.6757,
      "step": 1686800
    },
    {
      "epoch": 15.392546901233667,
      "grad_norm": 4.289610385894775,
      "learning_rate": 3.717287758230528e-05,
      "loss": 0.6665,
      "step": 1686900
    },
    {
      "epoch": 15.393459376596832,
      "grad_norm": 2.569831132888794,
      "learning_rate": 3.7172117186169306e-05,
      "loss": 0.6294,
      "step": 1687000
    },
    {
      "epoch": 15.394371851959997,
      "grad_norm": 3.7903237342834473,
      "learning_rate": 3.7171356790033336e-05,
      "loss": 0.6593,
      "step": 1687100
    },
    {
      "epoch": 15.395284327323163,
      "grad_norm": 3.4593000411987305,
      "learning_rate": 3.7170596393897366e-05,
      "loss": 0.6317,
      "step": 1687200
    },
    {
      "epoch": 15.396196802686328,
      "grad_norm": 3.213731050491333,
      "learning_rate": 3.716983599776139e-05,
      "loss": 0.6713,
      "step": 1687300
    },
    {
      "epoch": 15.397109278049493,
      "grad_norm": 4.731935024261475,
      "learning_rate": 3.7169075601625427e-05,
      "loss": 0.6701,
      "step": 1687400
    },
    {
      "epoch": 15.398021753412658,
      "grad_norm": 4.853127956390381,
      "learning_rate": 3.716831520548945e-05,
      "loss": 0.6984,
      "step": 1687500
    },
    {
      "epoch": 15.398934228775824,
      "grad_norm": 3.8826427459716797,
      "learning_rate": 3.716755480935348e-05,
      "loss": 0.6882,
      "step": 1687600
    },
    {
      "epoch": 15.399846704138989,
      "grad_norm": 4.29044246673584,
      "learning_rate": 3.716679441321751e-05,
      "loss": 0.6449,
      "step": 1687700
    },
    {
      "epoch": 15.400759179502153,
      "grad_norm": 3.869286060333252,
      "learning_rate": 3.716603401708154e-05,
      "loss": 0.6917,
      "step": 1687800
    },
    {
      "epoch": 15.401671654865318,
      "grad_norm": 4.15446138381958,
      "learning_rate": 3.716527362094557e-05,
      "loss": 0.6802,
      "step": 1687900
    },
    {
      "epoch": 15.402584130228483,
      "grad_norm": 4.137108325958252,
      "learning_rate": 3.71645132248096e-05,
      "loss": 0.6497,
      "step": 1688000
    },
    {
      "epoch": 15.403496605591648,
      "grad_norm": 4.518894672393799,
      "learning_rate": 3.7163752828673624e-05,
      "loss": 0.6474,
      "step": 1688100
    },
    {
      "epoch": 15.404409080954814,
      "grad_norm": 3.5185513496398926,
      "learning_rate": 3.716299243253766e-05,
      "loss": 0.7153,
      "step": 1688200
    },
    {
      "epoch": 15.405321556317979,
      "grad_norm": 3.9746971130371094,
      "learning_rate": 3.7162232036401684e-05,
      "loss": 0.6784,
      "step": 1688300
    },
    {
      "epoch": 15.406234031681144,
      "grad_norm": 5.002354145050049,
      "learning_rate": 3.7161471640265714e-05,
      "loss": 0.651,
      "step": 1688400
    },
    {
      "epoch": 15.40714650704431,
      "grad_norm": 3.2004666328430176,
      "learning_rate": 3.7160711244129744e-05,
      "loss": 0.6843,
      "step": 1688500
    },
    {
      "epoch": 15.408058982407475,
      "grad_norm": 4.175314426422119,
      "learning_rate": 3.7159950847993774e-05,
      "loss": 0.6601,
      "step": 1688600
    },
    {
      "epoch": 15.40897145777064,
      "grad_norm": 4.353363990783691,
      "learning_rate": 3.71591904518578e-05,
      "loss": 0.6479,
      "step": 1688700
    },
    {
      "epoch": 15.409883933133806,
      "grad_norm": 4.120945453643799,
      "learning_rate": 3.7158430055721834e-05,
      "loss": 0.6912,
      "step": 1688800
    },
    {
      "epoch": 15.41079640849697,
      "grad_norm": 4.105353355407715,
      "learning_rate": 3.715766965958586e-05,
      "loss": 0.6469,
      "step": 1688900
    },
    {
      "epoch": 15.411708883860136,
      "grad_norm": 4.312320709228516,
      "learning_rate": 3.715690926344989e-05,
      "loss": 0.6794,
      "step": 1689000
    },
    {
      "epoch": 15.412621359223301,
      "grad_norm": 4.518721103668213,
      "learning_rate": 3.715614886731392e-05,
      "loss": 0.7074,
      "step": 1689100
    },
    {
      "epoch": 15.413533834586467,
      "grad_norm": 4.4277753829956055,
      "learning_rate": 3.715538847117795e-05,
      "loss": 0.6893,
      "step": 1689200
    },
    {
      "epoch": 15.414446309949632,
      "grad_norm": 3.8953888416290283,
      "learning_rate": 3.715462807504198e-05,
      "loss": 0.6496,
      "step": 1689300
    },
    {
      "epoch": 15.415358785312797,
      "grad_norm": 3.3112552165985107,
      "learning_rate": 3.715386767890601e-05,
      "loss": 0.6415,
      "step": 1689400
    },
    {
      "epoch": 15.416271260675963,
      "grad_norm": 4.379939079284668,
      "learning_rate": 3.715310728277003e-05,
      "loss": 0.6736,
      "step": 1689500
    },
    {
      "epoch": 15.417183736039126,
      "grad_norm": 3.8976473808288574,
      "learning_rate": 3.715234688663407e-05,
      "loss": 0.6744,
      "step": 1689600
    },
    {
      "epoch": 15.418096211402291,
      "grad_norm": 4.435991287231445,
      "learning_rate": 3.715158649049809e-05,
      "loss": 0.6844,
      "step": 1689700
    },
    {
      "epoch": 15.419008686765457,
      "grad_norm": 2.1789493560791016,
      "learning_rate": 3.715082609436212e-05,
      "loss": 0.6269,
      "step": 1689800
    },
    {
      "epoch": 15.419921162128622,
      "grad_norm": 4.251760005950928,
      "learning_rate": 3.715006569822615e-05,
      "loss": 0.6949,
      "step": 1689900
    },
    {
      "epoch": 15.420833637491787,
      "grad_norm": 4.8074774742126465,
      "learning_rate": 3.7149305302090174e-05,
      "loss": 0.6433,
      "step": 1690000
    },
    {
      "epoch": 15.421746112854953,
      "grad_norm": 3.594491958618164,
      "learning_rate": 3.7148544905954205e-05,
      "loss": 0.6778,
      "step": 1690100
    },
    {
      "epoch": 15.422658588218118,
      "grad_norm": 3.6879100799560547,
      "learning_rate": 3.7147784509818235e-05,
      "loss": 0.6644,
      "step": 1690200
    },
    {
      "epoch": 15.423571063581283,
      "grad_norm": 3.620281934738159,
      "learning_rate": 3.7147024113682265e-05,
      "loss": 0.7101,
      "step": 1690300
    },
    {
      "epoch": 15.424483538944449,
      "grad_norm": 4.493088245391846,
      "learning_rate": 3.7146263717546295e-05,
      "loss": 0.6251,
      "step": 1690400
    },
    {
      "epoch": 15.425396014307614,
      "grad_norm": 3.8117752075195312,
      "learning_rate": 3.7145503321410325e-05,
      "loss": 0.6971,
      "step": 1690500
    },
    {
      "epoch": 15.42630848967078,
      "grad_norm": 3.6760170459747314,
      "learning_rate": 3.714474292527435e-05,
      "loss": 0.6843,
      "step": 1690600
    },
    {
      "epoch": 15.427220965033944,
      "grad_norm": 3.751465082168579,
      "learning_rate": 3.7143982529138385e-05,
      "loss": 0.6244,
      "step": 1690700
    },
    {
      "epoch": 15.42813344039711,
      "grad_norm": 3.7347571849823,
      "learning_rate": 3.714322213300241e-05,
      "loss": 0.669,
      "step": 1690800
    },
    {
      "epoch": 15.429045915760275,
      "grad_norm": 4.3841328620910645,
      "learning_rate": 3.714246173686644e-05,
      "loss": 0.6704,
      "step": 1690900
    },
    {
      "epoch": 15.42995839112344,
      "grad_norm": 4.014272689819336,
      "learning_rate": 3.714170134073047e-05,
      "loss": 0.6955,
      "step": 1691000
    },
    {
      "epoch": 15.430870866486606,
      "grad_norm": 2.7354400157928467,
      "learning_rate": 3.71409409445945e-05,
      "loss": 0.6359,
      "step": 1691100
    },
    {
      "epoch": 15.43178334184977,
      "grad_norm": 4.757497787475586,
      "learning_rate": 3.714018054845853e-05,
      "loss": 0.6734,
      "step": 1691200
    },
    {
      "epoch": 15.432695817212934,
      "grad_norm": 4.47097110748291,
      "learning_rate": 3.713942015232256e-05,
      "loss": 0.6773,
      "step": 1691300
    },
    {
      "epoch": 15.4336082925761,
      "grad_norm": 4.759372234344482,
      "learning_rate": 3.713865975618658e-05,
      "loss": 0.7021,
      "step": 1691400
    },
    {
      "epoch": 15.434520767939265,
      "grad_norm": 4.329884052276611,
      "learning_rate": 3.713789936005062e-05,
      "loss": 0.6646,
      "step": 1691500
    },
    {
      "epoch": 15.43543324330243,
      "grad_norm": 3.466588020324707,
      "learning_rate": 3.713713896391464e-05,
      "loss": 0.6788,
      "step": 1691600
    },
    {
      "epoch": 15.436345718665596,
      "grad_norm": 4.463345050811768,
      "learning_rate": 3.713637856777867e-05,
      "loss": 0.6652,
      "step": 1691700
    },
    {
      "epoch": 15.437258194028761,
      "grad_norm": 3.5965373516082764,
      "learning_rate": 3.71356181716427e-05,
      "loss": 0.6584,
      "step": 1691800
    },
    {
      "epoch": 15.438170669391926,
      "grad_norm": 4.020295143127441,
      "learning_rate": 3.713485777550673e-05,
      "loss": 0.6754,
      "step": 1691900
    },
    {
      "epoch": 15.439083144755092,
      "grad_norm": 4.039028644561768,
      "learning_rate": 3.7134097379370755e-05,
      "loss": 0.7031,
      "step": 1692000
    },
    {
      "epoch": 15.439995620118257,
      "grad_norm": 4.282838821411133,
      "learning_rate": 3.713333698323479e-05,
      "loss": 0.6873,
      "step": 1692100
    },
    {
      "epoch": 15.440908095481422,
      "grad_norm": 3.757382392883301,
      "learning_rate": 3.7132576587098816e-05,
      "loss": 0.6308,
      "step": 1692200
    },
    {
      "epoch": 15.441820570844587,
      "grad_norm": 4.233061790466309,
      "learning_rate": 3.7131816190962846e-05,
      "loss": 0.6906,
      "step": 1692300
    },
    {
      "epoch": 15.442733046207753,
      "grad_norm": 4.288633823394775,
      "learning_rate": 3.7131055794826876e-05,
      "loss": 0.6603,
      "step": 1692400
    },
    {
      "epoch": 15.443645521570918,
      "grad_norm": 4.202301502227783,
      "learning_rate": 3.7130295398690906e-05,
      "loss": 0.642,
      "step": 1692500
    },
    {
      "epoch": 15.444557996934083,
      "grad_norm": 4.709185600280762,
      "learning_rate": 3.7129535002554936e-05,
      "loss": 0.7069,
      "step": 1692600
    },
    {
      "epoch": 15.445470472297249,
      "grad_norm": 4.763954162597656,
      "learning_rate": 3.7128774606418966e-05,
      "loss": 0.6174,
      "step": 1692700
    },
    {
      "epoch": 15.446382947660414,
      "grad_norm": 4.358583450317383,
      "learning_rate": 3.712801421028299e-05,
      "loss": 0.6728,
      "step": 1692800
    },
    {
      "epoch": 15.44729542302358,
      "grad_norm": 4.25196647644043,
      "learning_rate": 3.712725381414702e-05,
      "loss": 0.6984,
      "step": 1692900
    },
    {
      "epoch": 15.448207898386743,
      "grad_norm": 3.556563377380371,
      "learning_rate": 3.712649341801105e-05,
      "loss": 0.6641,
      "step": 1693000
    },
    {
      "epoch": 15.449120373749908,
      "grad_norm": 3.5791780948638916,
      "learning_rate": 3.712573302187507e-05,
      "loss": 0.623,
      "step": 1693100
    },
    {
      "epoch": 15.450032849113073,
      "grad_norm": 4.069565296173096,
      "learning_rate": 3.712497262573911e-05,
      "loss": 0.6581,
      "step": 1693200
    },
    {
      "epoch": 15.450945324476239,
      "grad_norm": 3.784785747528076,
      "learning_rate": 3.712421222960313e-05,
      "loss": 0.6613,
      "step": 1693300
    },
    {
      "epoch": 15.451857799839404,
      "grad_norm": 4.281949520111084,
      "learning_rate": 3.712345183346716e-05,
      "loss": 0.6765,
      "step": 1693400
    },
    {
      "epoch": 15.45277027520257,
      "grad_norm": 4.3714776039123535,
      "learning_rate": 3.712269143733119e-05,
      "loss": 0.6835,
      "step": 1693500
    },
    {
      "epoch": 15.453682750565735,
      "grad_norm": 3.583183526992798,
      "learning_rate": 3.712193104119522e-05,
      "loss": 0.7356,
      "step": 1693600
    },
    {
      "epoch": 15.4545952259289,
      "grad_norm": 3.4959332942962646,
      "learning_rate": 3.712117064505925e-05,
      "loss": 0.6561,
      "step": 1693700
    },
    {
      "epoch": 15.455507701292065,
      "grad_norm": 3.858124256134033,
      "learning_rate": 3.712041024892328e-05,
      "loss": 0.6919,
      "step": 1693800
    },
    {
      "epoch": 15.45642017665523,
      "grad_norm": 3.896362781524658,
      "learning_rate": 3.7119649852787306e-05,
      "loss": 0.691,
      "step": 1693900
    },
    {
      "epoch": 15.457332652018396,
      "grad_norm": 4.091156005859375,
      "learning_rate": 3.711888945665134e-05,
      "loss": 0.6743,
      "step": 1694000
    },
    {
      "epoch": 15.458245127381561,
      "grad_norm": 4.379788875579834,
      "learning_rate": 3.7118129060515367e-05,
      "loss": 0.6666,
      "step": 1694100
    },
    {
      "epoch": 15.459157602744726,
      "grad_norm": 4.049065589904785,
      "learning_rate": 3.71173686643794e-05,
      "loss": 0.6615,
      "step": 1694200
    },
    {
      "epoch": 15.460070078107892,
      "grad_norm": 4.084452152252197,
      "learning_rate": 3.711660826824343e-05,
      "loss": 0.6588,
      "step": 1694300
    },
    {
      "epoch": 15.460982553471057,
      "grad_norm": 3.0776283740997314,
      "learning_rate": 3.711584787210746e-05,
      "loss": 0.644,
      "step": 1694400
    },
    {
      "epoch": 15.461895028834222,
      "grad_norm": 3.7905209064483643,
      "learning_rate": 3.711508747597148e-05,
      "loss": 0.6378,
      "step": 1694500
    },
    {
      "epoch": 15.462807504197386,
      "grad_norm": 3.6794674396514893,
      "learning_rate": 3.711432707983552e-05,
      "loss": 0.6671,
      "step": 1694600
    },
    {
      "epoch": 15.463719979560551,
      "grad_norm": 5.098208904266357,
      "learning_rate": 3.711356668369954e-05,
      "loss": 0.7136,
      "step": 1694700
    },
    {
      "epoch": 15.464632454923716,
      "grad_norm": 4.272510528564453,
      "learning_rate": 3.711280628756357e-05,
      "loss": 0.6682,
      "step": 1694800
    },
    {
      "epoch": 15.465544930286882,
      "grad_norm": 4.37568998336792,
      "learning_rate": 3.71120458914276e-05,
      "loss": 0.6248,
      "step": 1694900
    },
    {
      "epoch": 15.466457405650047,
      "grad_norm": 3.8448071479797363,
      "learning_rate": 3.711128549529163e-05,
      "loss": 0.6393,
      "step": 1695000
    },
    {
      "epoch": 15.467369881013212,
      "grad_norm": 3.5238044261932373,
      "learning_rate": 3.711052509915566e-05,
      "loss": 0.6437,
      "step": 1695100
    },
    {
      "epoch": 15.468282356376378,
      "grad_norm": 3.910264015197754,
      "learning_rate": 3.710976470301969e-05,
      "loss": 0.6687,
      "step": 1695200
    },
    {
      "epoch": 15.469194831739543,
      "grad_norm": 3.3585855960845947,
      "learning_rate": 3.7109004306883714e-05,
      "loss": 0.6525,
      "step": 1695300
    },
    {
      "epoch": 15.470107307102708,
      "grad_norm": 4.57682991027832,
      "learning_rate": 3.710824391074775e-05,
      "loss": 0.6691,
      "step": 1695400
    },
    {
      "epoch": 15.471019782465874,
      "grad_norm": 4.146503925323486,
      "learning_rate": 3.7107483514611774e-05,
      "loss": 0.6294,
      "step": 1695500
    },
    {
      "epoch": 15.471932257829039,
      "grad_norm": 4.311215877532959,
      "learning_rate": 3.71067231184758e-05,
      "loss": 0.6765,
      "step": 1695600
    },
    {
      "epoch": 15.472844733192204,
      "grad_norm": 3.741452693939209,
      "learning_rate": 3.7105962722339834e-05,
      "loss": 0.6723,
      "step": 1695700
    },
    {
      "epoch": 15.47375720855537,
      "grad_norm": 3.8822267055511475,
      "learning_rate": 3.710520232620386e-05,
      "loss": 0.6586,
      "step": 1695800
    },
    {
      "epoch": 15.474669683918535,
      "grad_norm": 3.287013530731201,
      "learning_rate": 3.710444193006789e-05,
      "loss": 0.6868,
      "step": 1695900
    },
    {
      "epoch": 15.4755821592817,
      "grad_norm": 3.8943824768066406,
      "learning_rate": 3.710368153393192e-05,
      "loss": 0.6576,
      "step": 1696000
    },
    {
      "epoch": 15.476494634644865,
      "grad_norm": 4.83600378036499,
      "learning_rate": 3.710292113779595e-05,
      "loss": 0.656,
      "step": 1696100
    },
    {
      "epoch": 15.47740711000803,
      "grad_norm": 4.8166327476501465,
      "learning_rate": 3.710216074165998e-05,
      "loss": 0.6898,
      "step": 1696200
    },
    {
      "epoch": 15.478319585371194,
      "grad_norm": 4.248542308807373,
      "learning_rate": 3.710140034552401e-05,
      "loss": 0.6436,
      "step": 1696300
    },
    {
      "epoch": 15.47923206073436,
      "grad_norm": 3.783724069595337,
      "learning_rate": 3.710063994938803e-05,
      "loss": 0.6085,
      "step": 1696400
    },
    {
      "epoch": 15.480144536097525,
      "grad_norm": 3.3865957260131836,
      "learning_rate": 3.709987955325207e-05,
      "loss": 0.6292,
      "step": 1696500
    },
    {
      "epoch": 15.48105701146069,
      "grad_norm": 4.762274742126465,
      "learning_rate": 3.709911915711609e-05,
      "loss": 0.6585,
      "step": 1696600
    },
    {
      "epoch": 15.481969486823855,
      "grad_norm": 4.070156097412109,
      "learning_rate": 3.709835876098012e-05,
      "loss": 0.6683,
      "step": 1696700
    },
    {
      "epoch": 15.48288196218702,
      "grad_norm": 2.564887523651123,
      "learning_rate": 3.709759836484415e-05,
      "loss": 0.6442,
      "step": 1696800
    },
    {
      "epoch": 15.483794437550186,
      "grad_norm": 3.690323829650879,
      "learning_rate": 3.709683796870818e-05,
      "loss": 0.6446,
      "step": 1696900
    },
    {
      "epoch": 15.484706912913351,
      "grad_norm": 3.9758265018463135,
      "learning_rate": 3.7096077572572205e-05,
      "loss": 0.6754,
      "step": 1697000
    },
    {
      "epoch": 15.485619388276517,
      "grad_norm": 3.5492098331451416,
      "learning_rate": 3.709531717643624e-05,
      "loss": 0.6561,
      "step": 1697100
    },
    {
      "epoch": 15.486531863639682,
      "grad_norm": 3.5771820545196533,
      "learning_rate": 3.7094556780300265e-05,
      "loss": 0.6896,
      "step": 1697200
    },
    {
      "epoch": 15.487444339002847,
      "grad_norm": 3.6367342472076416,
      "learning_rate": 3.7093796384164295e-05,
      "loss": 0.6781,
      "step": 1697300
    },
    {
      "epoch": 15.488356814366012,
      "grad_norm": 4.5057196617126465,
      "learning_rate": 3.7093035988028325e-05,
      "loss": 0.6763,
      "step": 1697400
    },
    {
      "epoch": 15.489269289729178,
      "grad_norm": 3.9820613861083984,
      "learning_rate": 3.7092275591892355e-05,
      "loss": 0.6849,
      "step": 1697500
    },
    {
      "epoch": 15.490181765092343,
      "grad_norm": 3.5216002464294434,
      "learning_rate": 3.7091515195756385e-05,
      "loss": 0.6427,
      "step": 1697600
    },
    {
      "epoch": 15.491094240455508,
      "grad_norm": 3.795948028564453,
      "learning_rate": 3.7090754799620415e-05,
      "loss": 0.6067,
      "step": 1697700
    },
    {
      "epoch": 15.492006715818674,
      "grad_norm": 4.223226547241211,
      "learning_rate": 3.708999440348444e-05,
      "loss": 0.6467,
      "step": 1697800
    },
    {
      "epoch": 15.492919191181839,
      "grad_norm": 3.2235026359558105,
      "learning_rate": 3.7089234007348475e-05,
      "loss": 0.6755,
      "step": 1697900
    },
    {
      "epoch": 15.493831666545002,
      "grad_norm": 3.5870730876922607,
      "learning_rate": 3.70884736112125e-05,
      "loss": 0.6257,
      "step": 1698000
    },
    {
      "epoch": 15.494744141908168,
      "grad_norm": 4.3122124671936035,
      "learning_rate": 3.708771321507653e-05,
      "loss": 0.6724,
      "step": 1698100
    },
    {
      "epoch": 15.495656617271333,
      "grad_norm": 3.74725604057312,
      "learning_rate": 3.708695281894056e-05,
      "loss": 0.647,
      "step": 1698200
    },
    {
      "epoch": 15.496569092634498,
      "grad_norm": 4.072632312774658,
      "learning_rate": 3.708619242280459e-05,
      "loss": 0.7078,
      "step": 1698300
    },
    {
      "epoch": 15.497481567997664,
      "grad_norm": 3.300823926925659,
      "learning_rate": 3.708543202666861e-05,
      "loss": 0.6503,
      "step": 1698400
    },
    {
      "epoch": 15.498394043360829,
      "grad_norm": 3.8651816844940186,
      "learning_rate": 3.708467163053264e-05,
      "loss": 0.6312,
      "step": 1698500
    },
    {
      "epoch": 15.499306518723994,
      "grad_norm": 3.4680745601654053,
      "learning_rate": 3.708391123439667e-05,
      "loss": 0.7157,
      "step": 1698600
    },
    {
      "epoch": 15.50021899408716,
      "grad_norm": 3.9060416221618652,
      "learning_rate": 3.70831508382607e-05,
      "loss": 0.6639,
      "step": 1698700
    },
    {
      "epoch": 15.501131469450325,
      "grad_norm": 3.4338650703430176,
      "learning_rate": 3.708239044212473e-05,
      "loss": 0.6923,
      "step": 1698800
    },
    {
      "epoch": 15.50204394481349,
      "grad_norm": 3.542788028717041,
      "learning_rate": 3.7081630045988756e-05,
      "loss": 0.6772,
      "step": 1698900
    },
    {
      "epoch": 15.502956420176655,
      "grad_norm": 3.609152317047119,
      "learning_rate": 3.708086964985279e-05,
      "loss": 0.7139,
      "step": 1699000
    },
    {
      "epoch": 15.50386889553982,
      "grad_norm": 3.544696807861328,
      "learning_rate": 3.7080109253716816e-05,
      "loss": 0.6662,
      "step": 1699100
    },
    {
      "epoch": 15.504781370902986,
      "grad_norm": 4.196390151977539,
      "learning_rate": 3.7079348857580846e-05,
      "loss": 0.6687,
      "step": 1699200
    },
    {
      "epoch": 15.505693846266151,
      "grad_norm": 3.4349546432495117,
      "learning_rate": 3.7078588461444876e-05,
      "loss": 0.6673,
      "step": 1699300
    },
    {
      "epoch": 15.506606321629317,
      "grad_norm": 3.759097099304199,
      "learning_rate": 3.7077828065308906e-05,
      "loss": 0.677,
      "step": 1699400
    },
    {
      "epoch": 15.507518796992482,
      "grad_norm": 3.802494525909424,
      "learning_rate": 3.707706766917293e-05,
      "loss": 0.6618,
      "step": 1699500
    },
    {
      "epoch": 15.508431272355647,
      "grad_norm": 3.7407681941986084,
      "learning_rate": 3.7076307273036966e-05,
      "loss": 0.6968,
      "step": 1699600
    },
    {
      "epoch": 15.509343747718813,
      "grad_norm": 4.1423869132995605,
      "learning_rate": 3.707554687690099e-05,
      "loss": 0.6593,
      "step": 1699700
    },
    {
      "epoch": 15.510256223081976,
      "grad_norm": 3.8599531650543213,
      "learning_rate": 3.707478648076502e-05,
      "loss": 0.6752,
      "step": 1699800
    },
    {
      "epoch": 15.511168698445141,
      "grad_norm": 4.2651472091674805,
      "learning_rate": 3.707402608462905e-05,
      "loss": 0.6705,
      "step": 1699900
    },
    {
      "epoch": 15.512081173808307,
      "grad_norm": 4.372077465057373,
      "learning_rate": 3.707326568849308e-05,
      "loss": 0.6631,
      "step": 1700000
    },
    {
      "epoch": 15.512993649171472,
      "grad_norm": 4.315489292144775,
      "learning_rate": 3.707250529235711e-05,
      "loss": 0.688,
      "step": 1700100
    },
    {
      "epoch": 15.513906124534637,
      "grad_norm": 3.2298669815063477,
      "learning_rate": 3.707174489622114e-05,
      "loss": 0.6595,
      "step": 1700200
    },
    {
      "epoch": 15.514818599897803,
      "grad_norm": 3.6321983337402344,
      "learning_rate": 3.707098450008516e-05,
      "loss": 0.6769,
      "step": 1700300
    },
    {
      "epoch": 15.515731075260968,
      "grad_norm": 4.633880138397217,
      "learning_rate": 3.70702241039492e-05,
      "loss": 0.7033,
      "step": 1700400
    },
    {
      "epoch": 15.516643550624133,
      "grad_norm": 3.7088541984558105,
      "learning_rate": 3.706946370781322e-05,
      "loss": 0.7112,
      "step": 1700500
    },
    {
      "epoch": 15.517556025987298,
      "grad_norm": 3.997267961502075,
      "learning_rate": 3.706870331167725e-05,
      "loss": 0.6444,
      "step": 1700600
    },
    {
      "epoch": 15.518468501350464,
      "grad_norm": 4.417137145996094,
      "learning_rate": 3.706794291554128e-05,
      "loss": 0.7486,
      "step": 1700700
    },
    {
      "epoch": 15.519380976713629,
      "grad_norm": 4.648475646972656,
      "learning_rate": 3.706718251940531e-05,
      "loss": 0.6912,
      "step": 1700800
    },
    {
      "epoch": 15.520293452076794,
      "grad_norm": 4.279820442199707,
      "learning_rate": 3.7066422123269337e-05,
      "loss": 0.6677,
      "step": 1700900
    },
    {
      "epoch": 15.52120592743996,
      "grad_norm": 4.484062671661377,
      "learning_rate": 3.7065661727133373e-05,
      "loss": 0.6522,
      "step": 1701000
    },
    {
      "epoch": 15.522118402803125,
      "grad_norm": 4.950146675109863,
      "learning_rate": 3.70649013309974e-05,
      "loss": 0.6951,
      "step": 1701100
    },
    {
      "epoch": 15.52303087816629,
      "grad_norm": 4.085859298706055,
      "learning_rate": 3.706414093486143e-05,
      "loss": 0.6554,
      "step": 1701200
    },
    {
      "epoch": 15.523943353529456,
      "grad_norm": 3.629709482192993,
      "learning_rate": 3.706338053872546e-05,
      "loss": 0.6477,
      "step": 1701300
    },
    {
      "epoch": 15.524855828892619,
      "grad_norm": 3.6477725505828857,
      "learning_rate": 3.706262014258948e-05,
      "loss": 0.6837,
      "step": 1701400
    },
    {
      "epoch": 15.525768304255784,
      "grad_norm": 3.9735500812530518,
      "learning_rate": 3.706185974645352e-05,
      "loss": 0.6498,
      "step": 1701500
    },
    {
      "epoch": 15.52668077961895,
      "grad_norm": 3.875364065170288,
      "learning_rate": 3.706109935031754e-05,
      "loss": 0.6647,
      "step": 1701600
    },
    {
      "epoch": 15.527593254982115,
      "grad_norm": 3.245466709136963,
      "learning_rate": 3.706033895418157e-05,
      "loss": 0.6908,
      "step": 1701700
    },
    {
      "epoch": 15.52850573034528,
      "grad_norm": 3.819772958755493,
      "learning_rate": 3.70595785580456e-05,
      "loss": 0.6529,
      "step": 1701800
    },
    {
      "epoch": 15.529418205708446,
      "grad_norm": 3.893531560897827,
      "learning_rate": 3.705881816190963e-05,
      "loss": 0.683,
      "step": 1701900
    },
    {
      "epoch": 15.53033068107161,
      "grad_norm": 5.40615177154541,
      "learning_rate": 3.705805776577366e-05,
      "loss": 0.6467,
      "step": 1702000
    },
    {
      "epoch": 15.531243156434776,
      "grad_norm": 3.367473602294922,
      "learning_rate": 3.705729736963769e-05,
      "loss": 0.6857,
      "step": 1702100
    },
    {
      "epoch": 15.532155631797941,
      "grad_norm": 4.430902481079102,
      "learning_rate": 3.7056536973501714e-05,
      "loss": 0.6731,
      "step": 1702200
    },
    {
      "epoch": 15.533068107161107,
      "grad_norm": 4.122712135314941,
      "learning_rate": 3.7055776577365744e-05,
      "loss": 0.7242,
      "step": 1702300
    },
    {
      "epoch": 15.533980582524272,
      "grad_norm": 3.9986414909362793,
      "learning_rate": 3.7055016181229774e-05,
      "loss": 0.6946,
      "step": 1702400
    },
    {
      "epoch": 15.534893057887437,
      "grad_norm": 3.561960220336914,
      "learning_rate": 3.7054255785093804e-05,
      "loss": 0.6433,
      "step": 1702500
    },
    {
      "epoch": 15.535805533250603,
      "grad_norm": 4.4577531814575195,
      "learning_rate": 3.7053495388957834e-05,
      "loss": 0.6762,
      "step": 1702600
    },
    {
      "epoch": 15.536718008613768,
      "grad_norm": 3.7848119735717773,
      "learning_rate": 3.7052734992821864e-05,
      "loss": 0.6725,
      "step": 1702700
    },
    {
      "epoch": 15.537630483976933,
      "grad_norm": 6.345758438110352,
      "learning_rate": 3.705197459668589e-05,
      "loss": 0.6807,
      "step": 1702800
    },
    {
      "epoch": 15.538542959340099,
      "grad_norm": 4.566585540771484,
      "learning_rate": 3.7051214200549924e-05,
      "loss": 0.6922,
      "step": 1702900
    },
    {
      "epoch": 15.539455434703264,
      "grad_norm": 3.5919952392578125,
      "learning_rate": 3.705045380441395e-05,
      "loss": 0.6694,
      "step": 1703000
    },
    {
      "epoch": 15.54036791006643,
      "grad_norm": 4.705540180206299,
      "learning_rate": 3.704969340827798e-05,
      "loss": 0.663,
      "step": 1703100
    },
    {
      "epoch": 15.541280385429593,
      "grad_norm": 3.2422327995300293,
      "learning_rate": 3.704893301214201e-05,
      "loss": 0.6701,
      "step": 1703200
    },
    {
      "epoch": 15.542192860792758,
      "grad_norm": 3.6288552284240723,
      "learning_rate": 3.704817261600604e-05,
      "loss": 0.6617,
      "step": 1703300
    },
    {
      "epoch": 15.543105336155923,
      "grad_norm": 4.696967601776123,
      "learning_rate": 3.704741221987007e-05,
      "loss": 0.7053,
      "step": 1703400
    },
    {
      "epoch": 15.544017811519089,
      "grad_norm": 3.8850674629211426,
      "learning_rate": 3.70466518237341e-05,
      "loss": 0.6743,
      "step": 1703500
    },
    {
      "epoch": 15.544930286882254,
      "grad_norm": 3.7095723152160645,
      "learning_rate": 3.704589142759812e-05,
      "loss": 0.6686,
      "step": 1703600
    },
    {
      "epoch": 15.54584276224542,
      "grad_norm": 3.9544334411621094,
      "learning_rate": 3.704513103146215e-05,
      "loss": 0.6681,
      "step": 1703700
    },
    {
      "epoch": 15.546755237608584,
      "grad_norm": 3.257913827896118,
      "learning_rate": 3.704437063532618e-05,
      "loss": 0.6614,
      "step": 1703800
    },
    {
      "epoch": 15.54766771297175,
      "grad_norm": 4.413937568664551,
      "learning_rate": 3.704361023919021e-05,
      "loss": 0.6223,
      "step": 1703900
    },
    {
      "epoch": 15.548580188334915,
      "grad_norm": 3.5550432205200195,
      "learning_rate": 3.704284984305424e-05,
      "loss": 0.6429,
      "step": 1704000
    },
    {
      "epoch": 15.54949266369808,
      "grad_norm": 5.096933841705322,
      "learning_rate": 3.7042089446918265e-05,
      "loss": 0.6869,
      "step": 1704100
    },
    {
      "epoch": 15.550405139061246,
      "grad_norm": 3.960881471633911,
      "learning_rate": 3.7041329050782295e-05,
      "loss": 0.689,
      "step": 1704200
    },
    {
      "epoch": 15.551317614424411,
      "grad_norm": 3.785212278366089,
      "learning_rate": 3.7040568654646325e-05,
      "loss": 0.6596,
      "step": 1704300
    },
    {
      "epoch": 15.552230089787576,
      "grad_norm": 2.8368232250213623,
      "learning_rate": 3.7039808258510355e-05,
      "loss": 0.6509,
      "step": 1704400
    },
    {
      "epoch": 15.553142565150742,
      "grad_norm": 2.49600887298584,
      "learning_rate": 3.7039047862374385e-05,
      "loss": 0.649,
      "step": 1704500
    },
    {
      "epoch": 15.554055040513907,
      "grad_norm": 4.040917873382568,
      "learning_rate": 3.7038287466238415e-05,
      "loss": 0.6994,
      "step": 1704600
    },
    {
      "epoch": 15.55496751587707,
      "grad_norm": 3.498636484146118,
      "learning_rate": 3.703752707010244e-05,
      "loss": 0.6694,
      "step": 1704700
    },
    {
      "epoch": 15.555879991240236,
      "grad_norm": 3.8091554641723633,
      "learning_rate": 3.7036766673966475e-05,
      "loss": 0.7074,
      "step": 1704800
    },
    {
      "epoch": 15.556792466603401,
      "grad_norm": 4.889760494232178,
      "learning_rate": 3.70360062778305e-05,
      "loss": 0.6653,
      "step": 1704900
    },
    {
      "epoch": 15.557704941966566,
      "grad_norm": 3.8171732425689697,
      "learning_rate": 3.703524588169453e-05,
      "loss": 0.662,
      "step": 1705000
    },
    {
      "epoch": 15.558617417329732,
      "grad_norm": 3.92181658744812,
      "learning_rate": 3.703448548555856e-05,
      "loss": 0.6329,
      "step": 1705100
    },
    {
      "epoch": 15.559529892692897,
      "grad_norm": 3.9870872497558594,
      "learning_rate": 3.703372508942259e-05,
      "loss": 0.6752,
      "step": 1705200
    },
    {
      "epoch": 15.560442368056062,
      "grad_norm": 2.1178245544433594,
      "learning_rate": 3.703296469328661e-05,
      "loss": 0.6657,
      "step": 1705300
    },
    {
      "epoch": 15.561354843419227,
      "grad_norm": 4.581848621368408,
      "learning_rate": 3.703220429715065e-05,
      "loss": 0.6732,
      "step": 1705400
    },
    {
      "epoch": 15.562267318782393,
      "grad_norm": 3.1694869995117188,
      "learning_rate": 3.703144390101467e-05,
      "loss": 0.6427,
      "step": 1705500
    },
    {
      "epoch": 15.563179794145558,
      "grad_norm": 4.196112632751465,
      "learning_rate": 3.70306835048787e-05,
      "loss": 0.6954,
      "step": 1705600
    },
    {
      "epoch": 15.564092269508723,
      "grad_norm": 3.285672187805176,
      "learning_rate": 3.702992310874273e-05,
      "loss": 0.6822,
      "step": 1705700
    },
    {
      "epoch": 15.565004744871889,
      "grad_norm": 4.27164363861084,
      "learning_rate": 3.702916271260676e-05,
      "loss": 0.614,
      "step": 1705800
    },
    {
      "epoch": 15.565917220235054,
      "grad_norm": 3.698127508163452,
      "learning_rate": 3.702840231647079e-05,
      "loss": 0.6933,
      "step": 1705900
    },
    {
      "epoch": 15.56682969559822,
      "grad_norm": 3.485750436782837,
      "learning_rate": 3.702764192033482e-05,
      "loss": 0.6863,
      "step": 1706000
    },
    {
      "epoch": 15.567742170961385,
      "grad_norm": 3.5671260356903076,
      "learning_rate": 3.7026881524198846e-05,
      "loss": 0.6777,
      "step": 1706100
    },
    {
      "epoch": 15.56865464632455,
      "grad_norm": 2.59013032913208,
      "learning_rate": 3.702612112806288e-05,
      "loss": 0.6874,
      "step": 1706200
    },
    {
      "epoch": 15.569567121687715,
      "grad_norm": 3.7289485931396484,
      "learning_rate": 3.7025360731926906e-05,
      "loss": 0.6481,
      "step": 1706300
    },
    {
      "epoch": 15.57047959705088,
      "grad_norm": 3.193359375,
      "learning_rate": 3.7024600335790936e-05,
      "loss": 0.6993,
      "step": 1706400
    },
    {
      "epoch": 15.571392072414044,
      "grad_norm": 3.647554874420166,
      "learning_rate": 3.7023839939654966e-05,
      "loss": 0.6598,
      "step": 1706500
    },
    {
      "epoch": 15.57230454777721,
      "grad_norm": 3.230708599090576,
      "learning_rate": 3.7023079543518996e-05,
      "loss": 0.7128,
      "step": 1706600
    },
    {
      "epoch": 15.573217023140375,
      "grad_norm": 3.8240723609924316,
      "learning_rate": 3.702231914738302e-05,
      "loss": 0.6745,
      "step": 1706700
    },
    {
      "epoch": 15.57412949850354,
      "grad_norm": 3.344407796859741,
      "learning_rate": 3.7021558751247056e-05,
      "loss": 0.6768,
      "step": 1706800
    },
    {
      "epoch": 15.575041973866705,
      "grad_norm": 3.651353120803833,
      "learning_rate": 3.702079835511108e-05,
      "loss": 0.6879,
      "step": 1706900
    },
    {
      "epoch": 15.57595444922987,
      "grad_norm": 4.821902751922607,
      "learning_rate": 3.702003795897511e-05,
      "loss": 0.682,
      "step": 1707000
    },
    {
      "epoch": 15.576866924593036,
      "grad_norm": 3.906101703643799,
      "learning_rate": 3.701927756283914e-05,
      "loss": 0.6762,
      "step": 1707100
    },
    {
      "epoch": 15.577779399956201,
      "grad_norm": 4.050625324249268,
      "learning_rate": 3.701851716670316e-05,
      "loss": 0.683,
      "step": 1707200
    },
    {
      "epoch": 15.578691875319366,
      "grad_norm": 6.390202522277832,
      "learning_rate": 3.70177567705672e-05,
      "loss": 0.6467,
      "step": 1707300
    },
    {
      "epoch": 15.579604350682532,
      "grad_norm": 3.491859197616577,
      "learning_rate": 3.701699637443122e-05,
      "loss": 0.6615,
      "step": 1707400
    },
    {
      "epoch": 15.580516826045697,
      "grad_norm": 3.915862798690796,
      "learning_rate": 3.701623597829525e-05,
      "loss": 0.6487,
      "step": 1707500
    },
    {
      "epoch": 15.581429301408862,
      "grad_norm": 3.690877676010132,
      "learning_rate": 3.701547558215928e-05,
      "loss": 0.6713,
      "step": 1707600
    },
    {
      "epoch": 15.582341776772028,
      "grad_norm": 3.663379430770874,
      "learning_rate": 3.7014715186023313e-05,
      "loss": 0.6804,
      "step": 1707700
    },
    {
      "epoch": 15.583254252135193,
      "grad_norm": 3.865579843521118,
      "learning_rate": 3.701395478988734e-05,
      "loss": 0.6199,
      "step": 1707800
    },
    {
      "epoch": 15.584166727498358,
      "grad_norm": 4.587036609649658,
      "learning_rate": 3.7013194393751374e-05,
      "loss": 0.6682,
      "step": 1707900
    },
    {
      "epoch": 15.585079202861523,
      "grad_norm": 4.547982692718506,
      "learning_rate": 3.70124339976154e-05,
      "loss": 0.6534,
      "step": 1708000
    },
    {
      "epoch": 15.585991678224687,
      "grad_norm": 3.6337153911590576,
      "learning_rate": 3.701167360147943e-05,
      "loss": 0.666,
      "step": 1708100
    },
    {
      "epoch": 15.586904153587852,
      "grad_norm": 4.255091667175293,
      "learning_rate": 3.701091320534346e-05,
      "loss": 0.7071,
      "step": 1708200
    },
    {
      "epoch": 15.587816628951018,
      "grad_norm": 4.353413105010986,
      "learning_rate": 3.701015280920749e-05,
      "loss": 0.6841,
      "step": 1708300
    },
    {
      "epoch": 15.588729104314183,
      "grad_norm": 4.170732498168945,
      "learning_rate": 3.700939241307152e-05,
      "loss": 0.6471,
      "step": 1708400
    },
    {
      "epoch": 15.589641579677348,
      "grad_norm": 3.4347004890441895,
      "learning_rate": 3.700863201693555e-05,
      "loss": 0.66,
      "step": 1708500
    },
    {
      "epoch": 15.590554055040514,
      "grad_norm": 4.721887588500977,
      "learning_rate": 3.700787162079957e-05,
      "loss": 0.6593,
      "step": 1708600
    },
    {
      "epoch": 15.591466530403679,
      "grad_norm": 3.5915544033050537,
      "learning_rate": 3.700711122466361e-05,
      "loss": 0.6534,
      "step": 1708700
    },
    {
      "epoch": 15.592379005766844,
      "grad_norm": 4.209943771362305,
      "learning_rate": 3.700635082852763e-05,
      "loss": 0.6613,
      "step": 1708800
    },
    {
      "epoch": 15.59329148113001,
      "grad_norm": 4.3935699462890625,
      "learning_rate": 3.700559043239166e-05,
      "loss": 0.6488,
      "step": 1708900
    },
    {
      "epoch": 15.594203956493175,
      "grad_norm": 4.398563385009766,
      "learning_rate": 3.700483003625569e-05,
      "loss": 0.649,
      "step": 1709000
    },
    {
      "epoch": 15.59511643185634,
      "grad_norm": 2.0656325817108154,
      "learning_rate": 3.700406964011972e-05,
      "loss": 0.6326,
      "step": 1709100
    },
    {
      "epoch": 15.596028907219505,
      "grad_norm": 4.069526195526123,
      "learning_rate": 3.7003309243983744e-05,
      "loss": 0.6797,
      "step": 1709200
    },
    {
      "epoch": 15.59694138258267,
      "grad_norm": 4.1456122398376465,
      "learning_rate": 3.700254884784778e-05,
      "loss": 0.6314,
      "step": 1709300
    },
    {
      "epoch": 15.597853857945836,
      "grad_norm": 4.473871231079102,
      "learning_rate": 3.7001788451711804e-05,
      "loss": 0.6875,
      "step": 1709400
    },
    {
      "epoch": 15.598766333309001,
      "grad_norm": 5.291499614715576,
      "learning_rate": 3.7001028055575834e-05,
      "loss": 0.6722,
      "step": 1709500
    },
    {
      "epoch": 15.599678808672166,
      "grad_norm": 4.584805011749268,
      "learning_rate": 3.7000267659439864e-05,
      "loss": 0.6213,
      "step": 1709600
    },
    {
      "epoch": 15.600591284035332,
      "grad_norm": 4.603287696838379,
      "learning_rate": 3.6999507263303894e-05,
      "loss": 0.6746,
      "step": 1709700
    },
    {
      "epoch": 15.601503759398497,
      "grad_norm": 4.757828235626221,
      "learning_rate": 3.6998746867167924e-05,
      "loss": 0.6433,
      "step": 1709800
    },
    {
      "epoch": 15.60241623476166,
      "grad_norm": 3.3097238540649414,
      "learning_rate": 3.699798647103195e-05,
      "loss": 0.6585,
      "step": 1709900
    },
    {
      "epoch": 15.603328710124826,
      "grad_norm": 3.4700725078582764,
      "learning_rate": 3.699722607489598e-05,
      "loss": 0.7028,
      "step": 1710000
    },
    {
      "epoch": 15.604241185487991,
      "grad_norm": 3.553947925567627,
      "learning_rate": 3.699646567876001e-05,
      "loss": 0.6513,
      "step": 1710100
    },
    {
      "epoch": 15.605153660851157,
      "grad_norm": 4.128903865814209,
      "learning_rate": 3.699570528262404e-05,
      "loss": 0.6778,
      "step": 1710200
    },
    {
      "epoch": 15.606066136214322,
      "grad_norm": 3.829036235809326,
      "learning_rate": 3.699494488648806e-05,
      "loss": 0.6505,
      "step": 1710300
    },
    {
      "epoch": 15.606978611577487,
      "grad_norm": 3.7104411125183105,
      "learning_rate": 3.69941844903521e-05,
      "loss": 0.679,
      "step": 1710400
    },
    {
      "epoch": 15.607891086940652,
      "grad_norm": 5.422079563140869,
      "learning_rate": 3.699342409421612e-05,
      "loss": 0.6798,
      "step": 1710500
    },
    {
      "epoch": 15.608803562303818,
      "grad_norm": 3.716463565826416,
      "learning_rate": 3.699266369808015e-05,
      "loss": 0.6666,
      "step": 1710600
    },
    {
      "epoch": 15.609716037666983,
      "grad_norm": 4.47542667388916,
      "learning_rate": 3.699190330194418e-05,
      "loss": 0.6596,
      "step": 1710700
    },
    {
      "epoch": 15.610628513030148,
      "grad_norm": 3.749835968017578,
      "learning_rate": 3.699114290580821e-05,
      "loss": 0.6478,
      "step": 1710800
    },
    {
      "epoch": 15.611540988393314,
      "grad_norm": 4.636909484863281,
      "learning_rate": 3.699038250967224e-05,
      "loss": 0.654,
      "step": 1710900
    },
    {
      "epoch": 15.612453463756479,
      "grad_norm": 4.130585670471191,
      "learning_rate": 3.698962211353627e-05,
      "loss": 0.6457,
      "step": 1711000
    },
    {
      "epoch": 15.613365939119644,
      "grad_norm": 3.6085309982299805,
      "learning_rate": 3.6988861717400295e-05,
      "loss": 0.6567,
      "step": 1711100
    },
    {
      "epoch": 15.61427841448281,
      "grad_norm": 4.151758193969727,
      "learning_rate": 3.698810132126433e-05,
      "loss": 0.6536,
      "step": 1711200
    },
    {
      "epoch": 15.615190889845975,
      "grad_norm": 3.2102911472320557,
      "learning_rate": 3.6987340925128355e-05,
      "loss": 0.6828,
      "step": 1711300
    },
    {
      "epoch": 15.61610336520914,
      "grad_norm": 2.4982845783233643,
      "learning_rate": 3.6986580528992385e-05,
      "loss": 0.7102,
      "step": 1711400
    },
    {
      "epoch": 15.617015840572304,
      "grad_norm": 3.04895281791687,
      "learning_rate": 3.6985820132856415e-05,
      "loss": 0.6407,
      "step": 1711500
    },
    {
      "epoch": 15.617928315935469,
      "grad_norm": 3.881287097930908,
      "learning_rate": 3.6985059736720445e-05,
      "loss": 0.6451,
      "step": 1711600
    },
    {
      "epoch": 15.618840791298634,
      "grad_norm": 3.914365291595459,
      "learning_rate": 3.698429934058447e-05,
      "loss": 0.6867,
      "step": 1711700
    },
    {
      "epoch": 15.6197532666618,
      "grad_norm": 3.3342554569244385,
      "learning_rate": 3.6983538944448506e-05,
      "loss": 0.6305,
      "step": 1711800
    },
    {
      "epoch": 15.620665742024965,
      "grad_norm": 5.031467914581299,
      "learning_rate": 3.698277854831253e-05,
      "loss": 0.7053,
      "step": 1711900
    },
    {
      "epoch": 15.62157821738813,
      "grad_norm": 3.0587539672851562,
      "learning_rate": 3.698201815217656e-05,
      "loss": 0.6651,
      "step": 1712000
    },
    {
      "epoch": 15.622490692751295,
      "grad_norm": 4.296590805053711,
      "learning_rate": 3.698125775604059e-05,
      "loss": 0.6526,
      "step": 1712100
    },
    {
      "epoch": 15.62340316811446,
      "grad_norm": 4.336441516876221,
      "learning_rate": 3.698049735990462e-05,
      "loss": 0.6836,
      "step": 1712200
    },
    {
      "epoch": 15.624315643477626,
      "grad_norm": 4.493702411651611,
      "learning_rate": 3.697973696376865e-05,
      "loss": 0.6705,
      "step": 1712300
    },
    {
      "epoch": 15.625228118840791,
      "grad_norm": 4.342630863189697,
      "learning_rate": 3.697897656763268e-05,
      "loss": 0.6416,
      "step": 1712400
    },
    {
      "epoch": 15.626140594203957,
      "grad_norm": 3.085294246673584,
      "learning_rate": 3.69782161714967e-05,
      "loss": 0.6685,
      "step": 1712500
    },
    {
      "epoch": 15.627053069567122,
      "grad_norm": 3.385812520980835,
      "learning_rate": 3.697745577536074e-05,
      "loss": 0.6822,
      "step": 1712600
    },
    {
      "epoch": 15.627965544930287,
      "grad_norm": 4.286269187927246,
      "learning_rate": 3.697669537922476e-05,
      "loss": 0.6581,
      "step": 1712700
    },
    {
      "epoch": 15.628878020293453,
      "grad_norm": 3.40978741645813,
      "learning_rate": 3.6975934983088786e-05,
      "loss": 0.6343,
      "step": 1712800
    },
    {
      "epoch": 15.629790495656618,
      "grad_norm": 4.199979782104492,
      "learning_rate": 3.697517458695282e-05,
      "loss": 0.691,
      "step": 1712900
    },
    {
      "epoch": 15.630702971019783,
      "grad_norm": 3.763850688934326,
      "learning_rate": 3.6974414190816846e-05,
      "loss": 0.674,
      "step": 1713000
    },
    {
      "epoch": 15.631615446382948,
      "grad_norm": 3.957221031188965,
      "learning_rate": 3.6973653794680876e-05,
      "loss": 0.6475,
      "step": 1713100
    },
    {
      "epoch": 15.632527921746114,
      "grad_norm": 3.896350383758545,
      "learning_rate": 3.6972893398544906e-05,
      "loss": 0.6751,
      "step": 1713200
    },
    {
      "epoch": 15.633440397109277,
      "grad_norm": 4.031527042388916,
      "learning_rate": 3.6972133002408936e-05,
      "loss": 0.6712,
      "step": 1713300
    },
    {
      "epoch": 15.634352872472443,
      "grad_norm": 3.8142120838165283,
      "learning_rate": 3.6971372606272966e-05,
      "loss": 0.6831,
      "step": 1713400
    },
    {
      "epoch": 15.635265347835608,
      "grad_norm": 4.315945625305176,
      "learning_rate": 3.6970612210136996e-05,
      "loss": 0.631,
      "step": 1713500
    },
    {
      "epoch": 15.636177823198773,
      "grad_norm": 4.315678119659424,
      "learning_rate": 3.696985181400102e-05,
      "loss": 0.6731,
      "step": 1713600
    },
    {
      "epoch": 15.637090298561938,
      "grad_norm": 4.158395767211914,
      "learning_rate": 3.6969091417865056e-05,
      "loss": 0.6729,
      "step": 1713700
    },
    {
      "epoch": 15.638002773925104,
      "grad_norm": 3.910374402999878,
      "learning_rate": 3.696833102172908e-05,
      "loss": 0.6393,
      "step": 1713800
    },
    {
      "epoch": 15.638915249288269,
      "grad_norm": 5.358328342437744,
      "learning_rate": 3.696757062559311e-05,
      "loss": 0.6565,
      "step": 1713900
    },
    {
      "epoch": 15.639827724651434,
      "grad_norm": 3.9663987159729004,
      "learning_rate": 3.696681022945714e-05,
      "loss": 0.6782,
      "step": 1714000
    },
    {
      "epoch": 15.6407402000146,
      "grad_norm": 4.535085201263428,
      "learning_rate": 3.696604983332117e-05,
      "loss": 0.6695,
      "step": 1714100
    },
    {
      "epoch": 15.641652675377765,
      "grad_norm": 4.520230293273926,
      "learning_rate": 3.696528943718519e-05,
      "loss": 0.6561,
      "step": 1714200
    },
    {
      "epoch": 15.64256515074093,
      "grad_norm": 3.8260931968688965,
      "learning_rate": 3.696452904104923e-05,
      "loss": 0.6839,
      "step": 1714300
    },
    {
      "epoch": 15.643477626104096,
      "grad_norm": 4.542541980743408,
      "learning_rate": 3.696376864491325e-05,
      "loss": 0.6821,
      "step": 1714400
    },
    {
      "epoch": 15.64439010146726,
      "grad_norm": 4.528707027435303,
      "learning_rate": 3.6963008248777283e-05,
      "loss": 0.6772,
      "step": 1714500
    },
    {
      "epoch": 15.645302576830426,
      "grad_norm": 3.73738431930542,
      "learning_rate": 3.6962247852641314e-05,
      "loss": 0.6617,
      "step": 1714600
    },
    {
      "epoch": 15.646215052193591,
      "grad_norm": 3.269594192504883,
      "learning_rate": 3.6961487456505344e-05,
      "loss": 0.6489,
      "step": 1714700
    },
    {
      "epoch": 15.647127527556757,
      "grad_norm": 3.788468599319458,
      "learning_rate": 3.6960727060369374e-05,
      "loss": 0.6906,
      "step": 1714800
    },
    {
      "epoch": 15.64804000291992,
      "grad_norm": 3.840754747390747,
      "learning_rate": 3.6959966664233404e-05,
      "loss": 0.6538,
      "step": 1714900
    },
    {
      "epoch": 15.648952478283086,
      "grad_norm": 4.162376880645752,
      "learning_rate": 3.695920626809743e-05,
      "loss": 0.6464,
      "step": 1715000
    },
    {
      "epoch": 15.64986495364625,
      "grad_norm": 4.439846992492676,
      "learning_rate": 3.6958445871961464e-05,
      "loss": 0.6809,
      "step": 1715100
    },
    {
      "epoch": 15.650777429009416,
      "grad_norm": 4.163887977600098,
      "learning_rate": 3.695768547582549e-05,
      "loss": 0.6484,
      "step": 1715200
    },
    {
      "epoch": 15.651689904372581,
      "grad_norm": 3.857731819152832,
      "learning_rate": 3.695692507968952e-05,
      "loss": 0.6611,
      "step": 1715300
    },
    {
      "epoch": 15.652602379735747,
      "grad_norm": 4.118042945861816,
      "learning_rate": 3.695616468355355e-05,
      "loss": 0.6928,
      "step": 1715400
    },
    {
      "epoch": 15.653514855098912,
      "grad_norm": 3.8723502159118652,
      "learning_rate": 3.695540428741757e-05,
      "loss": 0.6993,
      "step": 1715500
    },
    {
      "epoch": 15.654427330462077,
      "grad_norm": 3.4306893348693848,
      "learning_rate": 3.69546438912816e-05,
      "loss": 0.7,
      "step": 1715600
    },
    {
      "epoch": 15.655339805825243,
      "grad_norm": 3.816761016845703,
      "learning_rate": 3.695388349514563e-05,
      "loss": 0.694,
      "step": 1715700
    },
    {
      "epoch": 15.656252281188408,
      "grad_norm": 4.097180366516113,
      "learning_rate": 3.695312309900966e-05,
      "loss": 0.6669,
      "step": 1715800
    },
    {
      "epoch": 15.657164756551573,
      "grad_norm": 3.9607388973236084,
      "learning_rate": 3.695236270287369e-05,
      "loss": 0.674,
      "step": 1715900
    },
    {
      "epoch": 15.658077231914739,
      "grad_norm": 4.3546624183654785,
      "learning_rate": 3.695160230673772e-05,
      "loss": 0.6847,
      "step": 1716000
    },
    {
      "epoch": 15.658989707277904,
      "grad_norm": 4.929322242736816,
      "learning_rate": 3.6950841910601744e-05,
      "loss": 0.6683,
      "step": 1716100
    },
    {
      "epoch": 15.65990218264107,
      "grad_norm": 3.9546847343444824,
      "learning_rate": 3.695008151446578e-05,
      "loss": 0.64,
      "step": 1716200
    },
    {
      "epoch": 15.660814658004234,
      "grad_norm": 2.472060203552246,
      "learning_rate": 3.6949321118329804e-05,
      "loss": 0.6818,
      "step": 1716300
    },
    {
      "epoch": 15.6617271333674,
      "grad_norm": 3.3278255462646484,
      "learning_rate": 3.6948560722193834e-05,
      "loss": 0.6622,
      "step": 1716400
    },
    {
      "epoch": 15.662639608730565,
      "grad_norm": 4.201410293579102,
      "learning_rate": 3.6947800326057864e-05,
      "loss": 0.6709,
      "step": 1716500
    },
    {
      "epoch": 15.66355208409373,
      "grad_norm": 2.9061691761016846,
      "learning_rate": 3.6947039929921895e-05,
      "loss": 0.676,
      "step": 1716600
    },
    {
      "epoch": 15.664464559456894,
      "grad_norm": 4.0632781982421875,
      "learning_rate": 3.6946279533785925e-05,
      "loss": 0.6532,
      "step": 1716700
    },
    {
      "epoch": 15.66537703482006,
      "grad_norm": 4.37137508392334,
      "learning_rate": 3.6945519137649955e-05,
      "loss": 0.6977,
      "step": 1716800
    },
    {
      "epoch": 15.666289510183224,
      "grad_norm": 3.4841558933258057,
      "learning_rate": 3.694475874151398e-05,
      "loss": 0.6856,
      "step": 1716900
    },
    {
      "epoch": 15.66720198554639,
      "grad_norm": 3.8101680278778076,
      "learning_rate": 3.6943998345378015e-05,
      "loss": 0.6265,
      "step": 1717000
    },
    {
      "epoch": 15.668114460909555,
      "grad_norm": 4.729506492614746,
      "learning_rate": 3.694323794924204e-05,
      "loss": 0.6446,
      "step": 1717100
    },
    {
      "epoch": 15.66902693627272,
      "grad_norm": 3.7602128982543945,
      "learning_rate": 3.694247755310607e-05,
      "loss": 0.6871,
      "step": 1717200
    },
    {
      "epoch": 15.669939411635886,
      "grad_norm": 3.7651939392089844,
      "learning_rate": 3.69417171569701e-05,
      "loss": 0.6963,
      "step": 1717300
    },
    {
      "epoch": 15.670851886999051,
      "grad_norm": 3.744245767593384,
      "learning_rate": 3.694095676083413e-05,
      "loss": 0.6727,
      "step": 1717400
    },
    {
      "epoch": 15.671764362362216,
      "grad_norm": 3.4470486640930176,
      "learning_rate": 3.694019636469815e-05,
      "loss": 0.6653,
      "step": 1717500
    },
    {
      "epoch": 15.672676837725382,
      "grad_norm": 3.9987287521362305,
      "learning_rate": 3.693943596856219e-05,
      "loss": 0.6602,
      "step": 1717600
    },
    {
      "epoch": 15.673589313088547,
      "grad_norm": 4.611398220062256,
      "learning_rate": 3.693867557242621e-05,
      "loss": 0.6618,
      "step": 1717700
    },
    {
      "epoch": 15.674501788451712,
      "grad_norm": 4.055607795715332,
      "learning_rate": 3.693791517629024e-05,
      "loss": 0.6624,
      "step": 1717800
    },
    {
      "epoch": 15.675414263814877,
      "grad_norm": 3.436145305633545,
      "learning_rate": 3.693715478015427e-05,
      "loss": 0.6672,
      "step": 1717900
    },
    {
      "epoch": 15.676326739178043,
      "grad_norm": 3.8137378692626953,
      "learning_rate": 3.69363943840183e-05,
      "loss": 0.6914,
      "step": 1718000
    },
    {
      "epoch": 15.677239214541208,
      "grad_norm": 4.039190292358398,
      "learning_rate": 3.693563398788233e-05,
      "loss": 0.6514,
      "step": 1718100
    },
    {
      "epoch": 15.678151689904373,
      "grad_norm": 3.5980989933013916,
      "learning_rate": 3.693487359174636e-05,
      "loss": 0.6507,
      "step": 1718200
    },
    {
      "epoch": 15.679064165267537,
      "grad_norm": 3.1622495651245117,
      "learning_rate": 3.6934113195610385e-05,
      "loss": 0.6607,
      "step": 1718300
    },
    {
      "epoch": 15.679976640630702,
      "grad_norm": 4.275416374206543,
      "learning_rate": 3.6933352799474415e-05,
      "loss": 0.6643,
      "step": 1718400
    },
    {
      "epoch": 15.680889115993867,
      "grad_norm": 4.113968849182129,
      "learning_rate": 3.6932592403338445e-05,
      "loss": 0.6326,
      "step": 1718500
    },
    {
      "epoch": 15.681801591357033,
      "grad_norm": 3.916224718093872,
      "learning_rate": 3.693183200720247e-05,
      "loss": 0.6777,
      "step": 1718600
    },
    {
      "epoch": 15.682714066720198,
      "grad_norm": 4.194757461547852,
      "learning_rate": 3.6931071611066506e-05,
      "loss": 0.6862,
      "step": 1718700
    },
    {
      "epoch": 15.683626542083363,
      "grad_norm": 4.110833644866943,
      "learning_rate": 3.693031121493053e-05,
      "loss": 0.6926,
      "step": 1718800
    },
    {
      "epoch": 15.684539017446529,
      "grad_norm": 3.1977298259735107,
      "learning_rate": 3.692955081879456e-05,
      "loss": 0.68,
      "step": 1718900
    },
    {
      "epoch": 15.685451492809694,
      "grad_norm": 2.6823837757110596,
      "learning_rate": 3.692879042265859e-05,
      "loss": 0.6609,
      "step": 1719000
    },
    {
      "epoch": 15.68636396817286,
      "grad_norm": 3.9752817153930664,
      "learning_rate": 3.692803002652262e-05,
      "loss": 0.6824,
      "step": 1719100
    },
    {
      "epoch": 15.687276443536025,
      "grad_norm": 4.470483303070068,
      "learning_rate": 3.692726963038665e-05,
      "loss": 0.6822,
      "step": 1719200
    },
    {
      "epoch": 15.68818891889919,
      "grad_norm": 4.191380023956299,
      "learning_rate": 3.692650923425068e-05,
      "loss": 0.6666,
      "step": 1719300
    },
    {
      "epoch": 15.689101394262355,
      "grad_norm": 3.194545269012451,
      "learning_rate": 3.69257488381147e-05,
      "loss": 0.642,
      "step": 1719400
    },
    {
      "epoch": 15.69001386962552,
      "grad_norm": 3.6396806240081787,
      "learning_rate": 3.692498844197874e-05,
      "loss": 0.6582,
      "step": 1719500
    },
    {
      "epoch": 15.690926344988686,
      "grad_norm": 5.378675937652588,
      "learning_rate": 3.692422804584276e-05,
      "loss": 0.674,
      "step": 1719600
    },
    {
      "epoch": 15.691838820351851,
      "grad_norm": 4.400886535644531,
      "learning_rate": 3.692346764970679e-05,
      "loss": 0.6892,
      "step": 1719700
    },
    {
      "epoch": 15.692751295715016,
      "grad_norm": 3.368795156478882,
      "learning_rate": 3.692270725357082e-05,
      "loss": 0.6582,
      "step": 1719800
    },
    {
      "epoch": 15.693663771078182,
      "grad_norm": 4.159119129180908,
      "learning_rate": 3.692194685743485e-05,
      "loss": 0.6497,
      "step": 1719900
    },
    {
      "epoch": 15.694576246441347,
      "grad_norm": 3.199901580810547,
      "learning_rate": 3.6921186461298876e-05,
      "loss": 0.6393,
      "step": 1720000
    },
    {
      "epoch": 15.69548872180451,
      "grad_norm": 4.196955680847168,
      "learning_rate": 3.692042606516291e-05,
      "loss": 0.6942,
      "step": 1720100
    },
    {
      "epoch": 15.696401197167676,
      "grad_norm": 4.070232391357422,
      "learning_rate": 3.6919665669026936e-05,
      "loss": 0.7014,
      "step": 1720200
    },
    {
      "epoch": 15.697313672530841,
      "grad_norm": 4.251147270202637,
      "learning_rate": 3.6918905272890966e-05,
      "loss": 0.6832,
      "step": 1720300
    },
    {
      "epoch": 15.698226147894006,
      "grad_norm": 4.471308708190918,
      "learning_rate": 3.6918144876754996e-05,
      "loss": 0.6436,
      "step": 1720400
    },
    {
      "epoch": 15.699138623257172,
      "grad_norm": 2.952117681503296,
      "learning_rate": 3.6917384480619026e-05,
      "loss": 0.6426,
      "step": 1720500
    },
    {
      "epoch": 15.700051098620337,
      "grad_norm": 4.028647422790527,
      "learning_rate": 3.6916624084483057e-05,
      "loss": 0.6622,
      "step": 1720600
    },
    {
      "epoch": 15.700963573983502,
      "grad_norm": 3.649378538131714,
      "learning_rate": 3.691586368834709e-05,
      "loss": 0.6761,
      "step": 1720700
    },
    {
      "epoch": 15.701876049346668,
      "grad_norm": 3.5454893112182617,
      "learning_rate": 3.691510329221111e-05,
      "loss": 0.6351,
      "step": 1720800
    },
    {
      "epoch": 15.702788524709833,
      "grad_norm": 3.856067419052124,
      "learning_rate": 3.691434289607515e-05,
      "loss": 0.678,
      "step": 1720900
    },
    {
      "epoch": 15.703701000072998,
      "grad_norm": 3.803636312484741,
      "learning_rate": 3.691358249993917e-05,
      "loss": 0.6662,
      "step": 1721000
    },
    {
      "epoch": 15.704613475436163,
      "grad_norm": 3.3580079078674316,
      "learning_rate": 3.69128221038032e-05,
      "loss": 0.6697,
      "step": 1721100
    },
    {
      "epoch": 15.705525950799329,
      "grad_norm": 2.5482475757598877,
      "learning_rate": 3.691206170766723e-05,
      "loss": 0.6855,
      "step": 1721200
    },
    {
      "epoch": 15.706438426162494,
      "grad_norm": 3.9947433471679688,
      "learning_rate": 3.6911301311531253e-05,
      "loss": 0.6465,
      "step": 1721300
    },
    {
      "epoch": 15.70735090152566,
      "grad_norm": 4.256042957305908,
      "learning_rate": 3.6910540915395284e-05,
      "loss": 0.7005,
      "step": 1721400
    },
    {
      "epoch": 15.708263376888825,
      "grad_norm": 2.7855522632598877,
      "learning_rate": 3.6909780519259314e-05,
      "loss": 0.6936,
      "step": 1721500
    },
    {
      "epoch": 15.70917585225199,
      "grad_norm": 3.9610166549682617,
      "learning_rate": 3.6909020123123344e-05,
      "loss": 0.6852,
      "step": 1721600
    },
    {
      "epoch": 15.710088327615154,
      "grad_norm": 5.394587993621826,
      "learning_rate": 3.6908259726987374e-05,
      "loss": 0.6701,
      "step": 1721700
    },
    {
      "epoch": 15.711000802978319,
      "grad_norm": 3.4917619228363037,
      "learning_rate": 3.6907499330851404e-05,
      "loss": 0.6884,
      "step": 1721800
    },
    {
      "epoch": 15.711913278341484,
      "grad_norm": 3.897010326385498,
      "learning_rate": 3.690673893471543e-05,
      "loss": 0.655,
      "step": 1721900
    },
    {
      "epoch": 15.71282575370465,
      "grad_norm": 3.382131338119507,
      "learning_rate": 3.6905978538579464e-05,
      "loss": 0.6449,
      "step": 1722000
    },
    {
      "epoch": 15.713738229067815,
      "grad_norm": 3.7716856002807617,
      "learning_rate": 3.690521814244349e-05,
      "loss": 0.6518,
      "step": 1722100
    },
    {
      "epoch": 15.71465070443098,
      "grad_norm": 4.23713493347168,
      "learning_rate": 3.690445774630752e-05,
      "loss": 0.6942,
      "step": 1722200
    },
    {
      "epoch": 15.715563179794145,
      "grad_norm": 4.240776062011719,
      "learning_rate": 3.690369735017155e-05,
      "loss": 0.6979,
      "step": 1722300
    },
    {
      "epoch": 15.71647565515731,
      "grad_norm": 4.89277982711792,
      "learning_rate": 3.690293695403558e-05,
      "loss": 0.695,
      "step": 1722400
    },
    {
      "epoch": 15.717388130520476,
      "grad_norm": 3.8696463108062744,
      "learning_rate": 3.69021765578996e-05,
      "loss": 0.6526,
      "step": 1722500
    },
    {
      "epoch": 15.718300605883641,
      "grad_norm": 4.139179706573486,
      "learning_rate": 3.690141616176364e-05,
      "loss": 0.6352,
      "step": 1722600
    },
    {
      "epoch": 15.719213081246806,
      "grad_norm": 5.010932922363281,
      "learning_rate": 3.690065576562766e-05,
      "loss": 0.6563,
      "step": 1722700
    },
    {
      "epoch": 15.720125556609972,
      "grad_norm": 4.794620513916016,
      "learning_rate": 3.689989536949169e-05,
      "loss": 0.6519,
      "step": 1722800
    },
    {
      "epoch": 15.721038031973137,
      "grad_norm": 3.8947389125823975,
      "learning_rate": 3.689913497335572e-05,
      "loss": 0.6905,
      "step": 1722900
    },
    {
      "epoch": 15.721950507336302,
      "grad_norm": 4.615567207336426,
      "learning_rate": 3.689837457721975e-05,
      "loss": 0.6548,
      "step": 1723000
    },
    {
      "epoch": 15.722862982699468,
      "grad_norm": 5.204794883728027,
      "learning_rate": 3.689761418108378e-05,
      "loss": 0.7154,
      "step": 1723100
    },
    {
      "epoch": 15.723775458062633,
      "grad_norm": 5.604641437530518,
      "learning_rate": 3.689685378494781e-05,
      "loss": 0.6783,
      "step": 1723200
    },
    {
      "epoch": 15.724687933425798,
      "grad_norm": 3.7125167846679688,
      "learning_rate": 3.6896093388811834e-05,
      "loss": 0.6535,
      "step": 1723300
    },
    {
      "epoch": 15.725600408788964,
      "grad_norm": 4.182399272918701,
      "learning_rate": 3.689533299267587e-05,
      "loss": 0.6455,
      "step": 1723400
    },
    {
      "epoch": 15.726512884152127,
      "grad_norm": 4.441341400146484,
      "learning_rate": 3.6894572596539895e-05,
      "loss": 0.6525,
      "step": 1723500
    },
    {
      "epoch": 15.727425359515292,
      "grad_norm": 4.293452739715576,
      "learning_rate": 3.6893812200403925e-05,
      "loss": 0.6437,
      "step": 1723600
    },
    {
      "epoch": 15.728337834878458,
      "grad_norm": 4.363161563873291,
      "learning_rate": 3.6893051804267955e-05,
      "loss": 0.6845,
      "step": 1723700
    },
    {
      "epoch": 15.729250310241623,
      "grad_norm": 4.837723255157471,
      "learning_rate": 3.6892291408131985e-05,
      "loss": 0.6806,
      "step": 1723800
    },
    {
      "epoch": 15.730162785604788,
      "grad_norm": 3.6759636402130127,
      "learning_rate": 3.689153101199601e-05,
      "loss": 0.6715,
      "step": 1723900
    },
    {
      "epoch": 15.731075260967954,
      "grad_norm": 3.421757698059082,
      "learning_rate": 3.689077061586004e-05,
      "loss": 0.6673,
      "step": 1724000
    },
    {
      "epoch": 15.731987736331119,
      "grad_norm": 3.9638900756835938,
      "learning_rate": 3.689001021972407e-05,
      "loss": 0.6592,
      "step": 1724100
    },
    {
      "epoch": 15.732900211694284,
      "grad_norm": 3.8893392086029053,
      "learning_rate": 3.68892498235881e-05,
      "loss": 0.6369,
      "step": 1724200
    },
    {
      "epoch": 15.73381268705745,
      "grad_norm": 3.9888553619384766,
      "learning_rate": 3.688848942745213e-05,
      "loss": 0.659,
      "step": 1724300
    },
    {
      "epoch": 15.734725162420615,
      "grad_norm": 3.7953715324401855,
      "learning_rate": 3.688772903131615e-05,
      "loss": 0.6839,
      "step": 1724400
    },
    {
      "epoch": 15.73563763778378,
      "grad_norm": 4.007664680480957,
      "learning_rate": 3.688696863518019e-05,
      "loss": 0.6849,
      "step": 1724500
    },
    {
      "epoch": 15.736550113146945,
      "grad_norm": 3.786048412322998,
      "learning_rate": 3.688620823904421e-05,
      "loss": 0.6516,
      "step": 1724600
    },
    {
      "epoch": 15.73746258851011,
      "grad_norm": 4.2413177490234375,
      "learning_rate": 3.688544784290824e-05,
      "loss": 0.6664,
      "step": 1724700
    },
    {
      "epoch": 15.738375063873276,
      "grad_norm": 3.713477373123169,
      "learning_rate": 3.688468744677227e-05,
      "loss": 0.6371,
      "step": 1724800
    },
    {
      "epoch": 15.739287539236441,
      "grad_norm": 3.851435661315918,
      "learning_rate": 3.68839270506363e-05,
      "loss": 0.6766,
      "step": 1724900
    },
    {
      "epoch": 15.740200014599607,
      "grad_norm": 5.301479339599609,
      "learning_rate": 3.6883166654500325e-05,
      "loss": 0.6486,
      "step": 1725000
    },
    {
      "epoch": 15.74111248996277,
      "grad_norm": 3.8017258644104004,
      "learning_rate": 3.688240625836436e-05,
      "loss": 0.6965,
      "step": 1725100
    },
    {
      "epoch": 15.742024965325935,
      "grad_norm": 4.142011642456055,
      "learning_rate": 3.6881645862228385e-05,
      "loss": 0.6819,
      "step": 1725200
    },
    {
      "epoch": 15.7429374406891,
      "grad_norm": 2.304393768310547,
      "learning_rate": 3.6880885466092416e-05,
      "loss": 0.6323,
      "step": 1725300
    },
    {
      "epoch": 15.743849916052266,
      "grad_norm": 3.718733549118042,
      "learning_rate": 3.6880125069956446e-05,
      "loss": 0.6845,
      "step": 1725400
    },
    {
      "epoch": 15.744762391415431,
      "grad_norm": 4.054228782653809,
      "learning_rate": 3.6879364673820476e-05,
      "loss": 0.6511,
      "step": 1725500
    },
    {
      "epoch": 15.745674866778597,
      "grad_norm": 3.857457399368286,
      "learning_rate": 3.6878604277684506e-05,
      "loss": 0.6743,
      "step": 1725600
    },
    {
      "epoch": 15.746587342141762,
      "grad_norm": 4.150065898895264,
      "learning_rate": 3.6877843881548536e-05,
      "loss": 0.6262,
      "step": 1725700
    },
    {
      "epoch": 15.747499817504927,
      "grad_norm": 4.090547561645508,
      "learning_rate": 3.687708348541256e-05,
      "loss": 0.6769,
      "step": 1725800
    },
    {
      "epoch": 15.748412292868093,
      "grad_norm": 4.117774963378906,
      "learning_rate": 3.6876323089276596e-05,
      "loss": 0.7067,
      "step": 1725900
    },
    {
      "epoch": 15.749324768231258,
      "grad_norm": 4.087928295135498,
      "learning_rate": 3.687556269314062e-05,
      "loss": 0.7126,
      "step": 1726000
    },
    {
      "epoch": 15.750237243594423,
      "grad_norm": 4.242974758148193,
      "learning_rate": 3.687480229700465e-05,
      "loss": 0.6685,
      "step": 1726100
    },
    {
      "epoch": 15.751149718957588,
      "grad_norm": 2.933628797531128,
      "learning_rate": 3.687404190086868e-05,
      "loss": 0.6855,
      "step": 1726200
    },
    {
      "epoch": 15.752062194320754,
      "grad_norm": 4.158094882965088,
      "learning_rate": 3.687328150473271e-05,
      "loss": 0.6745,
      "step": 1726300
    },
    {
      "epoch": 15.752974669683919,
      "grad_norm": 4.653981685638428,
      "learning_rate": 3.687252110859673e-05,
      "loss": 0.6483,
      "step": 1726400
    },
    {
      "epoch": 15.753887145047084,
      "grad_norm": 4.242000579833984,
      "learning_rate": 3.687176071246077e-05,
      "loss": 0.6894,
      "step": 1726500
    },
    {
      "epoch": 15.75479962041025,
      "grad_norm": 4.5106072425842285,
      "learning_rate": 3.687100031632479e-05,
      "loss": 0.7068,
      "step": 1726600
    },
    {
      "epoch": 15.755712095773415,
      "grad_norm": 3.5319626331329346,
      "learning_rate": 3.687023992018882e-05,
      "loss": 0.6358,
      "step": 1726700
    },
    {
      "epoch": 15.75662457113658,
      "grad_norm": 3.4915072917938232,
      "learning_rate": 3.686947952405285e-05,
      "loss": 0.6764,
      "step": 1726800
    },
    {
      "epoch": 15.757537046499744,
      "grad_norm": 3.247062921524048,
      "learning_rate": 3.6868719127916876e-05,
      "loss": 0.6929,
      "step": 1726900
    },
    {
      "epoch": 15.758449521862909,
      "grad_norm": 4.252177715301514,
      "learning_rate": 3.686795873178091e-05,
      "loss": 0.6783,
      "step": 1727000
    },
    {
      "epoch": 15.759361997226074,
      "grad_norm": 4.360902786254883,
      "learning_rate": 3.6867198335644936e-05,
      "loss": 0.6553,
      "step": 1727100
    },
    {
      "epoch": 15.76027447258924,
      "grad_norm": 3.873128890991211,
      "learning_rate": 3.6866437939508966e-05,
      "loss": 0.6446,
      "step": 1727200
    },
    {
      "epoch": 15.761186947952405,
      "grad_norm": 3.4394350051879883,
      "learning_rate": 3.6865677543372997e-05,
      "loss": 0.6832,
      "step": 1727300
    },
    {
      "epoch": 15.76209942331557,
      "grad_norm": 3.9126124382019043,
      "learning_rate": 3.6864917147237027e-05,
      "loss": 0.6428,
      "step": 1727400
    },
    {
      "epoch": 15.763011898678736,
      "grad_norm": 2.7618560791015625,
      "learning_rate": 3.686415675110106e-05,
      "loss": 0.696,
      "step": 1727500
    },
    {
      "epoch": 15.7639243740419,
      "grad_norm": 4.393126010894775,
      "learning_rate": 3.686339635496509e-05,
      "loss": 0.6771,
      "step": 1727600
    },
    {
      "epoch": 15.764836849405066,
      "grad_norm": 3.85068678855896,
      "learning_rate": 3.686263595882911e-05,
      "loss": 0.6497,
      "step": 1727700
    },
    {
      "epoch": 15.765749324768231,
      "grad_norm": 4.160356521606445,
      "learning_rate": 3.686187556269314e-05,
      "loss": 0.6601,
      "step": 1727800
    },
    {
      "epoch": 15.766661800131397,
      "grad_norm": 2.877000570297241,
      "learning_rate": 3.686111516655717e-05,
      "loss": 0.6845,
      "step": 1727900
    },
    {
      "epoch": 15.767574275494562,
      "grad_norm": 4.129363536834717,
      "learning_rate": 3.68603547704212e-05,
      "loss": 0.6566,
      "step": 1728000
    },
    {
      "epoch": 15.768486750857727,
      "grad_norm": 4.561949729919434,
      "learning_rate": 3.685959437428523e-05,
      "loss": 0.6783,
      "step": 1728100
    },
    {
      "epoch": 15.769399226220893,
      "grad_norm": 4.300740718841553,
      "learning_rate": 3.685883397814926e-05,
      "loss": 0.6448,
      "step": 1728200
    },
    {
      "epoch": 15.770311701584058,
      "grad_norm": 4.523426532745361,
      "learning_rate": 3.6858073582013284e-05,
      "loss": 0.6417,
      "step": 1728300
    },
    {
      "epoch": 15.771224176947223,
      "grad_norm": 4.2349534034729,
      "learning_rate": 3.685731318587732e-05,
      "loss": 0.6371,
      "step": 1728400
    },
    {
      "epoch": 15.772136652310387,
      "grad_norm": 3.8933231830596924,
      "learning_rate": 3.6856552789741344e-05,
      "loss": 0.671,
      "step": 1728500
    },
    {
      "epoch": 15.773049127673552,
      "grad_norm": 3.5752530097961426,
      "learning_rate": 3.6855792393605374e-05,
      "loss": 0.6362,
      "step": 1728600
    },
    {
      "epoch": 15.773961603036717,
      "grad_norm": 4.552439212799072,
      "learning_rate": 3.6855031997469404e-05,
      "loss": 0.662,
      "step": 1728700
    },
    {
      "epoch": 15.774874078399883,
      "grad_norm": 3.9581246376037598,
      "learning_rate": 3.6854271601333434e-05,
      "loss": 0.6786,
      "step": 1728800
    },
    {
      "epoch": 15.775786553763048,
      "grad_norm": 3.1368725299835205,
      "learning_rate": 3.6853511205197464e-05,
      "loss": 0.6778,
      "step": 1728900
    },
    {
      "epoch": 15.776699029126213,
      "grad_norm": 4.345371723175049,
      "learning_rate": 3.6852750809061494e-05,
      "loss": 0.6805,
      "step": 1729000
    },
    {
      "epoch": 15.777611504489379,
      "grad_norm": 3.345973014831543,
      "learning_rate": 3.685199041292552e-05,
      "loss": 0.6371,
      "step": 1729100
    },
    {
      "epoch": 15.778523979852544,
      "grad_norm": 4.79374885559082,
      "learning_rate": 3.685123001678955e-05,
      "loss": 0.6693,
      "step": 1729200
    },
    {
      "epoch": 15.77943645521571,
      "grad_norm": 3.661268949508667,
      "learning_rate": 3.685046962065358e-05,
      "loss": 0.6752,
      "step": 1729300
    },
    {
      "epoch": 15.780348930578874,
      "grad_norm": 4.991915702819824,
      "learning_rate": 3.684970922451761e-05,
      "loss": 0.6884,
      "step": 1729400
    },
    {
      "epoch": 15.78126140594204,
      "grad_norm": 4.280637741088867,
      "learning_rate": 3.684894882838164e-05,
      "loss": 0.6962,
      "step": 1729500
    },
    {
      "epoch": 15.782173881305205,
      "grad_norm": 4.071899890899658,
      "learning_rate": 3.684818843224567e-05,
      "loss": 0.6891,
      "step": 1729600
    },
    {
      "epoch": 15.78308635666837,
      "grad_norm": 2.7300217151641846,
      "learning_rate": 3.684742803610969e-05,
      "loss": 0.6375,
      "step": 1729700
    },
    {
      "epoch": 15.783998832031536,
      "grad_norm": 5.222233772277832,
      "learning_rate": 3.684666763997372e-05,
      "loss": 0.6746,
      "step": 1729800
    },
    {
      "epoch": 15.784911307394701,
      "grad_norm": 3.251832962036133,
      "learning_rate": 3.684590724383775e-05,
      "loss": 0.6957,
      "step": 1729900
    },
    {
      "epoch": 15.785823782757866,
      "grad_norm": 3.5411102771759033,
      "learning_rate": 3.684514684770178e-05,
      "loss": 0.7034,
      "step": 1730000
    },
    {
      "epoch": 15.786736258121032,
      "grad_norm": 4.777218341827393,
      "learning_rate": 3.684438645156581e-05,
      "loss": 0.662,
      "step": 1730100
    },
    {
      "epoch": 15.787648733484197,
      "grad_norm": 2.273940324783325,
      "learning_rate": 3.6843626055429835e-05,
      "loss": 0.6814,
      "step": 1730200
    },
    {
      "epoch": 15.78856120884736,
      "grad_norm": 3.9390459060668945,
      "learning_rate": 3.684286565929387e-05,
      "loss": 0.671,
      "step": 1730300
    },
    {
      "epoch": 15.789473684210526,
      "grad_norm": 4.393613815307617,
      "learning_rate": 3.6842105263157895e-05,
      "loss": 0.6151,
      "step": 1730400
    },
    {
      "epoch": 15.790386159573691,
      "grad_norm": 2.3816940784454346,
      "learning_rate": 3.6841344867021925e-05,
      "loss": 0.6707,
      "step": 1730500
    },
    {
      "epoch": 15.791298634936856,
      "grad_norm": 4.426286220550537,
      "learning_rate": 3.6840584470885955e-05,
      "loss": 0.6828,
      "step": 1730600
    },
    {
      "epoch": 15.792211110300022,
      "grad_norm": 3.1096725463867188,
      "learning_rate": 3.6839824074749985e-05,
      "loss": 0.6683,
      "step": 1730700
    },
    {
      "epoch": 15.793123585663187,
      "grad_norm": 4.360469818115234,
      "learning_rate": 3.683906367861401e-05,
      "loss": 0.6436,
      "step": 1730800
    },
    {
      "epoch": 15.794036061026352,
      "grad_norm": 3.676905632019043,
      "learning_rate": 3.6838303282478045e-05,
      "loss": 0.6684,
      "step": 1730900
    },
    {
      "epoch": 15.794948536389517,
      "grad_norm": 3.733800172805786,
      "learning_rate": 3.683754288634207e-05,
      "loss": 0.6655,
      "step": 1731000
    },
    {
      "epoch": 15.795861011752683,
      "grad_norm": 4.427998065948486,
      "learning_rate": 3.68367824902061e-05,
      "loss": 0.672,
      "step": 1731100
    },
    {
      "epoch": 15.796773487115848,
      "grad_norm": 4.546208381652832,
      "learning_rate": 3.683602209407013e-05,
      "loss": 0.726,
      "step": 1731200
    },
    {
      "epoch": 15.797685962479013,
      "grad_norm": 4.12507963180542,
      "learning_rate": 3.683526169793416e-05,
      "loss": 0.6679,
      "step": 1731300
    },
    {
      "epoch": 15.798598437842179,
      "grad_norm": 4.100751876831055,
      "learning_rate": 3.683450130179819e-05,
      "loss": 0.7196,
      "step": 1731400
    },
    {
      "epoch": 15.799510913205344,
      "grad_norm": 4.316218376159668,
      "learning_rate": 3.683374090566222e-05,
      "loss": 0.715,
      "step": 1731500
    },
    {
      "epoch": 15.80042338856851,
      "grad_norm": 3.3985519409179688,
      "learning_rate": 3.683298050952624e-05,
      "loss": 0.6856,
      "step": 1731600
    },
    {
      "epoch": 15.801335863931675,
      "grad_norm": 3.5981173515319824,
      "learning_rate": 3.683222011339028e-05,
      "loss": 0.6627,
      "step": 1731700
    },
    {
      "epoch": 15.80224833929484,
      "grad_norm": 4.658858299255371,
      "learning_rate": 3.68314597172543e-05,
      "loss": 0.6719,
      "step": 1731800
    },
    {
      "epoch": 15.803160814658003,
      "grad_norm": 4.717973709106445,
      "learning_rate": 3.683069932111833e-05,
      "loss": 0.6941,
      "step": 1731900
    },
    {
      "epoch": 15.804073290021169,
      "grad_norm": 3.1640350818634033,
      "learning_rate": 3.682993892498236e-05,
      "loss": 0.6515,
      "step": 1732000
    },
    {
      "epoch": 15.804985765384334,
      "grad_norm": 3.7058041095733643,
      "learning_rate": 3.682917852884639e-05,
      "loss": 0.6237,
      "step": 1732100
    },
    {
      "epoch": 15.8058982407475,
      "grad_norm": 4.066462993621826,
      "learning_rate": 3.6828418132710416e-05,
      "loss": 0.6633,
      "step": 1732200
    },
    {
      "epoch": 15.806810716110665,
      "grad_norm": 2.973154067993164,
      "learning_rate": 3.682765773657445e-05,
      "loss": 0.6588,
      "step": 1732300
    },
    {
      "epoch": 15.80772319147383,
      "grad_norm": 2.8782167434692383,
      "learning_rate": 3.6826897340438476e-05,
      "loss": 0.669,
      "step": 1732400
    },
    {
      "epoch": 15.808635666836995,
      "grad_norm": 3.792841672897339,
      "learning_rate": 3.6826136944302506e-05,
      "loss": 0.7028,
      "step": 1732500
    },
    {
      "epoch": 15.80954814220016,
      "grad_norm": 3.1901843547821045,
      "learning_rate": 3.6825376548166536e-05,
      "loss": 0.6559,
      "step": 1732600
    },
    {
      "epoch": 15.810460617563326,
      "grad_norm": 3.3141705989837646,
      "learning_rate": 3.682461615203056e-05,
      "loss": 0.6761,
      "step": 1732700
    },
    {
      "epoch": 15.811373092926491,
      "grad_norm": 3.9345617294311523,
      "learning_rate": 3.6823855755894596e-05,
      "loss": 0.6348,
      "step": 1732800
    },
    {
      "epoch": 15.812285568289656,
      "grad_norm": 3.44698166847229,
      "learning_rate": 3.682309535975862e-05,
      "loss": 0.6815,
      "step": 1732900
    },
    {
      "epoch": 15.813198043652822,
      "grad_norm": 4.973950386047363,
      "learning_rate": 3.682233496362265e-05,
      "loss": 0.6469,
      "step": 1733000
    },
    {
      "epoch": 15.814110519015987,
      "grad_norm": 4.113711833953857,
      "learning_rate": 3.682157456748668e-05,
      "loss": 0.6453,
      "step": 1733100
    },
    {
      "epoch": 15.815022994379152,
      "grad_norm": 4.192592144012451,
      "learning_rate": 3.682081417135071e-05,
      "loss": 0.6741,
      "step": 1733200
    },
    {
      "epoch": 15.815935469742318,
      "grad_norm": 4.25423002243042,
      "learning_rate": 3.682005377521473e-05,
      "loss": 0.6953,
      "step": 1733300
    },
    {
      "epoch": 15.816847945105483,
      "grad_norm": 3.7412352561950684,
      "learning_rate": 3.681929337907877e-05,
      "loss": 0.6384,
      "step": 1733400
    },
    {
      "epoch": 15.817760420468648,
      "grad_norm": 3.9589035511016846,
      "learning_rate": 3.681853298294279e-05,
      "loss": 0.675,
      "step": 1733500
    },
    {
      "epoch": 15.818672895831813,
      "grad_norm": 3.38166880607605,
      "learning_rate": 3.681777258680682e-05,
      "loss": 0.6409,
      "step": 1733600
    },
    {
      "epoch": 15.819585371194977,
      "grad_norm": 4.3202128410339355,
      "learning_rate": 3.681701219067085e-05,
      "loss": 0.6376,
      "step": 1733700
    },
    {
      "epoch": 15.820497846558142,
      "grad_norm": 4.046104907989502,
      "learning_rate": 3.681625179453488e-05,
      "loss": 0.6743,
      "step": 1733800
    },
    {
      "epoch": 15.821410321921308,
      "grad_norm": 3.3819057941436768,
      "learning_rate": 3.681549139839891e-05,
      "loss": 0.6614,
      "step": 1733900
    },
    {
      "epoch": 15.822322797284473,
      "grad_norm": 3.831120729446411,
      "learning_rate": 3.681473100226294e-05,
      "loss": 0.6934,
      "step": 1734000
    },
    {
      "epoch": 15.823235272647638,
      "grad_norm": 4.254180431365967,
      "learning_rate": 3.6813970606126967e-05,
      "loss": 0.6362,
      "step": 1734100
    },
    {
      "epoch": 15.824147748010803,
      "grad_norm": 3.436861276626587,
      "learning_rate": 3.6813210209991003e-05,
      "loss": 0.6656,
      "step": 1734200
    },
    {
      "epoch": 15.825060223373969,
      "grad_norm": 3.277277946472168,
      "learning_rate": 3.681244981385503e-05,
      "loss": 0.6589,
      "step": 1734300
    },
    {
      "epoch": 15.825972698737134,
      "grad_norm": 4.074390888214111,
      "learning_rate": 3.681168941771906e-05,
      "loss": 0.6388,
      "step": 1734400
    },
    {
      "epoch": 15.8268851741003,
      "grad_norm": 3.165679693222046,
      "learning_rate": 3.681092902158309e-05,
      "loss": 0.6573,
      "step": 1734500
    },
    {
      "epoch": 15.827797649463465,
      "grad_norm": 4.632577419281006,
      "learning_rate": 3.681016862544712e-05,
      "loss": 0.7037,
      "step": 1734600
    },
    {
      "epoch": 15.82871012482663,
      "grad_norm": 3.7398200035095215,
      "learning_rate": 3.680940822931114e-05,
      "loss": 0.6764,
      "step": 1734700
    },
    {
      "epoch": 15.829622600189795,
      "grad_norm": 3.8153436183929443,
      "learning_rate": 3.680864783317518e-05,
      "loss": 0.6928,
      "step": 1734800
    },
    {
      "epoch": 15.83053507555296,
      "grad_norm": 3.699423313140869,
      "learning_rate": 3.68078874370392e-05,
      "loss": 0.6535,
      "step": 1734900
    },
    {
      "epoch": 15.831447550916126,
      "grad_norm": 4.14674186706543,
      "learning_rate": 3.680712704090323e-05,
      "loss": 0.667,
      "step": 1735000
    },
    {
      "epoch": 15.832360026279291,
      "grad_norm": 4.462628364562988,
      "learning_rate": 3.680636664476726e-05,
      "loss": 0.6925,
      "step": 1735100
    },
    {
      "epoch": 15.833272501642456,
      "grad_norm": 3.4463508129119873,
      "learning_rate": 3.680560624863129e-05,
      "loss": 0.6729,
      "step": 1735200
    },
    {
      "epoch": 15.83418497700562,
      "grad_norm": 3.7825381755828857,
      "learning_rate": 3.680484585249532e-05,
      "loss": 0.6765,
      "step": 1735300
    },
    {
      "epoch": 15.835097452368785,
      "grad_norm": 3.074815034866333,
      "learning_rate": 3.6804085456359344e-05,
      "loss": 0.69,
      "step": 1735400
    },
    {
      "epoch": 15.83600992773195,
      "grad_norm": 4.385704517364502,
      "learning_rate": 3.6803325060223374e-05,
      "loss": 0.68,
      "step": 1735500
    },
    {
      "epoch": 15.836922403095116,
      "grad_norm": 4.60867977142334,
      "learning_rate": 3.6802564664087404e-05,
      "loss": 0.6542,
      "step": 1735600
    },
    {
      "epoch": 15.837834878458281,
      "grad_norm": 4.107113838195801,
      "learning_rate": 3.6801804267951434e-05,
      "loss": 0.6795,
      "step": 1735700
    },
    {
      "epoch": 15.838747353821446,
      "grad_norm": 4.067498207092285,
      "learning_rate": 3.680104387181546e-05,
      "loss": 0.6781,
      "step": 1735800
    },
    {
      "epoch": 15.839659829184612,
      "grad_norm": 4.5056023597717285,
      "learning_rate": 3.6800283475679494e-05,
      "loss": 0.669,
      "step": 1735900
    },
    {
      "epoch": 15.840572304547777,
      "grad_norm": 3.496683120727539,
      "learning_rate": 3.679952307954352e-05,
      "loss": 0.6784,
      "step": 1736000
    },
    {
      "epoch": 15.841484779910942,
      "grad_norm": 4.261984825134277,
      "learning_rate": 3.679876268340755e-05,
      "loss": 0.6686,
      "step": 1736100
    },
    {
      "epoch": 15.842397255274108,
      "grad_norm": 3.6190226078033447,
      "learning_rate": 3.679800228727158e-05,
      "loss": 0.6844,
      "step": 1736200
    },
    {
      "epoch": 15.843309730637273,
      "grad_norm": 3.5727312564849854,
      "learning_rate": 3.679724189113561e-05,
      "loss": 0.7058,
      "step": 1736300
    },
    {
      "epoch": 15.844222206000438,
      "grad_norm": 3.8148488998413086,
      "learning_rate": 3.679648149499964e-05,
      "loss": 0.698,
      "step": 1736400
    },
    {
      "epoch": 15.845134681363604,
      "grad_norm": 4.246639728546143,
      "learning_rate": 3.679572109886367e-05,
      "loss": 0.6431,
      "step": 1736500
    },
    {
      "epoch": 15.846047156726769,
      "grad_norm": 3.2474851608276367,
      "learning_rate": 3.679496070272769e-05,
      "loss": 0.6484,
      "step": 1736600
    },
    {
      "epoch": 15.846959632089934,
      "grad_norm": 4.145986557006836,
      "learning_rate": 3.679420030659173e-05,
      "loss": 0.6757,
      "step": 1736700
    },
    {
      "epoch": 15.8478721074531,
      "grad_norm": 3.5532712936401367,
      "learning_rate": 3.679343991045575e-05,
      "loss": 0.6445,
      "step": 1736800
    },
    {
      "epoch": 15.848784582816265,
      "grad_norm": 3.7712385654449463,
      "learning_rate": 3.679267951431978e-05,
      "loss": 0.6561,
      "step": 1736900
    },
    {
      "epoch": 15.84969705817943,
      "grad_norm": 2.3696112632751465,
      "learning_rate": 3.679191911818381e-05,
      "loss": 0.6233,
      "step": 1737000
    },
    {
      "epoch": 15.850609533542594,
      "grad_norm": 3.827742576599121,
      "learning_rate": 3.679115872204784e-05,
      "loss": 0.673,
      "step": 1737100
    },
    {
      "epoch": 15.851522008905759,
      "grad_norm": 4.3723649978637695,
      "learning_rate": 3.6790398325911865e-05,
      "loss": 0.6384,
      "step": 1737200
    },
    {
      "epoch": 15.852434484268924,
      "grad_norm": 3.0035035610198975,
      "learning_rate": 3.67896379297759e-05,
      "loss": 0.6518,
      "step": 1737300
    },
    {
      "epoch": 15.85334695963209,
      "grad_norm": 8.925289154052734,
      "learning_rate": 3.6788877533639925e-05,
      "loss": 0.6794,
      "step": 1737400
    },
    {
      "epoch": 15.854259434995255,
      "grad_norm": 3.491641044616699,
      "learning_rate": 3.6788117137503955e-05,
      "loss": 0.6746,
      "step": 1737500
    },
    {
      "epoch": 15.85517191035842,
      "grad_norm": 4.912239074707031,
      "learning_rate": 3.6787356741367985e-05,
      "loss": 0.6377,
      "step": 1737600
    },
    {
      "epoch": 15.856084385721585,
      "grad_norm": 4.61991548538208,
      "learning_rate": 3.6786596345232015e-05,
      "loss": 0.6513,
      "step": 1737700
    },
    {
      "epoch": 15.85699686108475,
      "grad_norm": 1.844023585319519,
      "learning_rate": 3.6785835949096045e-05,
      "loss": 0.626,
      "step": 1737800
    },
    {
      "epoch": 15.857909336447916,
      "grad_norm": 4.088386535644531,
      "learning_rate": 3.6785075552960075e-05,
      "loss": 0.6532,
      "step": 1737900
    },
    {
      "epoch": 15.858821811811081,
      "grad_norm": 3.680483341217041,
      "learning_rate": 3.67843151568241e-05,
      "loss": 0.6746,
      "step": 1738000
    },
    {
      "epoch": 15.859734287174247,
      "grad_norm": 4.826117515563965,
      "learning_rate": 3.6783554760688135e-05,
      "loss": 0.6713,
      "step": 1738100
    },
    {
      "epoch": 15.860646762537412,
      "grad_norm": 4.149909496307373,
      "learning_rate": 3.678279436455216e-05,
      "loss": 0.7093,
      "step": 1738200
    },
    {
      "epoch": 15.861559237900577,
      "grad_norm": 3.367138147354126,
      "learning_rate": 3.678203396841618e-05,
      "loss": 0.7145,
      "step": 1738300
    },
    {
      "epoch": 15.862471713263743,
      "grad_norm": 4.061219692230225,
      "learning_rate": 3.678127357228022e-05,
      "loss": 0.6741,
      "step": 1738400
    },
    {
      "epoch": 15.863384188626908,
      "grad_norm": 3.969902753829956,
      "learning_rate": 3.678051317614424e-05,
      "loss": 0.6672,
      "step": 1738500
    },
    {
      "epoch": 15.864296663990073,
      "grad_norm": 4.403420925140381,
      "learning_rate": 3.677975278000827e-05,
      "loss": 0.7156,
      "step": 1738600
    },
    {
      "epoch": 15.865209139353237,
      "grad_norm": 4.535434722900391,
      "learning_rate": 3.67789923838723e-05,
      "loss": 0.6847,
      "step": 1738700
    },
    {
      "epoch": 15.866121614716402,
      "grad_norm": 4.217838764190674,
      "learning_rate": 3.677823198773633e-05,
      "loss": 0.6564,
      "step": 1738800
    },
    {
      "epoch": 15.867034090079567,
      "grad_norm": 3.874206781387329,
      "learning_rate": 3.677747159160036e-05,
      "loss": 0.7157,
      "step": 1738900
    },
    {
      "epoch": 15.867946565442733,
      "grad_norm": 3.9905190467834473,
      "learning_rate": 3.677671119546439e-05,
      "loss": 0.6984,
      "step": 1739000
    },
    {
      "epoch": 15.868859040805898,
      "grad_norm": 3.6072518825531006,
      "learning_rate": 3.6775950799328416e-05,
      "loss": 0.6267,
      "step": 1739100
    },
    {
      "epoch": 15.869771516169063,
      "grad_norm": 4.1608381271362305,
      "learning_rate": 3.677519040319245e-05,
      "loss": 0.693,
      "step": 1739200
    },
    {
      "epoch": 15.870683991532228,
      "grad_norm": 3.820457935333252,
      "learning_rate": 3.6774430007056476e-05,
      "loss": 0.645,
      "step": 1739300
    },
    {
      "epoch": 15.871596466895394,
      "grad_norm": 4.062702178955078,
      "learning_rate": 3.6773669610920506e-05,
      "loss": 0.6405,
      "step": 1739400
    },
    {
      "epoch": 15.872508942258559,
      "grad_norm": 3.9705722332000732,
      "learning_rate": 3.6772909214784536e-05,
      "loss": 0.6534,
      "step": 1739500
    },
    {
      "epoch": 15.873421417621724,
      "grad_norm": 3.3578155040740967,
      "learning_rate": 3.6772148818648566e-05,
      "loss": 0.6365,
      "step": 1739600
    },
    {
      "epoch": 15.87433389298489,
      "grad_norm": 3.5644032955169678,
      "learning_rate": 3.677138842251259e-05,
      "loss": 0.6579,
      "step": 1739700
    },
    {
      "epoch": 15.875246368348055,
      "grad_norm": 4.663933277130127,
      "learning_rate": 3.6770628026376626e-05,
      "loss": 0.661,
      "step": 1739800
    },
    {
      "epoch": 15.87615884371122,
      "grad_norm": 3.8353257179260254,
      "learning_rate": 3.676986763024065e-05,
      "loss": 0.712,
      "step": 1739900
    },
    {
      "epoch": 15.877071319074386,
      "grad_norm": 3.622861385345459,
      "learning_rate": 3.676910723410468e-05,
      "loss": 0.6656,
      "step": 1740000
    },
    {
      "epoch": 15.87798379443755,
      "grad_norm": 2.4579577445983887,
      "learning_rate": 3.676834683796871e-05,
      "loss": 0.6707,
      "step": 1740100
    },
    {
      "epoch": 15.878896269800716,
      "grad_norm": 5.060544013977051,
      "learning_rate": 3.676758644183274e-05,
      "loss": 0.6526,
      "step": 1740200
    },
    {
      "epoch": 15.879808745163881,
      "grad_norm": 4.874864101409912,
      "learning_rate": 3.676682604569677e-05,
      "loss": 0.6266,
      "step": 1740300
    },
    {
      "epoch": 15.880721220527047,
      "grad_norm": 4.2593674659729,
      "learning_rate": 3.67660656495608e-05,
      "loss": 0.6805,
      "step": 1740400
    },
    {
      "epoch": 15.88163369589021,
      "grad_norm": 3.779021978378296,
      "learning_rate": 3.676530525342482e-05,
      "loss": 0.6291,
      "step": 1740500
    },
    {
      "epoch": 15.882546171253376,
      "grad_norm": 4.080862522125244,
      "learning_rate": 3.676454485728886e-05,
      "loss": 0.6537,
      "step": 1740600
    },
    {
      "epoch": 15.88345864661654,
      "grad_norm": 3.386857509613037,
      "learning_rate": 3.676378446115288e-05,
      "loss": 0.6758,
      "step": 1740700
    },
    {
      "epoch": 15.884371121979706,
      "grad_norm": 3.7719550132751465,
      "learning_rate": 3.676302406501691e-05,
      "loss": 0.6471,
      "step": 1740800
    },
    {
      "epoch": 15.885283597342871,
      "grad_norm": 4.428166389465332,
      "learning_rate": 3.676226366888094e-05,
      "loss": 0.7016,
      "step": 1740900
    },
    {
      "epoch": 15.886196072706037,
      "grad_norm": 2.8341293334960938,
      "learning_rate": 3.6761503272744973e-05,
      "loss": 0.6766,
      "step": 1741000
    },
    {
      "epoch": 15.887108548069202,
      "grad_norm": 2.8754563331604004,
      "learning_rate": 3.6760742876609e-05,
      "loss": 0.6635,
      "step": 1741100
    },
    {
      "epoch": 15.888021023432367,
      "grad_norm": 3.911031723022461,
      "learning_rate": 3.675998248047303e-05,
      "loss": 0.712,
      "step": 1741200
    },
    {
      "epoch": 15.888933498795533,
      "grad_norm": 4.481697082519531,
      "learning_rate": 3.675922208433706e-05,
      "loss": 0.6571,
      "step": 1741300
    },
    {
      "epoch": 15.889845974158698,
      "grad_norm": 6.83953857421875,
      "learning_rate": 3.675846168820109e-05,
      "loss": 0.6796,
      "step": 1741400
    },
    {
      "epoch": 15.890758449521863,
      "grad_norm": 3.7709193229675293,
      "learning_rate": 3.675770129206512e-05,
      "loss": 0.6445,
      "step": 1741500
    },
    {
      "epoch": 15.891670924885029,
      "grad_norm": 4.033744812011719,
      "learning_rate": 3.675694089592914e-05,
      "loss": 0.6494,
      "step": 1741600
    },
    {
      "epoch": 15.892583400248194,
      "grad_norm": 3.7239530086517334,
      "learning_rate": 3.675618049979318e-05,
      "loss": 0.6441,
      "step": 1741700
    },
    {
      "epoch": 15.89349587561136,
      "grad_norm": 2.934264898300171,
      "learning_rate": 3.67554201036572e-05,
      "loss": 0.6442,
      "step": 1741800
    },
    {
      "epoch": 15.894408350974524,
      "grad_norm": 3.791761875152588,
      "learning_rate": 3.675465970752123e-05,
      "loss": 0.6404,
      "step": 1741900
    },
    {
      "epoch": 15.895320826337688,
      "grad_norm": 3.7284936904907227,
      "learning_rate": 3.675389931138526e-05,
      "loss": 0.6926,
      "step": 1742000
    },
    {
      "epoch": 15.896233301700853,
      "grad_norm": 3.3416197299957275,
      "learning_rate": 3.675313891524929e-05,
      "loss": 0.6743,
      "step": 1742100
    },
    {
      "epoch": 15.897145777064019,
      "grad_norm": 3.99878191947937,
      "learning_rate": 3.675237851911332e-05,
      "loss": 0.6377,
      "step": 1742200
    },
    {
      "epoch": 15.898058252427184,
      "grad_norm": 4.868630886077881,
      "learning_rate": 3.675161812297735e-05,
      "loss": 0.6934,
      "step": 1742300
    },
    {
      "epoch": 15.89897072779035,
      "grad_norm": 4.473178386688232,
      "learning_rate": 3.6750857726841374e-05,
      "loss": 0.6366,
      "step": 1742400
    },
    {
      "epoch": 15.899883203153514,
      "grad_norm": 4.657271385192871,
      "learning_rate": 3.675009733070541e-05,
      "loss": 0.695,
      "step": 1742500
    },
    {
      "epoch": 15.90079567851668,
      "grad_norm": 4.895573616027832,
      "learning_rate": 3.6749336934569434e-05,
      "loss": 0.7033,
      "step": 1742600
    },
    {
      "epoch": 15.901708153879845,
      "grad_norm": 4.082825183868408,
      "learning_rate": 3.6748576538433464e-05,
      "loss": 0.6774,
      "step": 1742700
    },
    {
      "epoch": 15.90262062924301,
      "grad_norm": 3.4699630737304688,
      "learning_rate": 3.6747816142297494e-05,
      "loss": 0.6596,
      "step": 1742800
    },
    {
      "epoch": 15.903533104606176,
      "grad_norm": 3.7572906017303467,
      "learning_rate": 3.6747055746161524e-05,
      "loss": 0.6658,
      "step": 1742900
    },
    {
      "epoch": 15.904445579969341,
      "grad_norm": 4.233731746673584,
      "learning_rate": 3.674629535002555e-05,
      "loss": 0.6802,
      "step": 1743000
    },
    {
      "epoch": 15.905358055332506,
      "grad_norm": 4.041742324829102,
      "learning_rate": 3.6745534953889585e-05,
      "loss": 0.6729,
      "step": 1743100
    },
    {
      "epoch": 15.906270530695672,
      "grad_norm": 4.076582908630371,
      "learning_rate": 3.674477455775361e-05,
      "loss": 0.6788,
      "step": 1743200
    },
    {
      "epoch": 15.907183006058837,
      "grad_norm": 3.6483588218688965,
      "learning_rate": 3.674401416161764e-05,
      "loss": 0.6739,
      "step": 1743300
    },
    {
      "epoch": 15.908095481422002,
      "grad_norm": 4.453060150146484,
      "learning_rate": 3.674325376548167e-05,
      "loss": 0.6947,
      "step": 1743400
    },
    {
      "epoch": 15.909007956785167,
      "grad_norm": 4.470374584197998,
      "learning_rate": 3.67424933693457e-05,
      "loss": 0.696,
      "step": 1743500
    },
    {
      "epoch": 15.909920432148333,
      "grad_norm": 4.111817836761475,
      "learning_rate": 3.674173297320973e-05,
      "loss": 0.6582,
      "step": 1743600
    },
    {
      "epoch": 15.910832907511498,
      "grad_norm": 3.6627302169799805,
      "learning_rate": 3.674097257707376e-05,
      "loss": 0.6745,
      "step": 1743700
    },
    {
      "epoch": 15.911745382874662,
      "grad_norm": 4.90315055847168,
      "learning_rate": 3.674021218093778e-05,
      "loss": 0.6957,
      "step": 1743800
    },
    {
      "epoch": 15.912657858237827,
      "grad_norm": 4.635875225067139,
      "learning_rate": 3.673945178480181e-05,
      "loss": 0.6552,
      "step": 1743900
    },
    {
      "epoch": 15.913570333600992,
      "grad_norm": 4.438558578491211,
      "learning_rate": 3.673869138866584e-05,
      "loss": 0.6683,
      "step": 1744000
    },
    {
      "epoch": 15.914482808964157,
      "grad_norm": 3.8328168392181396,
      "learning_rate": 3.6737930992529865e-05,
      "loss": 0.6746,
      "step": 1744100
    },
    {
      "epoch": 15.915395284327323,
      "grad_norm": 3.7268271446228027,
      "learning_rate": 3.67371705963939e-05,
      "loss": 0.6556,
      "step": 1744200
    },
    {
      "epoch": 15.916307759690488,
      "grad_norm": 2.507582902908325,
      "learning_rate": 3.6736410200257925e-05,
      "loss": 0.6563,
      "step": 1744300
    },
    {
      "epoch": 15.917220235053653,
      "grad_norm": 4.065152168273926,
      "learning_rate": 3.6735649804121955e-05,
      "loss": 0.6765,
      "step": 1744400
    },
    {
      "epoch": 15.918132710416819,
      "grad_norm": 4.012670993804932,
      "learning_rate": 3.6734889407985985e-05,
      "loss": 0.6562,
      "step": 1744500
    },
    {
      "epoch": 15.919045185779984,
      "grad_norm": 3.8503811359405518,
      "learning_rate": 3.6734129011850015e-05,
      "loss": 0.648,
      "step": 1744600
    },
    {
      "epoch": 15.91995766114315,
      "grad_norm": 3.6949872970581055,
      "learning_rate": 3.6733368615714045e-05,
      "loss": 0.6463,
      "step": 1744700
    },
    {
      "epoch": 15.920870136506315,
      "grad_norm": 3.9840166568756104,
      "learning_rate": 3.6732608219578075e-05,
      "loss": 0.7015,
      "step": 1744800
    },
    {
      "epoch": 15.92178261186948,
      "grad_norm": 4.2656779289245605,
      "learning_rate": 3.67318478234421e-05,
      "loss": 0.6357,
      "step": 1744900
    },
    {
      "epoch": 15.922695087232645,
      "grad_norm": 3.680586814880371,
      "learning_rate": 3.6731087427306135e-05,
      "loss": 0.6893,
      "step": 1745000
    },
    {
      "epoch": 15.92360756259581,
      "grad_norm": 4.334374904632568,
      "learning_rate": 3.673032703117016e-05,
      "loss": 0.6717,
      "step": 1745100
    },
    {
      "epoch": 15.924520037958976,
      "grad_norm": 3.799865245819092,
      "learning_rate": 3.672956663503419e-05,
      "loss": 0.6751,
      "step": 1745200
    },
    {
      "epoch": 15.925432513322141,
      "grad_norm": 3.61271333694458,
      "learning_rate": 3.672880623889822e-05,
      "loss": 0.7005,
      "step": 1745300
    },
    {
      "epoch": 15.926344988685305,
      "grad_norm": 3.7487173080444336,
      "learning_rate": 3.672804584276225e-05,
      "loss": 0.6712,
      "step": 1745400
    },
    {
      "epoch": 15.92725746404847,
      "grad_norm": 4.051797389984131,
      "learning_rate": 3.672728544662627e-05,
      "loss": 0.6373,
      "step": 1745500
    },
    {
      "epoch": 15.928169939411635,
      "grad_norm": 3.6313092708587646,
      "learning_rate": 3.672652505049031e-05,
      "loss": 0.665,
      "step": 1745600
    },
    {
      "epoch": 15.9290824147748,
      "grad_norm": 3.7466366291046143,
      "learning_rate": 3.672576465435433e-05,
      "loss": 0.6289,
      "step": 1745700
    },
    {
      "epoch": 15.929994890137966,
      "grad_norm": 3.0674209594726562,
      "learning_rate": 3.672500425821836e-05,
      "loss": 0.6487,
      "step": 1745800
    },
    {
      "epoch": 15.930907365501131,
      "grad_norm": 3.782491683959961,
      "learning_rate": 3.672424386208239e-05,
      "loss": 0.6693,
      "step": 1745900
    },
    {
      "epoch": 15.931819840864296,
      "grad_norm": 4.59079647064209,
      "learning_rate": 3.672348346594642e-05,
      "loss": 0.675,
      "step": 1746000
    },
    {
      "epoch": 15.932732316227462,
      "grad_norm": 4.813939094543457,
      "learning_rate": 3.672272306981045e-05,
      "loss": 0.6954,
      "step": 1746100
    },
    {
      "epoch": 15.933644791590627,
      "grad_norm": 3.6484580039978027,
      "learning_rate": 3.672196267367448e-05,
      "loss": 0.681,
      "step": 1746200
    },
    {
      "epoch": 15.934557266953792,
      "grad_norm": 4.297537326812744,
      "learning_rate": 3.6721202277538506e-05,
      "loss": 0.7067,
      "step": 1746300
    },
    {
      "epoch": 15.935469742316958,
      "grad_norm": 3.5279436111450195,
      "learning_rate": 3.672044188140254e-05,
      "loss": 0.6341,
      "step": 1746400
    },
    {
      "epoch": 15.936382217680123,
      "grad_norm": 3.2357640266418457,
      "learning_rate": 3.6719681485266566e-05,
      "loss": 0.6794,
      "step": 1746500
    },
    {
      "epoch": 15.937294693043288,
      "grad_norm": 2.035309076309204,
      "learning_rate": 3.6718921089130596e-05,
      "loss": 0.6241,
      "step": 1746600
    },
    {
      "epoch": 15.938207168406453,
      "grad_norm": 4.286471366882324,
      "learning_rate": 3.6718160692994626e-05,
      "loss": 0.675,
      "step": 1746700
    },
    {
      "epoch": 15.939119643769619,
      "grad_norm": 3.4158153533935547,
      "learning_rate": 3.671740029685865e-05,
      "loss": 0.6535,
      "step": 1746800
    },
    {
      "epoch": 15.940032119132784,
      "grad_norm": 3.79162859916687,
      "learning_rate": 3.671663990072268e-05,
      "loss": 0.6823,
      "step": 1746900
    },
    {
      "epoch": 15.94094459449595,
      "grad_norm": 4.093719959259033,
      "learning_rate": 3.671587950458671e-05,
      "loss": 0.6591,
      "step": 1747000
    },
    {
      "epoch": 15.941857069859115,
      "grad_norm": 3.8354058265686035,
      "learning_rate": 3.671511910845074e-05,
      "loss": 0.6803,
      "step": 1747100
    },
    {
      "epoch": 15.942769545222278,
      "grad_norm": 2.848134756088257,
      "learning_rate": 3.671435871231477e-05,
      "loss": 0.6458,
      "step": 1747200
    },
    {
      "epoch": 15.943682020585443,
      "grad_norm": 3.681342840194702,
      "learning_rate": 3.67135983161788e-05,
      "loss": 0.6486,
      "step": 1747300
    },
    {
      "epoch": 15.944594495948609,
      "grad_norm": 3.8262808322906494,
      "learning_rate": 3.671283792004282e-05,
      "loss": 0.6885,
      "step": 1747400
    },
    {
      "epoch": 15.945506971311774,
      "grad_norm": 4.033045291900635,
      "learning_rate": 3.671207752390686e-05,
      "loss": 0.6569,
      "step": 1747500
    },
    {
      "epoch": 15.94641944667494,
      "grad_norm": 3.7680671215057373,
      "learning_rate": 3.671131712777088e-05,
      "loss": 0.66,
      "step": 1747600
    },
    {
      "epoch": 15.947331922038105,
      "grad_norm": 4.104574203491211,
      "learning_rate": 3.6710556731634913e-05,
      "loss": 0.7207,
      "step": 1747700
    },
    {
      "epoch": 15.94824439740127,
      "grad_norm": 4.035253524780273,
      "learning_rate": 3.6709796335498943e-05,
      "loss": 0.6559,
      "step": 1747800
    },
    {
      "epoch": 15.949156872764435,
      "grad_norm": 4.333845138549805,
      "learning_rate": 3.6709035939362974e-05,
      "loss": 0.6696,
      "step": 1747900
    },
    {
      "epoch": 15.9500693481276,
      "grad_norm": 3.848226308822632,
      "learning_rate": 3.6708275543227e-05,
      "loss": 0.6754,
      "step": 1748000
    },
    {
      "epoch": 15.950981823490766,
      "grad_norm": 3.698686361312866,
      "learning_rate": 3.6707515147091034e-05,
      "loss": 0.6983,
      "step": 1748100
    },
    {
      "epoch": 15.951894298853931,
      "grad_norm": 3.5263092517852783,
      "learning_rate": 3.670675475095506e-05,
      "loss": 0.669,
      "step": 1748200
    },
    {
      "epoch": 15.952806774217096,
      "grad_norm": 4.035146236419678,
      "learning_rate": 3.670599435481909e-05,
      "loss": 0.6268,
      "step": 1748300
    },
    {
      "epoch": 15.953719249580262,
      "grad_norm": 4.632355690002441,
      "learning_rate": 3.670523395868312e-05,
      "loss": 0.6596,
      "step": 1748400
    },
    {
      "epoch": 15.954631724943427,
      "grad_norm": 3.4747109413146973,
      "learning_rate": 3.670447356254715e-05,
      "loss": 0.6743,
      "step": 1748500
    },
    {
      "epoch": 15.955544200306592,
      "grad_norm": 2.541659116744995,
      "learning_rate": 3.670371316641118e-05,
      "loss": 0.6882,
      "step": 1748600
    },
    {
      "epoch": 15.956456675669758,
      "grad_norm": 3.9862499237060547,
      "learning_rate": 3.670295277027521e-05,
      "loss": 0.6783,
      "step": 1748700
    },
    {
      "epoch": 15.957369151032921,
      "grad_norm": 3.042583703994751,
      "learning_rate": 3.670219237413923e-05,
      "loss": 0.6496,
      "step": 1748800
    },
    {
      "epoch": 15.958281626396086,
      "grad_norm": 4.6752705574035645,
      "learning_rate": 3.670143197800327e-05,
      "loss": 0.6808,
      "step": 1748900
    },
    {
      "epoch": 15.959194101759252,
      "grad_norm": 3.914614200592041,
      "learning_rate": 3.670067158186729e-05,
      "loss": 0.671,
      "step": 1749000
    },
    {
      "epoch": 15.960106577122417,
      "grad_norm": 3.0557172298431396,
      "learning_rate": 3.669991118573132e-05,
      "loss": 0.676,
      "step": 1749100
    },
    {
      "epoch": 15.961019052485582,
      "grad_norm": 5.164611339569092,
      "learning_rate": 3.669915078959535e-05,
      "loss": 0.6585,
      "step": 1749200
    },
    {
      "epoch": 15.961931527848748,
      "grad_norm": 4.205611705780029,
      "learning_rate": 3.669839039345938e-05,
      "loss": 0.6555,
      "step": 1749300
    },
    {
      "epoch": 15.962844003211913,
      "grad_norm": 4.6070942878723145,
      "learning_rate": 3.6697629997323404e-05,
      "loss": 0.693,
      "step": 1749400
    },
    {
      "epoch": 15.963756478575078,
      "grad_norm": 3.914560317993164,
      "learning_rate": 3.669686960118744e-05,
      "loss": 0.683,
      "step": 1749500
    },
    {
      "epoch": 15.964668953938244,
      "grad_norm": 4.730931282043457,
      "learning_rate": 3.6696109205051464e-05,
      "loss": 0.671,
      "step": 1749600
    },
    {
      "epoch": 15.965581429301409,
      "grad_norm": 4.190812587738037,
      "learning_rate": 3.6695348808915494e-05,
      "loss": 0.6568,
      "step": 1749700
    },
    {
      "epoch": 15.966493904664574,
      "grad_norm": 2.997281551361084,
      "learning_rate": 3.6694588412779524e-05,
      "loss": 0.6621,
      "step": 1749800
    },
    {
      "epoch": 15.96740638002774,
      "grad_norm": 3.50553297996521,
      "learning_rate": 3.669382801664355e-05,
      "loss": 0.6496,
      "step": 1749900
    },
    {
      "epoch": 15.968318855390905,
      "grad_norm": 3.811412811279297,
      "learning_rate": 3.6693067620507585e-05,
      "loss": 0.6666,
      "step": 1750000
    },
    {
      "epoch": 15.96923133075407,
      "grad_norm": 3.103748083114624,
      "learning_rate": 3.669230722437161e-05,
      "loss": 0.6598,
      "step": 1750100
    },
    {
      "epoch": 15.970143806117235,
      "grad_norm": 5.6157684326171875,
      "learning_rate": 3.669154682823564e-05,
      "loss": 0.6872,
      "step": 1750200
    },
    {
      "epoch": 15.9710562814804,
      "grad_norm": 4.984020233154297,
      "learning_rate": 3.669078643209967e-05,
      "loss": 0.6904,
      "step": 1750300
    },
    {
      "epoch": 15.971968756843566,
      "grad_norm": 3.4805409908294678,
      "learning_rate": 3.66900260359637e-05,
      "loss": 0.7147,
      "step": 1750400
    },
    {
      "epoch": 15.972881232206731,
      "grad_norm": 3.644078254699707,
      "learning_rate": 3.668926563982772e-05,
      "loss": 0.6573,
      "step": 1750500
    },
    {
      "epoch": 15.973793707569895,
      "grad_norm": 4.298989772796631,
      "learning_rate": 3.668850524369176e-05,
      "loss": 0.7232,
      "step": 1750600
    },
    {
      "epoch": 15.97470618293306,
      "grad_norm": 4.048397541046143,
      "learning_rate": 3.668774484755578e-05,
      "loss": 0.6257,
      "step": 1750700
    },
    {
      "epoch": 15.975618658296225,
      "grad_norm": 4.229735851287842,
      "learning_rate": 3.668698445141981e-05,
      "loss": 0.6338,
      "step": 1750800
    },
    {
      "epoch": 15.97653113365939,
      "grad_norm": 4.307076930999756,
      "learning_rate": 3.668622405528384e-05,
      "loss": 0.6873,
      "step": 1750900
    },
    {
      "epoch": 15.977443609022556,
      "grad_norm": 3.8431739807128906,
      "learning_rate": 3.668546365914787e-05,
      "loss": 0.6726,
      "step": 1751000
    },
    {
      "epoch": 15.978356084385721,
      "grad_norm": 4.769768714904785,
      "learning_rate": 3.66847032630119e-05,
      "loss": 0.6813,
      "step": 1751100
    },
    {
      "epoch": 15.979268559748887,
      "grad_norm": 3.2625911235809326,
      "learning_rate": 3.668394286687593e-05,
      "loss": 0.6893,
      "step": 1751200
    },
    {
      "epoch": 15.980181035112052,
      "grad_norm": 2.2638444900512695,
      "learning_rate": 3.6683182470739955e-05,
      "loss": 0.6396,
      "step": 1751300
    },
    {
      "epoch": 15.981093510475217,
      "grad_norm": 3.8223164081573486,
      "learning_rate": 3.668242207460399e-05,
      "loss": 0.6852,
      "step": 1751400
    },
    {
      "epoch": 15.982005985838383,
      "grad_norm": 4.127124786376953,
      "learning_rate": 3.6681661678468015e-05,
      "loss": 0.6731,
      "step": 1751500
    },
    {
      "epoch": 15.982918461201548,
      "grad_norm": 4.145125389099121,
      "learning_rate": 3.6680901282332045e-05,
      "loss": 0.6694,
      "step": 1751600
    },
    {
      "epoch": 15.983830936564713,
      "grad_norm": 3.823784828186035,
      "learning_rate": 3.6680140886196075e-05,
      "loss": 0.6782,
      "step": 1751700
    },
    {
      "epoch": 15.984743411927878,
      "grad_norm": 3.9217982292175293,
      "learning_rate": 3.6679380490060105e-05,
      "loss": 0.6548,
      "step": 1751800
    },
    {
      "epoch": 15.985655887291044,
      "grad_norm": 4.114069938659668,
      "learning_rate": 3.667862009392413e-05,
      "loss": 0.6803,
      "step": 1751900
    },
    {
      "epoch": 15.986568362654209,
      "grad_norm": 3.2161734104156494,
      "learning_rate": 3.6677859697788166e-05,
      "loss": 0.6918,
      "step": 1752000
    },
    {
      "epoch": 15.987480838017374,
      "grad_norm": 4.813653469085693,
      "learning_rate": 3.667709930165219e-05,
      "loss": 0.6464,
      "step": 1752100
    },
    {
      "epoch": 15.988393313380538,
      "grad_norm": 3.7454895973205566,
      "learning_rate": 3.667633890551622e-05,
      "loss": 0.632,
      "step": 1752200
    },
    {
      "epoch": 15.989305788743703,
      "grad_norm": 4.107933521270752,
      "learning_rate": 3.667557850938025e-05,
      "loss": 0.6642,
      "step": 1752300
    },
    {
      "epoch": 15.990218264106868,
      "grad_norm": 3.6632204055786133,
      "learning_rate": 3.667481811324427e-05,
      "loss": 0.6757,
      "step": 1752400
    },
    {
      "epoch": 15.991130739470034,
      "grad_norm": 4.724335193634033,
      "learning_rate": 3.667405771710831e-05,
      "loss": 0.6903,
      "step": 1752500
    },
    {
      "epoch": 15.992043214833199,
      "grad_norm": 3.534203052520752,
      "learning_rate": 3.667329732097233e-05,
      "loss": 0.6877,
      "step": 1752600
    },
    {
      "epoch": 15.992955690196364,
      "grad_norm": 3.583505392074585,
      "learning_rate": 3.667253692483636e-05,
      "loss": 0.6915,
      "step": 1752700
    },
    {
      "epoch": 15.99386816555953,
      "grad_norm": 4.669125556945801,
      "learning_rate": 3.667177652870039e-05,
      "loss": 0.6782,
      "step": 1752800
    },
    {
      "epoch": 15.994780640922695,
      "grad_norm": 3.6794052124023438,
      "learning_rate": 3.667101613256442e-05,
      "loss": 0.6825,
      "step": 1752900
    },
    {
      "epoch": 15.99569311628586,
      "grad_norm": 4.099861145019531,
      "learning_rate": 3.667025573642845e-05,
      "loss": 0.6584,
      "step": 1753000
    },
    {
      "epoch": 15.996605591649026,
      "grad_norm": 3.768329620361328,
      "learning_rate": 3.666949534029248e-05,
      "loss": 0.6779,
      "step": 1753100
    },
    {
      "epoch": 15.99751806701219,
      "grad_norm": 4.0186967849731445,
      "learning_rate": 3.6668734944156506e-05,
      "loss": 0.6631,
      "step": 1753200
    },
    {
      "epoch": 15.998430542375356,
      "grad_norm": 3.767873764038086,
      "learning_rate": 3.6667974548020536e-05,
      "loss": 0.6471,
      "step": 1753300
    },
    {
      "epoch": 15.999343017738521,
      "grad_norm": 3.0294718742370605,
      "learning_rate": 3.6667214151884566e-05,
      "loss": 0.6404,
      "step": 1753400
    },
    {
      "epoch": 16.0,
      "eval_loss": 0.5446590781211853,
      "eval_runtime": 25.1445,
      "eval_samples_per_second": 229.434,
      "eval_steps_per_second": 229.434,
      "step": 1753472
    },
    {
      "epoch": 16.0,
      "eval_loss": 0.522283673286438,
      "eval_runtime": 480.745,
      "eval_samples_per_second": 227.963,
      "eval_steps_per_second": 227.963,
      "step": 1753472
    },
    {
      "epoch": 16.000255493101687,
      "grad_norm": 4.193859100341797,
      "learning_rate": 3.6666453755748596e-05,
      "loss": 0.6933,
      "step": 1753500
    },
    {
      "epoch": 16.001167968464852,
      "grad_norm": 4.287057876586914,
      "learning_rate": 3.6665693359612626e-05,
      "loss": 0.6625,
      "step": 1753600
    },
    {
      "epoch": 16.002080443828017,
      "grad_norm": 2.931025981903076,
      "learning_rate": 3.6664932963476656e-05,
      "loss": 0.6726,
      "step": 1753700
    },
    {
      "epoch": 16.002992919191183,
      "grad_norm": 3.9243063926696777,
      "learning_rate": 3.666417256734068e-05,
      "loss": 0.6915,
      "step": 1753800
    },
    {
      "epoch": 16.003905394554348,
      "grad_norm": 3.2994296550750732,
      "learning_rate": 3.6663412171204717e-05,
      "loss": 0.6307,
      "step": 1753900
    },
    {
      "epoch": 16.004817869917513,
      "grad_norm": 5.130234241485596,
      "learning_rate": 3.666265177506874e-05,
      "loss": 0.6405,
      "step": 1754000
    },
    {
      "epoch": 16.00573034528068,
      "grad_norm": 3.7526960372924805,
      "learning_rate": 3.666189137893277e-05,
      "loss": 0.6663,
      "step": 1754100
    },
    {
      "epoch": 16.006642820643844,
      "grad_norm": 4.142563819885254,
      "learning_rate": 3.66611309827968e-05,
      "loss": 0.6564,
      "step": 1754200
    },
    {
      "epoch": 16.00755529600701,
      "grad_norm": 4.780684947967529,
      "learning_rate": 3.666037058666083e-05,
      "loss": 0.6897,
      "step": 1754300
    },
    {
      "epoch": 16.008467771370174,
      "grad_norm": 4.248336315155029,
      "learning_rate": 3.665961019052486e-05,
      "loss": 0.6483,
      "step": 1754400
    },
    {
      "epoch": 16.00938024673334,
      "grad_norm": 3.59641432762146,
      "learning_rate": 3.665884979438889e-05,
      "loss": 0.6652,
      "step": 1754500
    },
    {
      "epoch": 16.010292722096505,
      "grad_norm": 4.144725322723389,
      "learning_rate": 3.6658089398252913e-05,
      "loss": 0.6394,
      "step": 1754600
    },
    {
      "epoch": 16.01120519745967,
      "grad_norm": 3.0819363594055176,
      "learning_rate": 3.6657329002116944e-05,
      "loss": 0.6585,
      "step": 1754700
    },
    {
      "epoch": 16.012117672822832,
      "grad_norm": 2.7452235221862793,
      "learning_rate": 3.6656568605980974e-05,
      "loss": 0.6292,
      "step": 1754800
    },
    {
      "epoch": 16.013030148185997,
      "grad_norm": 4.4798970222473145,
      "learning_rate": 3.6655808209845004e-05,
      "loss": 0.6667,
      "step": 1754900
    },
    {
      "epoch": 16.013942623549163,
      "grad_norm": 3.732516050338745,
      "learning_rate": 3.6655047813709034e-05,
      "loss": 0.666,
      "step": 1755000
    },
    {
      "epoch": 16.014855098912328,
      "grad_norm": 4.426407337188721,
      "learning_rate": 3.6654287417573064e-05,
      "loss": 0.6973,
      "step": 1755100
    },
    {
      "epoch": 16.015767574275493,
      "grad_norm": 3.2039103507995605,
      "learning_rate": 3.665352702143709e-05,
      "loss": 0.6464,
      "step": 1755200
    },
    {
      "epoch": 16.01668004963866,
      "grad_norm": 4.637967586517334,
      "learning_rate": 3.665276662530112e-05,
      "loss": 0.6515,
      "step": 1755300
    },
    {
      "epoch": 16.017592525001824,
      "grad_norm": 3.8837523460388184,
      "learning_rate": 3.665200622916515e-05,
      "loss": 0.6504,
      "step": 1755400
    },
    {
      "epoch": 16.01850500036499,
      "grad_norm": 3.732550859451294,
      "learning_rate": 3.665124583302918e-05,
      "loss": 0.6447,
      "step": 1755500
    },
    {
      "epoch": 16.019417475728154,
      "grad_norm": 3.2022054195404053,
      "learning_rate": 3.665048543689321e-05,
      "loss": 0.6399,
      "step": 1755600
    },
    {
      "epoch": 16.02032995109132,
      "grad_norm": 4.130858421325684,
      "learning_rate": 3.664972504075723e-05,
      "loss": 0.6589,
      "step": 1755700
    },
    {
      "epoch": 16.021242426454485,
      "grad_norm": 4.769571781158447,
      "learning_rate": 3.664896464462127e-05,
      "loss": 0.6963,
      "step": 1755800
    },
    {
      "epoch": 16.02215490181765,
      "grad_norm": 3.904448986053467,
      "learning_rate": 3.664820424848529e-05,
      "loss": 0.675,
      "step": 1755900
    },
    {
      "epoch": 16.023067377180816,
      "grad_norm": 3.7605507373809814,
      "learning_rate": 3.664744385234932e-05,
      "loss": 0.6466,
      "step": 1756000
    },
    {
      "epoch": 16.02397985254398,
      "grad_norm": 4.402413845062256,
      "learning_rate": 3.664668345621335e-05,
      "loss": 0.6543,
      "step": 1756100
    },
    {
      "epoch": 16.024892327907146,
      "grad_norm": 4.168111324310303,
      "learning_rate": 3.664592306007738e-05,
      "loss": 0.6445,
      "step": 1756200
    },
    {
      "epoch": 16.02580480327031,
      "grad_norm": 5.305585861206055,
      "learning_rate": 3.6645162663941404e-05,
      "loss": 0.662,
      "step": 1756300
    },
    {
      "epoch": 16.026717278633477,
      "grad_norm": 2.7207908630371094,
      "learning_rate": 3.664440226780544e-05,
      "loss": 0.6689,
      "step": 1756400
    },
    {
      "epoch": 16.027629753996642,
      "grad_norm": 2.514390707015991,
      "learning_rate": 3.6643641871669464e-05,
      "loss": 0.6209,
      "step": 1756500
    },
    {
      "epoch": 16.028542229359807,
      "grad_norm": 4.1779866218566895,
      "learning_rate": 3.6642881475533495e-05,
      "loss": 0.6572,
      "step": 1756600
    },
    {
      "epoch": 16.029454704722973,
      "grad_norm": 4.3295674324035645,
      "learning_rate": 3.6642121079397525e-05,
      "loss": 0.6484,
      "step": 1756700
    },
    {
      "epoch": 16.030367180086138,
      "grad_norm": 2.965254783630371,
      "learning_rate": 3.6641360683261555e-05,
      "loss": 0.6867,
      "step": 1756800
    },
    {
      "epoch": 16.031279655449303,
      "grad_norm": 3.3674421310424805,
      "learning_rate": 3.6640600287125585e-05,
      "loss": 0.665,
      "step": 1756900
    },
    {
      "epoch": 16.03219213081247,
      "grad_norm": 3.604839324951172,
      "learning_rate": 3.6639839890989615e-05,
      "loss": 0.6435,
      "step": 1757000
    },
    {
      "epoch": 16.033104606175634,
      "grad_norm": 2.9893553256988525,
      "learning_rate": 3.663907949485364e-05,
      "loss": 0.6209,
      "step": 1757100
    },
    {
      "epoch": 16.0340170815388,
      "grad_norm": 3.08264422416687,
      "learning_rate": 3.6638319098717675e-05,
      "loss": 0.648,
      "step": 1757200
    },
    {
      "epoch": 16.034929556901965,
      "grad_norm": 4.193324565887451,
      "learning_rate": 3.66375587025817e-05,
      "loss": 0.6729,
      "step": 1757300
    },
    {
      "epoch": 16.03584203226513,
      "grad_norm": 4.322494983673096,
      "learning_rate": 3.663679830644573e-05,
      "loss": 0.6717,
      "step": 1757400
    },
    {
      "epoch": 16.036754507628295,
      "grad_norm": 4.183561325073242,
      "learning_rate": 3.663603791030976e-05,
      "loss": 0.6417,
      "step": 1757500
    },
    {
      "epoch": 16.03766698299146,
      "grad_norm": 4.062326431274414,
      "learning_rate": 3.663527751417379e-05,
      "loss": 0.6919,
      "step": 1757600
    },
    {
      "epoch": 16.038579458354626,
      "grad_norm": 3.904641628265381,
      "learning_rate": 3.663451711803781e-05,
      "loss": 0.6421,
      "step": 1757700
    },
    {
      "epoch": 16.03949193371779,
      "grad_norm": 3.7281696796417236,
      "learning_rate": 3.663375672190185e-05,
      "loss": 0.6639,
      "step": 1757800
    },
    {
      "epoch": 16.040404409080956,
      "grad_norm": 4.239629745483398,
      "learning_rate": 3.663299632576587e-05,
      "loss": 0.6683,
      "step": 1757900
    },
    {
      "epoch": 16.04131688444412,
      "grad_norm": 4.417689800262451,
      "learning_rate": 3.66322359296299e-05,
      "loss": 0.6656,
      "step": 1758000
    },
    {
      "epoch": 16.042229359807287,
      "grad_norm": 3.540560007095337,
      "learning_rate": 3.663147553349393e-05,
      "loss": 0.6802,
      "step": 1758100
    },
    {
      "epoch": 16.04314183517045,
      "grad_norm": 5.549252033233643,
      "learning_rate": 3.6630715137357955e-05,
      "loss": 0.6714,
      "step": 1758200
    },
    {
      "epoch": 16.044054310533614,
      "grad_norm": 3.197582721710205,
      "learning_rate": 3.662995474122199e-05,
      "loss": 0.6874,
      "step": 1758300
    },
    {
      "epoch": 16.04496678589678,
      "grad_norm": 3.437218427658081,
      "learning_rate": 3.6629194345086015e-05,
      "loss": 0.699,
      "step": 1758400
    },
    {
      "epoch": 16.045879261259945,
      "grad_norm": 3.9873669147491455,
      "learning_rate": 3.6628433948950045e-05,
      "loss": 0.6373,
      "step": 1758500
    },
    {
      "epoch": 16.04679173662311,
      "grad_norm": 3.7670295238494873,
      "learning_rate": 3.6627673552814076e-05,
      "loss": 0.6506,
      "step": 1758600
    },
    {
      "epoch": 16.047704211986275,
      "grad_norm": 3.3119256496429443,
      "learning_rate": 3.6626913156678106e-05,
      "loss": 0.6293,
      "step": 1758700
    },
    {
      "epoch": 16.04861668734944,
      "grad_norm": 3.4464597702026367,
      "learning_rate": 3.662615276054213e-05,
      "loss": 0.6845,
      "step": 1758800
    },
    {
      "epoch": 16.049529162712606,
      "grad_norm": 4.149773597717285,
      "learning_rate": 3.6625392364406166e-05,
      "loss": 0.6988,
      "step": 1758900
    },
    {
      "epoch": 16.05044163807577,
      "grad_norm": 4.017535209655762,
      "learning_rate": 3.662463196827019e-05,
      "loss": 0.6596,
      "step": 1759000
    },
    {
      "epoch": 16.051354113438936,
      "grad_norm": 2.9397976398468018,
      "learning_rate": 3.662387157213422e-05,
      "loss": 0.656,
      "step": 1759100
    },
    {
      "epoch": 16.0522665888021,
      "grad_norm": 3.3198957443237305,
      "learning_rate": 3.662311117599825e-05,
      "loss": 0.6974,
      "step": 1759200
    },
    {
      "epoch": 16.053179064165267,
      "grad_norm": 2.8408803939819336,
      "learning_rate": 3.662235077986228e-05,
      "loss": 0.6549,
      "step": 1759300
    },
    {
      "epoch": 16.054091539528432,
      "grad_norm": 3.7764368057250977,
      "learning_rate": 3.662159038372631e-05,
      "loss": 0.6398,
      "step": 1759400
    },
    {
      "epoch": 16.055004014891598,
      "grad_norm": 4.055030822753906,
      "learning_rate": 3.662082998759034e-05,
      "loss": 0.6607,
      "step": 1759500
    },
    {
      "epoch": 16.055916490254763,
      "grad_norm": 3.59861421585083,
      "learning_rate": 3.662006959145436e-05,
      "loss": 0.655,
      "step": 1759600
    },
    {
      "epoch": 16.056828965617928,
      "grad_norm": 3.913776159286499,
      "learning_rate": 3.66193091953184e-05,
      "loss": 0.652,
      "step": 1759700
    },
    {
      "epoch": 16.057741440981093,
      "grad_norm": 4.490498065948486,
      "learning_rate": 3.661854879918242e-05,
      "loss": 0.6698,
      "step": 1759800
    },
    {
      "epoch": 16.05865391634426,
      "grad_norm": 3.259983539581299,
      "learning_rate": 3.661778840304645e-05,
      "loss": 0.6397,
      "step": 1759900
    },
    {
      "epoch": 16.059566391707424,
      "grad_norm": 4.293929576873779,
      "learning_rate": 3.661702800691048e-05,
      "loss": 0.6404,
      "step": 1760000
    },
    {
      "epoch": 16.06047886707059,
      "grad_norm": 3.6778838634490967,
      "learning_rate": 3.661626761077451e-05,
      "loss": 0.6538,
      "step": 1760100
    },
    {
      "epoch": 16.061391342433755,
      "grad_norm": 4.169013977050781,
      "learning_rate": 3.6615507214638536e-05,
      "loss": 0.674,
      "step": 1760200
    },
    {
      "epoch": 16.06230381779692,
      "grad_norm": 4.392884731292725,
      "learning_rate": 3.661474681850257e-05,
      "loss": 0.6544,
      "step": 1760300
    },
    {
      "epoch": 16.063216293160085,
      "grad_norm": 4.029214859008789,
      "learning_rate": 3.6613986422366596e-05,
      "loss": 0.6788,
      "step": 1760400
    },
    {
      "epoch": 16.06412876852325,
      "grad_norm": 4.24172306060791,
      "learning_rate": 3.6613226026230626e-05,
      "loss": 0.6777,
      "step": 1760500
    },
    {
      "epoch": 16.065041243886416,
      "grad_norm": 4.666665554046631,
      "learning_rate": 3.6612465630094657e-05,
      "loss": 0.6712,
      "step": 1760600
    },
    {
      "epoch": 16.06595371924958,
      "grad_norm": 3.554060697555542,
      "learning_rate": 3.6611705233958687e-05,
      "loss": 0.658,
      "step": 1760700
    },
    {
      "epoch": 16.066866194612746,
      "grad_norm": 4.084970474243164,
      "learning_rate": 3.661094483782272e-05,
      "loss": 0.6965,
      "step": 1760800
    },
    {
      "epoch": 16.06777866997591,
      "grad_norm": 3.9424197673797607,
      "learning_rate": 3.661018444168674e-05,
      "loss": 0.6779,
      "step": 1760900
    },
    {
      "epoch": 16.068691145339077,
      "grad_norm": 4.205458641052246,
      "learning_rate": 3.660942404555077e-05,
      "loss": 0.7053,
      "step": 1761000
    },
    {
      "epoch": 16.069603620702242,
      "grad_norm": 3.7138724327087402,
      "learning_rate": 3.66086636494148e-05,
      "loss": 0.6289,
      "step": 1761100
    },
    {
      "epoch": 16.070516096065408,
      "grad_norm": 4.070182800292969,
      "learning_rate": 3.660790325327883e-05,
      "loss": 0.6908,
      "step": 1761200
    },
    {
      "epoch": 16.071428571428573,
      "grad_norm": 4.462714672088623,
      "learning_rate": 3.6607142857142853e-05,
      "loss": 0.6707,
      "step": 1761300
    },
    {
      "epoch": 16.07234104679174,
      "grad_norm": 4.10819149017334,
      "learning_rate": 3.660638246100689e-05,
      "loss": 0.6595,
      "step": 1761400
    },
    {
      "epoch": 16.073253522154904,
      "grad_norm": 3.6254615783691406,
      "learning_rate": 3.6605622064870914e-05,
      "loss": 0.6581,
      "step": 1761500
    },
    {
      "epoch": 16.074165997518065,
      "grad_norm": 3.8120110034942627,
      "learning_rate": 3.6604861668734944e-05,
      "loss": 0.6596,
      "step": 1761600
    },
    {
      "epoch": 16.07507847288123,
      "grad_norm": 4.503115653991699,
      "learning_rate": 3.6604101272598974e-05,
      "loss": 0.6693,
      "step": 1761700
    },
    {
      "epoch": 16.075990948244396,
      "grad_norm": 3.921502113342285,
      "learning_rate": 3.6603340876463004e-05,
      "loss": 0.701,
      "step": 1761800
    },
    {
      "epoch": 16.07690342360756,
      "grad_norm": 4.4733076095581055,
      "learning_rate": 3.6602580480327034e-05,
      "loss": 0.645,
      "step": 1761900
    },
    {
      "epoch": 16.077815898970726,
      "grad_norm": 4.649137020111084,
      "learning_rate": 3.6601820084191064e-05,
      "loss": 0.6617,
      "step": 1762000
    },
    {
      "epoch": 16.078728374333892,
      "grad_norm": 4.269093990325928,
      "learning_rate": 3.660105968805509e-05,
      "loss": 0.6461,
      "step": 1762100
    },
    {
      "epoch": 16.079640849697057,
      "grad_norm": 3.6068713665008545,
      "learning_rate": 3.6600299291919124e-05,
      "loss": 0.6631,
      "step": 1762200
    },
    {
      "epoch": 16.080553325060222,
      "grad_norm": 3.653665542602539,
      "learning_rate": 3.659953889578315e-05,
      "loss": 0.6903,
      "step": 1762300
    },
    {
      "epoch": 16.081465800423388,
      "grad_norm": 4.522323131561279,
      "learning_rate": 3.659877849964718e-05,
      "loss": 0.6526,
      "step": 1762400
    },
    {
      "epoch": 16.082378275786553,
      "grad_norm": 3.1884567737579346,
      "learning_rate": 3.659801810351121e-05,
      "loss": 0.6301,
      "step": 1762500
    },
    {
      "epoch": 16.08329075114972,
      "grad_norm": 2.013620376586914,
      "learning_rate": 3.659725770737524e-05,
      "loss": 0.6798,
      "step": 1762600
    },
    {
      "epoch": 16.084203226512884,
      "grad_norm": 3.676931858062744,
      "learning_rate": 3.659649731123926e-05,
      "loss": 0.6887,
      "step": 1762700
    },
    {
      "epoch": 16.08511570187605,
      "grad_norm": 4.036329746246338,
      "learning_rate": 3.65957369151033e-05,
      "loss": 0.6504,
      "step": 1762800
    },
    {
      "epoch": 16.086028177239214,
      "grad_norm": 4.175897598266602,
      "learning_rate": 3.659497651896732e-05,
      "loss": 0.6616,
      "step": 1762900
    },
    {
      "epoch": 16.08694065260238,
      "grad_norm": 4.469559669494629,
      "learning_rate": 3.659421612283135e-05,
      "loss": 0.6599,
      "step": 1763000
    },
    {
      "epoch": 16.087853127965545,
      "grad_norm": 7.8078203201293945,
      "learning_rate": 3.659345572669538e-05,
      "loss": 0.6865,
      "step": 1763100
    },
    {
      "epoch": 16.08876560332871,
      "grad_norm": 4.044609069824219,
      "learning_rate": 3.659269533055941e-05,
      "loss": 0.6624,
      "step": 1763200
    },
    {
      "epoch": 16.089678078691875,
      "grad_norm": 4.081787109375,
      "learning_rate": 3.659193493442344e-05,
      "loss": 0.6579,
      "step": 1763300
    },
    {
      "epoch": 16.09059055405504,
      "grad_norm": 3.6784346103668213,
      "learning_rate": 3.659117453828747e-05,
      "loss": 0.6825,
      "step": 1763400
    },
    {
      "epoch": 16.091503029418206,
      "grad_norm": 2.880269765853882,
      "learning_rate": 3.6590414142151495e-05,
      "loss": 0.6578,
      "step": 1763500
    },
    {
      "epoch": 16.09241550478137,
      "grad_norm": 3.5461506843566895,
      "learning_rate": 3.658965374601553e-05,
      "loss": 0.6471,
      "step": 1763600
    },
    {
      "epoch": 16.093327980144537,
      "grad_norm": 3.6574645042419434,
      "learning_rate": 3.6588893349879555e-05,
      "loss": 0.6293,
      "step": 1763700
    },
    {
      "epoch": 16.094240455507702,
      "grad_norm": 4.134921550750732,
      "learning_rate": 3.658813295374358e-05,
      "loss": 0.6544,
      "step": 1763800
    },
    {
      "epoch": 16.095152930870867,
      "grad_norm": 4.2604522705078125,
      "learning_rate": 3.6587372557607615e-05,
      "loss": 0.6968,
      "step": 1763900
    },
    {
      "epoch": 16.096065406234032,
      "grad_norm": 4.0687456130981445,
      "learning_rate": 3.658661216147164e-05,
      "loss": 0.6471,
      "step": 1764000
    },
    {
      "epoch": 16.096977881597198,
      "grad_norm": 2.582662582397461,
      "learning_rate": 3.658585176533567e-05,
      "loss": 0.6462,
      "step": 1764100
    },
    {
      "epoch": 16.097890356960363,
      "grad_norm": 4.5340657234191895,
      "learning_rate": 3.65850913691997e-05,
      "loss": 0.6875,
      "step": 1764200
    },
    {
      "epoch": 16.09880283232353,
      "grad_norm": 3.8762123584747314,
      "learning_rate": 3.658433097306373e-05,
      "loss": 0.6912,
      "step": 1764300
    },
    {
      "epoch": 16.099715307686694,
      "grad_norm": 3.752807855606079,
      "learning_rate": 3.658357057692776e-05,
      "loss": 0.6813,
      "step": 1764400
    },
    {
      "epoch": 16.10062778304986,
      "grad_norm": 4.525762557983398,
      "learning_rate": 3.658281018079179e-05,
      "loss": 0.6525,
      "step": 1764500
    },
    {
      "epoch": 16.101540258413024,
      "grad_norm": 4.58803653717041,
      "learning_rate": 3.658204978465581e-05,
      "loss": 0.6492,
      "step": 1764600
    },
    {
      "epoch": 16.10245273377619,
      "grad_norm": 3.87009334564209,
      "learning_rate": 3.658128938851985e-05,
      "loss": 0.6675,
      "step": 1764700
    },
    {
      "epoch": 16.103365209139355,
      "grad_norm": 3.844670295715332,
      "learning_rate": 3.658052899238387e-05,
      "loss": 0.6675,
      "step": 1764800
    },
    {
      "epoch": 16.10427768450252,
      "grad_norm": 3.5382211208343506,
      "learning_rate": 3.65797685962479e-05,
      "loss": 0.6726,
      "step": 1764900
    },
    {
      "epoch": 16.105190159865682,
      "grad_norm": 3.7029013633728027,
      "learning_rate": 3.657900820011193e-05,
      "loss": 0.6708,
      "step": 1765000
    },
    {
      "epoch": 16.106102635228847,
      "grad_norm": 3.055549144744873,
      "learning_rate": 3.657824780397596e-05,
      "loss": 0.6626,
      "step": 1765100
    },
    {
      "epoch": 16.107015110592013,
      "grad_norm": 4.638087749481201,
      "learning_rate": 3.6577487407839985e-05,
      "loss": 0.6493,
      "step": 1765200
    },
    {
      "epoch": 16.107927585955178,
      "grad_norm": 4.140711307525635,
      "learning_rate": 3.657672701170402e-05,
      "loss": 0.6432,
      "step": 1765300
    },
    {
      "epoch": 16.108840061318343,
      "grad_norm": 3.0776050090789795,
      "learning_rate": 3.6575966615568046e-05,
      "loss": 0.6416,
      "step": 1765400
    },
    {
      "epoch": 16.10975253668151,
      "grad_norm": 4.344098091125488,
      "learning_rate": 3.6575206219432076e-05,
      "loss": 0.6745,
      "step": 1765500
    },
    {
      "epoch": 16.110665012044674,
      "grad_norm": 3.353635311126709,
      "learning_rate": 3.6574445823296106e-05,
      "loss": 0.6799,
      "step": 1765600
    },
    {
      "epoch": 16.11157748740784,
      "grad_norm": 3.8618900775909424,
      "learning_rate": 3.6573685427160136e-05,
      "loss": 0.6443,
      "step": 1765700
    },
    {
      "epoch": 16.112489962771004,
      "grad_norm": 3.359105348587036,
      "learning_rate": 3.6572925031024166e-05,
      "loss": 0.668,
      "step": 1765800
    },
    {
      "epoch": 16.11340243813417,
      "grad_norm": 4.219179630279541,
      "learning_rate": 3.6572164634888196e-05,
      "loss": 0.7002,
      "step": 1765900
    },
    {
      "epoch": 16.114314913497335,
      "grad_norm": 4.503678798675537,
      "learning_rate": 3.657140423875222e-05,
      "loss": 0.6618,
      "step": 1766000
    },
    {
      "epoch": 16.1152273888605,
      "grad_norm": 4.4214558601379395,
      "learning_rate": 3.6570643842616256e-05,
      "loss": 0.6748,
      "step": 1766100
    },
    {
      "epoch": 16.116139864223666,
      "grad_norm": 4.0710673332214355,
      "learning_rate": 3.656988344648028e-05,
      "loss": 0.672,
      "step": 1766200
    },
    {
      "epoch": 16.11705233958683,
      "grad_norm": 4.3720502853393555,
      "learning_rate": 3.656912305034431e-05,
      "loss": 0.6639,
      "step": 1766300
    },
    {
      "epoch": 16.117964814949996,
      "grad_norm": 4.490848541259766,
      "learning_rate": 3.656836265420834e-05,
      "loss": 0.6859,
      "step": 1766400
    },
    {
      "epoch": 16.11887729031316,
      "grad_norm": 3.8160462379455566,
      "learning_rate": 3.656760225807237e-05,
      "loss": 0.6843,
      "step": 1766500
    },
    {
      "epoch": 16.119789765676327,
      "grad_norm": 3.7107322216033936,
      "learning_rate": 3.656684186193639e-05,
      "loss": 0.6234,
      "step": 1766600
    },
    {
      "epoch": 16.120702241039492,
      "grad_norm": 4.013879776000977,
      "learning_rate": 3.656608146580042e-05,
      "loss": 0.6369,
      "step": 1766700
    },
    {
      "epoch": 16.121614716402657,
      "grad_norm": 4.7744035720825195,
      "learning_rate": 3.656532106966445e-05,
      "loss": 0.6829,
      "step": 1766800
    },
    {
      "epoch": 16.122527191765823,
      "grad_norm": 4.916916847229004,
      "learning_rate": 3.656456067352848e-05,
      "loss": 0.668,
      "step": 1766900
    },
    {
      "epoch": 16.123439667128988,
      "grad_norm": 4.366994380950928,
      "learning_rate": 3.656380027739251e-05,
      "loss": 0.6706,
      "step": 1767000
    },
    {
      "epoch": 16.124352142492153,
      "grad_norm": 4.24973726272583,
      "learning_rate": 3.6563039881256536e-05,
      "loss": 0.6725,
      "step": 1767100
    },
    {
      "epoch": 16.12526461785532,
      "grad_norm": 3.8877434730529785,
      "learning_rate": 3.656227948512057e-05,
      "loss": 0.6944,
      "step": 1767200
    },
    {
      "epoch": 16.126177093218484,
      "grad_norm": 4.025254249572754,
      "learning_rate": 3.6561519088984596e-05,
      "loss": 0.6511,
      "step": 1767300
    },
    {
      "epoch": 16.12708956858165,
      "grad_norm": 3.7341504096984863,
      "learning_rate": 3.6560758692848627e-05,
      "loss": 0.7019,
      "step": 1767400
    },
    {
      "epoch": 16.128002043944814,
      "grad_norm": 3.9173104763031006,
      "learning_rate": 3.655999829671266e-05,
      "loss": 0.6364,
      "step": 1767500
    },
    {
      "epoch": 16.12891451930798,
      "grad_norm": 4.009676456451416,
      "learning_rate": 3.655923790057669e-05,
      "loss": 0.6812,
      "step": 1767600
    },
    {
      "epoch": 16.129826994671145,
      "grad_norm": 3.937366247177124,
      "learning_rate": 3.655847750444072e-05,
      "loss": 0.6613,
      "step": 1767700
    },
    {
      "epoch": 16.13073947003431,
      "grad_norm": 4.348917007446289,
      "learning_rate": 3.655771710830475e-05,
      "loss": 0.6696,
      "step": 1767800
    },
    {
      "epoch": 16.131651945397476,
      "grad_norm": 4.194710731506348,
      "learning_rate": 3.655695671216877e-05,
      "loss": 0.7038,
      "step": 1767900
    },
    {
      "epoch": 16.13256442076064,
      "grad_norm": 4.747208118438721,
      "learning_rate": 3.655619631603281e-05,
      "loss": 0.6741,
      "step": 1768000
    },
    {
      "epoch": 16.133476896123806,
      "grad_norm": 3.8060195446014404,
      "learning_rate": 3.655543591989683e-05,
      "loss": 0.666,
      "step": 1768100
    },
    {
      "epoch": 16.13438937148697,
      "grad_norm": 3.681497812271118,
      "learning_rate": 3.655467552376086e-05,
      "loss": 0.6938,
      "step": 1768200
    },
    {
      "epoch": 16.135301846850137,
      "grad_norm": 5.132782936096191,
      "learning_rate": 3.655391512762489e-05,
      "loss": 0.6811,
      "step": 1768300
    },
    {
      "epoch": 16.1362143222133,
      "grad_norm": 3.853727102279663,
      "learning_rate": 3.655315473148892e-05,
      "loss": 0.7019,
      "step": 1768400
    },
    {
      "epoch": 16.137126797576464,
      "grad_norm": 4.320616245269775,
      "learning_rate": 3.6552394335352944e-05,
      "loss": 0.64,
      "step": 1768500
    },
    {
      "epoch": 16.13803927293963,
      "grad_norm": 2.6905901432037354,
      "learning_rate": 3.655163393921698e-05,
      "loss": 0.6819,
      "step": 1768600
    },
    {
      "epoch": 16.138951748302794,
      "grad_norm": 3.391237497329712,
      "learning_rate": 3.6550873543081004e-05,
      "loss": 0.6676,
      "step": 1768700
    },
    {
      "epoch": 16.13986422366596,
      "grad_norm": 4.646610736846924,
      "learning_rate": 3.6550113146945034e-05,
      "loss": 0.7004,
      "step": 1768800
    },
    {
      "epoch": 16.140776699029125,
      "grad_norm": 3.668452024459839,
      "learning_rate": 3.6549352750809064e-05,
      "loss": 0.6745,
      "step": 1768900
    },
    {
      "epoch": 16.14168917439229,
      "grad_norm": 3.828563690185547,
      "learning_rate": 3.6548592354673094e-05,
      "loss": 0.6623,
      "step": 1769000
    },
    {
      "epoch": 16.142601649755456,
      "grad_norm": 4.362573146820068,
      "learning_rate": 3.6547831958537124e-05,
      "loss": 0.669,
      "step": 1769100
    },
    {
      "epoch": 16.14351412511862,
      "grad_norm": 3.827763319015503,
      "learning_rate": 3.6547071562401154e-05,
      "loss": 0.6542,
      "step": 1769200
    },
    {
      "epoch": 16.144426600481786,
      "grad_norm": 4.582787990570068,
      "learning_rate": 3.654631116626518e-05,
      "loss": 0.6713,
      "step": 1769300
    },
    {
      "epoch": 16.14533907584495,
      "grad_norm": 3.349604606628418,
      "learning_rate": 3.6545550770129214e-05,
      "loss": 0.6726,
      "step": 1769400
    },
    {
      "epoch": 16.146251551208117,
      "grad_norm": 4.033316135406494,
      "learning_rate": 3.654479037399324e-05,
      "loss": 0.6944,
      "step": 1769500
    },
    {
      "epoch": 16.147164026571282,
      "grad_norm": 3.955096960067749,
      "learning_rate": 3.654402997785726e-05,
      "loss": 0.6624,
      "step": 1769600
    },
    {
      "epoch": 16.148076501934447,
      "grad_norm": 3.599971294403076,
      "learning_rate": 3.65432695817213e-05,
      "loss": 0.6974,
      "step": 1769700
    },
    {
      "epoch": 16.148988977297613,
      "grad_norm": 4.107842445373535,
      "learning_rate": 3.654250918558532e-05,
      "loss": 0.6606,
      "step": 1769800
    },
    {
      "epoch": 16.149901452660778,
      "grad_norm": 4.134200096130371,
      "learning_rate": 3.654174878944935e-05,
      "loss": 0.6923,
      "step": 1769900
    },
    {
      "epoch": 16.150813928023943,
      "grad_norm": 3.0336968898773193,
      "learning_rate": 3.654098839331338e-05,
      "loss": 0.6525,
      "step": 1770000
    },
    {
      "epoch": 16.15172640338711,
      "grad_norm": 4.065223217010498,
      "learning_rate": 3.654022799717741e-05,
      "loss": 0.6293,
      "step": 1770100
    },
    {
      "epoch": 16.152638878750274,
      "grad_norm": 3.7793972492218018,
      "learning_rate": 3.653946760104144e-05,
      "loss": 0.6518,
      "step": 1770200
    },
    {
      "epoch": 16.15355135411344,
      "grad_norm": 3.7213354110717773,
      "learning_rate": 3.653870720490547e-05,
      "loss": 0.7102,
      "step": 1770300
    },
    {
      "epoch": 16.154463829476605,
      "grad_norm": 3.723686933517456,
      "learning_rate": 3.6537946808769495e-05,
      "loss": 0.6976,
      "step": 1770400
    },
    {
      "epoch": 16.15537630483977,
      "grad_norm": 4.120838165283203,
      "learning_rate": 3.653718641263353e-05,
      "loss": 0.6545,
      "step": 1770500
    },
    {
      "epoch": 16.156288780202935,
      "grad_norm": 2.9466536045074463,
      "learning_rate": 3.6536426016497555e-05,
      "loss": 0.6531,
      "step": 1770600
    },
    {
      "epoch": 16.1572012555661,
      "grad_norm": 4.638083457946777,
      "learning_rate": 3.6535665620361585e-05,
      "loss": 0.6284,
      "step": 1770700
    },
    {
      "epoch": 16.158113730929266,
      "grad_norm": 3.803194761276245,
      "learning_rate": 3.6534905224225615e-05,
      "loss": 0.6507,
      "step": 1770800
    },
    {
      "epoch": 16.15902620629243,
      "grad_norm": 4.724917888641357,
      "learning_rate": 3.6534144828089645e-05,
      "loss": 0.6354,
      "step": 1770900
    },
    {
      "epoch": 16.159938681655596,
      "grad_norm": 4.18172550201416,
      "learning_rate": 3.653338443195367e-05,
      "loss": 0.6337,
      "step": 1771000
    },
    {
      "epoch": 16.16085115701876,
      "grad_norm": 3.3957059383392334,
      "learning_rate": 3.6532624035817705e-05,
      "loss": 0.6682,
      "step": 1771100
    },
    {
      "epoch": 16.161763632381927,
      "grad_norm": 4.225831985473633,
      "learning_rate": 3.653186363968173e-05,
      "loss": 0.6592,
      "step": 1771200
    },
    {
      "epoch": 16.162676107745092,
      "grad_norm": 3.9093563556671143,
      "learning_rate": 3.653110324354576e-05,
      "loss": 0.6558,
      "step": 1771300
    },
    {
      "epoch": 16.163588583108258,
      "grad_norm": 3.411975145339966,
      "learning_rate": 3.653034284740979e-05,
      "loss": 0.658,
      "step": 1771400
    },
    {
      "epoch": 16.164501058471423,
      "grad_norm": 3.8838677406311035,
      "learning_rate": 3.652958245127382e-05,
      "loss": 0.6387,
      "step": 1771500
    },
    {
      "epoch": 16.165413533834588,
      "grad_norm": 4.0360283851623535,
      "learning_rate": 3.652882205513785e-05,
      "loss": 0.6595,
      "step": 1771600
    },
    {
      "epoch": 16.166326009197753,
      "grad_norm": 4.332456588745117,
      "learning_rate": 3.652806165900188e-05,
      "loss": 0.603,
      "step": 1771700
    },
    {
      "epoch": 16.167238484560915,
      "grad_norm": 3.2473111152648926,
      "learning_rate": 3.65273012628659e-05,
      "loss": 0.6694,
      "step": 1771800
    },
    {
      "epoch": 16.16815095992408,
      "grad_norm": 4.405287265777588,
      "learning_rate": 3.652654086672994e-05,
      "loss": 0.6449,
      "step": 1771900
    },
    {
      "epoch": 16.169063435287246,
      "grad_norm": 4.3804450035095215,
      "learning_rate": 3.652578047059396e-05,
      "loss": 0.6743,
      "step": 1772000
    },
    {
      "epoch": 16.16997591065041,
      "grad_norm": 4.104443550109863,
      "learning_rate": 3.652502007445799e-05,
      "loss": 0.7038,
      "step": 1772100
    },
    {
      "epoch": 16.170888386013576,
      "grad_norm": 4.133163928985596,
      "learning_rate": 3.652425967832202e-05,
      "loss": 0.619,
      "step": 1772200
    },
    {
      "epoch": 16.17180086137674,
      "grad_norm": 4.514994144439697,
      "learning_rate": 3.6523499282186046e-05,
      "loss": 0.6673,
      "step": 1772300
    },
    {
      "epoch": 16.172713336739907,
      "grad_norm": 4.916492938995361,
      "learning_rate": 3.6522738886050076e-05,
      "loss": 0.6235,
      "step": 1772400
    },
    {
      "epoch": 16.173625812103072,
      "grad_norm": 3.9445877075195312,
      "learning_rate": 3.6521978489914106e-05,
      "loss": 0.6373,
      "step": 1772500
    },
    {
      "epoch": 16.174538287466238,
      "grad_norm": 3.4959919452667236,
      "learning_rate": 3.6521218093778136e-05,
      "loss": 0.6803,
      "step": 1772600
    },
    {
      "epoch": 16.175450762829403,
      "grad_norm": 4.365148067474365,
      "learning_rate": 3.6520457697642166e-05,
      "loss": 0.6591,
      "step": 1772700
    },
    {
      "epoch": 16.176363238192568,
      "grad_norm": 4.293030738830566,
      "learning_rate": 3.6519697301506196e-05,
      "loss": 0.652,
      "step": 1772800
    },
    {
      "epoch": 16.177275713555733,
      "grad_norm": 4.458066940307617,
      "learning_rate": 3.651893690537022e-05,
      "loss": 0.6183,
      "step": 1772900
    },
    {
      "epoch": 16.1781881889189,
      "grad_norm": 5.867793560028076,
      "learning_rate": 3.6518176509234256e-05,
      "loss": 0.6621,
      "step": 1773000
    },
    {
      "epoch": 16.179100664282064,
      "grad_norm": 4.947377681732178,
      "learning_rate": 3.651741611309828e-05,
      "loss": 0.6351,
      "step": 1773100
    },
    {
      "epoch": 16.18001313964523,
      "grad_norm": 4.871752738952637,
      "learning_rate": 3.651665571696231e-05,
      "loss": 0.6598,
      "step": 1773200
    },
    {
      "epoch": 16.180925615008395,
      "grad_norm": 4.658733367919922,
      "learning_rate": 3.651589532082634e-05,
      "loss": 0.6829,
      "step": 1773300
    },
    {
      "epoch": 16.18183809037156,
      "grad_norm": 3.607475519180298,
      "learning_rate": 3.651513492469037e-05,
      "loss": 0.6709,
      "step": 1773400
    },
    {
      "epoch": 16.182750565734725,
      "grad_norm": 4.085628986358643,
      "learning_rate": 3.651437452855439e-05,
      "loss": 0.647,
      "step": 1773500
    },
    {
      "epoch": 16.18366304109789,
      "grad_norm": 2.5805904865264893,
      "learning_rate": 3.651361413241843e-05,
      "loss": 0.6761,
      "step": 1773600
    },
    {
      "epoch": 16.184575516461056,
      "grad_norm": 4.831033229827881,
      "learning_rate": 3.651285373628245e-05,
      "loss": 0.6492,
      "step": 1773700
    },
    {
      "epoch": 16.18548799182422,
      "grad_norm": 4.206254482269287,
      "learning_rate": 3.651209334014648e-05,
      "loss": 0.6721,
      "step": 1773800
    },
    {
      "epoch": 16.186400467187386,
      "grad_norm": 4.068216800689697,
      "learning_rate": 3.651133294401051e-05,
      "loss": 0.6194,
      "step": 1773900
    },
    {
      "epoch": 16.18731294255055,
      "grad_norm": 3.966560125350952,
      "learning_rate": 3.651057254787454e-05,
      "loss": 0.621,
      "step": 1774000
    },
    {
      "epoch": 16.188225417913717,
      "grad_norm": 4.326894760131836,
      "learning_rate": 3.650981215173857e-05,
      "loss": 0.7027,
      "step": 1774100
    },
    {
      "epoch": 16.189137893276882,
      "grad_norm": 4.253261566162109,
      "learning_rate": 3.65090517556026e-05,
      "loss": 0.6518,
      "step": 1774200
    },
    {
      "epoch": 16.190050368640048,
      "grad_norm": 2.1577491760253906,
      "learning_rate": 3.650829135946663e-05,
      "loss": 0.6641,
      "step": 1774300
    },
    {
      "epoch": 16.190962844003213,
      "grad_norm": 3.364976167678833,
      "learning_rate": 3.6507530963330664e-05,
      "loss": 0.6714,
      "step": 1774400
    },
    {
      "epoch": 16.19187531936638,
      "grad_norm": 4.395696640014648,
      "learning_rate": 3.650677056719469e-05,
      "loss": 0.6558,
      "step": 1774500
    },
    {
      "epoch": 16.192787794729544,
      "grad_norm": 3.8693103790283203,
      "learning_rate": 3.650601017105872e-05,
      "loss": 0.6643,
      "step": 1774600
    },
    {
      "epoch": 16.19370027009271,
      "grad_norm": 2.209012269973755,
      "learning_rate": 3.650524977492275e-05,
      "loss": 0.6658,
      "step": 1774700
    },
    {
      "epoch": 16.194612745455874,
      "grad_norm": 4.057894229888916,
      "learning_rate": 3.650448937878678e-05,
      "loss": 0.6659,
      "step": 1774800
    },
    {
      "epoch": 16.19552522081904,
      "grad_norm": 4.045573711395264,
      "learning_rate": 3.65037289826508e-05,
      "loss": 0.7041,
      "step": 1774900
    },
    {
      "epoch": 16.196437696182205,
      "grad_norm": 4.089150428771973,
      "learning_rate": 3.650296858651484e-05,
      "loss": 0.6695,
      "step": 1775000
    },
    {
      "epoch": 16.197350171545366,
      "grad_norm": 3.8805389404296875,
      "learning_rate": 3.650220819037886e-05,
      "loss": 0.6332,
      "step": 1775100
    },
    {
      "epoch": 16.198262646908532,
      "grad_norm": 3.3674862384796143,
      "learning_rate": 3.650144779424289e-05,
      "loss": 0.7005,
      "step": 1775200
    },
    {
      "epoch": 16.199175122271697,
      "grad_norm": 4.429901599884033,
      "learning_rate": 3.650068739810692e-05,
      "loss": 0.6454,
      "step": 1775300
    },
    {
      "epoch": 16.200087597634862,
      "grad_norm": 4.19981050491333,
      "learning_rate": 3.6499927001970944e-05,
      "loss": 0.6325,
      "step": 1775400
    },
    {
      "epoch": 16.201000072998028,
      "grad_norm": 4.301180839538574,
      "learning_rate": 3.649916660583498e-05,
      "loss": 0.6756,
      "step": 1775500
    },
    {
      "epoch": 16.201912548361193,
      "grad_norm": 4.2069621086120605,
      "learning_rate": 3.6498406209699004e-05,
      "loss": 0.6507,
      "step": 1775600
    },
    {
      "epoch": 16.20282502372436,
      "grad_norm": 4.91458797454834,
      "learning_rate": 3.6497645813563034e-05,
      "loss": 0.6623,
      "step": 1775700
    },
    {
      "epoch": 16.203737499087524,
      "grad_norm": 4.044865131378174,
      "learning_rate": 3.6496885417427064e-05,
      "loss": 0.7025,
      "step": 1775800
    },
    {
      "epoch": 16.20464997445069,
      "grad_norm": 3.4853296279907227,
      "learning_rate": 3.6496125021291094e-05,
      "loss": 0.6415,
      "step": 1775900
    },
    {
      "epoch": 16.205562449813854,
      "grad_norm": 3.4232139587402344,
      "learning_rate": 3.649536462515512e-05,
      "loss": 0.6885,
      "step": 1776000
    },
    {
      "epoch": 16.20647492517702,
      "grad_norm": 4.009023666381836,
      "learning_rate": 3.6494604229019154e-05,
      "loss": 0.6577,
      "step": 1776100
    },
    {
      "epoch": 16.207387400540185,
      "grad_norm": 4.056875228881836,
      "learning_rate": 3.649384383288318e-05,
      "loss": 0.6697,
      "step": 1776200
    },
    {
      "epoch": 16.20829987590335,
      "grad_norm": 4.588807582855225,
      "learning_rate": 3.649308343674721e-05,
      "loss": 0.6406,
      "step": 1776300
    },
    {
      "epoch": 16.209212351266515,
      "grad_norm": 4.134195804595947,
      "learning_rate": 3.649232304061124e-05,
      "loss": 0.6319,
      "step": 1776400
    },
    {
      "epoch": 16.21012482662968,
      "grad_norm": 2.461242437362671,
      "learning_rate": 3.649156264447527e-05,
      "loss": 0.6398,
      "step": 1776500
    },
    {
      "epoch": 16.211037301992846,
      "grad_norm": 3.487433671951294,
      "learning_rate": 3.64908022483393e-05,
      "loss": 0.6962,
      "step": 1776600
    },
    {
      "epoch": 16.21194977735601,
      "grad_norm": 4.280880451202393,
      "learning_rate": 3.649004185220333e-05,
      "loss": 0.6555,
      "step": 1776700
    },
    {
      "epoch": 16.212862252719177,
      "grad_norm": 4.375243186950684,
      "learning_rate": 3.648928145606735e-05,
      "loss": 0.6982,
      "step": 1776800
    },
    {
      "epoch": 16.213774728082342,
      "grad_norm": 4.179991722106934,
      "learning_rate": 3.648852105993139e-05,
      "loss": 0.6601,
      "step": 1776900
    },
    {
      "epoch": 16.214687203445507,
      "grad_norm": 3.5953574180603027,
      "learning_rate": 3.648776066379541e-05,
      "loss": 0.6493,
      "step": 1777000
    },
    {
      "epoch": 16.215599678808672,
      "grad_norm": 2.92468523979187,
      "learning_rate": 3.648700026765944e-05,
      "loss": 0.6621,
      "step": 1777100
    },
    {
      "epoch": 16.216512154171838,
      "grad_norm": 4.700474739074707,
      "learning_rate": 3.648623987152347e-05,
      "loss": 0.6838,
      "step": 1777200
    },
    {
      "epoch": 16.217424629535003,
      "grad_norm": 4.039182662963867,
      "learning_rate": 3.64854794753875e-05,
      "loss": 0.6545,
      "step": 1777300
    },
    {
      "epoch": 16.21833710489817,
      "grad_norm": 5.039116859436035,
      "learning_rate": 3.6484719079251525e-05,
      "loss": 0.6646,
      "step": 1777400
    },
    {
      "epoch": 16.219249580261334,
      "grad_norm": 4.303893089294434,
      "learning_rate": 3.648395868311556e-05,
      "loss": 0.701,
      "step": 1777500
    },
    {
      "epoch": 16.2201620556245,
      "grad_norm": 3.6894750595092773,
      "learning_rate": 3.6483198286979585e-05,
      "loss": 0.6796,
      "step": 1777600
    },
    {
      "epoch": 16.221074530987664,
      "grad_norm": 3.6027138233184814,
      "learning_rate": 3.6482437890843615e-05,
      "loss": 0.6571,
      "step": 1777700
    },
    {
      "epoch": 16.22198700635083,
      "grad_norm": 2.9916465282440186,
      "learning_rate": 3.6481677494707645e-05,
      "loss": 0.6375,
      "step": 1777800
    },
    {
      "epoch": 16.222899481713995,
      "grad_norm": 3.851750612258911,
      "learning_rate": 3.6480917098571675e-05,
      "loss": 0.6555,
      "step": 1777900
    },
    {
      "epoch": 16.22381195707716,
      "grad_norm": 4.287527561187744,
      "learning_rate": 3.6480156702435705e-05,
      "loss": 0.6819,
      "step": 1778000
    },
    {
      "epoch": 16.224724432440325,
      "grad_norm": 4.52862548828125,
      "learning_rate": 3.647939630629973e-05,
      "loss": 0.6445,
      "step": 1778100
    },
    {
      "epoch": 16.22563690780349,
      "grad_norm": 3.847280979156494,
      "learning_rate": 3.647863591016376e-05,
      "loss": 0.6782,
      "step": 1778200
    },
    {
      "epoch": 16.226549383166656,
      "grad_norm": 4.488927841186523,
      "learning_rate": 3.647787551402779e-05,
      "loss": 0.6937,
      "step": 1778300
    },
    {
      "epoch": 16.22746185852982,
      "grad_norm": 4.853216648101807,
      "learning_rate": 3.647711511789182e-05,
      "loss": 0.659,
      "step": 1778400
    },
    {
      "epoch": 16.228374333892987,
      "grad_norm": 3.8364999294281006,
      "learning_rate": 3.647635472175585e-05,
      "loss": 0.6615,
      "step": 1778500
    },
    {
      "epoch": 16.22928680925615,
      "grad_norm": 5.323628902435303,
      "learning_rate": 3.647559432561988e-05,
      "loss": 0.6974,
      "step": 1778600
    },
    {
      "epoch": 16.230199284619314,
      "grad_norm": 3.8071892261505127,
      "learning_rate": 3.64748339294839e-05,
      "loss": 0.6653,
      "step": 1778700
    },
    {
      "epoch": 16.23111175998248,
      "grad_norm": 4.021655559539795,
      "learning_rate": 3.647407353334793e-05,
      "loss": 0.6431,
      "step": 1778800
    },
    {
      "epoch": 16.232024235345644,
      "grad_norm": 4.19157600402832,
      "learning_rate": 3.647331313721196e-05,
      "loss": 0.6576,
      "step": 1778900
    },
    {
      "epoch": 16.23293671070881,
      "grad_norm": 5.076266765594482,
      "learning_rate": 3.647255274107599e-05,
      "loss": 0.6481,
      "step": 1779000
    },
    {
      "epoch": 16.233849186071975,
      "grad_norm": 3.620100259780884,
      "learning_rate": 3.647179234494002e-05,
      "loss": 0.63,
      "step": 1779100
    },
    {
      "epoch": 16.23476166143514,
      "grad_norm": 2.8989131450653076,
      "learning_rate": 3.647103194880405e-05,
      "loss": 0.6351,
      "step": 1779200
    },
    {
      "epoch": 16.235674136798306,
      "grad_norm": 4.06798791885376,
      "learning_rate": 3.6470271552668076e-05,
      "loss": 0.653,
      "step": 1779300
    },
    {
      "epoch": 16.23658661216147,
      "grad_norm": 3.6799800395965576,
      "learning_rate": 3.646951115653211e-05,
      "loss": 0.6859,
      "step": 1779400
    },
    {
      "epoch": 16.237499087524636,
      "grad_norm": 3.5759799480438232,
      "learning_rate": 3.6468750760396136e-05,
      "loss": 0.6638,
      "step": 1779500
    },
    {
      "epoch": 16.2384115628878,
      "grad_norm": 2.8826513290405273,
      "learning_rate": 3.6467990364260166e-05,
      "loss": 0.6952,
      "step": 1779600
    },
    {
      "epoch": 16.239324038250967,
      "grad_norm": 4.286062240600586,
      "learning_rate": 3.6467229968124196e-05,
      "loss": 0.6642,
      "step": 1779700
    },
    {
      "epoch": 16.240236513614132,
      "grad_norm": 3.6496379375457764,
      "learning_rate": 3.6466469571988226e-05,
      "loss": 0.6738,
      "step": 1779800
    },
    {
      "epoch": 16.241148988977297,
      "grad_norm": 4.937335968017578,
      "learning_rate": 3.6465709175852256e-05,
      "loss": 0.6499,
      "step": 1779900
    },
    {
      "epoch": 16.242061464340463,
      "grad_norm": 4.384634971618652,
      "learning_rate": 3.6464948779716286e-05,
      "loss": 0.6774,
      "step": 1780000
    },
    {
      "epoch": 16.242973939703628,
      "grad_norm": 4.235093593597412,
      "learning_rate": 3.646418838358031e-05,
      "loss": 0.6678,
      "step": 1780100
    },
    {
      "epoch": 16.243886415066793,
      "grad_norm": 3.7518911361694336,
      "learning_rate": 3.646342798744434e-05,
      "loss": 0.6906,
      "step": 1780200
    },
    {
      "epoch": 16.24479889042996,
      "grad_norm": 4.119274616241455,
      "learning_rate": 3.646266759130837e-05,
      "loss": 0.6338,
      "step": 1780300
    },
    {
      "epoch": 16.245711365793124,
      "grad_norm": 3.5139033794403076,
      "learning_rate": 3.64619071951724e-05,
      "loss": 0.6448,
      "step": 1780400
    },
    {
      "epoch": 16.24662384115629,
      "grad_norm": 3.3568155765533447,
      "learning_rate": 3.646114679903643e-05,
      "loss": 0.6733,
      "step": 1780500
    },
    {
      "epoch": 16.247536316519454,
      "grad_norm": 4.2195587158203125,
      "learning_rate": 3.646038640290046e-05,
      "loss": 0.6138,
      "step": 1780600
    },
    {
      "epoch": 16.24844879188262,
      "grad_norm": 3.4999969005584717,
      "learning_rate": 3.645962600676448e-05,
      "loss": 0.6651,
      "step": 1780700
    },
    {
      "epoch": 16.249361267245785,
      "grad_norm": 3.6046807765960693,
      "learning_rate": 3.645886561062851e-05,
      "loss": 0.6906,
      "step": 1780800
    },
    {
      "epoch": 16.25027374260895,
      "grad_norm": 2.892760992050171,
      "learning_rate": 3.645810521449254e-05,
      "loss": 0.6748,
      "step": 1780900
    },
    {
      "epoch": 16.251186217972116,
      "grad_norm": 4.096234321594238,
      "learning_rate": 3.6457344818356573e-05,
      "loss": 0.6899,
      "step": 1781000
    },
    {
      "epoch": 16.25209869333528,
      "grad_norm": 3.181413173675537,
      "learning_rate": 3.6456584422220603e-05,
      "loss": 0.6893,
      "step": 1781100
    },
    {
      "epoch": 16.253011168698446,
      "grad_norm": 3.235055685043335,
      "learning_rate": 3.645582402608463e-05,
      "loss": 0.6671,
      "step": 1781200
    },
    {
      "epoch": 16.25392364406161,
      "grad_norm": 4.729072093963623,
      "learning_rate": 3.6455063629948664e-05,
      "loss": 0.6556,
      "step": 1781300
    },
    {
      "epoch": 16.254836119424777,
      "grad_norm": 3.577855348587036,
      "learning_rate": 3.645430323381269e-05,
      "loss": 0.6961,
      "step": 1781400
    },
    {
      "epoch": 16.255748594787942,
      "grad_norm": 3.5930490493774414,
      "learning_rate": 3.645354283767672e-05,
      "loss": 0.6891,
      "step": 1781500
    },
    {
      "epoch": 16.256661070151107,
      "grad_norm": 3.4783923625946045,
      "learning_rate": 3.645278244154075e-05,
      "loss": 0.6736,
      "step": 1781600
    },
    {
      "epoch": 16.257573545514273,
      "grad_norm": 4.659090518951416,
      "learning_rate": 3.645202204540478e-05,
      "loss": 0.6782,
      "step": 1781700
    },
    {
      "epoch": 16.258486020877438,
      "grad_norm": 3.7467687129974365,
      "learning_rate": 3.64512616492688e-05,
      "loss": 0.6498,
      "step": 1781800
    },
    {
      "epoch": 16.2593984962406,
      "grad_norm": 4.976726531982422,
      "learning_rate": 3.645050125313284e-05,
      "loss": 0.7124,
      "step": 1781900
    },
    {
      "epoch": 16.260310971603765,
      "grad_norm": 3.3957724571228027,
      "learning_rate": 3.644974085699686e-05,
      "loss": 0.6683,
      "step": 1782000
    },
    {
      "epoch": 16.26122344696693,
      "grad_norm": 3.6688709259033203,
      "learning_rate": 3.644898046086089e-05,
      "loss": 0.682,
      "step": 1782100
    },
    {
      "epoch": 16.262135922330096,
      "grad_norm": 4.951780796051025,
      "learning_rate": 3.644822006472492e-05,
      "loss": 0.6276,
      "step": 1782200
    },
    {
      "epoch": 16.26304839769326,
      "grad_norm": 3.9981625080108643,
      "learning_rate": 3.644745966858895e-05,
      "loss": 0.6873,
      "step": 1782300
    },
    {
      "epoch": 16.263960873056426,
      "grad_norm": 3.653132200241089,
      "learning_rate": 3.644669927245298e-05,
      "loss": 0.6464,
      "step": 1782400
    },
    {
      "epoch": 16.26487334841959,
      "grad_norm": 4.691114902496338,
      "learning_rate": 3.644593887631701e-05,
      "loss": 0.6894,
      "step": 1782500
    },
    {
      "epoch": 16.265785823782757,
      "grad_norm": 4.233917713165283,
      "learning_rate": 3.6445178480181034e-05,
      "loss": 0.662,
      "step": 1782600
    },
    {
      "epoch": 16.266698299145922,
      "grad_norm": 4.588338375091553,
      "learning_rate": 3.644441808404507e-05,
      "loss": 0.7077,
      "step": 1782700
    },
    {
      "epoch": 16.267610774509087,
      "grad_norm": 3.2684743404388428,
      "learning_rate": 3.6443657687909094e-05,
      "loss": 0.6653,
      "step": 1782800
    },
    {
      "epoch": 16.268523249872253,
      "grad_norm": 3.959129810333252,
      "learning_rate": 3.6442897291773124e-05,
      "loss": 0.6246,
      "step": 1782900
    },
    {
      "epoch": 16.269435725235418,
      "grad_norm": 2.9270968437194824,
      "learning_rate": 3.6442136895637154e-05,
      "loss": 0.6773,
      "step": 1783000
    },
    {
      "epoch": 16.270348200598583,
      "grad_norm": 4.821419715881348,
      "learning_rate": 3.6441376499501184e-05,
      "loss": 0.6472,
      "step": 1783100
    },
    {
      "epoch": 16.27126067596175,
      "grad_norm": 3.4203481674194336,
      "learning_rate": 3.644061610336521e-05,
      "loss": 0.6337,
      "step": 1783200
    },
    {
      "epoch": 16.272173151324914,
      "grad_norm": 3.6098456382751465,
      "learning_rate": 3.6439855707229245e-05,
      "loss": 0.639,
      "step": 1783300
    },
    {
      "epoch": 16.27308562668808,
      "grad_norm": 4.0966386795043945,
      "learning_rate": 3.643909531109327e-05,
      "loss": 0.6847,
      "step": 1783400
    },
    {
      "epoch": 16.273998102051245,
      "grad_norm": 3.8511016368865967,
      "learning_rate": 3.64383349149573e-05,
      "loss": 0.6635,
      "step": 1783500
    },
    {
      "epoch": 16.27491057741441,
      "grad_norm": 2.6826815605163574,
      "learning_rate": 3.643757451882133e-05,
      "loss": 0.634,
      "step": 1783600
    },
    {
      "epoch": 16.275823052777575,
      "grad_norm": 3.8310635089874268,
      "learning_rate": 3.643681412268535e-05,
      "loss": 0.6574,
      "step": 1783700
    },
    {
      "epoch": 16.27673552814074,
      "grad_norm": 4.0086236000061035,
      "learning_rate": 3.643605372654939e-05,
      "loss": 0.6592,
      "step": 1783800
    },
    {
      "epoch": 16.277648003503906,
      "grad_norm": 4.346793174743652,
      "learning_rate": 3.643529333041341e-05,
      "loss": 0.7005,
      "step": 1783900
    },
    {
      "epoch": 16.27856047886707,
      "grad_norm": 3.640073299407959,
      "learning_rate": 3.643453293427744e-05,
      "loss": 0.6945,
      "step": 1784000
    },
    {
      "epoch": 16.279472954230236,
      "grad_norm": 4.284692764282227,
      "learning_rate": 3.643377253814147e-05,
      "loss": 0.6318,
      "step": 1784100
    },
    {
      "epoch": 16.2803854295934,
      "grad_norm": 4.017010688781738,
      "learning_rate": 3.64330121420055e-05,
      "loss": 0.6589,
      "step": 1784200
    },
    {
      "epoch": 16.281297904956567,
      "grad_norm": 5.263228416442871,
      "learning_rate": 3.6432251745869525e-05,
      "loss": 0.6763,
      "step": 1784300
    },
    {
      "epoch": 16.282210380319732,
      "grad_norm": 3.554492473602295,
      "learning_rate": 3.643149134973356e-05,
      "loss": 0.6755,
      "step": 1784400
    },
    {
      "epoch": 16.283122855682898,
      "grad_norm": 4.151515007019043,
      "learning_rate": 3.6430730953597585e-05,
      "loss": 0.687,
      "step": 1784500
    },
    {
      "epoch": 16.284035331046063,
      "grad_norm": 3.8559465408325195,
      "learning_rate": 3.6429970557461615e-05,
      "loss": 0.6206,
      "step": 1784600
    },
    {
      "epoch": 16.284947806409228,
      "grad_norm": 4.492481708526611,
      "learning_rate": 3.6429210161325645e-05,
      "loss": 0.6996,
      "step": 1784700
    },
    {
      "epoch": 16.285860281772393,
      "grad_norm": 4.098680019378662,
      "learning_rate": 3.6428449765189675e-05,
      "loss": 0.6804,
      "step": 1784800
    },
    {
      "epoch": 16.28677275713556,
      "grad_norm": 3.812225580215454,
      "learning_rate": 3.6427689369053705e-05,
      "loss": 0.6491,
      "step": 1784900
    },
    {
      "epoch": 16.287685232498724,
      "grad_norm": 3.8324568271636963,
      "learning_rate": 3.6426928972917735e-05,
      "loss": 0.6735,
      "step": 1785000
    },
    {
      "epoch": 16.28859770786189,
      "grad_norm": 3.998439311981201,
      "learning_rate": 3.642616857678176e-05,
      "loss": 0.6767,
      "step": 1785100
    },
    {
      "epoch": 16.289510183225055,
      "grad_norm": 3.3157122135162354,
      "learning_rate": 3.6425408180645796e-05,
      "loss": 0.6377,
      "step": 1785200
    },
    {
      "epoch": 16.29042265858822,
      "grad_norm": 4.666989326477051,
      "learning_rate": 3.642464778450982e-05,
      "loss": 0.6694,
      "step": 1785300
    },
    {
      "epoch": 16.29133513395138,
      "grad_norm": 3.8983023166656494,
      "learning_rate": 3.642388738837385e-05,
      "loss": 0.7,
      "step": 1785400
    },
    {
      "epoch": 16.292247609314547,
      "grad_norm": 4.113914489746094,
      "learning_rate": 3.642312699223788e-05,
      "loss": 0.6636,
      "step": 1785500
    },
    {
      "epoch": 16.293160084677712,
      "grad_norm": 3.5237536430358887,
      "learning_rate": 3.642236659610191e-05,
      "loss": 0.6771,
      "step": 1785600
    },
    {
      "epoch": 16.294072560040878,
      "grad_norm": 5.2476606369018555,
      "learning_rate": 3.642160619996593e-05,
      "loss": 0.6774,
      "step": 1785700
    },
    {
      "epoch": 16.294985035404043,
      "grad_norm": 3.4362521171569824,
      "learning_rate": 3.642084580382997e-05,
      "loss": 0.6802,
      "step": 1785800
    },
    {
      "epoch": 16.295897510767208,
      "grad_norm": 4.087284564971924,
      "learning_rate": 3.642008540769399e-05,
      "loss": 0.6932,
      "step": 1785900
    },
    {
      "epoch": 16.296809986130373,
      "grad_norm": 3.9190280437469482,
      "learning_rate": 3.641932501155802e-05,
      "loss": 0.6416,
      "step": 1786000
    },
    {
      "epoch": 16.29772246149354,
      "grad_norm": 2.516326904296875,
      "learning_rate": 3.641856461542205e-05,
      "loss": 0.636,
      "step": 1786100
    },
    {
      "epoch": 16.298634936856704,
      "grad_norm": 4.34150505065918,
      "learning_rate": 3.641780421928608e-05,
      "loss": 0.6429,
      "step": 1786200
    },
    {
      "epoch": 16.29954741221987,
      "grad_norm": 5.305078983306885,
      "learning_rate": 3.641704382315011e-05,
      "loss": 0.7238,
      "step": 1786300
    },
    {
      "epoch": 16.300459887583035,
      "grad_norm": 5.12907075881958,
      "learning_rate": 3.641628342701414e-05,
      "loss": 0.6539,
      "step": 1786400
    },
    {
      "epoch": 16.3013723629462,
      "grad_norm": 3.595993757247925,
      "learning_rate": 3.6415523030878166e-05,
      "loss": 0.6578,
      "step": 1786500
    },
    {
      "epoch": 16.302284838309365,
      "grad_norm": 4.130917072296143,
      "learning_rate": 3.6414762634742196e-05,
      "loss": 0.6284,
      "step": 1786600
    },
    {
      "epoch": 16.30319731367253,
      "grad_norm": 4.074975967407227,
      "learning_rate": 3.6414002238606226e-05,
      "loss": 0.6717,
      "step": 1786700
    },
    {
      "epoch": 16.304109789035696,
      "grad_norm": 3.909181833267212,
      "learning_rate": 3.641324184247025e-05,
      "loss": 0.6841,
      "step": 1786800
    },
    {
      "epoch": 16.30502226439886,
      "grad_norm": 3.9248857498168945,
      "learning_rate": 3.6412481446334286e-05,
      "loss": 0.6553,
      "step": 1786900
    },
    {
      "epoch": 16.305934739762026,
      "grad_norm": 3.852289915084839,
      "learning_rate": 3.641172105019831e-05,
      "loss": 0.6569,
      "step": 1787000
    },
    {
      "epoch": 16.30684721512519,
      "grad_norm": 3.65753436088562,
      "learning_rate": 3.641096065406234e-05,
      "loss": 0.6548,
      "step": 1787100
    },
    {
      "epoch": 16.307759690488357,
      "grad_norm": 3.8789477348327637,
      "learning_rate": 3.641020025792637e-05,
      "loss": 0.6642,
      "step": 1787200
    },
    {
      "epoch": 16.308672165851522,
      "grad_norm": 3.9897780418395996,
      "learning_rate": 3.64094398617904e-05,
      "loss": 0.6656,
      "step": 1787300
    },
    {
      "epoch": 16.309584641214688,
      "grad_norm": 3.4335193634033203,
      "learning_rate": 3.640867946565443e-05,
      "loss": 0.6923,
      "step": 1787400
    },
    {
      "epoch": 16.310497116577853,
      "grad_norm": 4.451125144958496,
      "learning_rate": 3.640791906951846e-05,
      "loss": 0.6774,
      "step": 1787500
    },
    {
      "epoch": 16.31140959194102,
      "grad_norm": 3.672761917114258,
      "learning_rate": 3.640715867338248e-05,
      "loss": 0.648,
      "step": 1787600
    },
    {
      "epoch": 16.312322067304184,
      "grad_norm": 5.053705215454102,
      "learning_rate": 3.640639827724652e-05,
      "loss": 0.6989,
      "step": 1787700
    },
    {
      "epoch": 16.31323454266735,
      "grad_norm": 5.231783390045166,
      "learning_rate": 3.6405637881110543e-05,
      "loss": 0.6656,
      "step": 1787800
    },
    {
      "epoch": 16.314147018030514,
      "grad_norm": 3.305392026901245,
      "learning_rate": 3.6404877484974574e-05,
      "loss": 0.659,
      "step": 1787900
    },
    {
      "epoch": 16.31505949339368,
      "grad_norm": 4.0033721923828125,
      "learning_rate": 3.6404117088838604e-05,
      "loss": 0.6642,
      "step": 1788000
    },
    {
      "epoch": 16.315971968756845,
      "grad_norm": 3.6948890686035156,
      "learning_rate": 3.6403356692702634e-05,
      "loss": 0.6493,
      "step": 1788100
    },
    {
      "epoch": 16.31688444412001,
      "grad_norm": 2.6069979667663574,
      "learning_rate": 3.640259629656666e-05,
      "loss": 0.6412,
      "step": 1788200
    },
    {
      "epoch": 16.317796919483175,
      "grad_norm": 4.157704830169678,
      "learning_rate": 3.6401835900430694e-05,
      "loss": 0.6413,
      "step": 1788300
    },
    {
      "epoch": 16.31870939484634,
      "grad_norm": 4.324173927307129,
      "learning_rate": 3.640107550429472e-05,
      "loss": 0.6413,
      "step": 1788400
    },
    {
      "epoch": 16.319621870209506,
      "grad_norm": 3.7224364280700684,
      "learning_rate": 3.640031510815875e-05,
      "loss": 0.6448,
      "step": 1788500
    },
    {
      "epoch": 16.32053434557267,
      "grad_norm": 4.31280517578125,
      "learning_rate": 3.639955471202278e-05,
      "loss": 0.6546,
      "step": 1788600
    },
    {
      "epoch": 16.321446820935833,
      "grad_norm": 4.194573879241943,
      "learning_rate": 3.639879431588681e-05,
      "loss": 0.6643,
      "step": 1788700
    },
    {
      "epoch": 16.322359296299,
      "grad_norm": 2.977785348892212,
      "learning_rate": 3.639803391975084e-05,
      "loss": 0.6594,
      "step": 1788800
    },
    {
      "epoch": 16.323271771662164,
      "grad_norm": 3.9127087593078613,
      "learning_rate": 3.639727352361487e-05,
      "loss": 0.6988,
      "step": 1788900
    },
    {
      "epoch": 16.32418424702533,
      "grad_norm": 3.947554111480713,
      "learning_rate": 3.639651312747889e-05,
      "loss": 0.6731,
      "step": 1789000
    },
    {
      "epoch": 16.325096722388494,
      "grad_norm": 2.278791904449463,
      "learning_rate": 3.639575273134293e-05,
      "loss": 0.6764,
      "step": 1789100
    },
    {
      "epoch": 16.32600919775166,
      "grad_norm": 3.391554355621338,
      "learning_rate": 3.639499233520695e-05,
      "loss": 0.6422,
      "step": 1789200
    },
    {
      "epoch": 16.326921673114825,
      "grad_norm": 4.391666889190674,
      "learning_rate": 3.6394231939070974e-05,
      "loss": 0.6735,
      "step": 1789300
    },
    {
      "epoch": 16.32783414847799,
      "grad_norm": 3.5876851081848145,
      "learning_rate": 3.639347154293501e-05,
      "loss": 0.6408,
      "step": 1789400
    },
    {
      "epoch": 16.328746623841155,
      "grad_norm": 4.212030410766602,
      "learning_rate": 3.6392711146799034e-05,
      "loss": 0.6517,
      "step": 1789500
    },
    {
      "epoch": 16.32965909920432,
      "grad_norm": 3.692262649536133,
      "learning_rate": 3.6391950750663064e-05,
      "loss": 0.6393,
      "step": 1789600
    },
    {
      "epoch": 16.330571574567486,
      "grad_norm": 3.840278148651123,
      "learning_rate": 3.6391190354527094e-05,
      "loss": 0.6982,
      "step": 1789700
    },
    {
      "epoch": 16.33148404993065,
      "grad_norm": 3.7854690551757812,
      "learning_rate": 3.6390429958391124e-05,
      "loss": 0.6939,
      "step": 1789800
    },
    {
      "epoch": 16.332396525293817,
      "grad_norm": 3.1310274600982666,
      "learning_rate": 3.6389669562255155e-05,
      "loss": 0.6328,
      "step": 1789900
    },
    {
      "epoch": 16.333309000656982,
      "grad_norm": 3.3641357421875,
      "learning_rate": 3.6388909166119185e-05,
      "loss": 0.6677,
      "step": 1790000
    },
    {
      "epoch": 16.334221476020147,
      "grad_norm": 4.692554473876953,
      "learning_rate": 3.638814876998321e-05,
      "loss": 0.6878,
      "step": 1790100
    },
    {
      "epoch": 16.335133951383312,
      "grad_norm": 3.893012046813965,
      "learning_rate": 3.6387388373847245e-05,
      "loss": 0.6507,
      "step": 1790200
    },
    {
      "epoch": 16.336046426746478,
      "grad_norm": 3.615274667739868,
      "learning_rate": 3.638662797771127e-05,
      "loss": 0.6441,
      "step": 1790300
    },
    {
      "epoch": 16.336958902109643,
      "grad_norm": 3.5534722805023193,
      "learning_rate": 3.63858675815753e-05,
      "loss": 0.6321,
      "step": 1790400
    },
    {
      "epoch": 16.33787137747281,
      "grad_norm": 4.069265365600586,
      "learning_rate": 3.638510718543933e-05,
      "loss": 0.6648,
      "step": 1790500
    },
    {
      "epoch": 16.338783852835974,
      "grad_norm": 4.717524528503418,
      "learning_rate": 3.638434678930336e-05,
      "loss": 0.6227,
      "step": 1790600
    },
    {
      "epoch": 16.33969632819914,
      "grad_norm": 4.373808860778809,
      "learning_rate": 3.638358639316738e-05,
      "loss": 0.6806,
      "step": 1790700
    },
    {
      "epoch": 16.340608803562304,
      "grad_norm": 4.156101703643799,
      "learning_rate": 3.638282599703142e-05,
      "loss": 0.6263,
      "step": 1790800
    },
    {
      "epoch": 16.34152127892547,
      "grad_norm": 4.11665153503418,
      "learning_rate": 3.638206560089544e-05,
      "loss": 0.7041,
      "step": 1790900
    },
    {
      "epoch": 16.342433754288635,
      "grad_norm": 3.3594579696655273,
      "learning_rate": 3.638130520475947e-05,
      "loss": 0.6315,
      "step": 1791000
    },
    {
      "epoch": 16.3433462296518,
      "grad_norm": 3.988442897796631,
      "learning_rate": 3.63805448086235e-05,
      "loss": 0.6639,
      "step": 1791100
    },
    {
      "epoch": 16.344258705014965,
      "grad_norm": 2.1659913063049316,
      "learning_rate": 3.637978441248753e-05,
      "loss": 0.6371,
      "step": 1791200
    },
    {
      "epoch": 16.34517118037813,
      "grad_norm": 3.957361936569214,
      "learning_rate": 3.637902401635156e-05,
      "loss": 0.6375,
      "step": 1791300
    },
    {
      "epoch": 16.346083655741296,
      "grad_norm": 3.2932534217834473,
      "learning_rate": 3.637826362021559e-05,
      "loss": 0.6321,
      "step": 1791400
    },
    {
      "epoch": 16.34699613110446,
      "grad_norm": 4.081430912017822,
      "learning_rate": 3.6377503224079615e-05,
      "loss": 0.6868,
      "step": 1791500
    },
    {
      "epoch": 16.347908606467627,
      "grad_norm": 3.6413559913635254,
      "learning_rate": 3.637674282794365e-05,
      "loss": 0.6727,
      "step": 1791600
    },
    {
      "epoch": 16.348821081830792,
      "grad_norm": 3.535407066345215,
      "learning_rate": 3.6375982431807675e-05,
      "loss": 0.6771,
      "step": 1791700
    },
    {
      "epoch": 16.349733557193957,
      "grad_norm": 4.558991432189941,
      "learning_rate": 3.6375222035671705e-05,
      "loss": 0.6727,
      "step": 1791800
    },
    {
      "epoch": 16.350646032557123,
      "grad_norm": 4.335463047027588,
      "learning_rate": 3.6374461639535736e-05,
      "loss": 0.6122,
      "step": 1791900
    },
    {
      "epoch": 16.351558507920288,
      "grad_norm": 4.028724193572998,
      "learning_rate": 3.6373701243399766e-05,
      "loss": 0.671,
      "step": 1792000
    },
    {
      "epoch": 16.35247098328345,
      "grad_norm": 4.686032772064209,
      "learning_rate": 3.637294084726379e-05,
      "loss": 0.6553,
      "step": 1792100
    },
    {
      "epoch": 16.353383458646615,
      "grad_norm": 3.729987382888794,
      "learning_rate": 3.637218045112782e-05,
      "loss": 0.6368,
      "step": 1792200
    },
    {
      "epoch": 16.35429593400978,
      "grad_norm": 4.053333759307861,
      "learning_rate": 3.637142005499185e-05,
      "loss": 0.629,
      "step": 1792300
    },
    {
      "epoch": 16.355208409372946,
      "grad_norm": 4.820806503295898,
      "learning_rate": 3.637065965885588e-05,
      "loss": 0.6396,
      "step": 1792400
    },
    {
      "epoch": 16.35612088473611,
      "grad_norm": 4.42362117767334,
      "learning_rate": 3.636989926271991e-05,
      "loss": 0.6713,
      "step": 1792500
    },
    {
      "epoch": 16.357033360099276,
      "grad_norm": 3.5249719619750977,
      "learning_rate": 3.636913886658393e-05,
      "loss": 0.6208,
      "step": 1792600
    },
    {
      "epoch": 16.35794583546244,
      "grad_norm": 4.214632034301758,
      "learning_rate": 3.636837847044797e-05,
      "loss": 0.6685,
      "step": 1792700
    },
    {
      "epoch": 16.358858310825607,
      "grad_norm": 3.5719590187072754,
      "learning_rate": 3.636761807431199e-05,
      "loss": 0.6582,
      "step": 1792800
    },
    {
      "epoch": 16.359770786188772,
      "grad_norm": 3.7356908321380615,
      "learning_rate": 3.636685767817602e-05,
      "loss": 0.6556,
      "step": 1792900
    },
    {
      "epoch": 16.360683261551937,
      "grad_norm": 3.9628636837005615,
      "learning_rate": 3.636609728204005e-05,
      "loss": 0.6433,
      "step": 1793000
    },
    {
      "epoch": 16.361595736915103,
      "grad_norm": 3.94340443611145,
      "learning_rate": 3.636533688590408e-05,
      "loss": 0.6793,
      "step": 1793100
    },
    {
      "epoch": 16.362508212278268,
      "grad_norm": 4.704205513000488,
      "learning_rate": 3.636457648976811e-05,
      "loss": 0.6756,
      "step": 1793200
    },
    {
      "epoch": 16.363420687641433,
      "grad_norm": 4.424349784851074,
      "learning_rate": 3.636381609363214e-05,
      "loss": 0.6652,
      "step": 1793300
    },
    {
      "epoch": 16.3643331630046,
      "grad_norm": 4.56593132019043,
      "learning_rate": 3.6363055697496166e-05,
      "loss": 0.6636,
      "step": 1793400
    },
    {
      "epoch": 16.365245638367764,
      "grad_norm": 3.80104398727417,
      "learning_rate": 3.63622953013602e-05,
      "loss": 0.6656,
      "step": 1793500
    },
    {
      "epoch": 16.36615811373093,
      "grad_norm": 4.278558254241943,
      "learning_rate": 3.6361534905224226e-05,
      "loss": 0.6101,
      "step": 1793600
    },
    {
      "epoch": 16.367070589094094,
      "grad_norm": 4.0922136306762695,
      "learning_rate": 3.6360774509088256e-05,
      "loss": 0.6351,
      "step": 1793700
    },
    {
      "epoch": 16.36798306445726,
      "grad_norm": 4.0413618087768555,
      "learning_rate": 3.6360014112952286e-05,
      "loss": 0.6269,
      "step": 1793800
    },
    {
      "epoch": 16.368895539820425,
      "grad_norm": 5.0924906730651855,
      "learning_rate": 3.6359253716816317e-05,
      "loss": 0.6479,
      "step": 1793900
    },
    {
      "epoch": 16.36980801518359,
      "grad_norm": 3.5918591022491455,
      "learning_rate": 3.635849332068034e-05,
      "loss": 0.6714,
      "step": 1794000
    },
    {
      "epoch": 16.370720490546756,
      "grad_norm": 4.520963668823242,
      "learning_rate": 3.635773292454438e-05,
      "loss": 0.6809,
      "step": 1794100
    },
    {
      "epoch": 16.37163296590992,
      "grad_norm": 4.5807929039001465,
      "learning_rate": 3.63569725284084e-05,
      "loss": 0.6752,
      "step": 1794200
    },
    {
      "epoch": 16.372545441273086,
      "grad_norm": 3.1141321659088135,
      "learning_rate": 3.635621213227243e-05,
      "loss": 0.664,
      "step": 1794300
    },
    {
      "epoch": 16.37345791663625,
      "grad_norm": 4.543844699859619,
      "learning_rate": 3.635545173613646e-05,
      "loss": 0.6983,
      "step": 1794400
    },
    {
      "epoch": 16.374370391999417,
      "grad_norm": 5.036016464233398,
      "learning_rate": 3.635469134000049e-05,
      "loss": 0.6447,
      "step": 1794500
    },
    {
      "epoch": 16.375282867362582,
      "grad_norm": 4.207179546356201,
      "learning_rate": 3.635393094386452e-05,
      "loss": 0.6655,
      "step": 1794600
    },
    {
      "epoch": 16.376195342725747,
      "grad_norm": 3.8170714378356934,
      "learning_rate": 3.635317054772855e-05,
      "loss": 0.6455,
      "step": 1794700
    },
    {
      "epoch": 16.377107818088913,
      "grad_norm": 4.280882835388184,
      "learning_rate": 3.6352410151592574e-05,
      "loss": 0.6489,
      "step": 1794800
    },
    {
      "epoch": 16.378020293452078,
      "grad_norm": 4.301509380340576,
      "learning_rate": 3.635164975545661e-05,
      "loss": 0.6533,
      "step": 1794900
    },
    {
      "epoch": 16.378932768815243,
      "grad_norm": 3.4515140056610107,
      "learning_rate": 3.6350889359320634e-05,
      "loss": 0.6583,
      "step": 1795000
    },
    {
      "epoch": 16.37984524417841,
      "grad_norm": 4.480384826660156,
      "learning_rate": 3.635012896318466e-05,
      "loss": 0.6844,
      "step": 1795100
    },
    {
      "epoch": 16.380757719541574,
      "grad_norm": 3.9074084758758545,
      "learning_rate": 3.6349368567048694e-05,
      "loss": 0.6552,
      "step": 1795200
    },
    {
      "epoch": 16.38167019490474,
      "grad_norm": 3.991201639175415,
      "learning_rate": 3.634860817091272e-05,
      "loss": 0.685,
      "step": 1795300
    },
    {
      "epoch": 16.382582670267904,
      "grad_norm": 5.374780654907227,
      "learning_rate": 3.634784777477675e-05,
      "loss": 0.6637,
      "step": 1795400
    },
    {
      "epoch": 16.383495145631066,
      "grad_norm": 4.096351146697998,
      "learning_rate": 3.634708737864078e-05,
      "loss": 0.6515,
      "step": 1795500
    },
    {
      "epoch": 16.38440762099423,
      "grad_norm": 4.283436298370361,
      "learning_rate": 3.634632698250481e-05,
      "loss": 0.673,
      "step": 1795600
    },
    {
      "epoch": 16.385320096357397,
      "grad_norm": 3.6033153533935547,
      "learning_rate": 3.634556658636884e-05,
      "loss": 0.687,
      "step": 1795700
    },
    {
      "epoch": 16.386232571720562,
      "grad_norm": 3.874997615814209,
      "learning_rate": 3.634480619023287e-05,
      "loss": 0.6834,
      "step": 1795800
    },
    {
      "epoch": 16.387145047083727,
      "grad_norm": 4.296908378601074,
      "learning_rate": 3.634404579409689e-05,
      "loss": 0.6672,
      "step": 1795900
    },
    {
      "epoch": 16.388057522446893,
      "grad_norm": 4.402027606964111,
      "learning_rate": 3.634328539796093e-05,
      "loss": 0.6608,
      "step": 1796000
    },
    {
      "epoch": 16.388969997810058,
      "grad_norm": 2.870997190475464,
      "learning_rate": 3.634252500182495e-05,
      "loss": 0.6684,
      "step": 1796100
    },
    {
      "epoch": 16.389882473173223,
      "grad_norm": 3.8762593269348145,
      "learning_rate": 3.634176460568898e-05,
      "loss": 0.6467,
      "step": 1796200
    },
    {
      "epoch": 16.39079494853639,
      "grad_norm": 3.2740137577056885,
      "learning_rate": 3.634100420955301e-05,
      "loss": 0.6528,
      "step": 1796300
    },
    {
      "epoch": 16.391707423899554,
      "grad_norm": 3.892369031906128,
      "learning_rate": 3.634024381341704e-05,
      "loss": 0.6705,
      "step": 1796400
    },
    {
      "epoch": 16.39261989926272,
      "grad_norm": 3.645472526550293,
      "learning_rate": 3.6339483417281064e-05,
      "loss": 0.6735,
      "step": 1796500
    },
    {
      "epoch": 16.393532374625885,
      "grad_norm": 3.7282893657684326,
      "learning_rate": 3.63387230211451e-05,
      "loss": 0.6826,
      "step": 1796600
    },
    {
      "epoch": 16.39444484998905,
      "grad_norm": 4.430497646331787,
      "learning_rate": 3.6337962625009125e-05,
      "loss": 0.6435,
      "step": 1796700
    },
    {
      "epoch": 16.395357325352215,
      "grad_norm": 3.655561923980713,
      "learning_rate": 3.6337202228873155e-05,
      "loss": 0.6704,
      "step": 1796800
    },
    {
      "epoch": 16.39626980071538,
      "grad_norm": 4.058363914489746,
      "learning_rate": 3.6336441832737185e-05,
      "loss": 0.6612,
      "step": 1796900
    },
    {
      "epoch": 16.397182276078546,
      "grad_norm": 3.6413004398345947,
      "learning_rate": 3.6335681436601215e-05,
      "loss": 0.6496,
      "step": 1797000
    },
    {
      "epoch": 16.39809475144171,
      "grad_norm": 4.088883876800537,
      "learning_rate": 3.6334921040465245e-05,
      "loss": 0.6763,
      "step": 1797100
    },
    {
      "epoch": 16.399007226804876,
      "grad_norm": 3.7923805713653564,
      "learning_rate": 3.6334160644329275e-05,
      "loss": 0.6806,
      "step": 1797200
    },
    {
      "epoch": 16.39991970216804,
      "grad_norm": 3.3205995559692383,
      "learning_rate": 3.63334002481933e-05,
      "loss": 0.6841,
      "step": 1797300
    },
    {
      "epoch": 16.400832177531207,
      "grad_norm": 4.513842582702637,
      "learning_rate": 3.6332639852057335e-05,
      "loss": 0.6499,
      "step": 1797400
    },
    {
      "epoch": 16.401744652894372,
      "grad_norm": 2.700550079345703,
      "learning_rate": 3.633187945592136e-05,
      "loss": 0.7061,
      "step": 1797500
    },
    {
      "epoch": 16.402657128257538,
      "grad_norm": 4.64759635925293,
      "learning_rate": 3.633111905978539e-05,
      "loss": 0.6216,
      "step": 1797600
    },
    {
      "epoch": 16.403569603620703,
      "grad_norm": 3.3938305377960205,
      "learning_rate": 3.633035866364942e-05,
      "loss": 0.6918,
      "step": 1797700
    },
    {
      "epoch": 16.404482078983868,
      "grad_norm": 3.3340871334075928,
      "learning_rate": 3.632959826751345e-05,
      "loss": 0.6572,
      "step": 1797800
    },
    {
      "epoch": 16.405394554347033,
      "grad_norm": 2.174111843109131,
      "learning_rate": 3.632883787137747e-05,
      "loss": 0.6641,
      "step": 1797900
    },
    {
      "epoch": 16.4063070297102,
      "grad_norm": 4.490056037902832,
      "learning_rate": 3.63280774752415e-05,
      "loss": 0.6705,
      "step": 1798000
    },
    {
      "epoch": 16.407219505073364,
      "grad_norm": 2.5682363510131836,
      "learning_rate": 3.632731707910553e-05,
      "loss": 0.6485,
      "step": 1798100
    },
    {
      "epoch": 16.40813198043653,
      "grad_norm": 4.680168151855469,
      "learning_rate": 3.632655668296956e-05,
      "loss": 0.6319,
      "step": 1798200
    },
    {
      "epoch": 16.409044455799695,
      "grad_norm": 4.433180809020996,
      "learning_rate": 3.632579628683359e-05,
      "loss": 0.6613,
      "step": 1798300
    },
    {
      "epoch": 16.40995693116286,
      "grad_norm": 3.4588685035705566,
      "learning_rate": 3.6325035890697615e-05,
      "loss": 0.6775,
      "step": 1798400
    },
    {
      "epoch": 16.410869406526025,
      "grad_norm": 3.237826108932495,
      "learning_rate": 3.632427549456165e-05,
      "loss": 0.7026,
      "step": 1798500
    },
    {
      "epoch": 16.41178188188919,
      "grad_norm": 4.20722770690918,
      "learning_rate": 3.6323515098425676e-05,
      "loss": 0.6373,
      "step": 1798600
    },
    {
      "epoch": 16.412694357252356,
      "grad_norm": 3.874392509460449,
      "learning_rate": 3.6322754702289706e-05,
      "loss": 0.6761,
      "step": 1798700
    },
    {
      "epoch": 16.41360683261552,
      "grad_norm": 3.446946144104004,
      "learning_rate": 3.6321994306153736e-05,
      "loss": 0.6693,
      "step": 1798800
    },
    {
      "epoch": 16.414519307978683,
      "grad_norm": 4.549680233001709,
      "learning_rate": 3.6321233910017766e-05,
      "loss": 0.6872,
      "step": 1798900
    },
    {
      "epoch": 16.415431783341848,
      "grad_norm": 3.7123618125915527,
      "learning_rate": 3.632047351388179e-05,
      "loss": 0.7004,
      "step": 1799000
    },
    {
      "epoch": 16.416344258705013,
      "grad_norm": 3.931931734085083,
      "learning_rate": 3.6319713117745826e-05,
      "loss": 0.6396,
      "step": 1799100
    },
    {
      "epoch": 16.41725673406818,
      "grad_norm": 3.5138232707977295,
      "learning_rate": 3.631895272160985e-05,
      "loss": 0.6664,
      "step": 1799200
    },
    {
      "epoch": 16.418169209431344,
      "grad_norm": 3.8763861656188965,
      "learning_rate": 3.631819232547388e-05,
      "loss": 0.6807,
      "step": 1799300
    },
    {
      "epoch": 16.41908168479451,
      "grad_norm": 4.334440231323242,
      "learning_rate": 3.631743192933791e-05,
      "loss": 0.6641,
      "step": 1799400
    },
    {
      "epoch": 16.419994160157675,
      "grad_norm": 3.845022678375244,
      "learning_rate": 3.631667153320194e-05,
      "loss": 0.6869,
      "step": 1799500
    },
    {
      "epoch": 16.42090663552084,
      "grad_norm": 4.112525463104248,
      "learning_rate": 3.631591113706597e-05,
      "loss": 0.6769,
      "step": 1799600
    },
    {
      "epoch": 16.421819110884005,
      "grad_norm": 3.866387128829956,
      "learning_rate": 3.631515074093e-05,
      "loss": 0.7144,
      "step": 1799700
    },
    {
      "epoch": 16.42273158624717,
      "grad_norm": 3.870675563812256,
      "learning_rate": 3.631439034479402e-05,
      "loss": 0.6697,
      "step": 1799800
    },
    {
      "epoch": 16.423644061610336,
      "grad_norm": 3.9071805477142334,
      "learning_rate": 3.631362994865806e-05,
      "loss": 0.6896,
      "step": 1799900
    },
    {
      "epoch": 16.4245565369735,
      "grad_norm": 3.8545937538146973,
      "learning_rate": 3.631286955252208e-05,
      "loss": 0.7033,
      "step": 1800000
    },
    {
      "epoch": 16.425469012336666,
      "grad_norm": 3.864624261856079,
      "learning_rate": 3.631210915638611e-05,
      "loss": 0.6804,
      "step": 1800100
    },
    {
      "epoch": 16.42638148769983,
      "grad_norm": 4.138639450073242,
      "learning_rate": 3.631134876025014e-05,
      "loss": 0.6934,
      "step": 1800200
    },
    {
      "epoch": 16.427293963062997,
      "grad_norm": 4.522882461547852,
      "learning_rate": 3.631058836411417e-05,
      "loss": 0.6909,
      "step": 1800300
    },
    {
      "epoch": 16.428206438426162,
      "grad_norm": 3.7986316680908203,
      "learning_rate": 3.6309827967978196e-05,
      "loss": 0.6433,
      "step": 1800400
    },
    {
      "epoch": 16.429118913789328,
      "grad_norm": 3.3658089637756348,
      "learning_rate": 3.630906757184223e-05,
      "loss": 0.6409,
      "step": 1800500
    },
    {
      "epoch": 16.430031389152493,
      "grad_norm": 4.0467071533203125,
      "learning_rate": 3.6308307175706257e-05,
      "loss": 0.6516,
      "step": 1800600
    },
    {
      "epoch": 16.43094386451566,
      "grad_norm": 4.451765060424805,
      "learning_rate": 3.6307546779570287e-05,
      "loss": 0.6642,
      "step": 1800700
    },
    {
      "epoch": 16.431856339878824,
      "grad_norm": 3.9571495056152344,
      "learning_rate": 3.630678638343432e-05,
      "loss": 0.6852,
      "step": 1800800
    },
    {
      "epoch": 16.43276881524199,
      "grad_norm": 3.4941844940185547,
      "learning_rate": 3.630602598729834e-05,
      "loss": 0.682,
      "step": 1800900
    },
    {
      "epoch": 16.433681290605154,
      "grad_norm": 3.755704402923584,
      "learning_rate": 3.630526559116238e-05,
      "loss": 0.6378,
      "step": 1801000
    },
    {
      "epoch": 16.43459376596832,
      "grad_norm": 4.987076282501221,
      "learning_rate": 3.63045051950264e-05,
      "loss": 0.6326,
      "step": 1801100
    },
    {
      "epoch": 16.435506241331485,
      "grad_norm": 4.021572113037109,
      "learning_rate": 3.630374479889043e-05,
      "loss": 0.6788,
      "step": 1801200
    },
    {
      "epoch": 16.43641871669465,
      "grad_norm": 3.7663967609405518,
      "learning_rate": 3.630298440275446e-05,
      "loss": 0.6694,
      "step": 1801300
    },
    {
      "epoch": 16.437331192057815,
      "grad_norm": 3.5089263916015625,
      "learning_rate": 3.630222400661849e-05,
      "loss": 0.7058,
      "step": 1801400
    },
    {
      "epoch": 16.43824366742098,
      "grad_norm": 3.8445403575897217,
      "learning_rate": 3.6301463610482514e-05,
      "loss": 0.6986,
      "step": 1801500
    },
    {
      "epoch": 16.439156142784146,
      "grad_norm": 3.352484703063965,
      "learning_rate": 3.630070321434655e-05,
      "loss": 0.6409,
      "step": 1801600
    },
    {
      "epoch": 16.44006861814731,
      "grad_norm": 4.100114345550537,
      "learning_rate": 3.6299942818210574e-05,
      "loss": 0.6216,
      "step": 1801700
    },
    {
      "epoch": 16.440981093510477,
      "grad_norm": 4.149299621582031,
      "learning_rate": 3.6299182422074604e-05,
      "loss": 0.6532,
      "step": 1801800
    },
    {
      "epoch": 16.441893568873642,
      "grad_norm": 2.872053623199463,
      "learning_rate": 3.6298422025938634e-05,
      "loss": 0.6767,
      "step": 1801900
    },
    {
      "epoch": 16.442806044236807,
      "grad_norm": 2.616039991378784,
      "learning_rate": 3.6297661629802664e-05,
      "loss": 0.6137,
      "step": 1802000
    },
    {
      "epoch": 16.443718519599972,
      "grad_norm": 3.9099676609039307,
      "learning_rate": 3.6296901233666694e-05,
      "loss": 0.6661,
      "step": 1802100
    },
    {
      "epoch": 16.444630994963138,
      "grad_norm": 4.211624622344971,
      "learning_rate": 3.6296140837530724e-05,
      "loss": 0.6607,
      "step": 1802200
    },
    {
      "epoch": 16.4455434703263,
      "grad_norm": 2.446479558944702,
      "learning_rate": 3.629538044139475e-05,
      "loss": 0.6828,
      "step": 1802300
    },
    {
      "epoch": 16.446455945689465,
      "grad_norm": 2.8641300201416016,
      "learning_rate": 3.6294620045258784e-05,
      "loss": 0.6331,
      "step": 1802400
    },
    {
      "epoch": 16.44736842105263,
      "grad_norm": 4.072432994842529,
      "learning_rate": 3.629385964912281e-05,
      "loss": 0.6826,
      "step": 1802500
    },
    {
      "epoch": 16.448280896415795,
      "grad_norm": 3.7432494163513184,
      "learning_rate": 3.629309925298684e-05,
      "loss": 0.6793,
      "step": 1802600
    },
    {
      "epoch": 16.44919337177896,
      "grad_norm": 4.484418869018555,
      "learning_rate": 3.629233885685087e-05,
      "loss": 0.646,
      "step": 1802700
    },
    {
      "epoch": 16.450105847142126,
      "grad_norm": 4.08616828918457,
      "learning_rate": 3.62915784607149e-05,
      "loss": 0.6683,
      "step": 1802800
    },
    {
      "epoch": 16.45101832250529,
      "grad_norm": 3.300086498260498,
      "learning_rate": 3.629081806457892e-05,
      "loss": 0.6638,
      "step": 1802900
    },
    {
      "epoch": 16.451930797868457,
      "grad_norm": 4.607048511505127,
      "learning_rate": 3.629005766844296e-05,
      "loss": 0.6596,
      "step": 1803000
    },
    {
      "epoch": 16.452843273231622,
      "grad_norm": 3.1706461906433105,
      "learning_rate": 3.628929727230698e-05,
      "loss": 0.6458,
      "step": 1803100
    },
    {
      "epoch": 16.453755748594787,
      "grad_norm": 4.096943378448486,
      "learning_rate": 3.628853687617101e-05,
      "loss": 0.7069,
      "step": 1803200
    },
    {
      "epoch": 16.454668223957952,
      "grad_norm": 3.7138307094573975,
      "learning_rate": 3.628777648003504e-05,
      "loss": 0.6781,
      "step": 1803300
    },
    {
      "epoch": 16.455580699321118,
      "grad_norm": 3.8927245140075684,
      "learning_rate": 3.628701608389907e-05,
      "loss": 0.6526,
      "step": 1803400
    },
    {
      "epoch": 16.456493174684283,
      "grad_norm": 4.532376289367676,
      "learning_rate": 3.62862556877631e-05,
      "loss": 0.6417,
      "step": 1803500
    },
    {
      "epoch": 16.45740565004745,
      "grad_norm": 3.5013699531555176,
      "learning_rate": 3.6285495291627125e-05,
      "loss": 0.6902,
      "step": 1803600
    },
    {
      "epoch": 16.458318125410614,
      "grad_norm": 3.5710535049438477,
      "learning_rate": 3.6284734895491155e-05,
      "loss": 0.6733,
      "step": 1803700
    },
    {
      "epoch": 16.45923060077378,
      "grad_norm": 3.9373719692230225,
      "learning_rate": 3.6283974499355185e-05,
      "loss": 0.6284,
      "step": 1803800
    },
    {
      "epoch": 16.460143076136944,
      "grad_norm": 3.767141580581665,
      "learning_rate": 3.6283214103219215e-05,
      "loss": 0.6804,
      "step": 1803900
    },
    {
      "epoch": 16.46105555150011,
      "grad_norm": 3.074552297592163,
      "learning_rate": 3.6282453707083245e-05,
      "loss": 0.6721,
      "step": 1804000
    },
    {
      "epoch": 16.461968026863275,
      "grad_norm": 3.631298303604126,
      "learning_rate": 3.6281693310947275e-05,
      "loss": 0.6675,
      "step": 1804100
    },
    {
      "epoch": 16.46288050222644,
      "grad_norm": 5.106533527374268,
      "learning_rate": 3.62809329148113e-05,
      "loss": 0.6523,
      "step": 1804200
    },
    {
      "epoch": 16.463792977589605,
      "grad_norm": 3.1489875316619873,
      "learning_rate": 3.628017251867533e-05,
      "loss": 0.6145,
      "step": 1804300
    },
    {
      "epoch": 16.46470545295277,
      "grad_norm": 3.5343058109283447,
      "learning_rate": 3.627941212253936e-05,
      "loss": 0.6434,
      "step": 1804400
    },
    {
      "epoch": 16.465617928315936,
      "grad_norm": 4.056382179260254,
      "learning_rate": 3.627865172640339e-05,
      "loss": 0.6592,
      "step": 1804500
    },
    {
      "epoch": 16.4665304036791,
      "grad_norm": 3.1298775672912598,
      "learning_rate": 3.627789133026742e-05,
      "loss": 0.6229,
      "step": 1804600
    },
    {
      "epoch": 16.467442879042267,
      "grad_norm": 3.825068473815918,
      "learning_rate": 3.627713093413145e-05,
      "loss": 0.6835,
      "step": 1804700
    },
    {
      "epoch": 16.468355354405432,
      "grad_norm": 3.219714403152466,
      "learning_rate": 3.627637053799547e-05,
      "loss": 0.6574,
      "step": 1804800
    },
    {
      "epoch": 16.469267829768597,
      "grad_norm": 4.068734169006348,
      "learning_rate": 3.627561014185951e-05,
      "loss": 0.6906,
      "step": 1804900
    },
    {
      "epoch": 16.470180305131763,
      "grad_norm": 3.13071870803833,
      "learning_rate": 3.627484974572353e-05,
      "loss": 0.6328,
      "step": 1805000
    },
    {
      "epoch": 16.471092780494928,
      "grad_norm": 4.209164619445801,
      "learning_rate": 3.627408934958756e-05,
      "loss": 0.6782,
      "step": 1805100
    },
    {
      "epoch": 16.472005255858093,
      "grad_norm": 3.3758020401000977,
      "learning_rate": 3.627332895345159e-05,
      "loss": 0.6691,
      "step": 1805200
    },
    {
      "epoch": 16.47291773122126,
      "grad_norm": 3.4749157428741455,
      "learning_rate": 3.627256855731562e-05,
      "loss": 0.677,
      "step": 1805300
    },
    {
      "epoch": 16.473830206584424,
      "grad_norm": 2.9906325340270996,
      "learning_rate": 3.627180816117965e-05,
      "loss": 0.6768,
      "step": 1805400
    },
    {
      "epoch": 16.47474268194759,
      "grad_norm": 4.569178104400635,
      "learning_rate": 3.627104776504368e-05,
      "loss": 0.6938,
      "step": 1805500
    },
    {
      "epoch": 16.475655157310754,
      "grad_norm": 2.4433398246765137,
      "learning_rate": 3.6270287368907706e-05,
      "loss": 0.6544,
      "step": 1805600
    },
    {
      "epoch": 16.476567632673916,
      "grad_norm": 4.492464542388916,
      "learning_rate": 3.6269526972771736e-05,
      "loss": 0.6918,
      "step": 1805700
    },
    {
      "epoch": 16.47748010803708,
      "grad_norm": 4.320637226104736,
      "learning_rate": 3.6268766576635766e-05,
      "loss": 0.6674,
      "step": 1805800
    },
    {
      "epoch": 16.478392583400247,
      "grad_norm": 4.1146559715271,
      "learning_rate": 3.6268006180499796e-05,
      "loss": 0.6771,
      "step": 1805900
    },
    {
      "epoch": 16.479305058763412,
      "grad_norm": 5.227970600128174,
      "learning_rate": 3.6267245784363826e-05,
      "loss": 0.6742,
      "step": 1806000
    },
    {
      "epoch": 16.480217534126577,
      "grad_norm": 2.473991632461548,
      "learning_rate": 3.6266485388227856e-05,
      "loss": 0.6882,
      "step": 1806100
    },
    {
      "epoch": 16.481130009489743,
      "grad_norm": 3.246753692626953,
      "learning_rate": 3.626572499209188e-05,
      "loss": 0.6599,
      "step": 1806200
    },
    {
      "epoch": 16.482042484852908,
      "grad_norm": 3.9619174003601074,
      "learning_rate": 3.6264964595955916e-05,
      "loss": 0.662,
      "step": 1806300
    },
    {
      "epoch": 16.482954960216073,
      "grad_norm": 3.6758432388305664,
      "learning_rate": 3.626420419981994e-05,
      "loss": 0.6874,
      "step": 1806400
    },
    {
      "epoch": 16.48386743557924,
      "grad_norm": 3.251555919647217,
      "learning_rate": 3.626344380368397e-05,
      "loss": 0.6578,
      "step": 1806500
    },
    {
      "epoch": 16.484779910942404,
      "grad_norm": 4.87543249130249,
      "learning_rate": 3.6262683407548e-05,
      "loss": 0.6745,
      "step": 1806600
    },
    {
      "epoch": 16.48569238630557,
      "grad_norm": 3.7094459533691406,
      "learning_rate": 3.626192301141202e-05,
      "loss": 0.6249,
      "step": 1806700
    },
    {
      "epoch": 16.486604861668734,
      "grad_norm": 3.716705560684204,
      "learning_rate": 3.626116261527606e-05,
      "loss": 0.6538,
      "step": 1806800
    },
    {
      "epoch": 16.4875173370319,
      "grad_norm": 2.4018783569335938,
      "learning_rate": 3.626040221914008e-05,
      "loss": 0.6869,
      "step": 1806900
    },
    {
      "epoch": 16.488429812395065,
      "grad_norm": 3.5367608070373535,
      "learning_rate": 3.625964182300411e-05,
      "loss": 0.68,
      "step": 1807000
    },
    {
      "epoch": 16.48934228775823,
      "grad_norm": 4.2143778800964355,
      "learning_rate": 3.625888142686814e-05,
      "loss": 0.675,
      "step": 1807100
    },
    {
      "epoch": 16.490254763121396,
      "grad_norm": 3.2738356590270996,
      "learning_rate": 3.625812103073217e-05,
      "loss": 0.6621,
      "step": 1807200
    },
    {
      "epoch": 16.49116723848456,
      "grad_norm": 4.164508819580078,
      "learning_rate": 3.6257360634596196e-05,
      "loss": 0.6829,
      "step": 1807300
    },
    {
      "epoch": 16.492079713847726,
      "grad_norm": 4.01329231262207,
      "learning_rate": 3.625660023846023e-05,
      "loss": 0.5991,
      "step": 1807400
    },
    {
      "epoch": 16.49299218921089,
      "grad_norm": 3.110664129257202,
      "learning_rate": 3.625583984232426e-05,
      "loss": 0.6477,
      "step": 1807500
    },
    {
      "epoch": 16.493904664574057,
      "grad_norm": 3.7669854164123535,
      "learning_rate": 3.625507944618829e-05,
      "loss": 0.6384,
      "step": 1807600
    },
    {
      "epoch": 16.494817139937222,
      "grad_norm": 3.331751585006714,
      "learning_rate": 3.625431905005232e-05,
      "loss": 0.7046,
      "step": 1807700
    },
    {
      "epoch": 16.495729615300387,
      "grad_norm": 3.872861862182617,
      "learning_rate": 3.625355865391635e-05,
      "loss": 0.6718,
      "step": 1807800
    },
    {
      "epoch": 16.496642090663553,
      "grad_norm": 5.101405620574951,
      "learning_rate": 3.625279825778038e-05,
      "loss": 0.6253,
      "step": 1807900
    },
    {
      "epoch": 16.497554566026718,
      "grad_norm": 4.786834239959717,
      "learning_rate": 3.625203786164441e-05,
      "loss": 0.6529,
      "step": 1808000
    },
    {
      "epoch": 16.498467041389883,
      "grad_norm": 2.3577535152435303,
      "learning_rate": 3.625127746550843e-05,
      "loss": 0.657,
      "step": 1808100
    },
    {
      "epoch": 16.49937951675305,
      "grad_norm": 4.064598560333252,
      "learning_rate": 3.625051706937247e-05,
      "loss": 0.6731,
      "step": 1808200
    },
    {
      "epoch": 16.500291992116214,
      "grad_norm": 4.093067169189453,
      "learning_rate": 3.624975667323649e-05,
      "loss": 0.6837,
      "step": 1808300
    },
    {
      "epoch": 16.50120446747938,
      "grad_norm": 4.822312831878662,
      "learning_rate": 3.624899627710052e-05,
      "loss": 0.6681,
      "step": 1808400
    },
    {
      "epoch": 16.502116942842544,
      "grad_norm": 4.480752468109131,
      "learning_rate": 3.624823588096455e-05,
      "loss": 0.6595,
      "step": 1808500
    },
    {
      "epoch": 16.50302941820571,
      "grad_norm": 3.661991596221924,
      "learning_rate": 3.624747548482858e-05,
      "loss": 0.659,
      "step": 1808600
    },
    {
      "epoch": 16.503941893568875,
      "grad_norm": 3.4493067264556885,
      "learning_rate": 3.6246715088692604e-05,
      "loss": 0.6983,
      "step": 1808700
    },
    {
      "epoch": 16.50485436893204,
      "grad_norm": 4.081260681152344,
      "learning_rate": 3.624595469255664e-05,
      "loss": 0.6683,
      "step": 1808800
    },
    {
      "epoch": 16.505766844295206,
      "grad_norm": 4.820007801055908,
      "learning_rate": 3.6245194296420664e-05,
      "loss": 0.6133,
      "step": 1808900
    },
    {
      "epoch": 16.506679319658367,
      "grad_norm": 3.4608309268951416,
      "learning_rate": 3.6244433900284694e-05,
      "loss": 0.6465,
      "step": 1809000
    },
    {
      "epoch": 16.507591795021533,
      "grad_norm": 4.501169204711914,
      "learning_rate": 3.6243673504148724e-05,
      "loss": 0.6797,
      "step": 1809100
    },
    {
      "epoch": 16.508504270384698,
      "grad_norm": 4.131665229797363,
      "learning_rate": 3.624291310801275e-05,
      "loss": 0.6766,
      "step": 1809200
    },
    {
      "epoch": 16.509416745747863,
      "grad_norm": 4.362308502197266,
      "learning_rate": 3.6242152711876784e-05,
      "loss": 0.6576,
      "step": 1809300
    },
    {
      "epoch": 16.51032922111103,
      "grad_norm": 4.43965482711792,
      "learning_rate": 3.624139231574081e-05,
      "loss": 0.6569,
      "step": 1809400
    },
    {
      "epoch": 16.511241696474194,
      "grad_norm": 3.8734936714172363,
      "learning_rate": 3.624063191960484e-05,
      "loss": 0.6397,
      "step": 1809500
    },
    {
      "epoch": 16.51215417183736,
      "grad_norm": 3.642184257507324,
      "learning_rate": 3.623987152346887e-05,
      "loss": 0.6557,
      "step": 1809600
    },
    {
      "epoch": 16.513066647200525,
      "grad_norm": 3.3441827297210693,
      "learning_rate": 3.62391111273329e-05,
      "loss": 0.6502,
      "step": 1809700
    },
    {
      "epoch": 16.51397912256369,
      "grad_norm": 4.522927284240723,
      "learning_rate": 3.623835073119692e-05,
      "loss": 0.6294,
      "step": 1809800
    },
    {
      "epoch": 16.514891597926855,
      "grad_norm": 3.235863447189331,
      "learning_rate": 3.623759033506096e-05,
      "loss": 0.6579,
      "step": 1809900
    },
    {
      "epoch": 16.51580407329002,
      "grad_norm": 4.703058242797852,
      "learning_rate": 3.623682993892498e-05,
      "loss": 0.6293,
      "step": 1810000
    },
    {
      "epoch": 16.516716548653186,
      "grad_norm": 4.090347766876221,
      "learning_rate": 3.623606954278901e-05,
      "loss": 0.6693,
      "step": 1810100
    },
    {
      "epoch": 16.51762902401635,
      "grad_norm": 4.376636505126953,
      "learning_rate": 3.623530914665304e-05,
      "loss": 0.7063,
      "step": 1810200
    },
    {
      "epoch": 16.518541499379516,
      "grad_norm": 3.7270209789276123,
      "learning_rate": 3.623454875051707e-05,
      "loss": 0.6329,
      "step": 1810300
    },
    {
      "epoch": 16.51945397474268,
      "grad_norm": 3.894972085952759,
      "learning_rate": 3.62337883543811e-05,
      "loss": 0.683,
      "step": 1810400
    },
    {
      "epoch": 16.520366450105847,
      "grad_norm": 3.619642734527588,
      "learning_rate": 3.623302795824513e-05,
      "loss": 0.6166,
      "step": 1810500
    },
    {
      "epoch": 16.521278925469012,
      "grad_norm": 3.763594627380371,
      "learning_rate": 3.6232267562109155e-05,
      "loss": 0.6588,
      "step": 1810600
    },
    {
      "epoch": 16.522191400832178,
      "grad_norm": 3.7642579078674316,
      "learning_rate": 3.623150716597319e-05,
      "loss": 0.6715,
      "step": 1810700
    },
    {
      "epoch": 16.523103876195343,
      "grad_norm": 3.9123454093933105,
      "learning_rate": 3.6230746769837215e-05,
      "loss": 0.6835,
      "step": 1810800
    },
    {
      "epoch": 16.524016351558508,
      "grad_norm": 3.770472764968872,
      "learning_rate": 3.6229986373701245e-05,
      "loss": 0.6621,
      "step": 1810900
    },
    {
      "epoch": 16.524928826921673,
      "grad_norm": 3.8785758018493652,
      "learning_rate": 3.6229225977565275e-05,
      "loss": 0.6629,
      "step": 1811000
    },
    {
      "epoch": 16.52584130228484,
      "grad_norm": 3.865971803665161,
      "learning_rate": 3.6228465581429305e-05,
      "loss": 0.6542,
      "step": 1811100
    },
    {
      "epoch": 16.526753777648004,
      "grad_norm": 2.804788112640381,
      "learning_rate": 3.622770518529333e-05,
      "loss": 0.6969,
      "step": 1811200
    },
    {
      "epoch": 16.52766625301117,
      "grad_norm": 4.193015098571777,
      "learning_rate": 3.6226944789157365e-05,
      "loss": 0.6786,
      "step": 1811300
    },
    {
      "epoch": 16.528578728374335,
      "grad_norm": 3.7134780883789062,
      "learning_rate": 3.622618439302139e-05,
      "loss": 0.6703,
      "step": 1811400
    },
    {
      "epoch": 16.5294912037375,
      "grad_norm": 4.331505298614502,
      "learning_rate": 3.622542399688542e-05,
      "loss": 0.677,
      "step": 1811500
    },
    {
      "epoch": 16.530403679100665,
      "grad_norm": 3.8293702602386475,
      "learning_rate": 3.622466360074945e-05,
      "loss": 0.6882,
      "step": 1811600
    },
    {
      "epoch": 16.53131615446383,
      "grad_norm": 2.393413543701172,
      "learning_rate": 3.622390320461348e-05,
      "loss": 0.6255,
      "step": 1811700
    },
    {
      "epoch": 16.532228629826996,
      "grad_norm": 3.0423383712768555,
      "learning_rate": 3.622314280847751e-05,
      "loss": 0.6249,
      "step": 1811800
    },
    {
      "epoch": 16.53314110519016,
      "grad_norm": 3.2958829402923584,
      "learning_rate": 3.622238241234154e-05,
      "loss": 0.6909,
      "step": 1811900
    },
    {
      "epoch": 16.534053580553326,
      "grad_norm": 3.9192492961883545,
      "learning_rate": 3.622162201620556e-05,
      "loss": 0.6462,
      "step": 1812000
    },
    {
      "epoch": 16.53496605591649,
      "grad_norm": 3.65053129196167,
      "learning_rate": 3.622086162006959e-05,
      "loss": 0.7075,
      "step": 1812100
    },
    {
      "epoch": 16.535878531279657,
      "grad_norm": 4.374756813049316,
      "learning_rate": 3.622010122393362e-05,
      "loss": 0.6497,
      "step": 1812200
    },
    {
      "epoch": 16.536791006642822,
      "grad_norm": 3.4049484729766846,
      "learning_rate": 3.6219340827797646e-05,
      "loss": 0.6559,
      "step": 1812300
    },
    {
      "epoch": 16.537703482005988,
      "grad_norm": 3.7939682006835938,
      "learning_rate": 3.621858043166168e-05,
      "loss": 0.654,
      "step": 1812400
    },
    {
      "epoch": 16.53861595736915,
      "grad_norm": 4.3777995109558105,
      "learning_rate": 3.6217820035525706e-05,
      "loss": 0.662,
      "step": 1812500
    },
    {
      "epoch": 16.539528432732315,
      "grad_norm": 4.764039993286133,
      "learning_rate": 3.6217059639389736e-05,
      "loss": 0.6948,
      "step": 1812600
    },
    {
      "epoch": 16.54044090809548,
      "grad_norm": 3.9394118785858154,
      "learning_rate": 3.6216299243253766e-05,
      "loss": 0.6364,
      "step": 1812700
    },
    {
      "epoch": 16.541353383458645,
      "grad_norm": 5.472390174865723,
      "learning_rate": 3.6215538847117796e-05,
      "loss": 0.6277,
      "step": 1812800
    },
    {
      "epoch": 16.54226585882181,
      "grad_norm": 3.8939194679260254,
      "learning_rate": 3.6214778450981826e-05,
      "loss": 0.6295,
      "step": 1812900
    },
    {
      "epoch": 16.543178334184976,
      "grad_norm": 4.167966842651367,
      "learning_rate": 3.6214018054845856e-05,
      "loss": 0.6296,
      "step": 1813000
    },
    {
      "epoch": 16.54409080954814,
      "grad_norm": 3.78377103805542,
      "learning_rate": 3.621325765870988e-05,
      "loss": 0.6899,
      "step": 1813100
    },
    {
      "epoch": 16.545003284911306,
      "grad_norm": 2.776106595993042,
      "learning_rate": 3.6212497262573916e-05,
      "loss": 0.6905,
      "step": 1813200
    },
    {
      "epoch": 16.54591576027447,
      "grad_norm": 3.6640453338623047,
      "learning_rate": 3.621173686643794e-05,
      "loss": 0.6663,
      "step": 1813300
    },
    {
      "epoch": 16.546828235637637,
      "grad_norm": 3.9584829807281494,
      "learning_rate": 3.621097647030197e-05,
      "loss": 0.6775,
      "step": 1813400
    },
    {
      "epoch": 16.547740711000802,
      "grad_norm": 3.8664350509643555,
      "learning_rate": 3.6210216074166e-05,
      "loss": 0.6477,
      "step": 1813500
    },
    {
      "epoch": 16.548653186363968,
      "grad_norm": 4.416752815246582,
      "learning_rate": 3.620945567803003e-05,
      "loss": 0.6541,
      "step": 1813600
    },
    {
      "epoch": 16.549565661727133,
      "grad_norm": 3.0376901626586914,
      "learning_rate": 3.620869528189405e-05,
      "loss": 0.6967,
      "step": 1813700
    },
    {
      "epoch": 16.5504781370903,
      "grad_norm": 3.6395485401153564,
      "learning_rate": 3.620793488575809e-05,
      "loss": 0.7024,
      "step": 1813800
    },
    {
      "epoch": 16.551390612453464,
      "grad_norm": 4.552759647369385,
      "learning_rate": 3.620717448962211e-05,
      "loss": 0.6814,
      "step": 1813900
    },
    {
      "epoch": 16.55230308781663,
      "grad_norm": 4.13929557800293,
      "learning_rate": 3.620641409348614e-05,
      "loss": 0.6811,
      "step": 1814000
    },
    {
      "epoch": 16.553215563179794,
      "grad_norm": 3.7271127700805664,
      "learning_rate": 3.620565369735017e-05,
      "loss": 0.6754,
      "step": 1814100
    },
    {
      "epoch": 16.55412803854296,
      "grad_norm": 2.938524007797241,
      "learning_rate": 3.62048933012142e-05,
      "loss": 0.6888,
      "step": 1814200
    },
    {
      "epoch": 16.555040513906125,
      "grad_norm": 3.681654214859009,
      "learning_rate": 3.6204132905078233e-05,
      "loss": 0.7079,
      "step": 1814300
    },
    {
      "epoch": 16.55595298926929,
      "grad_norm": 3.30900502204895,
      "learning_rate": 3.6203372508942263e-05,
      "loss": 0.6595,
      "step": 1814400
    },
    {
      "epoch": 16.556865464632455,
      "grad_norm": 3.6528587341308594,
      "learning_rate": 3.620261211280629e-05,
      "loss": 0.6663,
      "step": 1814500
    },
    {
      "epoch": 16.55777793999562,
      "grad_norm": 3.870917797088623,
      "learning_rate": 3.6201851716670324e-05,
      "loss": 0.6447,
      "step": 1814600
    },
    {
      "epoch": 16.558690415358786,
      "grad_norm": 3.282224178314209,
      "learning_rate": 3.620109132053435e-05,
      "loss": 0.6729,
      "step": 1814700
    },
    {
      "epoch": 16.55960289072195,
      "grad_norm": 3.309330463409424,
      "learning_rate": 3.620033092439838e-05,
      "loss": 0.6866,
      "step": 1814800
    },
    {
      "epoch": 16.560515366085117,
      "grad_norm": 3.7873823642730713,
      "learning_rate": 3.619957052826241e-05,
      "loss": 0.6706,
      "step": 1814900
    },
    {
      "epoch": 16.561427841448282,
      "grad_norm": 4.037627220153809,
      "learning_rate": 3.619881013212643e-05,
      "loss": 0.6511,
      "step": 1815000
    },
    {
      "epoch": 16.562340316811447,
      "grad_norm": 2.9587090015411377,
      "learning_rate": 3.619804973599046e-05,
      "loss": 0.6704,
      "step": 1815100
    },
    {
      "epoch": 16.563252792174612,
      "grad_norm": 4.5646257400512695,
      "learning_rate": 3.619728933985449e-05,
      "loss": 0.6489,
      "step": 1815200
    },
    {
      "epoch": 16.564165267537778,
      "grad_norm": 4.013587951660156,
      "learning_rate": 3.619652894371852e-05,
      "loss": 0.6787,
      "step": 1815300
    },
    {
      "epoch": 16.565077742900943,
      "grad_norm": 3.38539719581604,
      "learning_rate": 3.619576854758255e-05,
      "loss": 0.6176,
      "step": 1815400
    },
    {
      "epoch": 16.56599021826411,
      "grad_norm": 3.394801139831543,
      "learning_rate": 3.619500815144658e-05,
      "loss": 0.6757,
      "step": 1815500
    },
    {
      "epoch": 16.566902693627274,
      "grad_norm": 4.104963779449463,
      "learning_rate": 3.6194247755310604e-05,
      "loss": 0.6675,
      "step": 1815600
    },
    {
      "epoch": 16.56781516899044,
      "grad_norm": 3.314877986907959,
      "learning_rate": 3.619348735917464e-05,
      "loss": 0.6682,
      "step": 1815700
    },
    {
      "epoch": 16.5687276443536,
      "grad_norm": 3.9498140811920166,
      "learning_rate": 3.6192726963038664e-05,
      "loss": 0.6715,
      "step": 1815800
    },
    {
      "epoch": 16.569640119716766,
      "grad_norm": 3.9509334564208984,
      "learning_rate": 3.6191966566902694e-05,
      "loss": 0.6721,
      "step": 1815900
    },
    {
      "epoch": 16.57055259507993,
      "grad_norm": 3.3454864025115967,
      "learning_rate": 3.6191206170766724e-05,
      "loss": 0.6322,
      "step": 1816000
    },
    {
      "epoch": 16.571465070443097,
      "grad_norm": 3.343635320663452,
      "learning_rate": 3.6190445774630754e-05,
      "loss": 0.6084,
      "step": 1816100
    },
    {
      "epoch": 16.572377545806262,
      "grad_norm": 4.365533828735352,
      "learning_rate": 3.618968537849478e-05,
      "loss": 0.6555,
      "step": 1816200
    },
    {
      "epoch": 16.573290021169427,
      "grad_norm": 4.1509833335876465,
      "learning_rate": 3.6188924982358814e-05,
      "loss": 0.6965,
      "step": 1816300
    },
    {
      "epoch": 16.574202496532592,
      "grad_norm": 3.8327691555023193,
      "learning_rate": 3.618816458622284e-05,
      "loss": 0.6619,
      "step": 1816400
    },
    {
      "epoch": 16.575114971895758,
      "grad_norm": 6.0530171394348145,
      "learning_rate": 3.618740419008687e-05,
      "loss": 0.6814,
      "step": 1816500
    },
    {
      "epoch": 16.576027447258923,
      "grad_norm": 4.580135822296143,
      "learning_rate": 3.61866437939509e-05,
      "loss": 0.6554,
      "step": 1816600
    },
    {
      "epoch": 16.57693992262209,
      "grad_norm": 4.362993240356445,
      "learning_rate": 3.618588339781493e-05,
      "loss": 0.6667,
      "step": 1816700
    },
    {
      "epoch": 16.577852397985254,
      "grad_norm": 4.23578405380249,
      "learning_rate": 3.618512300167896e-05,
      "loss": 0.6413,
      "step": 1816800
    },
    {
      "epoch": 16.57876487334842,
      "grad_norm": 4.167098045349121,
      "learning_rate": 3.618436260554299e-05,
      "loss": 0.6315,
      "step": 1816900
    },
    {
      "epoch": 16.579677348711584,
      "grad_norm": 3.2949066162109375,
      "learning_rate": 3.618360220940701e-05,
      "loss": 0.6621,
      "step": 1817000
    },
    {
      "epoch": 16.58058982407475,
      "grad_norm": 4.071094036102295,
      "learning_rate": 3.618284181327105e-05,
      "loss": 0.6433,
      "step": 1817100
    },
    {
      "epoch": 16.581502299437915,
      "grad_norm": 4.106357574462891,
      "learning_rate": 3.618208141713507e-05,
      "loss": 0.6968,
      "step": 1817200
    },
    {
      "epoch": 16.58241477480108,
      "grad_norm": 4.23779821395874,
      "learning_rate": 3.61813210209991e-05,
      "loss": 0.6834,
      "step": 1817300
    },
    {
      "epoch": 16.583327250164245,
      "grad_norm": 3.199577569961548,
      "learning_rate": 3.618056062486313e-05,
      "loss": 0.6387,
      "step": 1817400
    },
    {
      "epoch": 16.58423972552741,
      "grad_norm": 4.56257438659668,
      "learning_rate": 3.617980022872716e-05,
      "loss": 0.6122,
      "step": 1817500
    },
    {
      "epoch": 16.585152200890576,
      "grad_norm": 2.1445345878601074,
      "learning_rate": 3.6179039832591185e-05,
      "loss": 0.6861,
      "step": 1817600
    },
    {
      "epoch": 16.58606467625374,
      "grad_norm": 3.791463851928711,
      "learning_rate": 3.6178279436455215e-05,
      "loss": 0.6731,
      "step": 1817700
    },
    {
      "epoch": 16.586977151616907,
      "grad_norm": 3.319072961807251,
      "learning_rate": 3.6177519040319245e-05,
      "loss": 0.6852,
      "step": 1817800
    },
    {
      "epoch": 16.587889626980072,
      "grad_norm": 3.7043511867523193,
      "learning_rate": 3.6176758644183275e-05,
      "loss": 0.6796,
      "step": 1817900
    },
    {
      "epoch": 16.588802102343237,
      "grad_norm": 4.240328788757324,
      "learning_rate": 3.6175998248047305e-05,
      "loss": 0.6684,
      "step": 1818000
    },
    {
      "epoch": 16.589714577706403,
      "grad_norm": 2.887139081954956,
      "learning_rate": 3.617523785191133e-05,
      "loss": 0.6751,
      "step": 1818100
    },
    {
      "epoch": 16.590627053069568,
      "grad_norm": 4.048987865447998,
      "learning_rate": 3.6174477455775365e-05,
      "loss": 0.6801,
      "step": 1818200
    },
    {
      "epoch": 16.591539528432733,
      "grad_norm": 3.7041139602661133,
      "learning_rate": 3.617371705963939e-05,
      "loss": 0.6712,
      "step": 1818300
    },
    {
      "epoch": 16.5924520037959,
      "grad_norm": 4.013655662536621,
      "learning_rate": 3.617295666350342e-05,
      "loss": 0.651,
      "step": 1818400
    },
    {
      "epoch": 16.593364479159064,
      "grad_norm": 3.3166375160217285,
      "learning_rate": 3.617219626736745e-05,
      "loss": 0.6637,
      "step": 1818500
    },
    {
      "epoch": 16.59427695452223,
      "grad_norm": 3.432854652404785,
      "learning_rate": 3.617143587123148e-05,
      "loss": 0.6453,
      "step": 1818600
    },
    {
      "epoch": 16.595189429885394,
      "grad_norm": 4.182093620300293,
      "learning_rate": 3.617067547509551e-05,
      "loss": 0.691,
      "step": 1818700
    },
    {
      "epoch": 16.59610190524856,
      "grad_norm": 4.374001979827881,
      "learning_rate": 3.616991507895954e-05,
      "loss": 0.6701,
      "step": 1818800
    },
    {
      "epoch": 16.597014380611725,
      "grad_norm": 4.229606628417969,
      "learning_rate": 3.616915468282356e-05,
      "loss": 0.6562,
      "step": 1818900
    },
    {
      "epoch": 16.59792685597489,
      "grad_norm": 2.956571102142334,
      "learning_rate": 3.61683942866876e-05,
      "loss": 0.6825,
      "step": 1819000
    },
    {
      "epoch": 16.598839331338056,
      "grad_norm": 3.7674505710601807,
      "learning_rate": 3.616763389055162e-05,
      "loss": 0.7114,
      "step": 1819100
    },
    {
      "epoch": 16.59975180670122,
      "grad_norm": 4.757289886474609,
      "learning_rate": 3.616687349441565e-05,
      "loss": 0.6853,
      "step": 1819200
    },
    {
      "epoch": 16.600664282064383,
      "grad_norm": 3.279299736022949,
      "learning_rate": 3.616611309827968e-05,
      "loss": 0.6595,
      "step": 1819300
    },
    {
      "epoch": 16.601576757427548,
      "grad_norm": 4.015466213226318,
      "learning_rate": 3.616535270214371e-05,
      "loss": 0.6606,
      "step": 1819400
    },
    {
      "epoch": 16.602489232790713,
      "grad_norm": 5.656858444213867,
      "learning_rate": 3.6164592306007736e-05,
      "loss": 0.6832,
      "step": 1819500
    },
    {
      "epoch": 16.60340170815388,
      "grad_norm": 4.173622131347656,
      "learning_rate": 3.616383190987177e-05,
      "loss": 0.6716,
      "step": 1819600
    },
    {
      "epoch": 16.604314183517044,
      "grad_norm": 2.7038347721099854,
      "learning_rate": 3.6163071513735796e-05,
      "loss": 0.6658,
      "step": 1819700
    },
    {
      "epoch": 16.60522665888021,
      "grad_norm": 4.9942169189453125,
      "learning_rate": 3.6162311117599826e-05,
      "loss": 0.6494,
      "step": 1819800
    },
    {
      "epoch": 16.606139134243374,
      "grad_norm": 3.847761631011963,
      "learning_rate": 3.6161550721463856e-05,
      "loss": 0.6912,
      "step": 1819900
    },
    {
      "epoch": 16.60705160960654,
      "grad_norm": 3.3752589225769043,
      "learning_rate": 3.6160790325327886e-05,
      "loss": 0.6439,
      "step": 1820000
    },
    {
      "epoch": 16.607964084969705,
      "grad_norm": 4.024267673492432,
      "learning_rate": 3.6160029929191916e-05,
      "loss": 0.6628,
      "step": 1820100
    },
    {
      "epoch": 16.60887656033287,
      "grad_norm": 3.2719297409057617,
      "learning_rate": 3.6159269533055946e-05,
      "loss": 0.6709,
      "step": 1820200
    },
    {
      "epoch": 16.609789035696036,
      "grad_norm": 3.8885581493377686,
      "learning_rate": 3.615850913691997e-05,
      "loss": 0.6668,
      "step": 1820300
    },
    {
      "epoch": 16.6107015110592,
      "grad_norm": 5.1214518547058105,
      "learning_rate": 3.6157748740784007e-05,
      "loss": 0.6768,
      "step": 1820400
    },
    {
      "epoch": 16.611613986422366,
      "grad_norm": 3.5628767013549805,
      "learning_rate": 3.615698834464803e-05,
      "loss": 0.7024,
      "step": 1820500
    },
    {
      "epoch": 16.61252646178553,
      "grad_norm": 3.190680503845215,
      "learning_rate": 3.615622794851205e-05,
      "loss": 0.6992,
      "step": 1820600
    },
    {
      "epoch": 16.613438937148697,
      "grad_norm": 5.024055480957031,
      "learning_rate": 3.615546755237609e-05,
      "loss": 0.6204,
      "step": 1820700
    },
    {
      "epoch": 16.614351412511862,
      "grad_norm": 3.8002421855926514,
      "learning_rate": 3.615470715624011e-05,
      "loss": 0.6771,
      "step": 1820800
    },
    {
      "epoch": 16.615263887875027,
      "grad_norm": 3.8631279468536377,
      "learning_rate": 3.615394676010414e-05,
      "loss": 0.6319,
      "step": 1820900
    },
    {
      "epoch": 16.616176363238193,
      "grad_norm": 3.501523733139038,
      "learning_rate": 3.615318636396817e-05,
      "loss": 0.6463,
      "step": 1821000
    },
    {
      "epoch": 16.617088838601358,
      "grad_norm": 4.2790207862854,
      "learning_rate": 3.6152425967832203e-05,
      "loss": 0.7201,
      "step": 1821100
    },
    {
      "epoch": 16.618001313964523,
      "grad_norm": 4.07029914855957,
      "learning_rate": 3.6151665571696234e-05,
      "loss": 0.6568,
      "step": 1821200
    },
    {
      "epoch": 16.61891378932769,
      "grad_norm": 4.7118377685546875,
      "learning_rate": 3.6150905175560264e-05,
      "loss": 0.6255,
      "step": 1821300
    },
    {
      "epoch": 16.619826264690854,
      "grad_norm": 5.36543083190918,
      "learning_rate": 3.615014477942429e-05,
      "loss": 0.6634,
      "step": 1821400
    },
    {
      "epoch": 16.62073874005402,
      "grad_norm": 4.417015552520752,
      "learning_rate": 3.6149384383288324e-05,
      "loss": 0.6784,
      "step": 1821500
    },
    {
      "epoch": 16.621651215417184,
      "grad_norm": 3.4668266773223877,
      "learning_rate": 3.614862398715235e-05,
      "loss": 0.6869,
      "step": 1821600
    },
    {
      "epoch": 16.62256369078035,
      "grad_norm": 3.4352574348449707,
      "learning_rate": 3.614786359101638e-05,
      "loss": 0.6349,
      "step": 1821700
    },
    {
      "epoch": 16.623476166143515,
      "grad_norm": 3.288814067840576,
      "learning_rate": 3.614710319488041e-05,
      "loss": 0.6622,
      "step": 1821800
    },
    {
      "epoch": 16.62438864150668,
      "grad_norm": 3.858609676361084,
      "learning_rate": 3.614634279874444e-05,
      "loss": 0.6492,
      "step": 1821900
    },
    {
      "epoch": 16.625301116869846,
      "grad_norm": 3.566006660461426,
      "learning_rate": 3.614558240260846e-05,
      "loss": 0.6576,
      "step": 1822000
    },
    {
      "epoch": 16.62621359223301,
      "grad_norm": 3.834266424179077,
      "learning_rate": 3.61448220064725e-05,
      "loss": 0.6577,
      "step": 1822100
    },
    {
      "epoch": 16.627126067596176,
      "grad_norm": 3.9142262935638428,
      "learning_rate": 3.614406161033652e-05,
      "loss": 0.6608,
      "step": 1822200
    },
    {
      "epoch": 16.62803854295934,
      "grad_norm": 3.455409288406372,
      "learning_rate": 3.614330121420055e-05,
      "loss": 0.6568,
      "step": 1822300
    },
    {
      "epoch": 16.628951018322507,
      "grad_norm": 4.912857532501221,
      "learning_rate": 3.614254081806458e-05,
      "loss": 0.6644,
      "step": 1822400
    },
    {
      "epoch": 16.629863493685672,
      "grad_norm": 3.160543441772461,
      "learning_rate": 3.614178042192861e-05,
      "loss": 0.6856,
      "step": 1822500
    },
    {
      "epoch": 16.630775969048834,
      "grad_norm": 5.427046298980713,
      "learning_rate": 3.614102002579264e-05,
      "loss": 0.6456,
      "step": 1822600
    },
    {
      "epoch": 16.631688444412,
      "grad_norm": 3.5234568119049072,
      "learning_rate": 3.614025962965667e-05,
      "loss": 0.6666,
      "step": 1822700
    },
    {
      "epoch": 16.632600919775165,
      "grad_norm": 4.782942771911621,
      "learning_rate": 3.6139499233520694e-05,
      "loss": 0.6843,
      "step": 1822800
    },
    {
      "epoch": 16.63351339513833,
      "grad_norm": 3.8205313682556152,
      "learning_rate": 3.613873883738473e-05,
      "loss": 0.6523,
      "step": 1822900
    },
    {
      "epoch": 16.634425870501495,
      "grad_norm": 3.568260669708252,
      "learning_rate": 3.6137978441248754e-05,
      "loss": 0.6352,
      "step": 1823000
    },
    {
      "epoch": 16.63533834586466,
      "grad_norm": 3.7867026329040527,
      "learning_rate": 3.6137218045112784e-05,
      "loss": 0.6571,
      "step": 1823100
    },
    {
      "epoch": 16.636250821227826,
      "grad_norm": 3.997260570526123,
      "learning_rate": 3.6136457648976815e-05,
      "loss": 0.6611,
      "step": 1823200
    },
    {
      "epoch": 16.63716329659099,
      "grad_norm": 3.5560176372528076,
      "learning_rate": 3.6135697252840845e-05,
      "loss": 0.6674,
      "step": 1823300
    },
    {
      "epoch": 16.638075771954156,
      "grad_norm": 4.326803207397461,
      "learning_rate": 3.613493685670487e-05,
      "loss": 0.6938,
      "step": 1823400
    },
    {
      "epoch": 16.63898824731732,
      "grad_norm": 3.4297146797180176,
      "learning_rate": 3.61341764605689e-05,
      "loss": 0.639,
      "step": 1823500
    },
    {
      "epoch": 16.639900722680487,
      "grad_norm": 3.416651725769043,
      "learning_rate": 3.613341606443293e-05,
      "loss": 0.6838,
      "step": 1823600
    },
    {
      "epoch": 16.640813198043652,
      "grad_norm": 5.823789119720459,
      "learning_rate": 3.613265566829696e-05,
      "loss": 0.671,
      "step": 1823700
    },
    {
      "epoch": 16.641725673406818,
      "grad_norm": 3.4084298610687256,
      "learning_rate": 3.613189527216099e-05,
      "loss": 0.6523,
      "step": 1823800
    },
    {
      "epoch": 16.642638148769983,
      "grad_norm": 3.779512882232666,
      "learning_rate": 3.613113487602501e-05,
      "loss": 0.6674,
      "step": 1823900
    },
    {
      "epoch": 16.643550624133148,
      "grad_norm": 4.596780776977539,
      "learning_rate": 3.613037447988905e-05,
      "loss": 0.6515,
      "step": 1824000
    },
    {
      "epoch": 16.644463099496313,
      "grad_norm": 3.647454261779785,
      "learning_rate": 3.612961408375307e-05,
      "loss": 0.6571,
      "step": 1824100
    },
    {
      "epoch": 16.64537557485948,
      "grad_norm": 3.9313602447509766,
      "learning_rate": 3.61288536876171e-05,
      "loss": 0.6838,
      "step": 1824200
    },
    {
      "epoch": 16.646288050222644,
      "grad_norm": 2.927490234375,
      "learning_rate": 3.612809329148113e-05,
      "loss": 0.711,
      "step": 1824300
    },
    {
      "epoch": 16.64720052558581,
      "grad_norm": 4.443422317504883,
      "learning_rate": 3.612733289534516e-05,
      "loss": 0.6707,
      "step": 1824400
    },
    {
      "epoch": 16.648113000948975,
      "grad_norm": 3.9645233154296875,
      "learning_rate": 3.6126572499209185e-05,
      "loss": 0.6636,
      "step": 1824500
    },
    {
      "epoch": 16.64902547631214,
      "grad_norm": 4.340729236602783,
      "learning_rate": 3.612581210307322e-05,
      "loss": 0.6385,
      "step": 1824600
    },
    {
      "epoch": 16.649937951675305,
      "grad_norm": 4.526071548461914,
      "learning_rate": 3.6125051706937245e-05,
      "loss": 0.701,
      "step": 1824700
    },
    {
      "epoch": 16.65085042703847,
      "grad_norm": 3.408564805984497,
      "learning_rate": 3.6124291310801275e-05,
      "loss": 0.6634,
      "step": 1824800
    },
    {
      "epoch": 16.651762902401636,
      "grad_norm": 4.707152366638184,
      "learning_rate": 3.6123530914665305e-05,
      "loss": 0.6631,
      "step": 1824900
    },
    {
      "epoch": 16.6526753777648,
      "grad_norm": 4.561916828155518,
      "learning_rate": 3.6122770518529335e-05,
      "loss": 0.6577,
      "step": 1825000
    },
    {
      "epoch": 16.653587853127966,
      "grad_norm": 4.500221252441406,
      "learning_rate": 3.6122010122393365e-05,
      "loss": 0.6732,
      "step": 1825100
    },
    {
      "epoch": 16.65450032849113,
      "grad_norm": 3.3684194087982178,
      "learning_rate": 3.6121249726257396e-05,
      "loss": 0.6815,
      "step": 1825200
    },
    {
      "epoch": 16.655412803854297,
      "grad_norm": 3.877946138381958,
      "learning_rate": 3.612048933012142e-05,
      "loss": 0.6685,
      "step": 1825300
    },
    {
      "epoch": 16.656325279217462,
      "grad_norm": 4.548535346984863,
      "learning_rate": 3.6119728933985456e-05,
      "loss": 0.6725,
      "step": 1825400
    },
    {
      "epoch": 16.657237754580628,
      "grad_norm": 3.9889302253723145,
      "learning_rate": 3.611896853784948e-05,
      "loss": 0.6394,
      "step": 1825500
    },
    {
      "epoch": 16.658150229943793,
      "grad_norm": 5.472930431365967,
      "learning_rate": 3.611820814171351e-05,
      "loss": 0.636,
      "step": 1825600
    },
    {
      "epoch": 16.659062705306958,
      "grad_norm": 4.0107035636901855,
      "learning_rate": 3.611744774557754e-05,
      "loss": 0.6847,
      "step": 1825700
    },
    {
      "epoch": 16.659975180670124,
      "grad_norm": 4.2910308837890625,
      "learning_rate": 3.611668734944157e-05,
      "loss": 0.6693,
      "step": 1825800
    },
    {
      "epoch": 16.66088765603329,
      "grad_norm": 3.4402947425842285,
      "learning_rate": 3.611592695330559e-05,
      "loss": 0.6276,
      "step": 1825900
    },
    {
      "epoch": 16.661800131396454,
      "grad_norm": 4.25643253326416,
      "learning_rate": 3.611516655716963e-05,
      "loss": 0.6399,
      "step": 1826000
    },
    {
      "epoch": 16.662712606759616,
      "grad_norm": 3.626253128051758,
      "learning_rate": 3.611440616103365e-05,
      "loss": 0.6801,
      "step": 1826100
    },
    {
      "epoch": 16.66362508212278,
      "grad_norm": 3.93656325340271,
      "learning_rate": 3.611364576489768e-05,
      "loss": 0.6674,
      "step": 1826200
    },
    {
      "epoch": 16.664537557485946,
      "grad_norm": 4.431840896606445,
      "learning_rate": 3.611288536876171e-05,
      "loss": 0.6883,
      "step": 1826300
    },
    {
      "epoch": 16.66545003284911,
      "grad_norm": 3.4151415824890137,
      "learning_rate": 3.6112124972625736e-05,
      "loss": 0.6766,
      "step": 1826400
    },
    {
      "epoch": 16.666362508212277,
      "grad_norm": 4.1855010986328125,
      "learning_rate": 3.611136457648977e-05,
      "loss": 0.7037,
      "step": 1826500
    },
    {
      "epoch": 16.667274983575442,
      "grad_norm": 4.202958106994629,
      "learning_rate": 3.6110604180353796e-05,
      "loss": 0.6766,
      "step": 1826600
    },
    {
      "epoch": 16.668187458938608,
      "grad_norm": 2.7226288318634033,
      "learning_rate": 3.6109843784217826e-05,
      "loss": 0.659,
      "step": 1826700
    },
    {
      "epoch": 16.669099934301773,
      "grad_norm": 3.8141987323760986,
      "learning_rate": 3.6109083388081856e-05,
      "loss": 0.6975,
      "step": 1826800
    },
    {
      "epoch": 16.67001240966494,
      "grad_norm": 3.866882801055908,
      "learning_rate": 3.6108322991945886e-05,
      "loss": 0.6436,
      "step": 1826900
    },
    {
      "epoch": 16.670924885028104,
      "grad_norm": 4.072603225708008,
      "learning_rate": 3.610756259580991e-05,
      "loss": 0.7017,
      "step": 1827000
    },
    {
      "epoch": 16.67183736039127,
      "grad_norm": 4.4598894119262695,
      "learning_rate": 3.6106802199673947e-05,
      "loss": 0.681,
      "step": 1827100
    },
    {
      "epoch": 16.672749835754434,
      "grad_norm": 3.5035274028778076,
      "learning_rate": 3.610604180353797e-05,
      "loss": 0.6566,
      "step": 1827200
    },
    {
      "epoch": 16.6736623111176,
      "grad_norm": 3.900183916091919,
      "learning_rate": 3.6105281407402e-05,
      "loss": 0.6638,
      "step": 1827300
    },
    {
      "epoch": 16.674574786480765,
      "grad_norm": 4.354575157165527,
      "learning_rate": 3.610452101126603e-05,
      "loss": 0.7042,
      "step": 1827400
    },
    {
      "epoch": 16.67548726184393,
      "grad_norm": 3.5360190868377686,
      "learning_rate": 3.610376061513006e-05,
      "loss": 0.6737,
      "step": 1827500
    },
    {
      "epoch": 16.676399737207095,
      "grad_norm": 4.073783874511719,
      "learning_rate": 3.610300021899409e-05,
      "loss": 0.7015,
      "step": 1827600
    },
    {
      "epoch": 16.67731221257026,
      "grad_norm": 3.398996353149414,
      "learning_rate": 3.610223982285812e-05,
      "loss": 0.6771,
      "step": 1827700
    },
    {
      "epoch": 16.678224687933426,
      "grad_norm": 4.995832443237305,
      "learning_rate": 3.6101479426722143e-05,
      "loss": 0.6525,
      "step": 1827800
    },
    {
      "epoch": 16.67913716329659,
      "grad_norm": 4.272719860076904,
      "learning_rate": 3.610071903058618e-05,
      "loss": 0.6741,
      "step": 1827900
    },
    {
      "epoch": 16.680049638659757,
      "grad_norm": 3.6208744049072266,
      "learning_rate": 3.6099958634450204e-05,
      "loss": 0.6832,
      "step": 1828000
    },
    {
      "epoch": 16.680962114022922,
      "grad_norm": 3.109448194503784,
      "learning_rate": 3.6099198238314234e-05,
      "loss": 0.6233,
      "step": 1828100
    },
    {
      "epoch": 16.681874589386087,
      "grad_norm": 3.3074262142181396,
      "learning_rate": 3.6098437842178264e-05,
      "loss": 0.6378,
      "step": 1828200
    },
    {
      "epoch": 16.682787064749252,
      "grad_norm": 3.773228406906128,
      "learning_rate": 3.6097677446042294e-05,
      "loss": 0.6779,
      "step": 1828300
    },
    {
      "epoch": 16.683699540112418,
      "grad_norm": 2.7936270236968994,
      "learning_rate": 3.609691704990632e-05,
      "loss": 0.6855,
      "step": 1828400
    },
    {
      "epoch": 16.684612015475583,
      "grad_norm": 3.105555772781372,
      "learning_rate": 3.6096156653770354e-05,
      "loss": 0.6487,
      "step": 1828500
    },
    {
      "epoch": 16.68552449083875,
      "grad_norm": 3.6529648303985596,
      "learning_rate": 3.609539625763438e-05,
      "loss": 0.6776,
      "step": 1828600
    },
    {
      "epoch": 16.686436966201914,
      "grad_norm": 4.692626476287842,
      "learning_rate": 3.609463586149841e-05,
      "loss": 0.6102,
      "step": 1828700
    },
    {
      "epoch": 16.68734944156508,
      "grad_norm": 4.297533988952637,
      "learning_rate": 3.609387546536244e-05,
      "loss": 0.6406,
      "step": 1828800
    },
    {
      "epoch": 16.688261916928244,
      "grad_norm": 4.346888542175293,
      "learning_rate": 3.609311506922647e-05,
      "loss": 0.6442,
      "step": 1828900
    },
    {
      "epoch": 16.68917439229141,
      "grad_norm": 3.759458065032959,
      "learning_rate": 3.60923546730905e-05,
      "loss": 0.662,
      "step": 1829000
    },
    {
      "epoch": 16.690086867654575,
      "grad_norm": 4.541813373565674,
      "learning_rate": 3.609159427695452e-05,
      "loss": 0.6057,
      "step": 1829100
    },
    {
      "epoch": 16.69099934301774,
      "grad_norm": 3.2995338439941406,
      "learning_rate": 3.609083388081855e-05,
      "loss": 0.6505,
      "step": 1829200
    },
    {
      "epoch": 16.691911818380905,
      "grad_norm": 4.294292449951172,
      "learning_rate": 3.609007348468258e-05,
      "loss": 0.6611,
      "step": 1829300
    },
    {
      "epoch": 16.692824293744067,
      "grad_norm": 4.625403881072998,
      "learning_rate": 3.608931308854661e-05,
      "loss": 0.6554,
      "step": 1829400
    },
    {
      "epoch": 16.693736769107232,
      "grad_norm": 4.2970991134643555,
      "learning_rate": 3.608855269241064e-05,
      "loss": 0.6422,
      "step": 1829500
    },
    {
      "epoch": 16.694649244470398,
      "grad_norm": 4.299435615539551,
      "learning_rate": 3.608779229627467e-05,
      "loss": 0.6598,
      "step": 1829600
    },
    {
      "epoch": 16.695561719833563,
      "grad_norm": 2.0976624488830566,
      "learning_rate": 3.6087031900138694e-05,
      "loss": 0.6803,
      "step": 1829700
    },
    {
      "epoch": 16.69647419519673,
      "grad_norm": 3.6497578620910645,
      "learning_rate": 3.6086271504002724e-05,
      "loss": 0.6638,
      "step": 1829800
    },
    {
      "epoch": 16.697386670559894,
      "grad_norm": 3.2128617763519287,
      "learning_rate": 3.6085511107866755e-05,
      "loss": 0.6586,
      "step": 1829900
    },
    {
      "epoch": 16.69829914592306,
      "grad_norm": 3.702617645263672,
      "learning_rate": 3.6084750711730785e-05,
      "loss": 0.6649,
      "step": 1830000
    },
    {
      "epoch": 16.699211621286224,
      "grad_norm": 3.472618579864502,
      "learning_rate": 3.6083990315594815e-05,
      "loss": 0.6346,
      "step": 1830100
    },
    {
      "epoch": 16.70012409664939,
      "grad_norm": 4.377656936645508,
      "learning_rate": 3.6083229919458845e-05,
      "loss": 0.6605,
      "step": 1830200
    },
    {
      "epoch": 16.701036572012555,
      "grad_norm": 3.7280845642089844,
      "learning_rate": 3.608246952332287e-05,
      "loss": 0.6565,
      "step": 1830300
    },
    {
      "epoch": 16.70194904737572,
      "grad_norm": 4.307044506072998,
      "learning_rate": 3.6081709127186905e-05,
      "loss": 0.6722,
      "step": 1830400
    },
    {
      "epoch": 16.702861522738885,
      "grad_norm": 4.1580634117126465,
      "learning_rate": 3.608094873105093e-05,
      "loss": 0.7001,
      "step": 1830500
    },
    {
      "epoch": 16.70377399810205,
      "grad_norm": 3.833078384399414,
      "learning_rate": 3.608018833491496e-05,
      "loss": 0.6889,
      "step": 1830600
    },
    {
      "epoch": 16.704686473465216,
      "grad_norm": 3.4454431533813477,
      "learning_rate": 3.607942793877899e-05,
      "loss": 0.6161,
      "step": 1830700
    },
    {
      "epoch": 16.70559894882838,
      "grad_norm": 3.971748113632202,
      "learning_rate": 3.607866754264302e-05,
      "loss": 0.655,
      "step": 1830800
    },
    {
      "epoch": 16.706511424191547,
      "grad_norm": 4.112463474273682,
      "learning_rate": 3.607790714650705e-05,
      "loss": 0.674,
      "step": 1830900
    },
    {
      "epoch": 16.707423899554712,
      "grad_norm": 4.220448970794678,
      "learning_rate": 3.607714675037108e-05,
      "loss": 0.6386,
      "step": 1831000
    },
    {
      "epoch": 16.708336374917877,
      "grad_norm": 5.026934623718262,
      "learning_rate": 3.60763863542351e-05,
      "loss": 0.6292,
      "step": 1831100
    },
    {
      "epoch": 16.709248850281043,
      "grad_norm": 5.038518905639648,
      "learning_rate": 3.607562595809913e-05,
      "loss": 0.689,
      "step": 1831200
    },
    {
      "epoch": 16.710161325644208,
      "grad_norm": 3.0442018508911133,
      "learning_rate": 3.607486556196316e-05,
      "loss": 0.6725,
      "step": 1831300
    },
    {
      "epoch": 16.711073801007373,
      "grad_norm": 3.6289680004119873,
      "learning_rate": 3.607410516582719e-05,
      "loss": 0.7157,
      "step": 1831400
    },
    {
      "epoch": 16.71198627637054,
      "grad_norm": 3.9348270893096924,
      "learning_rate": 3.607334476969122e-05,
      "loss": 0.6672,
      "step": 1831500
    },
    {
      "epoch": 16.712898751733704,
      "grad_norm": 2.2099077701568604,
      "learning_rate": 3.607258437355525e-05,
      "loss": 0.664,
      "step": 1831600
    },
    {
      "epoch": 16.71381122709687,
      "grad_norm": 4.138378620147705,
      "learning_rate": 3.6071823977419275e-05,
      "loss": 0.6575,
      "step": 1831700
    },
    {
      "epoch": 16.714723702460034,
      "grad_norm": 3.6532483100891113,
      "learning_rate": 3.607106358128331e-05,
      "loss": 0.6651,
      "step": 1831800
    },
    {
      "epoch": 16.7156361778232,
      "grad_norm": 5.3977370262146,
      "learning_rate": 3.6070303185147336e-05,
      "loss": 0.6831,
      "step": 1831900
    },
    {
      "epoch": 16.716548653186365,
      "grad_norm": 4.059940338134766,
      "learning_rate": 3.6069542789011366e-05,
      "loss": 0.6536,
      "step": 1832000
    },
    {
      "epoch": 16.71746112854953,
      "grad_norm": 4.146424293518066,
      "learning_rate": 3.6068782392875396e-05,
      "loss": 0.6627,
      "step": 1832100
    },
    {
      "epoch": 16.718373603912696,
      "grad_norm": 4.261003017425537,
      "learning_rate": 3.606802199673942e-05,
      "loss": 0.7079,
      "step": 1832200
    },
    {
      "epoch": 16.71928607927586,
      "grad_norm": 3.6120290756225586,
      "learning_rate": 3.6067261600603456e-05,
      "loss": 0.6405,
      "step": 1832300
    },
    {
      "epoch": 16.720198554639026,
      "grad_norm": 4.374270915985107,
      "learning_rate": 3.606650120446748e-05,
      "loss": 0.6651,
      "step": 1832400
    },
    {
      "epoch": 16.72111103000219,
      "grad_norm": 4.244143009185791,
      "learning_rate": 3.606574080833151e-05,
      "loss": 0.6494,
      "step": 1832500
    },
    {
      "epoch": 16.722023505365357,
      "grad_norm": 4.142749786376953,
      "learning_rate": 3.606498041219554e-05,
      "loss": 0.6857,
      "step": 1832600
    },
    {
      "epoch": 16.722935980728522,
      "grad_norm": 4.2682294845581055,
      "learning_rate": 3.606422001605957e-05,
      "loss": 0.6276,
      "step": 1832700
    },
    {
      "epoch": 16.723848456091687,
      "grad_norm": 4.373926639556885,
      "learning_rate": 3.606345961992359e-05,
      "loss": 0.7075,
      "step": 1832800
    },
    {
      "epoch": 16.72476093145485,
      "grad_norm": 3.434051275253296,
      "learning_rate": 3.606269922378763e-05,
      "loss": 0.6772,
      "step": 1832900
    },
    {
      "epoch": 16.725673406818014,
      "grad_norm": 3.1598575115203857,
      "learning_rate": 3.606193882765165e-05,
      "loss": 0.6444,
      "step": 1833000
    },
    {
      "epoch": 16.72658588218118,
      "grad_norm": 4.411492824554443,
      "learning_rate": 3.606117843151568e-05,
      "loss": 0.6659,
      "step": 1833100
    },
    {
      "epoch": 16.727498357544345,
      "grad_norm": 3.7162177562713623,
      "learning_rate": 3.606041803537971e-05,
      "loss": 0.689,
      "step": 1833200
    },
    {
      "epoch": 16.72841083290751,
      "grad_norm": 4.0033416748046875,
      "learning_rate": 3.605965763924374e-05,
      "loss": 0.6448,
      "step": 1833300
    },
    {
      "epoch": 16.729323308270676,
      "grad_norm": 3.7188682556152344,
      "learning_rate": 3.605889724310777e-05,
      "loss": 0.6548,
      "step": 1833400
    },
    {
      "epoch": 16.73023578363384,
      "grad_norm": 3.8433096408843994,
      "learning_rate": 3.60581368469718e-05,
      "loss": 0.667,
      "step": 1833500
    },
    {
      "epoch": 16.731148258997006,
      "grad_norm": 4.255235195159912,
      "learning_rate": 3.6057376450835826e-05,
      "loss": 0.6741,
      "step": 1833600
    },
    {
      "epoch": 16.73206073436017,
      "grad_norm": 4.061708450317383,
      "learning_rate": 3.605661605469986e-05,
      "loss": 0.6798,
      "step": 1833700
    },
    {
      "epoch": 16.732973209723337,
      "grad_norm": 4.221785068511963,
      "learning_rate": 3.6055855658563886e-05,
      "loss": 0.6857,
      "step": 1833800
    },
    {
      "epoch": 16.733885685086502,
      "grad_norm": 3.8008055686950684,
      "learning_rate": 3.6055095262427917e-05,
      "loss": 0.6516,
      "step": 1833900
    },
    {
      "epoch": 16.734798160449667,
      "grad_norm": 3.7135560512542725,
      "learning_rate": 3.6054334866291947e-05,
      "loss": 0.6927,
      "step": 1834000
    },
    {
      "epoch": 16.735710635812833,
      "grad_norm": 3.8807168006896973,
      "learning_rate": 3.605357447015598e-05,
      "loss": 0.6738,
      "step": 1834100
    },
    {
      "epoch": 16.736623111175998,
      "grad_norm": 3.860330581665039,
      "learning_rate": 3.605281407402e-05,
      "loss": 0.6669,
      "step": 1834200
    },
    {
      "epoch": 16.737535586539163,
      "grad_norm": 4.4156928062438965,
      "learning_rate": 3.605205367788404e-05,
      "loss": 0.6718,
      "step": 1834300
    },
    {
      "epoch": 16.73844806190233,
      "grad_norm": 3.883643388748169,
      "learning_rate": 3.605129328174806e-05,
      "loss": 0.6104,
      "step": 1834400
    },
    {
      "epoch": 16.739360537265494,
      "grad_norm": 3.22768497467041,
      "learning_rate": 3.605053288561209e-05,
      "loss": 0.7257,
      "step": 1834500
    },
    {
      "epoch": 16.74027301262866,
      "grad_norm": 5.066476821899414,
      "learning_rate": 3.604977248947612e-05,
      "loss": 0.6843,
      "step": 1834600
    },
    {
      "epoch": 16.741185487991824,
      "grad_norm": 4.539577484130859,
      "learning_rate": 3.604901209334015e-05,
      "loss": 0.6505,
      "step": 1834700
    },
    {
      "epoch": 16.74209796335499,
      "grad_norm": 3.597698450088501,
      "learning_rate": 3.604825169720418e-05,
      "loss": 0.7054,
      "step": 1834800
    },
    {
      "epoch": 16.743010438718155,
      "grad_norm": 4.076860427856445,
      "learning_rate": 3.6047491301068204e-05,
      "loss": 0.6545,
      "step": 1834900
    },
    {
      "epoch": 16.74392291408132,
      "grad_norm": 3.370844841003418,
      "learning_rate": 3.6046730904932234e-05,
      "loss": 0.6968,
      "step": 1835000
    },
    {
      "epoch": 16.744835389444486,
      "grad_norm": 4.605171203613281,
      "learning_rate": 3.6045970508796264e-05,
      "loss": 0.668,
      "step": 1835100
    },
    {
      "epoch": 16.74574786480765,
      "grad_norm": 3.98117733001709,
      "learning_rate": 3.6045210112660294e-05,
      "loss": 0.6695,
      "step": 1835200
    },
    {
      "epoch": 16.746660340170816,
      "grad_norm": 5.0984625816345215,
      "learning_rate": 3.604444971652432e-05,
      "loss": 0.6772,
      "step": 1835300
    },
    {
      "epoch": 16.74757281553398,
      "grad_norm": 4.127337455749512,
      "learning_rate": 3.6043689320388354e-05,
      "loss": 0.6733,
      "step": 1835400
    },
    {
      "epoch": 16.748485290897147,
      "grad_norm": 4.210733413696289,
      "learning_rate": 3.604292892425238e-05,
      "loss": 0.6831,
      "step": 1835500
    },
    {
      "epoch": 16.749397766260312,
      "grad_norm": 3.6884477138519287,
      "learning_rate": 3.604216852811641e-05,
      "loss": 0.6596,
      "step": 1835600
    },
    {
      "epoch": 16.750310241623477,
      "grad_norm": 4.02333402633667,
      "learning_rate": 3.604140813198044e-05,
      "loss": 0.6889,
      "step": 1835700
    },
    {
      "epoch": 16.751222716986643,
      "grad_norm": 4.991639614105225,
      "learning_rate": 3.604064773584447e-05,
      "loss": 0.6901,
      "step": 1835800
    },
    {
      "epoch": 16.752135192349808,
      "grad_norm": 4.053496837615967,
      "learning_rate": 3.60398873397085e-05,
      "loss": 0.6663,
      "step": 1835900
    },
    {
      "epoch": 16.753047667712973,
      "grad_norm": 3.927320957183838,
      "learning_rate": 3.603912694357253e-05,
      "loss": 0.6797,
      "step": 1836000
    },
    {
      "epoch": 16.753960143076135,
      "grad_norm": 3.817950963973999,
      "learning_rate": 3.603836654743655e-05,
      "loss": 0.6586,
      "step": 1836100
    },
    {
      "epoch": 16.7548726184393,
      "grad_norm": 4.08279275894165,
      "learning_rate": 3.603760615130059e-05,
      "loss": 0.7012,
      "step": 1836200
    },
    {
      "epoch": 16.755785093802466,
      "grad_norm": 3.5729575157165527,
      "learning_rate": 3.603684575516461e-05,
      "loss": 0.6527,
      "step": 1836300
    },
    {
      "epoch": 16.75669756916563,
      "grad_norm": 3.257917642593384,
      "learning_rate": 3.603608535902864e-05,
      "loss": 0.6933,
      "step": 1836400
    },
    {
      "epoch": 16.757610044528796,
      "grad_norm": 3.5748543739318848,
      "learning_rate": 3.603532496289267e-05,
      "loss": 0.6663,
      "step": 1836500
    },
    {
      "epoch": 16.75852251989196,
      "grad_norm": 4.841734409332275,
      "learning_rate": 3.60345645667567e-05,
      "loss": 0.6798,
      "step": 1836600
    },
    {
      "epoch": 16.759434995255127,
      "grad_norm": 3.3129591941833496,
      "learning_rate": 3.6033804170620725e-05,
      "loss": 0.6732,
      "step": 1836700
    },
    {
      "epoch": 16.760347470618292,
      "grad_norm": 4.327933311462402,
      "learning_rate": 3.603304377448476e-05,
      "loss": 0.7226,
      "step": 1836800
    },
    {
      "epoch": 16.761259945981458,
      "grad_norm": 4.06509256362915,
      "learning_rate": 3.6032283378348785e-05,
      "loss": 0.6697,
      "step": 1836900
    },
    {
      "epoch": 16.762172421344623,
      "grad_norm": 4.239936351776123,
      "learning_rate": 3.6031522982212815e-05,
      "loss": 0.6582,
      "step": 1837000
    },
    {
      "epoch": 16.763084896707788,
      "grad_norm": 4.588553428649902,
      "learning_rate": 3.6030762586076845e-05,
      "loss": 0.6517,
      "step": 1837100
    },
    {
      "epoch": 16.763997372070953,
      "grad_norm": 4.053518295288086,
      "learning_rate": 3.6030002189940875e-05,
      "loss": 0.6288,
      "step": 1837200
    },
    {
      "epoch": 16.76490984743412,
      "grad_norm": 4.282814025878906,
      "learning_rate": 3.6029241793804905e-05,
      "loss": 0.678,
      "step": 1837300
    },
    {
      "epoch": 16.765822322797284,
      "grad_norm": 3.282902956008911,
      "learning_rate": 3.6028481397668935e-05,
      "loss": 0.6691,
      "step": 1837400
    },
    {
      "epoch": 16.76673479816045,
      "grad_norm": 3.899139881134033,
      "learning_rate": 3.602772100153296e-05,
      "loss": 0.6863,
      "step": 1837500
    },
    {
      "epoch": 16.767647273523615,
      "grad_norm": 3.0439021587371826,
      "learning_rate": 3.602696060539699e-05,
      "loss": 0.6588,
      "step": 1837600
    },
    {
      "epoch": 16.76855974888678,
      "grad_norm": 4.451881408691406,
      "learning_rate": 3.602620020926102e-05,
      "loss": 0.6592,
      "step": 1837700
    },
    {
      "epoch": 16.769472224249945,
      "grad_norm": 3.6152503490448,
      "learning_rate": 3.602543981312504e-05,
      "loss": 0.7137,
      "step": 1837800
    },
    {
      "epoch": 16.77038469961311,
      "grad_norm": 3.459368944168091,
      "learning_rate": 3.602467941698908e-05,
      "loss": 0.6775,
      "step": 1837900
    },
    {
      "epoch": 16.771297174976276,
      "grad_norm": 2.909670829772949,
      "learning_rate": 3.60239190208531e-05,
      "loss": 0.6586,
      "step": 1838000
    },
    {
      "epoch": 16.77220965033944,
      "grad_norm": 4.0366950035095215,
      "learning_rate": 3.602315862471713e-05,
      "loss": 0.6475,
      "step": 1838100
    },
    {
      "epoch": 16.773122125702606,
      "grad_norm": 4.773369789123535,
      "learning_rate": 3.602239822858116e-05,
      "loss": 0.6585,
      "step": 1838200
    },
    {
      "epoch": 16.77403460106577,
      "grad_norm": 2.7691783905029297,
      "learning_rate": 3.602163783244519e-05,
      "loss": 0.7105,
      "step": 1838300
    },
    {
      "epoch": 16.774947076428937,
      "grad_norm": 4.272103309631348,
      "learning_rate": 3.602087743630922e-05,
      "loss": 0.627,
      "step": 1838400
    },
    {
      "epoch": 16.775859551792102,
      "grad_norm": 3.717466354370117,
      "learning_rate": 3.602011704017325e-05,
      "loss": 0.6445,
      "step": 1838500
    },
    {
      "epoch": 16.776772027155268,
      "grad_norm": 4.149169921875,
      "learning_rate": 3.6019356644037275e-05,
      "loss": 0.6653,
      "step": 1838600
    },
    {
      "epoch": 16.777684502518433,
      "grad_norm": 2.65439510345459,
      "learning_rate": 3.601859624790131e-05,
      "loss": 0.6159,
      "step": 1838700
    },
    {
      "epoch": 16.778596977881598,
      "grad_norm": 4.4015913009643555,
      "learning_rate": 3.6017835851765336e-05,
      "loss": 0.6602,
      "step": 1838800
    },
    {
      "epoch": 16.779509453244764,
      "grad_norm": 3.411011219024658,
      "learning_rate": 3.6017075455629366e-05,
      "loss": 0.6631,
      "step": 1838900
    },
    {
      "epoch": 16.78042192860793,
      "grad_norm": 4.658787727355957,
      "learning_rate": 3.6016315059493396e-05,
      "loss": 0.6704,
      "step": 1839000
    },
    {
      "epoch": 16.781334403971094,
      "grad_norm": 3.860576868057251,
      "learning_rate": 3.6015554663357426e-05,
      "loss": 0.7023,
      "step": 1839100
    },
    {
      "epoch": 16.78224687933426,
      "grad_norm": 3.5509955883026123,
      "learning_rate": 3.601479426722145e-05,
      "loss": 0.7215,
      "step": 1839200
    },
    {
      "epoch": 16.783159354697425,
      "grad_norm": 3.5888190269470215,
      "learning_rate": 3.6014033871085486e-05,
      "loss": 0.6573,
      "step": 1839300
    },
    {
      "epoch": 16.78407183006059,
      "grad_norm": 3.3270461559295654,
      "learning_rate": 3.601327347494951e-05,
      "loss": 0.6654,
      "step": 1839400
    },
    {
      "epoch": 16.784984305423755,
      "grad_norm": 4.135682582855225,
      "learning_rate": 3.601251307881354e-05,
      "loss": 0.7062,
      "step": 1839500
    },
    {
      "epoch": 16.785896780786917,
      "grad_norm": 4.206246376037598,
      "learning_rate": 3.601175268267757e-05,
      "loss": 0.6505,
      "step": 1839600
    },
    {
      "epoch": 16.786809256150082,
      "grad_norm": 4.264734268188477,
      "learning_rate": 3.60109922865416e-05,
      "loss": 0.6344,
      "step": 1839700
    },
    {
      "epoch": 16.787721731513248,
      "grad_norm": 3.0137805938720703,
      "learning_rate": 3.601023189040563e-05,
      "loss": 0.664,
      "step": 1839800
    },
    {
      "epoch": 16.788634206876413,
      "grad_norm": 4.183020114898682,
      "learning_rate": 3.600947149426966e-05,
      "loss": 0.6692,
      "step": 1839900
    },
    {
      "epoch": 16.78954668223958,
      "grad_norm": 3.659585475921631,
      "learning_rate": 3.600871109813368e-05,
      "loss": 0.6819,
      "step": 1840000
    },
    {
      "epoch": 16.790459157602744,
      "grad_norm": 3.3039510250091553,
      "learning_rate": 3.600795070199772e-05,
      "loss": 0.6863,
      "step": 1840100
    },
    {
      "epoch": 16.79137163296591,
      "grad_norm": 3.5879712104797363,
      "learning_rate": 3.600719030586174e-05,
      "loss": 0.6697,
      "step": 1840200
    },
    {
      "epoch": 16.792284108329074,
      "grad_norm": 3.6647391319274902,
      "learning_rate": 3.600642990972577e-05,
      "loss": 0.648,
      "step": 1840300
    },
    {
      "epoch": 16.79319658369224,
      "grad_norm": 4.431703090667725,
      "learning_rate": 3.60056695135898e-05,
      "loss": 0.6366,
      "step": 1840400
    },
    {
      "epoch": 16.794109059055405,
      "grad_norm": 3.976722478866577,
      "learning_rate": 3.6004909117453826e-05,
      "loss": 0.6824,
      "step": 1840500
    },
    {
      "epoch": 16.79502153441857,
      "grad_norm": 4.564295291900635,
      "learning_rate": 3.6004148721317856e-05,
      "loss": 0.6255,
      "step": 1840600
    },
    {
      "epoch": 16.795934009781735,
      "grad_norm": 3.470816135406494,
      "learning_rate": 3.6003388325181887e-05,
      "loss": 0.6983,
      "step": 1840700
    },
    {
      "epoch": 16.7968464851449,
      "grad_norm": 4.435770511627197,
      "learning_rate": 3.600262792904592e-05,
      "loss": 0.6594,
      "step": 1840800
    },
    {
      "epoch": 16.797758960508066,
      "grad_norm": 4.309876918792725,
      "learning_rate": 3.600186753290995e-05,
      "loss": 0.6712,
      "step": 1840900
    },
    {
      "epoch": 16.79867143587123,
      "grad_norm": 3.957233190536499,
      "learning_rate": 3.600110713677398e-05,
      "loss": 0.6414,
      "step": 1841000
    },
    {
      "epoch": 16.799583911234397,
      "grad_norm": 4.416308403015137,
      "learning_rate": 3.6000346740638e-05,
      "loss": 0.6631,
      "step": 1841100
    },
    {
      "epoch": 16.800496386597562,
      "grad_norm": 4.328696250915527,
      "learning_rate": 3.599958634450204e-05,
      "loss": 0.7001,
      "step": 1841200
    },
    {
      "epoch": 16.801408861960727,
      "grad_norm": 4.072054386138916,
      "learning_rate": 3.599882594836606e-05,
      "loss": 0.682,
      "step": 1841300
    },
    {
      "epoch": 16.802321337323892,
      "grad_norm": 3.7131075859069824,
      "learning_rate": 3.599806555223009e-05,
      "loss": 0.6325,
      "step": 1841400
    },
    {
      "epoch": 16.803233812687058,
      "grad_norm": 4.537559986114502,
      "learning_rate": 3.599730515609412e-05,
      "loss": 0.6531,
      "step": 1841500
    },
    {
      "epoch": 16.804146288050223,
      "grad_norm": 3.426719903945923,
      "learning_rate": 3.599654475995815e-05,
      "loss": 0.6616,
      "step": 1841600
    },
    {
      "epoch": 16.80505876341339,
      "grad_norm": 4.59657096862793,
      "learning_rate": 3.5995784363822174e-05,
      "loss": 0.657,
      "step": 1841700
    },
    {
      "epoch": 16.805971238776554,
      "grad_norm": 3.9501168727874756,
      "learning_rate": 3.599502396768621e-05,
      "loss": 0.7074,
      "step": 1841800
    },
    {
      "epoch": 16.80688371413972,
      "grad_norm": 3.270284652709961,
      "learning_rate": 3.5994263571550234e-05,
      "loss": 0.6814,
      "step": 1841900
    },
    {
      "epoch": 16.807796189502884,
      "grad_norm": 3.9515624046325684,
      "learning_rate": 3.5993503175414264e-05,
      "loss": 0.6402,
      "step": 1842000
    },
    {
      "epoch": 16.80870866486605,
      "grad_norm": 3.6416749954223633,
      "learning_rate": 3.5992742779278294e-05,
      "loss": 0.6626,
      "step": 1842100
    },
    {
      "epoch": 16.809621140229215,
      "grad_norm": 3.0029022693634033,
      "learning_rate": 3.5991982383142324e-05,
      "loss": 0.697,
      "step": 1842200
    },
    {
      "epoch": 16.81053361559238,
      "grad_norm": 3.6523075103759766,
      "learning_rate": 3.5991221987006354e-05,
      "loss": 0.6829,
      "step": 1842300
    },
    {
      "epoch": 16.811446090955545,
      "grad_norm": 3.361105442047119,
      "learning_rate": 3.5990461590870384e-05,
      "loss": 0.6666,
      "step": 1842400
    },
    {
      "epoch": 16.81235856631871,
      "grad_norm": 3.540552854537964,
      "learning_rate": 3.598970119473441e-05,
      "loss": 0.672,
      "step": 1842500
    },
    {
      "epoch": 16.813271041681876,
      "grad_norm": 4.031570911407471,
      "learning_rate": 3.5988940798598444e-05,
      "loss": 0.6361,
      "step": 1842600
    },
    {
      "epoch": 16.81418351704504,
      "grad_norm": 3.4755027294158936,
      "learning_rate": 3.598818040246247e-05,
      "loss": 0.6292,
      "step": 1842700
    },
    {
      "epoch": 16.815095992408207,
      "grad_norm": 3.4123916625976562,
      "learning_rate": 3.59874200063265e-05,
      "loss": 0.6433,
      "step": 1842800
    },
    {
      "epoch": 16.81600846777137,
      "grad_norm": 2.671320676803589,
      "learning_rate": 3.598665961019053e-05,
      "loss": 0.6317,
      "step": 1842900
    },
    {
      "epoch": 16.816920943134534,
      "grad_norm": 4.212879657745361,
      "learning_rate": 3.598589921405456e-05,
      "loss": 0.6624,
      "step": 1843000
    },
    {
      "epoch": 16.8178334184977,
      "grad_norm": 3.162708044052124,
      "learning_rate": 3.598513881791858e-05,
      "loss": 0.6829,
      "step": 1843100
    },
    {
      "epoch": 16.818745893860864,
      "grad_norm": 3.8765857219696045,
      "learning_rate": 3.598437842178262e-05,
      "loss": 0.6486,
      "step": 1843200
    },
    {
      "epoch": 16.81965836922403,
      "grad_norm": 3.684542179107666,
      "learning_rate": 3.598361802564664e-05,
      "loss": 0.6432,
      "step": 1843300
    },
    {
      "epoch": 16.820570844587195,
      "grad_norm": 3.5548853874206543,
      "learning_rate": 3.598285762951067e-05,
      "loss": 0.6832,
      "step": 1843400
    },
    {
      "epoch": 16.82148331995036,
      "grad_norm": 4.474100112915039,
      "learning_rate": 3.59820972333747e-05,
      "loss": 0.6482,
      "step": 1843500
    },
    {
      "epoch": 16.822395795313525,
      "grad_norm": 3.676884889602661,
      "learning_rate": 3.5981336837238725e-05,
      "loss": 0.6582,
      "step": 1843600
    },
    {
      "epoch": 16.82330827067669,
      "grad_norm": 4.090293884277344,
      "learning_rate": 3.598057644110276e-05,
      "loss": 0.6889,
      "step": 1843700
    },
    {
      "epoch": 16.824220746039856,
      "grad_norm": 3.734555959701538,
      "learning_rate": 3.5979816044966785e-05,
      "loss": 0.6602,
      "step": 1843800
    },
    {
      "epoch": 16.82513322140302,
      "grad_norm": 3.0511131286621094,
      "learning_rate": 3.5979055648830815e-05,
      "loss": 0.6684,
      "step": 1843900
    },
    {
      "epoch": 16.826045696766187,
      "grad_norm": 4.5833353996276855,
      "learning_rate": 3.5978295252694845e-05,
      "loss": 0.6816,
      "step": 1844000
    },
    {
      "epoch": 16.826958172129352,
      "grad_norm": 4.201910018920898,
      "learning_rate": 3.5977534856558875e-05,
      "loss": 0.6547,
      "step": 1844100
    },
    {
      "epoch": 16.827870647492517,
      "grad_norm": 4.9254560470581055,
      "learning_rate": 3.5976774460422905e-05,
      "loss": 0.6477,
      "step": 1844200
    },
    {
      "epoch": 16.828783122855683,
      "grad_norm": 4.68946647644043,
      "learning_rate": 3.5976014064286935e-05,
      "loss": 0.6722,
      "step": 1844300
    },
    {
      "epoch": 16.829695598218848,
      "grad_norm": 2.6908209323883057,
      "learning_rate": 3.597525366815096e-05,
      "loss": 0.6908,
      "step": 1844400
    },
    {
      "epoch": 16.830608073582013,
      "grad_norm": 4.292359828948975,
      "learning_rate": 3.5974493272014995e-05,
      "loss": 0.6721,
      "step": 1844500
    },
    {
      "epoch": 16.83152054894518,
      "grad_norm": 4.074129104614258,
      "learning_rate": 3.597373287587902e-05,
      "loss": 0.6685,
      "step": 1844600
    },
    {
      "epoch": 16.832433024308344,
      "grad_norm": 4.255873203277588,
      "learning_rate": 3.597297247974305e-05,
      "loss": 0.707,
      "step": 1844700
    },
    {
      "epoch": 16.83334549967151,
      "grad_norm": 4.766605377197266,
      "learning_rate": 3.597221208360708e-05,
      "loss": 0.6759,
      "step": 1844800
    },
    {
      "epoch": 16.834257975034674,
      "grad_norm": 3.866086959838867,
      "learning_rate": 3.597145168747111e-05,
      "loss": 0.6694,
      "step": 1844900
    },
    {
      "epoch": 16.83517045039784,
      "grad_norm": 4.931597709655762,
      "learning_rate": 3.597069129133513e-05,
      "loss": 0.654,
      "step": 1845000
    },
    {
      "epoch": 16.836082925761005,
      "grad_norm": 2.961215019226074,
      "learning_rate": 3.596993089519917e-05,
      "loss": 0.6507,
      "step": 1845100
    },
    {
      "epoch": 16.83699540112417,
      "grad_norm": 5.639237403869629,
      "learning_rate": 3.596917049906319e-05,
      "loss": 0.6741,
      "step": 1845200
    },
    {
      "epoch": 16.837907876487336,
      "grad_norm": 3.958597421646118,
      "learning_rate": 3.596841010292722e-05,
      "loss": 0.689,
      "step": 1845300
    },
    {
      "epoch": 16.8388203518505,
      "grad_norm": 2.846489191055298,
      "learning_rate": 3.596764970679125e-05,
      "loss": 0.6956,
      "step": 1845400
    },
    {
      "epoch": 16.839732827213666,
      "grad_norm": 3.3941123485565186,
      "learning_rate": 3.596688931065528e-05,
      "loss": 0.6595,
      "step": 1845500
    },
    {
      "epoch": 16.84064530257683,
      "grad_norm": 3.943526268005371,
      "learning_rate": 3.596612891451931e-05,
      "loss": 0.6806,
      "step": 1845600
    },
    {
      "epoch": 16.841557777939997,
      "grad_norm": 2.9858713150024414,
      "learning_rate": 3.596536851838334e-05,
      "loss": 0.698,
      "step": 1845700
    },
    {
      "epoch": 16.842470253303162,
      "grad_norm": 4.812619209289551,
      "learning_rate": 3.5964608122247366e-05,
      "loss": 0.681,
      "step": 1845800
    },
    {
      "epoch": 16.843382728666327,
      "grad_norm": 4.169455528259277,
      "learning_rate": 3.59638477261114e-05,
      "loss": 0.6905,
      "step": 1845900
    },
    {
      "epoch": 16.844295204029493,
      "grad_norm": 5.672842025756836,
      "learning_rate": 3.5963087329975426e-05,
      "loss": 0.7026,
      "step": 1846000
    },
    {
      "epoch": 16.845207679392658,
      "grad_norm": 4.456024169921875,
      "learning_rate": 3.596232693383945e-05,
      "loss": 0.6784,
      "step": 1846100
    },
    {
      "epoch": 16.846120154755823,
      "grad_norm": 3.910851240158081,
      "learning_rate": 3.5961566537703486e-05,
      "loss": 0.6697,
      "step": 1846200
    },
    {
      "epoch": 16.84703263011899,
      "grad_norm": 3.5405828952789307,
      "learning_rate": 3.596080614156751e-05,
      "loss": 0.6623,
      "step": 1846300
    },
    {
      "epoch": 16.84794510548215,
      "grad_norm": 4.157938003540039,
      "learning_rate": 3.596004574543154e-05,
      "loss": 0.665,
      "step": 1846400
    },
    {
      "epoch": 16.848857580845316,
      "grad_norm": 3.827314615249634,
      "learning_rate": 3.595928534929557e-05,
      "loss": 0.6698,
      "step": 1846500
    },
    {
      "epoch": 16.84977005620848,
      "grad_norm": 3.9717910289764404,
      "learning_rate": 3.59585249531596e-05,
      "loss": 0.6722,
      "step": 1846600
    },
    {
      "epoch": 16.850682531571646,
      "grad_norm": 3.3626999855041504,
      "learning_rate": 3.595776455702363e-05,
      "loss": 0.6682,
      "step": 1846700
    },
    {
      "epoch": 16.85159500693481,
      "grad_norm": 4.423064708709717,
      "learning_rate": 3.595700416088766e-05,
      "loss": 0.6767,
      "step": 1846800
    },
    {
      "epoch": 16.852507482297977,
      "grad_norm": 3.596881866455078,
      "learning_rate": 3.595624376475168e-05,
      "loss": 0.6918,
      "step": 1846900
    },
    {
      "epoch": 16.853419957661142,
      "grad_norm": 4.107298851013184,
      "learning_rate": 3.595548336861572e-05,
      "loss": 0.6605,
      "step": 1847000
    },
    {
      "epoch": 16.854332433024307,
      "grad_norm": 4.047653675079346,
      "learning_rate": 3.595472297247974e-05,
      "loss": 0.6295,
      "step": 1847100
    },
    {
      "epoch": 16.855244908387473,
      "grad_norm": 3.7253646850585938,
      "learning_rate": 3.595396257634377e-05,
      "loss": 0.6434,
      "step": 1847200
    },
    {
      "epoch": 16.856157383750638,
      "grad_norm": 3.3914451599121094,
      "learning_rate": 3.59532021802078e-05,
      "loss": 0.6519,
      "step": 1847300
    },
    {
      "epoch": 16.857069859113803,
      "grad_norm": 3.268583059310913,
      "learning_rate": 3.595244178407183e-05,
      "loss": 0.6794,
      "step": 1847400
    },
    {
      "epoch": 16.85798233447697,
      "grad_norm": 4.638943195343018,
      "learning_rate": 3.5951681387935857e-05,
      "loss": 0.6644,
      "step": 1847500
    },
    {
      "epoch": 16.858894809840134,
      "grad_norm": 3.422861337661743,
      "learning_rate": 3.5950920991799893e-05,
      "loss": 0.6611,
      "step": 1847600
    },
    {
      "epoch": 16.8598072852033,
      "grad_norm": 4.245123386383057,
      "learning_rate": 3.595016059566392e-05,
      "loss": 0.6565,
      "step": 1847700
    },
    {
      "epoch": 16.860719760566464,
      "grad_norm": 3.2348885536193848,
      "learning_rate": 3.594940019952795e-05,
      "loss": 0.6166,
      "step": 1847800
    },
    {
      "epoch": 16.86163223592963,
      "grad_norm": 4.2992448806762695,
      "learning_rate": 3.594863980339198e-05,
      "loss": 0.6729,
      "step": 1847900
    },
    {
      "epoch": 16.862544711292795,
      "grad_norm": 3.6339499950408936,
      "learning_rate": 3.594787940725601e-05,
      "loss": 0.6542,
      "step": 1848000
    },
    {
      "epoch": 16.86345718665596,
      "grad_norm": 4.535705089569092,
      "learning_rate": 3.594711901112004e-05,
      "loss": 0.6391,
      "step": 1848100
    },
    {
      "epoch": 16.864369662019126,
      "grad_norm": 4.112241744995117,
      "learning_rate": 3.594635861498407e-05,
      "loss": 0.6567,
      "step": 1848200
    },
    {
      "epoch": 16.86528213738229,
      "grad_norm": 2.87528133392334,
      "learning_rate": 3.594559821884809e-05,
      "loss": 0.6855,
      "step": 1848300
    },
    {
      "epoch": 16.866194612745456,
      "grad_norm": 3.665590763092041,
      "learning_rate": 3.594483782271213e-05,
      "loss": 0.6708,
      "step": 1848400
    },
    {
      "epoch": 16.86710708810862,
      "grad_norm": 3.1395745277404785,
      "learning_rate": 3.594407742657615e-05,
      "loss": 0.637,
      "step": 1848500
    },
    {
      "epoch": 16.868019563471787,
      "grad_norm": 3.502577543258667,
      "learning_rate": 3.594331703044018e-05,
      "loss": 0.6837,
      "step": 1848600
    },
    {
      "epoch": 16.868932038834952,
      "grad_norm": 3.6736416816711426,
      "learning_rate": 3.594255663430421e-05,
      "loss": 0.6707,
      "step": 1848700
    },
    {
      "epoch": 16.869844514198117,
      "grad_norm": 4.242387771606445,
      "learning_rate": 3.594179623816824e-05,
      "loss": 0.6739,
      "step": 1848800
    },
    {
      "epoch": 16.870756989561283,
      "grad_norm": 3.0043158531188965,
      "learning_rate": 3.5941035842032264e-05,
      "loss": 0.6297,
      "step": 1848900
    },
    {
      "epoch": 16.871669464924448,
      "grad_norm": 4.095240592956543,
      "learning_rate": 3.5940275445896294e-05,
      "loss": 0.664,
      "step": 1849000
    },
    {
      "epoch": 16.872581940287613,
      "grad_norm": 3.7307047843933105,
      "learning_rate": 3.5939515049760324e-05,
      "loss": 0.6863,
      "step": 1849100
    },
    {
      "epoch": 16.87349441565078,
      "grad_norm": 3.48954701423645,
      "learning_rate": 3.5938754653624354e-05,
      "loss": 0.6346,
      "step": 1849200
    },
    {
      "epoch": 16.874406891013944,
      "grad_norm": 4.985819339752197,
      "learning_rate": 3.5937994257488384e-05,
      "loss": 0.6605,
      "step": 1849300
    },
    {
      "epoch": 16.87531936637711,
      "grad_norm": 4.443964958190918,
      "learning_rate": 3.593723386135241e-05,
      "loss": 0.662,
      "step": 1849400
    },
    {
      "epoch": 16.876231841740275,
      "grad_norm": 4.301729679107666,
      "learning_rate": 3.5936473465216444e-05,
      "loss": 0.6632,
      "step": 1849500
    },
    {
      "epoch": 16.87714431710344,
      "grad_norm": 3.9622600078582764,
      "learning_rate": 3.593571306908047e-05,
      "loss": 0.6488,
      "step": 1849600
    },
    {
      "epoch": 16.8780567924666,
      "grad_norm": 3.8659424781799316,
      "learning_rate": 3.59349526729445e-05,
      "loss": 0.6641,
      "step": 1849700
    },
    {
      "epoch": 16.878969267829767,
      "grad_norm": 4.984828948974609,
      "learning_rate": 3.593419227680853e-05,
      "loss": 0.6543,
      "step": 1849800
    },
    {
      "epoch": 16.879881743192932,
      "grad_norm": 3.1755878925323486,
      "learning_rate": 3.593343188067256e-05,
      "loss": 0.6651,
      "step": 1849900
    },
    {
      "epoch": 16.880794218556098,
      "grad_norm": 4.395596504211426,
      "learning_rate": 3.593267148453658e-05,
      "loss": 0.6903,
      "step": 1850000
    },
    {
      "epoch": 16.881706693919263,
      "grad_norm": 3.762845277786255,
      "learning_rate": 3.593191108840062e-05,
      "loss": 0.688,
      "step": 1850100
    },
    {
      "epoch": 16.882619169282428,
      "grad_norm": 4.060474872589111,
      "learning_rate": 3.593115069226464e-05,
      "loss": 0.6442,
      "step": 1850200
    },
    {
      "epoch": 16.883531644645593,
      "grad_norm": 3.983426332473755,
      "learning_rate": 3.593039029612867e-05,
      "loss": 0.6611,
      "step": 1850300
    },
    {
      "epoch": 16.88444412000876,
      "grad_norm": 3.8331637382507324,
      "learning_rate": 3.59296298999927e-05,
      "loss": 0.7007,
      "step": 1850400
    },
    {
      "epoch": 16.885356595371924,
      "grad_norm": 3.7517545223236084,
      "learning_rate": 3.592886950385673e-05,
      "loss": 0.6836,
      "step": 1850500
    },
    {
      "epoch": 16.88626907073509,
      "grad_norm": 4.045112609863281,
      "learning_rate": 3.592810910772076e-05,
      "loss": 0.6862,
      "step": 1850600
    },
    {
      "epoch": 16.887181546098255,
      "grad_norm": 3.7687809467315674,
      "learning_rate": 3.592734871158479e-05,
      "loss": 0.6862,
      "step": 1850700
    },
    {
      "epoch": 16.88809402146142,
      "grad_norm": 3.2475574016571045,
      "learning_rate": 3.5926588315448815e-05,
      "loss": 0.655,
      "step": 1850800
    },
    {
      "epoch": 16.889006496824585,
      "grad_norm": 3.4723827838897705,
      "learning_rate": 3.592582791931285e-05,
      "loss": 0.6425,
      "step": 1850900
    },
    {
      "epoch": 16.88991897218775,
      "grad_norm": 3.1851515769958496,
      "learning_rate": 3.5925067523176875e-05,
      "loss": 0.6939,
      "step": 1851000
    },
    {
      "epoch": 16.890831447550916,
      "grad_norm": 3.820326805114746,
      "learning_rate": 3.5924307127040905e-05,
      "loss": 0.6763,
      "step": 1851100
    },
    {
      "epoch": 16.89174392291408,
      "grad_norm": 2.7516183853149414,
      "learning_rate": 3.5923546730904935e-05,
      "loss": 0.6355,
      "step": 1851200
    },
    {
      "epoch": 16.892656398277246,
      "grad_norm": 3.4909818172454834,
      "learning_rate": 3.5922786334768965e-05,
      "loss": 0.6852,
      "step": 1851300
    },
    {
      "epoch": 16.89356887364041,
      "grad_norm": 3.2655935287475586,
      "learning_rate": 3.592202593863299e-05,
      "loss": 0.6277,
      "step": 1851400
    },
    {
      "epoch": 16.894481349003577,
      "grad_norm": 4.614185810089111,
      "learning_rate": 3.5921265542497025e-05,
      "loss": 0.6943,
      "step": 1851500
    },
    {
      "epoch": 16.895393824366742,
      "grad_norm": 3.198866128921509,
      "learning_rate": 3.592050514636105e-05,
      "loss": 0.6307,
      "step": 1851600
    },
    {
      "epoch": 16.896306299729908,
      "grad_norm": 3.006751298904419,
      "learning_rate": 3.591974475022508e-05,
      "loss": 0.6459,
      "step": 1851700
    },
    {
      "epoch": 16.897218775093073,
      "grad_norm": 4.038115501403809,
      "learning_rate": 3.591898435408911e-05,
      "loss": 0.6471,
      "step": 1851800
    },
    {
      "epoch": 16.898131250456238,
      "grad_norm": 3.819321632385254,
      "learning_rate": 3.591822395795313e-05,
      "loss": 0.6621,
      "step": 1851900
    },
    {
      "epoch": 16.899043725819404,
      "grad_norm": 4.628760814666748,
      "learning_rate": 3.591746356181717e-05,
      "loss": 0.6708,
      "step": 1852000
    },
    {
      "epoch": 16.89995620118257,
      "grad_norm": 4.154069900512695,
      "learning_rate": 3.591670316568119e-05,
      "loss": 0.6767,
      "step": 1852100
    },
    {
      "epoch": 16.900868676545734,
      "grad_norm": 3.5958495140075684,
      "learning_rate": 3.591594276954522e-05,
      "loss": 0.6366,
      "step": 1852200
    },
    {
      "epoch": 16.9017811519089,
      "grad_norm": 3.572801351547241,
      "learning_rate": 3.591518237340925e-05,
      "loss": 0.6799,
      "step": 1852300
    },
    {
      "epoch": 16.902693627272065,
      "grad_norm": 4.209238052368164,
      "learning_rate": 3.591442197727328e-05,
      "loss": 0.6979,
      "step": 1852400
    },
    {
      "epoch": 16.90360610263523,
      "grad_norm": 4.4904632568359375,
      "learning_rate": 3.5913661581137306e-05,
      "loss": 0.6738,
      "step": 1852500
    },
    {
      "epoch": 16.904518577998395,
      "grad_norm": 4.001555919647217,
      "learning_rate": 3.591290118500134e-05,
      "loss": 0.6575,
      "step": 1852600
    },
    {
      "epoch": 16.90543105336156,
      "grad_norm": 3.2656092643737793,
      "learning_rate": 3.5912140788865366e-05,
      "loss": 0.6472,
      "step": 1852700
    },
    {
      "epoch": 16.906343528724726,
      "grad_norm": 2.9670276641845703,
      "learning_rate": 3.5911380392729396e-05,
      "loss": 0.6724,
      "step": 1852800
    },
    {
      "epoch": 16.90725600408789,
      "grad_norm": 4.585134506225586,
      "learning_rate": 3.5910619996593426e-05,
      "loss": 0.6639,
      "step": 1852900
    },
    {
      "epoch": 16.908168479451056,
      "grad_norm": 4.0543107986450195,
      "learning_rate": 3.5909859600457456e-05,
      "loss": 0.6429,
      "step": 1853000
    },
    {
      "epoch": 16.909080954814222,
      "grad_norm": 3.5790817737579346,
      "learning_rate": 3.5909099204321486e-05,
      "loss": 0.6453,
      "step": 1853100
    },
    {
      "epoch": 16.909993430177384,
      "grad_norm": 3.883913516998291,
      "learning_rate": 3.5908338808185516e-05,
      "loss": 0.6444,
      "step": 1853200
    },
    {
      "epoch": 16.91090590554055,
      "grad_norm": 4.032289505004883,
      "learning_rate": 3.590757841204954e-05,
      "loss": 0.6426,
      "step": 1853300
    },
    {
      "epoch": 16.911818380903714,
      "grad_norm": 3.9737696647644043,
      "learning_rate": 3.5906818015913576e-05,
      "loss": 0.6444,
      "step": 1853400
    },
    {
      "epoch": 16.91273085626688,
      "grad_norm": 4.313273906707764,
      "learning_rate": 3.59060576197776e-05,
      "loss": 0.6401,
      "step": 1853500
    },
    {
      "epoch": 16.913643331630045,
      "grad_norm": 4.300268173217773,
      "learning_rate": 3.590529722364163e-05,
      "loss": 0.6775,
      "step": 1853600
    },
    {
      "epoch": 16.91455580699321,
      "grad_norm": 3.914635181427002,
      "learning_rate": 3.590453682750566e-05,
      "loss": 0.6736,
      "step": 1853700
    },
    {
      "epoch": 16.915468282356375,
      "grad_norm": 4.660594940185547,
      "learning_rate": 3.590377643136969e-05,
      "loss": 0.6794,
      "step": 1853800
    },
    {
      "epoch": 16.91638075771954,
      "grad_norm": 4.156484603881836,
      "learning_rate": 3.590301603523371e-05,
      "loss": 0.6727,
      "step": 1853900
    },
    {
      "epoch": 16.917293233082706,
      "grad_norm": 4.1799845695495605,
      "learning_rate": 3.590225563909775e-05,
      "loss": 0.6783,
      "step": 1854000
    },
    {
      "epoch": 16.91820570844587,
      "grad_norm": 3.950713872909546,
      "learning_rate": 3.590149524296177e-05,
      "loss": 0.67,
      "step": 1854100
    },
    {
      "epoch": 16.919118183809037,
      "grad_norm": 3.645577907562256,
      "learning_rate": 3.59007348468258e-05,
      "loss": 0.6355,
      "step": 1854200
    },
    {
      "epoch": 16.920030659172202,
      "grad_norm": 2.9352147579193115,
      "learning_rate": 3.5899974450689833e-05,
      "loss": 0.6894,
      "step": 1854300
    },
    {
      "epoch": 16.920943134535367,
      "grad_norm": 3.7318480014801025,
      "learning_rate": 3.5899214054553863e-05,
      "loss": 0.6565,
      "step": 1854400
    },
    {
      "epoch": 16.921855609898532,
      "grad_norm": 5.515807151794434,
      "learning_rate": 3.5898453658417894e-05,
      "loss": 0.6698,
      "step": 1854500
    },
    {
      "epoch": 16.922768085261698,
      "grad_norm": 3.6570076942443848,
      "learning_rate": 3.5897693262281924e-05,
      "loss": 0.7139,
      "step": 1854600
    },
    {
      "epoch": 16.923680560624863,
      "grad_norm": 3.3512494564056396,
      "learning_rate": 3.589693286614595e-05,
      "loss": 0.6528,
      "step": 1854700
    },
    {
      "epoch": 16.92459303598803,
      "grad_norm": 4.253427505493164,
      "learning_rate": 3.589617247000998e-05,
      "loss": 0.6893,
      "step": 1854800
    },
    {
      "epoch": 16.925505511351194,
      "grad_norm": 4.391242504119873,
      "learning_rate": 3.589541207387401e-05,
      "loss": 0.6546,
      "step": 1854900
    },
    {
      "epoch": 16.92641798671436,
      "grad_norm": 3.9625704288482666,
      "learning_rate": 3.589465167773804e-05,
      "loss": 0.6867,
      "step": 1855000
    },
    {
      "epoch": 16.927330462077524,
      "grad_norm": 4.2996110916137695,
      "learning_rate": 3.589389128160207e-05,
      "loss": 0.6949,
      "step": 1855100
    },
    {
      "epoch": 16.92824293744069,
      "grad_norm": 4.211639404296875,
      "learning_rate": 3.589313088546609e-05,
      "loss": 0.67,
      "step": 1855200
    },
    {
      "epoch": 16.929155412803855,
      "grad_norm": 3.574192523956299,
      "learning_rate": 3.589237048933012e-05,
      "loss": 0.6628,
      "step": 1855300
    },
    {
      "epoch": 16.93006788816702,
      "grad_norm": 3.143608331680298,
      "learning_rate": 3.589161009319415e-05,
      "loss": 0.6458,
      "step": 1855400
    },
    {
      "epoch": 16.930980363530185,
      "grad_norm": 3.4335596561431885,
      "learning_rate": 3.589084969705818e-05,
      "loss": 0.6755,
      "step": 1855500
    },
    {
      "epoch": 16.93189283889335,
      "grad_norm": 4.574939727783203,
      "learning_rate": 3.589008930092221e-05,
      "loss": 0.6673,
      "step": 1855600
    },
    {
      "epoch": 16.932805314256516,
      "grad_norm": 3.162370443344116,
      "learning_rate": 3.588932890478624e-05,
      "loss": 0.6908,
      "step": 1855700
    },
    {
      "epoch": 16.93371778961968,
      "grad_norm": 2.9615421295166016,
      "learning_rate": 3.5888568508650264e-05,
      "loss": 0.7074,
      "step": 1855800
    },
    {
      "epoch": 16.934630264982847,
      "grad_norm": 4.136284828186035,
      "learning_rate": 3.58878081125143e-05,
      "loss": 0.6541,
      "step": 1855900
    },
    {
      "epoch": 16.935542740346012,
      "grad_norm": 3.7332098484039307,
      "learning_rate": 3.5887047716378324e-05,
      "loss": 0.6846,
      "step": 1856000
    },
    {
      "epoch": 16.936455215709177,
      "grad_norm": 3.2935585975646973,
      "learning_rate": 3.5886287320242354e-05,
      "loss": 0.6356,
      "step": 1856100
    },
    {
      "epoch": 16.937367691072343,
      "grad_norm": 4.640660285949707,
      "learning_rate": 3.5885526924106384e-05,
      "loss": 0.6676,
      "step": 1856200
    },
    {
      "epoch": 16.938280166435508,
      "grad_norm": 4.138545513153076,
      "learning_rate": 3.5884766527970414e-05,
      "loss": 0.6888,
      "step": 1856300
    },
    {
      "epoch": 16.939192641798673,
      "grad_norm": 3.7257800102233887,
      "learning_rate": 3.5884006131834444e-05,
      "loss": 0.6799,
      "step": 1856400
    },
    {
      "epoch": 16.940105117161835,
      "grad_norm": 3.985980272293091,
      "learning_rate": 3.5883245735698475e-05,
      "loss": 0.6644,
      "step": 1856500
    },
    {
      "epoch": 16.941017592525,
      "grad_norm": 3.950875997543335,
      "learning_rate": 3.58824853395625e-05,
      "loss": 0.6863,
      "step": 1856600
    },
    {
      "epoch": 16.941930067888165,
      "grad_norm": 3.6490371227264404,
      "learning_rate": 3.588172494342653e-05,
      "loss": 0.6364,
      "step": 1856700
    },
    {
      "epoch": 16.94284254325133,
      "grad_norm": 4.023953914642334,
      "learning_rate": 3.588096454729056e-05,
      "loss": 0.6528,
      "step": 1856800
    },
    {
      "epoch": 16.943755018614496,
      "grad_norm": 2.676530599594116,
      "learning_rate": 3.588020415115459e-05,
      "loss": 0.6659,
      "step": 1856900
    },
    {
      "epoch": 16.94466749397766,
      "grad_norm": 3.267493486404419,
      "learning_rate": 3.587944375501862e-05,
      "loss": 0.6976,
      "step": 1857000
    },
    {
      "epoch": 16.945579969340827,
      "grad_norm": 3.8190789222717285,
      "learning_rate": 3.587868335888265e-05,
      "loss": 0.6989,
      "step": 1857100
    },
    {
      "epoch": 16.946492444703992,
      "grad_norm": 4.243678569793701,
      "learning_rate": 3.587792296274667e-05,
      "loss": 0.6787,
      "step": 1857200
    },
    {
      "epoch": 16.947404920067157,
      "grad_norm": 3.8096671104431152,
      "learning_rate": 3.587716256661071e-05,
      "loss": 0.6391,
      "step": 1857300
    },
    {
      "epoch": 16.948317395430323,
      "grad_norm": 3.3035991191864014,
      "learning_rate": 3.587640217047473e-05,
      "loss": 0.6483,
      "step": 1857400
    },
    {
      "epoch": 16.949229870793488,
      "grad_norm": 4.539987087249756,
      "learning_rate": 3.587564177433876e-05,
      "loss": 0.6821,
      "step": 1857500
    },
    {
      "epoch": 16.950142346156653,
      "grad_norm": 4.191954135894775,
      "learning_rate": 3.587488137820279e-05,
      "loss": 0.6749,
      "step": 1857600
    },
    {
      "epoch": 16.95105482151982,
      "grad_norm": 4.268424034118652,
      "learning_rate": 3.5874120982066815e-05,
      "loss": 0.6999,
      "step": 1857700
    },
    {
      "epoch": 16.951967296882984,
      "grad_norm": 4.483989238739014,
      "learning_rate": 3.587336058593085e-05,
      "loss": 0.6812,
      "step": 1857800
    },
    {
      "epoch": 16.95287977224615,
      "grad_norm": 3.7528421878814697,
      "learning_rate": 3.5872600189794875e-05,
      "loss": 0.6681,
      "step": 1857900
    },
    {
      "epoch": 16.953792247609314,
      "grad_norm": 3.9454104900360107,
      "learning_rate": 3.5871839793658905e-05,
      "loss": 0.6324,
      "step": 1858000
    },
    {
      "epoch": 16.95470472297248,
      "grad_norm": 3.7103183269500732,
      "learning_rate": 3.5871079397522935e-05,
      "loss": 0.6796,
      "step": 1858100
    },
    {
      "epoch": 16.955617198335645,
      "grad_norm": 3.6929683685302734,
      "learning_rate": 3.5870319001386965e-05,
      "loss": 0.6378,
      "step": 1858200
    },
    {
      "epoch": 16.95652967369881,
      "grad_norm": 3.512277364730835,
      "learning_rate": 3.586955860525099e-05,
      "loss": 0.6656,
      "step": 1858300
    },
    {
      "epoch": 16.957442149061976,
      "grad_norm": 3.2696027755737305,
      "learning_rate": 3.5868798209115026e-05,
      "loss": 0.6593,
      "step": 1858400
    },
    {
      "epoch": 16.95835462442514,
      "grad_norm": 3.863661289215088,
      "learning_rate": 3.586803781297905e-05,
      "loss": 0.6606,
      "step": 1858500
    },
    {
      "epoch": 16.959267099788306,
      "grad_norm": 3.504020929336548,
      "learning_rate": 3.586727741684308e-05,
      "loss": 0.6673,
      "step": 1858600
    },
    {
      "epoch": 16.96017957515147,
      "grad_norm": 3.7964162826538086,
      "learning_rate": 3.586651702070711e-05,
      "loss": 0.7074,
      "step": 1858700
    },
    {
      "epoch": 16.961092050514637,
      "grad_norm": 3.488225221633911,
      "learning_rate": 3.586575662457114e-05,
      "loss": 0.6687,
      "step": 1858800
    },
    {
      "epoch": 16.962004525877802,
      "grad_norm": 4.12342643737793,
      "learning_rate": 3.586499622843517e-05,
      "loss": 0.6658,
      "step": 1858900
    },
    {
      "epoch": 16.962917001240967,
      "grad_norm": 4.533052444458008,
      "learning_rate": 3.58642358322992e-05,
      "loss": 0.6885,
      "step": 1859000
    },
    {
      "epoch": 16.963829476604133,
      "grad_norm": 2.467485189437866,
      "learning_rate": 3.586347543616322e-05,
      "loss": 0.646,
      "step": 1859100
    },
    {
      "epoch": 16.964741951967298,
      "grad_norm": 4.595128536224365,
      "learning_rate": 3.586271504002726e-05,
      "loss": 0.6532,
      "step": 1859200
    },
    {
      "epoch": 16.965654427330463,
      "grad_norm": 3.581758499145508,
      "learning_rate": 3.586195464389128e-05,
      "loss": 0.714,
      "step": 1859300
    },
    {
      "epoch": 16.96656690269363,
      "grad_norm": 3.70733642578125,
      "learning_rate": 3.586119424775531e-05,
      "loss": 0.6716,
      "step": 1859400
    },
    {
      "epoch": 16.967479378056794,
      "grad_norm": 3.3570117950439453,
      "learning_rate": 3.586043385161934e-05,
      "loss": 0.6773,
      "step": 1859500
    },
    {
      "epoch": 16.96839185341996,
      "grad_norm": 4.051323890686035,
      "learning_rate": 3.585967345548337e-05,
      "loss": 0.6704,
      "step": 1859600
    },
    {
      "epoch": 16.969304328783124,
      "grad_norm": 4.598097324371338,
      "learning_rate": 3.5858913059347396e-05,
      "loss": 0.6575,
      "step": 1859700
    },
    {
      "epoch": 16.97021680414629,
      "grad_norm": 4.2599077224731445,
      "learning_rate": 3.585815266321143e-05,
      "loss": 0.646,
      "step": 1859800
    },
    {
      "epoch": 16.971129279509455,
      "grad_norm": 4.193840980529785,
      "learning_rate": 3.5857392267075456e-05,
      "loss": 0.6694,
      "step": 1859900
    },
    {
      "epoch": 16.972041754872617,
      "grad_norm": 4.065178871154785,
      "learning_rate": 3.5856631870939486e-05,
      "loss": 0.6674,
      "step": 1860000
    },
    {
      "epoch": 16.972954230235782,
      "grad_norm": 3.8818297386169434,
      "learning_rate": 3.5855871474803516e-05,
      "loss": 0.6475,
      "step": 1860100
    },
    {
      "epoch": 16.973866705598947,
      "grad_norm": 4.351805686950684,
      "learning_rate": 3.5855111078667546e-05,
      "loss": 0.6706,
      "step": 1860200
    },
    {
      "epoch": 16.974779180962113,
      "grad_norm": 4.542895317077637,
      "learning_rate": 3.5854350682531576e-05,
      "loss": 0.6656,
      "step": 1860300
    },
    {
      "epoch": 16.975691656325278,
      "grad_norm": 3.6348748207092285,
      "learning_rate": 3.58535902863956e-05,
      "loss": 0.6368,
      "step": 1860400
    },
    {
      "epoch": 16.976604131688443,
      "grad_norm": 3.7706782817840576,
      "learning_rate": 3.585282989025963e-05,
      "loss": 0.6713,
      "step": 1860500
    },
    {
      "epoch": 16.97751660705161,
      "grad_norm": 4.530024528503418,
      "learning_rate": 3.585206949412366e-05,
      "loss": 0.6465,
      "step": 1860600
    },
    {
      "epoch": 16.978429082414774,
      "grad_norm": 5.085880756378174,
      "learning_rate": 3.585130909798769e-05,
      "loss": 0.6529,
      "step": 1860700
    },
    {
      "epoch": 16.97934155777794,
      "grad_norm": 3.819523334503174,
      "learning_rate": 3.585054870185171e-05,
      "loss": 0.6679,
      "step": 1860800
    },
    {
      "epoch": 16.980254033141104,
      "grad_norm": 3.9158530235290527,
      "learning_rate": 3.584978830571575e-05,
      "loss": 0.6373,
      "step": 1860900
    },
    {
      "epoch": 16.98116650850427,
      "grad_norm": 5.204843044281006,
      "learning_rate": 3.584902790957977e-05,
      "loss": 0.652,
      "step": 1861000
    },
    {
      "epoch": 16.982078983867435,
      "grad_norm": 4.632448196411133,
      "learning_rate": 3.5848267513443803e-05,
      "loss": 0.6619,
      "step": 1861100
    },
    {
      "epoch": 16.9829914592306,
      "grad_norm": 3.7145566940307617,
      "learning_rate": 3.5847507117307834e-05,
      "loss": 0.663,
      "step": 1861200
    },
    {
      "epoch": 16.983903934593766,
      "grad_norm": 4.6994452476501465,
      "learning_rate": 3.5846746721171864e-05,
      "loss": 0.6791,
      "step": 1861300
    },
    {
      "epoch": 16.98481640995693,
      "grad_norm": 3.211120128631592,
      "learning_rate": 3.5845986325035894e-05,
      "loss": 0.6636,
      "step": 1861400
    },
    {
      "epoch": 16.985728885320096,
      "grad_norm": 4.1886396408081055,
      "learning_rate": 3.5845225928899924e-05,
      "loss": 0.6262,
      "step": 1861500
    },
    {
      "epoch": 16.98664136068326,
      "grad_norm": 3.228182315826416,
      "learning_rate": 3.584446553276395e-05,
      "loss": 0.6846,
      "step": 1861600
    },
    {
      "epoch": 16.987553836046427,
      "grad_norm": 4.421499252319336,
      "learning_rate": 3.5843705136627984e-05,
      "loss": 0.6847,
      "step": 1861700
    },
    {
      "epoch": 16.988466311409592,
      "grad_norm": 3.3066177368164062,
      "learning_rate": 3.584294474049201e-05,
      "loss": 0.6606,
      "step": 1861800
    },
    {
      "epoch": 16.989378786772757,
      "grad_norm": 4.062910079956055,
      "learning_rate": 3.584218434435604e-05,
      "loss": 0.686,
      "step": 1861900
    },
    {
      "epoch": 16.990291262135923,
      "grad_norm": 4.081923484802246,
      "learning_rate": 3.584142394822007e-05,
      "loss": 0.6929,
      "step": 1862000
    },
    {
      "epoch": 16.991203737499088,
      "grad_norm": 4.095438003540039,
      "learning_rate": 3.58406635520841e-05,
      "loss": 0.635,
      "step": 1862100
    },
    {
      "epoch": 16.992116212862253,
      "grad_norm": 3.9530751705169678,
      "learning_rate": 3.583990315594812e-05,
      "loss": 0.6739,
      "step": 1862200
    },
    {
      "epoch": 16.99302868822542,
      "grad_norm": 3.8087565898895264,
      "learning_rate": 3.583914275981216e-05,
      "loss": 0.6425,
      "step": 1862300
    },
    {
      "epoch": 16.993941163588584,
      "grad_norm": 3.7279317378997803,
      "learning_rate": 3.583838236367618e-05,
      "loss": 0.608,
      "step": 1862400
    },
    {
      "epoch": 16.99485363895175,
      "grad_norm": 4.425987243652344,
      "learning_rate": 3.583762196754021e-05,
      "loss": 0.6858,
      "step": 1862500
    },
    {
      "epoch": 16.995766114314915,
      "grad_norm": 4.0839056968688965,
      "learning_rate": 3.583686157140424e-05,
      "loss": 0.6366,
      "step": 1862600
    },
    {
      "epoch": 16.99667858967808,
      "grad_norm": 3.8260490894317627,
      "learning_rate": 3.583610117526827e-05,
      "loss": 0.6739,
      "step": 1862700
    },
    {
      "epoch": 16.997591065041245,
      "grad_norm": 4.939646244049072,
      "learning_rate": 3.58353407791323e-05,
      "loss": 0.6811,
      "step": 1862800
    },
    {
      "epoch": 16.99850354040441,
      "grad_norm": 3.6688413619995117,
      "learning_rate": 3.583458038299633e-05,
      "loss": 0.7001,
      "step": 1862900
    },
    {
      "epoch": 16.999416015767576,
      "grad_norm": 3.7007884979248047,
      "learning_rate": 3.5833819986860354e-05,
      "loss": 0.6826,
      "step": 1863000
    },
    {
      "epoch": 17.0,
      "eval_loss": 0.5414525270462036,
      "eval_runtime": 25.267,
      "eval_samples_per_second": 228.322,
      "eval_steps_per_second": 228.322,
      "step": 1863064
    },
    {
      "epoch": 17.0,
      "eval_loss": 0.5192579627037048,
      "eval_runtime": 482.4358,
      "eval_samples_per_second": 227.164,
      "eval_steps_per_second": 227.164,
      "step": 1863064
    },
    {
      "epoch": 17.00032849113074,
      "grad_norm": 3.3961877822875977,
      "learning_rate": 3.583305959072439e-05,
      "loss": 0.6494,
      "step": 1863100
    },
    {
      "epoch": 17.001240966493906,
      "grad_norm": 3.910116195678711,
      "learning_rate": 3.5832299194588415e-05,
      "loss": 0.6613,
      "step": 1863200
    },
    {
      "epoch": 17.002153441857068,
      "grad_norm": 3.484689235687256,
      "learning_rate": 3.583153879845244e-05,
      "loss": 0.6661,
      "step": 1863300
    },
    {
      "epoch": 17.003065917220233,
      "grad_norm": 4.8732428550720215,
      "learning_rate": 3.5830778402316475e-05,
      "loss": 0.6515,
      "step": 1863400
    },
    {
      "epoch": 17.0039783925834,
      "grad_norm": 4.488743782043457,
      "learning_rate": 3.58300180061805e-05,
      "loss": 0.6799,
      "step": 1863500
    },
    {
      "epoch": 17.004890867946564,
      "grad_norm": 3.7854397296905518,
      "learning_rate": 3.582925761004453e-05,
      "loss": 0.6604,
      "step": 1863600
    },
    {
      "epoch": 17.00580334330973,
      "grad_norm": 4.112785816192627,
      "learning_rate": 3.582849721390856e-05,
      "loss": 0.677,
      "step": 1863700
    },
    {
      "epoch": 17.006715818672895,
      "grad_norm": 3.930629253387451,
      "learning_rate": 3.582773681777259e-05,
      "loss": 0.6768,
      "step": 1863800
    },
    {
      "epoch": 17.00762829403606,
      "grad_norm": 4.1840500831604,
      "learning_rate": 3.582697642163662e-05,
      "loss": 0.6703,
      "step": 1863900
    },
    {
      "epoch": 17.008540769399225,
      "grad_norm": 3.347353458404541,
      "learning_rate": 3.582621602550065e-05,
      "loss": 0.6318,
      "step": 1864000
    },
    {
      "epoch": 17.00945324476239,
      "grad_norm": 4.032279968261719,
      "learning_rate": 3.582545562936467e-05,
      "loss": 0.6874,
      "step": 1864100
    },
    {
      "epoch": 17.010365720125556,
      "grad_norm": 3.7141871452331543,
      "learning_rate": 3.582469523322871e-05,
      "loss": 0.6703,
      "step": 1864200
    },
    {
      "epoch": 17.01127819548872,
      "grad_norm": 3.897848129272461,
      "learning_rate": 3.582393483709273e-05,
      "loss": 0.6698,
      "step": 1864300
    },
    {
      "epoch": 17.012190670851886,
      "grad_norm": 3.248410940170288,
      "learning_rate": 3.582317444095676e-05,
      "loss": 0.6642,
      "step": 1864400
    },
    {
      "epoch": 17.01310314621505,
      "grad_norm": 3.2052481174468994,
      "learning_rate": 3.582241404482079e-05,
      "loss": 0.6669,
      "step": 1864500
    },
    {
      "epoch": 17.014015621578217,
      "grad_norm": 4.031614303588867,
      "learning_rate": 3.582165364868482e-05,
      "loss": 0.6736,
      "step": 1864600
    },
    {
      "epoch": 17.014928096941382,
      "grad_norm": 4.458009243011475,
      "learning_rate": 3.5820893252548845e-05,
      "loss": 0.7037,
      "step": 1864700
    },
    {
      "epoch": 17.015840572304548,
      "grad_norm": 3.972820520401001,
      "learning_rate": 3.582013285641288e-05,
      "loss": 0.6712,
      "step": 1864800
    },
    {
      "epoch": 17.016753047667713,
      "grad_norm": 3.460149049758911,
      "learning_rate": 3.5819372460276905e-05,
      "loss": 0.6145,
      "step": 1864900
    },
    {
      "epoch": 17.017665523030878,
      "grad_norm": 4.600976943969727,
      "learning_rate": 3.5818612064140935e-05,
      "loss": 0.6591,
      "step": 1865000
    },
    {
      "epoch": 17.018577998394044,
      "grad_norm": 4.476378917694092,
      "learning_rate": 3.5817851668004965e-05,
      "loss": 0.6275,
      "step": 1865100
    },
    {
      "epoch": 17.01949047375721,
      "grad_norm": 3.811915397644043,
      "learning_rate": 3.5817091271868996e-05,
      "loss": 0.6683,
      "step": 1865200
    },
    {
      "epoch": 17.020402949120374,
      "grad_norm": 3.550015926361084,
      "learning_rate": 3.5816330875733026e-05,
      "loss": 0.6445,
      "step": 1865300
    },
    {
      "epoch": 17.02131542448354,
      "grad_norm": 3.3466529846191406,
      "learning_rate": 3.5815570479597056e-05,
      "loss": 0.6793,
      "step": 1865400
    },
    {
      "epoch": 17.022227899846705,
      "grad_norm": 3.1231045722961426,
      "learning_rate": 3.581481008346108e-05,
      "loss": 0.6366,
      "step": 1865500
    },
    {
      "epoch": 17.02314037520987,
      "grad_norm": 3.383493423461914,
      "learning_rate": 3.5814049687325116e-05,
      "loss": 0.7037,
      "step": 1865600
    },
    {
      "epoch": 17.024052850573035,
      "grad_norm": 2.9880285263061523,
      "learning_rate": 3.581328929118914e-05,
      "loss": 0.6271,
      "step": 1865700
    },
    {
      "epoch": 17.0249653259362,
      "grad_norm": 4.126856327056885,
      "learning_rate": 3.581252889505317e-05,
      "loss": 0.6664,
      "step": 1865800
    },
    {
      "epoch": 17.025877801299366,
      "grad_norm": 4.139505863189697,
      "learning_rate": 3.58117684989172e-05,
      "loss": 0.6416,
      "step": 1865900
    },
    {
      "epoch": 17.02679027666253,
      "grad_norm": 4.2704877853393555,
      "learning_rate": 3.581100810278122e-05,
      "loss": 0.7161,
      "step": 1866000
    },
    {
      "epoch": 17.027702752025696,
      "grad_norm": 3.934684991836548,
      "learning_rate": 3.581024770664525e-05,
      "loss": 0.6431,
      "step": 1866100
    },
    {
      "epoch": 17.028615227388862,
      "grad_norm": 2.73834490776062,
      "learning_rate": 3.580948731050928e-05,
      "loss": 0.6746,
      "step": 1866200
    },
    {
      "epoch": 17.029527702752027,
      "grad_norm": 3.713496685028076,
      "learning_rate": 3.580872691437331e-05,
      "loss": 0.6713,
      "step": 1866300
    },
    {
      "epoch": 17.030440178115192,
      "grad_norm": 4.003138065338135,
      "learning_rate": 3.580796651823734e-05,
      "loss": 0.6284,
      "step": 1866400
    },
    {
      "epoch": 17.031352653478358,
      "grad_norm": 3.741262674331665,
      "learning_rate": 3.580720612210137e-05,
      "loss": 0.6147,
      "step": 1866500
    },
    {
      "epoch": 17.032265128841523,
      "grad_norm": 3.9372589588165283,
      "learning_rate": 3.5806445725965396e-05,
      "loss": 0.6669,
      "step": 1866600
    },
    {
      "epoch": 17.033177604204685,
      "grad_norm": 4.654572486877441,
      "learning_rate": 3.580568532982943e-05,
      "loss": 0.6396,
      "step": 1866700
    },
    {
      "epoch": 17.03409007956785,
      "grad_norm": 4.200795650482178,
      "learning_rate": 3.5804924933693456e-05,
      "loss": 0.6519,
      "step": 1866800
    },
    {
      "epoch": 17.035002554931015,
      "grad_norm": 3.2238247394561768,
      "learning_rate": 3.5804164537557486e-05,
      "loss": 0.669,
      "step": 1866900
    },
    {
      "epoch": 17.03591503029418,
      "grad_norm": 3.2693610191345215,
      "learning_rate": 3.5803404141421516e-05,
      "loss": 0.6598,
      "step": 1867000
    },
    {
      "epoch": 17.036827505657346,
      "grad_norm": 4.41683292388916,
      "learning_rate": 3.5802643745285546e-05,
      "loss": 0.6521,
      "step": 1867100
    },
    {
      "epoch": 17.03773998102051,
      "grad_norm": 3.9063825607299805,
      "learning_rate": 3.580188334914957e-05,
      "loss": 0.6941,
      "step": 1867200
    },
    {
      "epoch": 17.038652456383677,
      "grad_norm": 3.211949110031128,
      "learning_rate": 3.580112295301361e-05,
      "loss": 0.6841,
      "step": 1867300
    },
    {
      "epoch": 17.039564931746842,
      "grad_norm": 3.5741682052612305,
      "learning_rate": 3.580036255687763e-05,
      "loss": 0.6681,
      "step": 1867400
    },
    {
      "epoch": 17.040477407110007,
      "grad_norm": 2.116891622543335,
      "learning_rate": 3.579960216074166e-05,
      "loss": 0.6385,
      "step": 1867500
    },
    {
      "epoch": 17.041389882473172,
      "grad_norm": 4.222330570220947,
      "learning_rate": 3.579884176460569e-05,
      "loss": 0.6445,
      "step": 1867600
    },
    {
      "epoch": 17.042302357836338,
      "grad_norm": 2.448852777481079,
      "learning_rate": 3.579808136846972e-05,
      "loss": 0.6299,
      "step": 1867700
    },
    {
      "epoch": 17.043214833199503,
      "grad_norm": 2.6842854022979736,
      "learning_rate": 3.579732097233375e-05,
      "loss": 0.6755,
      "step": 1867800
    },
    {
      "epoch": 17.04412730856267,
      "grad_norm": 4.464443206787109,
      "learning_rate": 3.579656057619778e-05,
      "loss": 0.6958,
      "step": 1867900
    },
    {
      "epoch": 17.045039783925834,
      "grad_norm": 3.66036057472229,
      "learning_rate": 3.5795800180061804e-05,
      "loss": 0.6749,
      "step": 1868000
    },
    {
      "epoch": 17.045952259289,
      "grad_norm": 3.1514151096343994,
      "learning_rate": 3.579503978392584e-05,
      "loss": 0.6637,
      "step": 1868100
    },
    {
      "epoch": 17.046864734652164,
      "grad_norm": 4.696743965148926,
      "learning_rate": 3.5794279387789864e-05,
      "loss": 0.7029,
      "step": 1868200
    },
    {
      "epoch": 17.04777721001533,
      "grad_norm": 3.3630712032318115,
      "learning_rate": 3.5793518991653894e-05,
      "loss": 0.6273,
      "step": 1868300
    },
    {
      "epoch": 17.048689685378495,
      "grad_norm": 3.608621120452881,
      "learning_rate": 3.5792758595517924e-05,
      "loss": 0.6461,
      "step": 1868400
    },
    {
      "epoch": 17.04960216074166,
      "grad_norm": 4.133382320404053,
      "learning_rate": 3.5791998199381954e-05,
      "loss": 0.6543,
      "step": 1868500
    },
    {
      "epoch": 17.050514636104825,
      "grad_norm": 4.137103080749512,
      "learning_rate": 3.579123780324598e-05,
      "loss": 0.6451,
      "step": 1868600
    },
    {
      "epoch": 17.05142711146799,
      "grad_norm": 3.6896653175354004,
      "learning_rate": 3.5790477407110014e-05,
      "loss": 0.6566,
      "step": 1868700
    },
    {
      "epoch": 17.052339586831156,
      "grad_norm": 4.23118257522583,
      "learning_rate": 3.578971701097404e-05,
      "loss": 0.6654,
      "step": 1868800
    },
    {
      "epoch": 17.05325206219432,
      "grad_norm": 4.798270225524902,
      "learning_rate": 3.578895661483807e-05,
      "loss": 0.6721,
      "step": 1868900
    },
    {
      "epoch": 17.054164537557487,
      "grad_norm": 4.973593711853027,
      "learning_rate": 3.57881962187021e-05,
      "loss": 0.6597,
      "step": 1869000
    },
    {
      "epoch": 17.055077012920652,
      "grad_norm": 4.208169460296631,
      "learning_rate": 3.578743582256612e-05,
      "loss": 0.6699,
      "step": 1869100
    },
    {
      "epoch": 17.055989488283817,
      "grad_norm": 4.337337493896484,
      "learning_rate": 3.578667542643016e-05,
      "loss": 0.6352,
      "step": 1869200
    },
    {
      "epoch": 17.056901963646983,
      "grad_norm": 2.5449211597442627,
      "learning_rate": 3.578591503029418e-05,
      "loss": 0.6225,
      "step": 1869300
    },
    {
      "epoch": 17.057814439010148,
      "grad_norm": 3.48860764503479,
      "learning_rate": 3.578515463415821e-05,
      "loss": 0.6801,
      "step": 1869400
    },
    {
      "epoch": 17.058726914373313,
      "grad_norm": 3.909973621368408,
      "learning_rate": 3.578439423802224e-05,
      "loss": 0.6374,
      "step": 1869500
    },
    {
      "epoch": 17.05963938973648,
      "grad_norm": 4.438810348510742,
      "learning_rate": 3.578363384188627e-05,
      "loss": 0.65,
      "step": 1869600
    },
    {
      "epoch": 17.060551865099644,
      "grad_norm": 3.4136407375335693,
      "learning_rate": 3.57828734457503e-05,
      "loss": 0.687,
      "step": 1869700
    },
    {
      "epoch": 17.06146434046281,
      "grad_norm": 5.777629852294922,
      "learning_rate": 3.578211304961433e-05,
      "loss": 0.7324,
      "step": 1869800
    },
    {
      "epoch": 17.062376815825974,
      "grad_norm": 4.116573333740234,
      "learning_rate": 3.5781352653478354e-05,
      "loss": 0.6762,
      "step": 1869900
    },
    {
      "epoch": 17.06328929118914,
      "grad_norm": 4.075562000274658,
      "learning_rate": 3.578059225734239e-05,
      "loss": 0.6533,
      "step": 1870000
    },
    {
      "epoch": 17.0642017665523,
      "grad_norm": 3.625439405441284,
      "learning_rate": 3.5779831861206415e-05,
      "loss": 0.6815,
      "step": 1870100
    },
    {
      "epoch": 17.065114241915467,
      "grad_norm": 3.850283145904541,
      "learning_rate": 3.5779071465070445e-05,
      "loss": 0.6643,
      "step": 1870200
    },
    {
      "epoch": 17.066026717278632,
      "grad_norm": 3.693506956100464,
      "learning_rate": 3.5778311068934475e-05,
      "loss": 0.624,
      "step": 1870300
    },
    {
      "epoch": 17.066939192641797,
      "grad_norm": 3.004300832748413,
      "learning_rate": 3.5777550672798505e-05,
      "loss": 0.6376,
      "step": 1870400
    },
    {
      "epoch": 17.067851668004963,
      "grad_norm": 4.439387798309326,
      "learning_rate": 3.577679027666253e-05,
      "loss": 0.691,
      "step": 1870500
    },
    {
      "epoch": 17.068764143368128,
      "grad_norm": 4.625782489776611,
      "learning_rate": 3.5776029880526565e-05,
      "loss": 0.6183,
      "step": 1870600
    },
    {
      "epoch": 17.069676618731293,
      "grad_norm": 4.027177810668945,
      "learning_rate": 3.577526948439059e-05,
      "loss": 0.6342,
      "step": 1870700
    },
    {
      "epoch": 17.07058909409446,
      "grad_norm": 4.869943141937256,
      "learning_rate": 3.577450908825462e-05,
      "loss": 0.662,
      "step": 1870800
    },
    {
      "epoch": 17.071501569457624,
      "grad_norm": 3.529381036758423,
      "learning_rate": 3.577374869211865e-05,
      "loss": 0.6778,
      "step": 1870900
    },
    {
      "epoch": 17.07241404482079,
      "grad_norm": 4.457080364227295,
      "learning_rate": 3.577298829598268e-05,
      "loss": 0.6156,
      "step": 1871000
    },
    {
      "epoch": 17.073326520183954,
      "grad_norm": 4.043243408203125,
      "learning_rate": 3.577222789984671e-05,
      "loss": 0.6697,
      "step": 1871100
    },
    {
      "epoch": 17.07423899554712,
      "grad_norm": 3.845871686935425,
      "learning_rate": 3.577146750371074e-05,
      "loss": 0.6945,
      "step": 1871200
    },
    {
      "epoch": 17.075151470910285,
      "grad_norm": 4.227226734161377,
      "learning_rate": 3.577070710757476e-05,
      "loss": 0.6454,
      "step": 1871300
    },
    {
      "epoch": 17.07606394627345,
      "grad_norm": 3.8916430473327637,
      "learning_rate": 3.57699467114388e-05,
      "loss": 0.6428,
      "step": 1871400
    },
    {
      "epoch": 17.076976421636616,
      "grad_norm": 3.811309814453125,
      "learning_rate": 3.576918631530282e-05,
      "loss": 0.6591,
      "step": 1871500
    },
    {
      "epoch": 17.07788889699978,
      "grad_norm": 4.176986217498779,
      "learning_rate": 3.576842591916685e-05,
      "loss": 0.6766,
      "step": 1871600
    },
    {
      "epoch": 17.078801372362946,
      "grad_norm": 4.050285816192627,
      "learning_rate": 3.576766552303088e-05,
      "loss": 0.6452,
      "step": 1871700
    },
    {
      "epoch": 17.07971384772611,
      "grad_norm": 4.163805961608887,
      "learning_rate": 3.5766905126894905e-05,
      "loss": 0.6308,
      "step": 1871800
    },
    {
      "epoch": 17.080626323089277,
      "grad_norm": 3.9552409648895264,
      "learning_rate": 3.5766144730758935e-05,
      "loss": 0.6269,
      "step": 1871900
    },
    {
      "epoch": 17.081538798452442,
      "grad_norm": 3.878744602203369,
      "learning_rate": 3.5765384334622966e-05,
      "loss": 0.6635,
      "step": 1872000
    },
    {
      "epoch": 17.082451273815607,
      "grad_norm": 4.097179412841797,
      "learning_rate": 3.5764623938486996e-05,
      "loss": 0.7237,
      "step": 1872100
    },
    {
      "epoch": 17.083363749178773,
      "grad_norm": 3.8695924282073975,
      "learning_rate": 3.5763863542351026e-05,
      "loss": 0.6646,
      "step": 1872200
    },
    {
      "epoch": 17.084276224541938,
      "grad_norm": 4.427994251251221,
      "learning_rate": 3.5763103146215056e-05,
      "loss": 0.6497,
      "step": 1872300
    },
    {
      "epoch": 17.085188699905103,
      "grad_norm": 3.7003400325775146,
      "learning_rate": 3.576234275007908e-05,
      "loss": 0.6947,
      "step": 1872400
    },
    {
      "epoch": 17.08610117526827,
      "grad_norm": 3.9233360290527344,
      "learning_rate": 3.5761582353943116e-05,
      "loss": 0.6476,
      "step": 1872500
    },
    {
      "epoch": 17.087013650631434,
      "grad_norm": 3.4331648349761963,
      "learning_rate": 3.576082195780714e-05,
      "loss": 0.6103,
      "step": 1872600
    },
    {
      "epoch": 17.0879261259946,
      "grad_norm": 4.283672332763672,
      "learning_rate": 3.576006156167117e-05,
      "loss": 0.6476,
      "step": 1872700
    },
    {
      "epoch": 17.088838601357764,
      "grad_norm": 3.29128360748291,
      "learning_rate": 3.57593011655352e-05,
      "loss": 0.6414,
      "step": 1872800
    },
    {
      "epoch": 17.08975107672093,
      "grad_norm": 3.8056552410125732,
      "learning_rate": 3.575854076939923e-05,
      "loss": 0.6493,
      "step": 1872900
    },
    {
      "epoch": 17.090663552084095,
      "grad_norm": 3.285316228866577,
      "learning_rate": 3.575778037326325e-05,
      "loss": 0.6251,
      "step": 1873000
    },
    {
      "epoch": 17.09157602744726,
      "grad_norm": 3.8071370124816895,
      "learning_rate": 3.575701997712729e-05,
      "loss": 0.6398,
      "step": 1873100
    },
    {
      "epoch": 17.092488502810426,
      "grad_norm": 3.620532512664795,
      "learning_rate": 3.575625958099131e-05,
      "loss": 0.6457,
      "step": 1873200
    },
    {
      "epoch": 17.09340097817359,
      "grad_norm": 4.006969451904297,
      "learning_rate": 3.575549918485534e-05,
      "loss": 0.6649,
      "step": 1873300
    },
    {
      "epoch": 17.094313453536756,
      "grad_norm": 4.103077411651611,
      "learning_rate": 3.575473878871937e-05,
      "loss": 0.6831,
      "step": 1873400
    },
    {
      "epoch": 17.095225928899918,
      "grad_norm": 4.057275772094727,
      "learning_rate": 3.57539783925834e-05,
      "loss": 0.6512,
      "step": 1873500
    },
    {
      "epoch": 17.096138404263083,
      "grad_norm": 4.691411972045898,
      "learning_rate": 3.575321799644743e-05,
      "loss": 0.6626,
      "step": 1873600
    },
    {
      "epoch": 17.09705087962625,
      "grad_norm": 3.9529640674591064,
      "learning_rate": 3.575245760031146e-05,
      "loss": 0.6533,
      "step": 1873700
    },
    {
      "epoch": 17.097963354989414,
      "grad_norm": 3.9131505489349365,
      "learning_rate": 3.5751697204175486e-05,
      "loss": 0.67,
      "step": 1873800
    },
    {
      "epoch": 17.09887583035258,
      "grad_norm": 2.920199155807495,
      "learning_rate": 3.575093680803952e-05,
      "loss": 0.6538,
      "step": 1873900
    },
    {
      "epoch": 17.099788305715744,
      "grad_norm": 3.982053518295288,
      "learning_rate": 3.5750176411903547e-05,
      "loss": 0.6808,
      "step": 1874000
    },
    {
      "epoch": 17.10070078107891,
      "grad_norm": 5.319494247436523,
      "learning_rate": 3.574941601576758e-05,
      "loss": 0.6528,
      "step": 1874100
    },
    {
      "epoch": 17.101613256442075,
      "grad_norm": 4.009594440460205,
      "learning_rate": 3.574865561963161e-05,
      "loss": 0.654,
      "step": 1874200
    },
    {
      "epoch": 17.10252573180524,
      "grad_norm": 3.3543686866760254,
      "learning_rate": 3.574789522349564e-05,
      "loss": 0.6474,
      "step": 1874300
    },
    {
      "epoch": 17.103438207168406,
      "grad_norm": 5.048376083374023,
      "learning_rate": 3.574713482735966e-05,
      "loss": 0.6571,
      "step": 1874400
    },
    {
      "epoch": 17.10435068253157,
      "grad_norm": 3.501768112182617,
      "learning_rate": 3.57463744312237e-05,
      "loss": 0.6522,
      "step": 1874500
    },
    {
      "epoch": 17.105263157894736,
      "grad_norm": 4.332716941833496,
      "learning_rate": 3.574561403508772e-05,
      "loss": 0.6595,
      "step": 1874600
    },
    {
      "epoch": 17.1061756332579,
      "grad_norm": 3.1667590141296387,
      "learning_rate": 3.574485363895175e-05,
      "loss": 0.655,
      "step": 1874700
    },
    {
      "epoch": 17.107088108621067,
      "grad_norm": 4.370345115661621,
      "learning_rate": 3.574409324281578e-05,
      "loss": 0.6288,
      "step": 1874800
    },
    {
      "epoch": 17.108000583984232,
      "grad_norm": 4.16849946975708,
      "learning_rate": 3.5743332846679804e-05,
      "loss": 0.6689,
      "step": 1874900
    },
    {
      "epoch": 17.108913059347397,
      "grad_norm": 2.8634190559387207,
      "learning_rate": 3.574257245054384e-05,
      "loss": 0.6938,
      "step": 1875000
    },
    {
      "epoch": 17.109825534710563,
      "grad_norm": 4.376382350921631,
      "learning_rate": 3.5741812054407864e-05,
      "loss": 0.6387,
      "step": 1875100
    },
    {
      "epoch": 17.110738010073728,
      "grad_norm": 4.2413225173950195,
      "learning_rate": 3.5741051658271894e-05,
      "loss": 0.6661,
      "step": 1875200
    },
    {
      "epoch": 17.111650485436893,
      "grad_norm": 3.7653284072875977,
      "learning_rate": 3.5740291262135924e-05,
      "loss": 0.6492,
      "step": 1875300
    },
    {
      "epoch": 17.11256296080006,
      "grad_norm": 4.408510684967041,
      "learning_rate": 3.5739530865999954e-05,
      "loss": 0.6483,
      "step": 1875400
    },
    {
      "epoch": 17.113475436163224,
      "grad_norm": 3.206413745880127,
      "learning_rate": 3.573877046986398e-05,
      "loss": 0.671,
      "step": 1875500
    },
    {
      "epoch": 17.11438791152639,
      "grad_norm": 3.869055986404419,
      "learning_rate": 3.5738010073728014e-05,
      "loss": 0.6629,
      "step": 1875600
    },
    {
      "epoch": 17.115300386889555,
      "grad_norm": 4.082890510559082,
      "learning_rate": 3.573724967759204e-05,
      "loss": 0.6634,
      "step": 1875700
    },
    {
      "epoch": 17.11621286225272,
      "grad_norm": 2.228203773498535,
      "learning_rate": 3.573648928145607e-05,
      "loss": 0.6625,
      "step": 1875800
    },
    {
      "epoch": 17.117125337615885,
      "grad_norm": 4.13055944442749,
      "learning_rate": 3.57357288853201e-05,
      "loss": 0.6772,
      "step": 1875900
    },
    {
      "epoch": 17.11803781297905,
      "grad_norm": 5.197136402130127,
      "learning_rate": 3.573496848918413e-05,
      "loss": 0.6969,
      "step": 1876000
    },
    {
      "epoch": 17.118950288342216,
      "grad_norm": 4.593947410583496,
      "learning_rate": 3.573420809304816e-05,
      "loss": 0.6208,
      "step": 1876100
    },
    {
      "epoch": 17.11986276370538,
      "grad_norm": 2.874706745147705,
      "learning_rate": 3.573344769691219e-05,
      "loss": 0.6436,
      "step": 1876200
    },
    {
      "epoch": 17.120775239068546,
      "grad_norm": 4.8639631271362305,
      "learning_rate": 3.573268730077621e-05,
      "loss": 0.6697,
      "step": 1876300
    },
    {
      "epoch": 17.12168771443171,
      "grad_norm": 5.350407123565674,
      "learning_rate": 3.573192690464025e-05,
      "loss": 0.6609,
      "step": 1876400
    },
    {
      "epoch": 17.122600189794877,
      "grad_norm": 5.496621131896973,
      "learning_rate": 3.573116650850427e-05,
      "loss": 0.6408,
      "step": 1876500
    },
    {
      "epoch": 17.123512665158042,
      "grad_norm": 4.377712726593018,
      "learning_rate": 3.57304061123683e-05,
      "loss": 0.7052,
      "step": 1876600
    },
    {
      "epoch": 17.124425140521208,
      "grad_norm": 3.582695484161377,
      "learning_rate": 3.572964571623233e-05,
      "loss": 0.6732,
      "step": 1876700
    },
    {
      "epoch": 17.125337615884373,
      "grad_norm": 1.8596850633621216,
      "learning_rate": 3.572888532009636e-05,
      "loss": 0.6935,
      "step": 1876800
    },
    {
      "epoch": 17.126250091247535,
      "grad_norm": 3.9366533756256104,
      "learning_rate": 3.5728124923960385e-05,
      "loss": 0.6551,
      "step": 1876900
    },
    {
      "epoch": 17.1271625666107,
      "grad_norm": 3.449587345123291,
      "learning_rate": 3.572736452782442e-05,
      "loss": 0.6112,
      "step": 1877000
    },
    {
      "epoch": 17.128075041973865,
      "grad_norm": 3.9191384315490723,
      "learning_rate": 3.5726604131688445e-05,
      "loss": 0.6664,
      "step": 1877100
    },
    {
      "epoch": 17.12898751733703,
      "grad_norm": 3.163001775741577,
      "learning_rate": 3.5725843735552475e-05,
      "loss": 0.6832,
      "step": 1877200
    },
    {
      "epoch": 17.129899992700196,
      "grad_norm": 4.25260066986084,
      "learning_rate": 3.5725083339416505e-05,
      "loss": 0.6761,
      "step": 1877300
    },
    {
      "epoch": 17.13081246806336,
      "grad_norm": 3.055607557296753,
      "learning_rate": 3.572432294328053e-05,
      "loss": 0.619,
      "step": 1877400
    },
    {
      "epoch": 17.131724943426526,
      "grad_norm": 2.378068208694458,
      "learning_rate": 3.5723562547144565e-05,
      "loss": 0.6568,
      "step": 1877500
    },
    {
      "epoch": 17.13263741878969,
      "grad_norm": 4.236119270324707,
      "learning_rate": 3.572280215100859e-05,
      "loss": 0.6613,
      "step": 1877600
    },
    {
      "epoch": 17.133549894152857,
      "grad_norm": 2.609454393386841,
      "learning_rate": 3.572204175487262e-05,
      "loss": 0.7006,
      "step": 1877700
    },
    {
      "epoch": 17.134462369516022,
      "grad_norm": 4.022975444793701,
      "learning_rate": 3.572128135873665e-05,
      "loss": 0.6361,
      "step": 1877800
    },
    {
      "epoch": 17.135374844879188,
      "grad_norm": 4.11399507522583,
      "learning_rate": 3.572052096260068e-05,
      "loss": 0.7092,
      "step": 1877900
    },
    {
      "epoch": 17.136287320242353,
      "grad_norm": 4.470083713531494,
      "learning_rate": 3.57197605664647e-05,
      "loss": 0.6811,
      "step": 1878000
    },
    {
      "epoch": 17.137199795605518,
      "grad_norm": 4.50253963470459,
      "learning_rate": 3.571900017032874e-05,
      "loss": 0.6455,
      "step": 1878100
    },
    {
      "epoch": 17.138112270968684,
      "grad_norm": 4.3947529792785645,
      "learning_rate": 3.571823977419276e-05,
      "loss": 0.6645,
      "step": 1878200
    },
    {
      "epoch": 17.13902474633185,
      "grad_norm": 4.323690891265869,
      "learning_rate": 3.571747937805679e-05,
      "loss": 0.6894,
      "step": 1878300
    },
    {
      "epoch": 17.139937221695014,
      "grad_norm": 3.751652240753174,
      "learning_rate": 3.571671898192082e-05,
      "loss": 0.6124,
      "step": 1878400
    },
    {
      "epoch": 17.14084969705818,
      "grad_norm": 4.389251232147217,
      "learning_rate": 3.571595858578485e-05,
      "loss": 0.689,
      "step": 1878500
    },
    {
      "epoch": 17.141762172421345,
      "grad_norm": 4.018028259277344,
      "learning_rate": 3.571519818964888e-05,
      "loss": 0.6326,
      "step": 1878600
    },
    {
      "epoch": 17.14267464778451,
      "grad_norm": 3.9200663566589355,
      "learning_rate": 3.571443779351291e-05,
      "loss": 0.6459,
      "step": 1878700
    },
    {
      "epoch": 17.143587123147675,
      "grad_norm": 4.085552215576172,
      "learning_rate": 3.5713677397376936e-05,
      "loss": 0.6707,
      "step": 1878800
    },
    {
      "epoch": 17.14449959851084,
      "grad_norm": 3.309159278869629,
      "learning_rate": 3.571291700124097e-05,
      "loss": 0.6623,
      "step": 1878900
    },
    {
      "epoch": 17.145412073874006,
      "grad_norm": 3.6816658973693848,
      "learning_rate": 3.5712156605104996e-05,
      "loss": 0.6958,
      "step": 1879000
    },
    {
      "epoch": 17.14632454923717,
      "grad_norm": 3.8669075965881348,
      "learning_rate": 3.5711396208969026e-05,
      "loss": 0.6454,
      "step": 1879100
    },
    {
      "epoch": 17.147237024600336,
      "grad_norm": 2.885998249053955,
      "learning_rate": 3.5710635812833056e-05,
      "loss": 0.6273,
      "step": 1879200
    },
    {
      "epoch": 17.148149499963502,
      "grad_norm": 3.7421348094940186,
      "learning_rate": 3.5709875416697086e-05,
      "loss": 0.6564,
      "step": 1879300
    },
    {
      "epoch": 17.149061975326667,
      "grad_norm": 5.018141269683838,
      "learning_rate": 3.570911502056111e-05,
      "loss": 0.6277,
      "step": 1879400
    },
    {
      "epoch": 17.149974450689832,
      "grad_norm": 4.352808475494385,
      "learning_rate": 3.5708354624425146e-05,
      "loss": 0.6843,
      "step": 1879500
    },
    {
      "epoch": 17.150886926052998,
      "grad_norm": 3.9772934913635254,
      "learning_rate": 3.570759422828917e-05,
      "loss": 0.6803,
      "step": 1879600
    },
    {
      "epoch": 17.151799401416163,
      "grad_norm": 3.6879732608795166,
      "learning_rate": 3.57068338321532e-05,
      "loss": 0.6631,
      "step": 1879700
    },
    {
      "epoch": 17.15271187677933,
      "grad_norm": 4.474514484405518,
      "learning_rate": 3.570607343601723e-05,
      "loss": 0.6756,
      "step": 1879800
    },
    {
      "epoch": 17.153624352142494,
      "grad_norm": 4.538430690765381,
      "learning_rate": 3.570531303988126e-05,
      "loss": 0.6559,
      "step": 1879900
    },
    {
      "epoch": 17.15453682750566,
      "grad_norm": 3.0857832431793213,
      "learning_rate": 3.570455264374529e-05,
      "loss": 0.6384,
      "step": 1880000
    },
    {
      "epoch": 17.155449302868824,
      "grad_norm": 3.6565427780151367,
      "learning_rate": 3.570379224760932e-05,
      "loss": 0.6966,
      "step": 1880100
    },
    {
      "epoch": 17.15636177823199,
      "grad_norm": 4.507987022399902,
      "learning_rate": 3.570303185147334e-05,
      "loss": 0.6601,
      "step": 1880200
    },
    {
      "epoch": 17.15727425359515,
      "grad_norm": 5.266281604766846,
      "learning_rate": 3.570227145533737e-05,
      "loss": 0.6445,
      "step": 1880300
    },
    {
      "epoch": 17.158186728958317,
      "grad_norm": 4.770707130432129,
      "learning_rate": 3.57015110592014e-05,
      "loss": 0.6587,
      "step": 1880400
    },
    {
      "epoch": 17.159099204321482,
      "grad_norm": 4.077201843261719,
      "learning_rate": 3.570075066306543e-05,
      "loss": 0.6645,
      "step": 1880500
    },
    {
      "epoch": 17.160011679684647,
      "grad_norm": 4.091410160064697,
      "learning_rate": 3.569999026692946e-05,
      "loss": 0.6653,
      "step": 1880600
    },
    {
      "epoch": 17.160924155047812,
      "grad_norm": 3.509021520614624,
      "learning_rate": 3.5699229870793487e-05,
      "loss": 0.6823,
      "step": 1880700
    },
    {
      "epoch": 17.161836630410978,
      "grad_norm": 4.057006359100342,
      "learning_rate": 3.569846947465752e-05,
      "loss": 0.6921,
      "step": 1880800
    },
    {
      "epoch": 17.162749105774143,
      "grad_norm": 4.645350456237793,
      "learning_rate": 3.569770907852155e-05,
      "loss": 0.6914,
      "step": 1880900
    },
    {
      "epoch": 17.16366158113731,
      "grad_norm": 1.6195660829544067,
      "learning_rate": 3.569694868238558e-05,
      "loss": 0.6399,
      "step": 1881000
    },
    {
      "epoch": 17.164574056500474,
      "grad_norm": 3.176731824874878,
      "learning_rate": 3.569618828624961e-05,
      "loss": 0.6398,
      "step": 1881100
    },
    {
      "epoch": 17.16548653186364,
      "grad_norm": 3.9657082557678223,
      "learning_rate": 3.569542789011364e-05,
      "loss": 0.666,
      "step": 1881200
    },
    {
      "epoch": 17.166399007226804,
      "grad_norm": 3.9548540115356445,
      "learning_rate": 3.569466749397766e-05,
      "loss": 0.6747,
      "step": 1881300
    },
    {
      "epoch": 17.16731148258997,
      "grad_norm": 4.249653339385986,
      "learning_rate": 3.56939070978417e-05,
      "loss": 0.6348,
      "step": 1881400
    },
    {
      "epoch": 17.168223957953135,
      "grad_norm": 3.1508092880249023,
      "learning_rate": 3.569314670170572e-05,
      "loss": 0.6272,
      "step": 1881500
    },
    {
      "epoch": 17.1691364333163,
      "grad_norm": 3.3996803760528564,
      "learning_rate": 3.569238630556975e-05,
      "loss": 0.6865,
      "step": 1881600
    },
    {
      "epoch": 17.170048908679465,
      "grad_norm": 3.8193600177764893,
      "learning_rate": 3.569162590943378e-05,
      "loss": 0.6592,
      "step": 1881700
    },
    {
      "epoch": 17.17096138404263,
      "grad_norm": 3.557504653930664,
      "learning_rate": 3.569086551329781e-05,
      "loss": 0.6338,
      "step": 1881800
    },
    {
      "epoch": 17.171873859405796,
      "grad_norm": 4.327785968780518,
      "learning_rate": 3.569010511716184e-05,
      "loss": 0.6915,
      "step": 1881900
    },
    {
      "epoch": 17.17278633476896,
      "grad_norm": 3.9257171154022217,
      "learning_rate": 3.568934472102587e-05,
      "loss": 0.6719,
      "step": 1882000
    },
    {
      "epoch": 17.173698810132127,
      "grad_norm": 3.913994073867798,
      "learning_rate": 3.5688584324889894e-05,
      "loss": 0.6426,
      "step": 1882100
    },
    {
      "epoch": 17.174611285495292,
      "grad_norm": 3.9965457916259766,
      "learning_rate": 3.5687823928753924e-05,
      "loss": 0.683,
      "step": 1882200
    },
    {
      "epoch": 17.175523760858457,
      "grad_norm": 5.117128372192383,
      "learning_rate": 3.5687063532617954e-05,
      "loss": 0.6588,
      "step": 1882300
    },
    {
      "epoch": 17.176436236221623,
      "grad_norm": 3.599780559539795,
      "learning_rate": 3.5686303136481984e-05,
      "loss": 0.6218,
      "step": 1882400
    },
    {
      "epoch": 17.177348711584788,
      "grad_norm": 5.163783073425293,
      "learning_rate": 3.5685542740346014e-05,
      "loss": 0.6238,
      "step": 1882500
    },
    {
      "epoch": 17.178261186947953,
      "grad_norm": 3.925330638885498,
      "learning_rate": 3.5684782344210044e-05,
      "loss": 0.6546,
      "step": 1882600
    },
    {
      "epoch": 17.17917366231112,
      "grad_norm": 3.629354476928711,
      "learning_rate": 3.568402194807407e-05,
      "loss": 0.6856,
      "step": 1882700
    },
    {
      "epoch": 17.180086137674284,
      "grad_norm": 3.15204119682312,
      "learning_rate": 3.5683261551938104e-05,
      "loss": 0.6536,
      "step": 1882800
    },
    {
      "epoch": 17.18099861303745,
      "grad_norm": 4.5637102127075195,
      "learning_rate": 3.568250115580213e-05,
      "loss": 0.6681,
      "step": 1882900
    },
    {
      "epoch": 17.181911088400614,
      "grad_norm": 4.289845943450928,
      "learning_rate": 3.568174075966616e-05,
      "loss": 0.6997,
      "step": 1883000
    },
    {
      "epoch": 17.18282356376378,
      "grad_norm": 4.018284797668457,
      "learning_rate": 3.568098036353019e-05,
      "loss": 0.671,
      "step": 1883100
    },
    {
      "epoch": 17.183736039126945,
      "grad_norm": 3.1754467487335205,
      "learning_rate": 3.568021996739421e-05,
      "loss": 0.6638,
      "step": 1883200
    },
    {
      "epoch": 17.18464851449011,
      "grad_norm": 2.896566152572632,
      "learning_rate": 3.567945957125825e-05,
      "loss": 0.6626,
      "step": 1883300
    },
    {
      "epoch": 17.185560989853276,
      "grad_norm": 3.279841661453247,
      "learning_rate": 3.567869917512227e-05,
      "loss": 0.646,
      "step": 1883400
    },
    {
      "epoch": 17.18647346521644,
      "grad_norm": 4.178499698638916,
      "learning_rate": 3.56779387789863e-05,
      "loss": 0.6435,
      "step": 1883500
    },
    {
      "epoch": 17.187385940579603,
      "grad_norm": 4.480915069580078,
      "learning_rate": 3.567717838285033e-05,
      "loss": 0.6706,
      "step": 1883600
    },
    {
      "epoch": 17.188298415942768,
      "grad_norm": 3.0560386180877686,
      "learning_rate": 3.567641798671436e-05,
      "loss": 0.6759,
      "step": 1883700
    },
    {
      "epoch": 17.189210891305933,
      "grad_norm": 4.096215724945068,
      "learning_rate": 3.5675657590578385e-05,
      "loss": 0.6736,
      "step": 1883800
    },
    {
      "epoch": 17.1901233666691,
      "grad_norm": 4.313704490661621,
      "learning_rate": 3.567489719444242e-05,
      "loss": 0.6506,
      "step": 1883900
    },
    {
      "epoch": 17.191035842032264,
      "grad_norm": 3.6914608478546143,
      "learning_rate": 3.5674136798306445e-05,
      "loss": 0.6205,
      "step": 1884000
    },
    {
      "epoch": 17.19194831739543,
      "grad_norm": 3.1860415935516357,
      "learning_rate": 3.5673376402170475e-05,
      "loss": 0.6791,
      "step": 1884100
    },
    {
      "epoch": 17.192860792758594,
      "grad_norm": 3.0169777870178223,
      "learning_rate": 3.5672616006034505e-05,
      "loss": 0.7192,
      "step": 1884200
    },
    {
      "epoch": 17.19377326812176,
      "grad_norm": 3.8413078784942627,
      "learning_rate": 3.5671855609898535e-05,
      "loss": 0.6401,
      "step": 1884300
    },
    {
      "epoch": 17.194685743484925,
      "grad_norm": 3.921476125717163,
      "learning_rate": 3.5671095213762565e-05,
      "loss": 0.6856,
      "step": 1884400
    },
    {
      "epoch": 17.19559821884809,
      "grad_norm": 4.127665042877197,
      "learning_rate": 3.5670334817626595e-05,
      "loss": 0.6509,
      "step": 1884500
    },
    {
      "epoch": 17.196510694211256,
      "grad_norm": 4.449526309967041,
      "learning_rate": 3.566957442149062e-05,
      "loss": 0.6982,
      "step": 1884600
    },
    {
      "epoch": 17.19742316957442,
      "grad_norm": 3.566293239593506,
      "learning_rate": 3.5668814025354655e-05,
      "loss": 0.6763,
      "step": 1884700
    },
    {
      "epoch": 17.198335644937586,
      "grad_norm": 3.539564371109009,
      "learning_rate": 3.566805362921868e-05,
      "loss": 0.6725,
      "step": 1884800
    },
    {
      "epoch": 17.19924812030075,
      "grad_norm": 3.463784694671631,
      "learning_rate": 3.566729323308271e-05,
      "loss": 0.7048,
      "step": 1884900
    },
    {
      "epoch": 17.200160595663917,
      "grad_norm": 3.6550021171569824,
      "learning_rate": 3.566653283694674e-05,
      "loss": 0.6642,
      "step": 1885000
    },
    {
      "epoch": 17.201073071027082,
      "grad_norm": 5.2585248947143555,
      "learning_rate": 3.566577244081077e-05,
      "loss": 0.6906,
      "step": 1885100
    },
    {
      "epoch": 17.201985546390247,
      "grad_norm": 4.3376078605651855,
      "learning_rate": 3.566501204467479e-05,
      "loss": 0.6388,
      "step": 1885200
    },
    {
      "epoch": 17.202898021753413,
      "grad_norm": 3.8513407707214355,
      "learning_rate": 3.566425164853883e-05,
      "loss": 0.6608,
      "step": 1885300
    },
    {
      "epoch": 17.203810497116578,
      "grad_norm": 2.6279702186584473,
      "learning_rate": 3.566349125240285e-05,
      "loss": 0.6768,
      "step": 1885400
    },
    {
      "epoch": 17.204722972479743,
      "grad_norm": 3.721588134765625,
      "learning_rate": 3.566273085626688e-05,
      "loss": 0.6533,
      "step": 1885500
    },
    {
      "epoch": 17.20563544784291,
      "grad_norm": 3.8981330394744873,
      "learning_rate": 3.566197046013091e-05,
      "loss": 0.6531,
      "step": 1885600
    },
    {
      "epoch": 17.206547923206074,
      "grad_norm": 3.5400443077087402,
      "learning_rate": 3.566121006399494e-05,
      "loss": 0.6666,
      "step": 1885700
    },
    {
      "epoch": 17.20746039856924,
      "grad_norm": 3.1925816535949707,
      "learning_rate": 3.566044966785897e-05,
      "loss": 0.6264,
      "step": 1885800
    },
    {
      "epoch": 17.208372873932404,
      "grad_norm": 3.0438907146453857,
      "learning_rate": 3.5659689271722996e-05,
      "loss": 0.6652,
      "step": 1885900
    },
    {
      "epoch": 17.20928534929557,
      "grad_norm": 4.817929267883301,
      "learning_rate": 3.5658928875587026e-05,
      "loss": 0.684,
      "step": 1886000
    },
    {
      "epoch": 17.210197824658735,
      "grad_norm": 4.158366680145264,
      "learning_rate": 3.5658168479451056e-05,
      "loss": 0.6375,
      "step": 1886100
    },
    {
      "epoch": 17.2111103000219,
      "grad_norm": 3.965601921081543,
      "learning_rate": 3.5657408083315086e-05,
      "loss": 0.6702,
      "step": 1886200
    },
    {
      "epoch": 17.212022775385066,
      "grad_norm": 3.8590598106384277,
      "learning_rate": 3.565664768717911e-05,
      "loss": 0.6207,
      "step": 1886300
    },
    {
      "epoch": 17.21293525074823,
      "grad_norm": 3.7166900634765625,
      "learning_rate": 3.5655887291043146e-05,
      "loss": 0.6444,
      "step": 1886400
    },
    {
      "epoch": 17.213847726111396,
      "grad_norm": 3.9158525466918945,
      "learning_rate": 3.565512689490717e-05,
      "loss": 0.6718,
      "step": 1886500
    },
    {
      "epoch": 17.21476020147456,
      "grad_norm": 5.122063636779785,
      "learning_rate": 3.56543664987712e-05,
      "loss": 0.6782,
      "step": 1886600
    },
    {
      "epoch": 17.215672676837727,
      "grad_norm": 3.669196367263794,
      "learning_rate": 3.565360610263523e-05,
      "loss": 0.6333,
      "step": 1886700
    },
    {
      "epoch": 17.216585152200892,
      "grad_norm": 4.218087673187256,
      "learning_rate": 3.565284570649926e-05,
      "loss": 0.6859,
      "step": 1886800
    },
    {
      "epoch": 17.217497627564057,
      "grad_norm": 4.525373935699463,
      "learning_rate": 3.565208531036329e-05,
      "loss": 0.6646,
      "step": 1886900
    },
    {
      "epoch": 17.218410102927223,
      "grad_norm": 4.237510681152344,
      "learning_rate": 3.565132491422732e-05,
      "loss": 0.6797,
      "step": 1887000
    },
    {
      "epoch": 17.219322578290384,
      "grad_norm": 4.100779056549072,
      "learning_rate": 3.565056451809134e-05,
      "loss": 0.6517,
      "step": 1887100
    },
    {
      "epoch": 17.22023505365355,
      "grad_norm": 3.526466131210327,
      "learning_rate": 3.564980412195538e-05,
      "loss": 0.6144,
      "step": 1887200
    },
    {
      "epoch": 17.221147529016715,
      "grad_norm": 4.006443500518799,
      "learning_rate": 3.56490437258194e-05,
      "loss": 0.6749,
      "step": 1887300
    },
    {
      "epoch": 17.22206000437988,
      "grad_norm": 4.373165130615234,
      "learning_rate": 3.564828332968343e-05,
      "loss": 0.6694,
      "step": 1887400
    },
    {
      "epoch": 17.222972479743046,
      "grad_norm": 4.4295735359191895,
      "learning_rate": 3.564752293354746e-05,
      "loss": 0.647,
      "step": 1887500
    },
    {
      "epoch": 17.22388495510621,
      "grad_norm": 3.9405875205993652,
      "learning_rate": 3.5646762537411493e-05,
      "loss": 0.663,
      "step": 1887600
    },
    {
      "epoch": 17.224797430469376,
      "grad_norm": 4.102017402648926,
      "learning_rate": 3.564600214127552e-05,
      "loss": 0.6638,
      "step": 1887700
    },
    {
      "epoch": 17.22570990583254,
      "grad_norm": 4.492506980895996,
      "learning_rate": 3.5645241745139554e-05,
      "loss": 0.6669,
      "step": 1887800
    },
    {
      "epoch": 17.226622381195707,
      "grad_norm": 3.5680038928985596,
      "learning_rate": 3.564448134900358e-05,
      "loss": 0.6871,
      "step": 1887900
    },
    {
      "epoch": 17.227534856558872,
      "grad_norm": 3.9998323917388916,
      "learning_rate": 3.564372095286761e-05,
      "loss": 0.6445,
      "step": 1888000
    },
    {
      "epoch": 17.228447331922037,
      "grad_norm": 4.63897180557251,
      "learning_rate": 3.564296055673164e-05,
      "loss": 0.6646,
      "step": 1888100
    },
    {
      "epoch": 17.229359807285203,
      "grad_norm": 4.074224472045898,
      "learning_rate": 3.564220016059567e-05,
      "loss": 0.653,
      "step": 1888200
    },
    {
      "epoch": 17.230272282648368,
      "grad_norm": 3.698084592819214,
      "learning_rate": 3.56414397644597e-05,
      "loss": 0.6542,
      "step": 1888300
    },
    {
      "epoch": 17.231184758011533,
      "grad_norm": 4.448064804077148,
      "learning_rate": 3.564067936832373e-05,
      "loss": 0.6912,
      "step": 1888400
    },
    {
      "epoch": 17.2320972333747,
      "grad_norm": 4.230901718139648,
      "learning_rate": 3.563991897218775e-05,
      "loss": 0.6362,
      "step": 1888500
    },
    {
      "epoch": 17.233009708737864,
      "grad_norm": 4.234065532684326,
      "learning_rate": 3.563915857605179e-05,
      "loss": 0.6697,
      "step": 1888600
    },
    {
      "epoch": 17.23392218410103,
      "grad_norm": 3.6304075717926025,
      "learning_rate": 3.563839817991581e-05,
      "loss": 0.6574,
      "step": 1888700
    },
    {
      "epoch": 17.234834659464195,
      "grad_norm": 3.6369566917419434,
      "learning_rate": 3.5637637783779834e-05,
      "loss": 0.6633,
      "step": 1888800
    },
    {
      "epoch": 17.23574713482736,
      "grad_norm": 5.27037239074707,
      "learning_rate": 3.563687738764387e-05,
      "loss": 0.6445,
      "step": 1888900
    },
    {
      "epoch": 17.236659610190525,
      "grad_norm": 3.450514316558838,
      "learning_rate": 3.5636116991507894e-05,
      "loss": 0.6772,
      "step": 1889000
    },
    {
      "epoch": 17.23757208555369,
      "grad_norm": 3.8654890060424805,
      "learning_rate": 3.5635356595371924e-05,
      "loss": 0.6535,
      "step": 1889100
    },
    {
      "epoch": 17.238484560916856,
      "grad_norm": 3.97526478767395,
      "learning_rate": 3.5634596199235954e-05,
      "loss": 0.6631,
      "step": 1889200
    },
    {
      "epoch": 17.23939703628002,
      "grad_norm": 3.901245355606079,
      "learning_rate": 3.5633835803099984e-05,
      "loss": 0.6593,
      "step": 1889300
    },
    {
      "epoch": 17.240309511643186,
      "grad_norm": 3.6316301822662354,
      "learning_rate": 3.5633075406964014e-05,
      "loss": 0.6823,
      "step": 1889400
    },
    {
      "epoch": 17.24122198700635,
      "grad_norm": 3.5158629417419434,
      "learning_rate": 3.5632315010828044e-05,
      "loss": 0.6778,
      "step": 1889500
    },
    {
      "epoch": 17.242134462369517,
      "grad_norm": 3.7309460639953613,
      "learning_rate": 3.563155461469207e-05,
      "loss": 0.6397,
      "step": 1889600
    },
    {
      "epoch": 17.243046937732682,
      "grad_norm": 3.600301504135132,
      "learning_rate": 3.5630794218556105e-05,
      "loss": 0.661,
      "step": 1889700
    },
    {
      "epoch": 17.243959413095848,
      "grad_norm": 4.498349666595459,
      "learning_rate": 3.563003382242013e-05,
      "loss": 0.6556,
      "step": 1889800
    },
    {
      "epoch": 17.244871888459013,
      "grad_norm": 4.808285236358643,
      "learning_rate": 3.562927342628416e-05,
      "loss": 0.633,
      "step": 1889900
    },
    {
      "epoch": 17.245784363822178,
      "grad_norm": 3.8888936042785645,
      "learning_rate": 3.562851303014819e-05,
      "loss": 0.6565,
      "step": 1890000
    },
    {
      "epoch": 17.246696839185343,
      "grad_norm": 4.172696113586426,
      "learning_rate": 3.562775263401222e-05,
      "loss": 0.6836,
      "step": 1890100
    },
    {
      "epoch": 17.24760931454851,
      "grad_norm": 4.263373374938965,
      "learning_rate": 3.562699223787624e-05,
      "loss": 0.6518,
      "step": 1890200
    },
    {
      "epoch": 17.248521789911674,
      "grad_norm": 4.463045120239258,
      "learning_rate": 3.562623184174028e-05,
      "loss": 0.6772,
      "step": 1890300
    },
    {
      "epoch": 17.249434265274836,
      "grad_norm": 4.148528099060059,
      "learning_rate": 3.56254714456043e-05,
      "loss": 0.6977,
      "step": 1890400
    },
    {
      "epoch": 17.250346740638,
      "grad_norm": 4.03472900390625,
      "learning_rate": 3.562471104946833e-05,
      "loss": 0.6831,
      "step": 1890500
    },
    {
      "epoch": 17.251259216001166,
      "grad_norm": 4.686975002288818,
      "learning_rate": 3.562395065333236e-05,
      "loss": 0.664,
      "step": 1890600
    },
    {
      "epoch": 17.25217169136433,
      "grad_norm": 3.7834527492523193,
      "learning_rate": 3.562319025719639e-05,
      "loss": 0.6539,
      "step": 1890700
    },
    {
      "epoch": 17.253084166727497,
      "grad_norm": 3.774224042892456,
      "learning_rate": 3.562242986106042e-05,
      "loss": 0.6566,
      "step": 1890800
    },
    {
      "epoch": 17.253996642090662,
      "grad_norm": 4.263487339019775,
      "learning_rate": 3.562166946492445e-05,
      "loss": 0.6353,
      "step": 1890900
    },
    {
      "epoch": 17.254909117453828,
      "grad_norm": 4.757113456726074,
      "learning_rate": 3.5620909068788475e-05,
      "loss": 0.6486,
      "step": 1891000
    },
    {
      "epoch": 17.255821592816993,
      "grad_norm": 3.971544027328491,
      "learning_rate": 3.562014867265251e-05,
      "loss": 0.6399,
      "step": 1891100
    },
    {
      "epoch": 17.256734068180158,
      "grad_norm": 3.427982807159424,
      "learning_rate": 3.5619388276516535e-05,
      "loss": 0.6791,
      "step": 1891200
    },
    {
      "epoch": 17.257646543543324,
      "grad_norm": 3.57824969291687,
      "learning_rate": 3.5618627880380565e-05,
      "loss": 0.6305,
      "step": 1891300
    },
    {
      "epoch": 17.25855901890649,
      "grad_norm": 3.9889068603515625,
      "learning_rate": 3.5617867484244595e-05,
      "loss": 0.6306,
      "step": 1891400
    },
    {
      "epoch": 17.259471494269654,
      "grad_norm": 4.338370323181152,
      "learning_rate": 3.5617107088108625e-05,
      "loss": 0.6489,
      "step": 1891500
    },
    {
      "epoch": 17.26038396963282,
      "grad_norm": 4.067773818969727,
      "learning_rate": 3.561634669197265e-05,
      "loss": 0.643,
      "step": 1891600
    },
    {
      "epoch": 17.261296444995985,
      "grad_norm": 1.905684471130371,
      "learning_rate": 3.561558629583668e-05,
      "loss": 0.6554,
      "step": 1891700
    },
    {
      "epoch": 17.26220892035915,
      "grad_norm": 3.7904765605926514,
      "learning_rate": 3.561482589970071e-05,
      "loss": 0.6585,
      "step": 1891800
    },
    {
      "epoch": 17.263121395722315,
      "grad_norm": 4.18109655380249,
      "learning_rate": 3.561406550356474e-05,
      "loss": 0.6591,
      "step": 1891900
    },
    {
      "epoch": 17.26403387108548,
      "grad_norm": 3.9602630138397217,
      "learning_rate": 3.561330510742877e-05,
      "loss": 0.6431,
      "step": 1892000
    },
    {
      "epoch": 17.264946346448646,
      "grad_norm": 4.582734107971191,
      "learning_rate": 3.561254471129279e-05,
      "loss": 0.5821,
      "step": 1892100
    },
    {
      "epoch": 17.26585882181181,
      "grad_norm": 3.8741040229797363,
      "learning_rate": 3.561178431515683e-05,
      "loss": 0.6777,
      "step": 1892200
    },
    {
      "epoch": 17.266771297174976,
      "grad_norm": 3.2238106727600098,
      "learning_rate": 3.561102391902085e-05,
      "loss": 0.652,
      "step": 1892300
    },
    {
      "epoch": 17.267683772538142,
      "grad_norm": 3.8671109676361084,
      "learning_rate": 3.561026352288488e-05,
      "loss": 0.6869,
      "step": 1892400
    },
    {
      "epoch": 17.268596247901307,
      "grad_norm": 4.501327991485596,
      "learning_rate": 3.560950312674891e-05,
      "loss": 0.6938,
      "step": 1892500
    },
    {
      "epoch": 17.269508723264472,
      "grad_norm": 3.8967490196228027,
      "learning_rate": 3.560874273061294e-05,
      "loss": 0.6838,
      "step": 1892600
    },
    {
      "epoch": 17.270421198627638,
      "grad_norm": 4.338650226593018,
      "learning_rate": 3.5607982334476966e-05,
      "loss": 0.7007,
      "step": 1892700
    },
    {
      "epoch": 17.271333673990803,
      "grad_norm": 1.7075164318084717,
      "learning_rate": 3.5607221938341e-05,
      "loss": 0.6558,
      "step": 1892800
    },
    {
      "epoch": 17.27224614935397,
      "grad_norm": 4.1990742683410645,
      "learning_rate": 3.5606461542205026e-05,
      "loss": 0.6458,
      "step": 1892900
    },
    {
      "epoch": 17.273158624717134,
      "grad_norm": 3.7879092693328857,
      "learning_rate": 3.5605701146069056e-05,
      "loss": 0.7091,
      "step": 1893000
    },
    {
      "epoch": 17.2740711000803,
      "grad_norm": 2.995936870574951,
      "learning_rate": 3.5604940749933086e-05,
      "loss": 0.6711,
      "step": 1893100
    },
    {
      "epoch": 17.274983575443464,
      "grad_norm": 3.5365097522735596,
      "learning_rate": 3.5604180353797116e-05,
      "loss": 0.6454,
      "step": 1893200
    },
    {
      "epoch": 17.27589605080663,
      "grad_norm": 4.943260192871094,
      "learning_rate": 3.5603419957661146e-05,
      "loss": 0.6428,
      "step": 1893300
    },
    {
      "epoch": 17.276808526169795,
      "grad_norm": 4.079154968261719,
      "learning_rate": 3.5602659561525176e-05,
      "loss": 0.6718,
      "step": 1893400
    },
    {
      "epoch": 17.27772100153296,
      "grad_norm": 4.414464950561523,
      "learning_rate": 3.56018991653892e-05,
      "loss": 0.6928,
      "step": 1893500
    },
    {
      "epoch": 17.278633476896125,
      "grad_norm": 4.236239910125732,
      "learning_rate": 3.5601138769253236e-05,
      "loss": 0.619,
      "step": 1893600
    },
    {
      "epoch": 17.27954595225929,
      "grad_norm": 4.086145877838135,
      "learning_rate": 3.560037837311726e-05,
      "loss": 0.6326,
      "step": 1893700
    },
    {
      "epoch": 17.280458427622456,
      "grad_norm": 4.220414638519287,
      "learning_rate": 3.559961797698129e-05,
      "loss": 0.7044,
      "step": 1893800
    },
    {
      "epoch": 17.281370902985618,
      "grad_norm": 3.8132715225219727,
      "learning_rate": 3.559885758084532e-05,
      "loss": 0.6353,
      "step": 1893900
    },
    {
      "epoch": 17.282283378348783,
      "grad_norm": 3.995260000228882,
      "learning_rate": 3.559809718470935e-05,
      "loss": 0.6861,
      "step": 1894000
    },
    {
      "epoch": 17.28319585371195,
      "grad_norm": 4.142604827880859,
      "learning_rate": 3.559733678857337e-05,
      "loss": 0.6672,
      "step": 1894100
    },
    {
      "epoch": 17.284108329075114,
      "grad_norm": 4.446295261383057,
      "learning_rate": 3.559657639243741e-05,
      "loss": 0.6605,
      "step": 1894200
    },
    {
      "epoch": 17.28502080443828,
      "grad_norm": 4.147053241729736,
      "learning_rate": 3.559581599630143e-05,
      "loss": 0.6532,
      "step": 1894300
    },
    {
      "epoch": 17.285933279801444,
      "grad_norm": 3.6386001110076904,
      "learning_rate": 3.5595055600165463e-05,
      "loss": 0.6633,
      "step": 1894400
    },
    {
      "epoch": 17.28684575516461,
      "grad_norm": 3.825972557067871,
      "learning_rate": 3.5594295204029494e-05,
      "loss": 0.6864,
      "step": 1894500
    },
    {
      "epoch": 17.287758230527775,
      "grad_norm": 2.4741172790527344,
      "learning_rate": 3.559353480789352e-05,
      "loss": 0.6597,
      "step": 1894600
    },
    {
      "epoch": 17.28867070589094,
      "grad_norm": 3.4024181365966797,
      "learning_rate": 3.5592774411757554e-05,
      "loss": 0.6765,
      "step": 1894700
    },
    {
      "epoch": 17.289583181254105,
      "grad_norm": 3.6118292808532715,
      "learning_rate": 3.559201401562158e-05,
      "loss": 0.6911,
      "step": 1894800
    },
    {
      "epoch": 17.29049565661727,
      "grad_norm": 4.105720520019531,
      "learning_rate": 3.559125361948561e-05,
      "loss": 0.687,
      "step": 1894900
    },
    {
      "epoch": 17.291408131980436,
      "grad_norm": 3.260118007659912,
      "learning_rate": 3.559049322334964e-05,
      "loss": 0.6969,
      "step": 1895000
    },
    {
      "epoch": 17.2923206073436,
      "grad_norm": 3.801140308380127,
      "learning_rate": 3.558973282721367e-05,
      "loss": 0.6786,
      "step": 1895100
    },
    {
      "epoch": 17.293233082706767,
      "grad_norm": 4.407562255859375,
      "learning_rate": 3.55889724310777e-05,
      "loss": 0.6877,
      "step": 1895200
    },
    {
      "epoch": 17.294145558069932,
      "grad_norm": 3.7036449909210205,
      "learning_rate": 3.558821203494173e-05,
      "loss": 0.6737,
      "step": 1895300
    },
    {
      "epoch": 17.295058033433097,
      "grad_norm": 4.610690593719482,
      "learning_rate": 3.558745163880575e-05,
      "loss": 0.669,
      "step": 1895400
    },
    {
      "epoch": 17.295970508796263,
      "grad_norm": 4.888512134552002,
      "learning_rate": 3.558669124266979e-05,
      "loss": 0.6389,
      "step": 1895500
    },
    {
      "epoch": 17.296882984159428,
      "grad_norm": 3.2105112075805664,
      "learning_rate": 3.558593084653381e-05,
      "loss": 0.6704,
      "step": 1895600
    },
    {
      "epoch": 17.297795459522593,
      "grad_norm": 4.734335422515869,
      "learning_rate": 3.558517045039784e-05,
      "loss": 0.6206,
      "step": 1895700
    },
    {
      "epoch": 17.29870793488576,
      "grad_norm": 3.7901723384857178,
      "learning_rate": 3.558441005426187e-05,
      "loss": 0.6696,
      "step": 1895800
    },
    {
      "epoch": 17.299620410248924,
      "grad_norm": 4.104913711547852,
      "learning_rate": 3.55836496581259e-05,
      "loss": 0.6673,
      "step": 1895900
    },
    {
      "epoch": 17.30053288561209,
      "grad_norm": 4.154345989227295,
      "learning_rate": 3.5582889261989924e-05,
      "loss": 0.672,
      "step": 1896000
    },
    {
      "epoch": 17.301445360975254,
      "grad_norm": 4.697198390960693,
      "learning_rate": 3.558212886585396e-05,
      "loss": 0.6845,
      "step": 1896100
    },
    {
      "epoch": 17.30235783633842,
      "grad_norm": 3.8004016876220703,
      "learning_rate": 3.5581368469717984e-05,
      "loss": 0.6582,
      "step": 1896200
    },
    {
      "epoch": 17.303270311701585,
      "grad_norm": 3.820329427719116,
      "learning_rate": 3.5580608073582014e-05,
      "loss": 0.6568,
      "step": 1896300
    },
    {
      "epoch": 17.30418278706475,
      "grad_norm": 3.19266676902771,
      "learning_rate": 3.5579847677446044e-05,
      "loss": 0.6792,
      "step": 1896400
    },
    {
      "epoch": 17.305095262427916,
      "grad_norm": 4.590514183044434,
      "learning_rate": 3.5579087281310075e-05,
      "loss": 0.6667,
      "step": 1896500
    },
    {
      "epoch": 17.30600773779108,
      "grad_norm": 3.465766429901123,
      "learning_rate": 3.5578326885174105e-05,
      "loss": 0.6687,
      "step": 1896600
    },
    {
      "epoch": 17.306920213154246,
      "grad_norm": 3.462238311767578,
      "learning_rate": 3.5577566489038135e-05,
      "loss": 0.7024,
      "step": 1896700
    },
    {
      "epoch": 17.30783268851741,
      "grad_norm": 3.9075942039489746,
      "learning_rate": 3.557680609290216e-05,
      "loss": 0.6594,
      "step": 1896800
    },
    {
      "epoch": 17.308745163880577,
      "grad_norm": 4.068620681762695,
      "learning_rate": 3.5576045696766195e-05,
      "loss": 0.6338,
      "step": 1896900
    },
    {
      "epoch": 17.309657639243742,
      "grad_norm": 4.564596652984619,
      "learning_rate": 3.557528530063022e-05,
      "loss": 0.6766,
      "step": 1897000
    },
    {
      "epoch": 17.310570114606907,
      "grad_norm": 3.7914113998413086,
      "learning_rate": 3.557452490449425e-05,
      "loss": 0.6511,
      "step": 1897100
    },
    {
      "epoch": 17.31148258997007,
      "grad_norm": 4.042691707611084,
      "learning_rate": 3.557376450835828e-05,
      "loss": 0.6835,
      "step": 1897200
    },
    {
      "epoch": 17.312395065333234,
      "grad_norm": 3.805189847946167,
      "learning_rate": 3.55730041122223e-05,
      "loss": 0.6637,
      "step": 1897300
    },
    {
      "epoch": 17.3133075406964,
      "grad_norm": 4.315132141113281,
      "learning_rate": 3.557224371608633e-05,
      "loss": 0.5939,
      "step": 1897400
    },
    {
      "epoch": 17.314220016059565,
      "grad_norm": 4.472751140594482,
      "learning_rate": 3.557148331995036e-05,
      "loss": 0.6954,
      "step": 1897500
    },
    {
      "epoch": 17.31513249142273,
      "grad_norm": 3.698647975921631,
      "learning_rate": 3.557072292381439e-05,
      "loss": 0.6546,
      "step": 1897600
    },
    {
      "epoch": 17.316044966785896,
      "grad_norm": 3.1756443977355957,
      "learning_rate": 3.556996252767842e-05,
      "loss": 0.6829,
      "step": 1897700
    },
    {
      "epoch": 17.31695744214906,
      "grad_norm": 3.9848363399505615,
      "learning_rate": 3.556920213154245e-05,
      "loss": 0.6649,
      "step": 1897800
    },
    {
      "epoch": 17.317869917512226,
      "grad_norm": 3.3778457641601562,
      "learning_rate": 3.5568441735406475e-05,
      "loss": 0.6814,
      "step": 1897900
    },
    {
      "epoch": 17.31878239287539,
      "grad_norm": 4.126793384552002,
      "learning_rate": 3.556768133927051e-05,
      "loss": 0.6393,
      "step": 1898000
    },
    {
      "epoch": 17.319694868238557,
      "grad_norm": 4.489184379577637,
      "learning_rate": 3.5566920943134535e-05,
      "loss": 0.6639,
      "step": 1898100
    },
    {
      "epoch": 17.320607343601722,
      "grad_norm": 3.889970541000366,
      "learning_rate": 3.5566160546998565e-05,
      "loss": 0.6728,
      "step": 1898200
    },
    {
      "epoch": 17.321519818964887,
      "grad_norm": 3.7188520431518555,
      "learning_rate": 3.5565400150862595e-05,
      "loss": 0.6671,
      "step": 1898300
    },
    {
      "epoch": 17.322432294328053,
      "grad_norm": 4.248022079467773,
      "learning_rate": 3.5564639754726625e-05,
      "loss": 0.6781,
      "step": 1898400
    },
    {
      "epoch": 17.323344769691218,
      "grad_norm": 2.985119104385376,
      "learning_rate": 3.556387935859065e-05,
      "loss": 0.6457,
      "step": 1898500
    },
    {
      "epoch": 17.324257245054383,
      "grad_norm": 2.7494475841522217,
      "learning_rate": 3.5563118962454686e-05,
      "loss": 0.6377,
      "step": 1898600
    },
    {
      "epoch": 17.32516972041755,
      "grad_norm": 3.934047222137451,
      "learning_rate": 3.556235856631871e-05,
      "loss": 0.6419,
      "step": 1898700
    },
    {
      "epoch": 17.326082195780714,
      "grad_norm": 4.2946553230285645,
      "learning_rate": 3.556159817018274e-05,
      "loss": 0.6405,
      "step": 1898800
    },
    {
      "epoch": 17.32699467114388,
      "grad_norm": 4.215884685516357,
      "learning_rate": 3.556083777404677e-05,
      "loss": 0.6207,
      "step": 1898900
    },
    {
      "epoch": 17.327907146507044,
      "grad_norm": 3.4111275672912598,
      "learning_rate": 3.55600773779108e-05,
      "loss": 0.6567,
      "step": 1899000
    },
    {
      "epoch": 17.32881962187021,
      "grad_norm": 2.7239251136779785,
      "learning_rate": 3.555931698177483e-05,
      "loss": 0.6525,
      "step": 1899100
    },
    {
      "epoch": 17.329732097233375,
      "grad_norm": 3.0624170303344727,
      "learning_rate": 3.555855658563886e-05,
      "loss": 0.6763,
      "step": 1899200
    },
    {
      "epoch": 17.33064457259654,
      "grad_norm": 3.879197597503662,
      "learning_rate": 3.555779618950288e-05,
      "loss": 0.6061,
      "step": 1899300
    },
    {
      "epoch": 17.331557047959706,
      "grad_norm": 3.7498505115509033,
      "learning_rate": 3.555703579336692e-05,
      "loss": 0.6617,
      "step": 1899400
    },
    {
      "epoch": 17.33246952332287,
      "grad_norm": 4.226162433624268,
      "learning_rate": 3.555627539723094e-05,
      "loss": 0.6838,
      "step": 1899500
    },
    {
      "epoch": 17.333381998686036,
      "grad_norm": 3.1614279747009277,
      "learning_rate": 3.555551500109497e-05,
      "loss": 0.6681,
      "step": 1899600
    },
    {
      "epoch": 17.3342944740492,
      "grad_norm": 3.6830315589904785,
      "learning_rate": 3.5554754604959e-05,
      "loss": 0.6898,
      "step": 1899700
    },
    {
      "epoch": 17.335206949412367,
      "grad_norm": 3.976680278778076,
      "learning_rate": 3.555399420882303e-05,
      "loss": 0.6736,
      "step": 1899800
    },
    {
      "epoch": 17.336119424775532,
      "grad_norm": 3.8284494876861572,
      "learning_rate": 3.5553233812687056e-05,
      "loss": 0.6791,
      "step": 1899900
    },
    {
      "epoch": 17.337031900138697,
      "grad_norm": 4.679433822631836,
      "learning_rate": 3.555247341655109e-05,
      "loss": 0.6469,
      "step": 1900000
    },
    {
      "epoch": 17.337944375501863,
      "grad_norm": 3.909871816635132,
      "learning_rate": 3.5551713020415116e-05,
      "loss": 0.6753,
      "step": 1900100
    },
    {
      "epoch": 17.338856850865028,
      "grad_norm": 3.5556979179382324,
      "learning_rate": 3.5550952624279146e-05,
      "loss": 0.6258,
      "step": 1900200
    },
    {
      "epoch": 17.339769326228193,
      "grad_norm": 3.8015244007110596,
      "learning_rate": 3.5550192228143176e-05,
      "loss": 0.6629,
      "step": 1900300
    },
    {
      "epoch": 17.34068180159136,
      "grad_norm": 3.31925106048584,
      "learning_rate": 3.55494318320072e-05,
      "loss": 0.6702,
      "step": 1900400
    },
    {
      "epoch": 17.341594276954524,
      "grad_norm": 4.396480560302734,
      "learning_rate": 3.5548671435871237e-05,
      "loss": 0.6746,
      "step": 1900500
    },
    {
      "epoch": 17.342506752317686,
      "grad_norm": 4.1018500328063965,
      "learning_rate": 3.554791103973526e-05,
      "loss": 0.6361,
      "step": 1900600
    },
    {
      "epoch": 17.34341922768085,
      "grad_norm": 3.368100643157959,
      "learning_rate": 3.554715064359929e-05,
      "loss": 0.6914,
      "step": 1900700
    },
    {
      "epoch": 17.344331703044016,
      "grad_norm": 4.96630334854126,
      "learning_rate": 3.554639024746332e-05,
      "loss": 0.7001,
      "step": 1900800
    },
    {
      "epoch": 17.34524417840718,
      "grad_norm": 4.020020008087158,
      "learning_rate": 3.554562985132735e-05,
      "loss": 0.6538,
      "step": 1900900
    },
    {
      "epoch": 17.346156653770347,
      "grad_norm": 3.801718235015869,
      "learning_rate": 3.554486945519137e-05,
      "loss": 0.6572,
      "step": 1901000
    },
    {
      "epoch": 17.347069129133512,
      "grad_norm": 4.273947238922119,
      "learning_rate": 3.554410905905541e-05,
      "loss": 0.6639,
      "step": 1901100
    },
    {
      "epoch": 17.347981604496677,
      "grad_norm": 3.85021710395813,
      "learning_rate": 3.5543348662919433e-05,
      "loss": 0.6447,
      "step": 1901200
    },
    {
      "epoch": 17.348894079859843,
      "grad_norm": 3.8873836994171143,
      "learning_rate": 3.5542588266783464e-05,
      "loss": 0.6517,
      "step": 1901300
    },
    {
      "epoch": 17.349806555223008,
      "grad_norm": 4.3998188972473145,
      "learning_rate": 3.5541827870647494e-05,
      "loss": 0.7013,
      "step": 1901400
    },
    {
      "epoch": 17.350719030586173,
      "grad_norm": 4.845686912536621,
      "learning_rate": 3.5541067474511524e-05,
      "loss": 0.677,
      "step": 1901500
    },
    {
      "epoch": 17.35163150594934,
      "grad_norm": 3.615786552429199,
      "learning_rate": 3.5540307078375554e-05,
      "loss": 0.6586,
      "step": 1901600
    },
    {
      "epoch": 17.352543981312504,
      "grad_norm": 4.698388576507568,
      "learning_rate": 3.5539546682239584e-05,
      "loss": 0.6739,
      "step": 1901700
    },
    {
      "epoch": 17.35345645667567,
      "grad_norm": 3.3331568241119385,
      "learning_rate": 3.553878628610361e-05,
      "loss": 0.6128,
      "step": 1901800
    },
    {
      "epoch": 17.354368932038835,
      "grad_norm": 3.6421234607696533,
      "learning_rate": 3.5538025889967644e-05,
      "loss": 0.6318,
      "step": 1901900
    },
    {
      "epoch": 17.355281407402,
      "grad_norm": 4.695465087890625,
      "learning_rate": 3.553726549383167e-05,
      "loss": 0.6535,
      "step": 1902000
    },
    {
      "epoch": 17.356193882765165,
      "grad_norm": 3.062368154525757,
      "learning_rate": 3.55365050976957e-05,
      "loss": 0.689,
      "step": 1902100
    },
    {
      "epoch": 17.35710635812833,
      "grad_norm": 3.9730823040008545,
      "learning_rate": 3.553574470155973e-05,
      "loss": 0.6253,
      "step": 1902200
    },
    {
      "epoch": 17.358018833491496,
      "grad_norm": 4.80063009262085,
      "learning_rate": 3.553498430542376e-05,
      "loss": 0.7001,
      "step": 1902300
    },
    {
      "epoch": 17.35893130885466,
      "grad_norm": 4.243978500366211,
      "learning_rate": 3.553422390928778e-05,
      "loss": 0.6721,
      "step": 1902400
    },
    {
      "epoch": 17.359843784217826,
      "grad_norm": 4.259760856628418,
      "learning_rate": 3.553346351315182e-05,
      "loss": 0.6332,
      "step": 1902500
    },
    {
      "epoch": 17.36075625958099,
      "grad_norm": 3.7678847312927246,
      "learning_rate": 3.553270311701584e-05,
      "loss": 0.6503,
      "step": 1902600
    },
    {
      "epoch": 17.361668734944157,
      "grad_norm": 3.7266910076141357,
      "learning_rate": 3.553194272087987e-05,
      "loss": 0.6792,
      "step": 1902700
    },
    {
      "epoch": 17.362581210307322,
      "grad_norm": 3.4819583892822266,
      "learning_rate": 3.55311823247439e-05,
      "loss": 0.6333,
      "step": 1902800
    },
    {
      "epoch": 17.363493685670488,
      "grad_norm": 3.8780858516693115,
      "learning_rate": 3.553042192860793e-05,
      "loss": 0.672,
      "step": 1902900
    },
    {
      "epoch": 17.364406161033653,
      "grad_norm": 3.395106315612793,
      "learning_rate": 3.552966153247196e-05,
      "loss": 0.6542,
      "step": 1903000
    },
    {
      "epoch": 17.365318636396818,
      "grad_norm": 4.631616115570068,
      "learning_rate": 3.5528901136335984e-05,
      "loss": 0.6766,
      "step": 1903100
    },
    {
      "epoch": 17.366231111759983,
      "grad_norm": 2.7531545162200928,
      "learning_rate": 3.5528140740200015e-05,
      "loss": 0.6655,
      "step": 1903200
    },
    {
      "epoch": 17.36714358712315,
      "grad_norm": 4.062729835510254,
      "learning_rate": 3.5527380344064045e-05,
      "loss": 0.6705,
      "step": 1903300
    },
    {
      "epoch": 17.368056062486314,
      "grad_norm": 5.018813610076904,
      "learning_rate": 3.5526619947928075e-05,
      "loss": 0.6726,
      "step": 1903400
    },
    {
      "epoch": 17.36896853784948,
      "grad_norm": 4.030475616455078,
      "learning_rate": 3.55258595517921e-05,
      "loss": 0.6236,
      "step": 1903500
    },
    {
      "epoch": 17.369881013212645,
      "grad_norm": 4.587553024291992,
      "learning_rate": 3.5525099155656135e-05,
      "loss": 0.6397,
      "step": 1903600
    },
    {
      "epoch": 17.37079348857581,
      "grad_norm": 4.280893325805664,
      "learning_rate": 3.552433875952016e-05,
      "loss": 0.636,
      "step": 1903700
    },
    {
      "epoch": 17.371705963938975,
      "grad_norm": 4.2229509353637695,
      "learning_rate": 3.552357836338419e-05,
      "loss": 0.6561,
      "step": 1903800
    },
    {
      "epoch": 17.37261843930214,
      "grad_norm": 4.22936487197876,
      "learning_rate": 3.552281796724822e-05,
      "loss": 0.6508,
      "step": 1903900
    },
    {
      "epoch": 17.373530914665302,
      "grad_norm": 4.480536460876465,
      "learning_rate": 3.552205757111225e-05,
      "loss": 0.6852,
      "step": 1904000
    },
    {
      "epoch": 17.374443390028468,
      "grad_norm": 3.912963628768921,
      "learning_rate": 3.552129717497628e-05,
      "loss": 0.6654,
      "step": 1904100
    },
    {
      "epoch": 17.375355865391633,
      "grad_norm": 4.189798355102539,
      "learning_rate": 3.552053677884031e-05,
      "loss": 0.6804,
      "step": 1904200
    },
    {
      "epoch": 17.376268340754798,
      "grad_norm": 4.134073257446289,
      "learning_rate": 3.551977638270433e-05,
      "loss": 0.6342,
      "step": 1904300
    },
    {
      "epoch": 17.377180816117964,
      "grad_norm": 3.9912590980529785,
      "learning_rate": 3.551901598656837e-05,
      "loss": 0.6566,
      "step": 1904400
    },
    {
      "epoch": 17.37809329148113,
      "grad_norm": 4.265529155731201,
      "learning_rate": 3.551825559043239e-05,
      "loss": 0.6243,
      "step": 1904500
    },
    {
      "epoch": 17.379005766844294,
      "grad_norm": 4.256220817565918,
      "learning_rate": 3.551749519429642e-05,
      "loss": 0.6743,
      "step": 1904600
    },
    {
      "epoch": 17.37991824220746,
      "grad_norm": 3.092935085296631,
      "learning_rate": 3.551673479816045e-05,
      "loss": 0.6195,
      "step": 1904700
    },
    {
      "epoch": 17.380830717570625,
      "grad_norm": 4.547989368438721,
      "learning_rate": 3.551597440202448e-05,
      "loss": 0.6568,
      "step": 1904800
    },
    {
      "epoch": 17.38174319293379,
      "grad_norm": 4.274098873138428,
      "learning_rate": 3.5515214005888505e-05,
      "loss": 0.6835,
      "step": 1904900
    },
    {
      "epoch": 17.382655668296955,
      "grad_norm": 4.384145736694336,
      "learning_rate": 3.551445360975254e-05,
      "loss": 0.6644,
      "step": 1905000
    },
    {
      "epoch": 17.38356814366012,
      "grad_norm": 3.3022375106811523,
      "learning_rate": 3.5513693213616565e-05,
      "loss": 0.6356,
      "step": 1905100
    },
    {
      "epoch": 17.384480619023286,
      "grad_norm": 2.8643910884857178,
      "learning_rate": 3.5512932817480596e-05,
      "loss": 0.6306,
      "step": 1905200
    },
    {
      "epoch": 17.38539309438645,
      "grad_norm": 4.388614654541016,
      "learning_rate": 3.5512172421344626e-05,
      "loss": 0.6507,
      "step": 1905300
    },
    {
      "epoch": 17.386305569749616,
      "grad_norm": 4.943795204162598,
      "learning_rate": 3.5511412025208656e-05,
      "loss": 0.6768,
      "step": 1905400
    },
    {
      "epoch": 17.387218045112782,
      "grad_norm": 3.8369107246398926,
      "learning_rate": 3.5510651629072686e-05,
      "loss": 0.6643,
      "step": 1905500
    },
    {
      "epoch": 17.388130520475947,
      "grad_norm": 3.9635720252990723,
      "learning_rate": 3.5509891232936716e-05,
      "loss": 0.6318,
      "step": 1905600
    },
    {
      "epoch": 17.389042995839112,
      "grad_norm": 3.3722782135009766,
      "learning_rate": 3.550913083680074e-05,
      "loss": 0.6517,
      "step": 1905700
    },
    {
      "epoch": 17.389955471202278,
      "grad_norm": 3.8617515563964844,
      "learning_rate": 3.550837044066477e-05,
      "loss": 0.6431,
      "step": 1905800
    },
    {
      "epoch": 17.390867946565443,
      "grad_norm": 3.0312411785125732,
      "learning_rate": 3.55076100445288e-05,
      "loss": 0.6349,
      "step": 1905900
    },
    {
      "epoch": 17.39178042192861,
      "grad_norm": 2.9856858253479004,
      "learning_rate": 3.550684964839283e-05,
      "loss": 0.6623,
      "step": 1906000
    },
    {
      "epoch": 17.392692897291774,
      "grad_norm": 3.453847885131836,
      "learning_rate": 3.550608925225686e-05,
      "loss": 0.6824,
      "step": 1906100
    },
    {
      "epoch": 17.39360537265494,
      "grad_norm": 3.9380457401275635,
      "learning_rate": 3.550532885612088e-05,
      "loss": 0.6669,
      "step": 1906200
    },
    {
      "epoch": 17.394517848018104,
      "grad_norm": 3.9407100677490234,
      "learning_rate": 3.550456845998491e-05,
      "loss": 0.6993,
      "step": 1906300
    },
    {
      "epoch": 17.39543032338127,
      "grad_norm": 3.503669261932373,
      "learning_rate": 3.550380806384894e-05,
      "loss": 0.67,
      "step": 1906400
    },
    {
      "epoch": 17.396342798744435,
      "grad_norm": 4.494558334350586,
      "learning_rate": 3.550304766771297e-05,
      "loss": 0.6621,
      "step": 1906500
    },
    {
      "epoch": 17.3972552741076,
      "grad_norm": 3.8344268798828125,
      "learning_rate": 3.5502287271577e-05,
      "loss": 0.6711,
      "step": 1906600
    },
    {
      "epoch": 17.398167749470765,
      "grad_norm": 4.090511798858643,
      "learning_rate": 3.550152687544103e-05,
      "loss": 0.6779,
      "step": 1906700
    },
    {
      "epoch": 17.39908022483393,
      "grad_norm": 3.8404736518859863,
      "learning_rate": 3.5500766479305056e-05,
      "loss": 0.6632,
      "step": 1906800
    },
    {
      "epoch": 17.399992700197096,
      "grad_norm": 3.8753716945648193,
      "learning_rate": 3.550000608316909e-05,
      "loss": 0.7152,
      "step": 1906900
    },
    {
      "epoch": 17.40090517556026,
      "grad_norm": 4.067323684692383,
      "learning_rate": 3.5499245687033116e-05,
      "loss": 0.6967,
      "step": 1907000
    },
    {
      "epoch": 17.401817650923427,
      "grad_norm": 3.5738039016723633,
      "learning_rate": 3.5498485290897146e-05,
      "loss": 0.6147,
      "step": 1907100
    },
    {
      "epoch": 17.402730126286592,
      "grad_norm": 3.8788530826568604,
      "learning_rate": 3.5497724894761177e-05,
      "loss": 0.649,
      "step": 1907200
    },
    {
      "epoch": 17.403642601649757,
      "grad_norm": 3.555826425552368,
      "learning_rate": 3.5496964498625207e-05,
      "loss": 0.6795,
      "step": 1907300
    },
    {
      "epoch": 17.40455507701292,
      "grad_norm": 4.189356327056885,
      "learning_rate": 3.549620410248924e-05,
      "loss": 0.675,
      "step": 1907400
    },
    {
      "epoch": 17.405467552376084,
      "grad_norm": 4.3296122550964355,
      "learning_rate": 3.549544370635327e-05,
      "loss": 0.6585,
      "step": 1907500
    },
    {
      "epoch": 17.40638002773925,
      "grad_norm": 4.399406909942627,
      "learning_rate": 3.549468331021729e-05,
      "loss": 0.6709,
      "step": 1907600
    },
    {
      "epoch": 17.407292503102415,
      "grad_norm": 4.966394901275635,
      "learning_rate": 3.549392291408132e-05,
      "loss": 0.6564,
      "step": 1907700
    },
    {
      "epoch": 17.40820497846558,
      "grad_norm": 3.8730392456054688,
      "learning_rate": 3.549316251794535e-05,
      "loss": 0.6668,
      "step": 1907800
    },
    {
      "epoch": 17.409117453828745,
      "grad_norm": 3.3005259037017822,
      "learning_rate": 3.549240212180938e-05,
      "loss": 0.7128,
      "step": 1907900
    },
    {
      "epoch": 17.41002992919191,
      "grad_norm": 3.6996023654937744,
      "learning_rate": 3.549164172567341e-05,
      "loss": 0.6583,
      "step": 1908000
    },
    {
      "epoch": 17.410942404555076,
      "grad_norm": 4.548239231109619,
      "learning_rate": 3.549088132953744e-05,
      "loss": 0.6448,
      "step": 1908100
    },
    {
      "epoch": 17.41185487991824,
      "grad_norm": 3.3323869705200195,
      "learning_rate": 3.5490120933401464e-05,
      "loss": 0.6722,
      "step": 1908200
    },
    {
      "epoch": 17.412767355281407,
      "grad_norm": 4.178524971008301,
      "learning_rate": 3.54893605372655e-05,
      "loss": 0.6667,
      "step": 1908300
    },
    {
      "epoch": 17.413679830644572,
      "grad_norm": 3.99009108543396,
      "learning_rate": 3.5488600141129524e-05,
      "loss": 0.6821,
      "step": 1908400
    },
    {
      "epoch": 17.414592306007737,
      "grad_norm": 3.4519999027252197,
      "learning_rate": 3.5487839744993554e-05,
      "loss": 0.684,
      "step": 1908500
    },
    {
      "epoch": 17.415504781370903,
      "grad_norm": 4.1627278327941895,
      "learning_rate": 3.5487079348857584e-05,
      "loss": 0.7007,
      "step": 1908600
    },
    {
      "epoch": 17.416417256734068,
      "grad_norm": 3.4760823249816895,
      "learning_rate": 3.548631895272161e-05,
      "loss": 0.646,
      "step": 1908700
    },
    {
      "epoch": 17.417329732097233,
      "grad_norm": 3.8971972465515137,
      "learning_rate": 3.5485558556585644e-05,
      "loss": 0.6618,
      "step": 1908800
    },
    {
      "epoch": 17.4182422074604,
      "grad_norm": 4.0397491455078125,
      "learning_rate": 3.548479816044967e-05,
      "loss": 0.674,
      "step": 1908900
    },
    {
      "epoch": 17.419154682823564,
      "grad_norm": 3.4455924034118652,
      "learning_rate": 3.54840377643137e-05,
      "loss": 0.6596,
      "step": 1909000
    },
    {
      "epoch": 17.42006715818673,
      "grad_norm": 5.7932868003845215,
      "learning_rate": 3.548327736817773e-05,
      "loss": 0.6767,
      "step": 1909100
    },
    {
      "epoch": 17.420979633549894,
      "grad_norm": 3.748762369155884,
      "learning_rate": 3.548251697204176e-05,
      "loss": 0.651,
      "step": 1909200
    },
    {
      "epoch": 17.42189210891306,
      "grad_norm": 2.9704723358154297,
      "learning_rate": 3.548175657590578e-05,
      "loss": 0.6324,
      "step": 1909300
    },
    {
      "epoch": 17.422804584276225,
      "grad_norm": 4.991077899932861,
      "learning_rate": 3.548099617976982e-05,
      "loss": 0.6556,
      "step": 1909400
    },
    {
      "epoch": 17.42371705963939,
      "grad_norm": 3.6831414699554443,
      "learning_rate": 3.548023578363384e-05,
      "loss": 0.6599,
      "step": 1909500
    },
    {
      "epoch": 17.424629535002556,
      "grad_norm": 3.7045888900756836,
      "learning_rate": 3.547947538749787e-05,
      "loss": 0.6787,
      "step": 1909600
    },
    {
      "epoch": 17.42554201036572,
      "grad_norm": 4.394229888916016,
      "learning_rate": 3.54787149913619e-05,
      "loss": 0.6679,
      "step": 1909700
    },
    {
      "epoch": 17.426454485728886,
      "grad_norm": 3.1642351150512695,
      "learning_rate": 3.547795459522593e-05,
      "loss": 0.6517,
      "step": 1909800
    },
    {
      "epoch": 17.42736696109205,
      "grad_norm": 3.3623173236846924,
      "learning_rate": 3.547719419908996e-05,
      "loss": 0.6617,
      "step": 1909900
    },
    {
      "epoch": 17.428279436455217,
      "grad_norm": 4.387604236602783,
      "learning_rate": 3.547643380295399e-05,
      "loss": 0.6514,
      "step": 1910000
    },
    {
      "epoch": 17.429191911818382,
      "grad_norm": 3.5129573345184326,
      "learning_rate": 3.5475673406818015e-05,
      "loss": 0.6478,
      "step": 1910100
    },
    {
      "epoch": 17.430104387181547,
      "grad_norm": 3.532486915588379,
      "learning_rate": 3.547491301068205e-05,
      "loss": 0.6837,
      "step": 1910200
    },
    {
      "epoch": 17.431016862544713,
      "grad_norm": 2.3973913192749023,
      "learning_rate": 3.5474152614546075e-05,
      "loss": 0.6563,
      "step": 1910300
    },
    {
      "epoch": 17.431929337907878,
      "grad_norm": 4.296777248382568,
      "learning_rate": 3.5473392218410105e-05,
      "loss": 0.6601,
      "step": 1910400
    },
    {
      "epoch": 17.432841813271043,
      "grad_norm": 3.943091869354248,
      "learning_rate": 3.5472631822274135e-05,
      "loss": 0.701,
      "step": 1910500
    },
    {
      "epoch": 17.43375428863421,
      "grad_norm": 3.4829795360565186,
      "learning_rate": 3.5471871426138165e-05,
      "loss": 0.6427,
      "step": 1910600
    },
    {
      "epoch": 17.434666763997374,
      "grad_norm": 4.693325996398926,
      "learning_rate": 3.547111103000219e-05,
      "loss": 0.6317,
      "step": 1910700
    },
    {
      "epoch": 17.435579239360536,
      "grad_norm": 2.1871557235717773,
      "learning_rate": 3.5470350633866225e-05,
      "loss": 0.6159,
      "step": 1910800
    },
    {
      "epoch": 17.4364917147237,
      "grad_norm": 3.660853862762451,
      "learning_rate": 3.546959023773025e-05,
      "loss": 0.6809,
      "step": 1910900
    },
    {
      "epoch": 17.437404190086866,
      "grad_norm": 3.7561161518096924,
      "learning_rate": 3.546882984159428e-05,
      "loss": 0.6441,
      "step": 1911000
    },
    {
      "epoch": 17.43831666545003,
      "grad_norm": 3.935783624649048,
      "learning_rate": 3.546806944545831e-05,
      "loss": 0.6878,
      "step": 1911100
    },
    {
      "epoch": 17.439229140813197,
      "grad_norm": 4.920603275299072,
      "learning_rate": 3.546730904932234e-05,
      "loss": 0.7004,
      "step": 1911200
    },
    {
      "epoch": 17.440141616176362,
      "grad_norm": 3.672382354736328,
      "learning_rate": 3.546654865318637e-05,
      "loss": 0.6263,
      "step": 1911300
    },
    {
      "epoch": 17.441054091539527,
      "grad_norm": 4.238093852996826,
      "learning_rate": 3.54657882570504e-05,
      "loss": 0.6499,
      "step": 1911400
    },
    {
      "epoch": 17.441966566902693,
      "grad_norm": 4.64290189743042,
      "learning_rate": 3.546502786091442e-05,
      "loss": 0.6782,
      "step": 1911500
    },
    {
      "epoch": 17.442879042265858,
      "grad_norm": 4.242572784423828,
      "learning_rate": 3.546426746477845e-05,
      "loss": 0.6827,
      "step": 1911600
    },
    {
      "epoch": 17.443791517629023,
      "grad_norm": 2.600081205368042,
      "learning_rate": 3.546350706864248e-05,
      "loss": 0.6765,
      "step": 1911700
    },
    {
      "epoch": 17.44470399299219,
      "grad_norm": 2.9384665489196777,
      "learning_rate": 3.5462746672506505e-05,
      "loss": 0.6793,
      "step": 1911800
    },
    {
      "epoch": 17.445616468355354,
      "grad_norm": 3.9555580615997314,
      "learning_rate": 3.546198627637054e-05,
      "loss": 0.6796,
      "step": 1911900
    },
    {
      "epoch": 17.44652894371852,
      "grad_norm": 4.27329683303833,
      "learning_rate": 3.5461225880234566e-05,
      "loss": 0.6667,
      "step": 1912000
    },
    {
      "epoch": 17.447441419081684,
      "grad_norm": 4.175464153289795,
      "learning_rate": 3.5460465484098596e-05,
      "loss": 0.657,
      "step": 1912100
    },
    {
      "epoch": 17.44835389444485,
      "grad_norm": 3.5715737342834473,
      "learning_rate": 3.5459705087962626e-05,
      "loss": 0.7132,
      "step": 1912200
    },
    {
      "epoch": 17.449266369808015,
      "grad_norm": 3.479375123977661,
      "learning_rate": 3.5458944691826656e-05,
      "loss": 0.6679,
      "step": 1912300
    },
    {
      "epoch": 17.45017884517118,
      "grad_norm": 4.79248046875,
      "learning_rate": 3.5458184295690686e-05,
      "loss": 0.6373,
      "step": 1912400
    },
    {
      "epoch": 17.451091320534346,
      "grad_norm": 4.613643169403076,
      "learning_rate": 3.5457423899554716e-05,
      "loss": 0.6504,
      "step": 1912500
    },
    {
      "epoch": 17.45200379589751,
      "grad_norm": 4.1720099449157715,
      "learning_rate": 3.545666350341874e-05,
      "loss": 0.6718,
      "step": 1912600
    },
    {
      "epoch": 17.452916271260676,
      "grad_norm": 3.0785744190216064,
      "learning_rate": 3.5455903107282776e-05,
      "loss": 0.6649,
      "step": 1912700
    },
    {
      "epoch": 17.45382874662384,
      "grad_norm": 3.093632221221924,
      "learning_rate": 3.54551427111468e-05,
      "loss": 0.6627,
      "step": 1912800
    },
    {
      "epoch": 17.454741221987007,
      "grad_norm": 3.282327175140381,
      "learning_rate": 3.545438231501083e-05,
      "loss": 0.6834,
      "step": 1912900
    },
    {
      "epoch": 17.455653697350172,
      "grad_norm": 4.000338077545166,
      "learning_rate": 3.545362191887486e-05,
      "loss": 0.6616,
      "step": 1913000
    },
    {
      "epoch": 17.456566172713337,
      "grad_norm": 2.829042673110962,
      "learning_rate": 3.545286152273889e-05,
      "loss": 0.6684,
      "step": 1913100
    },
    {
      "epoch": 17.457478648076503,
      "grad_norm": 4.109846591949463,
      "learning_rate": 3.545210112660291e-05,
      "loss": 0.6396,
      "step": 1913200
    },
    {
      "epoch": 17.458391123439668,
      "grad_norm": 4.758504867553711,
      "learning_rate": 3.545134073046695e-05,
      "loss": 0.6865,
      "step": 1913300
    },
    {
      "epoch": 17.459303598802833,
      "grad_norm": 3.7197482585906982,
      "learning_rate": 3.545058033433097e-05,
      "loss": 0.6616,
      "step": 1913400
    },
    {
      "epoch": 17.460216074166,
      "grad_norm": 3.1637392044067383,
      "learning_rate": 3.5449819938195e-05,
      "loss": 0.6502,
      "step": 1913500
    },
    {
      "epoch": 17.461128549529164,
      "grad_norm": 3.740443229675293,
      "learning_rate": 3.544905954205903e-05,
      "loss": 0.6517,
      "step": 1913600
    },
    {
      "epoch": 17.46204102489233,
      "grad_norm": 3.942934989929199,
      "learning_rate": 3.544829914592306e-05,
      "loss": 0.6964,
      "step": 1913700
    },
    {
      "epoch": 17.462953500255495,
      "grad_norm": 5.157169342041016,
      "learning_rate": 3.544753874978709e-05,
      "loss": 0.6546,
      "step": 1913800
    },
    {
      "epoch": 17.46386597561866,
      "grad_norm": 4.891768932342529,
      "learning_rate": 3.544677835365112e-05,
      "loss": 0.6984,
      "step": 1913900
    },
    {
      "epoch": 17.464778450981825,
      "grad_norm": 3.914720296859741,
      "learning_rate": 3.5446017957515147e-05,
      "loss": 0.6176,
      "step": 1914000
    },
    {
      "epoch": 17.46569092634499,
      "grad_norm": 4.268345355987549,
      "learning_rate": 3.5445257561379183e-05,
      "loss": 0.6185,
      "step": 1914100
    },
    {
      "epoch": 17.466603401708152,
      "grad_norm": 4.276447296142578,
      "learning_rate": 3.544449716524321e-05,
      "loss": 0.6804,
      "step": 1914200
    },
    {
      "epoch": 17.467515877071317,
      "grad_norm": 4.350679874420166,
      "learning_rate": 3.544373676910723e-05,
      "loss": 0.6529,
      "step": 1914300
    },
    {
      "epoch": 17.468428352434483,
      "grad_norm": 5.169715404510498,
      "learning_rate": 3.544297637297127e-05,
      "loss": 0.6934,
      "step": 1914400
    },
    {
      "epoch": 17.469340827797648,
      "grad_norm": 3.6190474033355713,
      "learning_rate": 3.544221597683529e-05,
      "loss": 0.6507,
      "step": 1914500
    },
    {
      "epoch": 17.470253303160813,
      "grad_norm": 3.921051025390625,
      "learning_rate": 3.544145558069932e-05,
      "loss": 0.6621,
      "step": 1914600
    },
    {
      "epoch": 17.47116577852398,
      "grad_norm": 4.850959300994873,
      "learning_rate": 3.544069518456335e-05,
      "loss": 0.6336,
      "step": 1914700
    },
    {
      "epoch": 17.472078253887144,
      "grad_norm": 4.117433071136475,
      "learning_rate": 3.543993478842738e-05,
      "loss": 0.6676,
      "step": 1914800
    },
    {
      "epoch": 17.47299072925031,
      "grad_norm": 2.2795300483703613,
      "learning_rate": 3.543917439229141e-05,
      "loss": 0.6435,
      "step": 1914900
    },
    {
      "epoch": 17.473903204613475,
      "grad_norm": 3.216104030609131,
      "learning_rate": 3.543841399615544e-05,
      "loss": 0.7029,
      "step": 1915000
    },
    {
      "epoch": 17.47481567997664,
      "grad_norm": 3.0775058269500732,
      "learning_rate": 3.5437653600019464e-05,
      "loss": 0.6288,
      "step": 1915100
    },
    {
      "epoch": 17.475728155339805,
      "grad_norm": 4.662219047546387,
      "learning_rate": 3.54368932038835e-05,
      "loss": 0.6644,
      "step": 1915200
    },
    {
      "epoch": 17.47664063070297,
      "grad_norm": 3.9476089477539062,
      "learning_rate": 3.5436132807747524e-05,
      "loss": 0.6737,
      "step": 1915300
    },
    {
      "epoch": 17.477553106066136,
      "grad_norm": 4.690914154052734,
      "learning_rate": 3.5435372411611554e-05,
      "loss": 0.6804,
      "step": 1915400
    },
    {
      "epoch": 17.4784655814293,
      "grad_norm": 3.1082875728607178,
      "learning_rate": 3.5434612015475584e-05,
      "loss": 0.6537,
      "step": 1915500
    },
    {
      "epoch": 17.479378056792466,
      "grad_norm": 4.3535380363464355,
      "learning_rate": 3.5433851619339614e-05,
      "loss": 0.6761,
      "step": 1915600
    },
    {
      "epoch": 17.48029053215563,
      "grad_norm": 4.116208076477051,
      "learning_rate": 3.543309122320364e-05,
      "loss": 0.6657,
      "step": 1915700
    },
    {
      "epoch": 17.481203007518797,
      "grad_norm": 4.299911022186279,
      "learning_rate": 3.5432330827067674e-05,
      "loss": 0.6915,
      "step": 1915800
    },
    {
      "epoch": 17.482115482881962,
      "grad_norm": 4.754478931427002,
      "learning_rate": 3.54315704309317e-05,
      "loss": 0.6849,
      "step": 1915900
    },
    {
      "epoch": 17.483027958245128,
      "grad_norm": 3.957502603530884,
      "learning_rate": 3.543081003479573e-05,
      "loss": 0.6295,
      "step": 1916000
    },
    {
      "epoch": 17.483940433608293,
      "grad_norm": 4.039613246917725,
      "learning_rate": 3.543004963865976e-05,
      "loss": 0.6765,
      "step": 1916100
    },
    {
      "epoch": 17.484852908971458,
      "grad_norm": 4.23472785949707,
      "learning_rate": 3.542928924252379e-05,
      "loss": 0.6548,
      "step": 1916200
    },
    {
      "epoch": 17.485765384334623,
      "grad_norm": 4.1304402351379395,
      "learning_rate": 3.542852884638782e-05,
      "loss": 0.659,
      "step": 1916300
    },
    {
      "epoch": 17.48667785969779,
      "grad_norm": 3.462831497192383,
      "learning_rate": 3.542776845025185e-05,
      "loss": 0.6684,
      "step": 1916400
    },
    {
      "epoch": 17.487590335060954,
      "grad_norm": 3.0159499645233154,
      "learning_rate": 3.542700805411587e-05,
      "loss": 0.6699,
      "step": 1916500
    },
    {
      "epoch": 17.48850281042412,
      "grad_norm": 4.572051525115967,
      "learning_rate": 3.542624765797991e-05,
      "loss": 0.621,
      "step": 1916600
    },
    {
      "epoch": 17.489415285787285,
      "grad_norm": 3.4142467975616455,
      "learning_rate": 3.542548726184393e-05,
      "loss": 0.646,
      "step": 1916700
    },
    {
      "epoch": 17.49032776115045,
      "grad_norm": 4.430640697479248,
      "learning_rate": 3.542472686570796e-05,
      "loss": 0.6441,
      "step": 1916800
    },
    {
      "epoch": 17.491240236513615,
      "grad_norm": 4.473754405975342,
      "learning_rate": 3.542396646957199e-05,
      "loss": 0.6605,
      "step": 1916900
    },
    {
      "epoch": 17.49215271187678,
      "grad_norm": 4.492190837860107,
      "learning_rate": 3.542320607343602e-05,
      "loss": 0.645,
      "step": 1917000
    },
    {
      "epoch": 17.493065187239946,
      "grad_norm": 3.5201873779296875,
      "learning_rate": 3.5422445677300045e-05,
      "loss": 0.6653,
      "step": 1917100
    },
    {
      "epoch": 17.49397766260311,
      "grad_norm": 3.960329055786133,
      "learning_rate": 3.5421685281164075e-05,
      "loss": 0.6757,
      "step": 1917200
    },
    {
      "epoch": 17.494890137966276,
      "grad_norm": 3.6067771911621094,
      "learning_rate": 3.5420924885028105e-05,
      "loss": 0.6495,
      "step": 1917300
    },
    {
      "epoch": 17.49580261332944,
      "grad_norm": 3.5898804664611816,
      "learning_rate": 3.5420164488892135e-05,
      "loss": 0.6455,
      "step": 1917400
    },
    {
      "epoch": 17.496715088692604,
      "grad_norm": 4.510793209075928,
      "learning_rate": 3.5419404092756165e-05,
      "loss": 0.6569,
      "step": 1917500
    },
    {
      "epoch": 17.49762756405577,
      "grad_norm": 3.531569004058838,
      "learning_rate": 3.541864369662019e-05,
      "loss": 0.706,
      "step": 1917600
    },
    {
      "epoch": 17.498540039418934,
      "grad_norm": 3.44405460357666,
      "learning_rate": 3.5417883300484225e-05,
      "loss": 0.6673,
      "step": 1917700
    },
    {
      "epoch": 17.4994525147821,
      "grad_norm": 3.567786931991577,
      "learning_rate": 3.541712290434825e-05,
      "loss": 0.682,
      "step": 1917800
    },
    {
      "epoch": 17.500364990145265,
      "grad_norm": 4.566964149475098,
      "learning_rate": 3.541636250821228e-05,
      "loss": 0.6726,
      "step": 1917900
    },
    {
      "epoch": 17.50127746550843,
      "grad_norm": 3.3891475200653076,
      "learning_rate": 3.541560211207631e-05,
      "loss": 0.6084,
      "step": 1918000
    },
    {
      "epoch": 17.502189940871595,
      "grad_norm": 3.9826736450195312,
      "learning_rate": 3.541484171594034e-05,
      "loss": 0.6945,
      "step": 1918100
    },
    {
      "epoch": 17.50310241623476,
      "grad_norm": 3.965818166732788,
      "learning_rate": 3.541408131980436e-05,
      "loss": 0.6709,
      "step": 1918200
    },
    {
      "epoch": 17.504014891597926,
      "grad_norm": 4.51971435546875,
      "learning_rate": 3.54133209236684e-05,
      "loss": 0.6524,
      "step": 1918300
    },
    {
      "epoch": 17.50492736696109,
      "grad_norm": 3.482407331466675,
      "learning_rate": 3.541256052753242e-05,
      "loss": 0.6579,
      "step": 1918400
    },
    {
      "epoch": 17.505839842324256,
      "grad_norm": 3.853688955307007,
      "learning_rate": 3.541180013139645e-05,
      "loss": 0.6857,
      "step": 1918500
    },
    {
      "epoch": 17.506752317687422,
      "grad_norm": 4.097033500671387,
      "learning_rate": 3.541103973526048e-05,
      "loss": 0.6426,
      "step": 1918600
    },
    {
      "epoch": 17.507664793050587,
      "grad_norm": 3.0311474800109863,
      "learning_rate": 3.541027933912451e-05,
      "loss": 0.6844,
      "step": 1918700
    },
    {
      "epoch": 17.508577268413752,
      "grad_norm": 2.911386489868164,
      "learning_rate": 3.540951894298854e-05,
      "loss": 0.6367,
      "step": 1918800
    },
    {
      "epoch": 17.509489743776918,
      "grad_norm": 4.4083709716796875,
      "learning_rate": 3.540875854685257e-05,
      "loss": 0.6702,
      "step": 1918900
    },
    {
      "epoch": 17.510402219140083,
      "grad_norm": 3.9373600482940674,
      "learning_rate": 3.5407998150716596e-05,
      "loss": 0.6542,
      "step": 1919000
    },
    {
      "epoch": 17.51131469450325,
      "grad_norm": 3.9267725944519043,
      "learning_rate": 3.540723775458063e-05,
      "loss": 0.6573,
      "step": 1919100
    },
    {
      "epoch": 17.512227169866414,
      "grad_norm": 3.780752658843994,
      "learning_rate": 3.5406477358444656e-05,
      "loss": 0.6652,
      "step": 1919200
    },
    {
      "epoch": 17.51313964522958,
      "grad_norm": 4.135382652282715,
      "learning_rate": 3.5405716962308686e-05,
      "loss": 0.6539,
      "step": 1919300
    },
    {
      "epoch": 17.514052120592744,
      "grad_norm": 3.7104570865631104,
      "learning_rate": 3.5404956566172716e-05,
      "loss": 0.687,
      "step": 1919400
    },
    {
      "epoch": 17.51496459595591,
      "grad_norm": 4.424365997314453,
      "learning_rate": 3.5404196170036746e-05,
      "loss": 0.6623,
      "step": 1919500
    },
    {
      "epoch": 17.515877071319075,
      "grad_norm": 4.413651943206787,
      "learning_rate": 3.540343577390077e-05,
      "loss": 0.6431,
      "step": 1919600
    },
    {
      "epoch": 17.51678954668224,
      "grad_norm": 3.987769603729248,
      "learning_rate": 3.5402675377764806e-05,
      "loss": 0.6733,
      "step": 1919700
    },
    {
      "epoch": 17.517702022045405,
      "grad_norm": 3.9732069969177246,
      "learning_rate": 3.540191498162883e-05,
      "loss": 0.6377,
      "step": 1919800
    },
    {
      "epoch": 17.51861449740857,
      "grad_norm": 4.815521717071533,
      "learning_rate": 3.540115458549286e-05,
      "loss": 0.6926,
      "step": 1919900
    },
    {
      "epoch": 17.519526972771736,
      "grad_norm": 3.8608720302581787,
      "learning_rate": 3.540039418935689e-05,
      "loss": 0.6807,
      "step": 1920000
    },
    {
      "epoch": 17.5204394481349,
      "grad_norm": 4.124125957489014,
      "learning_rate": 3.539963379322091e-05,
      "loss": 0.63,
      "step": 1920100
    },
    {
      "epoch": 17.521351923498067,
      "grad_norm": 3.6579360961914062,
      "learning_rate": 3.539887339708495e-05,
      "loss": 0.6406,
      "step": 1920200
    },
    {
      "epoch": 17.522264398861232,
      "grad_norm": 2.849424362182617,
      "learning_rate": 3.539811300094897e-05,
      "loss": 0.6726,
      "step": 1920300
    },
    {
      "epoch": 17.523176874224397,
      "grad_norm": 4.305442810058594,
      "learning_rate": 3.5397352604813e-05,
      "loss": 0.6442,
      "step": 1920400
    },
    {
      "epoch": 17.524089349587562,
      "grad_norm": 4.261017799377441,
      "learning_rate": 3.539659220867703e-05,
      "loss": 0.6839,
      "step": 1920500
    },
    {
      "epoch": 17.525001824950728,
      "grad_norm": 3.5136358737945557,
      "learning_rate": 3.539583181254106e-05,
      "loss": 0.6602,
      "step": 1920600
    },
    {
      "epoch": 17.525914300313893,
      "grad_norm": 3.3917293548583984,
      "learning_rate": 3.539507141640509e-05,
      "loss": 0.674,
      "step": 1920700
    },
    {
      "epoch": 17.52682677567706,
      "grad_norm": 3.9103736877441406,
      "learning_rate": 3.539431102026912e-05,
      "loss": 0.7067,
      "step": 1920800
    },
    {
      "epoch": 17.527739251040224,
      "grad_norm": 2.6820061206817627,
      "learning_rate": 3.539355062413315e-05,
      "loss": 0.6852,
      "step": 1920900
    },
    {
      "epoch": 17.528651726403385,
      "grad_norm": 3.689868688583374,
      "learning_rate": 3.5392790227997184e-05,
      "loss": 0.6567,
      "step": 1921000
    },
    {
      "epoch": 17.52956420176655,
      "grad_norm": 4.663314342498779,
      "learning_rate": 3.539202983186121e-05,
      "loss": 0.6697,
      "step": 1921100
    },
    {
      "epoch": 17.530476677129716,
      "grad_norm": 3.6558945178985596,
      "learning_rate": 3.539126943572524e-05,
      "loss": 0.6531,
      "step": 1921200
    },
    {
      "epoch": 17.53138915249288,
      "grad_norm": 3.1311662197113037,
      "learning_rate": 3.539050903958927e-05,
      "loss": 0.7016,
      "step": 1921300
    },
    {
      "epoch": 17.532301627856047,
      "grad_norm": 4.105340003967285,
      "learning_rate": 3.53897486434533e-05,
      "loss": 0.6744,
      "step": 1921400
    },
    {
      "epoch": 17.533214103219212,
      "grad_norm": 4.215277194976807,
      "learning_rate": 3.538898824731732e-05,
      "loss": 0.6979,
      "step": 1921500
    },
    {
      "epoch": 17.534126578582377,
      "grad_norm": 3.018094062805176,
      "learning_rate": 3.538822785118136e-05,
      "loss": 0.667,
      "step": 1921600
    },
    {
      "epoch": 17.535039053945543,
      "grad_norm": 2.5972213745117188,
      "learning_rate": 3.538746745504538e-05,
      "loss": 0.666,
      "step": 1921700
    },
    {
      "epoch": 17.535951529308708,
      "grad_norm": 3.886828660964966,
      "learning_rate": 3.538670705890941e-05,
      "loss": 0.6498,
      "step": 1921800
    },
    {
      "epoch": 17.536864004671873,
      "grad_norm": 4.279846668243408,
      "learning_rate": 3.538594666277344e-05,
      "loss": 0.6753,
      "step": 1921900
    },
    {
      "epoch": 17.53777648003504,
      "grad_norm": 3.0324628353118896,
      "learning_rate": 3.538518626663747e-05,
      "loss": 0.6413,
      "step": 1922000
    },
    {
      "epoch": 17.538688955398204,
      "grad_norm": 2.975473165512085,
      "learning_rate": 3.53844258705015e-05,
      "loss": 0.6631,
      "step": 1922100
    },
    {
      "epoch": 17.53960143076137,
      "grad_norm": 4.079655170440674,
      "learning_rate": 3.538366547436553e-05,
      "loss": 0.6832,
      "step": 1922200
    },
    {
      "epoch": 17.540513906124534,
      "grad_norm": 3.595370054244995,
      "learning_rate": 3.5382905078229554e-05,
      "loss": 0.6465,
      "step": 1922300
    },
    {
      "epoch": 17.5414263814877,
      "grad_norm": 3.7527170181274414,
      "learning_rate": 3.538214468209359e-05,
      "loss": 0.6253,
      "step": 1922400
    },
    {
      "epoch": 17.542338856850865,
      "grad_norm": 4.232062339782715,
      "learning_rate": 3.5381384285957614e-05,
      "loss": 0.6439,
      "step": 1922500
    },
    {
      "epoch": 17.54325133221403,
      "grad_norm": 4.031303405761719,
      "learning_rate": 3.5380623889821644e-05,
      "loss": 0.68,
      "step": 1922600
    },
    {
      "epoch": 17.544163807577196,
      "grad_norm": 3.7706735134124756,
      "learning_rate": 3.5379863493685674e-05,
      "loss": 0.6721,
      "step": 1922700
    },
    {
      "epoch": 17.54507628294036,
      "grad_norm": 4.235862731933594,
      "learning_rate": 3.53791030975497e-05,
      "loss": 0.6524,
      "step": 1922800
    },
    {
      "epoch": 17.545988758303526,
      "grad_norm": 3.686840772628784,
      "learning_rate": 3.537834270141373e-05,
      "loss": 0.6508,
      "step": 1922900
    },
    {
      "epoch": 17.54690123366669,
      "grad_norm": 3.3596556186676025,
      "learning_rate": 3.537758230527776e-05,
      "loss": 0.6501,
      "step": 1923000
    },
    {
      "epoch": 17.547813709029857,
      "grad_norm": 4.148458957672119,
      "learning_rate": 3.537682190914179e-05,
      "loss": 0.6658,
      "step": 1923100
    },
    {
      "epoch": 17.548726184393022,
      "grad_norm": 3.616537094116211,
      "learning_rate": 3.537606151300582e-05,
      "loss": 0.6415,
      "step": 1923200
    },
    {
      "epoch": 17.549638659756187,
      "grad_norm": 3.943096876144409,
      "learning_rate": 3.537530111686985e-05,
      "loss": 0.6441,
      "step": 1923300
    },
    {
      "epoch": 17.550551135119353,
      "grad_norm": 4.337550640106201,
      "learning_rate": 3.537454072073387e-05,
      "loss": 0.6714,
      "step": 1923400
    },
    {
      "epoch": 17.551463610482518,
      "grad_norm": 3.24337100982666,
      "learning_rate": 3.537378032459791e-05,
      "loss": 0.6616,
      "step": 1923500
    },
    {
      "epoch": 17.552376085845683,
      "grad_norm": 3.57100772857666,
      "learning_rate": 3.537301992846193e-05,
      "loss": 0.6478,
      "step": 1923600
    },
    {
      "epoch": 17.55328856120885,
      "grad_norm": 4.689530372619629,
      "learning_rate": 3.537225953232596e-05,
      "loss": 0.6573,
      "step": 1923700
    },
    {
      "epoch": 17.554201036572014,
      "grad_norm": 3.622316837310791,
      "learning_rate": 3.537149913618999e-05,
      "loss": 0.6552,
      "step": 1923800
    },
    {
      "epoch": 17.55511351193518,
      "grad_norm": 4.52882719039917,
      "learning_rate": 3.537073874005402e-05,
      "loss": 0.5991,
      "step": 1923900
    },
    {
      "epoch": 17.556025987298344,
      "grad_norm": 3.9388108253479004,
      "learning_rate": 3.5369978343918045e-05,
      "loss": 0.6623,
      "step": 1924000
    },
    {
      "epoch": 17.55693846266151,
      "grad_norm": 4.324592590332031,
      "learning_rate": 3.536921794778208e-05,
      "loss": 0.6706,
      "step": 1924100
    },
    {
      "epoch": 17.557850938024675,
      "grad_norm": 2.5941147804260254,
      "learning_rate": 3.5368457551646105e-05,
      "loss": 0.6731,
      "step": 1924200
    },
    {
      "epoch": 17.558763413387837,
      "grad_norm": 3.650026321411133,
      "learning_rate": 3.5367697155510135e-05,
      "loss": 0.6736,
      "step": 1924300
    },
    {
      "epoch": 17.559675888751002,
      "grad_norm": 4.473237037658691,
      "learning_rate": 3.5366936759374165e-05,
      "loss": 0.6374,
      "step": 1924400
    },
    {
      "epoch": 17.560588364114167,
      "grad_norm": 4.338778018951416,
      "learning_rate": 3.5366176363238195e-05,
      "loss": 0.6484,
      "step": 1924500
    },
    {
      "epoch": 17.561500839477333,
      "grad_norm": 4.324601650238037,
      "learning_rate": 3.5365415967102225e-05,
      "loss": 0.6837,
      "step": 1924600
    },
    {
      "epoch": 17.562413314840498,
      "grad_norm": 3.032012939453125,
      "learning_rate": 3.5364655570966255e-05,
      "loss": 0.6329,
      "step": 1924700
    },
    {
      "epoch": 17.563325790203663,
      "grad_norm": 4.078963279724121,
      "learning_rate": 3.536389517483028e-05,
      "loss": 0.6806,
      "step": 1924800
    },
    {
      "epoch": 17.56423826556683,
      "grad_norm": 4.529836654663086,
      "learning_rate": 3.5363134778694315e-05,
      "loss": 0.6775,
      "step": 1924900
    },
    {
      "epoch": 17.565150740929994,
      "grad_norm": 3.9582111835479736,
      "learning_rate": 3.536237438255834e-05,
      "loss": 0.6482,
      "step": 1925000
    },
    {
      "epoch": 17.56606321629316,
      "grad_norm": 4.077407360076904,
      "learning_rate": 3.536161398642237e-05,
      "loss": 0.646,
      "step": 1925100
    },
    {
      "epoch": 17.566975691656324,
      "grad_norm": 4.032812595367432,
      "learning_rate": 3.53608535902864e-05,
      "loss": 0.6552,
      "step": 1925200
    },
    {
      "epoch": 17.56788816701949,
      "grad_norm": 3.1568093299865723,
      "learning_rate": 3.536009319415043e-05,
      "loss": 0.6743,
      "step": 1925300
    },
    {
      "epoch": 17.568800642382655,
      "grad_norm": 3.8201839923858643,
      "learning_rate": 3.535933279801445e-05,
      "loss": 0.6254,
      "step": 1925400
    },
    {
      "epoch": 17.56971311774582,
      "grad_norm": 4.069768905639648,
      "learning_rate": 3.535857240187849e-05,
      "loss": 0.7027,
      "step": 1925500
    },
    {
      "epoch": 17.570625593108986,
      "grad_norm": 3.788893461227417,
      "learning_rate": 3.535781200574251e-05,
      "loss": 0.6722,
      "step": 1925600
    },
    {
      "epoch": 17.57153806847215,
      "grad_norm": 4.990140438079834,
      "learning_rate": 3.535705160960654e-05,
      "loss": 0.6649,
      "step": 1925700
    },
    {
      "epoch": 17.572450543835316,
      "grad_norm": 4.08880615234375,
      "learning_rate": 3.535629121347057e-05,
      "loss": 0.676,
      "step": 1925800
    },
    {
      "epoch": 17.57336301919848,
      "grad_norm": 3.1227073669433594,
      "learning_rate": 3.5355530817334596e-05,
      "loss": 0.6468,
      "step": 1925900
    },
    {
      "epoch": 17.574275494561647,
      "grad_norm": 4.302999496459961,
      "learning_rate": 3.535477042119863e-05,
      "loss": 0.6845,
      "step": 1926000
    },
    {
      "epoch": 17.575187969924812,
      "grad_norm": 3.1874561309814453,
      "learning_rate": 3.5354010025062656e-05,
      "loss": 0.624,
      "step": 1926100
    },
    {
      "epoch": 17.576100445287977,
      "grad_norm": 3.8154513835906982,
      "learning_rate": 3.5353249628926686e-05,
      "loss": 0.6768,
      "step": 1926200
    },
    {
      "epoch": 17.577012920651143,
      "grad_norm": 4.440071105957031,
      "learning_rate": 3.5352489232790716e-05,
      "loss": 0.67,
      "step": 1926300
    },
    {
      "epoch": 17.577925396014308,
      "grad_norm": 4.099349021911621,
      "learning_rate": 3.5351728836654746e-05,
      "loss": 0.6516,
      "step": 1926400
    },
    {
      "epoch": 17.578837871377473,
      "grad_norm": 4.043830871582031,
      "learning_rate": 3.535096844051877e-05,
      "loss": 0.6945,
      "step": 1926500
    },
    {
      "epoch": 17.57975034674064,
      "grad_norm": 3.252474308013916,
      "learning_rate": 3.5350208044382806e-05,
      "loss": 0.709,
      "step": 1926600
    },
    {
      "epoch": 17.580662822103804,
      "grad_norm": 4.6769700050354,
      "learning_rate": 3.534944764824683e-05,
      "loss": 0.6462,
      "step": 1926700
    },
    {
      "epoch": 17.58157529746697,
      "grad_norm": 3.9026856422424316,
      "learning_rate": 3.534868725211086e-05,
      "loss": 0.6907,
      "step": 1926800
    },
    {
      "epoch": 17.582487772830135,
      "grad_norm": 3.3936877250671387,
      "learning_rate": 3.534792685597489e-05,
      "loss": 0.6681,
      "step": 1926900
    },
    {
      "epoch": 17.5834002481933,
      "grad_norm": 3.471926212310791,
      "learning_rate": 3.534716645983892e-05,
      "loss": 0.6438,
      "step": 1927000
    },
    {
      "epoch": 17.584312723556465,
      "grad_norm": 3.854382276535034,
      "learning_rate": 3.534640606370295e-05,
      "loss": 0.6864,
      "step": 1927100
    },
    {
      "epoch": 17.58522519891963,
      "grad_norm": 3.664226770401001,
      "learning_rate": 3.534564566756698e-05,
      "loss": 0.6928,
      "step": 1927200
    },
    {
      "epoch": 17.586137674282796,
      "grad_norm": 4.166168212890625,
      "learning_rate": 3.5344885271431e-05,
      "loss": 0.6667,
      "step": 1927300
    },
    {
      "epoch": 17.58705014964596,
      "grad_norm": 4.471988201141357,
      "learning_rate": 3.534412487529504e-05,
      "loss": 0.6813,
      "step": 1927400
    },
    {
      "epoch": 17.587962625009126,
      "grad_norm": 4.052435398101807,
      "learning_rate": 3.534336447915906e-05,
      "loss": 0.6626,
      "step": 1927500
    },
    {
      "epoch": 17.58887510037229,
      "grad_norm": 4.039209842681885,
      "learning_rate": 3.5342604083023093e-05,
      "loss": 0.7088,
      "step": 1927600
    },
    {
      "epoch": 17.589787575735457,
      "grad_norm": 3.90356183052063,
      "learning_rate": 3.5341843686887123e-05,
      "loss": 0.6781,
      "step": 1927700
    },
    {
      "epoch": 17.59070005109862,
      "grad_norm": 3.6912941932678223,
      "learning_rate": 3.5341083290751154e-05,
      "loss": 0.6749,
      "step": 1927800
    },
    {
      "epoch": 17.591612526461784,
      "grad_norm": 4.409974098205566,
      "learning_rate": 3.534032289461518e-05,
      "loss": 0.6424,
      "step": 1927900
    },
    {
      "epoch": 17.59252500182495,
      "grad_norm": 3.705444574356079,
      "learning_rate": 3.5339562498479214e-05,
      "loss": 0.6324,
      "step": 1928000
    },
    {
      "epoch": 17.593437477188115,
      "grad_norm": 3.785013437271118,
      "learning_rate": 3.533880210234324e-05,
      "loss": 0.6939,
      "step": 1928100
    },
    {
      "epoch": 17.59434995255128,
      "grad_norm": 3.733750104904175,
      "learning_rate": 3.533804170620727e-05,
      "loss": 0.6465,
      "step": 1928200
    },
    {
      "epoch": 17.595262427914445,
      "grad_norm": 3.453129768371582,
      "learning_rate": 3.53372813100713e-05,
      "loss": 0.6502,
      "step": 1928300
    },
    {
      "epoch": 17.59617490327761,
      "grad_norm": 3.8003692626953125,
      "learning_rate": 3.533652091393533e-05,
      "loss": 0.6676,
      "step": 1928400
    },
    {
      "epoch": 17.597087378640776,
      "grad_norm": 3.744229793548584,
      "learning_rate": 3.533576051779936e-05,
      "loss": 0.6367,
      "step": 1928500
    },
    {
      "epoch": 17.59799985400394,
      "grad_norm": 3.3203043937683105,
      "learning_rate": 3.533500012166338e-05,
      "loss": 0.6543,
      "step": 1928600
    },
    {
      "epoch": 17.598912329367106,
      "grad_norm": 3.89090895652771,
      "learning_rate": 3.533423972552741e-05,
      "loss": 0.6778,
      "step": 1928700
    },
    {
      "epoch": 17.59982480473027,
      "grad_norm": 4.291157245635986,
      "learning_rate": 3.533347932939144e-05,
      "loss": 0.6773,
      "step": 1928800
    },
    {
      "epoch": 17.600737280093437,
      "grad_norm": 2.965501546859741,
      "learning_rate": 3.533271893325547e-05,
      "loss": 0.6338,
      "step": 1928900
    },
    {
      "epoch": 17.601649755456602,
      "grad_norm": 3.646475315093994,
      "learning_rate": 3.5331958537119494e-05,
      "loss": 0.6668,
      "step": 1929000
    },
    {
      "epoch": 17.602562230819768,
      "grad_norm": 3.395808696746826,
      "learning_rate": 3.533119814098353e-05,
      "loss": 0.6792,
      "step": 1929100
    },
    {
      "epoch": 17.603474706182933,
      "grad_norm": 4.471190452575684,
      "learning_rate": 3.5330437744847554e-05,
      "loss": 0.6471,
      "step": 1929200
    },
    {
      "epoch": 17.604387181546098,
      "grad_norm": 4.757167339324951,
      "learning_rate": 3.5329677348711584e-05,
      "loss": 0.6597,
      "step": 1929300
    },
    {
      "epoch": 17.605299656909263,
      "grad_norm": 3.155472755432129,
      "learning_rate": 3.5328916952575614e-05,
      "loss": 0.6494,
      "step": 1929400
    },
    {
      "epoch": 17.60621213227243,
      "grad_norm": 4.792952537536621,
      "learning_rate": 3.5328156556439644e-05,
      "loss": 0.662,
      "step": 1929500
    },
    {
      "epoch": 17.607124607635594,
      "grad_norm": 3.864377498626709,
      "learning_rate": 3.5327396160303674e-05,
      "loss": 0.6485,
      "step": 1929600
    },
    {
      "epoch": 17.60803708299876,
      "grad_norm": 4.42952299118042,
      "learning_rate": 3.5326635764167704e-05,
      "loss": 0.6726,
      "step": 1929700
    },
    {
      "epoch": 17.608949558361925,
      "grad_norm": 3.760704278945923,
      "learning_rate": 3.532587536803173e-05,
      "loss": 0.6573,
      "step": 1929800
    },
    {
      "epoch": 17.60986203372509,
      "grad_norm": 3.0887672901153564,
      "learning_rate": 3.5325114971895765e-05,
      "loss": 0.6617,
      "step": 1929900
    },
    {
      "epoch": 17.610774509088255,
      "grad_norm": 4.296797752380371,
      "learning_rate": 3.532435457575979e-05,
      "loss": 0.6751,
      "step": 1930000
    },
    {
      "epoch": 17.61168698445142,
      "grad_norm": 3.9241790771484375,
      "learning_rate": 3.532359417962382e-05,
      "loss": 0.6618,
      "step": 1930100
    },
    {
      "epoch": 17.612599459814586,
      "grad_norm": 3.228935718536377,
      "learning_rate": 3.532283378348785e-05,
      "loss": 0.6652,
      "step": 1930200
    },
    {
      "epoch": 17.61351193517775,
      "grad_norm": 4.1317362785339355,
      "learning_rate": 3.532207338735188e-05,
      "loss": 0.6699,
      "step": 1930300
    },
    {
      "epoch": 17.614424410540916,
      "grad_norm": 3.709860324859619,
      "learning_rate": 3.53213129912159e-05,
      "loss": 0.646,
      "step": 1930400
    },
    {
      "epoch": 17.61533688590408,
      "grad_norm": 3.5770466327667236,
      "learning_rate": 3.532055259507994e-05,
      "loss": 0.7088,
      "step": 1930500
    },
    {
      "epoch": 17.616249361267247,
      "grad_norm": 3.472059488296509,
      "learning_rate": 3.531979219894396e-05,
      "loss": 0.6798,
      "step": 1930600
    },
    {
      "epoch": 17.617161836630412,
      "grad_norm": 4.040395259857178,
      "learning_rate": 3.531903180280799e-05,
      "loss": 0.6526,
      "step": 1930700
    },
    {
      "epoch": 17.618074311993578,
      "grad_norm": 4.773786544799805,
      "learning_rate": 3.531827140667202e-05,
      "loss": 0.6667,
      "step": 1930800
    },
    {
      "epoch": 17.618986787356743,
      "grad_norm": 4.774935245513916,
      "learning_rate": 3.531751101053605e-05,
      "loss": 0.6542,
      "step": 1930900
    },
    {
      "epoch": 17.61989926271991,
      "grad_norm": 4.342265605926514,
      "learning_rate": 3.531675061440008e-05,
      "loss": 0.6256,
      "step": 1931000
    },
    {
      "epoch": 17.62081173808307,
      "grad_norm": 4.839704990386963,
      "learning_rate": 3.531599021826411e-05,
      "loss": 0.6523,
      "step": 1931100
    },
    {
      "epoch": 17.621724213446235,
      "grad_norm": 4.073239803314209,
      "learning_rate": 3.5315229822128135e-05,
      "loss": 0.6357,
      "step": 1931200
    },
    {
      "epoch": 17.6226366888094,
      "grad_norm": 3.2862281799316406,
      "learning_rate": 3.531446942599217e-05,
      "loss": 0.6614,
      "step": 1931300
    },
    {
      "epoch": 17.623549164172566,
      "grad_norm": 3.814375638961792,
      "learning_rate": 3.5313709029856195e-05,
      "loss": 0.6661,
      "step": 1931400
    },
    {
      "epoch": 17.62446163953573,
      "grad_norm": 4.256614685058594,
      "learning_rate": 3.5312948633720225e-05,
      "loss": 0.6821,
      "step": 1931500
    },
    {
      "epoch": 17.625374114898896,
      "grad_norm": 6.946083068847656,
      "learning_rate": 3.5312188237584255e-05,
      "loss": 0.6622,
      "step": 1931600
    },
    {
      "epoch": 17.626286590262062,
      "grad_norm": 3.3349392414093018,
      "learning_rate": 3.531142784144828e-05,
      "loss": 0.6389,
      "step": 1931700
    },
    {
      "epoch": 17.627199065625227,
      "grad_norm": 3.685922145843506,
      "learning_rate": 3.531066744531231e-05,
      "loss": 0.6709,
      "step": 1931800
    },
    {
      "epoch": 17.628111540988392,
      "grad_norm": 4.076741695404053,
      "learning_rate": 3.530990704917634e-05,
      "loss": 0.6832,
      "step": 1931900
    },
    {
      "epoch": 17.629024016351558,
      "grad_norm": 4.296673774719238,
      "learning_rate": 3.530914665304037e-05,
      "loss": 0.6339,
      "step": 1932000
    },
    {
      "epoch": 17.629936491714723,
      "grad_norm": 4.191167831420898,
      "learning_rate": 3.53083862569044e-05,
      "loss": 0.6719,
      "step": 1932100
    },
    {
      "epoch": 17.63084896707789,
      "grad_norm": 3.6433935165405273,
      "learning_rate": 3.530762586076843e-05,
      "loss": 0.6627,
      "step": 1932200
    },
    {
      "epoch": 17.631761442441054,
      "grad_norm": 4.048508167266846,
      "learning_rate": 3.530686546463245e-05,
      "loss": 0.6953,
      "step": 1932300
    },
    {
      "epoch": 17.63267391780422,
      "grad_norm": 3.272235155105591,
      "learning_rate": 3.530610506849649e-05,
      "loss": 0.6298,
      "step": 1932400
    },
    {
      "epoch": 17.633586393167384,
      "grad_norm": 3.414834976196289,
      "learning_rate": 3.530534467236051e-05,
      "loss": 0.6426,
      "step": 1932500
    },
    {
      "epoch": 17.63449886853055,
      "grad_norm": 3.6152994632720947,
      "learning_rate": 3.530458427622454e-05,
      "loss": 0.6065,
      "step": 1932600
    },
    {
      "epoch": 17.635411343893715,
      "grad_norm": 3.8983356952667236,
      "learning_rate": 3.530382388008857e-05,
      "loss": 0.6363,
      "step": 1932700
    },
    {
      "epoch": 17.63632381925688,
      "grad_norm": 4.961607456207275,
      "learning_rate": 3.53030634839526e-05,
      "loss": 0.6652,
      "step": 1932800
    },
    {
      "epoch": 17.637236294620045,
      "grad_norm": 4.526351451873779,
      "learning_rate": 3.530230308781663e-05,
      "loss": 0.6255,
      "step": 1932900
    },
    {
      "epoch": 17.63814876998321,
      "grad_norm": 3.733196496963501,
      "learning_rate": 3.530154269168066e-05,
      "loss": 0.697,
      "step": 1933000
    },
    {
      "epoch": 17.639061245346376,
      "grad_norm": 4.212805271148682,
      "learning_rate": 3.5300782295544686e-05,
      "loss": 0.6588,
      "step": 1933100
    },
    {
      "epoch": 17.63997372070954,
      "grad_norm": 4.244604587554932,
      "learning_rate": 3.5300021899408716e-05,
      "loss": 0.6154,
      "step": 1933200
    },
    {
      "epoch": 17.640886196072707,
      "grad_norm": 4.0965118408203125,
      "learning_rate": 3.5299261503272746e-05,
      "loss": 0.6856,
      "step": 1933300
    },
    {
      "epoch": 17.641798671435872,
      "grad_norm": 3.9975602626800537,
      "learning_rate": 3.5298501107136776e-05,
      "loss": 0.6403,
      "step": 1933400
    },
    {
      "epoch": 17.642711146799037,
      "grad_norm": 3.3098409175872803,
      "learning_rate": 3.5297740711000806e-05,
      "loss": 0.6486,
      "step": 1933500
    },
    {
      "epoch": 17.643623622162202,
      "grad_norm": 4.153195381164551,
      "learning_rate": 3.5296980314864836e-05,
      "loss": 0.6967,
      "step": 1933600
    },
    {
      "epoch": 17.644536097525368,
      "grad_norm": 4.09755277633667,
      "learning_rate": 3.529621991872886e-05,
      "loss": 0.6739,
      "step": 1933700
    },
    {
      "epoch": 17.645448572888533,
      "grad_norm": 4.317889213562012,
      "learning_rate": 3.5295459522592897e-05,
      "loss": 0.6497,
      "step": 1933800
    },
    {
      "epoch": 17.6463610482517,
      "grad_norm": 1.9032316207885742,
      "learning_rate": 3.529469912645692e-05,
      "loss": 0.6699,
      "step": 1933900
    },
    {
      "epoch": 17.647273523614864,
      "grad_norm": 4.009322166442871,
      "learning_rate": 3.529393873032095e-05,
      "loss": 0.6979,
      "step": 1934000
    },
    {
      "epoch": 17.64818599897803,
      "grad_norm": 4.0345330238342285,
      "learning_rate": 3.529317833418498e-05,
      "loss": 0.6725,
      "step": 1934100
    },
    {
      "epoch": 17.649098474341194,
      "grad_norm": 4.452723026275635,
      "learning_rate": 3.5292417938049e-05,
      "loss": 0.6468,
      "step": 1934200
    },
    {
      "epoch": 17.65001094970436,
      "grad_norm": 4.582799911499023,
      "learning_rate": 3.529165754191304e-05,
      "loss": 0.6793,
      "step": 1934300
    },
    {
      "epoch": 17.650923425067525,
      "grad_norm": 4.229984283447266,
      "learning_rate": 3.5290897145777063e-05,
      "loss": 0.6595,
      "step": 1934400
    },
    {
      "epoch": 17.65183590043069,
      "grad_norm": 2.817246437072754,
      "learning_rate": 3.5290136749641094e-05,
      "loss": 0.6734,
      "step": 1934500
    },
    {
      "epoch": 17.652748375793852,
      "grad_norm": 4.3436808586120605,
      "learning_rate": 3.5289376353505124e-05,
      "loss": 0.6351,
      "step": 1934600
    },
    {
      "epoch": 17.653660851157017,
      "grad_norm": 3.7857444286346436,
      "learning_rate": 3.5288615957369154e-05,
      "loss": 0.6508,
      "step": 1934700
    },
    {
      "epoch": 17.654573326520183,
      "grad_norm": 3.670618772506714,
      "learning_rate": 3.528785556123318e-05,
      "loss": 0.672,
      "step": 1934800
    },
    {
      "epoch": 17.655485801883348,
      "grad_norm": 4.270394325256348,
      "learning_rate": 3.5287095165097214e-05,
      "loss": 0.6451,
      "step": 1934900
    },
    {
      "epoch": 17.656398277246513,
      "grad_norm": 4.286997318267822,
      "learning_rate": 3.528633476896124e-05,
      "loss": 0.6554,
      "step": 1935000
    },
    {
      "epoch": 17.65731075260968,
      "grad_norm": 4.188915252685547,
      "learning_rate": 3.528557437282527e-05,
      "loss": 0.6904,
      "step": 1935100
    },
    {
      "epoch": 17.658223227972844,
      "grad_norm": 4.320805549621582,
      "learning_rate": 3.52848139766893e-05,
      "loss": 0.6875,
      "step": 1935200
    },
    {
      "epoch": 17.65913570333601,
      "grad_norm": 3.4058761596679688,
      "learning_rate": 3.528405358055333e-05,
      "loss": 0.6594,
      "step": 1935300
    },
    {
      "epoch": 17.660048178699174,
      "grad_norm": 4.969168663024902,
      "learning_rate": 3.528329318441736e-05,
      "loss": 0.6847,
      "step": 1935400
    },
    {
      "epoch": 17.66096065406234,
      "grad_norm": 3.995262622833252,
      "learning_rate": 3.528253278828139e-05,
      "loss": 0.6555,
      "step": 1935500
    },
    {
      "epoch": 17.661873129425505,
      "grad_norm": 4.897214412689209,
      "learning_rate": 3.528177239214541e-05,
      "loss": 0.6601,
      "step": 1935600
    },
    {
      "epoch": 17.66278560478867,
      "grad_norm": 3.667424440383911,
      "learning_rate": 3.528101199600945e-05,
      "loss": 0.6509,
      "step": 1935700
    },
    {
      "epoch": 17.663698080151836,
      "grad_norm": 4.2282490730285645,
      "learning_rate": 3.528025159987347e-05,
      "loss": 0.6589,
      "step": 1935800
    },
    {
      "epoch": 17.664610555515,
      "grad_norm": 3.4420011043548584,
      "learning_rate": 3.52794912037375e-05,
      "loss": 0.6584,
      "step": 1935900
    },
    {
      "epoch": 17.665523030878166,
      "grad_norm": 4.273754119873047,
      "learning_rate": 3.527873080760153e-05,
      "loss": 0.6657,
      "step": 1936000
    },
    {
      "epoch": 17.66643550624133,
      "grad_norm": 4.257164001464844,
      "learning_rate": 3.527797041146556e-05,
      "loss": 0.6898,
      "step": 1936100
    },
    {
      "epoch": 17.667347981604497,
      "grad_norm": 4.786818027496338,
      "learning_rate": 3.5277210015329584e-05,
      "loss": 0.6639,
      "step": 1936200
    },
    {
      "epoch": 17.668260456967662,
      "grad_norm": 2.807224750518799,
      "learning_rate": 3.527644961919362e-05,
      "loss": 0.6831,
      "step": 1936300
    },
    {
      "epoch": 17.669172932330827,
      "grad_norm": 4.53665018081665,
      "learning_rate": 3.5275689223057644e-05,
      "loss": 0.6666,
      "step": 1936400
    },
    {
      "epoch": 17.670085407693993,
      "grad_norm": 3.8904166221618652,
      "learning_rate": 3.5274928826921675e-05,
      "loss": 0.6677,
      "step": 1936500
    },
    {
      "epoch": 17.670997883057158,
      "grad_norm": 3.9374146461486816,
      "learning_rate": 3.5274168430785705e-05,
      "loss": 0.6629,
      "step": 1936600
    },
    {
      "epoch": 17.671910358420323,
      "grad_norm": 3.695981025695801,
      "learning_rate": 3.5273408034649735e-05,
      "loss": 0.6502,
      "step": 1936700
    },
    {
      "epoch": 17.67282283378349,
      "grad_norm": 4.043330192565918,
      "learning_rate": 3.5272647638513765e-05,
      "loss": 0.6671,
      "step": 1936800
    },
    {
      "epoch": 17.673735309146654,
      "grad_norm": 4.3077192306518555,
      "learning_rate": 3.5271887242377795e-05,
      "loss": 0.6836,
      "step": 1936900
    },
    {
      "epoch": 17.67464778450982,
      "grad_norm": 3.2450246810913086,
      "learning_rate": 3.527112684624182e-05,
      "loss": 0.6511,
      "step": 1937000
    },
    {
      "epoch": 17.675560259872984,
      "grad_norm": 5.681524276733398,
      "learning_rate": 3.527036645010585e-05,
      "loss": 0.6425,
      "step": 1937100
    },
    {
      "epoch": 17.67647273523615,
      "grad_norm": 3.5669922828674316,
      "learning_rate": 3.526960605396988e-05,
      "loss": 0.6829,
      "step": 1937200
    },
    {
      "epoch": 17.677385210599315,
      "grad_norm": 3.3381643295288086,
      "learning_rate": 3.52688456578339e-05,
      "loss": 0.6525,
      "step": 1937300
    },
    {
      "epoch": 17.67829768596248,
      "grad_norm": 3.8726677894592285,
      "learning_rate": 3.526808526169794e-05,
      "loss": 0.6453,
      "step": 1937400
    },
    {
      "epoch": 17.679210161325646,
      "grad_norm": 2.904695749282837,
      "learning_rate": 3.526732486556196e-05,
      "loss": 0.6901,
      "step": 1937500
    },
    {
      "epoch": 17.68012263668881,
      "grad_norm": 4.46962308883667,
      "learning_rate": 3.526656446942599e-05,
      "loss": 0.6458,
      "step": 1937600
    },
    {
      "epoch": 17.681035112051976,
      "grad_norm": 3.074659585952759,
      "learning_rate": 3.526580407329002e-05,
      "loss": 0.6643,
      "step": 1937700
    },
    {
      "epoch": 17.68194758741514,
      "grad_norm": 3.7062814235687256,
      "learning_rate": 3.526504367715405e-05,
      "loss": 0.6766,
      "step": 1937800
    },
    {
      "epoch": 17.682860062778303,
      "grad_norm": 4.331627368927002,
      "learning_rate": 3.526428328101808e-05,
      "loss": 0.6812,
      "step": 1937900
    },
    {
      "epoch": 17.68377253814147,
      "grad_norm": 4.629327774047852,
      "learning_rate": 3.526352288488211e-05,
      "loss": 0.6512,
      "step": 1938000
    },
    {
      "epoch": 17.684685013504634,
      "grad_norm": 3.580552816390991,
      "learning_rate": 3.5262762488746135e-05,
      "loss": 0.6753,
      "step": 1938100
    },
    {
      "epoch": 17.6855974888678,
      "grad_norm": 3.16974139213562,
      "learning_rate": 3.526200209261017e-05,
      "loss": 0.6579,
      "step": 1938200
    },
    {
      "epoch": 17.686509964230964,
      "grad_norm": 4.412785053253174,
      "learning_rate": 3.5261241696474195e-05,
      "loss": 0.684,
      "step": 1938300
    },
    {
      "epoch": 17.68742243959413,
      "grad_norm": 3.731236457824707,
      "learning_rate": 3.5260481300338225e-05,
      "loss": 0.6486,
      "step": 1938400
    },
    {
      "epoch": 17.688334914957295,
      "grad_norm": 4.1231465339660645,
      "learning_rate": 3.5259720904202256e-05,
      "loss": 0.6475,
      "step": 1938500
    },
    {
      "epoch": 17.68924739032046,
      "grad_norm": 4.052661895751953,
      "learning_rate": 3.5258960508066286e-05,
      "loss": 0.6707,
      "step": 1938600
    },
    {
      "epoch": 17.690159865683626,
      "grad_norm": 4.149877071380615,
      "learning_rate": 3.525820011193031e-05,
      "loss": 0.6778,
      "step": 1938700
    },
    {
      "epoch": 17.69107234104679,
      "grad_norm": 3.72009539604187,
      "learning_rate": 3.5257439715794346e-05,
      "loss": 0.6678,
      "step": 1938800
    },
    {
      "epoch": 17.691984816409956,
      "grad_norm": 3.852811813354492,
      "learning_rate": 3.525667931965837e-05,
      "loss": 0.66,
      "step": 1938900
    },
    {
      "epoch": 17.69289729177312,
      "grad_norm": 3.958378553390503,
      "learning_rate": 3.52559189235224e-05,
      "loss": 0.6904,
      "step": 1939000
    },
    {
      "epoch": 17.693809767136287,
      "grad_norm": 4.074651718139648,
      "learning_rate": 3.525515852738643e-05,
      "loss": 0.6627,
      "step": 1939100
    },
    {
      "epoch": 17.694722242499452,
      "grad_norm": 4.0922112464904785,
      "learning_rate": 3.525439813125046e-05,
      "loss": 0.6781,
      "step": 1939200
    },
    {
      "epoch": 17.695634717862617,
      "grad_norm": 4.598779678344727,
      "learning_rate": 3.525363773511449e-05,
      "loss": 0.6439,
      "step": 1939300
    },
    {
      "epoch": 17.696547193225783,
      "grad_norm": 3.4397106170654297,
      "learning_rate": 3.525287733897852e-05,
      "loss": 0.6493,
      "step": 1939400
    },
    {
      "epoch": 17.697459668588948,
      "grad_norm": 3.689148426055908,
      "learning_rate": 3.525211694284254e-05,
      "loss": 0.7011,
      "step": 1939500
    },
    {
      "epoch": 17.698372143952113,
      "grad_norm": 4.039477348327637,
      "learning_rate": 3.525135654670658e-05,
      "loss": 0.6945,
      "step": 1939600
    },
    {
      "epoch": 17.69928461931528,
      "grad_norm": 2.8638839721679688,
      "learning_rate": 3.52505961505706e-05,
      "loss": 0.6357,
      "step": 1939700
    },
    {
      "epoch": 17.700197094678444,
      "grad_norm": 3.7527990341186523,
      "learning_rate": 3.524983575443463e-05,
      "loss": 0.6122,
      "step": 1939800
    },
    {
      "epoch": 17.70110957004161,
      "grad_norm": 3.6595277786254883,
      "learning_rate": 3.524907535829866e-05,
      "loss": 0.677,
      "step": 1939900
    },
    {
      "epoch": 17.702022045404775,
      "grad_norm": 3.81643009185791,
      "learning_rate": 3.5248314962162686e-05,
      "loss": 0.6611,
      "step": 1940000
    },
    {
      "epoch": 17.70293452076794,
      "grad_norm": 3.4426889419555664,
      "learning_rate": 3.5247554566026716e-05,
      "loss": 0.646,
      "step": 1940100
    },
    {
      "epoch": 17.703846996131105,
      "grad_norm": 3.2265419960021973,
      "learning_rate": 3.5246794169890746e-05,
      "loss": 0.6742,
      "step": 1940200
    },
    {
      "epoch": 17.70475947149427,
      "grad_norm": 3.7022297382354736,
      "learning_rate": 3.5246033773754776e-05,
      "loss": 0.707,
      "step": 1940300
    },
    {
      "epoch": 17.705671946857436,
      "grad_norm": 3.675410032272339,
      "learning_rate": 3.5245273377618806e-05,
      "loss": 0.6404,
      "step": 1940400
    },
    {
      "epoch": 17.7065844222206,
      "grad_norm": 3.7230236530303955,
      "learning_rate": 3.5244512981482837e-05,
      "loss": 0.6632,
      "step": 1940500
    },
    {
      "epoch": 17.707496897583766,
      "grad_norm": 3.5599825382232666,
      "learning_rate": 3.524375258534686e-05,
      "loss": 0.6722,
      "step": 1940600
    },
    {
      "epoch": 17.70840937294693,
      "grad_norm": 3.9540512561798096,
      "learning_rate": 3.52429921892109e-05,
      "loss": 0.6878,
      "step": 1940700
    },
    {
      "epoch": 17.709321848310097,
      "grad_norm": 4.085270404815674,
      "learning_rate": 3.524223179307492e-05,
      "loss": 0.6349,
      "step": 1940800
    },
    {
      "epoch": 17.710234323673262,
      "grad_norm": 3.902719259262085,
      "learning_rate": 3.524147139693895e-05,
      "loss": 0.6372,
      "step": 1940900
    },
    {
      "epoch": 17.711146799036428,
      "grad_norm": 4.137360572814941,
      "learning_rate": 3.524071100080298e-05,
      "loss": 0.6601,
      "step": 1941000
    },
    {
      "epoch": 17.712059274399593,
      "grad_norm": 4.1526570320129395,
      "learning_rate": 3.523995060466701e-05,
      "loss": 0.6588,
      "step": 1941100
    },
    {
      "epoch": 17.712971749762758,
      "grad_norm": 5.219884395599365,
      "learning_rate": 3.5239190208531033e-05,
      "loss": 0.6728,
      "step": 1941200
    },
    {
      "epoch": 17.713884225125923,
      "grad_norm": 4.128304481506348,
      "learning_rate": 3.523842981239507e-05,
      "loss": 0.6736,
      "step": 1941300
    },
    {
      "epoch": 17.714796700489085,
      "grad_norm": 3.3147823810577393,
      "learning_rate": 3.5237669416259094e-05,
      "loss": 0.6322,
      "step": 1941400
    },
    {
      "epoch": 17.71570917585225,
      "grad_norm": 4.490541458129883,
      "learning_rate": 3.5236909020123124e-05,
      "loss": 0.6592,
      "step": 1941500
    },
    {
      "epoch": 17.716621651215416,
      "grad_norm": 3.271491765975952,
      "learning_rate": 3.5236148623987154e-05,
      "loss": 0.6729,
      "step": 1941600
    },
    {
      "epoch": 17.71753412657858,
      "grad_norm": 3.2245659828186035,
      "learning_rate": 3.5235388227851184e-05,
      "loss": 0.6867,
      "step": 1941700
    },
    {
      "epoch": 17.718446601941746,
      "grad_norm": 3.6354119777679443,
      "learning_rate": 3.5234627831715214e-05,
      "loss": 0.6706,
      "step": 1941800
    },
    {
      "epoch": 17.71935907730491,
      "grad_norm": 3.4353201389312744,
      "learning_rate": 3.5233867435579244e-05,
      "loss": 0.7246,
      "step": 1941900
    },
    {
      "epoch": 17.720271552668077,
      "grad_norm": 4.234534740447998,
      "learning_rate": 3.523310703944327e-05,
      "loss": 0.6542,
      "step": 1942000
    },
    {
      "epoch": 17.721184028031242,
      "grad_norm": 3.818709135055542,
      "learning_rate": 3.5232346643307304e-05,
      "loss": 0.647,
      "step": 1942100
    },
    {
      "epoch": 17.722096503394408,
      "grad_norm": 4.229571342468262,
      "learning_rate": 3.523158624717133e-05,
      "loss": 0.6344,
      "step": 1942200
    },
    {
      "epoch": 17.723008978757573,
      "grad_norm": 4.000092506408691,
      "learning_rate": 3.523082585103536e-05,
      "loss": 0.6732,
      "step": 1942300
    },
    {
      "epoch": 17.723921454120738,
      "grad_norm": 4.029375076293945,
      "learning_rate": 3.523006545489939e-05,
      "loss": 0.674,
      "step": 1942400
    },
    {
      "epoch": 17.724833929483903,
      "grad_norm": 4.105138301849365,
      "learning_rate": 3.522930505876342e-05,
      "loss": 0.659,
      "step": 1942500
    },
    {
      "epoch": 17.72574640484707,
      "grad_norm": 3.2587084770202637,
      "learning_rate": 3.522854466262744e-05,
      "loss": 0.6437,
      "step": 1942600
    },
    {
      "epoch": 17.726658880210234,
      "grad_norm": 3.9286723136901855,
      "learning_rate": 3.522778426649147e-05,
      "loss": 0.7003,
      "step": 1942700
    },
    {
      "epoch": 17.7275713555734,
      "grad_norm": 3.2281129360198975,
      "learning_rate": 3.52270238703555e-05,
      "loss": 0.6722,
      "step": 1942800
    },
    {
      "epoch": 17.728483830936565,
      "grad_norm": 4.279669284820557,
      "learning_rate": 3.522626347421953e-05,
      "loss": 0.6844,
      "step": 1942900
    },
    {
      "epoch": 17.72939630629973,
      "grad_norm": 3.8004581928253174,
      "learning_rate": 3.522550307808356e-05,
      "loss": 0.6618,
      "step": 1943000
    },
    {
      "epoch": 17.730308781662895,
      "grad_norm": 3.770415782928467,
      "learning_rate": 3.5224742681947584e-05,
      "loss": 0.6412,
      "step": 1943100
    },
    {
      "epoch": 17.73122125702606,
      "grad_norm": 4.020053863525391,
      "learning_rate": 3.522398228581162e-05,
      "loss": 0.685,
      "step": 1943200
    },
    {
      "epoch": 17.732133732389226,
      "grad_norm": 4.5264573097229,
      "learning_rate": 3.5223221889675645e-05,
      "loss": 0.6635,
      "step": 1943300
    },
    {
      "epoch": 17.73304620775239,
      "grad_norm": 4.956949710845947,
      "learning_rate": 3.5222461493539675e-05,
      "loss": 0.6608,
      "step": 1943400
    },
    {
      "epoch": 17.733958683115556,
      "grad_norm": 4.071403980255127,
      "learning_rate": 3.5221701097403705e-05,
      "loss": 0.6723,
      "step": 1943500
    },
    {
      "epoch": 17.73487115847872,
      "grad_norm": 4.633077144622803,
      "learning_rate": 3.5220940701267735e-05,
      "loss": 0.6463,
      "step": 1943600
    },
    {
      "epoch": 17.735783633841887,
      "grad_norm": 4.071625709533691,
      "learning_rate": 3.522018030513176e-05,
      "loss": 0.6578,
      "step": 1943700
    },
    {
      "epoch": 17.736696109205052,
      "grad_norm": 3.5563268661499023,
      "learning_rate": 3.5219419908995795e-05,
      "loss": 0.6277,
      "step": 1943800
    },
    {
      "epoch": 17.737608584568218,
      "grad_norm": 4.869225978851318,
      "learning_rate": 3.521865951285982e-05,
      "loss": 0.6723,
      "step": 1943900
    },
    {
      "epoch": 17.738521059931383,
      "grad_norm": 3.1463711261749268,
      "learning_rate": 3.521789911672385e-05,
      "loss": 0.6256,
      "step": 1944000
    },
    {
      "epoch": 17.73943353529455,
      "grad_norm": 3.433183193206787,
      "learning_rate": 3.521713872058788e-05,
      "loss": 0.6797,
      "step": 1944100
    },
    {
      "epoch": 17.740346010657714,
      "grad_norm": 3.9999732971191406,
      "learning_rate": 3.521637832445191e-05,
      "loss": 0.6442,
      "step": 1944200
    },
    {
      "epoch": 17.74125848602088,
      "grad_norm": 3.9753921031951904,
      "learning_rate": 3.521561792831594e-05,
      "loss": 0.6615,
      "step": 1944300
    },
    {
      "epoch": 17.742170961384044,
      "grad_norm": 3.250868320465088,
      "learning_rate": 3.521485753217997e-05,
      "loss": 0.643,
      "step": 1944400
    },
    {
      "epoch": 17.74308343674721,
      "grad_norm": 4.0413031578063965,
      "learning_rate": 3.521409713604399e-05,
      "loss": 0.6882,
      "step": 1944500
    },
    {
      "epoch": 17.74399591211037,
      "grad_norm": 3.6111490726470947,
      "learning_rate": 3.521333673990803e-05,
      "loss": 0.6632,
      "step": 1944600
    },
    {
      "epoch": 17.744908387473536,
      "grad_norm": 4.904956340789795,
      "learning_rate": 3.521257634377205e-05,
      "loss": 0.6577,
      "step": 1944700
    },
    {
      "epoch": 17.745820862836702,
      "grad_norm": 3.864288568496704,
      "learning_rate": 3.521181594763608e-05,
      "loss": 0.6096,
      "step": 1944800
    },
    {
      "epoch": 17.746733338199867,
      "grad_norm": 3.3174538612365723,
      "learning_rate": 3.521105555150011e-05,
      "loss": 0.6172,
      "step": 1944900
    },
    {
      "epoch": 17.747645813563032,
      "grad_norm": 3.714111089706421,
      "learning_rate": 3.521029515536414e-05,
      "loss": 0.6683,
      "step": 1945000
    },
    {
      "epoch": 17.748558288926198,
      "grad_norm": 3.9320626258850098,
      "learning_rate": 3.5209534759228165e-05,
      "loss": 0.6677,
      "step": 1945100
    },
    {
      "epoch": 17.749470764289363,
      "grad_norm": 4.118247985839844,
      "learning_rate": 3.52087743630922e-05,
      "loss": 0.6912,
      "step": 1945200
    },
    {
      "epoch": 17.75038323965253,
      "grad_norm": 3.7893333435058594,
      "learning_rate": 3.5208013966956226e-05,
      "loss": 0.6539,
      "step": 1945300
    },
    {
      "epoch": 17.751295715015694,
      "grad_norm": 4.481799602508545,
      "learning_rate": 3.5207253570820256e-05,
      "loss": 0.6793,
      "step": 1945400
    },
    {
      "epoch": 17.75220819037886,
      "grad_norm": 3.5975372791290283,
      "learning_rate": 3.5206493174684286e-05,
      "loss": 0.6593,
      "step": 1945500
    },
    {
      "epoch": 17.753120665742024,
      "grad_norm": 4.042518615722656,
      "learning_rate": 3.520573277854831e-05,
      "loss": 0.6329,
      "step": 1945600
    },
    {
      "epoch": 17.75403314110519,
      "grad_norm": 3.0597259998321533,
      "learning_rate": 3.5204972382412346e-05,
      "loss": 0.6796,
      "step": 1945700
    },
    {
      "epoch": 17.754945616468355,
      "grad_norm": 4.14574670791626,
      "learning_rate": 3.520421198627637e-05,
      "loss": 0.6591,
      "step": 1945800
    },
    {
      "epoch": 17.75585809183152,
      "grad_norm": 3.504512071609497,
      "learning_rate": 3.52034515901404e-05,
      "loss": 0.6678,
      "step": 1945900
    },
    {
      "epoch": 17.756770567194685,
      "grad_norm": 4.530949592590332,
      "learning_rate": 3.520269119400443e-05,
      "loss": 0.6605,
      "step": 1946000
    },
    {
      "epoch": 17.75768304255785,
      "grad_norm": 4.0081868171691895,
      "learning_rate": 3.520193079786846e-05,
      "loss": 0.6683,
      "step": 1946100
    },
    {
      "epoch": 17.758595517921016,
      "grad_norm": 3.8353354930877686,
      "learning_rate": 3.520117040173249e-05,
      "loss": 0.6893,
      "step": 1946200
    },
    {
      "epoch": 17.75950799328418,
      "grad_norm": 3.188438653945923,
      "learning_rate": 3.520041000559652e-05,
      "loss": 0.6883,
      "step": 1946300
    },
    {
      "epoch": 17.760420468647347,
      "grad_norm": 3.644376516342163,
      "learning_rate": 3.519964960946054e-05,
      "loss": 0.7127,
      "step": 1946400
    },
    {
      "epoch": 17.761332944010512,
      "grad_norm": 4.044541835784912,
      "learning_rate": 3.519888921332458e-05,
      "loss": 0.68,
      "step": 1946500
    },
    {
      "epoch": 17.762245419373677,
      "grad_norm": 3.181868553161621,
      "learning_rate": 3.51981288171886e-05,
      "loss": 0.6381,
      "step": 1946600
    },
    {
      "epoch": 17.763157894736842,
      "grad_norm": 4.156589031219482,
      "learning_rate": 3.519736842105263e-05,
      "loss": 0.6669,
      "step": 1946700
    },
    {
      "epoch": 17.764070370100008,
      "grad_norm": 4.396716594696045,
      "learning_rate": 3.519660802491666e-05,
      "loss": 0.6675,
      "step": 1946800
    },
    {
      "epoch": 17.764982845463173,
      "grad_norm": 3.9361889362335205,
      "learning_rate": 3.519584762878069e-05,
      "loss": 0.6777,
      "step": 1946900
    },
    {
      "epoch": 17.76589532082634,
      "grad_norm": 3.570327043533325,
      "learning_rate": 3.5195087232644716e-05,
      "loss": 0.6905,
      "step": 1947000
    },
    {
      "epoch": 17.766807796189504,
      "grad_norm": 3.9602444171905518,
      "learning_rate": 3.519432683650875e-05,
      "loss": 0.7131,
      "step": 1947100
    },
    {
      "epoch": 17.76772027155267,
      "grad_norm": 4.15000581741333,
      "learning_rate": 3.5193566440372777e-05,
      "loss": 0.6602,
      "step": 1947200
    },
    {
      "epoch": 17.768632746915834,
      "grad_norm": 4.232403755187988,
      "learning_rate": 3.5192806044236807e-05,
      "loss": 0.6629,
      "step": 1947300
    },
    {
      "epoch": 17.769545222279,
      "grad_norm": 4.191430568695068,
      "learning_rate": 3.519204564810084e-05,
      "loss": 0.6333,
      "step": 1947400
    },
    {
      "epoch": 17.770457697642165,
      "grad_norm": 2.576395273208618,
      "learning_rate": 3.519128525196487e-05,
      "loss": 0.6574,
      "step": 1947500
    },
    {
      "epoch": 17.77137017300533,
      "grad_norm": 4.035387992858887,
      "learning_rate": 3.51905248558289e-05,
      "loss": 0.6521,
      "step": 1947600
    },
    {
      "epoch": 17.772282648368495,
      "grad_norm": 4.70933723449707,
      "learning_rate": 3.518976445969293e-05,
      "loss": 0.7325,
      "step": 1947700
    },
    {
      "epoch": 17.77319512373166,
      "grad_norm": 3.3647708892822266,
      "learning_rate": 3.518900406355695e-05,
      "loss": 0.6597,
      "step": 1947800
    },
    {
      "epoch": 17.774107599094826,
      "grad_norm": 4.273189544677734,
      "learning_rate": 3.518824366742099e-05,
      "loss": 0.6663,
      "step": 1947900
    },
    {
      "epoch": 17.77502007445799,
      "grad_norm": 4.710216522216797,
      "learning_rate": 3.518748327128501e-05,
      "loss": 0.6517,
      "step": 1948000
    },
    {
      "epoch": 17.775932549821153,
      "grad_norm": 4.248786926269531,
      "learning_rate": 3.518672287514904e-05,
      "loss": 0.6554,
      "step": 1948100
    },
    {
      "epoch": 17.77684502518432,
      "grad_norm": 2.885593891143799,
      "learning_rate": 3.518596247901307e-05,
      "loss": 0.6539,
      "step": 1948200
    },
    {
      "epoch": 17.777757500547484,
      "grad_norm": 4.5555949211120605,
      "learning_rate": 3.51852020828771e-05,
      "loss": 0.6659,
      "step": 1948300
    },
    {
      "epoch": 17.77866997591065,
      "grad_norm": 4.3126444816589355,
      "learning_rate": 3.5184441686741124e-05,
      "loss": 0.673,
      "step": 1948400
    },
    {
      "epoch": 17.779582451273814,
      "grad_norm": 4.817412376403809,
      "learning_rate": 3.5183681290605154e-05,
      "loss": 0.6603,
      "step": 1948500
    },
    {
      "epoch": 17.78049492663698,
      "grad_norm": 3.8738715648651123,
      "learning_rate": 3.5182920894469184e-05,
      "loss": 0.64,
      "step": 1948600
    },
    {
      "epoch": 17.781407402000145,
      "grad_norm": 4.004903316497803,
      "learning_rate": 3.5182160498333214e-05,
      "loss": 0.696,
      "step": 1948700
    },
    {
      "epoch": 17.78231987736331,
      "grad_norm": 4.342950344085693,
      "learning_rate": 3.5181400102197244e-05,
      "loss": 0.6634,
      "step": 1948800
    },
    {
      "epoch": 17.783232352726476,
      "grad_norm": 4.104641914367676,
      "learning_rate": 3.518063970606127e-05,
      "loss": 0.6417,
      "step": 1948900
    },
    {
      "epoch": 17.78414482808964,
      "grad_norm": 5.144979000091553,
      "learning_rate": 3.5179879309925304e-05,
      "loss": 0.6577,
      "step": 1949000
    },
    {
      "epoch": 17.785057303452806,
      "grad_norm": 4.365946292877197,
      "learning_rate": 3.517911891378933e-05,
      "loss": 0.669,
      "step": 1949100
    },
    {
      "epoch": 17.78596977881597,
      "grad_norm": 3.9545066356658936,
      "learning_rate": 3.517835851765336e-05,
      "loss": 0.664,
      "step": 1949200
    },
    {
      "epoch": 17.786882254179137,
      "grad_norm": 3.158440113067627,
      "learning_rate": 3.517759812151739e-05,
      "loss": 0.6673,
      "step": 1949300
    },
    {
      "epoch": 17.787794729542302,
      "grad_norm": 4.001377582550049,
      "learning_rate": 3.517683772538142e-05,
      "loss": 0.6617,
      "step": 1949400
    },
    {
      "epoch": 17.788707204905467,
      "grad_norm": 3.981607675552368,
      "learning_rate": 3.517607732924544e-05,
      "loss": 0.6732,
      "step": 1949500
    },
    {
      "epoch": 17.789619680268633,
      "grad_norm": 5.127203941345215,
      "learning_rate": 3.517531693310948e-05,
      "loss": 0.6187,
      "step": 1949600
    },
    {
      "epoch": 17.790532155631798,
      "grad_norm": 2.9903368949890137,
      "learning_rate": 3.51745565369735e-05,
      "loss": 0.6448,
      "step": 1949700
    },
    {
      "epoch": 17.791444630994963,
      "grad_norm": 1.9597723484039307,
      "learning_rate": 3.517379614083753e-05,
      "loss": 0.6691,
      "step": 1949800
    },
    {
      "epoch": 17.79235710635813,
      "grad_norm": 3.5021862983703613,
      "learning_rate": 3.517303574470156e-05,
      "loss": 0.6355,
      "step": 1949900
    },
    {
      "epoch": 17.793269581721294,
      "grad_norm": 3.5935893058776855,
      "learning_rate": 3.517227534856559e-05,
      "loss": 0.6654,
      "step": 1950000
    },
    {
      "epoch": 17.79418205708446,
      "grad_norm": 4.415995121002197,
      "learning_rate": 3.517151495242962e-05,
      "loss": 0.6781,
      "step": 1950100
    },
    {
      "epoch": 17.795094532447624,
      "grad_norm": 4.172642707824707,
      "learning_rate": 3.517075455629365e-05,
      "loss": 0.6912,
      "step": 1950200
    },
    {
      "epoch": 17.79600700781079,
      "grad_norm": 3.897041082382202,
      "learning_rate": 3.5169994160157675e-05,
      "loss": 0.642,
      "step": 1950300
    },
    {
      "epoch": 17.796919483173955,
      "grad_norm": 3.789052724838257,
      "learning_rate": 3.516923376402171e-05,
      "loss": 0.668,
      "step": 1950400
    },
    {
      "epoch": 17.79783195853712,
      "grad_norm": 3.1789331436157227,
      "learning_rate": 3.5168473367885735e-05,
      "loss": 0.6345,
      "step": 1950500
    },
    {
      "epoch": 17.798744433900286,
      "grad_norm": 4.3162641525268555,
      "learning_rate": 3.5167712971749765e-05,
      "loss": 0.693,
      "step": 1950600
    },
    {
      "epoch": 17.79965690926345,
      "grad_norm": 2.5862925052642822,
      "learning_rate": 3.5166952575613795e-05,
      "loss": 0.6692,
      "step": 1950700
    },
    {
      "epoch": 17.800569384626616,
      "grad_norm": 4.781167984008789,
      "learning_rate": 3.5166192179477825e-05,
      "loss": 0.6481,
      "step": 1950800
    },
    {
      "epoch": 17.80148185998978,
      "grad_norm": 4.763978481292725,
      "learning_rate": 3.516543178334185e-05,
      "loss": 0.6663,
      "step": 1950900
    },
    {
      "epoch": 17.802394335352947,
      "grad_norm": 4.213461399078369,
      "learning_rate": 3.5164671387205885e-05,
      "loss": 0.6853,
      "step": 1951000
    },
    {
      "epoch": 17.803306810716112,
      "grad_norm": 4.341818809509277,
      "learning_rate": 3.516391099106991e-05,
      "loss": 0.6625,
      "step": 1951100
    },
    {
      "epoch": 17.804219286079277,
      "grad_norm": 3.491971492767334,
      "learning_rate": 3.516315059493394e-05,
      "loss": 0.6677,
      "step": 1951200
    },
    {
      "epoch": 17.805131761442443,
      "grad_norm": 4.090272903442383,
      "learning_rate": 3.516239019879797e-05,
      "loss": 0.6705,
      "step": 1951300
    },
    {
      "epoch": 17.806044236805604,
      "grad_norm": 3.4993374347686768,
      "learning_rate": 3.516162980266199e-05,
      "loss": 0.682,
      "step": 1951400
    },
    {
      "epoch": 17.80695671216877,
      "grad_norm": 3.605424165725708,
      "learning_rate": 3.516086940652603e-05,
      "loss": 0.6662,
      "step": 1951500
    },
    {
      "epoch": 17.807869187531935,
      "grad_norm": 4.5102386474609375,
      "learning_rate": 3.516010901039005e-05,
      "loss": 0.6619,
      "step": 1951600
    },
    {
      "epoch": 17.8087816628951,
      "grad_norm": 3.7896409034729004,
      "learning_rate": 3.515934861425408e-05,
      "loss": 0.6914,
      "step": 1951700
    },
    {
      "epoch": 17.809694138258266,
      "grad_norm": 3.6333940029144287,
      "learning_rate": 3.515858821811811e-05,
      "loss": 0.741,
      "step": 1951800
    },
    {
      "epoch": 17.81060661362143,
      "grad_norm": 4.4354567527771,
      "learning_rate": 3.515782782198214e-05,
      "loss": 0.6734,
      "step": 1951900
    },
    {
      "epoch": 17.811519088984596,
      "grad_norm": 3.8767430782318115,
      "learning_rate": 3.5157067425846166e-05,
      "loss": 0.6566,
      "step": 1952000
    },
    {
      "epoch": 17.81243156434776,
      "grad_norm": 3.68093204498291,
      "learning_rate": 3.51563070297102e-05,
      "loss": 0.677,
      "step": 1952100
    },
    {
      "epoch": 17.813344039710927,
      "grad_norm": 3.4989967346191406,
      "learning_rate": 3.5155546633574226e-05,
      "loss": 0.615,
      "step": 1952200
    },
    {
      "epoch": 17.814256515074092,
      "grad_norm": 3.1662514209747314,
      "learning_rate": 3.5154786237438256e-05,
      "loss": 0.6772,
      "step": 1952300
    },
    {
      "epoch": 17.815168990437257,
      "grad_norm": 4.252547740936279,
      "learning_rate": 3.5154025841302286e-05,
      "loss": 0.6599,
      "step": 1952400
    },
    {
      "epoch": 17.816081465800423,
      "grad_norm": 4.246352672576904,
      "learning_rate": 3.5153265445166316e-05,
      "loss": 0.6592,
      "step": 1952500
    },
    {
      "epoch": 17.816993941163588,
      "grad_norm": 4.026740074157715,
      "learning_rate": 3.5152505049030346e-05,
      "loss": 0.6489,
      "step": 1952600
    },
    {
      "epoch": 17.817906416526753,
      "grad_norm": 3.979694366455078,
      "learning_rate": 3.5151744652894376e-05,
      "loss": 0.7116,
      "step": 1952700
    },
    {
      "epoch": 17.81881889188992,
      "grad_norm": 4.127474784851074,
      "learning_rate": 3.51509842567584e-05,
      "loss": 0.6587,
      "step": 1952800
    },
    {
      "epoch": 17.819731367253084,
      "grad_norm": 3.976038932800293,
      "learning_rate": 3.5150223860622436e-05,
      "loss": 0.6785,
      "step": 1952900
    },
    {
      "epoch": 17.82064384261625,
      "grad_norm": 5.192864418029785,
      "learning_rate": 3.514946346448646e-05,
      "loss": 0.644,
      "step": 1953000
    },
    {
      "epoch": 17.821556317979415,
      "grad_norm": 3.7930808067321777,
      "learning_rate": 3.514870306835049e-05,
      "loss": 0.6907,
      "step": 1953100
    },
    {
      "epoch": 17.82246879334258,
      "grad_norm": 4.389439105987549,
      "learning_rate": 3.514794267221452e-05,
      "loss": 0.629,
      "step": 1953200
    },
    {
      "epoch": 17.823381268705745,
      "grad_norm": 3.592794895172119,
      "learning_rate": 3.514718227607855e-05,
      "loss": 0.6575,
      "step": 1953300
    },
    {
      "epoch": 17.82429374406891,
      "grad_norm": 4.109294891357422,
      "learning_rate": 3.514642187994257e-05,
      "loss": 0.671,
      "step": 1953400
    },
    {
      "epoch": 17.825206219432076,
      "grad_norm": 4.038992404937744,
      "learning_rate": 3.514566148380661e-05,
      "loss": 0.6855,
      "step": 1953500
    },
    {
      "epoch": 17.82611869479524,
      "grad_norm": 2.55005145072937,
      "learning_rate": 3.514490108767063e-05,
      "loss": 0.6628,
      "step": 1953600
    },
    {
      "epoch": 17.827031170158406,
      "grad_norm": 4.480920314788818,
      "learning_rate": 3.514414069153466e-05,
      "loss": 0.6711,
      "step": 1953700
    },
    {
      "epoch": 17.82794364552157,
      "grad_norm": 4.230335712432861,
      "learning_rate": 3.514338029539869e-05,
      "loss": 0.6924,
      "step": 1953800
    },
    {
      "epoch": 17.828856120884737,
      "grad_norm": 4.087033271789551,
      "learning_rate": 3.514261989926272e-05,
      "loss": 0.6479,
      "step": 1953900
    },
    {
      "epoch": 17.829768596247902,
      "grad_norm": 4.362547874450684,
      "learning_rate": 3.514185950312675e-05,
      "loss": 0.6577,
      "step": 1954000
    },
    {
      "epoch": 17.830681071611068,
      "grad_norm": 2.758101463317871,
      "learning_rate": 3.514109910699078e-05,
      "loss": 0.6722,
      "step": 1954100
    },
    {
      "epoch": 17.831593546974233,
      "grad_norm": 3.772437334060669,
      "learning_rate": 3.514033871085481e-05,
      "loss": 0.6592,
      "step": 1954200
    },
    {
      "epoch": 17.832506022337398,
      "grad_norm": 3.3818676471710205,
      "learning_rate": 3.513957831471884e-05,
      "loss": 0.6601,
      "step": 1954300
    },
    {
      "epoch": 17.833418497700563,
      "grad_norm": 4.697454929351807,
      "learning_rate": 3.513881791858287e-05,
      "loss": 0.6543,
      "step": 1954400
    },
    {
      "epoch": 17.83433097306373,
      "grad_norm": 4.051504611968994,
      "learning_rate": 3.513805752244689e-05,
      "loss": 0.6588,
      "step": 1954500
    },
    {
      "epoch": 17.835243448426894,
      "grad_norm": 4.019482135772705,
      "learning_rate": 3.513729712631093e-05,
      "loss": 0.6823,
      "step": 1954600
    },
    {
      "epoch": 17.83615592379006,
      "grad_norm": 4.807455062866211,
      "learning_rate": 3.513653673017495e-05,
      "loss": 0.6838,
      "step": 1954700
    },
    {
      "epoch": 17.837068399153225,
      "grad_norm": 3.943932294845581,
      "learning_rate": 3.513577633403898e-05,
      "loss": 0.6917,
      "step": 1954800
    },
    {
      "epoch": 17.837980874516386,
      "grad_norm": 4.896249294281006,
      "learning_rate": 3.513501593790301e-05,
      "loss": 0.6627,
      "step": 1954900
    },
    {
      "epoch": 17.83889334987955,
      "grad_norm": 4.798847198486328,
      "learning_rate": 3.513425554176704e-05,
      "loss": 0.6584,
      "step": 1955000
    },
    {
      "epoch": 17.839805825242717,
      "grad_norm": 4.315969467163086,
      "learning_rate": 3.513349514563107e-05,
      "loss": 0.614,
      "step": 1955100
    },
    {
      "epoch": 17.840718300605882,
      "grad_norm": 3.2711915969848633,
      "learning_rate": 3.51327347494951e-05,
      "loss": 0.6382,
      "step": 1955200
    },
    {
      "epoch": 17.841630775969048,
      "grad_norm": 4.644079685211182,
      "learning_rate": 3.5131974353359124e-05,
      "loss": 0.6507,
      "step": 1955300
    },
    {
      "epoch": 17.842543251332213,
      "grad_norm": 4.088729381561279,
      "learning_rate": 3.513121395722316e-05,
      "loss": 0.6569,
      "step": 1955400
    },
    {
      "epoch": 17.843455726695378,
      "grad_norm": 4.127146244049072,
      "learning_rate": 3.5130453561087184e-05,
      "loss": 0.6494,
      "step": 1955500
    },
    {
      "epoch": 17.844368202058543,
      "grad_norm": 3.5000133514404297,
      "learning_rate": 3.5129693164951214e-05,
      "loss": 0.66,
      "step": 1955600
    },
    {
      "epoch": 17.84528067742171,
      "grad_norm": 5.054481506347656,
      "learning_rate": 3.5128932768815244e-05,
      "loss": 0.6314,
      "step": 1955700
    },
    {
      "epoch": 17.846193152784874,
      "grad_norm": 3.75185227394104,
      "learning_rate": 3.5128172372679274e-05,
      "loss": 0.6588,
      "step": 1955800
    },
    {
      "epoch": 17.84710562814804,
      "grad_norm": 3.9758169651031494,
      "learning_rate": 3.51274119765433e-05,
      "loss": 0.6892,
      "step": 1955900
    },
    {
      "epoch": 17.848018103511205,
      "grad_norm": 4.042299747467041,
      "learning_rate": 3.5126651580407334e-05,
      "loss": 0.6538,
      "step": 1956000
    },
    {
      "epoch": 17.84893057887437,
      "grad_norm": 3.9722402095794678,
      "learning_rate": 3.512589118427136e-05,
      "loss": 0.7049,
      "step": 1956100
    },
    {
      "epoch": 17.849843054237535,
      "grad_norm": 4.930981159210205,
      "learning_rate": 3.512513078813539e-05,
      "loss": 0.6472,
      "step": 1956200
    },
    {
      "epoch": 17.8507555296007,
      "grad_norm": 3.628739595413208,
      "learning_rate": 3.512437039199942e-05,
      "loss": 0.6295,
      "step": 1956300
    },
    {
      "epoch": 17.851668004963866,
      "grad_norm": 3.777430295944214,
      "learning_rate": 3.512360999586345e-05,
      "loss": 0.6849,
      "step": 1956400
    },
    {
      "epoch": 17.85258048032703,
      "grad_norm": 3.5103118419647217,
      "learning_rate": 3.512284959972748e-05,
      "loss": 0.6723,
      "step": 1956500
    },
    {
      "epoch": 17.853492955690196,
      "grad_norm": 3.3397552967071533,
      "learning_rate": 3.512208920359151e-05,
      "loss": 0.6564,
      "step": 1956600
    },
    {
      "epoch": 17.85440543105336,
      "grad_norm": 4.855374813079834,
      "learning_rate": 3.512132880745553e-05,
      "loss": 0.6626,
      "step": 1956700
    },
    {
      "epoch": 17.855317906416527,
      "grad_norm": 3.6986567974090576,
      "learning_rate": 3.512056841131957e-05,
      "loss": 0.6895,
      "step": 1956800
    },
    {
      "epoch": 17.856230381779692,
      "grad_norm": 4.332161903381348,
      "learning_rate": 3.511980801518359e-05,
      "loss": 0.6531,
      "step": 1956900
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 4.824883460998535,
      "learning_rate": 3.511904761904762e-05,
      "loss": 0.6771,
      "step": 1957000
    },
    {
      "epoch": 17.858055332506023,
      "grad_norm": 4.213949680328369,
      "learning_rate": 3.511828722291165e-05,
      "loss": 0.6549,
      "step": 1957100
    },
    {
      "epoch": 17.85896780786919,
      "grad_norm": 3.71453857421875,
      "learning_rate": 3.5117526826775675e-05,
      "loss": 0.6789,
      "step": 1957200
    },
    {
      "epoch": 17.859880283232354,
      "grad_norm": 3.307360887527466,
      "learning_rate": 3.5116766430639705e-05,
      "loss": 0.6481,
      "step": 1957300
    },
    {
      "epoch": 17.86079275859552,
      "grad_norm": 3.494098424911499,
      "learning_rate": 3.5116006034503735e-05,
      "loss": 0.6708,
      "step": 1957400
    },
    {
      "epoch": 17.861705233958684,
      "grad_norm": 3.566248893737793,
      "learning_rate": 3.5115245638367765e-05,
      "loss": 0.6791,
      "step": 1957500
    },
    {
      "epoch": 17.86261770932185,
      "grad_norm": 3.372210741043091,
      "learning_rate": 3.5114485242231795e-05,
      "loss": 0.6602,
      "step": 1957600
    },
    {
      "epoch": 17.863530184685015,
      "grad_norm": 4.621039867401123,
      "learning_rate": 3.5113724846095825e-05,
      "loss": 0.6851,
      "step": 1957700
    },
    {
      "epoch": 17.86444266004818,
      "grad_norm": 2.8312532901763916,
      "learning_rate": 3.511296444995985e-05,
      "loss": 0.7039,
      "step": 1957800
    },
    {
      "epoch": 17.865355135411345,
      "grad_norm": 4.0451555252075195,
      "learning_rate": 3.5112204053823885e-05,
      "loss": 0.6784,
      "step": 1957900
    },
    {
      "epoch": 17.86626761077451,
      "grad_norm": 4.428068161010742,
      "learning_rate": 3.511144365768791e-05,
      "loss": 0.6489,
      "step": 1958000
    },
    {
      "epoch": 17.867180086137676,
      "grad_norm": 3.6698384284973145,
      "learning_rate": 3.511068326155194e-05,
      "loss": 0.696,
      "step": 1958100
    },
    {
      "epoch": 17.868092561500838,
      "grad_norm": 3.9246222972869873,
      "learning_rate": 3.510992286541597e-05,
      "loss": 0.6711,
      "step": 1958200
    },
    {
      "epoch": 17.869005036864003,
      "grad_norm": 4.149569511413574,
      "learning_rate": 3.510916246928e-05,
      "loss": 0.6553,
      "step": 1958300
    },
    {
      "epoch": 17.86991751222717,
      "grad_norm": 4.281075477600098,
      "learning_rate": 3.510840207314403e-05,
      "loss": 0.6529,
      "step": 1958400
    },
    {
      "epoch": 17.870829987590334,
      "grad_norm": 4.60377311706543,
      "learning_rate": 3.510764167700806e-05,
      "loss": 0.6929,
      "step": 1958500
    },
    {
      "epoch": 17.8717424629535,
      "grad_norm": 3.2650413513183594,
      "learning_rate": 3.510688128087208e-05,
      "loss": 0.6551,
      "step": 1958600
    },
    {
      "epoch": 17.872654938316664,
      "grad_norm": 3.388617515563965,
      "learning_rate": 3.510612088473611e-05,
      "loss": 0.6613,
      "step": 1958700
    },
    {
      "epoch": 17.87356741367983,
      "grad_norm": 3.3172342777252197,
      "learning_rate": 3.510536048860014e-05,
      "loss": 0.6423,
      "step": 1958800
    },
    {
      "epoch": 17.874479889042995,
      "grad_norm": 4.266847610473633,
      "learning_rate": 3.510460009246417e-05,
      "loss": 0.6666,
      "step": 1958900
    },
    {
      "epoch": 17.87539236440616,
      "grad_norm": 3.6467337608337402,
      "learning_rate": 3.51038396963282e-05,
      "loss": 0.6578,
      "step": 1959000
    },
    {
      "epoch": 17.876304839769325,
      "grad_norm": 4.0367913246154785,
      "learning_rate": 3.510307930019223e-05,
      "loss": 0.6768,
      "step": 1959100
    },
    {
      "epoch": 17.87721731513249,
      "grad_norm": 4.151163101196289,
      "learning_rate": 3.5102318904056256e-05,
      "loss": 0.6746,
      "step": 1959200
    },
    {
      "epoch": 17.878129790495656,
      "grad_norm": 3.921337842941284,
      "learning_rate": 3.510155850792029e-05,
      "loss": 0.6421,
      "step": 1959300
    },
    {
      "epoch": 17.87904226585882,
      "grad_norm": 5.307011127471924,
      "learning_rate": 3.5100798111784316e-05,
      "loss": 0.6412,
      "step": 1959400
    },
    {
      "epoch": 17.879954741221987,
      "grad_norm": 4.225725173950195,
      "learning_rate": 3.5100037715648346e-05,
      "loss": 0.6688,
      "step": 1959500
    },
    {
      "epoch": 17.880867216585152,
      "grad_norm": 3.93314528465271,
      "learning_rate": 3.5099277319512376e-05,
      "loss": 0.6211,
      "step": 1959600
    },
    {
      "epoch": 17.881779691948317,
      "grad_norm": 4.543482303619385,
      "learning_rate": 3.5098516923376406e-05,
      "loss": 0.6346,
      "step": 1959700
    },
    {
      "epoch": 17.882692167311482,
      "grad_norm": 3.6883134841918945,
      "learning_rate": 3.5097756527240436e-05,
      "loss": 0.6537,
      "step": 1959800
    },
    {
      "epoch": 17.883604642674648,
      "grad_norm": 4.344130039215088,
      "learning_rate": 3.509699613110446e-05,
      "loss": 0.6628,
      "step": 1959900
    },
    {
      "epoch": 17.884517118037813,
      "grad_norm": 3.3675200939178467,
      "learning_rate": 3.509623573496849e-05,
      "loss": 0.6838,
      "step": 1960000
    },
    {
      "epoch": 17.88542959340098,
      "grad_norm": 3.75345778465271,
      "learning_rate": 3.509547533883252e-05,
      "loss": 0.6537,
      "step": 1960100
    },
    {
      "epoch": 17.886342068764144,
      "grad_norm": 4.100133895874023,
      "learning_rate": 3.509471494269655e-05,
      "loss": 0.7064,
      "step": 1960200
    },
    {
      "epoch": 17.88725454412731,
      "grad_norm": 4.121508598327637,
      "learning_rate": 3.509395454656057e-05,
      "loss": 0.6824,
      "step": 1960300
    },
    {
      "epoch": 17.888167019490474,
      "grad_norm": 3.397169828414917,
      "learning_rate": 3.509319415042461e-05,
      "loss": 0.6426,
      "step": 1960400
    },
    {
      "epoch": 17.88907949485364,
      "grad_norm": 3.8835160732269287,
      "learning_rate": 3.509243375428863e-05,
      "loss": 0.672,
      "step": 1960500
    },
    {
      "epoch": 17.889991970216805,
      "grad_norm": 2.6042656898498535,
      "learning_rate": 3.509167335815266e-05,
      "loss": 0.6761,
      "step": 1960600
    },
    {
      "epoch": 17.89090444557997,
      "grad_norm": 2.8231287002563477,
      "learning_rate": 3.509091296201669e-05,
      "loss": 0.6596,
      "step": 1960700
    },
    {
      "epoch": 17.891816920943135,
      "grad_norm": 2.809666633605957,
      "learning_rate": 3.509015256588072e-05,
      "loss": 0.6548,
      "step": 1960800
    },
    {
      "epoch": 17.8927293963063,
      "grad_norm": 3.25052809715271,
      "learning_rate": 3.5089392169744753e-05,
      "loss": 0.6399,
      "step": 1960900
    },
    {
      "epoch": 17.893641871669466,
      "grad_norm": 4.173133373260498,
      "learning_rate": 3.5088631773608783e-05,
      "loss": 0.6298,
      "step": 1961000
    },
    {
      "epoch": 17.89455434703263,
      "grad_norm": 3.9679603576660156,
      "learning_rate": 3.508787137747281e-05,
      "loss": 0.6556,
      "step": 1961100
    },
    {
      "epoch": 17.895466822395797,
      "grad_norm": 4.19664192199707,
      "learning_rate": 3.5087110981336844e-05,
      "loss": 0.6367,
      "step": 1961200
    },
    {
      "epoch": 17.896379297758962,
      "grad_norm": 3.9212324619293213,
      "learning_rate": 3.508635058520087e-05,
      "loss": 0.6444,
      "step": 1961300
    },
    {
      "epoch": 17.897291773122127,
      "grad_norm": 5.061764240264893,
      "learning_rate": 3.50855901890649e-05,
      "loss": 0.6803,
      "step": 1961400
    },
    {
      "epoch": 17.898204248485293,
      "grad_norm": 3.1506431102752686,
      "learning_rate": 3.508482979292893e-05,
      "loss": 0.6945,
      "step": 1961500
    },
    {
      "epoch": 17.899116723848458,
      "grad_norm": 4.5650858879089355,
      "learning_rate": 3.508406939679296e-05,
      "loss": 0.6357,
      "step": 1961600
    },
    {
      "epoch": 17.90002919921162,
      "grad_norm": 3.4319674968719482,
      "learning_rate": 3.508330900065698e-05,
      "loss": 0.6665,
      "step": 1961700
    },
    {
      "epoch": 17.900941674574785,
      "grad_norm": 4.686060905456543,
      "learning_rate": 3.508254860452102e-05,
      "loss": 0.6311,
      "step": 1961800
    },
    {
      "epoch": 17.90185414993795,
      "grad_norm": 3.0329174995422363,
      "learning_rate": 3.508178820838504e-05,
      "loss": 0.6684,
      "step": 1961900
    },
    {
      "epoch": 17.902766625301116,
      "grad_norm": 3.569634199142456,
      "learning_rate": 3.508102781224907e-05,
      "loss": 0.6329,
      "step": 1962000
    },
    {
      "epoch": 17.90367910066428,
      "grad_norm": 4.2246246337890625,
      "learning_rate": 3.50802674161131e-05,
      "loss": 0.647,
      "step": 1962100
    },
    {
      "epoch": 17.904591576027446,
      "grad_norm": 4.49179220199585,
      "learning_rate": 3.507950701997713e-05,
      "loss": 0.6738,
      "step": 1962200
    },
    {
      "epoch": 17.90550405139061,
      "grad_norm": 3.8703675270080566,
      "learning_rate": 3.507874662384116e-05,
      "loss": 0.6502,
      "step": 1962300
    },
    {
      "epoch": 17.906416526753777,
      "grad_norm": 3.07145094871521,
      "learning_rate": 3.507798622770519e-05,
      "loss": 0.6385,
      "step": 1962400
    },
    {
      "epoch": 17.907329002116942,
      "grad_norm": 3.435105800628662,
      "learning_rate": 3.5077225831569214e-05,
      "loss": 0.6503,
      "step": 1962500
    },
    {
      "epoch": 17.908241477480107,
      "grad_norm": 3.120514154434204,
      "learning_rate": 3.5076465435433244e-05,
      "loss": 0.6961,
      "step": 1962600
    },
    {
      "epoch": 17.909153952843273,
      "grad_norm": 4.893608093261719,
      "learning_rate": 3.5075705039297274e-05,
      "loss": 0.6893,
      "step": 1962700
    },
    {
      "epoch": 17.910066428206438,
      "grad_norm": 4.381722927093506,
      "learning_rate": 3.50749446431613e-05,
      "loss": 0.6696,
      "step": 1962800
    },
    {
      "epoch": 17.910978903569603,
      "grad_norm": 4.137636184692383,
      "learning_rate": 3.5074184247025334e-05,
      "loss": 0.668,
      "step": 1962900
    },
    {
      "epoch": 17.91189137893277,
      "grad_norm": 4.1417036056518555,
      "learning_rate": 3.507342385088936e-05,
      "loss": 0.7107,
      "step": 1963000
    },
    {
      "epoch": 17.912803854295934,
      "grad_norm": 3.450408458709717,
      "learning_rate": 3.507266345475339e-05,
      "loss": 0.6352,
      "step": 1963100
    },
    {
      "epoch": 17.9137163296591,
      "grad_norm": 3.9332504272460938,
      "learning_rate": 3.507190305861742e-05,
      "loss": 0.6411,
      "step": 1963200
    },
    {
      "epoch": 17.914628805022264,
      "grad_norm": 4.527175426483154,
      "learning_rate": 3.507114266248145e-05,
      "loss": 0.6436,
      "step": 1963300
    },
    {
      "epoch": 17.91554128038543,
      "grad_norm": 4.311318874359131,
      "learning_rate": 3.507038226634548e-05,
      "loss": 0.6565,
      "step": 1963400
    },
    {
      "epoch": 17.916453755748595,
      "grad_norm": 3.681602716445923,
      "learning_rate": 3.506962187020951e-05,
      "loss": 0.6457,
      "step": 1963500
    },
    {
      "epoch": 17.91736623111176,
      "grad_norm": 3.5933079719543457,
      "learning_rate": 3.506886147407353e-05,
      "loss": 0.6723,
      "step": 1963600
    },
    {
      "epoch": 17.918278706474926,
      "grad_norm": 4.372521877288818,
      "learning_rate": 3.506810107793757e-05,
      "loss": 0.6334,
      "step": 1963700
    },
    {
      "epoch": 17.91919118183809,
      "grad_norm": 3.361689805984497,
      "learning_rate": 3.506734068180159e-05,
      "loss": 0.6663,
      "step": 1963800
    },
    {
      "epoch": 17.920103657201256,
      "grad_norm": 3.3500301837921143,
      "learning_rate": 3.506658028566562e-05,
      "loss": 0.6645,
      "step": 1963900
    },
    {
      "epoch": 17.92101613256442,
      "grad_norm": 3.6274657249450684,
      "learning_rate": 3.506581988952965e-05,
      "loss": 0.657,
      "step": 1964000
    },
    {
      "epoch": 17.921928607927587,
      "grad_norm": 2.9659035205841064,
      "learning_rate": 3.506505949339368e-05,
      "loss": 0.6552,
      "step": 1964100
    },
    {
      "epoch": 17.922841083290752,
      "grad_norm": 4.981869220733643,
      "learning_rate": 3.5064299097257705e-05,
      "loss": 0.6691,
      "step": 1964200
    },
    {
      "epoch": 17.923753558653917,
      "grad_norm": 4.604031562805176,
      "learning_rate": 3.506353870112174e-05,
      "loss": 0.618,
      "step": 1964300
    },
    {
      "epoch": 17.924666034017083,
      "grad_norm": 3.099837303161621,
      "learning_rate": 3.5062778304985765e-05,
      "loss": 0.6554,
      "step": 1964400
    },
    {
      "epoch": 17.925578509380248,
      "grad_norm": 4.162827968597412,
      "learning_rate": 3.5062017908849795e-05,
      "loss": 0.6371,
      "step": 1964500
    },
    {
      "epoch": 17.926490984743413,
      "grad_norm": 3.6106293201446533,
      "learning_rate": 3.5061257512713825e-05,
      "loss": 0.7258,
      "step": 1964600
    },
    {
      "epoch": 17.92740346010658,
      "grad_norm": 3.9966301918029785,
      "learning_rate": 3.5060497116577855e-05,
      "loss": 0.6823,
      "step": 1964700
    },
    {
      "epoch": 17.928315935469744,
      "grad_norm": 4.695626735687256,
      "learning_rate": 3.5059736720441885e-05,
      "loss": 0.6943,
      "step": 1964800
    },
    {
      "epoch": 17.92922841083291,
      "grad_norm": 4.478654861450195,
      "learning_rate": 3.5058976324305915e-05,
      "loss": 0.6686,
      "step": 1964900
    },
    {
      "epoch": 17.93014088619607,
      "grad_norm": 2.632326364517212,
      "learning_rate": 3.505821592816994e-05,
      "loss": 0.694,
      "step": 1965000
    },
    {
      "epoch": 17.931053361559236,
      "grad_norm": 4.11965274810791,
      "learning_rate": 3.5057455532033976e-05,
      "loss": 0.6453,
      "step": 1965100
    },
    {
      "epoch": 17.9319658369224,
      "grad_norm": 3.8876307010650635,
      "learning_rate": 3.5056695135898e-05,
      "loss": 0.6377,
      "step": 1965200
    },
    {
      "epoch": 17.932878312285567,
      "grad_norm": 4.8866047859191895,
      "learning_rate": 3.505593473976203e-05,
      "loss": 0.6279,
      "step": 1965300
    },
    {
      "epoch": 17.933790787648732,
      "grad_norm": 4.991801738739014,
      "learning_rate": 3.505517434362606e-05,
      "loss": 0.6814,
      "step": 1965400
    },
    {
      "epoch": 17.934703263011897,
      "grad_norm": 3.984496593475342,
      "learning_rate": 3.505441394749008e-05,
      "loss": 0.6548,
      "step": 1965500
    },
    {
      "epoch": 17.935615738375063,
      "grad_norm": 3.8160619735717773,
      "learning_rate": 3.505365355135411e-05,
      "loss": 0.6183,
      "step": 1965600
    },
    {
      "epoch": 17.936528213738228,
      "grad_norm": 3.9170639514923096,
      "learning_rate": 3.505289315521814e-05,
      "loss": 0.6457,
      "step": 1965700
    },
    {
      "epoch": 17.937440689101393,
      "grad_norm": 4.73167610168457,
      "learning_rate": 3.505213275908217e-05,
      "loss": 0.6614,
      "step": 1965800
    },
    {
      "epoch": 17.93835316446456,
      "grad_norm": 4.072345733642578,
      "learning_rate": 3.50513723629462e-05,
      "loss": 0.6424,
      "step": 1965900
    },
    {
      "epoch": 17.939265639827724,
      "grad_norm": 2.8969924449920654,
      "learning_rate": 3.505061196681023e-05,
      "loss": 0.6455,
      "step": 1966000
    },
    {
      "epoch": 17.94017811519089,
      "grad_norm": 3.6289355754852295,
      "learning_rate": 3.5049851570674256e-05,
      "loss": 0.6656,
      "step": 1966100
    },
    {
      "epoch": 17.941090590554055,
      "grad_norm": 3.7400448322296143,
      "learning_rate": 3.504909117453829e-05,
      "loss": 0.653,
      "step": 1966200
    },
    {
      "epoch": 17.94200306591722,
      "grad_norm": 3.2735066413879395,
      "learning_rate": 3.5048330778402316e-05,
      "loss": 0.6633,
      "step": 1966300
    },
    {
      "epoch": 17.942915541280385,
      "grad_norm": 4.561068534851074,
      "learning_rate": 3.5047570382266346e-05,
      "loss": 0.6741,
      "step": 1966400
    },
    {
      "epoch": 17.94382801664355,
      "grad_norm": 3.7644402980804443,
      "learning_rate": 3.5046809986130376e-05,
      "loss": 0.6559,
      "step": 1966500
    },
    {
      "epoch": 17.944740492006716,
      "grad_norm": 3.687257766723633,
      "learning_rate": 3.5046049589994406e-05,
      "loss": 0.6908,
      "step": 1966600
    },
    {
      "epoch": 17.94565296736988,
      "grad_norm": 3.5814311504364014,
      "learning_rate": 3.504528919385843e-05,
      "loss": 0.6594,
      "step": 1966700
    },
    {
      "epoch": 17.946565442733046,
      "grad_norm": 4.108476161956787,
      "learning_rate": 3.5044528797722466e-05,
      "loss": 0.6633,
      "step": 1966800
    },
    {
      "epoch": 17.94747791809621,
      "grad_norm": 3.9344940185546875,
      "learning_rate": 3.504376840158649e-05,
      "loss": 0.6851,
      "step": 1966900
    },
    {
      "epoch": 17.948390393459377,
      "grad_norm": 3.4869027137756348,
      "learning_rate": 3.504300800545052e-05,
      "loss": 0.6544,
      "step": 1967000
    },
    {
      "epoch": 17.949302868822542,
      "grad_norm": 3.655031681060791,
      "learning_rate": 3.504224760931455e-05,
      "loss": 0.6727,
      "step": 1967100
    },
    {
      "epoch": 17.950215344185708,
      "grad_norm": 3.011507987976074,
      "learning_rate": 3.504148721317858e-05,
      "loss": 0.6866,
      "step": 1967200
    },
    {
      "epoch": 17.951127819548873,
      "grad_norm": 4.0743560791015625,
      "learning_rate": 3.504072681704261e-05,
      "loss": 0.6839,
      "step": 1967300
    },
    {
      "epoch": 17.952040294912038,
      "grad_norm": 4.72623872756958,
      "learning_rate": 3.503996642090664e-05,
      "loss": 0.6612,
      "step": 1967400
    },
    {
      "epoch": 17.952952770275203,
      "grad_norm": 4.645308971405029,
      "learning_rate": 3.503920602477066e-05,
      "loss": 0.6696,
      "step": 1967500
    },
    {
      "epoch": 17.95386524563837,
      "grad_norm": 4.513869762420654,
      "learning_rate": 3.50384456286347e-05,
      "loss": 0.6819,
      "step": 1967600
    },
    {
      "epoch": 17.954777721001534,
      "grad_norm": 2.9726507663726807,
      "learning_rate": 3.5037685232498723e-05,
      "loss": 0.6681,
      "step": 1967700
    },
    {
      "epoch": 17.9556901963647,
      "grad_norm": 4.079641819000244,
      "learning_rate": 3.5036924836362754e-05,
      "loss": 0.6471,
      "step": 1967800
    },
    {
      "epoch": 17.956602671727865,
      "grad_norm": 3.7356584072113037,
      "learning_rate": 3.5036164440226784e-05,
      "loss": 0.6471,
      "step": 1967900
    },
    {
      "epoch": 17.95751514709103,
      "grad_norm": 3.4586939811706543,
      "learning_rate": 3.5035404044090814e-05,
      "loss": 0.6702,
      "step": 1968000
    },
    {
      "epoch": 17.958427622454195,
      "grad_norm": 3.35284423828125,
      "learning_rate": 3.503464364795484e-05,
      "loss": 0.6736,
      "step": 1968100
    },
    {
      "epoch": 17.95934009781736,
      "grad_norm": 3.845231533050537,
      "learning_rate": 3.5033883251818874e-05,
      "loss": 0.615,
      "step": 1968200
    },
    {
      "epoch": 17.960252573180526,
      "grad_norm": 2.9562132358551025,
      "learning_rate": 3.50331228556829e-05,
      "loss": 0.6452,
      "step": 1968300
    },
    {
      "epoch": 17.96116504854369,
      "grad_norm": 3.271514654159546,
      "learning_rate": 3.503236245954693e-05,
      "loss": 0.6708,
      "step": 1968400
    },
    {
      "epoch": 17.962077523906853,
      "grad_norm": 3.5880281925201416,
      "learning_rate": 3.503160206341096e-05,
      "loss": 0.667,
      "step": 1968500
    },
    {
      "epoch": 17.962989999270018,
      "grad_norm": 3.847837448120117,
      "learning_rate": 3.503084166727498e-05,
      "loss": 0.6642,
      "step": 1968600
    },
    {
      "epoch": 17.963902474633183,
      "grad_norm": 4.717809200286865,
      "learning_rate": 3.503008127113902e-05,
      "loss": 0.6763,
      "step": 1968700
    },
    {
      "epoch": 17.96481494999635,
      "grad_norm": 2.935948610305786,
      "learning_rate": 3.502932087500304e-05,
      "loss": 0.652,
      "step": 1968800
    },
    {
      "epoch": 17.965727425359514,
      "grad_norm": 3.612290143966675,
      "learning_rate": 3.502856047886707e-05,
      "loss": 0.682,
      "step": 1968900
    },
    {
      "epoch": 17.96663990072268,
      "grad_norm": 4.57483434677124,
      "learning_rate": 3.50278000827311e-05,
      "loss": 0.7004,
      "step": 1969000
    },
    {
      "epoch": 17.967552376085845,
      "grad_norm": 3.489041566848755,
      "learning_rate": 3.502703968659513e-05,
      "loss": 0.6575,
      "step": 1969100
    },
    {
      "epoch": 17.96846485144901,
      "grad_norm": 4.299228668212891,
      "learning_rate": 3.5026279290459154e-05,
      "loss": 0.6722,
      "step": 1969200
    },
    {
      "epoch": 17.969377326812175,
      "grad_norm": 3.888751983642578,
      "learning_rate": 3.502551889432319e-05,
      "loss": 0.6414,
      "step": 1969300
    },
    {
      "epoch": 17.97028980217534,
      "grad_norm": 4.143588542938232,
      "learning_rate": 3.5024758498187214e-05,
      "loss": 0.7237,
      "step": 1969400
    },
    {
      "epoch": 17.971202277538506,
      "grad_norm": 3.4545607566833496,
      "learning_rate": 3.5023998102051244e-05,
      "loss": 0.6523,
      "step": 1969500
    },
    {
      "epoch": 17.97211475290167,
      "grad_norm": 3.6641459465026855,
      "learning_rate": 3.5023237705915274e-05,
      "loss": 0.6544,
      "step": 1969600
    },
    {
      "epoch": 17.973027228264836,
      "grad_norm": 4.7883124351501465,
      "learning_rate": 3.5022477309779304e-05,
      "loss": 0.6694,
      "step": 1969700
    },
    {
      "epoch": 17.973939703628,
      "grad_norm": 4.462612152099609,
      "learning_rate": 3.5021716913643335e-05,
      "loss": 0.6348,
      "step": 1969800
    },
    {
      "epoch": 17.974852178991167,
      "grad_norm": 4.643739700317383,
      "learning_rate": 3.5020956517507365e-05,
      "loss": 0.6508,
      "step": 1969900
    },
    {
      "epoch": 17.975764654354332,
      "grad_norm": 4.412493705749512,
      "learning_rate": 3.502019612137139e-05,
      "loss": 0.6775,
      "step": 1970000
    },
    {
      "epoch": 17.976677129717498,
      "grad_norm": 3.262618064880371,
      "learning_rate": 3.5019435725235425e-05,
      "loss": 0.6448,
      "step": 1970100
    },
    {
      "epoch": 17.977589605080663,
      "grad_norm": 3.2957262992858887,
      "learning_rate": 3.501867532909945e-05,
      "loss": 0.6833,
      "step": 1970200
    },
    {
      "epoch": 17.97850208044383,
      "grad_norm": 3.891876459121704,
      "learning_rate": 3.501791493296348e-05,
      "loss": 0.6444,
      "step": 1970300
    },
    {
      "epoch": 17.979414555806994,
      "grad_norm": 4.188553333282471,
      "learning_rate": 3.501715453682751e-05,
      "loss": 0.6616,
      "step": 1970400
    },
    {
      "epoch": 17.98032703117016,
      "grad_norm": 2.9332633018493652,
      "learning_rate": 3.501639414069154e-05,
      "loss": 0.6625,
      "step": 1970500
    },
    {
      "epoch": 17.981239506533324,
      "grad_norm": 3.129812479019165,
      "learning_rate": 3.501563374455556e-05,
      "loss": 0.6916,
      "step": 1970600
    },
    {
      "epoch": 17.98215198189649,
      "grad_norm": 3.4388957023620605,
      "learning_rate": 3.50148733484196e-05,
      "loss": 0.6744,
      "step": 1970700
    },
    {
      "epoch": 17.983064457259655,
      "grad_norm": 3.977011203765869,
      "learning_rate": 3.501411295228362e-05,
      "loss": 0.6663,
      "step": 1970800
    },
    {
      "epoch": 17.98397693262282,
      "grad_norm": 4.15484094619751,
      "learning_rate": 3.501335255614765e-05,
      "loss": 0.6312,
      "step": 1970900
    },
    {
      "epoch": 17.984889407985985,
      "grad_norm": 3.8643152713775635,
      "learning_rate": 3.501259216001168e-05,
      "loss": 0.7025,
      "step": 1971000
    },
    {
      "epoch": 17.98580188334915,
      "grad_norm": 4.6900153160095215,
      "learning_rate": 3.5011831763875705e-05,
      "loss": 0.6886,
      "step": 1971100
    },
    {
      "epoch": 17.986714358712316,
      "grad_norm": 4.083518981933594,
      "learning_rate": 3.501107136773974e-05,
      "loss": 0.6779,
      "step": 1971200
    },
    {
      "epoch": 17.98762683407548,
      "grad_norm": 3.835951566696167,
      "learning_rate": 3.5010310971603765e-05,
      "loss": 0.677,
      "step": 1971300
    },
    {
      "epoch": 17.988539309438647,
      "grad_norm": 3.9100558757781982,
      "learning_rate": 3.5009550575467795e-05,
      "loss": 0.6454,
      "step": 1971400
    },
    {
      "epoch": 17.989451784801812,
      "grad_norm": 4.521246910095215,
      "learning_rate": 3.5008790179331825e-05,
      "loss": 0.6485,
      "step": 1971500
    },
    {
      "epoch": 17.990364260164977,
      "grad_norm": 3.429516315460205,
      "learning_rate": 3.5008029783195855e-05,
      "loss": 0.6538,
      "step": 1971600
    },
    {
      "epoch": 17.991276735528142,
      "grad_norm": 3.9260013103485107,
      "learning_rate": 3.5007269387059885e-05,
      "loss": 0.6557,
      "step": 1971700
    },
    {
      "epoch": 17.992189210891304,
      "grad_norm": 4.10460901260376,
      "learning_rate": 3.5006508990923916e-05,
      "loss": 0.7013,
      "step": 1971800
    },
    {
      "epoch": 17.99310168625447,
      "grad_norm": 4.037296772003174,
      "learning_rate": 3.500574859478794e-05,
      "loss": 0.6966,
      "step": 1971900
    },
    {
      "epoch": 17.994014161617635,
      "grad_norm": 3.6617836952209473,
      "learning_rate": 3.5004988198651976e-05,
      "loss": 0.6403,
      "step": 1972000
    },
    {
      "epoch": 17.9949266369808,
      "grad_norm": 4.618401050567627,
      "learning_rate": 3.5004227802516e-05,
      "loss": 0.6724,
      "step": 1972100
    },
    {
      "epoch": 17.995839112343965,
      "grad_norm": 3.1576523780822754,
      "learning_rate": 3.500346740638003e-05,
      "loss": 0.6821,
      "step": 1972200
    },
    {
      "epoch": 17.99675158770713,
      "grad_norm": 2.6544315814971924,
      "learning_rate": 3.500270701024406e-05,
      "loss": 0.6686,
      "step": 1972300
    },
    {
      "epoch": 17.997664063070296,
      "grad_norm": 4.585587501525879,
      "learning_rate": 3.500194661410809e-05,
      "loss": 0.6641,
      "step": 1972400
    },
    {
      "epoch": 17.99857653843346,
      "grad_norm": 5.178603172302246,
      "learning_rate": 3.500118621797211e-05,
      "loss": 0.6926,
      "step": 1972500
    },
    {
      "epoch": 17.999489013796627,
      "grad_norm": 3.774174451828003,
      "learning_rate": 3.500042582183615e-05,
      "loss": 0.6775,
      "step": 1972600
    },
    {
      "epoch": 18.0,
      "eval_loss": 0.5431416034698486,
      "eval_runtime": 25.3638,
      "eval_samples_per_second": 227.45,
      "eval_steps_per_second": 227.45,
      "step": 1972656
    },
    {
      "epoch": 18.0,
      "eval_loss": 0.520383894443512,
      "eval_runtime": 486.1706,
      "eval_samples_per_second": 225.419,
      "eval_steps_per_second": 225.419,
      "step": 1972656
    },
    {
      "epoch": 18.000401489159792,
      "grad_norm": 3.5732967853546143,
      "learning_rate": 3.499966542570017e-05,
      "loss": 0.6845,
      "step": 1972700
    },
    {
      "epoch": 18.001313964522957,
      "grad_norm": 4.0890092849731445,
      "learning_rate": 3.49989050295642e-05,
      "loss": 0.6792,
      "step": 1972800
    },
    {
      "epoch": 18.002226439886122,
      "grad_norm": 4.136828422546387,
      "learning_rate": 3.499814463342823e-05,
      "loss": 0.6509,
      "step": 1972900
    },
    {
      "epoch": 18.003138915249288,
      "grad_norm": 4.387551784515381,
      "learning_rate": 3.499738423729226e-05,
      "loss": 0.6479,
      "step": 1973000
    },
    {
      "epoch": 18.004051390612453,
      "grad_norm": 3.9902732372283936,
      "learning_rate": 3.499662384115629e-05,
      "loss": 0.6373,
      "step": 1973100
    },
    {
      "epoch": 18.00496386597562,
      "grad_norm": 3.770348310470581,
      "learning_rate": 3.499586344502032e-05,
      "loss": 0.6186,
      "step": 1973200
    },
    {
      "epoch": 18.005876341338784,
      "grad_norm": 3.282496213912964,
      "learning_rate": 3.4995103048884346e-05,
      "loss": 0.6557,
      "step": 1973300
    },
    {
      "epoch": 18.00678881670195,
      "grad_norm": 4.3843231201171875,
      "learning_rate": 3.499434265274838e-05,
      "loss": 0.6337,
      "step": 1973400
    },
    {
      "epoch": 18.007701292065114,
      "grad_norm": 3.746544361114502,
      "learning_rate": 3.4993582256612406e-05,
      "loss": 0.6505,
      "step": 1973500
    },
    {
      "epoch": 18.00861376742828,
      "grad_norm": 3.9093732833862305,
      "learning_rate": 3.4992821860476436e-05,
      "loss": 0.6978,
      "step": 1973600
    },
    {
      "epoch": 18.009526242791445,
      "grad_norm": 3.925232172012329,
      "learning_rate": 3.4992061464340466e-05,
      "loss": 0.6943,
      "step": 1973700
    },
    {
      "epoch": 18.01043871815461,
      "grad_norm": 3.3161585330963135,
      "learning_rate": 3.4991301068204497e-05,
      "loss": 0.6117,
      "step": 1973800
    },
    {
      "epoch": 18.011351193517775,
      "grad_norm": 3.3348793983459473,
      "learning_rate": 3.499054067206852e-05,
      "loss": 0.6529,
      "step": 1973900
    },
    {
      "epoch": 18.01226366888094,
      "grad_norm": 3.074526071548462,
      "learning_rate": 3.498978027593255e-05,
      "loss": 0.6878,
      "step": 1974000
    },
    {
      "epoch": 18.013176144244106,
      "grad_norm": 3.375485420227051,
      "learning_rate": 3.498901987979658e-05,
      "loss": 0.6949,
      "step": 1974100
    },
    {
      "epoch": 18.01408861960727,
      "grad_norm": 4.045595169067383,
      "learning_rate": 3.498825948366061e-05,
      "loss": 0.6428,
      "step": 1974200
    },
    {
      "epoch": 18.015001094970437,
      "grad_norm": 3.7616472244262695,
      "learning_rate": 3.498749908752464e-05,
      "loss": 0.7026,
      "step": 1974300
    },
    {
      "epoch": 18.015913570333602,
      "grad_norm": 4.575211048126221,
      "learning_rate": 3.4986738691388663e-05,
      "loss": 0.6768,
      "step": 1974400
    },
    {
      "epoch": 18.016826045696767,
      "grad_norm": 4.699946403503418,
      "learning_rate": 3.49859782952527e-05,
      "loss": 0.6395,
      "step": 1974500
    },
    {
      "epoch": 18.017738521059933,
      "grad_norm": 3.0499107837677,
      "learning_rate": 3.4985217899116724e-05,
      "loss": 0.6416,
      "step": 1974600
    },
    {
      "epoch": 18.018650996423098,
      "grad_norm": 3.169602632522583,
      "learning_rate": 3.4984457502980754e-05,
      "loss": 0.6484,
      "step": 1974700
    },
    {
      "epoch": 18.019563471786263,
      "grad_norm": 4.066051483154297,
      "learning_rate": 3.4983697106844784e-05,
      "loss": 0.6176,
      "step": 1974800
    },
    {
      "epoch": 18.02047594714943,
      "grad_norm": 4.842926502227783,
      "learning_rate": 3.4982936710708814e-05,
      "loss": 0.6676,
      "step": 1974900
    },
    {
      "epoch": 18.021388422512594,
      "grad_norm": 4.078286170959473,
      "learning_rate": 3.498217631457284e-05,
      "loss": 0.6276,
      "step": 1975000
    },
    {
      "epoch": 18.02230089787576,
      "grad_norm": 3.5023069381713867,
      "learning_rate": 3.4981415918436874e-05,
      "loss": 0.6409,
      "step": 1975100
    },
    {
      "epoch": 18.02321337323892,
      "grad_norm": 4.067444801330566,
      "learning_rate": 3.49806555223009e-05,
      "loss": 0.6582,
      "step": 1975200
    },
    {
      "epoch": 18.024125848602086,
      "grad_norm": 3.9259700775146484,
      "learning_rate": 3.497989512616493e-05,
      "loss": 0.6461,
      "step": 1975300
    },
    {
      "epoch": 18.02503832396525,
      "grad_norm": 5.473439693450928,
      "learning_rate": 3.497913473002896e-05,
      "loss": 0.6231,
      "step": 1975400
    },
    {
      "epoch": 18.025950799328417,
      "grad_norm": 4.654221057891846,
      "learning_rate": 3.497837433389299e-05,
      "loss": 0.6604,
      "step": 1975500
    },
    {
      "epoch": 18.026863274691582,
      "grad_norm": 3.1914165019989014,
      "learning_rate": 3.497761393775702e-05,
      "loss": 0.6386,
      "step": 1975600
    },
    {
      "epoch": 18.027775750054747,
      "grad_norm": 3.6818366050720215,
      "learning_rate": 3.497685354162105e-05,
      "loss": 0.6701,
      "step": 1975700
    },
    {
      "epoch": 18.028688225417913,
      "grad_norm": 3.641583204269409,
      "learning_rate": 3.497609314548507e-05,
      "loss": 0.627,
      "step": 1975800
    },
    {
      "epoch": 18.029600700781078,
      "grad_norm": 3.4428467750549316,
      "learning_rate": 3.497533274934911e-05,
      "loss": 0.6572,
      "step": 1975900
    },
    {
      "epoch": 18.030513176144243,
      "grad_norm": 3.639920473098755,
      "learning_rate": 3.497457235321313e-05,
      "loss": 0.6397,
      "step": 1976000
    },
    {
      "epoch": 18.03142565150741,
      "grad_norm": 4.389379501342773,
      "learning_rate": 3.497381195707716e-05,
      "loss": 0.6401,
      "step": 1976100
    },
    {
      "epoch": 18.032338126870574,
      "grad_norm": 4.222733020782471,
      "learning_rate": 3.497305156094119e-05,
      "loss": 0.6088,
      "step": 1976200
    },
    {
      "epoch": 18.03325060223374,
      "grad_norm": 3.1341328620910645,
      "learning_rate": 3.497229116480522e-05,
      "loss": 0.6586,
      "step": 1976300
    },
    {
      "epoch": 18.034163077596904,
      "grad_norm": 3.5691163539886475,
      "learning_rate": 3.4971530768669244e-05,
      "loss": 0.6572,
      "step": 1976400
    },
    {
      "epoch": 18.03507555296007,
      "grad_norm": 3.30989933013916,
      "learning_rate": 3.497077037253328e-05,
      "loss": 0.6353,
      "step": 1976500
    },
    {
      "epoch": 18.035988028323235,
      "grad_norm": 4.556172847747803,
      "learning_rate": 3.4970009976397305e-05,
      "loss": 0.6782,
      "step": 1976600
    },
    {
      "epoch": 18.0369005036864,
      "grad_norm": 3.980173349380493,
      "learning_rate": 3.4969249580261335e-05,
      "loss": 0.6101,
      "step": 1976700
    },
    {
      "epoch": 18.037812979049566,
      "grad_norm": 4.356168270111084,
      "learning_rate": 3.4968489184125365e-05,
      "loss": 0.6626,
      "step": 1976800
    },
    {
      "epoch": 18.03872545441273,
      "grad_norm": 4.07393741607666,
      "learning_rate": 3.496772878798939e-05,
      "loss": 0.6563,
      "step": 1976900
    },
    {
      "epoch": 18.039637929775896,
      "grad_norm": 3.9766945838928223,
      "learning_rate": 3.4966968391853425e-05,
      "loss": 0.6569,
      "step": 1977000
    },
    {
      "epoch": 18.04055040513906,
      "grad_norm": 4.880326271057129,
      "learning_rate": 3.496620799571745e-05,
      "loss": 0.6288,
      "step": 1977100
    },
    {
      "epoch": 18.041462880502227,
      "grad_norm": 3.862530469894409,
      "learning_rate": 3.496544759958148e-05,
      "loss": 0.6723,
      "step": 1977200
    },
    {
      "epoch": 18.042375355865392,
      "grad_norm": 3.9763429164886475,
      "learning_rate": 3.496468720344551e-05,
      "loss": 0.675,
      "step": 1977300
    },
    {
      "epoch": 18.043287831228557,
      "grad_norm": 4.542501449584961,
      "learning_rate": 3.496392680730954e-05,
      "loss": 0.6379,
      "step": 1977400
    },
    {
      "epoch": 18.044200306591723,
      "grad_norm": 3.6001791954040527,
      "learning_rate": 3.496316641117356e-05,
      "loss": 0.6456,
      "step": 1977500
    },
    {
      "epoch": 18.045112781954888,
      "grad_norm": 3.454435110092163,
      "learning_rate": 3.49624060150376e-05,
      "loss": 0.646,
      "step": 1977600
    },
    {
      "epoch": 18.046025257318053,
      "grad_norm": 3.2045340538024902,
      "learning_rate": 3.496164561890162e-05,
      "loss": 0.672,
      "step": 1977700
    },
    {
      "epoch": 18.04693773268122,
      "grad_norm": 3.143721342086792,
      "learning_rate": 3.496088522276565e-05,
      "loss": 0.6684,
      "step": 1977800
    },
    {
      "epoch": 18.047850208044384,
      "grad_norm": 3.5670342445373535,
      "learning_rate": 3.496012482662968e-05,
      "loss": 0.6236,
      "step": 1977900
    },
    {
      "epoch": 18.04876268340755,
      "grad_norm": 3.9151196479797363,
      "learning_rate": 3.495936443049371e-05,
      "loss": 0.6303,
      "step": 1978000
    },
    {
      "epoch": 18.049675158770714,
      "grad_norm": 3.669680118560791,
      "learning_rate": 3.495860403435774e-05,
      "loss": 0.6456,
      "step": 1978100
    },
    {
      "epoch": 18.05058763413388,
      "grad_norm": 4.871886253356934,
      "learning_rate": 3.495784363822177e-05,
      "loss": 0.6783,
      "step": 1978200
    },
    {
      "epoch": 18.051500109497045,
      "grad_norm": 4.297809600830078,
      "learning_rate": 3.4957083242085795e-05,
      "loss": 0.6722,
      "step": 1978300
    },
    {
      "epoch": 18.05241258486021,
      "grad_norm": 3.890484571456909,
      "learning_rate": 3.495632284594983e-05,
      "loss": 0.6624,
      "step": 1978400
    },
    {
      "epoch": 18.053325060223376,
      "grad_norm": 3.723679304122925,
      "learning_rate": 3.4955562449813856e-05,
      "loss": 0.6381,
      "step": 1978500
    },
    {
      "epoch": 18.054237535586537,
      "grad_norm": 3.0123836994171143,
      "learning_rate": 3.4954802053677886e-05,
      "loss": 0.703,
      "step": 1978600
    },
    {
      "epoch": 18.055150010949703,
      "grad_norm": 3.7230899333953857,
      "learning_rate": 3.4954041657541916e-05,
      "loss": 0.6555,
      "step": 1978700
    },
    {
      "epoch": 18.056062486312868,
      "grad_norm": 4.033907890319824,
      "learning_rate": 3.4953281261405946e-05,
      "loss": 0.6388,
      "step": 1978800
    },
    {
      "epoch": 18.056974961676033,
      "grad_norm": 4.099765777587891,
      "learning_rate": 3.495252086526997e-05,
      "loss": 0.6769,
      "step": 1978900
    },
    {
      "epoch": 18.0578874370392,
      "grad_norm": 4.167140483856201,
      "learning_rate": 3.4951760469134006e-05,
      "loss": 0.6332,
      "step": 1979000
    },
    {
      "epoch": 18.058799912402364,
      "grad_norm": 3.9736545085906982,
      "learning_rate": 3.495100007299803e-05,
      "loss": 0.6828,
      "step": 1979100
    },
    {
      "epoch": 18.05971238776553,
      "grad_norm": 4.165509223937988,
      "learning_rate": 3.495023967686206e-05,
      "loss": 0.6976,
      "step": 1979200
    },
    {
      "epoch": 18.060624863128695,
      "grad_norm": 3.497398853302002,
      "learning_rate": 3.494947928072609e-05,
      "loss": 0.6597,
      "step": 1979300
    },
    {
      "epoch": 18.06153733849186,
      "grad_norm": 3.394589900970459,
      "learning_rate": 3.494871888459012e-05,
      "loss": 0.5869,
      "step": 1979400
    },
    {
      "epoch": 18.062449813855025,
      "grad_norm": 3.6355090141296387,
      "learning_rate": 3.494795848845415e-05,
      "loss": 0.6468,
      "step": 1979500
    },
    {
      "epoch": 18.06336228921819,
      "grad_norm": 4.116028308868408,
      "learning_rate": 3.494719809231817e-05,
      "loss": 0.6532,
      "step": 1979600
    },
    {
      "epoch": 18.064274764581356,
      "grad_norm": 4.541937828063965,
      "learning_rate": 3.49464376961822e-05,
      "loss": 0.6502,
      "step": 1979700
    },
    {
      "epoch": 18.06518723994452,
      "grad_norm": 3.6588869094848633,
      "learning_rate": 3.494567730004623e-05,
      "loss": 0.6496,
      "step": 1979800
    },
    {
      "epoch": 18.066099715307686,
      "grad_norm": 3.0365099906921387,
      "learning_rate": 3.494491690391026e-05,
      "loss": 0.6258,
      "step": 1979900
    },
    {
      "epoch": 18.06701219067085,
      "grad_norm": 4.016226291656494,
      "learning_rate": 3.4944156507774286e-05,
      "loss": 0.6801,
      "step": 1980000
    },
    {
      "epoch": 18.067924666034017,
      "grad_norm": 4.028721332550049,
      "learning_rate": 3.494339611163832e-05,
      "loss": 0.6574,
      "step": 1980100
    },
    {
      "epoch": 18.068837141397182,
      "grad_norm": 3.1783711910247803,
      "learning_rate": 3.4942635715502346e-05,
      "loss": 0.6562,
      "step": 1980200
    },
    {
      "epoch": 18.069749616760348,
      "grad_norm": 5.48224401473999,
      "learning_rate": 3.4941875319366376e-05,
      "loss": 0.6247,
      "step": 1980300
    },
    {
      "epoch": 18.070662092123513,
      "grad_norm": 5.318403720855713,
      "learning_rate": 3.4941114923230406e-05,
      "loss": 0.6402,
      "step": 1980400
    },
    {
      "epoch": 18.071574567486678,
      "grad_norm": 4.008753299713135,
      "learning_rate": 3.4940354527094437e-05,
      "loss": 0.6171,
      "step": 1980500
    },
    {
      "epoch": 18.072487042849843,
      "grad_norm": 3.5480082035064697,
      "learning_rate": 3.4939594130958467e-05,
      "loss": 0.6549,
      "step": 1980600
    },
    {
      "epoch": 18.07339951821301,
      "grad_norm": 3.496929407119751,
      "learning_rate": 3.49388337348225e-05,
      "loss": 0.6798,
      "step": 1980700
    },
    {
      "epoch": 18.074311993576174,
      "grad_norm": 4.002126693725586,
      "learning_rate": 3.493807333868652e-05,
      "loss": 0.6901,
      "step": 1980800
    },
    {
      "epoch": 18.07522446893934,
      "grad_norm": 3.364462375640869,
      "learning_rate": 3.493731294255056e-05,
      "loss": 0.6668,
      "step": 1980900
    },
    {
      "epoch": 18.076136944302505,
      "grad_norm": 4.532066345214844,
      "learning_rate": 3.493655254641458e-05,
      "loss": 0.6533,
      "step": 1981000
    },
    {
      "epoch": 18.07704941966567,
      "grad_norm": 3.3451344966888428,
      "learning_rate": 3.493579215027861e-05,
      "loss": 0.6674,
      "step": 1981100
    },
    {
      "epoch": 18.077961895028835,
      "grad_norm": 3.291978359222412,
      "learning_rate": 3.493503175414264e-05,
      "loss": 0.6455,
      "step": 1981200
    },
    {
      "epoch": 18.078874370392,
      "grad_norm": 4.706718444824219,
      "learning_rate": 3.493427135800667e-05,
      "loss": 0.6615,
      "step": 1981300
    },
    {
      "epoch": 18.079786845755166,
      "grad_norm": 4.369358539581299,
      "learning_rate": 3.4933510961870694e-05,
      "loss": 0.6728,
      "step": 1981400
    },
    {
      "epoch": 18.08069932111833,
      "grad_norm": 4.329456329345703,
      "learning_rate": 3.493275056573473e-05,
      "loss": 0.6453,
      "step": 1981500
    },
    {
      "epoch": 18.081611796481496,
      "grad_norm": 4.4490203857421875,
      "learning_rate": 3.4931990169598754e-05,
      "loss": 0.6589,
      "step": 1981600
    },
    {
      "epoch": 18.08252427184466,
      "grad_norm": 3.5351459980010986,
      "learning_rate": 3.4931229773462784e-05,
      "loss": 0.6924,
      "step": 1981700
    },
    {
      "epoch": 18.083436747207827,
      "grad_norm": 3.7475602626800537,
      "learning_rate": 3.4930469377326814e-05,
      "loss": 0.6779,
      "step": 1981800
    },
    {
      "epoch": 18.084349222570992,
      "grad_norm": 3.7802536487579346,
      "learning_rate": 3.4929708981190844e-05,
      "loss": 0.6529,
      "step": 1981900
    },
    {
      "epoch": 18.085261697934154,
      "grad_norm": 3.9567911624908447,
      "learning_rate": 3.4928948585054874e-05,
      "loss": 0.6502,
      "step": 1982000
    },
    {
      "epoch": 18.08617417329732,
      "grad_norm": 3.4123010635375977,
      "learning_rate": 3.4928188188918904e-05,
      "loss": 0.6293,
      "step": 1982100
    },
    {
      "epoch": 18.087086648660485,
      "grad_norm": 4.3514227867126465,
      "learning_rate": 3.492742779278293e-05,
      "loss": 0.6595,
      "step": 1982200
    },
    {
      "epoch": 18.08799912402365,
      "grad_norm": 3.4644935131073,
      "learning_rate": 3.4926667396646964e-05,
      "loss": 0.6606,
      "step": 1982300
    },
    {
      "epoch": 18.088911599386815,
      "grad_norm": 3.93829345703125,
      "learning_rate": 3.492590700051099e-05,
      "loss": 0.6583,
      "step": 1982400
    },
    {
      "epoch": 18.08982407474998,
      "grad_norm": 4.219967365264893,
      "learning_rate": 3.492514660437502e-05,
      "loss": 0.6904,
      "step": 1982500
    },
    {
      "epoch": 18.090736550113146,
      "grad_norm": 4.159050464630127,
      "learning_rate": 3.492438620823905e-05,
      "loss": 0.6234,
      "step": 1982600
    },
    {
      "epoch": 18.09164902547631,
      "grad_norm": 4.043858528137207,
      "learning_rate": 3.492362581210307e-05,
      "loss": 0.6473,
      "step": 1982700
    },
    {
      "epoch": 18.092561500839476,
      "grad_norm": 3.7987639904022217,
      "learning_rate": 3.49228654159671e-05,
      "loss": 0.6523,
      "step": 1982800
    },
    {
      "epoch": 18.09347397620264,
      "grad_norm": 3.3727216720581055,
      "learning_rate": 3.492210501983113e-05,
      "loss": 0.6568,
      "step": 1982900
    },
    {
      "epoch": 18.094386451565807,
      "grad_norm": 3.800048589706421,
      "learning_rate": 3.492134462369516e-05,
      "loss": 0.6747,
      "step": 1983000
    },
    {
      "epoch": 18.095298926928972,
      "grad_norm": 2.835799217224121,
      "learning_rate": 3.492058422755919e-05,
      "loss": 0.6905,
      "step": 1983100
    },
    {
      "epoch": 18.096211402292138,
      "grad_norm": 3.8580613136291504,
      "learning_rate": 3.491982383142322e-05,
      "loss": 0.6671,
      "step": 1983200
    },
    {
      "epoch": 18.097123877655303,
      "grad_norm": 3.592102289199829,
      "learning_rate": 3.4919063435287245e-05,
      "loss": 0.6526,
      "step": 1983300
    },
    {
      "epoch": 18.09803635301847,
      "grad_norm": 3.2007596492767334,
      "learning_rate": 3.491830303915128e-05,
      "loss": 0.6824,
      "step": 1983400
    },
    {
      "epoch": 18.098948828381634,
      "grad_norm": 3.2563705444335938,
      "learning_rate": 3.4917542643015305e-05,
      "loss": 0.6473,
      "step": 1983500
    },
    {
      "epoch": 18.0998613037448,
      "grad_norm": 2.8013036251068115,
      "learning_rate": 3.4916782246879335e-05,
      "loss": 0.6708,
      "step": 1983600
    },
    {
      "epoch": 18.100773779107964,
      "grad_norm": 4.611971855163574,
      "learning_rate": 3.4916021850743365e-05,
      "loss": 0.6474,
      "step": 1983700
    },
    {
      "epoch": 18.10168625447113,
      "grad_norm": 3.450035572052002,
      "learning_rate": 3.4915261454607395e-05,
      "loss": 0.6813,
      "step": 1983800
    },
    {
      "epoch": 18.102598729834295,
      "grad_norm": 6.165046691894531,
      "learning_rate": 3.4914501058471425e-05,
      "loss": 0.6393,
      "step": 1983900
    },
    {
      "epoch": 18.10351120519746,
      "grad_norm": 4.522614479064941,
      "learning_rate": 3.4913740662335455e-05,
      "loss": 0.6455,
      "step": 1984000
    },
    {
      "epoch": 18.104423680560625,
      "grad_norm": 4.833306789398193,
      "learning_rate": 3.491298026619948e-05,
      "loss": 0.6706,
      "step": 1984100
    },
    {
      "epoch": 18.10533615592379,
      "grad_norm": 4.370779037475586,
      "learning_rate": 3.491221987006351e-05,
      "loss": 0.6798,
      "step": 1984200
    },
    {
      "epoch": 18.106248631286956,
      "grad_norm": 3.9935216903686523,
      "learning_rate": 3.491145947392754e-05,
      "loss": 0.6413,
      "step": 1984300
    },
    {
      "epoch": 18.10716110665012,
      "grad_norm": 3.275725841522217,
      "learning_rate": 3.491069907779157e-05,
      "loss": 0.6421,
      "step": 1984400
    },
    {
      "epoch": 18.108073582013287,
      "grad_norm": 3.6211280822753906,
      "learning_rate": 3.49099386816556e-05,
      "loss": 0.6541,
      "step": 1984500
    },
    {
      "epoch": 18.108986057376452,
      "grad_norm": 4.688901424407959,
      "learning_rate": 3.490917828551963e-05,
      "loss": 0.6509,
      "step": 1984600
    },
    {
      "epoch": 18.109898532739617,
      "grad_norm": 3.657334566116333,
      "learning_rate": 3.490841788938365e-05,
      "loss": 0.678,
      "step": 1984700
    },
    {
      "epoch": 18.110811008102782,
      "grad_norm": 5.009087085723877,
      "learning_rate": 3.490765749324769e-05,
      "loss": 0.7075,
      "step": 1984800
    },
    {
      "epoch": 18.111723483465948,
      "grad_norm": 3.777059555053711,
      "learning_rate": 3.490689709711171e-05,
      "loss": 0.6466,
      "step": 1984900
    },
    {
      "epoch": 18.112635958829113,
      "grad_norm": 4.203390121459961,
      "learning_rate": 3.490613670097574e-05,
      "loss": 0.689,
      "step": 1985000
    },
    {
      "epoch": 18.11354843419228,
      "grad_norm": 3.512937068939209,
      "learning_rate": 3.490537630483977e-05,
      "loss": 0.6569,
      "step": 1985100
    },
    {
      "epoch": 18.114460909555444,
      "grad_norm": 4.968255519866943,
      "learning_rate": 3.49046159087038e-05,
      "loss": 0.6633,
      "step": 1985200
    },
    {
      "epoch": 18.11537338491861,
      "grad_norm": 4.259048938751221,
      "learning_rate": 3.490385551256783e-05,
      "loss": 0.6542,
      "step": 1985300
    },
    {
      "epoch": 18.11628586028177,
      "grad_norm": 4.559853553771973,
      "learning_rate": 3.4903095116431856e-05,
      "loss": 0.6758,
      "step": 1985400
    },
    {
      "epoch": 18.117198335644936,
      "grad_norm": 2.0785348415374756,
      "learning_rate": 3.4902334720295886e-05,
      "loss": 0.6623,
      "step": 1985500
    },
    {
      "epoch": 18.1181108110081,
      "grad_norm": 3.384432315826416,
      "learning_rate": 3.4901574324159916e-05,
      "loss": 0.6392,
      "step": 1985600
    },
    {
      "epoch": 18.119023286371267,
      "grad_norm": 3.801445245742798,
      "learning_rate": 3.4900813928023946e-05,
      "loss": 0.6858,
      "step": 1985700
    },
    {
      "epoch": 18.119935761734432,
      "grad_norm": 3.7075324058532715,
      "learning_rate": 3.490005353188797e-05,
      "loss": 0.6657,
      "step": 1985800
    },
    {
      "epoch": 18.120848237097597,
      "grad_norm": 5.044905662536621,
      "learning_rate": 3.4899293135752006e-05,
      "loss": 0.6175,
      "step": 1985900
    },
    {
      "epoch": 18.121760712460762,
      "grad_norm": 4.85289192199707,
      "learning_rate": 3.489853273961603e-05,
      "loss": 0.6627,
      "step": 1986000
    },
    {
      "epoch": 18.122673187823928,
      "grad_norm": 2.6218674182891846,
      "learning_rate": 3.489777234348006e-05,
      "loss": 0.6523,
      "step": 1986100
    },
    {
      "epoch": 18.123585663187093,
      "grad_norm": 3.7366225719451904,
      "learning_rate": 3.489701194734409e-05,
      "loss": 0.6471,
      "step": 1986200
    },
    {
      "epoch": 18.12449813855026,
      "grad_norm": 5.296690464019775,
      "learning_rate": 3.489625155120812e-05,
      "loss": 0.6439,
      "step": 1986300
    },
    {
      "epoch": 18.125410613913424,
      "grad_norm": 3.190500497817993,
      "learning_rate": 3.489549115507215e-05,
      "loss": 0.6699,
      "step": 1986400
    },
    {
      "epoch": 18.12632308927659,
      "grad_norm": 5.0334882736206055,
      "learning_rate": 3.489473075893618e-05,
      "loss": 0.6955,
      "step": 1986500
    },
    {
      "epoch": 18.127235564639754,
      "grad_norm": 3.918083429336548,
      "learning_rate": 3.48939703628002e-05,
      "loss": 0.6712,
      "step": 1986600
    },
    {
      "epoch": 18.12814804000292,
      "grad_norm": 3.91140079498291,
      "learning_rate": 3.489320996666424e-05,
      "loss": 0.6552,
      "step": 1986700
    },
    {
      "epoch": 18.129060515366085,
      "grad_norm": 3.868028402328491,
      "learning_rate": 3.489244957052826e-05,
      "loss": 0.6602,
      "step": 1986800
    },
    {
      "epoch": 18.12997299072925,
      "grad_norm": 3.806652307510376,
      "learning_rate": 3.489168917439229e-05,
      "loss": 0.693,
      "step": 1986900
    },
    {
      "epoch": 18.130885466092415,
      "grad_norm": 4.558986186981201,
      "learning_rate": 3.489092877825632e-05,
      "loss": 0.6983,
      "step": 1987000
    },
    {
      "epoch": 18.13179794145558,
      "grad_norm": 3.301384687423706,
      "learning_rate": 3.489016838212035e-05,
      "loss": 0.6876,
      "step": 1987100
    },
    {
      "epoch": 18.132710416818746,
      "grad_norm": 4.165526866912842,
      "learning_rate": 3.4889407985984376e-05,
      "loss": 0.6469,
      "step": 1987200
    },
    {
      "epoch": 18.13362289218191,
      "grad_norm": 4.746331214904785,
      "learning_rate": 3.488864758984841e-05,
      "loss": 0.7153,
      "step": 1987300
    },
    {
      "epoch": 18.134535367545077,
      "grad_norm": 3.2649290561676025,
      "learning_rate": 3.488788719371244e-05,
      "loss": 0.6785,
      "step": 1987400
    },
    {
      "epoch": 18.135447842908242,
      "grad_norm": 3.8612968921661377,
      "learning_rate": 3.488712679757647e-05,
      "loss": 0.6667,
      "step": 1987500
    },
    {
      "epoch": 18.136360318271407,
      "grad_norm": 2.660179376602173,
      "learning_rate": 3.48863664014405e-05,
      "loss": 0.7087,
      "step": 1987600
    },
    {
      "epoch": 18.137272793634573,
      "grad_norm": 4.077864646911621,
      "learning_rate": 3.488560600530453e-05,
      "loss": 0.6709,
      "step": 1987700
    },
    {
      "epoch": 18.138185268997738,
      "grad_norm": 3.4818873405456543,
      "learning_rate": 3.488484560916856e-05,
      "loss": 0.6662,
      "step": 1987800
    },
    {
      "epoch": 18.139097744360903,
      "grad_norm": 3.2849607467651367,
      "learning_rate": 3.488408521303259e-05,
      "loss": 0.6114,
      "step": 1987900
    },
    {
      "epoch": 18.14001021972407,
      "grad_norm": 3.2939724922180176,
      "learning_rate": 3.488332481689661e-05,
      "loss": 0.6387,
      "step": 1988000
    },
    {
      "epoch": 18.140922695087234,
      "grad_norm": 4.175025463104248,
      "learning_rate": 3.488256442076065e-05,
      "loss": 0.656,
      "step": 1988100
    },
    {
      "epoch": 18.1418351704504,
      "grad_norm": 4.385551929473877,
      "learning_rate": 3.488180402462467e-05,
      "loss": 0.6847,
      "step": 1988200
    },
    {
      "epoch": 18.142747645813564,
      "grad_norm": 3.3747942447662354,
      "learning_rate": 3.4881043628488694e-05,
      "loss": 0.6206,
      "step": 1988300
    },
    {
      "epoch": 18.14366012117673,
      "grad_norm": 3.910473346710205,
      "learning_rate": 3.488028323235273e-05,
      "loss": 0.6638,
      "step": 1988400
    },
    {
      "epoch": 18.144572596539895,
      "grad_norm": 3.2242417335510254,
      "learning_rate": 3.4879522836216754e-05,
      "loss": 0.6792,
      "step": 1988500
    },
    {
      "epoch": 18.14548507190306,
      "grad_norm": 4.114373207092285,
      "learning_rate": 3.4878762440080784e-05,
      "loss": 0.6483,
      "step": 1988600
    },
    {
      "epoch": 18.146397547266226,
      "grad_norm": 4.01505184173584,
      "learning_rate": 3.4878002043944814e-05,
      "loss": 0.7014,
      "step": 1988700
    },
    {
      "epoch": 18.147310022629387,
      "grad_norm": 4.882212162017822,
      "learning_rate": 3.4877241647808844e-05,
      "loss": 0.6793,
      "step": 1988800
    },
    {
      "epoch": 18.148222497992553,
      "grad_norm": 3.696427583694458,
      "learning_rate": 3.4876481251672874e-05,
      "loss": 0.6394,
      "step": 1988900
    },
    {
      "epoch": 18.149134973355718,
      "grad_norm": 4.076636791229248,
      "learning_rate": 3.4875720855536904e-05,
      "loss": 0.6684,
      "step": 1989000
    },
    {
      "epoch": 18.150047448718883,
      "grad_norm": 3.8890416622161865,
      "learning_rate": 3.487496045940093e-05,
      "loss": 0.6908,
      "step": 1989100
    },
    {
      "epoch": 18.15095992408205,
      "grad_norm": 3.3447678089141846,
      "learning_rate": 3.4874200063264964e-05,
      "loss": 0.6555,
      "step": 1989200
    },
    {
      "epoch": 18.151872399445214,
      "grad_norm": 3.7131502628326416,
      "learning_rate": 3.487343966712899e-05,
      "loss": 0.6662,
      "step": 1989300
    },
    {
      "epoch": 18.15278487480838,
      "grad_norm": 4.749192714691162,
      "learning_rate": 3.487267927099302e-05,
      "loss": 0.6587,
      "step": 1989400
    },
    {
      "epoch": 18.153697350171544,
      "grad_norm": 3.0438008308410645,
      "learning_rate": 3.487191887485705e-05,
      "loss": 0.6864,
      "step": 1989500
    },
    {
      "epoch": 18.15460982553471,
      "grad_norm": 3.7708356380462646,
      "learning_rate": 3.487115847872108e-05,
      "loss": 0.6289,
      "step": 1989600
    },
    {
      "epoch": 18.155522300897875,
      "grad_norm": 3.1199471950531006,
      "learning_rate": 3.48703980825851e-05,
      "loss": 0.6468,
      "step": 1989700
    },
    {
      "epoch": 18.15643477626104,
      "grad_norm": 3.9123218059539795,
      "learning_rate": 3.486963768644914e-05,
      "loss": 0.6574,
      "step": 1989800
    },
    {
      "epoch": 18.157347251624206,
      "grad_norm": 3.8272454738616943,
      "learning_rate": 3.486887729031316e-05,
      "loss": 0.5997,
      "step": 1989900
    },
    {
      "epoch": 18.15825972698737,
      "grad_norm": 2.8761041164398193,
      "learning_rate": 3.486811689417719e-05,
      "loss": 0.6461,
      "step": 1990000
    },
    {
      "epoch": 18.159172202350536,
      "grad_norm": 4.403920650482178,
      "learning_rate": 3.486735649804122e-05,
      "loss": 0.6874,
      "step": 1990100
    },
    {
      "epoch": 18.1600846777137,
      "grad_norm": 3.3743414878845215,
      "learning_rate": 3.486659610190525e-05,
      "loss": 0.7037,
      "step": 1990200
    },
    {
      "epoch": 18.160997153076867,
      "grad_norm": 4.517474174499512,
      "learning_rate": 3.486583570576928e-05,
      "loss": 0.6672,
      "step": 1990300
    },
    {
      "epoch": 18.161909628440032,
      "grad_norm": 3.4846105575561523,
      "learning_rate": 3.486507530963331e-05,
      "loss": 0.6826,
      "step": 1990400
    },
    {
      "epoch": 18.162822103803197,
      "grad_norm": 2.2358381748199463,
      "learning_rate": 3.4864314913497335e-05,
      "loss": 0.6714,
      "step": 1990500
    },
    {
      "epoch": 18.163734579166363,
      "grad_norm": 3.7514560222625732,
      "learning_rate": 3.486355451736137e-05,
      "loss": 0.6547,
      "step": 1990600
    },
    {
      "epoch": 18.164647054529528,
      "grad_norm": 4.241264343261719,
      "learning_rate": 3.4862794121225395e-05,
      "loss": 0.6718,
      "step": 1990700
    },
    {
      "epoch": 18.165559529892693,
      "grad_norm": 3.6486563682556152,
      "learning_rate": 3.4862033725089425e-05,
      "loss": 0.6572,
      "step": 1990800
    },
    {
      "epoch": 18.16647200525586,
      "grad_norm": 4.022029399871826,
      "learning_rate": 3.4861273328953455e-05,
      "loss": 0.6372,
      "step": 1990900
    },
    {
      "epoch": 18.167384480619024,
      "grad_norm": 3.74647855758667,
      "learning_rate": 3.486051293281748e-05,
      "loss": 0.6217,
      "step": 1991000
    },
    {
      "epoch": 18.16829695598219,
      "grad_norm": 3.669422149658203,
      "learning_rate": 3.485975253668151e-05,
      "loss": 0.6699,
      "step": 1991100
    },
    {
      "epoch": 18.169209431345354,
      "grad_norm": 3.0330846309661865,
      "learning_rate": 3.485899214054554e-05,
      "loss": 0.6504,
      "step": 1991200
    },
    {
      "epoch": 18.17012190670852,
      "grad_norm": 4.265594482421875,
      "learning_rate": 3.485823174440957e-05,
      "loss": 0.6841,
      "step": 1991300
    },
    {
      "epoch": 18.171034382071685,
      "grad_norm": 5.500271797180176,
      "learning_rate": 3.48574713482736e-05,
      "loss": 0.6482,
      "step": 1991400
    },
    {
      "epoch": 18.17194685743485,
      "grad_norm": 3.919776201248169,
      "learning_rate": 3.485671095213763e-05,
      "loss": 0.6706,
      "step": 1991500
    },
    {
      "epoch": 18.172859332798016,
      "grad_norm": 3.8101866245269775,
      "learning_rate": 3.485595055600165e-05,
      "loss": 0.6423,
      "step": 1991600
    },
    {
      "epoch": 18.17377180816118,
      "grad_norm": 3.2181334495544434,
      "learning_rate": 3.485519015986569e-05,
      "loss": 0.653,
      "step": 1991700
    },
    {
      "epoch": 18.174684283524346,
      "grad_norm": 3.5757639408111572,
      "learning_rate": 3.485442976372971e-05,
      "loss": 0.6616,
      "step": 1991800
    },
    {
      "epoch": 18.17559675888751,
      "grad_norm": 2.97579288482666,
      "learning_rate": 3.485366936759374e-05,
      "loss": 0.6523,
      "step": 1991900
    },
    {
      "epoch": 18.176509234250677,
      "grad_norm": 4.186772346496582,
      "learning_rate": 3.485290897145777e-05,
      "loss": 0.6726,
      "step": 1992000
    },
    {
      "epoch": 18.177421709613842,
      "grad_norm": 3.7394521236419678,
      "learning_rate": 3.48521485753218e-05,
      "loss": 0.6426,
      "step": 1992100
    },
    {
      "epoch": 18.178334184977004,
      "grad_norm": 5.087024211883545,
      "learning_rate": 3.4851388179185826e-05,
      "loss": 0.6417,
      "step": 1992200
    },
    {
      "epoch": 18.17924666034017,
      "grad_norm": 5.08158540725708,
      "learning_rate": 3.485062778304986e-05,
      "loss": 0.662,
      "step": 1992300
    },
    {
      "epoch": 18.180159135703335,
      "grad_norm": 3.851522445678711,
      "learning_rate": 3.4849867386913886e-05,
      "loss": 0.6627,
      "step": 1992400
    },
    {
      "epoch": 18.1810716110665,
      "grad_norm": 3.836639165878296,
      "learning_rate": 3.4849106990777916e-05,
      "loss": 0.6673,
      "step": 1992500
    },
    {
      "epoch": 18.181984086429665,
      "grad_norm": 3.8713724613189697,
      "learning_rate": 3.4848346594641946e-05,
      "loss": 0.6139,
      "step": 1992600
    },
    {
      "epoch": 18.18289656179283,
      "grad_norm": 4.216254711151123,
      "learning_rate": 3.4847586198505976e-05,
      "loss": 0.6717,
      "step": 1992700
    },
    {
      "epoch": 18.183809037155996,
      "grad_norm": 3.8362555503845215,
      "learning_rate": 3.4846825802370006e-05,
      "loss": 0.6678,
      "step": 1992800
    },
    {
      "epoch": 18.18472151251916,
      "grad_norm": 4.64623498916626,
      "learning_rate": 3.4846065406234036e-05,
      "loss": 0.7166,
      "step": 1992900
    },
    {
      "epoch": 18.185633987882326,
      "grad_norm": 3.5476536750793457,
      "learning_rate": 3.484530501009806e-05,
      "loss": 0.6304,
      "step": 1993000
    },
    {
      "epoch": 18.18654646324549,
      "grad_norm": 3.9805688858032227,
      "learning_rate": 3.4844544613962096e-05,
      "loss": 0.6656,
      "step": 1993100
    },
    {
      "epoch": 18.187458938608657,
      "grad_norm": 3.7170934677124023,
      "learning_rate": 3.484378421782612e-05,
      "loss": 0.6855,
      "step": 1993200
    },
    {
      "epoch": 18.188371413971822,
      "grad_norm": 3.704853057861328,
      "learning_rate": 3.484302382169015e-05,
      "loss": 0.6946,
      "step": 1993300
    },
    {
      "epoch": 18.189283889334988,
      "grad_norm": 4.111608028411865,
      "learning_rate": 3.484226342555418e-05,
      "loss": 0.665,
      "step": 1993400
    },
    {
      "epoch": 18.190196364698153,
      "grad_norm": 3.516244411468506,
      "learning_rate": 3.484150302941821e-05,
      "loss": 0.6655,
      "step": 1993500
    },
    {
      "epoch": 18.191108840061318,
      "grad_norm": 3.7932052612304688,
      "learning_rate": 3.484074263328223e-05,
      "loss": 0.6859,
      "step": 1993600
    },
    {
      "epoch": 18.192021315424483,
      "grad_norm": 3.372648000717163,
      "learning_rate": 3.483998223714627e-05,
      "loss": 0.6287,
      "step": 1993700
    },
    {
      "epoch": 18.19293379078765,
      "grad_norm": 4.220521450042725,
      "learning_rate": 3.483922184101029e-05,
      "loss": 0.6662,
      "step": 1993800
    },
    {
      "epoch": 18.193846266150814,
      "grad_norm": 3.3241803646087646,
      "learning_rate": 3.483846144487432e-05,
      "loss": 0.7101,
      "step": 1993900
    },
    {
      "epoch": 18.19475874151398,
      "grad_norm": 4.628510475158691,
      "learning_rate": 3.483770104873835e-05,
      "loss": 0.6485,
      "step": 1994000
    },
    {
      "epoch": 18.195671216877145,
      "grad_norm": 3.7787301540374756,
      "learning_rate": 3.4836940652602377e-05,
      "loss": 0.6281,
      "step": 1994100
    },
    {
      "epoch": 18.19658369224031,
      "grad_norm": 3.696727991104126,
      "learning_rate": 3.4836180256466413e-05,
      "loss": 0.6611,
      "step": 1994200
    },
    {
      "epoch": 18.197496167603475,
      "grad_norm": 3.6278064250946045,
      "learning_rate": 3.483541986033044e-05,
      "loss": 0.6449,
      "step": 1994300
    },
    {
      "epoch": 18.19840864296664,
      "grad_norm": 4.236678600311279,
      "learning_rate": 3.483465946419447e-05,
      "loss": 0.6473,
      "step": 1994400
    },
    {
      "epoch": 18.199321118329806,
      "grad_norm": 2.827885866165161,
      "learning_rate": 3.48338990680585e-05,
      "loss": 0.6599,
      "step": 1994500
    },
    {
      "epoch": 18.20023359369297,
      "grad_norm": 4.000930309295654,
      "learning_rate": 3.483313867192253e-05,
      "loss": 0.6942,
      "step": 1994600
    },
    {
      "epoch": 18.201146069056136,
      "grad_norm": 4.151033878326416,
      "learning_rate": 3.483237827578655e-05,
      "loss": 0.6664,
      "step": 1994700
    },
    {
      "epoch": 18.2020585444193,
      "grad_norm": 4.387734889984131,
      "learning_rate": 3.483161787965059e-05,
      "loss": 0.641,
      "step": 1994800
    },
    {
      "epoch": 18.202971019782467,
      "grad_norm": 3.033424139022827,
      "learning_rate": 3.483085748351461e-05,
      "loss": 0.6679,
      "step": 1994900
    },
    {
      "epoch": 18.203883495145632,
      "grad_norm": 4.176782131195068,
      "learning_rate": 3.483009708737864e-05,
      "loss": 0.6783,
      "step": 1995000
    },
    {
      "epoch": 18.204795970508798,
      "grad_norm": 4.226688385009766,
      "learning_rate": 3.482933669124267e-05,
      "loss": 0.6694,
      "step": 1995100
    },
    {
      "epoch": 18.205708445871963,
      "grad_norm": 2.9650285243988037,
      "learning_rate": 3.48285762951067e-05,
      "loss": 0.6883,
      "step": 1995200
    },
    {
      "epoch": 18.206620921235128,
      "grad_norm": 4.461359024047852,
      "learning_rate": 3.482781589897073e-05,
      "loss": 0.671,
      "step": 1995300
    },
    {
      "epoch": 18.207533396598294,
      "grad_norm": 3.5104541778564453,
      "learning_rate": 3.482705550283476e-05,
      "loss": 0.6433,
      "step": 1995400
    },
    {
      "epoch": 18.20844587196146,
      "grad_norm": 3.7454564571380615,
      "learning_rate": 3.4826295106698784e-05,
      "loss": 0.6647,
      "step": 1995500
    },
    {
      "epoch": 18.20935834732462,
      "grad_norm": 3.5634706020355225,
      "learning_rate": 3.482553471056282e-05,
      "loss": 0.6673,
      "step": 1995600
    },
    {
      "epoch": 18.210270822687786,
      "grad_norm": 3.9751083850860596,
      "learning_rate": 3.4824774314426844e-05,
      "loss": 0.6648,
      "step": 1995700
    },
    {
      "epoch": 18.21118329805095,
      "grad_norm": 5.021740913391113,
      "learning_rate": 3.4824013918290874e-05,
      "loss": 0.6471,
      "step": 1995800
    },
    {
      "epoch": 18.212095773414116,
      "grad_norm": 4.057822227478027,
      "learning_rate": 3.4823253522154904e-05,
      "loss": 0.661,
      "step": 1995900
    },
    {
      "epoch": 18.21300824877728,
      "grad_norm": 4.681446075439453,
      "learning_rate": 3.4822493126018934e-05,
      "loss": 0.6674,
      "step": 1996000
    },
    {
      "epoch": 18.213920724140447,
      "grad_norm": 3.7493200302124023,
      "learning_rate": 3.482173272988296e-05,
      "loss": 0.6894,
      "step": 1996100
    },
    {
      "epoch": 18.214833199503612,
      "grad_norm": 4.232676982879639,
      "learning_rate": 3.4820972333746994e-05,
      "loss": 0.6823,
      "step": 1996200
    },
    {
      "epoch": 18.215745674866778,
      "grad_norm": 4.100653171539307,
      "learning_rate": 3.482021193761102e-05,
      "loss": 0.6525,
      "step": 1996300
    },
    {
      "epoch": 18.216658150229943,
      "grad_norm": 4.664899826049805,
      "learning_rate": 3.481945154147505e-05,
      "loss": 0.6814,
      "step": 1996400
    },
    {
      "epoch": 18.21757062559311,
      "grad_norm": 4.61256217956543,
      "learning_rate": 3.481869114533908e-05,
      "loss": 0.6813,
      "step": 1996500
    },
    {
      "epoch": 18.218483100956274,
      "grad_norm": 4.313982009887695,
      "learning_rate": 3.481793074920311e-05,
      "loss": 0.6906,
      "step": 1996600
    },
    {
      "epoch": 18.21939557631944,
      "grad_norm": 3.943955659866333,
      "learning_rate": 3.481717035306714e-05,
      "loss": 0.6092,
      "step": 1996700
    },
    {
      "epoch": 18.220308051682604,
      "grad_norm": 3.9755945205688477,
      "learning_rate": 3.481640995693116e-05,
      "loss": 0.6595,
      "step": 1996800
    },
    {
      "epoch": 18.22122052704577,
      "grad_norm": 4.377756595611572,
      "learning_rate": 3.481564956079519e-05,
      "loss": 0.6516,
      "step": 1996900
    },
    {
      "epoch": 18.222133002408935,
      "grad_norm": 4.47023868560791,
      "learning_rate": 3.481488916465922e-05,
      "loss": 0.6593,
      "step": 1997000
    },
    {
      "epoch": 18.2230454777721,
      "grad_norm": 3.4381215572357178,
      "learning_rate": 3.481412876852325e-05,
      "loss": 0.6427,
      "step": 1997100
    },
    {
      "epoch": 18.223957953135265,
      "grad_norm": 3.4642717838287354,
      "learning_rate": 3.481336837238728e-05,
      "loss": 0.6383,
      "step": 1997200
    },
    {
      "epoch": 18.22487042849843,
      "grad_norm": 3.6336522102355957,
      "learning_rate": 3.481260797625131e-05,
      "loss": 0.6531,
      "step": 1997300
    },
    {
      "epoch": 18.225782903861596,
      "grad_norm": 2.8060648441314697,
      "learning_rate": 3.4811847580115335e-05,
      "loss": 0.649,
      "step": 1997400
    },
    {
      "epoch": 18.22669537922476,
      "grad_norm": 3.4382519721984863,
      "learning_rate": 3.481108718397937e-05,
      "loss": 0.6565,
      "step": 1997500
    },
    {
      "epoch": 18.227607854587927,
      "grad_norm": 4.125943660736084,
      "learning_rate": 3.4810326787843395e-05,
      "loss": 0.6627,
      "step": 1997600
    },
    {
      "epoch": 18.228520329951092,
      "grad_norm": 3.9893155097961426,
      "learning_rate": 3.4809566391707425e-05,
      "loss": 0.6556,
      "step": 1997700
    },
    {
      "epoch": 18.229432805314257,
      "grad_norm": 3.1008284091949463,
      "learning_rate": 3.4808805995571455e-05,
      "loss": 0.6237,
      "step": 1997800
    },
    {
      "epoch": 18.230345280677422,
      "grad_norm": 3.2470052242279053,
      "learning_rate": 3.4808045599435485e-05,
      "loss": 0.6311,
      "step": 1997900
    },
    {
      "epoch": 18.231257756040588,
      "grad_norm": 4.047959804534912,
      "learning_rate": 3.480728520329951e-05,
      "loss": 0.6915,
      "step": 1998000
    },
    {
      "epoch": 18.232170231403753,
      "grad_norm": 2.832226276397705,
      "learning_rate": 3.4806524807163545e-05,
      "loss": 0.62,
      "step": 1998100
    },
    {
      "epoch": 18.23308270676692,
      "grad_norm": 3.795433282852173,
      "learning_rate": 3.480576441102757e-05,
      "loss": 0.6359,
      "step": 1998200
    },
    {
      "epoch": 18.233995182130084,
      "grad_norm": 4.322950839996338,
      "learning_rate": 3.48050040148916e-05,
      "loss": 0.6662,
      "step": 1998300
    },
    {
      "epoch": 18.23490765749325,
      "grad_norm": 4.373111248016357,
      "learning_rate": 3.480424361875563e-05,
      "loss": 0.6526,
      "step": 1998400
    },
    {
      "epoch": 18.235820132856414,
      "grad_norm": 3.5330235958099365,
      "learning_rate": 3.480348322261966e-05,
      "loss": 0.6492,
      "step": 1998500
    },
    {
      "epoch": 18.23673260821958,
      "grad_norm": 3.7230546474456787,
      "learning_rate": 3.480272282648369e-05,
      "loss": 0.634,
      "step": 1998600
    },
    {
      "epoch": 18.237645083582745,
      "grad_norm": 3.8902764320373535,
      "learning_rate": 3.480196243034772e-05,
      "loss": 0.6448,
      "step": 1998700
    },
    {
      "epoch": 18.23855755894591,
      "grad_norm": 3.566563844680786,
      "learning_rate": 3.480120203421174e-05,
      "loss": 0.6582,
      "step": 1998800
    },
    {
      "epoch": 18.239470034309072,
      "grad_norm": 3.872302770614624,
      "learning_rate": 3.480044163807578e-05,
      "loss": 0.6758,
      "step": 1998900
    },
    {
      "epoch": 18.240382509672237,
      "grad_norm": 2.42870831489563,
      "learning_rate": 3.47996812419398e-05,
      "loss": 0.6591,
      "step": 1999000
    },
    {
      "epoch": 18.241294985035402,
      "grad_norm": 4.291169166564941,
      "learning_rate": 3.479892084580383e-05,
      "loss": 0.623,
      "step": 1999100
    },
    {
      "epoch": 18.242207460398568,
      "grad_norm": 4.132530689239502,
      "learning_rate": 3.479816044966786e-05,
      "loss": 0.605,
      "step": 1999200
    },
    {
      "epoch": 18.243119935761733,
      "grad_norm": 4.103947639465332,
      "learning_rate": 3.479740005353189e-05,
      "loss": 0.633,
      "step": 1999300
    },
    {
      "epoch": 18.2440324111249,
      "grad_norm": 4.280576705932617,
      "learning_rate": 3.4796639657395916e-05,
      "loss": 0.6848,
      "step": 1999400
    },
    {
      "epoch": 18.244944886488064,
      "grad_norm": 4.037064552307129,
      "learning_rate": 3.4795879261259946e-05,
      "loss": 0.6345,
      "step": 1999500
    },
    {
      "epoch": 18.24585736185123,
      "grad_norm": 4.553775310516357,
      "learning_rate": 3.4795118865123976e-05,
      "loss": 0.6907,
      "step": 1999600
    },
    {
      "epoch": 18.246769837214394,
      "grad_norm": 2.9651873111724854,
      "learning_rate": 3.4794358468988006e-05,
      "loss": 0.6329,
      "step": 1999700
    },
    {
      "epoch": 18.24768231257756,
      "grad_norm": 3.3291850090026855,
      "learning_rate": 3.4793598072852036e-05,
      "loss": 0.6419,
      "step": 1999800
    },
    {
      "epoch": 18.248594787940725,
      "grad_norm": 3.6262929439544678,
      "learning_rate": 3.479283767671606e-05,
      "loss": 0.6695,
      "step": 1999900
    },
    {
      "epoch": 18.24950726330389,
      "grad_norm": 3.2055296897888184,
      "learning_rate": 3.4792077280580096e-05,
      "loss": 0.6962,
      "step": 2000000
    },
    {
      "epoch": 18.250419738667055,
      "grad_norm": 3.963540554046631,
      "learning_rate": 3.479131688444412e-05,
      "loss": 0.6502,
      "step": 2000100
    },
    {
      "epoch": 18.25133221403022,
      "grad_norm": 3.8179028034210205,
      "learning_rate": 3.479055648830815e-05,
      "loss": 0.6713,
      "step": 2000200
    },
    {
      "epoch": 18.252244689393386,
      "grad_norm": 4.505207061767578,
      "learning_rate": 3.478979609217218e-05,
      "loss": 0.667,
      "step": 2000300
    },
    {
      "epoch": 18.25315716475655,
      "grad_norm": 4.077126979827881,
      "learning_rate": 3.478903569603621e-05,
      "loss": 0.6581,
      "step": 2000400
    },
    {
      "epoch": 18.254069640119717,
      "grad_norm": 4.616117477416992,
      "learning_rate": 3.478827529990023e-05,
      "loss": 0.6282,
      "step": 2000500
    },
    {
      "epoch": 18.254982115482882,
      "grad_norm": 3.891291379928589,
      "learning_rate": 3.478751490376427e-05,
      "loss": 0.5977,
      "step": 2000600
    },
    {
      "epoch": 18.255894590846047,
      "grad_norm": 1.813482642173767,
      "learning_rate": 3.478675450762829e-05,
      "loss": 0.6608,
      "step": 2000700
    },
    {
      "epoch": 18.256807066209213,
      "grad_norm": 3.627248764038086,
      "learning_rate": 3.478599411149232e-05,
      "loss": 0.6695,
      "step": 2000800
    },
    {
      "epoch": 18.257719541572378,
      "grad_norm": 4.72678279876709,
      "learning_rate": 3.4785233715356353e-05,
      "loss": 0.6722,
      "step": 2000900
    },
    {
      "epoch": 18.258632016935543,
      "grad_norm": 4.571444511413574,
      "learning_rate": 3.4784473319220383e-05,
      "loss": 0.6689,
      "step": 2001000
    },
    {
      "epoch": 18.25954449229871,
      "grad_norm": 3.4357333183288574,
      "learning_rate": 3.4783712923084414e-05,
      "loss": 0.642,
      "step": 2001100
    },
    {
      "epoch": 18.260456967661874,
      "grad_norm": 4.0380377769470215,
      "learning_rate": 3.4782952526948444e-05,
      "loss": 0.6805,
      "step": 2001200
    },
    {
      "epoch": 18.26136944302504,
      "grad_norm": 4.26212739944458,
      "learning_rate": 3.478219213081247e-05,
      "loss": 0.6777,
      "step": 2001300
    },
    {
      "epoch": 18.262281918388204,
      "grad_norm": 4.577739238739014,
      "learning_rate": 3.4781431734676504e-05,
      "loss": 0.639,
      "step": 2001400
    },
    {
      "epoch": 18.26319439375137,
      "grad_norm": 4.253558158874512,
      "learning_rate": 3.478067133854053e-05,
      "loss": 0.6618,
      "step": 2001500
    },
    {
      "epoch": 18.264106869114535,
      "grad_norm": 4.131682872772217,
      "learning_rate": 3.477991094240456e-05,
      "loss": 0.6763,
      "step": 2001600
    },
    {
      "epoch": 18.2650193444777,
      "grad_norm": 4.521742343902588,
      "learning_rate": 3.477915054626859e-05,
      "loss": 0.6865,
      "step": 2001700
    },
    {
      "epoch": 18.265931819840866,
      "grad_norm": 2.3341383934020996,
      "learning_rate": 3.477839015013262e-05,
      "loss": 0.6157,
      "step": 2001800
    },
    {
      "epoch": 18.26684429520403,
      "grad_norm": 4.106389045715332,
      "learning_rate": 3.477762975399664e-05,
      "loss": 0.6985,
      "step": 2001900
    },
    {
      "epoch": 18.267756770567196,
      "grad_norm": 2.658644437789917,
      "learning_rate": 3.477686935786068e-05,
      "loss": 0.6794,
      "step": 2002000
    },
    {
      "epoch": 18.26866924593036,
      "grad_norm": 3.3865890502929688,
      "learning_rate": 3.47761089617247e-05,
      "loss": 0.675,
      "step": 2002100
    },
    {
      "epoch": 18.269581721293527,
      "grad_norm": 3.4218175411224365,
      "learning_rate": 3.477534856558873e-05,
      "loss": 0.6704,
      "step": 2002200
    },
    {
      "epoch": 18.270494196656692,
      "grad_norm": 4.309598922729492,
      "learning_rate": 3.477458816945276e-05,
      "loss": 0.6326,
      "step": 2002300
    },
    {
      "epoch": 18.271406672019854,
      "grad_norm": 3.1917033195495605,
      "learning_rate": 3.4773827773316784e-05,
      "loss": 0.667,
      "step": 2002400
    },
    {
      "epoch": 18.27231914738302,
      "grad_norm": 3.2309536933898926,
      "learning_rate": 3.477306737718082e-05,
      "loss": 0.6466,
      "step": 2002500
    },
    {
      "epoch": 18.273231622746184,
      "grad_norm": 3.7967019081115723,
      "learning_rate": 3.4772306981044844e-05,
      "loss": 0.6816,
      "step": 2002600
    },
    {
      "epoch": 18.27414409810935,
      "grad_norm": 4.573647499084473,
      "learning_rate": 3.4771546584908874e-05,
      "loss": 0.6362,
      "step": 2002700
    },
    {
      "epoch": 18.275056573472515,
      "grad_norm": 4.0732316970825195,
      "learning_rate": 3.4770786188772904e-05,
      "loss": 0.688,
      "step": 2002800
    },
    {
      "epoch": 18.27596904883568,
      "grad_norm": 3.8235952854156494,
      "learning_rate": 3.4770025792636934e-05,
      "loss": 0.6285,
      "step": 2002900
    },
    {
      "epoch": 18.276881524198846,
      "grad_norm": 4.221161842346191,
      "learning_rate": 3.476926539650096e-05,
      "loss": 0.6158,
      "step": 2003000
    },
    {
      "epoch": 18.27779399956201,
      "grad_norm": 4.349632263183594,
      "learning_rate": 3.4768505000364995e-05,
      "loss": 0.6367,
      "step": 2003100
    },
    {
      "epoch": 18.278706474925176,
      "grad_norm": 3.143781900405884,
      "learning_rate": 3.476774460422902e-05,
      "loss": 0.655,
      "step": 2003200
    },
    {
      "epoch": 18.27961895028834,
      "grad_norm": 3.940410614013672,
      "learning_rate": 3.476698420809305e-05,
      "loss": 0.6638,
      "step": 2003300
    },
    {
      "epoch": 18.280531425651507,
      "grad_norm": 3.955634593963623,
      "learning_rate": 3.476622381195708e-05,
      "loss": 0.6549,
      "step": 2003400
    },
    {
      "epoch": 18.281443901014672,
      "grad_norm": 4.1489105224609375,
      "learning_rate": 3.476546341582111e-05,
      "loss": 0.6544,
      "step": 2003500
    },
    {
      "epoch": 18.282356376377837,
      "grad_norm": 3.8661258220672607,
      "learning_rate": 3.476470301968514e-05,
      "loss": 0.6761,
      "step": 2003600
    },
    {
      "epoch": 18.283268851741003,
      "grad_norm": 3.5870211124420166,
      "learning_rate": 3.476394262354917e-05,
      "loss": 0.6582,
      "step": 2003700
    },
    {
      "epoch": 18.284181327104168,
      "grad_norm": 4.686864852905273,
      "learning_rate": 3.476318222741319e-05,
      "loss": 0.6615,
      "step": 2003800
    },
    {
      "epoch": 18.285093802467333,
      "grad_norm": 4.449038028717041,
      "learning_rate": 3.476242183127723e-05,
      "loss": 0.646,
      "step": 2003900
    },
    {
      "epoch": 18.2860062778305,
      "grad_norm": 4.209463119506836,
      "learning_rate": 3.476166143514125e-05,
      "loss": 0.6875,
      "step": 2004000
    },
    {
      "epoch": 18.286918753193664,
      "grad_norm": 3.853196382522583,
      "learning_rate": 3.476090103900528e-05,
      "loss": 0.6533,
      "step": 2004100
    },
    {
      "epoch": 18.28783122855683,
      "grad_norm": 4.224565505981445,
      "learning_rate": 3.476014064286931e-05,
      "loss": 0.6297,
      "step": 2004200
    },
    {
      "epoch": 18.288743703919994,
      "grad_norm": 3.4686245918273926,
      "learning_rate": 3.475938024673334e-05,
      "loss": 0.6596,
      "step": 2004300
    },
    {
      "epoch": 18.28965617928316,
      "grad_norm": 3.1489508152008057,
      "learning_rate": 3.4758619850597365e-05,
      "loss": 0.6485,
      "step": 2004400
    },
    {
      "epoch": 18.290568654646325,
      "grad_norm": 4.037525653839111,
      "learning_rate": 3.47578594544614e-05,
      "loss": 0.686,
      "step": 2004500
    },
    {
      "epoch": 18.29148113000949,
      "grad_norm": 4.826279640197754,
      "learning_rate": 3.4757099058325425e-05,
      "loss": 0.6817,
      "step": 2004600
    },
    {
      "epoch": 18.292393605372656,
      "grad_norm": 3.334751844406128,
      "learning_rate": 3.4756338662189455e-05,
      "loss": 0.6301,
      "step": 2004700
    },
    {
      "epoch": 18.29330608073582,
      "grad_norm": 3.805591344833374,
      "learning_rate": 3.4755578266053485e-05,
      "loss": 0.6565,
      "step": 2004800
    },
    {
      "epoch": 18.294218556098986,
      "grad_norm": 3.4781720638275146,
      "learning_rate": 3.4754817869917515e-05,
      "loss": 0.6894,
      "step": 2004900
    },
    {
      "epoch": 18.29513103146215,
      "grad_norm": 4.208785533905029,
      "learning_rate": 3.4754057473781546e-05,
      "loss": 0.6578,
      "step": 2005000
    },
    {
      "epoch": 18.296043506825317,
      "grad_norm": 3.3229002952575684,
      "learning_rate": 3.4753297077645576e-05,
      "loss": 0.6454,
      "step": 2005100
    },
    {
      "epoch": 18.296955982188482,
      "grad_norm": 4.989882469177246,
      "learning_rate": 3.47525366815096e-05,
      "loss": 0.6506,
      "step": 2005200
    },
    {
      "epoch": 18.297868457551647,
      "grad_norm": 3.9290809631347656,
      "learning_rate": 3.475177628537363e-05,
      "loss": 0.7127,
      "step": 2005300
    },
    {
      "epoch": 18.298780932914813,
      "grad_norm": 3.1454529762268066,
      "learning_rate": 3.475101588923766e-05,
      "loss": 0.6567,
      "step": 2005400
    },
    {
      "epoch": 18.299693408277978,
      "grad_norm": 3.1118528842926025,
      "learning_rate": 3.475025549310168e-05,
      "loss": 0.681,
      "step": 2005500
    },
    {
      "epoch": 18.300605883641143,
      "grad_norm": 3.353733777999878,
      "learning_rate": 3.474949509696572e-05,
      "loss": 0.6371,
      "step": 2005600
    },
    {
      "epoch": 18.301518359004305,
      "grad_norm": 4.43112325668335,
      "learning_rate": 3.474873470082974e-05,
      "loss": 0.6888,
      "step": 2005700
    },
    {
      "epoch": 18.30243083436747,
      "grad_norm": 4.356060981750488,
      "learning_rate": 3.474797430469377e-05,
      "loss": 0.6545,
      "step": 2005800
    },
    {
      "epoch": 18.303343309730636,
      "grad_norm": 2.8494129180908203,
      "learning_rate": 3.47472139085578e-05,
      "loss": 0.649,
      "step": 2005900
    },
    {
      "epoch": 18.3042557850938,
      "grad_norm": 2.7209556102752686,
      "learning_rate": 3.474645351242183e-05,
      "loss": 0.6624,
      "step": 2006000
    },
    {
      "epoch": 18.305168260456966,
      "grad_norm": 4.593338966369629,
      "learning_rate": 3.474569311628586e-05,
      "loss": 0.6405,
      "step": 2006100
    },
    {
      "epoch": 18.30608073582013,
      "grad_norm": 4.085873603820801,
      "learning_rate": 3.474493272014989e-05,
      "loss": 0.6624,
      "step": 2006200
    },
    {
      "epoch": 18.306993211183297,
      "grad_norm": 3.8900110721588135,
      "learning_rate": 3.4744172324013916e-05,
      "loss": 0.6389,
      "step": 2006300
    },
    {
      "epoch": 18.307905686546462,
      "grad_norm": 3.3825786113739014,
      "learning_rate": 3.474341192787795e-05,
      "loss": 0.6602,
      "step": 2006400
    },
    {
      "epoch": 18.308818161909628,
      "grad_norm": 4.757246971130371,
      "learning_rate": 3.4742651531741976e-05,
      "loss": 0.6922,
      "step": 2006500
    },
    {
      "epoch": 18.309730637272793,
      "grad_norm": 4.296060562133789,
      "learning_rate": 3.4741891135606006e-05,
      "loss": 0.6575,
      "step": 2006600
    },
    {
      "epoch": 18.310643112635958,
      "grad_norm": 4.286362171173096,
      "learning_rate": 3.4741130739470036e-05,
      "loss": 0.697,
      "step": 2006700
    },
    {
      "epoch": 18.311555587999123,
      "grad_norm": 4.1171956062316895,
      "learning_rate": 3.4740370343334066e-05,
      "loss": 0.6393,
      "step": 2006800
    },
    {
      "epoch": 18.31246806336229,
      "grad_norm": 4.707981586456299,
      "learning_rate": 3.473960994719809e-05,
      "loss": 0.6444,
      "step": 2006900
    },
    {
      "epoch": 18.313380538725454,
      "grad_norm": 2.8702259063720703,
      "learning_rate": 3.4738849551062127e-05,
      "loss": 0.6417,
      "step": 2007000
    },
    {
      "epoch": 18.31429301408862,
      "grad_norm": 3.9063868522644043,
      "learning_rate": 3.473808915492615e-05,
      "loss": 0.6867,
      "step": 2007100
    },
    {
      "epoch": 18.315205489451785,
      "grad_norm": 4.810158729553223,
      "learning_rate": 3.473732875879018e-05,
      "loss": 0.6624,
      "step": 2007200
    },
    {
      "epoch": 18.31611796481495,
      "grad_norm": 4.173300743103027,
      "learning_rate": 3.473656836265421e-05,
      "loss": 0.6545,
      "step": 2007300
    },
    {
      "epoch": 18.317030440178115,
      "grad_norm": 3.896460771560669,
      "learning_rate": 3.473580796651824e-05,
      "loss": 0.6458,
      "step": 2007400
    },
    {
      "epoch": 18.31794291554128,
      "grad_norm": 3.359036684036255,
      "learning_rate": 3.473504757038227e-05,
      "loss": 0.6463,
      "step": 2007500
    },
    {
      "epoch": 18.318855390904446,
      "grad_norm": 4.737515926361084,
      "learning_rate": 3.47342871742463e-05,
      "loss": 0.6558,
      "step": 2007600
    },
    {
      "epoch": 18.31976786626761,
      "grad_norm": 4.359951972961426,
      "learning_rate": 3.4733526778110323e-05,
      "loss": 0.6553,
      "step": 2007700
    },
    {
      "epoch": 18.320680341630776,
      "grad_norm": 3.756856679916382,
      "learning_rate": 3.473276638197436e-05,
      "loss": 0.7089,
      "step": 2007800
    },
    {
      "epoch": 18.32159281699394,
      "grad_norm": 3.7494235038757324,
      "learning_rate": 3.4732005985838384e-05,
      "loss": 0.6492,
      "step": 2007900
    },
    {
      "epoch": 18.322505292357107,
      "grad_norm": 3.6189820766448975,
      "learning_rate": 3.4731245589702414e-05,
      "loss": 0.6443,
      "step": 2008000
    },
    {
      "epoch": 18.323417767720272,
      "grad_norm": 3.4557411670684814,
      "learning_rate": 3.4730485193566444e-05,
      "loss": 0.6311,
      "step": 2008100
    },
    {
      "epoch": 18.324330243083438,
      "grad_norm": 3.8645055294036865,
      "learning_rate": 3.472972479743047e-05,
      "loss": 0.6443,
      "step": 2008200
    },
    {
      "epoch": 18.325242718446603,
      "grad_norm": 3.977210521697998,
      "learning_rate": 3.47289644012945e-05,
      "loss": 0.6804,
      "step": 2008300
    },
    {
      "epoch": 18.326155193809768,
      "grad_norm": 5.087830543518066,
      "learning_rate": 3.472820400515853e-05,
      "loss": 0.6706,
      "step": 2008400
    },
    {
      "epoch": 18.327067669172934,
      "grad_norm": 3.9764974117279053,
      "learning_rate": 3.472744360902256e-05,
      "loss": 0.7134,
      "step": 2008500
    },
    {
      "epoch": 18.3279801445361,
      "grad_norm": 3.289555788040161,
      "learning_rate": 3.472668321288659e-05,
      "loss": 0.6578,
      "step": 2008600
    },
    {
      "epoch": 18.328892619899264,
      "grad_norm": 4.259182453155518,
      "learning_rate": 3.472592281675062e-05,
      "loss": 0.6806,
      "step": 2008700
    },
    {
      "epoch": 18.32980509526243,
      "grad_norm": 4.70968770980835,
      "learning_rate": 3.472516242061464e-05,
      "loss": 0.6759,
      "step": 2008800
    },
    {
      "epoch": 18.330717570625595,
      "grad_norm": 3.787022352218628,
      "learning_rate": 3.472440202447868e-05,
      "loss": 0.6758,
      "step": 2008900
    },
    {
      "epoch": 18.33163004598876,
      "grad_norm": 3.275594711303711,
      "learning_rate": 3.47236416283427e-05,
      "loss": 0.6824,
      "step": 2009000
    },
    {
      "epoch": 18.332542521351925,
      "grad_norm": 3.4721133708953857,
      "learning_rate": 3.472288123220673e-05,
      "loss": 0.684,
      "step": 2009100
    },
    {
      "epoch": 18.333454996715087,
      "grad_norm": 3.0141143798828125,
      "learning_rate": 3.472212083607076e-05,
      "loss": 0.7012,
      "step": 2009200
    },
    {
      "epoch": 18.334367472078252,
      "grad_norm": 4.84250020980835,
      "learning_rate": 3.472136043993479e-05,
      "loss": 0.6493,
      "step": 2009300
    },
    {
      "epoch": 18.335279947441418,
      "grad_norm": 4.255535125732422,
      "learning_rate": 3.472060004379882e-05,
      "loss": 0.6768,
      "step": 2009400
    },
    {
      "epoch": 18.336192422804583,
      "grad_norm": 4.315920352935791,
      "learning_rate": 3.471983964766285e-05,
      "loss": 0.6443,
      "step": 2009500
    },
    {
      "epoch": 18.33710489816775,
      "grad_norm": 4.827982425689697,
      "learning_rate": 3.4719079251526874e-05,
      "loss": 0.6626,
      "step": 2009600
    },
    {
      "epoch": 18.338017373530914,
      "grad_norm": 3.6389005184173584,
      "learning_rate": 3.4718318855390904e-05,
      "loss": 0.6969,
      "step": 2009700
    },
    {
      "epoch": 18.33892984889408,
      "grad_norm": 2.5474252700805664,
      "learning_rate": 3.4717558459254935e-05,
      "loss": 0.6511,
      "step": 2009800
    },
    {
      "epoch": 18.339842324257244,
      "grad_norm": 4.239081859588623,
      "learning_rate": 3.4716798063118965e-05,
      "loss": 0.677,
      "step": 2009900
    },
    {
      "epoch": 18.34075479962041,
      "grad_norm": 5.167774200439453,
      "learning_rate": 3.4716037666982995e-05,
      "loss": 0.6853,
      "step": 2010000
    },
    {
      "epoch": 18.341667274983575,
      "grad_norm": 3.7571325302124023,
      "learning_rate": 3.4715277270847025e-05,
      "loss": 0.6634,
      "step": 2010100
    },
    {
      "epoch": 18.34257975034674,
      "grad_norm": 4.426602363586426,
      "learning_rate": 3.471451687471105e-05,
      "loss": 0.6604,
      "step": 2010200
    },
    {
      "epoch": 18.343492225709905,
      "grad_norm": 4.40079927444458,
      "learning_rate": 3.4713756478575085e-05,
      "loss": 0.6266,
      "step": 2010300
    },
    {
      "epoch": 18.34440470107307,
      "grad_norm": 3.3885183334350586,
      "learning_rate": 3.471299608243911e-05,
      "loss": 0.6439,
      "step": 2010400
    },
    {
      "epoch": 18.345317176436236,
      "grad_norm": 4.646867752075195,
      "learning_rate": 3.471223568630314e-05,
      "loss": 0.6299,
      "step": 2010500
    },
    {
      "epoch": 18.3462296517994,
      "grad_norm": 3.4549331665039062,
      "learning_rate": 3.471147529016717e-05,
      "loss": 0.7125,
      "step": 2010600
    },
    {
      "epoch": 18.347142127162567,
      "grad_norm": 4.4642839431762695,
      "learning_rate": 3.47107148940312e-05,
      "loss": 0.6598,
      "step": 2010700
    },
    {
      "epoch": 18.348054602525732,
      "grad_norm": 4.6896562576293945,
      "learning_rate": 3.470995449789523e-05,
      "loss": 0.6544,
      "step": 2010800
    },
    {
      "epoch": 18.348967077888897,
      "grad_norm": 2.755995988845825,
      "learning_rate": 3.470919410175925e-05,
      "loss": 0.6524,
      "step": 2010900
    },
    {
      "epoch": 18.349879553252062,
      "grad_norm": 4.3134260177612305,
      "learning_rate": 3.470843370562328e-05,
      "loss": 0.6686,
      "step": 2011000
    },
    {
      "epoch": 18.350792028615228,
      "grad_norm": 3.446561574935913,
      "learning_rate": 3.470767330948731e-05,
      "loss": 0.6741,
      "step": 2011100
    },
    {
      "epoch": 18.351704503978393,
      "grad_norm": 3.4502692222595215,
      "learning_rate": 3.470691291335134e-05,
      "loss": 0.6788,
      "step": 2011200
    },
    {
      "epoch": 18.35261697934156,
      "grad_norm": 3.741881847381592,
      "learning_rate": 3.4706152517215365e-05,
      "loss": 0.6875,
      "step": 2011300
    },
    {
      "epoch": 18.353529454704724,
      "grad_norm": 3.4625067710876465,
      "learning_rate": 3.47053921210794e-05,
      "loss": 0.6398,
      "step": 2011400
    },
    {
      "epoch": 18.35444193006789,
      "grad_norm": 3.7277541160583496,
      "learning_rate": 3.4704631724943425e-05,
      "loss": 0.6514,
      "step": 2011500
    },
    {
      "epoch": 18.355354405431054,
      "grad_norm": 3.933931827545166,
      "learning_rate": 3.4703871328807455e-05,
      "loss": 0.6233,
      "step": 2011600
    },
    {
      "epoch": 18.35626688079422,
      "grad_norm": 4.273439407348633,
      "learning_rate": 3.4703110932671485e-05,
      "loss": 0.6668,
      "step": 2011700
    },
    {
      "epoch": 18.357179356157385,
      "grad_norm": 4.176785469055176,
      "learning_rate": 3.4702350536535516e-05,
      "loss": 0.6438,
      "step": 2011800
    },
    {
      "epoch": 18.35809183152055,
      "grad_norm": 3.6169490814208984,
      "learning_rate": 3.4701590140399546e-05,
      "loss": 0.6502,
      "step": 2011900
    },
    {
      "epoch": 18.359004306883715,
      "grad_norm": 2.677272319793701,
      "learning_rate": 3.4700829744263576e-05,
      "loss": 0.6279,
      "step": 2012000
    },
    {
      "epoch": 18.35991678224688,
      "grad_norm": 4.494105339050293,
      "learning_rate": 3.47000693481276e-05,
      "loss": 0.6693,
      "step": 2012100
    },
    {
      "epoch": 18.360829257610046,
      "grad_norm": 3.5075583457946777,
      "learning_rate": 3.4699308951991636e-05,
      "loss": 0.6552,
      "step": 2012200
    },
    {
      "epoch": 18.36174173297321,
      "grad_norm": 3.584482431411743,
      "learning_rate": 3.469854855585566e-05,
      "loss": 0.6563,
      "step": 2012300
    },
    {
      "epoch": 18.362654208336377,
      "grad_norm": 3.6205501556396484,
      "learning_rate": 3.469778815971969e-05,
      "loss": 0.6405,
      "step": 2012400
    },
    {
      "epoch": 18.36356668369954,
      "grad_norm": 4.481043815612793,
      "learning_rate": 3.469702776358372e-05,
      "loss": 0.6412,
      "step": 2012500
    },
    {
      "epoch": 18.364479159062704,
      "grad_norm": 4.008182048797607,
      "learning_rate": 3.469626736744775e-05,
      "loss": 0.6397,
      "step": 2012600
    },
    {
      "epoch": 18.36539163442587,
      "grad_norm": 3.352322816848755,
      "learning_rate": 3.469550697131177e-05,
      "loss": 0.6652,
      "step": 2012700
    },
    {
      "epoch": 18.366304109789034,
      "grad_norm": 3.85193133354187,
      "learning_rate": 3.469474657517581e-05,
      "loss": 0.6152,
      "step": 2012800
    },
    {
      "epoch": 18.3672165851522,
      "grad_norm": 3.854661464691162,
      "learning_rate": 3.469398617903983e-05,
      "loss": 0.6402,
      "step": 2012900
    },
    {
      "epoch": 18.368129060515365,
      "grad_norm": 4.555644989013672,
      "learning_rate": 3.469322578290386e-05,
      "loss": 0.6132,
      "step": 2013000
    },
    {
      "epoch": 18.36904153587853,
      "grad_norm": 3.4894638061523438,
      "learning_rate": 3.469246538676789e-05,
      "loss": 0.6404,
      "step": 2013100
    },
    {
      "epoch": 18.369954011241695,
      "grad_norm": 3.9716763496398926,
      "learning_rate": 3.469170499063192e-05,
      "loss": 0.6786,
      "step": 2013200
    },
    {
      "epoch": 18.37086648660486,
      "grad_norm": 3.786987066268921,
      "learning_rate": 3.469094459449595e-05,
      "loss": 0.6461,
      "step": 2013300
    },
    {
      "epoch": 18.371778961968026,
      "grad_norm": 4.291545391082764,
      "learning_rate": 3.469018419835998e-05,
      "loss": 0.6639,
      "step": 2013400
    },
    {
      "epoch": 18.37269143733119,
      "grad_norm": 4.386203765869141,
      "learning_rate": 3.4689423802224006e-05,
      "loss": 0.6622,
      "step": 2013500
    },
    {
      "epoch": 18.373603912694357,
      "grad_norm": 4.186626434326172,
      "learning_rate": 3.468866340608804e-05,
      "loss": 0.6521,
      "step": 2013600
    },
    {
      "epoch": 18.374516388057522,
      "grad_norm": 4.300507068634033,
      "learning_rate": 3.4687903009952066e-05,
      "loss": 0.6629,
      "step": 2013700
    },
    {
      "epoch": 18.375428863420687,
      "grad_norm": 4.000420093536377,
      "learning_rate": 3.468714261381609e-05,
      "loss": 0.6645,
      "step": 2013800
    },
    {
      "epoch": 18.376341338783853,
      "grad_norm": 3.9117484092712402,
      "learning_rate": 3.468638221768013e-05,
      "loss": 0.6334,
      "step": 2013900
    },
    {
      "epoch": 18.377253814147018,
      "grad_norm": 4.198607921600342,
      "learning_rate": 3.468562182154415e-05,
      "loss": 0.6803,
      "step": 2014000
    },
    {
      "epoch": 18.378166289510183,
      "grad_norm": 3.505486249923706,
      "learning_rate": 3.468486142540818e-05,
      "loss": 0.7019,
      "step": 2014100
    },
    {
      "epoch": 18.37907876487335,
      "grad_norm": 3.748321294784546,
      "learning_rate": 3.468410102927221e-05,
      "loss": 0.6486,
      "step": 2014200
    },
    {
      "epoch": 18.379991240236514,
      "grad_norm": 4.631587028503418,
      "learning_rate": 3.468334063313624e-05,
      "loss": 0.6568,
      "step": 2014300
    },
    {
      "epoch": 18.38090371559968,
      "grad_norm": 4.457948207855225,
      "learning_rate": 3.468258023700027e-05,
      "loss": 0.6749,
      "step": 2014400
    },
    {
      "epoch": 18.381816190962844,
      "grad_norm": 4.3915534019470215,
      "learning_rate": 3.46818198408643e-05,
      "loss": 0.6304,
      "step": 2014500
    },
    {
      "epoch": 18.38272866632601,
      "grad_norm": 5.223546028137207,
      "learning_rate": 3.4681059444728324e-05,
      "loss": 0.6763,
      "step": 2014600
    },
    {
      "epoch": 18.383641141689175,
      "grad_norm": 3.696729898452759,
      "learning_rate": 3.468029904859236e-05,
      "loss": 0.6183,
      "step": 2014700
    },
    {
      "epoch": 18.38455361705234,
      "grad_norm": 4.077638626098633,
      "learning_rate": 3.4679538652456384e-05,
      "loss": 0.6437,
      "step": 2014800
    },
    {
      "epoch": 18.385466092415506,
      "grad_norm": 3.8178281784057617,
      "learning_rate": 3.4678778256320414e-05,
      "loss": 0.6572,
      "step": 2014900
    },
    {
      "epoch": 18.38637856777867,
      "grad_norm": 4.66951847076416,
      "learning_rate": 3.4678017860184444e-05,
      "loss": 0.6305,
      "step": 2015000
    },
    {
      "epoch": 18.387291043141836,
      "grad_norm": 5.659193515777588,
      "learning_rate": 3.4677257464048474e-05,
      "loss": 0.6731,
      "step": 2015100
    },
    {
      "epoch": 18.388203518505,
      "grad_norm": 3.871641159057617,
      "learning_rate": 3.46764970679125e-05,
      "loss": 0.6468,
      "step": 2015200
    },
    {
      "epoch": 18.389115993868167,
      "grad_norm": 3.6230344772338867,
      "learning_rate": 3.4675736671776534e-05,
      "loss": 0.6564,
      "step": 2015300
    },
    {
      "epoch": 18.390028469231332,
      "grad_norm": 3.5793466567993164,
      "learning_rate": 3.467497627564056e-05,
      "loss": 0.6853,
      "step": 2015400
    },
    {
      "epoch": 18.390940944594497,
      "grad_norm": 3.510291814804077,
      "learning_rate": 3.467421587950459e-05,
      "loss": 0.6181,
      "step": 2015500
    },
    {
      "epoch": 18.391853419957663,
      "grad_norm": 4.143657684326172,
      "learning_rate": 3.467345548336862e-05,
      "loss": 0.6936,
      "step": 2015600
    },
    {
      "epoch": 18.392765895320828,
      "grad_norm": 3.753798723220825,
      "learning_rate": 3.467269508723265e-05,
      "loss": 0.6243,
      "step": 2015700
    },
    {
      "epoch": 18.393678370683993,
      "grad_norm": 4.023031234741211,
      "learning_rate": 3.467193469109668e-05,
      "loss": 0.6946,
      "step": 2015800
    },
    {
      "epoch": 18.394590846047155,
      "grad_norm": 4.568119525909424,
      "learning_rate": 3.467117429496071e-05,
      "loss": 0.6698,
      "step": 2015900
    },
    {
      "epoch": 18.39550332141032,
      "grad_norm": 4.308811187744141,
      "learning_rate": 3.467041389882473e-05,
      "loss": 0.6358,
      "step": 2016000
    },
    {
      "epoch": 18.396415796773486,
      "grad_norm": 3.480032205581665,
      "learning_rate": 3.466965350268877e-05,
      "loss": 0.6647,
      "step": 2016100
    },
    {
      "epoch": 18.39732827213665,
      "grad_norm": 4.291230201721191,
      "learning_rate": 3.466889310655279e-05,
      "loss": 0.6186,
      "step": 2016200
    },
    {
      "epoch": 18.398240747499816,
      "grad_norm": 4.360567569732666,
      "learning_rate": 3.466813271041682e-05,
      "loss": 0.6804,
      "step": 2016300
    },
    {
      "epoch": 18.39915322286298,
      "grad_norm": 4.143163681030273,
      "learning_rate": 3.466737231428085e-05,
      "loss": 0.6557,
      "step": 2016400
    },
    {
      "epoch": 18.400065698226147,
      "grad_norm": 3.92498779296875,
      "learning_rate": 3.466661191814488e-05,
      "loss": 0.6399,
      "step": 2016500
    },
    {
      "epoch": 18.400978173589312,
      "grad_norm": 3.5064961910247803,
      "learning_rate": 3.4665851522008905e-05,
      "loss": 0.6592,
      "step": 2016600
    },
    {
      "epoch": 18.401890648952477,
      "grad_norm": 3.638700485229492,
      "learning_rate": 3.4665091125872935e-05,
      "loss": 0.6623,
      "step": 2016700
    },
    {
      "epoch": 18.402803124315643,
      "grad_norm": 4.179712295532227,
      "learning_rate": 3.4664330729736965e-05,
      "loss": 0.6668,
      "step": 2016800
    },
    {
      "epoch": 18.403715599678808,
      "grad_norm": 3.8628175258636475,
      "learning_rate": 3.4663570333600995e-05,
      "loss": 0.6334,
      "step": 2016900
    },
    {
      "epoch": 18.404628075041973,
      "grad_norm": 4.028865337371826,
      "learning_rate": 3.4662809937465025e-05,
      "loss": 0.6383,
      "step": 2017000
    },
    {
      "epoch": 18.40554055040514,
      "grad_norm": 3.533257007598877,
      "learning_rate": 3.466204954132905e-05,
      "loss": 0.6436,
      "step": 2017100
    },
    {
      "epoch": 18.406453025768304,
      "grad_norm": 3.430236339569092,
      "learning_rate": 3.4661289145193085e-05,
      "loss": 0.6558,
      "step": 2017200
    },
    {
      "epoch": 18.40736550113147,
      "grad_norm": 4.275951385498047,
      "learning_rate": 3.466052874905711e-05,
      "loss": 0.6611,
      "step": 2017300
    },
    {
      "epoch": 18.408277976494634,
      "grad_norm": 3.385530948638916,
      "learning_rate": 3.465976835292114e-05,
      "loss": 0.6821,
      "step": 2017400
    },
    {
      "epoch": 18.4091904518578,
      "grad_norm": 2.829254388809204,
      "learning_rate": 3.465900795678517e-05,
      "loss": 0.6642,
      "step": 2017500
    },
    {
      "epoch": 18.410102927220965,
      "grad_norm": 4.158469200134277,
      "learning_rate": 3.46582475606492e-05,
      "loss": 0.6154,
      "step": 2017600
    },
    {
      "epoch": 18.41101540258413,
      "grad_norm": 3.9022700786590576,
      "learning_rate": 3.465748716451322e-05,
      "loss": 0.6841,
      "step": 2017700
    },
    {
      "epoch": 18.411927877947296,
      "grad_norm": 4.117794990539551,
      "learning_rate": 3.465672676837726e-05,
      "loss": 0.6678,
      "step": 2017800
    },
    {
      "epoch": 18.41284035331046,
      "grad_norm": 3.7224783897399902,
      "learning_rate": 3.465596637224128e-05,
      "loss": 0.6784,
      "step": 2017900
    },
    {
      "epoch": 18.413752828673626,
      "grad_norm": 3.711467742919922,
      "learning_rate": 3.465520597610531e-05,
      "loss": 0.649,
      "step": 2018000
    },
    {
      "epoch": 18.41466530403679,
      "grad_norm": 3.1539855003356934,
      "learning_rate": 3.465444557996934e-05,
      "loss": 0.6673,
      "step": 2018100
    },
    {
      "epoch": 18.415577779399957,
      "grad_norm": 3.994568109512329,
      "learning_rate": 3.465368518383337e-05,
      "loss": 0.6487,
      "step": 2018200
    },
    {
      "epoch": 18.416490254763122,
      "grad_norm": 3.803131341934204,
      "learning_rate": 3.46529247876974e-05,
      "loss": 0.6348,
      "step": 2018300
    },
    {
      "epoch": 18.417402730126287,
      "grad_norm": 4.066296100616455,
      "learning_rate": 3.465216439156143e-05,
      "loss": 0.6168,
      "step": 2018400
    },
    {
      "epoch": 18.418315205489453,
      "grad_norm": 3.783745527267456,
      "learning_rate": 3.4651403995425455e-05,
      "loss": 0.6657,
      "step": 2018500
    },
    {
      "epoch": 18.419227680852618,
      "grad_norm": 4.624966621398926,
      "learning_rate": 3.465064359928949e-05,
      "loss": 0.6788,
      "step": 2018600
    },
    {
      "epoch": 18.420140156215783,
      "grad_norm": 4.275759220123291,
      "learning_rate": 3.4649883203153516e-05,
      "loss": 0.6735,
      "step": 2018700
    },
    {
      "epoch": 18.42105263157895,
      "grad_norm": 3.5939862728118896,
      "learning_rate": 3.4649122807017546e-05,
      "loss": 0.6823,
      "step": 2018800
    },
    {
      "epoch": 18.421965106942114,
      "grad_norm": 4.424862861633301,
      "learning_rate": 3.4648362410881576e-05,
      "loss": 0.7055,
      "step": 2018900
    },
    {
      "epoch": 18.42287758230528,
      "grad_norm": 4.462747573852539,
      "learning_rate": 3.4647602014745606e-05,
      "loss": 0.6843,
      "step": 2019000
    },
    {
      "epoch": 18.423790057668445,
      "grad_norm": 3.757047176361084,
      "learning_rate": 3.464684161860963e-05,
      "loss": 0.6598,
      "step": 2019100
    },
    {
      "epoch": 18.42470253303161,
      "grad_norm": 3.9600207805633545,
      "learning_rate": 3.4646081222473666e-05,
      "loss": 0.6606,
      "step": 2019200
    },
    {
      "epoch": 18.42561500839477,
      "grad_norm": 3.7794368267059326,
      "learning_rate": 3.464532082633769e-05,
      "loss": 0.6714,
      "step": 2019300
    },
    {
      "epoch": 18.426527483757937,
      "grad_norm": 4.413477420806885,
      "learning_rate": 3.464456043020172e-05,
      "loss": 0.6393,
      "step": 2019400
    },
    {
      "epoch": 18.427439959121102,
      "grad_norm": 3.676032543182373,
      "learning_rate": 3.464380003406575e-05,
      "loss": 0.6912,
      "step": 2019500
    },
    {
      "epoch": 18.428352434484268,
      "grad_norm": 3.6903560161590576,
      "learning_rate": 3.464303963792977e-05,
      "loss": 0.6237,
      "step": 2019600
    },
    {
      "epoch": 18.429264909847433,
      "grad_norm": 4.242886543273926,
      "learning_rate": 3.464227924179381e-05,
      "loss": 0.6651,
      "step": 2019700
    },
    {
      "epoch": 18.430177385210598,
      "grad_norm": 2.8836989402770996,
      "learning_rate": 3.464151884565783e-05,
      "loss": 0.6602,
      "step": 2019800
    },
    {
      "epoch": 18.431089860573763,
      "grad_norm": 3.8967745304107666,
      "learning_rate": 3.464075844952186e-05,
      "loss": 0.6646,
      "step": 2019900
    },
    {
      "epoch": 18.43200233593693,
      "grad_norm": 4.209493160247803,
      "learning_rate": 3.463999805338589e-05,
      "loss": 0.6644,
      "step": 2020000
    },
    {
      "epoch": 18.432914811300094,
      "grad_norm": 4.294310092926025,
      "learning_rate": 3.463923765724992e-05,
      "loss": 0.6732,
      "step": 2020100
    },
    {
      "epoch": 18.43382728666326,
      "grad_norm": 5.509245872497559,
      "learning_rate": 3.4638477261113946e-05,
      "loss": 0.6589,
      "step": 2020200
    },
    {
      "epoch": 18.434739762026425,
      "grad_norm": 4.153299331665039,
      "learning_rate": 3.463771686497798e-05,
      "loss": 0.653,
      "step": 2020300
    },
    {
      "epoch": 18.43565223738959,
      "grad_norm": 4.334758281707764,
      "learning_rate": 3.4636956468842006e-05,
      "loss": 0.6332,
      "step": 2020400
    },
    {
      "epoch": 18.436564712752755,
      "grad_norm": 3.8202240467071533,
      "learning_rate": 3.4636196072706037e-05,
      "loss": 0.6529,
      "step": 2020500
    },
    {
      "epoch": 18.43747718811592,
      "grad_norm": 3.6272594928741455,
      "learning_rate": 3.4635435676570067e-05,
      "loss": 0.6678,
      "step": 2020600
    },
    {
      "epoch": 18.438389663479086,
      "grad_norm": 3.5220470428466797,
      "learning_rate": 3.46346752804341e-05,
      "loss": 0.6972,
      "step": 2020700
    },
    {
      "epoch": 18.43930213884225,
      "grad_norm": 4.062695026397705,
      "learning_rate": 3.463391488429813e-05,
      "loss": 0.6538,
      "step": 2020800
    },
    {
      "epoch": 18.440214614205416,
      "grad_norm": 4.103816032409668,
      "learning_rate": 3.463315448816216e-05,
      "loss": 0.676,
      "step": 2020900
    },
    {
      "epoch": 18.44112708956858,
      "grad_norm": 4.1045637130737305,
      "learning_rate": 3.463239409202618e-05,
      "loss": 0.6462,
      "step": 2021000
    },
    {
      "epoch": 18.442039564931747,
      "grad_norm": 3.331730604171753,
      "learning_rate": 3.463163369589022e-05,
      "loss": 0.6655,
      "step": 2021100
    },
    {
      "epoch": 18.442952040294912,
      "grad_norm": 3.6931347846984863,
      "learning_rate": 3.463087329975424e-05,
      "loss": 0.6524,
      "step": 2021200
    },
    {
      "epoch": 18.443864515658078,
      "grad_norm": 4.874395847320557,
      "learning_rate": 3.463011290361827e-05,
      "loss": 0.6736,
      "step": 2021300
    },
    {
      "epoch": 18.444776991021243,
      "grad_norm": 2.6645734310150146,
      "learning_rate": 3.46293525074823e-05,
      "loss": 0.6112,
      "step": 2021400
    },
    {
      "epoch": 18.445689466384408,
      "grad_norm": 4.366859436035156,
      "learning_rate": 3.462859211134633e-05,
      "loss": 0.7024,
      "step": 2021500
    },
    {
      "epoch": 18.446601941747574,
      "grad_norm": 4.095798492431641,
      "learning_rate": 3.4627831715210354e-05,
      "loss": 0.6577,
      "step": 2021600
    },
    {
      "epoch": 18.44751441711074,
      "grad_norm": 3.817847490310669,
      "learning_rate": 3.462707131907439e-05,
      "loss": 0.6814,
      "step": 2021700
    },
    {
      "epoch": 18.448426892473904,
      "grad_norm": 4.139655113220215,
      "learning_rate": 3.4626310922938414e-05,
      "loss": 0.6861,
      "step": 2021800
    },
    {
      "epoch": 18.44933936783707,
      "grad_norm": 4.815114498138428,
      "learning_rate": 3.4625550526802444e-05,
      "loss": 0.6404,
      "step": 2021900
    },
    {
      "epoch": 18.450251843200235,
      "grad_norm": 3.563297986984253,
      "learning_rate": 3.4624790130666474e-05,
      "loss": 0.6449,
      "step": 2022000
    },
    {
      "epoch": 18.4511643185634,
      "grad_norm": 3.6564831733703613,
      "learning_rate": 3.4624029734530504e-05,
      "loss": 0.6399,
      "step": 2022100
    },
    {
      "epoch": 18.452076793926565,
      "grad_norm": 3.536977767944336,
      "learning_rate": 3.4623269338394534e-05,
      "loss": 0.6651,
      "step": 2022200
    },
    {
      "epoch": 18.45298926928973,
      "grad_norm": 4.682157516479492,
      "learning_rate": 3.462250894225856e-05,
      "loss": 0.6354,
      "step": 2022300
    },
    {
      "epoch": 18.453901744652896,
      "grad_norm": 4.009160995483398,
      "learning_rate": 3.462174854612259e-05,
      "loss": 0.6804,
      "step": 2022400
    },
    {
      "epoch": 18.45481422001606,
      "grad_norm": 4.222255706787109,
      "learning_rate": 3.462098814998662e-05,
      "loss": 0.6572,
      "step": 2022500
    },
    {
      "epoch": 18.455726695379226,
      "grad_norm": 3.958893060684204,
      "learning_rate": 3.462022775385065e-05,
      "loss": 0.674,
      "step": 2022600
    },
    {
      "epoch": 18.45663917074239,
      "grad_norm": 4.330913066864014,
      "learning_rate": 3.461946735771468e-05,
      "loss": 0.6743,
      "step": 2022700
    },
    {
      "epoch": 18.457551646105554,
      "grad_norm": 3.8615853786468506,
      "learning_rate": 3.461870696157871e-05,
      "loss": 0.6922,
      "step": 2022800
    },
    {
      "epoch": 18.45846412146872,
      "grad_norm": 3.445768356323242,
      "learning_rate": 3.461794656544273e-05,
      "loss": 0.6858,
      "step": 2022900
    },
    {
      "epoch": 18.459376596831884,
      "grad_norm": 4.159214496612549,
      "learning_rate": 3.461718616930677e-05,
      "loss": 0.6341,
      "step": 2023000
    },
    {
      "epoch": 18.46028907219505,
      "grad_norm": 3.482577323913574,
      "learning_rate": 3.461642577317079e-05,
      "loss": 0.6483,
      "step": 2023100
    },
    {
      "epoch": 18.461201547558215,
      "grad_norm": 3.636427640914917,
      "learning_rate": 3.461566537703482e-05,
      "loss": 0.6789,
      "step": 2023200
    },
    {
      "epoch": 18.46211402292138,
      "grad_norm": 3.4914236068725586,
      "learning_rate": 3.461490498089885e-05,
      "loss": 0.671,
      "step": 2023300
    },
    {
      "epoch": 18.463026498284545,
      "grad_norm": 3.582258462905884,
      "learning_rate": 3.461414458476288e-05,
      "loss": 0.6558,
      "step": 2023400
    },
    {
      "epoch": 18.46393897364771,
      "grad_norm": 4.470177173614502,
      "learning_rate": 3.4613384188626905e-05,
      "loss": 0.6547,
      "step": 2023500
    },
    {
      "epoch": 18.464851449010876,
      "grad_norm": 5.002279281616211,
      "learning_rate": 3.461262379249094e-05,
      "loss": 0.6564,
      "step": 2023600
    },
    {
      "epoch": 18.46576392437404,
      "grad_norm": 3.412414312362671,
      "learning_rate": 3.4611863396354965e-05,
      "loss": 0.6495,
      "step": 2023700
    },
    {
      "epoch": 18.466676399737207,
      "grad_norm": 3.955263614654541,
      "learning_rate": 3.4611103000218995e-05,
      "loss": 0.691,
      "step": 2023800
    },
    {
      "epoch": 18.467588875100372,
      "grad_norm": 4.011926174163818,
      "learning_rate": 3.4610342604083025e-05,
      "loss": 0.6657,
      "step": 2023900
    },
    {
      "epoch": 18.468501350463537,
      "grad_norm": 3.9613189697265625,
      "learning_rate": 3.4609582207947055e-05,
      "loss": 0.6554,
      "step": 2024000
    },
    {
      "epoch": 18.469413825826702,
      "grad_norm": 3.233020782470703,
      "learning_rate": 3.4608821811811085e-05,
      "loss": 0.6321,
      "step": 2024100
    },
    {
      "epoch": 18.470326301189868,
      "grad_norm": 4.265773296356201,
      "learning_rate": 3.4608061415675115e-05,
      "loss": 0.6241,
      "step": 2024200
    },
    {
      "epoch": 18.471238776553033,
      "grad_norm": 3.1106812953948975,
      "learning_rate": 3.460730101953914e-05,
      "loss": 0.6758,
      "step": 2024300
    },
    {
      "epoch": 18.4721512519162,
      "grad_norm": 4.80523157119751,
      "learning_rate": 3.4606540623403175e-05,
      "loss": 0.6929,
      "step": 2024400
    },
    {
      "epoch": 18.473063727279364,
      "grad_norm": 4.966398239135742,
      "learning_rate": 3.46057802272672e-05,
      "loss": 0.6738,
      "step": 2024500
    },
    {
      "epoch": 18.47397620264253,
      "grad_norm": 3.067352533340454,
      "learning_rate": 3.460501983113123e-05,
      "loss": 0.6617,
      "step": 2024600
    },
    {
      "epoch": 18.474888678005694,
      "grad_norm": 4.424552917480469,
      "learning_rate": 3.460425943499526e-05,
      "loss": 0.7104,
      "step": 2024700
    },
    {
      "epoch": 18.47580115336886,
      "grad_norm": 3.8568472862243652,
      "learning_rate": 3.460349903885929e-05,
      "loss": 0.6668,
      "step": 2024800
    },
    {
      "epoch": 18.476713628732025,
      "grad_norm": 3.9696896076202393,
      "learning_rate": 3.460273864272331e-05,
      "loss": 0.6635,
      "step": 2024900
    },
    {
      "epoch": 18.47762610409519,
      "grad_norm": 3.4484477043151855,
      "learning_rate": 3.460197824658735e-05,
      "loss": 0.655,
      "step": 2025000
    },
    {
      "epoch": 18.478538579458355,
      "grad_norm": 3.5364437103271484,
      "learning_rate": 3.460121785045137e-05,
      "loss": 0.6733,
      "step": 2025100
    },
    {
      "epoch": 18.47945105482152,
      "grad_norm": 2.7862300872802734,
      "learning_rate": 3.46004574543154e-05,
      "loss": 0.6699,
      "step": 2025200
    },
    {
      "epoch": 18.480363530184686,
      "grad_norm": 3.987082004547119,
      "learning_rate": 3.459969705817943e-05,
      "loss": 0.6805,
      "step": 2025300
    },
    {
      "epoch": 18.48127600554785,
      "grad_norm": 4.0139570236206055,
      "learning_rate": 3.4598936662043456e-05,
      "loss": 0.6648,
      "step": 2025400
    },
    {
      "epoch": 18.482188480911017,
      "grad_norm": 3.361384153366089,
      "learning_rate": 3.459817626590749e-05,
      "loss": 0.6019,
      "step": 2025500
    },
    {
      "epoch": 18.483100956274182,
      "grad_norm": 4.627669334411621,
      "learning_rate": 3.4597415869771516e-05,
      "loss": 0.6683,
      "step": 2025600
    },
    {
      "epoch": 18.484013431637347,
      "grad_norm": 3.899604320526123,
      "learning_rate": 3.4596655473635546e-05,
      "loss": 0.6555,
      "step": 2025700
    },
    {
      "epoch": 18.484925907000513,
      "grad_norm": 3.47670578956604,
      "learning_rate": 3.4595895077499576e-05,
      "loss": 0.6594,
      "step": 2025800
    },
    {
      "epoch": 18.485838382363678,
      "grad_norm": 3.735016345977783,
      "learning_rate": 3.4595134681363606e-05,
      "loss": 0.6901,
      "step": 2025900
    },
    {
      "epoch": 18.486750857726843,
      "grad_norm": 3.9975926876068115,
      "learning_rate": 3.459437428522763e-05,
      "loss": 0.6767,
      "step": 2026000
    },
    {
      "epoch": 18.487663333090005,
      "grad_norm": 4.195746421813965,
      "learning_rate": 3.4593613889091666e-05,
      "loss": 0.6587,
      "step": 2026100
    },
    {
      "epoch": 18.48857580845317,
      "grad_norm": 3.8490352630615234,
      "learning_rate": 3.459285349295569e-05,
      "loss": 0.6663,
      "step": 2026200
    },
    {
      "epoch": 18.489488283816335,
      "grad_norm": 3.620124340057373,
      "learning_rate": 3.459209309681972e-05,
      "loss": 0.6615,
      "step": 2026300
    },
    {
      "epoch": 18.4904007591795,
      "grad_norm": 3.620375633239746,
      "learning_rate": 3.459133270068375e-05,
      "loss": 0.6618,
      "step": 2026400
    },
    {
      "epoch": 18.491313234542666,
      "grad_norm": 3.7328848838806152,
      "learning_rate": 3.459057230454778e-05,
      "loss": 0.6768,
      "step": 2026500
    },
    {
      "epoch": 18.49222570990583,
      "grad_norm": 3.8906126022338867,
      "learning_rate": 3.458981190841181e-05,
      "loss": 0.6176,
      "step": 2026600
    },
    {
      "epoch": 18.493138185268997,
      "grad_norm": 4.2301106452941895,
      "learning_rate": 3.458905151227584e-05,
      "loss": 0.6238,
      "step": 2026700
    },
    {
      "epoch": 18.494050660632162,
      "grad_norm": 3.0754523277282715,
      "learning_rate": 3.458829111613986e-05,
      "loss": 0.6461,
      "step": 2026800
    },
    {
      "epoch": 18.494963135995327,
      "grad_norm": 2.357004404067993,
      "learning_rate": 3.45875307200039e-05,
      "loss": 0.6343,
      "step": 2026900
    },
    {
      "epoch": 18.495875611358493,
      "grad_norm": 3.9480814933776855,
      "learning_rate": 3.458677032386792e-05,
      "loss": 0.6328,
      "step": 2027000
    },
    {
      "epoch": 18.496788086721658,
      "grad_norm": 4.001951694488525,
      "learning_rate": 3.458600992773195e-05,
      "loss": 0.6988,
      "step": 2027100
    },
    {
      "epoch": 18.497700562084823,
      "grad_norm": 3.7810068130493164,
      "learning_rate": 3.458524953159598e-05,
      "loss": 0.6959,
      "step": 2027200
    },
    {
      "epoch": 18.49861303744799,
      "grad_norm": 4.175806045532227,
      "learning_rate": 3.458448913546001e-05,
      "loss": 0.6661,
      "step": 2027300
    },
    {
      "epoch": 18.499525512811154,
      "grad_norm": 3.8303775787353516,
      "learning_rate": 3.4583728739324037e-05,
      "loss": 0.6976,
      "step": 2027400
    },
    {
      "epoch": 18.50043798817432,
      "grad_norm": 4.419338703155518,
      "learning_rate": 3.4582968343188073e-05,
      "loss": 0.6713,
      "step": 2027500
    },
    {
      "epoch": 18.501350463537484,
      "grad_norm": 4.119745254516602,
      "learning_rate": 3.45822079470521e-05,
      "loss": 0.6997,
      "step": 2027600
    },
    {
      "epoch": 18.50226293890065,
      "grad_norm": 4.898101329803467,
      "learning_rate": 3.458144755091613e-05,
      "loss": 0.6715,
      "step": 2027700
    },
    {
      "epoch": 18.503175414263815,
      "grad_norm": 3.820190906524658,
      "learning_rate": 3.458068715478016e-05,
      "loss": 0.6606,
      "step": 2027800
    },
    {
      "epoch": 18.50408788962698,
      "grad_norm": 4.403531551361084,
      "learning_rate": 3.457992675864418e-05,
      "loss": 0.6576,
      "step": 2027900
    },
    {
      "epoch": 18.505000364990146,
      "grad_norm": 4.010942459106445,
      "learning_rate": 3.457916636250822e-05,
      "loss": 0.6488,
      "step": 2028000
    },
    {
      "epoch": 18.50591284035331,
      "grad_norm": 3.010793924331665,
      "learning_rate": 3.457840596637224e-05,
      "loss": 0.6828,
      "step": 2028100
    },
    {
      "epoch": 18.506825315716476,
      "grad_norm": 4.154729843139648,
      "learning_rate": 3.457764557023627e-05,
      "loss": 0.6467,
      "step": 2028200
    },
    {
      "epoch": 18.50773779107964,
      "grad_norm": 3.1929900646209717,
      "learning_rate": 3.45768851741003e-05,
      "loss": 0.661,
      "step": 2028300
    },
    {
      "epoch": 18.508650266442807,
      "grad_norm": 4.095394134521484,
      "learning_rate": 3.457612477796433e-05,
      "loss": 0.6493,
      "step": 2028400
    },
    {
      "epoch": 18.509562741805972,
      "grad_norm": 3.829615592956543,
      "learning_rate": 3.4575364381828354e-05,
      "loss": 0.6997,
      "step": 2028500
    },
    {
      "epoch": 18.510475217169137,
      "grad_norm": 4.092291355133057,
      "learning_rate": 3.457460398569239e-05,
      "loss": 0.6949,
      "step": 2028600
    },
    {
      "epoch": 18.511387692532303,
      "grad_norm": 3.7349393367767334,
      "learning_rate": 3.4573843589556414e-05,
      "loss": 0.6246,
      "step": 2028700
    },
    {
      "epoch": 18.512300167895468,
      "grad_norm": 2.725188970565796,
      "learning_rate": 3.4573083193420444e-05,
      "loss": 0.6281,
      "step": 2028800
    },
    {
      "epoch": 18.513212643258633,
      "grad_norm": 3.9173552989959717,
      "learning_rate": 3.4572322797284474e-05,
      "loss": 0.6819,
      "step": 2028900
    },
    {
      "epoch": 18.5141251186218,
      "grad_norm": 3.1866414546966553,
      "learning_rate": 3.4571562401148504e-05,
      "loss": 0.6277,
      "step": 2029000
    },
    {
      "epoch": 18.515037593984964,
      "grad_norm": 3.70243501663208,
      "learning_rate": 3.4570802005012534e-05,
      "loss": 0.6318,
      "step": 2029100
    },
    {
      "epoch": 18.51595006934813,
      "grad_norm": 4.977138996124268,
      "learning_rate": 3.4570041608876564e-05,
      "loss": 0.674,
      "step": 2029200
    },
    {
      "epoch": 18.516862544711294,
      "grad_norm": 3.5816476345062256,
      "learning_rate": 3.456928121274059e-05,
      "loss": 0.6819,
      "step": 2029300
    },
    {
      "epoch": 18.51777502007446,
      "grad_norm": 4.999594688415527,
      "learning_rate": 3.4568520816604624e-05,
      "loss": 0.7078,
      "step": 2029400
    },
    {
      "epoch": 18.51868749543762,
      "grad_norm": 4.383111953735352,
      "learning_rate": 3.456776042046865e-05,
      "loss": 0.6559,
      "step": 2029500
    },
    {
      "epoch": 18.519599970800787,
      "grad_norm": 3.6165642738342285,
      "learning_rate": 3.456700002433268e-05,
      "loss": 0.6346,
      "step": 2029600
    },
    {
      "epoch": 18.520512446163952,
      "grad_norm": 3.3139166831970215,
      "learning_rate": 3.456623962819671e-05,
      "loss": 0.6431,
      "step": 2029700
    },
    {
      "epoch": 18.521424921527117,
      "grad_norm": 4.0948710441589355,
      "learning_rate": 3.456547923206074e-05,
      "loss": 0.6838,
      "step": 2029800
    },
    {
      "epoch": 18.522337396890283,
      "grad_norm": 4.250151634216309,
      "learning_rate": 3.456471883592476e-05,
      "loss": 0.6349,
      "step": 2029900
    },
    {
      "epoch": 18.523249872253448,
      "grad_norm": 3.7922751903533936,
      "learning_rate": 3.45639584397888e-05,
      "loss": 0.6303,
      "step": 2030000
    },
    {
      "epoch": 18.524162347616613,
      "grad_norm": 3.6612799167633057,
      "learning_rate": 3.456319804365282e-05,
      "loss": 0.6529,
      "step": 2030100
    },
    {
      "epoch": 18.52507482297978,
      "grad_norm": 4.142031669616699,
      "learning_rate": 3.456243764751685e-05,
      "loss": 0.6947,
      "step": 2030200
    },
    {
      "epoch": 18.525987298342944,
      "grad_norm": 3.9239580631256104,
      "learning_rate": 3.456167725138088e-05,
      "loss": 0.681,
      "step": 2030300
    },
    {
      "epoch": 18.52689977370611,
      "grad_norm": 3.973811149597168,
      "learning_rate": 3.456091685524491e-05,
      "loss": 0.7026,
      "step": 2030400
    },
    {
      "epoch": 18.527812249069274,
      "grad_norm": 3.9981629848480225,
      "learning_rate": 3.456015645910894e-05,
      "loss": 0.6743,
      "step": 2030500
    },
    {
      "epoch": 18.52872472443244,
      "grad_norm": 2.9594786167144775,
      "learning_rate": 3.455939606297297e-05,
      "loss": 0.6454,
      "step": 2030600
    },
    {
      "epoch": 18.529637199795605,
      "grad_norm": 4.890873908996582,
      "learning_rate": 3.4558635666836995e-05,
      "loss": 0.6537,
      "step": 2030700
    },
    {
      "epoch": 18.53054967515877,
      "grad_norm": 4.403905868530273,
      "learning_rate": 3.4557875270701025e-05,
      "loss": 0.6667,
      "step": 2030800
    },
    {
      "epoch": 18.531462150521936,
      "grad_norm": 4.417304515838623,
      "learning_rate": 3.4557114874565055e-05,
      "loss": 0.6801,
      "step": 2030900
    },
    {
      "epoch": 18.5323746258851,
      "grad_norm": 2.3121066093444824,
      "learning_rate": 3.455635447842908e-05,
      "loss": 0.6631,
      "step": 2031000
    },
    {
      "epoch": 18.533287101248266,
      "grad_norm": 3.7235684394836426,
      "learning_rate": 3.4555594082293115e-05,
      "loss": 0.6595,
      "step": 2031100
    },
    {
      "epoch": 18.53419957661143,
      "grad_norm": 4.33593225479126,
      "learning_rate": 3.455483368615714e-05,
      "loss": 0.6724,
      "step": 2031200
    },
    {
      "epoch": 18.535112051974597,
      "grad_norm": 4.399942874908447,
      "learning_rate": 3.455407329002117e-05,
      "loss": 0.7022,
      "step": 2031300
    },
    {
      "epoch": 18.536024527337762,
      "grad_norm": 3.540127992630005,
      "learning_rate": 3.45533128938852e-05,
      "loss": 0.6294,
      "step": 2031400
    },
    {
      "epoch": 18.536937002700927,
      "grad_norm": 5.103181838989258,
      "learning_rate": 3.455255249774923e-05,
      "loss": 0.6508,
      "step": 2031500
    },
    {
      "epoch": 18.537849478064093,
      "grad_norm": 3.875666856765747,
      "learning_rate": 3.455179210161326e-05,
      "loss": 0.6605,
      "step": 2031600
    },
    {
      "epoch": 18.538761953427258,
      "grad_norm": 3.8429112434387207,
      "learning_rate": 3.455103170547729e-05,
      "loss": 0.6581,
      "step": 2031700
    },
    {
      "epoch": 18.539674428790423,
      "grad_norm": 2.4200491905212402,
      "learning_rate": 3.455027130934131e-05,
      "loss": 0.681,
      "step": 2031800
    },
    {
      "epoch": 18.54058690415359,
      "grad_norm": 3.4998090267181396,
      "learning_rate": 3.454951091320535e-05,
      "loss": 0.6462,
      "step": 2031900
    },
    {
      "epoch": 18.541499379516754,
      "grad_norm": 4.048198223114014,
      "learning_rate": 3.454875051706937e-05,
      "loss": 0.6388,
      "step": 2032000
    },
    {
      "epoch": 18.54241185487992,
      "grad_norm": 4.380774974822998,
      "learning_rate": 3.45479901209334e-05,
      "loss": 0.6887,
      "step": 2032100
    },
    {
      "epoch": 18.543324330243085,
      "grad_norm": 3.740628242492676,
      "learning_rate": 3.454722972479743e-05,
      "loss": 0.6721,
      "step": 2032200
    },
    {
      "epoch": 18.54423680560625,
      "grad_norm": 3.499554395675659,
      "learning_rate": 3.454646932866146e-05,
      "loss": 0.6849,
      "step": 2032300
    },
    {
      "epoch": 18.545149280969415,
      "grad_norm": 3.555802822113037,
      "learning_rate": 3.4545708932525486e-05,
      "loss": 0.6438,
      "step": 2032400
    },
    {
      "epoch": 18.54606175633258,
      "grad_norm": 4.494743347167969,
      "learning_rate": 3.454494853638952e-05,
      "loss": 0.6784,
      "step": 2032500
    },
    {
      "epoch": 18.546974231695746,
      "grad_norm": 4.5076446533203125,
      "learning_rate": 3.4544188140253546e-05,
      "loss": 0.6674,
      "step": 2032600
    },
    {
      "epoch": 18.54788670705891,
      "grad_norm": 3.316802501678467,
      "learning_rate": 3.4543427744117576e-05,
      "loss": 0.6799,
      "step": 2032700
    },
    {
      "epoch": 18.548799182422073,
      "grad_norm": 4.589843273162842,
      "learning_rate": 3.4542667347981606e-05,
      "loss": 0.6814,
      "step": 2032800
    },
    {
      "epoch": 18.549711657785238,
      "grad_norm": 3.5057473182678223,
      "learning_rate": 3.4541906951845636e-05,
      "loss": 0.6769,
      "step": 2032900
    },
    {
      "epoch": 18.550624133148403,
      "grad_norm": 3.980698585510254,
      "learning_rate": 3.4541146555709666e-05,
      "loss": 0.6339,
      "step": 2033000
    },
    {
      "epoch": 18.55153660851157,
      "grad_norm": 3.912642240524292,
      "learning_rate": 3.4540386159573696e-05,
      "loss": 0.6408,
      "step": 2033100
    },
    {
      "epoch": 18.552449083874734,
      "grad_norm": 4.167616844177246,
      "learning_rate": 3.453962576343772e-05,
      "loss": 0.6145,
      "step": 2033200
    },
    {
      "epoch": 18.5533615592379,
      "grad_norm": 3.988982677459717,
      "learning_rate": 3.4538865367301756e-05,
      "loss": 0.6803,
      "step": 2033300
    },
    {
      "epoch": 18.554274034601065,
      "grad_norm": 4.185359477996826,
      "learning_rate": 3.453810497116578e-05,
      "loss": 0.6235,
      "step": 2033400
    },
    {
      "epoch": 18.55518650996423,
      "grad_norm": 4.491994857788086,
      "learning_rate": 3.453734457502981e-05,
      "loss": 0.7062,
      "step": 2033500
    },
    {
      "epoch": 18.556098985327395,
      "grad_norm": 4.424516201019287,
      "learning_rate": 3.453658417889384e-05,
      "loss": 0.6793,
      "step": 2033600
    },
    {
      "epoch": 18.55701146069056,
      "grad_norm": 3.9193880558013916,
      "learning_rate": 3.453582378275786e-05,
      "loss": 0.6297,
      "step": 2033700
    },
    {
      "epoch": 18.557923936053726,
      "grad_norm": 3.3416054248809814,
      "learning_rate": 3.453506338662189e-05,
      "loss": 0.6516,
      "step": 2033800
    },
    {
      "epoch": 18.55883641141689,
      "grad_norm": 4.356344699859619,
      "learning_rate": 3.453430299048592e-05,
      "loss": 0.6541,
      "step": 2033900
    },
    {
      "epoch": 18.559748886780056,
      "grad_norm": 3.7535531520843506,
      "learning_rate": 3.453354259434995e-05,
      "loss": 0.6307,
      "step": 2034000
    },
    {
      "epoch": 18.56066136214322,
      "grad_norm": 3.8174619674682617,
      "learning_rate": 3.453278219821398e-05,
      "loss": 0.6772,
      "step": 2034100
    },
    {
      "epoch": 18.561573837506387,
      "grad_norm": 4.0690202713012695,
      "learning_rate": 3.4532021802078013e-05,
      "loss": 0.6676,
      "step": 2034200
    },
    {
      "epoch": 18.562486312869552,
      "grad_norm": 3.4618568420410156,
      "learning_rate": 3.453126140594204e-05,
      "loss": 0.6478,
      "step": 2034300
    },
    {
      "epoch": 18.563398788232718,
      "grad_norm": 4.330508232116699,
      "learning_rate": 3.4530501009806074e-05,
      "loss": 0.6456,
      "step": 2034400
    },
    {
      "epoch": 18.564311263595883,
      "grad_norm": 3.2848458290100098,
      "learning_rate": 3.45297406136701e-05,
      "loss": 0.6843,
      "step": 2034500
    },
    {
      "epoch": 18.565223738959048,
      "grad_norm": 4.505031108856201,
      "learning_rate": 3.452898021753413e-05,
      "loss": 0.66,
      "step": 2034600
    },
    {
      "epoch": 18.566136214322214,
      "grad_norm": 4.783444404602051,
      "learning_rate": 3.452821982139816e-05,
      "loss": 0.6514,
      "step": 2034700
    },
    {
      "epoch": 18.56704868968538,
      "grad_norm": 3.2305731773376465,
      "learning_rate": 3.452745942526219e-05,
      "loss": 0.6245,
      "step": 2034800
    },
    {
      "epoch": 18.567961165048544,
      "grad_norm": 3.9695613384246826,
      "learning_rate": 3.452669902912622e-05,
      "loss": 0.6724,
      "step": 2034900
    },
    {
      "epoch": 18.56887364041171,
      "grad_norm": 4.375524520874023,
      "learning_rate": 3.452593863299025e-05,
      "loss": 0.6611,
      "step": 2035000
    },
    {
      "epoch": 18.569786115774875,
      "grad_norm": 3.5802786350250244,
      "learning_rate": 3.452517823685427e-05,
      "loss": 0.676,
      "step": 2035100
    },
    {
      "epoch": 18.57069859113804,
      "grad_norm": 3.8919835090637207,
      "learning_rate": 3.45244178407183e-05,
      "loss": 0.5991,
      "step": 2035200
    },
    {
      "epoch": 18.571611066501205,
      "grad_norm": 4.07832145690918,
      "learning_rate": 3.452365744458233e-05,
      "loss": 0.6765,
      "step": 2035300
    },
    {
      "epoch": 18.57252354186437,
      "grad_norm": 2.958770275115967,
      "learning_rate": 3.452289704844636e-05,
      "loss": 0.6896,
      "step": 2035400
    },
    {
      "epoch": 18.573436017227536,
      "grad_norm": 4.851066589355469,
      "learning_rate": 3.452213665231039e-05,
      "loss": 0.6992,
      "step": 2035500
    },
    {
      "epoch": 18.5743484925907,
      "grad_norm": 3.7675282955169678,
      "learning_rate": 3.452137625617442e-05,
      "loss": 0.6518,
      "step": 2035600
    },
    {
      "epoch": 18.575260967953866,
      "grad_norm": 4.131585121154785,
      "learning_rate": 3.4520615860038444e-05,
      "loss": 0.6796,
      "step": 2035700
    },
    {
      "epoch": 18.576173443317032,
      "grad_norm": 2.8262157440185547,
      "learning_rate": 3.451985546390248e-05,
      "loss": 0.6799,
      "step": 2035800
    },
    {
      "epoch": 18.577085918680197,
      "grad_norm": 3.4000864028930664,
      "learning_rate": 3.4519095067766504e-05,
      "loss": 0.6721,
      "step": 2035900
    },
    {
      "epoch": 18.577998394043362,
      "grad_norm": 3.4670817852020264,
      "learning_rate": 3.4518334671630534e-05,
      "loss": 0.6513,
      "step": 2036000
    },
    {
      "epoch": 18.578910869406528,
      "grad_norm": 4.4852423667907715,
      "learning_rate": 3.4517574275494564e-05,
      "loss": 0.6756,
      "step": 2036100
    },
    {
      "epoch": 18.579823344769693,
      "grad_norm": 4.275267124176025,
      "learning_rate": 3.4516813879358594e-05,
      "loss": 0.6427,
      "step": 2036200
    },
    {
      "epoch": 18.580735820132855,
      "grad_norm": 3.520683765411377,
      "learning_rate": 3.4516053483222625e-05,
      "loss": 0.697,
      "step": 2036300
    },
    {
      "epoch": 18.58164829549602,
      "grad_norm": 4.226651191711426,
      "learning_rate": 3.451529308708665e-05,
      "loss": 0.657,
      "step": 2036400
    },
    {
      "epoch": 18.582560770859185,
      "grad_norm": 3.900944948196411,
      "learning_rate": 3.451453269095068e-05,
      "loss": 0.6631,
      "step": 2036500
    },
    {
      "epoch": 18.58347324622235,
      "grad_norm": 3.7035889625549316,
      "learning_rate": 3.451377229481471e-05,
      "loss": 0.6729,
      "step": 2036600
    },
    {
      "epoch": 18.584385721585516,
      "grad_norm": 3.5418334007263184,
      "learning_rate": 3.451301189867874e-05,
      "loss": 0.6067,
      "step": 2036700
    },
    {
      "epoch": 18.58529819694868,
      "grad_norm": 3.4919214248657227,
      "learning_rate": 3.451225150254276e-05,
      "loss": 0.7301,
      "step": 2036800
    },
    {
      "epoch": 18.586210672311847,
      "grad_norm": 3.5660388469696045,
      "learning_rate": 3.45114911064068e-05,
      "loss": 0.6779,
      "step": 2036900
    },
    {
      "epoch": 18.587123147675012,
      "grad_norm": 3.9850378036499023,
      "learning_rate": 3.451073071027082e-05,
      "loss": 0.6528,
      "step": 2037000
    },
    {
      "epoch": 18.588035623038177,
      "grad_norm": 2.653879404067993,
      "learning_rate": 3.450997031413485e-05,
      "loss": 0.6735,
      "step": 2037100
    },
    {
      "epoch": 18.588948098401342,
      "grad_norm": 3.4571449756622314,
      "learning_rate": 3.450920991799888e-05,
      "loss": 0.6479,
      "step": 2037200
    },
    {
      "epoch": 18.589860573764508,
      "grad_norm": 4.108133316040039,
      "learning_rate": 3.450844952186291e-05,
      "loss": 0.6574,
      "step": 2037300
    },
    {
      "epoch": 18.590773049127673,
      "grad_norm": 5.355432987213135,
      "learning_rate": 3.450768912572694e-05,
      "loss": 0.6249,
      "step": 2037400
    },
    {
      "epoch": 18.59168552449084,
      "grad_norm": 4.349244117736816,
      "learning_rate": 3.450692872959097e-05,
      "loss": 0.6162,
      "step": 2037500
    },
    {
      "epoch": 18.592597999854004,
      "grad_norm": 3.2146036624908447,
      "learning_rate": 3.4506168333454995e-05,
      "loss": 0.6662,
      "step": 2037600
    },
    {
      "epoch": 18.59351047521717,
      "grad_norm": 4.223233699798584,
      "learning_rate": 3.450540793731903e-05,
      "loss": 0.644,
      "step": 2037700
    },
    {
      "epoch": 18.594422950580334,
      "grad_norm": 3.1926205158233643,
      "learning_rate": 3.4504647541183055e-05,
      "loss": 0.6747,
      "step": 2037800
    },
    {
      "epoch": 18.5953354259435,
      "grad_norm": 2.799994707107544,
      "learning_rate": 3.4503887145047085e-05,
      "loss": 0.6314,
      "step": 2037900
    },
    {
      "epoch": 18.596247901306665,
      "grad_norm": 3.946042537689209,
      "learning_rate": 3.4503126748911115e-05,
      "loss": 0.614,
      "step": 2038000
    },
    {
      "epoch": 18.59716037666983,
      "grad_norm": 3.922356128692627,
      "learning_rate": 3.4502366352775145e-05,
      "loss": 0.6529,
      "step": 2038100
    },
    {
      "epoch": 18.598072852032995,
      "grad_norm": 3.127692699432373,
      "learning_rate": 3.450160595663917e-05,
      "loss": 0.6844,
      "step": 2038200
    },
    {
      "epoch": 18.59898532739616,
      "grad_norm": 3.817650079727173,
      "learning_rate": 3.4500845560503206e-05,
      "loss": 0.6816,
      "step": 2038300
    },
    {
      "epoch": 18.599897802759326,
      "grad_norm": 4.034178256988525,
      "learning_rate": 3.450008516436723e-05,
      "loss": 0.6384,
      "step": 2038400
    },
    {
      "epoch": 18.60081027812249,
      "grad_norm": 3.9953484535217285,
      "learning_rate": 3.449932476823126e-05,
      "loss": 0.6549,
      "step": 2038500
    },
    {
      "epoch": 18.601722753485657,
      "grad_norm": 4.642413139343262,
      "learning_rate": 3.449856437209529e-05,
      "loss": 0.6348,
      "step": 2038600
    },
    {
      "epoch": 18.602635228848822,
      "grad_norm": 4.35523796081543,
      "learning_rate": 3.449780397595932e-05,
      "loss": 0.6322,
      "step": 2038700
    },
    {
      "epoch": 18.603547704211987,
      "grad_norm": 2.841067314147949,
      "learning_rate": 3.449704357982335e-05,
      "loss": 0.6753,
      "step": 2038800
    },
    {
      "epoch": 18.604460179575153,
      "grad_norm": 4.478294372558594,
      "learning_rate": 3.449628318368738e-05,
      "loss": 0.6671,
      "step": 2038900
    },
    {
      "epoch": 18.605372654938318,
      "grad_norm": 2.079299211502075,
      "learning_rate": 3.44955227875514e-05,
      "loss": 0.6735,
      "step": 2039000
    },
    {
      "epoch": 18.606285130301483,
      "grad_norm": 4.123105049133301,
      "learning_rate": 3.449476239141544e-05,
      "loss": 0.6454,
      "step": 2039100
    },
    {
      "epoch": 18.60719760566465,
      "grad_norm": 3.8694307804107666,
      "learning_rate": 3.449400199527946e-05,
      "loss": 0.6603,
      "step": 2039200
    },
    {
      "epoch": 18.608110081027814,
      "grad_norm": 3.8803510665893555,
      "learning_rate": 3.4493241599143486e-05,
      "loss": 0.6128,
      "step": 2039300
    },
    {
      "epoch": 18.60902255639098,
      "grad_norm": 3.635925531387329,
      "learning_rate": 3.449248120300752e-05,
      "loss": 0.6552,
      "step": 2039400
    },
    {
      "epoch": 18.609935031754144,
      "grad_norm": 4.050549030303955,
      "learning_rate": 3.4491720806871546e-05,
      "loss": 0.6568,
      "step": 2039500
    },
    {
      "epoch": 18.610847507117306,
      "grad_norm": 3.045351982116699,
      "learning_rate": 3.4490960410735576e-05,
      "loss": 0.6767,
      "step": 2039600
    },
    {
      "epoch": 18.61175998248047,
      "grad_norm": 3.5719597339630127,
      "learning_rate": 3.4490200014599606e-05,
      "loss": 0.6885,
      "step": 2039700
    },
    {
      "epoch": 18.612672457843637,
      "grad_norm": 3.664665937423706,
      "learning_rate": 3.4489439618463636e-05,
      "loss": 0.6948,
      "step": 2039800
    },
    {
      "epoch": 18.613584933206802,
      "grad_norm": 2.4106156826019287,
      "learning_rate": 3.4488679222327666e-05,
      "loss": 0.6648,
      "step": 2039900
    },
    {
      "epoch": 18.614497408569967,
      "grad_norm": 4.217706680297852,
      "learning_rate": 3.4487918826191696e-05,
      "loss": 0.6828,
      "step": 2040000
    },
    {
      "epoch": 18.615409883933133,
      "grad_norm": 3.783059597015381,
      "learning_rate": 3.448715843005572e-05,
      "loss": 0.6861,
      "step": 2040100
    },
    {
      "epoch": 18.616322359296298,
      "grad_norm": 4.070423603057861,
      "learning_rate": 3.4486398033919756e-05,
      "loss": 0.6564,
      "step": 2040200
    },
    {
      "epoch": 18.617234834659463,
      "grad_norm": 4.468356609344482,
      "learning_rate": 3.448563763778378e-05,
      "loss": 0.6482,
      "step": 2040300
    },
    {
      "epoch": 18.61814731002263,
      "grad_norm": 4.2266645431518555,
      "learning_rate": 3.448487724164781e-05,
      "loss": 0.6625,
      "step": 2040400
    },
    {
      "epoch": 18.619059785385794,
      "grad_norm": 3.974323034286499,
      "learning_rate": 3.448411684551184e-05,
      "loss": 0.6569,
      "step": 2040500
    },
    {
      "epoch": 18.61997226074896,
      "grad_norm": 3.470974922180176,
      "learning_rate": 3.448335644937587e-05,
      "loss": 0.7019,
      "step": 2040600
    },
    {
      "epoch": 18.620884736112124,
      "grad_norm": 3.5308938026428223,
      "learning_rate": 3.448259605323989e-05,
      "loss": 0.6631,
      "step": 2040700
    },
    {
      "epoch": 18.62179721147529,
      "grad_norm": 4.235694408416748,
      "learning_rate": 3.448183565710393e-05,
      "loss": 0.6444,
      "step": 2040800
    },
    {
      "epoch": 18.622709686838455,
      "grad_norm": 4.41043758392334,
      "learning_rate": 3.448107526096795e-05,
      "loss": 0.66,
      "step": 2040900
    },
    {
      "epoch": 18.62362216220162,
      "grad_norm": 3.5194735527038574,
      "learning_rate": 3.4480314864831983e-05,
      "loss": 0.6676,
      "step": 2041000
    },
    {
      "epoch": 18.624534637564786,
      "grad_norm": 4.186332702636719,
      "learning_rate": 3.4479554468696014e-05,
      "loss": 0.6553,
      "step": 2041100
    },
    {
      "epoch": 18.62544711292795,
      "grad_norm": 3.5634138584136963,
      "learning_rate": 3.4478794072560044e-05,
      "loss": 0.6558,
      "step": 2041200
    },
    {
      "epoch": 18.626359588291116,
      "grad_norm": 4.793724536895752,
      "learning_rate": 3.4478033676424074e-05,
      "loss": 0.6938,
      "step": 2041300
    },
    {
      "epoch": 18.62727206365428,
      "grad_norm": 4.31157922744751,
      "learning_rate": 3.4477273280288104e-05,
      "loss": 0.6852,
      "step": 2041400
    },
    {
      "epoch": 18.628184539017447,
      "grad_norm": 4.611133098602295,
      "learning_rate": 3.447651288415213e-05,
      "loss": 0.6657,
      "step": 2041500
    },
    {
      "epoch": 18.629097014380612,
      "grad_norm": 3.9664783477783203,
      "learning_rate": 3.4475752488016164e-05,
      "loss": 0.6878,
      "step": 2041600
    },
    {
      "epoch": 18.630009489743777,
      "grad_norm": 3.753509521484375,
      "learning_rate": 3.447499209188019e-05,
      "loss": 0.6625,
      "step": 2041700
    },
    {
      "epoch": 18.630921965106943,
      "grad_norm": 4.686758518218994,
      "learning_rate": 3.447423169574422e-05,
      "loss": 0.6917,
      "step": 2041800
    },
    {
      "epoch": 18.631834440470108,
      "grad_norm": 3.6539065837860107,
      "learning_rate": 3.447347129960825e-05,
      "loss": 0.6663,
      "step": 2041900
    },
    {
      "epoch": 18.632746915833273,
      "grad_norm": 4.179256916046143,
      "learning_rate": 3.447271090347228e-05,
      "loss": 0.667,
      "step": 2042000
    },
    {
      "epoch": 18.63365939119644,
      "grad_norm": 3.8582494258880615,
      "learning_rate": 3.44719505073363e-05,
      "loss": 0.655,
      "step": 2042100
    },
    {
      "epoch": 18.634571866559604,
      "grad_norm": 3.249380350112915,
      "learning_rate": 3.447119011120033e-05,
      "loss": 0.6824,
      "step": 2042200
    },
    {
      "epoch": 18.63548434192277,
      "grad_norm": 3.3101248741149902,
      "learning_rate": 3.447042971506436e-05,
      "loss": 0.6309,
      "step": 2042300
    },
    {
      "epoch": 18.636396817285934,
      "grad_norm": 2.8958933353424072,
      "learning_rate": 3.446966931892839e-05,
      "loss": 0.6646,
      "step": 2042400
    },
    {
      "epoch": 18.6373092926491,
      "grad_norm": 3.1579880714416504,
      "learning_rate": 3.446890892279242e-05,
      "loss": 0.6542,
      "step": 2042500
    },
    {
      "epoch": 18.638221768012265,
      "grad_norm": 3.3028922080993652,
      "learning_rate": 3.4468148526656444e-05,
      "loss": 0.6604,
      "step": 2042600
    },
    {
      "epoch": 18.63913424337543,
      "grad_norm": 3.451585292816162,
      "learning_rate": 3.446738813052048e-05,
      "loss": 0.7061,
      "step": 2042700
    },
    {
      "epoch": 18.640046718738596,
      "grad_norm": 3.877666711807251,
      "learning_rate": 3.4466627734384504e-05,
      "loss": 0.6486,
      "step": 2042800
    },
    {
      "epoch": 18.64095919410176,
      "grad_norm": 4.307553768157959,
      "learning_rate": 3.4465867338248534e-05,
      "loss": 0.6742,
      "step": 2042900
    },
    {
      "epoch": 18.641871669464926,
      "grad_norm": 4.343006134033203,
      "learning_rate": 3.4465106942112564e-05,
      "loss": 0.6524,
      "step": 2043000
    },
    {
      "epoch": 18.642784144828088,
      "grad_norm": 3.048353910446167,
      "learning_rate": 3.4464346545976595e-05,
      "loss": 0.707,
      "step": 2043100
    },
    {
      "epoch": 18.643696620191253,
      "grad_norm": 3.5696964263916016,
      "learning_rate": 3.446358614984062e-05,
      "loss": 0.6373,
      "step": 2043200
    },
    {
      "epoch": 18.64460909555442,
      "grad_norm": 3.8306028842926025,
      "learning_rate": 3.4462825753704655e-05,
      "loss": 0.701,
      "step": 2043300
    },
    {
      "epoch": 18.645521570917584,
      "grad_norm": 4.781391143798828,
      "learning_rate": 3.446206535756868e-05,
      "loss": 0.6368,
      "step": 2043400
    },
    {
      "epoch": 18.64643404628075,
      "grad_norm": 3.6531319618225098,
      "learning_rate": 3.446130496143271e-05,
      "loss": 0.6731,
      "step": 2043500
    },
    {
      "epoch": 18.647346521643914,
      "grad_norm": 4.449125289916992,
      "learning_rate": 3.446054456529674e-05,
      "loss": 0.7046,
      "step": 2043600
    },
    {
      "epoch": 18.64825899700708,
      "grad_norm": 3.395045518875122,
      "learning_rate": 3.445978416916077e-05,
      "loss": 0.6327,
      "step": 2043700
    },
    {
      "epoch": 18.649171472370245,
      "grad_norm": 3.0116567611694336,
      "learning_rate": 3.44590237730248e-05,
      "loss": 0.6356,
      "step": 2043800
    },
    {
      "epoch": 18.65008394773341,
      "grad_norm": 4.710053443908691,
      "learning_rate": 3.445826337688883e-05,
      "loss": 0.6806,
      "step": 2043900
    },
    {
      "epoch": 18.650996423096576,
      "grad_norm": 3.6054835319519043,
      "learning_rate": 3.445750298075285e-05,
      "loss": 0.6252,
      "step": 2044000
    },
    {
      "epoch": 18.65190889845974,
      "grad_norm": 4.219585418701172,
      "learning_rate": 3.445674258461689e-05,
      "loss": 0.6442,
      "step": 2044100
    },
    {
      "epoch": 18.652821373822906,
      "grad_norm": 2.9701480865478516,
      "learning_rate": 3.445598218848091e-05,
      "loss": 0.6384,
      "step": 2044200
    },
    {
      "epoch": 18.65373384918607,
      "grad_norm": 3.9118313789367676,
      "learning_rate": 3.445522179234494e-05,
      "loss": 0.6417,
      "step": 2044300
    },
    {
      "epoch": 18.654646324549237,
      "grad_norm": 3.695051670074463,
      "learning_rate": 3.445446139620897e-05,
      "loss": 0.6589,
      "step": 2044400
    },
    {
      "epoch": 18.655558799912402,
      "grad_norm": 4.170443534851074,
      "learning_rate": 3.4453701000073e-05,
      "loss": 0.6319,
      "step": 2044500
    },
    {
      "epoch": 18.656471275275567,
      "grad_norm": 3.892101287841797,
      "learning_rate": 3.4452940603937025e-05,
      "loss": 0.6729,
      "step": 2044600
    },
    {
      "epoch": 18.657383750638733,
      "grad_norm": 3.761781930923462,
      "learning_rate": 3.445218020780106e-05,
      "loss": 0.7078,
      "step": 2044700
    },
    {
      "epoch": 18.658296226001898,
      "grad_norm": 4.1131792068481445,
      "learning_rate": 3.4451419811665085e-05,
      "loss": 0.6719,
      "step": 2044800
    },
    {
      "epoch": 18.659208701365063,
      "grad_norm": 3.394815683364868,
      "learning_rate": 3.4450659415529115e-05,
      "loss": 0.6472,
      "step": 2044900
    },
    {
      "epoch": 18.66012117672823,
      "grad_norm": 3.668107509613037,
      "learning_rate": 3.4449899019393145e-05,
      "loss": 0.6797,
      "step": 2045000
    },
    {
      "epoch": 18.661033652091394,
      "grad_norm": 3.649543523788452,
      "learning_rate": 3.444913862325717e-05,
      "loss": 0.6307,
      "step": 2045100
    },
    {
      "epoch": 18.66194612745456,
      "grad_norm": 3.2071733474731445,
      "learning_rate": 3.4448378227121206e-05,
      "loss": 0.6494,
      "step": 2045200
    },
    {
      "epoch": 18.662858602817725,
      "grad_norm": 5.239717960357666,
      "learning_rate": 3.444761783098523e-05,
      "loss": 0.6436,
      "step": 2045300
    },
    {
      "epoch": 18.66377107818089,
      "grad_norm": 3.9349851608276367,
      "learning_rate": 3.444685743484926e-05,
      "loss": 0.7082,
      "step": 2045400
    },
    {
      "epoch": 18.664683553544055,
      "grad_norm": 3.8521549701690674,
      "learning_rate": 3.444609703871329e-05,
      "loss": 0.6921,
      "step": 2045500
    },
    {
      "epoch": 18.66559602890722,
      "grad_norm": 3.9890143871307373,
      "learning_rate": 3.444533664257732e-05,
      "loss": 0.6673,
      "step": 2045600
    },
    {
      "epoch": 18.666508504270386,
      "grad_norm": 3.681570053100586,
      "learning_rate": 3.444457624644134e-05,
      "loss": 0.6713,
      "step": 2045700
    },
    {
      "epoch": 18.66742097963355,
      "grad_norm": 3.890061616897583,
      "learning_rate": 3.444381585030538e-05,
      "loss": 0.654,
      "step": 2045800
    },
    {
      "epoch": 18.668333454996716,
      "grad_norm": 3.427051544189453,
      "learning_rate": 3.44430554541694e-05,
      "loss": 0.6757,
      "step": 2045900
    },
    {
      "epoch": 18.66924593035988,
      "grad_norm": 4.140370845794678,
      "learning_rate": 3.444229505803343e-05,
      "loss": 0.635,
      "step": 2046000
    },
    {
      "epoch": 18.670158405723047,
      "grad_norm": 4.983792781829834,
      "learning_rate": 3.444153466189746e-05,
      "loss": 0.6431,
      "step": 2046100
    },
    {
      "epoch": 18.671070881086212,
      "grad_norm": 4.336313247680664,
      "learning_rate": 3.444077426576149e-05,
      "loss": 0.6483,
      "step": 2046200
    },
    {
      "epoch": 18.671983356449378,
      "grad_norm": 3.916810989379883,
      "learning_rate": 3.444001386962552e-05,
      "loss": 0.6819,
      "step": 2046300
    },
    {
      "epoch": 18.67289583181254,
      "grad_norm": 3.544405698776245,
      "learning_rate": 3.443925347348955e-05,
      "loss": 0.6298,
      "step": 2046400
    },
    {
      "epoch": 18.673808307175705,
      "grad_norm": 4.218563556671143,
      "learning_rate": 3.4438493077353576e-05,
      "loss": 0.6617,
      "step": 2046500
    },
    {
      "epoch": 18.67472078253887,
      "grad_norm": 4.364567756652832,
      "learning_rate": 3.443773268121761e-05,
      "loss": 0.6717,
      "step": 2046600
    },
    {
      "epoch": 18.675633257902035,
      "grad_norm": 4.2881011962890625,
      "learning_rate": 3.4436972285081636e-05,
      "loss": 0.6842,
      "step": 2046700
    },
    {
      "epoch": 18.6765457332652,
      "grad_norm": 2.668144702911377,
      "learning_rate": 3.4436211888945666e-05,
      "loss": 0.6709,
      "step": 2046800
    },
    {
      "epoch": 18.677458208628366,
      "grad_norm": 4.539759635925293,
      "learning_rate": 3.4435451492809696e-05,
      "loss": 0.6892,
      "step": 2046900
    },
    {
      "epoch": 18.67837068399153,
      "grad_norm": 4.015462398529053,
      "learning_rate": 3.4434691096673726e-05,
      "loss": 0.6982,
      "step": 2047000
    },
    {
      "epoch": 18.679283159354696,
      "grad_norm": 3.956573486328125,
      "learning_rate": 3.443393070053775e-05,
      "loss": 0.6706,
      "step": 2047100
    },
    {
      "epoch": 18.68019563471786,
      "grad_norm": 3.7268645763397217,
      "learning_rate": 3.443317030440179e-05,
      "loss": 0.7163,
      "step": 2047200
    },
    {
      "epoch": 18.681108110081027,
      "grad_norm": 4.645123481750488,
      "learning_rate": 3.443240990826581e-05,
      "loss": 0.66,
      "step": 2047300
    },
    {
      "epoch": 18.682020585444192,
      "grad_norm": 4.2568039894104,
      "learning_rate": 3.443164951212984e-05,
      "loss": 0.6278,
      "step": 2047400
    },
    {
      "epoch": 18.682933060807358,
      "grad_norm": 3.5943186283111572,
      "learning_rate": 3.443088911599387e-05,
      "loss": 0.6276,
      "step": 2047500
    },
    {
      "epoch": 18.683845536170523,
      "grad_norm": 3.9754743576049805,
      "learning_rate": 3.44301287198579e-05,
      "loss": 0.6357,
      "step": 2047600
    },
    {
      "epoch": 18.684758011533688,
      "grad_norm": 3.9056899547576904,
      "learning_rate": 3.442936832372193e-05,
      "loss": 0.6626,
      "step": 2047700
    },
    {
      "epoch": 18.685670486896854,
      "grad_norm": 3.6734163761138916,
      "learning_rate": 3.4428607927585953e-05,
      "loss": 0.7105,
      "step": 2047800
    },
    {
      "epoch": 18.68658296226002,
      "grad_norm": 4.004039764404297,
      "learning_rate": 3.4427847531449984e-05,
      "loss": 0.641,
      "step": 2047900
    },
    {
      "epoch": 18.687495437623184,
      "grad_norm": 4.427500247955322,
      "learning_rate": 3.4427087135314014e-05,
      "loss": 0.6655,
      "step": 2048000
    },
    {
      "epoch": 18.68840791298635,
      "grad_norm": 4.286149024963379,
      "learning_rate": 3.4426326739178044e-05,
      "loss": 0.6697,
      "step": 2048100
    },
    {
      "epoch": 18.689320388349515,
      "grad_norm": 2.9142510890960693,
      "learning_rate": 3.4425566343042074e-05,
      "loss": 0.669,
      "step": 2048200
    },
    {
      "epoch": 18.69023286371268,
      "grad_norm": 3.570091962814331,
      "learning_rate": 3.4424805946906104e-05,
      "loss": 0.6672,
      "step": 2048300
    },
    {
      "epoch": 18.691145339075845,
      "grad_norm": 4.392977714538574,
      "learning_rate": 3.442404555077013e-05,
      "loss": 0.6475,
      "step": 2048400
    },
    {
      "epoch": 18.69205781443901,
      "grad_norm": 3.2839088439941406,
      "learning_rate": 3.4423285154634164e-05,
      "loss": 0.6612,
      "step": 2048500
    },
    {
      "epoch": 18.692970289802176,
      "grad_norm": 3.529420852661133,
      "learning_rate": 3.442252475849819e-05,
      "loss": 0.6474,
      "step": 2048600
    },
    {
      "epoch": 18.69388276516534,
      "grad_norm": 4.694687843322754,
      "learning_rate": 3.442176436236222e-05,
      "loss": 0.6932,
      "step": 2048700
    },
    {
      "epoch": 18.694795240528506,
      "grad_norm": 3.4836127758026123,
      "learning_rate": 3.442100396622625e-05,
      "loss": 0.6656,
      "step": 2048800
    },
    {
      "epoch": 18.695707715891672,
      "grad_norm": 4.179278373718262,
      "learning_rate": 3.442024357009028e-05,
      "loss": 0.6677,
      "step": 2048900
    },
    {
      "epoch": 18.696620191254837,
      "grad_norm": 3.588254928588867,
      "learning_rate": 3.44194831739543e-05,
      "loss": 0.6687,
      "step": 2049000
    },
    {
      "epoch": 18.697532666618002,
      "grad_norm": 3.9789819717407227,
      "learning_rate": 3.441872277781834e-05,
      "loss": 0.6764,
      "step": 2049100
    },
    {
      "epoch": 18.698445141981168,
      "grad_norm": 3.7858285903930664,
      "learning_rate": 3.441796238168236e-05,
      "loss": 0.6929,
      "step": 2049200
    },
    {
      "epoch": 18.699357617344333,
      "grad_norm": 3.823819637298584,
      "learning_rate": 3.441720198554639e-05,
      "loss": 0.6782,
      "step": 2049300
    },
    {
      "epoch": 18.7002700927075,
      "grad_norm": 3.703623056411743,
      "learning_rate": 3.441644158941042e-05,
      "loss": 0.6821,
      "step": 2049400
    },
    {
      "epoch": 18.701182568070664,
      "grad_norm": 3.963956594467163,
      "learning_rate": 3.441568119327445e-05,
      "loss": 0.7053,
      "step": 2049500
    },
    {
      "epoch": 18.70209504343383,
      "grad_norm": 3.4973278045654297,
      "learning_rate": 3.441492079713848e-05,
      "loss": 0.6298,
      "step": 2049600
    },
    {
      "epoch": 18.703007518796994,
      "grad_norm": 2.980896472930908,
      "learning_rate": 3.441416040100251e-05,
      "loss": 0.6448,
      "step": 2049700
    },
    {
      "epoch": 18.70391999416016,
      "grad_norm": 4.654996395111084,
      "learning_rate": 3.4413400004866534e-05,
      "loss": 0.6366,
      "step": 2049800
    },
    {
      "epoch": 18.70483246952332,
      "grad_norm": 3.954725503921509,
      "learning_rate": 3.441263960873057e-05,
      "loss": 0.68,
      "step": 2049900
    },
    {
      "epoch": 18.705744944886487,
      "grad_norm": 4.8757243156433105,
      "learning_rate": 3.4411879212594595e-05,
      "loss": 0.6518,
      "step": 2050000
    },
    {
      "epoch": 18.706657420249652,
      "grad_norm": 4.122957229614258,
      "learning_rate": 3.4411118816458625e-05,
      "loss": 0.6554,
      "step": 2050100
    },
    {
      "epoch": 18.707569895612817,
      "grad_norm": 4.490984916687012,
      "learning_rate": 3.4410358420322655e-05,
      "loss": 0.6428,
      "step": 2050200
    },
    {
      "epoch": 18.708482370975982,
      "grad_norm": 3.506270408630371,
      "learning_rate": 3.4409598024186685e-05,
      "loss": 0.6753,
      "step": 2050300
    },
    {
      "epoch": 18.709394846339148,
      "grad_norm": 4.343392848968506,
      "learning_rate": 3.440883762805071e-05,
      "loss": 0.695,
      "step": 2050400
    },
    {
      "epoch": 18.710307321702313,
      "grad_norm": 3.509896993637085,
      "learning_rate": 3.4408077231914745e-05,
      "loss": 0.6339,
      "step": 2050500
    },
    {
      "epoch": 18.71121979706548,
      "grad_norm": 3.431234121322632,
      "learning_rate": 3.440731683577877e-05,
      "loss": 0.6806,
      "step": 2050600
    },
    {
      "epoch": 18.712132272428644,
      "grad_norm": 3.967587471008301,
      "learning_rate": 3.44065564396428e-05,
      "loss": 0.6791,
      "step": 2050700
    },
    {
      "epoch": 18.71304474779181,
      "grad_norm": 4.301313400268555,
      "learning_rate": 3.440579604350683e-05,
      "loss": 0.6614,
      "step": 2050800
    },
    {
      "epoch": 18.713957223154974,
      "grad_norm": 4.328845024108887,
      "learning_rate": 3.440503564737085e-05,
      "loss": 0.6569,
      "step": 2050900
    },
    {
      "epoch": 18.71486969851814,
      "grad_norm": 3.3957936763763428,
      "learning_rate": 3.440427525123489e-05,
      "loss": 0.6604,
      "step": 2051000
    },
    {
      "epoch": 18.715782173881305,
      "grad_norm": 3.8660197257995605,
      "learning_rate": 3.440351485509891e-05,
      "loss": 0.6873,
      "step": 2051100
    },
    {
      "epoch": 18.71669464924447,
      "grad_norm": 3.119368553161621,
      "learning_rate": 3.440275445896294e-05,
      "loss": 0.6775,
      "step": 2051200
    },
    {
      "epoch": 18.717607124607635,
      "grad_norm": 3.6911911964416504,
      "learning_rate": 3.440199406282697e-05,
      "loss": 0.686,
      "step": 2051300
    },
    {
      "epoch": 18.7185195999708,
      "grad_norm": 4.688561916351318,
      "learning_rate": 3.4401233666691e-05,
      "loss": 0.6743,
      "step": 2051400
    },
    {
      "epoch": 18.719432075333966,
      "grad_norm": 3.4843220710754395,
      "learning_rate": 3.4400473270555025e-05,
      "loss": 0.6661,
      "step": 2051500
    },
    {
      "epoch": 18.72034455069713,
      "grad_norm": 3.675950288772583,
      "learning_rate": 3.439971287441906e-05,
      "loss": 0.6517,
      "step": 2051600
    },
    {
      "epoch": 18.721257026060297,
      "grad_norm": 3.669710874557495,
      "learning_rate": 3.4398952478283085e-05,
      "loss": 0.6777,
      "step": 2051700
    },
    {
      "epoch": 18.722169501423462,
      "grad_norm": 4.330977439880371,
      "learning_rate": 3.4398192082147116e-05,
      "loss": 0.6813,
      "step": 2051800
    },
    {
      "epoch": 18.723081976786627,
      "grad_norm": 4.0731329917907715,
      "learning_rate": 3.4397431686011146e-05,
      "loss": 0.646,
      "step": 2051900
    },
    {
      "epoch": 18.723994452149793,
      "grad_norm": 3.761826515197754,
      "learning_rate": 3.4396671289875176e-05,
      "loss": 0.6769,
      "step": 2052000
    },
    {
      "epoch": 18.724906927512958,
      "grad_norm": 4.334430694580078,
      "learning_rate": 3.4395910893739206e-05,
      "loss": 0.6859,
      "step": 2052100
    },
    {
      "epoch": 18.725819402876123,
      "grad_norm": 4.976860046386719,
      "learning_rate": 3.4395150497603236e-05,
      "loss": 0.6646,
      "step": 2052200
    },
    {
      "epoch": 18.72673187823929,
      "grad_norm": 3.4832327365875244,
      "learning_rate": 3.439439010146726e-05,
      "loss": 0.6886,
      "step": 2052300
    },
    {
      "epoch": 18.727644353602454,
      "grad_norm": 3.512739419937134,
      "learning_rate": 3.4393629705331296e-05,
      "loss": 0.6491,
      "step": 2052400
    },
    {
      "epoch": 18.72855682896562,
      "grad_norm": 4.2361860275268555,
      "learning_rate": 3.439286930919532e-05,
      "loss": 0.6519,
      "step": 2052500
    },
    {
      "epoch": 18.729469304328784,
      "grad_norm": 3.7419586181640625,
      "learning_rate": 3.439210891305935e-05,
      "loss": 0.6871,
      "step": 2052600
    },
    {
      "epoch": 18.73038177969195,
      "grad_norm": 4.130328178405762,
      "learning_rate": 3.439134851692338e-05,
      "loss": 0.6502,
      "step": 2052700
    },
    {
      "epoch": 18.731294255055115,
      "grad_norm": 3.9236626625061035,
      "learning_rate": 3.439058812078741e-05,
      "loss": 0.6576,
      "step": 2052800
    },
    {
      "epoch": 18.73220673041828,
      "grad_norm": 3.592526912689209,
      "learning_rate": 3.438982772465143e-05,
      "loss": 0.636,
      "step": 2052900
    },
    {
      "epoch": 18.733119205781446,
      "grad_norm": 3.491276979446411,
      "learning_rate": 3.438906732851547e-05,
      "loss": 0.6513,
      "step": 2053000
    },
    {
      "epoch": 18.73403168114461,
      "grad_norm": 5.060975551605225,
      "learning_rate": 3.438830693237949e-05,
      "loss": 0.6988,
      "step": 2053100
    },
    {
      "epoch": 18.734944156507773,
      "grad_norm": 4.448373794555664,
      "learning_rate": 3.438754653624352e-05,
      "loss": 0.6614,
      "step": 2053200
    },
    {
      "epoch": 18.735856631870938,
      "grad_norm": 4.714470863342285,
      "learning_rate": 3.438678614010755e-05,
      "loss": 0.6566,
      "step": 2053300
    },
    {
      "epoch": 18.736769107234103,
      "grad_norm": 3.190805196762085,
      "learning_rate": 3.438602574397158e-05,
      "loss": 0.6536,
      "step": 2053400
    },
    {
      "epoch": 18.73768158259727,
      "grad_norm": 4.277966022491455,
      "learning_rate": 3.438526534783561e-05,
      "loss": 0.6569,
      "step": 2053500
    },
    {
      "epoch": 18.738594057960434,
      "grad_norm": 3.879363775253296,
      "learning_rate": 3.4384504951699636e-05,
      "loss": 0.6357,
      "step": 2053600
    },
    {
      "epoch": 18.7395065333236,
      "grad_norm": 4.877232551574707,
      "learning_rate": 3.4383744555563666e-05,
      "loss": 0.6664,
      "step": 2053700
    },
    {
      "epoch": 18.740419008686764,
      "grad_norm": 3.800968885421753,
      "learning_rate": 3.4382984159427697e-05,
      "loss": 0.6996,
      "step": 2053800
    },
    {
      "epoch": 18.74133148404993,
      "grad_norm": 4.4615654945373535,
      "learning_rate": 3.4382223763291727e-05,
      "loss": 0.6752,
      "step": 2053900
    },
    {
      "epoch": 18.742243959413095,
      "grad_norm": 3.5060222148895264,
      "learning_rate": 3.438146336715575e-05,
      "loss": 0.667,
      "step": 2054000
    },
    {
      "epoch": 18.74315643477626,
      "grad_norm": 3.786487340927124,
      "learning_rate": 3.438070297101979e-05,
      "loss": 0.6739,
      "step": 2054100
    },
    {
      "epoch": 18.744068910139426,
      "grad_norm": 4.544286727905273,
      "learning_rate": 3.437994257488381e-05,
      "loss": 0.6564,
      "step": 2054200
    },
    {
      "epoch": 18.74498138550259,
      "grad_norm": 3.9074666500091553,
      "learning_rate": 3.437918217874784e-05,
      "loss": 0.6437,
      "step": 2054300
    },
    {
      "epoch": 18.745893860865756,
      "grad_norm": 4.426806449890137,
      "learning_rate": 3.437842178261187e-05,
      "loss": 0.6882,
      "step": 2054400
    },
    {
      "epoch": 18.74680633622892,
      "grad_norm": 4.480005264282227,
      "learning_rate": 3.43776613864759e-05,
      "loss": 0.6559,
      "step": 2054500
    },
    {
      "epoch": 18.747718811592087,
      "grad_norm": 4.0352702140808105,
      "learning_rate": 3.437690099033993e-05,
      "loss": 0.6551,
      "step": 2054600
    },
    {
      "epoch": 18.748631286955252,
      "grad_norm": 3.809314012527466,
      "learning_rate": 3.437614059420396e-05,
      "loss": 0.6627,
      "step": 2054700
    },
    {
      "epoch": 18.749543762318417,
      "grad_norm": 4.625702857971191,
      "learning_rate": 3.4375380198067984e-05,
      "loss": 0.6532,
      "step": 2054800
    },
    {
      "epoch": 18.750456237681583,
      "grad_norm": 4.199047088623047,
      "learning_rate": 3.437461980193202e-05,
      "loss": 0.648,
      "step": 2054900
    },
    {
      "epoch": 18.751368713044748,
      "grad_norm": 4.058096885681152,
      "learning_rate": 3.4373859405796044e-05,
      "loss": 0.6597,
      "step": 2055000
    },
    {
      "epoch": 18.752281188407913,
      "grad_norm": 4.749964237213135,
      "learning_rate": 3.4373099009660074e-05,
      "loss": 0.6099,
      "step": 2055100
    },
    {
      "epoch": 18.75319366377108,
      "grad_norm": 4.564065456390381,
      "learning_rate": 3.4372338613524104e-05,
      "loss": 0.6655,
      "step": 2055200
    },
    {
      "epoch": 18.754106139134244,
      "grad_norm": 4.620087623596191,
      "learning_rate": 3.4371578217388134e-05,
      "loss": 0.6311,
      "step": 2055300
    },
    {
      "epoch": 18.75501861449741,
      "grad_norm": 4.46236515045166,
      "learning_rate": 3.437081782125216e-05,
      "loss": 0.6467,
      "step": 2055400
    },
    {
      "epoch": 18.755931089860574,
      "grad_norm": 3.800330877304077,
      "learning_rate": 3.4370057425116194e-05,
      "loss": 0.6588,
      "step": 2055500
    },
    {
      "epoch": 18.75684356522374,
      "grad_norm": 4.114412307739258,
      "learning_rate": 3.436929702898022e-05,
      "loss": 0.6682,
      "step": 2055600
    },
    {
      "epoch": 18.757756040586905,
      "grad_norm": 4.366447448730469,
      "learning_rate": 3.436853663284425e-05,
      "loss": 0.6393,
      "step": 2055700
    },
    {
      "epoch": 18.75866851595007,
      "grad_norm": 4.154671669006348,
      "learning_rate": 3.436777623670828e-05,
      "loss": 0.6631,
      "step": 2055800
    },
    {
      "epoch": 18.759580991313236,
      "grad_norm": 3.739814519882202,
      "learning_rate": 3.436701584057231e-05,
      "loss": 0.6506,
      "step": 2055900
    },
    {
      "epoch": 18.7604934666764,
      "grad_norm": 5.04339075088501,
      "learning_rate": 3.436625544443634e-05,
      "loss": 0.6824,
      "step": 2056000
    },
    {
      "epoch": 18.761405942039566,
      "grad_norm": 3.9655988216400146,
      "learning_rate": 3.436549504830037e-05,
      "loss": 0.6828,
      "step": 2056100
    },
    {
      "epoch": 18.76231841740273,
      "grad_norm": 3.7593016624450684,
      "learning_rate": 3.436473465216439e-05,
      "loss": 0.6256,
      "step": 2056200
    },
    {
      "epoch": 18.763230892765897,
      "grad_norm": 3.345925807952881,
      "learning_rate": 3.436397425602842e-05,
      "loss": 0.6549,
      "step": 2056300
    },
    {
      "epoch": 18.764143368129062,
      "grad_norm": 3.9710500240325928,
      "learning_rate": 3.436321385989245e-05,
      "loss": 0.6393,
      "step": 2056400
    },
    {
      "epoch": 18.765055843492227,
      "grad_norm": 3.530975103378296,
      "learning_rate": 3.4362453463756474e-05,
      "loss": 0.6575,
      "step": 2056500
    },
    {
      "epoch": 18.76596831885539,
      "grad_norm": 4.172142028808594,
      "learning_rate": 3.436169306762051e-05,
      "loss": 0.6769,
      "step": 2056600
    },
    {
      "epoch": 18.766880794218554,
      "grad_norm": 3.2701828479766846,
      "learning_rate": 3.4360932671484535e-05,
      "loss": 0.6595,
      "step": 2056700
    },
    {
      "epoch": 18.76779326958172,
      "grad_norm": 3.960599184036255,
      "learning_rate": 3.4360172275348565e-05,
      "loss": 0.6703,
      "step": 2056800
    },
    {
      "epoch": 18.768705744944885,
      "grad_norm": 3.7821338176727295,
      "learning_rate": 3.4359411879212595e-05,
      "loss": 0.6392,
      "step": 2056900
    },
    {
      "epoch": 18.76961822030805,
      "grad_norm": 4.563690662384033,
      "learning_rate": 3.4358651483076625e-05,
      "loss": 0.6833,
      "step": 2057000
    },
    {
      "epoch": 18.770530695671216,
      "grad_norm": 4.4298834800720215,
      "learning_rate": 3.4357891086940655e-05,
      "loss": 0.6308,
      "step": 2057100
    },
    {
      "epoch": 18.77144317103438,
      "grad_norm": 4.506730079650879,
      "learning_rate": 3.4357130690804685e-05,
      "loss": 0.6474,
      "step": 2057200
    },
    {
      "epoch": 18.772355646397546,
      "grad_norm": 4.179225921630859,
      "learning_rate": 3.435637029466871e-05,
      "loss": 0.6572,
      "step": 2057300
    },
    {
      "epoch": 18.77326812176071,
      "grad_norm": 4.289687633514404,
      "learning_rate": 3.4355609898532745e-05,
      "loss": 0.6436,
      "step": 2057400
    },
    {
      "epoch": 18.774180597123877,
      "grad_norm": 3.650991201400757,
      "learning_rate": 3.435484950239677e-05,
      "loss": 0.642,
      "step": 2057500
    },
    {
      "epoch": 18.775093072487042,
      "grad_norm": 4.417523384094238,
      "learning_rate": 3.43540891062608e-05,
      "loss": 0.6905,
      "step": 2057600
    },
    {
      "epoch": 18.776005547850207,
      "grad_norm": 3.211232900619507,
      "learning_rate": 3.435332871012483e-05,
      "loss": 0.6475,
      "step": 2057700
    },
    {
      "epoch": 18.776918023213373,
      "grad_norm": 4.6238837242126465,
      "learning_rate": 3.435256831398886e-05,
      "loss": 0.6583,
      "step": 2057800
    },
    {
      "epoch": 18.777830498576538,
      "grad_norm": 3.3852317333221436,
      "learning_rate": 3.435180791785288e-05,
      "loss": 0.6939,
      "step": 2057900
    },
    {
      "epoch": 18.778742973939703,
      "grad_norm": 4.049196243286133,
      "learning_rate": 3.435104752171692e-05,
      "loss": 0.6756,
      "step": 2058000
    },
    {
      "epoch": 18.77965544930287,
      "grad_norm": 4.262058258056641,
      "learning_rate": 3.435028712558094e-05,
      "loss": 0.6125,
      "step": 2058100
    },
    {
      "epoch": 18.780567924666034,
      "grad_norm": 4.497665882110596,
      "learning_rate": 3.434952672944497e-05,
      "loss": 0.6721,
      "step": 2058200
    },
    {
      "epoch": 18.7814804000292,
      "grad_norm": 4.05910062789917,
      "learning_rate": 3.4348766333309e-05,
      "loss": 0.6651,
      "step": 2058300
    },
    {
      "epoch": 18.782392875392365,
      "grad_norm": 4.0565266609191895,
      "learning_rate": 3.434800593717303e-05,
      "loss": 0.6745,
      "step": 2058400
    },
    {
      "epoch": 18.78330535075553,
      "grad_norm": 3.908290386199951,
      "learning_rate": 3.434724554103706e-05,
      "loss": 0.6976,
      "step": 2058500
    },
    {
      "epoch": 18.784217826118695,
      "grad_norm": 3.7433109283447266,
      "learning_rate": 3.434648514490109e-05,
      "loss": 0.6381,
      "step": 2058600
    },
    {
      "epoch": 18.78513030148186,
      "grad_norm": 3.5644521713256836,
      "learning_rate": 3.4345724748765116e-05,
      "loss": 0.6589,
      "step": 2058700
    },
    {
      "epoch": 18.786042776845026,
      "grad_norm": 4.0417375564575195,
      "learning_rate": 3.434496435262915e-05,
      "loss": 0.6514,
      "step": 2058800
    },
    {
      "epoch": 18.78695525220819,
      "grad_norm": 3.1883816719055176,
      "learning_rate": 3.4344203956493176e-05,
      "loss": 0.6561,
      "step": 2058900
    },
    {
      "epoch": 18.787867727571356,
      "grad_norm": 3.9514737129211426,
      "learning_rate": 3.4343443560357206e-05,
      "loss": 0.6189,
      "step": 2059000
    },
    {
      "epoch": 18.78878020293452,
      "grad_norm": 3.025578737258911,
      "learning_rate": 3.4342683164221236e-05,
      "loss": 0.6377,
      "step": 2059100
    },
    {
      "epoch": 18.789692678297687,
      "grad_norm": 5.433185577392578,
      "learning_rate": 3.434192276808526e-05,
      "loss": 0.6703,
      "step": 2059200
    },
    {
      "epoch": 18.790605153660852,
      "grad_norm": 3.333911895751953,
      "learning_rate": 3.434116237194929e-05,
      "loss": 0.6773,
      "step": 2059300
    },
    {
      "epoch": 18.791517629024018,
      "grad_norm": 4.141446113586426,
      "learning_rate": 3.434040197581332e-05,
      "loss": 0.633,
      "step": 2059400
    },
    {
      "epoch": 18.792430104387183,
      "grad_norm": 2.2014596462249756,
      "learning_rate": 3.433964157967735e-05,
      "loss": 0.6302,
      "step": 2059500
    },
    {
      "epoch": 18.793342579750348,
      "grad_norm": 5.630681037902832,
      "learning_rate": 3.433888118354138e-05,
      "loss": 0.6347,
      "step": 2059600
    },
    {
      "epoch": 18.794255055113513,
      "grad_norm": 3.1652729511260986,
      "learning_rate": 3.433812078740541e-05,
      "loss": 0.6145,
      "step": 2059700
    },
    {
      "epoch": 18.79516753047668,
      "grad_norm": 4.7735595703125,
      "learning_rate": 3.433736039126943e-05,
      "loss": 0.6578,
      "step": 2059800
    },
    {
      "epoch": 18.79608000583984,
      "grad_norm": 3.772001266479492,
      "learning_rate": 3.433659999513347e-05,
      "loss": 0.691,
      "step": 2059900
    },
    {
      "epoch": 18.796992481203006,
      "grad_norm": 3.8185486793518066,
      "learning_rate": 3.433583959899749e-05,
      "loss": 0.6465,
      "step": 2060000
    },
    {
      "epoch": 18.79790495656617,
      "grad_norm": 4.246148109436035,
      "learning_rate": 3.433507920286152e-05,
      "loss": 0.6744,
      "step": 2060100
    },
    {
      "epoch": 18.798817431929336,
      "grad_norm": 4.482675075531006,
      "learning_rate": 3.433431880672555e-05,
      "loss": 0.7008,
      "step": 2060200
    },
    {
      "epoch": 18.7997299072925,
      "grad_norm": 3.3463387489318848,
      "learning_rate": 3.433355841058958e-05,
      "loss": 0.6494,
      "step": 2060300
    },
    {
      "epoch": 18.800642382655667,
      "grad_norm": 4.250230312347412,
      "learning_rate": 3.433279801445361e-05,
      "loss": 0.6425,
      "step": 2060400
    },
    {
      "epoch": 18.801554858018832,
      "grad_norm": 4.236074924468994,
      "learning_rate": 3.433203761831764e-05,
      "loss": 0.6756,
      "step": 2060500
    },
    {
      "epoch": 18.802467333381998,
      "grad_norm": 4.90374231338501,
      "learning_rate": 3.4331277222181667e-05,
      "loss": 0.6466,
      "step": 2060600
    },
    {
      "epoch": 18.803379808745163,
      "grad_norm": 4.896083831787109,
      "learning_rate": 3.43305168260457e-05,
      "loss": 0.7022,
      "step": 2060700
    },
    {
      "epoch": 18.804292284108328,
      "grad_norm": 2.564847469329834,
      "learning_rate": 3.432975642990973e-05,
      "loss": 0.6426,
      "step": 2060800
    },
    {
      "epoch": 18.805204759471494,
      "grad_norm": 3.9366462230682373,
      "learning_rate": 3.432899603377376e-05,
      "loss": 0.6938,
      "step": 2060900
    },
    {
      "epoch": 18.80611723483466,
      "grad_norm": 4.639720916748047,
      "learning_rate": 3.432823563763779e-05,
      "loss": 0.6517,
      "step": 2061000
    },
    {
      "epoch": 18.807029710197824,
      "grad_norm": 3.600795030593872,
      "learning_rate": 3.432747524150182e-05,
      "loss": 0.6898,
      "step": 2061100
    },
    {
      "epoch": 18.80794218556099,
      "grad_norm": 3.346980571746826,
      "learning_rate": 3.432671484536584e-05,
      "loss": 0.6724,
      "step": 2061200
    },
    {
      "epoch": 18.808854660924155,
      "grad_norm": 3.537051200866699,
      "learning_rate": 3.432595444922988e-05,
      "loss": 0.6476,
      "step": 2061300
    },
    {
      "epoch": 18.80976713628732,
      "grad_norm": 4.598769187927246,
      "learning_rate": 3.43251940530939e-05,
      "loss": 0.6858,
      "step": 2061400
    },
    {
      "epoch": 18.810679611650485,
      "grad_norm": 4.010303497314453,
      "learning_rate": 3.432443365695793e-05,
      "loss": 0.6507,
      "step": 2061500
    },
    {
      "epoch": 18.81159208701365,
      "grad_norm": 4.072732925415039,
      "learning_rate": 3.432367326082196e-05,
      "loss": 0.634,
      "step": 2061600
    },
    {
      "epoch": 18.812504562376816,
      "grad_norm": 4.05571985244751,
      "learning_rate": 3.432291286468599e-05,
      "loss": 0.6525,
      "step": 2061700
    },
    {
      "epoch": 18.81341703773998,
      "grad_norm": 3.4414408206939697,
      "learning_rate": 3.432215246855002e-05,
      "loss": 0.6517,
      "step": 2061800
    },
    {
      "epoch": 18.814329513103146,
      "grad_norm": 3.9963345527648926,
      "learning_rate": 3.432139207241405e-05,
      "loss": 0.649,
      "step": 2061900
    },
    {
      "epoch": 18.815241988466312,
      "grad_norm": 4.109830856323242,
      "learning_rate": 3.4320631676278074e-05,
      "loss": 0.6534,
      "step": 2062000
    },
    {
      "epoch": 18.816154463829477,
      "grad_norm": 4.0648274421691895,
      "learning_rate": 3.4319871280142104e-05,
      "loss": 0.6759,
      "step": 2062100
    },
    {
      "epoch": 18.817066939192642,
      "grad_norm": 4.36636209487915,
      "learning_rate": 3.4319110884006134e-05,
      "loss": 0.6836,
      "step": 2062200
    },
    {
      "epoch": 18.817979414555808,
      "grad_norm": 3.394953966140747,
      "learning_rate": 3.431835048787016e-05,
      "loss": 0.6472,
      "step": 2062300
    },
    {
      "epoch": 18.818891889918973,
      "grad_norm": 3.2914934158325195,
      "learning_rate": 3.4317590091734194e-05,
      "loss": 0.6564,
      "step": 2062400
    },
    {
      "epoch": 18.81980436528214,
      "grad_norm": 4.365295886993408,
      "learning_rate": 3.431682969559822e-05,
      "loss": 0.6594,
      "step": 2062500
    },
    {
      "epoch": 18.820716840645304,
      "grad_norm": 3.9636642932891846,
      "learning_rate": 3.431606929946225e-05,
      "loss": 0.6859,
      "step": 2062600
    },
    {
      "epoch": 18.82162931600847,
      "grad_norm": 4.450382232666016,
      "learning_rate": 3.431530890332628e-05,
      "loss": 0.6698,
      "step": 2062700
    },
    {
      "epoch": 18.822541791371634,
      "grad_norm": 4.966142177581787,
      "learning_rate": 3.431454850719031e-05,
      "loss": 0.6516,
      "step": 2062800
    },
    {
      "epoch": 18.8234542667348,
      "grad_norm": 4.283867359161377,
      "learning_rate": 3.431378811105434e-05,
      "loss": 0.6544,
      "step": 2062900
    },
    {
      "epoch": 18.824366742097965,
      "grad_norm": 3.748436212539673,
      "learning_rate": 3.431302771491837e-05,
      "loss": 0.6526,
      "step": 2063000
    },
    {
      "epoch": 18.82527921746113,
      "grad_norm": 3.198777914047241,
      "learning_rate": 3.431226731878239e-05,
      "loss": 0.6793,
      "step": 2063100
    },
    {
      "epoch": 18.826191692824295,
      "grad_norm": 3.4490644931793213,
      "learning_rate": 3.431150692264643e-05,
      "loss": 0.5993,
      "step": 2063200
    },
    {
      "epoch": 18.82710416818746,
      "grad_norm": 3.233403444290161,
      "learning_rate": 3.431074652651045e-05,
      "loss": 0.6704,
      "step": 2063300
    },
    {
      "epoch": 18.828016643550622,
      "grad_norm": 3.5243237018585205,
      "learning_rate": 3.430998613037448e-05,
      "loss": 0.6469,
      "step": 2063400
    },
    {
      "epoch": 18.828929118913788,
      "grad_norm": 3.489478826522827,
      "learning_rate": 3.430922573423851e-05,
      "loss": 0.6734,
      "step": 2063500
    },
    {
      "epoch": 18.829841594276953,
      "grad_norm": 4.283382892608643,
      "learning_rate": 3.430846533810254e-05,
      "loss": 0.712,
      "step": 2063600
    },
    {
      "epoch": 18.83075406964012,
      "grad_norm": 3.8796865940093994,
      "learning_rate": 3.4307704941966565e-05,
      "loss": 0.6725,
      "step": 2063700
    },
    {
      "epoch": 18.831666545003284,
      "grad_norm": 4.845863342285156,
      "learning_rate": 3.43069445458306e-05,
      "loss": 0.6595,
      "step": 2063800
    },
    {
      "epoch": 18.83257902036645,
      "grad_norm": 3.687802314758301,
      "learning_rate": 3.4306184149694625e-05,
      "loss": 0.6373,
      "step": 2063900
    },
    {
      "epoch": 18.833491495729614,
      "grad_norm": 4.025689125061035,
      "learning_rate": 3.4305423753558655e-05,
      "loss": 0.6537,
      "step": 2064000
    },
    {
      "epoch": 18.83440397109278,
      "grad_norm": 4.264560699462891,
      "learning_rate": 3.4304663357422685e-05,
      "loss": 0.619,
      "step": 2064100
    },
    {
      "epoch": 18.835316446455945,
      "grad_norm": 3.4099795818328857,
      "learning_rate": 3.4303902961286715e-05,
      "loss": 0.6522,
      "step": 2064200
    },
    {
      "epoch": 18.83622892181911,
      "grad_norm": 4.0373640060424805,
      "learning_rate": 3.4303142565150745e-05,
      "loss": 0.6899,
      "step": 2064300
    },
    {
      "epoch": 18.837141397182275,
      "grad_norm": 4.099975109100342,
      "learning_rate": 3.4302382169014775e-05,
      "loss": 0.69,
      "step": 2064400
    },
    {
      "epoch": 18.83805387254544,
      "grad_norm": 3.6096153259277344,
      "learning_rate": 3.43016217728788e-05,
      "loss": 0.6312,
      "step": 2064500
    },
    {
      "epoch": 18.838966347908606,
      "grad_norm": 3.9198086261749268,
      "learning_rate": 3.4300861376742835e-05,
      "loss": 0.6462,
      "step": 2064600
    },
    {
      "epoch": 18.83987882327177,
      "grad_norm": 4.131561756134033,
      "learning_rate": 3.430010098060686e-05,
      "loss": 0.6758,
      "step": 2064700
    },
    {
      "epoch": 18.840791298634937,
      "grad_norm": 3.3511812686920166,
      "learning_rate": 3.429934058447088e-05,
      "loss": 0.6708,
      "step": 2064800
    },
    {
      "epoch": 18.841703773998102,
      "grad_norm": 3.715358018875122,
      "learning_rate": 3.429858018833492e-05,
      "loss": 0.6486,
      "step": 2064900
    },
    {
      "epoch": 18.842616249361267,
      "grad_norm": 2.45269513130188,
      "learning_rate": 3.429781979219894e-05,
      "loss": 0.6603,
      "step": 2065000
    },
    {
      "epoch": 18.843528724724433,
      "grad_norm": 4.065365791320801,
      "learning_rate": 3.429705939606297e-05,
      "loss": 0.6553,
      "step": 2065100
    },
    {
      "epoch": 18.844441200087598,
      "grad_norm": 3.6831178665161133,
      "learning_rate": 3.4296298999927e-05,
      "loss": 0.6875,
      "step": 2065200
    },
    {
      "epoch": 18.845353675450763,
      "grad_norm": 2.168480157852173,
      "learning_rate": 3.429553860379103e-05,
      "loss": 0.6556,
      "step": 2065300
    },
    {
      "epoch": 18.84626615081393,
      "grad_norm": 3.457791566848755,
      "learning_rate": 3.429477820765506e-05,
      "loss": 0.6488,
      "step": 2065400
    },
    {
      "epoch": 18.847178626177094,
      "grad_norm": 4.435444355010986,
      "learning_rate": 3.429401781151909e-05,
      "loss": 0.6485,
      "step": 2065500
    },
    {
      "epoch": 18.84809110154026,
      "grad_norm": 3.7195510864257812,
      "learning_rate": 3.4293257415383116e-05,
      "loss": 0.6296,
      "step": 2065600
    },
    {
      "epoch": 18.849003576903424,
      "grad_norm": 3.7725353240966797,
      "learning_rate": 3.429249701924715e-05,
      "loss": 0.6497,
      "step": 2065700
    },
    {
      "epoch": 18.84991605226659,
      "grad_norm": 3.939833164215088,
      "learning_rate": 3.4291736623111176e-05,
      "loss": 0.6978,
      "step": 2065800
    },
    {
      "epoch": 18.850828527629755,
      "grad_norm": 2.255985975265503,
      "learning_rate": 3.4290976226975206e-05,
      "loss": 0.6522,
      "step": 2065900
    },
    {
      "epoch": 18.85174100299292,
      "grad_norm": 5.047295570373535,
      "learning_rate": 3.4290215830839236e-05,
      "loss": 0.6636,
      "step": 2066000
    },
    {
      "epoch": 18.852653478356086,
      "grad_norm": 3.857736349105835,
      "learning_rate": 3.4289455434703266e-05,
      "loss": 0.6728,
      "step": 2066100
    },
    {
      "epoch": 18.85356595371925,
      "grad_norm": 3.7025656700134277,
      "learning_rate": 3.428869503856729e-05,
      "loss": 0.6436,
      "step": 2066200
    },
    {
      "epoch": 18.854478429082416,
      "grad_norm": 3.940217971801758,
      "learning_rate": 3.4287934642431326e-05,
      "loss": 0.6461,
      "step": 2066300
    },
    {
      "epoch": 18.85539090444558,
      "grad_norm": 3.7112228870391846,
      "learning_rate": 3.428717424629535e-05,
      "loss": 0.6517,
      "step": 2066400
    },
    {
      "epoch": 18.856303379808747,
      "grad_norm": 3.9939353466033936,
      "learning_rate": 3.428641385015938e-05,
      "loss": 0.6964,
      "step": 2066500
    },
    {
      "epoch": 18.857215855171912,
      "grad_norm": 3.8161871433258057,
      "learning_rate": 3.428565345402341e-05,
      "loss": 0.6618,
      "step": 2066600
    },
    {
      "epoch": 18.858128330535074,
      "grad_norm": 3.394709587097168,
      "learning_rate": 3.428489305788744e-05,
      "loss": 0.6469,
      "step": 2066700
    },
    {
      "epoch": 18.85904080589824,
      "grad_norm": 3.551581621170044,
      "learning_rate": 3.428413266175147e-05,
      "loss": 0.6832,
      "step": 2066800
    },
    {
      "epoch": 18.859953281261404,
      "grad_norm": 3.2745144367218018,
      "learning_rate": 3.42833722656155e-05,
      "loss": 0.644,
      "step": 2066900
    },
    {
      "epoch": 18.86086575662457,
      "grad_norm": 4.195124626159668,
      "learning_rate": 3.428261186947952e-05,
      "loss": 0.6638,
      "step": 2067000
    },
    {
      "epoch": 18.861778231987735,
      "grad_norm": 4.084928035736084,
      "learning_rate": 3.428185147334356e-05,
      "loss": 0.6654,
      "step": 2067100
    },
    {
      "epoch": 18.8626907073509,
      "grad_norm": 4.058353424072266,
      "learning_rate": 3.428109107720758e-05,
      "loss": 0.6349,
      "step": 2067200
    },
    {
      "epoch": 18.863603182714066,
      "grad_norm": 3.510653257369995,
      "learning_rate": 3.428033068107161e-05,
      "loss": 0.6532,
      "step": 2067300
    },
    {
      "epoch": 18.86451565807723,
      "grad_norm": 3.1722261905670166,
      "learning_rate": 3.427957028493564e-05,
      "loss": 0.6372,
      "step": 2067400
    },
    {
      "epoch": 18.865428133440396,
      "grad_norm": 3.247955083847046,
      "learning_rate": 3.4278809888799673e-05,
      "loss": 0.621,
      "step": 2067500
    },
    {
      "epoch": 18.86634060880356,
      "grad_norm": 4.762260913848877,
      "learning_rate": 3.42780494926637e-05,
      "loss": 0.6699,
      "step": 2067600
    },
    {
      "epoch": 18.867253084166727,
      "grad_norm": 4.650816440582275,
      "learning_rate": 3.427728909652773e-05,
      "loss": 0.7029,
      "step": 2067700
    },
    {
      "epoch": 18.868165559529892,
      "grad_norm": 4.274199962615967,
      "learning_rate": 3.427652870039176e-05,
      "loss": 0.6788,
      "step": 2067800
    },
    {
      "epoch": 18.869078034893057,
      "grad_norm": 4.779955863952637,
      "learning_rate": 3.427576830425579e-05,
      "loss": 0.6499,
      "step": 2067900
    },
    {
      "epoch": 18.869990510256223,
      "grad_norm": 4.01841402053833,
      "learning_rate": 3.427500790811982e-05,
      "loss": 0.6898,
      "step": 2068000
    },
    {
      "epoch": 18.870902985619388,
      "grad_norm": 4.137167453765869,
      "learning_rate": 3.427424751198384e-05,
      "loss": 0.654,
      "step": 2068100
    },
    {
      "epoch": 18.871815460982553,
      "grad_norm": 3.4072985649108887,
      "learning_rate": 3.427348711584788e-05,
      "loss": 0.6747,
      "step": 2068200
    },
    {
      "epoch": 18.87272793634572,
      "grad_norm": 3.9985854625701904,
      "learning_rate": 3.42727267197119e-05,
      "loss": 0.6753,
      "step": 2068300
    },
    {
      "epoch": 18.873640411708884,
      "grad_norm": 3.8460285663604736,
      "learning_rate": 3.427196632357593e-05,
      "loss": 0.6792,
      "step": 2068400
    },
    {
      "epoch": 18.87455288707205,
      "grad_norm": 4.29920768737793,
      "learning_rate": 3.427120592743996e-05,
      "loss": 0.6649,
      "step": 2068500
    },
    {
      "epoch": 18.875465362435214,
      "grad_norm": 4.244381904602051,
      "learning_rate": 3.427044553130399e-05,
      "loss": 0.6598,
      "step": 2068600
    },
    {
      "epoch": 18.87637783779838,
      "grad_norm": 3.873440980911255,
      "learning_rate": 3.4269685135168014e-05,
      "loss": 0.6819,
      "step": 2068700
    },
    {
      "epoch": 18.877290313161545,
      "grad_norm": 4.038865566253662,
      "learning_rate": 3.426892473903205e-05,
      "loss": 0.6419,
      "step": 2068800
    },
    {
      "epoch": 18.87820278852471,
      "grad_norm": 4.35584020614624,
      "learning_rate": 3.4268164342896074e-05,
      "loss": 0.6487,
      "step": 2068900
    },
    {
      "epoch": 18.879115263887876,
      "grad_norm": 3.829603433609009,
      "learning_rate": 3.4267403946760104e-05,
      "loss": 0.6213,
      "step": 2069000
    },
    {
      "epoch": 18.88002773925104,
      "grad_norm": 4.008986949920654,
      "learning_rate": 3.4266643550624134e-05,
      "loss": 0.6863,
      "step": 2069100
    },
    {
      "epoch": 18.880940214614206,
      "grad_norm": 3.816093921661377,
      "learning_rate": 3.4265883154488164e-05,
      "loss": 0.6424,
      "step": 2069200
    },
    {
      "epoch": 18.88185268997737,
      "grad_norm": 5.0749831199646,
      "learning_rate": 3.4265122758352194e-05,
      "loss": 0.6839,
      "step": 2069300
    },
    {
      "epoch": 18.882765165340537,
      "grad_norm": 3.778272867202759,
      "learning_rate": 3.4264362362216224e-05,
      "loss": 0.673,
      "step": 2069400
    },
    {
      "epoch": 18.883677640703702,
      "grad_norm": 4.9727630615234375,
      "learning_rate": 3.426360196608025e-05,
      "loss": 0.6526,
      "step": 2069500
    },
    {
      "epoch": 18.884590116066867,
      "grad_norm": 4.450722694396973,
      "learning_rate": 3.4262841569944285e-05,
      "loss": 0.6721,
      "step": 2069600
    },
    {
      "epoch": 18.885502591430033,
      "grad_norm": 3.9140472412109375,
      "learning_rate": 3.426208117380831e-05,
      "loss": 0.6374,
      "step": 2069700
    },
    {
      "epoch": 18.886415066793198,
      "grad_norm": 4.35570764541626,
      "learning_rate": 3.426132077767234e-05,
      "loss": 0.6355,
      "step": 2069800
    },
    {
      "epoch": 18.887327542156363,
      "grad_norm": 5.161202430725098,
      "learning_rate": 3.426056038153637e-05,
      "loss": 0.6446,
      "step": 2069900
    },
    {
      "epoch": 18.88824001751953,
      "grad_norm": 2.8842809200286865,
      "learning_rate": 3.42597999854004e-05,
      "loss": 0.6618,
      "step": 2070000
    },
    {
      "epoch": 18.889152492882694,
      "grad_norm": 4.476230621337891,
      "learning_rate": 3.425903958926442e-05,
      "loss": 0.6774,
      "step": 2070100
    },
    {
      "epoch": 18.890064968245856,
      "grad_norm": 4.478488445281982,
      "learning_rate": 3.425827919312846e-05,
      "loss": 0.6916,
      "step": 2070200
    },
    {
      "epoch": 18.89097744360902,
      "grad_norm": 3.015395164489746,
      "learning_rate": 3.425751879699248e-05,
      "loss": 0.6698,
      "step": 2070300
    },
    {
      "epoch": 18.891889918972186,
      "grad_norm": 3.854271411895752,
      "learning_rate": 3.425675840085651e-05,
      "loss": 0.6654,
      "step": 2070400
    },
    {
      "epoch": 18.89280239433535,
      "grad_norm": 4.361175060272217,
      "learning_rate": 3.425599800472054e-05,
      "loss": 0.6313,
      "step": 2070500
    },
    {
      "epoch": 18.893714869698517,
      "grad_norm": 4.606753826141357,
      "learning_rate": 3.4255237608584565e-05,
      "loss": 0.6554,
      "step": 2070600
    },
    {
      "epoch": 18.894627345061682,
      "grad_norm": 4.088351726531982,
      "learning_rate": 3.42544772124486e-05,
      "loss": 0.6675,
      "step": 2070700
    },
    {
      "epoch": 18.895539820424847,
      "grad_norm": 3.9901282787323,
      "learning_rate": 3.4253716816312625e-05,
      "loss": 0.6501,
      "step": 2070800
    },
    {
      "epoch": 18.896452295788013,
      "grad_norm": 4.075610637664795,
      "learning_rate": 3.4252956420176655e-05,
      "loss": 0.6831,
      "step": 2070900
    },
    {
      "epoch": 18.897364771151178,
      "grad_norm": 3.7776691913604736,
      "learning_rate": 3.4252196024040685e-05,
      "loss": 0.6529,
      "step": 2071000
    },
    {
      "epoch": 18.898277246514343,
      "grad_norm": 4.286424160003662,
      "learning_rate": 3.4251435627904715e-05,
      "loss": 0.6471,
      "step": 2071100
    },
    {
      "epoch": 18.89918972187751,
      "grad_norm": 4.726110935211182,
      "learning_rate": 3.425067523176874e-05,
      "loss": 0.673,
      "step": 2071200
    },
    {
      "epoch": 18.900102197240674,
      "grad_norm": 3.4756054878234863,
      "learning_rate": 3.4249914835632775e-05,
      "loss": 0.6536,
      "step": 2071300
    },
    {
      "epoch": 18.90101467260384,
      "grad_norm": 4.178794860839844,
      "learning_rate": 3.42491544394968e-05,
      "loss": 0.6509,
      "step": 2071400
    },
    {
      "epoch": 18.901927147967005,
      "grad_norm": 3.5376439094543457,
      "learning_rate": 3.424839404336083e-05,
      "loss": 0.6658,
      "step": 2071500
    },
    {
      "epoch": 18.90283962333017,
      "grad_norm": 4.304437160491943,
      "learning_rate": 3.424763364722486e-05,
      "loss": 0.6358,
      "step": 2071600
    },
    {
      "epoch": 18.903752098693335,
      "grad_norm": 3.808138608932495,
      "learning_rate": 3.424687325108889e-05,
      "loss": 0.6274,
      "step": 2071700
    },
    {
      "epoch": 18.9046645740565,
      "grad_norm": 2.5006465911865234,
      "learning_rate": 3.424611285495292e-05,
      "loss": 0.6484,
      "step": 2071800
    },
    {
      "epoch": 18.905577049419666,
      "grad_norm": 3.7574915885925293,
      "learning_rate": 3.424535245881695e-05,
      "loss": 0.6759,
      "step": 2071900
    },
    {
      "epoch": 18.90648952478283,
      "grad_norm": 3.0242135524749756,
      "learning_rate": 3.424459206268097e-05,
      "loss": 0.6484,
      "step": 2072000
    },
    {
      "epoch": 18.907402000145996,
      "grad_norm": 4.226471900939941,
      "learning_rate": 3.424383166654501e-05,
      "loss": 0.659,
      "step": 2072100
    },
    {
      "epoch": 18.90831447550916,
      "grad_norm": 4.144688606262207,
      "learning_rate": 3.424307127040903e-05,
      "loss": 0.6431,
      "step": 2072200
    },
    {
      "epoch": 18.909226950872327,
      "grad_norm": 4.26050329208374,
      "learning_rate": 3.424231087427306e-05,
      "loss": 0.6344,
      "step": 2072300
    },
    {
      "epoch": 18.910139426235492,
      "grad_norm": 2.722511053085327,
      "learning_rate": 3.424155047813709e-05,
      "loss": 0.6515,
      "step": 2072400
    },
    {
      "epoch": 18.911051901598658,
      "grad_norm": 3.701249361038208,
      "learning_rate": 3.424079008200112e-05,
      "loss": 0.702,
      "step": 2072500
    },
    {
      "epoch": 18.911964376961823,
      "grad_norm": 4.817305088043213,
      "learning_rate": 3.4240029685865146e-05,
      "loss": 0.6539,
      "step": 2072600
    },
    {
      "epoch": 18.912876852324988,
      "grad_norm": 4.036506652832031,
      "learning_rate": 3.423926928972918e-05,
      "loss": 0.6815,
      "step": 2072700
    },
    {
      "epoch": 18.913789327688153,
      "grad_norm": 3.2033448219299316,
      "learning_rate": 3.4238508893593206e-05,
      "loss": 0.6854,
      "step": 2072800
    },
    {
      "epoch": 18.91470180305132,
      "grad_norm": 3.17915678024292,
      "learning_rate": 3.4237748497457236e-05,
      "loss": 0.6842,
      "step": 2072900
    },
    {
      "epoch": 18.915614278414484,
      "grad_norm": 4.402348041534424,
      "learning_rate": 3.4236988101321266e-05,
      "loss": 0.6844,
      "step": 2073000
    },
    {
      "epoch": 18.91652675377765,
      "grad_norm": 3.4131245613098145,
      "learning_rate": 3.4236227705185296e-05,
      "loss": 0.677,
      "step": 2073100
    },
    {
      "epoch": 18.917439229140815,
      "grad_norm": 3.288940906524658,
      "learning_rate": 3.4235467309049326e-05,
      "loss": 0.6722,
      "step": 2073200
    },
    {
      "epoch": 18.91835170450398,
      "grad_norm": 3.754150152206421,
      "learning_rate": 3.4234706912913356e-05,
      "loss": 0.6444,
      "step": 2073300
    },
    {
      "epoch": 18.919264179867145,
      "grad_norm": 3.0955004692077637,
      "learning_rate": 3.423394651677738e-05,
      "loss": 0.679,
      "step": 2073400
    },
    {
      "epoch": 18.920176655230307,
      "grad_norm": 3.2500576972961426,
      "learning_rate": 3.423318612064141e-05,
      "loss": 0.6754,
      "step": 2073500
    },
    {
      "epoch": 18.921089130593472,
      "grad_norm": 4.260923385620117,
      "learning_rate": 3.423242572450544e-05,
      "loss": 0.6706,
      "step": 2073600
    },
    {
      "epoch": 18.922001605956638,
      "grad_norm": 4.684877395629883,
      "learning_rate": 3.423166532836947e-05,
      "loss": 0.6708,
      "step": 2073700
    },
    {
      "epoch": 18.922914081319803,
      "grad_norm": 4.855493545532227,
      "learning_rate": 3.42309049322335e-05,
      "loss": 0.6409,
      "step": 2073800
    },
    {
      "epoch": 18.923826556682968,
      "grad_norm": 3.394108772277832,
      "learning_rate": 3.423014453609752e-05,
      "loss": 0.6821,
      "step": 2073900
    },
    {
      "epoch": 18.924739032046134,
      "grad_norm": 3.456002950668335,
      "learning_rate": 3.422938413996156e-05,
      "loss": 0.6221,
      "step": 2074000
    },
    {
      "epoch": 18.9256515074093,
      "grad_norm": 3.6454710960388184,
      "learning_rate": 3.422862374382558e-05,
      "loss": 0.6363,
      "step": 2074100
    },
    {
      "epoch": 18.926563982772464,
      "grad_norm": 2.4642930030822754,
      "learning_rate": 3.4227863347689613e-05,
      "loss": 0.6558,
      "step": 2074200
    },
    {
      "epoch": 18.92747645813563,
      "grad_norm": 3.2647101879119873,
      "learning_rate": 3.4227102951553643e-05,
      "loss": 0.6336,
      "step": 2074300
    },
    {
      "epoch": 18.928388933498795,
      "grad_norm": 3.9499008655548096,
      "learning_rate": 3.4226342555417674e-05,
      "loss": 0.677,
      "step": 2074400
    },
    {
      "epoch": 18.92930140886196,
      "grad_norm": 3.7705912590026855,
      "learning_rate": 3.42255821592817e-05,
      "loss": 0.6711,
      "step": 2074500
    },
    {
      "epoch": 18.930213884225125,
      "grad_norm": 4.6275153160095215,
      "learning_rate": 3.4224821763145734e-05,
      "loss": 0.6518,
      "step": 2074600
    },
    {
      "epoch": 18.93112635958829,
      "grad_norm": 3.3773996829986572,
      "learning_rate": 3.422406136700976e-05,
      "loss": 0.6402,
      "step": 2074700
    },
    {
      "epoch": 18.932038834951456,
      "grad_norm": 4.109600067138672,
      "learning_rate": 3.422330097087379e-05,
      "loss": 0.6655,
      "step": 2074800
    },
    {
      "epoch": 18.93295131031462,
      "grad_norm": 3.4818544387817383,
      "learning_rate": 3.422254057473782e-05,
      "loss": 0.6474,
      "step": 2074900
    },
    {
      "epoch": 18.933863785677786,
      "grad_norm": 3.4218556880950928,
      "learning_rate": 3.422178017860185e-05,
      "loss": 0.6825,
      "step": 2075000
    },
    {
      "epoch": 18.934776261040952,
      "grad_norm": 4.130296230316162,
      "learning_rate": 3.422101978246588e-05,
      "loss": 0.6889,
      "step": 2075100
    },
    {
      "epoch": 18.935688736404117,
      "grad_norm": 5.762302875518799,
      "learning_rate": 3.422025938632991e-05,
      "loss": 0.6659,
      "step": 2075200
    },
    {
      "epoch": 18.936601211767282,
      "grad_norm": 4.524877071380615,
      "learning_rate": 3.421949899019393e-05,
      "loss": 0.6859,
      "step": 2075300
    },
    {
      "epoch": 18.937513687130448,
      "grad_norm": 3.757159948348999,
      "learning_rate": 3.421873859405797e-05,
      "loss": 0.639,
      "step": 2075400
    },
    {
      "epoch": 18.938426162493613,
      "grad_norm": 3.2984070777893066,
      "learning_rate": 3.421797819792199e-05,
      "loss": 0.6383,
      "step": 2075500
    },
    {
      "epoch": 18.93933863785678,
      "grad_norm": 3.7926244735717773,
      "learning_rate": 3.421721780178602e-05,
      "loss": 0.6231,
      "step": 2075600
    },
    {
      "epoch": 18.940251113219944,
      "grad_norm": 3.660127639770508,
      "learning_rate": 3.421645740565005e-05,
      "loss": 0.6537,
      "step": 2075700
    },
    {
      "epoch": 18.94116358858311,
      "grad_norm": 4.690124988555908,
      "learning_rate": 3.421569700951408e-05,
      "loss": 0.6636,
      "step": 2075800
    },
    {
      "epoch": 18.942076063946274,
      "grad_norm": 3.325028657913208,
      "learning_rate": 3.4214936613378104e-05,
      "loss": 0.6906,
      "step": 2075900
    },
    {
      "epoch": 18.94298853930944,
      "grad_norm": 4.207849025726318,
      "learning_rate": 3.421417621724214e-05,
      "loss": 0.6485,
      "step": 2076000
    },
    {
      "epoch": 18.943901014672605,
      "grad_norm": 4.10222864151001,
      "learning_rate": 3.4213415821106164e-05,
      "loss": 0.6397,
      "step": 2076100
    },
    {
      "epoch": 18.94481349003577,
      "grad_norm": 3.7646026611328125,
      "learning_rate": 3.4212655424970194e-05,
      "loss": 0.6649,
      "step": 2076200
    },
    {
      "epoch": 18.945725965398935,
      "grad_norm": 4.245425701141357,
      "learning_rate": 3.4211895028834224e-05,
      "loss": 0.6796,
      "step": 2076300
    },
    {
      "epoch": 18.9466384407621,
      "grad_norm": 4.635021209716797,
      "learning_rate": 3.421113463269825e-05,
      "loss": 0.6,
      "step": 2076400
    },
    {
      "epoch": 18.947550916125266,
      "grad_norm": 3.5853376388549805,
      "learning_rate": 3.4210374236562285e-05,
      "loss": 0.6749,
      "step": 2076500
    },
    {
      "epoch": 18.94846339148843,
      "grad_norm": 3.765963554382324,
      "learning_rate": 3.420961384042631e-05,
      "loss": 0.6834,
      "step": 2076600
    },
    {
      "epoch": 18.949375866851597,
      "grad_norm": 2.7450311183929443,
      "learning_rate": 3.420885344429034e-05,
      "loss": 0.6414,
      "step": 2076700
    },
    {
      "epoch": 18.950288342214762,
      "grad_norm": 4.909929275512695,
      "learning_rate": 3.420809304815437e-05,
      "loss": 0.6795,
      "step": 2076800
    },
    {
      "epoch": 18.951200817577927,
      "grad_norm": 4.623426914215088,
      "learning_rate": 3.42073326520184e-05,
      "loss": 0.6713,
      "step": 2076900
    },
    {
      "epoch": 18.95211329294109,
      "grad_norm": 4.476795196533203,
      "learning_rate": 3.420657225588242e-05,
      "loss": 0.6271,
      "step": 2077000
    },
    {
      "epoch": 18.953025768304254,
      "grad_norm": 4.275554180145264,
      "learning_rate": 3.420581185974646e-05,
      "loss": 0.6579,
      "step": 2077100
    },
    {
      "epoch": 18.95393824366742,
      "grad_norm": 2.7715647220611572,
      "learning_rate": 3.420505146361048e-05,
      "loss": 0.6645,
      "step": 2077200
    },
    {
      "epoch": 18.954850719030585,
      "grad_norm": 4.143239498138428,
      "learning_rate": 3.420429106747451e-05,
      "loss": 0.6588,
      "step": 2077300
    },
    {
      "epoch": 18.95576319439375,
      "grad_norm": 3.0858469009399414,
      "learning_rate": 3.420353067133854e-05,
      "loss": 0.6502,
      "step": 2077400
    },
    {
      "epoch": 18.956675669756915,
      "grad_norm": 4.446538925170898,
      "learning_rate": 3.420277027520257e-05,
      "loss": 0.705,
      "step": 2077500
    },
    {
      "epoch": 18.95758814512008,
      "grad_norm": 3.837149143218994,
      "learning_rate": 3.42020098790666e-05,
      "loss": 0.6686,
      "step": 2077600
    },
    {
      "epoch": 18.958500620483246,
      "grad_norm": 2.9551682472229004,
      "learning_rate": 3.420124948293063e-05,
      "loss": 0.6589,
      "step": 2077700
    },
    {
      "epoch": 18.95941309584641,
      "grad_norm": 3.4861669540405273,
      "learning_rate": 3.4200489086794655e-05,
      "loss": 0.663,
      "step": 2077800
    },
    {
      "epoch": 18.960325571209577,
      "grad_norm": 3.875019073486328,
      "learning_rate": 3.419972869065869e-05,
      "loss": 0.6638,
      "step": 2077900
    },
    {
      "epoch": 18.961238046572742,
      "grad_norm": 4.165215492248535,
      "learning_rate": 3.4198968294522715e-05,
      "loss": 0.6734,
      "step": 2078000
    },
    {
      "epoch": 18.962150521935907,
      "grad_norm": 3.980074167251587,
      "learning_rate": 3.4198207898386745e-05,
      "loss": 0.5989,
      "step": 2078100
    },
    {
      "epoch": 18.963062997299073,
      "grad_norm": 3.8187499046325684,
      "learning_rate": 3.4197447502250775e-05,
      "loss": 0.6633,
      "step": 2078200
    },
    {
      "epoch": 18.963975472662238,
      "grad_norm": 3.441415548324585,
      "learning_rate": 3.4196687106114805e-05,
      "loss": 0.6486,
      "step": 2078300
    },
    {
      "epoch": 18.964887948025403,
      "grad_norm": 3.618936777114868,
      "learning_rate": 3.419592670997883e-05,
      "loss": 0.6339,
      "step": 2078400
    },
    {
      "epoch": 18.96580042338857,
      "grad_norm": 4.280527114868164,
      "learning_rate": 3.4195166313842866e-05,
      "loss": 0.6566,
      "step": 2078500
    },
    {
      "epoch": 18.966712898751734,
      "grad_norm": 3.770862579345703,
      "learning_rate": 3.419440591770689e-05,
      "loss": 0.6801,
      "step": 2078600
    },
    {
      "epoch": 18.9676253741149,
      "grad_norm": 3.156001091003418,
      "learning_rate": 3.419364552157092e-05,
      "loss": 0.6468,
      "step": 2078700
    },
    {
      "epoch": 18.968537849478064,
      "grad_norm": 3.9315550327301025,
      "learning_rate": 3.419288512543495e-05,
      "loss": 0.6436,
      "step": 2078800
    },
    {
      "epoch": 18.96945032484123,
      "grad_norm": 4.301120281219482,
      "learning_rate": 3.419212472929898e-05,
      "loss": 0.6523,
      "step": 2078900
    },
    {
      "epoch": 18.970362800204395,
      "grad_norm": 3.9669322967529297,
      "learning_rate": 3.419136433316301e-05,
      "loss": 0.6537,
      "step": 2079000
    },
    {
      "epoch": 18.97127527556756,
      "grad_norm": 3.524634838104248,
      "learning_rate": 3.419060393702703e-05,
      "loss": 0.645,
      "step": 2079100
    },
    {
      "epoch": 18.972187750930726,
      "grad_norm": 2.9159038066864014,
      "learning_rate": 3.418984354089106e-05,
      "loss": 0.646,
      "step": 2079200
    },
    {
      "epoch": 18.97310022629389,
      "grad_norm": 3.0149004459381104,
      "learning_rate": 3.418908314475509e-05,
      "loss": 0.6279,
      "step": 2079300
    },
    {
      "epoch": 18.974012701657056,
      "grad_norm": 4.626775741577148,
      "learning_rate": 3.418832274861912e-05,
      "loss": 0.6419,
      "step": 2079400
    },
    {
      "epoch": 18.97492517702022,
      "grad_norm": 4.655593395233154,
      "learning_rate": 3.4187562352483146e-05,
      "loss": 0.6821,
      "step": 2079500
    },
    {
      "epoch": 18.975837652383387,
      "grad_norm": 4.210911273956299,
      "learning_rate": 3.418680195634718e-05,
      "loss": 0.6956,
      "step": 2079600
    },
    {
      "epoch": 18.976750127746552,
      "grad_norm": 2.8172550201416016,
      "learning_rate": 3.4186041560211206e-05,
      "loss": 0.6377,
      "step": 2079700
    },
    {
      "epoch": 18.977662603109717,
      "grad_norm": 4.196841716766357,
      "learning_rate": 3.4185281164075236e-05,
      "loss": 0.6531,
      "step": 2079800
    },
    {
      "epoch": 18.978575078472883,
      "grad_norm": 4.2663655281066895,
      "learning_rate": 3.4184520767939266e-05,
      "loss": 0.6441,
      "step": 2079900
    },
    {
      "epoch": 18.979487553836048,
      "grad_norm": 3.726947784423828,
      "learning_rate": 3.4183760371803296e-05,
      "loss": 0.633,
      "step": 2080000
    },
    {
      "epoch": 18.980400029199213,
      "grad_norm": 4.102299213409424,
      "learning_rate": 3.4182999975667326e-05,
      "loss": 0.6812,
      "step": 2080100
    },
    {
      "epoch": 18.98131250456238,
      "grad_norm": 4.323476314544678,
      "learning_rate": 3.4182239579531356e-05,
      "loss": 0.6887,
      "step": 2080200
    },
    {
      "epoch": 18.98222497992554,
      "grad_norm": 2.1691696643829346,
      "learning_rate": 3.418147918339538e-05,
      "loss": 0.6973,
      "step": 2080300
    },
    {
      "epoch": 18.983137455288706,
      "grad_norm": 3.897636890411377,
      "learning_rate": 3.4180718787259417e-05,
      "loss": 0.6521,
      "step": 2080400
    },
    {
      "epoch": 18.98404993065187,
      "grad_norm": 4.157162189483643,
      "learning_rate": 3.417995839112344e-05,
      "loss": 0.6992,
      "step": 2080500
    },
    {
      "epoch": 18.984962406015036,
      "grad_norm": 3.640805721282959,
      "learning_rate": 3.417919799498747e-05,
      "loss": 0.6725,
      "step": 2080600
    },
    {
      "epoch": 18.9858748813782,
      "grad_norm": 4.39886999130249,
      "learning_rate": 3.41784375988515e-05,
      "loss": 0.6535,
      "step": 2080700
    },
    {
      "epoch": 18.986787356741367,
      "grad_norm": 3.6554012298583984,
      "learning_rate": 3.417767720271553e-05,
      "loss": 0.6702,
      "step": 2080800
    },
    {
      "epoch": 18.987699832104532,
      "grad_norm": 4.1264424324035645,
      "learning_rate": 3.417691680657955e-05,
      "loss": 0.6605,
      "step": 2080900
    },
    {
      "epoch": 18.988612307467697,
      "grad_norm": 4.026177406311035,
      "learning_rate": 3.417615641044359e-05,
      "loss": 0.6724,
      "step": 2081000
    },
    {
      "epoch": 18.989524782830863,
      "grad_norm": 3.285374641418457,
      "learning_rate": 3.4175396014307614e-05,
      "loss": 0.635,
      "step": 2081100
    },
    {
      "epoch": 18.990437258194028,
      "grad_norm": 3.703045129776001,
      "learning_rate": 3.4174635618171644e-05,
      "loss": 0.6373,
      "step": 2081200
    },
    {
      "epoch": 18.991349733557193,
      "grad_norm": 3.9693710803985596,
      "learning_rate": 3.4173875222035674e-05,
      "loss": 0.6618,
      "step": 2081300
    },
    {
      "epoch": 18.99226220892036,
      "grad_norm": 3.4250659942626953,
      "learning_rate": 3.4173114825899704e-05,
      "loss": 0.6229,
      "step": 2081400
    },
    {
      "epoch": 18.993174684283524,
      "grad_norm": 3.668501853942871,
      "learning_rate": 3.4172354429763734e-05,
      "loss": 0.6816,
      "step": 2081500
    },
    {
      "epoch": 18.99408715964669,
      "grad_norm": 4.187991619110107,
      "learning_rate": 3.4171594033627764e-05,
      "loss": 0.6299,
      "step": 2081600
    },
    {
      "epoch": 18.994999635009854,
      "grad_norm": 3.9789106845855713,
      "learning_rate": 3.417083363749179e-05,
      "loss": 0.637,
      "step": 2081700
    },
    {
      "epoch": 18.99591211037302,
      "grad_norm": 4.647838592529297,
      "learning_rate": 3.4170073241355824e-05,
      "loss": 0.6724,
      "step": 2081800
    },
    {
      "epoch": 18.996824585736185,
      "grad_norm": 3.905294418334961,
      "learning_rate": 3.416931284521985e-05,
      "loss": 0.659,
      "step": 2081900
    },
    {
      "epoch": 18.99773706109935,
      "grad_norm": 3.8596558570861816,
      "learning_rate": 3.416855244908387e-05,
      "loss": 0.6807,
      "step": 2082000
    },
    {
      "epoch": 18.998649536462516,
      "grad_norm": 4.84862756729126,
      "learning_rate": 3.416779205294791e-05,
      "loss": 0.7308,
      "step": 2082100
    },
    {
      "epoch": 18.99956201182568,
      "grad_norm": 2.9884984493255615,
      "learning_rate": 3.416703165681193e-05,
      "loss": 0.6499,
      "step": 2082200
    },
    {
      "epoch": 19.0,
      "eval_loss": 0.5399547815322876,
      "eval_runtime": 25.6998,
      "eval_samples_per_second": 224.477,
      "eval_steps_per_second": 224.477,
      "step": 2082248
    },
    {
      "epoch": 19.0,
      "eval_loss": 0.5156940221786499,
      "eval_runtime": 485.9245,
      "eval_samples_per_second": 225.533,
      "eval_steps_per_second": 225.533,
      "step": 2082248
    },
    {
      "epoch": 19.000474487188846,
      "grad_norm": 5.332509994506836,
      "learning_rate": 3.416627126067596e-05,
      "loss": 0.6528,
      "step": 2082300
    },
    {
      "epoch": 19.00138696255201,
      "grad_norm": 3.3486058712005615,
      "learning_rate": 3.416551086453999e-05,
      "loss": 0.6262,
      "step": 2082400
    },
    {
      "epoch": 19.002299437915177,
      "grad_norm": 3.454909563064575,
      "learning_rate": 3.416475046840402e-05,
      "loss": 0.6819,
      "step": 2082500
    },
    {
      "epoch": 19.003211913278342,
      "grad_norm": 4.489269256591797,
      "learning_rate": 3.416399007226805e-05,
      "loss": 0.653,
      "step": 2082600
    },
    {
      "epoch": 19.004124388641507,
      "grad_norm": 3.310392379760742,
      "learning_rate": 3.416322967613208e-05,
      "loss": 0.699,
      "step": 2082700
    },
    {
      "epoch": 19.005036864004673,
      "grad_norm": 3.350688934326172,
      "learning_rate": 3.4162469279996104e-05,
      "loss": 0.6758,
      "step": 2082800
    },
    {
      "epoch": 19.005949339367838,
      "grad_norm": 3.7499537467956543,
      "learning_rate": 3.416170888386014e-05,
      "loss": 0.6599,
      "step": 2082900
    },
    {
      "epoch": 19.006861814731003,
      "grad_norm": 3.3860554695129395,
      "learning_rate": 3.4160948487724164e-05,
      "loss": 0.6596,
      "step": 2083000
    },
    {
      "epoch": 19.00777429009417,
      "grad_norm": 4.423153400421143,
      "learning_rate": 3.4160188091588195e-05,
      "loss": 0.6201,
      "step": 2083100
    },
    {
      "epoch": 19.008686765457334,
      "grad_norm": 3.03460955619812,
      "learning_rate": 3.4159427695452225e-05,
      "loss": 0.6403,
      "step": 2083200
    },
    {
      "epoch": 19.0095992408205,
      "grad_norm": 3.9764771461486816,
      "learning_rate": 3.4158667299316255e-05,
      "loss": 0.6323,
      "step": 2083300
    },
    {
      "epoch": 19.010511716183665,
      "grad_norm": 3.987558364868164,
      "learning_rate": 3.415790690318028e-05,
      "loss": 0.6448,
      "step": 2083400
    },
    {
      "epoch": 19.01142419154683,
      "grad_norm": 4.327525615692139,
      "learning_rate": 3.4157146507044315e-05,
      "loss": 0.6729,
      "step": 2083500
    },
    {
      "epoch": 19.012336666909995,
      "grad_norm": 3.5442593097686768,
      "learning_rate": 3.415638611090834e-05,
      "loss": 0.6933,
      "step": 2083600
    },
    {
      "epoch": 19.013249142273157,
      "grad_norm": 3.420276403427124,
      "learning_rate": 3.415562571477237e-05,
      "loss": 0.6582,
      "step": 2083700
    },
    {
      "epoch": 19.014161617636322,
      "grad_norm": 4.253035545349121,
      "learning_rate": 3.41548653186364e-05,
      "loss": 0.6368,
      "step": 2083800
    },
    {
      "epoch": 19.015074092999487,
      "grad_norm": 4.00848388671875,
      "learning_rate": 3.415410492250043e-05,
      "loss": 0.6367,
      "step": 2083900
    },
    {
      "epoch": 19.015986568362653,
      "grad_norm": 4.316895961761475,
      "learning_rate": 3.415334452636446e-05,
      "loss": 0.6465,
      "step": 2084000
    },
    {
      "epoch": 19.016899043725818,
      "grad_norm": 4.592290878295898,
      "learning_rate": 3.415258413022849e-05,
      "loss": 0.6641,
      "step": 2084100
    },
    {
      "epoch": 19.017811519088983,
      "grad_norm": 3.4898808002471924,
      "learning_rate": 3.415182373409251e-05,
      "loss": 0.653,
      "step": 2084200
    },
    {
      "epoch": 19.01872399445215,
      "grad_norm": 3.7338833808898926,
      "learning_rate": 3.415106333795655e-05,
      "loss": 0.6791,
      "step": 2084300
    },
    {
      "epoch": 19.019636469815314,
      "grad_norm": 4.774738311767578,
      "learning_rate": 3.415030294182057e-05,
      "loss": 0.6764,
      "step": 2084400
    },
    {
      "epoch": 19.02054894517848,
      "grad_norm": 3.2645485401153564,
      "learning_rate": 3.41495425456846e-05,
      "loss": 0.7002,
      "step": 2084500
    },
    {
      "epoch": 19.021461420541645,
      "grad_norm": 3.3786909580230713,
      "learning_rate": 3.414878214954863e-05,
      "loss": 0.5955,
      "step": 2084600
    },
    {
      "epoch": 19.02237389590481,
      "grad_norm": 3.116443395614624,
      "learning_rate": 3.4148021753412655e-05,
      "loss": 0.6521,
      "step": 2084700
    },
    {
      "epoch": 19.023286371267975,
      "grad_norm": 4.063996315002441,
      "learning_rate": 3.4147261357276685e-05,
      "loss": 0.6646,
      "step": 2084800
    },
    {
      "epoch": 19.02419884663114,
      "grad_norm": 4.824901103973389,
      "learning_rate": 3.4146500961140715e-05,
      "loss": 0.6343,
      "step": 2084900
    },
    {
      "epoch": 19.025111321994306,
      "grad_norm": 2.760667562484741,
      "learning_rate": 3.4145740565004745e-05,
      "loss": 0.6483,
      "step": 2085000
    },
    {
      "epoch": 19.02602379735747,
      "grad_norm": 3.5679163932800293,
      "learning_rate": 3.4144980168868776e-05,
      "loss": 0.6217,
      "step": 2085100
    },
    {
      "epoch": 19.026936272720636,
      "grad_norm": 4.719923496246338,
      "learning_rate": 3.4144219772732806e-05,
      "loss": 0.6949,
      "step": 2085200
    },
    {
      "epoch": 19.0278487480838,
      "grad_norm": 3.1105411052703857,
      "learning_rate": 3.414345937659683e-05,
      "loss": 0.6728,
      "step": 2085300
    },
    {
      "epoch": 19.028761223446967,
      "grad_norm": 4.321376323699951,
      "learning_rate": 3.4142698980460866e-05,
      "loss": 0.7052,
      "step": 2085400
    },
    {
      "epoch": 19.029673698810132,
      "grad_norm": 3.6304171085357666,
      "learning_rate": 3.414193858432489e-05,
      "loss": 0.674,
      "step": 2085500
    },
    {
      "epoch": 19.030586174173298,
      "grad_norm": 4.687772750854492,
      "learning_rate": 3.414117818818892e-05,
      "loss": 0.6396,
      "step": 2085600
    },
    {
      "epoch": 19.031498649536463,
      "grad_norm": 3.333115339279175,
      "learning_rate": 3.414041779205295e-05,
      "loss": 0.6563,
      "step": 2085700
    },
    {
      "epoch": 19.032411124899628,
      "grad_norm": 3.753236770629883,
      "learning_rate": 3.413965739591698e-05,
      "loss": 0.624,
      "step": 2085800
    },
    {
      "epoch": 19.033323600262793,
      "grad_norm": 3.1854422092437744,
      "learning_rate": 3.413889699978101e-05,
      "loss": 0.6704,
      "step": 2085900
    },
    {
      "epoch": 19.03423607562596,
      "grad_norm": 3.685445785522461,
      "learning_rate": 3.413813660364504e-05,
      "loss": 0.6402,
      "step": 2086000
    },
    {
      "epoch": 19.035148550989124,
      "grad_norm": 3.9296350479125977,
      "learning_rate": 3.413737620750906e-05,
      "loss": 0.6343,
      "step": 2086100
    },
    {
      "epoch": 19.03606102635229,
      "grad_norm": 4.598852157592773,
      "learning_rate": 3.413661581137309e-05,
      "loss": 0.6835,
      "step": 2086200
    },
    {
      "epoch": 19.036973501715455,
      "grad_norm": 4.297573089599609,
      "learning_rate": 3.413585541523712e-05,
      "loss": 0.6689,
      "step": 2086300
    },
    {
      "epoch": 19.03788597707862,
      "grad_norm": 3.246039390563965,
      "learning_rate": 3.413509501910115e-05,
      "loss": 0.6458,
      "step": 2086400
    },
    {
      "epoch": 19.038798452441785,
      "grad_norm": 4.533896446228027,
      "learning_rate": 3.413433462296518e-05,
      "loss": 0.6616,
      "step": 2086500
    },
    {
      "epoch": 19.03971092780495,
      "grad_norm": 3.3672564029693604,
      "learning_rate": 3.413357422682921e-05,
      "loss": 0.6729,
      "step": 2086600
    },
    {
      "epoch": 19.040623403168116,
      "grad_norm": 3.9317221641540527,
      "learning_rate": 3.4132813830693236e-05,
      "loss": 0.6796,
      "step": 2086700
    },
    {
      "epoch": 19.04153587853128,
      "grad_norm": 4.147039890289307,
      "learning_rate": 3.413205343455727e-05,
      "loss": 0.6682,
      "step": 2086800
    },
    {
      "epoch": 19.042448353894446,
      "grad_norm": 4.75547456741333,
      "learning_rate": 3.4131293038421296e-05,
      "loss": 0.6993,
      "step": 2086900
    },
    {
      "epoch": 19.04336082925761,
      "grad_norm": 4.119920253753662,
      "learning_rate": 3.4130532642285326e-05,
      "loss": 0.6671,
      "step": 2087000
    },
    {
      "epoch": 19.044273304620774,
      "grad_norm": 4.004727840423584,
      "learning_rate": 3.4129772246149357e-05,
      "loss": 0.6601,
      "step": 2087100
    },
    {
      "epoch": 19.04518577998394,
      "grad_norm": 3.624817371368408,
      "learning_rate": 3.412901185001339e-05,
      "loss": 0.7079,
      "step": 2087200
    },
    {
      "epoch": 19.046098255347104,
      "grad_norm": 3.7704360485076904,
      "learning_rate": 3.412825145387742e-05,
      "loss": 0.6582,
      "step": 2087300
    },
    {
      "epoch": 19.04701073071027,
      "grad_norm": 2.5018982887268066,
      "learning_rate": 3.412749105774145e-05,
      "loss": 0.6408,
      "step": 2087400
    },
    {
      "epoch": 19.047923206073435,
      "grad_norm": 4.268832206726074,
      "learning_rate": 3.412673066160547e-05,
      "loss": 0.6488,
      "step": 2087500
    },
    {
      "epoch": 19.0488356814366,
      "grad_norm": 4.982763290405273,
      "learning_rate": 3.41259702654695e-05,
      "loss": 0.6485,
      "step": 2087600
    },
    {
      "epoch": 19.049748156799765,
      "grad_norm": 3.2012767791748047,
      "learning_rate": 3.412520986933353e-05,
      "loss": 0.6579,
      "step": 2087700
    },
    {
      "epoch": 19.05066063216293,
      "grad_norm": 3.5260281562805176,
      "learning_rate": 3.4124449473197553e-05,
      "loss": 0.6852,
      "step": 2087800
    },
    {
      "epoch": 19.051573107526096,
      "grad_norm": 4.268176555633545,
      "learning_rate": 3.412368907706159e-05,
      "loss": 0.618,
      "step": 2087900
    },
    {
      "epoch": 19.05248558288926,
      "grad_norm": 3.8888306617736816,
      "learning_rate": 3.4122928680925614e-05,
      "loss": 0.6384,
      "step": 2088000
    },
    {
      "epoch": 19.053398058252426,
      "grad_norm": 3.7760491371154785,
      "learning_rate": 3.4122168284789644e-05,
      "loss": 0.6967,
      "step": 2088100
    },
    {
      "epoch": 19.054310533615592,
      "grad_norm": 3.414152145385742,
      "learning_rate": 3.4121407888653674e-05,
      "loss": 0.6936,
      "step": 2088200
    },
    {
      "epoch": 19.055223008978757,
      "grad_norm": 5.373423099517822,
      "learning_rate": 3.4120647492517704e-05,
      "loss": 0.6799,
      "step": 2088300
    },
    {
      "epoch": 19.056135484341922,
      "grad_norm": 3.380716323852539,
      "learning_rate": 3.4119887096381734e-05,
      "loss": 0.6689,
      "step": 2088400
    },
    {
      "epoch": 19.057047959705088,
      "grad_norm": 2.607316732406616,
      "learning_rate": 3.4119126700245764e-05,
      "loss": 0.6703,
      "step": 2088500
    },
    {
      "epoch": 19.057960435068253,
      "grad_norm": 4.854406833648682,
      "learning_rate": 3.411836630410979e-05,
      "loss": 0.6493,
      "step": 2088600
    },
    {
      "epoch": 19.05887291043142,
      "grad_norm": 3.6345343589782715,
      "learning_rate": 3.4117605907973824e-05,
      "loss": 0.6433,
      "step": 2088700
    },
    {
      "epoch": 19.059785385794584,
      "grad_norm": 2.263176918029785,
      "learning_rate": 3.411684551183785e-05,
      "loss": 0.6955,
      "step": 2088800
    },
    {
      "epoch": 19.06069786115775,
      "grad_norm": 5.031764030456543,
      "learning_rate": 3.411608511570188e-05,
      "loss": 0.6405,
      "step": 2088900
    },
    {
      "epoch": 19.061610336520914,
      "grad_norm": 3.6544101238250732,
      "learning_rate": 3.411532471956591e-05,
      "loss": 0.624,
      "step": 2089000
    },
    {
      "epoch": 19.06252281188408,
      "grad_norm": 4.103450775146484,
      "learning_rate": 3.411456432342994e-05,
      "loss": 0.6741,
      "step": 2089100
    },
    {
      "epoch": 19.063435287247245,
      "grad_norm": 4.108473300933838,
      "learning_rate": 3.411380392729396e-05,
      "loss": 0.6478,
      "step": 2089200
    },
    {
      "epoch": 19.06434776261041,
      "grad_norm": 3.6920247077941895,
      "learning_rate": 3.4113043531158e-05,
      "loss": 0.674,
      "step": 2089300
    },
    {
      "epoch": 19.065260237973575,
      "grad_norm": 4.7858567237854,
      "learning_rate": 3.411228313502202e-05,
      "loss": 0.6475,
      "step": 2089400
    },
    {
      "epoch": 19.06617271333674,
      "grad_norm": 4.1423516273498535,
      "learning_rate": 3.411152273888605e-05,
      "loss": 0.647,
      "step": 2089500
    },
    {
      "epoch": 19.067085188699906,
      "grad_norm": 4.757363319396973,
      "learning_rate": 3.411076234275008e-05,
      "loss": 0.6231,
      "step": 2089600
    },
    {
      "epoch": 19.06799766406307,
      "grad_norm": 3.8168225288391113,
      "learning_rate": 3.411000194661411e-05,
      "loss": 0.6453,
      "step": 2089700
    },
    {
      "epoch": 19.068910139426237,
      "grad_norm": 2.5422723293304443,
      "learning_rate": 3.410924155047814e-05,
      "loss": 0.7022,
      "step": 2089800
    },
    {
      "epoch": 19.069822614789402,
      "grad_norm": 4.160265922546387,
      "learning_rate": 3.410848115434217e-05,
      "loss": 0.677,
      "step": 2089900
    },
    {
      "epoch": 19.070735090152567,
      "grad_norm": 3.986989736557007,
      "learning_rate": 3.4107720758206195e-05,
      "loss": 0.6838,
      "step": 2090000
    },
    {
      "epoch": 19.071647565515732,
      "grad_norm": 3.8375909328460693,
      "learning_rate": 3.410696036207023e-05,
      "loss": 0.6519,
      "step": 2090100
    },
    {
      "epoch": 19.072560040878898,
      "grad_norm": 2.86910080909729,
      "learning_rate": 3.4106199965934255e-05,
      "loss": 0.6273,
      "step": 2090200
    },
    {
      "epoch": 19.073472516242063,
      "grad_norm": 3.3956449031829834,
      "learning_rate": 3.4105439569798285e-05,
      "loss": 0.6367,
      "step": 2090300
    },
    {
      "epoch": 19.07438499160523,
      "grad_norm": 3.3867974281311035,
      "learning_rate": 3.4104679173662315e-05,
      "loss": 0.6416,
      "step": 2090400
    },
    {
      "epoch": 19.07529746696839,
      "grad_norm": 4.056641101837158,
      "learning_rate": 3.410391877752634e-05,
      "loss": 0.6613,
      "step": 2090500
    },
    {
      "epoch": 19.076209942331555,
      "grad_norm": 4.260913848876953,
      "learning_rate": 3.410315838139037e-05,
      "loss": 0.6279,
      "step": 2090600
    },
    {
      "epoch": 19.07712241769472,
      "grad_norm": 5.080663204193115,
      "learning_rate": 3.41023979852544e-05,
      "loss": 0.7022,
      "step": 2090700
    },
    {
      "epoch": 19.078034893057886,
      "grad_norm": 3.918996810913086,
      "learning_rate": 3.410163758911843e-05,
      "loss": 0.6594,
      "step": 2090800
    },
    {
      "epoch": 19.07894736842105,
      "grad_norm": 3.875643491744995,
      "learning_rate": 3.410087719298246e-05,
      "loss": 0.6537,
      "step": 2090900
    },
    {
      "epoch": 19.079859843784217,
      "grad_norm": 3.386045217514038,
      "learning_rate": 3.410011679684649e-05,
      "loss": 0.6621,
      "step": 2091000
    },
    {
      "epoch": 19.080772319147382,
      "grad_norm": 4.889247894287109,
      "learning_rate": 3.409935640071051e-05,
      "loss": 0.65,
      "step": 2091100
    },
    {
      "epoch": 19.081684794510547,
      "grad_norm": 3.8483574390411377,
      "learning_rate": 3.409859600457455e-05,
      "loss": 0.6415,
      "step": 2091200
    },
    {
      "epoch": 19.082597269873713,
      "grad_norm": 4.385183811187744,
      "learning_rate": 3.409783560843857e-05,
      "loss": 0.6502,
      "step": 2091300
    },
    {
      "epoch": 19.083509745236878,
      "grad_norm": 3.3549704551696777,
      "learning_rate": 3.40970752123026e-05,
      "loss": 0.6742,
      "step": 2091400
    },
    {
      "epoch": 19.084422220600043,
      "grad_norm": 4.3081440925598145,
      "learning_rate": 3.409631481616663e-05,
      "loss": 0.6804,
      "step": 2091500
    },
    {
      "epoch": 19.08533469596321,
      "grad_norm": 4.053841590881348,
      "learning_rate": 3.409555442003066e-05,
      "loss": 0.6461,
      "step": 2091600
    },
    {
      "epoch": 19.086247171326374,
      "grad_norm": 3.4267051219940186,
      "learning_rate": 3.4094794023894685e-05,
      "loss": 0.6368,
      "step": 2091700
    },
    {
      "epoch": 19.08715964668954,
      "grad_norm": 4.1414337158203125,
      "learning_rate": 3.409403362775872e-05,
      "loss": 0.6228,
      "step": 2091800
    },
    {
      "epoch": 19.088072122052704,
      "grad_norm": 3.9381508827209473,
      "learning_rate": 3.4093273231622746e-05,
      "loss": 0.6574,
      "step": 2091900
    },
    {
      "epoch": 19.08898459741587,
      "grad_norm": 4.313752174377441,
      "learning_rate": 3.4092512835486776e-05,
      "loss": 0.629,
      "step": 2092000
    },
    {
      "epoch": 19.089897072779035,
      "grad_norm": 2.7663381099700928,
      "learning_rate": 3.4091752439350806e-05,
      "loss": 0.6588,
      "step": 2092100
    },
    {
      "epoch": 19.0908095481422,
      "grad_norm": 5.20036506652832,
      "learning_rate": 3.4090992043214836e-05,
      "loss": 0.6488,
      "step": 2092200
    },
    {
      "epoch": 19.091722023505366,
      "grad_norm": 2.7904930114746094,
      "learning_rate": 3.4090231647078866e-05,
      "loss": 0.656,
      "step": 2092300
    },
    {
      "epoch": 19.09263449886853,
      "grad_norm": 5.560576915740967,
      "learning_rate": 3.4089471250942896e-05,
      "loss": 0.6939,
      "step": 2092400
    },
    {
      "epoch": 19.093546974231696,
      "grad_norm": 3.0519049167633057,
      "learning_rate": 3.408871085480692e-05,
      "loss": 0.6466,
      "step": 2092500
    },
    {
      "epoch": 19.09445944959486,
      "grad_norm": 3.7792069911956787,
      "learning_rate": 3.4087950458670956e-05,
      "loss": 0.6402,
      "step": 2092600
    },
    {
      "epoch": 19.095371924958027,
      "grad_norm": 3.6550612449645996,
      "learning_rate": 3.408719006253498e-05,
      "loss": 0.7134,
      "step": 2092700
    },
    {
      "epoch": 19.096284400321192,
      "grad_norm": 3.628384590148926,
      "learning_rate": 3.408642966639901e-05,
      "loss": 0.6555,
      "step": 2092800
    },
    {
      "epoch": 19.097196875684357,
      "grad_norm": 4.415132522583008,
      "learning_rate": 3.408566927026304e-05,
      "loss": 0.6601,
      "step": 2092900
    },
    {
      "epoch": 19.098109351047523,
      "grad_norm": 4.699366092681885,
      "learning_rate": 3.408490887412707e-05,
      "loss": 0.6397,
      "step": 2093000
    },
    {
      "epoch": 19.099021826410688,
      "grad_norm": 4.7660298347473145,
      "learning_rate": 3.408414847799109e-05,
      "loss": 0.6741,
      "step": 2093100
    },
    {
      "epoch": 19.099934301773853,
      "grad_norm": 4.870597839355469,
      "learning_rate": 3.408338808185512e-05,
      "loss": 0.6968,
      "step": 2093200
    },
    {
      "epoch": 19.10084677713702,
      "grad_norm": 4.5758562088012695,
      "learning_rate": 3.408262768571915e-05,
      "loss": 0.6597,
      "step": 2093300
    },
    {
      "epoch": 19.101759252500184,
      "grad_norm": 3.724917411804199,
      "learning_rate": 3.408186728958318e-05,
      "loss": 0.6838,
      "step": 2093400
    },
    {
      "epoch": 19.10267172786335,
      "grad_norm": 3.1957669258117676,
      "learning_rate": 3.408110689344721e-05,
      "loss": 0.673,
      "step": 2093500
    },
    {
      "epoch": 19.103584203226514,
      "grad_norm": 3.2262115478515625,
      "learning_rate": 3.4080346497311236e-05,
      "loss": 0.628,
      "step": 2093600
    },
    {
      "epoch": 19.10449667858968,
      "grad_norm": 5.702823638916016,
      "learning_rate": 3.407958610117527e-05,
      "loss": 0.644,
      "step": 2093700
    },
    {
      "epoch": 19.105409153952845,
      "grad_norm": 3.591254234313965,
      "learning_rate": 3.4078825705039297e-05,
      "loss": 0.6168,
      "step": 2093800
    },
    {
      "epoch": 19.106321629316007,
      "grad_norm": 3.1617136001586914,
      "learning_rate": 3.4078065308903327e-05,
      "loss": 0.6049,
      "step": 2093900
    },
    {
      "epoch": 19.107234104679172,
      "grad_norm": 2.782362699508667,
      "learning_rate": 3.407730491276736e-05,
      "loss": 0.6192,
      "step": 2094000
    },
    {
      "epoch": 19.108146580042337,
      "grad_norm": 5.080167293548584,
      "learning_rate": 3.407654451663139e-05,
      "loss": 0.6537,
      "step": 2094100
    },
    {
      "epoch": 19.109059055405503,
      "grad_norm": 3.7400012016296387,
      "learning_rate": 3.407578412049541e-05,
      "loss": 0.6523,
      "step": 2094200
    },
    {
      "epoch": 19.109971530768668,
      "grad_norm": 4.007354259490967,
      "learning_rate": 3.407502372435945e-05,
      "loss": 0.6518,
      "step": 2094300
    },
    {
      "epoch": 19.110884006131833,
      "grad_norm": 4.234192371368408,
      "learning_rate": 3.407426332822347e-05,
      "loss": 0.6466,
      "step": 2094400
    },
    {
      "epoch": 19.111796481495,
      "grad_norm": 4.140522480010986,
      "learning_rate": 3.40735029320875e-05,
      "loss": 0.646,
      "step": 2094500
    },
    {
      "epoch": 19.112708956858164,
      "grad_norm": 3.47947096824646,
      "learning_rate": 3.407274253595153e-05,
      "loss": 0.6565,
      "step": 2094600
    },
    {
      "epoch": 19.11362143222133,
      "grad_norm": 2.7116119861602783,
      "learning_rate": 3.407198213981556e-05,
      "loss": 0.6679,
      "step": 2094700
    },
    {
      "epoch": 19.114533907584494,
      "grad_norm": 3.839022636413574,
      "learning_rate": 3.407122174367959e-05,
      "loss": 0.6653,
      "step": 2094800
    },
    {
      "epoch": 19.11544638294766,
      "grad_norm": 4.275763988494873,
      "learning_rate": 3.407046134754362e-05,
      "loss": 0.6664,
      "step": 2094900
    },
    {
      "epoch": 19.116358858310825,
      "grad_norm": 4.152560234069824,
      "learning_rate": 3.4069700951407644e-05,
      "loss": 0.6604,
      "step": 2095000
    },
    {
      "epoch": 19.11727133367399,
      "grad_norm": 3.7013423442840576,
      "learning_rate": 3.406894055527168e-05,
      "loss": 0.6376,
      "step": 2095100
    },
    {
      "epoch": 19.118183809037156,
      "grad_norm": 4.267921447753906,
      "learning_rate": 3.4068180159135704e-05,
      "loss": 0.6989,
      "step": 2095200
    },
    {
      "epoch": 19.11909628440032,
      "grad_norm": 4.711546897888184,
      "learning_rate": 3.4067419762999734e-05,
      "loss": 0.6442,
      "step": 2095300
    },
    {
      "epoch": 19.120008759763486,
      "grad_norm": 3.998589277267456,
      "learning_rate": 3.4066659366863764e-05,
      "loss": 0.6715,
      "step": 2095400
    },
    {
      "epoch": 19.12092123512665,
      "grad_norm": 4.489009857177734,
      "learning_rate": 3.4065898970727794e-05,
      "loss": 0.6706,
      "step": 2095500
    },
    {
      "epoch": 19.121833710489817,
      "grad_norm": 3.418901205062866,
      "learning_rate": 3.406513857459182e-05,
      "loss": 0.6913,
      "step": 2095600
    },
    {
      "epoch": 19.122746185852982,
      "grad_norm": 4.269759178161621,
      "learning_rate": 3.4064378178455854e-05,
      "loss": 0.6451,
      "step": 2095700
    },
    {
      "epoch": 19.123658661216147,
      "grad_norm": 3.741086721420288,
      "learning_rate": 3.406361778231988e-05,
      "loss": 0.6254,
      "step": 2095800
    },
    {
      "epoch": 19.124571136579313,
      "grad_norm": 3.9793851375579834,
      "learning_rate": 3.406285738618391e-05,
      "loss": 0.6585,
      "step": 2095900
    },
    {
      "epoch": 19.125483611942478,
      "grad_norm": 4.127858638763428,
      "learning_rate": 3.406209699004794e-05,
      "loss": 0.6506,
      "step": 2096000
    },
    {
      "epoch": 19.126396087305643,
      "grad_norm": 3.497469902038574,
      "learning_rate": 3.406133659391196e-05,
      "loss": 0.6328,
      "step": 2096100
    },
    {
      "epoch": 19.12730856266881,
      "grad_norm": 4.602517604827881,
      "learning_rate": 3.4060576197776e-05,
      "loss": 0.6595,
      "step": 2096200
    },
    {
      "epoch": 19.128221038031974,
      "grad_norm": 4.1714186668396,
      "learning_rate": 3.405981580164002e-05,
      "loss": 0.6603,
      "step": 2096300
    },
    {
      "epoch": 19.12913351339514,
      "grad_norm": 4.458227634429932,
      "learning_rate": 3.405905540550405e-05,
      "loss": 0.6432,
      "step": 2096400
    },
    {
      "epoch": 19.130045988758305,
      "grad_norm": 3.7022814750671387,
      "learning_rate": 3.405829500936808e-05,
      "loss": 0.6465,
      "step": 2096500
    },
    {
      "epoch": 19.13095846412147,
      "grad_norm": 3.6064553260803223,
      "learning_rate": 3.405753461323211e-05,
      "loss": 0.6539,
      "step": 2096600
    },
    {
      "epoch": 19.131870939484635,
      "grad_norm": 3.2054457664489746,
      "learning_rate": 3.4056774217096135e-05,
      "loss": 0.6833,
      "step": 2096700
    },
    {
      "epoch": 19.1327834148478,
      "grad_norm": 2.7967770099639893,
      "learning_rate": 3.405601382096017e-05,
      "loss": 0.6767,
      "step": 2096800
    },
    {
      "epoch": 19.133695890210966,
      "grad_norm": 4.247690677642822,
      "learning_rate": 3.4055253424824195e-05,
      "loss": 0.6646,
      "step": 2096900
    },
    {
      "epoch": 19.13460836557413,
      "grad_norm": 3.5578103065490723,
      "learning_rate": 3.4054493028688225e-05,
      "loss": 0.6539,
      "step": 2097000
    },
    {
      "epoch": 19.135520840937296,
      "grad_norm": 4.526177883148193,
      "learning_rate": 3.4053732632552255e-05,
      "loss": 0.6607,
      "step": 2097100
    },
    {
      "epoch": 19.13643331630046,
      "grad_norm": 2.885838508605957,
      "learning_rate": 3.4052972236416285e-05,
      "loss": 0.6389,
      "step": 2097200
    },
    {
      "epoch": 19.137345791663623,
      "grad_norm": 4.266629219055176,
      "learning_rate": 3.4052211840280315e-05,
      "loss": 0.6703,
      "step": 2097300
    },
    {
      "epoch": 19.13825826702679,
      "grad_norm": 3.616689443588257,
      "learning_rate": 3.4051451444144345e-05,
      "loss": 0.6435,
      "step": 2097400
    },
    {
      "epoch": 19.139170742389954,
      "grad_norm": 4.133505344390869,
      "learning_rate": 3.405069104800837e-05,
      "loss": 0.6474,
      "step": 2097500
    },
    {
      "epoch": 19.14008321775312,
      "grad_norm": 3.9341273307800293,
      "learning_rate": 3.4049930651872405e-05,
      "loss": 0.6593,
      "step": 2097600
    },
    {
      "epoch": 19.140995693116285,
      "grad_norm": 3.643827438354492,
      "learning_rate": 3.404917025573643e-05,
      "loss": 0.6466,
      "step": 2097700
    },
    {
      "epoch": 19.14190816847945,
      "grad_norm": 3.270385980606079,
      "learning_rate": 3.404840985960046e-05,
      "loss": 0.6597,
      "step": 2097800
    },
    {
      "epoch": 19.142820643842615,
      "grad_norm": 4.26613712310791,
      "learning_rate": 3.404764946346449e-05,
      "loss": 0.6622,
      "step": 2097900
    },
    {
      "epoch": 19.14373311920578,
      "grad_norm": 4.03573751449585,
      "learning_rate": 3.404688906732852e-05,
      "loss": 0.6778,
      "step": 2098000
    },
    {
      "epoch": 19.144645594568946,
      "grad_norm": 4.429363250732422,
      "learning_rate": 3.404612867119254e-05,
      "loss": 0.6848,
      "step": 2098100
    },
    {
      "epoch": 19.14555806993211,
      "grad_norm": 4.000682353973389,
      "learning_rate": 3.404536827505658e-05,
      "loss": 0.6692,
      "step": 2098200
    },
    {
      "epoch": 19.146470545295276,
      "grad_norm": 2.9363067150115967,
      "learning_rate": 3.40446078789206e-05,
      "loss": 0.6389,
      "step": 2098300
    },
    {
      "epoch": 19.14738302065844,
      "grad_norm": 4.872117042541504,
      "learning_rate": 3.404384748278463e-05,
      "loss": 0.6675,
      "step": 2098400
    },
    {
      "epoch": 19.148295496021607,
      "grad_norm": 4.150339126586914,
      "learning_rate": 3.404308708664866e-05,
      "loss": 0.7074,
      "step": 2098500
    },
    {
      "epoch": 19.149207971384772,
      "grad_norm": 4.488252639770508,
      "learning_rate": 3.404232669051269e-05,
      "loss": 0.6683,
      "step": 2098600
    },
    {
      "epoch": 19.150120446747938,
      "grad_norm": 3.2368290424346924,
      "learning_rate": 3.404156629437672e-05,
      "loss": 0.6335,
      "step": 2098700
    },
    {
      "epoch": 19.151032922111103,
      "grad_norm": 3.5084245204925537,
      "learning_rate": 3.404080589824075e-05,
      "loss": 0.6851,
      "step": 2098800
    },
    {
      "epoch": 19.151945397474268,
      "grad_norm": 4.3080549240112305,
      "learning_rate": 3.4040045502104776e-05,
      "loss": 0.686,
      "step": 2098900
    },
    {
      "epoch": 19.152857872837433,
      "grad_norm": 3.0160889625549316,
      "learning_rate": 3.4039285105968806e-05,
      "loss": 0.6595,
      "step": 2099000
    },
    {
      "epoch": 19.1537703482006,
      "grad_norm": 3.801708221435547,
      "learning_rate": 3.4038524709832836e-05,
      "loss": 0.6381,
      "step": 2099100
    },
    {
      "epoch": 19.154682823563764,
      "grad_norm": 4.478055000305176,
      "learning_rate": 3.4037764313696866e-05,
      "loss": 0.6491,
      "step": 2099200
    },
    {
      "epoch": 19.15559529892693,
      "grad_norm": 1.9754774570465088,
      "learning_rate": 3.4037003917560896e-05,
      "loss": 0.6528,
      "step": 2099300
    },
    {
      "epoch": 19.156507774290095,
      "grad_norm": 4.306735992431641,
      "learning_rate": 3.403624352142492e-05,
      "loss": 0.6814,
      "step": 2099400
    },
    {
      "epoch": 19.15742024965326,
      "grad_norm": 3.244910478591919,
      "learning_rate": 3.4035483125288956e-05,
      "loss": 0.6393,
      "step": 2099500
    },
    {
      "epoch": 19.158332725016425,
      "grad_norm": 3.36794114112854,
      "learning_rate": 3.403472272915298e-05,
      "loss": 0.6963,
      "step": 2099600
    },
    {
      "epoch": 19.15924520037959,
      "grad_norm": 4.77984619140625,
      "learning_rate": 3.403396233301701e-05,
      "loss": 0.6866,
      "step": 2099700
    },
    {
      "epoch": 19.160157675742756,
      "grad_norm": 3.912400484085083,
      "learning_rate": 3.403320193688104e-05,
      "loss": 0.6362,
      "step": 2099800
    },
    {
      "epoch": 19.16107015110592,
      "grad_norm": 4.131714820861816,
      "learning_rate": 3.403244154074507e-05,
      "loss": 0.685,
      "step": 2099900
    },
    {
      "epoch": 19.161982626469086,
      "grad_norm": 3.6051321029663086,
      "learning_rate": 3.403168114460909e-05,
      "loss": 0.6674,
      "step": 2100000
    },
    {
      "epoch": 19.16289510183225,
      "grad_norm": 3.772037982940674,
      "learning_rate": 3.403092074847313e-05,
      "loss": 0.6535,
      "step": 2100100
    },
    {
      "epoch": 19.163807577195417,
      "grad_norm": 4.749565124511719,
      "learning_rate": 3.403016035233715e-05,
      "loss": 0.6313,
      "step": 2100200
    },
    {
      "epoch": 19.164720052558582,
      "grad_norm": 3.8961493968963623,
      "learning_rate": 3.402939995620118e-05,
      "loss": 0.6606,
      "step": 2100300
    },
    {
      "epoch": 19.165632527921748,
      "grad_norm": 4.002046585083008,
      "learning_rate": 3.402863956006521e-05,
      "loss": 0.6339,
      "step": 2100400
    },
    {
      "epoch": 19.166545003284913,
      "grad_norm": 3.8875653743743896,
      "learning_rate": 3.402787916392924e-05,
      "loss": 0.6588,
      "step": 2100500
    },
    {
      "epoch": 19.16745747864808,
      "grad_norm": 5.017004489898682,
      "learning_rate": 3.402711876779327e-05,
      "loss": 0.6243,
      "step": 2100600
    },
    {
      "epoch": 19.16836995401124,
      "grad_norm": 3.7460567951202393,
      "learning_rate": 3.40263583716573e-05,
      "loss": 0.6585,
      "step": 2100700
    },
    {
      "epoch": 19.169282429374405,
      "grad_norm": 4.383819580078125,
      "learning_rate": 3.402559797552133e-05,
      "loss": 0.6309,
      "step": 2100800
    },
    {
      "epoch": 19.17019490473757,
      "grad_norm": 5.119711875915527,
      "learning_rate": 3.4024837579385364e-05,
      "loss": 0.6655,
      "step": 2100900
    },
    {
      "epoch": 19.171107380100736,
      "grad_norm": 4.421803951263428,
      "learning_rate": 3.402407718324939e-05,
      "loss": 0.6531,
      "step": 2101000
    },
    {
      "epoch": 19.1720198554639,
      "grad_norm": 3.531256914138794,
      "learning_rate": 3.402331678711342e-05,
      "loss": 0.6764,
      "step": 2101100
    },
    {
      "epoch": 19.172932330827066,
      "grad_norm": 4.057772636413574,
      "learning_rate": 3.402255639097745e-05,
      "loss": 0.6177,
      "step": 2101200
    },
    {
      "epoch": 19.173844806190232,
      "grad_norm": 4.297275066375732,
      "learning_rate": 3.402179599484148e-05,
      "loss": 0.6509,
      "step": 2101300
    },
    {
      "epoch": 19.174757281553397,
      "grad_norm": 4.653033256530762,
      "learning_rate": 3.40210355987055e-05,
      "loss": 0.681,
      "step": 2101400
    },
    {
      "epoch": 19.175669756916562,
      "grad_norm": 4.4758782386779785,
      "learning_rate": 3.402027520256954e-05,
      "loss": 0.6662,
      "step": 2101500
    },
    {
      "epoch": 19.176582232279728,
      "grad_norm": 3.61944842338562,
      "learning_rate": 3.401951480643356e-05,
      "loss": 0.6287,
      "step": 2101600
    },
    {
      "epoch": 19.177494707642893,
      "grad_norm": 4.425563812255859,
      "learning_rate": 3.401875441029759e-05,
      "loss": 0.6776,
      "step": 2101700
    },
    {
      "epoch": 19.17840718300606,
      "grad_norm": 3.7857565879821777,
      "learning_rate": 3.401799401416162e-05,
      "loss": 0.69,
      "step": 2101800
    },
    {
      "epoch": 19.179319658369224,
      "grad_norm": 4.931980133056641,
      "learning_rate": 3.4017233618025644e-05,
      "loss": 0.6625,
      "step": 2101900
    },
    {
      "epoch": 19.18023213373239,
      "grad_norm": 5.52363920211792,
      "learning_rate": 3.401647322188968e-05,
      "loss": 0.6402,
      "step": 2102000
    },
    {
      "epoch": 19.181144609095554,
      "grad_norm": 4.376934051513672,
      "learning_rate": 3.4015712825753704e-05,
      "loss": 0.6962,
      "step": 2102100
    },
    {
      "epoch": 19.18205708445872,
      "grad_norm": 3.565026044845581,
      "learning_rate": 3.4014952429617734e-05,
      "loss": 0.6684,
      "step": 2102200
    },
    {
      "epoch": 19.182969559821885,
      "grad_norm": 4.262533664703369,
      "learning_rate": 3.4014192033481764e-05,
      "loss": 0.6483,
      "step": 2102300
    },
    {
      "epoch": 19.18388203518505,
      "grad_norm": 3.951972484588623,
      "learning_rate": 3.4013431637345794e-05,
      "loss": 0.6282,
      "step": 2102400
    },
    {
      "epoch": 19.184794510548215,
      "grad_norm": 3.1608495712280273,
      "learning_rate": 3.401267124120982e-05,
      "loss": 0.6786,
      "step": 2102500
    },
    {
      "epoch": 19.18570698591138,
      "grad_norm": 3.9707067012786865,
      "learning_rate": 3.4011910845073854e-05,
      "loss": 0.6721,
      "step": 2102600
    },
    {
      "epoch": 19.186619461274546,
      "grad_norm": 4.164525985717773,
      "learning_rate": 3.401115044893788e-05,
      "loss": 0.6721,
      "step": 2102700
    },
    {
      "epoch": 19.18753193663771,
      "grad_norm": 3.3541715145111084,
      "learning_rate": 3.401039005280191e-05,
      "loss": 0.6284,
      "step": 2102800
    },
    {
      "epoch": 19.188444412000877,
      "grad_norm": 3.717928647994995,
      "learning_rate": 3.400962965666594e-05,
      "loss": 0.6375,
      "step": 2102900
    },
    {
      "epoch": 19.189356887364042,
      "grad_norm": 4.242250442504883,
      "learning_rate": 3.400886926052997e-05,
      "loss": 0.6662,
      "step": 2103000
    },
    {
      "epoch": 19.190269362727207,
      "grad_norm": 3.8678884506225586,
      "learning_rate": 3.4008108864394e-05,
      "loss": 0.6185,
      "step": 2103100
    },
    {
      "epoch": 19.191181838090372,
      "grad_norm": 3.5363709926605225,
      "learning_rate": 3.400734846825803e-05,
      "loss": 0.6554,
      "step": 2103200
    },
    {
      "epoch": 19.192094313453538,
      "grad_norm": 4.408821105957031,
      "learning_rate": 3.400658807212205e-05,
      "loss": 0.6722,
      "step": 2103300
    },
    {
      "epoch": 19.193006788816703,
      "grad_norm": 3.7054855823516846,
      "learning_rate": 3.400582767598609e-05,
      "loss": 0.6604,
      "step": 2103400
    },
    {
      "epoch": 19.19391926417987,
      "grad_norm": 4.468749523162842,
      "learning_rate": 3.400506727985011e-05,
      "loss": 0.6441,
      "step": 2103500
    },
    {
      "epoch": 19.194831739543034,
      "grad_norm": 3.631277561187744,
      "learning_rate": 3.400430688371414e-05,
      "loss": 0.6864,
      "step": 2103600
    },
    {
      "epoch": 19.1957442149062,
      "grad_norm": 3.7090115547180176,
      "learning_rate": 3.400354648757817e-05,
      "loss": 0.6197,
      "step": 2103700
    },
    {
      "epoch": 19.196656690269364,
      "grad_norm": 4.062993049621582,
      "learning_rate": 3.40027860914422e-05,
      "loss": 0.6859,
      "step": 2103800
    },
    {
      "epoch": 19.19756916563253,
      "grad_norm": 3.9673514366149902,
      "learning_rate": 3.4002025695306225e-05,
      "loss": 0.6462,
      "step": 2103900
    },
    {
      "epoch": 19.198481640995695,
      "grad_norm": 3.436692237854004,
      "learning_rate": 3.400126529917026e-05,
      "loss": 0.6491,
      "step": 2104000
    },
    {
      "epoch": 19.199394116358857,
      "grad_norm": 3.71282958984375,
      "learning_rate": 3.4000504903034285e-05,
      "loss": 0.6534,
      "step": 2104100
    },
    {
      "epoch": 19.200306591722022,
      "grad_norm": 3.4710171222686768,
      "learning_rate": 3.3999744506898315e-05,
      "loss": 0.6508,
      "step": 2104200
    },
    {
      "epoch": 19.201219067085187,
      "grad_norm": 4.868592739105225,
      "learning_rate": 3.3998984110762345e-05,
      "loss": 0.6504,
      "step": 2104300
    },
    {
      "epoch": 19.202131542448353,
      "grad_norm": 4.017151832580566,
      "learning_rate": 3.3998223714626375e-05,
      "loss": 0.6562,
      "step": 2104400
    },
    {
      "epoch": 19.203044017811518,
      "grad_norm": 4.089725017547607,
      "learning_rate": 3.3997463318490405e-05,
      "loss": 0.6862,
      "step": 2104500
    },
    {
      "epoch": 19.203956493174683,
      "grad_norm": 4.86830472946167,
      "learning_rate": 3.399670292235443e-05,
      "loss": 0.6274,
      "step": 2104600
    },
    {
      "epoch": 19.20486896853785,
      "grad_norm": 4.307725429534912,
      "learning_rate": 3.399594252621846e-05,
      "loss": 0.6628,
      "step": 2104700
    },
    {
      "epoch": 19.205781443901014,
      "grad_norm": 3.7412197589874268,
      "learning_rate": 3.399518213008249e-05,
      "loss": 0.6675,
      "step": 2104800
    },
    {
      "epoch": 19.20669391926418,
      "grad_norm": 4.678553104400635,
      "learning_rate": 3.399442173394652e-05,
      "loss": 0.6562,
      "step": 2104900
    },
    {
      "epoch": 19.207606394627344,
      "grad_norm": 3.2420928478240967,
      "learning_rate": 3.399366133781054e-05,
      "loss": 0.6733,
      "step": 2105000
    },
    {
      "epoch": 19.20851886999051,
      "grad_norm": 4.3844170570373535,
      "learning_rate": 3.399290094167458e-05,
      "loss": 0.6563,
      "step": 2105100
    },
    {
      "epoch": 19.209431345353675,
      "grad_norm": 4.144224166870117,
      "learning_rate": 3.39921405455386e-05,
      "loss": 0.6651,
      "step": 2105200
    },
    {
      "epoch": 19.21034382071684,
      "grad_norm": 4.607036113739014,
      "learning_rate": 3.399138014940263e-05,
      "loss": 0.6431,
      "step": 2105300
    },
    {
      "epoch": 19.211256296080006,
      "grad_norm": 4.541730880737305,
      "learning_rate": 3.399061975326666e-05,
      "loss": 0.6393,
      "step": 2105400
    },
    {
      "epoch": 19.21216877144317,
      "grad_norm": 4.03529167175293,
      "learning_rate": 3.398985935713069e-05,
      "loss": 0.6611,
      "step": 2105500
    },
    {
      "epoch": 19.213081246806336,
      "grad_norm": 4.249030590057373,
      "learning_rate": 3.398909896099472e-05,
      "loss": 0.6396,
      "step": 2105600
    },
    {
      "epoch": 19.2139937221695,
      "grad_norm": 3.9097342491149902,
      "learning_rate": 3.398833856485875e-05,
      "loss": 0.6963,
      "step": 2105700
    },
    {
      "epoch": 19.214906197532667,
      "grad_norm": 4.161280632019043,
      "learning_rate": 3.3987578168722776e-05,
      "loss": 0.646,
      "step": 2105800
    },
    {
      "epoch": 19.215818672895832,
      "grad_norm": 3.49770188331604,
      "learning_rate": 3.398681777258681e-05,
      "loss": 0.6436,
      "step": 2105900
    },
    {
      "epoch": 19.216731148258997,
      "grad_norm": 3.777683973312378,
      "learning_rate": 3.3986057376450836e-05,
      "loss": 0.6531,
      "step": 2106000
    },
    {
      "epoch": 19.217643623622163,
      "grad_norm": 3.4420650005340576,
      "learning_rate": 3.3985296980314866e-05,
      "loss": 0.6807,
      "step": 2106100
    },
    {
      "epoch": 19.218556098985328,
      "grad_norm": 4.134997367858887,
      "learning_rate": 3.3984536584178896e-05,
      "loss": 0.685,
      "step": 2106200
    },
    {
      "epoch": 19.219468574348493,
      "grad_norm": 2.031968116760254,
      "learning_rate": 3.3983776188042926e-05,
      "loss": 0.6558,
      "step": 2106300
    },
    {
      "epoch": 19.22038104971166,
      "grad_norm": 4.524570465087891,
      "learning_rate": 3.398301579190695e-05,
      "loss": 0.6854,
      "step": 2106400
    },
    {
      "epoch": 19.221293525074824,
      "grad_norm": 3.6630806922912598,
      "learning_rate": 3.3982255395770986e-05,
      "loss": 0.632,
      "step": 2106500
    },
    {
      "epoch": 19.22220600043799,
      "grad_norm": 4.203445911407471,
      "learning_rate": 3.398149499963501e-05,
      "loss": 0.6248,
      "step": 2106600
    },
    {
      "epoch": 19.223118475801154,
      "grad_norm": 3.6092891693115234,
      "learning_rate": 3.398073460349904e-05,
      "loss": 0.6594,
      "step": 2106700
    },
    {
      "epoch": 19.22403095116432,
      "grad_norm": 3.3390660285949707,
      "learning_rate": 3.397997420736307e-05,
      "loss": 0.6384,
      "step": 2106800
    },
    {
      "epoch": 19.224943426527485,
      "grad_norm": 3.2154438495635986,
      "learning_rate": 3.39792138112271e-05,
      "loss": 0.6682,
      "step": 2106900
    },
    {
      "epoch": 19.22585590189065,
      "grad_norm": 2.756542444229126,
      "learning_rate": 3.397845341509113e-05,
      "loss": 0.6737,
      "step": 2107000
    },
    {
      "epoch": 19.226768377253816,
      "grad_norm": 3.921541690826416,
      "learning_rate": 3.397769301895516e-05,
      "loss": 0.6773,
      "step": 2107100
    },
    {
      "epoch": 19.22768085261698,
      "grad_norm": 4.308002948760986,
      "learning_rate": 3.397693262281918e-05,
      "loss": 0.6787,
      "step": 2107200
    },
    {
      "epoch": 19.228593327980146,
      "grad_norm": 3.8636045455932617,
      "learning_rate": 3.397617222668322e-05,
      "loss": 0.6628,
      "step": 2107300
    },
    {
      "epoch": 19.229505803343308,
      "grad_norm": 4.034332275390625,
      "learning_rate": 3.397541183054724e-05,
      "loss": 0.6811,
      "step": 2107400
    },
    {
      "epoch": 19.230418278706473,
      "grad_norm": 3.4550023078918457,
      "learning_rate": 3.397465143441127e-05,
      "loss": 0.6641,
      "step": 2107500
    },
    {
      "epoch": 19.23133075406964,
      "grad_norm": 4.600757598876953,
      "learning_rate": 3.3973891038275303e-05,
      "loss": 0.6586,
      "step": 2107600
    },
    {
      "epoch": 19.232243229432804,
      "grad_norm": 4.64818000793457,
      "learning_rate": 3.397313064213933e-05,
      "loss": 0.6571,
      "step": 2107700
    },
    {
      "epoch": 19.23315570479597,
      "grad_norm": 4.0779876708984375,
      "learning_rate": 3.397237024600336e-05,
      "loss": 0.6553,
      "step": 2107800
    },
    {
      "epoch": 19.234068180159134,
      "grad_norm": 2.208818197250366,
      "learning_rate": 3.397160984986739e-05,
      "loss": 0.6669,
      "step": 2107900
    },
    {
      "epoch": 19.2349806555223,
      "grad_norm": 3.262033224105835,
      "learning_rate": 3.397084945373142e-05,
      "loss": 0.6309,
      "step": 2108000
    },
    {
      "epoch": 19.235893130885465,
      "grad_norm": 2.7936599254608154,
      "learning_rate": 3.397008905759545e-05,
      "loss": 0.6362,
      "step": 2108100
    },
    {
      "epoch": 19.23680560624863,
      "grad_norm": 4.160392761230469,
      "learning_rate": 3.396932866145948e-05,
      "loss": 0.6447,
      "step": 2108200
    },
    {
      "epoch": 19.237718081611796,
      "grad_norm": 3.6456685066223145,
      "learning_rate": 3.39685682653235e-05,
      "loss": 0.6825,
      "step": 2108300
    },
    {
      "epoch": 19.23863055697496,
      "grad_norm": 3.9967310428619385,
      "learning_rate": 3.396780786918754e-05,
      "loss": 0.6656,
      "step": 2108400
    },
    {
      "epoch": 19.239543032338126,
      "grad_norm": 3.831047296524048,
      "learning_rate": 3.396704747305156e-05,
      "loss": 0.6637,
      "step": 2108500
    },
    {
      "epoch": 19.24045550770129,
      "grad_norm": 3.7385542392730713,
      "learning_rate": 3.396628707691559e-05,
      "loss": 0.6268,
      "step": 2108600
    },
    {
      "epoch": 19.241367983064457,
      "grad_norm": 3.842899799346924,
      "learning_rate": 3.396552668077962e-05,
      "loss": 0.6688,
      "step": 2108700
    },
    {
      "epoch": 19.242280458427622,
      "grad_norm": 2.8836562633514404,
      "learning_rate": 3.396476628464365e-05,
      "loss": 0.6653,
      "step": 2108800
    },
    {
      "epoch": 19.243192933790787,
      "grad_norm": 3.747668504714966,
      "learning_rate": 3.3964005888507674e-05,
      "loss": 0.6326,
      "step": 2108900
    },
    {
      "epoch": 19.244105409153953,
      "grad_norm": 3.7200188636779785,
      "learning_rate": 3.396324549237171e-05,
      "loss": 0.6515,
      "step": 2109000
    },
    {
      "epoch": 19.245017884517118,
      "grad_norm": 3.812556505203247,
      "learning_rate": 3.3962485096235734e-05,
      "loss": 0.6202,
      "step": 2109100
    },
    {
      "epoch": 19.245930359880283,
      "grad_norm": 3.744786024093628,
      "learning_rate": 3.3961724700099764e-05,
      "loss": 0.6806,
      "step": 2109200
    },
    {
      "epoch": 19.24684283524345,
      "grad_norm": 5.085112571716309,
      "learning_rate": 3.3960964303963794e-05,
      "loss": 0.6721,
      "step": 2109300
    },
    {
      "epoch": 19.247755310606614,
      "grad_norm": 4.613269329071045,
      "learning_rate": 3.3960203907827824e-05,
      "loss": 0.6686,
      "step": 2109400
    },
    {
      "epoch": 19.24866778596978,
      "grad_norm": 4.174365520477295,
      "learning_rate": 3.3959443511691854e-05,
      "loss": 0.6639,
      "step": 2109500
    },
    {
      "epoch": 19.249580261332945,
      "grad_norm": 3.946136951446533,
      "learning_rate": 3.3958683115555885e-05,
      "loss": 0.6685,
      "step": 2109600
    },
    {
      "epoch": 19.25049273669611,
      "grad_norm": 4.340864181518555,
      "learning_rate": 3.395792271941991e-05,
      "loss": 0.6486,
      "step": 2109700
    },
    {
      "epoch": 19.251405212059275,
      "grad_norm": 4.3752760887146,
      "learning_rate": 3.3957162323283945e-05,
      "loss": 0.6827,
      "step": 2109800
    },
    {
      "epoch": 19.25231768742244,
      "grad_norm": 2.252378463745117,
      "learning_rate": 3.395640192714797e-05,
      "loss": 0.6626,
      "step": 2109900
    },
    {
      "epoch": 19.253230162785606,
      "grad_norm": 1.7736374139785767,
      "learning_rate": 3.3955641531012e-05,
      "loss": 0.638,
      "step": 2110000
    },
    {
      "epoch": 19.25414263814877,
      "grad_norm": 3.619262933731079,
      "learning_rate": 3.395488113487603e-05,
      "loss": 0.6566,
      "step": 2110100
    },
    {
      "epoch": 19.255055113511936,
      "grad_norm": 3.3086555004119873,
      "learning_rate": 3.395412073874006e-05,
      "loss": 0.6783,
      "step": 2110200
    },
    {
      "epoch": 19.2559675888751,
      "grad_norm": 4.098377704620361,
      "learning_rate": 3.395336034260408e-05,
      "loss": 0.6509,
      "step": 2110300
    },
    {
      "epoch": 19.256880064238267,
      "grad_norm": 3.5268712043762207,
      "learning_rate": 3.395259994646811e-05,
      "loss": 0.6718,
      "step": 2110400
    },
    {
      "epoch": 19.257792539601432,
      "grad_norm": 3.182358503341675,
      "learning_rate": 3.395183955033214e-05,
      "loss": 0.6567,
      "step": 2110500
    },
    {
      "epoch": 19.258705014964598,
      "grad_norm": 4.198294639587402,
      "learning_rate": 3.395107915419617e-05,
      "loss": 0.653,
      "step": 2110600
    },
    {
      "epoch": 19.259617490327763,
      "grad_norm": 4.40242338180542,
      "learning_rate": 3.39503187580602e-05,
      "loss": 0.6954,
      "step": 2110700
    },
    {
      "epoch": 19.260529965690928,
      "grad_norm": 4.209537029266357,
      "learning_rate": 3.3949558361924225e-05,
      "loss": 0.6669,
      "step": 2110800
    },
    {
      "epoch": 19.26144244105409,
      "grad_norm": 3.7054758071899414,
      "learning_rate": 3.394879796578826e-05,
      "loss": 0.6798,
      "step": 2110900
    },
    {
      "epoch": 19.262354916417255,
      "grad_norm": 3.60294508934021,
      "learning_rate": 3.3948037569652285e-05,
      "loss": 0.6441,
      "step": 2111000
    },
    {
      "epoch": 19.26326739178042,
      "grad_norm": 3.907418727874756,
      "learning_rate": 3.3947277173516315e-05,
      "loss": 0.6668,
      "step": 2111100
    },
    {
      "epoch": 19.264179867143586,
      "grad_norm": 3.897167682647705,
      "learning_rate": 3.3946516777380345e-05,
      "loss": 0.6754,
      "step": 2111200
    },
    {
      "epoch": 19.26509234250675,
      "grad_norm": 3.8529863357543945,
      "learning_rate": 3.3945756381244375e-05,
      "loss": 0.6632,
      "step": 2111300
    },
    {
      "epoch": 19.266004817869916,
      "grad_norm": 3.363830804824829,
      "learning_rate": 3.3944995985108405e-05,
      "loss": 0.6429,
      "step": 2111400
    },
    {
      "epoch": 19.26691729323308,
      "grad_norm": 4.372294902801514,
      "learning_rate": 3.3944235588972435e-05,
      "loss": 0.6723,
      "step": 2111500
    },
    {
      "epoch": 19.267829768596247,
      "grad_norm": 3.762112855911255,
      "learning_rate": 3.394347519283646e-05,
      "loss": 0.6244,
      "step": 2111600
    },
    {
      "epoch": 19.268742243959412,
      "grad_norm": 3.97509503364563,
      "learning_rate": 3.394271479670049e-05,
      "loss": 0.6824,
      "step": 2111700
    },
    {
      "epoch": 19.269654719322578,
      "grad_norm": 3.951353073120117,
      "learning_rate": 3.394195440056452e-05,
      "loss": 0.6293,
      "step": 2111800
    },
    {
      "epoch": 19.270567194685743,
      "grad_norm": 3.614811420440674,
      "learning_rate": 3.394119400442855e-05,
      "loss": 0.6626,
      "step": 2111900
    },
    {
      "epoch": 19.271479670048908,
      "grad_norm": 4.361310005187988,
      "learning_rate": 3.394043360829258e-05,
      "loss": 0.6581,
      "step": 2112000
    },
    {
      "epoch": 19.272392145412073,
      "grad_norm": 3.4229631423950195,
      "learning_rate": 3.393967321215661e-05,
      "loss": 0.6321,
      "step": 2112100
    },
    {
      "epoch": 19.27330462077524,
      "grad_norm": 5.306874752044678,
      "learning_rate": 3.393891281602063e-05,
      "loss": 0.6694,
      "step": 2112200
    },
    {
      "epoch": 19.274217096138404,
      "grad_norm": 3.2688210010528564,
      "learning_rate": 3.393815241988467e-05,
      "loss": 0.6653,
      "step": 2112300
    },
    {
      "epoch": 19.27512957150157,
      "grad_norm": 3.784332752227783,
      "learning_rate": 3.393739202374869e-05,
      "loss": 0.7044,
      "step": 2112400
    },
    {
      "epoch": 19.276042046864735,
      "grad_norm": 5.166625022888184,
      "learning_rate": 3.393663162761272e-05,
      "loss": 0.6424,
      "step": 2112500
    },
    {
      "epoch": 19.2769545222279,
      "grad_norm": 3.954097270965576,
      "learning_rate": 3.393587123147675e-05,
      "loss": 0.6296,
      "step": 2112600
    },
    {
      "epoch": 19.277866997591065,
      "grad_norm": 3.5998711585998535,
      "learning_rate": 3.393511083534078e-05,
      "loss": 0.6486,
      "step": 2112700
    },
    {
      "epoch": 19.27877947295423,
      "grad_norm": 3.200540781021118,
      "learning_rate": 3.393435043920481e-05,
      "loss": 0.6733,
      "step": 2112800
    },
    {
      "epoch": 19.279691948317396,
      "grad_norm": 4.602436542510986,
      "learning_rate": 3.393359004306884e-05,
      "loss": 0.6423,
      "step": 2112900
    },
    {
      "epoch": 19.28060442368056,
      "grad_norm": 3.0596776008605957,
      "learning_rate": 3.3932829646932866e-05,
      "loss": 0.6829,
      "step": 2113000
    },
    {
      "epoch": 19.281516899043726,
      "grad_norm": 3.212008237838745,
      "learning_rate": 3.3932069250796896e-05,
      "loss": 0.6287,
      "step": 2113100
    },
    {
      "epoch": 19.28242937440689,
      "grad_norm": 3.7936007976531982,
      "learning_rate": 3.3931308854660926e-05,
      "loss": 0.625,
      "step": 2113200
    },
    {
      "epoch": 19.283341849770057,
      "grad_norm": 4.276034832000732,
      "learning_rate": 3.393054845852495e-05,
      "loss": 0.6668,
      "step": 2113300
    },
    {
      "epoch": 19.284254325133222,
      "grad_norm": 3.010453701019287,
      "learning_rate": 3.3929788062388986e-05,
      "loss": 0.6795,
      "step": 2113400
    },
    {
      "epoch": 19.285166800496388,
      "grad_norm": 3.6090447902679443,
      "learning_rate": 3.392902766625301e-05,
      "loss": 0.6788,
      "step": 2113500
    },
    {
      "epoch": 19.286079275859553,
      "grad_norm": 3.8281822204589844,
      "learning_rate": 3.392826727011704e-05,
      "loss": 0.6568,
      "step": 2113600
    },
    {
      "epoch": 19.28699175122272,
      "grad_norm": 4.390522480010986,
      "learning_rate": 3.392750687398107e-05,
      "loss": 0.7033,
      "step": 2113700
    },
    {
      "epoch": 19.287904226585884,
      "grad_norm": 3.9369726181030273,
      "learning_rate": 3.39267464778451e-05,
      "loss": 0.666,
      "step": 2113800
    },
    {
      "epoch": 19.28881670194905,
      "grad_norm": 4.200992107391357,
      "learning_rate": 3.392598608170913e-05,
      "loss": 0.6552,
      "step": 2113900
    },
    {
      "epoch": 19.289729177312214,
      "grad_norm": 3.443690538406372,
      "learning_rate": 3.392522568557316e-05,
      "loss": 0.6834,
      "step": 2114000
    },
    {
      "epoch": 19.29064165267538,
      "grad_norm": 4.07524299621582,
      "learning_rate": 3.392446528943718e-05,
      "loss": 0.6765,
      "step": 2114100
    },
    {
      "epoch": 19.29155412803854,
      "grad_norm": 2.8159825801849365,
      "learning_rate": 3.392370489330122e-05,
      "loss": 0.6368,
      "step": 2114200
    },
    {
      "epoch": 19.292466603401706,
      "grad_norm": 4.496979236602783,
      "learning_rate": 3.3922944497165243e-05,
      "loss": 0.6509,
      "step": 2114300
    },
    {
      "epoch": 19.293379078764872,
      "grad_norm": 4.721644878387451,
      "learning_rate": 3.3922184101029274e-05,
      "loss": 0.6421,
      "step": 2114400
    },
    {
      "epoch": 19.294291554128037,
      "grad_norm": 4.647151947021484,
      "learning_rate": 3.3921423704893304e-05,
      "loss": 0.6636,
      "step": 2114500
    },
    {
      "epoch": 19.295204029491202,
      "grad_norm": 3.2267768383026123,
      "learning_rate": 3.3920663308757334e-05,
      "loss": 0.6567,
      "step": 2114600
    },
    {
      "epoch": 19.296116504854368,
      "grad_norm": 4.00879430770874,
      "learning_rate": 3.391990291262136e-05,
      "loss": 0.6549,
      "step": 2114700
    },
    {
      "epoch": 19.297028980217533,
      "grad_norm": 4.046448707580566,
      "learning_rate": 3.3919142516485394e-05,
      "loss": 0.6647,
      "step": 2114800
    },
    {
      "epoch": 19.2979414555807,
      "grad_norm": 3.448352813720703,
      "learning_rate": 3.391838212034942e-05,
      "loss": 0.6779,
      "step": 2114900
    },
    {
      "epoch": 19.298853930943864,
      "grad_norm": 2.882941246032715,
      "learning_rate": 3.391762172421345e-05,
      "loss": 0.6277,
      "step": 2115000
    },
    {
      "epoch": 19.29976640630703,
      "grad_norm": 3.583348512649536,
      "learning_rate": 3.391686132807748e-05,
      "loss": 0.6526,
      "step": 2115100
    },
    {
      "epoch": 19.300678881670194,
      "grad_norm": 4.056172847747803,
      "learning_rate": 3.391610093194151e-05,
      "loss": 0.6626,
      "step": 2115200
    },
    {
      "epoch": 19.30159135703336,
      "grad_norm": 3.7275173664093018,
      "learning_rate": 3.391534053580554e-05,
      "loss": 0.6627,
      "step": 2115300
    },
    {
      "epoch": 19.302503832396525,
      "grad_norm": 4.956900596618652,
      "learning_rate": 3.391458013966957e-05,
      "loss": 0.6337,
      "step": 2115400
    },
    {
      "epoch": 19.30341630775969,
      "grad_norm": 3.8604354858398438,
      "learning_rate": 3.391381974353359e-05,
      "loss": 0.6365,
      "step": 2115500
    },
    {
      "epoch": 19.304328783122855,
      "grad_norm": 4.760379314422607,
      "learning_rate": 3.391305934739763e-05,
      "loss": 0.6404,
      "step": 2115600
    },
    {
      "epoch": 19.30524125848602,
      "grad_norm": 4.598202228546143,
      "learning_rate": 3.391229895126165e-05,
      "loss": 0.684,
      "step": 2115700
    },
    {
      "epoch": 19.306153733849186,
      "grad_norm": 3.9394333362579346,
      "learning_rate": 3.391153855512568e-05,
      "loss": 0.6407,
      "step": 2115800
    },
    {
      "epoch": 19.30706620921235,
      "grad_norm": 3.3982672691345215,
      "learning_rate": 3.391077815898971e-05,
      "loss": 0.6345,
      "step": 2115900
    },
    {
      "epoch": 19.307978684575517,
      "grad_norm": 4.36496114730835,
      "learning_rate": 3.3910017762853734e-05,
      "loss": 0.6898,
      "step": 2116000
    },
    {
      "epoch": 19.308891159938682,
      "grad_norm": 4.395959377288818,
      "learning_rate": 3.3909257366717764e-05,
      "loss": 0.6547,
      "step": 2116100
    },
    {
      "epoch": 19.309803635301847,
      "grad_norm": 3.981799364089966,
      "learning_rate": 3.3908496970581794e-05,
      "loss": 0.6158,
      "step": 2116200
    },
    {
      "epoch": 19.310716110665012,
      "grad_norm": 4.588499069213867,
      "learning_rate": 3.3907736574445824e-05,
      "loss": 0.633,
      "step": 2116300
    },
    {
      "epoch": 19.311628586028178,
      "grad_norm": 3.4793784618377686,
      "learning_rate": 3.3906976178309855e-05,
      "loss": 0.6482,
      "step": 2116400
    },
    {
      "epoch": 19.312541061391343,
      "grad_norm": 3.9830784797668457,
      "learning_rate": 3.3906215782173885e-05,
      "loss": 0.6486,
      "step": 2116500
    },
    {
      "epoch": 19.31345353675451,
      "grad_norm": 3.4060873985290527,
      "learning_rate": 3.390545538603791e-05,
      "loss": 0.6978,
      "step": 2116600
    },
    {
      "epoch": 19.314366012117674,
      "grad_norm": 4.247495651245117,
      "learning_rate": 3.3904694989901945e-05,
      "loss": 0.638,
      "step": 2116700
    },
    {
      "epoch": 19.31527848748084,
      "grad_norm": 3.205550193786621,
      "learning_rate": 3.390393459376597e-05,
      "loss": 0.6204,
      "step": 2116800
    },
    {
      "epoch": 19.316190962844004,
      "grad_norm": 4.077789783477783,
      "learning_rate": 3.390317419763e-05,
      "loss": 0.6542,
      "step": 2116900
    },
    {
      "epoch": 19.31710343820717,
      "grad_norm": 4.2133049964904785,
      "learning_rate": 3.390241380149403e-05,
      "loss": 0.6556,
      "step": 2117000
    },
    {
      "epoch": 19.318015913570335,
      "grad_norm": 3.743834972381592,
      "learning_rate": 3.390165340535806e-05,
      "loss": 0.6627,
      "step": 2117100
    },
    {
      "epoch": 19.3189283889335,
      "grad_norm": 4.410275459289551,
      "learning_rate": 3.390089300922208e-05,
      "loss": 0.6395,
      "step": 2117200
    },
    {
      "epoch": 19.319840864296665,
      "grad_norm": 3.7720696926116943,
      "learning_rate": 3.390013261308612e-05,
      "loss": 0.6449,
      "step": 2117300
    },
    {
      "epoch": 19.32075333965983,
      "grad_norm": 4.005681037902832,
      "learning_rate": 3.389937221695014e-05,
      "loss": 0.6242,
      "step": 2117400
    },
    {
      "epoch": 19.321665815022996,
      "grad_norm": 3.790797710418701,
      "learning_rate": 3.389861182081417e-05,
      "loss": 0.681,
      "step": 2117500
    },
    {
      "epoch": 19.32257829038616,
      "grad_norm": 3.9553661346435547,
      "learning_rate": 3.38978514246782e-05,
      "loss": 0.6621,
      "step": 2117600
    },
    {
      "epoch": 19.323490765749323,
      "grad_norm": 3.951350212097168,
      "learning_rate": 3.389709102854223e-05,
      "loss": 0.6373,
      "step": 2117700
    },
    {
      "epoch": 19.32440324111249,
      "grad_norm": 4.932285308837891,
      "learning_rate": 3.389633063240626e-05,
      "loss": 0.6393,
      "step": 2117800
    },
    {
      "epoch": 19.325315716475654,
      "grad_norm": 3.126798391342163,
      "learning_rate": 3.389557023627029e-05,
      "loss": 0.6449,
      "step": 2117900
    },
    {
      "epoch": 19.32622819183882,
      "grad_norm": 3.0236551761627197,
      "learning_rate": 3.3894809840134315e-05,
      "loss": 0.6882,
      "step": 2118000
    },
    {
      "epoch": 19.327140667201984,
      "grad_norm": 3.549107789993286,
      "learning_rate": 3.389404944399835e-05,
      "loss": 0.6692,
      "step": 2118100
    },
    {
      "epoch": 19.32805314256515,
      "grad_norm": 4.379692554473877,
      "learning_rate": 3.3893289047862375e-05,
      "loss": 0.6318,
      "step": 2118200
    },
    {
      "epoch": 19.328965617928315,
      "grad_norm": 4.604608058929443,
      "learning_rate": 3.3892528651726405e-05,
      "loss": 0.636,
      "step": 2118300
    },
    {
      "epoch": 19.32987809329148,
      "grad_norm": 4.081544399261475,
      "learning_rate": 3.3891768255590436e-05,
      "loss": 0.6597,
      "step": 2118400
    },
    {
      "epoch": 19.330790568654646,
      "grad_norm": 3.6107559204101562,
      "learning_rate": 3.3891007859454466e-05,
      "loss": 0.6217,
      "step": 2118500
    },
    {
      "epoch": 19.33170304401781,
      "grad_norm": 3.5404212474823,
      "learning_rate": 3.389024746331849e-05,
      "loss": 0.6454,
      "step": 2118600
    },
    {
      "epoch": 19.332615519380976,
      "grad_norm": 3.8656814098358154,
      "learning_rate": 3.3889487067182526e-05,
      "loss": 0.6315,
      "step": 2118700
    },
    {
      "epoch": 19.33352799474414,
      "grad_norm": 3.006528854370117,
      "learning_rate": 3.388872667104655e-05,
      "loss": 0.6521,
      "step": 2118800
    },
    {
      "epoch": 19.334440470107307,
      "grad_norm": 3.7887582778930664,
      "learning_rate": 3.388796627491058e-05,
      "loss": 0.6648,
      "step": 2118900
    },
    {
      "epoch": 19.335352945470472,
      "grad_norm": 3.596726179122925,
      "learning_rate": 3.388720587877461e-05,
      "loss": 0.6673,
      "step": 2119000
    },
    {
      "epoch": 19.336265420833637,
      "grad_norm": 3.9627249240875244,
      "learning_rate": 3.388644548263863e-05,
      "loss": 0.6564,
      "step": 2119100
    },
    {
      "epoch": 19.337177896196803,
      "grad_norm": 3.528635025024414,
      "learning_rate": 3.388568508650267e-05,
      "loss": 0.625,
      "step": 2119200
    },
    {
      "epoch": 19.338090371559968,
      "grad_norm": 3.928772211074829,
      "learning_rate": 3.388492469036669e-05,
      "loss": 0.6641,
      "step": 2119300
    },
    {
      "epoch": 19.339002846923133,
      "grad_norm": 3.889328956604004,
      "learning_rate": 3.388416429423072e-05,
      "loss": 0.6374,
      "step": 2119400
    },
    {
      "epoch": 19.3399153222863,
      "grad_norm": 3.6856801509857178,
      "learning_rate": 3.388340389809475e-05,
      "loss": 0.651,
      "step": 2119500
    },
    {
      "epoch": 19.340827797649464,
      "grad_norm": 4.514466762542725,
      "learning_rate": 3.388264350195878e-05,
      "loss": 0.6232,
      "step": 2119600
    },
    {
      "epoch": 19.34174027301263,
      "grad_norm": 3.668977737426758,
      "learning_rate": 3.3881883105822806e-05,
      "loss": 0.6597,
      "step": 2119700
    },
    {
      "epoch": 19.342652748375794,
      "grad_norm": 4.2043375968933105,
      "learning_rate": 3.388112270968684e-05,
      "loss": 0.6713,
      "step": 2119800
    },
    {
      "epoch": 19.34356522373896,
      "grad_norm": 4.390701770782471,
      "learning_rate": 3.3880362313550866e-05,
      "loss": 0.6948,
      "step": 2119900
    },
    {
      "epoch": 19.344477699102125,
      "grad_norm": 3.0054144859313965,
      "learning_rate": 3.3879601917414896e-05,
      "loss": 0.653,
      "step": 2120000
    },
    {
      "epoch": 19.34539017446529,
      "grad_norm": 4.750865459442139,
      "learning_rate": 3.3878841521278926e-05,
      "loss": 0.6533,
      "step": 2120100
    },
    {
      "epoch": 19.346302649828456,
      "grad_norm": 3.5471417903900146,
      "learning_rate": 3.3878081125142956e-05,
      "loss": 0.6246,
      "step": 2120200
    },
    {
      "epoch": 19.34721512519162,
      "grad_norm": 4.0079827308654785,
      "learning_rate": 3.3877320729006986e-05,
      "loss": 0.6315,
      "step": 2120300
    },
    {
      "epoch": 19.348127600554786,
      "grad_norm": 3.333714723587036,
      "learning_rate": 3.3876560332871017e-05,
      "loss": 0.6259,
      "step": 2120400
    },
    {
      "epoch": 19.34904007591795,
      "grad_norm": 3.43017315864563,
      "learning_rate": 3.387579993673504e-05,
      "loss": 0.6558,
      "step": 2120500
    },
    {
      "epoch": 19.349952551281117,
      "grad_norm": 2.8814492225646973,
      "learning_rate": 3.387503954059908e-05,
      "loss": 0.6595,
      "step": 2120600
    },
    {
      "epoch": 19.350865026644282,
      "grad_norm": 2.977891445159912,
      "learning_rate": 3.38742791444631e-05,
      "loss": 0.6353,
      "step": 2120700
    },
    {
      "epoch": 19.351777502007447,
      "grad_norm": 4.6641740798950195,
      "learning_rate": 3.387351874832713e-05,
      "loss": 0.6784,
      "step": 2120800
    },
    {
      "epoch": 19.352689977370613,
      "grad_norm": 3.967510223388672,
      "learning_rate": 3.387275835219116e-05,
      "loss": 0.6754,
      "step": 2120900
    },
    {
      "epoch": 19.353602452733774,
      "grad_norm": 3.554837942123413,
      "learning_rate": 3.387199795605519e-05,
      "loss": 0.6757,
      "step": 2121000
    },
    {
      "epoch": 19.35451492809694,
      "grad_norm": 3.553114414215088,
      "learning_rate": 3.3871237559919213e-05,
      "loss": 0.6854,
      "step": 2121100
    },
    {
      "epoch": 19.355427403460105,
      "grad_norm": 3.0352518558502197,
      "learning_rate": 3.387047716378325e-05,
      "loss": 0.6862,
      "step": 2121200
    },
    {
      "epoch": 19.35633987882327,
      "grad_norm": 3.8185579776763916,
      "learning_rate": 3.3869716767647274e-05,
      "loss": 0.6682,
      "step": 2121300
    },
    {
      "epoch": 19.357252354186436,
      "grad_norm": 3.6183886528015137,
      "learning_rate": 3.3868956371511304e-05,
      "loss": 0.6393,
      "step": 2121400
    },
    {
      "epoch": 19.3581648295496,
      "grad_norm": 3.753258228302002,
      "learning_rate": 3.3868195975375334e-05,
      "loss": 0.6511,
      "step": 2121500
    },
    {
      "epoch": 19.359077304912766,
      "grad_norm": 3.594344139099121,
      "learning_rate": 3.386743557923936e-05,
      "loss": 0.6975,
      "step": 2121600
    },
    {
      "epoch": 19.35998978027593,
      "grad_norm": 5.141420364379883,
      "learning_rate": 3.3866675183103394e-05,
      "loss": 0.6641,
      "step": 2121700
    },
    {
      "epoch": 19.360902255639097,
      "grad_norm": 3.56526255607605,
      "learning_rate": 3.386591478696742e-05,
      "loss": 0.6424,
      "step": 2121800
    },
    {
      "epoch": 19.361814731002262,
      "grad_norm": 3.534473419189453,
      "learning_rate": 3.386515439083145e-05,
      "loss": 0.6407,
      "step": 2121900
    },
    {
      "epoch": 19.362727206365427,
      "grad_norm": 4.451866626739502,
      "learning_rate": 3.386439399469548e-05,
      "loss": 0.6869,
      "step": 2122000
    },
    {
      "epoch": 19.363639681728593,
      "grad_norm": 3.6127917766571045,
      "learning_rate": 3.386363359855951e-05,
      "loss": 0.656,
      "step": 2122100
    },
    {
      "epoch": 19.364552157091758,
      "grad_norm": 4.217395782470703,
      "learning_rate": 3.386287320242353e-05,
      "loss": 0.653,
      "step": 2122200
    },
    {
      "epoch": 19.365464632454923,
      "grad_norm": 5.166952133178711,
      "learning_rate": 3.386211280628757e-05,
      "loss": 0.6545,
      "step": 2122300
    },
    {
      "epoch": 19.36637710781809,
      "grad_norm": 3.6295385360717773,
      "learning_rate": 3.386135241015159e-05,
      "loss": 0.6523,
      "step": 2122400
    },
    {
      "epoch": 19.367289583181254,
      "grad_norm": 3.790522813796997,
      "learning_rate": 3.386059201401562e-05,
      "loss": 0.6462,
      "step": 2122500
    },
    {
      "epoch": 19.36820205854442,
      "grad_norm": 4.816112995147705,
      "learning_rate": 3.385983161787965e-05,
      "loss": 0.6751,
      "step": 2122600
    },
    {
      "epoch": 19.369114533907585,
      "grad_norm": 2.7339017391204834,
      "learning_rate": 3.385907122174368e-05,
      "loss": 0.6483,
      "step": 2122700
    },
    {
      "epoch": 19.37002700927075,
      "grad_norm": 3.4801690578460693,
      "learning_rate": 3.385831082560771e-05,
      "loss": 0.6404,
      "step": 2122800
    },
    {
      "epoch": 19.370939484633915,
      "grad_norm": 3.84966778755188,
      "learning_rate": 3.385755042947174e-05,
      "loss": 0.6674,
      "step": 2122900
    },
    {
      "epoch": 19.37185195999708,
      "grad_norm": 3.135371685028076,
      "learning_rate": 3.3856790033335764e-05,
      "loss": 0.6176,
      "step": 2123000
    },
    {
      "epoch": 19.372764435360246,
      "grad_norm": 4.2064361572265625,
      "learning_rate": 3.38560296371998e-05,
      "loss": 0.6755,
      "step": 2123100
    },
    {
      "epoch": 19.37367691072341,
      "grad_norm": 3.3733081817626953,
      "learning_rate": 3.3855269241063825e-05,
      "loss": 0.6781,
      "step": 2123200
    },
    {
      "epoch": 19.374589386086576,
      "grad_norm": 3.4104089736938477,
      "learning_rate": 3.3854508844927855e-05,
      "loss": 0.6279,
      "step": 2123300
    },
    {
      "epoch": 19.37550186144974,
      "grad_norm": 3.9500558376312256,
      "learning_rate": 3.3853748448791885e-05,
      "loss": 0.6777,
      "step": 2123400
    },
    {
      "epoch": 19.376414336812907,
      "grad_norm": 3.6845827102661133,
      "learning_rate": 3.3852988052655915e-05,
      "loss": 0.6643,
      "step": 2123500
    },
    {
      "epoch": 19.377326812176072,
      "grad_norm": 4.083874702453613,
      "learning_rate": 3.385222765651994e-05,
      "loss": 0.6589,
      "step": 2123600
    },
    {
      "epoch": 19.378239287539238,
      "grad_norm": 3.8517987728118896,
      "learning_rate": 3.3851467260383975e-05,
      "loss": 0.631,
      "step": 2123700
    },
    {
      "epoch": 19.379151762902403,
      "grad_norm": 4.226034164428711,
      "learning_rate": 3.3850706864248e-05,
      "loss": 0.6652,
      "step": 2123800
    },
    {
      "epoch": 19.380064238265568,
      "grad_norm": 4.9750776290893555,
      "learning_rate": 3.384994646811203e-05,
      "loss": 0.631,
      "step": 2123900
    },
    {
      "epoch": 19.380976713628733,
      "grad_norm": 4.033572196960449,
      "learning_rate": 3.384918607197606e-05,
      "loss": 0.703,
      "step": 2124000
    },
    {
      "epoch": 19.3818891889919,
      "grad_norm": 4.93384313583374,
      "learning_rate": 3.384842567584009e-05,
      "loss": 0.6841,
      "step": 2124100
    },
    {
      "epoch": 19.382801664355064,
      "grad_norm": 4.303603172302246,
      "learning_rate": 3.384766527970412e-05,
      "loss": 0.6751,
      "step": 2124200
    },
    {
      "epoch": 19.38371413971823,
      "grad_norm": 3.2767865657806396,
      "learning_rate": 3.384690488356815e-05,
      "loss": 0.6612,
      "step": 2124300
    },
    {
      "epoch": 19.38462661508139,
      "grad_norm": 3.2432332038879395,
      "learning_rate": 3.384614448743217e-05,
      "loss": 0.6643,
      "step": 2124400
    },
    {
      "epoch": 19.385539090444556,
      "grad_norm": 3.808523416519165,
      "learning_rate": 3.38453840912962e-05,
      "loss": 0.6456,
      "step": 2124500
    },
    {
      "epoch": 19.38645156580772,
      "grad_norm": 4.029568195343018,
      "learning_rate": 3.384462369516023e-05,
      "loss": 0.67,
      "step": 2124600
    },
    {
      "epoch": 19.387364041170887,
      "grad_norm": 2.3988301753997803,
      "learning_rate": 3.384386329902426e-05,
      "loss": 0.6915,
      "step": 2124700
    },
    {
      "epoch": 19.388276516534052,
      "grad_norm": 3.3554561138153076,
      "learning_rate": 3.384310290288829e-05,
      "loss": 0.6702,
      "step": 2124800
    },
    {
      "epoch": 19.389188991897218,
      "grad_norm": 4.047280788421631,
      "learning_rate": 3.3842342506752315e-05,
      "loss": 0.6555,
      "step": 2124900
    },
    {
      "epoch": 19.390101467260383,
      "grad_norm": 4.198117733001709,
      "learning_rate": 3.384158211061635e-05,
      "loss": 0.6513,
      "step": 2125000
    },
    {
      "epoch": 19.391013942623548,
      "grad_norm": 4.266197681427002,
      "learning_rate": 3.3840821714480376e-05,
      "loss": 0.6907,
      "step": 2125100
    },
    {
      "epoch": 19.391926417986713,
      "grad_norm": 2.9903013706207275,
      "learning_rate": 3.3840061318344406e-05,
      "loss": 0.648,
      "step": 2125200
    },
    {
      "epoch": 19.39283889334988,
      "grad_norm": 3.159074544906616,
      "learning_rate": 3.3839300922208436e-05,
      "loss": 0.6642,
      "step": 2125300
    },
    {
      "epoch": 19.393751368713044,
      "grad_norm": 4.397747993469238,
      "learning_rate": 3.3838540526072466e-05,
      "loss": 0.6818,
      "step": 2125400
    },
    {
      "epoch": 19.39466384407621,
      "grad_norm": 4.1068854331970215,
      "learning_rate": 3.383778012993649e-05,
      "loss": 0.6855,
      "step": 2125500
    },
    {
      "epoch": 19.395576319439375,
      "grad_norm": 4.350022792816162,
      "learning_rate": 3.3837019733800526e-05,
      "loss": 0.6763,
      "step": 2125600
    },
    {
      "epoch": 19.39648879480254,
      "grad_norm": 4.075577735900879,
      "learning_rate": 3.383625933766455e-05,
      "loss": 0.6744,
      "step": 2125700
    },
    {
      "epoch": 19.397401270165705,
      "grad_norm": 4.4494709968566895,
      "learning_rate": 3.383549894152858e-05,
      "loss": 0.6257,
      "step": 2125800
    },
    {
      "epoch": 19.39831374552887,
      "grad_norm": 3.9986417293548584,
      "learning_rate": 3.383473854539261e-05,
      "loss": 0.6718,
      "step": 2125900
    },
    {
      "epoch": 19.399226220892036,
      "grad_norm": 4.289795875549316,
      "learning_rate": 3.383397814925664e-05,
      "loss": 0.6458,
      "step": 2126000
    },
    {
      "epoch": 19.4001386962552,
      "grad_norm": 4.867717742919922,
      "learning_rate": 3.383321775312067e-05,
      "loss": 0.634,
      "step": 2126100
    },
    {
      "epoch": 19.401051171618366,
      "grad_norm": 3.7068543434143066,
      "learning_rate": 3.38324573569847e-05,
      "loss": 0.6411,
      "step": 2126200
    },
    {
      "epoch": 19.40196364698153,
      "grad_norm": 4.071134567260742,
      "learning_rate": 3.383169696084872e-05,
      "loss": 0.7015,
      "step": 2126300
    },
    {
      "epoch": 19.402876122344697,
      "grad_norm": 3.864932060241699,
      "learning_rate": 3.383093656471276e-05,
      "loss": 0.6252,
      "step": 2126400
    },
    {
      "epoch": 19.403788597707862,
      "grad_norm": 4.253486633300781,
      "learning_rate": 3.383017616857678e-05,
      "loss": 0.6321,
      "step": 2126500
    },
    {
      "epoch": 19.404701073071028,
      "grad_norm": 3.2019567489624023,
      "learning_rate": 3.382941577244081e-05,
      "loss": 0.6401,
      "step": 2126600
    },
    {
      "epoch": 19.405613548434193,
      "grad_norm": 4.524789810180664,
      "learning_rate": 3.382865537630484e-05,
      "loss": 0.6339,
      "step": 2126700
    },
    {
      "epoch": 19.40652602379736,
      "grad_norm": 3.817732334136963,
      "learning_rate": 3.382789498016887e-05,
      "loss": 0.6531,
      "step": 2126800
    },
    {
      "epoch": 19.407438499160524,
      "grad_norm": 3.4264323711395264,
      "learning_rate": 3.3827134584032896e-05,
      "loss": 0.6388,
      "step": 2126900
    },
    {
      "epoch": 19.40835097452369,
      "grad_norm": 4.2973127365112305,
      "learning_rate": 3.382637418789693e-05,
      "loss": 0.6857,
      "step": 2127000
    },
    {
      "epoch": 19.409263449886854,
      "grad_norm": 3.1140503883361816,
      "learning_rate": 3.3825613791760957e-05,
      "loss": 0.629,
      "step": 2127100
    },
    {
      "epoch": 19.41017592525002,
      "grad_norm": 4.600426197052002,
      "learning_rate": 3.3824853395624987e-05,
      "loss": 0.6592,
      "step": 2127200
    },
    {
      "epoch": 19.411088400613185,
      "grad_norm": 3.311365842819214,
      "learning_rate": 3.382409299948902e-05,
      "loss": 0.6529,
      "step": 2127300
    },
    {
      "epoch": 19.41200087597635,
      "grad_norm": 4.5866289138793945,
      "learning_rate": 3.382333260335304e-05,
      "loss": 0.6783,
      "step": 2127400
    },
    {
      "epoch": 19.412913351339515,
      "grad_norm": 4.00975227355957,
      "learning_rate": 3.382257220721708e-05,
      "loss": 0.6325,
      "step": 2127500
    },
    {
      "epoch": 19.41382582670268,
      "grad_norm": 4.522915363311768,
      "learning_rate": 3.38218118110811e-05,
      "loss": 0.6609,
      "step": 2127600
    },
    {
      "epoch": 19.414738302065846,
      "grad_norm": 3.7113747596740723,
      "learning_rate": 3.382105141494513e-05,
      "loss": 0.6675,
      "step": 2127700
    },
    {
      "epoch": 19.415650777429008,
      "grad_norm": 3.5401430130004883,
      "learning_rate": 3.382029101880916e-05,
      "loss": 0.6711,
      "step": 2127800
    },
    {
      "epoch": 19.416563252792173,
      "grad_norm": 3.4206418991088867,
      "learning_rate": 3.381953062267319e-05,
      "loss": 0.6531,
      "step": 2127900
    },
    {
      "epoch": 19.41747572815534,
      "grad_norm": 3.7957448959350586,
      "learning_rate": 3.3818770226537214e-05,
      "loss": 0.6801,
      "step": 2128000
    },
    {
      "epoch": 19.418388203518504,
      "grad_norm": 4.593735694885254,
      "learning_rate": 3.381800983040125e-05,
      "loss": 0.6453,
      "step": 2128100
    },
    {
      "epoch": 19.41930067888167,
      "grad_norm": 4.353493690490723,
      "learning_rate": 3.3817249434265274e-05,
      "loss": 0.6583,
      "step": 2128200
    },
    {
      "epoch": 19.420213154244834,
      "grad_norm": 4.284522533416748,
      "learning_rate": 3.3816489038129304e-05,
      "loss": 0.646,
      "step": 2128300
    },
    {
      "epoch": 19.421125629608,
      "grad_norm": 3.9294581413269043,
      "learning_rate": 3.3815728641993334e-05,
      "loss": 0.6651,
      "step": 2128400
    },
    {
      "epoch": 19.422038104971165,
      "grad_norm": 2.793537139892578,
      "learning_rate": 3.3814968245857364e-05,
      "loss": 0.6304,
      "step": 2128500
    },
    {
      "epoch": 19.42295058033433,
      "grad_norm": 3.4808900356292725,
      "learning_rate": 3.3814207849721394e-05,
      "loss": 0.6838,
      "step": 2128600
    },
    {
      "epoch": 19.423863055697495,
      "grad_norm": 3.4406816959381104,
      "learning_rate": 3.3813447453585424e-05,
      "loss": 0.6479,
      "step": 2128700
    },
    {
      "epoch": 19.42477553106066,
      "grad_norm": 3.2788898944854736,
      "learning_rate": 3.381268705744945e-05,
      "loss": 0.6834,
      "step": 2128800
    },
    {
      "epoch": 19.425688006423826,
      "grad_norm": 4.152876377105713,
      "learning_rate": 3.3811926661313484e-05,
      "loss": 0.6605,
      "step": 2128900
    },
    {
      "epoch": 19.42660048178699,
      "grad_norm": 4.109955310821533,
      "learning_rate": 3.381116626517751e-05,
      "loss": 0.7071,
      "step": 2129000
    },
    {
      "epoch": 19.427512957150157,
      "grad_norm": 4.298626899719238,
      "learning_rate": 3.381040586904154e-05,
      "loss": 0.6489,
      "step": 2129100
    },
    {
      "epoch": 19.428425432513322,
      "grad_norm": 3.568937301635742,
      "learning_rate": 3.380964547290557e-05,
      "loss": 0.6531,
      "step": 2129200
    },
    {
      "epoch": 19.429337907876487,
      "grad_norm": 3.169691801071167,
      "learning_rate": 3.38088850767696e-05,
      "loss": 0.6771,
      "step": 2129300
    },
    {
      "epoch": 19.430250383239652,
      "grad_norm": 3.145526170730591,
      "learning_rate": 3.380812468063362e-05,
      "loss": 0.6412,
      "step": 2129400
    },
    {
      "epoch": 19.431162858602818,
      "grad_norm": 4.399252891540527,
      "learning_rate": 3.380736428449766e-05,
      "loss": 0.6421,
      "step": 2129500
    },
    {
      "epoch": 19.432075333965983,
      "grad_norm": 3.0657095909118652,
      "learning_rate": 3.380660388836168e-05,
      "loss": 0.6646,
      "step": 2129600
    },
    {
      "epoch": 19.43298780932915,
      "grad_norm": 4.524474620819092,
      "learning_rate": 3.380584349222571e-05,
      "loss": 0.6591,
      "step": 2129700
    },
    {
      "epoch": 19.433900284692314,
      "grad_norm": 4.356208324432373,
      "learning_rate": 3.380508309608974e-05,
      "loss": 0.6359,
      "step": 2129800
    },
    {
      "epoch": 19.43481276005548,
      "grad_norm": 3.9404282569885254,
      "learning_rate": 3.380432269995377e-05,
      "loss": 0.6786,
      "step": 2129900
    },
    {
      "epoch": 19.435725235418644,
      "grad_norm": 4.23502779006958,
      "learning_rate": 3.38035623038178e-05,
      "loss": 0.6898,
      "step": 2130000
    },
    {
      "epoch": 19.43663771078181,
      "grad_norm": 4.137701988220215,
      "learning_rate": 3.380280190768183e-05,
      "loss": 0.6412,
      "step": 2130100
    },
    {
      "epoch": 19.437550186144975,
      "grad_norm": 3.4542417526245117,
      "learning_rate": 3.3802041511545855e-05,
      "loss": 0.6289,
      "step": 2130200
    },
    {
      "epoch": 19.43846266150814,
      "grad_norm": 3.0867817401885986,
      "learning_rate": 3.3801281115409885e-05,
      "loss": 0.6433,
      "step": 2130300
    },
    {
      "epoch": 19.439375136871305,
      "grad_norm": 4.092435359954834,
      "learning_rate": 3.3800520719273915e-05,
      "loss": 0.6736,
      "step": 2130400
    },
    {
      "epoch": 19.44028761223447,
      "grad_norm": 2.8622210025787354,
      "learning_rate": 3.379976032313794e-05,
      "loss": 0.672,
      "step": 2130500
    },
    {
      "epoch": 19.441200087597636,
      "grad_norm": 4.39905309677124,
      "learning_rate": 3.3798999927001975e-05,
      "loss": 0.6847,
      "step": 2130600
    },
    {
      "epoch": 19.4421125629608,
      "grad_norm": 4.00792932510376,
      "learning_rate": 3.3798239530866e-05,
      "loss": 0.6809,
      "step": 2130700
    },
    {
      "epoch": 19.443025038323967,
      "grad_norm": 4.371997356414795,
      "learning_rate": 3.379747913473003e-05,
      "loss": 0.6682,
      "step": 2130800
    },
    {
      "epoch": 19.443937513687132,
      "grad_norm": 4.008992671966553,
      "learning_rate": 3.379671873859406e-05,
      "loss": 0.6454,
      "step": 2130900
    },
    {
      "epoch": 19.444849989050297,
      "grad_norm": 2.559556007385254,
      "learning_rate": 3.379595834245809e-05,
      "loss": 0.6453,
      "step": 2131000
    },
    {
      "epoch": 19.445762464413463,
      "grad_norm": 3.135115385055542,
      "learning_rate": 3.379519794632212e-05,
      "loss": 0.6525,
      "step": 2131100
    },
    {
      "epoch": 19.446674939776624,
      "grad_norm": 2.846688985824585,
      "learning_rate": 3.379443755018615e-05,
      "loss": 0.6434,
      "step": 2131200
    },
    {
      "epoch": 19.44758741513979,
      "grad_norm": 3.2019622325897217,
      "learning_rate": 3.379367715405017e-05,
      "loss": 0.6683,
      "step": 2131300
    },
    {
      "epoch": 19.448499890502955,
      "grad_norm": 2.855910301208496,
      "learning_rate": 3.379291675791421e-05,
      "loss": 0.6522,
      "step": 2131400
    },
    {
      "epoch": 19.44941236586612,
      "grad_norm": 3.9542489051818848,
      "learning_rate": 3.379215636177823e-05,
      "loss": 0.6156,
      "step": 2131500
    },
    {
      "epoch": 19.450324841229286,
      "grad_norm": 2.819437026977539,
      "learning_rate": 3.379139596564226e-05,
      "loss": 0.6449,
      "step": 2131600
    },
    {
      "epoch": 19.45123731659245,
      "grad_norm": 3.7689268589019775,
      "learning_rate": 3.379063556950629e-05,
      "loss": 0.6966,
      "step": 2131700
    },
    {
      "epoch": 19.452149791955616,
      "grad_norm": 4.156976222991943,
      "learning_rate": 3.378987517337032e-05,
      "loss": 0.6452,
      "step": 2131800
    },
    {
      "epoch": 19.45306226731878,
      "grad_norm": 4.085134983062744,
      "learning_rate": 3.3789114777234346e-05,
      "loss": 0.6551,
      "step": 2131900
    },
    {
      "epoch": 19.453974742681947,
      "grad_norm": 4.049386024475098,
      "learning_rate": 3.378835438109838e-05,
      "loss": 0.6791,
      "step": 2132000
    },
    {
      "epoch": 19.454887218045112,
      "grad_norm": 3.8226985931396484,
      "learning_rate": 3.3787593984962406e-05,
      "loss": 0.6596,
      "step": 2132100
    },
    {
      "epoch": 19.455799693408277,
      "grad_norm": 3.5088953971862793,
      "learning_rate": 3.3786833588826436e-05,
      "loss": 0.6674,
      "step": 2132200
    },
    {
      "epoch": 19.456712168771443,
      "grad_norm": 4.307835578918457,
      "learning_rate": 3.3786073192690466e-05,
      "loss": 0.6747,
      "step": 2132300
    },
    {
      "epoch": 19.457624644134608,
      "grad_norm": 4.151526927947998,
      "learning_rate": 3.3785312796554496e-05,
      "loss": 0.6473,
      "step": 2132400
    },
    {
      "epoch": 19.458537119497773,
      "grad_norm": 4.145888805389404,
      "learning_rate": 3.3784552400418526e-05,
      "loss": 0.6114,
      "step": 2132500
    },
    {
      "epoch": 19.45944959486094,
      "grad_norm": 4.6230363845825195,
      "learning_rate": 3.3783792004282556e-05,
      "loss": 0.6867,
      "step": 2132600
    },
    {
      "epoch": 19.460362070224104,
      "grad_norm": 3.7182183265686035,
      "learning_rate": 3.378303160814658e-05,
      "loss": 0.6366,
      "step": 2132700
    },
    {
      "epoch": 19.46127454558727,
      "grad_norm": 3.1719634532928467,
      "learning_rate": 3.3782271212010616e-05,
      "loss": 0.6643,
      "step": 2132800
    },
    {
      "epoch": 19.462187020950434,
      "grad_norm": 5.481391429901123,
      "learning_rate": 3.378151081587464e-05,
      "loss": 0.6586,
      "step": 2132900
    },
    {
      "epoch": 19.4630994963136,
      "grad_norm": 3.870155096054077,
      "learning_rate": 3.378075041973866e-05,
      "loss": 0.661,
      "step": 2133000
    },
    {
      "epoch": 19.464011971676765,
      "grad_norm": 4.208769798278809,
      "learning_rate": 3.37799900236027e-05,
      "loss": 0.6221,
      "step": 2133100
    },
    {
      "epoch": 19.46492444703993,
      "grad_norm": 4.493390083312988,
      "learning_rate": 3.377922962746672e-05,
      "loss": 0.6782,
      "step": 2133200
    },
    {
      "epoch": 19.465836922403096,
      "grad_norm": 4.459145545959473,
      "learning_rate": 3.377846923133075e-05,
      "loss": 0.6241,
      "step": 2133300
    },
    {
      "epoch": 19.46674939776626,
      "grad_norm": 2.8957972526550293,
      "learning_rate": 3.377770883519478e-05,
      "loss": 0.6573,
      "step": 2133400
    },
    {
      "epoch": 19.467661873129426,
      "grad_norm": 3.5704362392425537,
      "learning_rate": 3.377694843905881e-05,
      "loss": 0.6703,
      "step": 2133500
    },
    {
      "epoch": 19.46857434849259,
      "grad_norm": 2.779181480407715,
      "learning_rate": 3.377618804292284e-05,
      "loss": 0.6946,
      "step": 2133600
    },
    {
      "epoch": 19.469486823855757,
      "grad_norm": 3.7733044624328613,
      "learning_rate": 3.377542764678687e-05,
      "loss": 0.6298,
      "step": 2133700
    },
    {
      "epoch": 19.470399299218922,
      "grad_norm": 4.041467666625977,
      "learning_rate": 3.3774667250650896e-05,
      "loss": 0.6808,
      "step": 2133800
    },
    {
      "epoch": 19.471311774582087,
      "grad_norm": 3.4085352420806885,
      "learning_rate": 3.377390685451493e-05,
      "loss": 0.6786,
      "step": 2133900
    },
    {
      "epoch": 19.472224249945253,
      "grad_norm": 4.046404838562012,
      "learning_rate": 3.377314645837896e-05,
      "loss": 0.6878,
      "step": 2134000
    },
    {
      "epoch": 19.473136725308418,
      "grad_norm": 3.3785269260406494,
      "learning_rate": 3.377238606224299e-05,
      "loss": 0.6652,
      "step": 2134100
    },
    {
      "epoch": 19.474049200671583,
      "grad_norm": 3.6465816497802734,
      "learning_rate": 3.377162566610702e-05,
      "loss": 0.6916,
      "step": 2134200
    },
    {
      "epoch": 19.47496167603475,
      "grad_norm": 4.254515647888184,
      "learning_rate": 3.377086526997105e-05,
      "loss": 0.6693,
      "step": 2134300
    },
    {
      "epoch": 19.475874151397914,
      "grad_norm": 3.559117078781128,
      "learning_rate": 3.377010487383507e-05,
      "loss": 0.6458,
      "step": 2134400
    },
    {
      "epoch": 19.47678662676108,
      "grad_norm": 3.828974485397339,
      "learning_rate": 3.376934447769911e-05,
      "loss": 0.6122,
      "step": 2134500
    },
    {
      "epoch": 19.47769910212424,
      "grad_norm": 3.9249508380889893,
      "learning_rate": 3.376858408156313e-05,
      "loss": 0.6337,
      "step": 2134600
    },
    {
      "epoch": 19.478611577487406,
      "grad_norm": 4.323112964630127,
      "learning_rate": 3.376782368542716e-05,
      "loss": 0.6634,
      "step": 2134700
    },
    {
      "epoch": 19.47952405285057,
      "grad_norm": 3.971282482147217,
      "learning_rate": 3.376706328929119e-05,
      "loss": 0.6952,
      "step": 2134800
    },
    {
      "epoch": 19.480436528213737,
      "grad_norm": 4.502384185791016,
      "learning_rate": 3.376630289315522e-05,
      "loss": 0.6586,
      "step": 2134900
    },
    {
      "epoch": 19.481349003576902,
      "grad_norm": 3.6679954528808594,
      "learning_rate": 3.376554249701925e-05,
      "loss": 0.6686,
      "step": 2135000
    },
    {
      "epoch": 19.482261478940067,
      "grad_norm": 4.046170711517334,
      "learning_rate": 3.376478210088328e-05,
      "loss": 0.6389,
      "step": 2135100
    },
    {
      "epoch": 19.483173954303233,
      "grad_norm": 3.9971063137054443,
      "learning_rate": 3.3764021704747304e-05,
      "loss": 0.6782,
      "step": 2135200
    },
    {
      "epoch": 19.484086429666398,
      "grad_norm": 4.19647741317749,
      "learning_rate": 3.376326130861134e-05,
      "loss": 0.6669,
      "step": 2135300
    },
    {
      "epoch": 19.484998905029563,
      "grad_norm": 4.710856914520264,
      "learning_rate": 3.3762500912475364e-05,
      "loss": 0.6291,
      "step": 2135400
    },
    {
      "epoch": 19.48591138039273,
      "grad_norm": 3.5181264877319336,
      "learning_rate": 3.3761740516339394e-05,
      "loss": 0.6616,
      "step": 2135500
    },
    {
      "epoch": 19.486823855755894,
      "grad_norm": 3.6275088787078857,
      "learning_rate": 3.3760980120203424e-05,
      "loss": 0.7002,
      "step": 2135600
    },
    {
      "epoch": 19.48773633111906,
      "grad_norm": 2.931917428970337,
      "learning_rate": 3.3760219724067454e-05,
      "loss": 0.6849,
      "step": 2135700
    },
    {
      "epoch": 19.488648806482225,
      "grad_norm": 4.87036657333374,
      "learning_rate": 3.375945932793148e-05,
      "loss": 0.672,
      "step": 2135800
    },
    {
      "epoch": 19.48956128184539,
      "grad_norm": 3.5316734313964844,
      "learning_rate": 3.375869893179551e-05,
      "loss": 0.6357,
      "step": 2135900
    },
    {
      "epoch": 19.490473757208555,
      "grad_norm": 2.900594711303711,
      "learning_rate": 3.375793853565954e-05,
      "loss": 0.6373,
      "step": 2136000
    },
    {
      "epoch": 19.49138623257172,
      "grad_norm": 2.900399923324585,
      "learning_rate": 3.375717813952357e-05,
      "loss": 0.6632,
      "step": 2136100
    },
    {
      "epoch": 19.492298707934886,
      "grad_norm": 3.514862298965454,
      "learning_rate": 3.37564177433876e-05,
      "loss": 0.6768,
      "step": 2136200
    },
    {
      "epoch": 19.49321118329805,
      "grad_norm": 3.3139631748199463,
      "learning_rate": 3.375565734725162e-05,
      "loss": 0.6873,
      "step": 2136300
    },
    {
      "epoch": 19.494123658661216,
      "grad_norm": 5.2328081130981445,
      "learning_rate": 3.375489695111566e-05,
      "loss": 0.6719,
      "step": 2136400
    },
    {
      "epoch": 19.49503613402438,
      "grad_norm": 4.418927192687988,
      "learning_rate": 3.375413655497968e-05,
      "loss": 0.6542,
      "step": 2136500
    },
    {
      "epoch": 19.495948609387547,
      "grad_norm": 3.074352502822876,
      "learning_rate": 3.375337615884371e-05,
      "loss": 0.6592,
      "step": 2136600
    },
    {
      "epoch": 19.496861084750712,
      "grad_norm": 3.7409000396728516,
      "learning_rate": 3.375261576270774e-05,
      "loss": 0.6544,
      "step": 2136700
    },
    {
      "epoch": 19.497773560113878,
      "grad_norm": 3.751248598098755,
      "learning_rate": 3.375185536657177e-05,
      "loss": 0.6945,
      "step": 2136800
    },
    {
      "epoch": 19.498686035477043,
      "grad_norm": 3.7269914150238037,
      "learning_rate": 3.37510949704358e-05,
      "loss": 0.6732,
      "step": 2136900
    },
    {
      "epoch": 19.499598510840208,
      "grad_norm": 3.961545944213867,
      "learning_rate": 3.375033457429983e-05,
      "loss": 0.6413,
      "step": 2137000
    },
    {
      "epoch": 19.500510986203373,
      "grad_norm": 3.031934976577759,
      "learning_rate": 3.3749574178163855e-05,
      "loss": 0.6658,
      "step": 2137100
    },
    {
      "epoch": 19.50142346156654,
      "grad_norm": 5.712660789489746,
      "learning_rate": 3.3748813782027885e-05,
      "loss": 0.6787,
      "step": 2137200
    },
    {
      "epoch": 19.502335936929704,
      "grad_norm": 4.414831638336182,
      "learning_rate": 3.3748053385891915e-05,
      "loss": 0.6533,
      "step": 2137300
    },
    {
      "epoch": 19.50324841229287,
      "grad_norm": 3.8379011154174805,
      "learning_rate": 3.3747292989755945e-05,
      "loss": 0.6461,
      "step": 2137400
    },
    {
      "epoch": 19.504160887656035,
      "grad_norm": 3.070297956466675,
      "learning_rate": 3.3746532593619975e-05,
      "loss": 0.611,
      "step": 2137500
    },
    {
      "epoch": 19.5050733630192,
      "grad_norm": 3.975695848464966,
      "learning_rate": 3.3745772197484005e-05,
      "loss": 0.6136,
      "step": 2137600
    },
    {
      "epoch": 19.505985838382365,
      "grad_norm": 3.1470859050750732,
      "learning_rate": 3.374501180134803e-05,
      "loss": 0.6562,
      "step": 2137700
    },
    {
      "epoch": 19.50689831374553,
      "grad_norm": 4.15853214263916,
      "learning_rate": 3.3744251405212065e-05,
      "loss": 0.6528,
      "step": 2137800
    },
    {
      "epoch": 19.507810789108696,
      "grad_norm": 2.5208566188812256,
      "learning_rate": 3.374349100907609e-05,
      "loss": 0.6273,
      "step": 2137900
    },
    {
      "epoch": 19.508723264471858,
      "grad_norm": 3.218357801437378,
      "learning_rate": 3.374273061294012e-05,
      "loss": 0.5953,
      "step": 2138000
    },
    {
      "epoch": 19.509635739835023,
      "grad_norm": 3.295630931854248,
      "learning_rate": 3.374197021680415e-05,
      "loss": 0.6566,
      "step": 2138100
    },
    {
      "epoch": 19.510548215198188,
      "grad_norm": 3.143181562423706,
      "learning_rate": 3.374120982066818e-05,
      "loss": 0.6843,
      "step": 2138200
    },
    {
      "epoch": 19.511460690561353,
      "grad_norm": 4.1231513023376465,
      "learning_rate": 3.374044942453221e-05,
      "loss": 0.6357,
      "step": 2138300
    },
    {
      "epoch": 19.51237316592452,
      "grad_norm": 3.9631917476654053,
      "learning_rate": 3.373968902839624e-05,
      "loss": 0.651,
      "step": 2138400
    },
    {
      "epoch": 19.513285641287684,
      "grad_norm": 4.0429582595825195,
      "learning_rate": 3.373892863226026e-05,
      "loss": 0.6473,
      "step": 2138500
    },
    {
      "epoch": 19.51419811665085,
      "grad_norm": 4.245796203613281,
      "learning_rate": 3.373816823612429e-05,
      "loss": 0.6617,
      "step": 2138600
    },
    {
      "epoch": 19.515110592014015,
      "grad_norm": 3.5345118045806885,
      "learning_rate": 3.373740783998832e-05,
      "loss": 0.6691,
      "step": 2138700
    },
    {
      "epoch": 19.51602306737718,
      "grad_norm": 2.9895553588867188,
      "learning_rate": 3.3736647443852346e-05,
      "loss": 0.6199,
      "step": 2138800
    },
    {
      "epoch": 19.516935542740345,
      "grad_norm": 3.4328503608703613,
      "learning_rate": 3.373588704771638e-05,
      "loss": 0.6487,
      "step": 2138900
    },
    {
      "epoch": 19.51784801810351,
      "grad_norm": 3.767350673675537,
      "learning_rate": 3.3735126651580406e-05,
      "loss": 0.7058,
      "step": 2139000
    },
    {
      "epoch": 19.518760493466676,
      "grad_norm": 4.593254566192627,
      "learning_rate": 3.3734366255444436e-05,
      "loss": 0.655,
      "step": 2139100
    },
    {
      "epoch": 19.51967296882984,
      "grad_norm": 4.196572303771973,
      "learning_rate": 3.3733605859308466e-05,
      "loss": 0.6742,
      "step": 2139200
    },
    {
      "epoch": 19.520585444193006,
      "grad_norm": 4.617607593536377,
      "learning_rate": 3.3732845463172496e-05,
      "loss": 0.6671,
      "step": 2139300
    },
    {
      "epoch": 19.52149791955617,
      "grad_norm": 4.298070907592773,
      "learning_rate": 3.3732085067036526e-05,
      "loss": 0.6599,
      "step": 2139400
    },
    {
      "epoch": 19.522410394919337,
      "grad_norm": 3.7007803916931152,
      "learning_rate": 3.3731324670900556e-05,
      "loss": 0.6472,
      "step": 2139500
    },
    {
      "epoch": 19.523322870282502,
      "grad_norm": 3.144163131713867,
      "learning_rate": 3.373056427476458e-05,
      "loss": 0.6756,
      "step": 2139600
    },
    {
      "epoch": 19.524235345645668,
      "grad_norm": 3.6745026111602783,
      "learning_rate": 3.3729803878628616e-05,
      "loss": 0.6725,
      "step": 2139700
    },
    {
      "epoch": 19.525147821008833,
      "grad_norm": 4.019291877746582,
      "learning_rate": 3.372904348249264e-05,
      "loss": 0.6387,
      "step": 2139800
    },
    {
      "epoch": 19.526060296372,
      "grad_norm": 3.4248135089874268,
      "learning_rate": 3.372828308635667e-05,
      "loss": 0.6515,
      "step": 2139900
    },
    {
      "epoch": 19.526972771735164,
      "grad_norm": 3.7674312591552734,
      "learning_rate": 3.37275226902207e-05,
      "loss": 0.6938,
      "step": 2140000
    },
    {
      "epoch": 19.52788524709833,
      "grad_norm": 4.648930549621582,
      "learning_rate": 3.372676229408473e-05,
      "loss": 0.6784,
      "step": 2140100
    },
    {
      "epoch": 19.528797722461494,
      "grad_norm": 3.527900218963623,
      "learning_rate": 3.372600189794875e-05,
      "loss": 0.6668,
      "step": 2140200
    },
    {
      "epoch": 19.52971019782466,
      "grad_norm": 3.765796422958374,
      "learning_rate": 3.372524150181279e-05,
      "loss": 0.6712,
      "step": 2140300
    },
    {
      "epoch": 19.530622673187825,
      "grad_norm": 3.908815383911133,
      "learning_rate": 3.372448110567681e-05,
      "loss": 0.7169,
      "step": 2140400
    },
    {
      "epoch": 19.53153514855099,
      "grad_norm": 2.8198812007904053,
      "learning_rate": 3.372372070954084e-05,
      "loss": 0.6355,
      "step": 2140500
    },
    {
      "epoch": 19.532447623914155,
      "grad_norm": 4.083322525024414,
      "learning_rate": 3.372296031340487e-05,
      "loss": 0.6615,
      "step": 2140600
    },
    {
      "epoch": 19.53336009927732,
      "grad_norm": 4.788400173187256,
      "learning_rate": 3.37221999172689e-05,
      "loss": 0.6771,
      "step": 2140700
    },
    {
      "epoch": 19.534272574640486,
      "grad_norm": 4.298824787139893,
      "learning_rate": 3.3721439521132933e-05,
      "loss": 0.6406,
      "step": 2140800
    },
    {
      "epoch": 19.53518505000365,
      "grad_norm": 4.9716877937316895,
      "learning_rate": 3.3720679124996964e-05,
      "loss": 0.6625,
      "step": 2140900
    },
    {
      "epoch": 19.536097525366817,
      "grad_norm": 4.147407531738281,
      "learning_rate": 3.371991872886099e-05,
      "loss": 0.6385,
      "step": 2141000
    },
    {
      "epoch": 19.537010000729982,
      "grad_norm": 4.306204795837402,
      "learning_rate": 3.3719158332725024e-05,
      "loss": 0.699,
      "step": 2141100
    },
    {
      "epoch": 19.537922476093147,
      "grad_norm": 3.455453872680664,
      "learning_rate": 3.371839793658905e-05,
      "loss": 0.6461,
      "step": 2141200
    },
    {
      "epoch": 19.53883495145631,
      "grad_norm": 4.698043346405029,
      "learning_rate": 3.371763754045308e-05,
      "loss": 0.6475,
      "step": 2141300
    },
    {
      "epoch": 19.539747426819474,
      "grad_norm": 3.655172109603882,
      "learning_rate": 3.371687714431711e-05,
      "loss": 0.6625,
      "step": 2141400
    },
    {
      "epoch": 19.54065990218264,
      "grad_norm": 4.103452682495117,
      "learning_rate": 3.371611674818113e-05,
      "loss": 0.6515,
      "step": 2141500
    },
    {
      "epoch": 19.541572377545805,
      "grad_norm": 3.4584295749664307,
      "learning_rate": 3.371535635204516e-05,
      "loss": 0.6426,
      "step": 2141600
    },
    {
      "epoch": 19.54248485290897,
      "grad_norm": 3.811220407485962,
      "learning_rate": 3.371459595590919e-05,
      "loss": 0.6318,
      "step": 2141700
    },
    {
      "epoch": 19.543397328272135,
      "grad_norm": 4.233445644378662,
      "learning_rate": 3.371383555977322e-05,
      "loss": 0.6693,
      "step": 2141800
    },
    {
      "epoch": 19.5443098036353,
      "grad_norm": 3.895759344100952,
      "learning_rate": 3.371307516363725e-05,
      "loss": 0.659,
      "step": 2141900
    },
    {
      "epoch": 19.545222278998466,
      "grad_norm": 3.868419885635376,
      "learning_rate": 3.371231476750128e-05,
      "loss": 0.6616,
      "step": 2142000
    },
    {
      "epoch": 19.54613475436163,
      "grad_norm": 4.187851905822754,
      "learning_rate": 3.3711554371365304e-05,
      "loss": 0.6712,
      "step": 2142100
    },
    {
      "epoch": 19.547047229724797,
      "grad_norm": 4.407778739929199,
      "learning_rate": 3.371079397522934e-05,
      "loss": 0.6567,
      "step": 2142200
    },
    {
      "epoch": 19.547959705087962,
      "grad_norm": 3.8377068042755127,
      "learning_rate": 3.3710033579093364e-05,
      "loss": 0.622,
      "step": 2142300
    },
    {
      "epoch": 19.548872180451127,
      "grad_norm": 3.772056818008423,
      "learning_rate": 3.3709273182957394e-05,
      "loss": 0.6604,
      "step": 2142400
    },
    {
      "epoch": 19.549784655814292,
      "grad_norm": 4.508906364440918,
      "learning_rate": 3.3708512786821424e-05,
      "loss": 0.6614,
      "step": 2142500
    },
    {
      "epoch": 19.550697131177458,
      "grad_norm": 3.8318474292755127,
      "learning_rate": 3.3707752390685454e-05,
      "loss": 0.6367,
      "step": 2142600
    },
    {
      "epoch": 19.551609606540623,
      "grad_norm": 3.2371954917907715,
      "learning_rate": 3.370699199454948e-05,
      "loss": 0.6476,
      "step": 2142700
    },
    {
      "epoch": 19.55252208190379,
      "grad_norm": 3.591163396835327,
      "learning_rate": 3.3706231598413514e-05,
      "loss": 0.6701,
      "step": 2142800
    },
    {
      "epoch": 19.553434557266954,
      "grad_norm": 3.7895960807800293,
      "learning_rate": 3.370547120227754e-05,
      "loss": 0.6616,
      "step": 2142900
    },
    {
      "epoch": 19.55434703263012,
      "grad_norm": 3.97501540184021,
      "learning_rate": 3.370471080614157e-05,
      "loss": 0.6671,
      "step": 2143000
    },
    {
      "epoch": 19.555259507993284,
      "grad_norm": 4.012160301208496,
      "learning_rate": 3.37039504100056e-05,
      "loss": 0.6811,
      "step": 2143100
    },
    {
      "epoch": 19.55617198335645,
      "grad_norm": 3.3642852306365967,
      "learning_rate": 3.370319001386963e-05,
      "loss": 0.6712,
      "step": 2143200
    },
    {
      "epoch": 19.557084458719615,
      "grad_norm": 3.5529561042785645,
      "learning_rate": 3.370242961773366e-05,
      "loss": 0.6336,
      "step": 2143300
    },
    {
      "epoch": 19.55799693408278,
      "grad_norm": 4.442352294921875,
      "learning_rate": 3.370166922159769e-05,
      "loss": 0.6277,
      "step": 2143400
    },
    {
      "epoch": 19.558909409445945,
      "grad_norm": 3.1451187133789062,
      "learning_rate": 3.370090882546171e-05,
      "loss": 0.604,
      "step": 2143500
    },
    {
      "epoch": 19.55982188480911,
      "grad_norm": 4.200306415557861,
      "learning_rate": 3.370014842932575e-05,
      "loss": 0.6719,
      "step": 2143600
    },
    {
      "epoch": 19.560734360172276,
      "grad_norm": 3.98224139213562,
      "learning_rate": 3.369938803318977e-05,
      "loss": 0.6443,
      "step": 2143700
    },
    {
      "epoch": 19.56164683553544,
      "grad_norm": 3.7520229816436768,
      "learning_rate": 3.36986276370538e-05,
      "loss": 0.6648,
      "step": 2143800
    },
    {
      "epoch": 19.562559310898607,
      "grad_norm": 3.9179847240448,
      "learning_rate": 3.369786724091783e-05,
      "loss": 0.6427,
      "step": 2143900
    },
    {
      "epoch": 19.563471786261772,
      "grad_norm": 3.4005887508392334,
      "learning_rate": 3.369710684478186e-05,
      "loss": 0.6754,
      "step": 2144000
    },
    {
      "epoch": 19.564384261624937,
      "grad_norm": 4.319519519805908,
      "learning_rate": 3.3696346448645885e-05,
      "loss": 0.6238,
      "step": 2144100
    },
    {
      "epoch": 19.565296736988103,
      "grad_norm": 4.781343936920166,
      "learning_rate": 3.369558605250992e-05,
      "loss": 0.6497,
      "step": 2144200
    },
    {
      "epoch": 19.566209212351268,
      "grad_norm": 3.906123161315918,
      "learning_rate": 3.3694825656373945e-05,
      "loss": 0.6868,
      "step": 2144300
    },
    {
      "epoch": 19.567121687714433,
      "grad_norm": 3.883455276489258,
      "learning_rate": 3.3694065260237975e-05,
      "loss": 0.6685,
      "step": 2144400
    },
    {
      "epoch": 19.5680341630776,
      "grad_norm": 4.1884236335754395,
      "learning_rate": 3.3693304864102005e-05,
      "loss": 0.7015,
      "step": 2144500
    },
    {
      "epoch": 19.568946638440764,
      "grad_norm": 4.4187469482421875,
      "learning_rate": 3.369254446796603e-05,
      "loss": 0.697,
      "step": 2144600
    },
    {
      "epoch": 19.56985911380393,
      "grad_norm": 4.075755596160889,
      "learning_rate": 3.3691784071830065e-05,
      "loss": 0.6911,
      "step": 2144700
    },
    {
      "epoch": 19.57077158916709,
      "grad_norm": 2.488232374191284,
      "learning_rate": 3.369102367569409e-05,
      "loss": 0.6462,
      "step": 2144800
    },
    {
      "epoch": 19.571684064530256,
      "grad_norm": 3.140165090560913,
      "learning_rate": 3.369026327955812e-05,
      "loss": 0.6517,
      "step": 2144900
    },
    {
      "epoch": 19.57259653989342,
      "grad_norm": 3.6246700286865234,
      "learning_rate": 3.368950288342215e-05,
      "loss": 0.6711,
      "step": 2145000
    },
    {
      "epoch": 19.573509015256587,
      "grad_norm": 3.5587286949157715,
      "learning_rate": 3.368874248728618e-05,
      "loss": 0.6957,
      "step": 2145100
    },
    {
      "epoch": 19.574421490619752,
      "grad_norm": 4.094731330871582,
      "learning_rate": 3.36879820911502e-05,
      "loss": 0.678,
      "step": 2145200
    },
    {
      "epoch": 19.575333965982917,
      "grad_norm": 4.737651824951172,
      "learning_rate": 3.368722169501424e-05,
      "loss": 0.6522,
      "step": 2145300
    },
    {
      "epoch": 19.576246441346083,
      "grad_norm": 3.927950382232666,
      "learning_rate": 3.368646129887826e-05,
      "loss": 0.6652,
      "step": 2145400
    },
    {
      "epoch": 19.577158916709248,
      "grad_norm": 4.852560043334961,
      "learning_rate": 3.368570090274229e-05,
      "loss": 0.6834,
      "step": 2145500
    },
    {
      "epoch": 19.578071392072413,
      "grad_norm": 2.5682451725006104,
      "learning_rate": 3.368494050660632e-05,
      "loss": 0.684,
      "step": 2145600
    },
    {
      "epoch": 19.57898386743558,
      "grad_norm": 4.008184909820557,
      "learning_rate": 3.368418011047035e-05,
      "loss": 0.6566,
      "step": 2145700
    },
    {
      "epoch": 19.579896342798744,
      "grad_norm": 4.076100826263428,
      "learning_rate": 3.368341971433438e-05,
      "loss": 0.6777,
      "step": 2145800
    },
    {
      "epoch": 19.58080881816191,
      "grad_norm": 4.5295491218566895,
      "learning_rate": 3.368265931819841e-05,
      "loss": 0.6555,
      "step": 2145900
    },
    {
      "epoch": 19.581721293525074,
      "grad_norm": 4.493892192840576,
      "learning_rate": 3.3681898922062436e-05,
      "loss": 0.633,
      "step": 2146000
    },
    {
      "epoch": 19.58263376888824,
      "grad_norm": 3.0515432357788086,
      "learning_rate": 3.368113852592647e-05,
      "loss": 0.6508,
      "step": 2146100
    },
    {
      "epoch": 19.583546244251405,
      "grad_norm": 3.9138975143432617,
      "learning_rate": 3.3680378129790496e-05,
      "loss": 0.6876,
      "step": 2146200
    },
    {
      "epoch": 19.58445871961457,
      "grad_norm": 4.100577354431152,
      "learning_rate": 3.3679617733654526e-05,
      "loss": 0.6641,
      "step": 2146300
    },
    {
      "epoch": 19.585371194977736,
      "grad_norm": 3.583991289138794,
      "learning_rate": 3.3678857337518556e-05,
      "loss": 0.6626,
      "step": 2146400
    },
    {
      "epoch": 19.5862836703409,
      "grad_norm": 3.5087196826934814,
      "learning_rate": 3.3678096941382586e-05,
      "loss": 0.639,
      "step": 2146500
    },
    {
      "epoch": 19.587196145704066,
      "grad_norm": 4.439925670623779,
      "learning_rate": 3.367733654524661e-05,
      "loss": 0.6877,
      "step": 2146600
    },
    {
      "epoch": 19.58810862106723,
      "grad_norm": 3.8900561332702637,
      "learning_rate": 3.3676576149110646e-05,
      "loss": 0.6286,
      "step": 2146700
    },
    {
      "epoch": 19.589021096430397,
      "grad_norm": 4.668920516967773,
      "learning_rate": 3.367581575297467e-05,
      "loss": 0.6803,
      "step": 2146800
    },
    {
      "epoch": 19.589933571793562,
      "grad_norm": 4.182346343994141,
      "learning_rate": 3.36750553568387e-05,
      "loss": 0.7092,
      "step": 2146900
    },
    {
      "epoch": 19.590846047156727,
      "grad_norm": 4.286154270172119,
      "learning_rate": 3.367429496070273e-05,
      "loss": 0.6476,
      "step": 2147000
    },
    {
      "epoch": 19.591758522519893,
      "grad_norm": 3.629692792892456,
      "learning_rate": 3.367353456456676e-05,
      "loss": 0.6365,
      "step": 2147100
    },
    {
      "epoch": 19.592670997883058,
      "grad_norm": 4.947627067565918,
      "learning_rate": 3.367277416843079e-05,
      "loss": 0.6259,
      "step": 2147200
    },
    {
      "epoch": 19.593583473246223,
      "grad_norm": 4.8518571853637695,
      "learning_rate": 3.367201377229481e-05,
      "loss": 0.6541,
      "step": 2147300
    },
    {
      "epoch": 19.59449594860939,
      "grad_norm": 4.075867176055908,
      "learning_rate": 3.367125337615884e-05,
      "loss": 0.6529,
      "step": 2147400
    },
    {
      "epoch": 19.595408423972554,
      "grad_norm": 4.032985687255859,
      "learning_rate": 3.3670492980022873e-05,
      "loss": 0.6625,
      "step": 2147500
    },
    {
      "epoch": 19.59632089933572,
      "grad_norm": 3.6570801734924316,
      "learning_rate": 3.3669732583886903e-05,
      "loss": 0.6394,
      "step": 2147600
    },
    {
      "epoch": 19.597233374698884,
      "grad_norm": 3.14898943901062,
      "learning_rate": 3.366897218775093e-05,
      "loss": 0.6429,
      "step": 2147700
    },
    {
      "epoch": 19.59814585006205,
      "grad_norm": 3.41363263130188,
      "learning_rate": 3.3668211791614964e-05,
      "loss": 0.6764,
      "step": 2147800
    },
    {
      "epoch": 19.599058325425215,
      "grad_norm": 4.306113243103027,
      "learning_rate": 3.366745139547899e-05,
      "loss": 0.6838,
      "step": 2147900
    },
    {
      "epoch": 19.59997080078838,
      "grad_norm": 4.923941135406494,
      "learning_rate": 3.366669099934302e-05,
      "loss": 0.6393,
      "step": 2148000
    },
    {
      "epoch": 19.600883276151542,
      "grad_norm": 3.8911073207855225,
      "learning_rate": 3.366593060320705e-05,
      "loss": 0.6634,
      "step": 2148100
    },
    {
      "epoch": 19.601795751514707,
      "grad_norm": 3.901871919631958,
      "learning_rate": 3.366517020707108e-05,
      "loss": 0.6587,
      "step": 2148200
    },
    {
      "epoch": 19.602708226877873,
      "grad_norm": 4.573378562927246,
      "learning_rate": 3.366440981093511e-05,
      "loss": 0.6828,
      "step": 2148300
    },
    {
      "epoch": 19.603620702241038,
      "grad_norm": 4.340607643127441,
      "learning_rate": 3.366364941479914e-05,
      "loss": 0.6381,
      "step": 2148400
    },
    {
      "epoch": 19.604533177604203,
      "grad_norm": 3.79862904548645,
      "learning_rate": 3.366288901866316e-05,
      "loss": 0.71,
      "step": 2148500
    },
    {
      "epoch": 19.60544565296737,
      "grad_norm": 3.8207054138183594,
      "learning_rate": 3.36621286225272e-05,
      "loss": 0.6583,
      "step": 2148600
    },
    {
      "epoch": 19.606358128330534,
      "grad_norm": 4.756105422973633,
      "learning_rate": 3.366136822639122e-05,
      "loss": 0.6671,
      "step": 2148700
    },
    {
      "epoch": 19.6072706036937,
      "grad_norm": 2.441026210784912,
      "learning_rate": 3.366060783025525e-05,
      "loss": 0.6478,
      "step": 2148800
    },
    {
      "epoch": 19.608183079056865,
      "grad_norm": 4.057123184204102,
      "learning_rate": 3.365984743411928e-05,
      "loss": 0.6407,
      "step": 2148900
    },
    {
      "epoch": 19.60909555442003,
      "grad_norm": 4.396004676818848,
      "learning_rate": 3.365908703798331e-05,
      "loss": 0.6844,
      "step": 2149000
    },
    {
      "epoch": 19.610008029783195,
      "grad_norm": 3.8230443000793457,
      "learning_rate": 3.3658326641847334e-05,
      "loss": 0.6513,
      "step": 2149100
    },
    {
      "epoch": 19.61092050514636,
      "grad_norm": 4.877041339874268,
      "learning_rate": 3.365756624571137e-05,
      "loss": 0.6396,
      "step": 2149200
    },
    {
      "epoch": 19.611832980509526,
      "grad_norm": 3.6457791328430176,
      "learning_rate": 3.3656805849575394e-05,
      "loss": 0.6764,
      "step": 2149300
    },
    {
      "epoch": 19.61274545587269,
      "grad_norm": 3.7622573375701904,
      "learning_rate": 3.3656045453439424e-05,
      "loss": 0.6408,
      "step": 2149400
    },
    {
      "epoch": 19.613657931235856,
      "grad_norm": 4.003777503967285,
      "learning_rate": 3.3655285057303454e-05,
      "loss": 0.6268,
      "step": 2149500
    },
    {
      "epoch": 19.61457040659902,
      "grad_norm": 2.8350560665130615,
      "learning_rate": 3.3654524661167484e-05,
      "loss": 0.664,
      "step": 2149600
    },
    {
      "epoch": 19.615482881962187,
      "grad_norm": 3.137817859649658,
      "learning_rate": 3.3653764265031515e-05,
      "loss": 0.6326,
      "step": 2149700
    },
    {
      "epoch": 19.616395357325352,
      "grad_norm": 3.4897842407226562,
      "learning_rate": 3.3653003868895545e-05,
      "loss": 0.6453,
      "step": 2149800
    },
    {
      "epoch": 19.617307832688518,
      "grad_norm": 3.600456476211548,
      "learning_rate": 3.365224347275957e-05,
      "loss": 0.6813,
      "step": 2149900
    },
    {
      "epoch": 19.618220308051683,
      "grad_norm": 5.04449987411499,
      "learning_rate": 3.36514830766236e-05,
      "loss": 0.6616,
      "step": 2150000
    },
    {
      "epoch": 19.619132783414848,
      "grad_norm": 3.6526899337768555,
      "learning_rate": 3.365072268048763e-05,
      "loss": 0.6672,
      "step": 2150100
    },
    {
      "epoch": 19.620045258778013,
      "grad_norm": 3.20259165763855,
      "learning_rate": 3.364996228435166e-05,
      "loss": 0.6664,
      "step": 2150200
    },
    {
      "epoch": 19.62095773414118,
      "grad_norm": 3.1112542152404785,
      "learning_rate": 3.364920188821569e-05,
      "loss": 0.6588,
      "step": 2150300
    },
    {
      "epoch": 19.621870209504344,
      "grad_norm": 4.387479305267334,
      "learning_rate": 3.364844149207971e-05,
      "loss": 0.6603,
      "step": 2150400
    },
    {
      "epoch": 19.62278268486751,
      "grad_norm": 2.274756669998169,
      "learning_rate": 3.364768109594375e-05,
      "loss": 0.6361,
      "step": 2150500
    },
    {
      "epoch": 19.623695160230675,
      "grad_norm": 3.2421298027038574,
      "learning_rate": 3.364692069980777e-05,
      "loss": 0.6332,
      "step": 2150600
    },
    {
      "epoch": 19.62460763559384,
      "grad_norm": 3.7823050022125244,
      "learning_rate": 3.36461603036718e-05,
      "loss": 0.6288,
      "step": 2150700
    },
    {
      "epoch": 19.625520110957005,
      "grad_norm": 4.259803295135498,
      "learning_rate": 3.364539990753583e-05,
      "loss": 0.6318,
      "step": 2150800
    },
    {
      "epoch": 19.62643258632017,
      "grad_norm": 3.5289714336395264,
      "learning_rate": 3.364463951139986e-05,
      "loss": 0.651,
      "step": 2150900
    },
    {
      "epoch": 19.627345061683336,
      "grad_norm": 6.9020490646362305,
      "learning_rate": 3.3643879115263885e-05,
      "loss": 0.6526,
      "step": 2151000
    },
    {
      "epoch": 19.6282575370465,
      "grad_norm": 3.9640278816223145,
      "learning_rate": 3.364311871912792e-05,
      "loss": 0.6492,
      "step": 2151100
    },
    {
      "epoch": 19.629170012409666,
      "grad_norm": 4.938612461090088,
      "learning_rate": 3.3642358322991945e-05,
      "loss": 0.6445,
      "step": 2151200
    },
    {
      "epoch": 19.63008248777283,
      "grad_norm": 2.8518731594085693,
      "learning_rate": 3.3641597926855975e-05,
      "loss": 0.6799,
      "step": 2151300
    },
    {
      "epoch": 19.630994963135997,
      "grad_norm": 3.444905996322632,
      "learning_rate": 3.3640837530720005e-05,
      "loss": 0.6698,
      "step": 2151400
    },
    {
      "epoch": 19.631907438499162,
      "grad_norm": 4.458230972290039,
      "learning_rate": 3.3640077134584035e-05,
      "loss": 0.6342,
      "step": 2151500
    },
    {
      "epoch": 19.632819913862324,
      "grad_norm": 4.377079486846924,
      "learning_rate": 3.3639316738448065e-05,
      "loss": 0.6574,
      "step": 2151600
    },
    {
      "epoch": 19.63373238922549,
      "grad_norm": 3.994474411010742,
      "learning_rate": 3.3638556342312096e-05,
      "loss": 0.7022,
      "step": 2151700
    },
    {
      "epoch": 19.634644864588655,
      "grad_norm": 4.044183731079102,
      "learning_rate": 3.363779594617612e-05,
      "loss": 0.6787,
      "step": 2151800
    },
    {
      "epoch": 19.63555733995182,
      "grad_norm": 5.736949443817139,
      "learning_rate": 3.3637035550040156e-05,
      "loss": 0.702,
      "step": 2151900
    },
    {
      "epoch": 19.636469815314985,
      "grad_norm": 3.187927007675171,
      "learning_rate": 3.363627515390418e-05,
      "loss": 0.6777,
      "step": 2152000
    },
    {
      "epoch": 19.63738229067815,
      "grad_norm": 3.846658706665039,
      "learning_rate": 3.363551475776821e-05,
      "loss": 0.6402,
      "step": 2152100
    },
    {
      "epoch": 19.638294766041316,
      "grad_norm": 3.2500200271606445,
      "learning_rate": 3.363475436163224e-05,
      "loss": 0.6626,
      "step": 2152200
    },
    {
      "epoch": 19.63920724140448,
      "grad_norm": 3.7790017127990723,
      "learning_rate": 3.363399396549627e-05,
      "loss": 0.6558,
      "step": 2152300
    },
    {
      "epoch": 19.640119716767646,
      "grad_norm": 3.440873384475708,
      "learning_rate": 3.363323356936029e-05,
      "loss": 0.6352,
      "step": 2152400
    },
    {
      "epoch": 19.64103219213081,
      "grad_norm": 5.4017157554626465,
      "learning_rate": 3.363247317322433e-05,
      "loss": 0.6929,
      "step": 2152500
    },
    {
      "epoch": 19.641944667493977,
      "grad_norm": 3.648538112640381,
      "learning_rate": 3.363171277708835e-05,
      "loss": 0.6949,
      "step": 2152600
    },
    {
      "epoch": 19.642857142857142,
      "grad_norm": 3.7324061393737793,
      "learning_rate": 3.363095238095238e-05,
      "loss": 0.6812,
      "step": 2152700
    },
    {
      "epoch": 19.643769618220308,
      "grad_norm": 3.597334861755371,
      "learning_rate": 3.363019198481641e-05,
      "loss": 0.6538,
      "step": 2152800
    },
    {
      "epoch": 19.644682093583473,
      "grad_norm": 3.9914438724517822,
      "learning_rate": 3.3629431588680436e-05,
      "loss": 0.666,
      "step": 2152900
    },
    {
      "epoch": 19.64559456894664,
      "grad_norm": 3.8454084396362305,
      "learning_rate": 3.362867119254447e-05,
      "loss": 0.6388,
      "step": 2153000
    },
    {
      "epoch": 19.646507044309804,
      "grad_norm": 4.202411651611328,
      "learning_rate": 3.3627910796408496e-05,
      "loss": 0.6577,
      "step": 2153100
    },
    {
      "epoch": 19.64741951967297,
      "grad_norm": 4.229759693145752,
      "learning_rate": 3.3627150400272526e-05,
      "loss": 0.6542,
      "step": 2153200
    },
    {
      "epoch": 19.648331995036134,
      "grad_norm": 2.836601972579956,
      "learning_rate": 3.3626390004136556e-05,
      "loss": 0.6881,
      "step": 2153300
    },
    {
      "epoch": 19.6492444703993,
      "grad_norm": 2.420013904571533,
      "learning_rate": 3.3625629608000586e-05,
      "loss": 0.6414,
      "step": 2153400
    },
    {
      "epoch": 19.650156945762465,
      "grad_norm": 3.7138216495513916,
      "learning_rate": 3.362486921186461e-05,
      "loss": 0.6544,
      "step": 2153500
    },
    {
      "epoch": 19.65106942112563,
      "grad_norm": 4.337948322296143,
      "learning_rate": 3.3624108815728647e-05,
      "loss": 0.6296,
      "step": 2153600
    },
    {
      "epoch": 19.651981896488795,
      "grad_norm": 3.3550941944122314,
      "learning_rate": 3.362334841959267e-05,
      "loss": 0.666,
      "step": 2153700
    },
    {
      "epoch": 19.65289437185196,
      "grad_norm": 3.3257369995117188,
      "learning_rate": 3.36225880234567e-05,
      "loss": 0.6509,
      "step": 2153800
    },
    {
      "epoch": 19.653806847215126,
      "grad_norm": 3.9597980976104736,
      "learning_rate": 3.362182762732073e-05,
      "loss": 0.667,
      "step": 2153900
    },
    {
      "epoch": 19.65471932257829,
      "grad_norm": 3.499516010284424,
      "learning_rate": 3.362106723118476e-05,
      "loss": 0.6558,
      "step": 2154000
    },
    {
      "epoch": 19.655631797941457,
      "grad_norm": 3.383216142654419,
      "learning_rate": 3.362030683504879e-05,
      "loss": 0.6227,
      "step": 2154100
    },
    {
      "epoch": 19.656544273304622,
      "grad_norm": 4.350225448608398,
      "learning_rate": 3.361954643891282e-05,
      "loss": 0.6438,
      "step": 2154200
    },
    {
      "epoch": 19.657456748667787,
      "grad_norm": 2.877075672149658,
      "learning_rate": 3.3618786042776843e-05,
      "loss": 0.6564,
      "step": 2154300
    },
    {
      "epoch": 19.658369224030952,
      "grad_norm": 3.536557912826538,
      "learning_rate": 3.361802564664088e-05,
      "loss": 0.6557,
      "step": 2154400
    },
    {
      "epoch": 19.659281699394118,
      "grad_norm": 4.10757303237915,
      "learning_rate": 3.3617265250504904e-05,
      "loss": 0.6615,
      "step": 2154500
    },
    {
      "epoch": 19.660194174757283,
      "grad_norm": 3.605893850326538,
      "learning_rate": 3.3616504854368934e-05,
      "loss": 0.6743,
      "step": 2154600
    },
    {
      "epoch": 19.66110665012045,
      "grad_norm": 5.006647109985352,
      "learning_rate": 3.3615744458232964e-05,
      "loss": 0.631,
      "step": 2154700
    },
    {
      "epoch": 19.662019125483614,
      "grad_norm": 3.8580169677734375,
      "learning_rate": 3.3614984062096994e-05,
      "loss": 0.6729,
      "step": 2154800
    },
    {
      "epoch": 19.662931600846775,
      "grad_norm": 3.9817898273468018,
      "learning_rate": 3.361422366596102e-05,
      "loss": 0.6649,
      "step": 2154900
    },
    {
      "epoch": 19.66384407620994,
      "grad_norm": 3.2525012493133545,
      "learning_rate": 3.3613463269825054e-05,
      "loss": 0.6368,
      "step": 2155000
    },
    {
      "epoch": 19.664756551573106,
      "grad_norm": 4.246980667114258,
      "learning_rate": 3.361270287368908e-05,
      "loss": 0.6802,
      "step": 2155100
    },
    {
      "epoch": 19.66566902693627,
      "grad_norm": 3.9333343505859375,
      "learning_rate": 3.361194247755311e-05,
      "loss": 0.6592,
      "step": 2155200
    },
    {
      "epoch": 19.666581502299437,
      "grad_norm": 3.392686605453491,
      "learning_rate": 3.361118208141714e-05,
      "loss": 0.6704,
      "step": 2155300
    },
    {
      "epoch": 19.667493977662602,
      "grad_norm": 4.731404781341553,
      "learning_rate": 3.361042168528117e-05,
      "loss": 0.6827,
      "step": 2155400
    },
    {
      "epoch": 19.668406453025767,
      "grad_norm": 3.891143321990967,
      "learning_rate": 3.36096612891452e-05,
      "loss": 0.6466,
      "step": 2155500
    },
    {
      "epoch": 19.669318928388932,
      "grad_norm": 4.334780693054199,
      "learning_rate": 3.360890089300923e-05,
      "loss": 0.659,
      "step": 2155600
    },
    {
      "epoch": 19.670231403752098,
      "grad_norm": 4.62886905670166,
      "learning_rate": 3.360814049687325e-05,
      "loss": 0.6704,
      "step": 2155700
    },
    {
      "epoch": 19.671143879115263,
      "grad_norm": 3.8101389408111572,
      "learning_rate": 3.360738010073728e-05,
      "loss": 0.6697,
      "step": 2155800
    },
    {
      "epoch": 19.67205635447843,
      "grad_norm": 4.488213539123535,
      "learning_rate": 3.360661970460131e-05,
      "loss": 0.6688,
      "step": 2155900
    },
    {
      "epoch": 19.672968829841594,
      "grad_norm": 4.116511821746826,
      "learning_rate": 3.3605859308465334e-05,
      "loss": 0.6533,
      "step": 2156000
    },
    {
      "epoch": 19.67388130520476,
      "grad_norm": 4.237286567687988,
      "learning_rate": 3.360509891232937e-05,
      "loss": 0.6782,
      "step": 2156100
    },
    {
      "epoch": 19.674793780567924,
      "grad_norm": 3.642462730407715,
      "learning_rate": 3.3604338516193394e-05,
      "loss": 0.6677,
      "step": 2156200
    },
    {
      "epoch": 19.67570625593109,
      "grad_norm": 4.390407085418701,
      "learning_rate": 3.3603578120057424e-05,
      "loss": 0.6956,
      "step": 2156300
    },
    {
      "epoch": 19.676618731294255,
      "grad_norm": 3.2305116653442383,
      "learning_rate": 3.3602817723921455e-05,
      "loss": 0.6446,
      "step": 2156400
    },
    {
      "epoch": 19.67753120665742,
      "grad_norm": 3.233060359954834,
      "learning_rate": 3.3602057327785485e-05,
      "loss": 0.6545,
      "step": 2156500
    },
    {
      "epoch": 19.678443682020585,
      "grad_norm": 3.867427349090576,
      "learning_rate": 3.3601296931649515e-05,
      "loss": 0.6922,
      "step": 2156600
    },
    {
      "epoch": 19.67935615738375,
      "grad_norm": 3.480147361755371,
      "learning_rate": 3.3600536535513545e-05,
      "loss": 0.6523,
      "step": 2156700
    },
    {
      "epoch": 19.680268632746916,
      "grad_norm": 4.2697625160217285,
      "learning_rate": 3.359977613937757e-05,
      "loss": 0.6279,
      "step": 2156800
    },
    {
      "epoch": 19.68118110811008,
      "grad_norm": 4.339561939239502,
      "learning_rate": 3.3599015743241605e-05,
      "loss": 0.673,
      "step": 2156900
    },
    {
      "epoch": 19.682093583473247,
      "grad_norm": 4.421523094177246,
      "learning_rate": 3.359825534710563e-05,
      "loss": 0.6406,
      "step": 2157000
    },
    {
      "epoch": 19.683006058836412,
      "grad_norm": 4.033050537109375,
      "learning_rate": 3.359749495096966e-05,
      "loss": 0.6529,
      "step": 2157100
    },
    {
      "epoch": 19.683918534199577,
      "grad_norm": 3.691366672515869,
      "learning_rate": 3.359673455483369e-05,
      "loss": 0.6639,
      "step": 2157200
    },
    {
      "epoch": 19.684831009562743,
      "grad_norm": 4.532599925994873,
      "learning_rate": 3.359597415869772e-05,
      "loss": 0.6445,
      "step": 2157300
    },
    {
      "epoch": 19.685743484925908,
      "grad_norm": 3.8766160011291504,
      "learning_rate": 3.359521376256174e-05,
      "loss": 0.6661,
      "step": 2157400
    },
    {
      "epoch": 19.686655960289073,
      "grad_norm": 3.3286032676696777,
      "learning_rate": 3.359445336642578e-05,
      "loss": 0.6604,
      "step": 2157500
    },
    {
      "epoch": 19.68756843565224,
      "grad_norm": 3.8226606845855713,
      "learning_rate": 3.35936929702898e-05,
      "loss": 0.66,
      "step": 2157600
    },
    {
      "epoch": 19.688480911015404,
      "grad_norm": 4.04109525680542,
      "learning_rate": 3.359293257415383e-05,
      "loss": 0.6571,
      "step": 2157700
    },
    {
      "epoch": 19.68939338637857,
      "grad_norm": 4.288272857666016,
      "learning_rate": 3.359217217801786e-05,
      "loss": 0.641,
      "step": 2157800
    },
    {
      "epoch": 19.690305861741734,
      "grad_norm": 3.384432077407837,
      "learning_rate": 3.359141178188189e-05,
      "loss": 0.6626,
      "step": 2157900
    },
    {
      "epoch": 19.6912183371049,
      "grad_norm": 5.131870269775391,
      "learning_rate": 3.359065138574592e-05,
      "loss": 0.6436,
      "step": 2158000
    },
    {
      "epoch": 19.692130812468065,
      "grad_norm": 4.228222370147705,
      "learning_rate": 3.358989098960995e-05,
      "loss": 0.6584,
      "step": 2158100
    },
    {
      "epoch": 19.69304328783123,
      "grad_norm": 4.965957164764404,
      "learning_rate": 3.3589130593473975e-05,
      "loss": 0.6368,
      "step": 2158200
    },
    {
      "epoch": 19.693955763194396,
      "grad_norm": 4.593132019042969,
      "learning_rate": 3.358837019733801e-05,
      "loss": 0.6835,
      "step": 2158300
    },
    {
      "epoch": 19.694868238557557,
      "grad_norm": 4.1906280517578125,
      "learning_rate": 3.3587609801202036e-05,
      "loss": 0.6819,
      "step": 2158400
    },
    {
      "epoch": 19.695780713920723,
      "grad_norm": 4.246232032775879,
      "learning_rate": 3.3586849405066066e-05,
      "loss": 0.6602,
      "step": 2158500
    },
    {
      "epoch": 19.696693189283888,
      "grad_norm": 3.8751816749572754,
      "learning_rate": 3.3586089008930096e-05,
      "loss": 0.6682,
      "step": 2158600
    },
    {
      "epoch": 19.697605664647053,
      "grad_norm": 3.1331002712249756,
      "learning_rate": 3.358532861279412e-05,
      "loss": 0.6606,
      "step": 2158700
    },
    {
      "epoch": 19.69851814001022,
      "grad_norm": 3.817904472351074,
      "learning_rate": 3.358456821665815e-05,
      "loss": 0.6632,
      "step": 2158800
    },
    {
      "epoch": 19.699430615373384,
      "grad_norm": 4.114922523498535,
      "learning_rate": 3.358380782052218e-05,
      "loss": 0.667,
      "step": 2158900
    },
    {
      "epoch": 19.70034309073655,
      "grad_norm": 3.623981237411499,
      "learning_rate": 3.358304742438621e-05,
      "loss": 0.6419,
      "step": 2159000
    },
    {
      "epoch": 19.701255566099714,
      "grad_norm": 3.6907777786254883,
      "learning_rate": 3.358228702825024e-05,
      "loss": 0.6593,
      "step": 2159100
    },
    {
      "epoch": 19.70216804146288,
      "grad_norm": 3.7193613052368164,
      "learning_rate": 3.358152663211427e-05,
      "loss": 0.6471,
      "step": 2159200
    },
    {
      "epoch": 19.703080516826045,
      "grad_norm": 4.620020389556885,
      "learning_rate": 3.358076623597829e-05,
      "loss": 0.6555,
      "step": 2159300
    },
    {
      "epoch": 19.70399299218921,
      "grad_norm": 3.9298512935638428,
      "learning_rate": 3.358000583984233e-05,
      "loss": 0.6474,
      "step": 2159400
    },
    {
      "epoch": 19.704905467552376,
      "grad_norm": 4.441051959991455,
      "learning_rate": 3.357924544370635e-05,
      "loss": 0.6374,
      "step": 2159500
    },
    {
      "epoch": 19.70581794291554,
      "grad_norm": 4.0993242263793945,
      "learning_rate": 3.357848504757038e-05,
      "loss": 0.6613,
      "step": 2159600
    },
    {
      "epoch": 19.706730418278706,
      "grad_norm": 4.215902805328369,
      "learning_rate": 3.357772465143441e-05,
      "loss": 0.6739,
      "step": 2159700
    },
    {
      "epoch": 19.70764289364187,
      "grad_norm": 3.8329291343688965,
      "learning_rate": 3.357696425529844e-05,
      "loss": 0.6258,
      "step": 2159800
    },
    {
      "epoch": 19.708555369005037,
      "grad_norm": 3.641069173812866,
      "learning_rate": 3.3576203859162466e-05,
      "loss": 0.6793,
      "step": 2159900
    },
    {
      "epoch": 19.709467844368202,
      "grad_norm": 3.647505760192871,
      "learning_rate": 3.35754434630265e-05,
      "loss": 0.6616,
      "step": 2160000
    },
    {
      "epoch": 19.710380319731367,
      "grad_norm": 4.7650580406188965,
      "learning_rate": 3.3574683066890526e-05,
      "loss": 0.6559,
      "step": 2160100
    },
    {
      "epoch": 19.711292795094533,
      "grad_norm": 4.530752182006836,
      "learning_rate": 3.3573922670754556e-05,
      "loss": 0.6339,
      "step": 2160200
    },
    {
      "epoch": 19.712205270457698,
      "grad_norm": 3.8842227458953857,
      "learning_rate": 3.3573162274618586e-05,
      "loss": 0.6419,
      "step": 2160300
    },
    {
      "epoch": 19.713117745820863,
      "grad_norm": 4.326247215270996,
      "learning_rate": 3.3572401878482617e-05,
      "loss": 0.6696,
      "step": 2160400
    },
    {
      "epoch": 19.71403022118403,
      "grad_norm": 3.9436590671539307,
      "learning_rate": 3.357164148234665e-05,
      "loss": 0.6677,
      "step": 2160500
    },
    {
      "epoch": 19.714942696547194,
      "grad_norm": 3.730917453765869,
      "learning_rate": 3.357088108621068e-05,
      "loss": 0.6616,
      "step": 2160600
    },
    {
      "epoch": 19.71585517191036,
      "grad_norm": 4.348630428314209,
      "learning_rate": 3.35701206900747e-05,
      "loss": 0.6428,
      "step": 2160700
    },
    {
      "epoch": 19.716767647273524,
      "grad_norm": 5.263469219207764,
      "learning_rate": 3.356936029393874e-05,
      "loss": 0.6698,
      "step": 2160800
    },
    {
      "epoch": 19.71768012263669,
      "grad_norm": 4.3152689933776855,
      "learning_rate": 3.356859989780276e-05,
      "loss": 0.665,
      "step": 2160900
    },
    {
      "epoch": 19.718592597999855,
      "grad_norm": 4.087985992431641,
      "learning_rate": 3.356783950166679e-05,
      "loss": 0.6393,
      "step": 2161000
    },
    {
      "epoch": 19.71950507336302,
      "grad_norm": 3.8901917934417725,
      "learning_rate": 3.356707910553082e-05,
      "loss": 0.6424,
      "step": 2161100
    },
    {
      "epoch": 19.720417548726186,
      "grad_norm": 4.727505207061768,
      "learning_rate": 3.356631870939485e-05,
      "loss": 0.6621,
      "step": 2161200
    },
    {
      "epoch": 19.72133002408935,
      "grad_norm": 4.2225236892700195,
      "learning_rate": 3.3565558313258874e-05,
      "loss": 0.6354,
      "step": 2161300
    },
    {
      "epoch": 19.722242499452516,
      "grad_norm": 3.323652982711792,
      "learning_rate": 3.3564797917122904e-05,
      "loss": 0.653,
      "step": 2161400
    },
    {
      "epoch": 19.72315497481568,
      "grad_norm": 4.377522945404053,
      "learning_rate": 3.3564037520986934e-05,
      "loss": 0.6421,
      "step": 2161500
    },
    {
      "epoch": 19.724067450178847,
      "grad_norm": 4.260517120361328,
      "learning_rate": 3.3563277124850964e-05,
      "loss": 0.6326,
      "step": 2161600
    },
    {
      "epoch": 19.72497992554201,
      "grad_norm": 3.9709348678588867,
      "learning_rate": 3.3562516728714994e-05,
      "loss": 0.6736,
      "step": 2161700
    },
    {
      "epoch": 19.725892400905174,
      "grad_norm": 3.8475594520568848,
      "learning_rate": 3.356175633257902e-05,
      "loss": 0.644,
      "step": 2161800
    },
    {
      "epoch": 19.72680487626834,
      "grad_norm": 4.735799312591553,
      "learning_rate": 3.3560995936443054e-05,
      "loss": 0.6947,
      "step": 2161900
    },
    {
      "epoch": 19.727717351631505,
      "grad_norm": 3.1379125118255615,
      "learning_rate": 3.356023554030708e-05,
      "loss": 0.6447,
      "step": 2162000
    },
    {
      "epoch": 19.72862982699467,
      "grad_norm": 4.687188625335693,
      "learning_rate": 3.355947514417111e-05,
      "loss": 0.6285,
      "step": 2162100
    },
    {
      "epoch": 19.729542302357835,
      "grad_norm": 4.9084391593933105,
      "learning_rate": 3.355871474803514e-05,
      "loss": 0.6428,
      "step": 2162200
    },
    {
      "epoch": 19.730454777721,
      "grad_norm": 4.317116737365723,
      "learning_rate": 3.355795435189917e-05,
      "loss": 0.6626,
      "step": 2162300
    },
    {
      "epoch": 19.731367253084166,
      "grad_norm": 3.856174945831299,
      "learning_rate": 3.35571939557632e-05,
      "loss": 0.6614,
      "step": 2162400
    },
    {
      "epoch": 19.73227972844733,
      "grad_norm": 3.988837957382202,
      "learning_rate": 3.355643355962723e-05,
      "loss": 0.6483,
      "step": 2162500
    },
    {
      "epoch": 19.733192203810496,
      "grad_norm": 3.7556331157684326,
      "learning_rate": 3.355567316349125e-05,
      "loss": 0.6813,
      "step": 2162600
    },
    {
      "epoch": 19.73410467917366,
      "grad_norm": 4.162512302398682,
      "learning_rate": 3.355491276735528e-05,
      "loss": 0.6735,
      "step": 2162700
    },
    {
      "epoch": 19.735017154536827,
      "grad_norm": 4.676398754119873,
      "learning_rate": 3.355415237121931e-05,
      "loss": 0.6946,
      "step": 2162800
    },
    {
      "epoch": 19.735929629899992,
      "grad_norm": 3.61024808883667,
      "learning_rate": 3.355339197508334e-05,
      "loss": 0.6388,
      "step": 2162900
    },
    {
      "epoch": 19.736842105263158,
      "grad_norm": 4.71037483215332,
      "learning_rate": 3.355263157894737e-05,
      "loss": 0.6891,
      "step": 2163000
    },
    {
      "epoch": 19.737754580626323,
      "grad_norm": 3.761280059814453,
      "learning_rate": 3.35518711828114e-05,
      "loss": 0.6703,
      "step": 2163100
    },
    {
      "epoch": 19.738667055989488,
      "grad_norm": 3.7799248695373535,
      "learning_rate": 3.3551110786675425e-05,
      "loss": 0.5934,
      "step": 2163200
    },
    {
      "epoch": 19.739579531352653,
      "grad_norm": 3.920778751373291,
      "learning_rate": 3.355035039053946e-05,
      "loss": 0.6635,
      "step": 2163300
    },
    {
      "epoch": 19.74049200671582,
      "grad_norm": 4.275636196136475,
      "learning_rate": 3.3549589994403485e-05,
      "loss": 0.6915,
      "step": 2163400
    },
    {
      "epoch": 19.741404482078984,
      "grad_norm": 3.5742805004119873,
      "learning_rate": 3.3548829598267515e-05,
      "loss": 0.6651,
      "step": 2163500
    },
    {
      "epoch": 19.74231695744215,
      "grad_norm": 4.0840277671813965,
      "learning_rate": 3.3548069202131545e-05,
      "loss": 0.6383,
      "step": 2163600
    },
    {
      "epoch": 19.743229432805315,
      "grad_norm": 3.7437095642089844,
      "learning_rate": 3.3547308805995575e-05,
      "loss": 0.6599,
      "step": 2163700
    },
    {
      "epoch": 19.74414190816848,
      "grad_norm": 3.464157819747925,
      "learning_rate": 3.3546548409859605e-05,
      "loss": 0.6392,
      "step": 2163800
    },
    {
      "epoch": 19.745054383531645,
      "grad_norm": 4.256086826324463,
      "learning_rate": 3.3545788013723635e-05,
      "loss": 0.6441,
      "step": 2163900
    },
    {
      "epoch": 19.74596685889481,
      "grad_norm": 3.952136516571045,
      "learning_rate": 3.354502761758766e-05,
      "loss": 0.6237,
      "step": 2164000
    },
    {
      "epoch": 19.746879334257976,
      "grad_norm": 4.253393650054932,
      "learning_rate": 3.354426722145169e-05,
      "loss": 0.662,
      "step": 2164100
    },
    {
      "epoch": 19.74779180962114,
      "grad_norm": 3.527552366256714,
      "learning_rate": 3.354350682531572e-05,
      "loss": 0.6175,
      "step": 2164200
    },
    {
      "epoch": 19.748704284984306,
      "grad_norm": 3.1934335231781006,
      "learning_rate": 3.354274642917974e-05,
      "loss": 0.6506,
      "step": 2164300
    },
    {
      "epoch": 19.74961676034747,
      "grad_norm": 5.517451763153076,
      "learning_rate": 3.354198603304378e-05,
      "loss": 0.7085,
      "step": 2164400
    },
    {
      "epoch": 19.750529235710637,
      "grad_norm": 3.375830888748169,
      "learning_rate": 3.35412256369078e-05,
      "loss": 0.652,
      "step": 2164500
    },
    {
      "epoch": 19.751441711073802,
      "grad_norm": 3.4997127056121826,
      "learning_rate": 3.354046524077183e-05,
      "loss": 0.66,
      "step": 2164600
    },
    {
      "epoch": 19.752354186436968,
      "grad_norm": 3.94865083694458,
      "learning_rate": 3.353970484463586e-05,
      "loss": 0.6452,
      "step": 2164700
    },
    {
      "epoch": 19.753266661800133,
      "grad_norm": 3.231393814086914,
      "learning_rate": 3.353894444849989e-05,
      "loss": 0.6699,
      "step": 2164800
    },
    {
      "epoch": 19.754179137163298,
      "grad_norm": 3.559203624725342,
      "learning_rate": 3.353818405236392e-05,
      "loss": 0.6709,
      "step": 2164900
    },
    {
      "epoch": 19.755091612526464,
      "grad_norm": 4.33151388168335,
      "learning_rate": 3.353742365622795e-05,
      "loss": 0.6702,
      "step": 2165000
    },
    {
      "epoch": 19.75600408788963,
      "grad_norm": 4.635923385620117,
      "learning_rate": 3.3536663260091975e-05,
      "loss": 0.6091,
      "step": 2165100
    },
    {
      "epoch": 19.75691656325279,
      "grad_norm": 4.123528003692627,
      "learning_rate": 3.353590286395601e-05,
      "loss": 0.631,
      "step": 2165200
    },
    {
      "epoch": 19.757829038615956,
      "grad_norm": 4.399818420410156,
      "learning_rate": 3.3535142467820036e-05,
      "loss": 0.6632,
      "step": 2165300
    },
    {
      "epoch": 19.75874151397912,
      "grad_norm": 4.116817951202393,
      "learning_rate": 3.3534382071684066e-05,
      "loss": 0.6698,
      "step": 2165400
    },
    {
      "epoch": 19.759653989342286,
      "grad_norm": 3.953387975692749,
      "learning_rate": 3.3533621675548096e-05,
      "loss": 0.6556,
      "step": 2165500
    },
    {
      "epoch": 19.76056646470545,
      "grad_norm": 3.508967399597168,
      "learning_rate": 3.3532861279412126e-05,
      "loss": 0.6885,
      "step": 2165600
    },
    {
      "epoch": 19.761478940068617,
      "grad_norm": 5.632164478302002,
      "learning_rate": 3.353210088327615e-05,
      "loss": 0.6468,
      "step": 2165700
    },
    {
      "epoch": 19.762391415431782,
      "grad_norm": 3.7871625423431396,
      "learning_rate": 3.3531340487140186e-05,
      "loss": 0.6846,
      "step": 2165800
    },
    {
      "epoch": 19.763303890794948,
      "grad_norm": 3.7377326488494873,
      "learning_rate": 3.353058009100421e-05,
      "loss": 0.6614,
      "step": 2165900
    },
    {
      "epoch": 19.764216366158113,
      "grad_norm": 3.940840721130371,
      "learning_rate": 3.352981969486824e-05,
      "loss": 0.6747,
      "step": 2166000
    },
    {
      "epoch": 19.76512884152128,
      "grad_norm": 3.099757671356201,
      "learning_rate": 3.352905929873227e-05,
      "loss": 0.6616,
      "step": 2166100
    },
    {
      "epoch": 19.766041316884444,
      "grad_norm": 4.90543270111084,
      "learning_rate": 3.35282989025963e-05,
      "loss": 0.6636,
      "step": 2166200
    },
    {
      "epoch": 19.76695379224761,
      "grad_norm": 3.1848092079162598,
      "learning_rate": 3.352753850646033e-05,
      "loss": 0.6326,
      "step": 2166300
    },
    {
      "epoch": 19.767866267610774,
      "grad_norm": 4.423487663269043,
      "learning_rate": 3.352677811032436e-05,
      "loss": 0.6628,
      "step": 2166400
    },
    {
      "epoch": 19.76877874297394,
      "grad_norm": 2.8019447326660156,
      "learning_rate": 3.352601771418838e-05,
      "loss": 0.6163,
      "step": 2166500
    },
    {
      "epoch": 19.769691218337105,
      "grad_norm": 3.4965782165527344,
      "learning_rate": 3.352525731805242e-05,
      "loss": 0.6118,
      "step": 2166600
    },
    {
      "epoch": 19.77060369370027,
      "grad_norm": 3.9937093257904053,
      "learning_rate": 3.352449692191644e-05,
      "loss": 0.7084,
      "step": 2166700
    },
    {
      "epoch": 19.771516169063435,
      "grad_norm": 3.3602209091186523,
      "learning_rate": 3.352373652578047e-05,
      "loss": 0.6457,
      "step": 2166800
    },
    {
      "epoch": 19.7724286444266,
      "grad_norm": 3.856407403945923,
      "learning_rate": 3.35229761296445e-05,
      "loss": 0.6443,
      "step": 2166900
    },
    {
      "epoch": 19.773341119789766,
      "grad_norm": 3.1033968925476074,
      "learning_rate": 3.352221573350853e-05,
      "loss": 0.6407,
      "step": 2167000
    },
    {
      "epoch": 19.77425359515293,
      "grad_norm": 4.4570417404174805,
      "learning_rate": 3.3521455337372557e-05,
      "loss": 0.6776,
      "step": 2167100
    },
    {
      "epoch": 19.775166070516097,
      "grad_norm": 3.339617967605591,
      "learning_rate": 3.3520694941236587e-05,
      "loss": 0.6351,
      "step": 2167200
    },
    {
      "epoch": 19.776078545879262,
      "grad_norm": 3.890704870223999,
      "learning_rate": 3.351993454510062e-05,
      "loss": 0.659,
      "step": 2167300
    },
    {
      "epoch": 19.776991021242427,
      "grad_norm": 4.439866542816162,
      "learning_rate": 3.351917414896465e-05,
      "loss": 0.6842,
      "step": 2167400
    },
    {
      "epoch": 19.777903496605592,
      "grad_norm": 3.6741929054260254,
      "learning_rate": 3.351841375282868e-05,
      "loss": 0.6496,
      "step": 2167500
    },
    {
      "epoch": 19.778815971968758,
      "grad_norm": 4.352293968200684,
      "learning_rate": 3.35176533566927e-05,
      "loss": 0.6531,
      "step": 2167600
    },
    {
      "epoch": 19.779728447331923,
      "grad_norm": 4.948409557342529,
      "learning_rate": 3.351689296055674e-05,
      "loss": 0.6563,
      "step": 2167700
    },
    {
      "epoch": 19.78064092269509,
      "grad_norm": 4.520767688751221,
      "learning_rate": 3.351613256442076e-05,
      "loss": 0.6964,
      "step": 2167800
    },
    {
      "epoch": 19.781553398058254,
      "grad_norm": 3.2559306621551514,
      "learning_rate": 3.351537216828479e-05,
      "loss": 0.676,
      "step": 2167900
    },
    {
      "epoch": 19.78246587342142,
      "grad_norm": 4.2097296714782715,
      "learning_rate": 3.351461177214882e-05,
      "loss": 0.6544,
      "step": 2168000
    },
    {
      "epoch": 19.783378348784584,
      "grad_norm": 4.198751449584961,
      "learning_rate": 3.351385137601285e-05,
      "loss": 0.6685,
      "step": 2168100
    },
    {
      "epoch": 19.78429082414775,
      "grad_norm": 4.057137489318848,
      "learning_rate": 3.3513090979876874e-05,
      "loss": 0.6732,
      "step": 2168200
    },
    {
      "epoch": 19.785203299510915,
      "grad_norm": 3.547178268432617,
      "learning_rate": 3.351233058374091e-05,
      "loss": 0.6466,
      "step": 2168300
    },
    {
      "epoch": 19.786115774874077,
      "grad_norm": 3.9265780448913574,
      "learning_rate": 3.3511570187604934e-05,
      "loss": 0.6409,
      "step": 2168400
    },
    {
      "epoch": 19.787028250237242,
      "grad_norm": 3.8000402450561523,
      "learning_rate": 3.3510809791468964e-05,
      "loss": 0.6432,
      "step": 2168500
    },
    {
      "epoch": 19.787940725600407,
      "grad_norm": 4.407390117645264,
      "learning_rate": 3.3510049395332994e-05,
      "loss": 0.6765,
      "step": 2168600
    },
    {
      "epoch": 19.788853200963572,
      "grad_norm": 3.6685678958892822,
      "learning_rate": 3.3509288999197024e-05,
      "loss": 0.6464,
      "step": 2168700
    },
    {
      "epoch": 19.789765676326738,
      "grad_norm": 5.097718715667725,
      "learning_rate": 3.3508528603061054e-05,
      "loss": 0.6497,
      "step": 2168800
    },
    {
      "epoch": 19.790678151689903,
      "grad_norm": 4.490697383880615,
      "learning_rate": 3.3507768206925084e-05,
      "loss": 0.67,
      "step": 2168900
    },
    {
      "epoch": 19.79159062705307,
      "grad_norm": 2.5275535583496094,
      "learning_rate": 3.350700781078911e-05,
      "loss": 0.6395,
      "step": 2169000
    },
    {
      "epoch": 19.792503102416234,
      "grad_norm": 4.323239803314209,
      "learning_rate": 3.3506247414653144e-05,
      "loss": 0.6493,
      "step": 2169100
    },
    {
      "epoch": 19.7934155777794,
      "grad_norm": 4.767548084259033,
      "learning_rate": 3.350548701851717e-05,
      "loss": 0.6573,
      "step": 2169200
    },
    {
      "epoch": 19.794328053142564,
      "grad_norm": 4.212225914001465,
      "learning_rate": 3.35047266223812e-05,
      "loss": 0.659,
      "step": 2169300
    },
    {
      "epoch": 19.79524052850573,
      "grad_norm": 3.3563787937164307,
      "learning_rate": 3.350396622624523e-05,
      "loss": 0.6388,
      "step": 2169400
    },
    {
      "epoch": 19.796153003868895,
      "grad_norm": 3.412980079650879,
      "learning_rate": 3.350320583010926e-05,
      "loss": 0.6748,
      "step": 2169500
    },
    {
      "epoch": 19.79706547923206,
      "grad_norm": 4.001012802124023,
      "learning_rate": 3.350244543397328e-05,
      "loss": 0.6638,
      "step": 2169600
    },
    {
      "epoch": 19.797977954595225,
      "grad_norm": 4.795766830444336,
      "learning_rate": 3.350168503783732e-05,
      "loss": 0.6305,
      "step": 2169700
    },
    {
      "epoch": 19.79889042995839,
      "grad_norm": 4.298096179962158,
      "learning_rate": 3.350092464170134e-05,
      "loss": 0.6332,
      "step": 2169800
    },
    {
      "epoch": 19.799802905321556,
      "grad_norm": 4.4103007316589355,
      "learning_rate": 3.350016424556537e-05,
      "loss": 0.6682,
      "step": 2169900
    },
    {
      "epoch": 19.80071538068472,
      "grad_norm": 4.444910049438477,
      "learning_rate": 3.34994038494294e-05,
      "loss": 0.6501,
      "step": 2170000
    },
    {
      "epoch": 19.801627856047887,
      "grad_norm": 3.8186752796173096,
      "learning_rate": 3.3498643453293425e-05,
      "loss": 0.6831,
      "step": 2170100
    },
    {
      "epoch": 19.802540331411052,
      "grad_norm": 4.025765419006348,
      "learning_rate": 3.349788305715746e-05,
      "loss": 0.6019,
      "step": 2170200
    },
    {
      "epoch": 19.803452806774217,
      "grad_norm": 4.600710868835449,
      "learning_rate": 3.3497122661021485e-05,
      "loss": 0.673,
      "step": 2170300
    },
    {
      "epoch": 19.804365282137383,
      "grad_norm": 4.216236591339111,
      "learning_rate": 3.3496362264885515e-05,
      "loss": 0.6908,
      "step": 2170400
    },
    {
      "epoch": 19.805277757500548,
      "grad_norm": 4.242452621459961,
      "learning_rate": 3.3495601868749545e-05,
      "loss": 0.655,
      "step": 2170500
    },
    {
      "epoch": 19.806190232863713,
      "grad_norm": 3.801812171936035,
      "learning_rate": 3.3494841472613575e-05,
      "loss": 0.6471,
      "step": 2170600
    },
    {
      "epoch": 19.80710270822688,
      "grad_norm": 3.2605130672454834,
      "learning_rate": 3.34940810764776e-05,
      "loss": 0.6121,
      "step": 2170700
    },
    {
      "epoch": 19.808015183590044,
      "grad_norm": 4.547389507293701,
      "learning_rate": 3.3493320680341635e-05,
      "loss": 0.6332,
      "step": 2170800
    },
    {
      "epoch": 19.80892765895321,
      "grad_norm": 4.288614273071289,
      "learning_rate": 3.349256028420566e-05,
      "loss": 0.6732,
      "step": 2170900
    },
    {
      "epoch": 19.809840134316374,
      "grad_norm": 4.2113165855407715,
      "learning_rate": 3.349179988806969e-05,
      "loss": 0.6557,
      "step": 2171000
    },
    {
      "epoch": 19.81075260967954,
      "grad_norm": 3.8907437324523926,
      "learning_rate": 3.349103949193372e-05,
      "loss": 0.6655,
      "step": 2171100
    },
    {
      "epoch": 19.811665085042705,
      "grad_norm": 3.4719462394714355,
      "learning_rate": 3.349027909579775e-05,
      "loss": 0.6677,
      "step": 2171200
    },
    {
      "epoch": 19.81257756040587,
      "grad_norm": 4.4112653732299805,
      "learning_rate": 3.348951869966178e-05,
      "loss": 0.6691,
      "step": 2171300
    },
    {
      "epoch": 19.813490035769036,
      "grad_norm": 5.567113399505615,
      "learning_rate": 3.348875830352581e-05,
      "loss": 0.6722,
      "step": 2171400
    },
    {
      "epoch": 19.8144025111322,
      "grad_norm": 4.804344177246094,
      "learning_rate": 3.348799790738983e-05,
      "loss": 0.6625,
      "step": 2171500
    },
    {
      "epoch": 19.815314986495366,
      "grad_norm": 3.958815336227417,
      "learning_rate": 3.348723751125387e-05,
      "loss": 0.649,
      "step": 2171600
    },
    {
      "epoch": 19.81622746185853,
      "grad_norm": 5.434484958648682,
      "learning_rate": 3.348647711511789e-05,
      "loss": 0.6499,
      "step": 2171700
    },
    {
      "epoch": 19.817139937221697,
      "grad_norm": 3.7737159729003906,
      "learning_rate": 3.348571671898192e-05,
      "loss": 0.6832,
      "step": 2171800
    },
    {
      "epoch": 19.81805241258486,
      "grad_norm": 3.390110492706299,
      "learning_rate": 3.348495632284595e-05,
      "loss": 0.6773,
      "step": 2171900
    },
    {
      "epoch": 19.818964887948024,
      "grad_norm": 3.9320855140686035,
      "learning_rate": 3.348419592670998e-05,
      "loss": 0.6329,
      "step": 2172000
    },
    {
      "epoch": 19.81987736331119,
      "grad_norm": 4.878382682800293,
      "learning_rate": 3.3483435530574006e-05,
      "loss": 0.6458,
      "step": 2172100
    },
    {
      "epoch": 19.820789838674354,
      "grad_norm": 3.612898588180542,
      "learning_rate": 3.348267513443804e-05,
      "loss": 0.6761,
      "step": 2172200
    },
    {
      "epoch": 19.82170231403752,
      "grad_norm": 3.770423412322998,
      "learning_rate": 3.3481914738302066e-05,
      "loss": 0.7084,
      "step": 2172300
    },
    {
      "epoch": 19.822614789400685,
      "grad_norm": 3.645106792449951,
      "learning_rate": 3.3481154342166096e-05,
      "loss": 0.6528,
      "step": 2172400
    },
    {
      "epoch": 19.82352726476385,
      "grad_norm": 4.569147109985352,
      "learning_rate": 3.3480393946030126e-05,
      "loss": 0.6914,
      "step": 2172500
    },
    {
      "epoch": 19.824439740127016,
      "grad_norm": 4.488712787628174,
      "learning_rate": 3.3479633549894156e-05,
      "loss": 0.6543,
      "step": 2172600
    },
    {
      "epoch": 19.82535221549018,
      "grad_norm": 3.5224764347076416,
      "learning_rate": 3.3478873153758186e-05,
      "loss": 0.6556,
      "step": 2172700
    },
    {
      "epoch": 19.826264690853346,
      "grad_norm": 3.6990292072296143,
      "learning_rate": 3.347811275762221e-05,
      "loss": 0.6311,
      "step": 2172800
    },
    {
      "epoch": 19.82717716621651,
      "grad_norm": 3.940263509750366,
      "learning_rate": 3.347735236148624e-05,
      "loss": 0.642,
      "step": 2172900
    },
    {
      "epoch": 19.828089641579677,
      "grad_norm": 3.341305732727051,
      "learning_rate": 3.347659196535027e-05,
      "loss": 0.6538,
      "step": 2173000
    },
    {
      "epoch": 19.829002116942842,
      "grad_norm": 4.17901086807251,
      "learning_rate": 3.34758315692143e-05,
      "loss": 0.6895,
      "step": 2173100
    },
    {
      "epoch": 19.829914592306007,
      "grad_norm": 3.5350120067596436,
      "learning_rate": 3.347507117307832e-05,
      "loss": 0.6843,
      "step": 2173200
    },
    {
      "epoch": 19.830827067669173,
      "grad_norm": 4.019009590148926,
      "learning_rate": 3.347431077694236e-05,
      "loss": 0.6514,
      "step": 2173300
    },
    {
      "epoch": 19.831739543032338,
      "grad_norm": 4.7023091316223145,
      "learning_rate": 3.347355038080638e-05,
      "loss": 0.6895,
      "step": 2173400
    },
    {
      "epoch": 19.832652018395503,
      "grad_norm": 3.3227832317352295,
      "learning_rate": 3.347278998467041e-05,
      "loss": 0.6729,
      "step": 2173500
    },
    {
      "epoch": 19.83356449375867,
      "grad_norm": 4.2525634765625,
      "learning_rate": 3.347202958853444e-05,
      "loss": 0.6286,
      "step": 2173600
    },
    {
      "epoch": 19.834476969121834,
      "grad_norm": 4.034518241882324,
      "learning_rate": 3.347126919239847e-05,
      "loss": 0.6681,
      "step": 2173700
    },
    {
      "epoch": 19.835389444485,
      "grad_norm": 3.8537847995758057,
      "learning_rate": 3.34705087962625e-05,
      "loss": 0.6205,
      "step": 2173800
    },
    {
      "epoch": 19.836301919848164,
      "grad_norm": 3.499452590942383,
      "learning_rate": 3.346974840012653e-05,
      "loss": 0.6866,
      "step": 2173900
    },
    {
      "epoch": 19.83721439521133,
      "grad_norm": 4.4927897453308105,
      "learning_rate": 3.3468988003990557e-05,
      "loss": 0.6902,
      "step": 2174000
    },
    {
      "epoch": 19.838126870574495,
      "grad_norm": 5.467358112335205,
      "learning_rate": 3.3468227607854593e-05,
      "loss": 0.6442,
      "step": 2174100
    },
    {
      "epoch": 19.83903934593766,
      "grad_norm": 3.751681089401245,
      "learning_rate": 3.346746721171862e-05,
      "loss": 0.7058,
      "step": 2174200
    },
    {
      "epoch": 19.839951821300826,
      "grad_norm": 3.6431350708007812,
      "learning_rate": 3.346670681558265e-05,
      "loss": 0.6529,
      "step": 2174300
    },
    {
      "epoch": 19.84086429666399,
      "grad_norm": 3.854820966720581,
      "learning_rate": 3.346594641944668e-05,
      "loss": 0.6617,
      "step": 2174400
    },
    {
      "epoch": 19.841776772027156,
      "grad_norm": 4.804774761199951,
      "learning_rate": 3.346518602331071e-05,
      "loss": 0.6953,
      "step": 2174500
    },
    {
      "epoch": 19.84268924739032,
      "grad_norm": 4.84568977355957,
      "learning_rate": 3.346442562717473e-05,
      "loss": 0.6881,
      "step": 2174600
    },
    {
      "epoch": 19.843601722753487,
      "grad_norm": 4.690847873687744,
      "learning_rate": 3.346366523103877e-05,
      "loss": 0.651,
      "step": 2174700
    },
    {
      "epoch": 19.844514198116652,
      "grad_norm": 3.869927167892456,
      "learning_rate": 3.346290483490279e-05,
      "loss": 0.6708,
      "step": 2174800
    },
    {
      "epoch": 19.845426673479817,
      "grad_norm": 2.975463390350342,
      "learning_rate": 3.346214443876682e-05,
      "loss": 0.6421,
      "step": 2174900
    },
    {
      "epoch": 19.846339148842983,
      "grad_norm": 4.0875983238220215,
      "learning_rate": 3.346138404263085e-05,
      "loss": 0.63,
      "step": 2175000
    },
    {
      "epoch": 19.847251624206148,
      "grad_norm": 4.462873458862305,
      "learning_rate": 3.346062364649488e-05,
      "loss": 0.6708,
      "step": 2175100
    },
    {
      "epoch": 19.84816409956931,
      "grad_norm": 3.907480001449585,
      "learning_rate": 3.345986325035891e-05,
      "loss": 0.6737,
      "step": 2175200
    },
    {
      "epoch": 19.849076574932475,
      "grad_norm": 3.551454782485962,
      "learning_rate": 3.345910285422294e-05,
      "loss": 0.6418,
      "step": 2175300
    },
    {
      "epoch": 19.84998905029564,
      "grad_norm": 4.023479461669922,
      "learning_rate": 3.3458342458086964e-05,
      "loss": 0.6847,
      "step": 2175400
    },
    {
      "epoch": 19.850901525658806,
      "grad_norm": 4.69552755355835,
      "learning_rate": 3.3457582061951e-05,
      "loss": 0.6672,
      "step": 2175500
    },
    {
      "epoch": 19.85181400102197,
      "grad_norm": 3.3289549350738525,
      "learning_rate": 3.3456821665815024e-05,
      "loss": 0.6579,
      "step": 2175600
    },
    {
      "epoch": 19.852726476385136,
      "grad_norm": 3.834646463394165,
      "learning_rate": 3.3456061269679054e-05,
      "loss": 0.636,
      "step": 2175700
    },
    {
      "epoch": 19.8536389517483,
      "grad_norm": 3.797551155090332,
      "learning_rate": 3.3455300873543084e-05,
      "loss": 0.648,
      "step": 2175800
    },
    {
      "epoch": 19.854551427111467,
      "grad_norm": 3.3333816528320312,
      "learning_rate": 3.345454047740711e-05,
      "loss": 0.6669,
      "step": 2175900
    },
    {
      "epoch": 19.855463902474632,
      "grad_norm": 4.187364101409912,
      "learning_rate": 3.3453780081271144e-05,
      "loss": 0.6475,
      "step": 2176000
    },
    {
      "epoch": 19.856376377837798,
      "grad_norm": 3.6365926265716553,
      "learning_rate": 3.345301968513517e-05,
      "loss": 0.6586,
      "step": 2176100
    },
    {
      "epoch": 19.857288853200963,
      "grad_norm": 4.391917705535889,
      "learning_rate": 3.34522592889992e-05,
      "loss": 0.6285,
      "step": 2176200
    },
    {
      "epoch": 19.858201328564128,
      "grad_norm": 5.103077411651611,
      "learning_rate": 3.345149889286323e-05,
      "loss": 0.6549,
      "step": 2176300
    },
    {
      "epoch": 19.859113803927293,
      "grad_norm": 4.269336223602295,
      "learning_rate": 3.345073849672726e-05,
      "loss": 0.66,
      "step": 2176400
    },
    {
      "epoch": 19.86002627929046,
      "grad_norm": 3.4911532402038574,
      "learning_rate": 3.344997810059128e-05,
      "loss": 0.6541,
      "step": 2176500
    },
    {
      "epoch": 19.860938754653624,
      "grad_norm": 4.82632303237915,
      "learning_rate": 3.344921770445532e-05,
      "loss": 0.6723,
      "step": 2176600
    },
    {
      "epoch": 19.86185123001679,
      "grad_norm": 3.79315185546875,
      "learning_rate": 3.344845730831934e-05,
      "loss": 0.6673,
      "step": 2176700
    },
    {
      "epoch": 19.862763705379955,
      "grad_norm": 4.560050964355469,
      "learning_rate": 3.344769691218337e-05,
      "loss": 0.6522,
      "step": 2176800
    },
    {
      "epoch": 19.86367618074312,
      "grad_norm": 3.5449912548065186,
      "learning_rate": 3.34469365160474e-05,
      "loss": 0.6653,
      "step": 2176900
    },
    {
      "epoch": 19.864588656106285,
      "grad_norm": 3.7508552074432373,
      "learning_rate": 3.344617611991143e-05,
      "loss": 0.6404,
      "step": 2177000
    },
    {
      "epoch": 19.86550113146945,
      "grad_norm": 4.503249168395996,
      "learning_rate": 3.344541572377546e-05,
      "loss": 0.6938,
      "step": 2177100
    },
    {
      "epoch": 19.866413606832616,
      "grad_norm": 3.8271231651306152,
      "learning_rate": 3.344465532763949e-05,
      "loss": 0.6642,
      "step": 2177200
    },
    {
      "epoch": 19.86732608219578,
      "grad_norm": 3.254849910736084,
      "learning_rate": 3.3443894931503515e-05,
      "loss": 0.6362,
      "step": 2177300
    },
    {
      "epoch": 19.868238557558946,
      "grad_norm": 3.241854190826416,
      "learning_rate": 3.344313453536755e-05,
      "loss": 0.6517,
      "step": 2177400
    },
    {
      "epoch": 19.86915103292211,
      "grad_norm": 3.190459966659546,
      "learning_rate": 3.3442374139231575e-05,
      "loss": 0.647,
      "step": 2177500
    },
    {
      "epoch": 19.870063508285277,
      "grad_norm": 5.219183444976807,
      "learning_rate": 3.3441613743095605e-05,
      "loss": 0.6773,
      "step": 2177600
    },
    {
      "epoch": 19.870975983648442,
      "grad_norm": 4.294064521789551,
      "learning_rate": 3.3440853346959635e-05,
      "loss": 0.6462,
      "step": 2177700
    },
    {
      "epoch": 19.871888459011608,
      "grad_norm": 3.6048128604888916,
      "learning_rate": 3.3440092950823665e-05,
      "loss": 0.6272,
      "step": 2177800
    },
    {
      "epoch": 19.872800934374773,
      "grad_norm": 3.4527504444122314,
      "learning_rate": 3.343933255468769e-05,
      "loss": 0.6689,
      "step": 2177900
    },
    {
      "epoch": 19.873713409737938,
      "grad_norm": 4.44175910949707,
      "learning_rate": 3.3438572158551725e-05,
      "loss": 0.6672,
      "step": 2178000
    },
    {
      "epoch": 19.874625885101104,
      "grad_norm": 3.6582422256469727,
      "learning_rate": 3.343781176241575e-05,
      "loss": 0.6607,
      "step": 2178100
    },
    {
      "epoch": 19.87553836046427,
      "grad_norm": 4.082878589630127,
      "learning_rate": 3.343705136627978e-05,
      "loss": 0.6683,
      "step": 2178200
    },
    {
      "epoch": 19.876450835827434,
      "grad_norm": 4.483198165893555,
      "learning_rate": 3.343629097014381e-05,
      "loss": 0.6661,
      "step": 2178300
    },
    {
      "epoch": 19.8773633111906,
      "grad_norm": 3.7273268699645996,
      "learning_rate": 3.343553057400783e-05,
      "loss": 0.6474,
      "step": 2178400
    },
    {
      "epoch": 19.878275786553765,
      "grad_norm": 3.339099884033203,
      "learning_rate": 3.343477017787187e-05,
      "loss": 0.6685,
      "step": 2178500
    },
    {
      "epoch": 19.87918826191693,
      "grad_norm": 4.678196430206299,
      "learning_rate": 3.343400978173589e-05,
      "loss": 0.655,
      "step": 2178600
    },
    {
      "epoch": 19.88010073728009,
      "grad_norm": 2.5891079902648926,
      "learning_rate": 3.343324938559992e-05,
      "loss": 0.6498,
      "step": 2178700
    },
    {
      "epoch": 19.881013212643257,
      "grad_norm": 3.5718235969543457,
      "learning_rate": 3.343248898946395e-05,
      "loss": 0.6477,
      "step": 2178800
    },
    {
      "epoch": 19.881925688006422,
      "grad_norm": 4.015026569366455,
      "learning_rate": 3.343172859332798e-05,
      "loss": 0.6493,
      "step": 2178900
    },
    {
      "epoch": 19.882838163369588,
      "grad_norm": 4.704263687133789,
      "learning_rate": 3.3430968197192006e-05,
      "loss": 0.6667,
      "step": 2179000
    },
    {
      "epoch": 19.883750638732753,
      "grad_norm": 4.125216007232666,
      "learning_rate": 3.343020780105604e-05,
      "loss": 0.6686,
      "step": 2179100
    },
    {
      "epoch": 19.88466311409592,
      "grad_norm": 4.490133762359619,
      "learning_rate": 3.3429447404920066e-05,
      "loss": 0.6476,
      "step": 2179200
    },
    {
      "epoch": 19.885575589459084,
      "grad_norm": 3.9766459465026855,
      "learning_rate": 3.3428687008784096e-05,
      "loss": 0.671,
      "step": 2179300
    },
    {
      "epoch": 19.88648806482225,
      "grad_norm": 3.539768695831299,
      "learning_rate": 3.3427926612648126e-05,
      "loss": 0.6427,
      "step": 2179400
    },
    {
      "epoch": 19.887400540185414,
      "grad_norm": 4.368009090423584,
      "learning_rate": 3.3427166216512156e-05,
      "loss": 0.6342,
      "step": 2179500
    },
    {
      "epoch": 19.88831301554858,
      "grad_norm": 3.1979596614837646,
      "learning_rate": 3.3426405820376186e-05,
      "loss": 0.6381,
      "step": 2179600
    },
    {
      "epoch": 19.889225490911745,
      "grad_norm": 3.9143989086151123,
      "learning_rate": 3.3425645424240216e-05,
      "loss": 0.7006,
      "step": 2179700
    },
    {
      "epoch": 19.89013796627491,
      "grad_norm": 4.314634799957275,
      "learning_rate": 3.342488502810424e-05,
      "loss": 0.6772,
      "step": 2179800
    },
    {
      "epoch": 19.891050441638075,
      "grad_norm": 4.056105613708496,
      "learning_rate": 3.3424124631968276e-05,
      "loss": 0.7006,
      "step": 2179900
    },
    {
      "epoch": 19.89196291700124,
      "grad_norm": 3.5477373600006104,
      "learning_rate": 3.34233642358323e-05,
      "loss": 0.6764,
      "step": 2180000
    },
    {
      "epoch": 19.892875392364406,
      "grad_norm": 3.5887629985809326,
      "learning_rate": 3.342260383969633e-05,
      "loss": 0.6798,
      "step": 2180100
    },
    {
      "epoch": 19.89378786772757,
      "grad_norm": 3.440715789794922,
      "learning_rate": 3.342184344356036e-05,
      "loss": 0.6476,
      "step": 2180200
    },
    {
      "epoch": 19.894700343090737,
      "grad_norm": 4.422971725463867,
      "learning_rate": 3.342108304742439e-05,
      "loss": 0.6944,
      "step": 2180300
    },
    {
      "epoch": 19.895612818453902,
      "grad_norm": 4.24985933303833,
      "learning_rate": 3.342032265128841e-05,
      "loss": 0.6512,
      "step": 2180400
    },
    {
      "epoch": 19.896525293817067,
      "grad_norm": 4.218108654022217,
      "learning_rate": 3.341956225515245e-05,
      "loss": 0.6798,
      "step": 2180500
    },
    {
      "epoch": 19.897437769180232,
      "grad_norm": 2.655585289001465,
      "learning_rate": 3.341880185901647e-05,
      "loss": 0.6268,
      "step": 2180600
    },
    {
      "epoch": 19.898350244543398,
      "grad_norm": 4.30967903137207,
      "learning_rate": 3.34180414628805e-05,
      "loss": 0.6259,
      "step": 2180700
    },
    {
      "epoch": 19.899262719906563,
      "grad_norm": 4.06139612197876,
      "learning_rate": 3.3417281066744533e-05,
      "loss": 0.6733,
      "step": 2180800
    },
    {
      "epoch": 19.90017519526973,
      "grad_norm": 4.085827827453613,
      "learning_rate": 3.3416520670608563e-05,
      "loss": 0.7048,
      "step": 2180900
    },
    {
      "epoch": 19.901087670632894,
      "grad_norm": 3.1835334300994873,
      "learning_rate": 3.3415760274472594e-05,
      "loss": 0.6685,
      "step": 2181000
    },
    {
      "epoch": 19.90200014599606,
      "grad_norm": 4.6017351150512695,
      "learning_rate": 3.3414999878336624e-05,
      "loss": 0.6946,
      "step": 2181100
    },
    {
      "epoch": 19.902912621359224,
      "grad_norm": 3.9312219619750977,
      "learning_rate": 3.341423948220065e-05,
      "loss": 0.6424,
      "step": 2181200
    },
    {
      "epoch": 19.90382509672239,
      "grad_norm": 3.6237711906433105,
      "learning_rate": 3.341347908606468e-05,
      "loss": 0.6393,
      "step": 2181300
    },
    {
      "epoch": 19.904737572085555,
      "grad_norm": 4.444231033325195,
      "learning_rate": 3.341271868992871e-05,
      "loss": 0.667,
      "step": 2181400
    },
    {
      "epoch": 19.90565004744872,
      "grad_norm": 5.04072904586792,
      "learning_rate": 3.341195829379273e-05,
      "loss": 0.6384,
      "step": 2181500
    },
    {
      "epoch": 19.906562522811885,
      "grad_norm": 3.868170738220215,
      "learning_rate": 3.341119789765677e-05,
      "loss": 0.6603,
      "step": 2181600
    },
    {
      "epoch": 19.90747499817505,
      "grad_norm": 4.057698726654053,
      "learning_rate": 3.341043750152079e-05,
      "loss": 0.6775,
      "step": 2181700
    },
    {
      "epoch": 19.908387473538216,
      "grad_norm": 3.1792030334472656,
      "learning_rate": 3.340967710538482e-05,
      "loss": 0.6592,
      "step": 2181800
    },
    {
      "epoch": 19.90929994890138,
      "grad_norm": 4.140980243682861,
      "learning_rate": 3.340891670924885e-05,
      "loss": 0.6492,
      "step": 2181900
    },
    {
      "epoch": 19.910212424264543,
      "grad_norm": 4.735257625579834,
      "learning_rate": 3.340815631311288e-05,
      "loss": 0.6473,
      "step": 2182000
    },
    {
      "epoch": 19.91112489962771,
      "grad_norm": 4.979235649108887,
      "learning_rate": 3.340739591697691e-05,
      "loss": 0.6723,
      "step": 2182100
    },
    {
      "epoch": 19.912037374990874,
      "grad_norm": 4.280767917633057,
      "learning_rate": 3.340663552084094e-05,
      "loss": 0.6549,
      "step": 2182200
    },
    {
      "epoch": 19.91294985035404,
      "grad_norm": 3.4579994678497314,
      "learning_rate": 3.3405875124704964e-05,
      "loss": 0.6013,
      "step": 2182300
    },
    {
      "epoch": 19.913862325717204,
      "grad_norm": 3.4516685009002686,
      "learning_rate": 3.3405114728569e-05,
      "loss": 0.6621,
      "step": 2182400
    },
    {
      "epoch": 19.91477480108037,
      "grad_norm": 3.9089009761810303,
      "learning_rate": 3.3404354332433024e-05,
      "loss": 0.6602,
      "step": 2182500
    },
    {
      "epoch": 19.915687276443535,
      "grad_norm": 4.637636184692383,
      "learning_rate": 3.3403593936297054e-05,
      "loss": 0.6559,
      "step": 2182600
    },
    {
      "epoch": 19.9165997518067,
      "grad_norm": 3.561129331588745,
      "learning_rate": 3.3402833540161084e-05,
      "loss": 0.6805,
      "step": 2182700
    },
    {
      "epoch": 19.917512227169865,
      "grad_norm": 3.8250889778137207,
      "learning_rate": 3.3402073144025114e-05,
      "loss": 0.6796,
      "step": 2182800
    },
    {
      "epoch": 19.91842470253303,
      "grad_norm": 4.545287609100342,
      "learning_rate": 3.340131274788914e-05,
      "loss": 0.6391,
      "step": 2182900
    },
    {
      "epoch": 19.919337177896196,
      "grad_norm": 4.877364635467529,
      "learning_rate": 3.3400552351753175e-05,
      "loss": 0.6665,
      "step": 2183000
    },
    {
      "epoch": 19.92024965325936,
      "grad_norm": 2.8773155212402344,
      "learning_rate": 3.33997919556172e-05,
      "loss": 0.6555,
      "step": 2183100
    },
    {
      "epoch": 19.921162128622527,
      "grad_norm": 3.7159721851348877,
      "learning_rate": 3.339903155948123e-05,
      "loss": 0.6726,
      "step": 2183200
    },
    {
      "epoch": 19.922074603985692,
      "grad_norm": 3.579901695251465,
      "learning_rate": 3.339827116334526e-05,
      "loss": 0.6459,
      "step": 2183300
    },
    {
      "epoch": 19.922987079348857,
      "grad_norm": 4.028712749481201,
      "learning_rate": 3.339751076720929e-05,
      "loss": 0.6567,
      "step": 2183400
    },
    {
      "epoch": 19.923899554712023,
      "grad_norm": 3.6713507175445557,
      "learning_rate": 3.339675037107332e-05,
      "loss": 0.6643,
      "step": 2183500
    },
    {
      "epoch": 19.924812030075188,
      "grad_norm": 3.808837413787842,
      "learning_rate": 3.339598997493735e-05,
      "loss": 0.6258,
      "step": 2183600
    },
    {
      "epoch": 19.925724505438353,
      "grad_norm": 3.9023120403289795,
      "learning_rate": 3.339522957880137e-05,
      "loss": 0.65,
      "step": 2183700
    },
    {
      "epoch": 19.92663698080152,
      "grad_norm": 4.097984313964844,
      "learning_rate": 3.339446918266541e-05,
      "loss": 0.7063,
      "step": 2183800
    },
    {
      "epoch": 19.927549456164684,
      "grad_norm": 2.7953407764434814,
      "learning_rate": 3.339370878652943e-05,
      "loss": 0.6721,
      "step": 2183900
    },
    {
      "epoch": 19.92846193152785,
      "grad_norm": 2.4484870433807373,
      "learning_rate": 3.339294839039346e-05,
      "loss": 0.6728,
      "step": 2184000
    },
    {
      "epoch": 19.929374406891014,
      "grad_norm": 3.6993491649627686,
      "learning_rate": 3.339218799425749e-05,
      "loss": 0.6358,
      "step": 2184100
    },
    {
      "epoch": 19.93028688225418,
      "grad_norm": 4.552591323852539,
      "learning_rate": 3.3391427598121515e-05,
      "loss": 0.6869,
      "step": 2184200
    },
    {
      "epoch": 19.931199357617345,
      "grad_norm": 3.5505926609039307,
      "learning_rate": 3.3390667201985545e-05,
      "loss": 0.6639,
      "step": 2184300
    },
    {
      "epoch": 19.93211183298051,
      "grad_norm": 3.8998312950134277,
      "learning_rate": 3.3389906805849575e-05,
      "loss": 0.6595,
      "step": 2184400
    },
    {
      "epoch": 19.933024308343676,
      "grad_norm": 4.372396469116211,
      "learning_rate": 3.3389146409713605e-05,
      "loss": 0.6322,
      "step": 2184500
    },
    {
      "epoch": 19.93393678370684,
      "grad_norm": 3.9266655445098877,
      "learning_rate": 3.3388386013577635e-05,
      "loss": 0.6677,
      "step": 2184600
    },
    {
      "epoch": 19.934849259070006,
      "grad_norm": 2.613952875137329,
      "learning_rate": 3.3387625617441665e-05,
      "loss": 0.6595,
      "step": 2184700
    },
    {
      "epoch": 19.93576173443317,
      "grad_norm": 4.209807872772217,
      "learning_rate": 3.338686522130569e-05,
      "loss": 0.6742,
      "step": 2184800
    },
    {
      "epoch": 19.936674209796337,
      "grad_norm": 4.0670247077941895,
      "learning_rate": 3.3386104825169726e-05,
      "loss": 0.6566,
      "step": 2184900
    },
    {
      "epoch": 19.937586685159502,
      "grad_norm": 2.3899033069610596,
      "learning_rate": 3.338534442903375e-05,
      "loss": 0.6728,
      "step": 2185000
    },
    {
      "epoch": 19.938499160522667,
      "grad_norm": 4.940489768981934,
      "learning_rate": 3.338458403289778e-05,
      "loss": 0.6787,
      "step": 2185100
    },
    {
      "epoch": 19.939411635885833,
      "grad_norm": 3.901524305343628,
      "learning_rate": 3.338382363676181e-05,
      "loss": 0.6326,
      "step": 2185200
    },
    {
      "epoch": 19.940324111248998,
      "grad_norm": 4.605539321899414,
      "learning_rate": 3.338306324062584e-05,
      "loss": 0.6729,
      "step": 2185300
    },
    {
      "epoch": 19.941236586612163,
      "grad_norm": 3.9579358100891113,
      "learning_rate": 3.338230284448986e-05,
      "loss": 0.6538,
      "step": 2185400
    },
    {
      "epoch": 19.942149061975325,
      "grad_norm": 3.9771084785461426,
      "learning_rate": 3.33815424483539e-05,
      "loss": 0.6758,
      "step": 2185500
    },
    {
      "epoch": 19.94306153733849,
      "grad_norm": 4.0032525062561035,
      "learning_rate": 3.338078205221792e-05,
      "loss": 0.6592,
      "step": 2185600
    },
    {
      "epoch": 19.943974012701656,
      "grad_norm": 4.443524360656738,
      "learning_rate": 3.338002165608195e-05,
      "loss": 0.6734,
      "step": 2185700
    },
    {
      "epoch": 19.94488648806482,
      "grad_norm": 4.073895454406738,
      "learning_rate": 3.337926125994598e-05,
      "loss": 0.648,
      "step": 2185800
    },
    {
      "epoch": 19.945798963427986,
      "grad_norm": 4.060341835021973,
      "learning_rate": 3.337850086381001e-05,
      "loss": 0.6769,
      "step": 2185900
    },
    {
      "epoch": 19.94671143879115,
      "grad_norm": 4.313488006591797,
      "learning_rate": 3.337774046767404e-05,
      "loss": 0.6287,
      "step": 2186000
    },
    {
      "epoch": 19.947623914154317,
      "grad_norm": 2.4947023391723633,
      "learning_rate": 3.337698007153807e-05,
      "loss": 0.6345,
      "step": 2186100
    },
    {
      "epoch": 19.948536389517482,
      "grad_norm": 4.314971923828125,
      "learning_rate": 3.3376219675402096e-05,
      "loss": 0.6609,
      "step": 2186200
    },
    {
      "epoch": 19.949448864880647,
      "grad_norm": 3.6716439723968506,
      "learning_rate": 3.337545927926613e-05,
      "loss": 0.6375,
      "step": 2186300
    },
    {
      "epoch": 19.950361340243813,
      "grad_norm": 4.550054550170898,
      "learning_rate": 3.3374698883130156e-05,
      "loss": 0.6612,
      "step": 2186400
    },
    {
      "epoch": 19.951273815606978,
      "grad_norm": 4.134733200073242,
      "learning_rate": 3.3373938486994186e-05,
      "loss": 0.6562,
      "step": 2186500
    },
    {
      "epoch": 19.952186290970143,
      "grad_norm": 3.227691173553467,
      "learning_rate": 3.3373178090858216e-05,
      "loss": 0.6448,
      "step": 2186600
    },
    {
      "epoch": 19.95309876633331,
      "grad_norm": 3.865834951400757,
      "learning_rate": 3.3372417694722246e-05,
      "loss": 0.664,
      "step": 2186700
    },
    {
      "epoch": 19.954011241696474,
      "grad_norm": 3.2575912475585938,
      "learning_rate": 3.337165729858627e-05,
      "loss": 0.642,
      "step": 2186800
    },
    {
      "epoch": 19.95492371705964,
      "grad_norm": 3.6560580730438232,
      "learning_rate": 3.3370896902450307e-05,
      "loss": 0.643,
      "step": 2186900
    },
    {
      "epoch": 19.955836192422804,
      "grad_norm": 5.098948955535889,
      "learning_rate": 3.337013650631433e-05,
      "loss": 0.6743,
      "step": 2187000
    },
    {
      "epoch": 19.95674866778597,
      "grad_norm": 4.582625865936279,
      "learning_rate": 3.336937611017836e-05,
      "loss": 0.649,
      "step": 2187100
    },
    {
      "epoch": 19.957661143149135,
      "grad_norm": 3.202314853668213,
      "learning_rate": 3.336861571404239e-05,
      "loss": 0.6452,
      "step": 2187200
    },
    {
      "epoch": 19.9585736185123,
      "grad_norm": 3.7307143211364746,
      "learning_rate": 3.336785531790641e-05,
      "loss": 0.6326,
      "step": 2187300
    },
    {
      "epoch": 19.959486093875466,
      "grad_norm": 3.6292595863342285,
      "learning_rate": 3.336709492177045e-05,
      "loss": 0.6267,
      "step": 2187400
    },
    {
      "epoch": 19.96039856923863,
      "grad_norm": 2.7696194648742676,
      "learning_rate": 3.336633452563447e-05,
      "loss": 0.6597,
      "step": 2187500
    },
    {
      "epoch": 19.961311044601796,
      "grad_norm": 3.6119256019592285,
      "learning_rate": 3.3365574129498503e-05,
      "loss": 0.6638,
      "step": 2187600
    },
    {
      "epoch": 19.96222351996496,
      "grad_norm": 4.217264652252197,
      "learning_rate": 3.3364813733362534e-05,
      "loss": 0.6901,
      "step": 2187700
    },
    {
      "epoch": 19.963135995328127,
      "grad_norm": 5.430269718170166,
      "learning_rate": 3.3364053337226564e-05,
      "loss": 0.6621,
      "step": 2187800
    },
    {
      "epoch": 19.964048470691292,
      "grad_norm": 3.421034336090088,
      "learning_rate": 3.3363292941090594e-05,
      "loss": 0.6591,
      "step": 2187900
    },
    {
      "epoch": 19.964960946054457,
      "grad_norm": 3.1671390533447266,
      "learning_rate": 3.3362532544954624e-05,
      "loss": 0.6417,
      "step": 2188000
    },
    {
      "epoch": 19.965873421417623,
      "grad_norm": 4.7270073890686035,
      "learning_rate": 3.336177214881865e-05,
      "loss": 0.6597,
      "step": 2188100
    },
    {
      "epoch": 19.966785896780788,
      "grad_norm": 2.4227335453033447,
      "learning_rate": 3.336101175268268e-05,
      "loss": 0.6536,
      "step": 2188200
    },
    {
      "epoch": 19.967698372143953,
      "grad_norm": 4.724389553070068,
      "learning_rate": 3.336025135654671e-05,
      "loss": 0.6884,
      "step": 2188300
    },
    {
      "epoch": 19.96861084750712,
      "grad_norm": 3.580172538757324,
      "learning_rate": 3.335949096041074e-05,
      "loss": 0.6225,
      "step": 2188400
    },
    {
      "epoch": 19.969523322870284,
      "grad_norm": 4.013550281524658,
      "learning_rate": 3.335873056427477e-05,
      "loss": 0.6274,
      "step": 2188500
    },
    {
      "epoch": 19.97043579823345,
      "grad_norm": 4.022640705108643,
      "learning_rate": 3.33579701681388e-05,
      "loss": 0.6655,
      "step": 2188600
    },
    {
      "epoch": 19.971348273596615,
      "grad_norm": 4.637431621551514,
      "learning_rate": 3.335720977200282e-05,
      "loss": 0.6849,
      "step": 2188700
    },
    {
      "epoch": 19.972260748959776,
      "grad_norm": 3.376845598220825,
      "learning_rate": 3.335644937586686e-05,
      "loss": 0.6408,
      "step": 2188800
    },
    {
      "epoch": 19.97317322432294,
      "grad_norm": 4.5665411949157715,
      "learning_rate": 3.335568897973088e-05,
      "loss": 0.677,
      "step": 2188900
    },
    {
      "epoch": 19.974085699686107,
      "grad_norm": 4.457162380218506,
      "learning_rate": 3.335492858359491e-05,
      "loss": 0.6414,
      "step": 2189000
    },
    {
      "epoch": 19.974998175049272,
      "grad_norm": 2.8136069774627686,
      "learning_rate": 3.335416818745894e-05,
      "loss": 0.6409,
      "step": 2189100
    },
    {
      "epoch": 19.975910650412438,
      "grad_norm": 3.5206735134124756,
      "learning_rate": 3.335340779132297e-05,
      "loss": 0.6879,
      "step": 2189200
    },
    {
      "epoch": 19.976823125775603,
      "grad_norm": 4.248454570770264,
      "learning_rate": 3.3352647395187e-05,
      "loss": 0.6468,
      "step": 2189300
    },
    {
      "epoch": 19.977735601138768,
      "grad_norm": 4.42119026184082,
      "learning_rate": 3.335188699905103e-05,
      "loss": 0.6411,
      "step": 2189400
    },
    {
      "epoch": 19.978648076501933,
      "grad_norm": 4.011476993560791,
      "learning_rate": 3.3351126602915054e-05,
      "loss": 0.6763,
      "step": 2189500
    },
    {
      "epoch": 19.9795605518651,
      "grad_norm": 4.780470371246338,
      "learning_rate": 3.3350366206779084e-05,
      "loss": 0.6321,
      "step": 2189600
    },
    {
      "epoch": 19.980473027228264,
      "grad_norm": 3.2228405475616455,
      "learning_rate": 3.3349605810643115e-05,
      "loss": 0.6463,
      "step": 2189700
    },
    {
      "epoch": 19.98138550259143,
      "grad_norm": 4.3596110343933105,
      "learning_rate": 3.334884541450714e-05,
      "loss": 0.6454,
      "step": 2189800
    },
    {
      "epoch": 19.982297977954595,
      "grad_norm": 4.263600826263428,
      "learning_rate": 3.3348085018371175e-05,
      "loss": 0.6619,
      "step": 2189900
    },
    {
      "epoch": 19.98321045331776,
      "grad_norm": 3.5963261127471924,
      "learning_rate": 3.33473246222352e-05,
      "loss": 0.6675,
      "step": 2190000
    },
    {
      "epoch": 19.984122928680925,
      "grad_norm": 2.6845273971557617,
      "learning_rate": 3.334656422609923e-05,
      "loss": 0.6922,
      "step": 2190100
    },
    {
      "epoch": 19.98503540404409,
      "grad_norm": 3.913008213043213,
      "learning_rate": 3.334580382996326e-05,
      "loss": 0.6901,
      "step": 2190200
    },
    {
      "epoch": 19.985947879407256,
      "grad_norm": 4.435342311859131,
      "learning_rate": 3.334504343382729e-05,
      "loss": 0.644,
      "step": 2190300
    },
    {
      "epoch": 19.98686035477042,
      "grad_norm": 4.257445812225342,
      "learning_rate": 3.334428303769132e-05,
      "loss": 0.6749,
      "step": 2190400
    },
    {
      "epoch": 19.987772830133586,
      "grad_norm": 4.060460567474365,
      "learning_rate": 3.334352264155535e-05,
      "loss": 0.6651,
      "step": 2190500
    },
    {
      "epoch": 19.98868530549675,
      "grad_norm": 3.891934394836426,
      "learning_rate": 3.334276224541937e-05,
      "loss": 0.6781,
      "step": 2190600
    },
    {
      "epoch": 19.989597780859917,
      "grad_norm": 3.1572070121765137,
      "learning_rate": 3.334200184928341e-05,
      "loss": 0.6584,
      "step": 2190700
    },
    {
      "epoch": 19.990510256223082,
      "grad_norm": 3.817582607269287,
      "learning_rate": 3.334124145314743e-05,
      "loss": 0.6589,
      "step": 2190800
    },
    {
      "epoch": 19.991422731586248,
      "grad_norm": 3.3540332317352295,
      "learning_rate": 3.334048105701146e-05,
      "loss": 0.6811,
      "step": 2190900
    },
    {
      "epoch": 19.992335206949413,
      "grad_norm": 4.197268009185791,
      "learning_rate": 3.333972066087549e-05,
      "loss": 0.6428,
      "step": 2191000
    },
    {
      "epoch": 19.993247682312578,
      "grad_norm": 3.6654117107391357,
      "learning_rate": 3.333896026473952e-05,
      "loss": 0.6858,
      "step": 2191100
    },
    {
      "epoch": 19.994160157675744,
      "grad_norm": 2.550408363342285,
      "learning_rate": 3.3338199868603545e-05,
      "loss": 0.654,
      "step": 2191200
    },
    {
      "epoch": 19.99507263303891,
      "grad_norm": 3.3101024627685547,
      "learning_rate": 3.333743947246758e-05,
      "loss": 0.654,
      "step": 2191300
    },
    {
      "epoch": 19.995985108402074,
      "grad_norm": 3.3507189750671387,
      "learning_rate": 3.3336679076331605e-05,
      "loss": 0.6418,
      "step": 2191400
    },
    {
      "epoch": 19.99689758376524,
      "grad_norm": 3.2234303951263428,
      "learning_rate": 3.3335918680195635e-05,
      "loss": 0.6487,
      "step": 2191500
    },
    {
      "epoch": 19.997810059128405,
      "grad_norm": 3.801985740661621,
      "learning_rate": 3.3335158284059665e-05,
      "loss": 0.6796,
      "step": 2191600
    },
    {
      "epoch": 19.99872253449157,
      "grad_norm": 3.5031795501708984,
      "learning_rate": 3.3334397887923696e-05,
      "loss": 0.6862,
      "step": 2191700
    },
    {
      "epoch": 19.999635009854735,
      "grad_norm": 3.9863345623016357,
      "learning_rate": 3.3333637491787726e-05,
      "loss": 0.6562,
      "step": 2191800
    },
    {
      "epoch": 20.0,
      "eval_loss": 0.5381515622138977,
      "eval_runtime": 25.6199,
      "eval_samples_per_second": 225.177,
      "eval_steps_per_second": 225.177,
      "step": 2191840
    },
    {
      "epoch": 20.0,
      "eval_loss": 0.5148605108261108,
      "eval_runtime": 484.5909,
      "eval_samples_per_second": 226.154,
      "eval_steps_per_second": 226.154,
      "step": 2191840
    }
  ],
  "logging_steps": 100,
  "max_steps": 6575520,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1356328828108800.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
