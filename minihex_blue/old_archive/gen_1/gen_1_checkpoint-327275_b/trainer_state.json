{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 25.0,
  "eval_steps": 500,
  "global_step": 327275,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007638835841417768,
      "grad_norm": 1.9224251508712769,
      "learning_rate": 4.9993634303465486e-05,
      "loss": 4.8617,
      "step": 100
    },
    {
      "epoch": 0.015277671682835536,
      "grad_norm": 4.647699356079102,
      "learning_rate": 4.998726860693097e-05,
      "loss": 4.7242,
      "step": 200
    },
    {
      "epoch": 0.022916507524253303,
      "grad_norm": 4.111614227294922,
      "learning_rate": 4.998090291039646e-05,
      "loss": 4.6157,
      "step": 300
    },
    {
      "epoch": 0.030555343365671072,
      "grad_norm": 5.251119136810303,
      "learning_rate": 4.997453721386194e-05,
      "loss": 4.5062,
      "step": 400
    },
    {
      "epoch": 0.03819417920708884,
      "grad_norm": 4.839983940124512,
      "learning_rate": 4.996817151732743e-05,
      "loss": 4.4066,
      "step": 500
    },
    {
      "epoch": 0.045833015048506606,
      "grad_norm": 5.360339164733887,
      "learning_rate": 4.996180582079291e-05,
      "loss": 4.33,
      "step": 600
    },
    {
      "epoch": 0.05347185088992438,
      "grad_norm": 4.539937496185303,
      "learning_rate": 4.99554401242584e-05,
      "loss": 4.2491,
      "step": 700
    },
    {
      "epoch": 0.061110686731342144,
      "grad_norm": 4.080824851989746,
      "learning_rate": 4.9949074427723884e-05,
      "loss": 4.1912,
      "step": 800
    },
    {
      "epoch": 0.06874952257275992,
      "grad_norm": 3.676185369491577,
      "learning_rate": 4.994270873118937e-05,
      "loss": 4.1005,
      "step": 900
    },
    {
      "epoch": 0.07638835841417768,
      "grad_norm": 4.236896514892578,
      "learning_rate": 4.993634303465485e-05,
      "loss": 4.0813,
      "step": 1000
    },
    {
      "epoch": 0.08402719425559545,
      "grad_norm": 3.6364333629608154,
      "learning_rate": 4.9929977338120335e-05,
      "loss": 4.0827,
      "step": 1100
    },
    {
      "epoch": 0.09166603009701321,
      "grad_norm": 3.6863701343536377,
      "learning_rate": 4.9923611641585825e-05,
      "loss": 3.9997,
      "step": 1200
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 3.7470264434814453,
      "learning_rate": 4.991724594505131e-05,
      "loss": 4.0042,
      "step": 1300
    },
    {
      "epoch": 0.10694370177984876,
      "grad_norm": 5.160604000091553,
      "learning_rate": 4.991088024851679e-05,
      "loss": 3.9227,
      "step": 1400
    },
    {
      "epoch": 0.11458253762126652,
      "grad_norm": 3.7036960124969482,
      "learning_rate": 4.9904514551982276e-05,
      "loss": 3.8839,
      "step": 1500
    },
    {
      "epoch": 0.12222137346268429,
      "grad_norm": 5.276404857635498,
      "learning_rate": 4.9898148855447766e-05,
      "loss": 3.897,
      "step": 1600
    },
    {
      "epoch": 0.12986020930410205,
      "grad_norm": 3.71661376953125,
      "learning_rate": 4.989178315891325e-05,
      "loss": 3.8308,
      "step": 1700
    },
    {
      "epoch": 0.13749904514551983,
      "grad_norm": 4.184957027435303,
      "learning_rate": 4.988541746237873e-05,
      "loss": 3.8095,
      "step": 1800
    },
    {
      "epoch": 0.14513788098693758,
      "grad_norm": 7.0809550285339355,
      "learning_rate": 4.9879051765844223e-05,
      "loss": 3.8014,
      "step": 1900
    },
    {
      "epoch": 0.15277671682835536,
      "grad_norm": 4.232694149017334,
      "learning_rate": 4.987268606930971e-05,
      "loss": 3.796,
      "step": 2000
    },
    {
      "epoch": 0.16041555266977311,
      "grad_norm": 5.2492828369140625,
      "learning_rate": 4.986632037277519e-05,
      "loss": 3.7471,
      "step": 2100
    },
    {
      "epoch": 0.1680543885111909,
      "grad_norm": 4.2890625,
      "learning_rate": 4.985995467624068e-05,
      "loss": 3.7231,
      "step": 2200
    },
    {
      "epoch": 0.17569322435260867,
      "grad_norm": 7.143892765045166,
      "learning_rate": 4.9853588979706164e-05,
      "loss": 3.6961,
      "step": 2300
    },
    {
      "epoch": 0.18333206019402642,
      "grad_norm": 4.175999164581299,
      "learning_rate": 4.984722328317165e-05,
      "loss": 3.7474,
      "step": 2400
    },
    {
      "epoch": 0.1909708960354442,
      "grad_norm": 6.190179347991943,
      "learning_rate": 4.984085758663713e-05,
      "loss": 3.7164,
      "step": 2500
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 4.170467376708984,
      "learning_rate": 4.983449189010262e-05,
      "loss": 3.6936,
      "step": 2600
    },
    {
      "epoch": 0.20624856771827974,
      "grad_norm": 6.845798015594482,
      "learning_rate": 4.9828126193568105e-05,
      "loss": 3.6779,
      "step": 2700
    },
    {
      "epoch": 0.21388740355969751,
      "grad_norm": 4.361955642700195,
      "learning_rate": 4.982176049703359e-05,
      "loss": 3.6486,
      "step": 2800
    },
    {
      "epoch": 0.22152623940111527,
      "grad_norm": 4.571598529815674,
      "learning_rate": 4.981539480049907e-05,
      "loss": 3.6175,
      "step": 2900
    },
    {
      "epoch": 0.22916507524253305,
      "grad_norm": 4.477099895477295,
      "learning_rate": 4.980902910396456e-05,
      "loss": 3.6361,
      "step": 3000
    },
    {
      "epoch": 0.2368039110839508,
      "grad_norm": 4.456125259399414,
      "learning_rate": 4.9802663407430046e-05,
      "loss": 3.6209,
      "step": 3100
    },
    {
      "epoch": 0.24444274692536858,
      "grad_norm": 4.205288887023926,
      "learning_rate": 4.979629771089553e-05,
      "loss": 3.5965,
      "step": 3200
    },
    {
      "epoch": 0.2520815827667863,
      "grad_norm": 5.21940279006958,
      "learning_rate": 4.978993201436101e-05,
      "loss": 3.609,
      "step": 3300
    },
    {
      "epoch": 0.2597204186082041,
      "grad_norm": 6.400247097015381,
      "learning_rate": 4.97835663178265e-05,
      "loss": 3.5892,
      "step": 3400
    },
    {
      "epoch": 0.2673592544496219,
      "grad_norm": 5.6948347091674805,
      "learning_rate": 4.977720062129199e-05,
      "loss": 3.5768,
      "step": 3500
    },
    {
      "epoch": 0.27499809029103967,
      "grad_norm": 5.317383289337158,
      "learning_rate": 4.977083492475747e-05,
      "loss": 3.54,
      "step": 3600
    },
    {
      "epoch": 0.2826369261324574,
      "grad_norm": 4.22895622253418,
      "learning_rate": 4.9764469228222954e-05,
      "loss": 3.5321,
      "step": 3700
    },
    {
      "epoch": 0.29027576197387517,
      "grad_norm": 5.271459102630615,
      "learning_rate": 4.975810353168844e-05,
      "loss": 3.559,
      "step": 3800
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 6.836220741271973,
      "learning_rate": 4.975173783515393e-05,
      "loss": 3.5425,
      "step": 3900
    },
    {
      "epoch": 0.3055534336567107,
      "grad_norm": 5.864293575286865,
      "learning_rate": 4.974537213861941e-05,
      "loss": 3.5592,
      "step": 4000
    },
    {
      "epoch": 0.3131922694981285,
      "grad_norm": 7.357604503631592,
      "learning_rate": 4.9739006442084895e-05,
      "loss": 3.5139,
      "step": 4100
    },
    {
      "epoch": 0.32083110533954623,
      "grad_norm": 5.601223945617676,
      "learning_rate": 4.973264074555038e-05,
      "loss": 3.5528,
      "step": 4200
    },
    {
      "epoch": 0.328469941180964,
      "grad_norm": 5.789613246917725,
      "learning_rate": 4.972627504901586e-05,
      "loss": 3.4906,
      "step": 4300
    },
    {
      "epoch": 0.3361087770223818,
      "grad_norm": 7.904165744781494,
      "learning_rate": 4.971990935248135e-05,
      "loss": 3.5384,
      "step": 4400
    },
    {
      "epoch": 0.34374761286379957,
      "grad_norm": 5.340511798858643,
      "learning_rate": 4.9713543655946836e-05,
      "loss": 3.468,
      "step": 4500
    },
    {
      "epoch": 0.35138644870521735,
      "grad_norm": 5.395275115966797,
      "learning_rate": 4.970717795941232e-05,
      "loss": 3.4975,
      "step": 4600
    },
    {
      "epoch": 0.35902528454663507,
      "grad_norm": 5.072201728820801,
      "learning_rate": 4.97008122628778e-05,
      "loss": 3.4904,
      "step": 4700
    },
    {
      "epoch": 0.36666412038805285,
      "grad_norm": 5.426990032196045,
      "learning_rate": 4.969444656634329e-05,
      "loss": 3.4949,
      "step": 4800
    },
    {
      "epoch": 0.37430295622947063,
      "grad_norm": 8.786684036254883,
      "learning_rate": 4.968808086980878e-05,
      "loss": 3.5178,
      "step": 4900
    },
    {
      "epoch": 0.3819417920708884,
      "grad_norm": 5.809678077697754,
      "learning_rate": 4.968171517327426e-05,
      "loss": 3.4199,
      "step": 5000
    },
    {
      "epoch": 0.3895806279123062,
      "grad_norm": 6.264673709869385,
      "learning_rate": 4.9675349476739744e-05,
      "loss": 3.4743,
      "step": 5100
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 6.329531669616699,
      "learning_rate": 4.966898378020523e-05,
      "loss": 3.4783,
      "step": 5200
    },
    {
      "epoch": 0.4048582995951417,
      "grad_norm": 4.527268886566162,
      "learning_rate": 4.966261808367072e-05,
      "loss": 3.4372,
      "step": 5300
    },
    {
      "epoch": 0.41249713543655947,
      "grad_norm": 5.358297348022461,
      "learning_rate": 4.96562523871362e-05,
      "loss": 3.4029,
      "step": 5400
    },
    {
      "epoch": 0.42013597127797725,
      "grad_norm": 6.632413864135742,
      "learning_rate": 4.9649886690601685e-05,
      "loss": 3.4771,
      "step": 5500
    },
    {
      "epoch": 0.42777480711939503,
      "grad_norm": 9.353227615356445,
      "learning_rate": 4.9643520994067175e-05,
      "loss": 3.4248,
      "step": 5600
    },
    {
      "epoch": 0.43541364296081275,
      "grad_norm": 6.259468078613281,
      "learning_rate": 4.963715529753266e-05,
      "loss": 3.3785,
      "step": 5700
    },
    {
      "epoch": 0.44305247880223053,
      "grad_norm": 6.580149173736572,
      "learning_rate": 4.963078960099814e-05,
      "loss": 3.4274,
      "step": 5800
    },
    {
      "epoch": 0.4506913146436483,
      "grad_norm": 6.672733306884766,
      "learning_rate": 4.962442390446363e-05,
      "loss": 3.4185,
      "step": 5900
    },
    {
      "epoch": 0.4583301504850661,
      "grad_norm": 5.968082904815674,
      "learning_rate": 4.9618058207929116e-05,
      "loss": 3.4424,
      "step": 6000
    },
    {
      "epoch": 0.46596898632648387,
      "grad_norm": 5.535644054412842,
      "learning_rate": 4.96116925113946e-05,
      "loss": 3.4254,
      "step": 6100
    },
    {
      "epoch": 0.4736078221679016,
      "grad_norm": 5.619033336639404,
      "learning_rate": 4.960532681486009e-05,
      "loss": 3.4118,
      "step": 6200
    },
    {
      "epoch": 0.4812466580093194,
      "grad_norm": 7.282227039337158,
      "learning_rate": 4.9598961118325574e-05,
      "loss": 3.4089,
      "step": 6300
    },
    {
      "epoch": 0.48888549385073715,
      "grad_norm": 5.634026050567627,
      "learning_rate": 4.959259542179106e-05,
      "loss": 3.4032,
      "step": 6400
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 8.95052433013916,
      "learning_rate": 4.958622972525654e-05,
      "loss": 3.3969,
      "step": 6500
    },
    {
      "epoch": 0.5041631655335727,
      "grad_norm": 6.863394737243652,
      "learning_rate": 4.9579864028722024e-05,
      "loss": 3.3101,
      "step": 6600
    },
    {
      "epoch": 0.5118020013749904,
      "grad_norm": 5.808692455291748,
      "learning_rate": 4.9573498332187515e-05,
      "loss": 3.3404,
      "step": 6700
    },
    {
      "epoch": 0.5194408372164082,
      "grad_norm": 8.168730735778809,
      "learning_rate": 4.9567132635653e-05,
      "loss": 3.3822,
      "step": 6800
    },
    {
      "epoch": 0.527079673057826,
      "grad_norm": 6.122939586639404,
      "learning_rate": 4.956076693911848e-05,
      "loss": 3.373,
      "step": 6900
    },
    {
      "epoch": 0.5347185088992438,
      "grad_norm": 6.1810712814331055,
      "learning_rate": 4.9554401242583965e-05,
      "loss": 3.3243,
      "step": 7000
    },
    {
      "epoch": 0.5423573447406616,
      "grad_norm": 8.205418586730957,
      "learning_rate": 4.9548035546049455e-05,
      "loss": 3.3666,
      "step": 7100
    },
    {
      "epoch": 0.5499961805820793,
      "grad_norm": 7.748049736022949,
      "learning_rate": 4.954166984951494e-05,
      "loss": 3.3048,
      "step": 7200
    },
    {
      "epoch": 0.5576350164234971,
      "grad_norm": 6.623600006103516,
      "learning_rate": 4.953530415298042e-05,
      "loss": 3.3588,
      "step": 7300
    },
    {
      "epoch": 0.5652738522649148,
      "grad_norm": 7.199778079986572,
      "learning_rate": 4.9528938456445906e-05,
      "loss": 3.3536,
      "step": 7400
    },
    {
      "epoch": 0.5729126881063326,
      "grad_norm": 8.376753807067871,
      "learning_rate": 4.952257275991139e-05,
      "loss": 3.3385,
      "step": 7500
    },
    {
      "epoch": 0.5805515239477503,
      "grad_norm": 6.335176944732666,
      "learning_rate": 4.951620706337688e-05,
      "loss": 3.2936,
      "step": 7600
    },
    {
      "epoch": 0.5881903597891681,
      "grad_norm": 6.094825267791748,
      "learning_rate": 4.9509841366842363e-05,
      "loss": 3.3689,
      "step": 7700
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 6.7682318687438965,
      "learning_rate": 4.950347567030785e-05,
      "loss": 3.3413,
      "step": 7800
    },
    {
      "epoch": 0.6034680314720037,
      "grad_norm": 6.75574254989624,
      "learning_rate": 4.949710997377333e-05,
      "loss": 3.3656,
      "step": 7900
    },
    {
      "epoch": 0.6111068673134215,
      "grad_norm": 7.405023574829102,
      "learning_rate": 4.9490744277238814e-05,
      "loss": 3.3251,
      "step": 8000
    },
    {
      "epoch": 0.6187457031548392,
      "grad_norm": 9.561871528625488,
      "learning_rate": 4.9484378580704304e-05,
      "loss": 3.3379,
      "step": 8100
    },
    {
      "epoch": 0.626384538996257,
      "grad_norm": 6.368677139282227,
      "learning_rate": 4.947801288416979e-05,
      "loss": 3.396,
      "step": 8200
    },
    {
      "epoch": 0.6340233748376748,
      "grad_norm": 7.326369762420654,
      "learning_rate": 4.947164718763527e-05,
      "loss": 3.315,
      "step": 8300
    },
    {
      "epoch": 0.6416622106790925,
      "grad_norm": 7.819555759429932,
      "learning_rate": 4.9465281491100755e-05,
      "loss": 3.3624,
      "step": 8400
    },
    {
      "epoch": 0.6493010465205102,
      "grad_norm": 7.31563663482666,
      "learning_rate": 4.9458915794566245e-05,
      "loss": 3.331,
      "step": 8500
    },
    {
      "epoch": 0.656939882361928,
      "grad_norm": 7.518764019012451,
      "learning_rate": 4.945255009803173e-05,
      "loss": 3.3496,
      "step": 8600
    },
    {
      "epoch": 0.6645787182033458,
      "grad_norm": 7.979475498199463,
      "learning_rate": 4.944618440149721e-05,
      "loss": 3.3006,
      "step": 8700
    },
    {
      "epoch": 0.6722175540447636,
      "grad_norm": 6.781445026397705,
      "learning_rate": 4.9439818704962696e-05,
      "loss": 3.3421,
      "step": 8800
    },
    {
      "epoch": 0.6798563898861814,
      "grad_norm": 7.574452877044678,
      "learning_rate": 4.943345300842818e-05,
      "loss": 3.2505,
      "step": 8900
    },
    {
      "epoch": 0.6874952257275991,
      "grad_norm": 7.180374622344971,
      "learning_rate": 4.942708731189367e-05,
      "loss": 3.2809,
      "step": 9000
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 7.44069242477417,
      "learning_rate": 4.942072161535915e-05,
      "loss": 3.2828,
      "step": 9100
    },
    {
      "epoch": 0.7027728974104347,
      "grad_norm": 6.827808380126953,
      "learning_rate": 4.941435591882464e-05,
      "loss": 3.256,
      "step": 9200
    },
    {
      "epoch": 0.7104117332518525,
      "grad_norm": 8.118017196655273,
      "learning_rate": 4.940799022229012e-05,
      "loss": 3.2492,
      "step": 9300
    },
    {
      "epoch": 0.7180505690932701,
      "grad_norm": 7.385493755340576,
      "learning_rate": 4.940162452575561e-05,
      "loss": 3.1758,
      "step": 9400
    },
    {
      "epoch": 0.7256894049346879,
      "grad_norm": 8.35650634765625,
      "learning_rate": 4.9395258829221094e-05,
      "loss": 3.2805,
      "step": 9500
    },
    {
      "epoch": 0.7333282407761057,
      "grad_norm": 9.416309356689453,
      "learning_rate": 4.938889313268658e-05,
      "loss": 3.287,
      "step": 9600
    },
    {
      "epoch": 0.7409670766175235,
      "grad_norm": 9.151082992553711,
      "learning_rate": 4.938252743615207e-05,
      "loss": 3.2582,
      "step": 9700
    },
    {
      "epoch": 0.7486059124589413,
      "grad_norm": 7.751544952392578,
      "learning_rate": 4.937616173961755e-05,
      "loss": 3.1938,
      "step": 9800
    },
    {
      "epoch": 0.756244748300359,
      "grad_norm": 6.668874263763428,
      "learning_rate": 4.936979604308304e-05,
      "loss": 3.2293,
      "step": 9900
    },
    {
      "epoch": 0.7638835841417768,
      "grad_norm": 8.43026065826416,
      "learning_rate": 4.9363430346548525e-05,
      "loss": 3.2175,
      "step": 10000
    },
    {
      "epoch": 0.7715224199831946,
      "grad_norm": 7.360154151916504,
      "learning_rate": 4.935706465001401e-05,
      "loss": 3.2062,
      "step": 10100
    },
    {
      "epoch": 0.7791612558246124,
      "grad_norm": 7.579013347625732,
      "learning_rate": 4.935069895347949e-05,
      "loss": 3.2277,
      "step": 10200
    },
    {
      "epoch": 0.78680009166603,
      "grad_norm": 7.869386196136475,
      "learning_rate": 4.9344333256944976e-05,
      "loss": 3.2041,
      "step": 10300
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 7.528840065002441,
      "learning_rate": 4.9337967560410466e-05,
      "loss": 3.2523,
      "step": 10400
    },
    {
      "epoch": 0.8020777633488656,
      "grad_norm": 7.0816521644592285,
      "learning_rate": 4.933160186387595e-05,
      "loss": 3.2716,
      "step": 10500
    },
    {
      "epoch": 0.8097165991902834,
      "grad_norm": 7.036823272705078,
      "learning_rate": 4.9325236167341433e-05,
      "loss": 3.2346,
      "step": 10600
    },
    {
      "epoch": 0.8173554350317012,
      "grad_norm": 7.875246047973633,
      "learning_rate": 4.931887047080692e-05,
      "loss": 3.2789,
      "step": 10700
    },
    {
      "epoch": 0.8249942708731189,
      "grad_norm": 10.94749641418457,
      "learning_rate": 4.931250477427241e-05,
      "loss": 3.1954,
      "step": 10800
    },
    {
      "epoch": 0.8326331067145367,
      "grad_norm": 7.305344581604004,
      "learning_rate": 4.930613907773789e-05,
      "loss": 3.235,
      "step": 10900
    },
    {
      "epoch": 0.8402719425559545,
      "grad_norm": 7.6156816482543945,
      "learning_rate": 4.9299773381203374e-05,
      "loss": 3.2505,
      "step": 11000
    },
    {
      "epoch": 0.8479107783973723,
      "grad_norm": 11.475753784179688,
      "learning_rate": 4.929340768466886e-05,
      "loss": 3.1959,
      "step": 11100
    },
    {
      "epoch": 0.8555496142387901,
      "grad_norm": 6.51742696762085,
      "learning_rate": 4.928704198813434e-05,
      "loss": 3.2259,
      "step": 11200
    },
    {
      "epoch": 0.8631884500802077,
      "grad_norm": 7.6544270515441895,
      "learning_rate": 4.928067629159983e-05,
      "loss": 3.2054,
      "step": 11300
    },
    {
      "epoch": 0.8708272859216255,
      "grad_norm": 9.158329963684082,
      "learning_rate": 4.9274310595065315e-05,
      "loss": 3.2183,
      "step": 11400
    },
    {
      "epoch": 0.8784661217630433,
      "grad_norm": 9.57143497467041,
      "learning_rate": 4.92679448985308e-05,
      "loss": 3.1776,
      "step": 11500
    },
    {
      "epoch": 0.8861049576044611,
      "grad_norm": 8.505741119384766,
      "learning_rate": 4.926157920199628e-05,
      "loss": 3.2032,
      "step": 11600
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 8.666261672973633,
      "learning_rate": 4.925521350546177e-05,
      "loss": 3.2142,
      "step": 11700
    },
    {
      "epoch": 0.9013826292872966,
      "grad_norm": 10.87314224243164,
      "learning_rate": 4.9248847808927256e-05,
      "loss": 3.247,
      "step": 11800
    },
    {
      "epoch": 0.9090214651287144,
      "grad_norm": 8.881102561950684,
      "learning_rate": 4.924248211239274e-05,
      "loss": 3.2529,
      "step": 11900
    },
    {
      "epoch": 0.9166603009701322,
      "grad_norm": 8.968210220336914,
      "learning_rate": 4.923611641585822e-05,
      "loss": 3.1525,
      "step": 12000
    },
    {
      "epoch": 0.92429913681155,
      "grad_norm": 9.73330020904541,
      "learning_rate": 4.922975071932371e-05,
      "loss": 3.1902,
      "step": 12100
    },
    {
      "epoch": 0.9319379726529677,
      "grad_norm": 8.442732810974121,
      "learning_rate": 4.92233850227892e-05,
      "loss": 3.2563,
      "step": 12200
    },
    {
      "epoch": 0.9395768084943854,
      "grad_norm": 8.15341567993164,
      "learning_rate": 4.921701932625468e-05,
      "loss": 3.1968,
      "step": 12300
    },
    {
      "epoch": 0.9472156443358032,
      "grad_norm": 10.846531867980957,
      "learning_rate": 4.9210653629720164e-05,
      "loss": 3.1871,
      "step": 12400
    },
    {
      "epoch": 0.954854480177221,
      "grad_norm": 7.617798805236816,
      "learning_rate": 4.920428793318565e-05,
      "loss": 3.1772,
      "step": 12500
    },
    {
      "epoch": 0.9624933160186387,
      "grad_norm": 9.602420806884766,
      "learning_rate": 4.919792223665114e-05,
      "loss": 3.1397,
      "step": 12600
    },
    {
      "epoch": 0.9701321518600565,
      "grad_norm": 8.30204963684082,
      "learning_rate": 4.919155654011662e-05,
      "loss": 3.1595,
      "step": 12700
    },
    {
      "epoch": 0.9777709877014743,
      "grad_norm": 7.84068489074707,
      "learning_rate": 4.9185190843582105e-05,
      "loss": 3.1517,
      "step": 12800
    },
    {
      "epoch": 0.9854098235428921,
      "grad_norm": 7.145804405212402,
      "learning_rate": 4.917882514704759e-05,
      "loss": 3.1726,
      "step": 12900
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 7.13494873046875,
      "learning_rate": 4.917245945051307e-05,
      "loss": 3.1248,
      "step": 13000
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.11002254486084,
      "eval_runtime": 1.6525,
      "eval_samples_per_second": 417.54,
      "eval_steps_per_second": 417.54,
      "step": 13091
    },
    {
      "epoch": 1.0,
      "eval_loss": 3.088857650756836,
      "eval_runtime": 31.217,
      "eval_samples_per_second": 419.356,
      "eval_steps_per_second": 419.356,
      "step": 13091
    },
    {
      "epoch": 1.0006874952257276,
      "grad_norm": 8.185274124145508,
      "learning_rate": 4.916609375397856e-05,
      "loss": 3.2293,
      "step": 13100
    },
    {
      "epoch": 1.0083263310671453,
      "grad_norm": 7.483427047729492,
      "learning_rate": 4.9159728057444046e-05,
      "loss": 3.1739,
      "step": 13200
    },
    {
      "epoch": 1.0159651669085632,
      "grad_norm": 7.3173980712890625,
      "learning_rate": 4.915336236090953e-05,
      "loss": 3.1639,
      "step": 13300
    },
    {
      "epoch": 1.0236040027499809,
      "grad_norm": 7.886783599853516,
      "learning_rate": 4.914699666437502e-05,
      "loss": 3.1757,
      "step": 13400
    },
    {
      "epoch": 1.0312428385913988,
      "grad_norm": 12.15076732635498,
      "learning_rate": 4.9140630967840503e-05,
      "loss": 3.1438,
      "step": 13500
    },
    {
      "epoch": 1.0388816744328164,
      "grad_norm": 11.72274398803711,
      "learning_rate": 4.913426527130599e-05,
      "loss": 3.1211,
      "step": 13600
    },
    {
      "epoch": 1.046520510274234,
      "grad_norm": 10.01315975189209,
      "learning_rate": 4.912789957477148e-05,
      "loss": 3.1228,
      "step": 13700
    },
    {
      "epoch": 1.054159346115652,
      "grad_norm": 7.8486247062683105,
      "learning_rate": 4.912153387823696e-05,
      "loss": 3.133,
      "step": 13800
    },
    {
      "epoch": 1.0617981819570697,
      "grad_norm": 7.237063884735107,
      "learning_rate": 4.9115168181702444e-05,
      "loss": 3.1575,
      "step": 13900
    },
    {
      "epoch": 1.0694370177984875,
      "grad_norm": 8.585794448852539,
      "learning_rate": 4.9108802485167935e-05,
      "loss": 3.1597,
      "step": 14000
    },
    {
      "epoch": 1.0770758536399052,
      "grad_norm": 7.703426837921143,
      "learning_rate": 4.910243678863342e-05,
      "loss": 3.1307,
      "step": 14100
    },
    {
      "epoch": 1.084714689481323,
      "grad_norm": 8.407651901245117,
      "learning_rate": 4.90960710920989e-05,
      "loss": 3.1284,
      "step": 14200
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 12.965242385864258,
      "learning_rate": 4.9089705395564385e-05,
      "loss": 3.1397,
      "step": 14300
    },
    {
      "epoch": 1.0999923611641587,
      "grad_norm": 7.886112213134766,
      "learning_rate": 4.908333969902987e-05,
      "loss": 3.1076,
      "step": 14400
    },
    {
      "epoch": 1.1076311970055763,
      "grad_norm": 9.209962844848633,
      "learning_rate": 4.907697400249536e-05,
      "loss": 3.1181,
      "step": 14500
    },
    {
      "epoch": 1.1152700328469942,
      "grad_norm": 9.93260383605957,
      "learning_rate": 4.907060830596084e-05,
      "loss": 3.1296,
      "step": 14600
    },
    {
      "epoch": 1.1229088686884119,
      "grad_norm": 7.961076259613037,
      "learning_rate": 4.9064242609426326e-05,
      "loss": 3.086,
      "step": 14700
    },
    {
      "epoch": 1.1305477045298296,
      "grad_norm": 8.644659042358398,
      "learning_rate": 4.905787691289181e-05,
      "loss": 3.1072,
      "step": 14800
    },
    {
      "epoch": 1.1381865403712474,
      "grad_norm": 8.690315246582031,
      "learning_rate": 4.90515112163573e-05,
      "loss": 3.1412,
      "step": 14900
    },
    {
      "epoch": 1.1458253762126651,
      "grad_norm": 9.92849349975586,
      "learning_rate": 4.9045145519822784e-05,
      "loss": 3.1581,
      "step": 15000
    },
    {
      "epoch": 1.153464212054083,
      "grad_norm": 8.59429931640625,
      "learning_rate": 4.903877982328827e-05,
      "loss": 3.1322,
      "step": 15100
    },
    {
      "epoch": 1.1611030478955007,
      "grad_norm": 7.49515438079834,
      "learning_rate": 4.903241412675375e-05,
      "loss": 3.0675,
      "step": 15200
    },
    {
      "epoch": 1.1687418837369186,
      "grad_norm": 11.824104309082031,
      "learning_rate": 4.9026048430219234e-05,
      "loss": 3.119,
      "step": 15300
    },
    {
      "epoch": 1.1763807195783362,
      "grad_norm": 9.057623863220215,
      "learning_rate": 4.9019682733684725e-05,
      "loss": 3.113,
      "step": 15400
    },
    {
      "epoch": 1.1840195554197541,
      "grad_norm": 8.353302001953125,
      "learning_rate": 4.901331703715021e-05,
      "loss": 3.1117,
      "step": 15500
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 8.113693237304688,
      "learning_rate": 4.900695134061569e-05,
      "loss": 3.132,
      "step": 15600
    },
    {
      "epoch": 1.1992972271025897,
      "grad_norm": 10.6905517578125,
      "learning_rate": 4.9000585644081175e-05,
      "loss": 3.0549,
      "step": 15700
    },
    {
      "epoch": 1.2069360629440073,
      "grad_norm": 7.982051372528076,
      "learning_rate": 4.8994219947546665e-05,
      "loss": 3.1237,
      "step": 15800
    },
    {
      "epoch": 1.214574898785425,
      "grad_norm": 8.574798583984375,
      "learning_rate": 4.898785425101215e-05,
      "loss": 3.0755,
      "step": 15900
    },
    {
      "epoch": 1.222213734626843,
      "grad_norm": 8.60447883605957,
      "learning_rate": 4.898148855447763e-05,
      "loss": 3.1426,
      "step": 16000
    },
    {
      "epoch": 1.2298525704682606,
      "grad_norm": 8.959065437316895,
      "learning_rate": 4.8975122857943116e-05,
      "loss": 3.0935,
      "step": 16100
    },
    {
      "epoch": 1.2374914063096785,
      "grad_norm": 10.18348503112793,
      "learning_rate": 4.89687571614086e-05,
      "loss": 3.0779,
      "step": 16200
    },
    {
      "epoch": 1.2451302421510961,
      "grad_norm": 10.054893493652344,
      "learning_rate": 4.896239146487409e-05,
      "loss": 3.162,
      "step": 16300
    },
    {
      "epoch": 1.252769077992514,
      "grad_norm": 9.365263938903809,
      "learning_rate": 4.8956025768339573e-05,
      "loss": 3.1674,
      "step": 16400
    },
    {
      "epoch": 1.2604079138339317,
      "grad_norm": 9.18578052520752,
      "learning_rate": 4.894966007180506e-05,
      "loss": 3.0975,
      "step": 16500
    },
    {
      "epoch": 1.2680467496753494,
      "grad_norm": 10.982135772705078,
      "learning_rate": 4.894329437527054e-05,
      "loss": 3.1117,
      "step": 16600
    },
    {
      "epoch": 1.2756855855167673,
      "grad_norm": 8.533815383911133,
      "learning_rate": 4.8936928678736024e-05,
      "loss": 3.0806,
      "step": 16700
    },
    {
      "epoch": 1.2833244213581851,
      "grad_norm": 7.991400718688965,
      "learning_rate": 4.8930562982201514e-05,
      "loss": 3.0899,
      "step": 16800
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 12.131318092346191,
      "learning_rate": 4.8924197285667e-05,
      "loss": 3.102,
      "step": 16900
    },
    {
      "epoch": 1.2986020930410205,
      "grad_norm": 18.22459602355957,
      "learning_rate": 4.891783158913248e-05,
      "loss": 3.0419,
      "step": 17000
    },
    {
      "epoch": 1.3062409288824384,
      "grad_norm": 9.459694862365723,
      "learning_rate": 4.891146589259797e-05,
      "loss": 3.0872,
      "step": 17100
    },
    {
      "epoch": 1.313879764723856,
      "grad_norm": 9.755025863647461,
      "learning_rate": 4.8905100196063455e-05,
      "loss": 3.098,
      "step": 17200
    },
    {
      "epoch": 1.321518600565274,
      "grad_norm": 7.857586860656738,
      "learning_rate": 4.889873449952894e-05,
      "loss": 3.0699,
      "step": 17300
    },
    {
      "epoch": 1.3291574364066916,
      "grad_norm": 13.449252128601074,
      "learning_rate": 4.889236880299443e-05,
      "loss": 3.0909,
      "step": 17400
    },
    {
      "epoch": 1.3367962722481095,
      "grad_norm": 8.583477973937988,
      "learning_rate": 4.888600310645991e-05,
      "loss": 3.0324,
      "step": 17500
    },
    {
      "epoch": 1.3444351080895272,
      "grad_norm": 7.17906379699707,
      "learning_rate": 4.8879637409925396e-05,
      "loss": 3.003,
      "step": 17600
    },
    {
      "epoch": 1.3520739439309448,
      "grad_norm": 10.57936954498291,
      "learning_rate": 4.8873271713390887e-05,
      "loss": 3.1056,
      "step": 17700
    },
    {
      "epoch": 1.3597127797723627,
      "grad_norm": 10.277166366577148,
      "learning_rate": 4.886690601685637e-05,
      "loss": 3.046,
      "step": 17800
    },
    {
      "epoch": 1.3673516156137804,
      "grad_norm": 8.733531951904297,
      "learning_rate": 4.8860540320321854e-05,
      "loss": 3.1122,
      "step": 17900
    },
    {
      "epoch": 1.3749904514551983,
      "grad_norm": 9.70688533782959,
      "learning_rate": 4.885417462378734e-05,
      "loss": 3.0281,
      "step": 18000
    },
    {
      "epoch": 1.382629287296616,
      "grad_norm": 12.337438583374023,
      "learning_rate": 4.884780892725283e-05,
      "loss": 3.1372,
      "step": 18100
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 10.536110877990723,
      "learning_rate": 4.884144323071831e-05,
      "loss": 3.0678,
      "step": 18200
    },
    {
      "epoch": 1.3979069589794515,
      "grad_norm": 9.585892677307129,
      "learning_rate": 4.8835077534183795e-05,
      "loss": 3.056,
      "step": 18300
    },
    {
      "epoch": 1.4055457948208692,
      "grad_norm": 9.080647468566895,
      "learning_rate": 4.882871183764928e-05,
      "loss": 3.013,
      "step": 18400
    },
    {
      "epoch": 1.413184630662287,
      "grad_norm": 9.82056999206543,
      "learning_rate": 4.882234614111476e-05,
      "loss": 3.043,
      "step": 18500
    },
    {
      "epoch": 1.420823466503705,
      "grad_norm": 14.279451370239258,
      "learning_rate": 4.881598044458025e-05,
      "loss": 3.0063,
      "step": 18600
    },
    {
      "epoch": 1.4284623023451226,
      "grad_norm": 8.40251350402832,
      "learning_rate": 4.8809614748045735e-05,
      "loss": 2.9619,
      "step": 18700
    },
    {
      "epoch": 1.4361011381865403,
      "grad_norm": 9.723932266235352,
      "learning_rate": 4.880324905151122e-05,
      "loss": 2.9815,
      "step": 18800
    },
    {
      "epoch": 1.4437399740279582,
      "grad_norm": 7.69944953918457,
      "learning_rate": 4.87968833549767e-05,
      "loss": 3.0033,
      "step": 18900
    },
    {
      "epoch": 1.4513788098693758,
      "grad_norm": 7.924909591674805,
      "learning_rate": 4.8790517658442186e-05,
      "loss": 3.0285,
      "step": 19000
    },
    {
      "epoch": 1.4590176457107937,
      "grad_norm": 9.778804779052734,
      "learning_rate": 4.8784151961907676e-05,
      "loss": 3.0511,
      "step": 19100
    },
    {
      "epoch": 1.4666564815522114,
      "grad_norm": 9.99161148071289,
      "learning_rate": 4.877778626537316e-05,
      "loss": 3.058,
      "step": 19200
    },
    {
      "epoch": 1.4742953173936293,
      "grad_norm": 13.470492362976074,
      "learning_rate": 4.8771420568838643e-05,
      "loss": 2.9832,
      "step": 19300
    },
    {
      "epoch": 1.481934153235047,
      "grad_norm": 10.873525619506836,
      "learning_rate": 4.876505487230413e-05,
      "loss": 3.0362,
      "step": 19400
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 8.912353515625,
      "learning_rate": 4.875868917576962e-05,
      "loss": 3.024,
      "step": 19500
    },
    {
      "epoch": 1.4972118249178825,
      "grad_norm": 11.43610954284668,
      "learning_rate": 4.87523234792351e-05,
      "loss": 3.0375,
      "step": 19600
    },
    {
      "epoch": 1.5048506607593004,
      "grad_norm": 12.332926750183105,
      "learning_rate": 4.8745957782700584e-05,
      "loss": 2.9421,
      "step": 19700
    },
    {
      "epoch": 1.512489496600718,
      "grad_norm": 11.877900123596191,
      "learning_rate": 4.873959208616607e-05,
      "loss": 3.0219,
      "step": 19800
    },
    {
      "epoch": 1.5201283324421357,
      "grad_norm": 10.170271873474121,
      "learning_rate": 4.873322638963155e-05,
      "loss": 3.0547,
      "step": 19900
    },
    {
      "epoch": 1.5277671682835536,
      "grad_norm": 10.463470458984375,
      "learning_rate": 4.872686069309704e-05,
      "loss": 3.0485,
      "step": 20000
    },
    {
      "epoch": 1.5354060041249713,
      "grad_norm": 10.622480392456055,
      "learning_rate": 4.8720494996562525e-05,
      "loss": 3.0178,
      "step": 20100
    },
    {
      "epoch": 1.543044839966389,
      "grad_norm": 9.698973655700684,
      "learning_rate": 4.871412930002801e-05,
      "loss": 3.0303,
      "step": 20200
    },
    {
      "epoch": 1.5506836758078069,
      "grad_norm": 9.204853057861328,
      "learning_rate": 4.870776360349349e-05,
      "loss": 2.9878,
      "step": 20300
    },
    {
      "epoch": 1.5583225116492248,
      "grad_norm": 9.105875015258789,
      "learning_rate": 4.870139790695898e-05,
      "loss": 3.0278,
      "step": 20400
    },
    {
      "epoch": 1.5659613474906424,
      "grad_norm": 8.193436622619629,
      "learning_rate": 4.8695032210424466e-05,
      "loss": 2.9961,
      "step": 20500
    },
    {
      "epoch": 1.57360018333206,
      "grad_norm": 10.074949264526367,
      "learning_rate": 4.868866651388995e-05,
      "loss": 3.0126,
      "step": 20600
    },
    {
      "epoch": 1.581239019173478,
      "grad_norm": 10.047717094421387,
      "learning_rate": 4.868230081735543e-05,
      "loss": 3.0119,
      "step": 20700
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 10.637880325317383,
      "learning_rate": 4.867593512082092e-05,
      "loss": 2.9937,
      "step": 20800
    },
    {
      "epoch": 1.5965166908563135,
      "grad_norm": 12.24228572845459,
      "learning_rate": 4.866956942428641e-05,
      "loss": 2.9939,
      "step": 20900
    },
    {
      "epoch": 1.6041555266977312,
      "grad_norm": 9.795334815979004,
      "learning_rate": 4.866320372775189e-05,
      "loss": 3.0192,
      "step": 21000
    },
    {
      "epoch": 1.611794362539149,
      "grad_norm": 8.746026039123535,
      "learning_rate": 4.865683803121738e-05,
      "loss": 3.0714,
      "step": 21100
    },
    {
      "epoch": 1.6194331983805668,
      "grad_norm": 7.9604315757751465,
      "learning_rate": 4.8650472334682865e-05,
      "loss": 3.026,
      "step": 21200
    },
    {
      "epoch": 1.6270720342219844,
      "grad_norm": 8.614174842834473,
      "learning_rate": 4.864410663814835e-05,
      "loss": 3.0282,
      "step": 21300
    },
    {
      "epoch": 1.6347108700634023,
      "grad_norm": 9.603585243225098,
      "learning_rate": 4.863774094161384e-05,
      "loss": 3.019,
      "step": 21400
    },
    {
      "epoch": 1.6423497059048202,
      "grad_norm": 12.219368934631348,
      "learning_rate": 4.863137524507932e-05,
      "loss": 3.0188,
      "step": 21500
    },
    {
      "epoch": 1.6499885417462379,
      "grad_norm": 10.912591934204102,
      "learning_rate": 4.8625009548544805e-05,
      "loss": 2.9383,
      "step": 21600
    },
    {
      "epoch": 1.6576273775876555,
      "grad_norm": 12.81847095489502,
      "learning_rate": 4.861864385201029e-05,
      "loss": 3.0281,
      "step": 21700
    },
    {
      "epoch": 1.6652662134290734,
      "grad_norm": 9.213736534118652,
      "learning_rate": 4.861227815547578e-05,
      "loss": 2.9697,
      "step": 21800
    },
    {
      "epoch": 1.6729050492704913,
      "grad_norm": 9.367480278015137,
      "learning_rate": 4.860591245894126e-05,
      "loss": 2.9467,
      "step": 21900
    },
    {
      "epoch": 1.680543885111909,
      "grad_norm": 13.518940925598145,
      "learning_rate": 4.8599546762406746e-05,
      "loss": 2.9407,
      "step": 22000
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 12.030056953430176,
      "learning_rate": 4.859318106587223e-05,
      "loss": 2.906,
      "step": 22100
    },
    {
      "epoch": 1.6958215567947446,
      "grad_norm": 8.682583808898926,
      "learning_rate": 4.8586815369337713e-05,
      "loss": 2.9296,
      "step": 22200
    },
    {
      "epoch": 1.7034603926361622,
      "grad_norm": 10.564687728881836,
      "learning_rate": 4.8580449672803204e-05,
      "loss": 2.9289,
      "step": 22300
    },
    {
      "epoch": 1.71109922847758,
      "grad_norm": 9.449978828430176,
      "learning_rate": 4.857408397626869e-05,
      "loss": 2.9506,
      "step": 22400
    },
    {
      "epoch": 1.7187380643189978,
      "grad_norm": 9.402620315551758,
      "learning_rate": 4.856771827973417e-05,
      "loss": 2.9678,
      "step": 22500
    },
    {
      "epoch": 1.7263769001604157,
      "grad_norm": 10.5589599609375,
      "learning_rate": 4.8561352583199654e-05,
      "loss": 2.9998,
      "step": 22600
    },
    {
      "epoch": 1.7340157360018333,
      "grad_norm": 9.154559135437012,
      "learning_rate": 4.8554986886665145e-05,
      "loss": 3.0418,
      "step": 22700
    },
    {
      "epoch": 1.741654571843251,
      "grad_norm": 9.01239013671875,
      "learning_rate": 4.854862119013063e-05,
      "loss": 3.0019,
      "step": 22800
    },
    {
      "epoch": 1.749293407684669,
      "grad_norm": 12.00153636932373,
      "learning_rate": 4.854225549359611e-05,
      "loss": 2.9507,
      "step": 22900
    },
    {
      "epoch": 1.7569322435260868,
      "grad_norm": 11.478479385375977,
      "learning_rate": 4.8535889797061595e-05,
      "loss": 2.9529,
      "step": 23000
    },
    {
      "epoch": 1.7645710793675042,
      "grad_norm": 16.088150024414062,
      "learning_rate": 4.852952410052708e-05,
      "loss": 2.9567,
      "step": 23100
    },
    {
      "epoch": 1.7722099152089221,
      "grad_norm": 9.272978782653809,
      "learning_rate": 4.852315840399257e-05,
      "loss": 2.9677,
      "step": 23200
    },
    {
      "epoch": 1.77984875105034,
      "grad_norm": 8.660088539123535,
      "learning_rate": 4.851679270745805e-05,
      "loss": 3.0252,
      "step": 23300
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 11.296581268310547,
      "learning_rate": 4.8510427010923536e-05,
      "loss": 2.9542,
      "step": 23400
    },
    {
      "epoch": 1.7951264227331754,
      "grad_norm": 9.621682167053223,
      "learning_rate": 4.850406131438902e-05,
      "loss": 2.9389,
      "step": 23500
    },
    {
      "epoch": 1.8027652585745932,
      "grad_norm": 10.5322904586792,
      "learning_rate": 4.849769561785451e-05,
      "loss": 2.9834,
      "step": 23600
    },
    {
      "epoch": 1.8104040944160111,
      "grad_norm": 14.708633422851562,
      "learning_rate": 4.8491329921319994e-05,
      "loss": 3.0024,
      "step": 23700
    },
    {
      "epoch": 1.8180429302574288,
      "grad_norm": 11.514076232910156,
      "learning_rate": 4.848496422478548e-05,
      "loss": 2.9575,
      "step": 23800
    },
    {
      "epoch": 1.8256817660988465,
      "grad_norm": 11.949612617492676,
      "learning_rate": 4.847859852825096e-05,
      "loss": 2.9405,
      "step": 23900
    },
    {
      "epoch": 1.8333206019402644,
      "grad_norm": 9.849968910217285,
      "learning_rate": 4.8472232831716444e-05,
      "loss": 2.9581,
      "step": 24000
    },
    {
      "epoch": 1.840959437781682,
      "grad_norm": 10.104744911193848,
      "learning_rate": 4.8465867135181935e-05,
      "loss": 2.905,
      "step": 24100
    },
    {
      "epoch": 1.8485982736230997,
      "grad_norm": 10.740575790405273,
      "learning_rate": 4.845950143864742e-05,
      "loss": 3.0311,
      "step": 24200
    },
    {
      "epoch": 1.8562371094645176,
      "grad_norm": 9.764845848083496,
      "learning_rate": 4.84531357421129e-05,
      "loss": 2.8283,
      "step": 24300
    },
    {
      "epoch": 1.8638759453059355,
      "grad_norm": 13.80967903137207,
      "learning_rate": 4.8446770045578385e-05,
      "loss": 2.9337,
      "step": 24400
    },
    {
      "epoch": 1.8715147811473531,
      "grad_norm": 10.127481460571289,
      "learning_rate": 4.8440404349043875e-05,
      "loss": 2.9885,
      "step": 24500
    },
    {
      "epoch": 1.8791536169887708,
      "grad_norm": 12.357507705688477,
      "learning_rate": 4.843403865250936e-05,
      "loss": 3.016,
      "step": 24600
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 13.97321891784668,
      "learning_rate": 4.842767295597484e-05,
      "loss": 2.9838,
      "step": 24700
    },
    {
      "epoch": 1.8944312886716066,
      "grad_norm": 17.089975357055664,
      "learning_rate": 4.8421307259440326e-05,
      "loss": 2.9471,
      "step": 24800
    },
    {
      "epoch": 1.9020701245130243,
      "grad_norm": 10.099959373474121,
      "learning_rate": 4.8414941562905816e-05,
      "loss": 2.9141,
      "step": 24900
    },
    {
      "epoch": 1.909708960354442,
      "grad_norm": 9.420065879821777,
      "learning_rate": 4.84085758663713e-05,
      "loss": 2.8708,
      "step": 25000
    },
    {
      "epoch": 1.9173477961958598,
      "grad_norm": 8.7780179977417,
      "learning_rate": 4.840221016983679e-05,
      "loss": 2.9568,
      "step": 25100
    },
    {
      "epoch": 1.9249866320372775,
      "grad_norm": 11.188103675842285,
      "learning_rate": 4.8395844473302274e-05,
      "loss": 2.9552,
      "step": 25200
    },
    {
      "epoch": 1.9326254678786952,
      "grad_norm": 9.551674842834473,
      "learning_rate": 4.838947877676776e-05,
      "loss": 2.9188,
      "step": 25300
    },
    {
      "epoch": 1.940264303720113,
      "grad_norm": 12.932255744934082,
      "learning_rate": 4.838311308023324e-05,
      "loss": 3.0125,
      "step": 25400
    },
    {
      "epoch": 1.947903139561531,
      "grad_norm": 11.809669494628906,
      "learning_rate": 4.837674738369873e-05,
      "loss": 2.9688,
      "step": 25500
    },
    {
      "epoch": 1.9555419754029486,
      "grad_norm": 8.819212913513184,
      "learning_rate": 4.8370381687164215e-05,
      "loss": 2.9379,
      "step": 25600
    },
    {
      "epoch": 1.9631808112443663,
      "grad_norm": 11.064353942871094,
      "learning_rate": 4.83640159906297e-05,
      "loss": 2.9625,
      "step": 25700
    },
    {
      "epoch": 1.9708196470857842,
      "grad_norm": 11.72291088104248,
      "learning_rate": 4.835765029409518e-05,
      "loss": 2.9587,
      "step": 25800
    },
    {
      "epoch": 1.978458482927202,
      "grad_norm": 15.697469711303711,
      "learning_rate": 4.835128459756067e-05,
      "loss": 2.8665,
      "step": 25900
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 14.307368278503418,
      "learning_rate": 4.8344918901026156e-05,
      "loss": 2.8486,
      "step": 26000
    },
    {
      "epoch": 1.9937361546100374,
      "grad_norm": 11.005256652832031,
      "learning_rate": 4.833855320449164e-05,
      "loss": 3.0449,
      "step": 26100
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.8311781883239746,
      "eval_runtime": 1.6395,
      "eval_samples_per_second": 420.865,
      "eval_steps_per_second": 420.865,
      "step": 26182
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.7914795875549316,
      "eval_runtime": 30.7823,
      "eval_samples_per_second": 425.276,
      "eval_steps_per_second": 425.276,
      "step": 26182
    },
    {
      "epoch": 2.0013749904514553,
      "grad_norm": 9.331389427185059,
      "learning_rate": 4.833218750795712e-05,
      "loss": 2.8924,
      "step": 26200
    },
    {
      "epoch": 2.009013826292873,
      "grad_norm": 29.2504825592041,
      "learning_rate": 4.8325821811422606e-05,
      "loss": 2.8653,
      "step": 26300
    },
    {
      "epoch": 2.0166526621342906,
      "grad_norm": 11.106678009033203,
      "learning_rate": 4.8319456114888097e-05,
      "loss": 2.8892,
      "step": 26400
    },
    {
      "epoch": 2.0242914979757085,
      "grad_norm": 10.799656867980957,
      "learning_rate": 4.831309041835358e-05,
      "loss": 2.8939,
      "step": 26500
    },
    {
      "epoch": 2.0319303338171264,
      "grad_norm": 10.337395668029785,
      "learning_rate": 4.8306724721819064e-05,
      "loss": 2.844,
      "step": 26600
    },
    {
      "epoch": 2.039569169658544,
      "grad_norm": 11.485433578491211,
      "learning_rate": 4.830035902528455e-05,
      "loss": 2.8031,
      "step": 26700
    },
    {
      "epoch": 2.0472080054999617,
      "grad_norm": 11.81839370727539,
      "learning_rate": 4.829399332875004e-05,
      "loss": 2.8206,
      "step": 26800
    },
    {
      "epoch": 2.0548468413413796,
      "grad_norm": 12.485873222351074,
      "learning_rate": 4.828762763221552e-05,
      "loss": 2.9049,
      "step": 26900
    },
    {
      "epoch": 2.0624856771827975,
      "grad_norm": 14.722820281982422,
      "learning_rate": 4.8281261935681005e-05,
      "loss": 2.9053,
      "step": 27000
    },
    {
      "epoch": 2.070124513024215,
      "grad_norm": 10.567130088806152,
      "learning_rate": 4.827489623914649e-05,
      "loss": 2.8247,
      "step": 27100
    },
    {
      "epoch": 2.077763348865633,
      "grad_norm": 11.353724479675293,
      "learning_rate": 4.826853054261197e-05,
      "loss": 2.9769,
      "step": 27200
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 10.722121238708496,
      "learning_rate": 4.826216484607746e-05,
      "loss": 2.9332,
      "step": 27300
    },
    {
      "epoch": 2.093041020548468,
      "grad_norm": 11.700359344482422,
      "learning_rate": 4.8255799149542945e-05,
      "loss": 2.8992,
      "step": 27400
    },
    {
      "epoch": 2.100679856389886,
      "grad_norm": 11.547778129577637,
      "learning_rate": 4.824943345300843e-05,
      "loss": 2.9431,
      "step": 27500
    },
    {
      "epoch": 2.108318692231304,
      "grad_norm": 10.34334659576416,
      "learning_rate": 4.824306775647391e-05,
      "loss": 2.8479,
      "step": 27600
    },
    {
      "epoch": 2.115957528072722,
      "grad_norm": 13.091033935546875,
      "learning_rate": 4.8236702059939396e-05,
      "loss": 2.9378,
      "step": 27700
    },
    {
      "epoch": 2.1235963639141393,
      "grad_norm": 10.59287166595459,
      "learning_rate": 4.8230336363404886e-05,
      "loss": 2.9073,
      "step": 27800
    },
    {
      "epoch": 2.131235199755557,
      "grad_norm": 12.563579559326172,
      "learning_rate": 4.822397066687037e-05,
      "loss": 2.8987,
      "step": 27900
    },
    {
      "epoch": 2.138874035596975,
      "grad_norm": 9.97887134552002,
      "learning_rate": 4.8217604970335853e-05,
      "loss": 2.8199,
      "step": 28000
    },
    {
      "epoch": 2.146512871438393,
      "grad_norm": 10.76391315460205,
      "learning_rate": 4.821123927380134e-05,
      "loss": 2.857,
      "step": 28100
    },
    {
      "epoch": 2.1541517072798104,
      "grad_norm": 10.023612022399902,
      "learning_rate": 4.820487357726683e-05,
      "loss": 2.9105,
      "step": 28200
    },
    {
      "epoch": 2.1617905431212283,
      "grad_norm": 11.95870304107666,
      "learning_rate": 4.819850788073231e-05,
      "loss": 2.8836,
      "step": 28300
    },
    {
      "epoch": 2.169429378962646,
      "grad_norm": 11.894952774047852,
      "learning_rate": 4.8192142184197794e-05,
      "loss": 2.8646,
      "step": 28400
    },
    {
      "epoch": 2.1770682148040637,
      "grad_norm": 9.714983940124512,
      "learning_rate": 4.818577648766328e-05,
      "loss": 2.9562,
      "step": 28500
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 11.63553524017334,
      "learning_rate": 4.817941079112877e-05,
      "loss": 2.9015,
      "step": 28600
    },
    {
      "epoch": 2.1923458864868994,
      "grad_norm": 12.315688133239746,
      "learning_rate": 4.817304509459425e-05,
      "loss": 2.9397,
      "step": 28700
    },
    {
      "epoch": 2.1999847223283173,
      "grad_norm": 12.90854549407959,
      "learning_rate": 4.8166679398059735e-05,
      "loss": 2.8848,
      "step": 28800
    },
    {
      "epoch": 2.2076235581697348,
      "grad_norm": 9.81644058227539,
      "learning_rate": 4.8160313701525226e-05,
      "loss": 2.8272,
      "step": 28900
    },
    {
      "epoch": 2.2152623940111527,
      "grad_norm": 9.624555587768555,
      "learning_rate": 4.815394800499071e-05,
      "loss": 2.8868,
      "step": 29000
    },
    {
      "epoch": 2.2229012298525705,
      "grad_norm": 12.266203880310059,
      "learning_rate": 4.81475823084562e-05,
      "loss": 2.8787,
      "step": 29100
    },
    {
      "epoch": 2.2305400656939884,
      "grad_norm": 10.647147178649902,
      "learning_rate": 4.814121661192168e-05,
      "loss": 2.8772,
      "step": 29200
    },
    {
      "epoch": 2.238178901535406,
      "grad_norm": 12.665358543395996,
      "learning_rate": 4.8134850915387167e-05,
      "loss": 2.8841,
      "step": 29300
    },
    {
      "epoch": 2.2458177373768238,
      "grad_norm": 11.808242797851562,
      "learning_rate": 4.812848521885265e-05,
      "loss": 2.8497,
      "step": 29400
    },
    {
      "epoch": 2.2534565732182417,
      "grad_norm": 12.329179763793945,
      "learning_rate": 4.8122119522318134e-05,
      "loss": 2.8631,
      "step": 29500
    },
    {
      "epoch": 2.261095409059659,
      "grad_norm": 11.916447639465332,
      "learning_rate": 4.8115753825783624e-05,
      "loss": 2.8713,
      "step": 29600
    },
    {
      "epoch": 2.268734244901077,
      "grad_norm": 11.27863883972168,
      "learning_rate": 4.810938812924911e-05,
      "loss": 2.8447,
      "step": 29700
    },
    {
      "epoch": 2.276373080742495,
      "grad_norm": 9.73775577545166,
      "learning_rate": 4.810302243271459e-05,
      "loss": 2.886,
      "step": 29800
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 12.297455787658691,
      "learning_rate": 4.8096656736180075e-05,
      "loss": 2.8606,
      "step": 29900
    },
    {
      "epoch": 2.2916507524253302,
      "grad_norm": 11.55332088470459,
      "learning_rate": 4.8090291039645565e-05,
      "loss": 2.7663,
      "step": 30000
    },
    {
      "epoch": 2.299289588266748,
      "grad_norm": 10.544173240661621,
      "learning_rate": 4.808392534311105e-05,
      "loss": 2.843,
      "step": 30100
    },
    {
      "epoch": 2.306928424108166,
      "grad_norm": 19.929471969604492,
      "learning_rate": 4.807755964657653e-05,
      "loss": 2.8707,
      "step": 30200
    },
    {
      "epoch": 2.314567259949584,
      "grad_norm": 11.517016410827637,
      "learning_rate": 4.8071193950042015e-05,
      "loss": 2.8607,
      "step": 30300
    },
    {
      "epoch": 2.3222060957910013,
      "grad_norm": 14.09231185913086,
      "learning_rate": 4.80648282535075e-05,
      "loss": 2.8852,
      "step": 30400
    },
    {
      "epoch": 2.3298449316324192,
      "grad_norm": 12.237916946411133,
      "learning_rate": 4.805846255697299e-05,
      "loss": 2.8789,
      "step": 30500
    },
    {
      "epoch": 2.337483767473837,
      "grad_norm": 11.620223045349121,
      "learning_rate": 4.805209686043847e-05,
      "loss": 2.8261,
      "step": 30600
    },
    {
      "epoch": 2.3451226033152546,
      "grad_norm": 10.878264427185059,
      "learning_rate": 4.8045731163903956e-05,
      "loss": 2.8783,
      "step": 30700
    },
    {
      "epoch": 2.3527614391566725,
      "grad_norm": 12.493897438049316,
      "learning_rate": 4.803936546736944e-05,
      "loss": 2.9493,
      "step": 30800
    },
    {
      "epoch": 2.3604002749980904,
      "grad_norm": 11.600050926208496,
      "learning_rate": 4.8032999770834923e-05,
      "loss": 2.7928,
      "step": 30900
    },
    {
      "epoch": 2.3680391108395082,
      "grad_norm": 9.597686767578125,
      "learning_rate": 4.8026634074300414e-05,
      "loss": 2.8256,
      "step": 31000
    },
    {
      "epoch": 2.3756779466809257,
      "grad_norm": 12.788039207458496,
      "learning_rate": 4.80202683777659e-05,
      "loss": 2.8208,
      "step": 31100
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 10.304950714111328,
      "learning_rate": 4.801390268123138e-05,
      "loss": 2.889,
      "step": 31200
    },
    {
      "epoch": 2.3909556183637615,
      "grad_norm": 9.402846336364746,
      "learning_rate": 4.8007536984696864e-05,
      "loss": 2.8537,
      "step": 31300
    },
    {
      "epoch": 2.3985944542051794,
      "grad_norm": 11.044880867004395,
      "learning_rate": 4.8001171288162355e-05,
      "loss": 2.8237,
      "step": 31400
    },
    {
      "epoch": 2.406233290046597,
      "grad_norm": 13.778874397277832,
      "learning_rate": 4.799480559162784e-05,
      "loss": 2.832,
      "step": 31500
    },
    {
      "epoch": 2.4138721258880147,
      "grad_norm": 14.691864967346191,
      "learning_rate": 4.798843989509332e-05,
      "loss": 2.8582,
      "step": 31600
    },
    {
      "epoch": 2.4215109617294326,
      "grad_norm": 11.53106689453125,
      "learning_rate": 4.7982074198558805e-05,
      "loss": 2.8518,
      "step": 31700
    },
    {
      "epoch": 2.42914979757085,
      "grad_norm": 9.284077644348145,
      "learning_rate": 4.797570850202429e-05,
      "loss": 2.8891,
      "step": 31800
    },
    {
      "epoch": 2.436788633412268,
      "grad_norm": 9.396300315856934,
      "learning_rate": 4.796934280548978e-05,
      "loss": 2.8757,
      "step": 31900
    },
    {
      "epoch": 2.444427469253686,
      "grad_norm": 10.805200576782227,
      "learning_rate": 4.796297710895526e-05,
      "loss": 2.8991,
      "step": 32000
    },
    {
      "epoch": 2.4520663050951033,
      "grad_norm": 8.425480842590332,
      "learning_rate": 4.7956611412420746e-05,
      "loss": 2.7901,
      "step": 32100
    },
    {
      "epoch": 2.459705140936521,
      "grad_norm": 11.781017303466797,
      "learning_rate": 4.795024571588623e-05,
      "loss": 2.8653,
      "step": 32200
    },
    {
      "epoch": 2.467343976777939,
      "grad_norm": 10.50875473022461,
      "learning_rate": 4.794388001935172e-05,
      "loss": 2.8465,
      "step": 32300
    },
    {
      "epoch": 2.474982812619357,
      "grad_norm": 12.720168113708496,
      "learning_rate": 4.7937514322817204e-05,
      "loss": 2.7924,
      "step": 32400
    },
    {
      "epoch": 2.482621648460775,
      "grad_norm": 8.67225170135498,
      "learning_rate": 4.793114862628269e-05,
      "loss": 2.8538,
      "step": 32500
    },
    {
      "epoch": 2.4902604843021923,
      "grad_norm": 11.93941879272461,
      "learning_rate": 4.792478292974818e-05,
      "loss": 2.8026,
      "step": 32600
    },
    {
      "epoch": 2.49789932014361,
      "grad_norm": 8.763579368591309,
      "learning_rate": 4.791841723321366e-05,
      "loss": 2.8525,
      "step": 32700
    },
    {
      "epoch": 2.505538155985028,
      "grad_norm": 11.570107460021973,
      "learning_rate": 4.7912051536679145e-05,
      "loss": 2.8641,
      "step": 32800
    },
    {
      "epoch": 2.5131769918264455,
      "grad_norm": 9.493631362915039,
      "learning_rate": 4.7905685840144635e-05,
      "loss": 2.8505,
      "step": 32900
    },
    {
      "epoch": 2.5208158276678634,
      "grad_norm": 14.715714454650879,
      "learning_rate": 4.789932014361012e-05,
      "loss": 2.8639,
      "step": 33000
    },
    {
      "epoch": 2.5284546635092813,
      "grad_norm": 10.055550575256348,
      "learning_rate": 4.78929544470756e-05,
      "loss": 2.7636,
      "step": 33100
    },
    {
      "epoch": 2.5360934993506987,
      "grad_norm": 12.10680103302002,
      "learning_rate": 4.7886588750541085e-05,
      "loss": 2.7673,
      "step": 33200
    },
    {
      "epoch": 2.5437323351921166,
      "grad_norm": 12.010740280151367,
      "learning_rate": 4.7880223054006576e-05,
      "loss": 2.8513,
      "step": 33300
    },
    {
      "epoch": 2.5513711710335345,
      "grad_norm": 10.002538681030273,
      "learning_rate": 4.787385735747206e-05,
      "loss": 2.8246,
      "step": 33400
    },
    {
      "epoch": 2.5590100068749524,
      "grad_norm": 10.887938499450684,
      "learning_rate": 4.786749166093754e-05,
      "loss": 2.8418,
      "step": 33500
    },
    {
      "epoch": 2.5666488427163703,
      "grad_norm": 10.172117233276367,
      "learning_rate": 4.7861125964403026e-05,
      "loss": 2.8576,
      "step": 33600
    },
    {
      "epoch": 2.5742876785577877,
      "grad_norm": 10.388664245605469,
      "learning_rate": 4.785476026786852e-05,
      "loss": 2.7839,
      "step": 33700
    },
    {
      "epoch": 2.5819265143992056,
      "grad_norm": 14.365283966064453,
      "learning_rate": 4.7848394571334e-05,
      "loss": 2.8522,
      "step": 33800
    },
    {
      "epoch": 2.5895653502406235,
      "grad_norm": 12.553672790527344,
      "learning_rate": 4.7842028874799484e-05,
      "loss": 2.8226,
      "step": 33900
    },
    {
      "epoch": 2.597204186082041,
      "grad_norm": 11.39941692352295,
      "learning_rate": 4.783566317826497e-05,
      "loss": 2.8,
      "step": 34000
    },
    {
      "epoch": 2.604843021923459,
      "grad_norm": 10.456103324890137,
      "learning_rate": 4.782929748173045e-05,
      "loss": 2.8426,
      "step": 34100
    },
    {
      "epoch": 2.6124818577648767,
      "grad_norm": 11.0621976852417,
      "learning_rate": 4.782293178519594e-05,
      "loss": 2.7924,
      "step": 34200
    },
    {
      "epoch": 2.620120693606294,
      "grad_norm": 10.28817081451416,
      "learning_rate": 4.7816566088661425e-05,
      "loss": 2.793,
      "step": 34300
    },
    {
      "epoch": 2.627759529447712,
      "grad_norm": 14.912901878356934,
      "learning_rate": 4.781020039212691e-05,
      "loss": 2.7049,
      "step": 34400
    },
    {
      "epoch": 2.63539836528913,
      "grad_norm": 10.545271873474121,
      "learning_rate": 4.780383469559239e-05,
      "loss": 2.733,
      "step": 34500
    },
    {
      "epoch": 2.643037201130548,
      "grad_norm": 10.923068046569824,
      "learning_rate": 4.779746899905788e-05,
      "loss": 2.7834,
      "step": 34600
    },
    {
      "epoch": 2.6506760369719657,
      "grad_norm": 10.680488586425781,
      "learning_rate": 4.7791103302523366e-05,
      "loss": 2.7565,
      "step": 34700
    },
    {
      "epoch": 2.658314872813383,
      "grad_norm": 11.07167911529541,
      "learning_rate": 4.778473760598885e-05,
      "loss": 2.8088,
      "step": 34800
    },
    {
      "epoch": 2.665953708654801,
      "grad_norm": 12.763583183288574,
      "learning_rate": 4.777837190945433e-05,
      "loss": 2.8584,
      "step": 34900
    },
    {
      "epoch": 2.673592544496219,
      "grad_norm": 10.125687599182129,
      "learning_rate": 4.7772006212919816e-05,
      "loss": 2.8202,
      "step": 35000
    },
    {
      "epoch": 2.6812313803376364,
      "grad_norm": 9.084308624267578,
      "learning_rate": 4.7765640516385307e-05,
      "loss": 2.8433,
      "step": 35100
    },
    {
      "epoch": 2.6888702161790543,
      "grad_norm": 12.054394721984863,
      "learning_rate": 4.775927481985079e-05,
      "loss": 2.7801,
      "step": 35200
    },
    {
      "epoch": 2.696509052020472,
      "grad_norm": 14.025508880615234,
      "learning_rate": 4.7752909123316274e-05,
      "loss": 2.8345,
      "step": 35300
    },
    {
      "epoch": 2.7041478878618896,
      "grad_norm": 13.948503494262695,
      "learning_rate": 4.774654342678176e-05,
      "loss": 2.7396,
      "step": 35400
    },
    {
      "epoch": 2.7117867237033075,
      "grad_norm": 9.858642578125,
      "learning_rate": 4.774017773024725e-05,
      "loss": 2.8665,
      "step": 35500
    },
    {
      "epoch": 2.7194255595447254,
      "grad_norm": 12.253228187561035,
      "learning_rate": 4.773381203371273e-05,
      "loss": 2.8378,
      "step": 35600
    },
    {
      "epoch": 2.7270643953861433,
      "grad_norm": 11.205698013305664,
      "learning_rate": 4.7727446337178215e-05,
      "loss": 2.7878,
      "step": 35700
    },
    {
      "epoch": 2.7347032312275608,
      "grad_norm": 10.241677284240723,
      "learning_rate": 4.77210806406437e-05,
      "loss": 2.7588,
      "step": 35800
    },
    {
      "epoch": 2.7423420670689787,
      "grad_norm": 12.677785873413086,
      "learning_rate": 4.771471494410918e-05,
      "loss": 2.8031,
      "step": 35900
    },
    {
      "epoch": 2.7499809029103965,
      "grad_norm": 10.369302749633789,
      "learning_rate": 4.770834924757467e-05,
      "loss": 2.6907,
      "step": 36000
    },
    {
      "epoch": 2.7576197387518144,
      "grad_norm": 9.42711067199707,
      "learning_rate": 4.7701983551040155e-05,
      "loss": 2.7617,
      "step": 36100
    },
    {
      "epoch": 2.765258574593232,
      "grad_norm": 10.203697204589844,
      "learning_rate": 4.769561785450564e-05,
      "loss": 2.7155,
      "step": 36200
    },
    {
      "epoch": 2.7728974104346498,
      "grad_norm": 12.118961334228516,
      "learning_rate": 4.768925215797113e-05,
      "loss": 2.762,
      "step": 36300
    },
    {
      "epoch": 2.7805362462760677,
      "grad_norm": 15.353365898132324,
      "learning_rate": 4.768288646143661e-05,
      "loss": 2.8165,
      "step": 36400
    },
    {
      "epoch": 2.788175082117485,
      "grad_norm": 11.509020805358887,
      "learning_rate": 4.7676520764902096e-05,
      "loss": 2.8522,
      "step": 36500
    },
    {
      "epoch": 2.795813917958903,
      "grad_norm": 10.219354629516602,
      "learning_rate": 4.767015506836759e-05,
      "loss": 2.8097,
      "step": 36600
    },
    {
      "epoch": 2.803452753800321,
      "grad_norm": 9.927324295043945,
      "learning_rate": 4.766378937183307e-05,
      "loss": 2.8035,
      "step": 36700
    },
    {
      "epoch": 2.8110915896417383,
      "grad_norm": 11.13225269317627,
      "learning_rate": 4.7657423675298554e-05,
      "loss": 2.7838,
      "step": 36800
    },
    {
      "epoch": 2.818730425483156,
      "grad_norm": 12.56425666809082,
      "learning_rate": 4.7651057978764044e-05,
      "loss": 2.7242,
      "step": 36900
    },
    {
      "epoch": 2.826369261324574,
      "grad_norm": 13.784795761108398,
      "learning_rate": 4.764469228222953e-05,
      "loss": 2.7865,
      "step": 37000
    },
    {
      "epoch": 2.834008097165992,
      "grad_norm": 11.030177116394043,
      "learning_rate": 4.763832658569501e-05,
      "loss": 2.7282,
      "step": 37100
    },
    {
      "epoch": 2.84164693300741,
      "grad_norm": 13.28055477142334,
      "learning_rate": 4.7631960889160495e-05,
      "loss": 2.8454,
      "step": 37200
    },
    {
      "epoch": 2.8492857688488273,
      "grad_norm": 10.099113464355469,
      "learning_rate": 4.762559519262598e-05,
      "loss": 2.7435,
      "step": 37300
    },
    {
      "epoch": 2.8569246046902452,
      "grad_norm": 11.845102310180664,
      "learning_rate": 4.761922949609147e-05,
      "loss": 2.8463,
      "step": 37400
    },
    {
      "epoch": 2.864563440531663,
      "grad_norm": 10.881771087646484,
      "learning_rate": 4.761286379955695e-05,
      "loss": 2.7352,
      "step": 37500
    },
    {
      "epoch": 2.8722022763730806,
      "grad_norm": 10.030574798583984,
      "learning_rate": 4.7606498103022436e-05,
      "loss": 2.8543,
      "step": 37600
    },
    {
      "epoch": 2.8798411122144985,
      "grad_norm": 11.320795059204102,
      "learning_rate": 4.760013240648792e-05,
      "loss": 2.7787,
      "step": 37700
    },
    {
      "epoch": 2.8874799480559163,
      "grad_norm": 13.366061210632324,
      "learning_rate": 4.759376670995341e-05,
      "loss": 2.7582,
      "step": 37800
    },
    {
      "epoch": 2.895118783897334,
      "grad_norm": 12.102131843566895,
      "learning_rate": 4.758740101341889e-05,
      "loss": 2.7492,
      "step": 37900
    },
    {
      "epoch": 2.9027576197387517,
      "grad_norm": 12.373821258544922,
      "learning_rate": 4.7581035316884377e-05,
      "loss": 2.7059,
      "step": 38000
    },
    {
      "epoch": 2.9103964555801696,
      "grad_norm": 11.01103401184082,
      "learning_rate": 4.757466962034986e-05,
      "loss": 2.8237,
      "step": 38100
    },
    {
      "epoch": 2.9180352914215875,
      "grad_norm": 12.484685897827148,
      "learning_rate": 4.7568303923815344e-05,
      "loss": 2.8623,
      "step": 38200
    },
    {
      "epoch": 2.9256741272630054,
      "grad_norm": 12.144451141357422,
      "learning_rate": 4.7561938227280834e-05,
      "loss": 2.7681,
      "step": 38300
    },
    {
      "epoch": 2.933312963104423,
      "grad_norm": 11.826107025146484,
      "learning_rate": 4.755557253074632e-05,
      "loss": 2.7702,
      "step": 38400
    },
    {
      "epoch": 2.9409517989458407,
      "grad_norm": 12.492891311645508,
      "learning_rate": 4.75492068342118e-05,
      "loss": 2.8082,
      "step": 38500
    },
    {
      "epoch": 2.9485906347872586,
      "grad_norm": 15.352744102478027,
      "learning_rate": 4.7542841137677285e-05,
      "loss": 2.7524,
      "step": 38600
    },
    {
      "epoch": 2.956229470628676,
      "grad_norm": 13.136030197143555,
      "learning_rate": 4.7536475441142775e-05,
      "loss": 2.7462,
      "step": 38700
    },
    {
      "epoch": 2.963868306470094,
      "grad_norm": 9.906787872314453,
      "learning_rate": 4.753010974460826e-05,
      "loss": 2.6665,
      "step": 38800
    },
    {
      "epoch": 2.971507142311512,
      "grad_norm": 11.492627143859863,
      "learning_rate": 4.752374404807374e-05,
      "loss": 2.896,
      "step": 38900
    },
    {
      "epoch": 2.9791459781529293,
      "grad_norm": 14.7212495803833,
      "learning_rate": 4.7517378351539225e-05,
      "loss": 2.875,
      "step": 39000
    },
    {
      "epoch": 2.986784813994347,
      "grad_norm": 16.52486228942871,
      "learning_rate": 4.751101265500471e-05,
      "loss": 2.6943,
      "step": 39100
    },
    {
      "epoch": 2.994423649835765,
      "grad_norm": 9.795726776123047,
      "learning_rate": 4.75046469584702e-05,
      "loss": 2.7536,
      "step": 39200
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.6477808952331543,
      "eval_runtime": 1.6449,
      "eval_samples_per_second": 419.473,
      "eval_steps_per_second": 419.473,
      "step": 39273
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.599668264389038,
      "eval_runtime": 30.6977,
      "eval_samples_per_second": 426.449,
      "eval_steps_per_second": 426.449,
      "step": 39273
    },
    {
      "epoch": 3.002062485677183,
      "grad_norm": 11.569381713867188,
      "learning_rate": 4.749828126193568e-05,
      "loss": 2.7753,
      "step": 39300
    },
    {
      "epoch": 3.0097013215186004,
      "grad_norm": 10.545292854309082,
      "learning_rate": 4.7491915565401166e-05,
      "loss": 2.6574,
      "step": 39400
    },
    {
      "epoch": 3.0173401573600183,
      "grad_norm": 14.473414421081543,
      "learning_rate": 4.748554986886665e-05,
      "loss": 2.7123,
      "step": 39500
    },
    {
      "epoch": 3.024978993201436,
      "grad_norm": 10.393683433532715,
      "learning_rate": 4.7479184172332133e-05,
      "loss": 2.7107,
      "step": 39600
    },
    {
      "epoch": 3.032617829042854,
      "grad_norm": 17.398468017578125,
      "learning_rate": 4.7472818475797624e-05,
      "loss": 2.7771,
      "step": 39700
    },
    {
      "epoch": 3.0402566648842715,
      "grad_norm": 11.194740295410156,
      "learning_rate": 4.746645277926311e-05,
      "loss": 2.6763,
      "step": 39800
    },
    {
      "epoch": 3.0478955007256894,
      "grad_norm": 9.068788528442383,
      "learning_rate": 4.746008708272859e-05,
      "loss": 2.6732,
      "step": 39900
    },
    {
      "epoch": 3.0555343365671073,
      "grad_norm": 12.55513858795166,
      "learning_rate": 4.7453721386194074e-05,
      "loss": 2.7237,
      "step": 40000
    },
    {
      "epoch": 3.0631731724085247,
      "grad_norm": 15.040215492248535,
      "learning_rate": 4.7447355689659565e-05,
      "loss": 2.7716,
      "step": 40100
    },
    {
      "epoch": 3.0708120082499426,
      "grad_norm": 11.187342643737793,
      "learning_rate": 4.744098999312505e-05,
      "loss": 2.7967,
      "step": 40200
    },
    {
      "epoch": 3.0784508440913605,
      "grad_norm": 10.066903114318848,
      "learning_rate": 4.743462429659054e-05,
      "loss": 2.7204,
      "step": 40300
    },
    {
      "epoch": 3.0860896799327784,
      "grad_norm": 17.00889778137207,
      "learning_rate": 4.742825860005602e-05,
      "loss": 2.8556,
      "step": 40400
    },
    {
      "epoch": 3.093728515774196,
      "grad_norm": 11.522228240966797,
      "learning_rate": 4.7421892903521506e-05,
      "loss": 2.7391,
      "step": 40500
    },
    {
      "epoch": 3.1013673516156137,
      "grad_norm": 11.158319473266602,
      "learning_rate": 4.7415527206986996e-05,
      "loss": 2.7575,
      "step": 40600
    },
    {
      "epoch": 3.1090061874570316,
      "grad_norm": 12.508089065551758,
      "learning_rate": 4.740916151045248e-05,
      "loss": 2.7742,
      "step": 40700
    },
    {
      "epoch": 3.1166450232984495,
      "grad_norm": 11.302560806274414,
      "learning_rate": 4.740279581391796e-05,
      "loss": 2.7217,
      "step": 40800
    },
    {
      "epoch": 3.124283859139867,
      "grad_norm": 16.45519256591797,
      "learning_rate": 4.7396430117383447e-05,
      "loss": 2.7578,
      "step": 40900
    },
    {
      "epoch": 3.131922694981285,
      "grad_norm": 10.14948844909668,
      "learning_rate": 4.739006442084894e-05,
      "loss": 2.7234,
      "step": 41000
    },
    {
      "epoch": 3.1395615308227027,
      "grad_norm": 13.9042329788208,
      "learning_rate": 4.738369872431442e-05,
      "loss": 2.6957,
      "step": 41100
    },
    {
      "epoch": 3.14720036666412,
      "grad_norm": 11.118634223937988,
      "learning_rate": 4.7377333027779904e-05,
      "loss": 2.7781,
      "step": 41200
    },
    {
      "epoch": 3.154839202505538,
      "grad_norm": 14.472848892211914,
      "learning_rate": 4.737096733124539e-05,
      "loss": 2.7226,
      "step": 41300
    },
    {
      "epoch": 3.162478038346956,
      "grad_norm": 10.770501136779785,
      "learning_rate": 4.736460163471087e-05,
      "loss": 2.7342,
      "step": 41400
    },
    {
      "epoch": 3.170116874188374,
      "grad_norm": 8.413652420043945,
      "learning_rate": 4.735823593817636e-05,
      "loss": 2.6703,
      "step": 41500
    },
    {
      "epoch": 3.1777557100297913,
      "grad_norm": 11.62536334991455,
      "learning_rate": 4.7351870241641845e-05,
      "loss": 2.6774,
      "step": 41600
    },
    {
      "epoch": 3.185394545871209,
      "grad_norm": 12.723170280456543,
      "learning_rate": 4.734550454510733e-05,
      "loss": 2.6506,
      "step": 41700
    },
    {
      "epoch": 3.193033381712627,
      "grad_norm": 11.249817848205566,
      "learning_rate": 4.733913884857281e-05,
      "loss": 2.7319,
      "step": 41800
    },
    {
      "epoch": 3.200672217554045,
      "grad_norm": 11.533368110656738,
      "learning_rate": 4.7332773152038295e-05,
      "loss": 2.6554,
      "step": 41900
    },
    {
      "epoch": 3.2083110533954624,
      "grad_norm": 13.684104919433594,
      "learning_rate": 4.7326407455503786e-05,
      "loss": 2.7622,
      "step": 42000
    },
    {
      "epoch": 3.2159498892368803,
      "grad_norm": 13.941910743713379,
      "learning_rate": 4.732004175896927e-05,
      "loss": 2.6495,
      "step": 42100
    },
    {
      "epoch": 3.223588725078298,
      "grad_norm": 13.065519332885742,
      "learning_rate": 4.731367606243475e-05,
      "loss": 2.7673,
      "step": 42200
    },
    {
      "epoch": 3.2312275609197156,
      "grad_norm": 11.79964828491211,
      "learning_rate": 4.7307310365900236e-05,
      "loss": 2.7444,
      "step": 42300
    },
    {
      "epoch": 3.2388663967611335,
      "grad_norm": 10.952837944030762,
      "learning_rate": 4.730094466936573e-05,
      "loss": 2.7191,
      "step": 42400
    },
    {
      "epoch": 3.2465052326025514,
      "grad_norm": 12.006735801696777,
      "learning_rate": 4.729457897283121e-05,
      "loss": 2.7573,
      "step": 42500
    },
    {
      "epoch": 3.2541440684439693,
      "grad_norm": 14.113825798034668,
      "learning_rate": 4.7288213276296694e-05,
      "loss": 2.6562,
      "step": 42600
    },
    {
      "epoch": 3.2617829042853868,
      "grad_norm": 11.828661918640137,
      "learning_rate": 4.728184757976218e-05,
      "loss": 2.7436,
      "step": 42700
    },
    {
      "epoch": 3.2694217401268046,
      "grad_norm": 12.677855491638184,
      "learning_rate": 4.727548188322766e-05,
      "loss": 2.7083,
      "step": 42800
    },
    {
      "epoch": 3.2770605759682225,
      "grad_norm": 12.43783187866211,
      "learning_rate": 4.726911618669315e-05,
      "loss": 2.6473,
      "step": 42900
    },
    {
      "epoch": 3.2846994118096404,
      "grad_norm": 12.415574073791504,
      "learning_rate": 4.7262750490158635e-05,
      "loss": 2.7303,
      "step": 43000
    },
    {
      "epoch": 3.292338247651058,
      "grad_norm": 11.834332466125488,
      "learning_rate": 4.725638479362412e-05,
      "loss": 2.6927,
      "step": 43100
    },
    {
      "epoch": 3.2999770834924758,
      "grad_norm": 11.737163543701172,
      "learning_rate": 4.72500190970896e-05,
      "loss": 2.6935,
      "step": 43200
    },
    {
      "epoch": 3.3076159193338937,
      "grad_norm": 10.888789176940918,
      "learning_rate": 4.724365340055509e-05,
      "loss": 2.7556,
      "step": 43300
    },
    {
      "epoch": 3.315254755175311,
      "grad_norm": 10.078100204467773,
      "learning_rate": 4.7237287704020576e-05,
      "loss": 2.7867,
      "step": 43400
    },
    {
      "epoch": 3.322893591016729,
      "grad_norm": 11.433088302612305,
      "learning_rate": 4.723092200748606e-05,
      "loss": 2.7042,
      "step": 43500
    },
    {
      "epoch": 3.330532426858147,
      "grad_norm": 11.09318733215332,
      "learning_rate": 4.722455631095154e-05,
      "loss": 2.6998,
      "step": 43600
    },
    {
      "epoch": 3.3381712626995648,
      "grad_norm": 13.459965705871582,
      "learning_rate": 4.7218190614417026e-05,
      "loss": 2.6565,
      "step": 43700
    },
    {
      "epoch": 3.345810098540982,
      "grad_norm": 10.118046760559082,
      "learning_rate": 4.7211824917882517e-05,
      "loss": 2.7353,
      "step": 43800
    },
    {
      "epoch": 3.3534489343824,
      "grad_norm": 15.856517791748047,
      "learning_rate": 4.7205459221348e-05,
      "loss": 2.7407,
      "step": 43900
    },
    {
      "epoch": 3.361087770223818,
      "grad_norm": 10.49406623840332,
      "learning_rate": 4.7199093524813484e-05,
      "loss": 2.7369,
      "step": 44000
    },
    {
      "epoch": 3.368726606065236,
      "grad_norm": 9.947816848754883,
      "learning_rate": 4.7192727828278974e-05,
      "loss": 2.5942,
      "step": 44100
    },
    {
      "epoch": 3.3763654419066533,
      "grad_norm": 14.613266944885254,
      "learning_rate": 4.718636213174446e-05,
      "loss": 2.6974,
      "step": 44200
    },
    {
      "epoch": 3.384004277748071,
      "grad_norm": 11.041863441467285,
      "learning_rate": 4.717999643520995e-05,
      "loss": 2.7213,
      "step": 44300
    },
    {
      "epoch": 3.391643113589489,
      "grad_norm": 11.250155448913574,
      "learning_rate": 4.717363073867543e-05,
      "loss": 2.7427,
      "step": 44400
    },
    {
      "epoch": 3.3992819494309066,
      "grad_norm": 12.256178855895996,
      "learning_rate": 4.7167265042140915e-05,
      "loss": 2.77,
      "step": 44500
    },
    {
      "epoch": 3.4069207852723244,
      "grad_norm": 12.829099655151367,
      "learning_rate": 4.71608993456064e-05,
      "loss": 2.7434,
      "step": 44600
    },
    {
      "epoch": 3.4145596211137423,
      "grad_norm": 12.07616901397705,
      "learning_rate": 4.715453364907189e-05,
      "loss": 2.6953,
      "step": 44700
    },
    {
      "epoch": 3.42219845695516,
      "grad_norm": 13.91765022277832,
      "learning_rate": 4.714816795253737e-05,
      "loss": 2.6975,
      "step": 44800
    },
    {
      "epoch": 3.4298372927965777,
      "grad_norm": 14.942191123962402,
      "learning_rate": 4.7141802256002856e-05,
      "loss": 2.7038,
      "step": 44900
    },
    {
      "epoch": 3.4374761286379956,
      "grad_norm": 12.576416969299316,
      "learning_rate": 4.713543655946834e-05,
      "loss": 2.7363,
      "step": 45000
    },
    {
      "epoch": 3.4451149644794135,
      "grad_norm": 11.142033576965332,
      "learning_rate": 4.712907086293382e-05,
      "loss": 2.7644,
      "step": 45100
    },
    {
      "epoch": 3.4527538003208313,
      "grad_norm": 11.953988075256348,
      "learning_rate": 4.712270516639931e-05,
      "loss": 2.7705,
      "step": 45200
    },
    {
      "epoch": 3.460392636162249,
      "grad_norm": 11.556465148925781,
      "learning_rate": 4.71163394698648e-05,
      "loss": 2.6977,
      "step": 45300
    },
    {
      "epoch": 3.4680314720036667,
      "grad_norm": 12.430388450622559,
      "learning_rate": 4.710997377333028e-05,
      "loss": 2.6973,
      "step": 45400
    },
    {
      "epoch": 3.4756703078450846,
      "grad_norm": 10.596195220947266,
      "learning_rate": 4.7103608076795764e-05,
      "loss": 2.7467,
      "step": 45500
    },
    {
      "epoch": 3.483309143686502,
      "grad_norm": 10.148565292358398,
      "learning_rate": 4.7097242380261254e-05,
      "loss": 2.7774,
      "step": 45600
    },
    {
      "epoch": 3.49094797952792,
      "grad_norm": 9.58594036102295,
      "learning_rate": 4.709087668372674e-05,
      "loss": 2.7158,
      "step": 45700
    },
    {
      "epoch": 3.498586815369338,
      "grad_norm": 12.75857162475586,
      "learning_rate": 4.708451098719222e-05,
      "loss": 2.7113,
      "step": 45800
    },
    {
      "epoch": 3.5062256512107552,
      "grad_norm": 13.681456565856934,
      "learning_rate": 4.7078145290657705e-05,
      "loss": 2.5804,
      "step": 45900
    },
    {
      "epoch": 3.513864487052173,
      "grad_norm": 12.175679206848145,
      "learning_rate": 4.707177959412319e-05,
      "loss": 2.6352,
      "step": 46000
    },
    {
      "epoch": 3.521503322893591,
      "grad_norm": 13.389067649841309,
      "learning_rate": 4.706541389758868e-05,
      "loss": 2.7164,
      "step": 46100
    },
    {
      "epoch": 3.529142158735009,
      "grad_norm": 17.626811981201172,
      "learning_rate": 4.705904820105416e-05,
      "loss": 2.7292,
      "step": 46200
    },
    {
      "epoch": 3.536780994576427,
      "grad_norm": 13.490824699401855,
      "learning_rate": 4.7052682504519646e-05,
      "loss": 2.7499,
      "step": 46300
    },
    {
      "epoch": 3.5444198304178443,
      "grad_norm": 11.704219818115234,
      "learning_rate": 4.704631680798513e-05,
      "loss": 2.7262,
      "step": 46400
    },
    {
      "epoch": 3.552058666259262,
      "grad_norm": 11.24171257019043,
      "learning_rate": 4.703995111145062e-05,
      "loss": 2.5831,
      "step": 46500
    },
    {
      "epoch": 3.55969750210068,
      "grad_norm": 11.192695617675781,
      "learning_rate": 4.70335854149161e-05,
      "loss": 2.6668,
      "step": 46600
    },
    {
      "epoch": 3.5673363379420975,
      "grad_norm": 11.633659362792969,
      "learning_rate": 4.7027219718381587e-05,
      "loss": 2.638,
      "step": 46700
    },
    {
      "epoch": 3.5749751737835154,
      "grad_norm": 12.946003913879395,
      "learning_rate": 4.702085402184707e-05,
      "loss": 2.6936,
      "step": 46800
    },
    {
      "epoch": 3.5826140096249333,
      "grad_norm": 12.627045631408691,
      "learning_rate": 4.7014488325312554e-05,
      "loss": 2.6797,
      "step": 46900
    },
    {
      "epoch": 3.5902528454663507,
      "grad_norm": 12.02216911315918,
      "learning_rate": 4.7008122628778044e-05,
      "loss": 2.7634,
      "step": 47000
    },
    {
      "epoch": 3.5978916813077686,
      "grad_norm": 11.276678085327148,
      "learning_rate": 4.700175693224353e-05,
      "loss": 2.6828,
      "step": 47100
    },
    {
      "epoch": 3.6055305171491865,
      "grad_norm": 11.80218505859375,
      "learning_rate": 4.699539123570901e-05,
      "loss": 2.6563,
      "step": 47200
    },
    {
      "epoch": 3.6131693529906044,
      "grad_norm": 13.885468482971191,
      "learning_rate": 4.6989025539174495e-05,
      "loss": 2.7145,
      "step": 47300
    },
    {
      "epoch": 3.6208081888320223,
      "grad_norm": 11.751206398010254,
      "learning_rate": 4.6982659842639985e-05,
      "loss": 2.6373,
      "step": 47400
    },
    {
      "epoch": 3.6284470246734397,
      "grad_norm": 10.778682708740234,
      "learning_rate": 4.697629414610547e-05,
      "loss": 2.6522,
      "step": 47500
    },
    {
      "epoch": 3.6360858605148576,
      "grad_norm": 9.941387176513672,
      "learning_rate": 4.696992844957095e-05,
      "loss": 2.6892,
      "step": 47600
    },
    {
      "epoch": 3.6437246963562755,
      "grad_norm": 13.642816543579102,
      "learning_rate": 4.6963562753036435e-05,
      "loss": 2.6603,
      "step": 47700
    },
    {
      "epoch": 3.651363532197693,
      "grad_norm": 13.094156265258789,
      "learning_rate": 4.6957197056501926e-05,
      "loss": 2.6911,
      "step": 47800
    },
    {
      "epoch": 3.659002368039111,
      "grad_norm": 15.206232070922852,
      "learning_rate": 4.695083135996741e-05,
      "loss": 2.7109,
      "step": 47900
    },
    {
      "epoch": 3.6666412038805287,
      "grad_norm": 11.820948600769043,
      "learning_rate": 4.694446566343289e-05,
      "loss": 2.6912,
      "step": 48000
    },
    {
      "epoch": 3.674280039721946,
      "grad_norm": 13.3233060836792,
      "learning_rate": 4.693809996689838e-05,
      "loss": 2.7303,
      "step": 48100
    },
    {
      "epoch": 3.681918875563364,
      "grad_norm": 12.224132537841797,
      "learning_rate": 4.693173427036387e-05,
      "loss": 2.7383,
      "step": 48200
    },
    {
      "epoch": 3.689557711404782,
      "grad_norm": 10.600349426269531,
      "learning_rate": 4.692536857382935e-05,
      "loss": 2.6443,
      "step": 48300
    },
    {
      "epoch": 3.6971965472462,
      "grad_norm": 13.399823188781738,
      "learning_rate": 4.691900287729484e-05,
      "loss": 2.7405,
      "step": 48400
    },
    {
      "epoch": 3.7048353830876173,
      "grad_norm": 10.48504638671875,
      "learning_rate": 4.6912637180760324e-05,
      "loss": 2.7285,
      "step": 48500
    },
    {
      "epoch": 3.712474218929035,
      "grad_norm": 12.898591995239258,
      "learning_rate": 4.690627148422581e-05,
      "loss": 2.6653,
      "step": 48600
    },
    {
      "epoch": 3.720113054770453,
      "grad_norm": 12.293551445007324,
      "learning_rate": 4.689990578769129e-05,
      "loss": 2.7067,
      "step": 48700
    },
    {
      "epoch": 3.727751890611871,
      "grad_norm": 10.556900978088379,
      "learning_rate": 4.689354009115678e-05,
      "loss": 2.6298,
      "step": 48800
    },
    {
      "epoch": 3.7353907264532884,
      "grad_norm": 10.809521675109863,
      "learning_rate": 4.6887174394622265e-05,
      "loss": 2.6575,
      "step": 48900
    },
    {
      "epoch": 3.7430295622947063,
      "grad_norm": 10.73783016204834,
      "learning_rate": 4.688080869808775e-05,
      "loss": 2.7218,
      "step": 49000
    },
    {
      "epoch": 3.750668398136124,
      "grad_norm": 10.590123176574707,
      "learning_rate": 4.687444300155323e-05,
      "loss": 2.621,
      "step": 49100
    },
    {
      "epoch": 3.7583072339775416,
      "grad_norm": 10.832308769226074,
      "learning_rate": 4.6868077305018716e-05,
      "loss": 2.6631,
      "step": 49200
    },
    {
      "epoch": 3.7659460698189595,
      "grad_norm": 12.57601547241211,
      "learning_rate": 4.6861711608484206e-05,
      "loss": 2.7002,
      "step": 49300
    },
    {
      "epoch": 3.7735849056603774,
      "grad_norm": 11.542390823364258,
      "learning_rate": 4.685534591194969e-05,
      "loss": 2.6497,
      "step": 49400
    },
    {
      "epoch": 3.781223741501795,
      "grad_norm": 16.874473571777344,
      "learning_rate": 4.684898021541517e-05,
      "loss": 2.6036,
      "step": 49500
    },
    {
      "epoch": 3.7888625773432127,
      "grad_norm": 14.123976707458496,
      "learning_rate": 4.6842614518880657e-05,
      "loss": 2.7199,
      "step": 49600
    },
    {
      "epoch": 3.7965014131846306,
      "grad_norm": 11.12617301940918,
      "learning_rate": 4.683624882234615e-05,
      "loss": 2.6617,
      "step": 49700
    },
    {
      "epoch": 3.8041402490260485,
      "grad_norm": 12.619585037231445,
      "learning_rate": 4.682988312581163e-05,
      "loss": 2.6507,
      "step": 49800
    },
    {
      "epoch": 3.8117790848674664,
      "grad_norm": 10.945950508117676,
      "learning_rate": 4.6823517429277114e-05,
      "loss": 2.6214,
      "step": 49900
    },
    {
      "epoch": 3.819417920708884,
      "grad_norm": 10.662724494934082,
      "learning_rate": 4.68171517327426e-05,
      "loss": 2.6202,
      "step": 50000
    },
    {
      "epoch": 3.8270567565503018,
      "grad_norm": 11.137476921081543,
      "learning_rate": 4.681078603620808e-05,
      "loss": 2.6145,
      "step": 50100
    },
    {
      "epoch": 3.8346955923917196,
      "grad_norm": 13.078369140625,
      "learning_rate": 4.680442033967357e-05,
      "loss": 2.606,
      "step": 50200
    },
    {
      "epoch": 3.842334428233137,
      "grad_norm": 13.380468368530273,
      "learning_rate": 4.6798054643139055e-05,
      "loss": 2.6396,
      "step": 50300
    },
    {
      "epoch": 3.849973264074555,
      "grad_norm": 9.377128601074219,
      "learning_rate": 4.679168894660454e-05,
      "loss": 2.604,
      "step": 50400
    },
    {
      "epoch": 3.857612099915973,
      "grad_norm": 13.078343391418457,
      "learning_rate": 4.678532325007002e-05,
      "loss": 2.6121,
      "step": 50500
    },
    {
      "epoch": 3.8652509357573903,
      "grad_norm": 10.0731840133667,
      "learning_rate": 4.6778957553535505e-05,
      "loss": 2.5883,
      "step": 50600
    },
    {
      "epoch": 3.872889771598808,
      "grad_norm": 12.650803565979004,
      "learning_rate": 4.6772591857000996e-05,
      "loss": 2.6457,
      "step": 50700
    },
    {
      "epoch": 3.880528607440226,
      "grad_norm": 11.83998966217041,
      "learning_rate": 4.676622616046648e-05,
      "loss": 2.7098,
      "step": 50800
    },
    {
      "epoch": 3.888167443281644,
      "grad_norm": 11.178842544555664,
      "learning_rate": 4.675986046393196e-05,
      "loss": 2.6987,
      "step": 50900
    },
    {
      "epoch": 3.895806279123062,
      "grad_norm": 11.583667755126953,
      "learning_rate": 4.6753494767397446e-05,
      "loss": 2.7107,
      "step": 51000
    },
    {
      "epoch": 3.9034451149644793,
      "grad_norm": 12.668622970581055,
      "learning_rate": 4.674712907086294e-05,
      "loss": 2.6366,
      "step": 51100
    },
    {
      "epoch": 3.911083950805897,
      "grad_norm": 10.923680305480957,
      "learning_rate": 4.674076337432842e-05,
      "loss": 2.5936,
      "step": 51200
    },
    {
      "epoch": 3.918722786647315,
      "grad_norm": 9.383166313171387,
      "learning_rate": 4.6734397677793904e-05,
      "loss": 2.7068,
      "step": 51300
    },
    {
      "epoch": 3.9263616224887326,
      "grad_norm": 11.609451293945312,
      "learning_rate": 4.672803198125939e-05,
      "loss": 2.6036,
      "step": 51400
    },
    {
      "epoch": 3.9340004583301504,
      "grad_norm": 12.36153507232666,
      "learning_rate": 4.672166628472488e-05,
      "loss": 2.7411,
      "step": 51500
    },
    {
      "epoch": 3.9416392941715683,
      "grad_norm": 11.468490600585938,
      "learning_rate": 4.671530058819036e-05,
      "loss": 2.6604,
      "step": 51600
    },
    {
      "epoch": 3.9492781300129858,
      "grad_norm": 12.037773132324219,
      "learning_rate": 4.6708934891655845e-05,
      "loss": 2.532,
      "step": 51700
    },
    {
      "epoch": 3.9569169658544037,
      "grad_norm": 10.445871353149414,
      "learning_rate": 4.6702569195121335e-05,
      "loss": 2.62,
      "step": 51800
    },
    {
      "epoch": 3.9645558016958216,
      "grad_norm": 12.123541831970215,
      "learning_rate": 4.669620349858682e-05,
      "loss": 2.6911,
      "step": 51900
    },
    {
      "epoch": 3.9721946375372394,
      "grad_norm": 12.763322830200195,
      "learning_rate": 4.66898378020523e-05,
      "loss": 2.7041,
      "step": 52000
    },
    {
      "epoch": 3.9798334733786573,
      "grad_norm": 13.037582397460938,
      "learning_rate": 4.668347210551779e-05,
      "loss": 2.6587,
      "step": 52100
    },
    {
      "epoch": 3.987472309220075,
      "grad_norm": 10.20703125,
      "learning_rate": 4.6677106408983276e-05,
      "loss": 2.6255,
      "step": 52200
    },
    {
      "epoch": 3.9951111450614927,
      "grad_norm": 12.569610595703125,
      "learning_rate": 4.667074071244876e-05,
      "loss": 2.6793,
      "step": 52300
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.527763843536377,
      "eval_runtime": 1.6378,
      "eval_samples_per_second": 421.29,
      "eval_steps_per_second": 421.29,
      "step": 52364
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.4665110111236572,
      "eval_runtime": 30.6148,
      "eval_samples_per_second": 427.604,
      "eval_steps_per_second": 427.604,
      "step": 52364
    },
    {
      "epoch": 4.002749980902911,
      "grad_norm": 16.731191635131836,
      "learning_rate": 4.666437501591424e-05,
      "loss": 2.6514,
      "step": 52400
    },
    {
      "epoch": 4.010388816744328,
      "grad_norm": 12.73814868927002,
      "learning_rate": 4.665800931937973e-05,
      "loss": 2.6762,
      "step": 52500
    },
    {
      "epoch": 4.018027652585746,
      "grad_norm": 13.877134323120117,
      "learning_rate": 4.665164362284522e-05,
      "loss": 2.6016,
      "step": 52600
    },
    {
      "epoch": 4.025666488427164,
      "grad_norm": 12.643067359924316,
      "learning_rate": 4.66452779263107e-05,
      "loss": 2.6401,
      "step": 52700
    },
    {
      "epoch": 4.033305324268581,
      "grad_norm": 11.22762393951416,
      "learning_rate": 4.6638912229776184e-05,
      "loss": 2.5706,
      "step": 52800
    },
    {
      "epoch": 4.04094416011,
      "grad_norm": 11.777252197265625,
      "learning_rate": 4.6632546533241674e-05,
      "loss": 2.6239,
      "step": 52900
    },
    {
      "epoch": 4.048582995951417,
      "grad_norm": 10.742127418518066,
      "learning_rate": 4.662618083670716e-05,
      "loss": 2.6036,
      "step": 53000
    },
    {
      "epoch": 4.0562218317928345,
      "grad_norm": 11.258295059204102,
      "learning_rate": 4.661981514017264e-05,
      "loss": 2.6001,
      "step": 53100
    },
    {
      "epoch": 4.063860667634253,
      "grad_norm": 13.083964347839355,
      "learning_rate": 4.6613449443638125e-05,
      "loss": 2.6452,
      "step": 53200
    },
    {
      "epoch": 4.07149950347567,
      "grad_norm": 11.510228157043457,
      "learning_rate": 4.660708374710361e-05,
      "loss": 2.6616,
      "step": 53300
    },
    {
      "epoch": 4.079138339317088,
      "grad_norm": 13.533269882202148,
      "learning_rate": 4.66007180505691e-05,
      "loss": 2.7046,
      "step": 53400
    },
    {
      "epoch": 4.086777175158506,
      "grad_norm": 14.627622604370117,
      "learning_rate": 4.659435235403458e-05,
      "loss": 2.6303,
      "step": 53500
    },
    {
      "epoch": 4.0944160109999235,
      "grad_norm": 15.112055778503418,
      "learning_rate": 4.6587986657500066e-05,
      "loss": 2.5763,
      "step": 53600
    },
    {
      "epoch": 4.102054846841342,
      "grad_norm": 13.067608833312988,
      "learning_rate": 4.658162096096555e-05,
      "loss": 2.6304,
      "step": 53700
    },
    {
      "epoch": 4.109693682682759,
      "grad_norm": 11.829179763793945,
      "learning_rate": 4.657525526443103e-05,
      "loss": 2.617,
      "step": 53800
    },
    {
      "epoch": 4.117332518524177,
      "grad_norm": 10.89604377746582,
      "learning_rate": 4.656888956789652e-05,
      "loss": 2.6685,
      "step": 53900
    },
    {
      "epoch": 4.124971354365595,
      "grad_norm": 12.75682258605957,
      "learning_rate": 4.656252387136201e-05,
      "loss": 2.6805,
      "step": 54000
    },
    {
      "epoch": 4.1326101902070125,
      "grad_norm": 10.698060989379883,
      "learning_rate": 4.655615817482749e-05,
      "loss": 2.6151,
      "step": 54100
    },
    {
      "epoch": 4.14024902604843,
      "grad_norm": 13.460410118103027,
      "learning_rate": 4.6549792478292974e-05,
      "loss": 2.621,
      "step": 54200
    },
    {
      "epoch": 4.147887861889848,
      "grad_norm": 10.944395065307617,
      "learning_rate": 4.6543426781758464e-05,
      "loss": 2.5754,
      "step": 54300
    },
    {
      "epoch": 4.155526697731266,
      "grad_norm": 20.555233001708984,
      "learning_rate": 4.653706108522395e-05,
      "loss": 2.5865,
      "step": 54400
    },
    {
      "epoch": 4.163165533572683,
      "grad_norm": 11.126383781433105,
      "learning_rate": 4.653069538868943e-05,
      "loss": 2.5722,
      "step": 54500
    },
    {
      "epoch": 4.1708043694141015,
      "grad_norm": 10.230183601379395,
      "learning_rate": 4.6524329692154915e-05,
      "loss": 2.6226,
      "step": 54600
    },
    {
      "epoch": 4.178443205255519,
      "grad_norm": 12.380426406860352,
      "learning_rate": 4.65179639956204e-05,
      "loss": 2.5361,
      "step": 54700
    },
    {
      "epoch": 4.186082041096936,
      "grad_norm": 13.245305061340332,
      "learning_rate": 4.651159829908589e-05,
      "loss": 2.596,
      "step": 54800
    },
    {
      "epoch": 4.193720876938355,
      "grad_norm": 9.978025436401367,
      "learning_rate": 4.650523260255137e-05,
      "loss": 2.6177,
      "step": 54900
    },
    {
      "epoch": 4.201359712779772,
      "grad_norm": 13.410404205322266,
      "learning_rate": 4.6498866906016856e-05,
      "loss": 2.5884,
      "step": 55000
    },
    {
      "epoch": 4.2089985486211905,
      "grad_norm": 12.405927658081055,
      "learning_rate": 4.649250120948234e-05,
      "loss": 2.6777,
      "step": 55100
    },
    {
      "epoch": 4.216637384462608,
      "grad_norm": 12.115375518798828,
      "learning_rate": 4.648613551294783e-05,
      "loss": 2.5874,
      "step": 55200
    },
    {
      "epoch": 4.224276220304025,
      "grad_norm": 14.061306953430176,
      "learning_rate": 4.647976981641331e-05,
      "loss": 2.6029,
      "step": 55300
    },
    {
      "epoch": 4.231915056145444,
      "grad_norm": 9.635717391967773,
      "learning_rate": 4.6473404119878797e-05,
      "loss": 2.6737,
      "step": 55400
    },
    {
      "epoch": 4.239553891986861,
      "grad_norm": 11.300573348999023,
      "learning_rate": 4.646703842334429e-05,
      "loss": 2.6634,
      "step": 55500
    },
    {
      "epoch": 4.247192727828279,
      "grad_norm": 14.741353034973145,
      "learning_rate": 4.646067272680977e-05,
      "loss": 2.6313,
      "step": 55600
    },
    {
      "epoch": 4.254831563669697,
      "grad_norm": 12.422467231750488,
      "learning_rate": 4.6454307030275254e-05,
      "loss": 2.539,
      "step": 55700
    },
    {
      "epoch": 4.262470399511114,
      "grad_norm": 9.042285919189453,
      "learning_rate": 4.6447941333740744e-05,
      "loss": 2.5519,
      "step": 55800
    },
    {
      "epoch": 4.270109235352532,
      "grad_norm": 12.968475341796875,
      "learning_rate": 4.644157563720623e-05,
      "loss": 2.6601,
      "step": 55900
    },
    {
      "epoch": 4.27774807119395,
      "grad_norm": 12.239646911621094,
      "learning_rate": 4.643520994067171e-05,
      "loss": 2.5582,
      "step": 56000
    },
    {
      "epoch": 4.285386907035368,
      "grad_norm": 15.801900863647461,
      "learning_rate": 4.64288442441372e-05,
      "loss": 2.6848,
      "step": 56100
    },
    {
      "epoch": 4.293025742876786,
      "grad_norm": 11.508085250854492,
      "learning_rate": 4.6422478547602685e-05,
      "loss": 2.7129,
      "step": 56200
    },
    {
      "epoch": 4.300664578718203,
      "grad_norm": 11.36323070526123,
      "learning_rate": 4.641611285106817e-05,
      "loss": 2.638,
      "step": 56300
    },
    {
      "epoch": 4.308303414559621,
      "grad_norm": 14.915742874145508,
      "learning_rate": 4.640974715453365e-05,
      "loss": 2.579,
      "step": 56400
    },
    {
      "epoch": 4.315942250401039,
      "grad_norm": 12.600286483764648,
      "learning_rate": 4.6403381457999136e-05,
      "loss": 2.6425,
      "step": 56500
    },
    {
      "epoch": 4.323581086242457,
      "grad_norm": 13.959782600402832,
      "learning_rate": 4.6397015761464626e-05,
      "loss": 2.5308,
      "step": 56600
    },
    {
      "epoch": 4.331219922083874,
      "grad_norm": 11.114635467529297,
      "learning_rate": 4.639065006493011e-05,
      "loss": 2.5985,
      "step": 56700
    },
    {
      "epoch": 4.338858757925292,
      "grad_norm": 9.552751541137695,
      "learning_rate": 4.638428436839559e-05,
      "loss": 2.5617,
      "step": 56800
    },
    {
      "epoch": 4.34649759376671,
      "grad_norm": 12.303633689880371,
      "learning_rate": 4.637791867186108e-05,
      "loss": 2.6098,
      "step": 56900
    },
    {
      "epoch": 4.354136429608127,
      "grad_norm": 11.56651496887207,
      "learning_rate": 4.637155297532656e-05,
      "loss": 2.6785,
      "step": 57000
    },
    {
      "epoch": 4.361775265449546,
      "grad_norm": 12.51960277557373,
      "learning_rate": 4.636518727879205e-05,
      "loss": 2.639,
      "step": 57100
    },
    {
      "epoch": 4.369414101290963,
      "grad_norm": 18.25579071044922,
      "learning_rate": 4.6358821582257534e-05,
      "loss": 2.6405,
      "step": 57200
    },
    {
      "epoch": 4.377052937132381,
      "grad_norm": 12.07351016998291,
      "learning_rate": 4.635245588572302e-05,
      "loss": 2.6017,
      "step": 57300
    },
    {
      "epoch": 4.384691772973799,
      "grad_norm": 11.645133018493652,
      "learning_rate": 4.63460901891885e-05,
      "loss": 2.6166,
      "step": 57400
    },
    {
      "epoch": 4.392330608815216,
      "grad_norm": 9.290871620178223,
      "learning_rate": 4.633972449265399e-05,
      "loss": 2.6276,
      "step": 57500
    },
    {
      "epoch": 4.399969444656635,
      "grad_norm": 13.98083209991455,
      "learning_rate": 4.6333358796119475e-05,
      "loss": 2.6021,
      "step": 57600
    },
    {
      "epoch": 4.407608280498052,
      "grad_norm": 15.507734298706055,
      "learning_rate": 4.632699309958496e-05,
      "loss": 2.5885,
      "step": 57700
    },
    {
      "epoch": 4.4152471163394695,
      "grad_norm": 11.6268892288208,
      "learning_rate": 4.632062740305044e-05,
      "loss": 2.5777,
      "step": 57800
    },
    {
      "epoch": 4.422885952180888,
      "grad_norm": 13.008574485778809,
      "learning_rate": 4.6314261706515926e-05,
      "loss": 2.7094,
      "step": 57900
    },
    {
      "epoch": 4.430524788022305,
      "grad_norm": 13.503666877746582,
      "learning_rate": 4.6307896009981416e-05,
      "loss": 2.5974,
      "step": 58000
    },
    {
      "epoch": 4.438163623863723,
      "grad_norm": 15.466397285461426,
      "learning_rate": 4.63015303134469e-05,
      "loss": 2.6321,
      "step": 58100
    },
    {
      "epoch": 4.445802459705141,
      "grad_norm": 14.395142555236816,
      "learning_rate": 4.629516461691238e-05,
      "loss": 2.573,
      "step": 58200
    },
    {
      "epoch": 4.4534412955465585,
      "grad_norm": 10.950016975402832,
      "learning_rate": 4.6288798920377867e-05,
      "loss": 2.6194,
      "step": 58300
    },
    {
      "epoch": 4.461080131387977,
      "grad_norm": 11.043023109436035,
      "learning_rate": 4.628243322384336e-05,
      "loss": 2.5633,
      "step": 58400
    },
    {
      "epoch": 4.468718967229394,
      "grad_norm": 13.13748836517334,
      "learning_rate": 4.627606752730884e-05,
      "loss": 2.5441,
      "step": 58500
    },
    {
      "epoch": 4.476357803070812,
      "grad_norm": 14.762683868408203,
      "learning_rate": 4.6269701830774324e-05,
      "loss": 2.6019,
      "step": 58600
    },
    {
      "epoch": 4.48399663891223,
      "grad_norm": 11.706695556640625,
      "learning_rate": 4.626333613423981e-05,
      "loss": 2.4576,
      "step": 58700
    },
    {
      "epoch": 4.4916354747536476,
      "grad_norm": 15.931842803955078,
      "learning_rate": 4.625697043770529e-05,
      "loss": 2.5963,
      "step": 58800
    },
    {
      "epoch": 4.499274310595065,
      "grad_norm": 12.142707824707031,
      "learning_rate": 4.625060474117078e-05,
      "loss": 2.5831,
      "step": 58900
    },
    {
      "epoch": 4.506913146436483,
      "grad_norm": 12.743989944458008,
      "learning_rate": 4.6244239044636265e-05,
      "loss": 2.5968,
      "step": 59000
    },
    {
      "epoch": 4.514551982277901,
      "grad_norm": 9.725059509277344,
      "learning_rate": 4.623787334810175e-05,
      "loss": 2.618,
      "step": 59100
    },
    {
      "epoch": 4.522190818119318,
      "grad_norm": 14.605328559875488,
      "learning_rate": 4.623150765156723e-05,
      "loss": 2.5862,
      "step": 59200
    },
    {
      "epoch": 4.529829653960737,
      "grad_norm": 9.880292892456055,
      "learning_rate": 4.622514195503272e-05,
      "loss": 2.5627,
      "step": 59300
    },
    {
      "epoch": 4.537468489802154,
      "grad_norm": 11.617218017578125,
      "learning_rate": 4.6218776258498206e-05,
      "loss": 2.6132,
      "step": 59400
    },
    {
      "epoch": 4.545107325643572,
      "grad_norm": 11.177408218383789,
      "learning_rate": 4.6212410561963696e-05,
      "loss": 2.5522,
      "step": 59500
    },
    {
      "epoch": 4.55274616148499,
      "grad_norm": 13.823161125183105,
      "learning_rate": 4.620604486542918e-05,
      "loss": 2.5831,
      "step": 59600
    },
    {
      "epoch": 4.560384997326407,
      "grad_norm": 13.60900592803955,
      "learning_rate": 4.619967916889466e-05,
      "loss": 2.5918,
      "step": 59700
    },
    {
      "epoch": 4.568023833167826,
      "grad_norm": 14.126338958740234,
      "learning_rate": 4.6193313472360153e-05,
      "loss": 2.6592,
      "step": 59800
    },
    {
      "epoch": 4.575662669009243,
      "grad_norm": 11.518656730651855,
      "learning_rate": 4.618694777582564e-05,
      "loss": 2.5175,
      "step": 59900
    },
    {
      "epoch": 4.5833015048506605,
      "grad_norm": 12.728620529174805,
      "learning_rate": 4.618058207929112e-05,
      "loss": 2.6231,
      "step": 60000
    },
    {
      "epoch": 4.590940340692079,
      "grad_norm": 11.131204605102539,
      "learning_rate": 4.6174216382756604e-05,
      "loss": 2.6638,
      "step": 60100
    },
    {
      "epoch": 4.598579176533496,
      "grad_norm": 9.709681510925293,
      "learning_rate": 4.616785068622209e-05,
      "loss": 2.4899,
      "step": 60200
    },
    {
      "epoch": 4.606218012374914,
      "grad_norm": 11.469944953918457,
      "learning_rate": 4.616148498968758e-05,
      "loss": 2.532,
      "step": 60300
    },
    {
      "epoch": 4.613856848216332,
      "grad_norm": 15.078635215759277,
      "learning_rate": 4.615511929315306e-05,
      "loss": 2.53,
      "step": 60400
    },
    {
      "epoch": 4.6214956840577495,
      "grad_norm": 12.808786392211914,
      "learning_rate": 4.6148753596618545e-05,
      "loss": 2.6309,
      "step": 60500
    },
    {
      "epoch": 4.629134519899168,
      "grad_norm": 9.602680206298828,
      "learning_rate": 4.614238790008403e-05,
      "loss": 2.5119,
      "step": 60600
    },
    {
      "epoch": 4.636773355740585,
      "grad_norm": 11.159003257751465,
      "learning_rate": 4.613602220354952e-05,
      "loss": 2.4777,
      "step": 60700
    },
    {
      "epoch": 4.644412191582003,
      "grad_norm": 13.528860092163086,
      "learning_rate": 4.6129656507015e-05,
      "loss": 2.5012,
      "step": 60800
    },
    {
      "epoch": 4.652051027423421,
      "grad_norm": 11.524214744567871,
      "learning_rate": 4.6123290810480486e-05,
      "loss": 2.5862,
      "step": 60900
    },
    {
      "epoch": 4.6596898632648385,
      "grad_norm": 11.003071784973145,
      "learning_rate": 4.611692511394597e-05,
      "loss": 2.6136,
      "step": 61000
    },
    {
      "epoch": 4.667328699106256,
      "grad_norm": 14.155433654785156,
      "learning_rate": 4.611055941741145e-05,
      "loss": 2.646,
      "step": 61100
    },
    {
      "epoch": 4.674967534947674,
      "grad_norm": 10.430368423461914,
      "learning_rate": 4.610419372087694e-05,
      "loss": 2.525,
      "step": 61200
    },
    {
      "epoch": 4.682606370789092,
      "grad_norm": 11.24439811706543,
      "learning_rate": 4.609782802434243e-05,
      "loss": 2.6111,
      "step": 61300
    },
    {
      "epoch": 4.690245206630509,
      "grad_norm": 11.422892570495605,
      "learning_rate": 4.609146232780791e-05,
      "loss": 2.5801,
      "step": 61400
    },
    {
      "epoch": 4.6978840424719275,
      "grad_norm": 7.839532375335693,
      "learning_rate": 4.6085096631273394e-05,
      "loss": 2.5052,
      "step": 61500
    },
    {
      "epoch": 4.705522878313345,
      "grad_norm": 19.565160751342773,
      "learning_rate": 4.6078730934738884e-05,
      "loss": 2.588,
      "step": 61600
    },
    {
      "epoch": 4.713161714154763,
      "grad_norm": 11.032620429992676,
      "learning_rate": 4.607236523820437e-05,
      "loss": 2.5555,
      "step": 61700
    },
    {
      "epoch": 4.720800549996181,
      "grad_norm": 11.664402961730957,
      "learning_rate": 4.606599954166985e-05,
      "loss": 2.536,
      "step": 61800
    },
    {
      "epoch": 4.728439385837598,
      "grad_norm": 11.756492614746094,
      "learning_rate": 4.6059633845135335e-05,
      "loss": 2.5862,
      "step": 61900
    },
    {
      "epoch": 4.7360782216790165,
      "grad_norm": 16.083263397216797,
      "learning_rate": 4.605326814860082e-05,
      "loss": 2.5914,
      "step": 62000
    },
    {
      "epoch": 4.743717057520434,
      "grad_norm": 14.16518783569336,
      "learning_rate": 4.604690245206631e-05,
      "loss": 2.5839,
      "step": 62100
    },
    {
      "epoch": 4.751355893361851,
      "grad_norm": 11.244046211242676,
      "learning_rate": 4.604053675553179e-05,
      "loss": 2.5573,
      "step": 62200
    },
    {
      "epoch": 4.75899472920327,
      "grad_norm": 12.599931716918945,
      "learning_rate": 4.6034171058997276e-05,
      "loss": 2.643,
      "step": 62300
    },
    {
      "epoch": 4.766633565044687,
      "grad_norm": 10.876522064208984,
      "learning_rate": 4.602780536246276e-05,
      "loss": 2.5176,
      "step": 62400
    },
    {
      "epoch": 4.774272400886105,
      "grad_norm": 11.492868423461914,
      "learning_rate": 4.602143966592824e-05,
      "loss": 2.5603,
      "step": 62500
    },
    {
      "epoch": 4.781911236727523,
      "grad_norm": 12.68039608001709,
      "learning_rate": 4.601507396939373e-05,
      "loss": 2.5582,
      "step": 62600
    },
    {
      "epoch": 4.78955007256894,
      "grad_norm": 11.967048645019531,
      "learning_rate": 4.600870827285922e-05,
      "loss": 2.4447,
      "step": 62700
    },
    {
      "epoch": 4.797188908410359,
      "grad_norm": 13.92325496673584,
      "learning_rate": 4.60023425763247e-05,
      "loss": 2.5431,
      "step": 62800
    },
    {
      "epoch": 4.804827744251776,
      "grad_norm": 11.720488548278809,
      "learning_rate": 4.5995976879790184e-05,
      "loss": 2.4625,
      "step": 62900
    },
    {
      "epoch": 4.812466580093194,
      "grad_norm": 11.101696014404297,
      "learning_rate": 4.5989611183255674e-05,
      "loss": 2.657,
      "step": 63000
    },
    {
      "epoch": 4.820105415934612,
      "grad_norm": 11.66348648071289,
      "learning_rate": 4.598324548672116e-05,
      "loss": 2.6853,
      "step": 63100
    },
    {
      "epoch": 4.827744251776029,
      "grad_norm": 11.402139663696289,
      "learning_rate": 4.597687979018664e-05,
      "loss": 2.5664,
      "step": 63200
    },
    {
      "epoch": 4.835383087617447,
      "grad_norm": 14.513174057006836,
      "learning_rate": 4.597051409365213e-05,
      "loss": 2.5682,
      "step": 63300
    },
    {
      "epoch": 4.843021923458865,
      "grad_norm": 14.244770050048828,
      "learning_rate": 4.5964148397117615e-05,
      "loss": 2.67,
      "step": 63400
    },
    {
      "epoch": 4.850660759300283,
      "grad_norm": 19.067211151123047,
      "learning_rate": 4.59577827005831e-05,
      "loss": 2.6148,
      "step": 63500
    },
    {
      "epoch": 4.8582995951417,
      "grad_norm": 13.271154403686523,
      "learning_rate": 4.595141700404859e-05,
      "loss": 2.5859,
      "step": 63600
    },
    {
      "epoch": 4.865938430983118,
      "grad_norm": 13.985251426696777,
      "learning_rate": 4.594505130751407e-05,
      "loss": 2.6258,
      "step": 63700
    },
    {
      "epoch": 4.873577266824536,
      "grad_norm": 11.443432807922363,
      "learning_rate": 4.5938685610979556e-05,
      "loss": 2.6473,
      "step": 63800
    },
    {
      "epoch": 4.881216102665954,
      "grad_norm": 11.31705379486084,
      "learning_rate": 4.5932319914445046e-05,
      "loss": 2.5497,
      "step": 63900
    },
    {
      "epoch": 4.888854938507372,
      "grad_norm": 13.477174758911133,
      "learning_rate": 4.592595421791053e-05,
      "loss": 2.615,
      "step": 64000
    },
    {
      "epoch": 4.896493774348789,
      "grad_norm": 12.450913429260254,
      "learning_rate": 4.591958852137601e-05,
      "loss": 2.606,
      "step": 64100
    },
    {
      "epoch": 4.9041326101902065,
      "grad_norm": 11.905034065246582,
      "learning_rate": 4.59132228248415e-05,
      "loss": 2.5428,
      "step": 64200
    },
    {
      "epoch": 4.911771446031625,
      "grad_norm": 13.590049743652344,
      "learning_rate": 4.590685712830698e-05,
      "loss": 2.6615,
      "step": 64300
    },
    {
      "epoch": 4.919410281873042,
      "grad_norm": 12.665510177612305,
      "learning_rate": 4.590049143177247e-05,
      "loss": 2.5934,
      "step": 64400
    },
    {
      "epoch": 4.927049117714461,
      "grad_norm": 11.746158599853516,
      "learning_rate": 4.5894125735237954e-05,
      "loss": 2.6137,
      "step": 64500
    },
    {
      "epoch": 4.934687953555878,
      "grad_norm": 13.022977828979492,
      "learning_rate": 4.588776003870344e-05,
      "loss": 2.5661,
      "step": 64600
    },
    {
      "epoch": 4.9423267893972955,
      "grad_norm": 13.890299797058105,
      "learning_rate": 4.588139434216892e-05,
      "loss": 2.5702,
      "step": 64700
    },
    {
      "epoch": 4.949965625238714,
      "grad_norm": 13.771960258483887,
      "learning_rate": 4.587502864563441e-05,
      "loss": 2.5521,
      "step": 64800
    },
    {
      "epoch": 4.957604461080131,
      "grad_norm": 12.491588592529297,
      "learning_rate": 4.5868662949099895e-05,
      "loss": 2.6635,
      "step": 64900
    },
    {
      "epoch": 4.96524329692155,
      "grad_norm": 11.714632034301758,
      "learning_rate": 4.586229725256538e-05,
      "loss": 2.5044,
      "step": 65000
    },
    {
      "epoch": 4.972882132762967,
      "grad_norm": 9.905863761901855,
      "learning_rate": 4.585593155603086e-05,
      "loss": 2.5829,
      "step": 65100
    },
    {
      "epoch": 4.9805209686043845,
      "grad_norm": 10.98426342010498,
      "learning_rate": 4.5849565859496346e-05,
      "loss": 2.568,
      "step": 65200
    },
    {
      "epoch": 4.988159804445802,
      "grad_norm": 10.98868179321289,
      "learning_rate": 4.5843200162961836e-05,
      "loss": 2.5632,
      "step": 65300
    },
    {
      "epoch": 4.99579864028722,
      "grad_norm": 18.84507179260254,
      "learning_rate": 4.583683446642732e-05,
      "loss": 2.6088,
      "step": 65400
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.4356746673583984,
      "eval_runtime": 1.6479,
      "eval_samples_per_second": 418.707,
      "eval_steps_per_second": 418.707,
      "step": 65455
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.3660035133361816,
      "eval_runtime": 30.443,
      "eval_samples_per_second": 430.017,
      "eval_steps_per_second": 430.017,
      "step": 65455
    },
    {
      "epoch": 5.003437476128638,
      "grad_norm": 13.197539329528809,
      "learning_rate": 4.58304687698928e-05,
      "loss": 2.6225,
      "step": 65500
    },
    {
      "epoch": 5.011076311970056,
      "grad_norm": 12.71426773071289,
      "learning_rate": 4.582410307335829e-05,
      "loss": 2.5138,
      "step": 65600
    },
    {
      "epoch": 5.0187151478114735,
      "grad_norm": 14.606546401977539,
      "learning_rate": 4.581773737682377e-05,
      "loss": 2.5856,
      "step": 65700
    },
    {
      "epoch": 5.026353983652891,
      "grad_norm": 12.630348205566406,
      "learning_rate": 4.581137168028926e-05,
      "loss": 2.5127,
      "step": 65800
    },
    {
      "epoch": 5.033992819494309,
      "grad_norm": 15.674154281616211,
      "learning_rate": 4.5805005983754744e-05,
      "loss": 2.5401,
      "step": 65900
    },
    {
      "epoch": 5.041631655335727,
      "grad_norm": 13.785345077514648,
      "learning_rate": 4.579864028722023e-05,
      "loss": 2.643,
      "step": 66000
    },
    {
      "epoch": 5.049270491177144,
      "grad_norm": 11.46075439453125,
      "learning_rate": 4.579227459068571e-05,
      "loss": 2.5114,
      "step": 66100
    },
    {
      "epoch": 5.0569093270185625,
      "grad_norm": 13.189969062805176,
      "learning_rate": 4.57859088941512e-05,
      "loss": 2.5397,
      "step": 66200
    },
    {
      "epoch": 5.06454816285998,
      "grad_norm": 13.62004280090332,
      "learning_rate": 4.5779543197616685e-05,
      "loss": 2.4999,
      "step": 66300
    },
    {
      "epoch": 5.072186998701398,
      "grad_norm": 11.051408767700195,
      "learning_rate": 4.577317750108217e-05,
      "loss": 2.5217,
      "step": 66400
    },
    {
      "epoch": 5.079825834542816,
      "grad_norm": 12.291513442993164,
      "learning_rate": 4.576681180454765e-05,
      "loss": 2.6305,
      "step": 66500
    },
    {
      "epoch": 5.087464670384233,
      "grad_norm": 14.475415229797363,
      "learning_rate": 4.5760446108013136e-05,
      "loss": 2.5653,
      "step": 66600
    },
    {
      "epoch": 5.095103506225652,
      "grad_norm": 13.988245964050293,
      "learning_rate": 4.5754080411478626e-05,
      "loss": 2.4608,
      "step": 66700
    },
    {
      "epoch": 5.102742342067069,
      "grad_norm": 13.618169784545898,
      "learning_rate": 4.574771471494411e-05,
      "loss": 2.6286,
      "step": 66800
    },
    {
      "epoch": 5.1103811779084864,
      "grad_norm": 11.45608901977539,
      "learning_rate": 4.574134901840959e-05,
      "loss": 2.5439,
      "step": 66900
    },
    {
      "epoch": 5.118020013749905,
      "grad_norm": 11.72204875946045,
      "learning_rate": 4.573498332187508e-05,
      "loss": 2.4633,
      "step": 67000
    },
    {
      "epoch": 5.125658849591322,
      "grad_norm": 19.657154083251953,
      "learning_rate": 4.572861762534057e-05,
      "loss": 2.5005,
      "step": 67100
    },
    {
      "epoch": 5.13329768543274,
      "grad_norm": 12.152372360229492,
      "learning_rate": 4.572225192880605e-05,
      "loss": 2.5842,
      "step": 67200
    },
    {
      "epoch": 5.140936521274158,
      "grad_norm": 12.692656517028809,
      "learning_rate": 4.571588623227154e-05,
      "loss": 2.6002,
      "step": 67300
    },
    {
      "epoch": 5.1485753571155755,
      "grad_norm": 10.16421890258789,
      "learning_rate": 4.5709520535737024e-05,
      "loss": 2.4961,
      "step": 67400
    },
    {
      "epoch": 5.156214192956993,
      "grad_norm": 14.288793563842773,
      "learning_rate": 4.570315483920251e-05,
      "loss": 2.6287,
      "step": 67500
    },
    {
      "epoch": 5.163853028798411,
      "grad_norm": 11.62149715423584,
      "learning_rate": 4.5696789142668e-05,
      "loss": 2.5086,
      "step": 67600
    },
    {
      "epoch": 5.171491864639829,
      "grad_norm": 13.072303771972656,
      "learning_rate": 4.569042344613348e-05,
      "loss": 2.6106,
      "step": 67700
    },
    {
      "epoch": 5.179130700481247,
      "grad_norm": 12.591666221618652,
      "learning_rate": 4.5684057749598965e-05,
      "loss": 2.6108,
      "step": 67800
    },
    {
      "epoch": 5.1867695363226645,
      "grad_norm": 12.86484432220459,
      "learning_rate": 4.567769205306445e-05,
      "loss": 2.56,
      "step": 67900
    },
    {
      "epoch": 5.194408372164082,
      "grad_norm": 10.574667930603027,
      "learning_rate": 4.567132635652993e-05,
      "loss": 2.4576,
      "step": 68000
    },
    {
      "epoch": 5.2020472080055,
      "grad_norm": 12.40671157836914,
      "learning_rate": 4.566496065999542e-05,
      "loss": 2.542,
      "step": 68100
    },
    {
      "epoch": 5.209686043846918,
      "grad_norm": 14.920111656188965,
      "learning_rate": 4.5658594963460906e-05,
      "loss": 2.5533,
      "step": 68200
    },
    {
      "epoch": 5.217324879688335,
      "grad_norm": 11.458268165588379,
      "learning_rate": 4.565222926692639e-05,
      "loss": 2.5682,
      "step": 68300
    },
    {
      "epoch": 5.2249637155297535,
      "grad_norm": 12.91925048828125,
      "learning_rate": 4.564586357039187e-05,
      "loss": 2.5796,
      "step": 68400
    },
    {
      "epoch": 5.232602551371171,
      "grad_norm": 13.042524337768555,
      "learning_rate": 4.5639497873857363e-05,
      "loss": 2.5394,
      "step": 68500
    },
    {
      "epoch": 5.240241387212588,
      "grad_norm": 16.77245330810547,
      "learning_rate": 4.563313217732285e-05,
      "loss": 2.5173,
      "step": 68600
    },
    {
      "epoch": 5.247880223054007,
      "grad_norm": 13.188921928405762,
      "learning_rate": 4.562676648078833e-05,
      "loss": 2.5056,
      "step": 68700
    },
    {
      "epoch": 5.255519058895424,
      "grad_norm": 12.07748031616211,
      "learning_rate": 4.5620400784253814e-05,
      "loss": 2.5403,
      "step": 68800
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 11.808687210083008,
      "learning_rate": 4.56140350877193e-05,
      "loss": 2.4735,
      "step": 68900
    },
    {
      "epoch": 5.27079673057826,
      "grad_norm": 14.153875350952148,
      "learning_rate": 4.560766939118479e-05,
      "loss": 2.5641,
      "step": 69000
    },
    {
      "epoch": 5.278435566419677,
      "grad_norm": 12.428528785705566,
      "learning_rate": 4.560130369465027e-05,
      "loss": 2.4967,
      "step": 69100
    },
    {
      "epoch": 5.286074402261096,
      "grad_norm": 12.41694450378418,
      "learning_rate": 4.5594937998115755e-05,
      "loss": 2.5594,
      "step": 69200
    },
    {
      "epoch": 5.293713238102513,
      "grad_norm": 10.821441650390625,
      "learning_rate": 4.558857230158124e-05,
      "loss": 2.441,
      "step": 69300
    },
    {
      "epoch": 5.301352073943931,
      "grad_norm": 12.591222763061523,
      "learning_rate": 4.558220660504673e-05,
      "loss": 2.521,
      "step": 69400
    },
    {
      "epoch": 5.308990909785349,
      "grad_norm": 9.638895034790039,
      "learning_rate": 4.557584090851221e-05,
      "loss": 2.5368,
      "step": 69500
    },
    {
      "epoch": 5.316629745626766,
      "grad_norm": 11.73212718963623,
      "learning_rate": 4.5569475211977696e-05,
      "loss": 2.4872,
      "step": 69600
    },
    {
      "epoch": 5.324268581468184,
      "grad_norm": 17.818147659301758,
      "learning_rate": 4.556310951544318e-05,
      "loss": 2.5646,
      "step": 69700
    },
    {
      "epoch": 5.331907417309602,
      "grad_norm": 10.776674270629883,
      "learning_rate": 4.555674381890866e-05,
      "loss": 2.5039,
      "step": 69800
    },
    {
      "epoch": 5.33954625315102,
      "grad_norm": 11.422981262207031,
      "learning_rate": 4.555037812237415e-05,
      "loss": 2.5345,
      "step": 69900
    },
    {
      "epoch": 5.347185088992438,
      "grad_norm": 11.319849014282227,
      "learning_rate": 4.554401242583964e-05,
      "loss": 2.5989,
      "step": 70000
    },
    {
      "epoch": 5.354823924833855,
      "grad_norm": 15.442116737365723,
      "learning_rate": 4.553764672930512e-05,
      "loss": 2.5788,
      "step": 70100
    },
    {
      "epoch": 5.362462760675273,
      "grad_norm": 12.267645835876465,
      "learning_rate": 4.5531281032770604e-05,
      "loss": 2.5091,
      "step": 70200
    },
    {
      "epoch": 5.370101596516691,
      "grad_norm": 11.8995361328125,
      "learning_rate": 4.5524915336236094e-05,
      "loss": 2.5192,
      "step": 70300
    },
    {
      "epoch": 5.377740432358109,
      "grad_norm": 10.150673866271973,
      "learning_rate": 4.551854963970158e-05,
      "loss": 2.5409,
      "step": 70400
    },
    {
      "epoch": 5.385379268199526,
      "grad_norm": 11.084003448486328,
      "learning_rate": 4.551218394316706e-05,
      "loss": 2.5306,
      "step": 70500
    },
    {
      "epoch": 5.393018104040944,
      "grad_norm": 10.767693519592285,
      "learning_rate": 4.5505818246632545e-05,
      "loss": 2.4968,
      "step": 70600
    },
    {
      "epoch": 5.400656939882362,
      "grad_norm": 12.288352012634277,
      "learning_rate": 4.5499452550098035e-05,
      "loss": 2.4854,
      "step": 70700
    },
    {
      "epoch": 5.408295775723779,
      "grad_norm": 12.677568435668945,
      "learning_rate": 4.549308685356352e-05,
      "loss": 2.5145,
      "step": 70800
    },
    {
      "epoch": 5.415934611565198,
      "grad_norm": 11.005996704101562,
      "learning_rate": 4.5486721157029e-05,
      "loss": 2.5257,
      "step": 70900
    },
    {
      "epoch": 5.423573447406615,
      "grad_norm": 14.743369102478027,
      "learning_rate": 4.548035546049449e-05,
      "loss": 2.5081,
      "step": 71000
    },
    {
      "epoch": 5.431212283248033,
      "grad_norm": 12.350117683410645,
      "learning_rate": 4.5473989763959976e-05,
      "loss": 2.4928,
      "step": 71100
    },
    {
      "epoch": 5.438851119089451,
      "grad_norm": 13.747608184814453,
      "learning_rate": 4.546762406742546e-05,
      "loss": 2.5155,
      "step": 71200
    },
    {
      "epoch": 5.446489954930868,
      "grad_norm": 10.426039695739746,
      "learning_rate": 4.546125837089095e-05,
      "loss": 2.5443,
      "step": 71300
    },
    {
      "epoch": 5.454128790772287,
      "grad_norm": 14.156600952148438,
      "learning_rate": 4.5454892674356433e-05,
      "loss": 2.6061,
      "step": 71400
    },
    {
      "epoch": 5.461767626613704,
      "grad_norm": 13.927447319030762,
      "learning_rate": 4.544852697782192e-05,
      "loss": 2.4681,
      "step": 71500
    },
    {
      "epoch": 5.4694064624551215,
      "grad_norm": 13.477836608886719,
      "learning_rate": 4.54421612812874e-05,
      "loss": 2.5108,
      "step": 71600
    },
    {
      "epoch": 5.47704529829654,
      "grad_norm": 14.043703079223633,
      "learning_rate": 4.543579558475289e-05,
      "loss": 2.4955,
      "step": 71700
    },
    {
      "epoch": 5.484684134137957,
      "grad_norm": 12.562923431396484,
      "learning_rate": 4.5429429888218374e-05,
      "loss": 2.535,
      "step": 71800
    },
    {
      "epoch": 5.492322969979375,
      "grad_norm": 12.372169494628906,
      "learning_rate": 4.542306419168386e-05,
      "loss": 2.5138,
      "step": 71900
    },
    {
      "epoch": 5.499961805820793,
      "grad_norm": 13.133774757385254,
      "learning_rate": 4.541669849514934e-05,
      "loss": 2.473,
      "step": 72000
    },
    {
      "epoch": 5.5076006416622105,
      "grad_norm": 10.13727855682373,
      "learning_rate": 4.5410332798614825e-05,
      "loss": 2.4976,
      "step": 72100
    },
    {
      "epoch": 5.515239477503629,
      "grad_norm": 16.066150665283203,
      "learning_rate": 4.5403967102080315e-05,
      "loss": 2.5598,
      "step": 72200
    },
    {
      "epoch": 5.522878313345046,
      "grad_norm": 10.873270988464355,
      "learning_rate": 4.53976014055458e-05,
      "loss": 2.4408,
      "step": 72300
    },
    {
      "epoch": 5.530517149186464,
      "grad_norm": 10.604732513427734,
      "learning_rate": 4.539123570901128e-05,
      "loss": 2.4844,
      "step": 72400
    },
    {
      "epoch": 5.538155985027882,
      "grad_norm": 10.50646686553955,
      "learning_rate": 4.5384870012476766e-05,
      "loss": 2.5168,
      "step": 72500
    },
    {
      "epoch": 5.5457948208692995,
      "grad_norm": 11.748832702636719,
      "learning_rate": 4.5378504315942256e-05,
      "loss": 2.5429,
      "step": 72600
    },
    {
      "epoch": 5.553433656710717,
      "grad_norm": 11.885974884033203,
      "learning_rate": 4.537213861940774e-05,
      "loss": 2.5227,
      "step": 72700
    },
    {
      "epoch": 5.561072492552135,
      "grad_norm": 12.076505661010742,
      "learning_rate": 4.536577292287322e-05,
      "loss": 2.3948,
      "step": 72800
    },
    {
      "epoch": 5.568711328393553,
      "grad_norm": 12.270059585571289,
      "learning_rate": 4.535940722633871e-05,
      "loss": 2.4609,
      "step": 72900
    },
    {
      "epoch": 5.57635016423497,
      "grad_norm": 11.030378341674805,
      "learning_rate": 4.535304152980419e-05,
      "loss": 2.5641,
      "step": 73000
    },
    {
      "epoch": 5.5839890000763885,
      "grad_norm": 10.769189834594727,
      "learning_rate": 4.534667583326968e-05,
      "loss": 2.5058,
      "step": 73100
    },
    {
      "epoch": 5.591627835917806,
      "grad_norm": 10.957050323486328,
      "learning_rate": 4.5340310136735164e-05,
      "loss": 2.5153,
      "step": 73200
    },
    {
      "epoch": 5.599266671759224,
      "grad_norm": 12.665390968322754,
      "learning_rate": 4.533394444020065e-05,
      "loss": 2.5023,
      "step": 73300
    },
    {
      "epoch": 5.606905507600642,
      "grad_norm": 11.3174467086792,
      "learning_rate": 4.532757874366613e-05,
      "loss": 2.5202,
      "step": 73400
    },
    {
      "epoch": 5.614544343442059,
      "grad_norm": 11.436213493347168,
      "learning_rate": 4.532121304713162e-05,
      "loss": 2.4861,
      "step": 73500
    },
    {
      "epoch": 5.6221831792834775,
      "grad_norm": 12.684370040893555,
      "learning_rate": 4.5314847350597105e-05,
      "loss": 2.4596,
      "step": 73600
    },
    {
      "epoch": 5.629822015124895,
      "grad_norm": 11.133293151855469,
      "learning_rate": 4.530848165406259e-05,
      "loss": 2.6126,
      "step": 73700
    },
    {
      "epoch": 5.637460850966312,
      "grad_norm": 12.149728775024414,
      "learning_rate": 4.530211595752807e-05,
      "loss": 2.5185,
      "step": 73800
    },
    {
      "epoch": 5.645099686807731,
      "grad_norm": 13.301546096801758,
      "learning_rate": 4.5295750260993556e-05,
      "loss": 2.4843,
      "step": 73900
    },
    {
      "epoch": 5.652738522649148,
      "grad_norm": 13.379875183105469,
      "learning_rate": 4.5289384564459046e-05,
      "loss": 2.5267,
      "step": 74000
    },
    {
      "epoch": 5.660377358490566,
      "grad_norm": 12.555618286132812,
      "learning_rate": 4.528301886792453e-05,
      "loss": 2.4838,
      "step": 74100
    },
    {
      "epoch": 5.668016194331984,
      "grad_norm": 10.703609466552734,
      "learning_rate": 4.527665317139001e-05,
      "loss": 2.5782,
      "step": 74200
    },
    {
      "epoch": 5.6756550301734014,
      "grad_norm": 13.535999298095703,
      "learning_rate": 4.52702874748555e-05,
      "loss": 2.5369,
      "step": 74300
    },
    {
      "epoch": 5.68329386601482,
      "grad_norm": 12.372775077819824,
      "learning_rate": 4.526392177832098e-05,
      "loss": 2.5299,
      "step": 74400
    },
    {
      "epoch": 5.690932701856237,
      "grad_norm": 12.402660369873047,
      "learning_rate": 4.525755608178647e-05,
      "loss": 2.5338,
      "step": 74500
    },
    {
      "epoch": 5.698571537697655,
      "grad_norm": 10.729736328125,
      "learning_rate": 4.5251190385251954e-05,
      "loss": 2.5726,
      "step": 74600
    },
    {
      "epoch": 5.706210373539073,
      "grad_norm": 11.162257194519043,
      "learning_rate": 4.524482468871744e-05,
      "loss": 2.564,
      "step": 74700
    },
    {
      "epoch": 5.7138492093804905,
      "grad_norm": 13.496204376220703,
      "learning_rate": 4.523845899218293e-05,
      "loss": 2.4452,
      "step": 74800
    },
    {
      "epoch": 5.721488045221908,
      "grad_norm": 11.805306434631348,
      "learning_rate": 4.523209329564841e-05,
      "loss": 2.5489,
      "step": 74900
    },
    {
      "epoch": 5.729126881063326,
      "grad_norm": 10.641571044921875,
      "learning_rate": 4.52257275991139e-05,
      "loss": 2.4528,
      "step": 75000
    },
    {
      "epoch": 5.736765716904744,
      "grad_norm": 13.076637268066406,
      "learning_rate": 4.5219361902579385e-05,
      "loss": 2.4936,
      "step": 75100
    },
    {
      "epoch": 5.744404552746161,
      "grad_norm": 15.080120086669922,
      "learning_rate": 4.521299620604487e-05,
      "loss": 2.4943,
      "step": 75200
    },
    {
      "epoch": 5.7520433885875795,
      "grad_norm": 17.234180450439453,
      "learning_rate": 4.520663050951035e-05,
      "loss": 2.4733,
      "step": 75300
    },
    {
      "epoch": 5.759682224428997,
      "grad_norm": 11.949589729309082,
      "learning_rate": 4.520026481297584e-05,
      "loss": 2.5159,
      "step": 75400
    },
    {
      "epoch": 5.767321060270415,
      "grad_norm": 11.214256286621094,
      "learning_rate": 4.5193899116441326e-05,
      "loss": 2.4903,
      "step": 75500
    },
    {
      "epoch": 5.774959896111833,
      "grad_norm": 15.24557113647461,
      "learning_rate": 4.518753341990681e-05,
      "loss": 2.5353,
      "step": 75600
    },
    {
      "epoch": 5.78259873195325,
      "grad_norm": 13.384842872619629,
      "learning_rate": 4.518116772337229e-05,
      "loss": 2.6216,
      "step": 75700
    },
    {
      "epoch": 5.7902375677946685,
      "grad_norm": 10.73222827911377,
      "learning_rate": 4.5174802026837784e-05,
      "loss": 2.4313,
      "step": 75800
    },
    {
      "epoch": 5.797876403636086,
      "grad_norm": 9.46877384185791,
      "learning_rate": 4.516843633030327e-05,
      "loss": 2.3913,
      "step": 75900
    },
    {
      "epoch": 5.805515239477503,
      "grad_norm": 12.380157470703125,
      "learning_rate": 4.516207063376875e-05,
      "loss": 2.5677,
      "step": 76000
    },
    {
      "epoch": 5.813154075318922,
      "grad_norm": 9.952995300292969,
      "learning_rate": 4.5155704937234234e-05,
      "loss": 2.5224,
      "step": 76100
    },
    {
      "epoch": 5.820792911160339,
      "grad_norm": 11.51608943939209,
      "learning_rate": 4.514933924069972e-05,
      "loss": 2.4657,
      "step": 76200
    },
    {
      "epoch": 5.828431747001757,
      "grad_norm": 13.088702201843262,
      "learning_rate": 4.514297354416521e-05,
      "loss": 2.4816,
      "step": 76300
    },
    {
      "epoch": 5.836070582843175,
      "grad_norm": 13.571502685546875,
      "learning_rate": 4.513660784763069e-05,
      "loss": 2.5051,
      "step": 76400
    },
    {
      "epoch": 5.843709418684592,
      "grad_norm": 13.219905853271484,
      "learning_rate": 4.5130242151096175e-05,
      "loss": 2.5872,
      "step": 76500
    },
    {
      "epoch": 5.851348254526011,
      "grad_norm": 12.378706932067871,
      "learning_rate": 4.512387645456166e-05,
      "loss": 2.5211,
      "step": 76600
    },
    {
      "epoch": 5.858987090367428,
      "grad_norm": 10.311132431030273,
      "learning_rate": 4.511751075802714e-05,
      "loss": 2.4974,
      "step": 76700
    },
    {
      "epoch": 5.866625926208846,
      "grad_norm": 12.382896423339844,
      "learning_rate": 4.511114506149263e-05,
      "loss": 2.4648,
      "step": 76800
    },
    {
      "epoch": 5.874264762050263,
      "grad_norm": 10.975468635559082,
      "learning_rate": 4.5104779364958116e-05,
      "loss": 2.5556,
      "step": 76900
    },
    {
      "epoch": 5.881903597891681,
      "grad_norm": 11.416581153869629,
      "learning_rate": 4.50984136684236e-05,
      "loss": 2.471,
      "step": 77000
    },
    {
      "epoch": 5.889542433733099,
      "grad_norm": 14.178534507751465,
      "learning_rate": 4.509204797188908e-05,
      "loss": 2.4679,
      "step": 77100
    },
    {
      "epoch": 5.897181269574517,
      "grad_norm": 13.325291633605957,
      "learning_rate": 4.5085682275354573e-05,
      "loss": 2.503,
      "step": 77200
    },
    {
      "epoch": 5.904820105415935,
      "grad_norm": 10.913036346435547,
      "learning_rate": 4.507931657882006e-05,
      "loss": 2.4527,
      "step": 77300
    },
    {
      "epoch": 5.912458941257352,
      "grad_norm": 11.839012145996094,
      "learning_rate": 4.507295088228554e-05,
      "loss": 2.4692,
      "step": 77400
    },
    {
      "epoch": 5.92009777709877,
      "grad_norm": 11.324040412902832,
      "learning_rate": 4.5066585185751024e-05,
      "loss": 2.5204,
      "step": 77500
    },
    {
      "epoch": 5.927736612940188,
      "grad_norm": 11.184429168701172,
      "learning_rate": 4.506021948921651e-05,
      "loss": 2.4602,
      "step": 77600
    },
    {
      "epoch": 5.935375448781606,
      "grad_norm": 14.613882064819336,
      "learning_rate": 4.5053853792682e-05,
      "loss": 2.4326,
      "step": 77700
    },
    {
      "epoch": 5.943014284623024,
      "grad_norm": 11.327520370483398,
      "learning_rate": 4.504748809614748e-05,
      "loss": 2.4665,
      "step": 77800
    },
    {
      "epoch": 5.950653120464441,
      "grad_norm": 12.544848442077637,
      "learning_rate": 4.5041122399612965e-05,
      "loss": 2.511,
      "step": 77900
    },
    {
      "epoch": 5.9582919563058585,
      "grad_norm": 15.577120780944824,
      "learning_rate": 4.503475670307845e-05,
      "loss": 2.5255,
      "step": 78000
    },
    {
      "epoch": 5.965930792147277,
      "grad_norm": 17.608137130737305,
      "learning_rate": 4.502839100654394e-05,
      "loss": 2.5264,
      "step": 78100
    },
    {
      "epoch": 5.973569627988694,
      "grad_norm": 13.258968353271484,
      "learning_rate": 4.502202531000942e-05,
      "loss": 2.4683,
      "step": 78200
    },
    {
      "epoch": 5.981208463830113,
      "grad_norm": 14.879775047302246,
      "learning_rate": 4.5015659613474906e-05,
      "loss": 2.5295,
      "step": 78300
    },
    {
      "epoch": 5.98884729967153,
      "grad_norm": 13.505859375,
      "learning_rate": 4.500929391694039e-05,
      "loss": 2.4435,
      "step": 78400
    },
    {
      "epoch": 5.9964861355129475,
      "grad_norm": 14.788415908813477,
      "learning_rate": 4.500292822040588e-05,
      "loss": 2.4522,
      "step": 78500
    },
    {
      "epoch": 6.0,
      "eval_loss": 2.370558023452759,
      "eval_runtime": 1.6459,
      "eval_samples_per_second": 419.232,
      "eval_steps_per_second": 419.232,
      "step": 78546
    },
    {
      "epoch": 6.0,
      "eval_loss": 2.2900497913360596,
      "eval_runtime": 30.4944,
      "eval_samples_per_second": 429.292,
      "eval_steps_per_second": 429.292,
      "step": 78546
    },
    {
      "epoch": 6.004124971354366,
      "grad_norm": 13.572650909423828,
      "learning_rate": 4.499656252387136e-05,
      "loss": 2.4403,
      "step": 78600
    },
    {
      "epoch": 6.011763807195783,
      "grad_norm": 13.177597999572754,
      "learning_rate": 4.499019682733685e-05,
      "loss": 2.4751,
      "step": 78700
    },
    {
      "epoch": 6.019402643037201,
      "grad_norm": 12.380672454833984,
      "learning_rate": 4.498383113080234e-05,
      "loss": 2.3597,
      "step": 78800
    },
    {
      "epoch": 6.027041478878619,
      "grad_norm": 11.584336280822754,
      "learning_rate": 4.497746543426782e-05,
      "loss": 2.4484,
      "step": 78900
    },
    {
      "epoch": 6.0346803147200365,
      "grad_norm": 18.193593978881836,
      "learning_rate": 4.497109973773331e-05,
      "loss": 2.5071,
      "step": 79000
    },
    {
      "epoch": 6.042319150561455,
      "grad_norm": 11.484601974487305,
      "learning_rate": 4.4964734041198795e-05,
      "loss": 2.5438,
      "step": 79100
    },
    {
      "epoch": 6.049957986402872,
      "grad_norm": 11.498671531677246,
      "learning_rate": 4.495836834466428e-05,
      "loss": 2.5009,
      "step": 79200
    },
    {
      "epoch": 6.05759682224429,
      "grad_norm": 10.258905410766602,
      "learning_rate": 4.495200264812976e-05,
      "loss": 2.5378,
      "step": 79300
    },
    {
      "epoch": 6.065235658085708,
      "grad_norm": 13.347442626953125,
      "learning_rate": 4.4945636951595245e-05,
      "loss": 2.4973,
      "step": 79400
    },
    {
      "epoch": 6.0728744939271255,
      "grad_norm": 13.578498840332031,
      "learning_rate": 4.4939271255060735e-05,
      "loss": 2.4644,
      "step": 79500
    },
    {
      "epoch": 6.080513329768543,
      "grad_norm": 12.653122901916504,
      "learning_rate": 4.493290555852622e-05,
      "loss": 2.4437,
      "step": 79600
    },
    {
      "epoch": 6.088152165609961,
      "grad_norm": 13.117034912109375,
      "learning_rate": 4.49265398619917e-05,
      "loss": 2.437,
      "step": 79700
    },
    {
      "epoch": 6.095791001451379,
      "grad_norm": 16.401193618774414,
      "learning_rate": 4.4920174165457186e-05,
      "loss": 2.4951,
      "step": 79800
    },
    {
      "epoch": 6.103429837292796,
      "grad_norm": 12.098478317260742,
      "learning_rate": 4.491380846892267e-05,
      "loss": 2.4083,
      "step": 79900
    },
    {
      "epoch": 6.1110686731342145,
      "grad_norm": 15.833762168884277,
      "learning_rate": 4.490744277238816e-05,
      "loss": 2.5322,
      "step": 80000
    },
    {
      "epoch": 6.118707508975632,
      "grad_norm": 10.533912658691406,
      "learning_rate": 4.4901077075853643e-05,
      "loss": 2.526,
      "step": 80100
    },
    {
      "epoch": 6.126346344817049,
      "grad_norm": 16.33697509765625,
      "learning_rate": 4.489471137931913e-05,
      "loss": 2.4653,
      "step": 80200
    },
    {
      "epoch": 6.133985180658468,
      "grad_norm": 11.717741966247559,
      "learning_rate": 4.488834568278461e-05,
      "loss": 2.3829,
      "step": 80300
    },
    {
      "epoch": 6.141624016499885,
      "grad_norm": 13.492165565490723,
      "learning_rate": 4.48819799862501e-05,
      "loss": 2.5102,
      "step": 80400
    },
    {
      "epoch": 6.1492628523413035,
      "grad_norm": 12.39285659790039,
      "learning_rate": 4.4875614289715584e-05,
      "loss": 2.4435,
      "step": 80500
    },
    {
      "epoch": 6.156901688182721,
      "grad_norm": 10.267317771911621,
      "learning_rate": 4.486924859318107e-05,
      "loss": 2.4541,
      "step": 80600
    },
    {
      "epoch": 6.164540524024138,
      "grad_norm": 11.374979019165039,
      "learning_rate": 4.486288289664655e-05,
      "loss": 2.408,
      "step": 80700
    },
    {
      "epoch": 6.172179359865557,
      "grad_norm": 12.484052658081055,
      "learning_rate": 4.4856517200112035e-05,
      "loss": 2.4405,
      "step": 80800
    },
    {
      "epoch": 6.179818195706974,
      "grad_norm": 11.328790664672852,
      "learning_rate": 4.4850151503577525e-05,
      "loss": 2.476,
      "step": 80900
    },
    {
      "epoch": 6.187457031548392,
      "grad_norm": 11.327890396118164,
      "learning_rate": 4.484378580704301e-05,
      "loss": 2.5081,
      "step": 81000
    },
    {
      "epoch": 6.19509586738981,
      "grad_norm": 11.697254180908203,
      "learning_rate": 4.483742011050849e-05,
      "loss": 2.5434,
      "step": 81100
    },
    {
      "epoch": 6.202734703231227,
      "grad_norm": 15.613945007324219,
      "learning_rate": 4.4831054413973976e-05,
      "loss": 2.5352,
      "step": 81200
    },
    {
      "epoch": 6.210373539072645,
      "grad_norm": 13.850234031677246,
      "learning_rate": 4.4824688717439466e-05,
      "loss": 2.4524,
      "step": 81300
    },
    {
      "epoch": 6.218012374914063,
      "grad_norm": 14.8237943649292,
      "learning_rate": 4.481832302090495e-05,
      "loss": 2.4632,
      "step": 81400
    },
    {
      "epoch": 6.225651210755481,
      "grad_norm": 12.937705993652344,
      "learning_rate": 4.481195732437043e-05,
      "loss": 2.4745,
      "step": 81500
    },
    {
      "epoch": 6.233290046596899,
      "grad_norm": 12.215503692626953,
      "learning_rate": 4.480559162783592e-05,
      "loss": 2.574,
      "step": 81600
    },
    {
      "epoch": 6.2409288824383164,
      "grad_norm": 13.820923805236816,
      "learning_rate": 4.47992259313014e-05,
      "loss": 2.4436,
      "step": 81700
    },
    {
      "epoch": 6.248567718279734,
      "grad_norm": 14.463455200195312,
      "learning_rate": 4.479286023476689e-05,
      "loss": 2.404,
      "step": 81800
    },
    {
      "epoch": 6.256206554121152,
      "grad_norm": 8.859297752380371,
      "learning_rate": 4.4786494538232374e-05,
      "loss": 2.3764,
      "step": 81900
    },
    {
      "epoch": 6.26384538996257,
      "grad_norm": 13.940421104431152,
      "learning_rate": 4.478012884169786e-05,
      "loss": 2.518,
      "step": 82000
    },
    {
      "epoch": 6.271484225803987,
      "grad_norm": 13.941246032714844,
      "learning_rate": 4.477376314516334e-05,
      "loss": 2.5267,
      "step": 82100
    },
    {
      "epoch": 6.2791230616454055,
      "grad_norm": 12.216660499572754,
      "learning_rate": 4.476739744862883e-05,
      "loss": 2.5301,
      "step": 82200
    },
    {
      "epoch": 6.286761897486823,
      "grad_norm": 9.873931884765625,
      "learning_rate": 4.4761031752094315e-05,
      "loss": 2.3737,
      "step": 82300
    },
    {
      "epoch": 6.29440073332824,
      "grad_norm": 12.163853645324707,
      "learning_rate": 4.47546660555598e-05,
      "loss": 2.4707,
      "step": 82400
    },
    {
      "epoch": 6.302039569169659,
      "grad_norm": 13.658315658569336,
      "learning_rate": 4.474830035902529e-05,
      "loss": 2.4209,
      "step": 82500
    },
    {
      "epoch": 6.309678405011076,
      "grad_norm": 16.77581214904785,
      "learning_rate": 4.474193466249077e-05,
      "loss": 2.4448,
      "step": 82600
    },
    {
      "epoch": 6.3173172408524945,
      "grad_norm": 13.275674819946289,
      "learning_rate": 4.4735568965956256e-05,
      "loss": 2.4904,
      "step": 82700
    },
    {
      "epoch": 6.324956076693912,
      "grad_norm": 14.882854461669922,
      "learning_rate": 4.4729203269421746e-05,
      "loss": 2.4928,
      "step": 82800
    },
    {
      "epoch": 6.332594912535329,
      "grad_norm": 10.600934028625488,
      "learning_rate": 4.472283757288723e-05,
      "loss": 2.3579,
      "step": 82900
    },
    {
      "epoch": 6.340233748376748,
      "grad_norm": 11.633917808532715,
      "learning_rate": 4.4716471876352713e-05,
      "loss": 2.5073,
      "step": 83000
    },
    {
      "epoch": 6.347872584218165,
      "grad_norm": 14.381379127502441,
      "learning_rate": 4.47101061798182e-05,
      "loss": 2.4235,
      "step": 83100
    },
    {
      "epoch": 6.355511420059583,
      "grad_norm": 12.788698196411133,
      "learning_rate": 4.470374048328369e-05,
      "loss": 2.4639,
      "step": 83200
    },
    {
      "epoch": 6.363150255901001,
      "grad_norm": 12.371739387512207,
      "learning_rate": 4.469737478674917e-05,
      "loss": 2.4883,
      "step": 83300
    },
    {
      "epoch": 6.370789091742418,
      "grad_norm": 14.545661926269531,
      "learning_rate": 4.4691009090214654e-05,
      "loss": 2.5158,
      "step": 83400
    },
    {
      "epoch": 6.378427927583836,
      "grad_norm": 14.134713172912598,
      "learning_rate": 4.468464339368014e-05,
      "loss": 2.4878,
      "step": 83500
    },
    {
      "epoch": 6.386066763425254,
      "grad_norm": 12.961318016052246,
      "learning_rate": 4.467827769714563e-05,
      "loss": 2.5076,
      "step": 83600
    },
    {
      "epoch": 6.393705599266672,
      "grad_norm": 10.21134090423584,
      "learning_rate": 4.467191200061111e-05,
      "loss": 2.4131,
      "step": 83700
    },
    {
      "epoch": 6.40134443510809,
      "grad_norm": 11.269109725952148,
      "learning_rate": 4.4665546304076595e-05,
      "loss": 2.4343,
      "step": 83800
    },
    {
      "epoch": 6.408983270949507,
      "grad_norm": 13.678780555725098,
      "learning_rate": 4.465918060754208e-05,
      "loss": 2.4971,
      "step": 83900
    },
    {
      "epoch": 6.416622106790925,
      "grad_norm": 15.545336723327637,
      "learning_rate": 4.465281491100756e-05,
      "loss": 2.442,
      "step": 84000
    },
    {
      "epoch": 6.424260942632343,
      "grad_norm": 13.038215637207031,
      "learning_rate": 4.464644921447305e-05,
      "loss": 2.531,
      "step": 84100
    },
    {
      "epoch": 6.431899778473761,
      "grad_norm": 14.475530624389648,
      "learning_rate": 4.4640083517938536e-05,
      "loss": 2.4542,
      "step": 84200
    },
    {
      "epoch": 6.439538614315178,
      "grad_norm": 12.456769943237305,
      "learning_rate": 4.463371782140402e-05,
      "loss": 2.4793,
      "step": 84300
    },
    {
      "epoch": 6.447177450156596,
      "grad_norm": 9.22740650177002,
      "learning_rate": 4.46273521248695e-05,
      "loss": 2.4099,
      "step": 84400
    },
    {
      "epoch": 6.454816285998014,
      "grad_norm": 8.403965950012207,
      "learning_rate": 4.4620986428334994e-05,
      "loss": 2.4469,
      "step": 84500
    },
    {
      "epoch": 6.462455121839431,
      "grad_norm": 10.841958999633789,
      "learning_rate": 4.461462073180048e-05,
      "loss": 2.5036,
      "step": 84600
    },
    {
      "epoch": 6.47009395768085,
      "grad_norm": 12.609073638916016,
      "learning_rate": 4.460825503526596e-05,
      "loss": 2.4654,
      "step": 84700
    },
    {
      "epoch": 6.477732793522267,
      "grad_norm": 17.41413116455078,
      "learning_rate": 4.4601889338731444e-05,
      "loss": 2.4366,
      "step": 84800
    },
    {
      "epoch": 6.485371629363685,
      "grad_norm": 11.425345420837402,
      "learning_rate": 4.459552364219693e-05,
      "loss": 2.4904,
      "step": 84900
    },
    {
      "epoch": 6.493010465205103,
      "grad_norm": 10.646919250488281,
      "learning_rate": 4.458915794566242e-05,
      "loss": 2.445,
      "step": 85000
    },
    {
      "epoch": 6.50064930104652,
      "grad_norm": 12.057345390319824,
      "learning_rate": 4.45827922491279e-05,
      "loss": 2.4801,
      "step": 85100
    },
    {
      "epoch": 6.508288136887939,
      "grad_norm": 14.715888977050781,
      "learning_rate": 4.4576426552593385e-05,
      "loss": 2.512,
      "step": 85200
    },
    {
      "epoch": 6.515926972729356,
      "grad_norm": 12.258204460144043,
      "learning_rate": 4.457006085605887e-05,
      "loss": 2.358,
      "step": 85300
    },
    {
      "epoch": 6.5235658085707735,
      "grad_norm": 14.871845245361328,
      "learning_rate": 4.456369515952435e-05,
      "loss": 2.4515,
      "step": 85400
    },
    {
      "epoch": 6.531204644412192,
      "grad_norm": 9.484603881835938,
      "learning_rate": 4.455732946298984e-05,
      "loss": 2.3902,
      "step": 85500
    },
    {
      "epoch": 6.538843480253609,
      "grad_norm": 11.273622512817383,
      "learning_rate": 4.4550963766455326e-05,
      "loss": 2.5421,
      "step": 85600
    },
    {
      "epoch": 6.546482316095027,
      "grad_norm": 11.898159980773926,
      "learning_rate": 4.454459806992081e-05,
      "loss": 2.4649,
      "step": 85700
    },
    {
      "epoch": 6.554121151936445,
      "grad_norm": 10.66866397857666,
      "learning_rate": 4.453823237338629e-05,
      "loss": 2.478,
      "step": 85800
    },
    {
      "epoch": 6.5617599877778625,
      "grad_norm": 13.212664604187012,
      "learning_rate": 4.4531866676851783e-05,
      "loss": 2.4172,
      "step": 85900
    },
    {
      "epoch": 6.569398823619281,
      "grad_norm": 14.722480773925781,
      "learning_rate": 4.452550098031727e-05,
      "loss": 2.4452,
      "step": 86000
    },
    {
      "epoch": 6.577037659460698,
      "grad_norm": 16.404556274414062,
      "learning_rate": 4.451913528378275e-05,
      "loss": 2.3671,
      "step": 86100
    },
    {
      "epoch": 6.584676495302116,
      "grad_norm": 17.101543426513672,
      "learning_rate": 4.451276958724824e-05,
      "loss": 2.4548,
      "step": 86200
    },
    {
      "epoch": 6.592315331143534,
      "grad_norm": 13.860397338867188,
      "learning_rate": 4.4506403890713724e-05,
      "loss": 2.378,
      "step": 86300
    },
    {
      "epoch": 6.5999541669849515,
      "grad_norm": 11.183494567871094,
      "learning_rate": 4.450003819417921e-05,
      "loss": 2.4353,
      "step": 86400
    },
    {
      "epoch": 6.607593002826369,
      "grad_norm": 13.81667423248291,
      "learning_rate": 4.44936724976447e-05,
      "loss": 2.4789,
      "step": 86500
    },
    {
      "epoch": 6.615231838667787,
      "grad_norm": 12.380939483642578,
      "learning_rate": 4.448730680111018e-05,
      "loss": 2.4685,
      "step": 86600
    },
    {
      "epoch": 6.622870674509205,
      "grad_norm": 15.984358787536621,
      "learning_rate": 4.4480941104575665e-05,
      "loss": 2.4096,
      "step": 86700
    },
    {
      "epoch": 6.630509510350622,
      "grad_norm": 12.018927574157715,
      "learning_rate": 4.4474575408041156e-05,
      "loss": 2.4956,
      "step": 86800
    },
    {
      "epoch": 6.6381483461920405,
      "grad_norm": 12.243518829345703,
      "learning_rate": 4.446820971150664e-05,
      "loss": 2.479,
      "step": 86900
    },
    {
      "epoch": 6.645787182033458,
      "grad_norm": 10.717767715454102,
      "learning_rate": 4.446184401497212e-05,
      "loss": 2.4293,
      "step": 87000
    },
    {
      "epoch": 6.653426017874876,
      "grad_norm": 11.857994079589844,
      "learning_rate": 4.4455478318437606e-05,
      "loss": 2.4915,
      "step": 87100
    },
    {
      "epoch": 6.661064853716294,
      "grad_norm": 12.861920356750488,
      "learning_rate": 4.444911262190309e-05,
      "loss": 2.407,
      "step": 87200
    },
    {
      "epoch": 6.668703689557711,
      "grad_norm": 10.041036605834961,
      "learning_rate": 4.444274692536858e-05,
      "loss": 2.5177,
      "step": 87300
    },
    {
      "epoch": 6.6763425253991295,
      "grad_norm": 10.925410270690918,
      "learning_rate": 4.4436381228834064e-05,
      "loss": 2.477,
      "step": 87400
    },
    {
      "epoch": 6.683981361240547,
      "grad_norm": 12.748122215270996,
      "learning_rate": 4.443001553229955e-05,
      "loss": 2.3797,
      "step": 87500
    },
    {
      "epoch": 6.691620197081964,
      "grad_norm": 12.811003684997559,
      "learning_rate": 4.442364983576503e-05,
      "loss": 2.4748,
      "step": 87600
    },
    {
      "epoch": 6.699259032923383,
      "grad_norm": 11.455586433410645,
      "learning_rate": 4.441728413923052e-05,
      "loss": 2.3684,
      "step": 87700
    },
    {
      "epoch": 6.7068978687648,
      "grad_norm": 13.338706970214844,
      "learning_rate": 4.4410918442696004e-05,
      "loss": 2.46,
      "step": 87800
    },
    {
      "epoch": 6.714536704606218,
      "grad_norm": 10.864389419555664,
      "learning_rate": 4.440455274616149e-05,
      "loss": 2.373,
      "step": 87900
    },
    {
      "epoch": 6.722175540447636,
      "grad_norm": 12.4636812210083,
      "learning_rate": 4.439818704962697e-05,
      "loss": 2.4504,
      "step": 88000
    },
    {
      "epoch": 6.729814376289053,
      "grad_norm": 9.605029106140137,
      "learning_rate": 4.4391821353092455e-05,
      "loss": 2.4928,
      "step": 88100
    },
    {
      "epoch": 6.737453212130472,
      "grad_norm": 11.704425811767578,
      "learning_rate": 4.4385455656557945e-05,
      "loss": 2.4757,
      "step": 88200
    },
    {
      "epoch": 6.745092047971889,
      "grad_norm": 11.576091766357422,
      "learning_rate": 4.437908996002343e-05,
      "loss": 2.3916,
      "step": 88300
    },
    {
      "epoch": 6.752730883813307,
      "grad_norm": 15.081165313720703,
      "learning_rate": 4.437272426348891e-05,
      "loss": 2.4224,
      "step": 88400
    },
    {
      "epoch": 6.760369719654725,
      "grad_norm": 12.066901206970215,
      "learning_rate": 4.4366358566954396e-05,
      "loss": 2.447,
      "step": 88500
    },
    {
      "epoch": 6.768008555496142,
      "grad_norm": 12.512107849121094,
      "learning_rate": 4.435999287041988e-05,
      "loss": 2.3804,
      "step": 88600
    },
    {
      "epoch": 6.77564739133756,
      "grad_norm": 10.544360160827637,
      "learning_rate": 4.435362717388537e-05,
      "loss": 2.4942,
      "step": 88700
    },
    {
      "epoch": 6.783286227178978,
      "grad_norm": 11.311840057373047,
      "learning_rate": 4.4347261477350853e-05,
      "loss": 2.4136,
      "step": 88800
    },
    {
      "epoch": 6.790925063020396,
      "grad_norm": 10.824625015258789,
      "learning_rate": 4.434089578081634e-05,
      "loss": 2.5093,
      "step": 88900
    },
    {
      "epoch": 6.798563898861813,
      "grad_norm": 15.773571968078613,
      "learning_rate": 4.433453008428182e-05,
      "loss": 2.3932,
      "step": 89000
    },
    {
      "epoch": 6.8062027347032314,
      "grad_norm": 10.760135650634766,
      "learning_rate": 4.432816438774731e-05,
      "loss": 2.4606,
      "step": 89100
    },
    {
      "epoch": 6.813841570544649,
      "grad_norm": 10.74740982055664,
      "learning_rate": 4.4321798691212794e-05,
      "loss": 2.4189,
      "step": 89200
    },
    {
      "epoch": 6.821480406386067,
      "grad_norm": 16.09577178955078,
      "learning_rate": 4.431543299467828e-05,
      "loss": 2.434,
      "step": 89300
    },
    {
      "epoch": 6.829119242227485,
      "grad_norm": 13.089216232299805,
      "learning_rate": 4.430906729814376e-05,
      "loss": 2.3857,
      "step": 89400
    },
    {
      "epoch": 6.836758078068902,
      "grad_norm": 13.20756721496582,
      "learning_rate": 4.4302701601609245e-05,
      "loss": 2.4113,
      "step": 89500
    },
    {
      "epoch": 6.84439691391032,
      "grad_norm": 13.611286163330078,
      "learning_rate": 4.4296335905074735e-05,
      "loss": 2.3956,
      "step": 89600
    },
    {
      "epoch": 6.852035749751738,
      "grad_norm": 12.789663314819336,
      "learning_rate": 4.428997020854022e-05,
      "loss": 2.3977,
      "step": 89700
    },
    {
      "epoch": 6.859674585593155,
      "grad_norm": 13.58879566192627,
      "learning_rate": 4.42836045120057e-05,
      "loss": 2.5069,
      "step": 89800
    },
    {
      "epoch": 6.867313421434574,
      "grad_norm": 12.806365966796875,
      "learning_rate": 4.4277238815471186e-05,
      "loss": 2.5142,
      "step": 89900
    },
    {
      "epoch": 6.874952257275991,
      "grad_norm": 13.350872039794922,
      "learning_rate": 4.4270873118936676e-05,
      "loss": 2.4428,
      "step": 90000
    },
    {
      "epoch": 6.882591093117409,
      "grad_norm": 16.46275520324707,
      "learning_rate": 4.426450742240216e-05,
      "loss": 2.4668,
      "step": 90100
    },
    {
      "epoch": 6.890229928958827,
      "grad_norm": 12.276466369628906,
      "learning_rate": 4.425814172586765e-05,
      "loss": 2.4552,
      "step": 90200
    },
    {
      "epoch": 6.897868764800244,
      "grad_norm": 11.628069877624512,
      "learning_rate": 4.4251776029333134e-05,
      "loss": 2.4221,
      "step": 90300
    },
    {
      "epoch": 6.905507600641663,
      "grad_norm": 10.653274536132812,
      "learning_rate": 4.424541033279862e-05,
      "loss": 2.3919,
      "step": 90400
    },
    {
      "epoch": 6.91314643648308,
      "grad_norm": 11.807222366333008,
      "learning_rate": 4.423904463626411e-05,
      "loss": 2.4477,
      "step": 90500
    },
    {
      "epoch": 6.920785272324498,
      "grad_norm": 12.031062126159668,
      "learning_rate": 4.423267893972959e-05,
      "loss": 2.5393,
      "step": 90600
    },
    {
      "epoch": 6.928424108165915,
      "grad_norm": 15.673295021057129,
      "learning_rate": 4.4226313243195074e-05,
      "loss": 2.3117,
      "step": 90700
    },
    {
      "epoch": 6.936062944007333,
      "grad_norm": 15.280451774597168,
      "learning_rate": 4.421994754666056e-05,
      "loss": 2.5776,
      "step": 90800
    },
    {
      "epoch": 6.943701779848751,
      "grad_norm": 11.797028541564941,
      "learning_rate": 4.421358185012604e-05,
      "loss": 2.4564,
      "step": 90900
    },
    {
      "epoch": 6.951340615690169,
      "grad_norm": 8.796141624450684,
      "learning_rate": 4.420721615359153e-05,
      "loss": 2.4425,
      "step": 91000
    },
    {
      "epoch": 6.958979451531587,
      "grad_norm": 15.894661903381348,
      "learning_rate": 4.4200850457057015e-05,
      "loss": 2.3954,
      "step": 91100
    },
    {
      "epoch": 6.966618287373004,
      "grad_norm": 12.677579879760742,
      "learning_rate": 4.41944847605225e-05,
      "loss": 2.5293,
      "step": 91200
    },
    {
      "epoch": 6.974257123214422,
      "grad_norm": 13.044416427612305,
      "learning_rate": 4.418811906398798e-05,
      "loss": 2.3648,
      "step": 91300
    },
    {
      "epoch": 6.98189595905584,
      "grad_norm": 13.435522079467773,
      "learning_rate": 4.418175336745347e-05,
      "loss": 2.4573,
      "step": 91400
    },
    {
      "epoch": 6.989534794897258,
      "grad_norm": 11.414043426513672,
      "learning_rate": 4.4175387670918956e-05,
      "loss": 2.3938,
      "step": 91500
    },
    {
      "epoch": 6.997173630738676,
      "grad_norm": 9.492674827575684,
      "learning_rate": 4.416902197438444e-05,
      "loss": 2.4373,
      "step": 91600
    },
    {
      "epoch": 7.0,
      "eval_loss": 2.327303886413574,
      "eval_runtime": 1.6378,
      "eval_samples_per_second": 421.3,
      "eval_steps_per_second": 421.3,
      "step": 91637
    },
    {
      "epoch": 7.0,
      "eval_loss": 2.236666679382324,
      "eval_runtime": 30.2928,
      "eval_samples_per_second": 432.149,
      "eval_steps_per_second": 432.149,
      "step": 91637
    },
    {
      "epoch": 7.004812466580093,
      "grad_norm": 11.873516082763672,
      "learning_rate": 4.4162656277849923e-05,
      "loss": 2.4147,
      "step": 91700
    },
    {
      "epoch": 7.012451302421511,
      "grad_norm": 14.952775955200195,
      "learning_rate": 4.415629058131541e-05,
      "loss": 2.3564,
      "step": 91800
    },
    {
      "epoch": 7.020090138262929,
      "grad_norm": 11.981884956359863,
      "learning_rate": 4.41499248847809e-05,
      "loss": 2.4152,
      "step": 91900
    },
    {
      "epoch": 7.027728974104346,
      "grad_norm": 12.737842559814453,
      "learning_rate": 4.414355918824638e-05,
      "loss": 2.4419,
      "step": 92000
    },
    {
      "epoch": 7.035367809945765,
      "grad_norm": 13.294007301330566,
      "learning_rate": 4.4137193491711864e-05,
      "loss": 2.3643,
      "step": 92100
    },
    {
      "epoch": 7.043006645787182,
      "grad_norm": 12.238472938537598,
      "learning_rate": 4.413082779517735e-05,
      "loss": 2.5515,
      "step": 92200
    },
    {
      "epoch": 7.0506454816285995,
      "grad_norm": 11.6513090133667,
      "learning_rate": 4.412446209864284e-05,
      "loss": 2.4412,
      "step": 92300
    },
    {
      "epoch": 7.058284317470018,
      "grad_norm": 11.812583923339844,
      "learning_rate": 4.411809640210832e-05,
      "loss": 2.5116,
      "step": 92400
    },
    {
      "epoch": 7.065923153311435,
      "grad_norm": 14.246237754821777,
      "learning_rate": 4.4111730705573805e-05,
      "loss": 2.2645,
      "step": 92500
    },
    {
      "epoch": 7.073561989152853,
      "grad_norm": 11.832343101501465,
      "learning_rate": 4.410536500903929e-05,
      "loss": 2.5212,
      "step": 92600
    },
    {
      "epoch": 7.081200824994271,
      "grad_norm": 15.317879676818848,
      "learning_rate": 4.409899931250477e-05,
      "loss": 2.3943,
      "step": 92700
    },
    {
      "epoch": 7.0888396608356885,
      "grad_norm": 13.582711219787598,
      "learning_rate": 4.409263361597026e-05,
      "loss": 2.4465,
      "step": 92800
    },
    {
      "epoch": 7.096478496677107,
      "grad_norm": 10.422452926635742,
      "learning_rate": 4.4086267919435746e-05,
      "loss": 2.4054,
      "step": 92900
    },
    {
      "epoch": 7.104117332518524,
      "grad_norm": 10.782130241394043,
      "learning_rate": 4.407990222290123e-05,
      "loss": 2.4287,
      "step": 93000
    },
    {
      "epoch": 7.111756168359942,
      "grad_norm": 13.388477325439453,
      "learning_rate": 4.407353652636671e-05,
      "loss": 2.3566,
      "step": 93100
    },
    {
      "epoch": 7.11939500420136,
      "grad_norm": 13.161887168884277,
      "learning_rate": 4.4067170829832204e-05,
      "loss": 2.4654,
      "step": 93200
    },
    {
      "epoch": 7.1270338400427775,
      "grad_norm": 10.52505874633789,
      "learning_rate": 4.406080513329769e-05,
      "loss": 2.4207,
      "step": 93300
    },
    {
      "epoch": 7.134672675884195,
      "grad_norm": 14.801097869873047,
      "learning_rate": 4.405443943676317e-05,
      "loss": 2.391,
      "step": 93400
    },
    {
      "epoch": 7.142311511725613,
      "grad_norm": 15.276142120361328,
      "learning_rate": 4.4048073740228654e-05,
      "loss": 2.4494,
      "step": 93500
    },
    {
      "epoch": 7.149950347567031,
      "grad_norm": 12.752968788146973,
      "learning_rate": 4.404170804369414e-05,
      "loss": 2.3992,
      "step": 93600
    },
    {
      "epoch": 7.157589183408448,
      "grad_norm": 13.837896347045898,
      "learning_rate": 4.403534234715963e-05,
      "loss": 2.4194,
      "step": 93700
    },
    {
      "epoch": 7.1652280192498665,
      "grad_norm": 12.036161422729492,
      "learning_rate": 4.402897665062511e-05,
      "loss": 2.3975,
      "step": 93800
    },
    {
      "epoch": 7.172866855091284,
      "grad_norm": 10.542064666748047,
      "learning_rate": 4.4022610954090595e-05,
      "loss": 2.3324,
      "step": 93900
    },
    {
      "epoch": 7.180505690932701,
      "grad_norm": 13.028186798095703,
      "learning_rate": 4.4016245257556085e-05,
      "loss": 2.3889,
      "step": 94000
    },
    {
      "epoch": 7.18814452677412,
      "grad_norm": 14.533820152282715,
      "learning_rate": 4.400987956102157e-05,
      "loss": 2.4238,
      "step": 94100
    },
    {
      "epoch": 7.195783362615537,
      "grad_norm": 13.873342514038086,
      "learning_rate": 4.400351386448706e-05,
      "loss": 2.4561,
      "step": 94200
    },
    {
      "epoch": 7.2034221984569555,
      "grad_norm": 14.882479667663574,
      "learning_rate": 4.399714816795254e-05,
      "loss": 2.3819,
      "step": 94300
    },
    {
      "epoch": 7.211061034298373,
      "grad_norm": 13.045881271362305,
      "learning_rate": 4.3990782471418026e-05,
      "loss": 2.3878,
      "step": 94400
    },
    {
      "epoch": 7.21869987013979,
      "grad_norm": 20.153501510620117,
      "learning_rate": 4.398441677488351e-05,
      "loss": 2.3306,
      "step": 94500
    },
    {
      "epoch": 7.226338705981209,
      "grad_norm": 14.157700538635254,
      "learning_rate": 4.3978051078349e-05,
      "loss": 2.4126,
      "step": 94600
    },
    {
      "epoch": 7.233977541822626,
      "grad_norm": 10.529792785644531,
      "learning_rate": 4.3971685381814484e-05,
      "loss": 2.4174,
      "step": 94700
    },
    {
      "epoch": 7.241616377664044,
      "grad_norm": 14.711276054382324,
      "learning_rate": 4.396531968527997e-05,
      "loss": 2.4802,
      "step": 94800
    },
    {
      "epoch": 7.249255213505462,
      "grad_norm": 11.132941246032715,
      "learning_rate": 4.395895398874545e-05,
      "loss": 2.4997,
      "step": 94900
    },
    {
      "epoch": 7.256894049346879,
      "grad_norm": 13.82658576965332,
      "learning_rate": 4.3952588292210934e-05,
      "loss": 2.3751,
      "step": 95000
    },
    {
      "epoch": 7.264532885188297,
      "grad_norm": 13.935169219970703,
      "learning_rate": 4.3946222595676425e-05,
      "loss": 2.4113,
      "step": 95100
    },
    {
      "epoch": 7.272171721029715,
      "grad_norm": 11.227822303771973,
      "learning_rate": 4.393985689914191e-05,
      "loss": 2.4302,
      "step": 95200
    },
    {
      "epoch": 7.279810556871133,
      "grad_norm": 11.476109504699707,
      "learning_rate": 4.393349120260739e-05,
      "loss": 2.5592,
      "step": 95300
    },
    {
      "epoch": 7.287449392712551,
      "grad_norm": 10.851920127868652,
      "learning_rate": 4.3927125506072875e-05,
      "loss": 2.3106,
      "step": 95400
    },
    {
      "epoch": 7.295088228553968,
      "grad_norm": 17.990257263183594,
      "learning_rate": 4.3920759809538366e-05,
      "loss": 2.4316,
      "step": 95500
    },
    {
      "epoch": 7.302727064395386,
      "grad_norm": 12.313132286071777,
      "learning_rate": 4.391439411300385e-05,
      "loss": 2.5192,
      "step": 95600
    },
    {
      "epoch": 7.310365900236804,
      "grad_norm": 12.216634750366211,
      "learning_rate": 4.390802841646933e-05,
      "loss": 2.4844,
      "step": 95700
    },
    {
      "epoch": 7.318004736078222,
      "grad_norm": 11.32927417755127,
      "learning_rate": 4.3901662719934816e-05,
      "loss": 2.4963,
      "step": 95800
    },
    {
      "epoch": 7.325643571919639,
      "grad_norm": 10.48082447052002,
      "learning_rate": 4.38952970234003e-05,
      "loss": 2.4256,
      "step": 95900
    },
    {
      "epoch": 7.333282407761057,
      "grad_norm": 11.59725570678711,
      "learning_rate": 4.388893132686579e-05,
      "loss": 2.4467,
      "step": 96000
    },
    {
      "epoch": 7.340921243602475,
      "grad_norm": 13.017173767089844,
      "learning_rate": 4.3882565630331274e-05,
      "loss": 2.3737,
      "step": 96100
    },
    {
      "epoch": 7.348560079443892,
      "grad_norm": 12.756741523742676,
      "learning_rate": 4.387619993379676e-05,
      "loss": 2.4392,
      "step": 96200
    },
    {
      "epoch": 7.356198915285311,
      "grad_norm": 16.319974899291992,
      "learning_rate": 4.386983423726224e-05,
      "loss": 2.366,
      "step": 96300
    },
    {
      "epoch": 7.363837751126728,
      "grad_norm": 9.50271224975586,
      "learning_rate": 4.386346854072773e-05,
      "loss": 2.3627,
      "step": 96400
    },
    {
      "epoch": 7.3714765869681464,
      "grad_norm": 11.69163703918457,
      "learning_rate": 4.3857102844193214e-05,
      "loss": 2.352,
      "step": 96500
    },
    {
      "epoch": 7.379115422809564,
      "grad_norm": 15.034463882446289,
      "learning_rate": 4.38507371476587e-05,
      "loss": 2.4143,
      "step": 96600
    },
    {
      "epoch": 7.386754258650981,
      "grad_norm": 12.296608924865723,
      "learning_rate": 4.384437145112418e-05,
      "loss": 2.4076,
      "step": 96700
    },
    {
      "epoch": 7.3943930944924,
      "grad_norm": 10.704483985900879,
      "learning_rate": 4.3838005754589665e-05,
      "loss": 2.4071,
      "step": 96800
    },
    {
      "epoch": 7.402031930333817,
      "grad_norm": 11.250645637512207,
      "learning_rate": 4.3831640058055155e-05,
      "loss": 2.3569,
      "step": 96900
    },
    {
      "epoch": 7.409670766175235,
      "grad_norm": 12.172455787658691,
      "learning_rate": 4.382527436152064e-05,
      "loss": 2.4445,
      "step": 97000
    },
    {
      "epoch": 7.417309602016653,
      "grad_norm": 15.41821002960205,
      "learning_rate": 4.381890866498612e-05,
      "loss": 2.3954,
      "step": 97100
    },
    {
      "epoch": 7.42494843785807,
      "grad_norm": 11.617393493652344,
      "learning_rate": 4.3812542968451606e-05,
      "loss": 2.5054,
      "step": 97200
    },
    {
      "epoch": 7.432587273699488,
      "grad_norm": 12.649940490722656,
      "learning_rate": 4.380617727191709e-05,
      "loss": 2.3216,
      "step": 97300
    },
    {
      "epoch": 7.440226109540906,
      "grad_norm": 10.852324485778809,
      "learning_rate": 4.379981157538258e-05,
      "loss": 2.3669,
      "step": 97400
    },
    {
      "epoch": 7.447864945382324,
      "grad_norm": 12.36818790435791,
      "learning_rate": 4.3793445878848063e-05,
      "loss": 2.3903,
      "step": 97500
    },
    {
      "epoch": 7.455503781223742,
      "grad_norm": 12.302620887756348,
      "learning_rate": 4.378708018231355e-05,
      "loss": 2.3236,
      "step": 97600
    },
    {
      "epoch": 7.463142617065159,
      "grad_norm": 15.821454048156738,
      "learning_rate": 4.378071448577904e-05,
      "loss": 2.4619,
      "step": 97700
    },
    {
      "epoch": 7.470781452906577,
      "grad_norm": 12.652711868286133,
      "learning_rate": 4.377434878924452e-05,
      "loss": 2.4241,
      "step": 97800
    },
    {
      "epoch": 7.478420288747995,
      "grad_norm": 8.7543363571167,
      "learning_rate": 4.3767983092710004e-05,
      "loss": 2.3853,
      "step": 97900
    },
    {
      "epoch": 7.486059124589413,
      "grad_norm": 11.818195343017578,
      "learning_rate": 4.3761617396175495e-05,
      "loss": 2.3795,
      "step": 98000
    },
    {
      "epoch": 7.49369796043083,
      "grad_norm": 12.650860786437988,
      "learning_rate": 4.375525169964098e-05,
      "loss": 2.3883,
      "step": 98100
    },
    {
      "epoch": 7.501336796272248,
      "grad_norm": 13.69461727142334,
      "learning_rate": 4.374888600310646e-05,
      "loss": 2.3564,
      "step": 98200
    },
    {
      "epoch": 7.508975632113666,
      "grad_norm": 11.31379222869873,
      "learning_rate": 4.374252030657195e-05,
      "loss": 2.3251,
      "step": 98300
    },
    {
      "epoch": 7.516614467955083,
      "grad_norm": 12.075193405151367,
      "learning_rate": 4.3736154610037436e-05,
      "loss": 2.3902,
      "step": 98400
    },
    {
      "epoch": 7.524253303796502,
      "grad_norm": 11.10255241394043,
      "learning_rate": 4.372978891350292e-05,
      "loss": 2.382,
      "step": 98500
    },
    {
      "epoch": 7.531892139637919,
      "grad_norm": 11.645291328430176,
      "learning_rate": 4.37234232169684e-05,
      "loss": 2.4029,
      "step": 98600
    },
    {
      "epoch": 7.539530975479337,
      "grad_norm": 11.3462553024292,
      "learning_rate": 4.371705752043389e-05,
      "loss": 2.388,
      "step": 98700
    },
    {
      "epoch": 7.547169811320755,
      "grad_norm": 14.053611755371094,
      "learning_rate": 4.3710691823899376e-05,
      "loss": 2.4443,
      "step": 98800
    },
    {
      "epoch": 7.554808647162172,
      "grad_norm": 15.378369331359863,
      "learning_rate": 4.370432612736486e-05,
      "loss": 2.427,
      "step": 98900
    },
    {
      "epoch": 7.562447483003591,
      "grad_norm": 9.637811660766602,
      "learning_rate": 4.3697960430830344e-05,
      "loss": 2.3711,
      "step": 99000
    },
    {
      "epoch": 7.570086318845008,
      "grad_norm": 14.945003509521484,
      "learning_rate": 4.369159473429583e-05,
      "loss": 2.2983,
      "step": 99100
    },
    {
      "epoch": 7.5777251546864255,
      "grad_norm": 11.609055519104004,
      "learning_rate": 4.368522903776132e-05,
      "loss": 2.3929,
      "step": 99200
    },
    {
      "epoch": 7.585363990527844,
      "grad_norm": 12.529069900512695,
      "learning_rate": 4.36788633412268e-05,
      "loss": 2.4085,
      "step": 99300
    },
    {
      "epoch": 7.593002826369261,
      "grad_norm": 16.469768524169922,
      "learning_rate": 4.3672497644692284e-05,
      "loss": 2.474,
      "step": 99400
    },
    {
      "epoch": 7.600641662210679,
      "grad_norm": 9.650498390197754,
      "learning_rate": 4.366613194815777e-05,
      "loss": 2.3505,
      "step": 99500
    },
    {
      "epoch": 7.608280498052097,
      "grad_norm": 15.312776565551758,
      "learning_rate": 4.365976625162325e-05,
      "loss": 2.4073,
      "step": 99600
    },
    {
      "epoch": 7.6159193338935145,
      "grad_norm": 11.892254829406738,
      "learning_rate": 4.365340055508874e-05,
      "loss": 2.3333,
      "step": 99700
    },
    {
      "epoch": 7.623558169734933,
      "grad_norm": 14.56708812713623,
      "learning_rate": 4.3647034858554225e-05,
      "loss": 2.4297,
      "step": 99800
    },
    {
      "epoch": 7.63119700557635,
      "grad_norm": 14.000262260437012,
      "learning_rate": 4.364066916201971e-05,
      "loss": 2.4149,
      "step": 99900
    },
    {
      "epoch": 7.638835841417768,
      "grad_norm": 13.481621742248535,
      "learning_rate": 4.363430346548519e-05,
      "loss": 2.4665,
      "step": 100000
    },
    {
      "epoch": 7.646474677259186,
      "grad_norm": 10.985011100769043,
      "learning_rate": 4.362793776895068e-05,
      "loss": 2.4218,
      "step": 100100
    },
    {
      "epoch": 7.6541135131006035,
      "grad_norm": 10.972450256347656,
      "learning_rate": 4.3621572072416166e-05,
      "loss": 2.3646,
      "step": 100200
    },
    {
      "epoch": 7.661752348942021,
      "grad_norm": 12.170304298400879,
      "learning_rate": 4.361520637588165e-05,
      "loss": 2.3928,
      "step": 100300
    },
    {
      "epoch": 7.669391184783439,
      "grad_norm": 13.108136177062988,
      "learning_rate": 4.3608840679347133e-05,
      "loss": 2.3826,
      "step": 100400
    },
    {
      "epoch": 7.677030020624857,
      "grad_norm": 13.831268310546875,
      "learning_rate": 4.360247498281262e-05,
      "loss": 2.4342,
      "step": 100500
    },
    {
      "epoch": 7.684668856466274,
      "grad_norm": 10.949342727661133,
      "learning_rate": 4.359610928627811e-05,
      "loss": 2.3284,
      "step": 100600
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 12.867829322814941,
      "learning_rate": 4.358974358974359e-05,
      "loss": 2.3037,
      "step": 100700
    },
    {
      "epoch": 7.69994652814911,
      "grad_norm": 12.320981979370117,
      "learning_rate": 4.3583377893209074e-05,
      "loss": 2.4116,
      "step": 100800
    },
    {
      "epoch": 7.707585363990528,
      "grad_norm": 14.569634437561035,
      "learning_rate": 4.357701219667456e-05,
      "loss": 2.4106,
      "step": 100900
    },
    {
      "epoch": 7.715224199831946,
      "grad_norm": 16.33366584777832,
      "learning_rate": 4.357064650014005e-05,
      "loss": 2.3969,
      "step": 101000
    },
    {
      "epoch": 7.722863035673363,
      "grad_norm": 13.698461532592773,
      "learning_rate": 4.356428080360553e-05,
      "loss": 2.43,
      "step": 101100
    },
    {
      "epoch": 7.7305018715147815,
      "grad_norm": 10.193178176879883,
      "learning_rate": 4.3557915107071015e-05,
      "loss": 2.3767,
      "step": 101200
    },
    {
      "epoch": 7.738140707356199,
      "grad_norm": 16.129423141479492,
      "learning_rate": 4.35515494105365e-05,
      "loss": 2.3768,
      "step": 101300
    },
    {
      "epoch": 7.745779543197616,
      "grad_norm": 13.597463607788086,
      "learning_rate": 4.354518371400199e-05,
      "loss": 2.4439,
      "step": 101400
    },
    {
      "epoch": 7.753418379039035,
      "grad_norm": 13.291452407836914,
      "learning_rate": 4.353881801746747e-05,
      "loss": 2.3543,
      "step": 101500
    },
    {
      "epoch": 7.761057214880452,
      "grad_norm": 13.645061492919922,
      "learning_rate": 4.3532452320932956e-05,
      "loss": 2.2949,
      "step": 101600
    },
    {
      "epoch": 7.76869605072187,
      "grad_norm": 13.25839614868164,
      "learning_rate": 4.3526086624398446e-05,
      "loss": 2.4366,
      "step": 101700
    },
    {
      "epoch": 7.776334886563288,
      "grad_norm": 15.025619506835938,
      "learning_rate": 4.351972092786393e-05,
      "loss": 2.3873,
      "step": 101800
    },
    {
      "epoch": 7.783973722404705,
      "grad_norm": 13.937986373901367,
      "learning_rate": 4.3513355231329414e-05,
      "loss": 2.3958,
      "step": 101900
    },
    {
      "epoch": 7.791612558246124,
      "grad_norm": 15.0819673538208,
      "learning_rate": 4.3506989534794904e-05,
      "loss": 2.3503,
      "step": 102000
    },
    {
      "epoch": 7.799251394087541,
      "grad_norm": 14.493078231811523,
      "learning_rate": 4.350062383826039e-05,
      "loss": 2.4038,
      "step": 102100
    },
    {
      "epoch": 7.806890229928959,
      "grad_norm": 11.676753997802734,
      "learning_rate": 4.349425814172587e-05,
      "loss": 2.4091,
      "step": 102200
    },
    {
      "epoch": 7.814529065770377,
      "grad_norm": 13.472325325012207,
      "learning_rate": 4.3487892445191354e-05,
      "loss": 2.4517,
      "step": 102300
    },
    {
      "epoch": 7.822167901611794,
      "grad_norm": 11.200647354125977,
      "learning_rate": 4.3481526748656845e-05,
      "loss": 2.4073,
      "step": 102400
    },
    {
      "epoch": 7.829806737453212,
      "grad_norm": 11.522992134094238,
      "learning_rate": 4.347516105212233e-05,
      "loss": 2.2905,
      "step": 102500
    },
    {
      "epoch": 7.83744557329463,
      "grad_norm": 12.593748092651367,
      "learning_rate": 4.346879535558781e-05,
      "loss": 2.4491,
      "step": 102600
    },
    {
      "epoch": 7.845084409136048,
      "grad_norm": 14.361520767211914,
      "learning_rate": 4.3462429659053295e-05,
      "loss": 2.4705,
      "step": 102700
    },
    {
      "epoch": 7.852723244977465,
      "grad_norm": 13.731799125671387,
      "learning_rate": 4.345606396251878e-05,
      "loss": 2.3201,
      "step": 102800
    },
    {
      "epoch": 7.860362080818883,
      "grad_norm": 12.78909683227539,
      "learning_rate": 4.344969826598427e-05,
      "loss": 2.3205,
      "step": 102900
    },
    {
      "epoch": 7.868000916660301,
      "grad_norm": 10.855985641479492,
      "learning_rate": 4.344333256944975e-05,
      "loss": 2.3703,
      "step": 103000
    },
    {
      "epoch": 7.875639752501719,
      "grad_norm": 12.352802276611328,
      "learning_rate": 4.3436966872915236e-05,
      "loss": 2.3559,
      "step": 103100
    },
    {
      "epoch": 7.883278588343137,
      "grad_norm": 9.348888397216797,
      "learning_rate": 4.343060117638072e-05,
      "loss": 2.4453,
      "step": 103200
    },
    {
      "epoch": 7.890917424184554,
      "grad_norm": 11.412617683410645,
      "learning_rate": 4.342423547984621e-05,
      "loss": 2.3778,
      "step": 103300
    },
    {
      "epoch": 7.8985562600259716,
      "grad_norm": 11.915925025939941,
      "learning_rate": 4.3417869783311694e-05,
      "loss": 2.4806,
      "step": 103400
    },
    {
      "epoch": 7.90619509586739,
      "grad_norm": 13.507403373718262,
      "learning_rate": 4.341150408677718e-05,
      "loss": 2.4294,
      "step": 103500
    },
    {
      "epoch": 7.913833931708807,
      "grad_norm": 12.506174087524414,
      "learning_rate": 4.340513839024266e-05,
      "loss": 2.3995,
      "step": 103600
    },
    {
      "epoch": 7.921472767550226,
      "grad_norm": 11.816146850585938,
      "learning_rate": 4.3398772693708144e-05,
      "loss": 2.4375,
      "step": 103700
    },
    {
      "epoch": 7.929111603391643,
      "grad_norm": 14.709593772888184,
      "learning_rate": 4.3392406997173635e-05,
      "loss": 2.3607,
      "step": 103800
    },
    {
      "epoch": 7.936750439233061,
      "grad_norm": 12.242431640625,
      "learning_rate": 4.338604130063912e-05,
      "loss": 2.3175,
      "step": 103900
    },
    {
      "epoch": 7.944389275074479,
      "grad_norm": 12.379432678222656,
      "learning_rate": 4.33796756041046e-05,
      "loss": 2.4195,
      "step": 104000
    },
    {
      "epoch": 7.952028110915896,
      "grad_norm": 12.651741027832031,
      "learning_rate": 4.3373309907570085e-05,
      "loss": 2.3978,
      "step": 104100
    },
    {
      "epoch": 7.959666946757315,
      "grad_norm": 13.486339569091797,
      "learning_rate": 4.3366944211035576e-05,
      "loss": 2.4808,
      "step": 104200
    },
    {
      "epoch": 7.967305782598732,
      "grad_norm": 15.431876182556152,
      "learning_rate": 4.336057851450106e-05,
      "loss": 2.3977,
      "step": 104300
    },
    {
      "epoch": 7.97494461844015,
      "grad_norm": 14.262152671813965,
      "learning_rate": 4.335421281796654e-05,
      "loss": 2.4447,
      "step": 104400
    },
    {
      "epoch": 7.982583454281567,
      "grad_norm": 10.784244537353516,
      "learning_rate": 4.3347847121432026e-05,
      "loss": 2.38,
      "step": 104500
    },
    {
      "epoch": 7.990222290122985,
      "grad_norm": 12.182097434997559,
      "learning_rate": 4.334148142489751e-05,
      "loss": 2.4188,
      "step": 104600
    },
    {
      "epoch": 7.997861125964403,
      "grad_norm": 11.004439353942871,
      "learning_rate": 4.3335115728363e-05,
      "loss": 2.3911,
      "step": 104700
    },
    {
      "epoch": 8.0,
      "eval_loss": 2.280926465988159,
      "eval_runtime": 1.6424,
      "eval_samples_per_second": 420.124,
      "eval_steps_per_second": 420.124,
      "step": 104728
    },
    {
      "epoch": 8.0,
      "eval_loss": 2.188246011734009,
      "eval_runtime": 30.2412,
      "eval_samples_per_second": 432.886,
      "eval_steps_per_second": 432.886,
      "step": 104728
    },
    {
      "epoch": 8.005499961805821,
      "grad_norm": 13.013114929199219,
      "learning_rate": 4.3328750031828484e-05,
      "loss": 2.396,
      "step": 104800
    },
    {
      "epoch": 8.013138797647239,
      "grad_norm": 10.859971046447754,
      "learning_rate": 4.332238433529397e-05,
      "loss": 2.4001,
      "step": 104900
    },
    {
      "epoch": 8.020777633488656,
      "grad_norm": 9.740642547607422,
      "learning_rate": 4.331601863875945e-05,
      "loss": 2.3351,
      "step": 105000
    },
    {
      "epoch": 8.028416469330073,
      "grad_norm": 12.235997200012207,
      "learning_rate": 4.330965294222494e-05,
      "loss": 2.3484,
      "step": 105100
    },
    {
      "epoch": 8.036055305171493,
      "grad_norm": 11.539746284484863,
      "learning_rate": 4.3303287245690424e-05,
      "loss": 2.3717,
      "step": 105200
    },
    {
      "epoch": 8.04369414101291,
      "grad_norm": 10.764915466308594,
      "learning_rate": 4.329692154915591e-05,
      "loss": 2.4189,
      "step": 105300
    },
    {
      "epoch": 8.051332976854328,
      "grad_norm": 9.17898941040039,
      "learning_rate": 4.32905558526214e-05,
      "loss": 2.3749,
      "step": 105400
    },
    {
      "epoch": 8.058971812695745,
      "grad_norm": 11.491275787353516,
      "learning_rate": 4.328419015608688e-05,
      "loss": 2.424,
      "step": 105500
    },
    {
      "epoch": 8.066610648537162,
      "grad_norm": 8.338994979858398,
      "learning_rate": 4.3277824459552365e-05,
      "loss": 2.3604,
      "step": 105600
    },
    {
      "epoch": 8.07424948437858,
      "grad_norm": 13.599175453186035,
      "learning_rate": 4.3271458763017856e-05,
      "loss": 2.4224,
      "step": 105700
    },
    {
      "epoch": 8.08188832022,
      "grad_norm": 12.118972778320312,
      "learning_rate": 4.326509306648334e-05,
      "loss": 2.3639,
      "step": 105800
    },
    {
      "epoch": 8.089527156061417,
      "grad_norm": 11.791149139404297,
      "learning_rate": 4.325872736994882e-05,
      "loss": 2.3594,
      "step": 105900
    },
    {
      "epoch": 8.097165991902834,
      "grad_norm": 14.846639633178711,
      "learning_rate": 4.3252361673414306e-05,
      "loss": 2.3662,
      "step": 106000
    },
    {
      "epoch": 8.104804827744251,
      "grad_norm": 12.389458656311035,
      "learning_rate": 4.32459959768798e-05,
      "loss": 2.411,
      "step": 106100
    },
    {
      "epoch": 8.112443663585669,
      "grad_norm": 10.219839096069336,
      "learning_rate": 4.323963028034528e-05,
      "loss": 2.3192,
      "step": 106200
    },
    {
      "epoch": 8.120082499427088,
      "grad_norm": 15.154587745666504,
      "learning_rate": 4.3233264583810764e-05,
      "loss": 2.3602,
      "step": 106300
    },
    {
      "epoch": 8.127721335268506,
      "grad_norm": 12.623950004577637,
      "learning_rate": 4.322689888727625e-05,
      "loss": 2.3103,
      "step": 106400
    },
    {
      "epoch": 8.135360171109923,
      "grad_norm": 15.383645057678223,
      "learning_rate": 4.322053319074174e-05,
      "loss": 2.2846,
      "step": 106500
    },
    {
      "epoch": 8.14299900695134,
      "grad_norm": 12.824359893798828,
      "learning_rate": 4.321416749420722e-05,
      "loss": 2.2546,
      "step": 106600
    },
    {
      "epoch": 8.150637842792758,
      "grad_norm": 12.167330741882324,
      "learning_rate": 4.3207801797672705e-05,
      "loss": 2.4567,
      "step": 106700
    },
    {
      "epoch": 8.158276678634175,
      "grad_norm": 12.18514347076416,
      "learning_rate": 4.320143610113819e-05,
      "loss": 2.423,
      "step": 106800
    },
    {
      "epoch": 8.165915514475595,
      "grad_norm": 13.184890747070312,
      "learning_rate": 4.319507040460367e-05,
      "loss": 2.3487,
      "step": 106900
    },
    {
      "epoch": 8.173554350317012,
      "grad_norm": 15.657023429870605,
      "learning_rate": 4.318870470806916e-05,
      "loss": 2.4603,
      "step": 107000
    },
    {
      "epoch": 8.18119318615843,
      "grad_norm": 13.151642799377441,
      "learning_rate": 4.3182339011534646e-05,
      "loss": 2.348,
      "step": 107100
    },
    {
      "epoch": 8.188832021999847,
      "grad_norm": 18.348190307617188,
      "learning_rate": 4.317597331500013e-05,
      "loss": 2.3264,
      "step": 107200
    },
    {
      "epoch": 8.196470857841264,
      "grad_norm": 10.770744323730469,
      "learning_rate": 4.316960761846561e-05,
      "loss": 2.3946,
      "step": 107300
    },
    {
      "epoch": 8.204109693682684,
      "grad_norm": 15.158049583435059,
      "learning_rate": 4.31632419219311e-05,
      "loss": 2.3888,
      "step": 107400
    },
    {
      "epoch": 8.211748529524101,
      "grad_norm": 12.727082252502441,
      "learning_rate": 4.3156876225396586e-05,
      "loss": 2.3701,
      "step": 107500
    },
    {
      "epoch": 8.219387365365519,
      "grad_norm": 11.880555152893066,
      "learning_rate": 4.315051052886207e-05,
      "loss": 2.4071,
      "step": 107600
    },
    {
      "epoch": 8.227026201206936,
      "grad_norm": 14.244477272033691,
      "learning_rate": 4.3144144832327554e-05,
      "loss": 2.3905,
      "step": 107700
    },
    {
      "epoch": 8.234665037048353,
      "grad_norm": 19.34425926208496,
      "learning_rate": 4.313777913579304e-05,
      "loss": 2.3652,
      "step": 107800
    },
    {
      "epoch": 8.24230387288977,
      "grad_norm": 14.148510932922363,
      "learning_rate": 4.313141343925853e-05,
      "loss": 2.3399,
      "step": 107900
    },
    {
      "epoch": 8.24994270873119,
      "grad_norm": 10.469809532165527,
      "learning_rate": 4.312504774272401e-05,
      "loss": 2.4186,
      "step": 108000
    },
    {
      "epoch": 8.257581544572608,
      "grad_norm": 13.26554012298584,
      "learning_rate": 4.3118682046189494e-05,
      "loss": 2.3419,
      "step": 108100
    },
    {
      "epoch": 8.265220380414025,
      "grad_norm": 12.12927532196045,
      "learning_rate": 4.311231634965498e-05,
      "loss": 2.3017,
      "step": 108200
    },
    {
      "epoch": 8.272859216255442,
      "grad_norm": 14.366251945495605,
      "learning_rate": 4.310595065312046e-05,
      "loss": 2.3076,
      "step": 108300
    },
    {
      "epoch": 8.28049805209686,
      "grad_norm": 12.691263198852539,
      "learning_rate": 4.309958495658595e-05,
      "loss": 2.4108,
      "step": 108400
    },
    {
      "epoch": 8.288136887938279,
      "grad_norm": 14.799612998962402,
      "learning_rate": 4.3093219260051435e-05,
      "loss": 2.3879,
      "step": 108500
    },
    {
      "epoch": 8.295775723779697,
      "grad_norm": 13.661195755004883,
      "learning_rate": 4.308685356351692e-05,
      "loss": 2.4291,
      "step": 108600
    },
    {
      "epoch": 8.303414559621114,
      "grad_norm": 16.545995712280273,
      "learning_rate": 4.30804878669824e-05,
      "loss": 2.3086,
      "step": 108700
    },
    {
      "epoch": 8.311053395462531,
      "grad_norm": 10.506036758422852,
      "learning_rate": 4.307412217044789e-05,
      "loss": 2.3611,
      "step": 108800
    },
    {
      "epoch": 8.318692231303949,
      "grad_norm": 13.64974308013916,
      "learning_rate": 4.3067756473913376e-05,
      "loss": 2.4686,
      "step": 108900
    },
    {
      "epoch": 8.326331067145366,
      "grad_norm": 12.067529678344727,
      "learning_rate": 4.306139077737886e-05,
      "loss": 2.4493,
      "step": 109000
    },
    {
      "epoch": 8.333969902986786,
      "grad_norm": 12.860563278198242,
      "learning_rate": 4.3055025080844343e-05,
      "loss": 2.4058,
      "step": 109100
    },
    {
      "epoch": 8.341608738828203,
      "grad_norm": 14.36697769165039,
      "learning_rate": 4.3048659384309834e-05,
      "loss": 2.3755,
      "step": 109200
    },
    {
      "epoch": 8.34924757466962,
      "grad_norm": 12.709248542785645,
      "learning_rate": 4.304229368777532e-05,
      "loss": 2.3087,
      "step": 109300
    },
    {
      "epoch": 8.356886410511038,
      "grad_norm": 11.238181114196777,
      "learning_rate": 4.303592799124081e-05,
      "loss": 2.3979,
      "step": 109400
    },
    {
      "epoch": 8.364525246352455,
      "grad_norm": 16.325618743896484,
      "learning_rate": 4.302956229470629e-05,
      "loss": 2.296,
      "step": 109500
    },
    {
      "epoch": 8.372164082193873,
      "grad_norm": 12.647509574890137,
      "learning_rate": 4.3023196598171775e-05,
      "loss": 2.3809,
      "step": 109600
    },
    {
      "epoch": 8.379802918035292,
      "grad_norm": 12.953984260559082,
      "learning_rate": 4.3016830901637265e-05,
      "loss": 2.4541,
      "step": 109700
    },
    {
      "epoch": 8.38744175387671,
      "grad_norm": 11.252396583557129,
      "learning_rate": 4.301046520510275e-05,
      "loss": 2.3641,
      "step": 109800
    },
    {
      "epoch": 8.395080589718127,
      "grad_norm": 16.151649475097656,
      "learning_rate": 4.300409950856823e-05,
      "loss": 2.3102,
      "step": 109900
    },
    {
      "epoch": 8.402719425559544,
      "grad_norm": 12.929667472839355,
      "learning_rate": 4.2997733812033716e-05,
      "loss": 2.3245,
      "step": 110000
    },
    {
      "epoch": 8.410358261400962,
      "grad_norm": 10.394039154052734,
      "learning_rate": 4.29913681154992e-05,
      "loss": 2.4308,
      "step": 110100
    },
    {
      "epoch": 8.417997097242381,
      "grad_norm": 13.798338890075684,
      "learning_rate": 4.298500241896469e-05,
      "loss": 2.361,
      "step": 110200
    },
    {
      "epoch": 8.425635933083798,
      "grad_norm": 8.119094848632812,
      "learning_rate": 4.297863672243017e-05,
      "loss": 2.4037,
      "step": 110300
    },
    {
      "epoch": 8.433274768925216,
      "grad_norm": 11.518197059631348,
      "learning_rate": 4.2972271025895656e-05,
      "loss": 2.3972,
      "step": 110400
    },
    {
      "epoch": 8.440913604766633,
      "grad_norm": 10.670292854309082,
      "learning_rate": 4.296590532936114e-05,
      "loss": 2.3759,
      "step": 110500
    },
    {
      "epoch": 8.44855244060805,
      "grad_norm": 12.353582382202148,
      "learning_rate": 4.295953963282663e-05,
      "loss": 2.4219,
      "step": 110600
    },
    {
      "epoch": 8.45619127644947,
      "grad_norm": 15.62572956085205,
      "learning_rate": 4.2953173936292114e-05,
      "loss": 2.372,
      "step": 110700
    },
    {
      "epoch": 8.463830112290887,
      "grad_norm": 13.098409652709961,
      "learning_rate": 4.29468082397576e-05,
      "loss": 2.3605,
      "step": 110800
    },
    {
      "epoch": 8.471468948132305,
      "grad_norm": 15.296984672546387,
      "learning_rate": 4.294044254322308e-05,
      "loss": 2.4068,
      "step": 110900
    },
    {
      "epoch": 8.479107783973722,
      "grad_norm": 12.478445053100586,
      "learning_rate": 4.2934076846688564e-05,
      "loss": 2.4174,
      "step": 111000
    },
    {
      "epoch": 8.48674661981514,
      "grad_norm": 14.50581169128418,
      "learning_rate": 4.2927711150154055e-05,
      "loss": 2.4308,
      "step": 111100
    },
    {
      "epoch": 8.494385455656557,
      "grad_norm": 13.024621963500977,
      "learning_rate": 4.292134545361954e-05,
      "loss": 2.3389,
      "step": 111200
    },
    {
      "epoch": 8.502024291497976,
      "grad_norm": 12.454377174377441,
      "learning_rate": 4.291497975708502e-05,
      "loss": 2.3295,
      "step": 111300
    },
    {
      "epoch": 8.509663127339394,
      "grad_norm": 13.290544509887695,
      "learning_rate": 4.2908614060550505e-05,
      "loss": 2.3831,
      "step": 111400
    },
    {
      "epoch": 8.517301963180811,
      "grad_norm": 12.451805114746094,
      "learning_rate": 4.290224836401599e-05,
      "loss": 2.3037,
      "step": 111500
    },
    {
      "epoch": 8.524940799022229,
      "grad_norm": 16.23788070678711,
      "learning_rate": 4.289588266748148e-05,
      "loss": 2.3881,
      "step": 111600
    },
    {
      "epoch": 8.532579634863646,
      "grad_norm": 11.066713333129883,
      "learning_rate": 4.288951697094696e-05,
      "loss": 2.2991,
      "step": 111700
    },
    {
      "epoch": 8.540218470705064,
      "grad_norm": 16.489120483398438,
      "learning_rate": 4.2883151274412446e-05,
      "loss": 2.3667,
      "step": 111800
    },
    {
      "epoch": 8.547857306546483,
      "grad_norm": 10.259170532226562,
      "learning_rate": 4.287678557787793e-05,
      "loss": 2.3854,
      "step": 111900
    },
    {
      "epoch": 8.5554961423879,
      "grad_norm": 9.543728828430176,
      "learning_rate": 4.287041988134342e-05,
      "loss": 2.3452,
      "step": 112000
    },
    {
      "epoch": 8.563134978229318,
      "grad_norm": 11.68853759765625,
      "learning_rate": 4.2864054184808904e-05,
      "loss": 2.3898,
      "step": 112100
    },
    {
      "epoch": 8.570773814070735,
      "grad_norm": 14.949081420898438,
      "learning_rate": 4.285768848827439e-05,
      "loss": 2.3935,
      "step": 112200
    },
    {
      "epoch": 8.578412649912153,
      "grad_norm": 10.284834861755371,
      "learning_rate": 4.285132279173987e-05,
      "loss": 2.3572,
      "step": 112300
    },
    {
      "epoch": 8.586051485753572,
      "grad_norm": 11.299718856811523,
      "learning_rate": 4.2844957095205354e-05,
      "loss": 2.3653,
      "step": 112400
    },
    {
      "epoch": 8.59369032159499,
      "grad_norm": 11.570976257324219,
      "learning_rate": 4.2838591398670845e-05,
      "loss": 2.2935,
      "step": 112500
    },
    {
      "epoch": 8.601329157436407,
      "grad_norm": 11.122565269470215,
      "learning_rate": 4.283222570213633e-05,
      "loss": 2.3836,
      "step": 112600
    },
    {
      "epoch": 8.608967993277824,
      "grad_norm": 14.706307411193848,
      "learning_rate": 4.282586000560181e-05,
      "loss": 2.2901,
      "step": 112700
    },
    {
      "epoch": 8.616606829119242,
      "grad_norm": 16.36821174621582,
      "learning_rate": 4.2819494309067295e-05,
      "loss": 2.3126,
      "step": 112800
    },
    {
      "epoch": 8.624245664960661,
      "grad_norm": 12.92264175415039,
      "learning_rate": 4.2813128612532786e-05,
      "loss": 2.4607,
      "step": 112900
    },
    {
      "epoch": 8.631884500802078,
      "grad_norm": 9.089835166931152,
      "learning_rate": 4.280676291599827e-05,
      "loss": 2.2797,
      "step": 113000
    },
    {
      "epoch": 8.639523336643496,
      "grad_norm": 15.723489761352539,
      "learning_rate": 4.280039721946375e-05,
      "loss": 2.3748,
      "step": 113100
    },
    {
      "epoch": 8.647162172484913,
      "grad_norm": 10.769129753112793,
      "learning_rate": 4.279403152292924e-05,
      "loss": 2.3225,
      "step": 113200
    },
    {
      "epoch": 8.65480100832633,
      "grad_norm": 15.944967269897461,
      "learning_rate": 4.2787665826394726e-05,
      "loss": 2.4285,
      "step": 113300
    },
    {
      "epoch": 8.662439844167748,
      "grad_norm": 11.640680313110352,
      "learning_rate": 4.278130012986022e-05,
      "loss": 2.395,
      "step": 113400
    },
    {
      "epoch": 8.670078680009167,
      "grad_norm": 12.842530250549316,
      "learning_rate": 4.27749344333257e-05,
      "loss": 2.4075,
      "step": 113500
    },
    {
      "epoch": 8.677717515850585,
      "grad_norm": 13.75312614440918,
      "learning_rate": 4.2768568736791184e-05,
      "loss": 2.4147,
      "step": 113600
    },
    {
      "epoch": 8.685356351692002,
      "grad_norm": 12.971735000610352,
      "learning_rate": 4.276220304025667e-05,
      "loss": 2.3993,
      "step": 113700
    },
    {
      "epoch": 8.69299518753342,
      "grad_norm": 13.979972839355469,
      "learning_rate": 4.275583734372216e-05,
      "loss": 2.3343,
      "step": 113800
    },
    {
      "epoch": 8.700634023374837,
      "grad_norm": 13.997529029846191,
      "learning_rate": 4.274947164718764e-05,
      "loss": 2.3147,
      "step": 113900
    },
    {
      "epoch": 8.708272859216255,
      "grad_norm": 12.340363502502441,
      "learning_rate": 4.2743105950653125e-05,
      "loss": 2.4807,
      "step": 114000
    },
    {
      "epoch": 8.715911695057674,
      "grad_norm": 12.619585037231445,
      "learning_rate": 4.273674025411861e-05,
      "loss": 2.3777,
      "step": 114100
    },
    {
      "epoch": 8.723550530899091,
      "grad_norm": 12.122126579284668,
      "learning_rate": 4.273037455758409e-05,
      "loss": 2.3333,
      "step": 114200
    },
    {
      "epoch": 8.731189366740509,
      "grad_norm": 14.009674072265625,
      "learning_rate": 4.272400886104958e-05,
      "loss": 2.3986,
      "step": 114300
    },
    {
      "epoch": 8.738828202581926,
      "grad_norm": 13.856466293334961,
      "learning_rate": 4.2717643164515066e-05,
      "loss": 2.3398,
      "step": 114400
    },
    {
      "epoch": 8.746467038423344,
      "grad_norm": 11.83262825012207,
      "learning_rate": 4.271127746798055e-05,
      "loss": 2.3255,
      "step": 114500
    },
    {
      "epoch": 8.754105874264763,
      "grad_norm": 10.815483093261719,
      "learning_rate": 4.270491177144603e-05,
      "loss": 2.3034,
      "step": 114600
    },
    {
      "epoch": 8.76174471010618,
      "grad_norm": 11.216512680053711,
      "learning_rate": 4.2698546074911516e-05,
      "loss": 2.3133,
      "step": 114700
    },
    {
      "epoch": 8.769383545947598,
      "grad_norm": 14.432374954223633,
      "learning_rate": 4.269218037837701e-05,
      "loss": 2.3301,
      "step": 114800
    },
    {
      "epoch": 8.777022381789015,
      "grad_norm": 12.588932991027832,
      "learning_rate": 4.268581468184249e-05,
      "loss": 2.3454,
      "step": 114900
    },
    {
      "epoch": 8.784661217630433,
      "grad_norm": 12.625945091247559,
      "learning_rate": 4.2679448985307974e-05,
      "loss": 2.2429,
      "step": 115000
    },
    {
      "epoch": 8.79230005347185,
      "grad_norm": 12.593923568725586,
      "learning_rate": 4.267308328877346e-05,
      "loss": 2.3232,
      "step": 115100
    },
    {
      "epoch": 8.79993888931327,
      "grad_norm": 12.62327766418457,
      "learning_rate": 4.266671759223895e-05,
      "loss": 2.317,
      "step": 115200
    },
    {
      "epoch": 8.807577725154687,
      "grad_norm": 11.617816925048828,
      "learning_rate": 4.266035189570443e-05,
      "loss": 2.3667,
      "step": 115300
    },
    {
      "epoch": 8.815216560996104,
      "grad_norm": 11.530806541442871,
      "learning_rate": 4.2653986199169915e-05,
      "loss": 2.393,
      "step": 115400
    },
    {
      "epoch": 8.822855396837522,
      "grad_norm": 13.63217544555664,
      "learning_rate": 4.26476205026354e-05,
      "loss": 2.3366,
      "step": 115500
    },
    {
      "epoch": 8.830494232678939,
      "grad_norm": 12.503697395324707,
      "learning_rate": 4.264125480610088e-05,
      "loss": 2.293,
      "step": 115600
    },
    {
      "epoch": 8.838133068520358,
      "grad_norm": 9.456995010375977,
      "learning_rate": 4.263488910956637e-05,
      "loss": 2.3299,
      "step": 115700
    },
    {
      "epoch": 8.845771904361776,
      "grad_norm": 12.09587287902832,
      "learning_rate": 4.2628523413031856e-05,
      "loss": 2.3063,
      "step": 115800
    },
    {
      "epoch": 8.853410740203193,
      "grad_norm": 14.312762260437012,
      "learning_rate": 4.262215771649734e-05,
      "loss": 2.3813,
      "step": 115900
    },
    {
      "epoch": 8.86104957604461,
      "grad_norm": 13.6698579788208,
      "learning_rate": 4.261579201996282e-05,
      "loss": 2.3795,
      "step": 116000
    },
    {
      "epoch": 8.868688411886028,
      "grad_norm": 10.828564643859863,
      "learning_rate": 4.260942632342831e-05,
      "loss": 2.3278,
      "step": 116100
    },
    {
      "epoch": 8.876327247727446,
      "grad_norm": 13.594456672668457,
      "learning_rate": 4.2603060626893796e-05,
      "loss": 2.3111,
      "step": 116200
    },
    {
      "epoch": 8.883966083568865,
      "grad_norm": 14.144133567810059,
      "learning_rate": 4.259669493035928e-05,
      "loss": 2.3338,
      "step": 116300
    },
    {
      "epoch": 8.891604919410282,
      "grad_norm": 11.91364860534668,
      "learning_rate": 4.2590329233824764e-05,
      "loss": 2.3106,
      "step": 116400
    },
    {
      "epoch": 8.8992437552517,
      "grad_norm": 10.7156400680542,
      "learning_rate": 4.258396353729025e-05,
      "loss": 2.3564,
      "step": 116500
    },
    {
      "epoch": 8.906882591093117,
      "grad_norm": 15.278196334838867,
      "learning_rate": 4.257759784075574e-05,
      "loss": 2.3673,
      "step": 116600
    },
    {
      "epoch": 8.914521426934535,
      "grad_norm": 11.458699226379395,
      "learning_rate": 4.257123214422122e-05,
      "loss": 2.3533,
      "step": 116700
    },
    {
      "epoch": 8.922160262775954,
      "grad_norm": 11.273475646972656,
      "learning_rate": 4.2564866447686704e-05,
      "loss": 2.27,
      "step": 116800
    },
    {
      "epoch": 8.929799098617371,
      "grad_norm": 12.315248489379883,
      "learning_rate": 4.2558500751152195e-05,
      "loss": 2.344,
      "step": 116900
    },
    {
      "epoch": 8.937437934458789,
      "grad_norm": 13.82740592956543,
      "learning_rate": 4.255213505461768e-05,
      "loss": 2.2948,
      "step": 117000
    },
    {
      "epoch": 8.945076770300206,
      "grad_norm": 11.190447807312012,
      "learning_rate": 4.254576935808316e-05,
      "loss": 2.3474,
      "step": 117100
    },
    {
      "epoch": 8.952715606141624,
      "grad_norm": 10.481279373168945,
      "learning_rate": 4.253940366154865e-05,
      "loss": 2.2507,
      "step": 117200
    },
    {
      "epoch": 8.960354441983041,
      "grad_norm": 13.663717269897461,
      "learning_rate": 4.2533037965014136e-05,
      "loss": 2.3413,
      "step": 117300
    },
    {
      "epoch": 8.96799327782446,
      "grad_norm": 8.402987480163574,
      "learning_rate": 4.252667226847962e-05,
      "loss": 2.2472,
      "step": 117400
    },
    {
      "epoch": 8.975632113665878,
      "grad_norm": 9.9199800491333,
      "learning_rate": 4.252030657194511e-05,
      "loss": 2.3404,
      "step": 117500
    },
    {
      "epoch": 8.983270949507295,
      "grad_norm": 15.111748695373535,
      "learning_rate": 4.251394087541059e-05,
      "loss": 2.3167,
      "step": 117600
    },
    {
      "epoch": 8.990909785348713,
      "grad_norm": 14.71917724609375,
      "learning_rate": 4.250757517887608e-05,
      "loss": 2.3492,
      "step": 117700
    },
    {
      "epoch": 8.99854862119013,
      "grad_norm": 13.028777122497559,
      "learning_rate": 4.250120948234156e-05,
      "loss": 2.3515,
      "step": 117800
    },
    {
      "epoch": 9.0,
      "eval_loss": 2.2356374263763428,
      "eval_runtime": 1.6062,
      "eval_samples_per_second": 429.596,
      "eval_steps_per_second": 429.596,
      "step": 117819
    },
    {
      "epoch": 9.0,
      "eval_loss": 2.1322267055511475,
      "eval_runtime": 30.4238,
      "eval_samples_per_second": 430.287,
      "eval_steps_per_second": 430.287,
      "step": 117819
    },
    {
      "epoch": 9.00618745703155,
      "grad_norm": 12.020628929138184,
      "learning_rate": 4.2494843785807044e-05,
      "loss": 2.2556,
      "step": 117900
    },
    {
      "epoch": 9.013826292872967,
      "grad_norm": 11.454495429992676,
      "learning_rate": 4.2488478089272534e-05,
      "loss": 2.3288,
      "step": 118000
    },
    {
      "epoch": 9.021465128714384,
      "grad_norm": 14.809259414672852,
      "learning_rate": 4.248211239273802e-05,
      "loss": 2.2311,
      "step": 118100
    },
    {
      "epoch": 9.029103964555802,
      "grad_norm": 14.122467994689941,
      "learning_rate": 4.24757466962035e-05,
      "loss": 2.3286,
      "step": 118200
    },
    {
      "epoch": 9.036742800397219,
      "grad_norm": 12.47260570526123,
      "learning_rate": 4.2469380999668985e-05,
      "loss": 2.2581,
      "step": 118300
    },
    {
      "epoch": 9.044381636238636,
      "grad_norm": 10.257587432861328,
      "learning_rate": 4.2463015303134475e-05,
      "loss": 2.2522,
      "step": 118400
    },
    {
      "epoch": 9.052020472080056,
      "grad_norm": 13.398682594299316,
      "learning_rate": 4.245664960659996e-05,
      "loss": 2.3438,
      "step": 118500
    },
    {
      "epoch": 9.059659307921473,
      "grad_norm": 11.57425308227539,
      "learning_rate": 4.245028391006544e-05,
      "loss": 2.3103,
      "step": 118600
    },
    {
      "epoch": 9.06729814376289,
      "grad_norm": 13.969311714172363,
      "learning_rate": 4.2443918213530926e-05,
      "loss": 2.3573,
      "step": 118700
    },
    {
      "epoch": 9.074936979604308,
      "grad_norm": 10.446962356567383,
      "learning_rate": 4.243755251699641e-05,
      "loss": 2.2558,
      "step": 118800
    },
    {
      "epoch": 9.082575815445725,
      "grad_norm": 11.374813079833984,
      "learning_rate": 4.24311868204619e-05,
      "loss": 2.4498,
      "step": 118900
    },
    {
      "epoch": 9.090214651287145,
      "grad_norm": 9.408309936523438,
      "learning_rate": 4.242482112392738e-05,
      "loss": 2.2426,
      "step": 119000
    },
    {
      "epoch": 9.097853487128562,
      "grad_norm": 13.458011627197266,
      "learning_rate": 4.2418455427392866e-05,
      "loss": 2.3356,
      "step": 119100
    },
    {
      "epoch": 9.10549232296998,
      "grad_norm": 11.533909797668457,
      "learning_rate": 4.241208973085835e-05,
      "loss": 2.3281,
      "step": 119200
    },
    {
      "epoch": 9.113131158811397,
      "grad_norm": 11.610344886779785,
      "learning_rate": 4.240572403432384e-05,
      "loss": 2.3115,
      "step": 119300
    },
    {
      "epoch": 9.120769994652814,
      "grad_norm": 12.07959270477295,
      "learning_rate": 4.2399358337789324e-05,
      "loss": 2.3588,
      "step": 119400
    },
    {
      "epoch": 9.128408830494232,
      "grad_norm": 13.440061569213867,
      "learning_rate": 4.239299264125481e-05,
      "loss": 2.2745,
      "step": 119500
    },
    {
      "epoch": 9.136047666335651,
      "grad_norm": 12.670297622680664,
      "learning_rate": 4.238662694472029e-05,
      "loss": 2.2695,
      "step": 119600
    },
    {
      "epoch": 9.143686502177069,
      "grad_norm": 15.411130905151367,
      "learning_rate": 4.2380261248185774e-05,
      "loss": 2.2916,
      "step": 119700
    },
    {
      "epoch": 9.151325338018486,
      "grad_norm": 11.108014106750488,
      "learning_rate": 4.2373895551651265e-05,
      "loss": 2.2359,
      "step": 119800
    },
    {
      "epoch": 9.158964173859903,
      "grad_norm": 9.548160552978516,
      "learning_rate": 4.236752985511675e-05,
      "loss": 2.3698,
      "step": 119900
    },
    {
      "epoch": 9.166603009701321,
      "grad_norm": 12.486522674560547,
      "learning_rate": 4.236116415858223e-05,
      "loss": 2.4441,
      "step": 120000
    },
    {
      "epoch": 9.17424184554274,
      "grad_norm": 12.15451431274414,
      "learning_rate": 4.2354798462047715e-05,
      "loss": 2.2909,
      "step": 120100
    },
    {
      "epoch": 9.181880681384158,
      "grad_norm": 13.634532928466797,
      "learning_rate": 4.23484327655132e-05,
      "loss": 2.2065,
      "step": 120200
    },
    {
      "epoch": 9.189519517225575,
      "grad_norm": 11.939863204956055,
      "learning_rate": 4.234206706897869e-05,
      "loss": 2.3125,
      "step": 120300
    },
    {
      "epoch": 9.197158353066992,
      "grad_norm": 17.462989807128906,
      "learning_rate": 4.233570137244417e-05,
      "loss": 2.3924,
      "step": 120400
    },
    {
      "epoch": 9.20479718890841,
      "grad_norm": 13.256092071533203,
      "learning_rate": 4.2329335675909656e-05,
      "loss": 2.3057,
      "step": 120500
    },
    {
      "epoch": 9.212436024749827,
      "grad_norm": 11.772704124450684,
      "learning_rate": 4.232296997937515e-05,
      "loss": 2.3869,
      "step": 120600
    },
    {
      "epoch": 9.220074860591247,
      "grad_norm": 11.0457763671875,
      "learning_rate": 4.231660428284063e-05,
      "loss": 2.3199,
      "step": 120700
    },
    {
      "epoch": 9.227713696432664,
      "grad_norm": 13.97637939453125,
      "learning_rate": 4.2310238586306114e-05,
      "loss": 2.2654,
      "step": 120800
    },
    {
      "epoch": 9.235352532274081,
      "grad_norm": 11.919353485107422,
      "learning_rate": 4.2303872889771604e-05,
      "loss": 2.2374,
      "step": 120900
    },
    {
      "epoch": 9.242991368115499,
      "grad_norm": 11.715339660644531,
      "learning_rate": 4.229750719323709e-05,
      "loss": 2.3376,
      "step": 121000
    },
    {
      "epoch": 9.250630203956916,
      "grad_norm": 14.52673625946045,
      "learning_rate": 4.229114149670257e-05,
      "loss": 2.2906,
      "step": 121100
    },
    {
      "epoch": 9.258269039798336,
      "grad_norm": 9.395730972290039,
      "learning_rate": 4.228477580016806e-05,
      "loss": 2.3036,
      "step": 121200
    },
    {
      "epoch": 9.265907875639753,
      "grad_norm": 11.917323112487793,
      "learning_rate": 4.2278410103633545e-05,
      "loss": 2.3456,
      "step": 121300
    },
    {
      "epoch": 9.27354671148117,
      "grad_norm": 12.613710403442383,
      "learning_rate": 4.227204440709903e-05,
      "loss": 2.3262,
      "step": 121400
    },
    {
      "epoch": 9.281185547322588,
      "grad_norm": 12.026589393615723,
      "learning_rate": 4.226567871056451e-05,
      "loss": 2.3094,
      "step": 121500
    },
    {
      "epoch": 9.288824383164005,
      "grad_norm": 10.730076789855957,
      "learning_rate": 4.225931301403e-05,
      "loss": 2.3557,
      "step": 121600
    },
    {
      "epoch": 9.296463219005423,
      "grad_norm": 12.521946907043457,
      "learning_rate": 4.2252947317495486e-05,
      "loss": 2.2536,
      "step": 121700
    },
    {
      "epoch": 9.304102054846842,
      "grad_norm": 11.561897277832031,
      "learning_rate": 4.224658162096097e-05,
      "loss": 2.3497,
      "step": 121800
    },
    {
      "epoch": 9.31174089068826,
      "grad_norm": 15.17620849609375,
      "learning_rate": 4.224021592442645e-05,
      "loss": 2.3561,
      "step": 121900
    },
    {
      "epoch": 9.319379726529677,
      "grad_norm": 8.642667770385742,
      "learning_rate": 4.2233850227891936e-05,
      "loss": 2.2291,
      "step": 122000
    },
    {
      "epoch": 9.327018562371094,
      "grad_norm": 13.602900505065918,
      "learning_rate": 4.222748453135743e-05,
      "loss": 2.3319,
      "step": 122100
    },
    {
      "epoch": 9.334657398212512,
      "grad_norm": 11.485854148864746,
      "learning_rate": 4.222111883482291e-05,
      "loss": 2.2572,
      "step": 122200
    },
    {
      "epoch": 9.34229623405393,
      "grad_norm": 14.715786933898926,
      "learning_rate": 4.2214753138288394e-05,
      "loss": 2.3618,
      "step": 122300
    },
    {
      "epoch": 9.349935069895349,
      "grad_norm": 12.487241744995117,
      "learning_rate": 4.220838744175388e-05,
      "loss": 2.4006,
      "step": 122400
    },
    {
      "epoch": 9.357573905736766,
      "grad_norm": 12.722503662109375,
      "learning_rate": 4.220202174521937e-05,
      "loss": 2.3582,
      "step": 122500
    },
    {
      "epoch": 9.365212741578183,
      "grad_norm": 13.500682830810547,
      "learning_rate": 4.219565604868485e-05,
      "loss": 2.4308,
      "step": 122600
    },
    {
      "epoch": 9.3728515774196,
      "grad_norm": 11.19852066040039,
      "learning_rate": 4.2189290352150335e-05,
      "loss": 2.3367,
      "step": 122700
    },
    {
      "epoch": 9.380490413261018,
      "grad_norm": 13.628344535827637,
      "learning_rate": 4.218292465561582e-05,
      "loss": 2.3636,
      "step": 122800
    },
    {
      "epoch": 9.388129249102438,
      "grad_norm": 12.808080673217773,
      "learning_rate": 4.21765589590813e-05,
      "loss": 2.4046,
      "step": 122900
    },
    {
      "epoch": 9.395768084943855,
      "grad_norm": 13.478802680969238,
      "learning_rate": 4.217019326254679e-05,
      "loss": 2.2742,
      "step": 123000
    },
    {
      "epoch": 9.403406920785272,
      "grad_norm": 13.716023445129395,
      "learning_rate": 4.2163827566012276e-05,
      "loss": 2.3345,
      "step": 123100
    },
    {
      "epoch": 9.41104575662669,
      "grad_norm": 8.588677406311035,
      "learning_rate": 4.215746186947776e-05,
      "loss": 2.3135,
      "step": 123200
    },
    {
      "epoch": 9.418684592468107,
      "grad_norm": 12.526897430419922,
      "learning_rate": 4.215109617294324e-05,
      "loss": 2.3118,
      "step": 123300
    },
    {
      "epoch": 9.426323428309527,
      "grad_norm": 12.791281700134277,
      "learning_rate": 4.2144730476408726e-05,
      "loss": 2.3018,
      "step": 123400
    },
    {
      "epoch": 9.433962264150944,
      "grad_norm": 12.937246322631836,
      "learning_rate": 4.213836477987422e-05,
      "loss": 2.4101,
      "step": 123500
    },
    {
      "epoch": 9.441601099992361,
      "grad_norm": 12.626323699951172,
      "learning_rate": 4.21319990833397e-05,
      "loss": 2.2704,
      "step": 123600
    },
    {
      "epoch": 9.449239935833779,
      "grad_norm": 13.870661735534668,
      "learning_rate": 4.2125633386805184e-05,
      "loss": 2.2493,
      "step": 123700
    },
    {
      "epoch": 9.456878771675196,
      "grad_norm": 13.954987525939941,
      "learning_rate": 4.211926769027067e-05,
      "loss": 2.2837,
      "step": 123800
    },
    {
      "epoch": 9.464517607516614,
      "grad_norm": 10.06058120727539,
      "learning_rate": 4.211290199373616e-05,
      "loss": 2.3496,
      "step": 123900
    },
    {
      "epoch": 9.472156443358033,
      "grad_norm": 16.673233032226562,
      "learning_rate": 4.210653629720164e-05,
      "loss": 2.3532,
      "step": 124000
    },
    {
      "epoch": 9.47979527919945,
      "grad_norm": 13.983983039855957,
      "learning_rate": 4.2100170600667125e-05,
      "loss": 2.2987,
      "step": 124100
    },
    {
      "epoch": 9.487434115040868,
      "grad_norm": 11.4613676071167,
      "learning_rate": 4.209380490413261e-05,
      "loss": 2.3853,
      "step": 124200
    },
    {
      "epoch": 9.495072950882285,
      "grad_norm": 19.074026107788086,
      "learning_rate": 4.208743920759809e-05,
      "loss": 2.3118,
      "step": 124300
    },
    {
      "epoch": 9.502711786723703,
      "grad_norm": 14.165992736816406,
      "learning_rate": 4.208107351106358e-05,
      "loss": 2.2974,
      "step": 124400
    },
    {
      "epoch": 9.51035062256512,
      "grad_norm": 12.323873519897461,
      "learning_rate": 4.2074707814529066e-05,
      "loss": 2.2962,
      "step": 124500
    },
    {
      "epoch": 9.51798945840654,
      "grad_norm": 13.38670825958252,
      "learning_rate": 4.2068342117994556e-05,
      "loss": 2.2728,
      "step": 124600
    },
    {
      "epoch": 9.525628294247957,
      "grad_norm": 13.784696578979492,
      "learning_rate": 4.206197642146004e-05,
      "loss": 2.3348,
      "step": 124700
    },
    {
      "epoch": 9.533267130089374,
      "grad_norm": 10.274032592773438,
      "learning_rate": 4.205561072492552e-05,
      "loss": 2.3407,
      "step": 124800
    },
    {
      "epoch": 9.540905965930792,
      "grad_norm": 14.9544677734375,
      "learning_rate": 4.204924502839101e-05,
      "loss": 2.2393,
      "step": 124900
    },
    {
      "epoch": 9.54854480177221,
      "grad_norm": 11.96551513671875,
      "learning_rate": 4.20428793318565e-05,
      "loss": 2.3132,
      "step": 125000
    },
    {
      "epoch": 9.556183637613628,
      "grad_norm": 11.234457015991211,
      "learning_rate": 4.203651363532198e-05,
      "loss": 2.3337,
      "step": 125100
    },
    {
      "epoch": 9.563822473455046,
      "grad_norm": 12.807305335998535,
      "learning_rate": 4.2030147938787464e-05,
      "loss": 2.3536,
      "step": 125200
    },
    {
      "epoch": 9.571461309296463,
      "grad_norm": 11.085638046264648,
      "learning_rate": 4.2023782242252954e-05,
      "loss": 2.305,
      "step": 125300
    },
    {
      "epoch": 9.57910014513788,
      "grad_norm": 13.391203880310059,
      "learning_rate": 4.201741654571844e-05,
      "loss": 2.2447,
      "step": 125400
    },
    {
      "epoch": 9.586738980979298,
      "grad_norm": 13.63766098022461,
      "learning_rate": 4.201105084918392e-05,
      "loss": 2.3672,
      "step": 125500
    },
    {
      "epoch": 9.594377816820717,
      "grad_norm": 11.663938522338867,
      "learning_rate": 4.2004685152649405e-05,
      "loss": 2.3089,
      "step": 125600
    },
    {
      "epoch": 9.602016652662135,
      "grad_norm": 11.258644104003906,
      "learning_rate": 4.199831945611489e-05,
      "loss": 2.3763,
      "step": 125700
    },
    {
      "epoch": 9.609655488503552,
      "grad_norm": 14.697426795959473,
      "learning_rate": 4.199195375958038e-05,
      "loss": 2.3348,
      "step": 125800
    },
    {
      "epoch": 9.61729432434497,
      "grad_norm": 11.677689552307129,
      "learning_rate": 4.198558806304586e-05,
      "loss": 2.3152,
      "step": 125900
    },
    {
      "epoch": 9.624933160186387,
      "grad_norm": 12.7584228515625,
      "learning_rate": 4.1979222366511346e-05,
      "loss": 2.1981,
      "step": 126000
    },
    {
      "epoch": 9.632571996027805,
      "grad_norm": 13.448373794555664,
      "learning_rate": 4.197285666997683e-05,
      "loss": 2.2774,
      "step": 126100
    },
    {
      "epoch": 9.640210831869224,
      "grad_norm": 13.09465217590332,
      "learning_rate": 4.196649097344232e-05,
      "loss": 2.3889,
      "step": 126200
    },
    {
      "epoch": 9.647849667710641,
      "grad_norm": 8.803935050964355,
      "learning_rate": 4.19601252769078e-05,
      "loss": 2.3131,
      "step": 126300
    },
    {
      "epoch": 9.655488503552059,
      "grad_norm": 14.643564224243164,
      "learning_rate": 4.195375958037329e-05,
      "loss": 2.3326,
      "step": 126400
    },
    {
      "epoch": 9.663127339393476,
      "grad_norm": 11.002854347229004,
      "learning_rate": 4.194739388383877e-05,
      "loss": 2.3183,
      "step": 126500
    },
    {
      "epoch": 9.670766175234894,
      "grad_norm": 13.270547866821289,
      "learning_rate": 4.1941028187304254e-05,
      "loss": 2.393,
      "step": 126600
    },
    {
      "epoch": 9.678405011076311,
      "grad_norm": 11.79617691040039,
      "learning_rate": 4.1934662490769744e-05,
      "loss": 2.3129,
      "step": 126700
    },
    {
      "epoch": 9.68604384691773,
      "grad_norm": 11.522567749023438,
      "learning_rate": 4.192829679423523e-05,
      "loss": 2.396,
      "step": 126800
    },
    {
      "epoch": 9.693682682759148,
      "grad_norm": 12.825750350952148,
      "learning_rate": 4.192193109770071e-05,
      "loss": 2.3211,
      "step": 126900
    },
    {
      "epoch": 9.701321518600565,
      "grad_norm": 11.185920715332031,
      "learning_rate": 4.1915565401166195e-05,
      "loss": 2.3694,
      "step": 127000
    },
    {
      "epoch": 9.708960354441983,
      "grad_norm": 11.772714614868164,
      "learning_rate": 4.1909199704631685e-05,
      "loss": 2.3187,
      "step": 127100
    },
    {
      "epoch": 9.7165991902834,
      "grad_norm": 13.863236427307129,
      "learning_rate": 4.190283400809717e-05,
      "loss": 2.3148,
      "step": 127200
    },
    {
      "epoch": 9.72423802612482,
      "grad_norm": 15.782785415649414,
      "learning_rate": 4.189646831156265e-05,
      "loss": 2.3355,
      "step": 127300
    },
    {
      "epoch": 9.731876861966237,
      "grad_norm": 14.724888801574707,
      "learning_rate": 4.1890102615028136e-05,
      "loss": 2.3827,
      "step": 127400
    },
    {
      "epoch": 9.739515697807654,
      "grad_norm": 12.193984031677246,
      "learning_rate": 4.188373691849362e-05,
      "loss": 2.267,
      "step": 127500
    },
    {
      "epoch": 9.747154533649072,
      "grad_norm": 11.290922164916992,
      "learning_rate": 4.187737122195911e-05,
      "loss": 2.3456,
      "step": 127600
    },
    {
      "epoch": 9.75479336949049,
      "grad_norm": 12.212319374084473,
      "learning_rate": 4.187100552542459e-05,
      "loss": 2.3023,
      "step": 127700
    },
    {
      "epoch": 9.762432205331907,
      "grad_norm": 11.955190658569336,
      "learning_rate": 4.1864639828890076e-05,
      "loss": 2.1973,
      "step": 127800
    },
    {
      "epoch": 9.770071041173326,
      "grad_norm": 14.021456718444824,
      "learning_rate": 4.185827413235556e-05,
      "loss": 2.3543,
      "step": 127900
    },
    {
      "epoch": 9.777709877014743,
      "grad_norm": 15.159181594848633,
      "learning_rate": 4.185190843582105e-05,
      "loss": 2.3797,
      "step": 128000
    },
    {
      "epoch": 9.78534871285616,
      "grad_norm": 14.63595962524414,
      "learning_rate": 4.1845542739286534e-05,
      "loss": 2.363,
      "step": 128100
    },
    {
      "epoch": 9.792987548697578,
      "grad_norm": 12.652132987976074,
      "learning_rate": 4.183917704275202e-05,
      "loss": 2.2709,
      "step": 128200
    },
    {
      "epoch": 9.800626384538996,
      "grad_norm": 9.36169719696045,
      "learning_rate": 4.18328113462175e-05,
      "loss": 2.3111,
      "step": 128300
    },
    {
      "epoch": 9.808265220380415,
      "grad_norm": 14.322298049926758,
      "learning_rate": 4.182644564968299e-05,
      "loss": 2.3238,
      "step": 128400
    },
    {
      "epoch": 9.815904056221832,
      "grad_norm": 11.494815826416016,
      "learning_rate": 4.1820079953148475e-05,
      "loss": 2.304,
      "step": 128500
    },
    {
      "epoch": 9.82354289206325,
      "grad_norm": 19.52973175048828,
      "learning_rate": 4.181371425661396e-05,
      "loss": 2.3027,
      "step": 128600
    },
    {
      "epoch": 9.831181727904667,
      "grad_norm": 13.112170219421387,
      "learning_rate": 4.180734856007945e-05,
      "loss": 2.3221,
      "step": 128700
    },
    {
      "epoch": 9.838820563746085,
      "grad_norm": 13.737753868103027,
      "learning_rate": 4.180098286354493e-05,
      "loss": 2.2889,
      "step": 128800
    },
    {
      "epoch": 9.846459399587502,
      "grad_norm": 9.782840728759766,
      "learning_rate": 4.1794617167010416e-05,
      "loss": 2.3155,
      "step": 128900
    },
    {
      "epoch": 9.854098235428921,
      "grad_norm": 14.252516746520996,
      "learning_rate": 4.1788251470475906e-05,
      "loss": 2.3078,
      "step": 129000
    },
    {
      "epoch": 9.861737071270339,
      "grad_norm": 10.870240211486816,
      "learning_rate": 4.178188577394139e-05,
      "loss": 2.2996,
      "step": 129100
    },
    {
      "epoch": 9.869375907111756,
      "grad_norm": 10.40499210357666,
      "learning_rate": 4.177552007740687e-05,
      "loss": 2.2771,
      "step": 129200
    },
    {
      "epoch": 9.877014742953174,
      "grad_norm": 11.982343673706055,
      "learning_rate": 4.176915438087236e-05,
      "loss": 2.4048,
      "step": 129300
    },
    {
      "epoch": 9.884653578794591,
      "grad_norm": 9.249918937683105,
      "learning_rate": 4.176278868433785e-05,
      "loss": 2.4044,
      "step": 129400
    },
    {
      "epoch": 9.89229241463601,
      "grad_norm": 13.184103965759277,
      "learning_rate": 4.175642298780333e-05,
      "loss": 2.1904,
      "step": 129500
    },
    {
      "epoch": 9.899931250477428,
      "grad_norm": 14.433857917785645,
      "learning_rate": 4.1750057291268814e-05,
      "loss": 2.2965,
      "step": 129600
    },
    {
      "epoch": 9.907570086318845,
      "grad_norm": 12.855159759521484,
      "learning_rate": 4.17436915947343e-05,
      "loss": 2.3422,
      "step": 129700
    },
    {
      "epoch": 9.915208922160263,
      "grad_norm": 10.412591934204102,
      "learning_rate": 4.173732589819978e-05,
      "loss": 2.3687,
      "step": 129800
    },
    {
      "epoch": 9.92284775800168,
      "grad_norm": 11.825763702392578,
      "learning_rate": 4.173096020166527e-05,
      "loss": 2.2923,
      "step": 129900
    },
    {
      "epoch": 9.930486593843098,
      "grad_norm": 11.20813274383545,
      "learning_rate": 4.1724594505130755e-05,
      "loss": 2.4432,
      "step": 130000
    },
    {
      "epoch": 9.938125429684517,
      "grad_norm": 12.821596145629883,
      "learning_rate": 4.171822880859624e-05,
      "loss": 2.3602,
      "step": 130100
    },
    {
      "epoch": 9.945764265525934,
      "grad_norm": 10.944746971130371,
      "learning_rate": 4.171186311206172e-05,
      "loss": 2.2339,
      "step": 130200
    },
    {
      "epoch": 9.953403101367352,
      "grad_norm": 13.615937232971191,
      "learning_rate": 4.170549741552721e-05,
      "loss": 2.1835,
      "step": 130300
    },
    {
      "epoch": 9.961041937208769,
      "grad_norm": 14.199700355529785,
      "learning_rate": 4.1699131718992696e-05,
      "loss": 2.4229,
      "step": 130400
    },
    {
      "epoch": 9.968680773050187,
      "grad_norm": 13.160396575927734,
      "learning_rate": 4.169276602245818e-05,
      "loss": 2.3249,
      "step": 130500
    },
    {
      "epoch": 9.976319608891606,
      "grad_norm": 13.412920951843262,
      "learning_rate": 4.168640032592366e-05,
      "loss": 2.3395,
      "step": 130600
    },
    {
      "epoch": 9.983958444733023,
      "grad_norm": 12.65102767944336,
      "learning_rate": 4.1680034629389146e-05,
      "loss": 2.3025,
      "step": 130700
    },
    {
      "epoch": 9.99159728057444,
      "grad_norm": 10.007410049438477,
      "learning_rate": 4.167366893285464e-05,
      "loss": 2.351,
      "step": 130800
    },
    {
      "epoch": 9.999236116415858,
      "grad_norm": 12.161279678344727,
      "learning_rate": 4.166730323632012e-05,
      "loss": 2.25,
      "step": 130900
    },
    {
      "epoch": 10.0,
      "eval_loss": 2.2176175117492676,
      "eval_runtime": 1.647,
      "eval_samples_per_second": 418.952,
      "eval_steps_per_second": 418.952,
      "step": 130910
    },
    {
      "epoch": 10.0,
      "eval_loss": 2.1035025119781494,
      "eval_runtime": 31.4319,
      "eval_samples_per_second": 416.488,
      "eval_steps_per_second": 416.488,
      "step": 130910
    },
    {
      "epoch": 10.006874952257276,
      "grad_norm": 15.214231491088867,
      "learning_rate": 4.1660937539785604e-05,
      "loss": 2.2361,
      "step": 131000
    },
    {
      "epoch": 10.014513788098693,
      "grad_norm": 13.640024185180664,
      "learning_rate": 4.165457184325109e-05,
      "loss": 2.2385,
      "step": 131100
    },
    {
      "epoch": 10.022152623940112,
      "grad_norm": 11.931138038635254,
      "learning_rate": 4.164820614671657e-05,
      "loss": 2.322,
      "step": 131200
    },
    {
      "epoch": 10.02979145978153,
      "grad_norm": 13.268077850341797,
      "learning_rate": 4.164184045018206e-05,
      "loss": 2.199,
      "step": 131300
    },
    {
      "epoch": 10.037430295622947,
      "grad_norm": 10.092631340026855,
      "learning_rate": 4.1635474753647545e-05,
      "loss": 2.2975,
      "step": 131400
    },
    {
      "epoch": 10.045069131464365,
      "grad_norm": 15.52129077911377,
      "learning_rate": 4.162910905711303e-05,
      "loss": 2.2281,
      "step": 131500
    },
    {
      "epoch": 10.052707967305782,
      "grad_norm": 10.371326446533203,
      "learning_rate": 4.162274336057851e-05,
      "loss": 2.3137,
      "step": 131600
    },
    {
      "epoch": 10.060346803147201,
      "grad_norm": 11.507570266723633,
      "learning_rate": 4.1616377664044e-05,
      "loss": 2.285,
      "step": 131700
    },
    {
      "epoch": 10.067985638988619,
      "grad_norm": 13.551527976989746,
      "learning_rate": 4.1610011967509486e-05,
      "loss": 2.348,
      "step": 131800
    },
    {
      "epoch": 10.075624474830036,
      "grad_norm": 10.906911849975586,
      "learning_rate": 4.160364627097497e-05,
      "loss": 2.2229,
      "step": 131900
    },
    {
      "epoch": 10.083263310671454,
      "grad_norm": 12.196173667907715,
      "learning_rate": 4.159728057444045e-05,
      "loss": 2.2964,
      "step": 132000
    },
    {
      "epoch": 10.090902146512871,
      "grad_norm": 12.263508796691895,
      "learning_rate": 4.159091487790594e-05,
      "loss": 2.2965,
      "step": 132100
    },
    {
      "epoch": 10.098540982354288,
      "grad_norm": 11.52536392211914,
      "learning_rate": 4.158454918137143e-05,
      "loss": 2.3565,
      "step": 132200
    },
    {
      "epoch": 10.106179818195708,
      "grad_norm": 11.204296112060547,
      "learning_rate": 4.157818348483691e-05,
      "loss": 2.2445,
      "step": 132300
    },
    {
      "epoch": 10.113818654037125,
      "grad_norm": 13.124375343322754,
      "learning_rate": 4.15718177883024e-05,
      "loss": 2.3264,
      "step": 132400
    },
    {
      "epoch": 10.121457489878543,
      "grad_norm": 14.43799877166748,
      "learning_rate": 4.1565452091767884e-05,
      "loss": 2.2454,
      "step": 132500
    },
    {
      "epoch": 10.12909632571996,
      "grad_norm": 11.316022872924805,
      "learning_rate": 4.155908639523337e-05,
      "loss": 2.3278,
      "step": 132600
    },
    {
      "epoch": 10.136735161561377,
      "grad_norm": 11.46986198425293,
      "learning_rate": 4.155272069869886e-05,
      "loss": 2.2215,
      "step": 132700
    },
    {
      "epoch": 10.144373997402797,
      "grad_norm": 11.402539253234863,
      "learning_rate": 4.154635500216434e-05,
      "loss": 2.2632,
      "step": 132800
    },
    {
      "epoch": 10.152012833244214,
      "grad_norm": 12.351699829101562,
      "learning_rate": 4.1539989305629825e-05,
      "loss": 2.2699,
      "step": 132900
    },
    {
      "epoch": 10.159651669085632,
      "grad_norm": 13.756522178649902,
      "learning_rate": 4.153362360909531e-05,
      "loss": 2.3291,
      "step": 133000
    },
    {
      "epoch": 10.167290504927049,
      "grad_norm": 11.5017671585083,
      "learning_rate": 4.15272579125608e-05,
      "loss": 2.2791,
      "step": 133100
    },
    {
      "epoch": 10.174929340768466,
      "grad_norm": 12.705045700073242,
      "learning_rate": 4.152089221602628e-05,
      "loss": 2.2728,
      "step": 133200
    },
    {
      "epoch": 10.182568176609884,
      "grad_norm": 11.401490211486816,
      "learning_rate": 4.1514526519491766e-05,
      "loss": 2.2408,
      "step": 133300
    },
    {
      "epoch": 10.190207012451303,
      "grad_norm": 11.266772270202637,
      "learning_rate": 4.150816082295725e-05,
      "loss": 2.2834,
      "step": 133400
    },
    {
      "epoch": 10.19784584829272,
      "grad_norm": 12.794754028320312,
      "learning_rate": 4.150179512642274e-05,
      "loss": 2.2699,
      "step": 133500
    },
    {
      "epoch": 10.205484684134138,
      "grad_norm": 12.156257629394531,
      "learning_rate": 4.149542942988822e-05,
      "loss": 2.3991,
      "step": 133600
    },
    {
      "epoch": 10.213123519975555,
      "grad_norm": 14.097603797912598,
      "learning_rate": 4.148906373335371e-05,
      "loss": 2.3129,
      "step": 133700
    },
    {
      "epoch": 10.220762355816973,
      "grad_norm": 11.8412504196167,
      "learning_rate": 4.148269803681919e-05,
      "loss": 2.1997,
      "step": 133800
    },
    {
      "epoch": 10.228401191658392,
      "grad_norm": 12.461237907409668,
      "learning_rate": 4.1476332340284674e-05,
      "loss": 2.2399,
      "step": 133900
    },
    {
      "epoch": 10.23604002749981,
      "grad_norm": 18.53157615661621,
      "learning_rate": 4.1469966643750164e-05,
      "loss": 2.2741,
      "step": 134000
    },
    {
      "epoch": 10.243678863341227,
      "grad_norm": 12.382804870605469,
      "learning_rate": 4.146360094721565e-05,
      "loss": 2.3076,
      "step": 134100
    },
    {
      "epoch": 10.251317699182644,
      "grad_norm": 11.634576797485352,
      "learning_rate": 4.145723525068113e-05,
      "loss": 2.2843,
      "step": 134200
    },
    {
      "epoch": 10.258956535024062,
      "grad_norm": 9.812400817871094,
      "learning_rate": 4.1450869554146615e-05,
      "loss": 2.3108,
      "step": 134300
    },
    {
      "epoch": 10.26659537086548,
      "grad_norm": 12.845361709594727,
      "learning_rate": 4.14445038576121e-05,
      "loss": 2.343,
      "step": 134400
    },
    {
      "epoch": 10.274234206706899,
      "grad_norm": 12.46524429321289,
      "learning_rate": 4.143813816107759e-05,
      "loss": 2.4063,
      "step": 134500
    },
    {
      "epoch": 10.281873042548316,
      "grad_norm": 15.437997817993164,
      "learning_rate": 4.143177246454307e-05,
      "loss": 2.288,
      "step": 134600
    },
    {
      "epoch": 10.289511878389733,
      "grad_norm": 12.872812271118164,
      "learning_rate": 4.1425406768008556e-05,
      "loss": 2.3144,
      "step": 134700
    },
    {
      "epoch": 10.297150714231151,
      "grad_norm": 9.149100303649902,
      "learning_rate": 4.141904107147404e-05,
      "loss": 2.298,
      "step": 134800
    },
    {
      "epoch": 10.304789550072568,
      "grad_norm": 12.160037994384766,
      "learning_rate": 4.141267537493953e-05,
      "loss": 2.2458,
      "step": 134900
    },
    {
      "epoch": 10.312428385913986,
      "grad_norm": 12.112093925476074,
      "learning_rate": 4.140630967840501e-05,
      "loss": 2.3077,
      "step": 135000
    },
    {
      "epoch": 10.320067221755405,
      "grad_norm": 16.290660858154297,
      "learning_rate": 4.13999439818705e-05,
      "loss": 2.2847,
      "step": 135100
    },
    {
      "epoch": 10.327706057596822,
      "grad_norm": 13.758832931518555,
      "learning_rate": 4.139357828533598e-05,
      "loss": 2.3225,
      "step": 135200
    },
    {
      "epoch": 10.33534489343824,
      "grad_norm": 12.096447944641113,
      "learning_rate": 4.1387212588801464e-05,
      "loss": 2.222,
      "step": 135300
    },
    {
      "epoch": 10.342983729279657,
      "grad_norm": 12.758147239685059,
      "learning_rate": 4.1380846892266954e-05,
      "loss": 2.3234,
      "step": 135400
    },
    {
      "epoch": 10.350622565121075,
      "grad_norm": 15.342291831970215,
      "learning_rate": 4.137448119573244e-05,
      "loss": 2.1859,
      "step": 135500
    },
    {
      "epoch": 10.358261400962494,
      "grad_norm": 12.916668891906738,
      "learning_rate": 4.136811549919792e-05,
      "loss": 2.1956,
      "step": 135600
    },
    {
      "epoch": 10.365900236803911,
      "grad_norm": 11.395158767700195,
      "learning_rate": 4.1361749802663405e-05,
      "loss": 2.3072,
      "step": 135700
    },
    {
      "epoch": 10.373539072645329,
      "grad_norm": 12.541548728942871,
      "learning_rate": 4.1355384106128895e-05,
      "loss": 2.333,
      "step": 135800
    },
    {
      "epoch": 10.381177908486746,
      "grad_norm": 10.79305362701416,
      "learning_rate": 4.134901840959438e-05,
      "loss": 2.2265,
      "step": 135900
    },
    {
      "epoch": 10.388816744328164,
      "grad_norm": 14.130770683288574,
      "learning_rate": 4.134265271305986e-05,
      "loss": 2.3286,
      "step": 136000
    },
    {
      "epoch": 10.396455580169583,
      "grad_norm": 13.000926971435547,
      "learning_rate": 4.133628701652535e-05,
      "loss": 2.308,
      "step": 136100
    },
    {
      "epoch": 10.404094416011,
      "grad_norm": 12.97460651397705,
      "learning_rate": 4.1329921319990836e-05,
      "loss": 2.3102,
      "step": 136200
    },
    {
      "epoch": 10.411733251852418,
      "grad_norm": 10.565502166748047,
      "learning_rate": 4.132355562345632e-05,
      "loss": 2.272,
      "step": 136300
    },
    {
      "epoch": 10.419372087693835,
      "grad_norm": 15.347818374633789,
      "learning_rate": 4.131718992692181e-05,
      "loss": 2.3363,
      "step": 136400
    },
    {
      "epoch": 10.427010923535253,
      "grad_norm": 13.206025123596191,
      "learning_rate": 4.131082423038729e-05,
      "loss": 2.3192,
      "step": 136500
    },
    {
      "epoch": 10.43464975937667,
      "grad_norm": 11.171476364135742,
      "learning_rate": 4.130445853385278e-05,
      "loss": 2.3574,
      "step": 136600
    },
    {
      "epoch": 10.44228859521809,
      "grad_norm": 10.876753807067871,
      "learning_rate": 4.129809283731827e-05,
      "loss": 2.2814,
      "step": 136700
    },
    {
      "epoch": 10.449927431059507,
      "grad_norm": 10.917226791381836,
      "learning_rate": 4.129172714078375e-05,
      "loss": 2.3085,
      "step": 136800
    },
    {
      "epoch": 10.457566266900924,
      "grad_norm": 12.33572006225586,
      "learning_rate": 4.1285361444249234e-05,
      "loss": 2.2061,
      "step": 136900
    },
    {
      "epoch": 10.465205102742342,
      "grad_norm": 10.619377136230469,
      "learning_rate": 4.127899574771472e-05,
      "loss": 2.2346,
      "step": 137000
    },
    {
      "epoch": 10.47284393858376,
      "grad_norm": 13.942087173461914,
      "learning_rate": 4.12726300511802e-05,
      "loss": 2.2649,
      "step": 137100
    },
    {
      "epoch": 10.480482774425177,
      "grad_norm": 11.48863410949707,
      "learning_rate": 4.126626435464569e-05,
      "loss": 2.315,
      "step": 137200
    },
    {
      "epoch": 10.488121610266596,
      "grad_norm": 12.793237686157227,
      "learning_rate": 4.1259898658111175e-05,
      "loss": 2.2553,
      "step": 137300
    },
    {
      "epoch": 10.495760446108013,
      "grad_norm": 12.441237449645996,
      "learning_rate": 4.125353296157666e-05,
      "loss": 2.174,
      "step": 137400
    },
    {
      "epoch": 10.50339928194943,
      "grad_norm": 14.094527244567871,
      "learning_rate": 4.124716726504214e-05,
      "loss": 2.2628,
      "step": 137500
    },
    {
      "epoch": 10.511038117790848,
      "grad_norm": 15.310464859008789,
      "learning_rate": 4.1240801568507626e-05,
      "loss": 2.3511,
      "step": 137600
    },
    {
      "epoch": 10.518676953632266,
      "grad_norm": 14.797348976135254,
      "learning_rate": 4.1234435871973116e-05,
      "loss": 2.2465,
      "step": 137700
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 15.111539840698242,
      "learning_rate": 4.12280701754386e-05,
      "loss": 2.293,
      "step": 137800
    },
    {
      "epoch": 10.533954625315102,
      "grad_norm": 12.574286460876465,
      "learning_rate": 4.122170447890408e-05,
      "loss": 2.2684,
      "step": 137900
    },
    {
      "epoch": 10.54159346115652,
      "grad_norm": 11.417036056518555,
      "learning_rate": 4.1215338782369567e-05,
      "loss": 2.1644,
      "step": 138000
    },
    {
      "epoch": 10.549232296997937,
      "grad_norm": 11.5624418258667,
      "learning_rate": 4.120897308583506e-05,
      "loss": 2.2812,
      "step": 138100
    },
    {
      "epoch": 10.556871132839355,
      "grad_norm": 12.524584770202637,
      "learning_rate": 4.120260738930054e-05,
      "loss": 2.3305,
      "step": 138200
    },
    {
      "epoch": 10.564509968680774,
      "grad_norm": 11.791937828063965,
      "learning_rate": 4.1196241692766024e-05,
      "loss": 2.2954,
      "step": 138300
    },
    {
      "epoch": 10.572148804522191,
      "grad_norm": 15.710227012634277,
      "learning_rate": 4.118987599623151e-05,
      "loss": 2.4238,
      "step": 138400
    },
    {
      "epoch": 10.579787640363609,
      "grad_norm": 12.25861644744873,
      "learning_rate": 4.118351029969699e-05,
      "loss": 2.2208,
      "step": 138500
    },
    {
      "epoch": 10.587426476205026,
      "grad_norm": 15.301997184753418,
      "learning_rate": 4.117714460316248e-05,
      "loss": 2.1067,
      "step": 138600
    },
    {
      "epoch": 10.595065312046444,
      "grad_norm": 9.873083114624023,
      "learning_rate": 4.1170778906627965e-05,
      "loss": 2.3266,
      "step": 138700
    },
    {
      "epoch": 10.602704147887861,
      "grad_norm": 14.076682090759277,
      "learning_rate": 4.116441321009345e-05,
      "loss": 2.3313,
      "step": 138800
    },
    {
      "epoch": 10.61034298372928,
      "grad_norm": 13.090181350708008,
      "learning_rate": 4.115804751355893e-05,
      "loss": 2.3237,
      "step": 138900
    },
    {
      "epoch": 10.617981819570698,
      "grad_norm": 15.970577239990234,
      "learning_rate": 4.115168181702442e-05,
      "loss": 2.277,
      "step": 139000
    },
    {
      "epoch": 10.625620655412115,
      "grad_norm": 11.8993501663208,
      "learning_rate": 4.1145316120489906e-05,
      "loss": 2.2978,
      "step": 139100
    },
    {
      "epoch": 10.633259491253533,
      "grad_norm": 11.128570556640625,
      "learning_rate": 4.113895042395539e-05,
      "loss": 2.3273,
      "step": 139200
    },
    {
      "epoch": 10.64089832709495,
      "grad_norm": 12.182540893554688,
      "learning_rate": 4.113258472742087e-05,
      "loss": 2.3526,
      "step": 139300
    },
    {
      "epoch": 10.648537162936368,
      "grad_norm": 13.175027847290039,
      "learning_rate": 4.1126219030886356e-05,
      "loss": 2.3218,
      "step": 139400
    },
    {
      "epoch": 10.656175998777787,
      "grad_norm": 12.009425163269043,
      "learning_rate": 4.111985333435185e-05,
      "loss": 2.2328,
      "step": 139500
    },
    {
      "epoch": 10.663814834619204,
      "grad_norm": 15.91930103302002,
      "learning_rate": 4.111348763781733e-05,
      "loss": 2.3333,
      "step": 139600
    },
    {
      "epoch": 10.671453670460622,
      "grad_norm": 12.491579055786133,
      "learning_rate": 4.1107121941282814e-05,
      "loss": 2.2846,
      "step": 139700
    },
    {
      "epoch": 10.67909250630204,
      "grad_norm": 14.512228965759277,
      "learning_rate": 4.1100756244748304e-05,
      "loss": 2.3108,
      "step": 139800
    },
    {
      "epoch": 10.686731342143457,
      "grad_norm": 13.023360252380371,
      "learning_rate": 4.109439054821379e-05,
      "loss": 2.2961,
      "step": 139900
    },
    {
      "epoch": 10.694370177984876,
      "grad_norm": 17.968910217285156,
      "learning_rate": 4.108802485167927e-05,
      "loss": 2.3102,
      "step": 140000
    },
    {
      "epoch": 10.702009013826293,
      "grad_norm": 15.51751708984375,
      "learning_rate": 4.108165915514476e-05,
      "loss": 2.293,
      "step": 140100
    },
    {
      "epoch": 10.70964784966771,
      "grad_norm": 12.349623680114746,
      "learning_rate": 4.1075293458610245e-05,
      "loss": 2.2347,
      "step": 140200
    },
    {
      "epoch": 10.717286685509128,
      "grad_norm": 13.63817024230957,
      "learning_rate": 4.106892776207573e-05,
      "loss": 2.3508,
      "step": 140300
    },
    {
      "epoch": 10.724925521350546,
      "grad_norm": 11.524520874023438,
      "learning_rate": 4.106256206554122e-05,
      "loss": 2.2313,
      "step": 140400
    },
    {
      "epoch": 10.732564357191965,
      "grad_norm": 15.46878719329834,
      "learning_rate": 4.10561963690067e-05,
      "loss": 2.2621,
      "step": 140500
    },
    {
      "epoch": 10.740203193033382,
      "grad_norm": 12.497889518737793,
      "learning_rate": 4.1049830672472186e-05,
      "loss": 2.2872,
      "step": 140600
    },
    {
      "epoch": 10.7478420288748,
      "grad_norm": 10.43407154083252,
      "learning_rate": 4.104346497593767e-05,
      "loss": 2.3438,
      "step": 140700
    },
    {
      "epoch": 10.755480864716217,
      "grad_norm": 14.92660140991211,
      "learning_rate": 4.103709927940315e-05,
      "loss": 2.2684,
      "step": 140800
    },
    {
      "epoch": 10.763119700557635,
      "grad_norm": 15.481254577636719,
      "learning_rate": 4.103073358286864e-05,
      "loss": 2.28,
      "step": 140900
    },
    {
      "epoch": 10.770758536399052,
      "grad_norm": 13.339113235473633,
      "learning_rate": 4.102436788633413e-05,
      "loss": 2.313,
      "step": 141000
    },
    {
      "epoch": 10.778397372240471,
      "grad_norm": 10.188651084899902,
      "learning_rate": 4.101800218979961e-05,
      "loss": 2.2455,
      "step": 141100
    },
    {
      "epoch": 10.786036208081889,
      "grad_norm": 16.478330612182617,
      "learning_rate": 4.1011636493265094e-05,
      "loss": 2.2578,
      "step": 141200
    },
    {
      "epoch": 10.793675043923306,
      "grad_norm": 12.345084190368652,
      "learning_rate": 4.1005270796730584e-05,
      "loss": 2.249,
      "step": 141300
    },
    {
      "epoch": 10.801313879764724,
      "grad_norm": 13.721733093261719,
      "learning_rate": 4.099890510019607e-05,
      "loss": 2.2877,
      "step": 141400
    },
    {
      "epoch": 10.808952715606141,
      "grad_norm": 12.047978401184082,
      "learning_rate": 4.099253940366155e-05,
      "loss": 2.1879,
      "step": 141500
    },
    {
      "epoch": 10.816591551447559,
      "grad_norm": 13.573960304260254,
      "learning_rate": 4.0986173707127035e-05,
      "loss": 2.2959,
      "step": 141600
    },
    {
      "epoch": 10.824230387288978,
      "grad_norm": 11.211859703063965,
      "learning_rate": 4.097980801059252e-05,
      "loss": 2.3066,
      "step": 141700
    },
    {
      "epoch": 10.831869223130395,
      "grad_norm": 12.119747161865234,
      "learning_rate": 4.097344231405801e-05,
      "loss": 2.3059,
      "step": 141800
    },
    {
      "epoch": 10.839508058971813,
      "grad_norm": 11.061963081359863,
      "learning_rate": 4.096707661752349e-05,
      "loss": 2.2849,
      "step": 141900
    },
    {
      "epoch": 10.84714689481323,
      "grad_norm": 12.457571983337402,
      "learning_rate": 4.0960710920988976e-05,
      "loss": 2.2529,
      "step": 142000
    },
    {
      "epoch": 10.854785730654648,
      "grad_norm": 13.126605987548828,
      "learning_rate": 4.095434522445446e-05,
      "loss": 2.3806,
      "step": 142100
    },
    {
      "epoch": 10.862424566496067,
      "grad_norm": 12.64804744720459,
      "learning_rate": 4.094797952791995e-05,
      "loss": 2.3466,
      "step": 142200
    },
    {
      "epoch": 10.870063402337484,
      "grad_norm": 13.27169132232666,
      "learning_rate": 4.094161383138543e-05,
      "loss": 2.2973,
      "step": 142300
    },
    {
      "epoch": 10.877702238178902,
      "grad_norm": 11.042877197265625,
      "learning_rate": 4.093524813485092e-05,
      "loss": 2.2617,
      "step": 142400
    },
    {
      "epoch": 10.88534107402032,
      "grad_norm": 11.629937171936035,
      "learning_rate": 4.09288824383164e-05,
      "loss": 2.2895,
      "step": 142500
    },
    {
      "epoch": 10.892979909861737,
      "grad_norm": 10.109213829040527,
      "learning_rate": 4.0922516741781884e-05,
      "loss": 2.2573,
      "step": 142600
    },
    {
      "epoch": 10.900618745703154,
      "grad_norm": 12.023371696472168,
      "learning_rate": 4.0916151045247374e-05,
      "loss": 2.2208,
      "step": 142700
    },
    {
      "epoch": 10.908257581544573,
      "grad_norm": 11.251315116882324,
      "learning_rate": 4.090978534871286e-05,
      "loss": 2.2375,
      "step": 142800
    },
    {
      "epoch": 10.91589641738599,
      "grad_norm": 14.74571418762207,
      "learning_rate": 4.090341965217834e-05,
      "loss": 2.3124,
      "step": 142900
    },
    {
      "epoch": 10.923535253227408,
      "grad_norm": 12.266222953796387,
      "learning_rate": 4.0897053955643825e-05,
      "loss": 2.2079,
      "step": 143000
    },
    {
      "epoch": 10.931174089068826,
      "grad_norm": 10.221892356872559,
      "learning_rate": 4.089068825910931e-05,
      "loss": 2.217,
      "step": 143100
    },
    {
      "epoch": 10.938812924910243,
      "grad_norm": 12.590216636657715,
      "learning_rate": 4.08843225625748e-05,
      "loss": 2.3776,
      "step": 143200
    },
    {
      "epoch": 10.946451760751662,
      "grad_norm": 11.034034729003906,
      "learning_rate": 4.087795686604028e-05,
      "loss": 2.2207,
      "step": 143300
    },
    {
      "epoch": 10.95409059659308,
      "grad_norm": 16.67832374572754,
      "learning_rate": 4.0871591169505766e-05,
      "loss": 2.2801,
      "step": 143400
    },
    {
      "epoch": 10.961729432434497,
      "grad_norm": 11.899962425231934,
      "learning_rate": 4.086522547297125e-05,
      "loss": 2.2935,
      "step": 143500
    },
    {
      "epoch": 10.969368268275915,
      "grad_norm": 12.84760856628418,
      "learning_rate": 4.085885977643674e-05,
      "loss": 2.2962,
      "step": 143600
    },
    {
      "epoch": 10.977007104117332,
      "grad_norm": 10.355552673339844,
      "learning_rate": 4.085249407990222e-05,
      "loss": 2.3789,
      "step": 143700
    },
    {
      "epoch": 10.98464593995875,
      "grad_norm": 12.502469062805176,
      "learning_rate": 4.0846128383367707e-05,
      "loss": 2.2936,
      "step": 143800
    },
    {
      "epoch": 10.992284775800169,
      "grad_norm": 12.9347562789917,
      "learning_rate": 4.08397626868332e-05,
      "loss": 2.2661,
      "step": 143900
    },
    {
      "epoch": 10.999923611641586,
      "grad_norm": 14.307559967041016,
      "learning_rate": 4.083339699029868e-05,
      "loss": 2.1983,
      "step": 144000
    },
    {
      "epoch": 11.0,
      "eval_loss": 2.1835761070251465,
      "eval_runtime": 1.6614,
      "eval_samples_per_second": 415.315,
      "eval_steps_per_second": 415.315,
      "step": 144001
    },
    {
      "epoch": 11.0,
      "eval_loss": 2.0600075721740723,
      "eval_runtime": 31.4397,
      "eval_samples_per_second": 416.385,
      "eval_steps_per_second": 416.385,
      "step": 144001
    },
    {
      "epoch": 11.007562447483004,
      "grad_norm": 15.921463012695312,
      "learning_rate": 4.082703129376417e-05,
      "loss": 2.2295,
      "step": 144100
    },
    {
      "epoch": 11.015201283324421,
      "grad_norm": 11.969362258911133,
      "learning_rate": 4.0820665597229654e-05,
      "loss": 2.2849,
      "step": 144200
    },
    {
      "epoch": 11.022840119165839,
      "grad_norm": 10.091947555541992,
      "learning_rate": 4.081429990069514e-05,
      "loss": 2.2884,
      "step": 144300
    },
    {
      "epoch": 11.030478955007258,
      "grad_norm": 10.392571449279785,
      "learning_rate": 4.080793420416062e-05,
      "loss": 2.1879,
      "step": 144400
    },
    {
      "epoch": 11.038117790848675,
      "grad_norm": 12.17588996887207,
      "learning_rate": 4.080156850762611e-05,
      "loss": 2.2911,
      "step": 144500
    },
    {
      "epoch": 11.045756626690093,
      "grad_norm": 14.688549995422363,
      "learning_rate": 4.0795202811091595e-05,
      "loss": 2.2174,
      "step": 144600
    },
    {
      "epoch": 11.05339546253151,
      "grad_norm": 13.568177223205566,
      "learning_rate": 4.078883711455708e-05,
      "loss": 2.31,
      "step": 144700
    },
    {
      "epoch": 11.061034298372928,
      "grad_norm": 15.881865501403809,
      "learning_rate": 4.078247141802256e-05,
      "loss": 2.2686,
      "step": 144800
    },
    {
      "epoch": 11.068673134214345,
      "grad_norm": 12.629382133483887,
      "learning_rate": 4.0776105721488046e-05,
      "loss": 2.2212,
      "step": 144900
    },
    {
      "epoch": 11.076311970055764,
      "grad_norm": 14.05068588256836,
      "learning_rate": 4.0769740024953536e-05,
      "loss": 2.2721,
      "step": 145000
    },
    {
      "epoch": 11.083950805897182,
      "grad_norm": 15.6568603515625,
      "learning_rate": 4.076337432841902e-05,
      "loss": 2.2624,
      "step": 145100
    },
    {
      "epoch": 11.091589641738599,
      "grad_norm": 13.91864013671875,
      "learning_rate": 4.07570086318845e-05,
      "loss": 2.3435,
      "step": 145200
    },
    {
      "epoch": 11.099228477580017,
      "grad_norm": 10.178975105285645,
      "learning_rate": 4.075064293534999e-05,
      "loss": 2.2625,
      "step": 145300
    },
    {
      "epoch": 11.106867313421434,
      "grad_norm": 13.646811485290527,
      "learning_rate": 4.074427723881548e-05,
      "loss": 2.3077,
      "step": 145400
    },
    {
      "epoch": 11.114506149262853,
      "grad_norm": 14.912749290466309,
      "learning_rate": 4.073791154228096e-05,
      "loss": 2.2293,
      "step": 145500
    },
    {
      "epoch": 11.12214498510427,
      "grad_norm": 12.779012680053711,
      "learning_rate": 4.0731545845746444e-05,
      "loss": 2.3021,
      "step": 145600
    },
    {
      "epoch": 11.129783820945688,
      "grad_norm": 14.185171127319336,
      "learning_rate": 4.072518014921193e-05,
      "loss": 2.2451,
      "step": 145700
    },
    {
      "epoch": 11.137422656787106,
      "grad_norm": 11.418625831604004,
      "learning_rate": 4.071881445267741e-05,
      "loss": 2.1914,
      "step": 145800
    },
    {
      "epoch": 11.145061492628523,
      "grad_norm": 11.955565452575684,
      "learning_rate": 4.07124487561429e-05,
      "loss": 2.2822,
      "step": 145900
    },
    {
      "epoch": 11.15270032846994,
      "grad_norm": 11.377037048339844,
      "learning_rate": 4.0706083059608385e-05,
      "loss": 2.2432,
      "step": 146000
    },
    {
      "epoch": 11.16033916431136,
      "grad_norm": 11.107795715332031,
      "learning_rate": 4.069971736307387e-05,
      "loss": 2.2241,
      "step": 146100
    },
    {
      "epoch": 11.167978000152777,
      "grad_norm": 13.221065521240234,
      "learning_rate": 4.069335166653935e-05,
      "loss": 2.2847,
      "step": 146200
    },
    {
      "epoch": 11.175616835994195,
      "grad_norm": 11.735332489013672,
      "learning_rate": 4.0686985970004836e-05,
      "loss": 2.1988,
      "step": 146300
    },
    {
      "epoch": 11.183255671835612,
      "grad_norm": 15.908673286437988,
      "learning_rate": 4.0680620273470326e-05,
      "loss": 2.2138,
      "step": 146400
    },
    {
      "epoch": 11.19089450767703,
      "grad_norm": 15.440223693847656,
      "learning_rate": 4.067425457693581e-05,
      "loss": 2.2897,
      "step": 146500
    },
    {
      "epoch": 11.198533343518449,
      "grad_norm": 13.212898254394531,
      "learning_rate": 4.066788888040129e-05,
      "loss": 2.2985,
      "step": 146600
    },
    {
      "epoch": 11.206172179359866,
      "grad_norm": 11.669283866882324,
      "learning_rate": 4.0661523183866777e-05,
      "loss": 2.3485,
      "step": 146700
    },
    {
      "epoch": 11.213811015201284,
      "grad_norm": 14.8068265914917,
      "learning_rate": 4.065515748733227e-05,
      "loss": 2.2541,
      "step": 146800
    },
    {
      "epoch": 11.221449851042701,
      "grad_norm": 12.1302490234375,
      "learning_rate": 4.064879179079775e-05,
      "loss": 2.1826,
      "step": 146900
    },
    {
      "epoch": 11.229088686884118,
      "grad_norm": 13.496055603027344,
      "learning_rate": 4.0642426094263234e-05,
      "loss": 2.1532,
      "step": 147000
    },
    {
      "epoch": 11.236727522725536,
      "grad_norm": 10.812214851379395,
      "learning_rate": 4.063606039772872e-05,
      "loss": 2.2609,
      "step": 147100
    },
    {
      "epoch": 11.244366358566955,
      "grad_norm": 11.90040111541748,
      "learning_rate": 4.06296947011942e-05,
      "loss": 2.1965,
      "step": 147200
    },
    {
      "epoch": 11.252005194408373,
      "grad_norm": 11.895791053771973,
      "learning_rate": 4.062332900465969e-05,
      "loss": 2.2257,
      "step": 147300
    },
    {
      "epoch": 11.25964403024979,
      "grad_norm": 10.33719253540039,
      "learning_rate": 4.0616963308125175e-05,
      "loss": 2.3079,
      "step": 147400
    },
    {
      "epoch": 11.267282866091207,
      "grad_norm": 10.74907112121582,
      "learning_rate": 4.061059761159066e-05,
      "loss": 2.19,
      "step": 147500
    },
    {
      "epoch": 11.274921701932625,
      "grad_norm": 10.399011611938477,
      "learning_rate": 4.060423191505615e-05,
      "loss": 2.2318,
      "step": 147600
    },
    {
      "epoch": 11.282560537774042,
      "grad_norm": 15.547609329223633,
      "learning_rate": 4.059786621852163e-05,
      "loss": 2.2154,
      "step": 147700
    },
    {
      "epoch": 11.290199373615462,
      "grad_norm": 9.93196964263916,
      "learning_rate": 4.0591500521987116e-05,
      "loss": 2.2284,
      "step": 147800
    },
    {
      "epoch": 11.297838209456879,
      "grad_norm": 10.449718475341797,
      "learning_rate": 4.0585134825452606e-05,
      "loss": 2.3682,
      "step": 147900
    },
    {
      "epoch": 11.305477045298296,
      "grad_norm": 13.366158485412598,
      "learning_rate": 4.057876912891809e-05,
      "loss": 2.3148,
      "step": 148000
    },
    {
      "epoch": 11.313115881139714,
      "grad_norm": 12.696928977966309,
      "learning_rate": 4.057240343238357e-05,
      "loss": 2.2415,
      "step": 148100
    },
    {
      "epoch": 11.320754716981131,
      "grad_norm": 10.766042709350586,
      "learning_rate": 4.0566037735849064e-05,
      "loss": 2.1826,
      "step": 148200
    },
    {
      "epoch": 11.32839355282255,
      "grad_norm": 13.136916160583496,
      "learning_rate": 4.055967203931455e-05,
      "loss": 2.3148,
      "step": 148300
    },
    {
      "epoch": 11.336032388663968,
      "grad_norm": 12.478577613830566,
      "learning_rate": 4.055330634278003e-05,
      "loss": 2.2912,
      "step": 148400
    },
    {
      "epoch": 11.343671224505385,
      "grad_norm": 15.371304512023926,
      "learning_rate": 4.0546940646245514e-05,
      "loss": 2.2529,
      "step": 148500
    },
    {
      "epoch": 11.351310060346803,
      "grad_norm": 10.83596420288086,
      "learning_rate": 4.0540574949711e-05,
      "loss": 2.2719,
      "step": 148600
    },
    {
      "epoch": 11.35894889618822,
      "grad_norm": 13.915789604187012,
      "learning_rate": 4.053420925317649e-05,
      "loss": 2.2009,
      "step": 148700
    },
    {
      "epoch": 11.36658773202964,
      "grad_norm": 14.141657829284668,
      "learning_rate": 4.052784355664197e-05,
      "loss": 2.2676,
      "step": 148800
    },
    {
      "epoch": 11.374226567871057,
      "grad_norm": 13.155235290527344,
      "learning_rate": 4.0521477860107455e-05,
      "loss": 2.2835,
      "step": 148900
    },
    {
      "epoch": 11.381865403712474,
      "grad_norm": 12.61658000946045,
      "learning_rate": 4.051511216357294e-05,
      "loss": 2.2086,
      "step": 149000
    },
    {
      "epoch": 11.389504239553892,
      "grad_norm": 11.537753105163574,
      "learning_rate": 4.050874646703843e-05,
      "loss": 2.1661,
      "step": 149100
    },
    {
      "epoch": 11.39714307539531,
      "grad_norm": 13.779468536376953,
      "learning_rate": 4.050238077050391e-05,
      "loss": 2.3022,
      "step": 149200
    },
    {
      "epoch": 11.404781911236727,
      "grad_norm": 14.181745529174805,
      "learning_rate": 4.0496015073969396e-05,
      "loss": 2.2599,
      "step": 149300
    },
    {
      "epoch": 11.412420747078146,
      "grad_norm": 11.022451400756836,
      "learning_rate": 4.048964937743488e-05,
      "loss": 2.2421,
      "step": 149400
    },
    {
      "epoch": 11.420059582919563,
      "grad_norm": 11.03013801574707,
      "learning_rate": 4.048328368090036e-05,
      "loss": 2.2108,
      "step": 149500
    },
    {
      "epoch": 11.427698418760981,
      "grad_norm": 12.305463790893555,
      "learning_rate": 4.047691798436585e-05,
      "loss": 2.2335,
      "step": 149600
    },
    {
      "epoch": 11.435337254602398,
      "grad_norm": 8.997669219970703,
      "learning_rate": 4.047055228783134e-05,
      "loss": 2.3657,
      "step": 149700
    },
    {
      "epoch": 11.442976090443816,
      "grad_norm": 14.033700942993164,
      "learning_rate": 4.046418659129682e-05,
      "loss": 2.2651,
      "step": 149800
    },
    {
      "epoch": 11.450614926285233,
      "grad_norm": 13.362041473388672,
      "learning_rate": 4.0457820894762304e-05,
      "loss": 2.2541,
      "step": 149900
    },
    {
      "epoch": 11.458253762126652,
      "grad_norm": 11.77243423461914,
      "learning_rate": 4.0451455198227794e-05,
      "loss": 2.1975,
      "step": 150000
    },
    {
      "epoch": 11.46589259796807,
      "grad_norm": 10.518608093261719,
      "learning_rate": 4.044508950169328e-05,
      "loss": 2.2465,
      "step": 150100
    },
    {
      "epoch": 11.473531433809487,
      "grad_norm": 14.516520500183105,
      "learning_rate": 4.043872380515876e-05,
      "loss": 2.2143,
      "step": 150200
    },
    {
      "epoch": 11.481170269650905,
      "grad_norm": 11.338297843933105,
      "learning_rate": 4.0432358108624245e-05,
      "loss": 2.22,
      "step": 150300
    },
    {
      "epoch": 11.488809105492322,
      "grad_norm": 13.717514991760254,
      "learning_rate": 4.042599241208973e-05,
      "loss": 2.2221,
      "step": 150400
    },
    {
      "epoch": 11.496447941333741,
      "grad_norm": 12.126251220703125,
      "learning_rate": 4.041962671555522e-05,
      "loss": 2.2426,
      "step": 150500
    },
    {
      "epoch": 11.504086777175159,
      "grad_norm": 12.127198219299316,
      "learning_rate": 4.04132610190207e-05,
      "loss": 2.3253,
      "step": 150600
    },
    {
      "epoch": 11.511725613016576,
      "grad_norm": 10.493544578552246,
      "learning_rate": 4.0406895322486186e-05,
      "loss": 2.2396,
      "step": 150700
    },
    {
      "epoch": 11.519364448857994,
      "grad_norm": 12.144506454467773,
      "learning_rate": 4.040052962595167e-05,
      "loss": 2.2541,
      "step": 150800
    },
    {
      "epoch": 11.527003284699411,
      "grad_norm": 11.895977973937988,
      "learning_rate": 4.039416392941716e-05,
      "loss": 2.2014,
      "step": 150900
    },
    {
      "epoch": 11.53464212054083,
      "grad_norm": 14.885167121887207,
      "learning_rate": 4.038779823288264e-05,
      "loss": 2.2192,
      "step": 151000
    },
    {
      "epoch": 11.542280956382248,
      "grad_norm": 16.357845306396484,
      "learning_rate": 4.038143253634813e-05,
      "loss": 2.2181,
      "step": 151100
    },
    {
      "epoch": 11.549919792223665,
      "grad_norm": 11.315853118896484,
      "learning_rate": 4.037506683981361e-05,
      "loss": 2.2023,
      "step": 151200
    },
    {
      "epoch": 11.557558628065083,
      "grad_norm": 12.847949028015137,
      "learning_rate": 4.03687011432791e-05,
      "loss": 2.2961,
      "step": 151300
    },
    {
      "epoch": 11.5651974639065,
      "grad_norm": 13.392065048217773,
      "learning_rate": 4.0362335446744584e-05,
      "loss": 2.1889,
      "step": 151400
    },
    {
      "epoch": 11.572836299747918,
      "grad_norm": 14.542071342468262,
      "learning_rate": 4.035596975021007e-05,
      "loss": 2.3196,
      "step": 151500
    },
    {
      "epoch": 11.580475135589337,
      "grad_norm": 14.13449478149414,
      "learning_rate": 4.034960405367556e-05,
      "loss": 2.154,
      "step": 151600
    },
    {
      "epoch": 11.588113971430754,
      "grad_norm": 15.249086380004883,
      "learning_rate": 4.034323835714104e-05,
      "loss": 2.2062,
      "step": 151700
    },
    {
      "epoch": 11.595752807272172,
      "grad_norm": 7.974541664123535,
      "learning_rate": 4.0336872660606525e-05,
      "loss": 2.3212,
      "step": 151800
    },
    {
      "epoch": 11.60339164311359,
      "grad_norm": 15.531872749328613,
      "learning_rate": 4.0330506964072015e-05,
      "loss": 2.2228,
      "step": 151900
    },
    {
      "epoch": 11.611030478955007,
      "grad_norm": 12.728860855102539,
      "learning_rate": 4.03241412675375e-05,
      "loss": 2.1917,
      "step": 152000
    },
    {
      "epoch": 11.618669314796424,
      "grad_norm": 10.248844146728516,
      "learning_rate": 4.031777557100298e-05,
      "loss": 2.2936,
      "step": 152100
    },
    {
      "epoch": 11.626308150637843,
      "grad_norm": 17.32232666015625,
      "learning_rate": 4.0311409874468466e-05,
      "loss": 2.3329,
      "step": 152200
    },
    {
      "epoch": 11.63394698647926,
      "grad_norm": 14.217774391174316,
      "learning_rate": 4.0305044177933956e-05,
      "loss": 2.2723,
      "step": 152300
    },
    {
      "epoch": 11.641585822320678,
      "grad_norm": 11.713573455810547,
      "learning_rate": 4.029867848139944e-05,
      "loss": 2.2141,
      "step": 152400
    },
    {
      "epoch": 11.649224658162096,
      "grad_norm": 11.398200988769531,
      "learning_rate": 4.029231278486492e-05,
      "loss": 2.2147,
      "step": 152500
    },
    {
      "epoch": 11.656863494003513,
      "grad_norm": 10.574548721313477,
      "learning_rate": 4.028594708833041e-05,
      "loss": 2.2501,
      "step": 152600
    },
    {
      "epoch": 11.664502329844932,
      "grad_norm": 15.059595108032227,
      "learning_rate": 4.027958139179589e-05,
      "loss": 2.1978,
      "step": 152700
    },
    {
      "epoch": 11.67214116568635,
      "grad_norm": 16.635663986206055,
      "learning_rate": 4.027321569526138e-05,
      "loss": 2.3037,
      "step": 152800
    },
    {
      "epoch": 11.679780001527767,
      "grad_norm": 11.87775707244873,
      "learning_rate": 4.0266849998726864e-05,
      "loss": 2.3213,
      "step": 152900
    },
    {
      "epoch": 11.687418837369185,
      "grad_norm": 11.305071830749512,
      "learning_rate": 4.026048430219235e-05,
      "loss": 2.3783,
      "step": 153000
    },
    {
      "epoch": 11.695057673210602,
      "grad_norm": 8.911953926086426,
      "learning_rate": 4.025411860565783e-05,
      "loss": 2.2629,
      "step": 153100
    },
    {
      "epoch": 11.702696509052021,
      "grad_norm": 11.464447975158691,
      "learning_rate": 4.024775290912332e-05,
      "loss": 2.2764,
      "step": 153200
    },
    {
      "epoch": 11.710335344893439,
      "grad_norm": 14.90256118774414,
      "learning_rate": 4.0241387212588805e-05,
      "loss": 2.2202,
      "step": 153300
    },
    {
      "epoch": 11.717974180734856,
      "grad_norm": 11.586444854736328,
      "learning_rate": 4.023502151605429e-05,
      "loss": 2.3394,
      "step": 153400
    },
    {
      "epoch": 11.725613016576274,
      "grad_norm": 11.899872779846191,
      "learning_rate": 4.022865581951977e-05,
      "loss": 2.2865,
      "step": 153500
    },
    {
      "epoch": 11.733251852417691,
      "grad_norm": 15.763957023620605,
      "learning_rate": 4.0222290122985256e-05,
      "loss": 2.2386,
      "step": 153600
    },
    {
      "epoch": 11.740890688259109,
      "grad_norm": 12.274805068969727,
      "learning_rate": 4.0215924426450746e-05,
      "loss": 2.2615,
      "step": 153700
    },
    {
      "epoch": 11.748529524100528,
      "grad_norm": 11.19431209564209,
      "learning_rate": 4.020955872991623e-05,
      "loss": 2.2111,
      "step": 153800
    },
    {
      "epoch": 11.756168359941945,
      "grad_norm": 13.82166862487793,
      "learning_rate": 4.020319303338171e-05,
      "loss": 2.2606,
      "step": 153900
    },
    {
      "epoch": 11.763807195783363,
      "grad_norm": 14.247857093811035,
      "learning_rate": 4.01968273368472e-05,
      "loss": 2.1764,
      "step": 154000
    },
    {
      "epoch": 11.77144603162478,
      "grad_norm": 12.330465316772461,
      "learning_rate": 4.019046164031269e-05,
      "loss": 2.2008,
      "step": 154100
    },
    {
      "epoch": 11.779084867466198,
      "grad_norm": 12.609094619750977,
      "learning_rate": 4.018409594377817e-05,
      "loss": 2.2975,
      "step": 154200
    },
    {
      "epoch": 11.786723703307615,
      "grad_norm": 14.291546821594238,
      "learning_rate": 4.0177730247243654e-05,
      "loss": 2.232,
      "step": 154300
    },
    {
      "epoch": 11.794362539149034,
      "grad_norm": 15.395613670349121,
      "learning_rate": 4.017136455070914e-05,
      "loss": 2.2838,
      "step": 154400
    },
    {
      "epoch": 11.802001374990452,
      "grad_norm": 11.916194915771484,
      "learning_rate": 4.016499885417462e-05,
      "loss": 2.2624,
      "step": 154500
    },
    {
      "epoch": 11.80964021083187,
      "grad_norm": 13.961406707763672,
      "learning_rate": 4.015863315764011e-05,
      "loss": 2.2508,
      "step": 154600
    },
    {
      "epoch": 11.817279046673287,
      "grad_norm": 13.263704299926758,
      "learning_rate": 4.0152267461105595e-05,
      "loss": 2.3092,
      "step": 154700
    },
    {
      "epoch": 11.824917882514704,
      "grad_norm": 14.529643058776855,
      "learning_rate": 4.014590176457108e-05,
      "loss": 2.3219,
      "step": 154800
    },
    {
      "epoch": 11.832556718356123,
      "grad_norm": 14.994911193847656,
      "learning_rate": 4.013953606803656e-05,
      "loss": 2.2712,
      "step": 154900
    },
    {
      "epoch": 11.84019555419754,
      "grad_norm": 13.14217758178711,
      "learning_rate": 4.0133170371502046e-05,
      "loss": 2.3075,
      "step": 155000
    },
    {
      "epoch": 11.847834390038958,
      "grad_norm": 11.873502731323242,
      "learning_rate": 4.0126804674967536e-05,
      "loss": 2.3693,
      "step": 155100
    },
    {
      "epoch": 11.855473225880376,
      "grad_norm": 11.428284645080566,
      "learning_rate": 4.012043897843302e-05,
      "loss": 2.1435,
      "step": 155200
    },
    {
      "epoch": 11.863112061721793,
      "grad_norm": 13.726422309875488,
      "learning_rate": 4.011407328189851e-05,
      "loss": 2.2593,
      "step": 155300
    },
    {
      "epoch": 11.87075089756321,
      "grad_norm": 14.758625030517578,
      "learning_rate": 4.010770758536399e-05,
      "loss": 2.1785,
      "step": 155400
    },
    {
      "epoch": 11.87838973340463,
      "grad_norm": 13.059130668640137,
      "learning_rate": 4.010134188882948e-05,
      "loss": 2.2854,
      "step": 155500
    },
    {
      "epoch": 11.886028569246047,
      "grad_norm": 12.115217208862305,
      "learning_rate": 4.009497619229497e-05,
      "loss": 2.2222,
      "step": 155600
    },
    {
      "epoch": 11.893667405087465,
      "grad_norm": 12.620257377624512,
      "learning_rate": 4.008861049576045e-05,
      "loss": 2.2214,
      "step": 155700
    },
    {
      "epoch": 11.901306240928882,
      "grad_norm": 13.881366729736328,
      "learning_rate": 4.0082244799225934e-05,
      "loss": 2.2212,
      "step": 155800
    },
    {
      "epoch": 11.9089450767703,
      "grad_norm": 11.545889854431152,
      "learning_rate": 4.007587910269142e-05,
      "loss": 2.2399,
      "step": 155900
    },
    {
      "epoch": 11.916583912611719,
      "grad_norm": 8.359167098999023,
      "learning_rate": 4.006951340615691e-05,
      "loss": 2.2587,
      "step": 156000
    },
    {
      "epoch": 11.924222748453136,
      "grad_norm": 12.789379119873047,
      "learning_rate": 4.006314770962239e-05,
      "loss": 2.3269,
      "step": 156100
    },
    {
      "epoch": 11.931861584294554,
      "grad_norm": 13.474867820739746,
      "learning_rate": 4.0056782013087875e-05,
      "loss": 2.2993,
      "step": 156200
    },
    {
      "epoch": 11.939500420135971,
      "grad_norm": 11.52541446685791,
      "learning_rate": 4.005041631655336e-05,
      "loss": 2.2433,
      "step": 156300
    },
    {
      "epoch": 11.947139255977389,
      "grad_norm": 13.294442176818848,
      "learning_rate": 4.004405062001885e-05,
      "loss": 2.2845,
      "step": 156400
    },
    {
      "epoch": 11.954778091818806,
      "grad_norm": 9.665593147277832,
      "learning_rate": 4.003768492348433e-05,
      "loss": 2.1955,
      "step": 156500
    },
    {
      "epoch": 11.962416927660225,
      "grad_norm": 14.010117530822754,
      "learning_rate": 4.0031319226949816e-05,
      "loss": 2.2615,
      "step": 156600
    },
    {
      "epoch": 11.970055763501643,
      "grad_norm": 15.432581901550293,
      "learning_rate": 4.00249535304153e-05,
      "loss": 2.1667,
      "step": 156700
    },
    {
      "epoch": 11.97769459934306,
      "grad_norm": 14.806614875793457,
      "learning_rate": 4.001858783388078e-05,
      "loss": 2.3225,
      "step": 156800
    },
    {
      "epoch": 11.985333435184478,
      "grad_norm": 13.021939277648926,
      "learning_rate": 4.0012222137346274e-05,
      "loss": 2.2117,
      "step": 156900
    },
    {
      "epoch": 11.992972271025895,
      "grad_norm": 11.779670715332031,
      "learning_rate": 4.000585644081176e-05,
      "loss": 2.1346,
      "step": 157000
    },
    {
      "epoch": 12.0,
      "eval_loss": 2.1561660766601562,
      "eval_runtime": 1.6746,
      "eval_samples_per_second": 412.049,
      "eval_steps_per_second": 412.049,
      "step": 157092
    },
    {
      "epoch": 12.0,
      "eval_loss": 2.026374340057373,
      "eval_runtime": 31.5488,
      "eval_samples_per_second": 414.944,
      "eval_steps_per_second": 414.944,
      "step": 157092
    },
    {
      "epoch": 12.000611106867314,
      "grad_norm": 15.86850357055664,
      "learning_rate": 3.999949074427724e-05,
      "loss": 2.1687,
      "step": 157100
    },
    {
      "epoch": 12.008249942708732,
      "grad_norm": 16.034284591674805,
      "learning_rate": 3.9993125047742724e-05,
      "loss": 2.1146,
      "step": 157200
    },
    {
      "epoch": 12.01588877855015,
      "grad_norm": 14.615873336791992,
      "learning_rate": 3.998675935120821e-05,
      "loss": 2.267,
      "step": 157300
    },
    {
      "epoch": 12.023527614391567,
      "grad_norm": 13.525557518005371,
      "learning_rate": 3.99803936546737e-05,
      "loss": 2.2636,
      "step": 157400
    },
    {
      "epoch": 12.031166450232984,
      "grad_norm": 11.778436660766602,
      "learning_rate": 3.997402795813918e-05,
      "loss": 2.2473,
      "step": 157500
    },
    {
      "epoch": 12.038805286074401,
      "grad_norm": 13.317465782165527,
      "learning_rate": 3.9967662261604665e-05,
      "loss": 2.2654,
      "step": 157600
    },
    {
      "epoch": 12.04644412191582,
      "grad_norm": 14.322324752807617,
      "learning_rate": 3.996129656507015e-05,
      "loss": 2.2796,
      "step": 157700
    },
    {
      "epoch": 12.054082957757238,
      "grad_norm": 16.164569854736328,
      "learning_rate": 3.995493086853564e-05,
      "loss": 2.3052,
      "step": 157800
    },
    {
      "epoch": 12.061721793598656,
      "grad_norm": 10.890151023864746,
      "learning_rate": 3.994856517200112e-05,
      "loss": 2.2088,
      "step": 157900
    },
    {
      "epoch": 12.069360629440073,
      "grad_norm": 12.534921646118164,
      "learning_rate": 3.9942199475466606e-05,
      "loss": 2.2163,
      "step": 158000
    },
    {
      "epoch": 12.07699946528149,
      "grad_norm": 12.460882186889648,
      "learning_rate": 3.993583377893209e-05,
      "loss": 2.2078,
      "step": 158100
    },
    {
      "epoch": 12.08463830112291,
      "grad_norm": 14.639782905578613,
      "learning_rate": 3.992946808239757e-05,
      "loss": 2.2596,
      "step": 158200
    },
    {
      "epoch": 12.092277136964327,
      "grad_norm": 13.121265411376953,
      "learning_rate": 3.992310238586306e-05,
      "loss": 2.1288,
      "step": 158300
    },
    {
      "epoch": 12.099915972805745,
      "grad_norm": 15.781208992004395,
      "learning_rate": 3.991673668932855e-05,
      "loss": 2.2918,
      "step": 158400
    },
    {
      "epoch": 12.107554808647162,
      "grad_norm": 13.216575622558594,
      "learning_rate": 3.991037099279403e-05,
      "loss": 2.1847,
      "step": 158500
    },
    {
      "epoch": 12.11519364448858,
      "grad_norm": 13.627788543701172,
      "learning_rate": 3.9904005296259514e-05,
      "loss": 2.206,
      "step": 158600
    },
    {
      "epoch": 12.122832480329997,
      "grad_norm": 12.919657707214355,
      "learning_rate": 3.9897639599725004e-05,
      "loss": 2.1705,
      "step": 158700
    },
    {
      "epoch": 12.130471316171416,
      "grad_norm": 12.005207061767578,
      "learning_rate": 3.989127390319049e-05,
      "loss": 2.2843,
      "step": 158800
    },
    {
      "epoch": 12.138110152012834,
      "grad_norm": 12.861180305480957,
      "learning_rate": 3.988490820665597e-05,
      "loss": 2.2579,
      "step": 158900
    },
    {
      "epoch": 12.145748987854251,
      "grad_norm": 16.634113311767578,
      "learning_rate": 3.9878542510121455e-05,
      "loss": 2.2004,
      "step": 159000
    },
    {
      "epoch": 12.153387823695669,
      "grad_norm": 13.300738334655762,
      "learning_rate": 3.9872176813586945e-05,
      "loss": 2.315,
      "step": 159100
    },
    {
      "epoch": 12.161026659537086,
      "grad_norm": 11.614553451538086,
      "learning_rate": 3.986581111705243e-05,
      "loss": 2.3087,
      "step": 159200
    },
    {
      "epoch": 12.168665495378505,
      "grad_norm": 7.719278812408447,
      "learning_rate": 3.985944542051792e-05,
      "loss": 2.2377,
      "step": 159300
    },
    {
      "epoch": 12.176304331219923,
      "grad_norm": 13.36379337310791,
      "learning_rate": 3.98530797239834e-05,
      "loss": 2.2102,
      "step": 159400
    },
    {
      "epoch": 12.18394316706134,
      "grad_norm": 12.467512130737305,
      "learning_rate": 3.9846714027448886e-05,
      "loss": 2.1064,
      "step": 159500
    },
    {
      "epoch": 12.191582002902758,
      "grad_norm": 11.1368408203125,
      "learning_rate": 3.9840348330914376e-05,
      "loss": 2.2368,
      "step": 159600
    },
    {
      "epoch": 12.199220838744175,
      "grad_norm": 14.236087799072266,
      "learning_rate": 3.983398263437986e-05,
      "loss": 2.2224,
      "step": 159700
    },
    {
      "epoch": 12.206859674585592,
      "grad_norm": 9.850852012634277,
      "learning_rate": 3.9827616937845344e-05,
      "loss": 2.1464,
      "step": 159800
    },
    {
      "epoch": 12.214498510427012,
      "grad_norm": 11.751359939575195,
      "learning_rate": 3.982125124131083e-05,
      "loss": 2.1969,
      "step": 159900
    },
    {
      "epoch": 12.222137346268429,
      "grad_norm": 12.010046005249023,
      "learning_rate": 3.981488554477631e-05,
      "loss": 2.1899,
      "step": 160000
    },
    {
      "epoch": 12.229776182109847,
      "grad_norm": 12.107489585876465,
      "learning_rate": 3.98085198482418e-05,
      "loss": 2.2327,
      "step": 160100
    },
    {
      "epoch": 12.237415017951264,
      "grad_norm": 11.405299186706543,
      "learning_rate": 3.9802154151707284e-05,
      "loss": 2.2905,
      "step": 160200
    },
    {
      "epoch": 12.245053853792681,
      "grad_norm": 16.968730926513672,
      "learning_rate": 3.979578845517277e-05,
      "loss": 2.2303,
      "step": 160300
    },
    {
      "epoch": 12.252692689634099,
      "grad_norm": 13.836402893066406,
      "learning_rate": 3.978942275863825e-05,
      "loss": 2.1777,
      "step": 160400
    },
    {
      "epoch": 12.260331525475518,
      "grad_norm": 10.5722074508667,
      "learning_rate": 3.9783057062103735e-05,
      "loss": 2.2466,
      "step": 160500
    },
    {
      "epoch": 12.267970361316936,
      "grad_norm": 14.78982925415039,
      "learning_rate": 3.9776691365569225e-05,
      "loss": 2.2054,
      "step": 160600
    },
    {
      "epoch": 12.275609197158353,
      "grad_norm": 11.771839141845703,
      "learning_rate": 3.977032566903471e-05,
      "loss": 2.2815,
      "step": 160700
    },
    {
      "epoch": 12.28324803299977,
      "grad_norm": 17.97897720336914,
      "learning_rate": 3.976395997250019e-05,
      "loss": 2.2578,
      "step": 160800
    },
    {
      "epoch": 12.290886868841188,
      "grad_norm": 12.814473152160645,
      "learning_rate": 3.9757594275965676e-05,
      "loss": 2.165,
      "step": 160900
    },
    {
      "epoch": 12.298525704682607,
      "grad_norm": 11.371504783630371,
      "learning_rate": 3.9751228579431166e-05,
      "loss": 2.2091,
      "step": 161000
    },
    {
      "epoch": 12.306164540524025,
      "grad_norm": 15.637360572814941,
      "learning_rate": 3.974486288289665e-05,
      "loss": 2.2835,
      "step": 161100
    },
    {
      "epoch": 12.313803376365442,
      "grad_norm": 10.05837631225586,
      "learning_rate": 3.973849718636213e-05,
      "loss": 2.1434,
      "step": 161200
    },
    {
      "epoch": 12.32144221220686,
      "grad_norm": 11.1673583984375,
      "learning_rate": 3.973213148982762e-05,
      "loss": 2.2704,
      "step": 161300
    },
    {
      "epoch": 12.329081048048277,
      "grad_norm": 13.995275497436523,
      "learning_rate": 3.97257657932931e-05,
      "loss": 2.1809,
      "step": 161400
    },
    {
      "epoch": 12.336719883889696,
      "grad_norm": 10.165550231933594,
      "learning_rate": 3.971940009675859e-05,
      "loss": 2.1504,
      "step": 161500
    },
    {
      "epoch": 12.344358719731114,
      "grad_norm": 13.646977424621582,
      "learning_rate": 3.9713034400224074e-05,
      "loss": 2.1795,
      "step": 161600
    },
    {
      "epoch": 12.351997555572531,
      "grad_norm": 8.9177827835083,
      "learning_rate": 3.970666870368956e-05,
      "loss": 2.2125,
      "step": 161700
    },
    {
      "epoch": 12.359636391413948,
      "grad_norm": 16.48185157775879,
      "learning_rate": 3.970030300715504e-05,
      "loss": 2.213,
      "step": 161800
    },
    {
      "epoch": 12.367275227255366,
      "grad_norm": 11.80943775177002,
      "learning_rate": 3.969393731062053e-05,
      "loss": 2.2822,
      "step": 161900
    },
    {
      "epoch": 12.374914063096783,
      "grad_norm": 14.793798446655273,
      "learning_rate": 3.9687571614086015e-05,
      "loss": 2.2002,
      "step": 162000
    },
    {
      "epoch": 12.382552898938203,
      "grad_norm": 11.740697860717773,
      "learning_rate": 3.96812059175515e-05,
      "loss": 2.1852,
      "step": 162100
    },
    {
      "epoch": 12.39019173477962,
      "grad_norm": 12.479034423828125,
      "learning_rate": 3.967484022101698e-05,
      "loss": 2.246,
      "step": 162200
    },
    {
      "epoch": 12.397830570621037,
      "grad_norm": 16.89427947998047,
      "learning_rate": 3.9668474524482466e-05,
      "loss": 2.2947,
      "step": 162300
    },
    {
      "epoch": 12.405469406462455,
      "grad_norm": 12.994152069091797,
      "learning_rate": 3.9662108827947956e-05,
      "loss": 2.226,
      "step": 162400
    },
    {
      "epoch": 12.413108242303872,
      "grad_norm": 13.130556106567383,
      "learning_rate": 3.965574313141344e-05,
      "loss": 2.1674,
      "step": 162500
    },
    {
      "epoch": 12.42074707814529,
      "grad_norm": 11.229581832885742,
      "learning_rate": 3.964937743487892e-05,
      "loss": 2.2337,
      "step": 162600
    },
    {
      "epoch": 12.428385913986709,
      "grad_norm": 11.779102325439453,
      "learning_rate": 3.964301173834441e-05,
      "loss": 2.2065,
      "step": 162700
    },
    {
      "epoch": 12.436024749828126,
      "grad_norm": 12.973716735839844,
      "learning_rate": 3.96366460418099e-05,
      "loss": 2.236,
      "step": 162800
    },
    {
      "epoch": 12.443663585669544,
      "grad_norm": 11.961963653564453,
      "learning_rate": 3.963028034527538e-05,
      "loss": 2.1988,
      "step": 162900
    },
    {
      "epoch": 12.451302421510961,
      "grad_norm": 11.896547317504883,
      "learning_rate": 3.9623914648740864e-05,
      "loss": 2.2717,
      "step": 163000
    },
    {
      "epoch": 12.458941257352379,
      "grad_norm": 15.547087669372559,
      "learning_rate": 3.9617548952206354e-05,
      "loss": 2.2404,
      "step": 163100
    },
    {
      "epoch": 12.466580093193798,
      "grad_norm": 16.847095489501953,
      "learning_rate": 3.961118325567184e-05,
      "loss": 2.2415,
      "step": 163200
    },
    {
      "epoch": 12.474218929035215,
      "grad_norm": 14.477012634277344,
      "learning_rate": 3.960481755913733e-05,
      "loss": 2.3034,
      "step": 163300
    },
    {
      "epoch": 12.481857764876633,
      "grad_norm": 12.297301292419434,
      "learning_rate": 3.959845186260281e-05,
      "loss": 2.3288,
      "step": 163400
    },
    {
      "epoch": 12.48949660071805,
      "grad_norm": 12.576231002807617,
      "learning_rate": 3.9592086166068295e-05,
      "loss": 2.2618,
      "step": 163500
    },
    {
      "epoch": 12.497135436559468,
      "grad_norm": 11.621800422668457,
      "learning_rate": 3.958572046953378e-05,
      "loss": 2.1962,
      "step": 163600
    },
    {
      "epoch": 12.504774272400887,
      "grad_norm": 14.68112564086914,
      "learning_rate": 3.957935477299926e-05,
      "loss": 2.2853,
      "step": 163700
    },
    {
      "epoch": 12.512413108242304,
      "grad_norm": 10.932845115661621,
      "learning_rate": 3.957298907646475e-05,
      "loss": 2.1853,
      "step": 163800
    },
    {
      "epoch": 12.520051944083722,
      "grad_norm": 16.812931060791016,
      "learning_rate": 3.9566623379930236e-05,
      "loss": 2.1879,
      "step": 163900
    },
    {
      "epoch": 12.52769077992514,
      "grad_norm": 16.856809616088867,
      "learning_rate": 3.956025768339572e-05,
      "loss": 2.1753,
      "step": 164000
    },
    {
      "epoch": 12.535329615766557,
      "grad_norm": 11.288249015808105,
      "learning_rate": 3.95538919868612e-05,
      "loss": 2.238,
      "step": 164100
    },
    {
      "epoch": 12.542968451607974,
      "grad_norm": 11.369956016540527,
      "learning_rate": 3.9547526290326694e-05,
      "loss": 2.3356,
      "step": 164200
    },
    {
      "epoch": 12.550607287449393,
      "grad_norm": 16.90991973876953,
      "learning_rate": 3.954116059379218e-05,
      "loss": 2.2321,
      "step": 164300
    },
    {
      "epoch": 12.558246123290811,
      "grad_norm": 11.002738952636719,
      "learning_rate": 3.953479489725766e-05,
      "loss": 2.1396,
      "step": 164400
    },
    {
      "epoch": 12.565884959132228,
      "grad_norm": 13.082596778869629,
      "learning_rate": 3.9528429200723144e-05,
      "loss": 2.1544,
      "step": 164500
    },
    {
      "epoch": 12.573523794973646,
      "grad_norm": 13.048343658447266,
      "learning_rate": 3.952206350418863e-05,
      "loss": 2.1933,
      "step": 164600
    },
    {
      "epoch": 12.581162630815063,
      "grad_norm": 13.67202377319336,
      "learning_rate": 3.951569780765412e-05,
      "loss": 2.1767,
      "step": 164700
    },
    {
      "epoch": 12.58880146665648,
      "grad_norm": 12.174562454223633,
      "learning_rate": 3.95093321111196e-05,
      "loss": 2.2627,
      "step": 164800
    },
    {
      "epoch": 12.5964403024979,
      "grad_norm": 12.300312995910645,
      "learning_rate": 3.9502966414585085e-05,
      "loss": 2.2533,
      "step": 164900
    },
    {
      "epoch": 12.604079138339317,
      "grad_norm": 12.822598457336426,
      "learning_rate": 3.949660071805057e-05,
      "loss": 2.214,
      "step": 165000
    },
    {
      "epoch": 12.611717974180735,
      "grad_norm": 11.313068389892578,
      "learning_rate": 3.949023502151606e-05,
      "loss": 2.2861,
      "step": 165100
    },
    {
      "epoch": 12.619356810022152,
      "grad_norm": 11.118033409118652,
      "learning_rate": 3.948386932498154e-05,
      "loss": 2.2024,
      "step": 165200
    },
    {
      "epoch": 12.62699564586357,
      "grad_norm": 11.62631607055664,
      "learning_rate": 3.9477503628447026e-05,
      "loss": 2.152,
      "step": 165300
    },
    {
      "epoch": 12.634634481704989,
      "grad_norm": 12.70150089263916,
      "learning_rate": 3.947113793191251e-05,
      "loss": 2.1355,
      "step": 165400
    },
    {
      "epoch": 12.642273317546406,
      "grad_norm": 14.822399139404297,
      "learning_rate": 3.946477223537799e-05,
      "loss": 2.234,
      "step": 165500
    },
    {
      "epoch": 12.649912153387824,
      "grad_norm": 13.992886543273926,
      "learning_rate": 3.9458406538843484e-05,
      "loss": 2.1718,
      "step": 165600
    },
    {
      "epoch": 12.657550989229241,
      "grad_norm": 14.032694816589355,
      "learning_rate": 3.945204084230897e-05,
      "loss": 2.29,
      "step": 165700
    },
    {
      "epoch": 12.665189825070659,
      "grad_norm": 13.745039939880371,
      "learning_rate": 3.944567514577445e-05,
      "loss": 2.2193,
      "step": 165800
    },
    {
      "epoch": 12.672828660912078,
      "grad_norm": 13.504483222961426,
      "learning_rate": 3.9439309449239934e-05,
      "loss": 2.1864,
      "step": 165900
    },
    {
      "epoch": 12.680467496753495,
      "grad_norm": 17.31768035888672,
      "learning_rate": 3.943294375270542e-05,
      "loss": 2.1814,
      "step": 166000
    },
    {
      "epoch": 12.688106332594913,
      "grad_norm": 11.347066879272461,
      "learning_rate": 3.942657805617091e-05,
      "loss": 2.2938,
      "step": 166100
    },
    {
      "epoch": 12.69574516843633,
      "grad_norm": 15.013669967651367,
      "learning_rate": 3.942021235963639e-05,
      "loss": 2.2001,
      "step": 166200
    },
    {
      "epoch": 12.703384004277748,
      "grad_norm": 13.955123901367188,
      "learning_rate": 3.9413846663101875e-05,
      "loss": 2.3047,
      "step": 166300
    },
    {
      "epoch": 12.711022840119165,
      "grad_norm": 11.901421546936035,
      "learning_rate": 3.940748096656736e-05,
      "loss": 2.1216,
      "step": 166400
    },
    {
      "epoch": 12.718661675960584,
      "grad_norm": 13.87187385559082,
      "learning_rate": 3.940111527003285e-05,
      "loss": 2.1858,
      "step": 166500
    },
    {
      "epoch": 12.726300511802002,
      "grad_norm": 15.267692565917969,
      "learning_rate": 3.939474957349833e-05,
      "loss": 2.1598,
      "step": 166600
    },
    {
      "epoch": 12.73393934764342,
      "grad_norm": 10.582671165466309,
      "learning_rate": 3.9388383876963816e-05,
      "loss": 2.2766,
      "step": 166700
    },
    {
      "epoch": 12.741578183484837,
      "grad_norm": 22.19660758972168,
      "learning_rate": 3.9382018180429306e-05,
      "loss": 2.3203,
      "step": 166800
    },
    {
      "epoch": 12.749217019326254,
      "grad_norm": 11.321172714233398,
      "learning_rate": 3.937565248389479e-05,
      "loss": 2.2847,
      "step": 166900
    },
    {
      "epoch": 12.756855855167672,
      "grad_norm": 13.728227615356445,
      "learning_rate": 3.936928678736027e-05,
      "loss": 2.189,
      "step": 167000
    },
    {
      "epoch": 12.76449469100909,
      "grad_norm": 13.653890609741211,
      "learning_rate": 3.9362921090825764e-05,
      "loss": 2.2419,
      "step": 167100
    },
    {
      "epoch": 12.772133526850508,
      "grad_norm": 11.82068920135498,
      "learning_rate": 3.935655539429125e-05,
      "loss": 2.1923,
      "step": 167200
    },
    {
      "epoch": 12.779772362691926,
      "grad_norm": 14.442943572998047,
      "learning_rate": 3.935018969775673e-05,
      "loss": 2.2662,
      "step": 167300
    },
    {
      "epoch": 12.787411198533343,
      "grad_norm": 14.596402168273926,
      "learning_rate": 3.934382400122222e-05,
      "loss": 2.2214,
      "step": 167400
    },
    {
      "epoch": 12.79505003437476,
      "grad_norm": 14.765775680541992,
      "learning_rate": 3.9337458304687705e-05,
      "loss": 2.2164,
      "step": 167500
    },
    {
      "epoch": 12.80268887021618,
      "grad_norm": 12.55073356628418,
      "learning_rate": 3.933109260815319e-05,
      "loss": 2.1734,
      "step": 167600
    },
    {
      "epoch": 12.810327706057597,
      "grad_norm": 15.955350875854492,
      "learning_rate": 3.932472691161867e-05,
      "loss": 2.1316,
      "step": 167700
    },
    {
      "epoch": 12.817966541899015,
      "grad_norm": 11.174346923828125,
      "learning_rate": 3.9318361215084155e-05,
      "loss": 2.2319,
      "step": 167800
    },
    {
      "epoch": 12.825605377740432,
      "grad_norm": 14.373730659484863,
      "learning_rate": 3.9311995518549646e-05,
      "loss": 2.1661,
      "step": 167900
    },
    {
      "epoch": 12.83324421358185,
      "grad_norm": 14.264310836791992,
      "learning_rate": 3.930562982201513e-05,
      "loss": 2.1885,
      "step": 168000
    },
    {
      "epoch": 12.840883049423267,
      "grad_norm": 17.27376937866211,
      "learning_rate": 3.929926412548061e-05,
      "loss": 2.2167,
      "step": 168100
    },
    {
      "epoch": 12.848521885264686,
      "grad_norm": 15.49442195892334,
      "learning_rate": 3.9292898428946096e-05,
      "loss": 2.1672,
      "step": 168200
    },
    {
      "epoch": 12.856160721106104,
      "grad_norm": 10.969882011413574,
      "learning_rate": 3.9286532732411586e-05,
      "loss": 2.3176,
      "step": 168300
    },
    {
      "epoch": 12.863799556947521,
      "grad_norm": 14.687919616699219,
      "learning_rate": 3.928016703587707e-05,
      "loss": 2.1623,
      "step": 168400
    },
    {
      "epoch": 12.871438392788939,
      "grad_norm": 15.938332557678223,
      "learning_rate": 3.9273801339342554e-05,
      "loss": 2.1818,
      "step": 168500
    },
    {
      "epoch": 12.879077228630356,
      "grad_norm": 12.726853370666504,
      "learning_rate": 3.926743564280804e-05,
      "loss": 2.2922,
      "step": 168600
    },
    {
      "epoch": 12.886716064471775,
      "grad_norm": 12.696952819824219,
      "learning_rate": 3.926106994627352e-05,
      "loss": 2.2219,
      "step": 168700
    },
    {
      "epoch": 12.894354900313193,
      "grad_norm": 13.945363998413086,
      "learning_rate": 3.925470424973901e-05,
      "loss": 2.2178,
      "step": 168800
    },
    {
      "epoch": 12.90199373615461,
      "grad_norm": 12.214315414428711,
      "learning_rate": 3.9248338553204494e-05,
      "loss": 2.2039,
      "step": 168900
    },
    {
      "epoch": 12.909632571996028,
      "grad_norm": 11.268684387207031,
      "learning_rate": 3.924197285666998e-05,
      "loss": 2.3211,
      "step": 169000
    },
    {
      "epoch": 12.917271407837445,
      "grad_norm": 16.771488189697266,
      "learning_rate": 3.923560716013546e-05,
      "loss": 2.2428,
      "step": 169100
    },
    {
      "epoch": 12.924910243678863,
      "grad_norm": 12.929618835449219,
      "learning_rate": 3.9229241463600945e-05,
      "loss": 2.1624,
      "step": 169200
    },
    {
      "epoch": 12.932549079520282,
      "grad_norm": 12.709798812866211,
      "learning_rate": 3.9222875767066435e-05,
      "loss": 2.2494,
      "step": 169300
    },
    {
      "epoch": 12.9401879153617,
      "grad_norm": 10.529964447021484,
      "learning_rate": 3.921651007053192e-05,
      "loss": 2.1931,
      "step": 169400
    },
    {
      "epoch": 12.947826751203117,
      "grad_norm": 11.520233154296875,
      "learning_rate": 3.92101443739974e-05,
      "loss": 2.2199,
      "step": 169500
    },
    {
      "epoch": 12.955465587044534,
      "grad_norm": 13.502278327941895,
      "learning_rate": 3.9203778677462886e-05,
      "loss": 2.262,
      "step": 169600
    },
    {
      "epoch": 12.963104422885952,
      "grad_norm": 12.836724281311035,
      "learning_rate": 3.9197412980928376e-05,
      "loss": 2.3266,
      "step": 169700
    },
    {
      "epoch": 12.97074325872737,
      "grad_norm": 11.698291778564453,
      "learning_rate": 3.919104728439386e-05,
      "loss": 2.154,
      "step": 169800
    },
    {
      "epoch": 12.978382094568788,
      "grad_norm": 12.284199714660645,
      "learning_rate": 3.918468158785934e-05,
      "loss": 2.2811,
      "step": 169900
    },
    {
      "epoch": 12.986020930410206,
      "grad_norm": 8.62675952911377,
      "learning_rate": 3.917831589132483e-05,
      "loss": 2.2666,
      "step": 170000
    },
    {
      "epoch": 12.993659766251623,
      "grad_norm": 13.120448112487793,
      "learning_rate": 3.917195019479031e-05,
      "loss": 2.2341,
      "step": 170100
    },
    {
      "epoch": 13.0,
      "eval_loss": 2.119539499282837,
      "eval_runtime": 1.6416,
      "eval_samples_per_second": 420.326,
      "eval_steps_per_second": 420.326,
      "step": 170183
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.985716700553894,
      "eval_runtime": 31.4549,
      "eval_samples_per_second": 416.183,
      "eval_steps_per_second": 416.183,
      "step": 170183
    },
    {
      "epoch": 13.00129860209304,
      "grad_norm": 10.941719055175781,
      "learning_rate": 3.91655844982558e-05,
      "loss": 2.2511,
      "step": 170200
    },
    {
      "epoch": 13.008937437934458,
      "grad_norm": 16.138532638549805,
      "learning_rate": 3.9159218801721284e-05,
      "loss": 2.2375,
      "step": 170300
    },
    {
      "epoch": 13.016576273775877,
      "grad_norm": 17.028911590576172,
      "learning_rate": 3.915285310518677e-05,
      "loss": 2.0841,
      "step": 170400
    },
    {
      "epoch": 13.024215109617295,
      "grad_norm": 11.333236694335938,
      "learning_rate": 3.914648740865226e-05,
      "loss": 2.0935,
      "step": 170500
    },
    {
      "epoch": 13.031853945458712,
      "grad_norm": 11.11782169342041,
      "learning_rate": 3.914012171211774e-05,
      "loss": 2.2742,
      "step": 170600
    },
    {
      "epoch": 13.03949278130013,
      "grad_norm": 15.957857131958008,
      "learning_rate": 3.9133756015583225e-05,
      "loss": 2.2002,
      "step": 170700
    },
    {
      "epoch": 13.047131617141547,
      "grad_norm": 15.338089942932129,
      "learning_rate": 3.9127390319048716e-05,
      "loss": 2.174,
      "step": 170800
    },
    {
      "epoch": 13.054770452982966,
      "grad_norm": 10.782257080078125,
      "learning_rate": 3.91210246225142e-05,
      "loss": 2.2405,
      "step": 170900
    },
    {
      "epoch": 13.062409288824384,
      "grad_norm": 11.189797401428223,
      "learning_rate": 3.911465892597968e-05,
      "loss": 2.1485,
      "step": 171000
    },
    {
      "epoch": 13.070048124665801,
      "grad_norm": 12.318922996520996,
      "learning_rate": 3.910829322944517e-05,
      "loss": 2.2303,
      "step": 171100
    },
    {
      "epoch": 13.077686960507219,
      "grad_norm": 12.88755989074707,
      "learning_rate": 3.9101927532910656e-05,
      "loss": 2.2647,
      "step": 171200
    },
    {
      "epoch": 13.085325796348636,
      "grad_norm": 11.970840454101562,
      "learning_rate": 3.909556183637614e-05,
      "loss": 2.1013,
      "step": 171300
    },
    {
      "epoch": 13.092964632190053,
      "grad_norm": 14.318175315856934,
      "learning_rate": 3.9089196139841624e-05,
      "loss": 2.2279,
      "step": 171400
    },
    {
      "epoch": 13.100603468031473,
      "grad_norm": 13.811168670654297,
      "learning_rate": 3.9082830443307114e-05,
      "loss": 2.1825,
      "step": 171500
    },
    {
      "epoch": 13.10824230387289,
      "grad_norm": 11.414320945739746,
      "learning_rate": 3.90764647467726e-05,
      "loss": 2.2267,
      "step": 171600
    },
    {
      "epoch": 13.115881139714308,
      "grad_norm": 12.039393424987793,
      "learning_rate": 3.907009905023808e-05,
      "loss": 2.2016,
      "step": 171700
    },
    {
      "epoch": 13.123519975555725,
      "grad_norm": 12.088828086853027,
      "learning_rate": 3.9063733353703564e-05,
      "loss": 2.2746,
      "step": 171800
    },
    {
      "epoch": 13.131158811397142,
      "grad_norm": 11.279056549072266,
      "learning_rate": 3.905736765716905e-05,
      "loss": 2.1479,
      "step": 171900
    },
    {
      "epoch": 13.138797647238562,
      "grad_norm": 16.621923446655273,
      "learning_rate": 3.905100196063454e-05,
      "loss": 2.2499,
      "step": 172000
    },
    {
      "epoch": 13.14643648307998,
      "grad_norm": 11.718693733215332,
      "learning_rate": 3.904463626410002e-05,
      "loss": 2.1275,
      "step": 172100
    },
    {
      "epoch": 13.154075318921397,
      "grad_norm": 13.024435997009277,
      "learning_rate": 3.9038270567565505e-05,
      "loss": 2.2495,
      "step": 172200
    },
    {
      "epoch": 13.161714154762814,
      "grad_norm": 12.940038681030273,
      "learning_rate": 3.903190487103099e-05,
      "loss": 2.1652,
      "step": 172300
    },
    {
      "epoch": 13.169352990604231,
      "grad_norm": 13.841461181640625,
      "learning_rate": 3.902553917449647e-05,
      "loss": 2.3189,
      "step": 172400
    },
    {
      "epoch": 13.176991826445649,
      "grad_norm": 13.380088806152344,
      "learning_rate": 3.901917347796196e-05,
      "loss": 2.179,
      "step": 172500
    },
    {
      "epoch": 13.184630662287068,
      "grad_norm": 11.029569625854492,
      "learning_rate": 3.9012807781427446e-05,
      "loss": 2.1828,
      "step": 172600
    },
    {
      "epoch": 13.192269498128486,
      "grad_norm": 12.506258964538574,
      "learning_rate": 3.900644208489293e-05,
      "loss": 2.1583,
      "step": 172700
    },
    {
      "epoch": 13.199908333969903,
      "grad_norm": 13.389841079711914,
      "learning_rate": 3.900007638835841e-05,
      "loss": 2.2103,
      "step": 172800
    },
    {
      "epoch": 13.20754716981132,
      "grad_norm": 16.073040008544922,
      "learning_rate": 3.8993710691823904e-05,
      "loss": 2.1977,
      "step": 172900
    },
    {
      "epoch": 13.215186005652738,
      "grad_norm": 10.251850128173828,
      "learning_rate": 3.898734499528939e-05,
      "loss": 2.1795,
      "step": 173000
    },
    {
      "epoch": 13.222824841494157,
      "grad_norm": 15.068641662597656,
      "learning_rate": 3.898097929875487e-05,
      "loss": 2.1995,
      "step": 173100
    },
    {
      "epoch": 13.230463677335575,
      "grad_norm": 10.113578796386719,
      "learning_rate": 3.8974613602220354e-05,
      "loss": 2.2409,
      "step": 173200
    },
    {
      "epoch": 13.238102513176992,
      "grad_norm": 11.428855895996094,
      "learning_rate": 3.896824790568584e-05,
      "loss": 2.2023,
      "step": 173300
    },
    {
      "epoch": 13.24574134901841,
      "grad_norm": 14.605215072631836,
      "learning_rate": 3.896188220915133e-05,
      "loss": 2.2853,
      "step": 173400
    },
    {
      "epoch": 13.253380184859827,
      "grad_norm": 12.020870208740234,
      "learning_rate": 3.895551651261681e-05,
      "loss": 2.1778,
      "step": 173500
    },
    {
      "epoch": 13.261019020701244,
      "grad_norm": 10.772192001342773,
      "learning_rate": 3.8949150816082295e-05,
      "loss": 2.1686,
      "step": 173600
    },
    {
      "epoch": 13.268657856542664,
      "grad_norm": 15.025172233581543,
      "learning_rate": 3.894278511954778e-05,
      "loss": 2.2812,
      "step": 173700
    },
    {
      "epoch": 13.276296692384081,
      "grad_norm": 13.132447242736816,
      "learning_rate": 3.893641942301327e-05,
      "loss": 2.2548,
      "step": 173800
    },
    {
      "epoch": 13.283935528225499,
      "grad_norm": 12.756260871887207,
      "learning_rate": 3.893005372647875e-05,
      "loss": 2.2579,
      "step": 173900
    },
    {
      "epoch": 13.291574364066916,
      "grad_norm": 12.715127944946289,
      "learning_rate": 3.8923688029944236e-05,
      "loss": 2.1828,
      "step": 174000
    },
    {
      "epoch": 13.299213199908333,
      "grad_norm": 12.516026496887207,
      "learning_rate": 3.891732233340972e-05,
      "loss": 2.1115,
      "step": 174100
    },
    {
      "epoch": 13.306852035749753,
      "grad_norm": 17.736339569091797,
      "learning_rate": 3.89109566368752e-05,
      "loss": 2.3205,
      "step": 174200
    },
    {
      "epoch": 13.31449087159117,
      "grad_norm": 10.533756256103516,
      "learning_rate": 3.8904590940340694e-05,
      "loss": 2.1882,
      "step": 174300
    },
    {
      "epoch": 13.322129707432588,
      "grad_norm": 9.61803150177002,
      "learning_rate": 3.889822524380618e-05,
      "loss": 2.1019,
      "step": 174400
    },
    {
      "epoch": 13.329768543274005,
      "grad_norm": 14.738837242126465,
      "learning_rate": 3.889185954727167e-05,
      "loss": 2.2767,
      "step": 174500
    },
    {
      "epoch": 13.337407379115422,
      "grad_norm": 11.824532508850098,
      "learning_rate": 3.888549385073715e-05,
      "loss": 2.2126,
      "step": 174600
    },
    {
      "epoch": 13.34504621495684,
      "grad_norm": 15.158162117004395,
      "learning_rate": 3.8879128154202634e-05,
      "loss": 2.0761,
      "step": 174700
    },
    {
      "epoch": 13.352685050798259,
      "grad_norm": 11.929004669189453,
      "learning_rate": 3.8872762457668125e-05,
      "loss": 2.1611,
      "step": 174800
    },
    {
      "epoch": 13.360323886639677,
      "grad_norm": 15.585350036621094,
      "learning_rate": 3.886639676113361e-05,
      "loss": 2.1243,
      "step": 174900
    },
    {
      "epoch": 13.367962722481094,
      "grad_norm": 16.00368309020996,
      "learning_rate": 3.886003106459909e-05,
      "loss": 2.2111,
      "step": 175000
    },
    {
      "epoch": 13.375601558322511,
      "grad_norm": 11.533309936523438,
      "learning_rate": 3.8853665368064575e-05,
      "loss": 2.2121,
      "step": 175100
    },
    {
      "epoch": 13.383240394163929,
      "grad_norm": 11.404597282409668,
      "learning_rate": 3.8847299671530066e-05,
      "loss": 2.2025,
      "step": 175200
    },
    {
      "epoch": 13.390879230005346,
      "grad_norm": 9.588454246520996,
      "learning_rate": 3.884093397499555e-05,
      "loss": 2.1488,
      "step": 175300
    },
    {
      "epoch": 13.398518065846766,
      "grad_norm": 12.490647315979004,
      "learning_rate": 3.883456827846103e-05,
      "loss": 2.218,
      "step": 175400
    },
    {
      "epoch": 13.406156901688183,
      "grad_norm": 9.98754596710205,
      "learning_rate": 3.8828202581926516e-05,
      "loss": 2.2389,
      "step": 175500
    },
    {
      "epoch": 13.4137957375296,
      "grad_norm": 17.27496910095215,
      "learning_rate": 3.8821836885392e-05,
      "loss": 2.1948,
      "step": 175600
    },
    {
      "epoch": 13.421434573371018,
      "grad_norm": 13.056113243103027,
      "learning_rate": 3.881547118885749e-05,
      "loss": 2.1959,
      "step": 175700
    },
    {
      "epoch": 13.429073409212435,
      "grad_norm": 12.21828556060791,
      "learning_rate": 3.8809105492322974e-05,
      "loss": 2.1584,
      "step": 175800
    },
    {
      "epoch": 13.436712245053855,
      "grad_norm": 14.012476921081543,
      "learning_rate": 3.880273979578846e-05,
      "loss": 2.2276,
      "step": 175900
    },
    {
      "epoch": 13.444351080895272,
      "grad_norm": 11.174999237060547,
      "learning_rate": 3.879637409925394e-05,
      "loss": 2.1778,
      "step": 176000
    },
    {
      "epoch": 13.45198991673669,
      "grad_norm": 13.334175109863281,
      "learning_rate": 3.879000840271943e-05,
      "loss": 2.2035,
      "step": 176100
    },
    {
      "epoch": 13.459628752578107,
      "grad_norm": 13.402300834655762,
      "learning_rate": 3.8783642706184915e-05,
      "loss": 2.1772,
      "step": 176200
    },
    {
      "epoch": 13.467267588419524,
      "grad_norm": 11.941120147705078,
      "learning_rate": 3.87772770096504e-05,
      "loss": 2.0984,
      "step": 176300
    },
    {
      "epoch": 13.474906424260944,
      "grad_norm": 15.050604820251465,
      "learning_rate": 3.877091131311588e-05,
      "loss": 2.1306,
      "step": 176400
    },
    {
      "epoch": 13.482545260102361,
      "grad_norm": 11.680146217346191,
      "learning_rate": 3.8764545616581365e-05,
      "loss": 2.1739,
      "step": 176500
    },
    {
      "epoch": 13.490184095943778,
      "grad_norm": 11.850842475891113,
      "learning_rate": 3.8758179920046856e-05,
      "loss": 2.2385,
      "step": 176600
    },
    {
      "epoch": 13.497822931785196,
      "grad_norm": 13.604866027832031,
      "learning_rate": 3.875181422351234e-05,
      "loss": 2.2123,
      "step": 176700
    },
    {
      "epoch": 13.505461767626613,
      "grad_norm": 10.20278549194336,
      "learning_rate": 3.874544852697782e-05,
      "loss": 2.1647,
      "step": 176800
    },
    {
      "epoch": 13.51310060346803,
      "grad_norm": 11.316570281982422,
      "learning_rate": 3.8739082830443306e-05,
      "loss": 2.2909,
      "step": 176900
    },
    {
      "epoch": 13.52073943930945,
      "grad_norm": 14.541590690612793,
      "learning_rate": 3.8732717133908796e-05,
      "loss": 2.2062,
      "step": 177000
    },
    {
      "epoch": 13.528378275150867,
      "grad_norm": 10.177092552185059,
      "learning_rate": 3.872635143737428e-05,
      "loss": 2.1272,
      "step": 177100
    },
    {
      "epoch": 13.536017110992285,
      "grad_norm": 13.647517204284668,
      "learning_rate": 3.8719985740839764e-05,
      "loss": 2.1907,
      "step": 177200
    },
    {
      "epoch": 13.543655946833702,
      "grad_norm": 9.684619903564453,
      "learning_rate": 3.871362004430525e-05,
      "loss": 2.2238,
      "step": 177300
    },
    {
      "epoch": 13.55129478267512,
      "grad_norm": 15.340890884399414,
      "learning_rate": 3.870725434777073e-05,
      "loss": 2.2161,
      "step": 177400
    },
    {
      "epoch": 13.558933618516537,
      "grad_norm": 15.098275184631348,
      "learning_rate": 3.870088865123622e-05,
      "loss": 2.3708,
      "step": 177500
    },
    {
      "epoch": 13.566572454357956,
      "grad_norm": 15.275945663452148,
      "learning_rate": 3.8694522954701704e-05,
      "loss": 2.2191,
      "step": 177600
    },
    {
      "epoch": 13.574211290199374,
      "grad_norm": 11.256567001342773,
      "learning_rate": 3.868815725816719e-05,
      "loss": 2.1256,
      "step": 177700
    },
    {
      "epoch": 13.581850126040791,
      "grad_norm": 14.284185409545898,
      "learning_rate": 3.868179156163267e-05,
      "loss": 2.1836,
      "step": 177800
    },
    {
      "epoch": 13.589488961882209,
      "grad_norm": 18.37296485900879,
      "learning_rate": 3.8675425865098155e-05,
      "loss": 2.1943,
      "step": 177900
    },
    {
      "epoch": 13.597127797723626,
      "grad_norm": 10.188274383544922,
      "learning_rate": 3.8669060168563645e-05,
      "loss": 2.2888,
      "step": 178000
    },
    {
      "epoch": 13.604766633565045,
      "grad_norm": 13.663546562194824,
      "learning_rate": 3.866269447202913e-05,
      "loss": 2.1857,
      "step": 178100
    },
    {
      "epoch": 13.612405469406463,
      "grad_norm": 12.704371452331543,
      "learning_rate": 3.865632877549461e-05,
      "loss": 2.1936,
      "step": 178200
    },
    {
      "epoch": 13.62004430524788,
      "grad_norm": 17.46596336364746,
      "learning_rate": 3.86499630789601e-05,
      "loss": 2.2151,
      "step": 178300
    },
    {
      "epoch": 13.627683141089298,
      "grad_norm": 14.263724327087402,
      "learning_rate": 3.8643597382425586e-05,
      "loss": 2.2249,
      "step": 178400
    },
    {
      "epoch": 13.635321976930715,
      "grad_norm": 10.948914527893066,
      "learning_rate": 3.8637231685891077e-05,
      "loss": 2.2143,
      "step": 178500
    },
    {
      "epoch": 13.642960812772134,
      "grad_norm": 12.842989921569824,
      "learning_rate": 3.863086598935656e-05,
      "loss": 2.1498,
      "step": 178600
    },
    {
      "epoch": 13.650599648613552,
      "grad_norm": 10.690284729003906,
      "learning_rate": 3.8624500292822044e-05,
      "loss": 2.1813,
      "step": 178700
    },
    {
      "epoch": 13.65823848445497,
      "grad_norm": 10.700925827026367,
      "learning_rate": 3.861813459628753e-05,
      "loss": 2.2143,
      "step": 178800
    },
    {
      "epoch": 13.665877320296387,
      "grad_norm": 12.996533393859863,
      "learning_rate": 3.861176889975302e-05,
      "loss": 2.206,
      "step": 178900
    },
    {
      "epoch": 13.673516156137804,
      "grad_norm": 12.305840492248535,
      "learning_rate": 3.86054032032185e-05,
      "loss": 2.173,
      "step": 179000
    },
    {
      "epoch": 13.681154991979222,
      "grad_norm": 10.983288764953613,
      "learning_rate": 3.8599037506683985e-05,
      "loss": 2.1574,
      "step": 179100
    },
    {
      "epoch": 13.688793827820641,
      "grad_norm": 12.569381713867188,
      "learning_rate": 3.859267181014947e-05,
      "loss": 2.1671,
      "step": 179200
    },
    {
      "epoch": 13.696432663662058,
      "grad_norm": 18.2988224029541,
      "learning_rate": 3.858630611361496e-05,
      "loss": 2.1573,
      "step": 179300
    },
    {
      "epoch": 13.704071499503476,
      "grad_norm": 12.135048866271973,
      "learning_rate": 3.857994041708044e-05,
      "loss": 2.1617,
      "step": 179400
    },
    {
      "epoch": 13.711710335344893,
      "grad_norm": 14.34567642211914,
      "learning_rate": 3.8573574720545926e-05,
      "loss": 2.2693,
      "step": 179500
    },
    {
      "epoch": 13.71934917118631,
      "grad_norm": 11.551663398742676,
      "learning_rate": 3.856720902401141e-05,
      "loss": 2.1858,
      "step": 179600
    },
    {
      "epoch": 13.726988007027728,
      "grad_norm": 12.134839057922363,
      "learning_rate": 3.856084332747689e-05,
      "loss": 2.1355,
      "step": 179700
    },
    {
      "epoch": 13.734626842869147,
      "grad_norm": 13.14155101776123,
      "learning_rate": 3.855447763094238e-05,
      "loss": 2.2026,
      "step": 179800
    },
    {
      "epoch": 13.742265678710565,
      "grad_norm": 13.959413528442383,
      "learning_rate": 3.8548111934407866e-05,
      "loss": 2.1143,
      "step": 179900
    },
    {
      "epoch": 13.749904514551982,
      "grad_norm": 14.260720252990723,
      "learning_rate": 3.854174623787335e-05,
      "loss": 2.3127,
      "step": 180000
    },
    {
      "epoch": 13.7575433503934,
      "grad_norm": 12.823201179504395,
      "learning_rate": 3.8535380541338834e-05,
      "loss": 2.1543,
      "step": 180100
    },
    {
      "epoch": 13.765182186234817,
      "grad_norm": 12.670567512512207,
      "learning_rate": 3.8529014844804324e-05,
      "loss": 2.2302,
      "step": 180200
    },
    {
      "epoch": 13.772821022076236,
      "grad_norm": 12.988075256347656,
      "learning_rate": 3.852264914826981e-05,
      "loss": 2.1832,
      "step": 180300
    },
    {
      "epoch": 13.780459857917654,
      "grad_norm": 13.637584686279297,
      "learning_rate": 3.851628345173529e-05,
      "loss": 2.1996,
      "step": 180400
    },
    {
      "epoch": 13.788098693759071,
      "grad_norm": 13.912005424499512,
      "learning_rate": 3.8509917755200774e-05,
      "loss": 2.1579,
      "step": 180500
    },
    {
      "epoch": 13.795737529600489,
      "grad_norm": 9.500020027160645,
      "learning_rate": 3.850355205866626e-05,
      "loss": 2.1672,
      "step": 180600
    },
    {
      "epoch": 13.803376365441906,
      "grad_norm": 11.721640586853027,
      "learning_rate": 3.849718636213175e-05,
      "loss": 2.2299,
      "step": 180700
    },
    {
      "epoch": 13.811015201283324,
      "grad_norm": 11.665746688842773,
      "learning_rate": 3.849082066559723e-05,
      "loss": 2.0765,
      "step": 180800
    },
    {
      "epoch": 13.818654037124743,
      "grad_norm": 13.608458518981934,
      "learning_rate": 3.8484454969062715e-05,
      "loss": 2.2053,
      "step": 180900
    },
    {
      "epoch": 13.82629287296616,
      "grad_norm": 14.674324989318848,
      "learning_rate": 3.84780892725282e-05,
      "loss": 2.2473,
      "step": 181000
    },
    {
      "epoch": 13.833931708807578,
      "grad_norm": 12.201543807983398,
      "learning_rate": 3.847172357599368e-05,
      "loss": 2.1826,
      "step": 181100
    },
    {
      "epoch": 13.841570544648995,
      "grad_norm": 13.669151306152344,
      "learning_rate": 3.846535787945917e-05,
      "loss": 2.239,
      "step": 181200
    },
    {
      "epoch": 13.849209380490413,
      "grad_norm": 9.405682563781738,
      "learning_rate": 3.8458992182924656e-05,
      "loss": 2.2215,
      "step": 181300
    },
    {
      "epoch": 13.856848216331832,
      "grad_norm": 9.927081108093262,
      "learning_rate": 3.845262648639014e-05,
      "loss": 2.2249,
      "step": 181400
    },
    {
      "epoch": 13.86448705217325,
      "grad_norm": 12.915486335754395,
      "learning_rate": 3.844626078985562e-05,
      "loss": 2.2329,
      "step": 181500
    },
    {
      "epoch": 13.872125888014667,
      "grad_norm": 13.625704765319824,
      "learning_rate": 3.8439895093321114e-05,
      "loss": 2.2284,
      "step": 181600
    },
    {
      "epoch": 13.879764723856084,
      "grad_norm": 7.876605987548828,
      "learning_rate": 3.84335293967866e-05,
      "loss": 2.2285,
      "step": 181700
    },
    {
      "epoch": 13.887403559697502,
      "grad_norm": 14.979620933532715,
      "learning_rate": 3.842716370025208e-05,
      "loss": 2.2025,
      "step": 181800
    },
    {
      "epoch": 13.895042395538919,
      "grad_norm": 10.118804931640625,
      "learning_rate": 3.8420798003717564e-05,
      "loss": 2.1159,
      "step": 181900
    },
    {
      "epoch": 13.902681231380338,
      "grad_norm": 15.17175006866455,
      "learning_rate": 3.8414432307183055e-05,
      "loss": 2.2075,
      "step": 182000
    },
    {
      "epoch": 13.910320067221756,
      "grad_norm": 14.548916816711426,
      "learning_rate": 3.840806661064854e-05,
      "loss": 2.0929,
      "step": 182100
    },
    {
      "epoch": 13.917958903063173,
      "grad_norm": 14.249285697937012,
      "learning_rate": 3.840170091411402e-05,
      "loss": 2.2508,
      "step": 182200
    },
    {
      "epoch": 13.92559773890459,
      "grad_norm": 10.569393157958984,
      "learning_rate": 3.839533521757951e-05,
      "loss": 2.1852,
      "step": 182300
    },
    {
      "epoch": 13.933236574746008,
      "grad_norm": 13.191926002502441,
      "learning_rate": 3.8388969521044996e-05,
      "loss": 2.1574,
      "step": 182400
    },
    {
      "epoch": 13.940875410587427,
      "grad_norm": 11.652409553527832,
      "learning_rate": 3.838260382451048e-05,
      "loss": 2.2616,
      "step": 182500
    },
    {
      "epoch": 13.948514246428845,
      "grad_norm": 10.632834434509277,
      "learning_rate": 3.837623812797597e-05,
      "loss": 2.1206,
      "step": 182600
    },
    {
      "epoch": 13.956153082270262,
      "grad_norm": 13.19283390045166,
      "learning_rate": 3.836987243144145e-05,
      "loss": 2.2457,
      "step": 182700
    },
    {
      "epoch": 13.96379191811168,
      "grad_norm": 11.47692584991455,
      "learning_rate": 3.8363506734906936e-05,
      "loss": 2.2061,
      "step": 182800
    },
    {
      "epoch": 13.971430753953097,
      "grad_norm": 12.824419021606445,
      "learning_rate": 3.835714103837242e-05,
      "loss": 2.2238,
      "step": 182900
    },
    {
      "epoch": 13.979069589794515,
      "grad_norm": 12.448460578918457,
      "learning_rate": 3.835077534183791e-05,
      "loss": 2.1707,
      "step": 183000
    },
    {
      "epoch": 13.986708425635934,
      "grad_norm": 10.519904136657715,
      "learning_rate": 3.8344409645303394e-05,
      "loss": 2.2337,
      "step": 183100
    },
    {
      "epoch": 13.994347261477351,
      "grad_norm": 10.659109115600586,
      "learning_rate": 3.833804394876888e-05,
      "loss": 2.1858,
      "step": 183200
    },
    {
      "epoch": 14.0,
      "eval_loss": 2.108785390853882,
      "eval_runtime": 1.6364,
      "eval_samples_per_second": 421.667,
      "eval_steps_per_second": 421.667,
      "step": 183274
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.9664613008499146,
      "eval_runtime": 31.5777,
      "eval_samples_per_second": 414.565,
      "eval_steps_per_second": 414.565,
      "step": 183274
    },
    {
      "epoch": 14.001986097318769,
      "grad_norm": 14.39413070678711,
      "learning_rate": 3.833167825223436e-05,
      "loss": 2.1711,
      "step": 183300
    },
    {
      "epoch": 14.009624933160186,
      "grad_norm": 14.143982887268066,
      "learning_rate": 3.8325312555699844e-05,
      "loss": 2.1322,
      "step": 183400
    },
    {
      "epoch": 14.017263769001604,
      "grad_norm": 12.416473388671875,
      "learning_rate": 3.8318946859165335e-05,
      "loss": 2.1627,
      "step": 183500
    },
    {
      "epoch": 14.024902604843023,
      "grad_norm": 12.587547302246094,
      "learning_rate": 3.831258116263082e-05,
      "loss": 2.1802,
      "step": 183600
    },
    {
      "epoch": 14.03254144068444,
      "grad_norm": 14.573284149169922,
      "learning_rate": 3.83062154660963e-05,
      "loss": 2.2018,
      "step": 183700
    },
    {
      "epoch": 14.040180276525858,
      "grad_norm": 12.063852310180664,
      "learning_rate": 3.8299849769561785e-05,
      "loss": 2.2023,
      "step": 183800
    },
    {
      "epoch": 14.047819112367275,
      "grad_norm": 14.702681541442871,
      "learning_rate": 3.8293484073027276e-05,
      "loss": 2.1792,
      "step": 183900
    },
    {
      "epoch": 14.055457948208693,
      "grad_norm": 11.725347518920898,
      "learning_rate": 3.828711837649276e-05,
      "loss": 2.1477,
      "step": 184000
    },
    {
      "epoch": 14.06309678405011,
      "grad_norm": 12.964736938476562,
      "learning_rate": 3.828075267995824e-05,
      "loss": 2.189,
      "step": 184100
    },
    {
      "epoch": 14.07073561989153,
      "grad_norm": 15.253150939941406,
      "learning_rate": 3.8274386983423726e-05,
      "loss": 2.1245,
      "step": 184200
    },
    {
      "epoch": 14.078374455732947,
      "grad_norm": 13.737220764160156,
      "learning_rate": 3.826802128688921e-05,
      "loss": 2.2135,
      "step": 184300
    },
    {
      "epoch": 14.086013291574364,
      "grad_norm": 12.285224914550781,
      "learning_rate": 3.82616555903547e-05,
      "loss": 2.2225,
      "step": 184400
    },
    {
      "epoch": 14.093652127415782,
      "grad_norm": 13.576801300048828,
      "learning_rate": 3.8255289893820184e-05,
      "loss": 2.1036,
      "step": 184500
    },
    {
      "epoch": 14.101290963257199,
      "grad_norm": 14.057724952697754,
      "learning_rate": 3.824892419728567e-05,
      "loss": 2.137,
      "step": 184600
    },
    {
      "epoch": 14.108929799098618,
      "grad_norm": 11.076440811157227,
      "learning_rate": 3.824255850075115e-05,
      "loss": 2.1881,
      "step": 184700
    },
    {
      "epoch": 14.116568634940036,
      "grad_norm": 14.950431823730469,
      "learning_rate": 3.823619280421664e-05,
      "loss": 2.1707,
      "step": 184800
    },
    {
      "epoch": 14.124207470781453,
      "grad_norm": 16.75904083251953,
      "learning_rate": 3.8229827107682125e-05,
      "loss": 2.2016,
      "step": 184900
    },
    {
      "epoch": 14.13184630662287,
      "grad_norm": 8.20775318145752,
      "learning_rate": 3.822346141114761e-05,
      "loss": 2.2269,
      "step": 185000
    },
    {
      "epoch": 14.139485142464288,
      "grad_norm": 9.660726547241211,
      "learning_rate": 3.821709571461309e-05,
      "loss": 2.1506,
      "step": 185100
    },
    {
      "epoch": 14.147123978305705,
      "grad_norm": 12.304224014282227,
      "learning_rate": 3.8210730018078575e-05,
      "loss": 2.2812,
      "step": 185200
    },
    {
      "epoch": 14.154762814147125,
      "grad_norm": 12.411186218261719,
      "learning_rate": 3.8204364321544066e-05,
      "loss": 2.2281,
      "step": 185300
    },
    {
      "epoch": 14.162401649988542,
      "grad_norm": 14.387375831604004,
      "learning_rate": 3.819799862500955e-05,
      "loss": 2.1623,
      "step": 185400
    },
    {
      "epoch": 14.17004048582996,
      "grad_norm": 13.525996208190918,
      "learning_rate": 3.819163292847503e-05,
      "loss": 2.2149,
      "step": 185500
    },
    {
      "epoch": 14.177679321671377,
      "grad_norm": 11.750657081604004,
      "learning_rate": 3.8185267231940516e-05,
      "loss": 2.148,
      "step": 185600
    },
    {
      "epoch": 14.185318157512794,
      "grad_norm": 15.444363594055176,
      "learning_rate": 3.8178901535406006e-05,
      "loss": 2.1833,
      "step": 185700
    },
    {
      "epoch": 14.192956993354214,
      "grad_norm": 9.90829849243164,
      "learning_rate": 3.817253583887149e-05,
      "loss": 2.1446,
      "step": 185800
    },
    {
      "epoch": 14.200595829195631,
      "grad_norm": 13.330802917480469,
      "learning_rate": 3.8166170142336974e-05,
      "loss": 2.1592,
      "step": 185900
    },
    {
      "epoch": 14.208234665037049,
      "grad_norm": 13.214778900146484,
      "learning_rate": 3.8159804445802464e-05,
      "loss": 2.2135,
      "step": 186000
    },
    {
      "epoch": 14.215873500878466,
      "grad_norm": 12.700499534606934,
      "learning_rate": 3.815343874926795e-05,
      "loss": 2.1814,
      "step": 186100
    },
    {
      "epoch": 14.223512336719883,
      "grad_norm": 14.314957618713379,
      "learning_rate": 3.814707305273343e-05,
      "loss": 2.1605,
      "step": 186200
    },
    {
      "epoch": 14.231151172561301,
      "grad_norm": 15.313559532165527,
      "learning_rate": 3.814070735619892e-05,
      "loss": 2.0698,
      "step": 186300
    },
    {
      "epoch": 14.23879000840272,
      "grad_norm": 11.863564491271973,
      "learning_rate": 3.8134341659664405e-05,
      "loss": 2.2236,
      "step": 186400
    },
    {
      "epoch": 14.246428844244138,
      "grad_norm": 14.479533195495605,
      "learning_rate": 3.812797596312989e-05,
      "loss": 2.1203,
      "step": 186500
    },
    {
      "epoch": 14.254067680085555,
      "grad_norm": 12.682890892028809,
      "learning_rate": 3.812161026659537e-05,
      "loss": 2.195,
      "step": 186600
    },
    {
      "epoch": 14.261706515926972,
      "grad_norm": 9.445316314697266,
      "learning_rate": 3.811524457006086e-05,
      "loss": 2.1954,
      "step": 186700
    },
    {
      "epoch": 14.26934535176839,
      "grad_norm": 9.0426664352417,
      "learning_rate": 3.8108878873526346e-05,
      "loss": 2.2829,
      "step": 186800
    },
    {
      "epoch": 14.27698418760981,
      "grad_norm": 12.149699211120605,
      "learning_rate": 3.810251317699183e-05,
      "loss": 2.192,
      "step": 186900
    },
    {
      "epoch": 14.284623023451227,
      "grad_norm": 11.563620567321777,
      "learning_rate": 3.809614748045731e-05,
      "loss": 2.1863,
      "step": 187000
    },
    {
      "epoch": 14.292261859292644,
      "grad_norm": 13.07982349395752,
      "learning_rate": 3.80897817839228e-05,
      "loss": 2.1984,
      "step": 187100
    },
    {
      "epoch": 14.299900695134061,
      "grad_norm": 9.8829927444458,
      "learning_rate": 3.8083416087388287e-05,
      "loss": 2.1662,
      "step": 187200
    },
    {
      "epoch": 14.307539530975479,
      "grad_norm": 11.967663764953613,
      "learning_rate": 3.807705039085377e-05,
      "loss": 2.1802,
      "step": 187300
    },
    {
      "epoch": 14.315178366816896,
      "grad_norm": 11.599926948547363,
      "learning_rate": 3.8070684694319254e-05,
      "loss": 2.1714,
      "step": 187400
    },
    {
      "epoch": 14.322817202658316,
      "grad_norm": 17.459575653076172,
      "learning_rate": 3.806431899778474e-05,
      "loss": 2.2138,
      "step": 187500
    },
    {
      "epoch": 14.330456038499733,
      "grad_norm": 14.149239540100098,
      "learning_rate": 3.805795330125023e-05,
      "loss": 2.213,
      "step": 187600
    },
    {
      "epoch": 14.33809487434115,
      "grad_norm": 11.896435737609863,
      "learning_rate": 3.805158760471571e-05,
      "loss": 2.1092,
      "step": 187700
    },
    {
      "epoch": 14.345733710182568,
      "grad_norm": 12.470197677612305,
      "learning_rate": 3.8045221908181195e-05,
      "loss": 2.1684,
      "step": 187800
    },
    {
      "epoch": 14.353372546023985,
      "grad_norm": 10.831377029418945,
      "learning_rate": 3.803885621164668e-05,
      "loss": 2.1801,
      "step": 187900
    },
    {
      "epoch": 14.361011381865403,
      "grad_norm": 15.796591758728027,
      "learning_rate": 3.803249051511217e-05,
      "loss": 2.1276,
      "step": 188000
    },
    {
      "epoch": 14.368650217706822,
      "grad_norm": 13.939173698425293,
      "learning_rate": 3.802612481857765e-05,
      "loss": 2.1571,
      "step": 188100
    },
    {
      "epoch": 14.37628905354824,
      "grad_norm": 10.226313591003418,
      "learning_rate": 3.8019759122043136e-05,
      "loss": 2.1801,
      "step": 188200
    },
    {
      "epoch": 14.383927889389657,
      "grad_norm": 11.686503410339355,
      "learning_rate": 3.801339342550862e-05,
      "loss": 2.1459,
      "step": 188300
    },
    {
      "epoch": 14.391566725231074,
      "grad_norm": 12.838508605957031,
      "learning_rate": 3.80070277289741e-05,
      "loss": 2.2143,
      "step": 188400
    },
    {
      "epoch": 14.399205561072492,
      "grad_norm": 12.975874900817871,
      "learning_rate": 3.800066203243959e-05,
      "loss": 2.243,
      "step": 188500
    },
    {
      "epoch": 14.406844396913911,
      "grad_norm": 13.048857688903809,
      "learning_rate": 3.7994296335905076e-05,
      "loss": 2.2253,
      "step": 188600
    },
    {
      "epoch": 14.414483232755329,
      "grad_norm": 17.141389846801758,
      "learning_rate": 3.798793063937056e-05,
      "loss": 2.221,
      "step": 188700
    },
    {
      "epoch": 14.422122068596746,
      "grad_norm": 11.514116287231445,
      "learning_rate": 3.7981564942836043e-05,
      "loss": 2.0676,
      "step": 188800
    },
    {
      "epoch": 14.429760904438163,
      "grad_norm": 13.374250411987305,
      "learning_rate": 3.797519924630153e-05,
      "loss": 2.0814,
      "step": 188900
    },
    {
      "epoch": 14.43739974027958,
      "grad_norm": 10.779365539550781,
      "learning_rate": 3.796883354976702e-05,
      "loss": 2.1499,
      "step": 189000
    },
    {
      "epoch": 14.445038576121,
      "grad_norm": 14.418195724487305,
      "learning_rate": 3.79624678532325e-05,
      "loss": 2.1498,
      "step": 189100
    },
    {
      "epoch": 14.452677411962418,
      "grad_norm": 17.677324295043945,
      "learning_rate": 3.7956102156697984e-05,
      "loss": 2.257,
      "step": 189200
    },
    {
      "epoch": 14.460316247803835,
      "grad_norm": 13.513812065124512,
      "learning_rate": 3.794973646016347e-05,
      "loss": 2.1207,
      "step": 189300
    },
    {
      "epoch": 14.467955083645252,
      "grad_norm": 11.245665550231934,
      "learning_rate": 3.794337076362896e-05,
      "loss": 2.2543,
      "step": 189400
    },
    {
      "epoch": 14.47559391948667,
      "grad_norm": 10.147494316101074,
      "learning_rate": 3.793700506709444e-05,
      "loss": 2.2092,
      "step": 189500
    },
    {
      "epoch": 14.483232755328087,
      "grad_norm": 13.049173355102539,
      "learning_rate": 3.7930639370559925e-05,
      "loss": 2.0266,
      "step": 189600
    },
    {
      "epoch": 14.490871591169507,
      "grad_norm": 11.09034538269043,
      "learning_rate": 3.7924273674025416e-05,
      "loss": 2.2916,
      "step": 189700
    },
    {
      "epoch": 14.498510427010924,
      "grad_norm": 14.720887184143066,
      "learning_rate": 3.79179079774909e-05,
      "loss": 2.1887,
      "step": 189800
    },
    {
      "epoch": 14.506149262852341,
      "grad_norm": 12.972820281982422,
      "learning_rate": 3.791154228095638e-05,
      "loss": 2.1083,
      "step": 189900
    },
    {
      "epoch": 14.513788098693759,
      "grad_norm": 12.130058288574219,
      "learning_rate": 3.790517658442187e-05,
      "loss": 2.1608,
      "step": 190000
    },
    {
      "epoch": 14.521426934535176,
      "grad_norm": 13.559045791625977,
      "learning_rate": 3.7898810887887357e-05,
      "loss": 2.1589,
      "step": 190100
    },
    {
      "epoch": 14.529065770376594,
      "grad_norm": 15.312180519104004,
      "learning_rate": 3.789244519135284e-05,
      "loss": 2.1665,
      "step": 190200
    },
    {
      "epoch": 14.536704606218013,
      "grad_norm": 12.53798770904541,
      "learning_rate": 3.788607949481833e-05,
      "loss": 2.0897,
      "step": 190300
    },
    {
      "epoch": 14.54434344205943,
      "grad_norm": 12.517952919006348,
      "learning_rate": 3.7879713798283814e-05,
      "loss": 2.1758,
      "step": 190400
    },
    {
      "epoch": 14.551982277900848,
      "grad_norm": 16.22547721862793,
      "learning_rate": 3.78733481017493e-05,
      "loss": 2.134,
      "step": 190500
    },
    {
      "epoch": 14.559621113742265,
      "grad_norm": 11.352189064025879,
      "learning_rate": 3.786698240521478e-05,
      "loss": 2.178,
      "step": 190600
    },
    {
      "epoch": 14.567259949583683,
      "grad_norm": 13.7422456741333,
      "learning_rate": 3.7860616708680265e-05,
      "loss": 2.1813,
      "step": 190700
    },
    {
      "epoch": 14.574898785425102,
      "grad_norm": 12.943663597106934,
      "learning_rate": 3.7854251012145755e-05,
      "loss": 2.0931,
      "step": 190800
    },
    {
      "epoch": 14.58253762126652,
      "grad_norm": 12.772830963134766,
      "learning_rate": 3.784788531561124e-05,
      "loss": 2.1602,
      "step": 190900
    },
    {
      "epoch": 14.590176457107937,
      "grad_norm": 11.678486824035645,
      "learning_rate": 3.784151961907672e-05,
      "loss": 2.1453,
      "step": 191000
    },
    {
      "epoch": 14.597815292949354,
      "grad_norm": 11.219430923461914,
      "learning_rate": 3.7835153922542205e-05,
      "loss": 2.1565,
      "step": 191100
    },
    {
      "epoch": 14.605454128790772,
      "grad_norm": 10.167972564697266,
      "learning_rate": 3.7828788226007696e-05,
      "loss": 2.1345,
      "step": 191200
    },
    {
      "epoch": 14.613092964632191,
      "grad_norm": 13.404273986816406,
      "learning_rate": 3.782242252947318e-05,
      "loss": 2.1248,
      "step": 191300
    },
    {
      "epoch": 14.620731800473608,
      "grad_norm": 12.890554428100586,
      "learning_rate": 3.781605683293866e-05,
      "loss": 2.1005,
      "step": 191400
    },
    {
      "epoch": 14.628370636315026,
      "grad_norm": 13.661998748779297,
      "learning_rate": 3.7809691136404146e-05,
      "loss": 2.2935,
      "step": 191500
    },
    {
      "epoch": 14.636009472156443,
      "grad_norm": 10.977055549621582,
      "learning_rate": 3.780332543986963e-05,
      "loss": 2.0924,
      "step": 191600
    },
    {
      "epoch": 14.64364830799786,
      "grad_norm": 14.98353099822998,
      "learning_rate": 3.779695974333512e-05,
      "loss": 2.1549,
      "step": 191700
    },
    {
      "epoch": 14.651287143839278,
      "grad_norm": 11.891453742980957,
      "learning_rate": 3.7790594046800604e-05,
      "loss": 2.2506,
      "step": 191800
    },
    {
      "epoch": 14.658925979680697,
      "grad_norm": 13.011414527893066,
      "learning_rate": 3.778422835026609e-05,
      "loss": 2.2371,
      "step": 191900
    },
    {
      "epoch": 14.666564815522115,
      "grad_norm": 11.676736831665039,
      "learning_rate": 3.777786265373157e-05,
      "loss": 2.1118,
      "step": 192000
    },
    {
      "epoch": 14.674203651363532,
      "grad_norm": 12.0053071975708,
      "learning_rate": 3.7771496957197054e-05,
      "loss": 2.2125,
      "step": 192100
    },
    {
      "epoch": 14.68184248720495,
      "grad_norm": 11.555295944213867,
      "learning_rate": 3.7765131260662545e-05,
      "loss": 2.1701,
      "step": 192200
    },
    {
      "epoch": 14.689481323046367,
      "grad_norm": 15.007265090942383,
      "learning_rate": 3.775876556412803e-05,
      "loss": 2.1209,
      "step": 192300
    },
    {
      "epoch": 14.697120158887785,
      "grad_norm": 14.387598037719727,
      "learning_rate": 3.775239986759351e-05,
      "loss": 2.2808,
      "step": 192400
    },
    {
      "epoch": 14.704758994729204,
      "grad_norm": 16.084569931030273,
      "learning_rate": 3.7746034171058995e-05,
      "loss": 2.1989,
      "step": 192500
    },
    {
      "epoch": 14.712397830570621,
      "grad_norm": 12.423810958862305,
      "learning_rate": 3.7739668474524486e-05,
      "loss": 2.2049,
      "step": 192600
    },
    {
      "epoch": 14.720036666412039,
      "grad_norm": 12.837947845458984,
      "learning_rate": 3.773330277798997e-05,
      "loss": 2.2256,
      "step": 192700
    },
    {
      "epoch": 14.727675502253456,
      "grad_norm": 12.561688423156738,
      "learning_rate": 3.772693708145545e-05,
      "loss": 2.1236,
      "step": 192800
    },
    {
      "epoch": 14.735314338094874,
      "grad_norm": 14.054532051086426,
      "learning_rate": 3.7720571384920936e-05,
      "loss": 2.1464,
      "step": 192900
    },
    {
      "epoch": 14.742953173936293,
      "grad_norm": 12.769083976745605,
      "learning_rate": 3.771420568838642e-05,
      "loss": 2.0581,
      "step": 193000
    },
    {
      "epoch": 14.75059200977771,
      "grad_norm": 15.504512786865234,
      "learning_rate": 3.770783999185191e-05,
      "loss": 2.1922,
      "step": 193100
    },
    {
      "epoch": 14.758230845619128,
      "grad_norm": 11.612757682800293,
      "learning_rate": 3.7701474295317394e-05,
      "loss": 2.145,
      "step": 193200
    },
    {
      "epoch": 14.765869681460545,
      "grad_norm": 10.519540786743164,
      "learning_rate": 3.769510859878288e-05,
      "loss": 2.1157,
      "step": 193300
    },
    {
      "epoch": 14.773508517301963,
      "grad_norm": 12.781218528747559,
      "learning_rate": 3.768874290224836e-05,
      "loss": 2.2623,
      "step": 193400
    },
    {
      "epoch": 14.78114735314338,
      "grad_norm": 11.253279685974121,
      "learning_rate": 3.768237720571385e-05,
      "loss": 2.1983,
      "step": 193500
    },
    {
      "epoch": 14.7887861889848,
      "grad_norm": 10.90091609954834,
      "learning_rate": 3.7676011509179335e-05,
      "loss": 2.1771,
      "step": 193600
    },
    {
      "epoch": 14.796425024826217,
      "grad_norm": 15.07668399810791,
      "learning_rate": 3.7669645812644825e-05,
      "loss": 2.2361,
      "step": 193700
    },
    {
      "epoch": 14.804063860667634,
      "grad_norm": 11.970412254333496,
      "learning_rate": 3.766328011611031e-05,
      "loss": 2.1548,
      "step": 193800
    },
    {
      "epoch": 14.811702696509052,
      "grad_norm": 16.80301284790039,
      "learning_rate": 3.765691441957579e-05,
      "loss": 2.1611,
      "step": 193900
    },
    {
      "epoch": 14.81934153235047,
      "grad_norm": 15.447830200195312,
      "learning_rate": 3.765054872304128e-05,
      "loss": 2.2223,
      "step": 194000
    },
    {
      "epoch": 14.826980368191888,
      "grad_norm": 12.85657787322998,
      "learning_rate": 3.7644183026506766e-05,
      "loss": 2.2085,
      "step": 194100
    },
    {
      "epoch": 14.834619204033306,
      "grad_norm": 12.102800369262695,
      "learning_rate": 3.763781732997225e-05,
      "loss": 2.1931,
      "step": 194200
    },
    {
      "epoch": 14.842258039874723,
      "grad_norm": 12.709563255310059,
      "learning_rate": 3.763145163343773e-05,
      "loss": 2.1633,
      "step": 194300
    },
    {
      "epoch": 14.84989687571614,
      "grad_norm": 14.27962875366211,
      "learning_rate": 3.762508593690322e-05,
      "loss": 2.1404,
      "step": 194400
    },
    {
      "epoch": 14.857535711557558,
      "grad_norm": 11.568184852600098,
      "learning_rate": 3.761872024036871e-05,
      "loss": 2.044,
      "step": 194500
    },
    {
      "epoch": 14.865174547398976,
      "grad_norm": 13.102487564086914,
      "learning_rate": 3.761235454383419e-05,
      "loss": 2.1894,
      "step": 194600
    },
    {
      "epoch": 14.872813383240395,
      "grad_norm": 9.800999641418457,
      "learning_rate": 3.7605988847299674e-05,
      "loss": 2.1429,
      "step": 194700
    },
    {
      "epoch": 14.880452219081812,
      "grad_norm": 11.812280654907227,
      "learning_rate": 3.759962315076516e-05,
      "loss": 2.1095,
      "step": 194800
    },
    {
      "epoch": 14.88809105492323,
      "grad_norm": 12.743535995483398,
      "learning_rate": 3.759325745423065e-05,
      "loss": 2.2036,
      "step": 194900
    },
    {
      "epoch": 14.895729890764647,
      "grad_norm": 14.525089263916016,
      "learning_rate": 3.758689175769613e-05,
      "loss": 2.2641,
      "step": 195000
    },
    {
      "epoch": 14.903368726606065,
      "grad_norm": 11.667403221130371,
      "learning_rate": 3.7580526061161615e-05,
      "loss": 2.171,
      "step": 195100
    },
    {
      "epoch": 14.911007562447484,
      "grad_norm": 8.51572322845459,
      "learning_rate": 3.75741603646271e-05,
      "loss": 2.1782,
      "step": 195200
    },
    {
      "epoch": 14.918646398288901,
      "grad_norm": 11.10194206237793,
      "learning_rate": 3.756779466809258e-05,
      "loss": 2.2229,
      "step": 195300
    },
    {
      "epoch": 14.926285234130319,
      "grad_norm": 10.503702163696289,
      "learning_rate": 3.756142897155807e-05,
      "loss": 2.1948,
      "step": 195400
    },
    {
      "epoch": 14.933924069971736,
      "grad_norm": 13.73560619354248,
      "learning_rate": 3.7555063275023556e-05,
      "loss": 2.1366,
      "step": 195500
    },
    {
      "epoch": 14.941562905813154,
      "grad_norm": 17.803815841674805,
      "learning_rate": 3.754869757848904e-05,
      "loss": 2.1638,
      "step": 195600
    },
    {
      "epoch": 14.949201741654571,
      "grad_norm": 13.768211364746094,
      "learning_rate": 3.754233188195452e-05,
      "loss": 2.1811,
      "step": 195700
    },
    {
      "epoch": 14.95684057749599,
      "grad_norm": 12.610936164855957,
      "learning_rate": 3.753596618542001e-05,
      "loss": 2.2037,
      "step": 195800
    },
    {
      "epoch": 14.964479413337408,
      "grad_norm": 9.971516609191895,
      "learning_rate": 3.7529600488885497e-05,
      "loss": 2.1755,
      "step": 195900
    },
    {
      "epoch": 14.972118249178825,
      "grad_norm": 15.768736839294434,
      "learning_rate": 3.752323479235098e-05,
      "loss": 2.0824,
      "step": 196000
    },
    {
      "epoch": 14.979757085020243,
      "grad_norm": 12.436610221862793,
      "learning_rate": 3.7516869095816464e-05,
      "loss": 2.1717,
      "step": 196100
    },
    {
      "epoch": 14.98739592086166,
      "grad_norm": 11.786477088928223,
      "learning_rate": 3.751050339928195e-05,
      "loss": 2.1452,
      "step": 196200
    },
    {
      "epoch": 14.99503475670308,
      "grad_norm": 13.799155235290527,
      "learning_rate": 3.750413770274744e-05,
      "loss": 2.1984,
      "step": 196300
    },
    {
      "epoch": 15.0,
      "eval_loss": 2.090836763381958,
      "eval_runtime": 1.6623,
      "eval_samples_per_second": 415.093,
      "eval_steps_per_second": 415.093,
      "step": 196365
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.9368709325790405,
      "eval_runtime": 31.4438,
      "eval_samples_per_second": 416.33,
      "eval_steps_per_second": 416.33,
      "step": 196365
    },
    {
      "epoch": 15.002673592544497,
      "grad_norm": 11.313323020935059,
      "learning_rate": 3.749777200621292e-05,
      "loss": 2.0771,
      "step": 196400
    },
    {
      "epoch": 15.010312428385914,
      "grad_norm": 11.681960105895996,
      "learning_rate": 3.7491406309678405e-05,
      "loss": 2.0983,
      "step": 196500
    },
    {
      "epoch": 15.017951264227332,
      "grad_norm": 10.14414119720459,
      "learning_rate": 3.748504061314389e-05,
      "loss": 2.191,
      "step": 196600
    },
    {
      "epoch": 15.025590100068749,
      "grad_norm": 14.755064964294434,
      "learning_rate": 3.747867491660938e-05,
      "loss": 2.118,
      "step": 196700
    },
    {
      "epoch": 15.033228935910167,
      "grad_norm": 11.18232536315918,
      "learning_rate": 3.747230922007486e-05,
      "loss": 2.1083,
      "step": 196800
    },
    {
      "epoch": 15.040867771751586,
      "grad_norm": 16.080337524414062,
      "learning_rate": 3.7465943523540345e-05,
      "loss": 2.1339,
      "step": 196900
    },
    {
      "epoch": 15.048506607593003,
      "grad_norm": 12.89517879486084,
      "learning_rate": 3.745957782700583e-05,
      "loss": 2.2031,
      "step": 197000
    },
    {
      "epoch": 15.05614544343442,
      "grad_norm": 9.152694702148438,
      "learning_rate": 3.745321213047131e-05,
      "loss": 2.1768,
      "step": 197100
    },
    {
      "epoch": 15.063784279275838,
      "grad_norm": 11.964941024780273,
      "learning_rate": 3.74468464339368e-05,
      "loss": 2.1142,
      "step": 197200
    },
    {
      "epoch": 15.071423115117256,
      "grad_norm": 14.129871368408203,
      "learning_rate": 3.7440480737402286e-05,
      "loss": 2.1051,
      "step": 197300
    },
    {
      "epoch": 15.079061950958675,
      "grad_norm": 10.898192405700684,
      "learning_rate": 3.743411504086777e-05,
      "loss": 2.2006,
      "step": 197400
    },
    {
      "epoch": 15.086700786800092,
      "grad_norm": 10.335447311401367,
      "learning_rate": 3.742774934433326e-05,
      "loss": 2.131,
      "step": 197500
    },
    {
      "epoch": 15.09433962264151,
      "grad_norm": 12.80953311920166,
      "learning_rate": 3.7421383647798744e-05,
      "loss": 2.1495,
      "step": 197600
    },
    {
      "epoch": 15.101978458482927,
      "grad_norm": 14.663941383361816,
      "learning_rate": 3.741501795126423e-05,
      "loss": 2.0669,
      "step": 197700
    },
    {
      "epoch": 15.109617294324345,
      "grad_norm": 9.684724807739258,
      "learning_rate": 3.740865225472972e-05,
      "loss": 2.1186,
      "step": 197800
    },
    {
      "epoch": 15.117256130165762,
      "grad_norm": 14.850240707397461,
      "learning_rate": 3.74022865581952e-05,
      "loss": 2.1261,
      "step": 197900
    },
    {
      "epoch": 15.124894966007181,
      "grad_norm": 13.277939796447754,
      "learning_rate": 3.7395920861660685e-05,
      "loss": 2.1317,
      "step": 198000
    },
    {
      "epoch": 15.132533801848599,
      "grad_norm": 14.226212501525879,
      "learning_rate": 3.7389555165126175e-05,
      "loss": 2.0916,
      "step": 198100
    },
    {
      "epoch": 15.140172637690016,
      "grad_norm": 12.425361633300781,
      "learning_rate": 3.738318946859166e-05,
      "loss": 2.2,
      "step": 198200
    },
    {
      "epoch": 15.147811473531434,
      "grad_norm": 10.778106689453125,
      "learning_rate": 3.737682377205714e-05,
      "loss": 2.1765,
      "step": 198300
    },
    {
      "epoch": 15.155450309372851,
      "grad_norm": 12.144451141357422,
      "learning_rate": 3.7370458075522626e-05,
      "loss": 2.1305,
      "step": 198400
    },
    {
      "epoch": 15.16308914521427,
      "grad_norm": 14.291565895080566,
      "learning_rate": 3.736409237898811e-05,
      "loss": 2.1482,
      "step": 198500
    },
    {
      "epoch": 15.170727981055688,
      "grad_norm": 11.188653945922852,
      "learning_rate": 3.73577266824536e-05,
      "loss": 2.1306,
      "step": 198600
    },
    {
      "epoch": 15.178366816897105,
      "grad_norm": 10.529557228088379,
      "learning_rate": 3.735136098591908e-05,
      "loss": 2.1649,
      "step": 198700
    },
    {
      "epoch": 15.186005652738523,
      "grad_norm": 11.447452545166016,
      "learning_rate": 3.7344995289384567e-05,
      "loss": 2.2094,
      "step": 198800
    },
    {
      "epoch": 15.19364448857994,
      "grad_norm": 11.771215438842773,
      "learning_rate": 3.733862959285005e-05,
      "loss": 2.2108,
      "step": 198900
    },
    {
      "epoch": 15.201283324421357,
      "grad_norm": 16.52143669128418,
      "learning_rate": 3.733226389631554e-05,
      "loss": 2.246,
      "step": 199000
    },
    {
      "epoch": 15.208922160262777,
      "grad_norm": 13.506881713867188,
      "learning_rate": 3.7325898199781024e-05,
      "loss": 2.2356,
      "step": 199100
    },
    {
      "epoch": 15.216560996104194,
      "grad_norm": 13.49264144897461,
      "learning_rate": 3.731953250324651e-05,
      "loss": 2.1742,
      "step": 199200
    },
    {
      "epoch": 15.224199831945612,
      "grad_norm": 13.15818977355957,
      "learning_rate": 3.731316680671199e-05,
      "loss": 2.1129,
      "step": 199300
    },
    {
      "epoch": 15.231838667787029,
      "grad_norm": 10.643621444702148,
      "learning_rate": 3.7306801110177475e-05,
      "loss": 2.1003,
      "step": 199400
    },
    {
      "epoch": 15.239477503628446,
      "grad_norm": 12.309159278869629,
      "learning_rate": 3.7300435413642965e-05,
      "loss": 2.1058,
      "step": 199500
    },
    {
      "epoch": 15.247116339469866,
      "grad_norm": 11.200448036193848,
      "learning_rate": 3.729406971710845e-05,
      "loss": 2.1238,
      "step": 199600
    },
    {
      "epoch": 15.254755175311283,
      "grad_norm": 13.882223129272461,
      "learning_rate": 3.728770402057393e-05,
      "loss": 2.1136,
      "step": 199700
    },
    {
      "epoch": 15.2623940111527,
      "grad_norm": 10.454594612121582,
      "learning_rate": 3.7281338324039415e-05,
      "loss": 2.1745,
      "step": 199800
    },
    {
      "epoch": 15.270032846994118,
      "grad_norm": 12.961606979370117,
      "learning_rate": 3.7274972627504906e-05,
      "loss": 2.0674,
      "step": 199900
    },
    {
      "epoch": 15.277671682835535,
      "grad_norm": 11.831300735473633,
      "learning_rate": 3.726860693097039e-05,
      "loss": 2.0956,
      "step": 200000
    },
    {
      "epoch": 15.285310518676953,
      "grad_norm": 14.955941200256348,
      "learning_rate": 3.726224123443587e-05,
      "loss": 2.1332,
      "step": 200100
    },
    {
      "epoch": 15.292949354518372,
      "grad_norm": 15.701264381408691,
      "learning_rate": 3.7255875537901356e-05,
      "loss": 2.0444,
      "step": 200200
    },
    {
      "epoch": 15.30058819035979,
      "grad_norm": 12.411001205444336,
      "learning_rate": 3.724950984136684e-05,
      "loss": 2.0737,
      "step": 200300
    },
    {
      "epoch": 15.308227026201207,
      "grad_norm": 13.853578567504883,
      "learning_rate": 3.724314414483233e-05,
      "loss": 2.1029,
      "step": 200400
    },
    {
      "epoch": 15.315865862042624,
      "grad_norm": 15.401276588439941,
      "learning_rate": 3.7236778448297814e-05,
      "loss": 2.147,
      "step": 200500
    },
    {
      "epoch": 15.323504697884042,
      "grad_norm": 9.041434288024902,
      "learning_rate": 3.72304127517633e-05,
      "loss": 2.0398,
      "step": 200600
    },
    {
      "epoch": 15.33114353372546,
      "grad_norm": 10.869949340820312,
      "learning_rate": 3.722404705522878e-05,
      "loss": 2.2285,
      "step": 200700
    },
    {
      "epoch": 15.338782369566879,
      "grad_norm": 14.250551223754883,
      "learning_rate": 3.7217681358694264e-05,
      "loss": 2.1978,
      "step": 200800
    },
    {
      "epoch": 15.346421205408296,
      "grad_norm": 13.317386627197266,
      "learning_rate": 3.7211315662159755e-05,
      "loss": 2.1149,
      "step": 200900
    },
    {
      "epoch": 15.354060041249713,
      "grad_norm": 10.830184936523438,
      "learning_rate": 3.720494996562524e-05,
      "loss": 2.1521,
      "step": 201000
    },
    {
      "epoch": 15.361698877091131,
      "grad_norm": 18.261070251464844,
      "learning_rate": 3.719858426909072e-05,
      "loss": 2.0964,
      "step": 201100
    },
    {
      "epoch": 15.369337712932548,
      "grad_norm": 12.422809600830078,
      "learning_rate": 3.719221857255621e-05,
      "loss": 2.186,
      "step": 201200
    },
    {
      "epoch": 15.376976548773968,
      "grad_norm": 12.425959587097168,
      "learning_rate": 3.7185852876021696e-05,
      "loss": 2.3065,
      "step": 201300
    },
    {
      "epoch": 15.384615384615385,
      "grad_norm": 12.046032905578613,
      "learning_rate": 3.717948717948718e-05,
      "loss": 2.1634,
      "step": 201400
    },
    {
      "epoch": 15.392254220456802,
      "grad_norm": 15.67604923248291,
      "learning_rate": 3.717312148295267e-05,
      "loss": 2.2197,
      "step": 201500
    },
    {
      "epoch": 15.39989305629822,
      "grad_norm": 14.005568504333496,
      "learning_rate": 3.716675578641815e-05,
      "loss": 2.1101,
      "step": 201600
    },
    {
      "epoch": 15.407531892139637,
      "grad_norm": 12.41108512878418,
      "learning_rate": 3.7160390089883637e-05,
      "loss": 2.1347,
      "step": 201700
    },
    {
      "epoch": 15.415170727981057,
      "grad_norm": 21.309629440307617,
      "learning_rate": 3.715402439334913e-05,
      "loss": 2.2279,
      "step": 201800
    },
    {
      "epoch": 15.422809563822474,
      "grad_norm": 15.558369636535645,
      "learning_rate": 3.714765869681461e-05,
      "loss": 2.2183,
      "step": 201900
    },
    {
      "epoch": 15.430448399663891,
      "grad_norm": 16.185388565063477,
      "learning_rate": 3.7141293000280094e-05,
      "loss": 2.2248,
      "step": 202000
    },
    {
      "epoch": 15.438087235505309,
      "grad_norm": 9.756138801574707,
      "learning_rate": 3.713492730374558e-05,
      "loss": 2.16,
      "step": 202100
    },
    {
      "epoch": 15.445726071346726,
      "grad_norm": 19.006895065307617,
      "learning_rate": 3.712856160721107e-05,
      "loss": 2.1809,
      "step": 202200
    },
    {
      "epoch": 15.453364907188144,
      "grad_norm": 11.991792678833008,
      "learning_rate": 3.712219591067655e-05,
      "loss": 2.1972,
      "step": 202300
    },
    {
      "epoch": 15.461003743029563,
      "grad_norm": 12.218226432800293,
      "learning_rate": 3.7115830214142035e-05,
      "loss": 2.0777,
      "step": 202400
    },
    {
      "epoch": 15.46864257887098,
      "grad_norm": 12.573463439941406,
      "learning_rate": 3.710946451760752e-05,
      "loss": 2.1404,
      "step": 202500
    },
    {
      "epoch": 15.476281414712398,
      "grad_norm": 14.747452735900879,
      "learning_rate": 3.7103098821073e-05,
      "loss": 2.2172,
      "step": 202600
    },
    {
      "epoch": 15.483920250553815,
      "grad_norm": 13.055932998657227,
      "learning_rate": 3.709673312453849e-05,
      "loss": 2.169,
      "step": 202700
    },
    {
      "epoch": 15.491559086395233,
      "grad_norm": 13.39217472076416,
      "learning_rate": 3.7090367428003976e-05,
      "loss": 2.1355,
      "step": 202800
    },
    {
      "epoch": 15.49919792223665,
      "grad_norm": 9.947547912597656,
      "learning_rate": 3.708400173146946e-05,
      "loss": 2.1836,
      "step": 202900
    },
    {
      "epoch": 15.50683675807807,
      "grad_norm": 12.23954963684082,
      "learning_rate": 3.707763603493494e-05,
      "loss": 2.1295,
      "step": 203000
    },
    {
      "epoch": 15.514475593919487,
      "grad_norm": 12.539525985717773,
      "learning_rate": 3.707127033840043e-05,
      "loss": 2.1703,
      "step": 203100
    },
    {
      "epoch": 15.522114429760904,
      "grad_norm": 10.748640060424805,
      "learning_rate": 3.706490464186592e-05,
      "loss": 2.1577,
      "step": 203200
    },
    {
      "epoch": 15.529753265602322,
      "grad_norm": 11.289464950561523,
      "learning_rate": 3.70585389453314e-05,
      "loss": 2.2196,
      "step": 203300
    },
    {
      "epoch": 15.53739210144374,
      "grad_norm": 11.667168617248535,
      "learning_rate": 3.7052173248796884e-05,
      "loss": 2.0713,
      "step": 203400
    },
    {
      "epoch": 15.545030937285159,
      "grad_norm": 11.833268165588379,
      "learning_rate": 3.704580755226237e-05,
      "loss": 2.1469,
      "step": 203500
    },
    {
      "epoch": 15.552669773126576,
      "grad_norm": 12.126291275024414,
      "learning_rate": 3.703944185572786e-05,
      "loss": 2.1793,
      "step": 203600
    },
    {
      "epoch": 15.560308608967993,
      "grad_norm": 9.656156539916992,
      "learning_rate": 3.703307615919334e-05,
      "loss": 2.1505,
      "step": 203700
    },
    {
      "epoch": 15.56794744480941,
      "grad_norm": 12.594526290893555,
      "learning_rate": 3.7026710462658825e-05,
      "loss": 2.1511,
      "step": 203800
    },
    {
      "epoch": 15.575586280650828,
      "grad_norm": 12.507973670959473,
      "learning_rate": 3.702034476612431e-05,
      "loss": 2.2493,
      "step": 203900
    },
    {
      "epoch": 15.583225116492248,
      "grad_norm": 11.57239818572998,
      "learning_rate": 3.701397906958979e-05,
      "loss": 2.0951,
      "step": 204000
    },
    {
      "epoch": 15.590863952333665,
      "grad_norm": 13.492258071899414,
      "learning_rate": 3.700761337305528e-05,
      "loss": 2.087,
      "step": 204100
    },
    {
      "epoch": 15.598502788175082,
      "grad_norm": 11.751713752746582,
      "learning_rate": 3.7001247676520766e-05,
      "loss": 2.112,
      "step": 204200
    },
    {
      "epoch": 15.6061416240165,
      "grad_norm": 13.594096183776855,
      "learning_rate": 3.699488197998625e-05,
      "loss": 2.1997,
      "step": 204300
    },
    {
      "epoch": 15.613780459857917,
      "grad_norm": 8.711906433105469,
      "learning_rate": 3.698851628345173e-05,
      "loss": 2.228,
      "step": 204400
    },
    {
      "epoch": 15.621419295699335,
      "grad_norm": 17.06834602355957,
      "learning_rate": 3.698215058691722e-05,
      "loss": 2.1783,
      "step": 204500
    },
    {
      "epoch": 15.629058131540754,
      "grad_norm": 13.440129280090332,
      "learning_rate": 3.6975784890382707e-05,
      "loss": 2.1098,
      "step": 204600
    },
    {
      "epoch": 15.636696967382171,
      "grad_norm": 13.833138465881348,
      "learning_rate": 3.696941919384819e-05,
      "loss": 2.1378,
      "step": 204700
    },
    {
      "epoch": 15.644335803223589,
      "grad_norm": 13.723859786987305,
      "learning_rate": 3.6963053497313674e-05,
      "loss": 2.1963,
      "step": 204800
    },
    {
      "epoch": 15.651974639065006,
      "grad_norm": 15.995585441589355,
      "learning_rate": 3.6956687800779164e-05,
      "loss": 2.1581,
      "step": 204900
    },
    {
      "epoch": 15.659613474906424,
      "grad_norm": 13.516495704650879,
      "learning_rate": 3.695032210424465e-05,
      "loss": 2.0629,
      "step": 205000
    },
    {
      "epoch": 15.667252310747841,
      "grad_norm": 14.800617218017578,
      "learning_rate": 3.694395640771013e-05,
      "loss": 2.1218,
      "step": 205100
    },
    {
      "epoch": 15.67489114658926,
      "grad_norm": 10.85538387298584,
      "learning_rate": 3.693759071117562e-05,
      "loss": 2.1657,
      "step": 205200
    },
    {
      "epoch": 15.682529982430678,
      "grad_norm": 10.432964324951172,
      "learning_rate": 3.6931225014641105e-05,
      "loss": 2.2471,
      "step": 205300
    },
    {
      "epoch": 15.690168818272095,
      "grad_norm": 15.18375301361084,
      "learning_rate": 3.692485931810659e-05,
      "loss": 2.1183,
      "step": 205400
    },
    {
      "epoch": 15.697807654113513,
      "grad_norm": 10.613359451293945,
      "learning_rate": 3.691849362157208e-05,
      "loss": 2.1378,
      "step": 205500
    },
    {
      "epoch": 15.70544648995493,
      "grad_norm": 10.829682350158691,
      "learning_rate": 3.691212792503756e-05,
      "loss": 2.1948,
      "step": 205600
    },
    {
      "epoch": 15.71308532579635,
      "grad_norm": 12.409266471862793,
      "learning_rate": 3.6905762228503046e-05,
      "loss": 2.1326,
      "step": 205700
    },
    {
      "epoch": 15.720724161637767,
      "grad_norm": 12.212350845336914,
      "learning_rate": 3.689939653196853e-05,
      "loss": 2.2023,
      "step": 205800
    },
    {
      "epoch": 15.728362997479184,
      "grad_norm": 15.146367073059082,
      "learning_rate": 3.689303083543402e-05,
      "loss": 2.1675,
      "step": 205900
    },
    {
      "epoch": 15.736001833320602,
      "grad_norm": 12.854686737060547,
      "learning_rate": 3.68866651388995e-05,
      "loss": 2.1416,
      "step": 206000
    },
    {
      "epoch": 15.74364066916202,
      "grad_norm": 8.559530258178711,
      "learning_rate": 3.688029944236499e-05,
      "loss": 2.0773,
      "step": 206100
    },
    {
      "epoch": 15.751279505003437,
      "grad_norm": 11.290271759033203,
      "learning_rate": 3.687393374583047e-05,
      "loss": 2.1255,
      "step": 206200
    },
    {
      "epoch": 15.758918340844856,
      "grad_norm": 12.92215633392334,
      "learning_rate": 3.6867568049295954e-05,
      "loss": 2.0937,
      "step": 206300
    },
    {
      "epoch": 15.766557176686273,
      "grad_norm": 13.20952033996582,
      "learning_rate": 3.6861202352761444e-05,
      "loss": 2.246,
      "step": 206400
    },
    {
      "epoch": 15.77419601252769,
      "grad_norm": 11.362266540527344,
      "learning_rate": 3.685483665622693e-05,
      "loss": 2.1233,
      "step": 206500
    },
    {
      "epoch": 15.781834848369108,
      "grad_norm": 13.512085914611816,
      "learning_rate": 3.684847095969241e-05,
      "loss": 2.0478,
      "step": 206600
    },
    {
      "epoch": 15.789473684210526,
      "grad_norm": 11.951586723327637,
      "learning_rate": 3.6842105263157895e-05,
      "loss": 2.203,
      "step": 206700
    },
    {
      "epoch": 15.797112520051945,
      "grad_norm": 12.392080307006836,
      "learning_rate": 3.6835739566623385e-05,
      "loss": 2.1292,
      "step": 206800
    },
    {
      "epoch": 15.804751355893362,
      "grad_norm": 16.035045623779297,
      "learning_rate": 3.682937387008887e-05,
      "loss": 2.1698,
      "step": 206900
    },
    {
      "epoch": 15.81239019173478,
      "grad_norm": 12.802123069763184,
      "learning_rate": 3.682300817355435e-05,
      "loss": 2.202,
      "step": 207000
    },
    {
      "epoch": 15.820029027576197,
      "grad_norm": 13.556523323059082,
      "learning_rate": 3.6816642477019836e-05,
      "loss": 2.1394,
      "step": 207100
    },
    {
      "epoch": 15.827667863417615,
      "grad_norm": 12.624262809753418,
      "learning_rate": 3.681027678048532e-05,
      "loss": 2.1144,
      "step": 207200
    },
    {
      "epoch": 15.835306699259032,
      "grad_norm": 10.73044204711914,
      "learning_rate": 3.680391108395081e-05,
      "loss": 2.2005,
      "step": 207300
    },
    {
      "epoch": 15.842945535100451,
      "grad_norm": 13.718304634094238,
      "learning_rate": 3.679754538741629e-05,
      "loss": 2.0578,
      "step": 207400
    },
    {
      "epoch": 15.850584370941869,
      "grad_norm": 13.470218658447266,
      "learning_rate": 3.6791179690881777e-05,
      "loss": 2.2593,
      "step": 207500
    },
    {
      "epoch": 15.858223206783286,
      "grad_norm": 12.086334228515625,
      "learning_rate": 3.678481399434726e-05,
      "loss": 2.1418,
      "step": 207600
    },
    {
      "epoch": 15.865862042624704,
      "grad_norm": 11.411468505859375,
      "learning_rate": 3.677844829781275e-05,
      "loss": 2.1956,
      "step": 207700
    },
    {
      "epoch": 15.873500878466121,
      "grad_norm": 9.465941429138184,
      "learning_rate": 3.6772082601278234e-05,
      "loss": 2.0878,
      "step": 207800
    },
    {
      "epoch": 15.88113971430754,
      "grad_norm": 11.943511009216309,
      "learning_rate": 3.676571690474372e-05,
      "loss": 2.2139,
      "step": 207900
    },
    {
      "epoch": 15.888778550148958,
      "grad_norm": 13.190025329589844,
      "learning_rate": 3.67593512082092e-05,
      "loss": 2.1564,
      "step": 208000
    },
    {
      "epoch": 15.896417385990375,
      "grad_norm": 15.04835033416748,
      "learning_rate": 3.6752985511674685e-05,
      "loss": 2.182,
      "step": 208100
    },
    {
      "epoch": 15.904056221831793,
      "grad_norm": 13.055715560913086,
      "learning_rate": 3.6746619815140175e-05,
      "loss": 2.1258,
      "step": 208200
    },
    {
      "epoch": 15.91169505767321,
      "grad_norm": 14.104351997375488,
      "learning_rate": 3.674025411860566e-05,
      "loss": 2.1632,
      "step": 208300
    },
    {
      "epoch": 15.919333893514628,
      "grad_norm": 9.848987579345703,
      "learning_rate": 3.673388842207114e-05,
      "loss": 2.1105,
      "step": 208400
    },
    {
      "epoch": 15.926972729356047,
      "grad_norm": 11.675895690917969,
      "learning_rate": 3.6727522725536625e-05,
      "loss": 2.1307,
      "step": 208500
    },
    {
      "epoch": 15.934611565197464,
      "grad_norm": 12.1397123336792,
      "learning_rate": 3.6721157029002116e-05,
      "loss": 2.1126,
      "step": 208600
    },
    {
      "epoch": 15.942250401038882,
      "grad_norm": 12.800797462463379,
      "learning_rate": 3.67147913324676e-05,
      "loss": 2.2562,
      "step": 208700
    },
    {
      "epoch": 15.9498892368803,
      "grad_norm": 13.499065399169922,
      "learning_rate": 3.670842563593308e-05,
      "loss": 2.1828,
      "step": 208800
    },
    {
      "epoch": 15.957528072721717,
      "grad_norm": 10.876072883605957,
      "learning_rate": 3.6702059939398566e-05,
      "loss": 2.0727,
      "step": 208900
    },
    {
      "epoch": 15.965166908563136,
      "grad_norm": 9.58495807647705,
      "learning_rate": 3.669569424286406e-05,
      "loss": 2.2384,
      "step": 209000
    },
    {
      "epoch": 15.972805744404553,
      "grad_norm": 12.250081062316895,
      "learning_rate": 3.668932854632954e-05,
      "loss": 2.1671,
      "step": 209100
    },
    {
      "epoch": 15.98044458024597,
      "grad_norm": 14.444337844848633,
      "learning_rate": 3.668296284979503e-05,
      "loss": 2.0871,
      "step": 209200
    },
    {
      "epoch": 15.988083416087388,
      "grad_norm": 11.915468215942383,
      "learning_rate": 3.6676597153260514e-05,
      "loss": 2.1375,
      "step": 209300
    },
    {
      "epoch": 15.995722251928806,
      "grad_norm": 12.238037109375,
      "learning_rate": 3.6670231456726e-05,
      "loss": 2.2005,
      "step": 209400
    },
    {
      "epoch": 16.0,
      "eval_loss": 2.0729143619537354,
      "eval_runtime": 1.6532,
      "eval_samples_per_second": 417.375,
      "eval_steps_per_second": 417.375,
      "step": 209456
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.9162770509719849,
      "eval_runtime": 31.4429,
      "eval_samples_per_second": 416.342,
      "eval_steps_per_second": 416.342,
      "step": 209456
    },
    {
      "epoch": 16.003361087770223,
      "grad_norm": 13.873867988586426,
      "learning_rate": 3.666386576019148e-05,
      "loss": 2.201,
      "step": 209500
    },
    {
      "epoch": 16.010999923611642,
      "grad_norm": 13.000877380371094,
      "learning_rate": 3.665750006365697e-05,
      "loss": 2.1356,
      "step": 209600
    },
    {
      "epoch": 16.018638759453058,
      "grad_norm": 10.020513534545898,
      "learning_rate": 3.6651134367122455e-05,
      "loss": 2.1105,
      "step": 209700
    },
    {
      "epoch": 16.026277595294477,
      "grad_norm": 14.363022804260254,
      "learning_rate": 3.664476867058794e-05,
      "loss": 2.1021,
      "step": 209800
    },
    {
      "epoch": 16.033916431135896,
      "grad_norm": 14.467839241027832,
      "learning_rate": 3.663840297405342e-05,
      "loss": 2.1075,
      "step": 209900
    },
    {
      "epoch": 16.041555266977312,
      "grad_norm": 12.645888328552246,
      "learning_rate": 3.663203727751891e-05,
      "loss": 2.103,
      "step": 210000
    },
    {
      "epoch": 16.04919410281873,
      "grad_norm": 12.701190948486328,
      "learning_rate": 3.6625671580984396e-05,
      "loss": 2.0636,
      "step": 210100
    },
    {
      "epoch": 16.056832938660147,
      "grad_norm": 8.996957778930664,
      "learning_rate": 3.661930588444988e-05,
      "loss": 2.2158,
      "step": 210200
    },
    {
      "epoch": 16.064471774501566,
      "grad_norm": 15.259638786315918,
      "learning_rate": 3.661294018791536e-05,
      "loss": 2.0192,
      "step": 210300
    },
    {
      "epoch": 16.072110610342985,
      "grad_norm": 12.902704238891602,
      "learning_rate": 3.6606574491380847e-05,
      "loss": 2.0515,
      "step": 210400
    },
    {
      "epoch": 16.0797494461844,
      "grad_norm": 13.933260917663574,
      "learning_rate": 3.660020879484634e-05,
      "loss": 2.1424,
      "step": 210500
    },
    {
      "epoch": 16.08738828202582,
      "grad_norm": 11.790661811828613,
      "learning_rate": 3.659384309831182e-05,
      "loss": 2.1726,
      "step": 210600
    },
    {
      "epoch": 16.095027117867236,
      "grad_norm": 12.107081413269043,
      "learning_rate": 3.6587477401777304e-05,
      "loss": 2.1751,
      "step": 210700
    },
    {
      "epoch": 16.102665953708655,
      "grad_norm": 13.713560104370117,
      "learning_rate": 3.658111170524279e-05,
      "loss": 2.0347,
      "step": 210800
    },
    {
      "epoch": 16.11030478955007,
      "grad_norm": 12.936334609985352,
      "learning_rate": 3.657474600870828e-05,
      "loss": 2.1324,
      "step": 210900
    },
    {
      "epoch": 16.11794362539149,
      "grad_norm": 13.244132041931152,
      "learning_rate": 3.656838031217376e-05,
      "loss": 2.0906,
      "step": 211000
    },
    {
      "epoch": 16.12558246123291,
      "grad_norm": 11.097580909729004,
      "learning_rate": 3.6562014615639245e-05,
      "loss": 2.1498,
      "step": 211100
    },
    {
      "epoch": 16.133221297074325,
      "grad_norm": 13.997064590454102,
      "learning_rate": 3.655564891910473e-05,
      "loss": 2.142,
      "step": 211200
    },
    {
      "epoch": 16.140860132915744,
      "grad_norm": 13.710309982299805,
      "learning_rate": 3.654928322257021e-05,
      "loss": 2.1301,
      "step": 211300
    },
    {
      "epoch": 16.14849896875716,
      "grad_norm": 19.010698318481445,
      "learning_rate": 3.65429175260357e-05,
      "loss": 2.1925,
      "step": 211400
    },
    {
      "epoch": 16.15613780459858,
      "grad_norm": 14.215954780578613,
      "learning_rate": 3.6536551829501186e-05,
      "loss": 2.1123,
      "step": 211500
    },
    {
      "epoch": 16.16377664044,
      "grad_norm": 13.420480728149414,
      "learning_rate": 3.653018613296667e-05,
      "loss": 2.1333,
      "step": 211600
    },
    {
      "epoch": 16.171415476281414,
      "grad_norm": 12.268409729003906,
      "learning_rate": 3.652382043643215e-05,
      "loss": 2.1221,
      "step": 211700
    },
    {
      "epoch": 16.179054312122833,
      "grad_norm": 14.222174644470215,
      "learning_rate": 3.651745473989764e-05,
      "loss": 2.0233,
      "step": 211800
    },
    {
      "epoch": 16.18669314796425,
      "grad_norm": 13.126059532165527,
      "learning_rate": 3.651108904336313e-05,
      "loss": 2.1647,
      "step": 211900
    },
    {
      "epoch": 16.194331983805668,
      "grad_norm": 11.36531925201416,
      "learning_rate": 3.650472334682861e-05,
      "loss": 2.1093,
      "step": 212000
    },
    {
      "epoch": 16.201970819647087,
      "grad_norm": 14.313443183898926,
      "learning_rate": 3.6498357650294094e-05,
      "loss": 2.1835,
      "step": 212100
    },
    {
      "epoch": 16.209609655488503,
      "grad_norm": 12.398269653320312,
      "learning_rate": 3.649199195375958e-05,
      "loss": 2.14,
      "step": 212200
    },
    {
      "epoch": 16.217248491329922,
      "grad_norm": 12.54822826385498,
      "learning_rate": 3.648562625722507e-05,
      "loss": 2.1329,
      "step": 212300
    },
    {
      "epoch": 16.224887327171338,
      "grad_norm": 12.532292366027832,
      "learning_rate": 3.647926056069055e-05,
      "loss": 2.1435,
      "step": 212400
    },
    {
      "epoch": 16.232526163012757,
      "grad_norm": 12.260537147521973,
      "learning_rate": 3.6472894864156035e-05,
      "loss": 2.1233,
      "step": 212500
    },
    {
      "epoch": 16.240164998854176,
      "grad_norm": 11.585461616516113,
      "learning_rate": 3.646652916762152e-05,
      "loss": 2.0381,
      "step": 212600
    },
    {
      "epoch": 16.247803834695592,
      "grad_norm": 12.185444831848145,
      "learning_rate": 3.646016347108701e-05,
      "loss": 2.0961,
      "step": 212700
    },
    {
      "epoch": 16.25544267053701,
      "grad_norm": 13.873574256896973,
      "learning_rate": 3.645379777455249e-05,
      "loss": 2.1491,
      "step": 212800
    },
    {
      "epoch": 16.263081506378427,
      "grad_norm": 11.959729194641113,
      "learning_rate": 3.6447432078017976e-05,
      "loss": 2.0342,
      "step": 212900
    },
    {
      "epoch": 16.270720342219846,
      "grad_norm": 11.646170616149902,
      "learning_rate": 3.6441066381483466e-05,
      "loss": 2.192,
      "step": 213000
    },
    {
      "epoch": 16.27835917806126,
      "grad_norm": 14.635065078735352,
      "learning_rate": 3.643470068494895e-05,
      "loss": 2.1131,
      "step": 213100
    },
    {
      "epoch": 16.28599801390268,
      "grad_norm": 12.234585762023926,
      "learning_rate": 3.642833498841444e-05,
      "loss": 2.0964,
      "step": 213200
    },
    {
      "epoch": 16.2936368497441,
      "grad_norm": 15.40322208404541,
      "learning_rate": 3.642196929187992e-05,
      "loss": 2.1872,
      "step": 213300
    },
    {
      "epoch": 16.301275685585516,
      "grad_norm": 11.229436874389648,
      "learning_rate": 3.641560359534541e-05,
      "loss": 2.1626,
      "step": 213400
    },
    {
      "epoch": 16.308914521426935,
      "grad_norm": 15.996947288513184,
      "learning_rate": 3.640923789881089e-05,
      "loss": 2.071,
      "step": 213500
    },
    {
      "epoch": 16.31655335726835,
      "grad_norm": 11.788522720336914,
      "learning_rate": 3.6402872202276374e-05,
      "loss": 2.1167,
      "step": 213600
    },
    {
      "epoch": 16.32419219310977,
      "grad_norm": 13.343548774719238,
      "learning_rate": 3.6396506505741864e-05,
      "loss": 2.1212,
      "step": 213700
    },
    {
      "epoch": 16.33183102895119,
      "grad_norm": 16.3294620513916,
      "learning_rate": 3.639014080920735e-05,
      "loss": 2.0651,
      "step": 213800
    },
    {
      "epoch": 16.339469864792605,
      "grad_norm": 12.480877876281738,
      "learning_rate": 3.638377511267283e-05,
      "loss": 2.2227,
      "step": 213900
    },
    {
      "epoch": 16.347108700634024,
      "grad_norm": 11.119718551635742,
      "learning_rate": 3.6377409416138315e-05,
      "loss": 2.2088,
      "step": 214000
    },
    {
      "epoch": 16.35474753647544,
      "grad_norm": 18.2623233795166,
      "learning_rate": 3.6371043719603805e-05,
      "loss": 2.1981,
      "step": 214100
    },
    {
      "epoch": 16.36238637231686,
      "grad_norm": 14.883655548095703,
      "learning_rate": 3.636467802306929e-05,
      "loss": 2.0593,
      "step": 214200
    },
    {
      "epoch": 16.370025208158278,
      "grad_norm": 9.25625991821289,
      "learning_rate": 3.635831232653477e-05,
      "loss": 2.1617,
      "step": 214300
    },
    {
      "epoch": 16.377664043999694,
      "grad_norm": 14.33054256439209,
      "learning_rate": 3.6351946630000256e-05,
      "loss": 2.2067,
      "step": 214400
    },
    {
      "epoch": 16.385302879841113,
      "grad_norm": 13.513447761535645,
      "learning_rate": 3.634558093346574e-05,
      "loss": 2.071,
      "step": 214500
    },
    {
      "epoch": 16.39294171568253,
      "grad_norm": 15.631813049316406,
      "learning_rate": 3.633921523693123e-05,
      "loss": 2.1979,
      "step": 214600
    },
    {
      "epoch": 16.400580551523948,
      "grad_norm": 17.224470138549805,
      "learning_rate": 3.633284954039671e-05,
      "loss": 2.1129,
      "step": 214700
    },
    {
      "epoch": 16.408219387365367,
      "grad_norm": 13.357924461364746,
      "learning_rate": 3.63264838438622e-05,
      "loss": 2.1292,
      "step": 214800
    },
    {
      "epoch": 16.415858223206783,
      "grad_norm": 11.621383666992188,
      "learning_rate": 3.632011814732768e-05,
      "loss": 2.1991,
      "step": 214900
    },
    {
      "epoch": 16.423497059048202,
      "grad_norm": 12.696731567382812,
      "learning_rate": 3.6313752450793164e-05,
      "loss": 2.1342,
      "step": 215000
    },
    {
      "epoch": 16.431135894889618,
      "grad_norm": 11.004683494567871,
      "learning_rate": 3.6307386754258654e-05,
      "loss": 2.081,
      "step": 215100
    },
    {
      "epoch": 16.438774730731037,
      "grad_norm": 15.440583229064941,
      "learning_rate": 3.630102105772414e-05,
      "loss": 2.171,
      "step": 215200
    },
    {
      "epoch": 16.446413566572453,
      "grad_norm": 11.374707221984863,
      "learning_rate": 3.629465536118962e-05,
      "loss": 2.1855,
      "step": 215300
    },
    {
      "epoch": 16.454052402413872,
      "grad_norm": 11.957903861999512,
      "learning_rate": 3.6288289664655105e-05,
      "loss": 2.2043,
      "step": 215400
    },
    {
      "epoch": 16.46169123825529,
      "grad_norm": 13.197442054748535,
      "learning_rate": 3.6281923968120595e-05,
      "loss": 2.1414,
      "step": 215500
    },
    {
      "epoch": 16.469330074096707,
      "grad_norm": 12.376931190490723,
      "learning_rate": 3.627555827158608e-05,
      "loss": 2.1543,
      "step": 215600
    },
    {
      "epoch": 16.476968909938126,
      "grad_norm": 13.127053260803223,
      "learning_rate": 3.626919257505156e-05,
      "loss": 2.0637,
      "step": 215700
    },
    {
      "epoch": 16.48460774577954,
      "grad_norm": 14.293994903564453,
      "learning_rate": 3.6262826878517046e-05,
      "loss": 2.1159,
      "step": 215800
    },
    {
      "epoch": 16.49224658162096,
      "grad_norm": 11.777029991149902,
      "learning_rate": 3.625646118198253e-05,
      "loss": 2.184,
      "step": 215900
    },
    {
      "epoch": 16.49988541746238,
      "grad_norm": 14.98167610168457,
      "learning_rate": 3.625009548544802e-05,
      "loss": 2.1301,
      "step": 216000
    },
    {
      "epoch": 16.507524253303796,
      "grad_norm": 11.892556190490723,
      "learning_rate": 3.62437297889135e-05,
      "loss": 2.1125,
      "step": 216100
    },
    {
      "epoch": 16.515163089145215,
      "grad_norm": 9.671723365783691,
      "learning_rate": 3.6237364092378987e-05,
      "loss": 2.0837,
      "step": 216200
    },
    {
      "epoch": 16.52280192498663,
      "grad_norm": 10.434066772460938,
      "learning_rate": 3.623099839584447e-05,
      "loss": 2.2418,
      "step": 216300
    },
    {
      "epoch": 16.53044076082805,
      "grad_norm": 12.129302024841309,
      "learning_rate": 3.622463269930996e-05,
      "loss": 2.167,
      "step": 216400
    },
    {
      "epoch": 16.53807959666947,
      "grad_norm": 16.253082275390625,
      "learning_rate": 3.6218267002775444e-05,
      "loss": 2.1656,
      "step": 216500
    },
    {
      "epoch": 16.545718432510885,
      "grad_norm": 16.670269012451172,
      "learning_rate": 3.621190130624093e-05,
      "loss": 2.1185,
      "step": 216600
    },
    {
      "epoch": 16.553357268352304,
      "grad_norm": 11.667023658752441,
      "learning_rate": 3.620553560970642e-05,
      "loss": 2.2829,
      "step": 216700
    },
    {
      "epoch": 16.56099610419372,
      "grad_norm": 13.203851699829102,
      "learning_rate": 3.61991699131719e-05,
      "loss": 2.0907,
      "step": 216800
    },
    {
      "epoch": 16.56863494003514,
      "grad_norm": 13.058185577392578,
      "learning_rate": 3.6192804216637385e-05,
      "loss": 2.0267,
      "step": 216900
    },
    {
      "epoch": 16.576273775876558,
      "grad_norm": 11.564288139343262,
      "learning_rate": 3.6186438520102875e-05,
      "loss": 2.0469,
      "step": 217000
    },
    {
      "epoch": 16.583912611717974,
      "grad_norm": 17.183134078979492,
      "learning_rate": 3.618007282356836e-05,
      "loss": 2.0586,
      "step": 217100
    },
    {
      "epoch": 16.591551447559393,
      "grad_norm": 15.428696632385254,
      "learning_rate": 3.617370712703384e-05,
      "loss": 2.1984,
      "step": 217200
    },
    {
      "epoch": 16.59919028340081,
      "grad_norm": 8.835199356079102,
      "learning_rate": 3.616734143049933e-05,
      "loss": 2.1163,
      "step": 217300
    },
    {
      "epoch": 16.606829119242228,
      "grad_norm": 12.924062728881836,
      "learning_rate": 3.6160975733964816e-05,
      "loss": 2.1141,
      "step": 217400
    },
    {
      "epoch": 16.614467955083644,
      "grad_norm": 12.308679580688477,
      "learning_rate": 3.61546100374303e-05,
      "loss": 2.1226,
      "step": 217500
    },
    {
      "epoch": 16.622106790925063,
      "grad_norm": 13.252886772155762,
      "learning_rate": 3.614824434089578e-05,
      "loss": 2.1528,
      "step": 217600
    },
    {
      "epoch": 16.629745626766482,
      "grad_norm": 13.061787605285645,
      "learning_rate": 3.614187864436127e-05,
      "loss": 2.2538,
      "step": 217700
    },
    {
      "epoch": 16.637384462607898,
      "grad_norm": 14.412225723266602,
      "learning_rate": 3.613551294782676e-05,
      "loss": 2.2789,
      "step": 217800
    },
    {
      "epoch": 16.645023298449317,
      "grad_norm": 14.05652904510498,
      "learning_rate": 3.612914725129224e-05,
      "loss": 2.1158,
      "step": 217900
    },
    {
      "epoch": 16.652662134290733,
      "grad_norm": 12.3207426071167,
      "learning_rate": 3.6122781554757724e-05,
      "loss": 2.1592,
      "step": 218000
    },
    {
      "epoch": 16.660300970132152,
      "grad_norm": 10.9146728515625,
      "learning_rate": 3.611641585822321e-05,
      "loss": 2.0907,
      "step": 218100
    },
    {
      "epoch": 16.66793980597357,
      "grad_norm": 14.109923362731934,
      "learning_rate": 3.611005016168869e-05,
      "loss": 2.125,
      "step": 218200
    },
    {
      "epoch": 16.675578641814987,
      "grad_norm": 14.993022918701172,
      "learning_rate": 3.610368446515418e-05,
      "loss": 1.9836,
      "step": 218300
    },
    {
      "epoch": 16.683217477656406,
      "grad_norm": 12.49057674407959,
      "learning_rate": 3.6097318768619665e-05,
      "loss": 2.0712,
      "step": 218400
    },
    {
      "epoch": 16.69085631349782,
      "grad_norm": 12.63706111907959,
      "learning_rate": 3.609095307208515e-05,
      "loss": 2.1312,
      "step": 218500
    },
    {
      "epoch": 16.69849514933924,
      "grad_norm": 10.709574699401855,
      "learning_rate": 3.608458737555063e-05,
      "loss": 2.0344,
      "step": 218600
    },
    {
      "epoch": 16.70613398518066,
      "grad_norm": 14.709465980529785,
      "learning_rate": 3.607822167901612e-05,
      "loss": 2.1043,
      "step": 218700
    },
    {
      "epoch": 16.713772821022076,
      "grad_norm": 15.726107597351074,
      "learning_rate": 3.6071855982481606e-05,
      "loss": 2.238,
      "step": 218800
    },
    {
      "epoch": 16.721411656863495,
      "grad_norm": 13.185787200927734,
      "learning_rate": 3.606549028594709e-05,
      "loss": 2.1875,
      "step": 218900
    },
    {
      "epoch": 16.72905049270491,
      "grad_norm": 11.603071212768555,
      "learning_rate": 3.605912458941257e-05,
      "loss": 2.1034,
      "step": 219000
    },
    {
      "epoch": 16.73668932854633,
      "grad_norm": 8.962105751037598,
      "learning_rate": 3.6052758892878057e-05,
      "loss": 2.0841,
      "step": 219100
    },
    {
      "epoch": 16.744328164387746,
      "grad_norm": 12.395968437194824,
      "learning_rate": 3.604639319634355e-05,
      "loss": 2.125,
      "step": 219200
    },
    {
      "epoch": 16.751967000229165,
      "grad_norm": 12.90219783782959,
      "learning_rate": 3.604002749980903e-05,
      "loss": 2.0803,
      "step": 219300
    },
    {
      "epoch": 16.759605836070584,
      "grad_norm": 14.047609329223633,
      "learning_rate": 3.6033661803274514e-05,
      "loss": 2.1497,
      "step": 219400
    },
    {
      "epoch": 16.767244671912,
      "grad_norm": 13.946245193481445,
      "learning_rate": 3.602729610674e-05,
      "loss": 2.1995,
      "step": 219500
    },
    {
      "epoch": 16.77488350775342,
      "grad_norm": 12.247198104858398,
      "learning_rate": 3.602093041020549e-05,
      "loss": 2.1009,
      "step": 219600
    },
    {
      "epoch": 16.782522343594835,
      "grad_norm": 11.465847969055176,
      "learning_rate": 3.601456471367097e-05,
      "loss": 2.0963,
      "step": 219700
    },
    {
      "epoch": 16.790161179436254,
      "grad_norm": 12.649885177612305,
      "learning_rate": 3.6008199017136455e-05,
      "loss": 2.1263,
      "step": 219800
    },
    {
      "epoch": 16.797800015277673,
      "grad_norm": 16.726293563842773,
      "learning_rate": 3.600183332060194e-05,
      "loss": 2.1411,
      "step": 219900
    },
    {
      "epoch": 16.80543885111909,
      "grad_norm": 11.822035789489746,
      "learning_rate": 3.599546762406742e-05,
      "loss": 2.1708,
      "step": 220000
    },
    {
      "epoch": 16.813077686960508,
      "grad_norm": 10.366186141967773,
      "learning_rate": 3.598910192753291e-05,
      "loss": 2.2059,
      "step": 220100
    },
    {
      "epoch": 16.820716522801924,
      "grad_norm": 13.179232597351074,
      "learning_rate": 3.5982736230998396e-05,
      "loss": 2.1025,
      "step": 220200
    },
    {
      "epoch": 16.828355358643343,
      "grad_norm": 12.856365203857422,
      "learning_rate": 3.597637053446388e-05,
      "loss": 2.1688,
      "step": 220300
    },
    {
      "epoch": 16.835994194484762,
      "grad_norm": 7.704110622406006,
      "learning_rate": 3.597000483792937e-05,
      "loss": 2.1703,
      "step": 220400
    },
    {
      "epoch": 16.843633030326178,
      "grad_norm": 11.468053817749023,
      "learning_rate": 3.596363914139485e-05,
      "loss": 2.1739,
      "step": 220500
    },
    {
      "epoch": 16.851271866167597,
      "grad_norm": 11.67102336883545,
      "learning_rate": 3.595727344486034e-05,
      "loss": 2.1075,
      "step": 220600
    },
    {
      "epoch": 16.858910702009013,
      "grad_norm": 11.327106475830078,
      "learning_rate": 3.595090774832583e-05,
      "loss": 2.11,
      "step": 220700
    },
    {
      "epoch": 16.86654953785043,
      "grad_norm": 10.406511306762695,
      "learning_rate": 3.594454205179131e-05,
      "loss": 2.0372,
      "step": 220800
    },
    {
      "epoch": 16.87418837369185,
      "grad_norm": 14.101410865783691,
      "learning_rate": 3.5938176355256794e-05,
      "loss": 2.0789,
      "step": 220900
    },
    {
      "epoch": 16.881827209533267,
      "grad_norm": 9.835273742675781,
      "learning_rate": 3.5931810658722284e-05,
      "loss": 2.1437,
      "step": 221000
    },
    {
      "epoch": 16.889466045374686,
      "grad_norm": 11.716948509216309,
      "learning_rate": 3.592544496218777e-05,
      "loss": 2.112,
      "step": 221100
    },
    {
      "epoch": 16.8971048812161,
      "grad_norm": 10.226289749145508,
      "learning_rate": 3.591907926565325e-05,
      "loss": 2.219,
      "step": 221200
    },
    {
      "epoch": 16.90474371705752,
      "grad_norm": 13.859461784362793,
      "learning_rate": 3.5912713569118735e-05,
      "loss": 2.1357,
      "step": 221300
    },
    {
      "epoch": 16.91238255289894,
      "grad_norm": 18.131559371948242,
      "learning_rate": 3.590634787258422e-05,
      "loss": 2.1225,
      "step": 221400
    },
    {
      "epoch": 16.920021388740356,
      "grad_norm": 13.898797988891602,
      "learning_rate": 3.589998217604971e-05,
      "loss": 2.1279,
      "step": 221500
    },
    {
      "epoch": 16.927660224581775,
      "grad_norm": 12.072751998901367,
      "learning_rate": 3.589361647951519e-05,
      "loss": 2.2164,
      "step": 221600
    },
    {
      "epoch": 16.93529906042319,
      "grad_norm": 16.808713912963867,
      "learning_rate": 3.5887250782980676e-05,
      "loss": 2.1183,
      "step": 221700
    },
    {
      "epoch": 16.94293789626461,
      "grad_norm": 11.489182472229004,
      "learning_rate": 3.588088508644616e-05,
      "loss": 2.1715,
      "step": 221800
    },
    {
      "epoch": 16.950576732106025,
      "grad_norm": 13.31653881072998,
      "learning_rate": 3.587451938991165e-05,
      "loss": 2.1123,
      "step": 221900
    },
    {
      "epoch": 16.958215567947445,
      "grad_norm": 10.74730110168457,
      "learning_rate": 3.586815369337713e-05,
      "loss": 2.0474,
      "step": 222000
    },
    {
      "epoch": 16.965854403788864,
      "grad_norm": 14.211461067199707,
      "learning_rate": 3.586178799684262e-05,
      "loss": 2.0349,
      "step": 222100
    },
    {
      "epoch": 16.97349323963028,
      "grad_norm": 15.700240135192871,
      "learning_rate": 3.58554223003081e-05,
      "loss": 2.1302,
      "step": 222200
    },
    {
      "epoch": 16.9811320754717,
      "grad_norm": 11.558380126953125,
      "learning_rate": 3.5849056603773584e-05,
      "loss": 2.096,
      "step": 222300
    },
    {
      "epoch": 16.988770911313114,
      "grad_norm": 10.984417915344238,
      "learning_rate": 3.5842690907239074e-05,
      "loss": 2.09,
      "step": 222400
    },
    {
      "epoch": 16.996409747154534,
      "grad_norm": 11.216231346130371,
      "learning_rate": 3.583632521070456e-05,
      "loss": 2.0508,
      "step": 222500
    },
    {
      "epoch": 17.0,
      "eval_loss": 2.0714995861053467,
      "eval_runtime": 1.6603,
      "eval_samples_per_second": 415.584,
      "eval_steps_per_second": 415.584,
      "step": 222547
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.9058740139007568,
      "eval_runtime": 31.571,
      "eval_samples_per_second": 414.653,
      "eval_steps_per_second": 414.653,
      "step": 222547
    },
    {
      "epoch": 17.004048582995953,
      "grad_norm": 14.67715835571289,
      "learning_rate": 3.582995951417004e-05,
      "loss": 2.1038,
      "step": 222600
    },
    {
      "epoch": 17.01168741883737,
      "grad_norm": 13.027698516845703,
      "learning_rate": 3.5823593817635525e-05,
      "loss": 2.172,
      "step": 222700
    },
    {
      "epoch": 17.019326254678788,
      "grad_norm": 11.945464134216309,
      "learning_rate": 3.5817228121101015e-05,
      "loss": 2.0666,
      "step": 222800
    },
    {
      "epoch": 17.026965090520203,
      "grad_norm": 12.02793025970459,
      "learning_rate": 3.58108624245665e-05,
      "loss": 2.0855,
      "step": 222900
    },
    {
      "epoch": 17.034603926361623,
      "grad_norm": 14.223761558532715,
      "learning_rate": 3.580449672803198e-05,
      "loss": 2.0674,
      "step": 223000
    },
    {
      "epoch": 17.042242762203042,
      "grad_norm": 10.819985389709473,
      "learning_rate": 3.5798131031497466e-05,
      "loss": 2.114,
      "step": 223100
    },
    {
      "epoch": 17.049881598044458,
      "grad_norm": 13.3596773147583,
      "learning_rate": 3.579176533496295e-05,
      "loss": 2.0335,
      "step": 223200
    },
    {
      "epoch": 17.057520433885877,
      "grad_norm": 15.041101455688477,
      "learning_rate": 3.578539963842844e-05,
      "loss": 2.0859,
      "step": 223300
    },
    {
      "epoch": 17.065159269727292,
      "grad_norm": 12.965655326843262,
      "learning_rate": 3.577903394189392e-05,
      "loss": 2.0542,
      "step": 223400
    },
    {
      "epoch": 17.07279810556871,
      "grad_norm": 15.50722599029541,
      "learning_rate": 3.577266824535941e-05,
      "loss": 2.0862,
      "step": 223500
    },
    {
      "epoch": 17.080436941410127,
      "grad_norm": 12.202534675598145,
      "learning_rate": 3.576630254882489e-05,
      "loss": 2.1159,
      "step": 223600
    },
    {
      "epoch": 17.088075777251547,
      "grad_norm": 15.86054515838623,
      "learning_rate": 3.5759936852290374e-05,
      "loss": 2.1324,
      "step": 223700
    },
    {
      "epoch": 17.095714613092966,
      "grad_norm": 10.915310859680176,
      "learning_rate": 3.5753571155755864e-05,
      "loss": 2.0462,
      "step": 223800
    },
    {
      "epoch": 17.10335344893438,
      "grad_norm": 14.000845909118652,
      "learning_rate": 3.574720545922135e-05,
      "loss": 2.0557,
      "step": 223900
    },
    {
      "epoch": 17.1109922847758,
      "grad_norm": 11.784231185913086,
      "learning_rate": 3.574083976268683e-05,
      "loss": 2.143,
      "step": 224000
    },
    {
      "epoch": 17.118631120617216,
      "grad_norm": 12.384053230285645,
      "learning_rate": 3.5734474066152315e-05,
      "loss": 2.0959,
      "step": 224100
    },
    {
      "epoch": 17.126269956458636,
      "grad_norm": 11.5087308883667,
      "learning_rate": 3.5728108369617805e-05,
      "loss": 2.0697,
      "step": 224200
    },
    {
      "epoch": 17.133908792300055,
      "grad_norm": 8.605622291564941,
      "learning_rate": 3.572174267308329e-05,
      "loss": 2.0855,
      "step": 224300
    },
    {
      "epoch": 17.14154762814147,
      "grad_norm": 11.836276054382324,
      "learning_rate": 3.571537697654878e-05,
      "loss": 2.0978,
      "step": 224400
    },
    {
      "epoch": 17.14918646398289,
      "grad_norm": 12.477705955505371,
      "learning_rate": 3.570901128001426e-05,
      "loss": 2.1038,
      "step": 224500
    },
    {
      "epoch": 17.156825299824305,
      "grad_norm": 13.139780044555664,
      "learning_rate": 3.5702645583479746e-05,
      "loss": 2.1287,
      "step": 224600
    },
    {
      "epoch": 17.164464135665725,
      "grad_norm": 13.481766700744629,
      "learning_rate": 3.5696279886945236e-05,
      "loss": 2.061,
      "step": 224700
    },
    {
      "epoch": 17.172102971507144,
      "grad_norm": 11.83916187286377,
      "learning_rate": 3.568991419041072e-05,
      "loss": 2.1102,
      "step": 224800
    },
    {
      "epoch": 17.17974180734856,
      "grad_norm": 14.632420539855957,
      "learning_rate": 3.56835484938762e-05,
      "loss": 2.1096,
      "step": 224900
    },
    {
      "epoch": 17.18738064318998,
      "grad_norm": 15.424025535583496,
      "learning_rate": 3.567718279734169e-05,
      "loss": 2.1413,
      "step": 225000
    },
    {
      "epoch": 17.195019479031394,
      "grad_norm": 13.60554027557373,
      "learning_rate": 3.567081710080718e-05,
      "loss": 2.0314,
      "step": 225100
    },
    {
      "epoch": 17.202658314872814,
      "grad_norm": 14.480573654174805,
      "learning_rate": 3.566445140427266e-05,
      "loss": 2.1572,
      "step": 225200
    },
    {
      "epoch": 17.210297150714233,
      "grad_norm": 12.560161590576172,
      "learning_rate": 3.5658085707738144e-05,
      "loss": 2.1531,
      "step": 225300
    },
    {
      "epoch": 17.21793598655565,
      "grad_norm": 15.018614768981934,
      "learning_rate": 3.565172001120363e-05,
      "loss": 2.0474,
      "step": 225400
    },
    {
      "epoch": 17.225574822397068,
      "grad_norm": 9.963353157043457,
      "learning_rate": 3.564535431466911e-05,
      "loss": 2.0807,
      "step": 225500
    },
    {
      "epoch": 17.233213658238483,
      "grad_norm": 13.326916694641113,
      "learning_rate": 3.56389886181346e-05,
      "loss": 2.1005,
      "step": 225600
    },
    {
      "epoch": 17.240852494079903,
      "grad_norm": 11.155878067016602,
      "learning_rate": 3.5632622921600085e-05,
      "loss": 2.0996,
      "step": 225700
    },
    {
      "epoch": 17.24849132992132,
      "grad_norm": 11.968658447265625,
      "learning_rate": 3.562625722506557e-05,
      "loss": 2.0722,
      "step": 225800
    },
    {
      "epoch": 17.256130165762738,
      "grad_norm": 12.732179641723633,
      "learning_rate": 3.561989152853105e-05,
      "loss": 2.1618,
      "step": 225900
    },
    {
      "epoch": 17.263769001604157,
      "grad_norm": 13.217844009399414,
      "learning_rate": 3.561352583199654e-05,
      "loss": 2.0953,
      "step": 226000
    },
    {
      "epoch": 17.271407837445572,
      "grad_norm": 13.582860946655273,
      "learning_rate": 3.5607160135462026e-05,
      "loss": 2.1349,
      "step": 226100
    },
    {
      "epoch": 17.27904667328699,
      "grad_norm": 11.389851570129395,
      "learning_rate": 3.560079443892751e-05,
      "loss": 2.117,
      "step": 226200
    },
    {
      "epoch": 17.286685509128407,
      "grad_norm": 14.901081085205078,
      "learning_rate": 3.559442874239299e-05,
      "loss": 2.066,
      "step": 226300
    },
    {
      "epoch": 17.294324344969827,
      "grad_norm": 13.845561027526855,
      "learning_rate": 3.558806304585848e-05,
      "loss": 2.1002,
      "step": 226400
    },
    {
      "epoch": 17.301963180811246,
      "grad_norm": 13.490626335144043,
      "learning_rate": 3.558169734932397e-05,
      "loss": 2.098,
      "step": 226500
    },
    {
      "epoch": 17.30960201665266,
      "grad_norm": 11.518848419189453,
      "learning_rate": 3.557533165278945e-05,
      "loss": 2.125,
      "step": 226600
    },
    {
      "epoch": 17.31724085249408,
      "grad_norm": 15.446636199951172,
      "learning_rate": 3.5568965956254934e-05,
      "loss": 2.1535,
      "step": 226700
    },
    {
      "epoch": 17.324879688335496,
      "grad_norm": 12.765968322753906,
      "learning_rate": 3.556260025972042e-05,
      "loss": 2.1291,
      "step": 226800
    },
    {
      "epoch": 17.332518524176916,
      "grad_norm": 11.905295372009277,
      "learning_rate": 3.55562345631859e-05,
      "loss": 2.1093,
      "step": 226900
    },
    {
      "epoch": 17.340157360018335,
      "grad_norm": 13.57815170288086,
      "learning_rate": 3.554986886665139e-05,
      "loss": 2.1479,
      "step": 227000
    },
    {
      "epoch": 17.34779619585975,
      "grad_norm": 11.51270580291748,
      "learning_rate": 3.5543503170116875e-05,
      "loss": 2.0834,
      "step": 227100
    },
    {
      "epoch": 17.35543503170117,
      "grad_norm": 12.293787002563477,
      "learning_rate": 3.553713747358236e-05,
      "loss": 2.0894,
      "step": 227200
    },
    {
      "epoch": 17.363073867542585,
      "grad_norm": 12.002008438110352,
      "learning_rate": 3.553077177704784e-05,
      "loss": 2.1418,
      "step": 227300
    },
    {
      "epoch": 17.370712703384005,
      "grad_norm": 14.87354564666748,
      "learning_rate": 3.552440608051333e-05,
      "loss": 2.1333,
      "step": 227400
    },
    {
      "epoch": 17.378351539225424,
      "grad_norm": 16.121070861816406,
      "learning_rate": 3.5518040383978816e-05,
      "loss": 2.0592,
      "step": 227500
    },
    {
      "epoch": 17.38599037506684,
      "grad_norm": 12.239392280578613,
      "learning_rate": 3.55116746874443e-05,
      "loss": 2.1528,
      "step": 227600
    },
    {
      "epoch": 17.39362921090826,
      "grad_norm": 13.914753913879395,
      "learning_rate": 3.550530899090978e-05,
      "loss": 2.1264,
      "step": 227700
    },
    {
      "epoch": 17.401268046749674,
      "grad_norm": 13.598938941955566,
      "learning_rate": 3.5498943294375267e-05,
      "loss": 2.0431,
      "step": 227800
    },
    {
      "epoch": 17.408906882591094,
      "grad_norm": 13.526383399963379,
      "learning_rate": 3.549257759784076e-05,
      "loss": 2.0907,
      "step": 227900
    },
    {
      "epoch": 17.41654571843251,
      "grad_norm": 14.455870628356934,
      "learning_rate": 3.548621190130624e-05,
      "loss": 2.1866,
      "step": 228000
    },
    {
      "epoch": 17.42418455427393,
      "grad_norm": 15.162615776062012,
      "learning_rate": 3.5479846204771724e-05,
      "loss": 2.1212,
      "step": 228100
    },
    {
      "epoch": 17.431823390115348,
      "grad_norm": 11.99301815032959,
      "learning_rate": 3.5473480508237214e-05,
      "loss": 2.1246,
      "step": 228200
    },
    {
      "epoch": 17.439462225956763,
      "grad_norm": 12.224287033081055,
      "learning_rate": 3.54671148117027e-05,
      "loss": 2.0328,
      "step": 228300
    },
    {
      "epoch": 17.447101061798183,
      "grad_norm": 16.344249725341797,
      "learning_rate": 3.546074911516819e-05,
      "loss": 2.1063,
      "step": 228400
    },
    {
      "epoch": 17.454739897639598,
      "grad_norm": 11.678057670593262,
      "learning_rate": 3.545438341863367e-05,
      "loss": 2.041,
      "step": 228500
    },
    {
      "epoch": 17.462378733481017,
      "grad_norm": 12.48201847076416,
      "learning_rate": 3.5448017722099155e-05,
      "loss": 2.0486,
      "step": 228600
    },
    {
      "epoch": 17.470017569322437,
      "grad_norm": 11.456225395202637,
      "learning_rate": 3.544165202556464e-05,
      "loss": 2.122,
      "step": 228700
    },
    {
      "epoch": 17.477656405163852,
      "grad_norm": 11.797414779663086,
      "learning_rate": 3.543528632903013e-05,
      "loss": 2.1547,
      "step": 228800
    },
    {
      "epoch": 17.48529524100527,
      "grad_norm": 10.74299430847168,
      "learning_rate": 3.542892063249561e-05,
      "loss": 2.0808,
      "step": 228900
    },
    {
      "epoch": 17.492934076846687,
      "grad_norm": 15.908602714538574,
      "learning_rate": 3.5422554935961096e-05,
      "loss": 2.1234,
      "step": 229000
    },
    {
      "epoch": 17.500572912688106,
      "grad_norm": 10.946669578552246,
      "learning_rate": 3.541618923942658e-05,
      "loss": 2.1432,
      "step": 229100
    },
    {
      "epoch": 17.508211748529526,
      "grad_norm": 12.97157096862793,
      "learning_rate": 3.540982354289207e-05,
      "loss": 2.0094,
      "step": 229200
    },
    {
      "epoch": 17.51585058437094,
      "grad_norm": 8.197916984558105,
      "learning_rate": 3.5403457846357553e-05,
      "loss": 2.0396,
      "step": 229300
    },
    {
      "epoch": 17.52348942021236,
      "grad_norm": 14.037379264831543,
      "learning_rate": 3.539709214982304e-05,
      "loss": 2.1769,
      "step": 229400
    },
    {
      "epoch": 17.531128256053776,
      "grad_norm": 13.139705657958984,
      "learning_rate": 3.539072645328852e-05,
      "loss": 2.1089,
      "step": 229500
    },
    {
      "epoch": 17.538767091895195,
      "grad_norm": 17.12495994567871,
      "learning_rate": 3.5384360756754004e-05,
      "loss": 2.2119,
      "step": 229600
    },
    {
      "epoch": 17.546405927736615,
      "grad_norm": 9.690494537353516,
      "learning_rate": 3.5377995060219494e-05,
      "loss": 2.0642,
      "step": 229700
    },
    {
      "epoch": 17.55404476357803,
      "grad_norm": 13.266286849975586,
      "learning_rate": 3.537162936368498e-05,
      "loss": 2.1086,
      "step": 229800
    },
    {
      "epoch": 17.56168359941945,
      "grad_norm": 12.505117416381836,
      "learning_rate": 3.536526366715046e-05,
      "loss": 2.0835,
      "step": 229900
    },
    {
      "epoch": 17.569322435260865,
      "grad_norm": 12.928618431091309,
      "learning_rate": 3.5358897970615945e-05,
      "loss": 2.0929,
      "step": 230000
    },
    {
      "epoch": 17.576961271102284,
      "grad_norm": 16.195423126220703,
      "learning_rate": 3.535253227408143e-05,
      "loss": 2.1183,
      "step": 230100
    },
    {
      "epoch": 17.5846001069437,
      "grad_norm": 13.66006088256836,
      "learning_rate": 3.534616657754692e-05,
      "loss": 2.128,
      "step": 230200
    },
    {
      "epoch": 17.59223894278512,
      "grad_norm": 8.594844818115234,
      "learning_rate": 3.53398008810124e-05,
      "loss": 2.0342,
      "step": 230300
    },
    {
      "epoch": 17.59987777862654,
      "grad_norm": 13.600167274475098,
      "learning_rate": 3.5333435184477886e-05,
      "loss": 2.107,
      "step": 230400
    },
    {
      "epoch": 17.607516614467954,
      "grad_norm": 10.791810989379883,
      "learning_rate": 3.532706948794337e-05,
      "loss": 2.2315,
      "step": 230500
    },
    {
      "epoch": 17.615155450309373,
      "grad_norm": 16.71078109741211,
      "learning_rate": 3.532070379140886e-05,
      "loss": 2.0936,
      "step": 230600
    },
    {
      "epoch": 17.62279428615079,
      "grad_norm": 11.590099334716797,
      "learning_rate": 3.531433809487434e-05,
      "loss": 2.1729,
      "step": 230700
    },
    {
      "epoch": 17.63043312199221,
      "grad_norm": 14.987615585327148,
      "learning_rate": 3.530797239833983e-05,
      "loss": 2.1226,
      "step": 230800
    },
    {
      "epoch": 17.638071957833628,
      "grad_norm": 14.893168449401855,
      "learning_rate": 3.530160670180531e-05,
      "loss": 2.1132,
      "step": 230900
    },
    {
      "epoch": 17.645710793675043,
      "grad_norm": 12.117257118225098,
      "learning_rate": 3.5295241005270794e-05,
      "loss": 2.082,
      "step": 231000
    },
    {
      "epoch": 17.653349629516462,
      "grad_norm": 12.703852653503418,
      "learning_rate": 3.5288875308736284e-05,
      "loss": 2.1718,
      "step": 231100
    },
    {
      "epoch": 17.660988465357878,
      "grad_norm": 16.294452667236328,
      "learning_rate": 3.528250961220177e-05,
      "loss": 2.1557,
      "step": 231200
    },
    {
      "epoch": 17.668627301199297,
      "grad_norm": 13.103028297424316,
      "learning_rate": 3.527614391566725e-05,
      "loss": 2.1382,
      "step": 231300
    },
    {
      "epoch": 17.676266137040717,
      "grad_norm": 12.747329711914062,
      "learning_rate": 3.5269778219132735e-05,
      "loss": 2.1146,
      "step": 231400
    },
    {
      "epoch": 17.683904972882132,
      "grad_norm": 13.421841621398926,
      "learning_rate": 3.5263412522598225e-05,
      "loss": 2.0916,
      "step": 231500
    },
    {
      "epoch": 17.69154380872355,
      "grad_norm": 11.585070610046387,
      "learning_rate": 3.525704682606371e-05,
      "loss": 2.1395,
      "step": 231600
    },
    {
      "epoch": 17.699182644564967,
      "grad_norm": 14.347378730773926,
      "learning_rate": 3.525068112952919e-05,
      "loss": 2.1273,
      "step": 231700
    },
    {
      "epoch": 17.706821480406386,
      "grad_norm": 9.957381248474121,
      "learning_rate": 3.5244315432994676e-05,
      "loss": 2.2044,
      "step": 231800
    },
    {
      "epoch": 17.714460316247802,
      "grad_norm": 13.896587371826172,
      "learning_rate": 3.5237949736460166e-05,
      "loss": 2.0746,
      "step": 231900
    },
    {
      "epoch": 17.72209915208922,
      "grad_norm": 11.637548446655273,
      "learning_rate": 3.523158403992565e-05,
      "loss": 2.1379,
      "step": 232000
    },
    {
      "epoch": 17.72973798793064,
      "grad_norm": 13.997594833374023,
      "learning_rate": 3.522521834339113e-05,
      "loss": 2.124,
      "step": 232100
    },
    {
      "epoch": 17.737376823772056,
      "grad_norm": 13.094886779785156,
      "learning_rate": 3.5218852646856623e-05,
      "loss": 2.1795,
      "step": 232200
    },
    {
      "epoch": 17.745015659613475,
      "grad_norm": 13.274136543273926,
      "learning_rate": 3.521248695032211e-05,
      "loss": 2.173,
      "step": 232300
    },
    {
      "epoch": 17.75265449545489,
      "grad_norm": 13.633441925048828,
      "learning_rate": 3.520612125378759e-05,
      "loss": 2.1597,
      "step": 232400
    },
    {
      "epoch": 17.76029333129631,
      "grad_norm": 11.93500804901123,
      "learning_rate": 3.519975555725308e-05,
      "loss": 2.1843,
      "step": 232500
    },
    {
      "epoch": 17.76793216713773,
      "grad_norm": 11.879755973815918,
      "learning_rate": 3.5193389860718564e-05,
      "loss": 2.1292,
      "step": 232600
    },
    {
      "epoch": 17.775571002979145,
      "grad_norm": 13.111851692199707,
      "learning_rate": 3.518702416418405e-05,
      "loss": 2.1063,
      "step": 232700
    },
    {
      "epoch": 17.783209838820564,
      "grad_norm": 14.687191009521484,
      "learning_rate": 3.518065846764953e-05,
      "loss": 2.1075,
      "step": 232800
    },
    {
      "epoch": 17.79084867466198,
      "grad_norm": 9.349640846252441,
      "learning_rate": 3.517429277111502e-05,
      "loss": 2.1536,
      "step": 232900
    },
    {
      "epoch": 17.7984875105034,
      "grad_norm": 12.929647445678711,
      "learning_rate": 3.5167927074580505e-05,
      "loss": 2.2262,
      "step": 233000
    },
    {
      "epoch": 17.80612634634482,
      "grad_norm": 12.695504188537598,
      "learning_rate": 3.516156137804599e-05,
      "loss": 2.0698,
      "step": 233100
    },
    {
      "epoch": 17.813765182186234,
      "grad_norm": 16.794536590576172,
      "learning_rate": 3.515519568151147e-05,
      "loss": 2.0718,
      "step": 233200
    },
    {
      "epoch": 17.821404018027653,
      "grad_norm": 13.556076049804688,
      "learning_rate": 3.5148829984976956e-05,
      "loss": 2.1049,
      "step": 233300
    },
    {
      "epoch": 17.82904285386907,
      "grad_norm": 13.424513816833496,
      "learning_rate": 3.5142464288442446e-05,
      "loss": 2.1566,
      "step": 233400
    },
    {
      "epoch": 17.83668168971049,
      "grad_norm": 16.616588592529297,
      "learning_rate": 3.513609859190793e-05,
      "loss": 2.1474,
      "step": 233500
    },
    {
      "epoch": 17.844320525551908,
      "grad_norm": 17.54076385498047,
      "learning_rate": 3.512973289537341e-05,
      "loss": 2.0576,
      "step": 233600
    },
    {
      "epoch": 17.851959361393323,
      "grad_norm": 14.738818168640137,
      "learning_rate": 3.51233671988389e-05,
      "loss": 2.1434,
      "step": 233700
    },
    {
      "epoch": 17.859598197234742,
      "grad_norm": 10.139150619506836,
      "learning_rate": 3.511700150230439e-05,
      "loss": 2.0807,
      "step": 233800
    },
    {
      "epoch": 17.867237033076158,
      "grad_norm": 10.55897045135498,
      "learning_rate": 3.511063580576987e-05,
      "loss": 2.0392,
      "step": 233900
    },
    {
      "epoch": 17.874875868917577,
      "grad_norm": 16.941062927246094,
      "learning_rate": 3.5104270109235354e-05,
      "loss": 2.1189,
      "step": 234000
    },
    {
      "epoch": 17.882514704758997,
      "grad_norm": 16.1124267578125,
      "learning_rate": 3.509790441270084e-05,
      "loss": 2.0563,
      "step": 234100
    },
    {
      "epoch": 17.890153540600412,
      "grad_norm": 16.22909164428711,
      "learning_rate": 3.509153871616632e-05,
      "loss": 2.2213,
      "step": 234200
    },
    {
      "epoch": 17.89779237644183,
      "grad_norm": 14.611163139343262,
      "learning_rate": 3.508517301963181e-05,
      "loss": 2.1077,
      "step": 234300
    },
    {
      "epoch": 17.905431212283247,
      "grad_norm": 13.106304168701172,
      "learning_rate": 3.5078807323097295e-05,
      "loss": 2.2021,
      "step": 234400
    },
    {
      "epoch": 17.913070048124666,
      "grad_norm": 12.338543891906738,
      "learning_rate": 3.507244162656278e-05,
      "loss": 2.0025,
      "step": 234500
    },
    {
      "epoch": 17.920708883966082,
      "grad_norm": 11.567808151245117,
      "learning_rate": 3.506607593002826e-05,
      "loss": 2.065,
      "step": 234600
    },
    {
      "epoch": 17.9283477198075,
      "grad_norm": 16.160350799560547,
      "learning_rate": 3.505971023349375e-05,
      "loss": 2.0945,
      "step": 234700
    },
    {
      "epoch": 17.93598655564892,
      "grad_norm": 10.229180335998535,
      "learning_rate": 3.5053344536959236e-05,
      "loss": 2.1078,
      "step": 234800
    },
    {
      "epoch": 17.943625391490336,
      "grad_norm": 13.943765640258789,
      "learning_rate": 3.504697884042472e-05,
      "loss": 2.142,
      "step": 234900
    },
    {
      "epoch": 17.951264227331755,
      "grad_norm": 13.300485610961914,
      "learning_rate": 3.50406131438902e-05,
      "loss": 2.0856,
      "step": 235000
    },
    {
      "epoch": 17.95890306317317,
      "grad_norm": 15.24988842010498,
      "learning_rate": 3.503424744735569e-05,
      "loss": 2.1259,
      "step": 235100
    },
    {
      "epoch": 17.96654189901459,
      "grad_norm": 10.903462409973145,
      "learning_rate": 3.502788175082118e-05,
      "loss": 2.1938,
      "step": 235200
    },
    {
      "epoch": 17.97418073485601,
      "grad_norm": 13.362791061401367,
      "learning_rate": 3.502151605428666e-05,
      "loss": 2.1597,
      "step": 235300
    },
    {
      "epoch": 17.981819570697425,
      "grad_norm": 11.376482009887695,
      "learning_rate": 3.5015150357752144e-05,
      "loss": 2.0816,
      "step": 235400
    },
    {
      "epoch": 17.989458406538844,
      "grad_norm": 16.2037410736084,
      "learning_rate": 3.500878466121763e-05,
      "loss": 2.1534,
      "step": 235500
    },
    {
      "epoch": 17.99709724238026,
      "grad_norm": 15.09834098815918,
      "learning_rate": 3.500241896468312e-05,
      "loss": 2.15,
      "step": 235600
    },
    {
      "epoch": 18.0,
      "eval_loss": 2.0470962524414062,
      "eval_runtime": 1.6532,
      "eval_samples_per_second": 417.366,
      "eval_steps_per_second": 417.366,
      "step": 235638
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.8758373260498047,
      "eval_runtime": 31.5814,
      "eval_samples_per_second": 414.517,
      "eval_steps_per_second": 414.517,
      "step": 235638
    },
    {
      "epoch": 18.00473607822168,
      "grad_norm": 16.787370681762695,
      "learning_rate": 3.49960532681486e-05,
      "loss": 2.0008,
      "step": 235700
    },
    {
      "epoch": 18.0123749140631,
      "grad_norm": 11.946370124816895,
      "learning_rate": 3.4989687571614085e-05,
      "loss": 2.1506,
      "step": 235800
    },
    {
      "epoch": 18.020013749904514,
      "grad_norm": 12.507742881774902,
      "learning_rate": 3.4983321875079575e-05,
      "loss": 2.0886,
      "step": 235900
    },
    {
      "epoch": 18.027652585745933,
      "grad_norm": 14.14236068725586,
      "learning_rate": 3.497695617854506e-05,
      "loss": 2.104,
      "step": 236000
    },
    {
      "epoch": 18.03529142158735,
      "grad_norm": 12.406912803649902,
      "learning_rate": 3.497059048201054e-05,
      "loss": 2.1843,
      "step": 236100
    },
    {
      "epoch": 18.042930257428768,
      "grad_norm": 13.202028274536133,
      "learning_rate": 3.496422478547603e-05,
      "loss": 2.0212,
      "step": 236200
    },
    {
      "epoch": 18.050569093270184,
      "grad_norm": 13.911638259887695,
      "learning_rate": 3.4957859088941516e-05,
      "loss": 2.1103,
      "step": 236300
    },
    {
      "epoch": 18.058207929111603,
      "grad_norm": 13.453886032104492,
      "learning_rate": 3.4951493392407e-05,
      "loss": 2.0706,
      "step": 236400
    },
    {
      "epoch": 18.065846764953022,
      "grad_norm": 14.159879684448242,
      "learning_rate": 3.494512769587248e-05,
      "loss": 2.0488,
      "step": 236500
    },
    {
      "epoch": 18.073485600794438,
      "grad_norm": 12.49429702758789,
      "learning_rate": 3.4938761999337974e-05,
      "loss": 1.9561,
      "step": 236600
    },
    {
      "epoch": 18.081124436635857,
      "grad_norm": 11.318991661071777,
      "learning_rate": 3.493239630280346e-05,
      "loss": 2.0676,
      "step": 236700
    },
    {
      "epoch": 18.088763272477273,
      "grad_norm": 11.783439636230469,
      "learning_rate": 3.492603060626894e-05,
      "loss": 2.1517,
      "step": 236800
    },
    {
      "epoch": 18.096402108318692,
      "grad_norm": 13.478021621704102,
      "learning_rate": 3.4919664909734424e-05,
      "loss": 2.0612,
      "step": 236900
    },
    {
      "epoch": 18.10404094416011,
      "grad_norm": 13.377878189086914,
      "learning_rate": 3.4913299213199915e-05,
      "loss": 2.1405,
      "step": 237000
    },
    {
      "epoch": 18.111679780001527,
      "grad_norm": 11.927621841430664,
      "learning_rate": 3.49069335166654e-05,
      "loss": 2.0966,
      "step": 237100
    },
    {
      "epoch": 18.119318615842946,
      "grad_norm": 12.867121696472168,
      "learning_rate": 3.490056782013088e-05,
      "loss": 2.0975,
      "step": 237200
    },
    {
      "epoch": 18.126957451684362,
      "grad_norm": 12.844125747680664,
      "learning_rate": 3.4894202123596365e-05,
      "loss": 2.0126,
      "step": 237300
    },
    {
      "epoch": 18.13459628752578,
      "grad_norm": 15.582332611083984,
      "learning_rate": 3.488783642706185e-05,
      "loss": 1.979,
      "step": 237400
    },
    {
      "epoch": 18.1422351233672,
      "grad_norm": 14.861043930053711,
      "learning_rate": 3.488147073052734e-05,
      "loss": 2.0945,
      "step": 237500
    },
    {
      "epoch": 18.149873959208616,
      "grad_norm": 10.314325332641602,
      "learning_rate": 3.487510503399282e-05,
      "loss": 2.0588,
      "step": 237600
    },
    {
      "epoch": 18.157512795050035,
      "grad_norm": 12.022539138793945,
      "learning_rate": 3.4868739337458306e-05,
      "loss": 2.0548,
      "step": 237700
    },
    {
      "epoch": 18.16515163089145,
      "grad_norm": 11.357710838317871,
      "learning_rate": 3.486237364092379e-05,
      "loss": 2.1537,
      "step": 237800
    },
    {
      "epoch": 18.17279046673287,
      "grad_norm": 13.855997085571289,
      "learning_rate": 3.485600794438927e-05,
      "loss": 2.131,
      "step": 237900
    },
    {
      "epoch": 18.18042930257429,
      "grad_norm": 13.976186752319336,
      "learning_rate": 3.4849642247854763e-05,
      "loss": 2.0796,
      "step": 238000
    },
    {
      "epoch": 18.188068138415705,
      "grad_norm": 10.457742691040039,
      "learning_rate": 3.484327655132025e-05,
      "loss": 2.0329,
      "step": 238100
    },
    {
      "epoch": 18.195706974257124,
      "grad_norm": 13.978781700134277,
      "learning_rate": 3.483691085478573e-05,
      "loss": 2.0681,
      "step": 238200
    },
    {
      "epoch": 18.20334581009854,
      "grad_norm": 12.940312385559082,
      "learning_rate": 3.4830545158251214e-05,
      "loss": 2.1803,
      "step": 238300
    },
    {
      "epoch": 18.21098464593996,
      "grad_norm": 12.825789451599121,
      "learning_rate": 3.4824179461716704e-05,
      "loss": 2.0927,
      "step": 238400
    },
    {
      "epoch": 18.218623481781375,
      "grad_norm": 13.245633125305176,
      "learning_rate": 3.481781376518219e-05,
      "loss": 2.076,
      "step": 238500
    },
    {
      "epoch": 18.226262317622794,
      "grad_norm": 12.767157554626465,
      "learning_rate": 3.481144806864767e-05,
      "loss": 2.1407,
      "step": 238600
    },
    {
      "epoch": 18.233901153464213,
      "grad_norm": 13.750307083129883,
      "learning_rate": 3.4805082372113155e-05,
      "loss": 2.1397,
      "step": 238700
    },
    {
      "epoch": 18.24153998930563,
      "grad_norm": 12.284467697143555,
      "learning_rate": 3.479871667557864e-05,
      "loss": 2.0383,
      "step": 238800
    },
    {
      "epoch": 18.249178825147048,
      "grad_norm": 10.68464183807373,
      "learning_rate": 3.479235097904413e-05,
      "loss": 2.1162,
      "step": 238900
    },
    {
      "epoch": 18.256817660988464,
      "grad_norm": 9.473942756652832,
      "learning_rate": 3.478598528250961e-05,
      "loss": 2.0228,
      "step": 239000
    },
    {
      "epoch": 18.264456496829883,
      "grad_norm": 13.305285453796387,
      "learning_rate": 3.4779619585975096e-05,
      "loss": 2.1234,
      "step": 239100
    },
    {
      "epoch": 18.272095332671302,
      "grad_norm": 11.951668739318848,
      "learning_rate": 3.477325388944058e-05,
      "loss": 2.1075,
      "step": 239200
    },
    {
      "epoch": 18.279734168512718,
      "grad_norm": 14.676526069641113,
      "learning_rate": 3.476688819290607e-05,
      "loss": 2.0799,
      "step": 239300
    },
    {
      "epoch": 18.287373004354137,
      "grad_norm": 14.779996871948242,
      "learning_rate": 3.476052249637155e-05,
      "loss": 2.1359,
      "step": 239400
    },
    {
      "epoch": 18.295011840195553,
      "grad_norm": 11.886086463928223,
      "learning_rate": 3.475415679983704e-05,
      "loss": 2.2176,
      "step": 239500
    },
    {
      "epoch": 18.302650676036972,
      "grad_norm": 14.075926780700684,
      "learning_rate": 3.474779110330253e-05,
      "loss": 2.0225,
      "step": 239600
    },
    {
      "epoch": 18.31028951187839,
      "grad_norm": 13.570181846618652,
      "learning_rate": 3.474142540676801e-05,
      "loss": 2.1058,
      "step": 239700
    },
    {
      "epoch": 18.317928347719807,
      "grad_norm": 12.476811408996582,
      "learning_rate": 3.4735059710233494e-05,
      "loss": 2.11,
      "step": 239800
    },
    {
      "epoch": 18.325567183561226,
      "grad_norm": 13.442719459533691,
      "learning_rate": 3.4728694013698985e-05,
      "loss": 2.0916,
      "step": 239900
    },
    {
      "epoch": 18.333206019402642,
      "grad_norm": 12.861128807067871,
      "learning_rate": 3.472232831716447e-05,
      "loss": 2.0706,
      "step": 240000
    },
    {
      "epoch": 18.34084485524406,
      "grad_norm": 9.807973861694336,
      "learning_rate": 3.471596262062995e-05,
      "loss": 2.1171,
      "step": 240100
    },
    {
      "epoch": 18.34848369108548,
      "grad_norm": 13.441035270690918,
      "learning_rate": 3.470959692409544e-05,
      "loss": 2.0528,
      "step": 240200
    },
    {
      "epoch": 18.356122526926896,
      "grad_norm": 12.643301963806152,
      "learning_rate": 3.4703231227560925e-05,
      "loss": 2.1363,
      "step": 240300
    },
    {
      "epoch": 18.363761362768315,
      "grad_norm": 11.756702423095703,
      "learning_rate": 3.469686553102641e-05,
      "loss": 2.1632,
      "step": 240400
    },
    {
      "epoch": 18.37140019860973,
      "grad_norm": 14.813108444213867,
      "learning_rate": 3.469049983449189e-05,
      "loss": 2.0453,
      "step": 240500
    },
    {
      "epoch": 18.37903903445115,
      "grad_norm": 12.70621395111084,
      "learning_rate": 3.4684134137957376e-05,
      "loss": 2.0752,
      "step": 240600
    },
    {
      "epoch": 18.386677870292566,
      "grad_norm": 11.985164642333984,
      "learning_rate": 3.4677768441422866e-05,
      "loss": 2.0735,
      "step": 240700
    },
    {
      "epoch": 18.394316706133985,
      "grad_norm": 11.822270393371582,
      "learning_rate": 3.467140274488835e-05,
      "loss": 2.0846,
      "step": 240800
    },
    {
      "epoch": 18.401955541975404,
      "grad_norm": 11.860910415649414,
      "learning_rate": 3.4665037048353833e-05,
      "loss": 2.0928,
      "step": 240900
    },
    {
      "epoch": 18.40959437781682,
      "grad_norm": 13.761871337890625,
      "learning_rate": 3.465867135181932e-05,
      "loss": 2.1627,
      "step": 241000
    },
    {
      "epoch": 18.41723321365824,
      "grad_norm": 12.921562194824219,
      "learning_rate": 3.46523056552848e-05,
      "loss": 2.1129,
      "step": 241100
    },
    {
      "epoch": 18.424872049499655,
      "grad_norm": 14.47107982635498,
      "learning_rate": 3.464593995875029e-05,
      "loss": 2.0007,
      "step": 241200
    },
    {
      "epoch": 18.432510885341074,
      "grad_norm": 13.846102714538574,
      "learning_rate": 3.4639574262215774e-05,
      "loss": 2.0819,
      "step": 241300
    },
    {
      "epoch": 18.440149721182493,
      "grad_norm": 14.366775512695312,
      "learning_rate": 3.463320856568126e-05,
      "loss": 2.1076,
      "step": 241400
    },
    {
      "epoch": 18.44778855702391,
      "grad_norm": 13.501742362976074,
      "learning_rate": 3.462684286914674e-05,
      "loss": 2.0839,
      "step": 241500
    },
    {
      "epoch": 18.455427392865328,
      "grad_norm": 10.512246131896973,
      "learning_rate": 3.462047717261223e-05,
      "loss": 2.0187,
      "step": 241600
    },
    {
      "epoch": 18.463066228706744,
      "grad_norm": 10.47600269317627,
      "learning_rate": 3.4614111476077715e-05,
      "loss": 2.0971,
      "step": 241700
    },
    {
      "epoch": 18.470705064548163,
      "grad_norm": 12.913299560546875,
      "learning_rate": 3.46077457795432e-05,
      "loss": 2.0004,
      "step": 241800
    },
    {
      "epoch": 18.478343900389582,
      "grad_norm": 13.14317798614502,
      "learning_rate": 3.460138008300868e-05,
      "loss": 2.0194,
      "step": 241900
    },
    {
      "epoch": 18.485982736230998,
      "grad_norm": 14.965564727783203,
      "learning_rate": 3.4595014386474166e-05,
      "loss": 2.1102,
      "step": 242000
    },
    {
      "epoch": 18.493621572072417,
      "grad_norm": 13.767402648925781,
      "learning_rate": 3.4588648689939656e-05,
      "loss": 2.1619,
      "step": 242100
    },
    {
      "epoch": 18.501260407913833,
      "grad_norm": 10.987645149230957,
      "learning_rate": 3.458228299340514e-05,
      "loss": 2.1293,
      "step": 242200
    },
    {
      "epoch": 18.508899243755252,
      "grad_norm": 9.249165534973145,
      "learning_rate": 3.457591729687062e-05,
      "loss": 2.1574,
      "step": 242300
    },
    {
      "epoch": 18.51653807959667,
      "grad_norm": 10.999225616455078,
      "learning_rate": 3.456955160033611e-05,
      "loss": 2.1402,
      "step": 242400
    },
    {
      "epoch": 18.524176915438087,
      "grad_norm": 15.792588233947754,
      "learning_rate": 3.45631859038016e-05,
      "loss": 2.0215,
      "step": 242500
    },
    {
      "epoch": 18.531815751279506,
      "grad_norm": 12.205106735229492,
      "learning_rate": 3.455682020726708e-05,
      "loss": 2.1692,
      "step": 242600
    },
    {
      "epoch": 18.53945458712092,
      "grad_norm": 13.146014213562012,
      "learning_rate": 3.4550454510732564e-05,
      "loss": 2.103,
      "step": 242700
    },
    {
      "epoch": 18.54709342296234,
      "grad_norm": 12.764541625976562,
      "learning_rate": 3.454408881419805e-05,
      "loss": 2.0923,
      "step": 242800
    },
    {
      "epoch": 18.554732258803757,
      "grad_norm": 15.221383094787598,
      "learning_rate": 3.453772311766353e-05,
      "loss": 2.0945,
      "step": 242900
    },
    {
      "epoch": 18.562371094645176,
      "grad_norm": 13.152852058410645,
      "learning_rate": 3.453135742112902e-05,
      "loss": 2.1544,
      "step": 243000
    },
    {
      "epoch": 18.570009930486595,
      "grad_norm": 8.85071849822998,
      "learning_rate": 3.4524991724594505e-05,
      "loss": 2.0743,
      "step": 243100
    },
    {
      "epoch": 18.57764876632801,
      "grad_norm": 11.462501525878906,
      "learning_rate": 3.451862602805999e-05,
      "loss": 2.1744,
      "step": 243200
    },
    {
      "epoch": 18.58528760216943,
      "grad_norm": 13.896522521972656,
      "learning_rate": 3.451226033152547e-05,
      "loss": 2.2556,
      "step": 243300
    },
    {
      "epoch": 18.592926438010846,
      "grad_norm": 16.903383255004883,
      "learning_rate": 3.450589463499096e-05,
      "loss": 2.0749,
      "step": 243400
    },
    {
      "epoch": 18.600565273852265,
      "grad_norm": 13.976325988769531,
      "learning_rate": 3.4499528938456446e-05,
      "loss": 2.1181,
      "step": 243500
    },
    {
      "epoch": 18.608204109693684,
      "grad_norm": 11.351581573486328,
      "learning_rate": 3.4493163241921936e-05,
      "loss": 2.1933,
      "step": 243600
    },
    {
      "epoch": 18.6158429455351,
      "grad_norm": 16.93348503112793,
      "learning_rate": 3.448679754538742e-05,
      "loss": 2.0102,
      "step": 243700
    },
    {
      "epoch": 18.62348178137652,
      "grad_norm": 20.568157196044922,
      "learning_rate": 3.4480431848852903e-05,
      "loss": 2.1167,
      "step": 243800
    },
    {
      "epoch": 18.631120617217935,
      "grad_norm": 13.116199493408203,
      "learning_rate": 3.4474066152318394e-05,
      "loss": 2.107,
      "step": 243900
    },
    {
      "epoch": 18.638759453059354,
      "grad_norm": 11.250837326049805,
      "learning_rate": 3.446770045578388e-05,
      "loss": 2.083,
      "step": 244000
    },
    {
      "epoch": 18.646398288900773,
      "grad_norm": 7.6624956130981445,
      "learning_rate": 3.446133475924936e-05,
      "loss": 2.1285,
      "step": 244100
    },
    {
      "epoch": 18.65403712474219,
      "grad_norm": 12.482561111450195,
      "learning_rate": 3.4454969062714844e-05,
      "loss": 2.068,
      "step": 244200
    },
    {
      "epoch": 18.661675960583608,
      "grad_norm": 9.49726676940918,
      "learning_rate": 3.444860336618033e-05,
      "loss": 2.1946,
      "step": 244300
    },
    {
      "epoch": 18.669314796425024,
      "grad_norm": 14.79772663116455,
      "learning_rate": 3.444223766964582e-05,
      "loss": 2.105,
      "step": 244400
    },
    {
      "epoch": 18.676953632266443,
      "grad_norm": 12.208321571350098,
      "learning_rate": 3.44358719731113e-05,
      "loss": 2.0986,
      "step": 244500
    },
    {
      "epoch": 18.68459246810786,
      "grad_norm": 10.169499397277832,
      "learning_rate": 3.4429506276576785e-05,
      "loss": 2.0276,
      "step": 244600
    },
    {
      "epoch": 18.692231303949278,
      "grad_norm": 13.030695915222168,
      "learning_rate": 3.442314058004227e-05,
      "loss": 2.1294,
      "step": 244700
    },
    {
      "epoch": 18.699870139790697,
      "grad_norm": 16.16405487060547,
      "learning_rate": 3.441677488350776e-05,
      "loss": 2.0275,
      "step": 244800
    },
    {
      "epoch": 18.707508975632113,
      "grad_norm": 15.191445350646973,
      "learning_rate": 3.441040918697324e-05,
      "loss": 2.1421,
      "step": 244900
    },
    {
      "epoch": 18.715147811473532,
      "grad_norm": 12.05743408203125,
      "learning_rate": 3.4404043490438726e-05,
      "loss": 2.116,
      "step": 245000
    },
    {
      "epoch": 18.722786647314948,
      "grad_norm": 11.948233604431152,
      "learning_rate": 3.439767779390421e-05,
      "loss": 2.0543,
      "step": 245100
    },
    {
      "epoch": 18.730425483156367,
      "grad_norm": 15.808985710144043,
      "learning_rate": 3.439131209736969e-05,
      "loss": 2.0653,
      "step": 245200
    },
    {
      "epoch": 18.738064318997786,
      "grad_norm": 14.545052528381348,
      "learning_rate": 3.4384946400835184e-05,
      "loss": 2.1204,
      "step": 245300
    },
    {
      "epoch": 18.7457031548392,
      "grad_norm": 11.287555694580078,
      "learning_rate": 3.437858070430067e-05,
      "loss": 2.1375,
      "step": 245400
    },
    {
      "epoch": 18.75334199068062,
      "grad_norm": 13.081913948059082,
      "learning_rate": 3.437221500776615e-05,
      "loss": 2.0753,
      "step": 245500
    },
    {
      "epoch": 18.760980826522037,
      "grad_norm": 16.698076248168945,
      "learning_rate": 3.4365849311231634e-05,
      "loss": 2.1318,
      "step": 245600
    },
    {
      "epoch": 18.768619662363456,
      "grad_norm": 14.081320762634277,
      "learning_rate": 3.4359483614697125e-05,
      "loss": 2.0823,
      "step": 245700
    },
    {
      "epoch": 18.776258498204875,
      "grad_norm": 12.100522994995117,
      "learning_rate": 3.435311791816261e-05,
      "loss": 2.0556,
      "step": 245800
    },
    {
      "epoch": 18.78389733404629,
      "grad_norm": 13.989959716796875,
      "learning_rate": 3.434675222162809e-05,
      "loss": 2.0641,
      "step": 245900
    },
    {
      "epoch": 18.79153616988771,
      "grad_norm": 13.510555267333984,
      "learning_rate": 3.4340386525093575e-05,
      "loss": 2.0867,
      "step": 246000
    },
    {
      "epoch": 18.799175005729126,
      "grad_norm": 10.922192573547363,
      "learning_rate": 3.433402082855906e-05,
      "loss": 1.9715,
      "step": 246100
    },
    {
      "epoch": 18.806813841570545,
      "grad_norm": 13.027303695678711,
      "learning_rate": 3.432765513202455e-05,
      "loss": 1.9478,
      "step": 246200
    },
    {
      "epoch": 18.814452677411964,
      "grad_norm": 11.526430130004883,
      "learning_rate": 3.432128943549003e-05,
      "loss": 2.1019,
      "step": 246300
    },
    {
      "epoch": 18.82209151325338,
      "grad_norm": 14.979422569274902,
      "learning_rate": 3.4314923738955516e-05,
      "loss": 2.0921,
      "step": 246400
    },
    {
      "epoch": 18.8297303490948,
      "grad_norm": 12.575380325317383,
      "learning_rate": 3.4308558042421e-05,
      "loss": 2.0474,
      "step": 246500
    },
    {
      "epoch": 18.837369184936215,
      "grad_norm": 12.404394149780273,
      "learning_rate": 3.430219234588648e-05,
      "loss": 2.0728,
      "step": 246600
    },
    {
      "epoch": 18.845008020777634,
      "grad_norm": 15.313169479370117,
      "learning_rate": 3.4295826649351973e-05,
      "loss": 1.9976,
      "step": 246700
    },
    {
      "epoch": 18.852646856619053,
      "grad_norm": 13.870992660522461,
      "learning_rate": 3.428946095281746e-05,
      "loss": 2.1183,
      "step": 246800
    },
    {
      "epoch": 18.86028569246047,
      "grad_norm": 12.447124481201172,
      "learning_rate": 3.428309525628294e-05,
      "loss": 2.1005,
      "step": 246900
    },
    {
      "epoch": 18.867924528301888,
      "grad_norm": 12.52527141571045,
      "learning_rate": 3.4276729559748424e-05,
      "loss": 2.1561,
      "step": 247000
    },
    {
      "epoch": 18.875563364143304,
      "grad_norm": 13.372766494750977,
      "learning_rate": 3.4270363863213914e-05,
      "loss": 2.0945,
      "step": 247100
    },
    {
      "epoch": 18.883202199984723,
      "grad_norm": 6.774862766265869,
      "learning_rate": 3.42639981666794e-05,
      "loss": 2.076,
      "step": 247200
    },
    {
      "epoch": 18.89084103582614,
      "grad_norm": 12.590235710144043,
      "learning_rate": 3.425763247014488e-05,
      "loss": 2.1323,
      "step": 247300
    },
    {
      "epoch": 18.898479871667558,
      "grad_norm": 20.755327224731445,
      "learning_rate": 3.425126677361037e-05,
      "loss": 2.1114,
      "step": 247400
    },
    {
      "epoch": 18.906118707508977,
      "grad_norm": 9.856746673583984,
      "learning_rate": 3.4244901077075855e-05,
      "loss": 2.126,
      "step": 247500
    },
    {
      "epoch": 18.913757543350393,
      "grad_norm": 11.427094459533691,
      "learning_rate": 3.4238535380541346e-05,
      "loss": 2.1126,
      "step": 247600
    },
    {
      "epoch": 18.921396379191812,
      "grad_norm": 11.438555717468262,
      "learning_rate": 3.423216968400683e-05,
      "loss": 2.0961,
      "step": 247700
    },
    {
      "epoch": 18.929035215033228,
      "grad_norm": 12.77993106842041,
      "learning_rate": 3.422580398747231e-05,
      "loss": 2.1061,
      "step": 247800
    },
    {
      "epoch": 18.936674050874647,
      "grad_norm": 10.142244338989258,
      "learning_rate": 3.4219438290937796e-05,
      "loss": 2.0964,
      "step": 247900
    },
    {
      "epoch": 18.944312886716066,
      "grad_norm": 10.573030471801758,
      "learning_rate": 3.4213072594403287e-05,
      "loss": 2.0202,
      "step": 248000
    },
    {
      "epoch": 18.95195172255748,
      "grad_norm": 11.298771858215332,
      "learning_rate": 3.420670689786877e-05,
      "loss": 2.1725,
      "step": 248100
    },
    {
      "epoch": 18.9595905583989,
      "grad_norm": 12.576390266418457,
      "learning_rate": 3.4200341201334254e-05,
      "loss": 2.2012,
      "step": 248200
    },
    {
      "epoch": 18.967229394240317,
      "grad_norm": 15.034299850463867,
      "learning_rate": 3.419397550479974e-05,
      "loss": 2.1151,
      "step": 248300
    },
    {
      "epoch": 18.974868230081736,
      "grad_norm": 13.140756607055664,
      "learning_rate": 3.418760980826522e-05,
      "loss": 2.1819,
      "step": 248400
    },
    {
      "epoch": 18.982507065923155,
      "grad_norm": 12.569345474243164,
      "learning_rate": 3.418124411173071e-05,
      "loss": 2.09,
      "step": 248500
    },
    {
      "epoch": 18.99014590176457,
      "grad_norm": 9.988480567932129,
      "learning_rate": 3.4174878415196195e-05,
      "loss": 2.0655,
      "step": 248600
    },
    {
      "epoch": 18.99778473760599,
      "grad_norm": 10.747567176818848,
      "learning_rate": 3.416851271866168e-05,
      "loss": 2.0455,
      "step": 248700
    },
    {
      "epoch": 19.0,
      "eval_loss": 2.036696195602417,
      "eval_runtime": 1.6685,
      "eval_samples_per_second": 413.557,
      "eval_steps_per_second": 413.557,
      "step": 248729
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.8614110946655273,
      "eval_runtime": 31.444,
      "eval_samples_per_second": 416.327,
      "eval_steps_per_second": 416.327,
      "step": 248729
    },
    {
      "epoch": 19.005423573447406,
      "grad_norm": 13.254262924194336,
      "learning_rate": 3.416214702212716e-05,
      "loss": 2.0118,
      "step": 248800
    },
    {
      "epoch": 19.013062409288825,
      "grad_norm": 12.719123840332031,
      "learning_rate": 3.415578132559265e-05,
      "loss": 2.1052,
      "step": 248900
    },
    {
      "epoch": 19.02070124513024,
      "grad_norm": 11.986207962036133,
      "learning_rate": 3.4149415629058135e-05,
      "loss": 1.9886,
      "step": 249000
    },
    {
      "epoch": 19.02834008097166,
      "grad_norm": 10.28903865814209,
      "learning_rate": 3.414304993252362e-05,
      "loss": 2.0314,
      "step": 249100
    },
    {
      "epoch": 19.03597891681308,
      "grad_norm": 13.823890686035156,
      "learning_rate": 3.41366842359891e-05,
      "loss": 2.1054,
      "step": 249200
    },
    {
      "epoch": 19.043617752654495,
      "grad_norm": 11.493913650512695,
      "learning_rate": 3.4130318539454586e-05,
      "loss": 2.1302,
      "step": 249300
    },
    {
      "epoch": 19.051256588495914,
      "grad_norm": 12.67982006072998,
      "learning_rate": 3.4123952842920076e-05,
      "loss": 2.1059,
      "step": 249400
    },
    {
      "epoch": 19.05889542433733,
      "grad_norm": 11.926085472106934,
      "learning_rate": 3.411758714638556e-05,
      "loss": 2.0513,
      "step": 249500
    },
    {
      "epoch": 19.06653426017875,
      "grad_norm": 10.354992866516113,
      "learning_rate": 3.4111221449851043e-05,
      "loss": 2.0787,
      "step": 249600
    },
    {
      "epoch": 19.074173096020168,
      "grad_norm": 14.214909553527832,
      "learning_rate": 3.410485575331653e-05,
      "loss": 2.0595,
      "step": 249700
    },
    {
      "epoch": 19.081811931861584,
      "grad_norm": 13.452881813049316,
      "learning_rate": 3.409849005678201e-05,
      "loss": 2.1071,
      "step": 249800
    },
    {
      "epoch": 19.089450767703003,
      "grad_norm": 14.694561958312988,
      "learning_rate": 3.40921243602475e-05,
      "loss": 2.1289,
      "step": 249900
    },
    {
      "epoch": 19.09708960354442,
      "grad_norm": 16.513412475585938,
      "learning_rate": 3.4085758663712984e-05,
      "loss": 2.0547,
      "step": 250000
    },
    {
      "epoch": 19.104728439385838,
      "grad_norm": 13.280101776123047,
      "learning_rate": 3.407939296717847e-05,
      "loss": 1.9696,
      "step": 250100
    },
    {
      "epoch": 19.112367275227257,
      "grad_norm": 10.889450073242188,
      "learning_rate": 3.407302727064395e-05,
      "loss": 2.1074,
      "step": 250200
    },
    {
      "epoch": 19.120006111068673,
      "grad_norm": 11.418779373168945,
      "learning_rate": 3.406666157410944e-05,
      "loss": 2.0446,
      "step": 250300
    },
    {
      "epoch": 19.12764494691009,
      "grad_norm": 13.547948837280273,
      "learning_rate": 3.4060295877574925e-05,
      "loss": 2.0686,
      "step": 250400
    },
    {
      "epoch": 19.135283782751507,
      "grad_norm": 15.373659133911133,
      "learning_rate": 3.405393018104041e-05,
      "loss": 2.1081,
      "step": 250500
    },
    {
      "epoch": 19.142922618592927,
      "grad_norm": 13.166688919067383,
      "learning_rate": 3.404756448450589e-05,
      "loss": 2.0261,
      "step": 250600
    },
    {
      "epoch": 19.150561454434346,
      "grad_norm": 13.996211051940918,
      "learning_rate": 3.4041198787971376e-05,
      "loss": 2.0261,
      "step": 250700
    },
    {
      "epoch": 19.15820029027576,
      "grad_norm": 12.872163772583008,
      "learning_rate": 3.4034833091436866e-05,
      "loss": 2.0413,
      "step": 250800
    },
    {
      "epoch": 19.16583912611718,
      "grad_norm": 12.454550743103027,
      "learning_rate": 3.402846739490235e-05,
      "loss": 2.0656,
      "step": 250900
    },
    {
      "epoch": 19.173477961958596,
      "grad_norm": 15.955484390258789,
      "learning_rate": 3.402210169836783e-05,
      "loss": 2.0125,
      "step": 251000
    },
    {
      "epoch": 19.181116797800016,
      "grad_norm": 12.363680839538574,
      "learning_rate": 3.4015736001833324e-05,
      "loss": 2.1683,
      "step": 251100
    },
    {
      "epoch": 19.18875563364143,
      "grad_norm": 13.328923225402832,
      "learning_rate": 3.400937030529881e-05,
      "loss": 2.0843,
      "step": 251200
    },
    {
      "epoch": 19.19639446948285,
      "grad_norm": 13.385270118713379,
      "learning_rate": 3.400300460876429e-05,
      "loss": 2.0754,
      "step": 251300
    },
    {
      "epoch": 19.20403330532427,
      "grad_norm": 14.915104866027832,
      "learning_rate": 3.399663891222978e-05,
      "loss": 2.0922,
      "step": 251400
    },
    {
      "epoch": 19.211672141165685,
      "grad_norm": 10.786749839782715,
      "learning_rate": 3.3990273215695265e-05,
      "loss": 2.1031,
      "step": 251500
    },
    {
      "epoch": 19.219310977007105,
      "grad_norm": 11.361808776855469,
      "learning_rate": 3.398390751916075e-05,
      "loss": 2.0242,
      "step": 251600
    },
    {
      "epoch": 19.22694981284852,
      "grad_norm": 12.34173583984375,
      "learning_rate": 3.397754182262624e-05,
      "loss": 2.0473,
      "step": 251700
    },
    {
      "epoch": 19.23458864868994,
      "grad_norm": 11.985143661499023,
      "learning_rate": 3.397117612609172e-05,
      "loss": 2.0937,
      "step": 251800
    },
    {
      "epoch": 19.24222748453136,
      "grad_norm": 13.258938789367676,
      "learning_rate": 3.3964810429557205e-05,
      "loss": 2.1038,
      "step": 251900
    },
    {
      "epoch": 19.249866320372774,
      "grad_norm": 13.902046203613281,
      "learning_rate": 3.395844473302269e-05,
      "loss": 2.1057,
      "step": 252000
    },
    {
      "epoch": 19.257505156214194,
      "grad_norm": 14.293932914733887,
      "learning_rate": 3.395207903648818e-05,
      "loss": 2.1866,
      "step": 252100
    },
    {
      "epoch": 19.26514399205561,
      "grad_norm": 13.944438934326172,
      "learning_rate": 3.394571333995366e-05,
      "loss": 2.0565,
      "step": 252200
    },
    {
      "epoch": 19.27278282789703,
      "grad_norm": 14.177010536193848,
      "learning_rate": 3.3939347643419146e-05,
      "loss": 2.0608,
      "step": 252300
    },
    {
      "epoch": 19.280421663738448,
      "grad_norm": 13.831871032714844,
      "learning_rate": 3.393298194688463e-05,
      "loss": 2.1454,
      "step": 252400
    },
    {
      "epoch": 19.288060499579863,
      "grad_norm": 10.509208679199219,
      "learning_rate": 3.3926616250350113e-05,
      "loss": 2.1234,
      "step": 252500
    },
    {
      "epoch": 19.295699335421283,
      "grad_norm": 13.21684741973877,
      "learning_rate": 3.3920250553815604e-05,
      "loss": 2.0558,
      "step": 252600
    },
    {
      "epoch": 19.3033381712627,
      "grad_norm": 18.471765518188477,
      "learning_rate": 3.391388485728109e-05,
      "loss": 2.0561,
      "step": 252700
    },
    {
      "epoch": 19.310977007104118,
      "grad_norm": 15.324724197387695,
      "learning_rate": 3.390751916074657e-05,
      "loss": 2.0932,
      "step": 252800
    },
    {
      "epoch": 19.318615842945537,
      "grad_norm": 13.370865821838379,
      "learning_rate": 3.3901153464212054e-05,
      "loss": 2.0401,
      "step": 252900
    },
    {
      "epoch": 19.326254678786952,
      "grad_norm": 15.001452445983887,
      "learning_rate": 3.389478776767754e-05,
      "loss": 2.058,
      "step": 253000
    },
    {
      "epoch": 19.33389351462837,
      "grad_norm": 10.339410781860352,
      "learning_rate": 3.388842207114303e-05,
      "loss": 1.983,
      "step": 253100
    },
    {
      "epoch": 19.341532350469787,
      "grad_norm": 13.600251197814941,
      "learning_rate": 3.388205637460851e-05,
      "loss": 1.9696,
      "step": 253200
    },
    {
      "epoch": 19.349171186311207,
      "grad_norm": 12.790955543518066,
      "learning_rate": 3.3875690678073995e-05,
      "loss": 2.1047,
      "step": 253300
    },
    {
      "epoch": 19.356810022152622,
      "grad_norm": 15.108624458312988,
      "learning_rate": 3.386932498153948e-05,
      "loss": 2.0538,
      "step": 253400
    },
    {
      "epoch": 19.36444885799404,
      "grad_norm": 10.722074508666992,
      "learning_rate": 3.386295928500497e-05,
      "loss": 1.9802,
      "step": 253500
    },
    {
      "epoch": 19.37208769383546,
      "grad_norm": 14.048823356628418,
      "learning_rate": 3.385659358847045e-05,
      "loss": 2.0932,
      "step": 253600
    },
    {
      "epoch": 19.379726529676876,
      "grad_norm": 13.676209449768066,
      "learning_rate": 3.3850227891935936e-05,
      "loss": 2.1785,
      "step": 253700
    },
    {
      "epoch": 19.387365365518296,
      "grad_norm": 11.541144371032715,
      "learning_rate": 3.384386219540142e-05,
      "loss": 2.099,
      "step": 253800
    },
    {
      "epoch": 19.39500420135971,
      "grad_norm": 13.843926429748535,
      "learning_rate": 3.38374964988669e-05,
      "loss": 2.1414,
      "step": 253900
    },
    {
      "epoch": 19.40264303720113,
      "grad_norm": 13.294447898864746,
      "learning_rate": 3.3831130802332394e-05,
      "loss": 2.1766,
      "step": 254000
    },
    {
      "epoch": 19.41028187304255,
      "grad_norm": 13.438077926635742,
      "learning_rate": 3.382476510579788e-05,
      "loss": 2.0914,
      "step": 254100
    },
    {
      "epoch": 19.417920708883965,
      "grad_norm": 21.434646606445312,
      "learning_rate": 3.381839940926336e-05,
      "loss": 2.2307,
      "step": 254200
    },
    {
      "epoch": 19.425559544725385,
      "grad_norm": 12.661882400512695,
      "learning_rate": 3.3812033712728844e-05,
      "loss": 2.0605,
      "step": 254300
    },
    {
      "epoch": 19.4331983805668,
      "grad_norm": 13.093085289001465,
      "learning_rate": 3.3805668016194335e-05,
      "loss": 2.0012,
      "step": 254400
    },
    {
      "epoch": 19.44083721640822,
      "grad_norm": 14.371010780334473,
      "learning_rate": 3.379930231965982e-05,
      "loss": 2.0654,
      "step": 254500
    },
    {
      "epoch": 19.44847605224964,
      "grad_norm": 11.057520866394043,
      "learning_rate": 3.37929366231253e-05,
      "loss": 2.0837,
      "step": 254600
    },
    {
      "epoch": 19.456114888091054,
      "grad_norm": 11.324158668518066,
      "learning_rate": 3.3786570926590785e-05,
      "loss": 2.0546,
      "step": 254700
    },
    {
      "epoch": 19.463753723932474,
      "grad_norm": 11.542601585388184,
      "learning_rate": 3.3780205230056275e-05,
      "loss": 2.1389,
      "step": 254800
    },
    {
      "epoch": 19.47139255977389,
      "grad_norm": 12.767667770385742,
      "learning_rate": 3.377383953352176e-05,
      "loss": 2.1122,
      "step": 254900
    },
    {
      "epoch": 19.47903139561531,
      "grad_norm": 10.145195007324219,
      "learning_rate": 3.376747383698724e-05,
      "loss": 2.0917,
      "step": 255000
    },
    {
      "epoch": 19.486670231456728,
      "grad_norm": 6.174191951751709,
      "learning_rate": 3.376110814045273e-05,
      "loss": 2.023,
      "step": 255100
    },
    {
      "epoch": 19.494309067298143,
      "grad_norm": 13.864543914794922,
      "learning_rate": 3.3754742443918216e-05,
      "loss": 2.2091,
      "step": 255200
    },
    {
      "epoch": 19.501947903139563,
      "grad_norm": 7.560367107391357,
      "learning_rate": 3.37483767473837e-05,
      "loss": 2.1185,
      "step": 255300
    },
    {
      "epoch": 19.50958673898098,
      "grad_norm": 11.18832778930664,
      "learning_rate": 3.374201105084919e-05,
      "loss": 2.0305,
      "step": 255400
    },
    {
      "epoch": 19.517225574822398,
      "grad_norm": 15.756166458129883,
      "learning_rate": 3.3735645354314674e-05,
      "loss": 1.9766,
      "step": 255500
    },
    {
      "epoch": 19.524864410663813,
      "grad_norm": 12.491522789001465,
      "learning_rate": 3.372927965778016e-05,
      "loss": 2.1001,
      "step": 255600
    },
    {
      "epoch": 19.532503246505232,
      "grad_norm": 15.46630859375,
      "learning_rate": 3.372291396124564e-05,
      "loss": 2.1267,
      "step": 255700
    },
    {
      "epoch": 19.54014208234665,
      "grad_norm": 10.3668212890625,
      "learning_rate": 3.371654826471113e-05,
      "loss": 2.0514,
      "step": 255800
    },
    {
      "epoch": 19.547780918188067,
      "grad_norm": 16.30332374572754,
      "learning_rate": 3.3710182568176615e-05,
      "loss": 2.0837,
      "step": 255900
    },
    {
      "epoch": 19.555419754029487,
      "grad_norm": 9.679580688476562,
      "learning_rate": 3.37038168716421e-05,
      "loss": 2.0714,
      "step": 256000
    },
    {
      "epoch": 19.563058589870902,
      "grad_norm": 13.160985946655273,
      "learning_rate": 3.369745117510758e-05,
      "loss": 2.1126,
      "step": 256100
    },
    {
      "epoch": 19.57069742571232,
      "grad_norm": 14.942341804504395,
      "learning_rate": 3.3691085478573065e-05,
      "loss": 2.1212,
      "step": 256200
    },
    {
      "epoch": 19.57833626155374,
      "grad_norm": 13.80395793914795,
      "learning_rate": 3.3684719782038556e-05,
      "loss": 2.0204,
      "step": 256300
    },
    {
      "epoch": 19.585975097395156,
      "grad_norm": 10.587867736816406,
      "learning_rate": 3.367835408550404e-05,
      "loss": 2.0246,
      "step": 256400
    },
    {
      "epoch": 19.593613933236576,
      "grad_norm": 11.991792678833008,
      "learning_rate": 3.367198838896952e-05,
      "loss": 2.0707,
      "step": 256500
    },
    {
      "epoch": 19.60125276907799,
      "grad_norm": 17.462024688720703,
      "learning_rate": 3.3665622692435006e-05,
      "loss": 2.101,
      "step": 256600
    },
    {
      "epoch": 19.60889160491941,
      "grad_norm": 12.899674415588379,
      "learning_rate": 3.3659256995900497e-05,
      "loss": 2.0049,
      "step": 256700
    },
    {
      "epoch": 19.61653044076083,
      "grad_norm": 11.492748260498047,
      "learning_rate": 3.365289129936598e-05,
      "loss": 2.0544,
      "step": 256800
    },
    {
      "epoch": 19.624169276602245,
      "grad_norm": 13.244207382202148,
      "learning_rate": 3.3646525602831464e-05,
      "loss": 2.1166,
      "step": 256900
    },
    {
      "epoch": 19.631808112443665,
      "grad_norm": 12.19788932800293,
      "learning_rate": 3.364015990629695e-05,
      "loss": 2.1105,
      "step": 257000
    },
    {
      "epoch": 19.63944694828508,
      "grad_norm": 12.165236473083496,
      "learning_rate": 3.363379420976243e-05,
      "loss": 2.1566,
      "step": 257100
    },
    {
      "epoch": 19.6470857841265,
      "grad_norm": 13.435474395751953,
      "learning_rate": 3.362742851322792e-05,
      "loss": 2.0678,
      "step": 257200
    },
    {
      "epoch": 19.654724619967915,
      "grad_norm": 15.223013877868652,
      "learning_rate": 3.3621062816693405e-05,
      "loss": 2.0112,
      "step": 257300
    },
    {
      "epoch": 19.662363455809334,
      "grad_norm": 12.164268493652344,
      "learning_rate": 3.361469712015889e-05,
      "loss": 2.0784,
      "step": 257400
    },
    {
      "epoch": 19.670002291650754,
      "grad_norm": 14.276515007019043,
      "learning_rate": 3.360833142362437e-05,
      "loss": 2.1401,
      "step": 257500
    },
    {
      "epoch": 19.67764112749217,
      "grad_norm": 12.750102996826172,
      "learning_rate": 3.360196572708986e-05,
      "loss": 2.0722,
      "step": 257600
    },
    {
      "epoch": 19.68527996333359,
      "grad_norm": 14.535481452941895,
      "learning_rate": 3.3595600030555345e-05,
      "loss": 2.0602,
      "step": 257700
    },
    {
      "epoch": 19.692918799175004,
      "grad_norm": 11.497406005859375,
      "learning_rate": 3.358923433402083e-05,
      "loss": 2.0971,
      "step": 257800
    },
    {
      "epoch": 19.700557635016423,
      "grad_norm": 16.075035095214844,
      "learning_rate": 3.358286863748631e-05,
      "loss": 2.1213,
      "step": 257900
    },
    {
      "epoch": 19.708196470857843,
      "grad_norm": 14.37215805053711,
      "learning_rate": 3.3576502940951796e-05,
      "loss": 2.127,
      "step": 258000
    },
    {
      "epoch": 19.715835306699258,
      "grad_norm": 13.017743110656738,
      "learning_rate": 3.3570137244417286e-05,
      "loss": 2.0679,
      "step": 258100
    },
    {
      "epoch": 19.723474142540677,
      "grad_norm": 15.58653450012207,
      "learning_rate": 3.356377154788277e-05,
      "loss": 1.9012,
      "step": 258200
    },
    {
      "epoch": 19.731112978382093,
      "grad_norm": 13.851632118225098,
      "learning_rate": 3.3557405851348253e-05,
      "loss": 2.1206,
      "step": 258300
    },
    {
      "epoch": 19.738751814223512,
      "grad_norm": 14.202563285827637,
      "learning_rate": 3.355104015481374e-05,
      "loss": 1.9802,
      "step": 258400
    },
    {
      "epoch": 19.74639065006493,
      "grad_norm": 13.20174503326416,
      "learning_rate": 3.354467445827922e-05,
      "loss": 2.1058,
      "step": 258500
    },
    {
      "epoch": 19.754029485906347,
      "grad_norm": 11.24333381652832,
      "learning_rate": 3.353830876174471e-05,
      "loss": 2.1235,
      "step": 258600
    },
    {
      "epoch": 19.761668321747766,
      "grad_norm": 10.98646068572998,
      "learning_rate": 3.3531943065210194e-05,
      "loss": 1.9895,
      "step": 258700
    },
    {
      "epoch": 19.769307157589182,
      "grad_norm": 14.277290344238281,
      "learning_rate": 3.3525577368675685e-05,
      "loss": 2.1587,
      "step": 258800
    },
    {
      "epoch": 19.7769459934306,
      "grad_norm": 11.778285026550293,
      "learning_rate": 3.351921167214117e-05,
      "loss": 2.0583,
      "step": 258900
    },
    {
      "epoch": 19.78458482927202,
      "grad_norm": 17.19613265991211,
      "learning_rate": 3.351284597560665e-05,
      "loss": 2.0863,
      "step": 259000
    },
    {
      "epoch": 19.792223665113436,
      "grad_norm": 15.150127410888672,
      "learning_rate": 3.350648027907214e-05,
      "loss": 2.043,
      "step": 259100
    },
    {
      "epoch": 19.799862500954855,
      "grad_norm": 13.669838905334473,
      "learning_rate": 3.3500114582537626e-05,
      "loss": 2.0158,
      "step": 259200
    },
    {
      "epoch": 19.80750133679627,
      "grad_norm": 13.172154426574707,
      "learning_rate": 3.349374888600311e-05,
      "loss": 2.0884,
      "step": 259300
    },
    {
      "epoch": 19.81514017263769,
      "grad_norm": 15.817887306213379,
      "learning_rate": 3.348738318946859e-05,
      "loss": 2.1241,
      "step": 259400
    },
    {
      "epoch": 19.82277900847911,
      "grad_norm": 19.678945541381836,
      "learning_rate": 3.348101749293408e-05,
      "loss": 2.0718,
      "step": 259500
    },
    {
      "epoch": 19.830417844320525,
      "grad_norm": 12.397322654724121,
      "learning_rate": 3.3474651796399567e-05,
      "loss": 2.1189,
      "step": 259600
    },
    {
      "epoch": 19.838056680161944,
      "grad_norm": 11.07293701171875,
      "learning_rate": 3.346828609986505e-05,
      "loss": 1.9513,
      "step": 259700
    },
    {
      "epoch": 19.84569551600336,
      "grad_norm": 15.712896347045898,
      "learning_rate": 3.3461920403330534e-05,
      "loss": 2.0858,
      "step": 259800
    },
    {
      "epoch": 19.85333435184478,
      "grad_norm": 11.101595878601074,
      "learning_rate": 3.3455554706796024e-05,
      "loss": 2.1688,
      "step": 259900
    },
    {
      "epoch": 19.860973187686195,
      "grad_norm": 11.110542297363281,
      "learning_rate": 3.344918901026151e-05,
      "loss": 2.0538,
      "step": 260000
    },
    {
      "epoch": 19.868612023527614,
      "grad_norm": 13.795449256896973,
      "learning_rate": 3.344282331372699e-05,
      "loss": 2.0433,
      "step": 260100
    },
    {
      "epoch": 19.876250859369033,
      "grad_norm": 11.099774360656738,
      "learning_rate": 3.3436457617192475e-05,
      "loss": 2.0399,
      "step": 260200
    },
    {
      "epoch": 19.88388969521045,
      "grad_norm": 10.736090660095215,
      "learning_rate": 3.343009192065796e-05,
      "loss": 2.055,
      "step": 260300
    },
    {
      "epoch": 19.89152853105187,
      "grad_norm": 13.301834106445312,
      "learning_rate": 3.342372622412345e-05,
      "loss": 2.0622,
      "step": 260400
    },
    {
      "epoch": 19.899167366893284,
      "grad_norm": 13.082432746887207,
      "learning_rate": 3.341736052758893e-05,
      "loss": 2.0426,
      "step": 260500
    },
    {
      "epoch": 19.906806202734703,
      "grad_norm": 10.608699798583984,
      "learning_rate": 3.3410994831054415e-05,
      "loss": 2.1531,
      "step": 260600
    },
    {
      "epoch": 19.914445038576122,
      "grad_norm": 16.476520538330078,
      "learning_rate": 3.34046291345199e-05,
      "loss": 2.1109,
      "step": 260700
    },
    {
      "epoch": 19.922083874417538,
      "grad_norm": 12.425085067749023,
      "learning_rate": 3.339826343798539e-05,
      "loss": 2.0899,
      "step": 260800
    },
    {
      "epoch": 19.929722710258957,
      "grad_norm": 9.628870964050293,
      "learning_rate": 3.339189774145087e-05,
      "loss": 2.076,
      "step": 260900
    },
    {
      "epoch": 19.937361546100373,
      "grad_norm": 12.834942817687988,
      "learning_rate": 3.3385532044916356e-05,
      "loss": 2.0846,
      "step": 261000
    },
    {
      "epoch": 19.945000381941792,
      "grad_norm": 17.88326644897461,
      "learning_rate": 3.337916634838184e-05,
      "loss": 2.13,
      "step": 261100
    },
    {
      "epoch": 19.95263921778321,
      "grad_norm": 11.263105392456055,
      "learning_rate": 3.3372800651847323e-05,
      "loss": 2.0909,
      "step": 261200
    },
    {
      "epoch": 19.960278053624627,
      "grad_norm": 12.040725708007812,
      "learning_rate": 3.3366434955312814e-05,
      "loss": 2.0762,
      "step": 261300
    },
    {
      "epoch": 19.967916889466046,
      "grad_norm": 10.010744094848633,
      "learning_rate": 3.33600692587783e-05,
      "loss": 2.1087,
      "step": 261400
    },
    {
      "epoch": 19.975555725307462,
      "grad_norm": 15.034242630004883,
      "learning_rate": 3.335370356224378e-05,
      "loss": 2.0722,
      "step": 261500
    },
    {
      "epoch": 19.98319456114888,
      "grad_norm": 12.91950511932373,
      "learning_rate": 3.3347337865709264e-05,
      "loss": 1.9815,
      "step": 261600
    },
    {
      "epoch": 19.990833396990297,
      "grad_norm": 12.620043754577637,
      "learning_rate": 3.334097216917475e-05,
      "loss": 2.1125,
      "step": 261700
    },
    {
      "epoch": 19.998472232831716,
      "grad_norm": 12.188952445983887,
      "learning_rate": 3.333460647264024e-05,
      "loss": 2.148,
      "step": 261800
    },
    {
      "epoch": 20.0,
      "eval_loss": 2.0223639011383057,
      "eval_runtime": 1.6738,
      "eval_samples_per_second": 412.246,
      "eval_steps_per_second": 412.246,
      "step": 261820
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.8387739658355713,
      "eval_runtime": 31.4931,
      "eval_samples_per_second": 415.678,
      "eval_steps_per_second": 415.678,
      "step": 261820
    },
    {
      "epoch": 20.006111068673135,
      "grad_norm": 12.121868133544922,
      "learning_rate": 3.332824077610572e-05,
      "loss": 2.1195,
      "step": 261900
    },
    {
      "epoch": 20.01374990451455,
      "grad_norm": 9.479621887207031,
      "learning_rate": 3.3321875079571205e-05,
      "loss": 2.0387,
      "step": 262000
    },
    {
      "epoch": 20.02138874035597,
      "grad_norm": 10.617668151855469,
      "learning_rate": 3.331550938303669e-05,
      "loss": 2.0328,
      "step": 262100
    },
    {
      "epoch": 20.029027576197386,
      "grad_norm": 15.67666244506836,
      "learning_rate": 3.330914368650218e-05,
      "loss": 2.0366,
      "step": 262200
    },
    {
      "epoch": 20.036666412038805,
      "grad_norm": 11.01291275024414,
      "learning_rate": 3.330277798996766e-05,
      "loss": 2.0785,
      "step": 262300
    },
    {
      "epoch": 20.044305247880224,
      "grad_norm": 17.02582550048828,
      "learning_rate": 3.3296412293433146e-05,
      "loss": 2.0728,
      "step": 262400
    },
    {
      "epoch": 20.05194408372164,
      "grad_norm": 12.898252487182617,
      "learning_rate": 3.329004659689863e-05,
      "loss": 2.0371,
      "step": 262500
    },
    {
      "epoch": 20.05958291956306,
      "grad_norm": 16.4218807220459,
      "learning_rate": 3.328368090036412e-05,
      "loss": 2.0994,
      "step": 262600
    },
    {
      "epoch": 20.067221755404475,
      "grad_norm": 12.269967079162598,
      "learning_rate": 3.3277315203829604e-05,
      "loss": 2.0744,
      "step": 262700
    },
    {
      "epoch": 20.074860591245894,
      "grad_norm": 10.124835014343262,
      "learning_rate": 3.327094950729509e-05,
      "loss": 2.0939,
      "step": 262800
    },
    {
      "epoch": 20.082499427087313,
      "grad_norm": 12.003997802734375,
      "learning_rate": 3.326458381076058e-05,
      "loss": 2.0467,
      "step": 262900
    },
    {
      "epoch": 20.09013826292873,
      "grad_norm": 14.28530216217041,
      "learning_rate": 3.325821811422606e-05,
      "loss": 2.1009,
      "step": 263000
    },
    {
      "epoch": 20.09777709877015,
      "grad_norm": 14.353489875793457,
      "learning_rate": 3.325185241769155e-05,
      "loss": 2.0032,
      "step": 263100
    },
    {
      "epoch": 20.105415934611564,
      "grad_norm": 12.411043167114258,
      "learning_rate": 3.3245486721157035e-05,
      "loss": 2.139,
      "step": 263200
    },
    {
      "epoch": 20.113054770452983,
      "grad_norm": 12.318740844726562,
      "learning_rate": 3.323912102462252e-05,
      "loss": 1.9876,
      "step": 263300
    },
    {
      "epoch": 20.120693606294402,
      "grad_norm": 14.596272468566895,
      "learning_rate": 3.3232755328088e-05,
      "loss": 2.056,
      "step": 263400
    },
    {
      "epoch": 20.128332442135818,
      "grad_norm": 12.190990447998047,
      "learning_rate": 3.3226389631553485e-05,
      "loss": 2.0917,
      "step": 263500
    },
    {
      "epoch": 20.135971277977237,
      "grad_norm": 7.79421854019165,
      "learning_rate": 3.3220023935018976e-05,
      "loss": 2.0518,
      "step": 263600
    },
    {
      "epoch": 20.143610113818653,
      "grad_norm": 16.15091896057129,
      "learning_rate": 3.321365823848446e-05,
      "loss": 2.0865,
      "step": 263700
    },
    {
      "epoch": 20.151248949660072,
      "grad_norm": 13.908945083618164,
      "learning_rate": 3.320729254194994e-05,
      "loss": 2.0862,
      "step": 263800
    },
    {
      "epoch": 20.158887785501488,
      "grad_norm": 8.949390411376953,
      "learning_rate": 3.3200926845415426e-05,
      "loss": 2.0521,
      "step": 263900
    },
    {
      "epoch": 20.166526621342907,
      "grad_norm": 13.882705688476562,
      "learning_rate": 3.319456114888091e-05,
      "loss": 2.0546,
      "step": 264000
    },
    {
      "epoch": 20.174165457184326,
      "grad_norm": 13.347040176391602,
      "learning_rate": 3.31881954523464e-05,
      "loss": 2.0616,
      "step": 264100
    },
    {
      "epoch": 20.181804293025742,
      "grad_norm": 10.763666152954102,
      "learning_rate": 3.3181829755811884e-05,
      "loss": 2.1126,
      "step": 264200
    },
    {
      "epoch": 20.18944312886716,
      "grad_norm": 13.823967933654785,
      "learning_rate": 3.317546405927737e-05,
      "loss": 2.0105,
      "step": 264300
    },
    {
      "epoch": 20.197081964708577,
      "grad_norm": 14.780989646911621,
      "learning_rate": 3.316909836274285e-05,
      "loss": 2.0025,
      "step": 264400
    },
    {
      "epoch": 20.204720800549996,
      "grad_norm": 13.031429290771484,
      "learning_rate": 3.316273266620834e-05,
      "loss": 2.062,
      "step": 264500
    },
    {
      "epoch": 20.212359636391415,
      "grad_norm": 9.674997329711914,
      "learning_rate": 3.3156366969673825e-05,
      "loss": 1.9561,
      "step": 264600
    },
    {
      "epoch": 20.21999847223283,
      "grad_norm": 12.419463157653809,
      "learning_rate": 3.315000127313931e-05,
      "loss": 2.0866,
      "step": 264700
    },
    {
      "epoch": 20.22763730807425,
      "grad_norm": 13.584417343139648,
      "learning_rate": 3.314363557660479e-05,
      "loss": 2.0535,
      "step": 264800
    },
    {
      "epoch": 20.235276143915666,
      "grad_norm": 13.533876419067383,
      "learning_rate": 3.3137269880070275e-05,
      "loss": 2.0967,
      "step": 264900
    },
    {
      "epoch": 20.242914979757085,
      "grad_norm": 13.033184051513672,
      "learning_rate": 3.3130904183535766e-05,
      "loss": 2.0189,
      "step": 265000
    },
    {
      "epoch": 20.250553815598504,
      "grad_norm": 9.402298927307129,
      "learning_rate": 3.312453848700125e-05,
      "loss": 1.957,
      "step": 265100
    },
    {
      "epoch": 20.25819265143992,
      "grad_norm": 14.718960762023926,
      "learning_rate": 3.311817279046673e-05,
      "loss": 2.1056,
      "step": 265200
    },
    {
      "epoch": 20.26583148728134,
      "grad_norm": 16.217355728149414,
      "learning_rate": 3.3111807093932216e-05,
      "loss": 2.114,
      "step": 265300
    },
    {
      "epoch": 20.273470323122755,
      "grad_norm": 12.822077751159668,
      "learning_rate": 3.3105441397397707e-05,
      "loss": 2.1368,
      "step": 265400
    },
    {
      "epoch": 20.281109158964174,
      "grad_norm": 10.304716110229492,
      "learning_rate": 3.309907570086319e-05,
      "loss": 2.0638,
      "step": 265500
    },
    {
      "epoch": 20.288747994805593,
      "grad_norm": 13.533939361572266,
      "learning_rate": 3.3092710004328674e-05,
      "loss": 2.0505,
      "step": 265600
    },
    {
      "epoch": 20.29638683064701,
      "grad_norm": 11.905762672424316,
      "learning_rate": 3.308634430779416e-05,
      "loss": 2.052,
      "step": 265700
    },
    {
      "epoch": 20.304025666488428,
      "grad_norm": 11.903183937072754,
      "learning_rate": 3.307997861125964e-05,
      "loss": 2.0461,
      "step": 265800
    },
    {
      "epoch": 20.311664502329844,
      "grad_norm": 15.341432571411133,
      "learning_rate": 3.307361291472513e-05,
      "loss": 2.0911,
      "step": 265900
    },
    {
      "epoch": 20.319303338171263,
      "grad_norm": 10.734755516052246,
      "learning_rate": 3.3067247218190615e-05,
      "loss": 2.0887,
      "step": 266000
    },
    {
      "epoch": 20.32694217401268,
      "grad_norm": 13.48318862915039,
      "learning_rate": 3.30608815216561e-05,
      "loss": 2.0579,
      "step": 266100
    },
    {
      "epoch": 20.334581009854098,
      "grad_norm": 12.958892822265625,
      "learning_rate": 3.305451582512158e-05,
      "loss": 2.1092,
      "step": 266200
    },
    {
      "epoch": 20.342219845695517,
      "grad_norm": 10.280925750732422,
      "learning_rate": 3.304815012858707e-05,
      "loss": 1.9913,
      "step": 266300
    },
    {
      "epoch": 20.349858681536933,
      "grad_norm": 17.503910064697266,
      "learning_rate": 3.3041784432052555e-05,
      "loss": 2.0889,
      "step": 266400
    },
    {
      "epoch": 20.357497517378352,
      "grad_norm": 13.307802200317383,
      "learning_rate": 3.303541873551804e-05,
      "loss": 2.0457,
      "step": 266500
    },
    {
      "epoch": 20.365136353219768,
      "grad_norm": 12.915948867797852,
      "learning_rate": 3.302905303898353e-05,
      "loss": 2.0712,
      "step": 266600
    },
    {
      "epoch": 20.372775189061187,
      "grad_norm": 14.86758041381836,
      "learning_rate": 3.302268734244901e-05,
      "loss": 2.0568,
      "step": 266700
    },
    {
      "epoch": 20.380414024902606,
      "grad_norm": 12.079787254333496,
      "learning_rate": 3.3016321645914496e-05,
      "loss": 2.1214,
      "step": 266800
    },
    {
      "epoch": 20.388052860744022,
      "grad_norm": 12.886848449707031,
      "learning_rate": 3.300995594937999e-05,
      "loss": 2.1174,
      "step": 266900
    },
    {
      "epoch": 20.39569169658544,
      "grad_norm": 13.943543434143066,
      "learning_rate": 3.300359025284547e-05,
      "loss": 2.1057,
      "step": 267000
    },
    {
      "epoch": 20.403330532426857,
      "grad_norm": 10.546770095825195,
      "learning_rate": 3.2997224556310954e-05,
      "loss": 2.179,
      "step": 267100
    },
    {
      "epoch": 20.410969368268276,
      "grad_norm": 14.981375694274902,
      "learning_rate": 3.299085885977644e-05,
      "loss": 2.0482,
      "step": 267200
    },
    {
      "epoch": 20.418608204109695,
      "grad_norm": 15.432541847229004,
      "learning_rate": 3.298449316324193e-05,
      "loss": 2.1611,
      "step": 267300
    },
    {
      "epoch": 20.42624703995111,
      "grad_norm": 11.739068031311035,
      "learning_rate": 3.297812746670741e-05,
      "loss": 2.0309,
      "step": 267400
    },
    {
      "epoch": 20.43388587579253,
      "grad_norm": 15.651278495788574,
      "learning_rate": 3.2971761770172895e-05,
      "loss": 1.9347,
      "step": 267500
    },
    {
      "epoch": 20.441524711633946,
      "grad_norm": 16.58391571044922,
      "learning_rate": 3.296539607363838e-05,
      "loss": 2.0219,
      "step": 267600
    },
    {
      "epoch": 20.449163547475365,
      "grad_norm": 13.352967262268066,
      "learning_rate": 3.295903037710387e-05,
      "loss": 2.1273,
      "step": 267700
    },
    {
      "epoch": 20.456802383316784,
      "grad_norm": 19.430980682373047,
      "learning_rate": 3.295266468056935e-05,
      "loss": 2.0038,
      "step": 267800
    },
    {
      "epoch": 20.4644412191582,
      "grad_norm": 16.36313819885254,
      "learning_rate": 3.2946298984034836e-05,
      "loss": 2.099,
      "step": 267900
    },
    {
      "epoch": 20.47208005499962,
      "grad_norm": 13.204229354858398,
      "learning_rate": 3.293993328750032e-05,
      "loss": 2.0897,
      "step": 268000
    },
    {
      "epoch": 20.479718890841035,
      "grad_norm": 13.638526916503906,
      "learning_rate": 3.29335675909658e-05,
      "loss": 2.0693,
      "step": 268100
    },
    {
      "epoch": 20.487357726682454,
      "grad_norm": 13.791193962097168,
      "learning_rate": 3.292720189443129e-05,
      "loss": 1.9509,
      "step": 268200
    },
    {
      "epoch": 20.49499656252387,
      "grad_norm": 12.55335807800293,
      "learning_rate": 3.2920836197896777e-05,
      "loss": 2.1413,
      "step": 268300
    },
    {
      "epoch": 20.50263539836529,
      "grad_norm": 16.641521453857422,
      "learning_rate": 3.291447050136226e-05,
      "loss": 2.0117,
      "step": 268400
    },
    {
      "epoch": 20.510274234206708,
      "grad_norm": 11.175378799438477,
      "learning_rate": 3.2908104804827744e-05,
      "loss": 2.152,
      "step": 268500
    },
    {
      "epoch": 20.517913070048124,
      "grad_norm": 13.713858604431152,
      "learning_rate": 3.2901739108293234e-05,
      "loss": 2.0892,
      "step": 268600
    },
    {
      "epoch": 20.525551905889543,
      "grad_norm": 11.970418930053711,
      "learning_rate": 3.289537341175872e-05,
      "loss": 2.0849,
      "step": 268700
    },
    {
      "epoch": 20.53319074173096,
      "grad_norm": 17.77449607849121,
      "learning_rate": 3.28890077152242e-05,
      "loss": 1.9619,
      "step": 268800
    },
    {
      "epoch": 20.540829577572378,
      "grad_norm": 15.979852676391602,
      "learning_rate": 3.2882642018689685e-05,
      "loss": 2.0688,
      "step": 268900
    },
    {
      "epoch": 20.548468413413797,
      "grad_norm": 15.16553783416748,
      "learning_rate": 3.287627632215517e-05,
      "loss": 2.0374,
      "step": 269000
    },
    {
      "epoch": 20.556107249255213,
      "grad_norm": 15.466216087341309,
      "learning_rate": 3.286991062562066e-05,
      "loss": 2.1131,
      "step": 269100
    },
    {
      "epoch": 20.563746085096632,
      "grad_norm": 8.578741073608398,
      "learning_rate": 3.286354492908614e-05,
      "loss": 1.9777,
      "step": 269200
    },
    {
      "epoch": 20.571384920938048,
      "grad_norm": 12.302896499633789,
      "learning_rate": 3.2857179232551625e-05,
      "loss": 2.1823,
      "step": 269300
    },
    {
      "epoch": 20.579023756779467,
      "grad_norm": 14.35296630859375,
      "learning_rate": 3.285081353601711e-05,
      "loss": 1.9912,
      "step": 269400
    },
    {
      "epoch": 20.586662592620886,
      "grad_norm": 12.59939956665039,
      "learning_rate": 3.28444478394826e-05,
      "loss": 2.1088,
      "step": 269500
    },
    {
      "epoch": 20.594301428462302,
      "grad_norm": 10.609692573547363,
      "learning_rate": 3.283808214294808e-05,
      "loss": 1.9862,
      "step": 269600
    },
    {
      "epoch": 20.60194026430372,
      "grad_norm": 13.684261322021484,
      "learning_rate": 3.2831716446413566e-05,
      "loss": 1.9866,
      "step": 269700
    },
    {
      "epoch": 20.609579100145137,
      "grad_norm": 11.978408813476562,
      "learning_rate": 3.282535074987905e-05,
      "loss": 2.0942,
      "step": 269800
    },
    {
      "epoch": 20.617217935986556,
      "grad_norm": 13.812335968017578,
      "learning_rate": 3.2818985053344533e-05,
      "loss": 1.9752,
      "step": 269900
    },
    {
      "epoch": 20.62485677182797,
      "grad_norm": 14.390270233154297,
      "learning_rate": 3.2812619356810024e-05,
      "loss": 2.0197,
      "step": 270000
    },
    {
      "epoch": 20.63249560766939,
      "grad_norm": 11.341568946838379,
      "learning_rate": 3.280625366027551e-05,
      "loss": 2.0599,
      "step": 270100
    },
    {
      "epoch": 20.64013444351081,
      "grad_norm": 17.939287185668945,
      "learning_rate": 3.279988796374099e-05,
      "loss": 2.0471,
      "step": 270200
    },
    {
      "epoch": 20.647773279352226,
      "grad_norm": 12.697382926940918,
      "learning_rate": 3.279352226720648e-05,
      "loss": 2.0607,
      "step": 270300
    },
    {
      "epoch": 20.655412115193645,
      "grad_norm": 12.64279842376709,
      "learning_rate": 3.2787156570671965e-05,
      "loss": 2.0955,
      "step": 270400
    },
    {
      "epoch": 20.66305095103506,
      "grad_norm": 12.483399391174316,
      "learning_rate": 3.278079087413745e-05,
      "loss": 2.0853,
      "step": 270500
    },
    {
      "epoch": 20.67068978687648,
      "grad_norm": 13.741935729980469,
      "learning_rate": 3.277442517760294e-05,
      "loss": 2.0374,
      "step": 270600
    },
    {
      "epoch": 20.6783286227179,
      "grad_norm": 13.151552200317383,
      "learning_rate": 3.276805948106842e-05,
      "loss": 2.0759,
      "step": 270700
    },
    {
      "epoch": 20.685967458559315,
      "grad_norm": 13.13223934173584,
      "learning_rate": 3.2761693784533906e-05,
      "loss": 1.9904,
      "step": 270800
    },
    {
      "epoch": 20.693606294400734,
      "grad_norm": 11.597668647766113,
      "learning_rate": 3.2755328087999396e-05,
      "loss": 2.0014,
      "step": 270900
    },
    {
      "epoch": 20.70124513024215,
      "grad_norm": 10.410308837890625,
      "learning_rate": 3.274896239146488e-05,
      "loss": 2.0429,
      "step": 271000
    },
    {
      "epoch": 20.70888396608357,
      "grad_norm": 11.594585418701172,
      "learning_rate": 3.274259669493036e-05,
      "loss": 2.0889,
      "step": 271100
    },
    {
      "epoch": 20.716522801924988,
      "grad_norm": 12.921244621276855,
      "learning_rate": 3.2736230998395847e-05,
      "loss": 2.0147,
      "step": 271200
    },
    {
      "epoch": 20.724161637766404,
      "grad_norm": 10.18234634399414,
      "learning_rate": 3.272986530186133e-05,
      "loss": 2.0118,
      "step": 271300
    },
    {
      "epoch": 20.731800473607823,
      "grad_norm": 14.128885269165039,
      "learning_rate": 3.272349960532682e-05,
      "loss": 2.0675,
      "step": 271400
    },
    {
      "epoch": 20.73943930944924,
      "grad_norm": 12.07778549194336,
      "learning_rate": 3.2717133908792304e-05,
      "loss": 1.9783,
      "step": 271500
    },
    {
      "epoch": 20.747078145290658,
      "grad_norm": 10.716675758361816,
      "learning_rate": 3.271076821225779e-05,
      "loss": 2.0535,
      "step": 271600
    },
    {
      "epoch": 20.754716981132077,
      "grad_norm": 12.826248168945312,
      "learning_rate": 3.270440251572327e-05,
      "loss": 2.0401,
      "step": 271700
    },
    {
      "epoch": 20.762355816973493,
      "grad_norm": 11.544990539550781,
      "learning_rate": 3.269803681918876e-05,
      "loss": 2.002,
      "step": 271800
    },
    {
      "epoch": 20.769994652814912,
      "grad_norm": 11.429842948913574,
      "learning_rate": 3.2691671122654245e-05,
      "loss": 2.0114,
      "step": 271900
    },
    {
      "epoch": 20.777633488656328,
      "grad_norm": 12.635689735412598,
      "learning_rate": 3.268530542611973e-05,
      "loss": 2.0398,
      "step": 272000
    },
    {
      "epoch": 20.785272324497747,
      "grad_norm": 14.733986854553223,
      "learning_rate": 3.267893972958521e-05,
      "loss": 2.1097,
      "step": 272100
    },
    {
      "epoch": 20.792911160339166,
      "grad_norm": 12.694751739501953,
      "learning_rate": 3.2672574033050695e-05,
      "loss": 2.0523,
      "step": 272200
    },
    {
      "epoch": 20.80054999618058,
      "grad_norm": 12.213613510131836,
      "learning_rate": 3.2666208336516186e-05,
      "loss": 2.0488,
      "step": 272300
    },
    {
      "epoch": 20.808188832022,
      "grad_norm": 12.886119842529297,
      "learning_rate": 3.265984263998167e-05,
      "loss": 2.0943,
      "step": 272400
    },
    {
      "epoch": 20.815827667863417,
      "grad_norm": 10.868965148925781,
      "learning_rate": 3.265347694344715e-05,
      "loss": 2.0825,
      "step": 272500
    },
    {
      "epoch": 20.823466503704836,
      "grad_norm": 13.257588386535645,
      "learning_rate": 3.2647111246912636e-05,
      "loss": 2.1551,
      "step": 272600
    },
    {
      "epoch": 20.83110533954625,
      "grad_norm": 15.845860481262207,
      "learning_rate": 3.264074555037812e-05,
      "loss": 2.0714,
      "step": 272700
    },
    {
      "epoch": 20.83874417538767,
      "grad_norm": 11.670845031738281,
      "learning_rate": 3.263437985384361e-05,
      "loss": 1.9984,
      "step": 272800
    },
    {
      "epoch": 20.84638301122909,
      "grad_norm": 10.454553604125977,
      "learning_rate": 3.2628014157309094e-05,
      "loss": 2.0274,
      "step": 272900
    },
    {
      "epoch": 20.854021847070506,
      "grad_norm": 12.479989051818848,
      "learning_rate": 3.262164846077458e-05,
      "loss": 2.0523,
      "step": 273000
    },
    {
      "epoch": 20.861660682911925,
      "grad_norm": 14.709243774414062,
      "learning_rate": 3.261528276424006e-05,
      "loss": 2.0344,
      "step": 273100
    },
    {
      "epoch": 20.86929951875334,
      "grad_norm": 13.210712432861328,
      "learning_rate": 3.260891706770555e-05,
      "loss": 2.1051,
      "step": 273200
    },
    {
      "epoch": 20.87693835459476,
      "grad_norm": 12.875359535217285,
      "learning_rate": 3.2602551371171035e-05,
      "loss": 2.1318,
      "step": 273300
    },
    {
      "epoch": 20.88457719043618,
      "grad_norm": 12.736559867858887,
      "learning_rate": 3.259618567463652e-05,
      "loss": 2.1462,
      "step": 273400
    },
    {
      "epoch": 20.892216026277595,
      "grad_norm": 11.688036918640137,
      "learning_rate": 3.2589819978102e-05,
      "loss": 2.0613,
      "step": 273500
    },
    {
      "epoch": 20.899854862119014,
      "grad_norm": 12.588654518127441,
      "learning_rate": 3.2583454281567485e-05,
      "loss": 2.0408,
      "step": 273600
    },
    {
      "epoch": 20.90749369796043,
      "grad_norm": 18.78708267211914,
      "learning_rate": 3.2577088585032976e-05,
      "loss": 2.021,
      "step": 273700
    },
    {
      "epoch": 20.91513253380185,
      "grad_norm": 11.80312442779541,
      "learning_rate": 3.257072288849846e-05,
      "loss": 2.1268,
      "step": 273800
    },
    {
      "epoch": 20.922771369643268,
      "grad_norm": 13.037186622619629,
      "learning_rate": 3.256435719196394e-05,
      "loss": 2.0939,
      "step": 273900
    },
    {
      "epoch": 20.930410205484684,
      "grad_norm": 12.460803985595703,
      "learning_rate": 3.255799149542943e-05,
      "loss": 2.0863,
      "step": 274000
    },
    {
      "epoch": 20.938049041326103,
      "grad_norm": 10.59872055053711,
      "learning_rate": 3.2551625798894917e-05,
      "loss": 1.9604,
      "step": 274100
    },
    {
      "epoch": 20.94568787716752,
      "grad_norm": 12.026385307312012,
      "learning_rate": 3.25452601023604e-05,
      "loss": 2.1743,
      "step": 274200
    },
    {
      "epoch": 20.953326713008938,
      "grad_norm": 11.689923286437988,
      "learning_rate": 3.253889440582589e-05,
      "loss": 2.0948,
      "step": 274300
    },
    {
      "epoch": 20.960965548850353,
      "grad_norm": 13.719222068786621,
      "learning_rate": 3.2532528709291374e-05,
      "loss": 2.151,
      "step": 274400
    },
    {
      "epoch": 20.968604384691773,
      "grad_norm": 12.916858673095703,
      "learning_rate": 3.252616301275686e-05,
      "loss": 2.1047,
      "step": 274500
    },
    {
      "epoch": 20.976243220533192,
      "grad_norm": 15.064166069030762,
      "learning_rate": 3.251979731622235e-05,
      "loss": 1.9937,
      "step": 274600
    },
    {
      "epoch": 20.983882056374608,
      "grad_norm": 13.399092674255371,
      "learning_rate": 3.251343161968783e-05,
      "loss": 2.0998,
      "step": 274700
    },
    {
      "epoch": 20.991520892216027,
      "grad_norm": 13.0147705078125,
      "learning_rate": 3.2507065923153315e-05,
      "loss": 2.0362,
      "step": 274800
    },
    {
      "epoch": 20.999159728057442,
      "grad_norm": 12.83838176727295,
      "learning_rate": 3.25007002266188e-05,
      "loss": 2.0954,
      "step": 274900
    },
    {
      "epoch": 21.0,
      "eval_loss": 2.0110301971435547,
      "eval_runtime": 1.6715,
      "eval_samples_per_second": 412.802,
      "eval_steps_per_second": 412.802,
      "step": 274911
    },
    {
      "epoch": 21.0,
      "eval_loss": 1.8214470148086548,
      "eval_runtime": 31.4154,
      "eval_samples_per_second": 416.706,
      "eval_steps_per_second": 416.706,
      "step": 274911
    },
    {
      "epoch": 21.00679856389886,
      "grad_norm": 12.37592887878418,
      "learning_rate": 3.249433453008429e-05,
      "loss": 2.0813,
      "step": 275000
    },
    {
      "epoch": 21.01443739974028,
      "grad_norm": 10.880769729614258,
      "learning_rate": 3.248796883354977e-05,
      "loss": 2.0318,
      "step": 275100
    },
    {
      "epoch": 21.022076235581697,
      "grad_norm": 11.118809700012207,
      "learning_rate": 3.2481603137015256e-05,
      "loss": 2.0064,
      "step": 275200
    },
    {
      "epoch": 21.029715071423116,
      "grad_norm": 12.927722930908203,
      "learning_rate": 3.247523744048074e-05,
      "loss": 2.0302,
      "step": 275300
    },
    {
      "epoch": 21.03735390726453,
      "grad_norm": 14.28867244720459,
      "learning_rate": 3.246887174394622e-05,
      "loss": 2.0365,
      "step": 275400
    },
    {
      "epoch": 21.04499274310595,
      "grad_norm": 11.648653030395508,
      "learning_rate": 3.246250604741171e-05,
      "loss": 2.0947,
      "step": 275500
    },
    {
      "epoch": 21.05263157894737,
      "grad_norm": 12.729653358459473,
      "learning_rate": 3.24561403508772e-05,
      "loss": 2.0283,
      "step": 275600
    },
    {
      "epoch": 21.060270414788786,
      "grad_norm": 13.652911186218262,
      "learning_rate": 3.244977465434268e-05,
      "loss": 1.9291,
      "step": 275700
    },
    {
      "epoch": 21.067909250630205,
      "grad_norm": 14.069069862365723,
      "learning_rate": 3.2443408957808164e-05,
      "loss": 2.1113,
      "step": 275800
    },
    {
      "epoch": 21.07554808647162,
      "grad_norm": 18.409690856933594,
      "learning_rate": 3.243704326127365e-05,
      "loss": 1.9267,
      "step": 275900
    },
    {
      "epoch": 21.08318692231304,
      "grad_norm": 12.29500961303711,
      "learning_rate": 3.243067756473914e-05,
      "loss": 2.1381,
      "step": 276000
    },
    {
      "epoch": 21.09082575815446,
      "grad_norm": 13.772829055786133,
      "learning_rate": 3.242431186820462e-05,
      "loss": 1.9909,
      "step": 276100
    },
    {
      "epoch": 21.098464593995875,
      "grad_norm": 15.08454418182373,
      "learning_rate": 3.2417946171670105e-05,
      "loss": 1.9566,
      "step": 276200
    },
    {
      "epoch": 21.106103429837294,
      "grad_norm": 12.073336601257324,
      "learning_rate": 3.241158047513559e-05,
      "loss": 1.9658,
      "step": 276300
    },
    {
      "epoch": 21.11374226567871,
      "grad_norm": 15.245719909667969,
      "learning_rate": 3.240521477860108e-05,
      "loss": 2.0769,
      "step": 276400
    },
    {
      "epoch": 21.12138110152013,
      "grad_norm": 11.560382843017578,
      "learning_rate": 3.239884908206656e-05,
      "loss": 2.1163,
      "step": 276500
    },
    {
      "epoch": 21.129019937361544,
      "grad_norm": 13.099343299865723,
      "learning_rate": 3.2392483385532046e-05,
      "loss": 2.0395,
      "step": 276600
    },
    {
      "epoch": 21.136658773202964,
      "grad_norm": 13.758622169494629,
      "learning_rate": 3.238611768899753e-05,
      "loss": 1.9826,
      "step": 276700
    },
    {
      "epoch": 21.144297609044383,
      "grad_norm": 20.127029418945312,
      "learning_rate": 3.237975199246301e-05,
      "loss": 2.0307,
      "step": 276800
    },
    {
      "epoch": 21.1519364448858,
      "grad_norm": 10.316998481750488,
      "learning_rate": 3.23733862959285e-05,
      "loss": 1.8261,
      "step": 276900
    },
    {
      "epoch": 21.159575280727218,
      "grad_norm": 13.101715087890625,
      "learning_rate": 3.2367020599393987e-05,
      "loss": 1.9886,
      "step": 277000
    },
    {
      "epoch": 21.167214116568633,
      "grad_norm": 13.698309898376465,
      "learning_rate": 3.236065490285947e-05,
      "loss": 2.0406,
      "step": 277100
    },
    {
      "epoch": 21.174852952410053,
      "grad_norm": 12.044465065002441,
      "learning_rate": 3.2354289206324954e-05,
      "loss": 2.0007,
      "step": 277200
    },
    {
      "epoch": 21.182491788251472,
      "grad_norm": 17.00368881225586,
      "learning_rate": 3.2347923509790444e-05,
      "loss": 2.1023,
      "step": 277300
    },
    {
      "epoch": 21.190130624092888,
      "grad_norm": 11.280447959899902,
      "learning_rate": 3.234155781325593e-05,
      "loss": 2.1623,
      "step": 277400
    },
    {
      "epoch": 21.197769459934307,
      "grad_norm": 10.508428573608398,
      "learning_rate": 3.233519211672141e-05,
      "loss": 1.9883,
      "step": 277500
    },
    {
      "epoch": 21.205408295775722,
      "grad_norm": 11.83983039855957,
      "learning_rate": 3.2328826420186895e-05,
      "loss": 2.0545,
      "step": 277600
    },
    {
      "epoch": 21.21304713161714,
      "grad_norm": 10.227349281311035,
      "learning_rate": 3.232246072365238e-05,
      "loss": 2.0576,
      "step": 277700
    },
    {
      "epoch": 21.22068596745856,
      "grad_norm": 11.09418773651123,
      "learning_rate": 3.231609502711787e-05,
      "loss": 2.0767,
      "step": 277800
    },
    {
      "epoch": 21.228324803299977,
      "grad_norm": 12.215863227844238,
      "learning_rate": 3.230972933058335e-05,
      "loss": 2.0552,
      "step": 277900
    },
    {
      "epoch": 21.235963639141396,
      "grad_norm": 16.597532272338867,
      "learning_rate": 3.2303363634048835e-05,
      "loss": 2.1007,
      "step": 278000
    },
    {
      "epoch": 21.24360247498281,
      "grad_norm": 19.028316497802734,
      "learning_rate": 3.2296997937514326e-05,
      "loss": 1.9015,
      "step": 278100
    },
    {
      "epoch": 21.25124131082423,
      "grad_norm": 11.770817756652832,
      "learning_rate": 3.229063224097981e-05,
      "loss": 2.0856,
      "step": 278200
    },
    {
      "epoch": 21.25888014666565,
      "grad_norm": 13.620092391967773,
      "learning_rate": 3.22842665444453e-05,
      "loss": 2.0097,
      "step": 278300
    },
    {
      "epoch": 21.266518982507066,
      "grad_norm": 14.166446685791016,
      "learning_rate": 3.227790084791078e-05,
      "loss": 2.0992,
      "step": 278400
    },
    {
      "epoch": 21.274157818348485,
      "grad_norm": 12.781479835510254,
      "learning_rate": 3.227153515137627e-05,
      "loss": 2.0331,
      "step": 278500
    },
    {
      "epoch": 21.2817966541899,
      "grad_norm": 14.816267967224121,
      "learning_rate": 3.226516945484175e-05,
      "loss": 2.1297,
      "step": 278600
    },
    {
      "epoch": 21.28943549003132,
      "grad_norm": 12.023052215576172,
      "learning_rate": 3.225880375830724e-05,
      "loss": 1.9894,
      "step": 278700
    },
    {
      "epoch": 21.297074325872735,
      "grad_norm": 12.658793449401855,
      "learning_rate": 3.2252438061772724e-05,
      "loss": 1.9126,
      "step": 278800
    },
    {
      "epoch": 21.304713161714155,
      "grad_norm": 15.719122886657715,
      "learning_rate": 3.224607236523821e-05,
      "loss": 2.0246,
      "step": 278900
    },
    {
      "epoch": 21.312351997555574,
      "grad_norm": 14.921857833862305,
      "learning_rate": 3.223970666870369e-05,
      "loss": 2.0009,
      "step": 279000
    },
    {
      "epoch": 21.31999083339699,
      "grad_norm": 12.049389839172363,
      "learning_rate": 3.2233340972169175e-05,
      "loss": 2.0673,
      "step": 279100
    },
    {
      "epoch": 21.32762966923841,
      "grad_norm": 10.831286430358887,
      "learning_rate": 3.2226975275634665e-05,
      "loss": 2.1112,
      "step": 279200
    },
    {
      "epoch": 21.335268505079824,
      "grad_norm": 12.28432846069336,
      "learning_rate": 3.222060957910015e-05,
      "loss": 2.0443,
      "step": 279300
    },
    {
      "epoch": 21.342907340921244,
      "grad_norm": 13.20230484008789,
      "learning_rate": 3.221424388256563e-05,
      "loss": 2.0788,
      "step": 279400
    },
    {
      "epoch": 21.350546176762663,
      "grad_norm": 14.401912689208984,
      "learning_rate": 3.2207878186031116e-05,
      "loss": 2.13,
      "step": 279500
    },
    {
      "epoch": 21.35818501260408,
      "grad_norm": 12.559088706970215,
      "learning_rate": 3.2201512489496606e-05,
      "loss": 1.9926,
      "step": 279600
    },
    {
      "epoch": 21.365823848445498,
      "grad_norm": 9.488547325134277,
      "learning_rate": 3.219514679296209e-05,
      "loss": 2.1659,
      "step": 279700
    },
    {
      "epoch": 21.373462684286913,
      "grad_norm": 11.953709602355957,
      "learning_rate": 3.218878109642757e-05,
      "loss": 2.0418,
      "step": 279800
    },
    {
      "epoch": 21.381101520128333,
      "grad_norm": 14.64238452911377,
      "learning_rate": 3.2182415399893057e-05,
      "loss": 2.0928,
      "step": 279900
    },
    {
      "epoch": 21.38874035596975,
      "grad_norm": 10.773487091064453,
      "learning_rate": 3.217604970335854e-05,
      "loss": 2.0772,
      "step": 280000
    },
    {
      "epoch": 21.396379191811167,
      "grad_norm": 9.992624282836914,
      "learning_rate": 3.216968400682403e-05,
      "loss": 2.0742,
      "step": 280100
    },
    {
      "epoch": 21.404018027652587,
      "grad_norm": 14.025322914123535,
      "learning_rate": 3.2163318310289514e-05,
      "loss": 2.0772,
      "step": 280200
    },
    {
      "epoch": 21.411656863494002,
      "grad_norm": 12.475214958190918,
      "learning_rate": 3.2156952613755e-05,
      "loss": 2.0508,
      "step": 280300
    },
    {
      "epoch": 21.41929569933542,
      "grad_norm": 12.049681663513184,
      "learning_rate": 3.215058691722048e-05,
      "loss": 2.1678,
      "step": 280400
    },
    {
      "epoch": 21.42693453517684,
      "grad_norm": 15.171379089355469,
      "learning_rate": 3.214422122068597e-05,
      "loss": 2.0525,
      "step": 280500
    },
    {
      "epoch": 21.434573371018256,
      "grad_norm": 14.47773551940918,
      "learning_rate": 3.2137855524151455e-05,
      "loss": 2.1009,
      "step": 280600
    },
    {
      "epoch": 21.442212206859676,
      "grad_norm": 11.732221603393555,
      "learning_rate": 3.213148982761694e-05,
      "loss": 2.0668,
      "step": 280700
    },
    {
      "epoch": 21.44985104270109,
      "grad_norm": 15.682761192321777,
      "learning_rate": 3.212512413108242e-05,
      "loss": 2.1315,
      "step": 280800
    },
    {
      "epoch": 21.45748987854251,
      "grad_norm": 14.635522842407227,
      "learning_rate": 3.2118758434547905e-05,
      "loss": 2.0393,
      "step": 280900
    },
    {
      "epoch": 21.465128714383926,
      "grad_norm": 12.117718696594238,
      "learning_rate": 3.2112392738013396e-05,
      "loss": 2.0783,
      "step": 281000
    },
    {
      "epoch": 21.472767550225345,
      "grad_norm": 13.101868629455566,
      "learning_rate": 3.210602704147888e-05,
      "loss": 1.9962,
      "step": 281100
    },
    {
      "epoch": 21.480406386066765,
      "grad_norm": 9.411620140075684,
      "learning_rate": 3.209966134494436e-05,
      "loss": 2.0459,
      "step": 281200
    },
    {
      "epoch": 21.48804522190818,
      "grad_norm": 14.795165061950684,
      "learning_rate": 3.2093295648409846e-05,
      "loss": 1.9757,
      "step": 281300
    },
    {
      "epoch": 21.4956840577496,
      "grad_norm": 12.545639038085938,
      "learning_rate": 3.208692995187533e-05,
      "loss": 2.0469,
      "step": 281400
    },
    {
      "epoch": 21.503322893591015,
      "grad_norm": 11.84428882598877,
      "learning_rate": 3.208056425534082e-05,
      "loss": 2.0187,
      "step": 281500
    },
    {
      "epoch": 21.510961729432434,
      "grad_norm": 11.525296211242676,
      "learning_rate": 3.2074198558806304e-05,
      "loss": 2.0522,
      "step": 281600
    },
    {
      "epoch": 21.518600565273854,
      "grad_norm": 10.393786430358887,
      "learning_rate": 3.206783286227179e-05,
      "loss": 2.0344,
      "step": 281700
    },
    {
      "epoch": 21.52623940111527,
      "grad_norm": 12.81714153289795,
      "learning_rate": 3.206146716573728e-05,
      "loss": 2.0245,
      "step": 281800
    },
    {
      "epoch": 21.53387823695669,
      "grad_norm": 14.624424934387207,
      "learning_rate": 3.205510146920276e-05,
      "loss": 2.0093,
      "step": 281900
    },
    {
      "epoch": 21.541517072798104,
      "grad_norm": 11.914190292358398,
      "learning_rate": 3.2048735772668245e-05,
      "loss": 2.0497,
      "step": 282000
    },
    {
      "epoch": 21.549155908639523,
      "grad_norm": 13.521567344665527,
      "learning_rate": 3.2042370076133735e-05,
      "loss": 2.0128,
      "step": 282100
    },
    {
      "epoch": 21.556794744480943,
      "grad_norm": 13.58781623840332,
      "learning_rate": 3.203600437959922e-05,
      "loss": 2.0493,
      "step": 282200
    },
    {
      "epoch": 21.56443358032236,
      "grad_norm": 16.414501190185547,
      "learning_rate": 3.20296386830647e-05,
      "loss": 2.0874,
      "step": 282300
    },
    {
      "epoch": 21.572072416163778,
      "grad_norm": 14.208332061767578,
      "learning_rate": 3.202327298653019e-05,
      "loss": 2.0425,
      "step": 282400
    },
    {
      "epoch": 21.579711252005193,
      "grad_norm": 15.230324745178223,
      "learning_rate": 3.2016907289995676e-05,
      "loss": 2.0472,
      "step": 282500
    },
    {
      "epoch": 21.587350087846612,
      "grad_norm": 9.337244033813477,
      "learning_rate": 3.201054159346116e-05,
      "loss": 2.0127,
      "step": 282600
    },
    {
      "epoch": 21.594988923688028,
      "grad_norm": 10.99490737915039,
      "learning_rate": 3.200417589692664e-05,
      "loss": 2.0195,
      "step": 282700
    },
    {
      "epoch": 21.602627759529447,
      "grad_norm": 13.581491470336914,
      "learning_rate": 3.199781020039213e-05,
      "loss": 2.1073,
      "step": 282800
    },
    {
      "epoch": 21.610266595370867,
      "grad_norm": 13.939098358154297,
      "learning_rate": 3.199144450385762e-05,
      "loss": 2.085,
      "step": 282900
    },
    {
      "epoch": 21.617905431212282,
      "grad_norm": 16.41144561767578,
      "learning_rate": 3.19850788073231e-05,
      "loss": 1.9194,
      "step": 283000
    },
    {
      "epoch": 21.6255442670537,
      "grad_norm": 11.603838920593262,
      "learning_rate": 3.1978713110788584e-05,
      "loss": 2.0652,
      "step": 283100
    },
    {
      "epoch": 21.633183102895117,
      "grad_norm": 14.413956642150879,
      "learning_rate": 3.197234741425407e-05,
      "loss": 2.1159,
      "step": 283200
    },
    {
      "epoch": 21.640821938736536,
      "grad_norm": 14.441609382629395,
      "learning_rate": 3.196598171771956e-05,
      "loss": 2.0963,
      "step": 283300
    },
    {
      "epoch": 21.648460774577956,
      "grad_norm": 15.295737266540527,
      "learning_rate": 3.195961602118504e-05,
      "loss": 2.1125,
      "step": 283400
    },
    {
      "epoch": 21.65609961041937,
      "grad_norm": 12.212137222290039,
      "learning_rate": 3.1953250324650525e-05,
      "loss": 1.9261,
      "step": 283500
    },
    {
      "epoch": 21.66373844626079,
      "grad_norm": 12.027444839477539,
      "learning_rate": 3.194688462811601e-05,
      "loss": 2.0603,
      "step": 283600
    },
    {
      "epoch": 21.671377282102206,
      "grad_norm": 13.579628944396973,
      "learning_rate": 3.19405189315815e-05,
      "loss": 2.0195,
      "step": 283700
    },
    {
      "epoch": 21.679016117943625,
      "grad_norm": 13.166056632995605,
      "learning_rate": 3.193415323504698e-05,
      "loss": 2.0436,
      "step": 283800
    },
    {
      "epoch": 21.686654953785045,
      "grad_norm": 11.099656105041504,
      "learning_rate": 3.1927787538512466e-05,
      "loss": 2.1393,
      "step": 283900
    },
    {
      "epoch": 21.69429378962646,
      "grad_norm": 12.043978691101074,
      "learning_rate": 3.192142184197795e-05,
      "loss": 2.0358,
      "step": 284000
    },
    {
      "epoch": 21.70193262546788,
      "grad_norm": 13.609236717224121,
      "learning_rate": 3.191505614544343e-05,
      "loss": 1.9805,
      "step": 284100
    },
    {
      "epoch": 21.709571461309295,
      "grad_norm": 14.711625099182129,
      "learning_rate": 3.190869044890892e-05,
      "loss": 2.072,
      "step": 284200
    },
    {
      "epoch": 21.717210297150714,
      "grad_norm": 13.919818878173828,
      "learning_rate": 3.190232475237441e-05,
      "loss": 2.0528,
      "step": 284300
    },
    {
      "epoch": 21.724849132992134,
      "grad_norm": 12.999114990234375,
      "learning_rate": 3.189595905583989e-05,
      "loss": 2.1158,
      "step": 284400
    },
    {
      "epoch": 21.73248796883355,
      "grad_norm": 12.43041706085205,
      "learning_rate": 3.1889593359305374e-05,
      "loss": 2.0347,
      "step": 284500
    },
    {
      "epoch": 21.74012680467497,
      "grad_norm": 12.661376953125,
      "learning_rate": 3.188322766277086e-05,
      "loss": 2.0228,
      "step": 284600
    },
    {
      "epoch": 21.747765640516384,
      "grad_norm": 13.984111785888672,
      "learning_rate": 3.187686196623635e-05,
      "loss": 2.1457,
      "step": 284700
    },
    {
      "epoch": 21.755404476357803,
      "grad_norm": 15.342596054077148,
      "learning_rate": 3.187049626970183e-05,
      "loss": 2.0788,
      "step": 284800
    },
    {
      "epoch": 21.763043312199223,
      "grad_norm": 12.6051664352417,
      "learning_rate": 3.1864130573167315e-05,
      "loss": 2.0723,
      "step": 284900
    },
    {
      "epoch": 21.77068214804064,
      "grad_norm": 14.502498626708984,
      "learning_rate": 3.18577648766328e-05,
      "loss": 2.082,
      "step": 285000
    },
    {
      "epoch": 21.778320983882058,
      "grad_norm": 13.547051429748535,
      "learning_rate": 3.185139918009829e-05,
      "loss": 2.0371,
      "step": 285100
    },
    {
      "epoch": 21.785959819723473,
      "grad_norm": 10.383190155029297,
      "learning_rate": 3.184503348356377e-05,
      "loss": 2.0336,
      "step": 285200
    },
    {
      "epoch": 21.793598655564892,
      "grad_norm": 11.68163013458252,
      "learning_rate": 3.1838667787029256e-05,
      "loss": 2.1017,
      "step": 285300
    },
    {
      "epoch": 21.801237491406308,
      "grad_norm": 10.935911178588867,
      "learning_rate": 3.183230209049474e-05,
      "loss": 2.0755,
      "step": 285400
    },
    {
      "epoch": 21.808876327247727,
      "grad_norm": 12.363079071044922,
      "learning_rate": 3.182593639396023e-05,
      "loss": 2.0814,
      "step": 285500
    },
    {
      "epoch": 21.816515163089147,
      "grad_norm": 12.480875015258789,
      "learning_rate": 3.181957069742571e-05,
      "loss": 2.0807,
      "step": 285600
    },
    {
      "epoch": 21.824153998930562,
      "grad_norm": 12.630727767944336,
      "learning_rate": 3.1813205000891197e-05,
      "loss": 2.162,
      "step": 285700
    },
    {
      "epoch": 21.83179283477198,
      "grad_norm": 12.813990592956543,
      "learning_rate": 3.180683930435669e-05,
      "loss": 1.9944,
      "step": 285800
    },
    {
      "epoch": 21.839431670613397,
      "grad_norm": 13.220494270324707,
      "learning_rate": 3.180047360782217e-05,
      "loss": 2.079,
      "step": 285900
    },
    {
      "epoch": 21.847070506454816,
      "grad_norm": 11.88097095489502,
      "learning_rate": 3.1794107911287654e-05,
      "loss": 2.0027,
      "step": 286000
    },
    {
      "epoch": 21.854709342296236,
      "grad_norm": 10.819574356079102,
      "learning_rate": 3.1787742214753144e-05,
      "loss": 2.0971,
      "step": 286100
    },
    {
      "epoch": 21.86234817813765,
      "grad_norm": 12.553604125976562,
      "learning_rate": 3.178137651821863e-05,
      "loss": 1.9812,
      "step": 286200
    },
    {
      "epoch": 21.86998701397907,
      "grad_norm": 12.194401741027832,
      "learning_rate": 3.177501082168411e-05,
      "loss": 2.0303,
      "step": 286300
    },
    {
      "epoch": 21.877625849820486,
      "grad_norm": 13.462055206298828,
      "learning_rate": 3.1768645125149595e-05,
      "loss": 1.9366,
      "step": 286400
    },
    {
      "epoch": 21.885264685661905,
      "grad_norm": 13.401054382324219,
      "learning_rate": 3.1762279428615085e-05,
      "loss": 2.0105,
      "step": 286500
    },
    {
      "epoch": 21.892903521503325,
      "grad_norm": 11.830535888671875,
      "learning_rate": 3.175591373208057e-05,
      "loss": 2.0732,
      "step": 286600
    },
    {
      "epoch": 21.90054235734474,
      "grad_norm": 15.156174659729004,
      "learning_rate": 3.174954803554605e-05,
      "loss": 2.1067,
      "step": 286700
    },
    {
      "epoch": 21.90818119318616,
      "grad_norm": 11.255624771118164,
      "learning_rate": 3.1743182339011536e-05,
      "loss": 2.089,
      "step": 286800
    },
    {
      "epoch": 21.915820029027575,
      "grad_norm": 14.681828498840332,
      "learning_rate": 3.1736816642477026e-05,
      "loss": 2.0249,
      "step": 286900
    },
    {
      "epoch": 21.923458864868994,
      "grad_norm": 15.013611793518066,
      "learning_rate": 3.173045094594251e-05,
      "loss": 2.0797,
      "step": 287000
    },
    {
      "epoch": 21.93109770071041,
      "grad_norm": 13.532455444335938,
      "learning_rate": 3.172408524940799e-05,
      "loss": 2.1083,
      "step": 287100
    },
    {
      "epoch": 21.93873653655183,
      "grad_norm": 12.296004295349121,
      "learning_rate": 3.171771955287348e-05,
      "loss": 1.9897,
      "step": 287200
    },
    {
      "epoch": 21.94637537239325,
      "grad_norm": 15.78441333770752,
      "learning_rate": 3.171135385633896e-05,
      "loss": 2.075,
      "step": 287300
    },
    {
      "epoch": 21.954014208234664,
      "grad_norm": 13.709786415100098,
      "learning_rate": 3.170498815980445e-05,
      "loss": 2.0053,
      "step": 287400
    },
    {
      "epoch": 21.961653044076083,
      "grad_norm": 12.868882179260254,
      "learning_rate": 3.1698622463269934e-05,
      "loss": 2.0337,
      "step": 287500
    },
    {
      "epoch": 21.9692918799175,
      "grad_norm": 10.371159553527832,
      "learning_rate": 3.169225676673542e-05,
      "loss": 1.9933,
      "step": 287600
    },
    {
      "epoch": 21.976930715758918,
      "grad_norm": 14.760143280029297,
      "learning_rate": 3.16858910702009e-05,
      "loss": 2.0584,
      "step": 287700
    },
    {
      "epoch": 21.984569551600337,
      "grad_norm": 11.653572082519531,
      "learning_rate": 3.1679525373666385e-05,
      "loss": 2.0487,
      "step": 287800
    },
    {
      "epoch": 21.992208387441753,
      "grad_norm": 16.649036407470703,
      "learning_rate": 3.1673159677131875e-05,
      "loss": 2.0911,
      "step": 287900
    },
    {
      "epoch": 21.999847223283172,
      "grad_norm": 19.706218719482422,
      "learning_rate": 3.166679398059736e-05,
      "loss": 2.0448,
      "step": 288000
    },
    {
      "epoch": 22.0,
      "eval_loss": 2.0095458030700684,
      "eval_runtime": 1.6612,
      "eval_samples_per_second": 415.369,
      "eval_steps_per_second": 415.369,
      "step": 288002
    },
    {
      "epoch": 22.0,
      "eval_loss": 1.8116540908813477,
      "eval_runtime": 31.4349,
      "eval_samples_per_second": 416.448,
      "eval_steps_per_second": 416.448,
      "step": 288002
    },
    {
      "epoch": 22.007486059124588,
      "grad_norm": 13.322497367858887,
      "learning_rate": 3.166042828406284e-05,
      "loss": 1.986,
      "step": 288100
    },
    {
      "epoch": 22.015124894966007,
      "grad_norm": 15.252457618713379,
      "learning_rate": 3.1654062587528326e-05,
      "loss": 1.8794,
      "step": 288200
    },
    {
      "epoch": 22.022763730807426,
      "grad_norm": 14.645246505737305,
      "learning_rate": 3.1647696890993816e-05,
      "loss": 2.0675,
      "step": 288300
    },
    {
      "epoch": 22.030402566648842,
      "grad_norm": 9.72935962677002,
      "learning_rate": 3.16413311944593e-05,
      "loss": 2.0788,
      "step": 288400
    },
    {
      "epoch": 22.03804140249026,
      "grad_norm": 16.28357696533203,
      "learning_rate": 3.163496549792478e-05,
      "loss": 2.0124,
      "step": 288500
    },
    {
      "epoch": 22.045680238331677,
      "grad_norm": 14.868739128112793,
      "learning_rate": 3.1628599801390267e-05,
      "loss": 2.0348,
      "step": 288600
    },
    {
      "epoch": 22.053319074173096,
      "grad_norm": 10.492697715759277,
      "learning_rate": 3.162223410485575e-05,
      "loss": 1.9856,
      "step": 288700
    },
    {
      "epoch": 22.060957910014515,
      "grad_norm": 13.779026985168457,
      "learning_rate": 3.161586840832124e-05,
      "loss": 2.0942,
      "step": 288800
    },
    {
      "epoch": 22.06859674585593,
      "grad_norm": 12.188924789428711,
      "learning_rate": 3.1609502711786724e-05,
      "loss": 1.997,
      "step": 288900
    },
    {
      "epoch": 22.07623558169735,
      "grad_norm": 21.01285743713379,
      "learning_rate": 3.160313701525221e-05,
      "loss": 2.0844,
      "step": 289000
    },
    {
      "epoch": 22.083874417538766,
      "grad_norm": 11.789835929870605,
      "learning_rate": 3.159677131871769e-05,
      "loss": 1.9342,
      "step": 289100
    },
    {
      "epoch": 22.091513253380185,
      "grad_norm": 11.05614185333252,
      "learning_rate": 3.159040562218318e-05,
      "loss": 2.026,
      "step": 289200
    },
    {
      "epoch": 22.0991520892216,
      "grad_norm": 13.299731254577637,
      "learning_rate": 3.1584039925648665e-05,
      "loss": 1.9858,
      "step": 289300
    },
    {
      "epoch": 22.10679092506302,
      "grad_norm": 12.237661361694336,
      "learning_rate": 3.157767422911415e-05,
      "loss": 1.8662,
      "step": 289400
    },
    {
      "epoch": 22.11442976090444,
      "grad_norm": 10.199490547180176,
      "learning_rate": 3.157130853257964e-05,
      "loss": 1.9727,
      "step": 289500
    },
    {
      "epoch": 22.122068596745855,
      "grad_norm": 14.49400806427002,
      "learning_rate": 3.156494283604512e-05,
      "loss": 2.0203,
      "step": 289600
    },
    {
      "epoch": 22.129707432587274,
      "grad_norm": 10.196542739868164,
      "learning_rate": 3.1558577139510606e-05,
      "loss": 2.0438,
      "step": 289700
    },
    {
      "epoch": 22.13734626842869,
      "grad_norm": 14.772384643554688,
      "learning_rate": 3.1552211442976096e-05,
      "loss": 2.0085,
      "step": 289800
    },
    {
      "epoch": 22.14498510427011,
      "grad_norm": 12.4092378616333,
      "learning_rate": 3.154584574644158e-05,
      "loss": 1.9456,
      "step": 289900
    },
    {
      "epoch": 22.15262394011153,
      "grad_norm": 12.389330863952637,
      "learning_rate": 3.153948004990706e-05,
      "loss": 1.9955,
      "step": 290000
    },
    {
      "epoch": 22.160262775952944,
      "grad_norm": 10.841511726379395,
      "learning_rate": 3.153311435337255e-05,
      "loss": 1.956,
      "step": 290100
    },
    {
      "epoch": 22.167901611794363,
      "grad_norm": 12.071027755737305,
      "learning_rate": 3.152674865683804e-05,
      "loss": 2.1523,
      "step": 290200
    },
    {
      "epoch": 22.17554044763578,
      "grad_norm": 13.768904685974121,
      "learning_rate": 3.152038296030352e-05,
      "loss": 2.0243,
      "step": 290300
    },
    {
      "epoch": 22.183179283477198,
      "grad_norm": 11.372714042663574,
      "learning_rate": 3.1514017263769004e-05,
      "loss": 2.0562,
      "step": 290400
    },
    {
      "epoch": 22.190818119318617,
      "grad_norm": 14.075465202331543,
      "learning_rate": 3.150765156723449e-05,
      "loss": 2.1029,
      "step": 290500
    },
    {
      "epoch": 22.198456955160033,
      "grad_norm": 11.89559555053711,
      "learning_rate": 3.150128587069998e-05,
      "loss": 1.9569,
      "step": 290600
    },
    {
      "epoch": 22.206095791001452,
      "grad_norm": 14.042054176330566,
      "learning_rate": 3.149492017416546e-05,
      "loss": 2.1092,
      "step": 290700
    },
    {
      "epoch": 22.213734626842868,
      "grad_norm": 13.3807373046875,
      "learning_rate": 3.1488554477630945e-05,
      "loss": 2.0611,
      "step": 290800
    },
    {
      "epoch": 22.221373462684287,
      "grad_norm": 13.878767013549805,
      "learning_rate": 3.148218878109643e-05,
      "loss": 2.0453,
      "step": 290900
    },
    {
      "epoch": 22.229012298525706,
      "grad_norm": 10.732579231262207,
      "learning_rate": 3.147582308456191e-05,
      "loss": 1.9933,
      "step": 291000
    },
    {
      "epoch": 22.236651134367122,
      "grad_norm": 13.193855285644531,
      "learning_rate": 3.14694573880274e-05,
      "loss": 1.9939,
      "step": 291100
    },
    {
      "epoch": 22.24428997020854,
      "grad_norm": 17.617942810058594,
      "learning_rate": 3.1463091691492886e-05,
      "loss": 2.0554,
      "step": 291200
    },
    {
      "epoch": 22.251928806049957,
      "grad_norm": 11.53173542022705,
      "learning_rate": 3.145672599495837e-05,
      "loss": 2.0243,
      "step": 291300
    },
    {
      "epoch": 22.259567641891376,
      "grad_norm": 14.747692108154297,
      "learning_rate": 3.145036029842385e-05,
      "loss": 1.9931,
      "step": 291400
    },
    {
      "epoch": 22.267206477732792,
      "grad_norm": 14.764936447143555,
      "learning_rate": 3.144399460188934e-05,
      "loss": 1.9882,
      "step": 291500
    },
    {
      "epoch": 22.27484531357421,
      "grad_norm": 14.322219848632812,
      "learning_rate": 3.143762890535483e-05,
      "loss": 2.06,
      "step": 291600
    },
    {
      "epoch": 22.28248414941563,
      "grad_norm": 8.476486206054688,
      "learning_rate": 3.143126320882031e-05,
      "loss": 1.9282,
      "step": 291700
    },
    {
      "epoch": 22.290122985257046,
      "grad_norm": 11.79621696472168,
      "learning_rate": 3.1424897512285794e-05,
      "loss": 1.9717,
      "step": 291800
    },
    {
      "epoch": 22.297761821098465,
      "grad_norm": 12.718875885009766,
      "learning_rate": 3.141853181575128e-05,
      "loss": 2.0669,
      "step": 291900
    },
    {
      "epoch": 22.30540065693988,
      "grad_norm": 14.325115203857422,
      "learning_rate": 3.141216611921677e-05,
      "loss": 1.9902,
      "step": 292000
    },
    {
      "epoch": 22.3130394927813,
      "grad_norm": 14.844858169555664,
      "learning_rate": 3.140580042268225e-05,
      "loss": 2.0027,
      "step": 292100
    },
    {
      "epoch": 22.32067832862272,
      "grad_norm": 11.13025188446045,
      "learning_rate": 3.1399434726147735e-05,
      "loss": 2.0784,
      "step": 292200
    },
    {
      "epoch": 22.328317164464135,
      "grad_norm": 15.262883186340332,
      "learning_rate": 3.139306902961322e-05,
      "loss": 2.0787,
      "step": 292300
    },
    {
      "epoch": 22.335956000305554,
      "grad_norm": 12.603391647338867,
      "learning_rate": 3.138670333307871e-05,
      "loss": 2.085,
      "step": 292400
    },
    {
      "epoch": 22.34359483614697,
      "grad_norm": 13.866058349609375,
      "learning_rate": 3.138033763654419e-05,
      "loss": 2.0614,
      "step": 292500
    },
    {
      "epoch": 22.35123367198839,
      "grad_norm": 14.676116943359375,
      "learning_rate": 3.1373971940009676e-05,
      "loss": 2.0201,
      "step": 292600
    },
    {
      "epoch": 22.35887250782981,
      "grad_norm": 13.696850776672363,
      "learning_rate": 3.136760624347516e-05,
      "loss": 2.1143,
      "step": 292700
    },
    {
      "epoch": 22.366511343671224,
      "grad_norm": 9.992698669433594,
      "learning_rate": 3.136124054694064e-05,
      "loss": 1.9828,
      "step": 292800
    },
    {
      "epoch": 22.374150179512643,
      "grad_norm": 14.104240417480469,
      "learning_rate": 3.135487485040613e-05,
      "loss": 2.058,
      "step": 292900
    },
    {
      "epoch": 22.38178901535406,
      "grad_norm": 12.370453834533691,
      "learning_rate": 3.134850915387162e-05,
      "loss": 2.0641,
      "step": 293000
    },
    {
      "epoch": 22.389427851195478,
      "grad_norm": 11.258346557617188,
      "learning_rate": 3.13421434573371e-05,
      "loss": 1.9854,
      "step": 293100
    },
    {
      "epoch": 22.397066687036897,
      "grad_norm": 15.330503463745117,
      "learning_rate": 3.1335777760802584e-05,
      "loss": 2.0018,
      "step": 293200
    },
    {
      "epoch": 22.404705522878313,
      "grad_norm": 10.109923362731934,
      "learning_rate": 3.1329412064268074e-05,
      "loss": 2.067,
      "step": 293300
    },
    {
      "epoch": 22.412344358719732,
      "grad_norm": 14.127466201782227,
      "learning_rate": 3.132304636773356e-05,
      "loss": 2.0535,
      "step": 293400
    },
    {
      "epoch": 22.419983194561148,
      "grad_norm": 11.043057441711426,
      "learning_rate": 3.131668067119905e-05,
      "loss": 2.0307,
      "step": 293500
    },
    {
      "epoch": 22.427622030402567,
      "grad_norm": 14.345227241516113,
      "learning_rate": 3.131031497466453e-05,
      "loss": 2.0399,
      "step": 293600
    },
    {
      "epoch": 22.435260866243983,
      "grad_norm": 11.615251541137695,
      "learning_rate": 3.1303949278130015e-05,
      "loss": 2.0544,
      "step": 293700
    },
    {
      "epoch": 22.442899702085402,
      "grad_norm": 14.789608001708984,
      "learning_rate": 3.1297583581595505e-05,
      "loss": 1.9824,
      "step": 293800
    },
    {
      "epoch": 22.45053853792682,
      "grad_norm": 13.850207328796387,
      "learning_rate": 3.129121788506099e-05,
      "loss": 2.0604,
      "step": 293900
    },
    {
      "epoch": 22.458177373768237,
      "grad_norm": 10.188603401184082,
      "learning_rate": 3.128485218852647e-05,
      "loss": 2.0987,
      "step": 294000
    },
    {
      "epoch": 22.465816209609656,
      "grad_norm": 15.734508514404297,
      "learning_rate": 3.1278486491991956e-05,
      "loss": 2.1405,
      "step": 294100
    },
    {
      "epoch": 22.47345504545107,
      "grad_norm": 12.15664005279541,
      "learning_rate": 3.127212079545744e-05,
      "loss": 2.0929,
      "step": 294200
    },
    {
      "epoch": 22.48109388129249,
      "grad_norm": 13.654535293579102,
      "learning_rate": 3.126575509892293e-05,
      "loss": 2.0673,
      "step": 294300
    },
    {
      "epoch": 22.48873271713391,
      "grad_norm": 10.813403129577637,
      "learning_rate": 3.125938940238841e-05,
      "loss": 2.1242,
      "step": 294400
    },
    {
      "epoch": 22.496371552975326,
      "grad_norm": 14.831398963928223,
      "learning_rate": 3.12530237058539e-05,
      "loss": 1.908,
      "step": 294500
    },
    {
      "epoch": 22.504010388816745,
      "grad_norm": 14.472614288330078,
      "learning_rate": 3.124665800931938e-05,
      "loss": 2.0201,
      "step": 294600
    },
    {
      "epoch": 22.51164922465816,
      "grad_norm": 11.457274436950684,
      "learning_rate": 3.124029231278487e-05,
      "loss": 2.0301,
      "step": 294700
    },
    {
      "epoch": 22.51928806049958,
      "grad_norm": 13.280221939086914,
      "learning_rate": 3.1233926616250354e-05,
      "loss": 1.9998,
      "step": 294800
    },
    {
      "epoch": 22.526926896341,
      "grad_norm": 13.42233657836914,
      "learning_rate": 3.122756091971584e-05,
      "loss": 2.0187,
      "step": 294900
    },
    {
      "epoch": 22.534565732182415,
      "grad_norm": 14.224837303161621,
      "learning_rate": 3.122119522318132e-05,
      "loss": 2.0737,
      "step": 295000
    },
    {
      "epoch": 22.542204568023834,
      "grad_norm": 12.530924797058105,
      "learning_rate": 3.1214829526646805e-05,
      "loss": 2.0592,
      "step": 295100
    },
    {
      "epoch": 22.54984340386525,
      "grad_norm": 13.8040771484375,
      "learning_rate": 3.1208463830112295e-05,
      "loss": 2.0557,
      "step": 295200
    },
    {
      "epoch": 22.55748223970667,
      "grad_norm": 14.443604469299316,
      "learning_rate": 3.120209813357778e-05,
      "loss": 2.0387,
      "step": 295300
    },
    {
      "epoch": 22.565121075548085,
      "grad_norm": 15.631448745727539,
      "learning_rate": 3.119573243704326e-05,
      "loss": 1.995,
      "step": 295400
    },
    {
      "epoch": 22.572759911389504,
      "grad_norm": 12.508417129516602,
      "learning_rate": 3.1189366740508746e-05,
      "loss": 2.026,
      "step": 295500
    },
    {
      "epoch": 22.580398747230923,
      "grad_norm": 14.749866485595703,
      "learning_rate": 3.118300104397423e-05,
      "loss": 2.0448,
      "step": 295600
    },
    {
      "epoch": 22.58803758307234,
      "grad_norm": 11.007599830627441,
      "learning_rate": 3.117663534743972e-05,
      "loss": 2.0222,
      "step": 295700
    },
    {
      "epoch": 22.595676418913758,
      "grad_norm": 17.331655502319336,
      "learning_rate": 3.11702696509052e-05,
      "loss": 2.0251,
      "step": 295800
    },
    {
      "epoch": 22.603315254755174,
      "grad_norm": 13.751110076904297,
      "learning_rate": 3.116390395437069e-05,
      "loss": 2.0032,
      "step": 295900
    },
    {
      "epoch": 22.610954090596593,
      "grad_norm": 14.379318237304688,
      "learning_rate": 3.115753825783617e-05,
      "loss": 2.1232,
      "step": 296000
    },
    {
      "epoch": 22.618592926438012,
      "grad_norm": 16.351337432861328,
      "learning_rate": 3.115117256130166e-05,
      "loss": 2.0912,
      "step": 296100
    },
    {
      "epoch": 22.626231762279428,
      "grad_norm": 13.465080261230469,
      "learning_rate": 3.1144806864767144e-05,
      "loss": 2.0474,
      "step": 296200
    },
    {
      "epoch": 22.633870598120847,
      "grad_norm": 12.239498138427734,
      "learning_rate": 3.113844116823263e-05,
      "loss": 1.9969,
      "step": 296300
    },
    {
      "epoch": 22.641509433962263,
      "grad_norm": 11.535027503967285,
      "learning_rate": 3.113207547169811e-05,
      "loss": 2.0392,
      "step": 296400
    },
    {
      "epoch": 22.649148269803682,
      "grad_norm": 14.28195858001709,
      "learning_rate": 3.1125709775163595e-05,
      "loss": 2.0498,
      "step": 296500
    },
    {
      "epoch": 22.6567871056451,
      "grad_norm": 13.100492477416992,
      "learning_rate": 3.1119344078629085e-05,
      "loss": 2.1423,
      "step": 296600
    },
    {
      "epoch": 22.664425941486517,
      "grad_norm": 11.14620590209961,
      "learning_rate": 3.111297838209457e-05,
      "loss": 1.9368,
      "step": 296700
    },
    {
      "epoch": 22.672064777327936,
      "grad_norm": 18.528892517089844,
      "learning_rate": 3.110661268556005e-05,
      "loss": 2.067,
      "step": 296800
    },
    {
      "epoch": 22.67970361316935,
      "grad_norm": 10.842865943908691,
      "learning_rate": 3.1100246989025536e-05,
      "loss": 2.1161,
      "step": 296900
    },
    {
      "epoch": 22.68734244901077,
      "grad_norm": 17.31770896911621,
      "learning_rate": 3.1093881292491026e-05,
      "loss": 2.0165,
      "step": 297000
    },
    {
      "epoch": 22.69498128485219,
      "grad_norm": 13.3505277633667,
      "learning_rate": 3.108751559595651e-05,
      "loss": 1.9423,
      "step": 297100
    },
    {
      "epoch": 22.702620120693606,
      "grad_norm": 11.6701021194458,
      "learning_rate": 3.108114989942199e-05,
      "loss": 2.1406,
      "step": 297200
    },
    {
      "epoch": 22.710258956535025,
      "grad_norm": 12.79488468170166,
      "learning_rate": 3.107478420288748e-05,
      "loss": 2.0448,
      "step": 297300
    },
    {
      "epoch": 22.71789779237644,
      "grad_norm": 21.150135040283203,
      "learning_rate": 3.106841850635297e-05,
      "loss": 2.0783,
      "step": 297400
    },
    {
      "epoch": 22.72553662821786,
      "grad_norm": 13.91871166229248,
      "learning_rate": 3.106205280981846e-05,
      "loss": 1.9865,
      "step": 297500
    },
    {
      "epoch": 22.73317546405928,
      "grad_norm": 16.0230655670166,
      "learning_rate": 3.105568711328394e-05,
      "loss": 2.0594,
      "step": 297600
    },
    {
      "epoch": 22.740814299900695,
      "grad_norm": 10.706029891967773,
      "learning_rate": 3.1049321416749424e-05,
      "loss": 2.0265,
      "step": 297700
    },
    {
      "epoch": 22.748453135742114,
      "grad_norm": 15.152961730957031,
      "learning_rate": 3.104295572021491e-05,
      "loss": 2.0143,
      "step": 297800
    },
    {
      "epoch": 22.75609197158353,
      "grad_norm": 15.196144104003906,
      "learning_rate": 3.10365900236804e-05,
      "loss": 1.9855,
      "step": 297900
    },
    {
      "epoch": 22.76373080742495,
      "grad_norm": 15.158842086791992,
      "learning_rate": 3.103022432714588e-05,
      "loss": 2.0688,
      "step": 298000
    },
    {
      "epoch": 22.771369643266365,
      "grad_norm": 13.033696174621582,
      "learning_rate": 3.1023858630611365e-05,
      "loss": 1.9816,
      "step": 298100
    },
    {
      "epoch": 22.779008479107784,
      "grad_norm": 12.463202476501465,
      "learning_rate": 3.101749293407685e-05,
      "loss": 2.0518,
      "step": 298200
    },
    {
      "epoch": 22.786647314949203,
      "grad_norm": 14.698529243469238,
      "learning_rate": 3.101112723754233e-05,
      "loss": 2.0323,
      "step": 298300
    },
    {
      "epoch": 22.79428615079062,
      "grad_norm": 10.785595893859863,
      "learning_rate": 3.100476154100782e-05,
      "loss": 1.9873,
      "step": 298400
    },
    {
      "epoch": 22.801924986632038,
      "grad_norm": 12.033815383911133,
      "learning_rate": 3.0998395844473306e-05,
      "loss": 2.0106,
      "step": 298500
    },
    {
      "epoch": 22.809563822473454,
      "grad_norm": 13.472123146057129,
      "learning_rate": 3.099203014793879e-05,
      "loss": 2.0105,
      "step": 298600
    },
    {
      "epoch": 22.817202658314873,
      "grad_norm": 14.29093074798584,
      "learning_rate": 3.098566445140427e-05,
      "loss": 1.9755,
      "step": 298700
    },
    {
      "epoch": 22.824841494156292,
      "grad_norm": 10.908890724182129,
      "learning_rate": 3.097929875486976e-05,
      "loss": 2.0723,
      "step": 298800
    },
    {
      "epoch": 22.832480329997708,
      "grad_norm": 11.61544418334961,
      "learning_rate": 3.097293305833525e-05,
      "loss": 2.0659,
      "step": 298900
    },
    {
      "epoch": 22.840119165839127,
      "grad_norm": 9.82474136352539,
      "learning_rate": 3.096656736180073e-05,
      "loss": 2.0285,
      "step": 299000
    },
    {
      "epoch": 22.847758001680543,
      "grad_norm": 9.321744918823242,
      "learning_rate": 3.0960201665266214e-05,
      "loss": 2.0502,
      "step": 299100
    },
    {
      "epoch": 22.855396837521962,
      "grad_norm": 13.567490577697754,
      "learning_rate": 3.09538359687317e-05,
      "loss": 2.0069,
      "step": 299200
    },
    {
      "epoch": 22.86303567336338,
      "grad_norm": 13.17525577545166,
      "learning_rate": 3.094747027219719e-05,
      "loss": 2.0344,
      "step": 299300
    },
    {
      "epoch": 22.870674509204797,
      "grad_norm": 16.11965560913086,
      "learning_rate": 3.094110457566267e-05,
      "loss": 2.0358,
      "step": 299400
    },
    {
      "epoch": 22.878313345046216,
      "grad_norm": 13.668399810791016,
      "learning_rate": 3.0934738879128155e-05,
      "loss": 2.0656,
      "step": 299500
    },
    {
      "epoch": 22.88595218088763,
      "grad_norm": 10.99407958984375,
      "learning_rate": 3.092837318259364e-05,
      "loss": 2.0191,
      "step": 299600
    },
    {
      "epoch": 22.89359101672905,
      "grad_norm": 11.882545471191406,
      "learning_rate": 3.092200748605912e-05,
      "loss": 2.0102,
      "step": 299700
    },
    {
      "epoch": 22.901229852570467,
      "grad_norm": 12.404572486877441,
      "learning_rate": 3.091564178952461e-05,
      "loss": 2.0664,
      "step": 299800
    },
    {
      "epoch": 22.908868688411886,
      "grad_norm": 14.08880615234375,
      "learning_rate": 3.0909276092990096e-05,
      "loss": 1.9243,
      "step": 299900
    },
    {
      "epoch": 22.916507524253305,
      "grad_norm": 12.871520042419434,
      "learning_rate": 3.090291039645558e-05,
      "loss": 2.1123,
      "step": 300000
    },
    {
      "epoch": 22.92414636009472,
      "grad_norm": 11.383977890014648,
      "learning_rate": 3.089654469992106e-05,
      "loss": 2.0739,
      "step": 300100
    },
    {
      "epoch": 22.93178519593614,
      "grad_norm": 14.709548950195312,
      "learning_rate": 3.089017900338655e-05,
      "loss": 2.0673,
      "step": 300200
    },
    {
      "epoch": 22.939424031777556,
      "grad_norm": 11.641801834106445,
      "learning_rate": 3.088381330685204e-05,
      "loss": 2.1373,
      "step": 300300
    },
    {
      "epoch": 22.947062867618975,
      "grad_norm": 11.295683860778809,
      "learning_rate": 3.087744761031752e-05,
      "loss": 1.9782,
      "step": 300400
    },
    {
      "epoch": 22.954701703460394,
      "grad_norm": 16.14689826965332,
      "learning_rate": 3.0871081913783004e-05,
      "loss": 2.0658,
      "step": 300500
    },
    {
      "epoch": 22.96234053930181,
      "grad_norm": 12.01555347442627,
      "learning_rate": 3.086471621724849e-05,
      "loss": 2.1257,
      "step": 300600
    },
    {
      "epoch": 22.96997937514323,
      "grad_norm": 15.95701789855957,
      "learning_rate": 3.085835052071398e-05,
      "loss": 2.1289,
      "step": 300700
    },
    {
      "epoch": 22.977618210984645,
      "grad_norm": 13.021402359008789,
      "learning_rate": 3.085198482417946e-05,
      "loss": 2.0448,
      "step": 300800
    },
    {
      "epoch": 22.985257046826064,
      "grad_norm": 11.643715858459473,
      "learning_rate": 3.0845619127644945e-05,
      "loss": 2.1466,
      "step": 300900
    },
    {
      "epoch": 22.992895882667483,
      "grad_norm": 13.694936752319336,
      "learning_rate": 3.0839253431110435e-05,
      "loss": 2.0633,
      "step": 301000
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.9917105436325073,
      "eval_runtime": 1.6545,
      "eval_samples_per_second": 417.036,
      "eval_steps_per_second": 417.036,
      "step": 301093
    },
    {
      "epoch": 23.0,
      "eval_loss": 1.7917121648788452,
      "eval_runtime": 31.6055,
      "eval_samples_per_second": 414.2,
      "eval_steps_per_second": 414.2,
      "step": 301093
    },
    {
      "epoch": 23.0005347185089,
      "grad_norm": 11.89106273651123,
      "learning_rate": 3.083288773457592e-05,
      "loss": 2.0284,
      "step": 301100
    },
    {
      "epoch": 23.008173554350318,
      "grad_norm": 11.234658241271973,
      "learning_rate": 3.08265220380414e-05,
      "loss": 1.9735,
      "step": 301200
    },
    {
      "epoch": 23.015812390191734,
      "grad_norm": 12.674633026123047,
      "learning_rate": 3.082015634150689e-05,
      "loss": 2.0684,
      "step": 301300
    },
    {
      "epoch": 23.023451226033153,
      "grad_norm": 11.600417137145996,
      "learning_rate": 3.0813790644972376e-05,
      "loss": 1.9644,
      "step": 301400
    },
    {
      "epoch": 23.031090061874572,
      "grad_norm": 16.635066986083984,
      "learning_rate": 3.080742494843786e-05,
      "loss": 1.9815,
      "step": 301500
    },
    {
      "epoch": 23.038728897715988,
      "grad_norm": 14.705778121948242,
      "learning_rate": 3.080105925190335e-05,
      "loss": 2.0409,
      "step": 301600
    },
    {
      "epoch": 23.046367733557407,
      "grad_norm": 13.321218490600586,
      "learning_rate": 3.0794693555368833e-05,
      "loss": 2.0987,
      "step": 301700
    },
    {
      "epoch": 23.054006569398823,
      "grad_norm": 11.36139965057373,
      "learning_rate": 3.078832785883432e-05,
      "loss": 1.9628,
      "step": 301800
    },
    {
      "epoch": 23.06164540524024,
      "grad_norm": 13.163331031799316,
      "learning_rate": 3.07819621622998e-05,
      "loss": 1.9707,
      "step": 301900
    },
    {
      "epoch": 23.069284241081657,
      "grad_norm": 10.513618469238281,
      "learning_rate": 3.0775596465765284e-05,
      "loss": 2.0375,
      "step": 302000
    },
    {
      "epoch": 23.076923076923077,
      "grad_norm": 12.326860427856445,
      "learning_rate": 3.0769230769230774e-05,
      "loss": 2.0318,
      "step": 302100
    },
    {
      "epoch": 23.084561912764496,
      "grad_norm": 10.38514232635498,
      "learning_rate": 3.076286507269626e-05,
      "loss": 1.8986,
      "step": 302200
    },
    {
      "epoch": 23.09220074860591,
      "grad_norm": 11.334403991699219,
      "learning_rate": 3.075649937616174e-05,
      "loss": 2.0758,
      "step": 302300
    },
    {
      "epoch": 23.09983958444733,
      "grad_norm": 21.63746452331543,
      "learning_rate": 3.0750133679627225e-05,
      "loss": 2.062,
      "step": 302400
    },
    {
      "epoch": 23.107478420288746,
      "grad_norm": 10.562881469726562,
      "learning_rate": 3.0743767983092715e-05,
      "loss": 1.992,
      "step": 302500
    },
    {
      "epoch": 23.115117256130166,
      "grad_norm": 13.983562469482422,
      "learning_rate": 3.07374022865582e-05,
      "loss": 2.1508,
      "step": 302600
    },
    {
      "epoch": 23.122756091971585,
      "grad_norm": 10.659744262695312,
      "learning_rate": 3.073103659002368e-05,
      "loss": 2.0196,
      "step": 302700
    },
    {
      "epoch": 23.130394927813,
      "grad_norm": 9.970416069030762,
      "learning_rate": 3.0724670893489166e-05,
      "loss": 2.0612,
      "step": 302800
    },
    {
      "epoch": 23.13803376365442,
      "grad_norm": 13.423052787780762,
      "learning_rate": 3.071830519695465e-05,
      "loss": 2.0534,
      "step": 302900
    },
    {
      "epoch": 23.145672599495835,
      "grad_norm": 12.663599967956543,
      "learning_rate": 3.071193950042014e-05,
      "loss": 1.89,
      "step": 303000
    },
    {
      "epoch": 23.153311435337255,
      "grad_norm": 14.9697847366333,
      "learning_rate": 3.070557380388562e-05,
      "loss": 2.0333,
      "step": 303100
    },
    {
      "epoch": 23.160950271178674,
      "grad_norm": 13.324655532836914,
      "learning_rate": 3.069920810735111e-05,
      "loss": 2.0461,
      "step": 303200
    },
    {
      "epoch": 23.16858910702009,
      "grad_norm": 15.573861122131348,
      "learning_rate": 3.069284241081659e-05,
      "loss": 1.9855,
      "step": 303300
    },
    {
      "epoch": 23.17622794286151,
      "grad_norm": 12.64005184173584,
      "learning_rate": 3.068647671428208e-05,
      "loss": 2.0301,
      "step": 303400
    },
    {
      "epoch": 23.183866778702924,
      "grad_norm": 17.263912200927734,
      "learning_rate": 3.0680111017747564e-05,
      "loss": 2.0714,
      "step": 303500
    },
    {
      "epoch": 23.191505614544344,
      "grad_norm": 12.965731620788574,
      "learning_rate": 3.067374532121305e-05,
      "loss": 1.9416,
      "step": 303600
    },
    {
      "epoch": 23.199144450385763,
      "grad_norm": 12.884519577026367,
      "learning_rate": 3.066737962467853e-05,
      "loss": 1.9936,
      "step": 303700
    },
    {
      "epoch": 23.20678328622718,
      "grad_norm": 13.730158805847168,
      "learning_rate": 3.0661013928144015e-05,
      "loss": 1.9982,
      "step": 303800
    },
    {
      "epoch": 23.214422122068598,
      "grad_norm": 12.236348152160645,
      "learning_rate": 3.0654648231609505e-05,
      "loss": 2.0219,
      "step": 303900
    },
    {
      "epoch": 23.222060957910013,
      "grad_norm": 15.221883773803711,
      "learning_rate": 3.064828253507499e-05,
      "loss": 2.0181,
      "step": 304000
    },
    {
      "epoch": 23.229699793751433,
      "grad_norm": 11.087963104248047,
      "learning_rate": 3.064191683854047e-05,
      "loss": 1.9675,
      "step": 304100
    },
    {
      "epoch": 23.23733862959285,
      "grad_norm": 17.45682716369629,
      "learning_rate": 3.0635551142005956e-05,
      "loss": 1.9147,
      "step": 304200
    },
    {
      "epoch": 23.244977465434268,
      "grad_norm": 11.28791332244873,
      "learning_rate": 3.062918544547144e-05,
      "loss": 2.1132,
      "step": 304300
    },
    {
      "epoch": 23.252616301275687,
      "grad_norm": 13.112788200378418,
      "learning_rate": 3.062281974893693e-05,
      "loss": 2.0014,
      "step": 304400
    },
    {
      "epoch": 23.260255137117102,
      "grad_norm": 13.388094902038574,
      "learning_rate": 3.061645405240241e-05,
      "loss": 2.0481,
      "step": 304500
    },
    {
      "epoch": 23.26789397295852,
      "grad_norm": 15.681493759155273,
      "learning_rate": 3.06100883558679e-05,
      "loss": 2.074,
      "step": 304600
    },
    {
      "epoch": 23.275532808799937,
      "grad_norm": 12.740388870239258,
      "learning_rate": 3.060372265933339e-05,
      "loss": 1.9943,
      "step": 304700
    },
    {
      "epoch": 23.283171644641357,
      "grad_norm": 8.460578918457031,
      "learning_rate": 3.059735696279887e-05,
      "loss": 1.9759,
      "step": 304800
    },
    {
      "epoch": 23.290810480482776,
      "grad_norm": 18.508560180664062,
      "learning_rate": 3.0590991266264354e-05,
      "loss": 1.9168,
      "step": 304900
    },
    {
      "epoch": 23.29844931632419,
      "grad_norm": 11.829797744750977,
      "learning_rate": 3.0584625569729844e-05,
      "loss": 2.0303,
      "step": 305000
    },
    {
      "epoch": 23.30608815216561,
      "grad_norm": 13.37525749206543,
      "learning_rate": 3.057825987319533e-05,
      "loss": 1.9672,
      "step": 305100
    },
    {
      "epoch": 23.313726988007026,
      "grad_norm": 12.134613037109375,
      "learning_rate": 3.057189417666081e-05,
      "loss": 2.0289,
      "step": 305200
    },
    {
      "epoch": 23.321365823848446,
      "grad_norm": 11.113391876220703,
      "learning_rate": 3.05655284801263e-05,
      "loss": 2.0074,
      "step": 305300
    },
    {
      "epoch": 23.329004659689865,
      "grad_norm": 11.72319507598877,
      "learning_rate": 3.0559162783591785e-05,
      "loss": 2.0728,
      "step": 305400
    },
    {
      "epoch": 23.33664349553128,
      "grad_norm": 12.986213684082031,
      "learning_rate": 3.055279708705727e-05,
      "loss": 2.0372,
      "step": 305500
    },
    {
      "epoch": 23.3442823313727,
      "grad_norm": 16.09933090209961,
      "learning_rate": 3.054643139052275e-05,
      "loss": 1.9705,
      "step": 305600
    },
    {
      "epoch": 23.351921167214115,
      "grad_norm": 16.427141189575195,
      "learning_rate": 3.054006569398824e-05,
      "loss": 2.0497,
      "step": 305700
    },
    {
      "epoch": 23.359560003055535,
      "grad_norm": 13.31539249420166,
      "learning_rate": 3.0533699997453726e-05,
      "loss": 1.9421,
      "step": 305800
    },
    {
      "epoch": 23.367198838896954,
      "grad_norm": 14.140393257141113,
      "learning_rate": 3.052733430091921e-05,
      "loss": 2.0504,
      "step": 305900
    },
    {
      "epoch": 23.37483767473837,
      "grad_norm": 14.35165786743164,
      "learning_rate": 3.052096860438469e-05,
      "loss": 1.981,
      "step": 306000
    },
    {
      "epoch": 23.38247651057979,
      "grad_norm": 14.17176342010498,
      "learning_rate": 3.0514602907850177e-05,
      "loss": 2.1267,
      "step": 306100
    },
    {
      "epoch": 23.390115346421204,
      "grad_norm": 10.992486000061035,
      "learning_rate": 3.0508237211315667e-05,
      "loss": 1.9686,
      "step": 306200
    },
    {
      "epoch": 23.397754182262624,
      "grad_norm": 20.978986740112305,
      "learning_rate": 3.050187151478115e-05,
      "loss": 2.0535,
      "step": 306300
    },
    {
      "epoch": 23.40539301810404,
      "grad_norm": 14.09765911102295,
      "learning_rate": 3.0495505818246634e-05,
      "loss": 2.0527,
      "step": 306400
    },
    {
      "epoch": 23.41303185394546,
      "grad_norm": 12.564366340637207,
      "learning_rate": 3.0489140121712118e-05,
      "loss": 2.0553,
      "step": 306500
    },
    {
      "epoch": 23.420670689786878,
      "grad_norm": 15.412032127380371,
      "learning_rate": 3.0482774425177608e-05,
      "loss": 2.1192,
      "step": 306600
    },
    {
      "epoch": 23.428309525628293,
      "grad_norm": 15.182394981384277,
      "learning_rate": 3.047640872864309e-05,
      "loss": 2.1144,
      "step": 306700
    },
    {
      "epoch": 23.435948361469713,
      "grad_norm": 14.951699256896973,
      "learning_rate": 3.0470043032108575e-05,
      "loss": 2.058,
      "step": 306800
    },
    {
      "epoch": 23.44358719731113,
      "grad_norm": 18.735191345214844,
      "learning_rate": 3.046367733557406e-05,
      "loss": 1.9293,
      "step": 306900
    },
    {
      "epoch": 23.451226033152548,
      "grad_norm": 10.677852630615234,
      "learning_rate": 3.0457311639039542e-05,
      "loss": 1.9662,
      "step": 307000
    },
    {
      "epoch": 23.458864868993967,
      "grad_norm": 11.590147972106934,
      "learning_rate": 3.0450945942505033e-05,
      "loss": 1.9363,
      "step": 307100
    },
    {
      "epoch": 23.466503704835382,
      "grad_norm": 16.70212745666504,
      "learning_rate": 3.0444580245970516e-05,
      "loss": 1.9887,
      "step": 307200
    },
    {
      "epoch": 23.4741425406768,
      "grad_norm": 16.68768882751465,
      "learning_rate": 3.0438214549436e-05,
      "loss": 1.9766,
      "step": 307300
    },
    {
      "epoch": 23.481781376518217,
      "grad_norm": 15.155961036682129,
      "learning_rate": 3.0431848852901483e-05,
      "loss": 2.1644,
      "step": 307400
    },
    {
      "epoch": 23.489420212359637,
      "grad_norm": 14.402576446533203,
      "learning_rate": 3.042548315636697e-05,
      "loss": 2.0734,
      "step": 307500
    },
    {
      "epoch": 23.497059048201056,
      "grad_norm": 18.22794532775879,
      "learning_rate": 3.0419117459832457e-05,
      "loss": 2.1046,
      "step": 307600
    },
    {
      "epoch": 23.50469788404247,
      "grad_norm": 18.49169158935547,
      "learning_rate": 3.041275176329794e-05,
      "loss": 2.0223,
      "step": 307700
    },
    {
      "epoch": 23.51233671988389,
      "grad_norm": 12.315788269042969,
      "learning_rate": 3.0406386066763427e-05,
      "loss": 1.9298,
      "step": 307800
    },
    {
      "epoch": 23.519975555725306,
      "grad_norm": 13.639424324035645,
      "learning_rate": 3.040002037022891e-05,
      "loss": 1.9956,
      "step": 307900
    },
    {
      "epoch": 23.527614391566726,
      "grad_norm": 11.678757667541504,
      "learning_rate": 3.0393654673694398e-05,
      "loss": 2.036,
      "step": 308000
    },
    {
      "epoch": 23.53525322740814,
      "grad_norm": 14.561613082885742,
      "learning_rate": 3.0387288977159885e-05,
      "loss": 2.138,
      "step": 308100
    },
    {
      "epoch": 23.54289206324956,
      "grad_norm": 14.758959770202637,
      "learning_rate": 3.038092328062537e-05,
      "loss": 1.9607,
      "step": 308200
    },
    {
      "epoch": 23.55053089909098,
      "grad_norm": 10.090564727783203,
      "learning_rate": 3.0374557584090852e-05,
      "loss": 2.0258,
      "step": 308300
    },
    {
      "epoch": 23.558169734932395,
      "grad_norm": 14.981793403625488,
      "learning_rate": 3.0368191887556335e-05,
      "loss": 2.0679,
      "step": 308400
    },
    {
      "epoch": 23.565808570773815,
      "grad_norm": 12.933143615722656,
      "learning_rate": 3.0361826191021826e-05,
      "loss": 2.0225,
      "step": 308500
    },
    {
      "epoch": 23.57344740661523,
      "grad_norm": 12.77597427368164,
      "learning_rate": 3.035546049448731e-05,
      "loss": 2.0308,
      "step": 308600
    },
    {
      "epoch": 23.58108624245665,
      "grad_norm": 12.87093448638916,
      "learning_rate": 3.0349094797952793e-05,
      "loss": 2.0653,
      "step": 308700
    },
    {
      "epoch": 23.58872507829807,
      "grad_norm": 12.588467597961426,
      "learning_rate": 3.0342729101418276e-05,
      "loss": 2.0864,
      "step": 308800
    },
    {
      "epoch": 23.596363914139484,
      "grad_norm": 14.18863582611084,
      "learning_rate": 3.0336363404883767e-05,
      "loss": 1.9941,
      "step": 308900
    },
    {
      "epoch": 23.604002749980904,
      "grad_norm": 13.088605880737305,
      "learning_rate": 3.032999770834925e-05,
      "loss": 2.0862,
      "step": 309000
    },
    {
      "epoch": 23.61164158582232,
      "grad_norm": 11.976263046264648,
      "learning_rate": 3.0323632011814734e-05,
      "loss": 2.0418,
      "step": 309100
    },
    {
      "epoch": 23.61928042166374,
      "grad_norm": 13.146882057189941,
      "learning_rate": 3.0317266315280217e-05,
      "loss": 2.0292,
      "step": 309200
    },
    {
      "epoch": 23.626919257505158,
      "grad_norm": 14.889880180358887,
      "learning_rate": 3.03109006187457e-05,
      "loss": 2.0863,
      "step": 309300
    },
    {
      "epoch": 23.634558093346573,
      "grad_norm": 14.924824714660645,
      "learning_rate": 3.030453492221119e-05,
      "loss": 2.0173,
      "step": 309400
    },
    {
      "epoch": 23.642196929187993,
      "grad_norm": 13.132645606994629,
      "learning_rate": 3.0298169225676675e-05,
      "loss": 2.0347,
      "step": 309500
    },
    {
      "epoch": 23.649835765029408,
      "grad_norm": 14.224660873413086,
      "learning_rate": 3.0291803529142158e-05,
      "loss": 2.0047,
      "step": 309600
    },
    {
      "epoch": 23.657474600870827,
      "grad_norm": 13.975934982299805,
      "learning_rate": 3.0285437832607645e-05,
      "loss": 2.0221,
      "step": 309700
    },
    {
      "epoch": 23.665113436712247,
      "grad_norm": 12.14752197265625,
      "learning_rate": 3.0279072136073132e-05,
      "loss": 2.0542,
      "step": 309800
    },
    {
      "epoch": 23.672752272553662,
      "grad_norm": 15.372121810913086,
      "learning_rate": 3.027270643953862e-05,
      "loss": 2.0891,
      "step": 309900
    },
    {
      "epoch": 23.68039110839508,
      "grad_norm": 11.72783088684082,
      "learning_rate": 3.0266340743004103e-05,
      "loss": 1.9599,
      "step": 310000
    },
    {
      "epoch": 23.688029944236497,
      "grad_norm": 12.810064315795898,
      "learning_rate": 3.0259975046469586e-05,
      "loss": 1.998,
      "step": 310100
    },
    {
      "epoch": 23.695668780077916,
      "grad_norm": 13.237126350402832,
      "learning_rate": 3.025360934993507e-05,
      "loss": 1.9685,
      "step": 310200
    },
    {
      "epoch": 23.703307615919336,
      "grad_norm": 15.983824729919434,
      "learning_rate": 3.024724365340056e-05,
      "loss": 2.0001,
      "step": 310300
    },
    {
      "epoch": 23.71094645176075,
      "grad_norm": 14.002366065979004,
      "learning_rate": 3.0240877956866043e-05,
      "loss": 2.1192,
      "step": 310400
    },
    {
      "epoch": 23.71858528760217,
      "grad_norm": 15.311044692993164,
      "learning_rate": 3.0234512260331527e-05,
      "loss": 2.0465,
      "step": 310500
    },
    {
      "epoch": 23.726224123443586,
      "grad_norm": 15.812417984008789,
      "learning_rate": 3.022814656379701e-05,
      "loss": 2.0145,
      "step": 310600
    },
    {
      "epoch": 23.733862959285005,
      "grad_norm": 13.700460433959961,
      "learning_rate": 3.0221780867262494e-05,
      "loss": 2.0607,
      "step": 310700
    },
    {
      "epoch": 23.74150179512642,
      "grad_norm": 14.107748985290527,
      "learning_rate": 3.0215415170727984e-05,
      "loss": 2.1197,
      "step": 310800
    },
    {
      "epoch": 23.74914063096784,
      "grad_norm": 13.8817720413208,
      "learning_rate": 3.0209049474193468e-05,
      "loss": 2.0724,
      "step": 310900
    },
    {
      "epoch": 23.75677946680926,
      "grad_norm": 10.486729621887207,
      "learning_rate": 3.020268377765895e-05,
      "loss": 1.9595,
      "step": 311000
    },
    {
      "epoch": 23.764418302650675,
      "grad_norm": 14.361981391906738,
      "learning_rate": 3.0196318081124435e-05,
      "loss": 2.0243,
      "step": 311100
    },
    {
      "epoch": 23.772057138492094,
      "grad_norm": 13.062451362609863,
      "learning_rate": 3.0189952384589925e-05,
      "loss": 2.005,
      "step": 311200
    },
    {
      "epoch": 23.77969597433351,
      "grad_norm": 13.106522560119629,
      "learning_rate": 3.018358668805541e-05,
      "loss": 2.0354,
      "step": 311300
    },
    {
      "epoch": 23.78733481017493,
      "grad_norm": 11.550097465515137,
      "learning_rate": 3.0177220991520892e-05,
      "loss": 1.9835,
      "step": 311400
    },
    {
      "epoch": 23.79497364601635,
      "grad_norm": 11.280640602111816,
      "learning_rate": 3.017085529498638e-05,
      "loss": 1.9347,
      "step": 311500
    },
    {
      "epoch": 23.802612481857764,
      "grad_norm": 14.22769546508789,
      "learning_rate": 3.0164489598451863e-05,
      "loss": 2.0966,
      "step": 311600
    },
    {
      "epoch": 23.810251317699183,
      "grad_norm": 11.660478591918945,
      "learning_rate": 3.015812390191735e-05,
      "loss": 1.9531,
      "step": 311700
    },
    {
      "epoch": 23.8178901535406,
      "grad_norm": 14.26367473602295,
      "learning_rate": 3.0151758205382837e-05,
      "loss": 1.9783,
      "step": 311800
    },
    {
      "epoch": 23.82552898938202,
      "grad_norm": 12.025114059448242,
      "learning_rate": 3.014539250884832e-05,
      "loss": 2.0012,
      "step": 311900
    },
    {
      "epoch": 23.833167825223438,
      "grad_norm": 20.391672134399414,
      "learning_rate": 3.0139026812313804e-05,
      "loss": 2.1299,
      "step": 312000
    },
    {
      "epoch": 23.840806661064853,
      "grad_norm": 14.953594207763672,
      "learning_rate": 3.0132661115779294e-05,
      "loss": 2.0495,
      "step": 312100
    },
    {
      "epoch": 23.848445496906272,
      "grad_norm": 13.9273681640625,
      "learning_rate": 3.0126295419244778e-05,
      "loss": 2.0068,
      "step": 312200
    },
    {
      "epoch": 23.856084332747688,
      "grad_norm": 11.567481994628906,
      "learning_rate": 3.011992972271026e-05,
      "loss": 1.9525,
      "step": 312300
    },
    {
      "epoch": 23.863723168589107,
      "grad_norm": 13.318582534790039,
      "learning_rate": 3.0113564026175745e-05,
      "loss": 1.9672,
      "step": 312400
    },
    {
      "epoch": 23.871362004430523,
      "grad_norm": 16.251401901245117,
      "learning_rate": 3.0107198329641228e-05,
      "loss": 1.9296,
      "step": 312500
    },
    {
      "epoch": 23.879000840271942,
      "grad_norm": 8.91064453125,
      "learning_rate": 3.010083263310672e-05,
      "loss": 2.0495,
      "step": 312600
    },
    {
      "epoch": 23.88663967611336,
      "grad_norm": 13.472156524658203,
      "learning_rate": 3.0094466936572202e-05,
      "loss": 2.0253,
      "step": 312700
    },
    {
      "epoch": 23.894278511954777,
      "grad_norm": 14.173140525817871,
      "learning_rate": 3.0088101240037686e-05,
      "loss": 1.9578,
      "step": 312800
    },
    {
      "epoch": 23.901917347796196,
      "grad_norm": 8.940546989440918,
      "learning_rate": 3.008173554350317e-05,
      "loss": 2.0365,
      "step": 312900
    },
    {
      "epoch": 23.909556183637612,
      "grad_norm": 12.839938163757324,
      "learning_rate": 3.0075369846968653e-05,
      "loss": 1.9884,
      "step": 313000
    },
    {
      "epoch": 23.91719501947903,
      "grad_norm": 11.668736457824707,
      "learning_rate": 3.0069004150434143e-05,
      "loss": 1.9456,
      "step": 313100
    },
    {
      "epoch": 23.92483385532045,
      "grad_norm": 9.818814277648926,
      "learning_rate": 3.0062638453899626e-05,
      "loss": 2.0761,
      "step": 313200
    },
    {
      "epoch": 23.932472691161866,
      "grad_norm": 15.873788833618164,
      "learning_rate": 3.005627275736511e-05,
      "loss": 2.0178,
      "step": 313300
    },
    {
      "epoch": 23.940111527003285,
      "grad_norm": 11.248766899108887,
      "learning_rate": 3.0049907060830597e-05,
      "loss": 2.0059,
      "step": 313400
    },
    {
      "epoch": 23.9477503628447,
      "grad_norm": 15.484789848327637,
      "learning_rate": 3.0043541364296084e-05,
      "loss": 1.9851,
      "step": 313500
    },
    {
      "epoch": 23.95538919868612,
      "grad_norm": 15.560505867004395,
      "learning_rate": 3.0037175667761567e-05,
      "loss": 2.0492,
      "step": 313600
    },
    {
      "epoch": 23.96302803452754,
      "grad_norm": 11.674057960510254,
      "learning_rate": 3.0030809971227054e-05,
      "loss": 2.0164,
      "step": 313700
    },
    {
      "epoch": 23.970666870368955,
      "grad_norm": 13.287994384765625,
      "learning_rate": 3.0024444274692538e-05,
      "loss": 2.0925,
      "step": 313800
    },
    {
      "epoch": 23.978305706210374,
      "grad_norm": 18.148645401000977,
      "learning_rate": 3.001807857815802e-05,
      "loss": 2.0384,
      "step": 313900
    },
    {
      "epoch": 23.98594454205179,
      "grad_norm": 12.427074432373047,
      "learning_rate": 3.0011712881623512e-05,
      "loss": 1.9828,
      "step": 314000
    },
    {
      "epoch": 23.99358337789321,
      "grad_norm": 11.852315902709961,
      "learning_rate": 3.0005347185088995e-05,
      "loss": 2.0237,
      "step": 314100
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.9950213432312012,
      "eval_runtime": 1.6478,
      "eval_samples_per_second": 418.735,
      "eval_steps_per_second": 418.735,
      "step": 314184
    },
    {
      "epoch": 24.0,
      "eval_loss": 1.7856438159942627,
      "eval_runtime": 31.5767,
      "eval_samples_per_second": 414.578,
      "eval_steps_per_second": 414.578,
      "step": 314184
    },
    {
      "epoch": 24.00122221373463,
      "grad_norm": 14.045949935913086,
      "learning_rate": 2.999898148855448e-05,
      "loss": 2.0275,
      "step": 314200
    },
    {
      "epoch": 24.008861049576044,
      "grad_norm": 13.473245620727539,
      "learning_rate": 2.9992615792019962e-05,
      "loss": 1.9302,
      "step": 314300
    },
    {
      "epoch": 24.016499885417463,
      "grad_norm": 12.946468353271484,
      "learning_rate": 2.9986250095485453e-05,
      "loss": 2.0063,
      "step": 314400
    },
    {
      "epoch": 24.02413872125888,
      "grad_norm": 11.984902381896973,
      "learning_rate": 2.9979884398950936e-05,
      "loss": 2.0096,
      "step": 314500
    },
    {
      "epoch": 24.0317775571003,
      "grad_norm": 14.287444114685059,
      "learning_rate": 2.997351870241642e-05,
      "loss": 2.0218,
      "step": 314600
    },
    {
      "epoch": 24.039416392941714,
      "grad_norm": 13.605144500732422,
      "learning_rate": 2.9967153005881903e-05,
      "loss": 1.9999,
      "step": 314700
    },
    {
      "epoch": 24.047055228783133,
      "grad_norm": 12.314289093017578,
      "learning_rate": 2.9960787309347387e-05,
      "loss": 1.9656,
      "step": 314800
    },
    {
      "epoch": 24.054694064624552,
      "grad_norm": 13.002714157104492,
      "learning_rate": 2.9954421612812877e-05,
      "loss": 1.9649,
      "step": 314900
    },
    {
      "epoch": 24.062332900465968,
      "grad_norm": 11.580607414245605,
      "learning_rate": 2.994805591627836e-05,
      "loss": 1.8867,
      "step": 315000
    },
    {
      "epoch": 24.069971736307387,
      "grad_norm": 13.325885772705078,
      "learning_rate": 2.9941690219743844e-05,
      "loss": 2.1053,
      "step": 315100
    },
    {
      "epoch": 24.077610572148803,
      "grad_norm": 10.47291088104248,
      "learning_rate": 2.9935324523209328e-05,
      "loss": 1.9843,
      "step": 315200
    },
    {
      "epoch": 24.085249407990222,
      "grad_norm": 11.801681518554688,
      "learning_rate": 2.9928958826674818e-05,
      "loss": 1.8967,
      "step": 315300
    },
    {
      "epoch": 24.09288824383164,
      "grad_norm": 13.046098709106445,
      "learning_rate": 2.99225931301403e-05,
      "loss": 1.9991,
      "step": 315400
    },
    {
      "epoch": 24.100527079673057,
      "grad_norm": 12.927186012268066,
      "learning_rate": 2.991622743360579e-05,
      "loss": 2.0475,
      "step": 315500
    },
    {
      "epoch": 24.108165915514476,
      "grad_norm": 12.33355712890625,
      "learning_rate": 2.9909861737071272e-05,
      "loss": 1.9054,
      "step": 315600
    },
    {
      "epoch": 24.115804751355892,
      "grad_norm": 14.128328323364258,
      "learning_rate": 2.9903496040536756e-05,
      "loss": 1.9621,
      "step": 315700
    },
    {
      "epoch": 24.12344358719731,
      "grad_norm": 13.653563499450684,
      "learning_rate": 2.9897130344002246e-05,
      "loss": 1.9751,
      "step": 315800
    },
    {
      "epoch": 24.13108242303873,
      "grad_norm": 17.782840728759766,
      "learning_rate": 2.989076464746773e-05,
      "loss": 2.13,
      "step": 315900
    },
    {
      "epoch": 24.138721258880146,
      "grad_norm": 10.899347305297852,
      "learning_rate": 2.9884398950933213e-05,
      "loss": 1.966,
      "step": 316000
    },
    {
      "epoch": 24.146360094721565,
      "grad_norm": 13.987441062927246,
      "learning_rate": 2.9878033254398696e-05,
      "loss": 2.1107,
      "step": 316100
    },
    {
      "epoch": 24.15399893056298,
      "grad_norm": 10.119205474853516,
      "learning_rate": 2.987166755786418e-05,
      "loss": 2.0044,
      "step": 316200
    },
    {
      "epoch": 24.1616377664044,
      "grad_norm": 10.802231788635254,
      "learning_rate": 2.986530186132967e-05,
      "loss": 2.0057,
      "step": 316300
    },
    {
      "epoch": 24.16927660224582,
      "grad_norm": 10.919313430786133,
      "learning_rate": 2.9858936164795154e-05,
      "loss": 2.0906,
      "step": 316400
    },
    {
      "epoch": 24.176915438087235,
      "grad_norm": 13.257119178771973,
      "learning_rate": 2.9852570468260637e-05,
      "loss": 1.9495,
      "step": 316500
    },
    {
      "epoch": 24.184554273928654,
      "grad_norm": 12.147128105163574,
      "learning_rate": 2.984620477172612e-05,
      "loss": 2.0566,
      "step": 316600
    },
    {
      "epoch": 24.19219310977007,
      "grad_norm": 11.996719360351562,
      "learning_rate": 2.983983907519161e-05,
      "loss": 1.9746,
      "step": 316700
    },
    {
      "epoch": 24.19983194561149,
      "grad_norm": 14.658445358276367,
      "learning_rate": 2.9833473378657095e-05,
      "loss": 2.0312,
      "step": 316800
    },
    {
      "epoch": 24.207470781452905,
      "grad_norm": 14.795059204101562,
      "learning_rate": 2.982710768212258e-05,
      "loss": 1.966,
      "step": 316900
    },
    {
      "epoch": 24.215109617294324,
      "grad_norm": 14.748062133789062,
      "learning_rate": 2.9820741985588062e-05,
      "loss": 2.0401,
      "step": 317000
    },
    {
      "epoch": 24.222748453135743,
      "grad_norm": 11.603472709655762,
      "learning_rate": 2.981437628905355e-05,
      "loss": 1.9333,
      "step": 317100
    },
    {
      "epoch": 24.23038728897716,
      "grad_norm": 14.740470886230469,
      "learning_rate": 2.9808010592519036e-05,
      "loss": 2.0099,
      "step": 317200
    },
    {
      "epoch": 24.238026124818578,
      "grad_norm": 13.530374526977539,
      "learning_rate": 2.980164489598452e-05,
      "loss": 1.9486,
      "step": 317300
    },
    {
      "epoch": 24.245664960659994,
      "grad_norm": 11.563291549682617,
      "learning_rate": 2.9795279199450006e-05,
      "loss": 1.9343,
      "step": 317400
    },
    {
      "epoch": 24.253303796501413,
      "grad_norm": 13.981463432312012,
      "learning_rate": 2.978891350291549e-05,
      "loss": 2.0187,
      "step": 317500
    },
    {
      "epoch": 24.260942632342832,
      "grad_norm": 14.764402389526367,
      "learning_rate": 2.9782547806380977e-05,
      "loss": 2.0691,
      "step": 317600
    },
    {
      "epoch": 24.268581468184248,
      "grad_norm": 12.254852294921875,
      "learning_rate": 2.9776182109846464e-05,
      "loss": 1.89,
      "step": 317700
    },
    {
      "epoch": 24.276220304025667,
      "grad_norm": 19.279741287231445,
      "learning_rate": 2.9769816413311947e-05,
      "loss": 2.0094,
      "step": 317800
    },
    {
      "epoch": 24.283859139867083,
      "grad_norm": 12.669472694396973,
      "learning_rate": 2.976345071677743e-05,
      "loss": 2.0151,
      "step": 317900
    },
    {
      "epoch": 24.291497975708502,
      "grad_norm": 15.55101490020752,
      "learning_rate": 2.9757085020242914e-05,
      "loss": 1.9988,
      "step": 318000
    },
    {
      "epoch": 24.29913681154992,
      "grad_norm": 14.371994972229004,
      "learning_rate": 2.9750719323708404e-05,
      "loss": 2.0234,
      "step": 318100
    },
    {
      "epoch": 24.306775647391337,
      "grad_norm": 14.07239055633545,
      "learning_rate": 2.9744353627173888e-05,
      "loss": 2.0045,
      "step": 318200
    },
    {
      "epoch": 24.314414483232756,
      "grad_norm": 13.34524917602539,
      "learning_rate": 2.973798793063937e-05,
      "loss": 2.003,
      "step": 318300
    },
    {
      "epoch": 24.322053319074172,
      "grad_norm": 12.051833152770996,
      "learning_rate": 2.9731622234104855e-05,
      "loss": 2.105,
      "step": 318400
    },
    {
      "epoch": 24.32969215491559,
      "grad_norm": 11.27409839630127,
      "learning_rate": 2.9725256537570345e-05,
      "loss": 2.0012,
      "step": 318500
    },
    {
      "epoch": 24.33733099075701,
      "grad_norm": 13.433967590332031,
      "learning_rate": 2.971889084103583e-05,
      "loss": 1.9507,
      "step": 318600
    },
    {
      "epoch": 24.344969826598426,
      "grad_norm": 13.346979141235352,
      "learning_rate": 2.9712525144501312e-05,
      "loss": 2.0401,
      "step": 318700
    },
    {
      "epoch": 24.352608662439845,
      "grad_norm": 11.177075386047363,
      "learning_rate": 2.9706159447966796e-05,
      "loss": 1.9763,
      "step": 318800
    },
    {
      "epoch": 24.36024749828126,
      "grad_norm": 12.102593421936035,
      "learning_rate": 2.969979375143228e-05,
      "loss": 1.9617,
      "step": 318900
    },
    {
      "epoch": 24.36788633412268,
      "grad_norm": 14.695980072021484,
      "learning_rate": 2.969342805489777e-05,
      "loss": 1.9577,
      "step": 319000
    },
    {
      "epoch": 24.375525169964096,
      "grad_norm": 11.467146873474121,
      "learning_rate": 2.9687062358363253e-05,
      "loss": 1.9804,
      "step": 319100
    },
    {
      "epoch": 24.383164005805515,
      "grad_norm": 10.943455696105957,
      "learning_rate": 2.9680696661828737e-05,
      "loss": 2.0104,
      "step": 319200
    },
    {
      "epoch": 24.390802841646934,
      "grad_norm": 13.654062271118164,
      "learning_rate": 2.9674330965294224e-05,
      "loss": 1.9865,
      "step": 319300
    },
    {
      "epoch": 24.39844167748835,
      "grad_norm": 16.115848541259766,
      "learning_rate": 2.9667965268759707e-05,
      "loss": 2.0407,
      "step": 319400
    },
    {
      "epoch": 24.40608051332977,
      "grad_norm": 11.532691955566406,
      "learning_rate": 2.9661599572225194e-05,
      "loss": 2.0886,
      "step": 319500
    },
    {
      "epoch": 24.413719349171185,
      "grad_norm": 13.142005920410156,
      "learning_rate": 2.965523387569068e-05,
      "loss": 1.9003,
      "step": 319600
    },
    {
      "epoch": 24.421358185012604,
      "grad_norm": 13.888655662536621,
      "learning_rate": 2.9648868179156165e-05,
      "loss": 2.057,
      "step": 319700
    },
    {
      "epoch": 24.428997020854023,
      "grad_norm": 11.897536277770996,
      "learning_rate": 2.964250248262165e-05,
      "loss": 2.0344,
      "step": 319800
    },
    {
      "epoch": 24.43663585669544,
      "grad_norm": 15.033551216125488,
      "learning_rate": 2.963613678608714e-05,
      "loss": 2.0394,
      "step": 319900
    },
    {
      "epoch": 24.444274692536858,
      "grad_norm": 13.864797592163086,
      "learning_rate": 2.9629771089552622e-05,
      "loss": 1.8998,
      "step": 320000
    },
    {
      "epoch": 24.451913528378274,
      "grad_norm": 12.253620147705078,
      "learning_rate": 2.9623405393018106e-05,
      "loss": 1.9958,
      "step": 320100
    },
    {
      "epoch": 24.459552364219693,
      "grad_norm": 14.307621955871582,
      "learning_rate": 2.961703969648359e-05,
      "loss": 2.061,
      "step": 320200
    },
    {
      "epoch": 24.467191200061112,
      "grad_norm": 13.795572280883789,
      "learning_rate": 2.9610673999949073e-05,
      "loss": 2.0214,
      "step": 320300
    },
    {
      "epoch": 24.474830035902528,
      "grad_norm": 12.206265449523926,
      "learning_rate": 2.9604308303414563e-05,
      "loss": 2.0384,
      "step": 320400
    },
    {
      "epoch": 24.482468871743947,
      "grad_norm": 16.84872817993164,
      "learning_rate": 2.9597942606880047e-05,
      "loss": 2.0472,
      "step": 320500
    },
    {
      "epoch": 24.490107707585363,
      "grad_norm": 15.46809196472168,
      "learning_rate": 2.959157691034553e-05,
      "loss": 1.9707,
      "step": 320600
    },
    {
      "epoch": 24.497746543426782,
      "grad_norm": 14.355887413024902,
      "learning_rate": 2.9585211213811014e-05,
      "loss": 2.0802,
      "step": 320700
    },
    {
      "epoch": 24.505385379268198,
      "grad_norm": 17.891239166259766,
      "learning_rate": 2.9578845517276504e-05,
      "loss": 2.0386,
      "step": 320800
    },
    {
      "epoch": 24.513024215109617,
      "grad_norm": 23.39215660095215,
      "learning_rate": 2.9572479820741988e-05,
      "loss": 2.0157,
      "step": 320900
    },
    {
      "epoch": 24.520663050951036,
      "grad_norm": 12.691183090209961,
      "learning_rate": 2.956611412420747e-05,
      "loss": 1.9478,
      "step": 321000
    },
    {
      "epoch": 24.528301886792452,
      "grad_norm": 14.653671264648438,
      "learning_rate": 2.9559748427672958e-05,
      "loss": 2.0365,
      "step": 321100
    },
    {
      "epoch": 24.53594072263387,
      "grad_norm": 12.733253479003906,
      "learning_rate": 2.955338273113844e-05,
      "loss": 2.0174,
      "step": 321200
    },
    {
      "epoch": 24.543579558475287,
      "grad_norm": 11.792166709899902,
      "learning_rate": 2.954701703460393e-05,
      "loss": 1.9323,
      "step": 321300
    },
    {
      "epoch": 24.551218394316706,
      "grad_norm": 15.54946231842041,
      "learning_rate": 2.9540651338069415e-05,
      "loss": 2.0629,
      "step": 321400
    },
    {
      "epoch": 24.558857230158125,
      "grad_norm": 10.563587188720703,
      "learning_rate": 2.95342856415349e-05,
      "loss": 2.0477,
      "step": 321500
    },
    {
      "epoch": 24.56649606599954,
      "grad_norm": 13.219491958618164,
      "learning_rate": 2.9527919945000382e-05,
      "loss": 2.0438,
      "step": 321600
    },
    {
      "epoch": 24.57413490184096,
      "grad_norm": 13.515164375305176,
      "learning_rate": 2.9521554248465866e-05,
      "loss": 1.9951,
      "step": 321700
    },
    {
      "epoch": 24.581773737682376,
      "grad_norm": 11.76093864440918,
      "learning_rate": 2.9515188551931356e-05,
      "loss": 2.0382,
      "step": 321800
    },
    {
      "epoch": 24.589412573523795,
      "grad_norm": 13.838109016418457,
      "learning_rate": 2.950882285539684e-05,
      "loss": 2.0856,
      "step": 321900
    },
    {
      "epoch": 24.597051409365214,
      "grad_norm": 9.102964401245117,
      "learning_rate": 2.9502457158862323e-05,
      "loss": 2.0225,
      "step": 322000
    },
    {
      "epoch": 24.60469024520663,
      "grad_norm": 18.03590965270996,
      "learning_rate": 2.9496091462327807e-05,
      "loss": 2.0144,
      "step": 322100
    },
    {
      "epoch": 24.61232908104805,
      "grad_norm": 12.473917961120605,
      "learning_rate": 2.9489725765793297e-05,
      "loss": 2.0688,
      "step": 322200
    },
    {
      "epoch": 24.619967916889465,
      "grad_norm": 12.227736473083496,
      "learning_rate": 2.948336006925878e-05,
      "loss": 1.9184,
      "step": 322300
    },
    {
      "epoch": 24.627606752730884,
      "grad_norm": 17.528322219848633,
      "learning_rate": 2.9476994372724264e-05,
      "loss": 2.1075,
      "step": 322400
    },
    {
      "epoch": 24.635245588572303,
      "grad_norm": 13.987131118774414,
      "learning_rate": 2.9470628676189748e-05,
      "loss": 1.9376,
      "step": 322500
    },
    {
      "epoch": 24.64288442441372,
      "grad_norm": 17.45693016052246,
      "learning_rate": 2.946426297965523e-05,
      "loss": 2.123,
      "step": 322600
    },
    {
      "epoch": 24.650523260255138,
      "grad_norm": 15.379114151000977,
      "learning_rate": 2.9457897283120722e-05,
      "loss": 2.0076,
      "step": 322700
    },
    {
      "epoch": 24.658162096096554,
      "grad_norm": 14.243234634399414,
      "learning_rate": 2.9451531586586205e-05,
      "loss": 2.0333,
      "step": 322800
    },
    {
      "epoch": 24.665800931937973,
      "grad_norm": 17.05530548095703,
      "learning_rate": 2.944516589005169e-05,
      "loss": 2.0942,
      "step": 322900
    },
    {
      "epoch": 24.673439767779392,
      "grad_norm": 11.376911163330078,
      "learning_rate": 2.9438800193517176e-05,
      "loss": 2.0756,
      "step": 323000
    },
    {
      "epoch": 24.681078603620808,
      "grad_norm": 16.678869247436523,
      "learning_rate": 2.9432434496982663e-05,
      "loss": 1.9078,
      "step": 323100
    },
    {
      "epoch": 24.688717439462227,
      "grad_norm": 11.034854888916016,
      "learning_rate": 2.9426068800448146e-05,
      "loss": 1.9691,
      "step": 323200
    },
    {
      "epoch": 24.696356275303643,
      "grad_norm": 14.950064659118652,
      "learning_rate": 2.9419703103913633e-05,
      "loss": 2.0588,
      "step": 323300
    },
    {
      "epoch": 24.703995111145062,
      "grad_norm": 15.035813331604004,
      "learning_rate": 2.9413337407379117e-05,
      "loss": 2.0688,
      "step": 323400
    },
    {
      "epoch": 24.711633946986478,
      "grad_norm": 14.843127250671387,
      "learning_rate": 2.94069717108446e-05,
      "loss": 1.9054,
      "step": 323500
    },
    {
      "epoch": 24.719272782827897,
      "grad_norm": 13.92562484741211,
      "learning_rate": 2.940060601431009e-05,
      "loss": 2.0182,
      "step": 323600
    },
    {
      "epoch": 24.726911618669316,
      "grad_norm": 11.513664245605469,
      "learning_rate": 2.9394240317775574e-05,
      "loss": 2.0138,
      "step": 323700
    },
    {
      "epoch": 24.73455045451073,
      "grad_norm": 14.83057689666748,
      "learning_rate": 2.9387874621241058e-05,
      "loss": 2.0349,
      "step": 323800
    },
    {
      "epoch": 24.74218929035215,
      "grad_norm": 12.851360321044922,
      "learning_rate": 2.938150892470654e-05,
      "loss": 2.0156,
      "step": 323900
    },
    {
      "epoch": 24.749828126193567,
      "grad_norm": 13.130278587341309,
      "learning_rate": 2.937514322817203e-05,
      "loss": 2.0736,
      "step": 324000
    },
    {
      "epoch": 24.757466962034986,
      "grad_norm": 21.10912322998047,
      "learning_rate": 2.9368777531637515e-05,
      "loss": 2.0066,
      "step": 324100
    },
    {
      "epoch": 24.765105797876405,
      "grad_norm": 18.007186889648438,
      "learning_rate": 2.9362411835103e-05,
      "loss": 2.0217,
      "step": 324200
    },
    {
      "epoch": 24.77274463371782,
      "grad_norm": 10.612418174743652,
      "learning_rate": 2.9356046138568482e-05,
      "loss": 1.9737,
      "step": 324300
    },
    {
      "epoch": 24.78038346955924,
      "grad_norm": 13.171984672546387,
      "learning_rate": 2.9349680442033966e-05,
      "loss": 2.0457,
      "step": 324400
    },
    {
      "epoch": 24.788022305400656,
      "grad_norm": 15.09793472290039,
      "learning_rate": 2.9343314745499456e-05,
      "loss": 2.0235,
      "step": 324500
    },
    {
      "epoch": 24.795661141242075,
      "grad_norm": 13.979959487915039,
      "learning_rate": 2.933694904896494e-05,
      "loss": 2.0814,
      "step": 324600
    },
    {
      "epoch": 24.803299977083494,
      "grad_norm": 14.09533405303955,
      "learning_rate": 2.9330583352430423e-05,
      "loss": 2.0137,
      "step": 324700
    },
    {
      "epoch": 24.81093881292491,
      "grad_norm": 13.370728492736816,
      "learning_rate": 2.9324217655895906e-05,
      "loss": 2.0732,
      "step": 324800
    },
    {
      "epoch": 24.81857764876633,
      "grad_norm": 14.5905179977417,
      "learning_rate": 2.9317851959361393e-05,
      "loss": 2.0681,
      "step": 324900
    },
    {
      "epoch": 24.826216484607745,
      "grad_norm": 11.071066856384277,
      "learning_rate": 2.931148626282688e-05,
      "loss": 1.9824,
      "step": 325000
    },
    {
      "epoch": 24.833855320449164,
      "grad_norm": 12.42683219909668,
      "learning_rate": 2.9305120566292367e-05,
      "loss": 2.0359,
      "step": 325100
    },
    {
      "epoch": 24.84149415629058,
      "grad_norm": 14.471479415893555,
      "learning_rate": 2.929875486975785e-05,
      "loss": 2.0874,
      "step": 325200
    },
    {
      "epoch": 24.849132992132,
      "grad_norm": 12.868785858154297,
      "learning_rate": 2.9292389173223334e-05,
      "loss": 1.9587,
      "step": 325300
    },
    {
      "epoch": 24.856771827973418,
      "grad_norm": 14.30985164642334,
      "learning_rate": 2.9286023476688825e-05,
      "loss": 2.0054,
      "step": 325400
    },
    {
      "epoch": 24.864410663814834,
      "grad_norm": 14.386893272399902,
      "learning_rate": 2.9279657780154308e-05,
      "loss": 2.0084,
      "step": 325500
    },
    {
      "epoch": 24.872049499656253,
      "grad_norm": 13.931686401367188,
      "learning_rate": 2.9273292083619792e-05,
      "loss": 2.0846,
      "step": 325600
    },
    {
      "epoch": 24.87968833549767,
      "grad_norm": 13.3452787399292,
      "learning_rate": 2.9266926387085275e-05,
      "loss": 1.991,
      "step": 325700
    },
    {
      "epoch": 24.887327171339088,
      "grad_norm": 14.215516090393066,
      "learning_rate": 2.926056069055076e-05,
      "loss": 2.0213,
      "step": 325800
    },
    {
      "epoch": 24.894966007180507,
      "grad_norm": 12.871792793273926,
      "learning_rate": 2.925419499401625e-05,
      "loss": 1.9987,
      "step": 325900
    },
    {
      "epoch": 24.902604843021923,
      "grad_norm": 15.125370025634766,
      "learning_rate": 2.9247829297481733e-05,
      "loss": 2.01,
      "step": 326000
    },
    {
      "epoch": 24.910243678863342,
      "grad_norm": 10.775851249694824,
      "learning_rate": 2.9241463600947216e-05,
      "loss": 1.9965,
      "step": 326100
    },
    {
      "epoch": 24.917882514704758,
      "grad_norm": 13.740142822265625,
      "learning_rate": 2.92350979044127e-05,
      "loss": 1.8965,
      "step": 326200
    },
    {
      "epoch": 24.925521350546177,
      "grad_norm": 12.215462684631348,
      "learning_rate": 2.922873220787819e-05,
      "loss": 2.0586,
      "step": 326300
    },
    {
      "epoch": 24.933160186387596,
      "grad_norm": 18.03266143798828,
      "learning_rate": 2.9222366511343674e-05,
      "loss": 1.9624,
      "step": 326400
    },
    {
      "epoch": 24.94079902222901,
      "grad_norm": 15.209237098693848,
      "learning_rate": 2.9216000814809157e-05,
      "loss": 2.0485,
      "step": 326500
    },
    {
      "epoch": 24.94843785807043,
      "grad_norm": 17.58045196533203,
      "learning_rate": 2.920963511827464e-05,
      "loss": 1.9765,
      "step": 326600
    },
    {
      "epoch": 24.956076693911847,
      "grad_norm": 12.943202018737793,
      "learning_rate": 2.9203269421740128e-05,
      "loss": 1.9867,
      "step": 326700
    },
    {
      "epoch": 24.963715529753266,
      "grad_norm": 16.9558162689209,
      "learning_rate": 2.9196903725205614e-05,
      "loss": 2.005,
      "step": 326800
    },
    {
      "epoch": 24.971354365594685,
      "grad_norm": 12.520269393920898,
      "learning_rate": 2.9190538028671098e-05,
      "loss": 1.8957,
      "step": 326900
    },
    {
      "epoch": 24.9789932014361,
      "grad_norm": 9.644694328308105,
      "learning_rate": 2.9184172332136585e-05,
      "loss": 2.0959,
      "step": 327000
    },
    {
      "epoch": 24.98663203727752,
      "grad_norm": 11.586548805236816,
      "learning_rate": 2.917780663560207e-05,
      "loss": 2.0085,
      "step": 327100
    },
    {
      "epoch": 24.994270873118936,
      "grad_norm": 16.34889793395996,
      "learning_rate": 2.9171440939067555e-05,
      "loss": 2.0192,
      "step": 327200
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.9721390008926392,
      "eval_runtime": 1.6904,
      "eval_samples_per_second": 408.177,
      "eval_steps_per_second": 408.177,
      "step": 327275
    },
    {
      "epoch": 25.0,
      "eval_loss": 1.7593858242034912,
      "eval_runtime": 31.513,
      "eval_samples_per_second": 415.416,
      "eval_steps_per_second": 415.416,
      "step": 327275
    }
  ],
  "logging_steps": 100,
  "max_steps": 785460,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 202520492928000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
