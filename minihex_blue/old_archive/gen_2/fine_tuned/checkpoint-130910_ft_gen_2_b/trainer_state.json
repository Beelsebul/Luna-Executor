{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 130910,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007638835841417768,
      "grad_norm": 10.657843589782715,
      "learning_rate": 4.999045145519823e-05,
      "loss": 3.3775,
      "step": 100
    },
    {
      "epoch": 0.015277671682835536,
      "grad_norm": 8.593043327331543,
      "learning_rate": 4.998090291039646e-05,
      "loss": 2.7733,
      "step": 200
    },
    {
      "epoch": 0.022916507524253303,
      "grad_norm": 6.839597702026367,
      "learning_rate": 4.997135436559469e-05,
      "loss": 2.5876,
      "step": 300
    },
    {
      "epoch": 0.030555343365671072,
      "grad_norm": 6.752279758453369,
      "learning_rate": 4.996180582079291e-05,
      "loss": 2.5862,
      "step": 400
    },
    {
      "epoch": 0.03819417920708884,
      "grad_norm": 7.681690216064453,
      "learning_rate": 4.9952257275991146e-05,
      "loss": 2.4859,
      "step": 500
    },
    {
      "epoch": 0.045833015048506606,
      "grad_norm": 8.203631401062012,
      "learning_rate": 4.994270873118937e-05,
      "loss": 2.5746,
      "step": 600
    },
    {
      "epoch": 0.05347185088992438,
      "grad_norm": 8.797572135925293,
      "learning_rate": 4.9933160186387596e-05,
      "loss": 2.5264,
      "step": 700
    },
    {
      "epoch": 0.061110686731342144,
      "grad_norm": 8.149998664855957,
      "learning_rate": 4.9923611641585825e-05,
      "loss": 2.4843,
      "step": 800
    },
    {
      "epoch": 0.06874952257275992,
      "grad_norm": 6.0527191162109375,
      "learning_rate": 4.9914063096784054e-05,
      "loss": 2.352,
      "step": 900
    },
    {
      "epoch": 0.07638835841417768,
      "grad_norm": 8.628549575805664,
      "learning_rate": 4.9904514551982276e-05,
      "loss": 2.4528,
      "step": 1000
    },
    {
      "epoch": 0.08402719425559545,
      "grad_norm": 6.464604377746582,
      "learning_rate": 4.989496600718051e-05,
      "loss": 2.5456,
      "step": 1100
    },
    {
      "epoch": 0.09166603009701321,
      "grad_norm": 5.797004699707031,
      "learning_rate": 4.988541746237873e-05,
      "loss": 2.4371,
      "step": 1200
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 6.0449748039245605,
      "learning_rate": 4.987586891757696e-05,
      "loss": 2.4681,
      "step": 1300
    },
    {
      "epoch": 0.10694370177984876,
      "grad_norm": 6.874170780181885,
      "learning_rate": 4.986632037277519e-05,
      "loss": 2.4058,
      "step": 1400
    },
    {
      "epoch": 0.11458253762126652,
      "grad_norm": 5.592609882354736,
      "learning_rate": 4.985677182797342e-05,
      "loss": 2.3223,
      "step": 1500
    },
    {
      "epoch": 0.12222137346268429,
      "grad_norm": 9.184085845947266,
      "learning_rate": 4.984722328317165e-05,
      "loss": 2.4646,
      "step": 1600
    },
    {
      "epoch": 0.12986020930410205,
      "grad_norm": 6.0286712646484375,
      "learning_rate": 4.983767473836988e-05,
      "loss": 2.3133,
      "step": 1700
    },
    {
      "epoch": 0.13749904514551983,
      "grad_norm": 6.738003253936768,
      "learning_rate": 4.9828126193568105e-05,
      "loss": 2.3394,
      "step": 1800
    },
    {
      "epoch": 0.14513788098693758,
      "grad_norm": 8.912747383117676,
      "learning_rate": 4.981857764876633e-05,
      "loss": 2.3457,
      "step": 1900
    },
    {
      "epoch": 0.15277671682835536,
      "grad_norm": 5.332336902618408,
      "learning_rate": 4.980902910396456e-05,
      "loss": 2.45,
      "step": 2000
    },
    {
      "epoch": 0.16041555266977311,
      "grad_norm": 7.68809700012207,
      "learning_rate": 4.9799480559162785e-05,
      "loss": 2.2977,
      "step": 2100
    },
    {
      "epoch": 0.1680543885111909,
      "grad_norm": 7.897040843963623,
      "learning_rate": 4.978993201436101e-05,
      "loss": 2.2737,
      "step": 2200
    },
    {
      "epoch": 0.17569322435260867,
      "grad_norm": 8.165068626403809,
      "learning_rate": 4.978038346955924e-05,
      "loss": 2.2572,
      "step": 2300
    },
    {
      "epoch": 0.18333206019402642,
      "grad_norm": 6.318943023681641,
      "learning_rate": 4.977083492475747e-05,
      "loss": 2.3903,
      "step": 2400
    },
    {
      "epoch": 0.1909708960354442,
      "grad_norm": 8.665945053100586,
      "learning_rate": 4.97612863799557e-05,
      "loss": 2.338,
      "step": 2500
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 7.775863170623779,
      "learning_rate": 4.975173783515393e-05,
      "loss": 2.3601,
      "step": 2600
    },
    {
      "epoch": 0.20624856771827974,
      "grad_norm": 7.405876636505127,
      "learning_rate": 4.974218929035216e-05,
      "loss": 2.2724,
      "step": 2700
    },
    {
      "epoch": 0.21388740355969751,
      "grad_norm": 6.559520721435547,
      "learning_rate": 4.973264074555038e-05,
      "loss": 2.2707,
      "step": 2800
    },
    {
      "epoch": 0.22152623940111527,
      "grad_norm": 6.22802734375,
      "learning_rate": 4.972309220074861e-05,
      "loss": 2.2552,
      "step": 2900
    },
    {
      "epoch": 0.22916507524253305,
      "grad_norm": 6.395657062530518,
      "learning_rate": 4.9713543655946836e-05,
      "loss": 2.3393,
      "step": 3000
    },
    {
      "epoch": 0.2368039110839508,
      "grad_norm": 5.595874309539795,
      "learning_rate": 4.9703995111145065e-05,
      "loss": 2.2991,
      "step": 3100
    },
    {
      "epoch": 0.24444274692536858,
      "grad_norm": 5.636528968811035,
      "learning_rate": 4.969444656634329e-05,
      "loss": 2.2773,
      "step": 3200
    },
    {
      "epoch": 0.2520815827667863,
      "grad_norm": 7.409404277801514,
      "learning_rate": 4.968489802154152e-05,
      "loss": 2.3463,
      "step": 3300
    },
    {
      "epoch": 0.2597204186082041,
      "grad_norm": 5.971039295196533,
      "learning_rate": 4.9675349476739744e-05,
      "loss": 2.28,
      "step": 3400
    },
    {
      "epoch": 0.2673592544496219,
      "grad_norm": 6.9893107414245605,
      "learning_rate": 4.966580093193797e-05,
      "loss": 2.2475,
      "step": 3500
    },
    {
      "epoch": 0.27499809029103967,
      "grad_norm": 7.263493537902832,
      "learning_rate": 4.96562523871362e-05,
      "loss": 2.2545,
      "step": 3600
    },
    {
      "epoch": 0.2826369261324574,
      "grad_norm": 5.20176887512207,
      "learning_rate": 4.964670384233443e-05,
      "loss": 2.2369,
      "step": 3700
    },
    {
      "epoch": 0.29027576197387517,
      "grad_norm": 7.313406944274902,
      "learning_rate": 4.963715529753266e-05,
      "loss": 2.3171,
      "step": 3800
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 7.786764144897461,
      "learning_rate": 4.962760675273089e-05,
      "loss": 2.2247,
      "step": 3900
    },
    {
      "epoch": 0.3055534336567107,
      "grad_norm": 7.441811561584473,
      "learning_rate": 4.9618058207929116e-05,
      "loss": 2.2796,
      "step": 4000
    },
    {
      "epoch": 0.3131922694981285,
      "grad_norm": 8.033016204833984,
      "learning_rate": 4.960850966312734e-05,
      "loss": 2.2316,
      "step": 4100
    },
    {
      "epoch": 0.32083110533954623,
      "grad_norm": 6.029402732849121,
      "learning_rate": 4.9598961118325574e-05,
      "loss": 2.3599,
      "step": 4200
    },
    {
      "epoch": 0.328469941180964,
      "grad_norm": 5.907043933868408,
      "learning_rate": 4.9589412573523796e-05,
      "loss": 2.1529,
      "step": 4300
    },
    {
      "epoch": 0.3361087770223818,
      "grad_norm": 8.1845064163208,
      "learning_rate": 4.9579864028722024e-05,
      "loss": 2.2844,
      "step": 4400
    },
    {
      "epoch": 0.34374761286379957,
      "grad_norm": 6.590937614440918,
      "learning_rate": 4.957031548392025e-05,
      "loss": 2.21,
      "step": 4500
    },
    {
      "epoch": 0.35138644870521735,
      "grad_norm": 6.501490592956543,
      "learning_rate": 4.956076693911848e-05,
      "loss": 2.2644,
      "step": 4600
    },
    {
      "epoch": 0.35902528454663507,
      "grad_norm": 7.2427659034729,
      "learning_rate": 4.9551218394316704e-05,
      "loss": 2.2885,
      "step": 4700
    },
    {
      "epoch": 0.36666412038805285,
      "grad_norm": 6.897184371948242,
      "learning_rate": 4.954166984951494e-05,
      "loss": 2.3156,
      "step": 4800
    },
    {
      "epoch": 0.37430295622947063,
      "grad_norm": 8.671448707580566,
      "learning_rate": 4.953212130471316e-05,
      "loss": 2.3517,
      "step": 4900
    },
    {
      "epoch": 0.3819417920708884,
      "grad_norm": 6.977689266204834,
      "learning_rate": 4.952257275991139e-05,
      "loss": 2.2134,
      "step": 5000
    },
    {
      "epoch": 0.3895806279123062,
      "grad_norm": 7.203551769256592,
      "learning_rate": 4.951302421510962e-05,
      "loss": 2.3118,
      "step": 5100
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 8.980148315429688,
      "learning_rate": 4.950347567030785e-05,
      "loss": 2.2231,
      "step": 5200
    },
    {
      "epoch": 0.4048582995951417,
      "grad_norm": 5.946340084075928,
      "learning_rate": 4.9493927125506076e-05,
      "loss": 2.2185,
      "step": 5300
    },
    {
      "epoch": 0.41249713543655947,
      "grad_norm": 5.594563961029053,
      "learning_rate": 4.9484378580704304e-05,
      "loss": 2.1813,
      "step": 5400
    },
    {
      "epoch": 0.42013597127797725,
      "grad_norm": 7.857141971588135,
      "learning_rate": 4.947483003590253e-05,
      "loss": 2.364,
      "step": 5500
    },
    {
      "epoch": 0.42777480711939503,
      "grad_norm": 7.803111553192139,
      "learning_rate": 4.9465281491100755e-05,
      "loss": 2.1521,
      "step": 5600
    },
    {
      "epoch": 0.43541364296081275,
      "grad_norm": 6.6732635498046875,
      "learning_rate": 4.945573294629899e-05,
      "loss": 2.1603,
      "step": 5700
    },
    {
      "epoch": 0.44305247880223053,
      "grad_norm": 5.010250091552734,
      "learning_rate": 4.944618440149721e-05,
      "loss": 2.2466,
      "step": 5800
    },
    {
      "epoch": 0.4506913146436483,
      "grad_norm": 6.27398681640625,
      "learning_rate": 4.943663585669544e-05,
      "loss": 2.2057,
      "step": 5900
    },
    {
      "epoch": 0.4583301504850661,
      "grad_norm": 6.938957214355469,
      "learning_rate": 4.942708731189367e-05,
      "loss": 2.295,
      "step": 6000
    },
    {
      "epoch": 0.46596898632648387,
      "grad_norm": 5.0990190505981445,
      "learning_rate": 4.94175387670919e-05,
      "loss": 2.2928,
      "step": 6100
    },
    {
      "epoch": 0.4736078221679016,
      "grad_norm": 6.596705913543701,
      "learning_rate": 4.940799022229012e-05,
      "loss": 2.2548,
      "step": 6200
    },
    {
      "epoch": 0.4812466580093194,
      "grad_norm": 7.948777198791504,
      "learning_rate": 4.9398441677488356e-05,
      "loss": 2.2261,
      "step": 6300
    },
    {
      "epoch": 0.48888549385073715,
      "grad_norm": 7.873510360717773,
      "learning_rate": 4.938889313268658e-05,
      "loss": 2.2501,
      "step": 6400
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 6.329504013061523,
      "learning_rate": 4.9379344587884806e-05,
      "loss": 2.1904,
      "step": 6500
    },
    {
      "epoch": 0.5041631655335727,
      "grad_norm": 7.776589870452881,
      "learning_rate": 4.936979604308304e-05,
      "loss": 2.0766,
      "step": 6600
    },
    {
      "epoch": 0.5118020013749904,
      "grad_norm": 4.324591636657715,
      "learning_rate": 4.9360247498281264e-05,
      "loss": 2.1708,
      "step": 6700
    },
    {
      "epoch": 0.5194408372164082,
      "grad_norm": 8.112269401550293,
      "learning_rate": 4.935069895347949e-05,
      "loss": 2.1964,
      "step": 6800
    },
    {
      "epoch": 0.527079673057826,
      "grad_norm": 6.158720970153809,
      "learning_rate": 4.934115040867772e-05,
      "loss": 2.1896,
      "step": 6900
    },
    {
      "epoch": 0.5347185088992438,
      "grad_norm": 7.265646457672119,
      "learning_rate": 4.933160186387595e-05,
      "loss": 2.1536,
      "step": 7000
    },
    {
      "epoch": 0.5423573447406616,
      "grad_norm": 6.98304557800293,
      "learning_rate": 4.932205331907417e-05,
      "loss": 2.2335,
      "step": 7100
    },
    {
      "epoch": 0.5499961805820793,
      "grad_norm": 7.57611608505249,
      "learning_rate": 4.931250477427241e-05,
      "loss": 2.1411,
      "step": 7200
    },
    {
      "epoch": 0.5576350164234971,
      "grad_norm": 6.316906452178955,
      "learning_rate": 4.930295622947063e-05,
      "loss": 2.1838,
      "step": 7300
    },
    {
      "epoch": 0.5652738522649148,
      "grad_norm": 7.626104354858398,
      "learning_rate": 4.929340768466886e-05,
      "loss": 2.2287,
      "step": 7400
    },
    {
      "epoch": 0.5729126881063326,
      "grad_norm": 5.8879194259643555,
      "learning_rate": 4.928385913986709e-05,
      "loss": 2.2305,
      "step": 7500
    },
    {
      "epoch": 0.5805515239477503,
      "grad_norm": 6.38561487197876,
      "learning_rate": 4.9274310595065315e-05,
      "loss": 2.1803,
      "step": 7600
    },
    {
      "epoch": 0.5881903597891681,
      "grad_norm": 5.906652927398682,
      "learning_rate": 4.9264762050263544e-05,
      "loss": 2.2461,
      "step": 7700
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 6.4330735206604,
      "learning_rate": 4.925521350546177e-05,
      "loss": 2.1703,
      "step": 7800
    },
    {
      "epoch": 0.6034680314720037,
      "grad_norm": 7.464908123016357,
      "learning_rate": 4.924566496066e-05,
      "loss": 2.3294,
      "step": 7900
    },
    {
      "epoch": 0.6111068673134215,
      "grad_norm": 6.857761383056641,
      "learning_rate": 4.923611641585822e-05,
      "loss": 2.1936,
      "step": 8000
    },
    {
      "epoch": 0.6187457031548392,
      "grad_norm": 9.278703689575195,
      "learning_rate": 4.922656787105646e-05,
      "loss": 2.2414,
      "step": 8100
    },
    {
      "epoch": 0.626384538996257,
      "grad_norm": 6.639617443084717,
      "learning_rate": 4.921701932625468e-05,
      "loss": 2.3605,
      "step": 8200
    },
    {
      "epoch": 0.6340233748376748,
      "grad_norm": 7.915894031524658,
      "learning_rate": 4.920747078145291e-05,
      "loss": 2.184,
      "step": 8300
    },
    {
      "epoch": 0.6416622106790925,
      "grad_norm": 6.4702277183532715,
      "learning_rate": 4.919792223665114e-05,
      "loss": 2.3064,
      "step": 8400
    },
    {
      "epoch": 0.6493010465205102,
      "grad_norm": 5.8580193519592285,
      "learning_rate": 4.918837369184937e-05,
      "loss": 2.2587,
      "step": 8500
    },
    {
      "epoch": 0.656939882361928,
      "grad_norm": 6.946641445159912,
      "learning_rate": 4.917882514704759e-05,
      "loss": 2.3173,
      "step": 8600
    },
    {
      "epoch": 0.6645787182033458,
      "grad_norm": 6.735767364501953,
      "learning_rate": 4.916927660224582e-05,
      "loss": 2.2043,
      "step": 8700
    },
    {
      "epoch": 0.6722175540447636,
      "grad_norm": 6.466943740844727,
      "learning_rate": 4.9159728057444046e-05,
      "loss": 2.3036,
      "step": 8800
    },
    {
      "epoch": 0.6798563898861814,
      "grad_norm": 7.8700947761535645,
      "learning_rate": 4.9150179512642275e-05,
      "loss": 2.1279,
      "step": 8900
    },
    {
      "epoch": 0.6874952257275991,
      "grad_norm": 6.589061260223389,
      "learning_rate": 4.9140630967840503e-05,
      "loss": 2.1611,
      "step": 9000
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 7.277041912078857,
      "learning_rate": 4.913108242303873e-05,
      "loss": 2.2382,
      "step": 9100
    },
    {
      "epoch": 0.7027728974104347,
      "grad_norm": 7.649940490722656,
      "learning_rate": 4.912153387823696e-05,
      "loss": 2.1537,
      "step": 9200
    },
    {
      "epoch": 0.7104117332518525,
      "grad_norm": 8.28800106048584,
      "learning_rate": 4.911198533343518e-05,
      "loss": 2.123,
      "step": 9300
    },
    {
      "epoch": 0.7180505690932701,
      "grad_norm": 6.725391387939453,
      "learning_rate": 4.910243678863342e-05,
      "loss": 2.0346,
      "step": 9400
    },
    {
      "epoch": 0.7256894049346879,
      "grad_norm": 7.296143054962158,
      "learning_rate": 4.909288824383164e-05,
      "loss": 2.1828,
      "step": 9500
    },
    {
      "epoch": 0.7333282407761057,
      "grad_norm": 6.575263500213623,
      "learning_rate": 4.908333969902987e-05,
      "loss": 2.2687,
      "step": 9600
    },
    {
      "epoch": 0.7409670766175235,
      "grad_norm": 8.261496543884277,
      "learning_rate": 4.90737911542281e-05,
      "loss": 2.1705,
      "step": 9700
    },
    {
      "epoch": 0.7486059124589413,
      "grad_norm": 6.935733795166016,
      "learning_rate": 4.9064242609426326e-05,
      "loss": 2.0446,
      "step": 9800
    },
    {
      "epoch": 0.756244748300359,
      "grad_norm": 6.254842758178711,
      "learning_rate": 4.905469406462455e-05,
      "loss": 2.1122,
      "step": 9900
    },
    {
      "epoch": 0.7638835841417768,
      "grad_norm": 5.258333206176758,
      "learning_rate": 4.9045145519822784e-05,
      "loss": 2.1235,
      "step": 10000
    },
    {
      "epoch": 0.7715224199831946,
      "grad_norm": 6.896238327026367,
      "learning_rate": 4.9035596975021006e-05,
      "loss": 2.1002,
      "step": 10100
    },
    {
      "epoch": 0.7791612558246124,
      "grad_norm": 7.804656505584717,
      "learning_rate": 4.9026048430219234e-05,
      "loss": 2.1662,
      "step": 10200
    },
    {
      "epoch": 0.78680009166603,
      "grad_norm": 6.888854026794434,
      "learning_rate": 4.901649988541746e-05,
      "loss": 2.0903,
      "step": 10300
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 6.146206855773926,
      "learning_rate": 4.900695134061569e-05,
      "loss": 2.2039,
      "step": 10400
    },
    {
      "epoch": 0.8020777633488656,
      "grad_norm": 7.039429187774658,
      "learning_rate": 4.899740279581392e-05,
      "loss": 2.2645,
      "step": 10500
    },
    {
      "epoch": 0.8097165991902834,
      "grad_norm": 5.6176276206970215,
      "learning_rate": 4.898785425101215e-05,
      "loss": 2.1505,
      "step": 10600
    },
    {
      "epoch": 0.8173554350317012,
      "grad_norm": 7.000094890594482,
      "learning_rate": 4.897830570621038e-05,
      "loss": 2.2589,
      "step": 10700
    },
    {
      "epoch": 0.8249942708731189,
      "grad_norm": 5.843156814575195,
      "learning_rate": 4.89687571614086e-05,
      "loss": 2.1184,
      "step": 10800
    },
    {
      "epoch": 0.8326331067145367,
      "grad_norm": 6.515773773193359,
      "learning_rate": 4.8959208616606835e-05,
      "loss": 2.2086,
      "step": 10900
    },
    {
      "epoch": 0.8402719425559545,
      "grad_norm": 7.739624500274658,
      "learning_rate": 4.894966007180506e-05,
      "loss": 2.1981,
      "step": 11000
    },
    {
      "epoch": 0.8479107783973723,
      "grad_norm": 10.633212089538574,
      "learning_rate": 4.8940111527003286e-05,
      "loss": 2.1193,
      "step": 11100
    },
    {
      "epoch": 0.8555496142387901,
      "grad_norm": 5.956071376800537,
      "learning_rate": 4.8930562982201514e-05,
      "loss": 2.1799,
      "step": 11200
    },
    {
      "epoch": 0.8631884500802077,
      "grad_norm": 6.470799446105957,
      "learning_rate": 4.892101443739974e-05,
      "loss": 2.1821,
      "step": 11300
    },
    {
      "epoch": 0.8708272859216255,
      "grad_norm": 7.282951831817627,
      "learning_rate": 4.891146589259797e-05,
      "loss": 2.2075,
      "step": 11400
    },
    {
      "epoch": 0.8784661217630433,
      "grad_norm": 7.042235374450684,
      "learning_rate": 4.89019173477962e-05,
      "loss": 2.1455,
      "step": 11500
    },
    {
      "epoch": 0.8861049576044611,
      "grad_norm": 6.879594802856445,
      "learning_rate": 4.889236880299443e-05,
      "loss": 2.2092,
      "step": 11600
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 6.679892063140869,
      "learning_rate": 4.888282025819265e-05,
      "loss": 2.1634,
      "step": 11700
    },
    {
      "epoch": 0.9013826292872966,
      "grad_norm": 7.85321569442749,
      "learning_rate": 4.8873271713390887e-05,
      "loss": 2.2462,
      "step": 11800
    },
    {
      "epoch": 0.9090214651287144,
      "grad_norm": 7.814871788024902,
      "learning_rate": 4.886372316858911e-05,
      "loss": 2.2701,
      "step": 11900
    },
    {
      "epoch": 0.9166603009701322,
      "grad_norm": 6.750171184539795,
      "learning_rate": 4.885417462378734e-05,
      "loss": 2.1487,
      "step": 12000
    },
    {
      "epoch": 0.92429913681155,
      "grad_norm": 6.45812463760376,
      "learning_rate": 4.8844626078985566e-05,
      "loss": 2.1453,
      "step": 12100
    },
    {
      "epoch": 0.9319379726529677,
      "grad_norm": 6.799169063568115,
      "learning_rate": 4.8835077534183795e-05,
      "loss": 2.2721,
      "step": 12200
    },
    {
      "epoch": 0.9395768084943854,
      "grad_norm": 6.565350532531738,
      "learning_rate": 4.8825528989382016e-05,
      "loss": 2.1543,
      "step": 12300
    },
    {
      "epoch": 0.9472156443358032,
      "grad_norm": 8.927435874938965,
      "learning_rate": 4.881598044458025e-05,
      "loss": 2.1455,
      "step": 12400
    },
    {
      "epoch": 0.954854480177221,
      "grad_norm": 6.2788896560668945,
      "learning_rate": 4.8806431899778474e-05,
      "loss": 2.1331,
      "step": 12500
    },
    {
      "epoch": 0.9624933160186387,
      "grad_norm": 8.168878555297852,
      "learning_rate": 4.87968833549767e-05,
      "loss": 2.0968,
      "step": 12600
    },
    {
      "epoch": 0.9701321518600565,
      "grad_norm": 6.079451560974121,
      "learning_rate": 4.878733481017493e-05,
      "loss": 2.0883,
      "step": 12700
    },
    {
      "epoch": 0.9777709877014743,
      "grad_norm": 5.483613014221191,
      "learning_rate": 4.877778626537316e-05,
      "loss": 2.1369,
      "step": 12800
    },
    {
      "epoch": 0.9854098235428921,
      "grad_norm": 7.280965328216553,
      "learning_rate": 4.876823772057139e-05,
      "loss": 2.1447,
      "step": 12900
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 6.0614705085754395,
      "learning_rate": 4.875868917576962e-05,
      "loss": 2.1192,
      "step": 13000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.0605130195617676,
      "eval_runtime": 2.9739,
      "eval_samples_per_second": 232.022,
      "eval_steps_per_second": 232.022,
      "step": 13091
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.964215874671936,
      "eval_runtime": 36.4766,
      "eval_samples_per_second": 358.887,
      "eval_steps_per_second": 358.887,
      "step": 13091
    },
    {
      "epoch": 1.0006874952257276,
      "grad_norm": 5.976796627044678,
      "learning_rate": 4.8749140630967846e-05,
      "loss": 2.2336,
      "step": 13100
    },
    {
      "epoch": 1.0083263310671453,
      "grad_norm": 5.739107608795166,
      "learning_rate": 4.873959208616607e-05,
      "loss": 2.1473,
      "step": 13200
    },
    {
      "epoch": 1.0159651669085632,
      "grad_norm": 6.680917263031006,
      "learning_rate": 4.87300435413643e-05,
      "loss": 2.1207,
      "step": 13300
    },
    {
      "epoch": 1.0236040027499809,
      "grad_norm": 7.165970325469971,
      "learning_rate": 4.8720494996562525e-05,
      "loss": 2.1709,
      "step": 13400
    },
    {
      "epoch": 1.0312428385913988,
      "grad_norm": 9.24274730682373,
      "learning_rate": 4.8710946451760754e-05,
      "loss": 2.1452,
      "step": 13500
    },
    {
      "epoch": 1.0388816744328164,
      "grad_norm": 6.563086032867432,
      "learning_rate": 4.870139790695898e-05,
      "loss": 2.0929,
      "step": 13600
    },
    {
      "epoch": 1.046520510274234,
      "grad_norm": 5.805669784545898,
      "learning_rate": 4.869184936215721e-05,
      "loss": 2.0913,
      "step": 13700
    },
    {
      "epoch": 1.054159346115652,
      "grad_norm": 5.48603630065918,
      "learning_rate": 4.868230081735543e-05,
      "loss": 2.0956,
      "step": 13800
    },
    {
      "epoch": 1.0617981819570697,
      "grad_norm": 5.809831619262695,
      "learning_rate": 4.867275227255367e-05,
      "loss": 2.1545,
      "step": 13900
    },
    {
      "epoch": 1.0694370177984875,
      "grad_norm": 5.466857433319092,
      "learning_rate": 4.866320372775189e-05,
      "loss": 2.1173,
      "step": 14000
    },
    {
      "epoch": 1.0770758536399052,
      "grad_norm": 5.6993794441223145,
      "learning_rate": 4.865365518295012e-05,
      "loss": 2.1158,
      "step": 14100
    },
    {
      "epoch": 1.084714689481323,
      "grad_norm": 7.3900017738342285,
      "learning_rate": 4.864410663814835e-05,
      "loss": 2.1329,
      "step": 14200
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 7.998657703399658,
      "learning_rate": 4.863455809334658e-05,
      "loss": 2.0912,
      "step": 14300
    },
    {
      "epoch": 1.0999923611641587,
      "grad_norm": 7.3166680335998535,
      "learning_rate": 4.8625009548544805e-05,
      "loss": 2.1136,
      "step": 14400
    },
    {
      "epoch": 1.1076311970055763,
      "grad_norm": 6.926626682281494,
      "learning_rate": 4.861546100374303e-05,
      "loss": 2.06,
      "step": 14500
    },
    {
      "epoch": 1.1152700328469942,
      "grad_norm": 6.549658298492432,
      "learning_rate": 4.860591245894126e-05,
      "loss": 2.0724,
      "step": 14600
    },
    {
      "epoch": 1.1229088686884119,
      "grad_norm": 6.694014072418213,
      "learning_rate": 4.8596363914139485e-05,
      "loss": 2.0763,
      "step": 14700
    },
    {
      "epoch": 1.1305477045298296,
      "grad_norm": 5.7263946533203125,
      "learning_rate": 4.8586815369337713e-05,
      "loss": 2.0898,
      "step": 14800
    },
    {
      "epoch": 1.1381865403712474,
      "grad_norm": 5.81451940536499,
      "learning_rate": 4.857726682453594e-05,
      "loss": 2.1451,
      "step": 14900
    },
    {
      "epoch": 1.1458253762126651,
      "grad_norm": 9.874438285827637,
      "learning_rate": 4.856771827973417e-05,
      "loss": 2.2135,
      "step": 15000
    },
    {
      "epoch": 1.153464212054083,
      "grad_norm": 4.557074069976807,
      "learning_rate": 4.855816973493239e-05,
      "loss": 2.1432,
      "step": 15100
    },
    {
      "epoch": 1.1611030478955007,
      "grad_norm": 4.87415075302124,
      "learning_rate": 4.854862119013063e-05,
      "loss": 2.0759,
      "step": 15200
    },
    {
      "epoch": 1.1687418837369186,
      "grad_norm": 7.1597723960876465,
      "learning_rate": 4.853907264532886e-05,
      "loss": 2.1188,
      "step": 15300
    },
    {
      "epoch": 1.1763807195783362,
      "grad_norm": 6.168932914733887,
      "learning_rate": 4.852952410052708e-05,
      "loss": 2.1498,
      "step": 15400
    },
    {
      "epoch": 1.1840195554197541,
      "grad_norm": 6.645564079284668,
      "learning_rate": 4.8519975555725314e-05,
      "loss": 2.1152,
      "step": 15500
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 6.894056797027588,
      "learning_rate": 4.8510427010923536e-05,
      "loss": 2.1489,
      "step": 15600
    },
    {
      "epoch": 1.1992972271025897,
      "grad_norm": 6.829993724822998,
      "learning_rate": 4.8500878466121765e-05,
      "loss": 2.0378,
      "step": 15700
    },
    {
      "epoch": 1.2069360629440073,
      "grad_norm": 6.725090980529785,
      "learning_rate": 4.8491329921319994e-05,
      "loss": 2.1796,
      "step": 15800
    },
    {
      "epoch": 1.214574898785425,
      "grad_norm": 6.777667999267578,
      "learning_rate": 4.848178137651822e-05,
      "loss": 2.0818,
      "step": 15900
    },
    {
      "epoch": 1.222213734626843,
      "grad_norm": 7.45310640335083,
      "learning_rate": 4.8472232831716444e-05,
      "loss": 2.1842,
      "step": 16000
    },
    {
      "epoch": 1.2298525704682606,
      "grad_norm": 6.648919105529785,
      "learning_rate": 4.846268428691468e-05,
      "loss": 2.1156,
      "step": 16100
    },
    {
      "epoch": 1.2374914063096785,
      "grad_norm": 7.586540222167969,
      "learning_rate": 4.84531357421129e-05,
      "loss": 2.1148,
      "step": 16200
    },
    {
      "epoch": 1.2451302421510961,
      "grad_norm": 6.660101413726807,
      "learning_rate": 4.844358719731113e-05,
      "loss": 2.2371,
      "step": 16300
    },
    {
      "epoch": 1.252769077992514,
      "grad_norm": 7.958584785461426,
      "learning_rate": 4.843403865250936e-05,
      "loss": 2.2351,
      "step": 16400
    },
    {
      "epoch": 1.2604079138339317,
      "grad_norm": 8.549927711486816,
      "learning_rate": 4.842449010770759e-05,
      "loss": 2.1212,
      "step": 16500
    },
    {
      "epoch": 1.2680467496753494,
      "grad_norm": 7.2710089683532715,
      "learning_rate": 4.8414941562905816e-05,
      "loss": 2.1221,
      "step": 16600
    },
    {
      "epoch": 1.2756855855167673,
      "grad_norm": 5.3004231452941895,
      "learning_rate": 4.8405393018104045e-05,
      "loss": 2.1152,
      "step": 16700
    },
    {
      "epoch": 1.2833244213581851,
      "grad_norm": 6.430453300476074,
      "learning_rate": 4.8395844473302274e-05,
      "loss": 2.1212,
      "step": 16800
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 7.614453315734863,
      "learning_rate": 4.8386295928500496e-05,
      "loss": 2.1312,
      "step": 16900
    },
    {
      "epoch": 1.2986020930410205,
      "grad_norm": 7.870596885681152,
      "learning_rate": 4.837674738369873e-05,
      "loss": 2.0513,
      "step": 17000
    },
    {
      "epoch": 1.3062409288824384,
      "grad_norm": 6.774415016174316,
      "learning_rate": 4.836719883889695e-05,
      "loss": 2.1048,
      "step": 17100
    },
    {
      "epoch": 1.313879764723856,
      "grad_norm": 4.5582499504089355,
      "learning_rate": 4.835765029409518e-05,
      "loss": 2.0919,
      "step": 17200
    },
    {
      "epoch": 1.321518600565274,
      "grad_norm": 5.6097731590271,
      "learning_rate": 4.834810174929341e-05,
      "loss": 2.1085,
      "step": 17300
    },
    {
      "epoch": 1.3291574364066916,
      "grad_norm": 5.653775215148926,
      "learning_rate": 4.833855320449164e-05,
      "loss": 2.1336,
      "step": 17400
    },
    {
      "epoch": 1.3367962722481095,
      "grad_norm": 5.48486852645874,
      "learning_rate": 4.832900465968986e-05,
      "loss": 2.039,
      "step": 17500
    },
    {
      "epoch": 1.3444351080895272,
      "grad_norm": 4.896012783050537,
      "learning_rate": 4.8319456114888097e-05,
      "loss": 2.0115,
      "step": 17600
    },
    {
      "epoch": 1.3520739439309448,
      "grad_norm": 4.84825325012207,
      "learning_rate": 4.830990757008632e-05,
      "loss": 2.1481,
      "step": 17700
    },
    {
      "epoch": 1.3597127797723627,
      "grad_norm": 6.462705612182617,
      "learning_rate": 4.830035902528455e-05,
      "loss": 2.0996,
      "step": 17800
    },
    {
      "epoch": 1.3673516156137804,
      "grad_norm": 5.775611877441406,
      "learning_rate": 4.8290810480482776e-05,
      "loss": 2.1751,
      "step": 17900
    },
    {
      "epoch": 1.3749904514551983,
      "grad_norm": 6.040096282958984,
      "learning_rate": 4.8281261935681005e-05,
      "loss": 2.0795,
      "step": 18000
    },
    {
      "epoch": 1.382629287296616,
      "grad_norm": 14.455910682678223,
      "learning_rate": 4.827171339087923e-05,
      "loss": 2.2635,
      "step": 18100
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 7.291331768035889,
      "learning_rate": 4.826216484607746e-05,
      "loss": 2.1056,
      "step": 18200
    },
    {
      "epoch": 1.3979069589794515,
      "grad_norm": 6.325390815734863,
      "learning_rate": 4.825261630127569e-05,
      "loss": 2.1085,
      "step": 18300
    },
    {
      "epoch": 1.4055457948208692,
      "grad_norm": 6.30556058883667,
      "learning_rate": 4.824306775647391e-05,
      "loss": 2.0872,
      "step": 18400
    },
    {
      "epoch": 1.413184630662287,
      "grad_norm": 6.4961371421813965,
      "learning_rate": 4.823351921167215e-05,
      "loss": 2.1252,
      "step": 18500
    },
    {
      "epoch": 1.420823466503705,
      "grad_norm": 7.309135437011719,
      "learning_rate": 4.822397066687037e-05,
      "loss": 2.0696,
      "step": 18600
    },
    {
      "epoch": 1.4284623023451226,
      "grad_norm": 5.6706862449646,
      "learning_rate": 4.82144221220686e-05,
      "loss": 2.0141,
      "step": 18700
    },
    {
      "epoch": 1.4361011381865403,
      "grad_norm": 8.420165061950684,
      "learning_rate": 4.820487357726683e-05,
      "loss": 2.0232,
      "step": 18800
    },
    {
      "epoch": 1.4437399740279582,
      "grad_norm": 6.647212505340576,
      "learning_rate": 4.8195325032465056e-05,
      "loss": 2.0762,
      "step": 18900
    },
    {
      "epoch": 1.4513788098693758,
      "grad_norm": 6.73564338684082,
      "learning_rate": 4.818577648766328e-05,
      "loss": 2.0803,
      "step": 19000
    },
    {
      "epoch": 1.4590176457107937,
      "grad_norm": 6.335738658905029,
      "learning_rate": 4.817622794286151e-05,
      "loss": 2.1247,
      "step": 19100
    },
    {
      "epoch": 1.4666564815522114,
      "grad_norm": 6.929213523864746,
      "learning_rate": 4.8166679398059735e-05,
      "loss": 2.1158,
      "step": 19200
    },
    {
      "epoch": 1.4742953173936293,
      "grad_norm": 10.119876861572266,
      "learning_rate": 4.8157130853257964e-05,
      "loss": 2.0233,
      "step": 19300
    },
    {
      "epoch": 1.481934153235047,
      "grad_norm": 7.158876419067383,
      "learning_rate": 4.81475823084562e-05,
      "loss": 2.1422,
      "step": 19400
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 6.830562114715576,
      "learning_rate": 4.813803376365442e-05,
      "loss": 2.0661,
      "step": 19500
    },
    {
      "epoch": 1.4972118249178825,
      "grad_norm": 10.07793140411377,
      "learning_rate": 4.812848521885265e-05,
      "loss": 2.131,
      "step": 19600
    },
    {
      "epoch": 1.5048506607593004,
      "grad_norm": 8.671719551086426,
      "learning_rate": 4.811893667405088e-05,
      "loss": 1.9824,
      "step": 19700
    },
    {
      "epoch": 1.512489496600718,
      "grad_norm": 7.908980846405029,
      "learning_rate": 4.810938812924911e-05,
      "loss": 2.0756,
      "step": 19800
    },
    {
      "epoch": 1.5201283324421357,
      "grad_norm": 8.15880012512207,
      "learning_rate": 4.809983958444733e-05,
      "loss": 2.1846,
      "step": 19900
    },
    {
      "epoch": 1.5277671682835536,
      "grad_norm": 7.454707622528076,
      "learning_rate": 4.8090291039645565e-05,
      "loss": 2.12,
      "step": 20000
    },
    {
      "epoch": 1.5354060041249713,
      "grad_norm": 7.984645366668701,
      "learning_rate": 4.808074249484379e-05,
      "loss": 2.0787,
      "step": 20100
    },
    {
      "epoch": 1.543044839966389,
      "grad_norm": 6.085328102111816,
      "learning_rate": 4.8071193950042015e-05,
      "loss": 2.0866,
      "step": 20200
    },
    {
      "epoch": 1.5506836758078069,
      "grad_norm": 6.883598804473877,
      "learning_rate": 4.8061645405240244e-05,
      "loss": 2.0859,
      "step": 20300
    },
    {
      "epoch": 1.5583225116492248,
      "grad_norm": 4.631409645080566,
      "learning_rate": 4.805209686043847e-05,
      "loss": 2.0848,
      "step": 20400
    },
    {
      "epoch": 1.5659613474906424,
      "grad_norm": 6.208423614501953,
      "learning_rate": 4.80425483156367e-05,
      "loss": 2.0695,
      "step": 20500
    },
    {
      "epoch": 1.57360018333206,
      "grad_norm": 5.781252384185791,
      "learning_rate": 4.8032999770834923e-05,
      "loss": 2.0689,
      "step": 20600
    },
    {
      "epoch": 1.581239019173478,
      "grad_norm": 7.69540548324585,
      "learning_rate": 4.802345122603316e-05,
      "loss": 2.1134,
      "step": 20700
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 7.608880996704102,
      "learning_rate": 4.801390268123138e-05,
      "loss": 2.1149,
      "step": 20800
    },
    {
      "epoch": 1.5965166908563135,
      "grad_norm": 5.767796993255615,
      "learning_rate": 4.800435413642961e-05,
      "loss": 2.0865,
      "step": 20900
    },
    {
      "epoch": 1.6041555266977312,
      "grad_norm": 7.411304473876953,
      "learning_rate": 4.799480559162784e-05,
      "loss": 2.1134,
      "step": 21000
    },
    {
      "epoch": 1.611794362539149,
      "grad_norm": 9.366665840148926,
      "learning_rate": 4.798525704682607e-05,
      "loss": 2.2083,
      "step": 21100
    },
    {
      "epoch": 1.6194331983805668,
      "grad_norm": 5.280661582946777,
      "learning_rate": 4.797570850202429e-05,
      "loss": 2.1291,
      "step": 21200
    },
    {
      "epoch": 1.6270720342219844,
      "grad_norm": 7.239759922027588,
      "learning_rate": 4.7966159957222524e-05,
      "loss": 2.1111,
      "step": 21300
    },
    {
      "epoch": 1.6347108700634023,
      "grad_norm": 6.495144367218018,
      "learning_rate": 4.7956611412420746e-05,
      "loss": 2.1279,
      "step": 21400
    },
    {
      "epoch": 1.6423497059048202,
      "grad_norm": 11.616400718688965,
      "learning_rate": 4.7947062867618975e-05,
      "loss": 2.1289,
      "step": 21500
    },
    {
      "epoch": 1.6499885417462379,
      "grad_norm": 6.318342685699463,
      "learning_rate": 4.7937514322817204e-05,
      "loss": 2.0135,
      "step": 21600
    },
    {
      "epoch": 1.6576273775876555,
      "grad_norm": 6.691891193389893,
      "learning_rate": 4.792796577801543e-05,
      "loss": 2.1528,
      "step": 21700
    },
    {
      "epoch": 1.6652662134290734,
      "grad_norm": 6.510889530181885,
      "learning_rate": 4.791841723321366e-05,
      "loss": 2.0998,
      "step": 21800
    },
    {
      "epoch": 1.6729050492704913,
      "grad_norm": 6.448395252227783,
      "learning_rate": 4.790886868841189e-05,
      "loss": 2.0227,
      "step": 21900
    },
    {
      "epoch": 1.680543885111909,
      "grad_norm": 7.7097883224487305,
      "learning_rate": 4.789932014361012e-05,
      "loss": 2.1052,
      "step": 22000
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 7.259430408477783,
      "learning_rate": 4.788977159880834e-05,
      "loss": 1.9969,
      "step": 22100
    },
    {
      "epoch": 1.6958215567947446,
      "grad_norm": 5.595186710357666,
      "learning_rate": 4.7880223054006576e-05,
      "loss": 2.051,
      "step": 22200
    },
    {
      "epoch": 1.7034603926361622,
      "grad_norm": 7.063502788543701,
      "learning_rate": 4.78706745092048e-05,
      "loss": 2.0621,
      "step": 22300
    },
    {
      "epoch": 1.71109922847758,
      "grad_norm": 4.706174850463867,
      "learning_rate": 4.7861125964403026e-05,
      "loss": 2.0619,
      "step": 22400
    },
    {
      "epoch": 1.7187380643189978,
      "grad_norm": 6.189508438110352,
      "learning_rate": 4.7851577419601255e-05,
      "loss": 2.073,
      "step": 22500
    },
    {
      "epoch": 1.7263769001604157,
      "grad_norm": 5.720125198364258,
      "learning_rate": 4.7842028874799484e-05,
      "loss": 2.1133,
      "step": 22600
    },
    {
      "epoch": 1.7340157360018333,
      "grad_norm": 6.075953483581543,
      "learning_rate": 4.7832480329997706e-05,
      "loss": 2.1745,
      "step": 22700
    },
    {
      "epoch": 1.741654571843251,
      "grad_norm": 5.3307204246521,
      "learning_rate": 4.782293178519594e-05,
      "loss": 2.1429,
      "step": 22800
    },
    {
      "epoch": 1.749293407684669,
      "grad_norm": 7.866816997528076,
      "learning_rate": 4.781338324039416e-05,
      "loss": 2.0203,
      "step": 22900
    },
    {
      "epoch": 1.7569322435260868,
      "grad_norm": 6.780681610107422,
      "learning_rate": 4.780383469559239e-05,
      "loss": 2.0573,
      "step": 23000
    },
    {
      "epoch": 1.7645710793675042,
      "grad_norm": 19.883563995361328,
      "learning_rate": 4.779428615079062e-05,
      "loss": 2.0596,
      "step": 23100
    },
    {
      "epoch": 1.7722099152089221,
      "grad_norm": 6.18963098526001,
      "learning_rate": 4.778473760598885e-05,
      "loss": 2.0956,
      "step": 23200
    },
    {
      "epoch": 1.77984875105034,
      "grad_norm": 5.8509368896484375,
      "learning_rate": 4.777518906118708e-05,
      "loss": 2.1604,
      "step": 23300
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 7.531468868255615,
      "learning_rate": 4.7765640516385307e-05,
      "loss": 2.0937,
      "step": 23400
    },
    {
      "epoch": 1.7951264227331754,
      "grad_norm": 5.8480448722839355,
      "learning_rate": 4.7756091971583535e-05,
      "loss": 2.0255,
      "step": 23500
    },
    {
      "epoch": 1.8027652585745932,
      "grad_norm": 6.717202663421631,
      "learning_rate": 4.774654342678176e-05,
      "loss": 2.1257,
      "step": 23600
    },
    {
      "epoch": 1.8104040944160111,
      "grad_norm": 8.620172500610352,
      "learning_rate": 4.773699488197999e-05,
      "loss": 2.1146,
      "step": 23700
    },
    {
      "epoch": 1.8180429302574288,
      "grad_norm": 6.350916862487793,
      "learning_rate": 4.7727446337178215e-05,
      "loss": 2.1073,
      "step": 23800
    },
    {
      "epoch": 1.8256817660988465,
      "grad_norm": 7.68890380859375,
      "learning_rate": 4.771789779237644e-05,
      "loss": 2.045,
      "step": 23900
    },
    {
      "epoch": 1.8333206019402644,
      "grad_norm": 6.118442535400391,
      "learning_rate": 4.770834924757467e-05,
      "loss": 2.1102,
      "step": 24000
    },
    {
      "epoch": 1.840959437781682,
      "grad_norm": 7.501459121704102,
      "learning_rate": 4.76988007027729e-05,
      "loss": 2.003,
      "step": 24100
    },
    {
      "epoch": 1.8485982736230997,
      "grad_norm": 7.404692649841309,
      "learning_rate": 4.768925215797113e-05,
      "loss": 2.1705,
      "step": 24200
    },
    {
      "epoch": 1.8562371094645176,
      "grad_norm": 5.780813217163086,
      "learning_rate": 4.767970361316936e-05,
      "loss": 1.9208,
      "step": 24300
    },
    {
      "epoch": 1.8638759453059355,
      "grad_norm": 8.854307174682617,
      "learning_rate": 4.767015506836759e-05,
      "loss": 2.0863,
      "step": 24400
    },
    {
      "epoch": 1.8715147811473531,
      "grad_norm": 6.320497035980225,
      "learning_rate": 4.766060652356581e-05,
      "loss": 2.1524,
      "step": 24500
    },
    {
      "epoch": 1.8791536169887708,
      "grad_norm": 5.635225772857666,
      "learning_rate": 4.7651057978764044e-05,
      "loss": 2.1911,
      "step": 24600
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 9.815608024597168,
      "learning_rate": 4.7641509433962266e-05,
      "loss": 2.1289,
      "step": 24700
    },
    {
      "epoch": 1.8944312886716066,
      "grad_norm": 7.404934406280518,
      "learning_rate": 4.7631960889160495e-05,
      "loss": 2.0928,
      "step": 24800
    },
    {
      "epoch": 1.9020701245130243,
      "grad_norm": 7.061444282531738,
      "learning_rate": 4.762241234435872e-05,
      "loss": 2.0445,
      "step": 24900
    },
    {
      "epoch": 1.909708960354442,
      "grad_norm": 6.627874851226807,
      "learning_rate": 4.761286379955695e-05,
      "loss": 2.0147,
      "step": 25000
    },
    {
      "epoch": 1.9173477961958598,
      "grad_norm": 6.008119583129883,
      "learning_rate": 4.7603315254755174e-05,
      "loss": 2.0846,
      "step": 25100
    },
    {
      "epoch": 1.9249866320372775,
      "grad_norm": 7.064857482910156,
      "learning_rate": 4.759376670995341e-05,
      "loss": 2.1278,
      "step": 25200
    },
    {
      "epoch": 1.9326254678786952,
      "grad_norm": 5.387286186218262,
      "learning_rate": 4.758421816515163e-05,
      "loss": 2.0353,
      "step": 25300
    },
    {
      "epoch": 1.940264303720113,
      "grad_norm": 8.503556251525879,
      "learning_rate": 4.757466962034986e-05,
      "loss": 2.188,
      "step": 25400
    },
    {
      "epoch": 1.947903139561531,
      "grad_norm": 7.054375648498535,
      "learning_rate": 4.756512107554809e-05,
      "loss": 2.1458,
      "step": 25500
    },
    {
      "epoch": 1.9555419754029486,
      "grad_norm": 5.338288307189941,
      "learning_rate": 4.755557253074632e-05,
      "loss": 2.1058,
      "step": 25600
    },
    {
      "epoch": 1.9631808112443663,
      "grad_norm": 7.746931076049805,
      "learning_rate": 4.7546023985944546e-05,
      "loss": 2.0863,
      "step": 25700
    },
    {
      "epoch": 1.9708196470857842,
      "grad_norm": 6.400956630706787,
      "learning_rate": 4.7536475441142775e-05,
      "loss": 2.1427,
      "step": 25800
    },
    {
      "epoch": 1.978458482927202,
      "grad_norm": 10.486132621765137,
      "learning_rate": 4.7526926896341004e-05,
      "loss": 2.0076,
      "step": 25900
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 8.27220344543457,
      "learning_rate": 4.7517378351539225e-05,
      "loss": 2.0328,
      "step": 26000
    },
    {
      "epoch": 1.9937361546100374,
      "grad_norm": 5.7237935066223145,
      "learning_rate": 4.7507829806737454e-05,
      "loss": 2.2377,
      "step": 26100
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9962618350982666,
      "eval_runtime": 1.4704,
      "eval_samples_per_second": 469.274,
      "eval_steps_per_second": 469.274,
      "step": 26182
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8800349235534668,
      "eval_runtime": 29.0522,
      "eval_samples_per_second": 450.603,
      "eval_steps_per_second": 450.603,
      "step": 26182
    },
    {
      "epoch": 2.0013749904514553,
      "grad_norm": 6.817381858825684,
      "learning_rate": 4.749828126193568e-05,
      "loss": 2.0565,
      "step": 26200
    },
    {
      "epoch": 2.009013826292873,
      "grad_norm": 11.244606018066406,
      "learning_rate": 4.748873271713391e-05,
      "loss": 2.0057,
      "step": 26300
    },
    {
      "epoch": 2.0166526621342906,
      "grad_norm": 6.868297100067139,
      "learning_rate": 4.7479184172332133e-05,
      "loss": 2.0217,
      "step": 26400
    },
    {
      "epoch": 2.0242914979757085,
      "grad_norm": 6.660096645355225,
      "learning_rate": 4.746963562753037e-05,
      "loss": 1.9928,
      "step": 26500
    },
    {
      "epoch": 2.0319303338171264,
      "grad_norm": 6.91060209274292,
      "learning_rate": 4.746008708272859e-05,
      "loss": 1.9819,
      "step": 26600
    },
    {
      "epoch": 2.039569169658544,
      "grad_norm": 7.778163433074951,
      "learning_rate": 4.745053853792682e-05,
      "loss": 1.9089,
      "step": 26700
    },
    {
      "epoch": 2.0472080054999617,
      "grad_norm": 7.306168079376221,
      "learning_rate": 4.744098999312505e-05,
      "loss": 1.9492,
      "step": 26800
    },
    {
      "epoch": 2.0548468413413796,
      "grad_norm": 7.843794822692871,
      "learning_rate": 4.743144144832328e-05,
      "loss": 2.0876,
      "step": 26900
    },
    {
      "epoch": 2.0624856771827975,
      "grad_norm": 7.8681960105896,
      "learning_rate": 4.7421892903521506e-05,
      "loss": 2.0907,
      "step": 27000
    },
    {
      "epoch": 2.070124513024215,
      "grad_norm": 8.103500366210938,
      "learning_rate": 4.7412344358719734e-05,
      "loss": 1.9656,
      "step": 27100
    },
    {
      "epoch": 2.077763348865633,
      "grad_norm": 7.608842372894287,
      "learning_rate": 4.740279581391796e-05,
      "loss": 2.1192,
      "step": 27200
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 7.439151763916016,
      "learning_rate": 4.7393247269116185e-05,
      "loss": 2.0896,
      "step": 27300
    },
    {
      "epoch": 2.093041020548468,
      "grad_norm": 7.122698783874512,
      "learning_rate": 4.738369872431442e-05,
      "loss": 2.1007,
      "step": 27400
    },
    {
      "epoch": 2.100679856389886,
      "grad_norm": 8.457472801208496,
      "learning_rate": 4.737415017951264e-05,
      "loss": 2.0949,
      "step": 27500
    },
    {
      "epoch": 2.108318692231304,
      "grad_norm": 6.767539978027344,
      "learning_rate": 4.736460163471087e-05,
      "loss": 2.0088,
      "step": 27600
    },
    {
      "epoch": 2.115957528072722,
      "grad_norm": 10.456007957458496,
      "learning_rate": 4.73550530899091e-05,
      "loss": 2.1249,
      "step": 27700
    },
    {
      "epoch": 2.1235963639141393,
      "grad_norm": 6.959116458892822,
      "learning_rate": 4.734550454510733e-05,
      "loss": 2.0668,
      "step": 27800
    },
    {
      "epoch": 2.131235199755557,
      "grad_norm": 6.9348554611206055,
      "learning_rate": 4.733595600030555e-05,
      "loss": 2.0704,
      "step": 27900
    },
    {
      "epoch": 2.138874035596975,
      "grad_norm": 7.02557897567749,
      "learning_rate": 4.7326407455503786e-05,
      "loss": 1.9644,
      "step": 28000
    },
    {
      "epoch": 2.146512871438393,
      "grad_norm": 5.740081310272217,
      "learning_rate": 4.731685891070201e-05,
      "loss": 1.9997,
      "step": 28100
    },
    {
      "epoch": 2.1541517072798104,
      "grad_norm": 5.333535671234131,
      "learning_rate": 4.7307310365900236e-05,
      "loss": 2.034,
      "step": 28200
    },
    {
      "epoch": 2.1617905431212283,
      "grad_norm": 7.8875627517700195,
      "learning_rate": 4.729776182109847e-05,
      "loss": 2.064,
      "step": 28300
    },
    {
      "epoch": 2.169429378962646,
      "grad_norm": 6.151160717010498,
      "learning_rate": 4.7288213276296694e-05,
      "loss": 2.0566,
      "step": 28400
    },
    {
      "epoch": 2.1770682148040637,
      "grad_norm": 6.696822166442871,
      "learning_rate": 4.727866473149492e-05,
      "loss": 2.1381,
      "step": 28500
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 8.55803394317627,
      "learning_rate": 4.726911618669315e-05,
      "loss": 2.0781,
      "step": 28600
    },
    {
      "epoch": 2.1923458864868994,
      "grad_norm": 8.144116401672363,
      "learning_rate": 4.725956764189138e-05,
      "loss": 2.1067,
      "step": 28700
    },
    {
      "epoch": 2.1999847223283173,
      "grad_norm": 7.96404504776001,
      "learning_rate": 4.72500190970896e-05,
      "loss": 2.0211,
      "step": 28800
    },
    {
      "epoch": 2.2076235581697348,
      "grad_norm": 4.944112777709961,
      "learning_rate": 4.724047055228784e-05,
      "loss": 2.0507,
      "step": 28900
    },
    {
      "epoch": 2.2152623940111527,
      "grad_norm": 7.3640546798706055,
      "learning_rate": 4.723092200748606e-05,
      "loss": 2.0425,
      "step": 29000
    },
    {
      "epoch": 2.2229012298525705,
      "grad_norm": 4.651801586151123,
      "learning_rate": 4.722137346268429e-05,
      "loss": 2.0633,
      "step": 29100
    },
    {
      "epoch": 2.2305400656939884,
      "grad_norm": 6.244165420532227,
      "learning_rate": 4.7211824917882517e-05,
      "loss": 2.0743,
      "step": 29200
    },
    {
      "epoch": 2.238178901535406,
      "grad_norm": 5.743988990783691,
      "learning_rate": 4.7202276373080745e-05,
      "loss": 2.0717,
      "step": 29300
    },
    {
      "epoch": 2.2458177373768238,
      "grad_norm": 7.269573211669922,
      "learning_rate": 4.7192727828278974e-05,
      "loss": 2.023,
      "step": 29400
    },
    {
      "epoch": 2.2534565732182417,
      "grad_norm": 10.042598724365234,
      "learning_rate": 4.71831792834772e-05,
      "loss": 2.0277,
      "step": 29500
    },
    {
      "epoch": 2.261095409059659,
      "grad_norm": 5.945451736450195,
      "learning_rate": 4.717363073867543e-05,
      "loss": 2.0532,
      "step": 29600
    },
    {
      "epoch": 2.268734244901077,
      "grad_norm": 6.320197582244873,
      "learning_rate": 4.716408219387365e-05,
      "loss": 2.0466,
      "step": 29700
    },
    {
      "epoch": 2.276373080742495,
      "grad_norm": 7.087867259979248,
      "learning_rate": 4.715453364907189e-05,
      "loss": 2.1105,
      "step": 29800
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 7.793264865875244,
      "learning_rate": 4.714498510427011e-05,
      "loss": 2.0119,
      "step": 29900
    },
    {
      "epoch": 2.2916507524253302,
      "grad_norm": 6.077270984649658,
      "learning_rate": 4.713543655946834e-05,
      "loss": 1.9424,
      "step": 30000
    },
    {
      "epoch": 2.299289588266748,
      "grad_norm": 7.086963653564453,
      "learning_rate": 4.712588801466657e-05,
      "loss": 2.0196,
      "step": 30100
    },
    {
      "epoch": 2.306928424108166,
      "grad_norm": 10.96168041229248,
      "learning_rate": 4.71163394698648e-05,
      "loss": 2.0765,
      "step": 30200
    },
    {
      "epoch": 2.314567259949584,
      "grad_norm": 5.819302558898926,
      "learning_rate": 4.710679092506302e-05,
      "loss": 2.0435,
      "step": 30300
    },
    {
      "epoch": 2.3222060957910013,
      "grad_norm": 8.288297653198242,
      "learning_rate": 4.7097242380261254e-05,
      "loss": 2.0802,
      "step": 30400
    },
    {
      "epoch": 2.3298449316324192,
      "grad_norm": 7.493959426879883,
      "learning_rate": 4.7087693835459476e-05,
      "loss": 2.0668,
      "step": 30500
    },
    {
      "epoch": 2.337483767473837,
      "grad_norm": 6.819565296173096,
      "learning_rate": 4.7078145290657705e-05,
      "loss": 1.9919,
      "step": 30600
    },
    {
      "epoch": 2.3451226033152546,
      "grad_norm": 6.398709774017334,
      "learning_rate": 4.706859674585593e-05,
      "loss": 2.0856,
      "step": 30700
    },
    {
      "epoch": 2.3527614391566725,
      "grad_norm": 6.601033687591553,
      "learning_rate": 4.705904820105416e-05,
      "loss": 2.1292,
      "step": 30800
    },
    {
      "epoch": 2.3604002749980904,
      "grad_norm": 6.466694355010986,
      "learning_rate": 4.704949965625239e-05,
      "loss": 1.986,
      "step": 30900
    },
    {
      "epoch": 2.3680391108395082,
      "grad_norm": 6.904707431793213,
      "learning_rate": 4.703995111145062e-05,
      "loss": 1.9932,
      "step": 31000
    },
    {
      "epoch": 2.3756779466809257,
      "grad_norm": 7.548303604125977,
      "learning_rate": 4.703040256664885e-05,
      "loss": 2.0323,
      "step": 31100
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 6.41606330871582,
      "learning_rate": 4.702085402184707e-05,
      "loss": 2.0982,
      "step": 31200
    },
    {
      "epoch": 2.3909556183637615,
      "grad_norm": 6.027804374694824,
      "learning_rate": 4.7011305477045306e-05,
      "loss": 2.037,
      "step": 31300
    },
    {
      "epoch": 2.3985944542051794,
      "grad_norm": 6.4340386390686035,
      "learning_rate": 4.700175693224353e-05,
      "loss": 2.0111,
      "step": 31400
    },
    {
      "epoch": 2.406233290046597,
      "grad_norm": 8.73943042755127,
      "learning_rate": 4.6992208387441756e-05,
      "loss": 2.0277,
      "step": 31500
    },
    {
      "epoch": 2.4138721258880147,
      "grad_norm": 7.977520942687988,
      "learning_rate": 4.6982659842639985e-05,
      "loss": 2.0634,
      "step": 31600
    },
    {
      "epoch": 2.4215109617294326,
      "grad_norm": 7.668363571166992,
      "learning_rate": 4.6973111297838214e-05,
      "loss": 2.0443,
      "step": 31700
    },
    {
      "epoch": 2.42914979757085,
      "grad_norm": 6.828366756439209,
      "learning_rate": 4.6963562753036435e-05,
      "loss": 2.0871,
      "step": 31800
    },
    {
      "epoch": 2.436788633412268,
      "grad_norm": 5.7837371826171875,
      "learning_rate": 4.6954014208234664e-05,
      "loss": 2.0751,
      "step": 31900
    },
    {
      "epoch": 2.444427469253686,
      "grad_norm": 7.245515823364258,
      "learning_rate": 4.694446566343289e-05,
      "loss": 2.1526,
      "step": 32000
    },
    {
      "epoch": 2.4520663050951033,
      "grad_norm": 7.257945537567139,
      "learning_rate": 4.693491711863112e-05,
      "loss": 1.9555,
      "step": 32100
    },
    {
      "epoch": 2.459705140936521,
      "grad_norm": 6.512086868286133,
      "learning_rate": 4.692536857382935e-05,
      "loss": 2.1093,
      "step": 32200
    },
    {
      "epoch": 2.467343976777939,
      "grad_norm": 6.712020397186279,
      "learning_rate": 4.691582002902758e-05,
      "loss": 2.0778,
      "step": 32300
    },
    {
      "epoch": 2.474982812619357,
      "grad_norm": 9.774744033813477,
      "learning_rate": 4.690627148422581e-05,
      "loss": 1.9949,
      "step": 32400
    },
    {
      "epoch": 2.482621648460775,
      "grad_norm": 5.743819236755371,
      "learning_rate": 4.689672293942403e-05,
      "loss": 2.0392,
      "step": 32500
    },
    {
      "epoch": 2.4902604843021923,
      "grad_norm": 8.905407905578613,
      "learning_rate": 4.6887174394622265e-05,
      "loss": 2.0175,
      "step": 32600
    },
    {
      "epoch": 2.49789932014361,
      "grad_norm": 6.232547760009766,
      "learning_rate": 4.687762584982049e-05,
      "loss": 2.075,
      "step": 32700
    },
    {
      "epoch": 2.505538155985028,
      "grad_norm": 6.273828983306885,
      "learning_rate": 4.6868077305018716e-05,
      "loss": 2.0673,
      "step": 32800
    },
    {
      "epoch": 2.5131769918264455,
      "grad_norm": 5.690840721130371,
      "learning_rate": 4.6858528760216944e-05,
      "loss": 2.0156,
      "step": 32900
    },
    {
      "epoch": 2.5208158276678634,
      "grad_norm": 7.847567081451416,
      "learning_rate": 4.684898021541517e-05,
      "loss": 2.0722,
      "step": 33000
    },
    {
      "epoch": 2.5284546635092813,
      "grad_norm": 5.735627174377441,
      "learning_rate": 4.68394316706134e-05,
      "loss": 1.9621,
      "step": 33100
    },
    {
      "epoch": 2.5360934993506987,
      "grad_norm": 7.62235689163208,
      "learning_rate": 4.682988312581163e-05,
      "loss": 1.9697,
      "step": 33200
    },
    {
      "epoch": 2.5437323351921166,
      "grad_norm": 7.807041645050049,
      "learning_rate": 4.682033458100986e-05,
      "loss": 2.0385,
      "step": 33300
    },
    {
      "epoch": 2.5513711710335345,
      "grad_norm": 6.06990909576416,
      "learning_rate": 4.681078603620808e-05,
      "loss": 2.0684,
      "step": 33400
    },
    {
      "epoch": 2.5590100068749524,
      "grad_norm": 7.017930507659912,
      "learning_rate": 4.6801237491406316e-05,
      "loss": 2.0018,
      "step": 33500
    },
    {
      "epoch": 2.5666488427163703,
      "grad_norm": 6.334123611450195,
      "learning_rate": 4.679168894660454e-05,
      "loss": 2.1273,
      "step": 33600
    },
    {
      "epoch": 2.5742876785577877,
      "grad_norm": 5.8035101890563965,
      "learning_rate": 4.678214040180277e-05,
      "loss": 2.0262,
      "step": 33700
    },
    {
      "epoch": 2.5819265143992056,
      "grad_norm": 8.67058277130127,
      "learning_rate": 4.6772591857000996e-05,
      "loss": 2.05,
      "step": 33800
    },
    {
      "epoch": 2.5895653502406235,
      "grad_norm": 6.027514457702637,
      "learning_rate": 4.6763043312199224e-05,
      "loss": 2.0558,
      "step": 33900
    },
    {
      "epoch": 2.597204186082041,
      "grad_norm": 8.157918930053711,
      "learning_rate": 4.6753494767397446e-05,
      "loss": 2.0364,
      "step": 34000
    },
    {
      "epoch": 2.604843021923459,
      "grad_norm": 6.71254301071167,
      "learning_rate": 4.674394622259568e-05,
      "loss": 2.071,
      "step": 34100
    },
    {
      "epoch": 2.6124818577648767,
      "grad_norm": 6.104252338409424,
      "learning_rate": 4.6734397677793904e-05,
      "loss": 1.9971,
      "step": 34200
    },
    {
      "epoch": 2.620120693606294,
      "grad_norm": 6.495115756988525,
      "learning_rate": 4.672484913299213e-05,
      "loss": 2.017,
      "step": 34300
    },
    {
      "epoch": 2.627759529447712,
      "grad_norm": 9.145675659179688,
      "learning_rate": 4.671530058819036e-05,
      "loss": 1.9045,
      "step": 34400
    },
    {
      "epoch": 2.63539836528913,
      "grad_norm": 5.56348180770874,
      "learning_rate": 4.670575204338859e-05,
      "loss": 1.975,
      "step": 34500
    },
    {
      "epoch": 2.643037201130548,
      "grad_norm": 8.201665878295898,
      "learning_rate": 4.669620349858682e-05,
      "loss": 2.0265,
      "step": 34600
    },
    {
      "epoch": 2.6506760369719657,
      "grad_norm": 7.426966667175293,
      "learning_rate": 4.668665495378505e-05,
      "loss": 1.9824,
      "step": 34700
    },
    {
      "epoch": 2.658314872813383,
      "grad_norm": 6.087449550628662,
      "learning_rate": 4.6677106408983276e-05,
      "loss": 2.0742,
      "step": 34800
    },
    {
      "epoch": 2.665953708654801,
      "grad_norm": 8.503381729125977,
      "learning_rate": 4.66675578641815e-05,
      "loss": 2.1246,
      "step": 34900
    },
    {
      "epoch": 2.673592544496219,
      "grad_norm": 6.27324104309082,
      "learning_rate": 4.665800931937973e-05,
      "loss": 2.1401,
      "step": 35000
    },
    {
      "epoch": 2.6812313803376364,
      "grad_norm": 6.7396087646484375,
      "learning_rate": 4.6648460774577955e-05,
      "loss": 2.1122,
      "step": 35100
    },
    {
      "epoch": 2.6888702161790543,
      "grad_norm": 5.610233306884766,
      "learning_rate": 4.6638912229776184e-05,
      "loss": 2.043,
      "step": 35200
    },
    {
      "epoch": 2.696509052020472,
      "grad_norm": 7.961142539978027,
      "learning_rate": 4.662936368497441e-05,
      "loss": 2.0619,
      "step": 35300
    },
    {
      "epoch": 2.7041478878618896,
      "grad_norm": 7.008105278015137,
      "learning_rate": 4.661981514017264e-05,
      "loss": 1.9604,
      "step": 35400
    },
    {
      "epoch": 2.7117867237033075,
      "grad_norm": 5.190394401550293,
      "learning_rate": 4.661026659537086e-05,
      "loss": 2.1064,
      "step": 35500
    },
    {
      "epoch": 2.7194255595447254,
      "grad_norm": 6.54360294342041,
      "learning_rate": 4.66007180505691e-05,
      "loss": 2.1196,
      "step": 35600
    },
    {
      "epoch": 2.7270643953861433,
      "grad_norm": 6.34268856048584,
      "learning_rate": 4.659116950576732e-05,
      "loss": 2.027,
      "step": 35700
    },
    {
      "epoch": 2.7347032312275608,
      "grad_norm": 5.506364822387695,
      "learning_rate": 4.658162096096555e-05,
      "loss": 2.0029,
      "step": 35800
    },
    {
      "epoch": 2.7423420670689787,
      "grad_norm": 6.506877899169922,
      "learning_rate": 4.657207241616378e-05,
      "loss": 2.0305,
      "step": 35900
    },
    {
      "epoch": 2.7499809029103965,
      "grad_norm": 3.9788358211517334,
      "learning_rate": 4.656252387136201e-05,
      "loss": 1.9248,
      "step": 36000
    },
    {
      "epoch": 2.7576197387518144,
      "grad_norm": 7.015749931335449,
      "learning_rate": 4.6552975326560235e-05,
      "loss": 1.9649,
      "step": 36100
    },
    {
      "epoch": 2.765258574593232,
      "grad_norm": 6.213406562805176,
      "learning_rate": 4.6543426781758464e-05,
      "loss": 1.9604,
      "step": 36200
    },
    {
      "epoch": 2.7728974104346498,
      "grad_norm": 7.5010199546813965,
      "learning_rate": 4.653387823695669e-05,
      "loss": 2.01,
      "step": 36300
    },
    {
      "epoch": 2.7805362462760677,
      "grad_norm": 10.815936088562012,
      "learning_rate": 4.6524329692154915e-05,
      "loss": 2.0721,
      "step": 36400
    },
    {
      "epoch": 2.788175082117485,
      "grad_norm": 8.307320594787598,
      "learning_rate": 4.651478114735315e-05,
      "loss": 2.0448,
      "step": 36500
    },
    {
      "epoch": 2.795813917958903,
      "grad_norm": 6.190130233764648,
      "learning_rate": 4.650523260255137e-05,
      "loss": 2.0245,
      "step": 36600
    },
    {
      "epoch": 2.803452753800321,
      "grad_norm": 6.052091121673584,
      "learning_rate": 4.64956840577496e-05,
      "loss": 2.0836,
      "step": 36700
    },
    {
      "epoch": 2.8110915896417383,
      "grad_norm": 7.273357391357422,
      "learning_rate": 4.648613551294783e-05,
      "loss": 2.0031,
      "step": 36800
    },
    {
      "epoch": 2.818730425483156,
      "grad_norm": 7.748610019683838,
      "learning_rate": 4.647658696814606e-05,
      "loss": 1.9415,
      "step": 36900
    },
    {
      "epoch": 2.826369261324574,
      "grad_norm": 7.363180637359619,
      "learning_rate": 4.646703842334429e-05,
      "loss": 2.042,
      "step": 37000
    },
    {
      "epoch": 2.834008097165992,
      "grad_norm": 6.2733564376831055,
      "learning_rate": 4.6457489878542516e-05,
      "loss": 1.9737,
      "step": 37100
    },
    {
      "epoch": 2.84164693300741,
      "grad_norm": 8.402475357055664,
      "learning_rate": 4.6447941333740744e-05,
      "loss": 2.0863,
      "step": 37200
    },
    {
      "epoch": 2.8492857688488273,
      "grad_norm": 7.812938690185547,
      "learning_rate": 4.6438392788938966e-05,
      "loss": 2.0119,
      "step": 37300
    },
    {
      "epoch": 2.8569246046902452,
      "grad_norm": 7.788450717926025,
      "learning_rate": 4.64288442441372e-05,
      "loss": 2.1179,
      "step": 37400
    },
    {
      "epoch": 2.864563440531663,
      "grad_norm": 6.900868892669678,
      "learning_rate": 4.6419295699335424e-05,
      "loss": 1.9481,
      "step": 37500
    },
    {
      "epoch": 2.8722022763730806,
      "grad_norm": 5.8211259841918945,
      "learning_rate": 4.640974715453365e-05,
      "loss": 2.1289,
      "step": 37600
    },
    {
      "epoch": 2.8798411122144985,
      "grad_norm": 5.377330780029297,
      "learning_rate": 4.6400198609731874e-05,
      "loss": 2.0466,
      "step": 37700
    },
    {
      "epoch": 2.8874799480559163,
      "grad_norm": 6.862639427185059,
      "learning_rate": 4.639065006493011e-05,
      "loss": 2.0121,
      "step": 37800
    },
    {
      "epoch": 2.895118783897334,
      "grad_norm": 5.8920698165893555,
      "learning_rate": 4.638110152012833e-05,
      "loss": 1.9999,
      "step": 37900
    },
    {
      "epoch": 2.9027576197387517,
      "grad_norm": 7.047788143157959,
      "learning_rate": 4.637155297532656e-05,
      "loss": 1.9514,
      "step": 38000
    },
    {
      "epoch": 2.9103964555801696,
      "grad_norm": 6.60552453994751,
      "learning_rate": 4.636200443052479e-05,
      "loss": 2.0771,
      "step": 38100
    },
    {
      "epoch": 2.9180352914215875,
      "grad_norm": 6.756577491760254,
      "learning_rate": 4.635245588572302e-05,
      "loss": 2.1688,
      "step": 38200
    },
    {
      "epoch": 2.9256741272630054,
      "grad_norm": 6.406771659851074,
      "learning_rate": 4.6342907340921246e-05,
      "loss": 2.0141,
      "step": 38300
    },
    {
      "epoch": 2.933312963104423,
      "grad_norm": 6.147496700286865,
      "learning_rate": 4.6333358796119475e-05,
      "loss": 2.037,
      "step": 38400
    },
    {
      "epoch": 2.9409517989458407,
      "grad_norm": 6.549181938171387,
      "learning_rate": 4.6323810251317704e-05,
      "loss": 2.103,
      "step": 38500
    },
    {
      "epoch": 2.9485906347872586,
      "grad_norm": 8.879324913024902,
      "learning_rate": 4.6314261706515926e-05,
      "loss": 2.0206,
      "step": 38600
    },
    {
      "epoch": 2.956229470628676,
      "grad_norm": 6.552047252655029,
      "learning_rate": 4.630471316171416e-05,
      "loss": 1.9887,
      "step": 38700
    },
    {
      "epoch": 2.963868306470094,
      "grad_norm": 6.169914722442627,
      "learning_rate": 4.629516461691238e-05,
      "loss": 1.9505,
      "step": 38800
    },
    {
      "epoch": 2.971507142311512,
      "grad_norm": 6.05596399307251,
      "learning_rate": 4.628561607211061e-05,
      "loss": 2.1698,
      "step": 38900
    },
    {
      "epoch": 2.9791459781529293,
      "grad_norm": 6.614627361297607,
      "learning_rate": 4.627606752730884e-05,
      "loss": 2.1571,
      "step": 39000
    },
    {
      "epoch": 2.986784813994347,
      "grad_norm": 8.500330924987793,
      "learning_rate": 4.626651898250707e-05,
      "loss": 1.9714,
      "step": 39100
    },
    {
      "epoch": 2.994423649835765,
      "grad_norm": 6.359987735748291,
      "learning_rate": 4.625697043770529e-05,
      "loss": 2.0664,
      "step": 39200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.956020712852478,
      "eval_runtime": 3.0419,
      "eval_samples_per_second": 226.832,
      "eval_steps_per_second": 226.832,
      "step": 39273
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.8215460777282715,
      "eval_runtime": 57.9188,
      "eval_samples_per_second": 226.023,
      "eval_steps_per_second": 226.023,
      "step": 39273
    },
    {
      "epoch": 3.002062485677183,
      "grad_norm": 8.853819847106934,
      "learning_rate": 4.6247421892903526e-05,
      "loss": 2.0702,
      "step": 39300
    },
    {
      "epoch": 3.0097013215186004,
      "grad_norm": 6.595559120178223,
      "learning_rate": 4.623787334810175e-05,
      "loss": 1.905,
      "step": 39400
    },
    {
      "epoch": 3.0173401573600183,
      "grad_norm": 10.330395698547363,
      "learning_rate": 4.622832480329998e-05,
      "loss": 1.9888,
      "step": 39500
    },
    {
      "epoch": 3.024978993201436,
      "grad_norm": 6.6225762367248535,
      "learning_rate": 4.6218776258498206e-05,
      "loss": 1.9949,
      "step": 39600
    },
    {
      "epoch": 3.032617829042854,
      "grad_norm": 14.128105163574219,
      "learning_rate": 4.6209227713696434e-05,
      "loss": 2.0285,
      "step": 39700
    },
    {
      "epoch": 3.0402566648842715,
      "grad_norm": 6.09885311126709,
      "learning_rate": 4.619967916889466e-05,
      "loss": 1.9186,
      "step": 39800
    },
    {
      "epoch": 3.0478955007256894,
      "grad_norm": 5.306074619293213,
      "learning_rate": 4.619013062409289e-05,
      "loss": 1.9025,
      "step": 39900
    },
    {
      "epoch": 3.0555343365671073,
      "grad_norm": 8.875261306762695,
      "learning_rate": 4.618058207929112e-05,
      "loss": 1.9754,
      "step": 40000
    },
    {
      "epoch": 3.0631731724085247,
      "grad_norm": 8.786972045898438,
      "learning_rate": 4.617103353448934e-05,
      "loss": 2.0414,
      "step": 40100
    },
    {
      "epoch": 3.0708120082499426,
      "grad_norm": 5.99552059173584,
      "learning_rate": 4.616148498968758e-05,
      "loss": 2.1144,
      "step": 40200
    },
    {
      "epoch": 3.0784508440913605,
      "grad_norm": 6.164271354675293,
      "learning_rate": 4.61519364448858e-05,
      "loss": 1.9699,
      "step": 40300
    },
    {
      "epoch": 3.0860896799327784,
      "grad_norm": 10.463454246520996,
      "learning_rate": 4.614238790008403e-05,
      "loss": 2.1251,
      "step": 40400
    },
    {
      "epoch": 3.093728515774196,
      "grad_norm": 6.676143646240234,
      "learning_rate": 4.613283935528226e-05,
      "loss": 1.9814,
      "step": 40500
    },
    {
      "epoch": 3.1013673516156137,
      "grad_norm": 6.554495334625244,
      "learning_rate": 4.6123290810480486e-05,
      "loss": 2.048,
      "step": 40600
    },
    {
      "epoch": 3.1090061874570316,
      "grad_norm": 8.24526596069336,
      "learning_rate": 4.611374226567871e-05,
      "loss": 2.0579,
      "step": 40700
    },
    {
      "epoch": 3.1166450232984495,
      "grad_norm": 6.204075813293457,
      "learning_rate": 4.610419372087694e-05,
      "loss": 1.9798,
      "step": 40800
    },
    {
      "epoch": 3.124283859139867,
      "grad_norm": 10.288970947265625,
      "learning_rate": 4.6094645176075165e-05,
      "loss": 2.0361,
      "step": 40900
    },
    {
      "epoch": 3.131922694981285,
      "grad_norm": 4.832223892211914,
      "learning_rate": 4.6085096631273394e-05,
      "loss": 1.9967,
      "step": 41000
    },
    {
      "epoch": 3.1395615308227027,
      "grad_norm": 9.446552276611328,
      "learning_rate": 4.607554808647163e-05,
      "loss": 1.9676,
      "step": 41100
    },
    {
      "epoch": 3.14720036666412,
      "grad_norm": 6.816980361938477,
      "learning_rate": 4.606599954166985e-05,
      "loss": 2.0608,
      "step": 41200
    },
    {
      "epoch": 3.154839202505538,
      "grad_norm": 7.757654190063477,
      "learning_rate": 4.605645099686808e-05,
      "loss": 2.0318,
      "step": 41300
    },
    {
      "epoch": 3.162478038346956,
      "grad_norm": 7.175195693969727,
      "learning_rate": 4.604690245206631e-05,
      "loss": 2.0196,
      "step": 41400
    },
    {
      "epoch": 3.170116874188374,
      "grad_norm": 5.186682224273682,
      "learning_rate": 4.603735390726454e-05,
      "loss": 1.9421,
      "step": 41500
    },
    {
      "epoch": 3.1777557100297913,
      "grad_norm": 7.818681716918945,
      "learning_rate": 4.602780536246276e-05,
      "loss": 1.9723,
      "step": 41600
    },
    {
      "epoch": 3.185394545871209,
      "grad_norm": 6.980311870574951,
      "learning_rate": 4.6018256817660995e-05,
      "loss": 1.9165,
      "step": 41700
    },
    {
      "epoch": 3.193033381712627,
      "grad_norm": 7.583540439605713,
      "learning_rate": 4.600870827285922e-05,
      "loss": 1.9788,
      "step": 41800
    },
    {
      "epoch": 3.200672217554045,
      "grad_norm": 7.46795129776001,
      "learning_rate": 4.5999159728057445e-05,
      "loss": 1.9436,
      "step": 41900
    },
    {
      "epoch": 3.2083110533954624,
      "grad_norm": 7.82221794128418,
      "learning_rate": 4.5989611183255674e-05,
      "loss": 2.0255,
      "step": 42000
    },
    {
      "epoch": 3.2159498892368803,
      "grad_norm": 9.661761283874512,
      "learning_rate": 4.59800626384539e-05,
      "loss": 1.8896,
      "step": 42100
    },
    {
      "epoch": 3.223588725078298,
      "grad_norm": 6.426016330718994,
      "learning_rate": 4.597051409365213e-05,
      "loss": 2.0628,
      "step": 42200
    },
    {
      "epoch": 3.2312275609197156,
      "grad_norm": 7.0409626960754395,
      "learning_rate": 4.596096554885036e-05,
      "loss": 2.0107,
      "step": 42300
    },
    {
      "epoch": 3.2388663967611335,
      "grad_norm": 5.736508369445801,
      "learning_rate": 4.595141700404859e-05,
      "loss": 2.0262,
      "step": 42400
    },
    {
      "epoch": 3.2465052326025514,
      "grad_norm": 6.778249740600586,
      "learning_rate": 4.594186845924681e-05,
      "loss": 2.0228,
      "step": 42500
    },
    {
      "epoch": 3.2541440684439693,
      "grad_norm": 8.188097953796387,
      "learning_rate": 4.5932319914445046e-05,
      "loss": 1.9206,
      "step": 42600
    },
    {
      "epoch": 3.2617829042853868,
      "grad_norm": 6.598724842071533,
      "learning_rate": 4.592277136964327e-05,
      "loss": 2.0492,
      "step": 42700
    },
    {
      "epoch": 3.2694217401268046,
      "grad_norm": 8.242944717407227,
      "learning_rate": 4.59132228248415e-05,
      "loss": 1.982,
      "step": 42800
    },
    {
      "epoch": 3.2770605759682225,
      "grad_norm": 8.155143737792969,
      "learning_rate": 4.5903674280039726e-05,
      "loss": 1.9606,
      "step": 42900
    },
    {
      "epoch": 3.2846994118096404,
      "grad_norm": 8.229649543762207,
      "learning_rate": 4.5894125735237954e-05,
      "loss": 2.0188,
      "step": 43000
    },
    {
      "epoch": 3.292338247651058,
      "grad_norm": 7.207655906677246,
      "learning_rate": 4.5884577190436176e-05,
      "loss": 1.9789,
      "step": 43100
    },
    {
      "epoch": 3.2999770834924758,
      "grad_norm": 8.212202072143555,
      "learning_rate": 4.587502864563441e-05,
      "loss": 2.0081,
      "step": 43200
    },
    {
      "epoch": 3.3076159193338937,
      "grad_norm": 6.047013282775879,
      "learning_rate": 4.5865480100832634e-05,
      "loss": 2.0135,
      "step": 43300
    },
    {
      "epoch": 3.315254755175311,
      "grad_norm": 5.854637622833252,
      "learning_rate": 4.585593155603086e-05,
      "loss": 2.0698,
      "step": 43400
    },
    {
      "epoch": 3.322893591016729,
      "grad_norm": 5.609196186065674,
      "learning_rate": 4.584638301122909e-05,
      "loss": 1.949,
      "step": 43500
    },
    {
      "epoch": 3.330532426858147,
      "grad_norm": 8.607077598571777,
      "learning_rate": 4.583683446642732e-05,
      "loss": 2.0034,
      "step": 43600
    },
    {
      "epoch": 3.3381712626995648,
      "grad_norm": 7.13531494140625,
      "learning_rate": 4.582728592162555e-05,
      "loss": 1.9281,
      "step": 43700
    },
    {
      "epoch": 3.345810098540982,
      "grad_norm": 5.353226661682129,
      "learning_rate": 4.581773737682377e-05,
      "loss": 2.0378,
      "step": 43800
    },
    {
      "epoch": 3.3534489343824,
      "grad_norm": 6.871962070465088,
      "learning_rate": 4.5808188832022006e-05,
      "loss": 2.0381,
      "step": 43900
    },
    {
      "epoch": 3.361087770223818,
      "grad_norm": 7.070187091827393,
      "learning_rate": 4.579864028722023e-05,
      "loss": 2.0414,
      "step": 44000
    },
    {
      "epoch": 3.368726606065236,
      "grad_norm": 5.815462589263916,
      "learning_rate": 4.5789091742418456e-05,
      "loss": 1.846,
      "step": 44100
    },
    {
      "epoch": 3.3763654419066533,
      "grad_norm": 6.808154582977295,
      "learning_rate": 4.5779543197616685e-05,
      "loss": 1.9798,
      "step": 44200
    },
    {
      "epoch": 3.384004277748071,
      "grad_norm": 7.205514430999756,
      "learning_rate": 4.5769994652814914e-05,
      "loss": 2.0217,
      "step": 44300
    },
    {
      "epoch": 3.391643113589489,
      "grad_norm": 6.275455474853516,
      "learning_rate": 4.5760446108013136e-05,
      "loss": 2.0749,
      "step": 44400
    },
    {
      "epoch": 3.3992819494309066,
      "grad_norm": 7.210625648498535,
      "learning_rate": 4.575089756321137e-05,
      "loss": 2.0981,
      "step": 44500
    },
    {
      "epoch": 3.4069207852723244,
      "grad_norm": 7.109739780426025,
      "learning_rate": 4.574134901840959e-05,
      "loss": 2.0452,
      "step": 44600
    },
    {
      "epoch": 3.4145596211137423,
      "grad_norm": 6.4921979904174805,
      "learning_rate": 4.573180047360782e-05,
      "loss": 2.0038,
      "step": 44700
    },
    {
      "epoch": 3.42219845695516,
      "grad_norm": 6.5360822677612305,
      "learning_rate": 4.572225192880605e-05,
      "loss": 2.0164,
      "step": 44800
    },
    {
      "epoch": 3.4298372927965777,
      "grad_norm": 6.190916538238525,
      "learning_rate": 4.571270338400428e-05,
      "loss": 2.0037,
      "step": 44900
    },
    {
      "epoch": 3.4374761286379956,
      "grad_norm": 7.206357002258301,
      "learning_rate": 4.570315483920251e-05,
      "loss": 2.0718,
      "step": 45000
    },
    {
      "epoch": 3.4451149644794135,
      "grad_norm": 8.121214866638184,
      "learning_rate": 4.5693606294400736e-05,
      "loss": 2.0669,
      "step": 45100
    },
    {
      "epoch": 3.4527538003208313,
      "grad_norm": 6.4986724853515625,
      "learning_rate": 4.5684057749598965e-05,
      "loss": 2.0822,
      "step": 45200
    },
    {
      "epoch": 3.460392636162249,
      "grad_norm": 5.510618686676025,
      "learning_rate": 4.567450920479719e-05,
      "loss": 1.9723,
      "step": 45300
    },
    {
      "epoch": 3.4680314720036667,
      "grad_norm": 7.590890407562256,
      "learning_rate": 4.566496065999542e-05,
      "loss": 2.0038,
      "step": 45400
    },
    {
      "epoch": 3.4756703078450846,
      "grad_norm": 6.417936325073242,
      "learning_rate": 4.5655412115193644e-05,
      "loss": 2.0518,
      "step": 45500
    },
    {
      "epoch": 3.483309143686502,
      "grad_norm": 5.924548149108887,
      "learning_rate": 4.564586357039187e-05,
      "loss": 2.0806,
      "step": 45600
    },
    {
      "epoch": 3.49094797952792,
      "grad_norm": 6.933248043060303,
      "learning_rate": 4.56363150255901e-05,
      "loss": 2.0207,
      "step": 45700
    },
    {
      "epoch": 3.498586815369338,
      "grad_norm": 9.148240089416504,
      "learning_rate": 4.562676648078833e-05,
      "loss": 1.9876,
      "step": 45800
    },
    {
      "epoch": 3.5062256512107552,
      "grad_norm": 8.503438949584961,
      "learning_rate": 4.561721793598656e-05,
      "loss": 1.8333,
      "step": 45900
    },
    {
      "epoch": 3.513864487052173,
      "grad_norm": 7.609984874725342,
      "learning_rate": 4.560766939118479e-05,
      "loss": 1.9453,
      "step": 46000
    },
    {
      "epoch": 3.521503322893591,
      "grad_norm": 8.995497703552246,
      "learning_rate": 4.5598120846383017e-05,
      "loss": 1.9898,
      "step": 46100
    },
    {
      "epoch": 3.529142158735009,
      "grad_norm": 8.925196647644043,
      "learning_rate": 4.558857230158124e-05,
      "loss": 2.0588,
      "step": 46200
    },
    {
      "epoch": 3.536780994576427,
      "grad_norm": 7.179499626159668,
      "learning_rate": 4.5579023756779474e-05,
      "loss": 2.0441,
      "step": 46300
    },
    {
      "epoch": 3.5444198304178443,
      "grad_norm": 5.773114204406738,
      "learning_rate": 4.5569475211977696e-05,
      "loss": 2.0275,
      "step": 46400
    },
    {
      "epoch": 3.552058666259262,
      "grad_norm": 5.931734561920166,
      "learning_rate": 4.5559926667175925e-05,
      "loss": 1.8768,
      "step": 46500
    },
    {
      "epoch": 3.55969750210068,
      "grad_norm": 5.921706676483154,
      "learning_rate": 4.555037812237415e-05,
      "loss": 1.9509,
      "step": 46600
    },
    {
      "epoch": 3.5673363379420975,
      "grad_norm": 7.752471923828125,
      "learning_rate": 4.554082957757238e-05,
      "loss": 1.9046,
      "step": 46700
    },
    {
      "epoch": 3.5749751737835154,
      "grad_norm": 9.368300437927246,
      "learning_rate": 4.5531281032770604e-05,
      "loss": 1.9905,
      "step": 46800
    },
    {
      "epoch": 3.5826140096249333,
      "grad_norm": 6.952995300292969,
      "learning_rate": 4.552173248796884e-05,
      "loss": 2.0278,
      "step": 46900
    },
    {
      "epoch": 3.5902528454663507,
      "grad_norm": 6.365713119506836,
      "learning_rate": 4.551218394316706e-05,
      "loss": 2.0738,
      "step": 47000
    },
    {
      "epoch": 3.5978916813077686,
      "grad_norm": 6.764621734619141,
      "learning_rate": 4.550263539836529e-05,
      "loss": 1.9885,
      "step": 47100
    },
    {
      "epoch": 3.6055305171491865,
      "grad_norm": 7.812902450561523,
      "learning_rate": 4.549308685356352e-05,
      "loss": 1.9548,
      "step": 47200
    },
    {
      "epoch": 3.6131693529906044,
      "grad_norm": 7.405207633972168,
      "learning_rate": 4.548353830876175e-05,
      "loss": 2.0384,
      "step": 47300
    },
    {
      "epoch": 3.6208081888320223,
      "grad_norm": 8.965970039367676,
      "learning_rate": 4.5473989763959976e-05,
      "loss": 1.9923,
      "step": 47400
    },
    {
      "epoch": 3.6284470246734397,
      "grad_norm": 6.6861443519592285,
      "learning_rate": 4.5464441219158205e-05,
      "loss": 1.9756,
      "step": 47500
    },
    {
      "epoch": 3.6360858605148576,
      "grad_norm": 5.839308261871338,
      "learning_rate": 4.5454892674356433e-05,
      "loss": 2.0055,
      "step": 47600
    },
    {
      "epoch": 3.6437246963562755,
      "grad_norm": 9.701184272766113,
      "learning_rate": 4.5445344129554655e-05,
      "loss": 1.9728,
      "step": 47700
    },
    {
      "epoch": 3.651363532197693,
      "grad_norm": 6.794645309448242,
      "learning_rate": 4.543579558475289e-05,
      "loss": 2.0321,
      "step": 47800
    },
    {
      "epoch": 3.659002368039111,
      "grad_norm": 8.940417289733887,
      "learning_rate": 4.542624703995111e-05,
      "loss": 2.0372,
      "step": 47900
    },
    {
      "epoch": 3.6666412038805287,
      "grad_norm": 6.787924766540527,
      "learning_rate": 4.541669849514934e-05,
      "loss": 2.0457,
      "step": 48000
    },
    {
      "epoch": 3.674280039721946,
      "grad_norm": 8.246941566467285,
      "learning_rate": 4.540714995034757e-05,
      "loss": 2.0214,
      "step": 48100
    },
    {
      "epoch": 3.681918875563364,
      "grad_norm": 6.680868625640869,
      "learning_rate": 4.53976014055458e-05,
      "loss": 2.1036,
      "step": 48200
    },
    {
      "epoch": 3.689557711404782,
      "grad_norm": 6.381515979766846,
      "learning_rate": 4.538805286074402e-05,
      "loss": 1.9878,
      "step": 48300
    },
    {
      "epoch": 3.6971965472462,
      "grad_norm": 7.496274948120117,
      "learning_rate": 4.5378504315942256e-05,
      "loss": 2.0583,
      "step": 48400
    },
    {
      "epoch": 3.7048353830876173,
      "grad_norm": 6.436630725860596,
      "learning_rate": 4.536895577114048e-05,
      "loss": 2.0585,
      "step": 48500
    },
    {
      "epoch": 3.712474218929035,
      "grad_norm": 6.811107158660889,
      "learning_rate": 4.535940722633871e-05,
      "loss": 1.9973,
      "step": 48600
    },
    {
      "epoch": 3.720113054770453,
      "grad_norm": 5.0561628341674805,
      "learning_rate": 4.5349858681536936e-05,
      "loss": 2.0431,
      "step": 48700
    },
    {
      "epoch": 3.727751890611871,
      "grad_norm": 6.530940055847168,
      "learning_rate": 4.5340310136735164e-05,
      "loss": 1.9765,
      "step": 48800
    },
    {
      "epoch": 3.7353907264532884,
      "grad_norm": 6.262913703918457,
      "learning_rate": 4.533076159193339e-05,
      "loss": 1.9986,
      "step": 48900
    },
    {
      "epoch": 3.7430295622947063,
      "grad_norm": 5.263329029083252,
      "learning_rate": 4.532121304713162e-05,
      "loss": 2.0563,
      "step": 49000
    },
    {
      "epoch": 3.750668398136124,
      "grad_norm": 6.313994884490967,
      "learning_rate": 4.531166450232985e-05,
      "loss": 1.9602,
      "step": 49100
    },
    {
      "epoch": 3.7583072339775416,
      "grad_norm": 6.686409950256348,
      "learning_rate": 4.530211595752807e-05,
      "loss": 1.9809,
      "step": 49200
    },
    {
      "epoch": 3.7659460698189595,
      "grad_norm": 7.2685675621032715,
      "learning_rate": 4.52925674127263e-05,
      "loss": 2.0372,
      "step": 49300
    },
    {
      "epoch": 3.7735849056603774,
      "grad_norm": 6.320578575134277,
      "learning_rate": 4.528301886792453e-05,
      "loss": 1.9472,
      "step": 49400
    },
    {
      "epoch": 3.781223741501795,
      "grad_norm": 8.661364555358887,
      "learning_rate": 4.527347032312276e-05,
      "loss": 1.9197,
      "step": 49500
    },
    {
      "epoch": 3.7888625773432127,
      "grad_norm": 8.701038360595703,
      "learning_rate": 4.526392177832098e-05,
      "loss": 2.0922,
      "step": 49600
    },
    {
      "epoch": 3.7965014131846306,
      "grad_norm": 6.471027374267578,
      "learning_rate": 4.5254373233519216e-05,
      "loss": 2.0278,
      "step": 49700
    },
    {
      "epoch": 3.8041402490260485,
      "grad_norm": 8.41858959197998,
      "learning_rate": 4.524482468871744e-05,
      "loss": 1.9727,
      "step": 49800
    },
    {
      "epoch": 3.8117790848674664,
      "grad_norm": 6.340753078460693,
      "learning_rate": 4.5235276143915666e-05,
      "loss": 1.9682,
      "step": 49900
    },
    {
      "epoch": 3.819417920708884,
      "grad_norm": 6.557938575744629,
      "learning_rate": 4.52257275991139e-05,
      "loss": 1.9616,
      "step": 50000
    },
    {
      "epoch": 3.8270567565503018,
      "grad_norm": 5.470424652099609,
      "learning_rate": 4.5216179054312124e-05,
      "loss": 1.9374,
      "step": 50100
    },
    {
      "epoch": 3.8346955923917196,
      "grad_norm": 8.79299545288086,
      "learning_rate": 4.520663050951035e-05,
      "loss": 1.9392,
      "step": 50200
    },
    {
      "epoch": 3.842334428233137,
      "grad_norm": 5.646761417388916,
      "learning_rate": 4.519708196470858e-05,
      "loss": 1.9746,
      "step": 50300
    },
    {
      "epoch": 3.849973264074555,
      "grad_norm": 3.9076032638549805,
      "learning_rate": 4.518753341990681e-05,
      "loss": 1.9359,
      "step": 50400
    },
    {
      "epoch": 3.857612099915973,
      "grad_norm": 5.63897180557251,
      "learning_rate": 4.517798487510503e-05,
      "loss": 1.986,
      "step": 50500
    },
    {
      "epoch": 3.8652509357573903,
      "grad_norm": 5.407123565673828,
      "learning_rate": 4.516843633030327e-05,
      "loss": 1.9624,
      "step": 50600
    },
    {
      "epoch": 3.872889771598808,
      "grad_norm": 6.732994556427002,
      "learning_rate": 4.515888778550149e-05,
      "loss": 2.0008,
      "step": 50700
    },
    {
      "epoch": 3.880528607440226,
      "grad_norm": 7.200456619262695,
      "learning_rate": 4.514933924069972e-05,
      "loss": 2.0571,
      "step": 50800
    },
    {
      "epoch": 3.888167443281644,
      "grad_norm": 6.585333824157715,
      "learning_rate": 4.5139790695897946e-05,
      "loss": 2.0315,
      "step": 50900
    },
    {
      "epoch": 3.895806279123062,
      "grad_norm": 5.874913215637207,
      "learning_rate": 4.5130242151096175e-05,
      "loss": 2.0467,
      "step": 51000
    },
    {
      "epoch": 3.9034451149644793,
      "grad_norm": 8.3264741897583,
      "learning_rate": 4.5120693606294404e-05,
      "loss": 1.9699,
      "step": 51100
    },
    {
      "epoch": 3.911083950805897,
      "grad_norm": 6.979541778564453,
      "learning_rate": 4.511114506149263e-05,
      "loss": 1.9673,
      "step": 51200
    },
    {
      "epoch": 3.918722786647315,
      "grad_norm": 4.919912338256836,
      "learning_rate": 4.510159651669086e-05,
      "loss": 2.0659,
      "step": 51300
    },
    {
      "epoch": 3.9263616224887326,
      "grad_norm": 6.604058265686035,
      "learning_rate": 4.509204797188908e-05,
      "loss": 1.9504,
      "step": 51400
    },
    {
      "epoch": 3.9340004583301504,
      "grad_norm": 7.700245380401611,
      "learning_rate": 4.508249942708732e-05,
      "loss": 2.1178,
      "step": 51500
    },
    {
      "epoch": 3.9416392941715683,
      "grad_norm": 8.08391284942627,
      "learning_rate": 4.507295088228554e-05,
      "loss": 1.9923,
      "step": 51600
    },
    {
      "epoch": 3.9492781300129858,
      "grad_norm": 6.759174346923828,
      "learning_rate": 4.506340233748377e-05,
      "loss": 1.8691,
      "step": 51700
    },
    {
      "epoch": 3.9569169658544037,
      "grad_norm": 5.603715896606445,
      "learning_rate": 4.5053853792682e-05,
      "loss": 1.9299,
      "step": 51800
    },
    {
      "epoch": 3.9645558016958216,
      "grad_norm": 6.781336784362793,
      "learning_rate": 4.5044305247880227e-05,
      "loss": 2.0232,
      "step": 51900
    },
    {
      "epoch": 3.9721946375372394,
      "grad_norm": 6.775801181793213,
      "learning_rate": 4.503475670307845e-05,
      "loss": 2.0433,
      "step": 52000
    },
    {
      "epoch": 3.9798334733786573,
      "grad_norm": 8.976642608642578,
      "learning_rate": 4.5025208158276684e-05,
      "loss": 2.0242,
      "step": 52100
    },
    {
      "epoch": 3.987472309220075,
      "grad_norm": 5.605884075164795,
      "learning_rate": 4.5015659613474906e-05,
      "loss": 1.9461,
      "step": 52200
    },
    {
      "epoch": 3.9951111450614927,
      "grad_norm": 5.7515997886657715,
      "learning_rate": 4.5006111068673135e-05,
      "loss": 2.0327,
      "step": 52300
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9326626062393188,
      "eval_runtime": 3.0245,
      "eval_samples_per_second": 228.14,
      "eval_steps_per_second": 228.14,
      "step": 52364
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.7839194536209106,
      "eval_runtime": 57.129,
      "eval_samples_per_second": 229.148,
      "eval_steps_per_second": 229.148,
      "step": 52364
    },
    {
      "epoch": 4.002749980902911,
      "grad_norm": 8.508038520812988,
      "learning_rate": 4.499656252387136e-05,
      "loss": 1.9709,
      "step": 52400
    },
    {
      "epoch": 4.010388816744328,
      "grad_norm": 7.96737813949585,
      "learning_rate": 4.498701397906959e-05,
      "loss": 2.0317,
      "step": 52500
    },
    {
      "epoch": 4.018027652585746,
      "grad_norm": 7.937137603759766,
      "learning_rate": 4.497746543426782e-05,
      "loss": 1.9303,
      "step": 52600
    },
    {
      "epoch": 4.025666488427164,
      "grad_norm": 6.5024285316467285,
      "learning_rate": 4.496791688946605e-05,
      "loss": 1.9336,
      "step": 52700
    },
    {
      "epoch": 4.033305324268581,
      "grad_norm": 6.824344158172607,
      "learning_rate": 4.495836834466428e-05,
      "loss": 1.9143,
      "step": 52800
    },
    {
      "epoch": 4.04094416011,
      "grad_norm": 6.322193622589111,
      "learning_rate": 4.49488197998625e-05,
      "loss": 1.9843,
      "step": 52900
    },
    {
      "epoch": 4.048582995951417,
      "grad_norm": 7.656571388244629,
      "learning_rate": 4.4939271255060735e-05,
      "loss": 1.9486,
      "step": 53000
    },
    {
      "epoch": 4.0562218317928345,
      "grad_norm": 5.755255699157715,
      "learning_rate": 4.492972271025896e-05,
      "loss": 1.9418,
      "step": 53100
    },
    {
      "epoch": 4.063860667634253,
      "grad_norm": 8.343432426452637,
      "learning_rate": 4.4920174165457186e-05,
      "loss": 1.9642,
      "step": 53200
    },
    {
      "epoch": 4.07149950347567,
      "grad_norm": 6.103137493133545,
      "learning_rate": 4.4910625620655415e-05,
      "loss": 2.0084,
      "step": 53300
    },
    {
      "epoch": 4.079138339317088,
      "grad_norm": 8.019643783569336,
      "learning_rate": 4.4901077075853643e-05,
      "loss": 2.0208,
      "step": 53400
    },
    {
      "epoch": 4.086777175158506,
      "grad_norm": 8.313331604003906,
      "learning_rate": 4.4891528531051865e-05,
      "loss": 2.0144,
      "step": 53500
    },
    {
      "epoch": 4.0944160109999235,
      "grad_norm": 8.188743591308594,
      "learning_rate": 4.48819799862501e-05,
      "loss": 1.9031,
      "step": 53600
    },
    {
      "epoch": 4.102054846841342,
      "grad_norm": 7.807158946990967,
      "learning_rate": 4.487243144144832e-05,
      "loss": 2.0103,
      "step": 53700
    },
    {
      "epoch": 4.109693682682759,
      "grad_norm": 5.625879764556885,
      "learning_rate": 4.486288289664655e-05,
      "loss": 1.9777,
      "step": 53800
    },
    {
      "epoch": 4.117332518524177,
      "grad_norm": 7.48794412612915,
      "learning_rate": 4.485333435184478e-05,
      "loss": 2.0394,
      "step": 53900
    },
    {
      "epoch": 4.124971354365595,
      "grad_norm": 7.894604682922363,
      "learning_rate": 4.484378580704301e-05,
      "loss": 2.0643,
      "step": 54000
    },
    {
      "epoch": 4.1326101902070125,
      "grad_norm": 6.035438060760498,
      "learning_rate": 4.483423726224124e-05,
      "loss": 1.9581,
      "step": 54100
    },
    {
      "epoch": 4.14024902604843,
      "grad_norm": 9.646893501281738,
      "learning_rate": 4.4824688717439466e-05,
      "loss": 1.9861,
      "step": 54200
    },
    {
      "epoch": 4.147887861889848,
      "grad_norm": 6.093780040740967,
      "learning_rate": 4.4815140172637695e-05,
      "loss": 1.9112,
      "step": 54300
    },
    {
      "epoch": 4.155526697731266,
      "grad_norm": 8.302281379699707,
      "learning_rate": 4.480559162783592e-05,
      "loss": 1.9369,
      "step": 54400
    },
    {
      "epoch": 4.163165533572683,
      "grad_norm": 7.694951057434082,
      "learning_rate": 4.479604308303415e-05,
      "loss": 1.8962,
      "step": 54500
    },
    {
      "epoch": 4.1708043694141015,
      "grad_norm": 5.155496120452881,
      "learning_rate": 4.4786494538232374e-05,
      "loss": 1.9833,
      "step": 54600
    },
    {
      "epoch": 4.178443205255519,
      "grad_norm": 5.507457733154297,
      "learning_rate": 4.47769459934306e-05,
      "loss": 1.8555,
      "step": 54700
    },
    {
      "epoch": 4.186082041096936,
      "grad_norm": 6.579160690307617,
      "learning_rate": 4.476739744862883e-05,
      "loss": 1.953,
      "step": 54800
    },
    {
      "epoch": 4.193720876938355,
      "grad_norm": 6.491966724395752,
      "learning_rate": 4.475784890382706e-05,
      "loss": 1.9537,
      "step": 54900
    },
    {
      "epoch": 4.201359712779772,
      "grad_norm": 6.084700584411621,
      "learning_rate": 4.474830035902529e-05,
      "loss": 1.9398,
      "step": 55000
    },
    {
      "epoch": 4.2089985486211905,
      "grad_norm": 7.608360767364502,
      "learning_rate": 4.473875181422351e-05,
      "loss": 2.0359,
      "step": 55100
    },
    {
      "epoch": 4.216637384462608,
      "grad_norm": 7.041507244110107,
      "learning_rate": 4.4729203269421746e-05,
      "loss": 1.9514,
      "step": 55200
    },
    {
      "epoch": 4.224276220304025,
      "grad_norm": 8.634286880493164,
      "learning_rate": 4.471965472461997e-05,
      "loss": 1.9751,
      "step": 55300
    },
    {
      "epoch": 4.231915056145444,
      "grad_norm": 5.8987717628479,
      "learning_rate": 4.47101061798182e-05,
      "loss": 2.0173,
      "step": 55400
    },
    {
      "epoch": 4.239553891986861,
      "grad_norm": 5.461936950683594,
      "learning_rate": 4.4700557635016426e-05,
      "loss": 2.0241,
      "step": 55500
    },
    {
      "epoch": 4.247192727828279,
      "grad_norm": 7.499683856964111,
      "learning_rate": 4.4691009090214654e-05,
      "loss": 2.0103,
      "step": 55600
    },
    {
      "epoch": 4.254831563669697,
      "grad_norm": 7.233046054840088,
      "learning_rate": 4.4681460545412876e-05,
      "loss": 1.8797,
      "step": 55700
    },
    {
      "epoch": 4.262470399511114,
      "grad_norm": 6.561880588531494,
      "learning_rate": 4.467191200061111e-05,
      "loss": 1.9274,
      "step": 55800
    },
    {
      "epoch": 4.270109235352532,
      "grad_norm": 6.521469593048096,
      "learning_rate": 4.4662363455809334e-05,
      "loss": 2.0291,
      "step": 55900
    },
    {
      "epoch": 4.27774807119395,
      "grad_norm": 7.503207683563232,
      "learning_rate": 4.465281491100756e-05,
      "loss": 1.9189,
      "step": 56000
    },
    {
      "epoch": 4.285386907035368,
      "grad_norm": 9.410048484802246,
      "learning_rate": 4.464326636620579e-05,
      "loss": 2.0294,
      "step": 56100
    },
    {
      "epoch": 4.293025742876786,
      "grad_norm": 7.940431594848633,
      "learning_rate": 4.463371782140402e-05,
      "loss": 2.0973,
      "step": 56200
    },
    {
      "epoch": 4.300664578718203,
      "grad_norm": 4.950938701629639,
      "learning_rate": 4.462416927660225e-05,
      "loss": 2.0314,
      "step": 56300
    },
    {
      "epoch": 4.308303414559621,
      "grad_norm": 8.065248489379883,
      "learning_rate": 4.461462073180048e-05,
      "loss": 1.9599,
      "step": 56400
    },
    {
      "epoch": 4.315942250401039,
      "grad_norm": 7.789547920227051,
      "learning_rate": 4.4605072186998706e-05,
      "loss": 2.0135,
      "step": 56500
    },
    {
      "epoch": 4.323581086242457,
      "grad_norm": 7.07938814163208,
      "learning_rate": 4.459552364219693e-05,
      "loss": 1.8887,
      "step": 56600
    },
    {
      "epoch": 4.331219922083874,
      "grad_norm": 7.223280429840088,
      "learning_rate": 4.458597509739516e-05,
      "loss": 1.9328,
      "step": 56700
    },
    {
      "epoch": 4.338858757925292,
      "grad_norm": 5.282668590545654,
      "learning_rate": 4.4576426552593385e-05,
      "loss": 1.8914,
      "step": 56800
    },
    {
      "epoch": 4.34649759376671,
      "grad_norm": 7.545406818389893,
      "learning_rate": 4.4566878007791614e-05,
      "loss": 2.0297,
      "step": 56900
    },
    {
      "epoch": 4.354136429608127,
      "grad_norm": 7.173372268676758,
      "learning_rate": 4.455732946298984e-05,
      "loss": 2.035,
      "step": 57000
    },
    {
      "epoch": 4.361775265449546,
      "grad_norm": 6.260436058044434,
      "learning_rate": 4.454778091818807e-05,
      "loss": 2.0087,
      "step": 57100
    },
    {
      "epoch": 4.369414101290963,
      "grad_norm": 10.63586139678955,
      "learning_rate": 4.453823237338629e-05,
      "loss": 2.0108,
      "step": 57200
    },
    {
      "epoch": 4.377052937132381,
      "grad_norm": 6.055496692657471,
      "learning_rate": 4.452868382858453e-05,
      "loss": 1.9937,
      "step": 57300
    },
    {
      "epoch": 4.384691772973799,
      "grad_norm": 4.529940605163574,
      "learning_rate": 4.451913528378275e-05,
      "loss": 1.9762,
      "step": 57400
    },
    {
      "epoch": 4.392330608815216,
      "grad_norm": 5.336458683013916,
      "learning_rate": 4.450958673898098e-05,
      "loss": 1.9803,
      "step": 57500
    },
    {
      "epoch": 4.399969444656635,
      "grad_norm": 7.936870098114014,
      "learning_rate": 4.450003819417921e-05,
      "loss": 2.0048,
      "step": 57600
    },
    {
      "epoch": 4.407608280498052,
      "grad_norm": 6.1727800369262695,
      "learning_rate": 4.4490489649377437e-05,
      "loss": 1.9583,
      "step": 57700
    },
    {
      "epoch": 4.4152471163394695,
      "grad_norm": 6.8504791259765625,
      "learning_rate": 4.4480941104575665e-05,
      "loss": 1.9156,
      "step": 57800
    },
    {
      "epoch": 4.422885952180888,
      "grad_norm": 5.553173542022705,
      "learning_rate": 4.4471392559773894e-05,
      "loss": 2.1044,
      "step": 57900
    },
    {
      "epoch": 4.430524788022305,
      "grad_norm": 8.633173942565918,
      "learning_rate": 4.446184401497212e-05,
      "loss": 1.9696,
      "step": 58000
    },
    {
      "epoch": 4.438163623863723,
      "grad_norm": 9.197064399719238,
      "learning_rate": 4.4452295470170345e-05,
      "loss": 2.009,
      "step": 58100
    },
    {
      "epoch": 4.445802459705141,
      "grad_norm": 7.6563262939453125,
      "learning_rate": 4.444274692536858e-05,
      "loss": 1.9462,
      "step": 58200
    },
    {
      "epoch": 4.4534412955465585,
      "grad_norm": 7.038527011871338,
      "learning_rate": 4.44331983805668e-05,
      "loss": 2.0086,
      "step": 58300
    },
    {
      "epoch": 4.461080131387977,
      "grad_norm": 6.402144908905029,
      "learning_rate": 4.442364983576503e-05,
      "loss": 1.926,
      "step": 58400
    },
    {
      "epoch": 4.468718967229394,
      "grad_norm": 9.302424430847168,
      "learning_rate": 4.441410129096326e-05,
      "loss": 1.8922,
      "step": 58500
    },
    {
      "epoch": 4.476357803070812,
      "grad_norm": 7.2428998947143555,
      "learning_rate": 4.440455274616149e-05,
      "loss": 1.9853,
      "step": 58600
    },
    {
      "epoch": 4.48399663891223,
      "grad_norm": 6.064920425415039,
      "learning_rate": 4.439500420135972e-05,
      "loss": 1.793,
      "step": 58700
    },
    {
      "epoch": 4.4916354747536476,
      "grad_norm": 12.604310989379883,
      "learning_rate": 4.4385455656557945e-05,
      "loss": 1.9379,
      "step": 58800
    },
    {
      "epoch": 4.499274310595065,
      "grad_norm": 8.235838890075684,
      "learning_rate": 4.4375907111756174e-05,
      "loss": 1.9827,
      "step": 58900
    },
    {
      "epoch": 4.506913146436483,
      "grad_norm": 7.30864953994751,
      "learning_rate": 4.4366358566954396e-05,
      "loss": 2.014,
      "step": 59000
    },
    {
      "epoch": 4.514551982277901,
      "grad_norm": 5.918211936950684,
      "learning_rate": 4.435681002215263e-05,
      "loss": 1.9833,
      "step": 59100
    },
    {
      "epoch": 4.522190818119318,
      "grad_norm": 8.649335861206055,
      "learning_rate": 4.4347261477350853e-05,
      "loss": 1.9271,
      "step": 59200
    },
    {
      "epoch": 4.529829653960737,
      "grad_norm": 6.060311317443848,
      "learning_rate": 4.433771293254908e-05,
      "loss": 1.9613,
      "step": 59300
    },
    {
      "epoch": 4.537468489802154,
      "grad_norm": 6.388965129852295,
      "learning_rate": 4.432816438774731e-05,
      "loss": 1.9895,
      "step": 59400
    },
    {
      "epoch": 4.545107325643572,
      "grad_norm": 6.666771411895752,
      "learning_rate": 4.431861584294554e-05,
      "loss": 1.9075,
      "step": 59500
    },
    {
      "epoch": 4.55274616148499,
      "grad_norm": 7.97987174987793,
      "learning_rate": 4.430906729814376e-05,
      "loss": 1.9604,
      "step": 59600
    },
    {
      "epoch": 4.560384997326407,
      "grad_norm": 9.024439811706543,
      "learning_rate": 4.4299518753342e-05,
      "loss": 1.9002,
      "step": 59700
    },
    {
      "epoch": 4.568023833167826,
      "grad_norm": 7.852283477783203,
      "learning_rate": 4.428997020854022e-05,
      "loss": 2.0282,
      "step": 59800
    },
    {
      "epoch": 4.575662669009243,
      "grad_norm": 8.69587230682373,
      "learning_rate": 4.428042166373845e-05,
      "loss": 1.9062,
      "step": 59900
    },
    {
      "epoch": 4.5833015048506605,
      "grad_norm": 8.097671508789062,
      "learning_rate": 4.4270873118936676e-05,
      "loss": 2.0076,
      "step": 60000
    },
    {
      "epoch": 4.590940340692079,
      "grad_norm": 7.062027454376221,
      "learning_rate": 4.4261324574134905e-05,
      "loss": 2.0645,
      "step": 60100
    },
    {
      "epoch": 4.598579176533496,
      "grad_norm": 5.7104573249816895,
      "learning_rate": 4.4251776029333134e-05,
      "loss": 1.8537,
      "step": 60200
    },
    {
      "epoch": 4.606218012374914,
      "grad_norm": 7.345786094665527,
      "learning_rate": 4.424222748453136e-05,
      "loss": 1.8794,
      "step": 60300
    },
    {
      "epoch": 4.613856848216332,
      "grad_norm": 7.426344871520996,
      "learning_rate": 4.423267893972959e-05,
      "loss": 1.9451,
      "step": 60400
    },
    {
      "epoch": 4.6214956840577495,
      "grad_norm": 6.954622745513916,
      "learning_rate": 4.422313039492781e-05,
      "loss": 1.9907,
      "step": 60500
    },
    {
      "epoch": 4.629134519899168,
      "grad_norm": 5.775232315063477,
      "learning_rate": 4.421358185012604e-05,
      "loss": 1.9152,
      "step": 60600
    },
    {
      "epoch": 4.636773355740585,
      "grad_norm": 6.703962802886963,
      "learning_rate": 4.420403330532427e-05,
      "loss": 1.8844,
      "step": 60700
    },
    {
      "epoch": 4.644412191582003,
      "grad_norm": 9.11629581451416,
      "learning_rate": 4.41944847605225e-05,
      "loss": 1.8694,
      "step": 60800
    },
    {
      "epoch": 4.652051027423421,
      "grad_norm": 6.8822736740112305,
      "learning_rate": 4.418493621572072e-05,
      "loss": 1.9916,
      "step": 60900
    },
    {
      "epoch": 4.6596898632648385,
      "grad_norm": 5.972846984863281,
      "learning_rate": 4.4175387670918956e-05,
      "loss": 2.0354,
      "step": 61000
    },
    {
      "epoch": 4.667328699106256,
      "grad_norm": 7.193078517913818,
      "learning_rate": 4.416583912611718e-05,
      "loss": 2.04,
      "step": 61100
    },
    {
      "epoch": 4.674967534947674,
      "grad_norm": 5.817370414733887,
      "learning_rate": 4.415629058131541e-05,
      "loss": 1.92,
      "step": 61200
    },
    {
      "epoch": 4.682606370789092,
      "grad_norm": 5.4264702796936035,
      "learning_rate": 4.4146742036513636e-05,
      "loss": 2.0153,
      "step": 61300
    },
    {
      "epoch": 4.690245206630509,
      "grad_norm": 6.6571736335754395,
      "learning_rate": 4.4137193491711864e-05,
      "loss": 1.9897,
      "step": 61400
    },
    {
      "epoch": 4.6978840424719275,
      "grad_norm": 4.65815544128418,
      "learning_rate": 4.412764494691009e-05,
      "loss": 1.8937,
      "step": 61500
    },
    {
      "epoch": 4.705522878313345,
      "grad_norm": 10.356531143188477,
      "learning_rate": 4.411809640210832e-05,
      "loss": 1.953,
      "step": 61600
    },
    {
      "epoch": 4.713161714154763,
      "grad_norm": 7.291105270385742,
      "learning_rate": 4.410854785730655e-05,
      "loss": 1.9523,
      "step": 61700
    },
    {
      "epoch": 4.720800549996181,
      "grad_norm": 6.386364936828613,
      "learning_rate": 4.409899931250477e-05,
      "loss": 1.9086,
      "step": 61800
    },
    {
      "epoch": 4.728439385837598,
      "grad_norm": 7.79782247543335,
      "learning_rate": 4.408945076770301e-05,
      "loss": 1.9587,
      "step": 61900
    },
    {
      "epoch": 4.7360782216790165,
      "grad_norm": 9.829187393188477,
      "learning_rate": 4.407990222290123e-05,
      "loss": 1.9727,
      "step": 62000
    },
    {
      "epoch": 4.743717057520434,
      "grad_norm": 8.782186508178711,
      "learning_rate": 4.407035367809946e-05,
      "loss": 1.9872,
      "step": 62100
    },
    {
      "epoch": 4.751355893361851,
      "grad_norm": 6.709308624267578,
      "learning_rate": 4.406080513329769e-05,
      "loss": 1.9239,
      "step": 62200
    },
    {
      "epoch": 4.75899472920327,
      "grad_norm": 7.0496954917907715,
      "learning_rate": 4.4051256588495916e-05,
      "loss": 2.0348,
      "step": 62300
    },
    {
      "epoch": 4.766633565044687,
      "grad_norm": 6.455063343048096,
      "learning_rate": 4.404170804369414e-05,
      "loss": 1.9052,
      "step": 62400
    },
    {
      "epoch": 4.774272400886105,
      "grad_norm": 5.545165061950684,
      "learning_rate": 4.403215949889237e-05,
      "loss": 1.9463,
      "step": 62500
    },
    {
      "epoch": 4.781911236727523,
      "grad_norm": 6.495011329650879,
      "learning_rate": 4.4022610954090595e-05,
      "loss": 1.9441,
      "step": 62600
    },
    {
      "epoch": 4.78955007256894,
      "grad_norm": 6.53316593170166,
      "learning_rate": 4.4013062409288824e-05,
      "loss": 1.8408,
      "step": 62700
    },
    {
      "epoch": 4.797188908410359,
      "grad_norm": 8.846343040466309,
      "learning_rate": 4.400351386448706e-05,
      "loss": 1.9356,
      "step": 62800
    },
    {
      "epoch": 4.804827744251776,
      "grad_norm": 5.349152088165283,
      "learning_rate": 4.399396531968528e-05,
      "loss": 1.8432,
      "step": 62900
    },
    {
      "epoch": 4.812466580093194,
      "grad_norm": 4.980449199676514,
      "learning_rate": 4.398441677488351e-05,
      "loss": 2.0562,
      "step": 63000
    },
    {
      "epoch": 4.820105415934612,
      "grad_norm": 6.319592475891113,
      "learning_rate": 4.397486823008174e-05,
      "loss": 2.0765,
      "step": 63100
    },
    {
      "epoch": 4.827744251776029,
      "grad_norm": 7.001639366149902,
      "learning_rate": 4.396531968527997e-05,
      "loss": 1.9397,
      "step": 63200
    },
    {
      "epoch": 4.835383087617447,
      "grad_norm": 7.432343482971191,
      "learning_rate": 4.395577114047819e-05,
      "loss": 2.0075,
      "step": 63300
    },
    {
      "epoch": 4.843021923458865,
      "grad_norm": 7.410534381866455,
      "learning_rate": 4.3946222595676425e-05,
      "loss": 2.0926,
      "step": 63400
    },
    {
      "epoch": 4.850660759300283,
      "grad_norm": 10.68178653717041,
      "learning_rate": 4.3936674050874647e-05,
      "loss": 2.0201,
      "step": 63500
    },
    {
      "epoch": 4.8582995951417,
      "grad_norm": 7.585289478302002,
      "learning_rate": 4.3927125506072875e-05,
      "loss": 1.9729,
      "step": 63600
    },
    {
      "epoch": 4.865938430983118,
      "grad_norm": 8.411153793334961,
      "learning_rate": 4.3917576961271104e-05,
      "loss": 2.0187,
      "step": 63700
    },
    {
      "epoch": 4.873577266824536,
      "grad_norm": 6.060450553894043,
      "learning_rate": 4.390802841646933e-05,
      "loss": 2.0294,
      "step": 63800
    },
    {
      "epoch": 4.881216102665954,
      "grad_norm": 6.7687835693359375,
      "learning_rate": 4.389847987166756e-05,
      "loss": 1.9438,
      "step": 63900
    },
    {
      "epoch": 4.888854938507372,
      "grad_norm": 8.03898811340332,
      "learning_rate": 4.388893132686579e-05,
      "loss": 2.0187,
      "step": 64000
    },
    {
      "epoch": 4.896493774348789,
      "grad_norm": 7.919973373413086,
      "learning_rate": 4.387938278206402e-05,
      "loss": 2.0279,
      "step": 64100
    },
    {
      "epoch": 4.9041326101902065,
      "grad_norm": 6.375036239624023,
      "learning_rate": 4.386983423726224e-05,
      "loss": 1.9331,
      "step": 64200
    },
    {
      "epoch": 4.911771446031625,
      "grad_norm": 7.394561290740967,
      "learning_rate": 4.3860285692460476e-05,
      "loss": 2.0774,
      "step": 64300
    },
    {
      "epoch": 4.919410281873042,
      "grad_norm": 7.815742015838623,
      "learning_rate": 4.38507371476587e-05,
      "loss": 1.9936,
      "step": 64400
    },
    {
      "epoch": 4.927049117714461,
      "grad_norm": 5.976259708404541,
      "learning_rate": 4.384118860285693e-05,
      "loss": 2.0311,
      "step": 64500
    },
    {
      "epoch": 4.934687953555878,
      "grad_norm": 8.8474702835083,
      "learning_rate": 4.3831640058055155e-05,
      "loss": 1.9679,
      "step": 64600
    },
    {
      "epoch": 4.9423267893972955,
      "grad_norm": 6.681094646453857,
      "learning_rate": 4.3822091513253384e-05,
      "loss": 1.9613,
      "step": 64700
    },
    {
      "epoch": 4.949965625238714,
      "grad_norm": 7.476039886474609,
      "learning_rate": 4.3812542968451606e-05,
      "loss": 1.9983,
      "step": 64800
    },
    {
      "epoch": 4.957604461080131,
      "grad_norm": 6.650850772857666,
      "learning_rate": 4.380299442364984e-05,
      "loss": 2.07,
      "step": 64900
    },
    {
      "epoch": 4.96524329692155,
      "grad_norm": 6.500988006591797,
      "learning_rate": 4.3793445878848063e-05,
      "loss": 1.9295,
      "step": 65000
    },
    {
      "epoch": 4.972882132762967,
      "grad_norm": 5.788300514221191,
      "learning_rate": 4.378389733404629e-05,
      "loss": 1.9698,
      "step": 65100
    },
    {
      "epoch": 4.9805209686043845,
      "grad_norm": 5.706064701080322,
      "learning_rate": 4.377434878924452e-05,
      "loss": 1.973,
      "step": 65200
    },
    {
      "epoch": 4.988159804445802,
      "grad_norm": 6.058688163757324,
      "learning_rate": 4.376480024444275e-05,
      "loss": 1.9481,
      "step": 65300
    },
    {
      "epoch": 4.99579864028722,
      "grad_norm": 9.368419647216797,
      "learning_rate": 4.375525169964098e-05,
      "loss": 2.0272,
      "step": 65400
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.9154002666473389,
      "eval_runtime": 3.0198,
      "eval_samples_per_second": 228.493,
      "eval_steps_per_second": 228.493,
      "step": 65455
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7580475807189941,
      "eval_runtime": 56.7306,
      "eval_samples_per_second": 230.757,
      "eval_steps_per_second": 230.757,
      "step": 65455
    },
    {
      "epoch": 5.003437476128638,
      "grad_norm": 8.282947540283203,
      "learning_rate": 4.374570315483921e-05,
      "loss": 2.0336,
      "step": 65500
    },
    {
      "epoch": 5.011076311970056,
      "grad_norm": 6.394428253173828,
      "learning_rate": 4.3736154610037436e-05,
      "loss": 1.8649,
      "step": 65600
    },
    {
      "epoch": 5.0187151478114735,
      "grad_norm": 6.796335220336914,
      "learning_rate": 4.372660606523566e-05,
      "loss": 1.9792,
      "step": 65700
    },
    {
      "epoch": 5.026353983652891,
      "grad_norm": 5.67604398727417,
      "learning_rate": 4.371705752043389e-05,
      "loss": 1.9136,
      "step": 65800
    },
    {
      "epoch": 5.033992819494309,
      "grad_norm": 10.116143226623535,
      "learning_rate": 4.3707508975632115e-05,
      "loss": 1.946,
      "step": 65900
    },
    {
      "epoch": 5.041631655335727,
      "grad_norm": 7.464380741119385,
      "learning_rate": 4.3697960430830344e-05,
      "loss": 2.0773,
      "step": 66000
    },
    {
      "epoch": 5.049270491177144,
      "grad_norm": 6.860954284667969,
      "learning_rate": 4.368841188602857e-05,
      "loss": 1.9128,
      "step": 66100
    },
    {
      "epoch": 5.0569093270185625,
      "grad_norm": 6.405147552490234,
      "learning_rate": 4.36788633412268e-05,
      "loss": 1.9297,
      "step": 66200
    },
    {
      "epoch": 5.06454816285998,
      "grad_norm": 7.0616350173950195,
      "learning_rate": 4.366931479642502e-05,
      "loss": 1.9405,
      "step": 66300
    },
    {
      "epoch": 5.072186998701398,
      "grad_norm": 6.259587287902832,
      "learning_rate": 4.365976625162325e-05,
      "loss": 1.8854,
      "step": 66400
    },
    {
      "epoch": 5.079825834542816,
      "grad_norm": 7.995483875274658,
      "learning_rate": 4.365021770682148e-05,
      "loss": 2.0372,
      "step": 66500
    },
    {
      "epoch": 5.087464670384233,
      "grad_norm": 9.740421295166016,
      "learning_rate": 4.364066916201971e-05,
      "loss": 1.9864,
      "step": 66600
    },
    {
      "epoch": 5.095103506225652,
      "grad_norm": 7.8686347007751465,
      "learning_rate": 4.363112061721794e-05,
      "loss": 1.8479,
      "step": 66700
    },
    {
      "epoch": 5.102742342067069,
      "grad_norm": 7.35790491104126,
      "learning_rate": 4.3621572072416166e-05,
      "loss": 2.0305,
      "step": 66800
    },
    {
      "epoch": 5.1103811779084864,
      "grad_norm": 8.271369934082031,
      "learning_rate": 4.3612023527614395e-05,
      "loss": 1.9209,
      "step": 66900
    },
    {
      "epoch": 5.118020013749905,
      "grad_norm": 6.007514476776123,
      "learning_rate": 4.360247498281262e-05,
      "loss": 1.8589,
      "step": 67000
    },
    {
      "epoch": 5.125658849591322,
      "grad_norm": 8.321661949157715,
      "learning_rate": 4.359292643801085e-05,
      "loss": 1.8796,
      "step": 67100
    },
    {
      "epoch": 5.13329768543274,
      "grad_norm": 5.827358722686768,
      "learning_rate": 4.3583377893209074e-05,
      "loss": 1.9613,
      "step": 67200
    },
    {
      "epoch": 5.140936521274158,
      "grad_norm": 7.435380935668945,
      "learning_rate": 4.35738293484073e-05,
      "loss": 1.9846,
      "step": 67300
    },
    {
      "epoch": 5.1485753571155755,
      "grad_norm": 5.969575881958008,
      "learning_rate": 4.356428080360553e-05,
      "loss": 1.908,
      "step": 67400
    },
    {
      "epoch": 5.156214192956993,
      "grad_norm": 7.079057693481445,
      "learning_rate": 4.355473225880376e-05,
      "loss": 2.053,
      "step": 67500
    },
    {
      "epoch": 5.163853028798411,
      "grad_norm": 6.2350077629089355,
      "learning_rate": 4.354518371400199e-05,
      "loss": 1.9051,
      "step": 67600
    },
    {
      "epoch": 5.171491864639829,
      "grad_norm": 5.9908952713012695,
      "learning_rate": 4.353563516920022e-05,
      "loss": 2.0638,
      "step": 67700
    },
    {
      "epoch": 5.179130700481247,
      "grad_norm": 7.075287342071533,
      "learning_rate": 4.3526086624398446e-05,
      "loss": 2.0523,
      "step": 67800
    },
    {
      "epoch": 5.1867695363226645,
      "grad_norm": 6.432960033416748,
      "learning_rate": 4.351653807959667e-05,
      "loss": 1.9438,
      "step": 67900
    },
    {
      "epoch": 5.194408372164082,
      "grad_norm": 5.300244331359863,
      "learning_rate": 4.3506989534794904e-05,
      "loss": 1.868,
      "step": 68000
    },
    {
      "epoch": 5.2020472080055,
      "grad_norm": 7.110914707183838,
      "learning_rate": 4.3497440989993126e-05,
      "loss": 1.9814,
      "step": 68100
    },
    {
      "epoch": 5.209686043846918,
      "grad_norm": 7.360859394073486,
      "learning_rate": 4.3487892445191354e-05,
      "loss": 1.9489,
      "step": 68200
    },
    {
      "epoch": 5.217324879688335,
      "grad_norm": 6.817523002624512,
      "learning_rate": 4.347834390038958e-05,
      "loss": 1.9863,
      "step": 68300
    },
    {
      "epoch": 5.2249637155297535,
      "grad_norm": 7.010962009429932,
      "learning_rate": 4.346879535558781e-05,
      "loss": 1.9771,
      "step": 68400
    },
    {
      "epoch": 5.232602551371171,
      "grad_norm": 7.446442604064941,
      "learning_rate": 4.3459246810786034e-05,
      "loss": 1.9669,
      "step": 68500
    },
    {
      "epoch": 5.240241387212588,
      "grad_norm": 10.620014190673828,
      "learning_rate": 4.344969826598427e-05,
      "loss": 1.9004,
      "step": 68600
    },
    {
      "epoch": 5.247880223054007,
      "grad_norm": 8.22510814666748,
      "learning_rate": 4.344014972118249e-05,
      "loss": 1.8788,
      "step": 68700
    },
    {
      "epoch": 5.255519058895424,
      "grad_norm": 5.826534271240234,
      "learning_rate": 4.343060117638072e-05,
      "loss": 1.9747,
      "step": 68800
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 6.887749671936035,
      "learning_rate": 4.342105263157895e-05,
      "loss": 1.8694,
      "step": 68900
    },
    {
      "epoch": 5.27079673057826,
      "grad_norm": 6.581635475158691,
      "learning_rate": 4.341150408677718e-05,
      "loss": 1.9686,
      "step": 69000
    },
    {
      "epoch": 5.278435566419677,
      "grad_norm": 6.657167434692383,
      "learning_rate": 4.3401955541975406e-05,
      "loss": 1.9053,
      "step": 69100
    },
    {
      "epoch": 5.286074402261096,
      "grad_norm": 7.257842063903809,
      "learning_rate": 4.3392406997173635e-05,
      "loss": 1.9613,
      "step": 69200
    },
    {
      "epoch": 5.293713238102513,
      "grad_norm": 6.011744022369385,
      "learning_rate": 4.338285845237186e-05,
      "loss": 1.8651,
      "step": 69300
    },
    {
      "epoch": 5.301352073943931,
      "grad_norm": 7.867451190948486,
      "learning_rate": 4.3373309907570085e-05,
      "loss": 1.9347,
      "step": 69400
    },
    {
      "epoch": 5.308990909785349,
      "grad_norm": 6.607656002044678,
      "learning_rate": 4.336376136276832e-05,
      "loss": 1.9191,
      "step": 69500
    },
    {
      "epoch": 5.316629745626766,
      "grad_norm": 6.8656535148620605,
      "learning_rate": 4.335421281796654e-05,
      "loss": 1.921,
      "step": 69600
    },
    {
      "epoch": 5.324268581468184,
      "grad_norm": 9.455994606018066,
      "learning_rate": 4.334466427316477e-05,
      "loss": 1.9806,
      "step": 69700
    },
    {
      "epoch": 5.331907417309602,
      "grad_norm": 6.293274402618408,
      "learning_rate": 4.3335115728363e-05,
      "loss": 1.9035,
      "step": 69800
    },
    {
      "epoch": 5.33954625315102,
      "grad_norm": 7.371517181396484,
      "learning_rate": 4.332556718356123e-05,
      "loss": 1.9708,
      "step": 69900
    },
    {
      "epoch": 5.347185088992438,
      "grad_norm": 7.388585090637207,
      "learning_rate": 4.331601863875945e-05,
      "loss": 2.0198,
      "step": 70000
    },
    {
      "epoch": 5.354823924833855,
      "grad_norm": 9.63520622253418,
      "learning_rate": 4.3306470093957686e-05,
      "loss": 2.0525,
      "step": 70100
    },
    {
      "epoch": 5.362462760675273,
      "grad_norm": 5.821413516998291,
      "learning_rate": 4.329692154915591e-05,
      "loss": 1.9456,
      "step": 70200
    },
    {
      "epoch": 5.370101596516691,
      "grad_norm": 5.730468273162842,
      "learning_rate": 4.328737300435414e-05,
      "loss": 1.9742,
      "step": 70300
    },
    {
      "epoch": 5.377740432358109,
      "grad_norm": 6.145426273345947,
      "learning_rate": 4.3277824459552365e-05,
      "loss": 1.9301,
      "step": 70400
    },
    {
      "epoch": 5.385379268199526,
      "grad_norm": 5.797201633453369,
      "learning_rate": 4.3268275914750594e-05,
      "loss": 1.9467,
      "step": 70500
    },
    {
      "epoch": 5.393018104040944,
      "grad_norm": 6.520362377166748,
      "learning_rate": 4.325872736994882e-05,
      "loss": 1.9146,
      "step": 70600
    },
    {
      "epoch": 5.400656939882362,
      "grad_norm": 6.545873641967773,
      "learning_rate": 4.324917882514705e-05,
      "loss": 1.8848,
      "step": 70700
    },
    {
      "epoch": 5.408295775723779,
      "grad_norm": 6.293325901031494,
      "learning_rate": 4.323963028034528e-05,
      "loss": 1.9217,
      "step": 70800
    },
    {
      "epoch": 5.415934611565198,
      "grad_norm": 6.723865032196045,
      "learning_rate": 4.32300817355435e-05,
      "loss": 1.9328,
      "step": 70900
    },
    {
      "epoch": 5.423573447406615,
      "grad_norm": 10.02058219909668,
      "learning_rate": 4.322053319074174e-05,
      "loss": 1.9413,
      "step": 71000
    },
    {
      "epoch": 5.431212283248033,
      "grad_norm": 7.466328144073486,
      "learning_rate": 4.321098464593996e-05,
      "loss": 1.9398,
      "step": 71100
    },
    {
      "epoch": 5.438851119089451,
      "grad_norm": 8.173851013183594,
      "learning_rate": 4.320143610113819e-05,
      "loss": 1.9592,
      "step": 71200
    },
    {
      "epoch": 5.446489954930868,
      "grad_norm": 5.881141662597656,
      "learning_rate": 4.319188755633642e-05,
      "loss": 1.9996,
      "step": 71300
    },
    {
      "epoch": 5.454128790772287,
      "grad_norm": 7.450631618499756,
      "learning_rate": 4.3182339011534646e-05,
      "loss": 2.0166,
      "step": 71400
    },
    {
      "epoch": 5.461767626613704,
      "grad_norm": 9.29874324798584,
      "learning_rate": 4.3172790466732874e-05,
      "loss": 1.8761,
      "step": 71500
    },
    {
      "epoch": 5.4694064624551215,
      "grad_norm": 9.069369316101074,
      "learning_rate": 4.31632419219311e-05,
      "loss": 1.9271,
      "step": 71600
    },
    {
      "epoch": 5.47704529829654,
      "grad_norm": 6.768238067626953,
      "learning_rate": 4.315369337712933e-05,
      "loss": 1.9061,
      "step": 71700
    },
    {
      "epoch": 5.484684134137957,
      "grad_norm": 9.053297996520996,
      "learning_rate": 4.3144144832327554e-05,
      "loss": 1.9841,
      "step": 71800
    },
    {
      "epoch": 5.492322969979375,
      "grad_norm": 6.8078765869140625,
      "learning_rate": 4.313459628752579e-05,
      "loss": 1.9296,
      "step": 71900
    },
    {
      "epoch": 5.499961805820793,
      "grad_norm": 7.635904788970947,
      "learning_rate": 4.312504774272401e-05,
      "loss": 1.9066,
      "step": 72000
    },
    {
      "epoch": 5.5076006416622105,
      "grad_norm": 6.712381362915039,
      "learning_rate": 4.311549919792224e-05,
      "loss": 1.9046,
      "step": 72100
    },
    {
      "epoch": 5.515239477503629,
      "grad_norm": 8.585090637207031,
      "learning_rate": 4.310595065312046e-05,
      "loss": 1.9823,
      "step": 72200
    },
    {
      "epoch": 5.522878313345046,
      "grad_norm": 6.261736869812012,
      "learning_rate": 4.30964021083187e-05,
      "loss": 1.8903,
      "step": 72300
    },
    {
      "epoch": 5.530517149186464,
      "grad_norm": 8.226005554199219,
      "learning_rate": 4.308685356351692e-05,
      "loss": 1.9226,
      "step": 72400
    },
    {
      "epoch": 5.538155985027882,
      "grad_norm": 6.0195465087890625,
      "learning_rate": 4.307730501871515e-05,
      "loss": 1.9595,
      "step": 72500
    },
    {
      "epoch": 5.5457948208692995,
      "grad_norm": 8.492079734802246,
      "learning_rate": 4.3067756473913376e-05,
      "loss": 1.963,
      "step": 72600
    },
    {
      "epoch": 5.553433656710717,
      "grad_norm": 5.914926052093506,
      "learning_rate": 4.3058207929111605e-05,
      "loss": 1.9576,
      "step": 72700
    },
    {
      "epoch": 5.561072492552135,
      "grad_norm": 6.397109508514404,
      "learning_rate": 4.3048659384309834e-05,
      "loss": 1.8233,
      "step": 72800
    },
    {
      "epoch": 5.568711328393553,
      "grad_norm": 7.171339988708496,
      "learning_rate": 4.303911083950806e-05,
      "loss": 1.8645,
      "step": 72900
    },
    {
      "epoch": 5.57635016423497,
      "grad_norm": 5.847250461578369,
      "learning_rate": 4.302956229470629e-05,
      "loss": 2.0166,
      "step": 73000
    },
    {
      "epoch": 5.5839890000763885,
      "grad_norm": 7.95049524307251,
      "learning_rate": 4.302001374990451e-05,
      "loss": 1.9279,
      "step": 73100
    },
    {
      "epoch": 5.591627835917806,
      "grad_norm": 6.161503791809082,
      "learning_rate": 4.301046520510275e-05,
      "loss": 1.9432,
      "step": 73200
    },
    {
      "epoch": 5.599266671759224,
      "grad_norm": 9.074132919311523,
      "learning_rate": 4.300091666030097e-05,
      "loss": 1.9615,
      "step": 73300
    },
    {
      "epoch": 5.606905507600642,
      "grad_norm": 6.698201656341553,
      "learning_rate": 4.29913681154992e-05,
      "loss": 1.9249,
      "step": 73400
    },
    {
      "epoch": 5.614544343442059,
      "grad_norm": 7.1738362312316895,
      "learning_rate": 4.298181957069743e-05,
      "loss": 1.915,
      "step": 73500
    },
    {
      "epoch": 5.6221831792834775,
      "grad_norm": 8.812691688537598,
      "learning_rate": 4.2972271025895656e-05,
      "loss": 1.9047,
      "step": 73600
    },
    {
      "epoch": 5.629822015124895,
      "grad_norm": 5.49664831161499,
      "learning_rate": 4.296272248109388e-05,
      "loss": 2.0223,
      "step": 73700
    },
    {
      "epoch": 5.637460850966312,
      "grad_norm": 5.5522308349609375,
      "learning_rate": 4.2953173936292114e-05,
      "loss": 1.9568,
      "step": 73800
    },
    {
      "epoch": 5.645099686807731,
      "grad_norm": 7.464873313903809,
      "learning_rate": 4.2943625391490336e-05,
      "loss": 1.9161,
      "step": 73900
    },
    {
      "epoch": 5.652738522649148,
      "grad_norm": 9.211423873901367,
      "learning_rate": 4.2934076846688564e-05,
      "loss": 1.9931,
      "step": 74000
    },
    {
      "epoch": 5.660377358490566,
      "grad_norm": 6.904640197753906,
      "learning_rate": 4.292452830188679e-05,
      "loss": 1.91,
      "step": 74100
    },
    {
      "epoch": 5.668016194331984,
      "grad_norm": 5.692299842834473,
      "learning_rate": 4.291497975708502e-05,
      "loss": 1.9866,
      "step": 74200
    },
    {
      "epoch": 5.6756550301734014,
      "grad_norm": 8.469564437866211,
      "learning_rate": 4.290543121228325e-05,
      "loss": 2.0173,
      "step": 74300
    },
    {
      "epoch": 5.68329386601482,
      "grad_norm": 7.360446453094482,
      "learning_rate": 4.289588266748148e-05,
      "loss": 1.9639,
      "step": 74400
    },
    {
      "epoch": 5.690932701856237,
      "grad_norm": 6.378288269042969,
      "learning_rate": 4.288633412267971e-05,
      "loss": 1.9766,
      "step": 74500
    },
    {
      "epoch": 5.698571537697655,
      "grad_norm": 5.957810401916504,
      "learning_rate": 4.287678557787793e-05,
      "loss": 2.0065,
      "step": 74600
    },
    {
      "epoch": 5.706210373539073,
      "grad_norm": 6.1016364097595215,
      "learning_rate": 4.2867237033076165e-05,
      "loss": 1.9703,
      "step": 74700
    },
    {
      "epoch": 5.7138492093804905,
      "grad_norm": 8.833038330078125,
      "learning_rate": 4.285768848827439e-05,
      "loss": 1.8403,
      "step": 74800
    },
    {
      "epoch": 5.721488045221908,
      "grad_norm": 6.779305458068848,
      "learning_rate": 4.2848139943472616e-05,
      "loss": 1.9795,
      "step": 74900
    },
    {
      "epoch": 5.729126881063326,
      "grad_norm": 5.619870185852051,
      "learning_rate": 4.2838591398670845e-05,
      "loss": 1.8976,
      "step": 75000
    },
    {
      "epoch": 5.736765716904744,
      "grad_norm": 9.622161865234375,
      "learning_rate": 4.282904285386907e-05,
      "loss": 1.9222,
      "step": 75100
    },
    {
      "epoch": 5.744404552746161,
      "grad_norm": 13.821327209472656,
      "learning_rate": 4.2819494309067295e-05,
      "loss": 1.977,
      "step": 75200
    },
    {
      "epoch": 5.7520433885875795,
      "grad_norm": 8.500223159790039,
      "learning_rate": 4.280994576426553e-05,
      "loss": 1.9018,
      "step": 75300
    },
    {
      "epoch": 5.759682224428997,
      "grad_norm": 7.1965789794921875,
      "learning_rate": 4.280039721946375e-05,
      "loss": 1.9721,
      "step": 75400
    },
    {
      "epoch": 5.767321060270415,
      "grad_norm": 6.635129928588867,
      "learning_rate": 4.279084867466198e-05,
      "loss": 1.9304,
      "step": 75500
    },
    {
      "epoch": 5.774959896111833,
      "grad_norm": 8.306882858276367,
      "learning_rate": 4.278130012986022e-05,
      "loss": 1.9804,
      "step": 75600
    },
    {
      "epoch": 5.78259873195325,
      "grad_norm": 6.787155628204346,
      "learning_rate": 4.277175158505844e-05,
      "loss": 2.073,
      "step": 75700
    },
    {
      "epoch": 5.7902375677946685,
      "grad_norm": 5.6442670822143555,
      "learning_rate": 4.276220304025667e-05,
      "loss": 1.8752,
      "step": 75800
    },
    {
      "epoch": 5.797876403636086,
      "grad_norm": 5.2231035232543945,
      "learning_rate": 4.2752654495454896e-05,
      "loss": 1.806,
      "step": 75900
    },
    {
      "epoch": 5.805515239477503,
      "grad_norm": 6.176200866699219,
      "learning_rate": 4.2743105950653125e-05,
      "loss": 2.0321,
      "step": 76000
    },
    {
      "epoch": 5.813154075318922,
      "grad_norm": 6.061190605163574,
      "learning_rate": 4.273355740585135e-05,
      "loss": 1.98,
      "step": 76100
    },
    {
      "epoch": 5.820792911160339,
      "grad_norm": 6.527734756469727,
      "learning_rate": 4.272400886104958e-05,
      "loss": 1.9082,
      "step": 76200
    },
    {
      "epoch": 5.828431747001757,
      "grad_norm": 8.996277809143066,
      "learning_rate": 4.2714460316247804e-05,
      "loss": 1.9097,
      "step": 76300
    },
    {
      "epoch": 5.836070582843175,
      "grad_norm": 6.12342643737793,
      "learning_rate": 4.270491177144603e-05,
      "loss": 1.9477,
      "step": 76400
    },
    {
      "epoch": 5.843709418684592,
      "grad_norm": 5.547939777374268,
      "learning_rate": 4.269536322664426e-05,
      "loss": 2.0288,
      "step": 76500
    },
    {
      "epoch": 5.851348254526011,
      "grad_norm": 6.079452037811279,
      "learning_rate": 4.268581468184249e-05,
      "loss": 1.9917,
      "step": 76600
    },
    {
      "epoch": 5.858987090367428,
      "grad_norm": 5.362397193908691,
      "learning_rate": 4.267626613704072e-05,
      "loss": 1.9439,
      "step": 76700
    },
    {
      "epoch": 5.866625926208846,
      "grad_norm": 7.105629920959473,
      "learning_rate": 4.266671759223895e-05,
      "loss": 1.9047,
      "step": 76800
    },
    {
      "epoch": 5.874264762050263,
      "grad_norm": 6.661077976226807,
      "learning_rate": 4.2657169047437176e-05,
      "loss": 2.0367,
      "step": 76900
    },
    {
      "epoch": 5.881903597891681,
      "grad_norm": 6.346085071563721,
      "learning_rate": 4.26476205026354e-05,
      "loss": 1.8912,
      "step": 77000
    },
    {
      "epoch": 5.889542433733099,
      "grad_norm": 7.967213153839111,
      "learning_rate": 4.2638071957833634e-05,
      "loss": 1.9213,
      "step": 77100
    },
    {
      "epoch": 5.897181269574517,
      "grad_norm": 7.528043746948242,
      "learning_rate": 4.2628523413031856e-05,
      "loss": 1.9448,
      "step": 77200
    },
    {
      "epoch": 5.904820105415935,
      "grad_norm": 6.253414154052734,
      "learning_rate": 4.2618974868230084e-05,
      "loss": 1.8728,
      "step": 77300
    },
    {
      "epoch": 5.912458941257352,
      "grad_norm": 5.67581033706665,
      "learning_rate": 4.260942632342831e-05,
      "loss": 1.9074,
      "step": 77400
    },
    {
      "epoch": 5.92009777709877,
      "grad_norm": 8.113992691040039,
      "learning_rate": 4.259987777862654e-05,
      "loss": 1.9973,
      "step": 77500
    },
    {
      "epoch": 5.927736612940188,
      "grad_norm": 6.8124494552612305,
      "learning_rate": 4.2590329233824764e-05,
      "loss": 1.9268,
      "step": 77600
    },
    {
      "epoch": 5.935375448781606,
      "grad_norm": 7.727973937988281,
      "learning_rate": 4.2580780689023e-05,
      "loss": 1.8916,
      "step": 77700
    },
    {
      "epoch": 5.943014284623024,
      "grad_norm": 6.607117176055908,
      "learning_rate": 4.257123214422122e-05,
      "loss": 1.9057,
      "step": 77800
    },
    {
      "epoch": 5.950653120464441,
      "grad_norm": 7.215540409088135,
      "learning_rate": 4.256168359941945e-05,
      "loss": 1.9848,
      "step": 77900
    },
    {
      "epoch": 5.9582919563058585,
      "grad_norm": 13.654770851135254,
      "learning_rate": 4.255213505461768e-05,
      "loss": 1.9968,
      "step": 78000
    },
    {
      "epoch": 5.965930792147277,
      "grad_norm": 9.93260383605957,
      "learning_rate": 4.254258650981591e-05,
      "loss": 2.0099,
      "step": 78100
    },
    {
      "epoch": 5.973569627988694,
      "grad_norm": 9.13723373413086,
      "learning_rate": 4.2533037965014136e-05,
      "loss": 1.9496,
      "step": 78200
    },
    {
      "epoch": 5.981208463830113,
      "grad_norm": 7.36950159072876,
      "learning_rate": 4.252348942021236e-05,
      "loss": 1.9862,
      "step": 78300
    },
    {
      "epoch": 5.98884729967153,
      "grad_norm": 8.551340103149414,
      "learning_rate": 4.251394087541059e-05,
      "loss": 1.8955,
      "step": 78400
    },
    {
      "epoch": 5.9964861355129475,
      "grad_norm": 9.279335975646973,
      "learning_rate": 4.2504392330608815e-05,
      "loss": 1.9052,
      "step": 78500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.8964697122573853,
      "eval_runtime": 2.9963,
      "eval_samples_per_second": 230.284,
      "eval_steps_per_second": 230.284,
      "step": 78546
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.7266267538070679,
      "eval_runtime": 46.7596,
      "eval_samples_per_second": 279.964,
      "eval_steps_per_second": 279.964,
      "step": 78546
    },
    {
      "epoch": 6.004124971354366,
      "grad_norm": 7.2588958740234375,
      "learning_rate": 4.2494843785807044e-05,
      "loss": 1.8891,
      "step": 78600
    },
    {
      "epoch": 6.011763807195783,
      "grad_norm": 9.24697494506836,
      "learning_rate": 4.248529524100527e-05,
      "loss": 1.8871,
      "step": 78700
    },
    {
      "epoch": 6.019402643037201,
      "grad_norm": 6.401134490966797,
      "learning_rate": 4.24757466962035e-05,
      "loss": 1.7921,
      "step": 78800
    },
    {
      "epoch": 6.027041478878619,
      "grad_norm": 6.63100004196167,
      "learning_rate": 4.246619815140172e-05,
      "loss": 1.9173,
      "step": 78900
    },
    {
      "epoch": 6.0346803147200365,
      "grad_norm": 11.566176414489746,
      "learning_rate": 4.245664960659996e-05,
      "loss": 1.9175,
      "step": 79000
    },
    {
      "epoch": 6.042319150561455,
      "grad_norm": 6.049561977386475,
      "learning_rate": 4.244710106179818e-05,
      "loss": 1.9972,
      "step": 79100
    },
    {
      "epoch": 6.049957986402872,
      "grad_norm": 7.422032356262207,
      "learning_rate": 4.243755251699641e-05,
      "loss": 1.9896,
      "step": 79200
    },
    {
      "epoch": 6.05759682224429,
      "grad_norm": 5.330448627471924,
      "learning_rate": 4.242800397219464e-05,
      "loss": 1.9791,
      "step": 79300
    },
    {
      "epoch": 6.065235658085708,
      "grad_norm": 6.8091278076171875,
      "learning_rate": 4.2418455427392866e-05,
      "loss": 1.9218,
      "step": 79400
    },
    {
      "epoch": 6.0728744939271255,
      "grad_norm": 7.123209476470947,
      "learning_rate": 4.2408906882591095e-05,
      "loss": 1.9146,
      "step": 79500
    },
    {
      "epoch": 6.080513329768543,
      "grad_norm": 7.321145534515381,
      "learning_rate": 4.2399358337789324e-05,
      "loss": 1.913,
      "step": 79600
    },
    {
      "epoch": 6.088152165609961,
      "grad_norm": 9.134397506713867,
      "learning_rate": 4.238980979298755e-05,
      "loss": 1.8856,
      "step": 79700
    },
    {
      "epoch": 6.095791001451379,
      "grad_norm": 11.001266479492188,
      "learning_rate": 4.2380261248185774e-05,
      "loss": 1.9432,
      "step": 79800
    },
    {
      "epoch": 6.103429837292796,
      "grad_norm": 6.815329551696777,
      "learning_rate": 4.237071270338401e-05,
      "loss": 1.8768,
      "step": 79900
    },
    {
      "epoch": 6.1110686731342145,
      "grad_norm": 7.128061294555664,
      "learning_rate": 4.236116415858223e-05,
      "loss": 1.9972,
      "step": 80000
    },
    {
      "epoch": 6.118707508975632,
      "grad_norm": 4.323829650878906,
      "learning_rate": 4.235161561378046e-05,
      "loss": 1.9623,
      "step": 80100
    },
    {
      "epoch": 6.126346344817049,
      "grad_norm": 7.189273357391357,
      "learning_rate": 4.234206706897869e-05,
      "loss": 1.9225,
      "step": 80200
    },
    {
      "epoch": 6.133985180658468,
      "grad_norm": 7.266464710235596,
      "learning_rate": 4.233251852417692e-05,
      "loss": 1.8256,
      "step": 80300
    },
    {
      "epoch": 6.141624016499885,
      "grad_norm": 7.267820835113525,
      "learning_rate": 4.232296997937515e-05,
      "loss": 1.968,
      "step": 80400
    },
    {
      "epoch": 6.1492628523413035,
      "grad_norm": 6.351726531982422,
      "learning_rate": 4.2313421434573375e-05,
      "loss": 1.8566,
      "step": 80500
    },
    {
      "epoch": 6.156901688182721,
      "grad_norm": 6.630866050720215,
      "learning_rate": 4.2303872889771604e-05,
      "loss": 1.8851,
      "step": 80600
    },
    {
      "epoch": 6.164540524024138,
      "grad_norm": 6.620143413543701,
      "learning_rate": 4.2294324344969826e-05,
      "loss": 1.8518,
      "step": 80700
    },
    {
      "epoch": 6.172179359865557,
      "grad_norm": 8.944049835205078,
      "learning_rate": 4.228477580016806e-05,
      "loss": 1.8931,
      "step": 80800
    },
    {
      "epoch": 6.179818195706974,
      "grad_norm": 5.639257431030273,
      "learning_rate": 4.227522725536628e-05,
      "loss": 1.9448,
      "step": 80900
    },
    {
      "epoch": 6.187457031548392,
      "grad_norm": 6.991621017456055,
      "learning_rate": 4.226567871056451e-05,
      "loss": 1.9526,
      "step": 81000
    },
    {
      "epoch": 6.19509586738981,
      "grad_norm": 7.248317718505859,
      "learning_rate": 4.225613016576274e-05,
      "loss": 2.0004,
      "step": 81100
    },
    {
      "epoch": 6.202734703231227,
      "grad_norm": 6.782402038574219,
      "learning_rate": 4.224658162096097e-05,
      "loss": 1.9792,
      "step": 81200
    },
    {
      "epoch": 6.210373539072645,
      "grad_norm": 9.6021089553833,
      "learning_rate": 4.223703307615919e-05,
      "loss": 1.9141,
      "step": 81300
    },
    {
      "epoch": 6.218012374914063,
      "grad_norm": 6.939171314239502,
      "learning_rate": 4.222748453135743e-05,
      "loss": 1.928,
      "step": 81400
    },
    {
      "epoch": 6.225651210755481,
      "grad_norm": 6.419640064239502,
      "learning_rate": 4.221793598655565e-05,
      "loss": 1.9332,
      "step": 81500
    },
    {
      "epoch": 6.233290046596899,
      "grad_norm": 5.223515510559082,
      "learning_rate": 4.220838744175388e-05,
      "loss": 2.0264,
      "step": 81600
    },
    {
      "epoch": 6.2409288824383164,
      "grad_norm": 8.486376762390137,
      "learning_rate": 4.2198838896952106e-05,
      "loss": 1.8791,
      "step": 81700
    },
    {
      "epoch": 6.248567718279734,
      "grad_norm": 5.002850532531738,
      "learning_rate": 4.2189290352150335e-05,
      "loss": 1.8622,
      "step": 81800
    },
    {
      "epoch": 6.256206554121152,
      "grad_norm": 5.97911262512207,
      "learning_rate": 4.2179741807348563e-05,
      "loss": 1.8277,
      "step": 81900
    },
    {
      "epoch": 6.26384538996257,
      "grad_norm": 6.976006984710693,
      "learning_rate": 4.217019326254679e-05,
      "loss": 1.9763,
      "step": 82000
    },
    {
      "epoch": 6.271484225803987,
      "grad_norm": 9.626128196716309,
      "learning_rate": 4.216064471774502e-05,
      "loss": 1.9803,
      "step": 82100
    },
    {
      "epoch": 6.2791230616454055,
      "grad_norm": 6.96754264831543,
      "learning_rate": 4.215109617294324e-05,
      "loss": 2.0178,
      "step": 82200
    },
    {
      "epoch": 6.286761897486823,
      "grad_norm": 3.6281023025512695,
      "learning_rate": 4.214154762814148e-05,
      "loss": 1.8227,
      "step": 82300
    },
    {
      "epoch": 6.29440073332824,
      "grad_norm": 7.05527925491333,
      "learning_rate": 4.21319990833397e-05,
      "loss": 1.9248,
      "step": 82400
    },
    {
      "epoch": 6.302039569169659,
      "grad_norm": 7.9635419845581055,
      "learning_rate": 4.212245053853793e-05,
      "loss": 1.8619,
      "step": 82500
    },
    {
      "epoch": 6.309678405011076,
      "grad_norm": 6.859588623046875,
      "learning_rate": 4.211290199373616e-05,
      "loss": 1.9144,
      "step": 82600
    },
    {
      "epoch": 6.3173172408524945,
      "grad_norm": 10.175554275512695,
      "learning_rate": 4.2103353448934386e-05,
      "loss": 1.9486,
      "step": 82700
    },
    {
      "epoch": 6.324956076693912,
      "grad_norm": 9.976264953613281,
      "learning_rate": 4.209380490413261e-05,
      "loss": 1.9721,
      "step": 82800
    },
    {
      "epoch": 6.332594912535329,
      "grad_norm": 6.885308742523193,
      "learning_rate": 4.2084256359330844e-05,
      "loss": 1.8342,
      "step": 82900
    },
    {
      "epoch": 6.340233748376748,
      "grad_norm": 7.772210121154785,
      "learning_rate": 4.2074707814529066e-05,
      "loss": 2.0137,
      "step": 83000
    },
    {
      "epoch": 6.347872584218165,
      "grad_norm": 6.790281772613525,
      "learning_rate": 4.2065159269727294e-05,
      "loss": 1.846,
      "step": 83100
    },
    {
      "epoch": 6.355511420059583,
      "grad_norm": 7.9725422859191895,
      "learning_rate": 4.205561072492552e-05,
      "loss": 1.9147,
      "step": 83200
    },
    {
      "epoch": 6.363150255901001,
      "grad_norm": 7.0568647384643555,
      "learning_rate": 4.204606218012375e-05,
      "loss": 1.9707,
      "step": 83300
    },
    {
      "epoch": 6.370789091742418,
      "grad_norm": 7.256809711456299,
      "learning_rate": 4.203651363532198e-05,
      "loss": 1.9695,
      "step": 83400
    },
    {
      "epoch": 6.378427927583836,
      "grad_norm": 8.897439956665039,
      "learning_rate": 4.202696509052021e-05,
      "loss": 1.9733,
      "step": 83500
    },
    {
      "epoch": 6.386066763425254,
      "grad_norm": 6.171499252319336,
      "learning_rate": 4.201741654571844e-05,
      "loss": 1.94,
      "step": 83600
    },
    {
      "epoch": 6.393705599266672,
      "grad_norm": 6.594298362731934,
      "learning_rate": 4.200786800091666e-05,
      "loss": 1.8652,
      "step": 83700
    },
    {
      "epoch": 6.40134443510809,
      "grad_norm": 6.3220391273498535,
      "learning_rate": 4.199831945611489e-05,
      "loss": 1.9137,
      "step": 83800
    },
    {
      "epoch": 6.408983270949507,
      "grad_norm": 6.991079330444336,
      "learning_rate": 4.198877091131312e-05,
      "loss": 1.9749,
      "step": 83900
    },
    {
      "epoch": 6.416622106790925,
      "grad_norm": 10.734260559082031,
      "learning_rate": 4.1979222366511346e-05,
      "loss": 1.9002,
      "step": 84000
    },
    {
      "epoch": 6.424260942632343,
      "grad_norm": 6.801245212554932,
      "learning_rate": 4.196967382170957e-05,
      "loss": 2.0107,
      "step": 84100
    },
    {
      "epoch": 6.431899778473761,
      "grad_norm": 7.216887474060059,
      "learning_rate": 4.19601252769078e-05,
      "loss": 1.9265,
      "step": 84200
    },
    {
      "epoch": 6.439538614315178,
      "grad_norm": 7.5171332359313965,
      "learning_rate": 4.1950576732106025e-05,
      "loss": 1.9452,
      "step": 84300
    },
    {
      "epoch": 6.447177450156596,
      "grad_norm": 5.0432915687561035,
      "learning_rate": 4.1941028187304254e-05,
      "loss": 1.8935,
      "step": 84400
    },
    {
      "epoch": 6.454816285998014,
      "grad_norm": 4.544709205627441,
      "learning_rate": 4.193147964250249e-05,
      "loss": 1.9256,
      "step": 84500
    },
    {
      "epoch": 6.462455121839431,
      "grad_norm": 5.434264659881592,
      "learning_rate": 4.192193109770071e-05,
      "loss": 1.9317,
      "step": 84600
    },
    {
      "epoch": 6.47009395768085,
      "grad_norm": 7.697701930999756,
      "learning_rate": 4.191238255289894e-05,
      "loss": 1.9326,
      "step": 84700
    },
    {
      "epoch": 6.477732793522267,
      "grad_norm": 9.785834312438965,
      "learning_rate": 4.190283400809717e-05,
      "loss": 1.9059,
      "step": 84800
    },
    {
      "epoch": 6.485371629363685,
      "grad_norm": 7.708517551422119,
      "learning_rate": 4.18932854632954e-05,
      "loss": 1.969,
      "step": 84900
    },
    {
      "epoch": 6.493010465205103,
      "grad_norm": 5.659229755401611,
      "learning_rate": 4.188373691849362e-05,
      "loss": 1.9059,
      "step": 85000
    },
    {
      "epoch": 6.50064930104652,
      "grad_norm": 7.607316017150879,
      "learning_rate": 4.1874188373691855e-05,
      "loss": 1.9495,
      "step": 85100
    },
    {
      "epoch": 6.508288136887939,
      "grad_norm": 8.921313285827637,
      "learning_rate": 4.1864639828890076e-05,
      "loss": 2.0022,
      "step": 85200
    },
    {
      "epoch": 6.515926972729356,
      "grad_norm": 7.5027174949646,
      "learning_rate": 4.1855091284088305e-05,
      "loss": 1.8083,
      "step": 85300
    },
    {
      "epoch": 6.5235658085707735,
      "grad_norm": 6.637967586517334,
      "learning_rate": 4.1845542739286534e-05,
      "loss": 1.9427,
      "step": 85400
    },
    {
      "epoch": 6.531204644412192,
      "grad_norm": 4.853468894958496,
      "learning_rate": 4.183599419448476e-05,
      "loss": 1.9019,
      "step": 85500
    },
    {
      "epoch": 6.538843480253609,
      "grad_norm": 7.57349967956543,
      "learning_rate": 4.182644564968299e-05,
      "loss": 2.0049,
      "step": 85600
    },
    {
      "epoch": 6.546482316095027,
      "grad_norm": 7.171957969665527,
      "learning_rate": 4.181689710488122e-05,
      "loss": 1.9313,
      "step": 85700
    },
    {
      "epoch": 6.554121151936445,
      "grad_norm": 5.476834297180176,
      "learning_rate": 4.180734856007945e-05,
      "loss": 1.9735,
      "step": 85800
    },
    {
      "epoch": 6.5617599877778625,
      "grad_norm": 8.476651191711426,
      "learning_rate": 4.179780001527767e-05,
      "loss": 1.8766,
      "step": 85900
    },
    {
      "epoch": 6.569398823619281,
      "grad_norm": 8.496095657348633,
      "learning_rate": 4.1788251470475906e-05,
      "loss": 1.8979,
      "step": 86000
    },
    {
      "epoch": 6.577037659460698,
      "grad_norm": 7.45686674118042,
      "learning_rate": 4.177870292567413e-05,
      "loss": 1.852,
      "step": 86100
    },
    {
      "epoch": 6.584676495302116,
      "grad_norm": 11.81496810913086,
      "learning_rate": 4.176915438087236e-05,
      "loss": 1.9533,
      "step": 86200
    },
    {
      "epoch": 6.592315331143534,
      "grad_norm": 7.235648155212402,
      "learning_rate": 4.1759605836070585e-05,
      "loss": 1.8169,
      "step": 86300
    },
    {
      "epoch": 6.5999541669849515,
      "grad_norm": 7.336433410644531,
      "learning_rate": 4.1750057291268814e-05,
      "loss": 1.87,
      "step": 86400
    },
    {
      "epoch": 6.607593002826369,
      "grad_norm": 6.872750282287598,
      "learning_rate": 4.1740508746467036e-05,
      "loss": 1.9333,
      "step": 86500
    },
    {
      "epoch": 6.615231838667787,
      "grad_norm": 6.837333679199219,
      "learning_rate": 4.173096020166527e-05,
      "loss": 1.9059,
      "step": 86600
    },
    {
      "epoch": 6.622870674509205,
      "grad_norm": 8.98557186126709,
      "learning_rate": 4.172141165686349e-05,
      "loss": 1.8932,
      "step": 86700
    },
    {
      "epoch": 6.630509510350622,
      "grad_norm": 6.85732889175415,
      "learning_rate": 4.171186311206172e-05,
      "loss": 1.976,
      "step": 86800
    },
    {
      "epoch": 6.6381483461920405,
      "grad_norm": 7.461789131164551,
      "learning_rate": 4.170231456725995e-05,
      "loss": 1.9498,
      "step": 86900
    },
    {
      "epoch": 6.645787182033458,
      "grad_norm": 6.117241859436035,
      "learning_rate": 4.169276602245818e-05,
      "loss": 1.9221,
      "step": 87000
    },
    {
      "epoch": 6.653426017874876,
      "grad_norm": 7.47758150100708,
      "learning_rate": 4.168321747765641e-05,
      "loss": 1.9592,
      "step": 87100
    },
    {
      "epoch": 6.661064853716294,
      "grad_norm": 7.245948791503906,
      "learning_rate": 4.167366893285464e-05,
      "loss": 1.8858,
      "step": 87200
    },
    {
      "epoch": 6.668703689557711,
      "grad_norm": 5.874728679656982,
      "learning_rate": 4.1664120388052865e-05,
      "loss": 2.0189,
      "step": 87300
    },
    {
      "epoch": 6.6763425253991295,
      "grad_norm": 7.7914910316467285,
      "learning_rate": 4.165457184325109e-05,
      "loss": 1.9484,
      "step": 87400
    },
    {
      "epoch": 6.683981361240547,
      "grad_norm": 7.909430027008057,
      "learning_rate": 4.164502329844932e-05,
      "loss": 1.8694,
      "step": 87500
    },
    {
      "epoch": 6.691620197081964,
      "grad_norm": 5.933159351348877,
      "learning_rate": 4.1635474753647545e-05,
      "loss": 1.9413,
      "step": 87600
    },
    {
      "epoch": 6.699259032923383,
      "grad_norm": 7.5720438957214355,
      "learning_rate": 4.1625926208845773e-05,
      "loss": 1.8254,
      "step": 87700
    },
    {
      "epoch": 6.7068978687648,
      "grad_norm": 8.064353942871094,
      "learning_rate": 4.1616377664044e-05,
      "loss": 1.9439,
      "step": 87800
    },
    {
      "epoch": 6.714536704606218,
      "grad_norm": 6.204165458679199,
      "learning_rate": 4.160682911924223e-05,
      "loss": 1.8619,
      "step": 87900
    },
    {
      "epoch": 6.722175540447636,
      "grad_norm": 6.340297698974609,
      "learning_rate": 4.159728057444045e-05,
      "loss": 1.9384,
      "step": 88000
    },
    {
      "epoch": 6.729814376289053,
      "grad_norm": 5.755966663360596,
      "learning_rate": 4.158773202963869e-05,
      "loss": 1.9791,
      "step": 88100
    },
    {
      "epoch": 6.737453212130472,
      "grad_norm": 6.966954231262207,
      "learning_rate": 4.157818348483691e-05,
      "loss": 1.9357,
      "step": 88200
    },
    {
      "epoch": 6.745092047971889,
      "grad_norm": 6.085020542144775,
      "learning_rate": 4.156863494003514e-05,
      "loss": 1.8884,
      "step": 88300
    },
    {
      "epoch": 6.752730883813307,
      "grad_norm": 9.461395263671875,
      "learning_rate": 4.155908639523337e-05,
      "loss": 1.8886,
      "step": 88400
    },
    {
      "epoch": 6.760369719654725,
      "grad_norm": 7.6721062660217285,
      "learning_rate": 4.1549537850431596e-05,
      "loss": 1.8945,
      "step": 88500
    },
    {
      "epoch": 6.768008555496142,
      "grad_norm": 7.371833801269531,
      "learning_rate": 4.1539989305629825e-05,
      "loss": 1.8673,
      "step": 88600
    },
    {
      "epoch": 6.77564739133756,
      "grad_norm": 8.066665649414062,
      "learning_rate": 4.1530440760828054e-05,
      "loss": 1.9913,
      "step": 88700
    },
    {
      "epoch": 6.783286227178978,
      "grad_norm": 7.75169038772583,
      "learning_rate": 4.152089221602628e-05,
      "loss": 1.8635,
      "step": 88800
    },
    {
      "epoch": 6.790925063020396,
      "grad_norm": 6.0054144859313965,
      "learning_rate": 4.1511343671224504e-05,
      "loss": 2.0015,
      "step": 88900
    },
    {
      "epoch": 6.798563898861813,
      "grad_norm": 6.238811016082764,
      "learning_rate": 4.150179512642274e-05,
      "loss": 1.8922,
      "step": 89000
    },
    {
      "epoch": 6.8062027347032314,
      "grad_norm": 5.727021217346191,
      "learning_rate": 4.149224658162096e-05,
      "loss": 1.9518,
      "step": 89100
    },
    {
      "epoch": 6.813841570544649,
      "grad_norm": 6.055706024169922,
      "learning_rate": 4.148269803681919e-05,
      "loss": 1.8945,
      "step": 89200
    },
    {
      "epoch": 6.821480406386067,
      "grad_norm": 9.420087814331055,
      "learning_rate": 4.147314949201742e-05,
      "loss": 1.9133,
      "step": 89300
    },
    {
      "epoch": 6.829119242227485,
      "grad_norm": 6.720590591430664,
      "learning_rate": 4.146360094721565e-05,
      "loss": 1.8952,
      "step": 89400
    },
    {
      "epoch": 6.836758078068902,
      "grad_norm": 7.4601521492004395,
      "learning_rate": 4.1454052402413876e-05,
      "loss": 1.8607,
      "step": 89500
    },
    {
      "epoch": 6.84439691391032,
      "grad_norm": 7.076979160308838,
      "learning_rate": 4.14445038576121e-05,
      "loss": 1.8711,
      "step": 89600
    },
    {
      "epoch": 6.852035749751738,
      "grad_norm": 7.241462707519531,
      "learning_rate": 4.1434955312810334e-05,
      "loss": 1.8753,
      "step": 89700
    },
    {
      "epoch": 6.859674585593155,
      "grad_norm": 9.004343032836914,
      "learning_rate": 4.1425406768008556e-05,
      "loss": 2.0075,
      "step": 89800
    },
    {
      "epoch": 6.867313421434574,
      "grad_norm": 6.704357147216797,
      "learning_rate": 4.1415858223206784e-05,
      "loss": 2.0125,
      "step": 89900
    },
    {
      "epoch": 6.874952257275991,
      "grad_norm": 7.32601261138916,
      "learning_rate": 4.140630967840501e-05,
      "loss": 1.928,
      "step": 90000
    },
    {
      "epoch": 6.882591093117409,
      "grad_norm": 8.687419891357422,
      "learning_rate": 4.139676113360324e-05,
      "loss": 1.9677,
      "step": 90100
    },
    {
      "epoch": 6.890229928958827,
      "grad_norm": 6.943373203277588,
      "learning_rate": 4.1387212588801464e-05,
      "loss": 1.9241,
      "step": 90200
    },
    {
      "epoch": 6.897868764800244,
      "grad_norm": 6.364107608795166,
      "learning_rate": 4.13776640439997e-05,
      "loss": 1.887,
      "step": 90300
    },
    {
      "epoch": 6.905507600641663,
      "grad_norm": 7.611913204193115,
      "learning_rate": 4.136811549919792e-05,
      "loss": 1.879,
      "step": 90400
    },
    {
      "epoch": 6.91314643648308,
      "grad_norm": 7.49164342880249,
      "learning_rate": 4.135856695439615e-05,
      "loss": 1.9371,
      "step": 90500
    },
    {
      "epoch": 6.920785272324498,
      "grad_norm": 6.3699631690979,
      "learning_rate": 4.134901840959438e-05,
      "loss": 2.0319,
      "step": 90600
    },
    {
      "epoch": 6.928424108165915,
      "grad_norm": 8.229853630065918,
      "learning_rate": 4.133946986479261e-05,
      "loss": 1.8281,
      "step": 90700
    },
    {
      "epoch": 6.936062944007333,
      "grad_norm": 9.470013618469238,
      "learning_rate": 4.1329921319990836e-05,
      "loss": 2.0553,
      "step": 90800
    },
    {
      "epoch": 6.943701779848751,
      "grad_norm": 8.67546558380127,
      "learning_rate": 4.1320372775189065e-05,
      "loss": 1.9468,
      "step": 90900
    },
    {
      "epoch": 6.951340615690169,
      "grad_norm": 4.956768989562988,
      "learning_rate": 4.131082423038729e-05,
      "loss": 1.9553,
      "step": 91000
    },
    {
      "epoch": 6.958979451531587,
      "grad_norm": 10.878795623779297,
      "learning_rate": 4.1301275685585515e-05,
      "loss": 1.8833,
      "step": 91100
    },
    {
      "epoch": 6.966618287373004,
      "grad_norm": 6.5218000411987305,
      "learning_rate": 4.129172714078375e-05,
      "loss": 2.0165,
      "step": 91200
    },
    {
      "epoch": 6.974257123214422,
      "grad_norm": 7.992405414581299,
      "learning_rate": 4.128217859598197e-05,
      "loss": 1.8266,
      "step": 91300
    },
    {
      "epoch": 6.98189595905584,
      "grad_norm": 6.887451171875,
      "learning_rate": 4.12726300511802e-05,
      "loss": 1.96,
      "step": 91400
    },
    {
      "epoch": 6.989534794897258,
      "grad_norm": 6.1659650802612305,
      "learning_rate": 4.126308150637843e-05,
      "loss": 1.8838,
      "step": 91500
    },
    {
      "epoch": 6.997173630738676,
      "grad_norm": 5.208205223083496,
      "learning_rate": 4.125353296157666e-05,
      "loss": 1.9101,
      "step": 91600
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8913630247116089,
      "eval_runtime": 2.9989,
      "eval_samples_per_second": 230.087,
      "eval_steps_per_second": 230.087,
      "step": 91637
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.7092511653900146,
      "eval_runtime": 57.8408,
      "eval_samples_per_second": 226.328,
      "eval_steps_per_second": 226.328,
      "step": 91637
    },
    {
      "epoch": 7.004812466580093,
      "grad_norm": 6.932521343231201,
      "learning_rate": 4.124398441677488e-05,
      "loss": 1.8892,
      "step": 91700
    },
    {
      "epoch": 7.012451302421511,
      "grad_norm": 6.427302837371826,
      "learning_rate": 4.1234435871973116e-05,
      "loss": 1.8183,
      "step": 91800
    },
    {
      "epoch": 7.020090138262929,
      "grad_norm": 6.158998012542725,
      "learning_rate": 4.122488732717134e-05,
      "loss": 1.8829,
      "step": 91900
    },
    {
      "epoch": 7.027728974104346,
      "grad_norm": 6.991139888763428,
      "learning_rate": 4.1215338782369567e-05,
      "loss": 1.9157,
      "step": 92000
    },
    {
      "epoch": 7.035367809945765,
      "grad_norm": 7.17530632019043,
      "learning_rate": 4.1205790237567795e-05,
      "loss": 1.8419,
      "step": 92100
    },
    {
      "epoch": 7.043006645787182,
      "grad_norm": 6.022710800170898,
      "learning_rate": 4.1196241692766024e-05,
      "loss": 2.0433,
      "step": 92200
    },
    {
      "epoch": 7.0506454816285995,
      "grad_norm": 6.657947540283203,
      "learning_rate": 4.118669314796425e-05,
      "loss": 1.9207,
      "step": 92300
    },
    {
      "epoch": 7.058284317470018,
      "grad_norm": 5.211402893066406,
      "learning_rate": 4.117714460316248e-05,
      "loss": 2.0017,
      "step": 92400
    },
    {
      "epoch": 7.065923153311435,
      "grad_norm": 7.960660457611084,
      "learning_rate": 4.116759605836071e-05,
      "loss": 1.7492,
      "step": 92500
    },
    {
      "epoch": 7.073561989152853,
      "grad_norm": 6.219730377197266,
      "learning_rate": 4.115804751355893e-05,
      "loss": 2.0069,
      "step": 92600
    },
    {
      "epoch": 7.081200824994271,
      "grad_norm": 8.795258522033691,
      "learning_rate": 4.114849896875717e-05,
      "loss": 1.8998,
      "step": 92700
    },
    {
      "epoch": 7.0888396608356885,
      "grad_norm": 8.61971664428711,
      "learning_rate": 4.113895042395539e-05,
      "loss": 1.9345,
      "step": 92800
    },
    {
      "epoch": 7.096478496677107,
      "grad_norm": 6.234696388244629,
      "learning_rate": 4.112940187915362e-05,
      "loss": 1.8842,
      "step": 92900
    },
    {
      "epoch": 7.104117332518524,
      "grad_norm": 6.76055908203125,
      "learning_rate": 4.111985333435185e-05,
      "loss": 1.9317,
      "step": 93000
    },
    {
      "epoch": 7.111756168359942,
      "grad_norm": 7.445959091186523,
      "learning_rate": 4.1110304789550075e-05,
      "loss": 1.8688,
      "step": 93100
    },
    {
      "epoch": 7.11939500420136,
      "grad_norm": 5.848552703857422,
      "learning_rate": 4.1100756244748304e-05,
      "loss": 1.9318,
      "step": 93200
    },
    {
      "epoch": 7.1270338400427775,
      "grad_norm": 6.057756423950195,
      "learning_rate": 4.109120769994653e-05,
      "loss": 1.8848,
      "step": 93300
    },
    {
      "epoch": 7.134672675884195,
      "grad_norm": 7.712060451507568,
      "learning_rate": 4.108165915514476e-05,
      "loss": 1.8438,
      "step": 93400
    },
    {
      "epoch": 7.142311511725613,
      "grad_norm": 8.070381164550781,
      "learning_rate": 4.1072110610342983e-05,
      "loss": 1.9601,
      "step": 93500
    },
    {
      "epoch": 7.149950347567031,
      "grad_norm": 6.406667232513428,
      "learning_rate": 4.106256206554122e-05,
      "loss": 1.9016,
      "step": 93600
    },
    {
      "epoch": 7.157589183408448,
      "grad_norm": 8.080098152160645,
      "learning_rate": 4.105301352073944e-05,
      "loss": 1.9074,
      "step": 93700
    },
    {
      "epoch": 7.1652280192498665,
      "grad_norm": 6.708651542663574,
      "learning_rate": 4.104346497593767e-05,
      "loss": 1.8514,
      "step": 93800
    },
    {
      "epoch": 7.172866855091284,
      "grad_norm": 6.7183074951171875,
      "learning_rate": 4.10339164311359e-05,
      "loss": 1.8213,
      "step": 93900
    },
    {
      "epoch": 7.180505690932701,
      "grad_norm": 8.498520851135254,
      "learning_rate": 4.102436788633413e-05,
      "loss": 1.8668,
      "step": 94000
    },
    {
      "epoch": 7.18814452677412,
      "grad_norm": 7.96572732925415,
      "learning_rate": 4.101481934153235e-05,
      "loss": 1.9368,
      "step": 94100
    },
    {
      "epoch": 7.195783362615537,
      "grad_norm": 8.766860961914062,
      "learning_rate": 4.1005270796730584e-05,
      "loss": 1.918,
      "step": 94200
    },
    {
      "epoch": 7.2034221984569555,
      "grad_norm": 7.426931858062744,
      "learning_rate": 4.0995722251928806e-05,
      "loss": 1.8539,
      "step": 94300
    },
    {
      "epoch": 7.211061034298373,
      "grad_norm": 6.38274621963501,
      "learning_rate": 4.0986173707127035e-05,
      "loss": 1.8814,
      "step": 94400
    },
    {
      "epoch": 7.21869987013979,
      "grad_norm": 10.551604270935059,
      "learning_rate": 4.0976625162325264e-05,
      "loss": 1.8195,
      "step": 94500
    },
    {
      "epoch": 7.226338705981209,
      "grad_norm": 7.053380012512207,
      "learning_rate": 4.096707661752349e-05,
      "loss": 1.9008,
      "step": 94600
    },
    {
      "epoch": 7.233977541822626,
      "grad_norm": 6.207006454467773,
      "learning_rate": 4.095752807272172e-05,
      "loss": 1.9047,
      "step": 94700
    },
    {
      "epoch": 7.241616377664044,
      "grad_norm": 7.182300090789795,
      "learning_rate": 4.094797952791995e-05,
      "loss": 1.9938,
      "step": 94800
    },
    {
      "epoch": 7.249255213505462,
      "grad_norm": 6.7203874588012695,
      "learning_rate": 4.093843098311818e-05,
      "loss": 2.0022,
      "step": 94900
    },
    {
      "epoch": 7.256894049346879,
      "grad_norm": 10.05907154083252,
      "learning_rate": 4.09288824383164e-05,
      "loss": 1.8518,
      "step": 95000
    },
    {
      "epoch": 7.264532885188297,
      "grad_norm": 7.607400417327881,
      "learning_rate": 4.0919333893514636e-05,
      "loss": 1.9055,
      "step": 95100
    },
    {
      "epoch": 7.272171721029715,
      "grad_norm": 6.218204498291016,
      "learning_rate": 4.090978534871286e-05,
      "loss": 1.9059,
      "step": 95200
    },
    {
      "epoch": 7.279810556871133,
      "grad_norm": 7.508163928985596,
      "learning_rate": 4.0900236803911086e-05,
      "loss": 2.0816,
      "step": 95300
    },
    {
      "epoch": 7.287449392712551,
      "grad_norm": 12.678997993469238,
      "learning_rate": 4.089068825910931e-05,
      "loss": 1.8184,
      "step": 95400
    },
    {
      "epoch": 7.295088228553968,
      "grad_norm": 9.505678176879883,
      "learning_rate": 4.0881139714307544e-05,
      "loss": 1.9486,
      "step": 95500
    },
    {
      "epoch": 7.302727064395386,
      "grad_norm": 6.216363906860352,
      "learning_rate": 4.0871591169505766e-05,
      "loss": 2.0309,
      "step": 95600
    },
    {
      "epoch": 7.310365900236804,
      "grad_norm": 7.41878080368042,
      "learning_rate": 4.0862042624703994e-05,
      "loss": 1.9819,
      "step": 95700
    },
    {
      "epoch": 7.318004736078222,
      "grad_norm": 7.145654201507568,
      "learning_rate": 4.085249407990222e-05,
      "loss": 2.0067,
      "step": 95800
    },
    {
      "epoch": 7.325643571919639,
      "grad_norm": 5.664588928222656,
      "learning_rate": 4.084294553510045e-05,
      "loss": 1.954,
      "step": 95900
    },
    {
      "epoch": 7.333282407761057,
      "grad_norm": 6.4692864418029785,
      "learning_rate": 4.083339699029868e-05,
      "loss": 1.93,
      "step": 96000
    },
    {
      "epoch": 7.340921243602475,
      "grad_norm": 7.881568431854248,
      "learning_rate": 4.082384844549691e-05,
      "loss": 1.8849,
      "step": 96100
    },
    {
      "epoch": 7.348560079443892,
      "grad_norm": 7.278811454772949,
      "learning_rate": 4.081429990069514e-05,
      "loss": 1.913,
      "step": 96200
    },
    {
      "epoch": 7.356198915285311,
      "grad_norm": 9.083024978637695,
      "learning_rate": 4.080475135589336e-05,
      "loss": 1.877,
      "step": 96300
    },
    {
      "epoch": 7.363837751126728,
      "grad_norm": 6.05536413192749,
      "learning_rate": 4.0795202811091595e-05,
      "loss": 1.8406,
      "step": 96400
    },
    {
      "epoch": 7.3714765869681464,
      "grad_norm": 6.170466423034668,
      "learning_rate": 4.078565426628982e-05,
      "loss": 1.8806,
      "step": 96500
    },
    {
      "epoch": 7.379115422809564,
      "grad_norm": 8.576214790344238,
      "learning_rate": 4.0776105721488046e-05,
      "loss": 1.8889,
      "step": 96600
    },
    {
      "epoch": 7.386754258650981,
      "grad_norm": 5.960804462432861,
      "learning_rate": 4.0766557176686275e-05,
      "loss": 1.891,
      "step": 96700
    },
    {
      "epoch": 7.3943930944924,
      "grad_norm": 6.188625812530518,
      "learning_rate": 4.07570086318845e-05,
      "loss": 1.9233,
      "step": 96800
    },
    {
      "epoch": 7.402031930333817,
      "grad_norm": 6.06535530090332,
      "learning_rate": 4.0747460087082725e-05,
      "loss": 1.8542,
      "step": 96900
    },
    {
      "epoch": 7.409670766175235,
      "grad_norm": 6.934112071990967,
      "learning_rate": 4.073791154228096e-05,
      "loss": 1.9735,
      "step": 97000
    },
    {
      "epoch": 7.417309602016653,
      "grad_norm": 9.519174575805664,
      "learning_rate": 4.072836299747918e-05,
      "loss": 1.8936,
      "step": 97100
    },
    {
      "epoch": 7.42494843785807,
      "grad_norm": 7.354151725769043,
      "learning_rate": 4.071881445267741e-05,
      "loss": 1.9963,
      "step": 97200
    },
    {
      "epoch": 7.432587273699488,
      "grad_norm": 8.076648712158203,
      "learning_rate": 4.070926590787565e-05,
      "loss": 1.8102,
      "step": 97300
    },
    {
      "epoch": 7.440226109540906,
      "grad_norm": 6.066806316375732,
      "learning_rate": 4.069971736307387e-05,
      "loss": 1.8871,
      "step": 97400
    },
    {
      "epoch": 7.447864945382324,
      "grad_norm": 8.888785362243652,
      "learning_rate": 4.06901688182721e-05,
      "loss": 1.915,
      "step": 97500
    },
    {
      "epoch": 7.455503781223742,
      "grad_norm": 6.44157075881958,
      "learning_rate": 4.0680620273470326e-05,
      "loss": 1.8079,
      "step": 97600
    },
    {
      "epoch": 7.463142617065159,
      "grad_norm": 9.03123950958252,
      "learning_rate": 4.0671071728668555e-05,
      "loss": 1.9568,
      "step": 97700
    },
    {
      "epoch": 7.470781452906577,
      "grad_norm": 8.766897201538086,
      "learning_rate": 4.0661523183866777e-05,
      "loss": 1.9345,
      "step": 97800
    },
    {
      "epoch": 7.478420288747995,
      "grad_norm": 4.641652584075928,
      "learning_rate": 4.065197463906501e-05,
      "loss": 1.9092,
      "step": 97900
    },
    {
      "epoch": 7.486059124589413,
      "grad_norm": 9.084683418273926,
      "learning_rate": 4.0642426094263234e-05,
      "loss": 1.8957,
      "step": 98000
    },
    {
      "epoch": 7.49369796043083,
      "grad_norm": 7.0007734298706055,
      "learning_rate": 4.063287754946146e-05,
      "loss": 1.8585,
      "step": 98100
    },
    {
      "epoch": 7.501336796272248,
      "grad_norm": 8.322876930236816,
      "learning_rate": 4.062332900465969e-05,
      "loss": 1.8668,
      "step": 98200
    },
    {
      "epoch": 7.508975632113666,
      "grad_norm": 5.848152160644531,
      "learning_rate": 4.061378045985792e-05,
      "loss": 1.8486,
      "step": 98300
    },
    {
      "epoch": 7.516614467955083,
      "grad_norm": 6.196250915527344,
      "learning_rate": 4.060423191505615e-05,
      "loss": 1.9036,
      "step": 98400
    },
    {
      "epoch": 7.524253303796502,
      "grad_norm": 6.28348445892334,
      "learning_rate": 4.059468337025438e-05,
      "loss": 1.9008,
      "step": 98500
    },
    {
      "epoch": 7.531892139637919,
      "grad_norm": 7.236030578613281,
      "learning_rate": 4.0585134825452606e-05,
      "loss": 1.9302,
      "step": 98600
    },
    {
      "epoch": 7.539530975479337,
      "grad_norm": 5.648194313049316,
      "learning_rate": 4.057558628065083e-05,
      "loss": 1.8836,
      "step": 98700
    },
    {
      "epoch": 7.547169811320755,
      "grad_norm": 6.7764997482299805,
      "learning_rate": 4.0566037735849064e-05,
      "loss": 1.9395,
      "step": 98800
    },
    {
      "epoch": 7.554808647162172,
      "grad_norm": 7.252432346343994,
      "learning_rate": 4.0556489191047285e-05,
      "loss": 1.9088,
      "step": 98900
    },
    {
      "epoch": 7.562447483003591,
      "grad_norm": 6.0941925048828125,
      "learning_rate": 4.0546940646245514e-05,
      "loss": 1.8785,
      "step": 99000
    },
    {
      "epoch": 7.570086318845008,
      "grad_norm": 9.956002235412598,
      "learning_rate": 4.053739210144374e-05,
      "loss": 1.8053,
      "step": 99100
    },
    {
      "epoch": 7.5777251546864255,
      "grad_norm": 6.594324588775635,
      "learning_rate": 4.052784355664197e-05,
      "loss": 1.8949,
      "step": 99200
    },
    {
      "epoch": 7.585363990527844,
      "grad_norm": 6.864537239074707,
      "learning_rate": 4.0518295011840193e-05,
      "loss": 1.9165,
      "step": 99300
    },
    {
      "epoch": 7.593002826369261,
      "grad_norm": 9.640536308288574,
      "learning_rate": 4.050874646703843e-05,
      "loss": 1.9912,
      "step": 99400
    },
    {
      "epoch": 7.600641662210679,
      "grad_norm": 7.250792980194092,
      "learning_rate": 4.049919792223665e-05,
      "loss": 1.8496,
      "step": 99500
    },
    {
      "epoch": 7.608280498052097,
      "grad_norm": 8.725637435913086,
      "learning_rate": 4.048964937743488e-05,
      "loss": 1.8908,
      "step": 99600
    },
    {
      "epoch": 7.6159193338935145,
      "grad_norm": 6.160686492919922,
      "learning_rate": 4.048010083263311e-05,
      "loss": 1.864,
      "step": 99700
    },
    {
      "epoch": 7.623558169734933,
      "grad_norm": 8.03596305847168,
      "learning_rate": 4.047055228783134e-05,
      "loss": 1.947,
      "step": 99800
    },
    {
      "epoch": 7.63119700557635,
      "grad_norm": 7.6055426597595215,
      "learning_rate": 4.0461003743029566e-05,
      "loss": 1.9075,
      "step": 99900
    },
    {
      "epoch": 7.638835841417768,
      "grad_norm": 5.871568202972412,
      "learning_rate": 4.0451455198227794e-05,
      "loss": 1.9848,
      "step": 100000
    },
    {
      "epoch": 7.646474677259186,
      "grad_norm": 6.180854797363281,
      "learning_rate": 4.044190665342602e-05,
      "loss": 1.9297,
      "step": 100100
    },
    {
      "epoch": 7.6541135131006035,
      "grad_norm": 6.05702018737793,
      "learning_rate": 4.0432358108624245e-05,
      "loss": 1.8946,
      "step": 100200
    },
    {
      "epoch": 7.661752348942021,
      "grad_norm": 7.784659385681152,
      "learning_rate": 4.042280956382248e-05,
      "loss": 1.9405,
      "step": 100300
    },
    {
      "epoch": 7.669391184783439,
      "grad_norm": 8.33116340637207,
      "learning_rate": 4.04132610190207e-05,
      "loss": 1.8977,
      "step": 100400
    },
    {
      "epoch": 7.677030020624857,
      "grad_norm": 8.386066436767578,
      "learning_rate": 4.040371247421893e-05,
      "loss": 1.9205,
      "step": 100500
    },
    {
      "epoch": 7.684668856466274,
      "grad_norm": 5.732582092285156,
      "learning_rate": 4.039416392941716e-05,
      "loss": 1.832,
      "step": 100600
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 6.9475274085998535,
      "learning_rate": 4.038461538461539e-05,
      "loss": 1.8131,
      "step": 100700
    },
    {
      "epoch": 7.69994652814911,
      "grad_norm": 6.400876045227051,
      "learning_rate": 4.037506683981361e-05,
      "loss": 1.9557,
      "step": 100800
    },
    {
      "epoch": 7.707585363990528,
      "grad_norm": 8.037620544433594,
      "learning_rate": 4.0365518295011846e-05,
      "loss": 1.9327,
      "step": 100900
    },
    {
      "epoch": 7.715224199831946,
      "grad_norm": 8.677791595458984,
      "learning_rate": 4.035596975021007e-05,
      "loss": 1.9038,
      "step": 101000
    },
    {
      "epoch": 7.722863035673363,
      "grad_norm": 7.136337757110596,
      "learning_rate": 4.0346421205408296e-05,
      "loss": 1.9671,
      "step": 101100
    },
    {
      "epoch": 7.7305018715147815,
      "grad_norm": 4.773012161254883,
      "learning_rate": 4.0336872660606525e-05,
      "loss": 1.8622,
      "step": 101200
    },
    {
      "epoch": 7.738140707356199,
      "grad_norm": 8.2261323928833,
      "learning_rate": 4.0327324115804754e-05,
      "loss": 1.864,
      "step": 101300
    },
    {
      "epoch": 7.745779543197616,
      "grad_norm": 7.265775680541992,
      "learning_rate": 4.031777557100298e-05,
      "loss": 1.9654,
      "step": 101400
    },
    {
      "epoch": 7.753418379039035,
      "grad_norm": 8.69002914428711,
      "learning_rate": 4.0308227026201204e-05,
      "loss": 1.8385,
      "step": 101500
    },
    {
      "epoch": 7.761057214880452,
      "grad_norm": 6.8413543701171875,
      "learning_rate": 4.029867848139944e-05,
      "loss": 1.8089,
      "step": 101600
    },
    {
      "epoch": 7.76869605072187,
      "grad_norm": 7.541243553161621,
      "learning_rate": 4.028912993659766e-05,
      "loss": 1.9456,
      "step": 101700
    },
    {
      "epoch": 7.776334886563288,
      "grad_norm": 9.413260459899902,
      "learning_rate": 4.027958139179589e-05,
      "loss": 1.8986,
      "step": 101800
    },
    {
      "epoch": 7.783973722404705,
      "grad_norm": 7.666357517242432,
      "learning_rate": 4.027003284699412e-05,
      "loss": 1.9329,
      "step": 101900
    },
    {
      "epoch": 7.791612558246124,
      "grad_norm": 8.372316360473633,
      "learning_rate": 4.026048430219235e-05,
      "loss": 1.8814,
      "step": 102000
    },
    {
      "epoch": 7.799251394087541,
      "grad_norm": 8.546683311462402,
      "learning_rate": 4.0250935757390577e-05,
      "loss": 1.8974,
      "step": 102100
    },
    {
      "epoch": 7.806890229928959,
      "grad_norm": 5.898068428039551,
      "learning_rate": 4.0241387212588805e-05,
      "loss": 1.9245,
      "step": 102200
    },
    {
      "epoch": 7.814529065770377,
      "grad_norm": 5.68485164642334,
      "learning_rate": 4.0231838667787034e-05,
      "loss": 1.9423,
      "step": 102300
    },
    {
      "epoch": 7.822167901611794,
      "grad_norm": 5.412017345428467,
      "learning_rate": 4.0222290122985256e-05,
      "loss": 1.9185,
      "step": 102400
    },
    {
      "epoch": 7.829806737453212,
      "grad_norm": 7.513479709625244,
      "learning_rate": 4.021274157818349e-05,
      "loss": 1.7846,
      "step": 102500
    },
    {
      "epoch": 7.83744557329463,
      "grad_norm": 6.964447975158691,
      "learning_rate": 4.020319303338171e-05,
      "loss": 1.9571,
      "step": 102600
    },
    {
      "epoch": 7.845084409136048,
      "grad_norm": 6.706334114074707,
      "learning_rate": 4.019364448857994e-05,
      "loss": 1.9725,
      "step": 102700
    },
    {
      "epoch": 7.852723244977465,
      "grad_norm": 7.611640453338623,
      "learning_rate": 4.018409594377817e-05,
      "loss": 1.8651,
      "step": 102800
    },
    {
      "epoch": 7.860362080818883,
      "grad_norm": 7.732966423034668,
      "learning_rate": 4.01745473989764e-05,
      "loss": 1.8359,
      "step": 102900
    },
    {
      "epoch": 7.868000916660301,
      "grad_norm": 6.5699872970581055,
      "learning_rate": 4.016499885417462e-05,
      "loss": 1.8462,
      "step": 103000
    },
    {
      "epoch": 7.875639752501719,
      "grad_norm": 7.856704235076904,
      "learning_rate": 4.015545030937286e-05,
      "loss": 1.8733,
      "step": 103100
    },
    {
      "epoch": 7.883278588343137,
      "grad_norm": 4.298622131347656,
      "learning_rate": 4.014590176457108e-05,
      "loss": 1.9769,
      "step": 103200
    },
    {
      "epoch": 7.890917424184554,
      "grad_norm": 8.297654151916504,
      "learning_rate": 4.013635321976931e-05,
      "loss": 1.8735,
      "step": 103300
    },
    {
      "epoch": 7.8985562600259716,
      "grad_norm": 5.989462375640869,
      "learning_rate": 4.0126804674967536e-05,
      "loss": 1.9757,
      "step": 103400
    },
    {
      "epoch": 7.90619509586739,
      "grad_norm": 7.762766361236572,
      "learning_rate": 4.0117256130165765e-05,
      "loss": 1.9552,
      "step": 103500
    },
    {
      "epoch": 7.913833931708807,
      "grad_norm": 6.12166690826416,
      "learning_rate": 4.010770758536399e-05,
      "loss": 1.9369,
      "step": 103600
    },
    {
      "epoch": 7.921472767550226,
      "grad_norm": 5.359522342681885,
      "learning_rate": 4.009815904056222e-05,
      "loss": 1.9337,
      "step": 103700
    },
    {
      "epoch": 7.929111603391643,
      "grad_norm": 6.5337300300598145,
      "learning_rate": 4.008861049576045e-05,
      "loss": 1.8817,
      "step": 103800
    },
    {
      "epoch": 7.936750439233061,
      "grad_norm": 6.515676498413086,
      "learning_rate": 4.007906195095867e-05,
      "loss": 1.8102,
      "step": 103900
    },
    {
      "epoch": 7.944389275074479,
      "grad_norm": 7.667769432067871,
      "learning_rate": 4.006951340615691e-05,
      "loss": 1.9503,
      "step": 104000
    },
    {
      "epoch": 7.952028110915896,
      "grad_norm": 7.604263782501221,
      "learning_rate": 4.005996486135513e-05,
      "loss": 1.894,
      "step": 104100
    },
    {
      "epoch": 7.959666946757315,
      "grad_norm": 7.073005199432373,
      "learning_rate": 4.005041631655336e-05,
      "loss": 2.0075,
      "step": 104200
    },
    {
      "epoch": 7.967305782598732,
      "grad_norm": 8.985316276550293,
      "learning_rate": 4.004086777175159e-05,
      "loss": 1.9329,
      "step": 104300
    },
    {
      "epoch": 7.97494461844015,
      "grad_norm": 7.686166763305664,
      "learning_rate": 4.0031319226949816e-05,
      "loss": 1.9738,
      "step": 104400
    },
    {
      "epoch": 7.982583454281567,
      "grad_norm": 7.622411251068115,
      "learning_rate": 4.002177068214804e-05,
      "loss": 1.9224,
      "step": 104500
    },
    {
      "epoch": 7.990222290122985,
      "grad_norm": 5.347498416900635,
      "learning_rate": 4.0012222137346274e-05,
      "loss": 1.9507,
      "step": 104600
    },
    {
      "epoch": 7.997861125964403,
      "grad_norm": 4.974489212036133,
      "learning_rate": 4.0002673592544495e-05,
      "loss": 1.9244,
      "step": 104700
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8740402460098267,
      "eval_runtime": 3.0618,
      "eval_samples_per_second": 225.358,
      "eval_steps_per_second": 225.358,
      "step": 104728
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.6848863363265991,
      "eval_runtime": 58.1864,
      "eval_samples_per_second": 224.984,
      "eval_steps_per_second": 224.984,
      "step": 104728
    },
    {
      "epoch": 8.005499961805821,
      "grad_norm": 7.3907670974731445,
      "learning_rate": 3.9993125047742724e-05,
      "loss": 1.9201,
      "step": 104800
    },
    {
      "epoch": 8.013138797647239,
      "grad_norm": 6.791952610015869,
      "learning_rate": 3.998357650294095e-05,
      "loss": 1.8614,
      "step": 104900
    },
    {
      "epoch": 8.020777633488656,
      "grad_norm": 5.657793045043945,
      "learning_rate": 3.997402795813918e-05,
      "loss": 1.854,
      "step": 105000
    },
    {
      "epoch": 8.028416469330073,
      "grad_norm": 6.661211013793945,
      "learning_rate": 3.996447941333741e-05,
      "loss": 1.8711,
      "step": 105100
    },
    {
      "epoch": 8.036055305171493,
      "grad_norm": 6.627518177032471,
      "learning_rate": 3.995493086853564e-05,
      "loss": 1.8808,
      "step": 105200
    },
    {
      "epoch": 8.04369414101291,
      "grad_norm": 7.4173808097839355,
      "learning_rate": 3.994538232373387e-05,
      "loss": 1.9262,
      "step": 105300
    },
    {
      "epoch": 8.051332976854328,
      "grad_norm": 5.119053840637207,
      "learning_rate": 3.993583377893209e-05,
      "loss": 1.915,
      "step": 105400
    },
    {
      "epoch": 8.058971812695745,
      "grad_norm": 5.93503475189209,
      "learning_rate": 3.9926285234130325e-05,
      "loss": 1.9438,
      "step": 105500
    },
    {
      "epoch": 8.066610648537162,
      "grad_norm": 3.209401845932007,
      "learning_rate": 3.991673668932855e-05,
      "loss": 1.8717,
      "step": 105600
    },
    {
      "epoch": 8.07424948437858,
      "grad_norm": 7.119684219360352,
      "learning_rate": 3.9907188144526776e-05,
      "loss": 1.9568,
      "step": 105700
    },
    {
      "epoch": 8.08188832022,
      "grad_norm": 7.428574085235596,
      "learning_rate": 3.9897639599725004e-05,
      "loss": 1.8568,
      "step": 105800
    },
    {
      "epoch": 8.089527156061417,
      "grad_norm": 6.651711940765381,
      "learning_rate": 3.988809105492323e-05,
      "loss": 1.8542,
      "step": 105900
    },
    {
      "epoch": 8.097165991902834,
      "grad_norm": 7.854244232177734,
      "learning_rate": 3.9878542510121455e-05,
      "loss": 1.8829,
      "step": 106000
    },
    {
      "epoch": 8.104804827744251,
      "grad_norm": 7.359543323516846,
      "learning_rate": 3.986899396531969e-05,
      "loss": 1.9042,
      "step": 106100
    },
    {
      "epoch": 8.112443663585669,
      "grad_norm": 6.333889007568359,
      "learning_rate": 3.985944542051792e-05,
      "loss": 1.853,
      "step": 106200
    },
    {
      "epoch": 8.120082499427088,
      "grad_norm": 6.736392021179199,
      "learning_rate": 3.984989687571614e-05,
      "loss": 1.8664,
      "step": 106300
    },
    {
      "epoch": 8.127721335268506,
      "grad_norm": 6.361771583557129,
      "learning_rate": 3.9840348330914376e-05,
      "loss": 1.8389,
      "step": 106400
    },
    {
      "epoch": 8.135360171109923,
      "grad_norm": 9.79892635345459,
      "learning_rate": 3.98307997861126e-05,
      "loss": 1.8046,
      "step": 106500
    },
    {
      "epoch": 8.14299900695134,
      "grad_norm": 8.218104362487793,
      "learning_rate": 3.982125124131083e-05,
      "loss": 1.7747,
      "step": 106600
    },
    {
      "epoch": 8.150637842792758,
      "grad_norm": 7.302126407623291,
      "learning_rate": 3.981170269650905e-05,
      "loss": 1.9796,
      "step": 106700
    },
    {
      "epoch": 8.158276678634175,
      "grad_norm": 5.342555999755859,
      "learning_rate": 3.9802154151707284e-05,
      "loss": 1.9677,
      "step": 106800
    },
    {
      "epoch": 8.165915514475595,
      "grad_norm": 6.39257287979126,
      "learning_rate": 3.9792605606905506e-05,
      "loss": 1.8577,
      "step": 106900
    },
    {
      "epoch": 8.173554350317012,
      "grad_norm": 8.983709335327148,
      "learning_rate": 3.9783057062103735e-05,
      "loss": 1.9912,
      "step": 107000
    },
    {
      "epoch": 8.18119318615843,
      "grad_norm": 7.216982841491699,
      "learning_rate": 3.9773508517301964e-05,
      "loss": 1.8629,
      "step": 107100
    },
    {
      "epoch": 8.188832021999847,
      "grad_norm": 10.208097457885742,
      "learning_rate": 3.976395997250019e-05,
      "loss": 1.8556,
      "step": 107200
    },
    {
      "epoch": 8.196470857841264,
      "grad_norm": 6.35841703414917,
      "learning_rate": 3.975441142769842e-05,
      "loss": 1.8756,
      "step": 107300
    },
    {
      "epoch": 8.204109693682684,
      "grad_norm": 7.2651777267456055,
      "learning_rate": 3.974486288289665e-05,
      "loss": 1.9212,
      "step": 107400
    },
    {
      "epoch": 8.211748529524101,
      "grad_norm": 6.866548538208008,
      "learning_rate": 3.973531433809488e-05,
      "loss": 1.871,
      "step": 107500
    },
    {
      "epoch": 8.219387365365519,
      "grad_norm": 6.816832065582275,
      "learning_rate": 3.97257657932931e-05,
      "loss": 1.9264,
      "step": 107600
    },
    {
      "epoch": 8.227026201206936,
      "grad_norm": 7.62563943862915,
      "learning_rate": 3.9716217248491336e-05,
      "loss": 1.9088,
      "step": 107700
    },
    {
      "epoch": 8.234665037048353,
      "grad_norm": 9.884265899658203,
      "learning_rate": 3.970666870368956e-05,
      "loss": 1.9152,
      "step": 107800
    },
    {
      "epoch": 8.24230387288977,
      "grad_norm": 7.094790935516357,
      "learning_rate": 3.9697120158887787e-05,
      "loss": 1.8622,
      "step": 107900
    },
    {
      "epoch": 8.24994270873119,
      "grad_norm": 6.558166027069092,
      "learning_rate": 3.9687571614086015e-05,
      "loss": 1.9326,
      "step": 108000
    },
    {
      "epoch": 8.257581544572608,
      "grad_norm": 6.865811347961426,
      "learning_rate": 3.9678023069284244e-05,
      "loss": 1.8545,
      "step": 108100
    },
    {
      "epoch": 8.265220380414025,
      "grad_norm": 8.12035083770752,
      "learning_rate": 3.9668474524482466e-05,
      "loss": 1.8344,
      "step": 108200
    },
    {
      "epoch": 8.272859216255442,
      "grad_norm": 7.868272304534912,
      "learning_rate": 3.96589259796807e-05,
      "loss": 1.8378,
      "step": 108300
    },
    {
      "epoch": 8.28049805209686,
      "grad_norm": 7.348315715789795,
      "learning_rate": 3.964937743487892e-05,
      "loss": 1.929,
      "step": 108400
    },
    {
      "epoch": 8.288136887938279,
      "grad_norm": 8.818187713623047,
      "learning_rate": 3.963982889007715e-05,
      "loss": 1.9339,
      "step": 108500
    },
    {
      "epoch": 8.295775723779697,
      "grad_norm": 8.826532363891602,
      "learning_rate": 3.963028034527538e-05,
      "loss": 1.958,
      "step": 108600
    },
    {
      "epoch": 8.303414559621114,
      "grad_norm": 9.114503860473633,
      "learning_rate": 3.962073180047361e-05,
      "loss": 1.8071,
      "step": 108700
    },
    {
      "epoch": 8.311053395462531,
      "grad_norm": 6.113762855529785,
      "learning_rate": 3.961118325567184e-05,
      "loss": 1.9154,
      "step": 108800
    },
    {
      "epoch": 8.318692231303949,
      "grad_norm": 5.867588043212891,
      "learning_rate": 3.960163471087007e-05,
      "loss": 2.0005,
      "step": 108900
    },
    {
      "epoch": 8.326331067145366,
      "grad_norm": 6.136462211608887,
      "learning_rate": 3.9592086166068295e-05,
      "loss": 1.9744,
      "step": 109000
    },
    {
      "epoch": 8.333969902986786,
      "grad_norm": 7.160208225250244,
      "learning_rate": 3.958253762126652e-05,
      "loss": 1.9355,
      "step": 109100
    },
    {
      "epoch": 8.341608738828203,
      "grad_norm": 9.849566459655762,
      "learning_rate": 3.957298907646475e-05,
      "loss": 1.8866,
      "step": 109200
    },
    {
      "epoch": 8.34924757466962,
      "grad_norm": 6.370260715484619,
      "learning_rate": 3.9563440531662975e-05,
      "loss": 1.8334,
      "step": 109300
    },
    {
      "epoch": 8.356886410511038,
      "grad_norm": 6.12431001663208,
      "learning_rate": 3.95538919868612e-05,
      "loss": 1.8979,
      "step": 109400
    },
    {
      "epoch": 8.364525246352455,
      "grad_norm": 8.905058860778809,
      "learning_rate": 3.954434344205943e-05,
      "loss": 1.7918,
      "step": 109500
    },
    {
      "epoch": 8.372164082193873,
      "grad_norm": 6.47322416305542,
      "learning_rate": 3.953479489725766e-05,
      "loss": 1.9078,
      "step": 109600
    },
    {
      "epoch": 8.379802918035292,
      "grad_norm": 7.281745910644531,
      "learning_rate": 3.952524635245588e-05,
      "loss": 1.9862,
      "step": 109700
    },
    {
      "epoch": 8.38744175387671,
      "grad_norm": 6.587599754333496,
      "learning_rate": 3.951569780765412e-05,
      "loss": 1.8953,
      "step": 109800
    },
    {
      "epoch": 8.395080589718127,
      "grad_norm": 7.781975269317627,
      "learning_rate": 3.950614926285234e-05,
      "loss": 1.8105,
      "step": 109900
    },
    {
      "epoch": 8.402719425559544,
      "grad_norm": 7.487137794494629,
      "learning_rate": 3.949660071805057e-05,
      "loss": 1.877,
      "step": 110000
    },
    {
      "epoch": 8.410358261400962,
      "grad_norm": 6.340487957000732,
      "learning_rate": 3.94870521732488e-05,
      "loss": 1.9517,
      "step": 110100
    },
    {
      "epoch": 8.417997097242381,
      "grad_norm": 7.213089942932129,
      "learning_rate": 3.9477503628447026e-05,
      "loss": 1.8611,
      "step": 110200
    },
    {
      "epoch": 8.425635933083798,
      "grad_norm": 5.665067672729492,
      "learning_rate": 3.9467955083645255e-05,
      "loss": 1.9275,
      "step": 110300
    },
    {
      "epoch": 8.433274768925216,
      "grad_norm": 6.735391616821289,
      "learning_rate": 3.9458406538843484e-05,
      "loss": 1.9364,
      "step": 110400
    },
    {
      "epoch": 8.440913604766633,
      "grad_norm": 6.6625165939331055,
      "learning_rate": 3.944885799404171e-05,
      "loss": 1.9108,
      "step": 110500
    },
    {
      "epoch": 8.44855244060805,
      "grad_norm": 7.028385162353516,
      "learning_rate": 3.9439309449239934e-05,
      "loss": 1.9646,
      "step": 110600
    },
    {
      "epoch": 8.45619127644947,
      "grad_norm": 9.916251182556152,
      "learning_rate": 3.942976090443817e-05,
      "loss": 1.8526,
      "step": 110700
    },
    {
      "epoch": 8.463830112290887,
      "grad_norm": 7.446462631225586,
      "learning_rate": 3.942021235963639e-05,
      "loss": 1.8819,
      "step": 110800
    },
    {
      "epoch": 8.471468948132305,
      "grad_norm": 7.576835632324219,
      "learning_rate": 3.941066381483462e-05,
      "loss": 1.9291,
      "step": 110900
    },
    {
      "epoch": 8.479107783973722,
      "grad_norm": 6.80585241317749,
      "learning_rate": 3.940111527003285e-05,
      "loss": 1.9482,
      "step": 111000
    },
    {
      "epoch": 8.48674661981514,
      "grad_norm": 8.397109985351562,
      "learning_rate": 3.939156672523108e-05,
      "loss": 1.9299,
      "step": 111100
    },
    {
      "epoch": 8.494385455656557,
      "grad_norm": 6.896072864532471,
      "learning_rate": 3.9382018180429306e-05,
      "loss": 1.8515,
      "step": 111200
    },
    {
      "epoch": 8.502024291497976,
      "grad_norm": 6.7842206954956055,
      "learning_rate": 3.9372469635627535e-05,
      "loss": 1.8498,
      "step": 111300
    },
    {
      "epoch": 8.509663127339394,
      "grad_norm": 6.4165449142456055,
      "learning_rate": 3.9362921090825764e-05,
      "loss": 1.9243,
      "step": 111400
    },
    {
      "epoch": 8.517301963180811,
      "grad_norm": 6.893211841583252,
      "learning_rate": 3.9353372546023986e-05,
      "loss": 1.8055,
      "step": 111500
    },
    {
      "epoch": 8.524940799022229,
      "grad_norm": 8.69597053527832,
      "learning_rate": 3.934382400122222e-05,
      "loss": 1.9339,
      "step": 111600
    },
    {
      "epoch": 8.532579634863646,
      "grad_norm": 6.884964942932129,
      "learning_rate": 3.933427545642044e-05,
      "loss": 1.8557,
      "step": 111700
    },
    {
      "epoch": 8.540218470705064,
      "grad_norm": 7.719566345214844,
      "learning_rate": 3.932472691161867e-05,
      "loss": 1.8688,
      "step": 111800
    },
    {
      "epoch": 8.547857306546483,
      "grad_norm": 5.11598014831543,
      "learning_rate": 3.93151783668169e-05,
      "loss": 1.9059,
      "step": 111900
    },
    {
      "epoch": 8.5554961423879,
      "grad_norm": 6.1615400314331055,
      "learning_rate": 3.930562982201513e-05,
      "loss": 1.8533,
      "step": 112000
    },
    {
      "epoch": 8.563134978229318,
      "grad_norm": 7.095123767852783,
      "learning_rate": 3.929608127721335e-05,
      "loss": 1.9158,
      "step": 112100
    },
    {
      "epoch": 8.570773814070735,
      "grad_norm": 11.11430549621582,
      "learning_rate": 3.9286532732411586e-05,
      "loss": 1.9522,
      "step": 112200
    },
    {
      "epoch": 8.578412649912153,
      "grad_norm": 5.468932628631592,
      "learning_rate": 3.927698418760981e-05,
      "loss": 1.9004,
      "step": 112300
    },
    {
      "epoch": 8.586051485753572,
      "grad_norm": 6.322302341461182,
      "learning_rate": 3.926743564280804e-05,
      "loss": 1.9001,
      "step": 112400
    },
    {
      "epoch": 8.59369032159499,
      "grad_norm": 7.594686508178711,
      "learning_rate": 3.9257887098006266e-05,
      "loss": 1.8404,
      "step": 112500
    },
    {
      "epoch": 8.601329157436407,
      "grad_norm": 6.646020412445068,
      "learning_rate": 3.9248338553204494e-05,
      "loss": 1.9015,
      "step": 112600
    },
    {
      "epoch": 8.608967993277824,
      "grad_norm": 9.000675201416016,
      "learning_rate": 3.923879000840272e-05,
      "loss": 1.822,
      "step": 112700
    },
    {
      "epoch": 8.616606829119242,
      "grad_norm": 8.228757858276367,
      "learning_rate": 3.9229241463600945e-05,
      "loss": 1.8488,
      "step": 112800
    },
    {
      "epoch": 8.624245664960661,
      "grad_norm": 8.474183082580566,
      "learning_rate": 3.921969291879918e-05,
      "loss": 2.0328,
      "step": 112900
    },
    {
      "epoch": 8.631884500802078,
      "grad_norm": 5.269909858703613,
      "learning_rate": 3.92101443739974e-05,
      "loss": 1.8034,
      "step": 113000
    },
    {
      "epoch": 8.639523336643496,
      "grad_norm": 8.857209205627441,
      "learning_rate": 3.920059582919563e-05,
      "loss": 1.9153,
      "step": 113100
    },
    {
      "epoch": 8.647162172484913,
      "grad_norm": 5.47458553314209,
      "learning_rate": 3.919104728439386e-05,
      "loss": 1.8601,
      "step": 113200
    },
    {
      "epoch": 8.65480100832633,
      "grad_norm": 10.326153755187988,
      "learning_rate": 3.918149873959209e-05,
      "loss": 1.9446,
      "step": 113300
    },
    {
      "epoch": 8.662439844167748,
      "grad_norm": 5.72794771194458,
      "learning_rate": 3.917195019479031e-05,
      "loss": 1.9336,
      "step": 113400
    },
    {
      "epoch": 8.670078680009167,
      "grad_norm": 8.302186965942383,
      "learning_rate": 3.9162401649988546e-05,
      "loss": 1.9507,
      "step": 113500
    },
    {
      "epoch": 8.677717515850585,
      "grad_norm": 7.666833877563477,
      "learning_rate": 3.915285310518677e-05,
      "loss": 1.9545,
      "step": 113600
    },
    {
      "epoch": 8.685356351692002,
      "grad_norm": 7.432289123535156,
      "learning_rate": 3.9143304560384997e-05,
      "loss": 1.9562,
      "step": 113700
    },
    {
      "epoch": 8.69299518753342,
      "grad_norm": 8.397261619567871,
      "learning_rate": 3.9133756015583225e-05,
      "loss": 1.8765,
      "step": 113800
    },
    {
      "epoch": 8.700634023374837,
      "grad_norm": 9.030607223510742,
      "learning_rate": 3.9124207470781454e-05,
      "loss": 1.8745,
      "step": 113900
    },
    {
      "epoch": 8.708272859216255,
      "grad_norm": 5.546651363372803,
      "learning_rate": 3.911465892597968e-05,
      "loss": 2.004,
      "step": 114000
    },
    {
      "epoch": 8.715911695057674,
      "grad_norm": 7.382510185241699,
      "learning_rate": 3.910511038117791e-05,
      "loss": 1.9403,
      "step": 114100
    },
    {
      "epoch": 8.723550530899091,
      "grad_norm": 6.479975700378418,
      "learning_rate": 3.909556183637614e-05,
      "loss": 1.8881,
      "step": 114200
    },
    {
      "epoch": 8.731189366740509,
      "grad_norm": 8.03995132446289,
      "learning_rate": 3.908601329157436e-05,
      "loss": 1.9251,
      "step": 114300
    },
    {
      "epoch": 8.738828202581926,
      "grad_norm": 7.698390007019043,
      "learning_rate": 3.90764647467726e-05,
      "loss": 1.8886,
      "step": 114400
    },
    {
      "epoch": 8.746467038423344,
      "grad_norm": 7.368738174438477,
      "learning_rate": 3.906691620197082e-05,
      "loss": 1.846,
      "step": 114500
    },
    {
      "epoch": 8.754105874264763,
      "grad_norm": 6.531544208526611,
      "learning_rate": 3.905736765716905e-05,
      "loss": 1.8791,
      "step": 114600
    },
    {
      "epoch": 8.76174471010618,
      "grad_norm": 6.104403495788574,
      "learning_rate": 3.904781911236728e-05,
      "loss": 1.8486,
      "step": 114700
    },
    {
      "epoch": 8.769383545947598,
      "grad_norm": 7.05229377746582,
      "learning_rate": 3.9038270567565505e-05,
      "loss": 1.8618,
      "step": 114800
    },
    {
      "epoch": 8.777022381789015,
      "grad_norm": 6.806093215942383,
      "learning_rate": 3.9028722022763734e-05,
      "loss": 1.8588,
      "step": 114900
    },
    {
      "epoch": 8.784661217630433,
      "grad_norm": 7.4027791023254395,
      "learning_rate": 3.901917347796196e-05,
      "loss": 1.7898,
      "step": 115000
    },
    {
      "epoch": 8.79230005347185,
      "grad_norm": 8.029460906982422,
      "learning_rate": 3.900962493316019e-05,
      "loss": 1.8693,
      "step": 115100
    },
    {
      "epoch": 8.79993888931327,
      "grad_norm": 7.686296463012695,
      "learning_rate": 3.900007638835841e-05,
      "loss": 1.874,
      "step": 115200
    },
    {
      "epoch": 8.807577725154687,
      "grad_norm": 6.934915542602539,
      "learning_rate": 3.899052784355665e-05,
      "loss": 1.8864,
      "step": 115300
    },
    {
      "epoch": 8.815216560996104,
      "grad_norm": 6.947837829589844,
      "learning_rate": 3.898097929875487e-05,
      "loss": 1.9323,
      "step": 115400
    },
    {
      "epoch": 8.822855396837522,
      "grad_norm": 7.605008602142334,
      "learning_rate": 3.89714307539531e-05,
      "loss": 1.8685,
      "step": 115500
    },
    {
      "epoch": 8.830494232678939,
      "grad_norm": 5.987123012542725,
      "learning_rate": 3.896188220915133e-05,
      "loss": 1.8467,
      "step": 115600
    },
    {
      "epoch": 8.838133068520358,
      "grad_norm": 4.49821662902832,
      "learning_rate": 3.895233366434956e-05,
      "loss": 1.8969,
      "step": 115700
    },
    {
      "epoch": 8.845771904361776,
      "grad_norm": 6.457118034362793,
      "learning_rate": 3.894278511954778e-05,
      "loss": 1.8798,
      "step": 115800
    },
    {
      "epoch": 8.853410740203193,
      "grad_norm": 8.133734703063965,
      "learning_rate": 3.8933236574746014e-05,
      "loss": 1.9292,
      "step": 115900
    },
    {
      "epoch": 8.86104957604461,
      "grad_norm": 6.973058223724365,
      "learning_rate": 3.8923688029944236e-05,
      "loss": 1.9438,
      "step": 116000
    },
    {
      "epoch": 8.868688411886028,
      "grad_norm": 5.9709649085998535,
      "learning_rate": 3.8914139485142465e-05,
      "loss": 1.8853,
      "step": 116100
    },
    {
      "epoch": 8.876327247727446,
      "grad_norm": 9.76035213470459,
      "learning_rate": 3.8904590940340694e-05,
      "loss": 1.85,
      "step": 116200
    },
    {
      "epoch": 8.883966083568865,
      "grad_norm": 6.897641658782959,
      "learning_rate": 3.889504239553892e-05,
      "loss": 1.8787,
      "step": 116300
    },
    {
      "epoch": 8.891604919410282,
      "grad_norm": 6.446702003479004,
      "learning_rate": 3.888549385073715e-05,
      "loss": 1.8675,
      "step": 116400
    },
    {
      "epoch": 8.8992437552517,
      "grad_norm": 5.5054731369018555,
      "learning_rate": 3.887594530593538e-05,
      "loss": 1.8786,
      "step": 116500
    },
    {
      "epoch": 8.906882591093117,
      "grad_norm": 7.537604808807373,
      "learning_rate": 3.886639676113361e-05,
      "loss": 1.9299,
      "step": 116600
    },
    {
      "epoch": 8.914521426934535,
      "grad_norm": 7.923770904541016,
      "learning_rate": 3.885684821633183e-05,
      "loss": 1.9001,
      "step": 116700
    },
    {
      "epoch": 8.922160262775954,
      "grad_norm": 6.256862640380859,
      "learning_rate": 3.8847299671530066e-05,
      "loss": 1.8088,
      "step": 116800
    },
    {
      "epoch": 8.929799098617371,
      "grad_norm": 7.23984956741333,
      "learning_rate": 3.883775112672829e-05,
      "loss": 1.8998,
      "step": 116900
    },
    {
      "epoch": 8.937437934458789,
      "grad_norm": 8.56287956237793,
      "learning_rate": 3.8828202581926516e-05,
      "loss": 1.8393,
      "step": 117000
    },
    {
      "epoch": 8.945076770300206,
      "grad_norm": 6.151912212371826,
      "learning_rate": 3.8818654037124745e-05,
      "loss": 1.9055,
      "step": 117100
    },
    {
      "epoch": 8.952715606141624,
      "grad_norm": 6.146624565124512,
      "learning_rate": 3.8809105492322974e-05,
      "loss": 1.7918,
      "step": 117200
    },
    {
      "epoch": 8.960354441983041,
      "grad_norm": 6.025155544281006,
      "learning_rate": 3.8799556947521196e-05,
      "loss": 1.8701,
      "step": 117300
    },
    {
      "epoch": 8.96799327782446,
      "grad_norm": 4.958206653594971,
      "learning_rate": 3.879000840271943e-05,
      "loss": 1.7928,
      "step": 117400
    },
    {
      "epoch": 8.975632113665878,
      "grad_norm": 5.605995178222656,
      "learning_rate": 3.878045985791765e-05,
      "loss": 1.9038,
      "step": 117500
    },
    {
      "epoch": 8.983270949507295,
      "grad_norm": 9.76594066619873,
      "learning_rate": 3.877091131311588e-05,
      "loss": 1.8753,
      "step": 117600
    },
    {
      "epoch": 8.990909785348713,
      "grad_norm": 7.737526893615723,
      "learning_rate": 3.876136276831411e-05,
      "loss": 1.892,
      "step": 117700
    },
    {
      "epoch": 8.99854862119013,
      "grad_norm": 6.636022090911865,
      "learning_rate": 3.875181422351234e-05,
      "loss": 1.882,
      "step": 117800
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8690009117126465,
      "eval_runtime": 3.0008,
      "eval_samples_per_second": 229.937,
      "eval_steps_per_second": 229.937,
      "step": 117819
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.6703754663467407,
      "eval_runtime": 56.5542,
      "eval_samples_per_second": 231.477,
      "eval_steps_per_second": 231.477,
      "step": 117819
    },
    {
      "epoch": 9.00618745703155,
      "grad_norm": 6.747211933135986,
      "learning_rate": 3.874226567871057e-05,
      "loss": 1.7932,
      "step": 117900
    },
    {
      "epoch": 9.013826292872967,
      "grad_norm": 5.9481682777404785,
      "learning_rate": 3.8732717133908796e-05,
      "loss": 1.9173,
      "step": 118000
    },
    {
      "epoch": 9.021465128714384,
      "grad_norm": 7.05121374130249,
      "learning_rate": 3.8723168589107025e-05,
      "loss": 1.8036,
      "step": 118100
    },
    {
      "epoch": 9.029103964555802,
      "grad_norm": 8.426045417785645,
      "learning_rate": 3.871362004430525e-05,
      "loss": 1.8932,
      "step": 118200
    },
    {
      "epoch": 9.036742800397219,
      "grad_norm": 6.716715335845947,
      "learning_rate": 3.8704071499503476e-05,
      "loss": 1.8316,
      "step": 118300
    },
    {
      "epoch": 9.044381636238636,
      "grad_norm": 6.4266886711120605,
      "learning_rate": 3.8694522954701704e-05,
      "loss": 1.7899,
      "step": 118400
    },
    {
      "epoch": 9.052020472080056,
      "grad_norm": 6.269611358642578,
      "learning_rate": 3.868497440989993e-05,
      "loss": 1.8958,
      "step": 118500
    },
    {
      "epoch": 9.059659307921473,
      "grad_norm": 7.0102410316467285,
      "learning_rate": 3.8675425865098155e-05,
      "loss": 1.8691,
      "step": 118600
    },
    {
      "epoch": 9.06729814376289,
      "grad_norm": 8.516416549682617,
      "learning_rate": 3.866587732029639e-05,
      "loss": 1.9311,
      "step": 118700
    },
    {
      "epoch": 9.074936979604308,
      "grad_norm": 6.952121734619141,
      "learning_rate": 3.865632877549461e-05,
      "loss": 1.7745,
      "step": 118800
    },
    {
      "epoch": 9.082575815445725,
      "grad_norm": 6.414597034454346,
      "learning_rate": 3.864678023069284e-05,
      "loss": 2.0035,
      "step": 118900
    },
    {
      "epoch": 9.090214651287145,
      "grad_norm": 5.515068054199219,
      "learning_rate": 3.8637231685891077e-05,
      "loss": 1.7935,
      "step": 119000
    },
    {
      "epoch": 9.097853487128562,
      "grad_norm": 7.817251205444336,
      "learning_rate": 3.86276831410893e-05,
      "loss": 1.9066,
      "step": 119100
    },
    {
      "epoch": 9.10549232296998,
      "grad_norm": 5.996766090393066,
      "learning_rate": 3.861813459628753e-05,
      "loss": 1.8912,
      "step": 119200
    },
    {
      "epoch": 9.113131158811397,
      "grad_norm": 6.388099670410156,
      "learning_rate": 3.8608586051485756e-05,
      "loss": 1.8733,
      "step": 119300
    },
    {
      "epoch": 9.120769994652814,
      "grad_norm": 6.855689525604248,
      "learning_rate": 3.8599037506683985e-05,
      "loss": 1.8757,
      "step": 119400
    },
    {
      "epoch": 9.128408830494232,
      "grad_norm": 7.06190299987793,
      "learning_rate": 3.8589488961882207e-05,
      "loss": 1.8188,
      "step": 119500
    },
    {
      "epoch": 9.136047666335651,
      "grad_norm": 8.225825309753418,
      "learning_rate": 3.857994041708044e-05,
      "loss": 1.821,
      "step": 119600
    },
    {
      "epoch": 9.143686502177069,
      "grad_norm": 9.17504596710205,
      "learning_rate": 3.8570391872278664e-05,
      "loss": 1.8251,
      "step": 119700
    },
    {
      "epoch": 9.151325338018486,
      "grad_norm": 5.024287223815918,
      "learning_rate": 3.856084332747689e-05,
      "loss": 1.7796,
      "step": 119800
    },
    {
      "epoch": 9.158964173859903,
      "grad_norm": 5.596312999725342,
      "learning_rate": 3.855129478267512e-05,
      "loss": 1.9084,
      "step": 119900
    },
    {
      "epoch": 9.166603009701321,
      "grad_norm": 7.062551975250244,
      "learning_rate": 3.854174623787335e-05,
      "loss": 1.9745,
      "step": 120000
    },
    {
      "epoch": 9.17424184554274,
      "grad_norm": 7.4750075340271,
      "learning_rate": 3.853219769307158e-05,
      "loss": 1.8248,
      "step": 120100
    },
    {
      "epoch": 9.181880681384158,
      "grad_norm": 8.929186820983887,
      "learning_rate": 3.852264914826981e-05,
      "loss": 1.7361,
      "step": 120200
    },
    {
      "epoch": 9.189519517225575,
      "grad_norm": 6.495011329650879,
      "learning_rate": 3.8513100603468036e-05,
      "loss": 1.862,
      "step": 120300
    },
    {
      "epoch": 9.197158353066992,
      "grad_norm": 8.399240493774414,
      "learning_rate": 3.850355205866626e-05,
      "loss": 1.9057,
      "step": 120400
    },
    {
      "epoch": 9.20479718890841,
      "grad_norm": 6.776754379272461,
      "learning_rate": 3.8494003513864493e-05,
      "loss": 1.8252,
      "step": 120500
    },
    {
      "epoch": 9.212436024749827,
      "grad_norm": 6.055707931518555,
      "learning_rate": 3.8484454969062715e-05,
      "loss": 1.9124,
      "step": 120600
    },
    {
      "epoch": 9.220074860591247,
      "grad_norm": 6.4599127769470215,
      "learning_rate": 3.8474906424260944e-05,
      "loss": 1.8385,
      "step": 120700
    },
    {
      "epoch": 9.227713696432664,
      "grad_norm": 9.4035005569458,
      "learning_rate": 3.846535787945917e-05,
      "loss": 1.8397,
      "step": 120800
    },
    {
      "epoch": 9.235352532274081,
      "grad_norm": 5.84060001373291,
      "learning_rate": 3.84558093346574e-05,
      "loss": 1.8037,
      "step": 120900
    },
    {
      "epoch": 9.242991368115499,
      "grad_norm": 5.818294048309326,
      "learning_rate": 3.844626078985562e-05,
      "loss": 1.8945,
      "step": 121000
    },
    {
      "epoch": 9.250630203956916,
      "grad_norm": 7.6006011962890625,
      "learning_rate": 3.843671224505386e-05,
      "loss": 1.8439,
      "step": 121100
    },
    {
      "epoch": 9.258269039798336,
      "grad_norm": 6.071735382080078,
      "learning_rate": 3.842716370025208e-05,
      "loss": 1.8432,
      "step": 121200
    },
    {
      "epoch": 9.265907875639753,
      "grad_norm": 6.131033420562744,
      "learning_rate": 3.841761515545031e-05,
      "loss": 1.9166,
      "step": 121300
    },
    {
      "epoch": 9.27354671148117,
      "grad_norm": 9.130270004272461,
      "learning_rate": 3.840806661064854e-05,
      "loss": 1.8481,
      "step": 121400
    },
    {
      "epoch": 9.281185547322588,
      "grad_norm": 6.630194187164307,
      "learning_rate": 3.839851806584677e-05,
      "loss": 1.8529,
      "step": 121500
    },
    {
      "epoch": 9.288824383164005,
      "grad_norm": 6.643918514251709,
      "learning_rate": 3.8388969521044996e-05,
      "loss": 1.9393,
      "step": 121600
    },
    {
      "epoch": 9.296463219005423,
      "grad_norm": 6.630173683166504,
      "learning_rate": 3.8379420976243224e-05,
      "loss": 1.795,
      "step": 121700
    },
    {
      "epoch": 9.304102054846842,
      "grad_norm": 4.87973690032959,
      "learning_rate": 3.836987243144145e-05,
      "loss": 1.9228,
      "step": 121800
    },
    {
      "epoch": 9.31174089068826,
      "grad_norm": 9.39721393585205,
      "learning_rate": 3.8360323886639675e-05,
      "loss": 1.8776,
      "step": 121900
    },
    {
      "epoch": 9.319379726529677,
      "grad_norm": 5.941691875457764,
      "learning_rate": 3.835077534183791e-05,
      "loss": 1.7931,
      "step": 122000
    },
    {
      "epoch": 9.327018562371094,
      "grad_norm": 7.64035701751709,
      "learning_rate": 3.834122679703613e-05,
      "loss": 1.8968,
      "step": 122100
    },
    {
      "epoch": 9.334657398212512,
      "grad_norm": 6.646050930023193,
      "learning_rate": 3.833167825223436e-05,
      "loss": 1.7941,
      "step": 122200
    },
    {
      "epoch": 9.34229623405393,
      "grad_norm": 7.750432014465332,
      "learning_rate": 3.832212970743259e-05,
      "loss": 1.9,
      "step": 122300
    },
    {
      "epoch": 9.349935069895349,
      "grad_norm": 6.744895935058594,
      "learning_rate": 3.831258116263082e-05,
      "loss": 1.9621,
      "step": 122400
    },
    {
      "epoch": 9.357573905736766,
      "grad_norm": 7.4470319747924805,
      "learning_rate": 3.830303261782904e-05,
      "loss": 1.9145,
      "step": 122500
    },
    {
      "epoch": 9.365212741578183,
      "grad_norm": 8.337971687316895,
      "learning_rate": 3.8293484073027276e-05,
      "loss": 1.9717,
      "step": 122600
    },
    {
      "epoch": 9.3728515774196,
      "grad_norm": 6.6585774421691895,
      "learning_rate": 3.82839355282255e-05,
      "loss": 1.9382,
      "step": 122700
    },
    {
      "epoch": 9.380490413261018,
      "grad_norm": 7.737183570861816,
      "learning_rate": 3.8274386983423726e-05,
      "loss": 1.9135,
      "step": 122800
    },
    {
      "epoch": 9.388129249102438,
      "grad_norm": 8.179949760437012,
      "learning_rate": 3.8264838438621955e-05,
      "loss": 1.975,
      "step": 122900
    },
    {
      "epoch": 9.395768084943855,
      "grad_norm": 7.117197513580322,
      "learning_rate": 3.8255289893820184e-05,
      "loss": 1.8145,
      "step": 123000
    },
    {
      "epoch": 9.403406920785272,
      "grad_norm": 7.18269681930542,
      "learning_rate": 3.824574134901841e-05,
      "loss": 1.8385,
      "step": 123100
    },
    {
      "epoch": 9.41104575662669,
      "grad_norm": 4.6340227127075195,
      "learning_rate": 3.823619280421664e-05,
      "loss": 1.8802,
      "step": 123200
    },
    {
      "epoch": 9.418684592468107,
      "grad_norm": 7.850452423095703,
      "learning_rate": 3.822664425941487e-05,
      "loss": 1.8721,
      "step": 123300
    },
    {
      "epoch": 9.426323428309527,
      "grad_norm": 7.826013088226318,
      "learning_rate": 3.821709571461309e-05,
      "loss": 1.8677,
      "step": 123400
    },
    {
      "epoch": 9.433962264150944,
      "grad_norm": 8.321167945861816,
      "learning_rate": 3.820754716981133e-05,
      "loss": 1.9477,
      "step": 123500
    },
    {
      "epoch": 9.441601099992361,
      "grad_norm": 6.246913909912109,
      "learning_rate": 3.819799862500955e-05,
      "loss": 1.8461,
      "step": 123600
    },
    {
      "epoch": 9.449239935833779,
      "grad_norm": 7.957286357879639,
      "learning_rate": 3.818845008020778e-05,
      "loss": 1.8046,
      "step": 123700
    },
    {
      "epoch": 9.456878771675196,
      "grad_norm": 7.493075847625732,
      "learning_rate": 3.8178901535406006e-05,
      "loss": 1.8236,
      "step": 123800
    },
    {
      "epoch": 9.464517607516614,
      "grad_norm": 7.453444004058838,
      "learning_rate": 3.8169352990604235e-05,
      "loss": 1.9321,
      "step": 123900
    },
    {
      "epoch": 9.472156443358033,
      "grad_norm": 9.636492729187012,
      "learning_rate": 3.8159804445802464e-05,
      "loss": 1.9305,
      "step": 124000
    },
    {
      "epoch": 9.47979527919945,
      "grad_norm": 8.210527420043945,
      "learning_rate": 3.8150255901000686e-05,
      "loss": 1.8425,
      "step": 124100
    },
    {
      "epoch": 9.487434115040868,
      "grad_norm": 6.102813243865967,
      "learning_rate": 3.814070735619892e-05,
      "loss": 1.9282,
      "step": 124200
    },
    {
      "epoch": 9.495072950882285,
      "grad_norm": 10.403205871582031,
      "learning_rate": 3.813115881139714e-05,
      "loss": 1.8876,
      "step": 124300
    },
    {
      "epoch": 9.502711786723703,
      "grad_norm": 7.939474105834961,
      "learning_rate": 3.812161026659537e-05,
      "loss": 1.8614,
      "step": 124400
    },
    {
      "epoch": 9.51035062256512,
      "grad_norm": 6.381683349609375,
      "learning_rate": 3.81120617217936e-05,
      "loss": 1.8416,
      "step": 124500
    },
    {
      "epoch": 9.51798945840654,
      "grad_norm": 8.34192943572998,
      "learning_rate": 3.810251317699183e-05,
      "loss": 1.8332,
      "step": 124600
    },
    {
      "epoch": 9.525628294247957,
      "grad_norm": 7.305561065673828,
      "learning_rate": 3.809296463219005e-05,
      "loss": 1.8984,
      "step": 124700
    },
    {
      "epoch": 9.533267130089374,
      "grad_norm": 5.843663215637207,
      "learning_rate": 3.8083416087388287e-05,
      "loss": 1.9041,
      "step": 124800
    },
    {
      "epoch": 9.540905965930792,
      "grad_norm": 10.594039916992188,
      "learning_rate": 3.807386754258651e-05,
      "loss": 1.7788,
      "step": 124900
    },
    {
      "epoch": 9.54854480177221,
      "grad_norm": 6.0338215827941895,
      "learning_rate": 3.806431899778474e-05,
      "loss": 1.882,
      "step": 125000
    },
    {
      "epoch": 9.556183637613628,
      "grad_norm": 6.9411773681640625,
      "learning_rate": 3.8054770452982966e-05,
      "loss": 1.9084,
      "step": 125100
    },
    {
      "epoch": 9.563822473455046,
      "grad_norm": 8.136488914489746,
      "learning_rate": 3.8045221908181195e-05,
      "loss": 1.8905,
      "step": 125200
    },
    {
      "epoch": 9.571461309296463,
      "grad_norm": 6.268734455108643,
      "learning_rate": 3.803567336337942e-05,
      "loss": 1.8631,
      "step": 125300
    },
    {
      "epoch": 9.57910014513788,
      "grad_norm": 6.793465614318848,
      "learning_rate": 3.802612481857765e-05,
      "loss": 1.8372,
      "step": 125400
    },
    {
      "epoch": 9.586738980979298,
      "grad_norm": 7.537399768829346,
      "learning_rate": 3.801657627377588e-05,
      "loss": 1.9455,
      "step": 125500
    },
    {
      "epoch": 9.594377816820717,
      "grad_norm": 7.555716037750244,
      "learning_rate": 3.80070277289741e-05,
      "loss": 1.8467,
      "step": 125600
    },
    {
      "epoch": 9.602016652662135,
      "grad_norm": 6.037914752960205,
      "learning_rate": 3.799747918417234e-05,
      "loss": 1.9531,
      "step": 125700
    },
    {
      "epoch": 9.609655488503552,
      "grad_norm": 7.178473472595215,
      "learning_rate": 3.798793063937056e-05,
      "loss": 1.9051,
      "step": 125800
    },
    {
      "epoch": 9.61729432434497,
      "grad_norm": 5.613424777984619,
      "learning_rate": 3.797838209456879e-05,
      "loss": 1.8909,
      "step": 125900
    },
    {
      "epoch": 9.624933160186387,
      "grad_norm": 7.685488700866699,
      "learning_rate": 3.796883354976702e-05,
      "loss": 1.7352,
      "step": 126000
    },
    {
      "epoch": 9.632571996027805,
      "grad_norm": 6.3588361740112305,
      "learning_rate": 3.7959285004965246e-05,
      "loss": 1.8078,
      "step": 126100
    },
    {
      "epoch": 9.640210831869224,
      "grad_norm": 7.462490081787109,
      "learning_rate": 3.794973646016347e-05,
      "loss": 1.9565,
      "step": 126200
    },
    {
      "epoch": 9.647849667710641,
      "grad_norm": 4.789541244506836,
      "learning_rate": 3.7940187915361703e-05,
      "loss": 1.8563,
      "step": 126300
    },
    {
      "epoch": 9.655488503552059,
      "grad_norm": 7.781432151794434,
      "learning_rate": 3.7930639370559925e-05,
      "loss": 1.8951,
      "step": 126400
    },
    {
      "epoch": 9.663127339393476,
      "grad_norm": 6.1348795890808105,
      "learning_rate": 3.7921090825758154e-05,
      "loss": 1.8556,
      "step": 126500
    },
    {
      "epoch": 9.670766175234894,
      "grad_norm": 8.821931838989258,
      "learning_rate": 3.791154228095638e-05,
      "loss": 1.951,
      "step": 126600
    },
    {
      "epoch": 9.678405011076311,
      "grad_norm": 6.498342037200928,
      "learning_rate": 3.790199373615461e-05,
      "loss": 1.8779,
      "step": 126700
    },
    {
      "epoch": 9.68604384691773,
      "grad_norm": 5.851454257965088,
      "learning_rate": 3.789244519135284e-05,
      "loss": 1.9812,
      "step": 126800
    },
    {
      "epoch": 9.693682682759148,
      "grad_norm": 8.265083312988281,
      "learning_rate": 3.788289664655107e-05,
      "loss": 1.8704,
      "step": 126900
    },
    {
      "epoch": 9.701321518600565,
      "grad_norm": 6.537714004516602,
      "learning_rate": 3.78733481017493e-05,
      "loss": 1.9282,
      "step": 127000
    },
    {
      "epoch": 9.708960354441983,
      "grad_norm": 7.147091865539551,
      "learning_rate": 3.786379955694752e-05,
      "loss": 1.896,
      "step": 127100
    },
    {
      "epoch": 9.7165991902834,
      "grad_norm": 7.240853786468506,
      "learning_rate": 3.7854251012145755e-05,
      "loss": 1.8871,
      "step": 127200
    },
    {
      "epoch": 9.72423802612482,
      "grad_norm": 10.622154235839844,
      "learning_rate": 3.784470246734398e-05,
      "loss": 1.929,
      "step": 127300
    },
    {
      "epoch": 9.731876861966237,
      "grad_norm": 9.556715965270996,
      "learning_rate": 3.7835153922542205e-05,
      "loss": 1.9415,
      "step": 127400
    },
    {
      "epoch": 9.739515697807654,
      "grad_norm": 6.836235046386719,
      "learning_rate": 3.7825605377740434e-05,
      "loss": 1.8551,
      "step": 127500
    },
    {
      "epoch": 9.747154533649072,
      "grad_norm": 7.701223850250244,
      "learning_rate": 3.781605683293866e-05,
      "loss": 1.8662,
      "step": 127600
    },
    {
      "epoch": 9.75479336949049,
      "grad_norm": 6.79110860824585,
      "learning_rate": 3.7806508288136885e-05,
      "loss": 1.8657,
      "step": 127700
    },
    {
      "epoch": 9.762432205331907,
      "grad_norm": 6.678338050842285,
      "learning_rate": 3.779695974333512e-05,
      "loss": 1.7695,
      "step": 127800
    },
    {
      "epoch": 9.770071041173326,
      "grad_norm": 7.051865577697754,
      "learning_rate": 3.778741119853335e-05,
      "loss": 1.8924,
      "step": 127900
    },
    {
      "epoch": 9.777709877014743,
      "grad_norm": 13.131865501403809,
      "learning_rate": 3.777786265373157e-05,
      "loss": 1.9197,
      "step": 128000
    },
    {
      "epoch": 9.78534871285616,
      "grad_norm": 7.984005928039551,
      "learning_rate": 3.7768314108929806e-05,
      "loss": 1.9349,
      "step": 128100
    },
    {
      "epoch": 9.792987548697578,
      "grad_norm": 7.101174831390381,
      "learning_rate": 3.775876556412803e-05,
      "loss": 1.8216,
      "step": 128200
    },
    {
      "epoch": 9.800626384538996,
      "grad_norm": 6.41784143447876,
      "learning_rate": 3.774921701932626e-05,
      "loss": 1.8756,
      "step": 128300
    },
    {
      "epoch": 9.808265220380415,
      "grad_norm": 7.439627647399902,
      "learning_rate": 3.7739668474524486e-05,
      "loss": 1.8968,
      "step": 128400
    },
    {
      "epoch": 9.815904056221832,
      "grad_norm": 6.93464994430542,
      "learning_rate": 3.7730119929722714e-05,
      "loss": 1.8401,
      "step": 128500
    },
    {
      "epoch": 9.82354289206325,
      "grad_norm": 7.821578025817871,
      "learning_rate": 3.7720571384920936e-05,
      "loss": 1.8786,
      "step": 128600
    },
    {
      "epoch": 9.831181727904667,
      "grad_norm": 7.291140079498291,
      "learning_rate": 3.771102284011917e-05,
      "loss": 1.8857,
      "step": 128700
    },
    {
      "epoch": 9.838820563746085,
      "grad_norm": 6.4490790367126465,
      "learning_rate": 3.7701474295317394e-05,
      "loss": 1.832,
      "step": 128800
    },
    {
      "epoch": 9.846459399587502,
      "grad_norm": 5.56199836730957,
      "learning_rate": 3.769192575051562e-05,
      "loss": 1.8685,
      "step": 128900
    },
    {
      "epoch": 9.854098235428921,
      "grad_norm": 8.559523582458496,
      "learning_rate": 3.768237720571385e-05,
      "loss": 1.8686,
      "step": 129000
    },
    {
      "epoch": 9.861737071270339,
      "grad_norm": 5.1879048347473145,
      "learning_rate": 3.767282866091208e-05,
      "loss": 1.8624,
      "step": 129100
    },
    {
      "epoch": 9.869375907111756,
      "grad_norm": 7.24326753616333,
      "learning_rate": 3.766328011611031e-05,
      "loss": 1.8444,
      "step": 129200
    },
    {
      "epoch": 9.877014742953174,
      "grad_norm": 5.27509069442749,
      "learning_rate": 3.765373157130854e-05,
      "loss": 1.9756,
      "step": 129300
    },
    {
      "epoch": 9.884653578794591,
      "grad_norm": 7.0064873695373535,
      "learning_rate": 3.7644183026506766e-05,
      "loss": 1.9527,
      "step": 129400
    },
    {
      "epoch": 9.89229241463601,
      "grad_norm": 6.558749675750732,
      "learning_rate": 3.763463448170499e-05,
      "loss": 1.7571,
      "step": 129500
    },
    {
      "epoch": 9.899931250477428,
      "grad_norm": 6.539523601531982,
      "learning_rate": 3.762508593690322e-05,
      "loss": 1.8764,
      "step": 129600
    },
    {
      "epoch": 9.907570086318845,
      "grad_norm": 6.285376071929932,
      "learning_rate": 3.7615537392101445e-05,
      "loss": 1.9177,
      "step": 129700
    },
    {
      "epoch": 9.915208922160263,
      "grad_norm": 6.258729934692383,
      "learning_rate": 3.7605988847299674e-05,
      "loss": 1.9501,
      "step": 129800
    },
    {
      "epoch": 9.92284775800168,
      "grad_norm": 8.337038040161133,
      "learning_rate": 3.7596440302497896e-05,
      "loss": 1.8376,
      "step": 129900
    },
    {
      "epoch": 9.930486593843098,
      "grad_norm": 5.765321254730225,
      "learning_rate": 3.758689175769613e-05,
      "loss": 1.999,
      "step": 130000
    },
    {
      "epoch": 9.938125429684517,
      "grad_norm": 7.138646125793457,
      "learning_rate": 3.757734321289435e-05,
      "loss": 1.9408,
      "step": 130100
    },
    {
      "epoch": 9.945764265525934,
      "grad_norm": 5.975494384765625,
      "learning_rate": 3.756779466809258e-05,
      "loss": 1.7806,
      "step": 130200
    },
    {
      "epoch": 9.953403101367352,
      "grad_norm": 6.708974361419678,
      "learning_rate": 3.755824612329081e-05,
      "loss": 1.778,
      "step": 130300
    },
    {
      "epoch": 9.961041937208769,
      "grad_norm": 8.916874885559082,
      "learning_rate": 3.754869757848904e-05,
      "loss": 1.9818,
      "step": 130400
    },
    {
      "epoch": 9.968680773050187,
      "grad_norm": 7.778391361236572,
      "learning_rate": 3.753914903368727e-05,
      "loss": 1.8987,
      "step": 130500
    },
    {
      "epoch": 9.976319608891606,
      "grad_norm": 9.2202787399292,
      "learning_rate": 3.7529600488885497e-05,
      "loss": 1.8861,
      "step": 130600
    },
    {
      "epoch": 9.983958444733023,
      "grad_norm": 5.720977306365967,
      "learning_rate": 3.7520051944083725e-05,
      "loss": 1.8533,
      "step": 130700
    },
    {
      "epoch": 9.99159728057444,
      "grad_norm": 5.4271111488342285,
      "learning_rate": 3.751050339928195e-05,
      "loss": 1.9275,
      "step": 130800
    },
    {
      "epoch": 9.999236116415858,
      "grad_norm": 6.454805850982666,
      "learning_rate": 3.750095485448018e-05,
      "loss": 1.8254,
      "step": 130900
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8665070533752441,
      "eval_runtime": 3.0089,
      "eval_samples_per_second": 229.323,
      "eval_steps_per_second": 229.323,
      "step": 130910
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.658726453781128,
      "eval_runtime": 56.0814,
      "eval_samples_per_second": 233.429,
      "eval_steps_per_second": 233.429,
      "step": 130910
    }
  ],
  "logging_steps": 100,
  "max_steps": 523640,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 40,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 81008197171200.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
