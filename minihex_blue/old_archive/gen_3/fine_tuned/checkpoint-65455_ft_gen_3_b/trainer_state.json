{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 65455,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007638835841417768,
      "grad_norm": 9.616304397583008,
      "learning_rate": 4.9993634303465486e-05,
      "loss": 3.937,
      "step": 100
    },
    {
      "epoch": 0.015277671682835536,
      "grad_norm": 8.151701927185059,
      "learning_rate": 4.998726860693097e-05,
      "loss": 3.0601,
      "step": 200
    },
    {
      "epoch": 0.022916507524253303,
      "grad_norm": 6.0536041259765625,
      "learning_rate": 4.998090291039646e-05,
      "loss": 2.7687,
      "step": 300
    },
    {
      "epoch": 0.030555343365671072,
      "grad_norm": 6.605126857757568,
      "learning_rate": 4.997453721386194e-05,
      "loss": 2.7223,
      "step": 400
    },
    {
      "epoch": 0.03819417920708884,
      "grad_norm": 6.9565958976745605,
      "learning_rate": 4.996817151732743e-05,
      "loss": 2.6542,
      "step": 500
    },
    {
      "epoch": 0.045833015048506606,
      "grad_norm": 6.40494441986084,
      "learning_rate": 4.996180582079291e-05,
      "loss": 2.7054,
      "step": 600
    },
    {
      "epoch": 0.05347185088992438,
      "grad_norm": 7.622507572174072,
      "learning_rate": 4.99554401242584e-05,
      "loss": 2.6205,
      "step": 700
    },
    {
      "epoch": 0.061110686731342144,
      "grad_norm": 7.909270763397217,
      "learning_rate": 4.9949074427723884e-05,
      "loss": 2.6121,
      "step": 800
    },
    {
      "epoch": 0.06874952257275992,
      "grad_norm": 5.539137363433838,
      "learning_rate": 4.994270873118937e-05,
      "loss": 2.4423,
      "step": 900
    },
    {
      "epoch": 0.07638835841417768,
      "grad_norm": 8.158707618713379,
      "learning_rate": 4.993634303465485e-05,
      "loss": 2.5281,
      "step": 1000
    },
    {
      "epoch": 0.08402719425559545,
      "grad_norm": 5.5602216720581055,
      "learning_rate": 4.9929977338120335e-05,
      "loss": 2.6171,
      "step": 1100
    },
    {
      "epoch": 0.09166603009701321,
      "grad_norm": 5.058753490447998,
      "learning_rate": 4.9923611641585825e-05,
      "loss": 2.5049,
      "step": 1200
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 5.4211745262146,
      "learning_rate": 4.991724594505131e-05,
      "loss": 2.5609,
      "step": 1300
    },
    {
      "epoch": 0.10694370177984876,
      "grad_norm": 6.5574846267700195,
      "learning_rate": 4.991088024851679e-05,
      "loss": 2.4511,
      "step": 1400
    },
    {
      "epoch": 0.11458253762126652,
      "grad_norm": 5.055769443511963,
      "learning_rate": 4.9904514551982276e-05,
      "loss": 2.386,
      "step": 1500
    },
    {
      "epoch": 0.12222137346268429,
      "grad_norm": 7.65941858291626,
      "learning_rate": 4.9898148855447766e-05,
      "loss": 2.5017,
      "step": 1600
    },
    {
      "epoch": 0.12986020930410205,
      "grad_norm": 5.7708024978637695,
      "learning_rate": 4.989178315891325e-05,
      "loss": 2.3639,
      "step": 1700
    },
    {
      "epoch": 0.13749904514551983,
      "grad_norm": 5.883342742919922,
      "learning_rate": 4.988541746237873e-05,
      "loss": 2.4005,
      "step": 1800
    },
    {
      "epoch": 0.14513788098693758,
      "grad_norm": 8.11298942565918,
      "learning_rate": 4.9879051765844223e-05,
      "loss": 2.3905,
      "step": 1900
    },
    {
      "epoch": 0.15277671682835536,
      "grad_norm": 4.589460849761963,
      "learning_rate": 4.987268606930971e-05,
      "loss": 2.4804,
      "step": 2000
    },
    {
      "epoch": 0.16041555266977311,
      "grad_norm": 6.401552200317383,
      "learning_rate": 4.986632037277519e-05,
      "loss": 2.3687,
      "step": 2100
    },
    {
      "epoch": 0.1680543885111909,
      "grad_norm": 6.15183162689209,
      "learning_rate": 4.985995467624068e-05,
      "loss": 2.3465,
      "step": 2200
    },
    {
      "epoch": 0.17569322435260867,
      "grad_norm": 7.055343151092529,
      "learning_rate": 4.9853588979706164e-05,
      "loss": 2.2905,
      "step": 2300
    },
    {
      "epoch": 0.18333206019402642,
      "grad_norm": 5.634449005126953,
      "learning_rate": 4.984722328317165e-05,
      "loss": 2.4168,
      "step": 2400
    },
    {
      "epoch": 0.1909708960354442,
      "grad_norm": 7.139743328094482,
      "learning_rate": 4.984085758663713e-05,
      "loss": 2.3977,
      "step": 2500
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 6.0577826499938965,
      "learning_rate": 4.983449189010262e-05,
      "loss": 2.3961,
      "step": 2600
    },
    {
      "epoch": 0.20624856771827974,
      "grad_norm": 6.6876325607299805,
      "learning_rate": 4.9828126193568105e-05,
      "loss": 2.3246,
      "step": 2700
    },
    {
      "epoch": 0.21388740355969751,
      "grad_norm": 4.725283622741699,
      "learning_rate": 4.982176049703359e-05,
      "loss": 2.3051,
      "step": 2800
    },
    {
      "epoch": 0.22152623940111527,
      "grad_norm": 5.203609466552734,
      "learning_rate": 4.981539480049907e-05,
      "loss": 2.2771,
      "step": 2900
    },
    {
      "epoch": 0.22916507524253305,
      "grad_norm": 4.983560085296631,
      "learning_rate": 4.980902910396456e-05,
      "loss": 2.3898,
      "step": 3000
    },
    {
      "epoch": 0.2368039110839508,
      "grad_norm": 4.943581581115723,
      "learning_rate": 4.9802663407430046e-05,
      "loss": 2.3188,
      "step": 3100
    },
    {
      "epoch": 0.24444274692536858,
      "grad_norm": 4.924206733703613,
      "learning_rate": 4.979629771089553e-05,
      "loss": 2.3041,
      "step": 3200
    },
    {
      "epoch": 0.2520815827667863,
      "grad_norm": 5.870883464813232,
      "learning_rate": 4.978993201436101e-05,
      "loss": 2.3655,
      "step": 3300
    },
    {
      "epoch": 0.2597204186082041,
      "grad_norm": 5.586599826812744,
      "learning_rate": 4.97835663178265e-05,
      "loss": 2.3186,
      "step": 3400
    },
    {
      "epoch": 0.2673592544496219,
      "grad_norm": 5.799406051635742,
      "learning_rate": 4.977720062129199e-05,
      "loss": 2.2846,
      "step": 3500
    },
    {
      "epoch": 0.27499809029103967,
      "grad_norm": 5.551529884338379,
      "learning_rate": 4.977083492475747e-05,
      "loss": 2.2777,
      "step": 3600
    },
    {
      "epoch": 0.2826369261324574,
      "grad_norm": 4.432833194732666,
      "learning_rate": 4.9764469228222954e-05,
      "loss": 2.2875,
      "step": 3700
    },
    {
      "epoch": 0.29027576197387517,
      "grad_norm": 6.308449745178223,
      "learning_rate": 4.975810353168844e-05,
      "loss": 2.3589,
      "step": 3800
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 6.065944671630859,
      "learning_rate": 4.975173783515393e-05,
      "loss": 2.266,
      "step": 3900
    },
    {
      "epoch": 0.3055534336567107,
      "grad_norm": 5.729177951812744,
      "learning_rate": 4.974537213861941e-05,
      "loss": 2.3251,
      "step": 4000
    },
    {
      "epoch": 0.3131922694981285,
      "grad_norm": 7.179457664489746,
      "learning_rate": 4.9739006442084895e-05,
      "loss": 2.2544,
      "step": 4100
    },
    {
      "epoch": 0.32083110533954623,
      "grad_norm": 4.727491855621338,
      "learning_rate": 4.973264074555038e-05,
      "loss": 2.3912,
      "step": 4200
    },
    {
      "epoch": 0.328469941180964,
      "grad_norm": 5.356302261352539,
      "learning_rate": 4.972627504901586e-05,
      "loss": 2.1869,
      "step": 4300
    },
    {
      "epoch": 0.3361087770223818,
      "grad_norm": 6.656553268432617,
      "learning_rate": 4.971990935248135e-05,
      "loss": 2.3464,
      "step": 4400
    },
    {
      "epoch": 0.34374761286379957,
      "grad_norm": 5.727645397186279,
      "learning_rate": 4.9713543655946836e-05,
      "loss": 2.2505,
      "step": 4500
    },
    {
      "epoch": 0.35138644870521735,
      "grad_norm": 5.574492454528809,
      "learning_rate": 4.970717795941232e-05,
      "loss": 2.2926,
      "step": 4600
    },
    {
      "epoch": 0.35902528454663507,
      "grad_norm": 5.604026794433594,
      "learning_rate": 4.97008122628778e-05,
      "loss": 2.3222,
      "step": 4700
    },
    {
      "epoch": 0.36666412038805285,
      "grad_norm": 5.712973594665527,
      "learning_rate": 4.969444656634329e-05,
      "loss": 2.3392,
      "step": 4800
    },
    {
      "epoch": 0.37430295622947063,
      "grad_norm": 7.068645000457764,
      "learning_rate": 4.968808086980878e-05,
      "loss": 2.3713,
      "step": 4900
    },
    {
      "epoch": 0.3819417920708884,
      "grad_norm": 5.712131023406982,
      "learning_rate": 4.968171517327426e-05,
      "loss": 2.2104,
      "step": 5000
    },
    {
      "epoch": 0.3895806279123062,
      "grad_norm": 6.191450119018555,
      "learning_rate": 4.9675349476739744e-05,
      "loss": 2.3287,
      "step": 5100
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 6.704756259918213,
      "learning_rate": 4.966898378020523e-05,
      "loss": 2.2476,
      "step": 5200
    },
    {
      "epoch": 0.4048582995951417,
      "grad_norm": 4.464864730834961,
      "learning_rate": 4.966261808367072e-05,
      "loss": 2.24,
      "step": 5300
    },
    {
      "epoch": 0.41249713543655947,
      "grad_norm": 4.801627159118652,
      "learning_rate": 4.96562523871362e-05,
      "loss": 2.2028,
      "step": 5400
    },
    {
      "epoch": 0.42013597127797725,
      "grad_norm": 6.303460597991943,
      "learning_rate": 4.9649886690601685e-05,
      "loss": 2.3563,
      "step": 5500
    },
    {
      "epoch": 0.42777480711939503,
      "grad_norm": 5.361160755157471,
      "learning_rate": 4.9643520994067175e-05,
      "loss": 2.1672,
      "step": 5600
    },
    {
      "epoch": 0.43541364296081275,
      "grad_norm": 5.63322114944458,
      "learning_rate": 4.963715529753266e-05,
      "loss": 2.1642,
      "step": 5700
    },
    {
      "epoch": 0.44305247880223053,
      "grad_norm": 4.496484756469727,
      "learning_rate": 4.963078960099814e-05,
      "loss": 2.2615,
      "step": 5800
    },
    {
      "epoch": 0.4506913146436483,
      "grad_norm": 4.634159564971924,
      "learning_rate": 4.962442390446363e-05,
      "loss": 2.2254,
      "step": 5900
    },
    {
      "epoch": 0.4583301504850661,
      "grad_norm": 6.410694122314453,
      "learning_rate": 4.9618058207929116e-05,
      "loss": 2.3141,
      "step": 6000
    },
    {
      "epoch": 0.46596898632648387,
      "grad_norm": 4.223718643188477,
      "learning_rate": 4.96116925113946e-05,
      "loss": 2.2958,
      "step": 6100
    },
    {
      "epoch": 0.4736078221679016,
      "grad_norm": 5.524945259094238,
      "learning_rate": 4.960532681486009e-05,
      "loss": 2.2643,
      "step": 6200
    },
    {
      "epoch": 0.4812466580093194,
      "grad_norm": 5.703848361968994,
      "learning_rate": 4.9598961118325574e-05,
      "loss": 2.2595,
      "step": 6300
    },
    {
      "epoch": 0.48888549385073715,
      "grad_norm": 5.8327460289001465,
      "learning_rate": 4.959259542179106e-05,
      "loss": 2.2858,
      "step": 6400
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 5.9313578605651855,
      "learning_rate": 4.958622972525654e-05,
      "loss": 2.204,
      "step": 6500
    },
    {
      "epoch": 0.5041631655335727,
      "grad_norm": 6.740568161010742,
      "learning_rate": 4.9579864028722024e-05,
      "loss": 2.0985,
      "step": 6600
    },
    {
      "epoch": 0.5118020013749904,
      "grad_norm": 3.8944225311279297,
      "learning_rate": 4.9573498332187515e-05,
      "loss": 2.1913,
      "step": 6700
    },
    {
      "epoch": 0.5194408372164082,
      "grad_norm": 6.740309238433838,
      "learning_rate": 4.9567132635653e-05,
      "loss": 2.193,
      "step": 6800
    },
    {
      "epoch": 0.527079673057826,
      "grad_norm": 4.46602725982666,
      "learning_rate": 4.956076693911848e-05,
      "loss": 2.1996,
      "step": 6900
    },
    {
      "epoch": 0.5347185088992438,
      "grad_norm": 5.942750453948975,
      "learning_rate": 4.9554401242583965e-05,
      "loss": 2.1785,
      "step": 7000
    },
    {
      "epoch": 0.5423573447406616,
      "grad_norm": 6.05792760848999,
      "learning_rate": 4.9548035546049455e-05,
      "loss": 2.2465,
      "step": 7100
    },
    {
      "epoch": 0.5499961805820793,
      "grad_norm": 5.64909029006958,
      "learning_rate": 4.954166984951494e-05,
      "loss": 2.1558,
      "step": 7200
    },
    {
      "epoch": 0.5576350164234971,
      "grad_norm": 5.248727798461914,
      "learning_rate": 4.953530415298042e-05,
      "loss": 2.2097,
      "step": 7300
    },
    {
      "epoch": 0.5652738522649148,
      "grad_norm": 6.6400556564331055,
      "learning_rate": 4.9528938456445906e-05,
      "loss": 2.2292,
      "step": 7400
    },
    {
      "epoch": 0.5729126881063326,
      "grad_norm": 5.061389923095703,
      "learning_rate": 4.952257275991139e-05,
      "loss": 2.2529,
      "step": 7500
    },
    {
      "epoch": 0.5805515239477503,
      "grad_norm": 5.304447650909424,
      "learning_rate": 4.951620706337688e-05,
      "loss": 2.2008,
      "step": 7600
    },
    {
      "epoch": 0.5881903597891681,
      "grad_norm": 4.999083995819092,
      "learning_rate": 4.9509841366842363e-05,
      "loss": 2.2596,
      "step": 7700
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 5.455264091491699,
      "learning_rate": 4.950347567030785e-05,
      "loss": 2.1943,
      "step": 7800
    },
    {
      "epoch": 0.6034680314720037,
      "grad_norm": 6.516719818115234,
      "learning_rate": 4.949710997377333e-05,
      "loss": 2.3342,
      "step": 7900
    },
    {
      "epoch": 0.6111068673134215,
      "grad_norm": 5.990109443664551,
      "learning_rate": 4.9490744277238814e-05,
      "loss": 2.2105,
      "step": 8000
    },
    {
      "epoch": 0.6187457031548392,
      "grad_norm": 7.817955017089844,
      "learning_rate": 4.9484378580704304e-05,
      "loss": 2.2355,
      "step": 8100
    },
    {
      "epoch": 0.626384538996257,
      "grad_norm": 4.904414653778076,
      "learning_rate": 4.947801288416979e-05,
      "loss": 2.3853,
      "step": 8200
    },
    {
      "epoch": 0.6340233748376748,
      "grad_norm": 5.830631256103516,
      "learning_rate": 4.947164718763527e-05,
      "loss": 2.1895,
      "step": 8300
    },
    {
      "epoch": 0.6416622106790925,
      "grad_norm": 5.176312446594238,
      "learning_rate": 4.9465281491100755e-05,
      "loss": 2.3031,
      "step": 8400
    },
    {
      "epoch": 0.6493010465205102,
      "grad_norm": 4.834622383117676,
      "learning_rate": 4.9458915794566245e-05,
      "loss": 2.2634,
      "step": 8500
    },
    {
      "epoch": 0.656939882361928,
      "grad_norm": 6.458419322967529,
      "learning_rate": 4.945255009803173e-05,
      "loss": 2.3257,
      "step": 8600
    },
    {
      "epoch": 0.6645787182033458,
      "grad_norm": 7.003514289855957,
      "learning_rate": 4.944618440149721e-05,
      "loss": 2.2252,
      "step": 8700
    },
    {
      "epoch": 0.6722175540447636,
      "grad_norm": 5.700473785400391,
      "learning_rate": 4.9439818704962696e-05,
      "loss": 2.2867,
      "step": 8800
    },
    {
      "epoch": 0.6798563898861814,
      "grad_norm": 5.903298854827881,
      "learning_rate": 4.943345300842818e-05,
      "loss": 2.1409,
      "step": 8900
    },
    {
      "epoch": 0.6874952257275991,
      "grad_norm": 5.447477340698242,
      "learning_rate": 4.942708731189367e-05,
      "loss": 2.1751,
      "step": 9000
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 5.830201148986816,
      "learning_rate": 4.942072161535915e-05,
      "loss": 2.2451,
      "step": 9100
    },
    {
      "epoch": 0.7027728974104347,
      "grad_norm": 5.6821184158325195,
      "learning_rate": 4.941435591882464e-05,
      "loss": 2.1543,
      "step": 9200
    },
    {
      "epoch": 0.7104117332518525,
      "grad_norm": 7.409090518951416,
      "learning_rate": 4.940799022229012e-05,
      "loss": 2.1139,
      "step": 9300
    },
    {
      "epoch": 0.7180505690932701,
      "grad_norm": 5.77468204498291,
      "learning_rate": 4.940162452575561e-05,
      "loss": 2.0247,
      "step": 9400
    },
    {
      "epoch": 0.7256894049346879,
      "grad_norm": 6.67192268371582,
      "learning_rate": 4.9395258829221094e-05,
      "loss": 2.1983,
      "step": 9500
    },
    {
      "epoch": 0.7333282407761057,
      "grad_norm": 6.025003433227539,
      "learning_rate": 4.938889313268658e-05,
      "loss": 2.2739,
      "step": 9600
    },
    {
      "epoch": 0.7409670766175235,
      "grad_norm": 6.283199310302734,
      "learning_rate": 4.938252743615207e-05,
      "loss": 2.1735,
      "step": 9700
    },
    {
      "epoch": 0.7486059124589413,
      "grad_norm": 6.228161811828613,
      "learning_rate": 4.937616173961755e-05,
      "loss": 2.0578,
      "step": 9800
    },
    {
      "epoch": 0.756244748300359,
      "grad_norm": 5.581986427307129,
      "learning_rate": 4.936979604308304e-05,
      "loss": 2.1353,
      "step": 9900
    },
    {
      "epoch": 0.7638835841417768,
      "grad_norm": 3.991896390914917,
      "learning_rate": 4.9363430346548525e-05,
      "loss": 2.145,
      "step": 10000
    },
    {
      "epoch": 0.7715224199831946,
      "grad_norm": 5.745916366577148,
      "learning_rate": 4.935706465001401e-05,
      "loss": 2.0885,
      "step": 10100
    },
    {
      "epoch": 0.7791612558246124,
      "grad_norm": 6.662975311279297,
      "learning_rate": 4.935069895347949e-05,
      "loss": 2.1631,
      "step": 10200
    },
    {
      "epoch": 0.78680009166603,
      "grad_norm": 5.892897129058838,
      "learning_rate": 4.9344333256944976e-05,
      "loss": 2.1019,
      "step": 10300
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 5.545729637145996,
      "learning_rate": 4.9337967560410466e-05,
      "loss": 2.1991,
      "step": 10400
    },
    {
      "epoch": 0.8020777633488656,
      "grad_norm": 5.687185764312744,
      "learning_rate": 4.933160186387595e-05,
      "loss": 2.2526,
      "step": 10500
    },
    {
      "epoch": 0.8097165991902834,
      "grad_norm": 4.688138961791992,
      "learning_rate": 4.9325236167341433e-05,
      "loss": 2.161,
      "step": 10600
    },
    {
      "epoch": 0.8173554350317012,
      "grad_norm": 5.643739700317383,
      "learning_rate": 4.931887047080692e-05,
      "loss": 2.256,
      "step": 10700
    },
    {
      "epoch": 0.8249942708731189,
      "grad_norm": 5.172930717468262,
      "learning_rate": 4.931250477427241e-05,
      "loss": 2.1459,
      "step": 10800
    },
    {
      "epoch": 0.8326331067145367,
      "grad_norm": 5.044783115386963,
      "learning_rate": 4.930613907773789e-05,
      "loss": 2.2041,
      "step": 10900
    },
    {
      "epoch": 0.8402719425559545,
      "grad_norm": 6.764038562774658,
      "learning_rate": 4.9299773381203374e-05,
      "loss": 2.2002,
      "step": 11000
    },
    {
      "epoch": 0.8479107783973723,
      "grad_norm": 9.812024116516113,
      "learning_rate": 4.929340768466886e-05,
      "loss": 2.1302,
      "step": 11100
    },
    {
      "epoch": 0.8555496142387901,
      "grad_norm": 5.745163440704346,
      "learning_rate": 4.928704198813434e-05,
      "loss": 2.1919,
      "step": 11200
    },
    {
      "epoch": 0.8631884500802077,
      "grad_norm": 5.257004261016846,
      "learning_rate": 4.928067629159983e-05,
      "loss": 2.1856,
      "step": 11300
    },
    {
      "epoch": 0.8708272859216255,
      "grad_norm": 6.1897053718566895,
      "learning_rate": 4.9274310595065315e-05,
      "loss": 2.2074,
      "step": 11400
    },
    {
      "epoch": 0.8784661217630433,
      "grad_norm": 5.990265846252441,
      "learning_rate": 4.92679448985308e-05,
      "loss": 2.142,
      "step": 11500
    },
    {
      "epoch": 0.8861049576044611,
      "grad_norm": 5.677268981933594,
      "learning_rate": 4.926157920199628e-05,
      "loss": 2.2158,
      "step": 11600
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 5.151512145996094,
      "learning_rate": 4.925521350546177e-05,
      "loss": 2.1789,
      "step": 11700
    },
    {
      "epoch": 0.9013826292872966,
      "grad_norm": 6.832089900970459,
      "learning_rate": 4.9248847808927256e-05,
      "loss": 2.2525,
      "step": 11800
    },
    {
      "epoch": 0.9090214651287144,
      "grad_norm": 6.421391487121582,
      "learning_rate": 4.924248211239274e-05,
      "loss": 2.2572,
      "step": 11900
    },
    {
      "epoch": 0.9166603009701322,
      "grad_norm": 5.610409259796143,
      "learning_rate": 4.923611641585822e-05,
      "loss": 2.1603,
      "step": 12000
    },
    {
      "epoch": 0.92429913681155,
      "grad_norm": 5.067419052124023,
      "learning_rate": 4.922975071932371e-05,
      "loss": 2.165,
      "step": 12100
    },
    {
      "epoch": 0.9319379726529677,
      "grad_norm": 5.97158145904541,
      "learning_rate": 4.92233850227892e-05,
      "loss": 2.2877,
      "step": 12200
    },
    {
      "epoch": 0.9395768084943854,
      "grad_norm": 5.3011393547058105,
      "learning_rate": 4.921701932625468e-05,
      "loss": 2.1561,
      "step": 12300
    },
    {
      "epoch": 0.9472156443358032,
      "grad_norm": 6.893111705780029,
      "learning_rate": 4.9210653629720164e-05,
      "loss": 2.162,
      "step": 12400
    },
    {
      "epoch": 0.954854480177221,
      "grad_norm": 5.501214504241943,
      "learning_rate": 4.920428793318565e-05,
      "loss": 2.1388,
      "step": 12500
    },
    {
      "epoch": 0.9624933160186387,
      "grad_norm": 6.839544773101807,
      "learning_rate": 4.919792223665114e-05,
      "loss": 2.1043,
      "step": 12600
    },
    {
      "epoch": 0.9701321518600565,
      "grad_norm": 4.73917818069458,
      "learning_rate": 4.919155654011662e-05,
      "loss": 2.1012,
      "step": 12700
    },
    {
      "epoch": 0.9777709877014743,
      "grad_norm": 4.778277397155762,
      "learning_rate": 4.9185190843582105e-05,
      "loss": 2.1227,
      "step": 12800
    },
    {
      "epoch": 0.9854098235428921,
      "grad_norm": 5.9979472160339355,
      "learning_rate": 4.917882514704759e-05,
      "loss": 2.1268,
      "step": 12900
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 4.717005729675293,
      "learning_rate": 4.917245945051307e-05,
      "loss": 2.1014,
      "step": 13000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.051760196685791,
      "eval_runtime": 1.5639,
      "eval_samples_per_second": 441.193,
      "eval_steps_per_second": 441.193,
      "step": 13091
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.950911521911621,
      "eval_runtime": 30.0951,
      "eval_samples_per_second": 434.988,
      "eval_steps_per_second": 434.988,
      "step": 13091
    },
    {
      "epoch": 1.0006874952257276,
      "grad_norm": 5.1652631759643555,
      "learning_rate": 4.916609375397856e-05,
      "loss": 2.236,
      "step": 13100
    },
    {
      "epoch": 1.0083263310671453,
      "grad_norm": 4.589023590087891,
      "learning_rate": 4.9159728057444046e-05,
      "loss": 2.152,
      "step": 13200
    },
    {
      "epoch": 1.0159651669085632,
      "grad_norm": 5.34266996383667,
      "learning_rate": 4.915336236090953e-05,
      "loss": 2.1121,
      "step": 13300
    },
    {
      "epoch": 1.0236040027499809,
      "grad_norm": 6.804360866546631,
      "learning_rate": 4.914699666437502e-05,
      "loss": 2.1837,
      "step": 13400
    },
    {
      "epoch": 1.0312428385913988,
      "grad_norm": 6.627771377563477,
      "learning_rate": 4.9140630967840503e-05,
      "loss": 2.1424,
      "step": 13500
    },
    {
      "epoch": 1.0388816744328164,
      "grad_norm": 6.809438705444336,
      "learning_rate": 4.913426527130599e-05,
      "loss": 2.0842,
      "step": 13600
    },
    {
      "epoch": 1.046520510274234,
      "grad_norm": 4.7298760414123535,
      "learning_rate": 4.912789957477148e-05,
      "loss": 2.1065,
      "step": 13700
    },
    {
      "epoch": 1.054159346115652,
      "grad_norm": 4.436557292938232,
      "learning_rate": 4.912153387823696e-05,
      "loss": 2.0692,
      "step": 13800
    },
    {
      "epoch": 1.0617981819570697,
      "grad_norm": 4.991581439971924,
      "learning_rate": 4.9115168181702444e-05,
      "loss": 2.1543,
      "step": 13900
    },
    {
      "epoch": 1.0694370177984875,
      "grad_norm": 5.263034820556641,
      "learning_rate": 4.9108802485167935e-05,
      "loss": 2.1144,
      "step": 14000
    },
    {
      "epoch": 1.0770758536399052,
      "grad_norm": 4.462190628051758,
      "learning_rate": 4.910243678863342e-05,
      "loss": 2.1147,
      "step": 14100
    },
    {
      "epoch": 1.084714689481323,
      "grad_norm": 5.93663215637207,
      "learning_rate": 4.90960710920989e-05,
      "loss": 2.1244,
      "step": 14200
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 6.328474044799805,
      "learning_rate": 4.9089705395564385e-05,
      "loss": 2.0881,
      "step": 14300
    },
    {
      "epoch": 1.0999923611641587,
      "grad_norm": 6.152097702026367,
      "learning_rate": 4.908333969902987e-05,
      "loss": 2.0833,
      "step": 14400
    },
    {
      "epoch": 1.1076311970055763,
      "grad_norm": 6.632983207702637,
      "learning_rate": 4.907697400249536e-05,
      "loss": 2.0643,
      "step": 14500
    },
    {
      "epoch": 1.1152700328469942,
      "grad_norm": 5.6964945793151855,
      "learning_rate": 4.907060830596084e-05,
      "loss": 2.0728,
      "step": 14600
    },
    {
      "epoch": 1.1229088686884119,
      "grad_norm": 6.0844292640686035,
      "learning_rate": 4.9064242609426326e-05,
      "loss": 2.0812,
      "step": 14700
    },
    {
      "epoch": 1.1305477045298296,
      "grad_norm": 4.882937431335449,
      "learning_rate": 4.905787691289181e-05,
      "loss": 2.0823,
      "step": 14800
    },
    {
      "epoch": 1.1381865403712474,
      "grad_norm": 4.6405816078186035,
      "learning_rate": 4.90515112163573e-05,
      "loss": 2.1399,
      "step": 14900
    },
    {
      "epoch": 1.1458253762126651,
      "grad_norm": 7.294325351715088,
      "learning_rate": 4.9045145519822784e-05,
      "loss": 2.2139,
      "step": 15000
    },
    {
      "epoch": 1.153464212054083,
      "grad_norm": 4.018648147583008,
      "learning_rate": 4.903877982328827e-05,
      "loss": 2.1411,
      "step": 15100
    },
    {
      "epoch": 1.1611030478955007,
      "grad_norm": 5.181830883026123,
      "learning_rate": 4.903241412675375e-05,
      "loss": 2.0773,
      "step": 15200
    },
    {
      "epoch": 1.1687418837369186,
      "grad_norm": 6.119314670562744,
      "learning_rate": 4.9026048430219234e-05,
      "loss": 2.1237,
      "step": 15300
    },
    {
      "epoch": 1.1763807195783362,
      "grad_norm": 5.18373966217041,
      "learning_rate": 4.9019682733684725e-05,
      "loss": 2.1419,
      "step": 15400
    },
    {
      "epoch": 1.1840195554197541,
      "grad_norm": 5.341952800750732,
      "learning_rate": 4.901331703715021e-05,
      "loss": 2.0963,
      "step": 15500
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 5.534604072570801,
      "learning_rate": 4.900695134061569e-05,
      "loss": 2.144,
      "step": 15600
    },
    {
      "epoch": 1.1992972271025897,
      "grad_norm": 5.851578235626221,
      "learning_rate": 4.9000585644081175e-05,
      "loss": 2.0345,
      "step": 15700
    },
    {
      "epoch": 1.2069360629440073,
      "grad_norm": 5.909367561340332,
      "learning_rate": 4.8994219947546665e-05,
      "loss": 2.191,
      "step": 15800
    },
    {
      "epoch": 1.214574898785425,
      "grad_norm": 5.51537561416626,
      "learning_rate": 4.898785425101215e-05,
      "loss": 2.0861,
      "step": 15900
    },
    {
      "epoch": 1.222213734626843,
      "grad_norm": 5.823410987854004,
      "learning_rate": 4.898148855447763e-05,
      "loss": 2.1926,
      "step": 16000
    },
    {
      "epoch": 1.2298525704682606,
      "grad_norm": 5.5368499755859375,
      "learning_rate": 4.8975122857943116e-05,
      "loss": 2.1254,
      "step": 16100
    },
    {
      "epoch": 1.2374914063096785,
      "grad_norm": 6.621552467346191,
      "learning_rate": 4.89687571614086e-05,
      "loss": 2.1157,
      "step": 16200
    },
    {
      "epoch": 1.2451302421510961,
      "grad_norm": 5.106004238128662,
      "learning_rate": 4.896239146487409e-05,
      "loss": 2.2423,
      "step": 16300
    },
    {
      "epoch": 1.252769077992514,
      "grad_norm": 6.567890644073486,
      "learning_rate": 4.8956025768339573e-05,
      "loss": 2.2314,
      "step": 16400
    },
    {
      "epoch": 1.2604079138339317,
      "grad_norm": 6.426704406738281,
      "learning_rate": 4.894966007180506e-05,
      "loss": 2.1244,
      "step": 16500
    },
    {
      "epoch": 1.2680467496753494,
      "grad_norm": 5.7304816246032715,
      "learning_rate": 4.894329437527054e-05,
      "loss": 2.1112,
      "step": 16600
    },
    {
      "epoch": 1.2756855855167673,
      "grad_norm": 4.280444622039795,
      "learning_rate": 4.8936928678736024e-05,
      "loss": 2.091,
      "step": 16700
    },
    {
      "epoch": 1.2833244213581851,
      "grad_norm": 5.793206691741943,
      "learning_rate": 4.8930562982201514e-05,
      "loss": 2.1133,
      "step": 16800
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 6.49682092666626,
      "learning_rate": 4.8924197285667e-05,
      "loss": 2.136,
      "step": 16900
    },
    {
      "epoch": 1.2986020930410205,
      "grad_norm": 6.488775253295898,
      "learning_rate": 4.891783158913248e-05,
      "loss": 2.0453,
      "step": 17000
    },
    {
      "epoch": 1.3062409288824384,
      "grad_norm": 5.92938756942749,
      "learning_rate": 4.891146589259797e-05,
      "loss": 2.0975,
      "step": 17100
    },
    {
      "epoch": 1.313879764723856,
      "grad_norm": 3.6715056896209717,
      "learning_rate": 4.8905100196063455e-05,
      "loss": 2.1063,
      "step": 17200
    },
    {
      "epoch": 1.321518600565274,
      "grad_norm": 4.453599452972412,
      "learning_rate": 4.889873449952894e-05,
      "loss": 2.0922,
      "step": 17300
    },
    {
      "epoch": 1.3291574364066916,
      "grad_norm": 4.691103935241699,
      "learning_rate": 4.889236880299443e-05,
      "loss": 2.1499,
      "step": 17400
    },
    {
      "epoch": 1.3367962722481095,
      "grad_norm": 4.605099201202393,
      "learning_rate": 4.888600310645991e-05,
      "loss": 2.0196,
      "step": 17500
    },
    {
      "epoch": 1.3444351080895272,
      "grad_norm": 4.160121917724609,
      "learning_rate": 4.8879637409925396e-05,
      "loss": 2.0162,
      "step": 17600
    },
    {
      "epoch": 1.3520739439309448,
      "grad_norm": 4.139669418334961,
      "learning_rate": 4.8873271713390887e-05,
      "loss": 2.1381,
      "step": 17700
    },
    {
      "epoch": 1.3597127797723627,
      "grad_norm": 5.6353230476379395,
      "learning_rate": 4.886690601685637e-05,
      "loss": 2.1081,
      "step": 17800
    },
    {
      "epoch": 1.3673516156137804,
      "grad_norm": 4.549559116363525,
      "learning_rate": 4.8860540320321854e-05,
      "loss": 2.1764,
      "step": 17900
    },
    {
      "epoch": 1.3749904514551983,
      "grad_norm": 4.950136184692383,
      "learning_rate": 4.885417462378734e-05,
      "loss": 2.0815,
      "step": 18000
    },
    {
      "epoch": 1.382629287296616,
      "grad_norm": 11.186939239501953,
      "learning_rate": 4.884780892725283e-05,
      "loss": 2.2507,
      "step": 18100
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 6.5943098068237305,
      "learning_rate": 4.884144323071831e-05,
      "loss": 2.0942,
      "step": 18200
    },
    {
      "epoch": 1.3979069589794515,
      "grad_norm": 4.803326606750488,
      "learning_rate": 4.8835077534183795e-05,
      "loss": 2.1005,
      "step": 18300
    },
    {
      "epoch": 1.4055457948208692,
      "grad_norm": 5.466151237487793,
      "learning_rate": 4.882871183764928e-05,
      "loss": 2.0885,
      "step": 18400
    },
    {
      "epoch": 1.413184630662287,
      "grad_norm": 5.385415077209473,
      "learning_rate": 4.882234614111476e-05,
      "loss": 2.1393,
      "step": 18500
    },
    {
      "epoch": 1.420823466503705,
      "grad_norm": 6.12500524520874,
      "learning_rate": 4.881598044458025e-05,
      "loss": 2.0623,
      "step": 18600
    },
    {
      "epoch": 1.4284623023451226,
      "grad_norm": 4.586738586425781,
      "learning_rate": 4.8809614748045735e-05,
      "loss": 2.0114,
      "step": 18700
    },
    {
      "epoch": 1.4361011381865403,
      "grad_norm": 6.058650493621826,
      "learning_rate": 4.880324905151122e-05,
      "loss": 2.033,
      "step": 18800
    },
    {
      "epoch": 1.4437399740279582,
      "grad_norm": 5.500997066497803,
      "learning_rate": 4.87968833549767e-05,
      "loss": 2.0812,
      "step": 18900
    },
    {
      "epoch": 1.4513788098693758,
      "grad_norm": 5.73040771484375,
      "learning_rate": 4.8790517658442186e-05,
      "loss": 2.069,
      "step": 19000
    },
    {
      "epoch": 1.4590176457107937,
      "grad_norm": 5.549583911895752,
      "learning_rate": 4.8784151961907676e-05,
      "loss": 2.1373,
      "step": 19100
    },
    {
      "epoch": 1.4666564815522114,
      "grad_norm": 5.493922233581543,
      "learning_rate": 4.877778626537316e-05,
      "loss": 2.1244,
      "step": 19200
    },
    {
      "epoch": 1.4742953173936293,
      "grad_norm": 8.328752517700195,
      "learning_rate": 4.8771420568838643e-05,
      "loss": 2.0284,
      "step": 19300
    },
    {
      "epoch": 1.481934153235047,
      "grad_norm": 5.464628219604492,
      "learning_rate": 4.876505487230413e-05,
      "loss": 2.1462,
      "step": 19400
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 5.208817005157471,
      "learning_rate": 4.875868917576962e-05,
      "loss": 2.0681,
      "step": 19500
    },
    {
      "epoch": 1.4972118249178825,
      "grad_norm": 7.869354724884033,
      "learning_rate": 4.87523234792351e-05,
      "loss": 2.125,
      "step": 19600
    },
    {
      "epoch": 1.5048506607593004,
      "grad_norm": 6.8567633628845215,
      "learning_rate": 4.8745957782700584e-05,
      "loss": 1.974,
      "step": 19700
    },
    {
      "epoch": 1.512489496600718,
      "grad_norm": 6.396224498748779,
      "learning_rate": 4.873959208616607e-05,
      "loss": 2.0637,
      "step": 19800
    },
    {
      "epoch": 1.5201283324421357,
      "grad_norm": 7.382685661315918,
      "learning_rate": 4.873322638963155e-05,
      "loss": 2.1674,
      "step": 19900
    },
    {
      "epoch": 1.5277671682835536,
      "grad_norm": 5.381296634674072,
      "learning_rate": 4.872686069309704e-05,
      "loss": 2.121,
      "step": 20000
    },
    {
      "epoch": 1.5354060041249713,
      "grad_norm": 6.840145587921143,
      "learning_rate": 4.8720494996562525e-05,
      "loss": 2.078,
      "step": 20100
    },
    {
      "epoch": 1.543044839966389,
      "grad_norm": 5.170779705047607,
      "learning_rate": 4.871412930002801e-05,
      "loss": 2.0942,
      "step": 20200
    },
    {
      "epoch": 1.5506836758078069,
      "grad_norm": 6.810160160064697,
      "learning_rate": 4.870776360349349e-05,
      "loss": 2.0904,
      "step": 20300
    },
    {
      "epoch": 1.5583225116492248,
      "grad_norm": 4.106906890869141,
      "learning_rate": 4.870139790695898e-05,
      "loss": 2.0721,
      "step": 20400
    },
    {
      "epoch": 1.5659613474906424,
      "grad_norm": 5.241268157958984,
      "learning_rate": 4.8695032210424466e-05,
      "loss": 2.0771,
      "step": 20500
    },
    {
      "epoch": 1.57360018333206,
      "grad_norm": 4.393163681030273,
      "learning_rate": 4.868866651388995e-05,
      "loss": 2.0608,
      "step": 20600
    },
    {
      "epoch": 1.581239019173478,
      "grad_norm": 6.173206329345703,
      "learning_rate": 4.868230081735543e-05,
      "loss": 2.1253,
      "step": 20700
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 5.893133640289307,
      "learning_rate": 4.867593512082092e-05,
      "loss": 2.1113,
      "step": 20800
    },
    {
      "epoch": 1.5965166908563135,
      "grad_norm": 5.419064521789551,
      "learning_rate": 4.866956942428641e-05,
      "loss": 2.0641,
      "step": 20900
    },
    {
      "epoch": 1.6041555266977312,
      "grad_norm": 5.737093925476074,
      "learning_rate": 4.866320372775189e-05,
      "loss": 2.1051,
      "step": 21000
    },
    {
      "epoch": 1.611794362539149,
      "grad_norm": 6.8636627197265625,
      "learning_rate": 4.865683803121738e-05,
      "loss": 2.2132,
      "step": 21100
    },
    {
      "epoch": 1.6194331983805668,
      "grad_norm": 4.802732944488525,
      "learning_rate": 4.8650472334682865e-05,
      "loss": 2.1218,
      "step": 21200
    },
    {
      "epoch": 1.6270720342219844,
      "grad_norm": 6.862828254699707,
      "learning_rate": 4.864410663814835e-05,
      "loss": 2.1065,
      "step": 21300
    },
    {
      "epoch": 1.6347108700634023,
      "grad_norm": 5.166295051574707,
      "learning_rate": 4.863774094161384e-05,
      "loss": 2.1446,
      "step": 21400
    },
    {
      "epoch": 1.6423497059048202,
      "grad_norm": 10.080464363098145,
      "learning_rate": 4.863137524507932e-05,
      "loss": 2.1129,
      "step": 21500
    },
    {
      "epoch": 1.6499885417462379,
      "grad_norm": 5.515655040740967,
      "learning_rate": 4.8625009548544805e-05,
      "loss": 2.0004,
      "step": 21600
    },
    {
      "epoch": 1.6576273775876555,
      "grad_norm": 5.870816230773926,
      "learning_rate": 4.861864385201029e-05,
      "loss": 2.1448,
      "step": 21700
    },
    {
      "epoch": 1.6652662134290734,
      "grad_norm": 5.311190605163574,
      "learning_rate": 4.861227815547578e-05,
      "loss": 2.1022,
      "step": 21800
    },
    {
      "epoch": 1.6729050492704913,
      "grad_norm": 4.780289649963379,
      "learning_rate": 4.860591245894126e-05,
      "loss": 2.0075,
      "step": 21900
    },
    {
      "epoch": 1.680543885111909,
      "grad_norm": 7.580195426940918,
      "learning_rate": 4.8599546762406746e-05,
      "loss": 2.0891,
      "step": 22000
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 6.54934024810791,
      "learning_rate": 4.859318106587223e-05,
      "loss": 1.9894,
      "step": 22100
    },
    {
      "epoch": 1.6958215567947446,
      "grad_norm": 5.081126689910889,
      "learning_rate": 4.8586815369337713e-05,
      "loss": 2.0317,
      "step": 22200
    },
    {
      "epoch": 1.7034603926361622,
      "grad_norm": 6.155730724334717,
      "learning_rate": 4.8580449672803204e-05,
      "loss": 2.0394,
      "step": 22300
    },
    {
      "epoch": 1.71109922847758,
      "grad_norm": 3.7960727214813232,
      "learning_rate": 4.857408397626869e-05,
      "loss": 2.0681,
      "step": 22400
    },
    {
      "epoch": 1.7187380643189978,
      "grad_norm": 5.274985313415527,
      "learning_rate": 4.856771827973417e-05,
      "loss": 2.0833,
      "step": 22500
    },
    {
      "epoch": 1.7263769001604157,
      "grad_norm": 4.5190110206604,
      "learning_rate": 4.8561352583199654e-05,
      "loss": 2.1074,
      "step": 22600
    },
    {
      "epoch": 1.7340157360018333,
      "grad_norm": 4.960298538208008,
      "learning_rate": 4.8554986886665145e-05,
      "loss": 2.1643,
      "step": 22700
    },
    {
      "epoch": 1.741654571843251,
      "grad_norm": 4.580866813659668,
      "learning_rate": 4.854862119013063e-05,
      "loss": 2.1524,
      "step": 22800
    },
    {
      "epoch": 1.749293407684669,
      "grad_norm": 6.533280372619629,
      "learning_rate": 4.854225549359611e-05,
      "loss": 2.0292,
      "step": 22900
    },
    {
      "epoch": 1.7569322435260868,
      "grad_norm": 5.69689416885376,
      "learning_rate": 4.8535889797061595e-05,
      "loss": 2.0544,
      "step": 23000
    },
    {
      "epoch": 1.7645710793675042,
      "grad_norm": 14.413633346557617,
      "learning_rate": 4.852952410052708e-05,
      "loss": 2.0684,
      "step": 23100
    },
    {
      "epoch": 1.7722099152089221,
      "grad_norm": 5.597094535827637,
      "learning_rate": 4.852315840399257e-05,
      "loss": 2.0944,
      "step": 23200
    },
    {
      "epoch": 1.77984875105034,
      "grad_norm": 4.633741855621338,
      "learning_rate": 4.851679270745805e-05,
      "loss": 2.1503,
      "step": 23300
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 5.666462421417236,
      "learning_rate": 4.8510427010923536e-05,
      "loss": 2.0853,
      "step": 23400
    },
    {
      "epoch": 1.7951264227331754,
      "grad_norm": 4.447531700134277,
      "learning_rate": 4.850406131438902e-05,
      "loss": 2.0104,
      "step": 23500
    },
    {
      "epoch": 1.8027652585745932,
      "grad_norm": 5.980799674987793,
      "learning_rate": 4.849769561785451e-05,
      "loss": 2.1279,
      "step": 23600
    },
    {
      "epoch": 1.8104040944160111,
      "grad_norm": 7.651262283325195,
      "learning_rate": 4.8491329921319994e-05,
      "loss": 2.11,
      "step": 23700
    },
    {
      "epoch": 1.8180429302574288,
      "grad_norm": 5.339463233947754,
      "learning_rate": 4.848496422478548e-05,
      "loss": 2.1092,
      "step": 23800
    },
    {
      "epoch": 1.8256817660988465,
      "grad_norm": 6.898158073425293,
      "learning_rate": 4.847859852825096e-05,
      "loss": 2.0438,
      "step": 23900
    },
    {
      "epoch": 1.8333206019402644,
      "grad_norm": 5.219961166381836,
      "learning_rate": 4.8472232831716444e-05,
      "loss": 2.1058,
      "step": 24000
    },
    {
      "epoch": 1.840959437781682,
      "grad_norm": 6.720767974853516,
      "learning_rate": 4.8465867135181935e-05,
      "loss": 1.9917,
      "step": 24100
    },
    {
      "epoch": 1.8485982736230997,
      "grad_norm": 6.295424938201904,
      "learning_rate": 4.845950143864742e-05,
      "loss": 2.1936,
      "step": 24200
    },
    {
      "epoch": 1.8562371094645176,
      "grad_norm": 4.826303482055664,
      "learning_rate": 4.84531357421129e-05,
      "loss": 1.9214,
      "step": 24300
    },
    {
      "epoch": 1.8638759453059355,
      "grad_norm": 8.05586051940918,
      "learning_rate": 4.8446770045578385e-05,
      "loss": 2.0795,
      "step": 24400
    },
    {
      "epoch": 1.8715147811473531,
      "grad_norm": 5.3393096923828125,
      "learning_rate": 4.8440404349043875e-05,
      "loss": 2.1264,
      "step": 24500
    },
    {
      "epoch": 1.8791536169887708,
      "grad_norm": 4.885913848876953,
      "learning_rate": 4.843403865250936e-05,
      "loss": 2.1879,
      "step": 24600
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 7.572757720947266,
      "learning_rate": 4.842767295597484e-05,
      "loss": 2.1276,
      "step": 24700
    },
    {
      "epoch": 1.8944312886716066,
      "grad_norm": 6.479361534118652,
      "learning_rate": 4.8421307259440326e-05,
      "loss": 2.0806,
      "step": 24800
    },
    {
      "epoch": 1.9020701245130243,
      "grad_norm": 5.834392070770264,
      "learning_rate": 4.8414941562905816e-05,
      "loss": 2.0504,
      "step": 24900
    },
    {
      "epoch": 1.909708960354442,
      "grad_norm": 5.735567092895508,
      "learning_rate": 4.84085758663713e-05,
      "loss": 1.9985,
      "step": 25000
    },
    {
      "epoch": 1.9173477961958598,
      "grad_norm": 5.564517021179199,
      "learning_rate": 4.840221016983679e-05,
      "loss": 2.0872,
      "step": 25100
    },
    {
      "epoch": 1.9249866320372775,
      "grad_norm": 6.1440110206604,
      "learning_rate": 4.8395844473302274e-05,
      "loss": 2.1082,
      "step": 25200
    },
    {
      "epoch": 1.9326254678786952,
      "grad_norm": 4.586952209472656,
      "learning_rate": 4.838947877676776e-05,
      "loss": 2.0224,
      "step": 25300
    },
    {
      "epoch": 1.940264303720113,
      "grad_norm": 7.023982048034668,
      "learning_rate": 4.838311308023324e-05,
      "loss": 2.1686,
      "step": 25400
    },
    {
      "epoch": 1.947903139561531,
      "grad_norm": 6.17502498626709,
      "learning_rate": 4.837674738369873e-05,
      "loss": 2.1339,
      "step": 25500
    },
    {
      "epoch": 1.9555419754029486,
      "grad_norm": 4.7290215492248535,
      "learning_rate": 4.8370381687164215e-05,
      "loss": 2.0817,
      "step": 25600
    },
    {
      "epoch": 1.9631808112443663,
      "grad_norm": 6.954816818237305,
      "learning_rate": 4.83640159906297e-05,
      "loss": 2.0845,
      "step": 25700
    },
    {
      "epoch": 1.9708196470857842,
      "grad_norm": 5.419379234313965,
      "learning_rate": 4.835765029409518e-05,
      "loss": 2.1449,
      "step": 25800
    },
    {
      "epoch": 1.978458482927202,
      "grad_norm": 6.62744665145874,
      "learning_rate": 4.835128459756067e-05,
      "loss": 1.9924,
      "step": 25900
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 6.926636695861816,
      "learning_rate": 4.8344918901026156e-05,
      "loss": 2.026,
      "step": 26000
    },
    {
      "epoch": 1.9937361546100374,
      "grad_norm": 4.431833744049072,
      "learning_rate": 4.833855320449164e-05,
      "loss": 2.2406,
      "step": 26100
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9846583604812622,
      "eval_runtime": 1.5879,
      "eval_samples_per_second": 434.54,
      "eval_steps_per_second": 434.54,
      "step": 26182
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8606984615325928,
      "eval_runtime": 30.1086,
      "eval_samples_per_second": 434.792,
      "eval_steps_per_second": 434.792,
      "step": 26182
    },
    {
      "epoch": 2.0013749904514553,
      "grad_norm": 5.950566291809082,
      "learning_rate": 4.833218750795712e-05,
      "loss": 2.0238,
      "step": 26200
    },
    {
      "epoch": 2.009013826292873,
      "grad_norm": 9.99737548828125,
      "learning_rate": 4.8325821811422606e-05,
      "loss": 1.9941,
      "step": 26300
    },
    {
      "epoch": 2.0166526621342906,
      "grad_norm": 6.44681978225708,
      "learning_rate": 4.8319456114888097e-05,
      "loss": 2.0219,
      "step": 26400
    },
    {
      "epoch": 2.0242914979757085,
      "grad_norm": 5.253053665161133,
      "learning_rate": 4.831309041835358e-05,
      "loss": 2.0002,
      "step": 26500
    },
    {
      "epoch": 2.0319303338171264,
      "grad_norm": 5.84773063659668,
      "learning_rate": 4.8306724721819064e-05,
      "loss": 1.9781,
      "step": 26600
    },
    {
      "epoch": 2.039569169658544,
      "grad_norm": 7.012181758880615,
      "learning_rate": 4.830035902528455e-05,
      "loss": 1.9006,
      "step": 26700
    },
    {
      "epoch": 2.0472080054999617,
      "grad_norm": 6.921151638031006,
      "learning_rate": 4.829399332875004e-05,
      "loss": 1.9567,
      "step": 26800
    },
    {
      "epoch": 2.0548468413413796,
      "grad_norm": 6.202018737792969,
      "learning_rate": 4.828762763221552e-05,
      "loss": 2.0808,
      "step": 26900
    },
    {
      "epoch": 2.0624856771827975,
      "grad_norm": 7.00048303604126,
      "learning_rate": 4.8281261935681005e-05,
      "loss": 2.0894,
      "step": 27000
    },
    {
      "epoch": 2.070124513024215,
      "grad_norm": 6.901838779449463,
      "learning_rate": 4.827489623914649e-05,
      "loss": 1.9564,
      "step": 27100
    },
    {
      "epoch": 2.077763348865633,
      "grad_norm": 5.966187477111816,
      "learning_rate": 4.826853054261197e-05,
      "loss": 2.1349,
      "step": 27200
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 5.797510147094727,
      "learning_rate": 4.826216484607746e-05,
      "loss": 2.0865,
      "step": 27300
    },
    {
      "epoch": 2.093041020548468,
      "grad_norm": 5.3418869972229,
      "learning_rate": 4.8255799149542945e-05,
      "loss": 2.0951,
      "step": 27400
    },
    {
      "epoch": 2.100679856389886,
      "grad_norm": 7.292758941650391,
      "learning_rate": 4.824943345300843e-05,
      "loss": 2.0839,
      "step": 27500
    },
    {
      "epoch": 2.108318692231304,
      "grad_norm": 5.246260643005371,
      "learning_rate": 4.824306775647391e-05,
      "loss": 1.9875,
      "step": 27600
    },
    {
      "epoch": 2.115957528072722,
      "grad_norm": 8.36473560333252,
      "learning_rate": 4.8236702059939396e-05,
      "loss": 2.1234,
      "step": 27700
    },
    {
      "epoch": 2.1235963639141393,
      "grad_norm": 6.6102519035339355,
      "learning_rate": 4.8230336363404886e-05,
      "loss": 2.0619,
      "step": 27800
    },
    {
      "epoch": 2.131235199755557,
      "grad_norm": 6.237936496734619,
      "learning_rate": 4.822397066687037e-05,
      "loss": 2.0555,
      "step": 27900
    },
    {
      "epoch": 2.138874035596975,
      "grad_norm": 6.054935455322266,
      "learning_rate": 4.8217604970335853e-05,
      "loss": 1.9449,
      "step": 28000
    },
    {
      "epoch": 2.146512871438393,
      "grad_norm": 4.890835285186768,
      "learning_rate": 4.821123927380134e-05,
      "loss": 1.9925,
      "step": 28100
    },
    {
      "epoch": 2.1541517072798104,
      "grad_norm": 4.241631031036377,
      "learning_rate": 4.820487357726683e-05,
      "loss": 2.0188,
      "step": 28200
    },
    {
      "epoch": 2.1617905431212283,
      "grad_norm": 7.30115270614624,
      "learning_rate": 4.819850788073231e-05,
      "loss": 2.0519,
      "step": 28300
    },
    {
      "epoch": 2.169429378962646,
      "grad_norm": 5.281064510345459,
      "learning_rate": 4.8192142184197794e-05,
      "loss": 2.0368,
      "step": 28400
    },
    {
      "epoch": 2.1770682148040637,
      "grad_norm": 5.906428813934326,
      "learning_rate": 4.818577648766328e-05,
      "loss": 2.1343,
      "step": 28500
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 7.159975051879883,
      "learning_rate": 4.817941079112877e-05,
      "loss": 2.0683,
      "step": 28600
    },
    {
      "epoch": 2.1923458864868994,
      "grad_norm": 6.812931537628174,
      "learning_rate": 4.817304509459425e-05,
      "loss": 2.0976,
      "step": 28700
    },
    {
      "epoch": 2.1999847223283173,
      "grad_norm": 6.089564323425293,
      "learning_rate": 4.8166679398059735e-05,
      "loss": 2.0088,
      "step": 28800
    },
    {
      "epoch": 2.2076235581697348,
      "grad_norm": 3.981692314147949,
      "learning_rate": 4.8160313701525226e-05,
      "loss": 2.0516,
      "step": 28900
    },
    {
      "epoch": 2.2152623940111527,
      "grad_norm": 5.943568706512451,
      "learning_rate": 4.815394800499071e-05,
      "loss": 2.0499,
      "step": 29000
    },
    {
      "epoch": 2.2229012298525705,
      "grad_norm": 4.386171340942383,
      "learning_rate": 4.81475823084562e-05,
      "loss": 2.0451,
      "step": 29100
    },
    {
      "epoch": 2.2305400656939884,
      "grad_norm": 5.8829803466796875,
      "learning_rate": 4.814121661192168e-05,
      "loss": 2.0631,
      "step": 29200
    },
    {
      "epoch": 2.238178901535406,
      "grad_norm": 4.876271724700928,
      "learning_rate": 4.8134850915387167e-05,
      "loss": 2.0432,
      "step": 29300
    },
    {
      "epoch": 2.2458177373768238,
      "grad_norm": 6.325215816497803,
      "learning_rate": 4.812848521885265e-05,
      "loss": 2.0036,
      "step": 29400
    },
    {
      "epoch": 2.2534565732182417,
      "grad_norm": 8.808916091918945,
      "learning_rate": 4.8122119522318134e-05,
      "loss": 2.0206,
      "step": 29500
    },
    {
      "epoch": 2.261095409059659,
      "grad_norm": 5.4486799240112305,
      "learning_rate": 4.8115753825783624e-05,
      "loss": 2.0547,
      "step": 29600
    },
    {
      "epoch": 2.268734244901077,
      "grad_norm": 5.0665411949157715,
      "learning_rate": 4.810938812924911e-05,
      "loss": 2.0543,
      "step": 29700
    },
    {
      "epoch": 2.276373080742495,
      "grad_norm": 5.754215717315674,
      "learning_rate": 4.810302243271459e-05,
      "loss": 2.0934,
      "step": 29800
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 6.137582778930664,
      "learning_rate": 4.8096656736180075e-05,
      "loss": 2.0023,
      "step": 29900
    },
    {
      "epoch": 2.2916507524253302,
      "grad_norm": 5.766072750091553,
      "learning_rate": 4.8090291039645565e-05,
      "loss": 1.9385,
      "step": 30000
    },
    {
      "epoch": 2.299289588266748,
      "grad_norm": 5.748718738555908,
      "learning_rate": 4.808392534311105e-05,
      "loss": 2.0159,
      "step": 30100
    },
    {
      "epoch": 2.306928424108166,
      "grad_norm": 8.573970794677734,
      "learning_rate": 4.807755964657653e-05,
      "loss": 2.0791,
      "step": 30200
    },
    {
      "epoch": 2.314567259949584,
      "grad_norm": 5.100586414337158,
      "learning_rate": 4.8071193950042015e-05,
      "loss": 2.0181,
      "step": 30300
    },
    {
      "epoch": 2.3222060957910013,
      "grad_norm": 6.933751106262207,
      "learning_rate": 4.80648282535075e-05,
      "loss": 2.057,
      "step": 30400
    },
    {
      "epoch": 2.3298449316324192,
      "grad_norm": 6.277702808380127,
      "learning_rate": 4.805846255697299e-05,
      "loss": 2.0722,
      "step": 30500
    },
    {
      "epoch": 2.337483767473837,
      "grad_norm": 5.68227481842041,
      "learning_rate": 4.805209686043847e-05,
      "loss": 1.9991,
      "step": 30600
    },
    {
      "epoch": 2.3451226033152546,
      "grad_norm": 5.003713607788086,
      "learning_rate": 4.8045731163903956e-05,
      "loss": 2.0849,
      "step": 30700
    },
    {
      "epoch": 2.3527614391566725,
      "grad_norm": 5.61934232711792,
      "learning_rate": 4.803936546736944e-05,
      "loss": 2.1332,
      "step": 30800
    },
    {
      "epoch": 2.3604002749980904,
      "grad_norm": 5.77763032913208,
      "learning_rate": 4.8032999770834923e-05,
      "loss": 1.9759,
      "step": 30900
    },
    {
      "epoch": 2.3680391108395082,
      "grad_norm": 5.221537113189697,
      "learning_rate": 4.8026634074300414e-05,
      "loss": 1.9899,
      "step": 31000
    },
    {
      "epoch": 2.3756779466809257,
      "grad_norm": 6.0240912437438965,
      "learning_rate": 4.80202683777659e-05,
      "loss": 2.0215,
      "step": 31100
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 5.177725791931152,
      "learning_rate": 4.801390268123138e-05,
      "loss": 2.0815,
      "step": 31200
    },
    {
      "epoch": 2.3909556183637615,
      "grad_norm": 4.6330647468566895,
      "learning_rate": 4.8007536984696864e-05,
      "loss": 2.042,
      "step": 31300
    },
    {
      "epoch": 2.3985944542051794,
      "grad_norm": 5.797443866729736,
      "learning_rate": 4.8001171288162355e-05,
      "loss": 2.006,
      "step": 31400
    },
    {
      "epoch": 2.406233290046597,
      "grad_norm": 7.3135786056518555,
      "learning_rate": 4.799480559162784e-05,
      "loss": 2.0208,
      "step": 31500
    },
    {
      "epoch": 2.4138721258880147,
      "grad_norm": 7.213516712188721,
      "learning_rate": 4.798843989509332e-05,
      "loss": 2.0596,
      "step": 31600
    },
    {
      "epoch": 2.4215109617294326,
      "grad_norm": 5.991611957550049,
      "learning_rate": 4.7982074198558805e-05,
      "loss": 2.0258,
      "step": 31700
    },
    {
      "epoch": 2.42914979757085,
      "grad_norm": 5.510334014892578,
      "learning_rate": 4.797570850202429e-05,
      "loss": 2.0754,
      "step": 31800
    },
    {
      "epoch": 2.436788633412268,
      "grad_norm": 4.686400890350342,
      "learning_rate": 4.796934280548978e-05,
      "loss": 2.0662,
      "step": 31900
    },
    {
      "epoch": 2.444427469253686,
      "grad_norm": 5.602585792541504,
      "learning_rate": 4.796297710895526e-05,
      "loss": 2.1458,
      "step": 32000
    },
    {
      "epoch": 2.4520663050951033,
      "grad_norm": 6.115407943725586,
      "learning_rate": 4.7956611412420746e-05,
      "loss": 1.9565,
      "step": 32100
    },
    {
      "epoch": 2.459705140936521,
      "grad_norm": 5.713426113128662,
      "learning_rate": 4.795024571588623e-05,
      "loss": 2.1086,
      "step": 32200
    },
    {
      "epoch": 2.467343976777939,
      "grad_norm": 5.789259433746338,
      "learning_rate": 4.794388001935172e-05,
      "loss": 2.0585,
      "step": 32300
    },
    {
      "epoch": 2.474982812619357,
      "grad_norm": 7.800492763519287,
      "learning_rate": 4.7937514322817204e-05,
      "loss": 2.0023,
      "step": 32400
    },
    {
      "epoch": 2.482621648460775,
      "grad_norm": 5.012712001800537,
      "learning_rate": 4.793114862628269e-05,
      "loss": 2.0327,
      "step": 32500
    },
    {
      "epoch": 2.4902604843021923,
      "grad_norm": 7.399918079376221,
      "learning_rate": 4.792478292974818e-05,
      "loss": 2.0084,
      "step": 32600
    },
    {
      "epoch": 2.49789932014361,
      "grad_norm": 5.23894739151001,
      "learning_rate": 4.791841723321366e-05,
      "loss": 2.0717,
      "step": 32700
    },
    {
      "epoch": 2.505538155985028,
      "grad_norm": 5.336965560913086,
      "learning_rate": 4.7912051536679145e-05,
      "loss": 2.0564,
      "step": 32800
    },
    {
      "epoch": 2.5131769918264455,
      "grad_norm": 4.746248245239258,
      "learning_rate": 4.7905685840144635e-05,
      "loss": 2.0255,
      "step": 32900
    },
    {
      "epoch": 2.5208158276678634,
      "grad_norm": 7.244431972503662,
      "learning_rate": 4.789932014361012e-05,
      "loss": 2.0791,
      "step": 33000
    },
    {
      "epoch": 2.5284546635092813,
      "grad_norm": 4.365599632263184,
      "learning_rate": 4.78929544470756e-05,
      "loss": 1.9449,
      "step": 33100
    },
    {
      "epoch": 2.5360934993506987,
      "grad_norm": 6.474541187286377,
      "learning_rate": 4.7886588750541085e-05,
      "loss": 1.9595,
      "step": 33200
    },
    {
      "epoch": 2.5437323351921166,
      "grad_norm": 6.150533676147461,
      "learning_rate": 4.7880223054006576e-05,
      "loss": 2.0339,
      "step": 33300
    },
    {
      "epoch": 2.5513711710335345,
      "grad_norm": 5.065194606781006,
      "learning_rate": 4.787385735747206e-05,
      "loss": 2.0425,
      "step": 33400
    },
    {
      "epoch": 2.5590100068749524,
      "grad_norm": 5.568298816680908,
      "learning_rate": 4.786749166093754e-05,
      "loss": 1.9772,
      "step": 33500
    },
    {
      "epoch": 2.5666488427163703,
      "grad_norm": 5.161732196807861,
      "learning_rate": 4.7861125964403026e-05,
      "loss": 2.108,
      "step": 33600
    },
    {
      "epoch": 2.5742876785577877,
      "grad_norm": 5.9029669761657715,
      "learning_rate": 4.785476026786852e-05,
      "loss": 2.0083,
      "step": 33700
    },
    {
      "epoch": 2.5819265143992056,
      "grad_norm": 7.695694923400879,
      "learning_rate": 4.7848394571334e-05,
      "loss": 2.0355,
      "step": 33800
    },
    {
      "epoch": 2.5895653502406235,
      "grad_norm": 4.744174003601074,
      "learning_rate": 4.7842028874799484e-05,
      "loss": 2.0567,
      "step": 33900
    },
    {
      "epoch": 2.597204186082041,
      "grad_norm": 6.0987629890441895,
      "learning_rate": 4.783566317826497e-05,
      "loss": 2.019,
      "step": 34000
    },
    {
      "epoch": 2.604843021923459,
      "grad_norm": 5.930823802947998,
      "learning_rate": 4.782929748173045e-05,
      "loss": 2.0671,
      "step": 34100
    },
    {
      "epoch": 2.6124818577648767,
      "grad_norm": 5.213524341583252,
      "learning_rate": 4.782293178519594e-05,
      "loss": 1.9958,
      "step": 34200
    },
    {
      "epoch": 2.620120693606294,
      "grad_norm": 5.034520626068115,
      "learning_rate": 4.7816566088661425e-05,
      "loss": 2.0123,
      "step": 34300
    },
    {
      "epoch": 2.627759529447712,
      "grad_norm": 7.213338375091553,
      "learning_rate": 4.781020039212691e-05,
      "loss": 1.9,
      "step": 34400
    },
    {
      "epoch": 2.63539836528913,
      "grad_norm": 4.886950492858887,
      "learning_rate": 4.780383469559239e-05,
      "loss": 1.9589,
      "step": 34500
    },
    {
      "epoch": 2.643037201130548,
      "grad_norm": 6.640381336212158,
      "learning_rate": 4.779746899905788e-05,
      "loss": 2.0085,
      "step": 34600
    },
    {
      "epoch": 2.6506760369719657,
      "grad_norm": 5.7937517166137695,
      "learning_rate": 4.7791103302523366e-05,
      "loss": 1.9613,
      "step": 34700
    },
    {
      "epoch": 2.658314872813383,
      "grad_norm": 4.450939178466797,
      "learning_rate": 4.778473760598885e-05,
      "loss": 2.0595,
      "step": 34800
    },
    {
      "epoch": 2.665953708654801,
      "grad_norm": 6.985577583312988,
      "learning_rate": 4.777837190945433e-05,
      "loss": 2.1156,
      "step": 34900
    },
    {
      "epoch": 2.673592544496219,
      "grad_norm": 4.613126277923584,
      "learning_rate": 4.7772006212919816e-05,
      "loss": 2.1117,
      "step": 35000
    },
    {
      "epoch": 2.6812313803376364,
      "grad_norm": 5.554779052734375,
      "learning_rate": 4.7765640516385307e-05,
      "loss": 2.1139,
      "step": 35100
    },
    {
      "epoch": 2.6888702161790543,
      "grad_norm": 4.749013900756836,
      "learning_rate": 4.775927481985079e-05,
      "loss": 2.0161,
      "step": 35200
    },
    {
      "epoch": 2.696509052020472,
      "grad_norm": 6.164757251739502,
      "learning_rate": 4.7752909123316274e-05,
      "loss": 2.047,
      "step": 35300
    },
    {
      "epoch": 2.7041478878618896,
      "grad_norm": 5.553055286407471,
      "learning_rate": 4.774654342678176e-05,
      "loss": 1.9584,
      "step": 35400
    },
    {
      "epoch": 2.7117867237033075,
      "grad_norm": 4.341431140899658,
      "learning_rate": 4.774017773024725e-05,
      "loss": 2.0927,
      "step": 35500
    },
    {
      "epoch": 2.7194255595447254,
      "grad_norm": 5.3511481285095215,
      "learning_rate": 4.773381203371273e-05,
      "loss": 2.1132,
      "step": 35600
    },
    {
      "epoch": 2.7270643953861433,
      "grad_norm": 5.124410629272461,
      "learning_rate": 4.7727446337178215e-05,
      "loss": 2.0389,
      "step": 35700
    },
    {
      "epoch": 2.7347032312275608,
      "grad_norm": 4.569272041320801,
      "learning_rate": 4.77210806406437e-05,
      "loss": 1.9934,
      "step": 35800
    },
    {
      "epoch": 2.7423420670689787,
      "grad_norm": 5.052617073059082,
      "learning_rate": 4.771471494410918e-05,
      "loss": 2.0253,
      "step": 35900
    },
    {
      "epoch": 2.7499809029103965,
      "grad_norm": 4.092185020446777,
      "learning_rate": 4.770834924757467e-05,
      "loss": 1.9075,
      "step": 36000
    },
    {
      "epoch": 2.7576197387518144,
      "grad_norm": 5.258288383483887,
      "learning_rate": 4.7701983551040155e-05,
      "loss": 1.9347,
      "step": 36100
    },
    {
      "epoch": 2.765258574593232,
      "grad_norm": 5.291908264160156,
      "learning_rate": 4.769561785450564e-05,
      "loss": 1.957,
      "step": 36200
    },
    {
      "epoch": 2.7728974104346498,
      "grad_norm": 6.225048542022705,
      "learning_rate": 4.768925215797113e-05,
      "loss": 2.0009,
      "step": 36300
    },
    {
      "epoch": 2.7805362462760677,
      "grad_norm": 7.8860273361206055,
      "learning_rate": 4.768288646143661e-05,
      "loss": 2.0516,
      "step": 36400
    },
    {
      "epoch": 2.788175082117485,
      "grad_norm": 6.543915271759033,
      "learning_rate": 4.7676520764902096e-05,
      "loss": 2.0421,
      "step": 36500
    },
    {
      "epoch": 2.795813917958903,
      "grad_norm": 5.307788848876953,
      "learning_rate": 4.767015506836759e-05,
      "loss": 2.026,
      "step": 36600
    },
    {
      "epoch": 2.803452753800321,
      "grad_norm": 5.05996036529541,
      "learning_rate": 4.766378937183307e-05,
      "loss": 2.0635,
      "step": 36700
    },
    {
      "epoch": 2.8110915896417383,
      "grad_norm": 5.592319488525391,
      "learning_rate": 4.7657423675298554e-05,
      "loss": 1.9762,
      "step": 36800
    },
    {
      "epoch": 2.818730425483156,
      "grad_norm": 6.186068058013916,
      "learning_rate": 4.7651057978764044e-05,
      "loss": 1.9455,
      "step": 36900
    },
    {
      "epoch": 2.826369261324574,
      "grad_norm": 6.108480930328369,
      "learning_rate": 4.764469228222953e-05,
      "loss": 2.0316,
      "step": 37000
    },
    {
      "epoch": 2.834008097165992,
      "grad_norm": 5.423502445220947,
      "learning_rate": 4.763832658569501e-05,
      "loss": 1.9763,
      "step": 37100
    },
    {
      "epoch": 2.84164693300741,
      "grad_norm": 6.912816524505615,
      "learning_rate": 4.7631960889160495e-05,
      "loss": 2.085,
      "step": 37200
    },
    {
      "epoch": 2.8492857688488273,
      "grad_norm": 6.704920291900635,
      "learning_rate": 4.762559519262598e-05,
      "loss": 2.0164,
      "step": 37300
    },
    {
      "epoch": 2.8569246046902452,
      "grad_norm": 6.517366409301758,
      "learning_rate": 4.761922949609147e-05,
      "loss": 2.1139,
      "step": 37400
    },
    {
      "epoch": 2.864563440531663,
      "grad_norm": 5.06290340423584,
      "learning_rate": 4.761286379955695e-05,
      "loss": 1.9332,
      "step": 37500
    },
    {
      "epoch": 2.8722022763730806,
      "grad_norm": 4.601421356201172,
      "learning_rate": 4.7606498103022436e-05,
      "loss": 2.0983,
      "step": 37600
    },
    {
      "epoch": 2.8798411122144985,
      "grad_norm": 3.946596622467041,
      "learning_rate": 4.760013240648792e-05,
      "loss": 2.0387,
      "step": 37700
    },
    {
      "epoch": 2.8874799480559163,
      "grad_norm": 5.37327241897583,
      "learning_rate": 4.759376670995341e-05,
      "loss": 1.9947,
      "step": 37800
    },
    {
      "epoch": 2.895118783897334,
      "grad_norm": 4.541653156280518,
      "learning_rate": 4.758740101341889e-05,
      "loss": 1.9845,
      "step": 37900
    },
    {
      "epoch": 2.9027576197387517,
      "grad_norm": 5.527278900146484,
      "learning_rate": 4.7581035316884377e-05,
      "loss": 1.9391,
      "step": 38000
    },
    {
      "epoch": 2.9103964555801696,
      "grad_norm": 5.3603973388671875,
      "learning_rate": 4.757466962034986e-05,
      "loss": 2.0637,
      "step": 38100
    },
    {
      "epoch": 2.9180352914215875,
      "grad_norm": 6.174798488616943,
      "learning_rate": 4.7568303923815344e-05,
      "loss": 2.1327,
      "step": 38200
    },
    {
      "epoch": 2.9256741272630054,
      "grad_norm": 5.1799492835998535,
      "learning_rate": 4.7561938227280834e-05,
      "loss": 2.0031,
      "step": 38300
    },
    {
      "epoch": 2.933312963104423,
      "grad_norm": 5.095165252685547,
      "learning_rate": 4.755557253074632e-05,
      "loss": 2.0236,
      "step": 38400
    },
    {
      "epoch": 2.9409517989458407,
      "grad_norm": 5.397559642791748,
      "learning_rate": 4.75492068342118e-05,
      "loss": 2.0848,
      "step": 38500
    },
    {
      "epoch": 2.9485906347872586,
      "grad_norm": 7.755533218383789,
      "learning_rate": 4.7542841137677285e-05,
      "loss": 1.9946,
      "step": 38600
    },
    {
      "epoch": 2.956229470628676,
      "grad_norm": 5.733693599700928,
      "learning_rate": 4.7536475441142775e-05,
      "loss": 1.9804,
      "step": 38700
    },
    {
      "epoch": 2.963868306470094,
      "grad_norm": 5.20625114440918,
      "learning_rate": 4.753010974460826e-05,
      "loss": 1.9419,
      "step": 38800
    },
    {
      "epoch": 2.971507142311512,
      "grad_norm": 5.1734418869018555,
      "learning_rate": 4.752374404807374e-05,
      "loss": 2.157,
      "step": 38900
    },
    {
      "epoch": 2.9791459781529293,
      "grad_norm": 5.5127949714660645,
      "learning_rate": 4.7517378351539225e-05,
      "loss": 2.1461,
      "step": 39000
    },
    {
      "epoch": 2.986784813994347,
      "grad_norm": 7.195877552032471,
      "learning_rate": 4.751101265500471e-05,
      "loss": 1.9757,
      "step": 39100
    },
    {
      "epoch": 2.994423649835765,
      "grad_norm": 4.758700370788574,
      "learning_rate": 4.75046469584702e-05,
      "loss": 2.0629,
      "step": 39200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9468870162963867,
      "eval_runtime": 1.656,
      "eval_samples_per_second": 416.656,
      "eval_steps_per_second": 416.656,
      "step": 39273
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.803314208984375,
      "eval_runtime": 32.0889,
      "eval_samples_per_second": 407.96,
      "eval_steps_per_second": 407.96,
      "step": 39273
    },
    {
      "epoch": 3.002062485677183,
      "grad_norm": 7.290513038635254,
      "learning_rate": 4.749828126193568e-05,
      "loss": 2.0622,
      "step": 39300
    },
    {
      "epoch": 3.0097013215186004,
      "grad_norm": 5.617524147033691,
      "learning_rate": 4.7491915565401166e-05,
      "loss": 1.9092,
      "step": 39400
    },
    {
      "epoch": 3.0173401573600183,
      "grad_norm": 8.197392463684082,
      "learning_rate": 4.748554986886665e-05,
      "loss": 1.9719,
      "step": 39500
    },
    {
      "epoch": 3.024978993201436,
      "grad_norm": 5.304454803466797,
      "learning_rate": 4.7479184172332133e-05,
      "loss": 1.9798,
      "step": 39600
    },
    {
      "epoch": 3.032617829042854,
      "grad_norm": 12.339706420898438,
      "learning_rate": 4.7472818475797624e-05,
      "loss": 2.0365,
      "step": 39700
    },
    {
      "epoch": 3.0402566648842715,
      "grad_norm": 5.0507378578186035,
      "learning_rate": 4.746645277926311e-05,
      "loss": 1.9051,
      "step": 39800
    },
    {
      "epoch": 3.0478955007256894,
      "grad_norm": 4.048762798309326,
      "learning_rate": 4.746008708272859e-05,
      "loss": 1.9011,
      "step": 39900
    },
    {
      "epoch": 3.0555343365671073,
      "grad_norm": 7.273528575897217,
      "learning_rate": 4.7453721386194074e-05,
      "loss": 1.9663,
      "step": 40000
    },
    {
      "epoch": 3.0631731724085247,
      "grad_norm": 6.660534381866455,
      "learning_rate": 4.7447355689659565e-05,
      "loss": 2.0383,
      "step": 40100
    },
    {
      "epoch": 3.0708120082499426,
      "grad_norm": 5.093486309051514,
      "learning_rate": 4.744098999312505e-05,
      "loss": 2.0899,
      "step": 40200
    },
    {
      "epoch": 3.0784508440913605,
      "grad_norm": 4.743340969085693,
      "learning_rate": 4.743462429659054e-05,
      "loss": 1.9693,
      "step": 40300
    },
    {
      "epoch": 3.0860896799327784,
      "grad_norm": 9.660433769226074,
      "learning_rate": 4.742825860005602e-05,
      "loss": 2.1328,
      "step": 40400
    },
    {
      "epoch": 3.093728515774196,
      "grad_norm": 6.044274806976318,
      "learning_rate": 4.7421892903521506e-05,
      "loss": 1.9548,
      "step": 40500
    },
    {
      "epoch": 3.1013673516156137,
      "grad_norm": 5.044424533843994,
      "learning_rate": 4.7415527206986996e-05,
      "loss": 2.0636,
      "step": 40600
    },
    {
      "epoch": 3.1090061874570316,
      "grad_norm": 6.506568908691406,
      "learning_rate": 4.740916151045248e-05,
      "loss": 2.0494,
      "step": 40700
    },
    {
      "epoch": 3.1166450232984495,
      "grad_norm": 4.715335369110107,
      "learning_rate": 4.740279581391796e-05,
      "loss": 1.9583,
      "step": 40800
    },
    {
      "epoch": 3.124283859139867,
      "grad_norm": 7.75506591796875,
      "learning_rate": 4.7396430117383447e-05,
      "loss": 2.0359,
      "step": 40900
    },
    {
      "epoch": 3.131922694981285,
      "grad_norm": 4.1952900886535645,
      "learning_rate": 4.739006442084894e-05,
      "loss": 1.9899,
      "step": 41000
    },
    {
      "epoch": 3.1395615308227027,
      "grad_norm": 7.368591785430908,
      "learning_rate": 4.738369872431442e-05,
      "loss": 1.9509,
      "step": 41100
    },
    {
      "epoch": 3.14720036666412,
      "grad_norm": 5.82589054107666,
      "learning_rate": 4.7377333027779904e-05,
      "loss": 2.0598,
      "step": 41200
    },
    {
      "epoch": 3.154839202505538,
      "grad_norm": 6.8241376876831055,
      "learning_rate": 4.737096733124539e-05,
      "loss": 2.0186,
      "step": 41300
    },
    {
      "epoch": 3.162478038346956,
      "grad_norm": 5.692176818847656,
      "learning_rate": 4.736460163471087e-05,
      "loss": 2.0151,
      "step": 41400
    },
    {
      "epoch": 3.170116874188374,
      "grad_norm": 4.566624164581299,
      "learning_rate": 4.735823593817636e-05,
      "loss": 1.9382,
      "step": 41500
    },
    {
      "epoch": 3.1777557100297913,
      "grad_norm": 6.670254230499268,
      "learning_rate": 4.7351870241641845e-05,
      "loss": 1.9612,
      "step": 41600
    },
    {
      "epoch": 3.185394545871209,
      "grad_norm": 6.2681145668029785,
      "learning_rate": 4.734550454510733e-05,
      "loss": 1.9056,
      "step": 41700
    },
    {
      "epoch": 3.193033381712627,
      "grad_norm": 6.009606838226318,
      "learning_rate": 4.733913884857281e-05,
      "loss": 1.9643,
      "step": 41800
    },
    {
      "epoch": 3.200672217554045,
      "grad_norm": 6.42953634262085,
      "learning_rate": 4.7332773152038295e-05,
      "loss": 1.9375,
      "step": 41900
    },
    {
      "epoch": 3.2083110533954624,
      "grad_norm": 7.146541118621826,
      "learning_rate": 4.7326407455503786e-05,
      "loss": 1.9979,
      "step": 42000
    },
    {
      "epoch": 3.2159498892368803,
      "grad_norm": 7.6325883865356445,
      "learning_rate": 4.732004175896927e-05,
      "loss": 1.8745,
      "step": 42100
    },
    {
      "epoch": 3.223588725078298,
      "grad_norm": 5.143475532531738,
      "learning_rate": 4.731367606243475e-05,
      "loss": 2.0695,
      "step": 42200
    },
    {
      "epoch": 3.2312275609197156,
      "grad_norm": 6.101351261138916,
      "learning_rate": 4.7307310365900236e-05,
      "loss": 2.0129,
      "step": 42300
    },
    {
      "epoch": 3.2388663967611335,
      "grad_norm": 5.0296525955200195,
      "learning_rate": 4.730094466936573e-05,
      "loss": 2.0046,
      "step": 42400
    },
    {
      "epoch": 3.2465052326025514,
      "grad_norm": 5.436621189117432,
      "learning_rate": 4.729457897283121e-05,
      "loss": 2.0401,
      "step": 42500
    },
    {
      "epoch": 3.2541440684439693,
      "grad_norm": 7.283679962158203,
      "learning_rate": 4.7288213276296694e-05,
      "loss": 1.9053,
      "step": 42600
    },
    {
      "epoch": 3.2617829042853868,
      "grad_norm": 5.767353057861328,
      "learning_rate": 4.728184757976218e-05,
      "loss": 2.0405,
      "step": 42700
    },
    {
      "epoch": 3.2694217401268046,
      "grad_norm": 6.808155536651611,
      "learning_rate": 4.727548188322766e-05,
      "loss": 1.982,
      "step": 42800
    },
    {
      "epoch": 3.2770605759682225,
      "grad_norm": 6.65542459487915,
      "learning_rate": 4.726911618669315e-05,
      "loss": 1.9419,
      "step": 42900
    },
    {
      "epoch": 3.2846994118096404,
      "grad_norm": 6.912124156951904,
      "learning_rate": 4.7262750490158635e-05,
      "loss": 2.0087,
      "step": 43000
    },
    {
      "epoch": 3.292338247651058,
      "grad_norm": 5.111575126647949,
      "learning_rate": 4.725638479362412e-05,
      "loss": 1.9677,
      "step": 43100
    },
    {
      "epoch": 3.2999770834924758,
      "grad_norm": 7.323481559753418,
      "learning_rate": 4.72500190970896e-05,
      "loss": 2.0033,
      "step": 43200
    },
    {
      "epoch": 3.3076159193338937,
      "grad_norm": 4.7093329429626465,
      "learning_rate": 4.724365340055509e-05,
      "loss": 1.9845,
      "step": 43300
    },
    {
      "epoch": 3.315254755175311,
      "grad_norm": 4.963481903076172,
      "learning_rate": 4.7237287704020576e-05,
      "loss": 2.066,
      "step": 43400
    },
    {
      "epoch": 3.322893591016729,
      "grad_norm": 4.849010944366455,
      "learning_rate": 4.723092200748606e-05,
      "loss": 1.9572,
      "step": 43500
    },
    {
      "epoch": 3.330532426858147,
      "grad_norm": 6.953652381896973,
      "learning_rate": 4.722455631095154e-05,
      "loss": 1.9995,
      "step": 43600
    },
    {
      "epoch": 3.3381712626995648,
      "grad_norm": 6.2171831130981445,
      "learning_rate": 4.7218190614417026e-05,
      "loss": 1.9397,
      "step": 43700
    },
    {
      "epoch": 3.345810098540982,
      "grad_norm": 4.090816974639893,
      "learning_rate": 4.7211824917882517e-05,
      "loss": 2.0161,
      "step": 43800
    },
    {
      "epoch": 3.3534489343824,
      "grad_norm": 5.5140767097473145,
      "learning_rate": 4.7205459221348e-05,
      "loss": 2.0307,
      "step": 43900
    },
    {
      "epoch": 3.361087770223818,
      "grad_norm": 5.972023963928223,
      "learning_rate": 4.7199093524813484e-05,
      "loss": 2.0146,
      "step": 44000
    },
    {
      "epoch": 3.368726606065236,
      "grad_norm": 4.871480941772461,
      "learning_rate": 4.7192727828278974e-05,
      "loss": 1.8482,
      "step": 44100
    },
    {
      "epoch": 3.3763654419066533,
      "grad_norm": 6.590645790100098,
      "learning_rate": 4.718636213174446e-05,
      "loss": 1.9759,
      "step": 44200
    },
    {
      "epoch": 3.384004277748071,
      "grad_norm": 5.7040886878967285,
      "learning_rate": 4.717999643520995e-05,
      "loss": 2.0019,
      "step": 44300
    },
    {
      "epoch": 3.391643113589489,
      "grad_norm": 5.220544815063477,
      "learning_rate": 4.717363073867543e-05,
      "loss": 2.0461,
      "step": 44400
    },
    {
      "epoch": 3.3992819494309066,
      "grad_norm": 5.867302894592285,
      "learning_rate": 4.7167265042140915e-05,
      "loss": 2.0834,
      "step": 44500
    },
    {
      "epoch": 3.4069207852723244,
      "grad_norm": 6.006386756896973,
      "learning_rate": 4.71608993456064e-05,
      "loss": 2.0092,
      "step": 44600
    },
    {
      "epoch": 3.4145596211137423,
      "grad_norm": 5.242704391479492,
      "learning_rate": 4.715453364907189e-05,
      "loss": 1.9885,
      "step": 44700
    },
    {
      "epoch": 3.42219845695516,
      "grad_norm": 5.758867263793945,
      "learning_rate": 4.714816795253737e-05,
      "loss": 2.0009,
      "step": 44800
    },
    {
      "epoch": 3.4298372927965777,
      "grad_norm": 4.8147969245910645,
      "learning_rate": 4.7141802256002856e-05,
      "loss": 1.9797,
      "step": 44900
    },
    {
      "epoch": 3.4374761286379956,
      "grad_norm": 6.039651870727539,
      "learning_rate": 4.713543655946834e-05,
      "loss": 2.0535,
      "step": 45000
    },
    {
      "epoch": 3.4451149644794135,
      "grad_norm": 7.391009330749512,
      "learning_rate": 4.712907086293382e-05,
      "loss": 2.0489,
      "step": 45100
    },
    {
      "epoch": 3.4527538003208313,
      "grad_norm": 5.303275108337402,
      "learning_rate": 4.712270516639931e-05,
      "loss": 2.0678,
      "step": 45200
    },
    {
      "epoch": 3.460392636162249,
      "grad_norm": 4.585079193115234,
      "learning_rate": 4.71163394698648e-05,
      "loss": 1.981,
      "step": 45300
    },
    {
      "epoch": 3.4680314720036667,
      "grad_norm": 7.310197353363037,
      "learning_rate": 4.710997377333028e-05,
      "loss": 1.9823,
      "step": 45400
    },
    {
      "epoch": 3.4756703078450846,
      "grad_norm": 5.645636558532715,
      "learning_rate": 4.7103608076795764e-05,
      "loss": 2.0462,
      "step": 45500
    },
    {
      "epoch": 3.483309143686502,
      "grad_norm": 4.865365982055664,
      "learning_rate": 4.7097242380261254e-05,
      "loss": 2.0685,
      "step": 45600
    },
    {
      "epoch": 3.49094797952792,
      "grad_norm": 5.471009254455566,
      "learning_rate": 4.709087668372674e-05,
      "loss": 1.9951,
      "step": 45700
    },
    {
      "epoch": 3.498586815369338,
      "grad_norm": 8.290167808532715,
      "learning_rate": 4.708451098719222e-05,
      "loss": 1.965,
      "step": 45800
    },
    {
      "epoch": 3.5062256512107552,
      "grad_norm": 7.400575637817383,
      "learning_rate": 4.7078145290657705e-05,
      "loss": 1.8216,
      "step": 45900
    },
    {
      "epoch": 3.513864487052173,
      "grad_norm": 5.654378414154053,
      "learning_rate": 4.707177959412319e-05,
      "loss": 1.9386,
      "step": 46000
    },
    {
      "epoch": 3.521503322893591,
      "grad_norm": 7.024972915649414,
      "learning_rate": 4.706541389758868e-05,
      "loss": 1.971,
      "step": 46100
    },
    {
      "epoch": 3.529142158735009,
      "grad_norm": 6.977832794189453,
      "learning_rate": 4.705904820105416e-05,
      "loss": 2.0436,
      "step": 46200
    },
    {
      "epoch": 3.536780994576427,
      "grad_norm": 5.344612121582031,
      "learning_rate": 4.7052682504519646e-05,
      "loss": 2.0318,
      "step": 46300
    },
    {
      "epoch": 3.5444198304178443,
      "grad_norm": 4.636826038360596,
      "learning_rate": 4.704631680798513e-05,
      "loss": 2.0151,
      "step": 46400
    },
    {
      "epoch": 3.552058666259262,
      "grad_norm": 5.049445152282715,
      "learning_rate": 4.703995111145062e-05,
      "loss": 1.8697,
      "step": 46500
    },
    {
      "epoch": 3.55969750210068,
      "grad_norm": 4.779690742492676,
      "learning_rate": 4.70335854149161e-05,
      "loss": 1.9433,
      "step": 46600
    },
    {
      "epoch": 3.5673363379420975,
      "grad_norm": 7.027048587799072,
      "learning_rate": 4.7027219718381587e-05,
      "loss": 1.8943,
      "step": 46700
    },
    {
      "epoch": 3.5749751737835154,
      "grad_norm": 7.132155418395996,
      "learning_rate": 4.702085402184707e-05,
      "loss": 1.9812,
      "step": 46800
    },
    {
      "epoch": 3.5826140096249333,
      "grad_norm": 5.960237979888916,
      "learning_rate": 4.7014488325312554e-05,
      "loss": 2.0148,
      "step": 46900
    },
    {
      "epoch": 3.5902528454663507,
      "grad_norm": 5.437136173248291,
      "learning_rate": 4.7008122628778044e-05,
      "loss": 2.0772,
      "step": 47000
    },
    {
      "epoch": 3.5978916813077686,
      "grad_norm": 5.949313640594482,
      "learning_rate": 4.700175693224353e-05,
      "loss": 1.9788,
      "step": 47100
    },
    {
      "epoch": 3.6055305171491865,
      "grad_norm": 6.849006652832031,
      "learning_rate": 4.699539123570901e-05,
      "loss": 1.9503,
      "step": 47200
    },
    {
      "epoch": 3.6131693529906044,
      "grad_norm": 6.066008567810059,
      "learning_rate": 4.6989025539174495e-05,
      "loss": 2.0181,
      "step": 47300
    },
    {
      "epoch": 3.6208081888320223,
      "grad_norm": 8.36220645904541,
      "learning_rate": 4.6982659842639985e-05,
      "loss": 1.9637,
      "step": 47400
    },
    {
      "epoch": 3.6284470246734397,
      "grad_norm": 5.7708258628845215,
      "learning_rate": 4.697629414610547e-05,
      "loss": 1.9563,
      "step": 47500
    },
    {
      "epoch": 3.6360858605148576,
      "grad_norm": 4.858521461486816,
      "learning_rate": 4.696992844957095e-05,
      "loss": 1.9916,
      "step": 47600
    },
    {
      "epoch": 3.6437246963562755,
      "grad_norm": 7.052722930908203,
      "learning_rate": 4.6963562753036435e-05,
      "loss": 1.952,
      "step": 47700
    },
    {
      "epoch": 3.651363532197693,
      "grad_norm": 5.940395355224609,
      "learning_rate": 4.6957197056501926e-05,
      "loss": 2.0252,
      "step": 47800
    },
    {
      "epoch": 3.659002368039111,
      "grad_norm": 7.704006195068359,
      "learning_rate": 4.695083135996741e-05,
      "loss": 2.0298,
      "step": 47900
    },
    {
      "epoch": 3.6666412038805287,
      "grad_norm": 5.838247299194336,
      "learning_rate": 4.694446566343289e-05,
      "loss": 2.0261,
      "step": 48000
    },
    {
      "epoch": 3.674280039721946,
      "grad_norm": 6.521697044372559,
      "learning_rate": 4.693809996689838e-05,
      "loss": 2.0215,
      "step": 48100
    },
    {
      "epoch": 3.681918875563364,
      "grad_norm": 5.504314422607422,
      "learning_rate": 4.693173427036387e-05,
      "loss": 2.0898,
      "step": 48200
    },
    {
      "epoch": 3.689557711404782,
      "grad_norm": 5.938118934631348,
      "learning_rate": 4.692536857382935e-05,
      "loss": 1.9834,
      "step": 48300
    },
    {
      "epoch": 3.6971965472462,
      "grad_norm": 5.414301872253418,
      "learning_rate": 4.691900287729484e-05,
      "loss": 2.0336,
      "step": 48400
    },
    {
      "epoch": 3.7048353830876173,
      "grad_norm": 5.213018894195557,
      "learning_rate": 4.6912637180760324e-05,
      "loss": 2.034,
      "step": 48500
    },
    {
      "epoch": 3.712474218929035,
      "grad_norm": 5.563769817352295,
      "learning_rate": 4.690627148422581e-05,
      "loss": 1.9601,
      "step": 48600
    },
    {
      "epoch": 3.720113054770453,
      "grad_norm": 4.4282307624816895,
      "learning_rate": 4.689990578769129e-05,
      "loss": 2.0302,
      "step": 48700
    },
    {
      "epoch": 3.727751890611871,
      "grad_norm": 5.319873332977295,
      "learning_rate": 4.689354009115678e-05,
      "loss": 1.9491,
      "step": 48800
    },
    {
      "epoch": 3.7353907264532884,
      "grad_norm": 5.224166393280029,
      "learning_rate": 4.6887174394622265e-05,
      "loss": 1.9717,
      "step": 48900
    },
    {
      "epoch": 3.7430295622947063,
      "grad_norm": 4.4538774490356445,
      "learning_rate": 4.688080869808775e-05,
      "loss": 2.0502,
      "step": 49000
    },
    {
      "epoch": 3.750668398136124,
      "grad_norm": 5.300047874450684,
      "learning_rate": 4.687444300155323e-05,
      "loss": 1.9337,
      "step": 49100
    },
    {
      "epoch": 3.7583072339775416,
      "grad_norm": 4.853061676025391,
      "learning_rate": 4.6868077305018716e-05,
      "loss": 1.9664,
      "step": 49200
    },
    {
      "epoch": 3.7659460698189595,
      "grad_norm": 6.565224647521973,
      "learning_rate": 4.6861711608484206e-05,
      "loss": 2.0139,
      "step": 49300
    },
    {
      "epoch": 3.7735849056603774,
      "grad_norm": 4.823673725128174,
      "learning_rate": 4.685534591194969e-05,
      "loss": 1.928,
      "step": 49400
    },
    {
      "epoch": 3.781223741501795,
      "grad_norm": 6.99901008605957,
      "learning_rate": 4.684898021541517e-05,
      "loss": 1.9023,
      "step": 49500
    },
    {
      "epoch": 3.7888625773432127,
      "grad_norm": 8.31137752532959,
      "learning_rate": 4.6842614518880657e-05,
      "loss": 2.0734,
      "step": 49600
    },
    {
      "epoch": 3.7965014131846306,
      "grad_norm": 5.622682094573975,
      "learning_rate": 4.683624882234615e-05,
      "loss": 1.9919,
      "step": 49700
    },
    {
      "epoch": 3.8041402490260485,
      "grad_norm": 7.491743564605713,
      "learning_rate": 4.682988312581163e-05,
      "loss": 1.9667,
      "step": 49800
    },
    {
      "epoch": 3.8117790848674664,
      "grad_norm": 4.859404563903809,
      "learning_rate": 4.6823517429277114e-05,
      "loss": 1.9586,
      "step": 49900
    },
    {
      "epoch": 3.819417920708884,
      "grad_norm": 5.2389750480651855,
      "learning_rate": 4.68171517327426e-05,
      "loss": 1.919,
      "step": 50000
    },
    {
      "epoch": 3.8270567565503018,
      "grad_norm": 5.549896717071533,
      "learning_rate": 4.681078603620808e-05,
      "loss": 1.9163,
      "step": 50100
    },
    {
      "epoch": 3.8346955923917196,
      "grad_norm": 6.952061653137207,
      "learning_rate": 4.680442033967357e-05,
      "loss": 1.9196,
      "step": 50200
    },
    {
      "epoch": 3.842334428233137,
      "grad_norm": 5.211640357971191,
      "learning_rate": 4.6798054643139055e-05,
      "loss": 1.9544,
      "step": 50300
    },
    {
      "epoch": 3.849973264074555,
      "grad_norm": 3.5685226917266846,
      "learning_rate": 4.679168894660454e-05,
      "loss": 1.9314,
      "step": 50400
    },
    {
      "epoch": 3.857612099915973,
      "grad_norm": 4.5850419998168945,
      "learning_rate": 4.678532325007002e-05,
      "loss": 1.9786,
      "step": 50500
    },
    {
      "epoch": 3.8652509357573903,
      "grad_norm": 4.600704669952393,
      "learning_rate": 4.6778957553535505e-05,
      "loss": 1.9347,
      "step": 50600
    },
    {
      "epoch": 3.872889771598808,
      "grad_norm": 5.259885787963867,
      "learning_rate": 4.6772591857000996e-05,
      "loss": 1.9876,
      "step": 50700
    },
    {
      "epoch": 3.880528607440226,
      "grad_norm": 6.254337310791016,
      "learning_rate": 4.676622616046648e-05,
      "loss": 2.0417,
      "step": 50800
    },
    {
      "epoch": 3.888167443281644,
      "grad_norm": 4.956241607666016,
      "learning_rate": 4.675986046393196e-05,
      "loss": 2.0147,
      "step": 50900
    },
    {
      "epoch": 3.895806279123062,
      "grad_norm": 4.5055952072143555,
      "learning_rate": 4.6753494767397446e-05,
      "loss": 2.0199,
      "step": 51000
    },
    {
      "epoch": 3.9034451149644793,
      "grad_norm": 6.597069263458252,
      "learning_rate": 4.674712907086294e-05,
      "loss": 1.9538,
      "step": 51100
    },
    {
      "epoch": 3.911083950805897,
      "grad_norm": 5.9962310791015625,
      "learning_rate": 4.674076337432842e-05,
      "loss": 1.9339,
      "step": 51200
    },
    {
      "epoch": 3.918722786647315,
      "grad_norm": 4.42221212387085,
      "learning_rate": 4.6734397677793904e-05,
      "loss": 2.045,
      "step": 51300
    },
    {
      "epoch": 3.9263616224887326,
      "grad_norm": 6.094216823577881,
      "learning_rate": 4.672803198125939e-05,
      "loss": 1.9197,
      "step": 51400
    },
    {
      "epoch": 3.9340004583301504,
      "grad_norm": 6.566367149353027,
      "learning_rate": 4.672166628472488e-05,
      "loss": 2.0897,
      "step": 51500
    },
    {
      "epoch": 3.9416392941715683,
      "grad_norm": 7.173037052154541,
      "learning_rate": 4.671530058819036e-05,
      "loss": 1.9828,
      "step": 51600
    },
    {
      "epoch": 3.9492781300129858,
      "grad_norm": 5.549594879150391,
      "learning_rate": 4.6708934891655845e-05,
      "loss": 1.8398,
      "step": 51700
    },
    {
      "epoch": 3.9569169658544037,
      "grad_norm": 4.991846084594727,
      "learning_rate": 4.6702569195121335e-05,
      "loss": 1.9348,
      "step": 51800
    },
    {
      "epoch": 3.9645558016958216,
      "grad_norm": 5.718492031097412,
      "learning_rate": 4.669620349858682e-05,
      "loss": 2.0001,
      "step": 51900
    },
    {
      "epoch": 3.9721946375372394,
      "grad_norm": 5.067304611206055,
      "learning_rate": 4.66898378020523e-05,
      "loss": 2.0429,
      "step": 52000
    },
    {
      "epoch": 3.9798334733786573,
      "grad_norm": 6.6068501472473145,
      "learning_rate": 4.668347210551779e-05,
      "loss": 2.0226,
      "step": 52100
    },
    {
      "epoch": 3.987472309220075,
      "grad_norm": 4.955467224121094,
      "learning_rate": 4.6677106408983276e-05,
      "loss": 1.9392,
      "step": 52200
    },
    {
      "epoch": 3.9951111450614927,
      "grad_norm": 4.68868350982666,
      "learning_rate": 4.667074071244876e-05,
      "loss": 2.0152,
      "step": 52300
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9166086912155151,
      "eval_runtime": 1.657,
      "eval_samples_per_second": 416.412,
      "eval_steps_per_second": 416.412,
      "step": 52364
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.7583417892456055,
      "eval_runtime": 31.9302,
      "eval_samples_per_second": 409.988,
      "eval_steps_per_second": 409.988,
      "step": 52364
    },
    {
      "epoch": 4.002749980902911,
      "grad_norm": 6.448780059814453,
      "learning_rate": 4.666437501591424e-05,
      "loss": 1.9665,
      "step": 52400
    },
    {
      "epoch": 4.010388816744328,
      "grad_norm": 6.403815746307373,
      "learning_rate": 4.665800931937973e-05,
      "loss": 2.0222,
      "step": 52500
    },
    {
      "epoch": 4.018027652585746,
      "grad_norm": 6.542734146118164,
      "learning_rate": 4.665164362284522e-05,
      "loss": 1.9106,
      "step": 52600
    },
    {
      "epoch": 4.025666488427164,
      "grad_norm": 5.314322471618652,
      "learning_rate": 4.66452779263107e-05,
      "loss": 1.9449,
      "step": 52700
    },
    {
      "epoch": 4.033305324268581,
      "grad_norm": 6.547646522521973,
      "learning_rate": 4.6638912229776184e-05,
      "loss": 1.8953,
      "step": 52800
    },
    {
      "epoch": 4.04094416011,
      "grad_norm": 5.365969181060791,
      "learning_rate": 4.6632546533241674e-05,
      "loss": 1.9623,
      "step": 52900
    },
    {
      "epoch": 4.048582995951417,
      "grad_norm": 6.538881301879883,
      "learning_rate": 4.662618083670716e-05,
      "loss": 1.9305,
      "step": 53000
    },
    {
      "epoch": 4.0562218317928345,
      "grad_norm": 5.152493953704834,
      "learning_rate": 4.661981514017264e-05,
      "loss": 1.9339,
      "step": 53100
    },
    {
      "epoch": 4.063860667634253,
      "grad_norm": 6.382138729095459,
      "learning_rate": 4.6613449443638125e-05,
      "loss": 1.965,
      "step": 53200
    },
    {
      "epoch": 4.07149950347567,
      "grad_norm": 5.1736345291137695,
      "learning_rate": 4.660708374710361e-05,
      "loss": 1.9943,
      "step": 53300
    },
    {
      "epoch": 4.079138339317088,
      "grad_norm": 5.426162242889404,
      "learning_rate": 4.66007180505691e-05,
      "loss": 2.0123,
      "step": 53400
    },
    {
      "epoch": 4.086777175158506,
      "grad_norm": 6.582852363586426,
      "learning_rate": 4.659435235403458e-05,
      "loss": 1.9988,
      "step": 53500
    },
    {
      "epoch": 4.0944160109999235,
      "grad_norm": 6.405938625335693,
      "learning_rate": 4.6587986657500066e-05,
      "loss": 1.8869,
      "step": 53600
    },
    {
      "epoch": 4.102054846841342,
      "grad_norm": 6.425739288330078,
      "learning_rate": 4.658162096096555e-05,
      "loss": 1.9899,
      "step": 53700
    },
    {
      "epoch": 4.109693682682759,
      "grad_norm": 4.624861717224121,
      "learning_rate": 4.657525526443103e-05,
      "loss": 1.9766,
      "step": 53800
    },
    {
      "epoch": 4.117332518524177,
      "grad_norm": 6.13306999206543,
      "learning_rate": 4.656888956789652e-05,
      "loss": 2.0366,
      "step": 53900
    },
    {
      "epoch": 4.124971354365595,
      "grad_norm": 6.495079040527344,
      "learning_rate": 4.656252387136201e-05,
      "loss": 2.0422,
      "step": 54000
    },
    {
      "epoch": 4.1326101902070125,
      "grad_norm": 4.758920669555664,
      "learning_rate": 4.655615817482749e-05,
      "loss": 1.9276,
      "step": 54100
    },
    {
      "epoch": 4.14024902604843,
      "grad_norm": 8.942544937133789,
      "learning_rate": 4.6549792478292974e-05,
      "loss": 1.9513,
      "step": 54200
    },
    {
      "epoch": 4.147887861889848,
      "grad_norm": 5.705257415771484,
      "learning_rate": 4.6543426781758464e-05,
      "loss": 1.8973,
      "step": 54300
    },
    {
      "epoch": 4.155526697731266,
      "grad_norm": 7.478598594665527,
      "learning_rate": 4.653706108522395e-05,
      "loss": 1.9165,
      "step": 54400
    },
    {
      "epoch": 4.163165533572683,
      "grad_norm": 6.536148548126221,
      "learning_rate": 4.653069538868943e-05,
      "loss": 1.8926,
      "step": 54500
    },
    {
      "epoch": 4.1708043694141015,
      "grad_norm": 4.334852695465088,
      "learning_rate": 4.6524329692154915e-05,
      "loss": 1.9629,
      "step": 54600
    },
    {
      "epoch": 4.178443205255519,
      "grad_norm": 5.075918197631836,
      "learning_rate": 4.65179639956204e-05,
      "loss": 1.8615,
      "step": 54700
    },
    {
      "epoch": 4.186082041096936,
      "grad_norm": 5.603137016296387,
      "learning_rate": 4.651159829908589e-05,
      "loss": 1.9307,
      "step": 54800
    },
    {
      "epoch": 4.193720876938355,
      "grad_norm": 5.045955181121826,
      "learning_rate": 4.650523260255137e-05,
      "loss": 1.9325,
      "step": 54900
    },
    {
      "epoch": 4.201359712779772,
      "grad_norm": 4.989462375640869,
      "learning_rate": 4.6498866906016856e-05,
      "loss": 1.9319,
      "step": 55000
    },
    {
      "epoch": 4.2089985486211905,
      "grad_norm": 6.568370342254639,
      "learning_rate": 4.649250120948234e-05,
      "loss": 2.0307,
      "step": 55100
    },
    {
      "epoch": 4.216637384462608,
      "grad_norm": 6.052523136138916,
      "learning_rate": 4.648613551294783e-05,
      "loss": 1.9241,
      "step": 55200
    },
    {
      "epoch": 4.224276220304025,
      "grad_norm": 6.223960876464844,
      "learning_rate": 4.647976981641331e-05,
      "loss": 1.9726,
      "step": 55300
    },
    {
      "epoch": 4.231915056145444,
      "grad_norm": 4.6997785568237305,
      "learning_rate": 4.6473404119878797e-05,
      "loss": 2.0012,
      "step": 55400
    },
    {
      "epoch": 4.239553891986861,
      "grad_norm": 4.625126361846924,
      "learning_rate": 4.646703842334429e-05,
      "loss": 2.0173,
      "step": 55500
    },
    {
      "epoch": 4.247192727828279,
      "grad_norm": 6.2219133377075195,
      "learning_rate": 4.646067272680977e-05,
      "loss": 1.9919,
      "step": 55600
    },
    {
      "epoch": 4.254831563669697,
      "grad_norm": 6.355790615081787,
      "learning_rate": 4.6454307030275254e-05,
      "loss": 1.8704,
      "step": 55700
    },
    {
      "epoch": 4.262470399511114,
      "grad_norm": 4.704167366027832,
      "learning_rate": 4.6447941333740744e-05,
      "loss": 1.9014,
      "step": 55800
    },
    {
      "epoch": 4.270109235352532,
      "grad_norm": 5.239686489105225,
      "learning_rate": 4.644157563720623e-05,
      "loss": 2.0006,
      "step": 55900
    },
    {
      "epoch": 4.27774807119395,
      "grad_norm": 6.845952033996582,
      "learning_rate": 4.643520994067171e-05,
      "loss": 1.8874,
      "step": 56000
    },
    {
      "epoch": 4.285386907035368,
      "grad_norm": 8.183524131774902,
      "learning_rate": 4.64288442441372e-05,
      "loss": 2.0087,
      "step": 56100
    },
    {
      "epoch": 4.293025742876786,
      "grad_norm": 7.044088840484619,
      "learning_rate": 4.6422478547602685e-05,
      "loss": 2.0971,
      "step": 56200
    },
    {
      "epoch": 4.300664578718203,
      "grad_norm": 4.524477481842041,
      "learning_rate": 4.641611285106817e-05,
      "loss": 2.01,
      "step": 56300
    },
    {
      "epoch": 4.308303414559621,
      "grad_norm": 6.2249064445495605,
      "learning_rate": 4.640974715453365e-05,
      "loss": 1.9466,
      "step": 56400
    },
    {
      "epoch": 4.315942250401039,
      "grad_norm": 6.742652416229248,
      "learning_rate": 4.6403381457999136e-05,
      "loss": 1.9888,
      "step": 56500
    },
    {
      "epoch": 4.323581086242457,
      "grad_norm": 5.876722812652588,
      "learning_rate": 4.6397015761464626e-05,
      "loss": 1.8542,
      "step": 56600
    },
    {
      "epoch": 4.331219922083874,
      "grad_norm": 6.740420818328857,
      "learning_rate": 4.639065006493011e-05,
      "loss": 1.9074,
      "step": 56700
    },
    {
      "epoch": 4.338858757925292,
      "grad_norm": 4.535489082336426,
      "learning_rate": 4.638428436839559e-05,
      "loss": 1.8712,
      "step": 56800
    },
    {
      "epoch": 4.34649759376671,
      "grad_norm": 5.831934928894043,
      "learning_rate": 4.637791867186108e-05,
      "loss": 2.0153,
      "step": 56900
    },
    {
      "epoch": 4.354136429608127,
      "grad_norm": 6.008670806884766,
      "learning_rate": 4.637155297532656e-05,
      "loss": 2.0208,
      "step": 57000
    },
    {
      "epoch": 4.361775265449546,
      "grad_norm": 5.704031944274902,
      "learning_rate": 4.636518727879205e-05,
      "loss": 1.981,
      "step": 57100
    },
    {
      "epoch": 4.369414101290963,
      "grad_norm": 7.782782554626465,
      "learning_rate": 4.6358821582257534e-05,
      "loss": 1.9965,
      "step": 57200
    },
    {
      "epoch": 4.377052937132381,
      "grad_norm": 5.69127082824707,
      "learning_rate": 4.635245588572302e-05,
      "loss": 1.9584,
      "step": 57300
    },
    {
      "epoch": 4.384691772973799,
      "grad_norm": 3.8195607662200928,
      "learning_rate": 4.63460901891885e-05,
      "loss": 1.9569,
      "step": 57400
    },
    {
      "epoch": 4.392330608815216,
      "grad_norm": 4.106348514556885,
      "learning_rate": 4.633972449265399e-05,
      "loss": 1.9672,
      "step": 57500
    },
    {
      "epoch": 4.399969444656635,
      "grad_norm": 6.700559616088867,
      "learning_rate": 4.6333358796119475e-05,
      "loss": 1.9882,
      "step": 57600
    },
    {
      "epoch": 4.407608280498052,
      "grad_norm": 4.866472244262695,
      "learning_rate": 4.632699309958496e-05,
      "loss": 1.9391,
      "step": 57700
    },
    {
      "epoch": 4.4152471163394695,
      "grad_norm": 6.074792385101318,
      "learning_rate": 4.632062740305044e-05,
      "loss": 1.8916,
      "step": 57800
    },
    {
      "epoch": 4.422885952180888,
      "grad_norm": 4.3914361000061035,
      "learning_rate": 4.6314261706515926e-05,
      "loss": 2.1082,
      "step": 57900
    },
    {
      "epoch": 4.430524788022305,
      "grad_norm": 6.6841325759887695,
      "learning_rate": 4.6307896009981416e-05,
      "loss": 1.9575,
      "step": 58000
    },
    {
      "epoch": 4.438163623863723,
      "grad_norm": 6.831588268280029,
      "learning_rate": 4.63015303134469e-05,
      "loss": 1.9968,
      "step": 58100
    },
    {
      "epoch": 4.445802459705141,
      "grad_norm": 5.930288791656494,
      "learning_rate": 4.629516461691238e-05,
      "loss": 1.9262,
      "step": 58200
    },
    {
      "epoch": 4.4534412955465585,
      "grad_norm": 6.001265525817871,
      "learning_rate": 4.6288798920377867e-05,
      "loss": 1.9992,
      "step": 58300
    },
    {
      "epoch": 4.461080131387977,
      "grad_norm": 5.476141929626465,
      "learning_rate": 4.628243322384336e-05,
      "loss": 1.9067,
      "step": 58400
    },
    {
      "epoch": 4.468718967229394,
      "grad_norm": 8.71904468536377,
      "learning_rate": 4.627606752730884e-05,
      "loss": 1.871,
      "step": 58500
    },
    {
      "epoch": 4.476357803070812,
      "grad_norm": 5.768266201019287,
      "learning_rate": 4.6269701830774324e-05,
      "loss": 1.9689,
      "step": 58600
    },
    {
      "epoch": 4.48399663891223,
      "grad_norm": 5.7835540771484375,
      "learning_rate": 4.626333613423981e-05,
      "loss": 1.7772,
      "step": 58700
    },
    {
      "epoch": 4.4916354747536476,
      "grad_norm": 10.74528694152832,
      "learning_rate": 4.625697043770529e-05,
      "loss": 1.9431,
      "step": 58800
    },
    {
      "epoch": 4.499274310595065,
      "grad_norm": 6.648823261260986,
      "learning_rate": 4.625060474117078e-05,
      "loss": 1.9565,
      "step": 58900
    },
    {
      "epoch": 4.506913146436483,
      "grad_norm": 6.0485100746154785,
      "learning_rate": 4.6244239044636265e-05,
      "loss": 2.0014,
      "step": 59000
    },
    {
      "epoch": 4.514551982277901,
      "grad_norm": 5.07541036605835,
      "learning_rate": 4.623787334810175e-05,
      "loss": 1.9692,
      "step": 59100
    },
    {
      "epoch": 4.522190818119318,
      "grad_norm": 6.937198638916016,
      "learning_rate": 4.623150765156723e-05,
      "loss": 1.9114,
      "step": 59200
    },
    {
      "epoch": 4.529829653960737,
      "grad_norm": 4.988402843475342,
      "learning_rate": 4.622514195503272e-05,
      "loss": 1.9401,
      "step": 59300
    },
    {
      "epoch": 4.537468489802154,
      "grad_norm": 5.163907051086426,
      "learning_rate": 4.6218776258498206e-05,
      "loss": 1.9699,
      "step": 59400
    },
    {
      "epoch": 4.545107325643572,
      "grad_norm": 5.94899320602417,
      "learning_rate": 4.6212410561963696e-05,
      "loss": 1.9207,
      "step": 59500
    },
    {
      "epoch": 4.55274616148499,
      "grad_norm": 6.215063571929932,
      "learning_rate": 4.620604486542918e-05,
      "loss": 1.935,
      "step": 59600
    },
    {
      "epoch": 4.560384997326407,
      "grad_norm": 7.467189788818359,
      "learning_rate": 4.619967916889466e-05,
      "loss": 1.8898,
      "step": 59700
    },
    {
      "epoch": 4.568023833167826,
      "grad_norm": 6.628042221069336,
      "learning_rate": 4.6193313472360153e-05,
      "loss": 2.0142,
      "step": 59800
    },
    {
      "epoch": 4.575662669009243,
      "grad_norm": 6.576235771179199,
      "learning_rate": 4.618694777582564e-05,
      "loss": 1.8918,
      "step": 59900
    },
    {
      "epoch": 4.5833015048506605,
      "grad_norm": 6.481663227081299,
      "learning_rate": 4.618058207929112e-05,
      "loss": 1.9745,
      "step": 60000
    },
    {
      "epoch": 4.590940340692079,
      "grad_norm": 5.650027751922607,
      "learning_rate": 4.6174216382756604e-05,
      "loss": 2.0433,
      "step": 60100
    },
    {
      "epoch": 4.598579176533496,
      "grad_norm": 5.4851155281066895,
      "learning_rate": 4.616785068622209e-05,
      "loss": 1.8198,
      "step": 60200
    },
    {
      "epoch": 4.606218012374914,
      "grad_norm": 5.784546852111816,
      "learning_rate": 4.616148498968758e-05,
      "loss": 1.8767,
      "step": 60300
    },
    {
      "epoch": 4.613856848216332,
      "grad_norm": 5.73061466217041,
      "learning_rate": 4.615511929315306e-05,
      "loss": 1.9203,
      "step": 60400
    },
    {
      "epoch": 4.6214956840577495,
      "grad_norm": 6.226838111877441,
      "learning_rate": 4.6148753596618545e-05,
      "loss": 1.9733,
      "step": 60500
    },
    {
      "epoch": 4.629134519899168,
      "grad_norm": 4.003977298736572,
      "learning_rate": 4.614238790008403e-05,
      "loss": 1.8927,
      "step": 60600
    },
    {
      "epoch": 4.636773355740585,
      "grad_norm": 5.631470203399658,
      "learning_rate": 4.613602220354952e-05,
      "loss": 1.8679,
      "step": 60700
    },
    {
      "epoch": 4.644412191582003,
      "grad_norm": 7.626317977905273,
      "learning_rate": 4.6129656507015e-05,
      "loss": 1.8636,
      "step": 60800
    },
    {
      "epoch": 4.652051027423421,
      "grad_norm": 5.639631748199463,
      "learning_rate": 4.6123290810480486e-05,
      "loss": 1.9752,
      "step": 60900
    },
    {
      "epoch": 4.6596898632648385,
      "grad_norm": 4.846602439880371,
      "learning_rate": 4.611692511394597e-05,
      "loss": 2.0119,
      "step": 61000
    },
    {
      "epoch": 4.667328699106256,
      "grad_norm": 6.459568023681641,
      "learning_rate": 4.611055941741145e-05,
      "loss": 2.0286,
      "step": 61100
    },
    {
      "epoch": 4.674967534947674,
      "grad_norm": 4.674814224243164,
      "learning_rate": 4.610419372087694e-05,
      "loss": 1.8987,
      "step": 61200
    },
    {
      "epoch": 4.682606370789092,
      "grad_norm": 5.069769382476807,
      "learning_rate": 4.609782802434243e-05,
      "loss": 1.9914,
      "step": 61300
    },
    {
      "epoch": 4.690245206630509,
      "grad_norm": 5.329572677612305,
      "learning_rate": 4.609146232780791e-05,
      "loss": 1.9541,
      "step": 61400
    },
    {
      "epoch": 4.6978840424719275,
      "grad_norm": 3.709733009338379,
      "learning_rate": 4.6085096631273394e-05,
      "loss": 1.8726,
      "step": 61500
    },
    {
      "epoch": 4.705522878313345,
      "grad_norm": 8.043600082397461,
      "learning_rate": 4.6078730934738884e-05,
      "loss": 1.9497,
      "step": 61600
    },
    {
      "epoch": 4.713161714154763,
      "grad_norm": 5.747442722320557,
      "learning_rate": 4.607236523820437e-05,
      "loss": 1.9185,
      "step": 61700
    },
    {
      "epoch": 4.720800549996181,
      "grad_norm": 5.504051685333252,
      "learning_rate": 4.606599954166985e-05,
      "loss": 1.8946,
      "step": 61800
    },
    {
      "epoch": 4.728439385837598,
      "grad_norm": 6.701175212860107,
      "learning_rate": 4.6059633845135335e-05,
      "loss": 1.9537,
      "step": 61900
    },
    {
      "epoch": 4.7360782216790165,
      "grad_norm": 8.318120956420898,
      "learning_rate": 4.605326814860082e-05,
      "loss": 1.9584,
      "step": 62000
    },
    {
      "epoch": 4.743717057520434,
      "grad_norm": 6.6491522789001465,
      "learning_rate": 4.604690245206631e-05,
      "loss": 1.9674,
      "step": 62100
    },
    {
      "epoch": 4.751355893361851,
      "grad_norm": 5.5645623207092285,
      "learning_rate": 4.604053675553179e-05,
      "loss": 1.9189,
      "step": 62200
    },
    {
      "epoch": 4.75899472920327,
      "grad_norm": 5.686075210571289,
      "learning_rate": 4.6034171058997276e-05,
      "loss": 2.0145,
      "step": 62300
    },
    {
      "epoch": 4.766633565044687,
      "grad_norm": 5.1874799728393555,
      "learning_rate": 4.602780536246276e-05,
      "loss": 1.8908,
      "step": 62400
    },
    {
      "epoch": 4.774272400886105,
      "grad_norm": 4.884268283843994,
      "learning_rate": 4.602143966592824e-05,
      "loss": 1.9249,
      "step": 62500
    },
    {
      "epoch": 4.781911236727523,
      "grad_norm": 5.904646873474121,
      "learning_rate": 4.601507396939373e-05,
      "loss": 1.9299,
      "step": 62600
    },
    {
      "epoch": 4.78955007256894,
      "grad_norm": 5.8514723777771,
      "learning_rate": 4.600870827285922e-05,
      "loss": 1.8113,
      "step": 62700
    },
    {
      "epoch": 4.797188908410359,
      "grad_norm": 6.495887279510498,
      "learning_rate": 4.60023425763247e-05,
      "loss": 1.9303,
      "step": 62800
    },
    {
      "epoch": 4.804827744251776,
      "grad_norm": 4.292220592498779,
      "learning_rate": 4.5995976879790184e-05,
      "loss": 1.8395,
      "step": 62900
    },
    {
      "epoch": 4.812466580093194,
      "grad_norm": 4.041690826416016,
      "learning_rate": 4.5989611183255674e-05,
      "loss": 2.0327,
      "step": 63000
    },
    {
      "epoch": 4.820105415934612,
      "grad_norm": 5.000965595245361,
      "learning_rate": 4.598324548672116e-05,
      "loss": 2.0544,
      "step": 63100
    },
    {
      "epoch": 4.827744251776029,
      "grad_norm": 5.802923679351807,
      "learning_rate": 4.597687979018664e-05,
      "loss": 1.9361,
      "step": 63200
    },
    {
      "epoch": 4.835383087617447,
      "grad_norm": 6.698937892913818,
      "learning_rate": 4.597051409365213e-05,
      "loss": 1.9805,
      "step": 63300
    },
    {
      "epoch": 4.843021923458865,
      "grad_norm": 6.307214736938477,
      "learning_rate": 4.5964148397117615e-05,
      "loss": 2.0743,
      "step": 63400
    },
    {
      "epoch": 4.850660759300283,
      "grad_norm": 8.695208549499512,
      "learning_rate": 4.59577827005831e-05,
      "loss": 2.0071,
      "step": 63500
    },
    {
      "epoch": 4.8582995951417,
      "grad_norm": 6.336278438568115,
      "learning_rate": 4.595141700404859e-05,
      "loss": 1.9616,
      "step": 63600
    },
    {
      "epoch": 4.865938430983118,
      "grad_norm": 7.049503326416016,
      "learning_rate": 4.594505130751407e-05,
      "loss": 1.9991,
      "step": 63700
    },
    {
      "epoch": 4.873577266824536,
      "grad_norm": 4.496309757232666,
      "learning_rate": 4.5938685610979556e-05,
      "loss": 2.0199,
      "step": 63800
    },
    {
      "epoch": 4.881216102665954,
      "grad_norm": 5.532925128936768,
      "learning_rate": 4.5932319914445046e-05,
      "loss": 1.924,
      "step": 63900
    },
    {
      "epoch": 4.888854938507372,
      "grad_norm": 6.094738483428955,
      "learning_rate": 4.592595421791053e-05,
      "loss": 2.0056,
      "step": 64000
    },
    {
      "epoch": 4.896493774348789,
      "grad_norm": 6.008172988891602,
      "learning_rate": 4.591958852137601e-05,
      "loss": 2.0016,
      "step": 64100
    },
    {
      "epoch": 4.9041326101902065,
      "grad_norm": 5.180637836456299,
      "learning_rate": 4.59132228248415e-05,
      "loss": 1.9166,
      "step": 64200
    },
    {
      "epoch": 4.911771446031625,
      "grad_norm": 6.357650279998779,
      "learning_rate": 4.590685712830698e-05,
      "loss": 2.0549,
      "step": 64300
    },
    {
      "epoch": 4.919410281873042,
      "grad_norm": 6.3531341552734375,
      "learning_rate": 4.590049143177247e-05,
      "loss": 1.9784,
      "step": 64400
    },
    {
      "epoch": 4.927049117714461,
      "grad_norm": 4.839306354522705,
      "learning_rate": 4.5894125735237954e-05,
      "loss": 2.0142,
      "step": 64500
    },
    {
      "epoch": 4.934687953555878,
      "grad_norm": 7.30161714553833,
      "learning_rate": 4.588776003870344e-05,
      "loss": 1.9384,
      "step": 64600
    },
    {
      "epoch": 4.9423267893972955,
      "grad_norm": 5.7992658615112305,
      "learning_rate": 4.588139434216892e-05,
      "loss": 1.9444,
      "step": 64700
    },
    {
      "epoch": 4.949965625238714,
      "grad_norm": 6.23362398147583,
      "learning_rate": 4.587502864563441e-05,
      "loss": 1.9916,
      "step": 64800
    },
    {
      "epoch": 4.957604461080131,
      "grad_norm": 5.355942726135254,
      "learning_rate": 4.5868662949099895e-05,
      "loss": 2.0433,
      "step": 64900
    },
    {
      "epoch": 4.96524329692155,
      "grad_norm": 6.017828941345215,
      "learning_rate": 4.586229725256538e-05,
      "loss": 1.9356,
      "step": 65000
    },
    {
      "epoch": 4.972882132762967,
      "grad_norm": 4.73491096496582,
      "learning_rate": 4.585593155603086e-05,
      "loss": 1.9505,
      "step": 65100
    },
    {
      "epoch": 4.9805209686043845,
      "grad_norm": 4.790602207183838,
      "learning_rate": 4.5849565859496346e-05,
      "loss": 1.9355,
      "step": 65200
    },
    {
      "epoch": 4.988159804445802,
      "grad_norm": 4.999473571777344,
      "learning_rate": 4.5843200162961836e-05,
      "loss": 1.929,
      "step": 65300
    },
    {
      "epoch": 4.99579864028722,
      "grad_norm": 9.54749584197998,
      "learning_rate": 4.583683446642732e-05,
      "loss": 1.9843,
      "step": 65400
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.8943908214569092,
      "eval_runtime": 1.6614,
      "eval_samples_per_second": 415.312,
      "eval_steps_per_second": 415.312,
      "step": 65455
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.729557752609253,
      "eval_runtime": 31.6737,
      "eval_samples_per_second": 413.308,
      "eval_steps_per_second": 413.308,
      "step": 65455
    }
  ],
  "logging_steps": 100,
  "max_steps": 785460,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 40504098585600.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
