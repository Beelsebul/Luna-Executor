{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 196365,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007638835841417768,
      "grad_norm": 11.003396987915039,
      "learning_rate": 4.9993634303465486e-05,
      "loss": 4.7987,
      "step": 100
    },
    {
      "epoch": 0.015277671682835536,
      "grad_norm": 8.900592803955078,
      "learning_rate": 4.998726860693097e-05,
      "loss": 3.3992,
      "step": 200
    },
    {
      "epoch": 0.022916507524253303,
      "grad_norm": 5.975394248962402,
      "learning_rate": 4.998090291039646e-05,
      "loss": 3.0005,
      "step": 300
    },
    {
      "epoch": 0.030555343365671072,
      "grad_norm": 5.7895307540893555,
      "learning_rate": 4.997453721386194e-05,
      "loss": 2.9206,
      "step": 400
    },
    {
      "epoch": 0.03819417920708884,
      "grad_norm": 7.866730690002441,
      "learning_rate": 4.996817151732743e-05,
      "loss": 2.8266,
      "step": 500
    },
    {
      "epoch": 0.045833015048506606,
      "grad_norm": 6.887518882751465,
      "learning_rate": 4.996180582079291e-05,
      "loss": 2.8767,
      "step": 600
    },
    {
      "epoch": 0.05347185088992438,
      "grad_norm": 7.774272918701172,
      "learning_rate": 4.99554401242584e-05,
      "loss": 2.7593,
      "step": 700
    },
    {
      "epoch": 0.061110686731342144,
      "grad_norm": 6.494357109069824,
      "learning_rate": 4.9949074427723884e-05,
      "loss": 2.7513,
      "step": 800
    },
    {
      "epoch": 0.06874952257275992,
      "grad_norm": 5.112570762634277,
      "learning_rate": 4.994270873118937e-05,
      "loss": 2.58,
      "step": 900
    },
    {
      "epoch": 0.07638835841417768,
      "grad_norm": 7.6285014152526855,
      "learning_rate": 4.993634303465485e-05,
      "loss": 2.6628,
      "step": 1000
    },
    {
      "epoch": 0.08402719425559545,
      "grad_norm": 5.320619106292725,
      "learning_rate": 4.9929977338120335e-05,
      "loss": 2.7452,
      "step": 1100
    },
    {
      "epoch": 0.09166603009701321,
      "grad_norm": 4.718387603759766,
      "learning_rate": 4.9923611641585825e-05,
      "loss": 2.6365,
      "step": 1200
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 4.754127502441406,
      "learning_rate": 4.991724594505131e-05,
      "loss": 2.6765,
      "step": 1300
    },
    {
      "epoch": 0.10694370177984876,
      "grad_norm": 5.459957599639893,
      "learning_rate": 4.991088024851679e-05,
      "loss": 2.5782,
      "step": 1400
    },
    {
      "epoch": 0.11458253762126652,
      "grad_norm": 4.47595739364624,
      "learning_rate": 4.9904514551982276e-05,
      "loss": 2.5001,
      "step": 1500
    },
    {
      "epoch": 0.12222137346268429,
      "grad_norm": 7.569705963134766,
      "learning_rate": 4.9898148855447766e-05,
      "loss": 2.64,
      "step": 1600
    },
    {
      "epoch": 0.12986020930410205,
      "grad_norm": 4.624007225036621,
      "learning_rate": 4.989178315891325e-05,
      "loss": 2.4831,
      "step": 1700
    },
    {
      "epoch": 0.13749904514551983,
      "grad_norm": 5.883856773376465,
      "learning_rate": 4.988541746237873e-05,
      "loss": 2.5157,
      "step": 1800
    },
    {
      "epoch": 0.14513788098693758,
      "grad_norm": 6.40806770324707,
      "learning_rate": 4.9879051765844223e-05,
      "loss": 2.506,
      "step": 1900
    },
    {
      "epoch": 0.15277671682835536,
      "grad_norm": 4.2202959060668945,
      "learning_rate": 4.987268606930971e-05,
      "loss": 2.5831,
      "step": 2000
    },
    {
      "epoch": 0.16041555266977311,
      "grad_norm": 5.745185852050781,
      "learning_rate": 4.986632037277519e-05,
      "loss": 2.4737,
      "step": 2100
    },
    {
      "epoch": 0.1680543885111909,
      "grad_norm": 5.628921985626221,
      "learning_rate": 4.985995467624068e-05,
      "loss": 2.4374,
      "step": 2200
    },
    {
      "epoch": 0.17569322435260867,
      "grad_norm": 6.821033477783203,
      "learning_rate": 4.9853588979706164e-05,
      "loss": 2.4101,
      "step": 2300
    },
    {
      "epoch": 0.18333206019402642,
      "grad_norm": 5.130759239196777,
      "learning_rate": 4.984722328317165e-05,
      "loss": 2.5305,
      "step": 2400
    },
    {
      "epoch": 0.1909708960354442,
      "grad_norm": 6.336085796356201,
      "learning_rate": 4.984085758663713e-05,
      "loss": 2.4942,
      "step": 2500
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 5.207876682281494,
      "learning_rate": 4.983449189010262e-05,
      "loss": 2.4839,
      "step": 2600
    },
    {
      "epoch": 0.20624856771827974,
      "grad_norm": 6.232301235198975,
      "learning_rate": 4.9828126193568105e-05,
      "loss": 2.4191,
      "step": 2700
    },
    {
      "epoch": 0.21388740355969751,
      "grad_norm": 4.579092502593994,
      "learning_rate": 4.982176049703359e-05,
      "loss": 2.4134,
      "step": 2800
    },
    {
      "epoch": 0.22152623940111527,
      "grad_norm": 4.447245121002197,
      "learning_rate": 4.981539480049907e-05,
      "loss": 2.3616,
      "step": 2900
    },
    {
      "epoch": 0.22916507524253305,
      "grad_norm": 4.76075553894043,
      "learning_rate": 4.980902910396456e-05,
      "loss": 2.4775,
      "step": 3000
    },
    {
      "epoch": 0.2368039110839508,
      "grad_norm": 3.732889413833618,
      "learning_rate": 4.9802663407430046e-05,
      "loss": 2.4313,
      "step": 3100
    },
    {
      "epoch": 0.24444274692536858,
      "grad_norm": 4.529886722564697,
      "learning_rate": 4.979629771089553e-05,
      "loss": 2.3847,
      "step": 3200
    },
    {
      "epoch": 0.2520815827667863,
      "grad_norm": 5.568953990936279,
      "learning_rate": 4.978993201436101e-05,
      "loss": 2.4514,
      "step": 3300
    },
    {
      "epoch": 0.2597204186082041,
      "grad_norm": 4.899194717407227,
      "learning_rate": 4.97835663178265e-05,
      "loss": 2.4084,
      "step": 3400
    },
    {
      "epoch": 0.2673592544496219,
      "grad_norm": 5.895698547363281,
      "learning_rate": 4.977720062129199e-05,
      "loss": 2.3911,
      "step": 3500
    },
    {
      "epoch": 0.27499809029103967,
      "grad_norm": 5.283671855926514,
      "learning_rate": 4.977083492475747e-05,
      "loss": 2.3701,
      "step": 3600
    },
    {
      "epoch": 0.2826369261324574,
      "grad_norm": 4.288642406463623,
      "learning_rate": 4.9764469228222954e-05,
      "loss": 2.3728,
      "step": 3700
    },
    {
      "epoch": 0.29027576197387517,
      "grad_norm": 5.616775989532471,
      "learning_rate": 4.975810353168844e-05,
      "loss": 2.4251,
      "step": 3800
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 6.505110263824463,
      "learning_rate": 4.975173783515393e-05,
      "loss": 2.3596,
      "step": 3900
    },
    {
      "epoch": 0.3055534336567107,
      "grad_norm": 5.286049842834473,
      "learning_rate": 4.974537213861941e-05,
      "loss": 2.4107,
      "step": 4000
    },
    {
      "epoch": 0.3131922694981285,
      "grad_norm": 6.731076717376709,
      "learning_rate": 4.9739006442084895e-05,
      "loss": 2.3262,
      "step": 4100
    },
    {
      "epoch": 0.32083110533954623,
      "grad_norm": 4.681948661804199,
      "learning_rate": 4.973264074555038e-05,
      "loss": 2.4641,
      "step": 4200
    },
    {
      "epoch": 0.328469941180964,
      "grad_norm": 4.909715175628662,
      "learning_rate": 4.972627504901586e-05,
      "loss": 2.2768,
      "step": 4300
    },
    {
      "epoch": 0.3361087770223818,
      "grad_norm": 6.321661949157715,
      "learning_rate": 4.971990935248135e-05,
      "loss": 2.4101,
      "step": 4400
    },
    {
      "epoch": 0.34374761286379957,
      "grad_norm": 4.7248711585998535,
      "learning_rate": 4.9713543655946836e-05,
      "loss": 2.339,
      "step": 4500
    },
    {
      "epoch": 0.35138644870521735,
      "grad_norm": 5.027790069580078,
      "learning_rate": 4.970717795941232e-05,
      "loss": 2.3546,
      "step": 4600
    },
    {
      "epoch": 0.35902528454663507,
      "grad_norm": 4.733400344848633,
      "learning_rate": 4.97008122628778e-05,
      "loss": 2.4172,
      "step": 4700
    },
    {
      "epoch": 0.36666412038805285,
      "grad_norm": 5.185433387756348,
      "learning_rate": 4.969444656634329e-05,
      "loss": 2.417,
      "step": 4800
    },
    {
      "epoch": 0.37430295622947063,
      "grad_norm": 6.965174198150635,
      "learning_rate": 4.968808086980878e-05,
      "loss": 2.4498,
      "step": 4900
    },
    {
      "epoch": 0.3819417920708884,
      "grad_norm": 5.102669715881348,
      "learning_rate": 4.968171517327426e-05,
      "loss": 2.3239,
      "step": 5000
    },
    {
      "epoch": 0.3895806279123062,
      "grad_norm": 5.339658737182617,
      "learning_rate": 4.9675349476739744e-05,
      "loss": 2.3836,
      "step": 5100
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 6.19642972946167,
      "learning_rate": 4.966898378020523e-05,
      "loss": 2.3158,
      "step": 5200
    },
    {
      "epoch": 0.4048582995951417,
      "grad_norm": 3.971287727355957,
      "learning_rate": 4.966261808367072e-05,
      "loss": 2.3134,
      "step": 5300
    },
    {
      "epoch": 0.41249713543655947,
      "grad_norm": 4.2142133712768555,
      "learning_rate": 4.96562523871362e-05,
      "loss": 2.2675,
      "step": 5400
    },
    {
      "epoch": 0.42013597127797725,
      "grad_norm": 5.646320343017578,
      "learning_rate": 4.9649886690601685e-05,
      "loss": 2.4329,
      "step": 5500
    },
    {
      "epoch": 0.42777480711939503,
      "grad_norm": 5.131170749664307,
      "learning_rate": 4.9643520994067175e-05,
      "loss": 2.249,
      "step": 5600
    },
    {
      "epoch": 0.43541364296081275,
      "grad_norm": 4.929803848266602,
      "learning_rate": 4.963715529753266e-05,
      "loss": 2.2427,
      "step": 5700
    },
    {
      "epoch": 0.44305247880223053,
      "grad_norm": 4.086399078369141,
      "learning_rate": 4.963078960099814e-05,
      "loss": 2.3378,
      "step": 5800
    },
    {
      "epoch": 0.4506913146436483,
      "grad_norm": 4.7853779792785645,
      "learning_rate": 4.962442390446363e-05,
      "loss": 2.3175,
      "step": 5900
    },
    {
      "epoch": 0.4583301504850661,
      "grad_norm": 5.705160140991211,
      "learning_rate": 4.9618058207929116e-05,
      "loss": 2.3939,
      "step": 6000
    },
    {
      "epoch": 0.46596898632648387,
      "grad_norm": 4.324826240539551,
      "learning_rate": 4.96116925113946e-05,
      "loss": 2.3565,
      "step": 6100
    },
    {
      "epoch": 0.4736078221679016,
      "grad_norm": 5.057748794555664,
      "learning_rate": 4.960532681486009e-05,
      "loss": 2.3273,
      "step": 6200
    },
    {
      "epoch": 0.4812466580093194,
      "grad_norm": 5.834131717681885,
      "learning_rate": 4.9598961118325574e-05,
      "loss": 2.3206,
      "step": 6300
    },
    {
      "epoch": 0.48888549385073715,
      "grad_norm": 5.706160545349121,
      "learning_rate": 4.959259542179106e-05,
      "loss": 2.3716,
      "step": 6400
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 5.269311904907227,
      "learning_rate": 4.958622972525654e-05,
      "loss": 2.2627,
      "step": 6500
    },
    {
      "epoch": 0.5041631655335727,
      "grad_norm": 5.729904651641846,
      "learning_rate": 4.9579864028722024e-05,
      "loss": 2.1671,
      "step": 6600
    },
    {
      "epoch": 0.5118020013749904,
      "grad_norm": 3.571481466293335,
      "learning_rate": 4.9573498332187515e-05,
      "loss": 2.2704,
      "step": 6700
    },
    {
      "epoch": 0.5194408372164082,
      "grad_norm": 6.264553546905518,
      "learning_rate": 4.9567132635653e-05,
      "loss": 2.2505,
      "step": 6800
    },
    {
      "epoch": 0.527079673057826,
      "grad_norm": 4.309935092926025,
      "learning_rate": 4.956076693911848e-05,
      "loss": 2.2722,
      "step": 6900
    },
    {
      "epoch": 0.5347185088992438,
      "grad_norm": 6.211354732513428,
      "learning_rate": 4.9554401242583965e-05,
      "loss": 2.2451,
      "step": 7000
    },
    {
      "epoch": 0.5423573447406616,
      "grad_norm": 5.986732006072998,
      "learning_rate": 4.9548035546049455e-05,
      "loss": 2.3056,
      "step": 7100
    },
    {
      "epoch": 0.5499961805820793,
      "grad_norm": 5.57463264465332,
      "learning_rate": 4.954166984951494e-05,
      "loss": 2.2282,
      "step": 7200
    },
    {
      "epoch": 0.5576350164234971,
      "grad_norm": 4.415623188018799,
      "learning_rate": 4.953530415298042e-05,
      "loss": 2.2761,
      "step": 7300
    },
    {
      "epoch": 0.5652738522649148,
      "grad_norm": 5.856958389282227,
      "learning_rate": 4.9528938456445906e-05,
      "loss": 2.2874,
      "step": 7400
    },
    {
      "epoch": 0.5729126881063326,
      "grad_norm": 4.492795944213867,
      "learning_rate": 4.952257275991139e-05,
      "loss": 2.3064,
      "step": 7500
    },
    {
      "epoch": 0.5805515239477503,
      "grad_norm": 4.8552069664001465,
      "learning_rate": 4.951620706337688e-05,
      "loss": 2.2689,
      "step": 7600
    },
    {
      "epoch": 0.5881903597891681,
      "grad_norm": 4.370059490203857,
      "learning_rate": 4.9509841366842363e-05,
      "loss": 2.3461,
      "step": 7700
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 5.262638568878174,
      "learning_rate": 4.950347567030785e-05,
      "loss": 2.2502,
      "step": 7800
    },
    {
      "epoch": 0.6034680314720037,
      "grad_norm": 6.289999485015869,
      "learning_rate": 4.949710997377333e-05,
      "loss": 2.3907,
      "step": 7900
    },
    {
      "epoch": 0.6111068673134215,
      "grad_norm": 5.156611442565918,
      "learning_rate": 4.9490744277238814e-05,
      "loss": 2.2792,
      "step": 8000
    },
    {
      "epoch": 0.6187457031548392,
      "grad_norm": 8.065458297729492,
      "learning_rate": 4.9484378580704304e-05,
      "loss": 2.3101,
      "step": 8100
    },
    {
      "epoch": 0.626384538996257,
      "grad_norm": 4.8647589683532715,
      "learning_rate": 4.947801288416979e-05,
      "loss": 2.4493,
      "step": 8200
    },
    {
      "epoch": 0.6340233748376748,
      "grad_norm": 5.609063625335693,
      "learning_rate": 4.947164718763527e-05,
      "loss": 2.2455,
      "step": 8300
    },
    {
      "epoch": 0.6416622106790925,
      "grad_norm": 4.696290493011475,
      "learning_rate": 4.9465281491100755e-05,
      "loss": 2.3689,
      "step": 8400
    },
    {
      "epoch": 0.6493010465205102,
      "grad_norm": 4.671238899230957,
      "learning_rate": 4.9458915794566245e-05,
      "loss": 2.3385,
      "step": 8500
    },
    {
      "epoch": 0.656939882361928,
      "grad_norm": 5.878252983093262,
      "learning_rate": 4.945255009803173e-05,
      "loss": 2.373,
      "step": 8600
    },
    {
      "epoch": 0.6645787182033458,
      "grad_norm": 5.9969258308410645,
      "learning_rate": 4.944618440149721e-05,
      "loss": 2.2998,
      "step": 8700
    },
    {
      "epoch": 0.6722175540447636,
      "grad_norm": 5.083381652832031,
      "learning_rate": 4.9439818704962696e-05,
      "loss": 2.3673,
      "step": 8800
    },
    {
      "epoch": 0.6798563898861814,
      "grad_norm": 6.138138771057129,
      "learning_rate": 4.943345300842818e-05,
      "loss": 2.1902,
      "step": 8900
    },
    {
      "epoch": 0.6874952257275991,
      "grad_norm": 4.921735763549805,
      "learning_rate": 4.942708731189367e-05,
      "loss": 2.2429,
      "step": 9000
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 5.999809741973877,
      "learning_rate": 4.942072161535915e-05,
      "loss": 2.3055,
      "step": 9100
    },
    {
      "epoch": 0.7027728974104347,
      "grad_norm": 5.198227882385254,
      "learning_rate": 4.941435591882464e-05,
      "loss": 2.2199,
      "step": 9200
    },
    {
      "epoch": 0.7104117332518525,
      "grad_norm": 5.976837158203125,
      "learning_rate": 4.940799022229012e-05,
      "loss": 2.1992,
      "step": 9300
    },
    {
      "epoch": 0.7180505690932701,
      "grad_norm": 5.504315376281738,
      "learning_rate": 4.940162452575561e-05,
      "loss": 2.0797,
      "step": 9400
    },
    {
      "epoch": 0.7256894049346879,
      "grad_norm": 5.1509175300598145,
      "learning_rate": 4.9395258829221094e-05,
      "loss": 2.2534,
      "step": 9500
    },
    {
      "epoch": 0.7333282407761057,
      "grad_norm": 5.14268684387207,
      "learning_rate": 4.938889313268658e-05,
      "loss": 2.3399,
      "step": 9600
    },
    {
      "epoch": 0.7409670766175235,
      "grad_norm": 5.848530292510986,
      "learning_rate": 4.938252743615207e-05,
      "loss": 2.2494,
      "step": 9700
    },
    {
      "epoch": 0.7486059124589413,
      "grad_norm": 5.223510265350342,
      "learning_rate": 4.937616173961755e-05,
      "loss": 2.1326,
      "step": 9800
    },
    {
      "epoch": 0.756244748300359,
      "grad_norm": 4.658531665802002,
      "learning_rate": 4.936979604308304e-05,
      "loss": 2.2067,
      "step": 9900
    },
    {
      "epoch": 0.7638835841417768,
      "grad_norm": 3.829406976699829,
      "learning_rate": 4.9363430346548525e-05,
      "loss": 2.1967,
      "step": 10000
    },
    {
      "epoch": 0.7715224199831946,
      "grad_norm": 5.2110748291015625,
      "learning_rate": 4.935706465001401e-05,
      "loss": 2.1784,
      "step": 10100
    },
    {
      "epoch": 0.7791612558246124,
      "grad_norm": 5.96177339553833,
      "learning_rate": 4.935069895347949e-05,
      "loss": 2.2164,
      "step": 10200
    },
    {
      "epoch": 0.78680009166603,
      "grad_norm": 5.595090866088867,
      "learning_rate": 4.9344333256944976e-05,
      "loss": 2.1664,
      "step": 10300
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 5.02117919921875,
      "learning_rate": 4.9337967560410466e-05,
      "loss": 2.2654,
      "step": 10400
    },
    {
      "epoch": 0.8020777633488656,
      "grad_norm": 4.5138983726501465,
      "learning_rate": 4.933160186387595e-05,
      "loss": 2.321,
      "step": 10500
    },
    {
      "epoch": 0.8097165991902834,
      "grad_norm": 4.387924671173096,
      "learning_rate": 4.9325236167341433e-05,
      "loss": 2.2242,
      "step": 10600
    },
    {
      "epoch": 0.8173554350317012,
      "grad_norm": 5.320981025695801,
      "learning_rate": 4.931887047080692e-05,
      "loss": 2.3197,
      "step": 10700
    },
    {
      "epoch": 0.8249942708731189,
      "grad_norm": 4.889512538909912,
      "learning_rate": 4.931250477427241e-05,
      "loss": 2.1835,
      "step": 10800
    },
    {
      "epoch": 0.8326331067145367,
      "grad_norm": 4.650567531585693,
      "learning_rate": 4.930613907773789e-05,
      "loss": 2.2562,
      "step": 10900
    },
    {
      "epoch": 0.8402719425559545,
      "grad_norm": 5.706696510314941,
      "learning_rate": 4.9299773381203374e-05,
      "loss": 2.2688,
      "step": 11000
    },
    {
      "epoch": 0.8479107783973723,
      "grad_norm": 8.170299530029297,
      "learning_rate": 4.929340768466886e-05,
      "loss": 2.1902,
      "step": 11100
    },
    {
      "epoch": 0.8555496142387901,
      "grad_norm": 4.71286678314209,
      "learning_rate": 4.928704198813434e-05,
      "loss": 2.2445,
      "step": 11200
    },
    {
      "epoch": 0.8631884500802077,
      "grad_norm": 5.262633800506592,
      "learning_rate": 4.928067629159983e-05,
      "loss": 2.2422,
      "step": 11300
    },
    {
      "epoch": 0.8708272859216255,
      "grad_norm": 6.1787004470825195,
      "learning_rate": 4.9274310595065315e-05,
      "loss": 2.2594,
      "step": 11400
    },
    {
      "epoch": 0.8784661217630433,
      "grad_norm": 5.776530742645264,
      "learning_rate": 4.92679448985308e-05,
      "loss": 2.2076,
      "step": 11500
    },
    {
      "epoch": 0.8861049576044611,
      "grad_norm": 5.155461311340332,
      "learning_rate": 4.926157920199628e-05,
      "loss": 2.2628,
      "step": 11600
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 5.225574970245361,
      "learning_rate": 4.925521350546177e-05,
      "loss": 2.2179,
      "step": 11700
    },
    {
      "epoch": 0.9013826292872966,
      "grad_norm": 5.672508716583252,
      "learning_rate": 4.9248847808927256e-05,
      "loss": 2.3103,
      "step": 11800
    },
    {
      "epoch": 0.9090214651287144,
      "grad_norm": 6.074906826019287,
      "learning_rate": 4.924248211239274e-05,
      "loss": 2.3162,
      "step": 11900
    },
    {
      "epoch": 0.9166603009701322,
      "grad_norm": 5.03071403503418,
      "learning_rate": 4.923611641585822e-05,
      "loss": 2.2034,
      "step": 12000
    },
    {
      "epoch": 0.92429913681155,
      "grad_norm": 4.268101692199707,
      "learning_rate": 4.922975071932371e-05,
      "loss": 2.2248,
      "step": 12100
    },
    {
      "epoch": 0.9319379726529677,
      "grad_norm": 5.669069766998291,
      "learning_rate": 4.92233850227892e-05,
      "loss": 2.3092,
      "step": 12200
    },
    {
      "epoch": 0.9395768084943854,
      "grad_norm": 4.495509147644043,
      "learning_rate": 4.921701932625468e-05,
      "loss": 2.1965,
      "step": 12300
    },
    {
      "epoch": 0.9472156443358032,
      "grad_norm": 7.0822954177856445,
      "learning_rate": 4.9210653629720164e-05,
      "loss": 2.207,
      "step": 12400
    },
    {
      "epoch": 0.954854480177221,
      "grad_norm": 4.783316135406494,
      "learning_rate": 4.920428793318565e-05,
      "loss": 2.2018,
      "step": 12500
    },
    {
      "epoch": 0.9624933160186387,
      "grad_norm": 6.816341876983643,
      "learning_rate": 4.919792223665114e-05,
      "loss": 2.1585,
      "step": 12600
    },
    {
      "epoch": 0.9701321518600565,
      "grad_norm": 4.57766056060791,
      "learning_rate": 4.919155654011662e-05,
      "loss": 2.1525,
      "step": 12700
    },
    {
      "epoch": 0.9777709877014743,
      "grad_norm": 4.292228698730469,
      "learning_rate": 4.9185190843582105e-05,
      "loss": 2.1817,
      "step": 12800
    },
    {
      "epoch": 0.9854098235428921,
      "grad_norm": 6.101471424102783,
      "learning_rate": 4.917882514704759e-05,
      "loss": 2.1908,
      "step": 12900
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 4.360482692718506,
      "learning_rate": 4.917245945051307e-05,
      "loss": 2.1565,
      "step": 13000
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.1020638942718506,
      "eval_runtime": 3.3368,
      "eval_samples_per_second": 206.787,
      "eval_steps_per_second": 206.787,
      "step": 13091
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.9957842826843262,
      "eval_runtime": 59.7947,
      "eval_samples_per_second": 218.932,
      "eval_steps_per_second": 218.932,
      "step": 13091
    },
    {
      "epoch": 1.0006874952257276,
      "grad_norm": 4.266941547393799,
      "learning_rate": 4.916609375397856e-05,
      "loss": 2.2963,
      "step": 13100
    },
    {
      "epoch": 1.0083263310671453,
      "grad_norm": 4.117493152618408,
      "learning_rate": 4.9159728057444046e-05,
      "loss": 2.2097,
      "step": 13200
    },
    {
      "epoch": 1.0159651669085632,
      "grad_norm": 5.203220367431641,
      "learning_rate": 4.915336236090953e-05,
      "loss": 2.1886,
      "step": 13300
    },
    {
      "epoch": 1.0236040027499809,
      "grad_norm": 5.174717903137207,
      "learning_rate": 4.914699666437502e-05,
      "loss": 2.2313,
      "step": 13400
    },
    {
      "epoch": 1.0312428385913988,
      "grad_norm": 5.837007522583008,
      "learning_rate": 4.9140630967840503e-05,
      "loss": 2.2045,
      "step": 13500
    },
    {
      "epoch": 1.0388816744328164,
      "grad_norm": 5.140186309814453,
      "learning_rate": 4.913426527130599e-05,
      "loss": 2.1467,
      "step": 13600
    },
    {
      "epoch": 1.046520510274234,
      "grad_norm": 4.459240436553955,
      "learning_rate": 4.912789957477148e-05,
      "loss": 2.1501,
      "step": 13700
    },
    {
      "epoch": 1.054159346115652,
      "grad_norm": 3.984614849090576,
      "learning_rate": 4.912153387823696e-05,
      "loss": 2.1225,
      "step": 13800
    },
    {
      "epoch": 1.0617981819570697,
      "grad_norm": 4.381406784057617,
      "learning_rate": 4.9115168181702444e-05,
      "loss": 2.213,
      "step": 13900
    },
    {
      "epoch": 1.0694370177984875,
      "grad_norm": 4.571962356567383,
      "learning_rate": 4.9108802485167935e-05,
      "loss": 2.1812,
      "step": 14000
    },
    {
      "epoch": 1.0770758536399052,
      "grad_norm": 4.495993614196777,
      "learning_rate": 4.910243678863342e-05,
      "loss": 2.1612,
      "step": 14100
    },
    {
      "epoch": 1.084714689481323,
      "grad_norm": 5.194322109222412,
      "learning_rate": 4.90960710920989e-05,
      "loss": 2.1773,
      "step": 14200
    },
    {
      "epoch": 1.0923535253227408,
      "grad_norm": 6.22868537902832,
      "learning_rate": 4.9089705395564385e-05,
      "loss": 2.156,
      "step": 14300
    },
    {
      "epoch": 1.0999923611641587,
      "grad_norm": 6.081085681915283,
      "learning_rate": 4.908333969902987e-05,
      "loss": 2.1497,
      "step": 14400
    },
    {
      "epoch": 1.1076311970055763,
      "grad_norm": 5.961086750030518,
      "learning_rate": 4.907697400249536e-05,
      "loss": 2.1399,
      "step": 14500
    },
    {
      "epoch": 1.1152700328469942,
      "grad_norm": 5.200144290924072,
      "learning_rate": 4.907060830596084e-05,
      "loss": 2.1204,
      "step": 14600
    },
    {
      "epoch": 1.1229088686884119,
      "grad_norm": 5.688019752502441,
      "learning_rate": 4.9064242609426326e-05,
      "loss": 2.1568,
      "step": 14700
    },
    {
      "epoch": 1.1305477045298296,
      "grad_norm": 4.145893573760986,
      "learning_rate": 4.905787691289181e-05,
      "loss": 2.1535,
      "step": 14800
    },
    {
      "epoch": 1.1381865403712474,
      "grad_norm": 4.054906368255615,
      "learning_rate": 4.90515112163573e-05,
      "loss": 2.1917,
      "step": 14900
    },
    {
      "epoch": 1.1458253762126651,
      "grad_norm": 6.779367923736572,
      "learning_rate": 4.9045145519822784e-05,
      "loss": 2.2658,
      "step": 15000
    },
    {
      "epoch": 1.153464212054083,
      "grad_norm": 3.653660535812378,
      "learning_rate": 4.903877982328827e-05,
      "loss": 2.2033,
      "step": 15100
    },
    {
      "epoch": 1.1611030478955007,
      "grad_norm": 4.047735691070557,
      "learning_rate": 4.903241412675375e-05,
      "loss": 2.1292,
      "step": 15200
    },
    {
      "epoch": 1.1687418837369186,
      "grad_norm": 5.862399578094482,
      "learning_rate": 4.9026048430219234e-05,
      "loss": 2.1728,
      "step": 15300
    },
    {
      "epoch": 1.1763807195783362,
      "grad_norm": 4.829909324645996,
      "learning_rate": 4.9019682733684725e-05,
      "loss": 2.1782,
      "step": 15400
    },
    {
      "epoch": 1.1840195554197541,
      "grad_norm": 4.674121379852295,
      "learning_rate": 4.901331703715021e-05,
      "loss": 2.1537,
      "step": 15500
    },
    {
      "epoch": 1.1916583912611718,
      "grad_norm": 5.156543731689453,
      "learning_rate": 4.900695134061569e-05,
      "loss": 2.2121,
      "step": 15600
    },
    {
      "epoch": 1.1992972271025897,
      "grad_norm": 4.994520664215088,
      "learning_rate": 4.9000585644081175e-05,
      "loss": 2.1037,
      "step": 15700
    },
    {
      "epoch": 1.2069360629440073,
      "grad_norm": 4.793277740478516,
      "learning_rate": 4.8994219947546665e-05,
      "loss": 2.2251,
      "step": 15800
    },
    {
      "epoch": 1.214574898785425,
      "grad_norm": 5.018063068389893,
      "learning_rate": 4.898785425101215e-05,
      "loss": 2.1234,
      "step": 15900
    },
    {
      "epoch": 1.222213734626843,
      "grad_norm": 6.0472540855407715,
      "learning_rate": 4.898148855447763e-05,
      "loss": 2.2256,
      "step": 16000
    },
    {
      "epoch": 1.2298525704682606,
      "grad_norm": 4.678605556488037,
      "learning_rate": 4.8975122857943116e-05,
      "loss": 2.179,
      "step": 16100
    },
    {
      "epoch": 1.2374914063096785,
      "grad_norm": 6.100599765777588,
      "learning_rate": 4.89687571614086e-05,
      "loss": 2.1707,
      "step": 16200
    },
    {
      "epoch": 1.2451302421510961,
      "grad_norm": 5.2731475830078125,
      "learning_rate": 4.896239146487409e-05,
      "loss": 2.281,
      "step": 16300
    },
    {
      "epoch": 1.252769077992514,
      "grad_norm": 6.650087833404541,
      "learning_rate": 4.8956025768339573e-05,
      "loss": 2.2748,
      "step": 16400
    },
    {
      "epoch": 1.2604079138339317,
      "grad_norm": 5.361921787261963,
      "learning_rate": 4.894966007180506e-05,
      "loss": 2.1837,
      "step": 16500
    },
    {
      "epoch": 1.2680467496753494,
      "grad_norm": 5.769726276397705,
      "learning_rate": 4.894329437527054e-05,
      "loss": 2.1696,
      "step": 16600
    },
    {
      "epoch": 1.2756855855167673,
      "grad_norm": 4.066039085388184,
      "learning_rate": 4.8936928678736024e-05,
      "loss": 2.1457,
      "step": 16700
    },
    {
      "epoch": 1.2833244213581851,
      "grad_norm": 5.172082424163818,
      "learning_rate": 4.8930562982201514e-05,
      "loss": 2.1773,
      "step": 16800
    },
    {
      "epoch": 1.2909632571996028,
      "grad_norm": 5.9817094802856445,
      "learning_rate": 4.8924197285667e-05,
      "loss": 2.1951,
      "step": 16900
    },
    {
      "epoch": 1.2986020930410205,
      "grad_norm": 6.422817707061768,
      "learning_rate": 4.891783158913248e-05,
      "loss": 2.1133,
      "step": 17000
    },
    {
      "epoch": 1.3062409288824384,
      "grad_norm": 5.376896381378174,
      "learning_rate": 4.891146589259797e-05,
      "loss": 2.1469,
      "step": 17100
    },
    {
      "epoch": 1.313879764723856,
      "grad_norm": 3.4142510890960693,
      "learning_rate": 4.8905100196063455e-05,
      "loss": 2.1766,
      "step": 17200
    },
    {
      "epoch": 1.321518600565274,
      "grad_norm": 4.558021068572998,
      "learning_rate": 4.889873449952894e-05,
      "loss": 2.1528,
      "step": 17300
    },
    {
      "epoch": 1.3291574364066916,
      "grad_norm": 4.238364219665527,
      "learning_rate": 4.889236880299443e-05,
      "loss": 2.1989,
      "step": 17400
    },
    {
      "epoch": 1.3367962722481095,
      "grad_norm": 4.637102127075195,
      "learning_rate": 4.888600310645991e-05,
      "loss": 2.0712,
      "step": 17500
    },
    {
      "epoch": 1.3444351080895272,
      "grad_norm": 4.062856197357178,
      "learning_rate": 4.8879637409925396e-05,
      "loss": 2.0617,
      "step": 17600
    },
    {
      "epoch": 1.3520739439309448,
      "grad_norm": 3.9050891399383545,
      "learning_rate": 4.8873271713390887e-05,
      "loss": 2.1914,
      "step": 17700
    },
    {
      "epoch": 1.3597127797723627,
      "grad_norm": 5.718042373657227,
      "learning_rate": 4.886690601685637e-05,
      "loss": 2.1699,
      "step": 17800
    },
    {
      "epoch": 1.3673516156137804,
      "grad_norm": 4.354116916656494,
      "learning_rate": 4.8860540320321854e-05,
      "loss": 2.2103,
      "step": 17900
    },
    {
      "epoch": 1.3749904514551983,
      "grad_norm": 4.899850845336914,
      "learning_rate": 4.885417462378734e-05,
      "loss": 2.1215,
      "step": 18000
    },
    {
      "epoch": 1.382629287296616,
      "grad_norm": 10.652284622192383,
      "learning_rate": 4.884780892725283e-05,
      "loss": 2.2907,
      "step": 18100
    },
    {
      "epoch": 1.3902681231380338,
      "grad_norm": 5.675821781158447,
      "learning_rate": 4.884144323071831e-05,
      "loss": 2.1683,
      "step": 18200
    },
    {
      "epoch": 1.3979069589794515,
      "grad_norm": 4.043569087982178,
      "learning_rate": 4.8835077534183795e-05,
      "loss": 2.1464,
      "step": 18300
    },
    {
      "epoch": 1.4055457948208692,
      "grad_norm": 4.624657154083252,
      "learning_rate": 4.882871183764928e-05,
      "loss": 2.1238,
      "step": 18400
    },
    {
      "epoch": 1.413184630662287,
      "grad_norm": 4.769337177276611,
      "learning_rate": 4.882234614111476e-05,
      "loss": 2.1673,
      "step": 18500
    },
    {
      "epoch": 1.420823466503705,
      "grad_norm": 5.787594795227051,
      "learning_rate": 4.881598044458025e-05,
      "loss": 2.1006,
      "step": 18600
    },
    {
      "epoch": 1.4284623023451226,
      "grad_norm": 4.485126972198486,
      "learning_rate": 4.8809614748045735e-05,
      "loss": 2.0529,
      "step": 18700
    },
    {
      "epoch": 1.4361011381865403,
      "grad_norm": 5.658504962921143,
      "learning_rate": 4.880324905151122e-05,
      "loss": 2.0698,
      "step": 18800
    },
    {
      "epoch": 1.4437399740279582,
      "grad_norm": 5.069826602935791,
      "learning_rate": 4.87968833549767e-05,
      "loss": 2.1246,
      "step": 18900
    },
    {
      "epoch": 1.4513788098693758,
      "grad_norm": 4.87768030166626,
      "learning_rate": 4.8790517658442186e-05,
      "loss": 2.1206,
      "step": 19000
    },
    {
      "epoch": 1.4590176457107937,
      "grad_norm": 5.155447959899902,
      "learning_rate": 4.8784151961907676e-05,
      "loss": 2.1964,
      "step": 19100
    },
    {
      "epoch": 1.4666564815522114,
      "grad_norm": 5.651037216186523,
      "learning_rate": 4.877778626537316e-05,
      "loss": 2.1596,
      "step": 19200
    },
    {
      "epoch": 1.4742953173936293,
      "grad_norm": 6.657303810119629,
      "learning_rate": 4.8771420568838643e-05,
      "loss": 2.079,
      "step": 19300
    },
    {
      "epoch": 1.481934153235047,
      "grad_norm": 4.782303810119629,
      "learning_rate": 4.876505487230413e-05,
      "loss": 2.1995,
      "step": 19400
    },
    {
      "epoch": 1.4895729890764646,
      "grad_norm": 4.925703048706055,
      "learning_rate": 4.875868917576962e-05,
      "loss": 2.1218,
      "step": 19500
    },
    {
      "epoch": 1.4972118249178825,
      "grad_norm": 6.810848712921143,
      "learning_rate": 4.87523234792351e-05,
      "loss": 2.1854,
      "step": 19600
    },
    {
      "epoch": 1.5048506607593004,
      "grad_norm": 6.130570888519287,
      "learning_rate": 4.8745957782700584e-05,
      "loss": 2.0251,
      "step": 19700
    },
    {
      "epoch": 1.512489496600718,
      "grad_norm": 5.918476581573486,
      "learning_rate": 4.873959208616607e-05,
      "loss": 2.0872,
      "step": 19800
    },
    {
      "epoch": 1.5201283324421357,
      "grad_norm": 7.562942028045654,
      "learning_rate": 4.873322638963155e-05,
      "loss": 2.2041,
      "step": 19900
    },
    {
      "epoch": 1.5277671682835536,
      "grad_norm": 5.463088035583496,
      "learning_rate": 4.872686069309704e-05,
      "loss": 2.1747,
      "step": 20000
    },
    {
      "epoch": 1.5354060041249713,
      "grad_norm": 6.329188823699951,
      "learning_rate": 4.8720494996562525e-05,
      "loss": 2.1182,
      "step": 20100
    },
    {
      "epoch": 1.543044839966389,
      "grad_norm": 4.3021111488342285,
      "learning_rate": 4.871412930002801e-05,
      "loss": 2.1375,
      "step": 20200
    },
    {
      "epoch": 1.5506836758078069,
      "grad_norm": 5.480384826660156,
      "learning_rate": 4.870776360349349e-05,
      "loss": 2.1221,
      "step": 20300
    },
    {
      "epoch": 1.5583225116492248,
      "grad_norm": 4.1673197746276855,
      "learning_rate": 4.870139790695898e-05,
      "loss": 2.1011,
      "step": 20400
    },
    {
      "epoch": 1.5659613474906424,
      "grad_norm": 4.677631855010986,
      "learning_rate": 4.8695032210424466e-05,
      "loss": 2.1229,
      "step": 20500
    },
    {
      "epoch": 1.57360018333206,
      "grad_norm": 4.346617221832275,
      "learning_rate": 4.868866651388995e-05,
      "loss": 2.1143,
      "step": 20600
    },
    {
      "epoch": 1.581239019173478,
      "grad_norm": 5.942032814025879,
      "learning_rate": 4.868230081735543e-05,
      "loss": 2.1784,
      "step": 20700
    },
    {
      "epoch": 1.5888778550148959,
      "grad_norm": 5.14401388168335,
      "learning_rate": 4.867593512082092e-05,
      "loss": 2.1496,
      "step": 20800
    },
    {
      "epoch": 1.5965166908563135,
      "grad_norm": 4.923608779907227,
      "learning_rate": 4.866956942428641e-05,
      "loss": 2.1213,
      "step": 20900
    },
    {
      "epoch": 1.6041555266977312,
      "grad_norm": 4.917679309844971,
      "learning_rate": 4.866320372775189e-05,
      "loss": 2.1582,
      "step": 21000
    },
    {
      "epoch": 1.611794362539149,
      "grad_norm": 6.510858535766602,
      "learning_rate": 4.865683803121738e-05,
      "loss": 2.2582,
      "step": 21100
    },
    {
      "epoch": 1.6194331983805668,
      "grad_norm": 4.243424415588379,
      "learning_rate": 4.8650472334682865e-05,
      "loss": 2.1764,
      "step": 21200
    },
    {
      "epoch": 1.6270720342219844,
      "grad_norm": 6.301328659057617,
      "learning_rate": 4.864410663814835e-05,
      "loss": 2.158,
      "step": 21300
    },
    {
      "epoch": 1.6347108700634023,
      "grad_norm": 5.111812114715576,
      "learning_rate": 4.863774094161384e-05,
      "loss": 2.1786,
      "step": 21400
    },
    {
      "epoch": 1.6423497059048202,
      "grad_norm": 7.983246326446533,
      "learning_rate": 4.863137524507932e-05,
      "loss": 2.1528,
      "step": 21500
    },
    {
      "epoch": 1.6499885417462379,
      "grad_norm": 5.050661563873291,
      "learning_rate": 4.8625009548544805e-05,
      "loss": 2.0533,
      "step": 21600
    },
    {
      "epoch": 1.6576273775876555,
      "grad_norm": 6.025188446044922,
      "learning_rate": 4.861864385201029e-05,
      "loss": 2.1889,
      "step": 21700
    },
    {
      "epoch": 1.6652662134290734,
      "grad_norm": 4.917980194091797,
      "learning_rate": 4.861227815547578e-05,
      "loss": 2.1376,
      "step": 21800
    },
    {
      "epoch": 1.6729050492704913,
      "grad_norm": 4.371609210968018,
      "learning_rate": 4.860591245894126e-05,
      "loss": 2.0667,
      "step": 21900
    },
    {
      "epoch": 1.680543885111909,
      "grad_norm": 6.1302690505981445,
      "learning_rate": 4.8599546762406746e-05,
      "loss": 2.1436,
      "step": 22000
    },
    {
      "epoch": 1.6881827209533267,
      "grad_norm": 5.644843578338623,
      "learning_rate": 4.859318106587223e-05,
      "loss": 2.0392,
      "step": 22100
    },
    {
      "epoch": 1.6958215567947446,
      "grad_norm": 5.595630645751953,
      "learning_rate": 4.8586815369337713e-05,
      "loss": 2.0955,
      "step": 22200
    },
    {
      "epoch": 1.7034603926361622,
      "grad_norm": 4.8115081787109375,
      "learning_rate": 4.8580449672803204e-05,
      "loss": 2.0765,
      "step": 22300
    },
    {
      "epoch": 1.71109922847758,
      "grad_norm": 3.4439241886138916,
      "learning_rate": 4.857408397626869e-05,
      "loss": 2.1073,
      "step": 22400
    },
    {
      "epoch": 1.7187380643189978,
      "grad_norm": 4.72652530670166,
      "learning_rate": 4.856771827973417e-05,
      "loss": 2.1051,
      "step": 22500
    },
    {
      "epoch": 1.7263769001604157,
      "grad_norm": 4.072229385375977,
      "learning_rate": 4.8561352583199654e-05,
      "loss": 2.1477,
      "step": 22600
    },
    {
      "epoch": 1.7340157360018333,
      "grad_norm": 4.202186107635498,
      "learning_rate": 4.8554986886665145e-05,
      "loss": 2.2049,
      "step": 22700
    },
    {
      "epoch": 1.741654571843251,
      "grad_norm": 4.197848320007324,
      "learning_rate": 4.854862119013063e-05,
      "loss": 2.1878,
      "step": 22800
    },
    {
      "epoch": 1.749293407684669,
      "grad_norm": 6.1550822257995605,
      "learning_rate": 4.854225549359611e-05,
      "loss": 2.0561,
      "step": 22900
    },
    {
      "epoch": 1.7569322435260868,
      "grad_norm": 5.276869773864746,
      "learning_rate": 4.8535889797061595e-05,
      "loss": 2.1004,
      "step": 23000
    },
    {
      "epoch": 1.7645710793675042,
      "grad_norm": 12.029422760009766,
      "learning_rate": 4.852952410052708e-05,
      "loss": 2.1107,
      "step": 23100
    },
    {
      "epoch": 1.7722099152089221,
      "grad_norm": 4.983710765838623,
      "learning_rate": 4.852315840399257e-05,
      "loss": 2.131,
      "step": 23200
    },
    {
      "epoch": 1.77984875105034,
      "grad_norm": 4.1371002197265625,
      "learning_rate": 4.851679270745805e-05,
      "loss": 2.1985,
      "step": 23300
    },
    {
      "epoch": 1.7874875868917577,
      "grad_norm": 5.068281650543213,
      "learning_rate": 4.8510427010923536e-05,
      "loss": 2.1359,
      "step": 23400
    },
    {
      "epoch": 1.7951264227331754,
      "grad_norm": 4.510528564453125,
      "learning_rate": 4.850406131438902e-05,
      "loss": 2.0488,
      "step": 23500
    },
    {
      "epoch": 1.8027652585745932,
      "grad_norm": 5.442463397979736,
      "learning_rate": 4.849769561785451e-05,
      "loss": 2.1733,
      "step": 23600
    },
    {
      "epoch": 1.8104040944160111,
      "grad_norm": 6.8283257484436035,
      "learning_rate": 4.8491329921319994e-05,
      "loss": 2.1572,
      "step": 23700
    },
    {
      "epoch": 1.8180429302574288,
      "grad_norm": 5.184465408325195,
      "learning_rate": 4.848496422478548e-05,
      "loss": 2.1329,
      "step": 23800
    },
    {
      "epoch": 1.8256817660988465,
      "grad_norm": 6.569472789764404,
      "learning_rate": 4.847859852825096e-05,
      "loss": 2.0786,
      "step": 23900
    },
    {
      "epoch": 1.8333206019402644,
      "grad_norm": 4.285816192626953,
      "learning_rate": 4.8472232831716444e-05,
      "loss": 2.1484,
      "step": 24000
    },
    {
      "epoch": 1.840959437781682,
      "grad_norm": 6.099305629730225,
      "learning_rate": 4.8465867135181935e-05,
      "loss": 2.0387,
      "step": 24100
    },
    {
      "epoch": 1.8485982736230997,
      "grad_norm": 5.492764472961426,
      "learning_rate": 4.845950143864742e-05,
      "loss": 2.1993,
      "step": 24200
    },
    {
      "epoch": 1.8562371094645176,
      "grad_norm": 4.292374134063721,
      "learning_rate": 4.84531357421129e-05,
      "loss": 1.9753,
      "step": 24300
    },
    {
      "epoch": 1.8638759453059355,
      "grad_norm": 6.739704132080078,
      "learning_rate": 4.8446770045578385e-05,
      "loss": 2.1097,
      "step": 24400
    },
    {
      "epoch": 1.8715147811473531,
      "grad_norm": 4.284154891967773,
      "learning_rate": 4.8440404349043875e-05,
      "loss": 2.1677,
      "step": 24500
    },
    {
      "epoch": 1.8791536169887708,
      "grad_norm": 4.1605095863342285,
      "learning_rate": 4.843403865250936e-05,
      "loss": 2.2365,
      "step": 24600
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 8.439342498779297,
      "learning_rate": 4.842767295597484e-05,
      "loss": 2.1714,
      "step": 24700
    },
    {
      "epoch": 1.8944312886716066,
      "grad_norm": 5.864032745361328,
      "learning_rate": 4.8421307259440326e-05,
      "loss": 2.1274,
      "step": 24800
    },
    {
      "epoch": 1.9020701245130243,
      "grad_norm": 5.383835315704346,
      "learning_rate": 4.8414941562905816e-05,
      "loss": 2.0716,
      "step": 24900
    },
    {
      "epoch": 1.909708960354442,
      "grad_norm": 5.177967548370361,
      "learning_rate": 4.84085758663713e-05,
      "loss": 2.0403,
      "step": 25000
    },
    {
      "epoch": 1.9173477961958598,
      "grad_norm": 5.032341957092285,
      "learning_rate": 4.840221016983679e-05,
      "loss": 2.1198,
      "step": 25100
    },
    {
      "epoch": 1.9249866320372775,
      "grad_norm": 5.435822486877441,
      "learning_rate": 4.8395844473302274e-05,
      "loss": 2.1566,
      "step": 25200
    },
    {
      "epoch": 1.9326254678786952,
      "grad_norm": 4.061986923217773,
      "learning_rate": 4.838947877676776e-05,
      "loss": 2.0638,
      "step": 25300
    },
    {
      "epoch": 1.940264303720113,
      "grad_norm": 5.794314384460449,
      "learning_rate": 4.838311308023324e-05,
      "loss": 2.227,
      "step": 25400
    },
    {
      "epoch": 1.947903139561531,
      "grad_norm": 5.677358627319336,
      "learning_rate": 4.837674738369873e-05,
      "loss": 2.1671,
      "step": 25500
    },
    {
      "epoch": 1.9555419754029486,
      "grad_norm": 4.265524387359619,
      "learning_rate": 4.8370381687164215e-05,
      "loss": 2.1347,
      "step": 25600
    },
    {
      "epoch": 1.9631808112443663,
      "grad_norm": 5.251564025878906,
      "learning_rate": 4.83640159906297e-05,
      "loss": 2.1373,
      "step": 25700
    },
    {
      "epoch": 1.9708196470857842,
      "grad_norm": 5.316563606262207,
      "learning_rate": 4.835765029409518e-05,
      "loss": 2.1637,
      "step": 25800
    },
    {
      "epoch": 1.978458482927202,
      "grad_norm": 6.519346714019775,
      "learning_rate": 4.835128459756067e-05,
      "loss": 2.0343,
      "step": 25900
    },
    {
      "epoch": 1.9860973187686195,
      "grad_norm": 6.586745262145996,
      "learning_rate": 4.8344918901026156e-05,
      "loss": 2.068,
      "step": 26000
    },
    {
      "epoch": 1.9937361546100374,
      "grad_norm": 4.336559772491455,
      "learning_rate": 4.833855320449164e-05,
      "loss": 2.2537,
      "step": 26100
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.020484209060669,
      "eval_runtime": 3.1424,
      "eval_samples_per_second": 219.577,
      "eval_steps_per_second": 219.577,
      "step": 26182
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8968026638031006,
      "eval_runtime": 58.4097,
      "eval_samples_per_second": 224.124,
      "eval_steps_per_second": 224.124,
      "step": 26182
    },
    {
      "epoch": 2.0013749904514553,
      "grad_norm": 5.239777565002441,
      "learning_rate": 4.833218750795712e-05,
      "loss": 2.096,
      "step": 26200
    },
    {
      "epoch": 2.009013826292873,
      "grad_norm": 9.349827766418457,
      "learning_rate": 4.8325821811422606e-05,
      "loss": 2.0455,
      "step": 26300
    },
    {
      "epoch": 2.0166526621342906,
      "grad_norm": 5.598684310913086,
      "learning_rate": 4.8319456114888097e-05,
      "loss": 2.0727,
      "step": 26400
    },
    {
      "epoch": 2.0242914979757085,
      "grad_norm": 4.535653114318848,
      "learning_rate": 4.831309041835358e-05,
      "loss": 2.0427,
      "step": 26500
    },
    {
      "epoch": 2.0319303338171264,
      "grad_norm": 5.4314422607421875,
      "learning_rate": 4.8306724721819064e-05,
      "loss": 2.0167,
      "step": 26600
    },
    {
      "epoch": 2.039569169658544,
      "grad_norm": 6.214211463928223,
      "learning_rate": 4.830035902528455e-05,
      "loss": 1.9405,
      "step": 26700
    },
    {
      "epoch": 2.0472080054999617,
      "grad_norm": 7.109127521514893,
      "learning_rate": 4.829399332875004e-05,
      "loss": 1.9976,
      "step": 26800
    },
    {
      "epoch": 2.0548468413413796,
      "grad_norm": 4.740929126739502,
      "learning_rate": 4.828762763221552e-05,
      "loss": 2.1325,
      "step": 26900
    },
    {
      "epoch": 2.0624856771827975,
      "grad_norm": 6.149768352508545,
      "learning_rate": 4.8281261935681005e-05,
      "loss": 2.1174,
      "step": 27000
    },
    {
      "epoch": 2.070124513024215,
      "grad_norm": 5.684814453125,
      "learning_rate": 4.827489623914649e-05,
      "loss": 2.0005,
      "step": 27100
    },
    {
      "epoch": 2.077763348865633,
      "grad_norm": 5.925021648406982,
      "learning_rate": 4.826853054261197e-05,
      "loss": 2.1668,
      "step": 27200
    },
    {
      "epoch": 2.0854021847070507,
      "grad_norm": 6.058468341827393,
      "learning_rate": 4.826216484607746e-05,
      "loss": 2.141,
      "step": 27300
    },
    {
      "epoch": 2.093041020548468,
      "grad_norm": 4.875472068786621,
      "learning_rate": 4.8255799149542945e-05,
      "loss": 2.1204,
      "step": 27400
    },
    {
      "epoch": 2.100679856389886,
      "grad_norm": 6.417136192321777,
      "learning_rate": 4.824943345300843e-05,
      "loss": 2.1333,
      "step": 27500
    },
    {
      "epoch": 2.108318692231304,
      "grad_norm": 5.101943016052246,
      "learning_rate": 4.824306775647391e-05,
      "loss": 2.035,
      "step": 27600
    },
    {
      "epoch": 2.115957528072722,
      "grad_norm": 9.323779106140137,
      "learning_rate": 4.8236702059939396e-05,
      "loss": 2.1425,
      "step": 27700
    },
    {
      "epoch": 2.1235963639141393,
      "grad_norm": 6.243385314941406,
      "learning_rate": 4.8230336363404886e-05,
      "loss": 2.1066,
      "step": 27800
    },
    {
      "epoch": 2.131235199755557,
      "grad_norm": 5.291448593139648,
      "learning_rate": 4.822397066687037e-05,
      "loss": 2.1073,
      "step": 27900
    },
    {
      "epoch": 2.138874035596975,
      "grad_norm": 5.441615104675293,
      "learning_rate": 4.8217604970335853e-05,
      "loss": 1.9885,
      "step": 28000
    },
    {
      "epoch": 2.146512871438393,
      "grad_norm": 4.194351673126221,
      "learning_rate": 4.821123927380134e-05,
      "loss": 2.0316,
      "step": 28100
    },
    {
      "epoch": 2.1541517072798104,
      "grad_norm": 4.211719036102295,
      "learning_rate": 4.820487357726683e-05,
      "loss": 2.057,
      "step": 28200
    },
    {
      "epoch": 2.1617905431212283,
      "grad_norm": 5.979389190673828,
      "learning_rate": 4.819850788073231e-05,
      "loss": 2.09,
      "step": 28300
    },
    {
      "epoch": 2.169429378962646,
      "grad_norm": 4.7104268074035645,
      "learning_rate": 4.8192142184197794e-05,
      "loss": 2.0834,
      "step": 28400
    },
    {
      "epoch": 2.1770682148040637,
      "grad_norm": 5.146792411804199,
      "learning_rate": 4.818577648766328e-05,
      "loss": 2.1638,
      "step": 28500
    },
    {
      "epoch": 2.1847070506454815,
      "grad_norm": 5.3610920906066895,
      "learning_rate": 4.817941079112877e-05,
      "loss": 2.107,
      "step": 28600
    },
    {
      "epoch": 2.1923458864868994,
      "grad_norm": 6.014113903045654,
      "learning_rate": 4.817304509459425e-05,
      "loss": 2.1409,
      "step": 28700
    },
    {
      "epoch": 2.1999847223283173,
      "grad_norm": 6.0757317543029785,
      "learning_rate": 4.8166679398059735e-05,
      "loss": 2.0646,
      "step": 28800
    },
    {
      "epoch": 2.2076235581697348,
      "grad_norm": 3.780609369277954,
      "learning_rate": 4.8160313701525226e-05,
      "loss": 2.0984,
      "step": 28900
    },
    {
      "epoch": 2.2152623940111527,
      "grad_norm": 4.89990758895874,
      "learning_rate": 4.815394800499071e-05,
      "loss": 2.0923,
      "step": 29000
    },
    {
      "epoch": 2.2229012298525705,
      "grad_norm": 3.7411105632781982,
      "learning_rate": 4.81475823084562e-05,
      "loss": 2.0936,
      "step": 29100
    },
    {
      "epoch": 2.2305400656939884,
      "grad_norm": 4.921263694763184,
      "learning_rate": 4.814121661192168e-05,
      "loss": 2.1132,
      "step": 29200
    },
    {
      "epoch": 2.238178901535406,
      "grad_norm": 4.462282657623291,
      "learning_rate": 4.8134850915387167e-05,
      "loss": 2.0996,
      "step": 29300
    },
    {
      "epoch": 2.2458177373768238,
      "grad_norm": 5.4401702880859375,
      "learning_rate": 4.812848521885265e-05,
      "loss": 2.0292,
      "step": 29400
    },
    {
      "epoch": 2.2534565732182417,
      "grad_norm": 8.745314598083496,
      "learning_rate": 4.8122119522318134e-05,
      "loss": 2.0795,
      "step": 29500
    },
    {
      "epoch": 2.261095409059659,
      "grad_norm": 5.076837062835693,
      "learning_rate": 4.8115753825783624e-05,
      "loss": 2.1051,
      "step": 29600
    },
    {
      "epoch": 2.268734244901077,
      "grad_norm": 4.635329246520996,
      "learning_rate": 4.810938812924911e-05,
      "loss": 2.0986,
      "step": 29700
    },
    {
      "epoch": 2.276373080742495,
      "grad_norm": 5.220306873321533,
      "learning_rate": 4.810302243271459e-05,
      "loss": 2.1423,
      "step": 29800
    },
    {
      "epoch": 2.284011916583913,
      "grad_norm": 5.391396522521973,
      "learning_rate": 4.8096656736180075e-05,
      "loss": 2.0505,
      "step": 29900
    },
    {
      "epoch": 2.2916507524253302,
      "grad_norm": 4.772590637207031,
      "learning_rate": 4.8090291039645565e-05,
      "loss": 1.9732,
      "step": 30000
    },
    {
      "epoch": 2.299289588266748,
      "grad_norm": 5.256459712982178,
      "learning_rate": 4.808392534311105e-05,
      "loss": 2.064,
      "step": 30100
    },
    {
      "epoch": 2.306928424108166,
      "grad_norm": 8.409823417663574,
      "learning_rate": 4.807755964657653e-05,
      "loss": 2.1115,
      "step": 30200
    },
    {
      "epoch": 2.314567259949584,
      "grad_norm": 4.639118671417236,
      "learning_rate": 4.8071193950042015e-05,
      "loss": 2.0753,
      "step": 30300
    },
    {
      "epoch": 2.3222060957910013,
      "grad_norm": 5.826315402984619,
      "learning_rate": 4.80648282535075e-05,
      "loss": 2.1097,
      "step": 30400
    },
    {
      "epoch": 2.3298449316324192,
      "grad_norm": 6.313990116119385,
      "learning_rate": 4.805846255697299e-05,
      "loss": 2.1108,
      "step": 30500
    },
    {
      "epoch": 2.337483767473837,
      "grad_norm": 6.015549659729004,
      "learning_rate": 4.805209686043847e-05,
      "loss": 2.0456,
      "step": 30600
    },
    {
      "epoch": 2.3451226033152546,
      "grad_norm": 4.639851093292236,
      "learning_rate": 4.8045731163903956e-05,
      "loss": 2.098,
      "step": 30700
    },
    {
      "epoch": 2.3527614391566725,
      "grad_norm": 4.938508987426758,
      "learning_rate": 4.803936546736944e-05,
      "loss": 2.1979,
      "step": 30800
    },
    {
      "epoch": 2.3604002749980904,
      "grad_norm": 4.838897705078125,
      "learning_rate": 4.8032999770834923e-05,
      "loss": 2.0043,
      "step": 30900
    },
    {
      "epoch": 2.3680391108395082,
      "grad_norm": 4.856192111968994,
      "learning_rate": 4.8026634074300414e-05,
      "loss": 2.019,
      "step": 31000
    },
    {
      "epoch": 2.3756779466809257,
      "grad_norm": 5.483775615692139,
      "learning_rate": 4.80202683777659e-05,
      "loss": 2.0729,
      "step": 31100
    },
    {
      "epoch": 2.3833167825223436,
      "grad_norm": 4.7839250564575195,
      "learning_rate": 4.801390268123138e-05,
      "loss": 2.123,
      "step": 31200
    },
    {
      "epoch": 2.3909556183637615,
      "grad_norm": 4.284111499786377,
      "learning_rate": 4.8007536984696864e-05,
      "loss": 2.0884,
      "step": 31300
    },
    {
      "epoch": 2.3985944542051794,
      "grad_norm": 5.394376277923584,
      "learning_rate": 4.8001171288162355e-05,
      "loss": 2.037,
      "step": 31400
    },
    {
      "epoch": 2.406233290046597,
      "grad_norm": 6.648130416870117,
      "learning_rate": 4.799480559162784e-05,
      "loss": 2.0557,
      "step": 31500
    },
    {
      "epoch": 2.4138721258880147,
      "grad_norm": 6.490131378173828,
      "learning_rate": 4.798843989509332e-05,
      "loss": 2.0922,
      "step": 31600
    },
    {
      "epoch": 2.4215109617294326,
      "grad_norm": 5.608483791351318,
      "learning_rate": 4.7982074198558805e-05,
      "loss": 2.0661,
      "step": 31700
    },
    {
      "epoch": 2.42914979757085,
      "grad_norm": 4.925465106964111,
      "learning_rate": 4.797570850202429e-05,
      "loss": 2.1197,
      "step": 31800
    },
    {
      "epoch": 2.436788633412268,
      "grad_norm": 4.274147033691406,
      "learning_rate": 4.796934280548978e-05,
      "loss": 2.0894,
      "step": 31900
    },
    {
      "epoch": 2.444427469253686,
      "grad_norm": 5.028322696685791,
      "learning_rate": 4.796297710895526e-05,
      "loss": 2.1839,
      "step": 32000
    },
    {
      "epoch": 2.4520663050951033,
      "grad_norm": 5.157658100128174,
      "learning_rate": 4.7956611412420746e-05,
      "loss": 2.0122,
      "step": 32100
    },
    {
      "epoch": 2.459705140936521,
      "grad_norm": 5.35587739944458,
      "learning_rate": 4.795024571588623e-05,
      "loss": 2.1586,
      "step": 32200
    },
    {
      "epoch": 2.467343976777939,
      "grad_norm": 4.257548809051514,
      "learning_rate": 4.794388001935172e-05,
      "loss": 2.0823,
      "step": 32300
    },
    {
      "epoch": 2.474982812619357,
      "grad_norm": 6.824571132659912,
      "learning_rate": 4.7937514322817204e-05,
      "loss": 2.0326,
      "step": 32400
    },
    {
      "epoch": 2.482621648460775,
      "grad_norm": 4.206033229827881,
      "learning_rate": 4.793114862628269e-05,
      "loss": 2.0533,
      "step": 32500
    },
    {
      "epoch": 2.4902604843021923,
      "grad_norm": 7.227175235748291,
      "learning_rate": 4.792478292974818e-05,
      "loss": 2.0373,
      "step": 32600
    },
    {
      "epoch": 2.49789932014361,
      "grad_norm": 4.7988152503967285,
      "learning_rate": 4.791841723321366e-05,
      "loss": 2.1041,
      "step": 32700
    },
    {
      "epoch": 2.505538155985028,
      "grad_norm": 4.865692615509033,
      "learning_rate": 4.7912051536679145e-05,
      "loss": 2.0838,
      "step": 32800
    },
    {
      "epoch": 2.5131769918264455,
      "grad_norm": 4.131801128387451,
      "learning_rate": 4.7905685840144635e-05,
      "loss": 2.0647,
      "step": 32900
    },
    {
      "epoch": 2.5208158276678634,
      "grad_norm": 6.135159969329834,
      "learning_rate": 4.789932014361012e-05,
      "loss": 2.1144,
      "step": 33000
    },
    {
      "epoch": 2.5284546635092813,
      "grad_norm": 4.225569248199463,
      "learning_rate": 4.78929544470756e-05,
      "loss": 1.9846,
      "step": 33100
    },
    {
      "epoch": 2.5360934993506987,
      "grad_norm": 5.735033988952637,
      "learning_rate": 4.7886588750541085e-05,
      "loss": 1.9852,
      "step": 33200
    },
    {
      "epoch": 2.5437323351921166,
      "grad_norm": 5.599527835845947,
      "learning_rate": 4.7880223054006576e-05,
      "loss": 2.0807,
      "step": 33300
    },
    {
      "epoch": 2.5513711710335345,
      "grad_norm": 5.068694591522217,
      "learning_rate": 4.787385735747206e-05,
      "loss": 2.0887,
      "step": 33400
    },
    {
      "epoch": 2.5590100068749524,
      "grad_norm": 4.8097429275512695,
      "learning_rate": 4.786749166093754e-05,
      "loss": 2.0133,
      "step": 33500
    },
    {
      "epoch": 2.5666488427163703,
      "grad_norm": 4.6799750328063965,
      "learning_rate": 4.7861125964403026e-05,
      "loss": 2.1557,
      "step": 33600
    },
    {
      "epoch": 2.5742876785577877,
      "grad_norm": 5.470669269561768,
      "learning_rate": 4.785476026786852e-05,
      "loss": 2.0459,
      "step": 33700
    },
    {
      "epoch": 2.5819265143992056,
      "grad_norm": 6.791591167449951,
      "learning_rate": 4.7848394571334e-05,
      "loss": 2.0902,
      "step": 33800
    },
    {
      "epoch": 2.5895653502406235,
      "grad_norm": 4.428032398223877,
      "learning_rate": 4.7842028874799484e-05,
      "loss": 2.0803,
      "step": 33900
    },
    {
      "epoch": 2.597204186082041,
      "grad_norm": 5.734211444854736,
      "learning_rate": 4.783566317826497e-05,
      "loss": 2.0473,
      "step": 34000
    },
    {
      "epoch": 2.604843021923459,
      "grad_norm": 5.6737823486328125,
      "learning_rate": 4.782929748173045e-05,
      "loss": 2.0911,
      "step": 34100
    },
    {
      "epoch": 2.6124818577648767,
      "grad_norm": 4.2421650886535645,
      "learning_rate": 4.782293178519594e-05,
      "loss": 2.0211,
      "step": 34200
    },
    {
      "epoch": 2.620120693606294,
      "grad_norm": 4.961748123168945,
      "learning_rate": 4.7816566088661425e-05,
      "loss": 2.0429,
      "step": 34300
    },
    {
      "epoch": 2.627759529447712,
      "grad_norm": 6.896844863891602,
      "learning_rate": 4.781020039212691e-05,
      "loss": 1.9485,
      "step": 34400
    },
    {
      "epoch": 2.63539836528913,
      "grad_norm": 4.352271556854248,
      "learning_rate": 4.780383469559239e-05,
      "loss": 2.006,
      "step": 34500
    },
    {
      "epoch": 2.643037201130548,
      "grad_norm": 5.78906774520874,
      "learning_rate": 4.779746899905788e-05,
      "loss": 2.0499,
      "step": 34600
    },
    {
      "epoch": 2.6506760369719657,
      "grad_norm": 5.523979663848877,
      "learning_rate": 4.7791103302523366e-05,
      "loss": 1.9969,
      "step": 34700
    },
    {
      "epoch": 2.658314872813383,
      "grad_norm": 3.9258153438568115,
      "learning_rate": 4.778473760598885e-05,
      "loss": 2.108,
      "step": 34800
    },
    {
      "epoch": 2.665953708654801,
      "grad_norm": 6.43876314163208,
      "learning_rate": 4.777837190945433e-05,
      "loss": 2.1521,
      "step": 34900
    },
    {
      "epoch": 2.673592544496219,
      "grad_norm": 4.416392803192139,
      "learning_rate": 4.7772006212919816e-05,
      "loss": 2.1582,
      "step": 35000
    },
    {
      "epoch": 2.6812313803376364,
      "grad_norm": 5.201959609985352,
      "learning_rate": 4.7765640516385307e-05,
      "loss": 2.1457,
      "step": 35100
    },
    {
      "epoch": 2.6888702161790543,
      "grad_norm": 4.056188106536865,
      "learning_rate": 4.775927481985079e-05,
      "loss": 2.0516,
      "step": 35200
    },
    {
      "epoch": 2.696509052020472,
      "grad_norm": 5.854379653930664,
      "learning_rate": 4.7752909123316274e-05,
      "loss": 2.0607,
      "step": 35300
    },
    {
      "epoch": 2.7041478878618896,
      "grad_norm": 5.085122585296631,
      "learning_rate": 4.774654342678176e-05,
      "loss": 2.0034,
      "step": 35400
    },
    {
      "epoch": 2.7117867237033075,
      "grad_norm": 3.8859007358551025,
      "learning_rate": 4.774017773024725e-05,
      "loss": 2.1394,
      "step": 35500
    },
    {
      "epoch": 2.7194255595447254,
      "grad_norm": 4.854066848754883,
      "learning_rate": 4.773381203371273e-05,
      "loss": 2.1363,
      "step": 35600
    },
    {
      "epoch": 2.7270643953861433,
      "grad_norm": 4.935229301452637,
      "learning_rate": 4.7727446337178215e-05,
      "loss": 2.0532,
      "step": 35700
    },
    {
      "epoch": 2.7347032312275608,
      "grad_norm": 4.085946559906006,
      "learning_rate": 4.77210806406437e-05,
      "loss": 2.0394,
      "step": 35800
    },
    {
      "epoch": 2.7423420670689787,
      "grad_norm": 4.631667613983154,
      "learning_rate": 4.771471494410918e-05,
      "loss": 2.0588,
      "step": 35900
    },
    {
      "epoch": 2.7499809029103965,
      "grad_norm": 4.287802696228027,
      "learning_rate": 4.770834924757467e-05,
      "loss": 1.9409,
      "step": 36000
    },
    {
      "epoch": 2.7576197387518144,
      "grad_norm": 4.994728088378906,
      "learning_rate": 4.7701983551040155e-05,
      "loss": 1.987,
      "step": 36100
    },
    {
      "epoch": 2.765258574593232,
      "grad_norm": 4.268820285797119,
      "learning_rate": 4.769561785450564e-05,
      "loss": 1.9888,
      "step": 36200
    },
    {
      "epoch": 2.7728974104346498,
      "grad_norm": 5.370898246765137,
      "learning_rate": 4.768925215797113e-05,
      "loss": 2.0366,
      "step": 36300
    },
    {
      "epoch": 2.7805362462760677,
      "grad_norm": 7.054059028625488,
      "learning_rate": 4.768288646143661e-05,
      "loss": 2.0979,
      "step": 36400
    },
    {
      "epoch": 2.788175082117485,
      "grad_norm": 6.168173313140869,
      "learning_rate": 4.7676520764902096e-05,
      "loss": 2.0609,
      "step": 36500
    },
    {
      "epoch": 2.795813917958903,
      "grad_norm": 4.5591325759887695,
      "learning_rate": 4.767015506836759e-05,
      "loss": 2.0574,
      "step": 36600
    },
    {
      "epoch": 2.803452753800321,
      "grad_norm": 4.750640869140625,
      "learning_rate": 4.766378937183307e-05,
      "loss": 2.1063,
      "step": 36700
    },
    {
      "epoch": 2.8110915896417383,
      "grad_norm": 4.647895336151123,
      "learning_rate": 4.7657423675298554e-05,
      "loss": 2.0155,
      "step": 36800
    },
    {
      "epoch": 2.818730425483156,
      "grad_norm": 5.193914890289307,
      "learning_rate": 4.7651057978764044e-05,
      "loss": 1.9837,
      "step": 36900
    },
    {
      "epoch": 2.826369261324574,
      "grad_norm": 5.381611347198486,
      "learning_rate": 4.764469228222953e-05,
      "loss": 2.0764,
      "step": 37000
    },
    {
      "epoch": 2.834008097165992,
      "grad_norm": 5.065334796905518,
      "learning_rate": 4.763832658569501e-05,
      "loss": 1.9949,
      "step": 37100
    },
    {
      "epoch": 2.84164693300741,
      "grad_norm": 7.530979156494141,
      "learning_rate": 4.7631960889160495e-05,
      "loss": 2.0981,
      "step": 37200
    },
    {
      "epoch": 2.8492857688488273,
      "grad_norm": 6.000520706176758,
      "learning_rate": 4.762559519262598e-05,
      "loss": 2.0499,
      "step": 37300
    },
    {
      "epoch": 2.8569246046902452,
      "grad_norm": 5.795202732086182,
      "learning_rate": 4.761922949609147e-05,
      "loss": 2.1414,
      "step": 37400
    },
    {
      "epoch": 2.864563440531663,
      "grad_norm": 5.080810546875,
      "learning_rate": 4.761286379955695e-05,
      "loss": 1.9555,
      "step": 37500
    },
    {
      "epoch": 2.8722022763730806,
      "grad_norm": 4.061729431152344,
      "learning_rate": 4.7606498103022436e-05,
      "loss": 2.1304,
      "step": 37600
    },
    {
      "epoch": 2.8798411122144985,
      "grad_norm": 3.5828006267547607,
      "learning_rate": 4.760013240648792e-05,
      "loss": 2.0668,
      "step": 37700
    },
    {
      "epoch": 2.8874799480559163,
      "grad_norm": 5.215569496154785,
      "learning_rate": 4.759376670995341e-05,
      "loss": 2.0469,
      "step": 37800
    },
    {
      "epoch": 2.895118783897334,
      "grad_norm": 4.157955169677734,
      "learning_rate": 4.758740101341889e-05,
      "loss": 2.0211,
      "step": 37900
    },
    {
      "epoch": 2.9027576197387517,
      "grad_norm": 5.153299808502197,
      "learning_rate": 4.7581035316884377e-05,
      "loss": 1.9759,
      "step": 38000
    },
    {
      "epoch": 2.9103964555801696,
      "grad_norm": 5.149206161499023,
      "learning_rate": 4.757466962034986e-05,
      "loss": 2.1049,
      "step": 38100
    },
    {
      "epoch": 2.9180352914215875,
      "grad_norm": 5.8140130043029785,
      "learning_rate": 4.7568303923815344e-05,
      "loss": 2.1687,
      "step": 38200
    },
    {
      "epoch": 2.9256741272630054,
      "grad_norm": 4.380126476287842,
      "learning_rate": 4.7561938227280834e-05,
      "loss": 2.0356,
      "step": 38300
    },
    {
      "epoch": 2.933312963104423,
      "grad_norm": 4.340345859527588,
      "learning_rate": 4.755557253074632e-05,
      "loss": 2.0534,
      "step": 38400
    },
    {
      "epoch": 2.9409517989458407,
      "grad_norm": 4.6894989013671875,
      "learning_rate": 4.75492068342118e-05,
      "loss": 2.12,
      "step": 38500
    },
    {
      "epoch": 2.9485906347872586,
      "grad_norm": 6.527030944824219,
      "learning_rate": 4.7542841137677285e-05,
      "loss": 2.0311,
      "step": 38600
    },
    {
      "epoch": 2.956229470628676,
      "grad_norm": 5.363827705383301,
      "learning_rate": 4.7536475441142775e-05,
      "loss": 2.0215,
      "step": 38700
    },
    {
      "epoch": 2.963868306470094,
      "grad_norm": 4.951590061187744,
      "learning_rate": 4.753010974460826e-05,
      "loss": 1.9832,
      "step": 38800
    },
    {
      "epoch": 2.971507142311512,
      "grad_norm": 4.7511067390441895,
      "learning_rate": 4.752374404807374e-05,
      "loss": 2.1989,
      "step": 38900
    },
    {
      "epoch": 2.9791459781529293,
      "grad_norm": 5.65178918838501,
      "learning_rate": 4.7517378351539225e-05,
      "loss": 2.1729,
      "step": 39000
    },
    {
      "epoch": 2.986784813994347,
      "grad_norm": 6.222956657409668,
      "learning_rate": 4.751101265500471e-05,
      "loss": 1.9993,
      "step": 39100
    },
    {
      "epoch": 2.994423649835765,
      "grad_norm": 4.718719482421875,
      "learning_rate": 4.75046469584702e-05,
      "loss": 2.103,
      "step": 39200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9709210395812988,
      "eval_runtime": 3.0013,
      "eval_samples_per_second": 229.904,
      "eval_steps_per_second": 229.904,
      "step": 39273
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.8291467428207397,
      "eval_runtime": 57.746,
      "eval_samples_per_second": 226.7,
      "eval_steps_per_second": 226.7,
      "step": 39273
    },
    {
      "epoch": 3.002062485677183,
      "grad_norm": 6.920961380004883,
      "learning_rate": 4.749828126193568e-05,
      "loss": 2.0947,
      "step": 39300
    },
    {
      "epoch": 3.0097013215186004,
      "grad_norm": 4.873175621032715,
      "learning_rate": 4.7491915565401166e-05,
      "loss": 1.9453,
      "step": 39400
    },
    {
      "epoch": 3.0173401573600183,
      "grad_norm": 7.457477569580078,
      "learning_rate": 4.748554986886665e-05,
      "loss": 2.0029,
      "step": 39500
    },
    {
      "epoch": 3.024978993201436,
      "grad_norm": 5.837228298187256,
      "learning_rate": 4.7479184172332133e-05,
      "loss": 2.0186,
      "step": 39600
    },
    {
      "epoch": 3.032617829042854,
      "grad_norm": 11.411619186401367,
      "learning_rate": 4.7472818475797624e-05,
      "loss": 2.059,
      "step": 39700
    },
    {
      "epoch": 3.0402566648842715,
      "grad_norm": 4.735198497772217,
      "learning_rate": 4.746645277926311e-05,
      "loss": 1.9568,
      "step": 39800
    },
    {
      "epoch": 3.0478955007256894,
      "grad_norm": 3.876016616821289,
      "learning_rate": 4.746008708272859e-05,
      "loss": 1.9365,
      "step": 39900
    },
    {
      "epoch": 3.0555343365671073,
      "grad_norm": 6.221607208251953,
      "learning_rate": 4.7453721386194074e-05,
      "loss": 1.9899,
      "step": 40000
    },
    {
      "epoch": 3.0631731724085247,
      "grad_norm": 6.019495010375977,
      "learning_rate": 4.7447355689659565e-05,
      "loss": 2.0742,
      "step": 40100
    },
    {
      "epoch": 3.0708120082499426,
      "grad_norm": 4.211643218994141,
      "learning_rate": 4.744098999312505e-05,
      "loss": 2.1225,
      "step": 40200
    },
    {
      "epoch": 3.0784508440913605,
      "grad_norm": 4.1621270179748535,
      "learning_rate": 4.743462429659054e-05,
      "loss": 1.9934,
      "step": 40300
    },
    {
      "epoch": 3.0860896799327784,
      "grad_norm": 8.324265480041504,
      "learning_rate": 4.742825860005602e-05,
      "loss": 2.1573,
      "step": 40400
    },
    {
      "epoch": 3.093728515774196,
      "grad_norm": 5.026970863342285,
      "learning_rate": 4.7421892903521506e-05,
      "loss": 1.9929,
      "step": 40500
    },
    {
      "epoch": 3.1013673516156137,
      "grad_norm": 4.514750003814697,
      "learning_rate": 4.7415527206986996e-05,
      "loss": 2.082,
      "step": 40600
    },
    {
      "epoch": 3.1090061874570316,
      "grad_norm": 5.714109420776367,
      "learning_rate": 4.740916151045248e-05,
      "loss": 2.0858,
      "step": 40700
    },
    {
      "epoch": 3.1166450232984495,
      "grad_norm": 4.341569423675537,
      "learning_rate": 4.740279581391796e-05,
      "loss": 1.9853,
      "step": 40800
    },
    {
      "epoch": 3.124283859139867,
      "grad_norm": 7.402037620544434,
      "learning_rate": 4.7396430117383447e-05,
      "loss": 2.0766,
      "step": 40900
    },
    {
      "epoch": 3.131922694981285,
      "grad_norm": 4.035668849945068,
      "learning_rate": 4.739006442084894e-05,
      "loss": 2.0204,
      "step": 41000
    },
    {
      "epoch": 3.1395615308227027,
      "grad_norm": 7.0034613609313965,
      "learning_rate": 4.738369872431442e-05,
      "loss": 1.9795,
      "step": 41100
    },
    {
      "epoch": 3.14720036666412,
      "grad_norm": 5.146919250488281,
      "learning_rate": 4.7377333027779904e-05,
      "loss": 2.0825,
      "step": 41200
    },
    {
      "epoch": 3.154839202505538,
      "grad_norm": 6.133081912994385,
      "learning_rate": 4.737096733124539e-05,
      "loss": 2.0455,
      "step": 41300
    },
    {
      "epoch": 3.162478038346956,
      "grad_norm": 5.0865888595581055,
      "learning_rate": 4.736460163471087e-05,
      "loss": 2.0393,
      "step": 41400
    },
    {
      "epoch": 3.170116874188374,
      "grad_norm": 4.154977798461914,
      "learning_rate": 4.735823593817636e-05,
      "loss": 1.9669,
      "step": 41500
    },
    {
      "epoch": 3.1777557100297913,
      "grad_norm": 7.2478437423706055,
      "learning_rate": 4.7351870241641845e-05,
      "loss": 1.9831,
      "step": 41600
    },
    {
      "epoch": 3.185394545871209,
      "grad_norm": 6.258958339691162,
      "learning_rate": 4.734550454510733e-05,
      "loss": 1.9492,
      "step": 41700
    },
    {
      "epoch": 3.193033381712627,
      "grad_norm": 5.562872886657715,
      "learning_rate": 4.733913884857281e-05,
      "loss": 1.9849,
      "step": 41800
    },
    {
      "epoch": 3.200672217554045,
      "grad_norm": 6.222262859344482,
      "learning_rate": 4.7332773152038295e-05,
      "loss": 1.985,
      "step": 41900
    },
    {
      "epoch": 3.2083110533954624,
      "grad_norm": 7.17245626449585,
      "learning_rate": 4.7326407455503786e-05,
      "loss": 2.0292,
      "step": 42000
    },
    {
      "epoch": 3.2159498892368803,
      "grad_norm": 6.926325798034668,
      "learning_rate": 4.732004175896927e-05,
      "loss": 1.9187,
      "step": 42100
    },
    {
      "epoch": 3.223588725078298,
      "grad_norm": 4.941972255706787,
      "learning_rate": 4.731367606243475e-05,
      "loss": 2.0928,
      "step": 42200
    },
    {
      "epoch": 3.2312275609197156,
      "grad_norm": 5.363863468170166,
      "learning_rate": 4.7307310365900236e-05,
      "loss": 2.0439,
      "step": 42300
    },
    {
      "epoch": 3.2388663967611335,
      "grad_norm": 4.903682231903076,
      "learning_rate": 4.730094466936573e-05,
      "loss": 2.0426,
      "step": 42400
    },
    {
      "epoch": 3.2465052326025514,
      "grad_norm": 4.857275009155273,
      "learning_rate": 4.729457897283121e-05,
      "loss": 2.0729,
      "step": 42500
    },
    {
      "epoch": 3.2541440684439693,
      "grad_norm": 6.666849136352539,
      "learning_rate": 4.7288213276296694e-05,
      "loss": 1.931,
      "step": 42600
    },
    {
      "epoch": 3.2617829042853868,
      "grad_norm": 5.084465503692627,
      "learning_rate": 4.728184757976218e-05,
      "loss": 2.0547,
      "step": 42700
    },
    {
      "epoch": 3.2694217401268046,
      "grad_norm": 7.100191593170166,
      "learning_rate": 4.727548188322766e-05,
      "loss": 2.0215,
      "step": 42800
    },
    {
      "epoch": 3.2770605759682225,
      "grad_norm": 6.69305944442749,
      "learning_rate": 4.726911618669315e-05,
      "loss": 1.9749,
      "step": 42900
    },
    {
      "epoch": 3.2846994118096404,
      "grad_norm": 6.8603925704956055,
      "learning_rate": 4.7262750490158635e-05,
      "loss": 2.0229,
      "step": 43000
    },
    {
      "epoch": 3.292338247651058,
      "grad_norm": 4.830540657043457,
      "learning_rate": 4.725638479362412e-05,
      "loss": 2.0083,
      "step": 43100
    },
    {
      "epoch": 3.2999770834924758,
      "grad_norm": 6.379141330718994,
      "learning_rate": 4.72500190970896e-05,
      "loss": 2.0338,
      "step": 43200
    },
    {
      "epoch": 3.3076159193338937,
      "grad_norm": 4.356451511383057,
      "learning_rate": 4.724365340055509e-05,
      "loss": 2.0289,
      "step": 43300
    },
    {
      "epoch": 3.315254755175311,
      "grad_norm": 4.9136857986450195,
      "learning_rate": 4.7237287704020576e-05,
      "loss": 2.0895,
      "step": 43400
    },
    {
      "epoch": 3.322893591016729,
      "grad_norm": 4.277096748352051,
      "learning_rate": 4.723092200748606e-05,
      "loss": 1.9687,
      "step": 43500
    },
    {
      "epoch": 3.330532426858147,
      "grad_norm": 6.533212184906006,
      "learning_rate": 4.722455631095154e-05,
      "loss": 2.019,
      "step": 43600
    },
    {
      "epoch": 3.3381712626995648,
      "grad_norm": 6.22859525680542,
      "learning_rate": 4.7218190614417026e-05,
      "loss": 1.9602,
      "step": 43700
    },
    {
      "epoch": 3.345810098540982,
      "grad_norm": 3.645887851715088,
      "learning_rate": 4.7211824917882517e-05,
      "loss": 2.0555,
      "step": 43800
    },
    {
      "epoch": 3.3534489343824,
      "grad_norm": 5.759725093841553,
      "learning_rate": 4.7205459221348e-05,
      "loss": 2.0545,
      "step": 43900
    },
    {
      "epoch": 3.361087770223818,
      "grad_norm": 4.942416191101074,
      "learning_rate": 4.7199093524813484e-05,
      "loss": 2.0499,
      "step": 44000
    },
    {
      "epoch": 3.368726606065236,
      "grad_norm": 4.43211030960083,
      "learning_rate": 4.7192727828278974e-05,
      "loss": 1.8967,
      "step": 44100
    },
    {
      "epoch": 3.3763654419066533,
      "grad_norm": 5.230855464935303,
      "learning_rate": 4.718636213174446e-05,
      "loss": 2.001,
      "step": 44200
    },
    {
      "epoch": 3.384004277748071,
      "grad_norm": 5.503210067749023,
      "learning_rate": 4.717999643520995e-05,
      "loss": 2.0419,
      "step": 44300
    },
    {
      "epoch": 3.391643113589489,
      "grad_norm": 4.483783721923828,
      "learning_rate": 4.717363073867543e-05,
      "loss": 2.0752,
      "step": 44400
    },
    {
      "epoch": 3.3992819494309066,
      "grad_norm": 5.690219879150391,
      "learning_rate": 4.7167265042140915e-05,
      "loss": 2.1148,
      "step": 44500
    },
    {
      "epoch": 3.4069207852723244,
      "grad_norm": 5.363310813903809,
      "learning_rate": 4.71608993456064e-05,
      "loss": 2.0573,
      "step": 44600
    },
    {
      "epoch": 3.4145596211137423,
      "grad_norm": 4.754331588745117,
      "learning_rate": 4.715453364907189e-05,
      "loss": 2.0236,
      "step": 44700
    },
    {
      "epoch": 3.42219845695516,
      "grad_norm": 5.659271717071533,
      "learning_rate": 4.714816795253737e-05,
      "loss": 2.0388,
      "step": 44800
    },
    {
      "epoch": 3.4298372927965777,
      "grad_norm": 4.505490303039551,
      "learning_rate": 4.7141802256002856e-05,
      "loss": 2.0266,
      "step": 44900
    },
    {
      "epoch": 3.4374761286379956,
      "grad_norm": 5.492023944854736,
      "learning_rate": 4.713543655946834e-05,
      "loss": 2.0895,
      "step": 45000
    },
    {
      "epoch": 3.4451149644794135,
      "grad_norm": 6.5337700843811035,
      "learning_rate": 4.712907086293382e-05,
      "loss": 2.077,
      "step": 45100
    },
    {
      "epoch": 3.4527538003208313,
      "grad_norm": 4.201380729675293,
      "learning_rate": 4.712270516639931e-05,
      "loss": 2.1013,
      "step": 45200
    },
    {
      "epoch": 3.460392636162249,
      "grad_norm": 4.292972564697266,
      "learning_rate": 4.71163394698648e-05,
      "loss": 2.0033,
      "step": 45300
    },
    {
      "epoch": 3.4680314720036667,
      "grad_norm": 6.537202835083008,
      "learning_rate": 4.710997377333028e-05,
      "loss": 2.0022,
      "step": 45400
    },
    {
      "epoch": 3.4756703078450846,
      "grad_norm": 5.648343563079834,
      "learning_rate": 4.7103608076795764e-05,
      "loss": 2.0766,
      "step": 45500
    },
    {
      "epoch": 3.483309143686502,
      "grad_norm": 3.9729013442993164,
      "learning_rate": 4.7097242380261254e-05,
      "loss": 2.1069,
      "step": 45600
    },
    {
      "epoch": 3.49094797952792,
      "grad_norm": 4.85319709777832,
      "learning_rate": 4.709087668372674e-05,
      "loss": 2.039,
      "step": 45700
    },
    {
      "epoch": 3.498586815369338,
      "grad_norm": 7.199388027191162,
      "learning_rate": 4.708451098719222e-05,
      "loss": 1.9975,
      "step": 45800
    },
    {
      "epoch": 3.5062256512107552,
      "grad_norm": 6.2629828453063965,
      "learning_rate": 4.7078145290657705e-05,
      "loss": 1.8664,
      "step": 45900
    },
    {
      "epoch": 3.513864487052173,
      "grad_norm": 5.076056957244873,
      "learning_rate": 4.707177959412319e-05,
      "loss": 1.9562,
      "step": 46000
    },
    {
      "epoch": 3.521503322893591,
      "grad_norm": 6.042113304138184,
      "learning_rate": 4.706541389758868e-05,
      "loss": 2.0108,
      "step": 46100
    },
    {
      "epoch": 3.529142158735009,
      "grad_norm": 6.344681262969971,
      "learning_rate": 4.705904820105416e-05,
      "loss": 2.0762,
      "step": 46200
    },
    {
      "epoch": 3.536780994576427,
      "grad_norm": 5.069186687469482,
      "learning_rate": 4.7052682504519646e-05,
      "loss": 2.0522,
      "step": 46300
    },
    {
      "epoch": 3.5444198304178443,
      "grad_norm": 4.118061065673828,
      "learning_rate": 4.704631680798513e-05,
      "loss": 2.0448,
      "step": 46400
    },
    {
      "epoch": 3.552058666259262,
      "grad_norm": 4.52597713470459,
      "learning_rate": 4.703995111145062e-05,
      "loss": 1.9035,
      "step": 46500
    },
    {
      "epoch": 3.55969750210068,
      "grad_norm": 4.247780799865723,
      "learning_rate": 4.70335854149161e-05,
      "loss": 1.971,
      "step": 46600
    },
    {
      "epoch": 3.5673363379420975,
      "grad_norm": 5.884444236755371,
      "learning_rate": 4.7027219718381587e-05,
      "loss": 1.9424,
      "step": 46700
    },
    {
      "epoch": 3.5749751737835154,
      "grad_norm": 6.794717311859131,
      "learning_rate": 4.702085402184707e-05,
      "loss": 2.0194,
      "step": 46800
    },
    {
      "epoch": 3.5826140096249333,
      "grad_norm": 5.13367223739624,
      "learning_rate": 4.7014488325312554e-05,
      "loss": 2.0346,
      "step": 46900
    },
    {
      "epoch": 3.5902528454663507,
      "grad_norm": 4.690126895904541,
      "learning_rate": 4.7008122628778044e-05,
      "loss": 2.1053,
      "step": 47000
    },
    {
      "epoch": 3.5978916813077686,
      "grad_norm": 4.214417457580566,
      "learning_rate": 4.700175693224353e-05,
      "loss": 2.0128,
      "step": 47100
    },
    {
      "epoch": 3.6055305171491865,
      "grad_norm": 5.6644086837768555,
      "learning_rate": 4.699539123570901e-05,
      "loss": 1.9833,
      "step": 47200
    },
    {
      "epoch": 3.6131693529906044,
      "grad_norm": 5.890137672424316,
      "learning_rate": 4.6989025539174495e-05,
      "loss": 2.0551,
      "step": 47300
    },
    {
      "epoch": 3.6208081888320223,
      "grad_norm": 7.127089977264404,
      "learning_rate": 4.6982659842639985e-05,
      "loss": 2.0127,
      "step": 47400
    },
    {
      "epoch": 3.6284470246734397,
      "grad_norm": 5.129510879516602,
      "learning_rate": 4.697629414610547e-05,
      "loss": 1.9963,
      "step": 47500
    },
    {
      "epoch": 3.6360858605148576,
      "grad_norm": 4.292200088500977,
      "learning_rate": 4.696992844957095e-05,
      "loss": 2.0177,
      "step": 47600
    },
    {
      "epoch": 3.6437246963562755,
      "grad_norm": 6.82318639755249,
      "learning_rate": 4.6963562753036435e-05,
      "loss": 1.9799,
      "step": 47700
    },
    {
      "epoch": 3.651363532197693,
      "grad_norm": 5.414224147796631,
      "learning_rate": 4.6957197056501926e-05,
      "loss": 2.0639,
      "step": 47800
    },
    {
      "epoch": 3.659002368039111,
      "grad_norm": 6.823721885681152,
      "learning_rate": 4.695083135996741e-05,
      "loss": 2.0426,
      "step": 47900
    },
    {
      "epoch": 3.6666412038805287,
      "grad_norm": 5.086898326873779,
      "learning_rate": 4.694446566343289e-05,
      "loss": 2.0559,
      "step": 48000
    },
    {
      "epoch": 3.674280039721946,
      "grad_norm": 6.089254379272461,
      "learning_rate": 4.693809996689838e-05,
      "loss": 2.0366,
      "step": 48100
    },
    {
      "epoch": 3.681918875563364,
      "grad_norm": 5.277979373931885,
      "learning_rate": 4.693173427036387e-05,
      "loss": 2.1064,
      "step": 48200
    },
    {
      "epoch": 3.689557711404782,
      "grad_norm": 5.24579381942749,
      "learning_rate": 4.692536857382935e-05,
      "loss": 2.0082,
      "step": 48300
    },
    {
      "epoch": 3.6971965472462,
      "grad_norm": 4.998936653137207,
      "learning_rate": 4.691900287729484e-05,
      "loss": 2.0655,
      "step": 48400
    },
    {
      "epoch": 3.7048353830876173,
      "grad_norm": 4.973114967346191,
      "learning_rate": 4.6912637180760324e-05,
      "loss": 2.0698,
      "step": 48500
    },
    {
      "epoch": 3.712474218929035,
      "grad_norm": 4.945659160614014,
      "learning_rate": 4.690627148422581e-05,
      "loss": 1.9837,
      "step": 48600
    },
    {
      "epoch": 3.720113054770453,
      "grad_norm": 4.230228424072266,
      "learning_rate": 4.689990578769129e-05,
      "loss": 2.0618,
      "step": 48700
    },
    {
      "epoch": 3.727751890611871,
      "grad_norm": 4.634078502655029,
      "learning_rate": 4.689354009115678e-05,
      "loss": 1.9996,
      "step": 48800
    },
    {
      "epoch": 3.7353907264532884,
      "grad_norm": 4.939635276794434,
      "learning_rate": 4.6887174394622265e-05,
      "loss": 2.0065,
      "step": 48900
    },
    {
      "epoch": 3.7430295622947063,
      "grad_norm": 3.9844164848327637,
      "learning_rate": 4.688080869808775e-05,
      "loss": 2.0877,
      "step": 49000
    },
    {
      "epoch": 3.750668398136124,
      "grad_norm": 4.829095363616943,
      "learning_rate": 4.687444300155323e-05,
      "loss": 1.9613,
      "step": 49100
    },
    {
      "epoch": 3.7583072339775416,
      "grad_norm": 4.53411340713501,
      "learning_rate": 4.6868077305018716e-05,
      "loss": 1.9969,
      "step": 49200
    },
    {
      "epoch": 3.7659460698189595,
      "grad_norm": 6.00852632522583,
      "learning_rate": 4.6861711608484206e-05,
      "loss": 2.0438,
      "step": 49300
    },
    {
      "epoch": 3.7735849056603774,
      "grad_norm": 4.730018138885498,
      "learning_rate": 4.685534591194969e-05,
      "loss": 1.9591,
      "step": 49400
    },
    {
      "epoch": 3.781223741501795,
      "grad_norm": 6.662684917449951,
      "learning_rate": 4.684898021541517e-05,
      "loss": 1.9345,
      "step": 49500
    },
    {
      "epoch": 3.7888625773432127,
      "grad_norm": 6.456526279449463,
      "learning_rate": 4.6842614518880657e-05,
      "loss": 2.0955,
      "step": 49600
    },
    {
      "epoch": 3.7965014131846306,
      "grad_norm": 5.238156318664551,
      "learning_rate": 4.683624882234615e-05,
      "loss": 2.0363,
      "step": 49700
    },
    {
      "epoch": 3.8041402490260485,
      "grad_norm": 6.548165798187256,
      "learning_rate": 4.682988312581163e-05,
      "loss": 1.9782,
      "step": 49800
    },
    {
      "epoch": 3.8117790848674664,
      "grad_norm": 4.221246719360352,
      "learning_rate": 4.6823517429277114e-05,
      "loss": 1.9875,
      "step": 49900
    },
    {
      "epoch": 3.819417920708884,
      "grad_norm": 4.650042533874512,
      "learning_rate": 4.68171517327426e-05,
      "loss": 1.9442,
      "step": 50000
    },
    {
      "epoch": 3.8270567565503018,
      "grad_norm": 4.816494941711426,
      "learning_rate": 4.681078603620808e-05,
      "loss": 1.9474,
      "step": 50100
    },
    {
      "epoch": 3.8346955923917196,
      "grad_norm": 6.769612789154053,
      "learning_rate": 4.680442033967357e-05,
      "loss": 1.9586,
      "step": 50200
    },
    {
      "epoch": 3.842334428233137,
      "grad_norm": 4.5933074951171875,
      "learning_rate": 4.6798054643139055e-05,
      "loss": 1.9871,
      "step": 50300
    },
    {
      "epoch": 3.849973264074555,
      "grad_norm": 3.1836040019989014,
      "learning_rate": 4.679168894660454e-05,
      "loss": 1.9427,
      "step": 50400
    },
    {
      "epoch": 3.857612099915973,
      "grad_norm": 4.239912986755371,
      "learning_rate": 4.678532325007002e-05,
      "loss": 2.0044,
      "step": 50500
    },
    {
      "epoch": 3.8652509357573903,
      "grad_norm": 4.208387851715088,
      "learning_rate": 4.6778957553535505e-05,
      "loss": 1.9598,
      "step": 50600
    },
    {
      "epoch": 3.872889771598808,
      "grad_norm": 4.513430595397949,
      "learning_rate": 4.6772591857000996e-05,
      "loss": 2.0254,
      "step": 50700
    },
    {
      "epoch": 3.880528607440226,
      "grad_norm": 5.529609203338623,
      "learning_rate": 4.676622616046648e-05,
      "loss": 2.0653,
      "step": 50800
    },
    {
      "epoch": 3.888167443281644,
      "grad_norm": 4.370026111602783,
      "learning_rate": 4.675986046393196e-05,
      "loss": 2.0514,
      "step": 50900
    },
    {
      "epoch": 3.895806279123062,
      "grad_norm": 4.222750663757324,
      "learning_rate": 4.6753494767397446e-05,
      "loss": 2.0412,
      "step": 51000
    },
    {
      "epoch": 3.9034451149644793,
      "grad_norm": 7.344798564910889,
      "learning_rate": 4.674712907086294e-05,
      "loss": 1.9799,
      "step": 51100
    },
    {
      "epoch": 3.911083950805897,
      "grad_norm": 5.259044647216797,
      "learning_rate": 4.674076337432842e-05,
      "loss": 1.966,
      "step": 51200
    },
    {
      "epoch": 3.918722786647315,
      "grad_norm": 3.6799917221069336,
      "learning_rate": 4.6734397677793904e-05,
      "loss": 2.0642,
      "step": 51300
    },
    {
      "epoch": 3.9263616224887326,
      "grad_norm": 5.969090461730957,
      "learning_rate": 4.672803198125939e-05,
      "loss": 1.9558,
      "step": 51400
    },
    {
      "epoch": 3.9340004583301504,
      "grad_norm": 5.064054489135742,
      "learning_rate": 4.672166628472488e-05,
      "loss": 2.1197,
      "step": 51500
    },
    {
      "epoch": 3.9416392941715683,
      "grad_norm": 6.3181071281433105,
      "learning_rate": 4.671530058819036e-05,
      "loss": 2.0303,
      "step": 51600
    },
    {
      "epoch": 3.9492781300129858,
      "grad_norm": 5.344417095184326,
      "learning_rate": 4.6708934891655845e-05,
      "loss": 1.8803,
      "step": 51700
    },
    {
      "epoch": 3.9569169658544037,
      "grad_norm": 4.359766960144043,
      "learning_rate": 4.6702569195121335e-05,
      "loss": 1.9698,
      "step": 51800
    },
    {
      "epoch": 3.9645558016958216,
      "grad_norm": 4.679858207702637,
      "learning_rate": 4.669620349858682e-05,
      "loss": 2.0283,
      "step": 51900
    },
    {
      "epoch": 3.9721946375372394,
      "grad_norm": 4.576117992401123,
      "learning_rate": 4.66898378020523e-05,
      "loss": 2.0621,
      "step": 52000
    },
    {
      "epoch": 3.9798334733786573,
      "grad_norm": 5.5069661140441895,
      "learning_rate": 4.668347210551779e-05,
      "loss": 2.0308,
      "step": 52100
    },
    {
      "epoch": 3.987472309220075,
      "grad_norm": 4.196009635925293,
      "learning_rate": 4.6677106408983276e-05,
      "loss": 1.9756,
      "step": 52200
    },
    {
      "epoch": 3.9951111450614927,
      "grad_norm": 4.011051654815674,
      "learning_rate": 4.667074071244876e-05,
      "loss": 2.0461,
      "step": 52300
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9394561052322388,
      "eval_runtime": 3.0358,
      "eval_samples_per_second": 227.291,
      "eval_steps_per_second": 227.291,
      "step": 52364
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.7849482297897339,
      "eval_runtime": 57.072,
      "eval_samples_per_second": 229.377,
      "eval_steps_per_second": 229.377,
      "step": 52364
    },
    {
      "epoch": 4.002749980902911,
      "grad_norm": 5.968033313751221,
      "learning_rate": 4.666437501591424e-05,
      "loss": 1.9884,
      "step": 52400
    },
    {
      "epoch": 4.010388816744328,
      "grad_norm": 7.054060935974121,
      "learning_rate": 4.665800931937973e-05,
      "loss": 2.0565,
      "step": 52500
    },
    {
      "epoch": 4.018027652585746,
      "grad_norm": 5.600791931152344,
      "learning_rate": 4.665164362284522e-05,
      "loss": 1.9513,
      "step": 52600
    },
    {
      "epoch": 4.025666488427164,
      "grad_norm": 5.166355133056641,
      "learning_rate": 4.66452779263107e-05,
      "loss": 1.971,
      "step": 52700
    },
    {
      "epoch": 4.033305324268581,
      "grad_norm": 4.879295825958252,
      "learning_rate": 4.6638912229776184e-05,
      "loss": 1.9426,
      "step": 52800
    },
    {
      "epoch": 4.04094416011,
      "grad_norm": 5.286499500274658,
      "learning_rate": 4.6632546533241674e-05,
      "loss": 2.0056,
      "step": 52900
    },
    {
      "epoch": 4.048582995951417,
      "grad_norm": 5.955382823944092,
      "learning_rate": 4.662618083670716e-05,
      "loss": 1.9771,
      "step": 53000
    },
    {
      "epoch": 4.0562218317928345,
      "grad_norm": 4.8634843826293945,
      "learning_rate": 4.661981514017264e-05,
      "loss": 1.9652,
      "step": 53100
    },
    {
      "epoch": 4.063860667634253,
      "grad_norm": 5.226178169250488,
      "learning_rate": 4.6613449443638125e-05,
      "loss": 1.9844,
      "step": 53200
    },
    {
      "epoch": 4.07149950347567,
      "grad_norm": 4.338829517364502,
      "learning_rate": 4.660708374710361e-05,
      "loss": 2.0236,
      "step": 53300
    },
    {
      "epoch": 4.079138339317088,
      "grad_norm": 5.355194091796875,
      "learning_rate": 4.66007180505691e-05,
      "loss": 2.0338,
      "step": 53400
    },
    {
      "epoch": 4.086777175158506,
      "grad_norm": 6.502950191497803,
      "learning_rate": 4.659435235403458e-05,
      "loss": 2.0164,
      "step": 53500
    },
    {
      "epoch": 4.0944160109999235,
      "grad_norm": 5.423729419708252,
      "learning_rate": 4.6587986657500066e-05,
      "loss": 1.9264,
      "step": 53600
    },
    {
      "epoch": 4.102054846841342,
      "grad_norm": 5.438634872436523,
      "learning_rate": 4.658162096096555e-05,
      "loss": 2.0169,
      "step": 53700
    },
    {
      "epoch": 4.109693682682759,
      "grad_norm": 4.417952060699463,
      "learning_rate": 4.657525526443103e-05,
      "loss": 2.0047,
      "step": 53800
    },
    {
      "epoch": 4.117332518524177,
      "grad_norm": 5.2608489990234375,
      "learning_rate": 4.656888956789652e-05,
      "loss": 2.0877,
      "step": 53900
    },
    {
      "epoch": 4.124971354365595,
      "grad_norm": 5.583528995513916,
      "learning_rate": 4.656252387136201e-05,
      "loss": 2.073,
      "step": 54000
    },
    {
      "epoch": 4.1326101902070125,
      "grad_norm": 4.132389068603516,
      "learning_rate": 4.655615817482749e-05,
      "loss": 1.9521,
      "step": 54100
    },
    {
      "epoch": 4.14024902604843,
      "grad_norm": 6.9280853271484375,
      "learning_rate": 4.6549792478292974e-05,
      "loss": 1.9756,
      "step": 54200
    },
    {
      "epoch": 4.147887861889848,
      "grad_norm": 4.941770076751709,
      "learning_rate": 4.6543426781758464e-05,
      "loss": 1.9286,
      "step": 54300
    },
    {
      "epoch": 4.155526697731266,
      "grad_norm": 6.611552715301514,
      "learning_rate": 4.653706108522395e-05,
      "loss": 1.9628,
      "step": 54400
    },
    {
      "epoch": 4.163165533572683,
      "grad_norm": 6.190732955932617,
      "learning_rate": 4.653069538868943e-05,
      "loss": 1.911,
      "step": 54500
    },
    {
      "epoch": 4.1708043694141015,
      "grad_norm": 4.024224758148193,
      "learning_rate": 4.6524329692154915e-05,
      "loss": 1.9925,
      "step": 54600
    },
    {
      "epoch": 4.178443205255519,
      "grad_norm": 4.375622272491455,
      "learning_rate": 4.65179639956204e-05,
      "loss": 1.894,
      "step": 54700
    },
    {
      "epoch": 4.186082041096936,
      "grad_norm": 5.078554153442383,
      "learning_rate": 4.651159829908589e-05,
      "loss": 1.9658,
      "step": 54800
    },
    {
      "epoch": 4.193720876938355,
      "grad_norm": 4.605544090270996,
      "learning_rate": 4.650523260255137e-05,
      "loss": 1.9603,
      "step": 54900
    },
    {
      "epoch": 4.201359712779772,
      "grad_norm": 4.555361270904541,
      "learning_rate": 4.6498866906016856e-05,
      "loss": 1.9499,
      "step": 55000
    },
    {
      "epoch": 4.2089985486211905,
      "grad_norm": 6.414746284484863,
      "learning_rate": 4.649250120948234e-05,
      "loss": 2.0738,
      "step": 55100
    },
    {
      "epoch": 4.216637384462608,
      "grad_norm": 5.1826677322387695,
      "learning_rate": 4.648613551294783e-05,
      "loss": 1.9639,
      "step": 55200
    },
    {
      "epoch": 4.224276220304025,
      "grad_norm": 5.959232330322266,
      "learning_rate": 4.647976981641331e-05,
      "loss": 2.0122,
      "step": 55300
    },
    {
      "epoch": 4.231915056145444,
      "grad_norm": 4.505661964416504,
      "learning_rate": 4.6473404119878797e-05,
      "loss": 2.0443,
      "step": 55400
    },
    {
      "epoch": 4.239553891986861,
      "grad_norm": 3.938676595687866,
      "learning_rate": 4.646703842334429e-05,
      "loss": 2.0405,
      "step": 55500
    },
    {
      "epoch": 4.247192727828279,
      "grad_norm": 5.607074737548828,
      "learning_rate": 4.646067272680977e-05,
      "loss": 2.0118,
      "step": 55600
    },
    {
      "epoch": 4.254831563669697,
      "grad_norm": 5.613372325897217,
      "learning_rate": 4.6454307030275254e-05,
      "loss": 1.9073,
      "step": 55700
    },
    {
      "epoch": 4.262470399511114,
      "grad_norm": 5.298267841339111,
      "learning_rate": 4.6447941333740744e-05,
      "loss": 1.9277,
      "step": 55800
    },
    {
      "epoch": 4.270109235352532,
      "grad_norm": 4.565837383270264,
      "learning_rate": 4.644157563720623e-05,
      "loss": 2.0176,
      "step": 55900
    },
    {
      "epoch": 4.27774807119395,
      "grad_norm": 6.341546535491943,
      "learning_rate": 4.643520994067171e-05,
      "loss": 1.9266,
      "step": 56000
    },
    {
      "epoch": 4.285386907035368,
      "grad_norm": 6.417647361755371,
      "learning_rate": 4.64288442441372e-05,
      "loss": 2.0479,
      "step": 56100
    },
    {
      "epoch": 4.293025742876786,
      "grad_norm": 6.20062255859375,
      "learning_rate": 4.6422478547602685e-05,
      "loss": 2.1239,
      "step": 56200
    },
    {
      "epoch": 4.300664578718203,
      "grad_norm": 3.6899893283843994,
      "learning_rate": 4.641611285106817e-05,
      "loss": 2.0268,
      "step": 56300
    },
    {
      "epoch": 4.308303414559621,
      "grad_norm": 5.1761908531188965,
      "learning_rate": 4.640974715453365e-05,
      "loss": 1.984,
      "step": 56400
    },
    {
      "epoch": 4.315942250401039,
      "grad_norm": 6.955145359039307,
      "learning_rate": 4.6403381457999136e-05,
      "loss": 2.035,
      "step": 56500
    },
    {
      "epoch": 4.323581086242457,
      "grad_norm": 4.993860244750977,
      "learning_rate": 4.6397015761464626e-05,
      "loss": 1.9003,
      "step": 56600
    },
    {
      "epoch": 4.331219922083874,
      "grad_norm": 5.303086757659912,
      "learning_rate": 4.639065006493011e-05,
      "loss": 1.9433,
      "step": 56700
    },
    {
      "epoch": 4.338858757925292,
      "grad_norm": 3.908095598220825,
      "learning_rate": 4.638428436839559e-05,
      "loss": 1.8941,
      "step": 56800
    },
    {
      "epoch": 4.34649759376671,
      "grad_norm": 5.210081100463867,
      "learning_rate": 4.637791867186108e-05,
      "loss": 2.042,
      "step": 56900
    },
    {
      "epoch": 4.354136429608127,
      "grad_norm": 5.154153823852539,
      "learning_rate": 4.637155297532656e-05,
      "loss": 2.04,
      "step": 57000
    },
    {
      "epoch": 4.361775265449546,
      "grad_norm": 5.123912334442139,
      "learning_rate": 4.636518727879205e-05,
      "loss": 2.0043,
      "step": 57100
    },
    {
      "epoch": 4.369414101290963,
      "grad_norm": 6.965680122375488,
      "learning_rate": 4.6358821582257534e-05,
      "loss": 2.0313,
      "step": 57200
    },
    {
      "epoch": 4.377052937132381,
      "grad_norm": 5.040268898010254,
      "learning_rate": 4.635245588572302e-05,
      "loss": 1.9983,
      "step": 57300
    },
    {
      "epoch": 4.384691772973799,
      "grad_norm": 3.7932517528533936,
      "learning_rate": 4.63460901891885e-05,
      "loss": 1.9932,
      "step": 57400
    },
    {
      "epoch": 4.392330608815216,
      "grad_norm": 3.825294256210327,
      "learning_rate": 4.633972449265399e-05,
      "loss": 2.0002,
      "step": 57500
    },
    {
      "epoch": 4.399969444656635,
      "grad_norm": 5.374707221984863,
      "learning_rate": 4.6333358796119475e-05,
      "loss": 1.995,
      "step": 57600
    },
    {
      "epoch": 4.407608280498052,
      "grad_norm": 4.59088659286499,
      "learning_rate": 4.632699309958496e-05,
      "loss": 1.9639,
      "step": 57700
    },
    {
      "epoch": 4.4152471163394695,
      "grad_norm": 5.084624290466309,
      "learning_rate": 4.632062740305044e-05,
      "loss": 1.9342,
      "step": 57800
    },
    {
      "epoch": 4.422885952180888,
      "grad_norm": 3.7594170570373535,
      "learning_rate": 4.6314261706515926e-05,
      "loss": 2.1288,
      "step": 57900
    },
    {
      "epoch": 4.430524788022305,
      "grad_norm": 6.3802056312561035,
      "learning_rate": 4.6307896009981416e-05,
      "loss": 1.9768,
      "step": 58000
    },
    {
      "epoch": 4.438163623863723,
      "grad_norm": 5.944986343383789,
      "learning_rate": 4.63015303134469e-05,
      "loss": 2.0226,
      "step": 58100
    },
    {
      "epoch": 4.445802459705141,
      "grad_norm": 5.617832183837891,
      "learning_rate": 4.629516461691238e-05,
      "loss": 1.9474,
      "step": 58200
    },
    {
      "epoch": 4.4534412955465585,
      "grad_norm": 4.800806045532227,
      "learning_rate": 4.6288798920377867e-05,
      "loss": 2.032,
      "step": 58300
    },
    {
      "epoch": 4.461080131387977,
      "grad_norm": 4.38847541809082,
      "learning_rate": 4.628243322384336e-05,
      "loss": 1.9484,
      "step": 58400
    },
    {
      "epoch": 4.468718967229394,
      "grad_norm": 8.25306224822998,
      "learning_rate": 4.627606752730884e-05,
      "loss": 1.9083,
      "step": 58500
    },
    {
      "epoch": 4.476357803070812,
      "grad_norm": 6.641019821166992,
      "learning_rate": 4.6269701830774324e-05,
      "loss": 1.9959,
      "step": 58600
    },
    {
      "epoch": 4.48399663891223,
      "grad_norm": 4.589031219482422,
      "learning_rate": 4.626333613423981e-05,
      "loss": 1.8212,
      "step": 58700
    },
    {
      "epoch": 4.4916354747536476,
      "grad_norm": 10.745789527893066,
      "learning_rate": 4.625697043770529e-05,
      "loss": 1.9517,
      "step": 58800
    },
    {
      "epoch": 4.499274310595065,
      "grad_norm": 6.371495246887207,
      "learning_rate": 4.625060474117078e-05,
      "loss": 1.9699,
      "step": 58900
    },
    {
      "epoch": 4.506913146436483,
      "grad_norm": 4.918023586273193,
      "learning_rate": 4.6244239044636265e-05,
      "loss": 2.0231,
      "step": 59000
    },
    {
      "epoch": 4.514551982277901,
      "grad_norm": 4.435523986816406,
      "learning_rate": 4.623787334810175e-05,
      "loss": 2.0127,
      "step": 59100
    },
    {
      "epoch": 4.522190818119318,
      "grad_norm": 5.766720771789551,
      "learning_rate": 4.623150765156723e-05,
      "loss": 1.9302,
      "step": 59200
    },
    {
      "epoch": 4.529829653960737,
      "grad_norm": 4.266794204711914,
      "learning_rate": 4.622514195503272e-05,
      "loss": 1.9793,
      "step": 59300
    },
    {
      "epoch": 4.537468489802154,
      "grad_norm": 5.347245693206787,
      "learning_rate": 4.6218776258498206e-05,
      "loss": 2.0052,
      "step": 59400
    },
    {
      "epoch": 4.545107325643572,
      "grad_norm": 5.425571918487549,
      "learning_rate": 4.6212410561963696e-05,
      "loss": 1.9339,
      "step": 59500
    },
    {
      "epoch": 4.55274616148499,
      "grad_norm": 5.835730075836182,
      "learning_rate": 4.620604486542918e-05,
      "loss": 1.9614,
      "step": 59600
    },
    {
      "epoch": 4.560384997326407,
      "grad_norm": 6.820795059204102,
      "learning_rate": 4.619967916889466e-05,
      "loss": 1.9292,
      "step": 59700
    },
    {
      "epoch": 4.568023833167826,
      "grad_norm": 5.6686577796936035,
      "learning_rate": 4.6193313472360153e-05,
      "loss": 2.0331,
      "step": 59800
    },
    {
      "epoch": 4.575662669009243,
      "grad_norm": 5.918038845062256,
      "learning_rate": 4.618694777582564e-05,
      "loss": 1.9312,
      "step": 59900
    },
    {
      "epoch": 4.5833015048506605,
      "grad_norm": 6.079443454742432,
      "learning_rate": 4.618058207929112e-05,
      "loss": 2.0031,
      "step": 60000
    },
    {
      "epoch": 4.590940340692079,
      "grad_norm": 5.707193851470947,
      "learning_rate": 4.6174216382756604e-05,
      "loss": 2.0667,
      "step": 60100
    },
    {
      "epoch": 4.598579176533496,
      "grad_norm": 4.819544792175293,
      "learning_rate": 4.616785068622209e-05,
      "loss": 1.8487,
      "step": 60200
    },
    {
      "epoch": 4.606218012374914,
      "grad_norm": 5.257479667663574,
      "learning_rate": 4.616148498968758e-05,
      "loss": 1.9049,
      "step": 60300
    },
    {
      "epoch": 4.613856848216332,
      "grad_norm": 5.6171417236328125,
      "learning_rate": 4.615511929315306e-05,
      "loss": 1.9475,
      "step": 60400
    },
    {
      "epoch": 4.6214956840577495,
      "grad_norm": 5.813770294189453,
      "learning_rate": 4.6148753596618545e-05,
      "loss": 1.9915,
      "step": 60500
    },
    {
      "epoch": 4.629134519899168,
      "grad_norm": 3.7017946243286133,
      "learning_rate": 4.614238790008403e-05,
      "loss": 1.9217,
      "step": 60600
    },
    {
      "epoch": 4.636773355740585,
      "grad_norm": 4.644604206085205,
      "learning_rate": 4.613602220354952e-05,
      "loss": 1.8863,
      "step": 60700
    },
    {
      "epoch": 4.644412191582003,
      "grad_norm": 6.8807053565979,
      "learning_rate": 4.6129656507015e-05,
      "loss": 1.8832,
      "step": 60800
    },
    {
      "epoch": 4.652051027423421,
      "grad_norm": 4.899267673492432,
      "learning_rate": 4.6123290810480486e-05,
      "loss": 2.0201,
      "step": 60900
    },
    {
      "epoch": 4.6596898632648385,
      "grad_norm": 3.996875286102295,
      "learning_rate": 4.611692511394597e-05,
      "loss": 2.0484,
      "step": 61000
    },
    {
      "epoch": 4.667328699106256,
      "grad_norm": 5.476218223571777,
      "learning_rate": 4.611055941741145e-05,
      "loss": 2.0731,
      "step": 61100
    },
    {
      "epoch": 4.674967534947674,
      "grad_norm": 4.5578083992004395,
      "learning_rate": 4.610419372087694e-05,
      "loss": 1.9276,
      "step": 61200
    },
    {
      "epoch": 4.682606370789092,
      "grad_norm": 4.542959213256836,
      "learning_rate": 4.609782802434243e-05,
      "loss": 2.0011,
      "step": 61300
    },
    {
      "epoch": 4.690245206630509,
      "grad_norm": 4.646629333496094,
      "learning_rate": 4.609146232780791e-05,
      "loss": 1.9862,
      "step": 61400
    },
    {
      "epoch": 4.6978840424719275,
      "grad_norm": 3.342839002609253,
      "learning_rate": 4.6085096631273394e-05,
      "loss": 1.9154,
      "step": 61500
    },
    {
      "epoch": 4.705522878313345,
      "grad_norm": 6.490499496459961,
      "learning_rate": 4.6078730934738884e-05,
      "loss": 1.9746,
      "step": 61600
    },
    {
      "epoch": 4.713161714154763,
      "grad_norm": 5.257203578948975,
      "learning_rate": 4.607236523820437e-05,
      "loss": 1.9506,
      "step": 61700
    },
    {
      "epoch": 4.720800549996181,
      "grad_norm": 5.024448394775391,
      "learning_rate": 4.606599954166985e-05,
      "loss": 1.9206,
      "step": 61800
    },
    {
      "epoch": 4.728439385837598,
      "grad_norm": 5.991445064544678,
      "learning_rate": 4.6059633845135335e-05,
      "loss": 1.9803,
      "step": 61900
    },
    {
      "epoch": 4.7360782216790165,
      "grad_norm": 7.969479560852051,
      "learning_rate": 4.605326814860082e-05,
      "loss": 1.9792,
      "step": 62000
    },
    {
      "epoch": 4.743717057520434,
      "grad_norm": 6.069757461547852,
      "learning_rate": 4.604690245206631e-05,
      "loss": 2.0104,
      "step": 62100
    },
    {
      "epoch": 4.751355893361851,
      "grad_norm": 5.458887100219727,
      "learning_rate": 4.604053675553179e-05,
      "loss": 1.9505,
      "step": 62200
    },
    {
      "epoch": 4.75899472920327,
      "grad_norm": 4.88452672958374,
      "learning_rate": 4.6034171058997276e-05,
      "loss": 2.0568,
      "step": 62300
    },
    {
      "epoch": 4.766633565044687,
      "grad_norm": 4.410565376281738,
      "learning_rate": 4.602780536246276e-05,
      "loss": 1.914,
      "step": 62400
    },
    {
      "epoch": 4.774272400886105,
      "grad_norm": 4.190004348754883,
      "learning_rate": 4.602143966592824e-05,
      "loss": 1.9409,
      "step": 62500
    },
    {
      "epoch": 4.781911236727523,
      "grad_norm": 5.347715377807617,
      "learning_rate": 4.601507396939373e-05,
      "loss": 1.9586,
      "step": 62600
    },
    {
      "epoch": 4.78955007256894,
      "grad_norm": 4.3668131828308105,
      "learning_rate": 4.600870827285922e-05,
      "loss": 1.8453,
      "step": 62700
    },
    {
      "epoch": 4.797188908410359,
      "grad_norm": 6.341158390045166,
      "learning_rate": 4.60023425763247e-05,
      "loss": 1.962,
      "step": 62800
    },
    {
      "epoch": 4.804827744251776,
      "grad_norm": 3.948110342025757,
      "learning_rate": 4.5995976879790184e-05,
      "loss": 1.8645,
      "step": 62900
    },
    {
      "epoch": 4.812466580093194,
      "grad_norm": 4.195403099060059,
      "learning_rate": 4.5989611183255674e-05,
      "loss": 2.0641,
      "step": 63000
    },
    {
      "epoch": 4.820105415934612,
      "grad_norm": 4.517614364624023,
      "learning_rate": 4.598324548672116e-05,
      "loss": 2.0867,
      "step": 63100
    },
    {
      "epoch": 4.827744251776029,
      "grad_norm": 5.445192813873291,
      "learning_rate": 4.597687979018664e-05,
      "loss": 1.9522,
      "step": 63200
    },
    {
      "epoch": 4.835383087617447,
      "grad_norm": 6.436577320098877,
      "learning_rate": 4.597051409365213e-05,
      "loss": 2.0056,
      "step": 63300
    },
    {
      "epoch": 4.843021923458865,
      "grad_norm": 6.167810916900635,
      "learning_rate": 4.5964148397117615e-05,
      "loss": 2.1072,
      "step": 63400
    },
    {
      "epoch": 4.850660759300283,
      "grad_norm": 8.345593452453613,
      "learning_rate": 4.59577827005831e-05,
      "loss": 2.0202,
      "step": 63500
    },
    {
      "epoch": 4.8582995951417,
      "grad_norm": 5.288193702697754,
      "learning_rate": 4.595141700404859e-05,
      "loss": 1.9672,
      "step": 63600
    },
    {
      "epoch": 4.865938430983118,
      "grad_norm": 6.5141425132751465,
      "learning_rate": 4.594505130751407e-05,
      "loss": 2.0216,
      "step": 63700
    },
    {
      "epoch": 4.873577266824536,
      "grad_norm": 4.371536731719971,
      "learning_rate": 4.5938685610979556e-05,
      "loss": 2.0604,
      "step": 63800
    },
    {
      "epoch": 4.881216102665954,
      "grad_norm": 5.45941162109375,
      "learning_rate": 4.5932319914445046e-05,
      "loss": 1.9635,
      "step": 63900
    },
    {
      "epoch": 4.888854938507372,
      "grad_norm": 6.135395050048828,
      "learning_rate": 4.592595421791053e-05,
      "loss": 2.03,
      "step": 64000
    },
    {
      "epoch": 4.896493774348789,
      "grad_norm": 5.999543190002441,
      "learning_rate": 4.591958852137601e-05,
      "loss": 2.027,
      "step": 64100
    },
    {
      "epoch": 4.9041326101902065,
      "grad_norm": 4.519680500030518,
      "learning_rate": 4.59132228248415e-05,
      "loss": 1.9429,
      "step": 64200
    },
    {
      "epoch": 4.911771446031625,
      "grad_norm": 5.376014232635498,
      "learning_rate": 4.590685712830698e-05,
      "loss": 2.0703,
      "step": 64300
    },
    {
      "epoch": 4.919410281873042,
      "grad_norm": 5.949662685394287,
      "learning_rate": 4.590049143177247e-05,
      "loss": 1.9921,
      "step": 64400
    },
    {
      "epoch": 4.927049117714461,
      "grad_norm": 4.4464497566223145,
      "learning_rate": 4.5894125735237954e-05,
      "loss": 2.0386,
      "step": 64500
    },
    {
      "epoch": 4.934687953555878,
      "grad_norm": 6.31113862991333,
      "learning_rate": 4.588776003870344e-05,
      "loss": 1.9639,
      "step": 64600
    },
    {
      "epoch": 4.9423267893972955,
      "grad_norm": 4.842045783996582,
      "learning_rate": 4.588139434216892e-05,
      "loss": 1.97,
      "step": 64700
    },
    {
      "epoch": 4.949965625238714,
      "grad_norm": 5.735503196716309,
      "learning_rate": 4.587502864563441e-05,
      "loss": 1.9933,
      "step": 64800
    },
    {
      "epoch": 4.957604461080131,
      "grad_norm": 5.26561975479126,
      "learning_rate": 4.5868662949099895e-05,
      "loss": 2.0812,
      "step": 64900
    },
    {
      "epoch": 4.96524329692155,
      "grad_norm": 5.26536750793457,
      "learning_rate": 4.586229725256538e-05,
      "loss": 1.9652,
      "step": 65000
    },
    {
      "epoch": 4.972882132762967,
      "grad_norm": 4.055658340454102,
      "learning_rate": 4.585593155603086e-05,
      "loss": 1.9782,
      "step": 65100
    },
    {
      "epoch": 4.9805209686043845,
      "grad_norm": 4.685146331787109,
      "learning_rate": 4.5849565859496346e-05,
      "loss": 1.9564,
      "step": 65200
    },
    {
      "epoch": 4.988159804445802,
      "grad_norm": 4.63474702835083,
      "learning_rate": 4.5843200162961836e-05,
      "loss": 1.9549,
      "step": 65300
    },
    {
      "epoch": 4.99579864028722,
      "grad_norm": 8.4915189743042,
      "learning_rate": 4.583683446642732e-05,
      "loss": 2.0123,
      "step": 65400
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.9182157516479492,
      "eval_runtime": 3.0307,
      "eval_samples_per_second": 227.672,
      "eval_steps_per_second": 227.672,
      "step": 65455
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7551203966140747,
      "eval_runtime": 57.4922,
      "eval_samples_per_second": 227.7,
      "eval_steps_per_second": 227.7,
      "step": 65455
    },
    {
      "epoch": 5.003437476128638,
      "grad_norm": 6.137702465057373,
      "learning_rate": 4.58304687698928e-05,
      "loss": 2.0214,
      "step": 65500
    },
    {
      "epoch": 5.011076311970056,
      "grad_norm": 4.753273010253906,
      "learning_rate": 4.582410307335829e-05,
      "loss": 1.9009,
      "step": 65600
    },
    {
      "epoch": 5.0187151478114735,
      "grad_norm": 4.957821369171143,
      "learning_rate": 4.581773737682377e-05,
      "loss": 1.9908,
      "step": 65700
    },
    {
      "epoch": 5.026353983652891,
      "grad_norm": 4.568780422210693,
      "learning_rate": 4.581137168028926e-05,
      "loss": 1.9197,
      "step": 65800
    },
    {
      "epoch": 5.033992819494309,
      "grad_norm": 7.433024883270264,
      "learning_rate": 4.5805005983754744e-05,
      "loss": 1.9642,
      "step": 65900
    },
    {
      "epoch": 5.041631655335727,
      "grad_norm": 5.09379768371582,
      "learning_rate": 4.579864028722023e-05,
      "loss": 2.1104,
      "step": 66000
    },
    {
      "epoch": 5.049270491177144,
      "grad_norm": 4.953510284423828,
      "learning_rate": 4.579227459068571e-05,
      "loss": 1.9214,
      "step": 66100
    },
    {
      "epoch": 5.0569093270185625,
      "grad_norm": 4.331299781799316,
      "learning_rate": 4.57859088941512e-05,
      "loss": 1.9283,
      "step": 66200
    },
    {
      "epoch": 5.06454816285998,
      "grad_norm": 5.812013149261475,
      "learning_rate": 4.5779543197616685e-05,
      "loss": 1.9552,
      "step": 66300
    },
    {
      "epoch": 5.072186998701398,
      "grad_norm": 4.646178722381592,
      "learning_rate": 4.577317750108217e-05,
      "loss": 1.8975,
      "step": 66400
    },
    {
      "epoch": 5.079825834542816,
      "grad_norm": 5.573010444641113,
      "learning_rate": 4.576681180454765e-05,
      "loss": 2.061,
      "step": 66500
    },
    {
      "epoch": 5.087464670384233,
      "grad_norm": 7.208837509155273,
      "learning_rate": 4.5760446108013136e-05,
      "loss": 2.0014,
      "step": 66600
    },
    {
      "epoch": 5.095103506225652,
      "grad_norm": 6.60245943069458,
      "learning_rate": 4.5754080411478626e-05,
      "loss": 1.8657,
      "step": 66700
    },
    {
      "epoch": 5.102742342067069,
      "grad_norm": 5.491392135620117,
      "learning_rate": 4.574771471494411e-05,
      "loss": 2.0405,
      "step": 66800
    },
    {
      "epoch": 5.1103811779084864,
      "grad_norm": 5.405097484588623,
      "learning_rate": 4.574134901840959e-05,
      "loss": 1.946,
      "step": 66900
    },
    {
      "epoch": 5.118020013749905,
      "grad_norm": 4.340205192565918,
      "learning_rate": 4.573498332187508e-05,
      "loss": 1.8688,
      "step": 67000
    },
    {
      "epoch": 5.125658849591322,
      "grad_norm": 6.297792911529541,
      "learning_rate": 4.572861762534057e-05,
      "loss": 1.9069,
      "step": 67100
    },
    {
      "epoch": 5.13329768543274,
      "grad_norm": 4.246395111083984,
      "learning_rate": 4.572225192880605e-05,
      "loss": 1.9784,
      "step": 67200
    },
    {
      "epoch": 5.140936521274158,
      "grad_norm": 5.896659851074219,
      "learning_rate": 4.571588623227154e-05,
      "loss": 1.9865,
      "step": 67300
    },
    {
      "epoch": 5.1485753571155755,
      "grad_norm": 4.17970609664917,
      "learning_rate": 4.5709520535737024e-05,
      "loss": 1.9236,
      "step": 67400
    },
    {
      "epoch": 5.156214192956993,
      "grad_norm": 5.000166416168213,
      "learning_rate": 4.570315483920251e-05,
      "loss": 2.0424,
      "step": 67500
    },
    {
      "epoch": 5.163853028798411,
      "grad_norm": 4.6702880859375,
      "learning_rate": 4.5696789142668e-05,
      "loss": 1.9265,
      "step": 67600
    },
    {
      "epoch": 5.171491864639829,
      "grad_norm": 5.210743427276611,
      "learning_rate": 4.569042344613348e-05,
      "loss": 2.0642,
      "step": 67700
    },
    {
      "epoch": 5.179130700481247,
      "grad_norm": 5.381895065307617,
      "learning_rate": 4.5684057749598965e-05,
      "loss": 2.0521,
      "step": 67800
    },
    {
      "epoch": 5.1867695363226645,
      "grad_norm": 4.628066062927246,
      "learning_rate": 4.567769205306445e-05,
      "loss": 1.9628,
      "step": 67900
    },
    {
      "epoch": 5.194408372164082,
      "grad_norm": 4.6269683837890625,
      "learning_rate": 4.567132635652993e-05,
      "loss": 1.8891,
      "step": 68000
    },
    {
      "epoch": 5.2020472080055,
      "grad_norm": 4.674765586853027,
      "learning_rate": 4.566496065999542e-05,
      "loss": 1.9843,
      "step": 68100
    },
    {
      "epoch": 5.209686043846918,
      "grad_norm": 5.925546646118164,
      "learning_rate": 4.5658594963460906e-05,
      "loss": 1.9646,
      "step": 68200
    },
    {
      "epoch": 5.217324879688335,
      "grad_norm": 4.727754592895508,
      "learning_rate": 4.565222926692639e-05,
      "loss": 1.9732,
      "step": 68300
    },
    {
      "epoch": 5.2249637155297535,
      "grad_norm": 5.026760578155518,
      "learning_rate": 4.564586357039187e-05,
      "loss": 1.989,
      "step": 68400
    },
    {
      "epoch": 5.232602551371171,
      "grad_norm": 5.211971282958984,
      "learning_rate": 4.5639497873857363e-05,
      "loss": 1.9859,
      "step": 68500
    },
    {
      "epoch": 5.240241387212588,
      "grad_norm": 6.910489559173584,
      "learning_rate": 4.563313217732285e-05,
      "loss": 1.9408,
      "step": 68600
    },
    {
      "epoch": 5.247880223054007,
      "grad_norm": 6.91651725769043,
      "learning_rate": 4.562676648078833e-05,
      "loss": 1.8975,
      "step": 68700
    },
    {
      "epoch": 5.255519058895424,
      "grad_norm": 4.443906307220459,
      "learning_rate": 4.5620400784253814e-05,
      "loss": 1.9849,
      "step": 68800
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 5.428321361541748,
      "learning_rate": 4.56140350877193e-05,
      "loss": 1.8847,
      "step": 68900
    },
    {
      "epoch": 5.27079673057826,
      "grad_norm": 5.2514448165893555,
      "learning_rate": 4.560766939118479e-05,
      "loss": 1.9909,
      "step": 69000
    },
    {
      "epoch": 5.278435566419677,
      "grad_norm": 5.702043533325195,
      "learning_rate": 4.560130369465027e-05,
      "loss": 1.9139,
      "step": 69100
    },
    {
      "epoch": 5.286074402261096,
      "grad_norm": 5.4538469314575195,
      "learning_rate": 4.5594937998115755e-05,
      "loss": 1.986,
      "step": 69200
    },
    {
      "epoch": 5.293713238102513,
      "grad_norm": 5.525846481323242,
      "learning_rate": 4.558857230158124e-05,
      "loss": 1.8631,
      "step": 69300
    },
    {
      "epoch": 5.301352073943931,
      "grad_norm": 6.695493698120117,
      "learning_rate": 4.558220660504673e-05,
      "loss": 1.9244,
      "step": 69400
    },
    {
      "epoch": 5.308990909785349,
      "grad_norm": 4.024606704711914,
      "learning_rate": 4.557584090851221e-05,
      "loss": 1.9416,
      "step": 69500
    },
    {
      "epoch": 5.316629745626766,
      "grad_norm": 5.410897254943848,
      "learning_rate": 4.5569475211977696e-05,
      "loss": 1.9311,
      "step": 69600
    },
    {
      "epoch": 5.324268581468184,
      "grad_norm": 7.742481708526611,
      "learning_rate": 4.556310951544318e-05,
      "loss": 1.9801,
      "step": 69700
    },
    {
      "epoch": 5.331907417309602,
      "grad_norm": 4.748840808868408,
      "learning_rate": 4.555674381890866e-05,
      "loss": 1.9291,
      "step": 69800
    },
    {
      "epoch": 5.33954625315102,
      "grad_norm": 6.385909557342529,
      "learning_rate": 4.555037812237415e-05,
      "loss": 1.987,
      "step": 69900
    },
    {
      "epoch": 5.347185088992438,
      "grad_norm": 5.871299743652344,
      "learning_rate": 4.554401242583964e-05,
      "loss": 2.03,
      "step": 70000
    },
    {
      "epoch": 5.354823924833855,
      "grad_norm": 6.5986504554748535,
      "learning_rate": 4.553764672930512e-05,
      "loss": 2.0329,
      "step": 70100
    },
    {
      "epoch": 5.362462760675273,
      "grad_norm": 4.866158485412598,
      "learning_rate": 4.5531281032770604e-05,
      "loss": 1.9489,
      "step": 70200
    },
    {
      "epoch": 5.370101596516691,
      "grad_norm": 4.433085918426514,
      "learning_rate": 4.5524915336236094e-05,
      "loss": 1.9712,
      "step": 70300
    },
    {
      "epoch": 5.377740432358109,
      "grad_norm": 4.234788417816162,
      "learning_rate": 4.551854963970158e-05,
      "loss": 1.9446,
      "step": 70400
    },
    {
      "epoch": 5.385379268199526,
      "grad_norm": 4.877241134643555,
      "learning_rate": 4.551218394316706e-05,
      "loss": 1.9647,
      "step": 70500
    },
    {
      "epoch": 5.393018104040944,
      "grad_norm": 5.693511962890625,
      "learning_rate": 4.5505818246632545e-05,
      "loss": 1.9095,
      "step": 70600
    },
    {
      "epoch": 5.400656939882362,
      "grad_norm": 4.840832710266113,
      "learning_rate": 4.5499452550098035e-05,
      "loss": 1.8799,
      "step": 70700
    },
    {
      "epoch": 5.408295775723779,
      "grad_norm": 5.265841007232666,
      "learning_rate": 4.549308685356352e-05,
      "loss": 1.934,
      "step": 70800
    },
    {
      "epoch": 5.415934611565198,
      "grad_norm": 4.25891637802124,
      "learning_rate": 4.5486721157029e-05,
      "loss": 1.9413,
      "step": 70900
    },
    {
      "epoch": 5.423573447406615,
      "grad_norm": 6.699929237365723,
      "learning_rate": 4.548035546049449e-05,
      "loss": 1.9456,
      "step": 71000
    },
    {
      "epoch": 5.431212283248033,
      "grad_norm": 4.75304651260376,
      "learning_rate": 4.5473989763959976e-05,
      "loss": 1.9512,
      "step": 71100
    },
    {
      "epoch": 5.438851119089451,
      "grad_norm": 6.148152828216553,
      "learning_rate": 4.546762406742546e-05,
      "loss": 1.9624,
      "step": 71200
    },
    {
      "epoch": 5.446489954930868,
      "grad_norm": 3.9012699127197266,
      "learning_rate": 4.546125837089095e-05,
      "loss": 2.0012,
      "step": 71300
    },
    {
      "epoch": 5.454128790772287,
      "grad_norm": 5.309199333190918,
      "learning_rate": 4.5454892674356433e-05,
      "loss": 2.0269,
      "step": 71400
    },
    {
      "epoch": 5.461767626613704,
      "grad_norm": 6.914174556732178,
      "learning_rate": 4.544852697782192e-05,
      "loss": 1.8907,
      "step": 71500
    },
    {
      "epoch": 5.4694064624551215,
      "grad_norm": 6.744208812713623,
      "learning_rate": 4.54421612812874e-05,
      "loss": 1.9424,
      "step": 71600
    },
    {
      "epoch": 5.47704529829654,
      "grad_norm": 4.66702938079834,
      "learning_rate": 4.543579558475289e-05,
      "loss": 1.9437,
      "step": 71700
    },
    {
      "epoch": 5.484684134137957,
      "grad_norm": 8.35803508758545,
      "learning_rate": 4.5429429888218374e-05,
      "loss": 1.9971,
      "step": 71800
    },
    {
      "epoch": 5.492322969979375,
      "grad_norm": 5.199077606201172,
      "learning_rate": 4.542306419168386e-05,
      "loss": 1.9484,
      "step": 71900
    },
    {
      "epoch": 5.499961805820793,
      "grad_norm": 5.661782264709473,
      "learning_rate": 4.541669849514934e-05,
      "loss": 1.9173,
      "step": 72000
    },
    {
      "epoch": 5.5076006416622105,
      "grad_norm": 4.649659156799316,
      "learning_rate": 4.5410332798614825e-05,
      "loss": 1.9204,
      "step": 72100
    },
    {
      "epoch": 5.515239477503629,
      "grad_norm": 6.155493259429932,
      "learning_rate": 4.5403967102080315e-05,
      "loss": 2.0059,
      "step": 72200
    },
    {
      "epoch": 5.522878313345046,
      "grad_norm": 5.007844924926758,
      "learning_rate": 4.53976014055458e-05,
      "loss": 1.8812,
      "step": 72300
    },
    {
      "epoch": 5.530517149186464,
      "grad_norm": 5.220801830291748,
      "learning_rate": 4.539123570901128e-05,
      "loss": 1.9241,
      "step": 72400
    },
    {
      "epoch": 5.538155985027882,
      "grad_norm": 4.875824451446533,
      "learning_rate": 4.5384870012476766e-05,
      "loss": 1.9677,
      "step": 72500
    },
    {
      "epoch": 5.5457948208692995,
      "grad_norm": 6.7142205238342285,
      "learning_rate": 4.5378504315942256e-05,
      "loss": 1.9686,
      "step": 72600
    },
    {
      "epoch": 5.553433656710717,
      "grad_norm": 4.6450324058532715,
      "learning_rate": 4.537213861940774e-05,
      "loss": 2.0025,
      "step": 72700
    },
    {
      "epoch": 5.561072492552135,
      "grad_norm": 4.493157386779785,
      "learning_rate": 4.536577292287322e-05,
      "loss": 1.8157,
      "step": 72800
    },
    {
      "epoch": 5.568711328393553,
      "grad_norm": 5.36734676361084,
      "learning_rate": 4.535940722633871e-05,
      "loss": 1.8603,
      "step": 72900
    },
    {
      "epoch": 5.57635016423497,
      "grad_norm": 4.256871223449707,
      "learning_rate": 4.535304152980419e-05,
      "loss": 2.0113,
      "step": 73000
    },
    {
      "epoch": 5.5839890000763885,
      "grad_norm": 5.686335563659668,
      "learning_rate": 4.534667583326968e-05,
      "loss": 1.9383,
      "step": 73100
    },
    {
      "epoch": 5.591627835917806,
      "grad_norm": 4.419832229614258,
      "learning_rate": 4.5340310136735164e-05,
      "loss": 1.9504,
      "step": 73200
    },
    {
      "epoch": 5.599266671759224,
      "grad_norm": 6.313498020172119,
      "learning_rate": 4.533394444020065e-05,
      "loss": 1.9673,
      "step": 73300
    },
    {
      "epoch": 5.606905507600642,
      "grad_norm": 5.56048059463501,
      "learning_rate": 4.532757874366613e-05,
      "loss": 1.92,
      "step": 73400
    },
    {
      "epoch": 5.614544343442059,
      "grad_norm": 5.982112407684326,
      "learning_rate": 4.532121304713162e-05,
      "loss": 1.9381,
      "step": 73500
    },
    {
      "epoch": 5.6221831792834775,
      "grad_norm": 5.965380668640137,
      "learning_rate": 4.5314847350597105e-05,
      "loss": 1.9128,
      "step": 73600
    },
    {
      "epoch": 5.629822015124895,
      "grad_norm": 4.5175933837890625,
      "learning_rate": 4.530848165406259e-05,
      "loss": 2.0334,
      "step": 73700
    },
    {
      "epoch": 5.637460850966312,
      "grad_norm": 3.9150547981262207,
      "learning_rate": 4.530211595752807e-05,
      "loss": 1.9342,
      "step": 73800
    },
    {
      "epoch": 5.645099686807731,
      "grad_norm": 5.926520824432373,
      "learning_rate": 4.5295750260993556e-05,
      "loss": 1.9091,
      "step": 73900
    },
    {
      "epoch": 5.652738522649148,
      "grad_norm": 7.1926445960998535,
      "learning_rate": 4.5289384564459046e-05,
      "loss": 2.0075,
      "step": 74000
    },
    {
      "epoch": 5.660377358490566,
      "grad_norm": 5.4150166511535645,
      "learning_rate": 4.528301886792453e-05,
      "loss": 1.9182,
      "step": 74100
    },
    {
      "epoch": 5.668016194331984,
      "grad_norm": 4.129126071929932,
      "learning_rate": 4.527665317139001e-05,
      "loss": 1.9827,
      "step": 74200
    },
    {
      "epoch": 5.6756550301734014,
      "grad_norm": 6.892459392547607,
      "learning_rate": 4.52702874748555e-05,
      "loss": 2.0083,
      "step": 74300
    },
    {
      "epoch": 5.68329386601482,
      "grad_norm": 5.517465114593506,
      "learning_rate": 4.526392177832098e-05,
      "loss": 1.966,
      "step": 74400
    },
    {
      "epoch": 5.690932701856237,
      "grad_norm": 4.873973369598389,
      "learning_rate": 4.525755608178647e-05,
      "loss": 1.9927,
      "step": 74500
    },
    {
      "epoch": 5.698571537697655,
      "grad_norm": 4.352402210235596,
      "learning_rate": 4.5251190385251954e-05,
      "loss": 2.0126,
      "step": 74600
    },
    {
      "epoch": 5.706210373539073,
      "grad_norm": 4.469357967376709,
      "learning_rate": 4.524482468871744e-05,
      "loss": 1.9753,
      "step": 74700
    },
    {
      "epoch": 5.7138492093804905,
      "grad_norm": 7.5295562744140625,
      "learning_rate": 4.523845899218293e-05,
      "loss": 1.8737,
      "step": 74800
    },
    {
      "epoch": 5.721488045221908,
      "grad_norm": 5.772067546844482,
      "learning_rate": 4.523209329564841e-05,
      "loss": 2.0063,
      "step": 74900
    },
    {
      "epoch": 5.729126881063326,
      "grad_norm": 4.096925735473633,
      "learning_rate": 4.52257275991139e-05,
      "loss": 1.9087,
      "step": 75000
    },
    {
      "epoch": 5.736765716904744,
      "grad_norm": 7.565279483795166,
      "learning_rate": 4.5219361902579385e-05,
      "loss": 1.9367,
      "step": 75100
    },
    {
      "epoch": 5.744404552746161,
      "grad_norm": 9.746256828308105,
      "learning_rate": 4.521299620604487e-05,
      "loss": 1.9723,
      "step": 75200
    },
    {
      "epoch": 5.7520433885875795,
      "grad_norm": 6.366628646850586,
      "learning_rate": 4.520663050951035e-05,
      "loss": 1.9139,
      "step": 75300
    },
    {
      "epoch": 5.759682224428997,
      "grad_norm": 4.834724426269531,
      "learning_rate": 4.520026481297584e-05,
      "loss": 1.9831,
      "step": 75400
    },
    {
      "epoch": 5.767321060270415,
      "grad_norm": 4.890036582946777,
      "learning_rate": 4.5193899116441326e-05,
      "loss": 1.9392,
      "step": 75500
    },
    {
      "epoch": 5.774959896111833,
      "grad_norm": 5.9210004806518555,
      "learning_rate": 4.518753341990681e-05,
      "loss": 1.9871,
      "step": 75600
    },
    {
      "epoch": 5.78259873195325,
      "grad_norm": 5.184570789337158,
      "learning_rate": 4.518116772337229e-05,
      "loss": 2.0821,
      "step": 75700
    },
    {
      "epoch": 5.7902375677946685,
      "grad_norm": 4.012991428375244,
      "learning_rate": 4.5174802026837784e-05,
      "loss": 1.8902,
      "step": 75800
    },
    {
      "epoch": 5.797876403636086,
      "grad_norm": 3.7665839195251465,
      "learning_rate": 4.516843633030327e-05,
      "loss": 1.8136,
      "step": 75900
    },
    {
      "epoch": 5.805515239477503,
      "grad_norm": 5.176202774047852,
      "learning_rate": 4.516207063376875e-05,
      "loss": 2.0282,
      "step": 76000
    },
    {
      "epoch": 5.813154075318922,
      "grad_norm": 4.583958625793457,
      "learning_rate": 4.5155704937234234e-05,
      "loss": 1.9784,
      "step": 76100
    },
    {
      "epoch": 5.820792911160339,
      "grad_norm": 4.774069786071777,
      "learning_rate": 4.514933924069972e-05,
      "loss": 1.922,
      "step": 76200
    },
    {
      "epoch": 5.828431747001757,
      "grad_norm": 5.541167736053467,
      "learning_rate": 4.514297354416521e-05,
      "loss": 1.9009,
      "step": 76300
    },
    {
      "epoch": 5.836070582843175,
      "grad_norm": 4.450122356414795,
      "learning_rate": 4.513660784763069e-05,
      "loss": 1.9604,
      "step": 76400
    },
    {
      "epoch": 5.843709418684592,
      "grad_norm": 4.056705474853516,
      "learning_rate": 4.5130242151096175e-05,
      "loss": 2.0338,
      "step": 76500
    },
    {
      "epoch": 5.851348254526011,
      "grad_norm": 5.2177324295043945,
      "learning_rate": 4.512387645456166e-05,
      "loss": 1.9823,
      "step": 76600
    },
    {
      "epoch": 5.858987090367428,
      "grad_norm": 3.5976459980010986,
      "learning_rate": 4.511751075802714e-05,
      "loss": 1.9674,
      "step": 76700
    },
    {
      "epoch": 5.866625926208846,
      "grad_norm": 5.894534111022949,
      "learning_rate": 4.511114506149263e-05,
      "loss": 1.9051,
      "step": 76800
    },
    {
      "epoch": 5.874264762050263,
      "grad_norm": 5.045036315917969,
      "learning_rate": 4.5104779364958116e-05,
      "loss": 2.0245,
      "step": 76900
    },
    {
      "epoch": 5.881903597891681,
      "grad_norm": 4.626689910888672,
      "learning_rate": 4.50984136684236e-05,
      "loss": 1.8974,
      "step": 77000
    },
    {
      "epoch": 5.889542433733099,
      "grad_norm": 5.840139389038086,
      "learning_rate": 4.509204797188908e-05,
      "loss": 1.9333,
      "step": 77100
    },
    {
      "epoch": 5.897181269574517,
      "grad_norm": 6.163106441497803,
      "learning_rate": 4.5085682275354573e-05,
      "loss": 1.972,
      "step": 77200
    },
    {
      "epoch": 5.904820105415935,
      "grad_norm": 5.054567813873291,
      "learning_rate": 4.507931657882006e-05,
      "loss": 1.8799,
      "step": 77300
    },
    {
      "epoch": 5.912458941257352,
      "grad_norm": 3.715471029281616,
      "learning_rate": 4.507295088228554e-05,
      "loss": 1.9239,
      "step": 77400
    },
    {
      "epoch": 5.92009777709877,
      "grad_norm": 6.020042419433594,
      "learning_rate": 4.5066585185751024e-05,
      "loss": 1.9973,
      "step": 77500
    },
    {
      "epoch": 5.927736612940188,
      "grad_norm": 4.718035697937012,
      "learning_rate": 4.506021948921651e-05,
      "loss": 1.9184,
      "step": 77600
    },
    {
      "epoch": 5.935375448781606,
      "grad_norm": 6.004112720489502,
      "learning_rate": 4.5053853792682e-05,
      "loss": 1.9164,
      "step": 77700
    },
    {
      "epoch": 5.943014284623024,
      "grad_norm": 4.50485372543335,
      "learning_rate": 4.504748809614748e-05,
      "loss": 1.9029,
      "step": 77800
    },
    {
      "epoch": 5.950653120464441,
      "grad_norm": 5.192349910736084,
      "learning_rate": 4.5041122399612965e-05,
      "loss": 1.973,
      "step": 77900
    },
    {
      "epoch": 5.9582919563058585,
      "grad_norm": 9.964667320251465,
      "learning_rate": 4.503475670307845e-05,
      "loss": 1.9762,
      "step": 78000
    },
    {
      "epoch": 5.965930792147277,
      "grad_norm": 6.773638725280762,
      "learning_rate": 4.502839100654394e-05,
      "loss": 2.0127,
      "step": 78100
    },
    {
      "epoch": 5.973569627988694,
      "grad_norm": 6.9240031242370605,
      "learning_rate": 4.502202531000942e-05,
      "loss": 1.9725,
      "step": 78200
    },
    {
      "epoch": 5.981208463830113,
      "grad_norm": 5.077742576599121,
      "learning_rate": 4.5015659613474906e-05,
      "loss": 1.9704,
      "step": 78300
    },
    {
      "epoch": 5.98884729967153,
      "grad_norm": 6.7643723487854,
      "learning_rate": 4.500929391694039e-05,
      "loss": 1.9033,
      "step": 78400
    },
    {
      "epoch": 5.9964861355129475,
      "grad_norm": 6.140349388122559,
      "learning_rate": 4.500292822040588e-05,
      "loss": 1.9047,
      "step": 78500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.9135161638259888,
      "eval_runtime": 3.015,
      "eval_samples_per_second": 228.857,
      "eval_steps_per_second": 228.857,
      "step": 78546
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.733034610748291,
      "eval_runtime": 57.9731,
      "eval_samples_per_second": 225.812,
      "eval_steps_per_second": 225.812,
      "step": 78546
    },
    {
      "epoch": 6.004124971354366,
      "grad_norm": 5.904068946838379,
      "learning_rate": 4.499656252387136e-05,
      "loss": 1.9149,
      "step": 78600
    },
    {
      "epoch": 6.011763807195783,
      "grad_norm": 6.0260009765625,
      "learning_rate": 4.499019682733685e-05,
      "loss": 1.9242,
      "step": 78700
    },
    {
      "epoch": 6.019402643037201,
      "grad_norm": 4.907913684844971,
      "learning_rate": 4.498383113080234e-05,
      "loss": 1.7993,
      "step": 78800
    },
    {
      "epoch": 6.027041478878619,
      "grad_norm": 5.064624309539795,
      "learning_rate": 4.497746543426782e-05,
      "loss": 1.9019,
      "step": 78900
    },
    {
      "epoch": 6.0346803147200365,
      "grad_norm": 7.9353203773498535,
      "learning_rate": 4.497109973773331e-05,
      "loss": 1.927,
      "step": 79000
    },
    {
      "epoch": 6.042319150561455,
      "grad_norm": 4.443105697631836,
      "learning_rate": 4.4964734041198795e-05,
      "loss": 2.0084,
      "step": 79100
    },
    {
      "epoch": 6.049957986402872,
      "grad_norm": 4.915706634521484,
      "learning_rate": 4.495836834466428e-05,
      "loss": 1.9974,
      "step": 79200
    },
    {
      "epoch": 6.05759682224429,
      "grad_norm": 4.066573619842529,
      "learning_rate": 4.495200264812976e-05,
      "loss": 1.9866,
      "step": 79300
    },
    {
      "epoch": 6.065235658085708,
      "grad_norm": 5.437056541442871,
      "learning_rate": 4.4945636951595245e-05,
      "loss": 1.9389,
      "step": 79400
    },
    {
      "epoch": 6.0728744939271255,
      "grad_norm": 5.100918769836426,
      "learning_rate": 4.4939271255060735e-05,
      "loss": 1.9254,
      "step": 79500
    },
    {
      "epoch": 6.080513329768543,
      "grad_norm": 4.816897392272949,
      "learning_rate": 4.493290555852622e-05,
      "loss": 1.9281,
      "step": 79600
    },
    {
      "epoch": 6.088152165609961,
      "grad_norm": 6.231194019317627,
      "learning_rate": 4.49265398619917e-05,
      "loss": 1.9053,
      "step": 79700
    },
    {
      "epoch": 6.095791001451379,
      "grad_norm": 6.0748820304870605,
      "learning_rate": 4.4920174165457186e-05,
      "loss": 1.9582,
      "step": 79800
    },
    {
      "epoch": 6.103429837292796,
      "grad_norm": 4.4607462882995605,
      "learning_rate": 4.491380846892267e-05,
      "loss": 1.8958,
      "step": 79900
    },
    {
      "epoch": 6.1110686731342145,
      "grad_norm": 5.6619954109191895,
      "learning_rate": 4.490744277238816e-05,
      "loss": 1.999,
      "step": 80000
    },
    {
      "epoch": 6.118707508975632,
      "grad_norm": 3.6705126762390137,
      "learning_rate": 4.4901077075853643e-05,
      "loss": 1.9709,
      "step": 80100
    },
    {
      "epoch": 6.126346344817049,
      "grad_norm": 5.345674991607666,
      "learning_rate": 4.489471137931913e-05,
      "loss": 1.9225,
      "step": 80200
    },
    {
      "epoch": 6.133985180658468,
      "grad_norm": 5.591727256774902,
      "learning_rate": 4.488834568278461e-05,
      "loss": 1.8223,
      "step": 80300
    },
    {
      "epoch": 6.141624016499885,
      "grad_norm": 5.213663101196289,
      "learning_rate": 4.48819799862501e-05,
      "loss": 1.955,
      "step": 80400
    },
    {
      "epoch": 6.1492628523413035,
      "grad_norm": 4.702120780944824,
      "learning_rate": 4.4875614289715584e-05,
      "loss": 1.8721,
      "step": 80500
    },
    {
      "epoch": 6.156901688182721,
      "grad_norm": 5.206714630126953,
      "learning_rate": 4.486924859318107e-05,
      "loss": 1.9,
      "step": 80600
    },
    {
      "epoch": 6.164540524024138,
      "grad_norm": 5.208995819091797,
      "learning_rate": 4.486288289664655e-05,
      "loss": 1.8488,
      "step": 80700
    },
    {
      "epoch": 6.172179359865557,
      "grad_norm": 6.3213372230529785,
      "learning_rate": 4.4856517200112035e-05,
      "loss": 1.9035,
      "step": 80800
    },
    {
      "epoch": 6.179818195706974,
      "grad_norm": 4.823122024536133,
      "learning_rate": 4.4850151503577525e-05,
      "loss": 1.9726,
      "step": 80900
    },
    {
      "epoch": 6.187457031548392,
      "grad_norm": 5.2066731452941895,
      "learning_rate": 4.484378580704301e-05,
      "loss": 1.9628,
      "step": 81000
    },
    {
      "epoch": 6.19509586738981,
      "grad_norm": 5.329166412353516,
      "learning_rate": 4.483742011050849e-05,
      "loss": 2.0136,
      "step": 81100
    },
    {
      "epoch": 6.202734703231227,
      "grad_norm": 5.170571327209473,
      "learning_rate": 4.4831054413973976e-05,
      "loss": 1.985,
      "step": 81200
    },
    {
      "epoch": 6.210373539072645,
      "grad_norm": 7.5756144523620605,
      "learning_rate": 4.4824688717439466e-05,
      "loss": 1.9316,
      "step": 81300
    },
    {
      "epoch": 6.218012374914063,
      "grad_norm": 5.165488243103027,
      "learning_rate": 4.481832302090495e-05,
      "loss": 1.9175,
      "step": 81400
    },
    {
      "epoch": 6.225651210755481,
      "grad_norm": 5.0296478271484375,
      "learning_rate": 4.481195732437043e-05,
      "loss": 1.936,
      "step": 81500
    },
    {
      "epoch": 6.233290046596899,
      "grad_norm": 4.202127933502197,
      "learning_rate": 4.480559162783592e-05,
      "loss": 2.0359,
      "step": 81600
    },
    {
      "epoch": 6.2409288824383164,
      "grad_norm": 6.015239238739014,
      "learning_rate": 4.47992259313014e-05,
      "loss": 1.9006,
      "step": 81700
    },
    {
      "epoch": 6.248567718279734,
      "grad_norm": 3.763878583908081,
      "learning_rate": 4.479286023476689e-05,
      "loss": 1.8705,
      "step": 81800
    },
    {
      "epoch": 6.256206554121152,
      "grad_norm": 4.468059539794922,
      "learning_rate": 4.4786494538232374e-05,
      "loss": 1.8353,
      "step": 81900
    },
    {
      "epoch": 6.26384538996257,
      "grad_norm": 4.922019004821777,
      "learning_rate": 4.478012884169786e-05,
      "loss": 1.9731,
      "step": 82000
    },
    {
      "epoch": 6.271484225803987,
      "grad_norm": 6.819455623626709,
      "learning_rate": 4.477376314516334e-05,
      "loss": 1.9796,
      "step": 82100
    },
    {
      "epoch": 6.2791230616454055,
      "grad_norm": 5.424646854400635,
      "learning_rate": 4.476739744862883e-05,
      "loss": 2.0242,
      "step": 82200
    },
    {
      "epoch": 6.286761897486823,
      "grad_norm": 3.5880019664764404,
      "learning_rate": 4.4761031752094315e-05,
      "loss": 1.8438,
      "step": 82300
    },
    {
      "epoch": 6.29440073332824,
      "grad_norm": 5.634782314300537,
      "learning_rate": 4.47546660555598e-05,
      "loss": 1.9263,
      "step": 82400
    },
    {
      "epoch": 6.302039569169659,
      "grad_norm": 7.1964030265808105,
      "learning_rate": 4.474830035902529e-05,
      "loss": 1.854,
      "step": 82500
    },
    {
      "epoch": 6.309678405011076,
      "grad_norm": 5.158989906311035,
      "learning_rate": 4.474193466249077e-05,
      "loss": 1.9153,
      "step": 82600
    },
    {
      "epoch": 6.3173172408524945,
      "grad_norm": 7.915912628173828,
      "learning_rate": 4.4735568965956256e-05,
      "loss": 1.9643,
      "step": 82700
    },
    {
      "epoch": 6.324956076693912,
      "grad_norm": 7.484498500823975,
      "learning_rate": 4.4729203269421746e-05,
      "loss": 1.9716,
      "step": 82800
    },
    {
      "epoch": 6.332594912535329,
      "grad_norm": 4.917417049407959,
      "learning_rate": 4.472283757288723e-05,
      "loss": 1.8269,
      "step": 82900
    },
    {
      "epoch": 6.340233748376748,
      "grad_norm": 5.3463873863220215,
      "learning_rate": 4.4716471876352713e-05,
      "loss": 2.0003,
      "step": 83000
    },
    {
      "epoch": 6.347872584218165,
      "grad_norm": 5.176912307739258,
      "learning_rate": 4.47101061798182e-05,
      "loss": 1.8567,
      "step": 83100
    },
    {
      "epoch": 6.355511420059583,
      "grad_norm": 5.340311527252197,
      "learning_rate": 4.470374048328369e-05,
      "loss": 1.9113,
      "step": 83200
    },
    {
      "epoch": 6.363150255901001,
      "grad_norm": 5.24639368057251,
      "learning_rate": 4.469737478674917e-05,
      "loss": 1.9787,
      "step": 83300
    },
    {
      "epoch": 6.370789091742418,
      "grad_norm": 4.88355827331543,
      "learning_rate": 4.4691009090214654e-05,
      "loss": 1.9723,
      "step": 83400
    },
    {
      "epoch": 6.378427927583836,
      "grad_norm": 5.595099449157715,
      "learning_rate": 4.468464339368014e-05,
      "loss": 1.9588,
      "step": 83500
    },
    {
      "epoch": 6.386066763425254,
      "grad_norm": 4.738694667816162,
      "learning_rate": 4.467827769714563e-05,
      "loss": 1.9614,
      "step": 83600
    },
    {
      "epoch": 6.393705599266672,
      "grad_norm": 4.146310806274414,
      "learning_rate": 4.467191200061111e-05,
      "loss": 1.892,
      "step": 83700
    },
    {
      "epoch": 6.40134443510809,
      "grad_norm": 5.572367191314697,
      "learning_rate": 4.4665546304076595e-05,
      "loss": 1.9182,
      "step": 83800
    },
    {
      "epoch": 6.408983270949507,
      "grad_norm": 5.506856441497803,
      "learning_rate": 4.465918060754208e-05,
      "loss": 1.9682,
      "step": 83900
    },
    {
      "epoch": 6.416622106790925,
      "grad_norm": 6.118284225463867,
      "learning_rate": 4.465281491100756e-05,
      "loss": 1.9191,
      "step": 84000
    },
    {
      "epoch": 6.424260942632343,
      "grad_norm": 6.122117042541504,
      "learning_rate": 4.464644921447305e-05,
      "loss": 2.0094,
      "step": 84100
    },
    {
      "epoch": 6.431899778473761,
      "grad_norm": 5.517603874206543,
      "learning_rate": 4.4640083517938536e-05,
      "loss": 1.9283,
      "step": 84200
    },
    {
      "epoch": 6.439538614315178,
      "grad_norm": 5.055347442626953,
      "learning_rate": 4.463371782140402e-05,
      "loss": 1.9767,
      "step": 84300
    },
    {
      "epoch": 6.447177450156596,
      "grad_norm": 3.9393062591552734,
      "learning_rate": 4.46273521248695e-05,
      "loss": 1.8851,
      "step": 84400
    },
    {
      "epoch": 6.454816285998014,
      "grad_norm": 3.4724857807159424,
      "learning_rate": 4.4620986428334994e-05,
      "loss": 1.9186,
      "step": 84500
    },
    {
      "epoch": 6.462455121839431,
      "grad_norm": 3.792233943939209,
      "learning_rate": 4.461462073180048e-05,
      "loss": 1.9493,
      "step": 84600
    },
    {
      "epoch": 6.47009395768085,
      "grad_norm": 5.279947757720947,
      "learning_rate": 4.460825503526596e-05,
      "loss": 1.9512,
      "step": 84700
    },
    {
      "epoch": 6.477732793522267,
      "grad_norm": 7.234067916870117,
      "learning_rate": 4.4601889338731444e-05,
      "loss": 1.9075,
      "step": 84800
    },
    {
      "epoch": 6.485371629363685,
      "grad_norm": 5.501047611236572,
      "learning_rate": 4.459552364219693e-05,
      "loss": 1.9555,
      "step": 84900
    },
    {
      "epoch": 6.493010465205103,
      "grad_norm": 4.053994178771973,
      "learning_rate": 4.458915794566242e-05,
      "loss": 1.8989,
      "step": 85000
    },
    {
      "epoch": 6.50064930104652,
      "grad_norm": 5.6861572265625,
      "learning_rate": 4.45827922491279e-05,
      "loss": 1.9661,
      "step": 85100
    },
    {
      "epoch": 6.508288136887939,
      "grad_norm": 6.408703804016113,
      "learning_rate": 4.4576426552593385e-05,
      "loss": 2.0018,
      "step": 85200
    },
    {
      "epoch": 6.515926972729356,
      "grad_norm": 6.104319095611572,
      "learning_rate": 4.457006085605887e-05,
      "loss": 1.8136,
      "step": 85300
    },
    {
      "epoch": 6.5235658085707735,
      "grad_norm": 4.595305919647217,
      "learning_rate": 4.456369515952435e-05,
      "loss": 1.951,
      "step": 85400
    },
    {
      "epoch": 6.531204644412192,
      "grad_norm": 3.769369602203369,
      "learning_rate": 4.455732946298984e-05,
      "loss": 1.91,
      "step": 85500
    },
    {
      "epoch": 6.538843480253609,
      "grad_norm": 5.757225036621094,
      "learning_rate": 4.4550963766455326e-05,
      "loss": 1.9998,
      "step": 85600
    },
    {
      "epoch": 6.546482316095027,
      "grad_norm": 5.402205944061279,
      "learning_rate": 4.454459806992081e-05,
      "loss": 1.9171,
      "step": 85700
    },
    {
      "epoch": 6.554121151936445,
      "grad_norm": 4.131413459777832,
      "learning_rate": 4.453823237338629e-05,
      "loss": 1.9733,
      "step": 85800
    },
    {
      "epoch": 6.5617599877778625,
      "grad_norm": 6.339912414550781,
      "learning_rate": 4.4531866676851783e-05,
      "loss": 1.8762,
      "step": 85900
    },
    {
      "epoch": 6.569398823619281,
      "grad_norm": 7.006033897399902,
      "learning_rate": 4.452550098031727e-05,
      "loss": 1.9104,
      "step": 86000
    },
    {
      "epoch": 6.577037659460698,
      "grad_norm": 5.7486090660095215,
      "learning_rate": 4.451913528378275e-05,
      "loss": 1.839,
      "step": 86100
    },
    {
      "epoch": 6.584676495302116,
      "grad_norm": 9.521084785461426,
      "learning_rate": 4.451276958724824e-05,
      "loss": 1.9537,
      "step": 86200
    },
    {
      "epoch": 6.592315331143534,
      "grad_norm": 5.861858367919922,
      "learning_rate": 4.4506403890713724e-05,
      "loss": 1.8451,
      "step": 86300
    },
    {
      "epoch": 6.5999541669849515,
      "grad_norm": 5.300544261932373,
      "learning_rate": 4.450003819417921e-05,
      "loss": 1.8812,
      "step": 86400
    },
    {
      "epoch": 6.607593002826369,
      "grad_norm": 4.4408183097839355,
      "learning_rate": 4.44936724976447e-05,
      "loss": 1.9617,
      "step": 86500
    },
    {
      "epoch": 6.615231838667787,
      "grad_norm": 5.140110492706299,
      "learning_rate": 4.448730680111018e-05,
      "loss": 1.9114,
      "step": 86600
    },
    {
      "epoch": 6.622870674509205,
      "grad_norm": 7.422388076782227,
      "learning_rate": 4.4480941104575665e-05,
      "loss": 1.899,
      "step": 86700
    },
    {
      "epoch": 6.630509510350622,
      "grad_norm": 4.995553493499756,
      "learning_rate": 4.4474575408041156e-05,
      "loss": 1.9986,
      "step": 86800
    },
    {
      "epoch": 6.6381483461920405,
      "grad_norm": 5.366636753082275,
      "learning_rate": 4.446820971150664e-05,
      "loss": 1.9875,
      "step": 86900
    },
    {
      "epoch": 6.645787182033458,
      "grad_norm": 4.7010393142700195,
      "learning_rate": 4.446184401497212e-05,
      "loss": 1.9342,
      "step": 87000
    },
    {
      "epoch": 6.653426017874876,
      "grad_norm": 5.077617168426514,
      "learning_rate": 4.4455478318437606e-05,
      "loss": 1.9643,
      "step": 87100
    },
    {
      "epoch": 6.661064853716294,
      "grad_norm": 5.201131343841553,
      "learning_rate": 4.444911262190309e-05,
      "loss": 1.8797,
      "step": 87200
    },
    {
      "epoch": 6.668703689557711,
      "grad_norm": 4.063190460205078,
      "learning_rate": 4.444274692536858e-05,
      "loss": 2.0364,
      "step": 87300
    },
    {
      "epoch": 6.6763425253991295,
      "grad_norm": 5.14696741104126,
      "learning_rate": 4.4436381228834064e-05,
      "loss": 1.9559,
      "step": 87400
    },
    {
      "epoch": 6.683981361240547,
      "grad_norm": 5.87091588973999,
      "learning_rate": 4.443001553229955e-05,
      "loss": 1.8704,
      "step": 87500
    },
    {
      "epoch": 6.691620197081964,
      "grad_norm": 4.455717086791992,
      "learning_rate": 4.442364983576503e-05,
      "loss": 1.9506,
      "step": 87600
    },
    {
      "epoch": 6.699259032923383,
      "grad_norm": 6.018988609313965,
      "learning_rate": 4.441728413923052e-05,
      "loss": 1.8189,
      "step": 87700
    },
    {
      "epoch": 6.7068978687648,
      "grad_norm": 5.894256114959717,
      "learning_rate": 4.4410918442696004e-05,
      "loss": 1.9612,
      "step": 87800
    },
    {
      "epoch": 6.714536704606218,
      "grad_norm": 5.1913275718688965,
      "learning_rate": 4.440455274616149e-05,
      "loss": 1.879,
      "step": 87900
    },
    {
      "epoch": 6.722175540447636,
      "grad_norm": 4.610140800476074,
      "learning_rate": 4.439818704962697e-05,
      "loss": 1.9666,
      "step": 88000
    },
    {
      "epoch": 6.729814376289053,
      "grad_norm": 4.625627040863037,
      "learning_rate": 4.4391821353092455e-05,
      "loss": 1.9739,
      "step": 88100
    },
    {
      "epoch": 6.737453212130472,
      "grad_norm": 5.246644496917725,
      "learning_rate": 4.4385455656557945e-05,
      "loss": 1.9468,
      "step": 88200
    },
    {
      "epoch": 6.745092047971889,
      "grad_norm": 4.565964698791504,
      "learning_rate": 4.437908996002343e-05,
      "loss": 1.8917,
      "step": 88300
    },
    {
      "epoch": 6.752730883813307,
      "grad_norm": 7.794886589050293,
      "learning_rate": 4.437272426348891e-05,
      "loss": 1.8908,
      "step": 88400
    },
    {
      "epoch": 6.760369719654725,
      "grad_norm": 5.121796131134033,
      "learning_rate": 4.4366358566954396e-05,
      "loss": 1.8969,
      "step": 88500
    },
    {
      "epoch": 6.768008555496142,
      "grad_norm": 5.417040824890137,
      "learning_rate": 4.435999287041988e-05,
      "loss": 1.8477,
      "step": 88600
    },
    {
      "epoch": 6.77564739133756,
      "grad_norm": 7.016406536102295,
      "learning_rate": 4.435362717388537e-05,
      "loss": 1.9778,
      "step": 88700
    },
    {
      "epoch": 6.783286227178978,
      "grad_norm": 5.732724189758301,
      "learning_rate": 4.4347261477350853e-05,
      "loss": 1.8579,
      "step": 88800
    },
    {
      "epoch": 6.790925063020396,
      "grad_norm": 4.907149791717529,
      "learning_rate": 4.434089578081634e-05,
      "loss": 1.9975,
      "step": 88900
    },
    {
      "epoch": 6.798563898861813,
      "grad_norm": 4.811365127563477,
      "learning_rate": 4.433453008428182e-05,
      "loss": 1.8997,
      "step": 89000
    },
    {
      "epoch": 6.8062027347032314,
      "grad_norm": 4.583732604980469,
      "learning_rate": 4.432816438774731e-05,
      "loss": 1.951,
      "step": 89100
    },
    {
      "epoch": 6.813841570544649,
      "grad_norm": 4.723849773406982,
      "learning_rate": 4.4321798691212794e-05,
      "loss": 1.8945,
      "step": 89200
    },
    {
      "epoch": 6.821480406386067,
      "grad_norm": 7.965198040008545,
      "learning_rate": 4.431543299467828e-05,
      "loss": 1.9254,
      "step": 89300
    },
    {
      "epoch": 6.829119242227485,
      "grad_norm": 5.1803460121154785,
      "learning_rate": 4.430906729814376e-05,
      "loss": 1.8998,
      "step": 89400
    },
    {
      "epoch": 6.836758078068902,
      "grad_norm": 5.056241512298584,
      "learning_rate": 4.4302701601609245e-05,
      "loss": 1.8805,
      "step": 89500
    },
    {
      "epoch": 6.84439691391032,
      "grad_norm": 5.304034233093262,
      "learning_rate": 4.4296335905074735e-05,
      "loss": 1.8852,
      "step": 89600
    },
    {
      "epoch": 6.852035749751738,
      "grad_norm": 5.227508544921875,
      "learning_rate": 4.428997020854022e-05,
      "loss": 1.895,
      "step": 89700
    },
    {
      "epoch": 6.859674585593155,
      "grad_norm": 7.0499162673950195,
      "learning_rate": 4.42836045120057e-05,
      "loss": 1.9938,
      "step": 89800
    },
    {
      "epoch": 6.867313421434574,
      "grad_norm": 5.108076572418213,
      "learning_rate": 4.4277238815471186e-05,
      "loss": 2.0123,
      "step": 89900
    },
    {
      "epoch": 6.874952257275991,
      "grad_norm": 5.709650039672852,
      "learning_rate": 4.4270873118936676e-05,
      "loss": 1.9283,
      "step": 90000
    },
    {
      "epoch": 6.882591093117409,
      "grad_norm": 6.560221195220947,
      "learning_rate": 4.426450742240216e-05,
      "loss": 1.9795,
      "step": 90100
    },
    {
      "epoch": 6.890229928958827,
      "grad_norm": 4.747638702392578,
      "learning_rate": 4.425814172586765e-05,
      "loss": 1.9334,
      "step": 90200
    },
    {
      "epoch": 6.897868764800244,
      "grad_norm": 5.530727386474609,
      "learning_rate": 4.4251776029333134e-05,
      "loss": 1.8906,
      "step": 90300
    },
    {
      "epoch": 6.905507600641663,
      "grad_norm": 6.235299587249756,
      "learning_rate": 4.424541033279862e-05,
      "loss": 1.87,
      "step": 90400
    },
    {
      "epoch": 6.91314643648308,
      "grad_norm": 5.641899585723877,
      "learning_rate": 4.423904463626411e-05,
      "loss": 1.9412,
      "step": 90500
    },
    {
      "epoch": 6.920785272324498,
      "grad_norm": 4.567913055419922,
      "learning_rate": 4.423267893972959e-05,
      "loss": 2.0381,
      "step": 90600
    },
    {
      "epoch": 6.928424108165915,
      "grad_norm": 5.884411811828613,
      "learning_rate": 4.4226313243195074e-05,
      "loss": 1.832,
      "step": 90700
    },
    {
      "epoch": 6.936062944007333,
      "grad_norm": 7.638720512390137,
      "learning_rate": 4.421994754666056e-05,
      "loss": 2.0769,
      "step": 90800
    },
    {
      "epoch": 6.943701779848751,
      "grad_norm": 5.136369705200195,
      "learning_rate": 4.421358185012604e-05,
      "loss": 1.9503,
      "step": 90900
    },
    {
      "epoch": 6.951340615690169,
      "grad_norm": 3.418663501739502,
      "learning_rate": 4.420721615359153e-05,
      "loss": 1.9373,
      "step": 91000
    },
    {
      "epoch": 6.958979451531587,
      "grad_norm": 7.792577743530273,
      "learning_rate": 4.4200850457057015e-05,
      "loss": 1.8528,
      "step": 91100
    },
    {
      "epoch": 6.966618287373004,
      "grad_norm": 4.392591953277588,
      "learning_rate": 4.41944847605225e-05,
      "loss": 2.0193,
      "step": 91200
    },
    {
      "epoch": 6.974257123214422,
      "grad_norm": 5.989616870880127,
      "learning_rate": 4.418811906398798e-05,
      "loss": 1.8526,
      "step": 91300
    },
    {
      "epoch": 6.98189595905584,
      "grad_norm": 5.205139636993408,
      "learning_rate": 4.418175336745347e-05,
      "loss": 1.9493,
      "step": 91400
    },
    {
      "epoch": 6.989534794897258,
      "grad_norm": 4.314513683319092,
      "learning_rate": 4.4175387670918956e-05,
      "loss": 1.8668,
      "step": 91500
    },
    {
      "epoch": 6.997173630738676,
      "grad_norm": 3.5105326175689697,
      "learning_rate": 4.416902197438444e-05,
      "loss": 1.931,
      "step": 91600
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8962945938110352,
      "eval_runtime": 1.5919,
      "eval_samples_per_second": 433.454,
      "eval_steps_per_second": 433.454,
      "step": 91637
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.7074999809265137,
      "eval_runtime": 57.6513,
      "eval_samples_per_second": 227.072,
      "eval_steps_per_second": 227.072,
      "step": 91637
    },
    {
      "epoch": 7.004812466580093,
      "grad_norm": 4.7653889656066895,
      "learning_rate": 4.4162656277849923e-05,
      "loss": 1.892,
      "step": 91700
    },
    {
      "epoch": 7.012451302421511,
      "grad_norm": 4.855531692504883,
      "learning_rate": 4.415629058131541e-05,
      "loss": 1.8565,
      "step": 91800
    },
    {
      "epoch": 7.020090138262929,
      "grad_norm": 4.058398723602295,
      "learning_rate": 4.41499248847809e-05,
      "loss": 1.8917,
      "step": 91900
    },
    {
      "epoch": 7.027728974104346,
      "grad_norm": 5.318028450012207,
      "learning_rate": 4.414355918824638e-05,
      "loss": 1.9413,
      "step": 92000
    },
    {
      "epoch": 7.035367809945765,
      "grad_norm": 5.230833053588867,
      "learning_rate": 4.4137193491711864e-05,
      "loss": 1.8438,
      "step": 92100
    },
    {
      "epoch": 7.043006645787182,
      "grad_norm": 5.3205037117004395,
      "learning_rate": 4.413082779517735e-05,
      "loss": 2.0406,
      "step": 92200
    },
    {
      "epoch": 7.0506454816285995,
      "grad_norm": 4.438368797302246,
      "learning_rate": 4.412446209864284e-05,
      "loss": 1.9283,
      "step": 92300
    },
    {
      "epoch": 7.058284317470018,
      "grad_norm": 4.247425079345703,
      "learning_rate": 4.411809640210832e-05,
      "loss": 2.0039,
      "step": 92400
    },
    {
      "epoch": 7.065923153311435,
      "grad_norm": 6.024202823638916,
      "learning_rate": 4.4111730705573805e-05,
      "loss": 1.7512,
      "step": 92500
    },
    {
      "epoch": 7.073561989152853,
      "grad_norm": 4.6915717124938965,
      "learning_rate": 4.410536500903929e-05,
      "loss": 2.0237,
      "step": 92600
    },
    {
      "epoch": 7.081200824994271,
      "grad_norm": 5.808121681213379,
      "learning_rate": 4.409899931250477e-05,
      "loss": 1.9182,
      "step": 92700
    },
    {
      "epoch": 7.0888396608356885,
      "grad_norm": 6.564473628997803,
      "learning_rate": 4.409263361597026e-05,
      "loss": 1.9463,
      "step": 92800
    },
    {
      "epoch": 7.096478496677107,
      "grad_norm": 5.151823043823242,
      "learning_rate": 4.4086267919435746e-05,
      "loss": 1.8734,
      "step": 92900
    },
    {
      "epoch": 7.104117332518524,
      "grad_norm": 4.320760250091553,
      "learning_rate": 4.407990222290123e-05,
      "loss": 1.9472,
      "step": 93000
    },
    {
      "epoch": 7.111756168359942,
      "grad_norm": 5.771154403686523,
      "learning_rate": 4.407353652636671e-05,
      "loss": 1.8414,
      "step": 93100
    },
    {
      "epoch": 7.11939500420136,
      "grad_norm": 4.273876190185547,
      "learning_rate": 4.4067170829832204e-05,
      "loss": 1.9505,
      "step": 93200
    },
    {
      "epoch": 7.1270338400427775,
      "grad_norm": 4.12385892868042,
      "learning_rate": 4.406080513329769e-05,
      "loss": 1.8843,
      "step": 93300
    },
    {
      "epoch": 7.134672675884195,
      "grad_norm": 6.5187788009643555,
      "learning_rate": 4.405443943676317e-05,
      "loss": 1.8593,
      "step": 93400
    },
    {
      "epoch": 7.142311511725613,
      "grad_norm": 6.3286542892456055,
      "learning_rate": 4.4048073740228654e-05,
      "loss": 1.9735,
      "step": 93500
    },
    {
      "epoch": 7.149950347567031,
      "grad_norm": 4.717532157897949,
      "learning_rate": 4.404170804369414e-05,
      "loss": 1.8883,
      "step": 93600
    },
    {
      "epoch": 7.157589183408448,
      "grad_norm": 5.632380485534668,
      "learning_rate": 4.403534234715963e-05,
      "loss": 1.9289,
      "step": 93700
    },
    {
      "epoch": 7.1652280192498665,
      "grad_norm": 4.77441930770874,
      "learning_rate": 4.402897665062511e-05,
      "loss": 1.8785,
      "step": 93800
    },
    {
      "epoch": 7.172866855091284,
      "grad_norm": 5.279762268066406,
      "learning_rate": 4.4022610954090595e-05,
      "loss": 1.8203,
      "step": 93900
    },
    {
      "epoch": 7.180505690932701,
      "grad_norm": 5.7512617111206055,
      "learning_rate": 4.4016245257556085e-05,
      "loss": 1.868,
      "step": 94000
    },
    {
      "epoch": 7.18814452677412,
      "grad_norm": 4.989966869354248,
      "learning_rate": 4.400987956102157e-05,
      "loss": 1.9327,
      "step": 94100
    },
    {
      "epoch": 7.195783362615537,
      "grad_norm": 5.404567718505859,
      "learning_rate": 4.400351386448706e-05,
      "loss": 1.93,
      "step": 94200
    },
    {
      "epoch": 7.2034221984569555,
      "grad_norm": 5.418544292449951,
      "learning_rate": 4.399714816795254e-05,
      "loss": 1.8721,
      "step": 94300
    },
    {
      "epoch": 7.211061034298373,
      "grad_norm": 4.974229335784912,
      "learning_rate": 4.3990782471418026e-05,
      "loss": 1.8656,
      "step": 94400
    },
    {
      "epoch": 7.21869987013979,
      "grad_norm": 8.701114654541016,
      "learning_rate": 4.398441677488351e-05,
      "loss": 1.8352,
      "step": 94500
    },
    {
      "epoch": 7.226338705981209,
      "grad_norm": 4.673770427703857,
      "learning_rate": 4.3978051078349e-05,
      "loss": 1.9011,
      "step": 94600
    },
    {
      "epoch": 7.233977541822626,
      "grad_norm": 4.676106929779053,
      "learning_rate": 4.3971685381814484e-05,
      "loss": 1.9247,
      "step": 94700
    },
    {
      "epoch": 7.241616377664044,
      "grad_norm": 5.793699264526367,
      "learning_rate": 4.396531968527997e-05,
      "loss": 2.0065,
      "step": 94800
    },
    {
      "epoch": 7.249255213505462,
      "grad_norm": 4.6568708419799805,
      "learning_rate": 4.395895398874545e-05,
      "loss": 2.0023,
      "step": 94900
    },
    {
      "epoch": 7.256894049346879,
      "grad_norm": 6.698300361633301,
      "learning_rate": 4.3952588292210934e-05,
      "loss": 1.8786,
      "step": 95000
    },
    {
      "epoch": 7.264532885188297,
      "grad_norm": 5.2395853996276855,
      "learning_rate": 4.3946222595676425e-05,
      "loss": 1.8901,
      "step": 95100
    },
    {
      "epoch": 7.272171721029715,
      "grad_norm": 4.139072895050049,
      "learning_rate": 4.393985689914191e-05,
      "loss": 1.9092,
      "step": 95200
    },
    {
      "epoch": 7.279810556871133,
      "grad_norm": 5.80139684677124,
      "learning_rate": 4.393349120260739e-05,
      "loss": 2.0812,
      "step": 95300
    },
    {
      "epoch": 7.287449392712551,
      "grad_norm": 9.564288139343262,
      "learning_rate": 4.3927125506072875e-05,
      "loss": 1.8202,
      "step": 95400
    },
    {
      "epoch": 7.295088228553968,
      "grad_norm": 8.030281066894531,
      "learning_rate": 4.3920759809538366e-05,
      "loss": 1.9285,
      "step": 95500
    },
    {
      "epoch": 7.302727064395386,
      "grad_norm": 4.824806213378906,
      "learning_rate": 4.391439411300385e-05,
      "loss": 2.0351,
      "step": 95600
    },
    {
      "epoch": 7.310365900236804,
      "grad_norm": 5.361957550048828,
      "learning_rate": 4.390802841646933e-05,
      "loss": 1.9883,
      "step": 95700
    },
    {
      "epoch": 7.318004736078222,
      "grad_norm": 5.453330039978027,
      "learning_rate": 4.3901662719934816e-05,
      "loss": 2.0092,
      "step": 95800
    },
    {
      "epoch": 7.325643571919639,
      "grad_norm": 4.493899345397949,
      "learning_rate": 4.38952970234003e-05,
      "loss": 1.9585,
      "step": 95900
    },
    {
      "epoch": 7.333282407761057,
      "grad_norm": 4.283451080322266,
      "learning_rate": 4.388893132686579e-05,
      "loss": 1.9465,
      "step": 96000
    },
    {
      "epoch": 7.340921243602475,
      "grad_norm": 8.159506797790527,
      "learning_rate": 4.3882565630331274e-05,
      "loss": 1.8712,
      "step": 96100
    },
    {
      "epoch": 7.348560079443892,
      "grad_norm": 6.003170013427734,
      "learning_rate": 4.387619993379676e-05,
      "loss": 1.9259,
      "step": 96200
    },
    {
      "epoch": 7.356198915285311,
      "grad_norm": 7.913665771484375,
      "learning_rate": 4.386983423726224e-05,
      "loss": 1.8768,
      "step": 96300
    },
    {
      "epoch": 7.363837751126728,
      "grad_norm": 4.541495323181152,
      "learning_rate": 4.386346854072773e-05,
      "loss": 1.8338,
      "step": 96400
    },
    {
      "epoch": 7.3714765869681464,
      "grad_norm": 4.365513324737549,
      "learning_rate": 4.3857102844193214e-05,
      "loss": 1.8813,
      "step": 96500
    },
    {
      "epoch": 7.379115422809564,
      "grad_norm": 5.859654903411865,
      "learning_rate": 4.38507371476587e-05,
      "loss": 1.8927,
      "step": 96600
    },
    {
      "epoch": 7.386754258650981,
      "grad_norm": 4.638923168182373,
      "learning_rate": 4.384437145112418e-05,
      "loss": 1.8894,
      "step": 96700
    },
    {
      "epoch": 7.3943930944924,
      "grad_norm": 5.74122428894043,
      "learning_rate": 4.3838005754589665e-05,
      "loss": 1.9264,
      "step": 96800
    },
    {
      "epoch": 7.402031930333817,
      "grad_norm": 4.224832534790039,
      "learning_rate": 4.3831640058055155e-05,
      "loss": 1.8673,
      "step": 96900
    },
    {
      "epoch": 7.409670766175235,
      "grad_norm": 5.935471534729004,
      "learning_rate": 4.382527436152064e-05,
      "loss": 1.9697,
      "step": 97000
    },
    {
      "epoch": 7.417309602016653,
      "grad_norm": 6.379003524780273,
      "learning_rate": 4.381890866498612e-05,
      "loss": 1.9122,
      "step": 97100
    },
    {
      "epoch": 7.42494843785807,
      "grad_norm": 6.713342189788818,
      "learning_rate": 4.3812542968451606e-05,
      "loss": 1.9968,
      "step": 97200
    },
    {
      "epoch": 7.432587273699488,
      "grad_norm": 5.2732768058776855,
      "learning_rate": 4.380617727191709e-05,
      "loss": 1.8055,
      "step": 97300
    },
    {
      "epoch": 7.440226109540906,
      "grad_norm": 4.959253311157227,
      "learning_rate": 4.379981157538258e-05,
      "loss": 1.9007,
      "step": 97400
    },
    {
      "epoch": 7.447864945382324,
      "grad_norm": 5.911795616149902,
      "learning_rate": 4.3793445878848063e-05,
      "loss": 1.9011,
      "step": 97500
    },
    {
      "epoch": 7.455503781223742,
      "grad_norm": 5.375742435455322,
      "learning_rate": 4.378708018231355e-05,
      "loss": 1.8192,
      "step": 97600
    },
    {
      "epoch": 7.463142617065159,
      "grad_norm": 6.375746250152588,
      "learning_rate": 4.378071448577904e-05,
      "loss": 1.9659,
      "step": 97700
    },
    {
      "epoch": 7.470781452906577,
      "grad_norm": 5.067616939544678,
      "learning_rate": 4.377434878924452e-05,
      "loss": 1.93,
      "step": 97800
    },
    {
      "epoch": 7.478420288747995,
      "grad_norm": 3.695413112640381,
      "learning_rate": 4.3767983092710004e-05,
      "loss": 1.8962,
      "step": 97900
    },
    {
      "epoch": 7.486059124589413,
      "grad_norm": 5.808437347412109,
      "learning_rate": 4.3761617396175495e-05,
      "loss": 1.883,
      "step": 98000
    },
    {
      "epoch": 7.49369796043083,
      "grad_norm": 5.333313465118408,
      "learning_rate": 4.375525169964098e-05,
      "loss": 1.8558,
      "step": 98100
    },
    {
      "epoch": 7.501336796272248,
      "grad_norm": 6.168153285980225,
      "learning_rate": 4.374888600310646e-05,
      "loss": 1.8555,
      "step": 98200
    },
    {
      "epoch": 7.508975632113666,
      "grad_norm": 4.02331018447876,
      "learning_rate": 4.374252030657195e-05,
      "loss": 1.8518,
      "step": 98300
    },
    {
      "epoch": 7.516614467955083,
      "grad_norm": 4.9560370445251465,
      "learning_rate": 4.3736154610037436e-05,
      "loss": 1.9192,
      "step": 98400
    },
    {
      "epoch": 7.524253303796502,
      "grad_norm": 4.425736427307129,
      "learning_rate": 4.372978891350292e-05,
      "loss": 1.8993,
      "step": 98500
    },
    {
      "epoch": 7.531892139637919,
      "grad_norm": 6.167379379272461,
      "learning_rate": 4.37234232169684e-05,
      "loss": 1.9307,
      "step": 98600
    },
    {
      "epoch": 7.539530975479337,
      "grad_norm": 4.8537797927856445,
      "learning_rate": 4.371705752043389e-05,
      "loss": 1.8839,
      "step": 98700
    },
    {
      "epoch": 7.547169811320755,
      "grad_norm": 5.662596225738525,
      "learning_rate": 4.3710691823899376e-05,
      "loss": 1.9198,
      "step": 98800
    },
    {
      "epoch": 7.554808647162172,
      "grad_norm": 5.409363269805908,
      "learning_rate": 4.370432612736486e-05,
      "loss": 1.8837,
      "step": 98900
    },
    {
      "epoch": 7.562447483003591,
      "grad_norm": 4.958245754241943,
      "learning_rate": 4.3697960430830344e-05,
      "loss": 1.8822,
      "step": 99000
    },
    {
      "epoch": 7.570086318845008,
      "grad_norm": 7.68130350112915,
      "learning_rate": 4.369159473429583e-05,
      "loss": 1.8123,
      "step": 99100
    },
    {
      "epoch": 7.5777251546864255,
      "grad_norm": 5.989340305328369,
      "learning_rate": 4.368522903776132e-05,
      "loss": 1.9037,
      "step": 99200
    },
    {
      "epoch": 7.585363990527844,
      "grad_norm": 5.917642116546631,
      "learning_rate": 4.36788633412268e-05,
      "loss": 1.9177,
      "step": 99300
    },
    {
      "epoch": 7.593002826369261,
      "grad_norm": 6.259093284606934,
      "learning_rate": 4.3672497644692284e-05,
      "loss": 1.9912,
      "step": 99400
    },
    {
      "epoch": 7.600641662210679,
      "grad_norm": 5.739859580993652,
      "learning_rate": 4.366613194815777e-05,
      "loss": 1.862,
      "step": 99500
    },
    {
      "epoch": 7.608280498052097,
      "grad_norm": 6.05273962020874,
      "learning_rate": 4.365976625162325e-05,
      "loss": 1.8767,
      "step": 99600
    },
    {
      "epoch": 7.6159193338935145,
      "grad_norm": 4.461911201477051,
      "learning_rate": 4.365340055508874e-05,
      "loss": 1.8589,
      "step": 99700
    },
    {
      "epoch": 7.623558169734933,
      "grad_norm": 5.674760341644287,
      "learning_rate": 4.3647034858554225e-05,
      "loss": 1.9453,
      "step": 99800
    },
    {
      "epoch": 7.63119700557635,
      "grad_norm": 5.4024457931518555,
      "learning_rate": 4.364066916201971e-05,
      "loss": 1.9271,
      "step": 99900
    },
    {
      "epoch": 7.638835841417768,
      "grad_norm": 4.326831817626953,
      "learning_rate": 4.363430346548519e-05,
      "loss": 2.0027,
      "step": 100000
    },
    {
      "epoch": 7.646474677259186,
      "grad_norm": 4.974251747131348,
      "learning_rate": 4.362793776895068e-05,
      "loss": 1.9286,
      "step": 100100
    },
    {
      "epoch": 7.6541135131006035,
      "grad_norm": 4.804560661315918,
      "learning_rate": 4.3621572072416166e-05,
      "loss": 1.8856,
      "step": 100200
    },
    {
      "epoch": 7.661752348942021,
      "grad_norm": 5.244510650634766,
      "learning_rate": 4.361520637588165e-05,
      "loss": 1.9513,
      "step": 100300
    },
    {
      "epoch": 7.669391184783439,
      "grad_norm": 5.475290775299072,
      "learning_rate": 4.3608840679347133e-05,
      "loss": 1.9121,
      "step": 100400
    },
    {
      "epoch": 7.677030020624857,
      "grad_norm": 5.876389503479004,
      "learning_rate": 4.360247498281262e-05,
      "loss": 1.9292,
      "step": 100500
    },
    {
      "epoch": 7.684668856466274,
      "grad_norm": 4.047474384307861,
      "learning_rate": 4.359610928627811e-05,
      "loss": 1.8228,
      "step": 100600
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 5.195046901702881,
      "learning_rate": 4.358974358974359e-05,
      "loss": 1.8195,
      "step": 100700
    },
    {
      "epoch": 7.69994652814911,
      "grad_norm": 5.427616119384766,
      "learning_rate": 4.3583377893209074e-05,
      "loss": 1.9268,
      "step": 100800
    },
    {
      "epoch": 7.707585363990528,
      "grad_norm": 5.58164119720459,
      "learning_rate": 4.357701219667456e-05,
      "loss": 1.9513,
      "step": 100900
    },
    {
      "epoch": 7.715224199831946,
      "grad_norm": 7.171389579772949,
      "learning_rate": 4.357064650014005e-05,
      "loss": 1.9107,
      "step": 101000
    },
    {
      "epoch": 7.722863035673363,
      "grad_norm": 4.580379486083984,
      "learning_rate": 4.356428080360553e-05,
      "loss": 1.9696,
      "step": 101100
    },
    {
      "epoch": 7.7305018715147815,
      "grad_norm": 4.123369216918945,
      "learning_rate": 4.3557915107071015e-05,
      "loss": 1.8758,
      "step": 101200
    },
    {
      "epoch": 7.738140707356199,
      "grad_norm": 5.483968734741211,
      "learning_rate": 4.35515494105365e-05,
      "loss": 1.8744,
      "step": 101300
    },
    {
      "epoch": 7.745779543197616,
      "grad_norm": 5.096212387084961,
      "learning_rate": 4.354518371400199e-05,
      "loss": 1.944,
      "step": 101400
    },
    {
      "epoch": 7.753418379039035,
      "grad_norm": 6.531468391418457,
      "learning_rate": 4.353881801746747e-05,
      "loss": 1.8565,
      "step": 101500
    },
    {
      "epoch": 7.761057214880452,
      "grad_norm": 4.961297512054443,
      "learning_rate": 4.3532452320932956e-05,
      "loss": 1.7934,
      "step": 101600
    },
    {
      "epoch": 7.76869605072187,
      "grad_norm": 5.943989276885986,
      "learning_rate": 4.3526086624398446e-05,
      "loss": 1.9727,
      "step": 101700
    },
    {
      "epoch": 7.776334886563288,
      "grad_norm": 6.423222541809082,
      "learning_rate": 4.351972092786393e-05,
      "loss": 1.8788,
      "step": 101800
    },
    {
      "epoch": 7.783973722404705,
      "grad_norm": 5.675902366638184,
      "learning_rate": 4.3513355231329414e-05,
      "loss": 1.9214,
      "step": 101900
    },
    {
      "epoch": 7.791612558246124,
      "grad_norm": 5.880340576171875,
      "learning_rate": 4.3506989534794904e-05,
      "loss": 1.8544,
      "step": 102000
    },
    {
      "epoch": 7.799251394087541,
      "grad_norm": 5.277304172515869,
      "learning_rate": 4.350062383826039e-05,
      "loss": 1.8977,
      "step": 102100
    },
    {
      "epoch": 7.806890229928959,
      "grad_norm": 4.63175630569458,
      "learning_rate": 4.349425814172587e-05,
      "loss": 1.92,
      "step": 102200
    },
    {
      "epoch": 7.814529065770377,
      "grad_norm": 4.5592546463012695,
      "learning_rate": 4.3487892445191354e-05,
      "loss": 1.9482,
      "step": 102300
    },
    {
      "epoch": 7.822167901611794,
      "grad_norm": 4.465346336364746,
      "learning_rate": 4.3481526748656845e-05,
      "loss": 1.9138,
      "step": 102400
    },
    {
      "epoch": 7.829806737453212,
      "grad_norm": 4.813109397888184,
      "learning_rate": 4.347516105212233e-05,
      "loss": 1.8024,
      "step": 102500
    },
    {
      "epoch": 7.83744557329463,
      "grad_norm": 4.7985334396362305,
      "learning_rate": 4.346879535558781e-05,
      "loss": 1.9515,
      "step": 102600
    },
    {
      "epoch": 7.845084409136048,
      "grad_norm": 4.826459884643555,
      "learning_rate": 4.3462429659053295e-05,
      "loss": 1.9558,
      "step": 102700
    },
    {
      "epoch": 7.852723244977465,
      "grad_norm": 6.383771896362305,
      "learning_rate": 4.345606396251878e-05,
      "loss": 1.8676,
      "step": 102800
    },
    {
      "epoch": 7.860362080818883,
      "grad_norm": 5.79874324798584,
      "learning_rate": 4.344969826598427e-05,
      "loss": 1.8275,
      "step": 102900
    },
    {
      "epoch": 7.868000916660301,
      "grad_norm": 4.769158363342285,
      "learning_rate": 4.344333256944975e-05,
      "loss": 1.8732,
      "step": 103000
    },
    {
      "epoch": 7.875639752501719,
      "grad_norm": 5.05056095123291,
      "learning_rate": 4.3436966872915236e-05,
      "loss": 1.9047,
      "step": 103100
    },
    {
      "epoch": 7.883278588343137,
      "grad_norm": 3.457874298095703,
      "learning_rate": 4.343060117638072e-05,
      "loss": 1.9956,
      "step": 103200
    },
    {
      "epoch": 7.890917424184554,
      "grad_norm": 5.513462066650391,
      "learning_rate": 4.342423547984621e-05,
      "loss": 1.8719,
      "step": 103300
    },
    {
      "epoch": 7.8985562600259716,
      "grad_norm": 3.9101736545562744,
      "learning_rate": 4.3417869783311694e-05,
      "loss": 1.9815,
      "step": 103400
    },
    {
      "epoch": 7.90619509586739,
      "grad_norm": 4.772362232208252,
      "learning_rate": 4.341150408677718e-05,
      "loss": 1.9457,
      "step": 103500
    },
    {
      "epoch": 7.913833931708807,
      "grad_norm": 4.7447733879089355,
      "learning_rate": 4.340513839024266e-05,
      "loss": 1.9304,
      "step": 103600
    },
    {
      "epoch": 7.921472767550226,
      "grad_norm": 4.142722129821777,
      "learning_rate": 4.3398772693708144e-05,
      "loss": 1.9367,
      "step": 103700
    },
    {
      "epoch": 7.929111603391643,
      "grad_norm": 5.845932960510254,
      "learning_rate": 4.3392406997173635e-05,
      "loss": 1.8774,
      "step": 103800
    },
    {
      "epoch": 7.936750439233061,
      "grad_norm": 4.8506951332092285,
      "learning_rate": 4.338604130063912e-05,
      "loss": 1.8117,
      "step": 103900
    },
    {
      "epoch": 7.944389275074479,
      "grad_norm": 5.925570964813232,
      "learning_rate": 4.33796756041046e-05,
      "loss": 1.9702,
      "step": 104000
    },
    {
      "epoch": 7.952028110915896,
      "grad_norm": 5.126280307769775,
      "learning_rate": 4.3373309907570085e-05,
      "loss": 1.8942,
      "step": 104100
    },
    {
      "epoch": 7.959666946757315,
      "grad_norm": 5.213882923126221,
      "learning_rate": 4.3366944211035576e-05,
      "loss": 2.0165,
      "step": 104200
    },
    {
      "epoch": 7.967305782598732,
      "grad_norm": 6.405158042907715,
      "learning_rate": 4.336057851450106e-05,
      "loss": 1.9437,
      "step": 104300
    },
    {
      "epoch": 7.97494461844015,
      "grad_norm": 5.662927627563477,
      "learning_rate": 4.335421281796654e-05,
      "loss": 1.9743,
      "step": 104400
    },
    {
      "epoch": 7.982583454281567,
      "grad_norm": 5.570472717285156,
      "learning_rate": 4.3347847121432026e-05,
      "loss": 1.9107,
      "step": 104500
    },
    {
      "epoch": 7.990222290122985,
      "grad_norm": 3.8273117542266846,
      "learning_rate": 4.334148142489751e-05,
      "loss": 1.9419,
      "step": 104600
    },
    {
      "epoch": 7.997861125964403,
      "grad_norm": 3.7786877155303955,
      "learning_rate": 4.3335115728363e-05,
      "loss": 1.922,
      "step": 104700
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8718880414962769,
      "eval_runtime": 1.4883,
      "eval_samples_per_second": 463.621,
      "eval_steps_per_second": 463.621,
      "step": 104728
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.6798288822174072,
      "eval_runtime": 27.8367,
      "eval_samples_per_second": 470.279,
      "eval_steps_per_second": 470.279,
      "step": 104728
    },
    {
      "epoch": 8.005499961805821,
      "grad_norm": 5.270654678344727,
      "learning_rate": 4.3328750031828484e-05,
      "loss": 1.9149,
      "step": 104800
    },
    {
      "epoch": 8.013138797647239,
      "grad_norm": 4.946557521820068,
      "learning_rate": 4.332238433529397e-05,
      "loss": 1.8872,
      "step": 104900
    },
    {
      "epoch": 8.020777633488656,
      "grad_norm": 4.3522210121154785,
      "learning_rate": 4.331601863875945e-05,
      "loss": 1.8392,
      "step": 105000
    },
    {
      "epoch": 8.028416469330073,
      "grad_norm": 5.269619464874268,
      "learning_rate": 4.330965294222494e-05,
      "loss": 1.8631,
      "step": 105100
    },
    {
      "epoch": 8.036055305171493,
      "grad_norm": 5.059521675109863,
      "learning_rate": 4.3303287245690424e-05,
      "loss": 1.8951,
      "step": 105200
    },
    {
      "epoch": 8.04369414101291,
      "grad_norm": 5.2276763916015625,
      "learning_rate": 4.329692154915591e-05,
      "loss": 1.9141,
      "step": 105300
    },
    {
      "epoch": 8.051332976854328,
      "grad_norm": 3.7625601291656494,
      "learning_rate": 4.32905558526214e-05,
      "loss": 1.9025,
      "step": 105400
    },
    {
      "epoch": 8.058971812695745,
      "grad_norm": 4.366011619567871,
      "learning_rate": 4.328419015608688e-05,
      "loss": 1.9357,
      "step": 105500
    },
    {
      "epoch": 8.066610648537162,
      "grad_norm": 3.1189708709716797,
      "learning_rate": 4.3277824459552365e-05,
      "loss": 1.8728,
      "step": 105600
    },
    {
      "epoch": 8.07424948437858,
      "grad_norm": 4.425981521606445,
      "learning_rate": 4.3271458763017856e-05,
      "loss": 1.948,
      "step": 105700
    },
    {
      "epoch": 8.08188832022,
      "grad_norm": 5.1177077293396,
      "learning_rate": 4.326509306648334e-05,
      "loss": 1.8593,
      "step": 105800
    },
    {
      "epoch": 8.089527156061417,
      "grad_norm": 4.510285377502441,
      "learning_rate": 4.325872736994882e-05,
      "loss": 1.854,
      "step": 105900
    },
    {
      "epoch": 8.097165991902834,
      "grad_norm": 5.514438629150391,
      "learning_rate": 4.3252361673414306e-05,
      "loss": 1.881,
      "step": 106000
    },
    {
      "epoch": 8.104804827744251,
      "grad_norm": 4.948558330535889,
      "learning_rate": 4.32459959768798e-05,
      "loss": 1.918,
      "step": 106100
    },
    {
      "epoch": 8.112443663585669,
      "grad_norm": 4.215891361236572,
      "learning_rate": 4.323963028034528e-05,
      "loss": 1.8664,
      "step": 106200
    },
    {
      "epoch": 8.120082499427088,
      "grad_norm": 6.1273274421691895,
      "learning_rate": 4.3233264583810764e-05,
      "loss": 1.855,
      "step": 106300
    },
    {
      "epoch": 8.127721335268506,
      "grad_norm": 5.22762393951416,
      "learning_rate": 4.322689888727625e-05,
      "loss": 1.8482,
      "step": 106400
    },
    {
      "epoch": 8.135360171109923,
      "grad_norm": 8.188591003417969,
      "learning_rate": 4.322053319074174e-05,
      "loss": 1.8166,
      "step": 106500
    },
    {
      "epoch": 8.14299900695134,
      "grad_norm": 5.32204532623291,
      "learning_rate": 4.321416749420722e-05,
      "loss": 1.7979,
      "step": 106600
    },
    {
      "epoch": 8.150637842792758,
      "grad_norm": 5.350155353546143,
      "learning_rate": 4.3207801797672705e-05,
      "loss": 1.9813,
      "step": 106700
    },
    {
      "epoch": 8.158276678634175,
      "grad_norm": 3.8183765411376953,
      "learning_rate": 4.320143610113819e-05,
      "loss": 1.9645,
      "step": 106800
    },
    {
      "epoch": 8.165915514475595,
      "grad_norm": 4.968862056732178,
      "learning_rate": 4.319507040460367e-05,
      "loss": 1.8635,
      "step": 106900
    },
    {
      "epoch": 8.173554350317012,
      "grad_norm": 5.656919956207275,
      "learning_rate": 4.318870470806916e-05,
      "loss": 2.0025,
      "step": 107000
    },
    {
      "epoch": 8.18119318615843,
      "grad_norm": 6.096869945526123,
      "learning_rate": 4.3182339011534646e-05,
      "loss": 1.8753,
      "step": 107100
    },
    {
      "epoch": 8.188832021999847,
      "grad_norm": 8.388071060180664,
      "learning_rate": 4.317597331500013e-05,
      "loss": 1.8611,
      "step": 107200
    },
    {
      "epoch": 8.196470857841264,
      "grad_norm": 4.8184285163879395,
      "learning_rate": 4.316960761846561e-05,
      "loss": 1.8884,
      "step": 107300
    },
    {
      "epoch": 8.204109693682684,
      "grad_norm": 4.7639875411987305,
      "learning_rate": 4.31632419219311e-05,
      "loss": 1.9208,
      "step": 107400
    },
    {
      "epoch": 8.211748529524101,
      "grad_norm": 5.566692352294922,
      "learning_rate": 4.3156876225396586e-05,
      "loss": 1.8648,
      "step": 107500
    },
    {
      "epoch": 8.219387365365519,
      "grad_norm": 5.037610054016113,
      "learning_rate": 4.315051052886207e-05,
      "loss": 1.9211,
      "step": 107600
    },
    {
      "epoch": 8.227026201206936,
      "grad_norm": 5.3707075119018555,
      "learning_rate": 4.3144144832327554e-05,
      "loss": 1.93,
      "step": 107700
    },
    {
      "epoch": 8.234665037048353,
      "grad_norm": 7.694821357727051,
      "learning_rate": 4.313777913579304e-05,
      "loss": 1.9116,
      "step": 107800
    },
    {
      "epoch": 8.24230387288977,
      "grad_norm": 5.332866668701172,
      "learning_rate": 4.313141343925853e-05,
      "loss": 1.8601,
      "step": 107900
    },
    {
      "epoch": 8.24994270873119,
      "grad_norm": 5.214315891265869,
      "learning_rate": 4.312504774272401e-05,
      "loss": 1.9375,
      "step": 108000
    },
    {
      "epoch": 8.257581544572608,
      "grad_norm": 5.172670364379883,
      "learning_rate": 4.3118682046189494e-05,
      "loss": 1.8342,
      "step": 108100
    },
    {
      "epoch": 8.265220380414025,
      "grad_norm": 6.525402545928955,
      "learning_rate": 4.311231634965498e-05,
      "loss": 1.8332,
      "step": 108200
    },
    {
      "epoch": 8.272859216255442,
      "grad_norm": 6.8758015632629395,
      "learning_rate": 4.310595065312046e-05,
      "loss": 1.8509,
      "step": 108300
    },
    {
      "epoch": 8.28049805209686,
      "grad_norm": 5.262033462524414,
      "learning_rate": 4.309958495658595e-05,
      "loss": 1.9445,
      "step": 108400
    },
    {
      "epoch": 8.288136887938279,
      "grad_norm": 6.810665130615234,
      "learning_rate": 4.3093219260051435e-05,
      "loss": 1.9246,
      "step": 108500
    },
    {
      "epoch": 8.295775723779697,
      "grad_norm": 5.814034938812256,
      "learning_rate": 4.308685356351692e-05,
      "loss": 1.9502,
      "step": 108600
    },
    {
      "epoch": 8.303414559621114,
      "grad_norm": 6.799792289733887,
      "learning_rate": 4.30804878669824e-05,
      "loss": 1.7996,
      "step": 108700
    },
    {
      "epoch": 8.311053395462531,
      "grad_norm": 4.702723503112793,
      "learning_rate": 4.307412217044789e-05,
      "loss": 1.9034,
      "step": 108800
    },
    {
      "epoch": 8.318692231303949,
      "grad_norm": 4.239006519317627,
      "learning_rate": 4.3067756473913376e-05,
      "loss": 2.0055,
      "step": 108900
    },
    {
      "epoch": 8.326331067145366,
      "grad_norm": 4.6683197021484375,
      "learning_rate": 4.306139077737886e-05,
      "loss": 1.9763,
      "step": 109000
    },
    {
      "epoch": 8.333969902986786,
      "grad_norm": 4.511746883392334,
      "learning_rate": 4.3055025080844343e-05,
      "loss": 1.9618,
      "step": 109100
    },
    {
      "epoch": 8.341608738828203,
      "grad_norm": 7.426597595214844,
      "learning_rate": 4.3048659384309834e-05,
      "loss": 1.8957,
      "step": 109200
    },
    {
      "epoch": 8.34924757466962,
      "grad_norm": 4.637439250946045,
      "learning_rate": 4.304229368777532e-05,
      "loss": 1.831,
      "step": 109300
    },
    {
      "epoch": 8.356886410511038,
      "grad_norm": 4.543435573577881,
      "learning_rate": 4.303592799124081e-05,
      "loss": 1.8952,
      "step": 109400
    },
    {
      "epoch": 8.364525246352455,
      "grad_norm": 6.754870414733887,
      "learning_rate": 4.302956229470629e-05,
      "loss": 1.8041,
      "step": 109500
    },
    {
      "epoch": 8.372164082193873,
      "grad_norm": 4.394623279571533,
      "learning_rate": 4.3023196598171775e-05,
      "loss": 1.9327,
      "step": 109600
    },
    {
      "epoch": 8.379802918035292,
      "grad_norm": 5.713695049285889,
      "learning_rate": 4.3016830901637265e-05,
      "loss": 1.9897,
      "step": 109700
    },
    {
      "epoch": 8.38744175387671,
      "grad_norm": 5.062060832977295,
      "learning_rate": 4.301046520510275e-05,
      "loss": 1.917,
      "step": 109800
    },
    {
      "epoch": 8.395080589718127,
      "grad_norm": 5.121266841888428,
      "learning_rate": 4.300409950856823e-05,
      "loss": 1.8255,
      "step": 109900
    },
    {
      "epoch": 8.402719425559544,
      "grad_norm": 5.544003486633301,
      "learning_rate": 4.2997733812033716e-05,
      "loss": 1.8715,
      "step": 110000
    },
    {
      "epoch": 8.410358261400962,
      "grad_norm": 4.804728031158447,
      "learning_rate": 4.29913681154992e-05,
      "loss": 1.9552,
      "step": 110100
    },
    {
      "epoch": 8.417997097242381,
      "grad_norm": 4.843710422515869,
      "learning_rate": 4.298500241896469e-05,
      "loss": 1.8607,
      "step": 110200
    },
    {
      "epoch": 8.425635933083798,
      "grad_norm": 4.634023666381836,
      "learning_rate": 4.297863672243017e-05,
      "loss": 1.9281,
      "step": 110300
    },
    {
      "epoch": 8.433274768925216,
      "grad_norm": 5.014491081237793,
      "learning_rate": 4.2972271025895656e-05,
      "loss": 1.9246,
      "step": 110400
    },
    {
      "epoch": 8.440913604766633,
      "grad_norm": 5.605496883392334,
      "learning_rate": 4.296590532936114e-05,
      "loss": 1.9128,
      "step": 110500
    },
    {
      "epoch": 8.44855244060805,
      "grad_norm": 5.464090347290039,
      "learning_rate": 4.295953963282663e-05,
      "loss": 1.9441,
      "step": 110600
    },
    {
      "epoch": 8.45619127644947,
      "grad_norm": 7.384422779083252,
      "learning_rate": 4.2953173936292114e-05,
      "loss": 1.8795,
      "step": 110700
    },
    {
      "epoch": 8.463830112290887,
      "grad_norm": 5.164448261260986,
      "learning_rate": 4.29468082397576e-05,
      "loss": 1.8843,
      "step": 110800
    },
    {
      "epoch": 8.471468948132305,
      "grad_norm": 6.099098205566406,
      "learning_rate": 4.294044254322308e-05,
      "loss": 1.9346,
      "step": 110900
    },
    {
      "epoch": 8.479107783973722,
      "grad_norm": 4.833465099334717,
      "learning_rate": 4.2934076846688564e-05,
      "loss": 1.9435,
      "step": 111000
    },
    {
      "epoch": 8.48674661981514,
      "grad_norm": 5.668520450592041,
      "learning_rate": 4.2927711150154055e-05,
      "loss": 1.9468,
      "step": 111100
    },
    {
      "epoch": 8.494385455656557,
      "grad_norm": 5.388899326324463,
      "learning_rate": 4.292134545361954e-05,
      "loss": 1.8507,
      "step": 111200
    },
    {
      "epoch": 8.502024291497976,
      "grad_norm": 4.410058975219727,
      "learning_rate": 4.291497975708502e-05,
      "loss": 1.8755,
      "step": 111300
    },
    {
      "epoch": 8.509663127339394,
      "grad_norm": 4.905378818511963,
      "learning_rate": 4.2908614060550505e-05,
      "loss": 1.9219,
      "step": 111400
    },
    {
      "epoch": 8.517301963180811,
      "grad_norm": 5.721988201141357,
      "learning_rate": 4.290224836401599e-05,
      "loss": 1.829,
      "step": 111500
    },
    {
      "epoch": 8.524940799022229,
      "grad_norm": 6.329893112182617,
      "learning_rate": 4.289588266748148e-05,
      "loss": 1.9464,
      "step": 111600
    },
    {
      "epoch": 8.532579634863646,
      "grad_norm": 5.330678939819336,
      "learning_rate": 4.288951697094696e-05,
      "loss": 1.856,
      "step": 111700
    },
    {
      "epoch": 8.540218470705064,
      "grad_norm": 5.432226181030273,
      "learning_rate": 4.2883151274412446e-05,
      "loss": 1.887,
      "step": 111800
    },
    {
      "epoch": 8.547857306546483,
      "grad_norm": 3.975597381591797,
      "learning_rate": 4.287678557787793e-05,
      "loss": 1.9367,
      "step": 111900
    },
    {
      "epoch": 8.5554961423879,
      "grad_norm": 3.053105592727661,
      "learning_rate": 4.287041988134342e-05,
      "loss": 1.8536,
      "step": 112000
    },
    {
      "epoch": 8.563134978229318,
      "grad_norm": 5.949962615966797,
      "learning_rate": 4.2864054184808904e-05,
      "loss": 1.9218,
      "step": 112100
    },
    {
      "epoch": 8.570773814070735,
      "grad_norm": 9.353867530822754,
      "learning_rate": 4.285768848827439e-05,
      "loss": 1.9554,
      "step": 112200
    },
    {
      "epoch": 8.578412649912153,
      "grad_norm": 4.005197525024414,
      "learning_rate": 4.285132279173987e-05,
      "loss": 1.8836,
      "step": 112300
    },
    {
      "epoch": 8.586051485753572,
      "grad_norm": 4.726653099060059,
      "learning_rate": 4.2844957095205354e-05,
      "loss": 1.9071,
      "step": 112400
    },
    {
      "epoch": 8.59369032159499,
      "grad_norm": 4.980913162231445,
      "learning_rate": 4.2838591398670845e-05,
      "loss": 1.8281,
      "step": 112500
    },
    {
      "epoch": 8.601329157436407,
      "grad_norm": 4.320165157318115,
      "learning_rate": 4.283222570213633e-05,
      "loss": 1.8911,
      "step": 112600
    },
    {
      "epoch": 8.608967993277824,
      "grad_norm": 6.119589805603027,
      "learning_rate": 4.282586000560181e-05,
      "loss": 1.8204,
      "step": 112700
    },
    {
      "epoch": 8.616606829119242,
      "grad_norm": 6.76011848449707,
      "learning_rate": 4.2819494309067295e-05,
      "loss": 1.8455,
      "step": 112800
    },
    {
      "epoch": 8.624245664960661,
      "grad_norm": 6.199556350708008,
      "learning_rate": 4.2813128612532786e-05,
      "loss": 2.0238,
      "step": 112900
    },
    {
      "epoch": 8.631884500802078,
      "grad_norm": 4.1849212646484375,
      "learning_rate": 4.280676291599827e-05,
      "loss": 1.8008,
      "step": 113000
    },
    {
      "epoch": 8.639523336643496,
      "grad_norm": 6.2360310554504395,
      "learning_rate": 4.280039721946375e-05,
      "loss": 1.9318,
      "step": 113100
    },
    {
      "epoch": 8.647162172484913,
      "grad_norm": 4.270270824432373,
      "learning_rate": 4.279403152292924e-05,
      "loss": 1.8548,
      "step": 113200
    },
    {
      "epoch": 8.65480100832633,
      "grad_norm": 8.328608512878418,
      "learning_rate": 4.2787665826394726e-05,
      "loss": 1.9622,
      "step": 113300
    },
    {
      "epoch": 8.662439844167748,
      "grad_norm": 4.418557167053223,
      "learning_rate": 4.278130012986022e-05,
      "loss": 1.9366,
      "step": 113400
    },
    {
      "epoch": 8.670078680009167,
      "grad_norm": 6.416481971740723,
      "learning_rate": 4.27749344333257e-05,
      "loss": 1.9561,
      "step": 113500
    },
    {
      "epoch": 8.677717515850585,
      "grad_norm": 5.529240131378174,
      "learning_rate": 4.2768568736791184e-05,
      "loss": 1.9566,
      "step": 113600
    },
    {
      "epoch": 8.685356351692002,
      "grad_norm": 5.1254777908325195,
      "learning_rate": 4.276220304025667e-05,
      "loss": 1.9529,
      "step": 113700
    },
    {
      "epoch": 8.69299518753342,
      "grad_norm": 5.881425380706787,
      "learning_rate": 4.275583734372216e-05,
      "loss": 1.8761,
      "step": 113800
    },
    {
      "epoch": 8.700634023374837,
      "grad_norm": 6.415513515472412,
      "learning_rate": 4.274947164718764e-05,
      "loss": 1.8711,
      "step": 113900
    },
    {
      "epoch": 8.708272859216255,
      "grad_norm": 3.862842082977295,
      "learning_rate": 4.2743105950653125e-05,
      "loss": 1.9843,
      "step": 114000
    },
    {
      "epoch": 8.715911695057674,
      "grad_norm": 5.449680805206299,
      "learning_rate": 4.273674025411861e-05,
      "loss": 1.9385,
      "step": 114100
    },
    {
      "epoch": 8.723550530899091,
      "grad_norm": 4.512577533721924,
      "learning_rate": 4.273037455758409e-05,
      "loss": 1.9058,
      "step": 114200
    },
    {
      "epoch": 8.731189366740509,
      "grad_norm": 6.179937839508057,
      "learning_rate": 4.272400886104958e-05,
      "loss": 1.9299,
      "step": 114300
    },
    {
      "epoch": 8.738828202581926,
      "grad_norm": 5.923374652862549,
      "learning_rate": 4.2717643164515066e-05,
      "loss": 1.8792,
      "step": 114400
    },
    {
      "epoch": 8.746467038423344,
      "grad_norm": 5.310499668121338,
      "learning_rate": 4.271127746798055e-05,
      "loss": 1.8644,
      "step": 114500
    },
    {
      "epoch": 8.754105874264763,
      "grad_norm": 5.017524242401123,
      "learning_rate": 4.270491177144603e-05,
      "loss": 1.8629,
      "step": 114600
    },
    {
      "epoch": 8.76174471010618,
      "grad_norm": 5.148873329162598,
      "learning_rate": 4.2698546074911516e-05,
      "loss": 1.8403,
      "step": 114700
    },
    {
      "epoch": 8.769383545947598,
      "grad_norm": 5.112626075744629,
      "learning_rate": 4.269218037837701e-05,
      "loss": 1.8619,
      "step": 114800
    },
    {
      "epoch": 8.777022381789015,
      "grad_norm": 6.733065128326416,
      "learning_rate": 4.268581468184249e-05,
      "loss": 1.8578,
      "step": 114900
    },
    {
      "epoch": 8.784661217630433,
      "grad_norm": 6.080535411834717,
      "learning_rate": 4.2679448985307974e-05,
      "loss": 1.7998,
      "step": 115000
    },
    {
      "epoch": 8.79230005347185,
      "grad_norm": 5.466644763946533,
      "learning_rate": 4.267308328877346e-05,
      "loss": 1.8645,
      "step": 115100
    },
    {
      "epoch": 8.79993888931327,
      "grad_norm": 5.728806495666504,
      "learning_rate": 4.266671759223895e-05,
      "loss": 1.8612,
      "step": 115200
    },
    {
      "epoch": 8.807577725154687,
      "grad_norm": 5.089931011199951,
      "learning_rate": 4.266035189570443e-05,
      "loss": 1.8827,
      "step": 115300
    },
    {
      "epoch": 8.815216560996104,
      "grad_norm": 4.771980285644531,
      "learning_rate": 4.2653986199169915e-05,
      "loss": 1.9086,
      "step": 115400
    },
    {
      "epoch": 8.822855396837522,
      "grad_norm": 6.004822254180908,
      "learning_rate": 4.26476205026354e-05,
      "loss": 1.8794,
      "step": 115500
    },
    {
      "epoch": 8.830494232678939,
      "grad_norm": 4.689841270446777,
      "learning_rate": 4.264125480610088e-05,
      "loss": 1.8501,
      "step": 115600
    },
    {
      "epoch": 8.838133068520358,
      "grad_norm": 3.596522092819214,
      "learning_rate": 4.263488910956637e-05,
      "loss": 1.8801,
      "step": 115700
    },
    {
      "epoch": 8.845771904361776,
      "grad_norm": 4.697671890258789,
      "learning_rate": 4.2628523413031856e-05,
      "loss": 1.8618,
      "step": 115800
    },
    {
      "epoch": 8.853410740203193,
      "grad_norm": 5.788748264312744,
      "learning_rate": 4.262215771649734e-05,
      "loss": 1.9204,
      "step": 115900
    },
    {
      "epoch": 8.86104957604461,
      "grad_norm": 5.513919353485107,
      "learning_rate": 4.261579201996282e-05,
      "loss": 1.9164,
      "step": 116000
    },
    {
      "epoch": 8.868688411886028,
      "grad_norm": 4.739734172821045,
      "learning_rate": 4.260942632342831e-05,
      "loss": 1.9028,
      "step": 116100
    },
    {
      "epoch": 8.876327247727446,
      "grad_norm": 7.67496395111084,
      "learning_rate": 4.2603060626893796e-05,
      "loss": 1.831,
      "step": 116200
    },
    {
      "epoch": 8.883966083568865,
      "grad_norm": 4.9640727043151855,
      "learning_rate": 4.259669493035928e-05,
      "loss": 1.8664,
      "step": 116300
    },
    {
      "epoch": 8.891604919410282,
      "grad_norm": 5.4027814865112305,
      "learning_rate": 4.2590329233824764e-05,
      "loss": 1.8494,
      "step": 116400
    },
    {
      "epoch": 8.8992437552517,
      "grad_norm": 4.507627487182617,
      "learning_rate": 4.258396353729025e-05,
      "loss": 1.8805,
      "step": 116500
    },
    {
      "epoch": 8.906882591093117,
      "grad_norm": 5.848571300506592,
      "learning_rate": 4.257759784075574e-05,
      "loss": 1.9302,
      "step": 116600
    },
    {
      "epoch": 8.914521426934535,
      "grad_norm": 5.990423679351807,
      "learning_rate": 4.257123214422122e-05,
      "loss": 1.8938,
      "step": 116700
    },
    {
      "epoch": 8.922160262775954,
      "grad_norm": 4.367282867431641,
      "learning_rate": 4.2564866447686704e-05,
      "loss": 1.8086,
      "step": 116800
    },
    {
      "epoch": 8.929799098617371,
      "grad_norm": 5.5064167976379395,
      "learning_rate": 4.2558500751152195e-05,
      "loss": 1.8886,
      "step": 116900
    },
    {
      "epoch": 8.937437934458789,
      "grad_norm": 6.838388442993164,
      "learning_rate": 4.255213505461768e-05,
      "loss": 1.838,
      "step": 117000
    },
    {
      "epoch": 8.945076770300206,
      "grad_norm": 4.506416320800781,
      "learning_rate": 4.254576935808316e-05,
      "loss": 1.904,
      "step": 117100
    },
    {
      "epoch": 8.952715606141624,
      "grad_norm": 3.9996120929718018,
      "learning_rate": 4.253940366154865e-05,
      "loss": 1.7918,
      "step": 117200
    },
    {
      "epoch": 8.960354441983041,
      "grad_norm": 4.379598617553711,
      "learning_rate": 4.2533037965014136e-05,
      "loss": 1.8622,
      "step": 117300
    },
    {
      "epoch": 8.96799327782446,
      "grad_norm": 3.2533717155456543,
      "learning_rate": 4.252667226847962e-05,
      "loss": 1.7841,
      "step": 117400
    },
    {
      "epoch": 8.975632113665878,
      "grad_norm": 3.846233367919922,
      "learning_rate": 4.252030657194511e-05,
      "loss": 1.9264,
      "step": 117500
    },
    {
      "epoch": 8.983270949507295,
      "grad_norm": 8.09168815612793,
      "learning_rate": 4.251394087541059e-05,
      "loss": 1.8725,
      "step": 117600
    },
    {
      "epoch": 8.990909785348713,
      "grad_norm": 5.255865573883057,
      "learning_rate": 4.250757517887608e-05,
      "loss": 1.8942,
      "step": 117700
    },
    {
      "epoch": 8.99854862119013,
      "grad_norm": 5.121086120605469,
      "learning_rate": 4.250120948234156e-05,
      "loss": 1.8887,
      "step": 117800
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8682140111923218,
      "eval_runtime": 1.535,
      "eval_samples_per_second": 449.513,
      "eval_steps_per_second": 449.513,
      "step": 117819
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.6651471853256226,
      "eval_runtime": 28.503,
      "eval_samples_per_second": 459.285,
      "eval_steps_per_second": 459.285,
      "step": 117819
    },
    {
      "epoch": 9.00618745703155,
      "grad_norm": 4.876704216003418,
      "learning_rate": 4.2494843785807044e-05,
      "loss": 1.8035,
      "step": 117900
    },
    {
      "epoch": 9.013826292872967,
      "grad_norm": 4.577346324920654,
      "learning_rate": 4.2488478089272534e-05,
      "loss": 1.8996,
      "step": 118000
    },
    {
      "epoch": 9.021465128714384,
      "grad_norm": 5.6141228675842285,
      "learning_rate": 4.248211239273802e-05,
      "loss": 1.8014,
      "step": 118100
    },
    {
      "epoch": 9.029103964555802,
      "grad_norm": 6.518258571624756,
      "learning_rate": 4.24757466962035e-05,
      "loss": 1.921,
      "step": 118200
    },
    {
      "epoch": 9.036742800397219,
      "grad_norm": 4.674749374389648,
      "learning_rate": 4.2469380999668985e-05,
      "loss": 1.8157,
      "step": 118300
    },
    {
      "epoch": 9.044381636238636,
      "grad_norm": 4.873442649841309,
      "learning_rate": 4.2463015303134475e-05,
      "loss": 1.8154,
      "step": 118400
    },
    {
      "epoch": 9.052020472080056,
      "grad_norm": 4.952631950378418,
      "learning_rate": 4.245664960659996e-05,
      "loss": 1.907,
      "step": 118500
    },
    {
      "epoch": 9.059659307921473,
      "grad_norm": 5.643988132476807,
      "learning_rate": 4.245028391006544e-05,
      "loss": 1.8643,
      "step": 118600
    },
    {
      "epoch": 9.06729814376289,
      "grad_norm": 6.6830058097839355,
      "learning_rate": 4.2443918213530926e-05,
      "loss": 1.9238,
      "step": 118700
    },
    {
      "epoch": 9.074936979604308,
      "grad_norm": 4.975009441375732,
      "learning_rate": 4.243755251699641e-05,
      "loss": 1.778,
      "step": 118800
    },
    {
      "epoch": 9.082575815445725,
      "grad_norm": 4.591828346252441,
      "learning_rate": 4.24311868204619e-05,
      "loss": 1.9934,
      "step": 118900
    },
    {
      "epoch": 9.090214651287145,
      "grad_norm": 4.263481616973877,
      "learning_rate": 4.242482112392738e-05,
      "loss": 1.7981,
      "step": 119000
    },
    {
      "epoch": 9.097853487128562,
      "grad_norm": 5.380435466766357,
      "learning_rate": 4.2418455427392866e-05,
      "loss": 1.9083,
      "step": 119100
    },
    {
      "epoch": 9.10549232296998,
      "grad_norm": 4.7316389083862305,
      "learning_rate": 4.241208973085835e-05,
      "loss": 1.8886,
      "step": 119200
    },
    {
      "epoch": 9.113131158811397,
      "grad_norm": 4.634594440460205,
      "learning_rate": 4.240572403432384e-05,
      "loss": 1.8824,
      "step": 119300
    },
    {
      "epoch": 9.120769994652814,
      "grad_norm": 4.770493030548096,
      "learning_rate": 4.2399358337789324e-05,
      "loss": 1.8836,
      "step": 119400
    },
    {
      "epoch": 9.128408830494232,
      "grad_norm": 5.084421634674072,
      "learning_rate": 4.239299264125481e-05,
      "loss": 1.7968,
      "step": 119500
    },
    {
      "epoch": 9.136047666335651,
      "grad_norm": 6.036217212677002,
      "learning_rate": 4.238662694472029e-05,
      "loss": 1.8353,
      "step": 119600
    },
    {
      "epoch": 9.143686502177069,
      "grad_norm": 7.6096062660217285,
      "learning_rate": 4.2380261248185774e-05,
      "loss": 1.8335,
      "step": 119700
    },
    {
      "epoch": 9.151325338018486,
      "grad_norm": 4.0164899826049805,
      "learning_rate": 4.2373895551651265e-05,
      "loss": 1.7784,
      "step": 119800
    },
    {
      "epoch": 9.158964173859903,
      "grad_norm": 3.4492576122283936,
      "learning_rate": 4.236752985511675e-05,
      "loss": 1.9038,
      "step": 119900
    },
    {
      "epoch": 9.166603009701321,
      "grad_norm": 5.298243999481201,
      "learning_rate": 4.236116415858223e-05,
      "loss": 1.9726,
      "step": 120000
    },
    {
      "epoch": 9.17424184554274,
      "grad_norm": 5.004743576049805,
      "learning_rate": 4.2354798462047715e-05,
      "loss": 1.8218,
      "step": 120100
    },
    {
      "epoch": 9.181880681384158,
      "grad_norm": 6.579277038574219,
      "learning_rate": 4.23484327655132e-05,
      "loss": 1.744,
      "step": 120200
    },
    {
      "epoch": 9.189519517225575,
      "grad_norm": 5.390781879425049,
      "learning_rate": 4.234206706897869e-05,
      "loss": 1.8585,
      "step": 120300
    },
    {
      "epoch": 9.197158353066992,
      "grad_norm": 5.818085670471191,
      "learning_rate": 4.233570137244417e-05,
      "loss": 1.9077,
      "step": 120400
    },
    {
      "epoch": 9.20479718890841,
      "grad_norm": 4.688218116760254,
      "learning_rate": 4.2329335675909656e-05,
      "loss": 1.8426,
      "step": 120500
    },
    {
      "epoch": 9.212436024749827,
      "grad_norm": 4.102455139160156,
      "learning_rate": 4.232296997937515e-05,
      "loss": 1.9115,
      "step": 120600
    },
    {
      "epoch": 9.220074860591247,
      "grad_norm": 5.028476715087891,
      "learning_rate": 4.231660428284063e-05,
      "loss": 1.8519,
      "step": 120700
    },
    {
      "epoch": 9.227713696432664,
      "grad_norm": 5.7236175537109375,
      "learning_rate": 4.2310238586306114e-05,
      "loss": 1.8259,
      "step": 120800
    },
    {
      "epoch": 9.235352532274081,
      "grad_norm": 4.159270286560059,
      "learning_rate": 4.2303872889771604e-05,
      "loss": 1.7875,
      "step": 120900
    },
    {
      "epoch": 9.242991368115499,
      "grad_norm": 4.629364967346191,
      "learning_rate": 4.229750719323709e-05,
      "loss": 1.8971,
      "step": 121000
    },
    {
      "epoch": 9.250630203956916,
      "grad_norm": 6.041938304901123,
      "learning_rate": 4.229114149670257e-05,
      "loss": 1.8409,
      "step": 121100
    },
    {
      "epoch": 9.258269039798336,
      "grad_norm": 5.031254768371582,
      "learning_rate": 4.228477580016806e-05,
      "loss": 1.8539,
      "step": 121200
    },
    {
      "epoch": 9.265907875639753,
      "grad_norm": 4.93757963180542,
      "learning_rate": 4.2278410103633545e-05,
      "loss": 1.9024,
      "step": 121300
    },
    {
      "epoch": 9.27354671148117,
      "grad_norm": 7.047270774841309,
      "learning_rate": 4.227204440709903e-05,
      "loss": 1.8724,
      "step": 121400
    },
    {
      "epoch": 9.281185547322588,
      "grad_norm": 5.0850019454956055,
      "learning_rate": 4.226567871056451e-05,
      "loss": 1.8598,
      "step": 121500
    },
    {
      "epoch": 9.288824383164005,
      "grad_norm": 4.694998264312744,
      "learning_rate": 4.225931301403e-05,
      "loss": 1.9395,
      "step": 121600
    },
    {
      "epoch": 9.296463219005423,
      "grad_norm": 4.709160327911377,
      "learning_rate": 4.2252947317495486e-05,
      "loss": 1.7967,
      "step": 121700
    },
    {
      "epoch": 9.304102054846842,
      "grad_norm": 4.925492286682129,
      "learning_rate": 4.224658162096097e-05,
      "loss": 1.934,
      "step": 121800
    },
    {
      "epoch": 9.31174089068826,
      "grad_norm": 7.049617290496826,
      "learning_rate": 4.224021592442645e-05,
      "loss": 1.8881,
      "step": 121900
    },
    {
      "epoch": 9.319379726529677,
      "grad_norm": 6.343888759613037,
      "learning_rate": 4.2233850227891936e-05,
      "loss": 1.7886,
      "step": 122000
    },
    {
      "epoch": 9.327018562371094,
      "grad_norm": 5.368983745574951,
      "learning_rate": 4.222748453135743e-05,
      "loss": 1.892,
      "step": 122100
    },
    {
      "epoch": 9.334657398212512,
      "grad_norm": 4.923095226287842,
      "learning_rate": 4.222111883482291e-05,
      "loss": 1.8053,
      "step": 122200
    },
    {
      "epoch": 9.34229623405393,
      "grad_norm": 5.2665581703186035,
      "learning_rate": 4.2214753138288394e-05,
      "loss": 1.9152,
      "step": 122300
    },
    {
      "epoch": 9.349935069895349,
      "grad_norm": 5.551938056945801,
      "learning_rate": 4.220838744175388e-05,
      "loss": 1.9695,
      "step": 122400
    },
    {
      "epoch": 9.357573905736766,
      "grad_norm": 4.851152420043945,
      "learning_rate": 4.220202174521937e-05,
      "loss": 1.9227,
      "step": 122500
    },
    {
      "epoch": 9.365212741578183,
      "grad_norm": 5.662990093231201,
      "learning_rate": 4.219565604868485e-05,
      "loss": 1.9838,
      "step": 122600
    },
    {
      "epoch": 9.3728515774196,
      "grad_norm": 4.841716289520264,
      "learning_rate": 4.2189290352150335e-05,
      "loss": 1.9343,
      "step": 122700
    },
    {
      "epoch": 9.380490413261018,
      "grad_norm": 5.302859306335449,
      "learning_rate": 4.218292465561582e-05,
      "loss": 1.9362,
      "step": 122800
    },
    {
      "epoch": 9.388129249102438,
      "grad_norm": 6.306928634643555,
      "learning_rate": 4.21765589590813e-05,
      "loss": 1.9534,
      "step": 122900
    },
    {
      "epoch": 9.395768084943855,
      "grad_norm": 5.243575096130371,
      "learning_rate": 4.217019326254679e-05,
      "loss": 1.8098,
      "step": 123000
    },
    {
      "epoch": 9.403406920785272,
      "grad_norm": 5.853952407836914,
      "learning_rate": 4.2163827566012276e-05,
      "loss": 1.8379,
      "step": 123100
    },
    {
      "epoch": 9.41104575662669,
      "grad_norm": 3.315833330154419,
      "learning_rate": 4.215746186947776e-05,
      "loss": 1.8783,
      "step": 123200
    },
    {
      "epoch": 9.418684592468107,
      "grad_norm": 5.901674270629883,
      "learning_rate": 4.215109617294324e-05,
      "loss": 1.8781,
      "step": 123300
    },
    {
      "epoch": 9.426323428309527,
      "grad_norm": 5.911020278930664,
      "learning_rate": 4.2144730476408726e-05,
      "loss": 1.8562,
      "step": 123400
    },
    {
      "epoch": 9.433962264150944,
      "grad_norm": 5.382162094116211,
      "learning_rate": 4.213836477987422e-05,
      "loss": 1.9325,
      "step": 123500
    },
    {
      "epoch": 9.441601099992361,
      "grad_norm": 4.641994953155518,
      "learning_rate": 4.21319990833397e-05,
      "loss": 1.8539,
      "step": 123600
    },
    {
      "epoch": 9.449239935833779,
      "grad_norm": 5.543191432952881,
      "learning_rate": 4.2125633386805184e-05,
      "loss": 1.8122,
      "step": 123700
    },
    {
      "epoch": 9.456878771675196,
      "grad_norm": 5.760679721832275,
      "learning_rate": 4.211926769027067e-05,
      "loss": 1.8395,
      "step": 123800
    },
    {
      "epoch": 9.464517607516614,
      "grad_norm": 4.609614372253418,
      "learning_rate": 4.211290199373616e-05,
      "loss": 1.9207,
      "step": 123900
    },
    {
      "epoch": 9.472156443358033,
      "grad_norm": 6.910390377044678,
      "learning_rate": 4.210653629720164e-05,
      "loss": 1.8921,
      "step": 124000
    },
    {
      "epoch": 9.47979527919945,
      "grad_norm": 5.661477088928223,
      "learning_rate": 4.2100170600667125e-05,
      "loss": 1.8407,
      "step": 124100
    },
    {
      "epoch": 9.487434115040868,
      "grad_norm": 5.075596809387207,
      "learning_rate": 4.209380490413261e-05,
      "loss": 1.9265,
      "step": 124200
    },
    {
      "epoch": 9.495072950882285,
      "grad_norm": 8.298215866088867,
      "learning_rate": 4.208743920759809e-05,
      "loss": 1.8817,
      "step": 124300
    },
    {
      "epoch": 9.502711786723703,
      "grad_norm": 5.470483303070068,
      "learning_rate": 4.208107351106358e-05,
      "loss": 1.8689,
      "step": 124400
    },
    {
      "epoch": 9.51035062256512,
      "grad_norm": 4.71275520324707,
      "learning_rate": 4.2074707814529066e-05,
      "loss": 1.8318,
      "step": 124500
    },
    {
      "epoch": 9.51798945840654,
      "grad_norm": 6.651081562042236,
      "learning_rate": 4.2068342117994556e-05,
      "loss": 1.8326,
      "step": 124600
    },
    {
      "epoch": 9.525628294247957,
      "grad_norm": 5.828207969665527,
      "learning_rate": 4.206197642146004e-05,
      "loss": 1.9193,
      "step": 124700
    },
    {
      "epoch": 9.533267130089374,
      "grad_norm": 3.998602867126465,
      "learning_rate": 4.205561072492552e-05,
      "loss": 1.9134,
      "step": 124800
    },
    {
      "epoch": 9.540905965930792,
      "grad_norm": 7.818151473999023,
      "learning_rate": 4.204924502839101e-05,
      "loss": 1.7708,
      "step": 124900
    },
    {
      "epoch": 9.54854480177221,
      "grad_norm": 4.606949806213379,
      "learning_rate": 4.20428793318565e-05,
      "loss": 1.8859,
      "step": 125000
    },
    {
      "epoch": 9.556183637613628,
      "grad_norm": 4.709493637084961,
      "learning_rate": 4.203651363532198e-05,
      "loss": 1.8971,
      "step": 125100
    },
    {
      "epoch": 9.563822473455046,
      "grad_norm": 5.737884998321533,
      "learning_rate": 4.2030147938787464e-05,
      "loss": 1.8805,
      "step": 125200
    },
    {
      "epoch": 9.571461309296463,
      "grad_norm": 4.542520046234131,
      "learning_rate": 4.2023782242252954e-05,
      "loss": 1.8677,
      "step": 125300
    },
    {
      "epoch": 9.57910014513788,
      "grad_norm": 5.496220111846924,
      "learning_rate": 4.201741654571844e-05,
      "loss": 1.8209,
      "step": 125400
    },
    {
      "epoch": 9.586738980979298,
      "grad_norm": 5.959680080413818,
      "learning_rate": 4.201105084918392e-05,
      "loss": 1.9372,
      "step": 125500
    },
    {
      "epoch": 9.594377816820717,
      "grad_norm": 5.4805731773376465,
      "learning_rate": 4.2004685152649405e-05,
      "loss": 1.8438,
      "step": 125600
    },
    {
      "epoch": 9.602016652662135,
      "grad_norm": 4.435595512390137,
      "learning_rate": 4.199831945611489e-05,
      "loss": 1.9546,
      "step": 125700
    },
    {
      "epoch": 9.609655488503552,
      "grad_norm": 5.016106605529785,
      "learning_rate": 4.199195375958038e-05,
      "loss": 1.904,
      "step": 125800
    },
    {
      "epoch": 9.61729432434497,
      "grad_norm": 3.99311900138855,
      "learning_rate": 4.198558806304586e-05,
      "loss": 1.8968,
      "step": 125900
    },
    {
      "epoch": 9.624933160186387,
      "grad_norm": 6.1620707511901855,
      "learning_rate": 4.1979222366511346e-05,
      "loss": 1.7323,
      "step": 126000
    },
    {
      "epoch": 9.632571996027805,
      "grad_norm": 5.217924118041992,
      "learning_rate": 4.197285666997683e-05,
      "loss": 1.8148,
      "step": 126100
    },
    {
      "epoch": 9.640210831869224,
      "grad_norm": 5.210739612579346,
      "learning_rate": 4.196649097344232e-05,
      "loss": 1.947,
      "step": 126200
    },
    {
      "epoch": 9.647849667710641,
      "grad_norm": 4.119085788726807,
      "learning_rate": 4.19601252769078e-05,
      "loss": 1.8302,
      "step": 126300
    },
    {
      "epoch": 9.655488503552059,
      "grad_norm": 6.8705315589904785,
      "learning_rate": 4.195375958037329e-05,
      "loss": 1.8705,
      "step": 126400
    },
    {
      "epoch": 9.663127339393476,
      "grad_norm": 4.17591667175293,
      "learning_rate": 4.194739388383877e-05,
      "loss": 1.8664,
      "step": 126500
    },
    {
      "epoch": 9.670766175234894,
      "grad_norm": 5.9550957679748535,
      "learning_rate": 4.1941028187304254e-05,
      "loss": 1.966,
      "step": 126600
    },
    {
      "epoch": 9.678405011076311,
      "grad_norm": 5.031418323516846,
      "learning_rate": 4.1934662490769744e-05,
      "loss": 1.8793,
      "step": 126700
    },
    {
      "epoch": 9.68604384691773,
      "grad_norm": 4.082370281219482,
      "learning_rate": 4.192829679423523e-05,
      "loss": 1.9938,
      "step": 126800
    },
    {
      "epoch": 9.693682682759148,
      "grad_norm": 7.381651401519775,
      "learning_rate": 4.192193109770071e-05,
      "loss": 1.8809,
      "step": 126900
    },
    {
      "epoch": 9.701321518600565,
      "grad_norm": 5.146286964416504,
      "learning_rate": 4.1915565401166195e-05,
      "loss": 1.9256,
      "step": 127000
    },
    {
      "epoch": 9.708960354441983,
      "grad_norm": 5.585173606872559,
      "learning_rate": 4.1909199704631685e-05,
      "loss": 1.889,
      "step": 127100
    },
    {
      "epoch": 9.7165991902834,
      "grad_norm": 5.712351322174072,
      "learning_rate": 4.190283400809717e-05,
      "loss": 1.8816,
      "step": 127200
    },
    {
      "epoch": 9.72423802612482,
      "grad_norm": 6.949220180511475,
      "learning_rate": 4.189646831156265e-05,
      "loss": 1.9192,
      "step": 127300
    },
    {
      "epoch": 9.731876861966237,
      "grad_norm": 7.271574974060059,
      "learning_rate": 4.1890102615028136e-05,
      "loss": 1.9265,
      "step": 127400
    },
    {
      "epoch": 9.739515697807654,
      "grad_norm": 5.091915130615234,
      "learning_rate": 4.188373691849362e-05,
      "loss": 1.8336,
      "step": 127500
    },
    {
      "epoch": 9.747154533649072,
      "grad_norm": 5.6979899406433105,
      "learning_rate": 4.187737122195911e-05,
      "loss": 1.8763,
      "step": 127600
    },
    {
      "epoch": 9.75479336949049,
      "grad_norm": 5.278911590576172,
      "learning_rate": 4.187100552542459e-05,
      "loss": 1.8718,
      "step": 127700
    },
    {
      "epoch": 9.762432205331907,
      "grad_norm": 4.8035736083984375,
      "learning_rate": 4.1864639828890076e-05,
      "loss": 1.7647,
      "step": 127800
    },
    {
      "epoch": 9.770071041173326,
      "grad_norm": 5.348084449768066,
      "learning_rate": 4.185827413235556e-05,
      "loss": 1.8905,
      "step": 127900
    },
    {
      "epoch": 9.777709877014743,
      "grad_norm": 9.652613639831543,
      "learning_rate": 4.185190843582105e-05,
      "loss": 1.9547,
      "step": 128000
    },
    {
      "epoch": 9.78534871285616,
      "grad_norm": 5.687643527984619,
      "learning_rate": 4.1845542739286534e-05,
      "loss": 1.9183,
      "step": 128100
    },
    {
      "epoch": 9.792987548697578,
      "grad_norm": 4.483093738555908,
      "learning_rate": 4.183917704275202e-05,
      "loss": 1.8474,
      "step": 128200
    },
    {
      "epoch": 9.800626384538996,
      "grad_norm": 4.2661967277526855,
      "learning_rate": 4.18328113462175e-05,
      "loss": 1.8743,
      "step": 128300
    },
    {
      "epoch": 9.808265220380415,
      "grad_norm": 5.443153381347656,
      "learning_rate": 4.182644564968299e-05,
      "loss": 1.8934,
      "step": 128400
    },
    {
      "epoch": 9.815904056221832,
      "grad_norm": 4.895206451416016,
      "learning_rate": 4.1820079953148475e-05,
      "loss": 1.85,
      "step": 128500
    },
    {
      "epoch": 9.82354289206325,
      "grad_norm": 5.329528331756592,
      "learning_rate": 4.181371425661396e-05,
      "loss": 1.8914,
      "step": 128600
    },
    {
      "epoch": 9.831181727904667,
      "grad_norm": 5.231879711151123,
      "learning_rate": 4.180734856007945e-05,
      "loss": 1.8985,
      "step": 128700
    },
    {
      "epoch": 9.838820563746085,
      "grad_norm": 4.843310356140137,
      "learning_rate": 4.180098286354493e-05,
      "loss": 1.8222,
      "step": 128800
    },
    {
      "epoch": 9.846459399587502,
      "grad_norm": 4.029630661010742,
      "learning_rate": 4.1794617167010416e-05,
      "loss": 1.8532,
      "step": 128900
    },
    {
      "epoch": 9.854098235428921,
      "grad_norm": 6.558986186981201,
      "learning_rate": 4.1788251470475906e-05,
      "loss": 1.8581,
      "step": 129000
    },
    {
      "epoch": 9.861737071270339,
      "grad_norm": 3.8280370235443115,
      "learning_rate": 4.178188577394139e-05,
      "loss": 1.8576,
      "step": 129100
    },
    {
      "epoch": 9.869375907111756,
      "grad_norm": 3.973668098449707,
      "learning_rate": 4.177552007740687e-05,
      "loss": 1.8402,
      "step": 129200
    },
    {
      "epoch": 9.877014742953174,
      "grad_norm": 4.2843122482299805,
      "learning_rate": 4.176915438087236e-05,
      "loss": 1.974,
      "step": 129300
    },
    {
      "epoch": 9.884653578794591,
      "grad_norm": 5.337863445281982,
      "learning_rate": 4.176278868433785e-05,
      "loss": 1.9647,
      "step": 129400
    },
    {
      "epoch": 9.89229241463601,
      "grad_norm": 4.8928704261779785,
      "learning_rate": 4.175642298780333e-05,
      "loss": 1.7653,
      "step": 129500
    },
    {
      "epoch": 9.899931250477428,
      "grad_norm": 6.218359470367432,
      "learning_rate": 4.1750057291268814e-05,
      "loss": 1.8837,
      "step": 129600
    },
    {
      "epoch": 9.907570086318845,
      "grad_norm": 4.917843818664551,
      "learning_rate": 4.17436915947343e-05,
      "loss": 1.9098,
      "step": 129700
    },
    {
      "epoch": 9.915208922160263,
      "grad_norm": 4.470651626586914,
      "learning_rate": 4.173732589819978e-05,
      "loss": 1.9445,
      "step": 129800
    },
    {
      "epoch": 9.92284775800168,
      "grad_norm": 6.626383304595947,
      "learning_rate": 4.173096020166527e-05,
      "loss": 1.8335,
      "step": 129900
    },
    {
      "epoch": 9.930486593843098,
      "grad_norm": 4.804535388946533,
      "learning_rate": 4.1724594505130755e-05,
      "loss": 1.9758,
      "step": 130000
    },
    {
      "epoch": 9.938125429684517,
      "grad_norm": 5.388918399810791,
      "learning_rate": 4.171822880859624e-05,
      "loss": 1.9383,
      "step": 130100
    },
    {
      "epoch": 9.945764265525934,
      "grad_norm": 5.366269111633301,
      "learning_rate": 4.171186311206172e-05,
      "loss": 1.7891,
      "step": 130200
    },
    {
      "epoch": 9.953403101367352,
      "grad_norm": 5.391961574554443,
      "learning_rate": 4.170549741552721e-05,
      "loss": 1.7587,
      "step": 130300
    },
    {
      "epoch": 9.961041937208769,
      "grad_norm": 6.334890842437744,
      "learning_rate": 4.1699131718992696e-05,
      "loss": 1.9505,
      "step": 130400
    },
    {
      "epoch": 9.968680773050187,
      "grad_norm": 5.848935127258301,
      "learning_rate": 4.169276602245818e-05,
      "loss": 1.8868,
      "step": 130500
    },
    {
      "epoch": 9.976319608891606,
      "grad_norm": 5.798586368560791,
      "learning_rate": 4.168640032592366e-05,
      "loss": 1.8989,
      "step": 130600
    },
    {
      "epoch": 9.983958444733023,
      "grad_norm": 4.251210689544678,
      "learning_rate": 4.1680034629389146e-05,
      "loss": 1.8534,
      "step": 130700
    },
    {
      "epoch": 9.99159728057444,
      "grad_norm": 4.301126956939697,
      "learning_rate": 4.167366893285464e-05,
      "loss": 1.9002,
      "step": 130800
    },
    {
      "epoch": 9.999236116415858,
      "grad_norm": 4.404673099517822,
      "learning_rate": 4.166730323632012e-05,
      "loss": 1.8323,
      "step": 130900
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8609400987625122,
      "eval_runtime": 1.5156,
      "eval_samples_per_second": 455.268,
      "eval_steps_per_second": 455.268,
      "step": 130910
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.6505169868469238,
      "eval_runtime": 28.1245,
      "eval_samples_per_second": 465.466,
      "eval_steps_per_second": 465.466,
      "step": 130910
    },
    {
      "epoch": 10.006874952257276,
      "grad_norm": 6.706752777099609,
      "learning_rate": 4.1660937539785604e-05,
      "loss": 1.7926,
      "step": 131000
    },
    {
      "epoch": 10.014513788098693,
      "grad_norm": 6.633245468139648,
      "learning_rate": 4.165457184325109e-05,
      "loss": 1.779,
      "step": 131100
    },
    {
      "epoch": 10.022152623940112,
      "grad_norm": 4.234188079833984,
      "learning_rate": 4.164820614671657e-05,
      "loss": 1.8688,
      "step": 131200
    },
    {
      "epoch": 10.02979145978153,
      "grad_norm": 4.446294784545898,
      "learning_rate": 4.164184045018206e-05,
      "loss": 1.7641,
      "step": 131300
    },
    {
      "epoch": 10.037430295622947,
      "grad_norm": 4.486045837402344,
      "learning_rate": 4.1635474753647545e-05,
      "loss": 1.8978,
      "step": 131400
    },
    {
      "epoch": 10.045069131464365,
      "grad_norm": 6.434033393859863,
      "learning_rate": 4.162910905711303e-05,
      "loss": 1.8012,
      "step": 131500
    },
    {
      "epoch": 10.052707967305782,
      "grad_norm": 4.881091117858887,
      "learning_rate": 4.162274336057851e-05,
      "loss": 1.9118,
      "step": 131600
    },
    {
      "epoch": 10.060346803147201,
      "grad_norm": 5.291952133178711,
      "learning_rate": 4.1616377664044e-05,
      "loss": 1.8439,
      "step": 131700
    },
    {
      "epoch": 10.067985638988619,
      "grad_norm": 4.870322227478027,
      "learning_rate": 4.1610011967509486e-05,
      "loss": 1.9357,
      "step": 131800
    },
    {
      "epoch": 10.075624474830036,
      "grad_norm": 5.011058807373047,
      "learning_rate": 4.160364627097497e-05,
      "loss": 1.7822,
      "step": 131900
    },
    {
      "epoch": 10.083263310671454,
      "grad_norm": 5.80817174911499,
      "learning_rate": 4.159728057444045e-05,
      "loss": 1.848,
      "step": 132000
    },
    {
      "epoch": 10.090902146512871,
      "grad_norm": 6.6157732009887695,
      "learning_rate": 4.159091487790594e-05,
      "loss": 1.8538,
      "step": 132100
    },
    {
      "epoch": 10.098540982354288,
      "grad_norm": 4.5814208984375,
      "learning_rate": 4.158454918137143e-05,
      "loss": 1.9095,
      "step": 132200
    },
    {
      "epoch": 10.106179818195708,
      "grad_norm": 6.971556663513184,
      "learning_rate": 4.157818348483691e-05,
      "loss": 1.8294,
      "step": 132300
    },
    {
      "epoch": 10.113818654037125,
      "grad_norm": 5.425800323486328,
      "learning_rate": 4.15718177883024e-05,
      "loss": 1.8543,
      "step": 132400
    },
    {
      "epoch": 10.121457489878543,
      "grad_norm": 6.554374694824219,
      "learning_rate": 4.1565452091767884e-05,
      "loss": 1.8416,
      "step": 132500
    },
    {
      "epoch": 10.12909632571996,
      "grad_norm": 5.202113151550293,
      "learning_rate": 4.155908639523337e-05,
      "loss": 1.8951,
      "step": 132600
    },
    {
      "epoch": 10.136735161561377,
      "grad_norm": 4.425475597381592,
      "learning_rate": 4.155272069869886e-05,
      "loss": 1.7914,
      "step": 132700
    },
    {
      "epoch": 10.144373997402797,
      "grad_norm": 4.710936546325684,
      "learning_rate": 4.154635500216434e-05,
      "loss": 1.8135,
      "step": 132800
    },
    {
      "epoch": 10.152012833244214,
      "grad_norm": 4.732395648956299,
      "learning_rate": 4.1539989305629825e-05,
      "loss": 1.8298,
      "step": 132900
    },
    {
      "epoch": 10.159651669085632,
      "grad_norm": 6.240360736846924,
      "learning_rate": 4.153362360909531e-05,
      "loss": 1.9237,
      "step": 133000
    },
    {
      "epoch": 10.167290504927049,
      "grad_norm": 4.416039943695068,
      "learning_rate": 4.15272579125608e-05,
      "loss": 1.8519,
      "step": 133100
    },
    {
      "epoch": 10.174929340768466,
      "grad_norm": 5.785648345947266,
      "learning_rate": 4.152089221602628e-05,
      "loss": 1.849,
      "step": 133200
    },
    {
      "epoch": 10.182568176609884,
      "grad_norm": 5.714556694030762,
      "learning_rate": 4.1514526519491766e-05,
      "loss": 1.8252,
      "step": 133300
    },
    {
      "epoch": 10.190207012451303,
      "grad_norm": 5.023358345031738,
      "learning_rate": 4.150816082295725e-05,
      "loss": 1.8693,
      "step": 133400
    },
    {
      "epoch": 10.19784584829272,
      "grad_norm": 4.774245262145996,
      "learning_rate": 4.150179512642274e-05,
      "loss": 1.8365,
      "step": 133500
    },
    {
      "epoch": 10.205484684134138,
      "grad_norm": 5.231987476348877,
      "learning_rate": 4.149542942988822e-05,
      "loss": 1.9784,
      "step": 133600
    },
    {
      "epoch": 10.213123519975555,
      "grad_norm": 8.737183570861816,
      "learning_rate": 4.148906373335371e-05,
      "loss": 1.8856,
      "step": 133700
    },
    {
      "epoch": 10.220762355816973,
      "grad_norm": 3.9530694484710693,
      "learning_rate": 4.148269803681919e-05,
      "loss": 1.7864,
      "step": 133800
    },
    {
      "epoch": 10.228401191658392,
      "grad_norm": 4.6878180503845215,
      "learning_rate": 4.1476332340284674e-05,
      "loss": 1.8359,
      "step": 133900
    },
    {
      "epoch": 10.23604002749981,
      "grad_norm": 8.272879600524902,
      "learning_rate": 4.1469966643750164e-05,
      "loss": 1.8554,
      "step": 134000
    },
    {
      "epoch": 10.243678863341227,
      "grad_norm": 4.511852741241455,
      "learning_rate": 4.146360094721565e-05,
      "loss": 1.8915,
      "step": 134100
    },
    {
      "epoch": 10.251317699182644,
      "grad_norm": 4.114581108093262,
      "learning_rate": 4.145723525068113e-05,
      "loss": 1.8519,
      "step": 134200
    },
    {
      "epoch": 10.258956535024062,
      "grad_norm": 4.496776103973389,
      "learning_rate": 4.1450869554146615e-05,
      "loss": 1.9006,
      "step": 134300
    },
    {
      "epoch": 10.26659537086548,
      "grad_norm": 5.465458393096924,
      "learning_rate": 4.14445038576121e-05,
      "loss": 1.8957,
      "step": 134400
    },
    {
      "epoch": 10.274234206706899,
      "grad_norm": 4.327534198760986,
      "learning_rate": 4.143813816107759e-05,
      "loss": 1.9952,
      "step": 134500
    },
    {
      "epoch": 10.281873042548316,
      "grad_norm": 4.981780529022217,
      "learning_rate": 4.143177246454307e-05,
      "loss": 1.8758,
      "step": 134600
    },
    {
      "epoch": 10.289511878389733,
      "grad_norm": 5.00352668762207,
      "learning_rate": 4.1425406768008556e-05,
      "loss": 1.8673,
      "step": 134700
    },
    {
      "epoch": 10.297150714231151,
      "grad_norm": 3.951958179473877,
      "learning_rate": 4.141904107147404e-05,
      "loss": 1.8883,
      "step": 134800
    },
    {
      "epoch": 10.304789550072568,
      "grad_norm": 4.300780296325684,
      "learning_rate": 4.141267537493953e-05,
      "loss": 1.8061,
      "step": 134900
    },
    {
      "epoch": 10.312428385913986,
      "grad_norm": 5.41746711730957,
      "learning_rate": 4.140630967840501e-05,
      "loss": 1.8939,
      "step": 135000
    },
    {
      "epoch": 10.320067221755405,
      "grad_norm": 6.924165725708008,
      "learning_rate": 4.13999439818705e-05,
      "loss": 1.8618,
      "step": 135100
    },
    {
      "epoch": 10.327706057596822,
      "grad_norm": 6.015490531921387,
      "learning_rate": 4.139357828533598e-05,
      "loss": 1.8787,
      "step": 135200
    },
    {
      "epoch": 10.33534489343824,
      "grad_norm": 4.9992780685424805,
      "learning_rate": 4.1387212588801464e-05,
      "loss": 1.8006,
      "step": 135300
    },
    {
      "epoch": 10.342983729279657,
      "grad_norm": 4.854994297027588,
      "learning_rate": 4.1380846892266954e-05,
      "loss": 1.8732,
      "step": 135400
    },
    {
      "epoch": 10.350622565121075,
      "grad_norm": 6.1212873458862305,
      "learning_rate": 4.137448119573244e-05,
      "loss": 1.7294,
      "step": 135500
    },
    {
      "epoch": 10.358261400962494,
      "grad_norm": 5.907104969024658,
      "learning_rate": 4.136811549919792e-05,
      "loss": 1.7557,
      "step": 135600
    },
    {
      "epoch": 10.365900236803911,
      "grad_norm": 5.25717830657959,
      "learning_rate": 4.1361749802663405e-05,
      "loss": 1.8498,
      "step": 135700
    },
    {
      "epoch": 10.373539072645329,
      "grad_norm": 5.197629928588867,
      "learning_rate": 4.1355384106128895e-05,
      "loss": 1.906,
      "step": 135800
    },
    {
      "epoch": 10.381177908486746,
      "grad_norm": 5.263003826141357,
      "learning_rate": 4.134901840959438e-05,
      "loss": 1.8158,
      "step": 135900
    },
    {
      "epoch": 10.388816744328164,
      "grad_norm": 6.090213298797607,
      "learning_rate": 4.134265271305986e-05,
      "loss": 1.8921,
      "step": 136000
    },
    {
      "epoch": 10.396455580169583,
      "grad_norm": 5.418923377990723,
      "learning_rate": 4.133628701652535e-05,
      "loss": 1.8988,
      "step": 136100
    },
    {
      "epoch": 10.404094416011,
      "grad_norm": 5.29148006439209,
      "learning_rate": 4.1329921319990836e-05,
      "loss": 1.8728,
      "step": 136200
    },
    {
      "epoch": 10.411733251852418,
      "grad_norm": 4.665146827697754,
      "learning_rate": 4.132355562345632e-05,
      "loss": 1.858,
      "step": 136300
    },
    {
      "epoch": 10.419372087693835,
      "grad_norm": 5.786968231201172,
      "learning_rate": 4.131718992692181e-05,
      "loss": 1.9216,
      "step": 136400
    },
    {
      "epoch": 10.427010923535253,
      "grad_norm": 5.112220287322998,
      "learning_rate": 4.131082423038729e-05,
      "loss": 1.8881,
      "step": 136500
    },
    {
      "epoch": 10.43464975937667,
      "grad_norm": 4.835946559906006,
      "learning_rate": 4.130445853385278e-05,
      "loss": 1.9062,
      "step": 136600
    },
    {
      "epoch": 10.44228859521809,
      "grad_norm": 3.890803098678589,
      "learning_rate": 4.129809283731827e-05,
      "loss": 1.8538,
      "step": 136700
    },
    {
      "epoch": 10.449927431059507,
      "grad_norm": 5.880978584289551,
      "learning_rate": 4.129172714078375e-05,
      "loss": 1.8582,
      "step": 136800
    },
    {
      "epoch": 10.457566266900924,
      "grad_norm": 4.826455593109131,
      "learning_rate": 4.1285361444249234e-05,
      "loss": 1.7866,
      "step": 136900
    },
    {
      "epoch": 10.465205102742342,
      "grad_norm": 4.935458660125732,
      "learning_rate": 4.127899574771472e-05,
      "loss": 1.8268,
      "step": 137000
    },
    {
      "epoch": 10.47284393858376,
      "grad_norm": 5.532260894775391,
      "learning_rate": 4.12726300511802e-05,
      "loss": 1.8422,
      "step": 137100
    },
    {
      "epoch": 10.480482774425177,
      "grad_norm": 5.067915439605713,
      "learning_rate": 4.126626435464569e-05,
      "loss": 1.872,
      "step": 137200
    },
    {
      "epoch": 10.488121610266596,
      "grad_norm": 5.032346248626709,
      "learning_rate": 4.1259898658111175e-05,
      "loss": 1.8261,
      "step": 137300
    },
    {
      "epoch": 10.495760446108013,
      "grad_norm": 6.992937088012695,
      "learning_rate": 4.125353296157666e-05,
      "loss": 1.755,
      "step": 137400
    },
    {
      "epoch": 10.50339928194943,
      "grad_norm": 6.736015319824219,
      "learning_rate": 4.124716726504214e-05,
      "loss": 1.8402,
      "step": 137500
    },
    {
      "epoch": 10.511038117790848,
      "grad_norm": 6.218776702880859,
      "learning_rate": 4.1240801568507626e-05,
      "loss": 1.8865,
      "step": 137600
    },
    {
      "epoch": 10.518676953632266,
      "grad_norm": 6.146563529968262,
      "learning_rate": 4.1234435871973116e-05,
      "loss": 1.8355,
      "step": 137700
    },
    {
      "epoch": 10.526315789473685,
      "grad_norm": 6.670068740844727,
      "learning_rate": 4.12280701754386e-05,
      "loss": 1.861,
      "step": 137800
    },
    {
      "epoch": 10.533954625315102,
      "grad_norm": 4.4622111320495605,
      "learning_rate": 4.122170447890408e-05,
      "loss": 1.8498,
      "step": 137900
    },
    {
      "epoch": 10.54159346115652,
      "grad_norm": 4.456503868103027,
      "learning_rate": 4.1215338782369567e-05,
      "loss": 1.7545,
      "step": 138000
    },
    {
      "epoch": 10.549232296997937,
      "grad_norm": 4.566029071807861,
      "learning_rate": 4.120897308583506e-05,
      "loss": 1.8538,
      "step": 138100
    },
    {
      "epoch": 10.556871132839355,
      "grad_norm": 6.164745330810547,
      "learning_rate": 4.120260738930054e-05,
      "loss": 1.9064,
      "step": 138200
    },
    {
      "epoch": 10.564509968680774,
      "grad_norm": 4.868379592895508,
      "learning_rate": 4.1196241692766024e-05,
      "loss": 1.8803,
      "step": 138300
    },
    {
      "epoch": 10.572148804522191,
      "grad_norm": 6.58086633682251,
      "learning_rate": 4.118987599623151e-05,
      "loss": 2.0003,
      "step": 138400
    },
    {
      "epoch": 10.579787640363609,
      "grad_norm": 4.3243408203125,
      "learning_rate": 4.118351029969699e-05,
      "loss": 1.8038,
      "step": 138500
    },
    {
      "epoch": 10.587426476205026,
      "grad_norm": 6.067200660705566,
      "learning_rate": 4.117714460316248e-05,
      "loss": 1.7198,
      "step": 138600
    },
    {
      "epoch": 10.595065312046444,
      "grad_norm": 5.443403244018555,
      "learning_rate": 4.1170778906627965e-05,
      "loss": 1.912,
      "step": 138700
    },
    {
      "epoch": 10.602704147887861,
      "grad_norm": 5.971498012542725,
      "learning_rate": 4.116441321009345e-05,
      "loss": 1.9144,
      "step": 138800
    },
    {
      "epoch": 10.61034298372928,
      "grad_norm": 5.211988925933838,
      "learning_rate": 4.115804751355893e-05,
      "loss": 1.9106,
      "step": 138900
    },
    {
      "epoch": 10.617981819570698,
      "grad_norm": 6.812435150146484,
      "learning_rate": 4.115168181702442e-05,
      "loss": 1.8705,
      "step": 139000
    },
    {
      "epoch": 10.625620655412115,
      "grad_norm": 4.841569423675537,
      "learning_rate": 4.1145316120489906e-05,
      "loss": 1.9087,
      "step": 139100
    },
    {
      "epoch": 10.633259491253533,
      "grad_norm": 4.803815841674805,
      "learning_rate": 4.113895042395539e-05,
      "loss": 1.8929,
      "step": 139200
    },
    {
      "epoch": 10.64089832709495,
      "grad_norm": 4.176607608795166,
      "learning_rate": 4.113258472742087e-05,
      "loss": 1.9216,
      "step": 139300
    },
    {
      "epoch": 10.648537162936368,
      "grad_norm": 5.03771448135376,
      "learning_rate": 4.1126219030886356e-05,
      "loss": 1.8806,
      "step": 139400
    },
    {
      "epoch": 10.656175998777787,
      "grad_norm": 4.333748817443848,
      "learning_rate": 4.111985333435185e-05,
      "loss": 1.8053,
      "step": 139500
    },
    {
      "epoch": 10.663814834619204,
      "grad_norm": 7.166929244995117,
      "learning_rate": 4.111348763781733e-05,
      "loss": 1.9275,
      "step": 139600
    },
    {
      "epoch": 10.671453670460622,
      "grad_norm": 6.710169315338135,
      "learning_rate": 4.1107121941282814e-05,
      "loss": 1.8683,
      "step": 139700
    },
    {
      "epoch": 10.67909250630204,
      "grad_norm": 5.458677291870117,
      "learning_rate": 4.1100756244748304e-05,
      "loss": 1.8942,
      "step": 139800
    },
    {
      "epoch": 10.686731342143457,
      "grad_norm": 6.161677360534668,
      "learning_rate": 4.109439054821379e-05,
      "loss": 1.8538,
      "step": 139900
    },
    {
      "epoch": 10.694370177984876,
      "grad_norm": 9.385972023010254,
      "learning_rate": 4.108802485167927e-05,
      "loss": 1.8729,
      "step": 140000
    },
    {
      "epoch": 10.702009013826293,
      "grad_norm": 6.973222255706787,
      "learning_rate": 4.108165915514476e-05,
      "loss": 1.8448,
      "step": 140100
    },
    {
      "epoch": 10.70964784966771,
      "grad_norm": 4.954450607299805,
      "learning_rate": 4.1075293458610245e-05,
      "loss": 1.8255,
      "step": 140200
    },
    {
      "epoch": 10.717286685509128,
      "grad_norm": 6.767470359802246,
      "learning_rate": 4.106892776207573e-05,
      "loss": 1.9229,
      "step": 140300
    },
    {
      "epoch": 10.724925521350546,
      "grad_norm": 5.627338886260986,
      "learning_rate": 4.106256206554122e-05,
      "loss": 1.8237,
      "step": 140400
    },
    {
      "epoch": 10.732564357191965,
      "grad_norm": 4.911874294281006,
      "learning_rate": 4.10561963690067e-05,
      "loss": 1.8315,
      "step": 140500
    },
    {
      "epoch": 10.740203193033382,
      "grad_norm": 6.691699504852295,
      "learning_rate": 4.1049830672472186e-05,
      "loss": 1.8739,
      "step": 140600
    },
    {
      "epoch": 10.7478420288748,
      "grad_norm": 3.7525007724761963,
      "learning_rate": 4.104346497593767e-05,
      "loss": 1.9145,
      "step": 140700
    },
    {
      "epoch": 10.755480864716217,
      "grad_norm": 6.60238790512085,
      "learning_rate": 4.103709927940315e-05,
      "loss": 1.8315,
      "step": 140800
    },
    {
      "epoch": 10.763119700557635,
      "grad_norm": 5.26823091506958,
      "learning_rate": 4.103073358286864e-05,
      "loss": 1.8659,
      "step": 140900
    },
    {
      "epoch": 10.770758536399052,
      "grad_norm": 5.843709945678711,
      "learning_rate": 4.102436788633413e-05,
      "loss": 1.881,
      "step": 141000
    },
    {
      "epoch": 10.778397372240471,
      "grad_norm": 4.0984625816345215,
      "learning_rate": 4.101800218979961e-05,
      "loss": 1.8526,
      "step": 141100
    },
    {
      "epoch": 10.786036208081889,
      "grad_norm": 5.549156665802002,
      "learning_rate": 4.1011636493265094e-05,
      "loss": 1.8557,
      "step": 141200
    },
    {
      "epoch": 10.793675043923306,
      "grad_norm": 5.893410682678223,
      "learning_rate": 4.1005270796730584e-05,
      "loss": 1.8548,
      "step": 141300
    },
    {
      "epoch": 10.801313879764724,
      "grad_norm": 6.072484016418457,
      "learning_rate": 4.099890510019607e-05,
      "loss": 1.8696,
      "step": 141400
    },
    {
      "epoch": 10.808952715606141,
      "grad_norm": 4.432455539703369,
      "learning_rate": 4.099253940366155e-05,
      "loss": 1.755,
      "step": 141500
    },
    {
      "epoch": 10.816591551447559,
      "grad_norm": 4.701449394226074,
      "learning_rate": 4.0986173707127035e-05,
      "loss": 1.857,
      "step": 141600
    },
    {
      "epoch": 10.824230387288978,
      "grad_norm": 5.208026885986328,
      "learning_rate": 4.097980801059252e-05,
      "loss": 1.8903,
      "step": 141700
    },
    {
      "epoch": 10.831869223130395,
      "grad_norm": 4.4475016593933105,
      "learning_rate": 4.097344231405801e-05,
      "loss": 1.8656,
      "step": 141800
    },
    {
      "epoch": 10.839508058971813,
      "grad_norm": 4.188937187194824,
      "learning_rate": 4.096707661752349e-05,
      "loss": 1.8684,
      "step": 141900
    },
    {
      "epoch": 10.84714689481323,
      "grad_norm": 6.510696887969971,
      "learning_rate": 4.0960710920988976e-05,
      "loss": 1.8509,
      "step": 142000
    },
    {
      "epoch": 10.854785730654648,
      "grad_norm": 5.493051052093506,
      "learning_rate": 4.095434522445446e-05,
      "loss": 1.9587,
      "step": 142100
    },
    {
      "epoch": 10.862424566496067,
      "grad_norm": 5.087826728820801,
      "learning_rate": 4.094797952791995e-05,
      "loss": 1.9188,
      "step": 142200
    },
    {
      "epoch": 10.870063402337484,
      "grad_norm": 4.634488105773926,
      "learning_rate": 4.094161383138543e-05,
      "loss": 1.8679,
      "step": 142300
    },
    {
      "epoch": 10.877702238178902,
      "grad_norm": 4.747011661529541,
      "learning_rate": 4.093524813485092e-05,
      "loss": 1.8257,
      "step": 142400
    },
    {
      "epoch": 10.88534107402032,
      "grad_norm": 4.825387477874756,
      "learning_rate": 4.09288824383164e-05,
      "loss": 1.869,
      "step": 142500
    },
    {
      "epoch": 10.892979909861737,
      "grad_norm": 5.451625823974609,
      "learning_rate": 4.0922516741781884e-05,
      "loss": 1.8404,
      "step": 142600
    },
    {
      "epoch": 10.900618745703154,
      "grad_norm": 5.485662460327148,
      "learning_rate": 4.0916151045247374e-05,
      "loss": 1.8049,
      "step": 142700
    },
    {
      "epoch": 10.908257581544573,
      "grad_norm": 5.30764102935791,
      "learning_rate": 4.090978534871286e-05,
      "loss": 1.8324,
      "step": 142800
    },
    {
      "epoch": 10.91589641738599,
      "grad_norm": 6.033719062805176,
      "learning_rate": 4.090341965217834e-05,
      "loss": 1.9052,
      "step": 142900
    },
    {
      "epoch": 10.923535253227408,
      "grad_norm": 5.545665264129639,
      "learning_rate": 4.0897053955643825e-05,
      "loss": 1.8285,
      "step": 143000
    },
    {
      "epoch": 10.931174089068826,
      "grad_norm": 3.4788780212402344,
      "learning_rate": 4.089068825910931e-05,
      "loss": 1.7912,
      "step": 143100
    },
    {
      "epoch": 10.938812924910243,
      "grad_norm": 5.170256614685059,
      "learning_rate": 4.08843225625748e-05,
      "loss": 1.9421,
      "step": 143200
    },
    {
      "epoch": 10.946451760751662,
      "grad_norm": 4.952393531799316,
      "learning_rate": 4.087795686604028e-05,
      "loss": 1.8416,
      "step": 143300
    },
    {
      "epoch": 10.95409059659308,
      "grad_norm": 6.264578819274902,
      "learning_rate": 4.0871591169505766e-05,
      "loss": 1.8663,
      "step": 143400
    },
    {
      "epoch": 10.961729432434497,
      "grad_norm": 5.530966758728027,
      "learning_rate": 4.086522547297125e-05,
      "loss": 1.8676,
      "step": 143500
    },
    {
      "epoch": 10.969368268275915,
      "grad_norm": 5.250735759735107,
      "learning_rate": 4.085885977643674e-05,
      "loss": 1.8761,
      "step": 143600
    },
    {
      "epoch": 10.977007104117332,
      "grad_norm": 3.5249621868133545,
      "learning_rate": 4.085249407990222e-05,
      "loss": 1.9205,
      "step": 143700
    },
    {
      "epoch": 10.98464593995875,
      "grad_norm": 6.176119327545166,
      "learning_rate": 4.0846128383367707e-05,
      "loss": 1.8676,
      "step": 143800
    },
    {
      "epoch": 10.992284775800169,
      "grad_norm": 6.261103630065918,
      "learning_rate": 4.08397626868332e-05,
      "loss": 1.8649,
      "step": 143900
    },
    {
      "epoch": 10.999923611641586,
      "grad_norm": 5.860236644744873,
      "learning_rate": 4.083339699029868e-05,
      "loss": 1.7846,
      "step": 144000
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.8539578914642334,
      "eval_runtime": 1.4978,
      "eval_samples_per_second": 460.676,
      "eval_steps_per_second": 460.676,
      "step": 144001
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.6357455253601074,
      "eval_runtime": 27.8347,
      "eval_samples_per_second": 470.312,
      "eval_steps_per_second": 470.312,
      "step": 144001
    },
    {
      "epoch": 11.007562447483004,
      "grad_norm": 6.030253887176514,
      "learning_rate": 4.082703129376417e-05,
      "loss": 1.8429,
      "step": 144100
    },
    {
      "epoch": 11.015201283324421,
      "grad_norm": 5.655215740203857,
      "learning_rate": 4.0820665597229654e-05,
      "loss": 1.8778,
      "step": 144200
    },
    {
      "epoch": 11.022840119165839,
      "grad_norm": 3.8039684295654297,
      "learning_rate": 4.081429990069514e-05,
      "loss": 1.8906,
      "step": 144300
    },
    {
      "epoch": 11.030478955007258,
      "grad_norm": 4.9105448722839355,
      "learning_rate": 4.080793420416062e-05,
      "loss": 1.7694,
      "step": 144400
    },
    {
      "epoch": 11.038117790848675,
      "grad_norm": 5.525341510772705,
      "learning_rate": 4.080156850762611e-05,
      "loss": 1.8905,
      "step": 144500
    },
    {
      "epoch": 11.045756626690093,
      "grad_norm": 7.52587366104126,
      "learning_rate": 4.0795202811091595e-05,
      "loss": 1.7941,
      "step": 144600
    },
    {
      "epoch": 11.05339546253151,
      "grad_norm": 4.799932479858398,
      "learning_rate": 4.078883711455708e-05,
      "loss": 1.9202,
      "step": 144700
    },
    {
      "epoch": 11.061034298372928,
      "grad_norm": 6.075981140136719,
      "learning_rate": 4.078247141802256e-05,
      "loss": 1.8425,
      "step": 144800
    },
    {
      "epoch": 11.068673134214345,
      "grad_norm": 5.232790470123291,
      "learning_rate": 4.0776105721488046e-05,
      "loss": 1.8084,
      "step": 144900
    },
    {
      "epoch": 11.076311970055764,
      "grad_norm": 6.094871997833252,
      "learning_rate": 4.0769740024953536e-05,
      "loss": 1.8544,
      "step": 145000
    },
    {
      "epoch": 11.083950805897182,
      "grad_norm": 6.991426467895508,
      "learning_rate": 4.076337432841902e-05,
      "loss": 1.8595,
      "step": 145100
    },
    {
      "epoch": 11.091589641738599,
      "grad_norm": 5.294029712677002,
      "learning_rate": 4.07570086318845e-05,
      "loss": 1.8966,
      "step": 145200
    },
    {
      "epoch": 11.099228477580017,
      "grad_norm": 4.626865386962891,
      "learning_rate": 4.075064293534999e-05,
      "loss": 1.8435,
      "step": 145300
    },
    {
      "epoch": 11.106867313421434,
      "grad_norm": 5.750087738037109,
      "learning_rate": 4.074427723881548e-05,
      "loss": 1.9175,
      "step": 145400
    },
    {
      "epoch": 11.114506149262853,
      "grad_norm": 6.642927646636963,
      "learning_rate": 4.073791154228096e-05,
      "loss": 1.8167,
      "step": 145500
    },
    {
      "epoch": 11.12214498510427,
      "grad_norm": 5.722772598266602,
      "learning_rate": 4.0731545845746444e-05,
      "loss": 1.9113,
      "step": 145600
    },
    {
      "epoch": 11.129783820945688,
      "grad_norm": 5.835316181182861,
      "learning_rate": 4.072518014921193e-05,
      "loss": 1.8509,
      "step": 145700
    },
    {
      "epoch": 11.137422656787106,
      "grad_norm": 6.429225444793701,
      "learning_rate": 4.071881445267741e-05,
      "loss": 1.7761,
      "step": 145800
    },
    {
      "epoch": 11.145061492628523,
      "grad_norm": 5.024006366729736,
      "learning_rate": 4.07124487561429e-05,
      "loss": 1.9028,
      "step": 145900
    },
    {
      "epoch": 11.15270032846994,
      "grad_norm": 5.545507907867432,
      "learning_rate": 4.0706083059608385e-05,
      "loss": 1.8375,
      "step": 146000
    },
    {
      "epoch": 11.16033916431136,
      "grad_norm": 4.964239120483398,
      "learning_rate": 4.069971736307387e-05,
      "loss": 1.8269,
      "step": 146100
    },
    {
      "epoch": 11.167978000152777,
      "grad_norm": 4.984625339508057,
      "learning_rate": 4.069335166653935e-05,
      "loss": 1.8605,
      "step": 146200
    },
    {
      "epoch": 11.175616835994195,
      "grad_norm": 4.768532752990723,
      "learning_rate": 4.0686985970004836e-05,
      "loss": 1.8014,
      "step": 146300
    },
    {
      "epoch": 11.183255671835612,
      "grad_norm": 6.9097394943237305,
      "learning_rate": 4.0680620273470326e-05,
      "loss": 1.7984,
      "step": 146400
    },
    {
      "epoch": 11.19089450767703,
      "grad_norm": 6.477261066436768,
      "learning_rate": 4.067425457693581e-05,
      "loss": 1.9215,
      "step": 146500
    },
    {
      "epoch": 11.198533343518449,
      "grad_norm": 5.811918258666992,
      "learning_rate": 4.066788888040129e-05,
      "loss": 1.8741,
      "step": 146600
    },
    {
      "epoch": 11.206172179359866,
      "grad_norm": 6.403720855712891,
      "learning_rate": 4.0661523183866777e-05,
      "loss": 1.97,
      "step": 146700
    },
    {
      "epoch": 11.213811015201284,
      "grad_norm": 5.967005729675293,
      "learning_rate": 4.065515748733227e-05,
      "loss": 1.8487,
      "step": 146800
    },
    {
      "epoch": 11.221449851042701,
      "grad_norm": 4.308151721954346,
      "learning_rate": 4.064879179079775e-05,
      "loss": 1.7647,
      "step": 146900
    },
    {
      "epoch": 11.229088686884118,
      "grad_norm": 6.793034553527832,
      "learning_rate": 4.0642426094263234e-05,
      "loss": 1.7498,
      "step": 147000
    },
    {
      "epoch": 11.236727522725536,
      "grad_norm": 4.175665855407715,
      "learning_rate": 4.063606039772872e-05,
      "loss": 1.8488,
      "step": 147100
    },
    {
      "epoch": 11.244366358566955,
      "grad_norm": 4.68775749206543,
      "learning_rate": 4.06296947011942e-05,
      "loss": 1.7717,
      "step": 147200
    },
    {
      "epoch": 11.252005194408373,
      "grad_norm": 5.180821418762207,
      "learning_rate": 4.062332900465969e-05,
      "loss": 1.8166,
      "step": 147300
    },
    {
      "epoch": 11.25964403024979,
      "grad_norm": 3.584824562072754,
      "learning_rate": 4.0616963308125175e-05,
      "loss": 1.8969,
      "step": 147400
    },
    {
      "epoch": 11.267282866091207,
      "grad_norm": 4.1817193031311035,
      "learning_rate": 4.061059761159066e-05,
      "loss": 1.7764,
      "step": 147500
    },
    {
      "epoch": 11.274921701932625,
      "grad_norm": 5.5145368576049805,
      "learning_rate": 4.060423191505615e-05,
      "loss": 1.8132,
      "step": 147600
    },
    {
      "epoch": 11.282560537774042,
      "grad_norm": 5.605390548706055,
      "learning_rate": 4.059786621852163e-05,
      "loss": 1.7968,
      "step": 147700
    },
    {
      "epoch": 11.290199373615462,
      "grad_norm": 6.512953758239746,
      "learning_rate": 4.0591500521987116e-05,
      "loss": 1.827,
      "step": 147800
    },
    {
      "epoch": 11.297838209456879,
      "grad_norm": 4.446318626403809,
      "learning_rate": 4.0585134825452606e-05,
      "loss": 1.9707,
      "step": 147900
    },
    {
      "epoch": 11.305477045298296,
      "grad_norm": 5.156891345977783,
      "learning_rate": 4.057876912891809e-05,
      "loss": 1.9218,
      "step": 148000
    },
    {
      "epoch": 11.313115881139714,
      "grad_norm": 9.18498420715332,
      "learning_rate": 4.057240343238357e-05,
      "loss": 1.8539,
      "step": 148100
    },
    {
      "epoch": 11.320754716981131,
      "grad_norm": 4.518728256225586,
      "learning_rate": 4.0566037735849064e-05,
      "loss": 1.7514,
      "step": 148200
    },
    {
      "epoch": 11.32839355282255,
      "grad_norm": 5.928748607635498,
      "learning_rate": 4.055967203931455e-05,
      "loss": 1.8641,
      "step": 148300
    },
    {
      "epoch": 11.336032388663968,
      "grad_norm": 4.54873514175415,
      "learning_rate": 4.055330634278003e-05,
      "loss": 1.8798,
      "step": 148400
    },
    {
      "epoch": 11.343671224505385,
      "grad_norm": 5.19312047958374,
      "learning_rate": 4.0546940646245514e-05,
      "loss": 1.8585,
      "step": 148500
    },
    {
      "epoch": 11.351310060346803,
      "grad_norm": 4.415255546569824,
      "learning_rate": 4.0540574949711e-05,
      "loss": 1.8626,
      "step": 148600
    },
    {
      "epoch": 11.35894889618822,
      "grad_norm": 5.41362190246582,
      "learning_rate": 4.053420925317649e-05,
      "loss": 1.787,
      "step": 148700
    },
    {
      "epoch": 11.36658773202964,
      "grad_norm": 6.010000228881836,
      "learning_rate": 4.052784355664197e-05,
      "loss": 1.8603,
      "step": 148800
    },
    {
      "epoch": 11.374226567871057,
      "grad_norm": 9.882304191589355,
      "learning_rate": 4.0521477860107455e-05,
      "loss": 1.8745,
      "step": 148900
    },
    {
      "epoch": 11.381865403712474,
      "grad_norm": 4.978533744812012,
      "learning_rate": 4.051511216357294e-05,
      "loss": 1.8155,
      "step": 149000
    },
    {
      "epoch": 11.389504239553892,
      "grad_norm": 6.97443151473999,
      "learning_rate": 4.050874646703843e-05,
      "loss": 1.7664,
      "step": 149100
    },
    {
      "epoch": 11.39714307539531,
      "grad_norm": 5.12738037109375,
      "learning_rate": 4.050238077050391e-05,
      "loss": 1.9095,
      "step": 149200
    },
    {
      "epoch": 11.404781911236727,
      "grad_norm": 5.117856025695801,
      "learning_rate": 4.0496015073969396e-05,
      "loss": 1.8564,
      "step": 149300
    },
    {
      "epoch": 11.412420747078146,
      "grad_norm": 4.121090412139893,
      "learning_rate": 4.048964937743488e-05,
      "loss": 1.8645,
      "step": 149400
    },
    {
      "epoch": 11.420059582919563,
      "grad_norm": 4.225499629974365,
      "learning_rate": 4.048328368090036e-05,
      "loss": 1.7734,
      "step": 149500
    },
    {
      "epoch": 11.427698418760981,
      "grad_norm": 6.54106330871582,
      "learning_rate": 4.047691798436585e-05,
      "loss": 1.8297,
      "step": 149600
    },
    {
      "epoch": 11.435337254602398,
      "grad_norm": 4.592362880706787,
      "learning_rate": 4.047055228783134e-05,
      "loss": 1.9601,
      "step": 149700
    },
    {
      "epoch": 11.442976090443816,
      "grad_norm": 4.278637409210205,
      "learning_rate": 4.046418659129682e-05,
      "loss": 1.889,
      "step": 149800
    },
    {
      "epoch": 11.450614926285233,
      "grad_norm": 5.17339563369751,
      "learning_rate": 4.0457820894762304e-05,
      "loss": 1.8438,
      "step": 149900
    },
    {
      "epoch": 11.458253762126652,
      "grad_norm": 5.995450973510742,
      "learning_rate": 4.0451455198227794e-05,
      "loss": 1.8067,
      "step": 150000
    },
    {
      "epoch": 11.46589259796807,
      "grad_norm": 4.450592994689941,
      "learning_rate": 4.044508950169328e-05,
      "loss": 1.8528,
      "step": 150100
    },
    {
      "epoch": 11.473531433809487,
      "grad_norm": 5.719061374664307,
      "learning_rate": 4.043872380515876e-05,
      "loss": 1.7929,
      "step": 150200
    },
    {
      "epoch": 11.481170269650905,
      "grad_norm": 4.258737564086914,
      "learning_rate": 4.0432358108624245e-05,
      "loss": 1.8131,
      "step": 150300
    },
    {
      "epoch": 11.488809105492322,
      "grad_norm": 5.429071426391602,
      "learning_rate": 4.042599241208973e-05,
      "loss": 1.8391,
      "step": 150400
    },
    {
      "epoch": 11.496447941333741,
      "grad_norm": 4.997654914855957,
      "learning_rate": 4.041962671555522e-05,
      "loss": 1.8221,
      "step": 150500
    },
    {
      "epoch": 11.504086777175159,
      "grad_norm": 4.6886515617370605,
      "learning_rate": 4.04132610190207e-05,
      "loss": 1.9159,
      "step": 150600
    },
    {
      "epoch": 11.511725613016576,
      "grad_norm": 3.546384334564209,
      "learning_rate": 4.0406895322486186e-05,
      "loss": 1.8415,
      "step": 150700
    },
    {
      "epoch": 11.519364448857994,
      "grad_norm": 5.589832782745361,
      "learning_rate": 4.040052962595167e-05,
      "loss": 1.8681,
      "step": 150800
    },
    {
      "epoch": 11.527003284699411,
      "grad_norm": 5.190148830413818,
      "learning_rate": 4.039416392941716e-05,
      "loss": 1.8155,
      "step": 150900
    },
    {
      "epoch": 11.53464212054083,
      "grad_norm": 5.655953884124756,
      "learning_rate": 4.038779823288264e-05,
      "loss": 1.8286,
      "step": 151000
    },
    {
      "epoch": 11.542280956382248,
      "grad_norm": 6.620731830596924,
      "learning_rate": 4.038143253634813e-05,
      "loss": 1.815,
      "step": 151100
    },
    {
      "epoch": 11.549919792223665,
      "grad_norm": 5.195694923400879,
      "learning_rate": 4.037506683981361e-05,
      "loss": 1.7829,
      "step": 151200
    },
    {
      "epoch": 11.557558628065083,
      "grad_norm": 5.218443870544434,
      "learning_rate": 4.03687011432791e-05,
      "loss": 1.8958,
      "step": 151300
    },
    {
      "epoch": 11.5651974639065,
      "grad_norm": 5.8651838302612305,
      "learning_rate": 4.0362335446744584e-05,
      "loss": 1.7923,
      "step": 151400
    },
    {
      "epoch": 11.572836299747918,
      "grad_norm": 5.541748046875,
      "learning_rate": 4.035596975021007e-05,
      "loss": 1.9159,
      "step": 151500
    },
    {
      "epoch": 11.580475135589337,
      "grad_norm": 5.533249855041504,
      "learning_rate": 4.034960405367556e-05,
      "loss": 1.7824,
      "step": 151600
    },
    {
      "epoch": 11.588113971430754,
      "grad_norm": 5.546606063842773,
      "learning_rate": 4.034323835714104e-05,
      "loss": 1.7917,
      "step": 151700
    },
    {
      "epoch": 11.595752807272172,
      "grad_norm": 3.7614402770996094,
      "learning_rate": 4.0336872660606525e-05,
      "loss": 1.92,
      "step": 151800
    },
    {
      "epoch": 11.60339164311359,
      "grad_norm": 6.133822441101074,
      "learning_rate": 4.0330506964072015e-05,
      "loss": 1.824,
      "step": 151900
    },
    {
      "epoch": 11.611030478955007,
      "grad_norm": 4.941623210906982,
      "learning_rate": 4.03241412675375e-05,
      "loss": 1.7785,
      "step": 152000
    },
    {
      "epoch": 11.618669314796424,
      "grad_norm": 4.42753791809082,
      "learning_rate": 4.031777557100298e-05,
      "loss": 1.9185,
      "step": 152100
    },
    {
      "epoch": 11.626308150637843,
      "grad_norm": 8.931780815124512,
      "learning_rate": 4.0311409874468466e-05,
      "loss": 1.924,
      "step": 152200
    },
    {
      "epoch": 11.63394698647926,
      "grad_norm": 6.032554626464844,
      "learning_rate": 4.0305044177933956e-05,
      "loss": 1.8695,
      "step": 152300
    },
    {
      "epoch": 11.641585822320678,
      "grad_norm": 4.886072635650635,
      "learning_rate": 4.029867848139944e-05,
      "loss": 1.7983,
      "step": 152400
    },
    {
      "epoch": 11.649224658162096,
      "grad_norm": 4.721553325653076,
      "learning_rate": 4.029231278486492e-05,
      "loss": 1.7938,
      "step": 152500
    },
    {
      "epoch": 11.656863494003513,
      "grad_norm": 3.890550374984741,
      "learning_rate": 4.028594708833041e-05,
      "loss": 1.861,
      "step": 152600
    },
    {
      "epoch": 11.664502329844932,
      "grad_norm": 5.862076282501221,
      "learning_rate": 4.027958139179589e-05,
      "loss": 1.7812,
      "step": 152700
    },
    {
      "epoch": 11.67214116568635,
      "grad_norm": 9.1677885055542,
      "learning_rate": 4.027321569526138e-05,
      "loss": 1.9017,
      "step": 152800
    },
    {
      "epoch": 11.679780001527767,
      "grad_norm": 4.61500358581543,
      "learning_rate": 4.0266849998726864e-05,
      "loss": 1.923,
      "step": 152900
    },
    {
      "epoch": 11.687418837369185,
      "grad_norm": 6.499555587768555,
      "learning_rate": 4.026048430219235e-05,
      "loss": 1.9487,
      "step": 153000
    },
    {
      "epoch": 11.695057673210602,
      "grad_norm": 3.868093252182007,
      "learning_rate": 4.025411860565783e-05,
      "loss": 1.8295,
      "step": 153100
    },
    {
      "epoch": 11.702696509052021,
      "grad_norm": 5.215762138366699,
      "learning_rate": 4.024775290912332e-05,
      "loss": 1.8713,
      "step": 153200
    },
    {
      "epoch": 11.710335344893439,
      "grad_norm": 7.369318008422852,
      "learning_rate": 4.0241387212588805e-05,
      "loss": 1.819,
      "step": 153300
    },
    {
      "epoch": 11.717974180734856,
      "grad_norm": 4.3237128257751465,
      "learning_rate": 4.023502151605429e-05,
      "loss": 1.9286,
      "step": 153400
    },
    {
      "epoch": 11.725613016576274,
      "grad_norm": 5.489984035491943,
      "learning_rate": 4.022865581951977e-05,
      "loss": 1.8803,
      "step": 153500
    },
    {
      "epoch": 11.733251852417691,
      "grad_norm": 7.887074947357178,
      "learning_rate": 4.0222290122985256e-05,
      "loss": 1.8295,
      "step": 153600
    },
    {
      "epoch": 11.740890688259109,
      "grad_norm": 5.443108081817627,
      "learning_rate": 4.0215924426450746e-05,
      "loss": 1.8786,
      "step": 153700
    },
    {
      "epoch": 11.748529524100528,
      "grad_norm": 4.322096824645996,
      "learning_rate": 4.020955872991623e-05,
      "loss": 1.7924,
      "step": 153800
    },
    {
      "epoch": 11.756168359941945,
      "grad_norm": 5.2919392585754395,
      "learning_rate": 4.020319303338171e-05,
      "loss": 1.8533,
      "step": 153900
    },
    {
      "epoch": 11.763807195783363,
      "grad_norm": 5.377161026000977,
      "learning_rate": 4.01968273368472e-05,
      "loss": 1.7938,
      "step": 154000
    },
    {
      "epoch": 11.77144603162478,
      "grad_norm": 5.039187908172607,
      "learning_rate": 4.019046164031269e-05,
      "loss": 1.8258,
      "step": 154100
    },
    {
      "epoch": 11.779084867466198,
      "grad_norm": 6.513119220733643,
      "learning_rate": 4.018409594377817e-05,
      "loss": 1.8902,
      "step": 154200
    },
    {
      "epoch": 11.786723703307615,
      "grad_norm": 6.352049827575684,
      "learning_rate": 4.0177730247243654e-05,
      "loss": 1.8457,
      "step": 154300
    },
    {
      "epoch": 11.794362539149034,
      "grad_norm": 5.0807108879089355,
      "learning_rate": 4.017136455070914e-05,
      "loss": 1.8845,
      "step": 154400
    },
    {
      "epoch": 11.802001374990452,
      "grad_norm": 4.730208396911621,
      "learning_rate": 4.016499885417462e-05,
      "loss": 1.8476,
      "step": 154500
    },
    {
      "epoch": 11.80964021083187,
      "grad_norm": 5.828777313232422,
      "learning_rate": 4.015863315764011e-05,
      "loss": 1.8356,
      "step": 154600
    },
    {
      "epoch": 11.817279046673287,
      "grad_norm": 5.245401382446289,
      "learning_rate": 4.0152267461105595e-05,
      "loss": 1.9099,
      "step": 154700
    },
    {
      "epoch": 11.824917882514704,
      "grad_norm": 6.56407356262207,
      "learning_rate": 4.014590176457108e-05,
      "loss": 1.8854,
      "step": 154800
    },
    {
      "epoch": 11.832556718356123,
      "grad_norm": 5.518560409545898,
      "learning_rate": 4.013953606803656e-05,
      "loss": 1.8667,
      "step": 154900
    },
    {
      "epoch": 11.84019555419754,
      "grad_norm": 6.274546146392822,
      "learning_rate": 4.0133170371502046e-05,
      "loss": 1.9017,
      "step": 155000
    },
    {
      "epoch": 11.847834390038958,
      "grad_norm": 5.631549835205078,
      "learning_rate": 4.0126804674967536e-05,
      "loss": 1.9445,
      "step": 155100
    },
    {
      "epoch": 11.855473225880376,
      "grad_norm": 4.802612781524658,
      "learning_rate": 4.012043897843302e-05,
      "loss": 1.7471,
      "step": 155200
    },
    {
      "epoch": 11.863112061721793,
      "grad_norm": 5.760357856750488,
      "learning_rate": 4.011407328189851e-05,
      "loss": 1.8632,
      "step": 155300
    },
    {
      "epoch": 11.87075089756321,
      "grad_norm": 5.6391425132751465,
      "learning_rate": 4.010770758536399e-05,
      "loss": 1.8056,
      "step": 155400
    },
    {
      "epoch": 11.87838973340463,
      "grad_norm": 5.161311626434326,
      "learning_rate": 4.010134188882948e-05,
      "loss": 1.8837,
      "step": 155500
    },
    {
      "epoch": 11.886028569246047,
      "grad_norm": 4.777400493621826,
      "learning_rate": 4.009497619229497e-05,
      "loss": 1.8177,
      "step": 155600
    },
    {
      "epoch": 11.893667405087465,
      "grad_norm": 4.977406024932861,
      "learning_rate": 4.008861049576045e-05,
      "loss": 1.8445,
      "step": 155700
    },
    {
      "epoch": 11.901306240928882,
      "grad_norm": 5.845963478088379,
      "learning_rate": 4.0082244799225934e-05,
      "loss": 1.8455,
      "step": 155800
    },
    {
      "epoch": 11.9089450767703,
      "grad_norm": 4.666486740112305,
      "learning_rate": 4.007587910269142e-05,
      "loss": 1.8451,
      "step": 155900
    },
    {
      "epoch": 11.916583912611719,
      "grad_norm": 4.661685943603516,
      "learning_rate": 4.006951340615691e-05,
      "loss": 1.8686,
      "step": 156000
    },
    {
      "epoch": 11.924222748453136,
      "grad_norm": 5.532107830047607,
      "learning_rate": 4.006314770962239e-05,
      "loss": 1.9327,
      "step": 156100
    },
    {
      "epoch": 11.931861584294554,
      "grad_norm": 5.096184253692627,
      "learning_rate": 4.0056782013087875e-05,
      "loss": 1.9185,
      "step": 156200
    },
    {
      "epoch": 11.939500420135971,
      "grad_norm": 4.326228618621826,
      "learning_rate": 4.005041631655336e-05,
      "loss": 1.8407,
      "step": 156300
    },
    {
      "epoch": 11.947139255977389,
      "grad_norm": 5.29226541519165,
      "learning_rate": 4.004405062001885e-05,
      "loss": 1.889,
      "step": 156400
    },
    {
      "epoch": 11.954778091818806,
      "grad_norm": 3.6224963665008545,
      "learning_rate": 4.003768492348433e-05,
      "loss": 1.7874,
      "step": 156500
    },
    {
      "epoch": 11.962416927660225,
      "grad_norm": 5.3562541007995605,
      "learning_rate": 4.0031319226949816e-05,
      "loss": 1.8577,
      "step": 156600
    },
    {
      "epoch": 11.970055763501643,
      "grad_norm": 6.1767897605896,
      "learning_rate": 4.00249535304153e-05,
      "loss": 1.7607,
      "step": 156700
    },
    {
      "epoch": 11.97769459934306,
      "grad_norm": 4.895781993865967,
      "learning_rate": 4.001858783388078e-05,
      "loss": 1.9094,
      "step": 156800
    },
    {
      "epoch": 11.985333435184478,
      "grad_norm": 5.535770893096924,
      "learning_rate": 4.0012222137346274e-05,
      "loss": 1.8291,
      "step": 156900
    },
    {
      "epoch": 11.992972271025895,
      "grad_norm": 5.5770134925842285,
      "learning_rate": 4.000585644081176e-05,
      "loss": 1.7517,
      "step": 157000
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.8469613790512085,
      "eval_runtime": 1.4976,
      "eval_samples_per_second": 460.751,
      "eval_steps_per_second": 460.751,
      "step": 157092
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.6212811470031738,
      "eval_runtime": 27.747,
      "eval_samples_per_second": 471.799,
      "eval_steps_per_second": 471.799,
      "step": 157092
    },
    {
      "epoch": 12.000611106867314,
      "grad_norm": 6.446471214294434,
      "learning_rate": 3.999949074427724e-05,
      "loss": 1.779,
      "step": 157100
    },
    {
      "epoch": 12.008249942708732,
      "grad_norm": 6.937873840332031,
      "learning_rate": 3.9993125047742724e-05,
      "loss": 1.7363,
      "step": 157200
    },
    {
      "epoch": 12.01588877855015,
      "grad_norm": 6.368455410003662,
      "learning_rate": 3.998675935120821e-05,
      "loss": 1.8567,
      "step": 157300
    },
    {
      "epoch": 12.023527614391567,
      "grad_norm": 6.562458038330078,
      "learning_rate": 3.99803936546737e-05,
      "loss": 1.875,
      "step": 157400
    },
    {
      "epoch": 12.031166450232984,
      "grad_norm": 4.698304653167725,
      "learning_rate": 3.997402795813918e-05,
      "loss": 1.8899,
      "step": 157500
    },
    {
      "epoch": 12.038805286074401,
      "grad_norm": 5.560490608215332,
      "learning_rate": 3.9967662261604665e-05,
      "loss": 1.9167,
      "step": 157600
    },
    {
      "epoch": 12.04644412191582,
      "grad_norm": 4.694397926330566,
      "learning_rate": 3.996129656507015e-05,
      "loss": 1.8702,
      "step": 157700
    },
    {
      "epoch": 12.054082957757238,
      "grad_norm": 5.583373069763184,
      "learning_rate": 3.995493086853564e-05,
      "loss": 1.8997,
      "step": 157800
    },
    {
      "epoch": 12.061721793598656,
      "grad_norm": 4.995743751525879,
      "learning_rate": 3.994856517200112e-05,
      "loss": 1.8321,
      "step": 157900
    },
    {
      "epoch": 12.069360629440073,
      "grad_norm": 5.5294976234436035,
      "learning_rate": 3.9942199475466606e-05,
      "loss": 1.8136,
      "step": 158000
    },
    {
      "epoch": 12.07699946528149,
      "grad_norm": 4.909212112426758,
      "learning_rate": 3.993583377893209e-05,
      "loss": 1.8265,
      "step": 158100
    },
    {
      "epoch": 12.08463830112291,
      "grad_norm": 5.640551567077637,
      "learning_rate": 3.992946808239757e-05,
      "loss": 1.857,
      "step": 158200
    },
    {
      "epoch": 12.092277136964327,
      "grad_norm": 5.105077743530273,
      "learning_rate": 3.992310238586306e-05,
      "loss": 1.7173,
      "step": 158300
    },
    {
      "epoch": 12.099915972805745,
      "grad_norm": 5.5635504722595215,
      "learning_rate": 3.991673668932855e-05,
      "loss": 1.9075,
      "step": 158400
    },
    {
      "epoch": 12.107554808647162,
      "grad_norm": 8.100696563720703,
      "learning_rate": 3.991037099279403e-05,
      "loss": 1.7911,
      "step": 158500
    },
    {
      "epoch": 12.11519364448858,
      "grad_norm": 5.242899417877197,
      "learning_rate": 3.9904005296259514e-05,
      "loss": 1.8511,
      "step": 158600
    },
    {
      "epoch": 12.122832480329997,
      "grad_norm": 5.246454238891602,
      "learning_rate": 3.9897639599725004e-05,
      "loss": 1.7688,
      "step": 158700
    },
    {
      "epoch": 12.130471316171416,
      "grad_norm": 4.741710186004639,
      "learning_rate": 3.989127390319049e-05,
      "loss": 1.8953,
      "step": 158800
    },
    {
      "epoch": 12.138110152012834,
      "grad_norm": 6.620207786560059,
      "learning_rate": 3.988490820665597e-05,
      "loss": 1.8512,
      "step": 158900
    },
    {
      "epoch": 12.145748987854251,
      "grad_norm": 7.531042098999023,
      "learning_rate": 3.9878542510121455e-05,
      "loss": 1.807,
      "step": 159000
    },
    {
      "epoch": 12.153387823695669,
      "grad_norm": 5.834083557128906,
      "learning_rate": 3.9872176813586945e-05,
      "loss": 1.9312,
      "step": 159100
    },
    {
      "epoch": 12.161026659537086,
      "grad_norm": 5.8931097984313965,
      "learning_rate": 3.986581111705243e-05,
      "loss": 1.9144,
      "step": 159200
    },
    {
      "epoch": 12.168665495378505,
      "grad_norm": 2.757404327392578,
      "learning_rate": 3.985944542051792e-05,
      "loss": 1.8646,
      "step": 159300
    },
    {
      "epoch": 12.176304331219923,
      "grad_norm": 5.269337177276611,
      "learning_rate": 3.98530797239834e-05,
      "loss": 1.8185,
      "step": 159400
    },
    {
      "epoch": 12.18394316706134,
      "grad_norm": 5.746139049530029,
      "learning_rate": 3.9846714027448886e-05,
      "loss": 1.7212,
      "step": 159500
    },
    {
      "epoch": 12.191582002902758,
      "grad_norm": 4.418362140655518,
      "learning_rate": 3.9840348330914376e-05,
      "loss": 1.8409,
      "step": 159600
    },
    {
      "epoch": 12.199220838744175,
      "grad_norm": 5.664613246917725,
      "learning_rate": 3.983398263437986e-05,
      "loss": 1.8059,
      "step": 159700
    },
    {
      "epoch": 12.206859674585592,
      "grad_norm": 3.708869695663452,
      "learning_rate": 3.9827616937845344e-05,
      "loss": 1.7687,
      "step": 159800
    },
    {
      "epoch": 12.214498510427012,
      "grad_norm": 4.982434272766113,
      "learning_rate": 3.982125124131083e-05,
      "loss": 1.8214,
      "step": 159900
    },
    {
      "epoch": 12.222137346268429,
      "grad_norm": 4.958555221557617,
      "learning_rate": 3.981488554477631e-05,
      "loss": 1.8036,
      "step": 160000
    },
    {
      "epoch": 12.229776182109847,
      "grad_norm": 5.28061580657959,
      "learning_rate": 3.98085198482418e-05,
      "loss": 1.8608,
      "step": 160100
    },
    {
      "epoch": 12.237415017951264,
      "grad_norm": 4.689594745635986,
      "learning_rate": 3.9802154151707284e-05,
      "loss": 1.8789,
      "step": 160200
    },
    {
      "epoch": 12.245053853792681,
      "grad_norm": 7.768669128417969,
      "learning_rate": 3.979578845517277e-05,
      "loss": 1.8668,
      "step": 160300
    },
    {
      "epoch": 12.252692689634099,
      "grad_norm": 5.500535011291504,
      "learning_rate": 3.978942275863825e-05,
      "loss": 1.7922,
      "step": 160400
    },
    {
      "epoch": 12.260331525475518,
      "grad_norm": 4.391575813293457,
      "learning_rate": 3.9783057062103735e-05,
      "loss": 1.8751,
      "step": 160500
    },
    {
      "epoch": 12.267970361316936,
      "grad_norm": 5.849666595458984,
      "learning_rate": 3.9776691365569225e-05,
      "loss": 1.8163,
      "step": 160600
    },
    {
      "epoch": 12.275609197158353,
      "grad_norm": 6.558366298675537,
      "learning_rate": 3.977032566903471e-05,
      "loss": 1.9049,
      "step": 160700
    },
    {
      "epoch": 12.28324803299977,
      "grad_norm": 8.351210594177246,
      "learning_rate": 3.976395997250019e-05,
      "loss": 1.851,
      "step": 160800
    },
    {
      "epoch": 12.290886868841188,
      "grad_norm": 7.03640079498291,
      "learning_rate": 3.9757594275965676e-05,
      "loss": 1.7753,
      "step": 160900
    },
    {
      "epoch": 12.298525704682607,
      "grad_norm": 4.762546062469482,
      "learning_rate": 3.9751228579431166e-05,
      "loss": 1.819,
      "step": 161000
    },
    {
      "epoch": 12.306164540524025,
      "grad_norm": 6.663815021514893,
      "learning_rate": 3.974486288289665e-05,
      "loss": 1.9163,
      "step": 161100
    },
    {
      "epoch": 12.313803376365442,
      "grad_norm": 5.230507850646973,
      "learning_rate": 3.973849718636213e-05,
      "loss": 1.7637,
      "step": 161200
    },
    {
      "epoch": 12.32144221220686,
      "grad_norm": 4.3060102462768555,
      "learning_rate": 3.973213148982762e-05,
      "loss": 1.8756,
      "step": 161300
    },
    {
      "epoch": 12.329081048048277,
      "grad_norm": 6.094742774963379,
      "learning_rate": 3.97257657932931e-05,
      "loss": 1.8112,
      "step": 161400
    },
    {
      "epoch": 12.336719883889696,
      "grad_norm": 4.695478439331055,
      "learning_rate": 3.971940009675859e-05,
      "loss": 1.7727,
      "step": 161500
    },
    {
      "epoch": 12.344358719731114,
      "grad_norm": 7.324253559112549,
      "learning_rate": 3.9713034400224074e-05,
      "loss": 1.8049,
      "step": 161600
    },
    {
      "epoch": 12.351997555572531,
      "grad_norm": 3.4897048473358154,
      "learning_rate": 3.970666870368956e-05,
      "loss": 1.8534,
      "step": 161700
    },
    {
      "epoch": 12.359636391413948,
      "grad_norm": 6.1406354904174805,
      "learning_rate": 3.970030300715504e-05,
      "loss": 1.8032,
      "step": 161800
    },
    {
      "epoch": 12.367275227255366,
      "grad_norm": 4.379632472991943,
      "learning_rate": 3.969393731062053e-05,
      "loss": 1.8784,
      "step": 161900
    },
    {
      "epoch": 12.374914063096783,
      "grad_norm": 6.165738582611084,
      "learning_rate": 3.9687571614086015e-05,
      "loss": 1.83,
      "step": 162000
    },
    {
      "epoch": 12.382552898938203,
      "grad_norm": 4.913509368896484,
      "learning_rate": 3.96812059175515e-05,
      "loss": 1.8082,
      "step": 162100
    },
    {
      "epoch": 12.39019173477962,
      "grad_norm": 5.010658264160156,
      "learning_rate": 3.967484022101698e-05,
      "loss": 1.8597,
      "step": 162200
    },
    {
      "epoch": 12.397830570621037,
      "grad_norm": 6.6100053787231445,
      "learning_rate": 3.9668474524482466e-05,
      "loss": 1.8863,
      "step": 162300
    },
    {
      "epoch": 12.405469406462455,
      "grad_norm": 5.544520378112793,
      "learning_rate": 3.9662108827947956e-05,
      "loss": 1.8145,
      "step": 162400
    },
    {
      "epoch": 12.413108242303872,
      "grad_norm": 5.741589546203613,
      "learning_rate": 3.965574313141344e-05,
      "loss": 1.7974,
      "step": 162500
    },
    {
      "epoch": 12.42074707814529,
      "grad_norm": 4.959589958190918,
      "learning_rate": 3.964937743487892e-05,
      "loss": 1.8893,
      "step": 162600
    },
    {
      "epoch": 12.428385913986709,
      "grad_norm": 5.093733787536621,
      "learning_rate": 3.964301173834441e-05,
      "loss": 1.8394,
      "step": 162700
    },
    {
      "epoch": 12.436024749828126,
      "grad_norm": 6.2189812660217285,
      "learning_rate": 3.96366460418099e-05,
      "loss": 1.829,
      "step": 162800
    },
    {
      "epoch": 12.443663585669544,
      "grad_norm": 5.373911380767822,
      "learning_rate": 3.963028034527538e-05,
      "loss": 1.8018,
      "step": 162900
    },
    {
      "epoch": 12.451302421510961,
      "grad_norm": 4.991939067840576,
      "learning_rate": 3.9623914648740864e-05,
      "loss": 1.8524,
      "step": 163000
    },
    {
      "epoch": 12.458941257352379,
      "grad_norm": 6.630722999572754,
      "learning_rate": 3.9617548952206354e-05,
      "loss": 1.8787,
      "step": 163100
    },
    {
      "epoch": 12.466580093193798,
      "grad_norm": 6.456304550170898,
      "learning_rate": 3.961118325567184e-05,
      "loss": 1.8613,
      "step": 163200
    },
    {
      "epoch": 12.474218929035215,
      "grad_norm": 5.200156211853027,
      "learning_rate": 3.960481755913733e-05,
      "loss": 1.925,
      "step": 163300
    },
    {
      "epoch": 12.481857764876633,
      "grad_norm": 5.307696342468262,
      "learning_rate": 3.959845186260281e-05,
      "loss": 1.9304,
      "step": 163400
    },
    {
      "epoch": 12.48949660071805,
      "grad_norm": 4.766269207000732,
      "learning_rate": 3.9592086166068295e-05,
      "loss": 1.8694,
      "step": 163500
    },
    {
      "epoch": 12.497135436559468,
      "grad_norm": 5.057775020599365,
      "learning_rate": 3.958572046953378e-05,
      "loss": 1.8212,
      "step": 163600
    },
    {
      "epoch": 12.504774272400887,
      "grad_norm": 5.245304584503174,
      "learning_rate": 3.957935477299926e-05,
      "loss": 1.9223,
      "step": 163700
    },
    {
      "epoch": 12.512413108242304,
      "grad_norm": 4.888546943664551,
      "learning_rate": 3.957298907646475e-05,
      "loss": 1.8011,
      "step": 163800
    },
    {
      "epoch": 12.520051944083722,
      "grad_norm": 5.5530900955200195,
      "learning_rate": 3.9566623379930236e-05,
      "loss": 1.7968,
      "step": 163900
    },
    {
      "epoch": 12.52769077992514,
      "grad_norm": 6.620009422302246,
      "learning_rate": 3.956025768339572e-05,
      "loss": 1.8045,
      "step": 164000
    },
    {
      "epoch": 12.535329615766557,
      "grad_norm": 4.518826007843018,
      "learning_rate": 3.95538919868612e-05,
      "loss": 1.8414,
      "step": 164100
    },
    {
      "epoch": 12.542968451607974,
      "grad_norm": 4.861342906951904,
      "learning_rate": 3.9547526290326694e-05,
      "loss": 1.9824,
      "step": 164200
    },
    {
      "epoch": 12.550607287449393,
      "grad_norm": 5.843341827392578,
      "learning_rate": 3.954116059379218e-05,
      "loss": 1.846,
      "step": 164300
    },
    {
      "epoch": 12.558246123290811,
      "grad_norm": 5.181055545806885,
      "learning_rate": 3.953479489725766e-05,
      "loss": 1.7606,
      "step": 164400
    },
    {
      "epoch": 12.565884959132228,
      "grad_norm": 5.452077388763428,
      "learning_rate": 3.9528429200723144e-05,
      "loss": 1.7518,
      "step": 164500
    },
    {
      "epoch": 12.573523794973646,
      "grad_norm": 4.543633460998535,
      "learning_rate": 3.952206350418863e-05,
      "loss": 1.7883,
      "step": 164600
    },
    {
      "epoch": 12.581162630815063,
      "grad_norm": 5.109367847442627,
      "learning_rate": 3.951569780765412e-05,
      "loss": 1.7921,
      "step": 164700
    },
    {
      "epoch": 12.58880146665648,
      "grad_norm": 5.788227081298828,
      "learning_rate": 3.95093321111196e-05,
      "loss": 1.8598,
      "step": 164800
    },
    {
      "epoch": 12.5964403024979,
      "grad_norm": 4.588552951812744,
      "learning_rate": 3.9502966414585085e-05,
      "loss": 1.8663,
      "step": 164900
    },
    {
      "epoch": 12.604079138339317,
      "grad_norm": 7.505429267883301,
      "learning_rate": 3.949660071805057e-05,
      "loss": 1.803,
      "step": 165000
    },
    {
      "epoch": 12.611717974180735,
      "grad_norm": 4.905646800994873,
      "learning_rate": 3.949023502151606e-05,
      "loss": 1.9381,
      "step": 165100
    },
    {
      "epoch": 12.619356810022152,
      "grad_norm": 8.043947219848633,
      "learning_rate": 3.948386932498154e-05,
      "loss": 1.8313,
      "step": 165200
    },
    {
      "epoch": 12.62699564586357,
      "grad_norm": 5.417477130889893,
      "learning_rate": 3.9477503628447026e-05,
      "loss": 1.7542,
      "step": 165300
    },
    {
      "epoch": 12.634634481704989,
      "grad_norm": 5.851044654846191,
      "learning_rate": 3.947113793191251e-05,
      "loss": 1.744,
      "step": 165400
    },
    {
      "epoch": 12.642273317546406,
      "grad_norm": 8.200261116027832,
      "learning_rate": 3.946477223537799e-05,
      "loss": 1.8566,
      "step": 165500
    },
    {
      "epoch": 12.649912153387824,
      "grad_norm": 4.5037407875061035,
      "learning_rate": 3.9458406538843484e-05,
      "loss": 1.7579,
      "step": 165600
    },
    {
      "epoch": 12.657550989229241,
      "grad_norm": 4.681352138519287,
      "learning_rate": 3.945204084230897e-05,
      "loss": 1.8765,
      "step": 165700
    },
    {
      "epoch": 12.665189825070659,
      "grad_norm": 5.384182929992676,
      "learning_rate": 3.944567514577445e-05,
      "loss": 1.8365,
      "step": 165800
    },
    {
      "epoch": 12.672828660912078,
      "grad_norm": 5.625169277191162,
      "learning_rate": 3.9439309449239934e-05,
      "loss": 1.7993,
      "step": 165900
    },
    {
      "epoch": 12.680467496753495,
      "grad_norm": 4.858057498931885,
      "learning_rate": 3.943294375270542e-05,
      "loss": 1.8149,
      "step": 166000
    },
    {
      "epoch": 12.688106332594913,
      "grad_norm": 4.839702606201172,
      "learning_rate": 3.942657805617091e-05,
      "loss": 1.8976,
      "step": 166100
    },
    {
      "epoch": 12.69574516843633,
      "grad_norm": 6.837968349456787,
      "learning_rate": 3.942021235963639e-05,
      "loss": 1.8143,
      "step": 166200
    },
    {
      "epoch": 12.703384004277748,
      "grad_norm": 5.195735454559326,
      "learning_rate": 3.9413846663101875e-05,
      "loss": 1.9244,
      "step": 166300
    },
    {
      "epoch": 12.711022840119165,
      "grad_norm": 7.582294940948486,
      "learning_rate": 3.940748096656736e-05,
      "loss": 1.7518,
      "step": 166400
    },
    {
      "epoch": 12.718661675960584,
      "grad_norm": 6.663910865783691,
      "learning_rate": 3.940111527003285e-05,
      "loss": 1.7915,
      "step": 166500
    },
    {
      "epoch": 12.726300511802002,
      "grad_norm": 6.5524797439575195,
      "learning_rate": 3.939474957349833e-05,
      "loss": 1.7922,
      "step": 166600
    },
    {
      "epoch": 12.73393934764342,
      "grad_norm": 4.916337013244629,
      "learning_rate": 3.9388383876963816e-05,
      "loss": 1.891,
      "step": 166700
    },
    {
      "epoch": 12.741578183484837,
      "grad_norm": 9.977198600769043,
      "learning_rate": 3.9382018180429306e-05,
      "loss": 1.9445,
      "step": 166800
    },
    {
      "epoch": 12.749217019326254,
      "grad_norm": 5.125825881958008,
      "learning_rate": 3.937565248389479e-05,
      "loss": 1.9162,
      "step": 166900
    },
    {
      "epoch": 12.756855855167672,
      "grad_norm": 5.176908493041992,
      "learning_rate": 3.936928678736027e-05,
      "loss": 1.7908,
      "step": 167000
    },
    {
      "epoch": 12.76449469100909,
      "grad_norm": 5.712869644165039,
      "learning_rate": 3.9362921090825764e-05,
      "loss": 1.8473,
      "step": 167100
    },
    {
      "epoch": 12.772133526850508,
      "grad_norm": 4.488892078399658,
      "learning_rate": 3.935655539429125e-05,
      "loss": 1.828,
      "step": 167200
    },
    {
      "epoch": 12.779772362691926,
      "grad_norm": 7.156550407409668,
      "learning_rate": 3.935018969775673e-05,
      "loss": 1.8961,
      "step": 167300
    },
    {
      "epoch": 12.787411198533343,
      "grad_norm": 6.4242844581604,
      "learning_rate": 3.934382400122222e-05,
      "loss": 1.8443,
      "step": 167400
    },
    {
      "epoch": 12.79505003437476,
      "grad_norm": 5.203643798828125,
      "learning_rate": 3.9337458304687705e-05,
      "loss": 1.8096,
      "step": 167500
    },
    {
      "epoch": 12.80268887021618,
      "grad_norm": 5.302122592926025,
      "learning_rate": 3.933109260815319e-05,
      "loss": 1.7553,
      "step": 167600
    },
    {
      "epoch": 12.810327706057597,
      "grad_norm": 6.4332499504089355,
      "learning_rate": 3.932472691161867e-05,
      "loss": 1.7421,
      "step": 167700
    },
    {
      "epoch": 12.817966541899015,
      "grad_norm": 4.010054111480713,
      "learning_rate": 3.9318361215084155e-05,
      "loss": 1.8791,
      "step": 167800
    },
    {
      "epoch": 12.825605377740432,
      "grad_norm": 6.880874156951904,
      "learning_rate": 3.9311995518549646e-05,
      "loss": 1.7721,
      "step": 167900
    },
    {
      "epoch": 12.83324421358185,
      "grad_norm": 5.477826118469238,
      "learning_rate": 3.930562982201513e-05,
      "loss": 1.8105,
      "step": 168000
    },
    {
      "epoch": 12.840883049423267,
      "grad_norm": 7.416146755218506,
      "learning_rate": 3.929926412548061e-05,
      "loss": 1.827,
      "step": 168100
    },
    {
      "epoch": 12.848521885264686,
      "grad_norm": 6.035399913787842,
      "learning_rate": 3.9292898428946096e-05,
      "loss": 1.7753,
      "step": 168200
    },
    {
      "epoch": 12.856160721106104,
      "grad_norm": 4.521886348724365,
      "learning_rate": 3.9286532732411586e-05,
      "loss": 1.9199,
      "step": 168300
    },
    {
      "epoch": 12.863799556947521,
      "grad_norm": 5.655632972717285,
      "learning_rate": 3.928016703587707e-05,
      "loss": 1.78,
      "step": 168400
    },
    {
      "epoch": 12.871438392788939,
      "grad_norm": 7.335537910461426,
      "learning_rate": 3.9273801339342554e-05,
      "loss": 1.8052,
      "step": 168500
    },
    {
      "epoch": 12.879077228630356,
      "grad_norm": 5.508925914764404,
      "learning_rate": 3.926743564280804e-05,
      "loss": 1.9103,
      "step": 168600
    },
    {
      "epoch": 12.886716064471775,
      "grad_norm": 5.600693702697754,
      "learning_rate": 3.926106994627352e-05,
      "loss": 1.8443,
      "step": 168700
    },
    {
      "epoch": 12.894354900313193,
      "grad_norm": 5.512646198272705,
      "learning_rate": 3.925470424973901e-05,
      "loss": 1.841,
      "step": 168800
    },
    {
      "epoch": 12.90199373615461,
      "grad_norm": 4.811711311340332,
      "learning_rate": 3.9248338553204494e-05,
      "loss": 1.8208,
      "step": 168900
    },
    {
      "epoch": 12.909632571996028,
      "grad_norm": 5.073237419128418,
      "learning_rate": 3.924197285666998e-05,
      "loss": 1.931,
      "step": 169000
    },
    {
      "epoch": 12.917271407837445,
      "grad_norm": 6.742221355438232,
      "learning_rate": 3.923560716013546e-05,
      "loss": 1.8712,
      "step": 169100
    },
    {
      "epoch": 12.924910243678863,
      "grad_norm": 4.652886867523193,
      "learning_rate": 3.9229241463600945e-05,
      "loss": 1.7682,
      "step": 169200
    },
    {
      "epoch": 12.932549079520282,
      "grad_norm": 5.555724620819092,
      "learning_rate": 3.9222875767066435e-05,
      "loss": 1.8659,
      "step": 169300
    },
    {
      "epoch": 12.9401879153617,
      "grad_norm": 3.8980870246887207,
      "learning_rate": 3.921651007053192e-05,
      "loss": 1.7882,
      "step": 169400
    },
    {
      "epoch": 12.947826751203117,
      "grad_norm": 6.398043632507324,
      "learning_rate": 3.92101443739974e-05,
      "loss": 1.8342,
      "step": 169500
    },
    {
      "epoch": 12.955465587044534,
      "grad_norm": 5.837795734405518,
      "learning_rate": 3.9203778677462886e-05,
      "loss": 1.8676,
      "step": 169600
    },
    {
      "epoch": 12.963104422885952,
      "grad_norm": 5.450746059417725,
      "learning_rate": 3.9197412980928376e-05,
      "loss": 1.9542,
      "step": 169700
    },
    {
      "epoch": 12.97074325872737,
      "grad_norm": 4.844111442565918,
      "learning_rate": 3.919104728439386e-05,
      "loss": 1.7783,
      "step": 169800
    },
    {
      "epoch": 12.978382094568788,
      "grad_norm": 5.79696798324585,
      "learning_rate": 3.918468158785934e-05,
      "loss": 1.8943,
      "step": 169900
    },
    {
      "epoch": 12.986020930410206,
      "grad_norm": 3.2237069606781006,
      "learning_rate": 3.917831589132483e-05,
      "loss": 1.8763,
      "step": 170000
    },
    {
      "epoch": 12.993659766251623,
      "grad_norm": 6.092180252075195,
      "learning_rate": 3.917195019479031e-05,
      "loss": 1.8557,
      "step": 170100
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.8346210718154907,
      "eval_runtime": 1.4492,
      "eval_samples_per_second": 476.139,
      "eval_steps_per_second": 476.139,
      "step": 170183
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.6046264171600342,
      "eval_runtime": 27.7224,
      "eval_samples_per_second": 472.218,
      "eval_steps_per_second": 472.218,
      "step": 170183
    },
    {
      "epoch": 13.00129860209304,
      "grad_norm": 4.710206985473633,
      "learning_rate": 3.91655844982558e-05,
      "loss": 1.8719,
      "step": 170200
    },
    {
      "epoch": 13.008937437934458,
      "grad_norm": 5.5999650955200195,
      "learning_rate": 3.9159218801721284e-05,
      "loss": 1.8772,
      "step": 170300
    },
    {
      "epoch": 13.016576273775877,
      "grad_norm": 6.711345672607422,
      "learning_rate": 3.915285310518677e-05,
      "loss": 1.7298,
      "step": 170400
    },
    {
      "epoch": 13.024215109617295,
      "grad_norm": 4.070184230804443,
      "learning_rate": 3.914648740865226e-05,
      "loss": 1.7396,
      "step": 170500
    },
    {
      "epoch": 13.031853945458712,
      "grad_norm": 4.501736640930176,
      "learning_rate": 3.914012171211774e-05,
      "loss": 1.8953,
      "step": 170600
    },
    {
      "epoch": 13.03949278130013,
      "grad_norm": 5.509273052215576,
      "learning_rate": 3.9133756015583225e-05,
      "loss": 1.8211,
      "step": 170700
    },
    {
      "epoch": 13.047131617141547,
      "grad_norm": 8.860713005065918,
      "learning_rate": 3.9127390319048716e-05,
      "loss": 1.8208,
      "step": 170800
    },
    {
      "epoch": 13.054770452982966,
      "grad_norm": 5.008581638336182,
      "learning_rate": 3.91210246225142e-05,
      "loss": 1.8519,
      "step": 170900
    },
    {
      "epoch": 13.062409288824384,
      "grad_norm": 5.309434413909912,
      "learning_rate": 3.911465892597968e-05,
      "loss": 1.7746,
      "step": 171000
    },
    {
      "epoch": 13.070048124665801,
      "grad_norm": 5.902771472930908,
      "learning_rate": 3.910829322944517e-05,
      "loss": 1.8547,
      "step": 171100
    },
    {
      "epoch": 13.077686960507219,
      "grad_norm": 5.892754077911377,
      "learning_rate": 3.9101927532910656e-05,
      "loss": 1.8876,
      "step": 171200
    },
    {
      "epoch": 13.085325796348636,
      "grad_norm": 5.286036014556885,
      "learning_rate": 3.909556183637614e-05,
      "loss": 1.7303,
      "step": 171300
    },
    {
      "epoch": 13.092964632190053,
      "grad_norm": 6.227453231811523,
      "learning_rate": 3.9089196139841624e-05,
      "loss": 1.8601,
      "step": 171400
    },
    {
      "epoch": 13.100603468031473,
      "grad_norm": 5.977067470550537,
      "learning_rate": 3.9082830443307114e-05,
      "loss": 1.8012,
      "step": 171500
    },
    {
      "epoch": 13.10824230387289,
      "grad_norm": 5.409896373748779,
      "learning_rate": 3.90764647467726e-05,
      "loss": 1.869,
      "step": 171600
    },
    {
      "epoch": 13.115881139714308,
      "grad_norm": 7.711591720581055,
      "learning_rate": 3.907009905023808e-05,
      "loss": 1.819,
      "step": 171700
    },
    {
      "epoch": 13.123519975555725,
      "grad_norm": 4.699853897094727,
      "learning_rate": 3.9063733353703564e-05,
      "loss": 1.9059,
      "step": 171800
    },
    {
      "epoch": 13.131158811397142,
      "grad_norm": 4.474802494049072,
      "learning_rate": 3.905736765716905e-05,
      "loss": 1.7561,
      "step": 171900
    },
    {
      "epoch": 13.138797647238562,
      "grad_norm": 7.114630699157715,
      "learning_rate": 3.905100196063454e-05,
      "loss": 1.8819,
      "step": 172000
    },
    {
      "epoch": 13.14643648307998,
      "grad_norm": 6.156732559204102,
      "learning_rate": 3.904463626410002e-05,
      "loss": 1.7631,
      "step": 172100
    },
    {
      "epoch": 13.154075318921397,
      "grad_norm": 4.05747127532959,
      "learning_rate": 3.9038270567565505e-05,
      "loss": 1.8846,
      "step": 172200
    },
    {
      "epoch": 13.161714154762814,
      "grad_norm": 5.037477970123291,
      "learning_rate": 3.903190487103099e-05,
      "loss": 1.8027,
      "step": 172300
    },
    {
      "epoch": 13.169352990604231,
      "grad_norm": 6.606853485107422,
      "learning_rate": 3.902553917449647e-05,
      "loss": 1.9276,
      "step": 172400
    },
    {
      "epoch": 13.176991826445649,
      "grad_norm": 4.962330341339111,
      "learning_rate": 3.901917347796196e-05,
      "loss": 1.7929,
      "step": 172500
    },
    {
      "epoch": 13.184630662287068,
      "grad_norm": 4.831428050994873,
      "learning_rate": 3.9012807781427446e-05,
      "loss": 1.7778,
      "step": 172600
    },
    {
      "epoch": 13.192269498128486,
      "grad_norm": 6.088496685028076,
      "learning_rate": 3.900644208489293e-05,
      "loss": 1.7791,
      "step": 172700
    },
    {
      "epoch": 13.199908333969903,
      "grad_norm": 5.375566005706787,
      "learning_rate": 3.900007638835841e-05,
      "loss": 1.8351,
      "step": 172800
    },
    {
      "epoch": 13.20754716981132,
      "grad_norm": 5.804463863372803,
      "learning_rate": 3.8993710691823904e-05,
      "loss": 1.8108,
      "step": 172900
    },
    {
      "epoch": 13.215186005652738,
      "grad_norm": 4.150753498077393,
      "learning_rate": 3.898734499528939e-05,
      "loss": 1.7999,
      "step": 173000
    },
    {
      "epoch": 13.222824841494157,
      "grad_norm": 6.214064598083496,
      "learning_rate": 3.898097929875487e-05,
      "loss": 1.8381,
      "step": 173100
    },
    {
      "epoch": 13.230463677335575,
      "grad_norm": 3.949186086654663,
      "learning_rate": 3.8974613602220354e-05,
      "loss": 1.8481,
      "step": 173200
    },
    {
      "epoch": 13.238102513176992,
      "grad_norm": 5.253249168395996,
      "learning_rate": 3.896824790568584e-05,
      "loss": 1.8619,
      "step": 173300
    },
    {
      "epoch": 13.24574134901841,
      "grad_norm": 5.110404014587402,
      "learning_rate": 3.896188220915133e-05,
      "loss": 1.8945,
      "step": 173400
    },
    {
      "epoch": 13.253380184859827,
      "grad_norm": 5.89371395111084,
      "learning_rate": 3.895551651261681e-05,
      "loss": 1.8073,
      "step": 173500
    },
    {
      "epoch": 13.261019020701244,
      "grad_norm": 4.608572959899902,
      "learning_rate": 3.8949150816082295e-05,
      "loss": 1.8004,
      "step": 173600
    },
    {
      "epoch": 13.268657856542664,
      "grad_norm": 6.230780601501465,
      "learning_rate": 3.894278511954778e-05,
      "loss": 1.8985,
      "step": 173700
    },
    {
      "epoch": 13.276296692384081,
      "grad_norm": 5.944443702697754,
      "learning_rate": 3.893641942301327e-05,
      "loss": 1.8824,
      "step": 173800
    },
    {
      "epoch": 13.283935528225499,
      "grad_norm": 5.567529201507568,
      "learning_rate": 3.893005372647875e-05,
      "loss": 1.8698,
      "step": 173900
    },
    {
      "epoch": 13.291574364066916,
      "grad_norm": 4.314164161682129,
      "learning_rate": 3.8923688029944236e-05,
      "loss": 1.8142,
      "step": 174000
    },
    {
      "epoch": 13.299213199908333,
      "grad_norm": 4.211754322052002,
      "learning_rate": 3.891732233340972e-05,
      "loss": 1.7301,
      "step": 174100
    },
    {
      "epoch": 13.306852035749753,
      "grad_norm": 6.045474529266357,
      "learning_rate": 3.89109566368752e-05,
      "loss": 1.9284,
      "step": 174200
    },
    {
      "epoch": 13.31449087159117,
      "grad_norm": 3.9187841415405273,
      "learning_rate": 3.8904590940340694e-05,
      "loss": 1.8346,
      "step": 174300
    },
    {
      "epoch": 13.322129707432588,
      "grad_norm": 3.6297755241394043,
      "learning_rate": 3.889822524380618e-05,
      "loss": 1.7326,
      "step": 174400
    },
    {
      "epoch": 13.329768543274005,
      "grad_norm": 7.889037609100342,
      "learning_rate": 3.889185954727167e-05,
      "loss": 1.8967,
      "step": 174500
    },
    {
      "epoch": 13.337407379115422,
      "grad_norm": 4.859457969665527,
      "learning_rate": 3.888549385073715e-05,
      "loss": 1.8462,
      "step": 174600
    },
    {
      "epoch": 13.34504621495684,
      "grad_norm": 6.745675086975098,
      "learning_rate": 3.8879128154202634e-05,
      "loss": 1.7153,
      "step": 174700
    },
    {
      "epoch": 13.352685050798259,
      "grad_norm": 5.049821376800537,
      "learning_rate": 3.8872762457668125e-05,
      "loss": 1.8184,
      "step": 174800
    },
    {
      "epoch": 13.360323886639677,
      "grad_norm": 6.910056114196777,
      "learning_rate": 3.886639676113361e-05,
      "loss": 1.748,
      "step": 174900
    },
    {
      "epoch": 13.367962722481094,
      "grad_norm": 6.78286075592041,
      "learning_rate": 3.886003106459909e-05,
      "loss": 1.8274,
      "step": 175000
    },
    {
      "epoch": 13.375601558322511,
      "grad_norm": 4.89032506942749,
      "learning_rate": 3.8853665368064575e-05,
      "loss": 1.8118,
      "step": 175100
    },
    {
      "epoch": 13.383240394163929,
      "grad_norm": 5.168984889984131,
      "learning_rate": 3.8847299671530066e-05,
      "loss": 1.8406,
      "step": 175200
    },
    {
      "epoch": 13.390879230005346,
      "grad_norm": 4.4897847175598145,
      "learning_rate": 3.884093397499555e-05,
      "loss": 1.7785,
      "step": 175300
    },
    {
      "epoch": 13.398518065846766,
      "grad_norm": 5.122321605682373,
      "learning_rate": 3.883456827846103e-05,
      "loss": 1.8798,
      "step": 175400
    },
    {
      "epoch": 13.406156901688183,
      "grad_norm": 4.502192497253418,
      "learning_rate": 3.8828202581926516e-05,
      "loss": 1.8589,
      "step": 175500
    },
    {
      "epoch": 13.4137957375296,
      "grad_norm": 5.588714122772217,
      "learning_rate": 3.8821836885392e-05,
      "loss": 1.8273,
      "step": 175600
    },
    {
      "epoch": 13.421434573371018,
      "grad_norm": 4.726004123687744,
      "learning_rate": 3.881547118885749e-05,
      "loss": 1.8608,
      "step": 175700
    },
    {
      "epoch": 13.429073409212435,
      "grad_norm": 4.31279182434082,
      "learning_rate": 3.8809105492322974e-05,
      "loss": 1.7725,
      "step": 175800
    },
    {
      "epoch": 13.436712245053855,
      "grad_norm": 4.69873046875,
      "learning_rate": 3.880273979578846e-05,
      "loss": 1.8733,
      "step": 175900
    },
    {
      "epoch": 13.444351080895272,
      "grad_norm": 5.004831790924072,
      "learning_rate": 3.879637409925394e-05,
      "loss": 1.7875,
      "step": 176000
    },
    {
      "epoch": 13.45198991673669,
      "grad_norm": 5.47604513168335,
      "learning_rate": 3.879000840271943e-05,
      "loss": 1.8237,
      "step": 176100
    },
    {
      "epoch": 13.459628752578107,
      "grad_norm": 4.749556541442871,
      "learning_rate": 3.8783642706184915e-05,
      "loss": 1.8113,
      "step": 176200
    },
    {
      "epoch": 13.467267588419524,
      "grad_norm": 4.837276458740234,
      "learning_rate": 3.87772770096504e-05,
      "loss": 1.7316,
      "step": 176300
    },
    {
      "epoch": 13.474906424260944,
      "grad_norm": 7.043848514556885,
      "learning_rate": 3.877091131311588e-05,
      "loss": 1.7399,
      "step": 176400
    },
    {
      "epoch": 13.482545260102361,
      "grad_norm": 5.399111747741699,
      "learning_rate": 3.8764545616581365e-05,
      "loss": 1.7862,
      "step": 176500
    },
    {
      "epoch": 13.490184095943778,
      "grad_norm": 5.233197212219238,
      "learning_rate": 3.8758179920046856e-05,
      "loss": 1.8488,
      "step": 176600
    },
    {
      "epoch": 13.497822931785196,
      "grad_norm": 6.1001200675964355,
      "learning_rate": 3.875181422351234e-05,
      "loss": 1.8463,
      "step": 176700
    },
    {
      "epoch": 13.505461767626613,
      "grad_norm": 4.528559684753418,
      "learning_rate": 3.874544852697782e-05,
      "loss": 1.8163,
      "step": 176800
    },
    {
      "epoch": 13.51310060346803,
      "grad_norm": 4.924741744995117,
      "learning_rate": 3.8739082830443306e-05,
      "loss": 1.8756,
      "step": 176900
    },
    {
      "epoch": 13.52073943930945,
      "grad_norm": 5.743013381958008,
      "learning_rate": 3.8732717133908796e-05,
      "loss": 1.823,
      "step": 177000
    },
    {
      "epoch": 13.528378275150867,
      "grad_norm": 3.9376959800720215,
      "learning_rate": 3.872635143737428e-05,
      "loss": 1.733,
      "step": 177100
    },
    {
      "epoch": 13.536017110992285,
      "grad_norm": 4.902726650238037,
      "learning_rate": 3.8719985740839764e-05,
      "loss": 1.8084,
      "step": 177200
    },
    {
      "epoch": 13.543655946833702,
      "grad_norm": 5.2610883712768555,
      "learning_rate": 3.871362004430525e-05,
      "loss": 1.8696,
      "step": 177300
    },
    {
      "epoch": 13.55129478267512,
      "grad_norm": 4.738827705383301,
      "learning_rate": 3.870725434777073e-05,
      "loss": 1.8335,
      "step": 177400
    },
    {
      "epoch": 13.558933618516537,
      "grad_norm": 5.505366802215576,
      "learning_rate": 3.870088865123622e-05,
      "loss": 2.0076,
      "step": 177500
    },
    {
      "epoch": 13.566572454357956,
      "grad_norm": 5.9900078773498535,
      "learning_rate": 3.8694522954701704e-05,
      "loss": 1.8111,
      "step": 177600
    },
    {
      "epoch": 13.574211290199374,
      "grad_norm": 6.127885341644287,
      "learning_rate": 3.868815725816719e-05,
      "loss": 1.7449,
      "step": 177700
    },
    {
      "epoch": 13.581850126040791,
      "grad_norm": 6.379360198974609,
      "learning_rate": 3.868179156163267e-05,
      "loss": 1.8077,
      "step": 177800
    },
    {
      "epoch": 13.589488961882209,
      "grad_norm": 5.929272651672363,
      "learning_rate": 3.8675425865098155e-05,
      "loss": 1.8281,
      "step": 177900
    },
    {
      "epoch": 13.597127797723626,
      "grad_norm": 4.587331771850586,
      "learning_rate": 3.8669060168563645e-05,
      "loss": 1.9296,
      "step": 178000
    },
    {
      "epoch": 13.604766633565045,
      "grad_norm": 4.986667633056641,
      "learning_rate": 3.866269447202913e-05,
      "loss": 1.8236,
      "step": 178100
    },
    {
      "epoch": 13.612405469406463,
      "grad_norm": 6.045711040496826,
      "learning_rate": 3.865632877549461e-05,
      "loss": 1.7974,
      "step": 178200
    },
    {
      "epoch": 13.62004430524788,
      "grad_norm": 9.765433311462402,
      "learning_rate": 3.86499630789601e-05,
      "loss": 1.8248,
      "step": 178300
    },
    {
      "epoch": 13.627683141089298,
      "grad_norm": 5.639453411102295,
      "learning_rate": 3.8643597382425586e-05,
      "loss": 1.8552,
      "step": 178400
    },
    {
      "epoch": 13.635321976930715,
      "grad_norm": 4.109952926635742,
      "learning_rate": 3.8637231685891077e-05,
      "loss": 1.8393,
      "step": 178500
    },
    {
      "epoch": 13.642960812772134,
      "grad_norm": 4.501670837402344,
      "learning_rate": 3.863086598935656e-05,
      "loss": 1.79,
      "step": 178600
    },
    {
      "epoch": 13.650599648613552,
      "grad_norm": 4.360686779022217,
      "learning_rate": 3.8624500292822044e-05,
      "loss": 1.8092,
      "step": 178700
    },
    {
      "epoch": 13.65823848445497,
      "grad_norm": 4.342092990875244,
      "learning_rate": 3.861813459628753e-05,
      "loss": 1.8303,
      "step": 178800
    },
    {
      "epoch": 13.665877320296387,
      "grad_norm": 5.415624618530273,
      "learning_rate": 3.861176889975302e-05,
      "loss": 1.8555,
      "step": 178900
    },
    {
      "epoch": 13.673516156137804,
      "grad_norm": 6.0785346031188965,
      "learning_rate": 3.86054032032185e-05,
      "loss": 1.812,
      "step": 179000
    },
    {
      "epoch": 13.681154991979222,
      "grad_norm": 4.136346340179443,
      "learning_rate": 3.8599037506683985e-05,
      "loss": 1.7801,
      "step": 179100
    },
    {
      "epoch": 13.688793827820641,
      "grad_norm": 4.863166809082031,
      "learning_rate": 3.859267181014947e-05,
      "loss": 1.8222,
      "step": 179200
    },
    {
      "epoch": 13.696432663662058,
      "grad_norm": 6.38881778717041,
      "learning_rate": 3.858630611361496e-05,
      "loss": 1.7847,
      "step": 179300
    },
    {
      "epoch": 13.704071499503476,
      "grad_norm": 5.215724468231201,
      "learning_rate": 3.857994041708044e-05,
      "loss": 1.7792,
      "step": 179400
    },
    {
      "epoch": 13.711710335344893,
      "grad_norm": 5.543735504150391,
      "learning_rate": 3.8573574720545926e-05,
      "loss": 1.8883,
      "step": 179500
    },
    {
      "epoch": 13.71934917118631,
      "grad_norm": 6.448500156402588,
      "learning_rate": 3.856720902401141e-05,
      "loss": 1.803,
      "step": 179600
    },
    {
      "epoch": 13.726988007027728,
      "grad_norm": 4.821537017822266,
      "learning_rate": 3.856084332747689e-05,
      "loss": 1.7728,
      "step": 179700
    },
    {
      "epoch": 13.734626842869147,
      "grad_norm": 6.10214900970459,
      "learning_rate": 3.855447763094238e-05,
      "loss": 1.853,
      "step": 179800
    },
    {
      "epoch": 13.742265678710565,
      "grad_norm": 6.424217224121094,
      "learning_rate": 3.8548111934407866e-05,
      "loss": 1.7414,
      "step": 179900
    },
    {
      "epoch": 13.749904514551982,
      "grad_norm": 6.114694595336914,
      "learning_rate": 3.854174623787335e-05,
      "loss": 1.9561,
      "step": 180000
    },
    {
      "epoch": 13.7575433503934,
      "grad_norm": 4.783621788024902,
      "learning_rate": 3.8535380541338834e-05,
      "loss": 1.7813,
      "step": 180100
    },
    {
      "epoch": 13.765182186234817,
      "grad_norm": 5.52407693862915,
      "learning_rate": 3.8529014844804324e-05,
      "loss": 1.8589,
      "step": 180200
    },
    {
      "epoch": 13.772821022076236,
      "grad_norm": 7.3548407554626465,
      "learning_rate": 3.852264914826981e-05,
      "loss": 1.8267,
      "step": 180300
    },
    {
      "epoch": 13.780459857917654,
      "grad_norm": 6.94813871383667,
      "learning_rate": 3.851628345173529e-05,
      "loss": 1.8362,
      "step": 180400
    },
    {
      "epoch": 13.788098693759071,
      "grad_norm": 6.700806140899658,
      "learning_rate": 3.8509917755200774e-05,
      "loss": 1.792,
      "step": 180500
    },
    {
      "epoch": 13.795737529600489,
      "grad_norm": 3.817150831222534,
      "learning_rate": 3.850355205866626e-05,
      "loss": 1.795,
      "step": 180600
    },
    {
      "epoch": 13.803376365441906,
      "grad_norm": 4.6931586265563965,
      "learning_rate": 3.849718636213175e-05,
      "loss": 1.8411,
      "step": 180700
    },
    {
      "epoch": 13.811015201283324,
      "grad_norm": 5.775790691375732,
      "learning_rate": 3.849082066559723e-05,
      "loss": 1.7098,
      "step": 180800
    },
    {
      "epoch": 13.818654037124743,
      "grad_norm": 5.6896467208862305,
      "learning_rate": 3.8484454969062715e-05,
      "loss": 1.831,
      "step": 180900
    },
    {
      "epoch": 13.82629287296616,
      "grad_norm": 5.450008392333984,
      "learning_rate": 3.84780892725282e-05,
      "loss": 1.9091,
      "step": 181000
    },
    {
      "epoch": 13.833931708807578,
      "grad_norm": 7.530760288238525,
      "learning_rate": 3.847172357599368e-05,
      "loss": 1.8324,
      "step": 181100
    },
    {
      "epoch": 13.841570544648995,
      "grad_norm": 4.785487651824951,
      "learning_rate": 3.846535787945917e-05,
      "loss": 1.8797,
      "step": 181200
    },
    {
      "epoch": 13.849209380490413,
      "grad_norm": 4.853565216064453,
      "learning_rate": 3.8458992182924656e-05,
      "loss": 1.879,
      "step": 181300
    },
    {
      "epoch": 13.856848216331832,
      "grad_norm": 4.605937957763672,
      "learning_rate": 3.845262648639014e-05,
      "loss": 1.8521,
      "step": 181400
    },
    {
      "epoch": 13.86448705217325,
      "grad_norm": 5.345425128936768,
      "learning_rate": 3.844626078985562e-05,
      "loss": 1.872,
      "step": 181500
    },
    {
      "epoch": 13.872125888014667,
      "grad_norm": 5.282697677612305,
      "learning_rate": 3.8439895093321114e-05,
      "loss": 1.8849,
      "step": 181600
    },
    {
      "epoch": 13.879764723856084,
      "grad_norm": 3.2926459312438965,
      "learning_rate": 3.84335293967866e-05,
      "loss": 1.8555,
      "step": 181700
    },
    {
      "epoch": 13.887403559697502,
      "grad_norm": 5.843127727508545,
      "learning_rate": 3.842716370025208e-05,
      "loss": 1.827,
      "step": 181800
    },
    {
      "epoch": 13.895042395538919,
      "grad_norm": 5.289483070373535,
      "learning_rate": 3.8420798003717564e-05,
      "loss": 1.738,
      "step": 181900
    },
    {
      "epoch": 13.902681231380338,
      "grad_norm": 5.25545072555542,
      "learning_rate": 3.8414432307183055e-05,
      "loss": 1.8423,
      "step": 182000
    },
    {
      "epoch": 13.910320067221756,
      "grad_norm": 10.602232933044434,
      "learning_rate": 3.840806661064854e-05,
      "loss": 1.7595,
      "step": 182100
    },
    {
      "epoch": 13.917958903063173,
      "grad_norm": 5.641664028167725,
      "learning_rate": 3.840170091411402e-05,
      "loss": 1.8823,
      "step": 182200
    },
    {
      "epoch": 13.92559773890459,
      "grad_norm": 4.001131057739258,
      "learning_rate": 3.839533521757951e-05,
      "loss": 1.7974,
      "step": 182300
    },
    {
      "epoch": 13.933236574746008,
      "grad_norm": 5.148408889770508,
      "learning_rate": 3.8388969521044996e-05,
      "loss": 1.7854,
      "step": 182400
    },
    {
      "epoch": 13.940875410587427,
      "grad_norm": 4.885071277618408,
      "learning_rate": 3.838260382451048e-05,
      "loss": 1.8881,
      "step": 182500
    },
    {
      "epoch": 13.948514246428845,
      "grad_norm": 4.743511199951172,
      "learning_rate": 3.837623812797597e-05,
      "loss": 1.7586,
      "step": 182600
    },
    {
      "epoch": 13.956153082270262,
      "grad_norm": 5.580963134765625,
      "learning_rate": 3.836987243144145e-05,
      "loss": 1.8966,
      "step": 182700
    },
    {
      "epoch": 13.96379191811168,
      "grad_norm": 5.801124572753906,
      "learning_rate": 3.8363506734906936e-05,
      "loss": 1.8324,
      "step": 182800
    },
    {
      "epoch": 13.971430753953097,
      "grad_norm": 5.366735458374023,
      "learning_rate": 3.835714103837242e-05,
      "loss": 1.8587,
      "step": 182900
    },
    {
      "epoch": 13.979069589794515,
      "grad_norm": 5.632201671600342,
      "learning_rate": 3.835077534183791e-05,
      "loss": 1.7905,
      "step": 183000
    },
    {
      "epoch": 13.986708425635934,
      "grad_norm": 4.217787742614746,
      "learning_rate": 3.8344409645303394e-05,
      "loss": 1.8891,
      "step": 183100
    },
    {
      "epoch": 13.994347261477351,
      "grad_norm": 3.9812886714935303,
      "learning_rate": 3.833804394876888e-05,
      "loss": 1.779,
      "step": 183200
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8287259340286255,
      "eval_runtime": 1.4499,
      "eval_samples_per_second": 475.891,
      "eval_steps_per_second": 475.891,
      "step": 183274
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.5927059650421143,
      "eval_runtime": 27.6265,
      "eval_samples_per_second": 473.857,
      "eval_steps_per_second": 473.857,
      "step": 183274
    },
    {
      "epoch": 14.001986097318769,
      "grad_norm": 5.490603446960449,
      "learning_rate": 3.833167825223436e-05,
      "loss": 1.7895,
      "step": 183300
    },
    {
      "epoch": 14.009624933160186,
      "grad_norm": 6.505168914794922,
      "learning_rate": 3.8325312555699844e-05,
      "loss": 1.7756,
      "step": 183400
    },
    {
      "epoch": 14.017263769001604,
      "grad_norm": 6.371565818786621,
      "learning_rate": 3.8318946859165335e-05,
      "loss": 1.8107,
      "step": 183500
    },
    {
      "epoch": 14.024902604843023,
      "grad_norm": 5.5305094718933105,
      "learning_rate": 3.831258116263082e-05,
      "loss": 1.8007,
      "step": 183600
    },
    {
      "epoch": 14.03254144068444,
      "grad_norm": 5.38185453414917,
      "learning_rate": 3.83062154660963e-05,
      "loss": 1.8315,
      "step": 183700
    },
    {
      "epoch": 14.040180276525858,
      "grad_norm": 6.248636722564697,
      "learning_rate": 3.8299849769561785e-05,
      "loss": 1.8257,
      "step": 183800
    },
    {
      "epoch": 14.047819112367275,
      "grad_norm": 5.194202423095703,
      "learning_rate": 3.8293484073027276e-05,
      "loss": 1.8107,
      "step": 183900
    },
    {
      "epoch": 14.055457948208693,
      "grad_norm": 5.187681198120117,
      "learning_rate": 3.828711837649276e-05,
      "loss": 1.834,
      "step": 184000
    },
    {
      "epoch": 14.06309678405011,
      "grad_norm": 4.915846824645996,
      "learning_rate": 3.828075267995824e-05,
      "loss": 1.8442,
      "step": 184100
    },
    {
      "epoch": 14.07073561989153,
      "grad_norm": 4.641064167022705,
      "learning_rate": 3.8274386983423726e-05,
      "loss": 1.7722,
      "step": 184200
    },
    {
      "epoch": 14.078374455732947,
      "grad_norm": 6.200819492340088,
      "learning_rate": 3.826802128688921e-05,
      "loss": 1.8368,
      "step": 184300
    },
    {
      "epoch": 14.086013291574364,
      "grad_norm": 5.604488849639893,
      "learning_rate": 3.82616555903547e-05,
      "loss": 1.8727,
      "step": 184400
    },
    {
      "epoch": 14.093652127415782,
      "grad_norm": 5.6115617752075195,
      "learning_rate": 3.8255289893820184e-05,
      "loss": 1.7278,
      "step": 184500
    },
    {
      "epoch": 14.101290963257199,
      "grad_norm": 6.565742015838623,
      "learning_rate": 3.824892419728567e-05,
      "loss": 1.7818,
      "step": 184600
    },
    {
      "epoch": 14.108929799098618,
      "grad_norm": 4.73701810836792,
      "learning_rate": 3.824255850075115e-05,
      "loss": 1.8311,
      "step": 184700
    },
    {
      "epoch": 14.116568634940036,
      "grad_norm": 4.981948375701904,
      "learning_rate": 3.823619280421664e-05,
      "loss": 1.8223,
      "step": 184800
    },
    {
      "epoch": 14.124207470781453,
      "grad_norm": 6.508930206298828,
      "learning_rate": 3.8229827107682125e-05,
      "loss": 1.8208,
      "step": 184900
    },
    {
      "epoch": 14.13184630662287,
      "grad_norm": 4.822388172149658,
      "learning_rate": 3.822346141114761e-05,
      "loss": 1.8499,
      "step": 185000
    },
    {
      "epoch": 14.139485142464288,
      "grad_norm": 4.289942264556885,
      "learning_rate": 3.821709571461309e-05,
      "loss": 1.7654,
      "step": 185100
    },
    {
      "epoch": 14.147123978305705,
      "grad_norm": 4.669174671173096,
      "learning_rate": 3.8210730018078575e-05,
      "loss": 1.9273,
      "step": 185200
    },
    {
      "epoch": 14.154762814147125,
      "grad_norm": 4.900228023529053,
      "learning_rate": 3.8204364321544066e-05,
      "loss": 1.8675,
      "step": 185300
    },
    {
      "epoch": 14.162401649988542,
      "grad_norm": 5.668473720550537,
      "learning_rate": 3.819799862500955e-05,
      "loss": 1.8147,
      "step": 185400
    },
    {
      "epoch": 14.17004048582996,
      "grad_norm": 5.702951908111572,
      "learning_rate": 3.819163292847503e-05,
      "loss": 1.8422,
      "step": 185500
    },
    {
      "epoch": 14.177679321671377,
      "grad_norm": 5.098916530609131,
      "learning_rate": 3.8185267231940516e-05,
      "loss": 1.7803,
      "step": 185600
    },
    {
      "epoch": 14.185318157512794,
      "grad_norm": 7.644272804260254,
      "learning_rate": 3.8178901535406006e-05,
      "loss": 1.8324,
      "step": 185700
    },
    {
      "epoch": 14.192956993354214,
      "grad_norm": 4.382500648498535,
      "learning_rate": 3.817253583887149e-05,
      "loss": 1.7987,
      "step": 185800
    },
    {
      "epoch": 14.200595829195631,
      "grad_norm": 4.963811874389648,
      "learning_rate": 3.8166170142336974e-05,
      "loss": 1.7711,
      "step": 185900
    },
    {
      "epoch": 14.208234665037049,
      "grad_norm": 5.207120895385742,
      "learning_rate": 3.8159804445802464e-05,
      "loss": 1.87,
      "step": 186000
    },
    {
      "epoch": 14.215873500878466,
      "grad_norm": 4.673795700073242,
      "learning_rate": 3.815343874926795e-05,
      "loss": 1.8308,
      "step": 186100
    },
    {
      "epoch": 14.223512336719883,
      "grad_norm": 5.392840385437012,
      "learning_rate": 3.814707305273343e-05,
      "loss": 1.8006,
      "step": 186200
    },
    {
      "epoch": 14.231151172561301,
      "grad_norm": 5.5638747215271,
      "learning_rate": 3.814070735619892e-05,
      "loss": 1.7174,
      "step": 186300
    },
    {
      "epoch": 14.23879000840272,
      "grad_norm": 4.9491095542907715,
      "learning_rate": 3.8134341659664405e-05,
      "loss": 1.8673,
      "step": 186400
    },
    {
      "epoch": 14.246428844244138,
      "grad_norm": 5.081693649291992,
      "learning_rate": 3.812797596312989e-05,
      "loss": 1.7707,
      "step": 186500
    },
    {
      "epoch": 14.254067680085555,
      "grad_norm": 5.1293110847473145,
      "learning_rate": 3.812161026659537e-05,
      "loss": 1.8335,
      "step": 186600
    },
    {
      "epoch": 14.261706515926972,
      "grad_norm": 3.936084032058716,
      "learning_rate": 3.811524457006086e-05,
      "loss": 1.8514,
      "step": 186700
    },
    {
      "epoch": 14.26934535176839,
      "grad_norm": 4.887711524963379,
      "learning_rate": 3.8108878873526346e-05,
      "loss": 1.9133,
      "step": 186800
    },
    {
      "epoch": 14.27698418760981,
      "grad_norm": 4.717027187347412,
      "learning_rate": 3.810251317699183e-05,
      "loss": 1.8462,
      "step": 186900
    },
    {
      "epoch": 14.284623023451227,
      "grad_norm": 4.999364376068115,
      "learning_rate": 3.809614748045731e-05,
      "loss": 1.8391,
      "step": 187000
    },
    {
      "epoch": 14.292261859292644,
      "grad_norm": 7.579647064208984,
      "learning_rate": 3.80897817839228e-05,
      "loss": 1.8122,
      "step": 187100
    },
    {
      "epoch": 14.299900695134061,
      "grad_norm": 4.562612056732178,
      "learning_rate": 3.8083416087388287e-05,
      "loss": 1.8077,
      "step": 187200
    },
    {
      "epoch": 14.307539530975479,
      "grad_norm": 7.683398246765137,
      "learning_rate": 3.807705039085377e-05,
      "loss": 1.7964,
      "step": 187300
    },
    {
      "epoch": 14.315178366816896,
      "grad_norm": 5.16595983505249,
      "learning_rate": 3.8070684694319254e-05,
      "loss": 1.8201,
      "step": 187400
    },
    {
      "epoch": 14.322817202658316,
      "grad_norm": 5.9960150718688965,
      "learning_rate": 3.806431899778474e-05,
      "loss": 1.8344,
      "step": 187500
    },
    {
      "epoch": 14.330456038499733,
      "grad_norm": 5.857773303985596,
      "learning_rate": 3.805795330125023e-05,
      "loss": 1.8634,
      "step": 187600
    },
    {
      "epoch": 14.33809487434115,
      "grad_norm": 5.251617908477783,
      "learning_rate": 3.805158760471571e-05,
      "loss": 1.7539,
      "step": 187700
    },
    {
      "epoch": 14.345733710182568,
      "grad_norm": 4.093522071838379,
      "learning_rate": 3.8045221908181195e-05,
      "loss": 1.8045,
      "step": 187800
    },
    {
      "epoch": 14.353372546023985,
      "grad_norm": 5.308021545410156,
      "learning_rate": 3.803885621164668e-05,
      "loss": 1.8271,
      "step": 187900
    },
    {
      "epoch": 14.361011381865403,
      "grad_norm": 6.34128475189209,
      "learning_rate": 3.803249051511217e-05,
      "loss": 1.7766,
      "step": 188000
    },
    {
      "epoch": 14.368650217706822,
      "grad_norm": 5.029150485992432,
      "learning_rate": 3.802612481857765e-05,
      "loss": 1.8043,
      "step": 188100
    },
    {
      "epoch": 14.37628905354824,
      "grad_norm": 4.191378116607666,
      "learning_rate": 3.8019759122043136e-05,
      "loss": 1.8281,
      "step": 188200
    },
    {
      "epoch": 14.383927889389657,
      "grad_norm": 5.190170764923096,
      "learning_rate": 3.801339342550862e-05,
      "loss": 1.8026,
      "step": 188300
    },
    {
      "epoch": 14.391566725231074,
      "grad_norm": 5.154567718505859,
      "learning_rate": 3.80070277289741e-05,
      "loss": 1.8719,
      "step": 188400
    },
    {
      "epoch": 14.399205561072492,
      "grad_norm": 4.43347692489624,
      "learning_rate": 3.800066203243959e-05,
      "loss": 1.8883,
      "step": 188500
    },
    {
      "epoch": 14.406844396913911,
      "grad_norm": 7.719357013702393,
      "learning_rate": 3.7994296335905076e-05,
      "loss": 1.8398,
      "step": 188600
    },
    {
      "epoch": 14.414483232755329,
      "grad_norm": 7.901889324188232,
      "learning_rate": 3.798793063937056e-05,
      "loss": 1.8449,
      "step": 188700
    },
    {
      "epoch": 14.422122068596746,
      "grad_norm": 4.781552314758301,
      "learning_rate": 3.7981564942836043e-05,
      "loss": 1.7059,
      "step": 188800
    },
    {
      "epoch": 14.429760904438163,
      "grad_norm": 6.478456974029541,
      "learning_rate": 3.797519924630153e-05,
      "loss": 1.7369,
      "step": 188900
    },
    {
      "epoch": 14.43739974027958,
      "grad_norm": 4.713522911071777,
      "learning_rate": 3.796883354976702e-05,
      "loss": 1.7899,
      "step": 189000
    },
    {
      "epoch": 14.445038576121,
      "grad_norm": 5.548066139221191,
      "learning_rate": 3.79624678532325e-05,
      "loss": 1.8004,
      "step": 189100
    },
    {
      "epoch": 14.452677411962418,
      "grad_norm": 7.757017612457275,
      "learning_rate": 3.7956102156697984e-05,
      "loss": 1.9098,
      "step": 189200
    },
    {
      "epoch": 14.460316247803835,
      "grad_norm": 4.637407302856445,
      "learning_rate": 3.794973646016347e-05,
      "loss": 1.7673,
      "step": 189300
    },
    {
      "epoch": 14.467955083645252,
      "grad_norm": 4.190675258636475,
      "learning_rate": 3.794337076362896e-05,
      "loss": 1.8932,
      "step": 189400
    },
    {
      "epoch": 14.47559391948667,
      "grad_norm": 4.250411033630371,
      "learning_rate": 3.793700506709444e-05,
      "loss": 1.8756,
      "step": 189500
    },
    {
      "epoch": 14.483232755328087,
      "grad_norm": 5.060983657836914,
      "learning_rate": 3.7930639370559925e-05,
      "loss": 1.6892,
      "step": 189600
    },
    {
      "epoch": 14.490871591169507,
      "grad_norm": 5.885972023010254,
      "learning_rate": 3.7924273674025416e-05,
      "loss": 1.9231,
      "step": 189700
    },
    {
      "epoch": 14.498510427010924,
      "grad_norm": 6.152629375457764,
      "learning_rate": 3.79179079774909e-05,
      "loss": 1.8201,
      "step": 189800
    },
    {
      "epoch": 14.506149262852341,
      "grad_norm": 6.008790016174316,
      "learning_rate": 3.791154228095638e-05,
      "loss": 1.7366,
      "step": 189900
    },
    {
      "epoch": 14.513788098693759,
      "grad_norm": 5.317386150360107,
      "learning_rate": 3.790517658442187e-05,
      "loss": 1.7853,
      "step": 190000
    },
    {
      "epoch": 14.521426934535176,
      "grad_norm": 5.231252670288086,
      "learning_rate": 3.7898810887887357e-05,
      "loss": 1.8029,
      "step": 190100
    },
    {
      "epoch": 14.529065770376594,
      "grad_norm": 6.336948394775391,
      "learning_rate": 3.789244519135284e-05,
      "loss": 1.8214,
      "step": 190200
    },
    {
      "epoch": 14.536704606218013,
      "grad_norm": 5.783840179443359,
      "learning_rate": 3.788607949481833e-05,
      "loss": 1.7506,
      "step": 190300
    },
    {
      "epoch": 14.54434344205943,
      "grad_norm": 6.304731845855713,
      "learning_rate": 3.7879713798283814e-05,
      "loss": 1.8191,
      "step": 190400
    },
    {
      "epoch": 14.551982277900848,
      "grad_norm": 5.418978691101074,
      "learning_rate": 3.78733481017493e-05,
      "loss": 1.8035,
      "step": 190500
    },
    {
      "epoch": 14.559621113742265,
      "grad_norm": 4.740294933319092,
      "learning_rate": 3.786698240521478e-05,
      "loss": 1.8217,
      "step": 190600
    },
    {
      "epoch": 14.567259949583683,
      "grad_norm": 4.893802642822266,
      "learning_rate": 3.7860616708680265e-05,
      "loss": 1.8014,
      "step": 190700
    },
    {
      "epoch": 14.574898785425102,
      "grad_norm": 5.015142917633057,
      "learning_rate": 3.7854251012145755e-05,
      "loss": 1.7347,
      "step": 190800
    },
    {
      "epoch": 14.58253762126652,
      "grad_norm": 4.5399603843688965,
      "learning_rate": 3.784788531561124e-05,
      "loss": 1.827,
      "step": 190900
    },
    {
      "epoch": 14.590176457107937,
      "grad_norm": 5.1302995681762695,
      "learning_rate": 3.784151961907672e-05,
      "loss": 1.7877,
      "step": 191000
    },
    {
      "epoch": 14.597815292949354,
      "grad_norm": 5.797058582305908,
      "learning_rate": 3.7835153922542205e-05,
      "loss": 1.7854,
      "step": 191100
    },
    {
      "epoch": 14.605454128790772,
      "grad_norm": 3.830613136291504,
      "learning_rate": 3.7828788226007696e-05,
      "loss": 1.7892,
      "step": 191200
    },
    {
      "epoch": 14.613092964632191,
      "grad_norm": 5.229764938354492,
      "learning_rate": 3.782242252947318e-05,
      "loss": 1.7876,
      "step": 191300
    },
    {
      "epoch": 14.620731800473608,
      "grad_norm": 5.584578514099121,
      "learning_rate": 3.781605683293866e-05,
      "loss": 1.7226,
      "step": 191400
    },
    {
      "epoch": 14.628370636315026,
      "grad_norm": 8.651413917541504,
      "learning_rate": 3.7809691136404146e-05,
      "loss": 1.8838,
      "step": 191500
    },
    {
      "epoch": 14.636009472156443,
      "grad_norm": 4.929304122924805,
      "learning_rate": 3.780332543986963e-05,
      "loss": 1.7218,
      "step": 191600
    },
    {
      "epoch": 14.64364830799786,
      "grad_norm": 5.8300700187683105,
      "learning_rate": 3.779695974333512e-05,
      "loss": 1.8282,
      "step": 191700
    },
    {
      "epoch": 14.651287143839278,
      "grad_norm": 4.6842360496521,
      "learning_rate": 3.7790594046800604e-05,
      "loss": 1.8777,
      "step": 191800
    },
    {
      "epoch": 14.658925979680697,
      "grad_norm": 5.350640296936035,
      "learning_rate": 3.778422835026609e-05,
      "loss": 1.8726,
      "step": 191900
    },
    {
      "epoch": 14.666564815522115,
      "grad_norm": 4.355090618133545,
      "learning_rate": 3.777786265373157e-05,
      "loss": 1.7497,
      "step": 192000
    },
    {
      "epoch": 14.674203651363532,
      "grad_norm": 3.750788688659668,
      "learning_rate": 3.7771496957197054e-05,
      "loss": 1.871,
      "step": 192100
    },
    {
      "epoch": 14.68184248720495,
      "grad_norm": 4.496485710144043,
      "learning_rate": 3.7765131260662545e-05,
      "loss": 1.8205,
      "step": 192200
    },
    {
      "epoch": 14.689481323046367,
      "grad_norm": 5.622904300689697,
      "learning_rate": 3.775876556412803e-05,
      "loss": 1.7606,
      "step": 192300
    },
    {
      "epoch": 14.697120158887785,
      "grad_norm": 5.947119235992432,
      "learning_rate": 3.775239986759351e-05,
      "loss": 1.9271,
      "step": 192400
    },
    {
      "epoch": 14.704758994729204,
      "grad_norm": 6.1795973777771,
      "learning_rate": 3.7746034171058995e-05,
      "loss": 1.8707,
      "step": 192500
    },
    {
      "epoch": 14.712397830570621,
      "grad_norm": 4.434571743011475,
      "learning_rate": 3.7739668474524486e-05,
      "loss": 1.8248,
      "step": 192600
    },
    {
      "epoch": 14.720036666412039,
      "grad_norm": 4.753141403198242,
      "learning_rate": 3.773330277798997e-05,
      "loss": 1.8471,
      "step": 192700
    },
    {
      "epoch": 14.727675502253456,
      "grad_norm": 5.927921772003174,
      "learning_rate": 3.772693708145545e-05,
      "loss": 1.7638,
      "step": 192800
    },
    {
      "epoch": 14.735314338094874,
      "grad_norm": 5.577388763427734,
      "learning_rate": 3.7720571384920936e-05,
      "loss": 1.8095,
      "step": 192900
    },
    {
      "epoch": 14.742953173936293,
      "grad_norm": 4.66100549697876,
      "learning_rate": 3.771420568838642e-05,
      "loss": 1.6878,
      "step": 193000
    },
    {
      "epoch": 14.75059200977771,
      "grad_norm": 11.991456985473633,
      "learning_rate": 3.770783999185191e-05,
      "loss": 1.8457,
      "step": 193100
    },
    {
      "epoch": 14.758230845619128,
      "grad_norm": 5.0383620262146,
      "learning_rate": 3.7701474295317394e-05,
      "loss": 1.8142,
      "step": 193200
    },
    {
      "epoch": 14.765869681460545,
      "grad_norm": 4.097818851470947,
      "learning_rate": 3.769510859878288e-05,
      "loss": 1.7829,
      "step": 193300
    },
    {
      "epoch": 14.773508517301963,
      "grad_norm": 4.973057746887207,
      "learning_rate": 3.768874290224836e-05,
      "loss": 1.9168,
      "step": 193400
    },
    {
      "epoch": 14.78114735314338,
      "grad_norm": 4.260682106018066,
      "learning_rate": 3.768237720571385e-05,
      "loss": 1.8579,
      "step": 193500
    },
    {
      "epoch": 14.7887861889848,
      "grad_norm": 4.144012451171875,
      "learning_rate": 3.7676011509179335e-05,
      "loss": 1.8273,
      "step": 193600
    },
    {
      "epoch": 14.796425024826217,
      "grad_norm": 6.231870174407959,
      "learning_rate": 3.7669645812644825e-05,
      "loss": 1.8688,
      "step": 193700
    },
    {
      "epoch": 14.804063860667634,
      "grad_norm": 4.208641529083252,
      "learning_rate": 3.766328011611031e-05,
      "loss": 1.8076,
      "step": 193800
    },
    {
      "epoch": 14.811702696509052,
      "grad_norm": 6.333381175994873,
      "learning_rate": 3.765691441957579e-05,
      "loss": 1.793,
      "step": 193900
    },
    {
      "epoch": 14.81934153235047,
      "grad_norm": 5.651453018188477,
      "learning_rate": 3.765054872304128e-05,
      "loss": 1.8707,
      "step": 194000
    },
    {
      "epoch": 14.826980368191888,
      "grad_norm": 5.172697067260742,
      "learning_rate": 3.7644183026506766e-05,
      "loss": 1.871,
      "step": 194100
    },
    {
      "epoch": 14.834619204033306,
      "grad_norm": 5.249651908874512,
      "learning_rate": 3.763781732997225e-05,
      "loss": 1.8459,
      "step": 194200
    },
    {
      "epoch": 14.842258039874723,
      "grad_norm": 5.136164665222168,
      "learning_rate": 3.763145163343773e-05,
      "loss": 1.8357,
      "step": 194300
    },
    {
      "epoch": 14.84989687571614,
      "grad_norm": 5.944066524505615,
      "learning_rate": 3.762508593690322e-05,
      "loss": 1.8163,
      "step": 194400
    },
    {
      "epoch": 14.857535711557558,
      "grad_norm": 5.657197952270508,
      "learning_rate": 3.761872024036871e-05,
      "loss": 1.6931,
      "step": 194500
    },
    {
      "epoch": 14.865174547398976,
      "grad_norm": 5.769853115081787,
      "learning_rate": 3.761235454383419e-05,
      "loss": 1.8376,
      "step": 194600
    },
    {
      "epoch": 14.872813383240395,
      "grad_norm": 4.033280372619629,
      "learning_rate": 3.7605988847299674e-05,
      "loss": 1.804,
      "step": 194700
    },
    {
      "epoch": 14.880452219081812,
      "grad_norm": 4.384440898895264,
      "learning_rate": 3.759962315076516e-05,
      "loss": 1.772,
      "step": 194800
    },
    {
      "epoch": 14.88809105492323,
      "grad_norm": 4.74094820022583,
      "learning_rate": 3.759325745423065e-05,
      "loss": 1.869,
      "step": 194900
    },
    {
      "epoch": 14.895729890764647,
      "grad_norm": 6.552380561828613,
      "learning_rate": 3.758689175769613e-05,
      "loss": 1.911,
      "step": 195000
    },
    {
      "epoch": 14.903368726606065,
      "grad_norm": 5.6379828453063965,
      "learning_rate": 3.7580526061161615e-05,
      "loss": 1.8533,
      "step": 195100
    },
    {
      "epoch": 14.911007562447484,
      "grad_norm": 3.6140952110290527,
      "learning_rate": 3.75741603646271e-05,
      "loss": 1.7954,
      "step": 195200
    },
    {
      "epoch": 14.918646398288901,
      "grad_norm": 4.672685623168945,
      "learning_rate": 3.756779466809258e-05,
      "loss": 1.8723,
      "step": 195300
    },
    {
      "epoch": 14.926285234130319,
      "grad_norm": 4.311548709869385,
      "learning_rate": 3.756142897155807e-05,
      "loss": 1.8373,
      "step": 195400
    },
    {
      "epoch": 14.933924069971736,
      "grad_norm": 6.652279376983643,
      "learning_rate": 3.7555063275023556e-05,
      "loss": 1.7928,
      "step": 195500
    },
    {
      "epoch": 14.941562905813154,
      "grad_norm": 10.010393142700195,
      "learning_rate": 3.754869757848904e-05,
      "loss": 1.8177,
      "step": 195600
    },
    {
      "epoch": 14.949201741654571,
      "grad_norm": 5.627156734466553,
      "learning_rate": 3.754233188195452e-05,
      "loss": 1.8105,
      "step": 195700
    },
    {
      "epoch": 14.95684057749599,
      "grad_norm": 5.494160175323486,
      "learning_rate": 3.753596618542001e-05,
      "loss": 1.8425,
      "step": 195800
    },
    {
      "epoch": 14.964479413337408,
      "grad_norm": 3.520433187484741,
      "learning_rate": 3.7529600488885497e-05,
      "loss": 1.8315,
      "step": 195900
    },
    {
      "epoch": 14.972118249178825,
      "grad_norm": 6.9137187004089355,
      "learning_rate": 3.752323479235098e-05,
      "loss": 1.767,
      "step": 196000
    },
    {
      "epoch": 14.979757085020243,
      "grad_norm": 4.630965709686279,
      "learning_rate": 3.7516869095816464e-05,
      "loss": 1.796,
      "step": 196100
    },
    {
      "epoch": 14.98739592086166,
      "grad_norm": 4.575767993927002,
      "learning_rate": 3.751050339928195e-05,
      "loss": 1.771,
      "step": 196200
    },
    {
      "epoch": 14.99503475670308,
      "grad_norm": 4.693465232849121,
      "learning_rate": 3.750413770274744e-05,
      "loss": 1.8283,
      "step": 196300
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.8209341764450073,
      "eval_runtime": 1.4716,
      "eval_samples_per_second": 468.87,
      "eval_steps_per_second": 468.87,
      "step": 196365
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.58086097240448,
      "eval_runtime": 27.7042,
      "eval_samples_per_second": 472.527,
      "eval_steps_per_second": 472.527,
      "step": 196365
    }
  ],
  "logging_steps": 100,
  "max_steps": 785460,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 121512295756800.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
