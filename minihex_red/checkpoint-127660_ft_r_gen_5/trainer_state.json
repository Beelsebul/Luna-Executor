{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 127660,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007833307222309259,
      "grad_norm": 11.540173530578613,
      "learning_rate": 4.999347224398141e-05,
      "loss": 4.6282,
      "step": 100
    },
    {
      "epoch": 0.015666614444618518,
      "grad_norm": 7.978172302246094,
      "learning_rate": 4.998694448796282e-05,
      "loss": 3.1429,
      "step": 200
    },
    {
      "epoch": 0.023499921666927777,
      "grad_norm": 7.5821146965026855,
      "learning_rate": 4.9980416731944226e-05,
      "loss": 2.8438,
      "step": 300
    },
    {
      "epoch": 0.031333228889237036,
      "grad_norm": 6.46599006652832,
      "learning_rate": 4.997388897592564e-05,
      "loss": 2.828,
      "step": 400
    },
    {
      "epoch": 0.039166536111546295,
      "grad_norm": 6.479909420013428,
      "learning_rate": 4.996736121990705e-05,
      "loss": 2.8127,
      "step": 500
    },
    {
      "epoch": 0.046999843333855554,
      "grad_norm": 5.755207061767578,
      "learning_rate": 4.996083346388846e-05,
      "loss": 2.6612,
      "step": 600
    },
    {
      "epoch": 0.05483315055616481,
      "grad_norm": 6.2831621170043945,
      "learning_rate": 4.995430570786986e-05,
      "loss": 2.6428,
      "step": 700
    },
    {
      "epoch": 0.06266645777847407,
      "grad_norm": 5.780747413635254,
      "learning_rate": 4.9947777951851276e-05,
      "loss": 2.6434,
      "step": 800
    },
    {
      "epoch": 0.07049976500078334,
      "grad_norm": 5.451594352722168,
      "learning_rate": 4.994125019583268e-05,
      "loss": 2.4887,
      "step": 900
    },
    {
      "epoch": 0.07833307222309259,
      "grad_norm": 5.487960338592529,
      "learning_rate": 4.9934722439814094e-05,
      "loss": 2.5645,
      "step": 1000
    },
    {
      "epoch": 0.08616637944540186,
      "grad_norm": 4.614451885223389,
      "learning_rate": 4.99281946837955e-05,
      "loss": 2.6156,
      "step": 1100
    },
    {
      "epoch": 0.09399968666771111,
      "grad_norm": 7.149703025817871,
      "learning_rate": 4.992166692777691e-05,
      "loss": 2.4604,
      "step": 1200
    },
    {
      "epoch": 0.10183299389002037,
      "grad_norm": 6.2258100509643555,
      "learning_rate": 4.991513917175832e-05,
      "loss": 2.5228,
      "step": 1300
    },
    {
      "epoch": 0.10966630111232963,
      "grad_norm": 5.4452009201049805,
      "learning_rate": 4.9908611415739724e-05,
      "loss": 2.5155,
      "step": 1400
    },
    {
      "epoch": 0.11749960833463889,
      "grad_norm": 6.061800479888916,
      "learning_rate": 4.9902083659721136e-05,
      "loss": 2.5487,
      "step": 1500
    },
    {
      "epoch": 0.12533291555694814,
      "grad_norm": 3.8109185695648193,
      "learning_rate": 4.989555590370255e-05,
      "loss": 2.4751,
      "step": 1600
    },
    {
      "epoch": 0.1331662227792574,
      "grad_norm": 6.115899562835693,
      "learning_rate": 4.9889028147683955e-05,
      "loss": 2.3672,
      "step": 1700
    },
    {
      "epoch": 0.14099953000156668,
      "grad_norm": 6.503889083862305,
      "learning_rate": 4.988250039166537e-05,
      "loss": 2.4864,
      "step": 1800
    },
    {
      "epoch": 0.1488328372238759,
      "grad_norm": 5.980323314666748,
      "learning_rate": 4.987597263564677e-05,
      "loss": 2.4674,
      "step": 1900
    },
    {
      "epoch": 0.15666614444618518,
      "grad_norm": 6.22133731842041,
      "learning_rate": 4.986944487962818e-05,
      "loss": 2.4429,
      "step": 2000
    },
    {
      "epoch": 0.16449945166849445,
      "grad_norm": 4.67347526550293,
      "learning_rate": 4.986291712360959e-05,
      "loss": 2.4223,
      "step": 2100
    },
    {
      "epoch": 0.1723327588908037,
      "grad_norm": 5.363992691040039,
      "learning_rate": 4.9856389367591e-05,
      "loss": 2.3662,
      "step": 2200
    },
    {
      "epoch": 0.18016606611311295,
      "grad_norm": 5.601268291473389,
      "learning_rate": 4.984986161157241e-05,
      "loss": 2.4572,
      "step": 2300
    },
    {
      "epoch": 0.18799937333542222,
      "grad_norm": 4.384665489196777,
      "learning_rate": 4.984333385555382e-05,
      "loss": 2.2841,
      "step": 2400
    },
    {
      "epoch": 0.19583268055773148,
      "grad_norm": 4.9136271476745605,
      "learning_rate": 4.983680609953523e-05,
      "loss": 2.4559,
      "step": 2500
    },
    {
      "epoch": 0.20366598778004075,
      "grad_norm": 4.50607442855835,
      "learning_rate": 4.9830278343516634e-05,
      "loss": 2.4366,
      "step": 2600
    },
    {
      "epoch": 0.21149929500234999,
      "grad_norm": 6.407818794250488,
      "learning_rate": 4.982375058749804e-05,
      "loss": 2.39,
      "step": 2700
    },
    {
      "epoch": 0.21933260222465925,
      "grad_norm": 4.789163112640381,
      "learning_rate": 4.981722283147945e-05,
      "loss": 2.3475,
      "step": 2800
    },
    {
      "epoch": 0.22716590944696852,
      "grad_norm": 6.096295356750488,
      "learning_rate": 4.9810695075460865e-05,
      "loss": 2.3772,
      "step": 2900
    },
    {
      "epoch": 0.23499921666927778,
      "grad_norm": 4.49558687210083,
      "learning_rate": 4.980416731944227e-05,
      "loss": 2.4344,
      "step": 3000
    },
    {
      "epoch": 0.24283252389158702,
      "grad_norm": 7.013221740722656,
      "learning_rate": 4.979763956342368e-05,
      "loss": 2.386,
      "step": 3100
    },
    {
      "epoch": 0.2506658311138963,
      "grad_norm": 5.5625200271606445,
      "learning_rate": 4.979111180740509e-05,
      "loss": 2.4325,
      "step": 3200
    },
    {
      "epoch": 0.2584991383362055,
      "grad_norm": 5.087408065795898,
      "learning_rate": 4.9784584051386494e-05,
      "loss": 2.2724,
      "step": 3300
    },
    {
      "epoch": 0.2663324455585148,
      "grad_norm": 5.005785942077637,
      "learning_rate": 4.977805629536791e-05,
      "loss": 2.3708,
      "step": 3400
    },
    {
      "epoch": 0.27416575278082406,
      "grad_norm": 5.801197528839111,
      "learning_rate": 4.977152853934931e-05,
      "loss": 2.345,
      "step": 3500
    },
    {
      "epoch": 0.28199906000313335,
      "grad_norm": 4.471706867218018,
      "learning_rate": 4.9765000783330725e-05,
      "loss": 2.3535,
      "step": 3600
    },
    {
      "epoch": 0.2898323672254426,
      "grad_norm": 5.845236778259277,
      "learning_rate": 4.975847302731214e-05,
      "loss": 2.3785,
      "step": 3700
    },
    {
      "epoch": 0.2976656744477518,
      "grad_norm": 5.005009174346924,
      "learning_rate": 4.9751945271293544e-05,
      "loss": 2.269,
      "step": 3800
    },
    {
      "epoch": 0.3054989816700611,
      "grad_norm": 4.719980716705322,
      "learning_rate": 4.974541751527495e-05,
      "loss": 2.2941,
      "step": 3900
    },
    {
      "epoch": 0.31333228889237036,
      "grad_norm": 5.734504222869873,
      "learning_rate": 4.973888975925636e-05,
      "loss": 2.3889,
      "step": 4000
    },
    {
      "epoch": 0.3211655961146796,
      "grad_norm": 5.682164669036865,
      "learning_rate": 4.973236200323777e-05,
      "loss": 2.3217,
      "step": 4100
    },
    {
      "epoch": 0.3289989033369889,
      "grad_norm": 4.343432426452637,
      "learning_rate": 4.972583424721918e-05,
      "loss": 2.3216,
      "step": 4200
    },
    {
      "epoch": 0.33683221055929813,
      "grad_norm": 4.926867485046387,
      "learning_rate": 4.9719306491200586e-05,
      "loss": 2.3243,
      "step": 4300
    },
    {
      "epoch": 0.3446655177816074,
      "grad_norm": 5.18257474899292,
      "learning_rate": 4.9712778735182e-05,
      "loss": 2.3182,
      "step": 4400
    },
    {
      "epoch": 0.35249882500391666,
      "grad_norm": 5.198777198791504,
      "learning_rate": 4.9706250979163404e-05,
      "loss": 2.3698,
      "step": 4500
    },
    {
      "epoch": 0.3603321322262259,
      "grad_norm": 4.421065330505371,
      "learning_rate": 4.969972322314481e-05,
      "loss": 2.3161,
      "step": 4600
    },
    {
      "epoch": 0.3681654394485352,
      "grad_norm": 4.067595481872559,
      "learning_rate": 4.969319546712622e-05,
      "loss": 2.3156,
      "step": 4700
    },
    {
      "epoch": 0.37599874667084443,
      "grad_norm": 6.632781028747559,
      "learning_rate": 4.9686667711107635e-05,
      "loss": 2.3182,
      "step": 4800
    },
    {
      "epoch": 0.38383205389315367,
      "grad_norm": 4.6471848487854,
      "learning_rate": 4.968013995508904e-05,
      "loss": 2.2918,
      "step": 4900
    },
    {
      "epoch": 0.39166536111546296,
      "grad_norm": 4.728133678436279,
      "learning_rate": 4.9673612199070453e-05,
      "loss": 2.2637,
      "step": 5000
    },
    {
      "epoch": 0.3994986683377722,
      "grad_norm": 5.720607280731201,
      "learning_rate": 4.966708444305186e-05,
      "loss": 2.3336,
      "step": 5100
    },
    {
      "epoch": 0.4073319755600815,
      "grad_norm": 4.715705394744873,
      "learning_rate": 4.9660556687033265e-05,
      "loss": 2.3017,
      "step": 5200
    },
    {
      "epoch": 0.41516528278239073,
      "grad_norm": 5.488768100738525,
      "learning_rate": 4.965402893101468e-05,
      "loss": 2.3278,
      "step": 5300
    },
    {
      "epoch": 0.42299859000469997,
      "grad_norm": 3.787397861480713,
      "learning_rate": 4.964750117499608e-05,
      "loss": 2.2712,
      "step": 5400
    },
    {
      "epoch": 0.43083189722700926,
      "grad_norm": 4.577098846435547,
      "learning_rate": 4.9640973418977496e-05,
      "loss": 2.3534,
      "step": 5500
    },
    {
      "epoch": 0.4386652044493185,
      "grad_norm": 5.038837909698486,
      "learning_rate": 4.963444566295891e-05,
      "loss": 2.269,
      "step": 5600
    },
    {
      "epoch": 0.44649851167162774,
      "grad_norm": 4.650664806365967,
      "learning_rate": 4.9627917906940314e-05,
      "loss": 2.3771,
      "step": 5700
    },
    {
      "epoch": 0.45433181889393703,
      "grad_norm": 4.2072248458862305,
      "learning_rate": 4.962139015092172e-05,
      "loss": 2.2643,
      "step": 5800
    },
    {
      "epoch": 0.46216512611624627,
      "grad_norm": 4.1768317222595215,
      "learning_rate": 4.9614862394903126e-05,
      "loss": 2.2683,
      "step": 5900
    },
    {
      "epoch": 0.46999843333855557,
      "grad_norm": 4.833404064178467,
      "learning_rate": 4.960833463888454e-05,
      "loss": 2.3385,
      "step": 6000
    },
    {
      "epoch": 0.4778317405608648,
      "grad_norm": 4.691647529602051,
      "learning_rate": 4.960180688286595e-05,
      "loss": 2.2282,
      "step": 6100
    },
    {
      "epoch": 0.48566504778317404,
      "grad_norm": 5.124332904815674,
      "learning_rate": 4.9595279126847357e-05,
      "loss": 2.2906,
      "step": 6200
    },
    {
      "epoch": 0.49349835500548334,
      "grad_norm": 4.049105644226074,
      "learning_rate": 4.958875137082877e-05,
      "loss": 2.2569,
      "step": 6300
    },
    {
      "epoch": 0.5013316622277926,
      "grad_norm": 4.43967342376709,
      "learning_rate": 4.9582223614810175e-05,
      "loss": 2.184,
      "step": 6400
    },
    {
      "epoch": 0.5091649694501018,
      "grad_norm": 6.591732978820801,
      "learning_rate": 4.957569585879158e-05,
      "loss": 2.2487,
      "step": 6500
    },
    {
      "epoch": 0.516998276672411,
      "grad_norm": 4.613829612731934,
      "learning_rate": 4.956916810277299e-05,
      "loss": 2.1903,
      "step": 6600
    },
    {
      "epoch": 0.5248315838947204,
      "grad_norm": 5.528923034667969,
      "learning_rate": 4.95626403467544e-05,
      "loss": 2.2963,
      "step": 6700
    },
    {
      "epoch": 0.5326648911170296,
      "grad_norm": 6.3916015625,
      "learning_rate": 4.955611259073581e-05,
      "loss": 2.2907,
      "step": 6800
    },
    {
      "epoch": 0.5404981983393389,
      "grad_norm": 6.273540496826172,
      "learning_rate": 4.9549584834717224e-05,
      "loss": 2.2568,
      "step": 6900
    },
    {
      "epoch": 0.5483315055616481,
      "grad_norm": 5.479650497436523,
      "learning_rate": 4.954305707869863e-05,
      "loss": 2.2672,
      "step": 7000
    },
    {
      "epoch": 0.5561648127839574,
      "grad_norm": 4.9421234130859375,
      "learning_rate": 4.9536529322680036e-05,
      "loss": 2.3497,
      "step": 7100
    },
    {
      "epoch": 0.5639981200062667,
      "grad_norm": 6.754128932952881,
      "learning_rate": 4.953000156666145e-05,
      "loss": 2.2255,
      "step": 7200
    },
    {
      "epoch": 0.5718314272285759,
      "grad_norm": 6.953390598297119,
      "learning_rate": 4.9523473810642854e-05,
      "loss": 2.2478,
      "step": 7300
    },
    {
      "epoch": 0.5796647344508852,
      "grad_norm": 4.988915920257568,
      "learning_rate": 4.9516946054624266e-05,
      "loss": 2.1897,
      "step": 7400
    },
    {
      "epoch": 0.5874980416731944,
      "grad_norm": 5.7152791023254395,
      "learning_rate": 4.951041829860568e-05,
      "loss": 2.2343,
      "step": 7500
    },
    {
      "epoch": 0.5953313488955037,
      "grad_norm": 5.915982246398926,
      "learning_rate": 4.9503890542587085e-05,
      "loss": 2.2887,
      "step": 7600
    },
    {
      "epoch": 0.6031646561178129,
      "grad_norm": 5.462746620178223,
      "learning_rate": 4.949736278656849e-05,
      "loss": 2.155,
      "step": 7700
    },
    {
      "epoch": 0.6109979633401222,
      "grad_norm": 5.5770487785339355,
      "learning_rate": 4.9490835030549896e-05,
      "loss": 2.2191,
      "step": 7800
    },
    {
      "epoch": 0.6188312705624315,
      "grad_norm": 5.446353912353516,
      "learning_rate": 4.948430727453131e-05,
      "loss": 2.2595,
      "step": 7900
    },
    {
      "epoch": 0.6266645777847407,
      "grad_norm": 4.3125081062316895,
      "learning_rate": 4.947777951851272e-05,
      "loss": 2.1652,
      "step": 8000
    },
    {
      "epoch": 0.63449788500705,
      "grad_norm": 4.842166423797607,
      "learning_rate": 4.947125176249413e-05,
      "loss": 2.2172,
      "step": 8100
    },
    {
      "epoch": 0.6423311922293592,
      "grad_norm": 4.451799392700195,
      "learning_rate": 4.946472400647554e-05,
      "loss": 2.1637,
      "step": 8200
    },
    {
      "epoch": 0.6501644994516685,
      "grad_norm": 5.307904243469238,
      "learning_rate": 4.9458196250456945e-05,
      "loss": 2.1575,
      "step": 8300
    },
    {
      "epoch": 0.6579978066739778,
      "grad_norm": 6.706522464752197,
      "learning_rate": 4.945166849443835e-05,
      "loss": 2.1968,
      "step": 8400
    },
    {
      "epoch": 0.665831113896287,
      "grad_norm": 5.0111823081970215,
      "learning_rate": 4.9445140738419764e-05,
      "loss": 2.331,
      "step": 8500
    },
    {
      "epoch": 0.6736644211185963,
      "grad_norm": 4.672807693481445,
      "learning_rate": 4.943861298240117e-05,
      "loss": 2.3216,
      "step": 8600
    },
    {
      "epoch": 0.6814977283409055,
      "grad_norm": 6.1778645515441895,
      "learning_rate": 4.943208522638258e-05,
      "loss": 2.1729,
      "step": 8700
    },
    {
      "epoch": 0.6893310355632148,
      "grad_norm": 5.289903163909912,
      "learning_rate": 4.9425557470363995e-05,
      "loss": 2.2065,
      "step": 8800
    },
    {
      "epoch": 0.6971643427855241,
      "grad_norm": 4.657099723815918,
      "learning_rate": 4.94190297143454e-05,
      "loss": 2.1855,
      "step": 8900
    },
    {
      "epoch": 0.7049976500078333,
      "grad_norm": 4.221887588500977,
      "learning_rate": 4.9412501958326806e-05,
      "loss": 2.3026,
      "step": 9000
    },
    {
      "epoch": 0.7128309572301426,
      "grad_norm": 5.26032829284668,
      "learning_rate": 4.940597420230822e-05,
      "loss": 2.203,
      "step": 9100
    },
    {
      "epoch": 0.7206642644524518,
      "grad_norm": 7.589452266693115,
      "learning_rate": 4.9399446446289624e-05,
      "loss": 2.1524,
      "step": 9200
    },
    {
      "epoch": 0.728497571674761,
      "grad_norm": 4.6626482009887695,
      "learning_rate": 4.939291869027104e-05,
      "loss": 2.1899,
      "step": 9300
    },
    {
      "epoch": 0.7363308788970704,
      "grad_norm": 4.3108625411987305,
      "learning_rate": 4.938639093425244e-05,
      "loss": 2.1656,
      "step": 9400
    },
    {
      "epoch": 0.7441641861193796,
      "grad_norm": 5.290939807891846,
      "learning_rate": 4.9379863178233855e-05,
      "loss": 2.2053,
      "step": 9500
    },
    {
      "epoch": 0.7519974933416889,
      "grad_norm": 6.35273551940918,
      "learning_rate": 4.937333542221526e-05,
      "loss": 2.2058,
      "step": 9600
    },
    {
      "epoch": 0.7598308005639981,
      "grad_norm": 5.47995662689209,
      "learning_rate": 4.936680766619667e-05,
      "loss": 2.2339,
      "step": 9700
    },
    {
      "epoch": 0.7676641077863073,
      "grad_norm": 5.519262313842773,
      "learning_rate": 4.936027991017808e-05,
      "loss": 2.1605,
      "step": 9800
    },
    {
      "epoch": 0.7754974150086167,
      "grad_norm": 4.763408184051514,
      "learning_rate": 4.9353752154159485e-05,
      "loss": 2.22,
      "step": 9900
    },
    {
      "epoch": 0.7833307222309259,
      "grad_norm": 5.280427932739258,
      "learning_rate": 4.93472243981409e-05,
      "loss": 2.3148,
      "step": 10000
    },
    {
      "epoch": 0.7911640294532352,
      "grad_norm": 4.728559970855713,
      "learning_rate": 4.934069664212231e-05,
      "loss": 2.2671,
      "step": 10100
    },
    {
      "epoch": 0.7989973366755444,
      "grad_norm": 6.843000411987305,
      "learning_rate": 4.9334168886103716e-05,
      "loss": 2.2031,
      "step": 10200
    },
    {
      "epoch": 0.8068306438978536,
      "grad_norm": 4.864438056945801,
      "learning_rate": 4.932764113008512e-05,
      "loss": 2.0999,
      "step": 10300
    },
    {
      "epoch": 0.814663951120163,
      "grad_norm": 4.980437755584717,
      "learning_rate": 4.9321113374066534e-05,
      "loss": 2.1602,
      "step": 10400
    },
    {
      "epoch": 0.8224972583424722,
      "grad_norm": 4.805253982543945,
      "learning_rate": 4.931458561804794e-05,
      "loss": 2.1446,
      "step": 10500
    },
    {
      "epoch": 0.8303305655647815,
      "grad_norm": 5.506919860839844,
      "learning_rate": 4.930805786202935e-05,
      "loss": 2.2293,
      "step": 10600
    },
    {
      "epoch": 0.8381638727870907,
      "grad_norm": 5.158807277679443,
      "learning_rate": 4.9301530106010765e-05,
      "loss": 2.188,
      "step": 10700
    },
    {
      "epoch": 0.8459971800093999,
      "grad_norm": 5.235378742218018,
      "learning_rate": 4.929500234999217e-05,
      "loss": 2.1438,
      "step": 10800
    },
    {
      "epoch": 0.8538304872317092,
      "grad_norm": 4.964086055755615,
      "learning_rate": 4.928847459397358e-05,
      "loss": 2.1181,
      "step": 10900
    },
    {
      "epoch": 0.8616637944540185,
      "grad_norm": 4.715092182159424,
      "learning_rate": 4.928194683795498e-05,
      "loss": 2.1524,
      "step": 11000
    },
    {
      "epoch": 0.8694971016763278,
      "grad_norm": 8.22596263885498,
      "learning_rate": 4.9275419081936395e-05,
      "loss": 2.2792,
      "step": 11100
    },
    {
      "epoch": 0.877330408898637,
      "grad_norm": 4.21669340133667,
      "learning_rate": 4.926889132591781e-05,
      "loss": 2.2717,
      "step": 11200
    },
    {
      "epoch": 0.8851637161209462,
      "grad_norm": 4.555757999420166,
      "learning_rate": 4.926236356989921e-05,
      "loss": 2.2667,
      "step": 11300
    },
    {
      "epoch": 0.8929970233432555,
      "grad_norm": 5.1033244132995605,
      "learning_rate": 4.9255835813880626e-05,
      "loss": 2.1403,
      "step": 11400
    },
    {
      "epoch": 0.9008303305655648,
      "grad_norm": 6.477269649505615,
      "learning_rate": 4.924930805786203e-05,
      "loss": 2.3175,
      "step": 11500
    },
    {
      "epoch": 0.9086636377878741,
      "grad_norm": 4.364744186401367,
      "learning_rate": 4.924278030184344e-05,
      "loss": 2.1666,
      "step": 11600
    },
    {
      "epoch": 0.9164969450101833,
      "grad_norm": 6.397687911987305,
      "learning_rate": 4.923625254582485e-05,
      "loss": 2.126,
      "step": 11700
    },
    {
      "epoch": 0.9243302522324925,
      "grad_norm": 5.804946422576904,
      "learning_rate": 4.9229724789806256e-05,
      "loss": 2.19,
      "step": 11800
    },
    {
      "epoch": 0.9321635594548018,
      "grad_norm": 5.370138645172119,
      "learning_rate": 4.922319703378767e-05,
      "loss": 2.1406,
      "step": 11900
    },
    {
      "epoch": 0.9399968666771111,
      "grad_norm": 5.041280269622803,
      "learning_rate": 4.921666927776908e-05,
      "loss": 2.2183,
      "step": 12000
    },
    {
      "epoch": 0.9478301738994204,
      "grad_norm": 3.7907209396362305,
      "learning_rate": 4.921014152175049e-05,
      "loss": 2.2769,
      "step": 12100
    },
    {
      "epoch": 0.9556634811217296,
      "grad_norm": 4.192178249359131,
      "learning_rate": 4.920361376573189e-05,
      "loss": 2.239,
      "step": 12200
    },
    {
      "epoch": 0.9634967883440388,
      "grad_norm": 6.872035503387451,
      "learning_rate": 4.9197086009713305e-05,
      "loss": 2.1131,
      "step": 12300
    },
    {
      "epoch": 0.9713300955663481,
      "grad_norm": 4.8373494148254395,
      "learning_rate": 4.919055825369471e-05,
      "loss": 2.1748,
      "step": 12400
    },
    {
      "epoch": 0.9791634027886573,
      "grad_norm": 4.575335502624512,
      "learning_rate": 4.918403049767612e-05,
      "loss": 2.221,
      "step": 12500
    },
    {
      "epoch": 0.9869967100109667,
      "grad_norm": 4.573701858520508,
      "learning_rate": 4.9177502741657536e-05,
      "loss": 2.1481,
      "step": 12600
    },
    {
      "epoch": 0.9948300172332759,
      "grad_norm": 5.283926963806152,
      "learning_rate": 4.917097498563894e-05,
      "loss": 2.1947,
      "step": 12700
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.069763660430908,
      "eval_runtime": 2.9834,
      "eval_samples_per_second": 225.249,
      "eval_steps_per_second": 225.249,
      "step": 12766
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.958105444908142,
      "eval_runtime": 58.6426,
      "eval_samples_per_second": 217.692,
      "eval_steps_per_second": 217.692,
      "step": 12766
    },
    {
      "epoch": 1.0026633244555851,
      "grad_norm": 4.797819137573242,
      "learning_rate": 4.916444722962035e-05,
      "loss": 2.1368,
      "step": 12800
    },
    {
      "epoch": 1.0104966316778945,
      "grad_norm": 4.901352882385254,
      "learning_rate": 4.915791947360175e-05,
      "loss": 2.1786,
      "step": 12900
    },
    {
      "epoch": 1.0183299389002036,
      "grad_norm": 5.069640159606934,
      "learning_rate": 4.9151391717583166e-05,
      "loss": 2.1312,
      "step": 13000
    },
    {
      "epoch": 1.026163246122513,
      "grad_norm": 4.644036769866943,
      "learning_rate": 4.914486396156457e-05,
      "loss": 2.098,
      "step": 13100
    },
    {
      "epoch": 1.033996553344822,
      "grad_norm": 4.969990253448486,
      "learning_rate": 4.9138336205545984e-05,
      "loss": 2.0808,
      "step": 13200
    },
    {
      "epoch": 1.0418298605671314,
      "grad_norm": 5.422013759613037,
      "learning_rate": 4.9131808449527396e-05,
      "loss": 2.1426,
      "step": 13300
    },
    {
      "epoch": 1.0496631677894408,
      "grad_norm": 5.716988563537598,
      "learning_rate": 4.91252806935088e-05,
      "loss": 2.1562,
      "step": 13400
    },
    {
      "epoch": 1.05749647501175,
      "grad_norm": 4.65447473526001,
      "learning_rate": 4.911875293749021e-05,
      "loss": 2.1109,
      "step": 13500
    },
    {
      "epoch": 1.0653297822340593,
      "grad_norm": 4.518239974975586,
      "learning_rate": 4.911222518147162e-05,
      "loss": 2.1415,
      "step": 13600
    },
    {
      "epoch": 1.0731630894563684,
      "grad_norm": 5.108067989349365,
      "learning_rate": 4.9105697425453026e-05,
      "loss": 2.1495,
      "step": 13700
    },
    {
      "epoch": 1.0809963966786778,
      "grad_norm": 5.551692962646484,
      "learning_rate": 4.909916966943444e-05,
      "loss": 2.1284,
      "step": 13800
    },
    {
      "epoch": 1.088829703900987,
      "grad_norm": 6.631732940673828,
      "learning_rate": 4.909264191341585e-05,
      "loss": 2.1699,
      "step": 13900
    },
    {
      "epoch": 1.0966630111232962,
      "grad_norm": 3.5112507343292236,
      "learning_rate": 4.908611415739726e-05,
      "loss": 2.1287,
      "step": 14000
    },
    {
      "epoch": 1.1044963183456056,
      "grad_norm": 5.482921123504639,
      "learning_rate": 4.907958640137866e-05,
      "loss": 2.1101,
      "step": 14100
    },
    {
      "epoch": 1.1123296255679147,
      "grad_norm": 5.71177864074707,
      "learning_rate": 4.9073058645360076e-05,
      "loss": 2.0993,
      "step": 14200
    },
    {
      "epoch": 1.120162932790224,
      "grad_norm": 5.233309745788574,
      "learning_rate": 4.906653088934148e-05,
      "loss": 2.1248,
      "step": 14300
    },
    {
      "epoch": 1.1279962400125334,
      "grad_norm": 4.268956661224365,
      "learning_rate": 4.9060003133322894e-05,
      "loss": 2.1269,
      "step": 14400
    },
    {
      "epoch": 1.1358295472348425,
      "grad_norm": 4.8624491691589355,
      "learning_rate": 4.90534753773043e-05,
      "loss": 2.0442,
      "step": 14500
    },
    {
      "epoch": 1.1436628544571519,
      "grad_norm": 5.60540771484375,
      "learning_rate": 4.904694762128571e-05,
      "loss": 2.1624,
      "step": 14600
    },
    {
      "epoch": 1.151496161679461,
      "grad_norm": 7.3562397956848145,
      "learning_rate": 4.904041986526712e-05,
      "loss": 2.1696,
      "step": 14700
    },
    {
      "epoch": 1.1593294689017704,
      "grad_norm": 5.473916053771973,
      "learning_rate": 4.9033892109248524e-05,
      "loss": 2.1813,
      "step": 14800
    },
    {
      "epoch": 1.1671627761240795,
      "grad_norm": 4.109088897705078,
      "learning_rate": 4.9027364353229936e-05,
      "loss": 2.1025,
      "step": 14900
    },
    {
      "epoch": 1.1749960833463888,
      "grad_norm": 7.242049694061279,
      "learning_rate": 4.902083659721134e-05,
      "loss": 2.0953,
      "step": 15000
    },
    {
      "epoch": 1.1828293905686982,
      "grad_norm": 5.121238708496094,
      "learning_rate": 4.9014308841192755e-05,
      "loss": 2.0978,
      "step": 15100
    },
    {
      "epoch": 1.1906626977910073,
      "grad_norm": 5.301401615142822,
      "learning_rate": 4.900778108517417e-05,
      "loss": 2.1584,
      "step": 15200
    },
    {
      "epoch": 1.1984960050133167,
      "grad_norm": 5.459949970245361,
      "learning_rate": 4.900125332915557e-05,
      "loss": 2.1273,
      "step": 15300
    },
    {
      "epoch": 1.206329312235626,
      "grad_norm": 6.621047019958496,
      "learning_rate": 4.899472557313698e-05,
      "loss": 2.1576,
      "step": 15400
    },
    {
      "epoch": 1.2141626194579351,
      "grad_norm": 4.139832019805908,
      "learning_rate": 4.898819781711839e-05,
      "loss": 2.0992,
      "step": 15500
    },
    {
      "epoch": 1.2219959266802445,
      "grad_norm": 6.305038928985596,
      "learning_rate": 4.89816700610998e-05,
      "loss": 2.1827,
      "step": 15600
    },
    {
      "epoch": 1.2298292339025536,
      "grad_norm": 4.325533390045166,
      "learning_rate": 4.897514230508121e-05,
      "loss": 2.1032,
      "step": 15700
    },
    {
      "epoch": 1.237662541124863,
      "grad_norm": 6.65619421005249,
      "learning_rate": 4.896861454906262e-05,
      "loss": 2.2169,
      "step": 15800
    },
    {
      "epoch": 1.245495848347172,
      "grad_norm": 4.267451286315918,
      "learning_rate": 4.896208679304403e-05,
      "loss": 2.1631,
      "step": 15900
    },
    {
      "epoch": 1.2533291555694814,
      "grad_norm": 4.514301300048828,
      "learning_rate": 4.8955559037025434e-05,
      "loss": 2.1983,
      "step": 16000
    },
    {
      "epoch": 1.2611624627917908,
      "grad_norm": 4.758921146392822,
      "learning_rate": 4.894903128100684e-05,
      "loss": 2.172,
      "step": 16100
    },
    {
      "epoch": 1.2689957700141,
      "grad_norm": 5.194299697875977,
      "learning_rate": 4.894250352498825e-05,
      "loss": 2.1682,
      "step": 16200
    },
    {
      "epoch": 1.2768290772364093,
      "grad_norm": 4.1105804443359375,
      "learning_rate": 4.893597576896966e-05,
      "loss": 2.1983,
      "step": 16300
    },
    {
      "epoch": 1.2846623844587186,
      "grad_norm": 4.361325263977051,
      "learning_rate": 4.892944801295107e-05,
      "loss": 2.1616,
      "step": 16400
    },
    {
      "epoch": 1.2924956916810277,
      "grad_norm": 4.6535563468933105,
      "learning_rate": 4.892292025693248e-05,
      "loss": 2.1683,
      "step": 16500
    },
    {
      "epoch": 1.3003289989033369,
      "grad_norm": 4.2735490798950195,
      "learning_rate": 4.891639250091389e-05,
      "loss": 2.1878,
      "step": 16600
    },
    {
      "epoch": 1.3081623061256462,
      "grad_norm": 5.239178657531738,
      "learning_rate": 4.8909864744895294e-05,
      "loss": 2.0747,
      "step": 16700
    },
    {
      "epoch": 1.3159956133479556,
      "grad_norm": 4.508316993713379,
      "learning_rate": 4.890333698887671e-05,
      "loss": 2.0831,
      "step": 16800
    },
    {
      "epoch": 1.3238289205702647,
      "grad_norm": 5.145707130432129,
      "learning_rate": 4.889680923285811e-05,
      "loss": 2.1291,
      "step": 16900
    },
    {
      "epoch": 1.331662227792574,
      "grad_norm": 5.75711727142334,
      "learning_rate": 4.8890281476839525e-05,
      "loss": 2.0739,
      "step": 17000
    },
    {
      "epoch": 1.3394955350148834,
      "grad_norm": 8.339942932128906,
      "learning_rate": 4.888375372082094e-05,
      "loss": 2.1796,
      "step": 17100
    },
    {
      "epoch": 1.3473288422371925,
      "grad_norm": 5.613107681274414,
      "learning_rate": 4.8877225964802343e-05,
      "loss": 2.1361,
      "step": 17200
    },
    {
      "epoch": 1.3551621494595019,
      "grad_norm": 5.384558200836182,
      "learning_rate": 4.887069820878375e-05,
      "loss": 2.0309,
      "step": 17300
    },
    {
      "epoch": 1.362995456681811,
      "grad_norm": 4.268584728240967,
      "learning_rate": 4.886417045276516e-05,
      "loss": 2.071,
      "step": 17400
    },
    {
      "epoch": 1.3708287639041203,
      "grad_norm": 3.901287794113159,
      "learning_rate": 4.885764269674657e-05,
      "loss": 2.069,
      "step": 17500
    },
    {
      "epoch": 1.3786620711264295,
      "grad_norm": 4.0657958984375,
      "learning_rate": 4.885111494072798e-05,
      "loss": 2.1827,
      "step": 17600
    },
    {
      "epoch": 1.3864953783487388,
      "grad_norm": 5.088072299957275,
      "learning_rate": 4.8844587184709386e-05,
      "loss": 2.1095,
      "step": 17700
    },
    {
      "epoch": 1.3943286855710482,
      "grad_norm": 5.862856864929199,
      "learning_rate": 4.88380594286908e-05,
      "loss": 2.2273,
      "step": 17800
    },
    {
      "epoch": 1.4021619927933573,
      "grad_norm": 5.161714553833008,
      "learning_rate": 4.8831531672672204e-05,
      "loss": 1.9926,
      "step": 17900
    },
    {
      "epoch": 1.4099953000156666,
      "grad_norm": 4.77968692779541,
      "learning_rate": 4.882500391665361e-05,
      "loss": 2.0401,
      "step": 18000
    },
    {
      "epoch": 1.417828607237976,
      "grad_norm": 5.626577377319336,
      "learning_rate": 4.881847616063502e-05,
      "loss": 2.0558,
      "step": 18100
    },
    {
      "epoch": 1.4256619144602851,
      "grad_norm": 4.7220282554626465,
      "learning_rate": 4.881194840461643e-05,
      "loss": 2.1226,
      "step": 18200
    },
    {
      "epoch": 1.4334952216825945,
      "grad_norm": 7.1769866943359375,
      "learning_rate": 4.880542064859784e-05,
      "loss": 2.0468,
      "step": 18300
    },
    {
      "epoch": 1.4413285289049036,
      "grad_norm": 4.1683669090271,
      "learning_rate": 4.879889289257925e-05,
      "loss": 2.2046,
      "step": 18400
    },
    {
      "epoch": 1.449161836127213,
      "grad_norm": 4.47035551071167,
      "learning_rate": 4.879236513656066e-05,
      "loss": 2.092,
      "step": 18500
    },
    {
      "epoch": 1.456995143349522,
      "grad_norm": 5.748067855834961,
      "learning_rate": 4.8785837380542065e-05,
      "loss": 2.1413,
      "step": 18600
    },
    {
      "epoch": 1.4648284505718314,
      "grad_norm": 5.2364606857299805,
      "learning_rate": 4.877930962452348e-05,
      "loss": 2.0636,
      "step": 18700
    },
    {
      "epoch": 1.4726617577941408,
      "grad_norm": 5.188057899475098,
      "learning_rate": 4.877278186850488e-05,
      "loss": 2.0378,
      "step": 18800
    },
    {
      "epoch": 1.48049506501645,
      "grad_norm": 4.300225734710693,
      "learning_rate": 4.8766254112486296e-05,
      "loss": 2.2095,
      "step": 18900
    },
    {
      "epoch": 1.4883283722387592,
      "grad_norm": 4.768073558807373,
      "learning_rate": 4.875972635646771e-05,
      "loss": 2.0524,
      "step": 19000
    },
    {
      "epoch": 1.4961616794610686,
      "grad_norm": 5.824512958526611,
      "learning_rate": 4.8753198600449114e-05,
      "loss": 2.0873,
      "step": 19100
    },
    {
      "epoch": 1.5039949866833777,
      "grad_norm": 5.633890628814697,
      "learning_rate": 4.874667084443052e-05,
      "loss": 2.1749,
      "step": 19200
    },
    {
      "epoch": 1.5118282939056868,
      "grad_norm": 6.51552677154541,
      "learning_rate": 4.874014308841193e-05,
      "loss": 2.1776,
      "step": 19300
    },
    {
      "epoch": 1.5196616011279962,
      "grad_norm": 4.0044474601745605,
      "learning_rate": 4.873361533239334e-05,
      "loss": 2.069,
      "step": 19400
    },
    {
      "epoch": 1.5274949083503055,
      "grad_norm": 6.214383602142334,
      "learning_rate": 4.8727087576374744e-05,
      "loss": 2.1173,
      "step": 19500
    },
    {
      "epoch": 1.5353282155726147,
      "grad_norm": 3.920518159866333,
      "learning_rate": 4.8720559820356156e-05,
      "loss": 2.0903,
      "step": 19600
    },
    {
      "epoch": 1.543161522794924,
      "grad_norm": 5.436163425445557,
      "learning_rate": 4.871403206433757e-05,
      "loss": 2.0448,
      "step": 19700
    },
    {
      "epoch": 1.5509948300172334,
      "grad_norm": 10.286617279052734,
      "learning_rate": 4.8707504308318975e-05,
      "loss": 2.0818,
      "step": 19800
    },
    {
      "epoch": 1.5588281372395425,
      "grad_norm": 6.115528106689453,
      "learning_rate": 4.870097655230038e-05,
      "loss": 2.063,
      "step": 19900
    },
    {
      "epoch": 1.5666614444618518,
      "grad_norm": 4.746628761291504,
      "learning_rate": 4.869444879628179e-05,
      "loss": 2.0866,
      "step": 20000
    },
    {
      "epoch": 1.5744947516841612,
      "grad_norm": 3.9922258853912354,
      "learning_rate": 4.86879210402632e-05,
      "loss": 2.1833,
      "step": 20100
    },
    {
      "epoch": 1.5823280589064703,
      "grad_norm": 4.578382968902588,
      "learning_rate": 4.868139328424461e-05,
      "loss": 2.0414,
      "step": 20200
    },
    {
      "epoch": 1.5901613661287795,
      "grad_norm": 5.890382289886475,
      "learning_rate": 4.8674865528226024e-05,
      "loss": 2.1318,
      "step": 20300
    },
    {
      "epoch": 1.5979946733510888,
      "grad_norm": 3.634385108947754,
      "learning_rate": 4.866833777220743e-05,
      "loss": 2.006,
      "step": 20400
    },
    {
      "epoch": 1.6058279805733982,
      "grad_norm": 5.451206684112549,
      "learning_rate": 4.8661810016188835e-05,
      "loss": 2.0362,
      "step": 20500
    },
    {
      "epoch": 1.6136612877957073,
      "grad_norm": 5.999538421630859,
      "learning_rate": 4.865528226017025e-05,
      "loss": 2.0817,
      "step": 20600
    },
    {
      "epoch": 1.6214945950180166,
      "grad_norm": 5.559510231018066,
      "learning_rate": 4.8648754504151654e-05,
      "loss": 2.0125,
      "step": 20700
    },
    {
      "epoch": 1.629327902240326,
      "grad_norm": 4.709551811218262,
      "learning_rate": 4.8642226748133066e-05,
      "loss": 2.0719,
      "step": 20800
    },
    {
      "epoch": 1.637161209462635,
      "grad_norm": 6.041172504425049,
      "learning_rate": 4.863569899211448e-05,
      "loss": 2.0954,
      "step": 20900
    },
    {
      "epoch": 1.6449945166849442,
      "grad_norm": 7.327115535736084,
      "learning_rate": 4.8629171236095885e-05,
      "loss": 2.1016,
      "step": 21000
    },
    {
      "epoch": 1.6528278239072538,
      "grad_norm": 7.45778226852417,
      "learning_rate": 4.862264348007729e-05,
      "loss": 2.0561,
      "step": 21100
    },
    {
      "epoch": 1.660661131129563,
      "grad_norm": 4.678922176361084,
      "learning_rate": 4.8616115724058696e-05,
      "loss": 2.1001,
      "step": 21200
    },
    {
      "epoch": 1.668494438351872,
      "grad_norm": 4.452406406402588,
      "learning_rate": 4.860958796804011e-05,
      "loss": 2.1256,
      "step": 21300
    },
    {
      "epoch": 1.6763277455741814,
      "grad_norm": 5.279855728149414,
      "learning_rate": 4.8603060212021514e-05,
      "loss": 2.1818,
      "step": 21400
    },
    {
      "epoch": 1.6841610527964908,
      "grad_norm": 4.0176215171813965,
      "learning_rate": 4.859653245600293e-05,
      "loss": 2.0457,
      "step": 21500
    },
    {
      "epoch": 1.6919943600187999,
      "grad_norm": 4.816084861755371,
      "learning_rate": 4.859000469998434e-05,
      "loss": 2.1614,
      "step": 21600
    },
    {
      "epoch": 1.6998276672411092,
      "grad_norm": 3.8773934841156006,
      "learning_rate": 4.8583476943965745e-05,
      "loss": 2.0626,
      "step": 21700
    },
    {
      "epoch": 1.7076609744634186,
      "grad_norm": 5.974483489990234,
      "learning_rate": 4.857694918794715e-05,
      "loss": 2.0787,
      "step": 21800
    },
    {
      "epoch": 1.7154942816857277,
      "grad_norm": 5.126582145690918,
      "learning_rate": 4.8570421431928564e-05,
      "loss": 2.0319,
      "step": 21900
    },
    {
      "epoch": 1.7233275889080368,
      "grad_norm": 5.642197608947754,
      "learning_rate": 4.856389367590997e-05,
      "loss": 2.1135,
      "step": 22000
    },
    {
      "epoch": 1.7311608961303462,
      "grad_norm": 3.667330741882324,
      "learning_rate": 4.855736591989138e-05,
      "loss": 2.1022,
      "step": 22100
    },
    {
      "epoch": 1.7389942033526555,
      "grad_norm": 6.468472480773926,
      "learning_rate": 4.8550838163872794e-05,
      "loss": 2.1112,
      "step": 22200
    },
    {
      "epoch": 1.7468275105749647,
      "grad_norm": 7.7331037521362305,
      "learning_rate": 4.85443104078542e-05,
      "loss": 2.135,
      "step": 22300
    },
    {
      "epoch": 1.754660817797274,
      "grad_norm": 5.047895908355713,
      "learning_rate": 4.8537782651835606e-05,
      "loss": 2.0381,
      "step": 22400
    },
    {
      "epoch": 1.7624941250195834,
      "grad_norm": 6.224412441253662,
      "learning_rate": 4.853125489581702e-05,
      "loss": 2.1075,
      "step": 22500
    },
    {
      "epoch": 1.7703274322418925,
      "grad_norm": 5.196223258972168,
      "learning_rate": 4.8524727139798424e-05,
      "loss": 2.1162,
      "step": 22600
    },
    {
      "epoch": 1.7781607394642018,
      "grad_norm": 3.587786912918091,
      "learning_rate": 4.851819938377983e-05,
      "loss": 2.0819,
      "step": 22700
    },
    {
      "epoch": 1.7859940466865112,
      "grad_norm": 3.8359479904174805,
      "learning_rate": 4.851167162776124e-05,
      "loss": 1.9985,
      "step": 22800
    },
    {
      "epoch": 1.7938273539088203,
      "grad_norm": 4.5959601402282715,
      "learning_rate": 4.8505143871742655e-05,
      "loss": 2.113,
      "step": 22900
    },
    {
      "epoch": 1.8016606611311294,
      "grad_norm": 4.782068252563477,
      "learning_rate": 4.849861611572406e-05,
      "loss": 2.1971,
      "step": 23000
    },
    {
      "epoch": 1.8094939683534388,
      "grad_norm": 5.934356212615967,
      "learning_rate": 4.849208835970547e-05,
      "loss": 2.0287,
      "step": 23100
    },
    {
      "epoch": 1.8173272755757481,
      "grad_norm": 5.647505283355713,
      "learning_rate": 4.848556060368688e-05,
      "loss": 2.0189,
      "step": 23200
    },
    {
      "epoch": 1.8251605827980573,
      "grad_norm": 4.303274631500244,
      "learning_rate": 4.8479032847668285e-05,
      "loss": 2.103,
      "step": 23300
    },
    {
      "epoch": 1.8329938900203666,
      "grad_norm": 6.072854042053223,
      "learning_rate": 4.84725050916497e-05,
      "loss": 2.0455,
      "step": 23400
    },
    {
      "epoch": 1.840827197242676,
      "grad_norm": 5.149067401885986,
      "learning_rate": 4.846597733563111e-05,
      "loss": 2.117,
      "step": 23500
    },
    {
      "epoch": 1.848660504464985,
      "grad_norm": 4.510815143585205,
      "learning_rate": 4.8459449579612516e-05,
      "loss": 2.1137,
      "step": 23600
    },
    {
      "epoch": 1.8564938116872942,
      "grad_norm": 5.205973148345947,
      "learning_rate": 4.845292182359392e-05,
      "loss": 2.0724,
      "step": 23700
    },
    {
      "epoch": 1.8643271189096038,
      "grad_norm": 5.6367363929748535,
      "learning_rate": 4.8446394067575334e-05,
      "loss": 2.0644,
      "step": 23800
    },
    {
      "epoch": 1.872160426131913,
      "grad_norm": 5.174858093261719,
      "learning_rate": 4.843986631155674e-05,
      "loss": 2.0895,
      "step": 23900
    },
    {
      "epoch": 1.879993733354222,
      "grad_norm": 6.238369464874268,
      "learning_rate": 4.843333855553815e-05,
      "loss": 2.0931,
      "step": 24000
    },
    {
      "epoch": 1.8878270405765314,
      "grad_norm": 5.053220748901367,
      "learning_rate": 4.8426810799519565e-05,
      "loss": 1.9801,
      "step": 24100
    },
    {
      "epoch": 1.8956603477988407,
      "grad_norm": 4.525503635406494,
      "learning_rate": 4.842028304350097e-05,
      "loss": 2.0371,
      "step": 24200
    },
    {
      "epoch": 1.9034936550211499,
      "grad_norm": 4.4244866371154785,
      "learning_rate": 4.8413755287482377e-05,
      "loss": 2.0858,
      "step": 24300
    },
    {
      "epoch": 1.9113269622434592,
      "grad_norm": 4.525151252746582,
      "learning_rate": 4.840722753146379e-05,
      "loss": 1.9738,
      "step": 24400
    },
    {
      "epoch": 1.9191602694657686,
      "grad_norm": 4.490096569061279,
      "learning_rate": 4.8400699775445195e-05,
      "loss": 2.0558,
      "step": 24500
    },
    {
      "epoch": 1.9269935766880777,
      "grad_norm": 4.1084747314453125,
      "learning_rate": 4.83941720194266e-05,
      "loss": 2.0753,
      "step": 24600
    },
    {
      "epoch": 1.9348268839103868,
      "grad_norm": 5.050004482269287,
      "learning_rate": 4.838764426340801e-05,
      "loss": 2.0647,
      "step": 24700
    },
    {
      "epoch": 1.9426601911326964,
      "grad_norm": 5.98903226852417,
      "learning_rate": 4.8381116507389426e-05,
      "loss": 2.2006,
      "step": 24800
    },
    {
      "epoch": 1.9504934983550055,
      "grad_norm": 5.696963787078857,
      "learning_rate": 4.837458875137083e-05,
      "loss": 2.0155,
      "step": 24900
    },
    {
      "epoch": 1.9583268055773146,
      "grad_norm": 4.867893218994141,
      "learning_rate": 4.836806099535224e-05,
      "loss": 2.0841,
      "step": 25000
    },
    {
      "epoch": 1.966160112799624,
      "grad_norm": 4.5824713706970215,
      "learning_rate": 4.836153323933365e-05,
      "loss": 2.0984,
      "step": 25100
    },
    {
      "epoch": 1.9739934200219333,
      "grad_norm": 7.60143518447876,
      "learning_rate": 4.8355005483315056e-05,
      "loss": 1.9979,
      "step": 25200
    },
    {
      "epoch": 1.9818267272442425,
      "grad_norm": 5.19178581237793,
      "learning_rate": 4.834847772729647e-05,
      "loss": 2.086,
      "step": 25300
    },
    {
      "epoch": 1.9896600344665518,
      "grad_norm": 4.1170172691345215,
      "learning_rate": 4.834194997127788e-05,
      "loss": 1.9972,
      "step": 25400
    },
    {
      "epoch": 1.9974933416888612,
      "grad_norm": 5.304464340209961,
      "learning_rate": 4.8335422215259286e-05,
      "loss": 2.0785,
      "step": 25500
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.9926196336746216,
      "eval_runtime": 3.0986,
      "eval_samples_per_second": 216.871,
      "eval_steps_per_second": 216.871,
      "step": 25532
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.8570411205291748,
      "eval_runtime": 56.9504,
      "eval_samples_per_second": 224.16,
      "eval_steps_per_second": 224.16,
      "step": 25532
    },
    {
      "epoch": 2.0053266489111703,
      "grad_norm": 4.647586345672607,
      "learning_rate": 4.832889445924069e-05,
      "loss": 1.9907,
      "step": 25600
    },
    {
      "epoch": 2.0131599561334794,
      "grad_norm": 4.918145656585693,
      "learning_rate": 4.8322366703222105e-05,
      "loss": 2.0435,
      "step": 25700
    },
    {
      "epoch": 2.020993263355789,
      "grad_norm": 5.131568431854248,
      "learning_rate": 4.831583894720351e-05,
      "loss": 2.0264,
      "step": 25800
    },
    {
      "epoch": 2.028826570578098,
      "grad_norm": 6.219016075134277,
      "learning_rate": 4.8309311191184916e-05,
      "loss": 2.011,
      "step": 25900
    },
    {
      "epoch": 2.0366598778004072,
      "grad_norm": 4.211695671081543,
      "learning_rate": 4.830278343516633e-05,
      "loss": 1.9835,
      "step": 26000
    },
    {
      "epoch": 2.0444931850227164,
      "grad_norm": 8.442184448242188,
      "learning_rate": 4.829625567914774e-05,
      "loss": 2.1073,
      "step": 26100
    },
    {
      "epoch": 2.052326492245026,
      "grad_norm": 5.090169429779053,
      "learning_rate": 4.828972792312915e-05,
      "loss": 1.9926,
      "step": 26200
    },
    {
      "epoch": 2.060159799467335,
      "grad_norm": 4.051022529602051,
      "learning_rate": 4.828320016711055e-05,
      "loss": 1.9418,
      "step": 26300
    },
    {
      "epoch": 2.067993106689644,
      "grad_norm": 5.895142555236816,
      "learning_rate": 4.8276672411091966e-05,
      "loss": 1.9812,
      "step": 26400
    },
    {
      "epoch": 2.0758264139119538,
      "grad_norm": 4.491461753845215,
      "learning_rate": 4.827014465507337e-05,
      "loss": 2.0044,
      "step": 26500
    },
    {
      "epoch": 2.083659721134263,
      "grad_norm": 5.628273010253906,
      "learning_rate": 4.8263616899054784e-05,
      "loss": 2.0867,
      "step": 26600
    },
    {
      "epoch": 2.091493028356572,
      "grad_norm": 4.808006286621094,
      "learning_rate": 4.8257089143036196e-05,
      "loss": 2.0164,
      "step": 26700
    },
    {
      "epoch": 2.0993263355788816,
      "grad_norm": 4.4526286125183105,
      "learning_rate": 4.82505613870176e-05,
      "loss": 2.1019,
      "step": 26800
    },
    {
      "epoch": 2.1071596428011907,
      "grad_norm": 5.413354396820068,
      "learning_rate": 4.824403363099901e-05,
      "loss": 1.9749,
      "step": 26900
    },
    {
      "epoch": 2.1149929500235,
      "grad_norm": 5.673831462860107,
      "learning_rate": 4.823750587498042e-05,
      "loss": 1.9838,
      "step": 27000
    },
    {
      "epoch": 2.122826257245809,
      "grad_norm": 4.428493022918701,
      "learning_rate": 4.8230978118961826e-05,
      "loss": 2.0,
      "step": 27100
    },
    {
      "epoch": 2.1306595644681185,
      "grad_norm": 4.539832592010498,
      "learning_rate": 4.822445036294324e-05,
      "loss": 2.0592,
      "step": 27200
    },
    {
      "epoch": 2.1384928716904277,
      "grad_norm": 4.693939208984375,
      "learning_rate": 4.821792260692465e-05,
      "loss": 2.0501,
      "step": 27300
    },
    {
      "epoch": 2.146326178912737,
      "grad_norm": 4.423044204711914,
      "learning_rate": 4.821139485090606e-05,
      "loss": 2.0658,
      "step": 27400
    },
    {
      "epoch": 2.1541594861350464,
      "grad_norm": 4.509613037109375,
      "learning_rate": 4.820486709488746e-05,
      "loss": 2.0188,
      "step": 27500
    },
    {
      "epoch": 2.1619927933573555,
      "grad_norm": 5.899482727050781,
      "learning_rate": 4.8198339338868875e-05,
      "loss": 1.9908,
      "step": 27600
    },
    {
      "epoch": 2.1698261005796646,
      "grad_norm": 4.978906631469727,
      "learning_rate": 4.819181158285028e-05,
      "loss": 2.0053,
      "step": 27700
    },
    {
      "epoch": 2.177659407801974,
      "grad_norm": 5.497633457183838,
      "learning_rate": 4.818528382683169e-05,
      "loss": 2.0575,
      "step": 27800
    },
    {
      "epoch": 2.1854927150242833,
      "grad_norm": 5.5597710609436035,
      "learning_rate": 4.81787560708131e-05,
      "loss": 2.0517,
      "step": 27900
    },
    {
      "epoch": 2.1933260222465925,
      "grad_norm": 7.378326416015625,
      "learning_rate": 4.817222831479451e-05,
      "loss": 2.0427,
      "step": 28000
    },
    {
      "epoch": 2.2011593294689016,
      "grad_norm": 4.1333723068237305,
      "learning_rate": 4.816570055877592e-05,
      "loss": 2.2187,
      "step": 28100
    },
    {
      "epoch": 2.208992636691211,
      "grad_norm": 4.790182113647461,
      "learning_rate": 4.8159172802757324e-05,
      "loss": 1.9944,
      "step": 28200
    },
    {
      "epoch": 2.2168259439135203,
      "grad_norm": 4.856724262237549,
      "learning_rate": 4.8152645046738736e-05,
      "loss": 2.0849,
      "step": 28300
    },
    {
      "epoch": 2.2246592511358294,
      "grad_norm": 5.2033538818359375,
      "learning_rate": 4.814611729072014e-05,
      "loss": 1.9844,
      "step": 28400
    },
    {
      "epoch": 2.232492558358139,
      "grad_norm": 4.204851150512695,
      "learning_rate": 4.8139589534701554e-05,
      "loss": 1.9913,
      "step": 28500
    },
    {
      "epoch": 2.240325865580448,
      "grad_norm": 5.331219673156738,
      "learning_rate": 4.813306177868297e-05,
      "loss": 2.1017,
      "step": 28600
    },
    {
      "epoch": 2.2481591728027572,
      "grad_norm": 5.712644577026367,
      "learning_rate": 4.812653402266437e-05,
      "loss": 2.0101,
      "step": 28700
    },
    {
      "epoch": 2.255992480025067,
      "grad_norm": 6.39498233795166,
      "learning_rate": 4.812000626664578e-05,
      "loss": 2.0544,
      "step": 28800
    },
    {
      "epoch": 2.263825787247376,
      "grad_norm": 5.979741096496582,
      "learning_rate": 4.811347851062719e-05,
      "loss": 2.0442,
      "step": 28900
    },
    {
      "epoch": 2.271659094469685,
      "grad_norm": 4.897599697113037,
      "learning_rate": 4.81069507546086e-05,
      "loss": 2.0231,
      "step": 29000
    },
    {
      "epoch": 2.279492401691994,
      "grad_norm": 5.766157627105713,
      "learning_rate": 4.810042299859e-05,
      "loss": 2.0718,
      "step": 29100
    },
    {
      "epoch": 2.2873257089143038,
      "grad_norm": 4.085831642150879,
      "learning_rate": 4.8093895242571415e-05,
      "loss": 2.051,
      "step": 29200
    },
    {
      "epoch": 2.295159016136613,
      "grad_norm": 5.157279968261719,
      "learning_rate": 4.808736748655283e-05,
      "loss": 1.9919,
      "step": 29300
    },
    {
      "epoch": 2.302992323358922,
      "grad_norm": 3.5968217849731445,
      "learning_rate": 4.8080839730534233e-05,
      "loss": 2.002,
      "step": 29400
    },
    {
      "epoch": 2.3108256305812316,
      "grad_norm": 6.232082366943359,
      "learning_rate": 4.807431197451564e-05,
      "loss": 1.9398,
      "step": 29500
    },
    {
      "epoch": 2.3186589378035407,
      "grad_norm": 4.789041042327881,
      "learning_rate": 4.806778421849705e-05,
      "loss": 2.0289,
      "step": 29600
    },
    {
      "epoch": 2.32649224502585,
      "grad_norm": 5.949288845062256,
      "learning_rate": 4.806125646247846e-05,
      "loss": 2.0367,
      "step": 29700
    },
    {
      "epoch": 2.334325552248159,
      "grad_norm": 5.601947784423828,
      "learning_rate": 4.805472870645987e-05,
      "loss": 2.0666,
      "step": 29800
    },
    {
      "epoch": 2.3421588594704685,
      "grad_norm": 5.9471330642700195,
      "learning_rate": 4.804820095044128e-05,
      "loss": 2.0434,
      "step": 29900
    },
    {
      "epoch": 2.3499921666927777,
      "grad_norm": 6.798578262329102,
      "learning_rate": 4.804167319442269e-05,
      "loss": 2.0212,
      "step": 30000
    },
    {
      "epoch": 2.357825473915087,
      "grad_norm": 6.152658462524414,
      "learning_rate": 4.8035145438404094e-05,
      "loss": 2.1209,
      "step": 30100
    },
    {
      "epoch": 2.3656587811373964,
      "grad_norm": 5.509180068969727,
      "learning_rate": 4.802861768238551e-05,
      "loss": 2.0901,
      "step": 30200
    },
    {
      "epoch": 2.3734920883597055,
      "grad_norm": 4.957502365112305,
      "learning_rate": 4.802208992636691e-05,
      "loss": 2.0739,
      "step": 30300
    },
    {
      "epoch": 2.3813253955820146,
      "grad_norm": 4.160114765167236,
      "learning_rate": 4.8015562170348325e-05,
      "loss": 2.0093,
      "step": 30400
    },
    {
      "epoch": 2.389158702804324,
      "grad_norm": 3.3765249252319336,
      "learning_rate": 4.800903441432974e-05,
      "loss": 2.0956,
      "step": 30500
    },
    {
      "epoch": 2.3969920100266333,
      "grad_norm": 5.6804327964782715,
      "learning_rate": 4.800250665831114e-05,
      "loss": 2.1074,
      "step": 30600
    },
    {
      "epoch": 2.4048253172489424,
      "grad_norm": 4.528505802154541,
      "learning_rate": 4.799597890229255e-05,
      "loss": 2.087,
      "step": 30700
    },
    {
      "epoch": 2.412658624471252,
      "grad_norm": 6.331862449645996,
      "learning_rate": 4.798945114627396e-05,
      "loss": 1.9673,
      "step": 30800
    },
    {
      "epoch": 2.420491931693561,
      "grad_norm": 6.034541606903076,
      "learning_rate": 4.798292339025537e-05,
      "loss": 2.0849,
      "step": 30900
    },
    {
      "epoch": 2.4283252389158703,
      "grad_norm": 5.536960601806641,
      "learning_rate": 4.797639563423677e-05,
      "loss": 2.075,
      "step": 31000
    },
    {
      "epoch": 2.4361585461381794,
      "grad_norm": 6.1350555419921875,
      "learning_rate": 4.7969867878218186e-05,
      "loss": 2.0537,
      "step": 31100
    },
    {
      "epoch": 2.443991853360489,
      "grad_norm": 7.098172664642334,
      "learning_rate": 4.79633401221996e-05,
      "loss": 1.9613,
      "step": 31200
    },
    {
      "epoch": 2.451825160582798,
      "grad_norm": 5.754804611206055,
      "learning_rate": 4.7956812366181004e-05,
      "loss": 2.0291,
      "step": 31300
    },
    {
      "epoch": 2.459658467805107,
      "grad_norm": 4.887819766998291,
      "learning_rate": 4.795028461016241e-05,
      "loss": 1.9919,
      "step": 31400
    },
    {
      "epoch": 2.4674917750274163,
      "grad_norm": 5.117165565490723,
      "learning_rate": 4.794375685414382e-05,
      "loss": 1.9546,
      "step": 31500
    },
    {
      "epoch": 2.475325082249726,
      "grad_norm": 5.01502799987793,
      "learning_rate": 4.793722909812523e-05,
      "loss": 1.975,
      "step": 31600
    },
    {
      "epoch": 2.483158389472035,
      "grad_norm": 4.936805725097656,
      "learning_rate": 4.793070134210664e-05,
      "loss": 2.0295,
      "step": 31700
    },
    {
      "epoch": 2.490991696694344,
      "grad_norm": 5.335522174835205,
      "learning_rate": 4.792417358608805e-05,
      "loss": 2.0158,
      "step": 31800
    },
    {
      "epoch": 2.4988250039166537,
      "grad_norm": 5.413890361785889,
      "learning_rate": 4.791764583006946e-05,
      "loss": 2.0313,
      "step": 31900
    },
    {
      "epoch": 2.506658311138963,
      "grad_norm": 3.931697368621826,
      "learning_rate": 4.7911118074050865e-05,
      "loss": 2.0274,
      "step": 32000
    },
    {
      "epoch": 2.514491618361272,
      "grad_norm": 4.442944526672363,
      "learning_rate": 4.790459031803228e-05,
      "loss": 1.9546,
      "step": 32100
    },
    {
      "epoch": 2.5223249255835816,
      "grad_norm": 6.700174331665039,
      "learning_rate": 4.789806256201368e-05,
      "loss": 1.9775,
      "step": 32200
    },
    {
      "epoch": 2.5301582328058907,
      "grad_norm": 5.069040298461914,
      "learning_rate": 4.789153480599509e-05,
      "loss": 2.0949,
      "step": 32300
    },
    {
      "epoch": 2.5379915400282,
      "grad_norm": 5.111856460571289,
      "learning_rate": 4.78850070499765e-05,
      "loss": 2.0763,
      "step": 32400
    },
    {
      "epoch": 2.5458248472505094,
      "grad_norm": 5.5258469581604,
      "learning_rate": 4.7878479293957914e-05,
      "loss": 2.1291,
      "step": 32500
    },
    {
      "epoch": 2.5536581544728185,
      "grad_norm": 4.8423686027526855,
      "learning_rate": 4.787195153793932e-05,
      "loss": 1.8896,
      "step": 32600
    },
    {
      "epoch": 2.5614914616951276,
      "grad_norm": 3.407818555831909,
      "learning_rate": 4.786542378192073e-05,
      "loss": 1.8983,
      "step": 32700
    },
    {
      "epoch": 2.569324768917437,
      "grad_norm": 4.490142822265625,
      "learning_rate": 4.785889602590214e-05,
      "loss": 2.0167,
      "step": 32800
    },
    {
      "epoch": 2.5771580761397463,
      "grad_norm": 4.310635566711426,
      "learning_rate": 4.7852368269883544e-05,
      "loss": 2.1107,
      "step": 32900
    },
    {
      "epoch": 2.5849913833620555,
      "grad_norm": 4.995942115783691,
      "learning_rate": 4.7845840513864956e-05,
      "loss": 2.0942,
      "step": 33000
    },
    {
      "epoch": 2.5928246905843646,
      "grad_norm": 6.022937774658203,
      "learning_rate": 4.783931275784637e-05,
      "loss": 2.0169,
      "step": 33100
    },
    {
      "epoch": 2.6006579978066737,
      "grad_norm": 5.1805949211120605,
      "learning_rate": 4.7832785001827775e-05,
      "loss": 2.029,
      "step": 33200
    },
    {
      "epoch": 2.6084913050289833,
      "grad_norm": 6.150389194488525,
      "learning_rate": 4.782625724580918e-05,
      "loss": 2.1012,
      "step": 33300
    },
    {
      "epoch": 2.6163246122512924,
      "grad_norm": 4.912558078765869,
      "learning_rate": 4.781972948979059e-05,
      "loss": 1.98,
      "step": 33400
    },
    {
      "epoch": 2.6241579194736016,
      "grad_norm": 8.236820220947266,
      "learning_rate": 4.7813201733772e-05,
      "loss": 2.1108,
      "step": 33500
    },
    {
      "epoch": 2.631991226695911,
      "grad_norm": 4.543659210205078,
      "learning_rate": 4.780667397775341e-05,
      "loss": 2.0407,
      "step": 33600
    },
    {
      "epoch": 2.6398245339182203,
      "grad_norm": 5.348236083984375,
      "learning_rate": 4.7800146221734824e-05,
      "loss": 1.9951,
      "step": 33700
    },
    {
      "epoch": 2.6476578411405294,
      "grad_norm": 4.854011535644531,
      "learning_rate": 4.779361846571623e-05,
      "loss": 2.1174,
      "step": 33800
    },
    {
      "epoch": 2.655491148362839,
      "grad_norm": 6.149148941040039,
      "learning_rate": 4.7787090709697635e-05,
      "loss": 2.0303,
      "step": 33900
    },
    {
      "epoch": 2.663324455585148,
      "grad_norm": 5.227893352508545,
      "learning_rate": 4.778056295367905e-05,
      "loss": 2.0686,
      "step": 34000
    },
    {
      "epoch": 2.671157762807457,
      "grad_norm": 4.293994903564453,
      "learning_rate": 4.7774035197660454e-05,
      "loss": 1.9837,
      "step": 34100
    },
    {
      "epoch": 2.6789910700297668,
      "grad_norm": 5.301433086395264,
      "learning_rate": 4.776750744164186e-05,
      "loss": 1.944,
      "step": 34200
    },
    {
      "epoch": 2.686824377252076,
      "grad_norm": 5.991118431091309,
      "learning_rate": 4.776097968562327e-05,
      "loss": 2.0219,
      "step": 34300
    },
    {
      "epoch": 2.694657684474385,
      "grad_norm": 4.294913291931152,
      "learning_rate": 4.7754451929604684e-05,
      "loss": 2.023,
      "step": 34400
    },
    {
      "epoch": 2.7024909916966946,
      "grad_norm": 4.132788181304932,
      "learning_rate": 4.774792417358609e-05,
      "loss": 1.9586,
      "step": 34500
    },
    {
      "epoch": 2.7103242989190037,
      "grad_norm": 5.738503456115723,
      "learning_rate": 4.7741396417567496e-05,
      "loss": 2.0867,
      "step": 34600
    },
    {
      "epoch": 2.718157606141313,
      "grad_norm": 4.3815531730651855,
      "learning_rate": 4.773486866154891e-05,
      "loss": 2.1201,
      "step": 34700
    },
    {
      "epoch": 2.725990913363622,
      "grad_norm": 3.993252754211426,
      "learning_rate": 4.7728340905530314e-05,
      "loss": 2.0165,
      "step": 34800
    },
    {
      "epoch": 2.733824220585931,
      "grad_norm": 4.293881893157959,
      "learning_rate": 4.772181314951173e-05,
      "loss": 2.0249,
      "step": 34900
    },
    {
      "epoch": 2.7416575278082407,
      "grad_norm": 6.0127458572387695,
      "learning_rate": 4.771528539349314e-05,
      "loss": 2.0362,
      "step": 35000
    },
    {
      "epoch": 2.74949083503055,
      "grad_norm": 5.952290058135986,
      "learning_rate": 4.7708757637474545e-05,
      "loss": 1.9284,
      "step": 35100
    },
    {
      "epoch": 2.757324142252859,
      "grad_norm": 5.145452499389648,
      "learning_rate": 4.770222988145595e-05,
      "loss": 1.9584,
      "step": 35200
    },
    {
      "epoch": 2.7651574494751685,
      "grad_norm": 6.231517791748047,
      "learning_rate": 4.7695702125437363e-05,
      "loss": 2.007,
      "step": 35300
    },
    {
      "epoch": 2.7729907566974776,
      "grad_norm": 3.9751076698303223,
      "learning_rate": 4.768917436941877e-05,
      "loss": 2.0255,
      "step": 35400
    },
    {
      "epoch": 2.7808240639197868,
      "grad_norm": 4.392862319946289,
      "learning_rate": 4.7682646613400175e-05,
      "loss": 1.9803,
      "step": 35500
    },
    {
      "epoch": 2.7886573711420963,
      "grad_norm": 4.6267313957214355,
      "learning_rate": 4.767611885738159e-05,
      "loss": 1.9617,
      "step": 35600
    },
    {
      "epoch": 2.7964906783644055,
      "grad_norm": 5.865799427032471,
      "learning_rate": 4.7669591101363e-05,
      "loss": 1.9689,
      "step": 35700
    },
    {
      "epoch": 2.8043239855867146,
      "grad_norm": 5.371832847595215,
      "learning_rate": 4.7663063345344406e-05,
      "loss": 1.927,
      "step": 35800
    },
    {
      "epoch": 2.812157292809024,
      "grad_norm": 5.959019660949707,
      "learning_rate": 4.765653558932582e-05,
      "loss": 2.0654,
      "step": 35900
    },
    {
      "epoch": 2.8199906000313333,
      "grad_norm": 7.48266077041626,
      "learning_rate": 4.7650007833307224e-05,
      "loss": 2.0634,
      "step": 36000
    },
    {
      "epoch": 2.8278239072536424,
      "grad_norm": 4.054315567016602,
      "learning_rate": 4.764348007728863e-05,
      "loss": 1.9793,
      "step": 36100
    },
    {
      "epoch": 2.835657214475952,
      "grad_norm": 4.394195079803467,
      "learning_rate": 4.763695232127004e-05,
      "loss": 2.0373,
      "step": 36200
    },
    {
      "epoch": 2.843490521698261,
      "grad_norm": 5.215391635894775,
      "learning_rate": 4.7630424565251455e-05,
      "loss": 2.039,
      "step": 36300
    },
    {
      "epoch": 2.8513238289205702,
      "grad_norm": 7.325978755950928,
      "learning_rate": 4.762389680923286e-05,
      "loss": 1.9066,
      "step": 36400
    },
    {
      "epoch": 2.8591571361428794,
      "grad_norm": 4.393141269683838,
      "learning_rate": 4.7617369053214267e-05,
      "loss": 2.0871,
      "step": 36500
    },
    {
      "epoch": 2.866990443365189,
      "grad_norm": 5.580824851989746,
      "learning_rate": 4.761084129719568e-05,
      "loss": 1.9558,
      "step": 36600
    },
    {
      "epoch": 2.874823750587498,
      "grad_norm": 9.187762260437012,
      "learning_rate": 4.7604313541177085e-05,
      "loss": 2.0286,
      "step": 36700
    },
    {
      "epoch": 2.882657057809807,
      "grad_norm": 4.805226802825928,
      "learning_rate": 4.75977857851585e-05,
      "loss": 1.986,
      "step": 36800
    },
    {
      "epoch": 2.8904903650321163,
      "grad_norm": 5.553100109100342,
      "learning_rate": 4.759125802913991e-05,
      "loss": 1.9454,
      "step": 36900
    },
    {
      "epoch": 2.898323672254426,
      "grad_norm": 4.190942287445068,
      "learning_rate": 4.7584730273121316e-05,
      "loss": 1.9665,
      "step": 37000
    },
    {
      "epoch": 2.906156979476735,
      "grad_norm": 4.751959323883057,
      "learning_rate": 4.757820251710272e-05,
      "loss": 2.1286,
      "step": 37100
    },
    {
      "epoch": 2.913990286699044,
      "grad_norm": 4.421791076660156,
      "learning_rate": 4.7571674761084134e-05,
      "loss": 2.0396,
      "step": 37200
    },
    {
      "epoch": 2.9218235939213537,
      "grad_norm": 4.424722671508789,
      "learning_rate": 4.756514700506554e-05,
      "loss": 2.0157,
      "step": 37300
    },
    {
      "epoch": 2.929656901143663,
      "grad_norm": 6.847751140594482,
      "learning_rate": 4.7558619249046946e-05,
      "loss": 2.0194,
      "step": 37400
    },
    {
      "epoch": 2.937490208365972,
      "grad_norm": 4.041317462921143,
      "learning_rate": 4.755209149302836e-05,
      "loss": 2.0036,
      "step": 37500
    },
    {
      "epoch": 2.9453235155882815,
      "grad_norm": 6.053011417388916,
      "learning_rate": 4.754556373700977e-05,
      "loss": 2.0117,
      "step": 37600
    },
    {
      "epoch": 2.9531568228105907,
      "grad_norm": 5.634842395782471,
      "learning_rate": 4.7539035980991176e-05,
      "loss": 2.0174,
      "step": 37700
    },
    {
      "epoch": 2.9609901300329,
      "grad_norm": 4.756145477294922,
      "learning_rate": 4.753250822497259e-05,
      "loss": 2.0362,
      "step": 37800
    },
    {
      "epoch": 2.9688234372552094,
      "grad_norm": 3.5491766929626465,
      "learning_rate": 4.7525980468953995e-05,
      "loss": 1.9977,
      "step": 37900
    },
    {
      "epoch": 2.9766567444775185,
      "grad_norm": 6.277848243713379,
      "learning_rate": 4.75194527129354e-05,
      "loss": 1.96,
      "step": 38000
    },
    {
      "epoch": 2.9844900516998276,
      "grad_norm": 6.121026992797852,
      "learning_rate": 4.751292495691681e-05,
      "loss": 2.0172,
      "step": 38100
    },
    {
      "epoch": 2.992323358922137,
      "grad_norm": 6.077446937561035,
      "learning_rate": 4.7506397200898226e-05,
      "loss": 2.0143,
      "step": 38200
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.9475297927856445,
      "eval_runtime": 2.9215,
      "eval_samples_per_second": 230.017,
      "eval_steps_per_second": 230.017,
      "step": 38298
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.7918555736541748,
      "eval_runtime": 56.1154,
      "eval_samples_per_second": 227.496,
      "eval_steps_per_second": 227.496,
      "step": 38298
    },
    {
      "epoch": 3.0001566661444463,
      "grad_norm": 4.636927604675293,
      "learning_rate": 4.749986944487963e-05,
      "loss": 1.977,
      "step": 38300
    },
    {
      "epoch": 3.0079899733667554,
      "grad_norm": 5.9925737380981445,
      "learning_rate": 4.749334168886104e-05,
      "loss": 1.962,
      "step": 38400
    },
    {
      "epoch": 3.0158232805890646,
      "grad_norm": 5.154314041137695,
      "learning_rate": 4.748681393284245e-05,
      "loss": 1.9268,
      "step": 38500
    },
    {
      "epoch": 3.023656587811374,
      "grad_norm": 6.141176223754883,
      "learning_rate": 4.7480286176823855e-05,
      "loss": 1.973,
      "step": 38600
    },
    {
      "epoch": 3.0314898950336833,
      "grad_norm": 6.1372456550598145,
      "learning_rate": 4.747375842080526e-05,
      "loss": 2.0035,
      "step": 38700
    },
    {
      "epoch": 3.0393232022559924,
      "grad_norm": 4.307765007019043,
      "learning_rate": 4.7467230664786674e-05,
      "loss": 2.05,
      "step": 38800
    },
    {
      "epoch": 3.0471565094783015,
      "grad_norm": 6.1045002937316895,
      "learning_rate": 4.7460702908768086e-05,
      "loss": 2.0058,
      "step": 38900
    },
    {
      "epoch": 3.054989816700611,
      "grad_norm": 4.864141464233398,
      "learning_rate": 4.745417515274949e-05,
      "loss": 1.9875,
      "step": 39000
    },
    {
      "epoch": 3.06282312392292,
      "grad_norm": 5.343522548675537,
      "learning_rate": 4.7447647396730905e-05,
      "loss": 1.9602,
      "step": 39100
    },
    {
      "epoch": 3.0706564311452293,
      "grad_norm": 7.915684700012207,
      "learning_rate": 4.744111964071231e-05,
      "loss": 1.9677,
      "step": 39200
    },
    {
      "epoch": 3.078489738367539,
      "grad_norm": 6.613976955413818,
      "learning_rate": 4.7434591884693716e-05,
      "loss": 1.9345,
      "step": 39300
    },
    {
      "epoch": 3.086323045589848,
      "grad_norm": 5.661594867706299,
      "learning_rate": 4.742806412867513e-05,
      "loss": 2.0448,
      "step": 39400
    },
    {
      "epoch": 3.094156352812157,
      "grad_norm": 4.928496360778809,
      "learning_rate": 4.742153637265654e-05,
      "loss": 2.082,
      "step": 39500
    },
    {
      "epoch": 3.1019896600344667,
      "grad_norm": 4.719184398651123,
      "learning_rate": 4.741500861663795e-05,
      "loss": 2.0936,
      "step": 39600
    },
    {
      "epoch": 3.109822967256776,
      "grad_norm": 6.52072286605835,
      "learning_rate": 4.740848086061935e-05,
      "loss": 2.018,
      "step": 39700
    },
    {
      "epoch": 3.117656274479085,
      "grad_norm": 4.251194477081299,
      "learning_rate": 4.7401953104600765e-05,
      "loss": 1.9784,
      "step": 39800
    },
    {
      "epoch": 3.125489581701394,
      "grad_norm": 4.509903430938721,
      "learning_rate": 4.739542534858217e-05,
      "loss": 1.9693,
      "step": 39900
    },
    {
      "epoch": 3.1333228889237037,
      "grad_norm": 4.902053356170654,
      "learning_rate": 4.7388897592563584e-05,
      "loss": 1.963,
      "step": 40000
    },
    {
      "epoch": 3.141156196146013,
      "grad_norm": 5.82466459274292,
      "learning_rate": 4.7382369836544996e-05,
      "loss": 1.9775,
      "step": 40100
    },
    {
      "epoch": 3.148989503368322,
      "grad_norm": 4.110559463500977,
      "learning_rate": 4.73758420805264e-05,
      "loss": 2.037,
      "step": 40200
    },
    {
      "epoch": 3.1568228105906315,
      "grad_norm": 5.265739440917969,
      "learning_rate": 4.736931432450781e-05,
      "loss": 1.9122,
      "step": 40300
    },
    {
      "epoch": 3.1646561178129406,
      "grad_norm": 3.624466896057129,
      "learning_rate": 4.736278656848922e-05,
      "loss": 1.9449,
      "step": 40400
    },
    {
      "epoch": 3.1724894250352498,
      "grad_norm": 4.001105785369873,
      "learning_rate": 4.7356258812470626e-05,
      "loss": 1.911,
      "step": 40500
    },
    {
      "epoch": 3.1803227322575593,
      "grad_norm": 4.9102783203125,
      "learning_rate": 4.734973105645203e-05,
      "loss": 2.0037,
      "step": 40600
    },
    {
      "epoch": 3.1881560394798685,
      "grad_norm": 5.845912456512451,
      "learning_rate": 4.7343203300433444e-05,
      "loss": 1.9677,
      "step": 40700
    },
    {
      "epoch": 3.1959893467021776,
      "grad_norm": 4.6486496925354,
      "learning_rate": 4.733667554441486e-05,
      "loss": 2.0247,
      "step": 40800
    },
    {
      "epoch": 3.2038226539244867,
      "grad_norm": 7.366896629333496,
      "learning_rate": 4.733014778839626e-05,
      "loss": 1.9894,
      "step": 40900
    },
    {
      "epoch": 3.2116559611467963,
      "grad_norm": 7.3248162269592285,
      "learning_rate": 4.7323620032377675e-05,
      "loss": 1.9522,
      "step": 41000
    },
    {
      "epoch": 3.2194892683691054,
      "grad_norm": 6.005098342895508,
      "learning_rate": 4.731709227635908e-05,
      "loss": 2.0402,
      "step": 41100
    },
    {
      "epoch": 3.2273225755914146,
      "grad_norm": 4.742285251617432,
      "learning_rate": 4.731056452034049e-05,
      "loss": 2.0371,
      "step": 41200
    },
    {
      "epoch": 3.235155882813724,
      "grad_norm": 4.019186496734619,
      "learning_rate": 4.73040367643219e-05,
      "loss": 1.9666,
      "step": 41300
    },
    {
      "epoch": 3.2429891900360333,
      "grad_norm": 3.997689962387085,
      "learning_rate": 4.729750900830331e-05,
      "loss": 1.8907,
      "step": 41400
    },
    {
      "epoch": 3.2508224972583424,
      "grad_norm": 4.601377487182617,
      "learning_rate": 4.729098125228472e-05,
      "loss": 2.0752,
      "step": 41500
    },
    {
      "epoch": 3.258655804480652,
      "grad_norm": 4.831997871398926,
      "learning_rate": 4.7284453496266123e-05,
      "loss": 2.012,
      "step": 41600
    },
    {
      "epoch": 3.266489111702961,
      "grad_norm": 5.057933807373047,
      "learning_rate": 4.7277925740247536e-05,
      "loss": 1.9192,
      "step": 41700
    },
    {
      "epoch": 3.27432241892527,
      "grad_norm": 5.203285217285156,
      "learning_rate": 4.727139798422894e-05,
      "loss": 2.0032,
      "step": 41800
    },
    {
      "epoch": 3.2821557261475793,
      "grad_norm": 6.520449161529541,
      "learning_rate": 4.726487022821035e-05,
      "loss": 1.8405,
      "step": 41900
    },
    {
      "epoch": 3.289989033369889,
      "grad_norm": 5.729855060577393,
      "learning_rate": 4.725834247219176e-05,
      "loss": 1.9935,
      "step": 42000
    },
    {
      "epoch": 3.297822340592198,
      "grad_norm": 7.090335369110107,
      "learning_rate": 4.725181471617317e-05,
      "loss": 1.9801,
      "step": 42100
    },
    {
      "epoch": 3.305655647814507,
      "grad_norm": 5.082674503326416,
      "learning_rate": 4.724528696015458e-05,
      "loss": 1.9828,
      "step": 42200
    },
    {
      "epoch": 3.3134889550368167,
      "grad_norm": 5.225092887878418,
      "learning_rate": 4.723875920413599e-05,
      "loss": 1.9576,
      "step": 42300
    },
    {
      "epoch": 3.321322262259126,
      "grad_norm": 5.09206485748291,
      "learning_rate": 4.72322314481174e-05,
      "loss": 1.9702,
      "step": 42400
    },
    {
      "epoch": 3.329155569481435,
      "grad_norm": 6.480985641479492,
      "learning_rate": 4.72257036920988e-05,
      "loss": 2.0823,
      "step": 42500
    },
    {
      "epoch": 3.336988876703744,
      "grad_norm": 4.236398696899414,
      "learning_rate": 4.7219175936080215e-05,
      "loss": 2.0021,
      "step": 42600
    },
    {
      "epoch": 3.3448221839260537,
      "grad_norm": 5.4189887046813965,
      "learning_rate": 4.721264818006163e-05,
      "loss": 2.1497,
      "step": 42700
    },
    {
      "epoch": 3.352655491148363,
      "grad_norm": 4.3686347007751465,
      "learning_rate": 4.720612042404303e-05,
      "loss": 1.9452,
      "step": 42800
    },
    {
      "epoch": 3.360488798370672,
      "grad_norm": 4.215681076049805,
      "learning_rate": 4.7199592668024446e-05,
      "loss": 2.0485,
      "step": 42900
    },
    {
      "epoch": 3.3683221055929815,
      "grad_norm": 4.722554683685303,
      "learning_rate": 4.719306491200585e-05,
      "loss": 2.0408,
      "step": 43000
    },
    {
      "epoch": 3.3761554128152906,
      "grad_norm": 4.073475360870361,
      "learning_rate": 4.718653715598726e-05,
      "loss": 1.9482,
      "step": 43100
    },
    {
      "epoch": 3.3839887200375998,
      "grad_norm": 4.20788049697876,
      "learning_rate": 4.718000939996867e-05,
      "loss": 1.9533,
      "step": 43200
    },
    {
      "epoch": 3.3918220272599093,
      "grad_norm": 6.734785556793213,
      "learning_rate": 4.717348164395008e-05,
      "loss": 1.9929,
      "step": 43300
    },
    {
      "epoch": 3.3996553344822185,
      "grad_norm": 5.801131248474121,
      "learning_rate": 4.716695388793149e-05,
      "loss": 2.02,
      "step": 43400
    },
    {
      "epoch": 3.4074886417045276,
      "grad_norm": 4.165582180023193,
      "learning_rate": 4.7160426131912894e-05,
      "loss": 1.963,
      "step": 43500
    },
    {
      "epoch": 3.415321948926837,
      "grad_norm": 4.016812801361084,
      "learning_rate": 4.7153898375894307e-05,
      "loss": 1.8999,
      "step": 43600
    },
    {
      "epoch": 3.4231552561491463,
      "grad_norm": 4.89268159866333,
      "learning_rate": 4.714737061987571e-05,
      "loss": 2.0004,
      "step": 43700
    },
    {
      "epoch": 3.4309885633714554,
      "grad_norm": 5.934228420257568,
      "learning_rate": 4.714084286385712e-05,
      "loss": 1.9043,
      "step": 43800
    },
    {
      "epoch": 3.4388218705937645,
      "grad_norm": 6.935124397277832,
      "learning_rate": 4.713431510783853e-05,
      "loss": 1.9628,
      "step": 43900
    },
    {
      "epoch": 3.446655177816074,
      "grad_norm": 4.984536647796631,
      "learning_rate": 4.712778735181994e-05,
      "loss": 1.9879,
      "step": 44000
    },
    {
      "epoch": 3.4544884850383832,
      "grad_norm": 5.5293660163879395,
      "learning_rate": 4.712125959580135e-05,
      "loss": 2.0099,
      "step": 44100
    },
    {
      "epoch": 3.4623217922606924,
      "grad_norm": 5.373193264007568,
      "learning_rate": 4.711473183978276e-05,
      "loss": 1.9452,
      "step": 44200
    },
    {
      "epoch": 3.4701550994830015,
      "grad_norm": 6.674759864807129,
      "learning_rate": 4.710820408376417e-05,
      "loss": 1.9106,
      "step": 44300
    },
    {
      "epoch": 3.477988406705311,
      "grad_norm": 4.554877758026123,
      "learning_rate": 4.710167632774557e-05,
      "loss": 1.9475,
      "step": 44400
    },
    {
      "epoch": 3.48582171392762,
      "grad_norm": 7.310024738311768,
      "learning_rate": 4.7095148571726986e-05,
      "loss": 1.9478,
      "step": 44500
    },
    {
      "epoch": 3.4936550211499293,
      "grad_norm": 5.205700874328613,
      "learning_rate": 4.70886208157084e-05,
      "loss": 2.0159,
      "step": 44600
    },
    {
      "epoch": 3.501488328372239,
      "grad_norm": 5.314542293548584,
      "learning_rate": 4.7082093059689804e-05,
      "loss": 2.0313,
      "step": 44700
    },
    {
      "epoch": 3.509321635594548,
      "grad_norm": 5.6562347412109375,
      "learning_rate": 4.707556530367121e-05,
      "loss": 1.8731,
      "step": 44800
    },
    {
      "epoch": 3.517154942816857,
      "grad_norm": 4.740940093994141,
      "learning_rate": 4.706903754765262e-05,
      "loss": 2.0149,
      "step": 44900
    },
    {
      "epoch": 3.5249882500391667,
      "grad_norm": 4.116093158721924,
      "learning_rate": 4.706250979163403e-05,
      "loss": 1.9663,
      "step": 45000
    },
    {
      "epoch": 3.532821557261476,
      "grad_norm": 4.1569647789001465,
      "learning_rate": 4.7055982035615434e-05,
      "loss": 1.9989,
      "step": 45100
    },
    {
      "epoch": 3.540654864483785,
      "grad_norm": 3.822322130203247,
      "learning_rate": 4.7049454279596846e-05,
      "loss": 2.0168,
      "step": 45200
    },
    {
      "epoch": 3.5484881717060945,
      "grad_norm": 5.49286413192749,
      "learning_rate": 4.704292652357826e-05,
      "loss": 2.0146,
      "step": 45300
    },
    {
      "epoch": 3.5563214789284037,
      "grad_norm": 4.291873455047607,
      "learning_rate": 4.7036398767559665e-05,
      "loss": 1.9422,
      "step": 45400
    },
    {
      "epoch": 3.564154786150713,
      "grad_norm": 4.723999977111816,
      "learning_rate": 4.702987101154108e-05,
      "loss": 2.0069,
      "step": 45500
    },
    {
      "epoch": 3.5719880933730224,
      "grad_norm": 5.211782455444336,
      "learning_rate": 4.702334325552248e-05,
      "loss": 1.9439,
      "step": 45600
    },
    {
      "epoch": 3.5798214005953315,
      "grad_norm": 4.3615193367004395,
      "learning_rate": 4.701681549950389e-05,
      "loss": 1.9543,
      "step": 45700
    },
    {
      "epoch": 3.5876547078176406,
      "grad_norm": 6.0828351974487305,
      "learning_rate": 4.70102877434853e-05,
      "loss": 1.9728,
      "step": 45800
    },
    {
      "epoch": 3.5954880150399497,
      "grad_norm": 5.256207466125488,
      "learning_rate": 4.7003759987466714e-05,
      "loss": 1.9522,
      "step": 45900
    },
    {
      "epoch": 3.603321322262259,
      "grad_norm": 4.051787853240967,
      "learning_rate": 4.699723223144812e-05,
      "loss": 2.0374,
      "step": 46000
    },
    {
      "epoch": 3.6111546294845684,
      "grad_norm": 8.156350135803223,
      "learning_rate": 4.699070447542953e-05,
      "loss": 2.0142,
      "step": 46100
    },
    {
      "epoch": 3.6189879367068776,
      "grad_norm": 4.171003818511963,
      "learning_rate": 4.698417671941094e-05,
      "loss": 1.9106,
      "step": 46200
    },
    {
      "epoch": 3.6268212439291867,
      "grad_norm": 4.047712326049805,
      "learning_rate": 4.6977648963392344e-05,
      "loss": 1.8943,
      "step": 46300
    },
    {
      "epoch": 3.6346545511514963,
      "grad_norm": 4.081398010253906,
      "learning_rate": 4.6971121207373756e-05,
      "loss": 1.9172,
      "step": 46400
    },
    {
      "epoch": 3.6424878583738054,
      "grad_norm": 5.919658660888672,
      "learning_rate": 4.696459345135517e-05,
      "loss": 1.9506,
      "step": 46500
    },
    {
      "epoch": 3.6503211655961145,
      "grad_norm": 5.920458793640137,
      "learning_rate": 4.6958065695336574e-05,
      "loss": 1.9588,
      "step": 46600
    },
    {
      "epoch": 3.658154472818424,
      "grad_norm": 7.047016620635986,
      "learning_rate": 4.695153793931798e-05,
      "loss": 2.0866,
      "step": 46700
    },
    {
      "epoch": 3.6659877800407332,
      "grad_norm": 4.861125469207764,
      "learning_rate": 4.694501018329939e-05,
      "loss": 1.8911,
      "step": 46800
    },
    {
      "epoch": 3.6738210872630424,
      "grad_norm": 7.108783721923828,
      "learning_rate": 4.69384824272808e-05,
      "loss": 2.0136,
      "step": 46900
    },
    {
      "epoch": 3.681654394485352,
      "grad_norm": 6.572436332702637,
      "learning_rate": 4.6931954671262204e-05,
      "loss": 1.9627,
      "step": 47000
    },
    {
      "epoch": 3.689487701707661,
      "grad_norm": 5.4652628898620605,
      "learning_rate": 4.692542691524362e-05,
      "loss": 1.9804,
      "step": 47100
    },
    {
      "epoch": 3.69732100892997,
      "grad_norm": 4.791603088378906,
      "learning_rate": 4.691889915922503e-05,
      "loss": 1.9659,
      "step": 47200
    },
    {
      "epoch": 3.7051543161522797,
      "grad_norm": 3.7998054027557373,
      "learning_rate": 4.6912371403206435e-05,
      "loss": 1.9728,
      "step": 47300
    },
    {
      "epoch": 3.712987623374589,
      "grad_norm": 5.755447864532471,
      "learning_rate": 4.690584364718785e-05,
      "loss": 1.9111,
      "step": 47400
    },
    {
      "epoch": 3.720820930596898,
      "grad_norm": 5.247085094451904,
      "learning_rate": 4.6899315891169253e-05,
      "loss": 1.9948,
      "step": 47500
    },
    {
      "epoch": 3.728654237819207,
      "grad_norm": 3.909461498260498,
      "learning_rate": 4.689278813515066e-05,
      "loss": 1.9086,
      "step": 47600
    },
    {
      "epoch": 3.7364875450415163,
      "grad_norm": 4.993703365325928,
      "learning_rate": 4.688626037913207e-05,
      "loss": 1.9971,
      "step": 47700
    },
    {
      "epoch": 3.744320852263826,
      "grad_norm": 4.290987491607666,
      "learning_rate": 4.6879732623113484e-05,
      "loss": 1.8683,
      "step": 47800
    },
    {
      "epoch": 3.752154159486135,
      "grad_norm": 4.814509868621826,
      "learning_rate": 4.687320486709489e-05,
      "loss": 1.9713,
      "step": 47900
    },
    {
      "epoch": 3.759987466708444,
      "grad_norm": 4.895517349243164,
      "learning_rate": 4.6866677111076296e-05,
      "loss": 1.9361,
      "step": 48000
    },
    {
      "epoch": 3.7678207739307537,
      "grad_norm": 4.671459674835205,
      "learning_rate": 4.686014935505771e-05,
      "loss": 2.0604,
      "step": 48100
    },
    {
      "epoch": 3.775654081153063,
      "grad_norm": 4.8160624504089355,
      "learning_rate": 4.6853621599039114e-05,
      "loss": 1.9223,
      "step": 48200
    },
    {
      "epoch": 3.783487388375372,
      "grad_norm": 3.6596035957336426,
      "learning_rate": 4.684709384302052e-05,
      "loss": 1.9517,
      "step": 48300
    },
    {
      "epoch": 3.7913206955976815,
      "grad_norm": 4.883203029632568,
      "learning_rate": 4.684056608700193e-05,
      "loss": 1.9956,
      "step": 48400
    },
    {
      "epoch": 3.7991540028199906,
      "grad_norm": 10.296404838562012,
      "learning_rate": 4.6834038330983345e-05,
      "loss": 1.9206,
      "step": 48500
    },
    {
      "epoch": 3.8069873100422997,
      "grad_norm": 4.398230075836182,
      "learning_rate": 4.682751057496475e-05,
      "loss": 1.9431,
      "step": 48600
    },
    {
      "epoch": 3.8148206172646093,
      "grad_norm": 4.663690567016602,
      "learning_rate": 4.682098281894616e-05,
      "loss": 2.005,
      "step": 48700
    },
    {
      "epoch": 3.8226539244869184,
      "grad_norm": 7.266565322875977,
      "learning_rate": 4.681445506292757e-05,
      "loss": 1.9591,
      "step": 48800
    },
    {
      "epoch": 3.8304872317092276,
      "grad_norm": 4.052662372589111,
      "learning_rate": 4.6807927306908975e-05,
      "loss": 1.9075,
      "step": 48900
    },
    {
      "epoch": 3.838320538931537,
      "grad_norm": 4.892978668212891,
      "learning_rate": 4.680139955089039e-05,
      "loss": 1.9469,
      "step": 49000
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 4.058023929595947,
      "learning_rate": 4.67948717948718e-05,
      "loss": 2.0338,
      "step": 49100
    },
    {
      "epoch": 3.8539871533761554,
      "grad_norm": 5.16387939453125,
      "learning_rate": 4.6788344038853206e-05,
      "loss": 1.9466,
      "step": 49200
    },
    {
      "epoch": 3.8618204605984645,
      "grad_norm": 5.260441303253174,
      "learning_rate": 4.678181628283462e-05,
      "loss": 1.9135,
      "step": 49300
    },
    {
      "epoch": 3.869653767820774,
      "grad_norm": 5.09376859664917,
      "learning_rate": 4.6775288526816024e-05,
      "loss": 1.991,
      "step": 49400
    },
    {
      "epoch": 3.877487075043083,
      "grad_norm": 7.395714282989502,
      "learning_rate": 4.676876077079743e-05,
      "loss": 1.909,
      "step": 49500
    },
    {
      "epoch": 3.8853203822653923,
      "grad_norm": 5.353390216827393,
      "learning_rate": 4.676223301477884e-05,
      "loss": 1.9812,
      "step": 49600
    },
    {
      "epoch": 3.8931536894877015,
      "grad_norm": 4.7648725509643555,
      "learning_rate": 4.6755705258760255e-05,
      "loss": 2.0299,
      "step": 49700
    },
    {
      "epoch": 3.900986996710011,
      "grad_norm": 7.057986259460449,
      "learning_rate": 4.674917750274166e-05,
      "loss": 1.9521,
      "step": 49800
    },
    {
      "epoch": 3.90882030393232,
      "grad_norm": 5.142226219177246,
      "learning_rate": 4.6742649746723066e-05,
      "loss": 1.9243,
      "step": 49900
    },
    {
      "epoch": 3.9166536111546293,
      "grad_norm": 5.858100414276123,
      "learning_rate": 4.673612199070448e-05,
      "loss": 2.0,
      "step": 50000
    },
    {
      "epoch": 3.924486918376939,
      "grad_norm": 4.686342239379883,
      "learning_rate": 4.6729594234685885e-05,
      "loss": 1.9045,
      "step": 50100
    },
    {
      "epoch": 3.932320225599248,
      "grad_norm": 4.831070899963379,
      "learning_rate": 4.672306647866729e-05,
      "loss": 1.9593,
      "step": 50200
    },
    {
      "epoch": 3.940153532821557,
      "grad_norm": 4.784497261047363,
      "learning_rate": 4.67165387226487e-05,
      "loss": 1.9987,
      "step": 50300
    },
    {
      "epoch": 3.9479868400438667,
      "grad_norm": 4.021237373352051,
      "learning_rate": 4.6710010966630116e-05,
      "loss": 1.9143,
      "step": 50400
    },
    {
      "epoch": 3.955820147266176,
      "grad_norm": 5.357560634613037,
      "learning_rate": 4.670348321061152e-05,
      "loss": 1.9378,
      "step": 50500
    },
    {
      "epoch": 3.963653454488485,
      "grad_norm": 4.596705913543701,
      "learning_rate": 4.6696955454592934e-05,
      "loss": 1.9803,
      "step": 50600
    },
    {
      "epoch": 3.9714867617107945,
      "grad_norm": 5.299375057220459,
      "learning_rate": 4.669042769857434e-05,
      "loss": 2.0289,
      "step": 50700
    },
    {
      "epoch": 3.9793200689331036,
      "grad_norm": 3.8512747287750244,
      "learning_rate": 4.6683899942555745e-05,
      "loss": 2.0438,
      "step": 50800
    },
    {
      "epoch": 3.9871533761554128,
      "grad_norm": 5.033165454864502,
      "learning_rate": 4.667737218653716e-05,
      "loss": 1.9972,
      "step": 50900
    },
    {
      "epoch": 3.9949866833777223,
      "grad_norm": 6.007688999176025,
      "learning_rate": 4.667084443051857e-05,
      "loss": 1.9653,
      "step": 51000
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.9149173498153687,
      "eval_runtime": 2.9686,
      "eval_samples_per_second": 226.37,
      "eval_steps_per_second": 226.37,
      "step": 51064
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.747023344039917,
      "eval_runtime": 56.7346,
      "eval_samples_per_second": 225.013,
      "eval_steps_per_second": 225.013,
      "step": 51064
    },
    {
      "epoch": 4.002819990600031,
      "grad_norm": 4.73599100112915,
      "learning_rate": 4.6664316674499976e-05,
      "loss": 1.9587,
      "step": 51100
    },
    {
      "epoch": 4.010653297822341,
      "grad_norm": 4.116372585296631,
      "learning_rate": 4.665778891848139e-05,
      "loss": 1.9759,
      "step": 51200
    },
    {
      "epoch": 4.01848660504465,
      "grad_norm": 5.862637519836426,
      "learning_rate": 4.6651261162462795e-05,
      "loss": 1.9789,
      "step": 51300
    },
    {
      "epoch": 4.026319912266959,
      "grad_norm": 3.6694886684417725,
      "learning_rate": 4.66447334064442e-05,
      "loss": 1.8675,
      "step": 51400
    },
    {
      "epoch": 4.034153219489268,
      "grad_norm": 5.357460021972656,
      "learning_rate": 4.6638205650425606e-05,
      "loss": 1.9992,
      "step": 51500
    },
    {
      "epoch": 4.041986526711578,
      "grad_norm": 5.275086402893066,
      "learning_rate": 4.663167789440702e-05,
      "loss": 1.954,
      "step": 51600
    },
    {
      "epoch": 4.049819833933887,
      "grad_norm": 5.671922206878662,
      "learning_rate": 4.662515013838843e-05,
      "loss": 1.9461,
      "step": 51700
    },
    {
      "epoch": 4.057653141156196,
      "grad_norm": 4.344459533691406,
      "learning_rate": 4.661862238236984e-05,
      "loss": 1.8785,
      "step": 51800
    },
    {
      "epoch": 4.065486448378506,
      "grad_norm": 5.000631332397461,
      "learning_rate": 4.661209462635125e-05,
      "loss": 1.8334,
      "step": 51900
    },
    {
      "epoch": 4.0733197556008145,
      "grad_norm": 4.398412227630615,
      "learning_rate": 4.6605566870332655e-05,
      "loss": 1.909,
      "step": 52000
    },
    {
      "epoch": 4.081153062823124,
      "grad_norm": 5.303780555725098,
      "learning_rate": 4.659903911431406e-05,
      "loss": 1.971,
      "step": 52100
    },
    {
      "epoch": 4.088986370045433,
      "grad_norm": 5.425512313842773,
      "learning_rate": 4.6592511358295474e-05,
      "loss": 2.0456,
      "step": 52200
    },
    {
      "epoch": 4.096819677267742,
      "grad_norm": 3.5480504035949707,
      "learning_rate": 4.6585983602276886e-05,
      "loss": 1.9665,
      "step": 52300
    },
    {
      "epoch": 4.104652984490052,
      "grad_norm": 4.742230415344238,
      "learning_rate": 4.657945584625829e-05,
      "loss": 1.9756,
      "step": 52400
    },
    {
      "epoch": 4.112486291712361,
      "grad_norm": 4.7486724853515625,
      "learning_rate": 4.6572928090239705e-05,
      "loss": 1.92,
      "step": 52500
    },
    {
      "epoch": 4.12031959893467,
      "grad_norm": 5.577468395233154,
      "learning_rate": 4.656640033422111e-05,
      "loss": 1.9132,
      "step": 52600
    },
    {
      "epoch": 4.12815290615698,
      "grad_norm": 5.340426921844482,
      "learning_rate": 4.6559872578202516e-05,
      "loss": 1.8737,
      "step": 52700
    },
    {
      "epoch": 4.135986213379288,
      "grad_norm": 7.77296781539917,
      "learning_rate": 4.655334482218393e-05,
      "loss": 1.8886,
      "step": 52800
    },
    {
      "epoch": 4.143819520601598,
      "grad_norm": 6.200047016143799,
      "learning_rate": 4.654681706616534e-05,
      "loss": 1.8976,
      "step": 52900
    },
    {
      "epoch": 4.1516528278239075,
      "grad_norm": 5.212062358856201,
      "learning_rate": 4.654028931014675e-05,
      "loss": 1.956,
      "step": 53000
    },
    {
      "epoch": 4.159486135046216,
      "grad_norm": 5.024997234344482,
      "learning_rate": 4.653376155412815e-05,
      "loss": 1.8966,
      "step": 53100
    },
    {
      "epoch": 4.167319442268526,
      "grad_norm": 6.029552936553955,
      "learning_rate": 4.6527233798109565e-05,
      "loss": 1.9397,
      "step": 53200
    },
    {
      "epoch": 4.175152749490835,
      "grad_norm": 5.129266738891602,
      "learning_rate": 4.652070604209097e-05,
      "loss": 1.9084,
      "step": 53300
    },
    {
      "epoch": 4.182986056713144,
      "grad_norm": 7.777626037597656,
      "learning_rate": 4.651417828607238e-05,
      "loss": 1.9968,
      "step": 53400
    },
    {
      "epoch": 4.190819363935454,
      "grad_norm": 4.421821594238281,
      "learning_rate": 4.650765053005379e-05,
      "loss": 1.9779,
      "step": 53500
    },
    {
      "epoch": 4.198652671157763,
      "grad_norm": 3.0350399017333984,
      "learning_rate": 4.65011227740352e-05,
      "loss": 1.962,
      "step": 53600
    },
    {
      "epoch": 4.206485978380072,
      "grad_norm": 3.7959320545196533,
      "learning_rate": 4.649459501801661e-05,
      "loss": 1.8387,
      "step": 53700
    },
    {
      "epoch": 4.2143192856023814,
      "grad_norm": 4.628771781921387,
      "learning_rate": 4.648806726199802e-05,
      "loss": 1.8503,
      "step": 53800
    },
    {
      "epoch": 4.22215259282469,
      "grad_norm": 4.4930219650268555,
      "learning_rate": 4.6481539505979426e-05,
      "loss": 1.9192,
      "step": 53900
    },
    {
      "epoch": 4.229985900047,
      "grad_norm": 4.151788234710693,
      "learning_rate": 4.647501174996083e-05,
      "loss": 2.0152,
      "step": 54000
    },
    {
      "epoch": 4.237819207269309,
      "grad_norm": 4.571745872497559,
      "learning_rate": 4.6468483993942244e-05,
      "loss": 1.9153,
      "step": 54100
    },
    {
      "epoch": 4.245652514491618,
      "grad_norm": 5.496715545654297,
      "learning_rate": 4.646195623792366e-05,
      "loss": 1.8661,
      "step": 54200
    },
    {
      "epoch": 4.2534858217139275,
      "grad_norm": 4.2988691329956055,
      "learning_rate": 4.645542848190506e-05,
      "loss": 1.9865,
      "step": 54300
    },
    {
      "epoch": 4.261319128936237,
      "grad_norm": 4.263460636138916,
      "learning_rate": 4.6448900725886475e-05,
      "loss": 1.9643,
      "step": 54400
    },
    {
      "epoch": 4.269152436158546,
      "grad_norm": 4.280045986175537,
      "learning_rate": 4.644237296986788e-05,
      "loss": 1.9562,
      "step": 54500
    },
    {
      "epoch": 4.276985743380855,
      "grad_norm": 4.559769153594971,
      "learning_rate": 4.643584521384929e-05,
      "loss": 1.8967,
      "step": 54600
    },
    {
      "epoch": 4.284819050603165,
      "grad_norm": 5.882992744445801,
      "learning_rate": 4.64293174578307e-05,
      "loss": 1.8974,
      "step": 54700
    },
    {
      "epoch": 4.292652357825474,
      "grad_norm": 3.8449904918670654,
      "learning_rate": 4.6422789701812105e-05,
      "loss": 2.0923,
      "step": 54800
    },
    {
      "epoch": 4.300485665047783,
      "grad_norm": 5.681944370269775,
      "learning_rate": 4.641626194579352e-05,
      "loss": 1.9697,
      "step": 54900
    },
    {
      "epoch": 4.308318972270093,
      "grad_norm": 4.876407623291016,
      "learning_rate": 4.640973418977492e-05,
      "loss": 1.9933,
      "step": 55000
    },
    {
      "epoch": 4.316152279492401,
      "grad_norm": 5.0336503982543945,
      "learning_rate": 4.6403206433756336e-05,
      "loss": 1.9605,
      "step": 55100
    },
    {
      "epoch": 4.323985586714711,
      "grad_norm": 5.312171459197998,
      "learning_rate": 4.639667867773774e-05,
      "loss": 1.9589,
      "step": 55200
    },
    {
      "epoch": 4.331818893937021,
      "grad_norm": 4.728010177612305,
      "learning_rate": 4.639015092171915e-05,
      "loss": 1.9358,
      "step": 55300
    },
    {
      "epoch": 4.339652201159329,
      "grad_norm": 5.129451751708984,
      "learning_rate": 4.638362316570056e-05,
      "loss": 1.8881,
      "step": 55400
    },
    {
      "epoch": 4.347485508381639,
      "grad_norm": 6.500182628631592,
      "learning_rate": 4.637709540968197e-05,
      "loss": 1.8771,
      "step": 55500
    },
    {
      "epoch": 4.355318815603948,
      "grad_norm": 7.308565139770508,
      "learning_rate": 4.637056765366338e-05,
      "loss": 1.9841,
      "step": 55600
    },
    {
      "epoch": 4.363152122826257,
      "grad_norm": 5.475771903991699,
      "learning_rate": 4.636403989764479e-05,
      "loss": 1.8928,
      "step": 55700
    },
    {
      "epoch": 4.370985430048567,
      "grad_norm": 5.126159191131592,
      "learning_rate": 4.6357512141626197e-05,
      "loss": 1.9653,
      "step": 55800
    },
    {
      "epoch": 4.378818737270876,
      "grad_norm": 5.04464054107666,
      "learning_rate": 4.63509843856076e-05,
      "loss": 1.8468,
      "step": 55900
    },
    {
      "epoch": 4.386652044493185,
      "grad_norm": 5.185324668884277,
      "learning_rate": 4.6344456629589015e-05,
      "loss": 2.0299,
      "step": 56000
    },
    {
      "epoch": 4.3944853517154945,
      "grad_norm": 5.36722469329834,
      "learning_rate": 4.633792887357043e-05,
      "loss": 1.98,
      "step": 56100
    },
    {
      "epoch": 4.402318658937803,
      "grad_norm": 5.149343013763428,
      "learning_rate": 4.633140111755183e-05,
      "loss": 1.9782,
      "step": 56200
    },
    {
      "epoch": 4.410151966160113,
      "grad_norm": 4.906771183013916,
      "learning_rate": 4.6324873361533246e-05,
      "loss": 1.9005,
      "step": 56300
    },
    {
      "epoch": 4.417985273382422,
      "grad_norm": 7.8331379890441895,
      "learning_rate": 4.631834560551465e-05,
      "loss": 1.862,
      "step": 56400
    },
    {
      "epoch": 4.425818580604731,
      "grad_norm": 7.276994228363037,
      "learning_rate": 4.631181784949606e-05,
      "loss": 1.985,
      "step": 56500
    },
    {
      "epoch": 4.433651887827041,
      "grad_norm": 5.15367317199707,
      "learning_rate": 4.630529009347746e-05,
      "loss": 1.8899,
      "step": 56600
    },
    {
      "epoch": 4.44148519504935,
      "grad_norm": 6.927482604980469,
      "learning_rate": 4.6298762337458876e-05,
      "loss": 1.9639,
      "step": 56700
    },
    {
      "epoch": 4.449318502271659,
      "grad_norm": 5.940095901489258,
      "learning_rate": 4.629223458144029e-05,
      "loss": 1.9936,
      "step": 56800
    },
    {
      "epoch": 4.457151809493968,
      "grad_norm": 4.7363667488098145,
      "learning_rate": 4.6285706825421694e-05,
      "loss": 1.8935,
      "step": 56900
    },
    {
      "epoch": 4.464985116716278,
      "grad_norm": 5.728607654571533,
      "learning_rate": 4.6279179069403106e-05,
      "loss": 1.8644,
      "step": 57000
    },
    {
      "epoch": 4.472818423938587,
      "grad_norm": 6.137546539306641,
      "learning_rate": 4.627265131338451e-05,
      "loss": 2.0373,
      "step": 57100
    },
    {
      "epoch": 4.480651731160896,
      "grad_norm": 4.456728935241699,
      "learning_rate": 4.626612355736592e-05,
      "loss": 1.8568,
      "step": 57200
    },
    {
      "epoch": 4.488485038383206,
      "grad_norm": 6.564637184143066,
      "learning_rate": 4.625959580134733e-05,
      "loss": 1.9966,
      "step": 57300
    },
    {
      "epoch": 4.4963183456055145,
      "grad_norm": 4.697601318359375,
      "learning_rate": 4.625306804532874e-05,
      "loss": 2.0226,
      "step": 57400
    },
    {
      "epoch": 4.504151652827824,
      "grad_norm": 5.423552513122559,
      "learning_rate": 4.624654028931015e-05,
      "loss": 2.0282,
      "step": 57500
    },
    {
      "epoch": 4.511984960050134,
      "grad_norm": 4.056656360626221,
      "learning_rate": 4.624001253329156e-05,
      "loss": 1.8759,
      "step": 57600
    },
    {
      "epoch": 4.519818267272442,
      "grad_norm": 3.4788527488708496,
      "learning_rate": 4.623348477727297e-05,
      "loss": 1.9291,
      "step": 57700
    },
    {
      "epoch": 4.527651574494752,
      "grad_norm": 5.896433353424072,
      "learning_rate": 4.622695702125437e-05,
      "loss": 1.8309,
      "step": 57800
    },
    {
      "epoch": 4.5354848817170605,
      "grad_norm": 5.686951637268066,
      "learning_rate": 4.6220429265235785e-05,
      "loss": 2.0294,
      "step": 57900
    },
    {
      "epoch": 4.54331818893937,
      "grad_norm": 4.626039028167725,
      "learning_rate": 4.621390150921719e-05,
      "loss": 1.8874,
      "step": 58000
    },
    {
      "epoch": 4.55115149616168,
      "grad_norm": 5.361112594604492,
      "learning_rate": 4.6207373753198604e-05,
      "loss": 1.8962,
      "step": 58100
    },
    {
      "epoch": 4.558984803383988,
      "grad_norm": 7.768917560577393,
      "learning_rate": 4.620084599718001e-05,
      "loss": 1.8753,
      "step": 58200
    },
    {
      "epoch": 4.566818110606298,
      "grad_norm": 5.067788600921631,
      "learning_rate": 4.619431824116142e-05,
      "loss": 1.972,
      "step": 58300
    },
    {
      "epoch": 4.5746514178286075,
      "grad_norm": 4.722743034362793,
      "learning_rate": 4.618779048514283e-05,
      "loss": 1.9123,
      "step": 58400
    },
    {
      "epoch": 4.582484725050916,
      "grad_norm": 5.933180332183838,
      "learning_rate": 4.6181262729124234e-05,
      "loss": 1.9346,
      "step": 58500
    },
    {
      "epoch": 4.590318032273226,
      "grad_norm": 4.490505695343018,
      "learning_rate": 4.6174734973105646e-05,
      "loss": 1.9154,
      "step": 58600
    },
    {
      "epoch": 4.598151339495535,
      "grad_norm": 5.39658260345459,
      "learning_rate": 4.616820721708706e-05,
      "loss": 1.9577,
      "step": 58700
    },
    {
      "epoch": 4.605984646717844,
      "grad_norm": 5.341395854949951,
      "learning_rate": 4.6161679461068464e-05,
      "loss": 1.9963,
      "step": 58800
    },
    {
      "epoch": 4.613817953940154,
      "grad_norm": 4.147541522979736,
      "learning_rate": 4.615515170504988e-05,
      "loss": 1.9628,
      "step": 58900
    },
    {
      "epoch": 4.621651261162463,
      "grad_norm": 4.215973377227783,
      "learning_rate": 4.614862394903128e-05,
      "loss": 2.0125,
      "step": 59000
    },
    {
      "epoch": 4.629484568384772,
      "grad_norm": 4.402493000030518,
      "learning_rate": 4.614209619301269e-05,
      "loss": 1.9291,
      "step": 59100
    },
    {
      "epoch": 4.637317875607081,
      "grad_norm": 5.1038432121276855,
      "learning_rate": 4.61355684369941e-05,
      "loss": 1.9699,
      "step": 59200
    },
    {
      "epoch": 4.645151182829391,
      "grad_norm": 4.561689376831055,
      "learning_rate": 4.6129040680975514e-05,
      "loss": 1.9325,
      "step": 59300
    },
    {
      "epoch": 4.6529844900517,
      "grad_norm": 7.053696155548096,
      "learning_rate": 4.612251292495692e-05,
      "loss": 1.9705,
      "step": 59400
    },
    {
      "epoch": 4.660817797274009,
      "grad_norm": 4.661675930023193,
      "learning_rate": 4.611598516893833e-05,
      "loss": 1.9006,
      "step": 59500
    },
    {
      "epoch": 4.668651104496318,
      "grad_norm": 7.402167320251465,
      "learning_rate": 4.610945741291974e-05,
      "loss": 2.0031,
      "step": 59600
    },
    {
      "epoch": 4.6764844117186275,
      "grad_norm": 3.9730021953582764,
      "learning_rate": 4.6102929656901143e-05,
      "loss": 1.9046,
      "step": 59700
    },
    {
      "epoch": 4.684317718940937,
      "grad_norm": 4.787923336029053,
      "learning_rate": 4.609640190088255e-05,
      "loss": 1.9754,
      "step": 59800
    },
    {
      "epoch": 4.692151026163246,
      "grad_norm": 6.121640205383301,
      "learning_rate": 4.608987414486396e-05,
      "loss": 1.8781,
      "step": 59900
    },
    {
      "epoch": 4.699984333385555,
      "grad_norm": 5.577556610107422,
      "learning_rate": 4.6083346388845374e-05,
      "loss": 1.956,
      "step": 60000
    },
    {
      "epoch": 4.707817640607865,
      "grad_norm": 6.200906276702881,
      "learning_rate": 4.607681863282678e-05,
      "loss": 1.9942,
      "step": 60100
    },
    {
      "epoch": 4.715650947830174,
      "grad_norm": 6.5151472091674805,
      "learning_rate": 4.607029087680819e-05,
      "loss": 1.9838,
      "step": 60200
    },
    {
      "epoch": 4.723484255052483,
      "grad_norm": 3.950657844543457,
      "learning_rate": 4.60637631207896e-05,
      "loss": 1.8884,
      "step": 60300
    },
    {
      "epoch": 4.731317562274793,
      "grad_norm": 4.95066499710083,
      "learning_rate": 4.6057235364771004e-05,
      "loss": 1.9226,
      "step": 60400
    },
    {
      "epoch": 4.739150869497101,
      "grad_norm": 5.976319789886475,
      "learning_rate": 4.605070760875242e-05,
      "loss": 1.9579,
      "step": 60500
    },
    {
      "epoch": 4.746984176719411,
      "grad_norm": 14.596490859985352,
      "learning_rate": 4.604417985273383e-05,
      "loss": 1.9585,
      "step": 60600
    },
    {
      "epoch": 4.7548174839417205,
      "grad_norm": 4.651458263397217,
      "learning_rate": 4.6037652096715235e-05,
      "loss": 2.0064,
      "step": 60700
    },
    {
      "epoch": 4.762650791164029,
      "grad_norm": 4.446235179901123,
      "learning_rate": 4.603112434069665e-05,
      "loss": 1.8444,
      "step": 60800
    },
    {
      "epoch": 4.770484098386339,
      "grad_norm": 6.283600330352783,
      "learning_rate": 4.602459658467805e-05,
      "loss": 1.9519,
      "step": 60900
    },
    {
      "epoch": 4.778317405608648,
      "grad_norm": 6.495820045471191,
      "learning_rate": 4.601806882865946e-05,
      "loss": 1.9018,
      "step": 61000
    },
    {
      "epoch": 4.786150712830957,
      "grad_norm": 5.713624000549316,
      "learning_rate": 4.601154107264087e-05,
      "loss": 1.9228,
      "step": 61100
    },
    {
      "epoch": 4.793984020053267,
      "grad_norm": 4.563314914703369,
      "learning_rate": 4.600501331662228e-05,
      "loss": 1.9659,
      "step": 61200
    },
    {
      "epoch": 4.801817327275575,
      "grad_norm": 4.644680500030518,
      "learning_rate": 4.599848556060369e-05,
      "loss": 1.8518,
      "step": 61300
    },
    {
      "epoch": 4.809650634497885,
      "grad_norm": 6.122389793395996,
      "learning_rate": 4.59919578045851e-05,
      "loss": 1.8759,
      "step": 61400
    },
    {
      "epoch": 4.8174839417201945,
      "grad_norm": 4.81769323348999,
      "learning_rate": 4.598543004856651e-05,
      "loss": 1.8749,
      "step": 61500
    },
    {
      "epoch": 4.825317248942504,
      "grad_norm": 6.551992893218994,
      "learning_rate": 4.5978902292547914e-05,
      "loss": 1.9222,
      "step": 61600
    },
    {
      "epoch": 4.833150556164813,
      "grad_norm": 4.691587448120117,
      "learning_rate": 4.597237453652932e-05,
      "loss": 1.9928,
      "step": 61700
    },
    {
      "epoch": 4.840983863387122,
      "grad_norm": 5.501951217651367,
      "learning_rate": 4.596584678051073e-05,
      "loss": 1.863,
      "step": 61800
    },
    {
      "epoch": 4.848817170609431,
      "grad_norm": 5.22545862197876,
      "learning_rate": 4.5959319024492145e-05,
      "loss": 1.9689,
      "step": 61900
    },
    {
      "epoch": 4.8566504778317405,
      "grad_norm": 4.596985816955566,
      "learning_rate": 4.595279126847355e-05,
      "loss": 1.9537,
      "step": 62000
    },
    {
      "epoch": 4.86448378505405,
      "grad_norm": 5.174563407897949,
      "learning_rate": 4.594626351245496e-05,
      "loss": 2.0244,
      "step": 62100
    },
    {
      "epoch": 4.872317092276359,
      "grad_norm": 5.378050327301025,
      "learning_rate": 4.593973575643637e-05,
      "loss": 1.9074,
      "step": 62200
    },
    {
      "epoch": 4.880150399498668,
      "grad_norm": 4.832735061645508,
      "learning_rate": 4.5933208000417775e-05,
      "loss": 1.9424,
      "step": 62300
    },
    {
      "epoch": 4.887983706720978,
      "grad_norm": 4.615085601806641,
      "learning_rate": 4.592668024439919e-05,
      "loss": 1.9805,
      "step": 62400
    },
    {
      "epoch": 4.895817013943287,
      "grad_norm": 5.572385311126709,
      "learning_rate": 4.59201524883806e-05,
      "loss": 1.9134,
      "step": 62500
    },
    {
      "epoch": 4.903650321165596,
      "grad_norm": 4.020567417144775,
      "learning_rate": 4.5913624732362006e-05,
      "loss": 1.8987,
      "step": 62600
    },
    {
      "epoch": 4.911483628387906,
      "grad_norm": 5.641813278198242,
      "learning_rate": 4.590709697634342e-05,
      "loss": 1.8875,
      "step": 62700
    },
    {
      "epoch": 4.919316935610214,
      "grad_norm": 8.227210998535156,
      "learning_rate": 4.5900569220324824e-05,
      "loss": 1.9322,
      "step": 62800
    },
    {
      "epoch": 4.927150242832524,
      "grad_norm": 5.049123764038086,
      "learning_rate": 4.589404146430623e-05,
      "loss": 2.0361,
      "step": 62900
    },
    {
      "epoch": 4.934983550054833,
      "grad_norm": 5.518784046173096,
      "learning_rate": 4.588751370828764e-05,
      "loss": 1.9773,
      "step": 63000
    },
    {
      "epoch": 4.942816857277142,
      "grad_norm": 5.028581619262695,
      "learning_rate": 4.588098595226905e-05,
      "loss": 1.8717,
      "step": 63100
    },
    {
      "epoch": 4.950650164499452,
      "grad_norm": 4.9241437911987305,
      "learning_rate": 4.587445819625046e-05,
      "loss": 1.9541,
      "step": 63200
    },
    {
      "epoch": 4.958483471721761,
      "grad_norm": 4.767052173614502,
      "learning_rate": 4.5867930440231866e-05,
      "loss": 1.933,
      "step": 63300
    },
    {
      "epoch": 4.96631677894407,
      "grad_norm": 4.600002288818359,
      "learning_rate": 4.586140268421328e-05,
      "loss": 1.9381,
      "step": 63400
    },
    {
      "epoch": 4.97415008616638,
      "grad_norm": 5.7410173416137695,
      "learning_rate": 4.5854874928194685e-05,
      "loss": 1.8486,
      "step": 63500
    },
    {
      "epoch": 4.981983393388688,
      "grad_norm": 4.296876430511475,
      "learning_rate": 4.584834717217609e-05,
      "loss": 1.89,
      "step": 63600
    },
    {
      "epoch": 4.989816700610998,
      "grad_norm": 5.8164873123168945,
      "learning_rate": 4.58418194161575e-05,
      "loss": 1.9139,
      "step": 63700
    },
    {
      "epoch": 4.9976500078333075,
      "grad_norm": 5.5823845863342285,
      "learning_rate": 4.5835291660138915e-05,
      "loss": 2.0642,
      "step": 63800
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.8963106870651245,
      "eval_runtime": 3.0228,
      "eval_samples_per_second": 222.311,
      "eval_steps_per_second": 222.311,
      "step": 63830
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.7168855667114258,
      "eval_runtime": 55.6317,
      "eval_samples_per_second": 229.473,
      "eval_steps_per_second": 229.473,
      "step": 63830
    },
    {
      "epoch": 5.005483315055616,
      "grad_norm": 4.524844646453857,
      "learning_rate": 4.582876390412032e-05,
      "loss": 1.93,
      "step": 63900
    },
    {
      "epoch": 5.013316622277926,
      "grad_norm": 4.7701616287231445,
      "learning_rate": 4.5822236148101734e-05,
      "loss": 1.9348,
      "step": 64000
    },
    {
      "epoch": 5.021149929500235,
      "grad_norm": 4.567166328430176,
      "learning_rate": 4.581570839208314e-05,
      "loss": 1.9252,
      "step": 64100
    },
    {
      "epoch": 5.028983236722544,
      "grad_norm": 5.095538139343262,
      "learning_rate": 4.5809180636064545e-05,
      "loss": 1.8133,
      "step": 64200
    },
    {
      "epoch": 5.036816543944854,
      "grad_norm": 5.090572834014893,
      "learning_rate": 4.580265288004596e-05,
      "loss": 1.9125,
      "step": 64300
    },
    {
      "epoch": 5.044649851167163,
      "grad_norm": 5.7410888671875,
      "learning_rate": 4.5796125124027364e-05,
      "loss": 1.918,
      "step": 64400
    },
    {
      "epoch": 5.052483158389472,
      "grad_norm": 4.718165397644043,
      "learning_rate": 4.5789597368008776e-05,
      "loss": 1.9036,
      "step": 64500
    },
    {
      "epoch": 5.060316465611781,
      "grad_norm": 4.802165985107422,
      "learning_rate": 4.578306961199019e-05,
      "loss": 1.9074,
      "step": 64600
    },
    {
      "epoch": 5.068149772834091,
      "grad_norm": 4.768837928771973,
      "learning_rate": 4.5776541855971595e-05,
      "loss": 1.8961,
      "step": 64700
    },
    {
      "epoch": 5.0759830800564,
      "grad_norm": 4.549527645111084,
      "learning_rate": 4.5770014099953e-05,
      "loss": 1.9221,
      "step": 64800
    },
    {
      "epoch": 5.083816387278709,
      "grad_norm": 5.975907802581787,
      "learning_rate": 4.5763486343934406e-05,
      "loss": 1.9385,
      "step": 64900
    },
    {
      "epoch": 5.091649694501018,
      "grad_norm": 3.9046790599823,
      "learning_rate": 4.575695858791582e-05,
      "loss": 1.8884,
      "step": 65000
    },
    {
      "epoch": 5.0994830017233275,
      "grad_norm": 7.664243221282959,
      "learning_rate": 4.575043083189723e-05,
      "loss": 1.9632,
      "step": 65100
    },
    {
      "epoch": 5.107316308945637,
      "grad_norm": 5.912238597869873,
      "learning_rate": 4.574390307587864e-05,
      "loss": 1.8859,
      "step": 65200
    },
    {
      "epoch": 5.115149616167946,
      "grad_norm": 4.023863792419434,
      "learning_rate": 4.573737531986005e-05,
      "loss": 1.9588,
      "step": 65300
    },
    {
      "epoch": 5.122982923390255,
      "grad_norm": 5.728212356567383,
      "learning_rate": 4.5730847563841455e-05,
      "loss": 1.8706,
      "step": 65400
    },
    {
      "epoch": 5.130816230612565,
      "grad_norm": 4.790277481079102,
      "learning_rate": 4.572431980782286e-05,
      "loss": 1.8832,
      "step": 65500
    },
    {
      "epoch": 5.1386495378348735,
      "grad_norm": 5.846380233764648,
      "learning_rate": 4.5717792051804274e-05,
      "loss": 1.8468,
      "step": 65600
    },
    {
      "epoch": 5.146482845057183,
      "grad_norm": 4.2373127937316895,
      "learning_rate": 4.5711264295785686e-05,
      "loss": 1.9042,
      "step": 65700
    },
    {
      "epoch": 5.154316152279493,
      "grad_norm": 4.443782329559326,
      "learning_rate": 4.570473653976709e-05,
      "loss": 1.9083,
      "step": 65800
    },
    {
      "epoch": 5.162149459501801,
      "grad_norm": 5.306912422180176,
      "learning_rate": 4.5698208783748504e-05,
      "loss": 1.85,
      "step": 65900
    },
    {
      "epoch": 5.169982766724111,
      "grad_norm": 6.483170509338379,
      "learning_rate": 4.569168102772991e-05,
      "loss": 1.91,
      "step": 66000
    },
    {
      "epoch": 5.1778160739464205,
      "grad_norm": 4.826560974121094,
      "learning_rate": 4.5685153271711316e-05,
      "loss": 1.9908,
      "step": 66100
    },
    {
      "epoch": 5.185649381168729,
      "grad_norm": 4.96148157119751,
      "learning_rate": 4.567862551569273e-05,
      "loss": 1.8726,
      "step": 66200
    },
    {
      "epoch": 5.193482688391039,
      "grad_norm": 4.591657638549805,
      "learning_rate": 4.5672097759674134e-05,
      "loss": 1.9229,
      "step": 66300
    },
    {
      "epoch": 5.201315995613348,
      "grad_norm": 4.9867658615112305,
      "learning_rate": 4.566557000365555e-05,
      "loss": 2.0193,
      "step": 66400
    },
    {
      "epoch": 5.209149302835657,
      "grad_norm": 5.780063152313232,
      "learning_rate": 4.565904224763696e-05,
      "loss": 1.8817,
      "step": 66500
    },
    {
      "epoch": 5.216982610057967,
      "grad_norm": 5.35037088394165,
      "learning_rate": 4.5652514491618365e-05,
      "loss": 1.9492,
      "step": 66600
    },
    {
      "epoch": 5.224815917280276,
      "grad_norm": 5.177664756774902,
      "learning_rate": 4.564598673559977e-05,
      "loss": 2.0198,
      "step": 66700
    },
    {
      "epoch": 5.232649224502585,
      "grad_norm": 4.439562797546387,
      "learning_rate": 4.563945897958118e-05,
      "loss": 1.823,
      "step": 66800
    },
    {
      "epoch": 5.240482531724894,
      "grad_norm": 3.5388548374176025,
      "learning_rate": 4.563293122356259e-05,
      "loss": 1.8981,
      "step": 66900
    },
    {
      "epoch": 5.248315838947203,
      "grad_norm": 6.232329845428467,
      "learning_rate": 4.5626403467544e-05,
      "loss": 1.8627,
      "step": 67000
    },
    {
      "epoch": 5.256149146169513,
      "grad_norm": 5.903597354888916,
      "learning_rate": 4.561987571152541e-05,
      "loss": 1.8831,
      "step": 67100
    },
    {
      "epoch": 5.263982453391822,
      "grad_norm": 6.215305805206299,
      "learning_rate": 4.561334795550682e-05,
      "loss": 1.9012,
      "step": 67200
    },
    {
      "epoch": 5.271815760614131,
      "grad_norm": 5.023486614227295,
      "learning_rate": 4.5606820199488226e-05,
      "loss": 1.944,
      "step": 67300
    },
    {
      "epoch": 5.2796490678364405,
      "grad_norm": 5.846163272857666,
      "learning_rate": 4.560029244346963e-05,
      "loss": 1.9234,
      "step": 67400
    },
    {
      "epoch": 5.28748237505875,
      "grad_norm": 5.388189792633057,
      "learning_rate": 4.5593764687451044e-05,
      "loss": 1.8185,
      "step": 67500
    },
    {
      "epoch": 5.295315682281059,
      "grad_norm": 4.440972328186035,
      "learning_rate": 4.558723693143245e-05,
      "loss": 1.9169,
      "step": 67600
    },
    {
      "epoch": 5.303148989503368,
      "grad_norm": 3.4728081226348877,
      "learning_rate": 4.558070917541386e-05,
      "loss": 1.8984,
      "step": 67700
    },
    {
      "epoch": 5.310982296725678,
      "grad_norm": 6.486695289611816,
      "learning_rate": 4.5574181419395275e-05,
      "loss": 1.9497,
      "step": 67800
    },
    {
      "epoch": 5.318815603947987,
      "grad_norm": 7.733552932739258,
      "learning_rate": 4.556765366337668e-05,
      "loss": 1.893,
      "step": 67900
    },
    {
      "epoch": 5.326648911170296,
      "grad_norm": 6.2664618492126465,
      "learning_rate": 4.5561125907358087e-05,
      "loss": 1.9613,
      "step": 68000
    },
    {
      "epoch": 5.334482218392606,
      "grad_norm": 4.2856268882751465,
      "learning_rate": 4.55545981513395e-05,
      "loss": 1.8269,
      "step": 68100
    },
    {
      "epoch": 5.342315525614914,
      "grad_norm": 5.957225322723389,
      "learning_rate": 4.5548070395320905e-05,
      "loss": 1.9835,
      "step": 68200
    },
    {
      "epoch": 5.350148832837224,
      "grad_norm": 5.12491512298584,
      "learning_rate": 4.554154263930232e-05,
      "loss": 1.9766,
      "step": 68300
    },
    {
      "epoch": 5.3579821400595335,
      "grad_norm": 4.999460697174072,
      "learning_rate": 4.553501488328372e-05,
      "loss": 1.8364,
      "step": 68400
    },
    {
      "epoch": 5.365815447281842,
      "grad_norm": 7.33103084564209,
      "learning_rate": 4.5528487127265136e-05,
      "loss": 1.9515,
      "step": 68500
    },
    {
      "epoch": 5.373648754504152,
      "grad_norm": 4.939382076263428,
      "learning_rate": 4.552195937124654e-05,
      "loss": 1.9806,
      "step": 68600
    },
    {
      "epoch": 5.3814820617264605,
      "grad_norm": 6.108592987060547,
      "learning_rate": 4.551543161522795e-05,
      "loss": 1.8779,
      "step": 68700
    },
    {
      "epoch": 5.38931536894877,
      "grad_norm": 7.209583759307861,
      "learning_rate": 4.550890385920936e-05,
      "loss": 1.8086,
      "step": 68800
    },
    {
      "epoch": 5.39714867617108,
      "grad_norm": 7.0337700843811035,
      "learning_rate": 4.550237610319077e-05,
      "loss": 1.9895,
      "step": 68900
    },
    {
      "epoch": 5.404981983393388,
      "grad_norm": 4.7371439933776855,
      "learning_rate": 4.549584834717218e-05,
      "loss": 1.8817,
      "step": 69000
    },
    {
      "epoch": 5.412815290615698,
      "grad_norm": 5.732283115386963,
      "learning_rate": 4.548932059115359e-05,
      "loss": 1.9446,
      "step": 69100
    },
    {
      "epoch": 5.4206485978380075,
      "grad_norm": 3.8652586936950684,
      "learning_rate": 4.5482792835134996e-05,
      "loss": 1.8372,
      "step": 69200
    },
    {
      "epoch": 5.428481905060316,
      "grad_norm": 5.721510887145996,
      "learning_rate": 4.54762650791164e-05,
      "loss": 1.9636,
      "step": 69300
    },
    {
      "epoch": 5.436315212282626,
      "grad_norm": 6.198986053466797,
      "learning_rate": 4.5469737323097815e-05,
      "loss": 1.8813,
      "step": 69400
    },
    {
      "epoch": 5.444148519504935,
      "grad_norm": 4.929405212402344,
      "learning_rate": 4.546320956707922e-05,
      "loss": 1.9118,
      "step": 69500
    },
    {
      "epoch": 5.451981826727244,
      "grad_norm": 3.911088228225708,
      "learning_rate": 4.545668181106063e-05,
      "loss": 1.8976,
      "step": 69600
    },
    {
      "epoch": 5.4598151339495535,
      "grad_norm": 5.583773136138916,
      "learning_rate": 4.5450154055042046e-05,
      "loss": 1.8614,
      "step": 69700
    },
    {
      "epoch": 5.467648441171863,
      "grad_norm": 4.837012767791748,
      "learning_rate": 4.544362629902345e-05,
      "loss": 1.942,
      "step": 69800
    },
    {
      "epoch": 5.475481748394172,
      "grad_norm": 4.893595218658447,
      "learning_rate": 4.543709854300486e-05,
      "loss": 1.8767,
      "step": 69900
    },
    {
      "epoch": 5.483315055616481,
      "grad_norm": 5.253508567810059,
      "learning_rate": 4.543057078698626e-05,
      "loss": 1.7956,
      "step": 70000
    },
    {
      "epoch": 5.491148362838791,
      "grad_norm": 5.720953464508057,
      "learning_rate": 4.5424043030967675e-05,
      "loss": 1.874,
      "step": 70100
    },
    {
      "epoch": 5.4989816700611,
      "grad_norm": 5.39008092880249,
      "learning_rate": 4.541751527494909e-05,
      "loss": 1.8989,
      "step": 70200
    },
    {
      "epoch": 5.506814977283409,
      "grad_norm": 5.830794334411621,
      "learning_rate": 4.5410987518930494e-05,
      "loss": 1.83,
      "step": 70300
    },
    {
      "epoch": 5.514648284505718,
      "grad_norm": 5.797290325164795,
      "learning_rate": 4.5404459762911906e-05,
      "loss": 1.8817,
      "step": 70400
    },
    {
      "epoch": 5.522481591728027,
      "grad_norm": 4.614675521850586,
      "learning_rate": 4.539793200689331e-05,
      "loss": 1.9107,
      "step": 70500
    },
    {
      "epoch": 5.530314898950337,
      "grad_norm": 5.746186256408691,
      "learning_rate": 4.539140425087472e-05,
      "loss": 1.8792,
      "step": 70600
    },
    {
      "epoch": 5.538148206172647,
      "grad_norm": 7.284931659698486,
      "learning_rate": 4.538487649485613e-05,
      "loss": 1.8269,
      "step": 70700
    },
    {
      "epoch": 5.545981513394955,
      "grad_norm": 4.044444561004639,
      "learning_rate": 4.5378348738837536e-05,
      "loss": 1.8943,
      "step": 70800
    },
    {
      "epoch": 5.553814820617265,
      "grad_norm": 5.108213901519775,
      "learning_rate": 4.537182098281895e-05,
      "loss": 2.0363,
      "step": 70900
    },
    {
      "epoch": 5.5616481278395735,
      "grad_norm": 5.276669979095459,
      "learning_rate": 4.536529322680036e-05,
      "loss": 1.9786,
      "step": 71000
    },
    {
      "epoch": 5.569481435061883,
      "grad_norm": 4.162188529968262,
      "learning_rate": 4.535876547078177e-05,
      "loss": 1.8735,
      "step": 71100
    },
    {
      "epoch": 5.577314742284193,
      "grad_norm": 5.970276832580566,
      "learning_rate": 4.535223771476317e-05,
      "loss": 1.8649,
      "step": 71200
    },
    {
      "epoch": 5.585148049506501,
      "grad_norm": 5.1196208000183105,
      "learning_rate": 4.5345709958744585e-05,
      "loss": 1.8859,
      "step": 71300
    },
    {
      "epoch": 5.592981356728811,
      "grad_norm": 3.9120137691497803,
      "learning_rate": 4.533918220272599e-05,
      "loss": 1.8428,
      "step": 71400
    },
    {
      "epoch": 5.6008146639511205,
      "grad_norm": 4.690179824829102,
      "learning_rate": 4.5332654446707404e-05,
      "loss": 1.9664,
      "step": 71500
    },
    {
      "epoch": 5.608647971173429,
      "grad_norm": 4.872270107269287,
      "learning_rate": 4.532612669068881e-05,
      "loss": 1.9775,
      "step": 71600
    },
    {
      "epoch": 5.616481278395739,
      "grad_norm": 6.489995956420898,
      "learning_rate": 4.531959893467022e-05,
      "loss": 1.898,
      "step": 71700
    },
    {
      "epoch": 5.624314585618048,
      "grad_norm": 5.011394500732422,
      "learning_rate": 4.531307117865163e-05,
      "loss": 1.9603,
      "step": 71800
    },
    {
      "epoch": 5.632147892840357,
      "grad_norm": 4.909083843231201,
      "learning_rate": 4.5306543422633033e-05,
      "loss": 1.8668,
      "step": 71900
    },
    {
      "epoch": 5.639981200062667,
      "grad_norm": 6.325141429901123,
      "learning_rate": 4.5300015666614446e-05,
      "loss": 1.8966,
      "step": 72000
    },
    {
      "epoch": 5.647814507284975,
      "grad_norm": 6.732221603393555,
      "learning_rate": 4.529348791059586e-05,
      "loss": 1.9336,
      "step": 72100
    },
    {
      "epoch": 5.655647814507285,
      "grad_norm": 7.962328910827637,
      "learning_rate": 4.5286960154577264e-05,
      "loss": 1.939,
      "step": 72200
    },
    {
      "epoch": 5.663481121729594,
      "grad_norm": 5.140292644500732,
      "learning_rate": 4.528043239855868e-05,
      "loss": 1.8907,
      "step": 72300
    },
    {
      "epoch": 5.671314428951904,
      "grad_norm": 6.490638256072998,
      "learning_rate": 4.527390464254008e-05,
      "loss": 1.9043,
      "step": 72400
    },
    {
      "epoch": 5.679147736174213,
      "grad_norm": 4.540446758270264,
      "learning_rate": 4.526737688652149e-05,
      "loss": 1.853,
      "step": 72500
    },
    {
      "epoch": 5.686981043396522,
      "grad_norm": 4.5813212394714355,
      "learning_rate": 4.52608491305029e-05,
      "loss": 1.8741,
      "step": 72600
    },
    {
      "epoch": 5.694814350618831,
      "grad_norm": 6.098787307739258,
      "learning_rate": 4.525432137448431e-05,
      "loss": 1.8846,
      "step": 72700
    },
    {
      "epoch": 5.7026476578411405,
      "grad_norm": 6.793752670288086,
      "learning_rate": 4.524779361846572e-05,
      "loss": 1.9236,
      "step": 72800
    },
    {
      "epoch": 5.71048096506345,
      "grad_norm": 4.868001937866211,
      "learning_rate": 4.524126586244713e-05,
      "loss": 1.9791,
      "step": 72900
    },
    {
      "epoch": 5.718314272285759,
      "grad_norm": 5.120480060577393,
      "learning_rate": 4.523473810642854e-05,
      "loss": 1.889,
      "step": 73000
    },
    {
      "epoch": 5.726147579508068,
      "grad_norm": 4.559901237487793,
      "learning_rate": 4.522821035040994e-05,
      "loss": 1.9489,
      "step": 73100
    },
    {
      "epoch": 5.733980886730378,
      "grad_norm": 6.060333728790283,
      "learning_rate": 4.5221682594391356e-05,
      "loss": 1.8987,
      "step": 73200
    },
    {
      "epoch": 5.7418141939526866,
      "grad_norm": 4.902194976806641,
      "learning_rate": 4.521515483837276e-05,
      "loss": 1.9854,
      "step": 73300
    },
    {
      "epoch": 5.749647501174996,
      "grad_norm": 5.530200958251953,
      "learning_rate": 4.5208627082354174e-05,
      "loss": 1.9158,
      "step": 73400
    },
    {
      "epoch": 5.757480808397306,
      "grad_norm": 5.82634973526001,
      "learning_rate": 4.520209932633558e-05,
      "loss": 1.8622,
      "step": 73500
    },
    {
      "epoch": 5.765314115619614,
      "grad_norm": 6.046175956726074,
      "learning_rate": 4.519557157031699e-05,
      "loss": 1.8556,
      "step": 73600
    },
    {
      "epoch": 5.773147422841924,
      "grad_norm": 3.4712960720062256,
      "learning_rate": 4.51890438142984e-05,
      "loss": 1.8976,
      "step": 73700
    },
    {
      "epoch": 5.780980730064233,
      "grad_norm": 4.903661251068115,
      "learning_rate": 4.5182516058279804e-05,
      "loss": 1.9179,
      "step": 73800
    },
    {
      "epoch": 5.788814037286542,
      "grad_norm": 4.745216369628906,
      "learning_rate": 4.5175988302261217e-05,
      "loss": 1.9179,
      "step": 73900
    },
    {
      "epoch": 5.796647344508852,
      "grad_norm": 7.0701189041137695,
      "learning_rate": 4.516946054624262e-05,
      "loss": 1.863,
      "step": 74000
    },
    {
      "epoch": 5.804480651731161,
      "grad_norm": 5.199631214141846,
      "learning_rate": 4.5162932790224035e-05,
      "loss": 1.9731,
      "step": 74100
    },
    {
      "epoch": 5.81231395895347,
      "grad_norm": 4.9573493003845215,
      "learning_rate": 4.515640503420545e-05,
      "loss": 1.7923,
      "step": 74200
    },
    {
      "epoch": 5.82014726617578,
      "grad_norm": 7.317769527435303,
      "learning_rate": 4.514987727818685e-05,
      "loss": 1.8806,
      "step": 74300
    },
    {
      "epoch": 5.827980573398088,
      "grad_norm": 3.8146755695343018,
      "learning_rate": 4.514334952216826e-05,
      "loss": 1.8647,
      "step": 74400
    },
    {
      "epoch": 5.835813880620398,
      "grad_norm": 3.4732656478881836,
      "learning_rate": 4.513682176614967e-05,
      "loss": 1.972,
      "step": 74500
    },
    {
      "epoch": 5.843647187842707,
      "grad_norm": 4.6508097648620605,
      "learning_rate": 4.513029401013108e-05,
      "loss": 1.8851,
      "step": 74600
    },
    {
      "epoch": 5.851480495065016,
      "grad_norm": 6.346247673034668,
      "learning_rate": 4.512376625411249e-05,
      "loss": 1.9584,
      "step": 74700
    },
    {
      "epoch": 5.859313802287326,
      "grad_norm": 5.19428825378418,
      "learning_rate": 4.51172384980939e-05,
      "loss": 1.9264,
      "step": 74800
    },
    {
      "epoch": 5.867147109509635,
      "grad_norm": 6.994325637817383,
      "learning_rate": 4.511071074207531e-05,
      "loss": 2.0086,
      "step": 74900
    },
    {
      "epoch": 5.874980416731944,
      "grad_norm": 6.30063533782959,
      "learning_rate": 4.5104182986056714e-05,
      "loss": 1.9781,
      "step": 75000
    },
    {
      "epoch": 5.8828137239542535,
      "grad_norm": 4.586233139038086,
      "learning_rate": 4.509765523003812e-05,
      "loss": 1.9655,
      "step": 75100
    },
    {
      "epoch": 5.890647031176563,
      "grad_norm": 4.3467583656311035,
      "learning_rate": 4.509112747401953e-05,
      "loss": 1.9113,
      "step": 75200
    },
    {
      "epoch": 5.898480338398872,
      "grad_norm": 5.452412128448486,
      "learning_rate": 4.5084599718000945e-05,
      "loss": 1.9389,
      "step": 75300
    },
    {
      "epoch": 5.906313645621181,
      "grad_norm": 5.093293190002441,
      "learning_rate": 4.507807196198235e-05,
      "loss": 1.8534,
      "step": 75400
    },
    {
      "epoch": 5.914146952843491,
      "grad_norm": 4.573155879974365,
      "learning_rate": 4.507154420596376e-05,
      "loss": 2.0305,
      "step": 75500
    },
    {
      "epoch": 5.9219802600658,
      "grad_norm": 5.018923759460449,
      "learning_rate": 4.506501644994517e-05,
      "loss": 1.925,
      "step": 75600
    },
    {
      "epoch": 5.929813567288109,
      "grad_norm": 4.109551429748535,
      "learning_rate": 4.5058488693926575e-05,
      "loss": 1.8924,
      "step": 75700
    },
    {
      "epoch": 5.937646874510419,
      "grad_norm": 5.007028579711914,
      "learning_rate": 4.505196093790799e-05,
      "loss": 1.9493,
      "step": 75800
    },
    {
      "epoch": 5.945480181732727,
      "grad_norm": 4.269765377044678,
      "learning_rate": 4.504543318188939e-05,
      "loss": 1.9817,
      "step": 75900
    },
    {
      "epoch": 5.953313488955037,
      "grad_norm": 4.576515197753906,
      "learning_rate": 4.5038905425870805e-05,
      "loss": 1.9661,
      "step": 76000
    },
    {
      "epoch": 5.961146796177346,
      "grad_norm": 6.2868146896362305,
      "learning_rate": 4.503237766985222e-05,
      "loss": 2.0103,
      "step": 76100
    },
    {
      "epoch": 5.968980103399655,
      "grad_norm": 4.502749443054199,
      "learning_rate": 4.5025849913833624e-05,
      "loss": 1.8947,
      "step": 76200
    },
    {
      "epoch": 5.976813410621965,
      "grad_norm": 5.862651348114014,
      "learning_rate": 4.501932215781503e-05,
      "loss": 1.8195,
      "step": 76300
    },
    {
      "epoch": 5.9846467178442735,
      "grad_norm": 4.982394695281982,
      "learning_rate": 4.501279440179644e-05,
      "loss": 1.9056,
      "step": 76400
    },
    {
      "epoch": 5.992480025066583,
      "grad_norm": 4.759921073913574,
      "learning_rate": 4.500626664577785e-05,
      "loss": 1.8364,
      "step": 76500
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.867772102355957,
      "eval_runtime": 2.939,
      "eval_samples_per_second": 228.652,
      "eval_steps_per_second": 228.652,
      "step": 76596
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.6794581413269043,
      "eval_runtime": 56.5177,
      "eval_samples_per_second": 225.876,
      "eval_steps_per_second": 225.876,
      "step": 76596
    },
    {
      "epoch": 6.000313332288893,
      "grad_norm": 4.1915106773376465,
      "learning_rate": 4.499973888975926e-05,
      "loss": 2.0548,
      "step": 76600
    },
    {
      "epoch": 6.008146639511201,
      "grad_norm": 4.101052284240723,
      "learning_rate": 4.4993211133740666e-05,
      "loss": 1.9032,
      "step": 76700
    },
    {
      "epoch": 6.015979946733511,
      "grad_norm": 6.037019729614258,
      "learning_rate": 4.498668337772208e-05,
      "loss": 1.873,
      "step": 76800
    },
    {
      "epoch": 6.0238132539558205,
      "grad_norm": 5.5709638595581055,
      "learning_rate": 4.4980155621703485e-05,
      "loss": 2.0023,
      "step": 76900
    },
    {
      "epoch": 6.031646561178129,
      "grad_norm": 5.493380069732666,
      "learning_rate": 4.497362786568489e-05,
      "loss": 1.8703,
      "step": 77000
    },
    {
      "epoch": 6.039479868400439,
      "grad_norm": 5.706836223602295,
      "learning_rate": 4.49671001096663e-05,
      "loss": 1.9066,
      "step": 77100
    },
    {
      "epoch": 6.047313175622748,
      "grad_norm": 4.750121116638184,
      "learning_rate": 4.496057235364771e-05,
      "loss": 1.9404,
      "step": 77200
    },
    {
      "epoch": 6.055146482845057,
      "grad_norm": 4.485889434814453,
      "learning_rate": 4.495404459762912e-05,
      "loss": 1.8529,
      "step": 77300
    },
    {
      "epoch": 6.0629797900673665,
      "grad_norm": 4.57599401473999,
      "learning_rate": 4.4947516841610534e-05,
      "loss": 1.8285,
      "step": 77400
    },
    {
      "epoch": 6.070813097289676,
      "grad_norm": 5.019805431365967,
      "learning_rate": 4.494098908559194e-05,
      "loss": 1.884,
      "step": 77500
    },
    {
      "epoch": 6.078646404511985,
      "grad_norm": 6.550820827484131,
      "learning_rate": 4.4934461329573345e-05,
      "loss": 1.8916,
      "step": 77600
    },
    {
      "epoch": 6.086479711734294,
      "grad_norm": 4.7651896476745605,
      "learning_rate": 4.492793357355476e-05,
      "loss": 1.8269,
      "step": 77700
    },
    {
      "epoch": 6.094313018956603,
      "grad_norm": 6.260495662689209,
      "learning_rate": 4.4921405817536164e-05,
      "loss": 1.9092,
      "step": 77800
    },
    {
      "epoch": 6.102146326178913,
      "grad_norm": 4.538776874542236,
      "learning_rate": 4.4914878061517576e-05,
      "loss": 1.8872,
      "step": 77900
    },
    {
      "epoch": 6.109979633401222,
      "grad_norm": 5.7466325759887695,
      "learning_rate": 4.490835030549899e-05,
      "loss": 1.9071,
      "step": 78000
    },
    {
      "epoch": 6.117812940623531,
      "grad_norm": 5.322168350219727,
      "learning_rate": 4.4901822549480394e-05,
      "loss": 1.8691,
      "step": 78100
    },
    {
      "epoch": 6.12564624784584,
      "grad_norm": 5.718892574310303,
      "learning_rate": 4.48952947934618e-05,
      "loss": 1.9074,
      "step": 78200
    },
    {
      "epoch": 6.13347955506815,
      "grad_norm": 4.058574199676514,
      "learning_rate": 4.488876703744321e-05,
      "loss": 1.8653,
      "step": 78300
    },
    {
      "epoch": 6.141312862290459,
      "grad_norm": 5.2053399085998535,
      "learning_rate": 4.488223928142462e-05,
      "loss": 1.8814,
      "step": 78400
    },
    {
      "epoch": 6.149146169512768,
      "grad_norm": 5.648462772369385,
      "learning_rate": 4.487571152540603e-05,
      "loss": 1.8682,
      "step": 78500
    },
    {
      "epoch": 6.156979476735078,
      "grad_norm": 6.171565532684326,
      "learning_rate": 4.486918376938744e-05,
      "loss": 1.9984,
      "step": 78600
    },
    {
      "epoch": 6.1648127839573865,
      "grad_norm": 4.536208629608154,
      "learning_rate": 4.486265601336885e-05,
      "loss": 1.9019,
      "step": 78700
    },
    {
      "epoch": 6.172646091179696,
      "grad_norm": 4.748797416687012,
      "learning_rate": 4.4856128257350255e-05,
      "loss": 1.8452,
      "step": 78800
    },
    {
      "epoch": 6.180479398402006,
      "grad_norm": 6.341729164123535,
      "learning_rate": 4.484960050133166e-05,
      "loss": 1.8898,
      "step": 78900
    },
    {
      "epoch": 6.188312705624314,
      "grad_norm": 5.07527494430542,
      "learning_rate": 4.4843072745313073e-05,
      "loss": 1.9099,
      "step": 79000
    },
    {
      "epoch": 6.196146012846624,
      "grad_norm": 5.9496331214904785,
      "learning_rate": 4.483654498929448e-05,
      "loss": 1.9545,
      "step": 79100
    },
    {
      "epoch": 6.2039793200689335,
      "grad_norm": 5.686361789703369,
      "learning_rate": 4.483001723327589e-05,
      "loss": 1.8975,
      "step": 79200
    },
    {
      "epoch": 6.211812627291242,
      "grad_norm": 5.151727199554443,
      "learning_rate": 4.4823489477257304e-05,
      "loss": 1.9581,
      "step": 79300
    },
    {
      "epoch": 6.219645934513552,
      "grad_norm": 4.589910507202148,
      "learning_rate": 4.481696172123871e-05,
      "loss": 1.8731,
      "step": 79400
    },
    {
      "epoch": 6.22747924173586,
      "grad_norm": 5.16981840133667,
      "learning_rate": 4.4810433965220116e-05,
      "loss": 1.9005,
      "step": 79500
    },
    {
      "epoch": 6.23531254895817,
      "grad_norm": 5.0053300857543945,
      "learning_rate": 4.480390620920153e-05,
      "loss": 1.81,
      "step": 79600
    },
    {
      "epoch": 6.24314585618048,
      "grad_norm": 5.2259111404418945,
      "learning_rate": 4.4797378453182934e-05,
      "loss": 1.8709,
      "step": 79700
    },
    {
      "epoch": 6.250979163402788,
      "grad_norm": 5.287365913391113,
      "learning_rate": 4.479085069716435e-05,
      "loss": 1.7591,
      "step": 79800
    },
    {
      "epoch": 6.258812470625098,
      "grad_norm": 3.4519097805023193,
      "learning_rate": 4.478432294114576e-05,
      "loss": 1.8251,
      "step": 79900
    },
    {
      "epoch": 6.266645777847407,
      "grad_norm": 5.147543430328369,
      "learning_rate": 4.4777795185127165e-05,
      "loss": 1.9337,
      "step": 80000
    },
    {
      "epoch": 6.274479085069716,
      "grad_norm": 4.172781467437744,
      "learning_rate": 4.477126742910857e-05,
      "loss": 1.8103,
      "step": 80100
    },
    {
      "epoch": 6.282312392292026,
      "grad_norm": 5.355510711669922,
      "learning_rate": 4.4764739673089977e-05,
      "loss": 1.9107,
      "step": 80200
    },
    {
      "epoch": 6.290145699514335,
      "grad_norm": 4.905365943908691,
      "learning_rate": 4.475821191707139e-05,
      "loss": 1.903,
      "step": 80300
    },
    {
      "epoch": 6.297979006736644,
      "grad_norm": 4.739535331726074,
      "learning_rate": 4.4751684161052795e-05,
      "loss": 1.9432,
      "step": 80400
    },
    {
      "epoch": 6.3058123139589535,
      "grad_norm": 6.388297080993652,
      "learning_rate": 4.474515640503421e-05,
      "loss": 1.8176,
      "step": 80500
    },
    {
      "epoch": 6.313645621181263,
      "grad_norm": 4.920557022094727,
      "learning_rate": 4.473862864901562e-05,
      "loss": 1.8841,
      "step": 80600
    },
    {
      "epoch": 6.321478928403572,
      "grad_norm": 4.263242244720459,
      "learning_rate": 4.4732100892997026e-05,
      "loss": 1.9782,
      "step": 80700
    },
    {
      "epoch": 6.329312235625881,
      "grad_norm": 5.4849371910095215,
      "learning_rate": 4.472557313697843e-05,
      "loss": 1.9086,
      "step": 80800
    },
    {
      "epoch": 6.337145542848191,
      "grad_norm": 5.249056339263916,
      "learning_rate": 4.4719045380959844e-05,
      "loss": 1.7977,
      "step": 80900
    },
    {
      "epoch": 6.3449788500704996,
      "grad_norm": 6.1368088722229,
      "learning_rate": 4.471251762494125e-05,
      "loss": 1.9331,
      "step": 81000
    },
    {
      "epoch": 6.352812157292809,
      "grad_norm": 5.180891513824463,
      "learning_rate": 4.470598986892266e-05,
      "loss": 1.8518,
      "step": 81100
    },
    {
      "epoch": 6.360645464515119,
      "grad_norm": 4.820588111877441,
      "learning_rate": 4.4699462112904075e-05,
      "loss": 1.934,
      "step": 81200
    },
    {
      "epoch": 6.368478771737427,
      "grad_norm": 4.86692476272583,
      "learning_rate": 4.469293435688548e-05,
      "loss": 1.9202,
      "step": 81300
    },
    {
      "epoch": 6.376312078959737,
      "grad_norm": 6.052197456359863,
      "learning_rate": 4.4686406600866886e-05,
      "loss": 1.828,
      "step": 81400
    },
    {
      "epoch": 6.3841453861820465,
      "grad_norm": 4.64056921005249,
      "learning_rate": 4.46798788448483e-05,
      "loss": 1.8142,
      "step": 81500
    },
    {
      "epoch": 6.391978693404355,
      "grad_norm": 5.738348960876465,
      "learning_rate": 4.4673351088829705e-05,
      "loss": 1.8869,
      "step": 81600
    },
    {
      "epoch": 6.399812000626665,
      "grad_norm": 5.119624614715576,
      "learning_rate": 4.466682333281112e-05,
      "loss": 1.8516,
      "step": 81700
    },
    {
      "epoch": 6.4076453078489735,
      "grad_norm": 4.975688457489014,
      "learning_rate": 4.466029557679252e-05,
      "loss": 1.884,
      "step": 81800
    },
    {
      "epoch": 6.415478615071283,
      "grad_norm": 4.834557056427002,
      "learning_rate": 4.4653767820773936e-05,
      "loss": 1.9063,
      "step": 81900
    },
    {
      "epoch": 6.423311922293593,
      "grad_norm": 5.076468467712402,
      "learning_rate": 4.464724006475534e-05,
      "loss": 1.9076,
      "step": 82000
    },
    {
      "epoch": 6.431145229515901,
      "grad_norm": 4.847596645355225,
      "learning_rate": 4.464071230873675e-05,
      "loss": 1.8414,
      "step": 82100
    },
    {
      "epoch": 6.438978536738211,
      "grad_norm": 5.397582054138184,
      "learning_rate": 4.463418455271816e-05,
      "loss": 1.9037,
      "step": 82200
    },
    {
      "epoch": 6.44681184396052,
      "grad_norm": 4.873594760894775,
      "learning_rate": 4.4627656796699565e-05,
      "loss": 1.8368,
      "step": 82300
    },
    {
      "epoch": 6.454645151182829,
      "grad_norm": 6.103970527648926,
      "learning_rate": 4.462112904068098e-05,
      "loss": 1.865,
      "step": 82400
    },
    {
      "epoch": 6.462478458405139,
      "grad_norm": 5.092548847198486,
      "learning_rate": 4.461460128466239e-05,
      "loss": 1.8345,
      "step": 82500
    },
    {
      "epoch": 6.470311765627448,
      "grad_norm": 3.809267282485962,
      "learning_rate": 4.4608073528643796e-05,
      "loss": 1.8359,
      "step": 82600
    },
    {
      "epoch": 6.478145072849757,
      "grad_norm": 4.547642707824707,
      "learning_rate": 4.46015457726252e-05,
      "loss": 1.9997,
      "step": 82700
    },
    {
      "epoch": 6.4859783800720665,
      "grad_norm": 5.427545547485352,
      "learning_rate": 4.4595018016606615e-05,
      "loss": 1.9408,
      "step": 82800
    },
    {
      "epoch": 6.493811687294376,
      "grad_norm": 3.9555578231811523,
      "learning_rate": 4.458849026058802e-05,
      "loss": 1.9742,
      "step": 82900
    },
    {
      "epoch": 6.501644994516685,
      "grad_norm": 4.010596752166748,
      "learning_rate": 4.458196250456943e-05,
      "loss": 1.901,
      "step": 83000
    },
    {
      "epoch": 6.509478301738994,
      "grad_norm": 6.208824157714844,
      "learning_rate": 4.4575434748550845e-05,
      "loss": 1.9276,
      "step": 83100
    },
    {
      "epoch": 6.517311608961304,
      "grad_norm": 4.94537353515625,
      "learning_rate": 4.456890699253225e-05,
      "loss": 1.8828,
      "step": 83200
    },
    {
      "epoch": 6.525144916183613,
      "grad_norm": 6.604650497436523,
      "learning_rate": 4.456237923651366e-05,
      "loss": 1.8067,
      "step": 83300
    },
    {
      "epoch": 6.532978223405922,
      "grad_norm": 4.236048221588135,
      "learning_rate": 4.455585148049506e-05,
      "loss": 1.8933,
      "step": 83400
    },
    {
      "epoch": 6.540811530628231,
      "grad_norm": 5.718343257904053,
      "learning_rate": 4.4549323724476475e-05,
      "loss": 1.7853,
      "step": 83500
    },
    {
      "epoch": 6.54864483785054,
      "grad_norm": 4.885287761688232,
      "learning_rate": 4.454279596845788e-05,
      "loss": 1.8796,
      "step": 83600
    },
    {
      "epoch": 6.55647814507285,
      "grad_norm": 4.822196960449219,
      "learning_rate": 4.4536268212439294e-05,
      "loss": 1.8161,
      "step": 83700
    },
    {
      "epoch": 6.564311452295159,
      "grad_norm": 5.285719394683838,
      "learning_rate": 4.4529740456420706e-05,
      "loss": 1.8457,
      "step": 83800
    },
    {
      "epoch": 6.572144759517468,
      "grad_norm": 4.4988112449646,
      "learning_rate": 4.452321270040211e-05,
      "loss": 1.9092,
      "step": 83900
    },
    {
      "epoch": 6.579978066739778,
      "grad_norm": 5.438190937042236,
      "learning_rate": 4.451668494438352e-05,
      "loss": 1.847,
      "step": 84000
    },
    {
      "epoch": 6.5878113739620865,
      "grad_norm": 5.098347187042236,
      "learning_rate": 4.451015718836493e-05,
      "loss": 1.9015,
      "step": 84100
    },
    {
      "epoch": 6.595644681184396,
      "grad_norm": 6.450432777404785,
      "learning_rate": 4.4503629432346336e-05,
      "loss": 1.8271,
      "step": 84200
    },
    {
      "epoch": 6.603477988406706,
      "grad_norm": 4.119333267211914,
      "learning_rate": 4.449710167632775e-05,
      "loss": 1.8054,
      "step": 84300
    },
    {
      "epoch": 6.611311295629014,
      "grad_norm": 5.421143054962158,
      "learning_rate": 4.449057392030916e-05,
      "loss": 1.9047,
      "step": 84400
    },
    {
      "epoch": 6.619144602851324,
      "grad_norm": 5.675657749176025,
      "learning_rate": 4.448404616429057e-05,
      "loss": 1.7989,
      "step": 84500
    },
    {
      "epoch": 6.6269779100736335,
      "grad_norm": 4.604155540466309,
      "learning_rate": 4.447751840827197e-05,
      "loss": 1.8678,
      "step": 84600
    },
    {
      "epoch": 6.634811217295942,
      "grad_norm": 6.629822731018066,
      "learning_rate": 4.4470990652253385e-05,
      "loss": 1.8826,
      "step": 84700
    },
    {
      "epoch": 6.642644524518252,
      "grad_norm": 5.524709701538086,
      "learning_rate": 4.446446289623479e-05,
      "loss": 1.8644,
      "step": 84800
    },
    {
      "epoch": 6.650477831740561,
      "grad_norm": 7.591411590576172,
      "learning_rate": 4.4457935140216203e-05,
      "loss": 1.8286,
      "step": 84900
    },
    {
      "epoch": 6.65831113896287,
      "grad_norm": 4.686996936798096,
      "learning_rate": 4.4451407384197616e-05,
      "loss": 1.9372,
      "step": 85000
    },
    {
      "epoch": 6.6661444461851795,
      "grad_norm": 5.74160623550415,
      "learning_rate": 4.444487962817902e-05,
      "loss": 1.9674,
      "step": 85100
    },
    {
      "epoch": 6.673977753407488,
      "grad_norm": 4.9561309814453125,
      "learning_rate": 4.443835187216043e-05,
      "loss": 1.8918,
      "step": 85200
    },
    {
      "epoch": 6.681811060629798,
      "grad_norm": 5.591190338134766,
      "learning_rate": 4.443182411614183e-05,
      "loss": 1.8786,
      "step": 85300
    },
    {
      "epoch": 6.689644367852107,
      "grad_norm": 6.107593059539795,
      "learning_rate": 4.4425296360123246e-05,
      "loss": 1.9364,
      "step": 85400
    },
    {
      "epoch": 6.697477675074416,
      "grad_norm": 4.7317795753479,
      "learning_rate": 4.441876860410465e-05,
      "loss": 1.8516,
      "step": 85500
    },
    {
      "epoch": 6.705310982296726,
      "grad_norm": 7.954570293426514,
      "learning_rate": 4.4412240848086064e-05,
      "loss": 1.8526,
      "step": 85600
    },
    {
      "epoch": 6.713144289519035,
      "grad_norm": 4.504105091094971,
      "learning_rate": 4.440571309206748e-05,
      "loss": 1.9056,
      "step": 85700
    },
    {
      "epoch": 6.720977596741344,
      "grad_norm": 4.935783863067627,
      "learning_rate": 4.439918533604888e-05,
      "loss": 1.9122,
      "step": 85800
    },
    {
      "epoch": 6.7288109039636534,
      "grad_norm": 4.788303375244141,
      "learning_rate": 4.439265758003029e-05,
      "loss": 1.8789,
      "step": 85900
    },
    {
      "epoch": 6.736644211185963,
      "grad_norm": 6.581550121307373,
      "learning_rate": 4.43861298240117e-05,
      "loss": 1.968,
      "step": 86000
    },
    {
      "epoch": 6.744477518408272,
      "grad_norm": 3.987877607345581,
      "learning_rate": 4.4379602067993107e-05,
      "loss": 1.8874,
      "step": 86100
    },
    {
      "epoch": 6.752310825630581,
      "grad_norm": 6.595106601715088,
      "learning_rate": 4.437307431197452e-05,
      "loss": 1.8411,
      "step": 86200
    },
    {
      "epoch": 6.760144132852891,
      "grad_norm": 4.554531574249268,
      "learning_rate": 4.436654655595593e-05,
      "loss": 1.9071,
      "step": 86300
    },
    {
      "epoch": 6.7679774400751995,
      "grad_norm": 5.295754432678223,
      "learning_rate": 4.436001879993734e-05,
      "loss": 1.8783,
      "step": 86400
    },
    {
      "epoch": 6.775810747297509,
      "grad_norm": 5.124614238739014,
      "learning_rate": 4.435349104391874e-05,
      "loss": 1.883,
      "step": 86500
    },
    {
      "epoch": 6.783644054519819,
      "grad_norm": 4.368738174438477,
      "learning_rate": 4.4346963287900156e-05,
      "loss": 1.9424,
      "step": 86600
    },
    {
      "epoch": 6.791477361742127,
      "grad_norm": 5.263012409210205,
      "learning_rate": 4.434043553188156e-05,
      "loss": 1.9454,
      "step": 86700
    },
    {
      "epoch": 6.799310668964437,
      "grad_norm": 5.89384126663208,
      "learning_rate": 4.433390777586297e-05,
      "loss": 1.8522,
      "step": 86800
    },
    {
      "epoch": 6.807143976186746,
      "grad_norm": 5.274849891662598,
      "learning_rate": 4.432738001984438e-05,
      "loss": 1.9779,
      "step": 86900
    },
    {
      "epoch": 6.814977283409055,
      "grad_norm": 4.632771968841553,
      "learning_rate": 4.432085226382579e-05,
      "loss": 1.8908,
      "step": 87000
    },
    {
      "epoch": 6.822810590631365,
      "grad_norm": 4.20460844039917,
      "learning_rate": 4.43143245078072e-05,
      "loss": 1.8684,
      "step": 87100
    },
    {
      "epoch": 6.830643897853674,
      "grad_norm": 4.891379356384277,
      "learning_rate": 4.4307796751788604e-05,
      "loss": 1.8708,
      "step": 87200
    },
    {
      "epoch": 6.838477205075983,
      "grad_norm": 5.774539947509766,
      "learning_rate": 4.4301268995770016e-05,
      "loss": 1.8515,
      "step": 87300
    },
    {
      "epoch": 6.846310512298293,
      "grad_norm": 5.492575168609619,
      "learning_rate": 4.429474123975142e-05,
      "loss": 1.8912,
      "step": 87400
    },
    {
      "epoch": 6.854143819520601,
      "grad_norm": 4.398056507110596,
      "learning_rate": 4.4288213483732835e-05,
      "loss": 1.8965,
      "step": 87500
    },
    {
      "epoch": 6.861977126742911,
      "grad_norm": 6.250814914703369,
      "learning_rate": 4.428168572771425e-05,
      "loss": 1.9023,
      "step": 87600
    },
    {
      "epoch": 6.86981043396522,
      "grad_norm": 4.352684020996094,
      "learning_rate": 4.427515797169565e-05,
      "loss": 1.9492,
      "step": 87700
    },
    {
      "epoch": 6.877643741187529,
      "grad_norm": 5.8058762550354,
      "learning_rate": 4.426863021567706e-05,
      "loss": 1.8708,
      "step": 87800
    },
    {
      "epoch": 6.885477048409839,
      "grad_norm": 4.107537269592285,
      "learning_rate": 4.426210245965847e-05,
      "loss": 1.9121,
      "step": 87900
    },
    {
      "epoch": 6.893310355632148,
      "grad_norm": 4.467634201049805,
      "learning_rate": 4.425557470363988e-05,
      "loss": 1.9767,
      "step": 88000
    },
    {
      "epoch": 6.901143662854457,
      "grad_norm": 5.262410640716553,
      "learning_rate": 4.424904694762129e-05,
      "loss": 1.8488,
      "step": 88100
    },
    {
      "epoch": 6.9089769700767665,
      "grad_norm": 5.637834548950195,
      "learning_rate": 4.42425191916027e-05,
      "loss": 1.8325,
      "step": 88200
    },
    {
      "epoch": 6.916810277299076,
      "grad_norm": 7.634215354919434,
      "learning_rate": 4.423599143558411e-05,
      "loss": 1.8562,
      "step": 88300
    },
    {
      "epoch": 6.924643584521385,
      "grad_norm": 6.064032554626465,
      "learning_rate": 4.4229463679565514e-05,
      "loss": 1.9156,
      "step": 88400
    },
    {
      "epoch": 6.932476891743694,
      "grad_norm": 5.025382995605469,
      "learning_rate": 4.422293592354692e-05,
      "loss": 1.9571,
      "step": 88500
    },
    {
      "epoch": 6.940310198966003,
      "grad_norm": 4.433874607086182,
      "learning_rate": 4.421640816752833e-05,
      "loss": 1.9237,
      "step": 88600
    },
    {
      "epoch": 6.948143506188313,
      "grad_norm": 5.8554368019104,
      "learning_rate": 4.420988041150974e-05,
      "loss": 1.949,
      "step": 88700
    },
    {
      "epoch": 6.955976813410622,
      "grad_norm": 5.478399276733398,
      "learning_rate": 4.420335265549115e-05,
      "loss": 1.9383,
      "step": 88800
    },
    {
      "epoch": 6.963810120632932,
      "grad_norm": 4.845142364501953,
      "learning_rate": 4.419682489947256e-05,
      "loss": 1.8645,
      "step": 88900
    },
    {
      "epoch": 6.97164342785524,
      "grad_norm": 4.084550380706787,
      "learning_rate": 4.419029714345397e-05,
      "loss": 1.8595,
      "step": 89000
    },
    {
      "epoch": 6.97947673507755,
      "grad_norm": 6.044218063354492,
      "learning_rate": 4.4183769387435374e-05,
      "loss": 1.857,
      "step": 89100
    },
    {
      "epoch": 6.987310042299859,
      "grad_norm": 5.231873989105225,
      "learning_rate": 4.417724163141679e-05,
      "loss": 1.9044,
      "step": 89200
    },
    {
      "epoch": 6.995143349522168,
      "grad_norm": 5.063162326812744,
      "learning_rate": 4.417071387539819e-05,
      "loss": 1.8559,
      "step": 89300
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.8605657815933228,
      "eval_runtime": 1.4457,
      "eval_samples_per_second": 464.821,
      "eval_steps_per_second": 464.821,
      "step": 89362
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.6574029922485352,
      "eval_runtime": 43.0164,
      "eval_samples_per_second": 296.77,
      "eval_steps_per_second": 296.77,
      "step": 89362
    },
    {
      "epoch": 7.002976656744478,
      "grad_norm": 6.499480724334717,
      "learning_rate": 4.4164186119379605e-05,
      "loss": 1.9201,
      "step": 89400
    },
    {
      "epoch": 7.0108099639667865,
      "grad_norm": 6.688878059387207,
      "learning_rate": 4.415765836336102e-05,
      "loss": 1.8702,
      "step": 89500
    },
    {
      "epoch": 7.018643271189096,
      "grad_norm": 5.679152965545654,
      "learning_rate": 4.4151130607342424e-05,
      "loss": 1.8964,
      "step": 89600
    },
    {
      "epoch": 7.026476578411406,
      "grad_norm": 5.525883197784424,
      "learning_rate": 4.414460285132383e-05,
      "loss": 1.8369,
      "step": 89700
    },
    {
      "epoch": 7.034309885633714,
      "grad_norm": 4.546505451202393,
      "learning_rate": 4.413807509530524e-05,
      "loss": 1.923,
      "step": 89800
    },
    {
      "epoch": 7.042143192856024,
      "grad_norm": 4.976408958435059,
      "learning_rate": 4.413154733928665e-05,
      "loss": 1.8453,
      "step": 89900
    },
    {
      "epoch": 7.049976500078333,
      "grad_norm": 5.76019811630249,
      "learning_rate": 4.4125019583268054e-05,
      "loss": 1.7996,
      "step": 90000
    },
    {
      "epoch": 7.057809807300642,
      "grad_norm": 4.413512229919434,
      "learning_rate": 4.411849182724947e-05,
      "loss": 1.9298,
      "step": 90100
    },
    {
      "epoch": 7.065643114522952,
      "grad_norm": 4.031225681304932,
      "learning_rate": 4.411196407123088e-05,
      "loss": 1.8699,
      "step": 90200
    },
    {
      "epoch": 7.073476421745261,
      "grad_norm": 4.239649295806885,
      "learning_rate": 4.4105436315212284e-05,
      "loss": 1.9258,
      "step": 90300
    },
    {
      "epoch": 7.08130972896757,
      "grad_norm": 4.375614166259766,
      "learning_rate": 4.409890855919369e-05,
      "loss": 1.8715,
      "step": 90400
    },
    {
      "epoch": 7.0891430361898795,
      "grad_norm": 5.6234450340271,
      "learning_rate": 4.40923808031751e-05,
      "loss": 1.8549,
      "step": 90500
    },
    {
      "epoch": 7.096976343412188,
      "grad_norm": 5.410249710083008,
      "learning_rate": 4.408585304715651e-05,
      "loss": 1.7921,
      "step": 90600
    },
    {
      "epoch": 7.104809650634498,
      "grad_norm": 4.5057549476623535,
      "learning_rate": 4.407932529113792e-05,
      "loss": 1.8443,
      "step": 90700
    },
    {
      "epoch": 7.112642957856807,
      "grad_norm": 7.101842403411865,
      "learning_rate": 4.4072797535119334e-05,
      "loss": 1.8474,
      "step": 90800
    },
    {
      "epoch": 7.120476265079116,
      "grad_norm": 4.568471908569336,
      "learning_rate": 4.406626977910074e-05,
      "loss": 1.8962,
      "step": 90900
    },
    {
      "epoch": 7.128309572301426,
      "grad_norm": 3.6762197017669678,
      "learning_rate": 4.4059742023082145e-05,
      "loss": 1.8391,
      "step": 91000
    },
    {
      "epoch": 7.136142879523735,
      "grad_norm": 5.571392059326172,
      "learning_rate": 4.405321426706356e-05,
      "loss": 1.7481,
      "step": 91100
    },
    {
      "epoch": 7.143976186746044,
      "grad_norm": 4.548376560211182,
      "learning_rate": 4.404668651104496e-05,
      "loss": 1.8306,
      "step": 91200
    },
    {
      "epoch": 7.151809493968353,
      "grad_norm": 4.851761817932129,
      "learning_rate": 4.4040158755026376e-05,
      "loss": 1.8056,
      "step": 91300
    },
    {
      "epoch": 7.159642801190663,
      "grad_norm": 5.339156150817871,
      "learning_rate": 4.403363099900779e-05,
      "loss": 1.8455,
      "step": 91400
    },
    {
      "epoch": 7.167476108412972,
      "grad_norm": 5.268421649932861,
      "learning_rate": 4.4027103242989194e-05,
      "loss": 1.8811,
      "step": 91500
    },
    {
      "epoch": 7.175309415635281,
      "grad_norm": 6.410618782043457,
      "learning_rate": 4.40205754869706e-05,
      "loss": 1.893,
      "step": 91600
    },
    {
      "epoch": 7.183142722857591,
      "grad_norm": 5.148377895355225,
      "learning_rate": 4.401404773095201e-05,
      "loss": 1.8104,
      "step": 91700
    },
    {
      "epoch": 7.1909760300798995,
      "grad_norm": 6.272737979888916,
      "learning_rate": 4.400751997493342e-05,
      "loss": 1.8325,
      "step": 91800
    },
    {
      "epoch": 7.198809337302209,
      "grad_norm": 3.938178777694702,
      "learning_rate": 4.4000992218914824e-05,
      "loss": 1.8802,
      "step": 91900
    },
    {
      "epoch": 7.206642644524519,
      "grad_norm": 3.7361228466033936,
      "learning_rate": 4.399446446289624e-05,
      "loss": 1.8714,
      "step": 92000
    },
    {
      "epoch": 7.214475951746827,
      "grad_norm": 4.937767028808594,
      "learning_rate": 4.398793670687765e-05,
      "loss": 1.8992,
      "step": 92100
    },
    {
      "epoch": 7.222309258969137,
      "grad_norm": 4.7627458572387695,
      "learning_rate": 4.3981408950859055e-05,
      "loss": 1.9238,
      "step": 92200
    },
    {
      "epoch": 7.2301425661914465,
      "grad_norm": 6.302828311920166,
      "learning_rate": 4.397488119484046e-05,
      "loss": 1.8532,
      "step": 92300
    },
    {
      "epoch": 7.237975873413755,
      "grad_norm": 3.928489923477173,
      "learning_rate": 4.396835343882187e-05,
      "loss": 1.9165,
      "step": 92400
    },
    {
      "epoch": 7.245809180636065,
      "grad_norm": 3.9136111736297607,
      "learning_rate": 4.396182568280328e-05,
      "loss": 1.978,
      "step": 92500
    },
    {
      "epoch": 7.253642487858373,
      "grad_norm": 3.695496082305908,
      "learning_rate": 4.395529792678469e-05,
      "loss": 1.8179,
      "step": 92600
    },
    {
      "epoch": 7.261475795080683,
      "grad_norm": 4.839295864105225,
      "learning_rate": 4.3948770170766104e-05,
      "loss": 1.9167,
      "step": 92700
    },
    {
      "epoch": 7.2693091023029925,
      "grad_norm": 6.126452445983887,
      "learning_rate": 4.394224241474751e-05,
      "loss": 1.8908,
      "step": 92800
    },
    {
      "epoch": 7.277142409525301,
      "grad_norm": 6.412555694580078,
      "learning_rate": 4.3935714658728916e-05,
      "loss": 1.9305,
      "step": 92900
    },
    {
      "epoch": 7.284975716747611,
      "grad_norm": 3.926610231399536,
      "learning_rate": 4.392918690271033e-05,
      "loss": 1.8667,
      "step": 93000
    },
    {
      "epoch": 7.29280902396992,
      "grad_norm": 5.136283874511719,
      "learning_rate": 4.3922659146691734e-05,
      "loss": 1.8486,
      "step": 93100
    },
    {
      "epoch": 7.300642331192229,
      "grad_norm": 5.384939193725586,
      "learning_rate": 4.391613139067314e-05,
      "loss": 1.8792,
      "step": 93200
    },
    {
      "epoch": 7.308475638414539,
      "grad_norm": 6.444245338439941,
      "learning_rate": 4.390960363465456e-05,
      "loss": 1.8024,
      "step": 93300
    },
    {
      "epoch": 7.316308945636848,
      "grad_norm": 7.313949108123779,
      "learning_rate": 4.3903075878635965e-05,
      "loss": 1.8406,
      "step": 93400
    },
    {
      "epoch": 7.324142252859157,
      "grad_norm": 7.9215192794799805,
      "learning_rate": 4.389654812261737e-05,
      "loss": 1.9644,
      "step": 93500
    },
    {
      "epoch": 7.3319755600814664,
      "grad_norm": 5.883944034576416,
      "learning_rate": 4.3890020366598776e-05,
      "loss": 1.9222,
      "step": 93600
    },
    {
      "epoch": 7.339808867303776,
      "grad_norm": 5.657112121582031,
      "learning_rate": 4.388349261058019e-05,
      "loss": 1.9182,
      "step": 93700
    },
    {
      "epoch": 7.347642174526085,
      "grad_norm": 5.3730950355529785,
      "learning_rate": 4.3876964854561595e-05,
      "loss": 1.7703,
      "step": 93800
    },
    {
      "epoch": 7.355475481748394,
      "grad_norm": 5.514874458312988,
      "learning_rate": 4.387043709854301e-05,
      "loss": 1.8814,
      "step": 93900
    },
    {
      "epoch": 7.363308788970704,
      "grad_norm": 6.421929836273193,
      "learning_rate": 4.386390934252442e-05,
      "loss": 1.852,
      "step": 94000
    },
    {
      "epoch": 7.3711420961930125,
      "grad_norm": 4.404657363891602,
      "learning_rate": 4.3857381586505826e-05,
      "loss": 1.8085,
      "step": 94100
    },
    {
      "epoch": 7.378975403415322,
      "grad_norm": 4.86623477935791,
      "learning_rate": 4.385085383048723e-05,
      "loss": 1.8456,
      "step": 94200
    },
    {
      "epoch": 7.386808710637631,
      "grad_norm": 5.411877632141113,
      "learning_rate": 4.3844326074468644e-05,
      "loss": 1.8136,
      "step": 94300
    },
    {
      "epoch": 7.39464201785994,
      "grad_norm": 5.870655059814453,
      "learning_rate": 4.383779831845005e-05,
      "loss": 1.8956,
      "step": 94400
    },
    {
      "epoch": 7.40247532508225,
      "grad_norm": 4.382625102996826,
      "learning_rate": 4.383127056243146e-05,
      "loss": 1.819,
      "step": 94500
    },
    {
      "epoch": 7.410308632304559,
      "grad_norm": 5.534652233123779,
      "learning_rate": 4.3824742806412875e-05,
      "loss": 1.8186,
      "step": 94600
    },
    {
      "epoch": 7.418141939526868,
      "grad_norm": 5.082809925079346,
      "learning_rate": 4.381821505039428e-05,
      "loss": 1.8814,
      "step": 94700
    },
    {
      "epoch": 7.425975246749178,
      "grad_norm": 5.485293865203857,
      "learning_rate": 4.3811687294375686e-05,
      "loss": 1.8572,
      "step": 94800
    },
    {
      "epoch": 7.433808553971486,
      "grad_norm": 8.611104965209961,
      "learning_rate": 4.38051595383571e-05,
      "loss": 1.8818,
      "step": 94900
    },
    {
      "epoch": 7.441641861193796,
      "grad_norm": 4.357416152954102,
      "learning_rate": 4.3798631782338505e-05,
      "loss": 1.7661,
      "step": 95000
    },
    {
      "epoch": 7.449475168416106,
      "grad_norm": 5.575433254241943,
      "learning_rate": 4.379210402631991e-05,
      "loss": 1.8564,
      "step": 95100
    },
    {
      "epoch": 7.457308475638414,
      "grad_norm": 5.230414867401123,
      "learning_rate": 4.378557627030132e-05,
      "loss": 1.9107,
      "step": 95200
    },
    {
      "epoch": 7.465141782860724,
      "grad_norm": 4.930791854858398,
      "learning_rate": 4.3779048514282735e-05,
      "loss": 1.8338,
      "step": 95300
    },
    {
      "epoch": 7.472975090083033,
      "grad_norm": 6.035702705383301,
      "learning_rate": 4.377252075826414e-05,
      "loss": 1.895,
      "step": 95400
    },
    {
      "epoch": 7.480808397305342,
      "grad_norm": 4.162937164306641,
      "learning_rate": 4.376599300224555e-05,
      "loss": 1.8117,
      "step": 95500
    },
    {
      "epoch": 7.488641704527652,
      "grad_norm": 4.528042793273926,
      "learning_rate": 4.375946524622696e-05,
      "loss": 1.9223,
      "step": 95600
    },
    {
      "epoch": 7.496475011749961,
      "grad_norm": 5.927042007446289,
      "learning_rate": 4.3752937490208365e-05,
      "loss": 1.9046,
      "step": 95700
    },
    {
      "epoch": 7.50430831897227,
      "grad_norm": 6.103978157043457,
      "learning_rate": 4.374640973418978e-05,
      "loss": 1.8714,
      "step": 95800
    },
    {
      "epoch": 7.5121416261945795,
      "grad_norm": 5.04842472076416,
      "learning_rate": 4.373988197817119e-05,
      "loss": 1.8666,
      "step": 95900
    },
    {
      "epoch": 7.519974933416888,
      "grad_norm": 4.13715934753418,
      "learning_rate": 4.3733354222152596e-05,
      "loss": 1.8438,
      "step": 96000
    },
    {
      "epoch": 7.527808240639198,
      "grad_norm": 5.689407825469971,
      "learning_rate": 4.3726826466134e-05,
      "loss": 1.8659,
      "step": 96100
    },
    {
      "epoch": 7.535641547861507,
      "grad_norm": 5.168278217315674,
      "learning_rate": 4.3720298710115414e-05,
      "loss": 1.8434,
      "step": 96200
    },
    {
      "epoch": 7.543474855083817,
      "grad_norm": 4.838292121887207,
      "learning_rate": 4.371377095409682e-05,
      "loss": 1.8349,
      "step": 96300
    },
    {
      "epoch": 7.551308162306126,
      "grad_norm": 3.344209671020508,
      "learning_rate": 4.3707243198078226e-05,
      "loss": 1.9918,
      "step": 96400
    },
    {
      "epoch": 7.559141469528435,
      "grad_norm": 5.948869228363037,
      "learning_rate": 4.3700715442059645e-05,
      "loss": 1.878,
      "step": 96500
    },
    {
      "epoch": 7.566974776750744,
      "grad_norm": 5.565150737762451,
      "learning_rate": 4.369418768604105e-05,
      "loss": 1.8788,
      "step": 96600
    },
    {
      "epoch": 7.574808083973053,
      "grad_norm": 4.887584686279297,
      "learning_rate": 4.368765993002246e-05,
      "loss": 1.853,
      "step": 96700
    },
    {
      "epoch": 7.582641391195363,
      "grad_norm": 5.971219539642334,
      "learning_rate": 4.368113217400387e-05,
      "loss": 1.865,
      "step": 96800
    },
    {
      "epoch": 7.590474698417672,
      "grad_norm": 5.72511100769043,
      "learning_rate": 4.3674604417985275e-05,
      "loss": 1.811,
      "step": 96900
    },
    {
      "epoch": 7.598308005639981,
      "grad_norm": 7.589619159698486,
      "learning_rate": 4.366807666196668e-05,
      "loss": 1.8823,
      "step": 97000
    },
    {
      "epoch": 7.606141312862291,
      "grad_norm": 7.245207786560059,
      "learning_rate": 4.3661548905948093e-05,
      "loss": 1.8321,
      "step": 97100
    },
    {
      "epoch": 7.6139746200845995,
      "grad_norm": 3.9786863327026367,
      "learning_rate": 4.3655021149929506e-05,
      "loss": 1.8142,
      "step": 97200
    },
    {
      "epoch": 7.621807927306909,
      "grad_norm": 5.338998317718506,
      "learning_rate": 4.364849339391091e-05,
      "loss": 1.9095,
      "step": 97300
    },
    {
      "epoch": 7.629641234529219,
      "grad_norm": 5.823686122894287,
      "learning_rate": 4.364196563789232e-05,
      "loss": 1.8579,
      "step": 97400
    },
    {
      "epoch": 7.637474541751527,
      "grad_norm": 5.580321788787842,
      "learning_rate": 4.363543788187373e-05,
      "loss": 1.8161,
      "step": 97500
    },
    {
      "epoch": 7.645307848973837,
      "grad_norm": 5.439297199249268,
      "learning_rate": 4.3628910125855136e-05,
      "loss": 1.9051,
      "step": 97600
    },
    {
      "epoch": 7.6531411561961455,
      "grad_norm": 4.845741271972656,
      "learning_rate": 4.362238236983655e-05,
      "loss": 1.9804,
      "step": 97700
    },
    {
      "epoch": 7.660974463418455,
      "grad_norm": 5.6002631187438965,
      "learning_rate": 4.361585461381796e-05,
      "loss": 1.8032,
      "step": 97800
    },
    {
      "epoch": 7.668807770640765,
      "grad_norm": 5.441030025482178,
      "learning_rate": 4.360932685779937e-05,
      "loss": 1.8654,
      "step": 97900
    },
    {
      "epoch": 7.676641077863074,
      "grad_norm": 4.140481472015381,
      "learning_rate": 4.360279910178077e-05,
      "loss": 1.8183,
      "step": 98000
    },
    {
      "epoch": 7.684474385085383,
      "grad_norm": 4.7139434814453125,
      "learning_rate": 4.3596271345762185e-05,
      "loss": 1.8488,
      "step": 98100
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 4.999976634979248,
      "learning_rate": 4.358974358974359e-05,
      "loss": 1.8829,
      "step": 98200
    },
    {
      "epoch": 7.700140999530001,
      "grad_norm": 4.594523906707764,
      "learning_rate": 4.3583215833724997e-05,
      "loss": 1.8311,
      "step": 98300
    },
    {
      "epoch": 7.707974306752311,
      "grad_norm": 5.269774913787842,
      "learning_rate": 4.357668807770641e-05,
      "loss": 1.7747,
      "step": 98400
    },
    {
      "epoch": 7.71580761397462,
      "grad_norm": 4.423227310180664,
      "learning_rate": 4.357016032168782e-05,
      "loss": 1.9283,
      "step": 98500
    },
    {
      "epoch": 7.723640921196929,
      "grad_norm": 3.9966726303100586,
      "learning_rate": 4.356363256566923e-05,
      "loss": 1.8254,
      "step": 98600
    },
    {
      "epoch": 7.731474228419239,
      "grad_norm": 6.267873764038086,
      "learning_rate": 4.355710480965063e-05,
      "loss": 1.8333,
      "step": 98700
    },
    {
      "epoch": 7.739307535641548,
      "grad_norm": 5.418694496154785,
      "learning_rate": 4.3550577053632046e-05,
      "loss": 1.8821,
      "step": 98800
    },
    {
      "epoch": 7.747140842863857,
      "grad_norm": 4.874550819396973,
      "learning_rate": 4.354404929761345e-05,
      "loss": 1.8335,
      "step": 98900
    },
    {
      "epoch": 7.754974150086166,
      "grad_norm": 4.624449253082275,
      "learning_rate": 4.3537521541594864e-05,
      "loss": 1.8599,
      "step": 99000
    },
    {
      "epoch": 7.762807457308476,
      "grad_norm": 6.050155162811279,
      "learning_rate": 4.3530993785576277e-05,
      "loss": 1.905,
      "step": 99100
    },
    {
      "epoch": 7.770640764530785,
      "grad_norm": 4.847387790679932,
      "learning_rate": 4.352446602955768e-05,
      "loss": 1.9604,
      "step": 99200
    },
    {
      "epoch": 7.778474071753094,
      "grad_norm": 7.587813377380371,
      "learning_rate": 4.351793827353909e-05,
      "loss": 1.9191,
      "step": 99300
    },
    {
      "epoch": 7.786307378975403,
      "grad_norm": 5.18441915512085,
      "learning_rate": 4.35114105175205e-05,
      "loss": 1.8135,
      "step": 99400
    },
    {
      "epoch": 7.7941406861977125,
      "grad_norm": 3.5489132404327393,
      "learning_rate": 4.3504882761501906e-05,
      "loss": 1.8632,
      "step": 99500
    },
    {
      "epoch": 7.801973993420022,
      "grad_norm": 4.949942111968994,
      "learning_rate": 4.349835500548331e-05,
      "loss": 1.8876,
      "step": 99600
    },
    {
      "epoch": 7.809807300642332,
      "grad_norm": 4.723047256469727,
      "learning_rate": 4.349182724946473e-05,
      "loss": 1.8791,
      "step": 99700
    },
    {
      "epoch": 7.81764060786464,
      "grad_norm": 6.019229412078857,
      "learning_rate": 4.348529949344614e-05,
      "loss": 1.8996,
      "step": 99800
    },
    {
      "epoch": 7.82547391508695,
      "grad_norm": 6.156996726989746,
      "learning_rate": 4.347877173742754e-05,
      "loss": 1.7937,
      "step": 99900
    },
    {
      "epoch": 7.833307222309259,
      "grad_norm": 4.7639479637146,
      "learning_rate": 4.3472243981408956e-05,
      "loss": 1.9373,
      "step": 100000
    },
    {
      "epoch": 7.841140529531568,
      "grad_norm": 4.921067714691162,
      "learning_rate": 4.346571622539036e-05,
      "loss": 1.8837,
      "step": 100100
    },
    {
      "epoch": 7.848973836753878,
      "grad_norm": 3.8996567726135254,
      "learning_rate": 4.345918846937177e-05,
      "loss": 1.9278,
      "step": 100200
    },
    {
      "epoch": 7.856807143976186,
      "grad_norm": 6.562733173370361,
      "learning_rate": 4.345266071335318e-05,
      "loss": 1.8012,
      "step": 100300
    },
    {
      "epoch": 7.864640451198496,
      "grad_norm": 4.8470778465271,
      "learning_rate": 4.344613295733459e-05,
      "loss": 1.9085,
      "step": 100400
    },
    {
      "epoch": 7.8724737584208055,
      "grad_norm": 5.996266841888428,
      "learning_rate": 4.3439605201316e-05,
      "loss": 1.8707,
      "step": 100500
    },
    {
      "epoch": 7.880307065643114,
      "grad_norm": 4.8102641105651855,
      "learning_rate": 4.3433077445297404e-05,
      "loss": 1.9247,
      "step": 100600
    },
    {
      "epoch": 7.888140372865424,
      "grad_norm": 4.944815158843994,
      "learning_rate": 4.3426549689278816e-05,
      "loss": 1.7684,
      "step": 100700
    },
    {
      "epoch": 7.895973680087733,
      "grad_norm": 5.340024948120117,
      "learning_rate": 4.342002193326022e-05,
      "loss": 1.9078,
      "step": 100800
    },
    {
      "epoch": 7.903806987310042,
      "grad_norm": 7.194905757904053,
      "learning_rate": 4.3413494177241635e-05,
      "loss": 1.9172,
      "step": 100900
    },
    {
      "epoch": 7.911640294532352,
      "grad_norm": 3.897456645965576,
      "learning_rate": 4.340696642122305e-05,
      "loss": 1.9765,
      "step": 101000
    },
    {
      "epoch": 7.919473601754661,
      "grad_norm": 4.6745147705078125,
      "learning_rate": 4.340043866520445e-05,
      "loss": 1.782,
      "step": 101100
    },
    {
      "epoch": 7.92730690897697,
      "grad_norm": 6.08458137512207,
      "learning_rate": 4.339391090918586e-05,
      "loss": 1.7858,
      "step": 101200
    },
    {
      "epoch": 7.9351402161992795,
      "grad_norm": 5.9355292320251465,
      "learning_rate": 4.338738315316727e-05,
      "loss": 1.8111,
      "step": 101300
    },
    {
      "epoch": 7.942973523421589,
      "grad_norm": 3.794377326965332,
      "learning_rate": 4.338085539714868e-05,
      "loss": 1.852,
      "step": 101400
    },
    {
      "epoch": 7.950806830643898,
      "grad_norm": 4.842844009399414,
      "learning_rate": 4.337432764113008e-05,
      "loss": 1.9099,
      "step": 101500
    },
    {
      "epoch": 7.958640137866207,
      "grad_norm": 4.5869951248168945,
      "learning_rate": 4.3367799885111495e-05,
      "loss": 1.9057,
      "step": 101600
    },
    {
      "epoch": 7.966473445088516,
      "grad_norm": 6.069438934326172,
      "learning_rate": 4.336127212909291e-05,
      "loss": 1.8576,
      "step": 101700
    },
    {
      "epoch": 7.9743067523108255,
      "grad_norm": 5.4702348709106445,
      "learning_rate": 4.3354744373074314e-05,
      "loss": 1.9353,
      "step": 101800
    },
    {
      "epoch": 7.982140059533135,
      "grad_norm": 4.635383605957031,
      "learning_rate": 4.334821661705572e-05,
      "loss": 1.9123,
      "step": 101900
    },
    {
      "epoch": 7.989973366755444,
      "grad_norm": 4.750859260559082,
      "learning_rate": 4.334168886103713e-05,
      "loss": 1.9203,
      "step": 102000
    },
    {
      "epoch": 7.997806673977753,
      "grad_norm": 5.247661113739014,
      "learning_rate": 4.333516110501854e-05,
      "loss": 1.7964,
      "step": 102100
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.8454644680023193,
      "eval_runtime": 1.4231,
      "eval_samples_per_second": 472.218,
      "eval_steps_per_second": 472.218,
      "step": 102128
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.6358996629714966,
      "eval_runtime": 27.1756,
      "eval_samples_per_second": 469.759,
      "eval_steps_per_second": 469.759,
      "step": 102128
    },
    {
      "epoch": 8.005639981200062,
      "grad_norm": 4.77195930480957,
      "learning_rate": 4.332863334899995e-05,
      "loss": 1.8489,
      "step": 102200
    },
    {
      "epoch": 8.013473288422372,
      "grad_norm": 13.42249870300293,
      "learning_rate": 4.332210559298136e-05,
      "loss": 1.8254,
      "step": 102300
    },
    {
      "epoch": 8.021306595644681,
      "grad_norm": 6.091625213623047,
      "learning_rate": 4.331557783696277e-05,
      "loss": 1.8643,
      "step": 102400
    },
    {
      "epoch": 8.02913990286699,
      "grad_norm": 6.181840419769287,
      "learning_rate": 4.3309050080944174e-05,
      "loss": 1.8544,
      "step": 102500
    },
    {
      "epoch": 8.0369732100893,
      "grad_norm": 4.126466274261475,
      "learning_rate": 4.330252232492559e-05,
      "loss": 1.7649,
      "step": 102600
    },
    {
      "epoch": 8.044806517311608,
      "grad_norm": 7.92780876159668,
      "learning_rate": 4.329599456890699e-05,
      "loss": 1.921,
      "step": 102700
    },
    {
      "epoch": 8.052639824533918,
      "grad_norm": 5.012399196624756,
      "learning_rate": 4.32894668128884e-05,
      "loss": 1.8707,
      "step": 102800
    },
    {
      "epoch": 8.060473131756227,
      "grad_norm": 6.020811080932617,
      "learning_rate": 4.328293905686982e-05,
      "loss": 1.8582,
      "step": 102900
    },
    {
      "epoch": 8.068306438978537,
      "grad_norm": 5.009047508239746,
      "learning_rate": 4.3276411300851224e-05,
      "loss": 1.8798,
      "step": 103000
    },
    {
      "epoch": 8.076139746200846,
      "grad_norm": 4.938540935516357,
      "learning_rate": 4.326988354483263e-05,
      "loss": 1.874,
      "step": 103100
    },
    {
      "epoch": 8.083973053423156,
      "grad_norm": 6.089754104614258,
      "learning_rate": 4.326335578881404e-05,
      "loss": 1.7207,
      "step": 103200
    },
    {
      "epoch": 8.091806360645464,
      "grad_norm": 4.8066487312316895,
      "learning_rate": 4.325682803279545e-05,
      "loss": 1.838,
      "step": 103300
    },
    {
      "epoch": 8.099639667867773,
      "grad_norm": 5.29758358001709,
      "learning_rate": 4.325030027677685e-05,
      "loss": 1.7786,
      "step": 103400
    },
    {
      "epoch": 8.107472975090083,
      "grad_norm": 4.431085586547852,
      "learning_rate": 4.3243772520758266e-05,
      "loss": 1.9229,
      "step": 103500
    },
    {
      "epoch": 8.115306282312392,
      "grad_norm": 4.960023403167725,
      "learning_rate": 4.323724476473968e-05,
      "loss": 1.7855,
      "step": 103600
    },
    {
      "epoch": 8.123139589534702,
      "grad_norm": 6.230507850646973,
      "learning_rate": 4.3230717008721084e-05,
      "loss": 1.8893,
      "step": 103700
    },
    {
      "epoch": 8.130972896757012,
      "grad_norm": 4.578603267669678,
      "learning_rate": 4.322418925270249e-05,
      "loss": 1.8866,
      "step": 103800
    },
    {
      "epoch": 8.13880620397932,
      "grad_norm": 4.6527099609375,
      "learning_rate": 4.32176614966839e-05,
      "loss": 1.8201,
      "step": 103900
    },
    {
      "epoch": 8.146639511201629,
      "grad_norm": 5.434842586517334,
      "learning_rate": 4.321113374066531e-05,
      "loss": 1.8673,
      "step": 104000
    },
    {
      "epoch": 8.154472818423939,
      "grad_norm": 6.400641918182373,
      "learning_rate": 4.320460598464672e-05,
      "loss": 1.8085,
      "step": 104100
    },
    {
      "epoch": 8.162306125646248,
      "grad_norm": 6.200074195861816,
      "learning_rate": 4.3198078228628133e-05,
      "loss": 1.8509,
      "step": 104200
    },
    {
      "epoch": 8.170139432868558,
      "grad_norm": 4.566040992736816,
      "learning_rate": 4.319155047260954e-05,
      "loss": 1.8824,
      "step": 104300
    },
    {
      "epoch": 8.177972740090865,
      "grad_norm": 5.145117282867432,
      "learning_rate": 4.3185022716590945e-05,
      "loss": 1.8234,
      "step": 104400
    },
    {
      "epoch": 8.185806047313175,
      "grad_norm": 5.089197158813477,
      "learning_rate": 4.317849496057236e-05,
      "loss": 1.7349,
      "step": 104500
    },
    {
      "epoch": 8.193639354535485,
      "grad_norm": 4.0673017501831055,
      "learning_rate": 4.317196720455376e-05,
      "loss": 1.7716,
      "step": 104600
    },
    {
      "epoch": 8.201472661757794,
      "grad_norm": 4.057898998260498,
      "learning_rate": 4.316543944853517e-05,
      "loss": 1.813,
      "step": 104700
    },
    {
      "epoch": 8.209305968980104,
      "grad_norm": 6.435952663421631,
      "learning_rate": 4.315891169251658e-05,
      "loss": 1.8639,
      "step": 104800
    },
    {
      "epoch": 8.217139276202413,
      "grad_norm": 4.72334623336792,
      "learning_rate": 4.3152383936497994e-05,
      "loss": 1.7741,
      "step": 104900
    },
    {
      "epoch": 8.224972583424721,
      "grad_norm": 6.119784355163574,
      "learning_rate": 4.31458561804794e-05,
      "loss": 1.7683,
      "step": 105000
    },
    {
      "epoch": 8.23280589064703,
      "grad_norm": 5.071747779846191,
      "learning_rate": 4.313932842446081e-05,
      "loss": 1.8996,
      "step": 105100
    },
    {
      "epoch": 8.24063919786934,
      "grad_norm": 4.020365238189697,
      "learning_rate": 4.313280066844222e-05,
      "loss": 1.8644,
      "step": 105200
    },
    {
      "epoch": 8.24847250509165,
      "grad_norm": 4.614313125610352,
      "learning_rate": 4.3126272912423624e-05,
      "loss": 1.84,
      "step": 105300
    },
    {
      "epoch": 8.25630581231396,
      "grad_norm": 5.145082473754883,
      "learning_rate": 4.3119745156405037e-05,
      "loss": 1.8692,
      "step": 105400
    },
    {
      "epoch": 8.264139119536269,
      "grad_norm": 4.421053409576416,
      "learning_rate": 4.311321740038645e-05,
      "loss": 1.8098,
      "step": 105500
    },
    {
      "epoch": 8.271972426758577,
      "grad_norm": 5.564326763153076,
      "learning_rate": 4.3106689644367855e-05,
      "loss": 1.8036,
      "step": 105600
    },
    {
      "epoch": 8.279805733980886,
      "grad_norm": 5.269363880157471,
      "learning_rate": 4.310016188834926e-05,
      "loss": 1.7949,
      "step": 105700
    },
    {
      "epoch": 8.287639041203196,
      "grad_norm": 4.419637203216553,
      "learning_rate": 4.309363413233067e-05,
      "loss": 1.7465,
      "step": 105800
    },
    {
      "epoch": 8.295472348425506,
      "grad_norm": 3.7312533855438232,
      "learning_rate": 4.308710637631208e-05,
      "loss": 1.7848,
      "step": 105900
    },
    {
      "epoch": 8.303305655647815,
      "grad_norm": 4.010397911071777,
      "learning_rate": 4.3080578620293485e-05,
      "loss": 1.9027,
      "step": 106000
    },
    {
      "epoch": 8.311138962870125,
      "grad_norm": 7.3138346672058105,
      "learning_rate": 4.3074050864274904e-05,
      "loss": 1.8864,
      "step": 106100
    },
    {
      "epoch": 8.318972270092432,
      "grad_norm": 6.5535430908203125,
      "learning_rate": 4.306752310825631e-05,
      "loss": 1.8165,
      "step": 106200
    },
    {
      "epoch": 8.326805577314742,
      "grad_norm": 5.436423301696777,
      "learning_rate": 4.3060995352237716e-05,
      "loss": 1.8347,
      "step": 106300
    },
    {
      "epoch": 8.334638884537052,
      "grad_norm": 5.369569778442383,
      "learning_rate": 4.305446759621913e-05,
      "loss": 1.7649,
      "step": 106400
    },
    {
      "epoch": 8.342472191759361,
      "grad_norm": 5.784290313720703,
      "learning_rate": 4.3047939840200534e-05,
      "loss": 1.8443,
      "step": 106500
    },
    {
      "epoch": 8.35030549898167,
      "grad_norm": 3.7602951526641846,
      "learning_rate": 4.304141208418194e-05,
      "loss": 1.8111,
      "step": 106600
    },
    {
      "epoch": 8.358138806203979,
      "grad_norm": 4.990518569946289,
      "learning_rate": 4.303488432816335e-05,
      "loss": 1.7736,
      "step": 106700
    },
    {
      "epoch": 8.365972113426288,
      "grad_norm": 5.936410903930664,
      "learning_rate": 4.3028356572144765e-05,
      "loss": 1.8614,
      "step": 106800
    },
    {
      "epoch": 8.373805420648598,
      "grad_norm": 6.040550708770752,
      "learning_rate": 4.302182881612617e-05,
      "loss": 1.916,
      "step": 106900
    },
    {
      "epoch": 8.381638727870907,
      "grad_norm": 4.74567985534668,
      "learning_rate": 4.3015301060107576e-05,
      "loss": 1.8365,
      "step": 107000
    },
    {
      "epoch": 8.389472035093217,
      "grad_norm": 4.630902290344238,
      "learning_rate": 4.300877330408899e-05,
      "loss": 1.8125,
      "step": 107100
    },
    {
      "epoch": 8.397305342315526,
      "grad_norm": 5.046821117401123,
      "learning_rate": 4.3002245548070395e-05,
      "loss": 1.8464,
      "step": 107200
    },
    {
      "epoch": 8.405138649537834,
      "grad_norm": 4.085670471191406,
      "learning_rate": 4.299571779205181e-05,
      "loss": 1.9021,
      "step": 107300
    },
    {
      "epoch": 8.412971956760144,
      "grad_norm": 5.515462875366211,
      "learning_rate": 4.298919003603322e-05,
      "loss": 1.8142,
      "step": 107400
    },
    {
      "epoch": 8.420805263982453,
      "grad_norm": 5.229372501373291,
      "learning_rate": 4.2982662280014625e-05,
      "loss": 1.7691,
      "step": 107500
    },
    {
      "epoch": 8.428638571204763,
      "grad_norm": 4.134882926940918,
      "learning_rate": 4.297613452399603e-05,
      "loss": 1.8104,
      "step": 107600
    },
    {
      "epoch": 8.436471878427072,
      "grad_norm": 5.227293968200684,
      "learning_rate": 4.2969606767977444e-05,
      "loss": 1.9712,
      "step": 107700
    },
    {
      "epoch": 8.44430518564938,
      "grad_norm": 4.260936260223389,
      "learning_rate": 4.296307901195885e-05,
      "loss": 1.9172,
      "step": 107800
    },
    {
      "epoch": 8.45213849287169,
      "grad_norm": 4.839412212371826,
      "learning_rate": 4.2956551255940255e-05,
      "loss": 1.8882,
      "step": 107900
    },
    {
      "epoch": 8.459971800094,
      "grad_norm": 6.925136566162109,
      "learning_rate": 4.295002349992167e-05,
      "loss": 1.7543,
      "step": 108000
    },
    {
      "epoch": 8.467805107316309,
      "grad_norm": 5.305649757385254,
      "learning_rate": 4.294349574390308e-05,
      "loss": 1.9497,
      "step": 108100
    },
    {
      "epoch": 8.475638414538619,
      "grad_norm": 4.109306335449219,
      "learning_rate": 4.2936967987884486e-05,
      "loss": 1.9273,
      "step": 108200
    },
    {
      "epoch": 8.483471721760928,
      "grad_norm": 7.058071613311768,
      "learning_rate": 4.29304402318659e-05,
      "loss": 1.749,
      "step": 108300
    },
    {
      "epoch": 8.491305028983236,
      "grad_norm": 6.961357593536377,
      "learning_rate": 4.2923912475847304e-05,
      "loss": 1.9168,
      "step": 108400
    },
    {
      "epoch": 8.499138336205545,
      "grad_norm": 4.373779773712158,
      "learning_rate": 4.291738471982871e-05,
      "loss": 1.801,
      "step": 108500
    },
    {
      "epoch": 8.506971643427855,
      "grad_norm": 5.244163513183594,
      "learning_rate": 4.291085696381012e-05,
      "loss": 1.8905,
      "step": 108600
    },
    {
      "epoch": 8.514804950650165,
      "grad_norm": 4.127632141113281,
      "learning_rate": 4.2904329207791535e-05,
      "loss": 1.8294,
      "step": 108700
    },
    {
      "epoch": 8.522638257872474,
      "grad_norm": 5.909824371337891,
      "learning_rate": 4.289780145177294e-05,
      "loss": 1.7002,
      "step": 108800
    },
    {
      "epoch": 8.530471565094784,
      "grad_norm": 4.983743190765381,
      "learning_rate": 4.289127369575435e-05,
      "loss": 1.8291,
      "step": 108900
    },
    {
      "epoch": 8.538304872317092,
      "grad_norm": 6.7876105308532715,
      "learning_rate": 4.288474593973576e-05,
      "loss": 1.8117,
      "step": 109000
    },
    {
      "epoch": 8.546138179539401,
      "grad_norm": 4.662162780761719,
      "learning_rate": 4.2878218183717165e-05,
      "loss": 1.8657,
      "step": 109100
    },
    {
      "epoch": 8.55397148676171,
      "grad_norm": 4.938812255859375,
      "learning_rate": 4.287169042769857e-05,
      "loss": 1.8643,
      "step": 109200
    },
    {
      "epoch": 8.56180479398402,
      "grad_norm": 6.24184513092041,
      "learning_rate": 4.286516267167999e-05,
      "loss": 1.8739,
      "step": 109300
    },
    {
      "epoch": 8.56963810120633,
      "grad_norm": 5.543754577636719,
      "learning_rate": 4.2858634915661396e-05,
      "loss": 1.8303,
      "step": 109400
    },
    {
      "epoch": 8.57747140842864,
      "grad_norm": 6.559927463531494,
      "learning_rate": 4.28521071596428e-05,
      "loss": 1.8174,
      "step": 109500
    },
    {
      "epoch": 8.585304715650947,
      "grad_norm": 4.350579261779785,
      "learning_rate": 4.2845579403624214e-05,
      "loss": 1.9471,
      "step": 109600
    },
    {
      "epoch": 8.593138022873257,
      "grad_norm": 4.175530433654785,
      "learning_rate": 4.283905164760562e-05,
      "loss": 1.8749,
      "step": 109700
    },
    {
      "epoch": 8.600971330095566,
      "grad_norm": 5.62775993347168,
      "learning_rate": 4.2832523891587026e-05,
      "loss": 1.8037,
      "step": 109800
    },
    {
      "epoch": 8.608804637317876,
      "grad_norm": 5.300795555114746,
      "learning_rate": 4.282599613556844e-05,
      "loss": 1.815,
      "step": 109900
    },
    {
      "epoch": 8.616637944540186,
      "grad_norm": 4.02878999710083,
      "learning_rate": 4.281946837954985e-05,
      "loss": 1.9248,
      "step": 110000
    },
    {
      "epoch": 8.624471251762493,
      "grad_norm": 4.290795803070068,
      "learning_rate": 4.281294062353126e-05,
      "loss": 1.8536,
      "step": 110100
    },
    {
      "epoch": 8.632304558984803,
      "grad_norm": 4.287947654724121,
      "learning_rate": 4.280641286751267e-05,
      "loss": 1.8487,
      "step": 110200
    },
    {
      "epoch": 8.640137866207112,
      "grad_norm": 7.200594902038574,
      "learning_rate": 4.2799885111494075e-05,
      "loss": 1.8341,
      "step": 110300
    },
    {
      "epoch": 8.647971173429422,
      "grad_norm": 4.652311325073242,
      "learning_rate": 4.279335735547548e-05,
      "loss": 1.8701,
      "step": 110400
    },
    {
      "epoch": 8.655804480651732,
      "grad_norm": 6.051876068115234,
      "learning_rate": 4.278682959945689e-05,
      "loss": 1.8843,
      "step": 110500
    },
    {
      "epoch": 8.663637787874041,
      "grad_norm": 4.989205360412598,
      "learning_rate": 4.2780301843438306e-05,
      "loss": 1.8816,
      "step": 110600
    },
    {
      "epoch": 8.671471095096349,
      "grad_norm": 5.321943283081055,
      "learning_rate": 4.277377408741971e-05,
      "loss": 1.8279,
      "step": 110700
    },
    {
      "epoch": 8.679304402318659,
      "grad_norm": 3.482396364212036,
      "learning_rate": 4.276724633140112e-05,
      "loss": 1.8349,
      "step": 110800
    },
    {
      "epoch": 8.687137709540968,
      "grad_norm": 5.373305797576904,
      "learning_rate": 4.276071857538253e-05,
      "loss": 1.8916,
      "step": 110900
    },
    {
      "epoch": 8.694971016763278,
      "grad_norm": 7.245319843292236,
      "learning_rate": 4.2754190819363936e-05,
      "loss": 1.828,
      "step": 111000
    },
    {
      "epoch": 8.702804323985587,
      "grad_norm": 4.183032035827637,
      "learning_rate": 4.274766306334534e-05,
      "loss": 1.8904,
      "step": 111100
    },
    {
      "epoch": 8.710637631207897,
      "grad_norm": 5.132030963897705,
      "learning_rate": 4.2741135307326754e-05,
      "loss": 1.8679,
      "step": 111200
    },
    {
      "epoch": 8.718470938430205,
      "grad_norm": 4.956594944000244,
      "learning_rate": 4.2734607551308167e-05,
      "loss": 1.8412,
      "step": 111300
    },
    {
      "epoch": 8.726304245652514,
      "grad_norm": 6.643080234527588,
      "learning_rate": 4.272807979528957e-05,
      "loss": 1.8922,
      "step": 111400
    },
    {
      "epoch": 8.734137552874824,
      "grad_norm": 5.400099754333496,
      "learning_rate": 4.2721552039270985e-05,
      "loss": 1.8671,
      "step": 111500
    },
    {
      "epoch": 8.741970860097133,
      "grad_norm": 6.244022846221924,
      "learning_rate": 4.271502428325239e-05,
      "loss": 1.8031,
      "step": 111600
    },
    {
      "epoch": 8.749804167319443,
      "grad_norm": 5.36941385269165,
      "learning_rate": 4.2708496527233796e-05,
      "loss": 1.7755,
      "step": 111700
    },
    {
      "epoch": 8.757637474541752,
      "grad_norm": 5.435319900512695,
      "learning_rate": 4.270196877121521e-05,
      "loss": 1.8524,
      "step": 111800
    },
    {
      "epoch": 8.76547078176406,
      "grad_norm": 4.77193546295166,
      "learning_rate": 4.269544101519662e-05,
      "loss": 1.9681,
      "step": 111900
    },
    {
      "epoch": 8.77330408898637,
      "grad_norm": 5.129307746887207,
      "learning_rate": 4.268891325917803e-05,
      "loss": 1.8778,
      "step": 112000
    },
    {
      "epoch": 8.78113739620868,
      "grad_norm": 2.4777417182922363,
      "learning_rate": 4.268238550315943e-05,
      "loss": 1.8505,
      "step": 112100
    },
    {
      "epoch": 8.788970703430989,
      "grad_norm": 4.083472728729248,
      "learning_rate": 4.2675857747140846e-05,
      "loss": 1.8671,
      "step": 112200
    },
    {
      "epoch": 8.796804010653299,
      "grad_norm": 8.298789978027344,
      "learning_rate": 4.266932999112225e-05,
      "loss": 1.9206,
      "step": 112300
    },
    {
      "epoch": 8.804637317875606,
      "grad_norm": 4.7663421630859375,
      "learning_rate": 4.266280223510366e-05,
      "loss": 1.8926,
      "step": 112400
    },
    {
      "epoch": 8.812470625097916,
      "grad_norm": 3.937570333480835,
      "learning_rate": 4.2656274479085076e-05,
      "loss": 1.8654,
      "step": 112500
    },
    {
      "epoch": 8.820303932320225,
      "grad_norm": 6.055078983306885,
      "learning_rate": 4.264974672306648e-05,
      "loss": 1.9034,
      "step": 112600
    },
    {
      "epoch": 8.828137239542535,
      "grad_norm": 4.219819068908691,
      "learning_rate": 4.264321896704789e-05,
      "loss": 1.8265,
      "step": 112700
    },
    {
      "epoch": 8.835970546764845,
      "grad_norm": 6.997762203216553,
      "learning_rate": 4.26366912110293e-05,
      "loss": 1.9207,
      "step": 112800
    },
    {
      "epoch": 8.843803853987154,
      "grad_norm": 4.579605579376221,
      "learning_rate": 4.2630163455010706e-05,
      "loss": 1.8679,
      "step": 112900
    },
    {
      "epoch": 8.851637161209462,
      "grad_norm": 5.055825233459473,
      "learning_rate": 4.262363569899211e-05,
      "loss": 1.8432,
      "step": 113000
    },
    {
      "epoch": 8.859470468431772,
      "grad_norm": 5.2589921951293945,
      "learning_rate": 4.2617107942973525e-05,
      "loss": 1.8419,
      "step": 113100
    },
    {
      "epoch": 8.867303775654081,
      "grad_norm": 4.466538906097412,
      "learning_rate": 4.261058018695494e-05,
      "loss": 1.906,
      "step": 113200
    },
    {
      "epoch": 8.87513708287639,
      "grad_norm": 3.9973413944244385,
      "learning_rate": 4.260405243093634e-05,
      "loss": 1.7985,
      "step": 113300
    },
    {
      "epoch": 8.8829703900987,
      "grad_norm": 5.870123386383057,
      "learning_rate": 4.2597524674917755e-05,
      "loss": 1.7875,
      "step": 113400
    },
    {
      "epoch": 8.890803697321008,
      "grad_norm": 4.7968058586120605,
      "learning_rate": 4.259099691889916e-05,
      "loss": 1.8268,
      "step": 113500
    },
    {
      "epoch": 8.898637004543318,
      "grad_norm": 4.917143821716309,
      "learning_rate": 4.258446916288057e-05,
      "loss": 1.8804,
      "step": 113600
    },
    {
      "epoch": 8.906470311765627,
      "grad_norm": 4.923303127288818,
      "learning_rate": 4.257794140686198e-05,
      "loss": 1.9548,
      "step": 113700
    },
    {
      "epoch": 8.914303618987937,
      "grad_norm": 5.200554847717285,
      "learning_rate": 4.257141365084339e-05,
      "loss": 1.7484,
      "step": 113800
    },
    {
      "epoch": 8.922136926210246,
      "grad_norm": 5.805744171142578,
      "learning_rate": 4.25648858948248e-05,
      "loss": 1.8545,
      "step": 113900
    },
    {
      "epoch": 8.929970233432556,
      "grad_norm": 5.804266929626465,
      "learning_rate": 4.2558358138806204e-05,
      "loss": 1.9131,
      "step": 114000
    },
    {
      "epoch": 8.937803540654864,
      "grad_norm": 6.640910625457764,
      "learning_rate": 4.2551830382787616e-05,
      "loss": 1.8189,
      "step": 114100
    },
    {
      "epoch": 8.945636847877173,
      "grad_norm": 4.846467971801758,
      "learning_rate": 4.254530262676902e-05,
      "loss": 1.8715,
      "step": 114200
    },
    {
      "epoch": 8.953470155099483,
      "grad_norm": 3.7274386882781982,
      "learning_rate": 4.253877487075043e-05,
      "loss": 1.7727,
      "step": 114300
    },
    {
      "epoch": 8.961303462321792,
      "grad_norm": 6.105944633483887,
      "learning_rate": 4.253224711473184e-05,
      "loss": 1.8677,
      "step": 114400
    },
    {
      "epoch": 8.969136769544102,
      "grad_norm": 5.0612664222717285,
      "learning_rate": 4.252571935871325e-05,
      "loss": 1.988,
      "step": 114500
    },
    {
      "epoch": 8.976970076766412,
      "grad_norm": 3.727548837661743,
      "learning_rate": 4.251919160269466e-05,
      "loss": 1.8204,
      "step": 114600
    },
    {
      "epoch": 8.98480338398872,
      "grad_norm": 5.175848484039307,
      "learning_rate": 4.251266384667607e-05,
      "loss": 1.8656,
      "step": 114700
    },
    {
      "epoch": 8.992636691211029,
      "grad_norm": 3.7586917877197266,
      "learning_rate": 4.250613609065748e-05,
      "loss": 1.8817,
      "step": 114800
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.8444123268127441,
      "eval_runtime": 1.4184,
      "eval_samples_per_second": 473.781,
      "eval_steps_per_second": 473.781,
      "step": 114894
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.624441385269165,
      "eval_runtime": 27.6889,
      "eval_samples_per_second": 461.051,
      "eval_steps_per_second": 461.051,
      "step": 114894
    },
    {
      "epoch": 9.000469998433339,
      "grad_norm": 5.2214531898498535,
      "learning_rate": 4.249960833463888e-05,
      "loss": 1.9007,
      "step": 114900
    },
    {
      "epoch": 9.008303305655648,
      "grad_norm": 4.398146629333496,
      "learning_rate": 4.2493080578620295e-05,
      "loss": 1.8164,
      "step": 115000
    },
    {
      "epoch": 9.016136612877958,
      "grad_norm": 4.934291362762451,
      "learning_rate": 4.248655282260171e-05,
      "loss": 1.882,
      "step": 115100
    },
    {
      "epoch": 9.023969920100267,
      "grad_norm": 4.567049980163574,
      "learning_rate": 4.2480025066583114e-05,
      "loss": 1.7643,
      "step": 115200
    },
    {
      "epoch": 9.031803227322575,
      "grad_norm": 3.8193273544311523,
      "learning_rate": 4.2473497310564526e-05,
      "loss": 1.7777,
      "step": 115300
    },
    {
      "epoch": 9.039636534544885,
      "grad_norm": 5.091353893280029,
      "learning_rate": 4.246696955454593e-05,
      "loss": 1.8385,
      "step": 115400
    },
    {
      "epoch": 9.047469841767194,
      "grad_norm": 4.652123928070068,
      "learning_rate": 4.246044179852734e-05,
      "loss": 1.8849,
      "step": 115500
    },
    {
      "epoch": 9.055303148989504,
      "grad_norm": 5.575620174407959,
      "learning_rate": 4.245391404250874e-05,
      "loss": 1.7658,
      "step": 115600
    },
    {
      "epoch": 9.063136456211813,
      "grad_norm": 5.094728946685791,
      "learning_rate": 4.244738628649016e-05,
      "loss": 1.8184,
      "step": 115700
    },
    {
      "epoch": 9.070969763434121,
      "grad_norm": 5.398164749145508,
      "learning_rate": 4.244085853047157e-05,
      "loss": 1.8092,
      "step": 115800
    },
    {
      "epoch": 9.07880307065643,
      "grad_norm": 3.721482038497925,
      "learning_rate": 4.2434330774452974e-05,
      "loss": 1.7339,
      "step": 115900
    },
    {
      "epoch": 9.08663637787874,
      "grad_norm": 5.366099834442139,
      "learning_rate": 4.242780301843439e-05,
      "loss": 1.8055,
      "step": 116000
    },
    {
      "epoch": 9.09446968510105,
      "grad_norm": 6.641279697418213,
      "learning_rate": 4.242127526241579e-05,
      "loss": 1.8362,
      "step": 116100
    },
    {
      "epoch": 9.10230299232336,
      "grad_norm": 4.8624114990234375,
      "learning_rate": 4.24147475063972e-05,
      "loss": 1.8044,
      "step": 116200
    },
    {
      "epoch": 9.110136299545669,
      "grad_norm": 4.420355796813965,
      "learning_rate": 4.240821975037861e-05,
      "loss": 1.7355,
      "step": 116300
    },
    {
      "epoch": 9.117969606767977,
      "grad_norm": 3.8168303966522217,
      "learning_rate": 4.2401691994360023e-05,
      "loss": 1.8376,
      "step": 116400
    },
    {
      "epoch": 9.125802913990286,
      "grad_norm": 4.497209072113037,
      "learning_rate": 4.239516423834143e-05,
      "loss": 1.7395,
      "step": 116500
    },
    {
      "epoch": 9.133636221212596,
      "grad_norm": 5.052112579345703,
      "learning_rate": 4.238863648232284e-05,
      "loss": 1.7909,
      "step": 116600
    },
    {
      "epoch": 9.141469528434905,
      "grad_norm": 4.384313106536865,
      "learning_rate": 4.238210872630425e-05,
      "loss": 1.8306,
      "step": 116700
    },
    {
      "epoch": 9.149302835657215,
      "grad_norm": 5.787904739379883,
      "learning_rate": 4.237558097028565e-05,
      "loss": 1.7756,
      "step": 116800
    },
    {
      "epoch": 9.157136142879525,
      "grad_norm": 4.794641494750977,
      "learning_rate": 4.2369053214267066e-05,
      "loss": 1.7552,
      "step": 116900
    },
    {
      "epoch": 9.164969450101832,
      "grad_norm": 4.420494556427002,
      "learning_rate": 4.236252545824848e-05,
      "loss": 1.8496,
      "step": 117000
    },
    {
      "epoch": 9.172802757324142,
      "grad_norm": 4.016876697540283,
      "learning_rate": 4.2355997702229884e-05,
      "loss": 1.8497,
      "step": 117100
    },
    {
      "epoch": 9.180636064546452,
      "grad_norm": 5.543155670166016,
      "learning_rate": 4.234946994621129e-05,
      "loss": 1.8397,
      "step": 117200
    },
    {
      "epoch": 9.188469371768761,
      "grad_norm": 4.6086015701293945,
      "learning_rate": 4.23429421901927e-05,
      "loss": 1.8198,
      "step": 117300
    },
    {
      "epoch": 9.19630267899107,
      "grad_norm": 5.800994396209717,
      "learning_rate": 4.233641443417411e-05,
      "loss": 1.8735,
      "step": 117400
    },
    {
      "epoch": 9.204135986213378,
      "grad_norm": 5.780801773071289,
      "learning_rate": 4.2329886678155514e-05,
      "loss": 1.8468,
      "step": 117500
    },
    {
      "epoch": 9.211969293435688,
      "grad_norm": 4.961777687072754,
      "learning_rate": 4.2323358922136926e-05,
      "loss": 1.8403,
      "step": 117600
    },
    {
      "epoch": 9.219802600657998,
      "grad_norm": 4.718387126922607,
      "learning_rate": 4.231683116611834e-05,
      "loss": 1.8734,
      "step": 117700
    },
    {
      "epoch": 9.227635907880307,
      "grad_norm": 8.9718656539917,
      "learning_rate": 4.2310303410099745e-05,
      "loss": 1.8698,
      "step": 117800
    },
    {
      "epoch": 9.235469215102617,
      "grad_norm": 5.127786159515381,
      "learning_rate": 4.230377565408116e-05,
      "loss": 1.9011,
      "step": 117900
    },
    {
      "epoch": 9.243302522324926,
      "grad_norm": 3.869992971420288,
      "learning_rate": 4.229724789806256e-05,
      "loss": 1.8159,
      "step": 118000
    },
    {
      "epoch": 9.251135829547234,
      "grad_norm": 6.6330366134643555,
      "learning_rate": 4.229072014204397e-05,
      "loss": 1.7892,
      "step": 118100
    },
    {
      "epoch": 9.258969136769544,
      "grad_norm": 8.731983184814453,
      "learning_rate": 4.228419238602538e-05,
      "loss": 1.7474,
      "step": 118200
    },
    {
      "epoch": 9.266802443991853,
      "grad_norm": 4.04683780670166,
      "learning_rate": 4.2277664630006794e-05,
      "loss": 1.768,
      "step": 118300
    },
    {
      "epoch": 9.274635751214163,
      "grad_norm": 7.077352523803711,
      "learning_rate": 4.22711368739882e-05,
      "loss": 1.8428,
      "step": 118400
    },
    {
      "epoch": 9.282469058436472,
      "grad_norm": 4.793759346008301,
      "learning_rate": 4.226460911796961e-05,
      "loss": 1.845,
      "step": 118500
    },
    {
      "epoch": 9.290302365658782,
      "grad_norm": 4.951678276062012,
      "learning_rate": 4.225808136195102e-05,
      "loss": 1.8753,
      "step": 118600
    },
    {
      "epoch": 9.29813567288109,
      "grad_norm": 6.0950164794921875,
      "learning_rate": 4.2251553605932424e-05,
      "loss": 1.8059,
      "step": 118700
    },
    {
      "epoch": 9.3059689801034,
      "grad_norm": 5.920011043548584,
      "learning_rate": 4.224502584991383e-05,
      "loss": 1.9386,
      "step": 118800
    },
    {
      "epoch": 9.313802287325709,
      "grad_norm": 6.461268901824951,
      "learning_rate": 4.223849809389525e-05,
      "loss": 1.7389,
      "step": 118900
    },
    {
      "epoch": 9.321635594548018,
      "grad_norm": 6.266295433044434,
      "learning_rate": 4.2231970337876655e-05,
      "loss": 1.7544,
      "step": 119000
    },
    {
      "epoch": 9.329468901770328,
      "grad_norm": 5.948571681976318,
      "learning_rate": 4.222544258185806e-05,
      "loss": 1.862,
      "step": 119100
    },
    {
      "epoch": 9.337302208992636,
      "grad_norm": 4.5383172035217285,
      "learning_rate": 4.221891482583947e-05,
      "loss": 1.8516,
      "step": 119200
    },
    {
      "epoch": 9.345135516214945,
      "grad_norm": 5.92747163772583,
      "learning_rate": 4.221238706982088e-05,
      "loss": 1.8685,
      "step": 119300
    },
    {
      "epoch": 9.352968823437255,
      "grad_norm": 5.127749919891357,
      "learning_rate": 4.2205859313802285e-05,
      "loss": 1.764,
      "step": 119400
    },
    {
      "epoch": 9.360802130659565,
      "grad_norm": 4.420724868774414,
      "learning_rate": 4.21993315577837e-05,
      "loss": 1.8792,
      "step": 119500
    },
    {
      "epoch": 9.368635437881874,
      "grad_norm": 4.457455635070801,
      "learning_rate": 4.219280380176511e-05,
      "loss": 1.9708,
      "step": 119600
    },
    {
      "epoch": 9.376468745104184,
      "grad_norm": 6.21583890914917,
      "learning_rate": 4.2186276045746515e-05,
      "loss": 1.8174,
      "step": 119700
    },
    {
      "epoch": 9.384302052326492,
      "grad_norm": 4.394556999206543,
      "learning_rate": 4.217974828972793e-05,
      "loss": 1.7092,
      "step": 119800
    },
    {
      "epoch": 9.392135359548801,
      "grad_norm": 5.562828063964844,
      "learning_rate": 4.2173220533709334e-05,
      "loss": 1.801,
      "step": 119900
    },
    {
      "epoch": 9.39996866677111,
      "grad_norm": 5.573365688323975,
      "learning_rate": 4.216669277769074e-05,
      "loss": 1.7692,
      "step": 120000
    },
    {
      "epoch": 9.40780197399342,
      "grad_norm": 4.58308219909668,
      "learning_rate": 4.216016502167215e-05,
      "loss": 1.8912,
      "step": 120100
    },
    {
      "epoch": 9.41563528121573,
      "grad_norm": 5.246338844299316,
      "learning_rate": 4.2153637265653565e-05,
      "loss": 1.8913,
      "step": 120200
    },
    {
      "epoch": 9.42346858843804,
      "grad_norm": 7.556779384613037,
      "learning_rate": 4.214710950963497e-05,
      "loss": 1.7605,
      "step": 120300
    },
    {
      "epoch": 9.431301895660347,
      "grad_norm": 4.603209972381592,
      "learning_rate": 4.214058175361638e-05,
      "loss": 1.886,
      "step": 120400
    },
    {
      "epoch": 9.439135202882657,
      "grad_norm": 5.857675552368164,
      "learning_rate": 4.213405399759779e-05,
      "loss": 1.8079,
      "step": 120500
    },
    {
      "epoch": 9.446968510104966,
      "grad_norm": 5.350635528564453,
      "learning_rate": 4.2127526241579194e-05,
      "loss": 1.8146,
      "step": 120600
    },
    {
      "epoch": 9.454801817327276,
      "grad_norm": 5.048267364501953,
      "learning_rate": 4.21209984855606e-05,
      "loss": 1.9508,
      "step": 120700
    },
    {
      "epoch": 9.462635124549585,
      "grad_norm": 3.629134178161621,
      "learning_rate": 4.211447072954201e-05,
      "loss": 1.8196,
      "step": 120800
    },
    {
      "epoch": 9.470468431771893,
      "grad_norm": 4.243254661560059,
      "learning_rate": 4.2107942973523425e-05,
      "loss": 1.8717,
      "step": 120900
    },
    {
      "epoch": 9.478301738994203,
      "grad_norm": 4.468811511993408,
      "learning_rate": 4.210141521750483e-05,
      "loss": 1.8758,
      "step": 121000
    },
    {
      "epoch": 9.486135046216512,
      "grad_norm": 4.758539199829102,
      "learning_rate": 4.2094887461486244e-05,
      "loss": 1.9,
      "step": 121100
    },
    {
      "epoch": 9.493968353438822,
      "grad_norm": 4.1851115226745605,
      "learning_rate": 4.208835970546765e-05,
      "loss": 1.9063,
      "step": 121200
    },
    {
      "epoch": 9.501801660661132,
      "grad_norm": 5.849780559539795,
      "learning_rate": 4.2081831949449055e-05,
      "loss": 1.8112,
      "step": 121300
    },
    {
      "epoch": 9.509634967883441,
      "grad_norm": 4.7482380867004395,
      "learning_rate": 4.207530419343047e-05,
      "loss": 1.9289,
      "step": 121400
    },
    {
      "epoch": 9.517468275105749,
      "grad_norm": 5.662220001220703,
      "learning_rate": 4.206877643741188e-05,
      "loss": 1.8237,
      "step": 121500
    },
    {
      "epoch": 9.525301582328058,
      "grad_norm": 5.508352756500244,
      "learning_rate": 4.2062248681393286e-05,
      "loss": 1.7841,
      "step": 121600
    },
    {
      "epoch": 9.533134889550368,
      "grad_norm": 4.872033596038818,
      "learning_rate": 4.20557209253747e-05,
      "loss": 1.861,
      "step": 121700
    },
    {
      "epoch": 9.540968196772678,
      "grad_norm": 5.268369674682617,
      "learning_rate": 4.2049193169356104e-05,
      "loss": 1.8657,
      "step": 121800
    },
    {
      "epoch": 9.548801503994987,
      "grad_norm": 6.42230224609375,
      "learning_rate": 4.204266541333751e-05,
      "loss": 1.8683,
      "step": 121900
    },
    {
      "epoch": 9.556634811217297,
      "grad_norm": 5.267703533172607,
      "learning_rate": 4.203613765731892e-05,
      "loss": 1.8884,
      "step": 122000
    },
    {
      "epoch": 9.564468118439605,
      "grad_norm": 6.76984977722168,
      "learning_rate": 4.2029609901300335e-05,
      "loss": 1.9702,
      "step": 122100
    },
    {
      "epoch": 9.572301425661914,
      "grad_norm": 7.5139851570129395,
      "learning_rate": 4.202308214528174e-05,
      "loss": 1.8572,
      "step": 122200
    },
    {
      "epoch": 9.580134732884224,
      "grad_norm": 4.415567874908447,
      "learning_rate": 4.201655438926315e-05,
      "loss": 1.8164,
      "step": 122300
    },
    {
      "epoch": 9.587968040106533,
      "grad_norm": 5.634459972381592,
      "learning_rate": 4.201002663324456e-05,
      "loss": 1.8404,
      "step": 122400
    },
    {
      "epoch": 9.595801347328843,
      "grad_norm": 4.407571792602539,
      "learning_rate": 4.2003498877225965e-05,
      "loss": 1.8024,
      "step": 122500
    },
    {
      "epoch": 9.60363465455115,
      "grad_norm": 7.915002346038818,
      "learning_rate": 4.199697112120737e-05,
      "loss": 1.8418,
      "step": 122600
    },
    {
      "epoch": 9.61146796177346,
      "grad_norm": 5.313738822937012,
      "learning_rate": 4.199044336518878e-05,
      "loss": 1.8931,
      "step": 122700
    },
    {
      "epoch": 9.61930126899577,
      "grad_norm": 4.245553016662598,
      "learning_rate": 4.1983915609170196e-05,
      "loss": 1.7789,
      "step": 122800
    },
    {
      "epoch": 9.62713457621808,
      "grad_norm": 5.222876071929932,
      "learning_rate": 4.19773878531516e-05,
      "loss": 1.8132,
      "step": 122900
    },
    {
      "epoch": 9.634967883440389,
      "grad_norm": 4.196323871612549,
      "learning_rate": 4.1970860097133014e-05,
      "loss": 1.9617,
      "step": 123000
    },
    {
      "epoch": 9.642801190662698,
      "grad_norm": 5.9204230308532715,
      "learning_rate": 4.196433234111442e-05,
      "loss": 1.9682,
      "step": 123100
    },
    {
      "epoch": 9.650634497885006,
      "grad_norm": 5.641167163848877,
      "learning_rate": 4.1957804585095826e-05,
      "loss": 1.8764,
      "step": 123200
    },
    {
      "epoch": 9.658467805107316,
      "grad_norm": 4.699087619781494,
      "learning_rate": 4.195127682907724e-05,
      "loss": 1.8312,
      "step": 123300
    },
    {
      "epoch": 9.666301112329625,
      "grad_norm": 5.640275001525879,
      "learning_rate": 4.194474907305865e-05,
      "loss": 1.8761,
      "step": 123400
    },
    {
      "epoch": 9.674134419551935,
      "grad_norm": 5.064510345458984,
      "learning_rate": 4.1938221317040057e-05,
      "loss": 1.7832,
      "step": 123500
    },
    {
      "epoch": 9.681967726774245,
      "grad_norm": 6.452486038208008,
      "learning_rate": 4.193169356102147e-05,
      "loss": 1.7533,
      "step": 123600
    },
    {
      "epoch": 9.689801033996554,
      "grad_norm": 4.861899375915527,
      "learning_rate": 4.1925165805002875e-05,
      "loss": 1.8888,
      "step": 123700
    },
    {
      "epoch": 9.697634341218862,
      "grad_norm": 5.119953155517578,
      "learning_rate": 4.191863804898428e-05,
      "loss": 1.7905,
      "step": 123800
    },
    {
      "epoch": 9.705467648441171,
      "grad_norm": 6.686440467834473,
      "learning_rate": 4.1912110292965686e-05,
      "loss": 1.8554,
      "step": 123900
    },
    {
      "epoch": 9.713300955663481,
      "grad_norm": 5.002430438995361,
      "learning_rate": 4.19055825369471e-05,
      "loss": 1.8106,
      "step": 124000
    },
    {
      "epoch": 9.72113426288579,
      "grad_norm": 4.427046298980713,
      "learning_rate": 4.189905478092851e-05,
      "loss": 1.8035,
      "step": 124100
    },
    {
      "epoch": 9.7289675701081,
      "grad_norm": 6.025758266448975,
      "learning_rate": 4.189252702490992e-05,
      "loss": 1.85,
      "step": 124200
    },
    {
      "epoch": 9.73680087733041,
      "grad_norm": 4.636564254760742,
      "learning_rate": 4.188599926889133e-05,
      "loss": 1.8641,
      "step": 124300
    },
    {
      "epoch": 9.744634184552718,
      "grad_norm": 5.298236846923828,
      "learning_rate": 4.1879471512872736e-05,
      "loss": 1.8192,
      "step": 124400
    },
    {
      "epoch": 9.752467491775027,
      "grad_norm": 3.8266050815582275,
      "learning_rate": 4.187294375685414e-05,
      "loss": 1.7969,
      "step": 124500
    },
    {
      "epoch": 9.760300798997337,
      "grad_norm": 4.641727447509766,
      "learning_rate": 4.1866416000835554e-05,
      "loss": 1.7963,
      "step": 124600
    },
    {
      "epoch": 9.768134106219646,
      "grad_norm": 5.348908424377441,
      "learning_rate": 4.1859888244816966e-05,
      "loss": 1.8138,
      "step": 124700
    },
    {
      "epoch": 9.775967413441956,
      "grad_norm": 4.851718425750732,
      "learning_rate": 4.185336048879837e-05,
      "loss": 1.8047,
      "step": 124800
    },
    {
      "epoch": 9.783800720664264,
      "grad_norm": 7.169709205627441,
      "learning_rate": 4.1846832732779785e-05,
      "loss": 1.8632,
      "step": 124900
    },
    {
      "epoch": 9.791634027886573,
      "grad_norm": 3.469620943069458,
      "learning_rate": 4.184030497676119e-05,
      "loss": 1.7823,
      "step": 125000
    },
    {
      "epoch": 9.799467335108883,
      "grad_norm": 3.702895164489746,
      "learning_rate": 4.1833777220742596e-05,
      "loss": 1.8354,
      "step": 125100
    },
    {
      "epoch": 9.807300642331192,
      "grad_norm": 6.285669326782227,
      "learning_rate": 4.182724946472401e-05,
      "loss": 1.8453,
      "step": 125200
    },
    {
      "epoch": 9.815133949553502,
      "grad_norm": 5.0523247718811035,
      "learning_rate": 4.182072170870542e-05,
      "loss": 1.848,
      "step": 125300
    },
    {
      "epoch": 9.822967256775812,
      "grad_norm": 5.000265598297119,
      "learning_rate": 4.181419395268683e-05,
      "loss": 1.8256,
      "step": 125400
    },
    {
      "epoch": 9.83080056399812,
      "grad_norm": 4.963984489440918,
      "learning_rate": 4.180766619666823e-05,
      "loss": 1.8265,
      "step": 125500
    },
    {
      "epoch": 9.838633871220429,
      "grad_norm": 5.138875484466553,
      "learning_rate": 4.1801138440649645e-05,
      "loss": 1.8448,
      "step": 125600
    },
    {
      "epoch": 9.846467178442738,
      "grad_norm": 7.430324077606201,
      "learning_rate": 4.179461068463105e-05,
      "loss": 1.8316,
      "step": 125700
    },
    {
      "epoch": 9.854300485665048,
      "grad_norm": 5.509220600128174,
      "learning_rate": 4.178808292861246e-05,
      "loss": 1.8708,
      "step": 125800
    },
    {
      "epoch": 9.862133792887358,
      "grad_norm": 4.1423468589782715,
      "learning_rate": 4.178155517259387e-05,
      "loss": 1.9515,
      "step": 125900
    },
    {
      "epoch": 9.869967100109665,
      "grad_norm": 4.366488456726074,
      "learning_rate": 4.177502741657528e-05,
      "loss": 1.8961,
      "step": 126000
    },
    {
      "epoch": 9.877800407331975,
      "grad_norm": 6.915213108062744,
      "learning_rate": 4.176849966055669e-05,
      "loss": 1.8391,
      "step": 126100
    },
    {
      "epoch": 9.885633714554285,
      "grad_norm": 5.490963935852051,
      "learning_rate": 4.17619719045381e-05,
      "loss": 1.8502,
      "step": 126200
    },
    {
      "epoch": 9.893467021776594,
      "grad_norm": 4.212085247039795,
      "learning_rate": 4.1755444148519506e-05,
      "loss": 1.8795,
      "step": 126300
    },
    {
      "epoch": 9.901300328998904,
      "grad_norm": 4.1461710929870605,
      "learning_rate": 4.174891639250091e-05,
      "loss": 1.7414,
      "step": 126400
    },
    {
      "epoch": 9.909133636221213,
      "grad_norm": 5.238288879394531,
      "learning_rate": 4.1742388636482324e-05,
      "loss": 1.8924,
      "step": 126500
    },
    {
      "epoch": 9.916966943443521,
      "grad_norm": 6.263756275177002,
      "learning_rate": 4.173586088046374e-05,
      "loss": 1.8514,
      "step": 126600
    },
    {
      "epoch": 9.92480025066583,
      "grad_norm": 6.07481050491333,
      "learning_rate": 4.172933312444514e-05,
      "loss": 1.8031,
      "step": 126700
    },
    {
      "epoch": 9.93263355788814,
      "grad_norm": 7.825135231018066,
      "learning_rate": 4.1722805368426555e-05,
      "loss": 1.7747,
      "step": 126800
    },
    {
      "epoch": 9.94046686511045,
      "grad_norm": 5.424579620361328,
      "learning_rate": 4.171627761240796e-05,
      "loss": 1.8472,
      "step": 126900
    },
    {
      "epoch": 9.94830017233276,
      "grad_norm": 5.354381084442139,
      "learning_rate": 4.170974985638937e-05,
      "loss": 1.8125,
      "step": 127000
    },
    {
      "epoch": 9.956133479555069,
      "grad_norm": 6.476437091827393,
      "learning_rate": 4.170322210037078e-05,
      "loss": 1.8648,
      "step": 127100
    },
    {
      "epoch": 9.963966786777377,
      "grad_norm": 5.337865829467773,
      "learning_rate": 4.1696694344352185e-05,
      "loss": 1.8413,
      "step": 127200
    },
    {
      "epoch": 9.971800093999686,
      "grad_norm": 4.6442341804504395,
      "learning_rate": 4.16901665883336e-05,
      "loss": 1.8774,
      "step": 127300
    },
    {
      "epoch": 9.979633401221996,
      "grad_norm": 5.602654933929443,
      "learning_rate": 4.1683638832315004e-05,
      "loss": 1.7578,
      "step": 127400
    },
    {
      "epoch": 9.987466708444305,
      "grad_norm": 5.764124393463135,
      "learning_rate": 4.1677111076296416e-05,
      "loss": 1.8169,
      "step": 127500
    },
    {
      "epoch": 9.995300015666615,
      "grad_norm": 5.524801254272461,
      "learning_rate": 4.167058332027782e-05,
      "loss": 1.9211,
      "step": 127600
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.8385710716247559,
      "eval_runtime": 1.4305,
      "eval_samples_per_second": 469.756,
      "eval_steps_per_second": 469.756,
      "step": 127660
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.6072973012924194,
      "eval_runtime": 27.631,
      "eval_samples_per_second": 462.018,
      "eval_steps_per_second": 462.018,
      "step": 127660
    }
  ],
  "logging_steps": 100,
  "max_steps": 765960,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 78997070131200.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
