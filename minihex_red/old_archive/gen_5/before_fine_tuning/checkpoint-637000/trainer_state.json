{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 637000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0015698587127158557,
      "grad_norm": 6.659707546234131,
      "learning_rate": 4.999901883830455e-05,
      "loss": 1.9381,
      "step": 100
    },
    {
      "epoch": 0.0031397174254317113,
      "grad_norm": 6.4614338874816895,
      "learning_rate": 4.9998037676609104e-05,
      "loss": 1.6319,
      "step": 200
    },
    {
      "epoch": 0.004709576138147566,
      "grad_norm": 5.07819128036499,
      "learning_rate": 4.999705651491366e-05,
      "loss": 1.4807,
      "step": 300
    },
    {
      "epoch": 0.006279434850863423,
      "grad_norm": 6.320569038391113,
      "learning_rate": 4.999607535321821e-05,
      "loss": 1.4656,
      "step": 400
    },
    {
      "epoch": 0.007849293563579277,
      "grad_norm": 6.3309149742126465,
      "learning_rate": 4.999509419152277e-05,
      "loss": 1.3774,
      "step": 500
    },
    {
      "epoch": 0.009419152276295133,
      "grad_norm": 7.334023952484131,
      "learning_rate": 4.9994113029827315e-05,
      "loss": 1.3172,
      "step": 600
    },
    {
      "epoch": 0.01098901098901099,
      "grad_norm": 6.471298694610596,
      "learning_rate": 4.999313186813187e-05,
      "loss": 1.3362,
      "step": 700
    },
    {
      "epoch": 0.012558869701726845,
      "grad_norm": 5.702239990234375,
      "learning_rate": 4.9992150706436424e-05,
      "loss": 1.2417,
      "step": 800
    },
    {
      "epoch": 0.0141287284144427,
      "grad_norm": 5.828788757324219,
      "learning_rate": 4.9991169544740975e-05,
      "loss": 1.1856,
      "step": 900
    },
    {
      "epoch": 0.015698587127158554,
      "grad_norm": 5.186532974243164,
      "learning_rate": 4.9990188383045526e-05,
      "loss": 1.2562,
      "step": 1000
    },
    {
      "epoch": 0.01726844583987441,
      "grad_norm": 6.159421443939209,
      "learning_rate": 4.9989207221350084e-05,
      "loss": 1.2175,
      "step": 1100
    },
    {
      "epoch": 0.018838304552590265,
      "grad_norm": 6.110106468200684,
      "learning_rate": 4.9988226059654635e-05,
      "loss": 1.2268,
      "step": 1200
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 5.403290748596191,
      "learning_rate": 4.9987244897959185e-05,
      "loss": 1.1708,
      "step": 1300
    },
    {
      "epoch": 0.02197802197802198,
      "grad_norm": 4.6892194747924805,
      "learning_rate": 4.9986263736263736e-05,
      "loss": 1.1739,
      "step": 1400
    },
    {
      "epoch": 0.023547880690737835,
      "grad_norm": 4.931613922119141,
      "learning_rate": 4.9985282574568294e-05,
      "loss": 1.1056,
      "step": 1500
    },
    {
      "epoch": 0.02511773940345369,
      "grad_norm": 5.904727935791016,
      "learning_rate": 4.998430141287284e-05,
      "loss": 1.1675,
      "step": 1600
    },
    {
      "epoch": 0.026687598116169546,
      "grad_norm": 5.232728958129883,
      "learning_rate": 4.9983320251177396e-05,
      "loss": 1.1422,
      "step": 1700
    },
    {
      "epoch": 0.0282574568288854,
      "grad_norm": 6.030940532684326,
      "learning_rate": 4.998233908948195e-05,
      "loss": 1.1935,
      "step": 1800
    },
    {
      "epoch": 0.029827315541601257,
      "grad_norm": 5.702332496643066,
      "learning_rate": 4.9981357927786505e-05,
      "loss": 1.1456,
      "step": 1900
    },
    {
      "epoch": 0.03139717425431711,
      "grad_norm": 4.545316696166992,
      "learning_rate": 4.9980376766091056e-05,
      "loss": 1.1447,
      "step": 2000
    },
    {
      "epoch": 0.03296703296703297,
      "grad_norm": 5.262295722961426,
      "learning_rate": 4.997939560439561e-05,
      "loss": 1.1266,
      "step": 2100
    },
    {
      "epoch": 0.03453689167974882,
      "grad_norm": 5.472929000854492,
      "learning_rate": 4.997841444270016e-05,
      "loss": 1.0835,
      "step": 2200
    },
    {
      "epoch": 0.03610675039246468,
      "grad_norm": 5.505349636077881,
      "learning_rate": 4.997743328100471e-05,
      "loss": 1.1042,
      "step": 2300
    },
    {
      "epoch": 0.03767660910518053,
      "grad_norm": 4.782943248748779,
      "learning_rate": 4.9976452119309267e-05,
      "loss": 1.1291,
      "step": 2400
    },
    {
      "epoch": 0.03924646781789639,
      "grad_norm": 5.045526027679443,
      "learning_rate": 4.997547095761382e-05,
      "loss": 1.0765,
      "step": 2500
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 6.91735315322876,
      "learning_rate": 4.9974489795918375e-05,
      "loss": 1.0742,
      "step": 2600
    },
    {
      "epoch": 0.0423861852433281,
      "grad_norm": 4.927852630615234,
      "learning_rate": 4.997350863422292e-05,
      "loss": 1.1132,
      "step": 2700
    },
    {
      "epoch": 0.04395604395604396,
      "grad_norm": 5.663850784301758,
      "learning_rate": 4.997252747252748e-05,
      "loss": 1.1057,
      "step": 2800
    },
    {
      "epoch": 0.04552590266875981,
      "grad_norm": 6.098921298980713,
      "learning_rate": 4.997154631083203e-05,
      "loss": 1.1578,
      "step": 2900
    },
    {
      "epoch": 0.04709576138147567,
      "grad_norm": 5.214883804321289,
      "learning_rate": 4.997056514913658e-05,
      "loss": 1.06,
      "step": 3000
    },
    {
      "epoch": 0.04866562009419152,
      "grad_norm": 4.702010631561279,
      "learning_rate": 4.996958398744113e-05,
      "loss": 1.0542,
      "step": 3100
    },
    {
      "epoch": 0.05023547880690738,
      "grad_norm": 3.4948205947875977,
      "learning_rate": 4.996860282574569e-05,
      "loss": 1.0913,
      "step": 3200
    },
    {
      "epoch": 0.05180533751962323,
      "grad_norm": 5.717295169830322,
      "learning_rate": 4.996762166405024e-05,
      "loss": 1.1105,
      "step": 3300
    },
    {
      "epoch": 0.05337519623233909,
      "grad_norm": 4.8234429359436035,
      "learning_rate": 4.996664050235479e-05,
      "loss": 1.0403,
      "step": 3400
    },
    {
      "epoch": 0.054945054945054944,
      "grad_norm": 5.3488688468933105,
      "learning_rate": 4.996565934065934e-05,
      "loss": 1.0385,
      "step": 3500
    },
    {
      "epoch": 0.0565149136577708,
      "grad_norm": 4.841887950897217,
      "learning_rate": 4.99646781789639e-05,
      "loss": 1.0418,
      "step": 3600
    },
    {
      "epoch": 0.058084772370486655,
      "grad_norm": 4.999855041503906,
      "learning_rate": 4.996369701726844e-05,
      "loss": 1.0841,
      "step": 3700
    },
    {
      "epoch": 0.059654631083202514,
      "grad_norm": 5.906175136566162,
      "learning_rate": 4.9962715855573e-05,
      "loss": 1.0242,
      "step": 3800
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 3.356689214706421,
      "learning_rate": 4.996173469387755e-05,
      "loss": 1.0333,
      "step": 3900
    },
    {
      "epoch": 0.06279434850863422,
      "grad_norm": 4.593007564544678,
      "learning_rate": 4.996075353218211e-05,
      "loss": 1.0238,
      "step": 4000
    },
    {
      "epoch": 0.06436420722135008,
      "grad_norm": 4.568149566650391,
      "learning_rate": 4.995977237048666e-05,
      "loss": 1.0762,
      "step": 4100
    },
    {
      "epoch": 0.06593406593406594,
      "grad_norm": 5.709305763244629,
      "learning_rate": 4.995879120879121e-05,
      "loss": 1.0057,
      "step": 4200
    },
    {
      "epoch": 0.06750392464678179,
      "grad_norm": 6.071099281311035,
      "learning_rate": 4.995781004709576e-05,
      "loss": 0.9806,
      "step": 4300
    },
    {
      "epoch": 0.06907378335949764,
      "grad_norm": 5.7528862953186035,
      "learning_rate": 4.995682888540031e-05,
      "loss": 1.0852,
      "step": 4400
    },
    {
      "epoch": 0.0706436420722135,
      "grad_norm": 5.44267463684082,
      "learning_rate": 4.995584772370487e-05,
      "loss": 1.0103,
      "step": 4500
    },
    {
      "epoch": 0.07221350078492936,
      "grad_norm": 6.142986297607422,
      "learning_rate": 4.995486656200942e-05,
      "loss": 1.0668,
      "step": 4600
    },
    {
      "epoch": 0.07378335949764521,
      "grad_norm": 6.12429141998291,
      "learning_rate": 4.995388540031398e-05,
      "loss": 0.9911,
      "step": 4700
    },
    {
      "epoch": 0.07535321821036106,
      "grad_norm": 4.579387187957764,
      "learning_rate": 4.9952904238618524e-05,
      "loss": 1.0569,
      "step": 4800
    },
    {
      "epoch": 0.07692307692307693,
      "grad_norm": 4.637796878814697,
      "learning_rate": 4.995192307692308e-05,
      "loss": 1.0476,
      "step": 4900
    },
    {
      "epoch": 0.07849293563579278,
      "grad_norm": 5.085638999938965,
      "learning_rate": 4.995094191522763e-05,
      "loss": 1.0266,
      "step": 5000
    },
    {
      "epoch": 0.08006279434850863,
      "grad_norm": 4.370875358581543,
      "learning_rate": 4.9949960753532184e-05,
      "loss": 1.008,
      "step": 5100
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 5.739015102386475,
      "learning_rate": 4.9948979591836735e-05,
      "loss": 1.0262,
      "step": 5200
    },
    {
      "epoch": 0.08320251177394035,
      "grad_norm": 5.2862725257873535,
      "learning_rate": 4.994799843014129e-05,
      "loss": 1.028,
      "step": 5300
    },
    {
      "epoch": 0.0847723704866562,
      "grad_norm": 5.274377822875977,
      "learning_rate": 4.9947017268445843e-05,
      "loss": 1.0119,
      "step": 5400
    },
    {
      "epoch": 0.08634222919937205,
      "grad_norm": 6.166141510009766,
      "learning_rate": 4.9946036106750394e-05,
      "loss": 1.0437,
      "step": 5500
    },
    {
      "epoch": 0.08791208791208792,
      "grad_norm": 4.3717241287231445,
      "learning_rate": 4.9945054945054945e-05,
      "loss": 1.0207,
      "step": 5600
    },
    {
      "epoch": 0.08948194662480377,
      "grad_norm": 5.4477314949035645,
      "learning_rate": 4.99440737833595e-05,
      "loss": 1.0515,
      "step": 5700
    },
    {
      "epoch": 0.09105180533751962,
      "grad_norm": 4.570425987243652,
      "learning_rate": 4.994309262166405e-05,
      "loss": 0.9744,
      "step": 5800
    },
    {
      "epoch": 0.09262166405023547,
      "grad_norm": 4.4673075675964355,
      "learning_rate": 4.9942111459968605e-05,
      "loss": 1.0611,
      "step": 5900
    },
    {
      "epoch": 0.09419152276295134,
      "grad_norm": 5.835231304168701,
      "learning_rate": 4.9941130298273156e-05,
      "loss": 1.0347,
      "step": 6000
    },
    {
      "epoch": 0.09576138147566719,
      "grad_norm": 5.0285210609436035,
      "learning_rate": 4.9940149136577714e-05,
      "loss": 0.9548,
      "step": 6100
    },
    {
      "epoch": 0.09733124018838304,
      "grad_norm": 5.09715461730957,
      "learning_rate": 4.9939167974882265e-05,
      "loss": 0.999,
      "step": 6200
    },
    {
      "epoch": 0.0989010989010989,
      "grad_norm": 3.70473051071167,
      "learning_rate": 4.9938186813186816e-05,
      "loss": 0.9927,
      "step": 6300
    },
    {
      "epoch": 0.10047095761381476,
      "grad_norm": 4.671661853790283,
      "learning_rate": 4.993720565149137e-05,
      "loss": 0.9879,
      "step": 6400
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 4.0113959312438965,
      "learning_rate": 4.993622448979592e-05,
      "loss": 0.951,
      "step": 6500
    },
    {
      "epoch": 0.10361067503924647,
      "grad_norm": 5.585811614990234,
      "learning_rate": 4.9935243328100476e-05,
      "loss": 0.9578,
      "step": 6600
    },
    {
      "epoch": 0.10518053375196232,
      "grad_norm": 5.055874824523926,
      "learning_rate": 4.9934262166405027e-05,
      "loss": 0.9759,
      "step": 6700
    },
    {
      "epoch": 0.10675039246467818,
      "grad_norm": 5.564075469970703,
      "learning_rate": 4.9933281004709584e-05,
      "loss": 1.0407,
      "step": 6800
    },
    {
      "epoch": 0.10832025117739404,
      "grad_norm": 6.267728328704834,
      "learning_rate": 4.993229984301413e-05,
      "loss": 0.9686,
      "step": 6900
    },
    {
      "epoch": 0.10989010989010989,
      "grad_norm": 4.540794849395752,
      "learning_rate": 4.9931318681318686e-05,
      "loss": 0.9805,
      "step": 7000
    },
    {
      "epoch": 0.11145996860282574,
      "grad_norm": 4.743518352508545,
      "learning_rate": 4.993033751962324e-05,
      "loss": 0.9225,
      "step": 7100
    },
    {
      "epoch": 0.1130298273155416,
      "grad_norm": 6.597959041595459,
      "learning_rate": 4.992935635792779e-05,
      "loss": 0.9447,
      "step": 7200
    },
    {
      "epoch": 0.11459968602825746,
      "grad_norm": 5.192346572875977,
      "learning_rate": 4.992837519623234e-05,
      "loss": 0.9867,
      "step": 7300
    },
    {
      "epoch": 0.11616954474097331,
      "grad_norm": 4.73753547668457,
      "learning_rate": 4.99273940345369e-05,
      "loss": 0.9568,
      "step": 7400
    },
    {
      "epoch": 0.11773940345368916,
      "grad_norm": 4.9867777824401855,
      "learning_rate": 4.992641287284145e-05,
      "loss": 0.9803,
      "step": 7500
    },
    {
      "epoch": 0.11930926216640503,
      "grad_norm": 4.228918075561523,
      "learning_rate": 4.9925431711146e-05,
      "loss": 0.9967,
      "step": 7600
    },
    {
      "epoch": 0.12087912087912088,
      "grad_norm": 4.723293304443359,
      "learning_rate": 4.992445054945055e-05,
      "loss": 1.0411,
      "step": 7700
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 4.168940544128418,
      "learning_rate": 4.992346938775511e-05,
      "loss": 0.9997,
      "step": 7800
    },
    {
      "epoch": 0.12401883830455258,
      "grad_norm": 3.8881618976593018,
      "learning_rate": 4.992248822605965e-05,
      "loss": 1.0093,
      "step": 7900
    },
    {
      "epoch": 0.12558869701726844,
      "grad_norm": 4.23741340637207,
      "learning_rate": 4.992150706436421e-05,
      "loss": 0.9958,
      "step": 8000
    },
    {
      "epoch": 0.1271585557299843,
      "grad_norm": 4.415288925170898,
      "learning_rate": 4.992052590266876e-05,
      "loss": 0.9785,
      "step": 8100
    },
    {
      "epoch": 0.12872841444270017,
      "grad_norm": 5.500281810760498,
      "learning_rate": 4.991954474097332e-05,
      "loss": 0.9796,
      "step": 8200
    },
    {
      "epoch": 0.130298273155416,
      "grad_norm": 4.616672515869141,
      "learning_rate": 4.991856357927787e-05,
      "loss": 0.9766,
      "step": 8300
    },
    {
      "epoch": 0.13186813186813187,
      "grad_norm": 4.883312702178955,
      "learning_rate": 4.991758241758242e-05,
      "loss": 0.9655,
      "step": 8400
    },
    {
      "epoch": 0.13343799058084774,
      "grad_norm": 4.402161121368408,
      "learning_rate": 4.991660125588697e-05,
      "loss": 0.948,
      "step": 8500
    },
    {
      "epoch": 0.13500784929356358,
      "grad_norm": 4.585582733154297,
      "learning_rate": 4.991562009419152e-05,
      "loss": 0.9749,
      "step": 8600
    },
    {
      "epoch": 0.13657770800627944,
      "grad_norm": 5.074254512786865,
      "learning_rate": 4.991463893249608e-05,
      "loss": 0.9577,
      "step": 8700
    },
    {
      "epoch": 0.13814756671899528,
      "grad_norm": 4.685644149780273,
      "learning_rate": 4.991365777080063e-05,
      "loss": 0.9308,
      "step": 8800
    },
    {
      "epoch": 0.13971742543171115,
      "grad_norm": 5.164050579071045,
      "learning_rate": 4.991267660910519e-05,
      "loss": 0.9557,
      "step": 8900
    },
    {
      "epoch": 0.141287284144427,
      "grad_norm": 4.969560146331787,
      "learning_rate": 4.991169544740973e-05,
      "loss": 0.9281,
      "step": 9000
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 3.916684627532959,
      "learning_rate": 4.991071428571429e-05,
      "loss": 0.952,
      "step": 9100
    },
    {
      "epoch": 0.14442700156985872,
      "grad_norm": 5.325727462768555,
      "learning_rate": 4.990973312401884e-05,
      "loss": 1.0091,
      "step": 9200
    },
    {
      "epoch": 0.14599686028257458,
      "grad_norm": 4.465225696563721,
      "learning_rate": 4.990875196232339e-05,
      "loss": 1.0047,
      "step": 9300
    },
    {
      "epoch": 0.14756671899529042,
      "grad_norm": 5.328545570373535,
      "learning_rate": 4.9907770800627944e-05,
      "loss": 0.9548,
      "step": 9400
    },
    {
      "epoch": 0.14913657770800628,
      "grad_norm": 3.948073625564575,
      "learning_rate": 4.99067896389325e-05,
      "loss": 1.015,
      "step": 9500
    },
    {
      "epoch": 0.15070643642072212,
      "grad_norm": 4.98942756652832,
      "learning_rate": 4.990580847723705e-05,
      "loss": 0.9217,
      "step": 9600
    },
    {
      "epoch": 0.152276295133438,
      "grad_norm": 5.661880970001221,
      "learning_rate": 4.9904827315541603e-05,
      "loss": 0.9852,
      "step": 9700
    },
    {
      "epoch": 0.15384615384615385,
      "grad_norm": 3.887681722640991,
      "learning_rate": 4.9903846153846154e-05,
      "loss": 0.9743,
      "step": 9800
    },
    {
      "epoch": 0.1554160125588697,
      "grad_norm": 4.744401931762695,
      "learning_rate": 4.990286499215071e-05,
      "loss": 1.0173,
      "step": 9900
    },
    {
      "epoch": 0.15698587127158556,
      "grad_norm": 2.733383893966675,
      "learning_rate": 4.9901883830455256e-05,
      "loss": 0.9295,
      "step": 10000
    },
    {
      "epoch": 0.15855572998430142,
      "grad_norm": 4.4868083000183105,
      "learning_rate": 4.9900902668759814e-05,
      "loss": 0.9321,
      "step": 10100
    },
    {
      "epoch": 0.16012558869701726,
      "grad_norm": 4.484259128570557,
      "learning_rate": 4.9899921507064365e-05,
      "loss": 0.9287,
      "step": 10200
    },
    {
      "epoch": 0.16169544740973313,
      "grad_norm": 3.9875502586364746,
      "learning_rate": 4.989894034536892e-05,
      "loss": 0.9143,
      "step": 10300
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 4.977200031280518,
      "learning_rate": 4.9897959183673474e-05,
      "loss": 0.9312,
      "step": 10400
    },
    {
      "epoch": 0.16483516483516483,
      "grad_norm": 6.375011444091797,
      "learning_rate": 4.9896978021978025e-05,
      "loss": 0.8746,
      "step": 10500
    },
    {
      "epoch": 0.1664050235478807,
      "grad_norm": 5.114082336425781,
      "learning_rate": 4.9895996860282576e-05,
      "loss": 0.974,
      "step": 10600
    },
    {
      "epoch": 0.16797488226059654,
      "grad_norm": 3.5661051273345947,
      "learning_rate": 4.989501569858713e-05,
      "loss": 0.9111,
      "step": 10700
    },
    {
      "epoch": 0.1695447409733124,
      "grad_norm": 4.258633136749268,
      "learning_rate": 4.9894034536891685e-05,
      "loss": 0.8987,
      "step": 10800
    },
    {
      "epoch": 0.17111459968602827,
      "grad_norm": 5.159017562866211,
      "learning_rate": 4.9893053375196236e-05,
      "loss": 0.9102,
      "step": 10900
    },
    {
      "epoch": 0.1726844583987441,
      "grad_norm": 2.838740110397339,
      "learning_rate": 4.989207221350079e-05,
      "loss": 0.9552,
      "step": 11000
    },
    {
      "epoch": 0.17425431711145997,
      "grad_norm": 3.9449124336242676,
      "learning_rate": 4.989109105180534e-05,
      "loss": 0.8766,
      "step": 11100
    },
    {
      "epoch": 0.17582417582417584,
      "grad_norm": 4.341366291046143,
      "learning_rate": 4.9890109890109895e-05,
      "loss": 0.9091,
      "step": 11200
    },
    {
      "epoch": 0.17739403453689168,
      "grad_norm": 4.573445796966553,
      "learning_rate": 4.9889128728414446e-05,
      "loss": 0.9536,
      "step": 11300
    },
    {
      "epoch": 0.17896389324960754,
      "grad_norm": 4.362887859344482,
      "learning_rate": 4.9888147566719e-05,
      "loss": 0.9913,
      "step": 11400
    },
    {
      "epoch": 0.18053375196232338,
      "grad_norm": 4.592761516571045,
      "learning_rate": 4.988716640502355e-05,
      "loss": 0.9698,
      "step": 11500
    },
    {
      "epoch": 0.18210361067503925,
      "grad_norm": 5.4487152099609375,
      "learning_rate": 4.9886185243328106e-05,
      "loss": 0.966,
      "step": 11600
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 4.504117012023926,
      "learning_rate": 4.988520408163265e-05,
      "loss": 0.9007,
      "step": 11700
    },
    {
      "epoch": 0.18524332810047095,
      "grad_norm": 4.917527675628662,
      "learning_rate": 4.988422291993721e-05,
      "loss": 0.9413,
      "step": 11800
    },
    {
      "epoch": 0.18681318681318682,
      "grad_norm": 5.293863773345947,
      "learning_rate": 4.988324175824176e-05,
      "loss": 0.9268,
      "step": 11900
    },
    {
      "epoch": 0.18838304552590268,
      "grad_norm": 5.429148197174072,
      "learning_rate": 4.988226059654632e-05,
      "loss": 0.952,
      "step": 12000
    },
    {
      "epoch": 0.18995290423861852,
      "grad_norm": 5.2504754066467285,
      "learning_rate": 4.988127943485086e-05,
      "loss": 0.8827,
      "step": 12100
    },
    {
      "epoch": 0.19152276295133439,
      "grad_norm": 5.409339904785156,
      "learning_rate": 4.988029827315542e-05,
      "loss": 0.9488,
      "step": 12200
    },
    {
      "epoch": 0.19309262166405022,
      "grad_norm": 4.620992660522461,
      "learning_rate": 4.987931711145997e-05,
      "loss": 0.9021,
      "step": 12300
    },
    {
      "epoch": 0.1946624803767661,
      "grad_norm": 4.018003940582275,
      "learning_rate": 4.987833594976452e-05,
      "loss": 0.9456,
      "step": 12400
    },
    {
      "epoch": 0.19623233908948196,
      "grad_norm": 4.632608890533447,
      "learning_rate": 4.987735478806908e-05,
      "loss": 1.0144,
      "step": 12500
    },
    {
      "epoch": 0.1978021978021978,
      "grad_norm": 3.662468910217285,
      "learning_rate": 4.987637362637363e-05,
      "loss": 0.9134,
      "step": 12600
    },
    {
      "epoch": 0.19937205651491366,
      "grad_norm": 4.084578990936279,
      "learning_rate": 4.987539246467818e-05,
      "loss": 0.9598,
      "step": 12700
    },
    {
      "epoch": 0.20094191522762953,
      "grad_norm": 5.414217472076416,
      "learning_rate": 4.987441130298273e-05,
      "loss": 0.8866,
      "step": 12800
    },
    {
      "epoch": 0.20251177394034536,
      "grad_norm": 5.19182014465332,
      "learning_rate": 4.987343014128729e-05,
      "loss": 0.93,
      "step": 12900
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 4.727225303649902,
      "learning_rate": 4.987244897959184e-05,
      "loss": 0.9146,
      "step": 13000
    },
    {
      "epoch": 0.20565149136577707,
      "grad_norm": 4.905632019042969,
      "learning_rate": 4.987146781789639e-05,
      "loss": 0.9295,
      "step": 13100
    },
    {
      "epoch": 0.20722135007849293,
      "grad_norm": 5.132832050323486,
      "learning_rate": 4.987048665620094e-05,
      "loss": 0.9422,
      "step": 13200
    },
    {
      "epoch": 0.2087912087912088,
      "grad_norm": 3.657778263092041,
      "learning_rate": 4.98695054945055e-05,
      "loss": 0.9411,
      "step": 13300
    },
    {
      "epoch": 0.21036106750392464,
      "grad_norm": 4.934663772583008,
      "learning_rate": 4.986852433281005e-05,
      "loss": 0.8891,
      "step": 13400
    },
    {
      "epoch": 0.2119309262166405,
      "grad_norm": 5.589333534240723,
      "learning_rate": 4.98675431711146e-05,
      "loss": 0.9149,
      "step": 13500
    },
    {
      "epoch": 0.21350078492935637,
      "grad_norm": 4.431686878204346,
      "learning_rate": 4.986656200941915e-05,
      "loss": 0.8921,
      "step": 13600
    },
    {
      "epoch": 0.2150706436420722,
      "grad_norm": 5.177498817443848,
      "learning_rate": 4.986558084772371e-05,
      "loss": 0.9632,
      "step": 13700
    },
    {
      "epoch": 0.21664050235478807,
      "grad_norm": 3.815922260284424,
      "learning_rate": 4.9864599686028255e-05,
      "loss": 0.8683,
      "step": 13800
    },
    {
      "epoch": 0.21821036106750394,
      "grad_norm": 5.332522869110107,
      "learning_rate": 4.986361852433281e-05,
      "loss": 0.9204,
      "step": 13900
    },
    {
      "epoch": 0.21978021978021978,
      "grad_norm": 4.408043384552002,
      "learning_rate": 4.9862637362637363e-05,
      "loss": 0.8719,
      "step": 14000
    },
    {
      "epoch": 0.22135007849293564,
      "grad_norm": 4.757442474365234,
      "learning_rate": 4.986165620094192e-05,
      "loss": 0.9579,
      "step": 14100
    },
    {
      "epoch": 0.22291993720565148,
      "grad_norm": 4.7385454177856445,
      "learning_rate": 4.9860675039246465e-05,
      "loss": 0.8844,
      "step": 14200
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 5.729391574859619,
      "learning_rate": 4.985969387755102e-05,
      "loss": 0.9076,
      "step": 14300
    },
    {
      "epoch": 0.2260596546310832,
      "grad_norm": 5.627930164337158,
      "learning_rate": 4.9858712715855574e-05,
      "loss": 0.9659,
      "step": 14400
    },
    {
      "epoch": 0.22762951334379905,
      "grad_norm": 4.926706790924072,
      "learning_rate": 4.9857731554160125e-05,
      "loss": 0.8789,
      "step": 14500
    },
    {
      "epoch": 0.22919937205651492,
      "grad_norm": 4.708712577819824,
      "learning_rate": 4.985675039246468e-05,
      "loss": 0.932,
      "step": 14600
    },
    {
      "epoch": 0.23076923076923078,
      "grad_norm": 4.544669151306152,
      "learning_rate": 4.9855769230769234e-05,
      "loss": 0.9065,
      "step": 14700
    },
    {
      "epoch": 0.23233908948194662,
      "grad_norm": 4.104889869689941,
      "learning_rate": 4.9854788069073785e-05,
      "loss": 0.836,
      "step": 14800
    },
    {
      "epoch": 0.23390894819466249,
      "grad_norm": 5.149326324462891,
      "learning_rate": 4.9853806907378336e-05,
      "loss": 0.9297,
      "step": 14900
    },
    {
      "epoch": 0.23547880690737832,
      "grad_norm": 5.566789150238037,
      "learning_rate": 4.9852825745682894e-05,
      "loss": 0.922,
      "step": 15000
    },
    {
      "epoch": 0.2370486656200942,
      "grad_norm": 5.695842266082764,
      "learning_rate": 4.9851844583987445e-05,
      "loss": 0.8697,
      "step": 15100
    },
    {
      "epoch": 0.23861852433281006,
      "grad_norm": 4.891533851623535,
      "learning_rate": 4.9850863422291996e-05,
      "loss": 0.8823,
      "step": 15200
    },
    {
      "epoch": 0.2401883830455259,
      "grad_norm": 4.352617263793945,
      "learning_rate": 4.9849882260596546e-05,
      "loss": 0.9237,
      "step": 15300
    },
    {
      "epoch": 0.24175824175824176,
      "grad_norm": 4.296303749084473,
      "learning_rate": 4.9848901098901104e-05,
      "loss": 0.8457,
      "step": 15400
    },
    {
      "epoch": 0.24332810047095763,
      "grad_norm": 5.71793794631958,
      "learning_rate": 4.9847919937205655e-05,
      "loss": 0.9147,
      "step": 15500
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 4.307261943817139,
      "learning_rate": 4.9846938775510206e-05,
      "loss": 0.9239,
      "step": 15600
    },
    {
      "epoch": 0.24646781789638933,
      "grad_norm": 4.448241710662842,
      "learning_rate": 4.984595761381476e-05,
      "loss": 0.9211,
      "step": 15700
    },
    {
      "epoch": 0.24803767660910517,
      "grad_norm": 3.9853618144989014,
      "learning_rate": 4.9844976452119315e-05,
      "loss": 0.8466,
      "step": 15800
    },
    {
      "epoch": 0.24960753532182103,
      "grad_norm": 5.055346965789795,
      "learning_rate": 4.984399529042386e-05,
      "loss": 0.8896,
      "step": 15900
    },
    {
      "epoch": 0.25117739403453687,
      "grad_norm": 3.9759793281555176,
      "learning_rate": 4.984301412872842e-05,
      "loss": 0.8462,
      "step": 16000
    },
    {
      "epoch": 0.25274725274725274,
      "grad_norm": 5.231631755828857,
      "learning_rate": 4.984203296703297e-05,
      "loss": 0.9114,
      "step": 16100
    },
    {
      "epoch": 0.2543171114599686,
      "grad_norm": 4.687506675720215,
      "learning_rate": 4.9841051805337526e-05,
      "loss": 0.8988,
      "step": 16200
    },
    {
      "epoch": 0.25588697017268447,
      "grad_norm": 4.284743309020996,
      "learning_rate": 4.984007064364207e-05,
      "loss": 0.8755,
      "step": 16300
    },
    {
      "epoch": 0.25745682888540034,
      "grad_norm": 4.995612144470215,
      "learning_rate": 4.983908948194663e-05,
      "loss": 0.8886,
      "step": 16400
    },
    {
      "epoch": 0.25902668759811615,
      "grad_norm": 4.649316787719727,
      "learning_rate": 4.983810832025118e-05,
      "loss": 0.9014,
      "step": 16500
    },
    {
      "epoch": 0.260596546310832,
      "grad_norm": 4.561774730682373,
      "learning_rate": 4.983712715855573e-05,
      "loss": 0.9278,
      "step": 16600
    },
    {
      "epoch": 0.2621664050235479,
      "grad_norm": 4.501826763153076,
      "learning_rate": 4.983614599686029e-05,
      "loss": 0.8854,
      "step": 16700
    },
    {
      "epoch": 0.26373626373626374,
      "grad_norm": 5.7751665115356445,
      "learning_rate": 4.983516483516484e-05,
      "loss": 0.9376,
      "step": 16800
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 4.421820640563965,
      "learning_rate": 4.983418367346939e-05,
      "loss": 0.9228,
      "step": 16900
    },
    {
      "epoch": 0.2668759811616955,
      "grad_norm": 4.034546852111816,
      "learning_rate": 4.983320251177394e-05,
      "loss": 0.8934,
      "step": 17000
    },
    {
      "epoch": 0.2684458398744113,
      "grad_norm": 4.549082279205322,
      "learning_rate": 4.98322213500785e-05,
      "loss": 0.9072,
      "step": 17100
    },
    {
      "epoch": 0.27001569858712715,
      "grad_norm": 5.580557823181152,
      "learning_rate": 4.983124018838305e-05,
      "loss": 0.8756,
      "step": 17200
    },
    {
      "epoch": 0.271585557299843,
      "grad_norm": 4.75112247467041,
      "learning_rate": 4.98302590266876e-05,
      "loss": 0.8914,
      "step": 17300
    },
    {
      "epoch": 0.2731554160125589,
      "grad_norm": 4.95440673828125,
      "learning_rate": 4.982927786499215e-05,
      "loss": 0.8631,
      "step": 17400
    },
    {
      "epoch": 0.27472527472527475,
      "grad_norm": 5.567151069641113,
      "learning_rate": 4.982829670329671e-05,
      "loss": 0.8929,
      "step": 17500
    },
    {
      "epoch": 0.27629513343799056,
      "grad_norm": 4.201920509338379,
      "learning_rate": 4.982731554160126e-05,
      "loss": 0.8451,
      "step": 17600
    },
    {
      "epoch": 0.2778649921507064,
      "grad_norm": 5.08156156539917,
      "learning_rate": 4.982633437990581e-05,
      "loss": 0.9151,
      "step": 17700
    },
    {
      "epoch": 0.2794348508634223,
      "grad_norm": 4.701020240783691,
      "learning_rate": 4.982535321821036e-05,
      "loss": 0.8881,
      "step": 17800
    },
    {
      "epoch": 0.28100470957613816,
      "grad_norm": 4.981975078582764,
      "learning_rate": 4.982437205651492e-05,
      "loss": 0.8756,
      "step": 17900
    },
    {
      "epoch": 0.282574568288854,
      "grad_norm": 3.8580031394958496,
      "learning_rate": 4.9823390894819464e-05,
      "loss": 0.9089,
      "step": 18000
    },
    {
      "epoch": 0.28414442700156983,
      "grad_norm": 4.006866931915283,
      "learning_rate": 4.982240973312402e-05,
      "loss": 0.9401,
      "step": 18100
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 6.169759750366211,
      "learning_rate": 4.982142857142857e-05,
      "loss": 0.8939,
      "step": 18200
    },
    {
      "epoch": 0.28728414442700156,
      "grad_norm": 4.262808799743652,
      "learning_rate": 4.982044740973313e-05,
      "loss": 0.9019,
      "step": 18300
    },
    {
      "epoch": 0.28885400313971743,
      "grad_norm": 4.413633823394775,
      "learning_rate": 4.9819466248037674e-05,
      "loss": 0.9004,
      "step": 18400
    },
    {
      "epoch": 0.2904238618524333,
      "grad_norm": 5.382455348968506,
      "learning_rate": 4.981848508634223e-05,
      "loss": 0.8449,
      "step": 18500
    },
    {
      "epoch": 0.29199372056514916,
      "grad_norm": 4.257999897003174,
      "learning_rate": 4.981750392464678e-05,
      "loss": 0.9412,
      "step": 18600
    },
    {
      "epoch": 0.29356357927786497,
      "grad_norm": 3.6487550735473633,
      "learning_rate": 4.9816522762951334e-05,
      "loss": 0.9062,
      "step": 18700
    },
    {
      "epoch": 0.29513343799058084,
      "grad_norm": 4.198863506317139,
      "learning_rate": 4.9815541601255885e-05,
      "loss": 0.8717,
      "step": 18800
    },
    {
      "epoch": 0.2967032967032967,
      "grad_norm": 4.865671634674072,
      "learning_rate": 4.981456043956044e-05,
      "loss": 0.8965,
      "step": 18900
    },
    {
      "epoch": 0.29827315541601257,
      "grad_norm": 3.3522424697875977,
      "learning_rate": 4.9813579277864994e-05,
      "loss": 0.8882,
      "step": 19000
    },
    {
      "epoch": 0.29984301412872844,
      "grad_norm": 3.1848604679107666,
      "learning_rate": 4.9812598116169545e-05,
      "loss": 0.8826,
      "step": 19100
    },
    {
      "epoch": 0.30141287284144425,
      "grad_norm": 3.997774362564087,
      "learning_rate": 4.98116169544741e-05,
      "loss": 0.8821,
      "step": 19200
    },
    {
      "epoch": 0.3029827315541601,
      "grad_norm": 4.336977958679199,
      "learning_rate": 4.9810635792778654e-05,
      "loss": 0.899,
      "step": 19300
    },
    {
      "epoch": 0.304552590266876,
      "grad_norm": 3.7035305500030518,
      "learning_rate": 4.9809654631083204e-05,
      "loss": 0.8821,
      "step": 19400
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 3.776954174041748,
      "learning_rate": 4.9808673469387755e-05,
      "loss": 0.8477,
      "step": 19500
    },
    {
      "epoch": 0.3076923076923077,
      "grad_norm": 4.446315765380859,
      "learning_rate": 4.980769230769231e-05,
      "loss": 0.8809,
      "step": 19600
    },
    {
      "epoch": 0.3092621664050236,
      "grad_norm": 5.308003902435303,
      "learning_rate": 4.9806711145996864e-05,
      "loss": 0.9148,
      "step": 19700
    },
    {
      "epoch": 0.3108320251177394,
      "grad_norm": 4.9696245193481445,
      "learning_rate": 4.9805729984301415e-05,
      "loss": 0.9073,
      "step": 19800
    },
    {
      "epoch": 0.31240188383045525,
      "grad_norm": 4.489208221435547,
      "learning_rate": 4.9804748822605966e-05,
      "loss": 0.885,
      "step": 19900
    },
    {
      "epoch": 0.3139717425431711,
      "grad_norm": 3.7777445316314697,
      "learning_rate": 4.9803767660910524e-05,
      "loss": 0.9076,
      "step": 20000
    },
    {
      "epoch": 0.315541601255887,
      "grad_norm": 4.739073753356934,
      "learning_rate": 4.980278649921507e-05,
      "loss": 0.8849,
      "step": 20100
    },
    {
      "epoch": 0.31711145996860285,
      "grad_norm": 4.347486972808838,
      "learning_rate": 4.9801805337519626e-05,
      "loss": 0.8504,
      "step": 20200
    },
    {
      "epoch": 0.31868131868131866,
      "grad_norm": 4.766229152679443,
      "learning_rate": 4.980082417582418e-05,
      "loss": 0.9313,
      "step": 20300
    },
    {
      "epoch": 0.3202511773940345,
      "grad_norm": 5.2859673500061035,
      "learning_rate": 4.9799843014128735e-05,
      "loss": 0.8224,
      "step": 20400
    },
    {
      "epoch": 0.3218210361067504,
      "grad_norm": 4.786915302276611,
      "learning_rate": 4.979886185243328e-05,
      "loss": 0.8766,
      "step": 20500
    },
    {
      "epoch": 0.32339089481946626,
      "grad_norm": 3.7541604042053223,
      "learning_rate": 4.9797880690737837e-05,
      "loss": 0.8792,
      "step": 20600
    },
    {
      "epoch": 0.3249607535321821,
      "grad_norm": 4.7948079109191895,
      "learning_rate": 4.979689952904239e-05,
      "loss": 0.8687,
      "step": 20700
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 4.725884914398193,
      "learning_rate": 4.979591836734694e-05,
      "loss": 0.8733,
      "step": 20800
    },
    {
      "epoch": 0.3281004709576138,
      "grad_norm": 3.5970046520233154,
      "learning_rate": 4.979493720565149e-05,
      "loss": 0.899,
      "step": 20900
    },
    {
      "epoch": 0.32967032967032966,
      "grad_norm": 4.405435085296631,
      "learning_rate": 4.979395604395605e-05,
      "loss": 0.9027,
      "step": 21000
    },
    {
      "epoch": 0.33124018838304553,
      "grad_norm": 5.438555717468262,
      "learning_rate": 4.97929748822606e-05,
      "loss": 0.8813,
      "step": 21100
    },
    {
      "epoch": 0.3328100470957614,
      "grad_norm": 4.609579563140869,
      "learning_rate": 4.979199372056515e-05,
      "loss": 0.8889,
      "step": 21200
    },
    {
      "epoch": 0.33437990580847726,
      "grad_norm": 3.9554481506347656,
      "learning_rate": 4.979101255886971e-05,
      "loss": 0.8307,
      "step": 21300
    },
    {
      "epoch": 0.3359497645211931,
      "grad_norm": 4.20311164855957,
      "learning_rate": 4.979003139717426e-05,
      "loss": 0.893,
      "step": 21400
    },
    {
      "epoch": 0.33751962323390894,
      "grad_norm": 5.151994228363037,
      "learning_rate": 4.978905023547881e-05,
      "loss": 0.8519,
      "step": 21500
    },
    {
      "epoch": 0.3390894819466248,
      "grad_norm": 4.830306529998779,
      "learning_rate": 4.978806907378336e-05,
      "loss": 0.876,
      "step": 21600
    },
    {
      "epoch": 0.34065934065934067,
      "grad_norm": 5.205686569213867,
      "learning_rate": 4.978708791208792e-05,
      "loss": 0.8637,
      "step": 21700
    },
    {
      "epoch": 0.34222919937205654,
      "grad_norm": 4.7429938316345215,
      "learning_rate": 4.978610675039247e-05,
      "loss": 0.9128,
      "step": 21800
    },
    {
      "epoch": 0.34379905808477235,
      "grad_norm": 3.843940258026123,
      "learning_rate": 4.978512558869702e-05,
      "loss": 0.8878,
      "step": 21900
    },
    {
      "epoch": 0.3453689167974882,
      "grad_norm": 4.536914825439453,
      "learning_rate": 4.978414442700157e-05,
      "loss": 0.8822,
      "step": 22000
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 4.25846004486084,
      "learning_rate": 4.978316326530613e-05,
      "loss": 0.8859,
      "step": 22100
    },
    {
      "epoch": 0.34850863422291994,
      "grad_norm": 4.7789626121521,
      "learning_rate": 4.978218210361067e-05,
      "loss": 0.8686,
      "step": 22200
    },
    {
      "epoch": 0.3500784929356358,
      "grad_norm": 3.7835333347320557,
      "learning_rate": 4.978120094191523e-05,
      "loss": 0.8519,
      "step": 22300
    },
    {
      "epoch": 0.3516483516483517,
      "grad_norm": 4.215607643127441,
      "learning_rate": 4.978021978021978e-05,
      "loss": 0.8518,
      "step": 22400
    },
    {
      "epoch": 0.3532182103610675,
      "grad_norm": 3.9436442852020264,
      "learning_rate": 4.977923861852434e-05,
      "loss": 0.877,
      "step": 22500
    },
    {
      "epoch": 0.35478806907378335,
      "grad_norm": 5.369701862335205,
      "learning_rate": 4.977825745682888e-05,
      "loss": 0.8507,
      "step": 22600
    },
    {
      "epoch": 0.3563579277864992,
      "grad_norm": 4.215433597564697,
      "learning_rate": 4.977727629513344e-05,
      "loss": 0.8611,
      "step": 22700
    },
    {
      "epoch": 0.3579277864992151,
      "grad_norm": 4.808346271514893,
      "learning_rate": 4.977629513343799e-05,
      "loss": 0.8738,
      "step": 22800
    },
    {
      "epoch": 0.35949764521193095,
      "grad_norm": 4.72641134262085,
      "learning_rate": 4.977531397174254e-05,
      "loss": 0.8289,
      "step": 22900
    },
    {
      "epoch": 0.36106750392464676,
      "grad_norm": 4.751847267150879,
      "learning_rate": 4.9774332810047094e-05,
      "loss": 0.9285,
      "step": 23000
    },
    {
      "epoch": 0.3626373626373626,
      "grad_norm": 4.674197673797607,
      "learning_rate": 4.977335164835165e-05,
      "loss": 0.8142,
      "step": 23100
    },
    {
      "epoch": 0.3642072213500785,
      "grad_norm": 4.415899753570557,
      "learning_rate": 4.97723704866562e-05,
      "loss": 0.8477,
      "step": 23200
    },
    {
      "epoch": 0.36577708006279436,
      "grad_norm": 4.818538665771484,
      "learning_rate": 4.9771389324960754e-05,
      "loss": 0.8902,
      "step": 23300
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 5.125466823577881,
      "learning_rate": 4.977040816326531e-05,
      "loss": 0.8688,
      "step": 23400
    },
    {
      "epoch": 0.36891679748822603,
      "grad_norm": 3.999032974243164,
      "learning_rate": 4.976942700156986e-05,
      "loss": 0.8254,
      "step": 23500
    },
    {
      "epoch": 0.3704866562009419,
      "grad_norm": 4.531442165374756,
      "learning_rate": 4.9768445839874413e-05,
      "loss": 0.849,
      "step": 23600
    },
    {
      "epoch": 0.37205651491365777,
      "grad_norm": 4.886351108551025,
      "learning_rate": 4.9767464678178964e-05,
      "loss": 0.8821,
      "step": 23700
    },
    {
      "epoch": 0.37362637362637363,
      "grad_norm": 3.9916083812713623,
      "learning_rate": 4.976648351648352e-05,
      "loss": 0.8474,
      "step": 23800
    },
    {
      "epoch": 0.3751962323390895,
      "grad_norm": 4.660285472869873,
      "learning_rate": 4.976550235478807e-05,
      "loss": 0.8698,
      "step": 23900
    },
    {
      "epoch": 0.37676609105180536,
      "grad_norm": 4.247408866882324,
      "learning_rate": 4.9764521193092624e-05,
      "loss": 0.8381,
      "step": 24000
    },
    {
      "epoch": 0.3783359497645212,
      "grad_norm": 4.678214073181152,
      "learning_rate": 4.9763540031397175e-05,
      "loss": 0.8602,
      "step": 24100
    },
    {
      "epoch": 0.37990580847723704,
      "grad_norm": 3.258051872253418,
      "learning_rate": 4.976255886970173e-05,
      "loss": 0.8481,
      "step": 24200
    },
    {
      "epoch": 0.3814756671899529,
      "grad_norm": 4.727330684661865,
      "learning_rate": 4.976157770800628e-05,
      "loss": 0.8259,
      "step": 24300
    },
    {
      "epoch": 0.38304552590266877,
      "grad_norm": 4.7012529373168945,
      "learning_rate": 4.9760596546310835e-05,
      "loss": 0.8852,
      "step": 24400
    },
    {
      "epoch": 0.38461538461538464,
      "grad_norm": 4.26056432723999,
      "learning_rate": 4.9759615384615386e-05,
      "loss": 0.8115,
      "step": 24500
    },
    {
      "epoch": 0.38618524332810045,
      "grad_norm": 3.991880178451538,
      "learning_rate": 4.9758634222919944e-05,
      "loss": 0.8429,
      "step": 24600
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 4.178058624267578,
      "learning_rate": 4.975765306122449e-05,
      "loss": 0.8629,
      "step": 24700
    },
    {
      "epoch": 0.3893249607535322,
      "grad_norm": 3.929433584213257,
      "learning_rate": 4.9756671899529046e-05,
      "loss": 0.8345,
      "step": 24800
    },
    {
      "epoch": 0.39089481946624804,
      "grad_norm": 5.845942497253418,
      "learning_rate": 4.9755690737833597e-05,
      "loss": 0.8883,
      "step": 24900
    },
    {
      "epoch": 0.3924646781789639,
      "grad_norm": 4.635286331176758,
      "learning_rate": 4.975470957613815e-05,
      "loss": 0.8622,
      "step": 25000
    },
    {
      "epoch": 0.3940345368916798,
      "grad_norm": 3.863929510116577,
      "learning_rate": 4.97537284144427e-05,
      "loss": 0.8392,
      "step": 25100
    },
    {
      "epoch": 0.3956043956043956,
      "grad_norm": 4.990351676940918,
      "learning_rate": 4.9752747252747256e-05,
      "loss": 0.838,
      "step": 25200
    },
    {
      "epoch": 0.39717425431711145,
      "grad_norm": 5.487148761749268,
      "learning_rate": 4.975176609105181e-05,
      "loss": 0.8349,
      "step": 25300
    },
    {
      "epoch": 0.3987441130298273,
      "grad_norm": 5.012301921844482,
      "learning_rate": 4.975078492935636e-05,
      "loss": 0.8827,
      "step": 25400
    },
    {
      "epoch": 0.4003139717425432,
      "grad_norm": 5.095057964324951,
      "learning_rate": 4.9749803767660916e-05,
      "loss": 0.8476,
      "step": 25500
    },
    {
      "epoch": 0.40188383045525905,
      "grad_norm": 5.6058549880981445,
      "learning_rate": 4.974882260596547e-05,
      "loss": 0.8281,
      "step": 25600
    },
    {
      "epoch": 0.40345368916797486,
      "grad_norm": 3.7595551013946533,
      "learning_rate": 4.974784144427002e-05,
      "loss": 0.8866,
      "step": 25700
    },
    {
      "epoch": 0.4050235478806907,
      "grad_norm": 5.175686359405518,
      "learning_rate": 4.974686028257457e-05,
      "loss": 0.8489,
      "step": 25800
    },
    {
      "epoch": 0.4065934065934066,
      "grad_norm": 3.9783456325531006,
      "learning_rate": 4.974587912087913e-05,
      "loss": 0.9,
      "step": 25900
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 4.882892608642578,
      "learning_rate": 4.974489795918368e-05,
      "loss": 0.8425,
      "step": 26000
    },
    {
      "epoch": 0.4097331240188383,
      "grad_norm": 5.452770709991455,
      "learning_rate": 4.974391679748823e-05,
      "loss": 0.8098,
      "step": 26100
    },
    {
      "epoch": 0.41130298273155413,
      "grad_norm": 3.7227566242218018,
      "learning_rate": 4.974293563579278e-05,
      "loss": 0.9092,
      "step": 26200
    },
    {
      "epoch": 0.41287284144427,
      "grad_norm": 4.522994518280029,
      "learning_rate": 4.974195447409734e-05,
      "loss": 0.8604,
      "step": 26300
    },
    {
      "epoch": 0.41444270015698587,
      "grad_norm": 4.556796550750732,
      "learning_rate": 4.974097331240188e-05,
      "loss": 0.8478,
      "step": 26400
    },
    {
      "epoch": 0.41601255886970173,
      "grad_norm": 4.041748046875,
      "learning_rate": 4.973999215070644e-05,
      "loss": 0.8399,
      "step": 26500
    },
    {
      "epoch": 0.4175824175824176,
      "grad_norm": 3.9944002628326416,
      "learning_rate": 4.973901098901099e-05,
      "loss": 0.8874,
      "step": 26600
    },
    {
      "epoch": 0.41915227629513346,
      "grad_norm": 4.075374603271484,
      "learning_rate": 4.973802982731555e-05,
      "loss": 0.8771,
      "step": 26700
    },
    {
      "epoch": 0.4207221350078493,
      "grad_norm": 3.2981221675872803,
      "learning_rate": 4.973704866562009e-05,
      "loss": 0.809,
      "step": 26800
    },
    {
      "epoch": 0.42229199372056514,
      "grad_norm": 4.569493770599365,
      "learning_rate": 4.973606750392465e-05,
      "loss": 0.8493,
      "step": 26900
    },
    {
      "epoch": 0.423861852433281,
      "grad_norm": 4.869147777557373,
      "learning_rate": 4.97350863422292e-05,
      "loss": 0.8585,
      "step": 27000
    },
    {
      "epoch": 0.42543171114599687,
      "grad_norm": 4.422493934631348,
      "learning_rate": 4.973410518053375e-05,
      "loss": 0.8352,
      "step": 27100
    },
    {
      "epoch": 0.42700156985871274,
      "grad_norm": 4.23952579498291,
      "learning_rate": 4.97331240188383e-05,
      "loss": 0.8289,
      "step": 27200
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 4.951995849609375,
      "learning_rate": 4.973214285714286e-05,
      "loss": 0.8327,
      "step": 27300
    },
    {
      "epoch": 0.4301412872841444,
      "grad_norm": 4.685556411743164,
      "learning_rate": 4.973116169544741e-05,
      "loss": 0.9182,
      "step": 27400
    },
    {
      "epoch": 0.4317111459968603,
      "grad_norm": 5.1496148109436035,
      "learning_rate": 4.973018053375196e-05,
      "loss": 0.7978,
      "step": 27500
    },
    {
      "epoch": 0.43328100470957615,
      "grad_norm": 4.4139404296875,
      "learning_rate": 4.972919937205652e-05,
      "loss": 0.8895,
      "step": 27600
    },
    {
      "epoch": 0.434850863422292,
      "grad_norm": 4.447625160217285,
      "learning_rate": 4.972821821036107e-05,
      "loss": 0.8068,
      "step": 27700
    },
    {
      "epoch": 0.4364207221350079,
      "grad_norm": 4.858394145965576,
      "learning_rate": 4.972723704866562e-05,
      "loss": 0.8756,
      "step": 27800
    },
    {
      "epoch": 0.4379905808477237,
      "grad_norm": 4.273231506347656,
      "learning_rate": 4.9726255886970173e-05,
      "loss": 0.8855,
      "step": 27900
    },
    {
      "epoch": 0.43956043956043955,
      "grad_norm": 4.212184906005859,
      "learning_rate": 4.972527472527473e-05,
      "loss": 0.8766,
      "step": 28000
    },
    {
      "epoch": 0.4411302982731554,
      "grad_norm": 5.398836135864258,
      "learning_rate": 4.972429356357928e-05,
      "loss": 0.8084,
      "step": 28100
    },
    {
      "epoch": 0.4427001569858713,
      "grad_norm": 4.637157440185547,
      "learning_rate": 4.972331240188383e-05,
      "loss": 0.8858,
      "step": 28200
    },
    {
      "epoch": 0.44427001569858715,
      "grad_norm": 4.888664245605469,
      "learning_rate": 4.9722331240188384e-05,
      "loss": 0.8501,
      "step": 28300
    },
    {
      "epoch": 0.44583987441130296,
      "grad_norm": 4.172804832458496,
      "learning_rate": 4.972135007849294e-05,
      "loss": 0.8534,
      "step": 28400
    },
    {
      "epoch": 0.4474097331240188,
      "grad_norm": 4.276927947998047,
      "learning_rate": 4.9720368916797486e-05,
      "loss": 0.8338,
      "step": 28500
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 3.9716134071350098,
      "learning_rate": 4.9719387755102044e-05,
      "loss": 0.8563,
      "step": 28600
    },
    {
      "epoch": 0.45054945054945056,
      "grad_norm": 4.013527870178223,
      "learning_rate": 4.9718406593406595e-05,
      "loss": 0.8649,
      "step": 28700
    },
    {
      "epoch": 0.4521193092621664,
      "grad_norm": 3.9604105949401855,
      "learning_rate": 4.971742543171115e-05,
      "loss": 0.8663,
      "step": 28800
    },
    {
      "epoch": 0.45368916797488223,
      "grad_norm": 4.846940994262695,
      "learning_rate": 4.97164442700157e-05,
      "loss": 0.8213,
      "step": 28900
    },
    {
      "epoch": 0.4552590266875981,
      "grad_norm": 5.195522308349609,
      "learning_rate": 4.9715463108320255e-05,
      "loss": 0.826,
      "step": 29000
    },
    {
      "epoch": 0.45682888540031397,
      "grad_norm": 4.528500080108643,
      "learning_rate": 4.9714481946624806e-05,
      "loss": 0.8372,
      "step": 29100
    },
    {
      "epoch": 0.45839874411302983,
      "grad_norm": 4.436499118804932,
      "learning_rate": 4.9713500784929357e-05,
      "loss": 0.8355,
      "step": 29200
    },
    {
      "epoch": 0.4599686028257457,
      "grad_norm": 4.880756855010986,
      "learning_rate": 4.971251962323391e-05,
      "loss": 0.8411,
      "step": 29300
    },
    {
      "epoch": 0.46153846153846156,
      "grad_norm": 4.9406280517578125,
      "learning_rate": 4.9711538461538465e-05,
      "loss": 0.8142,
      "step": 29400
    },
    {
      "epoch": 0.4631083202511774,
      "grad_norm": 4.360330581665039,
      "learning_rate": 4.9710557299843016e-05,
      "loss": 0.8769,
      "step": 29500
    },
    {
      "epoch": 0.46467817896389324,
      "grad_norm": 4.251989841461182,
      "learning_rate": 4.970957613814757e-05,
      "loss": 0.8419,
      "step": 29600
    },
    {
      "epoch": 0.4662480376766091,
      "grad_norm": 4.737494468688965,
      "learning_rate": 4.9708594976452125e-05,
      "loss": 0.9146,
      "step": 29700
    },
    {
      "epoch": 0.46781789638932497,
      "grad_norm": 4.517764091491699,
      "learning_rate": 4.9707613814756676e-05,
      "loss": 0.8508,
      "step": 29800
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 3.6390717029571533,
      "learning_rate": 4.970663265306123e-05,
      "loss": 0.8389,
      "step": 29900
    },
    {
      "epoch": 0.47095761381475665,
      "grad_norm": 3.2756404876708984,
      "learning_rate": 4.970565149136578e-05,
      "loss": 0.8251,
      "step": 30000
    },
    {
      "epoch": 0.4725274725274725,
      "grad_norm": 4.5375213623046875,
      "learning_rate": 4.9704670329670336e-05,
      "loss": 0.8799,
      "step": 30100
    },
    {
      "epoch": 0.4740973312401884,
      "grad_norm": 4.665286540985107,
      "learning_rate": 4.970368916797489e-05,
      "loss": 0.8462,
      "step": 30200
    },
    {
      "epoch": 0.47566718995290425,
      "grad_norm": 5.115809917449951,
      "learning_rate": 4.970270800627944e-05,
      "loss": 0.8823,
      "step": 30300
    },
    {
      "epoch": 0.4772370486656201,
      "grad_norm": 4.510003566741943,
      "learning_rate": 4.970172684458399e-05,
      "loss": 0.838,
      "step": 30400
    },
    {
      "epoch": 0.478806907378336,
      "grad_norm": 5.092260360717773,
      "learning_rate": 4.9700745682888546e-05,
      "loss": 0.8375,
      "step": 30500
    },
    {
      "epoch": 0.4803767660910518,
      "grad_norm": 4.313407897949219,
      "learning_rate": 4.969976452119309e-05,
      "loss": 0.8351,
      "step": 30600
    },
    {
      "epoch": 0.48194662480376765,
      "grad_norm": 4.999098777770996,
      "learning_rate": 4.969878335949765e-05,
      "loss": 0.8357,
      "step": 30700
    },
    {
      "epoch": 0.4835164835164835,
      "grad_norm": 4.552789688110352,
      "learning_rate": 4.96978021978022e-05,
      "loss": 0.8153,
      "step": 30800
    },
    {
      "epoch": 0.4850863422291994,
      "grad_norm": 3.8199357986450195,
      "learning_rate": 4.969682103610676e-05,
      "loss": 0.8184,
      "step": 30900
    },
    {
      "epoch": 0.48665620094191525,
      "grad_norm": 4.259797096252441,
      "learning_rate": 4.96958398744113e-05,
      "loss": 0.8795,
      "step": 31000
    },
    {
      "epoch": 0.48822605965463106,
      "grad_norm": 4.07320499420166,
      "learning_rate": 4.969485871271586e-05,
      "loss": 0.8788,
      "step": 31100
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 4.832005023956299,
      "learning_rate": 4.969387755102041e-05,
      "loss": 0.8441,
      "step": 31200
    },
    {
      "epoch": 0.4913657770800628,
      "grad_norm": 4.208081245422363,
      "learning_rate": 4.969289638932496e-05,
      "loss": 0.8654,
      "step": 31300
    },
    {
      "epoch": 0.49293563579277866,
      "grad_norm": 3.82315731048584,
      "learning_rate": 4.969191522762951e-05,
      "loss": 0.828,
      "step": 31400
    },
    {
      "epoch": 0.4945054945054945,
      "grad_norm": 4.046810150146484,
      "learning_rate": 4.969093406593407e-05,
      "loss": 0.7997,
      "step": 31500
    },
    {
      "epoch": 0.49607535321821034,
      "grad_norm": 5.303223609924316,
      "learning_rate": 4.968995290423862e-05,
      "loss": 0.8527,
      "step": 31600
    },
    {
      "epoch": 0.4976452119309262,
      "grad_norm": 4.757104873657227,
      "learning_rate": 4.968897174254317e-05,
      "loss": 0.8306,
      "step": 31700
    },
    {
      "epoch": 0.49921507064364207,
      "grad_norm": 4.661187171936035,
      "learning_rate": 4.968799058084773e-05,
      "loss": 0.8156,
      "step": 31800
    },
    {
      "epoch": 0.5007849293563579,
      "grad_norm": 4.314532279968262,
      "learning_rate": 4.968700941915228e-05,
      "loss": 0.8433,
      "step": 31900
    },
    {
      "epoch": 0.5023547880690737,
      "grad_norm": 3.9675285816192627,
      "learning_rate": 4.968602825745683e-05,
      "loss": 0.8839,
      "step": 32000
    },
    {
      "epoch": 0.5039246467817896,
      "grad_norm": 5.246296405792236,
      "learning_rate": 4.968504709576138e-05,
      "loss": 0.8302,
      "step": 32100
    },
    {
      "epoch": 0.5054945054945055,
      "grad_norm": 4.5932440757751465,
      "learning_rate": 4.968406593406594e-05,
      "loss": 0.8711,
      "step": 32200
    },
    {
      "epoch": 0.5070643642072213,
      "grad_norm": 4.194288730621338,
      "learning_rate": 4.968308477237049e-05,
      "loss": 0.851,
      "step": 32300
    },
    {
      "epoch": 0.5086342229199372,
      "grad_norm": 4.338903903961182,
      "learning_rate": 4.968210361067504e-05,
      "loss": 0.7998,
      "step": 32400
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 5.136180877685547,
      "learning_rate": 4.968112244897959e-05,
      "loss": 0.8298,
      "step": 32500
    },
    {
      "epoch": 0.5117739403453689,
      "grad_norm": 5.825897216796875,
      "learning_rate": 4.968014128728415e-05,
      "loss": 0.8599,
      "step": 32600
    },
    {
      "epoch": 0.5133437990580848,
      "grad_norm": 5.152547359466553,
      "learning_rate": 4.9679160125588695e-05,
      "loss": 0.844,
      "step": 32700
    },
    {
      "epoch": 0.5149136577708007,
      "grad_norm": 4.597870349884033,
      "learning_rate": 4.967817896389325e-05,
      "loss": 0.8528,
      "step": 32800
    },
    {
      "epoch": 0.5164835164835165,
      "grad_norm": 4.083693981170654,
      "learning_rate": 4.9677197802197804e-05,
      "loss": 0.8487,
      "step": 32900
    },
    {
      "epoch": 0.5180533751962323,
      "grad_norm": 4.2721686363220215,
      "learning_rate": 4.967621664050236e-05,
      "loss": 0.8839,
      "step": 33000
    },
    {
      "epoch": 0.5196232339089482,
      "grad_norm": 4.0708537101745605,
      "learning_rate": 4.9675235478806906e-05,
      "loss": 0.8443,
      "step": 33100
    },
    {
      "epoch": 0.521193092621664,
      "grad_norm": 4.487679481506348,
      "learning_rate": 4.9674254317111464e-05,
      "loss": 0.8488,
      "step": 33200
    },
    {
      "epoch": 0.5227629513343799,
      "grad_norm": 3.655649185180664,
      "learning_rate": 4.9673273155416015e-05,
      "loss": 0.8314,
      "step": 33300
    },
    {
      "epoch": 0.5243328100470958,
      "grad_norm": 4.163240909576416,
      "learning_rate": 4.9672291993720566e-05,
      "loss": 0.8727,
      "step": 33400
    },
    {
      "epoch": 0.5259026687598116,
      "grad_norm": 4.246090412139893,
      "learning_rate": 4.9671310832025116e-05,
      "loss": 0.8707,
      "step": 33500
    },
    {
      "epoch": 0.5274725274725275,
      "grad_norm": 4.623417377471924,
      "learning_rate": 4.9670329670329674e-05,
      "loss": 0.8605,
      "step": 33600
    },
    {
      "epoch": 0.5290423861852434,
      "grad_norm": 4.777970790863037,
      "learning_rate": 4.9669348508634225e-05,
      "loss": 0.814,
      "step": 33700
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 4.589809894561768,
      "learning_rate": 4.9668367346938776e-05,
      "loss": 0.8047,
      "step": 33800
    },
    {
      "epoch": 0.5321821036106751,
      "grad_norm": 5.813843727111816,
      "learning_rate": 4.9667386185243334e-05,
      "loss": 0.8101,
      "step": 33900
    },
    {
      "epoch": 0.533751962323391,
      "grad_norm": 4.922992706298828,
      "learning_rate": 4.9666405023547885e-05,
      "loss": 0.8205,
      "step": 34000
    },
    {
      "epoch": 0.5353218210361067,
      "grad_norm": 3.6372506618499756,
      "learning_rate": 4.9665423861852436e-05,
      "loss": 0.7866,
      "step": 34100
    },
    {
      "epoch": 0.5368916797488226,
      "grad_norm": 4.754805088043213,
      "learning_rate": 4.966444270015699e-05,
      "loss": 0.8199,
      "step": 34200
    },
    {
      "epoch": 0.5384615384615384,
      "grad_norm": 4.970324516296387,
      "learning_rate": 4.9663461538461545e-05,
      "loss": 0.815,
      "step": 34300
    },
    {
      "epoch": 0.5400313971742543,
      "grad_norm": 4.21127462387085,
      "learning_rate": 4.966248037676609e-05,
      "loss": 0.7779,
      "step": 34400
    },
    {
      "epoch": 0.5416012558869702,
      "grad_norm": 4.554904937744141,
      "learning_rate": 4.966149921507065e-05,
      "loss": 0.8214,
      "step": 34500
    },
    {
      "epoch": 0.543171114599686,
      "grad_norm": 3.5540943145751953,
      "learning_rate": 4.96605180533752e-05,
      "loss": 0.8733,
      "step": 34600
    },
    {
      "epoch": 0.5447409733124019,
      "grad_norm": 4.382591724395752,
      "learning_rate": 4.9659536891679755e-05,
      "loss": 0.7941,
      "step": 34700
    },
    {
      "epoch": 0.5463108320251178,
      "grad_norm": 5.359100818634033,
      "learning_rate": 4.96585557299843e-05,
      "loss": 0.7992,
      "step": 34800
    },
    {
      "epoch": 0.5478806907378336,
      "grad_norm": 5.20298957824707,
      "learning_rate": 4.965757456828886e-05,
      "loss": 0.814,
      "step": 34900
    },
    {
      "epoch": 0.5494505494505495,
      "grad_norm": 5.4648542404174805,
      "learning_rate": 4.965659340659341e-05,
      "loss": 0.814,
      "step": 35000
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 5.01630163192749,
      "learning_rate": 4.965561224489796e-05,
      "loss": 0.8685,
      "step": 35100
    },
    {
      "epoch": 0.5525902668759811,
      "grad_norm": 4.647847652435303,
      "learning_rate": 4.965463108320251e-05,
      "loss": 0.8649,
      "step": 35200
    },
    {
      "epoch": 0.554160125588697,
      "grad_norm": 5.016851425170898,
      "learning_rate": 4.965364992150707e-05,
      "loss": 0.8833,
      "step": 35300
    },
    {
      "epoch": 0.5557299843014128,
      "grad_norm": 3.7312633991241455,
      "learning_rate": 4.965266875981162e-05,
      "loss": 0.8301,
      "step": 35400
    },
    {
      "epoch": 0.5572998430141287,
      "grad_norm": 5.070457935333252,
      "learning_rate": 4.965168759811617e-05,
      "loss": 0.842,
      "step": 35500
    },
    {
      "epoch": 0.5588697017268446,
      "grad_norm": 3.3875246047973633,
      "learning_rate": 4.965070643642072e-05,
      "loss": 0.8526,
      "step": 35600
    },
    {
      "epoch": 0.5604395604395604,
      "grad_norm": 4.121779441833496,
      "learning_rate": 4.964972527472528e-05,
      "loss": 0.8141,
      "step": 35700
    },
    {
      "epoch": 0.5620094191522763,
      "grad_norm": 5.672417163848877,
      "learning_rate": 4.964874411302983e-05,
      "loss": 0.8647,
      "step": 35800
    },
    {
      "epoch": 0.5635792778649922,
      "grad_norm": 5.058023452758789,
      "learning_rate": 4.964776295133438e-05,
      "loss": 0.8115,
      "step": 35900
    },
    {
      "epoch": 0.565149136577708,
      "grad_norm": 4.090623378753662,
      "learning_rate": 4.964678178963894e-05,
      "loss": 0.8141,
      "step": 36000
    },
    {
      "epoch": 0.5667189952904239,
      "grad_norm": 4.162033557891846,
      "learning_rate": 4.964580062794349e-05,
      "loss": 0.8407,
      "step": 36100
    },
    {
      "epoch": 0.5682888540031397,
      "grad_norm": 4.386438846588135,
      "learning_rate": 4.964481946624804e-05,
      "loss": 0.7621,
      "step": 36200
    },
    {
      "epoch": 0.5698587127158555,
      "grad_norm": 5.598862648010254,
      "learning_rate": 4.964383830455259e-05,
      "loss": 0.8461,
      "step": 36300
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 4.293498992919922,
      "learning_rate": 4.964285714285715e-05,
      "loss": 0.8158,
      "step": 36400
    },
    {
      "epoch": 0.5729984301412873,
      "grad_norm": 5.179502964019775,
      "learning_rate": 4.964187598116169e-05,
      "loss": 0.7944,
      "step": 36500
    },
    {
      "epoch": 0.5745682888540031,
      "grad_norm": 5.081101894378662,
      "learning_rate": 4.964089481946625e-05,
      "loss": 0.812,
      "step": 36600
    },
    {
      "epoch": 0.576138147566719,
      "grad_norm": 4.162936687469482,
      "learning_rate": 4.96399136577708e-05,
      "loss": 0.8513,
      "step": 36700
    },
    {
      "epoch": 0.5777080062794349,
      "grad_norm": 4.518476486206055,
      "learning_rate": 4.963893249607536e-05,
      "loss": 0.8319,
      "step": 36800
    },
    {
      "epoch": 0.5792778649921507,
      "grad_norm": 4.620619297027588,
      "learning_rate": 4.9637951334379904e-05,
      "loss": 0.8174,
      "step": 36900
    },
    {
      "epoch": 0.5808477237048666,
      "grad_norm": 4.944860458374023,
      "learning_rate": 4.963697017268446e-05,
      "loss": 0.8304,
      "step": 37000
    },
    {
      "epoch": 0.5824175824175825,
      "grad_norm": 5.311035633087158,
      "learning_rate": 4.963598901098901e-05,
      "loss": 0.8002,
      "step": 37100
    },
    {
      "epoch": 0.5839874411302983,
      "grad_norm": 3.031092643737793,
      "learning_rate": 4.9635007849293564e-05,
      "loss": 0.7981,
      "step": 37200
    },
    {
      "epoch": 0.5855572998430141,
      "grad_norm": 4.561835765838623,
      "learning_rate": 4.9634026687598115e-05,
      "loss": 0.8199,
      "step": 37300
    },
    {
      "epoch": 0.5871271585557299,
      "grad_norm": 5.077752590179443,
      "learning_rate": 4.963304552590267e-05,
      "loss": 0.8566,
      "step": 37400
    },
    {
      "epoch": 0.5886970172684458,
      "grad_norm": 4.487312316894531,
      "learning_rate": 4.9632064364207224e-05,
      "loss": 0.843,
      "step": 37500
    },
    {
      "epoch": 0.5902668759811617,
      "grad_norm": 5.228646278381348,
      "learning_rate": 4.9631083202511774e-05,
      "loss": 0.8713,
      "step": 37600
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 4.024026393890381,
      "learning_rate": 4.9630102040816325e-05,
      "loss": 0.8337,
      "step": 37700
    },
    {
      "epoch": 0.5934065934065934,
      "grad_norm": 3.358938217163086,
      "learning_rate": 4.962912087912088e-05,
      "loss": 0.7748,
      "step": 37800
    },
    {
      "epoch": 0.5949764521193093,
      "grad_norm": 4.524643421173096,
      "learning_rate": 4.9628139717425434e-05,
      "loss": 0.8667,
      "step": 37900
    },
    {
      "epoch": 0.5965463108320251,
      "grad_norm": 4.950138568878174,
      "learning_rate": 4.9627158555729985e-05,
      "loss": 0.7894,
      "step": 38000
    },
    {
      "epoch": 0.598116169544741,
      "grad_norm": 3.9200289249420166,
      "learning_rate": 4.962617739403454e-05,
      "loss": 0.8162,
      "step": 38100
    },
    {
      "epoch": 0.5996860282574569,
      "grad_norm": 4.307744979858398,
      "learning_rate": 4.9625196232339094e-05,
      "loss": 0.8217,
      "step": 38200
    },
    {
      "epoch": 0.6012558869701727,
      "grad_norm": 4.435638427734375,
      "learning_rate": 4.9624215070643645e-05,
      "loss": 0.8476,
      "step": 38300
    },
    {
      "epoch": 0.6028257456828885,
      "grad_norm": 4.018533229827881,
      "learning_rate": 4.9623233908948196e-05,
      "loss": 0.8121,
      "step": 38400
    },
    {
      "epoch": 0.6043956043956044,
      "grad_norm": 5.029332160949707,
      "learning_rate": 4.9622252747252754e-05,
      "loss": 0.8214,
      "step": 38500
    },
    {
      "epoch": 0.6059654631083202,
      "grad_norm": 3.5009119510650635,
      "learning_rate": 4.96212715855573e-05,
      "loss": 0.8166,
      "step": 38600
    },
    {
      "epoch": 0.6075353218210361,
      "grad_norm": 5.744623184204102,
      "learning_rate": 4.9620290423861856e-05,
      "loss": 0.8259,
      "step": 38700
    },
    {
      "epoch": 0.609105180533752,
      "grad_norm": 5.421357154846191,
      "learning_rate": 4.9619309262166407e-05,
      "loss": 0.8646,
      "step": 38800
    },
    {
      "epoch": 0.6106750392464678,
      "grad_norm": 4.245697021484375,
      "learning_rate": 4.9618328100470964e-05,
      "loss": 0.7851,
      "step": 38900
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 4.575499057769775,
      "learning_rate": 4.961734693877551e-05,
      "loss": 0.7848,
      "step": 39000
    },
    {
      "epoch": 0.6138147566718996,
      "grad_norm": 3.751803159713745,
      "learning_rate": 4.9616365777080066e-05,
      "loss": 0.7976,
      "step": 39100
    },
    {
      "epoch": 0.6153846153846154,
      "grad_norm": 4.5400872230529785,
      "learning_rate": 4.961538461538462e-05,
      "loss": 0.7846,
      "step": 39200
    },
    {
      "epoch": 0.6169544740973313,
      "grad_norm": 4.120179653167725,
      "learning_rate": 4.961440345368917e-05,
      "loss": 0.7807,
      "step": 39300
    },
    {
      "epoch": 0.6185243328100472,
      "grad_norm": 3.5969455242156982,
      "learning_rate": 4.961342229199372e-05,
      "loss": 0.818,
      "step": 39400
    },
    {
      "epoch": 0.6200941915227629,
      "grad_norm": 3.901196241378784,
      "learning_rate": 4.961244113029828e-05,
      "loss": 0.8487,
      "step": 39500
    },
    {
      "epoch": 0.6216640502354788,
      "grad_norm": 2.93708872795105,
      "learning_rate": 4.961145996860283e-05,
      "loss": 0.8372,
      "step": 39600
    },
    {
      "epoch": 0.6232339089481946,
      "grad_norm": 3.949522018432617,
      "learning_rate": 4.961047880690738e-05,
      "loss": 0.7988,
      "step": 39700
    },
    {
      "epoch": 0.6248037676609105,
      "grad_norm": 4.949484825134277,
      "learning_rate": 4.960949764521193e-05,
      "loss": 0.8117,
      "step": 39800
    },
    {
      "epoch": 0.6263736263736264,
      "grad_norm": 4.781815052032471,
      "learning_rate": 4.960851648351649e-05,
      "loss": 0.822,
      "step": 39900
    },
    {
      "epoch": 0.6279434850863422,
      "grad_norm": 3.361798048019409,
      "learning_rate": 4.960753532182104e-05,
      "loss": 0.8034,
      "step": 40000
    },
    {
      "epoch": 0.6295133437990581,
      "grad_norm": 4.359850883483887,
      "learning_rate": 4.960655416012559e-05,
      "loss": 0.7836,
      "step": 40100
    },
    {
      "epoch": 0.631083202511774,
      "grad_norm": 4.516506195068359,
      "learning_rate": 4.960557299843015e-05,
      "loss": 0.868,
      "step": 40200
    },
    {
      "epoch": 0.6326530612244898,
      "grad_norm": 3.539855480194092,
      "learning_rate": 4.96045918367347e-05,
      "loss": 0.8483,
      "step": 40300
    },
    {
      "epoch": 0.6342229199372057,
      "grad_norm": 5.321126461029053,
      "learning_rate": 4.960361067503925e-05,
      "loss": 0.84,
      "step": 40400
    },
    {
      "epoch": 0.6357927786499215,
      "grad_norm": 5.189310550689697,
      "learning_rate": 4.96026295133438e-05,
      "loss": 0.8078,
      "step": 40500
    },
    {
      "epoch": 0.6373626373626373,
      "grad_norm": 3.439690351486206,
      "learning_rate": 4.960164835164836e-05,
      "loss": 0.8187,
      "step": 40600
    },
    {
      "epoch": 0.6389324960753532,
      "grad_norm": 4.296807765960693,
      "learning_rate": 4.96006671899529e-05,
      "loss": 0.8323,
      "step": 40700
    },
    {
      "epoch": 0.640502354788069,
      "grad_norm": 4.093379020690918,
      "learning_rate": 4.959968602825746e-05,
      "loss": 0.7848,
      "step": 40800
    },
    {
      "epoch": 0.6420722135007849,
      "grad_norm": 3.459275960922241,
      "learning_rate": 4.959870486656201e-05,
      "loss": 0.7959,
      "step": 40900
    },
    {
      "epoch": 0.6436420722135008,
      "grad_norm": 4.98042106628418,
      "learning_rate": 4.959772370486657e-05,
      "loss": 0.8374,
      "step": 41000
    },
    {
      "epoch": 0.6452119309262166,
      "grad_norm": 4.80870246887207,
      "learning_rate": 4.959674254317111e-05,
      "loss": 0.8332,
      "step": 41100
    },
    {
      "epoch": 0.6467817896389325,
      "grad_norm": 4.477703094482422,
      "learning_rate": 4.959576138147567e-05,
      "loss": 0.8272,
      "step": 41200
    },
    {
      "epoch": 0.6483516483516484,
      "grad_norm": 3.8427345752716064,
      "learning_rate": 4.959478021978022e-05,
      "loss": 0.822,
      "step": 41300
    },
    {
      "epoch": 0.6499215070643642,
      "grad_norm": 4.456334590911865,
      "learning_rate": 4.959379905808477e-05,
      "loss": 0.8548,
      "step": 41400
    },
    {
      "epoch": 0.6514913657770801,
      "grad_norm": 4.913059711456299,
      "learning_rate": 4.9592817896389324e-05,
      "loss": 0.8254,
      "step": 41500
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 4.521251201629639,
      "learning_rate": 4.959183673469388e-05,
      "loss": 0.8125,
      "step": 41600
    },
    {
      "epoch": 0.6546310832025117,
      "grad_norm": 4.1708879470825195,
      "learning_rate": 4.959085557299843e-05,
      "loss": 0.8006,
      "step": 41700
    },
    {
      "epoch": 0.6562009419152276,
      "grad_norm": 4.145576000213623,
      "learning_rate": 4.9589874411302983e-05,
      "loss": 0.8193,
      "step": 41800
    },
    {
      "epoch": 0.6577708006279435,
      "grad_norm": 4.786271095275879,
      "learning_rate": 4.9588893249607534e-05,
      "loss": 0.8135,
      "step": 41900
    },
    {
      "epoch": 0.6593406593406593,
      "grad_norm": 4.494139671325684,
      "learning_rate": 4.958791208791209e-05,
      "loss": 0.8265,
      "step": 42000
    },
    {
      "epoch": 0.6609105180533752,
      "grad_norm": 4.308200836181641,
      "learning_rate": 4.958693092621664e-05,
      "loss": 0.7893,
      "step": 42100
    },
    {
      "epoch": 0.6624803767660911,
      "grad_norm": 5.376277446746826,
      "learning_rate": 4.9585949764521194e-05,
      "loss": 0.7966,
      "step": 42200
    },
    {
      "epoch": 0.6640502354788069,
      "grad_norm": 3.929201126098633,
      "learning_rate": 4.958496860282575e-05,
      "loss": 0.8112,
      "step": 42300
    },
    {
      "epoch": 0.6656200941915228,
      "grad_norm": 4.213441848754883,
      "learning_rate": 4.95839874411303e-05,
      "loss": 0.8163,
      "step": 42400
    },
    {
      "epoch": 0.6671899529042387,
      "grad_norm": 4.674266338348389,
      "learning_rate": 4.9583006279434854e-05,
      "loss": 0.7799,
      "step": 42500
    },
    {
      "epoch": 0.6687598116169545,
      "grad_norm": 4.493956089019775,
      "learning_rate": 4.9582025117739405e-05,
      "loss": 0.8121,
      "step": 42600
    },
    {
      "epoch": 0.6703296703296703,
      "grad_norm": 4.126016616821289,
      "learning_rate": 4.958104395604396e-05,
      "loss": 0.7707,
      "step": 42700
    },
    {
      "epoch": 0.6718995290423861,
      "grad_norm": 4.450253486633301,
      "learning_rate": 4.958006279434851e-05,
      "loss": 0.7515,
      "step": 42800
    },
    {
      "epoch": 0.673469387755102,
      "grad_norm": 4.440205097198486,
      "learning_rate": 4.9579081632653065e-05,
      "loss": 0.8745,
      "step": 42900
    },
    {
      "epoch": 0.6750392464678179,
      "grad_norm": 4.758547306060791,
      "learning_rate": 4.9578100470957616e-05,
      "loss": 0.8015,
      "step": 43000
    },
    {
      "epoch": 0.6766091051805337,
      "grad_norm": 4.656100273132324,
      "learning_rate": 4.957711930926217e-05,
      "loss": 0.7766,
      "step": 43100
    },
    {
      "epoch": 0.6781789638932496,
      "grad_norm": 4.071404933929443,
      "learning_rate": 4.957613814756672e-05,
      "loss": 0.8269,
      "step": 43200
    },
    {
      "epoch": 0.6797488226059655,
      "grad_norm": 3.8937442302703857,
      "learning_rate": 4.9575156985871275e-05,
      "loss": 0.8191,
      "step": 43300
    },
    {
      "epoch": 0.6813186813186813,
      "grad_norm": 4.244994640350342,
      "learning_rate": 4.9574175824175826e-05,
      "loss": 0.7833,
      "step": 43400
    },
    {
      "epoch": 0.6828885400313972,
      "grad_norm": 4.834120750427246,
      "learning_rate": 4.957319466248038e-05,
      "loss": 0.8064,
      "step": 43500
    },
    {
      "epoch": 0.6844583987441131,
      "grad_norm": 4.144522666931152,
      "learning_rate": 4.957221350078493e-05,
      "loss": 0.7915,
      "step": 43600
    },
    {
      "epoch": 0.6860282574568289,
      "grad_norm": 3.895869731903076,
      "learning_rate": 4.9571232339089486e-05,
      "loss": 0.8127,
      "step": 43700
    },
    {
      "epoch": 0.6875981161695447,
      "grad_norm": 4.47464656829834,
      "learning_rate": 4.957025117739404e-05,
      "loss": 0.8023,
      "step": 43800
    },
    {
      "epoch": 0.6891679748822606,
      "grad_norm": 4.930471897125244,
      "learning_rate": 4.956927001569859e-05,
      "loss": 0.7874,
      "step": 43900
    },
    {
      "epoch": 0.6907378335949764,
      "grad_norm": 4.672404766082764,
      "learning_rate": 4.956828885400314e-05,
      "loss": 0.7856,
      "step": 44000
    },
    {
      "epoch": 0.6923076923076923,
      "grad_norm": 4.356161117553711,
      "learning_rate": 4.95673076923077e-05,
      "loss": 0.8273,
      "step": 44100
    },
    {
      "epoch": 0.6938775510204082,
      "grad_norm": 4.0739359855651855,
      "learning_rate": 4.956632653061225e-05,
      "loss": 0.8137,
      "step": 44200
    },
    {
      "epoch": 0.695447409733124,
      "grad_norm": 2.1109395027160645,
      "learning_rate": 4.95653453689168e-05,
      "loss": 0.8099,
      "step": 44300
    },
    {
      "epoch": 0.6970172684458399,
      "grad_norm": 4.235645294189453,
      "learning_rate": 4.9564364207221356e-05,
      "loss": 0.8275,
      "step": 44400
    },
    {
      "epoch": 0.6985871271585558,
      "grad_norm": 5.277720928192139,
      "learning_rate": 4.956338304552591e-05,
      "loss": 0.8264,
      "step": 44500
    },
    {
      "epoch": 0.7001569858712716,
      "grad_norm": 3.993408203125,
      "learning_rate": 4.956240188383046e-05,
      "loss": 0.775,
      "step": 44600
    },
    {
      "epoch": 0.7017268445839875,
      "grad_norm": 4.822939872741699,
      "learning_rate": 4.956142072213501e-05,
      "loss": 0.7333,
      "step": 44700
    },
    {
      "epoch": 0.7032967032967034,
      "grad_norm": 5.109651565551758,
      "learning_rate": 4.956043956043957e-05,
      "loss": 0.807,
      "step": 44800
    },
    {
      "epoch": 0.7048665620094191,
      "grad_norm": 4.1397905349731445,
      "learning_rate": 4.955945839874411e-05,
      "loss": 0.7714,
      "step": 44900
    },
    {
      "epoch": 0.706436420722135,
      "grad_norm": 3.3680715560913086,
      "learning_rate": 4.955847723704867e-05,
      "loss": 0.8025,
      "step": 45000
    },
    {
      "epoch": 0.7080062794348508,
      "grad_norm": 3.9567527770996094,
      "learning_rate": 4.955749607535322e-05,
      "loss": 0.8215,
      "step": 45100
    },
    {
      "epoch": 0.7095761381475667,
      "grad_norm": 4.196969985961914,
      "learning_rate": 4.955651491365778e-05,
      "loss": 0.7799,
      "step": 45200
    },
    {
      "epoch": 0.7111459968602826,
      "grad_norm": 4.471611499786377,
      "learning_rate": 4.955553375196232e-05,
      "loss": 0.829,
      "step": 45300
    },
    {
      "epoch": 0.7127158555729984,
      "grad_norm": 3.9522032737731934,
      "learning_rate": 4.955455259026688e-05,
      "loss": 0.7936,
      "step": 45400
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 4.300734996795654,
      "learning_rate": 4.955357142857143e-05,
      "loss": 0.7712,
      "step": 45500
    },
    {
      "epoch": 0.7158555729984302,
      "grad_norm": 5.2931060791015625,
      "learning_rate": 4.955259026687598e-05,
      "loss": 0.8279,
      "step": 45600
    },
    {
      "epoch": 0.717425431711146,
      "grad_norm": 4.370993614196777,
      "learning_rate": 4.955160910518053e-05,
      "loss": 0.8486,
      "step": 45700
    },
    {
      "epoch": 0.7189952904238619,
      "grad_norm": 4.842863082885742,
      "learning_rate": 4.955062794348509e-05,
      "loss": 0.8389,
      "step": 45800
    },
    {
      "epoch": 0.7205651491365777,
      "grad_norm": 4.148212432861328,
      "learning_rate": 4.954964678178964e-05,
      "loss": 0.7821,
      "step": 45900
    },
    {
      "epoch": 0.7221350078492935,
      "grad_norm": 4.358235836029053,
      "learning_rate": 4.954866562009419e-05,
      "loss": 0.8045,
      "step": 46000
    },
    {
      "epoch": 0.7237048665620094,
      "grad_norm": 4.371609210968018,
      "learning_rate": 4.9547684458398743e-05,
      "loss": 0.8332,
      "step": 46100
    },
    {
      "epoch": 0.7252747252747253,
      "grad_norm": 5.273245334625244,
      "learning_rate": 4.95467032967033e-05,
      "loss": 0.766,
      "step": 46200
    },
    {
      "epoch": 0.7268445839874411,
      "grad_norm": 4.022218227386475,
      "learning_rate": 4.954572213500785e-05,
      "loss": 0.7717,
      "step": 46300
    },
    {
      "epoch": 0.728414442700157,
      "grad_norm": 4.915586471557617,
      "learning_rate": 4.95447409733124e-05,
      "loss": 0.7923,
      "step": 46400
    },
    {
      "epoch": 0.7299843014128728,
      "grad_norm": 4.439593315124512,
      "learning_rate": 4.954375981161696e-05,
      "loss": 0.7934,
      "step": 46500
    },
    {
      "epoch": 0.7315541601255887,
      "grad_norm": 4.911107540130615,
      "learning_rate": 4.954277864992151e-05,
      "loss": 0.7876,
      "step": 46600
    },
    {
      "epoch": 0.7331240188383046,
      "grad_norm": 4.947508335113525,
      "learning_rate": 4.954179748822606e-05,
      "loss": 0.7893,
      "step": 46700
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 4.673035144805908,
      "learning_rate": 4.9540816326530614e-05,
      "loss": 0.8009,
      "step": 46800
    },
    {
      "epoch": 0.7362637362637363,
      "grad_norm": 4.094566822052002,
      "learning_rate": 4.953983516483517e-05,
      "loss": 0.8034,
      "step": 46900
    },
    {
      "epoch": 0.7378335949764521,
      "grad_norm": 4.076046943664551,
      "learning_rate": 4.9538854003139716e-05,
      "loss": 0.8137,
      "step": 47000
    },
    {
      "epoch": 0.7394034536891679,
      "grad_norm": 5.077541351318359,
      "learning_rate": 4.9537872841444274e-05,
      "loss": 0.8119,
      "step": 47100
    },
    {
      "epoch": 0.7409733124018838,
      "grad_norm": 4.571070671081543,
      "learning_rate": 4.9536891679748825e-05,
      "loss": 0.8355,
      "step": 47200
    },
    {
      "epoch": 0.7425431711145997,
      "grad_norm": 3.5037777423858643,
      "learning_rate": 4.953591051805338e-05,
      "loss": 0.8378,
      "step": 47300
    },
    {
      "epoch": 0.7441130298273155,
      "grad_norm": 5.107666969299316,
      "learning_rate": 4.9534929356357927e-05,
      "loss": 0.8117,
      "step": 47400
    },
    {
      "epoch": 0.7456828885400314,
      "grad_norm": 4.243663311004639,
      "learning_rate": 4.9533948194662484e-05,
      "loss": 0.8038,
      "step": 47500
    },
    {
      "epoch": 0.7472527472527473,
      "grad_norm": 4.997926235198975,
      "learning_rate": 4.9532967032967035e-05,
      "loss": 0.8185,
      "step": 47600
    },
    {
      "epoch": 0.7488226059654631,
      "grad_norm": 3.7149832248687744,
      "learning_rate": 4.9531985871271586e-05,
      "loss": 0.8671,
      "step": 47700
    },
    {
      "epoch": 0.750392464678179,
      "grad_norm": 4.669812202453613,
      "learning_rate": 4.953100470957614e-05,
      "loss": 0.7762,
      "step": 47800
    },
    {
      "epoch": 0.7519623233908949,
      "grad_norm": 4.226022720336914,
      "learning_rate": 4.9530023547880695e-05,
      "loss": 0.8049,
      "step": 47900
    },
    {
      "epoch": 0.7535321821036107,
      "grad_norm": 4.629610538482666,
      "learning_rate": 4.9529042386185246e-05,
      "loss": 0.7928,
      "step": 48000
    },
    {
      "epoch": 0.7551020408163265,
      "grad_norm": 4.241724491119385,
      "learning_rate": 4.95280612244898e-05,
      "loss": 0.8252,
      "step": 48100
    },
    {
      "epoch": 0.7566718995290423,
      "grad_norm": 3.424590587615967,
      "learning_rate": 4.952708006279435e-05,
      "loss": 0.7769,
      "step": 48200
    },
    {
      "epoch": 0.7582417582417582,
      "grad_norm": 3.6467349529266357,
      "learning_rate": 4.9526098901098906e-05,
      "loss": 0.7843,
      "step": 48300
    },
    {
      "epoch": 0.7598116169544741,
      "grad_norm": 4.299863815307617,
      "learning_rate": 4.952511773940346e-05,
      "loss": 0.7767,
      "step": 48400
    },
    {
      "epoch": 0.7613814756671899,
      "grad_norm": 4.1070661544799805,
      "learning_rate": 4.952413657770801e-05,
      "loss": 0.8278,
      "step": 48500
    },
    {
      "epoch": 0.7629513343799058,
      "grad_norm": 4.720722675323486,
      "learning_rate": 4.9523155416012565e-05,
      "loss": 0.7506,
      "step": 48600
    },
    {
      "epoch": 0.7645211930926217,
      "grad_norm": 4.6446123123168945,
      "learning_rate": 4.9522174254317116e-05,
      "loss": 0.7004,
      "step": 48700
    },
    {
      "epoch": 0.7660910518053375,
      "grad_norm": 5.357848644256592,
      "learning_rate": 4.952119309262167e-05,
      "loss": 0.7723,
      "step": 48800
    },
    {
      "epoch": 0.7676609105180534,
      "grad_norm": 4.033183574676514,
      "learning_rate": 4.952021193092622e-05,
      "loss": 0.75,
      "step": 48900
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 5.214511871337891,
      "learning_rate": 4.9519230769230776e-05,
      "loss": 0.7769,
      "step": 49000
    },
    {
      "epoch": 0.7708006279434851,
      "grad_norm": 4.026225566864014,
      "learning_rate": 4.951824960753532e-05,
      "loss": 0.7904,
      "step": 49100
    },
    {
      "epoch": 0.7723704866562009,
      "grad_norm": 4.4892168045043945,
      "learning_rate": 4.951726844583988e-05,
      "loss": 0.8261,
      "step": 49200
    },
    {
      "epoch": 0.7739403453689168,
      "grad_norm": 4.645023345947266,
      "learning_rate": 4.951628728414443e-05,
      "loss": 0.8058,
      "step": 49300
    },
    {
      "epoch": 0.7755102040816326,
      "grad_norm": 2.8961074352264404,
      "learning_rate": 4.951530612244899e-05,
      "loss": 0.7559,
      "step": 49400
    },
    {
      "epoch": 0.7770800627943485,
      "grad_norm": 4.794370651245117,
      "learning_rate": 4.951432496075353e-05,
      "loss": 0.8189,
      "step": 49500
    },
    {
      "epoch": 0.7786499215070644,
      "grad_norm": 4.671910762786865,
      "learning_rate": 4.951334379905809e-05,
      "loss": 0.8214,
      "step": 49600
    },
    {
      "epoch": 0.7802197802197802,
      "grad_norm": 5.2236504554748535,
      "learning_rate": 4.951236263736264e-05,
      "loss": 0.7938,
      "step": 49700
    },
    {
      "epoch": 0.7817896389324961,
      "grad_norm": 5.531501770019531,
      "learning_rate": 4.951138147566719e-05,
      "loss": 0.8107,
      "step": 49800
    },
    {
      "epoch": 0.783359497645212,
      "grad_norm": 4.76591157913208,
      "learning_rate": 4.951040031397174e-05,
      "loss": 0.7694,
      "step": 49900
    },
    {
      "epoch": 0.7849293563579278,
      "grad_norm": 4.873288154602051,
      "learning_rate": 4.95094191522763e-05,
      "loss": 0.7827,
      "step": 50000
    },
    {
      "epoch": 0.7864992150706437,
      "grad_norm": 5.4819488525390625,
      "learning_rate": 4.950843799058085e-05,
      "loss": 0.797,
      "step": 50100
    },
    {
      "epoch": 0.7880690737833596,
      "grad_norm": 3.6167404651641846,
      "learning_rate": 4.95074568288854e-05,
      "loss": 0.7652,
      "step": 50200
    },
    {
      "epoch": 0.7896389324960753,
      "grad_norm": 4.158849239349365,
      "learning_rate": 4.950647566718995e-05,
      "loss": 0.7945,
      "step": 50300
    },
    {
      "epoch": 0.7912087912087912,
      "grad_norm": 4.741782188415527,
      "learning_rate": 4.950549450549451e-05,
      "loss": 0.7583,
      "step": 50400
    },
    {
      "epoch": 0.792778649921507,
      "grad_norm": 3.977769374847412,
      "learning_rate": 4.950451334379906e-05,
      "loss": 0.8326,
      "step": 50500
    },
    {
      "epoch": 0.7943485086342229,
      "grad_norm": 5.568187236785889,
      "learning_rate": 4.950353218210361e-05,
      "loss": 0.8325,
      "step": 50600
    },
    {
      "epoch": 0.7959183673469388,
      "grad_norm": 4.070608139038086,
      "learning_rate": 4.950255102040817e-05,
      "loss": 0.8095,
      "step": 50700
    },
    {
      "epoch": 0.7974882260596546,
      "grad_norm": 4.806958198547363,
      "learning_rate": 4.950156985871272e-05,
      "loss": 0.8134,
      "step": 50800
    },
    {
      "epoch": 0.7990580847723705,
      "grad_norm": 4.121597766876221,
      "learning_rate": 4.950058869701727e-05,
      "loss": 0.7683,
      "step": 50900
    },
    {
      "epoch": 0.8006279434850864,
      "grad_norm": 4.441647052764893,
      "learning_rate": 4.949960753532182e-05,
      "loss": 0.7578,
      "step": 51000
    },
    {
      "epoch": 0.8021978021978022,
      "grad_norm": 5.335739612579346,
      "learning_rate": 4.949862637362638e-05,
      "loss": 0.7598,
      "step": 51100
    },
    {
      "epoch": 0.8037676609105181,
      "grad_norm": 4.13727331161499,
      "learning_rate": 4.9497645211930925e-05,
      "loss": 0.7347,
      "step": 51200
    },
    {
      "epoch": 0.8053375196232339,
      "grad_norm": 4.671699047088623,
      "learning_rate": 4.949666405023548e-05,
      "loss": 0.847,
      "step": 51300
    },
    {
      "epoch": 0.8069073783359497,
      "grad_norm": 4.457029342651367,
      "learning_rate": 4.9495682888540034e-05,
      "loss": 0.7928,
      "step": 51400
    },
    {
      "epoch": 0.8084772370486656,
      "grad_norm": 4.218601703643799,
      "learning_rate": 4.949470172684459e-05,
      "loss": 0.779,
      "step": 51500
    },
    {
      "epoch": 0.8100470957613815,
      "grad_norm": 3.66839861869812,
      "learning_rate": 4.9493720565149135e-05,
      "loss": 0.7546,
      "step": 51600
    },
    {
      "epoch": 0.8116169544740973,
      "grad_norm": 4.08121919631958,
      "learning_rate": 4.949273940345369e-05,
      "loss": 0.7986,
      "step": 51700
    },
    {
      "epoch": 0.8131868131868132,
      "grad_norm": 4.505765438079834,
      "learning_rate": 4.9491758241758244e-05,
      "loss": 0.793,
      "step": 51800
    },
    {
      "epoch": 0.814756671899529,
      "grad_norm": 4.203800201416016,
      "learning_rate": 4.9490777080062795e-05,
      "loss": 0.7912,
      "step": 51900
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 5.390296936035156,
      "learning_rate": 4.9489795918367346e-05,
      "loss": 0.7601,
      "step": 52000
    },
    {
      "epoch": 0.8178963893249608,
      "grad_norm": 3.9066529273986816,
      "learning_rate": 4.9488814756671904e-05,
      "loss": 0.7815,
      "step": 52100
    },
    {
      "epoch": 0.8194662480376766,
      "grad_norm": 4.366098880767822,
      "learning_rate": 4.9487833594976455e-05,
      "loss": 0.8108,
      "step": 52200
    },
    {
      "epoch": 0.8210361067503925,
      "grad_norm": 4.602433204650879,
      "learning_rate": 4.9486852433281006e-05,
      "loss": 0.7757,
      "step": 52300
    },
    {
      "epoch": 0.8226059654631083,
      "grad_norm": 3.335156202316284,
      "learning_rate": 4.948587127158556e-05,
      "loss": 0.7878,
      "step": 52400
    },
    {
      "epoch": 0.8241758241758241,
      "grad_norm": 4.690138339996338,
      "learning_rate": 4.9484890109890115e-05,
      "loss": 0.7747,
      "step": 52500
    },
    {
      "epoch": 0.82574568288854,
      "grad_norm": 5.14065408706665,
      "learning_rate": 4.9483908948194666e-05,
      "loss": 0.7788,
      "step": 52600
    },
    {
      "epoch": 0.8273155416012559,
      "grad_norm": 4.856861114501953,
      "learning_rate": 4.948292778649922e-05,
      "loss": 0.7745,
      "step": 52700
    },
    {
      "epoch": 0.8288854003139717,
      "grad_norm": 4.157369613647461,
      "learning_rate": 4.9481946624803774e-05,
      "loss": 0.8036,
      "step": 52800
    },
    {
      "epoch": 0.8304552590266876,
      "grad_norm": 4.134673595428467,
      "learning_rate": 4.9480965463108325e-05,
      "loss": 0.7528,
      "step": 52900
    },
    {
      "epoch": 0.8320251177394035,
      "grad_norm": 4.87172794342041,
      "learning_rate": 4.9479984301412876e-05,
      "loss": 0.7944,
      "step": 53000
    },
    {
      "epoch": 0.8335949764521193,
      "grad_norm": 4.153871536254883,
      "learning_rate": 4.947900313971743e-05,
      "loss": 0.8089,
      "step": 53100
    },
    {
      "epoch": 0.8351648351648352,
      "grad_norm": 4.635761260986328,
      "learning_rate": 4.9478021978021985e-05,
      "loss": 0.8064,
      "step": 53200
    },
    {
      "epoch": 0.8367346938775511,
      "grad_norm": 3.9576518535614014,
      "learning_rate": 4.947704081632653e-05,
      "loss": 0.8609,
      "step": 53300
    },
    {
      "epoch": 0.8383045525902669,
      "grad_norm": 4.521429061889648,
      "learning_rate": 4.947605965463109e-05,
      "loss": 0.7973,
      "step": 53400
    },
    {
      "epoch": 0.8398744113029827,
      "grad_norm": 4.4791975021362305,
      "learning_rate": 4.947507849293564e-05,
      "loss": 0.7724,
      "step": 53500
    },
    {
      "epoch": 0.8414442700156985,
      "grad_norm": 4.351619243621826,
      "learning_rate": 4.9474097331240196e-05,
      "loss": 0.7904,
      "step": 53600
    },
    {
      "epoch": 0.8430141287284144,
      "grad_norm": 4.2489542961120605,
      "learning_rate": 4.947311616954474e-05,
      "loss": 0.8046,
      "step": 53700
    },
    {
      "epoch": 0.8445839874411303,
      "grad_norm": 4.700350761413574,
      "learning_rate": 4.94721350078493e-05,
      "loss": 0.7727,
      "step": 53800
    },
    {
      "epoch": 0.8461538461538461,
      "grad_norm": 4.180152416229248,
      "learning_rate": 4.947115384615385e-05,
      "loss": 0.7657,
      "step": 53900
    },
    {
      "epoch": 0.847723704866562,
      "grad_norm": 3.6030893325805664,
      "learning_rate": 4.94701726844584e-05,
      "loss": 0.7973,
      "step": 54000
    },
    {
      "epoch": 0.8492935635792779,
      "grad_norm": 3.358302593231201,
      "learning_rate": 4.946919152276295e-05,
      "loss": 0.7205,
      "step": 54100
    },
    {
      "epoch": 0.8508634222919937,
      "grad_norm": 4.1973981857299805,
      "learning_rate": 4.946821036106751e-05,
      "loss": 0.7839,
      "step": 54200
    },
    {
      "epoch": 0.8524332810047096,
      "grad_norm": 3.852498769760132,
      "learning_rate": 4.946722919937206e-05,
      "loss": 0.8525,
      "step": 54300
    },
    {
      "epoch": 0.8540031397174255,
      "grad_norm": 3.99365234375,
      "learning_rate": 4.946624803767661e-05,
      "loss": 0.8019,
      "step": 54400
    },
    {
      "epoch": 0.8555729984301413,
      "grad_norm": 4.403565883636475,
      "learning_rate": 4.946526687598116e-05,
      "loss": 0.8038,
      "step": 54500
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 4.237264156341553,
      "learning_rate": 4.946428571428572e-05,
      "loss": 0.7828,
      "step": 54600
    },
    {
      "epoch": 0.858712715855573,
      "grad_norm": 4.9246602058410645,
      "learning_rate": 4.946330455259027e-05,
      "loss": 0.7605,
      "step": 54700
    },
    {
      "epoch": 0.8602825745682888,
      "grad_norm": 4.106598854064941,
      "learning_rate": 4.946232339089482e-05,
      "loss": 0.777,
      "step": 54800
    },
    {
      "epoch": 0.8618524332810047,
      "grad_norm": 4.8271164894104,
      "learning_rate": 4.946134222919938e-05,
      "loss": 0.8079,
      "step": 54900
    },
    {
      "epoch": 0.8634222919937206,
      "grad_norm": 4.403468132019043,
      "learning_rate": 4.946036106750393e-05,
      "loss": 0.7984,
      "step": 55000
    },
    {
      "epoch": 0.8649921507064364,
      "grad_norm": 3.2624902725219727,
      "learning_rate": 4.945937990580848e-05,
      "loss": 0.803,
      "step": 55100
    },
    {
      "epoch": 0.8665620094191523,
      "grad_norm": 5.10495662689209,
      "learning_rate": 4.945839874411303e-05,
      "loss": 0.8237,
      "step": 55200
    },
    {
      "epoch": 0.8681318681318682,
      "grad_norm": 4.667323112487793,
      "learning_rate": 4.945741758241759e-05,
      "loss": 0.7933,
      "step": 55300
    },
    {
      "epoch": 0.869701726844584,
      "grad_norm": 4.930912017822266,
      "learning_rate": 4.9456436420722134e-05,
      "loss": 0.82,
      "step": 55400
    },
    {
      "epoch": 0.8712715855572999,
      "grad_norm": 4.054489612579346,
      "learning_rate": 4.945545525902669e-05,
      "loss": 0.8034,
      "step": 55500
    },
    {
      "epoch": 0.8728414442700158,
      "grad_norm": 3.563081979751587,
      "learning_rate": 4.945447409733124e-05,
      "loss": 0.7616,
      "step": 55600
    },
    {
      "epoch": 0.8744113029827315,
      "grad_norm": 3.8549752235412598,
      "learning_rate": 4.94534929356358e-05,
      "loss": 0.8263,
      "step": 55700
    },
    {
      "epoch": 0.8759811616954474,
      "grad_norm": 3.4894638061523438,
      "learning_rate": 4.9452511773940344e-05,
      "loss": 0.823,
      "step": 55800
    },
    {
      "epoch": 0.8775510204081632,
      "grad_norm": 4.764000415802002,
      "learning_rate": 4.94515306122449e-05,
      "loss": 0.7905,
      "step": 55900
    },
    {
      "epoch": 0.8791208791208791,
      "grad_norm": 4.222670078277588,
      "learning_rate": 4.945054945054945e-05,
      "loss": 0.7444,
      "step": 56000
    },
    {
      "epoch": 0.880690737833595,
      "grad_norm": 4.4973931312561035,
      "learning_rate": 4.9449568288854004e-05,
      "loss": 0.7677,
      "step": 56100
    },
    {
      "epoch": 0.8822605965463108,
      "grad_norm": 3.717649221420288,
      "learning_rate": 4.9448587127158555e-05,
      "loss": 0.7503,
      "step": 56200
    },
    {
      "epoch": 0.8838304552590267,
      "grad_norm": 3.1822550296783447,
      "learning_rate": 4.944760596546311e-05,
      "loss": 0.7966,
      "step": 56300
    },
    {
      "epoch": 0.8854003139717426,
      "grad_norm": 4.353942394256592,
      "learning_rate": 4.9446624803767664e-05,
      "loss": 0.7831,
      "step": 56400
    },
    {
      "epoch": 0.8869701726844584,
      "grad_norm": 5.910861015319824,
      "learning_rate": 4.9445643642072215e-05,
      "loss": 0.7666,
      "step": 56500
    },
    {
      "epoch": 0.8885400313971743,
      "grad_norm": 4.901097297668457,
      "learning_rate": 4.9444662480376766e-05,
      "loss": 0.8425,
      "step": 56600
    },
    {
      "epoch": 0.8901098901098901,
      "grad_norm": 4.522639751434326,
      "learning_rate": 4.9443681318681324e-05,
      "loss": 0.7548,
      "step": 56700
    },
    {
      "epoch": 0.8916797488226059,
      "grad_norm": 4.1565351486206055,
      "learning_rate": 4.9442700156985875e-05,
      "loss": 0.8211,
      "step": 56800
    },
    {
      "epoch": 0.8932496075353218,
      "grad_norm": 3.9302752017974854,
      "learning_rate": 4.9441718995290426e-05,
      "loss": 0.7799,
      "step": 56900
    },
    {
      "epoch": 0.8948194662480377,
      "grad_norm": 4.499650478363037,
      "learning_rate": 4.944073783359498e-05,
      "loss": 0.7907,
      "step": 57000
    },
    {
      "epoch": 0.8963893249607535,
      "grad_norm": 4.019600868225098,
      "learning_rate": 4.943975667189953e-05,
      "loss": 0.7824,
      "step": 57100
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 4.688340187072754,
      "learning_rate": 4.9438775510204085e-05,
      "loss": 0.7783,
      "step": 57200
    },
    {
      "epoch": 0.8995290423861853,
      "grad_norm": 3.7313051223754883,
      "learning_rate": 4.9437794348508636e-05,
      "loss": 0.8106,
      "step": 57300
    },
    {
      "epoch": 0.9010989010989011,
      "grad_norm": 4.158056259155273,
      "learning_rate": 4.9436813186813194e-05,
      "loss": 0.7987,
      "step": 57400
    },
    {
      "epoch": 0.902668759811617,
      "grad_norm": 4.418284893035889,
      "learning_rate": 4.943583202511774e-05,
      "loss": 0.7892,
      "step": 57500
    },
    {
      "epoch": 0.9042386185243328,
      "grad_norm": 4.428102970123291,
      "learning_rate": 4.9434850863422296e-05,
      "loss": 0.8078,
      "step": 57600
    },
    {
      "epoch": 0.9058084772370487,
      "grad_norm": 4.241779804229736,
      "learning_rate": 4.943386970172685e-05,
      "loss": 0.7727,
      "step": 57700
    },
    {
      "epoch": 0.9073783359497645,
      "grad_norm": 4.942924499511719,
      "learning_rate": 4.94328885400314e-05,
      "loss": 0.7971,
      "step": 57800
    },
    {
      "epoch": 0.9089481946624803,
      "grad_norm": 3.3066787719726562,
      "learning_rate": 4.943190737833595e-05,
      "loss": 0.8168,
      "step": 57900
    },
    {
      "epoch": 0.9105180533751962,
      "grad_norm": 3.420506238937378,
      "learning_rate": 4.943092621664051e-05,
      "loss": 0.8062,
      "step": 58000
    },
    {
      "epoch": 0.9120879120879121,
      "grad_norm": 4.296144008636475,
      "learning_rate": 4.942994505494506e-05,
      "loss": 0.7612,
      "step": 58100
    },
    {
      "epoch": 0.9136577708006279,
      "grad_norm": 5.026120662689209,
      "learning_rate": 4.942896389324961e-05,
      "loss": 0.8044,
      "step": 58200
    },
    {
      "epoch": 0.9152276295133438,
      "grad_norm": 5.829637050628662,
      "learning_rate": 4.942798273155416e-05,
      "loss": 0.7803,
      "step": 58300
    },
    {
      "epoch": 0.9167974882260597,
      "grad_norm": 4.7253828048706055,
      "learning_rate": 4.942700156985872e-05,
      "loss": 0.7989,
      "step": 58400
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 3.320732593536377,
      "learning_rate": 4.942602040816326e-05,
      "loss": 0.7889,
      "step": 58500
    },
    {
      "epoch": 0.9199372056514914,
      "grad_norm": 4.702397346496582,
      "learning_rate": 4.942503924646782e-05,
      "loss": 0.7598,
      "step": 58600
    },
    {
      "epoch": 0.9215070643642073,
      "grad_norm": 4.777096748352051,
      "learning_rate": 4.942405808477237e-05,
      "loss": 0.7753,
      "step": 58700
    },
    {
      "epoch": 0.9230769230769231,
      "grad_norm": 4.505520343780518,
      "learning_rate": 4.942307692307693e-05,
      "loss": 0.7853,
      "step": 58800
    },
    {
      "epoch": 0.9246467817896389,
      "grad_norm": 4.8396100997924805,
      "learning_rate": 4.942209576138148e-05,
      "loss": 0.8338,
      "step": 58900
    },
    {
      "epoch": 0.9262166405023547,
      "grad_norm": 4.810981273651123,
      "learning_rate": 4.942111459968603e-05,
      "loss": 0.7613,
      "step": 59000
    },
    {
      "epoch": 0.9277864992150706,
      "grad_norm": 5.50040340423584,
      "learning_rate": 4.942013343799059e-05,
      "loss": 0.7654,
      "step": 59100
    },
    {
      "epoch": 0.9293563579277865,
      "grad_norm": 4.58488130569458,
      "learning_rate": 4.941915227629513e-05,
      "loss": 0.7779,
      "step": 59200
    },
    {
      "epoch": 0.9309262166405023,
      "grad_norm": 4.215264797210693,
      "learning_rate": 4.941817111459969e-05,
      "loss": 0.8098,
      "step": 59300
    },
    {
      "epoch": 0.9324960753532182,
      "grad_norm": 4.275949001312256,
      "learning_rate": 4.941718995290424e-05,
      "loss": 0.7862,
      "step": 59400
    },
    {
      "epoch": 0.9340659340659341,
      "grad_norm": 5.319999694824219,
      "learning_rate": 4.94162087912088e-05,
      "loss": 0.7775,
      "step": 59500
    },
    {
      "epoch": 0.9356357927786499,
      "grad_norm": 4.717414379119873,
      "learning_rate": 4.941522762951334e-05,
      "loss": 0.8161,
      "step": 59600
    },
    {
      "epoch": 0.9372056514913658,
      "grad_norm": 3.8800721168518066,
      "learning_rate": 4.94142464678179e-05,
      "loss": 0.8283,
      "step": 59700
    },
    {
      "epoch": 0.9387755102040817,
      "grad_norm": 5.022037982940674,
      "learning_rate": 4.941326530612245e-05,
      "loss": 0.7762,
      "step": 59800
    },
    {
      "epoch": 0.9403453689167975,
      "grad_norm": 5.103584289550781,
      "learning_rate": 4.9412284144427e-05,
      "loss": 0.821,
      "step": 59900
    },
    {
      "epoch": 0.9419152276295133,
      "grad_norm": 4.27816104888916,
      "learning_rate": 4.9411302982731553e-05,
      "loss": 0.8195,
      "step": 60000
    },
    {
      "epoch": 0.9434850863422292,
      "grad_norm": 4.77872896194458,
      "learning_rate": 4.941032182103611e-05,
      "loss": 0.772,
      "step": 60100
    },
    {
      "epoch": 0.945054945054945,
      "grad_norm": 4.634819030761719,
      "learning_rate": 4.940934065934066e-05,
      "loss": 0.7698,
      "step": 60200
    },
    {
      "epoch": 0.9466248037676609,
      "grad_norm": 4.479843616485596,
      "learning_rate": 4.940835949764521e-05,
      "loss": 0.7999,
      "step": 60300
    },
    {
      "epoch": 0.9481946624803768,
      "grad_norm": 4.159773826599121,
      "learning_rate": 4.9407378335949764e-05,
      "loss": 0.8096,
      "step": 60400
    },
    {
      "epoch": 0.9497645211930926,
      "grad_norm": 3.958665132522583,
      "learning_rate": 4.940639717425432e-05,
      "loss": 0.7757,
      "step": 60500
    },
    {
      "epoch": 0.9513343799058085,
      "grad_norm": 4.86243200302124,
      "learning_rate": 4.9405416012558866e-05,
      "loss": 0.8037,
      "step": 60600
    },
    {
      "epoch": 0.9529042386185244,
      "grad_norm": 3.2407829761505127,
      "learning_rate": 4.9404434850863424e-05,
      "loss": 0.7823,
      "step": 60700
    },
    {
      "epoch": 0.9544740973312402,
      "grad_norm": 5.047921180725098,
      "learning_rate": 4.9403453689167975e-05,
      "loss": 0.8297,
      "step": 60800
    },
    {
      "epoch": 0.9560439560439561,
      "grad_norm": 4.508062362670898,
      "learning_rate": 4.940247252747253e-05,
      "loss": 0.7687,
      "step": 60900
    },
    {
      "epoch": 0.957613814756672,
      "grad_norm": 5.365232944488525,
      "learning_rate": 4.9401491365777084e-05,
      "loss": 0.7831,
      "step": 61000
    },
    {
      "epoch": 0.9591836734693877,
      "grad_norm": 4.590469837188721,
      "learning_rate": 4.9400510204081635e-05,
      "loss": 0.7506,
      "step": 61100
    },
    {
      "epoch": 0.9607535321821036,
      "grad_norm": 3.511080026626587,
      "learning_rate": 4.9399529042386186e-05,
      "loss": 0.7638,
      "step": 61200
    },
    {
      "epoch": 0.9623233908948194,
      "grad_norm": 4.74686861038208,
      "learning_rate": 4.9398547880690737e-05,
      "loss": 0.7635,
      "step": 61300
    },
    {
      "epoch": 0.9638932496075353,
      "grad_norm": 3.861149549484253,
      "learning_rate": 4.9397566718995294e-05,
      "loss": 0.7961,
      "step": 61400
    },
    {
      "epoch": 0.9654631083202512,
      "grad_norm": 3.756634473800659,
      "learning_rate": 4.9396585557299845e-05,
      "loss": 0.7729,
      "step": 61500
    },
    {
      "epoch": 0.967032967032967,
      "grad_norm": 4.768380641937256,
      "learning_rate": 4.93956043956044e-05,
      "loss": 0.7465,
      "step": 61600
    },
    {
      "epoch": 0.9686028257456829,
      "grad_norm": 4.977330207824707,
      "learning_rate": 4.939462323390895e-05,
      "loss": 0.7554,
      "step": 61700
    },
    {
      "epoch": 0.9701726844583988,
      "grad_norm": 3.89259934425354,
      "learning_rate": 4.9393642072213505e-05,
      "loss": 0.7631,
      "step": 61800
    },
    {
      "epoch": 0.9717425431711146,
      "grad_norm": 4.589770317077637,
      "learning_rate": 4.9392660910518056e-05,
      "loss": 0.7691,
      "step": 61900
    },
    {
      "epoch": 0.9733124018838305,
      "grad_norm": 4.950412750244141,
      "learning_rate": 4.939167974882261e-05,
      "loss": 0.7631,
      "step": 62000
    },
    {
      "epoch": 0.9748822605965463,
      "grad_norm": 4.264495849609375,
      "learning_rate": 4.939069858712716e-05,
      "loss": 0.7842,
      "step": 62100
    },
    {
      "epoch": 0.9764521193092621,
      "grad_norm": 4.878943920135498,
      "learning_rate": 4.9389717425431716e-05,
      "loss": 0.769,
      "step": 62200
    },
    {
      "epoch": 0.978021978021978,
      "grad_norm": 3.518317461013794,
      "learning_rate": 4.938873626373627e-05,
      "loss": 0.7641,
      "step": 62300
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 5.0724663734436035,
      "learning_rate": 4.938775510204082e-05,
      "loss": 0.742,
      "step": 62400
    },
    {
      "epoch": 0.9811616954474097,
      "grad_norm": 5.158936977386475,
      "learning_rate": 4.938677394034537e-05,
      "loss": 0.8099,
      "step": 62500
    },
    {
      "epoch": 0.9827315541601256,
      "grad_norm": 4.6417670249938965,
      "learning_rate": 4.9385792778649926e-05,
      "loss": 0.7827,
      "step": 62600
    },
    {
      "epoch": 0.9843014128728415,
      "grad_norm": 4.874378204345703,
      "learning_rate": 4.938481161695447e-05,
      "loss": 0.7628,
      "step": 62700
    },
    {
      "epoch": 0.9858712715855573,
      "grad_norm": 4.101387977600098,
      "learning_rate": 4.938383045525903e-05,
      "loss": 0.8006,
      "step": 62800
    },
    {
      "epoch": 0.9874411302982732,
      "grad_norm": 4.79415225982666,
      "learning_rate": 4.938284929356358e-05,
      "loss": 0.7946,
      "step": 62900
    },
    {
      "epoch": 0.989010989010989,
      "grad_norm": 4.505658149719238,
      "learning_rate": 4.938186813186814e-05,
      "loss": 0.7696,
      "step": 63000
    },
    {
      "epoch": 0.9905808477237049,
      "grad_norm": 4.9503889083862305,
      "learning_rate": 4.938088697017269e-05,
      "loss": 0.8052,
      "step": 63100
    },
    {
      "epoch": 0.9921507064364207,
      "grad_norm": 4.092396259307861,
      "learning_rate": 4.937990580847724e-05,
      "loss": 0.7747,
      "step": 63200
    },
    {
      "epoch": 0.9937205651491365,
      "grad_norm": 4.275240421295166,
      "learning_rate": 4.937892464678179e-05,
      "loss": 0.7819,
      "step": 63300
    },
    {
      "epoch": 0.9952904238618524,
      "grad_norm": 4.544571876525879,
      "learning_rate": 4.937794348508634e-05,
      "loss": 0.8009,
      "step": 63400
    },
    {
      "epoch": 0.9968602825745683,
      "grad_norm": 3.738800048828125,
      "learning_rate": 4.93769623233909e-05,
      "loss": 0.7328,
      "step": 63500
    },
    {
      "epoch": 0.9984301412872841,
      "grad_norm": 3.7675623893737793,
      "learning_rate": 4.937598116169545e-05,
      "loss": 0.7261,
      "step": 63600
    },
    {
      "epoch": 1.0,
      "grad_norm": 5.1250529289245605,
      "learning_rate": 4.937500000000001e-05,
      "loss": 0.7575,
      "step": 63700
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.0849114656448364,
      "eval_runtime": 14.8135,
      "eval_samples_per_second": 226.348,
      "eval_steps_per_second": 226.348,
      "step": 63700
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.6174501180648804,
      "eval_runtime": 280.2919,
      "eval_samples_per_second": 227.263,
      "eval_steps_per_second": 227.263,
      "step": 63700
    },
    {
      "epoch": 1.0015698587127158,
      "grad_norm": 4.735575199127197,
      "learning_rate": 4.937401883830455e-05,
      "loss": 0.7684,
      "step": 63800
    },
    {
      "epoch": 1.0031397174254317,
      "grad_norm": 5.10084867477417,
      "learning_rate": 4.937303767660911e-05,
      "loss": 0.7383,
      "step": 63900
    },
    {
      "epoch": 1.0047095761381475,
      "grad_norm": 5.410142421722412,
      "learning_rate": 4.937205651491366e-05,
      "loss": 0.743,
      "step": 64000
    },
    {
      "epoch": 1.0062794348508635,
      "grad_norm": 4.498143196105957,
      "learning_rate": 4.937107535321821e-05,
      "loss": 0.7887,
      "step": 64100
    },
    {
      "epoch": 1.0078492935635792,
      "grad_norm": 4.059206485748291,
      "learning_rate": 4.937009419152276e-05,
      "loss": 0.7782,
      "step": 64200
    },
    {
      "epoch": 1.0094191522762952,
      "grad_norm": 3.685535192489624,
      "learning_rate": 4.936911302982732e-05,
      "loss": 0.7757,
      "step": 64300
    },
    {
      "epoch": 1.010989010989011,
      "grad_norm": 4.831624984741211,
      "learning_rate": 4.936813186813187e-05,
      "loss": 0.8196,
      "step": 64400
    },
    {
      "epoch": 1.012558869701727,
      "grad_norm": 4.983075141906738,
      "learning_rate": 4.936715070643642e-05,
      "loss": 0.8081,
      "step": 64500
    },
    {
      "epoch": 1.0141287284144427,
      "grad_norm": 4.488121509552002,
      "learning_rate": 4.936616954474097e-05,
      "loss": 0.7811,
      "step": 64600
    },
    {
      "epoch": 1.0156985871271587,
      "grad_norm": 4.968857288360596,
      "learning_rate": 4.936518838304553e-05,
      "loss": 0.7916,
      "step": 64700
    },
    {
      "epoch": 1.0172684458398744,
      "grad_norm": 5.041505336761475,
      "learning_rate": 4.9364207221350075e-05,
      "loss": 0.7679,
      "step": 64800
    },
    {
      "epoch": 1.0188383045525902,
      "grad_norm": 4.59891414642334,
      "learning_rate": 4.936322605965463e-05,
      "loss": 0.7808,
      "step": 64900
    },
    {
      "epoch": 1.0204081632653061,
      "grad_norm": 4.516188621520996,
      "learning_rate": 4.9362244897959184e-05,
      "loss": 0.786,
      "step": 65000
    },
    {
      "epoch": 1.021978021978022,
      "grad_norm": 4.342172622680664,
      "learning_rate": 4.936126373626374e-05,
      "loss": 0.8072,
      "step": 65100
    },
    {
      "epoch": 1.0235478806907379,
      "grad_norm": 4.053976535797119,
      "learning_rate": 4.936028257456829e-05,
      "loss": 0.7411,
      "step": 65200
    },
    {
      "epoch": 1.0251177394034536,
      "grad_norm": 2.9384498596191406,
      "learning_rate": 4.9359301412872844e-05,
      "loss": 0.7737,
      "step": 65300
    },
    {
      "epoch": 1.0266875981161696,
      "grad_norm": 4.305695056915283,
      "learning_rate": 4.9358320251177395e-05,
      "loss": 0.7426,
      "step": 65400
    },
    {
      "epoch": 1.0282574568288854,
      "grad_norm": 4.207804203033447,
      "learning_rate": 4.9357339089481946e-05,
      "loss": 0.7389,
      "step": 65500
    },
    {
      "epoch": 1.0298273155416013,
      "grad_norm": 4.68092679977417,
      "learning_rate": 4.93563579277865e-05,
      "loss": 0.7739,
      "step": 65600
    },
    {
      "epoch": 1.031397174254317,
      "grad_norm": 3.606738328933716,
      "learning_rate": 4.9355376766091054e-05,
      "loss": 0.7793,
      "step": 65700
    },
    {
      "epoch": 1.032967032967033,
      "grad_norm": 4.4795002937316895,
      "learning_rate": 4.935439560439561e-05,
      "loss": 0.7823,
      "step": 65800
    },
    {
      "epoch": 1.0345368916797488,
      "grad_norm": 5.951931476593018,
      "learning_rate": 4.9353414442700156e-05,
      "loss": 0.7564,
      "step": 65900
    },
    {
      "epoch": 1.0361067503924646,
      "grad_norm": 4.110368728637695,
      "learning_rate": 4.9352433281004714e-05,
      "loss": 0.7547,
      "step": 66000
    },
    {
      "epoch": 1.0376766091051806,
      "grad_norm": 3.8160438537597656,
      "learning_rate": 4.9351452119309265e-05,
      "loss": 0.805,
      "step": 66100
    },
    {
      "epoch": 1.0392464678178963,
      "grad_norm": 4.310070514678955,
      "learning_rate": 4.9350470957613816e-05,
      "loss": 0.7705,
      "step": 66200
    },
    {
      "epoch": 1.0408163265306123,
      "grad_norm": 4.268405914306641,
      "learning_rate": 4.934948979591837e-05,
      "loss": 0.7402,
      "step": 66300
    },
    {
      "epoch": 1.042386185243328,
      "grad_norm": 3.6014232635498047,
      "learning_rate": 4.9348508634222925e-05,
      "loss": 0.76,
      "step": 66400
    },
    {
      "epoch": 1.043956043956044,
      "grad_norm": 3.0962562561035156,
      "learning_rate": 4.9347527472527476e-05,
      "loss": 0.7333,
      "step": 66500
    },
    {
      "epoch": 1.0455259026687598,
      "grad_norm": 4.29154109954834,
      "learning_rate": 4.934654631083203e-05,
      "loss": 0.7624,
      "step": 66600
    },
    {
      "epoch": 1.0470957613814758,
      "grad_norm": 4.773361682891846,
      "learning_rate": 4.934556514913658e-05,
      "loss": 0.7864,
      "step": 66700
    },
    {
      "epoch": 1.0486656200941915,
      "grad_norm": 4.320716857910156,
      "learning_rate": 4.9344583987441135e-05,
      "loss": 0.7501,
      "step": 66800
    },
    {
      "epoch": 1.0502354788069075,
      "grad_norm": 3.811213493347168,
      "learning_rate": 4.934360282574568e-05,
      "loss": 0.7482,
      "step": 66900
    },
    {
      "epoch": 1.0518053375196232,
      "grad_norm": 4.312719821929932,
      "learning_rate": 4.934262166405024e-05,
      "loss": 0.7766,
      "step": 67000
    },
    {
      "epoch": 1.053375196232339,
      "grad_norm": 4.188240051269531,
      "learning_rate": 4.934164050235479e-05,
      "loss": 0.7435,
      "step": 67100
    },
    {
      "epoch": 1.054945054945055,
      "grad_norm": 4.6111578941345215,
      "learning_rate": 4.9340659340659346e-05,
      "loss": 0.8119,
      "step": 67200
    },
    {
      "epoch": 1.0565149136577707,
      "grad_norm": 5.288651943206787,
      "learning_rate": 4.93396781789639e-05,
      "loss": 0.7739,
      "step": 67300
    },
    {
      "epoch": 1.0580847723704867,
      "grad_norm": 3.793752908706665,
      "learning_rate": 4.933869701726845e-05,
      "loss": 0.7642,
      "step": 67400
    },
    {
      "epoch": 1.0596546310832025,
      "grad_norm": 4.0095062255859375,
      "learning_rate": 4.9337715855573e-05,
      "loss": 0.7377,
      "step": 67500
    },
    {
      "epoch": 1.0612244897959184,
      "grad_norm": 6.023867607116699,
      "learning_rate": 4.933673469387755e-05,
      "loss": 0.7575,
      "step": 67600
    },
    {
      "epoch": 1.0627943485086342,
      "grad_norm": 5.041622638702393,
      "learning_rate": 4.933575353218211e-05,
      "loss": 0.7165,
      "step": 67700
    },
    {
      "epoch": 1.0643642072213502,
      "grad_norm": 4.919727802276611,
      "learning_rate": 4.933477237048666e-05,
      "loss": 0.7462,
      "step": 67800
    },
    {
      "epoch": 1.065934065934066,
      "grad_norm": 4.89773416519165,
      "learning_rate": 4.9333791208791217e-05,
      "loss": 0.7314,
      "step": 67900
    },
    {
      "epoch": 1.0675039246467817,
      "grad_norm": 5.5291290283203125,
      "learning_rate": 4.933281004709576e-05,
      "loss": 0.8505,
      "step": 68000
    },
    {
      "epoch": 1.0690737833594977,
      "grad_norm": 4.243226528167725,
      "learning_rate": 4.933182888540032e-05,
      "loss": 0.749,
      "step": 68100
    },
    {
      "epoch": 1.0706436420722134,
      "grad_norm": 4.328510761260986,
      "learning_rate": 4.933084772370487e-05,
      "loss": 0.8235,
      "step": 68200
    },
    {
      "epoch": 1.0722135007849294,
      "grad_norm": 3.4768388271331787,
      "learning_rate": 4.932986656200942e-05,
      "loss": 0.7981,
      "step": 68300
    },
    {
      "epoch": 1.0737833594976451,
      "grad_norm": 3.8654887676239014,
      "learning_rate": 4.932888540031397e-05,
      "loss": 0.7537,
      "step": 68400
    },
    {
      "epoch": 1.0753532182103611,
      "grad_norm": 4.518398284912109,
      "learning_rate": 4.932790423861853e-05,
      "loss": 0.7452,
      "step": 68500
    },
    {
      "epoch": 1.0769230769230769,
      "grad_norm": 3.4398064613342285,
      "learning_rate": 4.932692307692308e-05,
      "loss": 0.7516,
      "step": 68600
    },
    {
      "epoch": 1.0784929356357928,
      "grad_norm": 4.807605266571045,
      "learning_rate": 4.932594191522763e-05,
      "loss": 0.8114,
      "step": 68700
    },
    {
      "epoch": 1.0800627943485086,
      "grad_norm": 3.8993258476257324,
      "learning_rate": 4.932496075353218e-05,
      "loss": 0.8226,
      "step": 68800
    },
    {
      "epoch": 1.0816326530612246,
      "grad_norm": 3.788233995437622,
      "learning_rate": 4.932397959183674e-05,
      "loss": 0.7972,
      "step": 68900
    },
    {
      "epoch": 1.0832025117739403,
      "grad_norm": 3.390704870223999,
      "learning_rate": 4.9322998430141284e-05,
      "loss": 0.7188,
      "step": 69000
    },
    {
      "epoch": 1.084772370486656,
      "grad_norm": 4.252943515777588,
      "learning_rate": 4.932201726844584e-05,
      "loss": 0.7853,
      "step": 69100
    },
    {
      "epoch": 1.086342229199372,
      "grad_norm": 4.41823673248291,
      "learning_rate": 4.932103610675039e-05,
      "loss": 0.7575,
      "step": 69200
    },
    {
      "epoch": 1.0879120879120878,
      "grad_norm": 5.225766658782959,
      "learning_rate": 4.932005494505495e-05,
      "loss": 0.7404,
      "step": 69300
    },
    {
      "epoch": 1.0894819466248038,
      "grad_norm": 5.0644707679748535,
      "learning_rate": 4.93190737833595e-05,
      "loss": 0.77,
      "step": 69400
    },
    {
      "epoch": 1.0910518053375196,
      "grad_norm": 4.065659046173096,
      "learning_rate": 4.931809262166405e-05,
      "loss": 0.7621,
      "step": 69500
    },
    {
      "epoch": 1.0926216640502355,
      "grad_norm": 4.054235458374023,
      "learning_rate": 4.9317111459968604e-05,
      "loss": 0.76,
      "step": 69600
    },
    {
      "epoch": 1.0941915227629513,
      "grad_norm": 5.1040167808532715,
      "learning_rate": 4.9316130298273155e-05,
      "loss": 0.7409,
      "step": 69700
    },
    {
      "epoch": 1.0957613814756673,
      "grad_norm": 3.958892583847046,
      "learning_rate": 4.931514913657771e-05,
      "loss": 0.8131,
      "step": 69800
    },
    {
      "epoch": 1.097331240188383,
      "grad_norm": 3.2628085613250732,
      "learning_rate": 4.931416797488226e-05,
      "loss": 0.7419,
      "step": 69900
    },
    {
      "epoch": 1.098901098901099,
      "grad_norm": 3.4590272903442383,
      "learning_rate": 4.931318681318682e-05,
      "loss": 0.7305,
      "step": 70000
    },
    {
      "epoch": 1.1004709576138147,
      "grad_norm": 4.713141918182373,
      "learning_rate": 4.9312205651491365e-05,
      "loss": 0.7603,
      "step": 70100
    },
    {
      "epoch": 1.1020408163265305,
      "grad_norm": 4.7150492668151855,
      "learning_rate": 4.931122448979592e-05,
      "loss": 0.7576,
      "step": 70200
    },
    {
      "epoch": 1.1036106750392465,
      "grad_norm": 4.924499988555908,
      "learning_rate": 4.9310243328100474e-05,
      "loss": 0.7733,
      "step": 70300
    },
    {
      "epoch": 1.1051805337519622,
      "grad_norm": 4.781326770782471,
      "learning_rate": 4.9309262166405025e-05,
      "loss": 0.7481,
      "step": 70400
    },
    {
      "epoch": 1.1067503924646782,
      "grad_norm": 4.246909141540527,
      "learning_rate": 4.9308281004709576e-05,
      "loss": 0.7652,
      "step": 70500
    },
    {
      "epoch": 1.108320251177394,
      "grad_norm": 4.785752296447754,
      "learning_rate": 4.9307299843014134e-05,
      "loss": 0.7608,
      "step": 70600
    },
    {
      "epoch": 1.10989010989011,
      "grad_norm": 4.253579139709473,
      "learning_rate": 4.9306318681318685e-05,
      "loss": 0.7978,
      "step": 70700
    },
    {
      "epoch": 1.1114599686028257,
      "grad_norm": 4.0628557205200195,
      "learning_rate": 4.9305337519623236e-05,
      "loss": 0.7687,
      "step": 70800
    },
    {
      "epoch": 1.1130298273155417,
      "grad_norm": 4.115346431732178,
      "learning_rate": 4.930435635792779e-05,
      "loss": 0.7672,
      "step": 70900
    },
    {
      "epoch": 1.1145996860282574,
      "grad_norm": 3.3610916137695312,
      "learning_rate": 4.9303375196232344e-05,
      "loss": 0.7617,
      "step": 71000
    },
    {
      "epoch": 1.1161695447409734,
      "grad_norm": 4.448246955871582,
      "learning_rate": 4.930239403453689e-05,
      "loss": 0.712,
      "step": 71100
    },
    {
      "epoch": 1.1177394034536892,
      "grad_norm": 4.917850971221924,
      "learning_rate": 4.9301412872841446e-05,
      "loss": 0.7402,
      "step": 71200
    },
    {
      "epoch": 1.119309262166405,
      "grad_norm": 5.000461101531982,
      "learning_rate": 4.9300431711146e-05,
      "loss": 0.7846,
      "step": 71300
    },
    {
      "epoch": 1.120879120879121,
      "grad_norm": 4.254511833190918,
      "learning_rate": 4.9299450549450555e-05,
      "loss": 0.8019,
      "step": 71400
    },
    {
      "epoch": 1.1224489795918366,
      "grad_norm": 4.670138359069824,
      "learning_rate": 4.9298469387755106e-05,
      "loss": 0.7817,
      "step": 71500
    },
    {
      "epoch": 1.1240188383045526,
      "grad_norm": 4.252340316772461,
      "learning_rate": 4.929748822605966e-05,
      "loss": 0.8107,
      "step": 71600
    },
    {
      "epoch": 1.1255886970172684,
      "grad_norm": 4.739249229431152,
      "learning_rate": 4.929650706436421e-05,
      "loss": 0.7627,
      "step": 71700
    },
    {
      "epoch": 1.1271585557299844,
      "grad_norm": 3.2835488319396973,
      "learning_rate": 4.929552590266876e-05,
      "loss": 0.7586,
      "step": 71800
    },
    {
      "epoch": 1.1287284144427001,
      "grad_norm": 4.600184917449951,
      "learning_rate": 4.929454474097332e-05,
      "loss": 0.7501,
      "step": 71900
    },
    {
      "epoch": 1.130298273155416,
      "grad_norm": 3.744539976119995,
      "learning_rate": 4.929356357927787e-05,
      "loss": 0.7423,
      "step": 72000
    },
    {
      "epoch": 1.1318681318681318,
      "grad_norm": 4.760754108428955,
      "learning_rate": 4.9292582417582425e-05,
      "loss": 0.7997,
      "step": 72100
    },
    {
      "epoch": 1.1334379905808478,
      "grad_norm": 4.347365379333496,
      "learning_rate": 4.929160125588697e-05,
      "loss": 0.771,
      "step": 72200
    },
    {
      "epoch": 1.1350078492935636,
      "grad_norm": 4.141330718994141,
      "learning_rate": 4.929062009419153e-05,
      "loss": 0.7434,
      "step": 72300
    },
    {
      "epoch": 1.1365777080062793,
      "grad_norm": 4.2054219245910645,
      "learning_rate": 4.928963893249608e-05,
      "loss": 0.7595,
      "step": 72400
    },
    {
      "epoch": 1.1381475667189953,
      "grad_norm": 3.8328094482421875,
      "learning_rate": 4.928865777080063e-05,
      "loss": 0.7944,
      "step": 72500
    },
    {
      "epoch": 1.139717425431711,
      "grad_norm": 3.7347252368927,
      "learning_rate": 4.928767660910518e-05,
      "loss": 0.7626,
      "step": 72600
    },
    {
      "epoch": 1.141287284144427,
      "grad_norm": 4.485983848571777,
      "learning_rate": 4.928669544740974e-05,
      "loss": 0.808,
      "step": 72700
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 3.9514505863189697,
      "learning_rate": 4.928571428571429e-05,
      "loss": 0.7554,
      "step": 72800
    },
    {
      "epoch": 1.1444270015698588,
      "grad_norm": 3.8675410747528076,
      "learning_rate": 4.928473312401884e-05,
      "loss": 0.7615,
      "step": 72900
    },
    {
      "epoch": 1.1459968602825745,
      "grad_norm": 3.922215223312378,
      "learning_rate": 4.928375196232339e-05,
      "loss": 0.7819,
      "step": 73000
    },
    {
      "epoch": 1.1475667189952905,
      "grad_norm": 4.844797611236572,
      "learning_rate": 4.928277080062795e-05,
      "loss": 0.7326,
      "step": 73100
    },
    {
      "epoch": 1.1491365777080063,
      "grad_norm": 4.830899238586426,
      "learning_rate": 4.928178963893249e-05,
      "loss": 0.7179,
      "step": 73200
    },
    {
      "epoch": 1.1507064364207222,
      "grad_norm": 5.588432312011719,
      "learning_rate": 4.928080847723705e-05,
      "loss": 0.7558,
      "step": 73300
    },
    {
      "epoch": 1.152276295133438,
      "grad_norm": 4.542299270629883,
      "learning_rate": 4.92798273155416e-05,
      "loss": 0.7708,
      "step": 73400
    },
    {
      "epoch": 1.1538461538461537,
      "grad_norm": 3.761993646621704,
      "learning_rate": 4.927884615384616e-05,
      "loss": 0.7695,
      "step": 73500
    },
    {
      "epoch": 1.1554160125588697,
      "grad_norm": 4.378283977508545,
      "learning_rate": 4.927786499215071e-05,
      "loss": 0.7728,
      "step": 73600
    },
    {
      "epoch": 1.1569858712715855,
      "grad_norm": 3.736405611038208,
      "learning_rate": 4.927688383045526e-05,
      "loss": 0.7744,
      "step": 73700
    },
    {
      "epoch": 1.1585557299843015,
      "grad_norm": 3.738497495651245,
      "learning_rate": 4.927590266875981e-05,
      "loss": 0.7685,
      "step": 73800
    },
    {
      "epoch": 1.1601255886970172,
      "grad_norm": 4.773867130279541,
      "learning_rate": 4.9274921507064363e-05,
      "loss": 0.7463,
      "step": 73900
    },
    {
      "epoch": 1.1616954474097332,
      "grad_norm": 4.05750846862793,
      "learning_rate": 4.927394034536892e-05,
      "loss": 0.7809,
      "step": 74000
    },
    {
      "epoch": 1.163265306122449,
      "grad_norm": 4.350325107574463,
      "learning_rate": 4.927295918367347e-05,
      "loss": 0.7697,
      "step": 74100
    },
    {
      "epoch": 1.164835164835165,
      "grad_norm": 4.229294300079346,
      "learning_rate": 4.927197802197803e-05,
      "loss": 0.7232,
      "step": 74200
    },
    {
      "epoch": 1.1664050235478807,
      "grad_norm": 3.7898130416870117,
      "learning_rate": 4.9270996860282574e-05,
      "loss": 0.7519,
      "step": 74300
    },
    {
      "epoch": 1.1679748822605966,
      "grad_norm": 5.044505596160889,
      "learning_rate": 4.927001569858713e-05,
      "loss": 0.7066,
      "step": 74400
    },
    {
      "epoch": 1.1695447409733124,
      "grad_norm": 4.7236647605896,
      "learning_rate": 4.926903453689168e-05,
      "loss": 0.763,
      "step": 74500
    },
    {
      "epoch": 1.1711145996860282,
      "grad_norm": 4.515085220336914,
      "learning_rate": 4.9268053375196234e-05,
      "loss": 0.7622,
      "step": 74600
    },
    {
      "epoch": 1.1726844583987441,
      "grad_norm": 4.182746410369873,
      "learning_rate": 4.9267072213500785e-05,
      "loss": 0.7563,
      "step": 74700
    },
    {
      "epoch": 1.1742543171114599,
      "grad_norm": 4.507289409637451,
      "learning_rate": 4.926609105180534e-05,
      "loss": 0.7344,
      "step": 74800
    },
    {
      "epoch": 1.1758241758241759,
      "grad_norm": 3.416134834289551,
      "learning_rate": 4.9265109890109894e-05,
      "loss": 0.7931,
      "step": 74900
    },
    {
      "epoch": 1.1773940345368916,
      "grad_norm": 4.554042816162109,
      "learning_rate": 4.9264128728414445e-05,
      "loss": 0.7502,
      "step": 75000
    },
    {
      "epoch": 1.1789638932496076,
      "grad_norm": 4.207318305969238,
      "learning_rate": 4.9263147566718996e-05,
      "loss": 0.7865,
      "step": 75100
    },
    {
      "epoch": 1.1805337519623234,
      "grad_norm": 4.379655361175537,
      "learning_rate": 4.926216640502355e-05,
      "loss": 0.7458,
      "step": 75200
    },
    {
      "epoch": 1.1821036106750393,
      "grad_norm": 5.083130836486816,
      "learning_rate": 4.92611852433281e-05,
      "loss": 0.7758,
      "step": 75300
    },
    {
      "epoch": 1.183673469387755,
      "grad_norm": 4.174050331115723,
      "learning_rate": 4.9260204081632655e-05,
      "loss": 0.7735,
      "step": 75400
    },
    {
      "epoch": 1.185243328100471,
      "grad_norm": 4.466348171234131,
      "learning_rate": 4.9259222919937206e-05,
      "loss": 0.7498,
      "step": 75500
    },
    {
      "epoch": 1.1868131868131868,
      "grad_norm": 3.778017997741699,
      "learning_rate": 4.9258241758241764e-05,
      "loss": 0.7354,
      "step": 75600
    },
    {
      "epoch": 1.1883830455259026,
      "grad_norm": 3.678828239440918,
      "learning_rate": 4.9257260596546315e-05,
      "loss": 0.7398,
      "step": 75700
    },
    {
      "epoch": 1.1899529042386185,
      "grad_norm": 3.4315977096557617,
      "learning_rate": 4.9256279434850866e-05,
      "loss": 0.7614,
      "step": 75800
    },
    {
      "epoch": 1.1915227629513343,
      "grad_norm": 4.527956962585449,
      "learning_rate": 4.925529827315542e-05,
      "loss": 0.7824,
      "step": 75900
    },
    {
      "epoch": 1.1930926216640503,
      "grad_norm": 3.343449115753174,
      "learning_rate": 4.925431711145997e-05,
      "loss": 0.754,
      "step": 76000
    },
    {
      "epoch": 1.194662480376766,
      "grad_norm": 4.6009840965271,
      "learning_rate": 4.9253335949764526e-05,
      "loss": 0.7357,
      "step": 76100
    },
    {
      "epoch": 1.196232339089482,
      "grad_norm": 4.433352470397949,
      "learning_rate": 4.925235478806908e-05,
      "loss": 0.7644,
      "step": 76200
    },
    {
      "epoch": 1.1978021978021978,
      "grad_norm": 5.265154838562012,
      "learning_rate": 4.9251373626373634e-05,
      "loss": 0.7775,
      "step": 76300
    },
    {
      "epoch": 1.1993720565149137,
      "grad_norm": 4.484283447265625,
      "learning_rate": 4.925039246467818e-05,
      "loss": 0.7961,
      "step": 76400
    },
    {
      "epoch": 1.2009419152276295,
      "grad_norm": 4.174435138702393,
      "learning_rate": 4.9249411302982736e-05,
      "loss": 0.7946,
      "step": 76500
    },
    {
      "epoch": 1.2025117739403455,
      "grad_norm": 4.30825138092041,
      "learning_rate": 4.924843014128729e-05,
      "loss": 0.776,
      "step": 76600
    },
    {
      "epoch": 1.2040816326530612,
      "grad_norm": 3.5419061183929443,
      "learning_rate": 4.924744897959184e-05,
      "loss": 0.7512,
      "step": 76700
    },
    {
      "epoch": 1.205651491365777,
      "grad_norm": 4.659671783447266,
      "learning_rate": 4.924646781789639e-05,
      "loss": 0.7572,
      "step": 76800
    },
    {
      "epoch": 1.207221350078493,
      "grad_norm": 5.214090347290039,
      "learning_rate": 4.924548665620095e-05,
      "loss": 0.7479,
      "step": 76900
    },
    {
      "epoch": 1.2087912087912087,
      "grad_norm": 4.455277442932129,
      "learning_rate": 4.92445054945055e-05,
      "loss": 0.7503,
      "step": 77000
    },
    {
      "epoch": 1.2103610675039247,
      "grad_norm": 4.326798439025879,
      "learning_rate": 4.924352433281005e-05,
      "loss": 0.7628,
      "step": 77100
    },
    {
      "epoch": 1.2119309262166404,
      "grad_norm": 4.6865997314453125,
      "learning_rate": 4.92425431711146e-05,
      "loss": 0.7636,
      "step": 77200
    },
    {
      "epoch": 1.2135007849293564,
      "grad_norm": 4.510838508605957,
      "learning_rate": 4.924156200941916e-05,
      "loss": 0.7807,
      "step": 77300
    },
    {
      "epoch": 1.2150706436420722,
      "grad_norm": 4.178847789764404,
      "learning_rate": 4.92405808477237e-05,
      "loss": 0.7417,
      "step": 77400
    },
    {
      "epoch": 1.2166405023547882,
      "grad_norm": 3.9243884086608887,
      "learning_rate": 4.923959968602826e-05,
      "loss": 0.8151,
      "step": 77500
    },
    {
      "epoch": 1.218210361067504,
      "grad_norm": 3.8695285320281982,
      "learning_rate": 4.923861852433281e-05,
      "loss": 0.7774,
      "step": 77600
    },
    {
      "epoch": 1.2197802197802199,
      "grad_norm": 3.8613743782043457,
      "learning_rate": 4.923763736263737e-05,
      "loss": 0.7487,
      "step": 77700
    },
    {
      "epoch": 1.2213500784929356,
      "grad_norm": 4.893017292022705,
      "learning_rate": 4.923665620094192e-05,
      "loss": 0.7728,
      "step": 77800
    },
    {
      "epoch": 1.2229199372056514,
      "grad_norm": 4.628710746765137,
      "learning_rate": 4.923567503924647e-05,
      "loss": 0.7269,
      "step": 77900
    },
    {
      "epoch": 1.2244897959183674,
      "grad_norm": 3.9759554862976074,
      "learning_rate": 4.923469387755102e-05,
      "loss": 0.7335,
      "step": 78000
    },
    {
      "epoch": 1.2260596546310831,
      "grad_norm": 4.162051200866699,
      "learning_rate": 4.923371271585557e-05,
      "loss": 0.7611,
      "step": 78100
    },
    {
      "epoch": 1.227629513343799,
      "grad_norm": 3.1550369262695312,
      "learning_rate": 4.923273155416013e-05,
      "loss": 0.7509,
      "step": 78200
    },
    {
      "epoch": 1.2291993720565149,
      "grad_norm": 4.325747489929199,
      "learning_rate": 4.923175039246468e-05,
      "loss": 0.7548,
      "step": 78300
    },
    {
      "epoch": 1.2307692307692308,
      "grad_norm": 4.555430889129639,
      "learning_rate": 4.923076923076924e-05,
      "loss": 0.7671,
      "step": 78400
    },
    {
      "epoch": 1.2323390894819466,
      "grad_norm": 3.6396825313568115,
      "learning_rate": 4.922978806907378e-05,
      "loss": 0.7419,
      "step": 78500
    },
    {
      "epoch": 1.2339089481946626,
      "grad_norm": 4.309146881103516,
      "learning_rate": 4.922880690737834e-05,
      "loss": 0.7222,
      "step": 78600
    },
    {
      "epoch": 1.2354788069073783,
      "grad_norm": 4.630756855010986,
      "learning_rate": 4.922782574568289e-05,
      "loss": 0.7655,
      "step": 78700
    },
    {
      "epoch": 1.2370486656200943,
      "grad_norm": 4.492467880249023,
      "learning_rate": 4.922684458398744e-05,
      "loss": 0.7193,
      "step": 78800
    },
    {
      "epoch": 1.23861852433281,
      "grad_norm": 4.713857173919678,
      "learning_rate": 4.9225863422291994e-05,
      "loss": 0.774,
      "step": 78900
    },
    {
      "epoch": 1.2401883830455258,
      "grad_norm": 4.312688827514648,
      "learning_rate": 4.922488226059655e-05,
      "loss": 0.7385,
      "step": 79000
    },
    {
      "epoch": 1.2417582417582418,
      "grad_norm": 4.5998148918151855,
      "learning_rate": 4.92239010989011e-05,
      "loss": 0.764,
      "step": 79100
    },
    {
      "epoch": 1.2433281004709575,
      "grad_norm": 3.3202970027923584,
      "learning_rate": 4.9222919937205654e-05,
      "loss": 0.794,
      "step": 79200
    },
    {
      "epoch": 1.2448979591836735,
      "grad_norm": 4.351808071136475,
      "learning_rate": 4.9221938775510205e-05,
      "loss": 0.7774,
      "step": 79300
    },
    {
      "epoch": 1.2464678178963893,
      "grad_norm": 4.480055332183838,
      "learning_rate": 4.922095761381476e-05,
      "loss": 0.7564,
      "step": 79400
    },
    {
      "epoch": 1.2480376766091053,
      "grad_norm": 3.808682441711426,
      "learning_rate": 4.9219976452119307e-05,
      "loss": 0.7719,
      "step": 79500
    },
    {
      "epoch": 1.249607535321821,
      "grad_norm": 4.413092136383057,
      "learning_rate": 4.9218995290423864e-05,
      "loss": 0.7487,
      "step": 79600
    },
    {
      "epoch": 1.2511773940345368,
      "grad_norm": 3.805490255355835,
      "learning_rate": 4.9218014128728415e-05,
      "loss": 0.7454,
      "step": 79700
    },
    {
      "epoch": 1.2527472527472527,
      "grad_norm": 3.886428117752075,
      "learning_rate": 4.9217032967032966e-05,
      "loss": 0.7771,
      "step": 79800
    },
    {
      "epoch": 1.2543171114599687,
      "grad_norm": 3.833555221557617,
      "learning_rate": 4.9216051805337524e-05,
      "loss": 0.7309,
      "step": 79900
    },
    {
      "epoch": 1.2558869701726845,
      "grad_norm": 4.647435665130615,
      "learning_rate": 4.9215070643642075e-05,
      "loss": 0.7633,
      "step": 80000
    },
    {
      "epoch": 1.2574568288854002,
      "grad_norm": 4.678268909454346,
      "learning_rate": 4.9214089481946626e-05,
      "loss": 0.7374,
      "step": 80100
    },
    {
      "epoch": 1.2590266875981162,
      "grad_norm": 4.438427448272705,
      "learning_rate": 4.921310832025118e-05,
      "loss": 0.8024,
      "step": 80200
    },
    {
      "epoch": 1.260596546310832,
      "grad_norm": 4.053921222686768,
      "learning_rate": 4.9212127158555735e-05,
      "loss": 0.7777,
      "step": 80300
    },
    {
      "epoch": 1.262166405023548,
      "grad_norm": 4.349181175231934,
      "learning_rate": 4.9211145996860286e-05,
      "loss": 0.7839,
      "step": 80400
    },
    {
      "epoch": 1.2637362637362637,
      "grad_norm": 4.525758743286133,
      "learning_rate": 4.921016483516484e-05,
      "loss": 0.7964,
      "step": 80500
    },
    {
      "epoch": 1.2653061224489797,
      "grad_norm": 4.056788444519043,
      "learning_rate": 4.920918367346939e-05,
      "loss": 0.7124,
      "step": 80600
    },
    {
      "epoch": 1.2668759811616954,
      "grad_norm": 3.9436557292938232,
      "learning_rate": 4.9208202511773945e-05,
      "loss": 0.7802,
      "step": 80700
    },
    {
      "epoch": 1.2684458398744112,
      "grad_norm": 4.247996807098389,
      "learning_rate": 4.9207221350078496e-05,
      "loss": 0.7733,
      "step": 80800
    },
    {
      "epoch": 1.2700156985871272,
      "grad_norm": 3.72356915473938,
      "learning_rate": 4.920624018838305e-05,
      "loss": 0.7427,
      "step": 80900
    },
    {
      "epoch": 1.2715855572998431,
      "grad_norm": 2.8300180435180664,
      "learning_rate": 4.92052590266876e-05,
      "loss": 0.7648,
      "step": 81000
    },
    {
      "epoch": 1.2731554160125589,
      "grad_norm": 4.542043685913086,
      "learning_rate": 4.9204277864992156e-05,
      "loss": 0.7602,
      "step": 81100
    },
    {
      "epoch": 1.2747252747252746,
      "grad_norm": 4.328848361968994,
      "learning_rate": 4.92032967032967e-05,
      "loss": 0.7842,
      "step": 81200
    },
    {
      "epoch": 1.2762951334379906,
      "grad_norm": 5.081377029418945,
      "learning_rate": 4.920231554160126e-05,
      "loss": 0.7631,
      "step": 81300
    },
    {
      "epoch": 1.2778649921507064,
      "grad_norm": 4.185920238494873,
      "learning_rate": 4.920133437990581e-05,
      "loss": 0.7366,
      "step": 81400
    },
    {
      "epoch": 1.2794348508634223,
      "grad_norm": 5.538674354553223,
      "learning_rate": 4.920035321821037e-05,
      "loss": 0.8017,
      "step": 81500
    },
    {
      "epoch": 1.281004709576138,
      "grad_norm": 3.6200006008148193,
      "learning_rate": 4.919937205651491e-05,
      "loss": 0.735,
      "step": 81600
    },
    {
      "epoch": 1.282574568288854,
      "grad_norm": 4.684442520141602,
      "learning_rate": 4.919839089481947e-05,
      "loss": 0.7551,
      "step": 81700
    },
    {
      "epoch": 1.2841444270015698,
      "grad_norm": 4.98200798034668,
      "learning_rate": 4.919740973312402e-05,
      "loss": 0.7704,
      "step": 81800
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 5.24729585647583,
      "learning_rate": 4.919642857142857e-05,
      "loss": 0.7311,
      "step": 81900
    },
    {
      "epoch": 1.2872841444270016,
      "grad_norm": 3.587444305419922,
      "learning_rate": 4.919544740973313e-05,
      "loss": 0.7254,
      "step": 82000
    },
    {
      "epoch": 1.2888540031397175,
      "grad_norm": 3.7278337478637695,
      "learning_rate": 4.919446624803768e-05,
      "loss": 0.7831,
      "step": 82100
    },
    {
      "epoch": 1.2904238618524333,
      "grad_norm": 4.4115800857543945,
      "learning_rate": 4.919348508634223e-05,
      "loss": 0.7087,
      "step": 82200
    },
    {
      "epoch": 1.291993720565149,
      "grad_norm": 3.808784246444702,
      "learning_rate": 4.919250392464678e-05,
      "loss": 0.7479,
      "step": 82300
    },
    {
      "epoch": 1.293563579277865,
      "grad_norm": 4.047694683074951,
      "learning_rate": 4.919152276295134e-05,
      "loss": 0.7965,
      "step": 82400
    },
    {
      "epoch": 1.2951334379905808,
      "grad_norm": 4.937098503112793,
      "learning_rate": 4.919054160125589e-05,
      "loss": 0.722,
      "step": 82500
    },
    {
      "epoch": 1.2967032967032968,
      "grad_norm": 3.8027007579803467,
      "learning_rate": 4.918956043956044e-05,
      "loss": 0.7128,
      "step": 82600
    },
    {
      "epoch": 1.2982731554160125,
      "grad_norm": 4.357639312744141,
      "learning_rate": 4.918857927786499e-05,
      "loss": 0.7555,
      "step": 82700
    },
    {
      "epoch": 1.2998430141287285,
      "grad_norm": 4.858974456787109,
      "learning_rate": 4.918759811616955e-05,
      "loss": 0.7607,
      "step": 82800
    },
    {
      "epoch": 1.3014128728414442,
      "grad_norm": 4.849387168884277,
      "learning_rate": 4.91866169544741e-05,
      "loss": 0.7567,
      "step": 82900
    },
    {
      "epoch": 1.30298273155416,
      "grad_norm": 3.864598274230957,
      "learning_rate": 4.918563579277865e-05,
      "loss": 0.7337,
      "step": 83000
    },
    {
      "epoch": 1.304552590266876,
      "grad_norm": 3.933180570602417,
      "learning_rate": 4.91846546310832e-05,
      "loss": 0.7973,
      "step": 83100
    },
    {
      "epoch": 1.306122448979592,
      "grad_norm": 3.9738657474517822,
      "learning_rate": 4.918367346938776e-05,
      "loss": 0.8037,
      "step": 83200
    },
    {
      "epoch": 1.3076923076923077,
      "grad_norm": 4.717374324798584,
      "learning_rate": 4.9182692307692305e-05,
      "loss": 0.7932,
      "step": 83300
    },
    {
      "epoch": 1.3092621664050235,
      "grad_norm": 4.461583137512207,
      "learning_rate": 4.918171114599686e-05,
      "loss": 0.7855,
      "step": 83400
    },
    {
      "epoch": 1.3108320251177394,
      "grad_norm": 4.331399917602539,
      "learning_rate": 4.9180729984301414e-05,
      "loss": 0.7551,
      "step": 83500
    },
    {
      "epoch": 1.3124018838304552,
      "grad_norm": 4.750341415405273,
      "learning_rate": 4.917974882260597e-05,
      "loss": 0.78,
      "step": 83600
    },
    {
      "epoch": 1.3139717425431712,
      "grad_norm": 4.397961139678955,
      "learning_rate": 4.9178767660910516e-05,
      "loss": 0.771,
      "step": 83700
    },
    {
      "epoch": 1.315541601255887,
      "grad_norm": 4.385669231414795,
      "learning_rate": 4.917778649921507e-05,
      "loss": 0.7534,
      "step": 83800
    },
    {
      "epoch": 1.317111459968603,
      "grad_norm": 4.750272274017334,
      "learning_rate": 4.9176805337519624e-05,
      "loss": 0.7762,
      "step": 83900
    },
    {
      "epoch": 1.3186813186813187,
      "grad_norm": 4.085953235626221,
      "learning_rate": 4.9175824175824175e-05,
      "loss": 0.7229,
      "step": 84000
    },
    {
      "epoch": 1.3202511773940344,
      "grad_norm": 4.1732916831970215,
      "learning_rate": 4.917484301412873e-05,
      "loss": 0.7585,
      "step": 84100
    },
    {
      "epoch": 1.3218210361067504,
      "grad_norm": 4.51860237121582,
      "learning_rate": 4.9173861852433284e-05,
      "loss": 0.7454,
      "step": 84200
    },
    {
      "epoch": 1.3233908948194664,
      "grad_norm": 5.276190757751465,
      "learning_rate": 4.9172880690737835e-05,
      "loss": 0.7342,
      "step": 84300
    },
    {
      "epoch": 1.3249607535321821,
      "grad_norm": 4.620984077453613,
      "learning_rate": 4.9171899529042386e-05,
      "loss": 0.7412,
      "step": 84400
    },
    {
      "epoch": 1.3265306122448979,
      "grad_norm": 3.99985933303833,
      "learning_rate": 4.9170918367346944e-05,
      "loss": 0.7644,
      "step": 84500
    },
    {
      "epoch": 1.3281004709576139,
      "grad_norm": 4.926188945770264,
      "learning_rate": 4.9169937205651495e-05,
      "loss": 0.7341,
      "step": 84600
    },
    {
      "epoch": 1.3296703296703296,
      "grad_norm": 4.073534965515137,
      "learning_rate": 4.9168956043956046e-05,
      "loss": 0.7493,
      "step": 84700
    },
    {
      "epoch": 1.3312401883830456,
      "grad_norm": 4.787813663482666,
      "learning_rate": 4.91679748822606e-05,
      "loss": 0.7386,
      "step": 84800
    },
    {
      "epoch": 1.3328100470957613,
      "grad_norm": 3.871919870376587,
      "learning_rate": 4.9166993720565154e-05,
      "loss": 0.6941,
      "step": 84900
    },
    {
      "epoch": 1.3343799058084773,
      "grad_norm": 6.538022041320801,
      "learning_rate": 4.9166012558869705e-05,
      "loss": 0.7886,
      "step": 85000
    },
    {
      "epoch": 1.335949764521193,
      "grad_norm": 4.0036234855651855,
      "learning_rate": 4.9165031397174256e-05,
      "loss": 0.7544,
      "step": 85100
    },
    {
      "epoch": 1.3375196232339088,
      "grad_norm": 5.498431205749512,
      "learning_rate": 4.916405023547881e-05,
      "loss": 0.73,
      "step": 85200
    },
    {
      "epoch": 1.3390894819466248,
      "grad_norm": 4.951104164123535,
      "learning_rate": 4.9163069073783365e-05,
      "loss": 0.7859,
      "step": 85300
    },
    {
      "epoch": 1.3406593406593408,
      "grad_norm": 3.8102145195007324,
      "learning_rate": 4.916208791208791e-05,
      "loss": 0.7356,
      "step": 85400
    },
    {
      "epoch": 1.3422291993720565,
      "grad_norm": 5.068419933319092,
      "learning_rate": 4.916110675039247e-05,
      "loss": 0.7204,
      "step": 85500
    },
    {
      "epoch": 1.3437990580847723,
      "grad_norm": 5.229875087738037,
      "learning_rate": 4.916012558869702e-05,
      "loss": 0.7694,
      "step": 85600
    },
    {
      "epoch": 1.3453689167974883,
      "grad_norm": 3.1962132453918457,
      "learning_rate": 4.9159144427001576e-05,
      "loss": 0.7431,
      "step": 85700
    },
    {
      "epoch": 1.346938775510204,
      "grad_norm": 4.244580268859863,
      "learning_rate": 4.915816326530612e-05,
      "loss": 0.7549,
      "step": 85800
    },
    {
      "epoch": 1.34850863422292,
      "grad_norm": 4.156358242034912,
      "learning_rate": 4.915718210361068e-05,
      "loss": 0.7347,
      "step": 85900
    },
    {
      "epoch": 1.3500784929356358,
      "grad_norm": 5.537713527679443,
      "learning_rate": 4.915620094191523e-05,
      "loss": 0.7763,
      "step": 86000
    },
    {
      "epoch": 1.3516483516483517,
      "grad_norm": 4.181643009185791,
      "learning_rate": 4.915521978021978e-05,
      "loss": 0.7276,
      "step": 86100
    },
    {
      "epoch": 1.3532182103610675,
      "grad_norm": 3.248134136199951,
      "learning_rate": 4.915423861852434e-05,
      "loss": 0.771,
      "step": 86200
    },
    {
      "epoch": 1.3547880690737832,
      "grad_norm": 3.6310031414031982,
      "learning_rate": 4.915325745682889e-05,
      "loss": 0.7039,
      "step": 86300
    },
    {
      "epoch": 1.3563579277864992,
      "grad_norm": 4.884064674377441,
      "learning_rate": 4.915227629513344e-05,
      "loss": 0.7628,
      "step": 86400
    },
    {
      "epoch": 1.3579277864992152,
      "grad_norm": 3.5595436096191406,
      "learning_rate": 4.915129513343799e-05,
      "loss": 0.7485,
      "step": 86500
    },
    {
      "epoch": 1.359497645211931,
      "grad_norm": 3.982675075531006,
      "learning_rate": 4.915031397174255e-05,
      "loss": 0.7576,
      "step": 86600
    },
    {
      "epoch": 1.3610675039246467,
      "grad_norm": 4.6952433586120605,
      "learning_rate": 4.91493328100471e-05,
      "loss": 0.7863,
      "step": 86700
    },
    {
      "epoch": 1.3626373626373627,
      "grad_norm": 3.2875804901123047,
      "learning_rate": 4.914835164835165e-05,
      "loss": 0.721,
      "step": 86800
    },
    {
      "epoch": 1.3642072213500784,
      "grad_norm": 3.9342997074127197,
      "learning_rate": 4.91473704866562e-05,
      "loss": 0.7263,
      "step": 86900
    },
    {
      "epoch": 1.3657770800627944,
      "grad_norm": 5.08966588973999,
      "learning_rate": 4.914638932496076e-05,
      "loss": 0.7519,
      "step": 87000
    },
    {
      "epoch": 1.3673469387755102,
      "grad_norm": 3.9374608993530273,
      "learning_rate": 4.914540816326531e-05,
      "loss": 0.7435,
      "step": 87100
    },
    {
      "epoch": 1.3689167974882261,
      "grad_norm": 4.761080741882324,
      "learning_rate": 4.914442700156986e-05,
      "loss": 0.7753,
      "step": 87200
    },
    {
      "epoch": 1.370486656200942,
      "grad_norm": 3.1290974617004395,
      "learning_rate": 4.914344583987441e-05,
      "loss": 0.7891,
      "step": 87300
    },
    {
      "epoch": 1.3720565149136577,
      "grad_norm": 5.081421375274658,
      "learning_rate": 4.914246467817897e-05,
      "loss": 0.7715,
      "step": 87400
    },
    {
      "epoch": 1.3736263736263736,
      "grad_norm": 4.71203088760376,
      "learning_rate": 4.9141483516483514e-05,
      "loss": 0.7603,
      "step": 87500
    },
    {
      "epoch": 1.3751962323390896,
      "grad_norm": 4.577914237976074,
      "learning_rate": 4.914050235478807e-05,
      "loss": 0.7332,
      "step": 87600
    },
    {
      "epoch": 1.3767660910518054,
      "grad_norm": 3.9747090339660645,
      "learning_rate": 4.913952119309262e-05,
      "loss": 0.7474,
      "step": 87700
    },
    {
      "epoch": 1.3783359497645211,
      "grad_norm": 4.452672004699707,
      "learning_rate": 4.913854003139718e-05,
      "loss": 0.7621,
      "step": 87800
    },
    {
      "epoch": 1.379905808477237,
      "grad_norm": 4.59859561920166,
      "learning_rate": 4.9137558869701724e-05,
      "loss": 0.7406,
      "step": 87900
    },
    {
      "epoch": 1.3814756671899528,
      "grad_norm": 3.5600624084472656,
      "learning_rate": 4.913657770800628e-05,
      "loss": 0.7545,
      "step": 88000
    },
    {
      "epoch": 1.3830455259026688,
      "grad_norm": 4.550363063812256,
      "learning_rate": 4.913559654631083e-05,
      "loss": 0.7669,
      "step": 88100
    },
    {
      "epoch": 1.3846153846153846,
      "grad_norm": 3.9835803508758545,
      "learning_rate": 4.9134615384615384e-05,
      "loss": 0.7386,
      "step": 88200
    },
    {
      "epoch": 1.3861852433281006,
      "grad_norm": 4.698947429656982,
      "learning_rate": 4.913363422291994e-05,
      "loss": 0.7217,
      "step": 88300
    },
    {
      "epoch": 1.3877551020408163,
      "grad_norm": 3.57379150390625,
      "learning_rate": 4.913265306122449e-05,
      "loss": 0.7824,
      "step": 88400
    },
    {
      "epoch": 1.389324960753532,
      "grad_norm": 5.338812351226807,
      "learning_rate": 4.9131671899529044e-05,
      "loss": 0.7546,
      "step": 88500
    },
    {
      "epoch": 1.390894819466248,
      "grad_norm": 3.3336341381073,
      "learning_rate": 4.9130690737833595e-05,
      "loss": 0.7333,
      "step": 88600
    },
    {
      "epoch": 1.392464678178964,
      "grad_norm": 3.3685789108276367,
      "learning_rate": 4.912970957613815e-05,
      "loss": 0.7249,
      "step": 88700
    },
    {
      "epoch": 1.3940345368916798,
      "grad_norm": 3.517313003540039,
      "learning_rate": 4.9128728414442704e-05,
      "loss": 0.7818,
      "step": 88800
    },
    {
      "epoch": 1.3956043956043955,
      "grad_norm": 4.162750720977783,
      "learning_rate": 4.9127747252747255e-05,
      "loss": 0.7921,
      "step": 88900
    },
    {
      "epoch": 1.3971742543171115,
      "grad_norm": 3.630221366882324,
      "learning_rate": 4.9126766091051806e-05,
      "loss": 0.7827,
      "step": 89000
    },
    {
      "epoch": 1.3987441130298273,
      "grad_norm": 4.161312580108643,
      "learning_rate": 4.912578492935636e-05,
      "loss": 0.6899,
      "step": 89100
    },
    {
      "epoch": 1.4003139717425432,
      "grad_norm": 5.424911022186279,
      "learning_rate": 4.9124803767660914e-05,
      "loss": 0.7822,
      "step": 89200
    },
    {
      "epoch": 1.401883830455259,
      "grad_norm": 4.691526412963867,
      "learning_rate": 4.9123822605965465e-05,
      "loss": 0.7423,
      "step": 89300
    },
    {
      "epoch": 1.403453689167975,
      "grad_norm": 4.422622203826904,
      "learning_rate": 4.9122841444270016e-05,
      "loss": 0.7311,
      "step": 89400
    },
    {
      "epoch": 1.4050235478806907,
      "grad_norm": 4.242785453796387,
      "learning_rate": 4.9121860282574574e-05,
      "loss": 0.7231,
      "step": 89500
    },
    {
      "epoch": 1.4065934065934065,
      "grad_norm": 4.885825157165527,
      "learning_rate": 4.912087912087912e-05,
      "loss": 0.7364,
      "step": 89600
    },
    {
      "epoch": 1.4081632653061225,
      "grad_norm": 4.564454555511475,
      "learning_rate": 4.9119897959183676e-05,
      "loss": 0.7147,
      "step": 89700
    },
    {
      "epoch": 1.4097331240188384,
      "grad_norm": 4.632638931274414,
      "learning_rate": 4.911891679748823e-05,
      "loss": 0.7576,
      "step": 89800
    },
    {
      "epoch": 1.4113029827315542,
      "grad_norm": 3.935163974761963,
      "learning_rate": 4.9117935635792785e-05,
      "loss": 0.7843,
      "step": 89900
    },
    {
      "epoch": 1.41287284144427,
      "grad_norm": 4.383311748504639,
      "learning_rate": 4.911695447409733e-05,
      "loss": 0.7651,
      "step": 90000
    },
    {
      "epoch": 1.414442700156986,
      "grad_norm": 4.8329362869262695,
      "learning_rate": 4.911597331240189e-05,
      "loss": 0.7384,
      "step": 90100
    },
    {
      "epoch": 1.4160125588697017,
      "grad_norm": 4.082204341888428,
      "learning_rate": 4.911499215070644e-05,
      "loss": 0.7797,
      "step": 90200
    },
    {
      "epoch": 1.4175824175824177,
      "grad_norm": 4.335079193115234,
      "learning_rate": 4.911401098901099e-05,
      "loss": 0.7784,
      "step": 90300
    },
    {
      "epoch": 1.4191522762951334,
      "grad_norm": 4.383034706115723,
      "learning_rate": 4.9113029827315546e-05,
      "loss": 0.7406,
      "step": 90400
    },
    {
      "epoch": 1.4207221350078494,
      "grad_norm": 3.828953742980957,
      "learning_rate": 4.91120486656201e-05,
      "loss": 0.7818,
      "step": 90500
    },
    {
      "epoch": 1.4222919937205651,
      "grad_norm": 4.833615779876709,
      "learning_rate": 4.911106750392465e-05,
      "loss": 0.7461,
      "step": 90600
    },
    {
      "epoch": 1.423861852433281,
      "grad_norm": 3.2666144371032715,
      "learning_rate": 4.91100863422292e-05,
      "loss": 0.758,
      "step": 90700
    },
    {
      "epoch": 1.4254317111459969,
      "grad_norm": 4.679104804992676,
      "learning_rate": 4.910910518053376e-05,
      "loss": 0.7386,
      "step": 90800
    },
    {
      "epoch": 1.4270015698587128,
      "grad_norm": 4.387648582458496,
      "learning_rate": 4.910812401883831e-05,
      "loss": 0.7758,
      "step": 90900
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 4.515703201293945,
      "learning_rate": 4.910714285714286e-05,
      "loss": 0.7754,
      "step": 91000
    },
    {
      "epoch": 1.4301412872841444,
      "grad_norm": 4.9265666007995605,
      "learning_rate": 4.910616169544741e-05,
      "loss": 0.7584,
      "step": 91100
    },
    {
      "epoch": 1.4317111459968603,
      "grad_norm": 4.155123710632324,
      "learning_rate": 4.910518053375197e-05,
      "loss": 0.7415,
      "step": 91200
    },
    {
      "epoch": 1.433281004709576,
      "grad_norm": 4.1411638259887695,
      "learning_rate": 4.910419937205652e-05,
      "loss": 0.7195,
      "step": 91300
    },
    {
      "epoch": 1.434850863422292,
      "grad_norm": 3.938575267791748,
      "learning_rate": 4.910321821036107e-05,
      "loss": 0.7706,
      "step": 91400
    },
    {
      "epoch": 1.4364207221350078,
      "grad_norm": 2.8679251670837402,
      "learning_rate": 4.910223704866562e-05,
      "loss": 0.7486,
      "step": 91500
    },
    {
      "epoch": 1.4379905808477238,
      "grad_norm": 5.082307815551758,
      "learning_rate": 4.910125588697018e-05,
      "loss": 0.7429,
      "step": 91600
    },
    {
      "epoch": 1.4395604395604396,
      "grad_norm": 4.670349597930908,
      "learning_rate": 4.910027472527472e-05,
      "loss": 0.7851,
      "step": 91700
    },
    {
      "epoch": 1.4411302982731553,
      "grad_norm": 4.749091148376465,
      "learning_rate": 4.909929356357928e-05,
      "loss": 0.7987,
      "step": 91800
    },
    {
      "epoch": 1.4427001569858713,
      "grad_norm": 4.109521865844727,
      "learning_rate": 4.909831240188383e-05,
      "loss": 0.7451,
      "step": 91900
    },
    {
      "epoch": 1.4442700156985873,
      "grad_norm": 3.9905664920806885,
      "learning_rate": 4.909733124018839e-05,
      "loss": 0.6991,
      "step": 92000
    },
    {
      "epoch": 1.445839874411303,
      "grad_norm": 4.018106937408447,
      "learning_rate": 4.9096350078492933e-05,
      "loss": 0.7748,
      "step": 92100
    },
    {
      "epoch": 1.4474097331240188,
      "grad_norm": 3.3935327529907227,
      "learning_rate": 4.909536891679749e-05,
      "loss": 0.7603,
      "step": 92200
    },
    {
      "epoch": 1.4489795918367347,
      "grad_norm": 4.519028186798096,
      "learning_rate": 4.909438775510204e-05,
      "loss": 0.7746,
      "step": 92300
    },
    {
      "epoch": 1.4505494505494505,
      "grad_norm": 3.995676279067993,
      "learning_rate": 4.909340659340659e-05,
      "loss": 0.7563,
      "step": 92400
    },
    {
      "epoch": 1.4521193092621665,
      "grad_norm": 3.9571385383605957,
      "learning_rate": 4.909242543171115e-05,
      "loss": 0.7268,
      "step": 92500
    },
    {
      "epoch": 1.4536891679748822,
      "grad_norm": 3.664682149887085,
      "learning_rate": 4.90914442700157e-05,
      "loss": 0.7377,
      "step": 92600
    },
    {
      "epoch": 1.4552590266875982,
      "grad_norm": 4.744308948516846,
      "learning_rate": 4.909046310832025e-05,
      "loss": 0.7569,
      "step": 92700
    },
    {
      "epoch": 1.456828885400314,
      "grad_norm": 4.402767658233643,
      "learning_rate": 4.9089481946624804e-05,
      "loss": 0.7231,
      "step": 92800
    },
    {
      "epoch": 1.4583987441130297,
      "grad_norm": 4.517458438873291,
      "learning_rate": 4.908850078492936e-05,
      "loss": 0.7582,
      "step": 92900
    },
    {
      "epoch": 1.4599686028257457,
      "grad_norm": 3.1073105335235596,
      "learning_rate": 4.908751962323391e-05,
      "loss": 0.7245,
      "step": 93000
    },
    {
      "epoch": 1.4615384615384617,
      "grad_norm": 3.8838632106781006,
      "learning_rate": 4.9086538461538464e-05,
      "loss": 0.7561,
      "step": 93100
    },
    {
      "epoch": 1.4631083202511774,
      "grad_norm": 4.940913677215576,
      "learning_rate": 4.9085557299843015e-05,
      "loss": 0.772,
      "step": 93200
    },
    {
      "epoch": 1.4646781789638932,
      "grad_norm": 3.208104133605957,
      "learning_rate": 4.908457613814757e-05,
      "loss": 0.7544,
      "step": 93300
    },
    {
      "epoch": 1.4662480376766092,
      "grad_norm": 3.661371946334839,
      "learning_rate": 4.908359497645212e-05,
      "loss": 0.7287,
      "step": 93400
    },
    {
      "epoch": 1.467817896389325,
      "grad_norm": 3.922619104385376,
      "learning_rate": 4.9082613814756674e-05,
      "loss": 0.7168,
      "step": 93500
    },
    {
      "epoch": 1.469387755102041,
      "grad_norm": 4.566423416137695,
      "learning_rate": 4.9081632653061225e-05,
      "loss": 0.756,
      "step": 93600
    },
    {
      "epoch": 1.4709576138147566,
      "grad_norm": 4.9696125984191895,
      "learning_rate": 4.908065149136578e-05,
      "loss": 0.7577,
      "step": 93700
    },
    {
      "epoch": 1.4725274725274726,
      "grad_norm": 4.058633804321289,
      "learning_rate": 4.907967032967033e-05,
      "loss": 0.7477,
      "step": 93800
    },
    {
      "epoch": 1.4740973312401884,
      "grad_norm": 3.7493391036987305,
      "learning_rate": 4.9078689167974885e-05,
      "loss": 0.7225,
      "step": 93900
    },
    {
      "epoch": 1.4756671899529041,
      "grad_norm": 4.879592418670654,
      "learning_rate": 4.9077708006279436e-05,
      "loss": 0.7378,
      "step": 94000
    },
    {
      "epoch": 1.4772370486656201,
      "grad_norm": 3.816091775894165,
      "learning_rate": 4.9076726844583994e-05,
      "loss": 0.7271,
      "step": 94100
    },
    {
      "epoch": 1.478806907378336,
      "grad_norm": 4.415065288543701,
      "learning_rate": 4.907574568288854e-05,
      "loss": 0.7155,
      "step": 94200
    },
    {
      "epoch": 1.4803767660910518,
      "grad_norm": 3.5826196670532227,
      "learning_rate": 4.9074764521193096e-05,
      "loss": 0.7576,
      "step": 94300
    },
    {
      "epoch": 1.4819466248037676,
      "grad_norm": 3.806749105453491,
      "learning_rate": 4.907378335949765e-05,
      "loss": 0.7341,
      "step": 94400
    },
    {
      "epoch": 1.4835164835164836,
      "grad_norm": 3.3721072673797607,
      "learning_rate": 4.90728021978022e-05,
      "loss": 0.7298,
      "step": 94500
    },
    {
      "epoch": 1.4850863422291993,
      "grad_norm": 4.116828441619873,
      "learning_rate": 4.9071821036106755e-05,
      "loss": 0.7569,
      "step": 94600
    },
    {
      "epoch": 1.4866562009419153,
      "grad_norm": 5.105883598327637,
      "learning_rate": 4.9070839874411306e-05,
      "loss": 0.749,
      "step": 94700
    },
    {
      "epoch": 1.488226059654631,
      "grad_norm": 4.682467937469482,
      "learning_rate": 4.906985871271586e-05,
      "loss": 0.7461,
      "step": 94800
    },
    {
      "epoch": 1.489795918367347,
      "grad_norm": 3.1366701126098633,
      "learning_rate": 4.906887755102041e-05,
      "loss": 0.7291,
      "step": 94900
    },
    {
      "epoch": 1.4913657770800628,
      "grad_norm": 4.1312479972839355,
      "learning_rate": 4.9067896389324966e-05,
      "loss": 0.7339,
      "step": 95000
    },
    {
      "epoch": 1.4929356357927785,
      "grad_norm": 4.349081993103027,
      "learning_rate": 4.906691522762952e-05,
      "loss": 0.7623,
      "step": 95100
    },
    {
      "epoch": 1.4945054945054945,
      "grad_norm": 3.190756320953369,
      "learning_rate": 4.906593406593407e-05,
      "loss": 0.743,
      "step": 95200
    },
    {
      "epoch": 1.4960753532182103,
      "grad_norm": 3.2969138622283936,
      "learning_rate": 4.906495290423862e-05,
      "loss": 0.7374,
      "step": 95300
    },
    {
      "epoch": 1.4976452119309263,
      "grad_norm": 4.105679988861084,
      "learning_rate": 4.906397174254318e-05,
      "loss": 0.7154,
      "step": 95400
    },
    {
      "epoch": 1.499215070643642,
      "grad_norm": 3.5319979190826416,
      "learning_rate": 4.906299058084773e-05,
      "loss": 0.7425,
      "step": 95500
    },
    {
      "epoch": 1.500784929356358,
      "grad_norm": 3.9094362258911133,
      "learning_rate": 4.906200941915228e-05,
      "loss": 0.7758,
      "step": 95600
    },
    {
      "epoch": 1.5023547880690737,
      "grad_norm": 2.9128715991973877,
      "learning_rate": 4.906102825745683e-05,
      "loss": 0.7947,
      "step": 95700
    },
    {
      "epoch": 1.5039246467817895,
      "grad_norm": 3.642402172088623,
      "learning_rate": 4.906004709576139e-05,
      "loss": 0.703,
      "step": 95800
    },
    {
      "epoch": 1.5054945054945055,
      "grad_norm": 3.1922507286071777,
      "learning_rate": 4.905906593406593e-05,
      "loss": 0.7083,
      "step": 95900
    },
    {
      "epoch": 1.5070643642072215,
      "grad_norm": 3.9578311443328857,
      "learning_rate": 4.905808477237049e-05,
      "loss": 0.698,
      "step": 96000
    },
    {
      "epoch": 1.5086342229199372,
      "grad_norm": 4.6916913986206055,
      "learning_rate": 4.905710361067504e-05,
      "loss": 0.798,
      "step": 96100
    },
    {
      "epoch": 1.510204081632653,
      "grad_norm": 3.7722880840301514,
      "learning_rate": 4.90561224489796e-05,
      "loss": 0.7263,
      "step": 96200
    },
    {
      "epoch": 1.511773940345369,
      "grad_norm": 4.2974748611450195,
      "learning_rate": 4.905514128728414e-05,
      "loss": 0.7822,
      "step": 96300
    },
    {
      "epoch": 1.513343799058085,
      "grad_norm": 4.895412921905518,
      "learning_rate": 4.90541601255887e-05,
      "loss": 0.7876,
      "step": 96400
    },
    {
      "epoch": 1.5149136577708007,
      "grad_norm": 4.006764888763428,
      "learning_rate": 4.905317896389325e-05,
      "loss": 0.7447,
      "step": 96500
    },
    {
      "epoch": 1.5164835164835164,
      "grad_norm": 4.430293083190918,
      "learning_rate": 4.90521978021978e-05,
      "loss": 0.7373,
      "step": 96600
    },
    {
      "epoch": 1.5180533751962324,
      "grad_norm": 4.034417629241943,
      "learning_rate": 4.905121664050235e-05,
      "loss": 0.7388,
      "step": 96700
    },
    {
      "epoch": 1.5196232339089482,
      "grad_norm": 4.811624526977539,
      "learning_rate": 4.905023547880691e-05,
      "loss": 0.7824,
      "step": 96800
    },
    {
      "epoch": 1.521193092621664,
      "grad_norm": 4.373205184936523,
      "learning_rate": 4.904925431711146e-05,
      "loss": 0.7641,
      "step": 96900
    },
    {
      "epoch": 1.5227629513343799,
      "grad_norm": 3.9551987648010254,
      "learning_rate": 4.904827315541601e-05,
      "loss": 0.7378,
      "step": 97000
    },
    {
      "epoch": 1.5243328100470959,
      "grad_norm": 3.351914405822754,
      "learning_rate": 4.904729199372057e-05,
      "loss": 0.7681,
      "step": 97100
    },
    {
      "epoch": 1.5259026687598116,
      "grad_norm": 3.789283514022827,
      "learning_rate": 4.904631083202512e-05,
      "loss": 0.7377,
      "step": 97200
    },
    {
      "epoch": 1.5274725274725274,
      "grad_norm": 4.3244853019714355,
      "learning_rate": 4.904532967032967e-05,
      "loss": 0.7447,
      "step": 97300
    },
    {
      "epoch": 1.5290423861852434,
      "grad_norm": 1.927196979522705,
      "learning_rate": 4.9044348508634224e-05,
      "loss": 0.7167,
      "step": 97400
    },
    {
      "epoch": 1.5306122448979593,
      "grad_norm": 4.5181050300598145,
      "learning_rate": 4.904336734693878e-05,
      "loss": 0.7744,
      "step": 97500
    },
    {
      "epoch": 1.532182103610675,
      "grad_norm": 4.635211944580078,
      "learning_rate": 4.904238618524333e-05,
      "loss": 0.7365,
      "step": 97600
    },
    {
      "epoch": 1.5337519623233908,
      "grad_norm": 4.412332534790039,
      "learning_rate": 4.904140502354788e-05,
      "loss": 0.7706,
      "step": 97700
    },
    {
      "epoch": 1.5353218210361068,
      "grad_norm": 4.114267826080322,
      "learning_rate": 4.9040423861852434e-05,
      "loss": 0.7636,
      "step": 97800
    },
    {
      "epoch": 1.5368916797488226,
      "grad_norm": 5.386493682861328,
      "learning_rate": 4.903944270015699e-05,
      "loss": 0.7477,
      "step": 97900
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 4.257207870483398,
      "learning_rate": 4.9038461538461536e-05,
      "loss": 0.7432,
      "step": 98000
    },
    {
      "epoch": 1.5400313971742543,
      "grad_norm": 4.549527168273926,
      "learning_rate": 4.9037480376766094e-05,
      "loss": 0.7939,
      "step": 98100
    },
    {
      "epoch": 1.5416012558869703,
      "grad_norm": 3.979297161102295,
      "learning_rate": 4.9036499215070645e-05,
      "loss": 0.704,
      "step": 98200
    },
    {
      "epoch": 1.543171114599686,
      "grad_norm": 4.345700263977051,
      "learning_rate": 4.90355180533752e-05,
      "loss": 0.7408,
      "step": 98300
    },
    {
      "epoch": 1.5447409733124018,
      "grad_norm": 4.523331642150879,
      "learning_rate": 4.903453689167975e-05,
      "loss": 0.732,
      "step": 98400
    },
    {
      "epoch": 1.5463108320251178,
      "grad_norm": 3.7150957584381104,
      "learning_rate": 4.9033555729984305e-05,
      "loss": 0.7334,
      "step": 98500
    },
    {
      "epoch": 1.5478806907378337,
      "grad_norm": 3.753464698791504,
      "learning_rate": 4.9032574568288856e-05,
      "loss": 0.7128,
      "step": 98600
    },
    {
      "epoch": 1.5494505494505495,
      "grad_norm": 4.5967912673950195,
      "learning_rate": 4.903159340659341e-05,
      "loss": 0.7015,
      "step": 98700
    },
    {
      "epoch": 1.5510204081632653,
      "grad_norm": 4.561300754547119,
      "learning_rate": 4.903061224489796e-05,
      "loss": 0.7368,
      "step": 98800
    },
    {
      "epoch": 1.5525902668759812,
      "grad_norm": 4.789823055267334,
      "learning_rate": 4.9029631083202515e-05,
      "loss": 0.7182,
      "step": 98900
    },
    {
      "epoch": 1.554160125588697,
      "grad_norm": 3.0798685550689697,
      "learning_rate": 4.9028649921507066e-05,
      "loss": 0.7615,
      "step": 99000
    },
    {
      "epoch": 1.5557299843014127,
      "grad_norm": 4.970143795013428,
      "learning_rate": 4.902766875981162e-05,
      "loss": 0.7518,
      "step": 99100
    },
    {
      "epoch": 1.5572998430141287,
      "grad_norm": 4.174806594848633,
      "learning_rate": 4.9026687598116175e-05,
      "loss": 0.7429,
      "step": 99200
    },
    {
      "epoch": 1.5588697017268447,
      "grad_norm": 3.8004684448242188,
      "learning_rate": 4.9025706436420726e-05,
      "loss": 0.7775,
      "step": 99300
    },
    {
      "epoch": 1.5604395604395604,
      "grad_norm": 4.074008464813232,
      "learning_rate": 4.902472527472528e-05,
      "loss": 0.7758,
      "step": 99400
    },
    {
      "epoch": 1.5620094191522762,
      "grad_norm": 4.3148369789123535,
      "learning_rate": 4.902374411302983e-05,
      "loss": 0.7234,
      "step": 99500
    },
    {
      "epoch": 1.5635792778649922,
      "grad_norm": 4.3228302001953125,
      "learning_rate": 4.9022762951334386e-05,
      "loss": 0.7915,
      "step": 99600
    },
    {
      "epoch": 1.5651491365777082,
      "grad_norm": 4.465612411499023,
      "learning_rate": 4.902178178963894e-05,
      "loss": 0.7268,
      "step": 99700
    },
    {
      "epoch": 1.566718995290424,
      "grad_norm": 4.1957831382751465,
      "learning_rate": 4.902080062794349e-05,
      "loss": 0.7302,
      "step": 99800
    },
    {
      "epoch": 1.5682888540031397,
      "grad_norm": 4.283447742462158,
      "learning_rate": 4.901981946624804e-05,
      "loss": 0.7143,
      "step": 99900
    },
    {
      "epoch": 1.5698587127158556,
      "grad_norm": 3.544281482696533,
      "learning_rate": 4.9018838304552597e-05,
      "loss": 0.7255,
      "step": 100000
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 4.672060012817383,
      "learning_rate": 4.901785714285714e-05,
      "loss": 0.7215,
      "step": 100100
    },
    {
      "epoch": 1.5729984301412872,
      "grad_norm": 4.492674350738525,
      "learning_rate": 4.90168759811617e-05,
      "loss": 0.8008,
      "step": 100200
    },
    {
      "epoch": 1.5745682888540031,
      "grad_norm": 4.125282287597656,
      "learning_rate": 4.901589481946625e-05,
      "loss": 0.7362,
      "step": 100300
    },
    {
      "epoch": 1.576138147566719,
      "grad_norm": 4.0159101486206055,
      "learning_rate": 4.901491365777081e-05,
      "loss": 0.7392,
      "step": 100400
    },
    {
      "epoch": 1.5777080062794349,
      "grad_norm": 4.722817420959473,
      "learning_rate": 4.901393249607535e-05,
      "loss": 0.7129,
      "step": 100500
    },
    {
      "epoch": 1.5792778649921506,
      "grad_norm": 4.596367835998535,
      "learning_rate": 4.901295133437991e-05,
      "loss": 0.7626,
      "step": 100600
    },
    {
      "epoch": 1.5808477237048666,
      "grad_norm": 4.359861373901367,
      "learning_rate": 4.901197017268446e-05,
      "loss": 0.7503,
      "step": 100700
    },
    {
      "epoch": 1.5824175824175826,
      "grad_norm": 3.51080584526062,
      "learning_rate": 4.901098901098901e-05,
      "loss": 0.7399,
      "step": 100800
    },
    {
      "epoch": 1.5839874411302983,
      "grad_norm": 4.364923477172852,
      "learning_rate": 4.901000784929356e-05,
      "loss": 0.7458,
      "step": 100900
    },
    {
      "epoch": 1.585557299843014,
      "grad_norm": 2.4231462478637695,
      "learning_rate": 4.900902668759812e-05,
      "loss": 0.7282,
      "step": 101000
    },
    {
      "epoch": 1.58712715855573,
      "grad_norm": 5.580516338348389,
      "learning_rate": 4.900804552590267e-05,
      "loss": 0.7112,
      "step": 101100
    },
    {
      "epoch": 1.5886970172684458,
      "grad_norm": 4.8214802742004395,
      "learning_rate": 4.900706436420722e-05,
      "loss": 0.7138,
      "step": 101200
    },
    {
      "epoch": 1.5902668759811616,
      "grad_norm": 4.262359619140625,
      "learning_rate": 4.900608320251178e-05,
      "loss": 0.7496,
      "step": 101300
    },
    {
      "epoch": 1.5918367346938775,
      "grad_norm": 4.298004150390625,
      "learning_rate": 4.900510204081633e-05,
      "loss": 0.7734,
      "step": 101400
    },
    {
      "epoch": 1.5934065934065935,
      "grad_norm": 3.1699061393737793,
      "learning_rate": 4.900412087912088e-05,
      "loss": 0.7283,
      "step": 101500
    },
    {
      "epoch": 1.5949764521193093,
      "grad_norm": 4.558064937591553,
      "learning_rate": 4.900313971742543e-05,
      "loss": 0.768,
      "step": 101600
    },
    {
      "epoch": 1.596546310832025,
      "grad_norm": 3.495560646057129,
      "learning_rate": 4.900215855572999e-05,
      "loss": 0.7548,
      "step": 101700
    },
    {
      "epoch": 1.598116169544741,
      "grad_norm": 4.160162925720215,
      "learning_rate": 4.900117739403454e-05,
      "loss": 0.7543,
      "step": 101800
    },
    {
      "epoch": 1.599686028257457,
      "grad_norm": 3.8510425090789795,
      "learning_rate": 4.900019623233909e-05,
      "loss": 0.7458,
      "step": 101900
    },
    {
      "epoch": 1.6012558869701727,
      "grad_norm": 4.586343288421631,
      "learning_rate": 4.899921507064364e-05,
      "loss": 0.7686,
      "step": 102000
    },
    {
      "epoch": 1.6028257456828885,
      "grad_norm": 4.232314586639404,
      "learning_rate": 4.89982339089482e-05,
      "loss": 0.7517,
      "step": 102100
    },
    {
      "epoch": 1.6043956043956045,
      "grad_norm": 4.888771057128906,
      "learning_rate": 4.8997252747252745e-05,
      "loss": 0.7698,
      "step": 102200
    },
    {
      "epoch": 1.6059654631083202,
      "grad_norm": 4.226758003234863,
      "learning_rate": 4.89962715855573e-05,
      "loss": 0.7519,
      "step": 102300
    },
    {
      "epoch": 1.607535321821036,
      "grad_norm": 3.9484431743621826,
      "learning_rate": 4.8995290423861854e-05,
      "loss": 0.7614,
      "step": 102400
    },
    {
      "epoch": 1.609105180533752,
      "grad_norm": 3.717705726623535,
      "learning_rate": 4.8994309262166405e-05,
      "loss": 0.7215,
      "step": 102500
    },
    {
      "epoch": 1.610675039246468,
      "grad_norm": 4.08009147644043,
      "learning_rate": 4.8993328100470956e-05,
      "loss": 0.7451,
      "step": 102600
    },
    {
      "epoch": 1.6122448979591837,
      "grad_norm": 4.161942481994629,
      "learning_rate": 4.8992346938775514e-05,
      "loss": 0.7618,
      "step": 102700
    },
    {
      "epoch": 1.6138147566718994,
      "grad_norm": 4.306634902954102,
      "learning_rate": 4.8991365777080065e-05,
      "loss": 0.72,
      "step": 102800
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 4.586840629577637,
      "learning_rate": 4.8990384615384616e-05,
      "loss": 0.6905,
      "step": 102900
    },
    {
      "epoch": 1.6169544740973314,
      "grad_norm": 3.6396539211273193,
      "learning_rate": 4.898940345368917e-05,
      "loss": 0.7644,
      "step": 103000
    },
    {
      "epoch": 1.6185243328100472,
      "grad_norm": 4.2462592124938965,
      "learning_rate": 4.8988422291993724e-05,
      "loss": 0.7294,
      "step": 103100
    },
    {
      "epoch": 1.620094191522763,
      "grad_norm": 4.277878761291504,
      "learning_rate": 4.8987441130298275e-05,
      "loss": 0.6862,
      "step": 103200
    },
    {
      "epoch": 1.6216640502354789,
      "grad_norm": 4.203491687774658,
      "learning_rate": 4.8986459968602826e-05,
      "loss": 0.7072,
      "step": 103300
    },
    {
      "epoch": 1.6232339089481946,
      "grad_norm": 3.1142170429229736,
      "learning_rate": 4.8985478806907384e-05,
      "loss": 0.7543,
      "step": 103400
    },
    {
      "epoch": 1.6248037676609104,
      "grad_norm": 4.261941432952881,
      "learning_rate": 4.8984497645211935e-05,
      "loss": 0.7374,
      "step": 103500
    },
    {
      "epoch": 1.6263736263736264,
      "grad_norm": 3.892000675201416,
      "learning_rate": 4.8983516483516486e-05,
      "loss": 0.7458,
      "step": 103600
    },
    {
      "epoch": 1.6279434850863423,
      "grad_norm": 4.4653520584106445,
      "learning_rate": 4.898253532182104e-05,
      "loss": 0.7438,
      "step": 103700
    },
    {
      "epoch": 1.629513343799058,
      "grad_norm": 4.831782817840576,
      "learning_rate": 4.8981554160125595e-05,
      "loss": 0.7861,
      "step": 103800
    },
    {
      "epoch": 1.6310832025117739,
      "grad_norm": 3.5905299186706543,
      "learning_rate": 4.898057299843014e-05,
      "loss": 0.7674,
      "step": 103900
    },
    {
      "epoch": 1.6326530612244898,
      "grad_norm": 5.109299182891846,
      "learning_rate": 4.89795918367347e-05,
      "loss": 0.7715,
      "step": 104000
    },
    {
      "epoch": 1.6342229199372058,
      "grad_norm": 4.795836448669434,
      "learning_rate": 4.897861067503925e-05,
      "loss": 0.7235,
      "step": 104100
    },
    {
      "epoch": 1.6357927786499213,
      "grad_norm": 3.9185609817504883,
      "learning_rate": 4.8977629513343806e-05,
      "loss": 0.7368,
      "step": 104200
    },
    {
      "epoch": 1.6373626373626373,
      "grad_norm": 3.474360466003418,
      "learning_rate": 4.897664835164835e-05,
      "loss": 0.7165,
      "step": 104300
    },
    {
      "epoch": 1.6389324960753533,
      "grad_norm": 4.58082914352417,
      "learning_rate": 4.897566718995291e-05,
      "loss": 0.7275,
      "step": 104400
    },
    {
      "epoch": 1.640502354788069,
      "grad_norm": 4.820155620574951,
      "learning_rate": 4.897468602825746e-05,
      "loss": 0.7258,
      "step": 104500
    },
    {
      "epoch": 1.6420722135007848,
      "grad_norm": 3.1622085571289062,
      "learning_rate": 4.897370486656201e-05,
      "loss": 0.6909,
      "step": 104600
    },
    {
      "epoch": 1.6436420722135008,
      "grad_norm": 4.048205375671387,
      "learning_rate": 4.897272370486656e-05,
      "loss": 0.716,
      "step": 104700
    },
    {
      "epoch": 1.6452119309262168,
      "grad_norm": 3.542924404144287,
      "learning_rate": 4.897174254317112e-05,
      "loss": 0.7472,
      "step": 104800
    },
    {
      "epoch": 1.6467817896389325,
      "grad_norm": 4.053045272827148,
      "learning_rate": 4.897076138147567e-05,
      "loss": 0.7315,
      "step": 104900
    },
    {
      "epoch": 1.6483516483516483,
      "grad_norm": 4.468730926513672,
      "learning_rate": 4.896978021978022e-05,
      "loss": 0.7768,
      "step": 105000
    },
    {
      "epoch": 1.6499215070643642,
      "grad_norm": 3.283123016357422,
      "learning_rate": 4.896879905808477e-05,
      "loss": 0.7243,
      "step": 105100
    },
    {
      "epoch": 1.6514913657770802,
      "grad_norm": 4.0453691482543945,
      "learning_rate": 4.896781789638933e-05,
      "loss": 0.7187,
      "step": 105200
    },
    {
      "epoch": 1.6530612244897958,
      "grad_norm": 4.711455821990967,
      "learning_rate": 4.896683673469388e-05,
      "loss": 0.7631,
      "step": 105300
    },
    {
      "epoch": 1.6546310832025117,
      "grad_norm": 3.758070230484009,
      "learning_rate": 4.896585557299843e-05,
      "loss": 0.7608,
      "step": 105400
    },
    {
      "epoch": 1.6562009419152277,
      "grad_norm": 3.440945863723755,
      "learning_rate": 4.896487441130299e-05,
      "loss": 0.7462,
      "step": 105500
    },
    {
      "epoch": 1.6577708006279435,
      "grad_norm": 4.529268741607666,
      "learning_rate": 4.896389324960754e-05,
      "loss": 0.7804,
      "step": 105600
    },
    {
      "epoch": 1.6593406593406592,
      "grad_norm": 2.9590439796447754,
      "learning_rate": 4.896291208791209e-05,
      "loss": 0.7778,
      "step": 105700
    },
    {
      "epoch": 1.6609105180533752,
      "grad_norm": 3.3937759399414062,
      "learning_rate": 4.896193092621664e-05,
      "loss": 0.7013,
      "step": 105800
    },
    {
      "epoch": 1.6624803767660912,
      "grad_norm": 4.4515814781188965,
      "learning_rate": 4.89609497645212e-05,
      "loss": 0.7717,
      "step": 105900
    },
    {
      "epoch": 1.664050235478807,
      "grad_norm": 4.422486782073975,
      "learning_rate": 4.8959968602825744e-05,
      "loss": 0.7453,
      "step": 106000
    },
    {
      "epoch": 1.6656200941915227,
      "grad_norm": 3.641953945159912,
      "learning_rate": 4.89589874411303e-05,
      "loss": 0.7916,
      "step": 106100
    },
    {
      "epoch": 1.6671899529042387,
      "grad_norm": 5.515211582183838,
      "learning_rate": 4.895800627943485e-05,
      "loss": 0.7926,
      "step": 106200
    },
    {
      "epoch": 1.6687598116169546,
      "grad_norm": 4.559413909912109,
      "learning_rate": 4.895702511773941e-05,
      "loss": 0.7264,
      "step": 106300
    },
    {
      "epoch": 1.6703296703296702,
      "grad_norm": 5.041187286376953,
      "learning_rate": 4.8956043956043954e-05,
      "loss": 0.758,
      "step": 106400
    },
    {
      "epoch": 1.6718995290423861,
      "grad_norm": 3.5864200592041016,
      "learning_rate": 4.895506279434851e-05,
      "loss": 0.7525,
      "step": 106500
    },
    {
      "epoch": 1.6734693877551021,
      "grad_norm": 3.9477853775024414,
      "learning_rate": 4.895408163265306e-05,
      "loss": 0.7006,
      "step": 106600
    },
    {
      "epoch": 1.6750392464678179,
      "grad_norm": 4.068239688873291,
      "learning_rate": 4.8953100470957614e-05,
      "loss": 0.7387,
      "step": 106700
    },
    {
      "epoch": 1.6766091051805336,
      "grad_norm": 5.531103134155273,
      "learning_rate": 4.8952119309262165e-05,
      "loss": 0.726,
      "step": 106800
    },
    {
      "epoch": 1.6781789638932496,
      "grad_norm": 3.3881354331970215,
      "learning_rate": 4.895113814756672e-05,
      "loss": 0.7774,
      "step": 106900
    },
    {
      "epoch": 1.6797488226059656,
      "grad_norm": 3.22574782371521,
      "learning_rate": 4.8950156985871274e-05,
      "loss": 0.755,
      "step": 107000
    },
    {
      "epoch": 1.6813186813186813,
      "grad_norm": 4.342994213104248,
      "learning_rate": 4.8949175824175825e-05,
      "loss": 0.7339,
      "step": 107100
    },
    {
      "epoch": 1.682888540031397,
      "grad_norm": 4.368803977966309,
      "learning_rate": 4.8948194662480376e-05,
      "loss": 0.678,
      "step": 107200
    },
    {
      "epoch": 1.684458398744113,
      "grad_norm": 4.200274467468262,
      "learning_rate": 4.894721350078493e-05,
      "loss": 0.7335,
      "step": 107300
    },
    {
      "epoch": 1.686028257456829,
      "grad_norm": 4.575896263122559,
      "learning_rate": 4.8946232339089484e-05,
      "loss": 0.7428,
      "step": 107400
    },
    {
      "epoch": 1.6875981161695446,
      "grad_norm": 4.487062931060791,
      "learning_rate": 4.8945251177394035e-05,
      "loss": 0.7241,
      "step": 107500
    },
    {
      "epoch": 1.6891679748822606,
      "grad_norm": 3.92676043510437,
      "learning_rate": 4.894427001569859e-05,
      "loss": 0.6948,
      "step": 107600
    },
    {
      "epoch": 1.6907378335949765,
      "grad_norm": 4.35791015625,
      "learning_rate": 4.8943288854003144e-05,
      "loss": 0.7155,
      "step": 107700
    },
    {
      "epoch": 1.6923076923076923,
      "grad_norm": 4.686146259307861,
      "learning_rate": 4.8942307692307695e-05,
      "loss": 0.738,
      "step": 107800
    },
    {
      "epoch": 1.693877551020408,
      "grad_norm": 3.6447484493255615,
      "learning_rate": 4.8941326530612246e-05,
      "loss": 0.7114,
      "step": 107900
    },
    {
      "epoch": 1.695447409733124,
      "grad_norm": 4.188714504241943,
      "learning_rate": 4.8940345368916804e-05,
      "loss": 0.7352,
      "step": 108000
    },
    {
      "epoch": 1.69701726844584,
      "grad_norm": 3.7857043743133545,
      "learning_rate": 4.893936420722135e-05,
      "loss": 0.7424,
      "step": 108100
    },
    {
      "epoch": 1.6985871271585558,
      "grad_norm": 4.6335554122924805,
      "learning_rate": 4.8938383045525906e-05,
      "loss": 0.7181,
      "step": 108200
    },
    {
      "epoch": 1.7001569858712715,
      "grad_norm": 4.502342224121094,
      "learning_rate": 4.893740188383046e-05,
      "loss": 0.7543,
      "step": 108300
    },
    {
      "epoch": 1.7017268445839875,
      "grad_norm": 4.380910873413086,
      "learning_rate": 4.8936420722135014e-05,
      "loss": 0.7351,
      "step": 108400
    },
    {
      "epoch": 1.7032967032967035,
      "grad_norm": 3.9884772300720215,
      "learning_rate": 4.893543956043956e-05,
      "loss": 0.7009,
      "step": 108500
    },
    {
      "epoch": 1.704866562009419,
      "grad_norm": 4.936764240264893,
      "learning_rate": 4.8934458398744116e-05,
      "loss": 0.7108,
      "step": 108600
    },
    {
      "epoch": 1.706436420722135,
      "grad_norm": 4.604593753814697,
      "learning_rate": 4.893347723704867e-05,
      "loss": 0.7216,
      "step": 108700
    },
    {
      "epoch": 1.708006279434851,
      "grad_norm": 4.6564412117004395,
      "learning_rate": 4.893249607535322e-05,
      "loss": 0.7288,
      "step": 108800
    },
    {
      "epoch": 1.7095761381475667,
      "grad_norm": 4.116952419281006,
      "learning_rate": 4.893151491365777e-05,
      "loss": 0.7311,
      "step": 108900
    },
    {
      "epoch": 1.7111459968602825,
      "grad_norm": 4.064249038696289,
      "learning_rate": 4.893053375196233e-05,
      "loss": 0.744,
      "step": 109000
    },
    {
      "epoch": 1.7127158555729984,
      "grad_norm": 4.244164943695068,
      "learning_rate": 4.892955259026688e-05,
      "loss": 0.7518,
      "step": 109100
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 3.3097808361053467,
      "learning_rate": 4.892857142857143e-05,
      "loss": 0.7227,
      "step": 109200
    },
    {
      "epoch": 1.7158555729984302,
      "grad_norm": 4.041328430175781,
      "learning_rate": 4.892759026687598e-05,
      "loss": 0.7336,
      "step": 109300
    },
    {
      "epoch": 1.717425431711146,
      "grad_norm": 4.4679107666015625,
      "learning_rate": 4.892660910518054e-05,
      "loss": 0.7334,
      "step": 109400
    },
    {
      "epoch": 1.718995290423862,
      "grad_norm": 4.083629608154297,
      "learning_rate": 4.892562794348509e-05,
      "loss": 0.7267,
      "step": 109500
    },
    {
      "epoch": 1.7205651491365777,
      "grad_norm": 4.686398983001709,
      "learning_rate": 4.892464678178964e-05,
      "loss": 0.7559,
      "step": 109600
    },
    {
      "epoch": 1.7221350078492934,
      "grad_norm": 4.404486656188965,
      "learning_rate": 4.89236656200942e-05,
      "loss": 0.7125,
      "step": 109700
    },
    {
      "epoch": 1.7237048665620094,
      "grad_norm": 2.865426778793335,
      "learning_rate": 4.892268445839875e-05,
      "loss": 0.7269,
      "step": 109800
    },
    {
      "epoch": 1.7252747252747254,
      "grad_norm": 5.177528381347656,
      "learning_rate": 4.89217032967033e-05,
      "loss": 0.7734,
      "step": 109900
    },
    {
      "epoch": 1.7268445839874411,
      "grad_norm": 3.8308396339416504,
      "learning_rate": 4.892072213500785e-05,
      "loss": 0.7497,
      "step": 110000
    },
    {
      "epoch": 1.7284144427001569,
      "grad_norm": 4.666362285614014,
      "learning_rate": 4.891974097331241e-05,
      "loss": 0.7466,
      "step": 110100
    },
    {
      "epoch": 1.7299843014128728,
      "grad_norm": 3.224001884460449,
      "learning_rate": 4.891875981161695e-05,
      "loss": 0.7293,
      "step": 110200
    },
    {
      "epoch": 1.7315541601255888,
      "grad_norm": 5.1202311515808105,
      "learning_rate": 4.891777864992151e-05,
      "loss": 0.7039,
      "step": 110300
    },
    {
      "epoch": 1.7331240188383046,
      "grad_norm": 4.908113479614258,
      "learning_rate": 4.891679748822606e-05,
      "loss": 0.738,
      "step": 110400
    },
    {
      "epoch": 1.7346938775510203,
      "grad_norm": 4.646077632904053,
      "learning_rate": 4.891581632653062e-05,
      "loss": 0.7453,
      "step": 110500
    },
    {
      "epoch": 1.7362637362637363,
      "grad_norm": 4.8105292320251465,
      "learning_rate": 4.891483516483516e-05,
      "loss": 0.7269,
      "step": 110600
    },
    {
      "epoch": 1.737833594976452,
      "grad_norm": 4.652907371520996,
      "learning_rate": 4.891385400313972e-05,
      "loss": 0.7513,
      "step": 110700
    },
    {
      "epoch": 1.7394034536891678,
      "grad_norm": 4.340034008026123,
      "learning_rate": 4.891287284144427e-05,
      "loss": 0.7102,
      "step": 110800
    },
    {
      "epoch": 1.7409733124018838,
      "grad_norm": 4.846996307373047,
      "learning_rate": 4.891189167974882e-05,
      "loss": 0.7538,
      "step": 110900
    },
    {
      "epoch": 1.7425431711145998,
      "grad_norm": 3.636648416519165,
      "learning_rate": 4.8910910518053374e-05,
      "loss": 0.716,
      "step": 111000
    },
    {
      "epoch": 1.7441130298273155,
      "grad_norm": 5.0030717849731445,
      "learning_rate": 4.890992935635793e-05,
      "loss": 0.741,
      "step": 111100
    },
    {
      "epoch": 1.7456828885400313,
      "grad_norm": 4.780132293701172,
      "learning_rate": 4.890894819466248e-05,
      "loss": 0.7398,
      "step": 111200
    },
    {
      "epoch": 1.7472527472527473,
      "grad_norm": 4.2477874755859375,
      "learning_rate": 4.8907967032967034e-05,
      "loss": 0.7422,
      "step": 111300
    },
    {
      "epoch": 1.7488226059654632,
      "grad_norm": 4.65086030960083,
      "learning_rate": 4.8906985871271585e-05,
      "loss": 0.7634,
      "step": 111400
    },
    {
      "epoch": 1.750392464678179,
      "grad_norm": 4.7884626388549805,
      "learning_rate": 4.890600470957614e-05,
      "loss": 0.7394,
      "step": 111500
    },
    {
      "epoch": 1.7519623233908947,
      "grad_norm": 2.624330759048462,
      "learning_rate": 4.890502354788069e-05,
      "loss": 0.6893,
      "step": 111600
    },
    {
      "epoch": 1.7535321821036107,
      "grad_norm": 4.781460285186768,
      "learning_rate": 4.8904042386185244e-05,
      "loss": 0.7723,
      "step": 111700
    },
    {
      "epoch": 1.7551020408163265,
      "grad_norm": 4.113710880279541,
      "learning_rate": 4.89030612244898e-05,
      "loss": 0.7543,
      "step": 111800
    },
    {
      "epoch": 1.7566718995290422,
      "grad_norm": 3.584176540374756,
      "learning_rate": 4.890208006279435e-05,
      "loss": 0.7376,
      "step": 111900
    },
    {
      "epoch": 1.7582417582417582,
      "grad_norm": 4.019650459289551,
      "learning_rate": 4.8901098901098904e-05,
      "loss": 0.7318,
      "step": 112000
    },
    {
      "epoch": 1.7598116169544742,
      "grad_norm": 3.794076919555664,
      "learning_rate": 4.8900117739403455e-05,
      "loss": 0.6987,
      "step": 112100
    },
    {
      "epoch": 1.76138147566719,
      "grad_norm": 4.197059631347656,
      "learning_rate": 4.889913657770801e-05,
      "loss": 0.7152,
      "step": 112200
    },
    {
      "epoch": 1.7629513343799057,
      "grad_norm": 4.245429515838623,
      "learning_rate": 4.889815541601256e-05,
      "loss": 0.7239,
      "step": 112300
    },
    {
      "epoch": 1.7645211930926217,
      "grad_norm": 3.682360887527466,
      "learning_rate": 4.8897174254317115e-05,
      "loss": 0.742,
      "step": 112400
    },
    {
      "epoch": 1.7660910518053377,
      "grad_norm": 4.774621486663818,
      "learning_rate": 4.8896193092621666e-05,
      "loss": 0.7491,
      "step": 112500
    },
    {
      "epoch": 1.7676609105180534,
      "grad_norm": 4.543732166290283,
      "learning_rate": 4.8895211930926223e-05,
      "loss": 0.7639,
      "step": 112600
    },
    {
      "epoch": 1.7692307692307692,
      "grad_norm": 4.27077054977417,
      "learning_rate": 4.889423076923077e-05,
      "loss": 0.7522,
      "step": 112700
    },
    {
      "epoch": 1.7708006279434851,
      "grad_norm": 4.277409553527832,
      "learning_rate": 4.8893249607535325e-05,
      "loss": 0.7226,
      "step": 112800
    },
    {
      "epoch": 1.772370486656201,
      "grad_norm": 4.128832817077637,
      "learning_rate": 4.8892268445839876e-05,
      "loss": 0.794,
      "step": 112900
    },
    {
      "epoch": 1.7739403453689166,
      "grad_norm": 4.209840297698975,
      "learning_rate": 4.889128728414443e-05,
      "loss": 0.7367,
      "step": 113000
    },
    {
      "epoch": 1.7755102040816326,
      "grad_norm": 4.390556812286377,
      "learning_rate": 4.889030612244898e-05,
      "loss": 0.7369,
      "step": 113100
    },
    {
      "epoch": 1.7770800627943486,
      "grad_norm": 4.131532192230225,
      "learning_rate": 4.8889324960753536e-05,
      "loss": 0.7394,
      "step": 113200
    },
    {
      "epoch": 1.7786499215070644,
      "grad_norm": 3.9428822994232178,
      "learning_rate": 4.888834379905809e-05,
      "loss": 0.7527,
      "step": 113300
    },
    {
      "epoch": 1.7802197802197801,
      "grad_norm": 4.714062213897705,
      "learning_rate": 4.888736263736264e-05,
      "loss": 0.7689,
      "step": 113400
    },
    {
      "epoch": 1.781789638932496,
      "grad_norm": 4.371356010437012,
      "learning_rate": 4.888638147566719e-05,
      "loss": 0.7156,
      "step": 113500
    },
    {
      "epoch": 1.783359497645212,
      "grad_norm": 3.5214200019836426,
      "learning_rate": 4.888540031397175e-05,
      "loss": 0.7215,
      "step": 113600
    },
    {
      "epoch": 1.7849293563579278,
      "grad_norm": 3.6353812217712402,
      "learning_rate": 4.88844191522763e-05,
      "loss": 0.7103,
      "step": 113700
    },
    {
      "epoch": 1.7864992150706436,
      "grad_norm": 4.385167598724365,
      "learning_rate": 4.888343799058085e-05,
      "loss": 0.7439,
      "step": 113800
    },
    {
      "epoch": 1.7880690737833596,
      "grad_norm": 4.216983318328857,
      "learning_rate": 4.8882456828885407e-05,
      "loss": 0.7483,
      "step": 113900
    },
    {
      "epoch": 1.7896389324960753,
      "grad_norm": 4.3810133934021,
      "learning_rate": 4.888147566718996e-05,
      "loss": 0.7536,
      "step": 114000
    },
    {
      "epoch": 1.791208791208791,
      "grad_norm": 3.304018974304199,
      "learning_rate": 4.888049450549451e-05,
      "loss": 0.7297,
      "step": 114100
    },
    {
      "epoch": 1.792778649921507,
      "grad_norm": 4.63045597076416,
      "learning_rate": 4.887951334379906e-05,
      "loss": 0.7762,
      "step": 114200
    },
    {
      "epoch": 1.794348508634223,
      "grad_norm": 3.951047658920288,
      "learning_rate": 4.887853218210362e-05,
      "loss": 0.7025,
      "step": 114300
    },
    {
      "epoch": 1.7959183673469388,
      "grad_norm": 4.076790809631348,
      "learning_rate": 4.887755102040816e-05,
      "loss": 0.7378,
      "step": 114400
    },
    {
      "epoch": 1.7974882260596545,
      "grad_norm": 4.4807305335998535,
      "learning_rate": 4.887656985871272e-05,
      "loss": 0.7645,
      "step": 114500
    },
    {
      "epoch": 1.7990580847723705,
      "grad_norm": 2.6670212745666504,
      "learning_rate": 4.887558869701727e-05,
      "loss": 0.7453,
      "step": 114600
    },
    {
      "epoch": 1.8006279434850865,
      "grad_norm": 4.564827919006348,
      "learning_rate": 4.887460753532183e-05,
      "loss": 0.7624,
      "step": 114700
    },
    {
      "epoch": 1.8021978021978022,
      "grad_norm": 4.166738986968994,
      "learning_rate": 4.887362637362637e-05,
      "loss": 0.7431,
      "step": 114800
    },
    {
      "epoch": 1.803767660910518,
      "grad_norm": 3.5382139682769775,
      "learning_rate": 4.887264521193093e-05,
      "loss": 0.7167,
      "step": 114900
    },
    {
      "epoch": 1.805337519623234,
      "grad_norm": 4.684324741363525,
      "learning_rate": 4.887166405023548e-05,
      "loss": 0.7722,
      "step": 115000
    },
    {
      "epoch": 1.8069073783359497,
      "grad_norm": 4.270512580871582,
      "learning_rate": 4.887068288854003e-05,
      "loss": 0.7033,
      "step": 115100
    },
    {
      "epoch": 1.8084772370486655,
      "grad_norm": 3.9011852741241455,
      "learning_rate": 4.886970172684458e-05,
      "loss": 0.7201,
      "step": 115200
    },
    {
      "epoch": 1.8100470957613815,
      "grad_norm": 4.900398254394531,
      "learning_rate": 4.886872056514914e-05,
      "loss": 0.7368,
      "step": 115300
    },
    {
      "epoch": 1.8116169544740974,
      "grad_norm": 2.6841273307800293,
      "learning_rate": 4.886773940345369e-05,
      "loss": 0.7424,
      "step": 115400
    },
    {
      "epoch": 1.8131868131868132,
      "grad_norm": 4.101842880249023,
      "learning_rate": 4.886675824175824e-05,
      "loss": 0.7493,
      "step": 115500
    },
    {
      "epoch": 1.814756671899529,
      "grad_norm": 3.7755322456359863,
      "learning_rate": 4.8865777080062794e-05,
      "loss": 0.7501,
      "step": 115600
    },
    {
      "epoch": 1.816326530612245,
      "grad_norm": 4.586812973022461,
      "learning_rate": 4.886479591836735e-05,
      "loss": 0.7179,
      "step": 115700
    },
    {
      "epoch": 1.817896389324961,
      "grad_norm": 4.250763893127441,
      "learning_rate": 4.88638147566719e-05,
      "loss": 0.6956,
      "step": 115800
    },
    {
      "epoch": 1.8194662480376766,
      "grad_norm": 4.047979354858398,
      "learning_rate": 4.886283359497645e-05,
      "loss": 0.7144,
      "step": 115900
    },
    {
      "epoch": 1.8210361067503924,
      "grad_norm": 4.263500690460205,
      "learning_rate": 4.886185243328101e-05,
      "loss": 0.7151,
      "step": 116000
    },
    {
      "epoch": 1.8226059654631084,
      "grad_norm": 2.9747350215911865,
      "learning_rate": 4.886087127158556e-05,
      "loss": 0.6962,
      "step": 116100
    },
    {
      "epoch": 1.8241758241758241,
      "grad_norm": 2.8854575157165527,
      "learning_rate": 4.885989010989011e-05,
      "loss": 0.6726,
      "step": 116200
    },
    {
      "epoch": 1.82574568288854,
      "grad_norm": 3.789193868637085,
      "learning_rate": 4.8858908948194664e-05,
      "loss": 0.7242,
      "step": 116300
    },
    {
      "epoch": 1.8273155416012559,
      "grad_norm": 2.845167636871338,
      "learning_rate": 4.885792778649922e-05,
      "loss": 0.739,
      "step": 116400
    },
    {
      "epoch": 1.8288854003139718,
      "grad_norm": 3.604088068008423,
      "learning_rate": 4.8856946624803766e-05,
      "loss": 0.7043,
      "step": 116500
    },
    {
      "epoch": 1.8304552590266876,
      "grad_norm": 3.1008999347686768,
      "learning_rate": 4.8855965463108324e-05,
      "loss": 0.7159,
      "step": 116600
    },
    {
      "epoch": 1.8320251177394034,
      "grad_norm": 4.119625091552734,
      "learning_rate": 4.8854984301412875e-05,
      "loss": 0.8038,
      "step": 116700
    },
    {
      "epoch": 1.8335949764521193,
      "grad_norm": 3.8171703815460205,
      "learning_rate": 4.885400313971743e-05,
      "loss": 0.7182,
      "step": 116800
    },
    {
      "epoch": 1.8351648351648353,
      "grad_norm": 4.0498857498168945,
      "learning_rate": 4.885302197802198e-05,
      "loss": 0.7112,
      "step": 116900
    },
    {
      "epoch": 1.836734693877551,
      "grad_norm": 3.7505829334259033,
      "learning_rate": 4.8852040816326534e-05,
      "loss": 0.7562,
      "step": 117000
    },
    {
      "epoch": 1.8383045525902668,
      "grad_norm": 3.6187829971313477,
      "learning_rate": 4.8851059654631085e-05,
      "loss": 0.727,
      "step": 117100
    },
    {
      "epoch": 1.8398744113029828,
      "grad_norm": 3.8150103092193604,
      "learning_rate": 4.8850078492935636e-05,
      "loss": 0.7322,
      "step": 117200
    },
    {
      "epoch": 1.8414442700156985,
      "grad_norm": 5.222250938415527,
      "learning_rate": 4.884909733124019e-05,
      "loss": 0.7393,
      "step": 117300
    },
    {
      "epoch": 1.8430141287284143,
      "grad_norm": 3.6328394412994385,
      "learning_rate": 4.8848116169544745e-05,
      "loss": 0.7136,
      "step": 117400
    },
    {
      "epoch": 1.8445839874411303,
      "grad_norm": 4.773694038391113,
      "learning_rate": 4.8847135007849296e-05,
      "loss": 0.7214,
      "step": 117500
    },
    {
      "epoch": 1.8461538461538463,
      "grad_norm": 3.522176742553711,
      "learning_rate": 4.884615384615385e-05,
      "loss": 0.7301,
      "step": 117600
    },
    {
      "epoch": 1.847723704866562,
      "grad_norm": 3.3853397369384766,
      "learning_rate": 4.88451726844584e-05,
      "loss": 0.7361,
      "step": 117700
    },
    {
      "epoch": 1.8492935635792778,
      "grad_norm": 4.3494553565979,
      "learning_rate": 4.8844191522762956e-05,
      "loss": 0.7157,
      "step": 117800
    },
    {
      "epoch": 1.8508634222919937,
      "grad_norm": 3.962679624557495,
      "learning_rate": 4.884321036106751e-05,
      "loss": 0.7524,
      "step": 117900
    },
    {
      "epoch": 1.8524332810047097,
      "grad_norm": 4.7350358963012695,
      "learning_rate": 4.884222919937206e-05,
      "loss": 0.7303,
      "step": 118000
    },
    {
      "epoch": 1.8540031397174255,
      "grad_norm": 4.31149435043335,
      "learning_rate": 4.8841248037676616e-05,
      "loss": 0.7485,
      "step": 118100
    },
    {
      "epoch": 1.8555729984301412,
      "grad_norm": 3.5668201446533203,
      "learning_rate": 4.8840266875981167e-05,
      "loss": 0.7295,
      "step": 118200
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 3.6904518604278564,
      "learning_rate": 4.883928571428572e-05,
      "loss": 0.7167,
      "step": 118300
    },
    {
      "epoch": 1.858712715855573,
      "grad_norm": 3.878983497619629,
      "learning_rate": 4.883830455259027e-05,
      "loss": 0.735,
      "step": 118400
    },
    {
      "epoch": 1.8602825745682887,
      "grad_norm": 4.868516445159912,
      "learning_rate": 4.8837323390894826e-05,
      "loss": 0.7168,
      "step": 118500
    },
    {
      "epoch": 1.8618524332810047,
      "grad_norm": 4.303530216217041,
      "learning_rate": 4.883634222919937e-05,
      "loss": 0.73,
      "step": 118600
    },
    {
      "epoch": 1.8634222919937207,
      "grad_norm": 4.613081455230713,
      "learning_rate": 4.883536106750393e-05,
      "loss": 0.7371,
      "step": 118700
    },
    {
      "epoch": 1.8649921507064364,
      "grad_norm": 3.2400174140930176,
      "learning_rate": 4.883437990580848e-05,
      "loss": 0.7341,
      "step": 118800
    },
    {
      "epoch": 1.8665620094191522,
      "grad_norm": 4.700238227844238,
      "learning_rate": 4.883339874411304e-05,
      "loss": 0.716,
      "step": 118900
    },
    {
      "epoch": 1.8681318681318682,
      "grad_norm": 4.015198707580566,
      "learning_rate": 4.883241758241758e-05,
      "loss": 0.6993,
      "step": 119000
    },
    {
      "epoch": 1.8697017268445841,
      "grad_norm": 3.745785713195801,
      "learning_rate": 4.883143642072214e-05,
      "loss": 0.6852,
      "step": 119100
    },
    {
      "epoch": 1.8712715855572999,
      "grad_norm": 4.893641471862793,
      "learning_rate": 4.883045525902669e-05,
      "loss": 0.7401,
      "step": 119200
    },
    {
      "epoch": 1.8728414442700156,
      "grad_norm": 4.363086700439453,
      "learning_rate": 4.882947409733124e-05,
      "loss": 0.757,
      "step": 119300
    },
    {
      "epoch": 1.8744113029827316,
      "grad_norm": 4.307236194610596,
      "learning_rate": 4.882849293563579e-05,
      "loss": 0.7609,
      "step": 119400
    },
    {
      "epoch": 1.8759811616954474,
      "grad_norm": 4.720778942108154,
      "learning_rate": 4.882751177394035e-05,
      "loss": 0.7203,
      "step": 119500
    },
    {
      "epoch": 1.8775510204081631,
      "grad_norm": 4.490189075469971,
      "learning_rate": 4.88265306122449e-05,
      "loss": 0.7512,
      "step": 119600
    },
    {
      "epoch": 1.879120879120879,
      "grad_norm": 4.523027420043945,
      "learning_rate": 4.882554945054945e-05,
      "loss": 0.7801,
      "step": 119700
    },
    {
      "epoch": 1.880690737833595,
      "grad_norm": 4.544380187988281,
      "learning_rate": 4.8824568288854e-05,
      "loss": 0.7116,
      "step": 119800
    },
    {
      "epoch": 1.8822605965463108,
      "grad_norm": 4.278217792510986,
      "learning_rate": 4.882358712715856e-05,
      "loss": 0.7425,
      "step": 119900
    },
    {
      "epoch": 1.8838304552590266,
      "grad_norm": 3.665233850479126,
      "learning_rate": 4.882260596546311e-05,
      "loss": 0.7079,
      "step": 120000
    },
    {
      "epoch": 1.8854003139717426,
      "grad_norm": 4.2946319580078125,
      "learning_rate": 4.882162480376766e-05,
      "loss": 0.7372,
      "step": 120100
    },
    {
      "epoch": 1.8869701726844585,
      "grad_norm": 3.6681406497955322,
      "learning_rate": 4.882064364207222e-05,
      "loss": 0.691,
      "step": 120200
    },
    {
      "epoch": 1.8885400313971743,
      "grad_norm": 3.4942073822021484,
      "learning_rate": 4.881966248037677e-05,
      "loss": 0.7147,
      "step": 120300
    },
    {
      "epoch": 1.89010989010989,
      "grad_norm": 5.04554557800293,
      "learning_rate": 4.881868131868132e-05,
      "loss": 0.7138,
      "step": 120400
    },
    {
      "epoch": 1.891679748822606,
      "grad_norm": 4.005674839019775,
      "learning_rate": 4.881770015698587e-05,
      "loss": 0.6989,
      "step": 120500
    },
    {
      "epoch": 1.8932496075353218,
      "grad_norm": 4.5356950759887695,
      "learning_rate": 4.881671899529043e-05,
      "loss": 0.7168,
      "step": 120600
    },
    {
      "epoch": 1.8948194662480375,
      "grad_norm": 3.926809310913086,
      "learning_rate": 4.8815737833594975e-05,
      "loss": 0.7264,
      "step": 120700
    },
    {
      "epoch": 1.8963893249607535,
      "grad_norm": 4.0492143630981445,
      "learning_rate": 4.881475667189953e-05,
      "loss": 0.735,
      "step": 120800
    },
    {
      "epoch": 1.8979591836734695,
      "grad_norm": 4.574363708496094,
      "learning_rate": 4.8813775510204084e-05,
      "loss": 0.7171,
      "step": 120900
    },
    {
      "epoch": 1.8995290423861853,
      "grad_norm": 3.770958185195923,
      "learning_rate": 4.881279434850864e-05,
      "loss": 0.8037,
      "step": 121000
    },
    {
      "epoch": 1.901098901098901,
      "grad_norm": 4.472909927368164,
      "learning_rate": 4.8811813186813186e-05,
      "loss": 0.7643,
      "step": 121100
    },
    {
      "epoch": 1.902668759811617,
      "grad_norm": 3.1532602310180664,
      "learning_rate": 4.8810832025117743e-05,
      "loss": 0.8015,
      "step": 121200
    },
    {
      "epoch": 1.904238618524333,
      "grad_norm": 3.5716805458068848,
      "learning_rate": 4.8809850863422294e-05,
      "loss": 0.7308,
      "step": 121300
    },
    {
      "epoch": 1.9058084772370487,
      "grad_norm": 4.514975547790527,
      "learning_rate": 4.8808869701726845e-05,
      "loss": 0.7148,
      "step": 121400
    },
    {
      "epoch": 1.9073783359497645,
      "grad_norm": 4.9843974113464355,
      "learning_rate": 4.8807888540031396e-05,
      "loss": 0.7466,
      "step": 121500
    },
    {
      "epoch": 1.9089481946624804,
      "grad_norm": 3.390139579772949,
      "learning_rate": 4.8806907378335954e-05,
      "loss": 0.7035,
      "step": 121600
    },
    {
      "epoch": 1.9105180533751962,
      "grad_norm": 4.90725040435791,
      "learning_rate": 4.8805926216640505e-05,
      "loss": 0.7426,
      "step": 121700
    },
    {
      "epoch": 1.912087912087912,
      "grad_norm": 4.041497230529785,
      "learning_rate": 4.8804945054945056e-05,
      "loss": 0.6692,
      "step": 121800
    },
    {
      "epoch": 1.913657770800628,
      "grad_norm": 4.016250133514404,
      "learning_rate": 4.880396389324961e-05,
      "loss": 0.7517,
      "step": 121900
    },
    {
      "epoch": 1.915227629513344,
      "grad_norm": 4.859304428100586,
      "learning_rate": 4.8802982731554165e-05,
      "loss": 0.7247,
      "step": 122000
    },
    {
      "epoch": 1.9167974882260597,
      "grad_norm": 3.672137975692749,
      "learning_rate": 4.8802001569858716e-05,
      "loss": 0.7361,
      "step": 122100
    },
    {
      "epoch": 1.9183673469387754,
      "grad_norm": 3.736726760864258,
      "learning_rate": 4.880102040816327e-05,
      "loss": 0.7115,
      "step": 122200
    },
    {
      "epoch": 1.9199372056514914,
      "grad_norm": 4.236260414123535,
      "learning_rate": 4.8800039246467825e-05,
      "loss": 0.7442,
      "step": 122300
    },
    {
      "epoch": 1.9215070643642074,
      "grad_norm": 4.604803085327148,
      "learning_rate": 4.8799058084772376e-05,
      "loss": 0.7415,
      "step": 122400
    },
    {
      "epoch": 1.9230769230769231,
      "grad_norm": 3.6283302307128906,
      "learning_rate": 4.8798076923076926e-05,
      "loss": 0.732,
      "step": 122500
    },
    {
      "epoch": 1.9246467817896389,
      "grad_norm": 5.054747581481934,
      "learning_rate": 4.879709576138148e-05,
      "loss": 0.6672,
      "step": 122600
    },
    {
      "epoch": 1.9262166405023549,
      "grad_norm": 4.092116832733154,
      "learning_rate": 4.8796114599686035e-05,
      "loss": 0.7,
      "step": 122700
    },
    {
      "epoch": 1.9277864992150706,
      "grad_norm": 4.005279541015625,
      "learning_rate": 4.879513343799058e-05,
      "loss": 0.6901,
      "step": 122800
    },
    {
      "epoch": 1.9293563579277864,
      "grad_norm": 3.0696678161621094,
      "learning_rate": 4.879415227629514e-05,
      "loss": 0.7495,
      "step": 122900
    },
    {
      "epoch": 1.9309262166405023,
      "grad_norm": 3.9196860790252686,
      "learning_rate": 4.879317111459969e-05,
      "loss": 0.7304,
      "step": 123000
    },
    {
      "epoch": 1.9324960753532183,
      "grad_norm": 3.4908688068389893,
      "learning_rate": 4.8792189952904246e-05,
      "loss": 0.7084,
      "step": 123100
    },
    {
      "epoch": 1.934065934065934,
      "grad_norm": 4.178983688354492,
      "learning_rate": 4.879120879120879e-05,
      "loss": 0.7281,
      "step": 123200
    },
    {
      "epoch": 1.9356357927786498,
      "grad_norm": 4.1669816970825195,
      "learning_rate": 4.879022762951335e-05,
      "loss": 0.7437,
      "step": 123300
    },
    {
      "epoch": 1.9372056514913658,
      "grad_norm": 4.8267340660095215,
      "learning_rate": 4.87892464678179e-05,
      "loss": 0.7219,
      "step": 123400
    },
    {
      "epoch": 1.9387755102040818,
      "grad_norm": 3.906815528869629,
      "learning_rate": 4.878826530612245e-05,
      "loss": 0.6972,
      "step": 123500
    },
    {
      "epoch": 1.9403453689167975,
      "grad_norm": 4.39310884475708,
      "learning_rate": 4.8787284144427e-05,
      "loss": 0.7433,
      "step": 123600
    },
    {
      "epoch": 1.9419152276295133,
      "grad_norm": 3.1473727226257324,
      "learning_rate": 4.878630298273156e-05,
      "loss": 0.7493,
      "step": 123700
    },
    {
      "epoch": 1.9434850863422293,
      "grad_norm": 4.543034076690674,
      "learning_rate": 4.878532182103611e-05,
      "loss": 0.7286,
      "step": 123800
    },
    {
      "epoch": 1.945054945054945,
      "grad_norm": 4.500302791595459,
      "learning_rate": 4.878434065934066e-05,
      "loss": 0.7157,
      "step": 123900
    },
    {
      "epoch": 1.9466248037676608,
      "grad_norm": 3.9700911045074463,
      "learning_rate": 4.878335949764521e-05,
      "loss": 0.7647,
      "step": 124000
    },
    {
      "epoch": 1.9481946624803768,
      "grad_norm": 3.199429988861084,
      "learning_rate": 4.878237833594977e-05,
      "loss": 0.7584,
      "step": 124100
    },
    {
      "epoch": 1.9497645211930927,
      "grad_norm": 4.394949436187744,
      "learning_rate": 4.878139717425432e-05,
      "loss": 0.7338,
      "step": 124200
    },
    {
      "epoch": 1.9513343799058085,
      "grad_norm": 4.4921650886535645,
      "learning_rate": 4.878041601255887e-05,
      "loss": 0.6778,
      "step": 124300
    },
    {
      "epoch": 1.9529042386185242,
      "grad_norm": 4.248411655426025,
      "learning_rate": 4.877943485086343e-05,
      "loss": 0.7353,
      "step": 124400
    },
    {
      "epoch": 1.9544740973312402,
      "grad_norm": 3.491746664047241,
      "learning_rate": 4.877845368916798e-05,
      "loss": 0.7125,
      "step": 124500
    },
    {
      "epoch": 1.9560439560439562,
      "grad_norm": 3.7850332260131836,
      "learning_rate": 4.877747252747253e-05,
      "loss": 0.7426,
      "step": 124600
    },
    {
      "epoch": 1.957613814756672,
      "grad_norm": 4.411780834197998,
      "learning_rate": 4.877649136577708e-05,
      "loss": 0.7086,
      "step": 124700
    },
    {
      "epoch": 1.9591836734693877,
      "grad_norm": 3.0148510932922363,
      "learning_rate": 4.877551020408164e-05,
      "loss": 0.7148,
      "step": 124800
    },
    {
      "epoch": 1.9607535321821037,
      "grad_norm": 5.0430521965026855,
      "learning_rate": 4.8774529042386184e-05,
      "loss": 0.7391,
      "step": 124900
    },
    {
      "epoch": 1.9623233908948194,
      "grad_norm": 5.423569679260254,
      "learning_rate": 4.877354788069074e-05,
      "loss": 0.7437,
      "step": 125000
    },
    {
      "epoch": 1.9638932496075352,
      "grad_norm": 3.3332715034484863,
      "learning_rate": 4.877256671899529e-05,
      "loss": 0.7177,
      "step": 125100
    },
    {
      "epoch": 1.9654631083202512,
      "grad_norm": 4.183953285217285,
      "learning_rate": 4.8771585557299844e-05,
      "loss": 0.7122,
      "step": 125200
    },
    {
      "epoch": 1.9670329670329672,
      "grad_norm": 3.930504083633423,
      "learning_rate": 4.8770604395604395e-05,
      "loss": 0.6939,
      "step": 125300
    },
    {
      "epoch": 1.968602825745683,
      "grad_norm": 4.936831474304199,
      "learning_rate": 4.876962323390895e-05,
      "loss": 0.7109,
      "step": 125400
    },
    {
      "epoch": 1.9701726844583987,
      "grad_norm": 4.231422424316406,
      "learning_rate": 4.87686420722135e-05,
      "loss": 0.7142,
      "step": 125500
    },
    {
      "epoch": 1.9717425431711146,
      "grad_norm": 3.522296667098999,
      "learning_rate": 4.8767660910518054e-05,
      "loss": 0.7629,
      "step": 125600
    },
    {
      "epoch": 1.9733124018838306,
      "grad_norm": 4.692615032196045,
      "learning_rate": 4.8766679748822605e-05,
      "loss": 0.7289,
      "step": 125700
    },
    {
      "epoch": 1.9748822605965461,
      "grad_norm": 3.2110631465911865,
      "learning_rate": 4.876569858712716e-05,
      "loss": 0.7375,
      "step": 125800
    },
    {
      "epoch": 1.9764521193092621,
      "grad_norm": 3.7684319019317627,
      "learning_rate": 4.8764717425431714e-05,
      "loss": 0.7046,
      "step": 125900
    },
    {
      "epoch": 1.978021978021978,
      "grad_norm": 4.141819953918457,
      "learning_rate": 4.8763736263736265e-05,
      "loss": 0.6816,
      "step": 126000
    },
    {
      "epoch": 1.9795918367346939,
      "grad_norm": 4.182517051696777,
      "learning_rate": 4.8762755102040816e-05,
      "loss": 0.724,
      "step": 126100
    },
    {
      "epoch": 1.9811616954474096,
      "grad_norm": 4.3962836265563965,
      "learning_rate": 4.8761773940345374e-05,
      "loss": 0.6699,
      "step": 126200
    },
    {
      "epoch": 1.9827315541601256,
      "grad_norm": 3.578831672668457,
      "learning_rate": 4.8760792778649925e-05,
      "loss": 0.707,
      "step": 126300
    },
    {
      "epoch": 1.9843014128728416,
      "grad_norm": 4.505619049072266,
      "learning_rate": 4.8759811616954476e-05,
      "loss": 0.7411,
      "step": 126400
    },
    {
      "epoch": 1.9858712715855573,
      "grad_norm": 4.860474109649658,
      "learning_rate": 4.8758830455259034e-05,
      "loss": 0.7079,
      "step": 126500
    },
    {
      "epoch": 1.987441130298273,
      "grad_norm": 4.2994561195373535,
      "learning_rate": 4.875784929356358e-05,
      "loss": 0.7332,
      "step": 126600
    },
    {
      "epoch": 1.989010989010989,
      "grad_norm": 3.9309020042419434,
      "learning_rate": 4.8756868131868135e-05,
      "loss": 0.7139,
      "step": 126700
    },
    {
      "epoch": 1.990580847723705,
      "grad_norm": 3.5454111099243164,
      "learning_rate": 4.8755886970172686e-05,
      "loss": 0.7271,
      "step": 126800
    },
    {
      "epoch": 1.9921507064364206,
      "grad_norm": 4.4471845626831055,
      "learning_rate": 4.8754905808477244e-05,
      "loss": 0.7691,
      "step": 126900
    },
    {
      "epoch": 1.9937205651491365,
      "grad_norm": 4.673951625823975,
      "learning_rate": 4.875392464678179e-05,
      "loss": 0.7001,
      "step": 127000
    },
    {
      "epoch": 1.9952904238618525,
      "grad_norm": 2.7576942443847656,
      "learning_rate": 4.8752943485086346e-05,
      "loss": 0.7251,
      "step": 127100
    },
    {
      "epoch": 1.9968602825745683,
      "grad_norm": 4.8528900146484375,
      "learning_rate": 4.87519623233909e-05,
      "loss": 0.7586,
      "step": 127200
    },
    {
      "epoch": 1.998430141287284,
      "grad_norm": 3.4685580730438232,
      "learning_rate": 4.875098116169545e-05,
      "loss": 0.7112,
      "step": 127300
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.7686567306518555,
      "learning_rate": 4.875e-05,
      "loss": 0.7367,
      "step": 127400
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.0522571802139282,
      "eval_runtime": 14.6746,
      "eval_samples_per_second": 228.49,
      "eval_steps_per_second": 228.49,
      "step": 127400
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.5729185342788696,
      "eval_runtime": 182.392,
      "eval_samples_per_second": 349.248,
      "eval_steps_per_second": 349.248,
      "step": 127400
    },
    {
      "epoch": 2.001569858712716,
      "grad_norm": 3.6045608520507812,
      "learning_rate": 4.874901883830456e-05,
      "loss": 0.7213,
      "step": 127500
    },
    {
      "epoch": 2.0031397174254315,
      "grad_norm": 4.650818347930908,
      "learning_rate": 4.874803767660911e-05,
      "loss": 0.6855,
      "step": 127600
    },
    {
      "epoch": 2.0047095761381475,
      "grad_norm": 3.861605167388916,
      "learning_rate": 4.874705651491366e-05,
      "loss": 0.6846,
      "step": 127700
    },
    {
      "epoch": 2.0062794348508635,
      "grad_norm": 4.415987014770508,
      "learning_rate": 4.874607535321821e-05,
      "loss": 0.6978,
      "step": 127800
    },
    {
      "epoch": 2.0078492935635794,
      "grad_norm": 4.814986228942871,
      "learning_rate": 4.874509419152277e-05,
      "loss": 0.7366,
      "step": 127900
    },
    {
      "epoch": 2.009419152276295,
      "grad_norm": 4.663508415222168,
      "learning_rate": 4.874411302982732e-05,
      "loss": 0.7261,
      "step": 128000
    },
    {
      "epoch": 2.010989010989011,
      "grad_norm": 4.7788262367248535,
      "learning_rate": 4.874313186813187e-05,
      "loss": 0.6935,
      "step": 128100
    },
    {
      "epoch": 2.012558869701727,
      "grad_norm": 4.6081953048706055,
      "learning_rate": 4.874215070643642e-05,
      "loss": 0.7305,
      "step": 128200
    },
    {
      "epoch": 2.014128728414443,
      "grad_norm": 4.019172668457031,
      "learning_rate": 4.874116954474098e-05,
      "loss": 0.6997,
      "step": 128300
    },
    {
      "epoch": 2.0156985871271584,
      "grad_norm": 5.161468029022217,
      "learning_rate": 4.874018838304553e-05,
      "loss": 0.687,
      "step": 128400
    },
    {
      "epoch": 2.0172684458398744,
      "grad_norm": 4.3979573249816895,
      "learning_rate": 4.873920722135008e-05,
      "loss": 0.7219,
      "step": 128500
    },
    {
      "epoch": 2.0188383045525904,
      "grad_norm": 4.3778767585754395,
      "learning_rate": 4.873822605965464e-05,
      "loss": 0.7106,
      "step": 128600
    },
    {
      "epoch": 2.020408163265306,
      "grad_norm": 4.978126525878906,
      "learning_rate": 4.873724489795918e-05,
      "loss": 0.7281,
      "step": 128700
    },
    {
      "epoch": 2.021978021978022,
      "grad_norm": 3.267946243286133,
      "learning_rate": 4.873626373626374e-05,
      "loss": 0.7285,
      "step": 128800
    },
    {
      "epoch": 2.023547880690738,
      "grad_norm": 4.438923358917236,
      "learning_rate": 4.873528257456829e-05,
      "loss": 0.7232,
      "step": 128900
    },
    {
      "epoch": 2.025117739403454,
      "grad_norm": 3.6592540740966797,
      "learning_rate": 4.873430141287285e-05,
      "loss": 0.715,
      "step": 129000
    },
    {
      "epoch": 2.0266875981161694,
      "grad_norm": 5.048126697540283,
      "learning_rate": 4.873332025117739e-05,
      "loss": 0.6898,
      "step": 129100
    },
    {
      "epoch": 2.0282574568288854,
      "grad_norm": 4.545914649963379,
      "learning_rate": 4.873233908948195e-05,
      "loss": 0.6952,
      "step": 129200
    },
    {
      "epoch": 2.0298273155416013,
      "grad_norm": 4.046875,
      "learning_rate": 4.87313579277865e-05,
      "loss": 0.7635,
      "step": 129300
    },
    {
      "epoch": 2.0313971742543173,
      "grad_norm": 4.355501174926758,
      "learning_rate": 4.873037676609105e-05,
      "loss": 0.729,
      "step": 129400
    },
    {
      "epoch": 2.032967032967033,
      "grad_norm": 3.8731212615966797,
      "learning_rate": 4.8729395604395604e-05,
      "loss": 0.7464,
      "step": 129500
    },
    {
      "epoch": 2.034536891679749,
      "grad_norm": 4.222245693206787,
      "learning_rate": 4.872841444270016e-05,
      "loss": 0.7223,
      "step": 129600
    },
    {
      "epoch": 2.036106750392465,
      "grad_norm": 4.757095813751221,
      "learning_rate": 4.872743328100471e-05,
      "loss": 0.7353,
      "step": 129700
    },
    {
      "epoch": 2.0376766091051803,
      "grad_norm": 4.139702796936035,
      "learning_rate": 4.872645211930926e-05,
      "loss": 0.7257,
      "step": 129800
    },
    {
      "epoch": 2.0392464678178963,
      "grad_norm": 3.8962368965148926,
      "learning_rate": 4.8725470957613814e-05,
      "loss": 0.7295,
      "step": 129900
    },
    {
      "epoch": 2.0408163265306123,
      "grad_norm": 4.291989326477051,
      "learning_rate": 4.872448979591837e-05,
      "loss": 0.7331,
      "step": 130000
    },
    {
      "epoch": 2.0423861852433283,
      "grad_norm": 4.220066070556641,
      "learning_rate": 4.872350863422292e-05,
      "loss": 0.68,
      "step": 130100
    },
    {
      "epoch": 2.043956043956044,
      "grad_norm": 4.535017490386963,
      "learning_rate": 4.8722527472527474e-05,
      "loss": 0.7521,
      "step": 130200
    },
    {
      "epoch": 2.0455259026687598,
      "grad_norm": 4.603078365325928,
      "learning_rate": 4.8721546310832025e-05,
      "loss": 0.6978,
      "step": 130300
    },
    {
      "epoch": 2.0470957613814758,
      "grad_norm": 4.214568614959717,
      "learning_rate": 4.872056514913658e-05,
      "loss": 0.7494,
      "step": 130400
    },
    {
      "epoch": 2.0486656200941917,
      "grad_norm": 3.949662923812866,
      "learning_rate": 4.8719583987441134e-05,
      "loss": 0.7389,
      "step": 130500
    },
    {
      "epoch": 2.0502354788069073,
      "grad_norm": 4.214412212371826,
      "learning_rate": 4.8718602825745685e-05,
      "loss": 0.7096,
      "step": 130600
    },
    {
      "epoch": 2.0518053375196232,
      "grad_norm": 3.223839044570923,
      "learning_rate": 4.871762166405024e-05,
      "loss": 0.7469,
      "step": 130700
    },
    {
      "epoch": 2.053375196232339,
      "grad_norm": 3.9398014545440674,
      "learning_rate": 4.871664050235479e-05,
      "loss": 0.7154,
      "step": 130800
    },
    {
      "epoch": 2.0549450549450547,
      "grad_norm": 15.670774459838867,
      "learning_rate": 4.8715659340659344e-05,
      "loss": 0.702,
      "step": 130900
    },
    {
      "epoch": 2.0565149136577707,
      "grad_norm": 4.142209053039551,
      "learning_rate": 4.8714678178963895e-05,
      "loss": 0.7215,
      "step": 131000
    },
    {
      "epoch": 2.0580847723704867,
      "grad_norm": 3.7949910163879395,
      "learning_rate": 4.871369701726845e-05,
      "loss": 0.6896,
      "step": 131100
    },
    {
      "epoch": 2.0596546310832027,
      "grad_norm": 4.44201135635376,
      "learning_rate": 4.8712715855573e-05,
      "loss": 0.7234,
      "step": 131200
    },
    {
      "epoch": 2.061224489795918,
      "grad_norm": 4.066244125366211,
      "learning_rate": 4.8711734693877555e-05,
      "loss": 0.6901,
      "step": 131300
    },
    {
      "epoch": 2.062794348508634,
      "grad_norm": 3.825787305831909,
      "learning_rate": 4.8710753532182106e-05,
      "loss": 0.7106,
      "step": 131400
    },
    {
      "epoch": 2.06436420722135,
      "grad_norm": 4.2800469398498535,
      "learning_rate": 4.870977237048666e-05,
      "loss": 0.7493,
      "step": 131500
    },
    {
      "epoch": 2.065934065934066,
      "grad_norm": 4.092620372772217,
      "learning_rate": 4.870879120879121e-05,
      "loss": 0.7461,
      "step": 131600
    },
    {
      "epoch": 2.0675039246467817,
      "grad_norm": 3.7553534507751465,
      "learning_rate": 4.8707810047095766e-05,
      "loss": 0.6971,
      "step": 131700
    },
    {
      "epoch": 2.0690737833594977,
      "grad_norm": 3.825509548187256,
      "learning_rate": 4.870682888540032e-05,
      "loss": 0.7295,
      "step": 131800
    },
    {
      "epoch": 2.0706436420722136,
      "grad_norm": 3.7315635681152344,
      "learning_rate": 4.870584772370487e-05,
      "loss": 0.7192,
      "step": 131900
    },
    {
      "epoch": 2.072213500784929,
      "grad_norm": 4.447840690612793,
      "learning_rate": 4.870486656200942e-05,
      "loss": 0.7163,
      "step": 132000
    },
    {
      "epoch": 2.073783359497645,
      "grad_norm": 2.92120623588562,
      "learning_rate": 4.8703885400313977e-05,
      "loss": 0.7368,
      "step": 132100
    },
    {
      "epoch": 2.075353218210361,
      "grad_norm": 4.308827877044678,
      "learning_rate": 4.870290423861852e-05,
      "loss": 0.701,
      "step": 132200
    },
    {
      "epoch": 2.076923076923077,
      "grad_norm": 3.3487319946289062,
      "learning_rate": 4.870192307692308e-05,
      "loss": 0.7239,
      "step": 132300
    },
    {
      "epoch": 2.0784929356357926,
      "grad_norm": 3.619743585586548,
      "learning_rate": 4.870094191522763e-05,
      "loss": 0.7472,
      "step": 132400
    },
    {
      "epoch": 2.0800627943485086,
      "grad_norm": 4.298393726348877,
      "learning_rate": 4.869996075353219e-05,
      "loss": 0.7245,
      "step": 132500
    },
    {
      "epoch": 2.0816326530612246,
      "grad_norm": 4.581872463226318,
      "learning_rate": 4.869897959183674e-05,
      "loss": 0.7625,
      "step": 132600
    },
    {
      "epoch": 2.0832025117739406,
      "grad_norm": 4.093611240386963,
      "learning_rate": 4.869799843014129e-05,
      "loss": 0.7525,
      "step": 132700
    },
    {
      "epoch": 2.084772370486656,
      "grad_norm": 3.7484169006347656,
      "learning_rate": 4.869701726844585e-05,
      "loss": 0.717,
      "step": 132800
    },
    {
      "epoch": 2.086342229199372,
      "grad_norm": 3.8339712619781494,
      "learning_rate": 4.869603610675039e-05,
      "loss": 0.6967,
      "step": 132900
    },
    {
      "epoch": 2.087912087912088,
      "grad_norm": 4.867641448974609,
      "learning_rate": 4.869505494505495e-05,
      "loss": 0.7177,
      "step": 133000
    },
    {
      "epoch": 2.0894819466248036,
      "grad_norm": 3.61651349067688,
      "learning_rate": 4.86940737833595e-05,
      "loss": 0.7215,
      "step": 133100
    },
    {
      "epoch": 2.0910518053375196,
      "grad_norm": 3.258619546890259,
      "learning_rate": 4.869309262166406e-05,
      "loss": 0.6596,
      "step": 133200
    },
    {
      "epoch": 2.0926216640502355,
      "grad_norm": 3.7995920181274414,
      "learning_rate": 4.86921114599686e-05,
      "loss": 0.7139,
      "step": 133300
    },
    {
      "epoch": 2.0941915227629515,
      "grad_norm": 4.691001892089844,
      "learning_rate": 4.869113029827316e-05,
      "loss": 0.7155,
      "step": 133400
    },
    {
      "epoch": 2.095761381475667,
      "grad_norm": 4.417195796966553,
      "learning_rate": 4.869014913657771e-05,
      "loss": 0.6956,
      "step": 133500
    },
    {
      "epoch": 2.097331240188383,
      "grad_norm": 3.0445103645324707,
      "learning_rate": 4.868916797488226e-05,
      "loss": 0.7236,
      "step": 133600
    },
    {
      "epoch": 2.098901098901099,
      "grad_norm": 4.868675708770752,
      "learning_rate": 4.868818681318681e-05,
      "loss": 0.7133,
      "step": 133700
    },
    {
      "epoch": 2.100470957613815,
      "grad_norm": 4.529696941375732,
      "learning_rate": 4.868720565149137e-05,
      "loss": 0.6738,
      "step": 133800
    },
    {
      "epoch": 2.1020408163265305,
      "grad_norm": 4.813287258148193,
      "learning_rate": 4.868622448979592e-05,
      "loss": 0.7159,
      "step": 133900
    },
    {
      "epoch": 2.1036106750392465,
      "grad_norm": 4.289118766784668,
      "learning_rate": 4.868524332810047e-05,
      "loss": 0.7421,
      "step": 134000
    },
    {
      "epoch": 2.1051805337519625,
      "grad_norm": 4.976364612579346,
      "learning_rate": 4.868426216640502e-05,
      "loss": 0.7391,
      "step": 134100
    },
    {
      "epoch": 2.106750392464678,
      "grad_norm": 4.413183212280273,
      "learning_rate": 4.868328100470958e-05,
      "loss": 0.7157,
      "step": 134200
    },
    {
      "epoch": 2.108320251177394,
      "grad_norm": 4.350271701812744,
      "learning_rate": 4.8682299843014125e-05,
      "loss": 0.6817,
      "step": 134300
    },
    {
      "epoch": 2.10989010989011,
      "grad_norm": 4.4399285316467285,
      "learning_rate": 4.868131868131868e-05,
      "loss": 0.7488,
      "step": 134400
    },
    {
      "epoch": 2.111459968602826,
      "grad_norm": 4.202695369720459,
      "learning_rate": 4.8680337519623234e-05,
      "loss": 0.767,
      "step": 134500
    },
    {
      "epoch": 2.1130298273155415,
      "grad_norm": 3.7920782566070557,
      "learning_rate": 4.867935635792779e-05,
      "loss": 0.7451,
      "step": 134600
    },
    {
      "epoch": 2.1145996860282574,
      "grad_norm": 3.900094747543335,
      "learning_rate": 4.867837519623234e-05,
      "loss": 0.7445,
      "step": 134700
    },
    {
      "epoch": 2.1161695447409734,
      "grad_norm": 4.55816650390625,
      "learning_rate": 4.8677394034536894e-05,
      "loss": 0.7111,
      "step": 134800
    },
    {
      "epoch": 2.1177394034536894,
      "grad_norm": 3.5670900344848633,
      "learning_rate": 4.867641287284145e-05,
      "loss": 0.6938,
      "step": 134900
    },
    {
      "epoch": 2.119309262166405,
      "grad_norm": 4.12021017074585,
      "learning_rate": 4.8675431711145996e-05,
      "loss": 0.6865,
      "step": 135000
    },
    {
      "epoch": 2.120879120879121,
      "grad_norm": 3.521867036819458,
      "learning_rate": 4.8674450549450553e-05,
      "loss": 0.6862,
      "step": 135100
    },
    {
      "epoch": 2.122448979591837,
      "grad_norm": 3.743014335632324,
      "learning_rate": 4.8673469387755104e-05,
      "loss": 0.7349,
      "step": 135200
    },
    {
      "epoch": 2.1240188383045524,
      "grad_norm": 3.9066734313964844,
      "learning_rate": 4.867248822605966e-05,
      "loss": 0.6782,
      "step": 135300
    },
    {
      "epoch": 2.1255886970172684,
      "grad_norm": 3.953221321105957,
      "learning_rate": 4.8671507064364206e-05,
      "loss": 0.7337,
      "step": 135400
    },
    {
      "epoch": 2.1271585557299844,
      "grad_norm": 3.991132974624634,
      "learning_rate": 4.8670525902668764e-05,
      "loss": 0.7278,
      "step": 135500
    },
    {
      "epoch": 2.1287284144427003,
      "grad_norm": 5.267293453216553,
      "learning_rate": 4.8669544740973315e-05,
      "loss": 0.7233,
      "step": 135600
    },
    {
      "epoch": 2.130298273155416,
      "grad_norm": 4.973804950714111,
      "learning_rate": 4.8668563579277866e-05,
      "loss": 0.689,
      "step": 135700
    },
    {
      "epoch": 2.131868131868132,
      "grad_norm": 3.834956645965576,
      "learning_rate": 4.866758241758242e-05,
      "loss": 0.7402,
      "step": 135800
    },
    {
      "epoch": 2.133437990580848,
      "grad_norm": 4.216073989868164,
      "learning_rate": 4.8666601255886975e-05,
      "loss": 0.6589,
      "step": 135900
    },
    {
      "epoch": 2.1350078492935634,
      "grad_norm": 4.578843593597412,
      "learning_rate": 4.8665620094191526e-05,
      "loss": 0.7165,
      "step": 136000
    },
    {
      "epoch": 2.1365777080062793,
      "grad_norm": 4.796851634979248,
      "learning_rate": 4.866463893249608e-05,
      "loss": 0.691,
      "step": 136100
    },
    {
      "epoch": 2.1381475667189953,
      "grad_norm": 4.054928302764893,
      "learning_rate": 4.866365777080063e-05,
      "loss": 0.7498,
      "step": 136200
    },
    {
      "epoch": 2.1397174254317113,
      "grad_norm": 4.022515773773193,
      "learning_rate": 4.8662676609105186e-05,
      "loss": 0.6794,
      "step": 136300
    },
    {
      "epoch": 2.141287284144427,
      "grad_norm": 2.1515111923217773,
      "learning_rate": 4.866169544740973e-05,
      "loss": 0.7106,
      "step": 136400
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 3.9572412967681885,
      "learning_rate": 4.866071428571429e-05,
      "loss": 0.7031,
      "step": 136500
    },
    {
      "epoch": 2.1444270015698588,
      "grad_norm": 4.1131911277771,
      "learning_rate": 4.865973312401884e-05,
      "loss": 0.7296,
      "step": 136600
    },
    {
      "epoch": 2.1459968602825747,
      "grad_norm": 4.970435619354248,
      "learning_rate": 4.8658751962323396e-05,
      "loss": 0.7542,
      "step": 136700
    },
    {
      "epoch": 2.1475667189952903,
      "grad_norm": 4.441712379455566,
      "learning_rate": 4.865777080062795e-05,
      "loss": 0.751,
      "step": 136800
    },
    {
      "epoch": 2.1491365777080063,
      "grad_norm": 4.4130778312683105,
      "learning_rate": 4.86567896389325e-05,
      "loss": 0.7003,
      "step": 136900
    },
    {
      "epoch": 2.1507064364207222,
      "grad_norm": 4.211536407470703,
      "learning_rate": 4.8655808477237056e-05,
      "loss": 0.7125,
      "step": 137000
    },
    {
      "epoch": 2.152276295133438,
      "grad_norm": 3.5026729106903076,
      "learning_rate": 4.86548273155416e-05,
      "loss": 0.703,
      "step": 137100
    },
    {
      "epoch": 2.1538461538461537,
      "grad_norm": 3.4827120304107666,
      "learning_rate": 4.865384615384616e-05,
      "loss": 0.705,
      "step": 137200
    },
    {
      "epoch": 2.1554160125588697,
      "grad_norm": 3.9123401641845703,
      "learning_rate": 4.865286499215071e-05,
      "loss": 0.6906,
      "step": 137300
    },
    {
      "epoch": 2.1569858712715857,
      "grad_norm": 4.196873664855957,
      "learning_rate": 4.865188383045527e-05,
      "loss": 0.7203,
      "step": 137400
    },
    {
      "epoch": 2.1585557299843012,
      "grad_norm": 4.450971603393555,
      "learning_rate": 4.865090266875981e-05,
      "loss": 0.7001,
      "step": 137500
    },
    {
      "epoch": 2.160125588697017,
      "grad_norm": 4.190698623657227,
      "learning_rate": 4.864992150706437e-05,
      "loss": 0.6794,
      "step": 137600
    },
    {
      "epoch": 2.161695447409733,
      "grad_norm": 3.9671969413757324,
      "learning_rate": 4.864894034536892e-05,
      "loss": 0.7567,
      "step": 137700
    },
    {
      "epoch": 2.163265306122449,
      "grad_norm": 3.486301898956299,
      "learning_rate": 4.864795918367347e-05,
      "loss": 0.7246,
      "step": 137800
    },
    {
      "epoch": 2.1648351648351647,
      "grad_norm": 3.8813652992248535,
      "learning_rate": 4.864697802197802e-05,
      "loss": 0.7,
      "step": 137900
    },
    {
      "epoch": 2.1664050235478807,
      "grad_norm": 4.131479740142822,
      "learning_rate": 4.864599686028258e-05,
      "loss": 0.714,
      "step": 138000
    },
    {
      "epoch": 2.1679748822605966,
      "grad_norm": 3.500854253768921,
      "learning_rate": 4.864501569858713e-05,
      "loss": 0.7161,
      "step": 138100
    },
    {
      "epoch": 2.169544740973312,
      "grad_norm": 4.557347774505615,
      "learning_rate": 4.864403453689168e-05,
      "loss": 0.7524,
      "step": 138200
    },
    {
      "epoch": 2.171114599686028,
      "grad_norm": 4.051050662994385,
      "learning_rate": 4.864305337519623e-05,
      "loss": 0.7006,
      "step": 138300
    },
    {
      "epoch": 2.172684458398744,
      "grad_norm": 3.7614903450012207,
      "learning_rate": 4.864207221350079e-05,
      "loss": 0.7549,
      "step": 138400
    },
    {
      "epoch": 2.17425431711146,
      "grad_norm": 3.2153871059417725,
      "learning_rate": 4.8641091051805334e-05,
      "loss": 0.7026,
      "step": 138500
    },
    {
      "epoch": 2.1758241758241756,
      "grad_norm": 4.322448253631592,
      "learning_rate": 4.864010989010989e-05,
      "loss": 0.7253,
      "step": 138600
    },
    {
      "epoch": 2.1773940345368916,
      "grad_norm": 4.199275970458984,
      "learning_rate": 4.863912872841444e-05,
      "loss": 0.6803,
      "step": 138700
    },
    {
      "epoch": 2.1789638932496076,
      "grad_norm": 4.589212417602539,
      "learning_rate": 4.8638147566719e-05,
      "loss": 0.7236,
      "step": 138800
    },
    {
      "epoch": 2.1805337519623236,
      "grad_norm": 2.9052975177764893,
      "learning_rate": 4.863716640502355e-05,
      "loss": 0.7151,
      "step": 138900
    },
    {
      "epoch": 2.182103610675039,
      "grad_norm": 4.20217752456665,
      "learning_rate": 4.86361852433281e-05,
      "loss": 0.7141,
      "step": 139000
    },
    {
      "epoch": 2.183673469387755,
      "grad_norm": 4.490422248840332,
      "learning_rate": 4.863520408163266e-05,
      "loss": 0.6742,
      "step": 139100
    },
    {
      "epoch": 2.185243328100471,
      "grad_norm": 5.152436256408691,
      "learning_rate": 4.8634222919937205e-05,
      "loss": 0.7318,
      "step": 139200
    },
    {
      "epoch": 2.186813186813187,
      "grad_norm": 4.0577192306518555,
      "learning_rate": 4.863324175824176e-05,
      "loss": 0.6929,
      "step": 139300
    },
    {
      "epoch": 2.1883830455259026,
      "grad_norm": 3.9972217082977295,
      "learning_rate": 4.8632260596546313e-05,
      "loss": 0.7222,
      "step": 139400
    },
    {
      "epoch": 2.1899529042386185,
      "grad_norm": 3.2949092388153076,
      "learning_rate": 4.863127943485087e-05,
      "loss": 0.7084,
      "step": 139500
    },
    {
      "epoch": 2.1915227629513345,
      "grad_norm": 3.1939547061920166,
      "learning_rate": 4.8630298273155415e-05,
      "loss": 0.7112,
      "step": 139600
    },
    {
      "epoch": 2.19309262166405,
      "grad_norm": 3.9206461906433105,
      "learning_rate": 4.862931711145997e-05,
      "loss": 0.6961,
      "step": 139700
    },
    {
      "epoch": 2.194662480376766,
      "grad_norm": 4.684448719024658,
      "learning_rate": 4.8628335949764524e-05,
      "loss": 0.742,
      "step": 139800
    },
    {
      "epoch": 2.196232339089482,
      "grad_norm": 4.375234603881836,
      "learning_rate": 4.8627354788069075e-05,
      "loss": 0.7181,
      "step": 139900
    },
    {
      "epoch": 2.197802197802198,
      "grad_norm": 4.018752098083496,
      "learning_rate": 4.8626373626373626e-05,
      "loss": 0.7021,
      "step": 140000
    },
    {
      "epoch": 2.1993720565149135,
      "grad_norm": 4.525646686553955,
      "learning_rate": 4.8625392464678184e-05,
      "loss": 0.7348,
      "step": 140100
    },
    {
      "epoch": 2.2009419152276295,
      "grad_norm": 4.631534576416016,
      "learning_rate": 4.8624411302982735e-05,
      "loss": 0.688,
      "step": 140200
    },
    {
      "epoch": 2.2025117739403455,
      "grad_norm": 3.6773593425750732,
      "learning_rate": 4.8623430141287286e-05,
      "loss": 0.6831,
      "step": 140300
    },
    {
      "epoch": 2.204081632653061,
      "grad_norm": 4.248289585113525,
      "learning_rate": 4.862244897959184e-05,
      "loss": 0.7278,
      "step": 140400
    },
    {
      "epoch": 2.205651491365777,
      "grad_norm": 4.628339767456055,
      "learning_rate": 4.8621467817896395e-05,
      "loss": 0.676,
      "step": 140500
    },
    {
      "epoch": 2.207221350078493,
      "grad_norm": 4.454334259033203,
      "learning_rate": 4.862048665620094e-05,
      "loss": 0.6884,
      "step": 140600
    },
    {
      "epoch": 2.208791208791209,
      "grad_norm": 5.1812896728515625,
      "learning_rate": 4.8619505494505496e-05,
      "loss": 0.7361,
      "step": 140700
    },
    {
      "epoch": 2.2103610675039245,
      "grad_norm": 3.6113550662994385,
      "learning_rate": 4.861852433281005e-05,
      "loss": 0.6934,
      "step": 140800
    },
    {
      "epoch": 2.2119309262166404,
      "grad_norm": 3.884805202484131,
      "learning_rate": 4.8617543171114605e-05,
      "loss": 0.7219,
      "step": 140900
    },
    {
      "epoch": 2.2135007849293564,
      "grad_norm": 4.212229251861572,
      "learning_rate": 4.8616562009419156e-05,
      "loss": 0.7352,
      "step": 141000
    },
    {
      "epoch": 2.2150706436420724,
      "grad_norm": 4.497398376464844,
      "learning_rate": 4.861558084772371e-05,
      "loss": 0.703,
      "step": 141100
    },
    {
      "epoch": 2.216640502354788,
      "grad_norm": 3.583372116088867,
      "learning_rate": 4.861459968602826e-05,
      "loss": 0.7097,
      "step": 141200
    },
    {
      "epoch": 2.218210361067504,
      "grad_norm": 4.46348237991333,
      "learning_rate": 4.861361852433281e-05,
      "loss": 0.7085,
      "step": 141300
    },
    {
      "epoch": 2.21978021978022,
      "grad_norm": 4.823422431945801,
      "learning_rate": 4.861263736263737e-05,
      "loss": 0.7323,
      "step": 141400
    },
    {
      "epoch": 2.221350078492936,
      "grad_norm": 4.671446323394775,
      "learning_rate": 4.861165620094192e-05,
      "loss": 0.719,
      "step": 141500
    },
    {
      "epoch": 2.2229199372056514,
      "grad_norm": 4.277563571929932,
      "learning_rate": 4.8610675039246476e-05,
      "loss": 0.6891,
      "step": 141600
    },
    {
      "epoch": 2.2244897959183674,
      "grad_norm": 4.8380656242370605,
      "learning_rate": 4.860969387755102e-05,
      "loss": 0.7231,
      "step": 141700
    },
    {
      "epoch": 2.2260596546310834,
      "grad_norm": 4.855588912963867,
      "learning_rate": 4.860871271585558e-05,
      "loss": 0.7656,
      "step": 141800
    },
    {
      "epoch": 2.227629513343799,
      "grad_norm": 4.369980812072754,
      "learning_rate": 4.860773155416013e-05,
      "loss": 0.7374,
      "step": 141900
    },
    {
      "epoch": 2.229199372056515,
      "grad_norm": 4.353010654449463,
      "learning_rate": 4.860675039246468e-05,
      "loss": 0.6886,
      "step": 142000
    },
    {
      "epoch": 2.230769230769231,
      "grad_norm": 3.65934157371521,
      "learning_rate": 4.860576923076923e-05,
      "loss": 0.6903,
      "step": 142100
    },
    {
      "epoch": 2.232339089481947,
      "grad_norm": 4.1961541175842285,
      "learning_rate": 4.860478806907379e-05,
      "loss": 0.6928,
      "step": 142200
    },
    {
      "epoch": 2.2339089481946623,
      "grad_norm": 4.0096564292907715,
      "learning_rate": 4.860380690737834e-05,
      "loss": 0.7667,
      "step": 142300
    },
    {
      "epoch": 2.2354788069073783,
      "grad_norm": 3.0172572135925293,
      "learning_rate": 4.860282574568289e-05,
      "loss": 0.7317,
      "step": 142400
    },
    {
      "epoch": 2.2370486656200943,
      "grad_norm": 5.16865873336792,
      "learning_rate": 4.860184458398744e-05,
      "loss": 0.6735,
      "step": 142500
    },
    {
      "epoch": 2.23861852433281,
      "grad_norm": 4.08328914642334,
      "learning_rate": 4.8600863422292e-05,
      "loss": 0.752,
      "step": 142600
    },
    {
      "epoch": 2.240188383045526,
      "grad_norm": 4.977783203125,
      "learning_rate": 4.859988226059654e-05,
      "loss": 0.7652,
      "step": 142700
    },
    {
      "epoch": 2.241758241758242,
      "grad_norm": 3.321547031402588,
      "learning_rate": 4.85989010989011e-05,
      "loss": 0.7223,
      "step": 142800
    },
    {
      "epoch": 2.2433281004709578,
      "grad_norm": 3.8749144077301025,
      "learning_rate": 4.859791993720565e-05,
      "loss": 0.7167,
      "step": 142900
    },
    {
      "epoch": 2.2448979591836733,
      "grad_norm": 4.446248531341553,
      "learning_rate": 4.859693877551021e-05,
      "loss": 0.7249,
      "step": 143000
    },
    {
      "epoch": 2.2464678178963893,
      "grad_norm": 4.260748863220215,
      "learning_rate": 4.859595761381476e-05,
      "loss": 0.7008,
      "step": 143100
    },
    {
      "epoch": 2.2480376766091053,
      "grad_norm": 3.477736473083496,
      "learning_rate": 4.859497645211931e-05,
      "loss": 0.7278,
      "step": 143200
    },
    {
      "epoch": 2.2496075353218212,
      "grad_norm": 4.533027172088623,
      "learning_rate": 4.859399529042386e-05,
      "loss": 0.6718,
      "step": 143300
    },
    {
      "epoch": 2.2511773940345368,
      "grad_norm": 4.468142032623291,
      "learning_rate": 4.8593014128728414e-05,
      "loss": 0.7057,
      "step": 143400
    },
    {
      "epoch": 2.2527472527472527,
      "grad_norm": 3.782181978225708,
      "learning_rate": 4.859203296703297e-05,
      "loss": 0.7051,
      "step": 143500
    },
    {
      "epoch": 2.2543171114599687,
      "grad_norm": 4.31187629699707,
      "learning_rate": 4.859105180533752e-05,
      "loss": 0.7624,
      "step": 143600
    },
    {
      "epoch": 2.2558869701726847,
      "grad_norm": 4.120409965515137,
      "learning_rate": 4.859007064364208e-05,
      "loss": 0.7282,
      "step": 143700
    },
    {
      "epoch": 2.2574568288854002,
      "grad_norm": 3.1537411212921143,
      "learning_rate": 4.8589089481946624e-05,
      "loss": 0.6999,
      "step": 143800
    },
    {
      "epoch": 2.259026687598116,
      "grad_norm": 3.4244515895843506,
      "learning_rate": 4.858810832025118e-05,
      "loss": 0.7371,
      "step": 143900
    },
    {
      "epoch": 2.260596546310832,
      "grad_norm": 4.223016262054443,
      "learning_rate": 4.858712715855573e-05,
      "loss": 0.7168,
      "step": 144000
    },
    {
      "epoch": 2.2621664050235477,
      "grad_norm": 4.984370231628418,
      "learning_rate": 4.8586145996860284e-05,
      "loss": 0.7469,
      "step": 144100
    },
    {
      "epoch": 2.2637362637362637,
      "grad_norm": 2.1397576332092285,
      "learning_rate": 4.8585164835164835e-05,
      "loss": 0.7176,
      "step": 144200
    },
    {
      "epoch": 2.2653061224489797,
      "grad_norm": 5.04909086227417,
      "learning_rate": 4.858418367346939e-05,
      "loss": 0.7391,
      "step": 144300
    },
    {
      "epoch": 2.2668759811616956,
      "grad_norm": 4.219911575317383,
      "learning_rate": 4.8583202511773944e-05,
      "loss": 0.7257,
      "step": 144400
    },
    {
      "epoch": 2.268445839874411,
      "grad_norm": 4.288543224334717,
      "learning_rate": 4.8582221350078495e-05,
      "loss": 0.7315,
      "step": 144500
    },
    {
      "epoch": 2.270015698587127,
      "grad_norm": 4.086916446685791,
      "learning_rate": 4.8581240188383046e-05,
      "loss": 0.6997,
      "step": 144600
    },
    {
      "epoch": 2.271585557299843,
      "grad_norm": 3.7630300521850586,
      "learning_rate": 4.8580259026687604e-05,
      "loss": 0.7121,
      "step": 144700
    },
    {
      "epoch": 2.2731554160125587,
      "grad_norm": 3.92858624458313,
      "learning_rate": 4.857927786499215e-05,
      "loss": 0.7316,
      "step": 144800
    },
    {
      "epoch": 2.2747252747252746,
      "grad_norm": 3.551123857498169,
      "learning_rate": 4.8578296703296705e-05,
      "loss": 0.7623,
      "step": 144900
    },
    {
      "epoch": 2.2762951334379906,
      "grad_norm": 4.295485019683838,
      "learning_rate": 4.8577315541601256e-05,
      "loss": 0.6674,
      "step": 145000
    },
    {
      "epoch": 2.2778649921507066,
      "grad_norm": 4.694091796875,
      "learning_rate": 4.8576334379905814e-05,
      "loss": 0.708,
      "step": 145100
    },
    {
      "epoch": 2.279434850863422,
      "grad_norm": 3.442652940750122,
      "learning_rate": 4.8575353218210365e-05,
      "loss": 0.6988,
      "step": 145200
    },
    {
      "epoch": 2.281004709576138,
      "grad_norm": 4.192695140838623,
      "learning_rate": 4.8574372056514916e-05,
      "loss": 0.7428,
      "step": 145300
    },
    {
      "epoch": 2.282574568288854,
      "grad_norm": 5.879350662231445,
      "learning_rate": 4.857339089481947e-05,
      "loss": 0.721,
      "step": 145400
    },
    {
      "epoch": 2.2841444270015696,
      "grad_norm": 4.753203868865967,
      "learning_rate": 4.857240973312402e-05,
      "loss": 0.6841,
      "step": 145500
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 4.186070442199707,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 0.7242,
      "step": 145600
    },
    {
      "epoch": 2.2872841444270016,
      "grad_norm": 4.367593288421631,
      "learning_rate": 4.857044740973313e-05,
      "loss": 0.7541,
      "step": 145700
    },
    {
      "epoch": 2.2888540031397175,
      "grad_norm": 4.300141334533691,
      "learning_rate": 4.8569466248037685e-05,
      "loss": 0.6895,
      "step": 145800
    },
    {
      "epoch": 2.2904238618524335,
      "grad_norm": 4.363309860229492,
      "learning_rate": 4.856848508634223e-05,
      "loss": 0.6906,
      "step": 145900
    },
    {
      "epoch": 2.291993720565149,
      "grad_norm": 2.9977991580963135,
      "learning_rate": 4.8567503924646787e-05,
      "loss": 0.6908,
      "step": 146000
    },
    {
      "epoch": 2.293563579277865,
      "grad_norm": 3.9850752353668213,
      "learning_rate": 4.856652276295134e-05,
      "loss": 0.7401,
      "step": 146100
    },
    {
      "epoch": 2.295133437990581,
      "grad_norm": 4.084120273590088,
      "learning_rate": 4.856554160125589e-05,
      "loss": 0.7026,
      "step": 146200
    },
    {
      "epoch": 2.2967032967032965,
      "grad_norm": 2.1083006858825684,
      "learning_rate": 4.856456043956044e-05,
      "loss": 0.7054,
      "step": 146300
    },
    {
      "epoch": 2.2982731554160125,
      "grad_norm": 4.956758499145508,
      "learning_rate": 4.8563579277865e-05,
      "loss": 0.7011,
      "step": 146400
    },
    {
      "epoch": 2.2998430141287285,
      "grad_norm": 5.347259044647217,
      "learning_rate": 4.856259811616955e-05,
      "loss": 0.6993,
      "step": 146500
    },
    {
      "epoch": 2.3014128728414445,
      "grad_norm": 5.020639896392822,
      "learning_rate": 4.85616169544741e-05,
      "loss": 0.7301,
      "step": 146600
    },
    {
      "epoch": 2.30298273155416,
      "grad_norm": 3.799797773361206,
      "learning_rate": 4.856063579277865e-05,
      "loss": 0.7203,
      "step": 146700
    },
    {
      "epoch": 2.304552590266876,
      "grad_norm": 4.236428737640381,
      "learning_rate": 4.855965463108321e-05,
      "loss": 0.7425,
      "step": 146800
    },
    {
      "epoch": 2.306122448979592,
      "grad_norm": 3.4473519325256348,
      "learning_rate": 4.855867346938775e-05,
      "loss": 0.7418,
      "step": 146900
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 4.785095691680908,
      "learning_rate": 4.855769230769231e-05,
      "loss": 0.7273,
      "step": 147000
    },
    {
      "epoch": 2.3092621664050235,
      "grad_norm": 3.0891659259796143,
      "learning_rate": 4.855671114599686e-05,
      "loss": 0.7174,
      "step": 147100
    },
    {
      "epoch": 2.3108320251177394,
      "grad_norm": 5.065426349639893,
      "learning_rate": 4.855572998430142e-05,
      "loss": 0.743,
      "step": 147200
    },
    {
      "epoch": 2.3124018838304554,
      "grad_norm": 4.0544962882995605,
      "learning_rate": 4.855474882260597e-05,
      "loss": 0.7461,
      "step": 147300
    },
    {
      "epoch": 2.313971742543171,
      "grad_norm": 2.83767032623291,
      "learning_rate": 4.855376766091052e-05,
      "loss": 0.6878,
      "step": 147400
    },
    {
      "epoch": 2.315541601255887,
      "grad_norm": 4.820251941680908,
      "learning_rate": 4.855278649921507e-05,
      "loss": 0.7042,
      "step": 147500
    },
    {
      "epoch": 2.317111459968603,
      "grad_norm": 4.12872838973999,
      "learning_rate": 4.855180533751962e-05,
      "loss": 0.749,
      "step": 147600
    },
    {
      "epoch": 2.3186813186813184,
      "grad_norm": 3.0605194568634033,
      "learning_rate": 4.855082417582418e-05,
      "loss": 0.7022,
      "step": 147700
    },
    {
      "epoch": 2.3202511773940344,
      "grad_norm": 5.260761737823486,
      "learning_rate": 4.854984301412873e-05,
      "loss": 0.7391,
      "step": 147800
    },
    {
      "epoch": 2.3218210361067504,
      "grad_norm": 4.744481563568115,
      "learning_rate": 4.854886185243328e-05,
      "loss": 0.7002,
      "step": 147900
    },
    {
      "epoch": 2.3233908948194664,
      "grad_norm": 4.544833660125732,
      "learning_rate": 4.854788069073783e-05,
      "loss": 0.7139,
      "step": 148000
    },
    {
      "epoch": 2.3249607535321823,
      "grad_norm": 3.9290878772735596,
      "learning_rate": 4.854689952904239e-05,
      "loss": 0.7473,
      "step": 148100
    },
    {
      "epoch": 2.326530612244898,
      "grad_norm": 4.107222557067871,
      "learning_rate": 4.854591836734694e-05,
      "loss": 0.7205,
      "step": 148200
    },
    {
      "epoch": 2.328100470957614,
      "grad_norm": 4.13818883895874,
      "learning_rate": 4.854493720565149e-05,
      "loss": 0.7025,
      "step": 148300
    },
    {
      "epoch": 2.32967032967033,
      "grad_norm": 4.008937835693359,
      "learning_rate": 4.8543956043956044e-05,
      "loss": 0.7537,
      "step": 148400
    },
    {
      "epoch": 2.3312401883830454,
      "grad_norm": 4.244183540344238,
      "learning_rate": 4.85429748822606e-05,
      "loss": 0.7057,
      "step": 148500
    },
    {
      "epoch": 2.3328100470957613,
      "grad_norm": 3.3951401710510254,
      "learning_rate": 4.8541993720565146e-05,
      "loss": 0.7162,
      "step": 148600
    },
    {
      "epoch": 2.3343799058084773,
      "grad_norm": 3.872753143310547,
      "learning_rate": 4.8541012558869704e-05,
      "loss": 0.7102,
      "step": 148700
    },
    {
      "epoch": 2.3359497645211933,
      "grad_norm": 4.765328407287598,
      "learning_rate": 4.8540031397174255e-05,
      "loss": 0.6898,
      "step": 148800
    },
    {
      "epoch": 2.337519623233909,
      "grad_norm": 3.1752419471740723,
      "learning_rate": 4.853905023547881e-05,
      "loss": 0.6834,
      "step": 148900
    },
    {
      "epoch": 2.339089481946625,
      "grad_norm": 4.904603004455566,
      "learning_rate": 4.853806907378336e-05,
      "loss": 0.7221,
      "step": 149000
    },
    {
      "epoch": 2.340659340659341,
      "grad_norm": 3.539738178253174,
      "learning_rate": 4.8537087912087914e-05,
      "loss": 0.6777,
      "step": 149100
    },
    {
      "epoch": 2.3422291993720563,
      "grad_norm": 4.1053972244262695,
      "learning_rate": 4.8536106750392465e-05,
      "loss": 0.7344,
      "step": 149200
    },
    {
      "epoch": 2.3437990580847723,
      "grad_norm": 4.157251358032227,
      "learning_rate": 4.8535125588697016e-05,
      "loss": 0.7316,
      "step": 149300
    },
    {
      "epoch": 2.3453689167974883,
      "grad_norm": 4.561380863189697,
      "learning_rate": 4.8534144427001574e-05,
      "loss": 0.7235,
      "step": 149400
    },
    {
      "epoch": 2.3469387755102042,
      "grad_norm": 4.2983245849609375,
      "learning_rate": 4.8533163265306125e-05,
      "loss": 0.7191,
      "step": 149500
    },
    {
      "epoch": 2.3485086342229198,
      "grad_norm": 4.28715705871582,
      "learning_rate": 4.8532182103610676e-05,
      "loss": 0.7047,
      "step": 149600
    },
    {
      "epoch": 2.3500784929356358,
      "grad_norm": 5.0124711990356445,
      "learning_rate": 4.853120094191523e-05,
      "loss": 0.7202,
      "step": 149700
    },
    {
      "epoch": 2.3516483516483517,
      "grad_norm": 4.7173051834106445,
      "learning_rate": 4.8530219780219785e-05,
      "loss": 0.7085,
      "step": 149800
    },
    {
      "epoch": 2.3532182103610673,
      "grad_norm": 4.078597545623779,
      "learning_rate": 4.8529238618524336e-05,
      "loss": 0.703,
      "step": 149900
    },
    {
      "epoch": 2.3547880690737832,
      "grad_norm": 4.026644706726074,
      "learning_rate": 4.852825745682889e-05,
      "loss": 0.7227,
      "step": 150000
    },
    {
      "epoch": 2.356357927786499,
      "grad_norm": 3.6866352558135986,
      "learning_rate": 4.852727629513344e-05,
      "loss": 0.7041,
      "step": 150100
    },
    {
      "epoch": 2.357927786499215,
      "grad_norm": 3.733198881149292,
      "learning_rate": 4.8526295133437996e-05,
      "loss": 0.7711,
      "step": 150200
    },
    {
      "epoch": 2.359497645211931,
      "grad_norm": 4.4701056480407715,
      "learning_rate": 4.8525313971742547e-05,
      "loss": 0.6982,
      "step": 150300
    },
    {
      "epoch": 2.3610675039246467,
      "grad_norm": 4.748164176940918,
      "learning_rate": 4.85243328100471e-05,
      "loss": 0.7182,
      "step": 150400
    },
    {
      "epoch": 2.3626373626373627,
      "grad_norm": 4.447776794433594,
      "learning_rate": 4.852335164835165e-05,
      "loss": 0.7111,
      "step": 150500
    },
    {
      "epoch": 2.3642072213500787,
      "grad_norm": 4.045238494873047,
      "learning_rate": 4.8522370486656206e-05,
      "loss": 0.6771,
      "step": 150600
    },
    {
      "epoch": 2.365777080062794,
      "grad_norm": 3.6362829208374023,
      "learning_rate": 4.852138932496075e-05,
      "loss": 0.7029,
      "step": 150700
    },
    {
      "epoch": 2.36734693877551,
      "grad_norm": 4.844270706176758,
      "learning_rate": 4.852040816326531e-05,
      "loss": 0.7297,
      "step": 150800
    },
    {
      "epoch": 2.368916797488226,
      "grad_norm": 4.1913323402404785,
      "learning_rate": 4.851942700156986e-05,
      "loss": 0.7123,
      "step": 150900
    },
    {
      "epoch": 2.370486656200942,
      "grad_norm": 4.91420316696167,
      "learning_rate": 4.851844583987442e-05,
      "loss": 0.7045,
      "step": 151000
    },
    {
      "epoch": 2.3720565149136577,
      "grad_norm": 4.311274528503418,
      "learning_rate": 4.851746467817896e-05,
      "loss": 0.7185,
      "step": 151100
    },
    {
      "epoch": 2.3736263736263736,
      "grad_norm": 4.243536949157715,
      "learning_rate": 4.851648351648352e-05,
      "loss": 0.7076,
      "step": 151200
    },
    {
      "epoch": 2.3751962323390896,
      "grad_norm": 4.31722354888916,
      "learning_rate": 4.851550235478807e-05,
      "loss": 0.693,
      "step": 151300
    },
    {
      "epoch": 2.376766091051805,
      "grad_norm": 4.242668151855469,
      "learning_rate": 4.851452119309262e-05,
      "loss": 0.7535,
      "step": 151400
    },
    {
      "epoch": 2.378335949764521,
      "grad_norm": 3.691941738128662,
      "learning_rate": 4.851354003139718e-05,
      "loss": 0.739,
      "step": 151500
    },
    {
      "epoch": 2.379905808477237,
      "grad_norm": 3.816209316253662,
      "learning_rate": 4.851255886970173e-05,
      "loss": 0.6972,
      "step": 151600
    },
    {
      "epoch": 2.381475667189953,
      "grad_norm": 4.20884370803833,
      "learning_rate": 4.851157770800628e-05,
      "loss": 0.6967,
      "step": 151700
    },
    {
      "epoch": 2.3830455259026686,
      "grad_norm": 3.2652359008789062,
      "learning_rate": 4.851059654631083e-05,
      "loss": 0.7008,
      "step": 151800
    },
    {
      "epoch": 2.3846153846153846,
      "grad_norm": 4.257595539093018,
      "learning_rate": 4.850961538461539e-05,
      "loss": 0.7521,
      "step": 151900
    },
    {
      "epoch": 2.3861852433281006,
      "grad_norm": 3.098308801651001,
      "learning_rate": 4.850863422291994e-05,
      "loss": 0.686,
      "step": 152000
    },
    {
      "epoch": 2.387755102040816,
      "grad_norm": 4.930968284606934,
      "learning_rate": 4.850765306122449e-05,
      "loss": 0.7462,
      "step": 152100
    },
    {
      "epoch": 2.389324960753532,
      "grad_norm": 4.990589618682861,
      "learning_rate": 4.850667189952904e-05,
      "loss": 0.7705,
      "step": 152200
    },
    {
      "epoch": 2.390894819466248,
      "grad_norm": 5.802712917327881,
      "learning_rate": 4.85056907378336e-05,
      "loss": 0.712,
      "step": 152300
    },
    {
      "epoch": 2.392464678178964,
      "grad_norm": 4.657672882080078,
      "learning_rate": 4.850470957613815e-05,
      "loss": 0.715,
      "step": 152400
    },
    {
      "epoch": 2.39403453689168,
      "grad_norm": 4.6144890785217285,
      "learning_rate": 4.85037284144427e-05,
      "loss": 0.6871,
      "step": 152500
    },
    {
      "epoch": 2.3956043956043955,
      "grad_norm": 4.6686601638793945,
      "learning_rate": 4.850274725274725e-05,
      "loss": 0.713,
      "step": 152600
    },
    {
      "epoch": 2.3971742543171115,
      "grad_norm": 4.955989837646484,
      "learning_rate": 4.850176609105181e-05,
      "loss": 0.7639,
      "step": 152700
    },
    {
      "epoch": 2.3987441130298275,
      "grad_norm": 4.166932582855225,
      "learning_rate": 4.8500784929356355e-05,
      "loss": 0.7222,
      "step": 152800
    },
    {
      "epoch": 2.400313971742543,
      "grad_norm": 4.875722408294678,
      "learning_rate": 4.849980376766091e-05,
      "loss": 0.7075,
      "step": 152900
    },
    {
      "epoch": 2.401883830455259,
      "grad_norm": 4.13711404800415,
      "learning_rate": 4.8498822605965464e-05,
      "loss": 0.7388,
      "step": 153000
    },
    {
      "epoch": 2.403453689167975,
      "grad_norm": 4.340244770050049,
      "learning_rate": 4.849784144427002e-05,
      "loss": 0.6844,
      "step": 153100
    },
    {
      "epoch": 2.405023547880691,
      "grad_norm": 3.7313098907470703,
      "learning_rate": 4.8496860282574566e-05,
      "loss": 0.7288,
      "step": 153200
    },
    {
      "epoch": 2.4065934065934065,
      "grad_norm": 5.410903453826904,
      "learning_rate": 4.8495879120879123e-05,
      "loss": 0.7107,
      "step": 153300
    },
    {
      "epoch": 2.4081632653061225,
      "grad_norm": 4.403022766113281,
      "learning_rate": 4.8494897959183674e-05,
      "loss": 0.7307,
      "step": 153400
    },
    {
      "epoch": 2.4097331240188384,
      "grad_norm": 3.576876163482666,
      "learning_rate": 4.8493916797488225e-05,
      "loss": 0.7118,
      "step": 153500
    },
    {
      "epoch": 2.411302982731554,
      "grad_norm": 3.5816197395324707,
      "learning_rate": 4.849293563579278e-05,
      "loss": 0.7233,
      "step": 153600
    },
    {
      "epoch": 2.41287284144427,
      "grad_norm": 4.410873889923096,
      "learning_rate": 4.8491954474097334e-05,
      "loss": 0.6729,
      "step": 153700
    },
    {
      "epoch": 2.414442700156986,
      "grad_norm": 5.064492225646973,
      "learning_rate": 4.8490973312401885e-05,
      "loss": 0.6729,
      "step": 153800
    },
    {
      "epoch": 2.416012558869702,
      "grad_norm": 3.9214999675750732,
      "learning_rate": 4.8489992150706436e-05,
      "loss": 0.7128,
      "step": 153900
    },
    {
      "epoch": 2.4175824175824174,
      "grad_norm": 3.8891210556030273,
      "learning_rate": 4.8489010989010994e-05,
      "loss": 0.7327,
      "step": 154000
    },
    {
      "epoch": 2.4191522762951334,
      "grad_norm": 3.5188143253326416,
      "learning_rate": 4.8488029827315545e-05,
      "loss": 0.7147,
      "step": 154100
    },
    {
      "epoch": 2.4207221350078494,
      "grad_norm": 4.532094478607178,
      "learning_rate": 4.8487048665620096e-05,
      "loss": 0.6821,
      "step": 154200
    },
    {
      "epoch": 2.422291993720565,
      "grad_norm": 4.720531463623047,
      "learning_rate": 4.848606750392465e-05,
      "loss": 0.7119,
      "step": 154300
    },
    {
      "epoch": 2.423861852433281,
      "grad_norm": 4.066173076629639,
      "learning_rate": 4.8485086342229205e-05,
      "loss": 0.6628,
      "step": 154400
    },
    {
      "epoch": 2.425431711145997,
      "grad_norm": 4.759188652038574,
      "learning_rate": 4.8484105180533756e-05,
      "loss": 0.7259,
      "step": 154500
    },
    {
      "epoch": 2.427001569858713,
      "grad_norm": 5.303762435913086,
      "learning_rate": 4.8483124018838307e-05,
      "loss": 0.6985,
      "step": 154600
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 4.756664752960205,
      "learning_rate": 4.848214285714286e-05,
      "loss": 0.6965,
      "step": 154700
    },
    {
      "epoch": 2.4301412872841444,
      "grad_norm": 4.792757034301758,
      "learning_rate": 4.8481161695447415e-05,
      "loss": 0.7144,
      "step": 154800
    },
    {
      "epoch": 2.4317111459968603,
      "grad_norm": 3.102870225906372,
      "learning_rate": 4.848018053375196e-05,
      "loss": 0.6936,
      "step": 154900
    },
    {
      "epoch": 2.4332810047095763,
      "grad_norm": 3.7866811752319336,
      "learning_rate": 4.847919937205652e-05,
      "loss": 0.7113,
      "step": 155000
    },
    {
      "epoch": 2.434850863422292,
      "grad_norm": 3.765874147415161,
      "learning_rate": 4.847821821036107e-05,
      "loss": 0.721,
      "step": 155100
    },
    {
      "epoch": 2.436420722135008,
      "grad_norm": 4.2761406898498535,
      "learning_rate": 4.8477237048665626e-05,
      "loss": 0.7146,
      "step": 155200
    },
    {
      "epoch": 2.437990580847724,
      "grad_norm": 3.3485159873962402,
      "learning_rate": 4.847625588697017e-05,
      "loss": 0.7239,
      "step": 155300
    },
    {
      "epoch": 2.4395604395604398,
      "grad_norm": 4.463683128356934,
      "learning_rate": 4.847527472527473e-05,
      "loss": 0.7663,
      "step": 155400
    },
    {
      "epoch": 2.4411302982731553,
      "grad_norm": 5.384696960449219,
      "learning_rate": 4.847429356357928e-05,
      "loss": 0.7429,
      "step": 155500
    },
    {
      "epoch": 2.4427001569858713,
      "grad_norm": 3.9568960666656494,
      "learning_rate": 4.847331240188383e-05,
      "loss": 0.7588,
      "step": 155600
    },
    {
      "epoch": 2.4442700156985873,
      "grad_norm": 4.247960090637207,
      "learning_rate": 4.847233124018839e-05,
      "loss": 0.7307,
      "step": 155700
    },
    {
      "epoch": 2.445839874411303,
      "grad_norm": 4.341468811035156,
      "learning_rate": 4.847135007849294e-05,
      "loss": 0.7203,
      "step": 155800
    },
    {
      "epoch": 2.4474097331240188,
      "grad_norm": 3.852517604827881,
      "learning_rate": 4.847036891679749e-05,
      "loss": 0.6918,
      "step": 155900
    },
    {
      "epoch": 2.4489795918367347,
      "grad_norm": 3.7410898208618164,
      "learning_rate": 4.846938775510204e-05,
      "loss": 0.698,
      "step": 156000
    },
    {
      "epoch": 2.4505494505494507,
      "grad_norm": 4.187453746795654,
      "learning_rate": 4.84684065934066e-05,
      "loss": 0.7066,
      "step": 156100
    },
    {
      "epoch": 2.4521193092621663,
      "grad_norm": 2.39243483543396,
      "learning_rate": 4.846742543171115e-05,
      "loss": 0.6603,
      "step": 156200
    },
    {
      "epoch": 2.4536891679748822,
      "grad_norm": 3.9976003170013428,
      "learning_rate": 4.84664442700157e-05,
      "loss": 0.7453,
      "step": 156300
    },
    {
      "epoch": 2.455259026687598,
      "grad_norm": 3.696798324584961,
      "learning_rate": 4.846546310832025e-05,
      "loss": 0.6707,
      "step": 156400
    },
    {
      "epoch": 2.4568288854003137,
      "grad_norm": 3.5575473308563232,
      "learning_rate": 4.846448194662481e-05,
      "loss": 0.7387,
      "step": 156500
    },
    {
      "epoch": 2.4583987441130297,
      "grad_norm": 4.2697577476501465,
      "learning_rate": 4.846350078492936e-05,
      "loss": 0.7093,
      "step": 156600
    },
    {
      "epoch": 2.4599686028257457,
      "grad_norm": 4.055300235748291,
      "learning_rate": 4.846251962323391e-05,
      "loss": 0.7436,
      "step": 156700
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 4.312878131866455,
      "learning_rate": 4.846153846153846e-05,
      "loss": 0.706,
      "step": 156800
    },
    {
      "epoch": 2.463108320251177,
      "grad_norm": 4.279400825500488,
      "learning_rate": 4.846055729984302e-05,
      "loss": 0.7254,
      "step": 156900
    },
    {
      "epoch": 2.464678178963893,
      "grad_norm": 4.541538715362549,
      "learning_rate": 4.8459576138147564e-05,
      "loss": 0.7025,
      "step": 157000
    },
    {
      "epoch": 2.466248037676609,
      "grad_norm": 3.526071786880493,
      "learning_rate": 4.845859497645212e-05,
      "loss": 0.7638,
      "step": 157100
    },
    {
      "epoch": 2.467817896389325,
      "grad_norm": 2.7183098793029785,
      "learning_rate": 4.845761381475667e-05,
      "loss": 0.724,
      "step": 157200
    },
    {
      "epoch": 2.4693877551020407,
      "grad_norm": 4.095522880554199,
      "learning_rate": 4.845663265306123e-05,
      "loss": 0.7149,
      "step": 157300
    },
    {
      "epoch": 2.4709576138147566,
      "grad_norm": 3.4849131107330322,
      "learning_rate": 4.8455651491365775e-05,
      "loss": 0.7239,
      "step": 157400
    },
    {
      "epoch": 2.4725274725274726,
      "grad_norm": 3.460965394973755,
      "learning_rate": 4.845467032967033e-05,
      "loss": 0.7371,
      "step": 157500
    },
    {
      "epoch": 2.4740973312401886,
      "grad_norm": 4.458331108093262,
      "learning_rate": 4.845368916797488e-05,
      "loss": 0.7073,
      "step": 157600
    },
    {
      "epoch": 2.475667189952904,
      "grad_norm": 4.241382598876953,
      "learning_rate": 4.8452708006279434e-05,
      "loss": 0.6621,
      "step": 157700
    },
    {
      "epoch": 2.47723704866562,
      "grad_norm": 3.6820285320281982,
      "learning_rate": 4.845172684458399e-05,
      "loss": 0.6876,
      "step": 157800
    },
    {
      "epoch": 2.478806907378336,
      "grad_norm": 4.71167516708374,
      "learning_rate": 4.845074568288854e-05,
      "loss": 0.6939,
      "step": 157900
    },
    {
      "epoch": 2.4803767660910516,
      "grad_norm": 4.691235065460205,
      "learning_rate": 4.8449764521193094e-05,
      "loss": 0.6992,
      "step": 158000
    },
    {
      "epoch": 2.4819466248037676,
      "grad_norm": 4.650097846984863,
      "learning_rate": 4.8448783359497645e-05,
      "loss": 0.7511,
      "step": 158100
    },
    {
      "epoch": 2.4835164835164836,
      "grad_norm": 3.65376877784729,
      "learning_rate": 4.84478021978022e-05,
      "loss": 0.7504,
      "step": 158200
    },
    {
      "epoch": 2.4850863422291996,
      "grad_norm": 4.497293949127197,
      "learning_rate": 4.8446821036106754e-05,
      "loss": 0.6588,
      "step": 158300
    },
    {
      "epoch": 2.486656200941915,
      "grad_norm": 3.989990472793579,
      "learning_rate": 4.8445839874411305e-05,
      "loss": 0.6543,
      "step": 158400
    },
    {
      "epoch": 2.488226059654631,
      "grad_norm": 4.8265204429626465,
      "learning_rate": 4.8444858712715856e-05,
      "loss": 0.6853,
      "step": 158500
    },
    {
      "epoch": 2.489795918367347,
      "grad_norm": 4.268981456756592,
      "learning_rate": 4.8443877551020414e-05,
      "loss": 0.7106,
      "step": 158600
    },
    {
      "epoch": 2.4913657770800626,
      "grad_norm": 3.117323398590088,
      "learning_rate": 4.8442896389324965e-05,
      "loss": 0.7251,
      "step": 158700
    },
    {
      "epoch": 2.4929356357927785,
      "grad_norm": 4.213224411010742,
      "learning_rate": 4.8441915227629515e-05,
      "loss": 0.6974,
      "step": 158800
    },
    {
      "epoch": 2.4945054945054945,
      "grad_norm": 4.1890716552734375,
      "learning_rate": 4.8440934065934066e-05,
      "loss": 0.7207,
      "step": 158900
    },
    {
      "epoch": 2.4960753532182105,
      "grad_norm": 3.612565755844116,
      "learning_rate": 4.8439952904238624e-05,
      "loss": 0.7042,
      "step": 159000
    },
    {
      "epoch": 2.497645211930926,
      "grad_norm": 4.220539093017578,
      "learning_rate": 4.843897174254317e-05,
      "loss": 0.6617,
      "step": 159100
    },
    {
      "epoch": 2.499215070643642,
      "grad_norm": 3.507587194442749,
      "learning_rate": 4.8437990580847726e-05,
      "loss": 0.693,
      "step": 159200
    },
    {
      "epoch": 2.500784929356358,
      "grad_norm": 4.301030158996582,
      "learning_rate": 4.843700941915228e-05,
      "loss": 0.7374,
      "step": 159300
    },
    {
      "epoch": 2.5023547880690735,
      "grad_norm": 5.054028034210205,
      "learning_rate": 4.8436028257456835e-05,
      "loss": 0.6829,
      "step": 159400
    },
    {
      "epoch": 2.5039246467817895,
      "grad_norm": 3.1119139194488525,
      "learning_rate": 4.843504709576138e-05,
      "loss": 0.6922,
      "step": 159500
    },
    {
      "epoch": 2.5054945054945055,
      "grad_norm": 3.805905818939209,
      "learning_rate": 4.843406593406594e-05,
      "loss": 0.7302,
      "step": 159600
    },
    {
      "epoch": 2.5070643642072215,
      "grad_norm": 4.692047595977783,
      "learning_rate": 4.843308477237049e-05,
      "loss": 0.7253,
      "step": 159700
    },
    {
      "epoch": 2.5086342229199374,
      "grad_norm": 3.7894527912139893,
      "learning_rate": 4.843210361067504e-05,
      "loss": 0.7047,
      "step": 159800
    },
    {
      "epoch": 2.510204081632653,
      "grad_norm": 3.8887863159179688,
      "learning_rate": 4.84311224489796e-05,
      "loss": 0.7239,
      "step": 159900
    },
    {
      "epoch": 2.511773940345369,
      "grad_norm": 4.256416320800781,
      "learning_rate": 4.843014128728415e-05,
      "loss": 0.6627,
      "step": 160000
    },
    {
      "epoch": 2.513343799058085,
      "grad_norm": 3.382493734359741,
      "learning_rate": 4.84291601255887e-05,
      "loss": 0.7512,
      "step": 160100
    },
    {
      "epoch": 2.5149136577708004,
      "grad_norm": 3.946957588195801,
      "learning_rate": 4.842817896389325e-05,
      "loss": 0.7345,
      "step": 160200
    },
    {
      "epoch": 2.5164835164835164,
      "grad_norm": 4.498651504516602,
      "learning_rate": 4.842719780219781e-05,
      "loss": 0.7155,
      "step": 160300
    },
    {
      "epoch": 2.5180533751962324,
      "grad_norm": 3.464020013809204,
      "learning_rate": 4.842621664050236e-05,
      "loss": 0.7382,
      "step": 160400
    },
    {
      "epoch": 2.5196232339089484,
      "grad_norm": 3.0060648918151855,
      "learning_rate": 4.842523547880691e-05,
      "loss": 0.6924,
      "step": 160500
    },
    {
      "epoch": 2.521193092621664,
      "grad_norm": 4.467972278594971,
      "learning_rate": 4.842425431711146e-05,
      "loss": 0.7593,
      "step": 160600
    },
    {
      "epoch": 2.52276295133438,
      "grad_norm": 4.2225141525268555,
      "learning_rate": 4.842327315541602e-05,
      "loss": 0.7192,
      "step": 160700
    },
    {
      "epoch": 2.524332810047096,
      "grad_norm": 3.790287494659424,
      "learning_rate": 4.842229199372057e-05,
      "loss": 0.6946,
      "step": 160800
    },
    {
      "epoch": 2.5259026687598114,
      "grad_norm": 3.120614767074585,
      "learning_rate": 4.842131083202512e-05,
      "loss": 0.7118,
      "step": 160900
    },
    {
      "epoch": 2.5274725274725274,
      "grad_norm": 3.024409532546997,
      "learning_rate": 4.842032967032967e-05,
      "loss": 0.7232,
      "step": 161000
    },
    {
      "epoch": 2.5290423861852434,
      "grad_norm": 4.648998260498047,
      "learning_rate": 4.841934850863423e-05,
      "loss": 0.7332,
      "step": 161100
    },
    {
      "epoch": 2.5306122448979593,
      "grad_norm": 4.17644739151001,
      "learning_rate": 4.841836734693877e-05,
      "loss": 0.7506,
      "step": 161200
    },
    {
      "epoch": 2.5321821036106753,
      "grad_norm": 5.2884392738342285,
      "learning_rate": 4.841738618524333e-05,
      "loss": 0.7265,
      "step": 161300
    },
    {
      "epoch": 2.533751962323391,
      "grad_norm": 2.6238529682159424,
      "learning_rate": 4.841640502354788e-05,
      "loss": 0.6899,
      "step": 161400
    },
    {
      "epoch": 2.535321821036107,
      "grad_norm": 3.7436320781707764,
      "learning_rate": 4.841542386185244e-05,
      "loss": 0.6851,
      "step": 161500
    },
    {
      "epoch": 2.5368916797488223,
      "grad_norm": 4.608156204223633,
      "learning_rate": 4.8414442700156984e-05,
      "loss": 0.6761,
      "step": 161600
    },
    {
      "epoch": 2.5384615384615383,
      "grad_norm": 4.116640567779541,
      "learning_rate": 4.841346153846154e-05,
      "loss": 0.7327,
      "step": 161700
    },
    {
      "epoch": 2.5400313971742543,
      "grad_norm": 3.823760986328125,
      "learning_rate": 4.841248037676609e-05,
      "loss": 0.6911,
      "step": 161800
    },
    {
      "epoch": 2.5416012558869703,
      "grad_norm": 3.892338514328003,
      "learning_rate": 4.841149921507064e-05,
      "loss": 0.7109,
      "step": 161900
    },
    {
      "epoch": 2.5431711145996863,
      "grad_norm": 4.471756458282471,
      "learning_rate": 4.84105180533752e-05,
      "loss": 0.7311,
      "step": 162000
    },
    {
      "epoch": 2.544740973312402,
      "grad_norm": 2.9694786071777344,
      "learning_rate": 4.840953689167975e-05,
      "loss": 0.7012,
      "step": 162100
    },
    {
      "epoch": 2.5463108320251178,
      "grad_norm": 3.916623830795288,
      "learning_rate": 4.84085557299843e-05,
      "loss": 0.6937,
      "step": 162200
    },
    {
      "epoch": 2.5478806907378337,
      "grad_norm": 4.388896465301514,
      "learning_rate": 4.8407574568288854e-05,
      "loss": 0.7046,
      "step": 162300
    },
    {
      "epoch": 2.5494505494505493,
      "grad_norm": 4.974954605102539,
      "learning_rate": 4.840659340659341e-05,
      "loss": 0.7004,
      "step": 162400
    },
    {
      "epoch": 2.5510204081632653,
      "grad_norm": 4.463168144226074,
      "learning_rate": 4.840561224489796e-05,
      "loss": 0.7071,
      "step": 162500
    },
    {
      "epoch": 2.5525902668759812,
      "grad_norm": 3.5321645736694336,
      "learning_rate": 4.8404631083202514e-05,
      "loss": 0.6837,
      "step": 162600
    },
    {
      "epoch": 2.554160125588697,
      "grad_norm": 3.542165994644165,
      "learning_rate": 4.8403649921507065e-05,
      "loss": 0.682,
      "step": 162700
    },
    {
      "epoch": 2.5557299843014127,
      "grad_norm": 3.9739584922790527,
      "learning_rate": 4.840266875981162e-05,
      "loss": 0.7425,
      "step": 162800
    },
    {
      "epoch": 2.5572998430141287,
      "grad_norm": 1.9868435859680176,
      "learning_rate": 4.8401687598116173e-05,
      "loss": 0.6978,
      "step": 162900
    },
    {
      "epoch": 2.5588697017268447,
      "grad_norm": 4.303096294403076,
      "learning_rate": 4.8400706436420724e-05,
      "loss": 0.6814,
      "step": 163000
    },
    {
      "epoch": 2.5604395604395602,
      "grad_norm": 3.4438302516937256,
      "learning_rate": 4.8399725274725275e-05,
      "loss": 0.7172,
      "step": 163100
    },
    {
      "epoch": 2.562009419152276,
      "grad_norm": 2.6658456325531006,
      "learning_rate": 4.839874411302983e-05,
      "loss": 0.6838,
      "step": 163200
    },
    {
      "epoch": 2.563579277864992,
      "grad_norm": 4.658634662628174,
      "learning_rate": 4.839776295133438e-05,
      "loss": 0.6833,
      "step": 163300
    },
    {
      "epoch": 2.565149136577708,
      "grad_norm": 3.8148515224456787,
      "learning_rate": 4.8396781789638935e-05,
      "loss": 0.7376,
      "step": 163400
    },
    {
      "epoch": 2.566718995290424,
      "grad_norm": 4.52987003326416,
      "learning_rate": 4.8395800627943486e-05,
      "loss": 0.7323,
      "step": 163500
    },
    {
      "epoch": 2.5682888540031397,
      "grad_norm": 4.117944240570068,
      "learning_rate": 4.8394819466248044e-05,
      "loss": 0.7278,
      "step": 163600
    },
    {
      "epoch": 2.5698587127158556,
      "grad_norm": 3.2521655559539795,
      "learning_rate": 4.839383830455259e-05,
      "loss": 0.7274,
      "step": 163700
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 4.016576290130615,
      "learning_rate": 4.8392857142857146e-05,
      "loss": 0.6936,
      "step": 163800
    },
    {
      "epoch": 2.572998430141287,
      "grad_norm": 3.4647979736328125,
      "learning_rate": 4.83918759811617e-05,
      "loss": 0.6942,
      "step": 163900
    },
    {
      "epoch": 2.574568288854003,
      "grad_norm": 3.263556480407715,
      "learning_rate": 4.839089481946625e-05,
      "loss": 0.7013,
      "step": 164000
    },
    {
      "epoch": 2.576138147566719,
      "grad_norm": 4.526302337646484,
      "learning_rate": 4.8389913657770806e-05,
      "loss": 0.7003,
      "step": 164100
    },
    {
      "epoch": 2.577708006279435,
      "grad_norm": 4.048817157745361,
      "learning_rate": 4.8388932496075357e-05,
      "loss": 0.7169,
      "step": 164200
    },
    {
      "epoch": 2.5792778649921506,
      "grad_norm": 4.342134475708008,
      "learning_rate": 4.838795133437991e-05,
      "loss": 0.7461,
      "step": 164300
    },
    {
      "epoch": 2.5808477237048666,
      "grad_norm": 3.7842657566070557,
      "learning_rate": 4.838697017268446e-05,
      "loss": 0.6866,
      "step": 164400
    },
    {
      "epoch": 2.5824175824175826,
      "grad_norm": 3.8757312297821045,
      "learning_rate": 4.8385989010989016e-05,
      "loss": 0.7059,
      "step": 164500
    },
    {
      "epoch": 2.583987441130298,
      "grad_norm": 3.9625320434570312,
      "learning_rate": 4.838500784929357e-05,
      "loss": 0.7396,
      "step": 164600
    },
    {
      "epoch": 2.585557299843014,
      "grad_norm": 3.735710620880127,
      "learning_rate": 4.838402668759812e-05,
      "loss": 0.7624,
      "step": 164700
    },
    {
      "epoch": 2.58712715855573,
      "grad_norm": 3.907008409500122,
      "learning_rate": 4.838304552590267e-05,
      "loss": 0.7224,
      "step": 164800
    },
    {
      "epoch": 2.588697017268446,
      "grad_norm": 4.094396591186523,
      "learning_rate": 4.838206436420723e-05,
      "loss": 0.7226,
      "step": 164900
    },
    {
      "epoch": 2.5902668759811616,
      "grad_norm": 4.306395530700684,
      "learning_rate": 4.838108320251178e-05,
      "loss": 0.6958,
      "step": 165000
    },
    {
      "epoch": 2.5918367346938775,
      "grad_norm": 6.034765243530273,
      "learning_rate": 4.838010204081633e-05,
      "loss": 0.703,
      "step": 165100
    },
    {
      "epoch": 2.5934065934065935,
      "grad_norm": 3.441068410873413,
      "learning_rate": 4.837912087912088e-05,
      "loss": 0.6905,
      "step": 165200
    },
    {
      "epoch": 2.594976452119309,
      "grad_norm": 4.162937641143799,
      "learning_rate": 4.837813971742544e-05,
      "loss": 0.7655,
      "step": 165300
    },
    {
      "epoch": 2.596546310832025,
      "grad_norm": 4.402104377746582,
      "learning_rate": 4.837715855572998e-05,
      "loss": 0.7341,
      "step": 165400
    },
    {
      "epoch": 2.598116169544741,
      "grad_norm": 4.24057674407959,
      "learning_rate": 4.837617739403454e-05,
      "loss": 0.7011,
      "step": 165500
    },
    {
      "epoch": 2.599686028257457,
      "grad_norm": 3.6264724731445312,
      "learning_rate": 4.837519623233909e-05,
      "loss": 0.7195,
      "step": 165600
    },
    {
      "epoch": 2.601255886970173,
      "grad_norm": 4.339018821716309,
      "learning_rate": 4.837421507064365e-05,
      "loss": 0.7119,
      "step": 165700
    },
    {
      "epoch": 2.6028257456828885,
      "grad_norm": 3.5570805072784424,
      "learning_rate": 4.837323390894819e-05,
      "loss": 0.7457,
      "step": 165800
    },
    {
      "epoch": 2.6043956043956045,
      "grad_norm": 4.489461421966553,
      "learning_rate": 4.837225274725275e-05,
      "loss": 0.7232,
      "step": 165900
    },
    {
      "epoch": 2.60596546310832,
      "grad_norm": 3.992295026779175,
      "learning_rate": 4.83712715855573e-05,
      "loss": 0.7144,
      "step": 166000
    },
    {
      "epoch": 2.607535321821036,
      "grad_norm": 4.461860179901123,
      "learning_rate": 4.837029042386185e-05,
      "loss": 0.7115,
      "step": 166100
    },
    {
      "epoch": 2.609105180533752,
      "grad_norm": 2.98923921585083,
      "learning_rate": 4.836930926216641e-05,
      "loss": 0.7331,
      "step": 166200
    },
    {
      "epoch": 2.610675039246468,
      "grad_norm": 4.2869954109191895,
      "learning_rate": 4.836832810047096e-05,
      "loss": 0.7204,
      "step": 166300
    },
    {
      "epoch": 2.612244897959184,
      "grad_norm": 4.482283115386963,
      "learning_rate": 4.836734693877551e-05,
      "loss": 0.6852,
      "step": 166400
    },
    {
      "epoch": 2.6138147566718994,
      "grad_norm": 2.6750590801239014,
      "learning_rate": 4.836636577708006e-05,
      "loss": 0.6993,
      "step": 166500
    },
    {
      "epoch": 2.6153846153846154,
      "grad_norm": 3.727658748626709,
      "learning_rate": 4.836538461538462e-05,
      "loss": 0.7138,
      "step": 166600
    },
    {
      "epoch": 2.6169544740973314,
      "grad_norm": 4.7499003410339355,
      "learning_rate": 4.836440345368917e-05,
      "loss": 0.7356,
      "step": 166700
    },
    {
      "epoch": 2.618524332810047,
      "grad_norm": 3.896817684173584,
      "learning_rate": 4.836342229199372e-05,
      "loss": 0.6863,
      "step": 166800
    },
    {
      "epoch": 2.620094191522763,
      "grad_norm": 4.613949775695801,
      "learning_rate": 4.8362441130298274e-05,
      "loss": 0.6847,
      "step": 166900
    },
    {
      "epoch": 2.621664050235479,
      "grad_norm": 4.354069709777832,
      "learning_rate": 4.836145996860283e-05,
      "loss": 0.6884,
      "step": 167000
    },
    {
      "epoch": 2.623233908948195,
      "grad_norm": 3.8956172466278076,
      "learning_rate": 4.836047880690738e-05,
      "loss": 0.7309,
      "step": 167100
    },
    {
      "epoch": 2.6248037676609104,
      "grad_norm": 3.9516005516052246,
      "learning_rate": 4.8359497645211933e-05,
      "loss": 0.6657,
      "step": 167200
    },
    {
      "epoch": 2.6263736263736264,
      "grad_norm": 4.377869129180908,
      "learning_rate": 4.8358516483516484e-05,
      "loss": 0.7096,
      "step": 167300
    },
    {
      "epoch": 2.6279434850863423,
      "grad_norm": 4.136998176574707,
      "learning_rate": 4.835753532182104e-05,
      "loss": 0.7181,
      "step": 167400
    },
    {
      "epoch": 2.629513343799058,
      "grad_norm": 2.622858762741089,
      "learning_rate": 4.8356554160125586e-05,
      "loss": 0.6842,
      "step": 167500
    },
    {
      "epoch": 2.631083202511774,
      "grad_norm": 3.990797758102417,
      "learning_rate": 4.8355572998430144e-05,
      "loss": 0.6536,
      "step": 167600
    },
    {
      "epoch": 2.63265306122449,
      "grad_norm": 4.126570701599121,
      "learning_rate": 4.8354591836734695e-05,
      "loss": 0.7368,
      "step": 167700
    },
    {
      "epoch": 2.634222919937206,
      "grad_norm": 4.98611307144165,
      "learning_rate": 4.835361067503925e-05,
      "loss": 0.7222,
      "step": 167800
    },
    {
      "epoch": 2.6357927786499213,
      "grad_norm": 4.264703750610352,
      "learning_rate": 4.83526295133438e-05,
      "loss": 0.7067,
      "step": 167900
    },
    {
      "epoch": 2.6373626373626373,
      "grad_norm": 2.8257055282592773,
      "learning_rate": 4.8351648351648355e-05,
      "loss": 0.689,
      "step": 168000
    },
    {
      "epoch": 2.6389324960753533,
      "grad_norm": 4.2822394371032715,
      "learning_rate": 4.8350667189952906e-05,
      "loss": 0.7121,
      "step": 168100
    },
    {
      "epoch": 2.640502354788069,
      "grad_norm": 4.003798961639404,
      "learning_rate": 4.834968602825746e-05,
      "loss": 0.6863,
      "step": 168200
    },
    {
      "epoch": 2.642072213500785,
      "grad_norm": 3.3798577785491943,
      "learning_rate": 4.8348704866562015e-05,
      "loss": 0.7411,
      "step": 168300
    },
    {
      "epoch": 2.643642072213501,
      "grad_norm": 3.421505928039551,
      "learning_rate": 4.8347723704866566e-05,
      "loss": 0.7096,
      "step": 168400
    },
    {
      "epoch": 2.6452119309262168,
      "grad_norm": 4.535411834716797,
      "learning_rate": 4.8346742543171117e-05,
      "loss": 0.7219,
      "step": 168500
    },
    {
      "epoch": 2.6467817896389327,
      "grad_norm": 4.847121715545654,
      "learning_rate": 4.834576138147567e-05,
      "loss": 0.6816,
      "step": 168600
    },
    {
      "epoch": 2.6483516483516483,
      "grad_norm": 4.582287788391113,
      "learning_rate": 4.8344780219780225e-05,
      "loss": 0.6858,
      "step": 168700
    },
    {
      "epoch": 2.6499215070643642,
      "grad_norm": 4.5626301765441895,
      "learning_rate": 4.8343799058084776e-05,
      "loss": 0.7125,
      "step": 168800
    },
    {
      "epoch": 2.6514913657770802,
      "grad_norm": 3.8731625080108643,
      "learning_rate": 4.834281789638933e-05,
      "loss": 0.7337,
      "step": 168900
    },
    {
      "epoch": 2.6530612244897958,
      "grad_norm": 4.009683609008789,
      "learning_rate": 4.834183673469388e-05,
      "loss": 0.7479,
      "step": 169000
    },
    {
      "epoch": 2.6546310832025117,
      "grad_norm": 4.492085933685303,
      "learning_rate": 4.8340855572998436e-05,
      "loss": 0.6984,
      "step": 169100
    },
    {
      "epoch": 2.6562009419152277,
      "grad_norm": 3.447985887527466,
      "learning_rate": 4.833987441130299e-05,
      "loss": 0.7127,
      "step": 169200
    },
    {
      "epoch": 2.6577708006279437,
      "grad_norm": 4.267895221710205,
      "learning_rate": 4.833889324960754e-05,
      "loss": 0.7436,
      "step": 169300
    },
    {
      "epoch": 2.659340659340659,
      "grad_norm": 4.425050735473633,
      "learning_rate": 4.833791208791209e-05,
      "loss": 0.678,
      "step": 169400
    },
    {
      "epoch": 2.660910518053375,
      "grad_norm": 5.3042683601379395,
      "learning_rate": 4.833693092621665e-05,
      "loss": 0.7166,
      "step": 169500
    },
    {
      "epoch": 2.662480376766091,
      "grad_norm": 3.9922454357147217,
      "learning_rate": 4.833594976452119e-05,
      "loss": 0.6681,
      "step": 169600
    },
    {
      "epoch": 2.6640502354788067,
      "grad_norm": 3.6308610439300537,
      "learning_rate": 4.833496860282575e-05,
      "loss": 0.697,
      "step": 169700
    },
    {
      "epoch": 2.6656200941915227,
      "grad_norm": 3.849580764770508,
      "learning_rate": 4.83339874411303e-05,
      "loss": 0.6717,
      "step": 169800
    },
    {
      "epoch": 2.6671899529042387,
      "grad_norm": 3.7870965003967285,
      "learning_rate": 4.833300627943486e-05,
      "loss": 0.7285,
      "step": 169900
    },
    {
      "epoch": 2.6687598116169546,
      "grad_norm": 4.2259087562561035,
      "learning_rate": 4.83320251177394e-05,
      "loss": 0.7224,
      "step": 170000
    },
    {
      "epoch": 2.67032967032967,
      "grad_norm": 4.4084014892578125,
      "learning_rate": 4.833104395604396e-05,
      "loss": 0.7112,
      "step": 170100
    },
    {
      "epoch": 2.671899529042386,
      "grad_norm": 5.452540397644043,
      "learning_rate": 4.833006279434851e-05,
      "loss": 0.6588,
      "step": 170200
    },
    {
      "epoch": 2.673469387755102,
      "grad_norm": 4.854830741882324,
      "learning_rate": 4.832908163265306e-05,
      "loss": 0.7425,
      "step": 170300
    },
    {
      "epoch": 2.6750392464678177,
      "grad_norm": 3.6366240978240967,
      "learning_rate": 4.832810047095762e-05,
      "loss": 0.7178,
      "step": 170400
    },
    {
      "epoch": 2.6766091051805336,
      "grad_norm": 3.731379747390747,
      "learning_rate": 4.832711930926217e-05,
      "loss": 0.7294,
      "step": 170500
    },
    {
      "epoch": 2.6781789638932496,
      "grad_norm": 4.55043363571167,
      "learning_rate": 4.832613814756672e-05,
      "loss": 0.702,
      "step": 170600
    },
    {
      "epoch": 2.6797488226059656,
      "grad_norm": 4.21547269821167,
      "learning_rate": 4.832515698587127e-05,
      "loss": 0.7141,
      "step": 170700
    },
    {
      "epoch": 2.6813186813186816,
      "grad_norm": 4.498125076293945,
      "learning_rate": 4.832417582417583e-05,
      "loss": 0.6772,
      "step": 170800
    },
    {
      "epoch": 2.682888540031397,
      "grad_norm": 4.278616905212402,
      "learning_rate": 4.832319466248038e-05,
      "loss": 0.7164,
      "step": 170900
    },
    {
      "epoch": 2.684458398744113,
      "grad_norm": 3.8725810050964355,
      "learning_rate": 4.832221350078493e-05,
      "loss": 0.6641,
      "step": 171000
    },
    {
      "epoch": 2.686028257456829,
      "grad_norm": 3.191657066345215,
      "learning_rate": 4.832123233908948e-05,
      "loss": 0.6934,
      "step": 171100
    },
    {
      "epoch": 2.6875981161695446,
      "grad_norm": 4.403203964233398,
      "learning_rate": 4.832025117739404e-05,
      "loss": 0.7262,
      "step": 171200
    },
    {
      "epoch": 2.6891679748822606,
      "grad_norm": 3.9193899631500244,
      "learning_rate": 4.8319270015698585e-05,
      "loss": 0.7369,
      "step": 171300
    },
    {
      "epoch": 2.6907378335949765,
      "grad_norm": 4.375589370727539,
      "learning_rate": 4.831828885400314e-05,
      "loss": 0.7568,
      "step": 171400
    },
    {
      "epoch": 2.6923076923076925,
      "grad_norm": 3.7934141159057617,
      "learning_rate": 4.8317307692307693e-05,
      "loss": 0.7175,
      "step": 171500
    },
    {
      "epoch": 2.693877551020408,
      "grad_norm": 4.347580432891846,
      "learning_rate": 4.831632653061225e-05,
      "loss": 0.7378,
      "step": 171600
    },
    {
      "epoch": 2.695447409733124,
      "grad_norm": 3.7798168659210205,
      "learning_rate": 4.8315345368916795e-05,
      "loss": 0.7154,
      "step": 171700
    },
    {
      "epoch": 2.69701726844584,
      "grad_norm": 3.9933135509490967,
      "learning_rate": 4.831436420722135e-05,
      "loss": 0.7277,
      "step": 171800
    },
    {
      "epoch": 2.6985871271585555,
      "grad_norm": 4.2583489418029785,
      "learning_rate": 4.8313383045525904e-05,
      "loss": 0.733,
      "step": 171900
    },
    {
      "epoch": 2.7001569858712715,
      "grad_norm": 3.6262450218200684,
      "learning_rate": 4.8312401883830455e-05,
      "loss": 0.7142,
      "step": 172000
    },
    {
      "epoch": 2.7017268445839875,
      "grad_norm": 4.529608726501465,
      "learning_rate": 4.8311420722135006e-05,
      "loss": 0.7263,
      "step": 172100
    },
    {
      "epoch": 2.7032967032967035,
      "grad_norm": 3.416471004486084,
      "learning_rate": 4.8310439560439564e-05,
      "loss": 0.6838,
      "step": 172200
    },
    {
      "epoch": 2.704866562009419,
      "grad_norm": 4.276424884796143,
      "learning_rate": 4.8309458398744115e-05,
      "loss": 0.709,
      "step": 172300
    },
    {
      "epoch": 2.706436420722135,
      "grad_norm": 3.8270483016967773,
      "learning_rate": 4.8308477237048666e-05,
      "loss": 0.6973,
      "step": 172400
    },
    {
      "epoch": 2.708006279434851,
      "grad_norm": 3.073700189590454,
      "learning_rate": 4.8307496075353224e-05,
      "loss": 0.6675,
      "step": 172500
    },
    {
      "epoch": 2.7095761381475665,
      "grad_norm": 2.229605197906494,
      "learning_rate": 4.8306514913657775e-05,
      "loss": 0.6903,
      "step": 172600
    },
    {
      "epoch": 2.7111459968602825,
      "grad_norm": 4.328122138977051,
      "learning_rate": 4.8305533751962326e-05,
      "loss": 0.7086,
      "step": 172700
    },
    {
      "epoch": 2.7127158555729984,
      "grad_norm": 4.528508186340332,
      "learning_rate": 4.8304552590266877e-05,
      "loss": 0.7369,
      "step": 172800
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 3.1397712230682373,
      "learning_rate": 4.8303571428571434e-05,
      "loss": 0.6833,
      "step": 172900
    },
    {
      "epoch": 2.7158555729984304,
      "grad_norm": 4.300718784332275,
      "learning_rate": 4.8302590266875985e-05,
      "loss": 0.6303,
      "step": 173000
    },
    {
      "epoch": 2.717425431711146,
      "grad_norm": 4.466520309448242,
      "learning_rate": 4.8301609105180536e-05,
      "loss": 0.7644,
      "step": 173100
    },
    {
      "epoch": 2.718995290423862,
      "grad_norm": 4.537648677825928,
      "learning_rate": 4.830062794348509e-05,
      "loss": 0.6887,
      "step": 173200
    },
    {
      "epoch": 2.7205651491365774,
      "grad_norm": 3.602142095565796,
      "learning_rate": 4.8299646781789645e-05,
      "loss": 0.6791,
      "step": 173300
    },
    {
      "epoch": 2.7221350078492934,
      "grad_norm": 4.231595039367676,
      "learning_rate": 4.829866562009419e-05,
      "loss": 0.6828,
      "step": 173400
    },
    {
      "epoch": 2.7237048665620094,
      "grad_norm": 4.645932197570801,
      "learning_rate": 4.829768445839875e-05,
      "loss": 0.6867,
      "step": 173500
    },
    {
      "epoch": 2.7252747252747254,
      "grad_norm": 3.8230466842651367,
      "learning_rate": 4.82967032967033e-05,
      "loss": 0.7513,
      "step": 173600
    },
    {
      "epoch": 2.7268445839874413,
      "grad_norm": 4.479538440704346,
      "learning_rate": 4.8295722135007856e-05,
      "loss": 0.7525,
      "step": 173700
    },
    {
      "epoch": 2.728414442700157,
      "grad_norm": 3.6988818645477295,
      "learning_rate": 4.82947409733124e-05,
      "loss": 0.7378,
      "step": 173800
    },
    {
      "epoch": 2.729984301412873,
      "grad_norm": 4.346449375152588,
      "learning_rate": 4.829375981161696e-05,
      "loss": 0.7262,
      "step": 173900
    },
    {
      "epoch": 2.731554160125589,
      "grad_norm": 4.128853797912598,
      "learning_rate": 4.829277864992151e-05,
      "loss": 0.6798,
      "step": 174000
    },
    {
      "epoch": 2.7331240188383044,
      "grad_norm": 5.047815322875977,
      "learning_rate": 4.829179748822606e-05,
      "loss": 0.7403,
      "step": 174100
    },
    {
      "epoch": 2.7346938775510203,
      "grad_norm": 4.637297630310059,
      "learning_rate": 4.829081632653061e-05,
      "loss": 0.6988,
      "step": 174200
    },
    {
      "epoch": 2.7362637362637363,
      "grad_norm": 3.8183681964874268,
      "learning_rate": 4.828983516483517e-05,
      "loss": 0.7062,
      "step": 174300
    },
    {
      "epoch": 2.7378335949764523,
      "grad_norm": 3.2007949352264404,
      "learning_rate": 4.828885400313972e-05,
      "loss": 0.6895,
      "step": 174400
    },
    {
      "epoch": 2.739403453689168,
      "grad_norm": 4.390015125274658,
      "learning_rate": 4.828787284144427e-05,
      "loss": 0.6878,
      "step": 174500
    },
    {
      "epoch": 2.740973312401884,
      "grad_norm": 4.4993672370910645,
      "learning_rate": 4.828689167974882e-05,
      "loss": 0.6979,
      "step": 174600
    },
    {
      "epoch": 2.7425431711145998,
      "grad_norm": 3.7057318687438965,
      "learning_rate": 4.828591051805338e-05,
      "loss": 0.6812,
      "step": 174700
    },
    {
      "epoch": 2.7441130298273153,
      "grad_norm": 3.2861270904541016,
      "learning_rate": 4.828492935635793e-05,
      "loss": 0.7301,
      "step": 174800
    },
    {
      "epoch": 2.7456828885400313,
      "grad_norm": 4.517332077026367,
      "learning_rate": 4.828394819466248e-05,
      "loss": 0.7251,
      "step": 174900
    },
    {
      "epoch": 2.7472527472527473,
      "grad_norm": 4.0455193519592285,
      "learning_rate": 4.828296703296704e-05,
      "loss": 0.7284,
      "step": 175000
    },
    {
      "epoch": 2.7488226059654632,
      "grad_norm": 4.531797409057617,
      "learning_rate": 4.828198587127159e-05,
      "loss": 0.7109,
      "step": 175100
    },
    {
      "epoch": 2.750392464678179,
      "grad_norm": 4.537511825561523,
      "learning_rate": 4.828100470957614e-05,
      "loss": 0.692,
      "step": 175200
    },
    {
      "epoch": 2.7519623233908947,
      "grad_norm": 4.365380764007568,
      "learning_rate": 4.828002354788069e-05,
      "loss": 0.6876,
      "step": 175300
    },
    {
      "epoch": 2.7535321821036107,
      "grad_norm": 3.7495317459106445,
      "learning_rate": 4.827904238618525e-05,
      "loss": 0.7474,
      "step": 175400
    },
    {
      "epoch": 2.7551020408163263,
      "grad_norm": 3.4889419078826904,
      "learning_rate": 4.8278061224489794e-05,
      "loss": 0.7001,
      "step": 175500
    },
    {
      "epoch": 2.7566718995290422,
      "grad_norm": 4.501167297363281,
      "learning_rate": 4.827708006279435e-05,
      "loss": 0.7017,
      "step": 175600
    },
    {
      "epoch": 2.758241758241758,
      "grad_norm": 4.821932792663574,
      "learning_rate": 4.82760989010989e-05,
      "loss": 0.6831,
      "step": 175700
    },
    {
      "epoch": 2.759811616954474,
      "grad_norm": 4.46909761428833,
      "learning_rate": 4.827511773940346e-05,
      "loss": 0.705,
      "step": 175800
    },
    {
      "epoch": 2.76138147566719,
      "grad_norm": 3.7539279460906982,
      "learning_rate": 4.8274136577708004e-05,
      "loss": 0.68,
      "step": 175900
    },
    {
      "epoch": 2.7629513343799057,
      "grad_norm": 4.6806159019470215,
      "learning_rate": 4.827315541601256e-05,
      "loss": 0.6857,
      "step": 176000
    },
    {
      "epoch": 2.7645211930926217,
      "grad_norm": 4.202727794647217,
      "learning_rate": 4.827217425431711e-05,
      "loss": 0.6782,
      "step": 176100
    },
    {
      "epoch": 2.7660910518053377,
      "grad_norm": 3.899388313293457,
      "learning_rate": 4.8271193092621664e-05,
      "loss": 0.6622,
      "step": 176200
    },
    {
      "epoch": 2.767660910518053,
      "grad_norm": 4.106019020080566,
      "learning_rate": 4.8270211930926215e-05,
      "loss": 0.7037,
      "step": 176300
    },
    {
      "epoch": 2.769230769230769,
      "grad_norm": 4.220741271972656,
      "learning_rate": 4.826923076923077e-05,
      "loss": 0.743,
      "step": 176400
    },
    {
      "epoch": 2.770800627943485,
      "grad_norm": 2.8826231956481934,
      "learning_rate": 4.8268249607535324e-05,
      "loss": 0.7,
      "step": 176500
    },
    {
      "epoch": 2.772370486656201,
      "grad_norm": 3.354323387145996,
      "learning_rate": 4.8267268445839875e-05,
      "loss": 0.6915,
      "step": 176600
    },
    {
      "epoch": 2.7739403453689166,
      "grad_norm": 4.636471748352051,
      "learning_rate": 4.8266287284144426e-05,
      "loss": 0.7071,
      "step": 176700
    },
    {
      "epoch": 2.7755102040816326,
      "grad_norm": 2.6010494232177734,
      "learning_rate": 4.8265306122448984e-05,
      "loss": 0.7398,
      "step": 176800
    },
    {
      "epoch": 2.7770800627943486,
      "grad_norm": 4.230368137359619,
      "learning_rate": 4.8264324960753535e-05,
      "loss": 0.6913,
      "step": 176900
    },
    {
      "epoch": 2.778649921507064,
      "grad_norm": 4.4668498039245605,
      "learning_rate": 4.8263343799058085e-05,
      "loss": 0.6762,
      "step": 177000
    },
    {
      "epoch": 2.78021978021978,
      "grad_norm": 3.881147861480713,
      "learning_rate": 4.826236263736264e-05,
      "loss": 0.7324,
      "step": 177100
    },
    {
      "epoch": 2.781789638932496,
      "grad_norm": 4.228194236755371,
      "learning_rate": 4.8261381475667194e-05,
      "loss": 0.7557,
      "step": 177200
    },
    {
      "epoch": 2.783359497645212,
      "grad_norm": 4.8045878410339355,
      "learning_rate": 4.8260400313971745e-05,
      "loss": 0.714,
      "step": 177300
    },
    {
      "epoch": 2.784929356357928,
      "grad_norm": 4.154351234436035,
      "learning_rate": 4.8259419152276296e-05,
      "loss": 0.6093,
      "step": 177400
    },
    {
      "epoch": 2.7864992150706436,
      "grad_norm": 4.115942001342773,
      "learning_rate": 4.8258437990580854e-05,
      "loss": 0.6891,
      "step": 177500
    },
    {
      "epoch": 2.7880690737833596,
      "grad_norm": 3.2779858112335205,
      "learning_rate": 4.82574568288854e-05,
      "loss": 0.6932,
      "step": 177600
    },
    {
      "epoch": 2.789638932496075,
      "grad_norm": 4.531918525695801,
      "learning_rate": 4.8256475667189956e-05,
      "loss": 0.6899,
      "step": 177700
    },
    {
      "epoch": 2.791208791208791,
      "grad_norm": 4.421391487121582,
      "learning_rate": 4.825549450549451e-05,
      "loss": 0.6889,
      "step": 177800
    },
    {
      "epoch": 2.792778649921507,
      "grad_norm": 3.8425252437591553,
      "learning_rate": 4.8254513343799065e-05,
      "loss": 0.6758,
      "step": 177900
    },
    {
      "epoch": 2.794348508634223,
      "grad_norm": 3.864567995071411,
      "learning_rate": 4.825353218210361e-05,
      "loss": 0.7144,
      "step": 178000
    },
    {
      "epoch": 2.795918367346939,
      "grad_norm": 4.542953968048096,
      "learning_rate": 4.8252551020408167e-05,
      "loss": 0.695,
      "step": 178100
    },
    {
      "epoch": 2.7974882260596545,
      "grad_norm": 4.177713394165039,
      "learning_rate": 4.825156985871272e-05,
      "loss": 0.7081,
      "step": 178200
    },
    {
      "epoch": 2.7990580847723705,
      "grad_norm": 4.01711368560791,
      "learning_rate": 4.825058869701727e-05,
      "loss": 0.7272,
      "step": 178300
    },
    {
      "epoch": 2.8006279434850865,
      "grad_norm": 3.3783750534057617,
      "learning_rate": 4.824960753532182e-05,
      "loss": 0.7001,
      "step": 178400
    },
    {
      "epoch": 2.802197802197802,
      "grad_norm": 4.024144172668457,
      "learning_rate": 4.824862637362638e-05,
      "loss": 0.7425,
      "step": 178500
    },
    {
      "epoch": 2.803767660910518,
      "grad_norm": 4.057023048400879,
      "learning_rate": 4.824764521193093e-05,
      "loss": 0.6789,
      "step": 178600
    },
    {
      "epoch": 2.805337519623234,
      "grad_norm": 4.104526996612549,
      "learning_rate": 4.824666405023548e-05,
      "loss": 0.6979,
      "step": 178700
    },
    {
      "epoch": 2.80690737833595,
      "grad_norm": 3.7509853839874268,
      "learning_rate": 4.824568288854003e-05,
      "loss": 0.7097,
      "step": 178800
    },
    {
      "epoch": 2.8084772370486655,
      "grad_norm": 2.681591749191284,
      "learning_rate": 4.824470172684459e-05,
      "loss": 0.6776,
      "step": 178900
    },
    {
      "epoch": 2.8100470957613815,
      "grad_norm": 4.5703558921813965,
      "learning_rate": 4.824372056514914e-05,
      "loss": 0.7015,
      "step": 179000
    },
    {
      "epoch": 2.8116169544740974,
      "grad_norm": 3.510470151901245,
      "learning_rate": 4.824273940345369e-05,
      "loss": 0.6889,
      "step": 179100
    },
    {
      "epoch": 2.813186813186813,
      "grad_norm": 5.074857711791992,
      "learning_rate": 4.824175824175825e-05,
      "loss": 0.7252,
      "step": 179200
    },
    {
      "epoch": 2.814756671899529,
      "grad_norm": 5.101118564605713,
      "learning_rate": 4.82407770800628e-05,
      "loss": 0.7092,
      "step": 179300
    },
    {
      "epoch": 2.816326530612245,
      "grad_norm": 4.524838447570801,
      "learning_rate": 4.823979591836735e-05,
      "loss": 0.6611,
      "step": 179400
    },
    {
      "epoch": 2.817896389324961,
      "grad_norm": 3.6106512546539307,
      "learning_rate": 4.82388147566719e-05,
      "loss": 0.6955,
      "step": 179500
    },
    {
      "epoch": 2.819466248037677,
      "grad_norm": 3.458310127258301,
      "learning_rate": 4.823783359497646e-05,
      "loss": 0.7025,
      "step": 179600
    },
    {
      "epoch": 2.8210361067503924,
      "grad_norm": 5.124490737915039,
      "learning_rate": 4.8236852433281e-05,
      "loss": 0.6731,
      "step": 179700
    },
    {
      "epoch": 2.8226059654631084,
      "grad_norm": 4.225178241729736,
      "learning_rate": 4.823587127158556e-05,
      "loss": 0.7046,
      "step": 179800
    },
    {
      "epoch": 2.824175824175824,
      "grad_norm": 3.928683280944824,
      "learning_rate": 4.823489010989011e-05,
      "loss": 0.7064,
      "step": 179900
    },
    {
      "epoch": 2.82574568288854,
      "grad_norm": 3.6823554039001465,
      "learning_rate": 4.823390894819467e-05,
      "loss": 0.6821,
      "step": 180000
    },
    {
      "epoch": 2.827315541601256,
      "grad_norm": 4.349119186401367,
      "learning_rate": 4.823292778649921e-05,
      "loss": 0.7344,
      "step": 180100
    },
    {
      "epoch": 2.828885400313972,
      "grad_norm": 4.191588878631592,
      "learning_rate": 4.823194662480377e-05,
      "loss": 0.6933,
      "step": 180200
    },
    {
      "epoch": 2.830455259026688,
      "grad_norm": 3.9894027709960938,
      "learning_rate": 4.823096546310832e-05,
      "loss": 0.6721,
      "step": 180300
    },
    {
      "epoch": 2.8320251177394034,
      "grad_norm": 4.587655544281006,
      "learning_rate": 4.822998430141287e-05,
      "loss": 0.7215,
      "step": 180400
    },
    {
      "epoch": 2.8335949764521193,
      "grad_norm": 3.7256174087524414,
      "learning_rate": 4.8229003139717424e-05,
      "loss": 0.7083,
      "step": 180500
    },
    {
      "epoch": 2.8351648351648353,
      "grad_norm": 3.6274969577789307,
      "learning_rate": 4.822802197802198e-05,
      "loss": 0.6979,
      "step": 180600
    },
    {
      "epoch": 2.836734693877551,
      "grad_norm": 3.824345111846924,
      "learning_rate": 4.822704081632653e-05,
      "loss": 0.7289,
      "step": 180700
    },
    {
      "epoch": 2.838304552590267,
      "grad_norm": 3.458674907684326,
      "learning_rate": 4.8226059654631084e-05,
      "loss": 0.6607,
      "step": 180800
    },
    {
      "epoch": 2.839874411302983,
      "grad_norm": 3.5549142360687256,
      "learning_rate": 4.8225078492935635e-05,
      "loss": 0.697,
      "step": 180900
    },
    {
      "epoch": 2.8414442700156988,
      "grad_norm": 5.348664283752441,
      "learning_rate": 4.822409733124019e-05,
      "loss": 0.7138,
      "step": 181000
    },
    {
      "epoch": 2.8430141287284143,
      "grad_norm": 4.044829368591309,
      "learning_rate": 4.8223116169544743e-05,
      "loss": 0.696,
      "step": 181100
    },
    {
      "epoch": 2.8445839874411303,
      "grad_norm": 4.287741661071777,
      "learning_rate": 4.8222135007849294e-05,
      "loss": 0.7264,
      "step": 181200
    },
    {
      "epoch": 2.8461538461538463,
      "grad_norm": 4.405102729797363,
      "learning_rate": 4.822115384615385e-05,
      "loss": 0.6991,
      "step": 181300
    },
    {
      "epoch": 2.847723704866562,
      "grad_norm": 3.562575340270996,
      "learning_rate": 4.82201726844584e-05,
      "loss": 0.7515,
      "step": 181400
    },
    {
      "epoch": 2.8492935635792778,
      "grad_norm": 3.783560276031494,
      "learning_rate": 4.8219191522762954e-05,
      "loss": 0.6668,
      "step": 181500
    },
    {
      "epoch": 2.8508634222919937,
      "grad_norm": 3.932407855987549,
      "learning_rate": 4.8218210361067505e-05,
      "loss": 0.7021,
      "step": 181600
    },
    {
      "epoch": 2.8524332810047097,
      "grad_norm": 4.859043121337891,
      "learning_rate": 4.821722919937206e-05,
      "loss": 0.7071,
      "step": 181700
    },
    {
      "epoch": 2.8540031397174257,
      "grad_norm": 4.4511399269104,
      "learning_rate": 4.821624803767661e-05,
      "loss": 0.7011,
      "step": 181800
    },
    {
      "epoch": 2.8555729984301412,
      "grad_norm": 3.3892221450805664,
      "learning_rate": 4.8215266875981165e-05,
      "loss": 0.6702,
      "step": 181900
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 4.588706016540527,
      "learning_rate": 4.8214285714285716e-05,
      "loss": 0.7064,
      "step": 182000
    },
    {
      "epoch": 2.8587127158555727,
      "grad_norm": 3.9084575176239014,
      "learning_rate": 4.8213304552590274e-05,
      "loss": 0.6963,
      "step": 182100
    },
    {
      "epoch": 2.8602825745682887,
      "grad_norm": 4.83244514465332,
      "learning_rate": 4.821232339089482e-05,
      "loss": 0.6733,
      "step": 182200
    },
    {
      "epoch": 2.8618524332810047,
      "grad_norm": 3.8416154384613037,
      "learning_rate": 4.8211342229199376e-05,
      "loss": 0.7161,
      "step": 182300
    },
    {
      "epoch": 2.8634222919937207,
      "grad_norm": 4.5097479820251465,
      "learning_rate": 4.8210361067503927e-05,
      "loss": 0.7078,
      "step": 182400
    },
    {
      "epoch": 2.8649921507064366,
      "grad_norm": 4.545917510986328,
      "learning_rate": 4.820937990580848e-05,
      "loss": 0.6854,
      "step": 182500
    },
    {
      "epoch": 2.866562009419152,
      "grad_norm": 2.678083896636963,
      "learning_rate": 4.820839874411303e-05,
      "loss": 0.6791,
      "step": 182600
    },
    {
      "epoch": 2.868131868131868,
      "grad_norm": 3.0312271118164062,
      "learning_rate": 4.8207417582417586e-05,
      "loss": 0.681,
      "step": 182700
    },
    {
      "epoch": 2.869701726844584,
      "grad_norm": 4.905115604400635,
      "learning_rate": 4.820643642072214e-05,
      "loss": 0.6992,
      "step": 182800
    },
    {
      "epoch": 2.8712715855572997,
      "grad_norm": 3.825082778930664,
      "learning_rate": 4.820545525902669e-05,
      "loss": 0.7292,
      "step": 182900
    },
    {
      "epoch": 2.8728414442700156,
      "grad_norm": 5.162566184997559,
      "learning_rate": 4.820447409733124e-05,
      "loss": 0.703,
      "step": 183000
    },
    {
      "epoch": 2.8744113029827316,
      "grad_norm": 3.948223114013672,
      "learning_rate": 4.82034929356358e-05,
      "loss": 0.7057,
      "step": 183100
    },
    {
      "epoch": 2.8759811616954476,
      "grad_norm": 4.396376132965088,
      "learning_rate": 4.820251177394035e-05,
      "loss": 0.7318,
      "step": 183200
    },
    {
      "epoch": 2.877551020408163,
      "grad_norm": 4.4753828048706055,
      "learning_rate": 4.82015306122449e-05,
      "loss": 0.7448,
      "step": 183300
    },
    {
      "epoch": 2.879120879120879,
      "grad_norm": 4.089789867401123,
      "learning_rate": 4.820054945054946e-05,
      "loss": 0.7235,
      "step": 183400
    },
    {
      "epoch": 2.880690737833595,
      "grad_norm": 3.9084811210632324,
      "learning_rate": 4.819956828885401e-05,
      "loss": 0.6822,
      "step": 183500
    },
    {
      "epoch": 2.8822605965463106,
      "grad_norm": 4.752087593078613,
      "learning_rate": 4.819858712715856e-05,
      "loss": 0.6973,
      "step": 183600
    },
    {
      "epoch": 2.8838304552590266,
      "grad_norm": 4.040282726287842,
      "learning_rate": 4.819760596546311e-05,
      "loss": 0.7154,
      "step": 183700
    },
    {
      "epoch": 2.8854003139717426,
      "grad_norm": 4.774153232574463,
      "learning_rate": 4.819662480376767e-05,
      "loss": 0.7204,
      "step": 183800
    },
    {
      "epoch": 2.8869701726844585,
      "grad_norm": 2.9095423221588135,
      "learning_rate": 4.819564364207221e-05,
      "loss": 0.6409,
      "step": 183900
    },
    {
      "epoch": 2.8885400313971745,
      "grad_norm": 4.940536022186279,
      "learning_rate": 4.819466248037677e-05,
      "loss": 0.7232,
      "step": 184000
    },
    {
      "epoch": 2.89010989010989,
      "grad_norm": 3.7032344341278076,
      "learning_rate": 4.819368131868132e-05,
      "loss": 0.7538,
      "step": 184100
    },
    {
      "epoch": 2.891679748822606,
      "grad_norm": 4.724015712738037,
      "learning_rate": 4.819270015698588e-05,
      "loss": 0.7046,
      "step": 184200
    },
    {
      "epoch": 2.8932496075353216,
      "grad_norm": 3.496257781982422,
      "learning_rate": 4.819171899529042e-05,
      "loss": 0.7095,
      "step": 184300
    },
    {
      "epoch": 2.8948194662480375,
      "grad_norm": 4.9724907875061035,
      "learning_rate": 4.819073783359498e-05,
      "loss": 0.6893,
      "step": 184400
    },
    {
      "epoch": 2.8963893249607535,
      "grad_norm": 3.715578556060791,
      "learning_rate": 4.818975667189953e-05,
      "loss": 0.7221,
      "step": 184500
    },
    {
      "epoch": 2.8979591836734695,
      "grad_norm": 4.456667423248291,
      "learning_rate": 4.818877551020408e-05,
      "loss": 0.711,
      "step": 184600
    },
    {
      "epoch": 2.8995290423861855,
      "grad_norm": 3.7876675128936768,
      "learning_rate": 4.818779434850863e-05,
      "loss": 0.6862,
      "step": 184700
    },
    {
      "epoch": 2.901098901098901,
      "grad_norm": 4.576122760772705,
      "learning_rate": 4.818681318681319e-05,
      "loss": 0.7225,
      "step": 184800
    },
    {
      "epoch": 2.902668759811617,
      "grad_norm": 4.666158676147461,
      "learning_rate": 4.818583202511774e-05,
      "loss": 0.6809,
      "step": 184900
    },
    {
      "epoch": 2.904238618524333,
      "grad_norm": 4.602603435516357,
      "learning_rate": 4.818485086342229e-05,
      "loss": 0.7094,
      "step": 185000
    },
    {
      "epoch": 2.9058084772370485,
      "grad_norm": 2.5646872520446777,
      "learning_rate": 4.8183869701726844e-05,
      "loss": 0.7072,
      "step": 185100
    },
    {
      "epoch": 2.9073783359497645,
      "grad_norm": 4.250153064727783,
      "learning_rate": 4.81828885400314e-05,
      "loss": 0.7258,
      "step": 185200
    },
    {
      "epoch": 2.9089481946624804,
      "grad_norm": 3.8130693435668945,
      "learning_rate": 4.818190737833595e-05,
      "loss": 0.746,
      "step": 185300
    },
    {
      "epoch": 2.9105180533751964,
      "grad_norm": 4.824798107147217,
      "learning_rate": 4.8180926216640503e-05,
      "loss": 0.7183,
      "step": 185400
    },
    {
      "epoch": 2.912087912087912,
      "grad_norm": 4.410651683807373,
      "learning_rate": 4.817994505494506e-05,
      "loss": 0.7164,
      "step": 185500
    },
    {
      "epoch": 2.913657770800628,
      "grad_norm": 4.254293441772461,
      "learning_rate": 4.817896389324961e-05,
      "loss": 0.7128,
      "step": 185600
    },
    {
      "epoch": 2.915227629513344,
      "grad_norm": 3.7037100791931152,
      "learning_rate": 4.817798273155416e-05,
      "loss": 0.6581,
      "step": 185700
    },
    {
      "epoch": 2.9167974882260594,
      "grad_norm": 3.6017465591430664,
      "learning_rate": 4.8177001569858714e-05,
      "loss": 0.6562,
      "step": 185800
    },
    {
      "epoch": 2.9183673469387754,
      "grad_norm": 4.097520351409912,
      "learning_rate": 4.817602040816327e-05,
      "loss": 0.6623,
      "step": 185900
    },
    {
      "epoch": 2.9199372056514914,
      "grad_norm": 4.176619529724121,
      "learning_rate": 4.8175039246467816e-05,
      "loss": 0.7051,
      "step": 186000
    },
    {
      "epoch": 2.9215070643642074,
      "grad_norm": 3.5826070308685303,
      "learning_rate": 4.8174058084772374e-05,
      "loss": 0.72,
      "step": 186100
    },
    {
      "epoch": 2.9230769230769234,
      "grad_norm": 4.078954219818115,
      "learning_rate": 4.8173076923076925e-05,
      "loss": 0.7233,
      "step": 186200
    },
    {
      "epoch": 2.924646781789639,
      "grad_norm": 2.772111654281616,
      "learning_rate": 4.817209576138148e-05,
      "loss": 0.6443,
      "step": 186300
    },
    {
      "epoch": 2.926216640502355,
      "grad_norm": 4.233848571777344,
      "learning_rate": 4.817111459968603e-05,
      "loss": 0.7225,
      "step": 186400
    },
    {
      "epoch": 2.9277864992150704,
      "grad_norm": 3.857605457305908,
      "learning_rate": 4.8170133437990585e-05,
      "loss": 0.6925,
      "step": 186500
    },
    {
      "epoch": 2.9293563579277864,
      "grad_norm": 3.4322078227996826,
      "learning_rate": 4.8169152276295136e-05,
      "loss": 0.7097,
      "step": 186600
    },
    {
      "epoch": 2.9309262166405023,
      "grad_norm": 3.902927875518799,
      "learning_rate": 4.8168171114599687e-05,
      "loss": 0.7338,
      "step": 186700
    },
    {
      "epoch": 2.9324960753532183,
      "grad_norm": 4.2389936447143555,
      "learning_rate": 4.816718995290424e-05,
      "loss": 0.7096,
      "step": 186800
    },
    {
      "epoch": 2.9340659340659343,
      "grad_norm": 3.996779203414917,
      "learning_rate": 4.8166208791208795e-05,
      "loss": 0.7373,
      "step": 186900
    },
    {
      "epoch": 2.93563579277865,
      "grad_norm": 3.3019227981567383,
      "learning_rate": 4.8165227629513346e-05,
      "loss": 0.7108,
      "step": 187000
    },
    {
      "epoch": 2.937205651491366,
      "grad_norm": 3.039077043533325,
      "learning_rate": 4.81642464678179e-05,
      "loss": 0.6656,
      "step": 187100
    },
    {
      "epoch": 2.938775510204082,
      "grad_norm": 4.294804573059082,
      "learning_rate": 4.816326530612245e-05,
      "loss": 0.7081,
      "step": 187200
    },
    {
      "epoch": 2.9403453689167973,
      "grad_norm": 4.115298748016357,
      "learning_rate": 4.8162284144427006e-05,
      "loss": 0.7541,
      "step": 187300
    },
    {
      "epoch": 2.9419152276295133,
      "grad_norm": 4.396572589874268,
      "learning_rate": 4.816130298273156e-05,
      "loss": 0.6951,
      "step": 187400
    },
    {
      "epoch": 2.9434850863422293,
      "grad_norm": 4.493447780609131,
      "learning_rate": 4.816032182103611e-05,
      "loss": 0.6981,
      "step": 187500
    },
    {
      "epoch": 2.9450549450549453,
      "grad_norm": 3.401179790496826,
      "learning_rate": 4.8159340659340666e-05,
      "loss": 0.6833,
      "step": 187600
    },
    {
      "epoch": 2.946624803767661,
      "grad_norm": 4.176111698150635,
      "learning_rate": 4.815835949764522e-05,
      "loss": 0.7233,
      "step": 187700
    },
    {
      "epoch": 2.9481946624803768,
      "grad_norm": 4.05111837387085,
      "learning_rate": 4.815737833594977e-05,
      "loss": 0.7221,
      "step": 187800
    },
    {
      "epoch": 2.9497645211930927,
      "grad_norm": 4.788556098937988,
      "learning_rate": 4.815639717425432e-05,
      "loss": 0.6883,
      "step": 187900
    },
    {
      "epoch": 2.9513343799058083,
      "grad_norm": 4.021585941314697,
      "learning_rate": 4.8155416012558876e-05,
      "loss": 0.6774,
      "step": 188000
    },
    {
      "epoch": 2.9529042386185242,
      "grad_norm": 3.8725602626800537,
      "learning_rate": 4.815443485086342e-05,
      "loss": 0.7154,
      "step": 188100
    },
    {
      "epoch": 2.9544740973312402,
      "grad_norm": 2.822605609893799,
      "learning_rate": 4.815345368916798e-05,
      "loss": 0.7165,
      "step": 188200
    },
    {
      "epoch": 2.956043956043956,
      "grad_norm": 3.4479870796203613,
      "learning_rate": 4.815247252747253e-05,
      "loss": 0.6928,
      "step": 188300
    },
    {
      "epoch": 2.957613814756672,
      "grad_norm": 3.732884168624878,
      "learning_rate": 4.815149136577709e-05,
      "loss": 0.6798,
      "step": 188400
    },
    {
      "epoch": 2.9591836734693877,
      "grad_norm": 3.760666608810425,
      "learning_rate": 4.815051020408163e-05,
      "loss": 0.7118,
      "step": 188500
    },
    {
      "epoch": 2.9607535321821037,
      "grad_norm": 3.485470771789551,
      "learning_rate": 4.814952904238619e-05,
      "loss": 0.7013,
      "step": 188600
    },
    {
      "epoch": 2.962323390894819,
      "grad_norm": 3.44577693939209,
      "learning_rate": 4.814854788069074e-05,
      "loss": 0.7199,
      "step": 188700
    },
    {
      "epoch": 2.963893249607535,
      "grad_norm": 3.993751049041748,
      "learning_rate": 4.814756671899529e-05,
      "loss": 0.6891,
      "step": 188800
    },
    {
      "epoch": 2.965463108320251,
      "grad_norm": 4.188378810882568,
      "learning_rate": 4.814658555729984e-05,
      "loss": 0.7412,
      "step": 188900
    },
    {
      "epoch": 2.967032967032967,
      "grad_norm": 4.056826114654541,
      "learning_rate": 4.81456043956044e-05,
      "loss": 0.7366,
      "step": 189000
    },
    {
      "epoch": 2.968602825745683,
      "grad_norm": 3.965843677520752,
      "learning_rate": 4.814462323390895e-05,
      "loss": 0.6824,
      "step": 189100
    },
    {
      "epoch": 2.9701726844583987,
      "grad_norm": 3.856165885925293,
      "learning_rate": 4.81436420722135e-05,
      "loss": 0.662,
      "step": 189200
    },
    {
      "epoch": 2.9717425431711146,
      "grad_norm": 3.991565227508545,
      "learning_rate": 4.814266091051805e-05,
      "loss": 0.704,
      "step": 189300
    },
    {
      "epoch": 2.9733124018838306,
      "grad_norm": 3.4109885692596436,
      "learning_rate": 4.814167974882261e-05,
      "loss": 0.726,
      "step": 189400
    },
    {
      "epoch": 2.974882260596546,
      "grad_norm": 3.675649642944336,
      "learning_rate": 4.814069858712716e-05,
      "loss": 0.7081,
      "step": 189500
    },
    {
      "epoch": 2.976452119309262,
      "grad_norm": 3.790125846862793,
      "learning_rate": 4.813971742543171e-05,
      "loss": 0.7343,
      "step": 189600
    },
    {
      "epoch": 2.978021978021978,
      "grad_norm": 4.105866432189941,
      "learning_rate": 4.813873626373627e-05,
      "loss": 0.6868,
      "step": 189700
    },
    {
      "epoch": 2.979591836734694,
      "grad_norm": 4.681905269622803,
      "learning_rate": 4.813775510204082e-05,
      "loss": 0.7407,
      "step": 189800
    },
    {
      "epoch": 2.9811616954474096,
      "grad_norm": 4.361865043640137,
      "learning_rate": 4.813677394034537e-05,
      "loss": 0.714,
      "step": 189900
    },
    {
      "epoch": 2.9827315541601256,
      "grad_norm": 4.129542350769043,
      "learning_rate": 4.813579277864992e-05,
      "loss": 0.6539,
      "step": 190000
    },
    {
      "epoch": 2.9843014128728416,
      "grad_norm": 4.738590717315674,
      "learning_rate": 4.813481161695448e-05,
      "loss": 0.7301,
      "step": 190100
    },
    {
      "epoch": 2.985871271585557,
      "grad_norm": 4.121333599090576,
      "learning_rate": 4.8133830455259025e-05,
      "loss": 0.6929,
      "step": 190200
    },
    {
      "epoch": 2.987441130298273,
      "grad_norm": 4.665836334228516,
      "learning_rate": 4.813284929356358e-05,
      "loss": 0.7408,
      "step": 190300
    },
    {
      "epoch": 2.989010989010989,
      "grad_norm": 4.415154933929443,
      "learning_rate": 4.8131868131868134e-05,
      "loss": 0.7111,
      "step": 190400
    },
    {
      "epoch": 2.990580847723705,
      "grad_norm": 4.631103515625,
      "learning_rate": 4.813088697017269e-05,
      "loss": 0.7347,
      "step": 190500
    },
    {
      "epoch": 2.9921507064364206,
      "grad_norm": 4.725419521331787,
      "learning_rate": 4.8129905808477236e-05,
      "loss": 0.668,
      "step": 190600
    },
    {
      "epoch": 2.9937205651491365,
      "grad_norm": 4.021218776702881,
      "learning_rate": 4.8128924646781794e-05,
      "loss": 0.7227,
      "step": 190700
    },
    {
      "epoch": 2.9952904238618525,
      "grad_norm": 3.8532707691192627,
      "learning_rate": 4.8127943485086345e-05,
      "loss": 0.7361,
      "step": 190800
    },
    {
      "epoch": 2.996860282574568,
      "grad_norm": 5.26715612411499,
      "learning_rate": 4.8126962323390896e-05,
      "loss": 0.7286,
      "step": 190900
    },
    {
      "epoch": 2.998430141287284,
      "grad_norm": 4.286890506744385,
      "learning_rate": 4.8125981161695446e-05,
      "loss": 0.6867,
      "step": 191000
    },
    {
      "epoch": 3.0,
      "grad_norm": 4.036189556121826,
      "learning_rate": 4.8125000000000004e-05,
      "loss": 0.6573,
      "step": 191100
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.0369540452957153,
      "eval_runtime": 14.7705,
      "eval_samples_per_second": 227.007,
      "eval_steps_per_second": 227.007,
      "step": 191100
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.55019211769104,
      "eval_runtime": 280.5794,
      "eval_samples_per_second": 227.03,
      "eval_steps_per_second": 227.03,
      "step": 191100
    },
    {
      "epoch": 3.001569858712716,
      "grad_norm": 4.133156776428223,
      "learning_rate": 4.8124018838304555e-05,
      "loss": 0.6704,
      "step": 191200
    },
    {
      "epoch": 3.0031397174254315,
      "grad_norm": 4.652074813842773,
      "learning_rate": 4.8123037676609106e-05,
      "loss": 0.6817,
      "step": 191300
    },
    {
      "epoch": 3.0047095761381475,
      "grad_norm": 4.730746269226074,
      "learning_rate": 4.812205651491366e-05,
      "loss": 0.7163,
      "step": 191400
    },
    {
      "epoch": 3.0062794348508635,
      "grad_norm": 3.2052645683288574,
      "learning_rate": 4.8121075353218215e-05,
      "loss": 0.6912,
      "step": 191500
    },
    {
      "epoch": 3.0078492935635794,
      "grad_norm": 4.583684921264648,
      "learning_rate": 4.8120094191522766e-05,
      "loss": 0.7004,
      "step": 191600
    },
    {
      "epoch": 3.009419152276295,
      "grad_norm": 3.8467257022857666,
      "learning_rate": 4.811911302982732e-05,
      "loss": 0.675,
      "step": 191700
    },
    {
      "epoch": 3.010989010989011,
      "grad_norm": 4.407788276672363,
      "learning_rate": 4.8118131868131875e-05,
      "loss": 0.6877,
      "step": 191800
    },
    {
      "epoch": 3.012558869701727,
      "grad_norm": 3.724151611328125,
      "learning_rate": 4.8117150706436426e-05,
      "loss": 0.6886,
      "step": 191900
    },
    {
      "epoch": 3.014128728414443,
      "grad_norm": 4.762870788574219,
      "learning_rate": 4.811616954474098e-05,
      "loss": 0.6678,
      "step": 192000
    },
    {
      "epoch": 3.0156985871271584,
      "grad_norm": 5.092325210571289,
      "learning_rate": 4.811518838304553e-05,
      "loss": 0.7108,
      "step": 192100
    },
    {
      "epoch": 3.0172684458398744,
      "grad_norm": 3.3413734436035156,
      "learning_rate": 4.8114207221350085e-05,
      "loss": 0.6829,
      "step": 192200
    },
    {
      "epoch": 3.0188383045525904,
      "grad_norm": 4.295369625091553,
      "learning_rate": 4.811322605965463e-05,
      "loss": 0.6704,
      "step": 192300
    },
    {
      "epoch": 3.020408163265306,
      "grad_norm": 23.370685577392578,
      "learning_rate": 4.811224489795919e-05,
      "loss": 0.6768,
      "step": 192400
    },
    {
      "epoch": 3.021978021978022,
      "grad_norm": 4.092077255249023,
      "learning_rate": 4.811126373626374e-05,
      "loss": 0.6657,
      "step": 192500
    },
    {
      "epoch": 3.023547880690738,
      "grad_norm": 4.58964204788208,
      "learning_rate": 4.8110282574568296e-05,
      "loss": 0.7194,
      "step": 192600
    },
    {
      "epoch": 3.025117739403454,
      "grad_norm": 3.9247243404388428,
      "learning_rate": 4.810930141287284e-05,
      "loss": 0.7149,
      "step": 192700
    },
    {
      "epoch": 3.0266875981161694,
      "grad_norm": 5.6077961921691895,
      "learning_rate": 4.81083202511774e-05,
      "loss": 0.7003,
      "step": 192800
    },
    {
      "epoch": 3.0282574568288854,
      "grad_norm": 3.3341891765594482,
      "learning_rate": 4.810733908948195e-05,
      "loss": 0.7075,
      "step": 192900
    },
    {
      "epoch": 3.0298273155416013,
      "grad_norm": 3.721918821334839,
      "learning_rate": 4.81063579277865e-05,
      "loss": 0.6635,
      "step": 193000
    },
    {
      "epoch": 3.0313971742543173,
      "grad_norm": 3.786745071411133,
      "learning_rate": 4.810537676609105e-05,
      "loss": 0.7148,
      "step": 193100
    },
    {
      "epoch": 3.032967032967033,
      "grad_norm": 4.52922248840332,
      "learning_rate": 4.810439560439561e-05,
      "loss": 0.6966,
      "step": 193200
    },
    {
      "epoch": 3.034536891679749,
      "grad_norm": 4.808505058288574,
      "learning_rate": 4.810341444270016e-05,
      "loss": 0.6632,
      "step": 193300
    },
    {
      "epoch": 3.036106750392465,
      "grad_norm": 4.241766929626465,
      "learning_rate": 4.810243328100471e-05,
      "loss": 0.66,
      "step": 193400
    },
    {
      "epoch": 3.0376766091051803,
      "grad_norm": 3.00931978225708,
      "learning_rate": 4.810145211930926e-05,
      "loss": 0.6934,
      "step": 193500
    },
    {
      "epoch": 3.0392464678178963,
      "grad_norm": 3.9952845573425293,
      "learning_rate": 4.810047095761382e-05,
      "loss": 0.683,
      "step": 193600
    },
    {
      "epoch": 3.0408163265306123,
      "grad_norm": 1.9923311471939087,
      "learning_rate": 4.809948979591837e-05,
      "loss": 0.6902,
      "step": 193700
    },
    {
      "epoch": 3.0423861852433283,
      "grad_norm": 4.360772132873535,
      "learning_rate": 4.809850863422292e-05,
      "loss": 0.6418,
      "step": 193800
    },
    {
      "epoch": 3.043956043956044,
      "grad_norm": 2.8307223320007324,
      "learning_rate": 4.809752747252748e-05,
      "loss": 0.69,
      "step": 193900
    },
    {
      "epoch": 3.0455259026687598,
      "grad_norm": 4.034136772155762,
      "learning_rate": 4.809654631083202e-05,
      "loss": 0.6778,
      "step": 194000
    },
    {
      "epoch": 3.0470957613814758,
      "grad_norm": 3.2680962085723877,
      "learning_rate": 4.809556514913658e-05,
      "loss": 0.7199,
      "step": 194100
    },
    {
      "epoch": 3.0486656200941917,
      "grad_norm": 4.699560642242432,
      "learning_rate": 4.809458398744113e-05,
      "loss": 0.6952,
      "step": 194200
    },
    {
      "epoch": 3.0502354788069073,
      "grad_norm": 4.386282920837402,
      "learning_rate": 4.809360282574569e-05,
      "loss": 0.7294,
      "step": 194300
    },
    {
      "epoch": 3.0518053375196232,
      "grad_norm": 3.289034128189087,
      "learning_rate": 4.8092621664050234e-05,
      "loss": 0.7289,
      "step": 194400
    },
    {
      "epoch": 3.053375196232339,
      "grad_norm": 3.18230938911438,
      "learning_rate": 4.809164050235479e-05,
      "loss": 0.6854,
      "step": 194500
    },
    {
      "epoch": 3.0549450549450547,
      "grad_norm": 4.237265110015869,
      "learning_rate": 4.809065934065934e-05,
      "loss": 0.6734,
      "step": 194600
    },
    {
      "epoch": 3.0565149136577707,
      "grad_norm": 3.923238515853882,
      "learning_rate": 4.8089678178963894e-05,
      "loss": 0.7308,
      "step": 194700
    },
    {
      "epoch": 3.0580847723704867,
      "grad_norm": 3.522254705429077,
      "learning_rate": 4.8088697017268445e-05,
      "loss": 0.6861,
      "step": 194800
    },
    {
      "epoch": 3.0596546310832027,
      "grad_norm": 3.8023152351379395,
      "learning_rate": 4.8087715855573e-05,
      "loss": 0.7196,
      "step": 194900
    },
    {
      "epoch": 3.061224489795918,
      "grad_norm": 4.4151201248168945,
      "learning_rate": 4.8086734693877554e-05,
      "loss": 0.6858,
      "step": 195000
    },
    {
      "epoch": 3.062794348508634,
      "grad_norm": 3.909113883972168,
      "learning_rate": 4.8085753532182104e-05,
      "loss": 0.6622,
      "step": 195100
    },
    {
      "epoch": 3.06436420722135,
      "grad_norm": 3.9128644466400146,
      "learning_rate": 4.8084772370486655e-05,
      "loss": 0.6576,
      "step": 195200
    },
    {
      "epoch": 3.065934065934066,
      "grad_norm": 3.5027759075164795,
      "learning_rate": 4.808379120879121e-05,
      "loss": 0.6831,
      "step": 195300
    },
    {
      "epoch": 3.0675039246467817,
      "grad_norm": 4.168638229370117,
      "learning_rate": 4.8082810047095764e-05,
      "loss": 0.6985,
      "step": 195400
    },
    {
      "epoch": 3.0690737833594977,
      "grad_norm": 4.199560165405273,
      "learning_rate": 4.8081828885400315e-05,
      "loss": 0.7023,
      "step": 195500
    },
    {
      "epoch": 3.0706436420722136,
      "grad_norm": 4.771370887756348,
      "learning_rate": 4.8080847723704866e-05,
      "loss": 0.7148,
      "step": 195600
    },
    {
      "epoch": 3.072213500784929,
      "grad_norm": 3.82867169380188,
      "learning_rate": 4.8079866562009424e-05,
      "loss": 0.6772,
      "step": 195700
    },
    {
      "epoch": 3.073783359497645,
      "grad_norm": 3.4165894985198975,
      "learning_rate": 4.8078885400313975e-05,
      "loss": 0.6768,
      "step": 195800
    },
    {
      "epoch": 3.075353218210361,
      "grad_norm": 3.4058926105499268,
      "learning_rate": 4.8077904238618526e-05,
      "loss": 0.653,
      "step": 195900
    },
    {
      "epoch": 3.076923076923077,
      "grad_norm": 4.754837989807129,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 0.7452,
      "step": 196000
    },
    {
      "epoch": 3.0784929356357926,
      "grad_norm": 3.637115955352783,
      "learning_rate": 4.807594191522763e-05,
      "loss": 0.6937,
      "step": 196100
    },
    {
      "epoch": 3.0800627943485086,
      "grad_norm": 4.75600004196167,
      "learning_rate": 4.8074960753532186e-05,
      "loss": 0.7124,
      "step": 196200
    },
    {
      "epoch": 3.0816326530612246,
      "grad_norm": 3.627723217010498,
      "learning_rate": 4.8073979591836737e-05,
      "loss": 0.713,
      "step": 196300
    },
    {
      "epoch": 3.0832025117739406,
      "grad_norm": 4.0438103675842285,
      "learning_rate": 4.8072998430141294e-05,
      "loss": 0.6978,
      "step": 196400
    },
    {
      "epoch": 3.084772370486656,
      "grad_norm": 4.344779968261719,
      "learning_rate": 4.807201726844584e-05,
      "loss": 0.7086,
      "step": 196500
    },
    {
      "epoch": 3.086342229199372,
      "grad_norm": 3.185119390487671,
      "learning_rate": 4.8071036106750396e-05,
      "loss": 0.7067,
      "step": 196600
    },
    {
      "epoch": 3.087912087912088,
      "grad_norm": 4.195657253265381,
      "learning_rate": 4.807005494505495e-05,
      "loss": 0.6977,
      "step": 196700
    },
    {
      "epoch": 3.0894819466248036,
      "grad_norm": 3.0973894596099854,
      "learning_rate": 4.80690737833595e-05,
      "loss": 0.6838,
      "step": 196800
    },
    {
      "epoch": 3.0910518053375196,
      "grad_norm": 5.333001613616943,
      "learning_rate": 4.806809262166405e-05,
      "loss": 0.6762,
      "step": 196900
    },
    {
      "epoch": 3.0926216640502355,
      "grad_norm": 4.2792253494262695,
      "learning_rate": 4.806711145996861e-05,
      "loss": 0.7039,
      "step": 197000
    },
    {
      "epoch": 3.0941915227629515,
      "grad_norm": 3.9101977348327637,
      "learning_rate": 4.806613029827316e-05,
      "loss": 0.6902,
      "step": 197100
    },
    {
      "epoch": 3.095761381475667,
      "grad_norm": 4.778287887573242,
      "learning_rate": 4.806514913657771e-05,
      "loss": 0.7108,
      "step": 197200
    },
    {
      "epoch": 3.097331240188383,
      "grad_norm": 4.8320746421813965,
      "learning_rate": 4.806416797488226e-05,
      "loss": 0.7185,
      "step": 197300
    },
    {
      "epoch": 3.098901098901099,
      "grad_norm": 5.063838958740234,
      "learning_rate": 4.806318681318682e-05,
      "loss": 0.7056,
      "step": 197400
    },
    {
      "epoch": 3.100470957613815,
      "grad_norm": 4.57328987121582,
      "learning_rate": 4.806220565149137e-05,
      "loss": 0.7126,
      "step": 197500
    },
    {
      "epoch": 3.1020408163265305,
      "grad_norm": 4.648008346557617,
      "learning_rate": 4.806122448979592e-05,
      "loss": 0.7297,
      "step": 197600
    },
    {
      "epoch": 3.1036106750392465,
      "grad_norm": 4.534456729888916,
      "learning_rate": 4.806024332810047e-05,
      "loss": 0.6833,
      "step": 197700
    },
    {
      "epoch": 3.1051805337519625,
      "grad_norm": 4.193495273590088,
      "learning_rate": 4.805926216640503e-05,
      "loss": 0.6791,
      "step": 197800
    },
    {
      "epoch": 3.106750392464678,
      "grad_norm": 4.8057942390441895,
      "learning_rate": 4.805828100470958e-05,
      "loss": 0.7097,
      "step": 197900
    },
    {
      "epoch": 3.108320251177394,
      "grad_norm": 3.2807748317718506,
      "learning_rate": 4.805729984301413e-05,
      "loss": 0.6845,
      "step": 198000
    },
    {
      "epoch": 3.10989010989011,
      "grad_norm": 4.445310115814209,
      "learning_rate": 4.805631868131869e-05,
      "loss": 0.6911,
      "step": 198100
    },
    {
      "epoch": 3.111459968602826,
      "grad_norm": 4.377899646759033,
      "learning_rate": 4.805533751962323e-05,
      "loss": 0.6903,
      "step": 198200
    },
    {
      "epoch": 3.1130298273155415,
      "grad_norm": 4.688360691070557,
      "learning_rate": 4.805435635792779e-05,
      "loss": 0.7067,
      "step": 198300
    },
    {
      "epoch": 3.1145996860282574,
      "grad_norm": 3.4981529712677,
      "learning_rate": 4.805337519623234e-05,
      "loss": 0.7074,
      "step": 198400
    },
    {
      "epoch": 3.1161695447409734,
      "grad_norm": 4.0092453956604,
      "learning_rate": 4.80523940345369e-05,
      "loss": 0.7127,
      "step": 198500
    },
    {
      "epoch": 3.1177394034536894,
      "grad_norm": 4.18446683883667,
      "learning_rate": 4.805141287284144e-05,
      "loss": 0.6861,
      "step": 198600
    },
    {
      "epoch": 3.119309262166405,
      "grad_norm": 3.6966986656188965,
      "learning_rate": 4.8050431711146e-05,
      "loss": 0.6893,
      "step": 198700
    },
    {
      "epoch": 3.120879120879121,
      "grad_norm": 3.78495454788208,
      "learning_rate": 4.804945054945055e-05,
      "loss": 0.6705,
      "step": 198800
    },
    {
      "epoch": 3.122448979591837,
      "grad_norm": 4.813541412353516,
      "learning_rate": 4.80484693877551e-05,
      "loss": 0.6699,
      "step": 198900
    },
    {
      "epoch": 3.1240188383045524,
      "grad_norm": 3.9024057388305664,
      "learning_rate": 4.8047488226059654e-05,
      "loss": 0.7233,
      "step": 199000
    },
    {
      "epoch": 3.1255886970172684,
      "grad_norm": 4.3390960693359375,
      "learning_rate": 4.804650706436421e-05,
      "loss": 0.6689,
      "step": 199100
    },
    {
      "epoch": 3.1271585557299844,
      "grad_norm": 3.9814975261688232,
      "learning_rate": 4.804552590266876e-05,
      "loss": 0.6836,
      "step": 199200
    },
    {
      "epoch": 3.1287284144427003,
      "grad_norm": 4.189142227172852,
      "learning_rate": 4.8044544740973313e-05,
      "loss": 0.6451,
      "step": 199300
    },
    {
      "epoch": 3.130298273155416,
      "grad_norm": 5.039839267730713,
      "learning_rate": 4.8043563579277864e-05,
      "loss": 0.6887,
      "step": 199400
    },
    {
      "epoch": 3.131868131868132,
      "grad_norm": 3.6917901039123535,
      "learning_rate": 4.804258241758242e-05,
      "loss": 0.6936,
      "step": 199500
    },
    {
      "epoch": 3.133437990580848,
      "grad_norm": 4.028990268707275,
      "learning_rate": 4.804160125588697e-05,
      "loss": 0.6795,
      "step": 199600
    },
    {
      "epoch": 3.1350078492935634,
      "grad_norm": 3.742753505706787,
      "learning_rate": 4.8040620094191524e-05,
      "loss": 0.6743,
      "step": 199700
    },
    {
      "epoch": 3.1365777080062793,
      "grad_norm": 3.610783576965332,
      "learning_rate": 4.8039638932496075e-05,
      "loss": 0.6985,
      "step": 199800
    },
    {
      "epoch": 3.1381475667189953,
      "grad_norm": 4.233772277832031,
      "learning_rate": 4.803865777080063e-05,
      "loss": 0.6733,
      "step": 199900
    },
    {
      "epoch": 3.1397174254317113,
      "grad_norm": 2.945734977722168,
      "learning_rate": 4.8037676609105184e-05,
      "loss": 0.6827,
      "step": 200000
    },
    {
      "epoch": 3.141287284144427,
      "grad_norm": 5.29433536529541,
      "learning_rate": 4.8036695447409735e-05,
      "loss": 0.7433,
      "step": 200100
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 4.121285438537598,
      "learning_rate": 4.803571428571429e-05,
      "loss": 0.6692,
      "step": 200200
    },
    {
      "epoch": 3.1444270015698588,
      "grad_norm": 3.728273630142212,
      "learning_rate": 4.803473312401884e-05,
      "loss": 0.678,
      "step": 200300
    },
    {
      "epoch": 3.1459968602825747,
      "grad_norm": 4.28792667388916,
      "learning_rate": 4.8033751962323395e-05,
      "loss": 0.7181,
      "step": 200400
    },
    {
      "epoch": 3.1475667189952903,
      "grad_norm": 4.240950107574463,
      "learning_rate": 4.8032770800627946e-05,
      "loss": 0.6872,
      "step": 200500
    },
    {
      "epoch": 3.1491365777080063,
      "grad_norm": 1.7603801488876343,
      "learning_rate": 4.80317896389325e-05,
      "loss": 0.7246,
      "step": 200600
    },
    {
      "epoch": 3.1507064364207222,
      "grad_norm": 3.916630744934082,
      "learning_rate": 4.803080847723705e-05,
      "loss": 0.6756,
      "step": 200700
    },
    {
      "epoch": 3.152276295133438,
      "grad_norm": 4.644760608673096,
      "learning_rate": 4.8029827315541605e-05,
      "loss": 0.6958,
      "step": 200800
    },
    {
      "epoch": 3.1538461538461537,
      "grad_norm": 4.041586399078369,
      "learning_rate": 4.8028846153846156e-05,
      "loss": 0.7007,
      "step": 200900
    },
    {
      "epoch": 3.1554160125588697,
      "grad_norm": 3.3801541328430176,
      "learning_rate": 4.802786499215071e-05,
      "loss": 0.6621,
      "step": 201000
    },
    {
      "epoch": 3.1569858712715857,
      "grad_norm": 3.590944766998291,
      "learning_rate": 4.802688383045526e-05,
      "loss": 0.7149,
      "step": 201100
    },
    {
      "epoch": 3.1585557299843012,
      "grad_norm": 3.5210964679718018,
      "learning_rate": 4.8025902668759816e-05,
      "loss": 0.6693,
      "step": 201200
    },
    {
      "epoch": 3.160125588697017,
      "grad_norm": 5.1150803565979,
      "learning_rate": 4.802492150706437e-05,
      "loss": 0.6833,
      "step": 201300
    },
    {
      "epoch": 3.161695447409733,
      "grad_norm": 4.974294185638428,
      "learning_rate": 4.802394034536892e-05,
      "loss": 0.7117,
      "step": 201400
    },
    {
      "epoch": 3.163265306122449,
      "grad_norm": 3.479217290878296,
      "learning_rate": 4.802295918367347e-05,
      "loss": 0.7112,
      "step": 201500
    },
    {
      "epoch": 3.1648351648351647,
      "grad_norm": 3.958580493927002,
      "learning_rate": 4.802197802197803e-05,
      "loss": 0.6884,
      "step": 201600
    },
    {
      "epoch": 3.1664050235478807,
      "grad_norm": 2.168565034866333,
      "learning_rate": 4.802099686028258e-05,
      "loss": 0.6827,
      "step": 201700
    },
    {
      "epoch": 3.1679748822605966,
      "grad_norm": 4.35714864730835,
      "learning_rate": 4.802001569858713e-05,
      "loss": 0.6781,
      "step": 201800
    },
    {
      "epoch": 3.169544740973312,
      "grad_norm": 3.5007264614105225,
      "learning_rate": 4.801903453689168e-05,
      "loss": 0.6946,
      "step": 201900
    },
    {
      "epoch": 3.171114599686028,
      "grad_norm": 3.506655693054199,
      "learning_rate": 4.801805337519624e-05,
      "loss": 0.6827,
      "step": 202000
    },
    {
      "epoch": 3.172684458398744,
      "grad_norm": 4.814579486846924,
      "learning_rate": 4.801707221350079e-05,
      "loss": 0.7191,
      "step": 202100
    },
    {
      "epoch": 3.17425431711146,
      "grad_norm": 4.352530479431152,
      "learning_rate": 4.801609105180534e-05,
      "loss": 0.6564,
      "step": 202200
    },
    {
      "epoch": 3.1758241758241756,
      "grad_norm": 4.045085906982422,
      "learning_rate": 4.80151098901099e-05,
      "loss": 0.6838,
      "step": 202300
    },
    {
      "epoch": 3.1773940345368916,
      "grad_norm": 4.142557621002197,
      "learning_rate": 4.801412872841444e-05,
      "loss": 0.6861,
      "step": 202400
    },
    {
      "epoch": 3.1789638932496076,
      "grad_norm": 4.297815799713135,
      "learning_rate": 4.8013147566719e-05,
      "loss": 0.7215,
      "step": 202500
    },
    {
      "epoch": 3.1805337519623236,
      "grad_norm": 4.233547687530518,
      "learning_rate": 4.801216640502355e-05,
      "loss": 0.6834,
      "step": 202600
    },
    {
      "epoch": 3.182103610675039,
      "grad_norm": 3.703941822052002,
      "learning_rate": 4.801118524332811e-05,
      "loss": 0.7552,
      "step": 202700
    },
    {
      "epoch": 3.183673469387755,
      "grad_norm": 4.448319435119629,
      "learning_rate": 4.801020408163265e-05,
      "loss": 0.7093,
      "step": 202800
    },
    {
      "epoch": 3.185243328100471,
      "grad_norm": 3.0585689544677734,
      "learning_rate": 4.800922291993721e-05,
      "loss": 0.6855,
      "step": 202900
    },
    {
      "epoch": 3.186813186813187,
      "grad_norm": 5.047140121459961,
      "learning_rate": 4.800824175824176e-05,
      "loss": 0.7288,
      "step": 203000
    },
    {
      "epoch": 3.1883830455259026,
      "grad_norm": 4.426783561706543,
      "learning_rate": 4.800726059654631e-05,
      "loss": 0.6942,
      "step": 203100
    },
    {
      "epoch": 3.1899529042386185,
      "grad_norm": 4.042941093444824,
      "learning_rate": 4.800627943485086e-05,
      "loss": 0.6615,
      "step": 203200
    },
    {
      "epoch": 3.1915227629513345,
      "grad_norm": 3.144592761993408,
      "learning_rate": 4.800529827315542e-05,
      "loss": 0.7257,
      "step": 203300
    },
    {
      "epoch": 3.19309262166405,
      "grad_norm": 4.587233543395996,
      "learning_rate": 4.800431711145997e-05,
      "loss": 0.6901,
      "step": 203400
    },
    {
      "epoch": 3.194662480376766,
      "grad_norm": 3.180276870727539,
      "learning_rate": 4.800333594976452e-05,
      "loss": 0.7006,
      "step": 203500
    },
    {
      "epoch": 3.196232339089482,
      "grad_norm": 4.277712821960449,
      "learning_rate": 4.8002354788069073e-05,
      "loss": 0.7052,
      "step": 203600
    },
    {
      "epoch": 3.197802197802198,
      "grad_norm": 3.4610350131988525,
      "learning_rate": 4.800137362637363e-05,
      "loss": 0.694,
      "step": 203700
    },
    {
      "epoch": 3.1993720565149135,
      "grad_norm": 4.866978645324707,
      "learning_rate": 4.800039246467818e-05,
      "loss": 0.7201,
      "step": 203800
    },
    {
      "epoch": 3.2009419152276295,
      "grad_norm": 4.112037181854248,
      "learning_rate": 4.799941130298273e-05,
      "loss": 0.6579,
      "step": 203900
    },
    {
      "epoch": 3.2025117739403455,
      "grad_norm": 3.9884297847747803,
      "learning_rate": 4.7998430141287284e-05,
      "loss": 0.6672,
      "step": 204000
    },
    {
      "epoch": 3.204081632653061,
      "grad_norm": 3.899305582046509,
      "learning_rate": 4.799744897959184e-05,
      "loss": 0.6582,
      "step": 204100
    },
    {
      "epoch": 3.205651491365777,
      "grad_norm": 4.406505584716797,
      "learning_rate": 4.799646781789639e-05,
      "loss": 0.6617,
      "step": 204200
    },
    {
      "epoch": 3.207221350078493,
      "grad_norm": 3.3862626552581787,
      "learning_rate": 4.7995486656200944e-05,
      "loss": 0.6739,
      "step": 204300
    },
    {
      "epoch": 3.208791208791209,
      "grad_norm": 3.3353068828582764,
      "learning_rate": 4.79945054945055e-05,
      "loss": 0.6757,
      "step": 204400
    },
    {
      "epoch": 3.2103610675039245,
      "grad_norm": 4.722851753234863,
      "learning_rate": 4.7993524332810046e-05,
      "loss": 0.6944,
      "step": 204500
    },
    {
      "epoch": 3.2119309262166404,
      "grad_norm": 4.014890193939209,
      "learning_rate": 4.7992543171114604e-05,
      "loss": 0.6906,
      "step": 204600
    },
    {
      "epoch": 3.2135007849293564,
      "grad_norm": 4.624723434448242,
      "learning_rate": 4.7991562009419155e-05,
      "loss": 0.7162,
      "step": 204700
    },
    {
      "epoch": 3.2150706436420724,
      "grad_norm": 4.185469150543213,
      "learning_rate": 4.799058084772371e-05,
      "loss": 0.6533,
      "step": 204800
    },
    {
      "epoch": 3.216640502354788,
      "grad_norm": 4.550415515899658,
      "learning_rate": 4.7989599686028257e-05,
      "loss": 0.6856,
      "step": 204900
    },
    {
      "epoch": 3.218210361067504,
      "grad_norm": 4.314126491546631,
      "learning_rate": 4.7988618524332814e-05,
      "loss": 0.7202,
      "step": 205000
    },
    {
      "epoch": 3.21978021978022,
      "grad_norm": 4.165397644042969,
      "learning_rate": 4.7987637362637365e-05,
      "loss": 0.6755,
      "step": 205100
    },
    {
      "epoch": 3.221350078492936,
      "grad_norm": 3.8716561794281006,
      "learning_rate": 4.7986656200941916e-05,
      "loss": 0.6768,
      "step": 205200
    },
    {
      "epoch": 3.2229199372056514,
      "grad_norm": 3.298008680343628,
      "learning_rate": 4.798567503924647e-05,
      "loss": 0.701,
      "step": 205300
    },
    {
      "epoch": 3.2244897959183674,
      "grad_norm": 2.4203097820281982,
      "learning_rate": 4.7984693877551025e-05,
      "loss": 0.6837,
      "step": 205400
    },
    {
      "epoch": 3.2260596546310834,
      "grad_norm": 4.65772819519043,
      "learning_rate": 4.7983712715855576e-05,
      "loss": 0.7372,
      "step": 205500
    },
    {
      "epoch": 3.227629513343799,
      "grad_norm": 4.161471366882324,
      "learning_rate": 4.798273155416013e-05,
      "loss": 0.7234,
      "step": 205600
    },
    {
      "epoch": 3.229199372056515,
      "grad_norm": 4.539589881896973,
      "learning_rate": 4.798175039246468e-05,
      "loss": 0.7093,
      "step": 205700
    },
    {
      "epoch": 3.230769230769231,
      "grad_norm": 3.7908334732055664,
      "learning_rate": 4.7980769230769236e-05,
      "loss": 0.6636,
      "step": 205800
    },
    {
      "epoch": 3.232339089481947,
      "grad_norm": 3.6337897777557373,
      "learning_rate": 4.797978806907379e-05,
      "loss": 0.7323,
      "step": 205900
    },
    {
      "epoch": 3.2339089481946623,
      "grad_norm": 4.8512187004089355,
      "learning_rate": 4.797880690737834e-05,
      "loss": 0.6851,
      "step": 206000
    },
    {
      "epoch": 3.2354788069073783,
      "grad_norm": 3.5798540115356445,
      "learning_rate": 4.797782574568289e-05,
      "loss": 0.6764,
      "step": 206100
    },
    {
      "epoch": 3.2370486656200943,
      "grad_norm": 4.529168128967285,
      "learning_rate": 4.7976844583987446e-05,
      "loss": 0.6864,
      "step": 206200
    },
    {
      "epoch": 3.23861852433281,
      "grad_norm": 3.407867431640625,
      "learning_rate": 4.7975863422292e-05,
      "loss": 0.6727,
      "step": 206300
    },
    {
      "epoch": 3.240188383045526,
      "grad_norm": 3.8792498111724854,
      "learning_rate": 4.797488226059655e-05,
      "loss": 0.6912,
      "step": 206400
    },
    {
      "epoch": 3.241758241758242,
      "grad_norm": 4.287820339202881,
      "learning_rate": 4.7973901098901106e-05,
      "loss": 0.7242,
      "step": 206500
    },
    {
      "epoch": 3.2433281004709578,
      "grad_norm": 4.516058921813965,
      "learning_rate": 4.797291993720565e-05,
      "loss": 0.6773,
      "step": 206600
    },
    {
      "epoch": 3.2448979591836733,
      "grad_norm": 5.598888397216797,
      "learning_rate": 4.797193877551021e-05,
      "loss": 0.7085,
      "step": 206700
    },
    {
      "epoch": 3.2464678178963893,
      "grad_norm": 3.829332113265991,
      "learning_rate": 4.797095761381476e-05,
      "loss": 0.7217,
      "step": 206800
    },
    {
      "epoch": 3.2480376766091053,
      "grad_norm": 3.6929235458374023,
      "learning_rate": 4.796997645211932e-05,
      "loss": 0.7418,
      "step": 206900
    },
    {
      "epoch": 3.2496075353218212,
      "grad_norm": 3.827141761779785,
      "learning_rate": 4.796899529042386e-05,
      "loss": 0.7054,
      "step": 207000
    },
    {
      "epoch": 3.2511773940345368,
      "grad_norm": 4.752179145812988,
      "learning_rate": 4.796801412872842e-05,
      "loss": 0.6527,
      "step": 207100
    },
    {
      "epoch": 3.2527472527472527,
      "grad_norm": 4.0149407386779785,
      "learning_rate": 4.796703296703297e-05,
      "loss": 0.6662,
      "step": 207200
    },
    {
      "epoch": 3.2543171114599687,
      "grad_norm": 3.7168831825256348,
      "learning_rate": 4.796605180533752e-05,
      "loss": 0.6768,
      "step": 207300
    },
    {
      "epoch": 3.2558869701726847,
      "grad_norm": 4.445194721221924,
      "learning_rate": 4.796507064364207e-05,
      "loss": 0.6823,
      "step": 207400
    },
    {
      "epoch": 3.2574568288854002,
      "grad_norm": 3.8249340057373047,
      "learning_rate": 4.796408948194663e-05,
      "loss": 0.6786,
      "step": 207500
    },
    {
      "epoch": 3.259026687598116,
      "grad_norm": 3.947772264480591,
      "learning_rate": 4.796310832025118e-05,
      "loss": 0.6862,
      "step": 207600
    },
    {
      "epoch": 3.260596546310832,
      "grad_norm": 4.658844947814941,
      "learning_rate": 4.796212715855573e-05,
      "loss": 0.7042,
      "step": 207700
    },
    {
      "epoch": 3.2621664050235477,
      "grad_norm": 4.675922870635986,
      "learning_rate": 4.796114599686028e-05,
      "loss": 0.7274,
      "step": 207800
    },
    {
      "epoch": 3.2637362637362637,
      "grad_norm": 4.683229446411133,
      "learning_rate": 4.796016483516484e-05,
      "loss": 0.6947,
      "step": 207900
    },
    {
      "epoch": 3.2653061224489797,
      "grad_norm": 4.6421637535095215,
      "learning_rate": 4.795918367346939e-05,
      "loss": 0.7024,
      "step": 208000
    },
    {
      "epoch": 3.2668759811616956,
      "grad_norm": 3.9234297275543213,
      "learning_rate": 4.795820251177394e-05,
      "loss": 0.6919,
      "step": 208100
    },
    {
      "epoch": 3.268445839874411,
      "grad_norm": 3.621086835861206,
      "learning_rate": 4.795722135007849e-05,
      "loss": 0.6882,
      "step": 208200
    },
    {
      "epoch": 3.270015698587127,
      "grad_norm": 4.140910625457764,
      "learning_rate": 4.795624018838305e-05,
      "loss": 0.7273,
      "step": 208300
    },
    {
      "epoch": 3.271585557299843,
      "grad_norm": 3.713202476501465,
      "learning_rate": 4.79552590266876e-05,
      "loss": 0.6807,
      "step": 208400
    },
    {
      "epoch": 3.2731554160125587,
      "grad_norm": 4.478989601135254,
      "learning_rate": 4.795427786499215e-05,
      "loss": 0.7195,
      "step": 208500
    },
    {
      "epoch": 3.2747252747252746,
      "grad_norm": 3.440244197845459,
      "learning_rate": 4.795329670329671e-05,
      "loss": 0.7019,
      "step": 208600
    },
    {
      "epoch": 3.2762951334379906,
      "grad_norm": 3.7175703048706055,
      "learning_rate": 4.7952315541601255e-05,
      "loss": 0.7243,
      "step": 208700
    },
    {
      "epoch": 3.2778649921507066,
      "grad_norm": 3.8766286373138428,
      "learning_rate": 4.795133437990581e-05,
      "loss": 0.6636,
      "step": 208800
    },
    {
      "epoch": 3.279434850863422,
      "grad_norm": 3.752645492553711,
      "learning_rate": 4.7950353218210364e-05,
      "loss": 0.7051,
      "step": 208900
    },
    {
      "epoch": 3.281004709576138,
      "grad_norm": 4.268343448638916,
      "learning_rate": 4.794937205651492e-05,
      "loss": 0.6731,
      "step": 209000
    },
    {
      "epoch": 3.282574568288854,
      "grad_norm": 3.1846981048583984,
      "learning_rate": 4.7948390894819466e-05,
      "loss": 0.6893,
      "step": 209100
    },
    {
      "epoch": 3.2841444270015696,
      "grad_norm": 3.3723132610321045,
      "learning_rate": 4.794740973312402e-05,
      "loss": 0.6638,
      "step": 209200
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 5.220042705535889,
      "learning_rate": 4.7946428571428574e-05,
      "loss": 0.6689,
      "step": 209300
    },
    {
      "epoch": 3.2872841444270016,
      "grad_norm": 3.189896583557129,
      "learning_rate": 4.7945447409733125e-05,
      "loss": 0.6857,
      "step": 209400
    },
    {
      "epoch": 3.2888540031397175,
      "grad_norm": 4.353214740753174,
      "learning_rate": 4.7944466248037676e-05,
      "loss": 0.6846,
      "step": 209500
    },
    {
      "epoch": 3.2904238618524335,
      "grad_norm": 4.10971212387085,
      "learning_rate": 4.7943485086342234e-05,
      "loss": 0.7096,
      "step": 209600
    },
    {
      "epoch": 3.291993720565149,
      "grad_norm": 4.271107196807861,
      "learning_rate": 4.7942503924646785e-05,
      "loss": 0.6879,
      "step": 209700
    },
    {
      "epoch": 3.293563579277865,
      "grad_norm": 4.210685729980469,
      "learning_rate": 4.7941522762951336e-05,
      "loss": 0.661,
      "step": 209800
    },
    {
      "epoch": 3.295133437990581,
      "grad_norm": 3.923715353012085,
      "learning_rate": 4.794054160125589e-05,
      "loss": 0.7078,
      "step": 209900
    },
    {
      "epoch": 3.2967032967032965,
      "grad_norm": 4.020313739776611,
      "learning_rate": 4.7939560439560445e-05,
      "loss": 0.7165,
      "step": 210000
    },
    {
      "epoch": 3.2982731554160125,
      "grad_norm": 4.225745677947998,
      "learning_rate": 4.793857927786499e-05,
      "loss": 0.6874,
      "step": 210100
    },
    {
      "epoch": 3.2998430141287285,
      "grad_norm": 4.681971073150635,
      "learning_rate": 4.793759811616955e-05,
      "loss": 0.7163,
      "step": 210200
    },
    {
      "epoch": 3.3014128728414445,
      "grad_norm": 3.437471389770508,
      "learning_rate": 4.79366169544741e-05,
      "loss": 0.7276,
      "step": 210300
    },
    {
      "epoch": 3.30298273155416,
      "grad_norm": 4.211538791656494,
      "learning_rate": 4.7935635792778655e-05,
      "loss": 0.7093,
      "step": 210400
    },
    {
      "epoch": 3.304552590266876,
      "grad_norm": 5.091557502746582,
      "learning_rate": 4.7934654631083206e-05,
      "loss": 0.7048,
      "step": 210500
    },
    {
      "epoch": 3.306122448979592,
      "grad_norm": 5.010733127593994,
      "learning_rate": 4.793367346938776e-05,
      "loss": 0.6884,
      "step": 210600
    },
    {
      "epoch": 3.3076923076923075,
      "grad_norm": 4.32296085357666,
      "learning_rate": 4.7932692307692315e-05,
      "loss": 0.6637,
      "step": 210700
    },
    {
      "epoch": 3.3092621664050235,
      "grad_norm": 4.515056133270264,
      "learning_rate": 4.793171114599686e-05,
      "loss": 0.7145,
      "step": 210800
    },
    {
      "epoch": 3.3108320251177394,
      "grad_norm": 5.155807971954346,
      "learning_rate": 4.793072998430142e-05,
      "loss": 0.6936,
      "step": 210900
    },
    {
      "epoch": 3.3124018838304554,
      "grad_norm": 4.6141767501831055,
      "learning_rate": 4.792974882260597e-05,
      "loss": 0.6592,
      "step": 211000
    },
    {
      "epoch": 3.313971742543171,
      "grad_norm": 4.496230602264404,
      "learning_rate": 4.7928767660910526e-05,
      "loss": 0.6669,
      "step": 211100
    },
    {
      "epoch": 3.315541601255887,
      "grad_norm": 4.348483085632324,
      "learning_rate": 4.792778649921507e-05,
      "loss": 0.6477,
      "step": 211200
    },
    {
      "epoch": 3.317111459968603,
      "grad_norm": 5.601354122161865,
      "learning_rate": 4.792680533751963e-05,
      "loss": 0.685,
      "step": 211300
    },
    {
      "epoch": 3.3186813186813184,
      "grad_norm": 4.245335102081299,
      "learning_rate": 4.792582417582418e-05,
      "loss": 0.6756,
      "step": 211400
    },
    {
      "epoch": 3.3202511773940344,
      "grad_norm": 4.281673431396484,
      "learning_rate": 4.792484301412873e-05,
      "loss": 0.7156,
      "step": 211500
    },
    {
      "epoch": 3.3218210361067504,
      "grad_norm": 4.328022480010986,
      "learning_rate": 4.792386185243328e-05,
      "loss": 0.7177,
      "step": 211600
    },
    {
      "epoch": 3.3233908948194664,
      "grad_norm": 4.581658840179443,
      "learning_rate": 4.792288069073784e-05,
      "loss": 0.7127,
      "step": 211700
    },
    {
      "epoch": 3.3249607535321823,
      "grad_norm": 4.764644622802734,
      "learning_rate": 4.792189952904239e-05,
      "loss": 0.6722,
      "step": 211800
    },
    {
      "epoch": 3.326530612244898,
      "grad_norm": 4.687392234802246,
      "learning_rate": 4.792091836734694e-05,
      "loss": 0.6658,
      "step": 211900
    },
    {
      "epoch": 3.328100470957614,
      "grad_norm": 3.6554338932037354,
      "learning_rate": 4.791993720565149e-05,
      "loss": 0.6978,
      "step": 212000
    },
    {
      "epoch": 3.32967032967033,
      "grad_norm": 3.8632256984710693,
      "learning_rate": 4.791895604395605e-05,
      "loss": 0.6907,
      "step": 212100
    },
    {
      "epoch": 3.3312401883830454,
      "grad_norm": 5.13900089263916,
      "learning_rate": 4.791797488226059e-05,
      "loss": 0.6785,
      "step": 212200
    },
    {
      "epoch": 3.3328100470957613,
      "grad_norm": 3.2365853786468506,
      "learning_rate": 4.791699372056515e-05,
      "loss": 0.6813,
      "step": 212300
    },
    {
      "epoch": 3.3343799058084773,
      "grad_norm": 2.7702016830444336,
      "learning_rate": 4.79160125588697e-05,
      "loss": 0.7148,
      "step": 212400
    },
    {
      "epoch": 3.3359497645211933,
      "grad_norm": 4.037420749664307,
      "learning_rate": 4.791503139717426e-05,
      "loss": 0.69,
      "step": 212500
    },
    {
      "epoch": 3.337519623233909,
      "grad_norm": 3.3965201377868652,
      "learning_rate": 4.791405023547881e-05,
      "loss": 0.7144,
      "step": 212600
    },
    {
      "epoch": 3.339089481946625,
      "grad_norm": 4.543107509613037,
      "learning_rate": 4.791306907378336e-05,
      "loss": 0.7132,
      "step": 212700
    },
    {
      "epoch": 3.340659340659341,
      "grad_norm": 5.4719462394714355,
      "learning_rate": 4.791208791208792e-05,
      "loss": 0.7072,
      "step": 212800
    },
    {
      "epoch": 3.3422291993720563,
      "grad_norm": 3.376004219055176,
      "learning_rate": 4.7911106750392464e-05,
      "loss": 0.6842,
      "step": 212900
    },
    {
      "epoch": 3.3437990580847723,
      "grad_norm": 3.1474273204803467,
      "learning_rate": 4.791012558869702e-05,
      "loss": 0.6728,
      "step": 213000
    },
    {
      "epoch": 3.3453689167974883,
      "grad_norm": 4.199090003967285,
      "learning_rate": 4.790914442700157e-05,
      "loss": 0.6841,
      "step": 213100
    },
    {
      "epoch": 3.3469387755102042,
      "grad_norm": 4.764530658721924,
      "learning_rate": 4.790816326530613e-05,
      "loss": 0.7355,
      "step": 213200
    },
    {
      "epoch": 3.3485086342229198,
      "grad_norm": 3.754364252090454,
      "learning_rate": 4.7907182103610674e-05,
      "loss": 0.7061,
      "step": 213300
    },
    {
      "epoch": 3.3500784929356358,
      "grad_norm": 4.947940826416016,
      "learning_rate": 4.790620094191523e-05,
      "loss": 0.6885,
      "step": 213400
    },
    {
      "epoch": 3.3516483516483517,
      "grad_norm": 4.064205169677734,
      "learning_rate": 4.790521978021978e-05,
      "loss": 0.7233,
      "step": 213500
    },
    {
      "epoch": 3.3532182103610673,
      "grad_norm": 4.785403728485107,
      "learning_rate": 4.7904238618524334e-05,
      "loss": 0.6538,
      "step": 213600
    },
    {
      "epoch": 3.3547880690737832,
      "grad_norm": 3.754124641418457,
      "learning_rate": 4.7903257456828885e-05,
      "loss": 0.6499,
      "step": 213700
    },
    {
      "epoch": 3.356357927786499,
      "grad_norm": 4.371249198913574,
      "learning_rate": 4.790227629513344e-05,
      "loss": 0.7351,
      "step": 213800
    },
    {
      "epoch": 3.357927786499215,
      "grad_norm": 3.9318368434906006,
      "learning_rate": 4.7901295133437994e-05,
      "loss": 0.6859,
      "step": 213900
    },
    {
      "epoch": 3.359497645211931,
      "grad_norm": 3.1976985931396484,
      "learning_rate": 4.7900313971742545e-05,
      "loss": 0.7111,
      "step": 214000
    },
    {
      "epoch": 3.3610675039246467,
      "grad_norm": 4.110292434692383,
      "learning_rate": 4.7899332810047096e-05,
      "loss": 0.697,
      "step": 214100
    },
    {
      "epoch": 3.3626373626373627,
      "grad_norm": 3.89520263671875,
      "learning_rate": 4.7898351648351654e-05,
      "loss": 0.7149,
      "step": 214200
    },
    {
      "epoch": 3.3642072213500787,
      "grad_norm": 3.2158169746398926,
      "learning_rate": 4.78973704866562e-05,
      "loss": 0.7096,
      "step": 214300
    },
    {
      "epoch": 3.365777080062794,
      "grad_norm": 4.737054347991943,
      "learning_rate": 4.7896389324960756e-05,
      "loss": 0.7091,
      "step": 214400
    },
    {
      "epoch": 3.36734693877551,
      "grad_norm": 4.150075435638428,
      "learning_rate": 4.7895408163265307e-05,
      "loss": 0.698,
      "step": 214500
    },
    {
      "epoch": 3.368916797488226,
      "grad_norm": 3.9457950592041016,
      "learning_rate": 4.7894427001569864e-05,
      "loss": 0.7064,
      "step": 214600
    },
    {
      "epoch": 3.370486656200942,
      "grad_norm": 3.36194109916687,
      "learning_rate": 4.7893445839874415e-05,
      "loss": 0.6415,
      "step": 214700
    },
    {
      "epoch": 3.3720565149136577,
      "grad_norm": 4.351467609405518,
      "learning_rate": 4.7892464678178966e-05,
      "loss": 0.6149,
      "step": 214800
    },
    {
      "epoch": 3.3736263736263736,
      "grad_norm": 2.8573358058929443,
      "learning_rate": 4.7891483516483524e-05,
      "loss": 0.7263,
      "step": 214900
    },
    {
      "epoch": 3.3751962323390896,
      "grad_norm": 3.2432382106781006,
      "learning_rate": 4.789050235478807e-05,
      "loss": 0.6951,
      "step": 215000
    },
    {
      "epoch": 3.376766091051805,
      "grad_norm": 3.7690887451171875,
      "learning_rate": 4.7889521193092626e-05,
      "loss": 0.7165,
      "step": 215100
    },
    {
      "epoch": 3.378335949764521,
      "grad_norm": 4.808919429779053,
      "learning_rate": 4.788854003139718e-05,
      "loss": 0.7447,
      "step": 215200
    },
    {
      "epoch": 3.379905808477237,
      "grad_norm": 4.2885966300964355,
      "learning_rate": 4.7887558869701735e-05,
      "loss": 0.7066,
      "step": 215300
    },
    {
      "epoch": 3.381475667189953,
      "grad_norm": 3.9289674758911133,
      "learning_rate": 4.788657770800628e-05,
      "loss": 0.6429,
      "step": 215400
    },
    {
      "epoch": 3.3830455259026686,
      "grad_norm": 3.097151517868042,
      "learning_rate": 4.788559654631084e-05,
      "loss": 0.7303,
      "step": 215500
    },
    {
      "epoch": 3.3846153846153846,
      "grad_norm": 4.196166038513184,
      "learning_rate": 4.788461538461539e-05,
      "loss": 0.7164,
      "step": 215600
    },
    {
      "epoch": 3.3861852433281006,
      "grad_norm": 3.04011869430542,
      "learning_rate": 4.788363422291994e-05,
      "loss": 0.7128,
      "step": 215700
    },
    {
      "epoch": 3.387755102040816,
      "grad_norm": 3.5771305561065674,
      "learning_rate": 4.788265306122449e-05,
      "loss": 0.7247,
      "step": 215800
    },
    {
      "epoch": 3.389324960753532,
      "grad_norm": 4.702757835388184,
      "learning_rate": 4.788167189952905e-05,
      "loss": 0.7061,
      "step": 215900
    },
    {
      "epoch": 3.390894819466248,
      "grad_norm": 3.7025606632232666,
      "learning_rate": 4.78806907378336e-05,
      "loss": 0.7289,
      "step": 216000
    },
    {
      "epoch": 3.392464678178964,
      "grad_norm": 4.367710590362549,
      "learning_rate": 4.787970957613815e-05,
      "loss": 0.6685,
      "step": 216100
    },
    {
      "epoch": 3.39403453689168,
      "grad_norm": 3.556223154067993,
      "learning_rate": 4.78787284144427e-05,
      "loss": 0.6271,
      "step": 216200
    },
    {
      "epoch": 3.3956043956043955,
      "grad_norm": 4.174251079559326,
      "learning_rate": 4.787774725274726e-05,
      "loss": 0.7309,
      "step": 216300
    },
    {
      "epoch": 3.3971742543171115,
      "grad_norm": 4.420127868652344,
      "learning_rate": 4.78767660910518e-05,
      "loss": 0.6452,
      "step": 216400
    },
    {
      "epoch": 3.3987441130298275,
      "grad_norm": 3.642625570297241,
      "learning_rate": 4.787578492935636e-05,
      "loss": 0.6877,
      "step": 216500
    },
    {
      "epoch": 3.400313971742543,
      "grad_norm": 5.428676128387451,
      "learning_rate": 4.787480376766091e-05,
      "loss": 0.6714,
      "step": 216600
    },
    {
      "epoch": 3.401883830455259,
      "grad_norm": 5.189693927764893,
      "learning_rate": 4.787382260596546e-05,
      "loss": 0.714,
      "step": 216700
    },
    {
      "epoch": 3.403453689167975,
      "grad_norm": 3.232487440109253,
      "learning_rate": 4.787284144427002e-05,
      "loss": 0.6583,
      "step": 216800
    },
    {
      "epoch": 3.405023547880691,
      "grad_norm": 3.3971803188323975,
      "learning_rate": 4.787186028257457e-05,
      "loss": 0.6893,
      "step": 216900
    },
    {
      "epoch": 3.4065934065934065,
      "grad_norm": 3.3735454082489014,
      "learning_rate": 4.787087912087913e-05,
      "loss": 0.6808,
      "step": 217000
    },
    {
      "epoch": 3.4081632653061225,
      "grad_norm": 3.4411613941192627,
      "learning_rate": 4.786989795918367e-05,
      "loss": 0.691,
      "step": 217100
    },
    {
      "epoch": 3.4097331240188384,
      "grad_norm": 3.45717453956604,
      "learning_rate": 4.786891679748823e-05,
      "loss": 0.7252,
      "step": 217200
    },
    {
      "epoch": 3.411302982731554,
      "grad_norm": 4.157934188842773,
      "learning_rate": 4.786793563579278e-05,
      "loss": 0.6548,
      "step": 217300
    },
    {
      "epoch": 3.41287284144427,
      "grad_norm": 3.795490264892578,
      "learning_rate": 4.786695447409733e-05,
      "loss": 0.6885,
      "step": 217400
    },
    {
      "epoch": 3.414442700156986,
      "grad_norm": 3.751377820968628,
      "learning_rate": 4.7865973312401883e-05,
      "loss": 0.6683,
      "step": 217500
    },
    {
      "epoch": 3.416012558869702,
      "grad_norm": 4.472237586975098,
      "learning_rate": 4.786499215070644e-05,
      "loss": 0.6281,
      "step": 217600
    },
    {
      "epoch": 3.4175824175824174,
      "grad_norm": 4.188502311706543,
      "learning_rate": 4.786401098901099e-05,
      "loss": 0.6671,
      "step": 217700
    },
    {
      "epoch": 3.4191522762951334,
      "grad_norm": 4.575891971588135,
      "learning_rate": 4.786302982731554e-05,
      "loss": 0.6993,
      "step": 217800
    },
    {
      "epoch": 3.4207221350078494,
      "grad_norm": 3.7932653427124023,
      "learning_rate": 4.7862048665620094e-05,
      "loss": 0.7152,
      "step": 217900
    },
    {
      "epoch": 3.422291993720565,
      "grad_norm": 3.166539430618286,
      "learning_rate": 4.786106750392465e-05,
      "loss": 0.6952,
      "step": 218000
    },
    {
      "epoch": 3.423861852433281,
      "grad_norm": 3.8792901039123535,
      "learning_rate": 4.7860086342229196e-05,
      "loss": 0.7045,
      "step": 218100
    },
    {
      "epoch": 3.425431711145997,
      "grad_norm": 4.04907751083374,
      "learning_rate": 4.7859105180533754e-05,
      "loss": 0.6597,
      "step": 218200
    },
    {
      "epoch": 3.427001569858713,
      "grad_norm": 4.456523895263672,
      "learning_rate": 4.7858124018838305e-05,
      "loss": 0.7004,
      "step": 218300
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 3.1542158126831055,
      "learning_rate": 4.785714285714286e-05,
      "loss": 0.6995,
      "step": 218400
    },
    {
      "epoch": 3.4301412872841444,
      "grad_norm": 4.3637566566467285,
      "learning_rate": 4.785616169544741e-05,
      "loss": 0.6709,
      "step": 218500
    },
    {
      "epoch": 3.4317111459968603,
      "grad_norm": 5.240835189819336,
      "learning_rate": 4.7855180533751965e-05,
      "loss": 0.6862,
      "step": 218600
    },
    {
      "epoch": 3.4332810047095763,
      "grad_norm": 4.625594615936279,
      "learning_rate": 4.7854199372056516e-05,
      "loss": 0.6803,
      "step": 218700
    },
    {
      "epoch": 3.434850863422292,
      "grad_norm": 4.581073760986328,
      "learning_rate": 4.7853218210361067e-05,
      "loss": 0.6761,
      "step": 218800
    },
    {
      "epoch": 3.436420722135008,
      "grad_norm": 4.826159954071045,
      "learning_rate": 4.7852237048665624e-05,
      "loss": 0.6726,
      "step": 218900
    },
    {
      "epoch": 3.437990580847724,
      "grad_norm": 3.513436794281006,
      "learning_rate": 4.7851255886970175e-05,
      "loss": 0.7429,
      "step": 219000
    },
    {
      "epoch": 3.4395604395604398,
      "grad_norm": 3.750181198120117,
      "learning_rate": 4.7850274725274726e-05,
      "loss": 0.6928,
      "step": 219100
    },
    {
      "epoch": 3.4411302982731553,
      "grad_norm": 4.87345027923584,
      "learning_rate": 4.784929356357928e-05,
      "loss": 0.6901,
      "step": 219200
    },
    {
      "epoch": 3.4427001569858713,
      "grad_norm": 3.2250845432281494,
      "learning_rate": 4.7848312401883835e-05,
      "loss": 0.725,
      "step": 219300
    },
    {
      "epoch": 3.4442700156985873,
      "grad_norm": 4.136922359466553,
      "learning_rate": 4.7847331240188386e-05,
      "loss": 0.7143,
      "step": 219400
    },
    {
      "epoch": 3.445839874411303,
      "grad_norm": 3.780350923538208,
      "learning_rate": 4.784635007849294e-05,
      "loss": 0.7209,
      "step": 219500
    },
    {
      "epoch": 3.4474097331240188,
      "grad_norm": 3.39133882522583,
      "learning_rate": 4.784536891679749e-05,
      "loss": 0.6522,
      "step": 219600
    },
    {
      "epoch": 3.4489795918367347,
      "grad_norm": 4.381126880645752,
      "learning_rate": 4.7844387755102046e-05,
      "loss": 0.6763,
      "step": 219700
    },
    {
      "epoch": 3.4505494505494507,
      "grad_norm": 3.650371789932251,
      "learning_rate": 4.78434065934066e-05,
      "loss": 0.6732,
      "step": 219800
    },
    {
      "epoch": 3.4521193092621663,
      "grad_norm": 3.326218843460083,
      "learning_rate": 4.784242543171115e-05,
      "loss": 0.6802,
      "step": 219900
    },
    {
      "epoch": 3.4536891679748822,
      "grad_norm": 3.868427276611328,
      "learning_rate": 4.78414442700157e-05,
      "loss": 0.6873,
      "step": 220000
    },
    {
      "epoch": 3.455259026687598,
      "grad_norm": 4.239687442779541,
      "learning_rate": 4.7840463108320256e-05,
      "loss": 0.7201,
      "step": 220100
    },
    {
      "epoch": 3.4568288854003137,
      "grad_norm": 4.317376136779785,
      "learning_rate": 4.78394819466248e-05,
      "loss": 0.7038,
      "step": 220200
    },
    {
      "epoch": 3.4583987441130297,
      "grad_norm": 3.7604024410247803,
      "learning_rate": 4.783850078492936e-05,
      "loss": 0.6727,
      "step": 220300
    },
    {
      "epoch": 3.4599686028257457,
      "grad_norm": 3.353102684020996,
      "learning_rate": 4.783751962323391e-05,
      "loss": 0.6855,
      "step": 220400
    },
    {
      "epoch": 3.4615384615384617,
      "grad_norm": 3.5242061614990234,
      "learning_rate": 4.783653846153847e-05,
      "loss": 0.7129,
      "step": 220500
    },
    {
      "epoch": 3.463108320251177,
      "grad_norm": 4.2592620849609375,
      "learning_rate": 4.783555729984301e-05,
      "loss": 0.7084,
      "step": 220600
    },
    {
      "epoch": 3.464678178963893,
      "grad_norm": 4.1770710945129395,
      "learning_rate": 4.783457613814757e-05,
      "loss": 0.7151,
      "step": 220700
    },
    {
      "epoch": 3.466248037676609,
      "grad_norm": 4.07164192199707,
      "learning_rate": 4.783359497645212e-05,
      "loss": 0.7071,
      "step": 220800
    },
    {
      "epoch": 3.467817896389325,
      "grad_norm": 4.1988701820373535,
      "learning_rate": 4.783261381475667e-05,
      "loss": 0.675,
      "step": 220900
    },
    {
      "epoch": 3.4693877551020407,
      "grad_norm": 3.699130058288574,
      "learning_rate": 4.783163265306123e-05,
      "loss": 0.7108,
      "step": 221000
    },
    {
      "epoch": 3.4709576138147566,
      "grad_norm": 3.7885079383850098,
      "learning_rate": 4.783065149136578e-05,
      "loss": 0.7283,
      "step": 221100
    },
    {
      "epoch": 3.4725274725274726,
      "grad_norm": 3.844547986984253,
      "learning_rate": 4.782967032967033e-05,
      "loss": 0.6808,
      "step": 221200
    },
    {
      "epoch": 3.4740973312401886,
      "grad_norm": 2.4538497924804688,
      "learning_rate": 4.782868916797488e-05,
      "loss": 0.6753,
      "step": 221300
    },
    {
      "epoch": 3.475667189952904,
      "grad_norm": 3.0913333892822266,
      "learning_rate": 4.782770800627944e-05,
      "loss": 0.695,
      "step": 221400
    },
    {
      "epoch": 3.47723704866562,
      "grad_norm": 3.6291677951812744,
      "learning_rate": 4.782672684458399e-05,
      "loss": 0.699,
      "step": 221500
    },
    {
      "epoch": 3.478806907378336,
      "grad_norm": 4.483377933502197,
      "learning_rate": 4.782574568288854e-05,
      "loss": 0.6893,
      "step": 221600
    },
    {
      "epoch": 3.4803767660910516,
      "grad_norm": 5.081173896789551,
      "learning_rate": 4.782476452119309e-05,
      "loss": 0.713,
      "step": 221700
    },
    {
      "epoch": 3.4819466248037676,
      "grad_norm": 4.147609710693359,
      "learning_rate": 4.782378335949765e-05,
      "loss": 0.682,
      "step": 221800
    },
    {
      "epoch": 3.4835164835164836,
      "grad_norm": 3.539647102355957,
      "learning_rate": 4.78228021978022e-05,
      "loss": 0.6691,
      "step": 221900
    },
    {
      "epoch": 3.4850863422291996,
      "grad_norm": 4.393199920654297,
      "learning_rate": 4.782182103610675e-05,
      "loss": 0.6852,
      "step": 222000
    },
    {
      "epoch": 3.486656200941915,
      "grad_norm": 3.583676338195801,
      "learning_rate": 4.78208398744113e-05,
      "loss": 0.7076,
      "step": 222100
    },
    {
      "epoch": 3.488226059654631,
      "grad_norm": 4.680447578430176,
      "learning_rate": 4.781985871271586e-05,
      "loss": 0.7011,
      "step": 222200
    },
    {
      "epoch": 3.489795918367347,
      "grad_norm": 3.827868700027466,
      "learning_rate": 4.7818877551020405e-05,
      "loss": 0.7378,
      "step": 222300
    },
    {
      "epoch": 3.4913657770800626,
      "grad_norm": 4.2758965492248535,
      "learning_rate": 4.781789638932496e-05,
      "loss": 0.6846,
      "step": 222400
    },
    {
      "epoch": 3.4929356357927785,
      "grad_norm": 4.014399528503418,
      "learning_rate": 4.7816915227629514e-05,
      "loss": 0.7046,
      "step": 222500
    },
    {
      "epoch": 3.4945054945054945,
      "grad_norm": 4.686532974243164,
      "learning_rate": 4.781593406593407e-05,
      "loss": 0.6516,
      "step": 222600
    },
    {
      "epoch": 3.4960753532182105,
      "grad_norm": 4.494096279144287,
      "learning_rate": 4.7814952904238616e-05,
      "loss": 0.6939,
      "step": 222700
    },
    {
      "epoch": 3.497645211930926,
      "grad_norm": 4.035592555999756,
      "learning_rate": 4.7813971742543174e-05,
      "loss": 0.6798,
      "step": 222800
    },
    {
      "epoch": 3.499215070643642,
      "grad_norm": 4.727065086364746,
      "learning_rate": 4.7812990580847725e-05,
      "loss": 0.7183,
      "step": 222900
    },
    {
      "epoch": 3.500784929356358,
      "grad_norm": 3.82006573677063,
      "learning_rate": 4.7812009419152276e-05,
      "loss": 0.7034,
      "step": 223000
    },
    {
      "epoch": 3.5023547880690735,
      "grad_norm": 4.3005523681640625,
      "learning_rate": 4.781102825745683e-05,
      "loss": 0.6604,
      "step": 223100
    },
    {
      "epoch": 3.5039246467817895,
      "grad_norm": 5.038918495178223,
      "learning_rate": 4.7810047095761384e-05,
      "loss": 0.6404,
      "step": 223200
    },
    {
      "epoch": 3.5054945054945055,
      "grad_norm": 3.178699493408203,
      "learning_rate": 4.7809065934065935e-05,
      "loss": 0.7197,
      "step": 223300
    },
    {
      "epoch": 3.5070643642072215,
      "grad_norm": 4.969612121582031,
      "learning_rate": 4.7808084772370486e-05,
      "loss": 0.7406,
      "step": 223400
    },
    {
      "epoch": 3.5086342229199374,
      "grad_norm": 3.1650569438934326,
      "learning_rate": 4.7807103610675044e-05,
      "loss": 0.6993,
      "step": 223500
    },
    {
      "epoch": 3.510204081632653,
      "grad_norm": 3.771756410598755,
      "learning_rate": 4.7806122448979595e-05,
      "loss": 0.6768,
      "step": 223600
    },
    {
      "epoch": 3.511773940345369,
      "grad_norm": 5.390294075012207,
      "learning_rate": 4.7805141287284146e-05,
      "loss": 0.7157,
      "step": 223700
    },
    {
      "epoch": 3.513343799058085,
      "grad_norm": 4.750877380371094,
      "learning_rate": 4.78041601255887e-05,
      "loss": 0.6989,
      "step": 223800
    },
    {
      "epoch": 3.5149136577708004,
      "grad_norm": 5.250213623046875,
      "learning_rate": 4.7803178963893255e-05,
      "loss": 0.7198,
      "step": 223900
    },
    {
      "epoch": 3.5164835164835164,
      "grad_norm": 3.5712902545928955,
      "learning_rate": 4.7802197802197806e-05,
      "loss": 0.6884,
      "step": 224000
    },
    {
      "epoch": 3.5180533751962324,
      "grad_norm": 4.280002593994141,
      "learning_rate": 4.780121664050236e-05,
      "loss": 0.6971,
      "step": 224100
    },
    {
      "epoch": 3.5196232339089484,
      "grad_norm": 3.6416797637939453,
      "learning_rate": 4.780023547880691e-05,
      "loss": 0.6588,
      "step": 224200
    },
    {
      "epoch": 3.521193092621664,
      "grad_norm": 3.098189115524292,
      "learning_rate": 4.7799254317111465e-05,
      "loss": 0.6795,
      "step": 224300
    },
    {
      "epoch": 3.52276295133438,
      "grad_norm": 3.952065944671631,
      "learning_rate": 4.779827315541601e-05,
      "loss": 0.6909,
      "step": 224400
    },
    {
      "epoch": 3.524332810047096,
      "grad_norm": 3.67317271232605,
      "learning_rate": 4.779729199372057e-05,
      "loss": 0.6958,
      "step": 224500
    },
    {
      "epoch": 3.5259026687598114,
      "grad_norm": 4.301740646362305,
      "learning_rate": 4.779631083202512e-05,
      "loss": 0.6898,
      "step": 224600
    },
    {
      "epoch": 3.5274725274725274,
      "grad_norm": 3.5784852504730225,
      "learning_rate": 4.7795329670329676e-05,
      "loss": 0.689,
      "step": 224700
    },
    {
      "epoch": 3.5290423861852434,
      "grad_norm": 4.056456565856934,
      "learning_rate": 4.779434850863422e-05,
      "loss": 0.6916,
      "step": 224800
    },
    {
      "epoch": 3.5306122448979593,
      "grad_norm": 4.657114505767822,
      "learning_rate": 4.779336734693878e-05,
      "loss": 0.718,
      "step": 224900
    },
    {
      "epoch": 3.5321821036106753,
      "grad_norm": 4.878819465637207,
      "learning_rate": 4.779238618524333e-05,
      "loss": 0.6807,
      "step": 225000
    },
    {
      "epoch": 3.533751962323391,
      "grad_norm": 3.4901607036590576,
      "learning_rate": 4.779140502354788e-05,
      "loss": 0.6428,
      "step": 225100
    },
    {
      "epoch": 3.535321821036107,
      "grad_norm": 4.174561977386475,
      "learning_rate": 4.779042386185244e-05,
      "loss": 0.7172,
      "step": 225200
    },
    {
      "epoch": 3.5368916797488223,
      "grad_norm": 4.050912857055664,
      "learning_rate": 4.778944270015699e-05,
      "loss": 0.6814,
      "step": 225300
    },
    {
      "epoch": 3.5384615384615383,
      "grad_norm": 3.897526741027832,
      "learning_rate": 4.778846153846154e-05,
      "loss": 0.7199,
      "step": 225400
    },
    {
      "epoch": 3.5400313971742543,
      "grad_norm": 4.464590549468994,
      "learning_rate": 4.778748037676609e-05,
      "loss": 0.6961,
      "step": 225500
    },
    {
      "epoch": 3.5416012558869703,
      "grad_norm": 4.531529903411865,
      "learning_rate": 4.778649921507065e-05,
      "loss": 0.75,
      "step": 225600
    },
    {
      "epoch": 3.5431711145996863,
      "grad_norm": 3.463355779647827,
      "learning_rate": 4.77855180533752e-05,
      "loss": 0.6417,
      "step": 225700
    },
    {
      "epoch": 3.544740973312402,
      "grad_norm": 3.89846134185791,
      "learning_rate": 4.778453689167975e-05,
      "loss": 0.6603,
      "step": 225800
    },
    {
      "epoch": 3.5463108320251178,
      "grad_norm": 4.1738810539245605,
      "learning_rate": 4.77835557299843e-05,
      "loss": 0.7059,
      "step": 225900
    },
    {
      "epoch": 3.5478806907378337,
      "grad_norm": 4.637024402618408,
      "learning_rate": 4.778257456828886e-05,
      "loss": 0.7015,
      "step": 226000
    },
    {
      "epoch": 3.5494505494505493,
      "grad_norm": 4.783644199371338,
      "learning_rate": 4.778159340659341e-05,
      "loss": 0.6697,
      "step": 226100
    },
    {
      "epoch": 3.5510204081632653,
      "grad_norm": 2.692171335220337,
      "learning_rate": 4.778061224489796e-05,
      "loss": 0.6688,
      "step": 226200
    },
    {
      "epoch": 3.5525902668759812,
      "grad_norm": 4.346765041351318,
      "learning_rate": 4.777963108320251e-05,
      "loss": 0.709,
      "step": 226300
    },
    {
      "epoch": 3.554160125588697,
      "grad_norm": 4.247864246368408,
      "learning_rate": 4.777864992150707e-05,
      "loss": 0.6463,
      "step": 226400
    },
    {
      "epoch": 3.5557299843014127,
      "grad_norm": 3.809809923171997,
      "learning_rate": 4.7777668759811614e-05,
      "loss": 0.6825,
      "step": 226500
    },
    {
      "epoch": 3.5572998430141287,
      "grad_norm": 3.634667158126831,
      "learning_rate": 4.777668759811617e-05,
      "loss": 0.6734,
      "step": 226600
    },
    {
      "epoch": 3.5588697017268447,
      "grad_norm": 5.29538106918335,
      "learning_rate": 4.777570643642072e-05,
      "loss": 0.6759,
      "step": 226700
    },
    {
      "epoch": 3.5604395604395602,
      "grad_norm": 4.3552398681640625,
      "learning_rate": 4.777472527472528e-05,
      "loss": 0.6932,
      "step": 226800
    },
    {
      "epoch": 3.562009419152276,
      "grad_norm": 4.861806392669678,
      "learning_rate": 4.7773744113029825e-05,
      "loss": 0.7036,
      "step": 226900
    },
    {
      "epoch": 3.563579277864992,
      "grad_norm": 3.913529634475708,
      "learning_rate": 4.777276295133438e-05,
      "loss": 0.7102,
      "step": 227000
    },
    {
      "epoch": 3.565149136577708,
      "grad_norm": 4.876319885253906,
      "learning_rate": 4.7771781789638934e-05,
      "loss": 0.7103,
      "step": 227100
    },
    {
      "epoch": 3.566718995290424,
      "grad_norm": 4.102686405181885,
      "learning_rate": 4.7770800627943485e-05,
      "loss": 0.7109,
      "step": 227200
    },
    {
      "epoch": 3.5682888540031397,
      "grad_norm": 5.550441265106201,
      "learning_rate": 4.776981946624804e-05,
      "loss": 0.7192,
      "step": 227300
    },
    {
      "epoch": 3.5698587127158556,
      "grad_norm": 3.594783306121826,
      "learning_rate": 4.776883830455259e-05,
      "loss": 0.672,
      "step": 227400
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 3.4351115226745605,
      "learning_rate": 4.7767857142857144e-05,
      "loss": 0.7093,
      "step": 227500
    },
    {
      "epoch": 3.572998430141287,
      "grad_norm": 3.981306791305542,
      "learning_rate": 4.7766875981161695e-05,
      "loss": 0.6951,
      "step": 227600
    },
    {
      "epoch": 3.574568288854003,
      "grad_norm": 4.266631603240967,
      "learning_rate": 4.776589481946625e-05,
      "loss": 0.674,
      "step": 227700
    },
    {
      "epoch": 3.576138147566719,
      "grad_norm": 3.494598865509033,
      "learning_rate": 4.7764913657770804e-05,
      "loss": 0.6852,
      "step": 227800
    },
    {
      "epoch": 3.577708006279435,
      "grad_norm": 3.198509454727173,
      "learning_rate": 4.7763932496075355e-05,
      "loss": 0.655,
      "step": 227900
    },
    {
      "epoch": 3.5792778649921506,
      "grad_norm": 4.121933460235596,
      "learning_rate": 4.7762951334379906e-05,
      "loss": 0.7276,
      "step": 228000
    },
    {
      "epoch": 3.5808477237048666,
      "grad_norm": 3.9122860431671143,
      "learning_rate": 4.7761970172684464e-05,
      "loss": 0.6469,
      "step": 228100
    },
    {
      "epoch": 3.5824175824175826,
      "grad_norm": 4.032973289489746,
      "learning_rate": 4.7760989010989015e-05,
      "loss": 0.697,
      "step": 228200
    },
    {
      "epoch": 3.583987441130298,
      "grad_norm": 4.138460159301758,
      "learning_rate": 4.7760007849293566e-05,
      "loss": 0.727,
      "step": 228300
    },
    {
      "epoch": 3.585557299843014,
      "grad_norm": 4.039843559265137,
      "learning_rate": 4.775902668759812e-05,
      "loss": 0.6962,
      "step": 228400
    },
    {
      "epoch": 3.58712715855573,
      "grad_norm": 3.7112667560577393,
      "learning_rate": 4.7758045525902674e-05,
      "loss": 0.6879,
      "step": 228500
    },
    {
      "epoch": 3.588697017268446,
      "grad_norm": 4.366698265075684,
      "learning_rate": 4.775706436420722e-05,
      "loss": 0.6992,
      "step": 228600
    },
    {
      "epoch": 3.5902668759811616,
      "grad_norm": 5.600572109222412,
      "learning_rate": 4.7756083202511776e-05,
      "loss": 0.6342,
      "step": 228700
    },
    {
      "epoch": 3.5918367346938775,
      "grad_norm": 4.488298416137695,
      "learning_rate": 4.775510204081633e-05,
      "loss": 0.6787,
      "step": 228800
    },
    {
      "epoch": 3.5934065934065935,
      "grad_norm": 3.951507568359375,
      "learning_rate": 4.7754120879120885e-05,
      "loss": 0.727,
      "step": 228900
    },
    {
      "epoch": 3.594976452119309,
      "grad_norm": 3.9039535522460938,
      "learning_rate": 4.775313971742543e-05,
      "loss": 0.6919,
      "step": 229000
    },
    {
      "epoch": 3.596546310832025,
      "grad_norm": 3.4207828044891357,
      "learning_rate": 4.775215855572999e-05,
      "loss": 0.7181,
      "step": 229100
    },
    {
      "epoch": 3.598116169544741,
      "grad_norm": 4.585511684417725,
      "learning_rate": 4.775117739403454e-05,
      "loss": 0.6972,
      "step": 229200
    },
    {
      "epoch": 3.599686028257457,
      "grad_norm": 4.347285270690918,
      "learning_rate": 4.775019623233909e-05,
      "loss": 0.6967,
      "step": 229300
    },
    {
      "epoch": 3.601255886970173,
      "grad_norm": 4.319555759429932,
      "learning_rate": 4.774921507064365e-05,
      "loss": 0.7052,
      "step": 229400
    },
    {
      "epoch": 3.6028257456828885,
      "grad_norm": 4.9333953857421875,
      "learning_rate": 4.77482339089482e-05,
      "loss": 0.6841,
      "step": 229500
    },
    {
      "epoch": 3.6043956043956045,
      "grad_norm": 3.9019811153411865,
      "learning_rate": 4.774725274725275e-05,
      "loss": 0.7165,
      "step": 229600
    },
    {
      "epoch": 3.60596546310832,
      "grad_norm": 4.881191730499268,
      "learning_rate": 4.77462715855573e-05,
      "loss": 0.6712,
      "step": 229700
    },
    {
      "epoch": 3.607535321821036,
      "grad_norm": 3.478694438934326,
      "learning_rate": 4.774529042386186e-05,
      "loss": 0.6845,
      "step": 229800
    },
    {
      "epoch": 3.609105180533752,
      "grad_norm": 4.287099838256836,
      "learning_rate": 4.774430926216641e-05,
      "loss": 0.674,
      "step": 229900
    },
    {
      "epoch": 3.610675039246468,
      "grad_norm": 3.645693778991699,
      "learning_rate": 4.774332810047096e-05,
      "loss": 0.6772,
      "step": 230000
    },
    {
      "epoch": 3.612244897959184,
      "grad_norm": 3.8508198261260986,
      "learning_rate": 4.774234693877551e-05,
      "loss": 0.6924,
      "step": 230100
    },
    {
      "epoch": 3.6138147566718994,
      "grad_norm": 5.452381134033203,
      "learning_rate": 4.774136577708007e-05,
      "loss": 0.6858,
      "step": 230200
    },
    {
      "epoch": 3.6153846153846154,
      "grad_norm": 3.927577495574951,
      "learning_rate": 4.774038461538462e-05,
      "loss": 0.6627,
      "step": 230300
    },
    {
      "epoch": 3.6169544740973314,
      "grad_norm": 4.98906946182251,
      "learning_rate": 4.773940345368917e-05,
      "loss": 0.6897,
      "step": 230400
    },
    {
      "epoch": 3.618524332810047,
      "grad_norm": 5.066163063049316,
      "learning_rate": 4.773842229199372e-05,
      "loss": 0.6743,
      "step": 230500
    },
    {
      "epoch": 3.620094191522763,
      "grad_norm": 3.6577773094177246,
      "learning_rate": 4.773744113029828e-05,
      "loss": 0.7065,
      "step": 230600
    },
    {
      "epoch": 3.621664050235479,
      "grad_norm": 3.330843687057495,
      "learning_rate": 4.773645996860282e-05,
      "loss": 0.7205,
      "step": 230700
    },
    {
      "epoch": 3.623233908948195,
      "grad_norm": 3.9172463417053223,
      "learning_rate": 4.773547880690738e-05,
      "loss": 0.6709,
      "step": 230800
    },
    {
      "epoch": 3.6248037676609104,
      "grad_norm": 3.6632211208343506,
      "learning_rate": 4.773449764521193e-05,
      "loss": 0.695,
      "step": 230900
    },
    {
      "epoch": 3.6263736263736264,
      "grad_norm": 4.687806606292725,
      "learning_rate": 4.773351648351649e-05,
      "loss": 0.6895,
      "step": 231000
    },
    {
      "epoch": 3.6279434850863423,
      "grad_norm": 4.077116012573242,
      "learning_rate": 4.7732535321821034e-05,
      "loss": 0.6763,
      "step": 231100
    },
    {
      "epoch": 3.629513343799058,
      "grad_norm": 3.1819331645965576,
      "learning_rate": 4.773155416012559e-05,
      "loss": 0.6806,
      "step": 231200
    },
    {
      "epoch": 3.631083202511774,
      "grad_norm": 3.9189352989196777,
      "learning_rate": 4.773057299843014e-05,
      "loss": 0.6506,
      "step": 231300
    },
    {
      "epoch": 3.63265306122449,
      "grad_norm": 2.930842161178589,
      "learning_rate": 4.7729591836734693e-05,
      "loss": 0.6767,
      "step": 231400
    },
    {
      "epoch": 3.634222919937206,
      "grad_norm": 4.627021312713623,
      "learning_rate": 4.772861067503925e-05,
      "loss": 0.6897,
      "step": 231500
    },
    {
      "epoch": 3.6357927786499213,
      "grad_norm": 3.524167060852051,
      "learning_rate": 4.77276295133438e-05,
      "loss": 0.7288,
      "step": 231600
    },
    {
      "epoch": 3.6373626373626373,
      "grad_norm": 4.007374286651611,
      "learning_rate": 4.772664835164835e-05,
      "loss": 0.7242,
      "step": 231700
    },
    {
      "epoch": 3.6389324960753533,
      "grad_norm": 4.2353386878967285,
      "learning_rate": 4.7725667189952904e-05,
      "loss": 0.6497,
      "step": 231800
    },
    {
      "epoch": 3.640502354788069,
      "grad_norm": 4.398806095123291,
      "learning_rate": 4.772468602825746e-05,
      "loss": 0.671,
      "step": 231900
    },
    {
      "epoch": 3.642072213500785,
      "grad_norm": 5.020211219787598,
      "learning_rate": 4.772370486656201e-05,
      "loss": 0.6977,
      "step": 232000
    },
    {
      "epoch": 3.643642072213501,
      "grad_norm": 3.3754706382751465,
      "learning_rate": 4.7722723704866564e-05,
      "loss": 0.6927,
      "step": 232100
    },
    {
      "epoch": 3.6452119309262168,
      "grad_norm": 5.0176520347595215,
      "learning_rate": 4.7721742543171115e-05,
      "loss": 0.649,
      "step": 232200
    },
    {
      "epoch": 3.6467817896389327,
      "grad_norm": 2.8664467334747314,
      "learning_rate": 4.772076138147567e-05,
      "loss": 0.7343,
      "step": 232300
    },
    {
      "epoch": 3.6483516483516483,
      "grad_norm": 2.5946121215820312,
      "learning_rate": 4.7719780219780224e-05,
      "loss": 0.6917,
      "step": 232400
    },
    {
      "epoch": 3.6499215070643642,
      "grad_norm": 3.8393025398254395,
      "learning_rate": 4.7718799058084775e-05,
      "loss": 0.688,
      "step": 232500
    },
    {
      "epoch": 3.6514913657770802,
      "grad_norm": 4.6561360359191895,
      "learning_rate": 4.7717817896389326e-05,
      "loss": 0.6512,
      "step": 232600
    },
    {
      "epoch": 3.6530612244897958,
      "grad_norm": 3.3849096298217773,
      "learning_rate": 4.771683673469388e-05,
      "loss": 0.7291,
      "step": 232700
    },
    {
      "epoch": 3.6546310832025117,
      "grad_norm": 4.564155101776123,
      "learning_rate": 4.771585557299843e-05,
      "loss": 0.6863,
      "step": 232800
    },
    {
      "epoch": 3.6562009419152277,
      "grad_norm": 4.047068119049072,
      "learning_rate": 4.7714874411302985e-05,
      "loss": 0.7031,
      "step": 232900
    },
    {
      "epoch": 3.6577708006279437,
      "grad_norm": 3.4055898189544678,
      "learning_rate": 4.7713893249607536e-05,
      "loss": 0.655,
      "step": 233000
    },
    {
      "epoch": 3.659340659340659,
      "grad_norm": 4.955416202545166,
      "learning_rate": 4.7712912087912094e-05,
      "loss": 0.6801,
      "step": 233100
    },
    {
      "epoch": 3.660910518053375,
      "grad_norm": 2.9482765197753906,
      "learning_rate": 4.771193092621664e-05,
      "loss": 0.6815,
      "step": 233200
    },
    {
      "epoch": 3.662480376766091,
      "grad_norm": 3.6054627895355225,
      "learning_rate": 4.7710949764521196e-05,
      "loss": 0.6673,
      "step": 233300
    },
    {
      "epoch": 3.6640502354788067,
      "grad_norm": 4.300621509552002,
      "learning_rate": 4.770996860282575e-05,
      "loss": 0.7182,
      "step": 233400
    },
    {
      "epoch": 3.6656200941915227,
      "grad_norm": 4.10440731048584,
      "learning_rate": 4.77089874411303e-05,
      "loss": 0.6304,
      "step": 233500
    },
    {
      "epoch": 3.6671899529042387,
      "grad_norm": 4.010295391082764,
      "learning_rate": 4.7708006279434856e-05,
      "loss": 0.6892,
      "step": 233600
    },
    {
      "epoch": 3.6687598116169546,
      "grad_norm": 4.59210205078125,
      "learning_rate": 4.770702511773941e-05,
      "loss": 0.7083,
      "step": 233700
    },
    {
      "epoch": 3.67032967032967,
      "grad_norm": 3.043177366256714,
      "learning_rate": 4.770604395604396e-05,
      "loss": 0.696,
      "step": 233800
    },
    {
      "epoch": 3.671899529042386,
      "grad_norm": 4.246979236602783,
      "learning_rate": 4.770506279434851e-05,
      "loss": 0.6845,
      "step": 233900
    },
    {
      "epoch": 3.673469387755102,
      "grad_norm": 4.110363006591797,
      "learning_rate": 4.7704081632653066e-05,
      "loss": 0.6912,
      "step": 234000
    },
    {
      "epoch": 3.6750392464678177,
      "grad_norm": 3.593311309814453,
      "learning_rate": 4.770310047095762e-05,
      "loss": 0.6729,
      "step": 234100
    },
    {
      "epoch": 3.6766091051805336,
      "grad_norm": 4.459652900695801,
      "learning_rate": 4.770211930926217e-05,
      "loss": 0.6813,
      "step": 234200
    },
    {
      "epoch": 3.6781789638932496,
      "grad_norm": 4.6534743309021,
      "learning_rate": 4.770113814756672e-05,
      "loss": 0.6989,
      "step": 234300
    },
    {
      "epoch": 3.6797488226059656,
      "grad_norm": 4.4156904220581055,
      "learning_rate": 4.770015698587128e-05,
      "loss": 0.6377,
      "step": 234400
    },
    {
      "epoch": 3.6813186813186816,
      "grad_norm": 3.916687488555908,
      "learning_rate": 4.769917582417583e-05,
      "loss": 0.6609,
      "step": 234500
    },
    {
      "epoch": 3.682888540031397,
      "grad_norm": 3.103574275970459,
      "learning_rate": 4.769819466248038e-05,
      "loss": 0.7217,
      "step": 234600
    },
    {
      "epoch": 3.684458398744113,
      "grad_norm": 3.8520047664642334,
      "learning_rate": 4.769721350078493e-05,
      "loss": 0.6619,
      "step": 234700
    },
    {
      "epoch": 3.686028257456829,
      "grad_norm": 4.1017961502075195,
      "learning_rate": 4.769623233908949e-05,
      "loss": 0.6899,
      "step": 234800
    },
    {
      "epoch": 3.6875981161695446,
      "grad_norm": 4.562015056610107,
      "learning_rate": 4.769525117739403e-05,
      "loss": 0.6575,
      "step": 234900
    },
    {
      "epoch": 3.6891679748822606,
      "grad_norm": 4.485147953033447,
      "learning_rate": 4.769427001569859e-05,
      "loss": 0.6759,
      "step": 235000
    },
    {
      "epoch": 3.6907378335949765,
      "grad_norm": 3.787592649459839,
      "learning_rate": 4.769328885400314e-05,
      "loss": 0.6608,
      "step": 235100
    },
    {
      "epoch": 3.6923076923076925,
      "grad_norm": 3.3884575366973877,
      "learning_rate": 4.76923076923077e-05,
      "loss": 0.7036,
      "step": 235200
    },
    {
      "epoch": 3.693877551020408,
      "grad_norm": 4.249722003936768,
      "learning_rate": 4.769132653061224e-05,
      "loss": 0.6784,
      "step": 235300
    },
    {
      "epoch": 3.695447409733124,
      "grad_norm": 4.026219844818115,
      "learning_rate": 4.76903453689168e-05,
      "loss": 0.7074,
      "step": 235400
    },
    {
      "epoch": 3.69701726844584,
      "grad_norm": 4.867557525634766,
      "learning_rate": 4.768936420722135e-05,
      "loss": 0.6726,
      "step": 235500
    },
    {
      "epoch": 3.6985871271585555,
      "grad_norm": 3.1724276542663574,
      "learning_rate": 4.76883830455259e-05,
      "loss": 0.7221,
      "step": 235600
    },
    {
      "epoch": 3.7001569858712715,
      "grad_norm": 4.020410060882568,
      "learning_rate": 4.768740188383046e-05,
      "loss": 0.7122,
      "step": 235700
    },
    {
      "epoch": 3.7017268445839875,
      "grad_norm": 4.198667049407959,
      "learning_rate": 4.768642072213501e-05,
      "loss": 0.6918,
      "step": 235800
    },
    {
      "epoch": 3.7032967032967035,
      "grad_norm": 4.962991237640381,
      "learning_rate": 4.768543956043956e-05,
      "loss": 0.7115,
      "step": 235900
    },
    {
      "epoch": 3.704866562009419,
      "grad_norm": 3.8770546913146973,
      "learning_rate": 4.768445839874411e-05,
      "loss": 0.6788,
      "step": 236000
    },
    {
      "epoch": 3.706436420722135,
      "grad_norm": 2.8904082775115967,
      "learning_rate": 4.768347723704867e-05,
      "loss": 0.6679,
      "step": 236100
    },
    {
      "epoch": 3.708006279434851,
      "grad_norm": 3.2213547229766846,
      "learning_rate": 4.768249607535322e-05,
      "loss": 0.6623,
      "step": 236200
    },
    {
      "epoch": 3.7095761381475665,
      "grad_norm": 4.541619777679443,
      "learning_rate": 4.768151491365777e-05,
      "loss": 0.7051,
      "step": 236300
    },
    {
      "epoch": 3.7111459968602825,
      "grad_norm": 4.126660346984863,
      "learning_rate": 4.7680533751962324e-05,
      "loss": 0.7086,
      "step": 236400
    },
    {
      "epoch": 3.7127158555729984,
      "grad_norm": 4.0622639656066895,
      "learning_rate": 4.767955259026688e-05,
      "loss": 0.6891,
      "step": 236500
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 3.874770402908325,
      "learning_rate": 4.767857142857143e-05,
      "loss": 0.6592,
      "step": 236600
    },
    {
      "epoch": 3.7158555729984304,
      "grad_norm": 3.6094651222229004,
      "learning_rate": 4.7677590266875984e-05,
      "loss": 0.6623,
      "step": 236700
    },
    {
      "epoch": 3.717425431711146,
      "grad_norm": 3.6206774711608887,
      "learning_rate": 4.7676609105180535e-05,
      "loss": 0.6956,
      "step": 236800
    },
    {
      "epoch": 3.718995290423862,
      "grad_norm": 3.5374767780303955,
      "learning_rate": 4.767562794348509e-05,
      "loss": 0.6843,
      "step": 236900
    },
    {
      "epoch": 3.7205651491365774,
      "grad_norm": 3.8581113815307617,
      "learning_rate": 4.7674646781789637e-05,
      "loss": 0.6837,
      "step": 237000
    },
    {
      "epoch": 3.7221350078492934,
      "grad_norm": 4.745699882507324,
      "learning_rate": 4.7673665620094194e-05,
      "loss": 0.7215,
      "step": 237100
    },
    {
      "epoch": 3.7237048665620094,
      "grad_norm": 4.888535976409912,
      "learning_rate": 4.7672684458398745e-05,
      "loss": 0.695,
      "step": 237200
    },
    {
      "epoch": 3.7252747252747254,
      "grad_norm": 5.005124568939209,
      "learning_rate": 4.76717032967033e-05,
      "loss": 0.6879,
      "step": 237300
    },
    {
      "epoch": 3.7268445839874413,
      "grad_norm": 4.0859150886535645,
      "learning_rate": 4.767072213500785e-05,
      "loss": 0.6881,
      "step": 237400
    },
    {
      "epoch": 3.728414442700157,
      "grad_norm": 4.32421875,
      "learning_rate": 4.7669740973312405e-05,
      "loss": 0.6833,
      "step": 237500
    },
    {
      "epoch": 3.729984301412873,
      "grad_norm": 4.1114583015441895,
      "learning_rate": 4.7668759811616956e-05,
      "loss": 0.6933,
      "step": 237600
    },
    {
      "epoch": 3.731554160125589,
      "grad_norm": 3.0690104961395264,
      "learning_rate": 4.766777864992151e-05,
      "loss": 0.6802,
      "step": 237700
    },
    {
      "epoch": 3.7331240188383044,
      "grad_norm": 3.5142176151275635,
      "learning_rate": 4.7666797488226065e-05,
      "loss": 0.7081,
      "step": 237800
    },
    {
      "epoch": 3.7346938775510203,
      "grad_norm": 3.766014575958252,
      "learning_rate": 4.7665816326530616e-05,
      "loss": 0.6809,
      "step": 237900
    },
    {
      "epoch": 3.7362637362637363,
      "grad_norm": 3.3637101650238037,
      "learning_rate": 4.766483516483517e-05,
      "loss": 0.7017,
      "step": 238000
    },
    {
      "epoch": 3.7378335949764523,
      "grad_norm": 3.65096378326416,
      "learning_rate": 4.766385400313972e-05,
      "loss": 0.7295,
      "step": 238100
    },
    {
      "epoch": 3.739403453689168,
      "grad_norm": 3.908480405807495,
      "learning_rate": 4.7662872841444275e-05,
      "loss": 0.67,
      "step": 238200
    },
    {
      "epoch": 3.740973312401884,
      "grad_norm": 4.366663455963135,
      "learning_rate": 4.7661891679748826e-05,
      "loss": 0.7121,
      "step": 238300
    },
    {
      "epoch": 3.7425431711145998,
      "grad_norm": 3.1583683490753174,
      "learning_rate": 4.766091051805338e-05,
      "loss": 0.6818,
      "step": 238400
    },
    {
      "epoch": 3.7441130298273153,
      "grad_norm": 3.708124876022339,
      "learning_rate": 4.765992935635793e-05,
      "loss": 0.7106,
      "step": 238500
    },
    {
      "epoch": 3.7456828885400313,
      "grad_norm": 3.646629810333252,
      "learning_rate": 4.7658948194662486e-05,
      "loss": 0.6576,
      "step": 238600
    },
    {
      "epoch": 3.7472527472527473,
      "grad_norm": 4.795614719390869,
      "learning_rate": 4.765796703296704e-05,
      "loss": 0.7164,
      "step": 238700
    },
    {
      "epoch": 3.7488226059654632,
      "grad_norm": 3.6552939414978027,
      "learning_rate": 4.765698587127159e-05,
      "loss": 0.7223,
      "step": 238800
    },
    {
      "epoch": 3.750392464678179,
      "grad_norm": 4.855536937713623,
      "learning_rate": 4.765600470957614e-05,
      "loss": 0.6491,
      "step": 238900
    },
    {
      "epoch": 3.7519623233908947,
      "grad_norm": 4.335493087768555,
      "learning_rate": 4.76550235478807e-05,
      "loss": 0.7128,
      "step": 239000
    },
    {
      "epoch": 3.7535321821036107,
      "grad_norm": 4.0143938064575195,
      "learning_rate": 4.765404238618524e-05,
      "loss": 0.6843,
      "step": 239100
    },
    {
      "epoch": 3.7551020408163263,
      "grad_norm": 4.623118877410889,
      "learning_rate": 4.76530612244898e-05,
      "loss": 0.7198,
      "step": 239200
    },
    {
      "epoch": 3.7566718995290422,
      "grad_norm": 3.047985315322876,
      "learning_rate": 4.765208006279435e-05,
      "loss": 0.6861,
      "step": 239300
    },
    {
      "epoch": 3.758241758241758,
      "grad_norm": 4.324209690093994,
      "learning_rate": 4.76510989010989e-05,
      "loss": 0.68,
      "step": 239400
    },
    {
      "epoch": 3.759811616954474,
      "grad_norm": 3.979247570037842,
      "learning_rate": 4.765011773940345e-05,
      "loss": 0.7053,
      "step": 239500
    },
    {
      "epoch": 3.76138147566719,
      "grad_norm": 4.239807605743408,
      "learning_rate": 4.764913657770801e-05,
      "loss": 0.7021,
      "step": 239600
    },
    {
      "epoch": 3.7629513343799057,
      "grad_norm": 4.486297607421875,
      "learning_rate": 4.764815541601256e-05,
      "loss": 0.7113,
      "step": 239700
    },
    {
      "epoch": 3.7645211930926217,
      "grad_norm": 3.6892290115356445,
      "learning_rate": 4.764717425431711e-05,
      "loss": 0.7154,
      "step": 239800
    },
    {
      "epoch": 3.7660910518053377,
      "grad_norm": 4.043353080749512,
      "learning_rate": 4.764619309262167e-05,
      "loss": 0.6881,
      "step": 239900
    },
    {
      "epoch": 3.767660910518053,
      "grad_norm": 3.423354148864746,
      "learning_rate": 4.764521193092622e-05,
      "loss": 0.7022,
      "step": 240000
    },
    {
      "epoch": 3.769230769230769,
      "grad_norm": 4.679724216461182,
      "learning_rate": 4.764423076923077e-05,
      "loss": 0.7093,
      "step": 240100
    },
    {
      "epoch": 3.770800627943485,
      "grad_norm": 4.345253944396973,
      "learning_rate": 4.764324960753532e-05,
      "loss": 0.717,
      "step": 240200
    },
    {
      "epoch": 3.772370486656201,
      "grad_norm": 3.5818967819213867,
      "learning_rate": 4.764226844583988e-05,
      "loss": 0.6574,
      "step": 240300
    },
    {
      "epoch": 3.7739403453689166,
      "grad_norm": 3.95597505569458,
      "learning_rate": 4.764128728414443e-05,
      "loss": 0.6665,
      "step": 240400
    },
    {
      "epoch": 3.7755102040816326,
      "grad_norm": 4.388553142547607,
      "learning_rate": 4.764030612244898e-05,
      "loss": 0.7169,
      "step": 240500
    },
    {
      "epoch": 3.7770800627943486,
      "grad_norm": 3.6835851669311523,
      "learning_rate": 4.763932496075353e-05,
      "loss": 0.6849,
      "step": 240600
    },
    {
      "epoch": 3.778649921507064,
      "grad_norm": 4.331906795501709,
      "learning_rate": 4.763834379905809e-05,
      "loss": 0.6954,
      "step": 240700
    },
    {
      "epoch": 3.78021978021978,
      "grad_norm": 4.053897380828857,
      "learning_rate": 4.7637362637362635e-05,
      "loss": 0.6596,
      "step": 240800
    },
    {
      "epoch": 3.781789638932496,
      "grad_norm": 3.3056485652923584,
      "learning_rate": 4.763638147566719e-05,
      "loss": 0.7073,
      "step": 240900
    },
    {
      "epoch": 3.783359497645212,
      "grad_norm": 4.387648105621338,
      "learning_rate": 4.7635400313971744e-05,
      "loss": 0.7,
      "step": 241000
    },
    {
      "epoch": 3.784929356357928,
      "grad_norm": 3.6437828540802,
      "learning_rate": 4.76344191522763e-05,
      "loss": 0.688,
      "step": 241100
    },
    {
      "epoch": 3.7864992150706436,
      "grad_norm": 4.8755784034729,
      "learning_rate": 4.7633437990580846e-05,
      "loss": 0.6727,
      "step": 241200
    },
    {
      "epoch": 3.7880690737833596,
      "grad_norm": 4.5564703941345215,
      "learning_rate": 4.76324568288854e-05,
      "loss": 0.663,
      "step": 241300
    },
    {
      "epoch": 3.789638932496075,
      "grad_norm": 3.0793492794036865,
      "learning_rate": 4.7631475667189954e-05,
      "loss": 0.7089,
      "step": 241400
    },
    {
      "epoch": 3.791208791208791,
      "grad_norm": 3.3924546241760254,
      "learning_rate": 4.7630494505494505e-05,
      "loss": 0.6855,
      "step": 241500
    },
    {
      "epoch": 3.792778649921507,
      "grad_norm": 4.1591267585754395,
      "learning_rate": 4.7629513343799056e-05,
      "loss": 0.718,
      "step": 241600
    },
    {
      "epoch": 3.794348508634223,
      "grad_norm": 2.9173061847686768,
      "learning_rate": 4.7628532182103614e-05,
      "loss": 0.6441,
      "step": 241700
    },
    {
      "epoch": 3.795918367346939,
      "grad_norm": 4.191332817077637,
      "learning_rate": 4.7627551020408165e-05,
      "loss": 0.7053,
      "step": 241800
    },
    {
      "epoch": 3.7974882260596545,
      "grad_norm": 3.4156301021575928,
      "learning_rate": 4.7626569858712716e-05,
      "loss": 0.6942,
      "step": 241900
    },
    {
      "epoch": 3.7990580847723705,
      "grad_norm": 3.0482141971588135,
      "learning_rate": 4.7625588697017274e-05,
      "loss": 0.6933,
      "step": 242000
    },
    {
      "epoch": 3.8006279434850865,
      "grad_norm": 3.520942211151123,
      "learning_rate": 4.7624607535321825e-05,
      "loss": 0.6714,
      "step": 242100
    },
    {
      "epoch": 3.802197802197802,
      "grad_norm": 3.1074023246765137,
      "learning_rate": 4.7623626373626376e-05,
      "loss": 0.663,
      "step": 242200
    },
    {
      "epoch": 3.803767660910518,
      "grad_norm": 2.0947983264923096,
      "learning_rate": 4.762264521193093e-05,
      "loss": 0.7045,
      "step": 242300
    },
    {
      "epoch": 3.805337519623234,
      "grad_norm": 3.0715677738189697,
      "learning_rate": 4.7621664050235484e-05,
      "loss": 0.6793,
      "step": 242400
    },
    {
      "epoch": 3.80690737833595,
      "grad_norm": 3.716305732727051,
      "learning_rate": 4.7620682888540035e-05,
      "loss": 0.6864,
      "step": 242500
    },
    {
      "epoch": 3.8084772370486655,
      "grad_norm": 4.357000350952148,
      "learning_rate": 4.7619701726844586e-05,
      "loss": 0.7098,
      "step": 242600
    },
    {
      "epoch": 3.8100470957613815,
      "grad_norm": 5.171468734741211,
      "learning_rate": 4.761872056514914e-05,
      "loss": 0.6942,
      "step": 242700
    },
    {
      "epoch": 3.8116169544740974,
      "grad_norm": 4.014507293701172,
      "learning_rate": 4.7617739403453695e-05,
      "loss": 0.6759,
      "step": 242800
    },
    {
      "epoch": 3.813186813186813,
      "grad_norm": 3.9209797382354736,
      "learning_rate": 4.761675824175824e-05,
      "loss": 0.6855,
      "step": 242900
    },
    {
      "epoch": 3.814756671899529,
      "grad_norm": 4.227816104888916,
      "learning_rate": 4.76157770800628e-05,
      "loss": 0.6892,
      "step": 243000
    },
    {
      "epoch": 3.816326530612245,
      "grad_norm": 4.697353839874268,
      "learning_rate": 4.761479591836735e-05,
      "loss": 0.6793,
      "step": 243100
    },
    {
      "epoch": 3.817896389324961,
      "grad_norm": 4.495218276977539,
      "learning_rate": 4.7613814756671906e-05,
      "loss": 0.701,
      "step": 243200
    },
    {
      "epoch": 3.819466248037677,
      "grad_norm": 3.6297552585601807,
      "learning_rate": 4.761283359497645e-05,
      "loss": 0.653,
      "step": 243300
    },
    {
      "epoch": 3.8210361067503924,
      "grad_norm": 3.431245803833008,
      "learning_rate": 4.761185243328101e-05,
      "loss": 0.6621,
      "step": 243400
    },
    {
      "epoch": 3.8226059654631084,
      "grad_norm": 4.0326972007751465,
      "learning_rate": 4.761087127158556e-05,
      "loss": 0.7114,
      "step": 243500
    },
    {
      "epoch": 3.824175824175824,
      "grad_norm": 3.566188335418701,
      "learning_rate": 4.760989010989011e-05,
      "loss": 0.7614,
      "step": 243600
    },
    {
      "epoch": 3.82574568288854,
      "grad_norm": 4.695072174072266,
      "learning_rate": 4.760890894819466e-05,
      "loss": 0.6588,
      "step": 243700
    },
    {
      "epoch": 3.827315541601256,
      "grad_norm": 5.073031902313232,
      "learning_rate": 4.760792778649922e-05,
      "loss": 0.6973,
      "step": 243800
    },
    {
      "epoch": 3.828885400313972,
      "grad_norm": 4.2851338386535645,
      "learning_rate": 4.760694662480377e-05,
      "loss": 0.7062,
      "step": 243900
    },
    {
      "epoch": 3.830455259026688,
      "grad_norm": 4.1173200607299805,
      "learning_rate": 4.760596546310832e-05,
      "loss": 0.6646,
      "step": 244000
    },
    {
      "epoch": 3.8320251177394034,
      "grad_norm": 3.66601300239563,
      "learning_rate": 4.760498430141288e-05,
      "loss": 0.737,
      "step": 244100
    },
    {
      "epoch": 3.8335949764521193,
      "grad_norm": 4.204455375671387,
      "learning_rate": 4.760400313971743e-05,
      "loss": 0.6974,
      "step": 244200
    },
    {
      "epoch": 3.8351648351648353,
      "grad_norm": 3.8867499828338623,
      "learning_rate": 4.760302197802198e-05,
      "loss": 0.697,
      "step": 244300
    },
    {
      "epoch": 3.836734693877551,
      "grad_norm": 3.9635257720947266,
      "learning_rate": 4.760204081632653e-05,
      "loss": 0.7174,
      "step": 244400
    },
    {
      "epoch": 3.838304552590267,
      "grad_norm": 3.7982726097106934,
      "learning_rate": 4.760105965463109e-05,
      "loss": 0.7129,
      "step": 244500
    },
    {
      "epoch": 3.839874411302983,
      "grad_norm": 3.9761531352996826,
      "learning_rate": 4.760007849293564e-05,
      "loss": 0.6483,
      "step": 244600
    },
    {
      "epoch": 3.8414442700156988,
      "grad_norm": 3.6950788497924805,
      "learning_rate": 4.759909733124019e-05,
      "loss": 0.6764,
      "step": 244700
    },
    {
      "epoch": 3.8430141287284143,
      "grad_norm": 3.5429186820983887,
      "learning_rate": 4.759811616954474e-05,
      "loss": 0.715,
      "step": 244800
    },
    {
      "epoch": 3.8445839874411303,
      "grad_norm": 3.7545437812805176,
      "learning_rate": 4.75971350078493e-05,
      "loss": 0.6857,
      "step": 244900
    },
    {
      "epoch": 3.8461538461538463,
      "grad_norm": 2.4404137134552,
      "learning_rate": 4.7596153846153844e-05,
      "loss": 0.7747,
      "step": 245000
    },
    {
      "epoch": 3.847723704866562,
      "grad_norm": 4.129229545593262,
      "learning_rate": 4.75951726844584e-05,
      "loss": 0.6943,
      "step": 245100
    },
    {
      "epoch": 3.8492935635792778,
      "grad_norm": 4.377843856811523,
      "learning_rate": 4.759419152276295e-05,
      "loss": 0.6723,
      "step": 245200
    },
    {
      "epoch": 3.8508634222919937,
      "grad_norm": 4.82915735244751,
      "learning_rate": 4.759321036106751e-05,
      "loss": 0.6552,
      "step": 245300
    },
    {
      "epoch": 3.8524332810047097,
      "grad_norm": 4.065042972564697,
      "learning_rate": 4.7592229199372055e-05,
      "loss": 0.6835,
      "step": 245400
    },
    {
      "epoch": 3.8540031397174257,
      "grad_norm": 4.4250264167785645,
      "learning_rate": 4.759124803767661e-05,
      "loss": 0.6587,
      "step": 245500
    },
    {
      "epoch": 3.8555729984301412,
      "grad_norm": 4.673556804656982,
      "learning_rate": 4.759026687598116e-05,
      "loss": 0.7101,
      "step": 245600
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 3.7885959148406982,
      "learning_rate": 4.7589285714285714e-05,
      "loss": 0.7065,
      "step": 245700
    },
    {
      "epoch": 3.8587127158555727,
      "grad_norm": 3.7578866481781006,
      "learning_rate": 4.7588304552590265e-05,
      "loss": 0.7247,
      "step": 245800
    },
    {
      "epoch": 3.8602825745682887,
      "grad_norm": 3.660221815109253,
      "learning_rate": 4.758732339089482e-05,
      "loss": 0.6503,
      "step": 245900
    },
    {
      "epoch": 3.8618524332810047,
      "grad_norm": 4.642119407653809,
      "learning_rate": 4.7586342229199374e-05,
      "loss": 0.6862,
      "step": 246000
    },
    {
      "epoch": 3.8634222919937207,
      "grad_norm": 4.093353271484375,
      "learning_rate": 4.7585361067503925e-05,
      "loss": 0.739,
      "step": 246100
    },
    {
      "epoch": 3.8649921507064366,
      "grad_norm": 3.4898719787597656,
      "learning_rate": 4.758437990580848e-05,
      "loss": 0.6871,
      "step": 246200
    },
    {
      "epoch": 3.866562009419152,
      "grad_norm": 4.736448764801025,
      "learning_rate": 4.7583398744113034e-05,
      "loss": 0.692,
      "step": 246300
    },
    {
      "epoch": 3.868131868131868,
      "grad_norm": 3.366014003753662,
      "learning_rate": 4.7582417582417585e-05,
      "loss": 0.7187,
      "step": 246400
    },
    {
      "epoch": 3.869701726844584,
      "grad_norm": 3.687805652618408,
      "learning_rate": 4.7581436420722136e-05,
      "loss": 0.6997,
      "step": 246500
    },
    {
      "epoch": 3.8712715855572997,
      "grad_norm": 4.220821380615234,
      "learning_rate": 4.7580455259026693e-05,
      "loss": 0.6603,
      "step": 246600
    },
    {
      "epoch": 3.8728414442700156,
      "grad_norm": 3.4530739784240723,
      "learning_rate": 4.7579474097331244e-05,
      "loss": 0.7133,
      "step": 246700
    },
    {
      "epoch": 3.8744113029827316,
      "grad_norm": 3.483804225921631,
      "learning_rate": 4.7578492935635795e-05,
      "loss": 0.6866,
      "step": 246800
    },
    {
      "epoch": 3.8759811616954476,
      "grad_norm": 3.7649779319763184,
      "learning_rate": 4.7577511773940346e-05,
      "loss": 0.6669,
      "step": 246900
    },
    {
      "epoch": 3.877551020408163,
      "grad_norm": 5.206137657165527,
      "learning_rate": 4.7576530612244904e-05,
      "loss": 0.6928,
      "step": 247000
    },
    {
      "epoch": 3.879120879120879,
      "grad_norm": 3.409895658493042,
      "learning_rate": 4.757554945054945e-05,
      "loss": 0.6239,
      "step": 247100
    },
    {
      "epoch": 3.880690737833595,
      "grad_norm": 5.3336076736450195,
      "learning_rate": 4.7574568288854006e-05,
      "loss": 0.6421,
      "step": 247200
    },
    {
      "epoch": 3.8822605965463106,
      "grad_norm": 4.501804828643799,
      "learning_rate": 4.757358712715856e-05,
      "loss": 0.7134,
      "step": 247300
    },
    {
      "epoch": 3.8838304552590266,
      "grad_norm": 4.69303560256958,
      "learning_rate": 4.7572605965463115e-05,
      "loss": 0.7368,
      "step": 247400
    },
    {
      "epoch": 3.8854003139717426,
      "grad_norm": 2.5992839336395264,
      "learning_rate": 4.757162480376766e-05,
      "loss": 0.6617,
      "step": 247500
    },
    {
      "epoch": 3.8869701726844585,
      "grad_norm": 3.657256841659546,
      "learning_rate": 4.757064364207222e-05,
      "loss": 0.6535,
      "step": 247600
    },
    {
      "epoch": 3.8885400313971745,
      "grad_norm": 4.23308801651001,
      "learning_rate": 4.756966248037677e-05,
      "loss": 0.6667,
      "step": 247700
    },
    {
      "epoch": 3.89010989010989,
      "grad_norm": 4.582345962524414,
      "learning_rate": 4.756868131868132e-05,
      "loss": 0.7122,
      "step": 247800
    },
    {
      "epoch": 3.891679748822606,
      "grad_norm": 4.214420318603516,
      "learning_rate": 4.756770015698587e-05,
      "loss": 0.6414,
      "step": 247900
    },
    {
      "epoch": 3.8932496075353216,
      "grad_norm": 3.9537880420684814,
      "learning_rate": 4.756671899529043e-05,
      "loss": 0.673,
      "step": 248000
    },
    {
      "epoch": 3.8948194662480375,
      "grad_norm": 4.883286952972412,
      "learning_rate": 4.756573783359498e-05,
      "loss": 0.7172,
      "step": 248100
    },
    {
      "epoch": 3.8963893249607535,
      "grad_norm": 4.223415374755859,
      "learning_rate": 4.756475667189953e-05,
      "loss": 0.7381,
      "step": 248200
    },
    {
      "epoch": 3.8979591836734695,
      "grad_norm": 4.137406826019287,
      "learning_rate": 4.756377551020409e-05,
      "loss": 0.7407,
      "step": 248300
    },
    {
      "epoch": 3.8995290423861855,
      "grad_norm": 3.297464609146118,
      "learning_rate": 4.756279434850864e-05,
      "loss": 0.6495,
      "step": 248400
    },
    {
      "epoch": 3.901098901098901,
      "grad_norm": 3.9899330139160156,
      "learning_rate": 4.756181318681319e-05,
      "loss": 0.6488,
      "step": 248500
    },
    {
      "epoch": 3.902668759811617,
      "grad_norm": 3.642780065536499,
      "learning_rate": 4.756083202511774e-05,
      "loss": 0.6892,
      "step": 248600
    },
    {
      "epoch": 3.904238618524333,
      "grad_norm": 3.763911485671997,
      "learning_rate": 4.75598508634223e-05,
      "loss": 0.6832,
      "step": 248700
    },
    {
      "epoch": 3.9058084772370485,
      "grad_norm": 4.009809494018555,
      "learning_rate": 4.755886970172685e-05,
      "loss": 0.7325,
      "step": 248800
    },
    {
      "epoch": 3.9073783359497645,
      "grad_norm": 3.069417715072632,
      "learning_rate": 4.75578885400314e-05,
      "loss": 0.6556,
      "step": 248900
    },
    {
      "epoch": 3.9089481946624804,
      "grad_norm": 5.01071310043335,
      "learning_rate": 4.755690737833595e-05,
      "loss": 0.6872,
      "step": 249000
    },
    {
      "epoch": 3.9105180533751964,
      "grad_norm": 3.7713282108306885,
      "learning_rate": 4.755592621664051e-05,
      "loss": 0.6916,
      "step": 249100
    },
    {
      "epoch": 3.912087912087912,
      "grad_norm": 3.9430084228515625,
      "learning_rate": 4.755494505494505e-05,
      "loss": 0.648,
      "step": 249200
    },
    {
      "epoch": 3.913657770800628,
      "grad_norm": 4.471295356750488,
      "learning_rate": 4.755396389324961e-05,
      "loss": 0.6811,
      "step": 249300
    },
    {
      "epoch": 3.915227629513344,
      "grad_norm": 3.3953182697296143,
      "learning_rate": 4.755298273155416e-05,
      "loss": 0.6979,
      "step": 249400
    },
    {
      "epoch": 3.9167974882260594,
      "grad_norm": 4.397339820861816,
      "learning_rate": 4.755200156985872e-05,
      "loss": 0.6684,
      "step": 249500
    },
    {
      "epoch": 3.9183673469387754,
      "grad_norm": 4.414515495300293,
      "learning_rate": 4.7551020408163263e-05,
      "loss": 0.6749,
      "step": 249600
    },
    {
      "epoch": 3.9199372056514914,
      "grad_norm": 3.814574956893921,
      "learning_rate": 4.755003924646782e-05,
      "loss": 0.6944,
      "step": 249700
    },
    {
      "epoch": 3.9215070643642074,
      "grad_norm": 4.283221244812012,
      "learning_rate": 4.754905808477237e-05,
      "loss": 0.6448,
      "step": 249800
    },
    {
      "epoch": 3.9230769230769234,
      "grad_norm": 4.623457431793213,
      "learning_rate": 4.754807692307692e-05,
      "loss": 0.6378,
      "step": 249900
    },
    {
      "epoch": 3.924646781789639,
      "grad_norm": 4.400123119354248,
      "learning_rate": 4.7547095761381474e-05,
      "loss": 0.6846,
      "step": 250000
    },
    {
      "epoch": 3.926216640502355,
      "grad_norm": 3.1267006397247314,
      "learning_rate": 4.754611459968603e-05,
      "loss": 0.6886,
      "step": 250100
    },
    {
      "epoch": 3.9277864992150704,
      "grad_norm": 4.452840328216553,
      "learning_rate": 4.754513343799058e-05,
      "loss": 0.6901,
      "step": 250200
    },
    {
      "epoch": 3.9293563579277864,
      "grad_norm": 3.8437840938568115,
      "learning_rate": 4.7544152276295134e-05,
      "loss": 0.6593,
      "step": 250300
    },
    {
      "epoch": 3.9309262166405023,
      "grad_norm": 4.340795040130615,
      "learning_rate": 4.754317111459969e-05,
      "loss": 0.6999,
      "step": 250400
    },
    {
      "epoch": 3.9324960753532183,
      "grad_norm": 4.133960723876953,
      "learning_rate": 4.754218995290424e-05,
      "loss": 0.6515,
      "step": 250500
    },
    {
      "epoch": 3.9340659340659343,
      "grad_norm": 3.5292422771453857,
      "learning_rate": 4.7541208791208794e-05,
      "loss": 0.67,
      "step": 250600
    },
    {
      "epoch": 3.93563579277865,
      "grad_norm": 4.287811756134033,
      "learning_rate": 4.7540227629513345e-05,
      "loss": 0.663,
      "step": 250700
    },
    {
      "epoch": 3.937205651491366,
      "grad_norm": 4.127054691314697,
      "learning_rate": 4.75392464678179e-05,
      "loss": 0.7131,
      "step": 250800
    },
    {
      "epoch": 3.938775510204082,
      "grad_norm": 3.5598461627960205,
      "learning_rate": 4.753826530612245e-05,
      "loss": 0.6848,
      "step": 250900
    },
    {
      "epoch": 3.9403453689167973,
      "grad_norm": 3.218412160873413,
      "learning_rate": 4.7537284144427004e-05,
      "loss": 0.6733,
      "step": 251000
    },
    {
      "epoch": 3.9419152276295133,
      "grad_norm": 4.694291114807129,
      "learning_rate": 4.7536302982731555e-05,
      "loss": 0.7062,
      "step": 251100
    },
    {
      "epoch": 3.9434850863422293,
      "grad_norm": 4.6305365562438965,
      "learning_rate": 4.753532182103611e-05,
      "loss": 0.6671,
      "step": 251200
    },
    {
      "epoch": 3.9450549450549453,
      "grad_norm": 4.63388729095459,
      "learning_rate": 4.753434065934066e-05,
      "loss": 0.6705,
      "step": 251300
    },
    {
      "epoch": 3.946624803767661,
      "grad_norm": 3.868075370788574,
      "learning_rate": 4.7533359497645215e-05,
      "loss": 0.6621,
      "step": 251400
    },
    {
      "epoch": 3.9481946624803768,
      "grad_norm": 4.131197929382324,
      "learning_rate": 4.7532378335949766e-05,
      "loss": 0.6572,
      "step": 251500
    },
    {
      "epoch": 3.9497645211930927,
      "grad_norm": 4.215909004211426,
      "learning_rate": 4.7531397174254324e-05,
      "loss": 0.6697,
      "step": 251600
    },
    {
      "epoch": 3.9513343799058083,
      "grad_norm": 4.105564117431641,
      "learning_rate": 4.753041601255887e-05,
      "loss": 0.7085,
      "step": 251700
    },
    {
      "epoch": 3.9529042386185242,
      "grad_norm": 4.149998664855957,
      "learning_rate": 4.7529434850863426e-05,
      "loss": 0.7134,
      "step": 251800
    },
    {
      "epoch": 3.9544740973312402,
      "grad_norm": 4.087775707244873,
      "learning_rate": 4.752845368916798e-05,
      "loss": 0.6882,
      "step": 251900
    },
    {
      "epoch": 3.956043956043956,
      "grad_norm": 4.185553073883057,
      "learning_rate": 4.752747252747253e-05,
      "loss": 0.6797,
      "step": 252000
    },
    {
      "epoch": 3.957613814756672,
      "grad_norm": 3.6256632804870605,
      "learning_rate": 4.752649136577708e-05,
      "loss": 0.6954,
      "step": 252100
    },
    {
      "epoch": 3.9591836734693877,
      "grad_norm": 4.141051292419434,
      "learning_rate": 4.7525510204081636e-05,
      "loss": 0.6816,
      "step": 252200
    },
    {
      "epoch": 3.9607535321821037,
      "grad_norm": 4.059247016906738,
      "learning_rate": 4.752452904238619e-05,
      "loss": 0.6302,
      "step": 252300
    },
    {
      "epoch": 3.962323390894819,
      "grad_norm": 3.8980042934417725,
      "learning_rate": 4.752354788069074e-05,
      "loss": 0.6307,
      "step": 252400
    },
    {
      "epoch": 3.963893249607535,
      "grad_norm": 4.807615756988525,
      "learning_rate": 4.7522566718995296e-05,
      "loss": 0.6434,
      "step": 252500
    },
    {
      "epoch": 3.965463108320251,
      "grad_norm": 3.5339720249176025,
      "learning_rate": 4.752158555729985e-05,
      "loss": 0.6533,
      "step": 252600
    },
    {
      "epoch": 3.967032967032967,
      "grad_norm": 3.4444124698638916,
      "learning_rate": 4.75206043956044e-05,
      "loss": 0.7245,
      "step": 252700
    },
    {
      "epoch": 3.968602825745683,
      "grad_norm": 4.602416038513184,
      "learning_rate": 4.751962323390895e-05,
      "loss": 0.6963,
      "step": 252800
    },
    {
      "epoch": 3.9701726844583987,
      "grad_norm": 3.745795488357544,
      "learning_rate": 4.751864207221351e-05,
      "loss": 0.6681,
      "step": 252900
    },
    {
      "epoch": 3.9717425431711146,
      "grad_norm": 3.981602430343628,
      "learning_rate": 4.751766091051806e-05,
      "loss": 0.6451,
      "step": 253000
    },
    {
      "epoch": 3.9733124018838306,
      "grad_norm": 4.494424343109131,
      "learning_rate": 4.751667974882261e-05,
      "loss": 0.7142,
      "step": 253100
    },
    {
      "epoch": 3.974882260596546,
      "grad_norm": 4.5535478591918945,
      "learning_rate": 4.751569858712716e-05,
      "loss": 0.7252,
      "step": 253200
    },
    {
      "epoch": 3.976452119309262,
      "grad_norm": 4.147103309631348,
      "learning_rate": 4.751471742543172e-05,
      "loss": 0.694,
      "step": 253300
    },
    {
      "epoch": 3.978021978021978,
      "grad_norm": 3.8664567470550537,
      "learning_rate": 4.751373626373626e-05,
      "loss": 0.6744,
      "step": 253400
    },
    {
      "epoch": 3.979591836734694,
      "grad_norm": 4.09652853012085,
      "learning_rate": 4.751275510204082e-05,
      "loss": 0.7115,
      "step": 253500
    },
    {
      "epoch": 3.9811616954474096,
      "grad_norm": 3.9364781379699707,
      "learning_rate": 4.751177394034537e-05,
      "loss": 0.6815,
      "step": 253600
    },
    {
      "epoch": 3.9827315541601256,
      "grad_norm": 4.77299690246582,
      "learning_rate": 4.751079277864993e-05,
      "loss": 0.6676,
      "step": 253700
    },
    {
      "epoch": 3.9843014128728416,
      "grad_norm": 3.804586172103882,
      "learning_rate": 4.750981161695447e-05,
      "loss": 0.6684,
      "step": 253800
    },
    {
      "epoch": 3.985871271585557,
      "grad_norm": 3.4918863773345947,
      "learning_rate": 4.750883045525903e-05,
      "loss": 0.6697,
      "step": 253900
    },
    {
      "epoch": 3.987441130298273,
      "grad_norm": 4.613541126251221,
      "learning_rate": 4.750784929356358e-05,
      "loss": 0.6661,
      "step": 254000
    },
    {
      "epoch": 3.989010989010989,
      "grad_norm": 4.159115791320801,
      "learning_rate": 4.750686813186813e-05,
      "loss": 0.6767,
      "step": 254100
    },
    {
      "epoch": 3.990580847723705,
      "grad_norm": 3.4979844093322754,
      "learning_rate": 4.750588697017268e-05,
      "loss": 0.7001,
      "step": 254200
    },
    {
      "epoch": 3.9921507064364206,
      "grad_norm": 3.6873319149017334,
      "learning_rate": 4.750490580847724e-05,
      "loss": 0.7058,
      "step": 254300
    },
    {
      "epoch": 3.9937205651491365,
      "grad_norm": 3.9393324851989746,
      "learning_rate": 4.750392464678179e-05,
      "loss": 0.6433,
      "step": 254400
    },
    {
      "epoch": 3.9952904238618525,
      "grad_norm": 3.6136252880096436,
      "learning_rate": 4.750294348508634e-05,
      "loss": 0.6764,
      "step": 254500
    },
    {
      "epoch": 3.996860282574568,
      "grad_norm": 4.232896327972412,
      "learning_rate": 4.7501962323390894e-05,
      "loss": 0.687,
      "step": 254600
    },
    {
      "epoch": 3.998430141287284,
      "grad_norm": 4.203738689422607,
      "learning_rate": 4.750098116169545e-05,
      "loss": 0.6885,
      "step": 254700
    },
    {
      "epoch": 4.0,
      "grad_norm": 4.2277727127075195,
      "learning_rate": 4.75e-05,
      "loss": 0.7046,
      "step": 254800
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.0351793766021729,
      "eval_runtime": 14.638,
      "eval_samples_per_second": 229.062,
      "eval_steps_per_second": 229.062,
      "step": 254800
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5359585881233215,
      "eval_runtime": 281.66,
      "eval_samples_per_second": 226.159,
      "eval_steps_per_second": 226.159,
      "step": 254800
    },
    {
      "epoch": 4.001569858712716,
      "grad_norm": 4.652597904205322,
      "learning_rate": 4.7499018838304554e-05,
      "loss": 0.6883,
      "step": 254900
    },
    {
      "epoch": 4.003139717425432,
      "grad_norm": 4.473802089691162,
      "learning_rate": 4.749803767660911e-05,
      "loss": 0.659,
      "step": 255000
    },
    {
      "epoch": 4.004709576138148,
      "grad_norm": 4.421225547790527,
      "learning_rate": 4.749705651491366e-05,
      "loss": 0.7047,
      "step": 255100
    },
    {
      "epoch": 4.006279434850863,
      "grad_norm": 3.649533987045288,
      "learning_rate": 4.749607535321821e-05,
      "loss": 0.6699,
      "step": 255200
    },
    {
      "epoch": 4.007849293563579,
      "grad_norm": 3.423956871032715,
      "learning_rate": 4.7495094191522764e-05,
      "loss": 0.6724,
      "step": 255300
    },
    {
      "epoch": 4.009419152276295,
      "grad_norm": 4.009871959686279,
      "learning_rate": 4.749411302982732e-05,
      "loss": 0.7282,
      "step": 255400
    },
    {
      "epoch": 4.010989010989011,
      "grad_norm": 3.26784348487854,
      "learning_rate": 4.7493131868131866e-05,
      "loss": 0.6547,
      "step": 255500
    },
    {
      "epoch": 4.012558869701727,
      "grad_norm": 5.00852108001709,
      "learning_rate": 4.7492150706436424e-05,
      "loss": 0.6945,
      "step": 255600
    },
    {
      "epoch": 4.014128728414443,
      "grad_norm": 4.189489364624023,
      "learning_rate": 4.7491169544740975e-05,
      "loss": 0.732,
      "step": 255700
    },
    {
      "epoch": 4.015698587127159,
      "grad_norm": 3.7587037086486816,
      "learning_rate": 4.749018838304553e-05,
      "loss": 0.6664,
      "step": 255800
    },
    {
      "epoch": 4.017268445839874,
      "grad_norm": 3.5895981788635254,
      "learning_rate": 4.748920722135008e-05,
      "loss": 0.6887,
      "step": 255900
    },
    {
      "epoch": 4.01883830455259,
      "grad_norm": 3.8509328365325928,
      "learning_rate": 4.7488226059654635e-05,
      "loss": 0.6697,
      "step": 256000
    },
    {
      "epoch": 4.020408163265306,
      "grad_norm": 4.560542583465576,
      "learning_rate": 4.7487244897959186e-05,
      "loss": 0.6679,
      "step": 256100
    },
    {
      "epoch": 4.021978021978022,
      "grad_norm": 4.668769359588623,
      "learning_rate": 4.748626373626374e-05,
      "loss": 0.6583,
      "step": 256200
    },
    {
      "epoch": 4.023547880690738,
      "grad_norm": 3.8709611892700195,
      "learning_rate": 4.748528257456829e-05,
      "loss": 0.7058,
      "step": 256300
    },
    {
      "epoch": 4.025117739403454,
      "grad_norm": 3.425837993621826,
      "learning_rate": 4.7484301412872845e-05,
      "loss": 0.589,
      "step": 256400
    },
    {
      "epoch": 4.02668759811617,
      "grad_norm": 4.167825222015381,
      "learning_rate": 4.7483320251177396e-05,
      "loss": 0.7065,
      "step": 256500
    },
    {
      "epoch": 4.028257456828886,
      "grad_norm": 4.7050299644470215,
      "learning_rate": 4.748233908948195e-05,
      "loss": 0.7077,
      "step": 256600
    },
    {
      "epoch": 4.029827315541601,
      "grad_norm": 4.5900797843933105,
      "learning_rate": 4.74813579277865e-05,
      "loss": 0.6771,
      "step": 256700
    },
    {
      "epoch": 4.031397174254317,
      "grad_norm": 4.051831245422363,
      "learning_rate": 4.7480376766091056e-05,
      "loss": 0.6586,
      "step": 256800
    },
    {
      "epoch": 4.032967032967033,
      "grad_norm": 4.991147994995117,
      "learning_rate": 4.747939560439561e-05,
      "loss": 0.6958,
      "step": 256900
    },
    {
      "epoch": 4.034536891679749,
      "grad_norm": 4.355092525482178,
      "learning_rate": 4.747841444270016e-05,
      "loss": 0.659,
      "step": 257000
    },
    {
      "epoch": 4.036106750392465,
      "grad_norm": 4.848819255828857,
      "learning_rate": 4.7477433281004716e-05,
      "loss": 0.6958,
      "step": 257100
    },
    {
      "epoch": 4.037676609105181,
      "grad_norm": 4.078448295593262,
      "learning_rate": 4.747645211930927e-05,
      "loss": 0.6554,
      "step": 257200
    },
    {
      "epoch": 4.039246467817897,
      "grad_norm": 3.550682783126831,
      "learning_rate": 4.747547095761382e-05,
      "loss": 0.6298,
      "step": 257300
    },
    {
      "epoch": 4.040816326530612,
      "grad_norm": 4.584277629852295,
      "learning_rate": 4.747448979591837e-05,
      "loss": 0.6659,
      "step": 257400
    },
    {
      "epoch": 4.042386185243328,
      "grad_norm": 4.343369960784912,
      "learning_rate": 4.7473508634222927e-05,
      "loss": 0.6814,
      "step": 257500
    },
    {
      "epoch": 4.043956043956044,
      "grad_norm": 5.031847953796387,
      "learning_rate": 4.747252747252747e-05,
      "loss": 0.671,
      "step": 257600
    },
    {
      "epoch": 4.04552590266876,
      "grad_norm": 4.493315696716309,
      "learning_rate": 4.747154631083203e-05,
      "loss": 0.6997,
      "step": 257700
    },
    {
      "epoch": 4.047095761381476,
      "grad_norm": 4.292971134185791,
      "learning_rate": 4.747056514913658e-05,
      "loss": 0.6701,
      "step": 257800
    },
    {
      "epoch": 4.048665620094192,
      "grad_norm": 4.678450107574463,
      "learning_rate": 4.746958398744114e-05,
      "loss": 0.6516,
      "step": 257900
    },
    {
      "epoch": 4.050235478806908,
      "grad_norm": 3.729290008544922,
      "learning_rate": 4.746860282574568e-05,
      "loss": 0.6631,
      "step": 258000
    },
    {
      "epoch": 4.051805337519623,
      "grad_norm": 4.327523708343506,
      "learning_rate": 4.746762166405024e-05,
      "loss": 0.6827,
      "step": 258100
    },
    {
      "epoch": 4.053375196232339,
      "grad_norm": 4.180704593658447,
      "learning_rate": 4.746664050235479e-05,
      "loss": 0.7583,
      "step": 258200
    },
    {
      "epoch": 4.054945054945055,
      "grad_norm": 3.947230577468872,
      "learning_rate": 4.746565934065934e-05,
      "loss": 0.6841,
      "step": 258300
    },
    {
      "epoch": 4.056514913657771,
      "grad_norm": 6.477135181427002,
      "learning_rate": 4.746467817896389e-05,
      "loss": 0.6546,
      "step": 258400
    },
    {
      "epoch": 4.058084772370487,
      "grad_norm": 4.452513694763184,
      "learning_rate": 4.746369701726845e-05,
      "loss": 0.6776,
      "step": 258500
    },
    {
      "epoch": 4.059654631083203,
      "grad_norm": 3.9585049152374268,
      "learning_rate": 4.7462715855573e-05,
      "loss": 0.6625,
      "step": 258600
    },
    {
      "epoch": 4.061224489795919,
      "grad_norm": 3.566209077835083,
      "learning_rate": 4.746173469387755e-05,
      "loss": 0.6949,
      "step": 258700
    },
    {
      "epoch": 4.062794348508635,
      "grad_norm": 4.17811393737793,
      "learning_rate": 4.74607535321821e-05,
      "loss": 0.6598,
      "step": 258800
    },
    {
      "epoch": 4.06436420722135,
      "grad_norm": 4.464097023010254,
      "learning_rate": 4.745977237048666e-05,
      "loss": 0.6543,
      "step": 258900
    },
    {
      "epoch": 4.065934065934066,
      "grad_norm": 4.497383117675781,
      "learning_rate": 4.745879120879121e-05,
      "loss": 0.6781,
      "step": 259000
    },
    {
      "epoch": 4.067503924646782,
      "grad_norm": 4.464968681335449,
      "learning_rate": 4.745781004709576e-05,
      "loss": 0.6891,
      "step": 259100
    },
    {
      "epoch": 4.069073783359498,
      "grad_norm": 3.5341737270355225,
      "learning_rate": 4.745682888540032e-05,
      "loss": 0.6338,
      "step": 259200
    },
    {
      "epoch": 4.070643642072214,
      "grad_norm": 4.278619289398193,
      "learning_rate": 4.745584772370487e-05,
      "loss": 0.7074,
      "step": 259300
    },
    {
      "epoch": 4.07221350078493,
      "grad_norm": 3.486733913421631,
      "learning_rate": 4.745486656200942e-05,
      "loss": 0.6687,
      "step": 259400
    },
    {
      "epoch": 4.073783359497646,
      "grad_norm": 4.703190803527832,
      "learning_rate": 4.745388540031397e-05,
      "loss": 0.7026,
      "step": 259500
    },
    {
      "epoch": 4.075353218210361,
      "grad_norm": 2.956265926361084,
      "learning_rate": 4.745290423861853e-05,
      "loss": 0.6672,
      "step": 259600
    },
    {
      "epoch": 4.076923076923077,
      "grad_norm": 4.285943508148193,
      "learning_rate": 4.7451923076923075e-05,
      "loss": 0.6698,
      "step": 259700
    },
    {
      "epoch": 4.078492935635793,
      "grad_norm": 3.535538911819458,
      "learning_rate": 4.745094191522763e-05,
      "loss": 0.6606,
      "step": 259800
    },
    {
      "epoch": 4.080062794348509,
      "grad_norm": 3.4179601669311523,
      "learning_rate": 4.7449960753532184e-05,
      "loss": 0.6947,
      "step": 259900
    },
    {
      "epoch": 4.081632653061225,
      "grad_norm": 3.1264286041259766,
      "learning_rate": 4.744897959183674e-05,
      "loss": 0.6935,
      "step": 260000
    },
    {
      "epoch": 4.083202511773941,
      "grad_norm": 3.4268972873687744,
      "learning_rate": 4.7447998430141286e-05,
      "loss": 0.6565,
      "step": 260100
    },
    {
      "epoch": 4.0847723704866565,
      "grad_norm": 3.51084566116333,
      "learning_rate": 4.7447017268445844e-05,
      "loss": 0.6598,
      "step": 260200
    },
    {
      "epoch": 4.086342229199372,
      "grad_norm": 4.6614766120910645,
      "learning_rate": 4.7446036106750395e-05,
      "loss": 0.68,
      "step": 260300
    },
    {
      "epoch": 4.087912087912088,
      "grad_norm": 4.457892417907715,
      "learning_rate": 4.7445054945054946e-05,
      "loss": 0.6758,
      "step": 260400
    },
    {
      "epoch": 4.089481946624804,
      "grad_norm": 3.222954273223877,
      "learning_rate": 4.74440737833595e-05,
      "loss": 0.6767,
      "step": 260500
    },
    {
      "epoch": 4.0910518053375196,
      "grad_norm": 4.1093220710754395,
      "learning_rate": 4.7443092621664054e-05,
      "loss": 0.6864,
      "step": 260600
    },
    {
      "epoch": 4.0926216640502355,
      "grad_norm": 4.353708267211914,
      "learning_rate": 4.7442111459968605e-05,
      "loss": 0.6974,
      "step": 260700
    },
    {
      "epoch": 4.0941915227629515,
      "grad_norm": 3.9501101970672607,
      "learning_rate": 4.7441130298273156e-05,
      "loss": 0.6785,
      "step": 260800
    },
    {
      "epoch": 4.0957613814756675,
      "grad_norm": 4.332975387573242,
      "learning_rate": 4.744014913657771e-05,
      "loss": 0.7005,
      "step": 260900
    },
    {
      "epoch": 4.0973312401883835,
      "grad_norm": 3.7465312480926514,
      "learning_rate": 4.7439167974882265e-05,
      "loss": 0.6901,
      "step": 261000
    },
    {
      "epoch": 4.0989010989010985,
      "grad_norm": 3.5787510871887207,
      "learning_rate": 4.7438186813186816e-05,
      "loss": 0.6525,
      "step": 261100
    },
    {
      "epoch": 4.1004709576138145,
      "grad_norm": 4.514106750488281,
      "learning_rate": 4.743720565149137e-05,
      "loss": 0.627,
      "step": 261200
    },
    {
      "epoch": 4.1020408163265305,
      "grad_norm": 3.5358171463012695,
      "learning_rate": 4.7436224489795925e-05,
      "loss": 0.6635,
      "step": 261300
    },
    {
      "epoch": 4.1036106750392465,
      "grad_norm": 4.242919921875,
      "learning_rate": 4.7435243328100476e-05,
      "loss": 0.736,
      "step": 261400
    },
    {
      "epoch": 4.1051805337519625,
      "grad_norm": 2.896510601043701,
      "learning_rate": 4.743426216640503e-05,
      "loss": 0.6688,
      "step": 261500
    },
    {
      "epoch": 4.106750392464678,
      "grad_norm": 4.713226795196533,
      "learning_rate": 4.743328100470958e-05,
      "loss": 0.6674,
      "step": 261600
    },
    {
      "epoch": 4.108320251177394,
      "grad_norm": 3.4386277198791504,
      "learning_rate": 4.7432299843014136e-05,
      "loss": 0.6404,
      "step": 261700
    },
    {
      "epoch": 4.1098901098901095,
      "grad_norm": 3.5711352825164795,
      "learning_rate": 4.743131868131868e-05,
      "loss": 0.6886,
      "step": 261800
    },
    {
      "epoch": 4.1114599686028255,
      "grad_norm": 4.252394199371338,
      "learning_rate": 4.743033751962324e-05,
      "loss": 0.7279,
      "step": 261900
    },
    {
      "epoch": 4.1130298273155415,
      "grad_norm": 4.266851902008057,
      "learning_rate": 4.742935635792779e-05,
      "loss": 0.693,
      "step": 262000
    },
    {
      "epoch": 4.114599686028257,
      "grad_norm": 2.964576482772827,
      "learning_rate": 4.742837519623234e-05,
      "loss": 0.6615,
      "step": 262100
    },
    {
      "epoch": 4.116169544740973,
      "grad_norm": 4.477406024932861,
      "learning_rate": 4.742739403453689e-05,
      "loss": 0.6948,
      "step": 262200
    },
    {
      "epoch": 4.117739403453689,
      "grad_norm": 4.156589984893799,
      "learning_rate": 4.742641287284145e-05,
      "loss": 0.713,
      "step": 262300
    },
    {
      "epoch": 4.119309262166405,
      "grad_norm": 3.3142752647399902,
      "learning_rate": 4.7425431711146e-05,
      "loss": 0.6975,
      "step": 262400
    },
    {
      "epoch": 4.1208791208791204,
      "grad_norm": 3.7178261280059814,
      "learning_rate": 4.742445054945055e-05,
      "loss": 0.634,
      "step": 262500
    },
    {
      "epoch": 4.122448979591836,
      "grad_norm": 4.398045539855957,
      "learning_rate": 4.74234693877551e-05,
      "loss": 0.6813,
      "step": 262600
    },
    {
      "epoch": 4.124018838304552,
      "grad_norm": 4.4338059425354,
      "learning_rate": 4.742248822605966e-05,
      "loss": 0.665,
      "step": 262700
    },
    {
      "epoch": 4.125588697017268,
      "grad_norm": 3.8700103759765625,
      "learning_rate": 4.742150706436421e-05,
      "loss": 0.6731,
      "step": 262800
    },
    {
      "epoch": 4.127158555729984,
      "grad_norm": 3.9259886741638184,
      "learning_rate": 4.742052590266876e-05,
      "loss": 0.6988,
      "step": 262900
    },
    {
      "epoch": 4.1287284144427,
      "grad_norm": 4.242665767669678,
      "learning_rate": 4.741954474097331e-05,
      "loss": 0.6763,
      "step": 263000
    },
    {
      "epoch": 4.130298273155416,
      "grad_norm": 2.934295177459717,
      "learning_rate": 4.741856357927787e-05,
      "loss": 0.6694,
      "step": 263100
    },
    {
      "epoch": 4.131868131868132,
      "grad_norm": 3.5737879276275635,
      "learning_rate": 4.741758241758242e-05,
      "loss": 0.6844,
      "step": 263200
    },
    {
      "epoch": 4.133437990580847,
      "grad_norm": 4.007653713226318,
      "learning_rate": 4.741660125588697e-05,
      "loss": 0.6644,
      "step": 263300
    },
    {
      "epoch": 4.135007849293563,
      "grad_norm": 4.156789302825928,
      "learning_rate": 4.741562009419153e-05,
      "loss": 0.6811,
      "step": 263400
    },
    {
      "epoch": 4.136577708006279,
      "grad_norm": 4.594313621520996,
      "learning_rate": 4.7414638932496074e-05,
      "loss": 0.6493,
      "step": 263500
    },
    {
      "epoch": 4.138147566718995,
      "grad_norm": 4.743560314178467,
      "learning_rate": 4.741365777080063e-05,
      "loss": 0.7046,
      "step": 263600
    },
    {
      "epoch": 4.139717425431711,
      "grad_norm": 4.216530799865723,
      "learning_rate": 4.741267660910518e-05,
      "loss": 0.7139,
      "step": 263700
    },
    {
      "epoch": 4.141287284144427,
      "grad_norm": 4.316485404968262,
      "learning_rate": 4.741169544740974e-05,
      "loss": 0.6796,
      "step": 263800
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 4.177703857421875,
      "learning_rate": 4.7410714285714284e-05,
      "loss": 0.6888,
      "step": 263900
    },
    {
      "epoch": 4.144427001569858,
      "grad_norm": 4.221890449523926,
      "learning_rate": 4.740973312401884e-05,
      "loss": 0.6805,
      "step": 264000
    },
    {
      "epoch": 4.145996860282574,
      "grad_norm": 4.673862457275391,
      "learning_rate": 4.740875196232339e-05,
      "loss": 0.6794,
      "step": 264100
    },
    {
      "epoch": 4.14756671899529,
      "grad_norm": 4.927812576293945,
      "learning_rate": 4.7407770800627944e-05,
      "loss": 0.6934,
      "step": 264200
    },
    {
      "epoch": 4.149136577708006,
      "grad_norm": 3.4154303073883057,
      "learning_rate": 4.7406789638932495e-05,
      "loss": 0.6741,
      "step": 264300
    },
    {
      "epoch": 4.150706436420722,
      "grad_norm": 4.150356769561768,
      "learning_rate": 4.740580847723705e-05,
      "loss": 0.718,
      "step": 264400
    },
    {
      "epoch": 4.152276295133438,
      "grad_norm": 4.220087051391602,
      "learning_rate": 4.7404827315541604e-05,
      "loss": 0.7004,
      "step": 264500
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 3.4627060890197754,
      "learning_rate": 4.7403846153846155e-05,
      "loss": 0.6861,
      "step": 264600
    },
    {
      "epoch": 4.155416012558869,
      "grad_norm": 4.992697715759277,
      "learning_rate": 4.7402864992150706e-05,
      "loss": 0.6693,
      "step": 264700
    },
    {
      "epoch": 4.156985871271585,
      "grad_norm": 3.1079962253570557,
      "learning_rate": 4.740188383045526e-05,
      "loss": 0.6694,
      "step": 264800
    },
    {
      "epoch": 4.158555729984301,
      "grad_norm": 4.7777838706970215,
      "learning_rate": 4.7400902668759814e-05,
      "loss": 0.6734,
      "step": 264900
    },
    {
      "epoch": 4.160125588697017,
      "grad_norm": 4.102451324462891,
      "learning_rate": 4.7399921507064365e-05,
      "loss": 0.6685,
      "step": 265000
    },
    {
      "epoch": 4.161695447409733,
      "grad_norm": 4.765937805175781,
      "learning_rate": 4.7398940345368916e-05,
      "loss": 0.6848,
      "step": 265100
    },
    {
      "epoch": 4.163265306122449,
      "grad_norm": 3.9578027725219727,
      "learning_rate": 4.7397959183673474e-05,
      "loss": 0.6754,
      "step": 265200
    },
    {
      "epoch": 4.164835164835165,
      "grad_norm": 4.246636867523193,
      "learning_rate": 4.7396978021978025e-05,
      "loss": 0.6886,
      "step": 265300
    },
    {
      "epoch": 4.166405023547881,
      "grad_norm": 5.423036098480225,
      "learning_rate": 4.7395996860282576e-05,
      "loss": 0.6431,
      "step": 265400
    },
    {
      "epoch": 4.167974882260596,
      "grad_norm": 4.544802665710449,
      "learning_rate": 4.7395015698587134e-05,
      "loss": 0.6629,
      "step": 265500
    },
    {
      "epoch": 4.169544740973312,
      "grad_norm": 4.334299564361572,
      "learning_rate": 4.739403453689168e-05,
      "loss": 0.6857,
      "step": 265600
    },
    {
      "epoch": 4.171114599686028,
      "grad_norm": 3.8768105506896973,
      "learning_rate": 4.7393053375196236e-05,
      "loss": 0.7176,
      "step": 265700
    },
    {
      "epoch": 4.172684458398744,
      "grad_norm": 4.464404582977295,
      "learning_rate": 4.739207221350079e-05,
      "loss": 0.7111,
      "step": 265800
    },
    {
      "epoch": 4.17425431711146,
      "grad_norm": 5.05095100402832,
      "learning_rate": 4.7391091051805345e-05,
      "loss": 0.6943,
      "step": 265900
    },
    {
      "epoch": 4.175824175824176,
      "grad_norm": 4.118257522583008,
      "learning_rate": 4.739010989010989e-05,
      "loss": 0.6754,
      "step": 266000
    },
    {
      "epoch": 4.177394034536892,
      "grad_norm": 3.7404751777648926,
      "learning_rate": 4.7389128728414446e-05,
      "loss": 0.7042,
      "step": 266100
    },
    {
      "epoch": 4.178963893249607,
      "grad_norm": 4.610115051269531,
      "learning_rate": 4.7388147566719e-05,
      "loss": 0.6897,
      "step": 266200
    },
    {
      "epoch": 4.180533751962323,
      "grad_norm": 3.7864487171173096,
      "learning_rate": 4.738716640502355e-05,
      "loss": 0.6373,
      "step": 266300
    },
    {
      "epoch": 4.182103610675039,
      "grad_norm": 4.403534412384033,
      "learning_rate": 4.73861852433281e-05,
      "loss": 0.6715,
      "step": 266400
    },
    {
      "epoch": 4.183673469387755,
      "grad_norm": 3.2147576808929443,
      "learning_rate": 4.738520408163266e-05,
      "loss": 0.6999,
      "step": 266500
    },
    {
      "epoch": 4.185243328100471,
      "grad_norm": 3.9481735229492188,
      "learning_rate": 4.738422291993721e-05,
      "loss": 0.6551,
      "step": 266600
    },
    {
      "epoch": 4.186813186813187,
      "grad_norm": 4.737280368804932,
      "learning_rate": 4.738324175824176e-05,
      "loss": 0.7124,
      "step": 266700
    },
    {
      "epoch": 4.188383045525903,
      "grad_norm": 3.4166738986968994,
      "learning_rate": 4.738226059654631e-05,
      "loss": 0.6817,
      "step": 266800
    },
    {
      "epoch": 4.189952904238618,
      "grad_norm": 3.2787094116210938,
      "learning_rate": 4.738127943485087e-05,
      "loss": 0.7114,
      "step": 266900
    },
    {
      "epoch": 4.191522762951334,
      "grad_norm": 3.6831183433532715,
      "learning_rate": 4.738029827315542e-05,
      "loss": 0.6896,
      "step": 267000
    },
    {
      "epoch": 4.19309262166405,
      "grad_norm": 3.4388022422790527,
      "learning_rate": 4.737931711145997e-05,
      "loss": 0.6365,
      "step": 267100
    },
    {
      "epoch": 4.194662480376766,
      "grad_norm": 4.535200595855713,
      "learning_rate": 4.737833594976452e-05,
      "loss": 0.7065,
      "step": 267200
    },
    {
      "epoch": 4.196232339089482,
      "grad_norm": 3.33835506439209,
      "learning_rate": 4.737735478806908e-05,
      "loss": 0.6918,
      "step": 267300
    },
    {
      "epoch": 4.197802197802198,
      "grad_norm": 4.0304436683654785,
      "learning_rate": 4.737637362637363e-05,
      "loss": 0.6463,
      "step": 267400
    },
    {
      "epoch": 4.199372056514914,
      "grad_norm": 3.732553005218506,
      "learning_rate": 4.737539246467818e-05,
      "loss": 0.6611,
      "step": 267500
    },
    {
      "epoch": 4.20094191522763,
      "grad_norm": 4.089311122894287,
      "learning_rate": 4.737441130298274e-05,
      "loss": 0.6878,
      "step": 267600
    },
    {
      "epoch": 4.202511773940345,
      "grad_norm": 4.370321273803711,
      "learning_rate": 4.737343014128728e-05,
      "loss": 0.6995,
      "step": 267700
    },
    {
      "epoch": 4.204081632653061,
      "grad_norm": 4.479807376861572,
      "learning_rate": 4.737244897959184e-05,
      "loss": 0.6555,
      "step": 267800
    },
    {
      "epoch": 4.205651491365777,
      "grad_norm": 2.9668707847595215,
      "learning_rate": 4.737146781789639e-05,
      "loss": 0.6566,
      "step": 267900
    },
    {
      "epoch": 4.207221350078493,
      "grad_norm": 4.021942615509033,
      "learning_rate": 4.737048665620095e-05,
      "loss": 0.6543,
      "step": 268000
    },
    {
      "epoch": 4.208791208791209,
      "grad_norm": 3.7587828636169434,
      "learning_rate": 4.736950549450549e-05,
      "loss": 0.6577,
      "step": 268100
    },
    {
      "epoch": 4.210361067503925,
      "grad_norm": 4.482262134552002,
      "learning_rate": 4.736852433281005e-05,
      "loss": 0.6304,
      "step": 268200
    },
    {
      "epoch": 4.211930926216641,
      "grad_norm": 3.727091073989868,
      "learning_rate": 4.73675431711146e-05,
      "loss": 0.6833,
      "step": 268300
    },
    {
      "epoch": 4.213500784929356,
      "grad_norm": 3.9050674438476562,
      "learning_rate": 4.736656200941915e-05,
      "loss": 0.6958,
      "step": 268400
    },
    {
      "epoch": 4.215070643642072,
      "grad_norm": 3.674335241317749,
      "learning_rate": 4.7365580847723704e-05,
      "loss": 0.6986,
      "step": 268500
    },
    {
      "epoch": 4.216640502354788,
      "grad_norm": 4.6823601722717285,
      "learning_rate": 4.736459968602826e-05,
      "loss": 0.6984,
      "step": 268600
    },
    {
      "epoch": 4.218210361067504,
      "grad_norm": 3.1742818355560303,
      "learning_rate": 4.736361852433281e-05,
      "loss": 0.6779,
      "step": 268700
    },
    {
      "epoch": 4.21978021978022,
      "grad_norm": 4.317164421081543,
      "learning_rate": 4.7362637362637364e-05,
      "loss": 0.6572,
      "step": 268800
    },
    {
      "epoch": 4.221350078492936,
      "grad_norm": 3.8170523643493652,
      "learning_rate": 4.7361656200941915e-05,
      "loss": 0.7151,
      "step": 268900
    },
    {
      "epoch": 4.222919937205652,
      "grad_norm": 4.351582050323486,
      "learning_rate": 4.736067503924647e-05,
      "loss": 0.6668,
      "step": 269000
    },
    {
      "epoch": 4.224489795918367,
      "grad_norm": 4.22592306137085,
      "learning_rate": 4.735969387755102e-05,
      "loss": 0.6658,
      "step": 269100
    },
    {
      "epoch": 4.226059654631083,
      "grad_norm": 4.175691604614258,
      "learning_rate": 4.7358712715855574e-05,
      "loss": 0.6869,
      "step": 269200
    },
    {
      "epoch": 4.227629513343799,
      "grad_norm": 4.722596645355225,
      "learning_rate": 4.7357731554160125e-05,
      "loss": 0.7211,
      "step": 269300
    },
    {
      "epoch": 4.229199372056515,
      "grad_norm": 3.648923635482788,
      "learning_rate": 4.735675039246468e-05,
      "loss": 0.6985,
      "step": 269400
    },
    {
      "epoch": 4.230769230769231,
      "grad_norm": 3.417931079864502,
      "learning_rate": 4.7355769230769234e-05,
      "loss": 0.6789,
      "step": 269500
    },
    {
      "epoch": 4.232339089481947,
      "grad_norm": 4.277522563934326,
      "learning_rate": 4.7354788069073785e-05,
      "loss": 0.7213,
      "step": 269600
    },
    {
      "epoch": 4.233908948194663,
      "grad_norm": 3.273987293243408,
      "learning_rate": 4.735380690737834e-05,
      "loss": 0.7359,
      "step": 269700
    },
    {
      "epoch": 4.235478806907379,
      "grad_norm": 5.245060920715332,
      "learning_rate": 4.735282574568289e-05,
      "loss": 0.6619,
      "step": 269800
    },
    {
      "epoch": 4.237048665620094,
      "grad_norm": 3.1064109802246094,
      "learning_rate": 4.7351844583987445e-05,
      "loss": 0.708,
      "step": 269900
    },
    {
      "epoch": 4.23861852433281,
      "grad_norm": 3.5815956592559814,
      "learning_rate": 4.7350863422291996e-05,
      "loss": 0.6828,
      "step": 270000
    },
    {
      "epoch": 4.240188383045526,
      "grad_norm": 4.49785852432251,
      "learning_rate": 4.7349882260596553e-05,
      "loss": 0.6432,
      "step": 270100
    },
    {
      "epoch": 4.241758241758242,
      "grad_norm": 3.5908422470092773,
      "learning_rate": 4.73489010989011e-05,
      "loss": 0.6772,
      "step": 270200
    },
    {
      "epoch": 4.243328100470958,
      "grad_norm": 4.232873439788818,
      "learning_rate": 4.7347919937205655e-05,
      "loss": 0.7253,
      "step": 270300
    },
    {
      "epoch": 4.244897959183674,
      "grad_norm": 3.985429048538208,
      "learning_rate": 4.7346938775510206e-05,
      "loss": 0.6481,
      "step": 270400
    },
    {
      "epoch": 4.24646781789639,
      "grad_norm": 4.560540676116943,
      "learning_rate": 4.734595761381476e-05,
      "loss": 0.679,
      "step": 270500
    },
    {
      "epoch": 4.248037676609105,
      "grad_norm": 4.014875888824463,
      "learning_rate": 4.734497645211931e-05,
      "loss": 0.6506,
      "step": 270600
    },
    {
      "epoch": 4.249607535321821,
      "grad_norm": 4.70601224899292,
      "learning_rate": 4.7343995290423866e-05,
      "loss": 0.6565,
      "step": 270700
    },
    {
      "epoch": 4.251177394034537,
      "grad_norm": 4.508305072784424,
      "learning_rate": 4.734301412872842e-05,
      "loss": 0.6902,
      "step": 270800
    },
    {
      "epoch": 4.252747252747253,
      "grad_norm": 4.8132171630859375,
      "learning_rate": 4.734203296703297e-05,
      "loss": 0.6394,
      "step": 270900
    },
    {
      "epoch": 4.254317111459969,
      "grad_norm": 3.857975482940674,
      "learning_rate": 4.734105180533752e-05,
      "loss": 0.6693,
      "step": 271000
    },
    {
      "epoch": 4.255886970172685,
      "grad_norm": 3.8961634635925293,
      "learning_rate": 4.734007064364208e-05,
      "loss": 0.7003,
      "step": 271100
    },
    {
      "epoch": 4.257456828885401,
      "grad_norm": 3.2538836002349854,
      "learning_rate": 4.733908948194663e-05,
      "loss": 0.6951,
      "step": 271200
    },
    {
      "epoch": 4.259026687598116,
      "grad_norm": 4.447409629821777,
      "learning_rate": 4.733810832025118e-05,
      "loss": 0.6862,
      "step": 271300
    },
    {
      "epoch": 4.260596546310832,
      "grad_norm": 3.7799196243286133,
      "learning_rate": 4.733712715855573e-05,
      "loss": 0.6599,
      "step": 271400
    },
    {
      "epoch": 4.262166405023548,
      "grad_norm": 4.229420185089111,
      "learning_rate": 4.733614599686029e-05,
      "loss": 0.6875,
      "step": 271500
    },
    {
      "epoch": 4.263736263736264,
      "grad_norm": 3.8635332584381104,
      "learning_rate": 4.733516483516484e-05,
      "loss": 0.6539,
      "step": 271600
    },
    {
      "epoch": 4.26530612244898,
      "grad_norm": 4.644077777862549,
      "learning_rate": 4.733418367346939e-05,
      "loss": 0.6831,
      "step": 271700
    },
    {
      "epoch": 4.266875981161696,
      "grad_norm": 2.665104866027832,
      "learning_rate": 4.733320251177395e-05,
      "loss": 0.67,
      "step": 271800
    },
    {
      "epoch": 4.268445839874412,
      "grad_norm": 4.423269748687744,
      "learning_rate": 4.733222135007849e-05,
      "loss": 0.6321,
      "step": 271900
    },
    {
      "epoch": 4.270015698587127,
      "grad_norm": 4.0022053718566895,
      "learning_rate": 4.733124018838305e-05,
      "loss": 0.6369,
      "step": 272000
    },
    {
      "epoch": 4.271585557299843,
      "grad_norm": 3.0974130630493164,
      "learning_rate": 4.73302590266876e-05,
      "loss": 0.7127,
      "step": 272100
    },
    {
      "epoch": 4.273155416012559,
      "grad_norm": 4.650594711303711,
      "learning_rate": 4.732927786499216e-05,
      "loss": 0.7244,
      "step": 272200
    },
    {
      "epoch": 4.274725274725275,
      "grad_norm": 4.2395429611206055,
      "learning_rate": 4.73282967032967e-05,
      "loss": 0.695,
      "step": 272300
    },
    {
      "epoch": 4.276295133437991,
      "grad_norm": 4.10556697845459,
      "learning_rate": 4.732731554160126e-05,
      "loss": 0.6841,
      "step": 272400
    },
    {
      "epoch": 4.277864992150707,
      "grad_norm": 4.339336395263672,
      "learning_rate": 4.732633437990581e-05,
      "loss": 0.7019,
      "step": 272500
    },
    {
      "epoch": 4.279434850863423,
      "grad_norm": 3.325669288635254,
      "learning_rate": 4.732535321821036e-05,
      "loss": 0.6673,
      "step": 272600
    },
    {
      "epoch": 4.2810047095761385,
      "grad_norm": 2.954998016357422,
      "learning_rate": 4.732437205651491e-05,
      "loss": 0.7128,
      "step": 272700
    },
    {
      "epoch": 4.282574568288854,
      "grad_norm": 5.345810413360596,
      "learning_rate": 4.732339089481947e-05,
      "loss": 0.6503,
      "step": 272800
    },
    {
      "epoch": 4.28414442700157,
      "grad_norm": 4.440383434295654,
      "learning_rate": 4.732240973312402e-05,
      "loss": 0.7068,
      "step": 272900
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 4.672170162200928,
      "learning_rate": 4.732142857142857e-05,
      "loss": 0.6273,
      "step": 273000
    },
    {
      "epoch": 4.287284144427002,
      "grad_norm": 4.253457069396973,
      "learning_rate": 4.7320447409733124e-05,
      "loss": 0.6544,
      "step": 273100
    },
    {
      "epoch": 4.2888540031397175,
      "grad_norm": 4.292364597320557,
      "learning_rate": 4.731946624803768e-05,
      "loss": 0.6799,
      "step": 273200
    },
    {
      "epoch": 4.2904238618524335,
      "grad_norm": 3.510920763015747,
      "learning_rate": 4.731848508634223e-05,
      "loss": 0.6406,
      "step": 273300
    },
    {
      "epoch": 4.2919937205651495,
      "grad_norm": 4.070984840393066,
      "learning_rate": 4.731750392464678e-05,
      "loss": 0.6688,
      "step": 273400
    },
    {
      "epoch": 4.293563579277865,
      "grad_norm": 4.1145710945129395,
      "learning_rate": 4.7316522762951334e-05,
      "loss": 0.6798,
      "step": 273500
    },
    {
      "epoch": 4.295133437990581,
      "grad_norm": 4.897317409515381,
      "learning_rate": 4.731554160125589e-05,
      "loss": 0.6511,
      "step": 273600
    },
    {
      "epoch": 4.2967032967032965,
      "grad_norm": 4.7121195793151855,
      "learning_rate": 4.731456043956044e-05,
      "loss": 0.6555,
      "step": 273700
    },
    {
      "epoch": 4.2982731554160125,
      "grad_norm": 4.848147392272949,
      "learning_rate": 4.7313579277864994e-05,
      "loss": 0.6904,
      "step": 273800
    },
    {
      "epoch": 4.2998430141287285,
      "grad_norm": 4.857992172241211,
      "learning_rate": 4.731259811616955e-05,
      "loss": 0.6674,
      "step": 273900
    },
    {
      "epoch": 4.3014128728414445,
      "grad_norm": 3.346876621246338,
      "learning_rate": 4.7311616954474096e-05,
      "loss": 0.6912,
      "step": 274000
    },
    {
      "epoch": 4.3029827315541604,
      "grad_norm": 3.9647319316864014,
      "learning_rate": 4.7310635792778654e-05,
      "loss": 0.6764,
      "step": 274100
    },
    {
      "epoch": 4.304552590266876,
      "grad_norm": 3.324350357055664,
      "learning_rate": 4.7309654631083205e-05,
      "loss": 0.6815,
      "step": 274200
    },
    {
      "epoch": 4.3061224489795915,
      "grad_norm": 3.343475818634033,
      "learning_rate": 4.730867346938776e-05,
      "loss": 0.6654,
      "step": 274300
    },
    {
      "epoch": 4.3076923076923075,
      "grad_norm": 3.995119094848633,
      "learning_rate": 4.730769230769231e-05,
      "loss": 0.6938,
      "step": 274400
    },
    {
      "epoch": 4.3092621664050235,
      "grad_norm": 4.459177494049072,
      "learning_rate": 4.7306711145996864e-05,
      "loss": 0.6657,
      "step": 274500
    },
    {
      "epoch": 4.310832025117739,
      "grad_norm": 4.318443775177002,
      "learning_rate": 4.7305729984301415e-05,
      "loss": 0.6455,
      "step": 274600
    },
    {
      "epoch": 4.312401883830455,
      "grad_norm": 3.9016025066375732,
      "learning_rate": 4.7304748822605966e-05,
      "loss": 0.6398,
      "step": 274700
    },
    {
      "epoch": 4.313971742543171,
      "grad_norm": 4.621609687805176,
      "learning_rate": 4.730376766091052e-05,
      "loss": 0.7038,
      "step": 274800
    },
    {
      "epoch": 4.315541601255887,
      "grad_norm": 3.9087862968444824,
      "learning_rate": 4.7302786499215075e-05,
      "loss": 0.709,
      "step": 274900
    },
    {
      "epoch": 4.3171114599686025,
      "grad_norm": 3.638404369354248,
      "learning_rate": 4.7301805337519626e-05,
      "loss": 0.6707,
      "step": 275000
    },
    {
      "epoch": 4.318681318681318,
      "grad_norm": 5.272716999053955,
      "learning_rate": 4.730082417582418e-05,
      "loss": 0.6815,
      "step": 275100
    },
    {
      "epoch": 4.320251177394034,
      "grad_norm": 3.518348455429077,
      "learning_rate": 4.729984301412873e-05,
      "loss": 0.6948,
      "step": 275200
    },
    {
      "epoch": 4.32182103610675,
      "grad_norm": 3.466677665710449,
      "learning_rate": 4.7298861852433286e-05,
      "loss": 0.6365,
      "step": 275300
    },
    {
      "epoch": 4.323390894819466,
      "grad_norm": 4.550163745880127,
      "learning_rate": 4.729788069073784e-05,
      "loss": 0.6626,
      "step": 275400
    },
    {
      "epoch": 4.324960753532182,
      "grad_norm": 4.513132095336914,
      "learning_rate": 4.729689952904239e-05,
      "loss": 0.7282,
      "step": 275500
    },
    {
      "epoch": 4.326530612244898,
      "grad_norm": 4.420587062835693,
      "learning_rate": 4.729591836734694e-05,
      "loss": 0.687,
      "step": 275600
    },
    {
      "epoch": 4.328100470957613,
      "grad_norm": 3.7653167247772217,
      "learning_rate": 4.7294937205651497e-05,
      "loss": 0.6789,
      "step": 275700
    },
    {
      "epoch": 4.329670329670329,
      "grad_norm": 3.9975199699401855,
      "learning_rate": 4.729395604395605e-05,
      "loss": 0.648,
      "step": 275800
    },
    {
      "epoch": 4.331240188383045,
      "grad_norm": 3.6646153926849365,
      "learning_rate": 4.72929748822606e-05,
      "loss": 0.6924,
      "step": 275900
    },
    {
      "epoch": 4.332810047095761,
      "grad_norm": 3.964341640472412,
      "learning_rate": 4.7291993720565156e-05,
      "loss": 0.6838,
      "step": 276000
    },
    {
      "epoch": 4.334379905808477,
      "grad_norm": 5.312320232391357,
      "learning_rate": 4.72910125588697e-05,
      "loss": 0.6872,
      "step": 276100
    },
    {
      "epoch": 4.335949764521193,
      "grad_norm": 4.8357391357421875,
      "learning_rate": 4.729003139717426e-05,
      "loss": 0.6899,
      "step": 276200
    },
    {
      "epoch": 4.337519623233909,
      "grad_norm": 3.9009928703308105,
      "learning_rate": 4.728905023547881e-05,
      "loss": 0.6566,
      "step": 276300
    },
    {
      "epoch": 4.339089481946624,
      "grad_norm": 3.8670313358306885,
      "learning_rate": 4.728806907378337e-05,
      "loss": 0.6491,
      "step": 276400
    },
    {
      "epoch": 4.34065934065934,
      "grad_norm": 4.62401819229126,
      "learning_rate": 4.728708791208791e-05,
      "loss": 0.7059,
      "step": 276500
    },
    {
      "epoch": 4.342229199372056,
      "grad_norm": 4.564660549163818,
      "learning_rate": 4.728610675039247e-05,
      "loss": 0.6504,
      "step": 276600
    },
    {
      "epoch": 4.343799058084772,
      "grad_norm": 3.418977975845337,
      "learning_rate": 4.728512558869702e-05,
      "loss": 0.6777,
      "step": 276700
    },
    {
      "epoch": 4.345368916797488,
      "grad_norm": 3.3036065101623535,
      "learning_rate": 4.728414442700157e-05,
      "loss": 0.7051,
      "step": 276800
    },
    {
      "epoch": 4.346938775510204,
      "grad_norm": 4.082484722137451,
      "learning_rate": 4.728316326530612e-05,
      "loss": 0.6663,
      "step": 276900
    },
    {
      "epoch": 4.34850863422292,
      "grad_norm": 2.474083185195923,
      "learning_rate": 4.728218210361068e-05,
      "loss": 0.6643,
      "step": 277000
    },
    {
      "epoch": 4.350078492935636,
      "grad_norm": 3.5971100330352783,
      "learning_rate": 4.728120094191523e-05,
      "loss": 0.6667,
      "step": 277100
    },
    {
      "epoch": 4.351648351648351,
      "grad_norm": 3.9134743213653564,
      "learning_rate": 4.728021978021978e-05,
      "loss": 0.6843,
      "step": 277200
    },
    {
      "epoch": 4.353218210361067,
      "grad_norm": 4.962235450744629,
      "learning_rate": 4.727923861852433e-05,
      "loss": 0.6548,
      "step": 277300
    },
    {
      "epoch": 4.354788069073783,
      "grad_norm": 4.082308769226074,
      "learning_rate": 4.727825745682889e-05,
      "loss": 0.6654,
      "step": 277400
    },
    {
      "epoch": 4.356357927786499,
      "grad_norm": 4.111719131469727,
      "learning_rate": 4.727727629513344e-05,
      "loss": 0.6554,
      "step": 277500
    },
    {
      "epoch": 4.357927786499215,
      "grad_norm": 4.009565830230713,
      "learning_rate": 4.727629513343799e-05,
      "loss": 0.672,
      "step": 277600
    },
    {
      "epoch": 4.359497645211931,
      "grad_norm": 3.796727418899536,
      "learning_rate": 4.727531397174254e-05,
      "loss": 0.674,
      "step": 277700
    },
    {
      "epoch": 4.361067503924647,
      "grad_norm": 4.477943420410156,
      "learning_rate": 4.72743328100471e-05,
      "loss": 0.7013,
      "step": 277800
    },
    {
      "epoch": 4.362637362637362,
      "grad_norm": 3.933851718902588,
      "learning_rate": 4.727335164835165e-05,
      "loss": 0.6787,
      "step": 277900
    },
    {
      "epoch": 4.364207221350078,
      "grad_norm": 3.417968988418579,
      "learning_rate": 4.72723704866562e-05,
      "loss": 0.6561,
      "step": 278000
    },
    {
      "epoch": 4.365777080062794,
      "grad_norm": 3.5638020038604736,
      "learning_rate": 4.727138932496076e-05,
      "loss": 0.7058,
      "step": 278100
    },
    {
      "epoch": 4.36734693877551,
      "grad_norm": 4.163540840148926,
      "learning_rate": 4.7270408163265305e-05,
      "loss": 0.6413,
      "step": 278200
    },
    {
      "epoch": 4.368916797488226,
      "grad_norm": 4.11171293258667,
      "learning_rate": 4.726942700156986e-05,
      "loss": 0.676,
      "step": 278300
    },
    {
      "epoch": 4.370486656200942,
      "grad_norm": 4.407525539398193,
      "learning_rate": 4.7268445839874414e-05,
      "loss": 0.6704,
      "step": 278400
    },
    {
      "epoch": 4.372056514913658,
      "grad_norm": 3.6430282592773438,
      "learning_rate": 4.726746467817897e-05,
      "loss": 0.6754,
      "step": 278500
    },
    {
      "epoch": 4.373626373626374,
      "grad_norm": 2.7906055450439453,
      "learning_rate": 4.7266483516483516e-05,
      "loss": 0.7045,
      "step": 278600
    },
    {
      "epoch": 4.375196232339089,
      "grad_norm": 3.592686653137207,
      "learning_rate": 4.7265502354788073e-05,
      "loss": 0.6924,
      "step": 278700
    },
    {
      "epoch": 4.376766091051805,
      "grad_norm": 3.4248344898223877,
      "learning_rate": 4.7264521193092624e-05,
      "loss": 0.6669,
      "step": 278800
    },
    {
      "epoch": 4.378335949764521,
      "grad_norm": 3.9714596271514893,
      "learning_rate": 4.7263540031397175e-05,
      "loss": 0.7098,
      "step": 278900
    },
    {
      "epoch": 4.379905808477237,
      "grad_norm": 3.9093105792999268,
      "learning_rate": 4.7262558869701726e-05,
      "loss": 0.6753,
      "step": 279000
    },
    {
      "epoch": 4.381475667189953,
      "grad_norm": 3.0989701747894287,
      "learning_rate": 4.7261577708006284e-05,
      "loss": 0.7026,
      "step": 279100
    },
    {
      "epoch": 4.383045525902669,
      "grad_norm": 4.337825775146484,
      "learning_rate": 4.7260596546310835e-05,
      "loss": 0.6898,
      "step": 279200
    },
    {
      "epoch": 4.384615384615385,
      "grad_norm": 4.5941691398620605,
      "learning_rate": 4.7259615384615386e-05,
      "loss": 0.6754,
      "step": 279300
    },
    {
      "epoch": 4.3861852433281,
      "grad_norm": 4.119044303894043,
      "learning_rate": 4.725863422291994e-05,
      "loss": 0.6716,
      "step": 279400
    },
    {
      "epoch": 4.387755102040816,
      "grad_norm": 4.950125694274902,
      "learning_rate": 4.7257653061224495e-05,
      "loss": 0.6648,
      "step": 279500
    },
    {
      "epoch": 4.389324960753532,
      "grad_norm": 4.502707481384277,
      "learning_rate": 4.7256671899529046e-05,
      "loss": 0.7029,
      "step": 279600
    },
    {
      "epoch": 4.390894819466248,
      "grad_norm": 3.696927547454834,
      "learning_rate": 4.72556907378336e-05,
      "loss": 0.6531,
      "step": 279700
    },
    {
      "epoch": 4.392464678178964,
      "grad_norm": 4.712969779968262,
      "learning_rate": 4.725470957613815e-05,
      "loss": 0.7093,
      "step": 279800
    },
    {
      "epoch": 4.39403453689168,
      "grad_norm": 4.365996360778809,
      "learning_rate": 4.7253728414442706e-05,
      "loss": 0.6905,
      "step": 279900
    },
    {
      "epoch": 4.395604395604396,
      "grad_norm": 3.877267837524414,
      "learning_rate": 4.7252747252747257e-05,
      "loss": 0.6707,
      "step": 280000
    },
    {
      "epoch": 4.397174254317111,
      "grad_norm": 4.180154800415039,
      "learning_rate": 4.725176609105181e-05,
      "loss": 0.6942,
      "step": 280100
    },
    {
      "epoch": 4.398744113029827,
      "grad_norm": 4.671648025512695,
      "learning_rate": 4.7250784929356365e-05,
      "loss": 0.6688,
      "step": 280200
    },
    {
      "epoch": 4.400313971742543,
      "grad_norm": 4.170560359954834,
      "learning_rate": 4.724980376766091e-05,
      "loss": 0.6888,
      "step": 280300
    },
    {
      "epoch": 4.401883830455259,
      "grad_norm": 3.7037293910980225,
      "learning_rate": 4.724882260596547e-05,
      "loss": 0.6896,
      "step": 280400
    },
    {
      "epoch": 4.403453689167975,
      "grad_norm": 4.103370666503906,
      "learning_rate": 4.724784144427002e-05,
      "loss": 0.6961,
      "step": 280500
    },
    {
      "epoch": 4.405023547880691,
      "grad_norm": 3.59649920463562,
      "learning_rate": 4.7246860282574576e-05,
      "loss": 0.6795,
      "step": 280600
    },
    {
      "epoch": 4.406593406593407,
      "grad_norm": 4.172338008880615,
      "learning_rate": 4.724587912087912e-05,
      "loss": 0.663,
      "step": 280700
    },
    {
      "epoch": 4.408163265306122,
      "grad_norm": 3.773350238800049,
      "learning_rate": 4.724489795918368e-05,
      "loss": 0.6529,
      "step": 280800
    },
    {
      "epoch": 4.409733124018838,
      "grad_norm": 3.6588478088378906,
      "learning_rate": 4.724391679748823e-05,
      "loss": 0.6197,
      "step": 280900
    },
    {
      "epoch": 4.411302982731554,
      "grad_norm": 4.51162052154541,
      "learning_rate": 4.724293563579278e-05,
      "loss": 0.664,
      "step": 281000
    },
    {
      "epoch": 4.41287284144427,
      "grad_norm": 3.8780579566955566,
      "learning_rate": 4.724195447409733e-05,
      "loss": 0.6862,
      "step": 281100
    },
    {
      "epoch": 4.414442700156986,
      "grad_norm": 3.7979021072387695,
      "learning_rate": 4.724097331240189e-05,
      "loss": 0.6847,
      "step": 281200
    },
    {
      "epoch": 4.416012558869702,
      "grad_norm": 3.960148334503174,
      "learning_rate": 4.723999215070644e-05,
      "loss": 0.6797,
      "step": 281300
    },
    {
      "epoch": 4.417582417582418,
      "grad_norm": 4.183413982391357,
      "learning_rate": 4.723901098901099e-05,
      "loss": 0.6708,
      "step": 281400
    },
    {
      "epoch": 4.419152276295134,
      "grad_norm": 3.229950428009033,
      "learning_rate": 4.723802982731554e-05,
      "loss": 0.6809,
      "step": 281500
    },
    {
      "epoch": 4.420722135007849,
      "grad_norm": 5.175266742706299,
      "learning_rate": 4.72370486656201e-05,
      "loss": 0.6866,
      "step": 281600
    },
    {
      "epoch": 4.422291993720565,
      "grad_norm": 4.563126087188721,
      "learning_rate": 4.723606750392465e-05,
      "loss": 0.6924,
      "step": 281700
    },
    {
      "epoch": 4.423861852433281,
      "grad_norm": 4.186872959136963,
      "learning_rate": 4.72350863422292e-05,
      "loss": 0.6992,
      "step": 281800
    },
    {
      "epoch": 4.425431711145997,
      "grad_norm": 3.3960063457489014,
      "learning_rate": 4.723410518053375e-05,
      "loss": 0.6945,
      "step": 281900
    },
    {
      "epoch": 4.427001569858713,
      "grad_norm": 3.7048771381378174,
      "learning_rate": 4.723312401883831e-05,
      "loss": 0.679,
      "step": 282000
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 4.691397666931152,
      "learning_rate": 4.723214285714286e-05,
      "loss": 0.669,
      "step": 282100
    },
    {
      "epoch": 4.430141287284145,
      "grad_norm": 3.9942851066589355,
      "learning_rate": 4.723116169544741e-05,
      "loss": 0.692,
      "step": 282200
    },
    {
      "epoch": 4.43171114599686,
      "grad_norm": 3.791820764541626,
      "learning_rate": 4.723018053375197e-05,
      "loss": 0.6903,
      "step": 282300
    },
    {
      "epoch": 4.433281004709576,
      "grad_norm": 3.9380099773406982,
      "learning_rate": 4.7229199372056514e-05,
      "loss": 0.6903,
      "step": 282400
    },
    {
      "epoch": 4.434850863422292,
      "grad_norm": 3.307124376296997,
      "learning_rate": 4.722821821036107e-05,
      "loss": 0.6683,
      "step": 282500
    },
    {
      "epoch": 4.436420722135008,
      "grad_norm": 4.640432834625244,
      "learning_rate": 4.722723704866562e-05,
      "loss": 0.6772,
      "step": 282600
    },
    {
      "epoch": 4.437990580847724,
      "grad_norm": 3.922910213470459,
      "learning_rate": 4.722625588697018e-05,
      "loss": 0.6885,
      "step": 282700
    },
    {
      "epoch": 4.43956043956044,
      "grad_norm": 4.520632743835449,
      "learning_rate": 4.7225274725274725e-05,
      "loss": 0.67,
      "step": 282800
    },
    {
      "epoch": 4.441130298273156,
      "grad_norm": 4.192996501922607,
      "learning_rate": 4.722429356357928e-05,
      "loss": 0.7217,
      "step": 282900
    },
    {
      "epoch": 4.442700156985872,
      "grad_norm": 3.535147190093994,
      "learning_rate": 4.722331240188383e-05,
      "loss": 0.6751,
      "step": 283000
    },
    {
      "epoch": 4.444270015698587,
      "grad_norm": 3.458132266998291,
      "learning_rate": 4.7222331240188384e-05,
      "loss": 0.6573,
      "step": 283100
    },
    {
      "epoch": 4.445839874411303,
      "grad_norm": 3.7139086723327637,
      "learning_rate": 4.7221350078492935e-05,
      "loss": 0.7113,
      "step": 283200
    },
    {
      "epoch": 4.447409733124019,
      "grad_norm": 3.1788673400878906,
      "learning_rate": 4.722036891679749e-05,
      "loss": 0.6384,
      "step": 283300
    },
    {
      "epoch": 4.448979591836735,
      "grad_norm": 4.175965785980225,
      "learning_rate": 4.7219387755102044e-05,
      "loss": 0.6848,
      "step": 283400
    },
    {
      "epoch": 4.450549450549451,
      "grad_norm": 1.6006017923355103,
      "learning_rate": 4.7218406593406595e-05,
      "loss": 0.6543,
      "step": 283500
    },
    {
      "epoch": 4.452119309262167,
      "grad_norm": 4.301729679107666,
      "learning_rate": 4.7217425431711146e-05,
      "loss": 0.5996,
      "step": 283600
    },
    {
      "epoch": 4.453689167974883,
      "grad_norm": 4.074408531188965,
      "learning_rate": 4.7216444270015704e-05,
      "loss": 0.6545,
      "step": 283700
    },
    {
      "epoch": 4.455259026687598,
      "grad_norm": 4.456742763519287,
      "learning_rate": 4.7215463108320255e-05,
      "loss": 0.6729,
      "step": 283800
    },
    {
      "epoch": 4.456828885400314,
      "grad_norm": 3.4231069087982178,
      "learning_rate": 4.7214481946624806e-05,
      "loss": 0.6779,
      "step": 283900
    },
    {
      "epoch": 4.45839874411303,
      "grad_norm": 4.171637535095215,
      "learning_rate": 4.721350078492936e-05,
      "loss": 0.6742,
      "step": 284000
    },
    {
      "epoch": 4.459968602825746,
      "grad_norm": 4.440673828125,
      "learning_rate": 4.7212519623233915e-05,
      "loss": 0.7044,
      "step": 284100
    },
    {
      "epoch": 4.461538461538462,
      "grad_norm": 4.035192489624023,
      "learning_rate": 4.7211538461538465e-05,
      "loss": 0.6405,
      "step": 284200
    },
    {
      "epoch": 4.463108320251178,
      "grad_norm": 3.6815571784973145,
      "learning_rate": 4.7210557299843016e-05,
      "loss": 0.6731,
      "step": 284300
    },
    {
      "epoch": 4.464678178963894,
      "grad_norm": 3.958411931991577,
      "learning_rate": 4.7209576138147574e-05,
      "loss": 0.6615,
      "step": 284400
    },
    {
      "epoch": 4.466248037676609,
      "grad_norm": 3.755383014678955,
      "learning_rate": 4.720859497645212e-05,
      "loss": 0.6778,
      "step": 284500
    },
    {
      "epoch": 4.467817896389325,
      "grad_norm": 3.578469753265381,
      "learning_rate": 4.7207613814756676e-05,
      "loss": 0.664,
      "step": 284600
    },
    {
      "epoch": 4.469387755102041,
      "grad_norm": 3.8740530014038086,
      "learning_rate": 4.720663265306123e-05,
      "loss": 0.6883,
      "step": 284700
    },
    {
      "epoch": 4.470957613814757,
      "grad_norm": 4.5648322105407715,
      "learning_rate": 4.720565149136578e-05,
      "loss": 0.6977,
      "step": 284800
    },
    {
      "epoch": 4.472527472527473,
      "grad_norm": 2.686384677886963,
      "learning_rate": 4.720467032967033e-05,
      "loss": 0.6604,
      "step": 284900
    },
    {
      "epoch": 4.474097331240189,
      "grad_norm": 3.9306039810180664,
      "learning_rate": 4.720368916797489e-05,
      "loss": 0.6601,
      "step": 285000
    },
    {
      "epoch": 4.475667189952905,
      "grad_norm": 4.714221000671387,
      "learning_rate": 4.720270800627944e-05,
      "loss": 0.6739,
      "step": 285100
    },
    {
      "epoch": 4.47723704866562,
      "grad_norm": 2.980924367904663,
      "learning_rate": 4.720172684458399e-05,
      "loss": 0.6588,
      "step": 285200
    },
    {
      "epoch": 4.478806907378336,
      "grad_norm": 3.721318244934082,
      "learning_rate": 4.720074568288854e-05,
      "loss": 0.6714,
      "step": 285300
    },
    {
      "epoch": 4.480376766091052,
      "grad_norm": 2.7798027992248535,
      "learning_rate": 4.71997645211931e-05,
      "loss": 0.6591,
      "step": 285400
    },
    {
      "epoch": 4.481946624803768,
      "grad_norm": 4.408758163452148,
      "learning_rate": 4.719878335949764e-05,
      "loss": 0.6575,
      "step": 285500
    },
    {
      "epoch": 4.483516483516484,
      "grad_norm": 3.095017671585083,
      "learning_rate": 4.71978021978022e-05,
      "loss": 0.6381,
      "step": 285600
    },
    {
      "epoch": 4.4850863422291996,
      "grad_norm": 3.8953144550323486,
      "learning_rate": 4.719682103610675e-05,
      "loss": 0.6765,
      "step": 285700
    },
    {
      "epoch": 4.4866562009419155,
      "grad_norm": 3.381359338760376,
      "learning_rate": 4.719583987441131e-05,
      "loss": 0.6469,
      "step": 285800
    },
    {
      "epoch": 4.488226059654631,
      "grad_norm": 3.955670118331909,
      "learning_rate": 4.719485871271586e-05,
      "loss": 0.6876,
      "step": 285900
    },
    {
      "epoch": 4.489795918367347,
      "grad_norm": 4.285511493682861,
      "learning_rate": 4.719387755102041e-05,
      "loss": 0.6909,
      "step": 286000
    },
    {
      "epoch": 4.491365777080063,
      "grad_norm": 4.48303747177124,
      "learning_rate": 4.719289638932496e-05,
      "loss": 0.6462,
      "step": 286100
    },
    {
      "epoch": 4.4929356357927785,
      "grad_norm": 3.071065902709961,
      "learning_rate": 4.719191522762951e-05,
      "loss": 0.6995,
      "step": 286200
    },
    {
      "epoch": 4.4945054945054945,
      "grad_norm": 4.179928302764893,
      "learning_rate": 4.719093406593407e-05,
      "loss": 0.7118,
      "step": 286300
    },
    {
      "epoch": 4.4960753532182105,
      "grad_norm": 4.571705341339111,
      "learning_rate": 4.718995290423862e-05,
      "loss": 0.6634,
      "step": 286400
    },
    {
      "epoch": 4.4976452119309265,
      "grad_norm": 4.113286018371582,
      "learning_rate": 4.718897174254318e-05,
      "loss": 0.6903,
      "step": 286500
    },
    {
      "epoch": 4.4992150706436425,
      "grad_norm": 3.4205968379974365,
      "learning_rate": 4.718799058084772e-05,
      "loss": 0.6985,
      "step": 286600
    },
    {
      "epoch": 4.5007849293563575,
      "grad_norm": 4.5610480308532715,
      "learning_rate": 4.718700941915228e-05,
      "loss": 0.7082,
      "step": 286700
    },
    {
      "epoch": 4.5023547880690735,
      "grad_norm": 4.712759017944336,
      "learning_rate": 4.718602825745683e-05,
      "loss": 0.6654,
      "step": 286800
    },
    {
      "epoch": 4.5039246467817895,
      "grad_norm": 3.5302796363830566,
      "learning_rate": 4.718504709576138e-05,
      "loss": 0.6657,
      "step": 286900
    },
    {
      "epoch": 4.5054945054945055,
      "grad_norm": 3.552281141281128,
      "learning_rate": 4.7184065934065934e-05,
      "loss": 0.715,
      "step": 287000
    },
    {
      "epoch": 4.5070643642072215,
      "grad_norm": 3.195446014404297,
      "learning_rate": 4.718308477237049e-05,
      "loss": 0.6892,
      "step": 287100
    },
    {
      "epoch": 4.508634222919937,
      "grad_norm": 3.194026470184326,
      "learning_rate": 4.718210361067504e-05,
      "loss": 0.6591,
      "step": 287200
    },
    {
      "epoch": 4.510204081632653,
      "grad_norm": 4.26465368270874,
      "learning_rate": 4.718112244897959e-05,
      "loss": 0.6997,
      "step": 287300
    },
    {
      "epoch": 4.511773940345369,
      "grad_norm": 4.077883720397949,
      "learning_rate": 4.7180141287284144e-05,
      "loss": 0.6962,
      "step": 287400
    },
    {
      "epoch": 4.5133437990580845,
      "grad_norm": 4.177895545959473,
      "learning_rate": 4.71791601255887e-05,
      "loss": 0.6817,
      "step": 287500
    },
    {
      "epoch": 4.5149136577708004,
      "grad_norm": 4.715366840362549,
      "learning_rate": 4.7178178963893246e-05,
      "loss": 0.6854,
      "step": 287600
    },
    {
      "epoch": 4.516483516483516,
      "grad_norm": 4.1739821434021,
      "learning_rate": 4.7177197802197804e-05,
      "loss": 0.7044,
      "step": 287700
    },
    {
      "epoch": 4.518053375196232,
      "grad_norm": 3.6713593006134033,
      "learning_rate": 4.7176216640502355e-05,
      "loss": 0.6763,
      "step": 287800
    },
    {
      "epoch": 4.519623233908948,
      "grad_norm": 3.302339553833008,
      "learning_rate": 4.717523547880691e-05,
      "loss": 0.6535,
      "step": 287900
    },
    {
      "epoch": 4.521193092621664,
      "grad_norm": 3.41715669631958,
      "learning_rate": 4.7174254317111464e-05,
      "loss": 0.6924,
      "step": 288000
    },
    {
      "epoch": 4.52276295133438,
      "grad_norm": 4.5643839836120605,
      "learning_rate": 4.7173273155416015e-05,
      "loss": 0.6223,
      "step": 288100
    },
    {
      "epoch": 4.524332810047095,
      "grad_norm": 4.317234516143799,
      "learning_rate": 4.7172291993720566e-05,
      "loss": 0.6782,
      "step": 288200
    },
    {
      "epoch": 4.525902668759811,
      "grad_norm": 4.010705471038818,
      "learning_rate": 4.717131083202512e-05,
      "loss": 0.6321,
      "step": 288300
    },
    {
      "epoch": 4.527472527472527,
      "grad_norm": 3.599325180053711,
      "learning_rate": 4.7170329670329674e-05,
      "loss": 0.6955,
      "step": 288400
    },
    {
      "epoch": 4.529042386185243,
      "grad_norm": 4.352548122406006,
      "learning_rate": 4.7169348508634225e-05,
      "loss": 0.6651,
      "step": 288500
    },
    {
      "epoch": 4.530612244897959,
      "grad_norm": 3.5428144931793213,
      "learning_rate": 4.716836734693878e-05,
      "loss": 0.656,
      "step": 288600
    },
    {
      "epoch": 4.532182103610675,
      "grad_norm": 4.1319355964660645,
      "learning_rate": 4.716738618524333e-05,
      "loss": 0.7274,
      "step": 288700
    },
    {
      "epoch": 4.533751962323391,
      "grad_norm": 4.088152885437012,
      "learning_rate": 4.7166405023547885e-05,
      "loss": 0.6727,
      "step": 288800
    },
    {
      "epoch": 4.535321821036106,
      "grad_norm": 3.4749011993408203,
      "learning_rate": 4.7165423861852436e-05,
      "loss": 0.6642,
      "step": 288900
    },
    {
      "epoch": 4.536891679748822,
      "grad_norm": 4.224738121032715,
      "learning_rate": 4.716444270015699e-05,
      "loss": 0.6577,
      "step": 289000
    },
    {
      "epoch": 4.538461538461538,
      "grad_norm": 2.613027334213257,
      "learning_rate": 4.716346153846154e-05,
      "loss": 0.6728,
      "step": 289100
    },
    {
      "epoch": 4.540031397174254,
      "grad_norm": 3.6837265491485596,
      "learning_rate": 4.7162480376766096e-05,
      "loss": 0.6427,
      "step": 289200
    },
    {
      "epoch": 4.54160125588697,
      "grad_norm": 3.26383638381958,
      "learning_rate": 4.716149921507065e-05,
      "loss": 0.6857,
      "step": 289300
    },
    {
      "epoch": 4.543171114599686,
      "grad_norm": 3.623631000518799,
      "learning_rate": 4.71605180533752e-05,
      "loss": 0.6288,
      "step": 289400
    },
    {
      "epoch": 4.544740973312402,
      "grad_norm": 4.304204940795898,
      "learning_rate": 4.715953689167975e-05,
      "loss": 0.6923,
      "step": 289500
    },
    {
      "epoch": 4.546310832025117,
      "grad_norm": 4.67928409576416,
      "learning_rate": 4.7158555729984307e-05,
      "loss": 0.69,
      "step": 289600
    },
    {
      "epoch": 4.547880690737833,
      "grad_norm": 3.6097538471221924,
      "learning_rate": 4.715757456828885e-05,
      "loss": 0.6596,
      "step": 289700
    },
    {
      "epoch": 4.549450549450549,
      "grad_norm": 3.2071638107299805,
      "learning_rate": 4.715659340659341e-05,
      "loss": 0.6593,
      "step": 289800
    },
    {
      "epoch": 4.551020408163265,
      "grad_norm": 5.005595684051514,
      "learning_rate": 4.715561224489796e-05,
      "loss": 0.6629,
      "step": 289900
    },
    {
      "epoch": 4.552590266875981,
      "grad_norm": 5.083735942840576,
      "learning_rate": 4.715463108320252e-05,
      "loss": 0.6671,
      "step": 290000
    },
    {
      "epoch": 4.554160125588697,
      "grad_norm": 3.744908094406128,
      "learning_rate": 4.715364992150706e-05,
      "loss": 0.6747,
      "step": 290100
    },
    {
      "epoch": 4.555729984301413,
      "grad_norm": 4.271833896636963,
      "learning_rate": 4.715266875981162e-05,
      "loss": 0.6706,
      "step": 290200
    },
    {
      "epoch": 4.557299843014128,
      "grad_norm": 4.589900970458984,
      "learning_rate": 4.715168759811617e-05,
      "loss": 0.67,
      "step": 290300
    },
    {
      "epoch": 4.558869701726844,
      "grad_norm": 3.829991579055786,
      "learning_rate": 4.715070643642072e-05,
      "loss": 0.6666,
      "step": 290400
    },
    {
      "epoch": 4.56043956043956,
      "grad_norm": 4.816482067108154,
      "learning_rate": 4.714972527472528e-05,
      "loss": 0.666,
      "step": 290500
    },
    {
      "epoch": 4.562009419152276,
      "grad_norm": 3.3753244876861572,
      "learning_rate": 4.714874411302983e-05,
      "loss": 0.686,
      "step": 290600
    },
    {
      "epoch": 4.563579277864992,
      "grad_norm": 4.220893383026123,
      "learning_rate": 4.714776295133439e-05,
      "loss": 0.7206,
      "step": 290700
    },
    {
      "epoch": 4.565149136577708,
      "grad_norm": 3.293107271194458,
      "learning_rate": 4.714678178963893e-05,
      "loss": 0.6462,
      "step": 290800
    },
    {
      "epoch": 4.566718995290424,
      "grad_norm": 4.953923225402832,
      "learning_rate": 4.714580062794349e-05,
      "loss": 0.6278,
      "step": 290900
    },
    {
      "epoch": 4.568288854003139,
      "grad_norm": 4.246881484985352,
      "learning_rate": 4.714481946624804e-05,
      "loss": 0.6511,
      "step": 291000
    },
    {
      "epoch": 4.569858712715855,
      "grad_norm": 3.2393627166748047,
      "learning_rate": 4.714383830455259e-05,
      "loss": 0.6979,
      "step": 291100
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 4.704121112823486,
      "learning_rate": 4.714285714285714e-05,
      "loss": 0.6998,
      "step": 291200
    },
    {
      "epoch": 4.572998430141287,
      "grad_norm": 4.751491546630859,
      "learning_rate": 4.71418759811617e-05,
      "loss": 0.6798,
      "step": 291300
    },
    {
      "epoch": 4.574568288854003,
      "grad_norm": 3.675246000289917,
      "learning_rate": 4.714089481946625e-05,
      "loss": 0.7227,
      "step": 291400
    },
    {
      "epoch": 4.576138147566719,
      "grad_norm": 4.40720272064209,
      "learning_rate": 4.71399136577708e-05,
      "loss": 0.6653,
      "step": 291500
    },
    {
      "epoch": 4.577708006279435,
      "grad_norm": 4.983305931091309,
      "learning_rate": 4.713893249607535e-05,
      "loss": 0.6828,
      "step": 291600
    },
    {
      "epoch": 4.579277864992151,
      "grad_norm": 4.2581963539123535,
      "learning_rate": 4.713795133437991e-05,
      "loss": 0.7318,
      "step": 291700
    },
    {
      "epoch": 4.580847723704867,
      "grad_norm": 5.698441028594971,
      "learning_rate": 4.7136970172684455e-05,
      "loss": 0.6584,
      "step": 291800
    },
    {
      "epoch": 4.582417582417582,
      "grad_norm": 4.63938570022583,
      "learning_rate": 4.713598901098901e-05,
      "loss": 0.7046,
      "step": 291900
    },
    {
      "epoch": 4.583987441130298,
      "grad_norm": 4.561948299407959,
      "learning_rate": 4.7135007849293564e-05,
      "loss": 0.6971,
      "step": 292000
    },
    {
      "epoch": 4.585557299843014,
      "grad_norm": 3.950939655303955,
      "learning_rate": 4.713402668759812e-05,
      "loss": 0.6515,
      "step": 292100
    },
    {
      "epoch": 4.58712715855573,
      "grad_norm": 4.308842658996582,
      "learning_rate": 4.7133045525902666e-05,
      "loss": 0.67,
      "step": 292200
    },
    {
      "epoch": 4.588697017268446,
      "grad_norm": 3.8765954971313477,
      "learning_rate": 4.7132064364207224e-05,
      "loss": 0.642,
      "step": 292300
    },
    {
      "epoch": 4.590266875981162,
      "grad_norm": 3.968050479888916,
      "learning_rate": 4.7131083202511775e-05,
      "loss": 0.688,
      "step": 292400
    },
    {
      "epoch": 4.591836734693878,
      "grad_norm": 3.9059336185455322,
      "learning_rate": 4.7130102040816326e-05,
      "loss": 0.6433,
      "step": 292500
    },
    {
      "epoch": 4.593406593406593,
      "grad_norm": 3.174269437789917,
      "learning_rate": 4.7129120879120883e-05,
      "loss": 0.6756,
      "step": 292600
    },
    {
      "epoch": 4.594976452119309,
      "grad_norm": 3.8179261684417725,
      "learning_rate": 4.7128139717425434e-05,
      "loss": 0.6577,
      "step": 292700
    },
    {
      "epoch": 4.596546310832025,
      "grad_norm": 4.289277076721191,
      "learning_rate": 4.712715855572999e-05,
      "loss": 0.6731,
      "step": 292800
    },
    {
      "epoch": 4.598116169544741,
      "grad_norm": 4.107276439666748,
      "learning_rate": 4.7126177394034536e-05,
      "loss": 0.6962,
      "step": 292900
    },
    {
      "epoch": 4.599686028257457,
      "grad_norm": 4.7359185218811035,
      "learning_rate": 4.7125196232339094e-05,
      "loss": 0.6925,
      "step": 293000
    },
    {
      "epoch": 4.601255886970173,
      "grad_norm": 3.3622548580169678,
      "learning_rate": 4.7124215070643645e-05,
      "loss": 0.7012,
      "step": 293100
    },
    {
      "epoch": 4.602825745682889,
      "grad_norm": 4.95927619934082,
      "learning_rate": 4.7123233908948196e-05,
      "loss": 0.7109,
      "step": 293200
    },
    {
      "epoch": 4.604395604395604,
      "grad_norm": 4.360296249389648,
      "learning_rate": 4.712225274725275e-05,
      "loss": 0.6498,
      "step": 293300
    },
    {
      "epoch": 4.60596546310832,
      "grad_norm": 5.046242713928223,
      "learning_rate": 4.7121271585557305e-05,
      "loss": 0.7005,
      "step": 293400
    },
    {
      "epoch": 4.607535321821036,
      "grad_norm": 3.563018321990967,
      "learning_rate": 4.7120290423861856e-05,
      "loss": 0.6794,
      "step": 293500
    },
    {
      "epoch": 4.609105180533752,
      "grad_norm": 4.313770294189453,
      "learning_rate": 4.711930926216641e-05,
      "loss": 0.647,
      "step": 293600
    },
    {
      "epoch": 4.610675039246468,
      "grad_norm": 4.397770881652832,
      "learning_rate": 4.711832810047096e-05,
      "loss": 0.7155,
      "step": 293700
    },
    {
      "epoch": 4.612244897959184,
      "grad_norm": 3.289773464202881,
      "learning_rate": 4.7117346938775516e-05,
      "loss": 0.6417,
      "step": 293800
    },
    {
      "epoch": 4.6138147566719,
      "grad_norm": 4.540371417999268,
      "learning_rate": 4.711636577708006e-05,
      "loss": 0.6612,
      "step": 293900
    },
    {
      "epoch": 4.615384615384615,
      "grad_norm": 3.5849766731262207,
      "learning_rate": 4.711538461538462e-05,
      "loss": 0.6638,
      "step": 294000
    },
    {
      "epoch": 4.616954474097331,
      "grad_norm": 3.8784594535827637,
      "learning_rate": 4.711440345368917e-05,
      "loss": 0.6637,
      "step": 294100
    },
    {
      "epoch": 4.618524332810047,
      "grad_norm": 4.381464958190918,
      "learning_rate": 4.7113422291993726e-05,
      "loss": 0.714,
      "step": 294200
    },
    {
      "epoch": 4.620094191522763,
      "grad_norm": 4.532561779022217,
      "learning_rate": 4.711244113029827e-05,
      "loss": 0.6981,
      "step": 294300
    },
    {
      "epoch": 4.621664050235479,
      "grad_norm": 3.8347878456115723,
      "learning_rate": 4.711145996860283e-05,
      "loss": 0.6787,
      "step": 294400
    },
    {
      "epoch": 4.623233908948195,
      "grad_norm": 3.3082072734832764,
      "learning_rate": 4.711047880690738e-05,
      "loss": 0.651,
      "step": 294500
    },
    {
      "epoch": 4.624803767660911,
      "grad_norm": 4.14057731628418,
      "learning_rate": 4.710949764521193e-05,
      "loss": 0.6638,
      "step": 294600
    },
    {
      "epoch": 4.626373626373626,
      "grad_norm": 3.411313056945801,
      "learning_rate": 4.710851648351649e-05,
      "loss": 0.6596,
      "step": 294700
    },
    {
      "epoch": 4.627943485086342,
      "grad_norm": 4.1598711013793945,
      "learning_rate": 4.710753532182104e-05,
      "loss": 0.6836,
      "step": 294800
    },
    {
      "epoch": 4.629513343799058,
      "grad_norm": 2.675616979598999,
      "learning_rate": 4.71065541601256e-05,
      "loss": 0.6771,
      "step": 294900
    },
    {
      "epoch": 4.631083202511774,
      "grad_norm": 4.22223424911499,
      "learning_rate": 4.710557299843014e-05,
      "loss": 0.6929,
      "step": 295000
    },
    {
      "epoch": 4.63265306122449,
      "grad_norm": 3.9938178062438965,
      "learning_rate": 4.71045918367347e-05,
      "loss": 0.6757,
      "step": 295100
    },
    {
      "epoch": 4.634222919937206,
      "grad_norm": 4.042769432067871,
      "learning_rate": 4.710361067503925e-05,
      "loss": 0.6826,
      "step": 295200
    },
    {
      "epoch": 4.635792778649922,
      "grad_norm": 4.800907611846924,
      "learning_rate": 4.71026295133438e-05,
      "loss": 0.6567,
      "step": 295300
    },
    {
      "epoch": 4.637362637362637,
      "grad_norm": 3.901155471801758,
      "learning_rate": 4.710164835164835e-05,
      "loss": 0.6628,
      "step": 295400
    },
    {
      "epoch": 4.638932496075353,
      "grad_norm": 3.30061674118042,
      "learning_rate": 4.710066718995291e-05,
      "loss": 0.7134,
      "step": 295500
    },
    {
      "epoch": 4.640502354788069,
      "grad_norm": 3.6017441749572754,
      "learning_rate": 4.709968602825746e-05,
      "loss": 0.6253,
      "step": 295600
    },
    {
      "epoch": 4.642072213500785,
      "grad_norm": 4.064005374908447,
      "learning_rate": 4.709870486656201e-05,
      "loss": 0.69,
      "step": 295700
    },
    {
      "epoch": 4.643642072213501,
      "grad_norm": 4.573694705963135,
      "learning_rate": 4.709772370486656e-05,
      "loss": 0.6822,
      "step": 295800
    },
    {
      "epoch": 4.645211930926217,
      "grad_norm": 3.5792789459228516,
      "learning_rate": 4.709674254317112e-05,
      "loss": 0.6623,
      "step": 295900
    },
    {
      "epoch": 4.646781789638933,
      "grad_norm": 3.9597082138061523,
      "learning_rate": 4.7095761381475664e-05,
      "loss": 0.6824,
      "step": 296000
    },
    {
      "epoch": 4.648351648351649,
      "grad_norm": 4.356625556945801,
      "learning_rate": 4.709478021978022e-05,
      "loss": 0.6919,
      "step": 296100
    },
    {
      "epoch": 4.649921507064365,
      "grad_norm": 4.4317307472229,
      "learning_rate": 4.709379905808477e-05,
      "loss": 0.672,
      "step": 296200
    },
    {
      "epoch": 4.65149136577708,
      "grad_norm": 3.9809539318084717,
      "learning_rate": 4.709281789638933e-05,
      "loss": 0.6435,
      "step": 296300
    },
    {
      "epoch": 4.653061224489796,
      "grad_norm": 3.7955262660980225,
      "learning_rate": 4.7091836734693875e-05,
      "loss": 0.7124,
      "step": 296400
    },
    {
      "epoch": 4.654631083202512,
      "grad_norm": 3.4896152019500732,
      "learning_rate": 4.709085557299843e-05,
      "loss": 0.6797,
      "step": 296500
    },
    {
      "epoch": 4.656200941915228,
      "grad_norm": 3.5174617767333984,
      "learning_rate": 4.7089874411302984e-05,
      "loss": 0.6775,
      "step": 296600
    },
    {
      "epoch": 4.657770800627944,
      "grad_norm": 5.038230895996094,
      "learning_rate": 4.7088893249607535e-05,
      "loss": 0.6967,
      "step": 296700
    },
    {
      "epoch": 4.65934065934066,
      "grad_norm": 3.7017645835876465,
      "learning_rate": 4.708791208791209e-05,
      "loss": 0.6884,
      "step": 296800
    },
    {
      "epoch": 4.660910518053376,
      "grad_norm": 5.081867218017578,
      "learning_rate": 4.7086930926216643e-05,
      "loss": 0.6565,
      "step": 296900
    },
    {
      "epoch": 4.662480376766091,
      "grad_norm": 3.3381826877593994,
      "learning_rate": 4.7085949764521194e-05,
      "loss": 0.6878,
      "step": 297000
    },
    {
      "epoch": 4.664050235478807,
      "grad_norm": 4.537021160125732,
      "learning_rate": 4.7084968602825745e-05,
      "loss": 0.6815,
      "step": 297100
    },
    {
      "epoch": 4.665620094191523,
      "grad_norm": 4.191314697265625,
      "learning_rate": 4.70839874411303e-05,
      "loss": 0.6953,
      "step": 297200
    },
    {
      "epoch": 4.667189952904239,
      "grad_norm": 3.72737717628479,
      "learning_rate": 4.7083006279434854e-05,
      "loss": 0.6965,
      "step": 297300
    },
    {
      "epoch": 4.668759811616955,
      "grad_norm": 4.157914638519287,
      "learning_rate": 4.7082025117739405e-05,
      "loss": 0.6543,
      "step": 297400
    },
    {
      "epoch": 4.670329670329671,
      "grad_norm": 4.313502788543701,
      "learning_rate": 4.7081043956043956e-05,
      "loss": 0.6766,
      "step": 297500
    },
    {
      "epoch": 4.671899529042387,
      "grad_norm": 4.269144535064697,
      "learning_rate": 4.7080062794348514e-05,
      "loss": 0.6483,
      "step": 297600
    },
    {
      "epoch": 4.673469387755102,
      "grad_norm": 4.59192419052124,
      "learning_rate": 4.7079081632653065e-05,
      "loss": 0.6598,
      "step": 297700
    },
    {
      "epoch": 4.675039246467818,
      "grad_norm": 4.221196174621582,
      "learning_rate": 4.7078100470957616e-05,
      "loss": 0.6709,
      "step": 297800
    },
    {
      "epoch": 4.676609105180534,
      "grad_norm": 3.089789390563965,
      "learning_rate": 4.707711930926217e-05,
      "loss": 0.696,
      "step": 297900
    },
    {
      "epoch": 4.67817896389325,
      "grad_norm": 5.034912586212158,
      "learning_rate": 4.7076138147566725e-05,
      "loss": 0.6594,
      "step": 298000
    },
    {
      "epoch": 4.679748822605966,
      "grad_norm": 2.8415794372558594,
      "learning_rate": 4.707515698587127e-05,
      "loss": 0.6706,
      "step": 298100
    },
    {
      "epoch": 4.681318681318682,
      "grad_norm": 4.3535542488098145,
      "learning_rate": 4.7074175824175826e-05,
      "loss": 0.7212,
      "step": 298200
    },
    {
      "epoch": 4.6828885400313975,
      "grad_norm": 3.8680531978607178,
      "learning_rate": 4.707319466248038e-05,
      "loss": 0.6481,
      "step": 298300
    },
    {
      "epoch": 4.684458398744113,
      "grad_norm": 3.5587916374206543,
      "learning_rate": 4.7072213500784935e-05,
      "loss": 0.7013,
      "step": 298400
    },
    {
      "epoch": 4.686028257456829,
      "grad_norm": 4.935722827911377,
      "learning_rate": 4.707123233908948e-05,
      "loss": 0.7209,
      "step": 298500
    },
    {
      "epoch": 4.687598116169545,
      "grad_norm": 3.991694211959839,
      "learning_rate": 4.707025117739404e-05,
      "loss": 0.6766,
      "step": 298600
    },
    {
      "epoch": 4.689167974882261,
      "grad_norm": 4.2006916999816895,
      "learning_rate": 4.706927001569859e-05,
      "loss": 0.7021,
      "step": 298700
    },
    {
      "epoch": 4.6907378335949765,
      "grad_norm": 3.442377805709839,
      "learning_rate": 4.706828885400314e-05,
      "loss": 0.6374,
      "step": 298800
    },
    {
      "epoch": 4.6923076923076925,
      "grad_norm": 3.4888784885406494,
      "learning_rate": 4.70673076923077e-05,
      "loss": 0.6612,
      "step": 298900
    },
    {
      "epoch": 4.6938775510204085,
      "grad_norm": 4.294275283813477,
      "learning_rate": 4.706632653061225e-05,
      "loss": 0.6931,
      "step": 299000
    },
    {
      "epoch": 4.695447409733124,
      "grad_norm": 4.7136006355285645,
      "learning_rate": 4.70653453689168e-05,
      "loss": 0.627,
      "step": 299100
    },
    {
      "epoch": 4.6970172684458396,
      "grad_norm": 2.9293293952941895,
      "learning_rate": 4.706436420722135e-05,
      "loss": 0.7427,
      "step": 299200
    },
    {
      "epoch": 4.6985871271585555,
      "grad_norm": 4.03350305557251,
      "learning_rate": 4.706338304552591e-05,
      "loss": 0.6818,
      "step": 299300
    },
    {
      "epoch": 4.7001569858712715,
      "grad_norm": 4.070458889007568,
      "learning_rate": 4.706240188383046e-05,
      "loss": 0.6494,
      "step": 299400
    },
    {
      "epoch": 4.7017268445839875,
      "grad_norm": 3.4781455993652344,
      "learning_rate": 4.706142072213501e-05,
      "loss": 0.6699,
      "step": 299500
    },
    {
      "epoch": 4.7032967032967035,
      "grad_norm": 4.155882358551025,
      "learning_rate": 4.706043956043956e-05,
      "loss": 0.7333,
      "step": 299600
    },
    {
      "epoch": 4.704866562009419,
      "grad_norm": 3.2412548065185547,
      "learning_rate": 4.705945839874412e-05,
      "loss": 0.6672,
      "step": 299700
    },
    {
      "epoch": 4.7064364207221345,
      "grad_norm": 3.3004980087280273,
      "learning_rate": 4.705847723704867e-05,
      "loss": 0.6536,
      "step": 299800
    },
    {
      "epoch": 4.7080062794348505,
      "grad_norm": 4.0152363777160645,
      "learning_rate": 4.705749607535322e-05,
      "loss": 0.6743,
      "step": 299900
    },
    {
      "epoch": 4.7095761381475665,
      "grad_norm": 4.722228527069092,
      "learning_rate": 4.705651491365777e-05,
      "loss": 0.6845,
      "step": 300000
    },
    {
      "epoch": 4.7111459968602825,
      "grad_norm": 4.299479007720947,
      "learning_rate": 4.705553375196233e-05,
      "loss": 0.6951,
      "step": 300100
    },
    {
      "epoch": 4.712715855572998,
      "grad_norm": 3.7449779510498047,
      "learning_rate": 4.705455259026687e-05,
      "loss": 0.712,
      "step": 300200
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 3.8778815269470215,
      "learning_rate": 4.705357142857143e-05,
      "loss": 0.6542,
      "step": 300300
    },
    {
      "epoch": 4.71585557299843,
      "grad_norm": 4.395393371582031,
      "learning_rate": 4.705259026687598e-05,
      "loss": 0.6796,
      "step": 300400
    },
    {
      "epoch": 4.717425431711146,
      "grad_norm": 3.52555775642395,
      "learning_rate": 4.705160910518054e-05,
      "loss": 0.7016,
      "step": 300500
    },
    {
      "epoch": 4.718995290423862,
      "grad_norm": 4.140084266662598,
      "learning_rate": 4.7050627943485084e-05,
      "loss": 0.6481,
      "step": 300600
    },
    {
      "epoch": 4.720565149136577,
      "grad_norm": 3.4186019897460938,
      "learning_rate": 4.704964678178964e-05,
      "loss": 0.6502,
      "step": 300700
    },
    {
      "epoch": 4.722135007849293,
      "grad_norm": 3.7579658031463623,
      "learning_rate": 4.704866562009419e-05,
      "loss": 0.6452,
      "step": 300800
    },
    {
      "epoch": 4.723704866562009,
      "grad_norm": 4.088251113891602,
      "learning_rate": 4.7047684458398744e-05,
      "loss": 0.6745,
      "step": 300900
    },
    {
      "epoch": 4.725274725274725,
      "grad_norm": 5.283199787139893,
      "learning_rate": 4.70467032967033e-05,
      "loss": 0.6919,
      "step": 301000
    },
    {
      "epoch": 4.726844583987441,
      "grad_norm": 4.154319763183594,
      "learning_rate": 4.704572213500785e-05,
      "loss": 0.686,
      "step": 301100
    },
    {
      "epoch": 4.728414442700157,
      "grad_norm": 4.42350435256958,
      "learning_rate": 4.70447409733124e-05,
      "loss": 0.6567,
      "step": 301200
    },
    {
      "epoch": 4.729984301412873,
      "grad_norm": 5.263411045074463,
      "learning_rate": 4.7043759811616954e-05,
      "loss": 0.6684,
      "step": 301300
    },
    {
      "epoch": 4.731554160125588,
      "grad_norm": 3.7327983379364014,
      "learning_rate": 4.704277864992151e-05,
      "loss": 0.6935,
      "step": 301400
    },
    {
      "epoch": 4.733124018838304,
      "grad_norm": 4.04680871963501,
      "learning_rate": 4.704179748822606e-05,
      "loss": 0.6118,
      "step": 301500
    },
    {
      "epoch": 4.73469387755102,
      "grad_norm": 4.455444812774658,
      "learning_rate": 4.7040816326530614e-05,
      "loss": 0.6288,
      "step": 301600
    },
    {
      "epoch": 4.736263736263736,
      "grad_norm": 4.487597942352295,
      "learning_rate": 4.7039835164835165e-05,
      "loss": 0.6892,
      "step": 301700
    },
    {
      "epoch": 4.737833594976452,
      "grad_norm": 4.303781032562256,
      "learning_rate": 4.703885400313972e-05,
      "loss": 0.6396,
      "step": 301800
    },
    {
      "epoch": 4.739403453689168,
      "grad_norm": 3.4089341163635254,
      "learning_rate": 4.7037872841444274e-05,
      "loss": 0.7083,
      "step": 301900
    },
    {
      "epoch": 4.740973312401884,
      "grad_norm": 3.1110386848449707,
      "learning_rate": 4.7036891679748825e-05,
      "loss": 0.6844,
      "step": 302000
    },
    {
      "epoch": 4.742543171114599,
      "grad_norm": 4.011343002319336,
      "learning_rate": 4.7035910518053376e-05,
      "loss": 0.6484,
      "step": 302100
    },
    {
      "epoch": 4.744113029827315,
      "grad_norm": 3.8393337726593018,
      "learning_rate": 4.7034929356357934e-05,
      "loss": 0.7006,
      "step": 302200
    },
    {
      "epoch": 4.745682888540031,
      "grad_norm": 3.3312432765960693,
      "learning_rate": 4.703394819466248e-05,
      "loss": 0.7121,
      "step": 302300
    },
    {
      "epoch": 4.747252747252747,
      "grad_norm": 3.184554100036621,
      "learning_rate": 4.7032967032967035e-05,
      "loss": 0.6419,
      "step": 302400
    },
    {
      "epoch": 4.748822605965463,
      "grad_norm": 3.90944766998291,
      "learning_rate": 4.7031985871271586e-05,
      "loss": 0.6966,
      "step": 302500
    },
    {
      "epoch": 4.750392464678179,
      "grad_norm": 3.9492340087890625,
      "learning_rate": 4.7031004709576144e-05,
      "loss": 0.6706,
      "step": 302600
    },
    {
      "epoch": 4.751962323390895,
      "grad_norm": 4.442107200622559,
      "learning_rate": 4.703002354788069e-05,
      "loss": 0.7014,
      "step": 302700
    },
    {
      "epoch": 4.75353218210361,
      "grad_norm": 3.4604177474975586,
      "learning_rate": 4.7029042386185246e-05,
      "loss": 0.6802,
      "step": 302800
    },
    {
      "epoch": 4.755102040816326,
      "grad_norm": 4.025793075561523,
      "learning_rate": 4.70280612244898e-05,
      "loss": 0.6763,
      "step": 302900
    },
    {
      "epoch": 4.756671899529042,
      "grad_norm": 3.971118688583374,
      "learning_rate": 4.702708006279435e-05,
      "loss": 0.6062,
      "step": 303000
    },
    {
      "epoch": 4.758241758241758,
      "grad_norm": 3.7719905376434326,
      "learning_rate": 4.7026098901098906e-05,
      "loss": 0.6911,
      "step": 303100
    },
    {
      "epoch": 4.759811616954474,
      "grad_norm": 5.159405708312988,
      "learning_rate": 4.702511773940346e-05,
      "loss": 0.6947,
      "step": 303200
    },
    {
      "epoch": 4.76138147566719,
      "grad_norm": 3.955939769744873,
      "learning_rate": 4.702413657770801e-05,
      "loss": 0.6421,
      "step": 303300
    },
    {
      "epoch": 4.762951334379906,
      "grad_norm": 4.321470737457275,
      "learning_rate": 4.702315541601256e-05,
      "loss": 0.6896,
      "step": 303400
    },
    {
      "epoch": 4.764521193092621,
      "grad_norm": 4.20265007019043,
      "learning_rate": 4.7022174254317117e-05,
      "loss": 0.6485,
      "step": 303500
    },
    {
      "epoch": 4.766091051805337,
      "grad_norm": 4.936803340911865,
      "learning_rate": 4.702119309262167e-05,
      "loss": 0.6329,
      "step": 303600
    },
    {
      "epoch": 4.767660910518053,
      "grad_norm": 3.7521767616271973,
      "learning_rate": 4.702021193092622e-05,
      "loss": 0.6976,
      "step": 303700
    },
    {
      "epoch": 4.769230769230769,
      "grad_norm": 3.7460782527923584,
      "learning_rate": 4.701923076923077e-05,
      "loss": 0.6944,
      "step": 303800
    },
    {
      "epoch": 4.770800627943485,
      "grad_norm": 4.077909469604492,
      "learning_rate": 4.701824960753533e-05,
      "loss": 0.6978,
      "step": 303900
    },
    {
      "epoch": 4.772370486656201,
      "grad_norm": 4.251471519470215,
      "learning_rate": 4.701726844583988e-05,
      "loss": 0.7184,
      "step": 304000
    },
    {
      "epoch": 4.773940345368917,
      "grad_norm": 3.833857774734497,
      "learning_rate": 4.701628728414443e-05,
      "loss": 0.6621,
      "step": 304100
    },
    {
      "epoch": 4.775510204081632,
      "grad_norm": 5.295615196228027,
      "learning_rate": 4.701530612244898e-05,
      "loss": 0.6552,
      "step": 304200
    },
    {
      "epoch": 4.777080062794348,
      "grad_norm": 4.027021884918213,
      "learning_rate": 4.701432496075354e-05,
      "loss": 0.661,
      "step": 304300
    },
    {
      "epoch": 4.778649921507064,
      "grad_norm": 3.9470255374908447,
      "learning_rate": 4.701334379905808e-05,
      "loss": 0.6925,
      "step": 304400
    },
    {
      "epoch": 4.78021978021978,
      "grad_norm": 4.609099864959717,
      "learning_rate": 4.701236263736264e-05,
      "loss": 0.6267,
      "step": 304500
    },
    {
      "epoch": 4.781789638932496,
      "grad_norm": 3.5311429500579834,
      "learning_rate": 4.701138147566719e-05,
      "loss": 0.6403,
      "step": 304600
    },
    {
      "epoch": 4.783359497645212,
      "grad_norm": 3.722991943359375,
      "learning_rate": 4.701040031397175e-05,
      "loss": 0.6501,
      "step": 304700
    },
    {
      "epoch": 4.784929356357928,
      "grad_norm": 3.8659467697143555,
      "learning_rate": 4.700941915227629e-05,
      "loss": 0.6754,
      "step": 304800
    },
    {
      "epoch": 4.786499215070644,
      "grad_norm": 3.9785051345825195,
      "learning_rate": 4.700843799058085e-05,
      "loss": 0.6795,
      "step": 304900
    },
    {
      "epoch": 4.78806907378336,
      "grad_norm": 4.648197174072266,
      "learning_rate": 4.70074568288854e-05,
      "loss": 0.6814,
      "step": 305000
    },
    {
      "epoch": 4.789638932496075,
      "grad_norm": 3.7689149379730225,
      "learning_rate": 4.700647566718995e-05,
      "loss": 0.6719,
      "step": 305100
    },
    {
      "epoch": 4.791208791208791,
      "grad_norm": 3.883761167526245,
      "learning_rate": 4.700549450549451e-05,
      "loss": 0.662,
      "step": 305200
    },
    {
      "epoch": 4.792778649921507,
      "grad_norm": 4.101100444793701,
      "learning_rate": 4.700451334379906e-05,
      "loss": 0.7026,
      "step": 305300
    },
    {
      "epoch": 4.794348508634223,
      "grad_norm": 4.029136657714844,
      "learning_rate": 4.700353218210361e-05,
      "loss": 0.6906,
      "step": 305400
    },
    {
      "epoch": 4.795918367346939,
      "grad_norm": 3.9799768924713135,
      "learning_rate": 4.700255102040816e-05,
      "loss": 0.6797,
      "step": 305500
    },
    {
      "epoch": 4.797488226059655,
      "grad_norm": 4.36775016784668,
      "learning_rate": 4.700156985871272e-05,
      "loss": 0.6853,
      "step": 305600
    },
    {
      "epoch": 4.799058084772371,
      "grad_norm": 4.098068714141846,
      "learning_rate": 4.700058869701727e-05,
      "loss": 0.6872,
      "step": 305700
    },
    {
      "epoch": 4.800627943485086,
      "grad_norm": 2.8540453910827637,
      "learning_rate": 4.699960753532182e-05,
      "loss": 0.682,
      "step": 305800
    },
    {
      "epoch": 4.802197802197802,
      "grad_norm": 3.9762585163116455,
      "learning_rate": 4.6998626373626374e-05,
      "loss": 0.7034,
      "step": 305900
    },
    {
      "epoch": 4.803767660910518,
      "grad_norm": 3.4198811054229736,
      "learning_rate": 4.699764521193093e-05,
      "loss": 0.6813,
      "step": 306000
    },
    {
      "epoch": 4.805337519623234,
      "grad_norm": 4.079990386962891,
      "learning_rate": 4.699666405023548e-05,
      "loss": 0.6804,
      "step": 306100
    },
    {
      "epoch": 4.80690737833595,
      "grad_norm": 4.864233016967773,
      "learning_rate": 4.6995682888540034e-05,
      "loss": 0.6502,
      "step": 306200
    },
    {
      "epoch": 4.808477237048666,
      "grad_norm": 4.106625556945801,
      "learning_rate": 4.6994701726844585e-05,
      "loss": 0.6735,
      "step": 306300
    },
    {
      "epoch": 4.810047095761382,
      "grad_norm": 4.043144702911377,
      "learning_rate": 4.699372056514914e-05,
      "loss": 0.66,
      "step": 306400
    },
    {
      "epoch": 4.811616954474097,
      "grad_norm": 3.707141876220703,
      "learning_rate": 4.699273940345369e-05,
      "loss": 0.7061,
      "step": 306500
    },
    {
      "epoch": 4.813186813186813,
      "grad_norm": 4.16402006149292,
      "learning_rate": 4.6991758241758244e-05,
      "loss": 0.6475,
      "step": 306600
    },
    {
      "epoch": 4.814756671899529,
      "grad_norm": 3.6913371086120605,
      "learning_rate": 4.6990777080062795e-05,
      "loss": 0.6969,
      "step": 306700
    },
    {
      "epoch": 4.816326530612245,
      "grad_norm": 4.254756927490234,
      "learning_rate": 4.698979591836735e-05,
      "loss": 0.6696,
      "step": 306800
    },
    {
      "epoch": 4.817896389324961,
      "grad_norm": 2.9493207931518555,
      "learning_rate": 4.69888147566719e-05,
      "loss": 0.675,
      "step": 306900
    },
    {
      "epoch": 4.819466248037677,
      "grad_norm": 4.271134376525879,
      "learning_rate": 4.6987833594976455e-05,
      "loss": 0.6762,
      "step": 307000
    },
    {
      "epoch": 4.821036106750393,
      "grad_norm": 4.114038944244385,
      "learning_rate": 4.6986852433281006e-05,
      "loss": 0.7026,
      "step": 307100
    },
    {
      "epoch": 4.822605965463108,
      "grad_norm": 4.011929512023926,
      "learning_rate": 4.698587127158556e-05,
      "loss": 0.684,
      "step": 307200
    },
    {
      "epoch": 4.824175824175824,
      "grad_norm": 3.897146224975586,
      "learning_rate": 4.6984890109890115e-05,
      "loss": 0.6556,
      "step": 307300
    },
    {
      "epoch": 4.82574568288854,
      "grad_norm": 3.3840482234954834,
      "learning_rate": 4.6983908948194666e-05,
      "loss": 0.6641,
      "step": 307400
    },
    {
      "epoch": 4.827315541601256,
      "grad_norm": 3.9979588985443115,
      "learning_rate": 4.698292778649922e-05,
      "loss": 0.6848,
      "step": 307500
    },
    {
      "epoch": 4.828885400313972,
      "grad_norm": 3.2818315029144287,
      "learning_rate": 4.698194662480377e-05,
      "loss": 0.6453,
      "step": 307600
    },
    {
      "epoch": 4.830455259026688,
      "grad_norm": 2.859806776046753,
      "learning_rate": 4.6980965463108326e-05,
      "loss": 0.6637,
      "step": 307700
    },
    {
      "epoch": 4.832025117739404,
      "grad_norm": 4.642868995666504,
      "learning_rate": 4.6979984301412877e-05,
      "loss": 0.6389,
      "step": 307800
    },
    {
      "epoch": 4.833594976452119,
      "grad_norm": 2.9993324279785156,
      "learning_rate": 4.697900313971743e-05,
      "loss": 0.6996,
      "step": 307900
    },
    {
      "epoch": 4.835164835164835,
      "grad_norm": 3.8832247257232666,
      "learning_rate": 4.697802197802198e-05,
      "loss": 0.6503,
      "step": 308000
    },
    {
      "epoch": 4.836734693877551,
      "grad_norm": 3.0977115631103516,
      "learning_rate": 4.6977040816326536e-05,
      "loss": 0.6541,
      "step": 308100
    },
    {
      "epoch": 4.838304552590267,
      "grad_norm": 4.022557735443115,
      "learning_rate": 4.697605965463108e-05,
      "loss": 0.6848,
      "step": 308200
    },
    {
      "epoch": 4.839874411302983,
      "grad_norm": 4.354294300079346,
      "learning_rate": 4.697507849293564e-05,
      "loss": 0.6993,
      "step": 308300
    },
    {
      "epoch": 4.841444270015699,
      "grad_norm": 3.2843096256256104,
      "learning_rate": 4.697409733124019e-05,
      "loss": 0.6725,
      "step": 308400
    },
    {
      "epoch": 4.843014128728415,
      "grad_norm": 3.3959414958953857,
      "learning_rate": 4.697311616954475e-05,
      "loss": 0.6623,
      "step": 308500
    },
    {
      "epoch": 4.84458398744113,
      "grad_norm": 4.175196647644043,
      "learning_rate": 4.697213500784929e-05,
      "loss": 0.6895,
      "step": 308600
    },
    {
      "epoch": 4.846153846153846,
      "grad_norm": 3.5398621559143066,
      "learning_rate": 4.697115384615385e-05,
      "loss": 0.6642,
      "step": 308700
    },
    {
      "epoch": 4.847723704866562,
      "grad_norm": 5.479466915130615,
      "learning_rate": 4.69701726844584e-05,
      "loss": 0.6864,
      "step": 308800
    },
    {
      "epoch": 4.849293563579278,
      "grad_norm": 3.8167784214019775,
      "learning_rate": 4.696919152276295e-05,
      "loss": 0.6603,
      "step": 308900
    },
    {
      "epoch": 4.850863422291994,
      "grad_norm": 4.373660564422607,
      "learning_rate": 4.69682103610675e-05,
      "loss": 0.6841,
      "step": 309000
    },
    {
      "epoch": 4.85243328100471,
      "grad_norm": 4.655967712402344,
      "learning_rate": 4.696722919937206e-05,
      "loss": 0.6724,
      "step": 309100
    },
    {
      "epoch": 4.854003139717426,
      "grad_norm": 4.418309211730957,
      "learning_rate": 4.696624803767661e-05,
      "loss": 0.6843,
      "step": 309200
    },
    {
      "epoch": 4.855572998430142,
      "grad_norm": 3.122331142425537,
      "learning_rate": 4.696526687598116e-05,
      "loss": 0.726,
      "step": 309300
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 4.430784702301025,
      "learning_rate": 4.696428571428572e-05,
      "loss": 0.6886,
      "step": 309400
    },
    {
      "epoch": 4.858712715855573,
      "grad_norm": 3.549574851989746,
      "learning_rate": 4.696330455259027e-05,
      "loss": 0.6552,
      "step": 309500
    },
    {
      "epoch": 4.860282574568289,
      "grad_norm": 3.1286911964416504,
      "learning_rate": 4.696232339089482e-05,
      "loss": 0.694,
      "step": 309600
    },
    {
      "epoch": 4.861852433281005,
      "grad_norm": 4.551493167877197,
      "learning_rate": 4.696134222919937e-05,
      "loss": 0.6814,
      "step": 309700
    },
    {
      "epoch": 4.863422291993721,
      "grad_norm": 3.483138084411621,
      "learning_rate": 4.696036106750393e-05,
      "loss": 0.6954,
      "step": 309800
    },
    {
      "epoch": 4.864992150706437,
      "grad_norm": 4.306197643280029,
      "learning_rate": 4.695937990580848e-05,
      "loss": 0.6267,
      "step": 309900
    },
    {
      "epoch": 4.866562009419153,
      "grad_norm": 3.072192668914795,
      "learning_rate": 4.695839874411303e-05,
      "loss": 0.6842,
      "step": 310000
    },
    {
      "epoch": 4.868131868131869,
      "grad_norm": 4.466462135314941,
      "learning_rate": 4.695741758241758e-05,
      "loss": 0.7134,
      "step": 310100
    },
    {
      "epoch": 4.869701726844584,
      "grad_norm": 4.322058200836182,
      "learning_rate": 4.695643642072214e-05,
      "loss": 0.6342,
      "step": 310200
    },
    {
      "epoch": 4.8712715855573,
      "grad_norm": 4.6572747230529785,
      "learning_rate": 4.6955455259026685e-05,
      "loss": 0.6517,
      "step": 310300
    },
    {
      "epoch": 4.872841444270016,
      "grad_norm": 3.4166219234466553,
      "learning_rate": 4.695447409733124e-05,
      "loss": 0.6703,
      "step": 310400
    },
    {
      "epoch": 4.874411302982732,
      "grad_norm": 5.352090358734131,
      "learning_rate": 4.6953492935635794e-05,
      "loss": 0.648,
      "step": 310500
    },
    {
      "epoch": 4.875981161695448,
      "grad_norm": 3.816391944885254,
      "learning_rate": 4.695251177394035e-05,
      "loss": 0.6758,
      "step": 310600
    },
    {
      "epoch": 4.877551020408164,
      "grad_norm": 3.440355062484741,
      "learning_rate": 4.6951530612244896e-05,
      "loss": 0.6894,
      "step": 310700
    },
    {
      "epoch": 4.8791208791208796,
      "grad_norm": 3.336853504180908,
      "learning_rate": 4.6950549450549453e-05,
      "loss": 0.6397,
      "step": 310800
    },
    {
      "epoch": 4.880690737833595,
      "grad_norm": 4.841472625732422,
      "learning_rate": 4.6949568288854004e-05,
      "loss": 0.6953,
      "step": 310900
    },
    {
      "epoch": 4.882260596546311,
      "grad_norm": 3.720869541168213,
      "learning_rate": 4.6948587127158555e-05,
      "loss": 0.6726,
      "step": 311000
    },
    {
      "epoch": 4.883830455259027,
      "grad_norm": 3.5842316150665283,
      "learning_rate": 4.6947605965463106e-05,
      "loss": 0.6531,
      "step": 311100
    },
    {
      "epoch": 4.885400313971743,
      "grad_norm": 3.665050983428955,
      "learning_rate": 4.6946624803767664e-05,
      "loss": 0.6692,
      "step": 311200
    },
    {
      "epoch": 4.8869701726844585,
      "grad_norm": 3.380307674407959,
      "learning_rate": 4.6945643642072215e-05,
      "loss": 0.6854,
      "step": 311300
    },
    {
      "epoch": 4.8885400313971745,
      "grad_norm": 3.9227867126464844,
      "learning_rate": 4.6944662480376766e-05,
      "loss": 0.6608,
      "step": 311400
    },
    {
      "epoch": 4.8901098901098905,
      "grad_norm": 2.899013042449951,
      "learning_rate": 4.6943681318681324e-05,
      "loss": 0.6887,
      "step": 311500
    },
    {
      "epoch": 4.891679748822606,
      "grad_norm": 3.9743895530700684,
      "learning_rate": 4.6942700156985875e-05,
      "loss": 0.6785,
      "step": 311600
    },
    {
      "epoch": 4.893249607535322,
      "grad_norm": 4.038234233856201,
      "learning_rate": 4.6941718995290426e-05,
      "loss": 0.7067,
      "step": 311700
    },
    {
      "epoch": 4.8948194662480375,
      "grad_norm": 4.462766170501709,
      "learning_rate": 4.694073783359498e-05,
      "loss": 0.6493,
      "step": 311800
    },
    {
      "epoch": 4.8963893249607535,
      "grad_norm": 4.447329044342041,
      "learning_rate": 4.6939756671899535e-05,
      "loss": 0.631,
      "step": 311900
    },
    {
      "epoch": 4.8979591836734695,
      "grad_norm": 3.328228235244751,
      "learning_rate": 4.6938775510204086e-05,
      "loss": 0.646,
      "step": 312000
    },
    {
      "epoch": 4.8995290423861855,
      "grad_norm": 3.9237277507781982,
      "learning_rate": 4.6937794348508637e-05,
      "loss": 0.651,
      "step": 312100
    },
    {
      "epoch": 4.9010989010989015,
      "grad_norm": 5.009263515472412,
      "learning_rate": 4.693681318681319e-05,
      "loss": 0.6598,
      "step": 312200
    },
    {
      "epoch": 4.9026687598116165,
      "grad_norm": 4.105545520782471,
      "learning_rate": 4.6935832025117745e-05,
      "loss": 0.6857,
      "step": 312300
    },
    {
      "epoch": 4.9042386185243325,
      "grad_norm": 4.30509090423584,
      "learning_rate": 4.693485086342229e-05,
      "loss": 0.6779,
      "step": 312400
    },
    {
      "epoch": 4.9058084772370485,
      "grad_norm": 3.1533634662628174,
      "learning_rate": 4.693386970172685e-05,
      "loss": 0.673,
      "step": 312500
    },
    {
      "epoch": 4.9073783359497645,
      "grad_norm": 4.643099784851074,
      "learning_rate": 4.69328885400314e-05,
      "loss": 0.6785,
      "step": 312600
    },
    {
      "epoch": 4.9089481946624804,
      "grad_norm": 4.087155342102051,
      "learning_rate": 4.6931907378335956e-05,
      "loss": 0.6723,
      "step": 312700
    },
    {
      "epoch": 4.910518053375196,
      "grad_norm": 3.7905960083007812,
      "learning_rate": 4.69309262166405e-05,
      "loss": 0.7017,
      "step": 312800
    },
    {
      "epoch": 4.912087912087912,
      "grad_norm": 3.6476709842681885,
      "learning_rate": 4.692994505494506e-05,
      "loss": 0.7028,
      "step": 312900
    },
    {
      "epoch": 4.9136577708006275,
      "grad_norm": 4.042351722717285,
      "learning_rate": 4.692896389324961e-05,
      "loss": 0.7156,
      "step": 313000
    },
    {
      "epoch": 4.9152276295133435,
      "grad_norm": 3.519981861114502,
      "learning_rate": 4.692798273155416e-05,
      "loss": 0.6638,
      "step": 313100
    },
    {
      "epoch": 4.916797488226059,
      "grad_norm": 3.3178319931030273,
      "learning_rate": 4.692700156985871e-05,
      "loss": 0.6833,
      "step": 313200
    },
    {
      "epoch": 4.918367346938775,
      "grad_norm": 3.0280959606170654,
      "learning_rate": 4.692602040816327e-05,
      "loss": 0.6806,
      "step": 313300
    },
    {
      "epoch": 4.919937205651491,
      "grad_norm": 3.427962303161621,
      "learning_rate": 4.692503924646782e-05,
      "loss": 0.6662,
      "step": 313400
    },
    {
      "epoch": 4.921507064364207,
      "grad_norm": 3.4183928966522217,
      "learning_rate": 4.692405808477237e-05,
      "loss": 0.6606,
      "step": 313500
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 4.185716152191162,
      "learning_rate": 4.692307692307693e-05,
      "loss": 0.6433,
      "step": 313600
    },
    {
      "epoch": 4.924646781789638,
      "grad_norm": 4.965280532836914,
      "learning_rate": 4.692209576138148e-05,
      "loss": 0.7005,
      "step": 313700
    },
    {
      "epoch": 4.926216640502354,
      "grad_norm": 3.6015284061431885,
      "learning_rate": 4.692111459968603e-05,
      "loss": 0.6879,
      "step": 313800
    },
    {
      "epoch": 4.92778649921507,
      "grad_norm": 4.320811748504639,
      "learning_rate": 4.692013343799058e-05,
      "loss": 0.6642,
      "step": 313900
    },
    {
      "epoch": 4.929356357927786,
      "grad_norm": 2.886357069015503,
      "learning_rate": 4.691915227629514e-05,
      "loss": 0.708,
      "step": 314000
    },
    {
      "epoch": 4.930926216640502,
      "grad_norm": 4.002133369445801,
      "learning_rate": 4.691817111459969e-05,
      "loss": 0.6719,
      "step": 314100
    },
    {
      "epoch": 4.932496075353218,
      "grad_norm": 3.070521354675293,
      "learning_rate": 4.691718995290424e-05,
      "loss": 0.6639,
      "step": 314200
    },
    {
      "epoch": 4.934065934065934,
      "grad_norm": 4.023017883300781,
      "learning_rate": 4.691620879120879e-05,
      "loss": 0.6442,
      "step": 314300
    },
    {
      "epoch": 4.93563579277865,
      "grad_norm": 4.97515869140625,
      "learning_rate": 4.691522762951335e-05,
      "loss": 0.6759,
      "step": 314400
    },
    {
      "epoch": 4.937205651491366,
      "grad_norm": 4.223485946655273,
      "learning_rate": 4.6914246467817894e-05,
      "loss": 0.7059,
      "step": 314500
    },
    {
      "epoch": 4.938775510204081,
      "grad_norm": 2.1544268131256104,
      "learning_rate": 4.691326530612245e-05,
      "loss": 0.6712,
      "step": 314600
    },
    {
      "epoch": 4.940345368916797,
      "grad_norm": 3.812105894088745,
      "learning_rate": 4.6912284144427e-05,
      "loss": 0.6525,
      "step": 314700
    },
    {
      "epoch": 4.941915227629513,
      "grad_norm": 4.453269004821777,
      "learning_rate": 4.691130298273156e-05,
      "loss": 0.6731,
      "step": 314800
    },
    {
      "epoch": 4.943485086342229,
      "grad_norm": 2.9883129596710205,
      "learning_rate": 4.6910321821036105e-05,
      "loss": 0.6685,
      "step": 314900
    },
    {
      "epoch": 4.945054945054945,
      "grad_norm": 1.824498176574707,
      "learning_rate": 4.690934065934066e-05,
      "loss": 0.6886,
      "step": 315000
    },
    {
      "epoch": 4.946624803767661,
      "grad_norm": 2.820533514022827,
      "learning_rate": 4.6908359497645213e-05,
      "loss": 0.7115,
      "step": 315100
    },
    {
      "epoch": 4.948194662480377,
      "grad_norm": 3.039663314819336,
      "learning_rate": 4.6907378335949764e-05,
      "loss": 0.6456,
      "step": 315200
    },
    {
      "epoch": 4.949764521193092,
      "grad_norm": 3.8210256099700928,
      "learning_rate": 4.6906397174254315e-05,
      "loss": 0.6869,
      "step": 315300
    },
    {
      "epoch": 4.951334379905808,
      "grad_norm": 2.4554965496063232,
      "learning_rate": 4.690541601255887e-05,
      "loss": 0.6856,
      "step": 315400
    },
    {
      "epoch": 4.952904238618524,
      "grad_norm": 3.3040366172790527,
      "learning_rate": 4.6904434850863424e-05,
      "loss": 0.6431,
      "step": 315500
    },
    {
      "epoch": 4.95447409733124,
      "grad_norm": 3.5071535110473633,
      "learning_rate": 4.6903453689167975e-05,
      "loss": 0.6446,
      "step": 315600
    },
    {
      "epoch": 4.956043956043956,
      "grad_norm": 4.1980414390563965,
      "learning_rate": 4.690247252747253e-05,
      "loss": 0.6684,
      "step": 315700
    },
    {
      "epoch": 4.957613814756672,
      "grad_norm": 3.684361457824707,
      "learning_rate": 4.6901491365777084e-05,
      "loss": 0.6543,
      "step": 315800
    },
    {
      "epoch": 4.959183673469388,
      "grad_norm": 3.794440269470215,
      "learning_rate": 4.6900510204081635e-05,
      "loss": 0.6512,
      "step": 315900
    },
    {
      "epoch": 4.960753532182103,
      "grad_norm": 4.5481743812561035,
      "learning_rate": 4.6899529042386186e-05,
      "loss": 0.6478,
      "step": 316000
    },
    {
      "epoch": 4.962323390894819,
      "grad_norm": 4.4414262771606445,
      "learning_rate": 4.6898547880690744e-05,
      "loss": 0.658,
      "step": 316100
    },
    {
      "epoch": 4.963893249607535,
      "grad_norm": 4.8871049880981445,
      "learning_rate": 4.6897566718995295e-05,
      "loss": 0.7004,
      "step": 316200
    },
    {
      "epoch": 4.965463108320251,
      "grad_norm": 3.819378614425659,
      "learning_rate": 4.6896585557299846e-05,
      "loss": 0.6622,
      "step": 316300
    },
    {
      "epoch": 4.967032967032967,
      "grad_norm": 3.299570322036743,
      "learning_rate": 4.6895604395604396e-05,
      "loss": 0.6352,
      "step": 316400
    },
    {
      "epoch": 4.968602825745683,
      "grad_norm": 4.98503303527832,
      "learning_rate": 4.6894623233908954e-05,
      "loss": 0.658,
      "step": 316500
    },
    {
      "epoch": 4.970172684458399,
      "grad_norm": 3.8731000423431396,
      "learning_rate": 4.68936420722135e-05,
      "loss": 0.6613,
      "step": 316600
    },
    {
      "epoch": 4.971742543171114,
      "grad_norm": 3.288029670715332,
      "learning_rate": 4.6892660910518056e-05,
      "loss": 0.7058,
      "step": 316700
    },
    {
      "epoch": 4.97331240188383,
      "grad_norm": 4.217562675476074,
      "learning_rate": 4.689167974882261e-05,
      "loss": 0.6602,
      "step": 316800
    },
    {
      "epoch": 4.974882260596546,
      "grad_norm": 3.49578595161438,
      "learning_rate": 4.6890698587127165e-05,
      "loss": 0.6909,
      "step": 316900
    },
    {
      "epoch": 4.976452119309262,
      "grad_norm": 3.706855535507202,
      "learning_rate": 4.688971742543171e-05,
      "loss": 0.6782,
      "step": 317000
    },
    {
      "epoch": 4.978021978021978,
      "grad_norm": 2.9755406379699707,
      "learning_rate": 4.688873626373627e-05,
      "loss": 0.6577,
      "step": 317100
    },
    {
      "epoch": 4.979591836734694,
      "grad_norm": 4.0371599197387695,
      "learning_rate": 4.688775510204082e-05,
      "loss": 0.7015,
      "step": 317200
    },
    {
      "epoch": 4.98116169544741,
      "grad_norm": 3.8819961547851562,
      "learning_rate": 4.688677394034537e-05,
      "loss": 0.666,
      "step": 317300
    },
    {
      "epoch": 4.982731554160125,
      "grad_norm": 5.40482234954834,
      "learning_rate": 4.688579277864992e-05,
      "loss": 0.6605,
      "step": 317400
    },
    {
      "epoch": 4.984301412872841,
      "grad_norm": 3.185529947280884,
      "learning_rate": 4.688481161695448e-05,
      "loss": 0.6778,
      "step": 317500
    },
    {
      "epoch": 4.985871271585557,
      "grad_norm": 4.017259120941162,
      "learning_rate": 4.688383045525903e-05,
      "loss": 0.7008,
      "step": 317600
    },
    {
      "epoch": 4.987441130298273,
      "grad_norm": 3.9433982372283936,
      "learning_rate": 4.688284929356358e-05,
      "loss": 0.6803,
      "step": 317700
    },
    {
      "epoch": 4.989010989010989,
      "grad_norm": 3.3236372470855713,
      "learning_rate": 4.688186813186814e-05,
      "loss": 0.6587,
      "step": 317800
    },
    {
      "epoch": 4.990580847723705,
      "grad_norm": 4.844809055328369,
      "learning_rate": 4.688088697017269e-05,
      "loss": 0.6968,
      "step": 317900
    },
    {
      "epoch": 4.992150706436421,
      "grad_norm": 3.734311819076538,
      "learning_rate": 4.687990580847724e-05,
      "loss": 0.6721,
      "step": 318000
    },
    {
      "epoch": 4.993720565149136,
      "grad_norm": 4.881266117095947,
      "learning_rate": 4.687892464678179e-05,
      "loss": 0.631,
      "step": 318100
    },
    {
      "epoch": 4.995290423861852,
      "grad_norm": 3.784034013748169,
      "learning_rate": 4.687794348508635e-05,
      "loss": 0.7128,
      "step": 318200
    },
    {
      "epoch": 4.996860282574568,
      "grad_norm": 3.753624200820923,
      "learning_rate": 4.68769623233909e-05,
      "loss": 0.6436,
      "step": 318300
    },
    {
      "epoch": 4.998430141287284,
      "grad_norm": 3.5077760219573975,
      "learning_rate": 4.687598116169545e-05,
      "loss": 0.6674,
      "step": 318400
    },
    {
      "epoch": 5.0,
      "grad_norm": 3.9231975078582764,
      "learning_rate": 4.6875e-05,
      "loss": 0.7272,
      "step": 318500
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.021451473236084,
      "eval_runtime": 15.3348,
      "eval_samples_per_second": 218.652,
      "eval_steps_per_second": 218.652,
      "step": 318500
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5247576236724854,
      "eval_runtime": 291.7464,
      "eval_samples_per_second": 218.34,
      "eval_steps_per_second": 218.34,
      "step": 318500
    },
    {
      "epoch": 5.001569858712716,
      "grad_norm": 3.8899219036102295,
      "learning_rate": 4.687401883830456e-05,
      "loss": 0.6493,
      "step": 318600
    },
    {
      "epoch": 5.003139717425432,
      "grad_norm": 3.761291265487671,
      "learning_rate": 4.68730376766091e-05,
      "loss": 0.6758,
      "step": 318700
    },
    {
      "epoch": 5.004709576138148,
      "grad_norm": 4.149172782897949,
      "learning_rate": 4.687205651491366e-05,
      "loss": 0.623,
      "step": 318800
    },
    {
      "epoch": 5.006279434850863,
      "grad_norm": 4.002706050872803,
      "learning_rate": 4.687107535321821e-05,
      "loss": 0.644,
      "step": 318900
    },
    {
      "epoch": 5.007849293563579,
      "grad_norm": 4.105700492858887,
      "learning_rate": 4.687009419152277e-05,
      "loss": 0.7203,
      "step": 319000
    },
    {
      "epoch": 5.009419152276295,
      "grad_norm": 5.531335353851318,
      "learning_rate": 4.6869113029827314e-05,
      "loss": 0.6445,
      "step": 319100
    },
    {
      "epoch": 5.010989010989011,
      "grad_norm": 2.6235013008117676,
      "learning_rate": 4.686813186813187e-05,
      "loss": 0.6848,
      "step": 319200
    },
    {
      "epoch": 5.012558869701727,
      "grad_norm": 4.201631546020508,
      "learning_rate": 4.686715070643642e-05,
      "loss": 0.6751,
      "step": 319300
    },
    {
      "epoch": 5.014128728414443,
      "grad_norm": 3.1810660362243652,
      "learning_rate": 4.686616954474097e-05,
      "loss": 0.6751,
      "step": 319400
    },
    {
      "epoch": 5.015698587127159,
      "grad_norm": 3.47135066986084,
      "learning_rate": 4.6865188383045524e-05,
      "loss": 0.6454,
      "step": 319500
    },
    {
      "epoch": 5.017268445839874,
      "grad_norm": 3.4526405334472656,
      "learning_rate": 4.686420722135008e-05,
      "loss": 0.6587,
      "step": 319600
    },
    {
      "epoch": 5.01883830455259,
      "grad_norm": 3.338526487350464,
      "learning_rate": 4.686322605965463e-05,
      "loss": 0.6676,
      "step": 319700
    },
    {
      "epoch": 5.020408163265306,
      "grad_norm": 3.4724442958831787,
      "learning_rate": 4.6862244897959184e-05,
      "loss": 0.6441,
      "step": 319800
    },
    {
      "epoch": 5.021978021978022,
      "grad_norm": 2.436013698577881,
      "learning_rate": 4.686126373626374e-05,
      "loss": 0.6782,
      "step": 319900
    },
    {
      "epoch": 5.023547880690738,
      "grad_norm": 3.6659133434295654,
      "learning_rate": 4.686028257456829e-05,
      "loss": 0.6881,
      "step": 320000
    },
    {
      "epoch": 5.025117739403454,
      "grad_norm": 3.4519548416137695,
      "learning_rate": 4.6859301412872844e-05,
      "loss": 0.6347,
      "step": 320100
    },
    {
      "epoch": 5.02668759811617,
      "grad_norm": 5.240691184997559,
      "learning_rate": 4.6858320251177395e-05,
      "loss": 0.6799,
      "step": 320200
    },
    {
      "epoch": 5.028257456828886,
      "grad_norm": 3.8283073902130127,
      "learning_rate": 4.685733908948195e-05,
      "loss": 0.6987,
      "step": 320300
    },
    {
      "epoch": 5.029827315541601,
      "grad_norm": 4.798040866851807,
      "learning_rate": 4.6856357927786504e-05,
      "loss": 0.6849,
      "step": 320400
    },
    {
      "epoch": 5.031397174254317,
      "grad_norm": 5.168225288391113,
      "learning_rate": 4.6855376766091054e-05,
      "loss": 0.669,
      "step": 320500
    },
    {
      "epoch": 5.032967032967033,
      "grad_norm": 3.602501392364502,
      "learning_rate": 4.6854395604395605e-05,
      "loss": 0.7128,
      "step": 320600
    },
    {
      "epoch": 5.034536891679749,
      "grad_norm": 3.646557092666626,
      "learning_rate": 4.685341444270016e-05,
      "loss": 0.6462,
      "step": 320700
    },
    {
      "epoch": 5.036106750392465,
      "grad_norm": 3.1352956295013428,
      "learning_rate": 4.685243328100471e-05,
      "loss": 0.6072,
      "step": 320800
    },
    {
      "epoch": 5.037676609105181,
      "grad_norm": 4.4778900146484375,
      "learning_rate": 4.6851452119309265e-05,
      "loss": 0.6661,
      "step": 320900
    },
    {
      "epoch": 5.039246467817897,
      "grad_norm": 3.187004566192627,
      "learning_rate": 4.6850470957613816e-05,
      "loss": 0.6781,
      "step": 321000
    },
    {
      "epoch": 5.040816326530612,
      "grad_norm": 4.143573760986328,
      "learning_rate": 4.6849489795918374e-05,
      "loss": 0.672,
      "step": 321100
    },
    {
      "epoch": 5.042386185243328,
      "grad_norm": 4.673798084259033,
      "learning_rate": 4.684850863422292e-05,
      "loss": 0.6611,
      "step": 321200
    },
    {
      "epoch": 5.043956043956044,
      "grad_norm": 4.5276408195495605,
      "learning_rate": 4.6847527472527476e-05,
      "loss": 0.6819,
      "step": 321300
    },
    {
      "epoch": 5.04552590266876,
      "grad_norm": 4.179312229156494,
      "learning_rate": 4.684654631083203e-05,
      "loss": 0.6589,
      "step": 321400
    },
    {
      "epoch": 5.047095761381476,
      "grad_norm": 4.929173469543457,
      "learning_rate": 4.684556514913658e-05,
      "loss": 0.6516,
      "step": 321500
    },
    {
      "epoch": 5.048665620094192,
      "grad_norm": 4.6728644371032715,
      "learning_rate": 4.684458398744113e-05,
      "loss": 0.6471,
      "step": 321600
    },
    {
      "epoch": 5.050235478806908,
      "grad_norm": 3.9253883361816406,
      "learning_rate": 4.6843602825745687e-05,
      "loss": 0.658,
      "step": 321700
    },
    {
      "epoch": 5.051805337519623,
      "grad_norm": 4.552691459655762,
      "learning_rate": 4.684262166405024e-05,
      "loss": 0.6294,
      "step": 321800
    },
    {
      "epoch": 5.053375196232339,
      "grad_norm": 4.4280476570129395,
      "learning_rate": 4.684164050235479e-05,
      "loss": 0.6648,
      "step": 321900
    },
    {
      "epoch": 5.054945054945055,
      "grad_norm": 3.714308261871338,
      "learning_rate": 4.6840659340659346e-05,
      "loss": 0.6414,
      "step": 322000
    },
    {
      "epoch": 5.056514913657771,
      "grad_norm": 5.640679359436035,
      "learning_rate": 4.68396781789639e-05,
      "loss": 0.6207,
      "step": 322100
    },
    {
      "epoch": 5.058084772370487,
      "grad_norm": 3.7521719932556152,
      "learning_rate": 4.683869701726845e-05,
      "loss": 0.6601,
      "step": 322200
    },
    {
      "epoch": 5.059654631083203,
      "grad_norm": 4.391061305999756,
      "learning_rate": 4.6837715855573e-05,
      "loss": 0.6517,
      "step": 322300
    },
    {
      "epoch": 5.061224489795919,
      "grad_norm": 4.295456886291504,
      "learning_rate": 4.683673469387756e-05,
      "loss": 0.6526,
      "step": 322400
    },
    {
      "epoch": 5.062794348508635,
      "grad_norm": 3.788799285888672,
      "learning_rate": 4.683575353218211e-05,
      "loss": 0.6404,
      "step": 322500
    },
    {
      "epoch": 5.06436420722135,
      "grad_norm": 3.6494946479797363,
      "learning_rate": 4.683477237048666e-05,
      "loss": 0.656,
      "step": 322600
    },
    {
      "epoch": 5.065934065934066,
      "grad_norm": 5.4038987159729,
      "learning_rate": 4.683379120879121e-05,
      "loss": 0.6676,
      "step": 322700
    },
    {
      "epoch": 5.067503924646782,
      "grad_norm": 3.942432403564453,
      "learning_rate": 4.683281004709577e-05,
      "loss": 0.6743,
      "step": 322800
    },
    {
      "epoch": 5.069073783359498,
      "grad_norm": 3.8175272941589355,
      "learning_rate": 4.683182888540031e-05,
      "loss": 0.6757,
      "step": 322900
    },
    {
      "epoch": 5.070643642072214,
      "grad_norm": 4.457486629486084,
      "learning_rate": 4.683084772370487e-05,
      "loss": 0.6637,
      "step": 323000
    },
    {
      "epoch": 5.07221350078493,
      "grad_norm": 3.756697177886963,
      "learning_rate": 4.682986656200942e-05,
      "loss": 0.6638,
      "step": 323100
    },
    {
      "epoch": 5.073783359497646,
      "grad_norm": 4.085574150085449,
      "learning_rate": 4.682888540031398e-05,
      "loss": 0.6804,
      "step": 323200
    },
    {
      "epoch": 5.075353218210361,
      "grad_norm": 4.372395992279053,
      "learning_rate": 4.682790423861852e-05,
      "loss": 0.6234,
      "step": 323300
    },
    {
      "epoch": 5.076923076923077,
      "grad_norm": 3.640660047531128,
      "learning_rate": 4.682692307692308e-05,
      "loss": 0.6702,
      "step": 323400
    },
    {
      "epoch": 5.078492935635793,
      "grad_norm": 4.268948078155518,
      "learning_rate": 4.682594191522763e-05,
      "loss": 0.6824,
      "step": 323500
    },
    {
      "epoch": 5.080062794348509,
      "grad_norm": 4.158763885498047,
      "learning_rate": 4.682496075353218e-05,
      "loss": 0.6535,
      "step": 323600
    },
    {
      "epoch": 5.081632653061225,
      "grad_norm": 4.341344833374023,
      "learning_rate": 4.682397959183673e-05,
      "loss": 0.6568,
      "step": 323700
    },
    {
      "epoch": 5.083202511773941,
      "grad_norm": 3.4825682640075684,
      "learning_rate": 4.682299843014129e-05,
      "loss": 0.641,
      "step": 323800
    },
    {
      "epoch": 5.0847723704866565,
      "grad_norm": 4.286757946014404,
      "learning_rate": 4.682201726844584e-05,
      "loss": 0.6966,
      "step": 323900
    },
    {
      "epoch": 5.086342229199372,
      "grad_norm": 3.673963785171509,
      "learning_rate": 4.682103610675039e-05,
      "loss": 0.6532,
      "step": 324000
    },
    {
      "epoch": 5.087912087912088,
      "grad_norm": 4.662906169891357,
      "learning_rate": 4.682005494505495e-05,
      "loss": 0.6832,
      "step": 324100
    },
    {
      "epoch": 5.089481946624804,
      "grad_norm": 4.542591571807861,
      "learning_rate": 4.68190737833595e-05,
      "loss": 0.637,
      "step": 324200
    },
    {
      "epoch": 5.0910518053375196,
      "grad_norm": 4.675800800323486,
      "learning_rate": 4.681809262166405e-05,
      "loss": 0.637,
      "step": 324300
    },
    {
      "epoch": 5.0926216640502355,
      "grad_norm": 3.0784409046173096,
      "learning_rate": 4.6817111459968604e-05,
      "loss": 0.6765,
      "step": 324400
    },
    {
      "epoch": 5.0941915227629515,
      "grad_norm": 4.683157920837402,
      "learning_rate": 4.681613029827316e-05,
      "loss": 0.6526,
      "step": 324500
    },
    {
      "epoch": 5.0957613814756675,
      "grad_norm": 4.081713676452637,
      "learning_rate": 4.681514913657771e-05,
      "loss": 0.654,
      "step": 324600
    },
    {
      "epoch": 5.0973312401883835,
      "grad_norm": 4.407011985778809,
      "learning_rate": 4.6814167974882263e-05,
      "loss": 0.6257,
      "step": 324700
    },
    {
      "epoch": 5.0989010989010985,
      "grad_norm": 5.10946798324585,
      "learning_rate": 4.6813186813186814e-05,
      "loss": 0.6313,
      "step": 324800
    },
    {
      "epoch": 5.1004709576138145,
      "grad_norm": 4.0366530418396,
      "learning_rate": 4.681220565149137e-05,
      "loss": 0.6779,
      "step": 324900
    },
    {
      "epoch": 5.1020408163265305,
      "grad_norm": 3.556621789932251,
      "learning_rate": 4.6811224489795916e-05,
      "loss": 0.694,
      "step": 325000
    },
    {
      "epoch": 5.1036106750392465,
      "grad_norm": 3.0131676197052,
      "learning_rate": 4.6810243328100474e-05,
      "loss": 0.6598,
      "step": 325100
    },
    {
      "epoch": 5.1051805337519625,
      "grad_norm": 3.3924055099487305,
      "learning_rate": 4.6809262166405025e-05,
      "loss": 0.697,
      "step": 325200
    },
    {
      "epoch": 5.106750392464678,
      "grad_norm": 2.7948501110076904,
      "learning_rate": 4.680828100470958e-05,
      "loss": 0.6428,
      "step": 325300
    },
    {
      "epoch": 5.108320251177394,
      "grad_norm": 3.9757680892944336,
      "learning_rate": 4.680729984301413e-05,
      "loss": 0.6702,
      "step": 325400
    },
    {
      "epoch": 5.1098901098901095,
      "grad_norm": 3.970337390899658,
      "learning_rate": 4.6806318681318685e-05,
      "loss": 0.6762,
      "step": 325500
    },
    {
      "epoch": 5.1114599686028255,
      "grad_norm": 4.169715881347656,
      "learning_rate": 4.6805337519623236e-05,
      "loss": 0.6057,
      "step": 325600
    },
    {
      "epoch": 5.1130298273155415,
      "grad_norm": 3.6584253311157227,
      "learning_rate": 4.680435635792779e-05,
      "loss": 0.6568,
      "step": 325700
    },
    {
      "epoch": 5.114599686028257,
      "grad_norm": 3.651970386505127,
      "learning_rate": 4.680337519623234e-05,
      "loss": 0.6739,
      "step": 325800
    },
    {
      "epoch": 5.116169544740973,
      "grad_norm": 4.575229167938232,
      "learning_rate": 4.6802394034536896e-05,
      "loss": 0.6649,
      "step": 325900
    },
    {
      "epoch": 5.117739403453689,
      "grad_norm": 4.7632951736450195,
      "learning_rate": 4.6801412872841447e-05,
      "loss": 0.6896,
      "step": 326000
    },
    {
      "epoch": 5.119309262166405,
      "grad_norm": 3.7498552799224854,
      "learning_rate": 4.6800431711146e-05,
      "loss": 0.6478,
      "step": 326100
    },
    {
      "epoch": 5.1208791208791204,
      "grad_norm": 3.736820936203003,
      "learning_rate": 4.6799450549450555e-05,
      "loss": 0.6324,
      "step": 326200
    },
    {
      "epoch": 5.122448979591836,
      "grad_norm": 4.239162445068359,
      "learning_rate": 4.6798469387755106e-05,
      "loss": 0.6335,
      "step": 326300
    },
    {
      "epoch": 5.124018838304552,
      "grad_norm": 3.8917276859283447,
      "learning_rate": 4.679748822605966e-05,
      "loss": 0.6902,
      "step": 326400
    },
    {
      "epoch": 5.125588697017268,
      "grad_norm": 2.99218487739563,
      "learning_rate": 4.679650706436421e-05,
      "loss": 0.6481,
      "step": 326500
    },
    {
      "epoch": 5.127158555729984,
      "grad_norm": 3.752988338470459,
      "learning_rate": 4.6795525902668766e-05,
      "loss": 0.6595,
      "step": 326600
    },
    {
      "epoch": 5.1287284144427,
      "grad_norm": 4.356729030609131,
      "learning_rate": 4.679454474097332e-05,
      "loss": 0.6663,
      "step": 326700
    },
    {
      "epoch": 5.130298273155416,
      "grad_norm": 2.3822896480560303,
      "learning_rate": 4.679356357927787e-05,
      "loss": 0.6614,
      "step": 326800
    },
    {
      "epoch": 5.131868131868132,
      "grad_norm": 4.182880878448486,
      "learning_rate": 4.679258241758242e-05,
      "loss": 0.7076,
      "step": 326900
    },
    {
      "epoch": 5.133437990580847,
      "grad_norm": 4.10748815536499,
      "learning_rate": 4.679160125588698e-05,
      "loss": 0.655,
      "step": 327000
    },
    {
      "epoch": 5.135007849293563,
      "grad_norm": 4.558518886566162,
      "learning_rate": 4.679062009419152e-05,
      "loss": 0.6673,
      "step": 327100
    },
    {
      "epoch": 5.136577708006279,
      "grad_norm": 5.195411205291748,
      "learning_rate": 4.678963893249608e-05,
      "loss": 0.6676,
      "step": 327200
    },
    {
      "epoch": 5.138147566718995,
      "grad_norm": 3.945733070373535,
      "learning_rate": 4.678865777080063e-05,
      "loss": 0.6762,
      "step": 327300
    },
    {
      "epoch": 5.139717425431711,
      "grad_norm": 3.5418167114257812,
      "learning_rate": 4.678767660910519e-05,
      "loss": 0.6775,
      "step": 327400
    },
    {
      "epoch": 5.141287284144427,
      "grad_norm": 4.706028461456299,
      "learning_rate": 4.678669544740973e-05,
      "loss": 0.6437,
      "step": 327500
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 4.00163459777832,
      "learning_rate": 4.678571428571429e-05,
      "loss": 0.6294,
      "step": 327600
    },
    {
      "epoch": 5.144427001569858,
      "grad_norm": 4.054046630859375,
      "learning_rate": 4.678473312401884e-05,
      "loss": 0.6558,
      "step": 327700
    },
    {
      "epoch": 5.145996860282574,
      "grad_norm": 3.4377057552337646,
      "learning_rate": 4.678375196232339e-05,
      "loss": 0.6871,
      "step": 327800
    },
    {
      "epoch": 5.14756671899529,
      "grad_norm": 4.041555404663086,
      "learning_rate": 4.678277080062794e-05,
      "loss": 0.6718,
      "step": 327900
    },
    {
      "epoch": 5.149136577708006,
      "grad_norm": 4.551620960235596,
      "learning_rate": 4.67817896389325e-05,
      "loss": 0.6957,
      "step": 328000
    },
    {
      "epoch": 5.150706436420722,
      "grad_norm": 4.048908233642578,
      "learning_rate": 4.678080847723705e-05,
      "loss": 0.6463,
      "step": 328100
    },
    {
      "epoch": 5.152276295133438,
      "grad_norm": 4.126847267150879,
      "learning_rate": 4.67798273155416e-05,
      "loss": 0.6632,
      "step": 328200
    },
    {
      "epoch": 5.153846153846154,
      "grad_norm": 4.401174545288086,
      "learning_rate": 4.677884615384616e-05,
      "loss": 0.6643,
      "step": 328300
    },
    {
      "epoch": 5.155416012558869,
      "grad_norm": 3.4675114154815674,
      "learning_rate": 4.677786499215071e-05,
      "loss": 0.6565,
      "step": 328400
    },
    {
      "epoch": 5.156985871271585,
      "grad_norm": 5.144802093505859,
      "learning_rate": 4.677688383045526e-05,
      "loss": 0.6577,
      "step": 328500
    },
    {
      "epoch": 5.158555729984301,
      "grad_norm": 3.943711757659912,
      "learning_rate": 4.677590266875981e-05,
      "loss": 0.6535,
      "step": 328600
    },
    {
      "epoch": 5.160125588697017,
      "grad_norm": 4.9992523193359375,
      "learning_rate": 4.677492150706437e-05,
      "loss": 0.7013,
      "step": 328700
    },
    {
      "epoch": 5.161695447409733,
      "grad_norm": 3.9164645671844482,
      "learning_rate": 4.677394034536892e-05,
      "loss": 0.6435,
      "step": 328800
    },
    {
      "epoch": 5.163265306122449,
      "grad_norm": 3.9685721397399902,
      "learning_rate": 4.677295918367347e-05,
      "loss": 0.6753,
      "step": 328900
    },
    {
      "epoch": 5.164835164835165,
      "grad_norm": 7.174018859863281,
      "learning_rate": 4.6771978021978023e-05,
      "loss": 0.6203,
      "step": 329000
    },
    {
      "epoch": 5.166405023547881,
      "grad_norm": 4.208096981048584,
      "learning_rate": 4.677099686028258e-05,
      "loss": 0.653,
      "step": 329100
    },
    {
      "epoch": 5.167974882260596,
      "grad_norm": 4.248377323150635,
      "learning_rate": 4.6770015698587125e-05,
      "loss": 0.6879,
      "step": 329200
    },
    {
      "epoch": 5.169544740973312,
      "grad_norm": 3.4788734912872314,
      "learning_rate": 4.676903453689168e-05,
      "loss": 0.622,
      "step": 329300
    },
    {
      "epoch": 5.171114599686028,
      "grad_norm": 4.304073333740234,
      "learning_rate": 4.6768053375196234e-05,
      "loss": 0.6814,
      "step": 329400
    },
    {
      "epoch": 5.172684458398744,
      "grad_norm": 4.160642623901367,
      "learning_rate": 4.676707221350079e-05,
      "loss": 0.6648,
      "step": 329500
    },
    {
      "epoch": 5.17425431711146,
      "grad_norm": 3.6399216651916504,
      "learning_rate": 4.6766091051805336e-05,
      "loss": 0.6551,
      "step": 329600
    },
    {
      "epoch": 5.175824175824176,
      "grad_norm": 3.6923842430114746,
      "learning_rate": 4.6765109890109894e-05,
      "loss": 0.6775,
      "step": 329700
    },
    {
      "epoch": 5.177394034536892,
      "grad_norm": 3.8419065475463867,
      "learning_rate": 4.6764128728414445e-05,
      "loss": 0.6683,
      "step": 329800
    },
    {
      "epoch": 5.178963893249607,
      "grad_norm": 2.997887372970581,
      "learning_rate": 4.6763147566718996e-05,
      "loss": 0.6629,
      "step": 329900
    },
    {
      "epoch": 5.180533751962323,
      "grad_norm": 3.9138028621673584,
      "learning_rate": 4.676216640502355e-05,
      "loss": 0.6823,
      "step": 330000
    },
    {
      "epoch": 5.182103610675039,
      "grad_norm": 4.262688159942627,
      "learning_rate": 4.6761185243328105e-05,
      "loss": 0.6444,
      "step": 330100
    },
    {
      "epoch": 5.183673469387755,
      "grad_norm": 3.957052230834961,
      "learning_rate": 4.6760204081632656e-05,
      "loss": 0.6389,
      "step": 330200
    },
    {
      "epoch": 5.185243328100471,
      "grad_norm": 4.35952615737915,
      "learning_rate": 4.6759222919937207e-05,
      "loss": 0.6257,
      "step": 330300
    },
    {
      "epoch": 5.186813186813187,
      "grad_norm": 5.227481842041016,
      "learning_rate": 4.6758241758241764e-05,
      "loss": 0.6806,
      "step": 330400
    },
    {
      "epoch": 5.188383045525903,
      "grad_norm": 4.529896259307861,
      "learning_rate": 4.6757260596546315e-05,
      "loss": 0.6573,
      "step": 330500
    },
    {
      "epoch": 5.189952904238618,
      "grad_norm": 4.178516387939453,
      "learning_rate": 4.6756279434850866e-05,
      "loss": 0.6613,
      "step": 330600
    },
    {
      "epoch": 5.191522762951334,
      "grad_norm": 4.427163124084473,
      "learning_rate": 4.675529827315542e-05,
      "loss": 0.6867,
      "step": 330700
    },
    {
      "epoch": 5.19309262166405,
      "grad_norm": 4.269001483917236,
      "learning_rate": 4.6754317111459975e-05,
      "loss": 0.673,
      "step": 330800
    },
    {
      "epoch": 5.194662480376766,
      "grad_norm": 4.1773552894592285,
      "learning_rate": 4.675333594976452e-05,
      "loss": 0.6509,
      "step": 330900
    },
    {
      "epoch": 5.196232339089482,
      "grad_norm": 4.742835998535156,
      "learning_rate": 4.675235478806908e-05,
      "loss": 0.6396,
      "step": 331000
    },
    {
      "epoch": 5.197802197802198,
      "grad_norm": 4.413194179534912,
      "learning_rate": 4.675137362637363e-05,
      "loss": 0.6527,
      "step": 331100
    },
    {
      "epoch": 5.199372056514914,
      "grad_norm": 3.9214696884155273,
      "learning_rate": 4.6750392464678186e-05,
      "loss": 0.694,
      "step": 331200
    },
    {
      "epoch": 5.20094191522763,
      "grad_norm": 3.677924394607544,
      "learning_rate": 4.674941130298273e-05,
      "loss": 0.6687,
      "step": 331300
    },
    {
      "epoch": 5.202511773940345,
      "grad_norm": 2.819831609725952,
      "learning_rate": 4.674843014128729e-05,
      "loss": 0.6552,
      "step": 331400
    },
    {
      "epoch": 5.204081632653061,
      "grad_norm": 3.0791125297546387,
      "learning_rate": 4.674744897959184e-05,
      "loss": 0.6635,
      "step": 331500
    },
    {
      "epoch": 5.205651491365777,
      "grad_norm": 4.432744979858398,
      "learning_rate": 4.674646781789639e-05,
      "loss": 0.6818,
      "step": 331600
    },
    {
      "epoch": 5.207221350078493,
      "grad_norm": 3.4621732234954834,
      "learning_rate": 4.674548665620094e-05,
      "loss": 0.6626,
      "step": 331700
    },
    {
      "epoch": 5.208791208791209,
      "grad_norm": 3.8325107097625732,
      "learning_rate": 4.67445054945055e-05,
      "loss": 0.661,
      "step": 331800
    },
    {
      "epoch": 5.210361067503925,
      "grad_norm": 3.6310436725616455,
      "learning_rate": 4.674352433281005e-05,
      "loss": 0.7195,
      "step": 331900
    },
    {
      "epoch": 5.211930926216641,
      "grad_norm": 3.4905169010162354,
      "learning_rate": 4.67425431711146e-05,
      "loss": 0.6774,
      "step": 332000
    },
    {
      "epoch": 5.213500784929356,
      "grad_norm": 3.3326239585876465,
      "learning_rate": 4.674156200941915e-05,
      "loss": 0.6809,
      "step": 332100
    },
    {
      "epoch": 5.215070643642072,
      "grad_norm": 3.3005752563476562,
      "learning_rate": 4.674058084772371e-05,
      "loss": 0.6812,
      "step": 332200
    },
    {
      "epoch": 5.216640502354788,
      "grad_norm": 4.284373760223389,
      "learning_rate": 4.673959968602826e-05,
      "loss": 0.6472,
      "step": 332300
    },
    {
      "epoch": 5.218210361067504,
      "grad_norm": 3.765693187713623,
      "learning_rate": 4.673861852433281e-05,
      "loss": 0.6294,
      "step": 332400
    },
    {
      "epoch": 5.21978021978022,
      "grad_norm": 4.557390213012695,
      "learning_rate": 4.673763736263736e-05,
      "loss": 0.6513,
      "step": 332500
    },
    {
      "epoch": 5.221350078492936,
      "grad_norm": 3.8662946224212646,
      "learning_rate": 4.673665620094192e-05,
      "loss": 0.6518,
      "step": 332600
    },
    {
      "epoch": 5.222919937205652,
      "grad_norm": 4.272804260253906,
      "learning_rate": 4.673567503924647e-05,
      "loss": 0.6804,
      "step": 332700
    },
    {
      "epoch": 5.224489795918367,
      "grad_norm": 3.8986761569976807,
      "learning_rate": 4.673469387755102e-05,
      "loss": 0.6849,
      "step": 332800
    },
    {
      "epoch": 5.226059654631083,
      "grad_norm": 3.9841248989105225,
      "learning_rate": 4.673371271585558e-05,
      "loss": 0.6425,
      "step": 332900
    },
    {
      "epoch": 5.227629513343799,
      "grad_norm": 3.342165470123291,
      "learning_rate": 4.6732731554160124e-05,
      "loss": 0.6985,
      "step": 333000
    },
    {
      "epoch": 5.229199372056515,
      "grad_norm": 3.6751413345336914,
      "learning_rate": 4.673175039246468e-05,
      "loss": 0.6972,
      "step": 333100
    },
    {
      "epoch": 5.230769230769231,
      "grad_norm": 4.481664657592773,
      "learning_rate": 4.673076923076923e-05,
      "loss": 0.6409,
      "step": 333200
    },
    {
      "epoch": 5.232339089481947,
      "grad_norm": 1.7653312683105469,
      "learning_rate": 4.672978806907379e-05,
      "loss": 0.6669,
      "step": 333300
    },
    {
      "epoch": 5.233908948194663,
      "grad_norm": 3.4322497844696045,
      "learning_rate": 4.6728806907378334e-05,
      "loss": 0.6668,
      "step": 333400
    },
    {
      "epoch": 5.235478806907379,
      "grad_norm": 3.1909632682800293,
      "learning_rate": 4.672782574568289e-05,
      "loss": 0.6761,
      "step": 333500
    },
    {
      "epoch": 5.237048665620094,
      "grad_norm": 4.109644889831543,
      "learning_rate": 4.672684458398744e-05,
      "loss": 0.6572,
      "step": 333600
    },
    {
      "epoch": 5.23861852433281,
      "grad_norm": 2.876797676086426,
      "learning_rate": 4.6725863422291994e-05,
      "loss": 0.6724,
      "step": 333700
    },
    {
      "epoch": 5.240188383045526,
      "grad_norm": 3.5581021308898926,
      "learning_rate": 4.6724882260596545e-05,
      "loss": 0.7254,
      "step": 333800
    },
    {
      "epoch": 5.241758241758242,
      "grad_norm": 4.667768955230713,
      "learning_rate": 4.67239010989011e-05,
      "loss": 0.6397,
      "step": 333900
    },
    {
      "epoch": 5.243328100470958,
      "grad_norm": 4.42426872253418,
      "learning_rate": 4.6722919937205654e-05,
      "loss": 0.6405,
      "step": 334000
    },
    {
      "epoch": 5.244897959183674,
      "grad_norm": 4.376983165740967,
      "learning_rate": 4.6721938775510205e-05,
      "loss": 0.6714,
      "step": 334100
    },
    {
      "epoch": 5.24646781789639,
      "grad_norm": 4.515996932983398,
      "learning_rate": 4.6720957613814756e-05,
      "loss": 0.664,
      "step": 334200
    },
    {
      "epoch": 5.248037676609105,
      "grad_norm": 4.0845136642456055,
      "learning_rate": 4.6719976452119314e-05,
      "loss": 0.6763,
      "step": 334300
    },
    {
      "epoch": 5.249607535321821,
      "grad_norm": 4.374002933502197,
      "learning_rate": 4.6718995290423865e-05,
      "loss": 0.6519,
      "step": 334400
    },
    {
      "epoch": 5.251177394034537,
      "grad_norm": 3.157345771789551,
      "learning_rate": 4.6718014128728415e-05,
      "loss": 0.6613,
      "step": 334500
    },
    {
      "epoch": 5.252747252747253,
      "grad_norm": 3.6911561489105225,
      "learning_rate": 4.6717032967032966e-05,
      "loss": 0.6346,
      "step": 334600
    },
    {
      "epoch": 5.254317111459969,
      "grad_norm": 4.2307515144348145,
      "learning_rate": 4.6716051805337524e-05,
      "loss": 0.7049,
      "step": 334700
    },
    {
      "epoch": 5.255886970172685,
      "grad_norm": 3.429110527038574,
      "learning_rate": 4.6715070643642075e-05,
      "loss": 0.6618,
      "step": 334800
    },
    {
      "epoch": 5.257456828885401,
      "grad_norm": 4.771365165710449,
      "learning_rate": 4.6714089481946626e-05,
      "loss": 0.7114,
      "step": 334900
    },
    {
      "epoch": 5.259026687598116,
      "grad_norm": 3.224839925765991,
      "learning_rate": 4.6713108320251184e-05,
      "loss": 0.7028,
      "step": 335000
    },
    {
      "epoch": 5.260596546310832,
      "grad_norm": 4.736693382263184,
      "learning_rate": 4.671212715855573e-05,
      "loss": 0.6648,
      "step": 335100
    },
    {
      "epoch": 5.262166405023548,
      "grad_norm": 3.5509324073791504,
      "learning_rate": 4.6711145996860286e-05,
      "loss": 0.6716,
      "step": 335200
    },
    {
      "epoch": 5.263736263736264,
      "grad_norm": 3.8924922943115234,
      "learning_rate": 4.671016483516484e-05,
      "loss": 0.6782,
      "step": 335300
    },
    {
      "epoch": 5.26530612244898,
      "grad_norm": 4.793665885925293,
      "learning_rate": 4.6709183673469395e-05,
      "loss": 0.6618,
      "step": 335400
    },
    {
      "epoch": 5.266875981161696,
      "grad_norm": 4.022476673126221,
      "learning_rate": 4.670820251177394e-05,
      "loss": 0.6847,
      "step": 335500
    },
    {
      "epoch": 5.268445839874412,
      "grad_norm": 4.505865097045898,
      "learning_rate": 4.67072213500785e-05,
      "loss": 0.6686,
      "step": 335600
    },
    {
      "epoch": 5.270015698587127,
      "grad_norm": 3.8007431030273438,
      "learning_rate": 4.670624018838305e-05,
      "loss": 0.6615,
      "step": 335700
    },
    {
      "epoch": 5.271585557299843,
      "grad_norm": 4.13673210144043,
      "learning_rate": 4.67052590266876e-05,
      "loss": 0.658,
      "step": 335800
    },
    {
      "epoch": 5.273155416012559,
      "grad_norm": 4.043201446533203,
      "learning_rate": 4.670427786499215e-05,
      "loss": 0.6576,
      "step": 335900
    },
    {
      "epoch": 5.274725274725275,
      "grad_norm": 3.678246259689331,
      "learning_rate": 4.670329670329671e-05,
      "loss": 0.7263,
      "step": 336000
    },
    {
      "epoch": 5.276295133437991,
      "grad_norm": 4.72843599319458,
      "learning_rate": 4.670231554160126e-05,
      "loss": 0.6641,
      "step": 336100
    },
    {
      "epoch": 5.277864992150707,
      "grad_norm": 3.393179416656494,
      "learning_rate": 4.670133437990581e-05,
      "loss": 0.6612,
      "step": 336200
    },
    {
      "epoch": 5.279434850863423,
      "grad_norm": 4.036086559295654,
      "learning_rate": 4.670035321821036e-05,
      "loss": 0.6487,
      "step": 336300
    },
    {
      "epoch": 5.2810047095761385,
      "grad_norm": 3.2833642959594727,
      "learning_rate": 4.669937205651492e-05,
      "loss": 0.6817,
      "step": 336400
    },
    {
      "epoch": 5.282574568288854,
      "grad_norm": 3.946735143661499,
      "learning_rate": 4.669839089481947e-05,
      "loss": 0.6406,
      "step": 336500
    },
    {
      "epoch": 5.28414442700157,
      "grad_norm": 3.0479631423950195,
      "learning_rate": 4.669740973312402e-05,
      "loss": 0.6599,
      "step": 336600
    },
    {
      "epoch": 5.285714285714286,
      "grad_norm": 4.8882622718811035,
      "learning_rate": 4.669642857142857e-05,
      "loss": 0.6918,
      "step": 336700
    },
    {
      "epoch": 5.287284144427002,
      "grad_norm": 3.8720223903656006,
      "learning_rate": 4.669544740973313e-05,
      "loss": 0.6451,
      "step": 336800
    },
    {
      "epoch": 5.2888540031397175,
      "grad_norm": 3.9640228748321533,
      "learning_rate": 4.669446624803768e-05,
      "loss": 0.6447,
      "step": 336900
    },
    {
      "epoch": 5.2904238618524335,
      "grad_norm": 3.6137924194335938,
      "learning_rate": 4.669348508634223e-05,
      "loss": 0.7117,
      "step": 337000
    },
    {
      "epoch": 5.2919937205651495,
      "grad_norm": 3.24678635597229,
      "learning_rate": 4.669250392464679e-05,
      "loss": 0.674,
      "step": 337100
    },
    {
      "epoch": 5.293563579277865,
      "grad_norm": 3.1232264041900635,
      "learning_rate": 4.669152276295133e-05,
      "loss": 0.6914,
      "step": 337200
    },
    {
      "epoch": 5.295133437990581,
      "grad_norm": 3.826810836791992,
      "learning_rate": 4.669054160125589e-05,
      "loss": 0.6631,
      "step": 337300
    },
    {
      "epoch": 5.2967032967032965,
      "grad_norm": 4.49258279800415,
      "learning_rate": 4.668956043956044e-05,
      "loss": 0.6405,
      "step": 337400
    },
    {
      "epoch": 5.2982731554160125,
      "grad_norm": 3.439079999923706,
      "learning_rate": 4.6688579277865e-05,
      "loss": 0.657,
      "step": 337500
    },
    {
      "epoch": 5.2998430141287285,
      "grad_norm": 3.523379325866699,
      "learning_rate": 4.668759811616954e-05,
      "loss": 0.6449,
      "step": 337600
    },
    {
      "epoch": 5.3014128728414445,
      "grad_norm": 3.161262273788452,
      "learning_rate": 4.66866169544741e-05,
      "loss": 0.662,
      "step": 337700
    },
    {
      "epoch": 5.3029827315541604,
      "grad_norm": 3.899930953979492,
      "learning_rate": 4.668563579277865e-05,
      "loss": 0.6313,
      "step": 337800
    },
    {
      "epoch": 5.304552590266876,
      "grad_norm": 1.6669495105743408,
      "learning_rate": 4.66846546310832e-05,
      "loss": 0.6754,
      "step": 337900
    },
    {
      "epoch": 5.3061224489795915,
      "grad_norm": 4.334996700286865,
      "learning_rate": 4.6683673469387754e-05,
      "loss": 0.6241,
      "step": 338000
    },
    {
      "epoch": 5.3076923076923075,
      "grad_norm": 3.5603249073028564,
      "learning_rate": 4.668269230769231e-05,
      "loss": 0.7027,
      "step": 338100
    },
    {
      "epoch": 5.3092621664050235,
      "grad_norm": 3.184553623199463,
      "learning_rate": 4.668171114599686e-05,
      "loss": 0.7179,
      "step": 338200
    },
    {
      "epoch": 5.310832025117739,
      "grad_norm": 4.364271640777588,
      "learning_rate": 4.6680729984301414e-05,
      "loss": 0.6833,
      "step": 338300
    },
    {
      "epoch": 5.312401883830455,
      "grad_norm": 4.312171459197998,
      "learning_rate": 4.6679748822605965e-05,
      "loss": 0.7021,
      "step": 338400
    },
    {
      "epoch": 5.313971742543171,
      "grad_norm": 3.784696102142334,
      "learning_rate": 4.667876766091052e-05,
      "loss": 0.6505,
      "step": 338500
    },
    {
      "epoch": 5.315541601255887,
      "grad_norm": 5.42485237121582,
      "learning_rate": 4.6677786499215073e-05,
      "loss": 0.6462,
      "step": 338600
    },
    {
      "epoch": 5.3171114599686025,
      "grad_norm": 4.192597389221191,
      "learning_rate": 4.6676805337519624e-05,
      "loss": 0.6466,
      "step": 338700
    },
    {
      "epoch": 5.318681318681318,
      "grad_norm": 3.9957406520843506,
      "learning_rate": 4.6675824175824175e-05,
      "loss": 0.6415,
      "step": 338800
    },
    {
      "epoch": 5.320251177394034,
      "grad_norm": 3.6679582595825195,
      "learning_rate": 4.667484301412873e-05,
      "loss": 0.658,
      "step": 338900
    },
    {
      "epoch": 5.32182103610675,
      "grad_norm": 2.6486122608184814,
      "learning_rate": 4.6673861852433284e-05,
      "loss": 0.6455,
      "step": 339000
    },
    {
      "epoch": 5.323390894819466,
      "grad_norm": 3.5644760131835938,
      "learning_rate": 4.6672880690737835e-05,
      "loss": 0.6484,
      "step": 339100
    },
    {
      "epoch": 5.324960753532182,
      "grad_norm": 3.9896490573883057,
      "learning_rate": 4.667189952904239e-05,
      "loss": 0.6824,
      "step": 339200
    },
    {
      "epoch": 5.326530612244898,
      "grad_norm": 4.529460906982422,
      "learning_rate": 4.667091836734694e-05,
      "loss": 0.6692,
      "step": 339300
    },
    {
      "epoch": 5.328100470957613,
      "grad_norm": 4.000598907470703,
      "learning_rate": 4.6669937205651495e-05,
      "loss": 0.6464,
      "step": 339400
    },
    {
      "epoch": 5.329670329670329,
      "grad_norm": 4.278095722198486,
      "learning_rate": 4.6668956043956046e-05,
      "loss": 0.6619,
      "step": 339500
    },
    {
      "epoch": 5.331240188383045,
      "grad_norm": 4.1789751052856445,
      "learning_rate": 4.6667974882260604e-05,
      "loss": 0.6538,
      "step": 339600
    },
    {
      "epoch": 5.332810047095761,
      "grad_norm": 3.8569345474243164,
      "learning_rate": 4.666699372056515e-05,
      "loss": 0.6646,
      "step": 339700
    },
    {
      "epoch": 5.334379905808477,
      "grad_norm": 4.769703388214111,
      "learning_rate": 4.6666012558869706e-05,
      "loss": 0.6491,
      "step": 339800
    },
    {
      "epoch": 5.335949764521193,
      "grad_norm": 4.105967044830322,
      "learning_rate": 4.6665031397174257e-05,
      "loss": 0.6872,
      "step": 339900
    },
    {
      "epoch": 5.337519623233909,
      "grad_norm": 4.118862152099609,
      "learning_rate": 4.666405023547881e-05,
      "loss": 0.6562,
      "step": 340000
    },
    {
      "epoch": 5.339089481946624,
      "grad_norm": 3.681178569793701,
      "learning_rate": 4.666306907378336e-05,
      "loss": 0.6646,
      "step": 340100
    },
    {
      "epoch": 5.34065934065934,
      "grad_norm": 4.541553020477295,
      "learning_rate": 4.6662087912087916e-05,
      "loss": 0.6599,
      "step": 340200
    },
    {
      "epoch": 5.342229199372056,
      "grad_norm": 4.161733627319336,
      "learning_rate": 4.666110675039247e-05,
      "loss": 0.7137,
      "step": 340300
    },
    {
      "epoch": 5.343799058084772,
      "grad_norm": 3.9387989044189453,
      "learning_rate": 4.666012558869702e-05,
      "loss": 0.6642,
      "step": 340400
    },
    {
      "epoch": 5.345368916797488,
      "grad_norm": 4.087608814239502,
      "learning_rate": 4.665914442700157e-05,
      "loss": 0.658,
      "step": 340500
    },
    {
      "epoch": 5.346938775510204,
      "grad_norm": 3.049556255340576,
      "learning_rate": 4.665816326530613e-05,
      "loss": 0.6358,
      "step": 340600
    },
    {
      "epoch": 5.34850863422292,
      "grad_norm": 3.309182643890381,
      "learning_rate": 4.665718210361068e-05,
      "loss": 0.7087,
      "step": 340700
    },
    {
      "epoch": 5.350078492935636,
      "grad_norm": 5.0361328125,
      "learning_rate": 4.665620094191523e-05,
      "loss": 0.6342,
      "step": 340800
    },
    {
      "epoch": 5.351648351648351,
      "grad_norm": 3.0567634105682373,
      "learning_rate": 4.665521978021978e-05,
      "loss": 0.6723,
      "step": 340900
    },
    {
      "epoch": 5.353218210361067,
      "grad_norm": 4.36506986618042,
      "learning_rate": 4.665423861852434e-05,
      "loss": 0.6439,
      "step": 341000
    },
    {
      "epoch": 5.354788069073783,
      "grad_norm": 2.938774347305298,
      "learning_rate": 4.665325745682889e-05,
      "loss": 0.6156,
      "step": 341100
    },
    {
      "epoch": 5.356357927786499,
      "grad_norm": 4.181194305419922,
      "learning_rate": 4.665227629513344e-05,
      "loss": 0.6395,
      "step": 341200
    },
    {
      "epoch": 5.357927786499215,
      "grad_norm": 3.8971364498138428,
      "learning_rate": 4.6651295133438e-05,
      "loss": 0.6721,
      "step": 341300
    },
    {
      "epoch": 5.359497645211931,
      "grad_norm": 2.9118764400482178,
      "learning_rate": 4.665031397174254e-05,
      "loss": 0.6514,
      "step": 341400
    },
    {
      "epoch": 5.361067503924647,
      "grad_norm": 3.616655111312866,
      "learning_rate": 4.66493328100471e-05,
      "loss": 0.6781,
      "step": 341500
    },
    {
      "epoch": 5.362637362637362,
      "grad_norm": 3.7806804180145264,
      "learning_rate": 4.664835164835165e-05,
      "loss": 0.6335,
      "step": 341600
    },
    {
      "epoch": 5.364207221350078,
      "grad_norm": 3.90643310546875,
      "learning_rate": 4.664737048665621e-05,
      "loss": 0.6515,
      "step": 341700
    },
    {
      "epoch": 5.365777080062794,
      "grad_norm": 3.8871631622314453,
      "learning_rate": 4.664638932496075e-05,
      "loss": 0.6674,
      "step": 341800
    },
    {
      "epoch": 5.36734693877551,
      "grad_norm": 4.113308906555176,
      "learning_rate": 4.664540816326531e-05,
      "loss": 0.6112,
      "step": 341900
    },
    {
      "epoch": 5.368916797488226,
      "grad_norm": 4.266454219818115,
      "learning_rate": 4.664442700156986e-05,
      "loss": 0.6732,
      "step": 342000
    },
    {
      "epoch": 5.370486656200942,
      "grad_norm": 3.6816492080688477,
      "learning_rate": 4.664344583987441e-05,
      "loss": 0.6301,
      "step": 342100
    },
    {
      "epoch": 5.372056514913658,
      "grad_norm": 3.785098075866699,
      "learning_rate": 4.664246467817896e-05,
      "loss": 0.6818,
      "step": 342200
    },
    {
      "epoch": 5.373626373626374,
      "grad_norm": 3.89668869972229,
      "learning_rate": 4.664148351648352e-05,
      "loss": 0.6387,
      "step": 342300
    },
    {
      "epoch": 5.375196232339089,
      "grad_norm": 2.649013042449951,
      "learning_rate": 4.664050235478807e-05,
      "loss": 0.694,
      "step": 342400
    },
    {
      "epoch": 5.376766091051805,
      "grad_norm": 3.9241812229156494,
      "learning_rate": 4.663952119309262e-05,
      "loss": 0.633,
      "step": 342500
    },
    {
      "epoch": 5.378335949764521,
      "grad_norm": 3.118502140045166,
      "learning_rate": 4.6638540031397174e-05,
      "loss": 0.6461,
      "step": 342600
    },
    {
      "epoch": 5.379905808477237,
      "grad_norm": 4.726030349731445,
      "learning_rate": 4.663755886970173e-05,
      "loss": 0.7111,
      "step": 342700
    },
    {
      "epoch": 5.381475667189953,
      "grad_norm": 4.06329870223999,
      "learning_rate": 4.663657770800628e-05,
      "loss": 0.6635,
      "step": 342800
    },
    {
      "epoch": 5.383045525902669,
      "grad_norm": 4.103984355926514,
      "learning_rate": 4.6635596546310833e-05,
      "loss": 0.6486,
      "step": 342900
    },
    {
      "epoch": 5.384615384615385,
      "grad_norm": 4.432331085205078,
      "learning_rate": 4.6634615384615384e-05,
      "loss": 0.6605,
      "step": 343000
    },
    {
      "epoch": 5.3861852433281,
      "grad_norm": 3.8492345809936523,
      "learning_rate": 4.663363422291994e-05,
      "loss": 0.6635,
      "step": 343100
    },
    {
      "epoch": 5.387755102040816,
      "grad_norm": 2.8219897747039795,
      "learning_rate": 4.663265306122449e-05,
      "loss": 0.614,
      "step": 343200
    },
    {
      "epoch": 5.389324960753532,
      "grad_norm": 2.872979164123535,
      "learning_rate": 4.6631671899529044e-05,
      "loss": 0.6491,
      "step": 343300
    },
    {
      "epoch": 5.390894819466248,
      "grad_norm": 4.118152618408203,
      "learning_rate": 4.66306907378336e-05,
      "loss": 0.6653,
      "step": 343400
    },
    {
      "epoch": 5.392464678178964,
      "grad_norm": 4.573215484619141,
      "learning_rate": 4.6629709576138146e-05,
      "loss": 0.6418,
      "step": 343500
    },
    {
      "epoch": 5.39403453689168,
      "grad_norm": 4.275957107543945,
      "learning_rate": 4.6628728414442704e-05,
      "loss": 0.685,
      "step": 343600
    },
    {
      "epoch": 5.395604395604396,
      "grad_norm": 4.364469051361084,
      "learning_rate": 4.6627747252747255e-05,
      "loss": 0.6798,
      "step": 343700
    },
    {
      "epoch": 5.397174254317111,
      "grad_norm": 4.073806285858154,
      "learning_rate": 4.662676609105181e-05,
      "loss": 0.7214,
      "step": 343800
    },
    {
      "epoch": 5.398744113029827,
      "grad_norm": 2.9556772708892822,
      "learning_rate": 4.662578492935636e-05,
      "loss": 0.699,
      "step": 343900
    },
    {
      "epoch": 5.400313971742543,
      "grad_norm": 3.8185386657714844,
      "learning_rate": 4.6624803767660915e-05,
      "loss": 0.6695,
      "step": 344000
    },
    {
      "epoch": 5.401883830455259,
      "grad_norm": 4.355503559112549,
      "learning_rate": 4.6623822605965466e-05,
      "loss": 0.6652,
      "step": 344100
    },
    {
      "epoch": 5.403453689167975,
      "grad_norm": 4.9467549324035645,
      "learning_rate": 4.6622841444270017e-05,
      "loss": 0.6416,
      "step": 344200
    },
    {
      "epoch": 5.405023547880691,
      "grad_norm": 4.235684394836426,
      "learning_rate": 4.662186028257457e-05,
      "loss": 0.637,
      "step": 344300
    },
    {
      "epoch": 5.406593406593407,
      "grad_norm": 3.7317042350769043,
      "learning_rate": 4.6620879120879125e-05,
      "loss": 0.6361,
      "step": 344400
    },
    {
      "epoch": 5.408163265306122,
      "grad_norm": 4.352004528045654,
      "learning_rate": 4.6619897959183676e-05,
      "loss": 0.6653,
      "step": 344500
    },
    {
      "epoch": 5.409733124018838,
      "grad_norm": 4.444324493408203,
      "learning_rate": 4.661891679748823e-05,
      "loss": 0.6946,
      "step": 344600
    },
    {
      "epoch": 5.411302982731554,
      "grad_norm": 4.397879123687744,
      "learning_rate": 4.661793563579278e-05,
      "loss": 0.7053,
      "step": 344700
    },
    {
      "epoch": 5.41287284144427,
      "grad_norm": 4.123907566070557,
      "learning_rate": 4.6616954474097336e-05,
      "loss": 0.6749,
      "step": 344800
    },
    {
      "epoch": 5.414442700156986,
      "grad_norm": 3.9857177734375,
      "learning_rate": 4.661597331240189e-05,
      "loss": 0.6774,
      "step": 344900
    },
    {
      "epoch": 5.416012558869702,
      "grad_norm": 4.213938236236572,
      "learning_rate": 4.661499215070644e-05,
      "loss": 0.6419,
      "step": 345000
    },
    {
      "epoch": 5.417582417582418,
      "grad_norm": 3.48478364944458,
      "learning_rate": 4.661401098901099e-05,
      "loss": 0.6756,
      "step": 345100
    },
    {
      "epoch": 5.419152276295134,
      "grad_norm": 3.008223056793213,
      "learning_rate": 4.661302982731555e-05,
      "loss": 0.6625,
      "step": 345200
    },
    {
      "epoch": 5.420722135007849,
      "grad_norm": 5.0880889892578125,
      "learning_rate": 4.66120486656201e-05,
      "loss": 0.6401,
      "step": 345300
    },
    {
      "epoch": 5.422291993720565,
      "grad_norm": 3.8351593017578125,
      "learning_rate": 4.661106750392465e-05,
      "loss": 0.6882,
      "step": 345400
    },
    {
      "epoch": 5.423861852433281,
      "grad_norm": 4.745454788208008,
      "learning_rate": 4.6610086342229206e-05,
      "loss": 0.7123,
      "step": 345500
    },
    {
      "epoch": 5.425431711145997,
      "grad_norm": 4.202686309814453,
      "learning_rate": 4.660910518053375e-05,
      "loss": 0.6429,
      "step": 345600
    },
    {
      "epoch": 5.427001569858713,
      "grad_norm": 4.491085529327393,
      "learning_rate": 4.660812401883831e-05,
      "loss": 0.6737,
      "step": 345700
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 3.593479871749878,
      "learning_rate": 4.660714285714286e-05,
      "loss": 0.6563,
      "step": 345800
    },
    {
      "epoch": 5.430141287284145,
      "grad_norm": 4.399837970733643,
      "learning_rate": 4.660616169544742e-05,
      "loss": 0.6654,
      "step": 345900
    },
    {
      "epoch": 5.43171114599686,
      "grad_norm": 4.495527267456055,
      "learning_rate": 4.660518053375196e-05,
      "loss": 0.6576,
      "step": 346000
    },
    {
      "epoch": 5.433281004709576,
      "grad_norm": 4.611325740814209,
      "learning_rate": 4.660419937205652e-05,
      "loss": 0.686,
      "step": 346100
    },
    {
      "epoch": 5.434850863422292,
      "grad_norm": 3.407758951187134,
      "learning_rate": 4.660321821036107e-05,
      "loss": 0.6577,
      "step": 346200
    },
    {
      "epoch": 5.436420722135008,
      "grad_norm": 3.7011423110961914,
      "learning_rate": 4.660223704866562e-05,
      "loss": 0.636,
      "step": 346300
    },
    {
      "epoch": 5.437990580847724,
      "grad_norm": 4.176114559173584,
      "learning_rate": 4.660125588697017e-05,
      "loss": 0.7148,
      "step": 346400
    },
    {
      "epoch": 5.43956043956044,
      "grad_norm": 3.729356050491333,
      "learning_rate": 4.660027472527473e-05,
      "loss": 0.6502,
      "step": 346500
    },
    {
      "epoch": 5.441130298273156,
      "grad_norm": 3.40828013420105,
      "learning_rate": 4.659929356357928e-05,
      "loss": 0.6555,
      "step": 346600
    },
    {
      "epoch": 5.442700156985872,
      "grad_norm": 3.436732769012451,
      "learning_rate": 4.659831240188383e-05,
      "loss": 0.6515,
      "step": 346700
    },
    {
      "epoch": 5.444270015698587,
      "grad_norm": 4.819643974304199,
      "learning_rate": 4.659733124018838e-05,
      "loss": 0.6485,
      "step": 346800
    },
    {
      "epoch": 5.445839874411303,
      "grad_norm": 5.132820129394531,
      "learning_rate": 4.659635007849294e-05,
      "loss": 0.6364,
      "step": 346900
    },
    {
      "epoch": 5.447409733124019,
      "grad_norm": 3.6934616565704346,
      "learning_rate": 4.659536891679749e-05,
      "loss": 0.6214,
      "step": 347000
    },
    {
      "epoch": 5.448979591836735,
      "grad_norm": 4.209964275360107,
      "learning_rate": 4.659438775510204e-05,
      "loss": 0.6368,
      "step": 347100
    },
    {
      "epoch": 5.450549450549451,
      "grad_norm": 4.9085211753845215,
      "learning_rate": 4.6593406593406593e-05,
      "loss": 0.6639,
      "step": 347200
    },
    {
      "epoch": 5.452119309262167,
      "grad_norm": 3.713698387145996,
      "learning_rate": 4.659242543171115e-05,
      "loss": 0.6798,
      "step": 347300
    },
    {
      "epoch": 5.453689167974883,
      "grad_norm": 2.9701855182647705,
      "learning_rate": 4.65914442700157e-05,
      "loss": 0.6462,
      "step": 347400
    },
    {
      "epoch": 5.455259026687598,
      "grad_norm": 3.7234225273132324,
      "learning_rate": 4.659046310832025e-05,
      "loss": 0.6543,
      "step": 347500
    },
    {
      "epoch": 5.456828885400314,
      "grad_norm": 3.1575939655303955,
      "learning_rate": 4.658948194662481e-05,
      "loss": 0.6754,
      "step": 347600
    },
    {
      "epoch": 5.45839874411303,
      "grad_norm": 4.408132553100586,
      "learning_rate": 4.6588500784929355e-05,
      "loss": 0.6591,
      "step": 347700
    },
    {
      "epoch": 5.459968602825746,
      "grad_norm": 5.062893390655518,
      "learning_rate": 4.658751962323391e-05,
      "loss": 0.6408,
      "step": 347800
    },
    {
      "epoch": 5.461538461538462,
      "grad_norm": 3.530402660369873,
      "learning_rate": 4.6586538461538464e-05,
      "loss": 0.6714,
      "step": 347900
    },
    {
      "epoch": 5.463108320251178,
      "grad_norm": 4.2326226234436035,
      "learning_rate": 4.658555729984302e-05,
      "loss": 0.6557,
      "step": 348000
    },
    {
      "epoch": 5.464678178963894,
      "grad_norm": 4.017530918121338,
      "learning_rate": 4.6584576138147566e-05,
      "loss": 0.6443,
      "step": 348100
    },
    {
      "epoch": 5.466248037676609,
      "grad_norm": 4.102180480957031,
      "learning_rate": 4.6583594976452124e-05,
      "loss": 0.6724,
      "step": 348200
    },
    {
      "epoch": 5.467817896389325,
      "grad_norm": 4.050705432891846,
      "learning_rate": 4.6582613814756675e-05,
      "loss": 0.683,
      "step": 348300
    },
    {
      "epoch": 5.469387755102041,
      "grad_norm": 4.328230381011963,
      "learning_rate": 4.6581632653061226e-05,
      "loss": 0.6448,
      "step": 348400
    },
    {
      "epoch": 5.470957613814757,
      "grad_norm": 4.305591583251953,
      "learning_rate": 4.6580651491365777e-05,
      "loss": 0.6954,
      "step": 348500
    },
    {
      "epoch": 5.472527472527473,
      "grad_norm": 4.101687431335449,
      "learning_rate": 4.6579670329670334e-05,
      "loss": 0.692,
      "step": 348600
    },
    {
      "epoch": 5.474097331240189,
      "grad_norm": 3.63566255569458,
      "learning_rate": 4.6578689167974885e-05,
      "loss": 0.6352,
      "step": 348700
    },
    {
      "epoch": 5.475667189952905,
      "grad_norm": 4.719802379608154,
      "learning_rate": 4.6577708006279436e-05,
      "loss": 0.6796,
      "step": 348800
    },
    {
      "epoch": 5.47723704866562,
      "grad_norm": 3.8147151470184326,
      "learning_rate": 4.657672684458399e-05,
      "loss": 0.6478,
      "step": 348900
    },
    {
      "epoch": 5.478806907378336,
      "grad_norm": 3.631514310836792,
      "learning_rate": 4.6575745682888545e-05,
      "loss": 0.6552,
      "step": 349000
    },
    {
      "epoch": 5.480376766091052,
      "grad_norm": 3.0429327487945557,
      "learning_rate": 4.6574764521193096e-05,
      "loss": 0.6557,
      "step": 349100
    },
    {
      "epoch": 5.481946624803768,
      "grad_norm": 4.57565975189209,
      "learning_rate": 4.657378335949765e-05,
      "loss": 0.6889,
      "step": 349200
    },
    {
      "epoch": 5.483516483516484,
      "grad_norm": 3.80686616897583,
      "learning_rate": 4.65728021978022e-05,
      "loss": 0.6323,
      "step": 349300
    },
    {
      "epoch": 5.4850863422291996,
      "grad_norm": 4.3930864334106445,
      "learning_rate": 4.6571821036106756e-05,
      "loss": 0.6665,
      "step": 349400
    },
    {
      "epoch": 5.4866562009419155,
      "grad_norm": 3.9693188667297363,
      "learning_rate": 4.657083987441131e-05,
      "loss": 0.6373,
      "step": 349500
    },
    {
      "epoch": 5.488226059654631,
      "grad_norm": 3.9709508419036865,
      "learning_rate": 4.656985871271586e-05,
      "loss": 0.6587,
      "step": 349600
    },
    {
      "epoch": 5.489795918367347,
      "grad_norm": 4.301069736480713,
      "learning_rate": 4.6568877551020415e-05,
      "loss": 0.6603,
      "step": 349700
    },
    {
      "epoch": 5.491365777080063,
      "grad_norm": 3.9920735359191895,
      "learning_rate": 4.656789638932496e-05,
      "loss": 0.664,
      "step": 349800
    },
    {
      "epoch": 5.4929356357927785,
      "grad_norm": 3.6813769340515137,
      "learning_rate": 4.656691522762952e-05,
      "loss": 0.6506,
      "step": 349900
    },
    {
      "epoch": 5.4945054945054945,
      "grad_norm": 2.594759941101074,
      "learning_rate": 4.656593406593407e-05,
      "loss": 0.6593,
      "step": 350000
    },
    {
      "epoch": 5.4960753532182105,
      "grad_norm": 3.528402328491211,
      "learning_rate": 4.6564952904238626e-05,
      "loss": 0.633,
      "step": 350100
    },
    {
      "epoch": 5.4976452119309265,
      "grad_norm": 4.8264641761779785,
      "learning_rate": 4.656397174254317e-05,
      "loss": 0.6436,
      "step": 350200
    },
    {
      "epoch": 5.4992150706436425,
      "grad_norm": 3.840022087097168,
      "learning_rate": 4.656299058084773e-05,
      "loss": 0.6403,
      "step": 350300
    },
    {
      "epoch": 5.5007849293563575,
      "grad_norm": 4.415582656860352,
      "learning_rate": 4.656200941915228e-05,
      "loss": 0.6717,
      "step": 350400
    },
    {
      "epoch": 5.5023547880690735,
      "grad_norm": 3.958989381790161,
      "learning_rate": 4.656102825745683e-05,
      "loss": 0.6879,
      "step": 350500
    },
    {
      "epoch": 5.5039246467817895,
      "grad_norm": 3.505143165588379,
      "learning_rate": 4.656004709576138e-05,
      "loss": 0.6735,
      "step": 350600
    },
    {
      "epoch": 5.5054945054945055,
      "grad_norm": 3.3269505500793457,
      "learning_rate": 4.655906593406594e-05,
      "loss": 0.6723,
      "step": 350700
    },
    {
      "epoch": 5.5070643642072215,
      "grad_norm": 3.6489412784576416,
      "learning_rate": 4.655808477237049e-05,
      "loss": 0.6409,
      "step": 350800
    },
    {
      "epoch": 5.508634222919937,
      "grad_norm": 3.5117950439453125,
      "learning_rate": 4.655710361067504e-05,
      "loss": 0.6906,
      "step": 350900
    },
    {
      "epoch": 5.510204081632653,
      "grad_norm": 4.716924667358398,
      "learning_rate": 4.655612244897959e-05,
      "loss": 0.6821,
      "step": 351000
    },
    {
      "epoch": 5.511773940345369,
      "grad_norm": 4.815706729888916,
      "learning_rate": 4.655514128728415e-05,
      "loss": 0.6532,
      "step": 351100
    },
    {
      "epoch": 5.5133437990580845,
      "grad_norm": 4.7390007972717285,
      "learning_rate": 4.65541601255887e-05,
      "loss": 0.6836,
      "step": 351200
    },
    {
      "epoch": 5.5149136577708004,
      "grad_norm": 2.4299893379211426,
      "learning_rate": 4.655317896389325e-05,
      "loss": 0.6749,
      "step": 351300
    },
    {
      "epoch": 5.516483516483516,
      "grad_norm": 3.8045012950897217,
      "learning_rate": 4.65521978021978e-05,
      "loss": 0.6434,
      "step": 351400
    },
    {
      "epoch": 5.518053375196232,
      "grad_norm": 4.25892972946167,
      "learning_rate": 4.655121664050236e-05,
      "loss": 0.6891,
      "step": 351500
    },
    {
      "epoch": 5.519623233908948,
      "grad_norm": 3.5039541721343994,
      "learning_rate": 4.655023547880691e-05,
      "loss": 0.6737,
      "step": 351600
    },
    {
      "epoch": 5.521193092621664,
      "grad_norm": 4.249258995056152,
      "learning_rate": 4.654925431711146e-05,
      "loss": 0.6826,
      "step": 351700
    },
    {
      "epoch": 5.52276295133438,
      "grad_norm": 4.384886741638184,
      "learning_rate": 4.654827315541602e-05,
      "loss": 0.6268,
      "step": 351800
    },
    {
      "epoch": 5.524332810047095,
      "grad_norm": 4.9598541259765625,
      "learning_rate": 4.6547291993720564e-05,
      "loss": 0.7066,
      "step": 351900
    },
    {
      "epoch": 5.525902668759811,
      "grad_norm": 3.9141836166381836,
      "learning_rate": 4.654631083202512e-05,
      "loss": 0.6558,
      "step": 352000
    },
    {
      "epoch": 5.527472527472527,
      "grad_norm": 4.086628437042236,
      "learning_rate": 4.654532967032967e-05,
      "loss": 0.6732,
      "step": 352100
    },
    {
      "epoch": 5.529042386185243,
      "grad_norm": 4.243972301483154,
      "learning_rate": 4.654434850863423e-05,
      "loss": 0.6732,
      "step": 352200
    },
    {
      "epoch": 5.530612244897959,
      "grad_norm": 3.470395565032959,
      "learning_rate": 4.6543367346938775e-05,
      "loss": 0.6738,
      "step": 352300
    },
    {
      "epoch": 5.532182103610675,
      "grad_norm": 3.25649094581604,
      "learning_rate": 4.654238618524333e-05,
      "loss": 0.6475,
      "step": 352400
    },
    {
      "epoch": 5.533751962323391,
      "grad_norm": 4.454104423522949,
      "learning_rate": 4.6541405023547884e-05,
      "loss": 0.6362,
      "step": 352500
    },
    {
      "epoch": 5.535321821036106,
      "grad_norm": 3.637272596359253,
      "learning_rate": 4.6540423861852435e-05,
      "loss": 0.6577,
      "step": 352600
    },
    {
      "epoch": 5.536891679748822,
      "grad_norm": 3.7758231163024902,
      "learning_rate": 4.6539442700156985e-05,
      "loss": 0.7147,
      "step": 352700
    },
    {
      "epoch": 5.538461538461538,
      "grad_norm": 4.257971286773682,
      "learning_rate": 4.653846153846154e-05,
      "loss": 0.7259,
      "step": 352800
    },
    {
      "epoch": 5.540031397174254,
      "grad_norm": 4.01359748840332,
      "learning_rate": 4.653748037676609e-05,
      "loss": 0.6831,
      "step": 352900
    },
    {
      "epoch": 5.54160125588697,
      "grad_norm": 3.290703773498535,
      "learning_rate": 4.6536499215070645e-05,
      "loss": 0.6335,
      "step": 353000
    },
    {
      "epoch": 5.543171114599686,
      "grad_norm": 4.915111541748047,
      "learning_rate": 4.6535518053375196e-05,
      "loss": 0.649,
      "step": 353100
    },
    {
      "epoch": 5.544740973312402,
      "grad_norm": 4.558431625366211,
      "learning_rate": 4.6534536891679754e-05,
      "loss": 0.6825,
      "step": 353200
    },
    {
      "epoch": 5.546310832025117,
      "grad_norm": 4.467582702636719,
      "learning_rate": 4.6533555729984305e-05,
      "loss": 0.6621,
      "step": 353300
    },
    {
      "epoch": 5.547880690737833,
      "grad_norm": 3.6082687377929688,
      "learning_rate": 4.6532574568288856e-05,
      "loss": 0.6822,
      "step": 353400
    },
    {
      "epoch": 5.549450549450549,
      "grad_norm": 4.965259552001953,
      "learning_rate": 4.653159340659341e-05,
      "loss": 0.6816,
      "step": 353500
    },
    {
      "epoch": 5.551020408163265,
      "grad_norm": 3.4185242652893066,
      "learning_rate": 4.653061224489796e-05,
      "loss": 0.6745,
      "step": 353600
    },
    {
      "epoch": 5.552590266875981,
      "grad_norm": 4.023850440979004,
      "learning_rate": 4.6529631083202516e-05,
      "loss": 0.6714,
      "step": 353700
    },
    {
      "epoch": 5.554160125588697,
      "grad_norm": 3.7046163082122803,
      "learning_rate": 4.652864992150707e-05,
      "loss": 0.6693,
      "step": 353800
    },
    {
      "epoch": 5.555729984301413,
      "grad_norm": 4.510915756225586,
      "learning_rate": 4.6527668759811624e-05,
      "loss": 0.6578,
      "step": 353900
    },
    {
      "epoch": 5.557299843014128,
      "grad_norm": 3.3449227809906006,
      "learning_rate": 4.652668759811617e-05,
      "loss": 0.6472,
      "step": 354000
    },
    {
      "epoch": 5.558869701726844,
      "grad_norm": 3.753309726715088,
      "learning_rate": 4.6525706436420726e-05,
      "loss": 0.6445,
      "step": 354100
    },
    {
      "epoch": 5.56043956043956,
      "grad_norm": 3.1468069553375244,
      "learning_rate": 4.652472527472528e-05,
      "loss": 0.6389,
      "step": 354200
    },
    {
      "epoch": 5.562009419152276,
      "grad_norm": 3.6814181804656982,
      "learning_rate": 4.652374411302983e-05,
      "loss": 0.6652,
      "step": 354300
    },
    {
      "epoch": 5.563579277864992,
      "grad_norm": 4.358278274536133,
      "learning_rate": 4.652276295133438e-05,
      "loss": 0.6935,
      "step": 354400
    },
    {
      "epoch": 5.565149136577708,
      "grad_norm": 4.460705280303955,
      "learning_rate": 4.652178178963894e-05,
      "loss": 0.6369,
      "step": 354500
    },
    {
      "epoch": 5.566718995290424,
      "grad_norm": 4.855538845062256,
      "learning_rate": 4.652080062794349e-05,
      "loss": 0.6664,
      "step": 354600
    },
    {
      "epoch": 5.568288854003139,
      "grad_norm": 3.7632434368133545,
      "learning_rate": 4.651981946624804e-05,
      "loss": 0.6818,
      "step": 354700
    },
    {
      "epoch": 5.569858712715855,
      "grad_norm": 3.38041615486145,
      "learning_rate": 4.651883830455259e-05,
      "loss": 0.6459,
      "step": 354800
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 4.178966999053955,
      "learning_rate": 4.651785714285715e-05,
      "loss": 0.6926,
      "step": 354900
    },
    {
      "epoch": 5.572998430141287,
      "grad_norm": 3.77398681640625,
      "learning_rate": 4.651687598116169e-05,
      "loss": 0.6367,
      "step": 355000
    },
    {
      "epoch": 5.574568288854003,
      "grad_norm": 4.689624309539795,
      "learning_rate": 4.651589481946625e-05,
      "loss": 0.6606,
      "step": 355100
    },
    {
      "epoch": 5.576138147566719,
      "grad_norm": 3.9890942573547363,
      "learning_rate": 4.65149136577708e-05,
      "loss": 0.709,
      "step": 355200
    },
    {
      "epoch": 5.577708006279435,
      "grad_norm": 4.480832576751709,
      "learning_rate": 4.651393249607536e-05,
      "loss": 0.6762,
      "step": 355300
    },
    {
      "epoch": 5.579277864992151,
      "grad_norm": 3.953087091445923,
      "learning_rate": 4.651295133437991e-05,
      "loss": 0.6685,
      "step": 355400
    },
    {
      "epoch": 5.580847723704867,
      "grad_norm": 4.330842971801758,
      "learning_rate": 4.651197017268446e-05,
      "loss": 0.6889,
      "step": 355500
    },
    {
      "epoch": 5.582417582417582,
      "grad_norm": 4.258101463317871,
      "learning_rate": 4.651098901098901e-05,
      "loss": 0.673,
      "step": 355600
    },
    {
      "epoch": 5.583987441130298,
      "grad_norm": 3.6900155544281006,
      "learning_rate": 4.651000784929356e-05,
      "loss": 0.6786,
      "step": 355700
    },
    {
      "epoch": 5.585557299843014,
      "grad_norm": 3.882009267807007,
      "learning_rate": 4.650902668759812e-05,
      "loss": 0.7329,
      "step": 355800
    },
    {
      "epoch": 5.58712715855573,
      "grad_norm": 3.805756092071533,
      "learning_rate": 4.650804552590267e-05,
      "loss": 0.6879,
      "step": 355900
    },
    {
      "epoch": 5.588697017268446,
      "grad_norm": 4.841065406799316,
      "learning_rate": 4.650706436420723e-05,
      "loss": 0.7126,
      "step": 356000
    },
    {
      "epoch": 5.590266875981162,
      "grad_norm": 4.720370292663574,
      "learning_rate": 4.650608320251177e-05,
      "loss": 0.6852,
      "step": 356100
    },
    {
      "epoch": 5.591836734693878,
      "grad_norm": 3.7315380573272705,
      "learning_rate": 4.650510204081633e-05,
      "loss": 0.6486,
      "step": 356200
    },
    {
      "epoch": 5.593406593406593,
      "grad_norm": 4.426881313323975,
      "learning_rate": 4.650412087912088e-05,
      "loss": 0.6685,
      "step": 356300
    },
    {
      "epoch": 5.594976452119309,
      "grad_norm": 3.738783121109009,
      "learning_rate": 4.650313971742543e-05,
      "loss": 0.7038,
      "step": 356400
    },
    {
      "epoch": 5.596546310832025,
      "grad_norm": 3.4657976627349854,
      "learning_rate": 4.6502158555729984e-05,
      "loss": 0.6877,
      "step": 356500
    },
    {
      "epoch": 5.598116169544741,
      "grad_norm": 3.3131442070007324,
      "learning_rate": 4.650117739403454e-05,
      "loss": 0.6666,
      "step": 356600
    },
    {
      "epoch": 5.599686028257457,
      "grad_norm": 3.88067889213562,
      "learning_rate": 4.650019623233909e-05,
      "loss": 0.6508,
      "step": 356700
    },
    {
      "epoch": 5.601255886970173,
      "grad_norm": 3.5245134830474854,
      "learning_rate": 4.6499215070643643e-05,
      "loss": 0.6638,
      "step": 356800
    },
    {
      "epoch": 5.602825745682889,
      "grad_norm": 4.101591110229492,
      "learning_rate": 4.6498233908948194e-05,
      "loss": 0.7118,
      "step": 356900
    },
    {
      "epoch": 5.604395604395604,
      "grad_norm": 4.50521993637085,
      "learning_rate": 4.649725274725275e-05,
      "loss": 0.6935,
      "step": 357000
    },
    {
      "epoch": 5.60596546310832,
      "grad_norm": 3.3908913135528564,
      "learning_rate": 4.6496271585557296e-05,
      "loss": 0.6449,
      "step": 357100
    },
    {
      "epoch": 5.607535321821036,
      "grad_norm": 4.374117374420166,
      "learning_rate": 4.6495290423861854e-05,
      "loss": 0.6379,
      "step": 357200
    },
    {
      "epoch": 5.609105180533752,
      "grad_norm": 3.6457207202911377,
      "learning_rate": 4.6494309262166405e-05,
      "loss": 0.6799,
      "step": 357300
    },
    {
      "epoch": 5.610675039246468,
      "grad_norm": 3.2695670127868652,
      "learning_rate": 4.649332810047096e-05,
      "loss": 0.6626,
      "step": 357400
    },
    {
      "epoch": 5.612244897959184,
      "grad_norm": 4.085814952850342,
      "learning_rate": 4.6492346938775514e-05,
      "loss": 0.654,
      "step": 357500
    },
    {
      "epoch": 5.6138147566719,
      "grad_norm": 4.597539901733398,
      "learning_rate": 4.6491365777080065e-05,
      "loss": 0.6404,
      "step": 357600
    },
    {
      "epoch": 5.615384615384615,
      "grad_norm": 4.430565357208252,
      "learning_rate": 4.6490384615384616e-05,
      "loss": 0.6876,
      "step": 357700
    },
    {
      "epoch": 5.616954474097331,
      "grad_norm": 4.307713031768799,
      "learning_rate": 4.648940345368917e-05,
      "loss": 0.697,
      "step": 357800
    },
    {
      "epoch": 5.618524332810047,
      "grad_norm": 4.242955207824707,
      "learning_rate": 4.6488422291993725e-05,
      "loss": 0.6679,
      "step": 357900
    },
    {
      "epoch": 5.620094191522763,
      "grad_norm": 4.832005023956299,
      "learning_rate": 4.6487441130298276e-05,
      "loss": 0.6299,
      "step": 358000
    },
    {
      "epoch": 5.621664050235479,
      "grad_norm": 5.058720588684082,
      "learning_rate": 4.648645996860283e-05,
      "loss": 0.6395,
      "step": 358100
    },
    {
      "epoch": 5.623233908948195,
      "grad_norm": 4.051327228546143,
      "learning_rate": 4.648547880690738e-05,
      "loss": 0.6484,
      "step": 358200
    },
    {
      "epoch": 5.624803767660911,
      "grad_norm": 3.9856903553009033,
      "learning_rate": 4.6484497645211935e-05,
      "loss": 0.656,
      "step": 358300
    },
    {
      "epoch": 5.626373626373626,
      "grad_norm": 3.772324800491333,
      "learning_rate": 4.6483516483516486e-05,
      "loss": 0.6213,
      "step": 358400
    },
    {
      "epoch": 5.627943485086342,
      "grad_norm": 3.4850354194641113,
      "learning_rate": 4.648253532182104e-05,
      "loss": 0.6669,
      "step": 358500
    },
    {
      "epoch": 5.629513343799058,
      "grad_norm": 3.6699671745300293,
      "learning_rate": 4.648155416012559e-05,
      "loss": 0.6675,
      "step": 358600
    },
    {
      "epoch": 5.631083202511774,
      "grad_norm": 4.7226057052612305,
      "learning_rate": 4.6480572998430146e-05,
      "loss": 0.7019,
      "step": 358700
    },
    {
      "epoch": 5.63265306122449,
      "grad_norm": 4.166373252868652,
      "learning_rate": 4.64795918367347e-05,
      "loss": 0.6539,
      "step": 358800
    },
    {
      "epoch": 5.634222919937206,
      "grad_norm": 4.336981773376465,
      "learning_rate": 4.647861067503925e-05,
      "loss": 0.6301,
      "step": 358900
    },
    {
      "epoch": 5.635792778649922,
      "grad_norm": 4.007297515869141,
      "learning_rate": 4.64776295133438e-05,
      "loss": 0.6739,
      "step": 359000
    },
    {
      "epoch": 5.637362637362637,
      "grad_norm": 4.458922386169434,
      "learning_rate": 4.647664835164836e-05,
      "loss": 0.6708,
      "step": 359100
    },
    {
      "epoch": 5.638932496075353,
      "grad_norm": 3.121837615966797,
      "learning_rate": 4.64756671899529e-05,
      "loss": 0.677,
      "step": 359200
    },
    {
      "epoch": 5.640502354788069,
      "grad_norm": 4.263079643249512,
      "learning_rate": 4.647468602825746e-05,
      "loss": 0.6443,
      "step": 359300
    },
    {
      "epoch": 5.642072213500785,
      "grad_norm": 3.2478461265563965,
      "learning_rate": 4.647370486656201e-05,
      "loss": 0.6682,
      "step": 359400
    },
    {
      "epoch": 5.643642072213501,
      "grad_norm": 6.708268165588379,
      "learning_rate": 4.647272370486657e-05,
      "loss": 0.6653,
      "step": 359500
    },
    {
      "epoch": 5.645211930926217,
      "grad_norm": 3.215080738067627,
      "learning_rate": 4.647174254317112e-05,
      "loss": 0.6565,
      "step": 359600
    },
    {
      "epoch": 5.646781789638933,
      "grad_norm": 3.8892276287078857,
      "learning_rate": 4.647076138147567e-05,
      "loss": 0.682,
      "step": 359700
    },
    {
      "epoch": 5.648351648351649,
      "grad_norm": 3.1383657455444336,
      "learning_rate": 4.646978021978022e-05,
      "loss": 0.7036,
      "step": 359800
    },
    {
      "epoch": 5.649921507064365,
      "grad_norm": 4.3141770362854,
      "learning_rate": 4.646879905808477e-05,
      "loss": 0.6393,
      "step": 359900
    },
    {
      "epoch": 5.65149136577708,
      "grad_norm": 4.815280437469482,
      "learning_rate": 4.646781789638933e-05,
      "loss": 0.6465,
      "step": 360000
    },
    {
      "epoch": 5.653061224489796,
      "grad_norm": 3.7528398036956787,
      "learning_rate": 4.646683673469388e-05,
      "loss": 0.664,
      "step": 360100
    },
    {
      "epoch": 5.654631083202512,
      "grad_norm": 4.020031452178955,
      "learning_rate": 4.646585557299844e-05,
      "loss": 0.6767,
      "step": 360200
    },
    {
      "epoch": 5.656200941915228,
      "grad_norm": 3.8104970455169678,
      "learning_rate": 4.646487441130298e-05,
      "loss": 0.6797,
      "step": 360300
    },
    {
      "epoch": 5.657770800627944,
      "grad_norm": 3.2275636196136475,
      "learning_rate": 4.646389324960754e-05,
      "loss": 0.6586,
      "step": 360400
    },
    {
      "epoch": 5.65934065934066,
      "grad_norm": 4.347494125366211,
      "learning_rate": 4.646291208791209e-05,
      "loss": 0.7012,
      "step": 360500
    },
    {
      "epoch": 5.660910518053376,
      "grad_norm": 4.469321250915527,
      "learning_rate": 4.646193092621664e-05,
      "loss": 0.6527,
      "step": 360600
    },
    {
      "epoch": 5.662480376766091,
      "grad_norm": 4.442033767700195,
      "learning_rate": 4.646094976452119e-05,
      "loss": 0.662,
      "step": 360700
    },
    {
      "epoch": 5.664050235478807,
      "grad_norm": 3.5943150520324707,
      "learning_rate": 4.645996860282575e-05,
      "loss": 0.6652,
      "step": 360800
    },
    {
      "epoch": 5.665620094191523,
      "grad_norm": 3.9497804641723633,
      "learning_rate": 4.64589874411303e-05,
      "loss": 0.6552,
      "step": 360900
    },
    {
      "epoch": 5.667189952904239,
      "grad_norm": 4.429511070251465,
      "learning_rate": 4.645800627943485e-05,
      "loss": 0.6807,
      "step": 361000
    },
    {
      "epoch": 5.668759811616955,
      "grad_norm": 3.576270341873169,
      "learning_rate": 4.6457025117739403e-05,
      "loss": 0.7226,
      "step": 361100
    },
    {
      "epoch": 5.670329670329671,
      "grad_norm": 4.293335914611816,
      "learning_rate": 4.645604395604396e-05,
      "loss": 0.6234,
      "step": 361200
    },
    {
      "epoch": 5.671899529042387,
      "grad_norm": 4.018089294433594,
      "learning_rate": 4.6455062794348505e-05,
      "loss": 0.7063,
      "step": 361300
    },
    {
      "epoch": 5.673469387755102,
      "grad_norm": 3.824526071548462,
      "learning_rate": 4.645408163265306e-05,
      "loss": 0.675,
      "step": 361400
    },
    {
      "epoch": 5.675039246467818,
      "grad_norm": 4.560312271118164,
      "learning_rate": 4.6453100470957614e-05,
      "loss": 0.6548,
      "step": 361500
    },
    {
      "epoch": 5.676609105180534,
      "grad_norm": 2.9982337951660156,
      "learning_rate": 4.645211930926217e-05,
      "loss": 0.6564,
      "step": 361600
    },
    {
      "epoch": 5.67817896389325,
      "grad_norm": 3.7719497680664062,
      "learning_rate": 4.645113814756672e-05,
      "loss": 0.6714,
      "step": 361700
    },
    {
      "epoch": 5.679748822605966,
      "grad_norm": 4.503504753112793,
      "learning_rate": 4.6450156985871274e-05,
      "loss": 0.6548,
      "step": 361800
    },
    {
      "epoch": 5.681318681318682,
      "grad_norm": 4.156296253204346,
      "learning_rate": 4.6449175824175825e-05,
      "loss": 0.7053,
      "step": 361900
    },
    {
      "epoch": 5.6828885400313975,
      "grad_norm": 4.316360950469971,
      "learning_rate": 4.6448194662480376e-05,
      "loss": 0.6765,
      "step": 362000
    },
    {
      "epoch": 5.684458398744113,
      "grad_norm": 3.217301368713379,
      "learning_rate": 4.6447213500784934e-05,
      "loss": 0.697,
      "step": 362100
    },
    {
      "epoch": 5.686028257456829,
      "grad_norm": 4.141224384307861,
      "learning_rate": 4.6446232339089485e-05,
      "loss": 0.6828,
      "step": 362200
    },
    {
      "epoch": 5.687598116169545,
      "grad_norm": 4.510013580322266,
      "learning_rate": 4.644525117739404e-05,
      "loss": 0.6783,
      "step": 362300
    },
    {
      "epoch": 5.689167974882261,
      "grad_norm": 4.732990741729736,
      "learning_rate": 4.6444270015698587e-05,
      "loss": 0.6408,
      "step": 362400
    },
    {
      "epoch": 5.6907378335949765,
      "grad_norm": 4.0954742431640625,
      "learning_rate": 4.6443288854003144e-05,
      "loss": 0.6577,
      "step": 362500
    },
    {
      "epoch": 5.6923076923076925,
      "grad_norm": 5.0318684577941895,
      "learning_rate": 4.6442307692307695e-05,
      "loss": 0.6832,
      "step": 362600
    },
    {
      "epoch": 5.6938775510204085,
      "grad_norm": 4.017497539520264,
      "learning_rate": 4.6441326530612246e-05,
      "loss": 0.6767,
      "step": 362700
    },
    {
      "epoch": 5.695447409733124,
      "grad_norm": 3.382269859313965,
      "learning_rate": 4.64403453689168e-05,
      "loss": 0.6101,
      "step": 362800
    },
    {
      "epoch": 5.6970172684458396,
      "grad_norm": 3.4398720264434814,
      "learning_rate": 4.6439364207221355e-05,
      "loss": 0.7079,
      "step": 362900
    },
    {
      "epoch": 5.6985871271585555,
      "grad_norm": 3.2434353828430176,
      "learning_rate": 4.6438383045525906e-05,
      "loss": 0.6755,
      "step": 363000
    },
    {
      "epoch": 5.7001569858712715,
      "grad_norm": 4.193785667419434,
      "learning_rate": 4.643740188383046e-05,
      "loss": 0.6769,
      "step": 363100
    },
    {
      "epoch": 5.7017268445839875,
      "grad_norm": 4.217918872833252,
      "learning_rate": 4.643642072213501e-05,
      "loss": 0.661,
      "step": 363200
    },
    {
      "epoch": 5.7032967032967035,
      "grad_norm": 4.657800197601318,
      "learning_rate": 4.6435439560439566e-05,
      "loss": 0.6938,
      "step": 363300
    },
    {
      "epoch": 5.704866562009419,
      "grad_norm": 4.209283351898193,
      "learning_rate": 4.643445839874411e-05,
      "loss": 0.6764,
      "step": 363400
    },
    {
      "epoch": 5.7064364207221345,
      "grad_norm": 3.634992837905884,
      "learning_rate": 4.643347723704867e-05,
      "loss": 0.6119,
      "step": 363500
    },
    {
      "epoch": 5.7080062794348505,
      "grad_norm": 3.3028316497802734,
      "learning_rate": 4.643249607535322e-05,
      "loss": 0.6515,
      "step": 363600
    },
    {
      "epoch": 5.7095761381475665,
      "grad_norm": 4.169280052185059,
      "learning_rate": 4.6431514913657776e-05,
      "loss": 0.6367,
      "step": 363700
    },
    {
      "epoch": 5.7111459968602825,
      "grad_norm": 4.181911468505859,
      "learning_rate": 4.643053375196233e-05,
      "loss": 0.7122,
      "step": 363800
    },
    {
      "epoch": 5.712715855572998,
      "grad_norm": 4.058631420135498,
      "learning_rate": 4.642955259026688e-05,
      "loss": 0.6501,
      "step": 363900
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 4.3515472412109375,
      "learning_rate": 4.642857142857143e-05,
      "loss": 0.6714,
      "step": 364000
    },
    {
      "epoch": 5.71585557299843,
      "grad_norm": 3.176666736602783,
      "learning_rate": 4.642759026687598e-05,
      "loss": 0.6509,
      "step": 364100
    },
    {
      "epoch": 5.717425431711146,
      "grad_norm": 5.43344783782959,
      "learning_rate": 4.642660910518054e-05,
      "loss": 0.6636,
      "step": 364200
    },
    {
      "epoch": 5.718995290423862,
      "grad_norm": 3.279088020324707,
      "learning_rate": 4.642562794348509e-05,
      "loss": 0.6529,
      "step": 364300
    },
    {
      "epoch": 5.720565149136577,
      "grad_norm": 4.393679618835449,
      "learning_rate": 4.642464678178965e-05,
      "loss": 0.672,
      "step": 364400
    },
    {
      "epoch": 5.722135007849293,
      "grad_norm": 3.6130967140197754,
      "learning_rate": 4.642366562009419e-05,
      "loss": 0.6961,
      "step": 364500
    },
    {
      "epoch": 5.723704866562009,
      "grad_norm": 3.9686193466186523,
      "learning_rate": 4.642268445839875e-05,
      "loss": 0.6439,
      "step": 364600
    },
    {
      "epoch": 5.725274725274725,
      "grad_norm": 3.906390905380249,
      "learning_rate": 4.64217032967033e-05,
      "loss": 0.6894,
      "step": 364700
    },
    {
      "epoch": 5.726844583987441,
      "grad_norm": 3.154242992401123,
      "learning_rate": 4.642072213500785e-05,
      "loss": 0.6739,
      "step": 364800
    },
    {
      "epoch": 5.728414442700157,
      "grad_norm": 4.021996021270752,
      "learning_rate": 4.64197409733124e-05,
      "loss": 0.6702,
      "step": 364900
    },
    {
      "epoch": 5.729984301412873,
      "grad_norm": 4.014223098754883,
      "learning_rate": 4.641875981161696e-05,
      "loss": 0.6575,
      "step": 365000
    },
    {
      "epoch": 5.731554160125588,
      "grad_norm": 5.138747692108154,
      "learning_rate": 4.641777864992151e-05,
      "loss": 0.6909,
      "step": 365100
    },
    {
      "epoch": 5.733124018838304,
      "grad_norm": 3.1557281017303467,
      "learning_rate": 4.641679748822606e-05,
      "loss": 0.6569,
      "step": 365200
    },
    {
      "epoch": 5.73469387755102,
      "grad_norm": 4.078875541687012,
      "learning_rate": 4.641581632653061e-05,
      "loss": 0.6164,
      "step": 365300
    },
    {
      "epoch": 5.736263736263736,
      "grad_norm": 4.242930889129639,
      "learning_rate": 4.641483516483517e-05,
      "loss": 0.6247,
      "step": 365400
    },
    {
      "epoch": 5.737833594976452,
      "grad_norm": 3.568352222442627,
      "learning_rate": 4.6413854003139714e-05,
      "loss": 0.6735,
      "step": 365500
    },
    {
      "epoch": 5.739403453689168,
      "grad_norm": 3.853691816329956,
      "learning_rate": 4.641287284144427e-05,
      "loss": 0.6382,
      "step": 365600
    },
    {
      "epoch": 5.740973312401884,
      "grad_norm": 3.9484522342681885,
      "learning_rate": 4.641189167974882e-05,
      "loss": 0.6733,
      "step": 365700
    },
    {
      "epoch": 5.742543171114599,
      "grad_norm": 3.989123821258545,
      "learning_rate": 4.641091051805338e-05,
      "loss": 0.6375,
      "step": 365800
    },
    {
      "epoch": 5.744113029827315,
      "grad_norm": 4.888178825378418,
      "learning_rate": 4.640992935635793e-05,
      "loss": 0.6536,
      "step": 365900
    },
    {
      "epoch": 5.745682888540031,
      "grad_norm": 4.254112720489502,
      "learning_rate": 4.640894819466248e-05,
      "loss": 0.6479,
      "step": 366000
    },
    {
      "epoch": 5.747252747252747,
      "grad_norm": 2.6566765308380127,
      "learning_rate": 4.6407967032967034e-05,
      "loss": 0.6578,
      "step": 366100
    },
    {
      "epoch": 5.748822605965463,
      "grad_norm": 4.491400718688965,
      "learning_rate": 4.6406985871271585e-05,
      "loss": 0.704,
      "step": 366200
    },
    {
      "epoch": 5.750392464678179,
      "grad_norm": 4.131196022033691,
      "learning_rate": 4.640600470957614e-05,
      "loss": 0.6226,
      "step": 366300
    },
    {
      "epoch": 5.751962323390895,
      "grad_norm": 4.100867748260498,
      "learning_rate": 4.6405023547880694e-05,
      "loss": 0.685,
      "step": 366400
    },
    {
      "epoch": 5.75353218210361,
      "grad_norm": 3.828582286834717,
      "learning_rate": 4.640404238618525e-05,
      "loss": 0.6379,
      "step": 366500
    },
    {
      "epoch": 5.755102040816326,
      "grad_norm": 4.057051181793213,
      "learning_rate": 4.6403061224489796e-05,
      "loss": 0.645,
      "step": 366600
    },
    {
      "epoch": 5.756671899529042,
      "grad_norm": 3.4375345706939697,
      "learning_rate": 4.640208006279435e-05,
      "loss": 0.6627,
      "step": 366700
    },
    {
      "epoch": 5.758241758241758,
      "grad_norm": 4.193061351776123,
      "learning_rate": 4.6401098901098904e-05,
      "loss": 0.7035,
      "step": 366800
    },
    {
      "epoch": 5.759811616954474,
      "grad_norm": 3.2510769367218018,
      "learning_rate": 4.6400117739403455e-05,
      "loss": 0.6827,
      "step": 366900
    },
    {
      "epoch": 5.76138147566719,
      "grad_norm": 3.318105697631836,
      "learning_rate": 4.6399136577708006e-05,
      "loss": 0.6731,
      "step": 367000
    },
    {
      "epoch": 5.762951334379906,
      "grad_norm": 4.810370922088623,
      "learning_rate": 4.6398155416012564e-05,
      "loss": 0.6216,
      "step": 367100
    },
    {
      "epoch": 5.764521193092621,
      "grad_norm": 4.325926303863525,
      "learning_rate": 4.6397174254317115e-05,
      "loss": 0.6855,
      "step": 367200
    },
    {
      "epoch": 5.766091051805337,
      "grad_norm": 4.205986022949219,
      "learning_rate": 4.6396193092621666e-05,
      "loss": 0.6793,
      "step": 367300
    },
    {
      "epoch": 5.767660910518053,
      "grad_norm": 3.0998828411102295,
      "learning_rate": 4.639521193092622e-05,
      "loss": 0.6819,
      "step": 367400
    },
    {
      "epoch": 5.769230769230769,
      "grad_norm": 3.8857743740081787,
      "learning_rate": 4.6394230769230775e-05,
      "loss": 0.6684,
      "step": 367500
    },
    {
      "epoch": 5.770800627943485,
      "grad_norm": 3.0873019695281982,
      "learning_rate": 4.639324960753532e-05,
      "loss": 0.6463,
      "step": 367600
    },
    {
      "epoch": 5.772370486656201,
      "grad_norm": 4.441108226776123,
      "learning_rate": 4.639226844583988e-05,
      "loss": 0.6501,
      "step": 367700
    },
    {
      "epoch": 5.773940345368917,
      "grad_norm": 4.037863731384277,
      "learning_rate": 4.639128728414443e-05,
      "loss": 0.6469,
      "step": 367800
    },
    {
      "epoch": 5.775510204081632,
      "grad_norm": 4.091755390167236,
      "learning_rate": 4.6390306122448985e-05,
      "loss": 0.6929,
      "step": 367900
    },
    {
      "epoch": 5.777080062794348,
      "grad_norm": 4.314333438873291,
      "learning_rate": 4.638932496075353e-05,
      "loss": 0.6521,
      "step": 368000
    },
    {
      "epoch": 5.778649921507064,
      "grad_norm": 3.9465959072113037,
      "learning_rate": 4.638834379905809e-05,
      "loss": 0.6553,
      "step": 368100
    },
    {
      "epoch": 5.78021978021978,
      "grad_norm": 3.6041767597198486,
      "learning_rate": 4.638736263736264e-05,
      "loss": 0.6933,
      "step": 368200
    },
    {
      "epoch": 5.781789638932496,
      "grad_norm": 3.943971872329712,
      "learning_rate": 4.638638147566719e-05,
      "loss": 0.6513,
      "step": 368300
    },
    {
      "epoch": 5.783359497645212,
      "grad_norm": 3.2259621620178223,
      "learning_rate": 4.638540031397175e-05,
      "loss": 0.6379,
      "step": 368400
    },
    {
      "epoch": 5.784929356357928,
      "grad_norm": 4.530580520629883,
      "learning_rate": 4.63844191522763e-05,
      "loss": 0.6669,
      "step": 368500
    },
    {
      "epoch": 5.786499215070644,
      "grad_norm": 4.051909446716309,
      "learning_rate": 4.6383437990580856e-05,
      "loss": 0.6904,
      "step": 368600
    },
    {
      "epoch": 5.78806907378336,
      "grad_norm": 4.44315767288208,
      "learning_rate": 4.63824568288854e-05,
      "loss": 0.6637,
      "step": 368700
    },
    {
      "epoch": 5.789638932496075,
      "grad_norm": 3.9387099742889404,
      "learning_rate": 4.638147566718996e-05,
      "loss": 0.6721,
      "step": 368800
    },
    {
      "epoch": 5.791208791208791,
      "grad_norm": 3.411068916320801,
      "learning_rate": 4.638049450549451e-05,
      "loss": 0.7132,
      "step": 368900
    },
    {
      "epoch": 5.792778649921507,
      "grad_norm": 3.596254825592041,
      "learning_rate": 4.637951334379906e-05,
      "loss": 0.652,
      "step": 369000
    },
    {
      "epoch": 5.794348508634223,
      "grad_norm": 3.533430337905884,
      "learning_rate": 4.637853218210361e-05,
      "loss": 0.681,
      "step": 369100
    },
    {
      "epoch": 5.795918367346939,
      "grad_norm": 4.097634315490723,
      "learning_rate": 4.637755102040817e-05,
      "loss": 0.6691,
      "step": 369200
    },
    {
      "epoch": 5.797488226059655,
      "grad_norm": 3.677363395690918,
      "learning_rate": 4.637656985871272e-05,
      "loss": 0.6403,
      "step": 369300
    },
    {
      "epoch": 5.799058084772371,
      "grad_norm": 4.296504497528076,
      "learning_rate": 4.637558869701727e-05,
      "loss": 0.673,
      "step": 369400
    },
    {
      "epoch": 5.800627943485086,
      "grad_norm": 3.5437674522399902,
      "learning_rate": 4.637460753532182e-05,
      "loss": 0.677,
      "step": 369500
    },
    {
      "epoch": 5.802197802197802,
      "grad_norm": 3.2903525829315186,
      "learning_rate": 4.637362637362638e-05,
      "loss": 0.6664,
      "step": 369600
    },
    {
      "epoch": 5.803767660910518,
      "grad_norm": 2.6349081993103027,
      "learning_rate": 4.637264521193092e-05,
      "loss": 0.6235,
      "step": 369700
    },
    {
      "epoch": 5.805337519623234,
      "grad_norm": 3.9885849952697754,
      "learning_rate": 4.637166405023548e-05,
      "loss": 0.6608,
      "step": 369800
    },
    {
      "epoch": 5.80690737833595,
      "grad_norm": 4.024028778076172,
      "learning_rate": 4.637068288854003e-05,
      "loss": 0.6941,
      "step": 369900
    },
    {
      "epoch": 5.808477237048666,
      "grad_norm": 3.8917651176452637,
      "learning_rate": 4.636970172684459e-05,
      "loss": 0.6355,
      "step": 370000
    },
    {
      "epoch": 5.810047095761382,
      "grad_norm": 4.049811840057373,
      "learning_rate": 4.6368720565149134e-05,
      "loss": 0.674,
      "step": 370100
    },
    {
      "epoch": 5.811616954474097,
      "grad_norm": 3.4535574913024902,
      "learning_rate": 4.636773940345369e-05,
      "loss": 0.6616,
      "step": 370200
    },
    {
      "epoch": 5.813186813186813,
      "grad_norm": 3.867518424987793,
      "learning_rate": 4.636675824175824e-05,
      "loss": 0.6481,
      "step": 370300
    },
    {
      "epoch": 5.814756671899529,
      "grad_norm": 4.313273906707764,
      "learning_rate": 4.6365777080062794e-05,
      "loss": 0.676,
      "step": 370400
    },
    {
      "epoch": 5.816326530612245,
      "grad_norm": 5.203489780426025,
      "learning_rate": 4.636479591836735e-05,
      "loss": 0.6555,
      "step": 370500
    },
    {
      "epoch": 5.817896389324961,
      "grad_norm": 4.493654251098633,
      "learning_rate": 4.63638147566719e-05,
      "loss": 0.6463,
      "step": 370600
    },
    {
      "epoch": 5.819466248037677,
      "grad_norm": 3.4300990104675293,
      "learning_rate": 4.636283359497646e-05,
      "loss": 0.6421,
      "step": 370700
    },
    {
      "epoch": 5.821036106750393,
      "grad_norm": 4.633777141571045,
      "learning_rate": 4.6361852433281004e-05,
      "loss": 0.6549,
      "step": 370800
    },
    {
      "epoch": 5.822605965463108,
      "grad_norm": 4.6293487548828125,
      "learning_rate": 4.636087127158556e-05,
      "loss": 0.6647,
      "step": 370900
    },
    {
      "epoch": 5.824175824175824,
      "grad_norm": 3.6131374835968018,
      "learning_rate": 4.635989010989011e-05,
      "loss": 0.725,
      "step": 371000
    },
    {
      "epoch": 5.82574568288854,
      "grad_norm": 4.225136756896973,
      "learning_rate": 4.6358908948194664e-05,
      "loss": 0.6586,
      "step": 371100
    },
    {
      "epoch": 5.827315541601256,
      "grad_norm": 4.235537528991699,
      "learning_rate": 4.6357927786499215e-05,
      "loss": 0.6146,
      "step": 371200
    },
    {
      "epoch": 5.828885400313972,
      "grad_norm": 4.61490535736084,
      "learning_rate": 4.635694662480377e-05,
      "loss": 0.6764,
      "step": 371300
    },
    {
      "epoch": 5.830455259026688,
      "grad_norm": 2.3045108318328857,
      "learning_rate": 4.6355965463108324e-05,
      "loss": 0.6115,
      "step": 371400
    },
    {
      "epoch": 5.832025117739404,
      "grad_norm": 3.1196324825286865,
      "learning_rate": 4.6354984301412875e-05,
      "loss": 0.654,
      "step": 371500
    },
    {
      "epoch": 5.833594976452119,
      "grad_norm": 3.4111692905426025,
      "learning_rate": 4.6354003139717426e-05,
      "loss": 0.7101,
      "step": 371600
    },
    {
      "epoch": 5.835164835164835,
      "grad_norm": 4.449904918670654,
      "learning_rate": 4.6353021978021984e-05,
      "loss": 0.6485,
      "step": 371700
    },
    {
      "epoch": 5.836734693877551,
      "grad_norm": 3.559354543685913,
      "learning_rate": 4.635204081632653e-05,
      "loss": 0.6352,
      "step": 371800
    },
    {
      "epoch": 5.838304552590267,
      "grad_norm": 2.7259361743927,
      "learning_rate": 4.6351059654631086e-05,
      "loss": 0.6621,
      "step": 371900
    },
    {
      "epoch": 5.839874411302983,
      "grad_norm": 3.6533596515655518,
      "learning_rate": 4.6350078492935637e-05,
      "loss": 0.6925,
      "step": 372000
    },
    {
      "epoch": 5.841444270015699,
      "grad_norm": 3.4568326473236084,
      "learning_rate": 4.6349097331240194e-05,
      "loss": 0.6424,
      "step": 372100
    },
    {
      "epoch": 5.843014128728415,
      "grad_norm": 4.288630485534668,
      "learning_rate": 4.634811616954474e-05,
      "loss": 0.6932,
      "step": 372200
    },
    {
      "epoch": 5.84458398744113,
      "grad_norm": 2.3082873821258545,
      "learning_rate": 4.6347135007849296e-05,
      "loss": 0.6402,
      "step": 372300
    },
    {
      "epoch": 5.846153846153846,
      "grad_norm": 4.566142559051514,
      "learning_rate": 4.634615384615385e-05,
      "loss": 0.6668,
      "step": 372400
    },
    {
      "epoch": 5.847723704866562,
      "grad_norm": 3.401327133178711,
      "learning_rate": 4.63451726844584e-05,
      "loss": 0.6951,
      "step": 372500
    },
    {
      "epoch": 5.849293563579278,
      "grad_norm": 3.724867343902588,
      "learning_rate": 4.6344191522762956e-05,
      "loss": 0.6597,
      "step": 372600
    },
    {
      "epoch": 5.850863422291994,
      "grad_norm": 4.210358619689941,
      "learning_rate": 4.634321036106751e-05,
      "loss": 0.6708,
      "step": 372700
    },
    {
      "epoch": 5.85243328100471,
      "grad_norm": 4.9038825035095215,
      "learning_rate": 4.6342229199372065e-05,
      "loss": 0.6633,
      "step": 372800
    },
    {
      "epoch": 5.854003139717426,
      "grad_norm": 4.345468044281006,
      "learning_rate": 4.634124803767661e-05,
      "loss": 0.6999,
      "step": 372900
    },
    {
      "epoch": 5.855572998430142,
      "grad_norm": 4.249322891235352,
      "learning_rate": 4.634026687598117e-05,
      "loss": 0.7294,
      "step": 373000
    },
    {
      "epoch": 5.857142857142857,
      "grad_norm": 4.5452799797058105,
      "learning_rate": 4.633928571428572e-05,
      "loss": 0.6268,
      "step": 373100
    },
    {
      "epoch": 5.858712715855573,
      "grad_norm": 4.246339797973633,
      "learning_rate": 4.633830455259027e-05,
      "loss": 0.6649,
      "step": 373200
    },
    {
      "epoch": 5.860282574568289,
      "grad_norm": 3.556497812271118,
      "learning_rate": 4.633732339089482e-05,
      "loss": 0.6986,
      "step": 373300
    },
    {
      "epoch": 5.861852433281005,
      "grad_norm": 4.259708404541016,
      "learning_rate": 4.633634222919938e-05,
      "loss": 0.679,
      "step": 373400
    },
    {
      "epoch": 5.863422291993721,
      "grad_norm": 4.747313499450684,
      "learning_rate": 4.633536106750393e-05,
      "loss": 0.6472,
      "step": 373500
    },
    {
      "epoch": 5.864992150706437,
      "grad_norm": 5.503922939300537,
      "learning_rate": 4.633437990580848e-05,
      "loss": 0.6773,
      "step": 373600
    },
    {
      "epoch": 5.866562009419153,
      "grad_norm": 4.0377631187438965,
      "learning_rate": 4.633339874411303e-05,
      "loss": 0.6316,
      "step": 373700
    },
    {
      "epoch": 5.868131868131869,
      "grad_norm": 3.6532769203186035,
      "learning_rate": 4.633241758241759e-05,
      "loss": 0.6041,
      "step": 373800
    },
    {
      "epoch": 5.869701726844584,
      "grad_norm": 4.702557563781738,
      "learning_rate": 4.633143642072213e-05,
      "loss": 0.6659,
      "step": 373900
    },
    {
      "epoch": 5.8712715855573,
      "grad_norm": 3.841949224472046,
      "learning_rate": 4.633045525902669e-05,
      "loss": 0.6888,
      "step": 374000
    },
    {
      "epoch": 5.872841444270016,
      "grad_norm": 3.5866479873657227,
      "learning_rate": 4.632947409733124e-05,
      "loss": 0.6785,
      "step": 374100
    },
    {
      "epoch": 5.874411302982732,
      "grad_norm": 5.221521377563477,
      "learning_rate": 4.63284929356358e-05,
      "loss": 0.6472,
      "step": 374200
    },
    {
      "epoch": 5.875981161695448,
      "grad_norm": 5.299700736999512,
      "learning_rate": 4.632751177394034e-05,
      "loss": 0.6695,
      "step": 374300
    },
    {
      "epoch": 5.877551020408164,
      "grad_norm": 3.1743874549865723,
      "learning_rate": 4.63265306122449e-05,
      "loss": 0.6609,
      "step": 374400
    },
    {
      "epoch": 5.8791208791208796,
      "grad_norm": 3.6879920959472656,
      "learning_rate": 4.632554945054945e-05,
      "loss": 0.6736,
      "step": 374500
    },
    {
      "epoch": 5.880690737833595,
      "grad_norm": 3.464432954788208,
      "learning_rate": 4.6324568288854e-05,
      "loss": 0.6872,
      "step": 374600
    },
    {
      "epoch": 5.882260596546311,
      "grad_norm": 4.3694634437561035,
      "learning_rate": 4.632358712715856e-05,
      "loss": 0.6414,
      "step": 374700
    },
    {
      "epoch": 5.883830455259027,
      "grad_norm": 3.963237762451172,
      "learning_rate": 4.632260596546311e-05,
      "loss": 0.6814,
      "step": 374800
    },
    {
      "epoch": 5.885400313971743,
      "grad_norm": 7.250030517578125,
      "learning_rate": 4.632162480376767e-05,
      "loss": 0.6523,
      "step": 374900
    },
    {
      "epoch": 5.8869701726844585,
      "grad_norm": 3.488459587097168,
      "learning_rate": 4.6320643642072213e-05,
      "loss": 0.6865,
      "step": 375000
    },
    {
      "epoch": 5.8885400313971745,
      "grad_norm": 4.693628311157227,
      "learning_rate": 4.631966248037677e-05,
      "loss": 0.6997,
      "step": 375100
    },
    {
      "epoch": 5.8901098901098905,
      "grad_norm": 3.9027915000915527,
      "learning_rate": 4.631868131868132e-05,
      "loss": 0.6968,
      "step": 375200
    },
    {
      "epoch": 5.891679748822606,
      "grad_norm": 3.4118261337280273,
      "learning_rate": 4.631770015698587e-05,
      "loss": 0.6633,
      "step": 375300
    },
    {
      "epoch": 5.893249607535322,
      "grad_norm": 3.0108890533447266,
      "learning_rate": 4.6316718995290424e-05,
      "loss": 0.6464,
      "step": 375400
    },
    {
      "epoch": 5.8948194662480375,
      "grad_norm": 3.9634969234466553,
      "learning_rate": 4.631573783359498e-05,
      "loss": 0.6844,
      "step": 375500
    },
    {
      "epoch": 5.8963893249607535,
      "grad_norm": 3.6455769538879395,
      "learning_rate": 4.6314756671899526e-05,
      "loss": 0.6778,
      "step": 375600
    },
    {
      "epoch": 5.8979591836734695,
      "grad_norm": 4.754942893981934,
      "learning_rate": 4.6313775510204084e-05,
      "loss": 0.6599,
      "step": 375700
    },
    {
      "epoch": 5.8995290423861855,
      "grad_norm": 3.285341501235962,
      "learning_rate": 4.6312794348508635e-05,
      "loss": 0.6797,
      "step": 375800
    },
    {
      "epoch": 5.9010989010989015,
      "grad_norm": 4.112185001373291,
      "learning_rate": 4.631181318681319e-05,
      "loss": 0.6504,
      "step": 375900
    },
    {
      "epoch": 5.9026687598116165,
      "grad_norm": 4.051919937133789,
      "learning_rate": 4.631083202511774e-05,
      "loss": 0.6965,
      "step": 376000
    },
    {
      "epoch": 5.9042386185243325,
      "grad_norm": 4.371918678283691,
      "learning_rate": 4.6309850863422295e-05,
      "loss": 0.7046,
      "step": 376100
    },
    {
      "epoch": 5.9058084772370485,
      "grad_norm": 4.057939052581787,
      "learning_rate": 4.6308869701726846e-05,
      "loss": 0.6658,
      "step": 376200
    },
    {
      "epoch": 5.9073783359497645,
      "grad_norm": 4.234270095825195,
      "learning_rate": 4.6307888540031397e-05,
      "loss": 0.638,
      "step": 376300
    },
    {
      "epoch": 5.9089481946624804,
      "grad_norm": 4.362351417541504,
      "learning_rate": 4.630690737833595e-05,
      "loss": 0.6661,
      "step": 376400
    },
    {
      "epoch": 5.910518053375196,
      "grad_norm": 4.309915065765381,
      "learning_rate": 4.6305926216640505e-05,
      "loss": 0.7178,
      "step": 376500
    },
    {
      "epoch": 5.912087912087912,
      "grad_norm": 4.0230231285095215,
      "learning_rate": 4.6304945054945056e-05,
      "loss": 0.6338,
      "step": 376600
    },
    {
      "epoch": 5.9136577708006275,
      "grad_norm": 2.9753143787384033,
      "learning_rate": 4.630396389324961e-05,
      "loss": 0.6735,
      "step": 376700
    },
    {
      "epoch": 5.9152276295133435,
      "grad_norm": 3.457864761352539,
      "learning_rate": 4.6302982731554165e-05,
      "loss": 0.6402,
      "step": 376800
    },
    {
      "epoch": 5.916797488226059,
      "grad_norm": 3.2931642532348633,
      "learning_rate": 4.6302001569858716e-05,
      "loss": 0.6532,
      "step": 376900
    },
    {
      "epoch": 5.918367346938775,
      "grad_norm": 4.739506244659424,
      "learning_rate": 4.630102040816327e-05,
      "loss": 0.6463,
      "step": 377000
    },
    {
      "epoch": 5.919937205651491,
      "grad_norm": 2.9534997940063477,
      "learning_rate": 4.630003924646782e-05,
      "loss": 0.6243,
      "step": 377100
    },
    {
      "epoch": 5.921507064364207,
      "grad_norm": 4.144380569458008,
      "learning_rate": 4.6299058084772376e-05,
      "loss": 0.6368,
      "step": 377200
    },
    {
      "epoch": 5.923076923076923,
      "grad_norm": 3.6312615871429443,
      "learning_rate": 4.629807692307693e-05,
      "loss": 0.662,
      "step": 377300
    },
    {
      "epoch": 5.924646781789638,
      "grad_norm": 4.390826225280762,
      "learning_rate": 4.629709576138148e-05,
      "loss": 0.6609,
      "step": 377400
    },
    {
      "epoch": 5.926216640502354,
      "grad_norm": 4.177272796630859,
      "learning_rate": 4.629611459968603e-05,
      "loss": 0.6611,
      "step": 377500
    },
    {
      "epoch": 5.92778649921507,
      "grad_norm": 3.6739606857299805,
      "learning_rate": 4.6295133437990586e-05,
      "loss": 0.664,
      "step": 377600
    },
    {
      "epoch": 5.929356357927786,
      "grad_norm": 3.4477550983428955,
      "learning_rate": 4.629415227629513e-05,
      "loss": 0.6734,
      "step": 377700
    },
    {
      "epoch": 5.930926216640502,
      "grad_norm": 4.571457386016846,
      "learning_rate": 4.629317111459969e-05,
      "loss": 0.6347,
      "step": 377800
    },
    {
      "epoch": 5.932496075353218,
      "grad_norm": 3.491025924682617,
      "learning_rate": 4.629218995290424e-05,
      "loss": 0.6453,
      "step": 377900
    },
    {
      "epoch": 5.934065934065934,
      "grad_norm": 4.31247615814209,
      "learning_rate": 4.62912087912088e-05,
      "loss": 0.6275,
      "step": 378000
    },
    {
      "epoch": 5.93563579277865,
      "grad_norm": 4.095800876617432,
      "learning_rate": 4.629022762951334e-05,
      "loss": 0.6912,
      "step": 378100
    },
    {
      "epoch": 5.937205651491366,
      "grad_norm": 4.288998126983643,
      "learning_rate": 4.62892464678179e-05,
      "loss": 0.6299,
      "step": 378200
    },
    {
      "epoch": 5.938775510204081,
      "grad_norm": 3.5911662578582764,
      "learning_rate": 4.628826530612245e-05,
      "loss": 0.6407,
      "step": 378300
    },
    {
      "epoch": 5.940345368916797,
      "grad_norm": 4.331345558166504,
      "learning_rate": 4.6287284144427e-05,
      "loss": 0.6788,
      "step": 378400
    },
    {
      "epoch": 5.941915227629513,
      "grad_norm": 3.382161855697632,
      "learning_rate": 4.628630298273155e-05,
      "loss": 0.6101,
      "step": 378500
    },
    {
      "epoch": 5.943485086342229,
      "grad_norm": 4.6485819816589355,
      "learning_rate": 4.628532182103611e-05,
      "loss": 0.6537,
      "step": 378600
    },
    {
      "epoch": 5.945054945054945,
      "grad_norm": 2.74769926071167,
      "learning_rate": 4.628434065934066e-05,
      "loss": 0.6649,
      "step": 378700
    },
    {
      "epoch": 5.946624803767661,
      "grad_norm": 4.445611476898193,
      "learning_rate": 4.628335949764521e-05,
      "loss": 0.6511,
      "step": 378800
    },
    {
      "epoch": 5.948194662480377,
      "grad_norm": 3.267566442489624,
      "learning_rate": 4.628237833594977e-05,
      "loss": 0.6408,
      "step": 378900
    },
    {
      "epoch": 5.949764521193092,
      "grad_norm": 3.5087602138519287,
      "learning_rate": 4.628139717425432e-05,
      "loss": 0.6459,
      "step": 379000
    },
    {
      "epoch": 5.951334379905808,
      "grad_norm": 3.086792230606079,
      "learning_rate": 4.628041601255887e-05,
      "loss": 0.6743,
      "step": 379100
    },
    {
      "epoch": 5.952904238618524,
      "grad_norm": 3.8401358127593994,
      "learning_rate": 4.627943485086342e-05,
      "loss": 0.7153,
      "step": 379200
    },
    {
      "epoch": 5.95447409733124,
      "grad_norm": 5.0729780197143555,
      "learning_rate": 4.627845368916798e-05,
      "loss": 0.6797,
      "step": 379300
    },
    {
      "epoch": 5.956043956043956,
      "grad_norm": 3.624539613723755,
      "learning_rate": 4.627747252747253e-05,
      "loss": 0.7207,
      "step": 379400
    },
    {
      "epoch": 5.957613814756672,
      "grad_norm": 3.570833683013916,
      "learning_rate": 4.627649136577708e-05,
      "loss": 0.6625,
      "step": 379500
    },
    {
      "epoch": 5.959183673469388,
      "grad_norm": 3.964197874069214,
      "learning_rate": 4.627551020408163e-05,
      "loss": 0.6814,
      "step": 379600
    },
    {
      "epoch": 5.960753532182103,
      "grad_norm": 4.244136810302734,
      "learning_rate": 4.627452904238619e-05,
      "loss": 0.6816,
      "step": 379700
    },
    {
      "epoch": 5.962323390894819,
      "grad_norm": 4.867712497711182,
      "learning_rate": 4.6273547880690735e-05,
      "loss": 0.6705,
      "step": 379800
    },
    {
      "epoch": 5.963893249607535,
      "grad_norm": 3.880558729171753,
      "learning_rate": 4.627256671899529e-05,
      "loss": 0.6625,
      "step": 379900
    },
    {
      "epoch": 5.965463108320251,
      "grad_norm": 4.600466728210449,
      "learning_rate": 4.6271585557299844e-05,
      "loss": 0.6897,
      "step": 380000
    },
    {
      "epoch": 5.967032967032967,
      "grad_norm": 2.9872663021087646,
      "learning_rate": 4.62706043956044e-05,
      "loss": 0.6747,
      "step": 380100
    },
    {
      "epoch": 5.968602825745683,
      "grad_norm": 4.3771071434021,
      "learning_rate": 4.6269623233908946e-05,
      "loss": 0.6765,
      "step": 380200
    },
    {
      "epoch": 5.970172684458399,
      "grad_norm": 4.434260845184326,
      "learning_rate": 4.6268642072213504e-05,
      "loss": 0.6424,
      "step": 380300
    },
    {
      "epoch": 5.971742543171114,
      "grad_norm": 3.605756998062134,
      "learning_rate": 4.6267660910518055e-05,
      "loss": 0.6348,
      "step": 380400
    },
    {
      "epoch": 5.97331240188383,
      "grad_norm": 3.3112335205078125,
      "learning_rate": 4.6266679748822606e-05,
      "loss": 0.6744,
      "step": 380500
    },
    {
      "epoch": 5.974882260596546,
      "grad_norm": 4.3072638511657715,
      "learning_rate": 4.6265698587127157e-05,
      "loss": 0.6866,
      "step": 380600
    },
    {
      "epoch": 5.976452119309262,
      "grad_norm": 3.880683422088623,
      "learning_rate": 4.6264717425431714e-05,
      "loss": 0.6473,
      "step": 380700
    },
    {
      "epoch": 5.978021978021978,
      "grad_norm": 4.781772136688232,
      "learning_rate": 4.6263736263736265e-05,
      "loss": 0.6265,
      "step": 380800
    },
    {
      "epoch": 5.979591836734694,
      "grad_norm": 3.387913227081299,
      "learning_rate": 4.6262755102040816e-05,
      "loss": 0.6652,
      "step": 380900
    },
    {
      "epoch": 5.98116169544741,
      "grad_norm": 4.042553901672363,
      "learning_rate": 4.6261773940345374e-05,
      "loss": 0.655,
      "step": 381000
    },
    {
      "epoch": 5.982731554160125,
      "grad_norm": 3.7834274768829346,
      "learning_rate": 4.6260792778649925e-05,
      "loss": 0.6849,
      "step": 381100
    },
    {
      "epoch": 5.984301412872841,
      "grad_norm": 3.7385804653167725,
      "learning_rate": 4.6259811616954476e-05,
      "loss": 0.6867,
      "step": 381200
    },
    {
      "epoch": 5.985871271585557,
      "grad_norm": 4.341507911682129,
      "learning_rate": 4.625883045525903e-05,
      "loss": 0.6678,
      "step": 381300
    },
    {
      "epoch": 5.987441130298273,
      "grad_norm": 3.9179437160491943,
      "learning_rate": 4.6257849293563585e-05,
      "loss": 0.6636,
      "step": 381400
    },
    {
      "epoch": 5.989010989010989,
      "grad_norm": 2.9884016513824463,
      "learning_rate": 4.6256868131868136e-05,
      "loss": 0.6666,
      "step": 381500
    },
    {
      "epoch": 5.990580847723705,
      "grad_norm": 3.2087485790252686,
      "learning_rate": 4.625588697017269e-05,
      "loss": 0.6869,
      "step": 381600
    },
    {
      "epoch": 5.992150706436421,
      "grad_norm": 3.5302371978759766,
      "learning_rate": 4.625490580847724e-05,
      "loss": 0.6101,
      "step": 381700
    },
    {
      "epoch": 5.993720565149136,
      "grad_norm": 4.310197830200195,
      "learning_rate": 4.6253924646781795e-05,
      "loss": 0.6878,
      "step": 381800
    },
    {
      "epoch": 5.995290423861852,
      "grad_norm": 4.031547546386719,
      "learning_rate": 4.625294348508634e-05,
      "loss": 0.6889,
      "step": 381900
    },
    {
      "epoch": 5.996860282574568,
      "grad_norm": 3.9591472148895264,
      "learning_rate": 4.62519623233909e-05,
      "loss": 0.6761,
      "step": 382000
    },
    {
      "epoch": 5.998430141287284,
      "grad_norm": 3.003173589706421,
      "learning_rate": 4.625098116169545e-05,
      "loss": 0.637,
      "step": 382100
    },
    {
      "epoch": 6.0,
      "grad_norm": 3.7646963596343994,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.6962,
      "step": 382200
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.017717957496643,
      "eval_runtime": 14.9552,
      "eval_samples_per_second": 224.203,
      "eval_steps_per_second": 224.203,
      "step": 382200
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5163915157318115,
      "eval_runtime": 265.9211,
      "eval_samples_per_second": 239.545,
      "eval_steps_per_second": 239.545,
      "step": 382200
    },
    {
      "epoch": 6.001569858712716,
      "grad_norm": 3.7179269790649414,
      "learning_rate": 4.624901883830455e-05,
      "loss": 0.6704,
      "step": 382300
    },
    {
      "epoch": 6.003139717425432,
      "grad_norm": 4.952663898468018,
      "learning_rate": 4.624803767660911e-05,
      "loss": 0.6484,
      "step": 382400
    },
    {
      "epoch": 6.004709576138148,
      "grad_norm": 3.4903290271759033,
      "learning_rate": 4.624705651491366e-05,
      "loss": 0.6508,
      "step": 382500
    },
    {
      "epoch": 6.006279434850863,
      "grad_norm": 4.754391193389893,
      "learning_rate": 4.624607535321821e-05,
      "loss": 0.6746,
      "step": 382600
    },
    {
      "epoch": 6.007849293563579,
      "grad_norm": 3.389535665512085,
      "learning_rate": 4.624509419152276e-05,
      "loss": 0.6531,
      "step": 382700
    },
    {
      "epoch": 6.009419152276295,
      "grad_norm": 3.973822593688965,
      "learning_rate": 4.624411302982732e-05,
      "loss": 0.7116,
      "step": 382800
    },
    {
      "epoch": 6.010989010989011,
      "grad_norm": 4.427331924438477,
      "learning_rate": 4.624313186813187e-05,
      "loss": 0.6445,
      "step": 382900
    },
    {
      "epoch": 6.012558869701727,
      "grad_norm": 2.409346103668213,
      "learning_rate": 4.624215070643642e-05,
      "loss": 0.6546,
      "step": 383000
    },
    {
      "epoch": 6.014128728414443,
      "grad_norm": 3.4141550064086914,
      "learning_rate": 4.624116954474098e-05,
      "loss": 0.6469,
      "step": 383100
    },
    {
      "epoch": 6.015698587127159,
      "grad_norm": 4.0068464279174805,
      "learning_rate": 4.624018838304553e-05,
      "loss": 0.6317,
      "step": 383200
    },
    {
      "epoch": 6.017268445839874,
      "grad_norm": 3.163846492767334,
      "learning_rate": 4.623920722135008e-05,
      "loss": 0.6315,
      "step": 383300
    },
    {
      "epoch": 6.01883830455259,
      "grad_norm": 4.305241107940674,
      "learning_rate": 4.623822605965463e-05,
      "loss": 0.613,
      "step": 383400
    },
    {
      "epoch": 6.020408163265306,
      "grad_norm": 4.453533172607422,
      "learning_rate": 4.623724489795919e-05,
      "loss": 0.6509,
      "step": 383500
    },
    {
      "epoch": 6.021978021978022,
      "grad_norm": 3.1226980686187744,
      "learning_rate": 4.623626373626374e-05,
      "loss": 0.6475,
      "step": 383600
    },
    {
      "epoch": 6.023547880690738,
      "grad_norm": 4.919028282165527,
      "learning_rate": 4.623528257456829e-05,
      "loss": 0.6861,
      "step": 383700
    },
    {
      "epoch": 6.025117739403454,
      "grad_norm": 3.7719614505767822,
      "learning_rate": 4.623430141287284e-05,
      "loss": 0.6912,
      "step": 383800
    },
    {
      "epoch": 6.02668759811617,
      "grad_norm": 4.5295538902282715,
      "learning_rate": 4.62333202511774e-05,
      "loss": 0.6161,
      "step": 383900
    },
    {
      "epoch": 6.028257456828886,
      "grad_norm": 4.925210952758789,
      "learning_rate": 4.6232339089481944e-05,
      "loss": 0.6641,
      "step": 384000
    },
    {
      "epoch": 6.029827315541601,
      "grad_norm": 4.005380153656006,
      "learning_rate": 4.62313579277865e-05,
      "loss": 0.666,
      "step": 384100
    },
    {
      "epoch": 6.031397174254317,
      "grad_norm": 4.103209495544434,
      "learning_rate": 4.623037676609105e-05,
      "loss": 0.6329,
      "step": 384200
    },
    {
      "epoch": 6.032967032967033,
      "grad_norm": 4.347714900970459,
      "learning_rate": 4.622939560439561e-05,
      "loss": 0.6418,
      "step": 384300
    },
    {
      "epoch": 6.034536891679749,
      "grad_norm": 2.7830474376678467,
      "learning_rate": 4.6228414442700155e-05,
      "loss": 0.6495,
      "step": 384400
    },
    {
      "epoch": 6.036106750392465,
      "grad_norm": 4.087255001068115,
      "learning_rate": 4.622743328100471e-05,
      "loss": 0.6409,
      "step": 384500
    },
    {
      "epoch": 6.037676609105181,
      "grad_norm": 4.205899715423584,
      "learning_rate": 4.6226452119309264e-05,
      "loss": 0.6507,
      "step": 384600
    },
    {
      "epoch": 6.039246467817897,
      "grad_norm": 4.573139667510986,
      "learning_rate": 4.6225470957613815e-05,
      "loss": 0.7173,
      "step": 384700
    },
    {
      "epoch": 6.040816326530612,
      "grad_norm": 4.48893404006958,
      "learning_rate": 4.6224489795918366e-05,
      "loss": 0.6481,
      "step": 384800
    },
    {
      "epoch": 6.042386185243328,
      "grad_norm": 4.0609235763549805,
      "learning_rate": 4.622350863422292e-05,
      "loss": 0.6465,
      "step": 384900
    },
    {
      "epoch": 6.043956043956044,
      "grad_norm": 3.5424561500549316,
      "learning_rate": 4.6222527472527474e-05,
      "loss": 0.6826,
      "step": 385000
    },
    {
      "epoch": 6.04552590266876,
      "grad_norm": 3.560377836227417,
      "learning_rate": 4.6221546310832025e-05,
      "loss": 0.6548,
      "step": 385100
    },
    {
      "epoch": 6.047095761381476,
      "grad_norm": 4.104085922241211,
      "learning_rate": 4.622056514913658e-05,
      "loss": 0.6522,
      "step": 385200
    },
    {
      "epoch": 6.048665620094192,
      "grad_norm": 3.7108168601989746,
      "learning_rate": 4.6219583987441134e-05,
      "loss": 0.6877,
      "step": 385300
    },
    {
      "epoch": 6.050235478806908,
      "grad_norm": 2.6529319286346436,
      "learning_rate": 4.6218602825745685e-05,
      "loss": 0.6926,
      "step": 385400
    },
    {
      "epoch": 6.051805337519623,
      "grad_norm": 4.005117893218994,
      "learning_rate": 4.6217621664050236e-05,
      "loss": 0.666,
      "step": 385500
    },
    {
      "epoch": 6.053375196232339,
      "grad_norm": 3.3702213764190674,
      "learning_rate": 4.6216640502354794e-05,
      "loss": 0.6606,
      "step": 385600
    },
    {
      "epoch": 6.054945054945055,
      "grad_norm": 4.54873514175415,
      "learning_rate": 4.6215659340659345e-05,
      "loss": 0.6734,
      "step": 385700
    },
    {
      "epoch": 6.056514913657771,
      "grad_norm": 1.6776834726333618,
      "learning_rate": 4.6214678178963896e-05,
      "loss": 0.6549,
      "step": 385800
    },
    {
      "epoch": 6.058084772370487,
      "grad_norm": 4.066808700561523,
      "learning_rate": 4.621369701726845e-05,
      "loss": 0.6422,
      "step": 385900
    },
    {
      "epoch": 6.059654631083203,
      "grad_norm": 3.7277724742889404,
      "learning_rate": 4.6212715855573004e-05,
      "loss": 0.6517,
      "step": 386000
    },
    {
      "epoch": 6.061224489795919,
      "grad_norm": 3.0438520908355713,
      "learning_rate": 4.621173469387755e-05,
      "loss": 0.7132,
      "step": 386100
    },
    {
      "epoch": 6.062794348508635,
      "grad_norm": 3.3025009632110596,
      "learning_rate": 4.6210753532182106e-05,
      "loss": 0.6136,
      "step": 386200
    },
    {
      "epoch": 6.06436420722135,
      "grad_norm": 3.4012274742126465,
      "learning_rate": 4.620977237048666e-05,
      "loss": 0.6402,
      "step": 386300
    },
    {
      "epoch": 6.065934065934066,
      "grad_norm": 3.5316455364227295,
      "learning_rate": 4.6208791208791215e-05,
      "loss": 0.6222,
      "step": 386400
    },
    {
      "epoch": 6.067503924646782,
      "grad_norm": 3.9949822425842285,
      "learning_rate": 4.620781004709576e-05,
      "loss": 0.6489,
      "step": 386500
    },
    {
      "epoch": 6.069073783359498,
      "grad_norm": 4.166678428649902,
      "learning_rate": 4.620682888540032e-05,
      "loss": 0.6549,
      "step": 386600
    },
    {
      "epoch": 6.070643642072214,
      "grad_norm": 4.5390400886535645,
      "learning_rate": 4.620584772370487e-05,
      "loss": 0.6525,
      "step": 386700
    },
    {
      "epoch": 6.07221350078493,
      "grad_norm": 4.795370101928711,
      "learning_rate": 4.620486656200942e-05,
      "loss": 0.6588,
      "step": 386800
    },
    {
      "epoch": 6.073783359497646,
      "grad_norm": 4.757525444030762,
      "learning_rate": 4.620388540031397e-05,
      "loss": 0.6473,
      "step": 386900
    },
    {
      "epoch": 6.075353218210361,
      "grad_norm": 4.0784711837768555,
      "learning_rate": 4.620290423861853e-05,
      "loss": 0.6852,
      "step": 387000
    },
    {
      "epoch": 6.076923076923077,
      "grad_norm": 5.13023042678833,
      "learning_rate": 4.620192307692308e-05,
      "loss": 0.6525,
      "step": 387100
    },
    {
      "epoch": 6.078492935635793,
      "grad_norm": 4.087092399597168,
      "learning_rate": 4.620094191522763e-05,
      "loss": 0.6551,
      "step": 387200
    },
    {
      "epoch": 6.080062794348509,
      "grad_norm": 4.496434211730957,
      "learning_rate": 4.619996075353219e-05,
      "loss": 0.6742,
      "step": 387300
    },
    {
      "epoch": 6.081632653061225,
      "grad_norm": 2.2731575965881348,
      "learning_rate": 4.619897959183674e-05,
      "loss": 0.6544,
      "step": 387400
    },
    {
      "epoch": 6.083202511773941,
      "grad_norm": 3.756549835205078,
      "learning_rate": 4.619799843014129e-05,
      "loss": 0.6866,
      "step": 387500
    },
    {
      "epoch": 6.0847723704866565,
      "grad_norm": 4.095126152038574,
      "learning_rate": 4.619701726844584e-05,
      "loss": 0.6567,
      "step": 387600
    },
    {
      "epoch": 6.086342229199372,
      "grad_norm": 2.427556276321411,
      "learning_rate": 4.61960361067504e-05,
      "loss": 0.6336,
      "step": 387700
    },
    {
      "epoch": 6.087912087912088,
      "grad_norm": 4.060569763183594,
      "learning_rate": 4.619505494505495e-05,
      "loss": 0.64,
      "step": 387800
    },
    {
      "epoch": 6.089481946624804,
      "grad_norm": 3.849698543548584,
      "learning_rate": 4.61940737833595e-05,
      "loss": 0.6189,
      "step": 387900
    },
    {
      "epoch": 6.0910518053375196,
      "grad_norm": 3.714087963104248,
      "learning_rate": 4.619309262166405e-05,
      "loss": 0.6116,
      "step": 388000
    },
    {
      "epoch": 6.0926216640502355,
      "grad_norm": 2.8632028102874756,
      "learning_rate": 4.619211145996861e-05,
      "loss": 0.5928,
      "step": 388100
    },
    {
      "epoch": 6.0941915227629515,
      "grad_norm": 6.0088701248168945,
      "learning_rate": 4.619113029827315e-05,
      "loss": 0.6468,
      "step": 388200
    },
    {
      "epoch": 6.0957613814756675,
      "grad_norm": 3.610157012939453,
      "learning_rate": 4.619014913657771e-05,
      "loss": 0.6247,
      "step": 388300
    },
    {
      "epoch": 6.0973312401883835,
      "grad_norm": 3.730862855911255,
      "learning_rate": 4.618916797488226e-05,
      "loss": 0.6509,
      "step": 388400
    },
    {
      "epoch": 6.0989010989010985,
      "grad_norm": 4.066860198974609,
      "learning_rate": 4.618818681318682e-05,
      "loss": 0.663,
      "step": 388500
    },
    {
      "epoch": 6.1004709576138145,
      "grad_norm": 4.113926887512207,
      "learning_rate": 4.6187205651491364e-05,
      "loss": 0.6652,
      "step": 388600
    },
    {
      "epoch": 6.1020408163265305,
      "grad_norm": 2.4166042804718018,
      "learning_rate": 4.618622448979592e-05,
      "loss": 0.6446,
      "step": 388700
    },
    {
      "epoch": 6.1036106750392465,
      "grad_norm": 4.323604583740234,
      "learning_rate": 4.618524332810047e-05,
      "loss": 0.6877,
      "step": 388800
    },
    {
      "epoch": 6.1051805337519625,
      "grad_norm": 2.657805919647217,
      "learning_rate": 4.6184262166405024e-05,
      "loss": 0.6784,
      "step": 388900
    },
    {
      "epoch": 6.106750392464678,
      "grad_norm": 4.3692946434021,
      "learning_rate": 4.6183281004709574e-05,
      "loss": 0.6676,
      "step": 389000
    },
    {
      "epoch": 6.108320251177394,
      "grad_norm": 3.04266095161438,
      "learning_rate": 4.618229984301413e-05,
      "loss": 0.6378,
      "step": 389100
    },
    {
      "epoch": 6.1098901098901095,
      "grad_norm": 4.294496536254883,
      "learning_rate": 4.618131868131868e-05,
      "loss": 0.6719,
      "step": 389200
    },
    {
      "epoch": 6.1114599686028255,
      "grad_norm": 4.471674919128418,
      "learning_rate": 4.6180337519623234e-05,
      "loss": 0.6255,
      "step": 389300
    },
    {
      "epoch": 6.1130298273155415,
      "grad_norm": 3.787050485610962,
      "learning_rate": 4.617935635792779e-05,
      "loss": 0.6699,
      "step": 389400
    },
    {
      "epoch": 6.114599686028257,
      "grad_norm": 4.69165563583374,
      "learning_rate": 4.617837519623234e-05,
      "loss": 0.6353,
      "step": 389500
    },
    {
      "epoch": 6.116169544740973,
      "grad_norm": 2.9589686393737793,
      "learning_rate": 4.6177394034536894e-05,
      "loss": 0.6922,
      "step": 389600
    },
    {
      "epoch": 6.117739403453689,
      "grad_norm": 3.57450795173645,
      "learning_rate": 4.6176412872841445e-05,
      "loss": 0.6335,
      "step": 389700
    },
    {
      "epoch": 6.119309262166405,
      "grad_norm": 4.073467254638672,
      "learning_rate": 4.6175431711146e-05,
      "loss": 0.6648,
      "step": 389800
    },
    {
      "epoch": 6.1208791208791204,
      "grad_norm": 4.166979789733887,
      "learning_rate": 4.6174450549450554e-05,
      "loss": 0.6641,
      "step": 389900
    },
    {
      "epoch": 6.122448979591836,
      "grad_norm": 3.428457498550415,
      "learning_rate": 4.6173469387755105e-05,
      "loss": 0.641,
      "step": 390000
    },
    {
      "epoch": 6.124018838304552,
      "grad_norm": 3.6632397174835205,
      "learning_rate": 4.6172488226059656e-05,
      "loss": 0.6044,
      "step": 390100
    },
    {
      "epoch": 6.125588697017268,
      "grad_norm": 3.8194515705108643,
      "learning_rate": 4.617150706436421e-05,
      "loss": 0.6363,
      "step": 390200
    },
    {
      "epoch": 6.127158555729984,
      "grad_norm": 4.063122272491455,
      "learning_rate": 4.617052590266876e-05,
      "loss": 0.6583,
      "step": 390300
    },
    {
      "epoch": 6.1287284144427,
      "grad_norm": 4.0648417472839355,
      "learning_rate": 4.6169544740973315e-05,
      "loss": 0.6295,
      "step": 390400
    },
    {
      "epoch": 6.130298273155416,
      "grad_norm": 5.902928829193115,
      "learning_rate": 4.6168563579277866e-05,
      "loss": 0.7077,
      "step": 390500
    },
    {
      "epoch": 6.131868131868132,
      "grad_norm": 4.036954402923584,
      "learning_rate": 4.6167582417582424e-05,
      "loss": 0.6371,
      "step": 390600
    },
    {
      "epoch": 6.133437990580847,
      "grad_norm": 3.850349187850952,
      "learning_rate": 4.616660125588697e-05,
      "loss": 0.6522,
      "step": 390700
    },
    {
      "epoch": 6.135007849293563,
      "grad_norm": 4.401029586791992,
      "learning_rate": 4.6165620094191526e-05,
      "loss": 0.6237,
      "step": 390800
    },
    {
      "epoch": 6.136577708006279,
      "grad_norm": 4.423183441162109,
      "learning_rate": 4.616463893249608e-05,
      "loss": 0.671,
      "step": 390900
    },
    {
      "epoch": 6.138147566718995,
      "grad_norm": 4.40330171585083,
      "learning_rate": 4.616365777080063e-05,
      "loss": 0.6764,
      "step": 391000
    },
    {
      "epoch": 6.139717425431711,
      "grad_norm": 4.7165703773498535,
      "learning_rate": 4.616267660910518e-05,
      "loss": 0.6837,
      "step": 391100
    },
    {
      "epoch": 6.141287284144427,
      "grad_norm": 5.296135425567627,
      "learning_rate": 4.616169544740974e-05,
      "loss": 0.6936,
      "step": 391200
    },
    {
      "epoch": 6.142857142857143,
      "grad_norm": 4.62662935256958,
      "learning_rate": 4.616071428571429e-05,
      "loss": 0.6571,
      "step": 391300
    },
    {
      "epoch": 6.144427001569858,
      "grad_norm": 3.8662898540496826,
      "learning_rate": 4.615973312401884e-05,
      "loss": 0.6858,
      "step": 391400
    },
    {
      "epoch": 6.145996860282574,
      "grad_norm": 4.247626781463623,
      "learning_rate": 4.6158751962323396e-05,
      "loss": 0.6676,
      "step": 391500
    },
    {
      "epoch": 6.14756671899529,
      "grad_norm": 3.6978189945220947,
      "learning_rate": 4.615777080062795e-05,
      "loss": 0.6554,
      "step": 391600
    },
    {
      "epoch": 6.149136577708006,
      "grad_norm": 4.772575855255127,
      "learning_rate": 4.61567896389325e-05,
      "loss": 0.6812,
      "step": 391700
    },
    {
      "epoch": 6.150706436420722,
      "grad_norm": 3.0643975734710693,
      "learning_rate": 4.615580847723705e-05,
      "loss": 0.7136,
      "step": 391800
    },
    {
      "epoch": 6.152276295133438,
      "grad_norm": 4.3955841064453125,
      "learning_rate": 4.615482731554161e-05,
      "loss": 0.642,
      "step": 391900
    },
    {
      "epoch": 6.153846153846154,
      "grad_norm": 3.90242075920105,
      "learning_rate": 4.615384615384616e-05,
      "loss": 0.6963,
      "step": 392000
    },
    {
      "epoch": 6.155416012558869,
      "grad_norm": 4.307951927185059,
      "learning_rate": 4.615286499215071e-05,
      "loss": 0.6872,
      "step": 392100
    },
    {
      "epoch": 6.156985871271585,
      "grad_norm": 3.2686803340911865,
      "learning_rate": 4.615188383045526e-05,
      "loss": 0.6849,
      "step": 392200
    },
    {
      "epoch": 6.158555729984301,
      "grad_norm": 3.8355867862701416,
      "learning_rate": 4.615090266875982e-05,
      "loss": 0.6716,
      "step": 392300
    },
    {
      "epoch": 6.160125588697017,
      "grad_norm": 3.5767250061035156,
      "learning_rate": 4.614992150706436e-05,
      "loss": 0.6425,
      "step": 392400
    },
    {
      "epoch": 6.161695447409733,
      "grad_norm": 2.6582272052764893,
      "learning_rate": 4.614894034536892e-05,
      "loss": 0.6572,
      "step": 392500
    },
    {
      "epoch": 6.163265306122449,
      "grad_norm": 3.590242862701416,
      "learning_rate": 4.614795918367347e-05,
      "loss": 0.6361,
      "step": 392600
    },
    {
      "epoch": 6.164835164835165,
      "grad_norm": 3.915847063064575,
      "learning_rate": 4.614697802197803e-05,
      "loss": 0.6362,
      "step": 392700
    },
    {
      "epoch": 6.166405023547881,
      "grad_norm": 4.058078765869141,
      "learning_rate": 4.614599686028257e-05,
      "loss": 0.6613,
      "step": 392800
    },
    {
      "epoch": 6.167974882260596,
      "grad_norm": 3.6854915618896484,
      "learning_rate": 4.614501569858713e-05,
      "loss": 0.6422,
      "step": 392900
    },
    {
      "epoch": 6.169544740973312,
      "grad_norm": 3.6956498622894287,
      "learning_rate": 4.614403453689168e-05,
      "loss": 0.6596,
      "step": 393000
    },
    {
      "epoch": 6.171114599686028,
      "grad_norm": 4.3857502937316895,
      "learning_rate": 4.614305337519623e-05,
      "loss": 0.658,
      "step": 393100
    },
    {
      "epoch": 6.172684458398744,
      "grad_norm": 4.115122318267822,
      "learning_rate": 4.6142072213500783e-05,
      "loss": 0.6885,
      "step": 393200
    },
    {
      "epoch": 6.17425431711146,
      "grad_norm": 4.0746941566467285,
      "learning_rate": 4.614109105180534e-05,
      "loss": 0.6648,
      "step": 393300
    },
    {
      "epoch": 6.175824175824176,
      "grad_norm": 3.49263596534729,
      "learning_rate": 4.614010989010989e-05,
      "loss": 0.6728,
      "step": 393400
    },
    {
      "epoch": 6.177394034536892,
      "grad_norm": 3.2455365657806396,
      "learning_rate": 4.613912872841444e-05,
      "loss": 0.691,
      "step": 393500
    },
    {
      "epoch": 6.178963893249607,
      "grad_norm": 5.512495040893555,
      "learning_rate": 4.6138147566719e-05,
      "loss": 0.6662,
      "step": 393600
    },
    {
      "epoch": 6.180533751962323,
      "grad_norm": 4.8145527839660645,
      "learning_rate": 4.613716640502355e-05,
      "loss": 0.6465,
      "step": 393700
    },
    {
      "epoch": 6.182103610675039,
      "grad_norm": 4.158360481262207,
      "learning_rate": 4.61361852433281e-05,
      "loss": 0.6551,
      "step": 393800
    },
    {
      "epoch": 6.183673469387755,
      "grad_norm": 2.9003264904022217,
      "learning_rate": 4.6135204081632654e-05,
      "loss": 0.6946,
      "step": 393900
    },
    {
      "epoch": 6.185243328100471,
      "grad_norm": 3.6347200870513916,
      "learning_rate": 4.613422291993721e-05,
      "loss": 0.6611,
      "step": 394000
    },
    {
      "epoch": 6.186813186813187,
      "grad_norm": 4.981205463409424,
      "learning_rate": 4.613324175824176e-05,
      "loss": 0.6386,
      "step": 394100
    },
    {
      "epoch": 6.188383045525903,
      "grad_norm": 3.830956220626831,
      "learning_rate": 4.6132260596546314e-05,
      "loss": 0.6546,
      "step": 394200
    },
    {
      "epoch": 6.189952904238618,
      "grad_norm": 5.169563293457031,
      "learning_rate": 4.6131279434850865e-05,
      "loss": 0.669,
      "step": 394300
    },
    {
      "epoch": 6.191522762951334,
      "grad_norm": 3.8800466060638428,
      "learning_rate": 4.613029827315542e-05,
      "loss": 0.6554,
      "step": 394400
    },
    {
      "epoch": 6.19309262166405,
      "grad_norm": 3.5204355716705322,
      "learning_rate": 4.6129317111459967e-05,
      "loss": 0.6408,
      "step": 394500
    },
    {
      "epoch": 6.194662480376766,
      "grad_norm": 3.747825860977173,
      "learning_rate": 4.6128335949764524e-05,
      "loss": 0.6726,
      "step": 394600
    },
    {
      "epoch": 6.196232339089482,
      "grad_norm": 3.647535562515259,
      "learning_rate": 4.6127354788069075e-05,
      "loss": 0.657,
      "step": 394700
    },
    {
      "epoch": 6.197802197802198,
      "grad_norm": 4.29422664642334,
      "learning_rate": 4.612637362637363e-05,
      "loss": 0.6955,
      "step": 394800
    },
    {
      "epoch": 6.199372056514914,
      "grad_norm": 3.175557851791382,
      "learning_rate": 4.612539246467818e-05,
      "loss": 0.6379,
      "step": 394900
    },
    {
      "epoch": 6.20094191522763,
      "grad_norm": 4.320684432983398,
      "learning_rate": 4.6124411302982735e-05,
      "loss": 0.6596,
      "step": 395000
    },
    {
      "epoch": 6.202511773940345,
      "grad_norm": 3.91825270652771,
      "learning_rate": 4.6123430141287286e-05,
      "loss": 0.6707,
      "step": 395100
    },
    {
      "epoch": 6.204081632653061,
      "grad_norm": 4.197769641876221,
      "learning_rate": 4.612244897959184e-05,
      "loss": 0.6233,
      "step": 395200
    },
    {
      "epoch": 6.205651491365777,
      "grad_norm": 3.495575428009033,
      "learning_rate": 4.612146781789639e-05,
      "loss": 0.6637,
      "step": 395300
    },
    {
      "epoch": 6.207221350078493,
      "grad_norm": 2.845810890197754,
      "learning_rate": 4.6120486656200946e-05,
      "loss": 0.5899,
      "step": 395400
    },
    {
      "epoch": 6.208791208791209,
      "grad_norm": 3.8696489334106445,
      "learning_rate": 4.61195054945055e-05,
      "loss": 0.6561,
      "step": 395500
    },
    {
      "epoch": 6.210361067503925,
      "grad_norm": 3.52046537399292,
      "learning_rate": 4.611852433281005e-05,
      "loss": 0.6675,
      "step": 395600
    },
    {
      "epoch": 6.211930926216641,
      "grad_norm": 3.7354180812835693,
      "learning_rate": 4.6117543171114605e-05,
      "loss": 0.6182,
      "step": 395700
    },
    {
      "epoch": 6.213500784929356,
      "grad_norm": 3.168269395828247,
      "learning_rate": 4.6116562009419156e-05,
      "loss": 0.6239,
      "step": 395800
    },
    {
      "epoch": 6.215070643642072,
      "grad_norm": 3.126723051071167,
      "learning_rate": 4.611558084772371e-05,
      "loss": 0.6982,
      "step": 395900
    },
    {
      "epoch": 6.216640502354788,
      "grad_norm": 4.032322883605957,
      "learning_rate": 4.611459968602826e-05,
      "loss": 0.6736,
      "step": 396000
    },
    {
      "epoch": 6.218210361067504,
      "grad_norm": 4.670474052429199,
      "learning_rate": 4.6113618524332816e-05,
      "loss": 0.6719,
      "step": 396100
    },
    {
      "epoch": 6.21978021978022,
      "grad_norm": 4.292887210845947,
      "learning_rate": 4.611263736263737e-05,
      "loss": 0.6804,
      "step": 396200
    },
    {
      "epoch": 6.221350078492936,
      "grad_norm": 4.895878791809082,
      "learning_rate": 4.611165620094192e-05,
      "loss": 0.5932,
      "step": 396300
    },
    {
      "epoch": 6.222919937205652,
      "grad_norm": 5.1397013664245605,
      "learning_rate": 4.611067503924647e-05,
      "loss": 0.6829,
      "step": 396400
    },
    {
      "epoch": 6.224489795918367,
      "grad_norm": 3.7655820846557617,
      "learning_rate": 4.610969387755103e-05,
      "loss": 0.6514,
      "step": 396500
    },
    {
      "epoch": 6.226059654631083,
      "grad_norm": 3.5593180656433105,
      "learning_rate": 4.610871271585557e-05,
      "loss": 0.704,
      "step": 396600
    },
    {
      "epoch": 6.227629513343799,
      "grad_norm": 3.44907808303833,
      "learning_rate": 4.610773155416013e-05,
      "loss": 0.6073,
      "step": 396700
    },
    {
      "epoch": 6.229199372056515,
      "grad_norm": 4.882983207702637,
      "learning_rate": 4.610675039246468e-05,
      "loss": 0.6961,
      "step": 396800
    },
    {
      "epoch": 6.230769230769231,
      "grad_norm": 3.7932333946228027,
      "learning_rate": 4.610576923076924e-05,
      "loss": 0.6407,
      "step": 396900
    },
    {
      "epoch": 6.232339089481947,
      "grad_norm": 3.4371018409729004,
      "learning_rate": 4.610478806907378e-05,
      "loss": 0.6633,
      "step": 397000
    },
    {
      "epoch": 6.233908948194663,
      "grad_norm": 4.0678815841674805,
      "learning_rate": 4.610380690737834e-05,
      "loss": 0.655,
      "step": 397100
    },
    {
      "epoch": 6.235478806907379,
      "grad_norm": 4.31052827835083,
      "learning_rate": 4.610282574568289e-05,
      "loss": 0.6825,
      "step": 397200
    },
    {
      "epoch": 6.237048665620094,
      "grad_norm": 3.1597886085510254,
      "learning_rate": 4.610184458398744e-05,
      "loss": 0.6836,
      "step": 397300
    },
    {
      "epoch": 6.23861852433281,
      "grad_norm": 3.67272686958313,
      "learning_rate": 4.610086342229199e-05,
      "loss": 0.6287,
      "step": 397400
    },
    {
      "epoch": 6.240188383045526,
      "grad_norm": 3.6125333309173584,
      "learning_rate": 4.609988226059655e-05,
      "loss": 0.647,
      "step": 397500
    },
    {
      "epoch": 6.241758241758242,
      "grad_norm": 2.799238681793213,
      "learning_rate": 4.60989010989011e-05,
      "loss": 0.6301,
      "step": 397600
    },
    {
      "epoch": 6.243328100470958,
      "grad_norm": 3.629086494445801,
      "learning_rate": 4.609791993720565e-05,
      "loss": 0.6737,
      "step": 397700
    },
    {
      "epoch": 6.244897959183674,
      "grad_norm": 2.840510368347168,
      "learning_rate": 4.609693877551021e-05,
      "loss": 0.6612,
      "step": 397800
    },
    {
      "epoch": 6.24646781789639,
      "grad_norm": 3.395369052886963,
      "learning_rate": 4.609595761381476e-05,
      "loss": 0.6777,
      "step": 397900
    },
    {
      "epoch": 6.248037676609105,
      "grad_norm": 3.3162007331848145,
      "learning_rate": 4.609497645211931e-05,
      "loss": 0.6624,
      "step": 398000
    },
    {
      "epoch": 6.249607535321821,
      "grad_norm": 4.117309093475342,
      "learning_rate": 4.609399529042386e-05,
      "loss": 0.6465,
      "step": 398100
    },
    {
      "epoch": 6.251177394034537,
      "grad_norm": 4.531508922576904,
      "learning_rate": 4.609301412872842e-05,
      "loss": 0.6616,
      "step": 398200
    },
    {
      "epoch": 6.252747252747253,
      "grad_norm": 4.070562839508057,
      "learning_rate": 4.6092032967032965e-05,
      "loss": 0.6533,
      "step": 398300
    },
    {
      "epoch": 6.254317111459969,
      "grad_norm": 4.60153865814209,
      "learning_rate": 4.609105180533752e-05,
      "loss": 0.6338,
      "step": 398400
    },
    {
      "epoch": 6.255886970172685,
      "grad_norm": 3.409501552581787,
      "learning_rate": 4.6090070643642074e-05,
      "loss": 0.6618,
      "step": 398500
    },
    {
      "epoch": 6.257456828885401,
      "grad_norm": 4.827802658081055,
      "learning_rate": 4.608908948194663e-05,
      "loss": 0.6954,
      "step": 398600
    },
    {
      "epoch": 6.259026687598116,
      "grad_norm": 3.504678249359131,
      "learning_rate": 4.6088108320251176e-05,
      "loss": 0.6566,
      "step": 398700
    },
    {
      "epoch": 6.260596546310832,
      "grad_norm": 4.299414157867432,
      "learning_rate": 4.608712715855573e-05,
      "loss": 0.6821,
      "step": 398800
    },
    {
      "epoch": 6.262166405023548,
      "grad_norm": 3.6230015754699707,
      "learning_rate": 4.6086145996860284e-05,
      "loss": 0.6635,
      "step": 398900
    },
    {
      "epoch": 6.263736263736264,
      "grad_norm": 3.9049971103668213,
      "learning_rate": 4.6085164835164835e-05,
      "loss": 0.6718,
      "step": 399000
    },
    {
      "epoch": 6.26530612244898,
      "grad_norm": 4.8763861656188965,
      "learning_rate": 4.6084183673469386e-05,
      "loss": 0.6992,
      "step": 399100
    },
    {
      "epoch": 6.266875981161696,
      "grad_norm": 3.7426908016204834,
      "learning_rate": 4.6083202511773944e-05,
      "loss": 0.6654,
      "step": 399200
    },
    {
      "epoch": 6.268445839874412,
      "grad_norm": 4.4401936531066895,
      "learning_rate": 4.6082221350078495e-05,
      "loss": 0.6402,
      "step": 399300
    },
    {
      "epoch": 6.270015698587127,
      "grad_norm": 4.300060749053955,
      "learning_rate": 4.6081240188383046e-05,
      "loss": 0.645,
      "step": 399400
    },
    {
      "epoch": 6.271585557299843,
      "grad_norm": 4.123828887939453,
      "learning_rate": 4.60802590266876e-05,
      "loss": 0.6679,
      "step": 399500
    },
    {
      "epoch": 6.273155416012559,
      "grad_norm": 2.2068662643432617,
      "learning_rate": 4.6079277864992155e-05,
      "loss": 0.5964,
      "step": 399600
    },
    {
      "epoch": 6.274725274725275,
      "grad_norm": 3.78558611869812,
      "learning_rate": 4.6078296703296706e-05,
      "loss": 0.6619,
      "step": 399700
    },
    {
      "epoch": 6.276295133437991,
      "grad_norm": 2.972139358520508,
      "learning_rate": 4.607731554160126e-05,
      "loss": 0.6433,
      "step": 399800
    },
    {
      "epoch": 6.277864992150707,
      "grad_norm": 2.867070436477661,
      "learning_rate": 4.6076334379905814e-05,
      "loss": 0.6729,
      "step": 399900
    },
    {
      "epoch": 6.279434850863423,
      "grad_norm": 4.032867431640625,
      "learning_rate": 4.6075353218210365e-05,
      "loss": 0.6274,
      "step": 400000
    },
    {
      "epoch": 6.2810047095761385,
      "grad_norm": 3.6343114376068115,
      "learning_rate": 4.6074372056514916e-05,
      "loss": 0.6696,
      "step": 400100
    },
    {
      "epoch": 6.282574568288854,
      "grad_norm": 3.8901734352111816,
      "learning_rate": 4.607339089481947e-05,
      "loss": 0.6628,
      "step": 400200
    },
    {
      "epoch": 6.28414442700157,
      "grad_norm": 4.122868537902832,
      "learning_rate": 4.6072409733124025e-05,
      "loss": 0.6402,
      "step": 400300
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 4.396737575531006,
      "learning_rate": 4.607142857142857e-05,
      "loss": 0.634,
      "step": 400400
    },
    {
      "epoch": 6.287284144427002,
      "grad_norm": 4.031399250030518,
      "learning_rate": 4.607044740973313e-05,
      "loss": 0.6529,
      "step": 400500
    },
    {
      "epoch": 6.2888540031397175,
      "grad_norm": 3.7941553592681885,
      "learning_rate": 4.606946624803768e-05,
      "loss": 0.6577,
      "step": 400600
    },
    {
      "epoch": 6.2904238618524335,
      "grad_norm": 3.343022108078003,
      "learning_rate": 4.6068485086342236e-05,
      "loss": 0.674,
      "step": 400700
    },
    {
      "epoch": 6.2919937205651495,
      "grad_norm": 3.0518078804016113,
      "learning_rate": 4.606750392464678e-05,
      "loss": 0.6597,
      "step": 400800
    },
    {
      "epoch": 6.293563579277865,
      "grad_norm": 4.8988165855407715,
      "learning_rate": 4.606652276295134e-05,
      "loss": 0.6384,
      "step": 400900
    },
    {
      "epoch": 6.295133437990581,
      "grad_norm": 3.2389581203460693,
      "learning_rate": 4.606554160125589e-05,
      "loss": 0.6764,
      "step": 401000
    },
    {
      "epoch": 6.2967032967032965,
      "grad_norm": 3.668760061264038,
      "learning_rate": 4.606456043956044e-05,
      "loss": 0.6684,
      "step": 401100
    },
    {
      "epoch": 6.2982731554160125,
      "grad_norm": 3.988178014755249,
      "learning_rate": 4.606357927786499e-05,
      "loss": 0.6672,
      "step": 401200
    },
    {
      "epoch": 6.2998430141287285,
      "grad_norm": 4.420367240905762,
      "learning_rate": 4.606259811616955e-05,
      "loss": 0.6254,
      "step": 401300
    },
    {
      "epoch": 6.3014128728414445,
      "grad_norm": 5.635040283203125,
      "learning_rate": 4.60616169544741e-05,
      "loss": 0.6404,
      "step": 401400
    },
    {
      "epoch": 6.3029827315541604,
      "grad_norm": 3.858966112136841,
      "learning_rate": 4.606063579277865e-05,
      "loss": 0.6919,
      "step": 401500
    },
    {
      "epoch": 6.304552590266876,
      "grad_norm": 4.219160556793213,
      "learning_rate": 4.60596546310832e-05,
      "loss": 0.6693,
      "step": 401600
    },
    {
      "epoch": 6.3061224489795915,
      "grad_norm": 3.816847801208496,
      "learning_rate": 4.605867346938776e-05,
      "loss": 0.6811,
      "step": 401700
    },
    {
      "epoch": 6.3076923076923075,
      "grad_norm": 3.5989973545074463,
      "learning_rate": 4.605769230769231e-05,
      "loss": 0.6759,
      "step": 401800
    },
    {
      "epoch": 6.3092621664050235,
      "grad_norm": 3.7597851753234863,
      "learning_rate": 4.605671114599686e-05,
      "loss": 0.6281,
      "step": 401900
    },
    {
      "epoch": 6.310832025117739,
      "grad_norm": 3.1625800132751465,
      "learning_rate": 4.605572998430142e-05,
      "loss": 0.6423,
      "step": 402000
    },
    {
      "epoch": 6.312401883830455,
      "grad_norm": 4.231344699859619,
      "learning_rate": 4.605474882260597e-05,
      "loss": 0.6581,
      "step": 402100
    },
    {
      "epoch": 6.313971742543171,
      "grad_norm": 4.451351642608643,
      "learning_rate": 4.605376766091052e-05,
      "loss": 0.6736,
      "step": 402200
    },
    {
      "epoch": 6.315541601255887,
      "grad_norm": 3.671314001083374,
      "learning_rate": 4.605278649921507e-05,
      "loss": 0.6663,
      "step": 402300
    },
    {
      "epoch": 6.3171114599686025,
      "grad_norm": 3.0850892066955566,
      "learning_rate": 4.605180533751963e-05,
      "loss": 0.665,
      "step": 402400
    },
    {
      "epoch": 6.318681318681318,
      "grad_norm": 3.3119473457336426,
      "learning_rate": 4.6050824175824174e-05,
      "loss": 0.6262,
      "step": 402500
    },
    {
      "epoch": 6.320251177394034,
      "grad_norm": 3.8985564708709717,
      "learning_rate": 4.604984301412873e-05,
      "loss": 0.6456,
      "step": 402600
    },
    {
      "epoch": 6.32182103610675,
      "grad_norm": 3.931881904602051,
      "learning_rate": 4.604886185243328e-05,
      "loss": 0.6456,
      "step": 402700
    },
    {
      "epoch": 6.323390894819466,
      "grad_norm": 3.037724256515503,
      "learning_rate": 4.604788069073784e-05,
      "loss": 0.6186,
      "step": 402800
    },
    {
      "epoch": 6.324960753532182,
      "grad_norm": 4.845249176025391,
      "learning_rate": 4.6046899529042385e-05,
      "loss": 0.6674,
      "step": 402900
    },
    {
      "epoch": 6.326530612244898,
      "grad_norm": 3.765716791152954,
      "learning_rate": 4.604591836734694e-05,
      "loss": 0.6564,
      "step": 403000
    },
    {
      "epoch": 6.328100470957613,
      "grad_norm": 4.77306604385376,
      "learning_rate": 4.604493720565149e-05,
      "loss": 0.6339,
      "step": 403100
    },
    {
      "epoch": 6.329670329670329,
      "grad_norm": 3.925431728363037,
      "learning_rate": 4.6043956043956044e-05,
      "loss": 0.6744,
      "step": 403200
    },
    {
      "epoch": 6.331240188383045,
      "grad_norm": 3.8704535961151123,
      "learning_rate": 4.6042974882260595e-05,
      "loss": 0.6699,
      "step": 403300
    },
    {
      "epoch": 6.332810047095761,
      "grad_norm": 4.026076793670654,
      "learning_rate": 4.604199372056515e-05,
      "loss": 0.6491,
      "step": 403400
    },
    {
      "epoch": 6.334379905808477,
      "grad_norm": 4.2379841804504395,
      "learning_rate": 4.6041012558869704e-05,
      "loss": 0.6702,
      "step": 403500
    },
    {
      "epoch": 6.335949764521193,
      "grad_norm": 4.062709808349609,
      "learning_rate": 4.6040031397174255e-05,
      "loss": 0.6707,
      "step": 403600
    },
    {
      "epoch": 6.337519623233909,
      "grad_norm": 4.092155933380127,
      "learning_rate": 4.6039050235478806e-05,
      "loss": 0.6487,
      "step": 403700
    },
    {
      "epoch": 6.339089481946624,
      "grad_norm": 3.6414074897766113,
      "learning_rate": 4.6038069073783364e-05,
      "loss": 0.6915,
      "step": 403800
    },
    {
      "epoch": 6.34065934065934,
      "grad_norm": 4.107612133026123,
      "learning_rate": 4.6037087912087915e-05,
      "loss": 0.6973,
      "step": 403900
    },
    {
      "epoch": 6.342229199372056,
      "grad_norm": 4.771397113800049,
      "learning_rate": 4.6036106750392466e-05,
      "loss": 0.6803,
      "step": 404000
    },
    {
      "epoch": 6.343799058084772,
      "grad_norm": 1.9372316598892212,
      "learning_rate": 4.6035125588697023e-05,
      "loss": 0.6867,
      "step": 404100
    },
    {
      "epoch": 6.345368916797488,
      "grad_norm": 3.6564412117004395,
      "learning_rate": 4.6034144427001574e-05,
      "loss": 0.6047,
      "step": 404200
    },
    {
      "epoch": 6.346938775510204,
      "grad_norm": 2.77217960357666,
      "learning_rate": 4.6033163265306125e-05,
      "loss": 0.6651,
      "step": 404300
    },
    {
      "epoch": 6.34850863422292,
      "grad_norm": 4.977011680603027,
      "learning_rate": 4.6032182103610676e-05,
      "loss": 0.6489,
      "step": 404400
    },
    {
      "epoch": 6.350078492935636,
      "grad_norm": 3.8944027423858643,
      "learning_rate": 4.6031200941915234e-05,
      "loss": 0.6554,
      "step": 404500
    },
    {
      "epoch": 6.351648351648351,
      "grad_norm": 4.140940189361572,
      "learning_rate": 4.603021978021978e-05,
      "loss": 0.6691,
      "step": 404600
    },
    {
      "epoch": 6.353218210361067,
      "grad_norm": 3.7053704261779785,
      "learning_rate": 4.6029238618524336e-05,
      "loss": 0.6628,
      "step": 404700
    },
    {
      "epoch": 6.354788069073783,
      "grad_norm": 3.931229829788208,
      "learning_rate": 4.602825745682889e-05,
      "loss": 0.6692,
      "step": 404800
    },
    {
      "epoch": 6.356357927786499,
      "grad_norm": 4.072385311126709,
      "learning_rate": 4.6027276295133445e-05,
      "loss": 0.6624,
      "step": 404900
    },
    {
      "epoch": 6.357927786499215,
      "grad_norm": 4.165884494781494,
      "learning_rate": 4.602629513343799e-05,
      "loss": 0.6663,
      "step": 405000
    },
    {
      "epoch": 6.359497645211931,
      "grad_norm": 3.3730785846710205,
      "learning_rate": 4.602531397174255e-05,
      "loss": 0.6828,
      "step": 405100
    },
    {
      "epoch": 6.361067503924647,
      "grad_norm": 4.128364562988281,
      "learning_rate": 4.60243328100471e-05,
      "loss": 0.6708,
      "step": 405200
    },
    {
      "epoch": 6.362637362637362,
      "grad_norm": 3.6233744621276855,
      "learning_rate": 4.602335164835165e-05,
      "loss": 0.6343,
      "step": 405300
    },
    {
      "epoch": 6.364207221350078,
      "grad_norm": 3.23775577545166,
      "learning_rate": 4.60223704866562e-05,
      "loss": 0.6858,
      "step": 405400
    },
    {
      "epoch": 6.365777080062794,
      "grad_norm": 3.181443691253662,
      "learning_rate": 4.602138932496076e-05,
      "loss": 0.6659,
      "step": 405500
    },
    {
      "epoch": 6.36734693877551,
      "grad_norm": 3.9684760570526123,
      "learning_rate": 4.602040816326531e-05,
      "loss": 0.6689,
      "step": 405600
    },
    {
      "epoch": 6.368916797488226,
      "grad_norm": 5.733455657958984,
      "learning_rate": 4.601942700156986e-05,
      "loss": 0.6478,
      "step": 405700
    },
    {
      "epoch": 6.370486656200942,
      "grad_norm": 3.023944854736328,
      "learning_rate": 4.601844583987441e-05,
      "loss": 0.634,
      "step": 405800
    },
    {
      "epoch": 6.372056514913658,
      "grad_norm": 4.085328102111816,
      "learning_rate": 4.601746467817897e-05,
      "loss": 0.6664,
      "step": 405900
    },
    {
      "epoch": 6.373626373626374,
      "grad_norm": 5.41039514541626,
      "learning_rate": 4.601648351648352e-05,
      "loss": 0.6588,
      "step": 406000
    },
    {
      "epoch": 6.375196232339089,
      "grad_norm": 4.524205684661865,
      "learning_rate": 4.601550235478807e-05,
      "loss": 0.6547,
      "step": 406100
    },
    {
      "epoch": 6.376766091051805,
      "grad_norm": 3.3255224227905273,
      "learning_rate": 4.601452119309263e-05,
      "loss": 0.6714,
      "step": 406200
    },
    {
      "epoch": 6.378335949764521,
      "grad_norm": 4.22290563583374,
      "learning_rate": 4.601354003139718e-05,
      "loss": 0.6552,
      "step": 406300
    },
    {
      "epoch": 6.379905808477237,
      "grad_norm": 3.4978854656219482,
      "learning_rate": 4.601255886970173e-05,
      "loss": 0.6873,
      "step": 406400
    },
    {
      "epoch": 6.381475667189953,
      "grad_norm": 5.09364128112793,
      "learning_rate": 4.601157770800628e-05,
      "loss": 0.6709,
      "step": 406500
    },
    {
      "epoch": 6.383045525902669,
      "grad_norm": 4.772282123565674,
      "learning_rate": 4.601059654631084e-05,
      "loss": 0.6934,
      "step": 406600
    },
    {
      "epoch": 6.384615384615385,
      "grad_norm": 4.364409446716309,
      "learning_rate": 4.600961538461538e-05,
      "loss": 0.6573,
      "step": 406700
    },
    {
      "epoch": 6.3861852433281,
      "grad_norm": 4.080367088317871,
      "learning_rate": 4.600863422291994e-05,
      "loss": 0.6967,
      "step": 406800
    },
    {
      "epoch": 6.387755102040816,
      "grad_norm": 3.9387903213500977,
      "learning_rate": 4.600765306122449e-05,
      "loss": 0.687,
      "step": 406900
    },
    {
      "epoch": 6.389324960753532,
      "grad_norm": 4.315220832824707,
      "learning_rate": 4.600667189952905e-05,
      "loss": 0.6589,
      "step": 407000
    },
    {
      "epoch": 6.390894819466248,
      "grad_norm": 4.1642866134643555,
      "learning_rate": 4.6005690737833594e-05,
      "loss": 0.6724,
      "step": 407100
    },
    {
      "epoch": 6.392464678178964,
      "grad_norm": 4.871920108795166,
      "learning_rate": 4.600470957613815e-05,
      "loss": 0.6715,
      "step": 407200
    },
    {
      "epoch": 6.39403453689168,
      "grad_norm": 3.5306291580200195,
      "learning_rate": 4.60037284144427e-05,
      "loss": 0.688,
      "step": 407300
    },
    {
      "epoch": 6.395604395604396,
      "grad_norm": 3.9560906887054443,
      "learning_rate": 4.600274725274725e-05,
      "loss": 0.6414,
      "step": 407400
    },
    {
      "epoch": 6.397174254317111,
      "grad_norm": 4.532093048095703,
      "learning_rate": 4.6001766091051804e-05,
      "loss": 0.67,
      "step": 407500
    },
    {
      "epoch": 6.398744113029827,
      "grad_norm": 3.9816601276397705,
      "learning_rate": 4.600078492935636e-05,
      "loss": 0.654,
      "step": 407600
    },
    {
      "epoch": 6.400313971742543,
      "grad_norm": 4.671164512634277,
      "learning_rate": 4.599980376766091e-05,
      "loss": 0.6509,
      "step": 407700
    },
    {
      "epoch": 6.401883830455259,
      "grad_norm": 4.850506782531738,
      "learning_rate": 4.5998822605965464e-05,
      "loss": 0.6495,
      "step": 407800
    },
    {
      "epoch": 6.403453689167975,
      "grad_norm": 3.6252591609954834,
      "learning_rate": 4.5997841444270015e-05,
      "loss": 0.6726,
      "step": 407900
    },
    {
      "epoch": 6.405023547880691,
      "grad_norm": 4.689214706420898,
      "learning_rate": 4.599686028257457e-05,
      "loss": 0.6814,
      "step": 408000
    },
    {
      "epoch": 6.406593406593407,
      "grad_norm": 3.2930495738983154,
      "learning_rate": 4.5995879120879124e-05,
      "loss": 0.6973,
      "step": 408100
    },
    {
      "epoch": 6.408163265306122,
      "grad_norm": 3.2934858798980713,
      "learning_rate": 4.5994897959183675e-05,
      "loss": 0.7058,
      "step": 408200
    },
    {
      "epoch": 6.409733124018838,
      "grad_norm": 3.556320905685425,
      "learning_rate": 4.599391679748823e-05,
      "loss": 0.6563,
      "step": 408300
    },
    {
      "epoch": 6.411302982731554,
      "grad_norm": 4.291059494018555,
      "learning_rate": 4.599293563579278e-05,
      "loss": 0.6815,
      "step": 408400
    },
    {
      "epoch": 6.41287284144427,
      "grad_norm": 3.6324727535247803,
      "learning_rate": 4.5991954474097334e-05,
      "loss": 0.6511,
      "step": 408500
    },
    {
      "epoch": 6.414442700156986,
      "grad_norm": 4.378828048706055,
      "learning_rate": 4.5990973312401885e-05,
      "loss": 0.6359,
      "step": 408600
    },
    {
      "epoch": 6.416012558869702,
      "grad_norm": 3.9320549964904785,
      "learning_rate": 4.598999215070644e-05,
      "loss": 0.6417,
      "step": 408700
    },
    {
      "epoch": 6.417582417582418,
      "grad_norm": 3.596087694168091,
      "learning_rate": 4.598901098901099e-05,
      "loss": 0.6255,
      "step": 408800
    },
    {
      "epoch": 6.419152276295134,
      "grad_norm": 2.889838695526123,
      "learning_rate": 4.5988029827315545e-05,
      "loss": 0.6538,
      "step": 408900
    },
    {
      "epoch": 6.420722135007849,
      "grad_norm": 4.277511119842529,
      "learning_rate": 4.5987048665620096e-05,
      "loss": 0.6274,
      "step": 409000
    },
    {
      "epoch": 6.422291993720565,
      "grad_norm": 3.961832046508789,
      "learning_rate": 4.5986067503924654e-05,
      "loss": 0.6813,
      "step": 409100
    },
    {
      "epoch": 6.423861852433281,
      "grad_norm": 3.1675727367401123,
      "learning_rate": 4.59850863422292e-05,
      "loss": 0.6765,
      "step": 409200
    },
    {
      "epoch": 6.425431711145997,
      "grad_norm": 4.106078624725342,
      "learning_rate": 4.5984105180533756e-05,
      "loss": 0.6855,
      "step": 409300
    },
    {
      "epoch": 6.427001569858713,
      "grad_norm": 4.15324068069458,
      "learning_rate": 4.598312401883831e-05,
      "loss": 0.6757,
      "step": 409400
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 3.387913227081299,
      "learning_rate": 4.598214285714286e-05,
      "loss": 0.656,
      "step": 409500
    },
    {
      "epoch": 6.430141287284145,
      "grad_norm": 2.3119330406188965,
      "learning_rate": 4.598116169544741e-05,
      "loss": 0.6765,
      "step": 409600
    },
    {
      "epoch": 6.43171114599686,
      "grad_norm": 4.327392101287842,
      "learning_rate": 4.5980180533751966e-05,
      "loss": 0.6262,
      "step": 409700
    },
    {
      "epoch": 6.433281004709576,
      "grad_norm": 4.693124771118164,
      "learning_rate": 4.597919937205652e-05,
      "loss": 0.6529,
      "step": 409800
    },
    {
      "epoch": 6.434850863422292,
      "grad_norm": 3.595773696899414,
      "learning_rate": 4.597821821036107e-05,
      "loss": 0.6486,
      "step": 409900
    },
    {
      "epoch": 6.436420722135008,
      "grad_norm": 3.5381920337677,
      "learning_rate": 4.597723704866562e-05,
      "loss": 0.6627,
      "step": 410000
    },
    {
      "epoch": 6.437990580847724,
      "grad_norm": 3.298837661743164,
      "learning_rate": 4.597625588697018e-05,
      "loss": 0.6481,
      "step": 410100
    },
    {
      "epoch": 6.43956043956044,
      "grad_norm": 4.825660228729248,
      "learning_rate": 4.597527472527473e-05,
      "loss": 0.6504,
      "step": 410200
    },
    {
      "epoch": 6.441130298273156,
      "grad_norm": 4.411442279815674,
      "learning_rate": 4.597429356357928e-05,
      "loss": 0.6884,
      "step": 410300
    },
    {
      "epoch": 6.442700156985872,
      "grad_norm": 3.102088689804077,
      "learning_rate": 4.597331240188383e-05,
      "loss": 0.6639,
      "step": 410400
    },
    {
      "epoch": 6.444270015698587,
      "grad_norm": 3.426974296569824,
      "learning_rate": 4.597233124018839e-05,
      "loss": 0.6409,
      "step": 410500
    },
    {
      "epoch": 6.445839874411303,
      "grad_norm": 3.218946695327759,
      "learning_rate": 4.597135007849294e-05,
      "loss": 0.6464,
      "step": 410600
    },
    {
      "epoch": 6.447409733124019,
      "grad_norm": 6.024935245513916,
      "learning_rate": 4.597036891679749e-05,
      "loss": 0.6506,
      "step": 410700
    },
    {
      "epoch": 6.448979591836735,
      "grad_norm": 3.168421745300293,
      "learning_rate": 4.596938775510205e-05,
      "loss": 0.6878,
      "step": 410800
    },
    {
      "epoch": 6.450549450549451,
      "grad_norm": 4.106710433959961,
      "learning_rate": 4.596840659340659e-05,
      "loss": 0.6677,
      "step": 410900
    },
    {
      "epoch": 6.452119309262167,
      "grad_norm": 3.6787781715393066,
      "learning_rate": 4.596742543171115e-05,
      "loss": 0.6146,
      "step": 411000
    },
    {
      "epoch": 6.453689167974883,
      "grad_norm": 3.331718921661377,
      "learning_rate": 4.59664442700157e-05,
      "loss": 0.6711,
      "step": 411100
    },
    {
      "epoch": 6.455259026687598,
      "grad_norm": 3.76495099067688,
      "learning_rate": 4.596546310832026e-05,
      "loss": 0.6961,
      "step": 411200
    },
    {
      "epoch": 6.456828885400314,
      "grad_norm": 3.25457763671875,
      "learning_rate": 4.59644819466248e-05,
      "loss": 0.6502,
      "step": 411300
    },
    {
      "epoch": 6.45839874411303,
      "grad_norm": 4.9871063232421875,
      "learning_rate": 4.596350078492936e-05,
      "loss": 0.664,
      "step": 411400
    },
    {
      "epoch": 6.459968602825746,
      "grad_norm": 3.8176393508911133,
      "learning_rate": 4.596251962323391e-05,
      "loss": 0.6627,
      "step": 411500
    },
    {
      "epoch": 6.461538461538462,
      "grad_norm": 4.756746292114258,
      "learning_rate": 4.596153846153846e-05,
      "loss": 0.641,
      "step": 411600
    },
    {
      "epoch": 6.463108320251178,
      "grad_norm": 4.198232173919678,
      "learning_rate": 4.596055729984301e-05,
      "loss": 0.6736,
      "step": 411700
    },
    {
      "epoch": 6.464678178963894,
      "grad_norm": 4.733141899108887,
      "learning_rate": 4.595957613814757e-05,
      "loss": 0.6457,
      "step": 411800
    },
    {
      "epoch": 6.466248037676609,
      "grad_norm": 3.103459119796753,
      "learning_rate": 4.595859497645212e-05,
      "loss": 0.6381,
      "step": 411900
    },
    {
      "epoch": 6.467817896389325,
      "grad_norm": 3.1152758598327637,
      "learning_rate": 4.595761381475667e-05,
      "loss": 0.6394,
      "step": 412000
    },
    {
      "epoch": 6.469387755102041,
      "grad_norm": 4.9845194816589355,
      "learning_rate": 4.5956632653061224e-05,
      "loss": 0.6746,
      "step": 412100
    },
    {
      "epoch": 6.470957613814757,
      "grad_norm": 3.9237170219421387,
      "learning_rate": 4.595565149136578e-05,
      "loss": 0.695,
      "step": 412200
    },
    {
      "epoch": 6.472527472527473,
      "grad_norm": 4.477606773376465,
      "learning_rate": 4.595467032967033e-05,
      "loss": 0.6351,
      "step": 412300
    },
    {
      "epoch": 6.474097331240189,
      "grad_norm": 3.1128180027008057,
      "learning_rate": 4.5953689167974884e-05,
      "loss": 0.6439,
      "step": 412400
    },
    {
      "epoch": 6.475667189952905,
      "grad_norm": 4.069555282592773,
      "learning_rate": 4.5952708006279435e-05,
      "loss": 0.6751,
      "step": 412500
    },
    {
      "epoch": 6.47723704866562,
      "grad_norm": 3.776172399520874,
      "learning_rate": 4.595172684458399e-05,
      "loss": 0.6474,
      "step": 412600
    },
    {
      "epoch": 6.478806907378336,
      "grad_norm": 3.507148504257202,
      "learning_rate": 4.595074568288854e-05,
      "loss": 0.6463,
      "step": 412700
    },
    {
      "epoch": 6.480376766091052,
      "grad_norm": 3.026341199874878,
      "learning_rate": 4.5949764521193094e-05,
      "loss": 0.6414,
      "step": 412800
    },
    {
      "epoch": 6.481946624803768,
      "grad_norm": 3.1654083728790283,
      "learning_rate": 4.594878335949765e-05,
      "loss": 0.6382,
      "step": 412900
    },
    {
      "epoch": 6.483516483516484,
      "grad_norm": 4.6379780769348145,
      "learning_rate": 4.5947802197802196e-05,
      "loss": 0.6864,
      "step": 413000
    },
    {
      "epoch": 6.4850863422291996,
      "grad_norm": 4.487007141113281,
      "learning_rate": 4.5946821036106754e-05,
      "loss": 0.6305,
      "step": 413100
    },
    {
      "epoch": 6.4866562009419155,
      "grad_norm": 4.560346603393555,
      "learning_rate": 4.5945839874411305e-05,
      "loss": 0.6166,
      "step": 413200
    },
    {
      "epoch": 6.488226059654631,
      "grad_norm": 4.876959323883057,
      "learning_rate": 4.594485871271586e-05,
      "loss": 0.6786,
      "step": 413300
    },
    {
      "epoch": 6.489795918367347,
      "grad_norm": 4.436014652252197,
      "learning_rate": 4.594387755102041e-05,
      "loss": 0.6438,
      "step": 413400
    },
    {
      "epoch": 6.491365777080063,
      "grad_norm": 3.295520305633545,
      "learning_rate": 4.5942896389324965e-05,
      "loss": 0.6884,
      "step": 413500
    },
    {
      "epoch": 6.4929356357927785,
      "grad_norm": 3.3996286392211914,
      "learning_rate": 4.5941915227629516e-05,
      "loss": 0.6844,
      "step": 413600
    },
    {
      "epoch": 6.4945054945054945,
      "grad_norm": 3.568183183670044,
      "learning_rate": 4.594093406593407e-05,
      "loss": 0.594,
      "step": 413700
    },
    {
      "epoch": 6.4960753532182105,
      "grad_norm": 3.962428569793701,
      "learning_rate": 4.593995290423862e-05,
      "loss": 0.6973,
      "step": 413800
    },
    {
      "epoch": 6.4976452119309265,
      "grad_norm": 3.861335515975952,
      "learning_rate": 4.5938971742543175e-05,
      "loss": 0.6652,
      "step": 413900
    },
    {
      "epoch": 6.4992150706436425,
      "grad_norm": 4.007878303527832,
      "learning_rate": 4.5937990580847726e-05,
      "loss": 0.6339,
      "step": 414000
    },
    {
      "epoch": 6.5007849293563575,
      "grad_norm": 4.3895440101623535,
      "learning_rate": 4.593700941915228e-05,
      "loss": 0.6743,
      "step": 414100
    },
    {
      "epoch": 6.5023547880690735,
      "grad_norm": 4.199695587158203,
      "learning_rate": 4.593602825745683e-05,
      "loss": 0.6696,
      "step": 414200
    },
    {
      "epoch": 6.5039246467817895,
      "grad_norm": 3.367414712905884,
      "learning_rate": 4.5935047095761386e-05,
      "loss": 0.6214,
      "step": 414300
    },
    {
      "epoch": 6.5054945054945055,
      "grad_norm": 2.0535728931427,
      "learning_rate": 4.593406593406594e-05,
      "loss": 0.6606,
      "step": 414400
    },
    {
      "epoch": 6.5070643642072215,
      "grad_norm": 3.680453062057495,
      "learning_rate": 4.593308477237049e-05,
      "loss": 0.6855,
      "step": 414500
    },
    {
      "epoch": 6.508634222919937,
      "grad_norm": 4.249226093292236,
      "learning_rate": 4.593210361067504e-05,
      "loss": 0.6764,
      "step": 414600
    },
    {
      "epoch": 6.510204081632653,
      "grad_norm": 3.3060827255249023,
      "learning_rate": 4.59311224489796e-05,
      "loss": 0.6271,
      "step": 414700
    },
    {
      "epoch": 6.511773940345369,
      "grad_norm": 4.757967948913574,
      "learning_rate": 4.593014128728415e-05,
      "loss": 0.6298,
      "step": 414800
    },
    {
      "epoch": 6.5133437990580845,
      "grad_norm": 2.5183629989624023,
      "learning_rate": 4.59291601255887e-05,
      "loss": 0.6842,
      "step": 414900
    },
    {
      "epoch": 6.5149136577708004,
      "grad_norm": 3.4924886226654053,
      "learning_rate": 4.5928178963893257e-05,
      "loss": 0.6348,
      "step": 415000
    },
    {
      "epoch": 6.516483516483516,
      "grad_norm": 5.277151107788086,
      "learning_rate": 4.59271978021978e-05,
      "loss": 0.6884,
      "step": 415100
    },
    {
      "epoch": 6.518053375196232,
      "grad_norm": 3.903306722640991,
      "learning_rate": 4.592621664050236e-05,
      "loss": 0.64,
      "step": 415200
    },
    {
      "epoch": 6.519623233908948,
      "grad_norm": 3.5218987464904785,
      "learning_rate": 4.592523547880691e-05,
      "loss": 0.6759,
      "step": 415300
    },
    {
      "epoch": 6.521193092621664,
      "grad_norm": 4.750838756561279,
      "learning_rate": 4.592425431711147e-05,
      "loss": 0.6545,
      "step": 415400
    },
    {
      "epoch": 6.52276295133438,
      "grad_norm": 3.003324031829834,
      "learning_rate": 4.592327315541601e-05,
      "loss": 0.616,
      "step": 415500
    },
    {
      "epoch": 6.524332810047095,
      "grad_norm": 3.945167064666748,
      "learning_rate": 4.592229199372057e-05,
      "loss": 0.6552,
      "step": 415600
    },
    {
      "epoch": 6.525902668759811,
      "grad_norm": 3.3588783740997314,
      "learning_rate": 4.592131083202512e-05,
      "loss": 0.6753,
      "step": 415700
    },
    {
      "epoch": 6.527472527472527,
      "grad_norm": 4.515867233276367,
      "learning_rate": 4.592032967032967e-05,
      "loss": 0.6389,
      "step": 415800
    },
    {
      "epoch": 6.529042386185243,
      "grad_norm": 4.63041353225708,
      "learning_rate": 4.591934850863422e-05,
      "loss": 0.6872,
      "step": 415900
    },
    {
      "epoch": 6.530612244897959,
      "grad_norm": 4.3187408447265625,
      "learning_rate": 4.591836734693878e-05,
      "loss": 0.6498,
      "step": 416000
    },
    {
      "epoch": 6.532182103610675,
      "grad_norm": 4.074862480163574,
      "learning_rate": 4.591738618524333e-05,
      "loss": 0.6309,
      "step": 416100
    },
    {
      "epoch": 6.533751962323391,
      "grad_norm": 3.983651638031006,
      "learning_rate": 4.591640502354788e-05,
      "loss": 0.6392,
      "step": 416200
    },
    {
      "epoch": 6.535321821036106,
      "grad_norm": 4.597512722015381,
      "learning_rate": 4.591542386185243e-05,
      "loss": 0.7072,
      "step": 416300
    },
    {
      "epoch": 6.536891679748822,
      "grad_norm": 3.3223965167999268,
      "learning_rate": 4.591444270015699e-05,
      "loss": 0.6505,
      "step": 416400
    },
    {
      "epoch": 6.538461538461538,
      "grad_norm": 4.807534217834473,
      "learning_rate": 4.591346153846154e-05,
      "loss": 0.6515,
      "step": 416500
    },
    {
      "epoch": 6.540031397174254,
      "grad_norm": 3.340238332748413,
      "learning_rate": 4.591248037676609e-05,
      "loss": 0.6505,
      "step": 416600
    },
    {
      "epoch": 6.54160125588697,
      "grad_norm": 1.9629251956939697,
      "learning_rate": 4.5911499215070644e-05,
      "loss": 0.6106,
      "step": 416700
    },
    {
      "epoch": 6.543171114599686,
      "grad_norm": 4.2187418937683105,
      "learning_rate": 4.59105180533752e-05,
      "loss": 0.6429,
      "step": 416800
    },
    {
      "epoch": 6.544740973312402,
      "grad_norm": 3.4776408672332764,
      "learning_rate": 4.590953689167975e-05,
      "loss": 0.6569,
      "step": 416900
    },
    {
      "epoch": 6.546310832025117,
      "grad_norm": 2.9861671924591064,
      "learning_rate": 4.59085557299843e-05,
      "loss": 0.6309,
      "step": 417000
    },
    {
      "epoch": 6.547880690737833,
      "grad_norm": 2.0190045833587646,
      "learning_rate": 4.590757456828886e-05,
      "loss": 0.6615,
      "step": 417100
    },
    {
      "epoch": 6.549450549450549,
      "grad_norm": 3.419851541519165,
      "learning_rate": 4.5906593406593405e-05,
      "loss": 0.6288,
      "step": 417200
    },
    {
      "epoch": 6.551020408163265,
      "grad_norm": 4.096134662628174,
      "learning_rate": 4.590561224489796e-05,
      "loss": 0.6418,
      "step": 417300
    },
    {
      "epoch": 6.552590266875981,
      "grad_norm": 4.719537734985352,
      "learning_rate": 4.5904631083202514e-05,
      "loss": 0.6897,
      "step": 417400
    },
    {
      "epoch": 6.554160125588697,
      "grad_norm": 4.69447660446167,
      "learning_rate": 4.590364992150707e-05,
      "loss": 0.6675,
      "step": 417500
    },
    {
      "epoch": 6.555729984301413,
      "grad_norm": 4.773644924163818,
      "learning_rate": 4.5902668759811616e-05,
      "loss": 0.66,
      "step": 417600
    },
    {
      "epoch": 6.557299843014128,
      "grad_norm": 5.126326084136963,
      "learning_rate": 4.5901687598116174e-05,
      "loss": 0.7022,
      "step": 417700
    },
    {
      "epoch": 6.558869701726844,
      "grad_norm": 3.49926495552063,
      "learning_rate": 4.5900706436420725e-05,
      "loss": 0.6995,
      "step": 417800
    },
    {
      "epoch": 6.56043956043956,
      "grad_norm": 4.891801357269287,
      "learning_rate": 4.5899725274725276e-05,
      "loss": 0.635,
      "step": 417900
    },
    {
      "epoch": 6.562009419152276,
      "grad_norm": 3.204181671142578,
      "learning_rate": 4.589874411302983e-05,
      "loss": 0.634,
      "step": 418000
    },
    {
      "epoch": 6.563579277864992,
      "grad_norm": 3.1010265350341797,
      "learning_rate": 4.5897762951334384e-05,
      "loss": 0.6285,
      "step": 418100
    },
    {
      "epoch": 6.565149136577708,
      "grad_norm": 4.024437427520752,
      "learning_rate": 4.5896781789638935e-05,
      "loss": 0.655,
      "step": 418200
    },
    {
      "epoch": 6.566718995290424,
      "grad_norm": 4.98419189453125,
      "learning_rate": 4.5895800627943486e-05,
      "loss": 0.7013,
      "step": 418300
    },
    {
      "epoch": 6.568288854003139,
      "grad_norm": 3.555830240249634,
      "learning_rate": 4.589481946624804e-05,
      "loss": 0.6861,
      "step": 418400
    },
    {
      "epoch": 6.569858712715855,
      "grad_norm": 3.6048831939697266,
      "learning_rate": 4.5893838304552595e-05,
      "loss": 0.6472,
      "step": 418500
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 4.980076789855957,
      "learning_rate": 4.5892857142857146e-05,
      "loss": 0.6256,
      "step": 418600
    },
    {
      "epoch": 6.572998430141287,
      "grad_norm": 4.167980194091797,
      "learning_rate": 4.58918759811617e-05,
      "loss": 0.6583,
      "step": 418700
    },
    {
      "epoch": 6.574568288854003,
      "grad_norm": 3.813037157058716,
      "learning_rate": 4.589089481946625e-05,
      "loss": 0.6625,
      "step": 418800
    },
    {
      "epoch": 6.576138147566719,
      "grad_norm": 4.161264896392822,
      "learning_rate": 4.5889913657770806e-05,
      "loss": 0.6321,
      "step": 418900
    },
    {
      "epoch": 6.577708006279435,
      "grad_norm": 3.6531565189361572,
      "learning_rate": 4.588893249607536e-05,
      "loss": 0.6602,
      "step": 419000
    },
    {
      "epoch": 6.579277864992151,
      "grad_norm": 4.448050498962402,
      "learning_rate": 4.588795133437991e-05,
      "loss": 0.6982,
      "step": 419100
    },
    {
      "epoch": 6.580847723704867,
      "grad_norm": 2.6888890266418457,
      "learning_rate": 4.5886970172684466e-05,
      "loss": 0.6384,
      "step": 419200
    },
    {
      "epoch": 6.582417582417582,
      "grad_norm": 3.5435540676116943,
      "learning_rate": 4.588598901098901e-05,
      "loss": 0.6844,
      "step": 419300
    },
    {
      "epoch": 6.583987441130298,
      "grad_norm": 4.437620639801025,
      "learning_rate": 4.588500784929357e-05,
      "loss": 0.6663,
      "step": 419400
    },
    {
      "epoch": 6.585557299843014,
      "grad_norm": 3.6947429180145264,
      "learning_rate": 4.588402668759812e-05,
      "loss": 0.6792,
      "step": 419500
    },
    {
      "epoch": 6.58712715855573,
      "grad_norm": 4.414155960083008,
      "learning_rate": 4.5883045525902676e-05,
      "loss": 0.6662,
      "step": 419600
    },
    {
      "epoch": 6.588697017268446,
      "grad_norm": 4.632822036743164,
      "learning_rate": 4.588206436420722e-05,
      "loss": 0.6869,
      "step": 419700
    },
    {
      "epoch": 6.590266875981162,
      "grad_norm": 4.941387176513672,
      "learning_rate": 4.588108320251178e-05,
      "loss": 0.6462,
      "step": 419800
    },
    {
      "epoch": 6.591836734693878,
      "grad_norm": 4.128376007080078,
      "learning_rate": 4.588010204081633e-05,
      "loss": 0.6438,
      "step": 419900
    },
    {
      "epoch": 6.593406593406593,
      "grad_norm": 4.330506324768066,
      "learning_rate": 4.587912087912088e-05,
      "loss": 0.7086,
      "step": 420000
    },
    {
      "epoch": 6.594976452119309,
      "grad_norm": 3.6380932331085205,
      "learning_rate": 4.587813971742543e-05,
      "loss": 0.6378,
      "step": 420100
    },
    {
      "epoch": 6.596546310832025,
      "grad_norm": 2.215407371520996,
      "learning_rate": 4.587715855572999e-05,
      "loss": 0.6471,
      "step": 420200
    },
    {
      "epoch": 6.598116169544741,
      "grad_norm": 4.045429229736328,
      "learning_rate": 4.587617739403454e-05,
      "loss": 0.6643,
      "step": 420300
    },
    {
      "epoch": 6.599686028257457,
      "grad_norm": 3.878676652908325,
      "learning_rate": 4.587519623233909e-05,
      "loss": 0.6531,
      "step": 420400
    },
    {
      "epoch": 6.601255886970173,
      "grad_norm": 4.28791618347168,
      "learning_rate": 4.587421507064364e-05,
      "loss": 0.6929,
      "step": 420500
    },
    {
      "epoch": 6.602825745682889,
      "grad_norm": 4.032438278198242,
      "learning_rate": 4.58732339089482e-05,
      "loss": 0.6935,
      "step": 420600
    },
    {
      "epoch": 6.604395604395604,
      "grad_norm": 3.92838716506958,
      "learning_rate": 4.587225274725275e-05,
      "loss": 0.6638,
      "step": 420700
    },
    {
      "epoch": 6.60596546310832,
      "grad_norm": 4.5888671875,
      "learning_rate": 4.58712715855573e-05,
      "loss": 0.6743,
      "step": 420800
    },
    {
      "epoch": 6.607535321821036,
      "grad_norm": 4.326857566833496,
      "learning_rate": 4.587029042386185e-05,
      "loss": 0.6656,
      "step": 420900
    },
    {
      "epoch": 6.609105180533752,
      "grad_norm": 4.291805267333984,
      "learning_rate": 4.5869309262166404e-05,
      "loss": 0.6178,
      "step": 421000
    },
    {
      "epoch": 6.610675039246468,
      "grad_norm": 3.4836206436157227,
      "learning_rate": 4.586832810047096e-05,
      "loss": 0.6832,
      "step": 421100
    },
    {
      "epoch": 6.612244897959184,
      "grad_norm": 3.1322548389434814,
      "learning_rate": 4.586734693877551e-05,
      "loss": 0.6534,
      "step": 421200
    },
    {
      "epoch": 6.6138147566719,
      "grad_norm": 3.563323497772217,
      "learning_rate": 4.586636577708007e-05,
      "loss": 0.665,
      "step": 421300
    },
    {
      "epoch": 6.615384615384615,
      "grad_norm": 4.335646629333496,
      "learning_rate": 4.5865384615384614e-05,
      "loss": 0.6825,
      "step": 421400
    },
    {
      "epoch": 6.616954474097331,
      "grad_norm": 4.087550163269043,
      "learning_rate": 4.586440345368917e-05,
      "loss": 0.681,
      "step": 421500
    },
    {
      "epoch": 6.618524332810047,
      "grad_norm": 4.439944744110107,
      "learning_rate": 4.586342229199372e-05,
      "loss": 0.6488,
      "step": 421600
    },
    {
      "epoch": 6.620094191522763,
      "grad_norm": 3.452848196029663,
      "learning_rate": 4.5862441130298274e-05,
      "loss": 0.6733,
      "step": 421700
    },
    {
      "epoch": 6.621664050235479,
      "grad_norm": 4.168079853057861,
      "learning_rate": 4.5861459968602825e-05,
      "loss": 0.6581,
      "step": 421800
    },
    {
      "epoch": 6.623233908948195,
      "grad_norm": 5.123873710632324,
      "learning_rate": 4.586047880690738e-05,
      "loss": 0.6494,
      "step": 421900
    },
    {
      "epoch": 6.624803767660911,
      "grad_norm": 4.797772407531738,
      "learning_rate": 4.5859497645211934e-05,
      "loss": 0.653,
      "step": 422000
    },
    {
      "epoch": 6.626373626373626,
      "grad_norm": 3.767425060272217,
      "learning_rate": 4.5858516483516485e-05,
      "loss": 0.6672,
      "step": 422100
    },
    {
      "epoch": 6.627943485086342,
      "grad_norm": 3.510530471801758,
      "learning_rate": 4.5857535321821036e-05,
      "loss": 0.6521,
      "step": 422200
    },
    {
      "epoch": 6.629513343799058,
      "grad_norm": 3.823760747909546,
      "learning_rate": 4.5856554160125593e-05,
      "loss": 0.6086,
      "step": 422300
    },
    {
      "epoch": 6.631083202511774,
      "grad_norm": 3.0282325744628906,
      "learning_rate": 4.585557299843014e-05,
      "loss": 0.6451,
      "step": 422400
    },
    {
      "epoch": 6.63265306122449,
      "grad_norm": 5.055635929107666,
      "learning_rate": 4.5854591836734695e-05,
      "loss": 0.6656,
      "step": 422500
    },
    {
      "epoch": 6.634222919937206,
      "grad_norm": 3.741554021835327,
      "learning_rate": 4.5853610675039246e-05,
      "loss": 0.6501,
      "step": 422600
    },
    {
      "epoch": 6.635792778649922,
      "grad_norm": 3.335679292678833,
      "learning_rate": 4.5852629513343804e-05,
      "loss": 0.6621,
      "step": 422700
    },
    {
      "epoch": 6.637362637362637,
      "grad_norm": 3.3105015754699707,
      "learning_rate": 4.5851648351648355e-05,
      "loss": 0.6541,
      "step": 422800
    },
    {
      "epoch": 6.638932496075353,
      "grad_norm": 4.522146701812744,
      "learning_rate": 4.5850667189952906e-05,
      "loss": 0.6373,
      "step": 422900
    },
    {
      "epoch": 6.640502354788069,
      "grad_norm": 3.880875825881958,
      "learning_rate": 4.584968602825746e-05,
      "loss": 0.6554,
      "step": 423000
    },
    {
      "epoch": 6.642072213500785,
      "grad_norm": 4.012777805328369,
      "learning_rate": 4.584870486656201e-05,
      "loss": 0.6574,
      "step": 423100
    },
    {
      "epoch": 6.643642072213501,
      "grad_norm": 4.33594274520874,
      "learning_rate": 4.5847723704866566e-05,
      "loss": 0.6552,
      "step": 423200
    },
    {
      "epoch": 6.645211930926217,
      "grad_norm": 4.109043121337891,
      "learning_rate": 4.584674254317112e-05,
      "loss": 0.6459,
      "step": 423300
    },
    {
      "epoch": 6.646781789638933,
      "grad_norm": 3.6981091499328613,
      "learning_rate": 4.5845761381475675e-05,
      "loss": 0.6527,
      "step": 423400
    },
    {
      "epoch": 6.648351648351649,
      "grad_norm": 2.8393442630767822,
      "learning_rate": 4.584478021978022e-05,
      "loss": 0.6533,
      "step": 423500
    },
    {
      "epoch": 6.649921507064365,
      "grad_norm": 4.11692476272583,
      "learning_rate": 4.5843799058084776e-05,
      "loss": 0.646,
      "step": 423600
    },
    {
      "epoch": 6.65149136577708,
      "grad_norm": 4.896356105804443,
      "learning_rate": 4.584281789638933e-05,
      "loss": 0.6601,
      "step": 423700
    },
    {
      "epoch": 6.653061224489796,
      "grad_norm": 4.2451348304748535,
      "learning_rate": 4.584183673469388e-05,
      "loss": 0.6282,
      "step": 423800
    },
    {
      "epoch": 6.654631083202512,
      "grad_norm": 2.9617629051208496,
      "learning_rate": 4.584085557299843e-05,
      "loss": 0.6762,
      "step": 423900
    },
    {
      "epoch": 6.656200941915228,
      "grad_norm": 4.152562141418457,
      "learning_rate": 4.583987441130299e-05,
      "loss": 0.6663,
      "step": 424000
    },
    {
      "epoch": 6.657770800627944,
      "grad_norm": 3.903202533721924,
      "learning_rate": 4.583889324960754e-05,
      "loss": 0.658,
      "step": 424100
    },
    {
      "epoch": 6.65934065934066,
      "grad_norm": 3.8606700897216797,
      "learning_rate": 4.583791208791209e-05,
      "loss": 0.6723,
      "step": 424200
    },
    {
      "epoch": 6.660910518053376,
      "grad_norm": 4.6639580726623535,
      "learning_rate": 4.583693092621664e-05,
      "loss": 0.6419,
      "step": 424300
    },
    {
      "epoch": 6.662480376766091,
      "grad_norm": 3.5335583686828613,
      "learning_rate": 4.58359497645212e-05,
      "loss": 0.675,
      "step": 424400
    },
    {
      "epoch": 6.664050235478807,
      "grad_norm": 3.5940918922424316,
      "learning_rate": 4.583496860282574e-05,
      "loss": 0.6257,
      "step": 424500
    },
    {
      "epoch": 6.665620094191523,
      "grad_norm": 3.3623459339141846,
      "learning_rate": 4.58339874411303e-05,
      "loss": 0.6557,
      "step": 424600
    },
    {
      "epoch": 6.667189952904239,
      "grad_norm": 4.651628494262695,
      "learning_rate": 4.583300627943485e-05,
      "loss": 0.6425,
      "step": 424700
    },
    {
      "epoch": 6.668759811616955,
      "grad_norm": 7.007755279541016,
      "learning_rate": 4.583202511773941e-05,
      "loss": 0.6703,
      "step": 424800
    },
    {
      "epoch": 6.670329670329671,
      "grad_norm": 3.973419189453125,
      "learning_rate": 4.583104395604396e-05,
      "loss": 0.6912,
      "step": 424900
    },
    {
      "epoch": 6.671899529042387,
      "grad_norm": 5.108785629272461,
      "learning_rate": 4.583006279434851e-05,
      "loss": 0.667,
      "step": 425000
    },
    {
      "epoch": 6.673469387755102,
      "grad_norm": 4.459264755249023,
      "learning_rate": 4.582908163265306e-05,
      "loss": 0.6756,
      "step": 425100
    },
    {
      "epoch": 6.675039246467818,
      "grad_norm": 3.676427125930786,
      "learning_rate": 4.582810047095761e-05,
      "loss": 0.6494,
      "step": 425200
    },
    {
      "epoch": 6.676609105180534,
      "grad_norm": 2.5297088623046875,
      "learning_rate": 4.582711930926217e-05,
      "loss": 0.6285,
      "step": 425300
    },
    {
      "epoch": 6.67817896389325,
      "grad_norm": 4.20247745513916,
      "learning_rate": 4.582613814756672e-05,
      "loss": 0.6374,
      "step": 425400
    },
    {
      "epoch": 6.679748822605966,
      "grad_norm": 4.2741875648498535,
      "learning_rate": 4.582515698587128e-05,
      "loss": 0.6713,
      "step": 425500
    },
    {
      "epoch": 6.681318681318682,
      "grad_norm": 3.078528642654419,
      "learning_rate": 4.582417582417582e-05,
      "loss": 0.6586,
      "step": 425600
    },
    {
      "epoch": 6.6828885400313975,
      "grad_norm": 3.7387678623199463,
      "learning_rate": 4.582319466248038e-05,
      "loss": 0.6357,
      "step": 425700
    },
    {
      "epoch": 6.684458398744113,
      "grad_norm": 4.37501335144043,
      "learning_rate": 4.582221350078493e-05,
      "loss": 0.6244,
      "step": 425800
    },
    {
      "epoch": 6.686028257456829,
      "grad_norm": 3.8193628787994385,
      "learning_rate": 4.582123233908948e-05,
      "loss": 0.6473,
      "step": 425900
    },
    {
      "epoch": 6.687598116169545,
      "grad_norm": 4.281713962554932,
      "learning_rate": 4.5820251177394034e-05,
      "loss": 0.7034,
      "step": 426000
    },
    {
      "epoch": 6.689167974882261,
      "grad_norm": 4.0328192710876465,
      "learning_rate": 4.581927001569859e-05,
      "loss": 0.6899,
      "step": 426100
    },
    {
      "epoch": 6.6907378335949765,
      "grad_norm": 4.77145528793335,
      "learning_rate": 4.581828885400314e-05,
      "loss": 0.6437,
      "step": 426200
    },
    {
      "epoch": 6.6923076923076925,
      "grad_norm": 4.2229485511779785,
      "learning_rate": 4.5817307692307694e-05,
      "loss": 0.6657,
      "step": 426300
    },
    {
      "epoch": 6.6938775510204085,
      "grad_norm": 2.807551860809326,
      "learning_rate": 4.5816326530612245e-05,
      "loss": 0.7034,
      "step": 426400
    },
    {
      "epoch": 6.695447409733124,
      "grad_norm": 3.396970748901367,
      "learning_rate": 4.58153453689168e-05,
      "loss": 0.6596,
      "step": 426500
    },
    {
      "epoch": 6.6970172684458396,
      "grad_norm": 4.136072635650635,
      "learning_rate": 4.5814364207221347e-05,
      "loss": 0.6734,
      "step": 426600
    },
    {
      "epoch": 6.6985871271585555,
      "grad_norm": 4.392918586730957,
      "learning_rate": 4.5813383045525904e-05,
      "loss": 0.6913,
      "step": 426700
    },
    {
      "epoch": 6.7001569858712715,
      "grad_norm": 5.790802001953125,
      "learning_rate": 4.5812401883830455e-05,
      "loss": 0.6301,
      "step": 426800
    },
    {
      "epoch": 6.7017268445839875,
      "grad_norm": 3.874716281890869,
      "learning_rate": 4.581142072213501e-05,
      "loss": 0.6607,
      "step": 426900
    },
    {
      "epoch": 6.7032967032967035,
      "grad_norm": 4.395610332489014,
      "learning_rate": 4.5810439560439564e-05,
      "loss": 0.6691,
      "step": 427000
    },
    {
      "epoch": 6.704866562009419,
      "grad_norm": 3.387570858001709,
      "learning_rate": 4.5809458398744115e-05,
      "loss": 0.6282,
      "step": 427100
    },
    {
      "epoch": 6.7064364207221345,
      "grad_norm": 3.767056703567505,
      "learning_rate": 4.5808477237048666e-05,
      "loss": 0.713,
      "step": 427200
    },
    {
      "epoch": 6.7080062794348505,
      "grad_norm": 4.390620231628418,
      "learning_rate": 4.580749607535322e-05,
      "loss": 0.6369,
      "step": 427300
    },
    {
      "epoch": 6.7095761381475665,
      "grad_norm": 3.692119598388672,
      "learning_rate": 4.5806514913657775e-05,
      "loss": 0.6837,
      "step": 427400
    },
    {
      "epoch": 6.7111459968602825,
      "grad_norm": 4.337389945983887,
      "learning_rate": 4.5805533751962326e-05,
      "loss": 0.6362,
      "step": 427500
    },
    {
      "epoch": 6.712715855572998,
      "grad_norm": 3.284839391708374,
      "learning_rate": 4.5804552590266883e-05,
      "loss": 0.6517,
      "step": 427600
    },
    {
      "epoch": 6.714285714285714,
      "grad_norm": 3.8923492431640625,
      "learning_rate": 4.580357142857143e-05,
      "loss": 0.6605,
      "step": 427700
    },
    {
      "epoch": 6.71585557299843,
      "grad_norm": 3.559241533279419,
      "learning_rate": 4.5802590266875985e-05,
      "loss": 0.6372,
      "step": 427800
    },
    {
      "epoch": 6.717425431711146,
      "grad_norm": 4.244718551635742,
      "learning_rate": 4.5801609105180536e-05,
      "loss": 0.6618,
      "step": 427900
    },
    {
      "epoch": 6.718995290423862,
      "grad_norm": 4.218554496765137,
      "learning_rate": 4.580062794348509e-05,
      "loss": 0.676,
      "step": 428000
    },
    {
      "epoch": 6.720565149136577,
      "grad_norm": 4.280318737030029,
      "learning_rate": 4.579964678178964e-05,
      "loss": 0.6621,
      "step": 428100
    },
    {
      "epoch": 6.722135007849293,
      "grad_norm": 4.462216377258301,
      "learning_rate": 4.5798665620094196e-05,
      "loss": 0.6728,
      "step": 428200
    },
    {
      "epoch": 6.723704866562009,
      "grad_norm": 3.9067952632904053,
      "learning_rate": 4.579768445839875e-05,
      "loss": 0.6731,
      "step": 428300
    },
    {
      "epoch": 6.725274725274725,
      "grad_norm": 3.880420446395874,
      "learning_rate": 4.57967032967033e-05,
      "loss": 0.6565,
      "step": 428400
    },
    {
      "epoch": 6.726844583987441,
      "grad_norm": 4.482823371887207,
      "learning_rate": 4.579572213500785e-05,
      "loss": 0.6583,
      "step": 428500
    },
    {
      "epoch": 6.728414442700157,
      "grad_norm": 4.281001091003418,
      "learning_rate": 4.579474097331241e-05,
      "loss": 0.6334,
      "step": 428600
    },
    {
      "epoch": 6.729984301412873,
      "grad_norm": 2.972917318344116,
      "learning_rate": 4.579375981161695e-05,
      "loss": 0.6311,
      "step": 428700
    },
    {
      "epoch": 6.731554160125588,
      "grad_norm": 4.214217662811279,
      "learning_rate": 4.579277864992151e-05,
      "loss": 0.6351,
      "step": 428800
    },
    {
      "epoch": 6.733124018838304,
      "grad_norm": 5.009715557098389,
      "learning_rate": 4.579179748822606e-05,
      "loss": 0.6873,
      "step": 428900
    },
    {
      "epoch": 6.73469387755102,
      "grad_norm": 3.6489620208740234,
      "learning_rate": 4.579081632653062e-05,
      "loss": 0.6444,
      "step": 429000
    },
    {
      "epoch": 6.736263736263736,
      "grad_norm": 3.779873847961426,
      "learning_rate": 4.578983516483517e-05,
      "loss": 0.6229,
      "step": 429100
    },
    {
      "epoch": 6.737833594976452,
      "grad_norm": 4.247011184692383,
      "learning_rate": 4.578885400313972e-05,
      "loss": 0.6378,
      "step": 429200
    },
    {
      "epoch": 6.739403453689168,
      "grad_norm": 4.552279949188232,
      "learning_rate": 4.578787284144427e-05,
      "loss": 0.7053,
      "step": 429300
    },
    {
      "epoch": 6.740973312401884,
      "grad_norm": 4.827187538146973,
      "learning_rate": 4.578689167974882e-05,
      "loss": 0.6572,
      "step": 429400
    },
    {
      "epoch": 6.742543171114599,
      "grad_norm": 4.240299224853516,
      "learning_rate": 4.578591051805338e-05,
      "loss": 0.659,
      "step": 429500
    },
    {
      "epoch": 6.744113029827315,
      "grad_norm": 3.70780086517334,
      "learning_rate": 4.578492935635793e-05,
      "loss": 0.6653,
      "step": 429600
    },
    {
      "epoch": 6.745682888540031,
      "grad_norm": 4.4580912590026855,
      "learning_rate": 4.578394819466249e-05,
      "loss": 0.6633,
      "step": 429700
    },
    {
      "epoch": 6.747252747252747,
      "grad_norm": 4.233243942260742,
      "learning_rate": 4.578296703296703e-05,
      "loss": 0.649,
      "step": 429800
    },
    {
      "epoch": 6.748822605965463,
      "grad_norm": 4.81566047668457,
      "learning_rate": 4.578198587127159e-05,
      "loss": 0.6725,
      "step": 429900
    },
    {
      "epoch": 6.750392464678179,
      "grad_norm": 4.6708149909973145,
      "learning_rate": 4.578100470957614e-05,
      "loss": 0.6415,
      "step": 430000
    },
    {
      "epoch": 6.751962323390895,
      "grad_norm": 2.7481982707977295,
      "learning_rate": 4.578002354788069e-05,
      "loss": 0.6553,
      "step": 430100
    },
    {
      "epoch": 6.75353218210361,
      "grad_norm": 3.882148027420044,
      "learning_rate": 4.577904238618524e-05,
      "loss": 0.6383,
      "step": 430200
    },
    {
      "epoch": 6.755102040816326,
      "grad_norm": 4.112165927886963,
      "learning_rate": 4.57780612244898e-05,
      "loss": 0.6391,
      "step": 430300
    },
    {
      "epoch": 6.756671899529042,
      "grad_norm": 4.00404691696167,
      "learning_rate": 4.577708006279435e-05,
      "loss": 0.648,
      "step": 430400
    },
    {
      "epoch": 6.758241758241758,
      "grad_norm": 3.5009355545043945,
      "learning_rate": 4.57760989010989e-05,
      "loss": 0.6802,
      "step": 430500
    },
    {
      "epoch": 6.759811616954474,
      "grad_norm": 4.200953483581543,
      "learning_rate": 4.5775117739403454e-05,
      "loss": 0.6829,
      "step": 430600
    },
    {
      "epoch": 6.76138147566719,
      "grad_norm": 3.9032814502716064,
      "learning_rate": 4.577413657770801e-05,
      "loss": 0.649,
      "step": 430700
    },
    {
      "epoch": 6.762951334379906,
      "grad_norm": 3.642066240310669,
      "learning_rate": 4.5773155416012556e-05,
      "loss": 0.6723,
      "step": 430800
    },
    {
      "epoch": 6.764521193092621,
      "grad_norm": 3.618288516998291,
      "learning_rate": 4.577217425431711e-05,
      "loss": 0.6479,
      "step": 430900
    },
    {
      "epoch": 6.766091051805337,
      "grad_norm": 3.5966498851776123,
      "learning_rate": 4.5771193092621664e-05,
      "loss": 0.6114,
      "step": 431000
    },
    {
      "epoch": 6.767660910518053,
      "grad_norm": 3.9885480403900146,
      "learning_rate": 4.577021193092622e-05,
      "loss": 0.6362,
      "step": 431100
    },
    {
      "epoch": 6.769230769230769,
      "grad_norm": 4.490698337554932,
      "learning_rate": 4.576923076923077e-05,
      "loss": 0.6384,
      "step": 431200
    },
    {
      "epoch": 6.770800627943485,
      "grad_norm": 4.133393287658691,
      "learning_rate": 4.5768249607535324e-05,
      "loss": 0.6178,
      "step": 431300
    },
    {
      "epoch": 6.772370486656201,
      "grad_norm": 4.011916637420654,
      "learning_rate": 4.5767268445839875e-05,
      "loss": 0.6766,
      "step": 431400
    },
    {
      "epoch": 6.773940345368917,
      "grad_norm": 4.588395118713379,
      "learning_rate": 4.5766287284144426e-05,
      "loss": 0.6669,
      "step": 431500
    },
    {
      "epoch": 6.775510204081632,
      "grad_norm": 4.767653942108154,
      "learning_rate": 4.5765306122448984e-05,
      "loss": 0.6669,
      "step": 431600
    },
    {
      "epoch": 6.777080062794348,
      "grad_norm": 4.314510345458984,
      "learning_rate": 4.5764324960753535e-05,
      "loss": 0.6514,
      "step": 431700
    },
    {
      "epoch": 6.778649921507064,
      "grad_norm": 5.804250717163086,
      "learning_rate": 4.576334379905809e-05,
      "loss": 0.6511,
      "step": 431800
    },
    {
      "epoch": 6.78021978021978,
      "grad_norm": 3.3836562633514404,
      "learning_rate": 4.576236263736264e-05,
      "loss": 0.6669,
      "step": 431900
    },
    {
      "epoch": 6.781789638932496,
      "grad_norm": 3.5371930599212646,
      "learning_rate": 4.5761381475667194e-05,
      "loss": 0.6509,
      "step": 432000
    },
    {
      "epoch": 6.783359497645212,
      "grad_norm": 4.107825756072998,
      "learning_rate": 4.5760400313971745e-05,
      "loss": 0.6447,
      "step": 432100
    },
    {
      "epoch": 6.784929356357928,
      "grad_norm": 3.326155424118042,
      "learning_rate": 4.5759419152276296e-05,
      "loss": 0.605,
      "step": 432200
    },
    {
      "epoch": 6.786499215070644,
      "grad_norm": 3.370508909225464,
      "learning_rate": 4.575843799058085e-05,
      "loss": 0.6214,
      "step": 432300
    },
    {
      "epoch": 6.78806907378336,
      "grad_norm": 4.3597092628479,
      "learning_rate": 4.5757456828885405e-05,
      "loss": 0.6149,
      "step": 432400
    },
    {
      "epoch": 6.789638932496075,
      "grad_norm": 4.254698753356934,
      "learning_rate": 4.5756475667189956e-05,
      "loss": 0.6534,
      "step": 432500
    },
    {
      "epoch": 6.791208791208791,
      "grad_norm": 4.078469276428223,
      "learning_rate": 4.575549450549451e-05,
      "loss": 0.6842,
      "step": 432600
    },
    {
      "epoch": 6.792778649921507,
      "grad_norm": 4.405307769775391,
      "learning_rate": 4.575451334379906e-05,
      "loss": 0.6538,
      "step": 432700
    },
    {
      "epoch": 6.794348508634223,
      "grad_norm": 3.670112133026123,
      "learning_rate": 4.5753532182103616e-05,
      "loss": 0.6452,
      "step": 432800
    },
    {
      "epoch": 6.795918367346939,
      "grad_norm": 3.6952414512634277,
      "learning_rate": 4.575255102040816e-05,
      "loss": 0.6391,
      "step": 432900
    },
    {
      "epoch": 6.797488226059655,
      "grad_norm": 3.733358860015869,
      "learning_rate": 4.575156985871272e-05,
      "loss": 0.646,
      "step": 433000
    },
    {
      "epoch": 6.799058084772371,
      "grad_norm": 3.8675379753112793,
      "learning_rate": 4.575058869701727e-05,
      "loss": 0.628,
      "step": 433100
    },
    {
      "epoch": 6.800627943485086,
      "grad_norm": 3.778604745864868,
      "learning_rate": 4.5749607535321827e-05,
      "loss": 0.6433,
      "step": 433200
    },
    {
      "epoch": 6.802197802197802,
      "grad_norm": 4.621773719787598,
      "learning_rate": 4.574862637362638e-05,
      "loss": 0.6697,
      "step": 433300
    },
    {
      "epoch": 6.803767660910518,
      "grad_norm": 3.2460124492645264,
      "learning_rate": 4.574764521193093e-05,
      "loss": 0.6233,
      "step": 433400
    },
    {
      "epoch": 6.805337519623234,
      "grad_norm": 4.025027275085449,
      "learning_rate": 4.574666405023548e-05,
      "loss": 0.6617,
      "step": 433500
    },
    {
      "epoch": 6.80690737833595,
      "grad_norm": 3.1558876037597656,
      "learning_rate": 4.574568288854003e-05,
      "loss": 0.6098,
      "step": 433600
    },
    {
      "epoch": 6.808477237048666,
      "grad_norm": 3.818434715270996,
      "learning_rate": 4.574470172684459e-05,
      "loss": 0.635,
      "step": 433700
    },
    {
      "epoch": 6.810047095761382,
      "grad_norm": 4.230484485626221,
      "learning_rate": 4.574372056514914e-05,
      "loss": 0.6619,
      "step": 433800
    },
    {
      "epoch": 6.811616954474097,
      "grad_norm": 4.639458179473877,
      "learning_rate": 4.57427394034537e-05,
      "loss": 0.6252,
      "step": 433900
    },
    {
      "epoch": 6.813186813186813,
      "grad_norm": 3.927868127822876,
      "learning_rate": 4.574175824175824e-05,
      "loss": 0.6518,
      "step": 434000
    },
    {
      "epoch": 6.814756671899529,
      "grad_norm": 3.9438536167144775,
      "learning_rate": 4.57407770800628e-05,
      "loss": 0.6614,
      "step": 434100
    },
    {
      "epoch": 6.816326530612245,
      "grad_norm": 3.8913848400115967,
      "learning_rate": 4.573979591836735e-05,
      "loss": 0.677,
      "step": 434200
    },
    {
      "epoch": 6.817896389324961,
      "grad_norm": 3.881385087966919,
      "learning_rate": 4.57388147566719e-05,
      "loss": 0.6657,
      "step": 434300
    },
    {
      "epoch": 6.819466248037677,
      "grad_norm": 3.900059938430786,
      "learning_rate": 4.573783359497645e-05,
      "loss": 0.6016,
      "step": 434400
    },
    {
      "epoch": 6.821036106750393,
      "grad_norm": 4.1443915367126465,
      "learning_rate": 4.573685243328101e-05,
      "loss": 0.6776,
      "step": 434500
    },
    {
      "epoch": 6.822605965463108,
      "grad_norm": 3.6655824184417725,
      "learning_rate": 4.573587127158556e-05,
      "loss": 0.6375,
      "step": 434600
    },
    {
      "epoch": 6.824175824175824,
      "grad_norm": 3.664362907409668,
      "learning_rate": 4.573489010989011e-05,
      "loss": 0.6876,
      "step": 434700
    },
    {
      "epoch": 6.82574568288854,
      "grad_norm": 4.444950580596924,
      "learning_rate": 4.573390894819466e-05,
      "loss": 0.6816,
      "step": 434800
    },
    {
      "epoch": 6.827315541601256,
      "grad_norm": 4.488329887390137,
      "learning_rate": 4.573292778649922e-05,
      "loss": 0.6779,
      "step": 434900
    },
    {
      "epoch": 6.828885400313972,
      "grad_norm": 4.0432257652282715,
      "learning_rate": 4.5731946624803765e-05,
      "loss": 0.629,
      "step": 435000
    },
    {
      "epoch": 6.830455259026688,
      "grad_norm": 3.772587776184082,
      "learning_rate": 4.573096546310832e-05,
      "loss": 0.6133,
      "step": 435100
    },
    {
      "epoch": 6.832025117739404,
      "grad_norm": 3.7648396492004395,
      "learning_rate": 4.572998430141287e-05,
      "loss": 0.659,
      "step": 435200
    },
    {
      "epoch": 6.833594976452119,
      "grad_norm": 2.8326163291931152,
      "learning_rate": 4.572900313971743e-05,
      "loss": 0.6218,
      "step": 435300
    },
    {
      "epoch": 6.835164835164835,
      "grad_norm": 2.2693214416503906,
      "learning_rate": 4.572802197802198e-05,
      "loss": 0.6734,
      "step": 435400
    },
    {
      "epoch": 6.836734693877551,
      "grad_norm": 4.715219497680664,
      "learning_rate": 4.572704081632653e-05,
      "loss": 0.6448,
      "step": 435500
    },
    {
      "epoch": 6.838304552590267,
      "grad_norm": 3.176703453063965,
      "learning_rate": 4.5726059654631084e-05,
      "loss": 0.685,
      "step": 435600
    },
    {
      "epoch": 6.839874411302983,
      "grad_norm": 4.0978803634643555,
      "learning_rate": 4.5725078492935635e-05,
      "loss": 0.6433,
      "step": 435700
    },
    {
      "epoch": 6.841444270015699,
      "grad_norm": 3.7719240188598633,
      "learning_rate": 4.572409733124019e-05,
      "loss": 0.6597,
      "step": 435800
    },
    {
      "epoch": 6.843014128728415,
      "grad_norm": 3.508165121078491,
      "learning_rate": 4.5723116169544744e-05,
      "loss": 0.6352,
      "step": 435900
    },
    {
      "epoch": 6.84458398744113,
      "grad_norm": 3.145791530609131,
      "learning_rate": 4.57221350078493e-05,
      "loss": 0.6503,
      "step": 436000
    },
    {
      "epoch": 6.846153846153846,
      "grad_norm": 4.26174259185791,
      "learning_rate": 4.5721153846153846e-05,
      "loss": 0.6572,
      "step": 436100
    },
    {
      "epoch": 6.847723704866562,
      "grad_norm": 4.168955326080322,
      "learning_rate": 4.5720172684458403e-05,
      "loss": 0.6845,
      "step": 436200
    },
    {
      "epoch": 6.849293563579278,
      "grad_norm": 4.155153751373291,
      "learning_rate": 4.5719191522762954e-05,
      "loss": 0.6415,
      "step": 436300
    },
    {
      "epoch": 6.850863422291994,
      "grad_norm": 3.127309560775757,
      "learning_rate": 4.5718210361067505e-05,
      "loss": 0.6592,
      "step": 436400
    },
    {
      "epoch": 6.85243328100471,
      "grad_norm": 3.8592991828918457,
      "learning_rate": 4.5717229199372056e-05,
      "loss": 0.6628,
      "step": 436500
    },
    {
      "epoch": 6.854003139717426,
      "grad_norm": 3.7779603004455566,
      "learning_rate": 4.5716248037676614e-05,
      "loss": 0.7005,
      "step": 436600
    },
    {
      "epoch": 6.855572998430142,
      "grad_norm": 4.193482875823975,
      "learning_rate": 4.5715266875981165e-05,
      "loss": 0.7105,
      "step": 436700
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 4.125256538391113,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.6419,
      "step": 436800
    },
    {
      "epoch": 6.858712715855573,
      "grad_norm": 3.3390161991119385,
      "learning_rate": 4.571330455259027e-05,
      "loss": 0.6721,
      "step": 436900
    },
    {
      "epoch": 6.860282574568289,
      "grad_norm": 3.6562445163726807,
      "learning_rate": 4.5712323390894825e-05,
      "loss": 0.6075,
      "step": 437000
    },
    {
      "epoch": 6.861852433281005,
      "grad_norm": 4.802356243133545,
      "learning_rate": 4.571134222919937e-05,
      "loss": 0.6604,
      "step": 437100
    },
    {
      "epoch": 6.863422291993721,
      "grad_norm": 5.1092939376831055,
      "learning_rate": 4.571036106750393e-05,
      "loss": 0.6428,
      "step": 437200
    },
    {
      "epoch": 6.864992150706437,
      "grad_norm": 4.144022464752197,
      "learning_rate": 4.570937990580848e-05,
      "loss": 0.6464,
      "step": 437300
    },
    {
      "epoch": 6.866562009419153,
      "grad_norm": 4.088846206665039,
      "learning_rate": 4.5708398744113036e-05,
      "loss": 0.645,
      "step": 437400
    },
    {
      "epoch": 6.868131868131869,
      "grad_norm": 4.071243762969971,
      "learning_rate": 4.5707417582417587e-05,
      "loss": 0.6097,
      "step": 437500
    },
    {
      "epoch": 6.869701726844584,
      "grad_norm": 3.68660306930542,
      "learning_rate": 4.570643642072214e-05,
      "loss": 0.6901,
      "step": 437600
    },
    {
      "epoch": 6.8712715855573,
      "grad_norm": 4.038558483123779,
      "learning_rate": 4.570545525902669e-05,
      "loss": 0.6946,
      "step": 437700
    },
    {
      "epoch": 6.872841444270016,
      "grad_norm": 3.4169440269470215,
      "learning_rate": 4.570447409733124e-05,
      "loss": 0.6615,
      "step": 437800
    },
    {
      "epoch": 6.874411302982732,
      "grad_norm": 3.809004068374634,
      "learning_rate": 4.57034929356358e-05,
      "loss": 0.6651,
      "step": 437900
    },
    {
      "epoch": 6.875981161695448,
      "grad_norm": 3.7975666522979736,
      "learning_rate": 4.570251177394035e-05,
      "loss": 0.6477,
      "step": 438000
    },
    {
      "epoch": 6.877551020408164,
      "grad_norm": 2.993135452270508,
      "learning_rate": 4.5701530612244906e-05,
      "loss": 0.681,
      "step": 438100
    },
    {
      "epoch": 6.8791208791208796,
      "grad_norm": 3.2484428882598877,
      "learning_rate": 4.570054945054945e-05,
      "loss": 0.6755,
      "step": 438200
    },
    {
      "epoch": 6.880690737833595,
      "grad_norm": 4.178186416625977,
      "learning_rate": 4.569956828885401e-05,
      "loss": 0.6576,
      "step": 438300
    },
    {
      "epoch": 6.882260596546311,
      "grad_norm": 3.485551357269287,
      "learning_rate": 4.569858712715856e-05,
      "loss": 0.6409,
      "step": 438400
    },
    {
      "epoch": 6.883830455259027,
      "grad_norm": 4.192556381225586,
      "learning_rate": 4.569760596546311e-05,
      "loss": 0.6364,
      "step": 438500
    },
    {
      "epoch": 6.885400313971743,
      "grad_norm": 4.419401168823242,
      "learning_rate": 4.569662480376766e-05,
      "loss": 0.6978,
      "step": 438600
    },
    {
      "epoch": 6.8869701726844585,
      "grad_norm": 3.366205930709839,
      "learning_rate": 4.569564364207222e-05,
      "loss": 0.6146,
      "step": 438700
    },
    {
      "epoch": 6.8885400313971745,
      "grad_norm": 4.611764430999756,
      "learning_rate": 4.569466248037677e-05,
      "loss": 0.6384,
      "step": 438800
    },
    {
      "epoch": 6.8901098901098905,
      "grad_norm": 3.458566665649414,
      "learning_rate": 4.569368131868132e-05,
      "loss": 0.6092,
      "step": 438900
    },
    {
      "epoch": 6.891679748822606,
      "grad_norm": 3.3793044090270996,
      "learning_rate": 4.569270015698587e-05,
      "loss": 0.6594,
      "step": 439000
    },
    {
      "epoch": 6.893249607535322,
      "grad_norm": 3.071533441543579,
      "learning_rate": 4.569171899529043e-05,
      "loss": 0.6987,
      "step": 439100
    },
    {
      "epoch": 6.8948194662480375,
      "grad_norm": 4.055788516998291,
      "learning_rate": 4.5690737833594974e-05,
      "loss": 0.6434,
      "step": 439200
    },
    {
      "epoch": 6.8963893249607535,
      "grad_norm": 4.594134330749512,
      "learning_rate": 4.568975667189953e-05,
      "loss": 0.7011,
      "step": 439300
    },
    {
      "epoch": 6.8979591836734695,
      "grad_norm": 3.9044289588928223,
      "learning_rate": 4.568877551020408e-05,
      "loss": 0.6481,
      "step": 439400
    },
    {
      "epoch": 6.8995290423861855,
      "grad_norm": 3.741888999938965,
      "learning_rate": 4.568779434850864e-05,
      "loss": 0.5915,
      "step": 439500
    },
    {
      "epoch": 6.9010989010989015,
      "grad_norm": 4.198981761932373,
      "learning_rate": 4.568681318681319e-05,
      "loss": 0.6699,
      "step": 439600
    },
    {
      "epoch": 6.9026687598116165,
      "grad_norm": 2.9552738666534424,
      "learning_rate": 4.568583202511774e-05,
      "loss": 0.6301,
      "step": 439700
    },
    {
      "epoch": 6.9042386185243325,
      "grad_norm": 3.496197462081909,
      "learning_rate": 4.568485086342229e-05,
      "loss": 0.6149,
      "step": 439800
    },
    {
      "epoch": 6.9058084772370485,
      "grad_norm": 4.054678916931152,
      "learning_rate": 4.5683869701726844e-05,
      "loss": 0.6765,
      "step": 439900
    },
    {
      "epoch": 6.9073783359497645,
      "grad_norm": 4.607807636260986,
      "learning_rate": 4.56828885400314e-05,
      "loss": 0.6397,
      "step": 440000
    },
    {
      "epoch": 6.9089481946624804,
      "grad_norm": 3.396005630493164,
      "learning_rate": 4.568190737833595e-05,
      "loss": 0.6662,
      "step": 440100
    },
    {
      "epoch": 6.910518053375196,
      "grad_norm": 3.507615566253662,
      "learning_rate": 4.568092621664051e-05,
      "loss": 0.6519,
      "step": 440200
    },
    {
      "epoch": 6.912087912087912,
      "grad_norm": 4.660302639007568,
      "learning_rate": 4.5679945054945055e-05,
      "loss": 0.6686,
      "step": 440300
    },
    {
      "epoch": 6.9136577708006275,
      "grad_norm": 4.174223899841309,
      "learning_rate": 4.567896389324961e-05,
      "loss": 0.6535,
      "step": 440400
    },
    {
      "epoch": 6.9152276295133435,
      "grad_norm": 4.847259521484375,
      "learning_rate": 4.567798273155416e-05,
      "loss": 0.6689,
      "step": 440500
    },
    {
      "epoch": 6.916797488226059,
      "grad_norm": 3.7339024543762207,
      "learning_rate": 4.5677001569858714e-05,
      "loss": 0.6731,
      "step": 440600
    },
    {
      "epoch": 6.918367346938775,
      "grad_norm": 4.93514347076416,
      "learning_rate": 4.5676020408163265e-05,
      "loss": 0.6057,
      "step": 440700
    },
    {
      "epoch": 6.919937205651491,
      "grad_norm": 4.376393795013428,
      "learning_rate": 4.567503924646782e-05,
      "loss": 0.6253,
      "step": 440800
    },
    {
      "epoch": 6.921507064364207,
      "grad_norm": 3.21909761428833,
      "learning_rate": 4.5674058084772374e-05,
      "loss": 0.6455,
      "step": 440900
    },
    {
      "epoch": 6.923076923076923,
      "grad_norm": 4.241886138916016,
      "learning_rate": 4.5673076923076925e-05,
      "loss": 0.6736,
      "step": 441000
    },
    {
      "epoch": 6.924646781789638,
      "grad_norm": 4.486710548400879,
      "learning_rate": 4.5672095761381476e-05,
      "loss": 0.6515,
      "step": 441100
    },
    {
      "epoch": 6.926216640502354,
      "grad_norm": 3.533912420272827,
      "learning_rate": 4.5671114599686034e-05,
      "loss": 0.6573,
      "step": 441200
    },
    {
      "epoch": 6.92778649921507,
      "grad_norm": 4.5491557121276855,
      "learning_rate": 4.567013343799058e-05,
      "loss": 0.669,
      "step": 441300
    },
    {
      "epoch": 6.929356357927786,
      "grad_norm": 4.582521915435791,
      "learning_rate": 4.5669152276295136e-05,
      "loss": 0.6491,
      "step": 441400
    },
    {
      "epoch": 6.930926216640502,
      "grad_norm": 4.593319416046143,
      "learning_rate": 4.566817111459969e-05,
      "loss": 0.6534,
      "step": 441500
    },
    {
      "epoch": 6.932496075353218,
      "grad_norm": 4.321818828582764,
      "learning_rate": 4.5667189952904245e-05,
      "loss": 0.631,
      "step": 441600
    },
    {
      "epoch": 6.934065934065934,
      "grad_norm": 4.582895278930664,
      "learning_rate": 4.5666208791208795e-05,
      "loss": 0.6055,
      "step": 441700
    },
    {
      "epoch": 6.93563579277865,
      "grad_norm": 3.833357810974121,
      "learning_rate": 4.5665227629513346e-05,
      "loss": 0.671,
      "step": 441800
    },
    {
      "epoch": 6.937205651491366,
      "grad_norm": 4.236477851867676,
      "learning_rate": 4.56642464678179e-05,
      "loss": 0.6612,
      "step": 441900
    },
    {
      "epoch": 6.938775510204081,
      "grad_norm": 4.045938968658447,
      "learning_rate": 4.566326530612245e-05,
      "loss": 0.6617,
      "step": 442000
    },
    {
      "epoch": 6.940345368916797,
      "grad_norm": 5.2313456535339355,
      "learning_rate": 4.5662284144427006e-05,
      "loss": 0.6449,
      "step": 442100
    },
    {
      "epoch": 6.941915227629513,
      "grad_norm": 3.7538533210754395,
      "learning_rate": 4.566130298273156e-05,
      "loss": 0.618,
      "step": 442200
    },
    {
      "epoch": 6.943485086342229,
      "grad_norm": 4.1116743087768555,
      "learning_rate": 4.5660321821036115e-05,
      "loss": 0.6554,
      "step": 442300
    },
    {
      "epoch": 6.945054945054945,
      "grad_norm": 4.796196937561035,
      "learning_rate": 4.565934065934066e-05,
      "loss": 0.6554,
      "step": 442400
    },
    {
      "epoch": 6.946624803767661,
      "grad_norm": 4.226678848266602,
      "learning_rate": 4.565835949764522e-05,
      "loss": 0.6613,
      "step": 442500
    },
    {
      "epoch": 6.948194662480377,
      "grad_norm": 2.9684948921203613,
      "learning_rate": 4.565737833594977e-05,
      "loss": 0.6486,
      "step": 442600
    },
    {
      "epoch": 6.949764521193092,
      "grad_norm": 4.131485939025879,
      "learning_rate": 4.565639717425432e-05,
      "loss": 0.6338,
      "step": 442700
    },
    {
      "epoch": 6.951334379905808,
      "grad_norm": 3.94693922996521,
      "learning_rate": 4.565541601255887e-05,
      "loss": 0.6747,
      "step": 442800
    },
    {
      "epoch": 6.952904238618524,
      "grad_norm": 4.4829230308532715,
      "learning_rate": 4.565443485086343e-05,
      "loss": 0.6601,
      "step": 442900
    },
    {
      "epoch": 6.95447409733124,
      "grad_norm": 4.411757469177246,
      "learning_rate": 4.565345368916798e-05,
      "loss": 0.6661,
      "step": 443000
    },
    {
      "epoch": 6.956043956043956,
      "grad_norm": 3.9463860988616943,
      "learning_rate": 4.565247252747253e-05,
      "loss": 0.6267,
      "step": 443100
    },
    {
      "epoch": 6.957613814756672,
      "grad_norm": 3.9472568035125732,
      "learning_rate": 4.565149136577708e-05,
      "loss": 0.6362,
      "step": 443200
    },
    {
      "epoch": 6.959183673469388,
      "grad_norm": 3.568089246749878,
      "learning_rate": 4.565051020408164e-05,
      "loss": 0.6625,
      "step": 443300
    },
    {
      "epoch": 6.960753532182103,
      "grad_norm": 4.146876811981201,
      "learning_rate": 4.564952904238618e-05,
      "loss": 0.6545,
      "step": 443400
    },
    {
      "epoch": 6.962323390894819,
      "grad_norm": 4.471206188201904,
      "learning_rate": 4.564854788069074e-05,
      "loss": 0.6348,
      "step": 443500
    },
    {
      "epoch": 6.963893249607535,
      "grad_norm": 3.952955484390259,
      "learning_rate": 4.564756671899529e-05,
      "loss": 0.6258,
      "step": 443600
    },
    {
      "epoch": 6.965463108320251,
      "grad_norm": 4.1167144775390625,
      "learning_rate": 4.564658555729984e-05,
      "loss": 0.6477,
      "step": 443700
    },
    {
      "epoch": 6.967032967032967,
      "grad_norm": 3.9961342811584473,
      "learning_rate": 4.56456043956044e-05,
      "loss": 0.6302,
      "step": 443800
    },
    {
      "epoch": 6.968602825745683,
      "grad_norm": 3.9446825981140137,
      "learning_rate": 4.564462323390895e-05,
      "loss": 0.6273,
      "step": 443900
    },
    {
      "epoch": 6.970172684458399,
      "grad_norm": 4.291285991668701,
      "learning_rate": 4.56436420722135e-05,
      "loss": 0.6358,
      "step": 444000
    },
    {
      "epoch": 6.971742543171114,
      "grad_norm": 4.3148016929626465,
      "learning_rate": 4.564266091051805e-05,
      "loss": 0.676,
      "step": 444100
    },
    {
      "epoch": 6.97331240188383,
      "grad_norm": 3.8088858127593994,
      "learning_rate": 4.564167974882261e-05,
      "loss": 0.6854,
      "step": 444200
    },
    {
      "epoch": 6.974882260596546,
      "grad_norm": 4.160046100616455,
      "learning_rate": 4.564069858712716e-05,
      "loss": 0.6462,
      "step": 444300
    },
    {
      "epoch": 6.976452119309262,
      "grad_norm": 4.076075553894043,
      "learning_rate": 4.563971742543171e-05,
      "loss": 0.6428,
      "step": 444400
    },
    {
      "epoch": 6.978021978021978,
      "grad_norm": 4.511480808258057,
      "learning_rate": 4.5638736263736264e-05,
      "loss": 0.6681,
      "step": 444500
    },
    {
      "epoch": 6.979591836734694,
      "grad_norm": 4.225461483001709,
      "learning_rate": 4.563775510204082e-05,
      "loss": 0.6397,
      "step": 444600
    },
    {
      "epoch": 6.98116169544741,
      "grad_norm": 3.9902071952819824,
      "learning_rate": 4.563677394034537e-05,
      "loss": 0.6626,
      "step": 444700
    },
    {
      "epoch": 6.982731554160125,
      "grad_norm": 4.340320587158203,
      "learning_rate": 4.563579277864992e-05,
      "loss": 0.6585,
      "step": 444800
    },
    {
      "epoch": 6.984301412872841,
      "grad_norm": 4.411925315856934,
      "learning_rate": 4.5634811616954474e-05,
      "loss": 0.6571,
      "step": 444900
    },
    {
      "epoch": 6.985871271585557,
      "grad_norm": 4.4411163330078125,
      "learning_rate": 4.563383045525903e-05,
      "loss": 0.6301,
      "step": 445000
    },
    {
      "epoch": 6.987441130298273,
      "grad_norm": 4.145135402679443,
      "learning_rate": 4.5632849293563576e-05,
      "loss": 0.6421,
      "step": 445100
    },
    {
      "epoch": 6.989010989010989,
      "grad_norm": 4.247194766998291,
      "learning_rate": 4.5631868131868134e-05,
      "loss": 0.6572,
      "step": 445200
    },
    {
      "epoch": 6.990580847723705,
      "grad_norm": 4.656375408172607,
      "learning_rate": 4.5630886970172685e-05,
      "loss": 0.6422,
      "step": 445300
    },
    {
      "epoch": 6.992150706436421,
      "grad_norm": 4.644025802612305,
      "learning_rate": 4.562990580847724e-05,
      "loss": 0.6571,
      "step": 445400
    },
    {
      "epoch": 6.993720565149136,
      "grad_norm": 3.1176624298095703,
      "learning_rate": 4.562892464678179e-05,
      "loss": 0.6642,
      "step": 445500
    },
    {
      "epoch": 6.995290423861852,
      "grad_norm": 4.346960544586182,
      "learning_rate": 4.5627943485086345e-05,
      "loss": 0.6859,
      "step": 445600
    },
    {
      "epoch": 6.996860282574568,
      "grad_norm": 3.7239112854003906,
      "learning_rate": 4.5626962323390896e-05,
      "loss": 0.6485,
      "step": 445700
    },
    {
      "epoch": 6.998430141287284,
      "grad_norm": 5.153891086578369,
      "learning_rate": 4.562598116169545e-05,
      "loss": 0.6672,
      "step": 445800
    },
    {
      "epoch": 7.0,
      "grad_norm": 3.1124415397644043,
      "learning_rate": 4.5625e-05,
      "loss": 0.6548,
      "step": 445900
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.032477617263794,
      "eval_runtime": 15.2417,
      "eval_samples_per_second": 219.989,
      "eval_steps_per_second": 219.989,
      "step": 445900
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.5122681856155396,
      "eval_runtime": 281.4912,
      "eval_samples_per_second": 226.295,
      "eval_steps_per_second": 226.295,
      "step": 445900
    },
    {
      "epoch": 7.001569858712716,
      "grad_norm": 3.783388614654541,
      "learning_rate": 4.5624018838304555e-05,
      "loss": 0.682,
      "step": 446000
    },
    {
      "epoch": 7.003139717425432,
      "grad_norm": 2.8153388500213623,
      "learning_rate": 4.5623037676609106e-05,
      "loss": 0.6481,
      "step": 446100
    },
    {
      "epoch": 7.004709576138148,
      "grad_norm": 3.6206343173980713,
      "learning_rate": 4.562205651491366e-05,
      "loss": 0.6389,
      "step": 446200
    },
    {
      "epoch": 7.006279434850863,
      "grad_norm": 4.037407398223877,
      "learning_rate": 4.5621075353218215e-05,
      "loss": 0.6258,
      "step": 446300
    },
    {
      "epoch": 7.007849293563579,
      "grad_norm": 4.097532272338867,
      "learning_rate": 4.5620094191522766e-05,
      "loss": 0.635,
      "step": 446400
    },
    {
      "epoch": 7.009419152276295,
      "grad_norm": 4.634883403778076,
      "learning_rate": 4.561911302982732e-05,
      "loss": 0.6835,
      "step": 446500
    },
    {
      "epoch": 7.010989010989011,
      "grad_norm": 4.377307415008545,
      "learning_rate": 4.561813186813187e-05,
      "loss": 0.6347,
      "step": 446600
    },
    {
      "epoch": 7.012558869701727,
      "grad_norm": 3.480782985687256,
      "learning_rate": 4.5617150706436426e-05,
      "loss": 0.6917,
      "step": 446700
    },
    {
      "epoch": 7.014128728414443,
      "grad_norm": 3.6614880561828613,
      "learning_rate": 4.561616954474098e-05,
      "loss": 0.6335,
      "step": 446800
    },
    {
      "epoch": 7.015698587127159,
      "grad_norm": 4.266921520233154,
      "learning_rate": 4.561518838304553e-05,
      "loss": 0.6342,
      "step": 446900
    },
    {
      "epoch": 7.017268445839874,
      "grad_norm": 3.910398244857788,
      "learning_rate": 4.561420722135008e-05,
      "loss": 0.6201,
      "step": 447000
    },
    {
      "epoch": 7.01883830455259,
      "grad_norm": 4.898576736450195,
      "learning_rate": 4.5613226059654637e-05,
      "loss": 0.6818,
      "step": 447100
    },
    {
      "epoch": 7.020408163265306,
      "grad_norm": 3.980654001235962,
      "learning_rate": 4.561224489795918e-05,
      "loss": 0.6453,
      "step": 447200
    },
    {
      "epoch": 7.021978021978022,
      "grad_norm": 4.148647785186768,
      "learning_rate": 4.561126373626374e-05,
      "loss": 0.6118,
      "step": 447300
    },
    {
      "epoch": 7.023547880690738,
      "grad_norm": 4.684995174407959,
      "learning_rate": 4.561028257456829e-05,
      "loss": 0.6246,
      "step": 447400
    },
    {
      "epoch": 7.025117739403454,
      "grad_norm": 3.809290647506714,
      "learning_rate": 4.560930141287285e-05,
      "loss": 0.6372,
      "step": 447500
    },
    {
      "epoch": 7.02668759811617,
      "grad_norm": 4.507391929626465,
      "learning_rate": 4.560832025117739e-05,
      "loss": 0.6407,
      "step": 447600
    },
    {
      "epoch": 7.028257456828886,
      "grad_norm": 3.974778652191162,
      "learning_rate": 4.560733908948195e-05,
      "loss": 0.6176,
      "step": 447700
    },
    {
      "epoch": 7.029827315541601,
      "grad_norm": 3.4328134059906006,
      "learning_rate": 4.56063579277865e-05,
      "loss": 0.6474,
      "step": 447800
    },
    {
      "epoch": 7.031397174254317,
      "grad_norm": 3.684251546859741,
      "learning_rate": 4.560537676609105e-05,
      "loss": 0.6226,
      "step": 447900
    },
    {
      "epoch": 7.032967032967033,
      "grad_norm": 3.6001839637756348,
      "learning_rate": 4.56043956043956e-05,
      "loss": 0.5817,
      "step": 448000
    },
    {
      "epoch": 7.034536891679749,
      "grad_norm": 4.2871623039245605,
      "learning_rate": 4.560341444270016e-05,
      "loss": 0.6605,
      "step": 448100
    },
    {
      "epoch": 7.036106750392465,
      "grad_norm": 3.787943124771118,
      "learning_rate": 4.560243328100471e-05,
      "loss": 0.6894,
      "step": 448200
    },
    {
      "epoch": 7.037676609105181,
      "grad_norm": 3.056819438934326,
      "learning_rate": 4.560145211930926e-05,
      "loss": 0.6511,
      "step": 448300
    },
    {
      "epoch": 7.039246467817897,
      "grad_norm": 3.7058939933776855,
      "learning_rate": 4.560047095761382e-05,
      "loss": 0.6799,
      "step": 448400
    },
    {
      "epoch": 7.040816326530612,
      "grad_norm": 4.442514419555664,
      "learning_rate": 4.559948979591837e-05,
      "loss": 0.6589,
      "step": 448500
    },
    {
      "epoch": 7.042386185243328,
      "grad_norm": 4.401740550994873,
      "learning_rate": 4.559850863422292e-05,
      "loss": 0.6615,
      "step": 448600
    },
    {
      "epoch": 7.043956043956044,
      "grad_norm": 3.6890974044799805,
      "learning_rate": 4.559752747252747e-05,
      "loss": 0.7075,
      "step": 448700
    },
    {
      "epoch": 7.04552590266876,
      "grad_norm": 4.103818416595459,
      "learning_rate": 4.559654631083203e-05,
      "loss": 0.6331,
      "step": 448800
    },
    {
      "epoch": 7.047095761381476,
      "grad_norm": 3.8877649307250977,
      "learning_rate": 4.559556514913658e-05,
      "loss": 0.6334,
      "step": 448900
    },
    {
      "epoch": 7.048665620094192,
      "grad_norm": 4.039058208465576,
      "learning_rate": 4.559458398744113e-05,
      "loss": 0.6524,
      "step": 449000
    },
    {
      "epoch": 7.050235478806908,
      "grad_norm": 3.592456102371216,
      "learning_rate": 4.559360282574568e-05,
      "loss": 0.617,
      "step": 449100
    },
    {
      "epoch": 7.051805337519623,
      "grad_norm": 4.332650184631348,
      "learning_rate": 4.559262166405024e-05,
      "loss": 0.6576,
      "step": 449200
    },
    {
      "epoch": 7.053375196232339,
      "grad_norm": 4.932799339294434,
      "learning_rate": 4.5591640502354785e-05,
      "loss": 0.6663,
      "step": 449300
    },
    {
      "epoch": 7.054945054945055,
      "grad_norm": 3.992286205291748,
      "learning_rate": 4.559065934065934e-05,
      "loss": 0.6347,
      "step": 449400
    },
    {
      "epoch": 7.056514913657771,
      "grad_norm": 3.7790536880493164,
      "learning_rate": 4.5589678178963894e-05,
      "loss": 0.6089,
      "step": 449500
    },
    {
      "epoch": 7.058084772370487,
      "grad_norm": 3.7350618839263916,
      "learning_rate": 4.558869701726845e-05,
      "loss": 0.6407,
      "step": 449600
    },
    {
      "epoch": 7.059654631083203,
      "grad_norm": 3.5086886882781982,
      "learning_rate": 4.5587715855572996e-05,
      "loss": 0.668,
      "step": 449700
    },
    {
      "epoch": 7.061224489795919,
      "grad_norm": 5.327637195587158,
      "learning_rate": 4.5586734693877554e-05,
      "loss": 0.695,
      "step": 449800
    },
    {
      "epoch": 7.062794348508635,
      "grad_norm": 4.867947101593018,
      "learning_rate": 4.5585753532182105e-05,
      "loss": 0.5825,
      "step": 449900
    },
    {
      "epoch": 7.06436420722135,
      "grad_norm": 4.154505729675293,
      "learning_rate": 4.5584772370486656e-05,
      "loss": 0.692,
      "step": 450000
    },
    {
      "epoch": 7.065934065934066,
      "grad_norm": 3.6984362602233887,
      "learning_rate": 4.558379120879121e-05,
      "loss": 0.6395,
      "step": 450100
    },
    {
      "epoch": 7.067503924646782,
      "grad_norm": 4.075893878936768,
      "learning_rate": 4.5582810047095764e-05,
      "loss": 0.6575,
      "step": 450200
    },
    {
      "epoch": 7.069073783359498,
      "grad_norm": 5.353878021240234,
      "learning_rate": 4.5581828885400315e-05,
      "loss": 0.6323,
      "step": 450300
    },
    {
      "epoch": 7.070643642072214,
      "grad_norm": 5.016134738922119,
      "learning_rate": 4.5580847723704866e-05,
      "loss": 0.6364,
      "step": 450400
    },
    {
      "epoch": 7.07221350078493,
      "grad_norm": 4.601731300354004,
      "learning_rate": 4.5579866562009424e-05,
      "loss": 0.6073,
      "step": 450500
    },
    {
      "epoch": 7.073783359497646,
      "grad_norm": 2.831864595413208,
      "learning_rate": 4.5578885400313975e-05,
      "loss": 0.6411,
      "step": 450600
    },
    {
      "epoch": 7.075353218210361,
      "grad_norm": 3.1437957286834717,
      "learning_rate": 4.5577904238618526e-05,
      "loss": 0.6512,
      "step": 450700
    },
    {
      "epoch": 7.076923076923077,
      "grad_norm": 4.868432521820068,
      "learning_rate": 4.557692307692308e-05,
      "loss": 0.6459,
      "step": 450800
    },
    {
      "epoch": 7.078492935635793,
      "grad_norm": 4.3444623947143555,
      "learning_rate": 4.5575941915227635e-05,
      "loss": 0.654,
      "step": 450900
    },
    {
      "epoch": 7.080062794348509,
      "grad_norm": 4.620891094207764,
      "learning_rate": 4.5574960753532186e-05,
      "loss": 0.6338,
      "step": 451000
    },
    {
      "epoch": 7.081632653061225,
      "grad_norm": 3.6541121006011963,
      "learning_rate": 4.557397959183674e-05,
      "loss": 0.6238,
      "step": 451100
    },
    {
      "epoch": 7.083202511773941,
      "grad_norm": 3.839050054550171,
      "learning_rate": 4.557299843014129e-05,
      "loss": 0.6514,
      "step": 451200
    },
    {
      "epoch": 7.0847723704866565,
      "grad_norm": 3.94492769241333,
      "learning_rate": 4.5572017268445846e-05,
      "loss": 0.6614,
      "step": 451300
    },
    {
      "epoch": 7.086342229199372,
      "grad_norm": 3.5937318801879883,
      "learning_rate": 4.557103610675039e-05,
      "loss": 0.6699,
      "step": 451400
    },
    {
      "epoch": 7.087912087912088,
      "grad_norm": 4.4147114753723145,
      "learning_rate": 4.557005494505495e-05,
      "loss": 0.6625,
      "step": 451500
    },
    {
      "epoch": 7.089481946624804,
      "grad_norm": 4.566666126251221,
      "learning_rate": 4.55690737833595e-05,
      "loss": 0.6425,
      "step": 451600
    },
    {
      "epoch": 7.0910518053375196,
      "grad_norm": 4.342640399932861,
      "learning_rate": 4.5568092621664056e-05,
      "loss": 0.6527,
      "step": 451700
    },
    {
      "epoch": 7.0926216640502355,
      "grad_norm": 4.137521266937256,
      "learning_rate": 4.55671114599686e-05,
      "loss": 0.6576,
      "step": 451800
    },
    {
      "epoch": 7.0941915227629515,
      "grad_norm": 2.638197422027588,
      "learning_rate": 4.556613029827316e-05,
      "loss": 0.6562,
      "step": 451900
    },
    {
      "epoch": 7.0957613814756675,
      "grad_norm": 3.155062198638916,
      "learning_rate": 4.556514913657771e-05,
      "loss": 0.6263,
      "step": 452000
    },
    {
      "epoch": 7.0973312401883835,
      "grad_norm": 2.8020145893096924,
      "learning_rate": 4.556416797488226e-05,
      "loss": 0.6395,
      "step": 452100
    },
    {
      "epoch": 7.0989010989010985,
      "grad_norm": 3.7867136001586914,
      "learning_rate": 4.556318681318681e-05,
      "loss": 0.6042,
      "step": 452200
    },
    {
      "epoch": 7.1004709576138145,
      "grad_norm": 4.144199848175049,
      "learning_rate": 4.556220565149137e-05,
      "loss": 0.6753,
      "step": 452300
    },
    {
      "epoch": 7.1020408163265305,
      "grad_norm": 3.893301248550415,
      "learning_rate": 4.556122448979592e-05,
      "loss": 0.6554,
      "step": 452400
    },
    {
      "epoch": 7.1036106750392465,
      "grad_norm": 5.076624393463135,
      "learning_rate": 4.556024332810047e-05,
      "loss": 0.6562,
      "step": 452500
    },
    {
      "epoch": 7.1051805337519625,
      "grad_norm": 4.4472527503967285,
      "learning_rate": 4.555926216640503e-05,
      "loss": 0.6412,
      "step": 452600
    },
    {
      "epoch": 7.106750392464678,
      "grad_norm": 3.6717658042907715,
      "learning_rate": 4.555828100470958e-05,
      "loss": 0.6466,
      "step": 452700
    },
    {
      "epoch": 7.108320251177394,
      "grad_norm": 4.390102386474609,
      "learning_rate": 4.555729984301413e-05,
      "loss": 0.6722,
      "step": 452800
    },
    {
      "epoch": 7.1098901098901095,
      "grad_norm": 4.303506851196289,
      "learning_rate": 4.555631868131868e-05,
      "loss": 0.648,
      "step": 452900
    },
    {
      "epoch": 7.1114599686028255,
      "grad_norm": 4.223278999328613,
      "learning_rate": 4.555533751962324e-05,
      "loss": 0.6173,
      "step": 453000
    },
    {
      "epoch": 7.1130298273155415,
      "grad_norm": 4.585992813110352,
      "learning_rate": 4.555435635792779e-05,
      "loss": 0.6882,
      "step": 453100
    },
    {
      "epoch": 7.114599686028257,
      "grad_norm": 4.6158671379089355,
      "learning_rate": 4.555337519623234e-05,
      "loss": 0.6561,
      "step": 453200
    },
    {
      "epoch": 7.116169544740973,
      "grad_norm": 3.796340227127075,
      "learning_rate": 4.555239403453689e-05,
      "loss": 0.6655,
      "step": 453300
    },
    {
      "epoch": 7.117739403453689,
      "grad_norm": 4.5097784996032715,
      "learning_rate": 4.555141287284145e-05,
      "loss": 0.636,
      "step": 453400
    },
    {
      "epoch": 7.119309262166405,
      "grad_norm": 4.579285144805908,
      "learning_rate": 4.5550431711145994e-05,
      "loss": 0.6654,
      "step": 453500
    },
    {
      "epoch": 7.1208791208791204,
      "grad_norm": 3.6261069774627686,
      "learning_rate": 4.554945054945055e-05,
      "loss": 0.6947,
      "step": 453600
    },
    {
      "epoch": 7.122448979591836,
      "grad_norm": 4.214444160461426,
      "learning_rate": 4.55484693877551e-05,
      "loss": 0.6528,
      "step": 453700
    },
    {
      "epoch": 7.124018838304552,
      "grad_norm": 2.657478094100952,
      "learning_rate": 4.554748822605966e-05,
      "loss": 0.6758,
      "step": 453800
    },
    {
      "epoch": 7.125588697017268,
      "grad_norm": 2.0179569721221924,
      "learning_rate": 4.5546507064364205e-05,
      "loss": 0.6699,
      "step": 453900
    },
    {
      "epoch": 7.127158555729984,
      "grad_norm": 2.871246576309204,
      "learning_rate": 4.554552590266876e-05,
      "loss": 0.6322,
      "step": 454000
    },
    {
      "epoch": 7.1287284144427,
      "grad_norm": 3.4957590103149414,
      "learning_rate": 4.5544544740973314e-05,
      "loss": 0.66,
      "step": 454100
    },
    {
      "epoch": 7.130298273155416,
      "grad_norm": 5.723478317260742,
      "learning_rate": 4.5543563579277865e-05,
      "loss": 0.7091,
      "step": 454200
    },
    {
      "epoch": 7.131868131868132,
      "grad_norm": 4.352814674377441,
      "learning_rate": 4.5542582417582416e-05,
      "loss": 0.6533,
      "step": 454300
    },
    {
      "epoch": 7.133437990580847,
      "grad_norm": 3.700563669204712,
      "learning_rate": 4.5541601255886973e-05,
      "loss": 0.6628,
      "step": 454400
    },
    {
      "epoch": 7.135007849293563,
      "grad_norm": 4.111011505126953,
      "learning_rate": 4.5540620094191524e-05,
      "loss": 0.6635,
      "step": 454500
    },
    {
      "epoch": 7.136577708006279,
      "grad_norm": 4.412389755249023,
      "learning_rate": 4.5539638932496075e-05,
      "loss": 0.6569,
      "step": 454600
    },
    {
      "epoch": 7.138147566718995,
      "grad_norm": 4.69755220413208,
      "learning_rate": 4.553865777080063e-05,
      "loss": 0.649,
      "step": 454700
    },
    {
      "epoch": 7.139717425431711,
      "grad_norm": 4.983943939208984,
      "learning_rate": 4.5537676609105184e-05,
      "loss": 0.6771,
      "step": 454800
    },
    {
      "epoch": 7.141287284144427,
      "grad_norm": 4.464947700500488,
      "learning_rate": 4.5536695447409735e-05,
      "loss": 0.672,
      "step": 454900
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 3.4336624145507812,
      "learning_rate": 4.5535714285714286e-05,
      "loss": 0.626,
      "step": 455000
    },
    {
      "epoch": 7.144427001569858,
      "grad_norm": 3.8764636516571045,
      "learning_rate": 4.5534733124018844e-05,
      "loss": 0.6509,
      "step": 455100
    },
    {
      "epoch": 7.145996860282574,
      "grad_norm": 3.530121326446533,
      "learning_rate": 4.5533751962323395e-05,
      "loss": 0.6664,
      "step": 455200
    },
    {
      "epoch": 7.14756671899529,
      "grad_norm": 4.1399827003479,
      "learning_rate": 4.5532770800627946e-05,
      "loss": 0.6553,
      "step": 455300
    },
    {
      "epoch": 7.149136577708006,
      "grad_norm": 2.6951076984405518,
      "learning_rate": 4.55317896389325e-05,
      "loss": 0.6121,
      "step": 455400
    },
    {
      "epoch": 7.150706436420722,
      "grad_norm": 4.4756317138671875,
      "learning_rate": 4.5530808477237055e-05,
      "loss": 0.6954,
      "step": 455500
    },
    {
      "epoch": 7.152276295133438,
      "grad_norm": 3.922863483428955,
      "learning_rate": 4.55298273155416e-05,
      "loss": 0.6694,
      "step": 455600
    },
    {
      "epoch": 7.153846153846154,
      "grad_norm": 3.9857354164123535,
      "learning_rate": 4.5528846153846157e-05,
      "loss": 0.6189,
      "step": 455700
    },
    {
      "epoch": 7.155416012558869,
      "grad_norm": 3.4237496852874756,
      "learning_rate": 4.552786499215071e-05,
      "loss": 0.6342,
      "step": 455800
    },
    {
      "epoch": 7.156985871271585,
      "grad_norm": 3.473259449005127,
      "learning_rate": 4.5526883830455265e-05,
      "loss": 0.6133,
      "step": 455900
    },
    {
      "epoch": 7.158555729984301,
      "grad_norm": 2.467921018600464,
      "learning_rate": 4.552590266875981e-05,
      "loss": 0.6348,
      "step": 456000
    },
    {
      "epoch": 7.160125588697017,
      "grad_norm": 3.2813143730163574,
      "learning_rate": 4.552492150706437e-05,
      "loss": 0.6099,
      "step": 456100
    },
    {
      "epoch": 7.161695447409733,
      "grad_norm": 3.8727526664733887,
      "learning_rate": 4.552394034536892e-05,
      "loss": 0.6592,
      "step": 456200
    },
    {
      "epoch": 7.163265306122449,
      "grad_norm": 3.044179916381836,
      "learning_rate": 4.552295918367347e-05,
      "loss": 0.6538,
      "step": 456300
    },
    {
      "epoch": 7.164835164835165,
      "grad_norm": 3.7352852821350098,
      "learning_rate": 4.552197802197802e-05,
      "loss": 0.6705,
      "step": 456400
    },
    {
      "epoch": 7.166405023547881,
      "grad_norm": 4.0863823890686035,
      "learning_rate": 4.552099686028258e-05,
      "loss": 0.6262,
      "step": 456500
    },
    {
      "epoch": 7.167974882260596,
      "grad_norm": 3.249206781387329,
      "learning_rate": 4.552001569858713e-05,
      "loss": 0.6463,
      "step": 456600
    },
    {
      "epoch": 7.169544740973312,
      "grad_norm": 4.296398639678955,
      "learning_rate": 4.551903453689168e-05,
      "loss": 0.6203,
      "step": 456700
    },
    {
      "epoch": 7.171114599686028,
      "grad_norm": 4.366339206695557,
      "learning_rate": 4.551805337519624e-05,
      "loss": 0.6387,
      "step": 456800
    },
    {
      "epoch": 7.172684458398744,
      "grad_norm": 3.6365606784820557,
      "learning_rate": 4.551707221350079e-05,
      "loss": 0.6403,
      "step": 456900
    },
    {
      "epoch": 7.17425431711146,
      "grad_norm": 4.02263069152832,
      "learning_rate": 4.551609105180534e-05,
      "loss": 0.6614,
      "step": 457000
    },
    {
      "epoch": 7.175824175824176,
      "grad_norm": 3.576890468597412,
      "learning_rate": 4.551510989010989e-05,
      "loss": 0.6496,
      "step": 457100
    },
    {
      "epoch": 7.177394034536892,
      "grad_norm": 4.5338664054870605,
      "learning_rate": 4.551412872841445e-05,
      "loss": 0.6211,
      "step": 457200
    },
    {
      "epoch": 7.178963893249607,
      "grad_norm": 2.9071195125579834,
      "learning_rate": 4.5513147566719e-05,
      "loss": 0.6478,
      "step": 457300
    },
    {
      "epoch": 7.180533751962323,
      "grad_norm": 4.878489971160889,
      "learning_rate": 4.551216640502355e-05,
      "loss": 0.6426,
      "step": 457400
    },
    {
      "epoch": 7.182103610675039,
      "grad_norm": 4.731039524078369,
      "learning_rate": 4.55111852433281e-05,
      "loss": 0.6365,
      "step": 457500
    },
    {
      "epoch": 7.183673469387755,
      "grad_norm": 3.954954147338867,
      "learning_rate": 4.551020408163266e-05,
      "loss": 0.6729,
      "step": 457600
    },
    {
      "epoch": 7.185243328100471,
      "grad_norm": 4.050992012023926,
      "learning_rate": 4.55092229199372e-05,
      "loss": 0.6423,
      "step": 457700
    },
    {
      "epoch": 7.186813186813187,
      "grad_norm": 4.051702499389648,
      "learning_rate": 4.550824175824176e-05,
      "loss": 0.6299,
      "step": 457800
    },
    {
      "epoch": 7.188383045525903,
      "grad_norm": 3.257516622543335,
      "learning_rate": 4.550726059654631e-05,
      "loss": 0.7027,
      "step": 457900
    },
    {
      "epoch": 7.189952904238618,
      "grad_norm": 2.8242087364196777,
      "learning_rate": 4.550627943485087e-05,
      "loss": 0.6272,
      "step": 458000
    },
    {
      "epoch": 7.191522762951334,
      "grad_norm": 4.642301559448242,
      "learning_rate": 4.5505298273155414e-05,
      "loss": 0.6754,
      "step": 458100
    },
    {
      "epoch": 7.19309262166405,
      "grad_norm": 3.6992502212524414,
      "learning_rate": 4.550431711145997e-05,
      "loss": 0.6292,
      "step": 458200
    },
    {
      "epoch": 7.194662480376766,
      "grad_norm": 3.3661961555480957,
      "learning_rate": 4.550333594976452e-05,
      "loss": 0.625,
      "step": 458300
    },
    {
      "epoch": 7.196232339089482,
      "grad_norm": 3.424487590789795,
      "learning_rate": 4.5502354788069074e-05,
      "loss": 0.6506,
      "step": 458400
    },
    {
      "epoch": 7.197802197802198,
      "grad_norm": 4.2632155418396,
      "learning_rate": 4.5501373626373625e-05,
      "loss": 0.6601,
      "step": 458500
    },
    {
      "epoch": 7.199372056514914,
      "grad_norm": 3.0186636447906494,
      "learning_rate": 4.550039246467818e-05,
      "loss": 0.6076,
      "step": 458600
    },
    {
      "epoch": 7.20094191522763,
      "grad_norm": 4.644141674041748,
      "learning_rate": 4.549941130298273e-05,
      "loss": 0.6539,
      "step": 458700
    },
    {
      "epoch": 7.202511773940345,
      "grad_norm": 4.827728271484375,
      "learning_rate": 4.5498430141287284e-05,
      "loss": 0.67,
      "step": 458800
    },
    {
      "epoch": 7.204081632653061,
      "grad_norm": 4.727623462677002,
      "learning_rate": 4.549744897959184e-05,
      "loss": 0.6532,
      "step": 458900
    },
    {
      "epoch": 7.205651491365777,
      "grad_norm": 3.4934921264648438,
      "learning_rate": 4.549646781789639e-05,
      "loss": 0.6537,
      "step": 459000
    },
    {
      "epoch": 7.207221350078493,
      "grad_norm": 2.5027995109558105,
      "learning_rate": 4.5495486656200944e-05,
      "loss": 0.6649,
      "step": 459100
    },
    {
      "epoch": 7.208791208791209,
      "grad_norm": 4.366940975189209,
      "learning_rate": 4.5494505494505495e-05,
      "loss": 0.638,
      "step": 459200
    },
    {
      "epoch": 7.210361067503925,
      "grad_norm": 4.606823444366455,
      "learning_rate": 4.549352433281005e-05,
      "loss": 0.6763,
      "step": 459300
    },
    {
      "epoch": 7.211930926216641,
      "grad_norm": 4.167636871337891,
      "learning_rate": 4.5492543171114604e-05,
      "loss": 0.6645,
      "step": 459400
    },
    {
      "epoch": 7.213500784929356,
      "grad_norm": 3.5406711101531982,
      "learning_rate": 4.5491562009419155e-05,
      "loss": 0.6598,
      "step": 459500
    },
    {
      "epoch": 7.215070643642072,
      "grad_norm": 3.342681884765625,
      "learning_rate": 4.5490580847723706e-05,
      "loss": 0.6666,
      "step": 459600
    },
    {
      "epoch": 7.216640502354788,
      "grad_norm": 4.7269392013549805,
      "learning_rate": 4.5489599686028264e-05,
      "loss": 0.6414,
      "step": 459700
    },
    {
      "epoch": 7.218210361067504,
      "grad_norm": 4.263563632965088,
      "learning_rate": 4.548861852433281e-05,
      "loss": 0.654,
      "step": 459800
    },
    {
      "epoch": 7.21978021978022,
      "grad_norm": 4.498409748077393,
      "learning_rate": 4.5487637362637365e-05,
      "loss": 0.7082,
      "step": 459900
    },
    {
      "epoch": 7.221350078492936,
      "grad_norm": 4.60196590423584,
      "learning_rate": 4.5486656200941916e-05,
      "loss": 0.6457,
      "step": 460000
    },
    {
      "epoch": 7.222919937205652,
      "grad_norm": 2.922403335571289,
      "learning_rate": 4.5485675039246474e-05,
      "loss": 0.6698,
      "step": 460100
    },
    {
      "epoch": 7.224489795918367,
      "grad_norm": 3.84700608253479,
      "learning_rate": 4.548469387755102e-05,
      "loss": 0.6452,
      "step": 460200
    },
    {
      "epoch": 7.226059654631083,
      "grad_norm": 3.9126453399658203,
      "learning_rate": 4.5483712715855576e-05,
      "loss": 0.6721,
      "step": 460300
    },
    {
      "epoch": 7.227629513343799,
      "grad_norm": 4.218962669372559,
      "learning_rate": 4.548273155416013e-05,
      "loss": 0.6065,
      "step": 460400
    },
    {
      "epoch": 7.229199372056515,
      "grad_norm": 3.10908579826355,
      "learning_rate": 4.548175039246468e-05,
      "loss": 0.6614,
      "step": 460500
    },
    {
      "epoch": 7.230769230769231,
      "grad_norm": 4.590944766998291,
      "learning_rate": 4.548076923076923e-05,
      "loss": 0.5926,
      "step": 460600
    },
    {
      "epoch": 7.232339089481947,
      "grad_norm": 4.204300880432129,
      "learning_rate": 4.547978806907379e-05,
      "loss": 0.705,
      "step": 460700
    },
    {
      "epoch": 7.233908948194663,
      "grad_norm": 4.49310302734375,
      "learning_rate": 4.547880690737834e-05,
      "loss": 0.6286,
      "step": 460800
    },
    {
      "epoch": 7.235478806907379,
      "grad_norm": 4.148470401763916,
      "learning_rate": 4.547782574568289e-05,
      "loss": 0.7093,
      "step": 460900
    },
    {
      "epoch": 7.237048665620094,
      "grad_norm": 4.510499000549316,
      "learning_rate": 4.5476844583987447e-05,
      "loss": 0.7037,
      "step": 461000
    },
    {
      "epoch": 7.23861852433281,
      "grad_norm": 3.500936985015869,
      "learning_rate": 4.5475863422292e-05,
      "loss": 0.665,
      "step": 461100
    },
    {
      "epoch": 7.240188383045526,
      "grad_norm": 4.545742034912109,
      "learning_rate": 4.547488226059655e-05,
      "loss": 0.582,
      "step": 461200
    },
    {
      "epoch": 7.241758241758242,
      "grad_norm": 4.989836692810059,
      "learning_rate": 4.54739010989011e-05,
      "loss": 0.6984,
      "step": 461300
    },
    {
      "epoch": 7.243328100470958,
      "grad_norm": 3.6931962966918945,
      "learning_rate": 4.547291993720566e-05,
      "loss": 0.6588,
      "step": 461400
    },
    {
      "epoch": 7.244897959183674,
      "grad_norm": 3.5564870834350586,
      "learning_rate": 4.547193877551021e-05,
      "loss": 0.6076,
      "step": 461500
    },
    {
      "epoch": 7.24646781789639,
      "grad_norm": 3.5642995834350586,
      "learning_rate": 4.547095761381476e-05,
      "loss": 0.6751,
      "step": 461600
    },
    {
      "epoch": 7.248037676609105,
      "grad_norm": 4.239251613616943,
      "learning_rate": 4.546997645211931e-05,
      "loss": 0.6456,
      "step": 461700
    },
    {
      "epoch": 7.249607535321821,
      "grad_norm": 4.786087512969971,
      "learning_rate": 4.546899529042387e-05,
      "loss": 0.6246,
      "step": 461800
    },
    {
      "epoch": 7.251177394034537,
      "grad_norm": 3.7197821140289307,
      "learning_rate": 4.546801412872841e-05,
      "loss": 0.6724,
      "step": 461900
    },
    {
      "epoch": 7.252747252747253,
      "grad_norm": 3.5612001419067383,
      "learning_rate": 4.546703296703297e-05,
      "loss": 0.6735,
      "step": 462000
    },
    {
      "epoch": 7.254317111459969,
      "grad_norm": 4.019781112670898,
      "learning_rate": 4.546605180533752e-05,
      "loss": 0.6506,
      "step": 462100
    },
    {
      "epoch": 7.255886970172685,
      "grad_norm": 3.6256351470947266,
      "learning_rate": 4.546507064364208e-05,
      "loss": 0.5972,
      "step": 462200
    },
    {
      "epoch": 7.257456828885401,
      "grad_norm": 3.789149045944214,
      "learning_rate": 4.546408948194662e-05,
      "loss": 0.6421,
      "step": 462300
    },
    {
      "epoch": 7.259026687598116,
      "grad_norm": 2.6792960166931152,
      "learning_rate": 4.546310832025118e-05,
      "loss": 0.6595,
      "step": 462400
    },
    {
      "epoch": 7.260596546310832,
      "grad_norm": 3.761608839035034,
      "learning_rate": 4.546212715855573e-05,
      "loss": 0.6129,
      "step": 462500
    },
    {
      "epoch": 7.262166405023548,
      "grad_norm": 4.249887943267822,
      "learning_rate": 4.546114599686028e-05,
      "loss": 0.6553,
      "step": 462600
    },
    {
      "epoch": 7.263736263736264,
      "grad_norm": 4.004291534423828,
      "learning_rate": 4.5460164835164834e-05,
      "loss": 0.6157,
      "step": 462700
    },
    {
      "epoch": 7.26530612244898,
      "grad_norm": 3.6143836975097656,
      "learning_rate": 4.545918367346939e-05,
      "loss": 0.621,
      "step": 462800
    },
    {
      "epoch": 7.266875981161696,
      "grad_norm": 3.013557195663452,
      "learning_rate": 4.545820251177394e-05,
      "loss": 0.6572,
      "step": 462900
    },
    {
      "epoch": 7.268445839874412,
      "grad_norm": 3.878002405166626,
      "learning_rate": 4.545722135007849e-05,
      "loss": 0.6542,
      "step": 463000
    },
    {
      "epoch": 7.270015698587127,
      "grad_norm": 3.3143861293792725,
      "learning_rate": 4.545624018838305e-05,
      "loss": 0.674,
      "step": 463100
    },
    {
      "epoch": 7.271585557299843,
      "grad_norm": 3.548609733581543,
      "learning_rate": 4.54552590266876e-05,
      "loss": 0.6887,
      "step": 463200
    },
    {
      "epoch": 7.273155416012559,
      "grad_norm": 3.960719585418701,
      "learning_rate": 4.545427786499215e-05,
      "loss": 0.641,
      "step": 463300
    },
    {
      "epoch": 7.274725274725275,
      "grad_norm": 3.7682788372039795,
      "learning_rate": 4.5453296703296704e-05,
      "loss": 0.6848,
      "step": 463400
    },
    {
      "epoch": 7.276295133437991,
      "grad_norm": 3.889812469482422,
      "learning_rate": 4.545231554160126e-05,
      "loss": 0.6744,
      "step": 463500
    },
    {
      "epoch": 7.277864992150707,
      "grad_norm": 3.151843547821045,
      "learning_rate": 4.545133437990581e-05,
      "loss": 0.6422,
      "step": 463600
    },
    {
      "epoch": 7.279434850863423,
      "grad_norm": 4.18883752822876,
      "learning_rate": 4.5450353218210364e-05,
      "loss": 0.633,
      "step": 463700
    },
    {
      "epoch": 7.2810047095761385,
      "grad_norm": 4.392208576202393,
      "learning_rate": 4.5449372056514915e-05,
      "loss": 0.6596,
      "step": 463800
    },
    {
      "epoch": 7.282574568288854,
      "grad_norm": 3.9663162231445312,
      "learning_rate": 4.544839089481947e-05,
      "loss": 0.636,
      "step": 463900
    },
    {
      "epoch": 7.28414442700157,
      "grad_norm": 4.422399044036865,
      "learning_rate": 4.544740973312402e-05,
      "loss": 0.6358,
      "step": 464000
    },
    {
      "epoch": 7.285714285714286,
      "grad_norm": 3.892031192779541,
      "learning_rate": 4.5446428571428574e-05,
      "loss": 0.6315,
      "step": 464100
    },
    {
      "epoch": 7.287284144427002,
      "grad_norm": 4.221410751342773,
      "learning_rate": 4.5445447409733125e-05,
      "loss": 0.6544,
      "step": 464200
    },
    {
      "epoch": 7.2888540031397175,
      "grad_norm": 3.7553303241729736,
      "learning_rate": 4.544446624803768e-05,
      "loss": 0.6281,
      "step": 464300
    },
    {
      "epoch": 7.2904238618524335,
      "grad_norm": 4.323093891143799,
      "learning_rate": 4.544348508634223e-05,
      "loss": 0.621,
      "step": 464400
    },
    {
      "epoch": 7.2919937205651495,
      "grad_norm": 4.13469934463501,
      "learning_rate": 4.5442503924646785e-05,
      "loss": 0.6377,
      "step": 464500
    },
    {
      "epoch": 7.293563579277865,
      "grad_norm": 3.4296984672546387,
      "learning_rate": 4.5441522762951336e-05,
      "loss": 0.6741,
      "step": 464600
    },
    {
      "epoch": 7.295133437990581,
      "grad_norm": 3.8404088020324707,
      "learning_rate": 4.544054160125589e-05,
      "loss": 0.6377,
      "step": 464700
    },
    {
      "epoch": 7.2967032967032965,
      "grad_norm": 4.8893232345581055,
      "learning_rate": 4.543956043956044e-05,
      "loss": 0.6608,
      "step": 464800
    },
    {
      "epoch": 7.2982731554160125,
      "grad_norm": 4.361140251159668,
      "learning_rate": 4.5438579277864996e-05,
      "loss": 0.6504,
      "step": 464900
    },
    {
      "epoch": 7.2998430141287285,
      "grad_norm": 4.358987808227539,
      "learning_rate": 4.543759811616955e-05,
      "loss": 0.7222,
      "step": 465000
    },
    {
      "epoch": 7.3014128728414445,
      "grad_norm": 4.108227252960205,
      "learning_rate": 4.54366169544741e-05,
      "loss": 0.6737,
      "step": 465100
    },
    {
      "epoch": 7.3029827315541604,
      "grad_norm": 4.080066204071045,
      "learning_rate": 4.5435635792778656e-05,
      "loss": 0.6235,
      "step": 465200
    },
    {
      "epoch": 7.304552590266876,
      "grad_norm": 4.1577863693237305,
      "learning_rate": 4.5434654631083207e-05,
      "loss": 0.6552,
      "step": 465300
    },
    {
      "epoch": 7.3061224489795915,
      "grad_norm": 3.547325611114502,
      "learning_rate": 4.543367346938776e-05,
      "loss": 0.681,
      "step": 465400
    },
    {
      "epoch": 7.3076923076923075,
      "grad_norm": 3.1527349948883057,
      "learning_rate": 4.543269230769231e-05,
      "loss": 0.5925,
      "step": 465500
    },
    {
      "epoch": 7.3092621664050235,
      "grad_norm": 5.386803150177002,
      "learning_rate": 4.5431711145996866e-05,
      "loss": 0.5923,
      "step": 465600
    },
    {
      "epoch": 7.310832025117739,
      "grad_norm": 2.535693407058716,
      "learning_rate": 4.543072998430142e-05,
      "loss": 0.6548,
      "step": 465700
    },
    {
      "epoch": 7.312401883830455,
      "grad_norm": 4.085943222045898,
      "learning_rate": 4.542974882260597e-05,
      "loss": 0.6395,
      "step": 465800
    },
    {
      "epoch": 7.313971742543171,
      "grad_norm": 4.914498329162598,
      "learning_rate": 4.542876766091052e-05,
      "loss": 0.6373,
      "step": 465900
    },
    {
      "epoch": 7.315541601255887,
      "grad_norm": 3.9976444244384766,
      "learning_rate": 4.542778649921508e-05,
      "loss": 0.6616,
      "step": 466000
    },
    {
      "epoch": 7.3171114599686025,
      "grad_norm": 4.047825336456299,
      "learning_rate": 4.542680533751962e-05,
      "loss": 0.6535,
      "step": 466100
    },
    {
      "epoch": 7.318681318681318,
      "grad_norm": 3.3626389503479004,
      "learning_rate": 4.542582417582418e-05,
      "loss": 0.691,
      "step": 466200
    },
    {
      "epoch": 7.320251177394034,
      "grad_norm": 4.007037162780762,
      "learning_rate": 4.542484301412873e-05,
      "loss": 0.6263,
      "step": 466300
    },
    {
      "epoch": 7.32182103610675,
      "grad_norm": 2.6462738513946533,
      "learning_rate": 4.542386185243328e-05,
      "loss": 0.6329,
      "step": 466400
    },
    {
      "epoch": 7.323390894819466,
      "grad_norm": 2.9676918983459473,
      "learning_rate": 4.542288069073783e-05,
      "loss": 0.6414,
      "step": 466500
    },
    {
      "epoch": 7.324960753532182,
      "grad_norm": 4.113656520843506,
      "learning_rate": 4.542189952904239e-05,
      "loss": 0.7108,
      "step": 466600
    },
    {
      "epoch": 7.326530612244898,
      "grad_norm": 4.742531776428223,
      "learning_rate": 4.542091836734694e-05,
      "loss": 0.6225,
      "step": 466700
    },
    {
      "epoch": 7.328100470957613,
      "grad_norm": 2.807241439819336,
      "learning_rate": 4.541993720565149e-05,
      "loss": 0.61,
      "step": 466800
    },
    {
      "epoch": 7.329670329670329,
      "grad_norm": 3.8619470596313477,
      "learning_rate": 4.541895604395604e-05,
      "loss": 0.6592,
      "step": 466900
    },
    {
      "epoch": 7.331240188383045,
      "grad_norm": 3.636404037475586,
      "learning_rate": 4.54179748822606e-05,
      "loss": 0.6899,
      "step": 467000
    },
    {
      "epoch": 7.332810047095761,
      "grad_norm": 5.231335639953613,
      "learning_rate": 4.541699372056515e-05,
      "loss": 0.6688,
      "step": 467100
    },
    {
      "epoch": 7.334379905808477,
      "grad_norm": 3.812676429748535,
      "learning_rate": 4.54160125588697e-05,
      "loss": 0.6719,
      "step": 467200
    },
    {
      "epoch": 7.335949764521193,
      "grad_norm": 3.7361204624176025,
      "learning_rate": 4.541503139717426e-05,
      "loss": 0.6392,
      "step": 467300
    },
    {
      "epoch": 7.337519623233909,
      "grad_norm": 4.252860069274902,
      "learning_rate": 4.541405023547881e-05,
      "loss": 0.6319,
      "step": 467400
    },
    {
      "epoch": 7.339089481946624,
      "grad_norm": 4.239567279815674,
      "learning_rate": 4.541306907378336e-05,
      "loss": 0.6355,
      "step": 467500
    },
    {
      "epoch": 7.34065934065934,
      "grad_norm": 3.9657375812530518,
      "learning_rate": 4.541208791208791e-05,
      "loss": 0.6686,
      "step": 467600
    },
    {
      "epoch": 7.342229199372056,
      "grad_norm": 3.305171012878418,
      "learning_rate": 4.541110675039247e-05,
      "loss": 0.6655,
      "step": 467700
    },
    {
      "epoch": 7.343799058084772,
      "grad_norm": 3.5907716751098633,
      "learning_rate": 4.5410125588697015e-05,
      "loss": 0.6445,
      "step": 467800
    },
    {
      "epoch": 7.345368916797488,
      "grad_norm": 3.8187410831451416,
      "learning_rate": 4.540914442700157e-05,
      "loss": 0.6559,
      "step": 467900
    },
    {
      "epoch": 7.346938775510204,
      "grad_norm": 3.0889599323272705,
      "learning_rate": 4.5408163265306124e-05,
      "loss": 0.6587,
      "step": 468000
    },
    {
      "epoch": 7.34850863422292,
      "grad_norm": 4.093754291534424,
      "learning_rate": 4.540718210361068e-05,
      "loss": 0.6849,
      "step": 468100
    },
    {
      "epoch": 7.350078492935636,
      "grad_norm": 3.516540765762329,
      "learning_rate": 4.5406200941915226e-05,
      "loss": 0.6639,
      "step": 468200
    },
    {
      "epoch": 7.351648351648351,
      "grad_norm": 4.2896409034729,
      "learning_rate": 4.5405219780219783e-05,
      "loss": 0.6547,
      "step": 468300
    },
    {
      "epoch": 7.353218210361067,
      "grad_norm": 4.003179550170898,
      "learning_rate": 4.5404238618524334e-05,
      "loss": 0.6748,
      "step": 468400
    },
    {
      "epoch": 7.354788069073783,
      "grad_norm": 3.905810832977295,
      "learning_rate": 4.5403257456828885e-05,
      "loss": 0.6538,
      "step": 468500
    },
    {
      "epoch": 7.356357927786499,
      "grad_norm": 4.693512439727783,
      "learning_rate": 4.5402276295133436e-05,
      "loss": 0.6188,
      "step": 468600
    },
    {
      "epoch": 7.357927786499215,
      "grad_norm": 4.339682102203369,
      "learning_rate": 4.5401295133437994e-05,
      "loss": 0.6542,
      "step": 468700
    },
    {
      "epoch": 7.359497645211931,
      "grad_norm": 3.9683094024658203,
      "learning_rate": 4.5400313971742545e-05,
      "loss": 0.6376,
      "step": 468800
    },
    {
      "epoch": 7.361067503924647,
      "grad_norm": 4.3864006996154785,
      "learning_rate": 4.5399332810047096e-05,
      "loss": 0.6182,
      "step": 468900
    },
    {
      "epoch": 7.362637362637362,
      "grad_norm": 3.741731882095337,
      "learning_rate": 4.539835164835165e-05,
      "loss": 0.6449,
      "step": 469000
    },
    {
      "epoch": 7.364207221350078,
      "grad_norm": 3.519243001937866,
      "learning_rate": 4.5397370486656205e-05,
      "loss": 0.6634,
      "step": 469100
    },
    {
      "epoch": 7.365777080062794,
      "grad_norm": 3.913429021835327,
      "learning_rate": 4.5396389324960756e-05,
      "loss": 0.6524,
      "step": 469200
    },
    {
      "epoch": 7.36734693877551,
      "grad_norm": 3.0331814289093018,
      "learning_rate": 4.539540816326531e-05,
      "loss": 0.6491,
      "step": 469300
    },
    {
      "epoch": 7.368916797488226,
      "grad_norm": 4.39706563949585,
      "learning_rate": 4.5394427001569865e-05,
      "loss": 0.6922,
      "step": 469400
    },
    {
      "epoch": 7.370486656200942,
      "grad_norm": 2.9792239665985107,
      "learning_rate": 4.5393445839874416e-05,
      "loss": 0.6293,
      "step": 469500
    },
    {
      "epoch": 7.372056514913658,
      "grad_norm": 4.42266845703125,
      "learning_rate": 4.5392464678178967e-05,
      "loss": 0.6609,
      "step": 469600
    },
    {
      "epoch": 7.373626373626374,
      "grad_norm": 6.428913116455078,
      "learning_rate": 4.539148351648352e-05,
      "loss": 0.629,
      "step": 469700
    },
    {
      "epoch": 7.375196232339089,
      "grad_norm": 3.935105800628662,
      "learning_rate": 4.5390502354788075e-05,
      "loss": 0.635,
      "step": 469800
    },
    {
      "epoch": 7.376766091051805,
      "grad_norm": 4.162189960479736,
      "learning_rate": 4.538952119309262e-05,
      "loss": 0.6289,
      "step": 469900
    },
    {
      "epoch": 7.378335949764521,
      "grad_norm": 3.3749852180480957,
      "learning_rate": 4.538854003139718e-05,
      "loss": 0.6517,
      "step": 470000
    },
    {
      "epoch": 7.379905808477237,
      "grad_norm": 4.207951068878174,
      "learning_rate": 4.538755886970173e-05,
      "loss": 0.6559,
      "step": 470100
    },
    {
      "epoch": 7.381475667189953,
      "grad_norm": 4.971065044403076,
      "learning_rate": 4.5386577708006286e-05,
      "loss": 0.6424,
      "step": 470200
    },
    {
      "epoch": 7.383045525902669,
      "grad_norm": 4.1807026863098145,
      "learning_rate": 4.538559654631083e-05,
      "loss": 0.655,
      "step": 470300
    },
    {
      "epoch": 7.384615384615385,
      "grad_norm": 4.1094536781311035,
      "learning_rate": 4.538461538461539e-05,
      "loss": 0.6594,
      "step": 470400
    },
    {
      "epoch": 7.3861852433281,
      "grad_norm": 3.58874249458313,
      "learning_rate": 4.538363422291994e-05,
      "loss": 0.6246,
      "step": 470500
    },
    {
      "epoch": 7.387755102040816,
      "grad_norm": 4.665841102600098,
      "learning_rate": 4.538265306122449e-05,
      "loss": 0.6667,
      "step": 470600
    },
    {
      "epoch": 7.389324960753532,
      "grad_norm": 3.103484869003296,
      "learning_rate": 4.538167189952904e-05,
      "loss": 0.6363,
      "step": 470700
    },
    {
      "epoch": 7.390894819466248,
      "grad_norm": 3.5496017932891846,
      "learning_rate": 4.53806907378336e-05,
      "loss": 0.6777,
      "step": 470800
    },
    {
      "epoch": 7.392464678178964,
      "grad_norm": 3.9693915843963623,
      "learning_rate": 4.537970957613815e-05,
      "loss": 0.662,
      "step": 470900
    },
    {
      "epoch": 7.39403453689168,
      "grad_norm": 3.8371026515960693,
      "learning_rate": 4.53787284144427e-05,
      "loss": 0.6709,
      "step": 471000
    },
    {
      "epoch": 7.395604395604396,
      "grad_norm": 4.26023530960083,
      "learning_rate": 4.537774725274725e-05,
      "loss": 0.6662,
      "step": 471100
    },
    {
      "epoch": 7.397174254317111,
      "grad_norm": 3.346737861633301,
      "learning_rate": 4.537676609105181e-05,
      "loss": 0.6339,
      "step": 471200
    },
    {
      "epoch": 7.398744113029827,
      "grad_norm": 4.2112932205200195,
      "learning_rate": 4.537578492935636e-05,
      "loss": 0.6047,
      "step": 471300
    },
    {
      "epoch": 7.400313971742543,
      "grad_norm": 3.8139383792877197,
      "learning_rate": 4.537480376766091e-05,
      "loss": 0.6165,
      "step": 471400
    },
    {
      "epoch": 7.401883830455259,
      "grad_norm": 4.257140636444092,
      "learning_rate": 4.537382260596547e-05,
      "loss": 0.6504,
      "step": 471500
    },
    {
      "epoch": 7.403453689167975,
      "grad_norm": 4.199419021606445,
      "learning_rate": 4.537284144427002e-05,
      "loss": 0.663,
      "step": 471600
    },
    {
      "epoch": 7.405023547880691,
      "grad_norm": 4.478989601135254,
      "learning_rate": 4.537186028257457e-05,
      "loss": 0.6685,
      "step": 471700
    },
    {
      "epoch": 7.406593406593407,
      "grad_norm": 3.802480697631836,
      "learning_rate": 4.537087912087912e-05,
      "loss": 0.6094,
      "step": 471800
    },
    {
      "epoch": 7.408163265306122,
      "grad_norm": 3.9003472328186035,
      "learning_rate": 4.536989795918368e-05,
      "loss": 0.6396,
      "step": 471900
    },
    {
      "epoch": 7.409733124018838,
      "grad_norm": 4.06011962890625,
      "learning_rate": 4.5368916797488224e-05,
      "loss": 0.644,
      "step": 472000
    },
    {
      "epoch": 7.411302982731554,
      "grad_norm": 5.070458889007568,
      "learning_rate": 4.536793563579278e-05,
      "loss": 0.617,
      "step": 472100
    },
    {
      "epoch": 7.41287284144427,
      "grad_norm": 4.825700283050537,
      "learning_rate": 4.536695447409733e-05,
      "loss": 0.6792,
      "step": 472200
    },
    {
      "epoch": 7.414442700156986,
      "grad_norm": 3.6212563514709473,
      "learning_rate": 4.536597331240189e-05,
      "loss": 0.6626,
      "step": 472300
    },
    {
      "epoch": 7.416012558869702,
      "grad_norm": 4.982387065887451,
      "learning_rate": 4.5364992150706435e-05,
      "loss": 0.6262,
      "step": 472400
    },
    {
      "epoch": 7.417582417582418,
      "grad_norm": 4.491528034210205,
      "learning_rate": 4.536401098901099e-05,
      "loss": 0.6266,
      "step": 472500
    },
    {
      "epoch": 7.419152276295134,
      "grad_norm": 4.205935955047607,
      "learning_rate": 4.5363029827315543e-05,
      "loss": 0.6468,
      "step": 472600
    },
    {
      "epoch": 7.420722135007849,
      "grad_norm": 4.068990230560303,
      "learning_rate": 4.5362048665620094e-05,
      "loss": 0.6761,
      "step": 472700
    },
    {
      "epoch": 7.422291993720565,
      "grad_norm": 4.216148376464844,
      "learning_rate": 4.5361067503924645e-05,
      "loss": 0.6474,
      "step": 472800
    },
    {
      "epoch": 7.423861852433281,
      "grad_norm": 3.833446741104126,
      "learning_rate": 4.53600863422292e-05,
      "loss": 0.6186,
      "step": 472900
    },
    {
      "epoch": 7.425431711145997,
      "grad_norm": 3.546818256378174,
      "learning_rate": 4.5359105180533754e-05,
      "loss": 0.6595,
      "step": 473000
    },
    {
      "epoch": 7.427001569858713,
      "grad_norm": 2.915801763534546,
      "learning_rate": 4.5358124018838305e-05,
      "loss": 0.6426,
      "step": 473100
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 4.022762298583984,
      "learning_rate": 4.5357142857142856e-05,
      "loss": 0.6633,
      "step": 473200
    },
    {
      "epoch": 7.430141287284145,
      "grad_norm": 3.8851354122161865,
      "learning_rate": 4.5356161695447414e-05,
      "loss": 0.6246,
      "step": 473300
    },
    {
      "epoch": 7.43171114599686,
      "grad_norm": 4.38795280456543,
      "learning_rate": 4.5355180533751965e-05,
      "loss": 0.6424,
      "step": 473400
    },
    {
      "epoch": 7.433281004709576,
      "grad_norm": 2.7769322395324707,
      "learning_rate": 4.5354199372056516e-05,
      "loss": 0.6039,
      "step": 473500
    },
    {
      "epoch": 7.434850863422292,
      "grad_norm": 4.201911926269531,
      "learning_rate": 4.5353218210361074e-05,
      "loss": 0.678,
      "step": 473600
    },
    {
      "epoch": 7.436420722135008,
      "grad_norm": 4.0484466552734375,
      "learning_rate": 4.5352237048665625e-05,
      "loss": 0.6607,
      "step": 473700
    },
    {
      "epoch": 7.437990580847724,
      "grad_norm": 3.8108224868774414,
      "learning_rate": 4.5351255886970176e-05,
      "loss": 0.6349,
      "step": 473800
    },
    {
      "epoch": 7.43956043956044,
      "grad_norm": 4.396463394165039,
      "learning_rate": 4.5350274725274726e-05,
      "loss": 0.6759,
      "step": 473900
    },
    {
      "epoch": 7.441130298273156,
      "grad_norm": 4.635012149810791,
      "learning_rate": 4.5349293563579284e-05,
      "loss": 0.6774,
      "step": 474000
    },
    {
      "epoch": 7.442700156985872,
      "grad_norm": 5.11631441116333,
      "learning_rate": 4.534831240188383e-05,
      "loss": 0.6327,
      "step": 474100
    },
    {
      "epoch": 7.444270015698587,
      "grad_norm": 4.356932640075684,
      "learning_rate": 4.5347331240188386e-05,
      "loss": 0.6095,
      "step": 474200
    },
    {
      "epoch": 7.445839874411303,
      "grad_norm": 3.729050397872925,
      "learning_rate": 4.534635007849294e-05,
      "loss": 0.6345,
      "step": 474300
    },
    {
      "epoch": 7.447409733124019,
      "grad_norm": 4.683655738830566,
      "learning_rate": 4.5345368916797495e-05,
      "loss": 0.6304,
      "step": 474400
    },
    {
      "epoch": 7.448979591836735,
      "grad_norm": 4.921781063079834,
      "learning_rate": 4.534438775510204e-05,
      "loss": 0.598,
      "step": 474500
    },
    {
      "epoch": 7.450549450549451,
      "grad_norm": 3.8415212631225586,
      "learning_rate": 4.53434065934066e-05,
      "loss": 0.6083,
      "step": 474600
    },
    {
      "epoch": 7.452119309262167,
      "grad_norm": 2.6411654949188232,
      "learning_rate": 4.534242543171115e-05,
      "loss": 0.6763,
      "step": 474700
    },
    {
      "epoch": 7.453689167974883,
      "grad_norm": 3.4228641986846924,
      "learning_rate": 4.53414442700157e-05,
      "loss": 0.6687,
      "step": 474800
    },
    {
      "epoch": 7.455259026687598,
      "grad_norm": 3.529489278793335,
      "learning_rate": 4.534046310832025e-05,
      "loss": 0.5951,
      "step": 474900
    },
    {
      "epoch": 7.456828885400314,
      "grad_norm": 4.369470119476318,
      "learning_rate": 4.533948194662481e-05,
      "loss": 0.6607,
      "step": 475000
    },
    {
      "epoch": 7.45839874411303,
      "grad_norm": 2.572601795196533,
      "learning_rate": 4.533850078492936e-05,
      "loss": 0.6828,
      "step": 475100
    },
    {
      "epoch": 7.459968602825746,
      "grad_norm": 3.871875047683716,
      "learning_rate": 4.533751962323391e-05,
      "loss": 0.6836,
      "step": 475200
    },
    {
      "epoch": 7.461538461538462,
      "grad_norm": 3.322723865509033,
      "learning_rate": 4.533653846153846e-05,
      "loss": 0.6538,
      "step": 475300
    },
    {
      "epoch": 7.463108320251178,
      "grad_norm": 5.060608863830566,
      "learning_rate": 4.533555729984302e-05,
      "loss": 0.6618,
      "step": 475400
    },
    {
      "epoch": 7.464678178963894,
      "grad_norm": 3.6595566272735596,
      "learning_rate": 4.533457613814757e-05,
      "loss": 0.6797,
      "step": 475500
    },
    {
      "epoch": 7.466248037676609,
      "grad_norm": 2.43814754486084,
      "learning_rate": 4.533359497645212e-05,
      "loss": 0.6206,
      "step": 475600
    },
    {
      "epoch": 7.467817896389325,
      "grad_norm": 4.165511131286621,
      "learning_rate": 4.533261381475668e-05,
      "loss": 0.6151,
      "step": 475700
    },
    {
      "epoch": 7.469387755102041,
      "grad_norm": 3.7338080406188965,
      "learning_rate": 4.533163265306123e-05,
      "loss": 0.6698,
      "step": 475800
    },
    {
      "epoch": 7.470957613814757,
      "grad_norm": 4.448345184326172,
      "learning_rate": 4.533065149136578e-05,
      "loss": 0.6281,
      "step": 475900
    },
    {
      "epoch": 7.472527472527473,
      "grad_norm": 3.8401389122009277,
      "learning_rate": 4.532967032967033e-05,
      "loss": 0.6353,
      "step": 476000
    },
    {
      "epoch": 7.474097331240189,
      "grad_norm": 4.633065223693848,
      "learning_rate": 4.532868916797489e-05,
      "loss": 0.6569,
      "step": 476100
    },
    {
      "epoch": 7.475667189952905,
      "grad_norm": 4.629982948303223,
      "learning_rate": 4.532770800627943e-05,
      "loss": 0.6648,
      "step": 476200
    },
    {
      "epoch": 7.47723704866562,
      "grad_norm": 3.733835458755493,
      "learning_rate": 4.532672684458399e-05,
      "loss": 0.6434,
      "step": 476300
    },
    {
      "epoch": 7.478806907378336,
      "grad_norm": 3.3399040699005127,
      "learning_rate": 4.532574568288854e-05,
      "loss": 0.6001,
      "step": 476400
    },
    {
      "epoch": 7.480376766091052,
      "grad_norm": 3.80926251411438,
      "learning_rate": 4.53247645211931e-05,
      "loss": 0.6255,
      "step": 476500
    },
    {
      "epoch": 7.481946624803768,
      "grad_norm": 3.938115119934082,
      "learning_rate": 4.5323783359497644e-05,
      "loss": 0.6493,
      "step": 476600
    },
    {
      "epoch": 7.483516483516484,
      "grad_norm": 4.623208999633789,
      "learning_rate": 4.53228021978022e-05,
      "loss": 0.6729,
      "step": 476700
    },
    {
      "epoch": 7.4850863422291996,
      "grad_norm": 4.608475208282471,
      "learning_rate": 4.532182103610675e-05,
      "loss": 0.6334,
      "step": 476800
    },
    {
      "epoch": 7.4866562009419155,
      "grad_norm": 4.20109224319458,
      "learning_rate": 4.53208398744113e-05,
      "loss": 0.6164,
      "step": 476900
    },
    {
      "epoch": 7.488226059654631,
      "grad_norm": 3.6985058784484863,
      "learning_rate": 4.5319858712715854e-05,
      "loss": 0.6624,
      "step": 477000
    },
    {
      "epoch": 7.489795918367347,
      "grad_norm": 3.416900634765625,
      "learning_rate": 4.531887755102041e-05,
      "loss": 0.6317,
      "step": 477100
    },
    {
      "epoch": 7.491365777080063,
      "grad_norm": 4.3965020179748535,
      "learning_rate": 4.531789638932496e-05,
      "loss": 0.6418,
      "step": 477200
    },
    {
      "epoch": 7.4929356357927785,
      "grad_norm": 2.89514422416687,
      "learning_rate": 4.5316915227629514e-05,
      "loss": 0.6977,
      "step": 477300
    },
    {
      "epoch": 7.4945054945054945,
      "grad_norm": 3.7151083946228027,
      "learning_rate": 4.5315934065934065e-05,
      "loss": 0.6625,
      "step": 477400
    },
    {
      "epoch": 7.4960753532182105,
      "grad_norm": 3.0576725006103516,
      "learning_rate": 4.531495290423862e-05,
      "loss": 0.6273,
      "step": 477500
    },
    {
      "epoch": 7.4976452119309265,
      "grad_norm": 2.7338545322418213,
      "learning_rate": 4.5313971742543174e-05,
      "loss": 0.6822,
      "step": 477600
    },
    {
      "epoch": 7.4992150706436425,
      "grad_norm": 5.618361949920654,
      "learning_rate": 4.5312990580847725e-05,
      "loss": 0.6424,
      "step": 477700
    },
    {
      "epoch": 7.5007849293563575,
      "grad_norm": 3.0387072563171387,
      "learning_rate": 4.531200941915228e-05,
      "loss": 0.6341,
      "step": 477800
    },
    {
      "epoch": 7.5023547880690735,
      "grad_norm": 3.5874173641204834,
      "learning_rate": 4.5311028257456834e-05,
      "loss": 0.6704,
      "step": 477900
    },
    {
      "epoch": 7.5039246467817895,
      "grad_norm": 2.6424500942230225,
      "learning_rate": 4.5310047095761384e-05,
      "loss": 0.6446,
      "step": 478000
    },
    {
      "epoch": 7.5054945054945055,
      "grad_norm": 4.102578639984131,
      "learning_rate": 4.5309065934065935e-05,
      "loss": 0.6687,
      "step": 478100
    },
    {
      "epoch": 7.5070643642072215,
      "grad_norm": 3.676912307739258,
      "learning_rate": 4.530808477237049e-05,
      "loss": 0.6343,
      "step": 478200
    },
    {
      "epoch": 7.508634222919937,
      "grad_norm": 5.124135971069336,
      "learning_rate": 4.530710361067504e-05,
      "loss": 0.6644,
      "step": 478300
    },
    {
      "epoch": 7.510204081632653,
      "grad_norm": 3.7105367183685303,
      "learning_rate": 4.5306122448979595e-05,
      "loss": 0.6405,
      "step": 478400
    },
    {
      "epoch": 7.511773940345369,
      "grad_norm": 3.6658217906951904,
      "learning_rate": 4.5305141287284146e-05,
      "loss": 0.6187,
      "step": 478500
    },
    {
      "epoch": 7.5133437990580845,
      "grad_norm": 3.7219855785369873,
      "learning_rate": 4.5304160125588704e-05,
      "loss": 0.6609,
      "step": 478600
    },
    {
      "epoch": 7.5149136577708004,
      "grad_norm": 3.47794508934021,
      "learning_rate": 4.530317896389325e-05,
      "loss": 0.6308,
      "step": 478700
    },
    {
      "epoch": 7.516483516483516,
      "grad_norm": 4.265655994415283,
      "learning_rate": 4.5302197802197806e-05,
      "loss": 0.6376,
      "step": 478800
    },
    {
      "epoch": 7.518053375196232,
      "grad_norm": 3.2142043113708496,
      "learning_rate": 4.530121664050236e-05,
      "loss": 0.6448,
      "step": 478900
    },
    {
      "epoch": 7.519623233908948,
      "grad_norm": 2.881437063217163,
      "learning_rate": 4.530023547880691e-05,
      "loss": 0.6141,
      "step": 479000
    },
    {
      "epoch": 7.521193092621664,
      "grad_norm": 4.316728115081787,
      "learning_rate": 4.529925431711146e-05,
      "loss": 0.6543,
      "step": 479100
    },
    {
      "epoch": 7.52276295133438,
      "grad_norm": 3.070465564727783,
      "learning_rate": 4.5298273155416017e-05,
      "loss": 0.6398,
      "step": 479200
    },
    {
      "epoch": 7.524332810047095,
      "grad_norm": 3.083333730697632,
      "learning_rate": 4.529729199372057e-05,
      "loss": 0.6372,
      "step": 479300
    },
    {
      "epoch": 7.525902668759811,
      "grad_norm": 3.795051097869873,
      "learning_rate": 4.529631083202512e-05,
      "loss": 0.665,
      "step": 479400
    },
    {
      "epoch": 7.527472527472527,
      "grad_norm": 5.0198140144348145,
      "learning_rate": 4.529532967032967e-05,
      "loss": 0.6191,
      "step": 479500
    },
    {
      "epoch": 7.529042386185243,
      "grad_norm": 4.670448303222656,
      "learning_rate": 4.529434850863423e-05,
      "loss": 0.6865,
      "step": 479600
    },
    {
      "epoch": 7.530612244897959,
      "grad_norm": 4.3407511711120605,
      "learning_rate": 4.529336734693878e-05,
      "loss": 0.6661,
      "step": 479700
    },
    {
      "epoch": 7.532182103610675,
      "grad_norm": 4.204563617706299,
      "learning_rate": 4.529238618524333e-05,
      "loss": 0.6319,
      "step": 479800
    },
    {
      "epoch": 7.533751962323391,
      "grad_norm": 4.108593940734863,
      "learning_rate": 4.529140502354789e-05,
      "loss": 0.6523,
      "step": 479900
    },
    {
      "epoch": 7.535321821036106,
      "grad_norm": 2.339252233505249,
      "learning_rate": 4.529042386185244e-05,
      "loss": 0.6214,
      "step": 480000
    },
    {
      "epoch": 7.536891679748822,
      "grad_norm": 4.225778102874756,
      "learning_rate": 4.528944270015699e-05,
      "loss": 0.6779,
      "step": 480100
    },
    {
      "epoch": 7.538461538461538,
      "grad_norm": 4.514763355255127,
      "learning_rate": 4.528846153846154e-05,
      "loss": 0.6373,
      "step": 480200
    },
    {
      "epoch": 7.540031397174254,
      "grad_norm": 4.224471092224121,
      "learning_rate": 4.52874803767661e-05,
      "loss": 0.6318,
      "step": 480300
    },
    {
      "epoch": 7.54160125588697,
      "grad_norm": 3.9412248134613037,
      "learning_rate": 4.528649921507064e-05,
      "loss": 0.6592,
      "step": 480400
    },
    {
      "epoch": 7.543171114599686,
      "grad_norm": 4.244016170501709,
      "learning_rate": 4.52855180533752e-05,
      "loss": 0.6292,
      "step": 480500
    },
    {
      "epoch": 7.544740973312402,
      "grad_norm": 2.9770209789276123,
      "learning_rate": 4.528453689167975e-05,
      "loss": 0.6645,
      "step": 480600
    },
    {
      "epoch": 7.546310832025117,
      "grad_norm": 3.9249093532562256,
      "learning_rate": 4.528355572998431e-05,
      "loss": 0.6376,
      "step": 480700
    },
    {
      "epoch": 7.547880690737833,
      "grad_norm": 5.3057332038879395,
      "learning_rate": 4.528257456828885e-05,
      "loss": 0.6636,
      "step": 480800
    },
    {
      "epoch": 7.549450549450549,
      "grad_norm": 3.2127623558044434,
      "learning_rate": 4.528159340659341e-05,
      "loss": 0.6204,
      "step": 480900
    },
    {
      "epoch": 7.551020408163265,
      "grad_norm": 3.991739511489868,
      "learning_rate": 4.528061224489796e-05,
      "loss": 0.6538,
      "step": 481000
    },
    {
      "epoch": 7.552590266875981,
      "grad_norm": 4.497997760772705,
      "learning_rate": 4.527963108320251e-05,
      "loss": 0.6467,
      "step": 481100
    },
    {
      "epoch": 7.554160125588697,
      "grad_norm": 3.805983066558838,
      "learning_rate": 4.527864992150706e-05,
      "loss": 0.6888,
      "step": 481200
    },
    {
      "epoch": 7.555729984301413,
      "grad_norm": 4.852888584136963,
      "learning_rate": 4.527766875981162e-05,
      "loss": 0.6733,
      "step": 481300
    },
    {
      "epoch": 7.557299843014128,
      "grad_norm": 2.6775424480438232,
      "learning_rate": 4.527668759811617e-05,
      "loss": 0.6562,
      "step": 481400
    },
    {
      "epoch": 7.558869701726844,
      "grad_norm": 4.206821441650391,
      "learning_rate": 4.527570643642072e-05,
      "loss": 0.6048,
      "step": 481500
    },
    {
      "epoch": 7.56043956043956,
      "grad_norm": 2.8423948287963867,
      "learning_rate": 4.5274725274725274e-05,
      "loss": 0.6319,
      "step": 481600
    },
    {
      "epoch": 7.562009419152276,
      "grad_norm": 3.9187142848968506,
      "learning_rate": 4.527374411302983e-05,
      "loss": 0.6476,
      "step": 481700
    },
    {
      "epoch": 7.563579277864992,
      "grad_norm": 3.5776753425598145,
      "learning_rate": 4.527276295133438e-05,
      "loss": 0.6409,
      "step": 481800
    },
    {
      "epoch": 7.565149136577708,
      "grad_norm": 4.332414150238037,
      "learning_rate": 4.5271781789638934e-05,
      "loss": 0.6468,
      "step": 481900
    },
    {
      "epoch": 7.566718995290424,
      "grad_norm": 4.393723964691162,
      "learning_rate": 4.527080062794349e-05,
      "loss": 0.6601,
      "step": 482000
    },
    {
      "epoch": 7.568288854003139,
      "grad_norm": 3.813499689102173,
      "learning_rate": 4.526981946624804e-05,
      "loss": 0.6654,
      "step": 482100
    },
    {
      "epoch": 7.569858712715855,
      "grad_norm": 2.9579925537109375,
      "learning_rate": 4.5268838304552593e-05,
      "loss": 0.6457,
      "step": 482200
    },
    {
      "epoch": 7.571428571428571,
      "grad_norm": 3.898155450820923,
      "learning_rate": 4.5267857142857144e-05,
      "loss": 0.6154,
      "step": 482300
    },
    {
      "epoch": 7.572998430141287,
      "grad_norm": 3.828228235244751,
      "learning_rate": 4.52668759811617e-05,
      "loss": 0.6436,
      "step": 482400
    },
    {
      "epoch": 7.574568288854003,
      "grad_norm": 3.8732028007507324,
      "learning_rate": 4.5265894819466246e-05,
      "loss": 0.6188,
      "step": 482500
    },
    {
      "epoch": 7.576138147566719,
      "grad_norm": 4.896740913391113,
      "learning_rate": 4.5264913657770804e-05,
      "loss": 0.628,
      "step": 482600
    },
    {
      "epoch": 7.577708006279435,
      "grad_norm": 3.720137119293213,
      "learning_rate": 4.5263932496075355e-05,
      "loss": 0.6381,
      "step": 482700
    },
    {
      "epoch": 7.579277864992151,
      "grad_norm": 3.0683085918426514,
      "learning_rate": 4.526295133437991e-05,
      "loss": 0.6197,
      "step": 482800
    },
    {
      "epoch": 7.580847723704867,
      "grad_norm": 4.0870585441589355,
      "learning_rate": 4.526197017268446e-05,
      "loss": 0.6644,
      "step": 482900
    },
    {
      "epoch": 7.582417582417582,
      "grad_norm": 3.719151496887207,
      "learning_rate": 4.5260989010989015e-05,
      "loss": 0.6454,
      "step": 483000
    },
    {
      "epoch": 7.583987441130298,
      "grad_norm": 3.7578811645507812,
      "learning_rate": 4.5260007849293566e-05,
      "loss": 0.6348,
      "step": 483100
    },
    {
      "epoch": 7.585557299843014,
      "grad_norm": 4.585264682769775,
      "learning_rate": 4.525902668759812e-05,
      "loss": 0.5852,
      "step": 483200
    },
    {
      "epoch": 7.58712715855573,
      "grad_norm": 3.391922950744629,
      "learning_rate": 4.525804552590267e-05,
      "loss": 0.6312,
      "step": 483300
    },
    {
      "epoch": 7.588697017268446,
      "grad_norm": 4.920281410217285,
      "learning_rate": 4.5257064364207226e-05,
      "loss": 0.6812,
      "step": 483400
    },
    {
      "epoch": 7.590266875981162,
      "grad_norm": 4.021872043609619,
      "learning_rate": 4.5256083202511777e-05,
      "loss": 0.6455,
      "step": 483500
    },
    {
      "epoch": 7.591836734693878,
      "grad_norm": 4.286561012268066,
      "learning_rate": 4.525510204081633e-05,
      "loss": 0.6393,
      "step": 483600
    },
    {
      "epoch": 7.593406593406593,
      "grad_norm": 3.3233375549316406,
      "learning_rate": 4.525412087912088e-05,
      "loss": 0.6443,
      "step": 483700
    },
    {
      "epoch": 7.594976452119309,
      "grad_norm": 4.5823493003845215,
      "learning_rate": 4.5253139717425436e-05,
      "loss": 0.6385,
      "step": 483800
    },
    {
      "epoch": 7.596546310832025,
      "grad_norm": 4.647805213928223,
      "learning_rate": 4.525215855572999e-05,
      "loss": 0.6398,
      "step": 483900
    },
    {
      "epoch": 7.598116169544741,
      "grad_norm": 3.1560637950897217,
      "learning_rate": 4.525117739403454e-05,
      "loss": 0.6024,
      "step": 484000
    },
    {
      "epoch": 7.599686028257457,
      "grad_norm": 4.422451496124268,
      "learning_rate": 4.5250196232339096e-05,
      "loss": 0.6487,
      "step": 484100
    },
    {
      "epoch": 7.601255886970173,
      "grad_norm": 5.033768653869629,
      "learning_rate": 4.524921507064365e-05,
      "loss": 0.6643,
      "step": 484200
    },
    {
      "epoch": 7.602825745682889,
      "grad_norm": 2.9654648303985596,
      "learning_rate": 4.52482339089482e-05,
      "loss": 0.6731,
      "step": 484300
    },
    {
      "epoch": 7.604395604395604,
      "grad_norm": 3.7007224559783936,
      "learning_rate": 4.524725274725275e-05,
      "loss": 0.6463,
      "step": 484400
    },
    {
      "epoch": 7.60596546310832,
      "grad_norm": 3.0168607234954834,
      "learning_rate": 4.524627158555731e-05,
      "loss": 0.6504,
      "step": 484500
    },
    {
      "epoch": 7.607535321821036,
      "grad_norm": 4.358815670013428,
      "learning_rate": 4.524529042386185e-05,
      "loss": 0.6658,
      "step": 484600
    },
    {
      "epoch": 7.609105180533752,
      "grad_norm": 3.18831729888916,
      "learning_rate": 4.524430926216641e-05,
      "loss": 0.6742,
      "step": 484700
    },
    {
      "epoch": 7.610675039246468,
      "grad_norm": 3.9556946754455566,
      "learning_rate": 4.524332810047096e-05,
      "loss": 0.6635,
      "step": 484800
    },
    {
      "epoch": 7.612244897959184,
      "grad_norm": 4.132087230682373,
      "learning_rate": 4.524234693877552e-05,
      "loss": 0.6633,
      "step": 484900
    },
    {
      "epoch": 7.6138147566719,
      "grad_norm": 3.827343702316284,
      "learning_rate": 4.524136577708006e-05,
      "loss": 0.6534,
      "step": 485000
    },
    {
      "epoch": 7.615384615384615,
      "grad_norm": 4.599282741546631,
      "learning_rate": 4.524038461538462e-05,
      "loss": 0.6301,
      "step": 485100
    },
    {
      "epoch": 7.616954474097331,
      "grad_norm": 3.6659562587738037,
      "learning_rate": 4.523940345368917e-05,
      "loss": 0.6362,
      "step": 485200
    },
    {
      "epoch": 7.618524332810047,
      "grad_norm": 3.1517107486724854,
      "learning_rate": 4.523842229199372e-05,
      "loss": 0.6474,
      "step": 485300
    },
    {
      "epoch": 7.620094191522763,
      "grad_norm": 6.32534646987915,
      "learning_rate": 4.523744113029827e-05,
      "loss": 0.6268,
      "step": 485400
    },
    {
      "epoch": 7.621664050235479,
      "grad_norm": 4.287709712982178,
      "learning_rate": 4.523645996860283e-05,
      "loss": 0.6923,
      "step": 485500
    },
    {
      "epoch": 7.623233908948195,
      "grad_norm": 2.979832649230957,
      "learning_rate": 4.523547880690738e-05,
      "loss": 0.6583,
      "step": 485600
    },
    {
      "epoch": 7.624803767660911,
      "grad_norm": 3.716931104660034,
      "learning_rate": 4.523449764521193e-05,
      "loss": 0.6379,
      "step": 485700
    },
    {
      "epoch": 7.626373626373626,
      "grad_norm": 3.106661319732666,
      "learning_rate": 4.523351648351648e-05,
      "loss": 0.6635,
      "step": 485800
    },
    {
      "epoch": 7.627943485086342,
      "grad_norm": 4.073364734649658,
      "learning_rate": 4.523253532182104e-05,
      "loss": 0.653,
      "step": 485900
    },
    {
      "epoch": 7.629513343799058,
      "grad_norm": 4.437788009643555,
      "learning_rate": 4.523155416012559e-05,
      "loss": 0.6539,
      "step": 486000
    },
    {
      "epoch": 7.631083202511774,
      "grad_norm": 3.559818983078003,
      "learning_rate": 4.523057299843014e-05,
      "loss": 0.6375,
      "step": 486100
    },
    {
      "epoch": 7.63265306122449,
      "grad_norm": 4.6764302253723145,
      "learning_rate": 4.52295918367347e-05,
      "loss": 0.6443,
      "step": 486200
    },
    {
      "epoch": 7.634222919937206,
      "grad_norm": 4.011380672454834,
      "learning_rate": 4.522861067503925e-05,
      "loss": 0.6269,
      "step": 486300
    },
    {
      "epoch": 7.635792778649922,
      "grad_norm": 5.289903163909912,
      "learning_rate": 4.52276295133438e-05,
      "loss": 0.7057,
      "step": 486400
    },
    {
      "epoch": 7.637362637362637,
      "grad_norm": 4.100209712982178,
      "learning_rate": 4.5226648351648353e-05,
      "loss": 0.6511,
      "step": 486500
    },
    {
      "epoch": 7.638932496075353,
      "grad_norm": 3.881650924682617,
      "learning_rate": 4.522566718995291e-05,
      "loss": 0.6343,
      "step": 486600
    },
    {
      "epoch": 7.640502354788069,
      "grad_norm": 4.032605171203613,
      "learning_rate": 4.5224686028257455e-05,
      "loss": 0.6844,
      "step": 486700
    },
    {
      "epoch": 7.642072213500785,
      "grad_norm": 4.452290058135986,
      "learning_rate": 4.522370486656201e-05,
      "loss": 0.6418,
      "step": 486800
    },
    {
      "epoch": 7.643642072213501,
      "grad_norm": 2.576986312866211,
      "learning_rate": 4.5222723704866564e-05,
      "loss": 0.6364,
      "step": 486900
    },
    {
      "epoch": 7.645211930926217,
      "grad_norm": 4.293253421783447,
      "learning_rate": 4.522174254317112e-05,
      "loss": 0.6366,
      "step": 487000
    },
    {
      "epoch": 7.646781789638933,
      "grad_norm": 3.900942087173462,
      "learning_rate": 4.5220761381475666e-05,
      "loss": 0.6913,
      "step": 487100
    },
    {
      "epoch": 7.648351648351649,
      "grad_norm": 3.5812935829162598,
      "learning_rate": 4.5219780219780224e-05,
      "loss": 0.6403,
      "step": 487200
    },
    {
      "epoch": 7.649921507064365,
      "grad_norm": 4.115777492523193,
      "learning_rate": 4.5218799058084775e-05,
      "loss": 0.6327,
      "step": 487300
    },
    {
      "epoch": 7.65149136577708,
      "grad_norm": 3.3026397228240967,
      "learning_rate": 4.5217817896389326e-05,
      "loss": 0.6246,
      "step": 487400
    },
    {
      "epoch": 7.653061224489796,
      "grad_norm": 3.210282564163208,
      "learning_rate": 4.521683673469388e-05,
      "loss": 0.6673,
      "step": 487500
    },
    {
      "epoch": 7.654631083202512,
      "grad_norm": 3.9890334606170654,
      "learning_rate": 4.5215855572998435e-05,
      "loss": 0.6469,
      "step": 487600
    },
    {
      "epoch": 7.656200941915228,
      "grad_norm": 4.049375057220459,
      "learning_rate": 4.5214874411302986e-05,
      "loss": 0.6441,
      "step": 487700
    },
    {
      "epoch": 7.657770800627944,
      "grad_norm": 3.087475299835205,
      "learning_rate": 4.5213893249607537e-05,
      "loss": 0.6544,
      "step": 487800
    },
    {
      "epoch": 7.65934065934066,
      "grad_norm": 3.4424030780792236,
      "learning_rate": 4.521291208791209e-05,
      "loss": 0.6712,
      "step": 487900
    },
    {
      "epoch": 7.660910518053376,
      "grad_norm": 3.5721869468688965,
      "learning_rate": 4.5211930926216645e-05,
      "loss": 0.6346,
      "step": 488000
    },
    {
      "epoch": 7.662480376766091,
      "grad_norm": 3.8322794437408447,
      "learning_rate": 4.5210949764521196e-05,
      "loss": 0.6529,
      "step": 488100
    },
    {
      "epoch": 7.664050235478807,
      "grad_norm": 4.395904064178467,
      "learning_rate": 4.520996860282575e-05,
      "loss": 0.6451,
      "step": 488200
    },
    {
      "epoch": 7.665620094191523,
      "grad_norm": 4.610713005065918,
      "learning_rate": 4.5208987441130305e-05,
      "loss": 0.7039,
      "step": 488300
    },
    {
      "epoch": 7.667189952904239,
      "grad_norm": 4.875594615936279,
      "learning_rate": 4.5208006279434856e-05,
      "loss": 0.6267,
      "step": 488400
    },
    {
      "epoch": 7.668759811616955,
      "grad_norm": 3.628938674926758,
      "learning_rate": 4.520702511773941e-05,
      "loss": 0.6656,
      "step": 488500
    },
    {
      "epoch": 7.670329670329671,
      "grad_norm": 3.9174702167510986,
      "learning_rate": 4.520604395604396e-05,
      "loss": 0.5915,
      "step": 488600
    },
    {
      "epoch": 7.671899529042387,
      "grad_norm": 4.382264614105225,
      "learning_rate": 4.5205062794348516e-05,
      "loss": 0.6571,
      "step": 488700
    },
    {
      "epoch": 7.673469387755102,
      "grad_norm": 4.775045871734619,
      "learning_rate": 4.520408163265306e-05,
      "loss": 0.6552,
      "step": 488800
    },
    {
      "epoch": 7.675039246467818,
      "grad_norm": 3.385932683944702,
      "learning_rate": 4.520310047095762e-05,
      "loss": 0.5971,
      "step": 488900
    },
    {
      "epoch": 7.676609105180534,
      "grad_norm": 3.543081760406494,
      "learning_rate": 4.520211930926217e-05,
      "loss": 0.6703,
      "step": 489000
    },
    {
      "epoch": 7.67817896389325,
      "grad_norm": 2.739438772201538,
      "learning_rate": 4.5201138147566726e-05,
      "loss": 0.6883,
      "step": 489100
    },
    {
      "epoch": 7.679748822605966,
      "grad_norm": 3.0259385108947754,
      "learning_rate": 4.520015698587127e-05,
      "loss": 0.6263,
      "step": 489200
    },
    {
      "epoch": 7.681318681318682,
      "grad_norm": 3.596712112426758,
      "learning_rate": 4.519917582417583e-05,
      "loss": 0.6832,
      "step": 489300
    },
    {
      "epoch": 7.6828885400313975,
      "grad_norm": 4.197406768798828,
      "learning_rate": 4.519819466248038e-05,
      "loss": 0.6314,
      "step": 489400
    },
    {
      "epoch": 7.684458398744113,
      "grad_norm": 3.139620065689087,
      "learning_rate": 4.519721350078493e-05,
      "loss": 0.655,
      "step": 489500
    },
    {
      "epoch": 7.686028257456829,
      "grad_norm": 3.566957473754883,
      "learning_rate": 4.519623233908948e-05,
      "loss": 0.6539,
      "step": 489600
    },
    {
      "epoch": 7.687598116169545,
      "grad_norm": 3.811854600906372,
      "learning_rate": 4.519525117739404e-05,
      "loss": 0.6559,
      "step": 489700
    },
    {
      "epoch": 7.689167974882261,
      "grad_norm": 3.8980348110198975,
      "learning_rate": 4.519427001569858e-05,
      "loss": 0.6026,
      "step": 489800
    },
    {
      "epoch": 7.6907378335949765,
      "grad_norm": 3.160677909851074,
      "learning_rate": 4.519328885400314e-05,
      "loss": 0.6405,
      "step": 489900
    },
    {
      "epoch": 7.6923076923076925,
      "grad_norm": 3.758246421813965,
      "learning_rate": 4.519230769230769e-05,
      "loss": 0.6598,
      "step": 490000
    },
    {
      "epoch": 7.6938775510204085,
      "grad_norm": 3.993847608566284,
      "learning_rate": 4.519132653061225e-05,
      "loss": 0.6803,
      "step": 490100
    },
    {
      "epoch": 7.695447409733124,
      "grad_norm": 3.514887571334839,
      "learning_rate": 4.51903453689168e-05,
      "loss": 0.636,
      "step": 490200
    },
    {
      "epoch": 7.6970172684458396,
      "grad_norm": 3.704043388366699,
      "learning_rate": 4.518936420722135e-05,
      "loss": 0.674,
      "step": 490300
    },
    {
      "epoch": 7.6985871271585555,
      "grad_norm": 3.615494966506958,
      "learning_rate": 4.51883830455259e-05,
      "loss": 0.6473,
      "step": 490400
    },
    {
      "epoch": 7.7001569858712715,
      "grad_norm": 3.904005527496338,
      "learning_rate": 4.5187401883830454e-05,
      "loss": 0.6553,
      "step": 490500
    },
    {
      "epoch": 7.7017268445839875,
      "grad_norm": 4.4550957679748535,
      "learning_rate": 4.518642072213501e-05,
      "loss": 0.684,
      "step": 490600
    },
    {
      "epoch": 7.7032967032967035,
      "grad_norm": 4.611151695251465,
      "learning_rate": 4.518543956043956e-05,
      "loss": 0.6213,
      "step": 490700
    },
    {
      "epoch": 7.704866562009419,
      "grad_norm": 4.397189140319824,
      "learning_rate": 4.518445839874412e-05,
      "loss": 0.678,
      "step": 490800
    },
    {
      "epoch": 7.7064364207221345,
      "grad_norm": 3.070841073989868,
      "learning_rate": 4.5183477237048664e-05,
      "loss": 0.6551,
      "step": 490900
    },
    {
      "epoch": 7.7080062794348505,
      "grad_norm": 3.8983030319213867,
      "learning_rate": 4.518249607535322e-05,
      "loss": 0.6475,
      "step": 491000
    },
    {
      "epoch": 7.7095761381475665,
      "grad_norm": 4.3922014236450195,
      "learning_rate": 4.518151491365777e-05,
      "loss": 0.6302,
      "step": 491100
    },
    {
      "epoch": 7.7111459968602825,
      "grad_norm": 4.3992919921875,
      "learning_rate": 4.5180533751962324e-05,
      "loss": 0.6593,
      "step": 491200
    },
    {
      "epoch": 7.712715855572998,
      "grad_norm": 3.5287513732910156,
      "learning_rate": 4.5179552590266875e-05,
      "loss": 0.6635,
      "step": 491300
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 3.863615036010742,
      "learning_rate": 4.517857142857143e-05,
      "loss": 0.7054,
      "step": 491400
    },
    {
      "epoch": 7.71585557299843,
      "grad_norm": 3.3553619384765625,
      "learning_rate": 4.5177590266875984e-05,
      "loss": 0.647,
      "step": 491500
    },
    {
      "epoch": 7.717425431711146,
      "grad_norm": 4.247029781341553,
      "learning_rate": 4.5176609105180535e-05,
      "loss": 0.6427,
      "step": 491600
    },
    {
      "epoch": 7.718995290423862,
      "grad_norm": 3.856109142303467,
      "learning_rate": 4.5175627943485086e-05,
      "loss": 0.6306,
      "step": 491700
    },
    {
      "epoch": 7.720565149136577,
      "grad_norm": 3.2999625205993652,
      "learning_rate": 4.5174646781789644e-05,
      "loss": 0.6279,
      "step": 491800
    },
    {
      "epoch": 7.722135007849293,
      "grad_norm": 2.7190284729003906,
      "learning_rate": 4.517366562009419e-05,
      "loss": 0.6374,
      "step": 491900
    },
    {
      "epoch": 7.723704866562009,
      "grad_norm": 3.879354476928711,
      "learning_rate": 4.5172684458398746e-05,
      "loss": 0.646,
      "step": 492000
    },
    {
      "epoch": 7.725274725274725,
      "grad_norm": 3.0086593627929688,
      "learning_rate": 4.5171703296703296e-05,
      "loss": 0.6389,
      "step": 492100
    },
    {
      "epoch": 7.726844583987441,
      "grad_norm": 3.303194522857666,
      "learning_rate": 4.5170722135007854e-05,
      "loss": 0.6585,
      "step": 492200
    },
    {
      "epoch": 7.728414442700157,
      "grad_norm": 2.7390236854553223,
      "learning_rate": 4.5169740973312405e-05,
      "loss": 0.6264,
      "step": 492300
    },
    {
      "epoch": 7.729984301412873,
      "grad_norm": 3.4625802040100098,
      "learning_rate": 4.5168759811616956e-05,
      "loss": 0.6403,
      "step": 492400
    },
    {
      "epoch": 7.731554160125588,
      "grad_norm": 4.1185102462768555,
      "learning_rate": 4.516777864992151e-05,
      "loss": 0.6744,
      "step": 492500
    },
    {
      "epoch": 7.733124018838304,
      "grad_norm": 4.528750896453857,
      "learning_rate": 4.516679748822606e-05,
      "loss": 0.6269,
      "step": 492600
    },
    {
      "epoch": 7.73469387755102,
      "grad_norm": 4.254639625549316,
      "learning_rate": 4.5165816326530616e-05,
      "loss": 0.6448,
      "step": 492700
    },
    {
      "epoch": 7.736263736263736,
      "grad_norm": 3.6124820709228516,
      "learning_rate": 4.516483516483517e-05,
      "loss": 0.6403,
      "step": 492800
    },
    {
      "epoch": 7.737833594976452,
      "grad_norm": 4.512972831726074,
      "learning_rate": 4.5163854003139725e-05,
      "loss": 0.6598,
      "step": 492900
    },
    {
      "epoch": 7.739403453689168,
      "grad_norm": 5.512923717498779,
      "learning_rate": 4.516287284144427e-05,
      "loss": 0.6675,
      "step": 493000
    },
    {
      "epoch": 7.740973312401884,
      "grad_norm": 4.2947492599487305,
      "learning_rate": 4.516189167974883e-05,
      "loss": 0.6318,
      "step": 493100
    },
    {
      "epoch": 7.742543171114599,
      "grad_norm": 4.016552925109863,
      "learning_rate": 4.516091051805338e-05,
      "loss": 0.6445,
      "step": 493200
    },
    {
      "epoch": 7.744113029827315,
      "grad_norm": 3.2326908111572266,
      "learning_rate": 4.515992935635793e-05,
      "loss": 0.6232,
      "step": 493300
    },
    {
      "epoch": 7.745682888540031,
      "grad_norm": 2.3772716522216797,
      "learning_rate": 4.515894819466248e-05,
      "loss": 0.7066,
      "step": 493400
    },
    {
      "epoch": 7.747252747252747,
      "grad_norm": 3.798745632171631,
      "learning_rate": 4.515796703296704e-05,
      "loss": 0.6227,
      "step": 493500
    },
    {
      "epoch": 7.748822605965463,
      "grad_norm": 3.903792381286621,
      "learning_rate": 4.515698587127159e-05,
      "loss": 0.6528,
      "step": 493600
    },
    {
      "epoch": 7.750392464678179,
      "grad_norm": 3.661841630935669,
      "learning_rate": 4.515600470957614e-05,
      "loss": 0.6445,
      "step": 493700
    },
    {
      "epoch": 7.751962323390895,
      "grad_norm": 3.279142379760742,
      "learning_rate": 4.515502354788069e-05,
      "loss": 0.6169,
      "step": 493800
    },
    {
      "epoch": 7.75353218210361,
      "grad_norm": 3.4273793697357178,
      "learning_rate": 4.515404238618525e-05,
      "loss": 0.6716,
      "step": 493900
    },
    {
      "epoch": 7.755102040816326,
      "grad_norm": 3.372530460357666,
      "learning_rate": 4.515306122448979e-05,
      "loss": 0.6203,
      "step": 494000
    },
    {
      "epoch": 7.756671899529042,
      "grad_norm": 3.24513840675354,
      "learning_rate": 4.515208006279435e-05,
      "loss": 0.6721,
      "step": 494100
    },
    {
      "epoch": 7.758241758241758,
      "grad_norm": 4.123809337615967,
      "learning_rate": 4.51510989010989e-05,
      "loss": 0.65,
      "step": 494200
    },
    {
      "epoch": 7.759811616954474,
      "grad_norm": 5.030951023101807,
      "learning_rate": 4.515011773940346e-05,
      "loss": 0.6566,
      "step": 494300
    },
    {
      "epoch": 7.76138147566719,
      "grad_norm": 6.134461402893066,
      "learning_rate": 4.514913657770801e-05,
      "loss": 0.6418,
      "step": 494400
    },
    {
      "epoch": 7.762951334379906,
      "grad_norm": 4.459980010986328,
      "learning_rate": 4.514815541601256e-05,
      "loss": 0.6667,
      "step": 494500
    },
    {
      "epoch": 7.764521193092621,
      "grad_norm": 4.092518329620361,
      "learning_rate": 4.514717425431711e-05,
      "loss": 0.6633,
      "step": 494600
    },
    {
      "epoch": 7.766091051805337,
      "grad_norm": 4.584765434265137,
      "learning_rate": 4.514619309262166e-05,
      "loss": 0.658,
      "step": 494700
    },
    {
      "epoch": 7.767660910518053,
      "grad_norm": 3.2781903743743896,
      "learning_rate": 4.514521193092622e-05,
      "loss": 0.683,
      "step": 494800
    },
    {
      "epoch": 7.769230769230769,
      "grad_norm": 4.643771648406982,
      "learning_rate": 4.514423076923077e-05,
      "loss": 0.6231,
      "step": 494900
    },
    {
      "epoch": 7.770800627943485,
      "grad_norm": 3.8664679527282715,
      "learning_rate": 4.514324960753533e-05,
      "loss": 0.641,
      "step": 495000
    },
    {
      "epoch": 7.772370486656201,
      "grad_norm": 4.497760772705078,
      "learning_rate": 4.514226844583987e-05,
      "loss": 0.6191,
      "step": 495100
    },
    {
      "epoch": 7.773940345368917,
      "grad_norm": 3.632519483566284,
      "learning_rate": 4.514128728414443e-05,
      "loss": 0.6829,
      "step": 495200
    },
    {
      "epoch": 7.775510204081632,
      "grad_norm": 4.326026916503906,
      "learning_rate": 4.514030612244898e-05,
      "loss": 0.6441,
      "step": 495300
    },
    {
      "epoch": 7.777080062794348,
      "grad_norm": 3.771115779876709,
      "learning_rate": 4.513932496075353e-05,
      "loss": 0.6541,
      "step": 495400
    },
    {
      "epoch": 7.778649921507064,
      "grad_norm": 2.613415241241455,
      "learning_rate": 4.5138343799058084e-05,
      "loss": 0.5952,
      "step": 495500
    },
    {
      "epoch": 7.78021978021978,
      "grad_norm": 4.37483549118042,
      "learning_rate": 4.513736263736264e-05,
      "loss": 0.6479,
      "step": 495600
    },
    {
      "epoch": 7.781789638932496,
      "grad_norm": 3.46317982673645,
      "learning_rate": 4.513638147566719e-05,
      "loss": 0.657,
      "step": 495700
    },
    {
      "epoch": 7.783359497645212,
      "grad_norm": 3.7182979583740234,
      "learning_rate": 4.5135400313971744e-05,
      "loss": 0.7027,
      "step": 495800
    },
    {
      "epoch": 7.784929356357928,
      "grad_norm": 3.474538803100586,
      "learning_rate": 4.5134419152276295e-05,
      "loss": 0.6175,
      "step": 495900
    },
    {
      "epoch": 7.786499215070644,
      "grad_norm": 3.5245325565338135,
      "learning_rate": 4.513343799058085e-05,
      "loss": 0.6846,
      "step": 496000
    },
    {
      "epoch": 7.78806907378336,
      "grad_norm": 3.4690563678741455,
      "learning_rate": 4.51324568288854e-05,
      "loss": 0.6102,
      "step": 496100
    },
    {
      "epoch": 7.789638932496075,
      "grad_norm": 4.439887046813965,
      "learning_rate": 4.5131475667189954e-05,
      "loss": 0.6671,
      "step": 496200
    },
    {
      "epoch": 7.791208791208791,
      "grad_norm": 3.9803969860076904,
      "learning_rate": 4.5130494505494505e-05,
      "loss": 0.6542,
      "step": 496300
    },
    {
      "epoch": 7.792778649921507,
      "grad_norm": 3.8018267154693604,
      "learning_rate": 4.512951334379906e-05,
      "loss": 0.6503,
      "step": 496400
    },
    {
      "epoch": 7.794348508634223,
      "grad_norm": 3.153228282928467,
      "learning_rate": 4.5128532182103614e-05,
      "loss": 0.6327,
      "step": 496500
    },
    {
      "epoch": 7.795918367346939,
      "grad_norm": 3.659777879714966,
      "learning_rate": 4.5127551020408165e-05,
      "loss": 0.6114,
      "step": 496600
    },
    {
      "epoch": 7.797488226059655,
      "grad_norm": 3.3969638347625732,
      "learning_rate": 4.5126569858712716e-05,
      "loss": 0.687,
      "step": 496700
    },
    {
      "epoch": 7.799058084772371,
      "grad_norm": 2.690903425216675,
      "learning_rate": 4.512558869701727e-05,
      "loss": 0.65,
      "step": 496800
    },
    {
      "epoch": 7.800627943485086,
      "grad_norm": 3.899172306060791,
      "learning_rate": 4.5124607535321825e-05,
      "loss": 0.6656,
      "step": 496900
    },
    {
      "epoch": 7.802197802197802,
      "grad_norm": 4.124828815460205,
      "learning_rate": 4.5123626373626376e-05,
      "loss": 0.6419,
      "step": 497000
    },
    {
      "epoch": 7.803767660910518,
      "grad_norm": 3.9246811866760254,
      "learning_rate": 4.5122645211930934e-05,
      "loss": 0.6666,
      "step": 497100
    },
    {
      "epoch": 7.805337519623234,
      "grad_norm": 3.873521089553833,
      "learning_rate": 4.512166405023548e-05,
      "loss": 0.6809,
      "step": 497200
    },
    {
      "epoch": 7.80690737833595,
      "grad_norm": 3.1626956462860107,
      "learning_rate": 4.5120682888540036e-05,
      "loss": 0.6984,
      "step": 497300
    },
    {
      "epoch": 7.808477237048666,
      "grad_norm": 4.8493971824646,
      "learning_rate": 4.5119701726844587e-05,
      "loss": 0.6229,
      "step": 497400
    },
    {
      "epoch": 7.810047095761382,
      "grad_norm": 3.454341173171997,
      "learning_rate": 4.511872056514914e-05,
      "loss": 0.6961,
      "step": 497500
    },
    {
      "epoch": 7.811616954474097,
      "grad_norm": 4.050588130950928,
      "learning_rate": 4.511773940345369e-05,
      "loss": 0.645,
      "step": 497600
    },
    {
      "epoch": 7.813186813186813,
      "grad_norm": 3.4380204677581787,
      "learning_rate": 4.5116758241758246e-05,
      "loss": 0.6194,
      "step": 497700
    },
    {
      "epoch": 7.814756671899529,
      "grad_norm": 4.6230669021606445,
      "learning_rate": 4.51157770800628e-05,
      "loss": 0.6577,
      "step": 497800
    },
    {
      "epoch": 7.816326530612245,
      "grad_norm": 3.745710611343384,
      "learning_rate": 4.511479591836735e-05,
      "loss": 0.6316,
      "step": 497900
    },
    {
      "epoch": 7.817896389324961,
      "grad_norm": 4.148224830627441,
      "learning_rate": 4.51138147566719e-05,
      "loss": 0.7044,
      "step": 498000
    },
    {
      "epoch": 7.819466248037677,
      "grad_norm": 3.056553602218628,
      "learning_rate": 4.511283359497646e-05,
      "loss": 0.7156,
      "step": 498100
    },
    {
      "epoch": 7.821036106750393,
      "grad_norm": 3.608576774597168,
      "learning_rate": 4.5111852433281e-05,
      "loss": 0.6241,
      "step": 498200
    },
    {
      "epoch": 7.822605965463108,
      "grad_norm": 4.570631980895996,
      "learning_rate": 4.511087127158556e-05,
      "loss": 0.6462,
      "step": 498300
    },
    {
      "epoch": 7.824175824175824,
      "grad_norm": 3.42940092086792,
      "learning_rate": 4.510989010989011e-05,
      "loss": 0.661,
      "step": 498400
    },
    {
      "epoch": 7.82574568288854,
      "grad_norm": 3.495018482208252,
      "learning_rate": 4.510890894819467e-05,
      "loss": 0.6574,
      "step": 498500
    },
    {
      "epoch": 7.827315541601256,
      "grad_norm": 3.7217495441436768,
      "learning_rate": 4.510792778649922e-05,
      "loss": 0.655,
      "step": 498600
    },
    {
      "epoch": 7.828885400313972,
      "grad_norm": 2.0728330612182617,
      "learning_rate": 4.510694662480377e-05,
      "loss": 0.6816,
      "step": 498700
    },
    {
      "epoch": 7.830455259026688,
      "grad_norm": 3.2653515338897705,
      "learning_rate": 4.510596546310832e-05,
      "loss": 0.6313,
      "step": 498800
    },
    {
      "epoch": 7.832025117739404,
      "grad_norm": 2.9023234844207764,
      "learning_rate": 4.510498430141287e-05,
      "loss": 0.6039,
      "step": 498900
    },
    {
      "epoch": 7.833594976452119,
      "grad_norm": 4.355992794036865,
      "learning_rate": 4.510400313971743e-05,
      "loss": 0.6468,
      "step": 499000
    },
    {
      "epoch": 7.835164835164835,
      "grad_norm": 3.9053378105163574,
      "learning_rate": 4.510302197802198e-05,
      "loss": 0.6584,
      "step": 499100
    },
    {
      "epoch": 7.836734693877551,
      "grad_norm": 3.540404796600342,
      "learning_rate": 4.510204081632654e-05,
      "loss": 0.6388,
      "step": 499200
    },
    {
      "epoch": 7.838304552590267,
      "grad_norm": 4.537827014923096,
      "learning_rate": 4.510105965463108e-05,
      "loss": 0.6246,
      "step": 499300
    },
    {
      "epoch": 7.839874411302983,
      "grad_norm": 4.080810546875,
      "learning_rate": 4.510007849293564e-05,
      "loss": 0.6732,
      "step": 499400
    },
    {
      "epoch": 7.841444270015699,
      "grad_norm": 4.169857501983643,
      "learning_rate": 4.509909733124019e-05,
      "loss": 0.7045,
      "step": 499500
    },
    {
      "epoch": 7.843014128728415,
      "grad_norm": 3.372661828994751,
      "learning_rate": 4.509811616954474e-05,
      "loss": 0.6504,
      "step": 499600
    },
    {
      "epoch": 7.84458398744113,
      "grad_norm": 3.423595905303955,
      "learning_rate": 4.509713500784929e-05,
      "loss": 0.6629,
      "step": 499700
    },
    {
      "epoch": 7.846153846153846,
      "grad_norm": 3.8775250911712646,
      "learning_rate": 4.509615384615385e-05,
      "loss": 0.6458,
      "step": 499800
    },
    {
      "epoch": 7.847723704866562,
      "grad_norm": 4.140602111816406,
      "learning_rate": 4.50951726844584e-05,
      "loss": 0.6715,
      "step": 499900
    },
    {
      "epoch": 7.849293563579278,
      "grad_norm": 3.307495594024658,
      "learning_rate": 4.509419152276295e-05,
      "loss": 0.6522,
      "step": 500000
    },
    {
      "epoch": 7.850863422291994,
      "grad_norm": 3.7717180252075195,
      "learning_rate": 4.5093210361067504e-05,
      "loss": 0.6582,
      "step": 500100
    },
    {
      "epoch": 7.85243328100471,
      "grad_norm": 4.564573287963867,
      "learning_rate": 4.509222919937206e-05,
      "loss": 0.6891,
      "step": 500200
    },
    {
      "epoch": 7.854003139717426,
      "grad_norm": 3.3202037811279297,
      "learning_rate": 4.5091248037676606e-05,
      "loss": 0.6453,
      "step": 500300
    },
    {
      "epoch": 7.855572998430142,
      "grad_norm": 3.911801338195801,
      "learning_rate": 4.5090266875981163e-05,
      "loss": 0.6675,
      "step": 500400
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 3.8103926181793213,
      "learning_rate": 4.5089285714285714e-05,
      "loss": 0.7015,
      "step": 500500
    },
    {
      "epoch": 7.858712715855573,
      "grad_norm": 3.981905698776245,
      "learning_rate": 4.508830455259027e-05,
      "loss": 0.6528,
      "step": 500600
    },
    {
      "epoch": 7.860282574568289,
      "grad_norm": 4.569162845611572,
      "learning_rate": 4.508732339089482e-05,
      "loss": 0.675,
      "step": 500700
    },
    {
      "epoch": 7.861852433281005,
      "grad_norm": 4.310275077819824,
      "learning_rate": 4.5086342229199374e-05,
      "loss": 0.6574,
      "step": 500800
    },
    {
      "epoch": 7.863422291993721,
      "grad_norm": 4.476008415222168,
      "learning_rate": 4.5085361067503925e-05,
      "loss": 0.6582,
      "step": 500900
    },
    {
      "epoch": 7.864992150706437,
      "grad_norm": 3.148810863494873,
      "learning_rate": 4.5084379905808476e-05,
      "loss": 0.6346,
      "step": 501000
    },
    {
      "epoch": 7.866562009419153,
      "grad_norm": 3.6708905696868896,
      "learning_rate": 4.5083398744113034e-05,
      "loss": 0.6429,
      "step": 501100
    },
    {
      "epoch": 7.868131868131869,
      "grad_norm": 4.34429931640625,
      "learning_rate": 4.5082417582417585e-05,
      "loss": 0.6529,
      "step": 501200
    },
    {
      "epoch": 7.869701726844584,
      "grad_norm": 4.110969543457031,
      "learning_rate": 4.508143642072214e-05,
      "loss": 0.6587,
      "step": 501300
    },
    {
      "epoch": 7.8712715855573,
      "grad_norm": 3.481945276260376,
      "learning_rate": 4.508045525902669e-05,
      "loss": 0.6443,
      "step": 501400
    },
    {
      "epoch": 7.872841444270016,
      "grad_norm": 3.495173454284668,
      "learning_rate": 4.5079474097331245e-05,
      "loss": 0.5925,
      "step": 501500
    },
    {
      "epoch": 7.874411302982732,
      "grad_norm": 3.2681474685668945,
      "learning_rate": 4.5078492935635796e-05,
      "loss": 0.6695,
      "step": 501600
    },
    {
      "epoch": 7.875981161695448,
      "grad_norm": 3.7052133083343506,
      "learning_rate": 4.5077511773940347e-05,
      "loss": 0.597,
      "step": 501700
    },
    {
      "epoch": 7.877551020408164,
      "grad_norm": 3.5982632637023926,
      "learning_rate": 4.50765306122449e-05,
      "loss": 0.6794,
      "step": 501800
    },
    {
      "epoch": 7.8791208791208796,
      "grad_norm": 2.7845208644866943,
      "learning_rate": 4.5075549450549455e-05,
      "loss": 0.6283,
      "step": 501900
    },
    {
      "epoch": 7.880690737833595,
      "grad_norm": 4.24960470199585,
      "learning_rate": 4.5074568288854006e-05,
      "loss": 0.6275,
      "step": 502000
    },
    {
      "epoch": 7.882260596546311,
      "grad_norm": 5.035439968109131,
      "learning_rate": 4.507358712715856e-05,
      "loss": 0.6433,
      "step": 502100
    },
    {
      "epoch": 7.883830455259027,
      "grad_norm": 4.581474304199219,
      "learning_rate": 4.507260596546311e-05,
      "loss": 0.6633,
      "step": 502200
    },
    {
      "epoch": 7.885400313971743,
      "grad_norm": 3.891791343688965,
      "learning_rate": 4.5071624803767666e-05,
      "loss": 0.5988,
      "step": 502300
    },
    {
      "epoch": 7.8869701726844585,
      "grad_norm": 3.828943967819214,
      "learning_rate": 4.507064364207221e-05,
      "loss": 0.6526,
      "step": 502400
    },
    {
      "epoch": 7.8885400313971745,
      "grad_norm": 3.830289602279663,
      "learning_rate": 4.506966248037677e-05,
      "loss": 0.6386,
      "step": 502500
    },
    {
      "epoch": 7.8901098901098905,
      "grad_norm": 3.3754849433898926,
      "learning_rate": 4.506868131868132e-05,
      "loss": 0.6791,
      "step": 502600
    },
    {
      "epoch": 7.891679748822606,
      "grad_norm": 4.275303840637207,
      "learning_rate": 4.506770015698588e-05,
      "loss": 0.68,
      "step": 502700
    },
    {
      "epoch": 7.893249607535322,
      "grad_norm": 4.712838649749756,
      "learning_rate": 4.506671899529043e-05,
      "loss": 0.6311,
      "step": 502800
    },
    {
      "epoch": 7.8948194662480375,
      "grad_norm": 4.0013041496276855,
      "learning_rate": 4.506573783359498e-05,
      "loss": 0.629,
      "step": 502900
    },
    {
      "epoch": 7.8963893249607535,
      "grad_norm": 3.2868521213531494,
      "learning_rate": 4.506475667189953e-05,
      "loss": 0.6322,
      "step": 503000
    },
    {
      "epoch": 7.8979591836734695,
      "grad_norm": 3.6476149559020996,
      "learning_rate": 4.506377551020408e-05,
      "loss": 0.6603,
      "step": 503100
    },
    {
      "epoch": 7.8995290423861855,
      "grad_norm": 4.630504131317139,
      "learning_rate": 4.506279434850864e-05,
      "loss": 0.6837,
      "step": 503200
    },
    {
      "epoch": 7.9010989010989015,
      "grad_norm": 4.476953506469727,
      "learning_rate": 4.506181318681319e-05,
      "loss": 0.6106,
      "step": 503300
    },
    {
      "epoch": 7.9026687598116165,
      "grad_norm": 4.450602054595947,
      "learning_rate": 4.506083202511775e-05,
      "loss": 0.6382,
      "step": 503400
    },
    {
      "epoch": 7.9042386185243325,
      "grad_norm": 3.8026809692382812,
      "learning_rate": 4.505985086342229e-05,
      "loss": 0.6335,
      "step": 503500
    },
    {
      "epoch": 7.9058084772370485,
      "grad_norm": 3.8058063983917236,
      "learning_rate": 4.505886970172685e-05,
      "loss": 0.6252,
      "step": 503600
    },
    {
      "epoch": 7.9073783359497645,
      "grad_norm": 3.4022252559661865,
      "learning_rate": 4.50578885400314e-05,
      "loss": 0.6685,
      "step": 503700
    },
    {
      "epoch": 7.9089481946624804,
      "grad_norm": 4.300821304321289,
      "learning_rate": 4.505690737833595e-05,
      "loss": 0.6633,
      "step": 503800
    },
    {
      "epoch": 7.910518053375196,
      "grad_norm": 4.458932876586914,
      "learning_rate": 4.50559262166405e-05,
      "loss": 0.6325,
      "step": 503900
    },
    {
      "epoch": 7.912087912087912,
      "grad_norm": 2.6457297801971436,
      "learning_rate": 4.505494505494506e-05,
      "loss": 0.6742,
      "step": 504000
    },
    {
      "epoch": 7.9136577708006275,
      "grad_norm": 3.510307788848877,
      "learning_rate": 4.505396389324961e-05,
      "loss": 0.5876,
      "step": 504100
    },
    {
      "epoch": 7.9152276295133435,
      "grad_norm": 3.951763868331909,
      "learning_rate": 4.505298273155416e-05,
      "loss": 0.6693,
      "step": 504200
    },
    {
      "epoch": 7.916797488226059,
      "grad_norm": 4.243913173675537,
      "learning_rate": 4.505200156985871e-05,
      "loss": 0.6399,
      "step": 504300
    },
    {
      "epoch": 7.918367346938775,
      "grad_norm": 4.2362165451049805,
      "learning_rate": 4.505102040816327e-05,
      "loss": 0.5961,
      "step": 504400
    },
    {
      "epoch": 7.919937205651491,
      "grad_norm": 4.7934064865112305,
      "learning_rate": 4.5050039246467815e-05,
      "loss": 0.6275,
      "step": 504500
    },
    {
      "epoch": 7.921507064364207,
      "grad_norm": 4.754087924957275,
      "learning_rate": 4.504905808477237e-05,
      "loss": 0.6266,
      "step": 504600
    },
    {
      "epoch": 7.923076923076923,
      "grad_norm": 4.069921493530273,
      "learning_rate": 4.5048076923076923e-05,
      "loss": 0.6624,
      "step": 504700
    },
    {
      "epoch": 7.924646781789638,
      "grad_norm": 4.083944320678711,
      "learning_rate": 4.504709576138148e-05,
      "loss": 0.6468,
      "step": 504800
    },
    {
      "epoch": 7.926216640502354,
      "grad_norm": 4.148378372192383,
      "learning_rate": 4.504611459968603e-05,
      "loss": 0.6312,
      "step": 504900
    },
    {
      "epoch": 7.92778649921507,
      "grad_norm": 3.638946056365967,
      "learning_rate": 4.504513343799058e-05,
      "loss": 0.6808,
      "step": 505000
    },
    {
      "epoch": 7.929356357927786,
      "grad_norm": 4.673056602478027,
      "learning_rate": 4.5044152276295134e-05,
      "loss": 0.6835,
      "step": 505100
    },
    {
      "epoch": 7.930926216640502,
      "grad_norm": 4.0167059898376465,
      "learning_rate": 4.5043171114599685e-05,
      "loss": 0.628,
      "step": 505200
    },
    {
      "epoch": 7.932496075353218,
      "grad_norm": 5.0015106201171875,
      "learning_rate": 4.504218995290424e-05,
      "loss": 0.66,
      "step": 505300
    },
    {
      "epoch": 7.934065934065934,
      "grad_norm": 1.7976704835891724,
      "learning_rate": 4.5041208791208794e-05,
      "loss": 0.6293,
      "step": 505400
    },
    {
      "epoch": 7.93563579277865,
      "grad_norm": 4.616443157196045,
      "learning_rate": 4.504022762951335e-05,
      "loss": 0.6738,
      "step": 505500
    },
    {
      "epoch": 7.937205651491366,
      "grad_norm": 4.087133407592773,
      "learning_rate": 4.5039246467817896e-05,
      "loss": 0.6733,
      "step": 505600
    },
    {
      "epoch": 7.938775510204081,
      "grad_norm": 4.6771135330200195,
      "learning_rate": 4.5038265306122454e-05,
      "loss": 0.6675,
      "step": 505700
    },
    {
      "epoch": 7.940345368916797,
      "grad_norm": 4.244614124298096,
      "learning_rate": 4.5037284144427005e-05,
      "loss": 0.67,
      "step": 505800
    },
    {
      "epoch": 7.941915227629513,
      "grad_norm": 4.325164318084717,
      "learning_rate": 4.5036302982731556e-05,
      "loss": 0.6637,
      "step": 505900
    },
    {
      "epoch": 7.943485086342229,
      "grad_norm": 3.8340282440185547,
      "learning_rate": 4.5035321821036107e-05,
      "loss": 0.6383,
      "step": 506000
    },
    {
      "epoch": 7.945054945054945,
      "grad_norm": 4.646467208862305,
      "learning_rate": 4.5034340659340664e-05,
      "loss": 0.6812,
      "step": 506100
    },
    {
      "epoch": 7.946624803767661,
      "grad_norm": 4.929853439331055,
      "learning_rate": 4.5033359497645215e-05,
      "loss": 0.6705,
      "step": 506200
    },
    {
      "epoch": 7.948194662480377,
      "grad_norm": 4.444495677947998,
      "learning_rate": 4.5032378335949766e-05,
      "loss": 0.6569,
      "step": 506300
    },
    {
      "epoch": 7.949764521193092,
      "grad_norm": 4.9557719230651855,
      "learning_rate": 4.503139717425432e-05,
      "loss": 0.6269,
      "step": 506400
    },
    {
      "epoch": 7.951334379905808,
      "grad_norm": 3.809955596923828,
      "learning_rate": 4.5030416012558875e-05,
      "loss": 0.6292,
      "step": 506500
    },
    {
      "epoch": 7.952904238618524,
      "grad_norm": 4.157133102416992,
      "learning_rate": 4.502943485086342e-05,
      "loss": 0.68,
      "step": 506600
    },
    {
      "epoch": 7.95447409733124,
      "grad_norm": 4.580371379852295,
      "learning_rate": 4.502845368916798e-05,
      "loss": 0.6186,
      "step": 506700
    },
    {
      "epoch": 7.956043956043956,
      "grad_norm": 3.044100284576416,
      "learning_rate": 4.502747252747253e-05,
      "loss": 0.6272,
      "step": 506800
    },
    {
      "epoch": 7.957613814756672,
      "grad_norm": 4.395911693572998,
      "learning_rate": 4.5026491365777086e-05,
      "loss": 0.6636,
      "step": 506900
    },
    {
      "epoch": 7.959183673469388,
      "grad_norm": 4.179964542388916,
      "learning_rate": 4.502551020408164e-05,
      "loss": 0.6659,
      "step": 507000
    },
    {
      "epoch": 7.960753532182103,
      "grad_norm": 5.020493030548096,
      "learning_rate": 4.502452904238619e-05,
      "loss": 0.6548,
      "step": 507100
    },
    {
      "epoch": 7.962323390894819,
      "grad_norm": 3.6140213012695312,
      "learning_rate": 4.502354788069074e-05,
      "loss": 0.6086,
      "step": 507200
    },
    {
      "epoch": 7.963893249607535,
      "grad_norm": 3.8901419639587402,
      "learning_rate": 4.502256671899529e-05,
      "loss": 0.6728,
      "step": 507300
    },
    {
      "epoch": 7.965463108320251,
      "grad_norm": 4.819034576416016,
      "learning_rate": 4.502158555729985e-05,
      "loss": 0.6441,
      "step": 507400
    },
    {
      "epoch": 7.967032967032967,
      "grad_norm": 4.808409214019775,
      "learning_rate": 4.50206043956044e-05,
      "loss": 0.6507,
      "step": 507500
    },
    {
      "epoch": 7.968602825745683,
      "grad_norm": 3.0239076614379883,
      "learning_rate": 4.5019623233908956e-05,
      "loss": 0.6674,
      "step": 507600
    },
    {
      "epoch": 7.970172684458399,
      "grad_norm": 3.6864066123962402,
      "learning_rate": 4.50186420722135e-05,
      "loss": 0.651,
      "step": 507700
    },
    {
      "epoch": 7.971742543171114,
      "grad_norm": 4.5799031257629395,
      "learning_rate": 4.501766091051806e-05,
      "loss": 0.6632,
      "step": 507800
    },
    {
      "epoch": 7.97331240188383,
      "grad_norm": 4.177787780761719,
      "learning_rate": 4.501667974882261e-05,
      "loss": 0.7212,
      "step": 507900
    },
    {
      "epoch": 7.974882260596546,
      "grad_norm": 4.4919819831848145,
      "learning_rate": 4.501569858712716e-05,
      "loss": 0.7088,
      "step": 508000
    },
    {
      "epoch": 7.976452119309262,
      "grad_norm": 3.782589912414551,
      "learning_rate": 4.501471742543171e-05,
      "loss": 0.6762,
      "step": 508100
    },
    {
      "epoch": 7.978021978021978,
      "grad_norm": 3.002924680709839,
      "learning_rate": 4.501373626373627e-05,
      "loss": 0.6709,
      "step": 508200
    },
    {
      "epoch": 7.979591836734694,
      "grad_norm": 5.395791530609131,
      "learning_rate": 4.501275510204082e-05,
      "loss": 0.678,
      "step": 508300
    },
    {
      "epoch": 7.98116169544741,
      "grad_norm": 4.12830114364624,
      "learning_rate": 4.501177394034537e-05,
      "loss": 0.6165,
      "step": 508400
    },
    {
      "epoch": 7.982731554160125,
      "grad_norm": 4.0082526206970215,
      "learning_rate": 4.501079277864992e-05,
      "loss": 0.6955,
      "step": 508500
    },
    {
      "epoch": 7.984301412872841,
      "grad_norm": 3.291598320007324,
      "learning_rate": 4.500981161695448e-05,
      "loss": 0.617,
      "step": 508600
    },
    {
      "epoch": 7.985871271585557,
      "grad_norm": 3.7360451221466064,
      "learning_rate": 4.5008830455259024e-05,
      "loss": 0.6762,
      "step": 508700
    },
    {
      "epoch": 7.987441130298273,
      "grad_norm": 3.8412890434265137,
      "learning_rate": 4.500784929356358e-05,
      "loss": 0.6828,
      "step": 508800
    },
    {
      "epoch": 7.989010989010989,
      "grad_norm": 3.790454626083374,
      "learning_rate": 4.500686813186813e-05,
      "loss": 0.6192,
      "step": 508900
    },
    {
      "epoch": 7.990580847723705,
      "grad_norm": 3.627631425857544,
      "learning_rate": 4.500588697017269e-05,
      "loss": 0.6777,
      "step": 509000
    },
    {
      "epoch": 7.992150706436421,
      "grad_norm": 4.733345031738281,
      "learning_rate": 4.500490580847724e-05,
      "loss": 0.6195,
      "step": 509100
    },
    {
      "epoch": 7.993720565149136,
      "grad_norm": 3.6838812828063965,
      "learning_rate": 4.500392464678179e-05,
      "loss": 0.6741,
      "step": 509200
    },
    {
      "epoch": 7.995290423861852,
      "grad_norm": 3.438567638397217,
      "learning_rate": 4.500294348508634e-05,
      "loss": 0.6219,
      "step": 509300
    },
    {
      "epoch": 7.996860282574568,
      "grad_norm": 3.4952609539031982,
      "learning_rate": 4.5001962323390894e-05,
      "loss": 0.6045,
      "step": 509400
    },
    {
      "epoch": 7.998430141287284,
      "grad_norm": 3.27622127532959,
      "learning_rate": 4.500098116169545e-05,
      "loss": 0.6807,
      "step": 509500
    },
    {
      "epoch": 8.0,
      "grad_norm": 4.0394368171691895,
      "learning_rate": 4.5e-05,
      "loss": 0.6528,
      "step": 509600
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.018864631652832,
      "eval_runtime": 14.8325,
      "eval_samples_per_second": 226.058,
      "eval_steps_per_second": 226.058,
      "step": 509600
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.503132164478302,
      "eval_runtime": 280.0518,
      "eval_samples_per_second": 227.458,
      "eval_steps_per_second": 227.458,
      "step": 509600
    },
    {
      "epoch": 8.001569858712715,
      "grad_norm": 4.242687702178955,
      "learning_rate": 4.499901883830456e-05,
      "loss": 0.6851,
      "step": 509700
    },
    {
      "epoch": 8.003139717425432,
      "grad_norm": 4.469395160675049,
      "learning_rate": 4.4998037676609105e-05,
      "loss": 0.6401,
      "step": 509800
    },
    {
      "epoch": 8.004709576138147,
      "grad_norm": 2.743016242980957,
      "learning_rate": 4.499705651491366e-05,
      "loss": 0.6175,
      "step": 509900
    },
    {
      "epoch": 8.006279434850864,
      "grad_norm": 4.298671722412109,
      "learning_rate": 4.4996075353218214e-05,
      "loss": 0.5978,
      "step": 510000
    },
    {
      "epoch": 8.007849293563579,
      "grad_norm": 3.6611907482147217,
      "learning_rate": 4.4995094191522765e-05,
      "loss": 0.6212,
      "step": 510100
    },
    {
      "epoch": 8.009419152276296,
      "grad_norm": 4.334061622619629,
      "learning_rate": 4.4994113029827315e-05,
      "loss": 0.6567,
      "step": 510200
    },
    {
      "epoch": 8.010989010989011,
      "grad_norm": 2.7359094619750977,
      "learning_rate": 4.499313186813187e-05,
      "loss": 0.6127,
      "step": 510300
    },
    {
      "epoch": 8.012558869701726,
      "grad_norm": 3.775719165802002,
      "learning_rate": 4.4992150706436424e-05,
      "loss": 0.6439,
      "step": 510400
    },
    {
      "epoch": 8.014128728414443,
      "grad_norm": 4.516295433044434,
      "learning_rate": 4.4991169544740975e-05,
      "loss": 0.6459,
      "step": 510500
    },
    {
      "epoch": 8.015698587127158,
      "grad_norm": 4.42476749420166,
      "learning_rate": 4.4990188383045526e-05,
      "loss": 0.6537,
      "step": 510600
    },
    {
      "epoch": 8.017268445839875,
      "grad_norm": 3.7646806240081787,
      "learning_rate": 4.4989207221350084e-05,
      "loss": 0.6544,
      "step": 510700
    },
    {
      "epoch": 8.01883830455259,
      "grad_norm": 4.179827690124512,
      "learning_rate": 4.498822605965463e-05,
      "loss": 0.6366,
      "step": 510800
    },
    {
      "epoch": 8.020408163265307,
      "grad_norm": 3.758852481842041,
      "learning_rate": 4.4987244897959186e-05,
      "loss": 0.6292,
      "step": 510900
    },
    {
      "epoch": 8.021978021978022,
      "grad_norm": 4.021727085113525,
      "learning_rate": 4.498626373626374e-05,
      "loss": 0.6544,
      "step": 511000
    },
    {
      "epoch": 8.023547880690737,
      "grad_norm": 4.164616584777832,
      "learning_rate": 4.4985282574568295e-05,
      "loss": 0.6043,
      "step": 511100
    },
    {
      "epoch": 8.025117739403454,
      "grad_norm": 2.5130410194396973,
      "learning_rate": 4.4984301412872846e-05,
      "loss": 0.6344,
      "step": 511200
    },
    {
      "epoch": 8.026687598116169,
      "grad_norm": 4.409287929534912,
      "learning_rate": 4.49833202511774e-05,
      "loss": 0.6236,
      "step": 511300
    },
    {
      "epoch": 8.028257456828886,
      "grad_norm": 4.534342288970947,
      "learning_rate": 4.498233908948195e-05,
      "loss": 0.632,
      "step": 511400
    },
    {
      "epoch": 8.029827315541601,
      "grad_norm": 4.9688334465026855,
      "learning_rate": 4.49813579277865e-05,
      "loss": 0.6528,
      "step": 511500
    },
    {
      "epoch": 8.031397174254318,
      "grad_norm": 3.807053565979004,
      "learning_rate": 4.4980376766091056e-05,
      "loss": 0.598,
      "step": 511600
    },
    {
      "epoch": 8.032967032967033,
      "grad_norm": 4.0128397941589355,
      "learning_rate": 4.497939560439561e-05,
      "loss": 0.581,
      "step": 511700
    },
    {
      "epoch": 8.034536891679748,
      "grad_norm": 3.9351625442504883,
      "learning_rate": 4.4978414442700165e-05,
      "loss": 0.621,
      "step": 511800
    },
    {
      "epoch": 8.036106750392465,
      "grad_norm": 3.5377490520477295,
      "learning_rate": 4.497743328100471e-05,
      "loss": 0.66,
      "step": 511900
    },
    {
      "epoch": 8.03767660910518,
      "grad_norm": 4.654923439025879,
      "learning_rate": 4.497645211930927e-05,
      "loss": 0.6527,
      "step": 512000
    },
    {
      "epoch": 8.039246467817897,
      "grad_norm": 4.192917823791504,
      "learning_rate": 4.497547095761382e-05,
      "loss": 0.6282,
      "step": 512100
    },
    {
      "epoch": 8.040816326530612,
      "grad_norm": 3.7026119232177734,
      "learning_rate": 4.497448979591837e-05,
      "loss": 0.66,
      "step": 512200
    },
    {
      "epoch": 8.042386185243329,
      "grad_norm": 3.5766685009002686,
      "learning_rate": 4.497350863422292e-05,
      "loss": 0.6504,
      "step": 512300
    },
    {
      "epoch": 8.043956043956044,
      "grad_norm": 4.6989617347717285,
      "learning_rate": 4.497252747252748e-05,
      "loss": 0.637,
      "step": 512400
    },
    {
      "epoch": 8.04552590266876,
      "grad_norm": 4.7281928062438965,
      "learning_rate": 4.497154631083202e-05,
      "loss": 0.6589,
      "step": 512500
    },
    {
      "epoch": 8.047095761381476,
      "grad_norm": 4.341664791107178,
      "learning_rate": 4.497056514913658e-05,
      "loss": 0.6554,
      "step": 512600
    },
    {
      "epoch": 8.04866562009419,
      "grad_norm": 3.5629959106445312,
      "learning_rate": 4.496958398744113e-05,
      "loss": 0.6559,
      "step": 512700
    },
    {
      "epoch": 8.050235478806908,
      "grad_norm": 4.544864177703857,
      "learning_rate": 4.496860282574569e-05,
      "loss": 0.6357,
      "step": 512800
    },
    {
      "epoch": 8.051805337519623,
      "grad_norm": 4.56658935546875,
      "learning_rate": 4.496762166405023e-05,
      "loss": 0.6481,
      "step": 512900
    },
    {
      "epoch": 8.05337519623234,
      "grad_norm": 3.7336506843566895,
      "learning_rate": 4.496664050235479e-05,
      "loss": 0.5947,
      "step": 513000
    },
    {
      "epoch": 8.054945054945055,
      "grad_norm": 4.057068347930908,
      "learning_rate": 4.496565934065934e-05,
      "loss": 0.6181,
      "step": 513100
    },
    {
      "epoch": 8.056514913657772,
      "grad_norm": 5.137832164764404,
      "learning_rate": 4.496467817896389e-05,
      "loss": 0.6326,
      "step": 513200
    },
    {
      "epoch": 8.058084772370487,
      "grad_norm": 4.128127574920654,
      "learning_rate": 4.496369701726845e-05,
      "loss": 0.6531,
      "step": 513300
    },
    {
      "epoch": 8.059654631083202,
      "grad_norm": 3.482174873352051,
      "learning_rate": 4.4962715855573e-05,
      "loss": 0.6105,
      "step": 513400
    },
    {
      "epoch": 8.061224489795919,
      "grad_norm": 4.721871852874756,
      "learning_rate": 4.496173469387755e-05,
      "loss": 0.6507,
      "step": 513500
    },
    {
      "epoch": 8.062794348508634,
      "grad_norm": 4.155844688415527,
      "learning_rate": 4.49607535321821e-05,
      "loss": 0.6213,
      "step": 513600
    },
    {
      "epoch": 8.06436420722135,
      "grad_norm": 5.117886066436768,
      "learning_rate": 4.495977237048666e-05,
      "loss": 0.6439,
      "step": 513700
    },
    {
      "epoch": 8.065934065934066,
      "grad_norm": 4.384549140930176,
      "learning_rate": 4.495879120879121e-05,
      "loss": 0.6575,
      "step": 513800
    },
    {
      "epoch": 8.067503924646783,
      "grad_norm": 3.5843048095703125,
      "learning_rate": 4.495781004709576e-05,
      "loss": 0.6122,
      "step": 513900
    },
    {
      "epoch": 8.069073783359498,
      "grad_norm": 2.725214719772339,
      "learning_rate": 4.4956828885400314e-05,
      "loss": 0.5867,
      "step": 514000
    },
    {
      "epoch": 8.070643642072213,
      "grad_norm": 4.852629661560059,
      "learning_rate": 4.495584772370487e-05,
      "loss": 0.6461,
      "step": 514100
    },
    {
      "epoch": 8.07221350078493,
      "grad_norm": 3.5923714637756348,
      "learning_rate": 4.495486656200942e-05,
      "loss": 0.6299,
      "step": 514200
    },
    {
      "epoch": 8.073783359497645,
      "grad_norm": 4.710148334503174,
      "learning_rate": 4.4953885400313973e-05,
      "loss": 0.6392,
      "step": 514300
    },
    {
      "epoch": 8.075353218210362,
      "grad_norm": 3.8358333110809326,
      "learning_rate": 4.4952904238618524e-05,
      "loss": 0.6694,
      "step": 514400
    },
    {
      "epoch": 8.076923076923077,
      "grad_norm": 2.8247945308685303,
      "learning_rate": 4.495192307692308e-05,
      "loss": 0.6265,
      "step": 514500
    },
    {
      "epoch": 8.078492935635794,
      "grad_norm": 3.017829179763794,
      "learning_rate": 4.4950941915227626e-05,
      "loss": 0.6299,
      "step": 514600
    },
    {
      "epoch": 8.080062794348509,
      "grad_norm": 4.5231547355651855,
      "learning_rate": 4.4949960753532184e-05,
      "loss": 0.6485,
      "step": 514700
    },
    {
      "epoch": 8.081632653061224,
      "grad_norm": 4.6150031089782715,
      "learning_rate": 4.4948979591836735e-05,
      "loss": 0.6704,
      "step": 514800
    },
    {
      "epoch": 8.08320251177394,
      "grad_norm": 4.267635345458984,
      "learning_rate": 4.494799843014129e-05,
      "loss": 0.6507,
      "step": 514900
    },
    {
      "epoch": 8.084772370486656,
      "grad_norm": 3.7325215339660645,
      "learning_rate": 4.494701726844584e-05,
      "loss": 0.645,
      "step": 515000
    },
    {
      "epoch": 8.086342229199373,
      "grad_norm": 2.774656295776367,
      "learning_rate": 4.4946036106750395e-05,
      "loss": 0.6635,
      "step": 515100
    },
    {
      "epoch": 8.087912087912088,
      "grad_norm": 3.5095648765563965,
      "learning_rate": 4.4945054945054946e-05,
      "loss": 0.6473,
      "step": 515200
    },
    {
      "epoch": 8.089481946624804,
      "grad_norm": 4.7614054679870605,
      "learning_rate": 4.49440737833595e-05,
      "loss": 0.6514,
      "step": 515300
    },
    {
      "epoch": 8.09105180533752,
      "grad_norm": 3.888643264770508,
      "learning_rate": 4.4943092621664055e-05,
      "loss": 0.6286,
      "step": 515400
    },
    {
      "epoch": 8.092621664050235,
      "grad_norm": 3.1009652614593506,
      "learning_rate": 4.4942111459968606e-05,
      "loss": 0.62,
      "step": 515500
    },
    {
      "epoch": 8.094191522762952,
      "grad_norm": 5.641409873962402,
      "learning_rate": 4.4941130298273157e-05,
      "loss": 0.6456,
      "step": 515600
    },
    {
      "epoch": 8.095761381475667,
      "grad_norm": 3.9797794818878174,
      "learning_rate": 4.494014913657771e-05,
      "loss": 0.625,
      "step": 515700
    },
    {
      "epoch": 8.097331240188383,
      "grad_norm": 4.557952880859375,
      "learning_rate": 4.4939167974882265e-05,
      "loss": 0.6342,
      "step": 515800
    },
    {
      "epoch": 8.098901098901099,
      "grad_norm": 3.5000672340393066,
      "learning_rate": 4.4938186813186816e-05,
      "loss": 0.5963,
      "step": 515900
    },
    {
      "epoch": 8.100470957613815,
      "grad_norm": 4.177694797515869,
      "learning_rate": 4.493720565149137e-05,
      "loss": 0.6592,
      "step": 516000
    },
    {
      "epoch": 8.10204081632653,
      "grad_norm": 4.245131492614746,
      "learning_rate": 4.493622448979592e-05,
      "loss": 0.6538,
      "step": 516100
    },
    {
      "epoch": 8.103610675039246,
      "grad_norm": 3.2945234775543213,
      "learning_rate": 4.4935243328100476e-05,
      "loss": 0.656,
      "step": 516200
    },
    {
      "epoch": 8.105180533751962,
      "grad_norm": 4.270898342132568,
      "learning_rate": 4.493426216640503e-05,
      "loss": 0.684,
      "step": 516300
    },
    {
      "epoch": 8.106750392464678,
      "grad_norm": 4.244873523712158,
      "learning_rate": 4.493328100470958e-05,
      "loss": 0.6778,
      "step": 516400
    },
    {
      "epoch": 8.108320251177394,
      "grad_norm": 4.327705383300781,
      "learning_rate": 4.493229984301413e-05,
      "loss": 0.5945,
      "step": 516500
    },
    {
      "epoch": 8.10989010989011,
      "grad_norm": 5.757964611053467,
      "learning_rate": 4.493131868131869e-05,
      "loss": 0.6754,
      "step": 516600
    },
    {
      "epoch": 8.111459968602826,
      "grad_norm": 4.584496974945068,
      "learning_rate": 4.493033751962323e-05,
      "loss": 0.6192,
      "step": 516700
    },
    {
      "epoch": 8.113029827315541,
      "grad_norm": 4.157893180847168,
      "learning_rate": 4.492935635792779e-05,
      "loss": 0.6349,
      "step": 516800
    },
    {
      "epoch": 8.114599686028258,
      "grad_norm": 3.073273181915283,
      "learning_rate": 4.492837519623234e-05,
      "loss": 0.6335,
      "step": 516900
    },
    {
      "epoch": 8.116169544740973,
      "grad_norm": 4.044925212860107,
      "learning_rate": 4.49273940345369e-05,
      "loss": 0.6685,
      "step": 517000
    },
    {
      "epoch": 8.117739403453688,
      "grad_norm": 4.057754993438721,
      "learning_rate": 4.492641287284144e-05,
      "loss": 0.6454,
      "step": 517100
    },
    {
      "epoch": 8.119309262166405,
      "grad_norm": 3.9919161796569824,
      "learning_rate": 4.4925431711146e-05,
      "loss": 0.6427,
      "step": 517200
    },
    {
      "epoch": 8.12087912087912,
      "grad_norm": 4.286557197570801,
      "learning_rate": 4.492445054945055e-05,
      "loss": 0.6342,
      "step": 517300
    },
    {
      "epoch": 8.122448979591837,
      "grad_norm": 2.9356741905212402,
      "learning_rate": 4.49234693877551e-05,
      "loss": 0.6362,
      "step": 517400
    },
    {
      "epoch": 8.124018838304552,
      "grad_norm": 3.289193630218506,
      "learning_rate": 4.492248822605966e-05,
      "loss": 0.6637,
      "step": 517500
    },
    {
      "epoch": 8.12558869701727,
      "grad_norm": 4.266867160797119,
      "learning_rate": 4.492150706436421e-05,
      "loss": 0.664,
      "step": 517600
    },
    {
      "epoch": 8.127158555729984,
      "grad_norm": 3.8966667652130127,
      "learning_rate": 4.492052590266876e-05,
      "loss": 0.6088,
      "step": 517700
    },
    {
      "epoch": 8.1287284144427,
      "grad_norm": 4.149702548980713,
      "learning_rate": 4.491954474097331e-05,
      "loss": 0.6638,
      "step": 517800
    },
    {
      "epoch": 8.130298273155416,
      "grad_norm": 4.647461414337158,
      "learning_rate": 4.491856357927787e-05,
      "loss": 0.6318,
      "step": 517900
    },
    {
      "epoch": 8.131868131868131,
      "grad_norm": 3.812513589859009,
      "learning_rate": 4.491758241758242e-05,
      "loss": 0.6643,
      "step": 518000
    },
    {
      "epoch": 8.133437990580848,
      "grad_norm": 3.7640440464019775,
      "learning_rate": 4.491660125588697e-05,
      "loss": 0.6589,
      "step": 518100
    },
    {
      "epoch": 8.135007849293563,
      "grad_norm": 3.463369607925415,
      "learning_rate": 4.491562009419152e-05,
      "loss": 0.6466,
      "step": 518200
    },
    {
      "epoch": 8.13657770800628,
      "grad_norm": 2.7706046104431152,
      "learning_rate": 4.491463893249608e-05,
      "loss": 0.6574,
      "step": 518300
    },
    {
      "epoch": 8.138147566718995,
      "grad_norm": 3.0131306648254395,
      "learning_rate": 4.491365777080063e-05,
      "loss": 0.6856,
      "step": 518400
    },
    {
      "epoch": 8.13971742543171,
      "grad_norm": 2.199110984802246,
      "learning_rate": 4.491267660910518e-05,
      "loss": 0.6349,
      "step": 518500
    },
    {
      "epoch": 8.141287284144427,
      "grad_norm": 4.942772388458252,
      "learning_rate": 4.4911695447409733e-05,
      "loss": 0.6292,
      "step": 518600
    },
    {
      "epoch": 8.142857142857142,
      "grad_norm": 4.178452968597412,
      "learning_rate": 4.491071428571429e-05,
      "loss": 0.656,
      "step": 518700
    },
    {
      "epoch": 8.14442700156986,
      "grad_norm": 4.050422191619873,
      "learning_rate": 4.4909733124018835e-05,
      "loss": 0.6431,
      "step": 518800
    },
    {
      "epoch": 8.145996860282574,
      "grad_norm": 3.881131887435913,
      "learning_rate": 4.490875196232339e-05,
      "loss": 0.6434,
      "step": 518900
    },
    {
      "epoch": 8.147566718995291,
      "grad_norm": 4.855235576629639,
      "learning_rate": 4.4907770800627944e-05,
      "loss": 0.6933,
      "step": 519000
    },
    {
      "epoch": 8.149136577708006,
      "grad_norm": 4.351293563842773,
      "learning_rate": 4.49067896389325e-05,
      "loss": 0.6445,
      "step": 519100
    },
    {
      "epoch": 8.150706436420721,
      "grad_norm": 5.164736747741699,
      "learning_rate": 4.4905808477237046e-05,
      "loss": 0.6101,
      "step": 519200
    },
    {
      "epoch": 8.152276295133438,
      "grad_norm": 3.4527225494384766,
      "learning_rate": 4.4904827315541604e-05,
      "loss": 0.6337,
      "step": 519300
    },
    {
      "epoch": 8.153846153846153,
      "grad_norm": 2.7823009490966797,
      "learning_rate": 4.4903846153846155e-05,
      "loss": 0.6919,
      "step": 519400
    },
    {
      "epoch": 8.15541601255887,
      "grad_norm": 3.0727477073669434,
      "learning_rate": 4.4902864992150706e-05,
      "loss": 0.6625,
      "step": 519500
    },
    {
      "epoch": 8.156985871271585,
      "grad_norm": 4.085124969482422,
      "learning_rate": 4.4901883830455264e-05,
      "loss": 0.6755,
      "step": 519600
    },
    {
      "epoch": 8.158555729984302,
      "grad_norm": 3.4364430904388428,
      "learning_rate": 4.4900902668759815e-05,
      "loss": 0.6219,
      "step": 519700
    },
    {
      "epoch": 8.160125588697017,
      "grad_norm": 4.231364727020264,
      "learning_rate": 4.4899921507064366e-05,
      "loss": 0.6007,
      "step": 519800
    },
    {
      "epoch": 8.161695447409732,
      "grad_norm": 4.598134517669678,
      "learning_rate": 4.4898940345368917e-05,
      "loss": 0.614,
      "step": 519900
    },
    {
      "epoch": 8.16326530612245,
      "grad_norm": 2.8530054092407227,
      "learning_rate": 4.4897959183673474e-05,
      "loss": 0.6484,
      "step": 520000
    },
    {
      "epoch": 8.164835164835164,
      "grad_norm": 4.923865795135498,
      "learning_rate": 4.4896978021978025e-05,
      "loss": 0.6208,
      "step": 520100
    },
    {
      "epoch": 8.166405023547881,
      "grad_norm": 3.956770658493042,
      "learning_rate": 4.4895996860282576e-05,
      "loss": 0.666,
      "step": 520200
    },
    {
      "epoch": 8.167974882260596,
      "grad_norm": 3.9882707595825195,
      "learning_rate": 4.489501569858713e-05,
      "loss": 0.6596,
      "step": 520300
    },
    {
      "epoch": 8.169544740973313,
      "grad_norm": 4.533645153045654,
      "learning_rate": 4.4894034536891685e-05,
      "loss": 0.6873,
      "step": 520400
    },
    {
      "epoch": 8.171114599686028,
      "grad_norm": 4.8014397621154785,
      "learning_rate": 4.4893053375196236e-05,
      "loss": 0.6444,
      "step": 520500
    },
    {
      "epoch": 8.172684458398743,
      "grad_norm": 4.042928218841553,
      "learning_rate": 4.489207221350079e-05,
      "loss": 0.6195,
      "step": 520600
    },
    {
      "epoch": 8.17425431711146,
      "grad_norm": 3.3217368125915527,
      "learning_rate": 4.489109105180534e-05,
      "loss": 0.6538,
      "step": 520700
    },
    {
      "epoch": 8.175824175824175,
      "grad_norm": 4.036296367645264,
      "learning_rate": 4.4890109890109896e-05,
      "loss": 0.6571,
      "step": 520800
    },
    {
      "epoch": 8.177394034536892,
      "grad_norm": 4.071994781494141,
      "learning_rate": 4.488912872841444e-05,
      "loss": 0.6228,
      "step": 520900
    },
    {
      "epoch": 8.178963893249607,
      "grad_norm": 4.278880596160889,
      "learning_rate": 4.4888147566719e-05,
      "loss": 0.6448,
      "step": 521000
    },
    {
      "epoch": 8.180533751962324,
      "grad_norm": 4.2120866775512695,
      "learning_rate": 4.488716640502355e-05,
      "loss": 0.6247,
      "step": 521100
    },
    {
      "epoch": 8.182103610675039,
      "grad_norm": 4.589147567749023,
      "learning_rate": 4.4886185243328106e-05,
      "loss": 0.6608,
      "step": 521200
    },
    {
      "epoch": 8.183673469387756,
      "grad_norm": 4.529997825622559,
      "learning_rate": 4.488520408163265e-05,
      "loss": 0.6414,
      "step": 521300
    },
    {
      "epoch": 8.185243328100471,
      "grad_norm": 2.1450743675231934,
      "learning_rate": 4.488422291993721e-05,
      "loss": 0.6157,
      "step": 521400
    },
    {
      "epoch": 8.186813186813186,
      "grad_norm": 2.9252147674560547,
      "learning_rate": 4.488324175824176e-05,
      "loss": 0.6375,
      "step": 521500
    },
    {
      "epoch": 8.188383045525903,
      "grad_norm": 3.8914740085601807,
      "learning_rate": 4.488226059654631e-05,
      "loss": 0.6353,
      "step": 521600
    },
    {
      "epoch": 8.189952904238618,
      "grad_norm": 3.871157646179199,
      "learning_rate": 4.488127943485087e-05,
      "loss": 0.6177,
      "step": 521700
    },
    {
      "epoch": 8.191522762951335,
      "grad_norm": 4.113392353057861,
      "learning_rate": 4.488029827315542e-05,
      "loss": 0.6422,
      "step": 521800
    },
    {
      "epoch": 8.19309262166405,
      "grad_norm": 4.390649318695068,
      "learning_rate": 4.487931711145997e-05,
      "loss": 0.6214,
      "step": 521900
    },
    {
      "epoch": 8.194662480376767,
      "grad_norm": 4.588747978210449,
      "learning_rate": 4.487833594976452e-05,
      "loss": 0.6305,
      "step": 522000
    },
    {
      "epoch": 8.196232339089482,
      "grad_norm": 4.771842956542969,
      "learning_rate": 4.487735478806908e-05,
      "loss": 0.6437,
      "step": 522100
    },
    {
      "epoch": 8.197802197802197,
      "grad_norm": 3.5838210582733154,
      "learning_rate": 4.487637362637363e-05,
      "loss": 0.6509,
      "step": 522200
    },
    {
      "epoch": 8.199372056514914,
      "grad_norm": 2.825913429260254,
      "learning_rate": 4.487539246467818e-05,
      "loss": 0.6514,
      "step": 522300
    },
    {
      "epoch": 8.200941915227629,
      "grad_norm": 4.683938980102539,
      "learning_rate": 4.487441130298273e-05,
      "loss": 0.5911,
      "step": 522400
    },
    {
      "epoch": 8.202511773940346,
      "grad_norm": 3.7337820529937744,
      "learning_rate": 4.487343014128729e-05,
      "loss": 0.6747,
      "step": 522500
    },
    {
      "epoch": 8.204081632653061,
      "grad_norm": 4.332239151000977,
      "learning_rate": 4.487244897959184e-05,
      "loss": 0.6187,
      "step": 522600
    },
    {
      "epoch": 8.205651491365778,
      "grad_norm": 3.53818941116333,
      "learning_rate": 4.487146781789639e-05,
      "loss": 0.6224,
      "step": 522700
    },
    {
      "epoch": 8.207221350078493,
      "grad_norm": 4.207333564758301,
      "learning_rate": 4.487048665620094e-05,
      "loss": 0.6625,
      "step": 522800
    },
    {
      "epoch": 8.208791208791208,
      "grad_norm": 4.702861785888672,
      "learning_rate": 4.48695054945055e-05,
      "loss": 0.6357,
      "step": 522900
    },
    {
      "epoch": 8.210361067503925,
      "grad_norm": 4.068239688873291,
      "learning_rate": 4.4868524332810044e-05,
      "loss": 0.5862,
      "step": 523000
    },
    {
      "epoch": 8.21193092621664,
      "grad_norm": 4.369140625,
      "learning_rate": 4.48675431711146e-05,
      "loss": 0.6335,
      "step": 523100
    },
    {
      "epoch": 8.213500784929357,
      "grad_norm": 4.318854808807373,
      "learning_rate": 4.486656200941915e-05,
      "loss": 0.6496,
      "step": 523200
    },
    {
      "epoch": 8.215070643642072,
      "grad_norm": 2.984241008758545,
      "learning_rate": 4.486558084772371e-05,
      "loss": 0.6343,
      "step": 523300
    },
    {
      "epoch": 8.216640502354789,
      "grad_norm": 3.9206783771514893,
      "learning_rate": 4.4864599686028255e-05,
      "loss": 0.5594,
      "step": 523400
    },
    {
      "epoch": 8.218210361067504,
      "grad_norm": 4.410781383514404,
      "learning_rate": 4.486361852433281e-05,
      "loss": 0.6886,
      "step": 523500
    },
    {
      "epoch": 8.219780219780219,
      "grad_norm": 3.8809237480163574,
      "learning_rate": 4.4862637362637364e-05,
      "loss": 0.6499,
      "step": 523600
    },
    {
      "epoch": 8.221350078492936,
      "grad_norm": 3.4920196533203125,
      "learning_rate": 4.4861656200941915e-05,
      "loss": 0.6381,
      "step": 523700
    },
    {
      "epoch": 8.222919937205651,
      "grad_norm": 3.578723907470703,
      "learning_rate": 4.4860675039246466e-05,
      "loss": 0.6481,
      "step": 523800
    },
    {
      "epoch": 8.224489795918368,
      "grad_norm": 4.065830230712891,
      "learning_rate": 4.4859693877551024e-05,
      "loss": 0.6465,
      "step": 523900
    },
    {
      "epoch": 8.226059654631083,
      "grad_norm": 3.439362049102783,
      "learning_rate": 4.4858712715855575e-05,
      "loss": 0.6258,
      "step": 524000
    },
    {
      "epoch": 8.2276295133438,
      "grad_norm": 4.215141773223877,
      "learning_rate": 4.4857731554160126e-05,
      "loss": 0.6782,
      "step": 524100
    },
    {
      "epoch": 8.229199372056515,
      "grad_norm": 4.135444641113281,
      "learning_rate": 4.485675039246468e-05,
      "loss": 0.6553,
      "step": 524200
    },
    {
      "epoch": 8.23076923076923,
      "grad_norm": 4.58133602142334,
      "learning_rate": 4.4855769230769234e-05,
      "loss": 0.6541,
      "step": 524300
    },
    {
      "epoch": 8.232339089481947,
      "grad_norm": 4.187436580657959,
      "learning_rate": 4.4854788069073785e-05,
      "loss": 0.6233,
      "step": 524400
    },
    {
      "epoch": 8.233908948194662,
      "grad_norm": 4.380456924438477,
      "learning_rate": 4.4853806907378336e-05,
      "loss": 0.6571,
      "step": 524500
    },
    {
      "epoch": 8.235478806907379,
      "grad_norm": 3.1572515964508057,
      "learning_rate": 4.4852825745682894e-05,
      "loss": 0.6375,
      "step": 524600
    },
    {
      "epoch": 8.237048665620094,
      "grad_norm": 5.209408283233643,
      "learning_rate": 4.4851844583987445e-05,
      "loss": 0.6433,
      "step": 524700
    },
    {
      "epoch": 8.23861852433281,
      "grad_norm": 4.816285610198975,
      "learning_rate": 4.4850863422291996e-05,
      "loss": 0.6378,
      "step": 524800
    },
    {
      "epoch": 8.240188383045526,
      "grad_norm": 4.110683917999268,
      "learning_rate": 4.484988226059655e-05,
      "loss": 0.6085,
      "step": 524900
    },
    {
      "epoch": 8.241758241758241,
      "grad_norm": 4.940200328826904,
      "learning_rate": 4.4848901098901105e-05,
      "loss": 0.6746,
      "step": 525000
    },
    {
      "epoch": 8.243328100470958,
      "grad_norm": 3.3036859035491943,
      "learning_rate": 4.484791993720565e-05,
      "loss": 0.634,
      "step": 525100
    },
    {
      "epoch": 8.244897959183673,
      "grad_norm": 2.830061674118042,
      "learning_rate": 4.484693877551021e-05,
      "loss": 0.6329,
      "step": 525200
    },
    {
      "epoch": 8.24646781789639,
      "grad_norm": 3.5725252628326416,
      "learning_rate": 4.484595761381476e-05,
      "loss": 0.6331,
      "step": 525300
    },
    {
      "epoch": 8.248037676609105,
      "grad_norm": 2.9441075325012207,
      "learning_rate": 4.4844976452119315e-05,
      "loss": 0.6173,
      "step": 525400
    },
    {
      "epoch": 8.249607535321822,
      "grad_norm": 4.914233684539795,
      "learning_rate": 4.484399529042386e-05,
      "loss": 0.646,
      "step": 525500
    },
    {
      "epoch": 8.251177394034537,
      "grad_norm": 3.886608123779297,
      "learning_rate": 4.484301412872842e-05,
      "loss": 0.6372,
      "step": 525600
    },
    {
      "epoch": 8.252747252747252,
      "grad_norm": 3.7887370586395264,
      "learning_rate": 4.484203296703297e-05,
      "loss": 0.64,
      "step": 525700
    },
    {
      "epoch": 8.254317111459969,
      "grad_norm": 2.900068521499634,
      "learning_rate": 4.484105180533752e-05,
      "loss": 0.715,
      "step": 525800
    },
    {
      "epoch": 8.255886970172684,
      "grad_norm": 3.8259644508361816,
      "learning_rate": 4.484007064364207e-05,
      "loss": 0.6048,
      "step": 525900
    },
    {
      "epoch": 8.2574568288854,
      "grad_norm": 3.865527629852295,
      "learning_rate": 4.483908948194663e-05,
      "loss": 0.665,
      "step": 526000
    },
    {
      "epoch": 8.259026687598116,
      "grad_norm": 3.2556564807891846,
      "learning_rate": 4.483810832025118e-05,
      "loss": 0.597,
      "step": 526100
    },
    {
      "epoch": 8.260596546310833,
      "grad_norm": 4.722154140472412,
      "learning_rate": 4.483712715855573e-05,
      "loss": 0.6749,
      "step": 526200
    },
    {
      "epoch": 8.262166405023548,
      "grad_norm": 3.8695662021636963,
      "learning_rate": 4.483614599686029e-05,
      "loss": 0.6474,
      "step": 526300
    },
    {
      "epoch": 8.263736263736265,
      "grad_norm": 3.804967164993286,
      "learning_rate": 4.483516483516484e-05,
      "loss": 0.6591,
      "step": 526400
    },
    {
      "epoch": 8.26530612244898,
      "grad_norm": 4.0463457107543945,
      "learning_rate": 4.483418367346939e-05,
      "loss": 0.6467,
      "step": 526500
    },
    {
      "epoch": 8.266875981161695,
      "grad_norm": 3.0601298809051514,
      "learning_rate": 4.483320251177394e-05,
      "loss": 0.6021,
      "step": 526600
    },
    {
      "epoch": 8.268445839874412,
      "grad_norm": 4.206435203552246,
      "learning_rate": 4.48322213500785e-05,
      "loss": 0.6156,
      "step": 526700
    },
    {
      "epoch": 8.270015698587127,
      "grad_norm": 3.7739808559417725,
      "learning_rate": 4.483124018838305e-05,
      "loss": 0.6784,
      "step": 526800
    },
    {
      "epoch": 8.271585557299844,
      "grad_norm": 4.168003559112549,
      "learning_rate": 4.48302590266876e-05,
      "loss": 0.6613,
      "step": 526900
    },
    {
      "epoch": 8.273155416012559,
      "grad_norm": 3.521066665649414,
      "learning_rate": 4.482927786499215e-05,
      "loss": 0.6063,
      "step": 527000
    },
    {
      "epoch": 8.274725274725276,
      "grad_norm": 3.27634859085083,
      "learning_rate": 4.482829670329671e-05,
      "loss": 0.6568,
      "step": 527100
    },
    {
      "epoch": 8.27629513343799,
      "grad_norm": 4.277988910675049,
      "learning_rate": 4.482731554160125e-05,
      "loss": 0.6338,
      "step": 527200
    },
    {
      "epoch": 8.277864992150706,
      "grad_norm": 3.4101006984710693,
      "learning_rate": 4.482633437990581e-05,
      "loss": 0.6435,
      "step": 527300
    },
    {
      "epoch": 8.279434850863423,
      "grad_norm": 4.176324367523193,
      "learning_rate": 4.482535321821036e-05,
      "loss": 0.6697,
      "step": 527400
    },
    {
      "epoch": 8.281004709576138,
      "grad_norm": 3.958913803100586,
      "learning_rate": 4.482437205651492e-05,
      "loss": 0.5958,
      "step": 527500
    },
    {
      "epoch": 8.282574568288855,
      "grad_norm": 4.076059818267822,
      "learning_rate": 4.4823390894819464e-05,
      "loss": 0.6689,
      "step": 527600
    },
    {
      "epoch": 8.28414442700157,
      "grad_norm": 4.882373809814453,
      "learning_rate": 4.482240973312402e-05,
      "loss": 0.6115,
      "step": 527700
    },
    {
      "epoch": 8.285714285714286,
      "grad_norm": 4.279069423675537,
      "learning_rate": 4.482142857142857e-05,
      "loss": 0.6458,
      "step": 527800
    },
    {
      "epoch": 8.287284144427002,
      "grad_norm": 4.688267707824707,
      "learning_rate": 4.4820447409733124e-05,
      "loss": 0.6174,
      "step": 527900
    },
    {
      "epoch": 8.288854003139717,
      "grad_norm": 4.3394646644592285,
      "learning_rate": 4.4819466248037675e-05,
      "loss": 0.656,
      "step": 528000
    },
    {
      "epoch": 8.290423861852434,
      "grad_norm": 3.8685364723205566,
      "learning_rate": 4.481848508634223e-05,
      "loss": 0.6321,
      "step": 528100
    },
    {
      "epoch": 8.291993720565149,
      "grad_norm": 2.904529333114624,
      "learning_rate": 4.4817503924646784e-05,
      "loss": 0.6334,
      "step": 528200
    },
    {
      "epoch": 8.293563579277865,
      "grad_norm": 3.533555507659912,
      "learning_rate": 4.4816522762951335e-05,
      "loss": 0.6759,
      "step": 528300
    },
    {
      "epoch": 8.29513343799058,
      "grad_norm": 4.2849345207214355,
      "learning_rate": 4.481554160125589e-05,
      "loss": 0.6335,
      "step": 528400
    },
    {
      "epoch": 8.296703296703297,
      "grad_norm": 4.490040302276611,
      "learning_rate": 4.481456043956044e-05,
      "loss": 0.6682,
      "step": 528500
    },
    {
      "epoch": 8.298273155416013,
      "grad_norm": 4.175706386566162,
      "learning_rate": 4.4813579277864994e-05,
      "loss": 0.6402,
      "step": 528600
    },
    {
      "epoch": 8.299843014128728,
      "grad_norm": 4.762386798858643,
      "learning_rate": 4.4812598116169545e-05,
      "loss": 0.6653,
      "step": 528700
    },
    {
      "epoch": 8.301412872841444,
      "grad_norm": 4.110306739807129,
      "learning_rate": 4.48116169544741e-05,
      "loss": 0.678,
      "step": 528800
    },
    {
      "epoch": 8.30298273155416,
      "grad_norm": 4.122762203216553,
      "learning_rate": 4.4810635792778654e-05,
      "loss": 0.6307,
      "step": 528900
    },
    {
      "epoch": 8.304552590266876,
      "grad_norm": 3.9811949729919434,
      "learning_rate": 4.4809654631083205e-05,
      "loss": 0.6602,
      "step": 529000
    },
    {
      "epoch": 8.306122448979592,
      "grad_norm": 3.091981887817383,
      "learning_rate": 4.4808673469387756e-05,
      "loss": 0.6452,
      "step": 529100
    },
    {
      "epoch": 8.307692307692308,
      "grad_norm": 4.0108418464660645,
      "learning_rate": 4.4807692307692314e-05,
      "loss": 0.6271,
      "step": 529200
    },
    {
      "epoch": 8.309262166405023,
      "grad_norm": 4.314752578735352,
      "learning_rate": 4.480671114599686e-05,
      "loss": 0.6375,
      "step": 529300
    },
    {
      "epoch": 8.310832025117739,
      "grad_norm": 4.908102035522461,
      "learning_rate": 4.4805729984301416e-05,
      "loss": 0.643,
      "step": 529400
    },
    {
      "epoch": 8.312401883830455,
      "grad_norm": 3.6541342735290527,
      "learning_rate": 4.480474882260597e-05,
      "loss": 0.6719,
      "step": 529500
    },
    {
      "epoch": 8.31397174254317,
      "grad_norm": 4.798084735870361,
      "learning_rate": 4.4803767660910524e-05,
      "loss": 0.6492,
      "step": 529600
    },
    {
      "epoch": 8.315541601255887,
      "grad_norm": 3.7034695148468018,
      "learning_rate": 4.480278649921507e-05,
      "loss": 0.6341,
      "step": 529700
    },
    {
      "epoch": 8.317111459968602,
      "grad_norm": 4.217511177062988,
      "learning_rate": 4.4801805337519626e-05,
      "loss": 0.6686,
      "step": 529800
    },
    {
      "epoch": 8.31868131868132,
      "grad_norm": 3.8183271884918213,
      "learning_rate": 4.480082417582418e-05,
      "loss": 0.6301,
      "step": 529900
    },
    {
      "epoch": 8.320251177394034,
      "grad_norm": 2.9203028678894043,
      "learning_rate": 4.479984301412873e-05,
      "loss": 0.6524,
      "step": 530000
    },
    {
      "epoch": 8.321821036106751,
      "grad_norm": 3.706115484237671,
      "learning_rate": 4.479886185243328e-05,
      "loss": 0.6172,
      "step": 530100
    },
    {
      "epoch": 8.323390894819466,
      "grad_norm": 3.8985705375671387,
      "learning_rate": 4.479788069073784e-05,
      "loss": 0.6527,
      "step": 530200
    },
    {
      "epoch": 8.324960753532181,
      "grad_norm": 3.8274340629577637,
      "learning_rate": 4.479689952904239e-05,
      "loss": 0.6447,
      "step": 530300
    },
    {
      "epoch": 8.326530612244898,
      "grad_norm": 3.086526870727539,
      "learning_rate": 4.479591836734694e-05,
      "loss": 0.6905,
      "step": 530400
    },
    {
      "epoch": 8.328100470957613,
      "grad_norm": 4.256105422973633,
      "learning_rate": 4.47949372056515e-05,
      "loss": 0.639,
      "step": 530500
    },
    {
      "epoch": 8.32967032967033,
      "grad_norm": 3.171429395675659,
      "learning_rate": 4.479395604395605e-05,
      "loss": 0.6682,
      "step": 530600
    },
    {
      "epoch": 8.331240188383045,
      "grad_norm": 3.96547794342041,
      "learning_rate": 4.47929748822606e-05,
      "loss": 0.6183,
      "step": 530700
    },
    {
      "epoch": 8.332810047095762,
      "grad_norm": 3.4136292934417725,
      "learning_rate": 4.479199372056515e-05,
      "loss": 0.6356,
      "step": 530800
    },
    {
      "epoch": 8.334379905808477,
      "grad_norm": 4.218736171722412,
      "learning_rate": 4.479101255886971e-05,
      "loss": 0.661,
      "step": 530900
    },
    {
      "epoch": 8.335949764521192,
      "grad_norm": 4.600721836090088,
      "learning_rate": 4.479003139717426e-05,
      "loss": 0.6199,
      "step": 531000
    },
    {
      "epoch": 8.33751962323391,
      "grad_norm": 3.9654810428619385,
      "learning_rate": 4.478905023547881e-05,
      "loss": 0.6206,
      "step": 531100
    },
    {
      "epoch": 8.339089481946624,
      "grad_norm": 4.129423141479492,
      "learning_rate": 4.478806907378336e-05,
      "loss": 0.6198,
      "step": 531200
    },
    {
      "epoch": 8.340659340659341,
      "grad_norm": 3.672010898590088,
      "learning_rate": 4.478708791208792e-05,
      "loss": 0.6477,
      "step": 531300
    },
    {
      "epoch": 8.342229199372056,
      "grad_norm": 2.8386452198028564,
      "learning_rate": 4.478610675039246e-05,
      "loss": 0.6294,
      "step": 531400
    },
    {
      "epoch": 8.343799058084773,
      "grad_norm": 3.3084301948547363,
      "learning_rate": 4.478512558869702e-05,
      "loss": 0.6465,
      "step": 531500
    },
    {
      "epoch": 8.345368916797488,
      "grad_norm": 4.068749904632568,
      "learning_rate": 4.478414442700157e-05,
      "loss": 0.6144,
      "step": 531600
    },
    {
      "epoch": 8.346938775510203,
      "grad_norm": 4.46021842956543,
      "learning_rate": 4.478316326530613e-05,
      "loss": 0.6373,
      "step": 531700
    },
    {
      "epoch": 8.34850863422292,
      "grad_norm": 3.317385196685791,
      "learning_rate": 4.478218210361067e-05,
      "loss": 0.622,
      "step": 531800
    },
    {
      "epoch": 8.350078492935635,
      "grad_norm": 4.787199020385742,
      "learning_rate": 4.478120094191523e-05,
      "loss": 0.6634,
      "step": 531900
    },
    {
      "epoch": 8.351648351648352,
      "grad_norm": 3.6209139823913574,
      "learning_rate": 4.478021978021978e-05,
      "loss": 0.6735,
      "step": 532000
    },
    {
      "epoch": 8.353218210361067,
      "grad_norm": 4.523027420043945,
      "learning_rate": 4.477923861852433e-05,
      "loss": 0.6403,
      "step": 532100
    },
    {
      "epoch": 8.354788069073784,
      "grad_norm": 4.443858623504639,
      "learning_rate": 4.4778257456828884e-05,
      "loss": 0.6296,
      "step": 532200
    },
    {
      "epoch": 8.3563579277865,
      "grad_norm": 4.131259441375732,
      "learning_rate": 4.477727629513344e-05,
      "loss": 0.6031,
      "step": 532300
    },
    {
      "epoch": 8.357927786499214,
      "grad_norm": 3.9243509769439697,
      "learning_rate": 4.477629513343799e-05,
      "loss": 0.7066,
      "step": 532400
    },
    {
      "epoch": 8.359497645211931,
      "grad_norm": 3.5550169944763184,
      "learning_rate": 4.4775313971742543e-05,
      "loss": 0.6609,
      "step": 532500
    },
    {
      "epoch": 8.361067503924646,
      "grad_norm": 3.694040060043335,
      "learning_rate": 4.47743328100471e-05,
      "loss": 0.6621,
      "step": 532600
    },
    {
      "epoch": 8.362637362637363,
      "grad_norm": 4.788295269012451,
      "learning_rate": 4.477335164835165e-05,
      "loss": 0.6339,
      "step": 532700
    },
    {
      "epoch": 8.364207221350078,
      "grad_norm": 4.40284538269043,
      "learning_rate": 4.47723704866562e-05,
      "loss": 0.6477,
      "step": 532800
    },
    {
      "epoch": 8.365777080062795,
      "grad_norm": 1.9208120107650757,
      "learning_rate": 4.4771389324960754e-05,
      "loss": 0.6593,
      "step": 532900
    },
    {
      "epoch": 8.36734693877551,
      "grad_norm": 3.9230844974517822,
      "learning_rate": 4.477040816326531e-05,
      "loss": 0.6437,
      "step": 533000
    },
    {
      "epoch": 8.368916797488225,
      "grad_norm": 3.9480631351470947,
      "learning_rate": 4.476942700156986e-05,
      "loss": 0.6783,
      "step": 533100
    },
    {
      "epoch": 8.370486656200942,
      "grad_norm": 4.499701499938965,
      "learning_rate": 4.4768445839874414e-05,
      "loss": 0.6521,
      "step": 533200
    },
    {
      "epoch": 8.372056514913657,
      "grad_norm": 5.121220588684082,
      "learning_rate": 4.4767464678178965e-05,
      "loss": 0.6439,
      "step": 533300
    },
    {
      "epoch": 8.373626373626374,
      "grad_norm": 4.706462383270264,
      "learning_rate": 4.476648351648352e-05,
      "loss": 0.6363,
      "step": 533400
    },
    {
      "epoch": 8.37519623233909,
      "grad_norm": 3.1937026977539062,
      "learning_rate": 4.476550235478807e-05,
      "loss": 0.6489,
      "step": 533500
    },
    {
      "epoch": 8.376766091051806,
      "grad_norm": 4.239788055419922,
      "learning_rate": 4.4764521193092625e-05,
      "loss": 0.6211,
      "step": 533600
    },
    {
      "epoch": 8.378335949764521,
      "grad_norm": 3.5804169178009033,
      "learning_rate": 4.4763540031397176e-05,
      "loss": 0.6485,
      "step": 533700
    },
    {
      "epoch": 8.379905808477236,
      "grad_norm": 4.075547218322754,
      "learning_rate": 4.476255886970173e-05,
      "loss": 0.591,
      "step": 533800
    },
    {
      "epoch": 8.381475667189953,
      "grad_norm": 4.569753170013428,
      "learning_rate": 4.476157770800628e-05,
      "loss": 0.6533,
      "step": 533900
    },
    {
      "epoch": 8.383045525902668,
      "grad_norm": 3.635761022567749,
      "learning_rate": 4.4760596546310835e-05,
      "loss": 0.6562,
      "step": 534000
    },
    {
      "epoch": 8.384615384615385,
      "grad_norm": 3.511521816253662,
      "learning_rate": 4.4759615384615386e-05,
      "loss": 0.647,
      "step": 534100
    },
    {
      "epoch": 8.3861852433281,
      "grad_norm": 4.324837684631348,
      "learning_rate": 4.475863422291994e-05,
      "loss": 0.6512,
      "step": 534200
    },
    {
      "epoch": 8.387755102040817,
      "grad_norm": 3.3037590980529785,
      "learning_rate": 4.475765306122449e-05,
      "loss": 0.6703,
      "step": 534300
    },
    {
      "epoch": 8.389324960753532,
      "grad_norm": 4.238438606262207,
      "learning_rate": 4.4756671899529046e-05,
      "loss": 0.6642,
      "step": 534400
    },
    {
      "epoch": 8.390894819466247,
      "grad_norm": 2.998905897140503,
      "learning_rate": 4.47556907378336e-05,
      "loss": 0.6648,
      "step": 534500
    },
    {
      "epoch": 8.392464678178964,
      "grad_norm": 3.92497181892395,
      "learning_rate": 4.475470957613815e-05,
      "loss": 0.6688,
      "step": 534600
    },
    {
      "epoch": 8.394034536891679,
      "grad_norm": 3.2286624908447266,
      "learning_rate": 4.4753728414442706e-05,
      "loss": 0.6524,
      "step": 534700
    },
    {
      "epoch": 8.395604395604396,
      "grad_norm": 2.42856502532959,
      "learning_rate": 4.475274725274726e-05,
      "loss": 0.696,
      "step": 534800
    },
    {
      "epoch": 8.397174254317111,
      "grad_norm": 3.900069236755371,
      "learning_rate": 4.475176609105181e-05,
      "loss": 0.6351,
      "step": 534900
    },
    {
      "epoch": 8.398744113029828,
      "grad_norm": 4.0697832107543945,
      "learning_rate": 4.475078492935636e-05,
      "loss": 0.6311,
      "step": 535000
    },
    {
      "epoch": 8.400313971742543,
      "grad_norm": 3.9239869117736816,
      "learning_rate": 4.4749803767660916e-05,
      "loss": 0.618,
      "step": 535100
    },
    {
      "epoch": 8.40188383045526,
      "grad_norm": 3.197643280029297,
      "learning_rate": 4.474882260596546e-05,
      "loss": 0.6327,
      "step": 535200
    },
    {
      "epoch": 8.403453689167975,
      "grad_norm": 2.4098007678985596,
      "learning_rate": 4.474784144427002e-05,
      "loss": 0.6179,
      "step": 535300
    },
    {
      "epoch": 8.40502354788069,
      "grad_norm": 4.281312465667725,
      "learning_rate": 4.474686028257457e-05,
      "loss": 0.643,
      "step": 535400
    },
    {
      "epoch": 8.406593406593407,
      "grad_norm": 4.6715850830078125,
      "learning_rate": 4.474587912087913e-05,
      "loss": 0.6114,
      "step": 535500
    },
    {
      "epoch": 8.408163265306122,
      "grad_norm": 4.381338119506836,
      "learning_rate": 4.474489795918367e-05,
      "loss": 0.6258,
      "step": 535600
    },
    {
      "epoch": 8.409733124018839,
      "grad_norm": 2.615248203277588,
      "learning_rate": 4.474391679748823e-05,
      "loss": 0.6274,
      "step": 535700
    },
    {
      "epoch": 8.411302982731554,
      "grad_norm": 4.773179531097412,
      "learning_rate": 4.474293563579278e-05,
      "loss": 0.5864,
      "step": 535800
    },
    {
      "epoch": 8.41287284144427,
      "grad_norm": 3.364373207092285,
      "learning_rate": 4.474195447409733e-05,
      "loss": 0.673,
      "step": 535900
    },
    {
      "epoch": 8.414442700156986,
      "grad_norm": 2.054025650024414,
      "learning_rate": 4.474097331240188e-05,
      "loss": 0.6394,
      "step": 536000
    },
    {
      "epoch": 8.416012558869701,
      "grad_norm": 4.493728160858154,
      "learning_rate": 4.473999215070644e-05,
      "loss": 0.6604,
      "step": 536100
    },
    {
      "epoch": 8.417582417582418,
      "grad_norm": 3.3388826847076416,
      "learning_rate": 4.473901098901099e-05,
      "loss": 0.6491,
      "step": 536200
    },
    {
      "epoch": 8.419152276295133,
      "grad_norm": 3.4761271476745605,
      "learning_rate": 4.473802982731554e-05,
      "loss": 0.6388,
      "step": 536300
    },
    {
      "epoch": 8.42072213500785,
      "grad_norm": 4.8200201988220215,
      "learning_rate": 4.473704866562009e-05,
      "loss": 0.6521,
      "step": 536400
    },
    {
      "epoch": 8.422291993720565,
      "grad_norm": 4.308272361755371,
      "learning_rate": 4.473606750392465e-05,
      "loss": 0.6073,
      "step": 536500
    },
    {
      "epoch": 8.423861852433282,
      "grad_norm": 3.3884429931640625,
      "learning_rate": 4.47350863422292e-05,
      "loss": 0.6573,
      "step": 536600
    },
    {
      "epoch": 8.425431711145997,
      "grad_norm": 3.3282687664031982,
      "learning_rate": 4.473410518053375e-05,
      "loss": 0.6376,
      "step": 536700
    },
    {
      "epoch": 8.427001569858712,
      "grad_norm": 4.502938747406006,
      "learning_rate": 4.473312401883831e-05,
      "loss": 0.6473,
      "step": 536800
    },
    {
      "epoch": 8.428571428571429,
      "grad_norm": 4.364627361297607,
      "learning_rate": 4.473214285714286e-05,
      "loss": 0.6863,
      "step": 536900
    },
    {
      "epoch": 8.430141287284144,
      "grad_norm": 3.2116124629974365,
      "learning_rate": 4.473116169544741e-05,
      "loss": 0.6474,
      "step": 537000
    },
    {
      "epoch": 8.43171114599686,
      "grad_norm": 4.251326560974121,
      "learning_rate": 4.473018053375196e-05,
      "loss": 0.6657,
      "step": 537100
    },
    {
      "epoch": 8.433281004709576,
      "grad_norm": 4.091655731201172,
      "learning_rate": 4.472919937205652e-05,
      "loss": 0.628,
      "step": 537200
    },
    {
      "epoch": 8.434850863422293,
      "grad_norm": 1.8797426223754883,
      "learning_rate": 4.4728218210361065e-05,
      "loss": 0.6715,
      "step": 537300
    },
    {
      "epoch": 8.436420722135008,
      "grad_norm": 4.474593639373779,
      "learning_rate": 4.472723704866562e-05,
      "loss": 0.6292,
      "step": 537400
    },
    {
      "epoch": 8.437990580847723,
      "grad_norm": 5.1310930252075195,
      "learning_rate": 4.4726255886970174e-05,
      "loss": 0.6368,
      "step": 537500
    },
    {
      "epoch": 8.43956043956044,
      "grad_norm": 3.2669804096221924,
      "learning_rate": 4.472527472527473e-05,
      "loss": 0.6494,
      "step": 537600
    },
    {
      "epoch": 8.441130298273155,
      "grad_norm": 2.459599733352661,
      "learning_rate": 4.4724293563579276e-05,
      "loss": 0.6326,
      "step": 537700
    },
    {
      "epoch": 8.442700156985872,
      "grad_norm": 3.1536152362823486,
      "learning_rate": 4.4723312401883834e-05,
      "loss": 0.6507,
      "step": 537800
    },
    {
      "epoch": 8.444270015698587,
      "grad_norm": 3.021850109100342,
      "learning_rate": 4.4722331240188385e-05,
      "loss": 0.6678,
      "step": 537900
    },
    {
      "epoch": 8.445839874411304,
      "grad_norm": 3.9110283851623535,
      "learning_rate": 4.4721350078492936e-05,
      "loss": 0.6289,
      "step": 538000
    },
    {
      "epoch": 8.447409733124019,
      "grad_norm": 3.8693807125091553,
      "learning_rate": 4.4720368916797487e-05,
      "loss": 0.6806,
      "step": 538100
    },
    {
      "epoch": 8.448979591836734,
      "grad_norm": 3.0994656085968018,
      "learning_rate": 4.4719387755102044e-05,
      "loss": 0.6805,
      "step": 538200
    },
    {
      "epoch": 8.45054945054945,
      "grad_norm": 4.698944091796875,
      "learning_rate": 4.4718406593406595e-05,
      "loss": 0.6773,
      "step": 538300
    },
    {
      "epoch": 8.452119309262166,
      "grad_norm": 3.257948637008667,
      "learning_rate": 4.4717425431711146e-05,
      "loss": 0.6397,
      "step": 538400
    },
    {
      "epoch": 8.453689167974883,
      "grad_norm": 3.6990182399749756,
      "learning_rate": 4.47164442700157e-05,
      "loss": 0.6044,
      "step": 538500
    },
    {
      "epoch": 8.455259026687598,
      "grad_norm": 4.0761308670043945,
      "learning_rate": 4.4715463108320255e-05,
      "loss": 0.646,
      "step": 538600
    },
    {
      "epoch": 8.456828885400315,
      "grad_norm": 4.753509998321533,
      "learning_rate": 4.4714481946624806e-05,
      "loss": 0.6711,
      "step": 538700
    },
    {
      "epoch": 8.45839874411303,
      "grad_norm": 4.808549880981445,
      "learning_rate": 4.471350078492936e-05,
      "loss": 0.6508,
      "step": 538800
    },
    {
      "epoch": 8.459968602825747,
      "grad_norm": 3.9855878353118896,
      "learning_rate": 4.4712519623233915e-05,
      "loss": 0.6269,
      "step": 538900
    },
    {
      "epoch": 8.461538461538462,
      "grad_norm": 4.150574207305908,
      "learning_rate": 4.4711538461538466e-05,
      "loss": 0.669,
      "step": 539000
    },
    {
      "epoch": 8.463108320251177,
      "grad_norm": 3.8996689319610596,
      "learning_rate": 4.471055729984302e-05,
      "loss": 0.6357,
      "step": 539100
    },
    {
      "epoch": 8.464678178963894,
      "grad_norm": 3.845860242843628,
      "learning_rate": 4.470957613814757e-05,
      "loss": 0.6536,
      "step": 539200
    },
    {
      "epoch": 8.466248037676609,
      "grad_norm": 3.9259092807769775,
      "learning_rate": 4.4708594976452125e-05,
      "loss": 0.617,
      "step": 539300
    },
    {
      "epoch": 8.467817896389326,
      "grad_norm": 3.3004541397094727,
      "learning_rate": 4.470761381475667e-05,
      "loss": 0.6458,
      "step": 539400
    },
    {
      "epoch": 8.46938775510204,
      "grad_norm": 3.684607982635498,
      "learning_rate": 4.470663265306123e-05,
      "loss": 0.6424,
      "step": 539500
    },
    {
      "epoch": 8.470957613814758,
      "grad_norm": 3.714205503463745,
      "learning_rate": 4.470565149136578e-05,
      "loss": 0.6271,
      "step": 539600
    },
    {
      "epoch": 8.472527472527473,
      "grad_norm": 4.501523017883301,
      "learning_rate": 4.4704670329670336e-05,
      "loss": 0.6366,
      "step": 539700
    },
    {
      "epoch": 8.474097331240188,
      "grad_norm": 3.8780148029327393,
      "learning_rate": 4.470368916797488e-05,
      "loss": 0.6818,
      "step": 539800
    },
    {
      "epoch": 8.475667189952905,
      "grad_norm": 4.051945686340332,
      "learning_rate": 4.470270800627944e-05,
      "loss": 0.6687,
      "step": 539900
    },
    {
      "epoch": 8.47723704866562,
      "grad_norm": 4.537442207336426,
      "learning_rate": 4.470172684458399e-05,
      "loss": 0.6999,
      "step": 540000
    },
    {
      "epoch": 8.478806907378337,
      "grad_norm": 3.556654691696167,
      "learning_rate": 4.470074568288854e-05,
      "loss": 0.6187,
      "step": 540100
    },
    {
      "epoch": 8.480376766091052,
      "grad_norm": 5.403510570526123,
      "learning_rate": 4.469976452119309e-05,
      "loss": 0.68,
      "step": 540200
    },
    {
      "epoch": 8.481946624803768,
      "grad_norm": 4.717094421386719,
      "learning_rate": 4.469878335949765e-05,
      "loss": 0.6557,
      "step": 540300
    },
    {
      "epoch": 8.483516483516484,
      "grad_norm": 2.827012777328491,
      "learning_rate": 4.46978021978022e-05,
      "loss": 0.6396,
      "step": 540400
    },
    {
      "epoch": 8.485086342229199,
      "grad_norm": 4.428867340087891,
      "learning_rate": 4.469682103610675e-05,
      "loss": 0.6512,
      "step": 540500
    },
    {
      "epoch": 8.486656200941916,
      "grad_norm": 4.376957416534424,
      "learning_rate": 4.46958398744113e-05,
      "loss": 0.629,
      "step": 540600
    },
    {
      "epoch": 8.48822605965463,
      "grad_norm": 3.5685157775878906,
      "learning_rate": 4.469485871271586e-05,
      "loss": 0.6572,
      "step": 540700
    },
    {
      "epoch": 8.489795918367347,
      "grad_norm": 2.933317184448242,
      "learning_rate": 4.469387755102041e-05,
      "loss": 0.6313,
      "step": 540800
    },
    {
      "epoch": 8.491365777080063,
      "grad_norm": 3.8234193325042725,
      "learning_rate": 4.469289638932496e-05,
      "loss": 0.6305,
      "step": 540900
    },
    {
      "epoch": 8.49293563579278,
      "grad_norm": 3.352407693862915,
      "learning_rate": 4.469191522762952e-05,
      "loss": 0.6276,
      "step": 541000
    },
    {
      "epoch": 8.494505494505495,
      "grad_norm": 3.60353946685791,
      "learning_rate": 4.469093406593407e-05,
      "loss": 0.6596,
      "step": 541100
    },
    {
      "epoch": 8.49607535321821,
      "grad_norm": 3.3088064193725586,
      "learning_rate": 4.468995290423862e-05,
      "loss": 0.6504,
      "step": 541200
    },
    {
      "epoch": 8.497645211930926,
      "grad_norm": 3.5116209983825684,
      "learning_rate": 4.468897174254317e-05,
      "loss": 0.6179,
      "step": 541300
    },
    {
      "epoch": 8.499215070643642,
      "grad_norm": 4.320158004760742,
      "learning_rate": 4.468799058084773e-05,
      "loss": 0.6436,
      "step": 541400
    },
    {
      "epoch": 8.500784929356358,
      "grad_norm": 4.435278415679932,
      "learning_rate": 4.4687009419152274e-05,
      "loss": 0.6417,
      "step": 541500
    },
    {
      "epoch": 8.502354788069074,
      "grad_norm": 4.016184329986572,
      "learning_rate": 4.468602825745683e-05,
      "loss": 0.6148,
      "step": 541600
    },
    {
      "epoch": 8.50392464678179,
      "grad_norm": 5.264167308807373,
      "learning_rate": 4.468504709576138e-05,
      "loss": 0.6551,
      "step": 541700
    },
    {
      "epoch": 8.505494505494505,
      "grad_norm": 5.040525436401367,
      "learning_rate": 4.468406593406594e-05,
      "loss": 0.6235,
      "step": 541800
    },
    {
      "epoch": 8.50706436420722,
      "grad_norm": 3.767245292663574,
      "learning_rate": 4.4683084772370485e-05,
      "loss": 0.6404,
      "step": 541900
    },
    {
      "epoch": 8.508634222919937,
      "grad_norm": 3.855206251144409,
      "learning_rate": 4.468210361067504e-05,
      "loss": 0.665,
      "step": 542000
    },
    {
      "epoch": 8.510204081632653,
      "grad_norm": 3.9395158290863037,
      "learning_rate": 4.4681122448979594e-05,
      "loss": 0.663,
      "step": 542100
    },
    {
      "epoch": 8.51177394034537,
      "grad_norm": 3.565904378890991,
      "learning_rate": 4.4680141287284145e-05,
      "loss": 0.6236,
      "step": 542200
    },
    {
      "epoch": 8.513343799058084,
      "grad_norm": 3.820206880569458,
      "learning_rate": 4.4679160125588696e-05,
      "loss": 0.621,
      "step": 542300
    },
    {
      "epoch": 8.514913657770801,
      "grad_norm": 4.733306884765625,
      "learning_rate": 4.467817896389325e-05,
      "loss": 0.6176,
      "step": 542400
    },
    {
      "epoch": 8.516483516483516,
      "grad_norm": 4.109920024871826,
      "learning_rate": 4.4677197802197804e-05,
      "loss": 0.6083,
      "step": 542500
    },
    {
      "epoch": 8.518053375196232,
      "grad_norm": 2.7255892753601074,
      "learning_rate": 4.4676216640502355e-05,
      "loss": 0.6584,
      "step": 542600
    },
    {
      "epoch": 8.519623233908948,
      "grad_norm": 3.4321935176849365,
      "learning_rate": 4.4675235478806906e-05,
      "loss": 0.6466,
      "step": 542700
    },
    {
      "epoch": 8.521193092621663,
      "grad_norm": 5.287840366363525,
      "learning_rate": 4.4674254317111464e-05,
      "loss": 0.6062,
      "step": 542800
    },
    {
      "epoch": 8.52276295133438,
      "grad_norm": 4.623509883880615,
      "learning_rate": 4.4673273155416015e-05,
      "loss": 0.6338,
      "step": 542900
    },
    {
      "epoch": 8.524332810047095,
      "grad_norm": 3.1565206050872803,
      "learning_rate": 4.4672291993720566e-05,
      "loss": 0.6912,
      "step": 543000
    },
    {
      "epoch": 8.525902668759812,
      "grad_norm": 4.5904059410095215,
      "learning_rate": 4.4671310832025124e-05,
      "loss": 0.6821,
      "step": 543100
    },
    {
      "epoch": 8.527472527472527,
      "grad_norm": 3.780866861343384,
      "learning_rate": 4.4670329670329675e-05,
      "loss": 0.6542,
      "step": 543200
    },
    {
      "epoch": 8.529042386185242,
      "grad_norm": 4.102720260620117,
      "learning_rate": 4.4669348508634226e-05,
      "loss": 0.6136,
      "step": 543300
    },
    {
      "epoch": 8.53061224489796,
      "grad_norm": 4.199356555938721,
      "learning_rate": 4.466836734693878e-05,
      "loss": 0.6298,
      "step": 543400
    },
    {
      "epoch": 8.532182103610674,
      "grad_norm": 4.8231916427612305,
      "learning_rate": 4.4667386185243334e-05,
      "loss": 0.6505,
      "step": 543500
    },
    {
      "epoch": 8.533751962323391,
      "grad_norm": 3.7969090938568115,
      "learning_rate": 4.466640502354788e-05,
      "loss": 0.667,
      "step": 543600
    },
    {
      "epoch": 8.535321821036106,
      "grad_norm": 3.1607959270477295,
      "learning_rate": 4.4665423861852436e-05,
      "loss": 0.6338,
      "step": 543700
    },
    {
      "epoch": 8.536891679748823,
      "grad_norm": 3.8355040550231934,
      "learning_rate": 4.466444270015699e-05,
      "loss": 0.6286,
      "step": 543800
    },
    {
      "epoch": 8.538461538461538,
      "grad_norm": 5.527542591094971,
      "learning_rate": 4.4663461538461545e-05,
      "loss": 0.626,
      "step": 543900
    },
    {
      "epoch": 8.540031397174253,
      "grad_norm": 4.0768656730651855,
      "learning_rate": 4.466248037676609e-05,
      "loss": 0.6476,
      "step": 544000
    },
    {
      "epoch": 8.54160125588697,
      "grad_norm": 3.4996891021728516,
      "learning_rate": 4.466149921507065e-05,
      "loss": 0.6401,
      "step": 544100
    },
    {
      "epoch": 8.543171114599685,
      "grad_norm": 3.888002634048462,
      "learning_rate": 4.46605180533752e-05,
      "loss": 0.615,
      "step": 544200
    },
    {
      "epoch": 8.544740973312402,
      "grad_norm": 4.375980854034424,
      "learning_rate": 4.465953689167975e-05,
      "loss": 0.6411,
      "step": 544300
    },
    {
      "epoch": 8.546310832025117,
      "grad_norm": 4.511134624481201,
      "learning_rate": 4.46585557299843e-05,
      "loss": 0.6366,
      "step": 544400
    },
    {
      "epoch": 8.547880690737834,
      "grad_norm": 4.250370979309082,
      "learning_rate": 4.465757456828886e-05,
      "loss": 0.6499,
      "step": 544500
    },
    {
      "epoch": 8.54945054945055,
      "grad_norm": 2.863492727279663,
      "learning_rate": 4.465659340659341e-05,
      "loss": 0.6363,
      "step": 544600
    },
    {
      "epoch": 8.551020408163264,
      "grad_norm": 3.6356201171875,
      "learning_rate": 4.465561224489796e-05,
      "loss": 0.6376,
      "step": 544700
    },
    {
      "epoch": 8.552590266875981,
      "grad_norm": 4.0312819480896,
      "learning_rate": 4.465463108320251e-05,
      "loss": 0.6511,
      "step": 544800
    },
    {
      "epoch": 8.554160125588696,
      "grad_norm": 4.433473110198975,
      "learning_rate": 4.465364992150707e-05,
      "loss": 0.6488,
      "step": 544900
    },
    {
      "epoch": 8.555729984301413,
      "grad_norm": 3.255591869354248,
      "learning_rate": 4.465266875981162e-05,
      "loss": 0.6663,
      "step": 545000
    },
    {
      "epoch": 8.557299843014128,
      "grad_norm": 3.527977466583252,
      "learning_rate": 4.465168759811617e-05,
      "loss": 0.6286,
      "step": 545100
    },
    {
      "epoch": 8.558869701726845,
      "grad_norm": 2.8465285301208496,
      "learning_rate": 4.465070643642073e-05,
      "loss": 0.6153,
      "step": 545200
    },
    {
      "epoch": 8.56043956043956,
      "grad_norm": 4.005417823791504,
      "learning_rate": 4.464972527472528e-05,
      "loss": 0.6362,
      "step": 545300
    },
    {
      "epoch": 8.562009419152277,
      "grad_norm": 4.769573211669922,
      "learning_rate": 4.464874411302983e-05,
      "loss": 0.6525,
      "step": 545400
    },
    {
      "epoch": 8.563579277864992,
      "grad_norm": 3.8819329738616943,
      "learning_rate": 4.464776295133438e-05,
      "loss": 0.6343,
      "step": 545500
    },
    {
      "epoch": 8.565149136577707,
      "grad_norm": 4.2512664794921875,
      "learning_rate": 4.464678178963894e-05,
      "loss": 0.6794,
      "step": 545600
    },
    {
      "epoch": 8.566718995290424,
      "grad_norm": 5.388014316558838,
      "learning_rate": 4.464580062794348e-05,
      "loss": 0.6285,
      "step": 545700
    },
    {
      "epoch": 8.56828885400314,
      "grad_norm": 4.182073593139648,
      "learning_rate": 4.464481946624804e-05,
      "loss": 0.6435,
      "step": 545800
    },
    {
      "epoch": 8.569858712715856,
      "grad_norm": 3.7800118923187256,
      "learning_rate": 4.464383830455259e-05,
      "loss": 0.6246,
      "step": 545900
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 2.766309976577759,
      "learning_rate": 4.464285714285715e-05,
      "loss": 0.6809,
      "step": 546000
    },
    {
      "epoch": 8.572998430141288,
      "grad_norm": 4.774652004241943,
      "learning_rate": 4.4641875981161694e-05,
      "loss": 0.6506,
      "step": 546100
    },
    {
      "epoch": 8.574568288854003,
      "grad_norm": 4.470527648925781,
      "learning_rate": 4.464089481946625e-05,
      "loss": 0.6086,
      "step": 546200
    },
    {
      "epoch": 8.576138147566718,
      "grad_norm": 4.234553337097168,
      "learning_rate": 4.46399136577708e-05,
      "loss": 0.632,
      "step": 546300
    },
    {
      "epoch": 8.577708006279435,
      "grad_norm": 4.432703018188477,
      "learning_rate": 4.4638932496075354e-05,
      "loss": 0.5944,
      "step": 546400
    },
    {
      "epoch": 8.57927786499215,
      "grad_norm": 5.060828685760498,
      "learning_rate": 4.4637951334379905e-05,
      "loss": 0.6736,
      "step": 546500
    },
    {
      "epoch": 8.580847723704867,
      "grad_norm": 4.63459587097168,
      "learning_rate": 4.463697017268446e-05,
      "loss": 0.6466,
      "step": 546600
    },
    {
      "epoch": 8.582417582417582,
      "grad_norm": 3.2202625274658203,
      "learning_rate": 4.463598901098901e-05,
      "loss": 0.6217,
      "step": 546700
    },
    {
      "epoch": 8.583987441130299,
      "grad_norm": 4.257347583770752,
      "learning_rate": 4.4635007849293564e-05,
      "loss": 0.6592,
      "step": 546800
    },
    {
      "epoch": 8.585557299843014,
      "grad_norm": 3.724857807159424,
      "learning_rate": 4.4634026687598115e-05,
      "loss": 0.673,
      "step": 546900
    },
    {
      "epoch": 8.58712715855573,
      "grad_norm": 3.8960766792297363,
      "learning_rate": 4.463304552590267e-05,
      "loss": 0.6081,
      "step": 547000
    },
    {
      "epoch": 8.588697017268446,
      "grad_norm": 3.3294715881347656,
      "learning_rate": 4.4632064364207224e-05,
      "loss": 0.6466,
      "step": 547100
    },
    {
      "epoch": 8.590266875981161,
      "grad_norm": 4.33992338180542,
      "learning_rate": 4.4631083202511775e-05,
      "loss": 0.6712,
      "step": 547200
    },
    {
      "epoch": 8.591836734693878,
      "grad_norm": 4.404007434844971,
      "learning_rate": 4.463010204081633e-05,
      "loss": 0.6674,
      "step": 547300
    },
    {
      "epoch": 8.593406593406593,
      "grad_norm": 3.808558464050293,
      "learning_rate": 4.4629120879120884e-05,
      "loss": 0.6239,
      "step": 547400
    },
    {
      "epoch": 8.59497645211931,
      "grad_norm": 4.4272050857543945,
      "learning_rate": 4.4628139717425435e-05,
      "loss": 0.6468,
      "step": 547500
    },
    {
      "epoch": 8.596546310832025,
      "grad_norm": 3.196402072906494,
      "learning_rate": 4.4627158555729986e-05,
      "loss": 0.6316,
      "step": 547600
    },
    {
      "epoch": 8.598116169544742,
      "grad_norm": 4.481168746948242,
      "learning_rate": 4.462617739403454e-05,
      "loss": 0.6643,
      "step": 547700
    },
    {
      "epoch": 8.599686028257457,
      "grad_norm": 3.8344361782073975,
      "learning_rate": 4.462519623233909e-05,
      "loss": 0.612,
      "step": 547800
    },
    {
      "epoch": 8.601255886970172,
      "grad_norm": 3.7747762203216553,
      "learning_rate": 4.4624215070643645e-05,
      "loss": 0.6529,
      "step": 547900
    },
    {
      "epoch": 8.602825745682889,
      "grad_norm": 4.120547294616699,
      "learning_rate": 4.4623233908948196e-05,
      "loss": 0.64,
      "step": 548000
    },
    {
      "epoch": 8.604395604395604,
      "grad_norm": 3.2243587970733643,
      "learning_rate": 4.4622252747252754e-05,
      "loss": 0.6568,
      "step": 548100
    },
    {
      "epoch": 8.605965463108321,
      "grad_norm": 3.296102523803711,
      "learning_rate": 4.46212715855573e-05,
      "loss": 0.6687,
      "step": 548200
    },
    {
      "epoch": 8.607535321821036,
      "grad_norm": 3.4311892986297607,
      "learning_rate": 4.4620290423861856e-05,
      "loss": 0.6688,
      "step": 548300
    },
    {
      "epoch": 8.609105180533753,
      "grad_norm": 4.064205169677734,
      "learning_rate": 4.461930926216641e-05,
      "loss": 0.6488,
      "step": 548400
    },
    {
      "epoch": 8.610675039246468,
      "grad_norm": 3.5175349712371826,
      "learning_rate": 4.461832810047096e-05,
      "loss": 0.6287,
      "step": 548500
    },
    {
      "epoch": 8.612244897959183,
      "grad_norm": 4.864899635314941,
      "learning_rate": 4.461734693877551e-05,
      "loss": 0.6225,
      "step": 548600
    },
    {
      "epoch": 8.6138147566719,
      "grad_norm": 3.0411102771759033,
      "learning_rate": 4.461636577708007e-05,
      "loss": 0.612,
      "step": 548700
    },
    {
      "epoch": 8.615384615384615,
      "grad_norm": 5.207744598388672,
      "learning_rate": 4.461538461538462e-05,
      "loss": 0.635,
      "step": 548800
    },
    {
      "epoch": 8.616954474097332,
      "grad_norm": 4.767632961273193,
      "learning_rate": 4.461440345368917e-05,
      "loss": 0.6672,
      "step": 548900
    },
    {
      "epoch": 8.618524332810047,
      "grad_norm": 4.01978874206543,
      "learning_rate": 4.461342229199372e-05,
      "loss": 0.6217,
      "step": 549000
    },
    {
      "epoch": 8.620094191522764,
      "grad_norm": 3.93436861038208,
      "learning_rate": 4.461244113029828e-05,
      "loss": 0.6599,
      "step": 549100
    },
    {
      "epoch": 8.621664050235479,
      "grad_norm": 4.121201515197754,
      "learning_rate": 4.461145996860283e-05,
      "loss": 0.586,
      "step": 549200
    },
    {
      "epoch": 8.623233908948194,
      "grad_norm": 4.5815887451171875,
      "learning_rate": 4.461047880690738e-05,
      "loss": 0.6079,
      "step": 549300
    },
    {
      "epoch": 8.62480376766091,
      "grad_norm": 4.470512390136719,
      "learning_rate": 4.460949764521194e-05,
      "loss": 0.6784,
      "step": 549400
    },
    {
      "epoch": 8.626373626373626,
      "grad_norm": 2.1192734241485596,
      "learning_rate": 4.460851648351649e-05,
      "loss": 0.6376,
      "step": 549500
    },
    {
      "epoch": 8.627943485086343,
      "grad_norm": 3.729621410369873,
      "learning_rate": 4.460753532182104e-05,
      "loss": 0.6352,
      "step": 549600
    },
    {
      "epoch": 8.629513343799058,
      "grad_norm": 3.537909984588623,
      "learning_rate": 4.460655416012559e-05,
      "loss": 0.6375,
      "step": 549700
    },
    {
      "epoch": 8.631083202511775,
      "grad_norm": 4.16455078125,
      "learning_rate": 4.460557299843015e-05,
      "loss": 0.6762,
      "step": 549800
    },
    {
      "epoch": 8.63265306122449,
      "grad_norm": 3.821638584136963,
      "learning_rate": 4.460459183673469e-05,
      "loss": 0.6576,
      "step": 549900
    },
    {
      "epoch": 8.634222919937205,
      "grad_norm": 3.846587657928467,
      "learning_rate": 4.460361067503925e-05,
      "loss": 0.6117,
      "step": 550000
    },
    {
      "epoch": 8.635792778649922,
      "grad_norm": 3.9354944229125977,
      "learning_rate": 4.46026295133438e-05,
      "loss": 0.6539,
      "step": 550100
    },
    {
      "epoch": 8.637362637362637,
      "grad_norm": 4.443063259124756,
      "learning_rate": 4.460164835164836e-05,
      "loss": 0.6521,
      "step": 550200
    },
    {
      "epoch": 8.638932496075354,
      "grad_norm": 3.674755573272705,
      "learning_rate": 4.46006671899529e-05,
      "loss": 0.666,
      "step": 550300
    },
    {
      "epoch": 8.640502354788069,
      "grad_norm": 4.1220245361328125,
      "learning_rate": 4.459968602825746e-05,
      "loss": 0.626,
      "step": 550400
    },
    {
      "epoch": 8.642072213500786,
      "grad_norm": 4.283100128173828,
      "learning_rate": 4.459870486656201e-05,
      "loss": 0.6237,
      "step": 550500
    },
    {
      "epoch": 8.6436420722135,
      "grad_norm": 3.4150009155273438,
      "learning_rate": 4.459772370486656e-05,
      "loss": 0.6511,
      "step": 550600
    },
    {
      "epoch": 8.645211930926216,
      "grad_norm": 4.973848342895508,
      "learning_rate": 4.4596742543171113e-05,
      "loss": 0.6746,
      "step": 550700
    },
    {
      "epoch": 8.646781789638933,
      "grad_norm": 4.012418270111084,
      "learning_rate": 4.459576138147567e-05,
      "loss": 0.6428,
      "step": 550800
    },
    {
      "epoch": 8.648351648351648,
      "grad_norm": 3.8936188220977783,
      "learning_rate": 4.459478021978022e-05,
      "loss": 0.653,
      "step": 550900
    },
    {
      "epoch": 8.649921507064365,
      "grad_norm": 3.6226956844329834,
      "learning_rate": 4.459379905808477e-05,
      "loss": 0.6567,
      "step": 551000
    },
    {
      "epoch": 8.65149136577708,
      "grad_norm": 4.279961585998535,
      "learning_rate": 4.4592817896389324e-05,
      "loss": 0.7069,
      "step": 551100
    },
    {
      "epoch": 8.653061224489797,
      "grad_norm": 3.375014305114746,
      "learning_rate": 4.459183673469388e-05,
      "loss": 0.648,
      "step": 551200
    },
    {
      "epoch": 8.654631083202512,
      "grad_norm": 4.100111484527588,
      "learning_rate": 4.459085557299843e-05,
      "loss": 0.6527,
      "step": 551300
    },
    {
      "epoch": 8.656200941915227,
      "grad_norm": 3.1111583709716797,
      "learning_rate": 4.4589874411302984e-05,
      "loss": 0.6222,
      "step": 551400
    },
    {
      "epoch": 8.657770800627944,
      "grad_norm": 3.7138450145721436,
      "learning_rate": 4.458889324960754e-05,
      "loss": 0.5892,
      "step": 551500
    },
    {
      "epoch": 8.659340659340659,
      "grad_norm": 3.771125078201294,
      "learning_rate": 4.458791208791209e-05,
      "loss": 0.6605,
      "step": 551600
    },
    {
      "epoch": 8.660910518053376,
      "grad_norm": 3.944216728210449,
      "learning_rate": 4.4586930926216644e-05,
      "loss": 0.6147,
      "step": 551700
    },
    {
      "epoch": 8.66248037676609,
      "grad_norm": 3.929875135421753,
      "learning_rate": 4.4585949764521195e-05,
      "loss": 0.6779,
      "step": 551800
    },
    {
      "epoch": 8.664050235478808,
      "grad_norm": 3.8279013633728027,
      "learning_rate": 4.458496860282575e-05,
      "loss": 0.6668,
      "step": 551900
    },
    {
      "epoch": 8.665620094191523,
      "grad_norm": 4.511736869812012,
      "learning_rate": 4.4583987441130297e-05,
      "loss": 0.6744,
      "step": 552000
    },
    {
      "epoch": 8.667189952904238,
      "grad_norm": 2.843446731567383,
      "learning_rate": 4.4583006279434854e-05,
      "loss": 0.6352,
      "step": 552100
    },
    {
      "epoch": 8.668759811616955,
      "grad_norm": 4.571122646331787,
      "learning_rate": 4.4582025117739405e-05,
      "loss": 0.6647,
      "step": 552200
    },
    {
      "epoch": 8.67032967032967,
      "grad_norm": 5.201892375946045,
      "learning_rate": 4.458104395604396e-05,
      "loss": 0.6205,
      "step": 552300
    },
    {
      "epoch": 8.671899529042387,
      "grad_norm": 4.30837345123291,
      "learning_rate": 4.458006279434851e-05,
      "loss": 0.6388,
      "step": 552400
    },
    {
      "epoch": 8.673469387755102,
      "grad_norm": 3.4544293880462646,
      "learning_rate": 4.4579081632653065e-05,
      "loss": 0.5984,
      "step": 552500
    },
    {
      "epoch": 8.675039246467819,
      "grad_norm": 3.2559590339660645,
      "learning_rate": 4.4578100470957616e-05,
      "loss": 0.6331,
      "step": 552600
    },
    {
      "epoch": 8.676609105180534,
      "grad_norm": 3.398355484008789,
      "learning_rate": 4.457711930926217e-05,
      "loss": 0.6127,
      "step": 552700
    },
    {
      "epoch": 8.678178963893249,
      "grad_norm": 2.8165194988250732,
      "learning_rate": 4.457613814756672e-05,
      "loss": 0.623,
      "step": 552800
    },
    {
      "epoch": 8.679748822605966,
      "grad_norm": 3.741530418395996,
      "learning_rate": 4.4575156985871276e-05,
      "loss": 0.6544,
      "step": 552900
    },
    {
      "epoch": 8.68131868131868,
      "grad_norm": 4.590303421020508,
      "learning_rate": 4.457417582417583e-05,
      "loss": 0.693,
      "step": 553000
    },
    {
      "epoch": 8.682888540031398,
      "grad_norm": 4.471126079559326,
      "learning_rate": 4.457319466248038e-05,
      "loss": 0.6808,
      "step": 553100
    },
    {
      "epoch": 8.684458398744113,
      "grad_norm": 4.419036865234375,
      "learning_rate": 4.457221350078493e-05,
      "loss": 0.6218,
      "step": 553200
    },
    {
      "epoch": 8.68602825745683,
      "grad_norm": 3.5098297595977783,
      "learning_rate": 4.4571232339089486e-05,
      "loss": 0.6396,
      "step": 553300
    },
    {
      "epoch": 8.687598116169545,
      "grad_norm": 2.869407892227173,
      "learning_rate": 4.457025117739404e-05,
      "loss": 0.6246,
      "step": 553400
    },
    {
      "epoch": 8.68916797488226,
      "grad_norm": 3.36124849319458,
      "learning_rate": 4.456927001569859e-05,
      "loss": 0.6362,
      "step": 553500
    },
    {
      "epoch": 8.690737833594977,
      "grad_norm": 4.180881977081299,
      "learning_rate": 4.4568288854003146e-05,
      "loss": 0.6109,
      "step": 553600
    },
    {
      "epoch": 8.692307692307692,
      "grad_norm": 4.307601451873779,
      "learning_rate": 4.45673076923077e-05,
      "loss": 0.7003,
      "step": 553700
    },
    {
      "epoch": 8.693877551020408,
      "grad_norm": 3.42596697807312,
      "learning_rate": 4.456632653061225e-05,
      "loss": 0.6249,
      "step": 553800
    },
    {
      "epoch": 8.695447409733124,
      "grad_norm": 3.7110908031463623,
      "learning_rate": 4.45653453689168e-05,
      "loss": 0.6544,
      "step": 553900
    },
    {
      "epoch": 8.69701726844584,
      "grad_norm": 4.04210090637207,
      "learning_rate": 4.456436420722136e-05,
      "loss": 0.6268,
      "step": 554000
    },
    {
      "epoch": 8.698587127158556,
      "grad_norm": 4.530005931854248,
      "learning_rate": 4.45633830455259e-05,
      "loss": 0.6522,
      "step": 554100
    },
    {
      "epoch": 8.700156985871272,
      "grad_norm": 4.031763553619385,
      "learning_rate": 4.456240188383046e-05,
      "loss": 0.6565,
      "step": 554200
    },
    {
      "epoch": 8.701726844583987,
      "grad_norm": 4.1988139152526855,
      "learning_rate": 4.456142072213501e-05,
      "loss": 0.6268,
      "step": 554300
    },
    {
      "epoch": 8.703296703296703,
      "grad_norm": 3.078183889389038,
      "learning_rate": 4.456043956043957e-05,
      "loss": 0.6123,
      "step": 554400
    },
    {
      "epoch": 8.70486656200942,
      "grad_norm": 3.2686686515808105,
      "learning_rate": 4.455945839874411e-05,
      "loss": 0.6518,
      "step": 554500
    },
    {
      "epoch": 8.706436420722135,
      "grad_norm": 4.795871257781982,
      "learning_rate": 4.455847723704867e-05,
      "loss": 0.6399,
      "step": 554600
    },
    {
      "epoch": 8.708006279434851,
      "grad_norm": 4.001168251037598,
      "learning_rate": 4.455749607535322e-05,
      "loss": 0.6934,
      "step": 554700
    },
    {
      "epoch": 8.709576138147566,
      "grad_norm": 3.9208474159240723,
      "learning_rate": 4.455651491365777e-05,
      "loss": 0.6395,
      "step": 554800
    },
    {
      "epoch": 8.711145996860283,
      "grad_norm": 3.9280145168304443,
      "learning_rate": 4.455553375196232e-05,
      "loss": 0.6488,
      "step": 554900
    },
    {
      "epoch": 8.712715855572998,
      "grad_norm": 4.051980495452881,
      "learning_rate": 4.455455259026688e-05,
      "loss": 0.6648,
      "step": 555000
    },
    {
      "epoch": 8.714285714285714,
      "grad_norm": 3.843500852584839,
      "learning_rate": 4.455357142857143e-05,
      "loss": 0.6609,
      "step": 555100
    },
    {
      "epoch": 8.71585557299843,
      "grad_norm": 3.957681894302368,
      "learning_rate": 4.455259026687598e-05,
      "loss": 0.6753,
      "step": 555200
    },
    {
      "epoch": 8.717425431711145,
      "grad_norm": 2.9167137145996094,
      "learning_rate": 4.455160910518053e-05,
      "loss": 0.6451,
      "step": 555300
    },
    {
      "epoch": 8.718995290423862,
      "grad_norm": 3.4165329933166504,
      "learning_rate": 4.455062794348509e-05,
      "loss": 0.6349,
      "step": 555400
    },
    {
      "epoch": 8.720565149136577,
      "grad_norm": 3.6209864616394043,
      "learning_rate": 4.454964678178964e-05,
      "loss": 0.6149,
      "step": 555500
    },
    {
      "epoch": 8.722135007849294,
      "grad_norm": 3.7204530239105225,
      "learning_rate": 4.454866562009419e-05,
      "loss": 0.6396,
      "step": 555600
    },
    {
      "epoch": 8.72370486656201,
      "grad_norm": 4.470249176025391,
      "learning_rate": 4.454768445839875e-05,
      "loss": 0.6829,
      "step": 555700
    },
    {
      "epoch": 8.725274725274724,
      "grad_norm": 2.966470956802368,
      "learning_rate": 4.45467032967033e-05,
      "loss": 0.6444,
      "step": 555800
    },
    {
      "epoch": 8.726844583987441,
      "grad_norm": 3.297403335571289,
      "learning_rate": 4.454572213500785e-05,
      "loss": 0.6588,
      "step": 555900
    },
    {
      "epoch": 8.728414442700156,
      "grad_norm": 2.9590256214141846,
      "learning_rate": 4.4544740973312404e-05,
      "loss": 0.6629,
      "step": 556000
    },
    {
      "epoch": 8.729984301412873,
      "grad_norm": 3.3313941955566406,
      "learning_rate": 4.454375981161696e-05,
      "loss": 0.6177,
      "step": 556100
    },
    {
      "epoch": 8.731554160125588,
      "grad_norm": 4.161615371704102,
      "learning_rate": 4.4542778649921506e-05,
      "loss": 0.6975,
      "step": 556200
    },
    {
      "epoch": 8.733124018838305,
      "grad_norm": 3.210759401321411,
      "learning_rate": 4.454179748822606e-05,
      "loss": 0.6557,
      "step": 556300
    },
    {
      "epoch": 8.73469387755102,
      "grad_norm": 4.190515041351318,
      "learning_rate": 4.4540816326530614e-05,
      "loss": 0.65,
      "step": 556400
    },
    {
      "epoch": 8.736263736263737,
      "grad_norm": 4.531270503997803,
      "learning_rate": 4.453983516483517e-05,
      "loss": 0.6404,
      "step": 556500
    },
    {
      "epoch": 8.737833594976452,
      "grad_norm": 3.167632579803467,
      "learning_rate": 4.4538854003139716e-05,
      "loss": 0.6467,
      "step": 556600
    },
    {
      "epoch": 8.739403453689167,
      "grad_norm": 4.028703689575195,
      "learning_rate": 4.4537872841444274e-05,
      "loss": 0.6498,
      "step": 556700
    },
    {
      "epoch": 8.740973312401884,
      "grad_norm": 3.218050956726074,
      "learning_rate": 4.4536891679748825e-05,
      "loss": 0.631,
      "step": 556800
    },
    {
      "epoch": 8.7425431711146,
      "grad_norm": 4.76231575012207,
      "learning_rate": 4.4535910518053376e-05,
      "loss": 0.6234,
      "step": 556900
    },
    {
      "epoch": 8.744113029827316,
      "grad_norm": 4.300209045410156,
      "learning_rate": 4.453492935635793e-05,
      "loss": 0.6769,
      "step": 557000
    },
    {
      "epoch": 8.745682888540031,
      "grad_norm": 3.308732509613037,
      "learning_rate": 4.4533948194662485e-05,
      "loss": 0.6372,
      "step": 557100
    },
    {
      "epoch": 8.747252747252748,
      "grad_norm": 4.030964374542236,
      "learning_rate": 4.4532967032967036e-05,
      "loss": 0.6242,
      "step": 557200
    },
    {
      "epoch": 8.748822605965463,
      "grad_norm": 4.235559463500977,
      "learning_rate": 4.453198587127159e-05,
      "loss": 0.6215,
      "step": 557300
    },
    {
      "epoch": 8.750392464678178,
      "grad_norm": 4.718372344970703,
      "learning_rate": 4.453100470957614e-05,
      "loss": 0.6096,
      "step": 557400
    },
    {
      "epoch": 8.751962323390895,
      "grad_norm": 4.014254093170166,
      "learning_rate": 4.4530023547880695e-05,
      "loss": 0.6111,
      "step": 557500
    },
    {
      "epoch": 8.75353218210361,
      "grad_norm": 4.012407302856445,
      "learning_rate": 4.4529042386185246e-05,
      "loss": 0.6657,
      "step": 557600
    },
    {
      "epoch": 8.755102040816327,
      "grad_norm": 4.50803279876709,
      "learning_rate": 4.45280612244898e-05,
      "loss": 0.6503,
      "step": 557700
    },
    {
      "epoch": 8.756671899529042,
      "grad_norm": 5.019672870635986,
      "learning_rate": 4.4527080062794355e-05,
      "loss": 0.6466,
      "step": 557800
    },
    {
      "epoch": 8.758241758241759,
      "grad_norm": 3.5241639614105225,
      "learning_rate": 4.45260989010989e-05,
      "loss": 0.635,
      "step": 557900
    },
    {
      "epoch": 8.759811616954474,
      "grad_norm": 4.670170783996582,
      "learning_rate": 4.452511773940346e-05,
      "loss": 0.6351,
      "step": 558000
    },
    {
      "epoch": 8.76138147566719,
      "grad_norm": 4.257440090179443,
      "learning_rate": 4.452413657770801e-05,
      "loss": 0.6796,
      "step": 558100
    },
    {
      "epoch": 8.762951334379906,
      "grad_norm": 3.1512365341186523,
      "learning_rate": 4.4523155416012566e-05,
      "loss": 0.6722,
      "step": 558200
    },
    {
      "epoch": 8.764521193092621,
      "grad_norm": 3.655930757522583,
      "learning_rate": 4.452217425431711e-05,
      "loss": 0.6422,
      "step": 558300
    },
    {
      "epoch": 8.766091051805338,
      "grad_norm": 3.596062660217285,
      "learning_rate": 4.452119309262167e-05,
      "loss": 0.6332,
      "step": 558400
    },
    {
      "epoch": 8.767660910518053,
      "grad_norm": 3.7279787063598633,
      "learning_rate": 4.452021193092622e-05,
      "loss": 0.6387,
      "step": 558500
    },
    {
      "epoch": 8.76923076923077,
      "grad_norm": 4.023801803588867,
      "learning_rate": 4.451923076923077e-05,
      "loss": 0.6765,
      "step": 558600
    },
    {
      "epoch": 8.770800627943485,
      "grad_norm": 3.8874495029449463,
      "learning_rate": 4.451824960753532e-05,
      "loss": 0.585,
      "step": 558700
    },
    {
      "epoch": 8.7723704866562,
      "grad_norm": 3.361320734024048,
      "learning_rate": 4.451726844583988e-05,
      "loss": 0.6502,
      "step": 558800
    },
    {
      "epoch": 8.773940345368917,
      "grad_norm": 3.6477816104888916,
      "learning_rate": 4.451628728414443e-05,
      "loss": 0.6826,
      "step": 558900
    },
    {
      "epoch": 8.775510204081632,
      "grad_norm": 3.9823107719421387,
      "learning_rate": 4.451530612244898e-05,
      "loss": 0.6686,
      "step": 559000
    },
    {
      "epoch": 8.777080062794349,
      "grad_norm": 4.4856390953063965,
      "learning_rate": 4.451432496075353e-05,
      "loss": 0.6677,
      "step": 559100
    },
    {
      "epoch": 8.778649921507064,
      "grad_norm": 4.624337673187256,
      "learning_rate": 4.451334379905809e-05,
      "loss": 0.6346,
      "step": 559200
    },
    {
      "epoch": 8.780219780219781,
      "grad_norm": 3.988532781600952,
      "learning_rate": 4.4512362637362633e-05,
      "loss": 0.6357,
      "step": 559300
    },
    {
      "epoch": 8.781789638932496,
      "grad_norm": 3.189364433288574,
      "learning_rate": 4.451138147566719e-05,
      "loss": 0.6231,
      "step": 559400
    },
    {
      "epoch": 8.783359497645211,
      "grad_norm": 3.7843823432922363,
      "learning_rate": 4.451040031397174e-05,
      "loss": 0.6706,
      "step": 559500
    },
    {
      "epoch": 8.784929356357928,
      "grad_norm": 4.192131042480469,
      "learning_rate": 4.45094191522763e-05,
      "loss": 0.6571,
      "step": 559600
    },
    {
      "epoch": 8.786499215070643,
      "grad_norm": 4.055333614349365,
      "learning_rate": 4.450843799058085e-05,
      "loss": 0.634,
      "step": 559700
    },
    {
      "epoch": 8.78806907378336,
      "grad_norm": 4.419524669647217,
      "learning_rate": 4.45074568288854e-05,
      "loss": 0.6473,
      "step": 559800
    },
    {
      "epoch": 8.789638932496075,
      "grad_norm": 4.126574993133545,
      "learning_rate": 4.450647566718996e-05,
      "loss": 0.6554,
      "step": 559900
    },
    {
      "epoch": 8.791208791208792,
      "grad_norm": 4.475156784057617,
      "learning_rate": 4.4505494505494504e-05,
      "loss": 0.6355,
      "step": 560000
    },
    {
      "epoch": 8.792778649921507,
      "grad_norm": 3.9720458984375,
      "learning_rate": 4.450451334379906e-05,
      "loss": 0.6657,
      "step": 560100
    },
    {
      "epoch": 8.794348508634222,
      "grad_norm": 3.6774404048919678,
      "learning_rate": 4.450353218210361e-05,
      "loss": 0.6213,
      "step": 560200
    },
    {
      "epoch": 8.795918367346939,
      "grad_norm": 3.99025821685791,
      "learning_rate": 4.450255102040817e-05,
      "loss": 0.6392,
      "step": 560300
    },
    {
      "epoch": 8.797488226059654,
      "grad_norm": 4.240264892578125,
      "learning_rate": 4.4501569858712715e-05,
      "loss": 0.6579,
      "step": 560400
    },
    {
      "epoch": 8.799058084772371,
      "grad_norm": 3.417452335357666,
      "learning_rate": 4.450058869701727e-05,
      "loss": 0.6434,
      "step": 560500
    },
    {
      "epoch": 8.800627943485086,
      "grad_norm": 3.3225502967834473,
      "learning_rate": 4.449960753532182e-05,
      "loss": 0.639,
      "step": 560600
    },
    {
      "epoch": 8.802197802197803,
      "grad_norm": 3.087557792663574,
      "learning_rate": 4.4498626373626374e-05,
      "loss": 0.6309,
      "step": 560700
    },
    {
      "epoch": 8.803767660910518,
      "grad_norm": 3.965662956237793,
      "learning_rate": 4.4497645211930925e-05,
      "loss": 0.6417,
      "step": 560800
    },
    {
      "epoch": 8.805337519623233,
      "grad_norm": 3.309995174407959,
      "learning_rate": 4.449666405023548e-05,
      "loss": 0.6485,
      "step": 560900
    },
    {
      "epoch": 8.80690737833595,
      "grad_norm": 4.901336193084717,
      "learning_rate": 4.4495682888540034e-05,
      "loss": 0.6727,
      "step": 561000
    },
    {
      "epoch": 8.808477237048665,
      "grad_norm": 3.5064330101013184,
      "learning_rate": 4.4494701726844585e-05,
      "loss": 0.6421,
      "step": 561100
    },
    {
      "epoch": 8.810047095761382,
      "grad_norm": 4.088964462280273,
      "learning_rate": 4.4493720565149136e-05,
      "loss": 0.6436,
      "step": 561200
    },
    {
      "epoch": 8.811616954474097,
      "grad_norm": 4.304647445678711,
      "learning_rate": 4.4492739403453694e-05,
      "loss": 0.6405,
      "step": 561300
    },
    {
      "epoch": 8.813186813186814,
      "grad_norm": 3.98274302482605,
      "learning_rate": 4.449175824175824e-05,
      "loss": 0.67,
      "step": 561400
    },
    {
      "epoch": 8.814756671899529,
      "grad_norm": 3.8011562824249268,
      "learning_rate": 4.4490777080062796e-05,
      "loss": 0.6296,
      "step": 561500
    },
    {
      "epoch": 8.816326530612244,
      "grad_norm": 3.0933353900909424,
      "learning_rate": 4.448979591836735e-05,
      "loss": 0.636,
      "step": 561600
    },
    {
      "epoch": 8.817896389324961,
      "grad_norm": 3.461668014526367,
      "learning_rate": 4.4488814756671904e-05,
      "loss": 0.6247,
      "step": 561700
    },
    {
      "epoch": 8.819466248037676,
      "grad_norm": 2.7053306102752686,
      "learning_rate": 4.4487833594976455e-05,
      "loss": 0.6466,
      "step": 561800
    },
    {
      "epoch": 8.821036106750393,
      "grad_norm": 3.6608846187591553,
      "learning_rate": 4.4486852433281006e-05,
      "loss": 0.6344,
      "step": 561900
    },
    {
      "epoch": 8.822605965463108,
      "grad_norm": 5.028148651123047,
      "learning_rate": 4.4485871271585564e-05,
      "loss": 0.652,
      "step": 562000
    },
    {
      "epoch": 8.824175824175825,
      "grad_norm": 3.7917840480804443,
      "learning_rate": 4.448489010989011e-05,
      "loss": 0.6189,
      "step": 562100
    },
    {
      "epoch": 8.82574568288854,
      "grad_norm": 1.9886765480041504,
      "learning_rate": 4.4483908948194666e-05,
      "loss": 0.6391,
      "step": 562200
    },
    {
      "epoch": 8.827315541601255,
      "grad_norm": 4.151397705078125,
      "learning_rate": 4.448292778649922e-05,
      "loss": 0.6564,
      "step": 562300
    },
    {
      "epoch": 8.828885400313972,
      "grad_norm": 4.247830867767334,
      "learning_rate": 4.4481946624803775e-05,
      "loss": 0.627,
      "step": 562400
    },
    {
      "epoch": 8.830455259026687,
      "grad_norm": 4.367666244506836,
      "learning_rate": 4.448096546310832e-05,
      "loss": 0.6671,
      "step": 562500
    },
    {
      "epoch": 8.832025117739404,
      "grad_norm": 3.0184926986694336,
      "learning_rate": 4.447998430141288e-05,
      "loss": 0.6622,
      "step": 562600
    },
    {
      "epoch": 8.833594976452119,
      "grad_norm": 3.9979729652404785,
      "learning_rate": 4.447900313971743e-05,
      "loss": 0.668,
      "step": 562700
    },
    {
      "epoch": 8.835164835164836,
      "grad_norm": 3.1912038326263428,
      "learning_rate": 4.447802197802198e-05,
      "loss": 0.6398,
      "step": 562800
    },
    {
      "epoch": 8.83673469387755,
      "grad_norm": 3.602522134780884,
      "learning_rate": 4.447704081632653e-05,
      "loss": 0.6567,
      "step": 562900
    },
    {
      "epoch": 8.838304552590268,
      "grad_norm": 2.561302661895752,
      "learning_rate": 4.447605965463109e-05,
      "loss": 0.6419,
      "step": 563000
    },
    {
      "epoch": 8.839874411302983,
      "grad_norm": 3.5158636569976807,
      "learning_rate": 4.447507849293564e-05,
      "loss": 0.6472,
      "step": 563100
    },
    {
      "epoch": 8.841444270015698,
      "grad_norm": 4.085956573486328,
      "learning_rate": 4.447409733124019e-05,
      "loss": 0.6356,
      "step": 563200
    },
    {
      "epoch": 8.843014128728415,
      "grad_norm": 3.961315155029297,
      "learning_rate": 4.447311616954474e-05,
      "loss": 0.6293,
      "step": 563300
    },
    {
      "epoch": 8.84458398744113,
      "grad_norm": 2.8479888439178467,
      "learning_rate": 4.44721350078493e-05,
      "loss": 0.6327,
      "step": 563400
    },
    {
      "epoch": 8.846153846153847,
      "grad_norm": 4.108383655548096,
      "learning_rate": 4.447115384615384e-05,
      "loss": 0.6216,
      "step": 563500
    },
    {
      "epoch": 8.847723704866562,
      "grad_norm": 3.0711171627044678,
      "learning_rate": 4.44701726844584e-05,
      "loss": 0.6487,
      "step": 563600
    },
    {
      "epoch": 8.849293563579279,
      "grad_norm": 3.954439878463745,
      "learning_rate": 4.446919152276295e-05,
      "loss": 0.625,
      "step": 563700
    },
    {
      "epoch": 8.850863422291994,
      "grad_norm": 3.994515895843506,
      "learning_rate": 4.446821036106751e-05,
      "loss": 0.6204,
      "step": 563800
    },
    {
      "epoch": 8.852433281004709,
      "grad_norm": 4.649466037750244,
      "learning_rate": 4.446722919937206e-05,
      "loss": 0.6774,
      "step": 563900
    },
    {
      "epoch": 8.854003139717426,
      "grad_norm": 3.15256667137146,
      "learning_rate": 4.446624803767661e-05,
      "loss": 0.6686,
      "step": 564000
    },
    {
      "epoch": 8.85557299843014,
      "grad_norm": 5.386981010437012,
      "learning_rate": 4.446526687598117e-05,
      "loss": 0.6726,
      "step": 564100
    },
    {
      "epoch": 8.857142857142858,
      "grad_norm": 3.3913912773132324,
      "learning_rate": 4.446428571428571e-05,
      "loss": 0.6096,
      "step": 564200
    },
    {
      "epoch": 8.858712715855573,
      "grad_norm": 2.5069119930267334,
      "learning_rate": 4.446330455259027e-05,
      "loss": 0.6297,
      "step": 564300
    },
    {
      "epoch": 8.86028257456829,
      "grad_norm": 4.692251682281494,
      "learning_rate": 4.446232339089482e-05,
      "loss": 0.6228,
      "step": 564400
    },
    {
      "epoch": 8.861852433281005,
      "grad_norm": 4.037857532501221,
      "learning_rate": 4.446134222919938e-05,
      "loss": 0.6544,
      "step": 564500
    },
    {
      "epoch": 8.86342229199372,
      "grad_norm": 2.770890951156616,
      "learning_rate": 4.4460361067503924e-05,
      "loss": 0.6561,
      "step": 564600
    },
    {
      "epoch": 8.864992150706437,
      "grad_norm": 3.8150172233581543,
      "learning_rate": 4.445937990580848e-05,
      "loss": 0.6732,
      "step": 564700
    },
    {
      "epoch": 8.866562009419152,
      "grad_norm": 4.233813762664795,
      "learning_rate": 4.445839874411303e-05,
      "loss": 0.6259,
      "step": 564800
    },
    {
      "epoch": 8.868131868131869,
      "grad_norm": 3.139897108078003,
      "learning_rate": 4.445741758241758e-05,
      "loss": 0.6316,
      "step": 564900
    },
    {
      "epoch": 8.869701726844584,
      "grad_norm": 4.993257522583008,
      "learning_rate": 4.4456436420722134e-05,
      "loss": 0.6525,
      "step": 565000
    },
    {
      "epoch": 8.8712715855573,
      "grad_norm": 8.218441009521484,
      "learning_rate": 4.445545525902669e-05,
      "loss": 0.6954,
      "step": 565100
    },
    {
      "epoch": 8.872841444270016,
      "grad_norm": 3.635450839996338,
      "learning_rate": 4.445447409733124e-05,
      "loss": 0.6185,
      "step": 565200
    },
    {
      "epoch": 8.87441130298273,
      "grad_norm": 4.329414367675781,
      "learning_rate": 4.4453492935635794e-05,
      "loss": 0.6466,
      "step": 565300
    },
    {
      "epoch": 8.875981161695448,
      "grad_norm": 3.888765335083008,
      "learning_rate": 4.4452511773940345e-05,
      "loss": 0.6725,
      "step": 565400
    },
    {
      "epoch": 8.877551020408163,
      "grad_norm": 3.3240230083465576,
      "learning_rate": 4.44515306122449e-05,
      "loss": 0.6316,
      "step": 565500
    },
    {
      "epoch": 8.87912087912088,
      "grad_norm": 3.980121612548828,
      "learning_rate": 4.445054945054945e-05,
      "loss": 0.6623,
      "step": 565600
    },
    {
      "epoch": 8.880690737833595,
      "grad_norm": 3.178643226623535,
      "learning_rate": 4.4449568288854005e-05,
      "loss": 0.6593,
      "step": 565700
    },
    {
      "epoch": 8.882260596546312,
      "grad_norm": 3.5818917751312256,
      "learning_rate": 4.4448587127158556e-05,
      "loss": 0.614,
      "step": 565800
    },
    {
      "epoch": 8.883830455259027,
      "grad_norm": 3.376300573348999,
      "learning_rate": 4.444760596546311e-05,
      "loss": 0.6646,
      "step": 565900
    },
    {
      "epoch": 8.885400313971743,
      "grad_norm": 2.7814700603485107,
      "learning_rate": 4.4446624803767664e-05,
      "loss": 0.6527,
      "step": 566000
    },
    {
      "epoch": 8.886970172684459,
      "grad_norm": 3.0908775329589844,
      "learning_rate": 4.4445643642072215e-05,
      "loss": 0.6514,
      "step": 566100
    },
    {
      "epoch": 8.888540031397174,
      "grad_norm": 4.00169038772583,
      "learning_rate": 4.444466248037677e-05,
      "loss": 0.6804,
      "step": 566200
    },
    {
      "epoch": 8.89010989010989,
      "grad_norm": 4.626260757446289,
      "learning_rate": 4.444368131868132e-05,
      "loss": 0.6219,
      "step": 566300
    },
    {
      "epoch": 8.891679748822606,
      "grad_norm": 4.144922733306885,
      "learning_rate": 4.4442700156985875e-05,
      "loss": 0.6361,
      "step": 566400
    },
    {
      "epoch": 8.893249607535322,
      "grad_norm": 3.1737112998962402,
      "learning_rate": 4.4441718995290426e-05,
      "loss": 0.6472,
      "step": 566500
    },
    {
      "epoch": 8.894819466248038,
      "grad_norm": 3.774299383163452,
      "learning_rate": 4.4440737833594984e-05,
      "loss": 0.6005,
      "step": 566600
    },
    {
      "epoch": 8.896389324960754,
      "grad_norm": 4.737890243530273,
      "learning_rate": 4.443975667189953e-05,
      "loss": 0.661,
      "step": 566700
    },
    {
      "epoch": 8.89795918367347,
      "grad_norm": 4.61859130859375,
      "learning_rate": 4.4438775510204086e-05,
      "loss": 0.6187,
      "step": 566800
    },
    {
      "epoch": 8.899529042386185,
      "grad_norm": 4.501461982727051,
      "learning_rate": 4.443779434850864e-05,
      "loss": 0.6241,
      "step": 566900
    },
    {
      "epoch": 8.901098901098901,
      "grad_norm": 2.627760648727417,
      "learning_rate": 4.443681318681319e-05,
      "loss": 0.6358,
      "step": 567000
    },
    {
      "epoch": 8.902668759811617,
      "grad_norm": 4.632437705993652,
      "learning_rate": 4.443583202511774e-05,
      "loss": 0.6292,
      "step": 567100
    },
    {
      "epoch": 8.904238618524333,
      "grad_norm": 3.305931806564331,
      "learning_rate": 4.4434850863422296e-05,
      "loss": 0.6341,
      "step": 567200
    },
    {
      "epoch": 8.905808477237048,
      "grad_norm": 3.3185417652130127,
      "learning_rate": 4.443386970172685e-05,
      "loss": 0.6579,
      "step": 567300
    },
    {
      "epoch": 8.907378335949765,
      "grad_norm": 4.386584758758545,
      "learning_rate": 4.44328885400314e-05,
      "loss": 0.6668,
      "step": 567400
    },
    {
      "epoch": 8.90894819466248,
      "grad_norm": 3.4331185817718506,
      "learning_rate": 4.443190737833595e-05,
      "loss": 0.6472,
      "step": 567500
    },
    {
      "epoch": 8.910518053375196,
      "grad_norm": 5.535481929779053,
      "learning_rate": 4.443092621664051e-05,
      "loss": 0.6043,
      "step": 567600
    },
    {
      "epoch": 8.912087912087912,
      "grad_norm": 4.0937628746032715,
      "learning_rate": 4.442994505494505e-05,
      "loss": 0.624,
      "step": 567700
    },
    {
      "epoch": 8.913657770800627,
      "grad_norm": 3.3204545974731445,
      "learning_rate": 4.442896389324961e-05,
      "loss": 0.6267,
      "step": 567800
    },
    {
      "epoch": 8.915227629513344,
      "grad_norm": 4.244992733001709,
      "learning_rate": 4.442798273155416e-05,
      "loss": 0.6609,
      "step": 567900
    },
    {
      "epoch": 8.91679748822606,
      "grad_norm": 4.202767848968506,
      "learning_rate": 4.442700156985872e-05,
      "loss": 0.6014,
      "step": 568000
    },
    {
      "epoch": 8.918367346938776,
      "grad_norm": 4.857175827026367,
      "learning_rate": 4.442602040816327e-05,
      "loss": 0.6535,
      "step": 568100
    },
    {
      "epoch": 8.919937205651491,
      "grad_norm": 3.5065977573394775,
      "learning_rate": 4.442503924646782e-05,
      "loss": 0.6523,
      "step": 568200
    },
    {
      "epoch": 8.921507064364206,
      "grad_norm": 4.428284168243408,
      "learning_rate": 4.442405808477237e-05,
      "loss": 0.6946,
      "step": 568300
    },
    {
      "epoch": 8.923076923076923,
      "grad_norm": 3.8836138248443604,
      "learning_rate": 4.442307692307692e-05,
      "loss": 0.6421,
      "step": 568400
    },
    {
      "epoch": 8.924646781789638,
      "grad_norm": 4.229650497436523,
      "learning_rate": 4.442209576138148e-05,
      "loss": 0.6473,
      "step": 568500
    },
    {
      "epoch": 8.926216640502355,
      "grad_norm": 3.5428106784820557,
      "learning_rate": 4.442111459968603e-05,
      "loss": 0.6402,
      "step": 568600
    },
    {
      "epoch": 8.92778649921507,
      "grad_norm": 4.462611198425293,
      "learning_rate": 4.442013343799059e-05,
      "loss": 0.645,
      "step": 568700
    },
    {
      "epoch": 8.929356357927787,
      "grad_norm": 3.918684720993042,
      "learning_rate": 4.441915227629513e-05,
      "loss": 0.6403,
      "step": 568800
    },
    {
      "epoch": 8.930926216640502,
      "grad_norm": 4.077979564666748,
      "learning_rate": 4.441817111459969e-05,
      "loss": 0.6285,
      "step": 568900
    },
    {
      "epoch": 8.932496075353217,
      "grad_norm": 2.853278160095215,
      "learning_rate": 4.441718995290424e-05,
      "loss": 0.6252,
      "step": 569000
    },
    {
      "epoch": 8.934065934065934,
      "grad_norm": 4.472587585449219,
      "learning_rate": 4.441620879120879e-05,
      "loss": 0.635,
      "step": 569100
    },
    {
      "epoch": 8.93563579277865,
      "grad_norm": 3.3614041805267334,
      "learning_rate": 4.441522762951334e-05,
      "loss": 0.6516,
      "step": 569200
    },
    {
      "epoch": 8.937205651491366,
      "grad_norm": 3.4545929431915283,
      "learning_rate": 4.44142464678179e-05,
      "loss": 0.6282,
      "step": 569300
    },
    {
      "epoch": 8.938775510204081,
      "grad_norm": 3.969315528869629,
      "learning_rate": 4.441326530612245e-05,
      "loss": 0.6707,
      "step": 569400
    },
    {
      "epoch": 8.940345368916798,
      "grad_norm": 4.298430919647217,
      "learning_rate": 4.4412284144427e-05,
      "loss": 0.6465,
      "step": 569500
    },
    {
      "epoch": 8.941915227629513,
      "grad_norm": 3.849463701248169,
      "learning_rate": 4.4411302982731554e-05,
      "loss": 0.62,
      "step": 569600
    },
    {
      "epoch": 8.943485086342228,
      "grad_norm": 3.210645914077759,
      "learning_rate": 4.441032182103611e-05,
      "loss": 0.6259,
      "step": 569700
    },
    {
      "epoch": 8.945054945054945,
      "grad_norm": 3.0653576850891113,
      "learning_rate": 4.4409340659340656e-05,
      "loss": 0.6082,
      "step": 569800
    },
    {
      "epoch": 8.94662480376766,
      "grad_norm": 3.4595413208007812,
      "learning_rate": 4.4408359497645214e-05,
      "loss": 0.6489,
      "step": 569900
    },
    {
      "epoch": 8.948194662480377,
      "grad_norm": 4.947179794311523,
      "learning_rate": 4.4407378335949765e-05,
      "loss": 0.6737,
      "step": 570000
    },
    {
      "epoch": 8.949764521193092,
      "grad_norm": 5.0166168212890625,
      "learning_rate": 4.440639717425432e-05,
      "loss": 0.6321,
      "step": 570100
    },
    {
      "epoch": 8.95133437990581,
      "grad_norm": 4.871725559234619,
      "learning_rate": 4.440541601255887e-05,
      "loss": 0.6558,
      "step": 570200
    },
    {
      "epoch": 8.952904238618524,
      "grad_norm": 4.770059108734131,
      "learning_rate": 4.4404434850863424e-05,
      "loss": 0.6387,
      "step": 570300
    },
    {
      "epoch": 8.95447409733124,
      "grad_norm": 4.394201278686523,
      "learning_rate": 4.4403453689167975e-05,
      "loss": 0.6604,
      "step": 570400
    },
    {
      "epoch": 8.956043956043956,
      "grad_norm": 3.598999261856079,
      "learning_rate": 4.4402472527472526e-05,
      "loss": 0.6755,
      "step": 570500
    },
    {
      "epoch": 8.957613814756671,
      "grad_norm": 3.009249687194824,
      "learning_rate": 4.4401491365777084e-05,
      "loss": 0.6919,
      "step": 570600
    },
    {
      "epoch": 8.959183673469388,
      "grad_norm": 4.118425369262695,
      "learning_rate": 4.4400510204081635e-05,
      "loss": 0.6848,
      "step": 570700
    },
    {
      "epoch": 8.960753532182103,
      "grad_norm": 3.7398576736450195,
      "learning_rate": 4.439952904238619e-05,
      "loss": 0.621,
      "step": 570800
    },
    {
      "epoch": 8.96232339089482,
      "grad_norm": 4.775282382965088,
      "learning_rate": 4.439854788069074e-05,
      "loss": 0.6262,
      "step": 570900
    },
    {
      "epoch": 8.963893249607535,
      "grad_norm": 5.250401973724365,
      "learning_rate": 4.4397566718995295e-05,
      "loss": 0.6563,
      "step": 571000
    },
    {
      "epoch": 8.96546310832025,
      "grad_norm": 5.3502888679504395,
      "learning_rate": 4.4396585557299846e-05,
      "loss": 0.6587,
      "step": 571100
    },
    {
      "epoch": 8.967032967032967,
      "grad_norm": 4.030148029327393,
      "learning_rate": 4.43956043956044e-05,
      "loss": 0.6404,
      "step": 571200
    },
    {
      "epoch": 8.968602825745682,
      "grad_norm": 3.8257358074188232,
      "learning_rate": 4.439462323390895e-05,
      "loss": 0.6403,
      "step": 571300
    },
    {
      "epoch": 8.970172684458399,
      "grad_norm": 4.462395191192627,
      "learning_rate": 4.4393642072213505e-05,
      "loss": 0.6544,
      "step": 571400
    },
    {
      "epoch": 8.971742543171114,
      "grad_norm": 4.545942783355713,
      "learning_rate": 4.4392660910518056e-05,
      "loss": 0.6658,
      "step": 571500
    },
    {
      "epoch": 8.973312401883831,
      "grad_norm": 2.7545559406280518,
      "learning_rate": 4.439167974882261e-05,
      "loss": 0.6245,
      "step": 571600
    },
    {
      "epoch": 8.974882260596546,
      "grad_norm": 4.199215412139893,
      "learning_rate": 4.439069858712716e-05,
      "loss": 0.6803,
      "step": 571700
    },
    {
      "epoch": 8.976452119309261,
      "grad_norm": 3.0445783138275146,
      "learning_rate": 4.4389717425431716e-05,
      "loss": 0.6613,
      "step": 571800
    },
    {
      "epoch": 8.978021978021978,
      "grad_norm": 3.9185714721679688,
      "learning_rate": 4.438873626373626e-05,
      "loss": 0.6477,
      "step": 571900
    },
    {
      "epoch": 8.979591836734693,
      "grad_norm": 3.4325146675109863,
      "learning_rate": 4.438775510204082e-05,
      "loss": 0.6095,
      "step": 572000
    },
    {
      "epoch": 8.98116169544741,
      "grad_norm": 4.585019111633301,
      "learning_rate": 4.438677394034537e-05,
      "loss": 0.6357,
      "step": 572100
    },
    {
      "epoch": 8.982731554160125,
      "grad_norm": 5.190730571746826,
      "learning_rate": 4.438579277864993e-05,
      "loss": 0.6134,
      "step": 572200
    },
    {
      "epoch": 8.984301412872842,
      "grad_norm": 4.013345241546631,
      "learning_rate": 4.438481161695448e-05,
      "loss": 0.688,
      "step": 572300
    },
    {
      "epoch": 8.985871271585557,
      "grad_norm": 3.9723398685455322,
      "learning_rate": 4.438383045525903e-05,
      "loss": 0.6232,
      "step": 572400
    },
    {
      "epoch": 8.987441130298274,
      "grad_norm": 2.5706491470336914,
      "learning_rate": 4.438284929356358e-05,
      "loss": 0.6762,
      "step": 572500
    },
    {
      "epoch": 8.989010989010989,
      "grad_norm": 3.391627788543701,
      "learning_rate": 4.438186813186813e-05,
      "loss": 0.6055,
      "step": 572600
    },
    {
      "epoch": 8.990580847723704,
      "grad_norm": 4.079648017883301,
      "learning_rate": 4.438088697017269e-05,
      "loss": 0.6029,
      "step": 572700
    },
    {
      "epoch": 8.992150706436421,
      "grad_norm": 3.574521064758301,
      "learning_rate": 4.437990580847724e-05,
      "loss": 0.6172,
      "step": 572800
    },
    {
      "epoch": 8.993720565149136,
      "grad_norm": 3.4576077461242676,
      "learning_rate": 4.43789246467818e-05,
      "loss": 0.6433,
      "step": 572900
    },
    {
      "epoch": 8.995290423861853,
      "grad_norm": 3.8074405193328857,
      "learning_rate": 4.437794348508634e-05,
      "loss": 0.6723,
      "step": 573000
    },
    {
      "epoch": 8.996860282574568,
      "grad_norm": 4.01403284072876,
      "learning_rate": 4.43769623233909e-05,
      "loss": 0.6769,
      "step": 573100
    },
    {
      "epoch": 8.998430141287285,
      "grad_norm": 2.868176221847534,
      "learning_rate": 4.437598116169545e-05,
      "loss": 0.6665,
      "step": 573200
    },
    {
      "epoch": 9.0,
      "grad_norm": 3.872978687286377,
      "learning_rate": 4.4375e-05,
      "loss": 0.6551,
      "step": 573300
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.020621418952942,
      "eval_runtime": 14.6342,
      "eval_samples_per_second": 229.121,
      "eval_steps_per_second": 229.121,
      "step": 573300
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.49833229184150696,
      "eval_runtime": 280.5738,
      "eval_samples_per_second": 227.035,
      "eval_steps_per_second": 227.035,
      "step": 573300
    },
    {
      "epoch": 9.001569858712715,
      "grad_norm": 4.276501178741455,
      "learning_rate": 4.437401883830455e-05,
      "loss": 0.6185,
      "step": 573400
    },
    {
      "epoch": 9.003139717425432,
      "grad_norm": 4.202734470367432,
      "learning_rate": 4.437303767660911e-05,
      "loss": 0.6305,
      "step": 573500
    },
    {
      "epoch": 9.004709576138147,
      "grad_norm": 3.509608745574951,
      "learning_rate": 4.437205651491366e-05,
      "loss": 0.6557,
      "step": 573600
    },
    {
      "epoch": 9.006279434850864,
      "grad_norm": 5.0389509201049805,
      "learning_rate": 4.437107535321821e-05,
      "loss": 0.6179,
      "step": 573700
    },
    {
      "epoch": 9.007849293563579,
      "grad_norm": 3.202655553817749,
      "learning_rate": 4.437009419152276e-05,
      "loss": 0.5888,
      "step": 573800
    },
    {
      "epoch": 9.009419152276296,
      "grad_norm": 4.243131637573242,
      "learning_rate": 4.436911302982732e-05,
      "loss": 0.6562,
      "step": 573900
    },
    {
      "epoch": 9.010989010989011,
      "grad_norm": 4.224087715148926,
      "learning_rate": 4.4368131868131865e-05,
      "loss": 0.6318,
      "step": 574000
    },
    {
      "epoch": 9.012558869701726,
      "grad_norm": 3.9612653255462646,
      "learning_rate": 4.436715070643642e-05,
      "loss": 0.6382,
      "step": 574100
    },
    {
      "epoch": 9.014128728414443,
      "grad_norm": 4.030497074127197,
      "learning_rate": 4.4366169544740974e-05,
      "loss": 0.6249,
      "step": 574200
    },
    {
      "epoch": 9.015698587127158,
      "grad_norm": 4.480139255523682,
      "learning_rate": 4.436518838304553e-05,
      "loss": 0.6688,
      "step": 574300
    },
    {
      "epoch": 9.017268445839875,
      "grad_norm": 3.227296829223633,
      "learning_rate": 4.436420722135008e-05,
      "loss": 0.6366,
      "step": 574400
    },
    {
      "epoch": 9.01883830455259,
      "grad_norm": 3.844406843185425,
      "learning_rate": 4.436322605965463e-05,
      "loss": 0.6162,
      "step": 574500
    },
    {
      "epoch": 9.020408163265307,
      "grad_norm": 4.0624098777771,
      "learning_rate": 4.4362244897959184e-05,
      "loss": 0.6244,
      "step": 574600
    },
    {
      "epoch": 9.021978021978022,
      "grad_norm": 3.7804458141326904,
      "learning_rate": 4.4361263736263735e-05,
      "loss": 0.6151,
      "step": 574700
    },
    {
      "epoch": 9.023547880690737,
      "grad_norm": 4.729689121246338,
      "learning_rate": 4.436028257456829e-05,
      "loss": 0.6826,
      "step": 574800
    },
    {
      "epoch": 9.025117739403454,
      "grad_norm": 4.713578701019287,
      "learning_rate": 4.4359301412872844e-05,
      "loss": 0.6634,
      "step": 574900
    },
    {
      "epoch": 9.026687598116169,
      "grad_norm": 3.2944223880767822,
      "learning_rate": 4.43583202511774e-05,
      "loss": 0.6521,
      "step": 575000
    },
    {
      "epoch": 9.028257456828886,
      "grad_norm": 4.1800761222839355,
      "learning_rate": 4.4357339089481946e-05,
      "loss": 0.6238,
      "step": 575100
    },
    {
      "epoch": 9.029827315541601,
      "grad_norm": 3.8231492042541504,
      "learning_rate": 4.4356357927786504e-05,
      "loss": 0.6486,
      "step": 575200
    },
    {
      "epoch": 9.031397174254318,
      "grad_norm": 2.290706157684326,
      "learning_rate": 4.4355376766091055e-05,
      "loss": 0.6185,
      "step": 575300
    },
    {
      "epoch": 9.032967032967033,
      "grad_norm": 4.644010543823242,
      "learning_rate": 4.4354395604395606e-05,
      "loss": 0.6042,
      "step": 575400
    },
    {
      "epoch": 9.034536891679748,
      "grad_norm": 3.5952961444854736,
      "learning_rate": 4.435341444270016e-05,
      "loss": 0.6445,
      "step": 575500
    },
    {
      "epoch": 9.036106750392465,
      "grad_norm": 3.828535556793213,
      "learning_rate": 4.4352433281004714e-05,
      "loss": 0.6517,
      "step": 575600
    },
    {
      "epoch": 9.03767660910518,
      "grad_norm": 3.615887403488159,
      "learning_rate": 4.4351452119309265e-05,
      "loss": 0.6379,
      "step": 575700
    },
    {
      "epoch": 9.039246467817897,
      "grad_norm": 4.595581531524658,
      "learning_rate": 4.4350470957613816e-05,
      "loss": 0.5879,
      "step": 575800
    },
    {
      "epoch": 9.040816326530612,
      "grad_norm": 4.179681777954102,
      "learning_rate": 4.434948979591837e-05,
      "loss": 0.6119,
      "step": 575900
    },
    {
      "epoch": 9.042386185243329,
      "grad_norm": 3.9014201164245605,
      "learning_rate": 4.4348508634222925e-05,
      "loss": 0.6261,
      "step": 576000
    },
    {
      "epoch": 9.043956043956044,
      "grad_norm": 4.55007266998291,
      "learning_rate": 4.434752747252747e-05,
      "loss": 0.6181,
      "step": 576100
    },
    {
      "epoch": 9.04552590266876,
      "grad_norm": 4.51201868057251,
      "learning_rate": 4.434654631083203e-05,
      "loss": 0.6355,
      "step": 576200
    },
    {
      "epoch": 9.047095761381476,
      "grad_norm": 4.742146968841553,
      "learning_rate": 4.434556514913658e-05,
      "loss": 0.6489,
      "step": 576300
    },
    {
      "epoch": 9.04866562009419,
      "grad_norm": 4.256378650665283,
      "learning_rate": 4.4344583987441136e-05,
      "loss": 0.6618,
      "step": 576400
    },
    {
      "epoch": 9.050235478806908,
      "grad_norm": 3.9624216556549072,
      "learning_rate": 4.434360282574569e-05,
      "loss": 0.6381,
      "step": 576500
    },
    {
      "epoch": 9.051805337519623,
      "grad_norm": 4.116766452789307,
      "learning_rate": 4.434262166405024e-05,
      "loss": 0.6495,
      "step": 576600
    },
    {
      "epoch": 9.05337519623234,
      "grad_norm": 4.055208206176758,
      "learning_rate": 4.434164050235479e-05,
      "loss": 0.6077,
      "step": 576700
    },
    {
      "epoch": 9.054945054945055,
      "grad_norm": 4.2627129554748535,
      "learning_rate": 4.434065934065934e-05,
      "loss": 0.6413,
      "step": 576800
    },
    {
      "epoch": 9.056514913657772,
      "grad_norm": 4.580260753631592,
      "learning_rate": 4.43396781789639e-05,
      "loss": 0.6412,
      "step": 576900
    },
    {
      "epoch": 9.058084772370487,
      "grad_norm": 3.7464683055877686,
      "learning_rate": 4.433869701726845e-05,
      "loss": 0.66,
      "step": 577000
    },
    {
      "epoch": 9.059654631083202,
      "grad_norm": 3.199612617492676,
      "learning_rate": 4.4337715855573006e-05,
      "loss": 0.6684,
      "step": 577100
    },
    {
      "epoch": 9.061224489795919,
      "grad_norm": 3.367013931274414,
      "learning_rate": 4.433673469387755e-05,
      "loss": 0.6034,
      "step": 577200
    },
    {
      "epoch": 9.062794348508634,
      "grad_norm": 3.9326789379119873,
      "learning_rate": 4.433575353218211e-05,
      "loss": 0.6425,
      "step": 577300
    },
    {
      "epoch": 9.06436420722135,
      "grad_norm": 3.6100919246673584,
      "learning_rate": 4.433477237048666e-05,
      "loss": 0.643,
      "step": 577400
    },
    {
      "epoch": 9.065934065934066,
      "grad_norm": 3.049614429473877,
      "learning_rate": 4.433379120879121e-05,
      "loss": 0.6782,
      "step": 577500
    },
    {
      "epoch": 9.067503924646783,
      "grad_norm": 2.3052597045898438,
      "learning_rate": 4.433281004709576e-05,
      "loss": 0.6817,
      "step": 577600
    },
    {
      "epoch": 9.069073783359498,
      "grad_norm": 4.475391864776611,
      "learning_rate": 4.433182888540032e-05,
      "loss": 0.6333,
      "step": 577700
    },
    {
      "epoch": 9.070643642072213,
      "grad_norm": 2.5964272022247314,
      "learning_rate": 4.433084772370487e-05,
      "loss": 0.5835,
      "step": 577800
    },
    {
      "epoch": 9.07221350078493,
      "grad_norm": 3.142853021621704,
      "learning_rate": 4.432986656200942e-05,
      "loss": 0.6449,
      "step": 577900
    },
    {
      "epoch": 9.073783359497645,
      "grad_norm": 4.380989074707031,
      "learning_rate": 4.432888540031397e-05,
      "loss": 0.6553,
      "step": 578000
    },
    {
      "epoch": 9.075353218210362,
      "grad_norm": 3.5855817794799805,
      "learning_rate": 4.432790423861853e-05,
      "loss": 0.6117,
      "step": 578100
    },
    {
      "epoch": 9.076923076923077,
      "grad_norm": 3.9184298515319824,
      "learning_rate": 4.4326923076923074e-05,
      "loss": 0.6344,
      "step": 578200
    },
    {
      "epoch": 9.078492935635794,
      "grad_norm": 3.757733106613159,
      "learning_rate": 4.432594191522763e-05,
      "loss": 0.695,
      "step": 578300
    },
    {
      "epoch": 9.080062794348509,
      "grad_norm": 3.9382376670837402,
      "learning_rate": 4.432496075353218e-05,
      "loss": 0.6543,
      "step": 578400
    },
    {
      "epoch": 9.081632653061224,
      "grad_norm": 3.0806729793548584,
      "learning_rate": 4.432397959183674e-05,
      "loss": 0.6477,
      "step": 578500
    },
    {
      "epoch": 9.08320251177394,
      "grad_norm": 3.7275636196136475,
      "learning_rate": 4.432299843014129e-05,
      "loss": 0.646,
      "step": 578600
    },
    {
      "epoch": 9.084772370486656,
      "grad_norm": 4.100196838378906,
      "learning_rate": 4.432201726844584e-05,
      "loss": 0.642,
      "step": 578700
    },
    {
      "epoch": 9.086342229199373,
      "grad_norm": 2.789552927017212,
      "learning_rate": 4.432103610675039e-05,
      "loss": 0.6587,
      "step": 578800
    },
    {
      "epoch": 9.087912087912088,
      "grad_norm": 3.5481185913085938,
      "learning_rate": 4.4320054945054944e-05,
      "loss": 0.6535,
      "step": 578900
    },
    {
      "epoch": 9.089481946624804,
      "grad_norm": 3.435828924179077,
      "learning_rate": 4.43190737833595e-05,
      "loss": 0.6657,
      "step": 579000
    },
    {
      "epoch": 9.09105180533752,
      "grad_norm": 3.923189640045166,
      "learning_rate": 4.431809262166405e-05,
      "loss": 0.6833,
      "step": 579100
    },
    {
      "epoch": 9.092621664050235,
      "grad_norm": 2.5854263305664062,
      "learning_rate": 4.431711145996861e-05,
      "loss": 0.5777,
      "step": 579200
    },
    {
      "epoch": 9.094191522762952,
      "grad_norm": 4.543500900268555,
      "learning_rate": 4.4316130298273155e-05,
      "loss": 0.6555,
      "step": 579300
    },
    {
      "epoch": 9.095761381475667,
      "grad_norm": 4.353578090667725,
      "learning_rate": 4.431514913657771e-05,
      "loss": 0.6429,
      "step": 579400
    },
    {
      "epoch": 9.097331240188383,
      "grad_norm": 3.620790958404541,
      "learning_rate": 4.4314167974882264e-05,
      "loss": 0.6186,
      "step": 579500
    },
    {
      "epoch": 9.098901098901099,
      "grad_norm": 4.02151346206665,
      "learning_rate": 4.4313186813186815e-05,
      "loss": 0.6204,
      "step": 579600
    },
    {
      "epoch": 9.100470957613815,
      "grad_norm": 3.671782970428467,
      "learning_rate": 4.4312205651491366e-05,
      "loss": 0.6155,
      "step": 579700
    },
    {
      "epoch": 9.10204081632653,
      "grad_norm": 3.921137809753418,
      "learning_rate": 4.4311224489795923e-05,
      "loss": 0.6171,
      "step": 579800
    },
    {
      "epoch": 9.103610675039246,
      "grad_norm": 3.6462562084198,
      "learning_rate": 4.4310243328100474e-05,
      "loss": 0.652,
      "step": 579900
    },
    {
      "epoch": 9.105180533751962,
      "grad_norm": 4.2654571533203125,
      "learning_rate": 4.4309262166405025e-05,
      "loss": 0.654,
      "step": 580000
    },
    {
      "epoch": 9.106750392464678,
      "grad_norm": 5.073788166046143,
      "learning_rate": 4.4308281004709576e-05,
      "loss": 0.6625,
      "step": 580100
    },
    {
      "epoch": 9.108320251177394,
      "grad_norm": 5.466825485229492,
      "learning_rate": 4.4307299843014134e-05,
      "loss": 0.594,
      "step": 580200
    },
    {
      "epoch": 9.10989010989011,
      "grad_norm": 3.111438274383545,
      "learning_rate": 4.430631868131868e-05,
      "loss": 0.6431,
      "step": 580300
    },
    {
      "epoch": 9.111459968602826,
      "grad_norm": 4.185323238372803,
      "learning_rate": 4.4305337519623236e-05,
      "loss": 0.6158,
      "step": 580400
    },
    {
      "epoch": 9.113029827315541,
      "grad_norm": 4.735772132873535,
      "learning_rate": 4.430435635792779e-05,
      "loss": 0.6535,
      "step": 580500
    },
    {
      "epoch": 9.114599686028258,
      "grad_norm": 2.921154022216797,
      "learning_rate": 4.430337519623234e-05,
      "loss": 0.592,
      "step": 580600
    },
    {
      "epoch": 9.116169544740973,
      "grad_norm": 4.527531623840332,
      "learning_rate": 4.4302394034536896e-05,
      "loss": 0.6648,
      "step": 580700
    },
    {
      "epoch": 9.117739403453688,
      "grad_norm": 3.536393404006958,
      "learning_rate": 4.430141287284145e-05,
      "loss": 0.6286,
      "step": 580800
    },
    {
      "epoch": 9.119309262166405,
      "grad_norm": 3.9863102436065674,
      "learning_rate": 4.4300431711146e-05,
      "loss": 0.6156,
      "step": 580900
    },
    {
      "epoch": 9.12087912087912,
      "grad_norm": 4.479996204376221,
      "learning_rate": 4.429945054945055e-05,
      "loss": 0.6224,
      "step": 581000
    },
    {
      "epoch": 9.122448979591837,
      "grad_norm": 4.390353679656982,
      "learning_rate": 4.4298469387755106e-05,
      "loss": 0.6385,
      "step": 581100
    },
    {
      "epoch": 9.124018838304552,
      "grad_norm": 4.810579776763916,
      "learning_rate": 4.429748822605966e-05,
      "loss": 0.6338,
      "step": 581200
    },
    {
      "epoch": 9.12558869701727,
      "grad_norm": 3.508681535720825,
      "learning_rate": 4.429650706436421e-05,
      "loss": 0.6247,
      "step": 581300
    },
    {
      "epoch": 9.127158555729984,
      "grad_norm": 4.906167030334473,
      "learning_rate": 4.429552590266876e-05,
      "loss": 0.6629,
      "step": 581400
    },
    {
      "epoch": 9.1287284144427,
      "grad_norm": 3.832740306854248,
      "learning_rate": 4.429454474097332e-05,
      "loss": 0.6358,
      "step": 581500
    },
    {
      "epoch": 9.130298273155416,
      "grad_norm": 4.1116743087768555,
      "learning_rate": 4.429356357927787e-05,
      "loss": 0.6377,
      "step": 581600
    },
    {
      "epoch": 9.131868131868131,
      "grad_norm": 2.7142512798309326,
      "learning_rate": 4.429258241758242e-05,
      "loss": 0.6345,
      "step": 581700
    },
    {
      "epoch": 9.133437990580848,
      "grad_norm": 4.510814189910889,
      "learning_rate": 4.429160125588697e-05,
      "loss": 0.6328,
      "step": 581800
    },
    {
      "epoch": 9.135007849293563,
      "grad_norm": 3.540004014968872,
      "learning_rate": 4.429062009419153e-05,
      "loss": 0.6363,
      "step": 581900
    },
    {
      "epoch": 9.13657770800628,
      "grad_norm": 4.496328353881836,
      "learning_rate": 4.428963893249607e-05,
      "loss": 0.6195,
      "step": 582000
    },
    {
      "epoch": 9.138147566718995,
      "grad_norm": 3.9383480548858643,
      "learning_rate": 4.428865777080063e-05,
      "loss": 0.6554,
      "step": 582100
    },
    {
      "epoch": 9.13971742543171,
      "grad_norm": 3.3803634643554688,
      "learning_rate": 4.428767660910518e-05,
      "loss": 0.6443,
      "step": 582200
    },
    {
      "epoch": 9.141287284144427,
      "grad_norm": 4.096658706665039,
      "learning_rate": 4.428669544740974e-05,
      "loss": 0.6483,
      "step": 582300
    },
    {
      "epoch": 9.142857142857142,
      "grad_norm": 5.154634952545166,
      "learning_rate": 4.428571428571428e-05,
      "loss": 0.6365,
      "step": 582400
    },
    {
      "epoch": 9.14442700156986,
      "grad_norm": 4.602187633514404,
      "learning_rate": 4.428473312401884e-05,
      "loss": 0.6763,
      "step": 582500
    },
    {
      "epoch": 9.145996860282574,
      "grad_norm": 2.8970136642456055,
      "learning_rate": 4.428375196232339e-05,
      "loss": 0.6444,
      "step": 582600
    },
    {
      "epoch": 9.147566718995291,
      "grad_norm": 3.7105679512023926,
      "learning_rate": 4.428277080062794e-05,
      "loss": 0.6374,
      "step": 582700
    },
    {
      "epoch": 9.149136577708006,
      "grad_norm": 4.241799354553223,
      "learning_rate": 4.42817896389325e-05,
      "loss": 0.6044,
      "step": 582800
    },
    {
      "epoch": 9.150706436420721,
      "grad_norm": 3.3854894638061523,
      "learning_rate": 4.428080847723705e-05,
      "loss": 0.6476,
      "step": 582900
    },
    {
      "epoch": 9.152276295133438,
      "grad_norm": 3.997213363647461,
      "learning_rate": 4.42798273155416e-05,
      "loss": 0.6211,
      "step": 583000
    },
    {
      "epoch": 9.153846153846153,
      "grad_norm": 3.7747035026550293,
      "learning_rate": 4.427884615384615e-05,
      "loss": 0.636,
      "step": 583100
    },
    {
      "epoch": 9.15541601255887,
      "grad_norm": 3.6283624172210693,
      "learning_rate": 4.427786499215071e-05,
      "loss": 0.6568,
      "step": 583200
    },
    {
      "epoch": 9.156985871271585,
      "grad_norm": 3.4661924839019775,
      "learning_rate": 4.427688383045526e-05,
      "loss": 0.633,
      "step": 583300
    },
    {
      "epoch": 9.158555729984302,
      "grad_norm": 3.199023723602295,
      "learning_rate": 4.427590266875981e-05,
      "loss": 0.6008,
      "step": 583400
    },
    {
      "epoch": 9.160125588697017,
      "grad_norm": 4.908023357391357,
      "learning_rate": 4.4274921507064364e-05,
      "loss": 0.6675,
      "step": 583500
    },
    {
      "epoch": 9.161695447409732,
      "grad_norm": 5.589681148529053,
      "learning_rate": 4.427394034536892e-05,
      "loss": 0.6375,
      "step": 583600
    },
    {
      "epoch": 9.16326530612245,
      "grad_norm": 4.3915510177612305,
      "learning_rate": 4.427295918367347e-05,
      "loss": 0.6115,
      "step": 583700
    },
    {
      "epoch": 9.164835164835164,
      "grad_norm": 3.582932949066162,
      "learning_rate": 4.4271978021978024e-05,
      "loss": 0.6189,
      "step": 583800
    },
    {
      "epoch": 9.166405023547881,
      "grad_norm": 4.05797004699707,
      "learning_rate": 4.4270996860282575e-05,
      "loss": 0.6565,
      "step": 583900
    },
    {
      "epoch": 9.167974882260596,
      "grad_norm": 3.935171127319336,
      "learning_rate": 4.427001569858713e-05,
      "loss": 0.6702,
      "step": 584000
    },
    {
      "epoch": 9.169544740973313,
      "grad_norm": 3.746568441390991,
      "learning_rate": 4.4269034536891677e-05,
      "loss": 0.6678,
      "step": 584100
    },
    {
      "epoch": 9.171114599686028,
      "grad_norm": 3.270693778991699,
      "learning_rate": 4.4268053375196234e-05,
      "loss": 0.6549,
      "step": 584200
    },
    {
      "epoch": 9.172684458398743,
      "grad_norm": 2.887232542037964,
      "learning_rate": 4.4267072213500785e-05,
      "loss": 0.6311,
      "step": 584300
    },
    {
      "epoch": 9.17425431711146,
      "grad_norm": 4.598021984100342,
      "learning_rate": 4.426609105180534e-05,
      "loss": 0.6065,
      "step": 584400
    },
    {
      "epoch": 9.175824175824175,
      "grad_norm": 4.405954837799072,
      "learning_rate": 4.426510989010989e-05,
      "loss": 0.6325,
      "step": 584500
    },
    {
      "epoch": 9.177394034536892,
      "grad_norm": 3.8331234455108643,
      "learning_rate": 4.4264128728414445e-05,
      "loss": 0.6196,
      "step": 584600
    },
    {
      "epoch": 9.178963893249607,
      "grad_norm": 3.9580719470977783,
      "learning_rate": 4.4263147566718996e-05,
      "loss": 0.6302,
      "step": 584700
    },
    {
      "epoch": 9.180533751962324,
      "grad_norm": 3.9733028411865234,
      "learning_rate": 4.426216640502355e-05,
      "loss": 0.6412,
      "step": 584800
    },
    {
      "epoch": 9.182103610675039,
      "grad_norm": 3.781954050064087,
      "learning_rate": 4.4261185243328105e-05,
      "loss": 0.6501,
      "step": 584900
    },
    {
      "epoch": 9.183673469387756,
      "grad_norm": 3.7287518978118896,
      "learning_rate": 4.4260204081632656e-05,
      "loss": 0.6424,
      "step": 585000
    },
    {
      "epoch": 9.185243328100471,
      "grad_norm": 3.3836824893951416,
      "learning_rate": 4.425922291993721e-05,
      "loss": 0.6276,
      "step": 585100
    },
    {
      "epoch": 9.186813186813186,
      "grad_norm": 4.3072943687438965,
      "learning_rate": 4.425824175824176e-05,
      "loss": 0.6391,
      "step": 585200
    },
    {
      "epoch": 9.188383045525903,
      "grad_norm": 3.7263095378875732,
      "learning_rate": 4.4257260596546315e-05,
      "loss": 0.6076,
      "step": 585300
    },
    {
      "epoch": 9.189952904238618,
      "grad_norm": 3.7087643146514893,
      "learning_rate": 4.4256279434850866e-05,
      "loss": 0.6343,
      "step": 585400
    },
    {
      "epoch": 9.191522762951335,
      "grad_norm": 4.009994029998779,
      "learning_rate": 4.425529827315542e-05,
      "loss": 0.6824,
      "step": 585500
    },
    {
      "epoch": 9.19309262166405,
      "grad_norm": 3.8595566749572754,
      "learning_rate": 4.425431711145997e-05,
      "loss": 0.6509,
      "step": 585600
    },
    {
      "epoch": 9.194662480376767,
      "grad_norm": 4.034994602203369,
      "learning_rate": 4.4253335949764526e-05,
      "loss": 0.6091,
      "step": 585700
    },
    {
      "epoch": 9.196232339089482,
      "grad_norm": 2.504563093185425,
      "learning_rate": 4.425235478806908e-05,
      "loss": 0.6368,
      "step": 585800
    },
    {
      "epoch": 9.197802197802197,
      "grad_norm": 3.815308094024658,
      "learning_rate": 4.425137362637363e-05,
      "loss": 0.6553,
      "step": 585900
    },
    {
      "epoch": 9.199372056514914,
      "grad_norm": 3.687608003616333,
      "learning_rate": 4.425039246467818e-05,
      "loss": 0.6093,
      "step": 586000
    },
    {
      "epoch": 9.200941915227629,
      "grad_norm": 4.259789943695068,
      "learning_rate": 4.424941130298274e-05,
      "loss": 0.6391,
      "step": 586100
    },
    {
      "epoch": 9.202511773940346,
      "grad_norm": 3.152130365371704,
      "learning_rate": 4.424843014128728e-05,
      "loss": 0.6085,
      "step": 586200
    },
    {
      "epoch": 9.204081632653061,
      "grad_norm": 4.172093391418457,
      "learning_rate": 4.424744897959184e-05,
      "loss": 0.6764,
      "step": 586300
    },
    {
      "epoch": 9.205651491365778,
      "grad_norm": 2.795974016189575,
      "learning_rate": 4.424646781789639e-05,
      "loss": 0.6775,
      "step": 586400
    },
    {
      "epoch": 9.207221350078493,
      "grad_norm": 3.758168935775757,
      "learning_rate": 4.424548665620095e-05,
      "loss": 0.6672,
      "step": 586500
    },
    {
      "epoch": 9.208791208791208,
      "grad_norm": 4.5814528465271,
      "learning_rate": 4.424450549450549e-05,
      "loss": 0.6449,
      "step": 586600
    },
    {
      "epoch": 9.210361067503925,
      "grad_norm": 4.551886081695557,
      "learning_rate": 4.424352433281005e-05,
      "loss": 0.6069,
      "step": 586700
    },
    {
      "epoch": 9.21193092621664,
      "grad_norm": 3.4766194820404053,
      "learning_rate": 4.42425431711146e-05,
      "loss": 0.6402,
      "step": 586800
    },
    {
      "epoch": 9.213500784929357,
      "grad_norm": 4.071006774902344,
      "learning_rate": 4.424156200941915e-05,
      "loss": 0.659,
      "step": 586900
    },
    {
      "epoch": 9.215070643642072,
      "grad_norm": 3.8031697273254395,
      "learning_rate": 4.424058084772371e-05,
      "loss": 0.648,
      "step": 587000
    },
    {
      "epoch": 9.216640502354789,
      "grad_norm": 4.049775123596191,
      "learning_rate": 4.423959968602826e-05,
      "loss": 0.5889,
      "step": 587100
    },
    {
      "epoch": 9.218210361067504,
      "grad_norm": 3.0793137550354004,
      "learning_rate": 4.423861852433281e-05,
      "loss": 0.6296,
      "step": 587200
    },
    {
      "epoch": 9.219780219780219,
      "grad_norm": 4.115381240844727,
      "learning_rate": 4.423763736263736e-05,
      "loss": 0.6748,
      "step": 587300
    },
    {
      "epoch": 9.221350078492936,
      "grad_norm": 4.566145896911621,
      "learning_rate": 4.423665620094192e-05,
      "loss": 0.6546,
      "step": 587400
    },
    {
      "epoch": 9.222919937205651,
      "grad_norm": 3.3827717304229736,
      "learning_rate": 4.423567503924647e-05,
      "loss": 0.6406,
      "step": 587500
    },
    {
      "epoch": 9.224489795918368,
      "grad_norm": 4.33163595199585,
      "learning_rate": 4.423469387755102e-05,
      "loss": 0.6185,
      "step": 587600
    },
    {
      "epoch": 9.226059654631083,
      "grad_norm": 3.9895529747009277,
      "learning_rate": 4.423371271585557e-05,
      "loss": 0.6711,
      "step": 587700
    },
    {
      "epoch": 9.2276295133438,
      "grad_norm": 3.9664556980133057,
      "learning_rate": 4.423273155416013e-05,
      "loss": 0.6548,
      "step": 587800
    },
    {
      "epoch": 9.229199372056515,
      "grad_norm": 3.4540529251098633,
      "learning_rate": 4.423175039246468e-05,
      "loss": 0.6179,
      "step": 587900
    },
    {
      "epoch": 9.23076923076923,
      "grad_norm": 5.192612648010254,
      "learning_rate": 4.423076923076923e-05,
      "loss": 0.6587,
      "step": 588000
    },
    {
      "epoch": 9.232339089481947,
      "grad_norm": 4.527181625366211,
      "learning_rate": 4.4229788069073784e-05,
      "loss": 0.6138,
      "step": 588100
    },
    {
      "epoch": 9.233908948194662,
      "grad_norm": 4.177854061126709,
      "learning_rate": 4.422880690737834e-05,
      "loss": 0.5989,
      "step": 588200
    },
    {
      "epoch": 9.235478806907379,
      "grad_norm": 4.032135486602783,
      "learning_rate": 4.4227825745682886e-05,
      "loss": 0.6182,
      "step": 588300
    },
    {
      "epoch": 9.237048665620094,
      "grad_norm": 3.535625696182251,
      "learning_rate": 4.422684458398744e-05,
      "loss": 0.6583,
      "step": 588400
    },
    {
      "epoch": 9.23861852433281,
      "grad_norm": 3.870335102081299,
      "learning_rate": 4.4225863422291994e-05,
      "loss": 0.6528,
      "step": 588500
    },
    {
      "epoch": 9.240188383045526,
      "grad_norm": 3.6104390621185303,
      "learning_rate": 4.422488226059655e-05,
      "loss": 0.6759,
      "step": 588600
    },
    {
      "epoch": 9.241758241758241,
      "grad_norm": 3.3881852626800537,
      "learning_rate": 4.4223901098901096e-05,
      "loss": 0.6001,
      "step": 588700
    },
    {
      "epoch": 9.243328100470958,
      "grad_norm": 3.1807854175567627,
      "learning_rate": 4.4222919937205654e-05,
      "loss": 0.6037,
      "step": 588800
    },
    {
      "epoch": 9.244897959183673,
      "grad_norm": 4.309719085693359,
      "learning_rate": 4.4221938775510205e-05,
      "loss": 0.6714,
      "step": 588900
    },
    {
      "epoch": 9.24646781789639,
      "grad_norm": 3.770596981048584,
      "learning_rate": 4.4220957613814756e-05,
      "loss": 0.6758,
      "step": 589000
    },
    {
      "epoch": 9.248037676609105,
      "grad_norm": 4.336422443389893,
      "learning_rate": 4.4219976452119314e-05,
      "loss": 0.6656,
      "step": 589100
    },
    {
      "epoch": 9.249607535321822,
      "grad_norm": 4.355809211730957,
      "learning_rate": 4.4218995290423865e-05,
      "loss": 0.6177,
      "step": 589200
    },
    {
      "epoch": 9.251177394034537,
      "grad_norm": 3.6480636596679688,
      "learning_rate": 4.4218014128728416e-05,
      "loss": 0.6712,
      "step": 589300
    },
    {
      "epoch": 9.252747252747252,
      "grad_norm": 4.0236616134643555,
      "learning_rate": 4.421703296703297e-05,
      "loss": 0.6317,
      "step": 589400
    },
    {
      "epoch": 9.254317111459969,
      "grad_norm": 2.7342042922973633,
      "learning_rate": 4.4216051805337524e-05,
      "loss": 0.5933,
      "step": 589500
    },
    {
      "epoch": 9.255886970172684,
      "grad_norm": 3.6087124347686768,
      "learning_rate": 4.4215070643642075e-05,
      "loss": 0.6339,
      "step": 589600
    },
    {
      "epoch": 9.2574568288854,
      "grad_norm": 4.4313249588012695,
      "learning_rate": 4.4214089481946626e-05,
      "loss": 0.6652,
      "step": 589700
    },
    {
      "epoch": 9.259026687598116,
      "grad_norm": 4.117419242858887,
      "learning_rate": 4.421310832025118e-05,
      "loss": 0.6333,
      "step": 589800
    },
    {
      "epoch": 9.260596546310833,
      "grad_norm": 3.2541580200195312,
      "learning_rate": 4.4212127158555735e-05,
      "loss": 0.6229,
      "step": 589900
    },
    {
      "epoch": 9.262166405023548,
      "grad_norm": 3.57515025138855,
      "learning_rate": 4.4211145996860286e-05,
      "loss": 0.6436,
      "step": 590000
    },
    {
      "epoch": 9.263736263736265,
      "grad_norm": 4.1465983390808105,
      "learning_rate": 4.421016483516484e-05,
      "loss": 0.6441,
      "step": 590100
    },
    {
      "epoch": 9.26530612244898,
      "grad_norm": 2.7372965812683105,
      "learning_rate": 4.420918367346939e-05,
      "loss": 0.6377,
      "step": 590200
    },
    {
      "epoch": 9.266875981161695,
      "grad_norm": 3.8687496185302734,
      "learning_rate": 4.4208202511773946e-05,
      "loss": 0.6586,
      "step": 590300
    },
    {
      "epoch": 9.268445839874412,
      "grad_norm": 3.121110439300537,
      "learning_rate": 4.420722135007849e-05,
      "loss": 0.5939,
      "step": 590400
    },
    {
      "epoch": 9.270015698587127,
      "grad_norm": 3.453235387802124,
      "learning_rate": 4.420624018838305e-05,
      "loss": 0.623,
      "step": 590500
    },
    {
      "epoch": 9.271585557299844,
      "grad_norm": 3.8904881477355957,
      "learning_rate": 4.42052590266876e-05,
      "loss": 0.7136,
      "step": 590600
    },
    {
      "epoch": 9.273155416012559,
      "grad_norm": 3.793118953704834,
      "learning_rate": 4.4204277864992157e-05,
      "loss": 0.6609,
      "step": 590700
    },
    {
      "epoch": 9.274725274725276,
      "grad_norm": 3.9639577865600586,
      "learning_rate": 4.42032967032967e-05,
      "loss": 0.6711,
      "step": 590800
    },
    {
      "epoch": 9.27629513343799,
      "grad_norm": 4.213755130767822,
      "learning_rate": 4.420231554160126e-05,
      "loss": 0.641,
      "step": 590900
    },
    {
      "epoch": 9.277864992150706,
      "grad_norm": 4.7479424476623535,
      "learning_rate": 4.420133437990581e-05,
      "loss": 0.6282,
      "step": 591000
    },
    {
      "epoch": 9.279434850863423,
      "grad_norm": 4.590247631072998,
      "learning_rate": 4.420035321821036e-05,
      "loss": 0.6426,
      "step": 591100
    },
    {
      "epoch": 9.281004709576138,
      "grad_norm": 4.038216590881348,
      "learning_rate": 4.419937205651492e-05,
      "loss": 0.6344,
      "step": 591200
    },
    {
      "epoch": 9.282574568288855,
      "grad_norm": 3.521454334259033,
      "learning_rate": 4.419839089481947e-05,
      "loss": 0.6135,
      "step": 591300
    },
    {
      "epoch": 9.28414442700157,
      "grad_norm": 2.988018274307251,
      "learning_rate": 4.419740973312402e-05,
      "loss": 0.638,
      "step": 591400
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 3.609668016433716,
      "learning_rate": 4.419642857142857e-05,
      "loss": 0.6424,
      "step": 591500
    },
    {
      "epoch": 9.287284144427002,
      "grad_norm": 3.1048426628112793,
      "learning_rate": 4.419544740973313e-05,
      "loss": 0.6645,
      "step": 591600
    },
    {
      "epoch": 9.288854003139717,
      "grad_norm": 4.109087944030762,
      "learning_rate": 4.419446624803768e-05,
      "loss": 0.6527,
      "step": 591700
    },
    {
      "epoch": 9.290423861852434,
      "grad_norm": 4.419119358062744,
      "learning_rate": 4.419348508634223e-05,
      "loss": 0.6128,
      "step": 591800
    },
    {
      "epoch": 9.291993720565149,
      "grad_norm": 5.099108695983887,
      "learning_rate": 4.419250392464678e-05,
      "loss": 0.6731,
      "step": 591900
    },
    {
      "epoch": 9.293563579277865,
      "grad_norm": 5.01422119140625,
      "learning_rate": 4.419152276295134e-05,
      "loss": 0.6038,
      "step": 592000
    },
    {
      "epoch": 9.29513343799058,
      "grad_norm": 3.659001111984253,
      "learning_rate": 4.419054160125589e-05,
      "loss": 0.6864,
      "step": 592100
    },
    {
      "epoch": 9.296703296703297,
      "grad_norm": 3.8144779205322266,
      "learning_rate": 4.418956043956044e-05,
      "loss": 0.6623,
      "step": 592200
    },
    {
      "epoch": 9.298273155416013,
      "grad_norm": 2.8905484676361084,
      "learning_rate": 4.418857927786499e-05,
      "loss": 0.6506,
      "step": 592300
    },
    {
      "epoch": 9.299843014128728,
      "grad_norm": 4.109013080596924,
      "learning_rate": 4.418759811616955e-05,
      "loss": 0.6387,
      "step": 592400
    },
    {
      "epoch": 9.301412872841444,
      "grad_norm": 4.849026203155518,
      "learning_rate": 4.4186616954474095e-05,
      "loss": 0.6467,
      "step": 592500
    },
    {
      "epoch": 9.30298273155416,
      "grad_norm": 2.895051956176758,
      "learning_rate": 4.418563579277865e-05,
      "loss": 0.6075,
      "step": 592600
    },
    {
      "epoch": 9.304552590266876,
      "grad_norm": 4.290606498718262,
      "learning_rate": 4.41846546310832e-05,
      "loss": 0.6558,
      "step": 592700
    },
    {
      "epoch": 9.306122448979592,
      "grad_norm": 4.744297981262207,
      "learning_rate": 4.418367346938776e-05,
      "loss": 0.6367,
      "step": 592800
    },
    {
      "epoch": 9.307692307692308,
      "grad_norm": 2.665552854537964,
      "learning_rate": 4.4182692307692305e-05,
      "loss": 0.6124,
      "step": 592900
    },
    {
      "epoch": 9.309262166405023,
      "grad_norm": 4.51582670211792,
      "learning_rate": 4.418171114599686e-05,
      "loss": 0.6897,
      "step": 593000
    },
    {
      "epoch": 9.310832025117739,
      "grad_norm": 4.307895660400391,
      "learning_rate": 4.4180729984301414e-05,
      "loss": 0.6881,
      "step": 593100
    },
    {
      "epoch": 9.312401883830455,
      "grad_norm": 3.791788339614868,
      "learning_rate": 4.4179748822605965e-05,
      "loss": 0.623,
      "step": 593200
    },
    {
      "epoch": 9.31397174254317,
      "grad_norm": 3.4744889736175537,
      "learning_rate": 4.417876766091052e-05,
      "loss": 0.6687,
      "step": 593300
    },
    {
      "epoch": 9.315541601255887,
      "grad_norm": 3.5325849056243896,
      "learning_rate": 4.4177786499215074e-05,
      "loss": 0.6484,
      "step": 593400
    },
    {
      "epoch": 9.317111459968602,
      "grad_norm": 3.257935047149658,
      "learning_rate": 4.4176805337519625e-05,
      "loss": 0.603,
      "step": 593500
    },
    {
      "epoch": 9.31868131868132,
      "grad_norm": 4.378543376922607,
      "learning_rate": 4.4175824175824176e-05,
      "loss": 0.6253,
      "step": 593600
    },
    {
      "epoch": 9.320251177394034,
      "grad_norm": 3.868880271911621,
      "learning_rate": 4.4174843014128733e-05,
      "loss": 0.6136,
      "step": 593700
    },
    {
      "epoch": 9.321821036106751,
      "grad_norm": 4.728415012359619,
      "learning_rate": 4.4173861852433284e-05,
      "loss": 0.6634,
      "step": 593800
    },
    {
      "epoch": 9.323390894819466,
      "grad_norm": 4.377893447875977,
      "learning_rate": 4.4172880690737835e-05,
      "loss": 0.6298,
      "step": 593900
    },
    {
      "epoch": 9.324960753532181,
      "grad_norm": 4.629810810089111,
      "learning_rate": 4.4171899529042386e-05,
      "loss": 0.6597,
      "step": 594000
    },
    {
      "epoch": 9.326530612244898,
      "grad_norm": 4.41800594329834,
      "learning_rate": 4.4170918367346944e-05,
      "loss": 0.6649,
      "step": 594100
    },
    {
      "epoch": 9.328100470957613,
      "grad_norm": 2.476161479949951,
      "learning_rate": 4.4169937205651495e-05,
      "loss": 0.6556,
      "step": 594200
    },
    {
      "epoch": 9.32967032967033,
      "grad_norm": 4.154817581176758,
      "learning_rate": 4.4168956043956046e-05,
      "loss": 0.6498,
      "step": 594300
    },
    {
      "epoch": 9.331240188383045,
      "grad_norm": 3.591801643371582,
      "learning_rate": 4.41679748822606e-05,
      "loss": 0.6306,
      "step": 594400
    },
    {
      "epoch": 9.332810047095762,
      "grad_norm": 4.7201457023620605,
      "learning_rate": 4.4166993720565155e-05,
      "loss": 0.6395,
      "step": 594500
    },
    {
      "epoch": 9.334379905808477,
      "grad_norm": 3.3911726474761963,
      "learning_rate": 4.41660125588697e-05,
      "loss": 0.6634,
      "step": 594600
    },
    {
      "epoch": 9.335949764521192,
      "grad_norm": 4.0590434074401855,
      "learning_rate": 4.416503139717426e-05,
      "loss": 0.6384,
      "step": 594700
    },
    {
      "epoch": 9.33751962323391,
      "grad_norm": 2.976907730102539,
      "learning_rate": 4.416405023547881e-05,
      "loss": 0.6038,
      "step": 594800
    },
    {
      "epoch": 9.339089481946624,
      "grad_norm": 3.809500217437744,
      "learning_rate": 4.4163069073783366e-05,
      "loss": 0.6226,
      "step": 594900
    },
    {
      "epoch": 9.340659340659341,
      "grad_norm": 3.5622153282165527,
      "learning_rate": 4.416208791208791e-05,
      "loss": 0.6427,
      "step": 595000
    },
    {
      "epoch": 9.342229199372056,
      "grad_norm": 4.349527359008789,
      "learning_rate": 4.416110675039247e-05,
      "loss": 0.6353,
      "step": 595100
    },
    {
      "epoch": 9.343799058084773,
      "grad_norm": 3.2673099040985107,
      "learning_rate": 4.416012558869702e-05,
      "loss": 0.6235,
      "step": 595200
    },
    {
      "epoch": 9.345368916797488,
      "grad_norm": 3.7013909816741943,
      "learning_rate": 4.415914442700157e-05,
      "loss": 0.6295,
      "step": 595300
    },
    {
      "epoch": 9.346938775510203,
      "grad_norm": 4.1130051612854,
      "learning_rate": 4.415816326530613e-05,
      "loss": 0.6394,
      "step": 595400
    },
    {
      "epoch": 9.34850863422292,
      "grad_norm": 4.672569274902344,
      "learning_rate": 4.415718210361068e-05,
      "loss": 0.6276,
      "step": 595500
    },
    {
      "epoch": 9.350078492935635,
      "grad_norm": 4.372800827026367,
      "learning_rate": 4.415620094191523e-05,
      "loss": 0.6814,
      "step": 595600
    },
    {
      "epoch": 9.351648351648352,
      "grad_norm": 3.7609407901763916,
      "learning_rate": 4.415521978021978e-05,
      "loss": 0.6591,
      "step": 595700
    },
    {
      "epoch": 9.353218210361067,
      "grad_norm": 3.4446377754211426,
      "learning_rate": 4.415423861852434e-05,
      "loss": 0.659,
      "step": 595800
    },
    {
      "epoch": 9.354788069073784,
      "grad_norm": 4.080102920532227,
      "learning_rate": 4.415325745682889e-05,
      "loss": 0.6365,
      "step": 595900
    },
    {
      "epoch": 9.3563579277865,
      "grad_norm": 3.762996196746826,
      "learning_rate": 4.415227629513344e-05,
      "loss": 0.6459,
      "step": 596000
    },
    {
      "epoch": 9.357927786499214,
      "grad_norm": 4.621415615081787,
      "learning_rate": 4.415129513343799e-05,
      "loss": 0.6027,
      "step": 596100
    },
    {
      "epoch": 9.359497645211931,
      "grad_norm": 3.4900779724121094,
      "learning_rate": 4.415031397174255e-05,
      "loss": 0.6526,
      "step": 596200
    },
    {
      "epoch": 9.361067503924646,
      "grad_norm": 3.661271095275879,
      "learning_rate": 4.41493328100471e-05,
      "loss": 0.6413,
      "step": 596300
    },
    {
      "epoch": 9.362637362637363,
      "grad_norm": 2.6850390434265137,
      "learning_rate": 4.414835164835165e-05,
      "loss": 0.6107,
      "step": 596400
    },
    {
      "epoch": 9.364207221350078,
      "grad_norm": 3.5140950679779053,
      "learning_rate": 4.41473704866562e-05,
      "loss": 0.6371,
      "step": 596500
    },
    {
      "epoch": 9.365777080062795,
      "grad_norm": 3.8707387447357178,
      "learning_rate": 4.414638932496076e-05,
      "loss": 0.6489,
      "step": 596600
    },
    {
      "epoch": 9.36734693877551,
      "grad_norm": 3.643120288848877,
      "learning_rate": 4.4145408163265304e-05,
      "loss": 0.6065,
      "step": 596700
    },
    {
      "epoch": 9.368916797488225,
      "grad_norm": 4.229783058166504,
      "learning_rate": 4.414442700156986e-05,
      "loss": 0.6333,
      "step": 596800
    },
    {
      "epoch": 9.370486656200942,
      "grad_norm": 2.560737371444702,
      "learning_rate": 4.414344583987441e-05,
      "loss": 0.6786,
      "step": 596900
    },
    {
      "epoch": 9.372056514913657,
      "grad_norm": 3.643693685531616,
      "learning_rate": 4.414246467817897e-05,
      "loss": 0.6304,
      "step": 597000
    },
    {
      "epoch": 9.373626373626374,
      "grad_norm": 2.7599871158599854,
      "learning_rate": 4.4141483516483514e-05,
      "loss": 0.6397,
      "step": 597100
    },
    {
      "epoch": 9.37519623233909,
      "grad_norm": 5.0786452293396,
      "learning_rate": 4.414050235478807e-05,
      "loss": 0.6288,
      "step": 597200
    },
    {
      "epoch": 9.376766091051806,
      "grad_norm": 4.0391764640808105,
      "learning_rate": 4.413952119309262e-05,
      "loss": 0.6348,
      "step": 597300
    },
    {
      "epoch": 9.378335949764521,
      "grad_norm": 2.955796003341675,
      "learning_rate": 4.4138540031397174e-05,
      "loss": 0.6627,
      "step": 597400
    },
    {
      "epoch": 9.379905808477236,
      "grad_norm": 4.409688949584961,
      "learning_rate": 4.413755886970173e-05,
      "loss": 0.6557,
      "step": 597500
    },
    {
      "epoch": 9.381475667189953,
      "grad_norm": 3.6578803062438965,
      "learning_rate": 4.413657770800628e-05,
      "loss": 0.6268,
      "step": 597600
    },
    {
      "epoch": 9.383045525902668,
      "grad_norm": 2.2535903453826904,
      "learning_rate": 4.4135596546310834e-05,
      "loss": 0.6032,
      "step": 597700
    },
    {
      "epoch": 9.384615384615385,
      "grad_norm": 4.629758834838867,
      "learning_rate": 4.4134615384615385e-05,
      "loss": 0.6654,
      "step": 597800
    },
    {
      "epoch": 9.3861852433281,
      "grad_norm": 4.069544315338135,
      "learning_rate": 4.413363422291994e-05,
      "loss": 0.597,
      "step": 597900
    },
    {
      "epoch": 9.387755102040817,
      "grad_norm": 4.547878265380859,
      "learning_rate": 4.4132653061224493e-05,
      "loss": 0.629,
      "step": 598000
    },
    {
      "epoch": 9.389324960753532,
      "grad_norm": 3.1322526931762695,
      "learning_rate": 4.4131671899529044e-05,
      "loss": 0.6316,
      "step": 598100
    },
    {
      "epoch": 9.390894819466247,
      "grad_norm": 3.8272156715393066,
      "learning_rate": 4.4130690737833595e-05,
      "loss": 0.596,
      "step": 598200
    },
    {
      "epoch": 9.392464678178964,
      "grad_norm": 3.565558910369873,
      "learning_rate": 4.412970957613815e-05,
      "loss": 0.6374,
      "step": 598300
    },
    {
      "epoch": 9.394034536891679,
      "grad_norm": 3.316209316253662,
      "learning_rate": 4.4128728414442704e-05,
      "loss": 0.6719,
      "step": 598400
    },
    {
      "epoch": 9.395604395604396,
      "grad_norm": 2.660983085632324,
      "learning_rate": 4.4127747252747255e-05,
      "loss": 0.6582,
      "step": 598500
    },
    {
      "epoch": 9.397174254317111,
      "grad_norm": 4.033944606781006,
      "learning_rate": 4.4126766091051806e-05,
      "loss": 0.6367,
      "step": 598600
    },
    {
      "epoch": 9.398744113029828,
      "grad_norm": 3.8580803871154785,
      "learning_rate": 4.4125784929356364e-05,
      "loss": 0.6237,
      "step": 598700
    },
    {
      "epoch": 9.400313971742543,
      "grad_norm": 3.7990174293518066,
      "learning_rate": 4.412480376766091e-05,
      "loss": 0.6189,
      "step": 598800
    },
    {
      "epoch": 9.40188383045526,
      "grad_norm": 4.24904203414917,
      "learning_rate": 4.4123822605965466e-05,
      "loss": 0.6301,
      "step": 598900
    },
    {
      "epoch": 9.403453689167975,
      "grad_norm": 2.98219895362854,
      "learning_rate": 4.412284144427002e-05,
      "loss": 0.6059,
      "step": 599000
    },
    {
      "epoch": 9.40502354788069,
      "grad_norm": 3.6485838890075684,
      "learning_rate": 4.4121860282574575e-05,
      "loss": 0.6448,
      "step": 599100
    },
    {
      "epoch": 9.406593406593407,
      "grad_norm": 2.9223122596740723,
      "learning_rate": 4.412087912087912e-05,
      "loss": 0.6374,
      "step": 599200
    },
    {
      "epoch": 9.408163265306122,
      "grad_norm": 4.154269695281982,
      "learning_rate": 4.4119897959183676e-05,
      "loss": 0.6872,
      "step": 599300
    },
    {
      "epoch": 9.409733124018839,
      "grad_norm": 3.3387017250061035,
      "learning_rate": 4.411891679748823e-05,
      "loss": 0.6485,
      "step": 599400
    },
    {
      "epoch": 9.411302982731554,
      "grad_norm": 4.264563083648682,
      "learning_rate": 4.411793563579278e-05,
      "loss": 0.6345,
      "step": 599500
    },
    {
      "epoch": 9.41287284144427,
      "grad_norm": 6.306660175323486,
      "learning_rate": 4.4116954474097336e-05,
      "loss": 0.6124,
      "step": 599600
    },
    {
      "epoch": 9.414442700156986,
      "grad_norm": 4.512608528137207,
      "learning_rate": 4.411597331240189e-05,
      "loss": 0.629,
      "step": 599700
    },
    {
      "epoch": 9.416012558869701,
      "grad_norm": 3.7589120864868164,
      "learning_rate": 4.411499215070644e-05,
      "loss": 0.593,
      "step": 599800
    },
    {
      "epoch": 9.417582417582418,
      "grad_norm": 3.6926684379577637,
      "learning_rate": 4.411401098901099e-05,
      "loss": 0.6489,
      "step": 599900
    },
    {
      "epoch": 9.419152276295133,
      "grad_norm": 3.883150100708008,
      "learning_rate": 4.411302982731555e-05,
      "loss": 0.6542,
      "step": 600000
    },
    {
      "epoch": 9.42072213500785,
      "grad_norm": 4.611459255218506,
      "learning_rate": 4.41120486656201e-05,
      "loss": 0.5995,
      "step": 600100
    },
    {
      "epoch": 9.422291993720565,
      "grad_norm": 4.067291736602783,
      "learning_rate": 4.411106750392465e-05,
      "loss": 0.6202,
      "step": 600200
    },
    {
      "epoch": 9.423861852433282,
      "grad_norm": 4.2933526039123535,
      "learning_rate": 4.41100863422292e-05,
      "loss": 0.6459,
      "step": 600300
    },
    {
      "epoch": 9.425431711145997,
      "grad_norm": 3.2754666805267334,
      "learning_rate": 4.410910518053376e-05,
      "loss": 0.6268,
      "step": 600400
    },
    {
      "epoch": 9.427001569858712,
      "grad_norm": 4.377653121948242,
      "learning_rate": 4.410812401883831e-05,
      "loss": 0.6354,
      "step": 600500
    },
    {
      "epoch": 9.428571428571429,
      "grad_norm": 4.487782955169678,
      "learning_rate": 4.410714285714286e-05,
      "loss": 0.6371,
      "step": 600600
    },
    {
      "epoch": 9.430141287284144,
      "grad_norm": 3.976480484008789,
      "learning_rate": 4.410616169544741e-05,
      "loss": 0.6369,
      "step": 600700
    },
    {
      "epoch": 9.43171114599686,
      "grad_norm": 4.3702239990234375,
      "learning_rate": 4.410518053375197e-05,
      "loss": 0.6052,
      "step": 600800
    },
    {
      "epoch": 9.433281004709576,
      "grad_norm": 2.977355480194092,
      "learning_rate": 4.410419937205651e-05,
      "loss": 0.6214,
      "step": 600900
    },
    {
      "epoch": 9.434850863422293,
      "grad_norm": 2.788788318634033,
      "learning_rate": 4.410321821036107e-05,
      "loss": 0.6047,
      "step": 601000
    },
    {
      "epoch": 9.436420722135008,
      "grad_norm": 6.148041725158691,
      "learning_rate": 4.410223704866562e-05,
      "loss": 0.6268,
      "step": 601100
    },
    {
      "epoch": 9.437990580847723,
      "grad_norm": 4.460382461547852,
      "learning_rate": 4.410125588697018e-05,
      "loss": 0.6137,
      "step": 601200
    },
    {
      "epoch": 9.43956043956044,
      "grad_norm": 4.005179405212402,
      "learning_rate": 4.410027472527472e-05,
      "loss": 0.64,
      "step": 601300
    },
    {
      "epoch": 9.441130298273155,
      "grad_norm": 4.5853681564331055,
      "learning_rate": 4.409929356357928e-05,
      "loss": 0.6674,
      "step": 601400
    },
    {
      "epoch": 9.442700156985872,
      "grad_norm": 3.440518617630005,
      "learning_rate": 4.409831240188383e-05,
      "loss": 0.6281,
      "step": 601500
    },
    {
      "epoch": 9.444270015698587,
      "grad_norm": 3.1912131309509277,
      "learning_rate": 4.409733124018838e-05,
      "loss": 0.628,
      "step": 601600
    },
    {
      "epoch": 9.445839874411304,
      "grad_norm": 3.334181547164917,
      "learning_rate": 4.409635007849294e-05,
      "loss": 0.6166,
      "step": 601700
    },
    {
      "epoch": 9.447409733124019,
      "grad_norm": 3.983874797821045,
      "learning_rate": 4.409536891679749e-05,
      "loss": 0.6071,
      "step": 601800
    },
    {
      "epoch": 9.448979591836734,
      "grad_norm": 3.475761651992798,
      "learning_rate": 4.409438775510204e-05,
      "loss": 0.6525,
      "step": 601900
    },
    {
      "epoch": 9.45054945054945,
      "grad_norm": 4.376272678375244,
      "learning_rate": 4.4093406593406594e-05,
      "loss": 0.6477,
      "step": 602000
    },
    {
      "epoch": 9.452119309262166,
      "grad_norm": 2.7588448524475098,
      "learning_rate": 4.409242543171115e-05,
      "loss": 0.6138,
      "step": 602100
    },
    {
      "epoch": 9.453689167974883,
      "grad_norm": 3.9985218048095703,
      "learning_rate": 4.40914442700157e-05,
      "loss": 0.6953,
      "step": 602200
    },
    {
      "epoch": 9.455259026687598,
      "grad_norm": 5.083483695983887,
      "learning_rate": 4.409046310832025e-05,
      "loss": 0.6759,
      "step": 602300
    },
    {
      "epoch": 9.456828885400315,
      "grad_norm": 4.320095539093018,
      "learning_rate": 4.4089481946624804e-05,
      "loss": 0.6298,
      "step": 602400
    },
    {
      "epoch": 9.45839874411303,
      "grad_norm": 2.9164180755615234,
      "learning_rate": 4.408850078492936e-05,
      "loss": 0.6303,
      "step": 602500
    },
    {
      "epoch": 9.459968602825747,
      "grad_norm": 3.143343687057495,
      "learning_rate": 4.408751962323391e-05,
      "loss": 0.632,
      "step": 602600
    },
    {
      "epoch": 9.461538461538462,
      "grad_norm": 3.925668716430664,
      "learning_rate": 4.4086538461538464e-05,
      "loss": 0.5935,
      "step": 602700
    },
    {
      "epoch": 9.463108320251177,
      "grad_norm": 4.510740756988525,
      "learning_rate": 4.4085557299843015e-05,
      "loss": 0.6228,
      "step": 602800
    },
    {
      "epoch": 9.464678178963894,
      "grad_norm": 4.46993350982666,
      "learning_rate": 4.408457613814757e-05,
      "loss": 0.6533,
      "step": 602900
    },
    {
      "epoch": 9.466248037676609,
      "grad_norm": 3.6157405376434326,
      "learning_rate": 4.408359497645212e-05,
      "loss": 0.615,
      "step": 603000
    },
    {
      "epoch": 9.467817896389326,
      "grad_norm": 3.0190327167510986,
      "learning_rate": 4.4082613814756675e-05,
      "loss": 0.5924,
      "step": 603100
    },
    {
      "epoch": 9.46938775510204,
      "grad_norm": 3.1236438751220703,
      "learning_rate": 4.4081632653061226e-05,
      "loss": 0.6106,
      "step": 603200
    },
    {
      "epoch": 9.470957613814758,
      "grad_norm": 3.980372428894043,
      "learning_rate": 4.408065149136578e-05,
      "loss": 0.6231,
      "step": 603300
    },
    {
      "epoch": 9.472527472527473,
      "grad_norm": 4.925960063934326,
      "learning_rate": 4.407967032967033e-05,
      "loss": 0.6667,
      "step": 603400
    },
    {
      "epoch": 9.474097331240188,
      "grad_norm": 3.6862778663635254,
      "learning_rate": 4.4078689167974885e-05,
      "loss": 0.6378,
      "step": 603500
    },
    {
      "epoch": 9.475667189952905,
      "grad_norm": 4.27631950378418,
      "learning_rate": 4.4077708006279436e-05,
      "loss": 0.6822,
      "step": 603600
    },
    {
      "epoch": 9.47723704866562,
      "grad_norm": 4.539275646209717,
      "learning_rate": 4.407672684458399e-05,
      "loss": 0.6563,
      "step": 603700
    },
    {
      "epoch": 9.478806907378337,
      "grad_norm": 4.516127586364746,
      "learning_rate": 4.407574568288854e-05,
      "loss": 0.5924,
      "step": 603800
    },
    {
      "epoch": 9.480376766091052,
      "grad_norm": 3.5561864376068115,
      "learning_rate": 4.4074764521193096e-05,
      "loss": 0.6177,
      "step": 603900
    },
    {
      "epoch": 9.481946624803768,
      "grad_norm": 2.8851916790008545,
      "learning_rate": 4.407378335949765e-05,
      "loss": 0.6308,
      "step": 604000
    },
    {
      "epoch": 9.483516483516484,
      "grad_norm": 4.185103416442871,
      "learning_rate": 4.40728021978022e-05,
      "loss": 0.6581,
      "step": 604100
    },
    {
      "epoch": 9.485086342229199,
      "grad_norm": 4.273683547973633,
      "learning_rate": 4.4071821036106756e-05,
      "loss": 0.6433,
      "step": 604200
    },
    {
      "epoch": 9.486656200941916,
      "grad_norm": 3.4504380226135254,
      "learning_rate": 4.407083987441131e-05,
      "loss": 0.625,
      "step": 604300
    },
    {
      "epoch": 9.48822605965463,
      "grad_norm": 4.039930820465088,
      "learning_rate": 4.406985871271586e-05,
      "loss": 0.6049,
      "step": 604400
    },
    {
      "epoch": 9.489795918367347,
      "grad_norm": 4.281076908111572,
      "learning_rate": 4.406887755102041e-05,
      "loss": 0.6428,
      "step": 604500
    },
    {
      "epoch": 9.491365777080063,
      "grad_norm": 3.3098504543304443,
      "learning_rate": 4.4067896389324967e-05,
      "loss": 0.6074,
      "step": 604600
    },
    {
      "epoch": 9.49293563579278,
      "grad_norm": 4.617206573486328,
      "learning_rate": 4.406691522762951e-05,
      "loss": 0.6232,
      "step": 604700
    },
    {
      "epoch": 9.494505494505495,
      "grad_norm": 2.5603976249694824,
      "learning_rate": 4.406593406593407e-05,
      "loss": 0.6811,
      "step": 604800
    },
    {
      "epoch": 9.49607535321821,
      "grad_norm": 4.336328029632568,
      "learning_rate": 4.406495290423862e-05,
      "loss": 0.6622,
      "step": 604900
    },
    {
      "epoch": 9.497645211930926,
      "grad_norm": 3.7622785568237305,
      "learning_rate": 4.406397174254318e-05,
      "loss": 0.6496,
      "step": 605000
    },
    {
      "epoch": 9.499215070643642,
      "grad_norm": 3.7227532863616943,
      "learning_rate": 4.406299058084772e-05,
      "loss": 0.677,
      "step": 605100
    },
    {
      "epoch": 9.500784929356358,
      "grad_norm": 4.015052795410156,
      "learning_rate": 4.406200941915228e-05,
      "loss": 0.6017,
      "step": 605200
    },
    {
      "epoch": 9.502354788069074,
      "grad_norm": 3.7617645263671875,
      "learning_rate": 4.406102825745683e-05,
      "loss": 0.6638,
      "step": 605300
    },
    {
      "epoch": 9.50392464678179,
      "grad_norm": 3.1516928672790527,
      "learning_rate": 4.406004709576138e-05,
      "loss": 0.6068,
      "step": 605400
    },
    {
      "epoch": 9.505494505494505,
      "grad_norm": 4.145055294036865,
      "learning_rate": 4.405906593406593e-05,
      "loss": 0.624,
      "step": 605500
    },
    {
      "epoch": 9.50706436420722,
      "grad_norm": 3.559891700744629,
      "learning_rate": 4.405808477237049e-05,
      "loss": 0.6159,
      "step": 605600
    },
    {
      "epoch": 9.508634222919937,
      "grad_norm": 3.900806188583374,
      "learning_rate": 4.405710361067504e-05,
      "loss": 0.6477,
      "step": 605700
    },
    {
      "epoch": 9.510204081632653,
      "grad_norm": 4.437175273895264,
      "learning_rate": 4.405612244897959e-05,
      "loss": 0.6359,
      "step": 605800
    },
    {
      "epoch": 9.51177394034537,
      "grad_norm": 3.163242816925049,
      "learning_rate": 4.405514128728414e-05,
      "loss": 0.6411,
      "step": 605900
    },
    {
      "epoch": 9.513343799058084,
      "grad_norm": 4.359118938446045,
      "learning_rate": 4.40541601255887e-05,
      "loss": 0.6432,
      "step": 606000
    },
    {
      "epoch": 9.514913657770801,
      "grad_norm": 3.986407995223999,
      "learning_rate": 4.405317896389325e-05,
      "loss": 0.6208,
      "step": 606100
    },
    {
      "epoch": 9.516483516483516,
      "grad_norm": 2.1552093029022217,
      "learning_rate": 4.40521978021978e-05,
      "loss": 0.6403,
      "step": 606200
    },
    {
      "epoch": 9.518053375196232,
      "grad_norm": 3.5068228244781494,
      "learning_rate": 4.405121664050236e-05,
      "loss": 0.6663,
      "step": 606300
    },
    {
      "epoch": 9.519623233908948,
      "grad_norm": 4.229913711547852,
      "learning_rate": 4.405023547880691e-05,
      "loss": 0.6422,
      "step": 606400
    },
    {
      "epoch": 9.521193092621663,
      "grad_norm": 3.424772024154663,
      "learning_rate": 4.404925431711146e-05,
      "loss": 0.6489,
      "step": 606500
    },
    {
      "epoch": 9.52276295133438,
      "grad_norm": 4.119814395904541,
      "learning_rate": 4.404827315541601e-05,
      "loss": 0.654,
      "step": 606600
    },
    {
      "epoch": 9.524332810047095,
      "grad_norm": 4.064961910247803,
      "learning_rate": 4.404729199372057e-05,
      "loss": 0.6575,
      "step": 606700
    },
    {
      "epoch": 9.525902668759812,
      "grad_norm": 4.61184024810791,
      "learning_rate": 4.4046310832025115e-05,
      "loss": 0.6596,
      "step": 606800
    },
    {
      "epoch": 9.527472527472527,
      "grad_norm": 4.4586944580078125,
      "learning_rate": 4.404532967032967e-05,
      "loss": 0.6555,
      "step": 606900
    },
    {
      "epoch": 9.529042386185242,
      "grad_norm": 3.23734450340271,
      "learning_rate": 4.4044348508634224e-05,
      "loss": 0.6415,
      "step": 607000
    },
    {
      "epoch": 9.53061224489796,
      "grad_norm": 3.6184017658233643,
      "learning_rate": 4.404336734693878e-05,
      "loss": 0.6381,
      "step": 607100
    },
    {
      "epoch": 9.532182103610674,
      "grad_norm": 3.8092098236083984,
      "learning_rate": 4.4042386185243326e-05,
      "loss": 0.6267,
      "step": 607200
    },
    {
      "epoch": 9.533751962323391,
      "grad_norm": 3.806234359741211,
      "learning_rate": 4.4041405023547884e-05,
      "loss": 0.6408,
      "step": 607300
    },
    {
      "epoch": 9.535321821036106,
      "grad_norm": 4.036462306976318,
      "learning_rate": 4.4040423861852435e-05,
      "loss": 0.6318,
      "step": 607400
    },
    {
      "epoch": 9.536891679748823,
      "grad_norm": 3.555108070373535,
      "learning_rate": 4.4039442700156986e-05,
      "loss": 0.6914,
      "step": 607500
    },
    {
      "epoch": 9.538461538461538,
      "grad_norm": 2.733250141143799,
      "learning_rate": 4.403846153846154e-05,
      "loss": 0.6317,
      "step": 607600
    },
    {
      "epoch": 9.540031397174253,
      "grad_norm": 2.776287317276001,
      "learning_rate": 4.4037480376766094e-05,
      "loss": 0.6584,
      "step": 607700
    },
    {
      "epoch": 9.54160125588697,
      "grad_norm": 4.014490127563477,
      "learning_rate": 4.4036499215070645e-05,
      "loss": 0.6304,
      "step": 607800
    },
    {
      "epoch": 9.543171114599685,
      "grad_norm": 3.9011378288269043,
      "learning_rate": 4.4035518053375196e-05,
      "loss": 0.6566,
      "step": 607900
    },
    {
      "epoch": 9.544740973312402,
      "grad_norm": 3.6160123348236084,
      "learning_rate": 4.403453689167975e-05,
      "loss": 0.6172,
      "step": 608000
    },
    {
      "epoch": 9.546310832025117,
      "grad_norm": 4.373866558074951,
      "learning_rate": 4.4033555729984305e-05,
      "loss": 0.5954,
      "step": 608100
    },
    {
      "epoch": 9.547880690737834,
      "grad_norm": 4.547649383544922,
      "learning_rate": 4.4032574568288856e-05,
      "loss": 0.6435,
      "step": 608200
    },
    {
      "epoch": 9.54945054945055,
      "grad_norm": 3.233738660812378,
      "learning_rate": 4.403159340659341e-05,
      "loss": 0.6395,
      "step": 608300
    },
    {
      "epoch": 9.551020408163264,
      "grad_norm": 3.6539807319641113,
      "learning_rate": 4.4030612244897965e-05,
      "loss": 0.6304,
      "step": 608400
    },
    {
      "epoch": 9.552590266875981,
      "grad_norm": 3.101296901702881,
      "learning_rate": 4.4029631083202516e-05,
      "loss": 0.6391,
      "step": 608500
    },
    {
      "epoch": 9.554160125588696,
      "grad_norm": 5.5684638023376465,
      "learning_rate": 4.402864992150707e-05,
      "loss": 0.6454,
      "step": 608600
    },
    {
      "epoch": 9.555729984301413,
      "grad_norm": 4.175632953643799,
      "learning_rate": 4.402766875981162e-05,
      "loss": 0.6519,
      "step": 608700
    },
    {
      "epoch": 9.557299843014128,
      "grad_norm": 4.549045562744141,
      "learning_rate": 4.4026687598116176e-05,
      "loss": 0.6501,
      "step": 608800
    },
    {
      "epoch": 9.558869701726845,
      "grad_norm": 4.0246262550354,
      "learning_rate": 4.402570643642072e-05,
      "loss": 0.6859,
      "step": 608900
    },
    {
      "epoch": 9.56043956043956,
      "grad_norm": 4.122782230377197,
      "learning_rate": 4.402472527472528e-05,
      "loss": 0.6469,
      "step": 609000
    },
    {
      "epoch": 9.562009419152277,
      "grad_norm": 4.346149444580078,
      "learning_rate": 4.402374411302983e-05,
      "loss": 0.6513,
      "step": 609100
    },
    {
      "epoch": 9.563579277864992,
      "grad_norm": 3.5102484226226807,
      "learning_rate": 4.4022762951334386e-05,
      "loss": 0.6317,
      "step": 609200
    },
    {
      "epoch": 9.565149136577707,
      "grad_norm": 4.985483646392822,
      "learning_rate": 4.402178178963893e-05,
      "loss": 0.6472,
      "step": 609300
    },
    {
      "epoch": 9.566718995290424,
      "grad_norm": 3.899754285812378,
      "learning_rate": 4.402080062794349e-05,
      "loss": 0.6509,
      "step": 609400
    },
    {
      "epoch": 9.56828885400314,
      "grad_norm": 3.9271116256713867,
      "learning_rate": 4.401981946624804e-05,
      "loss": 0.6289,
      "step": 609500
    },
    {
      "epoch": 9.569858712715856,
      "grad_norm": 4.760791778564453,
      "learning_rate": 4.401883830455259e-05,
      "loss": 0.6537,
      "step": 609600
    },
    {
      "epoch": 9.571428571428571,
      "grad_norm": 4.059207916259766,
      "learning_rate": 4.401785714285714e-05,
      "loss": 0.6137,
      "step": 609700
    },
    {
      "epoch": 9.572998430141288,
      "grad_norm": 3.590353488922119,
      "learning_rate": 4.40168759811617e-05,
      "loss": 0.6798,
      "step": 609800
    },
    {
      "epoch": 9.574568288854003,
      "grad_norm": 4.08937406539917,
      "learning_rate": 4.401589481946625e-05,
      "loss": 0.6303,
      "step": 609900
    },
    {
      "epoch": 9.576138147566718,
      "grad_norm": 3.242337703704834,
      "learning_rate": 4.40149136577708e-05,
      "loss": 0.641,
      "step": 610000
    },
    {
      "epoch": 9.577708006279435,
      "grad_norm": 3.7473349571228027,
      "learning_rate": 4.401393249607535e-05,
      "loss": 0.6288,
      "step": 610100
    },
    {
      "epoch": 9.57927786499215,
      "grad_norm": 5.020595073699951,
      "learning_rate": 4.401295133437991e-05,
      "loss": 0.6842,
      "step": 610200
    },
    {
      "epoch": 9.580847723704867,
      "grad_norm": 2.5797531604766846,
      "learning_rate": 4.401197017268446e-05,
      "loss": 0.6448,
      "step": 610300
    },
    {
      "epoch": 9.582417582417582,
      "grad_norm": 4.004888534545898,
      "learning_rate": 4.401098901098901e-05,
      "loss": 0.6576,
      "step": 610400
    },
    {
      "epoch": 9.583987441130299,
      "grad_norm": 4.193017959594727,
      "learning_rate": 4.401000784929357e-05,
      "loss": 0.6533,
      "step": 610500
    },
    {
      "epoch": 9.585557299843014,
      "grad_norm": 3.2422432899475098,
      "learning_rate": 4.400902668759812e-05,
      "loss": 0.6349,
      "step": 610600
    },
    {
      "epoch": 9.58712715855573,
      "grad_norm": 4.763458251953125,
      "learning_rate": 4.400804552590267e-05,
      "loss": 0.6322,
      "step": 610700
    },
    {
      "epoch": 9.588697017268446,
      "grad_norm": 4.687840938568115,
      "learning_rate": 4.400706436420722e-05,
      "loss": 0.5999,
      "step": 610800
    },
    {
      "epoch": 9.590266875981161,
      "grad_norm": 5.558664798736572,
      "learning_rate": 4.400608320251178e-05,
      "loss": 0.6717,
      "step": 610900
    },
    {
      "epoch": 9.591836734693878,
      "grad_norm": 3.3884353637695312,
      "learning_rate": 4.4005102040816324e-05,
      "loss": 0.6861,
      "step": 611000
    },
    {
      "epoch": 9.593406593406593,
      "grad_norm": 3.3731119632720947,
      "learning_rate": 4.400412087912088e-05,
      "loss": 0.649,
      "step": 611100
    },
    {
      "epoch": 9.59497645211931,
      "grad_norm": 3.1333189010620117,
      "learning_rate": 4.400313971742543e-05,
      "loss": 0.6405,
      "step": 611200
    },
    {
      "epoch": 9.596546310832025,
      "grad_norm": 2.702329635620117,
      "learning_rate": 4.400215855572999e-05,
      "loss": 0.6323,
      "step": 611300
    },
    {
      "epoch": 9.598116169544742,
      "grad_norm": 3.5817110538482666,
      "learning_rate": 4.4001177394034535e-05,
      "loss": 0.631,
      "step": 611400
    },
    {
      "epoch": 9.599686028257457,
      "grad_norm": 4.505190372467041,
      "learning_rate": 4.400019623233909e-05,
      "loss": 0.6506,
      "step": 611500
    },
    {
      "epoch": 9.601255886970172,
      "grad_norm": 3.059271812438965,
      "learning_rate": 4.3999215070643644e-05,
      "loss": 0.6737,
      "step": 611600
    },
    {
      "epoch": 9.602825745682889,
      "grad_norm": 3.66367244720459,
      "learning_rate": 4.3998233908948195e-05,
      "loss": 0.663,
      "step": 611700
    },
    {
      "epoch": 9.604395604395604,
      "grad_norm": 3.338902711868286,
      "learning_rate": 4.3997252747252746e-05,
      "loss": 0.6252,
      "step": 611800
    },
    {
      "epoch": 9.605965463108321,
      "grad_norm": 3.1131629943847656,
      "learning_rate": 4.3996271585557303e-05,
      "loss": 0.666,
      "step": 611900
    },
    {
      "epoch": 9.607535321821036,
      "grad_norm": 3.636345863342285,
      "learning_rate": 4.3995290423861854e-05,
      "loss": 0.6478,
      "step": 612000
    },
    {
      "epoch": 9.609105180533753,
      "grad_norm": 3.75646710395813,
      "learning_rate": 4.3994309262166405e-05,
      "loss": 0.6396,
      "step": 612100
    },
    {
      "epoch": 9.610675039246468,
      "grad_norm": 3.4939651489257812,
      "learning_rate": 4.3993328100470956e-05,
      "loss": 0.6171,
      "step": 612200
    },
    {
      "epoch": 9.612244897959183,
      "grad_norm": 3.7487170696258545,
      "learning_rate": 4.3992346938775514e-05,
      "loss": 0.6143,
      "step": 612300
    },
    {
      "epoch": 9.6138147566719,
      "grad_norm": 4.074834823608398,
      "learning_rate": 4.3991365777080065e-05,
      "loss": 0.5932,
      "step": 612400
    },
    {
      "epoch": 9.615384615384615,
      "grad_norm": 4.134964942932129,
      "learning_rate": 4.3990384615384616e-05,
      "loss": 0.6442,
      "step": 612500
    },
    {
      "epoch": 9.616954474097332,
      "grad_norm": 2.3546273708343506,
      "learning_rate": 4.3989403453689174e-05,
      "loss": 0.6292,
      "step": 612600
    },
    {
      "epoch": 9.618524332810047,
      "grad_norm": 4.487773418426514,
      "learning_rate": 4.3988422291993725e-05,
      "loss": 0.6031,
      "step": 612700
    },
    {
      "epoch": 9.620094191522764,
      "grad_norm": 3.3363962173461914,
      "learning_rate": 4.3987441130298276e-05,
      "loss": 0.6279,
      "step": 612800
    },
    {
      "epoch": 9.621664050235479,
      "grad_norm": 3.8778278827667236,
      "learning_rate": 4.398645996860283e-05,
      "loss": 0.6595,
      "step": 612900
    },
    {
      "epoch": 9.623233908948194,
      "grad_norm": 3.3716108798980713,
      "learning_rate": 4.3985478806907385e-05,
      "loss": 0.6182,
      "step": 613000
    },
    {
      "epoch": 9.62480376766091,
      "grad_norm": 3.9826719760894775,
      "learning_rate": 4.398449764521193e-05,
      "loss": 0.6624,
      "step": 613100
    },
    {
      "epoch": 9.626373626373626,
      "grad_norm": 4.022034168243408,
      "learning_rate": 4.3983516483516487e-05,
      "loss": 0.6259,
      "step": 613200
    },
    {
      "epoch": 9.627943485086343,
      "grad_norm": 3.502257823944092,
      "learning_rate": 4.398253532182104e-05,
      "loss": 0.6641,
      "step": 613300
    },
    {
      "epoch": 9.629513343799058,
      "grad_norm": 3.8050968647003174,
      "learning_rate": 4.3981554160125595e-05,
      "loss": 0.6577,
      "step": 613400
    },
    {
      "epoch": 9.631083202511775,
      "grad_norm": 5.79041051864624,
      "learning_rate": 4.398057299843014e-05,
      "loss": 0.6512,
      "step": 613500
    },
    {
      "epoch": 9.63265306122449,
      "grad_norm": 3.794861078262329,
      "learning_rate": 4.39795918367347e-05,
      "loss": 0.6607,
      "step": 613600
    },
    {
      "epoch": 9.634222919937205,
      "grad_norm": 4.700565338134766,
      "learning_rate": 4.397861067503925e-05,
      "loss": 0.6529,
      "step": 613700
    },
    {
      "epoch": 9.635792778649922,
      "grad_norm": 3.906846523284912,
      "learning_rate": 4.39776295133438e-05,
      "loss": 0.649,
      "step": 613800
    },
    {
      "epoch": 9.637362637362637,
      "grad_norm": 4.638856410980225,
      "learning_rate": 4.397664835164835e-05,
      "loss": 0.6914,
      "step": 613900
    },
    {
      "epoch": 9.638932496075354,
      "grad_norm": 3.4893102645874023,
      "learning_rate": 4.397566718995291e-05,
      "loss": 0.6897,
      "step": 614000
    },
    {
      "epoch": 9.640502354788069,
      "grad_norm": 3.677182674407959,
      "learning_rate": 4.397468602825746e-05,
      "loss": 0.6516,
      "step": 614100
    },
    {
      "epoch": 9.642072213500786,
      "grad_norm": 3.8441691398620605,
      "learning_rate": 4.397370486656201e-05,
      "loss": 0.5823,
      "step": 614200
    },
    {
      "epoch": 9.6436420722135,
      "grad_norm": 4.512580394744873,
      "learning_rate": 4.397272370486656e-05,
      "loss": 0.615,
      "step": 614300
    },
    {
      "epoch": 9.645211930926216,
      "grad_norm": 2.8070290088653564,
      "learning_rate": 4.397174254317112e-05,
      "loss": 0.6415,
      "step": 614400
    },
    {
      "epoch": 9.646781789638933,
      "grad_norm": 4.498466968536377,
      "learning_rate": 4.397076138147567e-05,
      "loss": 0.6322,
      "step": 614500
    },
    {
      "epoch": 9.648351648351648,
      "grad_norm": 2.6728878021240234,
      "learning_rate": 4.396978021978022e-05,
      "loss": 0.6345,
      "step": 614600
    },
    {
      "epoch": 9.649921507064365,
      "grad_norm": 4.282426834106445,
      "learning_rate": 4.396879905808478e-05,
      "loss": 0.6062,
      "step": 614700
    },
    {
      "epoch": 9.65149136577708,
      "grad_norm": 3.2853353023529053,
      "learning_rate": 4.396781789638933e-05,
      "loss": 0.6446,
      "step": 614800
    },
    {
      "epoch": 9.653061224489797,
      "grad_norm": 4.553531646728516,
      "learning_rate": 4.396683673469388e-05,
      "loss": 0.6492,
      "step": 614900
    },
    {
      "epoch": 9.654631083202512,
      "grad_norm": 3.478520393371582,
      "learning_rate": 4.396585557299843e-05,
      "loss": 0.6251,
      "step": 615000
    },
    {
      "epoch": 9.656200941915227,
      "grad_norm": 3.375216245651245,
      "learning_rate": 4.396487441130299e-05,
      "loss": 0.6093,
      "step": 615100
    },
    {
      "epoch": 9.657770800627944,
      "grad_norm": 4.775806427001953,
      "learning_rate": 4.396389324960753e-05,
      "loss": 0.6423,
      "step": 615200
    },
    {
      "epoch": 9.659340659340659,
      "grad_norm": 3.739304542541504,
      "learning_rate": 4.396291208791209e-05,
      "loss": 0.6014,
      "step": 615300
    },
    {
      "epoch": 9.660910518053376,
      "grad_norm": 4.327025413513184,
      "learning_rate": 4.396193092621664e-05,
      "loss": 0.6297,
      "step": 615400
    },
    {
      "epoch": 9.66248037676609,
      "grad_norm": 4.673077583312988,
      "learning_rate": 4.39609497645212e-05,
      "loss": 0.6036,
      "step": 615500
    },
    {
      "epoch": 9.664050235478808,
      "grad_norm": 4.152600288391113,
      "learning_rate": 4.3959968602825744e-05,
      "loss": 0.6161,
      "step": 615600
    },
    {
      "epoch": 9.665620094191523,
      "grad_norm": 3.62921142578125,
      "learning_rate": 4.39589874411303e-05,
      "loss": 0.6323,
      "step": 615700
    },
    {
      "epoch": 9.667189952904238,
      "grad_norm": 4.054017066955566,
      "learning_rate": 4.395800627943485e-05,
      "loss": 0.6738,
      "step": 615800
    },
    {
      "epoch": 9.668759811616955,
      "grad_norm": 4.185325622558594,
      "learning_rate": 4.3957025117739404e-05,
      "loss": 0.6352,
      "step": 615900
    },
    {
      "epoch": 9.67032967032967,
      "grad_norm": 2.86398983001709,
      "learning_rate": 4.3956043956043955e-05,
      "loss": 0.6496,
      "step": 616000
    },
    {
      "epoch": 9.671899529042387,
      "grad_norm": 4.0204010009765625,
      "learning_rate": 4.395506279434851e-05,
      "loss": 0.6158,
      "step": 616100
    },
    {
      "epoch": 9.673469387755102,
      "grad_norm": 4.564062595367432,
      "learning_rate": 4.3954081632653063e-05,
      "loss": 0.6679,
      "step": 616200
    },
    {
      "epoch": 9.675039246467819,
      "grad_norm": 4.5321736335754395,
      "learning_rate": 4.3953100470957614e-05,
      "loss": 0.6241,
      "step": 616300
    },
    {
      "epoch": 9.676609105180534,
      "grad_norm": 3.2435216903686523,
      "learning_rate": 4.3952119309262165e-05,
      "loss": 0.6574,
      "step": 616400
    },
    {
      "epoch": 9.678178963893249,
      "grad_norm": 4.783077716827393,
      "learning_rate": 4.395113814756672e-05,
      "loss": 0.6281,
      "step": 616500
    },
    {
      "epoch": 9.679748822605966,
      "grad_norm": 3.4082083702087402,
      "learning_rate": 4.3950156985871274e-05,
      "loss": 0.596,
      "step": 616600
    },
    {
      "epoch": 9.68131868131868,
      "grad_norm": 2.9373931884765625,
      "learning_rate": 4.3949175824175825e-05,
      "loss": 0.6279,
      "step": 616700
    },
    {
      "epoch": 9.682888540031398,
      "grad_norm": 3.9443705081939697,
      "learning_rate": 4.394819466248038e-05,
      "loss": 0.6463,
      "step": 616800
    },
    {
      "epoch": 9.684458398744113,
      "grad_norm": 3.263409376144409,
      "learning_rate": 4.3947213500784934e-05,
      "loss": 0.6648,
      "step": 616900
    },
    {
      "epoch": 9.68602825745683,
      "grad_norm": 3.5146970748901367,
      "learning_rate": 4.3946232339089485e-05,
      "loss": 0.6748,
      "step": 617000
    },
    {
      "epoch": 9.687598116169545,
      "grad_norm": 3.570185899734497,
      "learning_rate": 4.3945251177394036e-05,
      "loss": 0.6215,
      "step": 617100
    },
    {
      "epoch": 9.68916797488226,
      "grad_norm": 4.037125110626221,
      "learning_rate": 4.3944270015698594e-05,
      "loss": 0.6363,
      "step": 617200
    },
    {
      "epoch": 9.690737833594977,
      "grad_norm": 3.61472749710083,
      "learning_rate": 4.394328885400314e-05,
      "loss": 0.6206,
      "step": 617300
    },
    {
      "epoch": 9.692307692307692,
      "grad_norm": 3.3515663146972656,
      "learning_rate": 4.3942307692307695e-05,
      "loss": 0.6665,
      "step": 617400
    },
    {
      "epoch": 9.693877551020408,
      "grad_norm": 2.3899927139282227,
      "learning_rate": 4.3941326530612246e-05,
      "loss": 0.6137,
      "step": 617500
    },
    {
      "epoch": 9.695447409733124,
      "grad_norm": 3.1537208557128906,
      "learning_rate": 4.3940345368916804e-05,
      "loss": 0.6389,
      "step": 617600
    },
    {
      "epoch": 9.69701726844584,
      "grad_norm": 4.074238300323486,
      "learning_rate": 4.393936420722135e-05,
      "loss": 0.6508,
      "step": 617700
    },
    {
      "epoch": 9.698587127158556,
      "grad_norm": 3.7857861518859863,
      "learning_rate": 4.3938383045525906e-05,
      "loss": 0.6253,
      "step": 617800
    },
    {
      "epoch": 9.700156985871272,
      "grad_norm": 4.2979350090026855,
      "learning_rate": 4.393740188383046e-05,
      "loss": 0.6352,
      "step": 617900
    },
    {
      "epoch": 9.701726844583987,
      "grad_norm": 4.299069404602051,
      "learning_rate": 4.393642072213501e-05,
      "loss": 0.6207,
      "step": 618000
    },
    {
      "epoch": 9.703296703296703,
      "grad_norm": 2.7442870140075684,
      "learning_rate": 4.393543956043956e-05,
      "loss": 0.6422,
      "step": 618100
    },
    {
      "epoch": 9.70486656200942,
      "grad_norm": 3.739853858947754,
      "learning_rate": 4.393445839874412e-05,
      "loss": 0.6888,
      "step": 618200
    },
    {
      "epoch": 9.706436420722135,
      "grad_norm": 3.7531604766845703,
      "learning_rate": 4.393347723704867e-05,
      "loss": 0.6345,
      "step": 618300
    },
    {
      "epoch": 9.708006279434851,
      "grad_norm": 3.9057810306549072,
      "learning_rate": 4.393249607535322e-05,
      "loss": 0.6349,
      "step": 618400
    },
    {
      "epoch": 9.709576138147566,
      "grad_norm": 3.604731559753418,
      "learning_rate": 4.393151491365777e-05,
      "loss": 0.6191,
      "step": 618500
    },
    {
      "epoch": 9.711145996860283,
      "grad_norm": 3.638193368911743,
      "learning_rate": 4.393053375196233e-05,
      "loss": 0.6551,
      "step": 618600
    },
    {
      "epoch": 9.712715855572998,
      "grad_norm": 4.440779685974121,
      "learning_rate": 4.392955259026688e-05,
      "loss": 0.649,
      "step": 618700
    },
    {
      "epoch": 9.714285714285714,
      "grad_norm": 3.3638508319854736,
      "learning_rate": 4.392857142857143e-05,
      "loss": 0.6288,
      "step": 618800
    },
    {
      "epoch": 9.71585557299843,
      "grad_norm": 3.580610513687134,
      "learning_rate": 4.392759026687599e-05,
      "loss": 0.6166,
      "step": 618900
    },
    {
      "epoch": 9.717425431711145,
      "grad_norm": 3.333390951156616,
      "learning_rate": 4.392660910518054e-05,
      "loss": 0.6439,
      "step": 619000
    },
    {
      "epoch": 9.718995290423862,
      "grad_norm": 4.2074875831604,
      "learning_rate": 4.392562794348509e-05,
      "loss": 0.6289,
      "step": 619100
    },
    {
      "epoch": 9.720565149136577,
      "grad_norm": 4.5496134757995605,
      "learning_rate": 4.392464678178964e-05,
      "loss": 0.5967,
      "step": 619200
    },
    {
      "epoch": 9.722135007849294,
      "grad_norm": 4.078187942504883,
      "learning_rate": 4.39236656200942e-05,
      "loss": 0.6601,
      "step": 619300
    },
    {
      "epoch": 9.72370486656201,
      "grad_norm": 3.0712850093841553,
      "learning_rate": 4.392268445839874e-05,
      "loss": 0.6529,
      "step": 619400
    },
    {
      "epoch": 9.725274725274724,
      "grad_norm": 4.906290054321289,
      "learning_rate": 4.39217032967033e-05,
      "loss": 0.6352,
      "step": 619500
    },
    {
      "epoch": 9.726844583987441,
      "grad_norm": 2.60261869430542,
      "learning_rate": 4.392072213500785e-05,
      "loss": 0.667,
      "step": 619600
    },
    {
      "epoch": 9.728414442700156,
      "grad_norm": 3.7763798236846924,
      "learning_rate": 4.391974097331241e-05,
      "loss": 0.6314,
      "step": 619700
    },
    {
      "epoch": 9.729984301412873,
      "grad_norm": 3.8660967350006104,
      "learning_rate": 4.391875981161695e-05,
      "loss": 0.6146,
      "step": 619800
    },
    {
      "epoch": 9.731554160125588,
      "grad_norm": 3.75880765914917,
      "learning_rate": 4.391777864992151e-05,
      "loss": 0.6519,
      "step": 619900
    },
    {
      "epoch": 9.733124018838305,
      "grad_norm": 4.344946384429932,
      "learning_rate": 4.391679748822606e-05,
      "loss": 0.6553,
      "step": 620000
    },
    {
      "epoch": 9.73469387755102,
      "grad_norm": 4.590633392333984,
      "learning_rate": 4.391581632653061e-05,
      "loss": 0.6321,
      "step": 620100
    },
    {
      "epoch": 9.736263736263737,
      "grad_norm": 3.750028371810913,
      "learning_rate": 4.3914835164835164e-05,
      "loss": 0.6527,
      "step": 620200
    },
    {
      "epoch": 9.737833594976452,
      "grad_norm": 4.509251117706299,
      "learning_rate": 4.391385400313972e-05,
      "loss": 0.6468,
      "step": 620300
    },
    {
      "epoch": 9.739403453689167,
      "grad_norm": 4.313596248626709,
      "learning_rate": 4.391287284144427e-05,
      "loss": 0.6769,
      "step": 620400
    },
    {
      "epoch": 9.740973312401884,
      "grad_norm": 3.561962127685547,
      "learning_rate": 4.391189167974882e-05,
      "loss": 0.6376,
      "step": 620500
    },
    {
      "epoch": 9.7425431711146,
      "grad_norm": 4.116892337799072,
      "learning_rate": 4.3910910518053374e-05,
      "loss": 0.6548,
      "step": 620600
    },
    {
      "epoch": 9.744113029827316,
      "grad_norm": 3.742690324783325,
      "learning_rate": 4.390992935635793e-05,
      "loss": 0.6442,
      "step": 620700
    },
    {
      "epoch": 9.745682888540031,
      "grad_norm": 4.158529758453369,
      "learning_rate": 4.390894819466248e-05,
      "loss": 0.6606,
      "step": 620800
    },
    {
      "epoch": 9.747252747252748,
      "grad_norm": 3.42997670173645,
      "learning_rate": 4.3907967032967034e-05,
      "loss": 0.6687,
      "step": 620900
    },
    {
      "epoch": 9.748822605965463,
      "grad_norm": 3.075289487838745,
      "learning_rate": 4.390698587127159e-05,
      "loss": 0.6675,
      "step": 621000
    },
    {
      "epoch": 9.750392464678178,
      "grad_norm": 4.45829963684082,
      "learning_rate": 4.390600470957614e-05,
      "loss": 0.5773,
      "step": 621100
    },
    {
      "epoch": 9.751962323390895,
      "grad_norm": 3.261223316192627,
      "learning_rate": 4.3905023547880694e-05,
      "loss": 0.6113,
      "step": 621200
    },
    {
      "epoch": 9.75353218210361,
      "grad_norm": 4.460567474365234,
      "learning_rate": 4.3904042386185245e-05,
      "loss": 0.6155,
      "step": 621300
    },
    {
      "epoch": 9.755102040816327,
      "grad_norm": 4.264063835144043,
      "learning_rate": 4.39030612244898e-05,
      "loss": 0.6239,
      "step": 621400
    },
    {
      "epoch": 9.756671899529042,
      "grad_norm": 3.5326006412506104,
      "learning_rate": 4.390208006279435e-05,
      "loss": 0.6665,
      "step": 621500
    },
    {
      "epoch": 9.758241758241759,
      "grad_norm": 3.8156449794769287,
      "learning_rate": 4.3901098901098904e-05,
      "loss": 0.6647,
      "step": 621600
    },
    {
      "epoch": 9.759811616954474,
      "grad_norm": 2.5330073833465576,
      "learning_rate": 4.3900117739403455e-05,
      "loss": 0.636,
      "step": 621700
    },
    {
      "epoch": 9.76138147566719,
      "grad_norm": 2.710867404937744,
      "learning_rate": 4.389913657770801e-05,
      "loss": 0.632,
      "step": 621800
    },
    {
      "epoch": 9.762951334379906,
      "grad_norm": 4.31583833694458,
      "learning_rate": 4.389815541601256e-05,
      "loss": 0.6675,
      "step": 621900
    },
    {
      "epoch": 9.764521193092621,
      "grad_norm": 5.057156085968018,
      "learning_rate": 4.3897174254317115e-05,
      "loss": 0.6583,
      "step": 622000
    },
    {
      "epoch": 9.766091051805338,
      "grad_norm": 3.628460645675659,
      "learning_rate": 4.3896193092621666e-05,
      "loss": 0.641,
      "step": 622100
    },
    {
      "epoch": 9.767660910518053,
      "grad_norm": 2.9683663845062256,
      "learning_rate": 4.389521193092622e-05,
      "loss": 0.5818,
      "step": 622200
    },
    {
      "epoch": 9.76923076923077,
      "grad_norm": 3.5725793838500977,
      "learning_rate": 4.389423076923077e-05,
      "loss": 0.6826,
      "step": 622300
    },
    {
      "epoch": 9.770800627943485,
      "grad_norm": 3.1081395149230957,
      "learning_rate": 4.3893249607535326e-05,
      "loss": 0.648,
      "step": 622400
    },
    {
      "epoch": 9.7723704866562,
      "grad_norm": 5.2286858558654785,
      "learning_rate": 4.389226844583988e-05,
      "loss": 0.6582,
      "step": 622500
    },
    {
      "epoch": 9.773940345368917,
      "grad_norm": 3.999187469482422,
      "learning_rate": 4.389128728414443e-05,
      "loss": 0.6206,
      "step": 622600
    },
    {
      "epoch": 9.775510204081632,
      "grad_norm": 3.8287625312805176,
      "learning_rate": 4.389030612244898e-05,
      "loss": 0.6396,
      "step": 622700
    },
    {
      "epoch": 9.777080062794349,
      "grad_norm": 3.996140956878662,
      "learning_rate": 4.3889324960753537e-05,
      "loss": 0.6085,
      "step": 622800
    },
    {
      "epoch": 9.778649921507064,
      "grad_norm": 3.6524899005889893,
      "learning_rate": 4.388834379905809e-05,
      "loss": 0.6327,
      "step": 622900
    },
    {
      "epoch": 9.780219780219781,
      "grad_norm": 2.4978749752044678,
      "learning_rate": 4.388736263736264e-05,
      "loss": 0.6263,
      "step": 623000
    },
    {
      "epoch": 9.781789638932496,
      "grad_norm": 3.6024138927459717,
      "learning_rate": 4.3886381475667196e-05,
      "loss": 0.6446,
      "step": 623100
    },
    {
      "epoch": 9.783359497645211,
      "grad_norm": 4.611268520355225,
      "learning_rate": 4.388540031397175e-05,
      "loss": 0.5721,
      "step": 623200
    },
    {
      "epoch": 9.784929356357928,
      "grad_norm": 3.8919105529785156,
      "learning_rate": 4.38844191522763e-05,
      "loss": 0.6402,
      "step": 623300
    },
    {
      "epoch": 9.786499215070643,
      "grad_norm": 2.5864791870117188,
      "learning_rate": 4.388343799058085e-05,
      "loss": 0.6267,
      "step": 623400
    },
    {
      "epoch": 9.78806907378336,
      "grad_norm": 3.758162498474121,
      "learning_rate": 4.388245682888541e-05,
      "loss": 0.6448,
      "step": 623500
    },
    {
      "epoch": 9.789638932496075,
      "grad_norm": 3.81994366645813,
      "learning_rate": 4.388147566718995e-05,
      "loss": 0.6017,
      "step": 623600
    },
    {
      "epoch": 9.791208791208792,
      "grad_norm": 4.310797691345215,
      "learning_rate": 4.388049450549451e-05,
      "loss": 0.6532,
      "step": 623700
    },
    {
      "epoch": 9.792778649921507,
      "grad_norm": 2.657047986984253,
      "learning_rate": 4.387951334379906e-05,
      "loss": 0.6215,
      "step": 623800
    },
    {
      "epoch": 9.794348508634222,
      "grad_norm": 3.401066303253174,
      "learning_rate": 4.387853218210362e-05,
      "loss": 0.6226,
      "step": 623900
    },
    {
      "epoch": 9.795918367346939,
      "grad_norm": 4.365729808807373,
      "learning_rate": 4.387755102040816e-05,
      "loss": 0.6607,
      "step": 624000
    },
    {
      "epoch": 9.797488226059654,
      "grad_norm": 4.66830587387085,
      "learning_rate": 4.387656985871272e-05,
      "loss": 0.6197,
      "step": 624100
    },
    {
      "epoch": 9.799058084772371,
      "grad_norm": 4.647177219390869,
      "learning_rate": 4.387558869701727e-05,
      "loss": 0.6361,
      "step": 624200
    },
    {
      "epoch": 9.800627943485086,
      "grad_norm": 3.9938137531280518,
      "learning_rate": 4.387460753532182e-05,
      "loss": 0.6317,
      "step": 624300
    },
    {
      "epoch": 9.802197802197803,
      "grad_norm": 3.5457093715667725,
      "learning_rate": 4.387362637362637e-05,
      "loss": 0.6167,
      "step": 624400
    },
    {
      "epoch": 9.803767660910518,
      "grad_norm": 2.6918954849243164,
      "learning_rate": 4.387264521193093e-05,
      "loss": 0.6259,
      "step": 624500
    },
    {
      "epoch": 9.805337519623233,
      "grad_norm": 4.416221618652344,
      "learning_rate": 4.387166405023548e-05,
      "loss": 0.6102,
      "step": 624600
    },
    {
      "epoch": 9.80690737833595,
      "grad_norm": 2.8357739448547363,
      "learning_rate": 4.387068288854003e-05,
      "loss": 0.6019,
      "step": 624700
    },
    {
      "epoch": 9.808477237048665,
      "grad_norm": 3.939213514328003,
      "learning_rate": 4.386970172684458e-05,
      "loss": 0.6284,
      "step": 624800
    },
    {
      "epoch": 9.810047095761382,
      "grad_norm": 4.740400314331055,
      "learning_rate": 4.386872056514914e-05,
      "loss": 0.6004,
      "step": 624900
    },
    {
      "epoch": 9.811616954474097,
      "grad_norm": 3.460897922515869,
      "learning_rate": 4.386773940345369e-05,
      "loss": 0.6518,
      "step": 625000
    },
    {
      "epoch": 9.813186813186814,
      "grad_norm": 3.8868460655212402,
      "learning_rate": 4.386675824175824e-05,
      "loss": 0.6777,
      "step": 625100
    },
    {
      "epoch": 9.814756671899529,
      "grad_norm": 3.454247236251831,
      "learning_rate": 4.38657770800628e-05,
      "loss": 0.6529,
      "step": 625200
    },
    {
      "epoch": 9.816326530612244,
      "grad_norm": 4.709226131439209,
      "learning_rate": 4.386479591836735e-05,
      "loss": 0.6182,
      "step": 625300
    },
    {
      "epoch": 9.817896389324961,
      "grad_norm": 5.339489459991455,
      "learning_rate": 4.38638147566719e-05,
      "loss": 0.6601,
      "step": 625400
    },
    {
      "epoch": 9.819466248037676,
      "grad_norm": 2.7944841384887695,
      "learning_rate": 4.3862833594976454e-05,
      "loss": 0.6239,
      "step": 625500
    },
    {
      "epoch": 9.821036106750393,
      "grad_norm": 3.9804232120513916,
      "learning_rate": 4.386185243328101e-05,
      "loss": 0.6711,
      "step": 625600
    },
    {
      "epoch": 9.822605965463108,
      "grad_norm": 4.550668716430664,
      "learning_rate": 4.3860871271585556e-05,
      "loss": 0.5854,
      "step": 625700
    },
    {
      "epoch": 9.824175824175825,
      "grad_norm": 3.179419994354248,
      "learning_rate": 4.3859890109890113e-05,
      "loss": 0.6669,
      "step": 625800
    },
    {
      "epoch": 9.82574568288854,
      "grad_norm": 3.941678285598755,
      "learning_rate": 4.3858908948194664e-05,
      "loss": 0.6508,
      "step": 625900
    },
    {
      "epoch": 9.827315541601255,
      "grad_norm": 3.9080417156219482,
      "learning_rate": 4.3857927786499215e-05,
      "loss": 0.6673,
      "step": 626000
    },
    {
      "epoch": 9.828885400313972,
      "grad_norm": 3.6411237716674805,
      "learning_rate": 4.3856946624803766e-05,
      "loss": 0.6425,
      "step": 626100
    },
    {
      "epoch": 9.830455259026687,
      "grad_norm": 4.847797393798828,
      "learning_rate": 4.3855965463108324e-05,
      "loss": 0.6389,
      "step": 626200
    },
    {
      "epoch": 9.832025117739404,
      "grad_norm": 5.292966842651367,
      "learning_rate": 4.3854984301412875e-05,
      "loss": 0.6681,
      "step": 626300
    },
    {
      "epoch": 9.833594976452119,
      "grad_norm": 3.87274432182312,
      "learning_rate": 4.3854003139717426e-05,
      "loss": 0.6272,
      "step": 626400
    },
    {
      "epoch": 9.835164835164836,
      "grad_norm": 4.417809009552002,
      "learning_rate": 4.385302197802198e-05,
      "loss": 0.6762,
      "step": 626500
    },
    {
      "epoch": 9.83673469387755,
      "grad_norm": 4.280989646911621,
      "learning_rate": 4.3852040816326535e-05,
      "loss": 0.6007,
      "step": 626600
    },
    {
      "epoch": 9.838304552590268,
      "grad_norm": 4.270031929016113,
      "learning_rate": 4.3851059654631086e-05,
      "loss": 0.6257,
      "step": 626700
    },
    {
      "epoch": 9.839874411302983,
      "grad_norm": 3.1644179821014404,
      "learning_rate": 4.385007849293564e-05,
      "loss": 0.6314,
      "step": 626800
    },
    {
      "epoch": 9.841444270015698,
      "grad_norm": 3.163424253463745,
      "learning_rate": 4.384909733124019e-05,
      "loss": 0.6503,
      "step": 626900
    },
    {
      "epoch": 9.843014128728415,
      "grad_norm": 2.877885341644287,
      "learning_rate": 4.3848116169544746e-05,
      "loss": 0.6532,
      "step": 627000
    },
    {
      "epoch": 9.84458398744113,
      "grad_norm": 3.963271379470825,
      "learning_rate": 4.3847135007849297e-05,
      "loss": 0.6425,
      "step": 627100
    },
    {
      "epoch": 9.846153846153847,
      "grad_norm": 3.6742851734161377,
      "learning_rate": 4.384615384615385e-05,
      "loss": 0.6204,
      "step": 627200
    },
    {
      "epoch": 9.847723704866562,
      "grad_norm": 4.094838619232178,
      "learning_rate": 4.3845172684458405e-05,
      "loss": 0.6457,
      "step": 627300
    },
    {
      "epoch": 9.849293563579279,
      "grad_norm": 2.949932098388672,
      "learning_rate": 4.384419152276295e-05,
      "loss": 0.6421,
      "step": 627400
    },
    {
      "epoch": 9.850863422291994,
      "grad_norm": 4.806472301483154,
      "learning_rate": 4.384321036106751e-05,
      "loss": 0.6372,
      "step": 627500
    },
    {
      "epoch": 9.852433281004709,
      "grad_norm": 3.8162927627563477,
      "learning_rate": 4.384222919937206e-05,
      "loss": 0.6296,
      "step": 627600
    },
    {
      "epoch": 9.854003139717426,
      "grad_norm": 3.4485232830047607,
      "learning_rate": 4.3841248037676616e-05,
      "loss": 0.6747,
      "step": 627700
    },
    {
      "epoch": 9.85557299843014,
      "grad_norm": 3.462498903274536,
      "learning_rate": 4.384026687598116e-05,
      "loss": 0.6566,
      "step": 627800
    },
    {
      "epoch": 9.857142857142858,
      "grad_norm": 4.065741062164307,
      "learning_rate": 4.383928571428572e-05,
      "loss": 0.5944,
      "step": 627900
    },
    {
      "epoch": 9.858712715855573,
      "grad_norm": 2.646062135696411,
      "learning_rate": 4.383830455259027e-05,
      "loss": 0.6322,
      "step": 628000
    },
    {
      "epoch": 9.86028257456829,
      "grad_norm": 4.841952323913574,
      "learning_rate": 4.383732339089482e-05,
      "loss": 0.6307,
      "step": 628100
    },
    {
      "epoch": 9.861852433281005,
      "grad_norm": 3.7055065631866455,
      "learning_rate": 4.383634222919937e-05,
      "loss": 0.5942,
      "step": 628200
    },
    {
      "epoch": 9.86342229199372,
      "grad_norm": 4.907088279724121,
      "learning_rate": 4.383536106750393e-05,
      "loss": 0.6511,
      "step": 628300
    },
    {
      "epoch": 9.864992150706437,
      "grad_norm": 4.436858654022217,
      "learning_rate": 4.383437990580848e-05,
      "loss": 0.6325,
      "step": 628400
    },
    {
      "epoch": 9.866562009419152,
      "grad_norm": 4.458293914794922,
      "learning_rate": 4.383339874411303e-05,
      "loss": 0.6828,
      "step": 628500
    },
    {
      "epoch": 9.868131868131869,
      "grad_norm": 4.875789165496826,
      "learning_rate": 4.383241758241758e-05,
      "loss": 0.6282,
      "step": 628600
    },
    {
      "epoch": 9.869701726844584,
      "grad_norm": 4.936241149902344,
      "learning_rate": 4.383143642072214e-05,
      "loss": 0.6589,
      "step": 628700
    },
    {
      "epoch": 9.8712715855573,
      "grad_norm": 4.197950839996338,
      "learning_rate": 4.383045525902669e-05,
      "loss": 0.6483,
      "step": 628800
    },
    {
      "epoch": 9.872841444270016,
      "grad_norm": 3.628412961959839,
      "learning_rate": 4.382947409733124e-05,
      "loss": 0.6166,
      "step": 628900
    },
    {
      "epoch": 9.87441130298273,
      "grad_norm": 3.4056050777435303,
      "learning_rate": 4.382849293563579e-05,
      "loss": 0.6722,
      "step": 629000
    },
    {
      "epoch": 9.875981161695448,
      "grad_norm": 4.333085060119629,
      "learning_rate": 4.382751177394035e-05,
      "loss": 0.6454,
      "step": 629100
    },
    {
      "epoch": 9.877551020408163,
      "grad_norm": 3.3419442176818848,
      "learning_rate": 4.38265306122449e-05,
      "loss": 0.6615,
      "step": 629200
    },
    {
      "epoch": 9.87912087912088,
      "grad_norm": 3.9133145809173584,
      "learning_rate": 4.382554945054945e-05,
      "loss": 0.637,
      "step": 629300
    },
    {
      "epoch": 9.880690737833595,
      "grad_norm": 4.277343273162842,
      "learning_rate": 4.382456828885401e-05,
      "loss": 0.68,
      "step": 629400
    },
    {
      "epoch": 9.882260596546312,
      "grad_norm": 4.192484378814697,
      "learning_rate": 4.3823587127158554e-05,
      "loss": 0.6764,
      "step": 629500
    },
    {
      "epoch": 9.883830455259027,
      "grad_norm": 4.26676082611084,
      "learning_rate": 4.382260596546311e-05,
      "loss": 0.6556,
      "step": 629600
    },
    {
      "epoch": 9.885400313971743,
      "grad_norm": 3.4550459384918213,
      "learning_rate": 4.382162480376766e-05,
      "loss": 0.6213,
      "step": 629700
    },
    {
      "epoch": 9.886970172684459,
      "grad_norm": 4.2321295738220215,
      "learning_rate": 4.382064364207222e-05,
      "loss": 0.6518,
      "step": 629800
    },
    {
      "epoch": 9.888540031397174,
      "grad_norm": 4.755461692810059,
      "learning_rate": 4.3819662480376765e-05,
      "loss": 0.6163,
      "step": 629900
    },
    {
      "epoch": 9.89010989010989,
      "grad_norm": 5.122980117797852,
      "learning_rate": 4.381868131868132e-05,
      "loss": 0.6628,
      "step": 630000
    },
    {
      "epoch": 9.891679748822606,
      "grad_norm": 3.9852325916290283,
      "learning_rate": 4.3817700156985873e-05,
      "loss": 0.649,
      "step": 630100
    },
    {
      "epoch": 9.893249607535322,
      "grad_norm": 4.291991710662842,
      "learning_rate": 4.3816718995290424e-05,
      "loss": 0.6496,
      "step": 630200
    },
    {
      "epoch": 9.894819466248038,
      "grad_norm": 4.054962635040283,
      "learning_rate": 4.3815737833594975e-05,
      "loss": 0.6148,
      "step": 630300
    },
    {
      "epoch": 9.896389324960754,
      "grad_norm": 3.2820956707000732,
      "learning_rate": 4.381475667189953e-05,
      "loss": 0.6335,
      "step": 630400
    },
    {
      "epoch": 9.89795918367347,
      "grad_norm": 4.1014556884765625,
      "learning_rate": 4.3813775510204084e-05,
      "loss": 0.6431,
      "step": 630500
    },
    {
      "epoch": 9.899529042386185,
      "grad_norm": 4.444730281829834,
      "learning_rate": 4.3812794348508635e-05,
      "loss": 0.5955,
      "step": 630600
    },
    {
      "epoch": 9.901098901098901,
      "grad_norm": 4.193126678466797,
      "learning_rate": 4.3811813186813186e-05,
      "loss": 0.6446,
      "step": 630700
    },
    {
      "epoch": 9.902668759811617,
      "grad_norm": 3.414092779159546,
      "learning_rate": 4.3810832025117744e-05,
      "loss": 0.6249,
      "step": 630800
    },
    {
      "epoch": 9.904238618524333,
      "grad_norm": 3.9313950538635254,
      "learning_rate": 4.3809850863422295e-05,
      "loss": 0.6343,
      "step": 630900
    },
    {
      "epoch": 9.905808477237048,
      "grad_norm": 3.882542610168457,
      "learning_rate": 4.3808869701726846e-05,
      "loss": 0.6219,
      "step": 631000
    },
    {
      "epoch": 9.907378335949765,
      "grad_norm": 3.1912200450897217,
      "learning_rate": 4.38078885400314e-05,
      "loss": 0.6171,
      "step": 631100
    },
    {
      "epoch": 9.90894819466248,
      "grad_norm": 3.1711978912353516,
      "learning_rate": 4.3806907378335955e-05,
      "loss": 0.6648,
      "step": 631200
    },
    {
      "epoch": 9.910518053375196,
      "grad_norm": 3.275099515914917,
      "learning_rate": 4.3805926216640506e-05,
      "loss": 0.6177,
      "step": 631300
    },
    {
      "epoch": 9.912087912087912,
      "grad_norm": 3.8719794750213623,
      "learning_rate": 4.3804945054945057e-05,
      "loss": 0.6721,
      "step": 631400
    },
    {
      "epoch": 9.913657770800627,
      "grad_norm": 4.2121453285217285,
      "learning_rate": 4.3803963893249614e-05,
      "loss": 0.6148,
      "step": 631500
    },
    {
      "epoch": 9.915227629513344,
      "grad_norm": 3.265451431274414,
      "learning_rate": 4.380298273155416e-05,
      "loss": 0.6422,
      "step": 631600
    },
    {
      "epoch": 9.91679748822606,
      "grad_norm": 3.371504306793213,
      "learning_rate": 4.3802001569858716e-05,
      "loss": 0.6439,
      "step": 631700
    },
    {
      "epoch": 9.918367346938776,
      "grad_norm": 3.9714269638061523,
      "learning_rate": 4.380102040816327e-05,
      "loss": 0.6129,
      "step": 631800
    },
    {
      "epoch": 9.919937205651491,
      "grad_norm": 2.477513551712036,
      "learning_rate": 4.3800039246467825e-05,
      "loss": 0.6382,
      "step": 631900
    },
    {
      "epoch": 9.921507064364206,
      "grad_norm": 3.3717217445373535,
      "learning_rate": 4.379905808477237e-05,
      "loss": 0.6347,
      "step": 632000
    },
    {
      "epoch": 9.923076923076923,
      "grad_norm": 3.508091449737549,
      "learning_rate": 4.379807692307693e-05,
      "loss": 0.594,
      "step": 632100
    },
    {
      "epoch": 9.924646781789638,
      "grad_norm": 3.094939708709717,
      "learning_rate": 4.379709576138148e-05,
      "loss": 0.6249,
      "step": 632200
    },
    {
      "epoch": 9.926216640502355,
      "grad_norm": 3.865330219268799,
      "learning_rate": 4.379611459968603e-05,
      "loss": 0.6612,
      "step": 632300
    },
    {
      "epoch": 9.92778649921507,
      "grad_norm": 4.285464286804199,
      "learning_rate": 4.379513343799058e-05,
      "loss": 0.6133,
      "step": 632400
    },
    {
      "epoch": 9.929356357927787,
      "grad_norm": 3.1110758781433105,
      "learning_rate": 4.379415227629514e-05,
      "loss": 0.6393,
      "step": 632500
    },
    {
      "epoch": 9.930926216640502,
      "grad_norm": 3.1513564586639404,
      "learning_rate": 4.379317111459969e-05,
      "loss": 0.6192,
      "step": 632600
    },
    {
      "epoch": 9.932496075353217,
      "grad_norm": 4.113479137420654,
      "learning_rate": 4.379218995290424e-05,
      "loss": 0.6751,
      "step": 632700
    },
    {
      "epoch": 9.934065934065934,
      "grad_norm": 3.0222158432006836,
      "learning_rate": 4.379120879120879e-05,
      "loss": 0.6349,
      "step": 632800
    },
    {
      "epoch": 9.93563579277865,
      "grad_norm": 3.9218714237213135,
      "learning_rate": 4.379022762951335e-05,
      "loss": 0.6847,
      "step": 632900
    },
    {
      "epoch": 9.937205651491366,
      "grad_norm": 3.844855785369873,
      "learning_rate": 4.37892464678179e-05,
      "loss": 0.6262,
      "step": 633000
    },
    {
      "epoch": 9.938775510204081,
      "grad_norm": 4.393561840057373,
      "learning_rate": 4.378826530612245e-05,
      "loss": 0.6595,
      "step": 633100
    },
    {
      "epoch": 9.940345368916798,
      "grad_norm": 3.4335579872131348,
      "learning_rate": 4.3787284144427e-05,
      "loss": 0.6471,
      "step": 633200
    },
    {
      "epoch": 9.941915227629513,
      "grad_norm": 4.110380172729492,
      "learning_rate": 4.378630298273156e-05,
      "loss": 0.6557,
      "step": 633300
    },
    {
      "epoch": 9.943485086342228,
      "grad_norm": 5.7784810066223145,
      "learning_rate": 4.378532182103611e-05,
      "loss": 0.6292,
      "step": 633400
    },
    {
      "epoch": 9.945054945054945,
      "grad_norm": 4.6134233474731445,
      "learning_rate": 4.378434065934066e-05,
      "loss": 0.643,
      "step": 633500
    },
    {
      "epoch": 9.94662480376766,
      "grad_norm": 3.746351718902588,
      "learning_rate": 4.378335949764522e-05,
      "loss": 0.6356,
      "step": 633600
    },
    {
      "epoch": 9.948194662480377,
      "grad_norm": 3.735839605331421,
      "learning_rate": 4.378237833594976e-05,
      "loss": 0.6314,
      "step": 633700
    },
    {
      "epoch": 9.949764521193092,
      "grad_norm": 3.847181558609009,
      "learning_rate": 4.378139717425432e-05,
      "loss": 0.6354,
      "step": 633800
    },
    {
      "epoch": 9.95133437990581,
      "grad_norm": 5.145418643951416,
      "learning_rate": 4.378041601255887e-05,
      "loss": 0.5921,
      "step": 633900
    },
    {
      "epoch": 9.952904238618524,
      "grad_norm": 4.853922367095947,
      "learning_rate": 4.377943485086343e-05,
      "loss": 0.6248,
      "step": 634000
    },
    {
      "epoch": 9.95447409733124,
      "grad_norm": 2.277501344680786,
      "learning_rate": 4.3778453689167974e-05,
      "loss": 0.663,
      "step": 634100
    },
    {
      "epoch": 9.956043956043956,
      "grad_norm": 4.857813835144043,
      "learning_rate": 4.377747252747253e-05,
      "loss": 0.5645,
      "step": 634200
    },
    {
      "epoch": 9.957613814756671,
      "grad_norm": 4.498058319091797,
      "learning_rate": 4.377649136577708e-05,
      "loss": 0.6575,
      "step": 634300
    },
    {
      "epoch": 9.959183673469388,
      "grad_norm": 4.364256381988525,
      "learning_rate": 4.377551020408163e-05,
      "loss": 0.6116,
      "step": 634400
    },
    {
      "epoch": 9.960753532182103,
      "grad_norm": 3.2823073863983154,
      "learning_rate": 4.3774529042386184e-05,
      "loss": 0.6067,
      "step": 634500
    },
    {
      "epoch": 9.96232339089482,
      "grad_norm": 3.2294678688049316,
      "learning_rate": 4.377354788069074e-05,
      "loss": 0.6315,
      "step": 634600
    },
    {
      "epoch": 9.963893249607535,
      "grad_norm": 3.46704363822937,
      "learning_rate": 4.377256671899529e-05,
      "loss": 0.6268,
      "step": 634700
    },
    {
      "epoch": 9.96546310832025,
      "grad_norm": 3.3459737300872803,
      "learning_rate": 4.3771585557299844e-05,
      "loss": 0.6122,
      "step": 634800
    },
    {
      "epoch": 9.967032967032967,
      "grad_norm": 3.852193593978882,
      "learning_rate": 4.3770604395604395e-05,
      "loss": 0.6301,
      "step": 634900
    },
    {
      "epoch": 9.968602825745682,
      "grad_norm": 3.3096840381622314,
      "learning_rate": 4.376962323390895e-05,
      "loss": 0.6646,
      "step": 635000
    },
    {
      "epoch": 9.970172684458399,
      "grad_norm": 4.3298797607421875,
      "learning_rate": 4.3768642072213504e-05,
      "loss": 0.6454,
      "step": 635100
    },
    {
      "epoch": 9.971742543171114,
      "grad_norm": 3.7418792247772217,
      "learning_rate": 4.3767660910518055e-05,
      "loss": 0.6509,
      "step": 635200
    },
    {
      "epoch": 9.973312401883831,
      "grad_norm": 3.7832090854644775,
      "learning_rate": 4.3766679748822606e-05,
      "loss": 0.6438,
      "step": 635300
    },
    {
      "epoch": 9.974882260596546,
      "grad_norm": 4.248976707458496,
      "learning_rate": 4.3765698587127164e-05,
      "loss": 0.6554,
      "step": 635400
    },
    {
      "epoch": 9.976452119309261,
      "grad_norm": 3.9852499961853027,
      "learning_rate": 4.3764717425431715e-05,
      "loss": 0.5954,
      "step": 635500
    },
    {
      "epoch": 9.978021978021978,
      "grad_norm": 4.685482501983643,
      "learning_rate": 4.3763736263736265e-05,
      "loss": 0.6738,
      "step": 635600
    },
    {
      "epoch": 9.979591836734693,
      "grad_norm": 5.051169395446777,
      "learning_rate": 4.376275510204082e-05,
      "loss": 0.6486,
      "step": 635700
    },
    {
      "epoch": 9.98116169544741,
      "grad_norm": 3.536027193069458,
      "learning_rate": 4.376177394034537e-05,
      "loss": 0.6231,
      "step": 635800
    },
    {
      "epoch": 9.982731554160125,
      "grad_norm": 3.1816678047180176,
      "learning_rate": 4.3760792778649925e-05,
      "loss": 0.6389,
      "step": 635900
    },
    {
      "epoch": 9.984301412872842,
      "grad_norm": 3.97847580909729,
      "learning_rate": 4.3759811616954476e-05,
      "loss": 0.6999,
      "step": 636000
    },
    {
      "epoch": 9.985871271585557,
      "grad_norm": 3.820483684539795,
      "learning_rate": 4.3758830455259034e-05,
      "loss": 0.6088,
      "step": 636100
    },
    {
      "epoch": 9.987441130298274,
      "grad_norm": 3.6788547039031982,
      "learning_rate": 4.375784929356358e-05,
      "loss": 0.6283,
      "step": 636200
    },
    {
      "epoch": 9.989010989010989,
      "grad_norm": 4.270578861236572,
      "learning_rate": 4.3756868131868136e-05,
      "loss": 0.5851,
      "step": 636300
    },
    {
      "epoch": 9.990580847723704,
      "grad_norm": 3.3676819801330566,
      "learning_rate": 4.375588697017269e-05,
      "loss": 0.6039,
      "step": 636400
    },
    {
      "epoch": 9.992150706436421,
      "grad_norm": 3.3410348892211914,
      "learning_rate": 4.375490580847724e-05,
      "loss": 0.5929,
      "step": 636500
    },
    {
      "epoch": 9.993720565149136,
      "grad_norm": 4.511521816253662,
      "learning_rate": 4.375392464678179e-05,
      "loss": 0.6665,
      "step": 636600
    },
    {
      "epoch": 9.995290423861853,
      "grad_norm": 4.214759349822998,
      "learning_rate": 4.3752943485086347e-05,
      "loss": 0.6342,
      "step": 636700
    },
    {
      "epoch": 9.996860282574568,
      "grad_norm": 4.383114337921143,
      "learning_rate": 4.37519623233909e-05,
      "loss": 0.6443,
      "step": 636800
    },
    {
      "epoch": 9.998430141287285,
      "grad_norm": 3.4905757904052734,
      "learning_rate": 4.375098116169545e-05,
      "loss": 0.6693,
      "step": 636900
    },
    {
      "epoch": 10.0,
      "grad_norm": 4.428003787994385,
      "learning_rate": 4.375e-05,
      "loss": 0.6601,
      "step": 637000
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.0186415910720825,
      "eval_runtime": 14.6746,
      "eval_samples_per_second": 228.49,
      "eval_steps_per_second": 228.49,
      "step": 637000
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.4953402578830719,
      "eval_runtime": 281.0332,
      "eval_samples_per_second": 226.664,
      "eval_steps_per_second": 226.664,
      "step": 637000
    }
  ],
  "logging_steps": 100,
  "max_steps": 5096000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 80,
  "save_steps": 10000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 394180899840000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
